1
00:00:04,010 --> 00:00:07,680
So today we are going to discuss, uh,

2
00:00:07,680 --> 00:00:11,190
heterogeneous graphs and, uh, knowledge graph embeddings.

3
00:00:11,190 --> 00:00:14,430
And in particular, we will be focusing on methods for,

4
00:00:14,430 --> 00:00:17,025
uh, knowledge graph completion.

5
00:00:17,025 --> 00:00:20,555
So the topic for today is about, uh,

6
00:00:20,555 --> 00:00:22,420
first to talk about,

7
00:00:22,420 --> 00:00:25,770
um, uh, a concept called heterogeneous graphs.

8
00:00:25,770 --> 00:00:28,770
And, uh, basically so far in our course,

9
00:00:28,770 --> 00:00:34,200
we had been handling graphs that have only one edge type and only one node type.

10
00:00:34,200 --> 00:00:38,760
We just said there is nodes and edges and you know nodes are connected with edges.

11
00:00:38,760 --> 00:00:40,290
So now the question would be,

12
00:00:40,290 --> 00:00:42,595
how do we handle graphs,

13
00:00:42,595 --> 00:00:46,490
directed- undirected graphs that have multiple types of, uh,

14
00:00:46,490 --> 00:00:48,620
connections, multiple types of nodes,

15
00:00:48,620 --> 00:00:50,600
multiple types of links between them.

16
00:00:50,600 --> 00:00:54,220
Um, and this is the notion of heterogeneous graphs.

17
00:00:54,220 --> 00:00:56,570
Uh, heterogeneous in a sense that they contain

18
00:00:56,570 --> 00:01:01,655
heterogeneous sets of entities and heterogeneous set of, uh, edges.

19
00:01:01,655 --> 00:01:06,290
Uh, and what we are going to talk today about in the first part is to discuss about, uh,

20
00:01:06,290 --> 00:01:09,730
relational GCN that takes care about different,

21
00:01:09,730 --> 00:01:11,935
uh, edge types, different relation types.

22
00:01:11,935 --> 00:01:13,960
And then we are going to, uh,

23
00:01:13,960 --> 00:01:17,015
talk about more specifically about knowledge graphs,

24
00:01:17,015 --> 00:01:19,580
which are a class of heterogeneous graphs.

25
00:01:19,580 --> 00:01:22,325
Um, how do we do knowledge graph embedding?

26
00:01:22,325 --> 00:01:24,350
And also how do we do, uh,

27
00:01:24,350 --> 00:01:28,240
a very common task which is called knowledge graph completion.

28
00:01:28,240 --> 00:01:30,955
So let's start with, uh,

29
00:01:30,955 --> 00:01:34,950
heterogeneous graphs and discuss relational GCNs.

30
00:01:34,950 --> 00:01:39,100
So relational graph convolutional, uh, neural network.

31
00:01:39,100 --> 00:01:41,990
So a heterogeneous graph is,

32
00:01:41,990 --> 00:01:44,284
uh, defined by a quadruple,

33
00:01:44,284 --> 00:01:47,060
is defined by a set of nodes, uh,

34
00:01:47,060 --> 00:01:48,380
we'll call them V. Uh,

35
00:01:48,380 --> 00:01:50,900
nodes could have, uh, different types.

36
00:01:50,900 --> 00:01:54,650
Uh, it is defined by a set of edges that connect

37
00:01:54,650 --> 00:01:59,135
different nodes and each edge is a- has a different,

38
00:01:59,135 --> 00:02:00,670
uh, uh, relation type.

39
00:02:00,670 --> 00:02:06,455
So this means that edge is now a triple that says node i is connected to node j,

40
00:02:06,455 --> 00:02:08,164
via relation, uh,

41
00:02:08,164 --> 00:02:10,874
R. And then also each node,

42
00:02:10,875 --> 00:02:16,050
um, is of a different type and I can get the type of the node by,

43
00:02:16,050 --> 00:02:18,210
uh, by looking into this, uh, uh,

44
00:02:18,210 --> 00:02:20,340
function or set, uh,

45
00:02:20,340 --> 00:02:25,220
T. So basically I have nodes and edges where every node is labeled by a type,

46
00:02:25,220 --> 00:02:26,960
and every relation, um,

47
00:02:26,960 --> 00:02:29,110
is labeled, uh, by a type.

48
00:02:29,110 --> 00:02:32,990
So what would be some examples of heterogeneous graphs?

49
00:02:32,990 --> 00:02:37,715
Uh, one very common example is in biomedical, um, uh,

50
00:02:37,715 --> 00:02:41,715
field where you can basically take different biomedical entities, um,

51
00:02:41,715 --> 00:02:43,935
and then relationships between them,

52
00:02:43,935 --> 00:02:46,385
and represent this as a heterogeneous graph.

53
00:02:46,385 --> 00:02:48,545
So for example, in- in this case,

54
00:02:48,545 --> 00:02:50,780
I would have different, uh, node types.

55
00:02:50,780 --> 00:02:51,980
I would have diseases,

56
00:02:51,980 --> 00:02:53,180
I would have drugs,

57
00:02:53,180 --> 00:02:54,725
I would have proteins.

58
00:02:54,725 --> 00:02:59,520
I would have different edge types which is different types of relations between these,

59
00:02:59,520 --> 00:03:01,250
uh, different types of entities.

60
00:03:01,250 --> 00:03:03,800
And then you know, an example of the node here could

61
00:03:03,800 --> 00:03:06,580
be the- could be the disease, uh, migraine.

62
00:03:06,580 --> 00:03:08,670
And, uh, then I also have,

63
00:03:08,670 --> 00:03:10,680
uh, different, uh, edge for example,

64
00:03:10,680 --> 00:03:12,030
it would be that, uh,

65
00:03:12,030 --> 00:03:15,510
a particular drug treats a particular, uh, disease.

66
00:03:15,510 --> 00:03:17,280
And then, um, you know,

67
00:03:17,280 --> 00:03:21,290
how can this- and- and then I can have different types of relations between them,

68
00:03:21,290 --> 00:03:23,510
like causes, uh, uh,

69
00:03:23,510 --> 00:03:24,840
and so on and so forth.

70
00:03:24,840 --> 00:03:26,660
So that is one example, uh,

71
00:03:26,660 --> 00:03:29,250
of a-, uh, of a heterogeneous graph,

72
00:03:29,250 --> 00:03:30,780
the different types of entities,

73
00:03:30,780 --> 00:03:32,325
uh, and different, uh,

74
00:03:32,325 --> 00:03:34,740
types of, um, uh,

75
00:03:34,740 --> 00:03:36,585
uh, relationships between them.

76
00:03:36,585 --> 00:03:41,425
You can also think of event graphs as another example of a heterogenous graph,

77
00:03:41,425 --> 00:03:44,090
where again, I could have different types of nodes,

78
00:03:44,090 --> 00:03:45,499
different types of entities.

79
00:03:45,499 --> 00:03:47,120
Uh, like for example,

80
00:03:47,120 --> 00:03:48,980
a node type could be a flight.

81
00:03:48,980 --> 00:03:51,080
I could have different, uh, edge types,

82
00:03:51,080 --> 00:03:53,670
it would be a destination of a flight, um,

83
00:03:53,670 --> 00:03:54,915
and then I could have,

84
00:03:54,915 --> 00:03:56,990
you know, um, the destination,

85
00:03:56,990 --> 00:03:59,810
different airports like the SFO, uh,

86
00:03:59,810 --> 00:04:01,910
and create different types of, uh,

87
00:04:01,910 --> 00:04:05,245
relationships, uh, between these, uh, different entities.

88
00:04:05,245 --> 00:04:08,710
This is, uh, another example of a heterogeneous graph.

89
00:04:08,710 --> 00:04:12,710
So now if we are given these type of a graph where we

90
00:04:12,710 --> 00:04:16,670
have different types of nodes and different types of relations between them,

91
00:04:16,670 --> 00:04:21,260
then we would like to be able for our model to handle this heterogeneity,

92
00:04:21,260 --> 00:04:23,165
in some sense, to take advantage of it,

93
00:04:23,165 --> 00:04:24,590
to learn that, uh,

94
00:04:24,590 --> 00:04:26,780
that you know, treats, uh,

95
00:04:26,780 --> 00:04:29,870
a given drug treating a disease is a different type of relationship,

96
00:04:29,870 --> 00:04:33,425
than a given protein interacting with another, uh, protein.

97
00:04:33,425 --> 00:04:36,140
So, um, wha- how are we going to do this?

98
00:04:36,140 --> 00:04:39,560
Is we are actually going to extend our, um, GCN.

99
00:04:39,560 --> 00:04:43,365
So, um, um, uh, graph convolutional, uh,

100
00:04:43,365 --> 00:04:48,380
neural network, uh, to be able to handle different types of, uh, relationships.

101
00:04:48,380 --> 00:04:51,820
We will call this our relational, uh, GCN.

102
00:04:51,820 --> 00:04:53,745
Um, and we will,

103
00:04:53,745 --> 00:04:55,995
uh, so far, right, we define the, uh,

104
00:04:55,995 --> 00:05:00,590
GCN on one type of a relation and one type of a node type,

105
00:05:00,590 --> 00:05:02,035
so on simple graphs,

106
00:05:02,035 --> 00:05:04,505
and now we would like to expand it to,

107
00:05:04,505 --> 00:05:07,205
uh, more complex heterogeneous graphs.

108
00:05:07,205 --> 00:05:10,550
Um, the way I want to explain this is first to remind you about

109
00:05:10,550 --> 00:05:14,910
the GCN and then it will become very natural, how do we extend it?

110
00:05:14,910 --> 00:05:19,235
So, uh, the first question when you think about a graph neural network is, you know,

111
00:05:19,235 --> 00:05:21,695
how do we- how does a GCN, uh,

112
00:05:21,695 --> 00:05:25,325
operate and how does it update a representation of a given,

113
00:05:25,325 --> 00:05:27,430
uh, target node, uh, in the graph?

114
00:05:27,430 --> 00:05:30,330
And we have discussed these through this notion of, uh, uh,

115
00:05:30,330 --> 00:05:33,080
message passing and- and computational graph,

116
00:05:33,080 --> 00:05:34,520
where for a given target node,

117
00:05:34,520 --> 00:05:38,480
let's say A, here we see its corresponding, uh, computation graph,

118
00:05:38,480 --> 00:05:43,045
um, where every node gets information from its neighbors.

119
00:05:43,045 --> 00:05:47,915
Notice that perhaps from the previous classes we talked about undirected graphs,

120
00:05:47,915 --> 00:05:50,465
here now the graph is directed,

121
00:05:50,465 --> 00:05:52,685
so every node only, uh,

122
00:05:52,685 --> 00:05:55,805
gathers information along the direction of the edges.

123
00:05:55,805 --> 00:05:58,280
So A gets information from B, C,

124
00:05:58,280 --> 00:05:59,840
and D. But for example,

125
00:05:59,840 --> 00:06:01,770
uh, node B, even though, um,

126
00:06:01,770 --> 00:06:04,580
uh, it has two edges adjacent to it,

127
00:06:04,580 --> 00:06:09,710
it only gathers information from C because C points to it and B points to A.

128
00:06:09,710 --> 00:06:12,670
So that's why, for example here, uh,

129
00:06:12,670 --> 00:06:15,810
the graph structure- the- the computation graph structure,

130
00:06:15,810 --> 00:06:16,895
uh, is like this.

131
00:06:16,895 --> 00:06:18,650
So this is now how you can, uh,

132
00:06:18,650 --> 00:06:20,810
take into consideration, uh,

133
00:06:20,810 --> 00:06:23,215
relations, uh, of edges.

134
00:06:23,215 --> 00:06:26,160
Now, uh, that we have, uh,

135
00:06:26,160 --> 00:06:27,825
re- reminded ourselves of, uh, uh,

136
00:06:27,825 --> 00:06:30,525
uh, wha- how to, uh,

137
00:06:30,525 --> 00:06:34,125
unfold, uh, the graph neural network over a directed graph,

138
00:06:34,125 --> 00:06:38,600
we then talked about what does define a single layer of a graph neural network?

139
00:06:38,600 --> 00:06:40,505
And we defined this notion of a message,

140
00:06:40,505 --> 00:06:43,035
where every me- every node, uh,

141
00:06:43,035 --> 00:06:45,645
computes, uh, the message coming,

142
00:06:45,645 --> 00:06:48,600
uh, uh, from its, uh, children and this message,

143
00:06:48,600 --> 00:06:51,300
uh, depends, uh, both on its, uh,

144
00:06:51,300 --> 00:06:54,090
uh, on its message from the previous level,

145
00:06:54,090 --> 00:06:56,115
um, as well as, uh,

146
00:06:56,115 --> 00:06:57,705
the- the neighbors, uh,

147
00:06:57,705 --> 00:07:00,600
that- that it has at the previous level.

148
00:07:00,600 --> 00:07:04,070
And then the way the aggregation of these messages happens is

149
00:07:04,070 --> 00:07:08,169
that- that the node V we will basically take these transformed messages,

150
00:07:08,169 --> 00:07:12,275
uh, from each of the children and aggregate it in some way.

151
00:07:12,275 --> 00:07:16,280
Uh, combine it with its own message and produce the message,

152
00:07:16,280 --> 00:07:20,170
uh, or the embedding of the node, uh, at the next level.

153
00:07:20,170 --> 00:07:25,490
And in order to add expressivity in terms of feature transformations,

154
00:07:25,490 --> 00:07:28,190
not in terms of capturing the graph structure,

155
00:07:28,190 --> 00:07:30,350
but in terms of capturing the feature, uh,

156
00:07:30,350 --> 00:07:33,780
transformations, uh, we can then also add the,

157
00:07:33,780 --> 00:07:37,584
uh, nonlinearity activation functions like sigmoid,

158
00:07:37,584 --> 00:07:39,720
uh, ReLU, uh, and so on.

159
00:07:39,720 --> 00:07:42,930
Um, and this is how we think of,

160
00:07:42,930 --> 00:07:44,375
uh, graph neural networks.

161
00:07:44,375 --> 00:07:47,105
Now, going back to the GCN,

162
00:07:47,105 --> 00:07:50,220
and a- and a single layer of GCN is defined,

163
00:07:50,220 --> 00:07:52,160
uh, by the following formula, right?

164
00:07:52,160 --> 00:07:55,460
It's basically a node goes over its neighbors u

165
00:07:55,460 --> 00:07:58,985
takes the previous layer representations of the nodes,

166
00:07:58,985 --> 00:08:02,280
normalizes it by the- by the, uh,

167
00:08:02,280 --> 00:08:04,760
parent's n-degrees, sums them up,

168
00:08:04,760 --> 00:08:07,670
transforms them, and sends them through a non-linearity.

169
00:08:07,670 --> 00:08:09,965
So, uh, what does this mean is that, uh,

170
00:08:09,965 --> 00:08:16,230
the way you can think of this is that the aggregation of a- of a GCN is,

171
00:08:16,230 --> 00:08:19,280
uh, uh, average pooling type of operator,

172
00:08:19,280 --> 00:08:22,100
because of a summation and normalization here.

173
00:08:22,100 --> 00:08:24,920
And then the message transformation is simple,

174
00:08:24,920 --> 00:08:27,810
uh, linear, uh, transformation.

175
00:08:27,810 --> 00:08:33,130
So now what happens if we have multiple relation types, right?

176
00:08:33,130 --> 00:08:34,626
What if I have a

177
00:08:34,626 --> 00:08:37,525
graph that has multiple types of relations?

178
00:08:37,525 --> 00:08:42,250
Er, the way I will denote this is that I will use edges of different colors,

179
00:08:42,250 --> 00:08:46,165
um, and, you know, here I labeled them as r_1, r_2, r_3.

180
00:08:46,165 --> 00:08:47,560
Where it basically says, aha,

181
00:08:47,560 --> 00:08:51,700
that A and B are connected according to the relation r_1, while,

182
00:08:51,700 --> 00:08:53,890
you know, uh, B and C, er,

183
00:08:53,890 --> 00:08:55,510
are connected by, er,

184
00:08:55,510 --> 00:08:57,310
a relation, er, r_3.

185
00:08:57,310 --> 00:08:59,440
So that's, er, the way, er,

186
00:08:59,440 --> 00:09:02,470
we are going to think of the input graph now.

187
00:09:02,470 --> 00:09:05,380
And the way we can now generalize a

188
00:09:05,380 --> 00:09:12,490
GCN to the- this kind of heterogeneous graph is to use different network transformations,

189
00:09:12,490 --> 00:09:14,725
different neural network, uh, weights,

190
00:09:14,725 --> 00:09:18,865
different neural network parameters for different, er, relation types.

191
00:09:18,865 --> 00:09:22,030
So it means that when we are doing relation transformation,

192
00:09:22,030 --> 00:09:24,100
when we are doing message transformation,

193
00:09:24,100 --> 00:09:30,895
we are not going to apply the same matrix W for every incoming edge,

194
00:09:30,895 --> 00:09:34,705
but we are going to apply a different matrix W, um,

195
00:09:34,705 --> 00:09:37,570
depending on whether- um,

196
00:09:37,570 --> 00:09:41,440
er, what is the kind of the relation that we are considering.

197
00:09:41,440 --> 00:09:44,125
So if we have three different relation types,

198
00:09:44,125 --> 00:09:46,495
we are going to use three different types of, uh,

199
00:09:46,495 --> 00:09:48,835
W matrices; one for relation 1,

200
00:09:48,835 --> 00:09:52,240
one for relation 2, and one for, uh, relation 3.

201
00:09:52,240 --> 00:09:55,600
So now, if I look at how does the, uh,

202
00:09:55,600 --> 00:09:59,845
neural network, uh, computation graph for, uh,

203
00:09:59,845 --> 00:10:01,405
the node A look like,

204
00:10:01,405 --> 00:10:06,280
what is different now is that these transformation- message transformation operators,

205
00:10:06,280 --> 00:10:10,105
these Ws, now have a color associated with them.

206
00:10:10,105 --> 00:10:12,550
So it means that the message coming from, let's say,

207
00:10:12,550 --> 00:10:16,030
node B and the message coming from node D, uh,

208
00:10:16,030 --> 00:10:20,275
towards node A will be transformed using the same operator, the same W,

209
00:10:20,275 --> 00:10:25,435
because the message is traveling along the relation type, uh, uh, r_1.

210
00:10:25,435 --> 00:10:28,645
While, for example, the message from node C that,

211
00:10:28,645 --> 00:10:30,400
uh, is connected to node A with, er,

212
00:10:30,400 --> 00:10:36,130
relation r_2 will be transformed with a different transformation operator, the red one.

213
00:10:36,130 --> 00:10:39,610
The red one is, er, designed for, er, r_2.

214
00:10:39,610 --> 00:10:42,550
So here, um, you see- uh,

215
00:10:42,550 --> 00:10:43,900
you see the difference, right?

216
00:10:43,900 --> 00:10:45,520
Then, for example, um,

217
00:10:45,520 --> 00:10:49,570
this means that now what- how we are going to write this up is that we have-

218
00:10:49,570 --> 00:10:54,025
we have these transformation matrices W that are also indexed,

219
00:10:54,025 --> 00:10:55,960
uh, by the relation type, right?

220
00:10:55,960 --> 00:10:58,435
So in a relational GCN,

221
00:10:58,435 --> 00:11:01,870
the way we write out the message propagation and

222
00:11:01,870 --> 00:11:05,755
aggregation is the following: we basically say, um,

223
00:11:05,755 --> 00:11:09,595
to compute the message of node v at level l plus 1,

224
00:11:09,595 --> 00:11:14,260
we are going to sum up over all the relation types.

225
00:11:14,260 --> 00:11:15,970
For every relation type,

226
00:11:15,970 --> 00:11:19,825
you are going to look at who are the neighbors, uh, of, uh,

227
00:11:19,825 --> 00:11:24,955
node v that- that are connected to node v according to this relation type

228
00:11:24,955 --> 00:11:30,865
r. Then we are going to take the transformation matrix specific to that,

229
00:11:30,865 --> 00:11:33,865
er, relation r to take the,

230
00:11:33,865 --> 00:11:35,860
uh, messages from the- uh,

231
00:11:35,860 --> 00:11:37,435
from the neighbors that are- er,

232
00:11:37,435 --> 00:11:42,460
neighbors u that are connected to v according to- to this relation type r,

233
00:11:42,460 --> 00:11:44,920
and we are are also going then to do the

234
00:11:44,920 --> 00:11:47,920
me- the embedding of the node v from previous layer,

235
00:11:47,920 --> 00:11:51,610
er, and, um, er, er transform it.

236
00:11:51,610 --> 00:11:54,940
Uh, the- the important difference here is that Ws,

237
00:11:54,940 --> 00:11:58,345
before, they used to be only differ layer by layer,

238
00:11:58,345 --> 00:12:03,280
now what happens here is that we have W for every layer,

239
00:12:03,280 --> 00:12:05,515
for every, uh, relation.

240
00:12:05,515 --> 00:12:10,675
So, uh, another thing I did not explain is what is this C, uh,

241
00:12:10,675 --> 00:12:14,260
er, sub v, r. This is simply a normalized node,

242
00:12:14,260 --> 00:12:17,470
er, in degree compared to that, er, relation.

243
00:12:17,470 --> 00:12:20,470
So it's basically the number of incoming, er, er,

244
00:12:20,470 --> 00:12:23,620
relations of a given type r to the node, er,

245
00:12:23,620 --> 00:12:27,970
v. This is still a- a graph neural network.

246
00:12:27,970 --> 00:12:30,160
It is still- I can write it out in

247
00:12:30,160 --> 00:12:33,850
this formalism of a message transformation and aggregation,

248
00:12:33,850 --> 00:12:37,465
where basically each neighbor of a given- uh,

249
00:12:37,465 --> 00:12:40,765
of a given node v transforms its- er,

250
00:12:40,765 --> 00:12:45,280
its previous layer embedding according to the matrix W,

251
00:12:45,280 --> 00:12:47,980
but now W changes for every, er,

252
00:12:47,980 --> 00:12:50,470
re- based on the type of the relation between,

253
00:12:50,470 --> 00:12:52,750
uh, you and me and, er, you know,

254
00:12:52,750 --> 00:12:55,480
here we transform the- the message,

255
00:12:55,480 --> 00:12:56,830
uh, from node v,

256
00:12:56,830 --> 00:12:59,200
from the previous layer to, the, uh,

257
00:12:59,200 --> 00:13:02,320
embedding from the previous layer into the message, uh,

258
00:13:02,320 --> 00:13:05,155
for the- from- for the- from the previous layer,

259
00:13:05,155 --> 00:13:07,570
and then the aggregation is still,

260
00:13:07,570 --> 00:13:09,070
uh, simply a summation, right?

261
00:13:09,070 --> 00:13:10,510
We take these, um,

262
00:13:10,510 --> 00:13:16,900
transformed messages fro- based on the embeddings from the neighbors from previous layer,

263
00:13:16,900 --> 00:13:19,930
where the transformation is relation-specific,

264
00:13:19,930 --> 00:13:25,690
as well as node's own embedding from the previous layer uh, transformed.

265
00:13:25,690 --> 00:13:28,690
We add all these together to get the embedding,

266
00:13:28,690 --> 00:13:30,175
uh, at the next layer.

267
00:13:30,175 --> 00:13:31,825
So this is called,

268
00:13:31,825 --> 00:13:34,135
a relational, er, GCN.

269
00:13:34,135 --> 00:13:37,750
So, um, basically, a relational graph neural

270
00:13:37,750 --> 00:13:41,620
network or a relational graph convolutional neural network,

271
00:13:41,620 --> 00:13:46,870
where the main difference is that now we have different message transformation,

272
00:13:46,870 --> 00:13:52,480
er, operators based on the type of the relationship between a pair of nodes.

273
00:13:52,480 --> 00:13:54,760
And this is denoted by this, uh, subscript,

274
00:13:54,760 --> 00:13:56,710
uh, r that denotes,

275
00:13:56,710 --> 00:13:58,675
er, the relation type.

276
00:13:58,675 --> 00:14:04,615
So, um, this is how we defined a RGCN.

277
00:14:04,615 --> 00:14:06,730
Um, the point is now, right?

278
00:14:06,730 --> 00:14:09,820
We said that for every relation r,

279
00:14:09,820 --> 00:14:13,135
we need L- capital L matrices, meaning, um,

280
00:14:13,135 --> 00:14:16,030
these transformation matrices are different for

281
00:14:16,030 --> 00:14:19,780
every layer of the neural network- of the graph neural network,

282
00:14:19,780 --> 00:14:22,780
and they are different for every relation.

283
00:14:22,780 --> 00:14:27,460
So the problem then becomes if you ask what is the size of this matrix?

284
00:14:27,460 --> 00:14:32,230
The size of the matrix W is simply the embedding dimension, er,

285
00:14:32,230 --> 00:14:35,215
on the- on the lower layer, um,

286
00:14:35,215 --> 00:14:38,350
times the embedding dimension at the- at the upper layer, right?

287
00:14:38,350 --> 00:14:41,500
So it's basically d, er, ^l and d, er,

288
00:14:41,500 --> 00:14:44,110
^l plus 1 are the embedding dimension at

289
00:14:44,110 --> 00:14:48,070
layer l and the embedding dimension at layer, er, l plus 1.

290
00:14:48,070 --> 00:14:50,740
And these can quickly be a few hundred- you know,

291
00:14:50,740 --> 00:14:53,410
these d's are in the order of a few hundred,

292
00:14:53,410 --> 00:14:56,320
maybe up to a- up to 1,000, right?

293
00:14:56,320 --> 00:15:00,220
So now the problem is that I have one such matrix per

294
00:15:00,220 --> 00:15:05,020
every layer and I have one such matrix per every relation type.

295
00:15:05,020 --> 00:15:08,065
And because, uh, heterogeneous graphs,

296
00:15:08,065 --> 00:15:11,050
and in particular, knowledge graphs, can have hundreds,

297
00:15:11,050 --> 00:15:14,545
can have thousands of different, er, relation types,

298
00:15:14,545 --> 00:15:16,615
then you get- uh,

299
00:15:16,615 --> 00:15:21,190
you can get tens of thousands of these different, er, matrices.

300
00:15:21,190 --> 00:15:26,185
And each matrix can ha- is- is dense and can have quite big size.

301
00:15:26,185 --> 00:15:29,529
And the problem then becomes that the number of parameters

302
00:15:29,529 --> 00:15:32,875
of this model- of this RGCN model, um,

303
00:15:32,875 --> 00:15:35,650
tends to explode because, uh,

304
00:15:35,650 --> 00:15:38,350
it grows with the number of different relation types,

305
00:15:38,350 --> 00:15:41,215
and there can be thousands of different relation types.

306
00:15:41,215 --> 00:15:45,400
And one problem is that the model becomes too big, um, to train,

307
00:15:45,400 --> 00:15:48,745
and then the other problem is that the model has too many parameters,

308
00:15:48,745 --> 00:15:52,480
um, and overfitting can quickly become an issue.

309
00:15:52,480 --> 00:15:57,460
So what I wanna discuss next is, er, two approaches,

310
00:15:57,460 --> 00:16:02,950
how to reduce the number of parameters of this RGCN-style model,

311
00:16:02,950 --> 00:16:04,750
um, using two techniques.

312
00:16:04,750 --> 00:16:06,670
One technique will be to use, uh,

313
00:16:06,670 --> 00:16:09,055
block diagonal matrices, uh,

314
00:16:09,055 --> 00:16:11,845
and the other one will be to use basis or,

315
00:16:11,845 --> 00:16:14,350
uh, dictionary learning as it is called.

316
00:16:14,350 --> 00:16:16,885
So let me first talk about, uh,

317
00:16:16,885 --> 00:16:19,930
use of block diagonal, er, matrices.

318
00:16:19,930 --> 00:16:25,420
So the key insight is that we wanna make these Ws- W matrices,

319
00:16:25,420 --> 00:16:29,095
the transformation matrices, we wanna make them sparse.

320
00:16:29,095 --> 00:16:34,570
And one way to make them sparse is to enforce, to have this block diagonal structure.

321
00:16:34,570 --> 00:16:39,459
So basically, non-zero elements are only along the specific blocks,

322
00:16:39,459 --> 00:16:40,810
uh, of this bigger,

323
00:16:40,810 --> 00:16:42,520
er, matrix, er, W.

324
00:16:42,520 --> 00:16:44,605
Uh, if you think about this,

325
00:16:44,605 --> 00:16:49,210
this will now reduce the number of non-zero, uh,

326
00:16:49,210 --> 00:16:52,735
elements- the number of parameters in each W you have to estimate,

327
00:16:52,735 --> 00:16:55,675
because you only have to estimate that the blocks,

328
00:16:55,675 --> 00:16:57,850
uh, these two green blocks,

329
00:16:57,850 --> 00:17:00,040
and you can basically ignore or assumed these,

330
00:17:00,040 --> 00:17:02,890
uh, empty parts of the matrix are 0.

331
00:17:02,890 --> 00:17:04,480
So if you assume,

332
00:17:04,480 --> 00:17:09,505
for example that W is composed from B, uh, low dimensional,

333
00:17:09,505 --> 00:17:12,280
uh, matrices, low-dimensional blocks,

334
00:17:12,280 --> 00:17:15,743
then the number of parameters is going to decrease,

335
00:17:15,743 --> 00:17:17,244
uh, by a factor B.

336
00:17:17,244 --> 00:17:19,194
So if you said B by 10, uh,

337
00:17:19,194 --> 00:17:23,664
you have just reduced the number of free parameters that you need to learn,

338
00:17:23,665 --> 00:17:26,665
uh, or estimate by a factor of 10.

339
00:17:26,665 --> 00:17:29,350
Of course, what do you lose?

340
00:17:29,350 --> 00:17:33,475
What you lose is now that if you think about this as a transformation matrix,

341
00:17:33,475 --> 00:17:36,070
what you lose is that, um,

342
00:17:36,070 --> 00:17:39,655
embedding dimensions that are far apart from each other,

343
00:17:39,655 --> 00:17:41,830
they cannot interact with each other, right?

344
00:17:41,830 --> 00:17:43,675
So this means that, for example here,

345
00:17:43,675 --> 00:17:47,380
only embedding dimensions 1 and 2 can interact with each other,

346
00:17:47,380 --> 00:17:51,850
but not, um, let's say 2 and 3 because they are in different blocks.

347
00:17:51,850 --> 00:17:55,435
So it may require several layers of propagations

348
00:17:55,435 --> 00:17:59,680
and- and different structures of block diagonal matrices to be able for,

349
00:17:59,680 --> 00:18:05,590
a embedding dimension 3 to kind of talk to embedding dimension 2.

350
00:18:05,590 --> 00:18:10,120
Um, so basically what this means is that only nearby neurons or neurons

351
00:18:10,120 --> 00:18:14,470
in the same block can talk to each other and exchange information with each other.

352
00:18:14,470 --> 00:18:16,630
So perhaps your GN- GCN,

353
00:18:16,630 --> 00:18:18,880
GNN may need to be a bit deeper,

354
00:18:18,880 --> 00:18:20,590
but you have reduced, uh,

355
00:18:20,590 --> 00:18:22,180
the number of parameters, uh,

356
00:18:22,180 --> 00:18:24,580
significantly, which will lead to, uh,

357
00:18:24,580 --> 00:18:27,205
faster training, perhaps more robust model,

358
00:18:27,205 --> 00:18:29,755
less overfitting, um, and so on.

359
00:18:29,755 --> 00:18:32,545
So that's, uh, the technique of,

360
00:18:32,545 --> 00:18:34,855
uh, block diagonal matrices, uh,

361
00:18:34,855 --> 00:18:37,780
where basically you reduce the dimensionality of

362
00:18:37,780 --> 00:18:42,889
each W by assuming a block diagonal structure.

363
00:18:43,080 --> 00:18:47,095
Uh, second idea how to, uh, make this, uh,

364
00:18:47,095 --> 00:18:52,540
RGCN more scalable and its learning more tractable is to- to

365
00:18:52,540 --> 00:18:54,970
build on the key insight which is we wanna

366
00:18:54,970 --> 00:18:58,210
share weights across different relations, right?

367
00:18:58,210 --> 00:19:02,635
We don't wanna consider relations as independent from each other,

368
00:19:02,635 --> 00:19:06,205
but we want relations to kind of share weights,

369
00:19:06,205 --> 00:19:09,130
uh, to also share some information.

370
00:19:09,130 --> 00:19:11,440
And the way you can achieve this very

371
00:19:11,440 --> 00:19:15,160
elegantly is that you will represent the weight matrix,

372
00:19:15,160 --> 00:19:22,835
the transformation matrix W for each relation as a linear combination of basis,

373
00:19:22,835 --> 00:19:26,730
uh, transformations or as of basis matrices.

374
00:19:26,730 --> 00:19:30,075
And you call these basis matrices as your dictionary.

375
00:19:30,075 --> 00:19:32,825
So this is also called dictionary learning.

376
00:19:32,825 --> 00:19:35,005
So let me now, uh, explain.

377
00:19:35,005 --> 00:19:37,570
The idea is that now your, uh, W,

378
00:19:37,570 --> 00:19:40,660
uh, r is simply, uh,

379
00:19:40,660 --> 00:19:46,255
weighted summation over this dictionary of matrices V,

380
00:19:46,255 --> 00:19:49,600
um, that- and I have B- B of them, right?

381
00:19:49,600 --> 00:19:53,425
So basically the idea is that I'll have a dictionary of matrices V.

382
00:19:53,425 --> 00:19:56,950
Uh, and then to come up with a- with a, um,

383
00:19:56,950 --> 00:19:59,350
transformation matrix for, um,

384
00:19:59,350 --> 00:20:03,460
node W, then basically what I have to do is, uh,

385
00:20:03,460 --> 00:20:07,135
have these, uh, weight- weight- weight scores,

386
00:20:07,135 --> 00:20:09,580
uh, important weights, uh,

387
00:20:09,580 --> 00:20:13,940
a, that say how important is a given matrix,

388
00:20:14,520 --> 00:20:18,760
uh, V_b, for a given relation R, right?

389
00:20:18,760 --> 00:20:20,020
And then the point is that these, uh,

390
00:20:20,020 --> 00:20:22,885
matrices V are shared across all the relations.

391
00:20:22,885 --> 00:20:28,600
So I can think of matrices V as my basis matrices or as my, uh, dictionary.

392
00:20:28,600 --> 00:20:31,600
And then these, uh, weights, uh, A,

393
00:20:31,600 --> 00:20:33,895
those are importance weights,

394
00:20:33,895 --> 00:20:35,290
uh, for each matrix.

395
00:20:35,290 --> 00:20:38,020
So basically I'll say, the transformation matrix for,

396
00:20:38,020 --> 00:20:39,820
uh, a given, um,

397
00:20:39,820 --> 00:20:43,240
relation R is some kind of linear combination of

398
00:20:43,240 --> 00:20:48,925
these basis matrices where these linear combination weights are learned for every,

399
00:20:48,925 --> 00:20:52,060
uh, for every, uh, relation, right?

400
00:20:52,060 --> 00:20:54,970
So this means that now every relation only needs to

401
00:20:54,970 --> 00:20:58,225
learn these importance, uh, weights, um,

402
00:20:58,225 --> 00:21:02,785
which is just capital V number of scalars rather than,

403
00:21:02,785 --> 00:21:06,865
uh, uh, a- a large number of different, uh, transformation matrices.

404
00:21:06,865 --> 00:21:09,400
So now basically, every relation

405
00:21:09,400 --> 00:21:12,910
specific transformation matrix is simply a linear combination,

406
00:21:12,910 --> 00:21:16,900
uh, from this dictionary of, uh, matrices, uh,

407
00:21:16,900 --> 00:21:18,370
V. And that's, uh,

408
00:21:18,370 --> 00:21:20,845
an elegant way how to reduce, uh,

409
00:21:20,845 --> 00:21:24,625
the number of parameters because b can be relatively small,

410
00:21:24,625 --> 00:21:26,260
let's say, I don't know, 100,

411
00:21:26,260 --> 00:21:27,940
uh, or 10 but,

412
00:21:27,940 --> 00:21:29,140
maybe you still have,

413
00:21:29,140 --> 00:21:32,980
um, uh, 1,000 different, uh, relation types.

414
00:21:32,980 --> 00:21:37,250
And you basically say, I will represent every of the 1,000 relations as

415
00:21:37,250 --> 00:21:42,310
a linear combination from my dictionary of 10, uh, weight matrices.

416
00:21:42,310 --> 00:21:45,325
And that's a very elegant way how to reduce,

417
00:21:45,325 --> 00:21:47,830
uh, the number of, uh, parameters.

418
00:21:47,830 --> 00:21:51,205
So now that we have talked about this, uh,

419
00:21:51,205 --> 00:21:53,920
scalability issue of, uh,

420
00:21:53,920 --> 00:21:57,385
uh, RGCN with two different approaches,

421
00:21:57,385 --> 00:22:03,475
one being, um, the block diagonal structure of the transformation matrix W,

422
00:22:03,475 --> 00:22:09,850
the other one being that we cast learning of W as a dictionary learning problem,

423
00:22:09,850 --> 00:22:13,810
which basically means we assume there is an underlying set of 10- let's say,

424
00:22:13,810 --> 00:22:17,050
10 different basis matrices which we call dictionary,

425
00:22:17,050 --> 00:22:19,630
and then every, uh, uh,

426
00:22:19,630 --> 00:22:25,045
relation specific transformation matrix is simply a linear combination of the, uh,

427
00:22:25,045 --> 00:22:29,649
of the matrices coming from the dictionary also reduces

428
00:22:29,649 --> 00:22:34,930
the number of parameters makes the method more robust and, uh, speeds it up.

429
00:22:34,930 --> 00:22:37,960
So now that we have RGCM, uh,

430
00:22:37,960 --> 00:22:41,560
what we want to do is also talk briefly about,

431
00:22:41,560 --> 00:22:45,745
how do you define various prediction tasks on the,

432
00:22:45,745 --> 00:22:49,405
um, on the graph- on the heterogeneous graph?

433
00:22:49,405 --> 00:22:53,500
So in terms of node classification or anti-classification,

434
00:22:53,500 --> 00:22:55,465
um, it is, uh, it is, uh,

435
00:22:55,465 --> 00:22:56,815
it is all- all, uh,

436
00:22:56,815 --> 00:22:58,240
kind of as it used to be, right?

437
00:22:58,240 --> 00:23:04,285
So, uh, RGCN will compute the representation or an embedding for every node,

438
00:23:04,285 --> 00:23:06,010
uh, for every node, um,

439
00:23:06,010 --> 00:23:07,720
then- then based on that embedding,

440
00:23:07,720 --> 00:23:11,320
we can then create a prediction head, uh, for example,

441
00:23:11,320 --> 00:23:13,480
if you want to classify nodes into k,

442
00:23:13,480 --> 00:23:15,565
uh, different classes, uh,

443
00:23:15,565 --> 00:23:18,415
then the final prediction head will simply be

444
00:23:18,415 --> 00:23:22,645
a linear transform that will basically take the embedding,

445
00:23:22,645 --> 00:23:24,610
multiply it with the weight vector,

446
00:23:24,610 --> 00:23:26,230
uh, pass it through a non, uh,

447
00:23:26,230 --> 00:23:31,060
non-linearity like a sigmoid and we can interpret that as a probability,

448
00:23:31,060 --> 00:23:32,710
uh, of that given class.

449
00:23:32,710 --> 00:23:36,010
So basically we will have a k head output rather than, uh,

450
00:23:36,010 --> 00:23:39,535
um, h, and then have a softmax on top of it,

451
00:23:39,535 --> 00:23:43,405
so that basically we interpret that as a probability that, uh,

452
00:23:43,405 --> 00:23:46,375
that a given node is of the kth,

453
00:23:46,375 --> 00:23:49,210
uh, given type or kth head.

454
00:23:49,210 --> 00:23:52,305
That's how we think of node classification.

455
00:23:52,305 --> 00:23:59,015
For link prediction, things get a bit more tricky because now links have different, ah,

456
00:23:59,015 --> 00:24:03,560
types and we don't want to simply split links, uh,

457
00:24:03,560 --> 00:24:05,750
randomly as we did it so far, uh,

458
00:24:05,750 --> 00:24:11,600
because some types might be very common and other types might be very uncommon, right?

459
00:24:11,600 --> 00:24:13,430
Some relation types might be very common,

460
00:24:13,430 --> 00:24:15,665
some relation types might be very uncommon.

461
00:24:15,665 --> 00:24:19,850
So what we would like to do is for every relation type,

462
00:24:19,850 --> 00:24:22,340
we would like to split it between,

463
00:24:22,340 --> 00:24:25,415
ah, training message edges, uh,

464
00:24:25,415 --> 00:24:29,105
training supervision edges, validation edges, and test edges.

465
00:24:29,105 --> 00:24:32,120
And then we would like to do this for every

466
00:24:32,120 --> 00:24:35,210
of the relation types and then kind of merge, uh,

467
00:24:35,210 --> 00:24:39,665
all of the message edges for the- for the training will, er,

468
00:24:39,665 --> 00:24:44,960
take all the training supervision edges for each separate relation type into

469
00:24:44,960 --> 00:24:47,240
the training supervision edges and then same

470
00:24:47,240 --> 00:24:49,970
for validation edges and for test edges, right?

471
00:24:49,970 --> 00:24:52,850
So kind of the point is that we wanna

472
00:24:52,850 --> 00:24:56,405
independently split edges from

473
00:24:56,405 --> 00:25:01,190
each relation type and then merge it together, uh, these splits.

474
00:25:01,190 --> 00:25:06,185
And the reason we wanna do this is because this means that even for a very infrequent,

475
00:25:06,185 --> 00:25:07,940
ah, very rare, uh,

476
00:25:07,940 --> 00:25:13,160
relation type, some of its- some of the instances of it will be- will be in the training,

477
00:25:13,160 --> 00:25:15,230
some of instances will be in the validation,

478
00:25:15,230 --> 00:25:16,835
and some will be in the test set.

479
00:25:16,835 --> 00:25:21,500
Because the point would be if you just blindly split the edges,

480
00:25:21,500 --> 00:25:26,990
just by chance it can happen that a very rare edge type does not appear in your,

481
00:25:26,990 --> 00:25:29,870
ah, validation set because it's just too rare and by

482
00:25:29,870 --> 00:25:33,080
chance none- none of the edges, uh, landed there.

483
00:25:33,080 --> 00:25:34,340
So that's why we wanna do this,

484
00:25:34,340 --> 00:25:36,215
what is called stratification,

485
00:25:36,215 --> 00:25:39,575
where we- where we independently split, uh,

486
00:25:39,575 --> 00:25:42,574
each relation- edges of each relation type separately,

487
00:25:42,574 --> 00:25:45,350
and then merge it all, ah, together.

488
00:25:45,350 --> 00:25:49,294
Um, so we have these four different edge buckets,

489
00:25:49,294 --> 00:25:51,035
training message as edges,

490
00:25:51,035 --> 00:25:54,485
training supervision edges, validation edges, and test edges.

491
00:25:54,485 --> 00:25:57,740
We do the splitting for each relation type separately

492
00:25:57,740 --> 00:26:01,325
and then merge it all together into the four sets.

493
00:26:01,325 --> 00:26:03,590
And then, you know, everything still, uh,

494
00:26:03,590 --> 00:26:07,205
applies as we talked about, ah, link prediction.

495
00:26:07,205 --> 00:26:12,140
So to tell more or give you more details about how you formalize link prediction,

496
00:26:12,140 --> 00:26:13,475
you look at heterogeneous graphs.

497
00:26:13,475 --> 00:26:16,940
Imagine, you know, I wanna be able to predict, um,

498
00:26:16,940 --> 00:26:21,800
whether there is an edge or what's the probability of an edge between nodes E and A,

499
00:26:21,800 --> 00:26:24,395
um, and that- that edge is of,

500
00:26:24,395 --> 00:26:26,870
ah, type 3- of relation type 3.

501
00:26:26,870 --> 00:26:31,340
Um, so imagine that this edge is a training supervision edge, right?

502
00:26:31,340 --> 00:26:34,370
And let's say that all edge- all other edges are,

503
00:26:34,370 --> 00:26:36,515
uh, training message edges.

504
00:26:36,515 --> 00:26:38,555
So the way I would now, uh,

505
00:26:38,555 --> 00:26:41,045
use the RGCN to score these edges,

506
00:26:41,045 --> 00:26:42,740
I would take the final, uh,

507
00:26:42,740 --> 00:26:47,795
layer embedding of node D. I would take the final layer embedding of node A.

508
00:26:47,795 --> 00:26:52,580
And then I would have a relation specific scoring function f, ah,

509
00:26:52,580 --> 00:26:54,320
that would basically, um,

510
00:26:54,320 --> 00:26:59,825
take these two embeddings and transform them into a real- ah, into a real value.

511
00:26:59,825 --> 00:27:03,590
So one app- approach to do this would be to use this kind of,

512
00:27:03,590 --> 00:27:06,695
ah, uh, linear- uh,

513
00:27:06,695 --> 00:27:09,920
bi-linear form, where basically I take one embedding,

514
00:27:09,920 --> 00:27:13,160
I have the transformation matrix in-between and another embedding,

515
00:27:13,160 --> 00:27:15,920
ah, so that at the end, basically this takes,

516
00:27:15,920 --> 00:27:18,590
take the embedding of node D, transform it,

517
00:27:18,590 --> 00:27:20,930
and then dot-product it with,

518
00:27:20,930 --> 00:27:23,600
uh, embedding of node A.

519
00:27:23,600 --> 00:27:25,280
And I can interpret these,

520
00:27:25,280 --> 00:27:28,370
perhaps send it through some sigmoid or something like that.

521
00:27:28,370 --> 00:27:32,300
I can interpret this simply as the probability that- ah,

522
00:27:32,300 --> 00:27:34,460
there is an edge of relation type,

523
00:27:34,460 --> 00:27:37,085
let's say r_1 between nodes E and A.

524
00:27:37,085 --> 00:27:39,020
So that would be one way, uh,

525
00:27:39,020 --> 00:27:42,020
to- to actually formalize and,

526
00:27:42,020 --> 00:27:46,175
uh, instantiate, ah, this, ah, problem.

527
00:27:46,175 --> 00:27:50,825
Now, um, how exactly am I thinking about these during training?

528
00:27:50,825 --> 00:27:53,345
So let's assume again that this is the edge,

529
00:27:53,345 --> 00:27:55,130
that is a supervision edge.

530
00:27:55,130 --> 00:27:58,940
Um, and let's think that, ah, all other, er,

531
00:27:58,940 --> 00:28:04,880
edges in the graph are- are the training message passing, ah, edges.

532
00:28:04,880 --> 00:28:08,825
So we wanna use the training message-passing edges to predict, ah,

533
00:28:08,825 --> 00:28:12,515
the likelihood or the existence of this,

534
00:28:12,515 --> 00:28:16,560
ah, training, ah, supervision edge of interest.

535
00:28:16,560 --> 00:28:22,420
What we also have to do in link prediction is that we have to create negative instances.

536
00:28:22,420 --> 00:28:25,485
We have to create, ah, negative edges.

537
00:28:25,485 --> 00:28:30,650
So the way we create negative edges is by perturbing the supervision edge.

538
00:28:30,650 --> 00:28:33,050
So for example, if this is the supervision edge,

539
00:28:33,050 --> 00:28:38,540
then one way how we can do- do it is that we corrupt the tail of it, right?

540
00:28:38,540 --> 00:28:40,070
So we maintain the head,

541
00:28:40,070 --> 00:28:41,585
we maintain node E,

542
00:28:41,585 --> 00:28:44,150
but we pick some other node,

543
00:28:44,150 --> 00:28:50,450
ah, that node E is not connected to with the relation of type r_3.

544
00:28:50,450 --> 00:28:52,460
So in our case, we could, for example,

545
00:28:52,460 --> 00:28:56,285
create an edge from E to B or from,

546
00:28:56,285 --> 00:29:00,125
uh, E to D. So these could be our negative edges.

547
00:29:00,125 --> 00:29:02,810
What is important when you create negative edges,

548
00:29:02,810 --> 00:29:04,880
the negative edges should not belong to

549
00:29:04,880 --> 00:29:08,165
training message edges or training supervision edges.

550
00:29:08,165 --> 00:29:13,385
So for example, edge to the node C cannot be a negative edge because

551
00:29:13,385 --> 00:29:18,680
node E is already connected to the node C with relation type 3.

552
00:29:18,680 --> 00:29:20,540
So this is not a negative- uh,

553
00:29:20,540 --> 00:29:24,590
a negative edge because the edge already exists and it's of the same type as here.

554
00:29:24,590 --> 00:29:27,665
So we don't wanna create this, uh, contradiction.

555
00:29:27,665 --> 00:29:32,870
So we have to be a bit careful when sampling these, uh, negative edges.

556
00:29:32,870 --> 00:29:35,120
Now that we have, uh,

557
00:29:35,120 --> 00:29:39,680
created a negative edge by perturbing the tail,

558
00:29:39,680 --> 00:29:44,075
uh, so the end point of the- of the supervision edge.

559
00:29:44,075 --> 00:29:45,575
We can now use,

560
00:29:45,575 --> 00:29:46,790
uh, the GNN model,

561
00:29:46,790 --> 00:29:51,740
the RGCN, to score- to score the positive as well as the negative edges.

562
00:29:51,740 --> 00:29:56,285
And the- you know, the loss function you would wanna optimize is a standard, uh,

563
00:29:56,285 --> 00:29:58,610
cross entropy loss where basically we want to

564
00:29:58,610 --> 00:30:02,105
maximize the scores- the score of the training supervision edge,

565
00:30:02,105 --> 00:30:04,355
we would like to maximize the scores of this guy,

566
00:30:04,355 --> 00:30:08,585
and we would like to minimize the score of negative edges, like for example,

567
00:30:08,585 --> 00:30:13,730
E to B or E to D. So that's how we- we would write down,

568
00:30:13,730 --> 00:30:16,730
ah, the penalty, uh, as I show it here.

569
00:30:16,730 --> 00:30:21,470
Um, and then, er, use the optimizer stochastic gradient descent to

570
00:30:21,470 --> 00:30:25,939
optimize the parameters of RGCN to basically assign high probability,

571
00:30:25,939 --> 00:30:29,210
high score to the training supervision edges and low score,

572
00:30:29,210 --> 00:30:31,235
uh, to negative edges.

573
00:30:31,235 --> 00:30:34,130
Now that we have the model trained,

574
00:30:34,130 --> 00:30:37,265
now assume we move to the validation time.

575
00:30:37,265 --> 00:30:39,275
We wanna validate, uh,

576
00:30:39,275 --> 00:30:43,190
the our- the- the performance of our models.

577
00:30:43,190 --> 00:30:44,720
So let's assume now that

578
00:30:44,720 --> 00:30:49,235
the validation edge that I'm interested in is this edge between node E,

579
00:30:49,235 --> 00:30:54,140
uh, and node D. And I'm interested whether it's of type r_3.

580
00:30:54,140 --> 00:30:55,415
Then in this case,

581
00:30:55,415 --> 00:30:59,870
training message edges and training supervision- supervision edges basically means,

582
00:30:59,870 --> 00:31:01,310
in- in my example,

583
00:31:01,310 --> 00:31:05,195
all existing edges of the graph are used, ah,

584
00:31:05,195 --> 00:31:08,930
for the message propagation and then at the validation time,

585
00:31:08,930 --> 00:31:13,880
I'm basically using all these solid edges to do the message propagation.

586
00:31:13,880 --> 00:31:18,305
And I'm trying to score the value of this particular, uh,

587
00:31:18,305 --> 00:31:23,300
edge from node E to node D, right?

588
00:31:23,300 --> 00:31:25,790
And again, the intuition here is that the score of

589
00:31:25,790 --> 00:31:28,775
this edge should be higher than the score of,

590
00:31:28,775 --> 00:31:31,190
let's say all other, ah,

591
00:31:31,190 --> 00:31:35,060
edges that are- that are in some sense

592
00:31:35,060 --> 00:31:39,230
negative or don't exist in the data set yet from node D. So for example,

593
00:31:39,230 --> 00:31:43,355
this would mean that the score of this node edge from E to D

594
00:31:43,355 --> 00:31:47,840
has to be higher than the one from E to B in our case.

595
00:31:47,840 --> 00:31:51,575
Um, and- and I cannot consider

596
00:31:51,575 --> 00:31:56,165
these other edges because they are already used as a message passing engages in my,

597
00:31:56,165 --> 00:31:58,520
uh, validation, uh, RGCN.

598
00:31:58,520 --> 00:32:00,950
So notice that it is important that

599
00:32:00,950 --> 00:32:03,875
these sets of edges that are independent from each other,

600
00:32:03,875 --> 00:32:06,620
uh, when we score them.

601
00:32:06,620 --> 00:32:09,095
So how would I evaluate this?

602
00:32:09,095 --> 00:32:14,330
Right, I would get- use the RGCN to calculate the score of edge ED,

603
00:32:14,330 --> 00:32:16,550
uh, according to the relation type 3.

604
00:32:16,550 --> 00:32:19,655
I would then calculate the score of all the negative edges.

605
00:32:19,655 --> 00:32:20,855
So in my case,

606
00:32:20,855 --> 00:32:27,755
only two possible negative edges of type r_3 are E to B and E to F, right?

607
00:32:27,755 --> 00:32:32,945
I cannot do A and C because they are already connected according to the relation 3.

608
00:32:32,945 --> 00:32:34,910
So that would be kind of a contradiction.

609
00:32:34,910 --> 00:32:37,280
So I have only two negative edges.

610
00:32:37,280 --> 00:32:44,690
Ah, and the- the- the goal then is basically to obtain a score for all these three edges.

611
00:32:44,690 --> 00:32:48,545
We rank them and hopefully the- the rank,

612
00:32:48,545 --> 00:32:51,155
the output score of the edge, ah,

613
00:32:51,155 --> 00:32:56,595
ED will be higher than the output score of EF and BF.

614
00:32:56,595 --> 00:33:01,450
And then how do I usually evaluate the calculation matrix?

615
00:33:01,450 --> 00:33:03,745
How do I usually, um, evaluate?

616
00:33:03,745 --> 00:33:07,370
Um, I can do either what is called hits, which would be,

617
00:33:07,370 --> 00:33:10,880
how often was the correct positive edge ranked among

618
00:33:10,880 --> 00:33:14,585
the top K of my predicted edges or I can do

619
00:33:14,585 --> 00:33:18,875
a reciprocal rank which is 1 over the rank of the-

620
00:33:18,875 --> 00:33:23,390
of the- of the positive edge ranked among all other, uh, negative edges.

621
00:33:23,390 --> 00:33:25,835
And then you can do mean reciprocal rank,

622
00:33:25,835 --> 00:33:28,070
and the higher the mean reciprocal rank,

623
00:33:28,070 --> 00:33:33,710
the better or high- the higher the hits score, the better.

624
00:33:33,710 --> 00:33:36,320
So let me summarize.

625
00:33:36,320 --> 00:33:38,510
We talked about relational GCN,

626
00:33:38,510 --> 00:33:42,020
which is a graph neural network for heterogeneous graphs.

627
00:33:42,020 --> 00:33:43,970
We talked about how to define it.

628
00:33:43,970 --> 00:33:49,280
We talked about how to have a relation type specific transformation functions.

629
00:33:49,280 --> 00:33:51,260
Uh, we discussed about, uh,

630
00:33:51,260 --> 00:33:55,640
how to do entity classification as well as link prediction tasks.

631
00:33:55,640 --> 00:34:01,070
We also discussed how- discussed how to make a relational GCN more scalable through, uh,

632
00:34:01,070 --> 00:34:05,495
making matrices W, block diagonal or using

633
00:34:05,495 --> 00:34:10,130
it as a- as a linear combination of basis transformations.

634
00:34:10,130 --> 00:34:12,364
So this kind of dictionary learning approach.

635
00:34:12,364 --> 00:34:18,079
And of course, you can take out GCN and extend it in any way you like, right?

636
00:34:18,080 --> 00:34:20,300
You can- you could think that you have- you know,

637
00:34:20,300 --> 00:34:21,679
you could have RGNN,

638
00:34:21,679 --> 00:34:23,539
you could have RGraphSAGE,

639
00:34:23,540 --> 00:34:26,420
you could have R graph attention networks.

640
00:34:26,420 --> 00:34:28,610
So all these, ah,

641
00:34:28,610 --> 00:34:33,695
basic fundamental GNN architectures that we already discussed.

642
00:34:33,695 --> 00:34:38,449
It's kind of natural to extend them to the- to this, er,

643
00:34:38,449 --> 00:34:41,149
multi-relational case, ah, by- by

644
00:34:41,150 --> 00:34:44,315
basically adding relation specific, uh, transformations.

645
00:34:44,315 --> 00:34:46,114
So it's kind of natural

646
00:34:46,114 --> 00:34:49,699
that- how you can play with this model and make it more expressive,

647
00:34:49,699 --> 00:34:51,929
and richer and so on.

