1
00:00:04,070 --> 00:00:10,260
Now we are going to talk about the second method in this idea of neighborhood, uh,

2
00:00:10,260 --> 00:00:14,100
sampling and trying to limit, uh, the- the,

3
00:00:14,100 --> 00:00:15,960
uh, batch size, uh,

4
00:00:15,960 --> 00:00:18,600
issues, uh, in graph neural networks.

5
00:00:18,600 --> 00:00:21,060
Um, and the way we are going to do this is,

6
00:00:21,060 --> 00:00:23,115
uh, to start with the following observation, right?

7
00:00:23,115 --> 00:00:25,935
That the size of computational graphs, um,

8
00:00:25,935 --> 00:00:27,685
can still become very big, uh,

9
00:00:27,685 --> 00:00:31,355
or increases exponentially with the number of, uh, GNN layers.

10
00:00:31,355 --> 00:00:35,020
And what we can also observe and notice is that, um,

11
00:00:35,020 --> 00:00:38,300
computation in these, uh- computational graphs

12
00:00:38,300 --> 00:00:41,900
can be very much redundant because of shared neighbors.

13
00:00:41,900 --> 00:00:43,895
Like if you think for example, two nodes,

14
00:00:43,895 --> 00:00:47,210
A and B, and you think about the computation graphs,

15
00:00:47,210 --> 00:00:51,645
then they have, uh- because they share- they have a lot of friends in common,

16
00:00:51,645 --> 00:00:54,590
these computation graphs are heavily redundant, right?

17
00:00:54,590 --> 00:00:55,985
Like the- the, uh,

18
00:00:55,985 --> 00:00:58,745
both to compute A and compute B,

19
00:00:58,745 --> 00:01:01,760
the subgraphs the comp- the parts of the computation graphs

20
00:01:01,760 --> 00:01:04,819
between- below node C and D are identical.

21
00:01:04,819 --> 00:01:09,430
So this would be the duplicate- this will be duplicated, uh, um, computation.

22
00:01:09,430 --> 00:01:12,870
And, uh, there are two approaches to do this.

23
00:01:12,870 --> 00:01:15,610
One is to, uh, realize that this, uh,

24
00:01:15,610 --> 00:01:19,360
computation is duplicated then computed only once.

25
00:01:19,360 --> 00:01:23,285
Uh, there is a very nice paper about this at last year KDD called,

26
00:01:23,285 --> 00:01:25,790
uh, uh, HAGs, hierarchical, um,

27
00:01:25,790 --> 00:01:29,499
aggregation, uh, graphs that basically prevent,

28
00:01:29,499 --> 00:01:33,455
um, uh, multiple computations, so redundant computations.

29
00:01:33,455 --> 00:01:37,895
Um, and the- the second case will be about Cluster-GCN,

30
00:01:37,895 --> 00:01:40,555
which is what I'm going to talk about today.

31
00:01:40,555 --> 00:01:46,770
And the way we are, uh, motivating Cluster-GCN is to remember what is a full ba- batch,

32
00:01:46,770 --> 00:01:48,210
uh, graph neural network, right?

33
00:01:48,210 --> 00:01:51,980
A full batch graph neural network is that all the embeddings

34
00:01:51,980 --> 00:01:55,850
for all the nodes are embedded together in a- in a single pass.

35
00:01:55,850 --> 00:02:00,065
So basically, um, embeddings of all the nodes are computed for layer 1,

36
00:02:00,065 --> 00:02:03,200
embeddings for all the nodes are then computed for layer 2,

37
00:02:03,200 --> 00:02:04,400
uh, and so on, right?

38
00:02:04,400 --> 00:02:07,040
The point is you need to have the embeddings of

39
00:02:07,040 --> 00:02:10,610
all the nodes at lab- at layer L minus 1 to be

40
00:02:10,610 --> 00:02:13,940
able to compute the embeddings for all the other nodes- for

41
00:02:13,940 --> 00:02:17,579
all the nodes at layer L. So it means that,

42
00:02:17,579 --> 00:02:18,750
uh, in each layer,

43
00:02:18,750 --> 00:02:20,595
2 times number of edges,

44
00:02:20,595 --> 00:02:22,180
uh, messages need to be, uh,

45
00:02:22,180 --> 00:02:24,685
exchanged because every node needs to connect-

46
00:02:24,685 --> 00:02:27,825
collect the- the message is from its neighbors,

47
00:02:27,825 --> 00:02:32,030
so there are two messages along each undirected edge, you know,

48
00:02:32,030 --> 00:02:34,375
for both, uh, endpoints,

49
00:02:34,375 --> 00:02:36,975
and, uh, what is the observation?

50
00:02:36,975 --> 00:02:38,305
The observation is that, uh,

51
00:02:38,305 --> 00:02:42,590
the amount of computation we need to do in a single layer of, uh, let's say,

52
00:02:42,590 --> 00:02:43,820
uh, full batch, uh,

53
00:02:43,820 --> 00:02:46,910
implementation is linear in the size of the graph.

54
00:02:46,910 --> 00:02:48,635
It's linear in the number of edges.

55
00:02:48,635 --> 00:02:50,640
So it means it is very fast.

56
00:02:50,640 --> 00:02:53,330
But the problem is that the graphs are too big.

57
00:02:53,330 --> 00:02:56,880
Uh, so we cannot do this kind of at once,

58
00:02:56,880 --> 00:02:58,410
uh, in the GPU.

59
00:02:58,410 --> 00:03:01,940
So the insight from full-batch GNN is that

60
00:03:01,940 --> 00:03:07,855
layer-wise node embedding updates allow us to reuse embeddings from the previous layer.

61
00:03:07,855 --> 00:03:12,440
Um, and this significantly reduces the computational redundancy of,

62
00:03:12,440 --> 00:03:13,880
uh, neighborhood sampling, right?

63
00:03:13,880 --> 00:03:17,150
Uh, this means that this layer-wise update to generate

64
00:03:17,150 --> 00:03:21,590
embeddings from one layer to the next is very cheap, right?

65
00:03:21,590 --> 00:03:23,760
All I need to do is previous layer embeddings.

66
00:03:23,760 --> 00:03:25,160
I need to aggregate them,

67
00:03:25,160 --> 00:03:27,920
combine them with the previous layer embedding of a given node

68
00:03:27,920 --> 00:03:30,650
and I have a new, uh- a new embedding.

69
00:03:30,650 --> 00:03:31,880
So what this means is that,

70
00:03:31,880 --> 00:03:33,680
uh, this can be done very fast.

71
00:03:33,680 --> 00:03:38,720
It's only linear time operation in the size of the graph because it's aggregation,

72
00:03:38,720 --> 00:03:41,330
uh, uh, basically takes time linear because

73
00:03:41,330 --> 00:03:44,275
it's linear number of edges over which we aggregate.

74
00:03:44,275 --> 00:03:48,700
So this sig- means that the computations are very fast.

75
00:03:48,700 --> 00:03:52,100
But of course the problem is that layer-wise update is not feasible

76
00:03:52,100 --> 00:03:55,970
for the entire graph at once because of the GPU memory.

77
00:03:55,970 --> 00:04:02,990
So now, what the idea is in the Cluster-GCN is that can we sample

78
00:04:02,990 --> 00:04:06,920
the entire graph into small sub-parts and then

79
00:04:06,920 --> 00:04:11,900
perform full batch implementation on those subgraphs.

80
00:04:11,900 --> 00:04:14,465
All right. So the idea is the following.

81
00:04:14,465 --> 00:04:15,860
I'll take the large graph.

82
00:04:15,860 --> 00:04:18,000
I'm going to sample a subgraph of it.

83
00:04:18,000 --> 00:04:20,634
I don't know maybe I'll just say I'll sample the subgraph,

84
00:04:20,634 --> 00:04:24,740
and then I'm going, uh, in this subgraph I'm able to fit it on a GPU.

85
00:04:24,740 --> 00:04:28,505
So now I can apply a full batch implementation,

86
00:04:28,505 --> 00:04:31,825
uh, in the GPU on that sampled subgraph.

87
00:04:31,825 --> 00:04:34,295
All right? So that's essentially the idea.

88
00:04:34,295 --> 00:04:37,040
So the difference between neighborhood sampling is that

89
00:04:37,040 --> 00:04:41,080
there we compute the computation graph for each individual node.

90
00:04:41,080 --> 00:04:46,790
Um, here, we are going to sample an entire subgraph of the original graph and

91
00:04:46,790 --> 00:04:49,400
then pretend that this is the graph of interest and

92
00:04:49,400 --> 00:04:52,990
just perform GNN on that, uh, subgraph.

93
00:04:52,990 --> 00:04:55,710
Right? So then the question is,

94
00:04:55,710 --> 00:04:57,600
how should I sample these subgraphs?

95
00:04:57,600 --> 00:05:00,375
What are good subgraphs for training GNNs?

96
00:05:00,375 --> 00:05:02,250
Um, and here's the reasoning.

97
00:05:02,250 --> 00:05:05,990
The important point to remember is that GNN performs

98
00:05:05,990 --> 00:05:10,070
embedding by passing messages via the edges of the network, right?

99
00:05:10,070 --> 00:05:12,740
Every node computes its embedding by collecting

100
00:05:12,740 --> 00:05:15,530
information from the neighbors, uh, right?

101
00:05:15,530 --> 00:05:18,130
So the idea is now if I wanna have a good subgraph,

102
00:05:18,130 --> 00:05:23,600
the good subgraph have- have- has to retain as many of the edges of the original graph as

103
00:05:23,600 --> 00:05:26,720
possible so that my computations mimic

104
00:05:26,720 --> 00:05:30,260
the computations on the big graph as well as possible.

105
00:05:30,260 --> 00:05:32,600
So our subgraphs should retain

106
00:05:32,600 --> 00:05:36,535
edge connectivity structure of the ne- original graph as much as possible.

107
00:05:36,535 --> 00:05:41,520
Which means that the GNN over the subgraph generates embeddings that are close,

108
00:05:41,520 --> 00:05:45,120
uh, to the- to the GNN over the original graph.

109
00:05:45,120 --> 00:05:47,810
So to give you- to give you an idea,

110
00:05:47,810 --> 00:05:51,500
if I have an original graph and let's say I sample, uh,

111
00:05:51,500 --> 00:05:55,650
I- I sample a subgraph on four nodes like I show here on the left

112
00:05:55,650 --> 00:05:59,945
or I sample another subgraph on four nodes like I show on the right,

113
00:05:59,945 --> 00:06:04,220
then it's clear that the left subgraph is kind of much better for training.

114
00:06:04,220 --> 00:06:05,645
Because if I now say,

115
00:06:05,645 --> 00:06:08,930
let's compute the embedding of this corner node up here,

116
00:06:08,930 --> 00:06:12,740
here, all the three neighbors of it are in the- in the subgraph.

117
00:06:12,740 --> 00:06:17,240
So I'll be able to get a quite good estimate of the embedding of this,

118
00:06:17,240 --> 00:06:20,575
uh, node, um, corner node up here.

119
00:06:20,575 --> 00:06:22,750
So right, it's- all of its, um, uh,

120
00:06:22,750 --> 00:06:26,065
uh, neighbors are- are part of my subgraph.

121
00:06:26,065 --> 00:06:28,555
While if I take this other subgraph here,

122
00:06:28,555 --> 00:06:30,280
we- denoted by green nodes.

123
00:06:30,280 --> 00:06:34,420
And I wanna predict or generate an embedding for this green node as well,

124
00:06:34,420 --> 00:06:38,935
then- then I- all I'm able to do is aggregate message from one of its neighbors,

125
00:06:38,935 --> 00:06:43,320
because only one of its neighbors is part of my, uh, subgraph, right?

126
00:06:43,320 --> 00:06:47,100
So the right subgraph drops many- many edges,

127
00:06:47,100 --> 00:06:49,480
um, and leading to isolated nodes,

128
00:06:49,480 --> 00:06:53,320
which means my computation graphs over this subgraph won't

129
00:06:53,320 --> 00:06:57,340
be representative of the computation graphs in the original graph.

130
00:06:57,340 --> 00:06:59,980
So left choice is far better,

131
00:06:59,980 --> 00:07:02,075
uh, than the right choice.

132
00:07:02,075 --> 00:07:04,750
And this now brings us back to

133
00:07:04,750 --> 00:07:10,015
the network community structure because real world graphs exhibit community structure.

134
00:07:10,015 --> 00:07:11,675
They exhibit clustering structure.

135
00:07:11,675 --> 00:07:15,820
So large graphs can be decomposed into many small, uh, communities.

136
00:07:15,820 --> 00:07:20,170
And the key insight would be that we can sample these subgraphs based on

137
00:07:20,170 --> 00:07:23,110
the community structure so that each subgraph

138
00:07:23,110 --> 00:07:27,470
retains most of the edges and only few edges, uh, are dropped.

139
00:07:27,470 --> 00:07:30,720
So a Cluster-GCN, the way it works,

140
00:07:30,720 --> 00:07:33,340
it has, uh, uh, a two-step process.

141
00:07:33,340 --> 00:07:34,930
In the preprocessing step,

142
00:07:34,930 --> 00:07:37,555
we take the original graph and split it,

143
00:07:37,555 --> 00:07:39,205
cut it into many small,

144
00:07:39,205 --> 00:07:41,425
uh- small, uh, subgraphs.

145
00:07:41,425 --> 00:07:45,430
Uh, and then, uh, mini-batch training means that we sample one node,

146
00:07:45,430 --> 00:07:47,565
group 1 subgraph, uh, and, uh,

147
00:07:47,565 --> 00:07:51,890
perform the- the full batch message passing over that entire subgraph,

148
00:07:51,890 --> 00:07:55,805
compute the node embe- uh- node embeddings of all the nodes in the subgraph,

149
00:07:55,805 --> 00:07:58,910
evaluate the loss, compute the gradient,

150
00:07:58,910 --> 00:08:00,500
and update the parameters, right?

151
00:08:00,500 --> 00:08:02,120
So the idea is input graph,

152
00:08:02,120 --> 00:08:04,445
split into many subgraphs.

153
00:08:04,445 --> 00:08:06,335
We take one subgraph,

154
00:08:06,335 --> 00:08:08,345
fit it into the GPU memory,

155
00:08:08,345 --> 00:08:11,840
do the full computation on the subgraph, compute the loss,

156
00:08:11,840 --> 00:08:15,705
update model parameters, load in another subgraph,

157
00:08:15,705 --> 00:08:18,510
do the, uh, a full- full batch,

158
00:08:18,510 --> 00:08:21,355
uh, layer embeddings of all the nodes in the subgraph.

159
00:08:21,355 --> 00:08:23,750
Compute the loss, update the gradient,

160
00:08:23,750 --> 00:08:25,700
load, uh, the next subgraphs.

161
00:08:25,700 --> 00:08:30,720
So that's how vanilla Cluster-GCN, uh, would work.

162
00:08:30,780 --> 00:08:34,058
So to give you more- more details,

163
00:08:34,058 --> 00:08:36,489
the idea is that given a large, uh, graph,

164
00:08:36,490 --> 00:08:38,140
we're going to partition it into,

165
00:08:38,140 --> 00:08:40,105
let's say, capital C groups.

166
00:08:40,105 --> 00:08:44,800
Um, we can use some scalable community detection method like Louvain, uh,

167
00:08:44,800 --> 00:08:47,110
like METIS method, um,

168
00:08:47,110 --> 00:08:48,655
or we can use, uh,

169
00:08:48,655 --> 00:08:50,560
even BIGCLAM is fine,

170
00:08:50,560 --> 00:08:52,375
um, and then basically this,

171
00:08:52,375 --> 00:08:54,370
um, set of, uh,

172
00:08:54,370 --> 00:08:56,500
node groups V. Uh,

173
00:08:56,500 --> 00:09:00,715
we are going to take an induced subgraph on the subset of nodes,

174
00:09:00,715 --> 00:09:02,920
um, and basically these induced subgraphs will

175
00:09:02,920 --> 00:09:05,310
include all the edges between the members, ah,

176
00:09:05,310 --> 00:09:07,720
of the- of the group, and of course,

177
00:09:07,720 --> 00:09:10,614
these edges that go across the subgraphs,

178
00:09:10,614 --> 00:09:11,920
those will be dropped.

179
00:09:11,920 --> 00:09:16,150
[NOISE] So between group edges are going to be dropped in these,

180
00:09:16,150 --> 00:09:21,100
uh, uh, graphs, G1 to GC which are these subgraphs we sampled.

181
00:09:21,100 --> 00:09:24,040
So now, for each, uh, mini-batch,

182
00:09:24,040 --> 00:09:28,210
we are going to sample one such node group, one such subgraph,

183
00:09:28,210 --> 00:09:32,275
create an induced subgraph over that set of nodes,

184
00:09:32,275 --> 00:09:35,830
and this is now our graph that we are going to use,

185
00:09:35,830 --> 00:09:38,575
um, as a computation in the GPU.

186
00:09:38,575 --> 00:09:42,940
So the point is that this induced subgraph has to fit into the GPU memory.

187
00:09:42,940 --> 00:09:45,760
So now, we do layer-wise node update

188
00:09:45,760 --> 00:09:48,745
basically the same thing as what we do in the full batch, uh,

189
00:09:48,745 --> 00:09:54,565
GNN to compute now the embeddings of all the nodes in the subgraph, recompute, uh,

190
00:09:54,565 --> 00:09:57,025
the loss, uh, and then do the, uh,

191
00:09:57,025 --> 00:10:01,420
compute the gradient with respect to the loss and then do the gradient update.

192
00:10:01,420 --> 00:10:07,300
So the point here is that when we were doing GraphSAGE neighborhood sampling, our nodes,

193
00:10:07,300 --> 00:10:10,870
we were able to kind of sample them all- all- all across the graphs.

194
00:10:10,870 --> 00:10:12,760
But in this case, um,

195
00:10:12,760 --> 00:10:14,500
we are going to sample a subgraph of

196
00:10:14,500 --> 00:10:17,470
the original graph and now only compute on those nodes,

197
00:10:17,470 --> 00:10:20,860
uh, we have sampled, and that's the big difference between,

198
00:10:20,860 --> 00:10:23,125
ah, the neighborhood sampling that can be kind of,

199
00:10:23,125 --> 00:10:27,175
those neighborhoods can be distributed across the entire network while here,

200
00:10:27,175 --> 00:10:31,360
we are going to create those neighborhoods basically just being limited to

201
00:10:31,360 --> 00:10:35,785
the subgraph we sample and we feed that subgraph into the GPU memory.

202
00:10:35,785 --> 00:10:39,190
So what are some issues and how do we fix them?

203
00:10:39,190 --> 00:10:45,205
The issue with Cluster-GCN is that these induced subgraphs are removed in- in,

204
00:10:45,205 --> 00:10:48,370
uh, between the group links, between subgraph links.

205
00:10:48,370 --> 00:10:51,115
And as a result, messages from other groups

206
00:10:51,115 --> 00:10:54,535
will be lost during message-passing which will hur- which can hurt,

207
00:10:54,535 --> 00:10:56,770
uh, GNN performance, right?

208
00:10:56,770 --> 00:11:03,250
So for example, if I sample this red graph here and I just zoom in into the node- uh,

209
00:11:03,250 --> 00:11:05,050
into it, what I see is that,

210
00:11:05,050 --> 00:11:06,385
for example, whenever, ah,

211
00:11:06,385 --> 00:11:10,375
this particular node aggregates information,

212
00:11:10,375 --> 00:11:14,185
it will never aggregate information from these two edges because these two- uh,

213
00:11:14,185 --> 00:11:17,710
the node endpoints are part of the se- the different subgraphs.

214
00:11:17,710 --> 00:11:21,160
So all the message aggregation will happen only between these four nodes,

215
00:11:21,160 --> 00:11:22,300
uh, and the five,

216
00:11:22,300 --> 00:11:23,905
uh, edges between them,

217
00:11:23,905 --> 00:11:28,000
so there will be in some sense, uh, lost messages.

218
00:11:28,000 --> 00:11:31,225
So, uh, the issue will be because the-

219
00:11:31,225 --> 00:11:35,785
the graph community detection put similar nodes together into the same group,

220
00:11:35,785 --> 00:11:38,005
uh, sample node groups tend to cover

221
00:11:38,005 --> 00:11:41,890
only a small concentrated portion of the entire graph, right?

222
00:11:41,890 --> 00:11:46,420
So we are going to learn only on a small concentrated portions of the graph while

223
00:11:46,420 --> 00:11:51,475
neighborhood sampling allows us to kind of learn on small but very diverse,

224
00:11:51,475 --> 00:11:53,590
uh, set of, uh, neighborhoods.

225
00:11:53,590 --> 00:11:56,470
Um, and this means that because this sample node,

226
00:11:56,470 --> 00:12:01,285
sample subgraph vor- will be- will be our concentrated part of the entire graph,

227
00:12:01,285 --> 00:12:04,270
it won't be kind of diverse or a representative to

228
00:12:04,270 --> 00:12:07,330
represent the entire, uh, training graphs.

229
00:12:07,330 --> 00:12:11,110
So what this means is that when we compute the gradient and the loss, uh,

230
00:12:11,110 --> 00:12:14,080
this is going to fluctuate a lot from one node group to

231
00:12:14,080 --> 00:12:17,350
another because- because of the social communities,

232
00:12:17,350 --> 00:12:19,015
uh, if you think of it that way.

233
00:12:19,015 --> 00:12:22,120
Each subgroup will be very concentrated in one given

234
00:12:22,120 --> 00:12:25,660
aspect and the- the model will have hard time to learn, right?

235
00:12:25,660 --> 00:12:26,965
You will- I know- you know,

236
00:12:26,965 --> 00:12:29,500
you have a cluster of computer scientists,

237
00:12:29,500 --> 00:12:31,720
so you will learn how to make- you will compute

238
00:12:31,720 --> 00:12:34,150
the gradient with respect to computer scientists,

239
00:12:34,150 --> 00:12:36,730
then you have a cluster of, let's say, uh,

240
00:12:36,730 --> 00:12:39,430
music students and now your gradient that

241
00:12:39,430 --> 00:12:42,130
you have just- your models that were just computed over

242
00:12:42,130 --> 00:12:45,160
the computer scientists in your social network are now going to

243
00:12:45,160 --> 00:12:48,370
be computed over music people which are very, very different.

244
00:12:48,370 --> 00:12:50,740
And then I don't know, you have mathematicians who are

245
00:12:50,740 --> 00:12:54,100
different- community of mathematicians who are different from the two.

246
00:12:54,100 --> 00:12:57,085
So the gradients are going to fluctuate quite a lot,

247
00:12:57,085 --> 00:13:03,250
and training is quite unstable which in practice leads to slow convergence of,

248
00:13:03,250 --> 00:13:06,070
uh, gradient descent or stochastic gradient descent.

249
00:13:06,070 --> 00:13:12,235
So, uh, a way to improve this is to- is what is called Advanced Cluster-GCN.

250
00:13:12,235 --> 00:13:13,810
And the idea is that here,

251
00:13:13,810 --> 00:13:17,290
we wanna aggregate multiple node groups per mini-batch, right?

252
00:13:17,290 --> 00:13:20,830
So the idea is to- to make the graphs we sampled even

253
00:13:20,830 --> 00:13:25,510
smaller but then we are going to sample multiple of these,

254
00:13:25,510 --> 00:13:28,510
uh, subgraphs, uh, into the mini-batch.

255
00:13:28,510 --> 00:13:30,610
Um, and we are going to create, uh,

256
00:13:30,610 --> 00:13:37,000
induced subgraph on the aggregated node group that is now composed of many small, uh,

257
00:13:37,000 --> 00:13:41,410
node groups, and the rest will be the same is- I- as in the Cluster-GCN,

258
00:13:41,410 --> 00:13:43,740
which is take now the induced subgraph,

259
00:13:43,740 --> 00:13:45,330
put it into the GPU memory,

260
00:13:45,330 --> 00:13:46,755
and create the update.

261
00:13:46,755 --> 00:13:48,900
But the point is now that the subgroups can be more

262
00:13:48,900 --> 00:13:52,245
diverse so the induced subgraph will be more diverse,

263
00:13:52,245 --> 00:13:56,255
so your gradients will be more stable and more, uh, representative.

264
00:13:56,255 --> 00:13:57,820
So here is the picture,

265
00:13:57,820 --> 00:14:01,510
the picture is that now my node groups are smaller than what I had before,

266
00:14:01,510 --> 00:14:03,265
but I can sample more of them.

267
00:14:03,265 --> 00:14:06,220
So I sample here two node groups and now

268
00:14:06,220 --> 00:14:12,685
the sample subgraph is the induced subgraph on the nodes belonging to the two groups.

269
00:14:12,685 --> 00:14:15,625
So I will also include, uh, this,

270
00:14:15,625 --> 00:14:19,300
uh, uh, blue, uh, edge that kind of connects the two groups.

271
00:14:19,300 --> 00:14:20,770
So it means that, um, uh,

272
00:14:20,770 --> 00:14:24,820
this makes the sample nodes more representative, more diverse.

273
00:14:24,820 --> 00:14:28,195
They are more representative of the entire node population,

274
00:14:28,195 --> 00:14:30,955
which will lead to better gradient estimate,

275
00:14:30,955 --> 00:14:32,844
less variance in gradients,

276
00:14:32,844 --> 00:14:36,265
and faster training, uh, and convergence.

277
00:14:36,265 --> 00:14:39,580
So, uh, just to say more, right?

278
00:14:39,580 --> 00:14:41,545
This is again a two-step approach.

279
00:14:41,545 --> 00:14:43,660
I take the nodes, um,

280
00:14:43,660 --> 00:14:47,095
I- I separate them into very small, uh,

281
00:14:47,095 --> 00:14:50,440
subgroups, much smaller than the original version, but, uh,

282
00:14:50,440 --> 00:14:52,285
when I'm doing mini-batch training,

283
00:14:52,285 --> 00:14:55,930
I will sample multiple groups of nodes, um,

284
00:14:55,930 --> 00:14:59,635
and then I will aggregate these groups of nodes into one super group,

285
00:14:59,635 --> 00:15:04,210
and then I'm going to create an induced subgraph on the entire, uh, super group.

286
00:15:04,210 --> 00:15:09,280
And then, um, I'm going to include the edges between all the members of the super group,

287
00:15:09,280 --> 00:15:12,085
which means it will be edges between the- the small, uh,

288
00:15:12,085 --> 00:15:16,825
subgroups as well as the edges between the small, uh, subgroups.

289
00:15:16,825 --> 00:15:19,540
Um, and then I will perfo- perform, uh,

290
00:15:19,540 --> 00:15:23,875
Cluster-GCN, uh, the same way as we did before.

291
00:15:23,875 --> 00:15:29,124
So now, um, if I compare the Cluster-GCN approach,

292
00:15:29,124 --> 00:15:31,285
with the neighborhood sampling approach,

293
00:15:31,285 --> 00:15:32,920
here is how the two compare.

294
00:15:32,920 --> 00:15:35,125
The neighborhood sampling says,

295
00:15:35,125 --> 00:15:36,655
uh, sample H, uh,

296
00:15:36,655 --> 00:15:38,230
nodes per layer, um,

297
00:15:38,230 --> 00:15:40,000
and let's do this for,

298
00:15:40,000 --> 00:15:41,620
uh, M nodes in the network.

299
00:15:41,620 --> 00:15:44,875
So the total size of the mini-batch will be

300
00:15:44,875 --> 00:15:49,690
M times H raised to the power of K. M is the number of computation graph,

301
00:15:49,690 --> 00:15:53,140
uh, H is the fan out of computation graphs,

302
00:15:53,140 --> 00:15:56,455
and K is the number of layers of the GNN.

303
00:15:56,455 --> 00:15:57,940
So for M, uh,

304
00:15:57,940 --> 00:15:59,965
nodes in the computation graph,

305
00:15:59,965 --> 00:16:05,665
our cost in terms of memory as well as computation will be M times, uh, H_K.

306
00:16:05,665 --> 00:16:10,000
In the Cluster-GCN, um, uh, the- uh,

307
00:16:10,000 --> 00:16:15,025
the- we are performing message passing over a subgraph induced by M nodes,

308
00:16:15,025 --> 00:16:18,490
and the subgraph over M nodes is going to

309
00:16:18,490 --> 00:16:22,615
include M times average degree number of edges, right?

310
00:16:22,615 --> 00:16:27,280
Each node in the subgraph has an average degree of, uh, D average,

311
00:16:27,280 --> 00:16:30,250
so in total, we- we'll have M times,

312
00:16:30,250 --> 00:16:31,870
uh, average degree, uh,

313
00:16:31,870 --> 00:16:33,595
number of edges in the graph.

314
00:16:33,595 --> 00:16:37,615
And because we are doing K hop, uh, message-passing,

315
00:16:37,615 --> 00:16:40,510
the computational cost of this approach will be

316
00:16:40,510 --> 00:16:45,895
K times M size of the subgraph times the average degree.

317
00:16:45,895 --> 00:16:49,090
So if you compare the two, in summary,

318
00:16:49,090 --> 00:16:55,735
the cost to generate embeddings from M nodes in a K layer GNN is M to the- M times H_K.

319
00:16:55,735 --> 00:16:58,675
In Cluster-GCN, it is, um,

320
00:16:58,675 --> 00:17:01,450
K times M times average degree,

321
00:17:01,450 --> 00:17:04,375
um, and, um, if you assume, let's say, uh,

322
00:17:04,375 --> 00:17:07,170
H to be half of the average degree,

323
00:17:07,170 --> 00:17:09,380
uh, then it would mean that, ah,

324
00:17:09,380 --> 00:17:12,464
Cluster-GCN is much more computationally efficient,

325
00:17:12,464 --> 00:17:14,409
um, than neighborhood sampling.

326
00:17:14,410 --> 00:17:17,000
So, um, it is linear ins- instead of

327
00:17:17,000 --> 00:17:20,464
exponential with respect to the- to the- to the depth.

328
00:17:20,464 --> 00:17:22,349
So what do we do in practice?

329
00:17:22,349 --> 00:17:24,679
It depends a bit on the- on the dataset.

330
00:17:24,680 --> 00:17:29,150
Usually, we set H to be bigger than the half average degree,

331
00:17:29,150 --> 00:17:30,980
uh, perhaps, you know, two times,

332
00:17:30,980 --> 00:17:34,205
three times, ah, average degree, um, and, uh,

333
00:17:34,205 --> 00:17:37,610
and because number of layers K is not- uh,

334
00:17:37,610 --> 00:17:39,430
is not that deep, um,

335
00:17:39,430 --> 00:17:43,460
the- the neighborhood sampling approach tends to be kind of more,

336
00:17:43,460 --> 00:17:45,945
uh, used, uh, in practice.

337
00:17:45,945 --> 00:17:51,159
So to summarize, Cluster-GCN first partitions the entire,

338
00:17:51,159 --> 00:17:54,260
um, set of nodes in the graph into small node groups.

339
00:17:54,260 --> 00:17:58,700
In each mini-batch, multiple node groups are sampled, um, and their,

340
00:17:58,700 --> 00:18:01,865
uh, and their nodes are kind of,

341
00:18:01,865 --> 00:18:03,980
um, uh, aggregated together.

342
00:18:03,980 --> 00:18:08,260
Then an induced subgraph on this, uh, uh, um,

343
00:18:08,260 --> 00:18:11,170
node- nodes, uh, from the union of

344
00:18:11,170 --> 00:18:14,755
the- of the groups that we have sampled, uh, is created.

345
00:18:14,755 --> 00:18:16,525
And then the GNN performs

346
00:18:16,525 --> 00:18:21,265
layer-wise node embeddings update over this induced, uh, subgraph.

347
00:18:21,265 --> 00:18:24,860
Generally, Cluster-GCN is more computationally efficient,

348
00:18:24,860 --> 00:18:30,110
than neighborhood sampling, especially when the number of layers is large, um,

349
00:18:30,110 --> 00:18:33,785
but Cluster-GCN leads to systematically biased gradients

350
00:18:33,785 --> 00:18:37,970
because of missing cross-community edges and also because,

351
00:18:37,970 --> 00:18:40,790
uh, if, uh, number of layers is deep,

352
00:18:40,790 --> 00:18:42,780
then in the original graph,

353
00:18:42,780 --> 00:18:44,390
um, if you do neighborhood sampling,

354
00:18:44,390 --> 00:18:45,675
you can really go deep.

355
00:18:45,675 --> 00:18:47,885
But in the Cluster-GCN,

356
00:18:47,885 --> 00:18:51,980
you are only going to go to the edge of the original of the sampled graph,

357
00:18:51,980 --> 00:18:53,390
and then you'll kind of bounce back.

358
00:18:53,390 --> 00:18:55,940
And even though you have a lot of depth,

359
00:18:55,940 --> 00:18:59,135
this depth would be kind of oscillating over the subgraph

360
00:18:59,135 --> 00:19:02,480
and won't even ec- really explore the real depth,

361
00:19:02,480 --> 00:19:05,315
uh, of the underlying original graph.

362
00:19:05,315 --> 00:19:07,670
So, uh, overall, um,

363
00:19:07,670 --> 00:19:13,500
I would say neighborhood sampling is used more because of additional, uh, flexibility.

