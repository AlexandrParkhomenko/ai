1
00:00:04,130 --> 00:00:08,880
So, um, the third topic I wanna discuss is

2
00:00:08,880 --> 00:00:13,140
scaling up graph neural networks by simplifying their architecture.

3
00:00:13,140 --> 00:00:15,820
Um, and this is kind of an orthogonal approach,

4
00:00:15,820 --> 00:00:17,520
uh, to the first two approaches.

5
00:00:17,520 --> 00:00:19,770
And here is how we are going to do this.

6
00:00:19,770 --> 00:00:21,869
We will start from a graph convolutional,

7
00:00:21,869 --> 00:00:24,210
uh, uh, network, uh,

8
00:00:24,210 --> 00:00:26,100
just as an example architecture,

9
00:00:26,100 --> 00:00:28,335
and we are going to simplify it by removing

10
00:00:28,335 --> 00:00:31,410
the non-linear activation from the GCN, right?

11
00:00:31,410 --> 00:00:33,210
And actually, there's been a paper, uh,

12
00:00:33,210 --> 00:00:37,500
two years ago or a year and a half ago that demonstrates that

13
00:00:37,500 --> 00:00:42,660
the performance on- on benchmarks is not too much lower because you have,

14
00:00:42,660 --> 00:00:44,920
uh, removed the non-linearity.

15
00:00:44,920 --> 00:00:46,590
Uh, and this means that now,

16
00:00:46,590 --> 00:00:50,840
this simplified GCN architecture turns out to be an extremely scalable,

17
00:00:50,840 --> 00:00:53,225
uh, model that we can train very fast.

18
00:00:53,225 --> 00:00:57,050
So basically, the idea here is you are going to simplify

19
00:00:57,050 --> 00:01:01,445
the expressive power of the graph neural network so that you can train it faster.

20
00:01:01,445 --> 00:01:05,390
Of course, this kind of deb- defeats the purpose of deep-learning a bit

21
00:01:05,390 --> 00:01:10,250
because the point of deep learning or representation learning is a lot of data over,

22
00:01:10,250 --> 00:01:15,770
uh, over complex models that can model complex, um, uh, representations.

23
00:01:15,770 --> 00:01:19,910
Here, we are saying, simplify the model so it's fast to run on big data,

24
00:01:19,910 --> 00:01:22,015
and of course, there is a trade off there.

25
00:01:22,015 --> 00:01:25,230
So let me, uh, introduce the, uh,

26
00:01:25,230 --> 00:01:28,350
GCN, uh, and then we'll continue from there, right?

27
00:01:28,350 --> 00:01:30,360
So GCN takes in, uh, a graph,

28
00:01:30,360 --> 00:01:33,135
uh, with, uh, node features.

29
00:01:33,135 --> 00:01:36,155
And let's assume that every node includes a self-loop.

30
00:01:36,155 --> 00:01:38,840
Uh, this would be, uh, convenient for,

31
00:01:38,840 --> 00:01:40,715
uh, mathematical notation later.

32
00:01:40,715 --> 00:01:43,520
And let's think of this as a full batch implementation, right?

33
00:01:43,520 --> 00:01:45,950
So we basically set that the node embeddings

34
00:01:45,950 --> 00:01:48,685
at layer- at layer 0 to be simply node features,

35
00:01:48,685 --> 00:01:50,990
and then we are going to iterate for K,

36
00:01:50,990 --> 00:01:53,375
uh, K layers, uh, where at, uh,

37
00:01:53,375 --> 00:01:56,840
every- every node at layer k plus 1 is going

38
00:01:56,840 --> 00:02:00,890
to take the embeddings of its neighbors from the previous layers,

39
00:02:00,890 --> 00:02:03,350
sum them up, um,

40
00:02:03,350 --> 00:02:07,070
and divide by the number of layers- number of neighbors, uh,

41
00:02:07,070 --> 00:02:11,825
um, transform by this learned matrix and pass through a non-linearity.

42
00:02:11,825 --> 00:02:13,910
And then we're going to run this, uh,

43
00:02:13,910 --> 00:02:18,950
recursion for k iterations and whatever we end up in the end with,

44
00:02:18,950 --> 00:02:20,825
whatever embedding we end up at the end,

45
00:02:20,825 --> 00:02:24,040
that is what we call, uh, final embedding.

46
00:02:24,040 --> 00:02:28,070
Now, what is- benefit of GCN is that it is so simple.

47
00:02:28,070 --> 00:02:31,970
We can very nicely write it into the- in the matrix form.

48
00:02:31,970 --> 00:02:35,450
So the way we are going to write it in the matrix form is that we are going to take

49
00:02:35,450 --> 00:02:40,010
these embeddings and we are going to stack them into an embedding matrix.

50
00:02:40,010 --> 00:02:45,580
And then A is our adjacency matrix where every node also has a self-loop.

51
00:02:45,580 --> 00:02:51,900
Then the way you can write the sum over the neighbors of a given node- sum

52
00:02:51,900 --> 00:02:57,840
of the embeddings of a- of a given no- of the neighbors of a given node,

53
00:02:57,840 --> 00:03:02,395
you can simply write this as a- as a product between the adjacency matrix A

54
00:03:02,395 --> 00:03:07,585
and the- and the embedding matrix H. And, uh,

55
00:03:07,585 --> 00:03:09,445
what this means is that now,

56
00:03:09,445 --> 00:03:13,810
we can also define this notion of D to be

57
00:03:13,810 --> 00:03:18,640
a diagonal matrix where we- it is all of 0 only on the diagonal,

58
00:03:18,640 --> 00:03:21,685
we have the degree of every node.

59
00:03:21,685 --> 00:03:24,100
And then the inverse of D,

60
00:03:24,100 --> 00:03:26,320
D to the minus 1 is the inverse of

61
00:03:26,320 --> 00:03:30,460
the diagonal matrix which is just you put 1 over the degree,

62
00:03:30,460 --> 00:03:33,740
uh, on the- on the diagonal entry for every node.

63
00:03:33,740 --> 00:03:37,070
So now, the way you can write a summation over the neighbors

64
00:03:37,070 --> 00:03:41,500
divided by the degree of that node is simply, uh,

65
00:03:41,500 --> 00:03:43,870
the inverse of the diagonal matrix,

66
00:03:43,870 --> 00:03:50,360
so one over the degree times the adjacency matrix A times the hidden- uh, uh,

67
00:03:50,360 --> 00:03:53,030
embeddings matrix H. So this means that now,

68
00:03:53,030 --> 00:03:54,680
given H at level l,

69
00:03:54,680 --> 00:03:59,150
if we multiply it by A and multiply it by D to the minus 1,

70
00:03:59,150 --> 00:04:02,270
we get the matrix of node embeddings at level h plus 1.

71
00:04:02,270 --> 00:04:05,750
So basically, what is elegant here is that you can rewrite

72
00:04:05,750 --> 00:04:10,140
this iteration just as a product of three matrices,

73
00:04:10,140 --> 00:04:11,840
and of course, in the GCN,

74
00:04:11,840 --> 00:04:14,840
we also have a ReLU non-linearity,

75
00:04:14,840 --> 00:04:18,370
um, uh, and, uh, transformation here.

76
00:04:18,370 --> 00:04:21,149
So going back to the GCN,

77
00:04:21,149 --> 00:04:23,470
here is the node-based, uh, uh,

78
00:04:23,470 --> 00:04:25,080
formulation of the GCN,

79
00:04:25,080 --> 00:04:27,230
if you write it in the matrix form,

80
00:04:27,230 --> 00:04:28,670
you can write it that this is

81
00:04:28,670 --> 00:04:34,515
a non-linearity times this A tilde that is simply degree times, uh,

82
00:04:34,515 --> 00:04:40,065
A, um, times the previous layer embeddings times,

83
00:04:40,065 --> 00:04:43,880
uh, the W matrix which is the transformation matrix.

84
00:04:43,880 --> 00:04:48,195
So basically, the equation- this equation,

85
00:04:48,195 --> 00:04:53,029
uh, that is based on the network and the following matrix equation, they are equivalent.

86
00:04:53,029 --> 00:04:56,525
So if you've- if you've computed this product of these matrices,

87
00:04:56,525 --> 00:04:58,755
you have just computed that the k plus 1 layer

88
00:04:58,755 --> 00:05:01,100
embedding for all the nodes of the network, and now,

89
00:05:01,100 --> 00:05:06,290
you can kind of iterate this capital K times to compute the k layers.

90
00:05:06,290 --> 00:05:11,810
So, um, this is what a matrix formulation of our GCN looks like.

91
00:05:11,810 --> 00:05:14,840
So let's now go and simplify the GCN.

92
00:05:14,840 --> 00:05:20,010
Let's assume and go and remove this ReLU, uh, non-linearity.

93
00:05:20,010 --> 00:05:21,270
So let's- lets say,

94
00:05:21,270 --> 00:05:25,610
what would happen if the GCN would- would be governed by the following equation, right?

95
00:05:25,610 --> 00:05:30,180
So going back, this is the equation with a non-linearity, now,

96
00:05:30,180 --> 00:05:33,420
we decided- now we decided to drop the non-linearity,

97
00:05:33,420 --> 00:05:35,250
so here is our, you know,

98
00:05:35,250 --> 00:05:37,650
simplified the GCN equation.

99
00:05:37,650 --> 00:05:41,955
So now, let's go and unroll this- uh,

100
00:05:41,955 --> 00:05:45,720
this iteration, let's- let's unroll this recursion, right?

101
00:05:45,720 --> 00:05:50,000
So we say, ah, here is the final layer embeddings for the nodes,

102
00:05:50,000 --> 00:05:53,780
uh, they depend on the layer k minus 1 embeddings of the nodes.

103
00:05:53,780 --> 00:05:58,280
So let's now take this H_k minus 1 and insert it, uh,

104
00:05:58,280 --> 00:06:00,290
with the way we compute, uh,

105
00:06:00,290 --> 00:06:04,160
H_k to the minus- H to the layer k minus 1.

106
00:06:04,160 --> 00:06:07,375
And you know, I take this part and just I insert it here.

107
00:06:07,375 --> 00:06:11,100
And now, I notice this depends on H_k minus 2.

108
00:06:11,100 --> 00:06:15,640
So again, I can go take H_k minus 2 and again expand it.

109
00:06:15,640 --> 00:06:19,655
And if I do this all the way down to the 0 layer,

110
00:06:19,655 --> 00:06:22,310
then, um, I- I know what to do, right?

111
00:06:22,310 --> 00:06:26,165
Like H0 is simply the vector of,

112
00:06:26,165 --> 00:06:28,765
uh- of, uh, node features X.

113
00:06:28,765 --> 00:06:32,100
These A tildes just kind of get multiplied together.

114
00:06:32,100 --> 00:06:37,515
So this is A tilde raised to the power k. And this here at the end, these, uh,

115
00:06:37,515 --> 00:06:43,295
parameter matrices, it is just a product of parameter matrices that is still a matrix.

116
00:06:43,295 --> 00:06:45,750
So I can rewrite this, uh,

117
00:06:45,750 --> 00:06:49,490
recursive equa- equation if I expand it in the following way,

118
00:06:49,490 --> 00:06:52,880
and then I realized that a product of

119
00:06:52,880 --> 00:06:56,340
parameter matrices is just a parameter matrix, right?

120
00:06:56,340 --> 00:06:59,810
So basically, I have just rewritten the K layer

121
00:06:59,810 --> 00:07:03,830
GCN into this very simple equation that is non-recursive.

122
00:07:03,830 --> 00:07:06,575
It's A tilde raised to the power k

123
00:07:06,575 --> 00:07:10,410
times the feature vector times the transformation matrix.

124
00:07:10,410 --> 00:07:12,570
So what is important?

125
00:07:12,570 --> 00:07:13,785
What do you need to, uh,

126
00:07:13,785 --> 00:07:17,285
remember here about A tilde raised to the power k?

127
00:07:17,285 --> 00:07:19,760
Remember in I think Lecture 1 or 2,

128
00:07:19,760 --> 00:07:25,400
we talked about what does powering the adjacency matrix to the- to the kth power mean?

129
00:07:25,400 --> 00:07:27,620
It means we are counting paths,

130
00:07:27,620 --> 00:07:30,245
it means we are connecting nodes that are neighbors of-

131
00:07:30,245 --> 00:07:33,110
that are neighbors- neighbors of neighbors and so on.

132
00:07:33,110 --> 00:07:37,100
So basically, what this means is that this til- A tilde to the k really

133
00:07:37,100 --> 00:07:40,970
connects the target node to its neighbors, neighbors of neighbors,

134
00:07:40,970 --> 00:07:42,470
neighbors of neighbors of neighbors,

135
00:07:42,470 --> 00:07:43,910
and so on, um,

136
00:07:43,910 --> 00:07:48,285
one-hop farther out in the network as we increase,

137
00:07:48,285 --> 00:07:50,085
uh, uh, K, um,

138
00:07:50,085 --> 00:07:51,840
and that's very interesting.

139
00:07:51,840 --> 00:07:54,885
So now, why did- what can we conclude?

140
00:07:54,885 --> 00:08:00,210
We can conclude that removing the non-linearity significantly simplifies the GCN.

141
00:08:00,210 --> 00:08:02,955
Uh, notice also that, um,

142
00:08:02,955 --> 00:08:05,670
these A tilde to the k times x does not

143
00:08:05,670 --> 00:08:09,050
include- contain any learnable parameters so we can

144
00:08:09,050 --> 00:08:15,150
actually pre-compute this on the- on the CPU even before we start, uh, training, right?

145
00:08:15,150 --> 00:08:19,160
And this can be very efficiently computed because all I have to do is,

146
00:08:19,160 --> 00:08:22,275
um, multiply the mat- uh,

147
00:08:22,275 --> 00:08:24,390
A tilde with, uh,

148
00:08:24,390 --> 00:08:26,940
x, uh, kind of with itself, uh,

149
00:08:26,940 --> 00:08:31,980
multiple times, and I will be able to get to the A tilde raised to the power K times x.

150
00:08:31,980 --> 00:08:34,260
So computing this part is very easy,

151
00:08:34,260 --> 00:08:37,835
it does not depend on any, um,

152
00:08:37,835 --> 00:08:42,445
model parameters so I can just pre-compute it even before I start learning.

153
00:08:42,445 --> 00:08:45,615
So now, um, how about,

154
00:08:45,615 --> 00:08:48,740
uh- so what we have learned is that this, uh,

155
00:08:48,740 --> 00:08:54,295
A_k times x could be pre-computed and let's call this X tilde.

156
00:08:54,295 --> 00:08:56,930
So now, uh, the simplified GCN,

157
00:08:56,930 --> 00:09:01,310
the final embedding layer is simply this X tilde times the parameter matrix,

158
00:09:01,310 --> 00:09:05,650
and this is just a linear transformation of the pre-computed matrix, right?

159
00:09:05,650 --> 00:09:07,840
So the way I can- I can think of this,

160
00:09:07,840 --> 00:09:11,260
this is basically just a pre-computed feature vector for node,

161
00:09:11,260 --> 00:09:14,355
uh, v, uh, times its- uh,

162
00:09:14,355 --> 00:09:18,515
times the matrix- learned matrix W. So embedding of node,

163
00:09:18,515 --> 00:09:22,940
uh, v only depends on its own pre-processed, uh, features,

164
00:09:22,940 --> 00:09:26,155
right, where I had- I say this X tilde is really

165
00:09:26,155 --> 00:09:31,935
X tilde computed by A- A tilde raised to the power K times X, right?

166
00:09:31,935 --> 00:09:36,160
But this is a matrix that has one row for every node,

167
00:09:36,160 --> 00:09:39,260
so if I say what is the final layer embedding of a given node?

168
00:09:39,260 --> 00:09:42,150
Is simply the- the corresponding row in

169
00:09:42,150 --> 00:09:45,600
the matrix times the learned parameter matrix, uh,

170
00:09:45,600 --> 00:09:51,470
W. But what is important to notice here is that once this X tilde has been computed,

171
00:09:51,470 --> 00:09:53,805
then learn- then, uh, um,

172
00:09:53,805 --> 00:09:56,645
the embedding of a given node only depends on

173
00:09:56,645 --> 00:10:00,065
a given row of the X tilde that is fixed and constant.

174
00:10:00,065 --> 00:10:06,000
And, uh, the only thing that changes that is learnable is W. So what this means is

175
00:10:06,000 --> 00:10:12,220
that embe- embeddings of M nodes can be generated in linear ti- in the time linear,

176
00:10:12,220 --> 00:10:15,755
um, with M because for a given, uh, node,

177
00:10:15,755 --> 00:10:19,555
its final embedding only depends on its own,

178
00:10:19,555 --> 00:10:22,485
uh, row in the matrix, uh, X tilde.

179
00:10:22,485 --> 00:10:25,890
So I can easily sample a mini batch of nodes,

180
00:10:25,890 --> 00:10:30,490
I sample a set of rows from the matrix X,

181
00:10:30,490 --> 00:10:35,200
and then I multiply that with W to get the final layer embeddings of those,

182
00:10:35,200 --> 00:10:37,985
uh, nodes, uh, in the mini-batch.

183
00:10:37,985 --> 00:10:43,790
So, um, and of course this would be super fast because there is no dependencies between

184
00:10:43,790 --> 00:10:49,430
the nodes or all the dependencies are already captured in this matrix, uh, X tilde.

185
00:10:49,430 --> 00:10:54,334
So in summary, uh, Simplified GCN consists of two steps,

186
00:10:54,334 --> 00:10:57,035
the pre-processing step where- uh,

187
00:10:57,035 --> 00:11:05,510
where a pre- where we pre-compute this X tilde to be simply the adjacency matrix A,

188
00:11:05,510 --> 00:11:09,350
um, with, uh, one over the degree of the node on the diagonal.

189
00:11:09,350 --> 00:11:10,715
We call this A tilde,

190
00:11:10,715 --> 00:11:14,515
we raise this to the Kth power so we multiply it with itself,

191
00:11:14,515 --> 00:11:16,230
K step, K times,

192
00:11:16,230 --> 00:11:21,830
and then we multiply this with the original features of the nodes x.

193
00:11:21,830 --> 00:11:26,420
And all of this can be done on a CPU even before we start training.

194
00:11:26,420 --> 00:11:30,920
So now what this means is that we have this matrix, uh, uh, uh,

195
00:11:30,920 --> 00:11:35,930
X, uh, X tilde that has one row for every node, um,

196
00:11:35,930 --> 00:11:38,690
and that's all- all we need for

197
00:11:38,690 --> 00:11:43,070
the training step where basically for each mini-batch we are going to sample uh,

198
00:11:43,070 --> 00:11:45,065
M nodes at random,

199
00:11:45,065 --> 00:11:48,905
and then we simply compute their embeddings by taking,

200
00:11:48,905 --> 00:11:50,225
uh, W and, uh,

201
00:11:50,225 --> 00:11:52,400
multiplying it with uh, with uh,

202
00:11:52,400 --> 00:11:55,490
uh, with the corresponding, uh, uh,

203
00:11:55,490 --> 00:11:58,385
entry in the- in the matrix, uh,

204
00:11:58,385 --> 00:12:00,560
X tilde that corresponds to that,

205
00:12:00,560 --> 00:12:03,390
uh, node, uh, um, uh, uh,

206
00:12:03,390 --> 00:12:08,090
v. And we simply compute the final layer embeddings of all the nodes in the mini-batch,

207
00:12:08,090 --> 00:12:09,470
um, and then, you know,

208
00:12:09,470 --> 00:12:11,480
we can use these embeddings to make predictions,

209
00:12:11,480 --> 00:12:12,785
compute the loss, uh,

210
00:12:12,785 --> 00:12:16,640
and then update the matrix- the parameter matrix,

211
00:12:16,640 --> 00:12:19,700
uh, W. So, um,

212
00:12:19,700 --> 00:12:23,540
the- the good point here is that now the embedding of every node,

213
00:12:23,540 --> 00:12:27,185
uh, computation of it is independent from the other nodes.

214
00:12:27,185 --> 00:12:28,550
It is a simple, uh,

215
00:12:28,550 --> 00:12:31,310
matrix times vector product where

216
00:12:31,310 --> 00:12:34,760
the matrix W is what we are trying to learn and this can be done,

217
00:12:34,760 --> 00:12:36,515
uh, super, super fast.

218
00:12:36,515 --> 00:12:38,900
So, um, now let's,

219
00:12:38,900 --> 00:12:41,195
uh, uh, summarize and kind of, uh,

220
00:12:41,195 --> 00:12:44,090
compare, uh, uh, Cluster-GCN with,

221
00:12:44,090 --> 00:12:47,330
uh, uh, other methods we have learned, uh, today.

222
00:12:47,330 --> 00:12:52,070
The simplified GCN generates node embeddings much more, uh, efficiently.

223
00:12:52,070 --> 00:12:55,040
There is no need to create the giant computation graph,

224
00:12:55,040 --> 00:12:57,064
there is no need to do graph sampling,

225
00:12:57,064 --> 00:12:59,450
it is all very, very simple, right?

226
00:12:59,450 --> 00:13:05,240
You just do those matrix products and then the learning step is also very easy.

227
00:13:05,240 --> 00:13:09,080
So, um, it seems great, but what do we lose?

228
00:13:09,080 --> 00:13:12,200
Um, right, compared to Cluster-GCN,

229
00:13:12,200 --> 00:13:17,960
mini-batch of nodes in a simplified GCN can be sampled completely at random from,

230
00:13:17,960 --> 00:13:19,625
uh, the entire set of nodes.

231
00:13:19,625 --> 00:13:22,325
As I said, there is no need to do group- uh,

232
00:13:22,325 --> 00:13:24,260
to do node groups to do the,

233
00:13:24,260 --> 00:13:25,970
uh, like in Cluster-GCN,

234
00:13:25,970 --> 00:13:29,120
no need to do the induced subgraph, uh, nothing like this.

235
00:13:29,120 --> 00:13:30,860
So this me- this means that, uh,

236
00:13:30,860 --> 00:13:34,250
our- the training is very stable, um,

237
00:13:34,250 --> 00:13:36,589
and the- the variance of the gradients,

238
00:13:36,589 --> 00:13:38,615
uh, is much more under control.

239
00:13:38,615 --> 00:13:41,240
But, right, what is the price?

240
00:13:41,240 --> 00:13:43,670
The price is that this model is far,

241
00:13:43,670 --> 00:13:45,930
far, uh, less expressive.

242
00:13:45,930 --> 00:13:49,450
Meaning, compared to the original graph neural network models,

243
00:13:49,450 --> 00:13:51,790
simplified GCN is far less

244
00:13:51,790 --> 00:13:55,900
expressive because it lacked- it doesn't have the non-linearity,

245
00:13:55,900 --> 00:13:58,465
uh, in generating, uh, node embedding.

246
00:13:58,465 --> 00:14:01,330
So it means that the theory that we discussed

247
00:14:01,330 --> 00:14:07,155
about computation graphs, Weisfeiler-Lehman isomorphism test,

248
00:14:07,155 --> 00:14:10,775
keeping the structure of the underlying subgraphs, uh,

249
00:14:10,775 --> 00:14:12,650
through that injective mapping,

250
00:14:12,650 --> 00:14:17,390
all that is out of the window because we don't have the non-linearity anymore.

251
00:14:17,390 --> 00:14:22,595
So that really makes a huge difference in the expressive power, uh, of the model.

252
00:14:22,595 --> 00:14:24,740
Right? But, you know,

253
00:14:24,740 --> 00:14:26,990
in many real-world cases,

254
00:14:26,990 --> 00:14:29,150
a simplified GCN, uh,

255
00:14:29,150 --> 00:14:31,205
tends to work well, um,

256
00:14:31,205 --> 00:14:36,530
and tends to kind of work just slightly worse than the original graph neural networks,

257
00:14:36,530 --> 00:14:38,510
even though being far less, uh,

258
00:14:38,510 --> 00:14:40,850
expressive in, uh, theory.

259
00:14:40,850 --> 00:14:44,345
So the question is, uh, why is that?

260
00:14:44,345 --> 00:14:48,875
And the reason for that is something that is called graph homophily,

261
00:14:48,875 --> 00:14:50,540
which basically means, uh,

262
00:14:50,540 --> 00:14:53,585
this is a concept from social science, uh,

263
00:14:53,585 --> 00:14:56,420
generally people like to call it, uh,

264
00:14:56,420 --> 00:14:59,615
birds of feather, uh, sto- stick together.

265
00:14:59,615 --> 00:15:01,970
So or birds of feather flock together.

266
00:15:01,970 --> 00:15:07,040
So basically the idea is that similar people tends to connect with each other, right?

267
00:15:07,040 --> 00:15:09,005
So that basically, you know,

268
00:15:09,005 --> 00:15:11,285
computer scientists know each other,

269
00:15:11,285 --> 00:15:13,370
uh, people who study music know each other.

270
00:15:13,370 --> 00:15:14,810
So essentially, the idea is that you have

271
00:15:14,810 --> 00:15:18,740
these social communities that are tightly compact where people share

272
00:15:18,740 --> 00:15:21,545
properties and attributes just because it is easier

273
00:15:21,545 --> 00:15:24,740
to connect to people who have something in common with you,

274
00:15:24,740 --> 00:15:27,530
with whom you share some, uh, interest, right?

275
00:15:27,530 --> 00:15:29,090
So basically what this means in,

276
00:15:29,090 --> 00:15:32,825
let's say networks, both in social, biological,

277
00:15:32,825 --> 00:15:36,320
knowledge graphs is that nodes connected by edges tend to

278
00:15:36,320 --> 00:15:40,655
share the same labels they tend to have similar features.

279
00:15:40,655 --> 00:15:43,025
Right? In citation networks,

280
00:15:43,025 --> 00:15:45,935
papers from the same- same area tend to cite each other.

281
00:15:45,935 --> 00:15:49,490
In movie recommendation, it's like people are interested in

282
00:15:49,490 --> 00:15:53,480
given genres and you watch multiple movies from the same genre.

283
00:15:53,480 --> 00:15:56,330
So these movies are kind of similar to each other.

284
00:15:56,330 --> 00:15:58,370
So, um, and, you know,

285
00:15:58,370 --> 00:16:01,100
why- why is this important for a simplified GCN?

286
00:16:01,100 --> 00:16:05,560
Because, um, the t- the three- pre-processing step of

287
00:16:05,560 --> 00:16:11,830
a simplified GCN is simply feature aggregation over a k-hop, uh, neighborhood, right?

288
00:16:11,830 --> 00:16:15,160
So the pre-processing features obtained by- is- are

289
00:16:15,160 --> 00:16:18,640
obtained by iteratively averaging, um, um, uh,

290
00:16:18,640 --> 00:16:22,835
features of the neighbors and features of the neighbors of neighbors

291
00:16:22,835 --> 00:16:27,455
without learned transformation and will- without any non-linearity.

292
00:16:27,455 --> 00:16:28,790
So, as a result,

293
00:16:28,790 --> 00:16:32,480
nodes connected by edges tend to have similar pre-processing,

294
00:16:32,480 --> 00:16:35,390
uh, features, and now with labels, um,

295
00:16:35,390 --> 00:16:41,060
are also clustered kind of across the homophilis parts of the network,

296
00:16:41,060 --> 00:16:44,210
um, if the labels kind of cluster a- across the network,

297
00:16:44,210 --> 00:16:47,660
then simplified GCN will work, uh, really well.

298
00:16:47,660 --> 00:16:52,610
So, uh, basically, the- the- when does the simplified GCN work?

299
00:16:52,610 --> 00:16:56,810
The premise is that the model uses the pre-process node features to make prediction.

300
00:16:56,810 --> 00:16:59,900
Nodes connected by edges tend to get

301
00:16:59,900 --> 00:17:02,450
similar pre-processed features because it's all

302
00:17:02,450 --> 00:17:05,540
about feature averaging across local neighborhoods.

303
00:17:05,540 --> 00:17:09,560
So if nodes connected by edges tend to be in the same class,

304
00:17:09,560 --> 00:17:13,025
tend to have the same label, then simplified GCN,

305
00:17:13,025 --> 00:17:14,540
uh, is going to, uh,

306
00:17:14,540 --> 00:17:16,520
make very accurate, uh,

307
00:17:16,520 --> 00:17:21,020
predictions, uh, so basically if the graph has this kind of homophily structure.

308
00:17:21,020 --> 00:17:23,944
Now, if the graph doesn't have this homophily structure,

309
00:17:23,944 --> 00:17:26,854
then the simplified GCN is going to fail,

310
00:17:26,855 --> 00:17:28,910
uh, quite, uh, quite bad.

311
00:17:28,910 --> 00:17:30,590
So that's kind of the intuition,

312
00:17:30,590 --> 00:17:32,000
and of course, ahead of time,

313
00:17:32,000 --> 00:17:35,690
we generally don't know whether labels are clustered together

314
00:17:35,690 --> 00:17:39,905
or they're kind of the- kind of sparkled, uh, across the network.

315
00:17:39,905 --> 00:17:46,309
So to summarize, simplified GCN removes non-linearity in GCN and then reduces,

316
00:17:46,309 --> 00:17:48,620
uh, the- uh, to simple, uh,

317
00:17:48,620 --> 00:17:52,205
pre-processing of the node features and graph adjacency matrix.

318
00:17:52,205 --> 00:17:57,199
When these pre-processed features are obtained on the CPU, a very scalable,

319
00:17:57,199 --> 00:18:00,860
simple mini-batch- batch gra- stochastic gradient descent

320
00:18:00,860 --> 00:18:04,595
can be directly applied to optimize the parameters.

321
00:18:04,595 --> 00:18:10,370
Um, simplified GCN works surprisingly well in- in many benchmarks.

322
00:18:10,370 --> 00:18:14,315
Uh, the reason for that being is that those benchmarks are easy.

323
00:18:14,315 --> 00:18:18,230
Um, uh, nodes of similar label tend to link to each other.

324
00:18:18,230 --> 00:18:19,415
They tend to be part,

325
00:18:19,415 --> 00:18:20,975
uh, of the same network,

326
00:18:20,975 --> 00:18:25,385
and meaning that just simple averaging of their features, um, uh,

327
00:18:25,385 --> 00:18:30,290
without any nonlinearities and without any weight or different weighing of them,

328
00:18:30,290 --> 00:18:31,700
um, gives you, uh,

329
00:18:31,700 --> 00:18:34,800
good performance, uh, in practice.

