1
00:00:04,550 --> 00:00:11,670
So now we are going to move forward and we are going to move to the next topic,

2
00:00:11,670 --> 00:00:15,630
which is called identity-aware graph neural networks, right?

3
00:00:15,630 --> 00:00:18,735
So in the previous part of the lecture, we talked about,

4
00:00:18,735 --> 00:00:22,410
how does the node encode its position in the network?

5
00:00:22,410 --> 00:00:26,610
How does the node know where in the network, the node is?

6
00:00:26,610 --> 00:00:28,185
Now in the second part,

7
00:00:28,185 --> 00:00:34,200
we are going to develop a more expressive graph neural network that is

8
00:00:34,200 --> 00:00:36,105
going to take care of

9
00:00:36,105 --> 00:00:40,880
all these different symmetries that can account- that can appear in the network and uh,

10
00:00:40,880 --> 00:00:41,975
in the underlying graph,

11
00:00:41,975 --> 00:00:45,955
and it - it will make the graph neural network more expressive.

12
00:00:45,955 --> 00:00:49,380
So what we have learned so far is that

13
00:00:49,380 --> 00:00:53,880
classical GNNs would fail for position-aware tasks.

14
00:00:53,880 --> 00:00:57,430
And we said, let's use, um,

15
00:00:57,430 --> 00:01:04,825
let's use anchors to improve graph neural network performance on position-aware tasks.

16
00:01:04,825 --> 00:01:10,445
Now, we are going to switch back and to- and focus more on structure-aware tasks.

17
00:01:10,445 --> 00:01:14,975
And say, can GNNs perform perfectly on structure-aware tasks?

18
00:01:14,975 --> 00:01:17,270
And as we have seen before,

19
00:01:17,270 --> 00:01:19,915
the answer here is unfortunately no.

20
00:01:19,915 --> 00:01:21,575
Uh, and the issue is that

21
00:01:21,575 --> 00:01:27,305
GNNs exhibit kind of three levels of failure cases in, uh structure-aware tasks.

22
00:01:27,305 --> 00:01:29,435
And I'm going to show you some,

23
00:01:29,435 --> 00:01:31,205
you know, failure cases.

24
00:01:31,205 --> 00:01:35,600
And of course, all these failure cases are kind of worst-case scenarios uh,

25
00:01:35,600 --> 00:01:38,390
that are very intricate in a sense that uh,

26
00:01:38,390 --> 00:01:39,770
due to the symmetries,

27
00:01:39,770 --> 00:01:41,330
the GNN is going to fail.

28
00:01:41,330 --> 00:01:46,115
So perhaps they don't necessarily appear in practice uh, too often,

29
00:01:46,115 --> 00:01:48,800
but they may appear in some parts of the data,

30
00:01:48,800 --> 00:01:52,495
and they are still very useful uh, to study.

31
00:01:52,495 --> 00:01:55,920
So here is the first uh, failure case.

32
00:01:55,920 --> 00:01:58,185
Uh, this is for the Node-level tasks.

33
00:01:58,185 --> 00:02:01,280
Imagine you wanna do a Node-level classification,

34
00:02:01,280 --> 00:02:03,295
you want to do Node-level prediction.

35
00:02:03,295 --> 00:02:07,695
Here, different inputs from the same, uh,

36
00:02:07,695 --> 00:02:09,830
basically different inputs, but at

37
00:02:09,830 --> 00:02:13,600
the same computational graph will result in the same embedding.

38
00:02:13,600 --> 00:02:15,710
So if I have these two nodes,

39
00:02:15,710 --> 00:02:16,985
v_1 and v_2, uh,

40
00:02:16,985 --> 00:02:21,470
you know, residing in these types of connected components, as we said before,

41
00:02:21,470 --> 00:02:23,580
their computational graphs, um,

42
00:02:23,580 --> 00:02:26,375
if you work it out are exactly the same because they have

43
00:02:26,375 --> 00:02:30,655
two neighbors and each of their neighbors has two neighbors and so on and so forth.

44
00:02:30,655 --> 00:02:34,385
So this means that these nodes v_1 and v_2 will be embedded into

45
00:02:34,385 --> 00:02:37,100
exactly the same point in the embedding space

46
00:02:37,100 --> 00:02:40,285
and we won't be able to assign them different labels.

47
00:02:40,285 --> 00:02:43,730
Now, the same type of things can happen also, for example,

48
00:02:43,730 --> 00:02:46,040
for link prediction, where for example,

49
00:02:46,040 --> 00:02:48,005
you can have this type of input graph.

50
00:02:48,005 --> 00:02:50,060
And you want to decide whether you know,

51
00:02:50,060 --> 00:02:53,695
v_0 should link to v_1 or should it link to v_2?

52
00:02:53,695 --> 00:02:56,930
And again, if you look at the computation graphs,

53
00:02:56,930 --> 00:03:00,410
um, the -the computation graphs are the same.

54
00:03:00,410 --> 00:03:04,760
So nodes v_1 and v_2 are going to have the same embedding.

55
00:03:04,760 --> 00:03:06,830
And because they have the same embedding,

56
00:03:06,830 --> 00:03:14,145
the neural network will give the same probability to- to edge A as well as to edge B.

57
00:03:14,145 --> 00:03:18,180
And perhaps that is not uh, the most realistic.

58
00:03:18,180 --> 00:03:24,110
So that's our failure case again for a different type of uh, input graph.

59
00:03:24,110 --> 00:03:25,450
And then, you know,

60
00:03:25,450 --> 00:03:27,100
for graph level tasks,

61
00:03:27,100 --> 00:03:28,660
there are also uh,

62
00:03:28,660 --> 00:03:31,390
well-known failure cases because

63
00:03:31,390 --> 00:03:34,180
different input graphs will still

64
00:03:34,180 --> 00:03:37,910
result in the same graph neural network based embedding.

65
00:03:37,910 --> 00:03:41,160
Um, and why- why is that the case?

66
00:03:41,160 --> 00:03:42,945
It's because if you, for example,

67
00:03:42,945 --> 00:03:46,765
uh, in these types of- in these types of networks,

68
00:03:46,765 --> 00:03:49,225
all the nodes have the same degree, um,

69
00:03:49,225 --> 00:03:51,700
but you notice that these two graphs are different because

70
00:03:51,700 --> 00:03:55,090
here the nodes link to exactly the immediate nodes.

71
00:03:55,090 --> 00:03:58,550
Here the- the nodes link kind of a bit farther out.

72
00:03:58,550 --> 00:04:00,615
But if you look at the computation graphs,

73
00:04:00,615 --> 00:04:03,475
the two computation graphs uh, will be the same.

74
00:04:03,475 --> 00:04:09,440
So again, uh, these two- these two entire graphs will get the same embedding.

75
00:04:09,440 --> 00:04:11,600
So again, this is, um,

76
00:04:11,600 --> 00:04:14,780
a very kind of highly symmetric uh, input graph.

77
00:04:14,780 --> 00:04:17,490
But still these two graphs are different.

78
00:04:17,490 --> 00:04:18,885
They are non-isomorphic.

79
00:04:18,885 --> 00:04:21,860
But you know, this is kind of a corner case for

80
00:04:21,860 --> 00:04:27,485
WL test and it is also a corner case for graph neural networks.

81
00:04:27,485 --> 00:04:29,900
So here again, uh, graph,

82
00:04:29,900 --> 00:04:32,390
A and graph neural network without

83
00:04:32,390 --> 00:04:36,430
any useful node features will always classify nodes A and B,

84
00:04:36,430 --> 00:04:41,935
uh, or graphs A and B into the same position, into the same class.

85
00:04:41,935 --> 00:04:45,190
So now, how are we going to resolve this?

86
00:04:45,190 --> 00:04:46,955
What is the big idea here?

87
00:04:46,955 --> 00:04:50,359
And the big idea in this second part of the lecture,

88
00:04:50,359 --> 00:04:54,500
is that we can assign a color to the node we want to embed.

89
00:04:54,500 --> 00:04:56,930
And that's why we call this identity-aware,

90
00:04:56,930 --> 00:04:58,790
because the neural network,

91
00:04:58,790 --> 00:04:59,945
as we unroll it,

92
00:04:59,945 --> 00:05:02,210
will know what is the starting node,

93
00:05:02,210 --> 00:05:03,965
what is the node where we started?

94
00:05:03,965 --> 00:05:08,505
So the idea is, if I want to embed nodes v_1- v_1,

95
00:05:08,505 --> 00:05:09,945
I'm going to color it.

96
00:05:09,945 --> 00:05:11,310
And if I go, um,

97
00:05:11,310 --> 00:05:14,485
and because I'm going to give it a color,

98
00:05:14,485 --> 00:05:18,180
um, now, the graph, the computational, uh,

99
00:05:18,180 --> 00:05:22,130
graph will be different because I will remember whenever I unroll uh,

100
00:05:22,130 --> 00:05:27,935
the computational graph, I will remember the color of this colored node.

101
00:05:27,935 --> 00:05:30,585
Right? So this means that, uh,

102
00:05:30,585 --> 00:05:33,435
now our computational graph,

103
00:05:33,435 --> 00:05:39,200
will- will- will remember whenever it hits the node of interest v_1.

104
00:05:39,200 --> 00:05:42,330
So it we'll have these colors um, and you know,

105
00:05:42,330 --> 00:05:43,830
why- why is this,

106
00:05:43,830 --> 00:05:46,095
um, uh, why is this useful?

107
00:05:46,095 --> 00:05:49,150
This is useful because it is inductive.

108
00:05:49,150 --> 00:05:52,030
Right? It is invariant to the node ordering, um,

109
00:05:52,030 --> 00:05:57,620
or identities of the nodes because the only node we color is the node where we started.

110
00:05:57,620 --> 00:05:59,315
And then we just look,

111
00:05:59,315 --> 00:06:00,620
how often does this node,

112
00:06:00,620 --> 00:06:04,260
where we start appear in the computation graph, right?

113
00:06:04,260 --> 00:06:08,070
So eventually, right, like if- if

114
00:06:08,070 --> 00:06:13,985
our graph is- is connected as- as we go deeper into more layers of a graph neural network,

115
00:06:13,985 --> 00:06:18,270
there will be some cycle that will lead us back to the starting node,

116
00:06:18,270 --> 00:06:20,420
and we will remember that and have that

117
00:06:20,420 --> 00:06:24,445
node colored in the computation graph, uh, as well.

118
00:06:24,445 --> 00:06:31,175
And the important point here is because- because the node coloring is inductive,

119
00:06:31,175 --> 00:06:33,035
even though I- I have these two,

120
00:06:33,035 --> 00:06:34,640
let's say, different input graphs,

121
00:06:34,640 --> 00:06:36,425
but I have labeled, uh,

122
00:06:36,425 --> 00:06:38,690
or numbered the nodes differently, right?

123
00:06:38,690 --> 00:06:40,070
I have 1, 2,

124
00:06:40,070 --> 00:06:41,780
3 versus 1, 2,

125
00:06:41,780 --> 00:06:46,085
3, the underlying computational graphs will be the same,

126
00:06:46,085 --> 00:06:49,550
which is good because they don't change under, uh,

127
00:06:49,550 --> 00:06:53,675
permuting the IDs or identities, uh, of the node.

128
00:06:53,675 --> 00:06:56,210
So this is a great feature to have because it

129
00:06:56,210 --> 00:06:59,905
means our models are able to generalize better.

130
00:06:59,905 --> 00:07:03,100
So let's now talk more about this, uh,

131
00:07:03,100 --> 00:07:06,500
inductive capability of node coloring.

132
00:07:06,500 --> 00:07:08,605
And let's look at the node level task.

133
00:07:08,605 --> 00:07:10,535
Um, and the point is that

134
00:07:10,535 --> 00:07:15,605
this inductive node coloring helps us with node classification tasks.

135
00:07:15,605 --> 00:07:17,990
For example, I have here,

136
00:07:17,990 --> 00:07:22,470
um, the case, we have already,

137
00:07:22,470 --> 00:07:23,685
uh, looked at before.

138
00:07:23,685 --> 00:07:25,370
I have the node on a triangle,

139
00:07:25,370 --> 00:07:26,720
I have a node on a square,

140
00:07:26,720 --> 00:07:28,355
um, I colored the root.

141
00:07:28,355 --> 00:07:30,815
And now I say, let's create the computation graphs.

142
00:07:30,815 --> 00:07:33,965
Here I create the computation graphs and you, um,

143
00:07:33,965 --> 00:07:37,265
very quickly see that the computation graphs,

144
00:07:37,265 --> 00:07:39,925
um, are quite- are quite different.

145
00:07:39,925 --> 00:07:43,260
And in particular they- they become different at, uh,

146
00:07:43,260 --> 00:07:44,510
at the bottom level,

147
00:07:44,510 --> 00:07:48,120
where in the- in the part B here,

148
00:07:48,120 --> 00:07:52,880
when I go to two hops- when I go two hops out,

149
00:07:52,880 --> 00:07:58,725
I only hit these nodes while in the- in the first case,

150
00:07:58,725 --> 00:08:02,710
um, I actually get- go and hit again the starting node.

151
00:08:02,710 --> 00:08:07,680
So now these two computation graphs are different because we also consider colors.

152
00:08:07,680 --> 00:08:13,470
So we will be able to successfully differentiate between nodes v_1 and nodes v_2.

153
00:08:13,470 --> 00:08:18,595
So, uh, this is a very elegant solution to the- to- to this, uh,

154
00:08:18,595 --> 00:08:21,115
to this, uh, uh problem that where

155
00:08:21,115 --> 00:08:24,220
a classical graph neural network, uh, would fail.

156
00:08:24,220 --> 00:08:27,535
Um, and similarly, we can do the same thing,

157
00:08:27,535 --> 00:08:30,520
uh, for- um, for graph classification, right?

158
00:08:30,520 --> 00:08:32,559
If I take my two input graphs, uh,

159
00:08:32,559 --> 00:08:37,509
the way I created the- aga- the embedding of the graph is to create an embedding,

160
00:08:37,510 --> 00:08:39,895
uh, of nodes and then aggregate those.

161
00:08:39,895 --> 00:08:42,955
So if I look at node-specific, um, uh,

162
00:08:42,955 --> 00:08:45,655
computation graphs, uh, structurally,

163
00:08:45,655 --> 00:08:46,750
they might be the same,

164
00:08:46,750 --> 00:08:51,160
but- but- but because I have labeled the starting node and

165
00:08:51,160 --> 00:08:56,035
now I- I know whenever my computation graph returns back to the starting node,

166
00:08:56,035 --> 00:08:59,620
you'll notice that now the coloring pattern between these two graphs,

167
00:08:59,620 --> 00:09:02,455
uh, is different- these two nodes is different,

168
00:09:02,455 --> 00:09:04,600
which means their embeddings will be

169
00:09:04,600 --> 00:09:07,480
different which means that when we aggregate the embeddings,

170
00:09:07,480 --> 00:09:09,745
the embeddings for the graphs will be different,

171
00:09:09,745 --> 00:09:12,700
which means we'll be able to take these two input graphs,

172
00:09:12,700 --> 00:09:13,900
A and B, um,

173
00:09:13,900 --> 00:09:16,795
and embed them into,

174
00:09:16,795 --> 00:09:19,885
um, different points and assign them different classes.

175
00:09:19,885 --> 00:09:21,965
So this is exactly what we want.

176
00:09:21,965 --> 00:09:23,550
And then, you know,

177
00:09:23,550 --> 00:09:25,110
for the edge level tasks,

178
00:09:25,110 --> 00:09:27,030
again, ah, if, you know,

179
00:09:27,030 --> 00:09:29,235
I start with V_0 and I say,

180
00:09:29,235 --> 00:09:32,355
you know I want to assign a different, uh, uh,

181
00:09:32,355 --> 00:09:36,435
probability to, um, uh, to nodes,

182
00:09:36,435 --> 00:09:38,565
V_1- to the edges A and B,

183
00:09:38,565 --> 00:09:42,325
I can say what will be the embedding of V_1, uh, here?

184
00:09:42,325 --> 00:09:44,185
What will be embedding of V_2?

185
00:09:44,185 --> 00:09:47,350
And I see that their corresponding computation graphs, uh,

186
00:09:47,350 --> 00:09:49,210
will be different because, uh,

187
00:09:49,210 --> 00:09:53,545
V_1 is going to hit V_0 sooner than, uh, V_2.

188
00:09:53,545 --> 00:09:59,005
So, um, here, the point is that when I'm embedding nodes for link prediction,

189
00:09:59,005 --> 00:10:01,165
I'm given a pair of nodes and here,

190
00:10:01,165 --> 00:10:02,890
I'm going to color,

191
00:10:02,890 --> 00:10:04,990
um, both- both nodes,

192
00:10:04,990 --> 00:10:08,245
the- the- the left node and the right node and this way,

193
00:10:08,245 --> 00:10:09,820
I'll be able to distinguish, um,

194
00:10:09,820 --> 00:10:13,495
uh, the two computational, ah, graphs.

195
00:10:13,495 --> 00:10:17,080
So this means it will allow us to, uh,

196
00:10:17,080 --> 00:10:21,790
assign a different probability to the node- to the edge A versus,

197
00:10:21,790 --> 00:10:25,400
uh, the edge B, which is, uh, what we want.

198
00:10:25,400 --> 00:10:29,700
So what you- what I have demonstrated so far is that

199
00:10:29,700 --> 00:10:35,135
this node coloring where we color the identity of the- uh,

200
00:10:35,135 --> 00:10:40,660
of the starting node or in link prediction of the- of this- of these both, uh,

201
00:10:40,660 --> 00:10:43,285
nodes in involving the link prediction task

202
00:10:43,285 --> 00:10:46,225
allows us to differentiate and distinguish, uh,

203
00:10:46,225 --> 00:10:48,550
these types of symmetric, uh,

204
00:10:48,550 --> 00:10:53,995
corner cases that make classical neural network- graph neural networks, uh, fail.

205
00:10:53,995 --> 00:10:55,525
So now, the question is,

206
00:10:55,525 --> 00:11:01,375
how do you build a- a GNN that uses this node coloring and that you- it will allow us,

207
00:11:01,375 --> 00:11:06,865
uh, to distinguish these different colored, uh, computation graphs.

208
00:11:06,865 --> 00:11:11,995
The idea is the following and the model is called identity aware, uh,

209
00:11:11,995 --> 00:11:14,650
graph neural network and what we wanna do

210
00:11:14,650 --> 00:11:17,380
is you've wanna utilize inductive node coloring in

211
00:11:17,380 --> 00:11:20,440
the embedding computation and the key idea

212
00:11:20,440 --> 00:11:24,355
is that you want to use heterogeneous message passing, right?

213
00:11:24,355 --> 00:11:25,930
Normally in a GNN,

214
00:11:25,930 --> 00:11:28,885
we apply the same message aggregation computation

215
00:11:28,885 --> 00:11:31,780
to all the children in the computation graph, right?

216
00:11:31,780 --> 00:11:35,125
So whenever we- we are, uh, aggregating, uh,

217
00:11:35,125 --> 00:11:37,345
masters and transporting messages,

218
00:11:37,345 --> 00:11:41,500
we apply the same aggregation in the same neural network operator, right?

219
00:11:41,500 --> 00:11:43,960
So, um, this is what we classically do.

220
00:11:43,960 --> 00:11:47,545
In our graph neural- identity aware graph neural network,

221
00:11:47,545 --> 00:11:50,380
we are going to do heterogeneous message passing.

222
00:11:50,380 --> 00:11:54,820
So we are going to use different types of aggregation, um,

223
00:11:54,820 --> 00:11:59,830
different types of message passing applied to different nodes based on their color.

224
00:11:59,830 --> 00:12:02,260
So it means that in an IDGNN,

225
00:12:02,260 --> 00:12:04,150
we are going to use, um,

226
00:12:04,150 --> 00:12:06,310
different message and aggregation, uh,

227
00:12:06,310 --> 00:12:09,400
functions, uh, for nodes with different, uh, colors.

228
00:12:09,400 --> 00:12:11,170
So it means that for example,

229
00:12:11,170 --> 00:12:14,290
whenever we are aggregating into a color node,

230
00:12:14,290 --> 00:12:16,600
we are going to use one type of, uh,

231
00:12:16,600 --> 00:12:19,360
transformations and message passing operator

232
00:12:19,360 --> 00:12:22,360
and whenever we are aggregating into a non-colored node,

233
00:12:22,360 --> 00:12:24,895
we are going to use a second type of,

234
00:12:24,895 --> 00:12:27,205
uh, aggregation and transformation.

235
00:12:27,205 --> 00:12:29,470
So this means that in a given layer,

236
00:12:29,470 --> 00:12:31,810
different message and aggregation, uh,

237
00:12:31,810 --> 00:12:35,050
functions would be used to nodes based on the color,

238
00:12:35,050 --> 00:12:37,720
uh, of the node and this is the key, right?

239
00:12:37,720 --> 00:12:40,450
Because if the node- nodes is color and it

240
00:12:40,450 --> 00:12:43,360
can use a different aggregator, this means that,

241
00:12:43,360 --> 00:12:48,310
uh- that the- the- the message will get transformed differently,

242
00:12:48,310 --> 00:12:52,450
which means the final results will be different depending on whether

243
00:12:52,450 --> 00:12:57,340
the nodes with colors were involved in aggregation versus nodes without colors,

244
00:12:57,340 --> 00:13:00,880
uh, uh, being involved in aggregation.

245
00:13:00,880 --> 00:13:05,770
So um, you know why does this heterogeneous message-passing work?

246
00:13:05,770 --> 00:13:07,720
Right? Suppose that two nodes,

247
00:13:07,720 --> 00:13:11,215
V_1 and V_2 have the same computational graph structure,

248
00:13:11,215 --> 00:13:13,975
but they have different node coloring, right?

249
00:13:13,975 --> 00:13:18,070
Um, and since we apply different neural network embedding computation,

250
00:13:18,070 --> 00:13:20,185
right- different, um, uh,

251
00:13:20,185 --> 00:13:23,650
message passing and different aggregation, right,

252
00:13:23,650 --> 00:13:26,695
we have different parameters for nodes with one,

253
00:13:26,695 --> 00:13:29,245
uh, color versus the nodes with the other color,

254
00:13:29,245 --> 00:13:31,975
this means that the final result, uh,

255
00:13:31,975 --> 00:13:35,680
will be different and this means that the final output-

256
00:13:35,680 --> 00:13:40,690
the final embedding between V_1 and V_2 is going to be, uh, different.

257
00:13:40,690 --> 00:13:43,495
So, you know, what is the key, uh,

258
00:13:43,495 --> 00:13:47,815
difference between GNN and identity aware GNN?

259
00:13:47,815 --> 00:13:49,240
If we look at this,

260
00:13:49,240 --> 00:13:51,250
uh, you know- uh, use, uh,

261
00:13:51,250 --> 00:13:54,190
this case- uh, this example we have looked so far,

262
00:13:54,190 --> 00:13:56,860
if I have nodes V_1 and V_2 I wanna distinguish them,

263
00:13:56,860 --> 00:13:59,440
in the classical GNN computation,

264
00:13:59,440 --> 00:14:01,975
the two computation graphs are the same,

265
00:14:01,975 --> 00:14:03,940
uh, all the nodes are the same,

266
00:14:03,940 --> 00:14:06,085
there is no differentiating node features,

267
00:14:06,085 --> 00:14:09,475
so the aggregations across these two, uh,

268
00:14:09,475 --> 00:14:11,110
trees will be the same,

269
00:14:11,110 --> 00:14:13,405
so we won't be able to distinguish A and B.

270
00:14:13,405 --> 00:14:15,820
In the case when we actually color

271
00:14:15,820 --> 00:14:20,380
the starting node and now the two computation graphs are different,

272
00:14:20,380 --> 00:14:23,200
so all we have to account for now that- is

273
00:14:23,200 --> 00:14:26,740
that when we aggregate the information from the- um, uh,

274
00:14:26,740 --> 00:14:29,290
from the leaves to the root of

275
00:14:29,290 --> 00:14:34,405
the graph neural network that this information about the color is preserved

276
00:14:34,405 --> 00:14:38,230
or somehow accounted for so that the final message will have

277
00:14:38,230 --> 00:14:42,070
a different value depending on one, uh, versus the other.

278
00:14:42,070 --> 00:14:43,419
And this is exactly,

279
00:14:43,419 --> 00:14:46,150
um, the case what is happening,

280
00:14:46,150 --> 00:14:49,090
and this is why IDGNN allows us,

281
00:14:49,090 --> 00:14:51,565
uh, to make this distinction.

282
00:14:51,565 --> 00:14:53,995
Um, another thing to think about it,

283
00:14:53,995 --> 00:14:56,740
what is G- IDGNN really doing?

284
00:14:56,740 --> 00:15:01,840
IDGNN is really counting cycles of different lengths,

285
00:15:01,840 --> 00:15:04,000
uh, uh starting at a given,

286
00:15:04,000 --> 00:15:05,440
uh, given the root node, right?

287
00:15:05,440 --> 00:15:06,655
So if I start here,

288
00:15:06,655 --> 00:15:12,820
IDGNN will now able to count or realize that there is a cycle of length 3, right?

289
00:15:12,820 --> 00:15:15,070
So here- this is now basically a cycle.

290
00:15:15,070 --> 00:15:17,860
You get off- you go from yourself to the- uh,

291
00:15:17,860 --> 00:15:19,990
to the neigh- to the neighbor, uh,

292
00:15:19,990 --> 00:15:22,015
and then to another neighbor,

293
00:15:22,015 --> 00:15:23,470
and you'll come back to the node, right?

294
00:15:23,470 --> 00:15:25,360
So this is a cycle of three hops.

295
00:15:25,360 --> 00:15:28,300
While- while in this case you- we- we'll

296
00:15:28,300 --> 00:15:32,500
reali- graph neural network is going to realize that this is a cycle of length 4,

297
00:15:32,500 --> 00:15:34,315
because you have to go to the node,

298
00:15:34,315 --> 00:15:36,070
to the neighbor, um,

299
00:15:36,070 --> 00:15:38,140
to the first neighbor, to the second neighbor,

300
00:15:38,140 --> 00:15:40,150
to the third neighbor, and from here,

301
00:15:40,150 --> 00:15:41,920
you only arrive, uh,

302
00:15:41,920 --> 00:15:43,930
to the starting node itself, right?

303
00:15:43,930 --> 00:15:48,160
So here we'll real- the- the computational graph will be able to capture

304
00:15:48,160 --> 00:15:53,425
that- or be able to compute that- that node is part of a cycle of length,

305
00:15:53,425 --> 00:15:56,305
uh, 4, but no cycles of length 3.

306
00:15:56,305 --> 00:15:59,080
While here, these will be able to capture that

307
00:15:59,080 --> 00:16:02,620
the node is a part of cycle of length o- uh,

308
00:16:02,620 --> 00:16:05,530
3 but not, uh, let's say 4.

309
00:16:05,530 --> 00:16:08,545
So this is what IG- IDGNN is able to do.

310
00:16:08,545 --> 00:16:14,020
It's able to count cycles and it's able to learn and count- count them through the,

311
00:16:14,020 --> 00:16:18,160
uh, uh, message passing of the graph neural network.

312
00:16:18,160 --> 00:16:21,400
So, um, how do you now,

313
00:16:21,400 --> 00:16:23,830
uh, uh, how do you do this now?

314
00:16:23,830 --> 00:16:25,705
So as I said, one is to use,

315
00:16:25,705 --> 00:16:27,820
uh, heterogeneous message passing.

316
00:16:27,820 --> 00:16:31,885
Um, and, uh, and the second way how you can do this is that,

317
00:16:31,885 --> 00:16:35,695
um, we can- based on the intuition we have, uh, just proposed,

318
00:16:35,695 --> 00:16:39,055
you can also use a simplified version of the IDGNN,

319
00:16:39,055 --> 00:16:42,145
where basically the idea is to include identity information

320
00:16:42,145 --> 00:16:45,640
as an augmented node feature, um, and, uh,

321
00:16:45,640 --> 00:16:49,915
sidestep the- the heterogenous node, uh, uh,

322
00:16:49,915 --> 00:16:54,940
message passing and the idea here is basically you can augment the node feature,

323
00:16:54,940 --> 00:16:57,520
by using the cycle counts in each layer

324
00:16:57,520 --> 00:17:00,630
as an- as an augmented node feature and then apply,

325
00:17:00,630 --> 00:17:02,770
a simple GNN, right?

326
00:17:02,770 --> 00:17:06,500
So basically, you want to use cycle counts in each layer as

327
00:17:06,500 --> 00:17:11,240
an augmented feature for the root node and then simply apply het- uh,

328
00:17:11,240 --> 00:17:15,440
homogeneous message-passing, basically drop the colors, right?

329
00:17:15,440 --> 00:17:18,260
So the idea would be that every node gets now,

330
00:17:18,260 --> 00:17:22,625
um, ah, uh, gets a description that simply says,

331
00:17:22,625 --> 00:17:25,525
how many cycles of length 0 are you part of,

332
00:17:25,525 --> 00:17:28,920
how many cycles of length 2- like- cycles of length 3s,

333
00:17:28,920 --> 00:17:30,330
and, uh, and so on.

334
00:17:30,330 --> 00:17:33,740
And this way, you will be able to, um, uh, uh,

335
00:17:33,740 --> 00:17:36,410
to distinguish the node- uh,

336
00:17:36,410 --> 00:17:38,225
the two computational graphs,

337
00:17:38,225 --> 00:17:40,445
and be able to distinguish the two nodes, uh,

338
00:17:40,445 --> 00:17:43,755
into two different, uh, classes.

339
00:17:43,755 --> 00:17:48,440
So let's summarize the identity aware graph neural networks.

340
00:17:48,440 --> 00:17:53,985
Uh, the- this is a general and powerful extension to graph a neural network framework.

341
00:17:53,985 --> 00:17:57,100
Um, it makes graph neural networks more expressive.

342
00:17:57,100 --> 00:18:01,280
Uh, the IDGNN, this idea of inductive node coloring and

343
00:18:01,280 --> 00:18:06,150
heterogeneous message passing can be applied to any graph neural network architecture,

344
00:18:06,150 --> 00:18:09,815
meaning, um, gra- graph convolutional neural network,

345
00:18:09,815 --> 00:18:11,870
GraphSAGE, um, GIN,,

346
00:18:11,870 --> 00:18:15,590
so graph isomorphism network and- and any other, uh, architecture.

347
00:18:15,590 --> 00:18:22,835
Um, and IDGNN will provide a consistent performance gain in many node-level, um, er,

348
00:18:22,835 --> 00:18:24,860
as well as edge and graph-level tasks,

349
00:18:24,860 --> 00:18:28,100
because it allows us to break the symmetries and allows

350
00:18:28,100 --> 00:18:31,330
us to the- basically identify how the node,

351
00:18:31,330 --> 00:18:34,070
uh, belongs to different, uh, cycles.

352
00:18:34,070 --> 00:18:37,920
This means that IDGNNs are more expressive than their, uh,

353
00:18:37,920 --> 00:18:42,455
graph- kind of classical graph neural network, uh, counterparts.

354
00:18:42,455 --> 00:18:46,810
Um, and this means that IDGNN is kind of this- the, uh,

355
00:18:46,810 --> 00:18:52,270
uh, the simplest model that is more expressive than 1-WL test.

356
00:18:52,270 --> 00:18:57,950
Um, and it can be easily implemented because it's basically just you color the root node,

357
00:18:57,950 --> 00:18:59,630
and that's all the information,

358
00:18:59,630 --> 00:19:01,700
uh, you need to, uh, worry about.

359
00:19:01,700 --> 00:19:04,820
So, uh, this is quite cool because it allows us now to

360
00:19:04,820 --> 00:19:08,580
distinguish this node on a triangle versus node on a square.

361
00:19:08,580 --> 00:19:13,040
Um, this was the first and the key idea is here to have this inductive node coloring,

362
00:19:13,040 --> 00:19:16,940
and then we also talked about position aware graph neural networks,

363
00:19:16,940 --> 00:19:20,330
where the idea is that you want to distinguish the position of the node in the graph,

364
00:19:20,330 --> 00:19:23,315
and the key idea there was to use the notion of

365
00:19:23,315 --> 00:19:27,515
anchors and characterize the location- the position of the node,

366
00:19:27,515 --> 00:19:29,440
by the location, uh,

367
00:19:29,440 --> 00:19:31,775
by the distance of the node to the anchors.

368
00:19:31,775 --> 00:19:36,244
And we talked about we want to have anchors- anchors of different sizes.

369
00:19:36,244 --> 00:19:39,050
And we wanna have a lot of anchors of size 1,

370
00:19:39,050 --> 00:19:40,480
we wanna have a lot- uh,

371
00:19:40,480 --> 00:19:42,085
fewer anchors of size 2,

372
00:19:42,085 --> 00:19:44,930
even fewer of size 4 and so on and so forth.

373
00:19:44,930 --> 00:19:48,695
Um, and the- the distance of a node to the anchor

374
00:19:48,695 --> 00:19:52,610
is the distance of the node to the- any node that is part of this,

375
00:19:52,610 --> 00:19:54,650
uh, anchor or, uh, anchor set.

376
00:19:54,650 --> 00:20:02,000
[NOISE]

