1
00:00:04,610 --> 00:00:08,280
Next, we are going to talk about deep learning for

2
00:00:08,280 --> 00:00:10,995
graphs and in particular, graph neural networks.

3
00:00:10,995 --> 00:00:14,475
So now that we have kind of refreshed our notion of,

4
00:00:14,475 --> 00:00:16,020
uh, how general, uh,

5
00:00:16,020 --> 00:00:18,660
neural networks- uh, deep neural networks work,

6
00:00:18,660 --> 00:00:23,160
let's now go and generalize neural networks so that they can be applicable to graphs.

7
00:00:23,160 --> 00:00:25,515
That's what we are going to do next.

8
00:00:25,515 --> 00:00:32,360
So, uh, the content is that we are going to talk about local network neighborhoods.

9
00:00:32,360 --> 00:00:34,175
And we are then going to describe

10
00:00:34,175 --> 00:00:39,155
aggregation strategies and define what is called, uh, computation graphs.

11
00:00:39,155 --> 00:00:41,840
And then we are going to talk about how do we

12
00:00:41,840 --> 00:00:45,860
stack multiple layers of these neural networks, uh,

13
00:00:45,860 --> 00:00:51,770
to talk about how do we describe the model parameters training- how do we fit the model,

14
00:00:51,770 --> 00:00:54,410
and how do we se- how do we- and give

15
00:00:54,410 --> 00:00:57,840
a simple example for unsupervised and supervised training.

16
00:00:57,840 --> 00:01:01,700
So this is what we are going to talk about and what we are going to learn,

17
00:01:01,700 --> 00:01:04,390
uh, in this part, uh, of the lecture.

18
00:01:04,390 --> 00:01:07,440
So the setup is as follows.

19
00:01:07,440 --> 00:01:12,830
Um, we are going to assume we have a graph G that has a set of vertices,

20
00:01:12,830 --> 00:01:13,940
a set of nodes.

21
00:01:13,940 --> 00:01:15,945
It has an adjacency matrix.

22
00:01:15,945 --> 00:01:17,510
Right now let's assume it's binary,

23
00:01:17,510 --> 00:01:18,980
so means unweighted graph.

24
00:01:18,980 --> 00:01:22,190
Let's also, for simplicity, assume it's undirected,

25
00:01:22,190 --> 00:01:25,985
but, uh, everything will generalize to directed graphs as well.

26
00:01:25,985 --> 00:01:28,845
And let's assume that every node, uh,

27
00:01:28,845 --> 00:01:30,495
also has a para- uh,

28
00:01:30,495 --> 00:01:34,340
a node feature vector X associated with it, right?

29
00:01:34,340 --> 00:01:36,240
So, um, you know,

30
00:01:36,240 --> 00:01:37,380
what are node features?

31
00:01:37,380 --> 00:01:38,730
For example, in social network,

32
00:01:38,730 --> 00:01:40,050
this could be user profile,

33
00:01:40,050 --> 00:01:42,210
user image, a user age.

34
00:01:42,210 --> 00:01:45,230
In biological network, it could be a gene expression profile,

35
00:01:45,230 --> 00:01:47,510
uh, gene functional information.

36
00:01:47,510 --> 00:01:49,760
And, for example, if there is no, um,

37
00:01:49,760 --> 00:01:53,415
node feature information the- in the dataset, um,

38
00:01:53,415 --> 00:01:56,750
what people like to do is either use indicator vectors,

39
00:01:56,750 --> 00:01:58,820
so one-hot encodings of nodes,

40
00:01:58,820 --> 00:02:02,395
or just a vector of all constants, uh, value 1.

41
00:02:02,395 --> 00:02:05,030
Uh, those are two, uh, popular choices.

42
00:02:05,030 --> 00:02:07,450
Sometimes people also use a degree, uh,

43
00:02:07,450 --> 00:02:11,305
as fe- a node degree as a feature, uh, of the node.

44
00:02:11,305 --> 00:02:13,460
And then another piece of notation,

45
00:02:13,460 --> 00:02:18,920
we are going to- to denote N of v to be a set of neighbors of a given node,

46
00:02:18,920 --> 00:02:22,405
uh, v. And that's basically the notation,

47
00:02:22,405 --> 00:02:24,800
the setup, uh, we are going to use,

48
00:02:24,800 --> 00:02:27,490
uh, for this part of the lecture.

49
00:02:27,490 --> 00:02:29,540
Now, if you say,

50
00:02:29,540 --> 00:02:32,690
how could I go and apply deep neural networks to graphs?

51
00:02:32,690 --> 00:02:35,690
Here is a naive, uh, simple approach.

52
00:02:35,690 --> 00:02:37,610
The idea you could have is to say,

53
00:02:37,610 --> 00:02:40,970
why don't I represent a network with adjacency matrix?

54
00:02:40,970 --> 00:02:45,560
And then why don't I append the node features to the adjacency matrix?

55
00:02:45,560 --> 00:02:51,210
Now think of this as a training example and feed it through a deep neural network, right?

56
00:02:51,210 --> 00:02:52,485
It seems very natural.

57
00:02:52,485 --> 00:02:53,775
I take my network,

58
00:02:53,775 --> 00:02:56,070
represent it as an adjacency matrix,

59
00:02:56,070 --> 00:02:58,195
I append node features to it.

60
00:02:58,195 --> 00:02:59,240
Now, this is, you know,

61
00:02:59,240 --> 00:03:01,350
the input to the neural network,

62
00:03:01,350 --> 00:03:03,910
um, and I'm able to make predictions.

63
00:03:03,910 --> 00:03:06,380
Uh, the issue with this idea is several.

64
00:03:06,380 --> 00:03:08,750
First is that the number of parameters of

65
00:03:08,750 --> 00:03:13,130
this neural network will be multiple times the number of nodes in the network.

66
00:03:13,130 --> 00:03:17,435
Because number of inputs here is the number of nodes plus the number of features.

67
00:03:17,435 --> 00:03:20,495
And if you say how- what is the- how many training examples I have,

68
00:03:20,495 --> 00:03:22,670
I have one training example per node.

69
00:03:22,670 --> 00:03:26,090
So it means you'll have more parameters than you have training examples

70
00:03:26,090 --> 00:03:30,070
and training will be very unstable and it will easily overfit.

71
00:03:30,070 --> 00:03:32,840
Another issue with this is that this model

72
00:03:32,840 --> 00:03:35,614
won't be applicable to graphs of different sizes.

73
00:03:35,614 --> 00:03:39,670
Because if now I have a graph that has a height of seven nodes,

74
00:03:39,670 --> 00:03:46,160
it's unclear how to fit a graph of seven nodes into five different inputs, right?

75
00:03:46,160 --> 00:03:48,715
Because here we have a graph of five nodes.

76
00:03:48,715 --> 00:03:50,475
So that's one, uh,

77
00:03:50,475 --> 00:03:52,895
big- uh, big issue.

78
00:03:52,895 --> 00:03:55,700
And another big issue is subtle but very

79
00:03:55,700 --> 00:03:59,275
important is that this approach will be sensitive to know node ordering.

80
00:03:59,275 --> 00:04:03,000
Meaning right now I number nodes as A, B,

81
00:04:03,000 --> 00:04:08,280
C, uh, D, and E in the following order so my adjacency matrix was like this.

82
00:04:08,280 --> 00:04:13,055
But now if I were to call for example node E- to call it A, B,

83
00:04:13,055 --> 00:04:14,620
C, D, and E,

84
00:04:14,620 --> 00:04:18,320
then the shape of my adjacency matrix would change, right?

85
00:04:18,320 --> 00:04:20,899
The nodes, uh, would be permuted.

86
00:04:20,899 --> 00:04:23,420
The rows and columns of the matrix will be permuted even

87
00:04:23,420 --> 00:04:26,245
though the information is still the same.

88
00:04:26,245 --> 00:04:27,510
And in that case,

89
00:04:27,510 --> 00:04:32,525
the mind would get totally confused and wouldn't know what to- what to output.

90
00:04:32,525 --> 00:04:36,080
So the point is that in graphs there is no fixed node ordering so

91
00:04:36,080 --> 00:04:40,010
it's unclear how to sort the nodes of the graph that you could,

92
00:04:40,010 --> 00:04:44,145
um, you know, put them as inputs in the matrix.

93
00:04:44,145 --> 00:04:48,245
That's much easier in images because you say I'll start with the top-left pixel,

94
00:04:48,245 --> 00:04:51,085
and then I- I'll kind of go line-by-line.

95
00:04:51,085 --> 00:04:52,545
In- in the graph,

96
00:04:52,545 --> 00:04:57,424
the same graph can represented by- in- by many different adjacency matrices,

97
00:04:57,424 --> 00:05:00,320
because it all depends on the ordering or- uh,

98
00:05:00,320 --> 00:05:03,935
in- in which you labeled or numbered the nodes.

99
00:05:03,935 --> 00:05:07,235
So we have to be invariant to node ordering.

100
00:05:07,235 --> 00:05:12,005
So the idea of what we are going to do is we're going to kind of,

101
00:05:12,005 --> 00:05:15,470
take- borrow some intuition from convolutional neural networks,

102
00:05:15,470 --> 00:05:19,220
from computer vision, but generalize them to graphs.

103
00:05:19,220 --> 00:05:23,565
So here's a quick idea about what convolutional neural networks do.

104
00:05:23,565 --> 00:05:25,650
Like if you have an image here,

105
00:05:25,650 --> 00:05:27,525
you know, represented as this grid,

106
00:05:27,525 --> 00:05:30,290
then you define this convolutional operator, basically,

107
00:05:30,290 --> 00:05:34,280
this sliding window that you are sliding over the image, right?

108
00:05:34,280 --> 00:05:35,650
You start at the top-left.

109
00:05:35,650 --> 00:05:37,905
This is now a three-by-three operator.

110
00:05:37,905 --> 00:05:39,530
You compute something over this,

111
00:05:39,530 --> 00:05:40,845
uh, area of the image,

112
00:05:40,845 --> 00:05:43,820
and then you slide the operator by, you know,

113
00:05:43,820 --> 00:05:48,025
some number of steps to the right and apply the same operator again.

114
00:05:48,025 --> 00:05:51,050
And- and you can keep doing this and you can imagine that now this will give

115
00:05:51,050 --> 00:05:55,470
you a new image of different size, of different- uh, um, uh,

116
00:05:55,940 --> 00:05:59,480
different number of rows and columns to which you can

117
00:05:59,480 --> 00:06:02,075
apply another, uh, convolutional operator,

118
00:06:02,075 --> 00:06:06,320
another kind of sliding window type operator that kind of goes over the rows,

119
00:06:06,320 --> 00:06:08,340
uh, uh, of that- of that image.

120
00:06:08,340 --> 00:06:09,840
And if you just chain this, uh,

121
00:06:09,840 --> 00:06:12,240
together, uh, you can then, uh,

122
00:06:12,240 --> 00:06:16,570
come up, uh, with convolutional neural networks and, uh, good predictions.

123
00:06:16,570 --> 00:06:21,860
Our goal here is to generalize this notion of convolution between simple lattices,

124
00:06:21,860 --> 00:06:24,440
between simple- uh, beyond simple matri- uh,

125
00:06:24,440 --> 00:06:28,555
matrices, uh, and also leverage node features and attributes.

126
00:06:28,555 --> 00:06:33,455
Like, for example, text or images that might be attached to the nodes of the network.

127
00:06:33,455 --> 00:06:37,505
The issue is that our networks are much more complex.

128
00:06:37,505 --> 00:06:40,610
So defining the node- the notion of a sliding window,

129
00:06:40,610 --> 00:06:41,735
let's- let's say, uh,

130
00:06:41,735 --> 00:06:43,390
you know, a three-by-three window,

131
00:06:43,390 --> 00:06:46,940
it's- is- is very strange because maybe in some case, you know,

132
00:06:46,940 --> 00:06:49,505
the sliding window may only cover three nodes in other-

133
00:06:49,505 --> 00:06:52,205
in other case the sliding window may cover many more nodes.

134
00:06:52,205 --> 00:06:56,730
And it's unclear how to define this notion of a window,

135
00:06:56,730 --> 00:07:01,765
and then it's also unclear how to define the notion of sliding the window over the graph.

136
00:07:01,765 --> 00:07:03,160
And this is a big, uh,

137
00:07:03,160 --> 00:07:06,215
complexity and a big challenge that, uh,

138
00:07:06,215 --> 00:07:08,660
graph neural networks have to do, uh,

139
00:07:08,660 --> 00:07:12,025
to be able to be applied to complex network data.

140
00:07:12,025 --> 00:07:14,940
So the idea that makes this,

141
00:07:14,940 --> 00:07:17,630
uh, work is the following step in intuition.

142
00:07:17,630 --> 00:07:22,070
So the idea is that single convolutional- single layer of a convolutional,

143
00:07:22,070 --> 00:07:24,840
uh, neural network, basically what it does it,

144
00:07:24,840 --> 00:07:27,030
for example, takes- uh, takes the, uh,

145
00:07:27,030 --> 00:07:28,875
area of three-by-three pixels,

146
00:07:28,875 --> 00:07:32,900
apply some transformation to them and creates a new pixel.

147
00:07:32,900 --> 00:07:35,390
And that's the way you can think of it, right?

148
00:07:35,390 --> 00:07:39,905
And now we can take this operator and slide it across the image.

149
00:07:39,905 --> 00:07:43,760
What we'd like to do in the graph is something similar, but, you know,

150
00:07:43,760 --> 00:07:47,435
if you wanna apply this operator in terms of a- uh,

151
00:07:47,435 --> 00:07:48,995
in terms of a, uh,

152
00:07:48,995 --> 00:07:50,585
let's say like a sliding window,

153
00:07:50,585 --> 00:07:53,665
then we will have a center of the sliding window, which is a node,

154
00:07:53,665 --> 00:07:55,980
and this center is going to kind of borrow,

155
00:07:55,980 --> 00:07:59,390
uh, aggregate information from its neighbors, right?

156
00:07:59,390 --> 00:08:03,080
The same way here you can imagine that this is the center of the operator and

157
00:08:03,080 --> 00:08:06,810
it's kind of collecting information from- from its neighbors,

158
00:08:06,810 --> 00:08:12,560
denoted by arrows, takes its own value as well and creates a new value,

159
00:08:12,560 --> 00:08:14,875
a new pixel, uh, for itself.

160
00:08:14,875 --> 00:08:19,595
So the idea is that- that what convolutional operators are certainly doing,

161
00:08:19,595 --> 00:08:23,030
they are transforming information from the neighbors,

162
00:08:23,030 --> 00:08:27,109
combining it, and creating a new kind of a message.

163
00:08:27,109 --> 00:08:31,700
So this is when today's lecture also relates to the,

164
00:08:31,700 --> 00:08:34,730
uh, last lecture when we talked about message passing, right?

165
00:08:34,730 --> 00:08:38,840
So today here we can think about a node collecting information from its neighbors,

166
00:08:38,840 --> 00:08:41,659
collecting messages from its neighbors, aggregating them,

167
00:08:41,659 --> 00:08:44,900
combining them, and creating a new message.

168
00:08:44,900 --> 00:08:48,410
So, uh, that is the- that is the idea.

169
00:08:48,410 --> 00:08:53,245
So how graph convolutional neural networks are going to work.

170
00:08:53,245 --> 00:08:58,910
Um, the idea is that node's neighborhood defines the neural network architecture.

171
00:08:58,910 --> 00:09:02,300
So basically the structure of the graph around a node of

172
00:09:02,300 --> 00:09:06,790
interest defines the structure of the neural network.

173
00:09:06,790 --> 00:09:08,370
Um, so the idea is,

174
00:09:08,370 --> 00:09:11,175
if i wanna make a prediction for this red node,

175
00:09:11,175 --> 00:09:13,310
I, here, uh, in the network,

176
00:09:13,310 --> 00:09:18,680
then the way we can think of this is that i is going to take information,

177
00:09:18,680 --> 00:09:21,290
uh, from its neighbors and neighbors are

178
00:09:21,290 --> 00:09:23,830
going to take information from neighbors of neighbors.

179
00:09:23,830 --> 00:09:27,350
And we are going to learn how to propagate this information,

180
00:09:27,350 --> 00:09:29,885
how to transform it along the edges of the network,

181
00:09:29,885 --> 00:09:32,030
how to aggregate the heat,

182
00:09:32,030 --> 00:09:37,775
and how to create a new message that then the next node up the chain can again aggregate,

183
00:09:37,775 --> 00:09:40,460
transform, um, and- and compute.

184
00:09:40,460 --> 00:09:42,260
So in some sense,

185
00:09:42,260 --> 00:09:45,845
the way we can think of graph neural networks is a two-step process.

186
00:09:45,845 --> 00:09:50,015
In the first step process we determine the node computation graph.

187
00:09:50,015 --> 00:09:51,500
And in the second process,

188
00:09:51,500 --> 00:09:53,510
we then propagate, um,

189
00:09:53,510 --> 00:09:55,430
the- the- the information,

190
00:09:55,430 --> 00:09:59,405
we propagate and transform it over this computation graph.

191
00:09:59,405 --> 00:10:05,930
And this computation graph defines the architecture or the structure of the underlying,

192
00:10:05,930 --> 00:10:07,850
uh, neural network, right?

193
00:10:07,850 --> 00:10:09,980
So, um, in this way,

194
00:10:09,980 --> 00:10:12,080
we learn how to propagate information across

195
00:10:12,080 --> 00:10:15,730
the graph structure to compute node features or,

196
00:10:15,730 --> 00:10:17,615
uh, node, uh, embeddings.

197
00:10:17,615 --> 00:10:19,635
So that's the intuition.

198
00:10:19,635 --> 00:10:22,915
Let me give you an example of what I mean by this.

199
00:10:22,915 --> 00:10:24,790
Consider here a very small,

200
00:10:24,790 --> 00:10:27,445
um, input graph on six nodes.

201
00:10:27,445 --> 00:10:30,250
Um, and what we will want to do is,

202
00:10:30,250 --> 00:10:34,450
the key idea would be that you want to generate base- node embeddings based on

203
00:10:34,450 --> 00:10:39,130
the local structure of the neighborhood around that target node.

204
00:10:39,130 --> 00:10:43,405
So for this input graph and this is the target node, uh,

205
00:10:43,405 --> 00:10:46,870
here is the structure of the neural network that is going to

206
00:10:46,870 --> 00:10:50,485
make computation to be able to make a prediction for node A.

207
00:10:50,485 --> 00:10:54,535
And let me explain you why is this neural network has this structure.

208
00:10:54,535 --> 00:10:57,460
The reason is that node A is going to

209
00:10:57,460 --> 00:11:01,000
take information from its neighbors in the network; B, C,

210
00:11:01,000 --> 00:11:02,830
and D. So here are B, C,

211
00:11:02,830 --> 00:11:05,395
and D. And then of course,

212
00:11:05,395 --> 00:11:07,750
this is kind of one layer of computation,

213
00:11:07,750 --> 00:11:10,300
but we can unfold this for multiple layers.

214
00:11:10,300 --> 00:11:12,550
So if we unfold this for one more layer,

215
00:11:12,550 --> 00:11:16,540
then node D takes information from its neighbor A.

216
00:11:16,540 --> 00:11:18,850
And that's why we have this edge here.

217
00:11:18,850 --> 00:11:20,680
Node C, for example,

218
00:11:20,680 --> 00:11:23,500
takes information from its neighbors A, B,

219
00:11:23,500 --> 00:11:26,320
uh, E, and F. They are- they are here; A, B,

220
00:11:26,320 --> 00:11:31,405
and F. And then D takes information from A and C because it's connected to nodes,

221
00:11:31,405 --> 00:11:33,205
uh, A and C. So now,

222
00:11:33,205 --> 00:11:36,340
what does this mean is that if this is the structure of the, uh,

223
00:11:36,340 --> 00:11:40,000
graph neural network, now what is- what we have to define,

224
00:11:40,000 --> 00:11:42,340
what we have to learn is we have to learn

225
00:11:42,340 --> 00:11:48,730
the message transformation operators along the edges as well as the aggregation operator.

226
00:11:48,730 --> 00:11:50,710
Say, because node B says,

227
00:11:50,710 --> 00:11:52,465
"I will collect the message from A,

228
00:11:52,465 --> 00:11:56,830
I will collect the message from C. These messages will be transformed,

229
00:11:56,830 --> 00:11:59,739
aggregated together in a single message,

230
00:11:59,739 --> 00:12:01,885
and then I'm going to pass it on."

231
00:12:01,885 --> 00:12:04,375
So that now node A can again say,

232
00:12:04,375 --> 00:12:05,620
"I'll take message from B,

233
00:12:05,620 --> 00:12:07,060
I will transform it.

234
00:12:07,060 --> 00:12:08,650
I will take a message from C,

235
00:12:08,650 --> 00:12:11,140
transform it, message from D, transform it.

236
00:12:11,140 --> 00:12:14,710
Now I am going to aggregate these three messages into the new message and

237
00:12:14,710 --> 00:12:18,715
pass it on to whoever is kind of in the layer, uh, above me."

238
00:12:18,715 --> 00:12:20,965
So that is essentially the idea.

239
00:12:20,965 --> 00:12:24,205
And of course, these transformations here,

240
00:12:24,205 --> 00:12:28,060
uh, uh, aggregations and transformations will be learned.

241
00:12:28,060 --> 00:12:32,440
They will be parameterized and distributed parameters of our model.

242
00:12:32,440 --> 00:12:38,485
What is interesting and fundamentally different from classical neural networks,

243
00:12:38,485 --> 00:12:44,455
is that every node gets to define its own neural network architecture.

244
00:12:44,455 --> 00:12:46,525
Or every node gets to define

245
00:12:46,525 --> 00:12:52,055
its own computation graph based on the structure of the network around it.

246
00:12:52,055 --> 00:12:53,820
So what this means is, for example,

247
00:12:53,820 --> 00:12:55,020
that, uh, blue node,

248
00:12:55,020 --> 00:12:58,395
D here, its computation graph will be like this,

249
00:12:58,395 --> 00:13:01,530
will very skinny because D takes information from A,

250
00:13:01,530 --> 00:13:05,080
and A takes it from B and C. So it's a- you know,

251
00:13:05,080 --> 00:13:07,030
this is now a two-layer neural network,

252
00:13:07,030 --> 00:13:09,115
but it's very skinny, very narrow.

253
00:13:09,115 --> 00:13:12,730
While for example, node C has a much bigger,

254
00:13:12,730 --> 00:13:14,110
much wider neural network,

255
00:13:14,110 --> 00:13:18,040
because C collects information from- from its four neighbors and

256
00:13:18,040 --> 00:13:22,165
then each of the neighbors collect it from its own set of neighbors.

257
00:13:22,165 --> 00:13:26,725
So that architecture, structure of this green neural network corresponding to nodes,

258
00:13:26,725 --> 00:13:31,210
target node C, is very different than the one from the node, uh,

259
00:13:31,210 --> 00:13:34,150
D. So what is interesting,

260
00:13:34,150 --> 00:13:36,025
um, conceptually is that now,

261
00:13:36,025 --> 00:13:38,950
every node has its own, uh,

262
00:13:38,950 --> 00:13:42,835
computational graph or it has its own, um, architecture.

263
00:13:42,835 --> 00:13:47,725
Um, some nodes, if the neighborhood of the network around them is similar,

264
00:13:47,725 --> 00:13:51,700
for example, these two nodes will have the same computation graphs.

265
00:13:51,700 --> 00:13:53,050
Like this is E and F,

266
00:13:53,050 --> 00:13:55,480
you see the kind of the structure of these computation graphs,

267
00:13:55,480 --> 00:13:59,500
these neural network architecture trees, uh, is the same.

268
00:13:59,500 --> 00:14:04,450
But in principle, every node can have its own computation graph, that's first thing.

269
00:14:04,450 --> 00:14:06,055
And then the second thing that's interesting,

270
00:14:06,055 --> 00:14:11,560
now we are going to train or learn over multiple architectures simultaneously, right?

271
00:14:11,560 --> 00:14:14,815
So it's not that we have one neural network on which we train

272
00:14:14,815 --> 00:14:18,670
now every node comes with its own neural network,

273
00:14:18,670 --> 00:14:20,680
uh, architecture, neural network structure.

274
00:14:20,680 --> 00:14:23,830
And of course, the structure of the neural network for

275
00:14:23,830 --> 00:14:28,825
a given node depends on the structure of the network around this node,

276
00:14:28,825 --> 00:14:31,615
because this is how we determine the computation graph.

277
00:14:31,615 --> 00:14:35,035
And that's kind of a very important, er,

278
00:14:35,035 --> 00:14:37,930
kind of deep insight into how these things are

279
00:14:37,930 --> 00:14:41,785
different than kind of classical, uh, deep learning.

280
00:14:41,785 --> 00:14:46,600
So now, how do- how does this work as we have multiple layers, right?

281
00:14:46,600 --> 00:14:49,540
So the point is that the model can be of arbitrary depth.

282
00:14:49,540 --> 00:14:52,585
We can create an arbitrary number, uh, of, uh,

283
00:14:52,585 --> 00:14:56,815
layers and nodes have embeddings at each layer.

284
00:14:56,815 --> 00:15:00,175
Um, and the embedding at layer 0 of a given node

285
00:15:00,175 --> 00:15:04,120
is simply initialized as its input features X.

286
00:15:04,120 --> 00:15:07,150
And then the layer K embedding gets information

287
00:15:07,150 --> 00:15:10,390
from nodes that are kind of K hops away, right?

288
00:15:10,390 --> 00:15:12,055
So the way this would work is,

289
00:15:12,055 --> 00:15:16,210
layer 0 embedding for nodes is simply their feature vectors,

290
00:15:16,210 --> 00:15:18,040
so here, denoted by X.

291
00:15:18,040 --> 00:15:21,640
Then for example, embedding of, um,

292
00:15:21,640 --> 00:15:27,385
node B at layer 1 would be sum aggregation of feature- feature vectors of, ah,

293
00:15:27,385 --> 00:15:30,240
of neighbors, uh, A and C, uh,

294
00:15:30,240 --> 00:15:31,920
plus its own feature vector,

295
00:15:31,920 --> 00:15:35,100
and this is now embedding of this node at layer 1.

296
00:15:35,100 --> 00:15:37,290
And then this will be passed on, uh,

297
00:15:37,290 --> 00:15:41,825
so that now node 2 can- node A can compute its embedding at layer 2.

298
00:15:41,825 --> 00:15:45,970
So what it means is that the embedding of A at layer 0

299
00:15:45,970 --> 00:15:50,215
is different than its embedding at layer 1- sorry, at layer 2.

300
00:15:50,215 --> 00:15:51,460
So at every layer,

301
00:15:51,460 --> 00:15:53,800
a node will have a different, uh, embedding.

302
00:15:53,800 --> 00:15:59,005
And also, we are only going to run this for a limited number of steps.

303
00:15:59,005 --> 00:16:02,170
We are not going to run this kind of infinitely long or

304
00:16:02,170 --> 00:16:05,860
until it converges as- as we did in the last lecture.

305
00:16:05,860 --> 00:16:07,750
We don't have this notion of convergence.

306
00:16:07,750 --> 00:16:10,734
We'd only do this for a limited number of steps.

307
00:16:10,734 --> 00:16:14,110
Each step corresponds to one layer of the neural network,

308
00:16:14,110 --> 00:16:16,180
corresponds to one hop,

309
00:16:16,180 --> 00:16:18,100
uh, in the underlying network.

310
00:16:18,100 --> 00:16:22,900
So if we want to collect information from K hops away from the starting node,

311
00:16:22,900 --> 00:16:23,995
from the target node,

312
00:16:23,995 --> 00:16:27,490
we are going to need a K layer neural network.

313
00:16:27,490 --> 00:16:30,430
And because networks have final diameter,

314
00:16:30,430 --> 00:16:32,590
it makes no sense to talk about, I know,

315
00:16:32,590 --> 00:16:35,245
100 layer, uh, deep neural networks.

316
00:16:35,245 --> 00:16:39,190
Again, unless your network has diameter or that you know,

317
00:16:39,190 --> 00:16:43,465
the longest, shortest path is of 100 hops.

318
00:16:43,465 --> 00:16:46,570
So now that we have, uh,

319
00:16:46,570 --> 00:16:49,450
defined the notion of how do we create

320
00:16:49,450 --> 00:16:53,575
this computation graph based on the structure of the neighborhood around a given node,

321
00:16:53,575 --> 00:16:59,320
now we need to talk about these transformations that happen in the neural network.

322
00:16:59,320 --> 00:17:02,995
And the key concept is neighborhood aggregation.

323
00:17:02,995 --> 00:17:05,710
Um, and the key distinction between different approaches,

324
00:17:05,710 --> 00:17:09,280
different graph neural network architectures is how different,

325
00:17:09,280 --> 00:17:12,415
uh, how this aggregation, uh, is done.

326
00:17:12,415 --> 00:17:14,365
How this information from, uh,

327
00:17:14,365 --> 00:17:17,680
children nodes is aggregated and combined,

328
00:17:17,680 --> 00:17:20,425
uh, with the information or message of the pattern.

329
00:17:20,425 --> 00:17:24,099
One important thing to notice is because

330
00:17:24,099 --> 00:17:27,849
the ordering of the nodes in a graph is arbitrary;

331
00:17:27,849 --> 00:17:33,385
this means that we have to have our aggregation operator to be,

332
00:17:33,385 --> 00:17:35,845
um, uh, permutation invariant.

333
00:17:35,845 --> 00:17:39,895
Right? So it- it- it means that, um,

334
00:17:39,895 --> 00:17:44,950
we- we can order the nodes in any order and if we aggregate them,

335
00:17:44,950 --> 00:17:47,620
the aggregation will always be the same, right?

336
00:17:47,620 --> 00:17:48,655
That's the- that's the idea.

337
00:17:48,655 --> 00:17:50,710
It doesn't matter in what order we aggregate,

338
00:17:50,710 --> 00:17:55,705
we want the- the result to be always the same because it doesn't matter whether,

339
00:17:55,705 --> 00:17:59,455
you know, node B has neighbors A- A and C,

340
00:17:59,455 --> 00:18:01,045
or C and A,

341
00:18:01,045 --> 00:18:03,280
they are just- it's just a set of elements.

342
00:18:03,280 --> 00:18:05,575
So it doesn't matter whether we're aggregating, you know,

343
00:18:05,575 --> 00:18:09,715
in that sense from A- A and C or C and A.

344
00:18:09,715 --> 00:18:11,170
You should always get the same result.

345
00:18:11,170 --> 00:18:14,185
So neighborhood aggregation function has to be,

346
00:18:14,185 --> 00:18:18,055
uh, order invariant or permutation invariant.

347
00:18:18,055 --> 00:18:20,260
So now, of course,

348
00:18:20,260 --> 00:18:23,050
the question here is- that we haven't yet answered is,

349
00:18:23,050 --> 00:18:24,685
what is happening in these boxes?

350
00:18:24,685 --> 00:18:26,350
What do we put into these boxes?

351
00:18:26,350 --> 00:18:28,240
How do we define these transformations?

352
00:18:28,240 --> 00:18:30,880
How are they parameterized? How do we learn?

353
00:18:30,880 --> 00:18:34,210
So the basic approach is, for example,

354
00:18:34,210 --> 00:18:37,090
is to simply average information from the neighbors,

355
00:18:37,090 --> 00:18:38,965
uh, and apply a neural network.

356
00:18:38,965 --> 00:18:41,305
So average- so a summation,

357
00:18:41,305 --> 00:18:44,830
use permutation or that invariant because in any number,

358
00:18:44,830 --> 00:18:47,365
you sum up the- the- the numbers,

359
00:18:47,365 --> 00:18:48,970
in any way you sum up the numbers,

360
00:18:48,970 --> 00:18:50,665
you will always get the same result.

361
00:18:50,665 --> 00:18:53,710
So average or a summation is a,

362
00:18:53,710 --> 00:18:56,935
uh, permutation invariant aggregation function.

363
00:18:56,935 --> 00:19:00,820
So for example, the idea here is that every- every of

364
00:19:00,820 --> 00:19:04,390
these operators here will simply take the messages from the children,

365
00:19:04,390 --> 00:19:06,175
uh, and average node,

366
00:19:06,175 --> 00:19:10,570
and then decide to take this average and make a new message out of it.

367
00:19:10,570 --> 00:19:12,520
So the way we can, um,

368
00:19:12,520 --> 00:19:14,710
do this is do the, er,

369
00:19:14,710 --> 00:19:17,890
we aggregate the messages and then we apply our neural network,

370
00:19:17,890 --> 00:19:21,280
which means we apply some linear transformation followed by

371
00:19:21,280 --> 00:19:25,600
a non-linearity to create a next, uh, level message.

372
00:19:25,600 --> 00:19:27,905
So let me, uh, give you an example.

373
00:19:27,905 --> 00:19:33,510
The basic approach is that we want to average messages coming from the children,

374
00:19:33,510 --> 00:19:36,570
coming from the neighbors and apply a neural network to them.

375
00:19:36,570 --> 00:19:38,145
So here is how this- uh,

376
00:19:38,145 --> 00:19:40,775
how this looks like in equation. So let me explain.

377
00:19:40,775 --> 00:19:46,150
So first, write H is a- our embedding and, uh, subscript means,

378
00:19:46,150 --> 00:19:50,200
uh, the node and superscript denotes the neu- uh,

379
00:19:50,200 --> 00:19:51,835
the level of the neural network.

380
00:19:51,835 --> 00:19:54,595
So at the beginning are the zeroth layer,

381
00:19:54,595 --> 00:19:59,395
the embedding of the node V is simply its feature representation.

382
00:19:59,395 --> 00:20:05,590
And now we are going to create higher-order embeddings of nodes.

383
00:20:05,590 --> 00:20:09,010
So right so this is a level zero so at level one,

384
00:20:09,010 --> 00:20:12,880
what are we going to do is- is getting an equation and let me explain.

385
00:20:12,880 --> 00:20:14,395
So first we say,

386
00:20:14,395 --> 00:20:18,595
let's take the embeddings of the nodes from the previous uh,

387
00:20:18,595 --> 00:20:19,900
from the previous layer.

388
00:20:19,900 --> 00:20:23,410
So this is the embedding of this node v from the previous layer.

389
00:20:23,410 --> 00:20:28,510
And let's multiply, transform it with some matrix B.

390
00:20:28,510 --> 00:20:33,895
Then, what we are saying is let's also go over the neighbor's uh,

391
00:20:33,895 --> 00:20:42,070
u of our node of interest v. And let's take the previous level embedding of every node u,

392
00:20:42,070 --> 00:20:45,130
let's sum up these embeddings uh,

393
00:20:45,130 --> 00:20:48,355
and average them so this is the total number of neighbor's.

394
00:20:48,355 --> 00:20:51,955
And then let's transform this uh, uh,

395
00:20:51,955 --> 00:20:56,770
aggregated average of embeddings of ma- of uh, children,

396
00:20:56,770 --> 00:20:59,740
multiply it with another matrix and uh,

397
00:20:59,740 --> 00:21:02,260
send these through a non-linearity, right?

398
00:21:02,260 --> 00:21:06,835
So basically we say, I have my own message- my own embedding if I'm node v,

399
00:21:06,835 --> 00:21:10,225
my own embedding from the previous layer, transform it.

400
00:21:10,225 --> 00:21:13,060
I aggregate embeddings from my children,

401
00:21:13,060 --> 00:21:15,265
from my neighbor's from previous level,

402
00:21:15,265 --> 00:21:18,400
I multiply that to be the different transformation matrix.

403
00:21:18,400 --> 00:21:22,420
I add these two together and send it through a non-linearity.

404
00:21:22,420 --> 00:21:25,660
And this is a one layer of my neural network.

405
00:21:25,660 --> 00:21:29,890
And now of course I can run this or do this for several times, right?

406
00:21:29,890 --> 00:21:32,185
So this is now how to go from level l,

407
00:21:32,185 --> 00:21:34,645
to level l plus 1, um,

408
00:21:34,645 --> 00:21:38,125
you know to- to- to compute the first layer embeddings,

409
00:21:38,125 --> 00:21:40,300
I use the embeddings from level zero,

410
00:21:40,300 --> 00:21:41,815
which is just the node features.

411
00:21:41,815 --> 00:21:45,040
But now I can run this for several kinds of iterations.

412
00:21:45,040 --> 00:21:47,230
Lets say several- several layers,

413
00:21:47,230 --> 00:21:48,835
maybe five, six, 10.

414
00:21:48,835 --> 00:21:52,270
And then whatever is the final um, eh, um,

415
00:21:52,270 --> 00:21:55,285
hidden representation of the final layer uh,

416
00:21:55,285 --> 00:21:56,845
that is what I call uh,

417
00:21:56,845 --> 00:21:58,450
the embedding of the node, right?

418
00:21:58,450 --> 00:22:01,960
So we have our total number of capital L layers and

419
00:22:01,960 --> 00:22:06,640
the final embedding of the node is simply h of v at the final uh,

420
00:22:06,640 --> 00:22:11,530
at the final layer- level- at the final level of uh, neighborhood uh, aggregation.

421
00:22:11,530 --> 00:22:13,975
And this is what is called a deep encoder,

422
00:22:13,975 --> 00:22:16,600
because it is encoding information from

423
00:22:16,600 --> 00:22:20,215
the previous layer- lay- lay- layer from a given node,

424
00:22:20,215 --> 00:22:24,655
plus its neighbor's transforming it using matrices B and W,

425
00:22:24,655 --> 00:22:29,440
and then sending it through a non-linearity to obs- to obtain next level uh,

426
00:22:29,440 --> 00:22:30,610
representation of the node.

427
00:22:30,610 --> 00:22:32,395
And we can basically now do this uh,

428
00:22:32,395 --> 00:22:36,265
for several iterations, for several uh, layers.

429
00:22:36,265 --> 00:22:42,925
So how do we train the model if every node gets its own computational architecture,

430
00:22:42,925 --> 00:22:45,910
and every node has its own transformation uh,

431
00:22:45,910 --> 00:22:49,030
parameters, basically this W and B?

432
00:22:49,030 --> 00:22:53,680
The way we train the model is that we want to define what are the parameters of it?

433
00:22:53,680 --> 00:22:57,640
And parameters are this matrices W and B,

434
00:22:57,640 --> 00:23:01,435
and they are indexed by l. So for every layer we have a different W,

435
00:23:01,435 --> 00:23:04,540
and for every layer we have a different uh, B.

436
00:23:04,540 --> 00:23:07,015
And the idea here is that we can now uh,

437
00:23:07,015 --> 00:23:10,420
feed this embeddings into the loss function and we can

438
00:23:10,420 --> 00:23:13,990
run stochastic gradient descent to train the rate parameters,

439
00:23:13,990 --> 00:23:16,720
meaning uh, B_l and uh, W_l.

440
00:23:16,720 --> 00:23:20,260
Um, and these are the two parameters,

441
00:23:20,260 --> 00:23:23,725
right one is the weight matrix for uh, neighborhood aggregation,

442
00:23:23,725 --> 00:23:29,095
and the other one is the weight matrix for transforming hidden um, hidden vector uh,

443
00:23:29,095 --> 00:23:32,470
embedding of uh, of- of the node itself uh,

444
00:23:32,470 --> 00:23:35,095
to create then the next level uh, embedding.

445
00:23:35,095 --> 00:23:36,820
So this is uh,

446
00:23:36,820 --> 00:23:38,365
how we- how we do this,

447
00:23:38,365 --> 00:23:44,335
what is important is that this weight matrices are shared across different nodes.

448
00:23:44,335 --> 00:23:51,265
So what- what this means is that this l and v are not indexed by the node,

449
00:23:51,265 --> 00:23:55,525
but all nodes in the network use the same transformation matrices.

450
00:23:55,525 --> 00:23:57,880
And this is an important detail.

451
00:23:57,880 --> 00:24:01,840
Um, as I have written things out so far,

452
00:24:01,840 --> 00:24:06,340
I have written them out in terms of nodes aggregating from neighbor's.

453
00:24:06,340 --> 00:24:09,775
But as we saw uh, earlier in graphs,

454
00:24:09,775 --> 00:24:13,570
many times you can also write things in the matrix form.

455
00:24:13,570 --> 00:24:16,000
So let me explain you how to write things uh,

456
00:24:16,000 --> 00:24:17,605
in the matrix form, right?

457
00:24:17,605 --> 00:24:21,370
Many aggregations can be performed efficiently if you uh,

458
00:24:21,370 --> 00:24:24,385
write them out in terms of matrix operations.

459
00:24:24,385 --> 00:24:27,295
So what you can do is you can take your um,

460
00:24:27,295 --> 00:24:32,470
matrix H and simply stag the embeddings of the nodes together,

461
00:24:32,470 --> 00:24:36,540
like here so every node is an embedding for a- for a- uh,

462
00:24:36,540 --> 00:24:40,375
given uh, node of a given layer.

463
00:24:40,375 --> 00:24:43,960
So we can define this notion of a matrix H^l,

464
00:24:43,960 --> 00:24:50,050
and then write the way I can simply compute them basically by saying,

465
00:24:50,050 --> 00:24:54,700
what is the sum o- of the embeddings- aggregation of embeddings um,

466
00:24:54,700 --> 00:24:58,240
of nodes u that are neighbor's of v. This is simply

467
00:24:58,240 --> 00:25:02,650
taking the- taking the uh, the correct um,

468
00:25:02,650 --> 00:25:08,050
entry in the- in the adjacency matrix and multiplying it with the matrix uh,

469
00:25:08,050 --> 00:25:09,475
H. And this way,

470
00:25:09,475 --> 00:25:14,770
I'm basically averaging or aggregating the embeddings coming from the neighbor' s. Um,

471
00:25:14,770 --> 00:25:18,505
then I can also define this notion of a diagonal matrix,

472
00:25:18,505 --> 00:25:22,255
where basically this matrix is zero only on the diagonal of it.

473
00:25:22,255 --> 00:25:24,925
We have uh, non-zero entries that are uh,

474
00:25:24,925 --> 00:25:27,040
the degrees of individual nodes,

475
00:25:27,040 --> 00:25:30,520
and then if you say what is the inverse of this diagonal matrix

476
00:25:30,520 --> 00:25:34,375
D. It is another diagonal matrix where on the uh,

477
00:25:34,375 --> 00:25:37,390
edges where around the entries on the diagonal,

478
00:25:37,390 --> 00:25:41,095
I have now one over the degree um- so that

479
00:25:41,095 --> 00:25:45,430
D- D times inverse of D is an identity matrix, right?

480
00:25:45,430 --> 00:25:49,570
So now if you take this and combine it with this uh,

481
00:25:49,570 --> 00:25:53,260
D minus 1 then you can write the neighborhood aggregation,

482
00:25:53,260 --> 00:25:58,540
basically averaging of the neighborhood embeddings simply has uh D to the minus 1.

483
00:25:58,540 --> 00:26:01,420
So the inverse of D, times adjacency matrix,

484
00:26:01,420 --> 00:26:04,210
times the embeddings at level H,

485
00:26:04,210 --> 00:26:06,414
at level l. So basically,

486
00:26:06,414 --> 00:26:10,450
what this means is that I can write things in terms of this summation and averaging,

487
00:26:10,450 --> 00:26:13,345
or I can write it as a product of three matrices,

488
00:26:13,345 --> 00:26:17,260
a product of this diagonal matrix that has one over the diagonal uh,

489
00:26:17,260 --> 00:26:18,880
one over the degree on the diagonal.

490
00:26:18,880 --> 00:26:20,845
So this corresponds to this theorem uh,

491
00:26:20,845 --> 00:26:26,410
A corresponds to summing over the neighbor's and H uh,

492
00:26:26,410 --> 00:26:31,270
uh, superscript l are the embeddings of the nodes from the previous layer.

493
00:26:31,270 --> 00:26:35,830
So this means I can think of this in terms of this kind of matrix equation,

494
00:26:35,830 --> 00:26:39,970
or I can write it basically as neighborhood uh, aggregation.

495
00:26:39,970 --> 00:26:45,460
So rewriting the update functioning matrix form then write- is- is like this, right?

496
00:26:45,460 --> 00:26:47,455
It's basically take the- uh,

497
00:26:47,455 --> 00:26:50,395
your embedding and multiply it with B, uh,

498
00:26:50,395 --> 00:26:54,310
take the embeddings of the neighbor's from previous layer um,

499
00:26:54,310 --> 00:26:57,460
and multiply them with W. Um,

500
00:26:57,460 --> 00:26:59,934
so red part of corresponds to neighborhood aggregation,

501
00:26:59,934 --> 00:27:04,780
blue part corresponds to uh, to self-transformation um.

502
00:27:04,780 --> 00:27:07,585
And in practice what this implies is that uh,

503
00:27:07,585 --> 00:27:10,765
efficient sparse matrix multiplication can be used

504
00:27:10,765 --> 00:27:14,260
to train these models very efficiently.

505
00:27:14,260 --> 00:27:18,249
So you can basically represent everything goes matrices,

506
00:27:18,249 --> 00:27:20,365
and then you have matrix gradients uh,

507
00:27:20,365 --> 00:27:23,455
and everything would work uh, very nicely.

508
00:27:23,455 --> 00:27:25,765
Now, in the last few minutes,

509
00:27:25,765 --> 00:27:27,865
I wanna talk about how to train this thing.

510
00:27:27,865 --> 00:27:33,010
So the node embeddings z are- are a function of the input graph.

511
00:27:33,010 --> 00:27:37,585
And we can train this in a supervised setting in a sense that we wanna minimize the loss,

512
00:27:37,585 --> 00:27:39,580
the same way as we discussed so far, right?

513
00:27:39,580 --> 00:27:42,130
I wanna make a prediction based on the embedding,

514
00:27:42,130 --> 00:27:45,550
and I wanna minimize the discrepancy between the prediction and the truth.

515
00:27:45,550 --> 00:27:46,735
Ur, where, you know,

516
00:27:46,735 --> 00:27:47,860
y could be, for example,

517
00:27:47,860 --> 00:27:50,380
an old label or a- or a scalar value.

518
00:27:50,380 --> 00:27:53,740
Um, or I could even apply this in an unsupervised setting,

519
00:27:53,740 --> 00:27:55,540
where I would say I want, you know,

520
00:27:55,540 --> 00:27:58,885
the- the similar- where node labels are unavailable,

521
00:27:58,885 --> 00:28:00,940
I can use the graph structure for supervision.

522
00:28:00,940 --> 00:28:03,580
So I could define the notion of similarity and say, you know,

523
00:28:03,580 --> 00:28:05,320
the dot product between the embeddings of

524
00:28:05,320 --> 00:28:08,380
two nodes has to correspond to their similarity in the network.

525
00:28:08,380 --> 00:28:13,030
And I could use now deep encoder to come up with the embeddings of the nodes,

526
00:28:13,030 --> 00:28:16,000
rather than using the shallow, uh, uh,

527
00:28:16,000 --> 00:28:20,350
shallow encoder, uh, but I could use the same decoder as in node to vet,

528
00:28:20,350 --> 00:28:22,555
meaning the- the random walk and,

529
00:28:22,555 --> 00:28:26,065
um, similarity matching using the dot problem.

530
00:28:26,065 --> 00:28:29,905
So that's you- I can apply this in both settings.

531
00:28:29,905 --> 00:28:32,695
Um, to explain how I could do,

532
00:28:32,695 --> 00:28:34,945
uh, unsupervised training a bit more.

533
00:28:34,945 --> 00:28:36,970
So the idea would- would be that similar,

534
00:28:36,970 --> 00:28:39,295
uh, nodes have similar embeddings.

535
00:28:39,295 --> 00:28:41,080
So the idea would be that, you know,

536
00:28:41,080 --> 00:28:43,390
let- let y- let y_u,

537
00:28:43,390 --> 00:28:47,875
v denote- be kept value one if node u and v are indeed similar,

538
00:28:47,875 --> 00:28:50,215
for example, they- they code- they, uh,

539
00:28:50,215 --> 00:28:53,770
code according to the same random work as defined in nodes to work.

540
00:28:53,770 --> 00:28:55,840
Decoder of the two embeddings,

541
00:28:55,840 --> 00:28:57,400
let's say, is a simple dot-product.

542
00:28:57,400 --> 00:29:00,520
It says embedding of one times the embedding of the other,

543
00:29:00,520 --> 00:29:04,765
and you want the- the discrepancy between the similarity and the, uh,

544
00:29:04,765 --> 00:29:09,850
similarity in the graph versus similarity in the embedding space to be,

545
00:29:09,850 --> 00:29:13,540
uh, small, so we can define this through the cross entropy.

546
00:29:13,540 --> 00:29:18,505
Um, and then we could again just basically run this, uh, optimization problem,

547
00:29:18,505 --> 00:29:21,835
to come up with the graph neural network that makes,

548
00:29:21,835 --> 00:29:23,800
uh, the predictions, um,

549
00:29:23,800 --> 00:29:25,870
as we- as we discussed.

550
00:29:25,870 --> 00:29:29,515
So, um, the way we are going to train this,

551
00:29:29,515 --> 00:29:32,440
is that we are going to train this either, as I said,

552
00:29:32,440 --> 00:29:34,205
in this kind of unsupervised way,

553
00:29:34,205 --> 00:29:37,110
and of course we can also directly train this in a supervised way.

554
00:29:37,110 --> 00:29:38,775
Which means that perhaps, you know,

555
00:29:38,775 --> 00:29:40,920
for a given node we have some, uh,

556
00:29:40,920 --> 00:29:43,110
we have- we have some label about a node,

557
00:29:43,110 --> 00:29:45,090
maybe this is a drug-drug interaction network,

558
00:29:45,090 --> 00:29:47,170
and you know whether this drug is toxic,

559
00:29:47,170 --> 00:29:49,840
uh, or not, whether it's safe or toxic.

560
00:29:49,840 --> 00:29:51,250
So we could then say, you know,

561
00:29:51,250 --> 00:29:52,645
given this neural network,

562
00:29:52,645 --> 00:29:54,880
predict at the end the label of the node.

563
00:29:54,880 --> 00:29:56,890
Whether it's safe or toxic.

564
00:29:56,890 --> 00:29:59,335
And now, we can backpropagate based on label.

565
00:29:59,335 --> 00:30:02,125
So both- both are o- both are possible,

566
00:30:02,125 --> 00:30:05,065
either we directly train to predict the labels,

567
00:30:05,065 --> 00:30:07,959
or we can train based on the network similarity,

568
00:30:07,959 --> 00:30:10,870
where network similarity can be defined using random works,

569
00:30:10,870 --> 00:30:12,070
the same way as in,

570
00:30:12,070 --> 00:30:14,020
uh, node to work.

571
00:30:14,020 --> 00:30:17,140
So for, uh, supervised training,

572
00:30:17,140 --> 00:30:19,030
uh, basically what we wanna do is,

573
00:30:19,030 --> 00:30:20,365
we wanna define, uh,

574
00:30:20,365 --> 00:30:24,685
the loss, uh, in terms of let's say classification,

575
00:30:24,685 --> 00:30:28,165
this is the cross entropy loss for a binary classification.

576
00:30:28,165 --> 00:30:32,890
Basically here is the prediction of the label for a- for a- uh,

577
00:30:32,890 --> 00:30:35,785
for a given color whether it's toxic or not,

578
00:30:35,785 --> 00:30:38,860
this is whether it is truly toxic or not, um,

579
00:30:38,860 --> 00:30:42,760
and then the way you can think of this is y takes value one if it's toxic,

580
00:30:42,760 --> 00:30:44,065
and zero if it's not.

581
00:30:44,065 --> 00:30:45,955
If the true value is zero,

582
00:30:45,955 --> 00:30:48,490
then this term is going to survive and it's

583
00:30:48,490 --> 00:30:51,370
basically one minus the lock predicted- prob- uh,

584
00:30:51,370 --> 00:30:53,245
one minus the predicted probability.

585
00:30:53,245 --> 00:30:57,310
So here we want this to be the predicted probability- to be as small as

586
00:30:57,310 --> 00:31:02,320
possible so that one minus it becomes close to one because log of 1 is 0,

587
00:31:02,320 --> 00:31:04,405
so that this discrepancy is small.

588
00:31:04,405 --> 00:31:05,980
And if the- if the,

589
00:31:05,980 --> 00:31:08,110
uh, class value is one,

590
00:31:08,110 --> 00:31:10,555
then this term is going to survive because

591
00:31:10,555 --> 00:31:13,660
1 plus 1- 1 minus 1 is 0 and this- this goes away.

592
00:31:13,660 --> 00:31:18,340
So here we want this term to be as close to one as possible.

593
00:31:18,340 --> 00:31:20,605
Which again would say, if it's- uh,

594
00:31:20,605 --> 00:31:23,710
if it's toxic, we want the probability to be high.

595
00:31:23,710 --> 00:31:25,045
If it's not toxic,

596
00:31:25,045 --> 00:31:27,625
we want the probability [NOISE] to be low- uh,

597
00:31:27,625 --> 00:31:30,370
the predicted probability, uh, of it being toxic.

598
00:31:30,370 --> 00:31:33,625
And this is the cross, uh, entropy loss.

599
00:31:33,625 --> 00:31:38,485
So this is the encoded input coming from node embeddings.

600
00:31:38,485 --> 00:31:40,930
Uh, these are the classification rates,

601
00:31:40,930 --> 00:31:43,105
uh, for the final classification.

602
00:31:43,105 --> 00:31:45,760
Uh and these are the, uh, node labels,

603
00:31:45,760 --> 00:31:48,385
is it basically toxic, uh, or not.

604
00:31:48,385 --> 00:31:50,950
Um, and I can optimize this loss function,

605
00:31:50,950 --> 00:31:53,110
uh, to basically come up with, uh,

606
00:31:53,110 --> 00:31:58,885
the- the parameters B and W that give me the embedding that then makes,

607
00:31:58,885 --> 00:32:01,795
uh, good or accurate predictions.

608
00:32:01,795 --> 00:32:04,735
So let me just give an overview and,

609
00:32:04,735 --> 00:32:06,835
uh, finish, uh, and complete the lecture.

610
00:32:06,835 --> 00:32:10,405
So the way we think of modern design is that given the graph,

611
00:32:10,405 --> 00:32:12,010
we wanna compute the,

612
00:32:12,010 --> 00:32:14,740
uh, the embedding of a target node.

613
00:32:14,740 --> 00:32:18,820
First, we need to define the neighborhood aggregation function.

614
00:32:18,820 --> 00:32:22,315
Um, uh, this is the first thing we have to do.

615
00:32:22,315 --> 00:32:26,815
The second thing we have to do then is to define the loss function on the embedding,

616
00:32:26,815 --> 00:32:29,830
like the cross entropy loss I was just discussing,

617
00:32:29,830 --> 00:32:31,780
um, and then we need to train.

618
00:32:31,780 --> 00:32:34,630
And the way we can train the model is that we train it on a set

619
00:32:34,630 --> 00:32:37,330
of nodes in a- on a batch of nodes.

620
00:32:37,330 --> 00:32:38,920
So we select a batch of nodes,

621
00:32:38,920 --> 00:32:40,735
create the computation graphs,

622
00:32:40,735 --> 00:32:43,330
and this is a batch of nodes on which we train.

623
00:32:43,330 --> 00:32:45,790
Um, and then what is interesting is that,

624
00:32:45,790 --> 00:32:46,945
after the model is trained,

625
00:32:46,945 --> 00:32:50,890
we can generate embeddings for any nodes, uh, as needed,

626
00:32:50,890 --> 00:32:56,170
and simply apply [NOISE] our model to them by basically just doing the forward pass.

627
00:32:56,170 --> 00:32:58,735
So this means that we can apply our model,

628
00:32:58,735 --> 00:33:01,945
even to nodes that we have never seen during training.

629
00:33:01,945 --> 00:33:03,880
So this means that, uh,

630
00:33:03,880 --> 00:33:06,970
we cannot train on one graph and transfer the model to

631
00:33:06,970 --> 00:33:11,035
the other graph because this particular set of, um,

632
00:33:11,035 --> 00:33:14,260
nodes could be used for training the parameters [NOISE] to be optimized here,

633
00:33:14,260 --> 00:33:16,780
and then we could apply this to a new set of nodes,

634
00:33:16,780 --> 00:33:19,390
to a new set of, uh, computation graphs.

635
00:33:19,390 --> 00:33:23,830
So this means that our model has inductive, uh, capability,

636
00:33:23,830 --> 00:33:26,860
which means that same aggregation parameters,

637
00:33:26,860 --> 00:33:28,300
same W and B,

638
00:33:28,300 --> 00:33:30,085
are shared across all nodes.

639
00:33:30,085 --> 00:33:32,530
Uh, and the number of model parameters in

640
00:33:32,530 --> 00:33:35,455
this case is sublinear with the size of the network,

641
00:33:35,455 --> 00:33:39,550
because W and B only depend on the embedding dimensionality and the size,

642
00:33:39,550 --> 00:33:42,610
number of features, and not on the size of the graph.

643
00:33:42,610 --> 00:33:47,695
And this means that graph neural networks are able to generalize to unseen nodes.

644
00:33:47,695 --> 00:33:49,360
And that's a super cool, uh,

645
00:33:49,360 --> 00:33:51,504
feature of them, because for example,

646
00:33:51,504 --> 00:33:54,670
this means that you can train your graph neural network on one graph,

647
00:33:54,670 --> 00:33:56,620
and you can apply it to a new graph.

648
00:33:56,620 --> 00:33:59,755
Because you determine matrices [NOISE] W and B here,

649
00:33:59,755 --> 00:34:01,285
and then you can transfer this,

650
00:34:01,285 --> 00:34:03,115
uh, to the new network.

651
00:34:03,115 --> 00:34:05,470
You can, uh, for example,

652
00:34:05,470 --> 00:34:06,670
train on one organism,

653
00:34:06,670 --> 00:34:08,830
and transfer this to the new organism here.

654
00:34:08,830 --> 00:34:10,719
This is, let's say, a biological network.

655
00:34:10,719 --> 00:34:13,449
This is also very useful for the networks that

656
00:34:13,449 --> 00:34:17,199
constantly evolve because you can take the snapshot of the network,

657
00:34:17,199 --> 00:34:19,929
create the computation graphs here,

658
00:34:19,929 --> 00:34:24,459
and determined the parameters so that when then in production,

659
00:34:24,460 --> 00:34:25,570
and you know, derives,

660
00:34:25,570 --> 00:34:28,315
you quickly create a computation graph for it.

661
00:34:28,315 --> 00:34:30,025
Just do the forward pass,

662
00:34:30,025 --> 00:34:32,170
and here you have the embedding for it.

663
00:34:32,170 --> 00:34:36,565
So no- no retraining the model, um, is necessary.

664
00:34:36,565 --> 00:34:40,780
And that is super cool because it means you can train- train on one graph,

665
00:34:40,780 --> 00:34:42,010
transfer to another one,

666
00:34:42,010 --> 00:34:43,495
train on a small graph,

667
00:34:43,495 --> 00:34:45,235
transfer the model to a big,

668
00:34:45,235 --> 00:34:47,395
uh, graph, or to an evolving graph.

669
00:34:47,395 --> 00:34:49,360
So let me summarize,

670
00:34:49,360 --> 00:34:51,594
uh, the lecture for today and finish here.

671
00:34:51,594 --> 00:34:54,174
What we did is, we generated node embeddings by

672
00:34:54,175 --> 00:34:57,355
aggregating no- node neighborhood information.

673
00:34:57,355 --> 00:35:00,340
We saw our first basic variant of this idea that

674
00:35:00,340 --> 00:35:03,865
simply averages up the messages coming from the neighbors.

675
00:35:03,865 --> 00:35:08,635
Um, and the key distinction between different architectures as we are going to see next,

676
00:35:08,635 --> 00:35:12,955
is that- is how this aggregation process is being done.

677
00:35:12,955 --> 00:35:15,430
And, uh, what I'm going to discuss in

678
00:35:15,430 --> 00:35:19,990
the next lecture will be the architecture called, um, um,

679
00:35:19,990 --> 00:35:22,390
GraphSAGE that generalizes this

680
00:35:22,390 --> 00:35:25,840
into basically a framework where different types of aggregations,

681
00:35:25,840 --> 00:35:29,155
different kas- kinds of transformations, uh, can be used.

682
00:35:29,155 --> 00:35:31,105
So, um, thank you very much, uh,

683
00:35:31,105 --> 00:35:32,350
for the lecture, um,

684
00:35:32,350 --> 00:35:33,985
and very happy, uh,

685
00:35:33,985 --> 00:35:36,140
to take questions now.

