1
00:00:00,000 --> 00:00:04,950


2
00:00:04,950 --> 00:00:06,450
Welcome to the class.

3
00:00:06,450 --> 00:00:09,090
Today we are going to
discuss deep generative

4
00:00:09,090 --> 00:00:10,740
models for graphs.

5
00:00:10,740 --> 00:00:14,380
So let me explain
that in more detail.

6
00:00:14,380 --> 00:00:19,530
So, so far we talked about how
to classify nodes and edges

7
00:00:19,530 --> 00:00:21,750
and perhaps entire graphs.

8
00:00:21,750 --> 00:00:24,720
But now we are going to
talk about a new task, which

9
00:00:24,720 --> 00:00:27,120
is the task of
generating the graph.

10
00:00:27,120 --> 00:00:30,240
The idea is that we want to
have a generative model that

11
00:00:30,240 --> 00:00:33,870
is going to generate
a synthetic graph that

12
00:00:33,870 --> 00:00:38,310
will be similar to
the real-world graph.

13
00:00:38,310 --> 00:00:42,030
An application of this
type of graph generation

14
00:00:42,030 --> 00:00:44,400
happens in many
different places.

15
00:00:44,400 --> 00:00:48,480
You can imagine that you can
represent molecules as graphs

16
00:00:48,480 --> 00:00:51,420
of bonds between the atoms.

17
00:00:51,420 --> 00:00:54,060
And then in this case,
you want to generate

18
00:00:54,060 --> 00:00:59,400
novel molecular structures
based on this generative model.

19
00:00:59,400 --> 00:01:01,540
Or for example in
material design,

20
00:01:01,540 --> 00:01:05,550
you may want to generate
optimal material structures

21
00:01:05,550 --> 00:01:06,870
and this is what you can do.

22
00:01:06,870 --> 00:01:08,610
In social network
modeling, you may

23
00:01:08,610 --> 00:01:11,530
want to generate
synthetic social networks

24
00:01:11,530 --> 00:01:14,760
so that you can then use
them for various kinds

25
00:01:14,760 --> 00:01:16,020
of downstream tasks.

26
00:01:16,020 --> 00:01:18,460
And even in some other
applications for example,

27
00:01:18,460 --> 00:01:21,930
if you think about generating
realistic road layouts.

28
00:01:21,930 --> 00:01:23,820
If you want to think
about generating

29
00:01:23,820 --> 00:01:27,585
realistic layouts of cities,
all these types of things

30
00:01:27,585 --> 00:01:32,100
you can model as a graph
generation process.

31
00:01:32,100 --> 00:01:35,190
And even for example, some
combinatorial problems

32
00:01:35,190 --> 00:01:38,820
like the satisfiability problem
or the Boolean satisfiability

33
00:01:38,820 --> 00:01:42,930
problem, you can generate
artificial instances

34
00:01:42,930 --> 00:01:47,040
of that problem by representing
the satisfiability instance

35
00:01:47,040 --> 00:01:51,190
as a graph and then learning
how to generate those graphs.

36
00:01:51,190 --> 00:01:54,370
So in all of these
cases basically,

37
00:01:54,370 --> 00:01:56,130
the goal is that
we want to learn

38
00:01:56,130 --> 00:01:59,910
how to generate a
graph that is somehow

39
00:01:59,910 --> 00:02:04,280
similar to the underlying
real-world graph.

40
00:02:04,280 --> 00:02:07,370
And the field of
graph generation

41
00:02:07,370 --> 00:02:09,919
has a rich tradition.

42
00:02:09,919 --> 00:02:11,990
And the way this
started, it started

43
00:02:11,990 --> 00:02:15,470
with the study of properties
of complex networks

44
00:02:15,470 --> 00:02:18,290
like real-world networks.

45
00:02:18,290 --> 00:02:21,620
And identifying what are
the fundamental properties

46
00:02:21,620 --> 00:02:23,510
that these real-world
networks have

47
00:02:23,510 --> 00:02:28,550
like power or scale-free
degree distributions

48
00:02:28,550 --> 00:02:31,160
and also like the
small world property

49
00:02:31,160 --> 00:02:32,750
and so on and so forth.

50
00:02:32,750 --> 00:02:35,870
Based on these fundamental
properties of complex networks,

51
00:02:35,870 --> 00:02:39,470
then there have been
a lot of development

52
00:02:39,470 --> 00:02:42,170
of generative models
for graphs, which

53
00:02:42,170 --> 00:02:45,440
generally fell into two camps.

54
00:02:45,440 --> 00:02:49,430
One camp was very
mechanistic generative models

55
00:02:49,430 --> 00:02:53,780
like the preferential attachment
model, that basically allowed

56
00:02:53,780 --> 00:02:56,240
us to explain how could
certain properties

57
00:02:56,240 --> 00:02:58,160
like the scale-free
property of networks

58
00:02:58,160 --> 00:03:01,790
arise from this microscopic
preferential attachment type

59
00:03:01,790 --> 00:03:02,780
model.

60
00:03:02,780 --> 00:03:06,530
Another set of models
for generating graphs,

61
00:03:06,530 --> 00:03:10,070
came mostly from the statistics
and the social networking

62
00:03:10,070 --> 00:03:12,950
literature where basically, the
idea was that there is maybe

63
00:03:12,950 --> 00:03:16,190
some, that there might be
some latent social groups

64
00:03:16,190 --> 00:03:19,520
and based on those
latent social groups,

65
00:03:19,520 --> 00:03:21,650
edges of the social
network get created.

66
00:03:21,650 --> 00:03:24,020
And then the question is,
how can you take that model,

67
00:03:24,020 --> 00:03:27,990
fit it to the data and
perhaps discover the groups?

68
00:03:27,990 --> 00:03:31,260
However, today and
in this lecture,

69
00:03:31,260 --> 00:03:33,170
we are going to
use deep learning

70
00:03:33,170 --> 00:03:35,420
and representation
learning to learn

71
00:03:35,420 --> 00:03:37,020
how to generate the graphs.

72
00:03:37,020 --> 00:03:43,050
So in contrast to prior work in
some sense that either assumed

73
00:03:43,050 --> 00:03:45,180
some mechanistic
generative process

74
00:03:45,180 --> 00:03:48,780
or assumed some statistical
model that was motivated

75
00:03:48,780 --> 00:03:51,210
by the-- let's say
social science, here

76
00:03:51,210 --> 00:03:56,220
we want to be kind of
agnostic in this respect.

77
00:03:56,220 --> 00:04:00,240
And the goal will be that,
can we basically given a graph

78
00:04:00,240 --> 00:04:03,660
or given a couple of graphs,
can we learn how to gene--

79
00:04:03,660 --> 00:04:05,820
what the properties
of those graphs are,

80
00:04:05,820 --> 00:04:08,580
and how can we
generate more instances

81
00:04:08,580 --> 00:04:09,970
of those types of graphs?

82
00:04:09,970 --> 00:04:12,010
So we'll be kind of
completely general,

83
00:04:12,010 --> 00:04:15,270
we'll just learn from the data
in this kind of representation

84
00:04:15,270 --> 00:04:17,440
learning framework.

85
00:04:17,440 --> 00:04:21,180
So this is one way how
we can look into this.

86
00:04:21,180 --> 00:04:22,920
Another way how we
can look into this

87
00:04:22,920 --> 00:04:25,080
is that, so far in
this class we've

88
00:04:25,080 --> 00:04:27,990
been talking about the deep
graph encoders, where basically

89
00:04:27,990 --> 00:04:32,160
the idea was that, we have a
complex network, complex graph,

90
00:04:32,160 --> 00:04:34,470
complex relational
structure on the input

91
00:04:34,470 --> 00:04:37,530
and you want to pass it
through several layers

92
00:04:37,530 --> 00:04:42,150
of this representation and
a deep learning network that

93
00:04:42,150 --> 00:04:45,570
at the end produces-- let's
say node embeddings, edge

94
00:04:45,570 --> 00:04:48,230
embeddings, entire
graph embeddings, right?

95
00:04:48,230 --> 00:04:50,700
So this is what we call
a deep graph encoder

96
00:04:50,700 --> 00:04:52,500
because it takes the
graph as the input

97
00:04:52,500 --> 00:04:55,860
and encodes it into some
kind of representation.

98
00:04:55,860 --> 00:04:58,380
The task of graph
generation actually

99
00:04:58,380 --> 00:05:00,600
goes in the other direction.

100
00:05:00,600 --> 00:05:02,880
It wants to start on
the right-hand side

101
00:05:02,880 --> 00:05:06,270
and then through a series of
complex nonlinear transforms

102
00:05:06,270 --> 00:05:09,190
wants to output the
entire graph, right?

103
00:05:09,190 --> 00:05:12,588
So our input perhaps will be
a little small noise parameter

104
00:05:12,588 --> 00:05:14,130
or something like
that and we'll want

105
00:05:14,130 --> 00:05:17,190
to kind of expand
that until we have

106
00:05:17,190 --> 00:05:19,370
the entire graph on the output.

107
00:05:19,370 --> 00:05:23,040
So we will be kind of decoding
rather than encoding, right?

108
00:05:23,040 --> 00:05:25,080
We'll take a small
piece of information

109
00:05:25,080 --> 00:05:27,060
and expand it into
the entire graph

110
00:05:27,060 --> 00:05:29,670
rather than taking
a complex structure

111
00:05:29,670 --> 00:05:33,820
and compress it into or
into its representation.

112
00:05:33,820 --> 00:05:37,410
So we are talking about
deep graph decoders,

113
00:05:37,410 --> 00:05:42,190
because on the output we want
to generate an entire network.

114
00:05:42,190 --> 00:05:45,240
So in order for us to do
that, I want to first tell you

115
00:05:45,240 --> 00:05:50,370
about kind of how are we going
to set up the problem in terms

116
00:05:50,370 --> 00:05:52,270
of its set up, in
terms of its kind

117
00:05:52,270 --> 00:05:54,780
of mathematical
statistical foundation.

118
00:05:54,780 --> 00:05:59,070
And then I'm going to talk
about what methods allow

119
00:05:59,070 --> 00:06:03,330
us to achieve the goal
of graph generation

120
00:06:03,330 --> 00:06:05,410
using representation learning.

121
00:06:05,410 --> 00:06:08,640
So let's talk about
graph generation.

122
00:06:08,640 --> 00:06:12,030
Generally, we have two
tasks we will talk about

123
00:06:12,030 --> 00:06:12,880
in this lecture.

124
00:06:12,880 --> 00:06:16,232
First, is what we will call
realistic graph generation,

125
00:06:16,232 --> 00:06:17,940
where we want to
generate the graphs that

126
00:06:17,940 --> 00:06:20,160
are similar to a
given set of graphs.

127
00:06:20,160 --> 00:06:22,920
And I'm going to define
this much more rigorously

128
00:06:22,920 --> 00:06:23,920
in a second.

129
00:06:23,920 --> 00:06:25,650
And then the second
task, I'm also

130
00:06:25,650 --> 00:06:28,710
going to talk about is what
we call goal-directed graph

131
00:06:28,710 --> 00:06:29,490
generation.

132
00:06:29,490 --> 00:06:31,440
What basically you want
to generate a graph

133
00:06:31,440 --> 00:06:33,600
that optimizes a
given constraint

134
00:06:33,600 --> 00:06:35,080
or a given objective.

135
00:06:35,080 --> 00:06:37,290
So if you are generating
a molecule you could say,

136
00:06:37,290 --> 00:06:40,830
I want to generate molecules
that have a given property,

137
00:06:40,830 --> 00:06:42,900
maybe the property
is solubility,

138
00:06:42,900 --> 00:06:46,110
maybe the property is that
these molecules are non-toxic.

139
00:06:46,110 --> 00:06:48,870
And you say I want to generate
molecules that are non-toxic.

140
00:06:48,870 --> 00:06:51,735
So how do I generate the
most non-toxic molecule?

141
00:06:51,735 --> 00:06:56,070
Or how do I generate the
molecule that is most soluble

142
00:06:56,070 --> 00:06:58,500
and still looks like drugs?

143
00:06:58,500 --> 00:07:00,900
Imagine another case
I was giving example

144
00:07:00,900 --> 00:07:03,690
before if I want to generate a
road network-- a realistic road

145
00:07:03,690 --> 00:07:06,600
network of a city, that is
a graph generation problem.

146
00:07:06,600 --> 00:07:09,000
I could say, I want to
generate the optimal road

147
00:07:09,000 --> 00:07:11,400
network of a city,
right, whatever

148
00:07:11,400 --> 00:07:16,590
the optimality constraint is
that is kind of, we assume

149
00:07:16,590 --> 00:07:17,850
it is given to us, right?

150
00:07:17,850 --> 00:07:21,665
So that's what we mean by
goal-directed graph generation

151
00:07:21,665 --> 00:07:24,240
where you want to generate
a graph with a given--

152
00:07:24,240 --> 00:07:28,080
with a given goal that optimizes
a given black box objective

153
00:07:28,080 --> 00:07:28,900
function.

154
00:07:28,900 --> 00:07:32,540
So that's the two parts
of the lecture today.

155
00:07:32,540 --> 00:07:35,310
But first, let's
talk about how do we

156
00:07:35,310 --> 00:07:39,540
set up this graph generation
task as a machine learning

157
00:07:39,540 --> 00:07:40,080
task?

158
00:07:40,080 --> 00:07:43,800
So we are going to proceed
in the following way.

159
00:07:43,800 --> 00:07:47,820
We are going to assume that the
graphs are sampled from this p

160
00:07:47,820 --> 00:07:49,060
data distribution.

161
00:07:49,060 --> 00:07:52,200
So basically nature is
sampling from p data

162
00:07:52,200 --> 00:07:53,800
and is giving us graphs.

163
00:07:53,800 --> 00:07:58,860
Our goal will be to learn
a distribution p model

164
00:07:58,860 --> 00:08:03,750
and then be able to learn how
to sample from this p model.

165
00:08:03,750 --> 00:08:07,050
So basically, given
the input data,

166
00:08:07,050 --> 00:08:10,620
we are going to learn a
probability distribution p

167
00:08:10,620 --> 00:08:12,540
model over the
graphs, and then we

168
00:08:12,540 --> 00:08:15,540
are going to sample new
graphs from that probability

169
00:08:15,540 --> 00:08:16,170
distribution.

170
00:08:16,170 --> 00:08:17,760
And right, and our
goal somehow will

171
00:08:17,760 --> 00:08:21,930
be, that we want this p model
distribution to be as close as

172
00:08:21,930 --> 00:08:26,130
possible to this unknown
p data distribution

173
00:08:26,130 --> 00:08:29,850
that we don't have access to,
that only nature has access to,

174
00:08:29,850 --> 00:08:32,760
right, the data set creator
has access to this p data.

175
00:08:32,760 --> 00:08:36,120
We want to approximate
p data with p model.

176
00:08:36,120 --> 00:08:39,730
And then as we have approximated
p data with p model,

177
00:08:39,730 --> 00:08:41,850
we want to draw
additional instances,

178
00:08:41,850 --> 00:08:43,770
we want to generate
additional graphs,

179
00:08:43,770 --> 00:08:48,180
we want to generate additional
samples from the p model

180
00:08:48,180 --> 00:08:51,830
and those would be the graphs
we will want to generate.

181
00:08:51,830 --> 00:08:54,970
So if we want to do
this, then there are--

182
00:08:54,970 --> 00:08:58,360
this is an instance of what we
call generative models, right?

183
00:08:58,360 --> 00:09:01,540
We assume we want to learn a
generative model for graphs

184
00:09:01,540 --> 00:09:05,530
from a set of input graphs,
let's call them x i.

185
00:09:05,530 --> 00:09:09,790
Here, as I said before p data
is the data distribution which

186
00:09:09,790 --> 00:09:12,430
is not known to us and we
don't have access to it,

187
00:09:12,430 --> 00:09:15,580
all we have access
to it are samples

188
00:09:15,580 --> 00:09:19,040
x that are sampled from
this unknown p data.

189
00:09:19,040 --> 00:09:23,770
We also will have another family
of probability distributions,

190
00:09:23,770 --> 00:09:27,280
let's call them p model
that are defined by theta,

191
00:09:27,280 --> 00:09:31,180
theta are parameters
of our model.

192
00:09:31,180 --> 00:09:33,520
And we will want to use
this p model distribution

193
00:09:33,520 --> 00:09:35,830
to approximate p data.

194
00:09:35,830 --> 00:09:37,910
And then, what
would be our goal?

195
00:09:37,910 --> 00:09:39,250
We have two-step goal.

196
00:09:39,250 --> 00:09:44,140
First goal is to find parameters
theta so that p model closely

197
00:09:44,140 --> 00:09:47,740
approximates p data, and this
is called a density estimation

198
00:09:47,740 --> 00:09:48,430
task.

199
00:09:48,430 --> 00:09:53,460
And then we also want to be
able to sample from p model.

200
00:09:53,460 --> 00:09:56,110
Basically means, we want to
be able to generate new graphs

201
00:09:56,110 --> 00:09:59,110
from this now p model
distribution to which we

202
00:09:59,110 --> 00:10:00,730
have access to, right?

203
00:10:00,730 --> 00:10:04,750
We want to generate new
samples, new graphs from it.

204
00:10:04,750 --> 00:10:07,090
So let me give you
more details, right?

205
00:10:07,090 --> 00:10:11,650
Our goal is to make p model be
as close to p data as possible.

206
00:10:11,650 --> 00:10:13,830
And the key principle we
are going to use here,

207
00:10:13,830 --> 00:10:16,990
is the principle of maximum
likelihood estimation,

208
00:10:16,990 --> 00:10:20,190
which is a fundamental approach
to modeling distributions.

209
00:10:20,190 --> 00:10:23,120
Basically the way you can
think of it is the following,

210
00:10:23,120 --> 00:10:27,720
we want to find parameters
theta star of our p model

211
00:10:27,720 --> 00:10:33,090
distribution such that the
log likelihood of the data

212
00:10:33,090 --> 00:10:36,810
points x of the graphs x that
are sampled from this p data

213
00:10:36,810 --> 00:10:41,670
distribution, their log
likelihood under our model--

214
00:10:41,670 --> 00:10:45,090
under p model that is basically
defined or parameterized

215
00:10:45,090 --> 00:10:47,740
by theta, is as large
as possible, right?

216
00:10:47,740 --> 00:10:50,160
So our goal is to
find parameters theta

217
00:10:50,160 --> 00:10:53,460
star, such that the
observed data points x.

218
00:10:53,460 --> 00:10:56,580
So basically, the
observed graphs

219
00:10:56,580 --> 00:11:00,840
have the highest log likelihood
among all possible choices

220
00:11:00,840 --> 00:11:02,695
of theta, right?

221
00:11:02,695 --> 00:11:04,320
And of course here,
the important thing

222
00:11:04,320 --> 00:11:07,080
will be that p model needs
to be flexible enough

223
00:11:07,080 --> 00:11:09,060
that it's able to model p data.

224
00:11:09,060 --> 00:11:12,240
And then the question
is, how do we search over

225
00:11:12,240 --> 00:11:17,310
all the instances of probability
distributions captured

226
00:11:17,310 --> 00:11:18,750
by this p model?

227
00:11:18,750 --> 00:11:23,520
So we actually kind of capture
by these parameters theta, so

228
00:11:23,520 --> 00:11:26,520
that the likelihood of
the data that we observe

229
00:11:26,520 --> 00:11:29,190
is as high as possible, right?

230
00:11:29,190 --> 00:11:31,380
And in other words,
the goal is to find

231
00:11:31,380 --> 00:11:34,500
the model that is most likely
to have generated the observed

232
00:11:34,500 --> 00:11:36,060
data x, right?

233
00:11:36,060 --> 00:11:38,700
Find the p model that
is most likely to have

234
00:11:38,700 --> 00:11:41,700
generated the observed data x.

235
00:11:41,700 --> 00:11:43,680
Now, this is the
first part, which

236
00:11:43,680 --> 00:11:45,300
is the density estimation.

237
00:11:45,300 --> 00:11:47,700
The second part
is also important,

238
00:11:47,700 --> 00:11:50,290
because once we have the
density that's not enough.

239
00:11:50,290 --> 00:11:52,800
We need to be able to draw
samples from it, right?

240
00:11:52,800 --> 00:11:56,520
We want to create samples from
this complex distribution.

241
00:11:56,520 --> 00:12:01,770
And a common approach how
you can generate samples

242
00:12:01,770 --> 00:12:05,160
from a complex distribution
would be the following.

243
00:12:05,160 --> 00:12:08,220
Is that first, you start with
the simple noise distribution

244
00:12:08,220 --> 00:12:12,270
just like a simple,
let's say a scalar value

245
00:12:12,270 --> 00:12:15,780
that's a normally distributed
0 mean unit need to variance.

246
00:12:15,780 --> 00:12:18,330
And then you want to have
this complex function that

247
00:12:18,330 --> 00:12:20,520
will take this
little noise kernel

248
00:12:20,520 --> 00:12:24,540
and is going to expand it
until you have the sample x.

249
00:12:24,540 --> 00:12:27,690
So in our case, we'll start
with a little random seed,

250
00:12:27,690 --> 00:12:32,430
and we are going to expand
it into a full graph x.

251
00:12:32,430 --> 00:12:35,430
Where of course, now the
hope is that x will follow

252
00:12:35,430 --> 00:12:37,930
the distribution of p model.

253
00:12:37,930 --> 00:12:41,190
So in our case, how do we
design this function f?

254
00:12:41,190 --> 00:12:43,170
We are going to use
deep neural networks,

255
00:12:43,170 --> 00:12:46,410
and train them so that they can
start with the little kernel

256
00:12:46,410 --> 00:12:48,910
and generate the graph.

257
00:12:48,910 --> 00:12:52,630
So that's how we are
going to do this.

258
00:12:52,630 --> 00:12:55,680
So now in terms of
deep generative models,

259
00:12:55,680 --> 00:13:00,480
our model will be an instance
of an auto-regressive model.

260
00:13:00,480 --> 00:13:04,440
Where this p model will be used
both for density estimation

261
00:13:04,440 --> 00:13:08,310
and sampling, right, because
we have these two goals, right?

262
00:13:08,310 --> 00:13:16,020
And in general you don't have
to use the same neural network

263
00:13:16,020 --> 00:13:18,360
to both do the
density estimation

264
00:13:18,360 --> 00:13:21,570
and to do the sampling and
there are other approaches

265
00:13:21,570 --> 00:13:25,020
that you could choose to do here
like variational autoencoders

266
00:13:25,020 --> 00:13:29,290
or generative adversarial
networks and so on.

267
00:13:29,290 --> 00:13:32,590
But in our case, we are going
to use an auto-regressive model.

268
00:13:32,590 --> 00:13:35,220
And the idea is
that, we are going

269
00:13:35,220 --> 00:13:38,130
to model this complex
distribution as a product

270
00:13:38,130 --> 00:13:39,990
of simpler distributions.

271
00:13:39,990 --> 00:13:41,970
And the reason why
we can do this,

272
00:13:41,970 --> 00:13:46,180
is of the chain rule in
probability and Bayesian

273
00:13:46,180 --> 00:13:46,680
networks.

274
00:13:46,680 --> 00:13:48,600
So we're basically--
which basically tells us,

275
00:13:48,600 --> 00:13:52,980
that any joint distribution
on a set of variables

276
00:13:52,980 --> 00:13:55,800
can be exactly
modeled or expressed

277
00:13:55,800 --> 00:14:01,640
as a product over the
conditional distribution,

278
00:14:01,640 --> 00:14:02,170
right?

279
00:14:02,170 --> 00:14:05,000
So basically I'm
saying this p model

280
00:14:05,000 --> 00:14:10,950
that is a complex
distribution over my x, which

281
00:14:10,950 --> 00:14:12,930
may be a set of
random variables,

282
00:14:12,930 --> 00:14:16,680
I can write it out as a product
over all these random variables

283
00:14:16,680 --> 00:14:18,840
from the first one
to the last one,

284
00:14:18,840 --> 00:14:22,470
where all I have
to now express is

285
00:14:22,470 --> 00:14:26,370
this probability of a
random variability t given

286
00:14:26,370 --> 00:14:30,570
the values instances of all
the previous random variables,

287
00:14:30,570 --> 00:14:31,540
right?

288
00:14:31,540 --> 00:14:34,750
So in our case for example,
if x would be a vector,

289
00:14:34,750 --> 00:14:38,797
then x sub t is the t-th
coordinate of that vector,

290
00:14:38,797 --> 00:14:40,230
right?

291
00:14:40,230 --> 00:14:43,810
If x is a sentence, then x sub
t would be the word, right?

292
00:14:43,810 --> 00:14:46,200
So I'm basically saying,
rather than generating

293
00:14:46,200 --> 00:14:50,020
an entire sentence or to write
out the probability of a given,

294
00:14:50,020 --> 00:14:51,930
let's say sentence,
I'm going to--

295
00:14:51,930 --> 00:14:53,850
I can model that
as a product where

296
00:14:53,850 --> 00:14:56,760
I say, given the words
so far, how likely

297
00:14:56,760 --> 00:14:58,800
or what is the probability
of the next word?

298
00:14:58,800 --> 00:15:02,400
And if I multiply this
out, I have the probability

299
00:15:02,400 --> 00:15:03,510
of the entire sentence.

300
00:15:03,510 --> 00:15:06,930
And we can do this without
any loss of generality

301
00:15:06,930 --> 00:15:09,330
or any approximation,
if we really

302
00:15:09,330 --> 00:15:12,480
condition on all the
previous words, all

303
00:15:12,480 --> 00:15:14,460
the previous elements, right?

304
00:15:14,460 --> 00:15:18,000
In our case, what this
means is that the way

305
00:15:18,000 --> 00:15:19,740
we apply these two
graphs is that, we

306
00:15:19,740 --> 00:15:23,250
are going to represent
a graph as a sequence

307
00:15:23,250 --> 00:15:24,690
as a set of actions.

308
00:15:24,690 --> 00:15:27,960
And we are going to say, a-ha,
the probability of next action

309
00:15:27,960 --> 00:15:31,350
is conditioned on all
the previous actions.

310
00:15:31,350 --> 00:15:35,460
And now what will the actions
be, it will be add a node,

311
00:15:35,460 --> 00:15:36,280
add an edge, right?

312
00:15:36,280 --> 00:15:39,770
That's the way we are
going to think of this.

313
00:15:39,770 --> 00:15:44,000


