1
00:00:00,000 --> 00:00:05,478


2
00:00:05,478 --> 00:00:05,980
OK.

3
00:00:05,980 --> 00:00:07,010
Hi, everyone.

4
00:00:07,010 --> 00:00:08,300
Welcome back to CS224N.

5
00:00:08,300 --> 00:00:10,810


6
00:00:10,810 --> 00:00:13,870
So today is a pretty
key lecture where

7
00:00:13,870 --> 00:00:17,140
we get through a number
of important topics

8
00:00:17,140 --> 00:00:19,850
for neural networks,
especially as it is applied

9
00:00:19,850 --> 00:00:21,700
to natural language processing.

10
00:00:21,700 --> 00:00:23,500
So right at the
end of last time,

11
00:00:23,500 --> 00:00:26,000
I started into recurrent
neural networks.

12
00:00:26,000 --> 00:00:28,750
So we'll talk in detail more
about recurrent neural networks

13
00:00:28,750 --> 00:00:31,090
in the first part of the class.

14
00:00:31,090 --> 00:00:34,820
And we emphasized
language models.

15
00:00:34,820 --> 00:00:38,170
But then also getting a
bit beyond that and then

16
00:00:38,170 --> 00:00:41,860
looked at more advanced kinds
of recurrent neural networks

17
00:00:41,860 --> 00:00:45,640
towards the end
part of the class.

18
00:00:45,640 --> 00:00:49,150
I just wanted to sort of say
a word before getting underway

19
00:00:49,150 --> 00:00:50,730
about the final project.

20
00:00:50,730 --> 00:00:52,690
So hopefully, by
now, you've started

21
00:00:52,690 --> 00:00:55,360
looking at assignment
3, which is

22
00:00:55,360 --> 00:00:57,790
the middle of the five
assignments for the first half

23
00:00:57,790 --> 00:00:58,780
of the course.

24
00:00:58,780 --> 00:01:01,870
And then in the second half of
the course, most of your effort

25
00:01:01,870 --> 00:01:04,220
goes into a final project.

26
00:01:04,220 --> 00:01:06,580
So next week, the
Thursday lecture

27
00:01:06,580 --> 00:01:08,470
is going to be
about final projects

28
00:01:08,470 --> 00:01:10,420
and choosing the final
project, and tips

29
00:01:10,420 --> 00:01:12,370
for final projects, et cetera.

30
00:01:12,370 --> 00:01:15,490
So it's fine to delay
thinking about final projects

31
00:01:15,490 --> 00:01:17,110
until next week if you want.

32
00:01:17,110 --> 00:01:19,360
But you shouldn't
delay it too long

33
00:01:19,360 --> 00:01:21,640
because we do want
you to get underway

34
00:01:21,640 --> 00:01:24,760
with what topic you're going
to do for your final project.

35
00:01:24,760 --> 00:01:26,710
If you are thinking
about final projects,

36
00:01:26,710 --> 00:01:29,770
you can find some
info on the website,

37
00:01:29,770 --> 00:01:31,840
but note that the info
that's there at the moment

38
00:01:31,840 --> 00:01:34,480
is still last
year's information,

39
00:01:34,480 --> 00:01:38,040
and it will be being updated
over the coming week.

40
00:01:38,040 --> 00:01:40,000
We'll also talk about
project mentors.

41
00:01:40,000 --> 00:01:42,790
If you've got ideas of
people who on your own

42
00:01:42,790 --> 00:01:44,720
you can line up as
a mentor, that now

43
00:01:44,720 --> 00:01:46,850
would be a good time
to ask them about it.

44
00:01:46,850 --> 00:01:51,070
And we'll sort of talk about
what the alternatives are.

45
00:01:51,070 --> 00:01:51,570
OK.

46
00:01:51,570 --> 00:01:56,250
So last lecture, I
introduced the idea

47
00:01:56,250 --> 00:01:59,070
of language models, so
probabilistic models that

48
00:01:59,070 --> 00:02:03,300
predict the probability of next
words after a word sequence,

49
00:02:03,300 --> 00:02:05,850
and then we looked at
n-gram language models

50
00:02:05,850 --> 00:02:08,830
and started into recurrent
neural network models.

51
00:02:08,830 --> 00:02:11,070
So today, we're
going to talk more

52
00:02:11,070 --> 00:02:14,880
about the simple RNNs we saw
before, talking about training

53
00:02:14,880 --> 00:02:17,040
RNNs and uses of RNNs.

54
00:02:17,040 --> 00:02:19,770
But then we'll also
look into the problems

55
00:02:19,770 --> 00:02:23,490
that occur with RNNs and
how we might fix them.

56
00:02:23,490 --> 00:02:26,640
These will motivate a more
sophisticated RNN architecture

57
00:02:26,640 --> 00:02:30,930
called LSTMS, and we'll talk
about other more complex RNN

58
00:02:30,930 --> 00:02:35,760
options, bidirectional
RNNs, and multilayer RNNs.

59
00:02:35,760 --> 00:02:38,790
Then next Tuesday,
we're essentially

60
00:02:38,790 --> 00:02:42,000
going to further
exploit and build

61
00:02:42,000 --> 00:02:45,990
on the RNN based architectures
that we've been looking at

62
00:02:45,990 --> 00:02:48,750
to discuss how to build a
neural machine translation

63
00:02:48,750 --> 00:02:51,510
system with the
sequence-to-sequence

64
00:02:51,510 --> 00:02:53,190
and model with attention.

65
00:02:53,190 --> 00:02:55,980
And effectively
it's that model is

66
00:02:55,980 --> 00:02:58,500
what you'll use in
assignment 4, but it also

67
00:02:58,500 --> 00:03:00,420
means that we'll be
using all of the stuff

68
00:03:00,420 --> 00:03:03,320
that we're talking about today.

69
00:03:03,320 --> 00:03:03,820
OK.

70
00:03:03,820 --> 00:03:05,740
So if you remember
from last time,

71
00:03:05,740 --> 00:03:09,460
this was the idea of a simple
recurrent neural network

72
00:03:09,460 --> 00:03:10,640
language model.

73
00:03:10,640 --> 00:03:15,700
So we had a sequence of words
as our context for which we've

74
00:03:15,700 --> 00:03:17,620
looked up word embeddings.

75
00:03:17,620 --> 00:03:20,200
And then the recurrent
neural network model

76
00:03:20,200 --> 00:03:24,580
ran this recurrent layer,
where at each point,

77
00:03:24,580 --> 00:03:27,700
we have a previous hidden
state, which can just

78
00:03:27,700 --> 00:03:31,090
be 0 at the beginning
of a sequence,

79
00:03:31,090 --> 00:03:35,830
and you have feeding it in
to the next hidden state,

80
00:03:35,830 --> 00:03:40,960
the previous hidden state,
and transform the encoding

81
00:03:40,960 --> 00:03:44,740
of a word using this
recurrent neural network

82
00:03:44,740 --> 00:03:48,040
equation that I have on the
left that's very central.

83
00:03:48,040 --> 00:03:52,120
And based on that, you compute
a new hidden representation

84
00:03:52,120 --> 00:03:54,160
for the next time step.

85
00:03:54,160 --> 00:03:59,200
And you can repeat that along
for successive time steps.

86
00:03:59,200 --> 00:04:03,250
Now, we also usually want
our recurrent neural networks

87
00:04:03,250 --> 00:04:04,930
to produce outputs.

88
00:04:04,930 --> 00:04:08,060
So I only show it
at the end here.

89
00:04:08,060 --> 00:04:10,840
But at each time
step we're then also

90
00:04:10,840 --> 00:04:12,520
going to generate an output.

91
00:04:12,520 --> 00:04:16,300
And so to do that, we're
feeding the hidden layer

92
00:04:16,300 --> 00:04:18,040
into a softmax layer.

93
00:04:18,040 --> 00:04:21,190
So we're doing another matrix
multiply, add on a bias,

94
00:04:21,190 --> 00:04:23,350
put it through the
softmax equation.

95
00:04:23,350 --> 00:04:25,360
And that then gives
the probability

96
00:04:25,360 --> 00:04:27,190
distribution over words.

97
00:04:27,190 --> 00:04:30,070
And we can use that to
predict how likely it

98
00:04:30,070 --> 00:04:31,810
is that different
words are going

99
00:04:31,810 --> 00:04:36,740
to occur after the
students opened their.

100
00:04:36,740 --> 00:04:37,240
OK.

101
00:04:37,240 --> 00:04:40,750
So I introduced that
model, but I hadn't really

102
00:04:40,750 --> 00:04:44,500
gone through the specifics of
how we train this model, how

103
00:04:44,500 --> 00:04:46,490
we use it and evaluate it.

104
00:04:46,490 --> 00:04:50,360
So let me go through this now.

105
00:04:50,360 --> 00:04:53,830
So here's how we train
an RNN language model.

106
00:04:53,830 --> 00:04:57,170
We get a big corpus of
text, just a lot of text.

107
00:04:57,170 --> 00:04:59,770
And so we can regard that
as just a long sequence

108
00:04:59,770 --> 00:05:02,980
of words, x1 to xT.

109
00:05:02,980 --> 00:05:07,510
And what we're going to do
is feed it into the RNN-LM.

110
00:05:07,510 --> 00:05:09,760
So for each
position, we're going

111
00:05:09,760 --> 00:05:13,240
to take prefixes
of that sequence.

112
00:05:13,240 --> 00:05:16,060
And based on each
prefix, we're going

113
00:05:16,060 --> 00:05:19,900
to want to predict the
probability distribution

114
00:05:19,900 --> 00:05:23,540
for the word that comes next.

115
00:05:23,540 --> 00:05:26,810
And then we're going
to train our model

116
00:05:26,810 --> 00:05:30,150
by assessing how good
a job we do about that.

117
00:05:30,150 --> 00:05:35,030
And so the loss function we use
is the loss function, normally

118
00:05:35,030 --> 00:05:38,010
referred to as cross-entropy
loss in the literature,

119
00:05:38,010 --> 00:05:40,950
which is this negative
log likelihood loss.

120
00:05:40,950 --> 00:05:45,630
So we are going to predict
some word to come next.

121
00:05:45,630 --> 00:05:49,460
Well, we have a probability
distribution over predictions

122
00:05:49,460 --> 00:05:51,140
of what word comes next.

123
00:05:51,140 --> 00:05:54,210
And actually, there was an
actual next word in the text.

124
00:05:54,210 --> 00:05:56,330
And so we say, well,
what probability

125
00:05:56,330 --> 00:05:57,530
did you give to that word?

126
00:05:57,530 --> 00:06:01,083
And maybe we gave it a
probability estimate of 0.01.

127
00:06:01,083 --> 00:06:03,500
Well, it would have been great
if we'd given a probability

128
00:06:03,500 --> 00:06:05,960
estimate of almost
1, because that

129
00:06:05,960 --> 00:06:08,060
meant we're almost
certain that what

130
00:06:08,060 --> 00:06:10,560
did come next in our model.

131
00:06:10,560 --> 00:06:13,700
And so we'll take a
loss to the extent

132
00:06:13,700 --> 00:06:16,610
that we are giving
the actual next word

133
00:06:16,610 --> 00:06:20,890
a predicted probability
of less than 1.

134
00:06:20,890 --> 00:06:22,870
So then getting an
idea of how well

135
00:06:22,870 --> 00:06:26,860
we're doing over
the entire corpus,

136
00:06:26,860 --> 00:06:30,160
we work out that
loss at each position

137
00:06:30,160 --> 00:06:33,700
and then we work out the average
loss of the entire training

138
00:06:33,700 --> 00:06:34,790
set.

139
00:06:34,790 --> 00:06:36,760
So let's just go
through that again

140
00:06:36,760 --> 00:06:39,860
more graphically in the
next couple of slides.

141
00:06:39,860 --> 00:06:44,440
So down at the bottom,
here is our corpus of text.

142
00:06:44,440 --> 00:06:47,530
We're running it through
our simple recurrent neural

143
00:06:47,530 --> 00:06:48,430
network.

144
00:06:48,430 --> 00:06:52,570
And at each position, we've
predicting a probability

145
00:06:52,570 --> 00:06:55,120
distribution over words.

146
00:06:55,120 --> 00:07:00,050
We then say, well,
actually, in each position,

147
00:07:00,050 --> 00:07:01,900
we know what word
is actually next.

148
00:07:01,900 --> 00:07:06,310
So when we're at time step
1, the actual next word

149
00:07:06,310 --> 00:07:09,610
is students, because we can see
it just to the right of us here

150
00:07:09,610 --> 00:07:11,440
and we say, what
probability estimate

151
00:07:11,440 --> 00:07:15,280
did you give to students and to
the extent that it's not high?

152
00:07:15,280 --> 00:07:16,780
I.e., it's not 1.

153
00:07:16,780 --> 00:07:18,160
We take a loss.

154
00:07:18,160 --> 00:07:20,710
And then we go on
to the time step 2,

155
00:07:20,710 --> 00:07:23,290
and we say, well,
at time step 2,

156
00:07:23,290 --> 00:07:26,650
you predict the probability
distribution over words.

157
00:07:26,650 --> 00:07:28,605
The actual next word is opened.

158
00:07:28,605 --> 00:07:29,980
So to the extent
that you haven't

159
00:07:29,980 --> 00:07:32,230
given the high
probability to open,

160
00:07:32,230 --> 00:07:35,230
you take a loss and
then that repeats.

161
00:07:35,230 --> 00:07:39,100
And time step 4, we're hoping
the model will predict their.

162
00:07:39,100 --> 00:07:43,360
At time step 4, we are hoping
the model will predict exams.

163
00:07:43,360 --> 00:07:49,000
And then to work out our overall
loss within averaging out

164
00:07:49,000 --> 00:07:51,610
per time step loss.

165
00:07:51,610 --> 00:07:55,690
So in a way this is a
pretty obvious thing to do.

166
00:07:55,690 --> 00:07:59,050
But note that there is
a little subtlety here.

167
00:07:59,050 --> 00:08:01,600
And in particular,
this algorithm

168
00:08:01,600 --> 00:08:04,910
is referred to in the
literature as teacher forcing.

169
00:08:04,910 --> 00:08:07,420
And so what does that mean?

170
00:08:07,420 --> 00:08:09,220
Well, you can
imagine what you can

171
00:08:09,220 --> 00:08:12,280
do with a recurrent
neural network is,

172
00:08:12,280 --> 00:08:15,340
say, okay, just
start generating.

173
00:08:15,340 --> 00:08:17,650
Maybe I'll give you a
hint as to where to start.

174
00:08:17,650 --> 00:08:20,290
I'll say the sentence
starts, the students,

175
00:08:20,290 --> 00:08:24,970
and then let it run and see
what it generates coming next.

176
00:08:24,970 --> 00:08:27,820
It might start
saying, the students

177
00:08:27,820 --> 00:08:29,560
have been locked
out of the classroom

178
00:08:29,560 --> 00:08:31,630
or whatever it is, right?

179
00:08:31,630 --> 00:08:34,270
And that, we could say
is, well, that's not

180
00:08:34,270 --> 00:08:36,370
very close to what
the actual text says.

181
00:08:36,370 --> 00:08:38,530
And somehow we want
to learn from that.

182
00:08:38,530 --> 00:08:40,870
And if you go in that
direction, there's

183
00:08:40,870 --> 00:08:42,820
a space of things
you can do that leads

184
00:08:42,820 --> 00:08:47,240
into more complex algorithms
such as reinforcement learning.

185
00:08:47,240 --> 00:08:51,550
But from the perspective of
training these neural models,

186
00:08:51,550 --> 00:08:55,400
that's unduly complex
and unnecessary.

187
00:08:55,400 --> 00:08:58,420
So we have this very
simple way of doing

188
00:08:58,420 --> 00:09:01,330
things, which is
what we do is just

189
00:09:01,330 --> 00:09:03,650
predict one time step forward.

190
00:09:03,650 --> 00:09:07,360
So we say, we know that
the prefix is the students.

191
00:09:07,360 --> 00:09:09,310
Predict the probability
distribution

192
00:09:09,310 --> 00:09:10,660
over the next word.

193
00:09:10,660 --> 00:09:13,060
It's good to the extent
that you give probability

194
00:09:13,060 --> 00:09:14,710
mass to opened.

195
00:09:14,710 --> 00:09:17,050
OK, now the prefix is
the students opened.

196
00:09:17,050 --> 00:09:19,630
Predict a probability
distribution

197
00:09:19,630 --> 00:09:20,950
over the next word.

198
00:09:20,950 --> 00:09:24,610
It's good to the extent that you
give probability mass to their.

199
00:09:24,610 --> 00:09:28,480
And so effectively,
at each step,

200
00:09:28,480 --> 00:09:31,380
we are resetting to what
was actually in the corpus.

201
00:09:31,380 --> 00:09:34,060
So you know it's possible
after the students opened,

202
00:09:34,060 --> 00:09:37,240
the model thinks that by
far the most probable thing

203
00:09:37,240 --> 00:09:40,120
to come next is a, or the, say.

204
00:09:40,120 --> 00:09:43,120
I mean, we don't actually
use what the model suggested.

205
00:09:43,120 --> 00:09:46,840
We penalize the model for
not having suggested their.

206
00:09:46,840 --> 00:09:49,420
But then we just go with
what's actually in the corpus

207
00:09:49,420 --> 00:09:50,980
and ask it to predict again.

208
00:09:50,980 --> 00:09:55,760


209
00:09:55,760 --> 00:09:59,420
This is just a
little side thing.

210
00:09:59,420 --> 00:10:01,790
But it's an important
part to know

211
00:10:01,790 --> 00:10:04,700
if you're actually training
your own neural language model.

212
00:10:04,700 --> 00:10:09,800
I sort of present it as one huge
corpus that we chug through.

213
00:10:09,800 --> 00:10:14,480
But in practice, we don't chug
through a whole corpus one

214
00:10:14,480 --> 00:10:15,890
step at a time.

215
00:10:15,890 --> 00:10:18,770
What we do is we
cut the whole corpus

216
00:10:18,770 --> 00:10:22,010
into shorter pieces,
which might commonly

217
00:10:22,010 --> 00:10:24,510
be sentences or
documents or sometimes

218
00:10:24,510 --> 00:10:27,270
they're literally just pieces
that are chopped, right?

219
00:10:27,270 --> 00:10:30,290
So you'll recall that stochastic
gradient descent allows

220
00:10:30,290 --> 00:10:33,020
us to compute a loss and
gradients for a small chunk

221
00:10:33,020 --> 00:10:34,820
of data and update.

222
00:10:34,820 --> 00:10:37,820
So what we do is we
take these small pieces,

223
00:10:37,820 --> 00:10:41,930
compute gradients from those,
and update weights and repeat.

224
00:10:41,930 --> 00:10:46,040
And in particular, we get a
lot more speed and efficiency

225
00:10:46,040 --> 00:10:48,740
in training if we
aren't actually

226
00:10:48,740 --> 00:10:52,250
doing an update for just
one sentence at a time

227
00:10:52,250 --> 00:10:54,560
but actually a
batch of sentences.

228
00:10:54,560 --> 00:10:56,840
So typically what
we'll actually do

229
00:10:56,840 --> 00:10:59,810
is we'll feed to the
model 32 sentences,

230
00:10:59,810 --> 00:11:02,600
say, of a similar
length at the same time,

231
00:11:02,600 --> 00:11:06,830
compute gradients for them,
update weights, and then get

232
00:11:06,830 --> 00:11:08,900
another batch of
sentences to train on.

233
00:11:08,900 --> 00:11:11,460


234
00:11:11,460 --> 00:11:12,660
How do we train?

235
00:11:12,660 --> 00:11:16,080
I haven't sort of gone
through the details of this.

236
00:11:16,080 --> 00:11:19,770
I mean, in one sense, the answer
is just like we talked about

237
00:11:19,770 --> 00:11:21,360
in lecture 3.

238
00:11:21,360 --> 00:11:24,090
We used backpropagation to
get gradients and update

239
00:11:24,090 --> 00:11:25,380
parameters.

240
00:11:25,380 --> 00:11:29,340
But let's take at least a minute
to go through the differences

241
00:11:29,340 --> 00:11:33,690
and subtleties of the
recurrent neural network case.

242
00:11:33,690 --> 00:11:35,820
And the central
thing that's a bit--

243
00:11:35,820 --> 00:11:39,870
as before, we're
going to take our loss

244
00:11:39,870 --> 00:11:41,940
and we're going to
back propagate it

245
00:11:41,940 --> 00:11:46,110
to all of the parameters of the
network, everything from word

246
00:11:46,110 --> 00:11:48,790
embeddings to biases, et cetera.

247
00:11:48,790 --> 00:11:51,780
But the central bit that's
a little bit different

248
00:11:51,780 --> 00:11:58,050
and is more complicated is that
we have this WH matrix that

249
00:11:58,050 --> 00:12:01,830
runs along the sequence
that we keep on applying

250
00:12:01,830 --> 00:12:05,200
to update our hidden state.

251
00:12:05,200 --> 00:12:09,270
So what's the derivative
of Jt of theta

252
00:12:09,270 --> 00:12:13,170
with respect to the
repeated weight matrix Wh?

253
00:12:13,170 --> 00:12:17,790
And well, the answer to
that is that what we do

254
00:12:17,790 --> 00:12:22,400
is we look at it
in each position

255
00:12:22,400 --> 00:12:29,810
and work out what the partials
are of Jt with respect to Wh

256
00:12:29,810 --> 00:12:34,940
in position 1 or position 2,
position 3, position 4, et

257
00:12:34,940 --> 00:12:36,890
cetera, right
along the sequence.

258
00:12:36,890 --> 00:12:39,740
And we just sum up
all of those partials.

259
00:12:39,740 --> 00:12:46,880
And that gives us partial for
Jt with respect to Wh overall.

260
00:12:46,880 --> 00:12:51,290
So the answer for
recurrent neural networks

261
00:12:51,290 --> 00:12:53,840
is the gradient with
respect to repeated

262
00:12:53,840 --> 00:12:56,510
weight in our current
network is the sum

263
00:12:56,510 --> 00:13:01,420
of the gradient with respect
to each time it appears.

264
00:13:01,420 --> 00:13:06,440
And let me just then go through
a little why that is the case.

265
00:13:06,440 --> 00:13:11,150
But before I do that, let
me just note one gocha.

266
00:13:11,150 --> 00:13:14,030
I mean, it's just not
the case that this

267
00:13:14,030 --> 00:13:20,210
means it equals t
times the partial of Jt

268
00:13:20,210 --> 00:13:22,040
with respect to Wh.

269
00:13:22,040 --> 00:13:25,820
Because we're using Wh
here, here, here, here, here

270
00:13:25,820 --> 00:13:27,300
through the sequence.

271
00:13:27,300 --> 00:13:29,720
And for each of the
places we use it,

272
00:13:29,720 --> 00:13:31,820
there's a different
upstream gradient

273
00:13:31,820 --> 00:13:33,260
that's being fed into it.

274
00:13:33,260 --> 00:13:35,690
So each of the
values in this sum

275
00:13:35,690 --> 00:13:39,890
will be completely
different from each other.

276
00:13:39,890 --> 00:13:44,010
Well, why we get this
answer is essentially

277
00:13:44,010 --> 00:13:49,560
a consequence of what we talked
about in the third lecture.

278
00:13:49,560 --> 00:13:52,280
So to take the simplest
case of it, right,

279
00:13:52,280 --> 00:13:55,340
that if you have a
multivariable function f(x,

280
00:13:55,340 --> 00:14:00,920
y) and you have two
single variable functions,

281
00:14:00,920 --> 00:14:05,960
x(t) and y(t) which are
fed one input t, well,

282
00:14:05,960 --> 00:14:12,020
then the simple version of
working out the derivative

283
00:14:12,020 --> 00:14:16,460
of this function is you take
the derivative down one path

284
00:14:16,460 --> 00:14:19,500
and you take the derivative
down the other path.

285
00:14:19,500 --> 00:14:23,060
And so, in the
slides in lecture 3,

286
00:14:23,060 --> 00:14:26,420
that was what was summarized
on a couple of slides

287
00:14:26,420 --> 00:14:30,230
by the slogan gradient
sum at outward branches.

288
00:14:30,230 --> 00:14:33,080
So t has outward branches.

289
00:14:33,080 --> 00:14:36,830
And so you take gradient
here on the left, gradient

290
00:14:36,830 --> 00:14:39,720
on the right, and you
sum them together.

291
00:14:39,720 --> 00:14:43,430
And so really what's happening
with a recurrent neural network

292
00:14:43,430 --> 00:14:47,760
is just many pieces
generalization of this.

293
00:14:47,760 --> 00:14:50,930
So we have one Wh
matrix and we're

294
00:14:50,930 --> 00:14:55,700
using it to keep on updating the
hidden state at time 1, time 2,

295
00:14:55,700 --> 00:14:58,850
time 3, right through time t.

296
00:14:58,850 --> 00:15:03,410
And so what we're going
to get is that this

297
00:15:03,410 --> 00:15:07,550
has a lot of outward branches.

298
00:15:07,550 --> 00:15:11,000
And we're going to
sum the gradient path

299
00:15:11,000 --> 00:15:12,950
at each one of them.

300
00:15:12,950 --> 00:15:16,580
But what is this
gradient path here?

301
00:15:16,580 --> 00:15:19,880
It kind of goes down here
and then goes down there.

302
00:15:19,880 --> 00:15:23,630
But actually the bottom
part is that we're just

303
00:15:23,630 --> 00:15:27,000
using Wh at each position.

304
00:15:27,000 --> 00:15:31,310
So we have the partial
Wh used at position i

305
00:15:31,310 --> 00:15:34,430
with respect to
the partial of Wh

306
00:15:34,430 --> 00:15:37,100
which is just our weight
matrix for our recurrent neural

307
00:15:37,100 --> 00:15:37,920
network.

308
00:15:37,920 --> 00:15:39,650
So that's just one.

309
00:15:39,650 --> 00:15:43,500
Because we're just using
the same matrix everywhere.

310
00:15:43,500 --> 00:15:49,160
And so we are just then summing
the partials in each position

311
00:15:49,160 --> 00:15:49,900
that we use that.

312
00:15:49,900 --> 00:15:53,350


313
00:15:53,350 --> 00:15:56,110
OK, practically, what
does that mean in terms

314
00:15:56,110 --> 00:15:59,000
of how you compute this?

315
00:15:59,000 --> 00:16:02,980
Well, if you're doing
it by hand, what happens

316
00:16:02,980 --> 00:16:06,970
is you start at the end just
like the general lecture 3

317
00:16:06,970 --> 00:16:07,960
story.

318
00:16:07,960 --> 00:16:14,500
You work out derivatives with
respect to the hidden layer

319
00:16:14,500 --> 00:16:19,030
and then with respect to
Wh at the last time step.

320
00:16:19,030 --> 00:16:23,170
And so that gives you
one update for Wh.

321
00:16:23,170 --> 00:16:25,540
But then you continue
passing the gradient back

322
00:16:25,540 --> 00:16:28,430
to the t minus 1 time step.

323
00:16:28,430 --> 00:16:31,190
And after a couple more
steps of the chain rule,

324
00:16:31,190 --> 00:16:34,330
you get another update for Wh.

325
00:16:34,330 --> 00:16:38,320
And you simply sum that onto
your previous update for Wh.

326
00:16:38,320 --> 00:16:41,140
And then you go to ht minus 2.

327
00:16:41,140 --> 00:16:43,210
You get another update for Wh.

328
00:16:43,210 --> 00:16:45,910
And you sum that onto
your update for Wh.

329
00:16:45,910 --> 00:16:48,970
And you go back all
the way and you sum up

330
00:16:48,970 --> 00:16:50,920
the gradients as you go.

331
00:16:50,920 --> 00:16:55,990
And that gives you a
total update for Wh.

332
00:16:55,990 --> 00:16:58,780
And so there's sort
of two tricks here.

333
00:16:58,780 --> 00:17:01,630
And I'll just mention
the two tricks.

334
00:17:01,630 --> 00:17:05,230
You have to kind of separately
sum the updates for Wh.

335
00:17:05,230 --> 00:17:09,108
And then once you've finished,
apply them all at once.

336
00:17:09,108 --> 00:17:11,259
You don't want to actually
be changing the Wh

337
00:17:11,260 --> 00:17:12,608
matrix as you go.

338
00:17:12,608 --> 00:17:15,519
Because that's then
invalid because

339
00:17:15,520 --> 00:17:19,780
the forward calculations were
done with the constant Wh

340
00:17:19,780 --> 00:17:25,760
that you had from the previous
state all through the network.

341
00:17:25,760 --> 00:17:28,030
The second trick
is, well, if you're

342
00:17:28,030 --> 00:17:31,420
doing this for sentences,
you can normally just go back

343
00:17:31,420 --> 00:17:33,680
to the beginning
of the sentence.

344
00:17:33,680 --> 00:17:36,400
But if you've got
very long sequences,

345
00:17:36,400 --> 00:17:38,530
this can really slow
you down if you're

346
00:17:38,530 --> 00:17:40,720
having to sort of run
this algorithm back

347
00:17:40,720 --> 00:17:43,850
for a huge amount of time.

348
00:17:43,850 --> 00:17:45,910
So something people
commonly do is

349
00:17:45,910 --> 00:17:49,840
what's called truncated back
propagation through time where

350
00:17:49,840 --> 00:17:52,480
you choose some
constant, say 20,

351
00:17:52,480 --> 00:17:54,010
and you say, well,
I'm just going

352
00:17:54,010 --> 00:17:58,900
to run this back propagation
for 20 time steps, sum those 20

353
00:17:58,900 --> 00:18:01,720
gradients, and
then I'm just done.

354
00:18:01,720 --> 00:18:06,280
That's what I'll update
the Wh matrix with.

355
00:18:06,280 --> 00:18:07,735
And that works just fine.

356
00:18:07,735 --> 00:18:10,440


357
00:18:10,440 --> 00:18:17,940
So now given a corpus, we
can train a simple RNN.

358
00:18:17,940 --> 00:18:20,410
So that's good progress.

359
00:18:20,410 --> 00:18:25,190
But this is a model that can
also generate text in general.

360
00:18:25,190 --> 00:18:27,510
So how do we generate text?

361
00:18:27,510 --> 00:18:30,600
Well, just like in our
n-gram language model,

362
00:18:30,600 --> 00:18:34,110
we're going to generate
text by repeated sampling.

363
00:18:34,110 --> 00:18:39,700
So we're going to start
off with an initial state

364
00:18:39,700 --> 00:18:44,830
and this slide is imperfect.

365
00:18:44,830 --> 00:18:48,500
So the initial state
for the hidden state

366
00:18:48,500 --> 00:18:52,290
is normally just taken
as a zero vector.

367
00:18:52,290 --> 00:18:55,620
And while then we need to have
something for a first input.

368
00:18:55,620 --> 00:18:57,480
And on this slide,
the first input

369
00:18:57,480 --> 00:18:59,610
is shown as the first word, my.

370
00:18:59,610 --> 00:19:01,650
And if you want to
feed a starting point,

371
00:19:01,650 --> 00:19:03,540
you could feed my.

372
00:19:03,540 --> 00:19:05,340
But a lot of the
time, you'd like

373
00:19:05,340 --> 00:19:07,230
to generate a
sentence from nothing.

374
00:19:07,230 --> 00:19:09,780
And if you want to do
that, what's conventional

375
00:19:09,780 --> 00:19:13,260
is to additionally have a
beginning of sequence token

376
00:19:13,260 --> 00:19:14,820
which is a special token.

377
00:19:14,820 --> 00:19:17,220
So you'll feed in the
beginning of sequence token

378
00:19:17,220 --> 00:19:19,650
in at the beginning
as the first token.

379
00:19:19,650 --> 00:19:21,180
It has an embedding.

380
00:19:21,180 --> 00:19:24,990
And then you use the RNN update.

381
00:19:24,990 --> 00:19:28,770
And then you generate using
the softmax a next word.

382
00:19:28,770 --> 00:19:33,450
And well, you generate a
probability distribution

383
00:19:33,450 --> 00:19:34,650
over next words.

384
00:19:34,650 --> 00:19:37,320
And then at that point,
you sample from that.

385
00:19:37,320 --> 00:19:40,260
And it chooses some
word like favorite.

386
00:19:40,260 --> 00:19:43,890
And so then, the trick
is for doing generation

387
00:19:43,890 --> 00:19:47,310
that you take this word that
you sampled and you copy it

388
00:19:47,310 --> 00:19:51,510
back down to the input and then
you feed it in as an input.

389
00:19:51,510 --> 00:19:55,050
Next step of your RNN,
sample from the softmax,

390
00:19:55,050 --> 00:19:58,920
get another word, and just keep
repeating this over and over

391
00:19:58,920 --> 00:19:59,670
again.

392
00:19:59,670 --> 00:20:02,580
And you start
generating the text.

393
00:20:02,580 --> 00:20:07,920
And how you end is as well as
having a beginning of sequence

394
00:20:07,920 --> 00:20:10,530
special symbol, you
usually have an end

395
00:20:10,530 --> 00:20:12,300
of sequence special symbol.

396
00:20:12,300 --> 00:20:15,600
And at some point, the
recurrent neural network

397
00:20:15,600 --> 00:20:19,110
will generate the end
of sequence symbol.

398
00:20:19,110 --> 00:20:21,760
And then you say, OK, I'm done.

399
00:20:21,760 --> 00:20:25,310
I'm finished generating text.

400
00:20:25,310 --> 00:20:30,230
So before going on for more
of the difficult content

401
00:20:30,230 --> 00:20:33,590
of the lecture, we can just have
a little bit of fun with this

402
00:20:33,590 --> 00:20:37,460
and try training up
and generating text

403
00:20:37,460 --> 00:20:39,560
with our recurrent
neural network model.

404
00:20:39,560 --> 00:20:41,480
So you can generate--
you can train

405
00:20:41,480 --> 00:20:43,920
an RNN on any kind of text.

406
00:20:43,920 --> 00:20:46,790
And so that means one of the
fun things that you can do

407
00:20:46,790 --> 00:20:50,150
is generate text
in different styles

408
00:20:50,150 --> 00:20:51,980
based on what you
could train it from.

409
00:20:51,980 --> 00:20:54,580


410
00:20:54,580 --> 00:20:58,950
So here, Harry Potter,
there is a fair amount

411
00:20:58,950 --> 00:21:00,150
of corpus of text.

412
00:21:00,150 --> 00:21:03,540
So you can train an RNN-LM
L on the Harry Potter books.

413
00:21:03,540 --> 00:21:06,480
And then say, go off
and generate some text.

414
00:21:06,480 --> 00:21:08,610
And it'll generate
text like this.

415
00:21:08,610 --> 00:21:11,100
"Sorry," Harry
shouted, panicking--

416
00:21:11,100 --> 00:21:13,950
"I'll leave those brooms
in London, are they?"

417
00:21:13,950 --> 00:21:16,980
"No idea," said Nearly Headless
Nick, casting low close

418
00:21:16,980 --> 00:21:19,890
by Cedric, carrying the
last bit of treacle Charms

419
00:21:19,890 --> 00:21:22,110
from Harry's shoulder,
and to answer him

420
00:21:22,110 --> 00:21:24,240
the common room perched
upon it, four arms

421
00:21:24,240 --> 00:21:27,270
held a shining knob from
when the spider hadn't

422
00:21:27,270 --> 00:21:28,500
felt it seemed.

423
00:21:28,500 --> 00:21:31,470
He reached the teams too.

424
00:21:31,470 --> 00:21:34,260
Well, so on the one
hand, that's still

425
00:21:34,260 --> 00:21:37,050
kind of a bit
incoherent as a story.

426
00:21:37,050 --> 00:21:40,590
On the other hand, it sort
of sounds like Harry Potter.

427
00:21:40,590 --> 00:21:44,010
It's certainly the kind of
vocabulary and constructions

428
00:21:44,010 --> 00:21:45,270
it uses.

429
00:21:45,270 --> 00:21:49,320
And I think you'd agree that,
even though it gets sort

430
00:21:49,320 --> 00:21:53,130
of incoherent, it's sort
of more coherent than what

431
00:21:53,130 --> 00:21:55,890
we got from an
n-gram language model

432
00:21:55,890 --> 00:22:00,630
when I showed a generation
in the last lecture.

433
00:22:00,630 --> 00:22:04,740
You can choose a very
different style of text.

434
00:22:04,740 --> 00:22:09,690
So you could, instead, train the
model on a bunch of cook books.

435
00:22:09,690 --> 00:22:11,820
And if you do that,
you can then say,

436
00:22:11,820 --> 00:22:16,110
generate based on what you've
learned about cook books.

437
00:22:16,110 --> 00:22:17,920
And it'll just
generate a recipe.

438
00:22:17,920 --> 00:22:22,980
So here's a recipe,
chocolate ranch barbecue.

439
00:22:22,980 --> 00:22:25,560
Categories, yield six servings.

440
00:22:25,560 --> 00:22:29,250
2 tablespoons of
Parmesan cheese, chopped.

441
00:22:29,250 --> 00:22:31,050
1 cup of coconut milk.

442
00:22:31,050 --> 00:22:32,940
Three eggs beaten.

443
00:22:32,940 --> 00:22:35,730
Place each pasta
over layers of lumps.

444
00:22:35,730 --> 00:22:39,990
Shape mixture into the moderate
oven and simmer untill firm.

445
00:22:39,990 --> 00:22:44,130
Serve hot in bodied fresh,
mustard, orange, and cheese.

446
00:22:44,130 --> 00:22:46,920
Combine the cheese
and salt together,

447
00:22:46,920 --> 00:22:49,860
the dough in a large
skillet, add the ingredients

448
00:22:49,860 --> 00:22:52,920
and stir in the
chocolate and pepper.

449
00:22:52,920 --> 00:22:56,860
So, you know, this
recipe makes no sense.

450
00:22:56,860 --> 00:22:58,860
And it's sufficiently
incoherent.

451
00:22:58,860 --> 00:23:00,870
There's actually even
no danger that you'll

452
00:23:00,870 --> 00:23:02,940
try cooking this at home.

453
00:23:02,940 --> 00:23:08,520
But something that's interesting
is although this really just

454
00:23:08,520 --> 00:23:11,400
isn't a recipe and
the things that

455
00:23:11,400 --> 00:23:13,980
are done in the
instructions have

456
00:23:13,980 --> 00:23:19,260
no relation to the ingredients,
the thing that's interesting

457
00:23:19,260 --> 00:23:22,920
that it has learned is this
recurrent neural network model

458
00:23:22,920 --> 00:23:27,300
is that it's really mastered the
overall structure of a recipe.

459
00:23:27,300 --> 00:23:29,730
It knows the recipe has a title.

460
00:23:29,730 --> 00:23:32,980
It often tells you about
how many people it serves.

461
00:23:32,980 --> 00:23:34,740
It lists the ingredients.

462
00:23:34,740 --> 00:23:37,350
And then it has
instructions to make it.

463
00:23:37,350 --> 00:23:40,650
So that's sort of fairly
impressive in some sense

464
00:23:40,650 --> 00:23:44,650
for high level text structuring.

465
00:23:44,650 --> 00:23:47,290
So the one other thing
I wanted to mention

466
00:23:47,290 --> 00:23:51,070
was when I say you can
train an RNN language

467
00:23:51,070 --> 00:23:53,920
model on any kind of text, the
other difference from where

468
00:23:53,920 --> 00:23:55,810
we were in n-gram
language models

469
00:23:55,810 --> 00:23:57,940
was on n-gram
language models, that

470
00:23:57,940 --> 00:24:00,340
just meant counting
n-grams and meant

471
00:24:00,340 --> 00:24:04,420
it took 2 minutes,
even on a large corpus,

472
00:24:04,420 --> 00:24:05,950
with any modern computer.

473
00:24:05,950 --> 00:24:08,170
Training your RNN
LM actually can then

474
00:24:08,170 --> 00:24:10,270
be a time intensive activity.

475
00:24:10,270 --> 00:24:12,520
And you can spend
hours doing that,

476
00:24:12,520 --> 00:24:15,580
as you might find next
week when you're training

477
00:24:15,580 --> 00:24:16,960
machine translation models.

478
00:24:16,960 --> 00:24:19,770


479
00:24:19,770 --> 00:24:23,460
OK, how do we decide if
our models are good or not?

480
00:24:23,460 --> 00:24:26,200


481
00:24:26,200 --> 00:24:30,750
So the standard evaluation
metric for language models

482
00:24:30,750 --> 00:24:33,750
is what's called perplexity.

483
00:24:33,750 --> 00:24:41,520
And what perplexity is is kind
of like when you were training

484
00:24:41,520 --> 00:24:45,450
your model, you use
teacher forcing over

485
00:24:45,450 --> 00:24:49,530
a piece of text that's a
different piece of text which

486
00:24:49,530 --> 00:24:52,290
isn't text that was
in the training data.

487
00:24:52,290 --> 00:24:55,140
And you say, well,
given a sequence

488
00:24:55,140 --> 00:24:59,130
of t words, what
probability do you give

489
00:24:59,130 --> 00:25:01,620
to the actual t plus 1-th word?

490
00:25:01,620 --> 00:25:05,040
And you repeat that
at each position.

491
00:25:05,040 --> 00:25:08,430
And then you take the
inverse of that probability

492
00:25:08,430 --> 00:25:14,460
and raise it to the 1 on t for
the length of your text sample.

493
00:25:14,460 --> 00:25:17,220
And that number
is the perplexity.

494
00:25:17,220 --> 00:25:22,330
So it's a geometric mean of
the inverse probabilities.

495
00:25:22,330 --> 00:25:26,020
Now after that explanation,
perhaps an easier way

496
00:25:26,020 --> 00:25:32,340
to think of it is that
the perplexity is simply

497
00:25:32,340 --> 00:25:34,560
the cross entropy
loss that I introduced

498
00:25:34,560 --> 00:25:36,730
before exponentiated.

499
00:25:36,730 --> 00:25:40,020


500
00:25:40,020 --> 00:25:42,790
But it's now the
other way around.

501
00:25:42,790 --> 00:25:46,110
So low perplexity is better.

502
00:25:46,110 --> 00:25:47,790
So there's actually
an interesting story

503
00:25:47,790 --> 00:25:50,190
about these perplexities.

504
00:25:50,190 --> 00:25:54,570
So a famous figure in the
development of probabilistic

505
00:25:54,570 --> 00:25:57,900
and machine learning approaches
to natural language processing

506
00:25:57,900 --> 00:26:01,620
is Fred Jelinek who
died a few years ago.

507
00:26:01,620 --> 00:26:08,960
And he was trying to interest
people in the idea of using

508
00:26:08,960 --> 00:26:12,530
probability models and machine
learning for natural language

509
00:26:12,530 --> 00:26:16,820
processing at a time-- i.e.,
this is the 1970s and early

510
00:26:16,820 --> 00:26:18,530
1980s--

511
00:26:18,530 --> 00:26:22,790
when nearly everyone
in the field of AI

512
00:26:22,790 --> 00:26:26,330
was still in the thrall
of logic-based models

513
00:26:26,330 --> 00:26:29,090
and blackboard
architectures and things

514
00:26:29,090 --> 00:26:31,850
like that for artificial
intelligence systems.

515
00:26:31,850 --> 00:26:35,630
And so Fred Jelinek was
actually an information theorist

516
00:26:35,630 --> 00:26:41,510
by background, then got
interested in working

517
00:26:41,510 --> 00:26:44,700
with speech and
then language data.

518
00:26:44,700 --> 00:26:47,690
So at that time,
the stuff that's--

519
00:26:47,690 --> 00:26:52,280
this sort of exponential or
using cross-entropy losses

520
00:26:52,280 --> 00:26:55,760
was completely bread and
butter to Fred Jelinek

521
00:26:55,760 --> 00:26:58,520
that he'd found
that no one in AI

522
00:26:58,520 --> 00:27:01,650
could understand the
bottom half of the slide.

523
00:27:01,650 --> 00:27:04,280
And so he wanted to come
up with something simple

524
00:27:04,280 --> 00:27:07,160
that AI people at that
time could understand.

525
00:27:07,160 --> 00:27:11,930
And perplexity has a kind
of a simple interpretation

526
00:27:11,930 --> 00:27:13,350
you can tell people.

527
00:27:13,350 --> 00:27:18,170
So if you get a
perplexity of 53,

528
00:27:18,170 --> 00:27:22,670
that means how uncertain
you are of the next word

529
00:27:22,670 --> 00:27:25,550
is equivalent to the
uncertainty of that

530
00:27:25,550 --> 00:27:32,180
you're tossing a 53-sided dice
and it coming up as a 1, right?

531
00:27:32,180 --> 00:27:35,660
So that was kind of an
easy, simple metric.

532
00:27:35,660 --> 00:27:40,400
And so he introduced that idea.

533
00:27:40,400 --> 00:27:42,560
But you know, I
guess things stick.

534
00:27:42,560 --> 00:27:47,420
And to this day, everyone
evaluates their language models

535
00:27:47,420 --> 00:27:49,310
by providing perplexity numbers.

536
00:27:49,310 --> 00:27:52,220
And so here are some
perplexity numbers.

537
00:27:52,220 --> 00:27:55,730
So traditional n-gram
language models commonly

538
00:27:55,730 --> 00:27:58,220
had perplexities over 100.

539
00:27:58,220 --> 00:28:02,240
But if you made them really
big and really carefully,

540
00:28:02,240 --> 00:28:06,050
you could get them down
into a number like 67.

541
00:28:06,050 --> 00:28:09,140
As people started to
build more advanced

542
00:28:09,140 --> 00:28:12,470
recurrent neural networks,
especially as they

543
00:28:12,470 --> 00:28:15,500
moved beyond the kind
of simple RNNs, which

544
00:28:15,500 --> 00:28:18,080
is all I've shown you
so far, which one of is

545
00:28:18,080 --> 00:28:23,750
in the second line of the slide,
into LSTMs, which I talk about

546
00:28:23,750 --> 00:28:26,660
later in this
course, that people

547
00:28:26,660 --> 00:28:30,440
started producing much
better perplexities.

548
00:28:30,440 --> 00:28:34,040
And here, we're getting
perplexities down to 30.

549
00:28:34,040 --> 00:28:37,580
And this is results actually
from a few years ago.

550
00:28:37,580 --> 00:28:39,650
So nowadays people
get perplexities

551
00:28:39,650 --> 00:28:42,110
of even lower than 30.

552
00:28:42,110 --> 00:28:45,050
You have to be realistic in
what you can expect, right?

553
00:28:45,050 --> 00:28:47,990
Because if you are
just generating a text,

554
00:28:47,990 --> 00:28:51,020
some words are
almost determined.

555
00:28:51,020 --> 00:28:58,260
So if it's something like
Sue gave the man a napkin.

556
00:28:58,260 --> 00:29:00,150
He said thank.

557
00:29:00,150 --> 00:29:02,970
Basically 100%, you should
be able to say the word that

558
00:29:02,970 --> 00:29:05,060
comes next is you.

559
00:29:05,060 --> 00:29:09,000
And so that you can
predict really well.

560
00:29:09,000 --> 00:29:10,920
But if it's a lot
of other sentences

561
00:29:10,920 --> 00:29:16,170
like, he looked out the
window and saw something,

562
00:29:16,170 --> 00:29:19,110
right, no probability
model in the world

563
00:29:19,110 --> 00:29:20,970
can give a very good
estimate of what's

564
00:29:20,970 --> 00:29:23,410
actually going to be
coming next at that point.

565
00:29:23,410 --> 00:29:26,940
And so that gives us the
sort of residual uncertainty

566
00:29:26,940 --> 00:29:29,400
that leads to perplexities
that are, on average,

567
00:29:29,400 --> 00:29:31,992
might be around 20 or something.

568
00:29:31,992 --> 00:29:35,300


569
00:29:35,300 --> 00:29:38,660
So we've talked a lot
about language models now.

570
00:29:38,660 --> 00:29:42,300
Why should we care
about language modeling?

571
00:29:42,300 --> 00:29:46,490
Well, there's sort of an
intellectual scientific answer

572
00:29:46,490 --> 00:29:49,850
that says, this is a
benchmark task, right?

573
00:29:49,850 --> 00:29:53,360
If want we want to do is
build machine learning models

574
00:29:53,360 --> 00:29:55,820
of language and our
ability to predict

575
00:29:55,820 --> 00:29:59,930
what word will come next in the
context, that shows how well we

576
00:29:59,930 --> 00:30:02,900
understand both the
structure of language

577
00:30:02,900 --> 00:30:05,120
and the structure
of the human world

578
00:30:05,120 --> 00:30:08,000
that language talks about.

579
00:30:08,000 --> 00:30:10,310
But there's a much
more practical answer

580
00:30:10,310 --> 00:30:15,470
than that, which is
language models are really

581
00:30:15,470 --> 00:30:18,650
the secret tool of natural
language processing.

582
00:30:18,650 --> 00:30:23,390
So if you're talking
to any NLP person

583
00:30:23,390 --> 00:30:27,680
and you've got
almost any task, it's

584
00:30:27,680 --> 00:30:31,790
quite likely they'll say, oh,
I bet we could use a language

585
00:30:31,790 --> 00:30:33,030
model for that.

586
00:30:33,030 --> 00:30:35,390
And so language
models are sort of

587
00:30:35,390 --> 00:30:41,480
used as not the whole solution,
but a part of almost any task.

588
00:30:41,480 --> 00:30:44,900
Any task that involves
generating or estimating

589
00:30:44,900 --> 00:30:46,710
the probability of text.

590
00:30:46,710 --> 00:30:50,900
So you can use it for predictive
typing, speech recognition,

591
00:30:50,900 --> 00:30:54,860
grammar correction, identifying
authors, machine translation,

592
00:30:54,860 --> 00:30:57,470
summarization, dialogue,
just about anything

593
00:30:57,470 --> 00:31:00,230
you do with natural language
involves language models.

594
00:31:00,230 --> 00:31:04,140
And we'll see examples of
that in following classes,

595
00:31:04,140 --> 00:31:07,250
including next Tuesday where
we're using language models

596
00:31:07,250 --> 00:31:10,430
for machine translation.

597
00:31:10,430 --> 00:31:13,670
OK, so a language
model is just a system

598
00:31:13,670 --> 00:31:16,370
that predicts the next word.

599
00:31:16,370 --> 00:31:21,240
A recurrent neural network is
a family of neural networks

600
00:31:21,240 --> 00:31:25,160
which can take sequential
input of any length,

601
00:31:25,160 --> 00:31:28,370
they reuse the same
weights to generate

602
00:31:28,370 --> 00:31:32,060
a hidden state and optionally,
but commonly, an output

603
00:31:32,060 --> 00:31:33,800
on each step.

604
00:31:33,800 --> 00:31:37,290
Note that these two
things are different.

605
00:31:37,290 --> 00:31:40,440
So we've talked about
two ways that you

606
00:31:40,440 --> 00:31:42,240
could build language models.

607
00:31:42,240 --> 00:31:45,450
But one of them is
RNNs being a great way.

608
00:31:45,450 --> 00:31:48,370
But RNNs can also be used
for a lot of other things.

609
00:31:48,370 --> 00:31:51,090
So let me just quickly
preview a few other things

610
00:31:51,090 --> 00:31:53,500
you can do with RNNs.

611
00:31:53,500 --> 00:31:55,410
So there are lots
of tasks that people

612
00:31:55,410 --> 00:31:58,740
want to do in NLP which
are referred to as sequence

613
00:31:58,740 --> 00:32:03,600
tagging tasks where we'd
like to take words of text

614
00:32:03,600 --> 00:32:05,940
and do some kind
of classification

615
00:32:05,940 --> 00:32:07,320
along the sequence.

616
00:32:07,320 --> 00:32:12,060
So one simple common one is
to give words parts of speech.

617
00:32:12,060 --> 00:32:14,790
The is a determiner.
startled is an adjective.

618
00:32:14,790 --> 00:32:15,780
Cat is a noun.

619
00:32:15,780 --> 00:32:17,640
Knocked is a verb.

620
00:32:17,640 --> 00:32:20,190
And well, you can do
this straightforwardly

621
00:32:20,190 --> 00:32:23,340
by using a recurrent
neural network

622
00:32:23,340 --> 00:32:26,910
as a sequential
classifier where it's now

623
00:32:26,910 --> 00:32:29,040
going to generate
parts of speech

624
00:32:29,040 --> 00:32:31,970
rather than the next word.

625
00:32:31,970 --> 00:32:34,640
You can use a recurrent
neural network, the sentiment

626
00:32:34,640 --> 00:32:36,330
classification.

627
00:32:36,330 --> 00:32:38,480
Well, this time
we don't actually

628
00:32:38,480 --> 00:32:43,040
want to generate an output
at each word, necessarily.

629
00:32:43,040 --> 00:32:46,350
But we want to know what the
overall sentiment looks like.

630
00:32:46,350 --> 00:32:49,400
So somehow we want to get
out a sentence encoding

631
00:32:49,400 --> 00:32:53,330
that we can perhaps put through
another neural network layer

632
00:32:53,330 --> 00:32:57,810
to judge whether the sentence
is positive or negative.

633
00:32:57,810 --> 00:32:59,780
Well, the simplest
way to do that

634
00:32:59,780 --> 00:33:04,190
is to think, well,
after I've run

635
00:33:04,190 --> 00:33:09,230
my LSTM through the
whole sentence, actually

636
00:33:09,230 --> 00:33:12,830
this final hidden state, it's
encoded the whole sentence.

637
00:33:12,830 --> 00:33:15,020
Because remember I
updated that hidden state

638
00:33:15,020 --> 00:33:16,970
based on each previous word.

639
00:33:16,970 --> 00:33:19,520
And so you could say that
this is the whole meaning

640
00:33:19,520 --> 00:33:20,670
of the sentence.

641
00:33:20,670 --> 00:33:24,470
So let's just say that
is the sentence encoding

642
00:33:24,470 --> 00:33:29,060
and then put an extra classifier
layer on that with something

643
00:33:29,060 --> 00:33:31,550
like a softmax classifier.

644
00:33:31,550 --> 00:33:33,140
That method has been used.

645
00:33:33,140 --> 00:33:35,320
And it actually works
reasonably well.

646
00:33:35,320 --> 00:33:38,120
And if you sort of train
this model end to end,

647
00:33:38,120 --> 00:33:42,230
well, it's actually then
motivated to preserve sentiment

648
00:33:42,230 --> 00:33:45,650
information in the hidden state
of the recurrent neural network

649
00:33:45,650 --> 00:33:48,080
because that will allow
it to better predict

650
00:33:48,080 --> 00:33:50,780
the sentiment of the
whole sentence, which

651
00:33:50,780 --> 00:33:53,990
is the final task and
hence loss function

652
00:33:53,990 --> 00:33:55,700
that we're giving the network.

653
00:33:55,700 --> 00:33:59,000
But it turns out that you can
commonly do better than that

654
00:33:59,000 --> 00:34:02,720
by actually doing things
like feeding all hidden

655
00:34:02,720 --> 00:34:05,450
states into the
sentence encoding,

656
00:34:05,450 --> 00:34:08,120
perhaps by making
the sentence encoding

657
00:34:08,120 --> 00:34:13,190
an element-wise max or an
element-wise mean of all

658
00:34:13,190 --> 00:34:14,449
the hidden states.

659
00:34:14,449 --> 00:34:16,969
Because this then
more symmetrically

660
00:34:16,969 --> 00:34:20,960
encodes the hidden state
over each time step.

661
00:34:20,960 --> 00:34:24,690


662
00:34:24,690 --> 00:34:27,719
Another big use of
recurrent neural networks

663
00:34:27,719 --> 00:34:33,540
is what I'll call language
encoder module uses.

664
00:34:33,540 --> 00:34:37,469
So any time you have some
text, for example here

665
00:34:37,469 --> 00:34:41,639
we have a question, what
nationality was Beethoven,

666
00:34:41,639 --> 00:34:45,750
we'd like to construct some
kind of neural representation

667
00:34:45,750 --> 00:34:46,719
of this.

668
00:34:46,719 --> 00:34:51,929
So one way to do it is to run
a recurrent neural network

669
00:34:51,929 --> 00:34:52,770
over it.

670
00:34:52,770 --> 00:34:56,159
And then just like last
time, to either take

671
00:34:56,159 --> 00:35:00,150
the final hidden state or
take some kind of function

672
00:35:00,150 --> 00:35:03,600
of all the hidden states
and say that's the sentence

673
00:35:03,600 --> 00:35:05,130
representation.

674
00:35:05,130 --> 00:35:08,670
And we could do the same
thing for the context.

675
00:35:08,670 --> 00:35:10,710
So for question
answering, we're going

676
00:35:10,710 --> 00:35:14,190
to build some more neural
net structure on top of that.

677
00:35:14,190 --> 00:35:17,460
And we'll learn more about
that in a couple of weeks

678
00:35:17,460 --> 00:35:19,710
when we have the question
answering lecture.

679
00:35:19,710 --> 00:35:23,130
But the key thing is
what we've built so far,

680
00:35:23,130 --> 00:35:26,170
we used to get sentence
representation.

681
00:35:26,170 --> 00:35:28,820
So it's a language
encoder module.

682
00:35:28,820 --> 00:35:31,590
So that was the
language encoding part.

683
00:35:31,590 --> 00:35:36,540
We can also use RNNs to
decode into language.

684
00:35:36,540 --> 00:35:39,870
And that's commonly used in
speech recognition, machine

685
00:35:39,870 --> 00:35:42,030
translation, summarization.

686
00:35:42,030 --> 00:35:44,130
So if we have a
speech recognizer,

687
00:35:44,130 --> 00:35:46,470
the input is an audio signal.

688
00:35:46,470 --> 00:35:50,190
And what we want to do is
decode that into language.

689
00:35:50,190 --> 00:35:53,490
Well, what we could do
is use some function

690
00:35:53,490 --> 00:35:55,500
of the input, which
is probably itself

691
00:35:55,500 --> 00:36:00,870
going to be a neural net,
as the initial hidden

692
00:36:00,870 --> 00:36:04,890
state of our RNN-LM.

693
00:36:04,890 --> 00:36:09,480
And then we say, start
generating text based on that.

694
00:36:09,480 --> 00:36:12,690
And so it should
then, we generate word

695
00:36:12,690 --> 00:36:15,480
at a time by the method
that we just looked at.

696
00:36:15,480 --> 00:36:18,130
We turn the speech into text.

697
00:36:18,130 --> 00:36:21,450
So this is an example of a
conditional language model

698
00:36:21,450 --> 00:36:25,110
because we're now generating
text conditioned on the speech

699
00:36:25,110 --> 00:36:26,100
signal.

700
00:36:26,100 --> 00:36:30,630
And a lot of the time, you can
do interesting more advanced

701
00:36:30,630 --> 00:36:32,670
things with recurrent
neural networks

702
00:36:32,670 --> 00:36:36,280
by building conditional
language models.

703
00:36:36,280 --> 00:36:39,150
Another place you can use
conditional language models

704
00:36:39,150 --> 00:36:42,270
is for text
classification tasks,

705
00:36:42,270 --> 00:36:44,760
including sentiment
classification.

706
00:36:44,760 --> 00:36:49,560
So if you can condition
your language model

707
00:36:49,560 --> 00:36:51,360
based on a kind
of sentiment, you

708
00:36:51,360 --> 00:36:53,680
can build a kind of
classifier for that.

709
00:36:53,680 --> 00:36:56,190
And another use that we'll
see a lot of next class

710
00:36:56,190 --> 00:36:57,780
is for machine translation.

711
00:36:57,780 --> 00:37:00,290


712
00:37:00,290 --> 00:37:03,020
OK, so that's the
end of the intro

713
00:37:03,020 --> 00:37:09,580
to doing things with recurrent
neural networks and language

714
00:37:09,580 --> 00:37:10,520
models.

715
00:37:10,520 --> 00:37:14,560
Now, I want to move on and
tell you about the fact

716
00:37:14,560 --> 00:37:17,590
that everything
is not perfect and

717
00:37:17,590 --> 00:37:20,470
these recurrent
neural networks tend

718
00:37:20,470 --> 00:37:22,570
to have a couple of problems.

719
00:37:22,570 --> 00:37:24,890
And we'll talk about those.

720
00:37:24,890 --> 00:37:26,890
And then, in part,
that will then

721
00:37:26,890 --> 00:37:29,800
motivate coming up with a
more advanced recurrent neural

722
00:37:29,800 --> 00:37:31,730
network architecture.

723
00:37:31,730 --> 00:37:34,630
So the first problem
to be mentioned

724
00:37:34,630 --> 00:37:39,010
is the idea of what's
called vanishing gradients.

725
00:37:39,010 --> 00:37:40,700
And what does that mean?

726
00:37:40,700 --> 00:37:43,540
Well, at the end
of our sequence,

727
00:37:43,540 --> 00:37:48,580
we have some overall loss
that we're calculating.

728
00:37:48,580 --> 00:37:53,830
And well, what we want to do
is back propagate that loss.

729
00:37:53,830 --> 00:37:57,590
And we want to back propagate
it right along the sequence.

730
00:37:57,590 --> 00:38:03,040
And so we're working out the
partials of J4 with respect

731
00:38:03,040 --> 00:38:05,170
to the hidden state at time 1.

732
00:38:05,170 --> 00:38:06,700
And when we have
a longer sequence,

733
00:38:06,700 --> 00:38:10,150
we'll be working out the
partials of J20 with respect

734
00:38:10,150 --> 00:38:12,430
to the hidden state at time 1.

735
00:38:12,430 --> 00:38:14,880
And how do we do that?

736
00:38:14,880 --> 00:38:19,960
Well, how we do it is by
composition and the chain rule.

737
00:38:19,960 --> 00:38:25,780
We've got a big long chain
rule along the whole sequence.

738
00:38:25,780 --> 00:38:32,110
Well, if we're doing that, we're
multiplying a ton of things

739
00:38:32,110 --> 00:38:33,800
together.

740
00:38:33,800 --> 00:38:37,540
And so the danger of
what tends to happen

741
00:38:37,540 --> 00:38:43,030
is that as we do these
multiplications, a lot of time

742
00:38:43,030 --> 00:38:47,320
these partials between
successive hidden states

743
00:38:47,320 --> 00:38:49,250
become small.

744
00:38:49,250 --> 00:38:52,240
And so what happens
is as we go along,

745
00:38:52,240 --> 00:38:56,170
the gradient gets smaller
and smaller and smaller,

746
00:38:56,170 --> 00:38:58,750
and starts to peter out.

747
00:38:58,750 --> 00:39:01,960
And to the extent
that it peters out,

748
00:39:01,960 --> 00:39:05,260
well, then we've kind of
gotten no upstream gradient

749
00:39:05,260 --> 00:39:09,100
and therefore we won't be
changing the parameters at all.

750
00:39:09,100 --> 00:39:12,370
And that turns out to
be pretty problematic.

751
00:39:12,370 --> 00:39:14,980


752
00:39:14,980 --> 00:39:18,960
So the next couple of slides
sort of say a little bit

753
00:39:18,960 --> 00:39:24,450
about the why and
how this happens.

754
00:39:24,450 --> 00:39:30,700
What's presented here is a
kind of only semi-formal wave

755
00:39:30,700 --> 00:39:34,535
your hands at the kind of
problems that you might expect.

756
00:39:34,535 --> 00:39:35,910
If you really want
to sort of get

757
00:39:35,910 --> 00:39:38,550
into all the
details of this, you

758
00:39:38,550 --> 00:39:40,560
should look at the
couple of papers

759
00:39:40,560 --> 00:39:42,180
that are mentioned
in small print

760
00:39:42,180 --> 00:39:43,990
at the bottom of the slide.

761
00:39:43,990 --> 00:39:46,020
But at any rate, if
you remember that this

762
00:39:46,020 --> 00:39:50,520
is our basic recurrent
neural network equation,

763
00:39:50,520 --> 00:39:52,020
let's consider an easy case.

764
00:39:52,020 --> 00:39:57,330
Suppose we sort of get
rid of our non-linearity

765
00:39:57,330 --> 00:40:01,140
and just assume that it's
an identity function.

766
00:40:01,140 --> 00:40:02,850
OK, so then when
we're working out

767
00:40:02,850 --> 00:40:05,190
the partials of the
hidden state with respect

768
00:40:05,190 --> 00:40:09,270
to the previous hidden
state, we can work those out

769
00:40:09,270 --> 00:40:13,180
in the usual way according
to the chain rule.

770
00:40:13,180 --> 00:40:21,030
And then, if sigma is simply
the identity function,

771
00:40:21,030 --> 00:40:23,800
well then, everything
gets really easy for us.

772
00:40:23,800 --> 00:40:28,050
So only-- the sigma
just goes away.

773
00:40:28,050 --> 00:40:33,820
And only the first term
involves h at time t minus 1.

774
00:40:33,820 --> 00:40:36,220
So the later terms go away.

775
00:40:36,220 --> 00:40:41,080
And so our gradient
ends up as Wh.

776
00:40:41,080 --> 00:40:44,490
Well, that's doing it
for just one time step.

777
00:40:44,490 --> 00:40:46,230
What happens when
you want to work out

778
00:40:46,230 --> 00:40:49,530
these partials a number
of time steps away?

779
00:40:49,530 --> 00:40:55,800
So we want to work it out,
the partial of time step i

780
00:40:55,800 --> 00:40:59,040
with respect to j.

781
00:40:59,040 --> 00:41:02,580
Well, what we end
up with is a product

782
00:41:02,580 --> 00:41:06,900
of the partials of
successive time steps.

783
00:41:06,900 --> 00:41:13,350
And well, each of those
is coming out as Wh

784
00:41:13,350 --> 00:41:21,331
and so we end up getting Wh
raised to the l-th power.

785
00:41:21,331 --> 00:41:24,090
And while our
potential problem is

786
00:41:24,090 --> 00:41:27,990
that if Wh is small
in some sense,

787
00:41:27,990 --> 00:41:30,840
then this term gets
exponentially problematic.

788
00:41:30,840 --> 00:41:34,830
It becomes vanishingly
small as our sequence length

789
00:41:34,830 --> 00:41:36,600
becomes long.

790
00:41:36,600 --> 00:41:39,240
Well, what can we mean by small?

791
00:41:39,240 --> 00:41:42,915
Well, a matrix is small
if its eigenvalues

792
00:41:42,915 --> 00:41:44,940
are all less than 1.

793
00:41:44,940 --> 00:41:47,640
So we can rewrite
what's happening

794
00:41:47,640 --> 00:41:49,950
with this success
of multiplication,

795
00:41:49,950 --> 00:41:53,670
using eigenvalues
and eigenvectors.

796
00:41:53,670 --> 00:41:57,300
And I should say that all
eigenvalues less than 1

797
00:41:57,300 --> 00:41:59,910
is sufficient, but not
necessary conditioned for what

798
00:41:59,910 --> 00:42:02,130
I'm about to say, right?

799
00:42:02,130 --> 00:42:08,240
So we can rewrite things using
the eigenvectors as a basis.

800
00:42:08,240 --> 00:42:15,630
And if we do that, we end
up getting the eigenvalues

801
00:42:15,630 --> 00:42:17,890
being raised to the lth power.

802
00:42:17,890 --> 00:42:22,360
And so if all of our
eigenvalues are less than 1,

803
00:42:22,360 --> 00:42:24,750
if we're taking a
number less than 1,

804
00:42:24,750 --> 00:42:26,610
then raising it
to the lth power,

805
00:42:26,610 --> 00:42:31,080
that's going to approach 0
as the sequence length grows.

806
00:42:31,080 --> 00:42:34,350
And so the gradient vanishes.

807
00:42:34,350 --> 00:42:36,690
OK, now the reality is
more complex than that.

808
00:42:36,690 --> 00:42:40,620
Because actually, we always use
a nonlinear activation sigma.

809
00:42:40,620 --> 00:42:45,270
But in principle, it's sort
of the same thing apart from,

810
00:42:45,270 --> 00:42:48,450
we have to consider in the
effect of the nonlinear

811
00:42:48,450 --> 00:42:51,440
activation.

812
00:42:51,440 --> 00:42:55,700
OK, so why is this a problem
that the gradients disappear?

813
00:42:55,700 --> 00:43:00,132
Well, suppose we're wanting to
look at the influence of time

814
00:43:00,132 --> 00:43:05,960
steps well in the future, on
the representations we want

815
00:43:05,960 --> 00:43:08,090
to have early in the sentence.

816
00:43:08,090 --> 00:43:12,560
Well, what's happening
late in the sentence

817
00:43:12,560 --> 00:43:16,130
just isn't going to be giving
much information about, what

818
00:43:16,130 --> 00:43:21,800
we should be storing in
the h at time 1 vector.

819
00:43:21,800 --> 00:43:25,850
Whereas on the other hand,
the loss at timestep 2

820
00:43:25,850 --> 00:43:28,340
is going to be giving
a lot of information

821
00:43:28,340 --> 00:43:33,260
at what should be stored in the
hidden vector at timestep 1.

822
00:43:33,260 --> 00:43:38,060
So the end result of
that is that what happens

823
00:43:38,060 --> 00:43:41,720
is that, these
simple RNN's are very

824
00:43:41,720 --> 00:43:45,360
good at modeling nearby effects.

825
00:43:45,360 --> 00:43:49,700
But they're not good at all
at modeling long-term effects.

826
00:43:49,700 --> 00:43:54,500
Because the gradient signal from
far away is just lost too much.

827
00:43:54,500 --> 00:43:58,430
And therefore, the model
never effectively gets

828
00:43:58,430 --> 00:44:02,750
to learn what information
from far away,

829
00:44:02,750 --> 00:44:06,180
it would be useful to
preserve into the future.

830
00:44:06,180 --> 00:44:08,600
So let's consider
that concretely,

831
00:44:08,600 --> 00:44:12,740
for the example of language
models that we've worked on.

832
00:44:12,740 --> 00:44:15,800
So here's a piece of text.

833
00:44:15,800 --> 00:44:17,570
When she tried to
print her tickets,

834
00:44:17,570 --> 00:44:20,300
she found that the
printer was out of toner.

835
00:44:20,300 --> 00:44:23,240
She went to the stationery
store to buy more toner.

836
00:44:23,240 --> 00:44:25,100
It was very overpriced.

837
00:44:25,100 --> 00:44:27,240
After installing the
toner into the printer,

838
00:44:27,240 --> 00:44:29,450
she finally printed her--

839
00:44:29,450 --> 00:44:32,120
and well, you're all
smart human beings.

840
00:44:32,120 --> 00:44:35,750
I trust you can all guess what
the word that comes next is.

841
00:44:35,750 --> 00:44:38,300
It should be tickets.

842
00:44:38,300 --> 00:44:41,630
But well, the problem
is that for the RNN

843
00:44:41,630 --> 00:44:44,570
to start to learn
cases like this,

844
00:44:44,570 --> 00:44:49,190
it would have to carry through
in its hidden state, a memory

845
00:44:49,190 --> 00:44:52,460
of the word tickets for
sort of, whatever it is,

846
00:44:52,460 --> 00:44:55,700
about 30 hidden state updates.

847
00:44:55,700 --> 00:44:59,790
And well, we'll train
on this example.

848
00:44:59,790 --> 00:45:03,500
And so we'll be wanting
it to predict tickets

849
00:45:03,500 --> 00:45:04,880
as the next word.

850
00:45:04,880 --> 00:45:08,300
And so a gradient update
will be sent right back

851
00:45:08,300 --> 00:45:12,200
through the hidden states
of the LSTM corresponding

852
00:45:12,200 --> 00:45:13,850
to this sentence.

853
00:45:13,850 --> 00:45:18,050
And that should
tell the model, it's

854
00:45:18,050 --> 00:45:20,690
good to preserve information
about the word tickets,

855
00:45:20,690 --> 00:45:22,740
because that might be
useful in the future.

856
00:45:22,740 --> 00:45:25,370
Here it was useful
in the future.

857
00:45:25,370 --> 00:45:28,850
But the problem is that, the
gradient signal will just

858
00:45:28,850 --> 00:45:33,230
become far too weak out
after a bunch of words.

859
00:45:33,230 --> 00:45:36,840
And it just never
learns that dependency.

860
00:45:36,840 --> 00:45:40,040
And so what we find
in practice is,

861
00:45:40,040 --> 00:45:42,170
the model is just
unable to predict

862
00:45:42,170 --> 00:45:45,265
similar long-distance
dependencies at test time.

863
00:45:45,265 --> 00:45:49,550
And I've spent quite a long
time on vanishing gradients.

864
00:45:49,550 --> 00:45:51,410
And then really,
vanishing gradients

865
00:45:51,410 --> 00:45:57,890
are the big problem
in practice with using

866
00:45:57,890 --> 00:46:01,520
recurrent neural networks
or the long sequences.

867
00:46:01,520 --> 00:46:04,310
But I have to do
justice to the fact

868
00:46:04,310 --> 00:46:06,710
that you could actually, also
have the opposite problem.

869
00:46:06,710 --> 00:46:09,690
You can also have
exploding gradients.

870
00:46:09,690 --> 00:46:15,940
So if a gradient becomes too
big, that's also a problem.

871
00:46:15,940 --> 00:46:20,650
And it's a problem because
the stochastic gradient update

872
00:46:20,650 --> 00:46:23,150
step becomes too big, right?

873
00:46:23,150 --> 00:46:25,760
So remember, our
parameter update

874
00:46:25,760 --> 00:46:30,720
is based on the product of the
learning rate and the gradient.

875
00:46:30,720 --> 00:46:33,815
So if your gradient is huge,
right, you've calculated,

876
00:46:33,815 --> 00:46:36,080
oh, it's got a
lot of slope here,

877
00:46:36,080 --> 00:46:42,080
this has a slope of 10,000,
then your parameter update

878
00:46:42,080 --> 00:46:45,560
can be arbitrarily large.

879
00:46:45,560 --> 00:46:47,600
And that's potentially
problematic.

880
00:46:47,600 --> 00:46:51,230
That can cause a bad update,
where you take a huge step

881
00:46:51,230 --> 00:46:56,300
and you end up at a weird and
bad parameter configuration.

882
00:46:56,300 --> 00:46:59,120
So you think you're
coming up with a--

883
00:46:59,120 --> 00:47:01,070
to a steep hill to climb.

884
00:47:01,070 --> 00:47:03,380
And while you want to
be climbing the hill

885
00:47:03,380 --> 00:47:05,270
to high likelihood,
that actually

886
00:47:05,270 --> 00:47:11,450
the gradient is so steep that
you make an enormous update.

887
00:47:11,450 --> 00:47:14,180
And then suddenly, your
parameters are over in Iowa.

888
00:47:14,180 --> 00:47:16,340
And you've lost your
hill altogether.

889
00:47:16,340 --> 00:47:17,920
There's also the
practical difficulty

890
00:47:17,920 --> 00:47:19,420
that we only have
so much resolution

891
00:47:19,420 --> 00:47:21,810
in our floating point numbers.

892
00:47:21,810 --> 00:47:25,070
So if your gradient
gets too steep,

893
00:47:25,070 --> 00:47:29,120
you start getting not a numbers
in your calculations, which

894
00:47:29,120 --> 00:47:32,700
ruin all your hard
training work.

895
00:47:32,700 --> 00:47:35,460
We use kind of an
easy fix to this,

896
00:47:35,460 --> 00:47:39,230
which is called gradient
clipping, which is, we

897
00:47:39,230 --> 00:47:42,560
choose some reasonable number.

898
00:47:42,560 --> 00:47:45,410
And we say, we're just not going
to deal with gradients that

899
00:47:45,410 --> 00:47:47,050
are bigger than this number.

900
00:47:47,050 --> 00:47:49,640
A commonly used number is 20.

901
00:47:49,640 --> 00:47:53,090
Some thing that's got a range
of spread, but not that high.

902
00:47:53,090 --> 00:47:54,950
You can use 10, 100, something.

903
00:47:54,950 --> 00:47:58,500
We're sort of in that range.

904
00:47:58,500 --> 00:48:03,020
And if the norm of the gradient
is greater than that threshold,

905
00:48:03,020 --> 00:48:07,280
we simply just scale it down,
which means that we then

906
00:48:07,280 --> 00:48:09,950
make a smaller gradient update.

907
00:48:09,950 --> 00:48:14,720
So we're still moving in
exactly the same direction,

908
00:48:14,720 --> 00:48:17,870
but we're taking a smaller step.

909
00:48:17,870 --> 00:48:22,430
So doing this gradient
clipping is important.

910
00:48:22,430 --> 00:48:24,035
But it's an easy
problem to solve.

911
00:48:24,035 --> 00:48:28,350


912
00:48:28,350 --> 00:48:32,400
OK, so the thing that we've
still got left to solve

913
00:48:32,400 --> 00:48:39,300
is, how to really solve this
problem of vanishing gradients.

914
00:48:39,300 --> 00:48:43,080
So the problem is,
yeah, these RNNs just

915
00:48:43,080 --> 00:48:47,250
can't preserve information
over many timesteps.

916
00:48:47,250 --> 00:48:51,880
And one way to think
about that intuitively

917
00:48:51,880 --> 00:48:56,410
is, at each timestep,
we have a hidden state.

918
00:48:56,410 --> 00:49:01,060
And the hidden state is
being completely changed

919
00:49:01,060 --> 00:49:02,800
at each timestep.

920
00:49:02,800 --> 00:49:06,580
And it's being changed in
a multiplicative manner

921
00:49:06,580 --> 00:49:09,250
by multiplying by Wh,
and then putting it

922
00:49:09,250 --> 00:49:14,080
through a nonlinearity.

923
00:49:14,080 --> 00:49:17,890
Maybe we could make
some more progress,

924
00:49:17,890 --> 00:49:21,790
if we could more
flexibly maintain

925
00:49:21,790 --> 00:49:26,290
a memory in our recurrent
neural network, which

926
00:49:26,290 --> 00:49:31,090
we can manipulate in a
more flexible manner, that

927
00:49:31,090 --> 00:49:34,940
allows us to more easily
preserve information.

928
00:49:34,940 --> 00:49:39,640
And so this was an idea that
people started thinking about.

929
00:49:39,640 --> 00:49:42,730
And actually, they started
thinking about it a long time

930
00:49:42,730 --> 00:49:48,560
ago in the late 1990s.

931
00:49:48,560 --> 00:49:51,320
And Hochreiter and
Schmidhuber came up

932
00:49:51,320 --> 00:49:57,440
with this idea that got called
long short-term memory RNNs.

933
00:49:57,440 --> 00:50:01,880
So a solution to the problem
of vanishing gradients.

934
00:50:01,880 --> 00:50:04,730
I mean, so this 1997
paper is the paper

935
00:50:04,730 --> 00:50:07,160
you always see cited for LSTMs.

936
00:50:07,160 --> 00:50:13,310
But actually, in terms of what
we now understand as an LSTM,

937
00:50:13,310 --> 00:50:15,360
it was missing part of it.

938
00:50:15,360 --> 00:50:18,590
In fact, it's missing what,
in retrospect, has turned out

939
00:50:18,590 --> 00:50:24,050
to be the most important
part of the modern LSTM.

940
00:50:24,050 --> 00:50:27,050
So really, in some
sense, the real paper

941
00:50:27,050 --> 00:50:32,000
that the modern LSTM is due to
is this slightly later paper

942
00:50:32,000 --> 00:50:36,350
by Gers, still Schmidhuber,
and Cummins from 2000,

943
00:50:36,350 --> 00:50:38,720
which additionally
introduces the forget gate

944
00:50:38,720 --> 00:50:42,450
that I'll explain in a minute.

945
00:50:42,450 --> 00:50:46,320
Yeah, so this was
some very clever stuff

946
00:50:46,320 --> 00:50:48,300
that was introduced.

947
00:50:48,300 --> 00:50:52,890
And it turned out later to
have an enormous impact.

948
00:50:52,890 --> 00:50:56,100
If I just diverge from
the technical part

949
00:50:56,100 --> 00:50:58,440
for one more moment.

950
00:50:58,440 --> 00:51:02,520
That for those of
you who, these days,

951
00:51:02,520 --> 00:51:06,990
think that mastering neural
networks is the path to fame

952
00:51:06,990 --> 00:51:11,280
and fortune, the funny
thing is, at the time

953
00:51:11,280 --> 00:51:15,750
that this work was done, that
just was not true, right?

954
00:51:15,750 --> 00:51:19,860
Very few people were
interested in neural networks.

955
00:51:19,860 --> 00:51:23,520
And although long short-term
memories have turned out

956
00:51:23,520 --> 00:51:26,820
to be one of the most important,
successful, and influential

957
00:51:26,820 --> 00:51:32,940
ideas in neural networks
for the following 25 years,

958
00:51:32,940 --> 00:51:36,330
really, the original authors
didn't get recognition

959
00:51:36,330 --> 00:51:36,970
for that.

960
00:51:36,970 --> 00:51:39,120
So both of them
are now professors

961
00:51:39,120 --> 00:51:41,370
at German universities.

962
00:51:41,370 --> 00:51:47,370
But Hochreiter moved over
into doing bioinformatics work

963
00:51:47,370 --> 00:51:50,700
to find something to do.

964
00:51:50,700 --> 00:51:55,880
And Gers, actually, is doing
kind of multimedia studies.

965
00:51:55,880 --> 00:51:58,290
So that's the fates of history.

966
00:51:58,290 --> 00:52:01,650


967
00:52:01,650 --> 00:52:03,840
OK, so what is an LSTM?

968
00:52:03,840 --> 00:52:08,490
So a crucial
innovation of an LSTM

969
00:52:08,490 --> 00:52:11,220
is to say, well,
rather than just having

970
00:52:11,220 --> 00:52:15,660
one hidden vector in
the recurrent model,

971
00:52:15,660 --> 00:52:23,010
we're going to build a model
with two hidden vectors

972
00:52:23,010 --> 00:52:26,140
at each timestep,
one of which is still

973
00:52:26,140 --> 00:52:29,580
called the hidden state
h, and the other of which

974
00:52:29,580 --> 00:52:32,880
is called the cell state.

975
00:52:32,880 --> 00:52:37,172
Now arguably, in retrospect,
these were named wrongly.

976
00:52:37,172 --> 00:52:38,880
Because as you'll see,
when we look at it

977
00:52:38,880 --> 00:52:41,790
in more detail, in
some sense, the cell

978
00:52:41,790 --> 00:52:45,150
is more equivalent to the
hidden state of the simple RNN

979
00:52:45,150 --> 00:52:47,030
than vice versa.

980
00:52:47,030 --> 00:52:50,350
But we're just going with the
names that everybody uses.

981
00:52:50,350 --> 00:52:53,700
So both of these are
vectors of length n.

982
00:52:53,700 --> 00:52:55,350
And it's going to
be the cell that

983
00:52:55,350 --> 00:52:58,830
stores long-term information.

984
00:52:58,830 --> 00:53:00,840
And so we want to
have something that's

985
00:53:00,840 --> 00:53:06,490
more like memory, meaning
like RAM in the computer.

986
00:53:06,490 --> 00:53:10,200
So the cell is designed
so you can read from it,

987
00:53:10,200 --> 00:53:13,050
you can erase parts of
it, and you can write

988
00:53:13,050 --> 00:53:15,990
new information to the cell.

989
00:53:15,990 --> 00:53:19,740
And the interesting
part of an LSTM is then,

990
00:53:19,740 --> 00:53:24,340
it's got control structures
to decide how you do that.

991
00:53:24,340 --> 00:53:26,580
So the selection of
which information

992
00:53:26,580 --> 00:53:29,700
to erase, write, and
read is controlled

993
00:53:29,700 --> 00:53:31,770
by probabilistic gates.

994
00:53:31,770 --> 00:53:35,220
So the gates are also
vectors of length n.

995
00:53:35,220 --> 00:53:39,120
And on each
timestep, we work out

996
00:53:39,120 --> 00:53:41,010
a state for the gate vectors.

997
00:53:41,010 --> 00:53:44,490
So each element of the gate
vectors is a probability.

998
00:53:44,490 --> 00:53:48,810
So they can be open probability,
1, closed probability, 0,

999
00:53:48,810 --> 00:53:50,490
or somewhere in between.

1000
00:53:50,490 --> 00:53:54,750
And their value will be
saying, how much do you erase?

1001
00:53:54,750 --> 00:53:56,490
How much do you write?

1002
00:53:56,490 --> 00:53:58,740
How much do you read?

1003
00:53:58,740 --> 00:54:01,530
And so these are dynamic
gates, with a value

1004
00:54:01,530 --> 00:54:04,140
that's computed based
on the current context.

1005
00:54:04,140 --> 00:54:06,870


1006
00:54:06,870 --> 00:54:10,350
OK, so in this next slide,
we go through the equations

1007
00:54:10,350 --> 00:54:11,115
of an LSTM.

1008
00:54:11,115 --> 00:54:14,320
But following this, there
are some more graphic slides,

1009
00:54:14,320 --> 00:54:17,430
which will probably be
easier to absorb, right?

1010
00:54:17,430 --> 00:54:19,770
So we, again-- just
like before, it's

1011
00:54:19,770 --> 00:54:21,270
a recurrent neural network.

1012
00:54:21,270 --> 00:54:25,350
We have a sequence
of inputs, xt.

1013
00:54:25,350 --> 00:54:27,360
And we're going to,
at each timestep,

1014
00:54:27,360 --> 00:54:30,670
compute a cell state
and a hidden state.

1015
00:54:30,670 --> 00:54:32,170
So how do we do that?

1016
00:54:32,170 --> 00:54:34,840
So firstly, we're
going to compute

1017
00:54:34,840 --> 00:54:37,950
values of the three gates.

1018
00:54:37,950 --> 00:54:40,920
And so we're computing
the gate values,

1019
00:54:40,920 --> 00:54:46,050
using an equation that's
identical to the equation

1020
00:54:46,050 --> 00:54:49,890
for the simple recurrent
neural network.

1021
00:54:49,890 --> 00:54:53,420
But in particular--
whoops, sorry, I'll

1022
00:54:53,420 --> 00:54:55,190
just say what the
gates are first.

1023
00:54:55,190 --> 00:54:58,410
So there's a forget
gate, which we

1024
00:54:58,410 --> 00:55:03,180
will control what is kept in
the cell at the next timestep,

1025
00:55:03,180 --> 00:55:04,980
versus what is forgotten.

1026
00:55:04,980 --> 00:55:07,230
There's an input
gate, which is going

1027
00:55:07,230 --> 00:55:11,940
to determine which parts of
a calculated new cell content

1028
00:55:11,940 --> 00:55:13,980
get written to the cell memory.

1029
00:55:13,980 --> 00:55:16,020
And there's an
output gate, which

1030
00:55:16,020 --> 00:55:19,020
is going to control what
parts of the cell memory

1031
00:55:19,020 --> 00:55:21,810
are moved over into
the hidden state.

1032
00:55:21,810 --> 00:55:26,220
And so each of these is
using the logistic function.

1033
00:55:26,220 --> 00:55:29,280
Because we want them
to be, in each element

1034
00:55:29,280 --> 00:55:32,220
of this vector, a
probability which

1035
00:55:32,220 --> 00:55:34,650
will say whether
to fully forget,

1036
00:55:34,650 --> 00:55:39,230
partially forget,
or fully remember.

1037
00:55:39,230 --> 00:55:41,810
Yeah, and the equation
for each of these

1038
00:55:41,810 --> 00:55:44,790
is exactly like the
simple RNN equation.

1039
00:55:44,790 --> 00:55:47,660
But note, of course, that
we've got different parameters

1040
00:55:47,660 --> 00:55:48,390
for each one.

1041
00:55:48,390 --> 00:55:51,350
So we've got forgetting
weight matrix

1042
00:55:51,350 --> 00:55:57,933
W with a forgetting bias,
and a forgetting multiplier

1043
00:55:57,933 --> 00:55:58,475
of the input.

1044
00:55:58,475 --> 00:56:01,380


1045
00:56:01,380 --> 00:56:05,370
So then we have the other
equations that are really

1046
00:56:05,370 --> 00:56:09,270
the mechanics of the LSTM.

1047
00:56:09,270 --> 00:56:14,620
So we have something that will
calculate a new cell content.

1048
00:56:14,620 --> 00:56:17,130
So this is our candidate update.

1049
00:56:17,130 --> 00:56:20,790
And so for calculating
the candidate update,

1050
00:56:20,790 --> 00:56:25,080
we're, again, essentially using
exactly the same simple RNN

1051
00:56:25,080 --> 00:56:26,100
equation.

1052
00:56:26,100 --> 00:56:29,310
Apart from now, it's
usual to use tahh.

1053
00:56:29,310 --> 00:56:32,220
So you get something that,
as discussed last time,

1054
00:56:32,220 --> 00:56:35,250
is balanced around 0.

1055
00:56:35,250 --> 00:56:40,020
Okay, so then to actually
update things, we use our gates.

1056
00:56:40,020 --> 00:56:44,190
So for our new cell
content, what the idea is,

1057
00:56:44,190 --> 00:56:47,820
is that we want to
remember some, but probably

1058
00:56:47,820 --> 00:56:52,500
not all of what we had in the
cell from previous timesteps.

1059
00:56:52,500 --> 00:56:59,130
And we want to store some, but
probably not all of the value

1060
00:56:59,130 --> 00:57:04,770
that we've calculated
as the new cell update.

1061
00:57:04,770 --> 00:57:11,220
And so the way we do that is, we
take the previous cell content.

1062
00:57:11,220 --> 00:57:17,490
And then, we take its Hadamard
product with the forget vector.

1063
00:57:17,490 --> 00:57:21,270
And then, we add to it
the Hadamard product

1064
00:57:21,270 --> 00:57:25,905
of the input gate times
the candidate cell update.

1065
00:57:25,905 --> 00:57:30,200


1066
00:57:30,200 --> 00:57:33,620
And then for working out
the new hidden state,

1067
00:57:33,620 --> 00:57:38,150
we then work out which
parts of the cell

1068
00:57:38,150 --> 00:57:41,060
to expose in the hidden state.

1069
00:57:41,060 --> 00:57:46,160
And so after taking a tanh
transform of the cell,

1070
00:57:46,160 --> 00:57:49,670
we then take the Hadamard
product with the output gate.

1071
00:57:49,670 --> 00:57:52,280
And that gives us our
hidden representation.

1072
00:57:52,280 --> 00:57:54,470
And it's this hidden
representation

1073
00:57:54,470 --> 00:57:57,890
that we then put
through a softmax layer,

1074
00:57:57,890 --> 00:58:02,210
to generate our next output
of our LSTM recurrent neural

1075
00:58:02,210 --> 00:58:03,170
network.

1076
00:58:03,170 --> 00:58:11,030
Yeah, so the gates and the
things that they're put with

1077
00:58:11,030 --> 00:58:13,070
are vectors of size n.

1078
00:58:13,070 --> 00:58:16,400
And what we're doing is, we're
taking each element of them

1079
00:58:16,400 --> 00:58:20,060
and multiplying them
element-wise to work out

1080
00:58:20,060 --> 00:58:21,380
a new vector.

1081
00:58:21,380 --> 00:58:24,480
And then, we get two vectors
that we're adding together.

1082
00:58:24,480 --> 00:58:28,400
So this way of doing
things element-wise,

1083
00:58:28,400 --> 00:58:32,930
you sort of don't really see in
standard linear algebra course.

1084
00:58:32,930 --> 00:58:35,990
It's referred to as
the Hadamard product.

1085
00:58:35,990 --> 00:58:39,170
It's represented by
some kind of circle.

1086
00:58:39,170 --> 00:58:40,670
Actually, in more
modern work, it's

1087
00:58:40,670 --> 00:58:43,820
been more usual to represent
it with this slightly bigger

1088
00:58:43,820 --> 00:58:47,350
circle, with the dot at the
middle as the Hadamard product

1089
00:58:47,350 --> 00:58:48,800
symbol.

1090
00:58:48,800 --> 00:58:51,780
And someday, I'll change
these slides to be like that.

1091
00:58:51,780 --> 00:58:54,530
But I was lazy in
redoing the equations.

1092
00:58:54,530 --> 00:58:57,350
But the other notation
you do see quite often

1093
00:58:57,350 --> 00:58:59,630
is, just using the
same little circle

1094
00:58:59,630 --> 00:59:01,790
that you use for
function composition,

1095
00:59:01,790 --> 00:59:04,880
to represent Hadamard product.

1096
00:59:04,880 --> 00:59:08,940
Okay, so all of these things
are being done as vectors

1097
00:59:08,940 --> 00:59:10,790
of the same length n.

1098
00:59:10,790 --> 00:59:13,670
And the other thing
that you might notice

1099
00:59:13,670 --> 00:59:20,630
is that the candidate update,
and the forget input and output

1100
00:59:20,630 --> 00:59:24,050
gates, all have a
very similar form.

1101
00:59:24,050 --> 00:59:28,130
The only difference is three
logistics and one tanh.

1102
00:59:28,130 --> 00:59:30,600
And none of them
depend on each other.

1103
00:59:30,600 --> 00:59:34,630
So all four of those can
be calculated in parallel.

1104
00:59:34,630 --> 00:59:37,820
And if you want to have an
efficient LSTM implementation,

1105
00:59:37,820 --> 00:59:40,070
that's what you do.

1106
00:59:40,070 --> 00:59:43,890
OK, so here's the more
graphical presentation of this.

1107
00:59:43,890 --> 00:59:47,300
So these pictures
come from Chris Olah.

1108
00:59:47,300 --> 00:59:50,880
And I guess he did such a
nice job at producing pictures

1109
00:59:50,880 --> 00:59:55,730
for LSTMs, that almost
everyone uses them these days.

1110
00:59:55,730 --> 01:00:00,740
And so this sort of pulls
apart the computation graph

1111
01:00:00,740 --> 01:00:02,190
of an LSTM unit.

1112
01:00:02,190 --> 01:00:08,390
So blowing this up, you've got
from the previous timestep,

1113
01:00:08,390 --> 01:00:14,000
both your cell and
hidden recurrent vectors.

1114
01:00:14,000 --> 01:00:18,840
And so you feed
the hidden vector

1115
01:00:18,840 --> 01:00:23,490
from the previous timestep
and the new input xt

1116
01:00:23,490 --> 01:00:26,490
into the computation of the
gates, which is happening down

1117
01:00:26,490 --> 01:00:27,450
at the bottom.

1118
01:00:27,450 --> 01:00:29,700
So you compute the forget gate.

1119
01:00:29,700 --> 01:00:33,600
And then, you use the forget
gate in a Hadamard product,

1120
01:00:33,600 --> 01:00:37,170
here drawn as actually
a time symbol.

1121
01:00:37,170 --> 01:00:39,570
So forget some cell content.

1122
01:00:39,570 --> 01:00:41,850
You work out the input gate.

1123
01:00:41,850 --> 01:00:47,490
And then, using the input gate
and a regular recurrent neural

1124
01:00:47,490 --> 01:00:51,930
network-like computation, you
can compute candidate new cell

1125
01:00:51,930 --> 01:00:54,030
content.

1126
01:00:54,030 --> 01:00:58,230
And so then, you add
those two together

1127
01:00:58,230 --> 01:01:02,370
to get the new cell content,
which then heads out

1128
01:01:02,370 --> 01:01:05,520
as the new cell
content, at time t.

1129
01:01:05,520 --> 01:01:09,870
But then you also have
worked out an output gate.

1130
01:01:09,870 --> 01:01:14,190
And so then you take
the cell content,

1131
01:01:14,190 --> 01:01:16,410
put it through
another nonlinearity,

1132
01:01:16,410 --> 01:01:21,510
and Hadamard product it
with the output gate.

1133
01:01:21,510 --> 01:01:26,260
And that then gives you
the new hidden state.

1134
01:01:26,260 --> 01:01:29,440
So this is all kind of complex.

1135
01:01:29,440 --> 01:01:33,400
But as to understanding why
something that's different

1136
01:01:33,400 --> 01:01:36,310
is happening here,
the thing to notice

1137
01:01:36,310 --> 01:01:40,930
is that the cell
state from t minus 1

1138
01:01:40,930 --> 01:01:46,540
is passing right through this
to be the cell state at time t,

1139
01:01:46,540 --> 01:01:49,430
without very much
happening to it.

1140
01:01:49,430 --> 01:01:55,180
So some of it is being
deleted by the forget gate.

1141
01:01:55,180 --> 01:01:59,980
And then some new stuff
is being written to it,

1142
01:01:59,980 --> 01:02:04,870
as a result of using this
candidate new cell content.

1143
01:02:04,870 --> 01:02:11,620
But the real secret
of the LSTM is

1144
01:02:11,620 --> 01:02:14,530
that new stuff is
just being added

1145
01:02:14,530 --> 01:02:17,270
to the cell with
an addition, right?

1146
01:02:17,270 --> 01:02:21,280
So in the simple RNN,
at each successive step,

1147
01:02:21,280 --> 01:02:23,410
you are doing a multiplication.

1148
01:02:23,410 --> 01:02:26,770
And that makes it
incredibly difficult

1149
01:02:26,770 --> 01:02:30,880
to learn to preserve
information in the hidden state,

1150
01:02:30,880 --> 01:02:33,110
over a long period of time.

1151
01:02:33,110 --> 01:02:35,230
It's not completely impossible.

1152
01:02:35,230 --> 01:02:37,840
But it's a very
difficult thing to learn.

1153
01:02:37,840 --> 01:02:41,500
Whereas with this new
LSTM architecture,

1154
01:02:41,500 --> 01:02:44,620
it's trivial to preserve
information in the cell

1155
01:02:44,620 --> 01:02:46,690
from one timestep to the next.

1156
01:02:46,690 --> 01:02:49,360
You just don't forget it.

1157
01:02:49,360 --> 01:02:51,580
And it'll carry
right through with,

1158
01:02:51,580 --> 01:02:55,810
perhaps some new stuff
added in to also remember.

1159
01:02:55,810 --> 01:03:00,610
And so that's the sense in
which the cell behaves much more

1160
01:03:00,610 --> 01:03:04,390
like RAM, in a conventional
computer that is storing stuff.

1161
01:03:04,390 --> 01:03:07,360
And extra stuff can
be stored into it.

1162
01:03:07,360 --> 01:03:10,270
And other stuff can be deleted
from it, as you go along.

1163
01:03:10,270 --> 01:03:13,240


1164
01:03:13,240 --> 01:03:15,670
OK, so the LSTM
architecture makes

1165
01:03:15,670 --> 01:03:19,670
it much easier to preserve
information for many timesteps,

1166
01:03:19,670 --> 01:03:20,170
right?

1167
01:03:20,170 --> 01:03:23,210


1168
01:03:23,210 --> 01:03:27,760
So in particular, standard
practice with LSTMs

1169
01:03:27,760 --> 01:03:31,840
is to initialize the forget
gate to a 1 vector, which

1170
01:03:31,840 --> 01:03:36,670
is just so that a starting point
is to say, preserve everything

1171
01:03:36,670 --> 01:03:39,310
from previous timesteps.

1172
01:03:39,310 --> 01:03:42,100
And then, it is then
learning when it's

1173
01:03:42,100 --> 01:03:44,540
appropriate to forget stuff.

1174
01:03:44,540 --> 01:03:45,040
All right.

1175
01:03:45,040 --> 01:03:50,750
In contrast, it's very hard to
get a simple RNN to preserve

1176
01:03:50,750 --> 01:03:53,840
stuff for a very long time.

1177
01:03:53,840 --> 01:03:56,520
I mean, what does
that actually mean?

1178
01:03:56,520 --> 01:04:01,170
Well, I've put down
some numbers here.

1179
01:04:01,170 --> 01:04:05,220
I mean, what you get in practice
depends on a million things.

1180
01:04:05,220 --> 01:04:07,100
It depends on the
nature of your data,

1181
01:04:07,100 --> 01:04:09,080
and how much data
you have, and what

1182
01:04:09,080 --> 01:04:13,100
dimensionality your hidden
states are, blurdy, blurdy,

1183
01:04:13,100 --> 01:04:14,120
blur.

1184
01:04:14,120 --> 01:04:18,050
But just to give you some
idea of what's going on

1185
01:04:18,050 --> 01:04:21,950
is, typically, if you
train a simple recurrent

1186
01:04:21,950 --> 01:04:25,730
neural network, that its
effective memory, its ability

1187
01:04:25,730 --> 01:04:29,060
to be able to use things in the
past to condition the future,

1188
01:04:29,060 --> 01:04:31,190
goes for about seven timesteps.

1189
01:04:31,190 --> 01:04:34,670
You just really can't get it
to remember stuff further back

1190
01:04:34,670 --> 01:04:36,110
in the past than that.

1191
01:04:36,110 --> 01:04:42,335
Whereas for the LSTM,
it's not complete magic.

1192
01:04:42,335 --> 01:04:44,150
It doesn't work forever.

1193
01:04:44,150 --> 01:04:48,770
But it's effectively able
to remember and use things

1194
01:04:48,770 --> 01:04:50,630
from much, much further back.

1195
01:04:50,630 --> 01:04:54,020
So typically, you find
that with an LSTM,

1196
01:04:54,020 --> 01:04:58,340
you can effectively remember and
use things about 100 time steps

1197
01:04:58,340 --> 01:04:58,910
back.

1198
01:04:58,910 --> 01:05:01,700
And that's just
enormously more useful

1199
01:05:01,700 --> 01:05:04,730
for a lot of the natural
language understanding tasks

1200
01:05:04,730 --> 01:05:08,220
that we want to do.

1201
01:05:08,220 --> 01:05:13,440
And so that was precisely what
the LSTM was designed to do.

1202
01:05:13,440 --> 01:05:16,760
And I mean, so in particular,
just going back to its name,

1203
01:05:16,760 --> 01:05:19,760
quite a few people
misparsed its name.

1204
01:05:19,760 --> 01:05:22,400
The idea of its
name was, there's

1205
01:05:22,400 --> 01:05:25,970
a concept of short-term memory,
which comes from psychology.

1206
01:05:25,970 --> 01:05:29,060
And it had been
suggested for simple RNNs

1207
01:05:29,060 --> 01:05:33,800
that the hidden state of the
RNN could be a model of humans'

1208
01:05:33,800 --> 01:05:35,270
short-term memory.

1209
01:05:35,270 --> 01:05:38,690
And then, there would be
something somewhere else

1210
01:05:38,690 --> 01:05:41,310
that would deal with
human long-term memory.

1211
01:05:41,310 --> 01:05:45,020
But while people had found that
this only gave you a very short

1212
01:05:45,020 --> 01:05:50,180
short-term memory, so what
Hochreiter and Schmidhuber were

1213
01:05:50,180 --> 01:05:54,050
interested in was
how we could give--

1214
01:05:54,050 --> 01:05:57,960
construct models with a
long short-term memory.

1215
01:05:57,960 --> 01:06:03,050
And so that, then, gave
us this name of LSTM.

1216
01:06:03,050 --> 01:06:06,730
LSTMs don't guarantee that there
are no vanishing or exploding

1217
01:06:06,730 --> 01:06:07,730
gradients.

1218
01:06:07,730 --> 01:06:10,970
But in practice, they provide--

1219
01:06:10,970 --> 01:06:15,540
they don't tend to explode
nearly the same way again.

1220
01:06:15,540 --> 01:06:18,770
That plus sign is crucial,
rather than a multiplication.

1221
01:06:18,770 --> 01:06:21,110
And so they're a much
more effective way

1222
01:06:21,110 --> 01:06:23,390
of learning long-distance
dependencies.

1223
01:06:23,390 --> 01:06:26,410


1224
01:06:26,410 --> 01:06:30,790
OK, so despite the fact
that LSTMs were developed

1225
01:06:30,790 --> 01:06:38,500
around 1997, 2000, it was
really only in the early 2010s,

1226
01:06:38,500 --> 01:06:42,430
that the world woke up to them
and how successful they were.

1227
01:06:42,430 --> 01:06:46,600
So it was really
around 2013 to 2015

1228
01:06:46,600 --> 01:06:50,020
that LSTMs sort
of hit the world,

1229
01:06:50,020 --> 01:06:52,060
achieving state
of the art results

1230
01:06:52,060 --> 01:06:54,200
on all kinds of problems.

1231
01:06:54,200 --> 01:06:55,960
One of the first
big demonstrations

1232
01:06:55,960 --> 01:07:00,370
was for handwriting recognition,
then speech recognition.

1233
01:07:00,370 --> 01:07:04,310
But then going on to a lot
of natural language tasks,

1234
01:07:04,310 --> 01:07:08,410
including machine translation,
parsing, vision and language

1235
01:07:08,410 --> 01:07:11,170
tasks like image captioning,
as well, of course,

1236
01:07:11,170 --> 01:07:13,180
using them for language models.

1237
01:07:13,180 --> 01:07:18,220
And around these years, LSTMs
became the dominant approach

1238
01:07:18,220 --> 01:07:19,840
for most NLP tasks.

1239
01:07:19,840 --> 01:07:22,450
The easiest way to build
a good strong model

1240
01:07:22,450 --> 01:07:26,470
was to approach the
problem with an LSTM.

1241
01:07:26,470 --> 01:07:30,430
So now in 2021,
actually, LSTMs are

1242
01:07:30,430 --> 01:07:33,190
starting to be supplanted,
or have been supplanted

1243
01:07:33,190 --> 01:07:35,890
by other approaches,
particularly transformer

1244
01:07:35,890 --> 01:07:39,760
models, which we'll get to in
the class in a couple of weeks

1245
01:07:39,760 --> 01:07:40,870
time.

1246
01:07:40,870 --> 01:07:42,940
So this is the sort of
picture you can see.

1247
01:07:42,940 --> 01:07:46,480
So for many years, there's
been a machine translation

1248
01:07:46,480 --> 01:07:47,140
conference.

1249
01:07:47,140 --> 01:07:50,860
And so a bake off competition
called WMT, Workshop

1250
01:07:50,860 --> 01:07:53,180
on Machine Translation.

1251
01:07:53,180 --> 01:07:58,930
So if you look at the
history of that, in WMT 2014,

1252
01:07:58,930 --> 01:08:02,350
there was zero neural
machine translation systems

1253
01:08:02,350 --> 01:08:03,520
in the competition.

1254
01:08:03,520 --> 01:08:06,730
2014 was actually
the first year,

1255
01:08:06,730 --> 01:08:12,610
that the success of LSTMs
for machine translation

1256
01:08:12,610 --> 01:08:15,520
was proven in a
conference paper.

1257
01:08:15,520 --> 01:08:18,670
But nothing occurred
in this competition.

1258
01:08:18,670 --> 01:08:26,770
By 2016, everyone had jumped
on LSTMs as working great.

1259
01:08:26,770 --> 01:08:30,100
And lots of people, including
the winner of the competition,

1260
01:08:30,100 --> 01:08:32,620
was using an LSTM model.

1261
01:08:32,620 --> 01:08:36,850
If you then jump ahead
to 2019, then there's

1262
01:08:36,850 --> 01:08:40,720
relatively little use of LSTMs.

1263
01:08:40,720 --> 01:08:44,510
And the vast majority of people
are now using transformers.

1264
01:08:44,510 --> 01:08:47,380
So things change quickly
in neural network land.

1265
01:08:47,380 --> 01:08:49,795
And I keep on having to
rewrite these lectures.

1266
01:08:49,795 --> 01:08:52,830


1267
01:08:52,830 --> 01:08:55,979
So a quick further note
on vanishing and exploding

1268
01:08:55,979 --> 01:08:56,910
gradients.

1269
01:08:56,910 --> 01:09:00,390
Is it only a problem with
recurrent neural networks?

1270
01:09:00,390 --> 01:09:01,180
It's not.

1271
01:09:01,180 --> 01:09:05,010
It's actually a problem
that also occurs anywhere

1272
01:09:05,010 --> 01:09:07,109
where you have a lot
of depth, including

1273
01:09:07,109 --> 01:09:11,399
feedforward and convolutional
neural networks.

1274
01:09:11,399 --> 01:09:16,020
Any time when you've got long
sequences of chain rules, which

1275
01:09:16,020 --> 01:09:18,359
give you multiplications,
the gradient

1276
01:09:18,359 --> 01:09:22,800
can become vanishingly
small as it backpropagates.

1277
01:09:22,800 --> 01:09:27,555
And so generally, lower
layers are learned very slowly

1278
01:09:27,555 --> 01:09:29,470
and are hard to train.

1279
01:09:29,470 --> 01:09:33,330
So there's been a lot of
effort in other places as well,

1280
01:09:33,330 --> 01:09:37,229
to come up with different
architectures that

1281
01:09:37,229 --> 01:09:42,270
let you learn more
efficiently in deep networks.

1282
01:09:42,270 --> 01:09:44,910
And the commonest
way to do that is

1283
01:09:44,910 --> 01:09:47,430
to add more direct
connections, that

1284
01:09:47,430 --> 01:09:49,649
allow the gradient to flow.

1285
01:09:49,649 --> 01:09:52,890
So the big thing and vision
in the last few years

1286
01:09:52,890 --> 01:09:55,140
has been ResNets,
where the Res stands

1287
01:09:55,140 --> 01:09:57,270
for residual connections.

1288
01:09:57,270 --> 01:10:00,210
And so the way they're
made-- this picture

1289
01:10:00,210 --> 01:10:03,720
is upside down, so the
input is at the top,

1290
01:10:03,720 --> 01:10:07,780
is that you have these
sort of two paths that

1291
01:10:07,780 --> 01:10:08,970
are summed together.

1292
01:10:08,970 --> 01:10:11,580
One path is just
an identity path.

1293
01:10:11,580 --> 01:10:14,700
And the other one goes through
some neural network layers.

1294
01:10:14,700 --> 01:10:17,430
And so therefore,
its default behavior

1295
01:10:17,430 --> 01:10:21,270
is just to preserve
the input, which

1296
01:10:21,270 --> 01:10:25,560
might sound a little bit like
what we just saw for LSTMs.

1297
01:10:25,560 --> 01:10:26,730
There are other methods.

1298
01:10:26,730 --> 01:10:29,850
There have been DenseNets,
where you add skip connections

1299
01:10:29,850 --> 01:10:33,210
forward to every latent layer.

1300
01:10:33,210 --> 01:10:37,860
HighwayNets were also actually
developed by Schmidhuber,

1301
01:10:37,860 --> 01:10:41,580
and sort of reminiscent of
what was done with LSTMs.

1302
01:10:41,580 --> 01:10:44,580
So rather than just having
an identity connection,

1303
01:10:44,580 --> 01:10:49,260
as a ResNet has, it
introduces an extra gate.

1304
01:10:49,260 --> 01:10:52,380
So it looks more like
an LSTM, which says,

1305
01:10:52,380 --> 01:10:57,630
how much to send the input
through the highway versus how

1306
01:10:57,630 --> 01:11:00,660
much to put it through
a neural net layer?

1307
01:11:00,660 --> 01:11:03,090
And those two are then
combined into the output.

1308
01:11:03,090 --> 01:11:06,240


1309
01:11:06,240 --> 01:11:11,300
So essentially, this
problem occurs anywhere

1310
01:11:11,300 --> 01:11:15,770
when you have a lot of depth in
your layers of neural network.

1311
01:11:15,770 --> 01:11:18,200
But it first arose--

1312
01:11:18,200 --> 01:11:19,880
and it turns out
to be especially

1313
01:11:19,880 --> 01:11:23,630
problematic with
recurrent neural networks.

1314
01:11:23,630 --> 01:11:27,020
They are particularly
unstable, because of the fact

1315
01:11:27,020 --> 01:11:31,310
that you've got this one weight
matrix that you're repeatedly

1316
01:11:31,310 --> 01:11:33,395
using through the time sequence.

1317
01:11:33,395 --> 01:11:40,180


1318
01:11:40,180 --> 01:11:41,440
OK, so--

1319
01:11:41,440 --> 01:11:44,740
Chris, we've got a couple
of questions, more or less,

1320
01:11:44,740 --> 01:11:46,960
about whether you would
ever want to use an--

1321
01:11:46,960 --> 01:11:49,780
like a simple RNN
instead of an LSTM?

1322
01:11:49,780 --> 01:11:53,680
How does the LSTM learn
what to do with its gates?

1323
01:11:53,680 --> 01:11:55,750
Can you opine on those things?

1324
01:11:55,750 --> 01:11:57,730
Sure.

1325
01:11:57,730 --> 01:12:01,450
So I think, basically
the answer is,

1326
01:12:01,450 --> 01:12:04,420
you should never use a
simple RNN these days.

1327
01:12:04,420 --> 01:12:08,072
You should always use an LSTM.

1328
01:12:08,072 --> 01:12:10,280
I mean, obviously, that
depends on what you're doing.

1329
01:12:10,280 --> 01:12:13,930
If you're wanting to do some
analytical paper or something,

1330
01:12:13,930 --> 01:12:17,020
you might prefer a simple RNN.

1331
01:12:17,020 --> 01:12:21,180
And it is the case
that you can actually

1332
01:12:21,180 --> 01:12:24,060
get decent results
with simple RNNs,

1333
01:12:24,060 --> 01:12:27,420
providing you are very careful
to make sure that things aren't

1334
01:12:27,420 --> 01:12:30,210
exploding nor vanishing.

1335
01:12:30,210 --> 01:12:32,980


1336
01:12:32,980 --> 01:12:37,030
But in practice,
getting simple RNNs

1337
01:12:37,030 --> 01:12:39,550
to work and preserve
long context

1338
01:12:39,550 --> 01:12:43,120
is incredibly difficult,
where you can train LSTMs

1339
01:12:43,120 --> 01:12:44,940
and they will just work.

1340
01:12:44,940 --> 01:12:49,600
So really, you should
always just use an LSTM.

1341
01:12:49,600 --> 01:12:52,930
Now wait, the
second question was?

1342
01:12:52,930 --> 01:12:54,520
I think there's a
bit of confusion

1343
01:12:54,520 --> 01:12:58,000
about whether the gates
are learning differently.

1344
01:12:58,000 --> 01:12:58,900
Oh, yeah.

1345
01:12:58,900 --> 01:13:04,220
So the gates are
also just learned.

1346
01:13:04,220 --> 01:13:09,490
So if we go back
to these equations,

1347
01:13:09,490 --> 01:13:11,470
this is the complete model.

1348
01:13:11,470 --> 01:13:14,920
And when we're training
the model, every one

1349
01:13:14,920 --> 01:13:20,530
of these parameters, so
all of these W, U, and b's,

1350
01:13:20,530 --> 01:13:25,570
everything is simultaneously
being trained by backprop.

1351
01:13:25,570 --> 01:13:30,280
So that what you hope,
and indeed it works,

1352
01:13:30,280 --> 01:13:33,520
is the model is
learning, what stuff

1353
01:13:33,520 --> 01:13:36,490
should I remember for a
long time, versus what

1354
01:13:36,490 --> 01:13:38,140
stuff should I forget?

1355
01:13:38,140 --> 01:13:40,390
What things in the
input are important,

1356
01:13:40,390 --> 01:13:43,510
versus what things in the
input don't really matter?

1357
01:13:43,510 --> 01:13:46,960
So it can learn things
like function words,

1358
01:13:46,960 --> 01:13:49,190
like a and the
don't really matter,

1359
01:13:49,190 --> 01:13:51,610
even though everyone
uses them in English.

1360
01:13:51,610 --> 01:13:54,280
So you can just not
worry about those.

1361
01:13:54,280 --> 01:13:55,840
So all of this is learned.

1362
01:13:55,840 --> 01:14:00,040
And the models do, actually,
successfully learn gate values

1363
01:14:00,040 --> 01:14:03,640
about what information is
useful to preserve long-term,

1364
01:14:03,640 --> 01:14:06,640
versus what information
is really only

1365
01:14:06,640 --> 01:14:09,670
useful short-term, for
predicting the next one or two

1366
01:14:09,670 --> 01:14:11,680
words.

1367
01:14:11,680 --> 01:14:17,350
Finally, the gradient
improvements, due to the-- so

1368
01:14:17,350 --> 01:14:18,850
you said that the
addition is really

1369
01:14:18,850 --> 01:14:21,430
important between the new cell
candidate and the cell state.

1370
01:14:21,430 --> 01:14:23,530
And I don't think-- at
least, a couple of students

1371
01:14:23,530 --> 01:14:25,430
have sort of questioned that.

1372
01:14:25,430 --> 01:14:28,710
So if you want to go over that
again, that might be useful.

1373
01:14:28,710 --> 01:14:29,210
Sure.

1374
01:14:29,210 --> 01:14:32,720


1375
01:14:32,720 --> 01:14:37,700
So what we would like is
an easy way for memory

1376
01:14:37,700 --> 01:14:41,750
to be preserved long-term.

1377
01:14:41,750 --> 01:14:46,720
And one way, which
is what ResNets use,

1378
01:14:46,720 --> 01:14:51,280
is just to completely have a
direct path from Ct minus 1

1379
01:14:51,280 --> 01:14:55,930
to Ct, and will preserve
entirely the history.

1380
01:14:55,930 --> 01:14:58,540
So kind of there's
a default action

1381
01:14:58,540 --> 01:15:03,630
of preserving information
about the past long-term.

1382
01:15:03,630 --> 01:15:06,610
LSTMs don't quite do that.

1383
01:15:06,610 --> 01:15:10,090
But they allow that
function to be easy.

1384
01:15:10,090 --> 01:15:13,870
So you start off with
the previous cell state.

1385
01:15:13,870 --> 01:15:17,170
And you can forget some
of it by the forget gate.

1386
01:15:17,170 --> 01:15:19,000
So you can delete stuff
out of your memory.

1387
01:15:19,000 --> 01:15:21,063
That's a useful operation.

1388
01:15:21,063 --> 01:15:22,480
And then, well,
you're going to be

1389
01:15:22,480 --> 01:15:25,240
able to update the
content of the cell

1390
01:15:25,240 --> 01:15:30,820
with the write operation that
occurs in the plus, where,

1391
01:15:30,820 --> 01:15:34,090
depending on the input
gate, some parts of what's

1392
01:15:34,090 --> 01:15:36,610
in the cell will be added too.

1393
01:15:36,610 --> 01:15:39,820
But you can think of
that adding as overlaying

1394
01:15:39,820 --> 01:15:41,470
extra information.

1395
01:15:41,470 --> 01:15:44,620
Everything that was in the
cell that wasn't forgotten

1396
01:15:44,620 --> 01:15:47,270
is still continuing on
to the next timestep.

1397
01:15:47,270 --> 01:15:50,010


1398
01:15:50,010 --> 01:15:53,550
And in particular, when you're
doing the backpropagation

1399
01:15:53,550 --> 01:15:58,080
through time, that there isn't--

1400
01:15:58,080 --> 01:16:00,330
I want to say, there
isn't a multiplication

1401
01:16:00,330 --> 01:16:02,820
between Ct and Ct minus 1.

1402
01:16:02,820 --> 01:16:06,540
And there's this unfortunate
time symbol here.

1403
01:16:06,540 --> 01:16:09,090
But remember that's the
Hadamard product, which

1404
01:16:09,090 --> 01:16:11,670
is zeroing out part of
it with the forget gate.

1405
01:16:11,670 --> 01:16:15,930
It's not a multiplication by a
matrix, like in the simple RNN.

1406
01:16:15,930 --> 01:16:22,560


1407
01:16:22,560 --> 01:16:25,560
I hope that's good.

1408
01:16:25,560 --> 01:16:27,540
OK, so there are a
couple of other things

1409
01:16:27,540 --> 01:16:30,510
that I wanted to get
through before the end.

1410
01:16:30,510 --> 01:16:32,760
I guess I'm not going to
have time to do both of them,

1411
01:16:32,760 --> 01:16:33,260
I think.

1412
01:16:33,260 --> 01:16:35,680
So I'll do the last one,
probably, next time.

1413
01:16:35,680 --> 01:16:37,950
Though these are
actually simple and easy

1414
01:16:37,950 --> 01:16:43,420
things, that they
complete our picture.

1415
01:16:43,420 --> 01:16:46,130
So I sort of briefly
alluded to this example

1416
01:16:46,130 --> 01:16:49,560
of sentiment classification,
where what we could do

1417
01:16:49,560 --> 01:16:55,050
is run an RNN, maybe an
LSTM, over a sentence,

1418
01:16:55,050 --> 01:16:59,940
call this our representation
of the sentence,

1419
01:16:59,940 --> 01:17:04,260
and you feed it into
a softmax classifier

1420
01:17:04,260 --> 01:17:07,500
to classify for sentiment.

1421
01:17:07,500 --> 01:17:10,320
So what we're actually
saying there is that,

1422
01:17:10,320 --> 01:17:14,820
we can regard the hidden
state as a representation

1423
01:17:14,820 --> 01:17:17,670
of a word in context.

1424
01:17:17,670 --> 01:17:22,230
That the low, that we have just
a word vector for terribly.

1425
01:17:22,230 --> 01:17:25,800
But we then looked
at our context

1426
01:17:25,800 --> 01:17:31,350
and say, OK, we've now created
a hidden state representation

1427
01:17:31,350 --> 01:17:35,880
for the word terribly, in
the context of the movie was.

1428
01:17:35,880 --> 01:17:39,150
And that proves to be
a really useful idea,

1429
01:17:39,150 --> 01:17:43,620
because words have different
meanings in different contexts.

1430
01:17:43,620 --> 01:17:46,830
But it seems like there's a
defect of what we've done here,

1431
01:17:46,830 --> 01:17:51,270
because our context only
contains information

1432
01:17:51,270 --> 01:17:52,500
from the left.

1433
01:17:52,500 --> 01:17:53,690
What about right context?

1434
01:17:53,690 --> 01:17:58,470
Surely, it would also be useful
to have the meaning of terribly

1435
01:17:58,470 --> 01:18:01,080
depend on exciting.

1436
01:18:01,080 --> 01:18:03,330
Because often, words
mean different things

1437
01:18:03,330 --> 01:18:06,660
based on what follows them.

1438
01:18:06,660 --> 01:18:09,300
So if you have
something like red wine,

1439
01:18:09,300 --> 01:18:14,240
it means something quite
different from a red light.

1440
01:18:14,240 --> 01:18:16,320
So how could we deal with that?

1441
01:18:16,320 --> 01:18:20,130
Well, an easy way to deal
with that would be to say,

1442
01:18:20,130 --> 01:18:22,250
well, if we're just
wanting to come up

1443
01:18:22,250 --> 01:18:24,680
with a neural encoding
of a sentence,

1444
01:18:24,680 --> 01:18:28,880
we could have a second RNN with
completely separate parameters

1445
01:18:28,880 --> 01:18:29,750
learned.

1446
01:18:29,750 --> 01:18:33,560
And we could run it backwards
through the sentence

1447
01:18:33,560 --> 01:18:36,680
to get a backward
representation of each word.

1448
01:18:36,680 --> 01:18:39,560
And then, we could get
an overall representation

1449
01:18:39,560 --> 01:18:43,370
of each word and context by
just concatenating those two

1450
01:18:43,370 --> 01:18:44,930
representations.

1451
01:18:44,930 --> 01:18:47,780
And now we've got a
representation of terribly

1452
01:18:47,780 --> 01:18:52,890
that has both left
and right context.

1453
01:18:52,890 --> 01:18:56,780
So we're simply
running a forward RNN.

1454
01:18:56,780 --> 01:18:59,330
And when I say RNN
in here, that just

1455
01:18:59,330 --> 01:19:01,590
means any kind of
recurrent neural network.

1456
01:19:01,590 --> 01:19:04,670
So commonly, it'll be an LSTM.

1457
01:19:04,670 --> 01:19:06,320
And a backward one.

1458
01:19:06,320 --> 01:19:08,060
And then at each
timestep, we're just

1459
01:19:08,060 --> 01:19:11,810
concatenating their
representations,

1460
01:19:11,810 --> 01:19:15,080
with each of these
having separate weights.

1461
01:19:15,080 --> 01:19:18,260
And so then we regard
this concatenated thing

1462
01:19:18,260 --> 01:19:21,320
as the hidden state, the
contextual representation

1463
01:19:21,320 --> 01:19:26,240
of a token at a particular
time, that we pass forward.

1464
01:19:26,240 --> 01:19:32,090
This is so common, that people
use a shortcut to denote that.

1465
01:19:32,090 --> 01:19:35,930
And they'll just draw this
picture with two-sided arrows.

1466
01:19:35,930 --> 01:19:39,260
And when you see that picture
with two-sided arrows,

1467
01:19:39,260 --> 01:19:46,790
it means that you're running
two RNNs, one in each direction.

1468
01:19:46,790 --> 01:19:50,120
And then, concatenating their
results at each timestep.

1469
01:19:50,120 --> 01:19:52,670
And that's what you're going
to use later in the model.

1470
01:19:52,670 --> 01:19:55,880


1471
01:19:55,880 --> 01:20:00,710
Okay, so if you're doing
an encoding problem,

1472
01:20:00,710 --> 01:20:03,290
like for sentiment
classification or question

1473
01:20:03,290 --> 01:20:10,190
answering, using bidirectional
RNNs is a great thing to do.

1474
01:20:10,190 --> 01:20:12,890
But they are only applicable
if you have access

1475
01:20:12,890 --> 01:20:15,770
to the entire input sequence.

1476
01:20:15,770 --> 01:20:19,640
They're not applicable
to language modeling.

1477
01:20:19,640 --> 01:20:22,100
Because in a language
model, necessarily, you

1478
01:20:22,100 --> 01:20:24,410
have to generate
the next word based

1479
01:20:24,410 --> 01:20:27,710
on only the preceding context.

1480
01:20:27,710 --> 01:20:30,260
But if you do have the
entire input sequence,

1481
01:20:30,260 --> 01:20:33,680
that bidirectionality
gives you greater power.

1482
01:20:33,680 --> 01:20:37,640
And indeed, that's been an
idea that people have built on

1483
01:20:37,640 --> 01:20:39,060
in subsequent work.

1484
01:20:39,060 --> 01:20:43,730
So when we get to transformers
in a couple of weeks,

1485
01:20:43,730 --> 01:20:47,360
we'll spend plenty of time
on the BERT model, where

1486
01:20:47,360 --> 01:20:50,730
that acronym stands for
Bidirectional Encoder

1487
01:20:50,730 --> 01:20:53,300
Representations
from Transformers.

1488
01:20:53,300 --> 01:20:58,730
So part of what's important in
that model is the transformer.

1489
01:20:58,730 --> 01:21:01,640
But really, a central
point of the paper

1490
01:21:01,640 --> 01:21:05,540
was to say that you could build
more powerful models using

1491
01:21:05,540 --> 01:21:13,070
transformers by, again,
exploiting bidirectionality.

1492
01:21:13,070 --> 01:21:15,560
OK, there's one teeny
bit left on RNNs.

1493
01:21:15,560 --> 01:21:17,510
But I'll sneak it
into next class.

1494
01:21:17,510 --> 01:21:20,190
And I'll call it
the end for today.

1495
01:21:20,190 --> 01:21:21,680
And if there are
other things you'd

1496
01:21:21,680 --> 01:21:25,910
like to ask questions about,
you can find me on Nooks

1497
01:21:25,910 --> 01:21:29,630
again in just a minute.

1498
01:21:29,630 --> 01:21:32,950
Okay, so I'll see you
again next Tuesday.

1499
01:21:32,950 --> 01:21:38,000


