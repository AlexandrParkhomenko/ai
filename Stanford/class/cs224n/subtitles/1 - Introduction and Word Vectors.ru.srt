1
00:00:04,960 --> 00:00:06,799
привет всем

2
00:00:06,799 --> 00:00:11,280
добро пожаловать в stanford cs224n, также

3
00:00:11,280 --> 00:00:13,759
известный как ling 284

4
00:00:13,759 --> 00:00:15,920
обработка естественного языка с глубоким

5
00:00:15,920 --> 00:00:18,000
обучением, я Кристофер Мэннинг, и

6
00:00:18,000 --> 00:00:22,000
я главный инструктор этого класса,

7
00:00:22,000 --> 00:00:22,800
поэтому

8
00:00:22,800 --> 00:00:25,439
то, что мы надеемся сделать сегодня,

9
00:00:25,439 --> 00:00:27,519
- это погрузиться прямо в него, так что я  Я собираюсь

10
00:00:27,519 --> 00:00:30,080
потратить около 10 минут, рассказывая о

11
00:00:30,080 --> 00:00:31,439
курсе,

12
00:00:31,439 --> 00:00:33,040
а затем мы перейдем прямо

13
00:00:33,040 --> 00:00:35,040
к содержанию, причины, которые я объясню

14
00:00:35,040 --> 00:00:37,600
через минуту, так что мы поговорим о

15
00:00:37,600 --> 00:00:39,840
человеческом языке и значении слов,

16
00:00:39,840 --> 00:00:42,559
затем я представлю  идеи алгоритма word to

17
00:00:42,559 --> 00:00:45,120
vec для изучения значения слова,

18
00:00:45,120 --> 00:00:47,280
а затем, исходя из этого, мы как бы

19
00:00:47,280 --> 00:00:49,920
конкретно проработаем, как вы можете

20
00:00:49,920 --> 00:00:52,320
разработать градиенты целевой функции по

21
00:00:52,320 --> 00:00:54,960
отношению к алгоритму word deveck,

22
00:00:54,960 --> 00:00:56,399
и немного расскажем о том, как

23
00:00:56,399 --> 00:00:59,120
работает оптимизация и затем прямо в

24
00:00:59,120 --> 00:01:01,440
конце урока я хочу потратить

25
00:01:01,440 --> 00:01:03,760
немного времени на то, чтобы дать вам представление о

26
00:01:03,760 --> 00:01:04,799
том, как

27
00:01:04,799 --> 00:01:07,200
работают эти векторы слов и что вы можете

28
00:01:07,200 --> 00:01:10,400
с ними делать, так что на самом деле ключевым уроком на

29
00:01:10,400 --> 00:01:13,600
сегодня является то, что я хочу дать вам смысл

30
00:01:13,600 --> 00:01:15,600
о том, как  удивительные

31
00:01:15,600 --> 00:01:18,159
векторы слов глубокого обучения, поэтому мы

32
00:01:18,159 --> 00:01:20,799
получили действительно удивительный результат, что

33
00:01:20,799 --> 00:01:23,040
значение слова может быть представлено не

34
00:01:23,040 --> 00:01:25,520
идеально, но довольно хорошо с помощью

35
00:01:25,520 --> 00:01:28,880
большого вектора действительных чисел, и вы

36
00:01:28,880 --> 00:01:30,640
знаете, что это своего рода обычное

37
00:01:30,640 --> 00:01:32,320
место в последнее десятилетие глубокого обучения.

38
00:01:32,320 --> 00:01:35,840
учиться, но это бросает вызов

39
00:01:35,840 --> 00:01:38,320
тысячелетней традиции, и это

40
00:01:38,320 --> 00:01:40,799
действительно довольно неожиданный результат -

41
00:01:40,799 --> 00:01:43,040
начать сосредотачиваться на том, что

42
00:01:43,040 --> 00:01:45,439
хорошо, так быстро, что мы надеемся

43
00:01:45,439 --> 00:01:48,320
научить в этом курсе, поэтому у нас есть три

44
00:01:48,320 --> 00:01:52,159
основные цели, первая - это  научить

45
00:01:52,159 --> 00:01:55,200
вас основам - хорошее глубокое

46
00:01:55,200 --> 00:01:57,439
понимание влияния современных

47
00:01:57,439 --> 00:02:00,079
методов глубокого обучения, применяемых к NLP,

48
00:02:00,079 --> 00:02:02,240
поэтому мы собираемся начать и пройтись

49
00:02:02,240 --> 00:02:05,200
по основам, а затем перейдем к ключевым

50
00:02:05,200 --> 00:02:07,759
методам, которые используются в рекуррентных

51
00:02:07,759 --> 00:02:10,160
сетях NLP, преобразователях внимания и

52
00:02:10,160 --> 00:02:11,840
тому подобном.  Таким образом,

53
00:02:11,840 --> 00:02:14,319
мы хотим сделать нечто большее, чем просто

54
00:02:14,319 --> 00:02:17,040
то, что мы также хотели бы дать вам некоторое

55
00:02:17,040 --> 00:02:19,360
представление об общей картине понимания

56
00:02:19,360 --> 00:02:21,920
человеческих языков и каковы причины того,

57
00:02:21,920 --> 00:02:24,000
почему они  На самом деле довольно

58
00:02:24,000 --> 00:02:26,319
сложно понять и воспроизвести, хотя

59
00:02:26,319 --> 00:02:27,840
люди, кажется,

60
00:02:27,840 --> 00:02:30,560
делают это сейчас легко, очевидно, если вы действительно

61
00:02:30,560 --> 00:02:32,480
хотите узнать много по этой теме, вам

62
00:02:32,480 --> 00:02:34,560
следует записаться и пойти и начать посещать

63
00:02:34,560 --> 00:02:36,000
некоторые классы на факультете лингвистики,

64
00:02:36,000 --> 00:02:38,400
но, тем не менее, для многих

65
00:02:38,400 --> 00:02:41,599
это единственный контент на человеческом языке, который

66
00:02:41,599 --> 00:02:43,519
вы увидите во время получения

67
00:02:43,519 --> 00:02:45,760
степени магистра или чего-то еще, и поэтому мы надеемся

68
00:02:45,760 --> 00:02:47,840
потратить на это немного времени,

69
00:02:47,840 --> 00:02:49,760
начиная с сегодняшнего дня,

70
00:02:49,760 --> 00:02:52,160
а затем, наконец, мы хотим дать вам

71
00:02:52,160 --> 00:02:54,160
понимание

72
00:02:54,160 --> 00:02:57,040
способности создавать системы в  пи-факел для некоторых

73
00:02:57,040 --> 00:02:59,360
из основных проблем в nlp, поэтому мы

74
00:02:59,360 --> 00:03:01,760
рассмотрим изучение значений слов, анализ зависимостей,

75
00:03:01,760 --> 00:03:04,239
машинный перевод,

76
00:03:04,239 --> 00:03:05,760
ответ на вопрос,

77
00:03:05,760 --> 00:03:10,400
давайте погрузимся в человеческий язык

78
00:03:10,400 --> 00:03:13,440
когда-то давно у меня было гораздо более длинное

79
00:03:13,440 --> 00:03:16,000
введение, в котором было много примеров

80
00:03:16,000 --> 00:03:18,480
того, как человек  языки могут быть

81
00:03:18,480 --> 00:03:21,200
непонятыми и сложными, я покажу

82
00:03:21,200 --> 00:03:23,040
некоторые из этих

83
00:03:23,040 --> 00:03:25,760
примеров в последующих лекциях,

84
00:03:25,760 --> 00:03:28,239
но, поскольку прямо сегодня

85
00:03:28,239 --> 00:03:30,000
мы собираемся сосредоточиться на значении слов,

86
00:03:30,000 --> 00:03:33,440
хотя  Я бы просто привел один

87
00:03:33,440 --> 00:03:36,159
пример, который взят из очень хорошего

88
00:03:36,159 --> 00:03:38,640
мультфильма xkcd,

89
00:03:38,640 --> 00:03:40,080
и

90
00:03:40,080 --> 00:03:42,480
это не похоже на какую-

91
00:03:42,480 --> 00:03:44,879
то синтаксическую двусмысленность

92
00:03:44,879 --> 00:03:47,599
предложений, но вместо этого он действительно

93
00:03:47,599 --> 00:03:50,000
подчеркивает важный момент, что

94
00:03:50,000 --> 00:03:52,879
язык - это построенная социальная система

95
00:03:52,879 --> 00:03:55,599
и интерпретируется людьми, и это

96
00:03:55,599 --> 00:03:58,480
часть того, как и оно меняется, когда люди

97
00:03:58,480 --> 00:04:00,480
решают

98
00:04:00,480 --> 00:04:02,480
адаптировать его конструкцию, и это

99
00:04:05,360 --> 00:04:07,599
одна из причин, почему человеческие языки хороши как адаптивная система для

100
00:04:07,599 --> 00:04:08,959
людей,

101
00:04:08,959 --> 00:04:12,159
но трудны

102
00:04:12,159 --> 00:04:14,640
для понимания как система или наши компьютеры по сей

103
00:04:14,640 --> 00:04:15,439
день

104
00:04:15,439 --> 00:04:18,000
так что в этом разговоре между

105
00:04:18,000 --> 00:04:20,560
двумя женщинами одна говорит, что в любом случае мне наплевать,

106
00:04:20,560 --> 00:04:23,520
а другая говорит, что я думаю, вы имеете в виду, что

107
00:04:23,520 --> 00:04:25,280
вам наплевать,

108
00:04:25,280 --> 00:04:27,840
говоря, что вам наплевать, подразумевает, что вы

109
00:04:27,840 --> 00:04:29,759
хоть немного заботитесь,

110
00:04:29,759 --> 00:04:32,479
а другая говорит, что мне наплевать.  Я знаю,

111
00:04:32,479 --> 00:04:34,560
где эти невероятно сложные

112
00:04:34,560 --> 00:04:37,440
мозги плывут по пустоте,

113
00:04:37,440 --> 00:04:39,840
тщетно пытаясь соединиться друг с другом,

114
00:04:39,840 --> 00:04:41,840
слепо убегая слов в

115
00:04:41,840 --> 00:04:43,120
темноту,

116
00:04:43,120 --> 00:04:45,680
каждый выбор формулировки по буквам  g,

117
00:04:45,680 --> 00:04:48,800
тон и время несут бесчисленные

118
00:04:48,800 --> 00:04:51,759
сигналы, контексты, подтексты и

119
00:04:51,759 --> 00:04:52,800
многое другое,

120
00:04:52,800 --> 00:04:54,880
и каждый слушатель интерпретирует эти

121
00:04:54,880 --> 00:04:56,960
сигналы по-своему.

122
00:04:56,960 --> 00:04:59,520
язык не является формальной системой.

123
00:05:06,000 --> 00:05:08,479
Что можно сделать, это попытаться

124
00:05:08,479 --> 00:05:11,199
научиться угадывать, как ваши слова влияют на людей, чтобы у

125
00:05:11,199 --> 00:05:13,120
вас был шанс найти

126
00:05:13,120 --> 00:05:15,199
те, которые заставят их почувствовать что-то

127
00:05:15,199 --> 00:05:17,360
вроде того, что вы хотите, чтобы они чувствовали

128
00:05:17,360 --> 00:05:19,520
все остальное бессмысленно,

129
00:05:19,520 --> 00:05:21,520
я полагаю, вы даете мне советы по поводу  как

130
00:05:21,520 --> 00:05:23,680
вы интерпретируете слова, потому что хотите, чтобы

131
00:05:23,680 --> 00:05:25,680
я чувствовал себя менее одиноким,

132
00:05:25,680 --> 00:05:29,600
если да, тогда спасибо, это очень много значит,

133
00:05:29,600 --> 00:05:31,919
но если вы просто пропускаете мои предложения

134
00:05:31,919 --> 00:05:34,320
мимо какого-то мысленного контрольного списка, чтобы вы могли

135
00:05:34,320 --> 00:05:36,320
показать, насколько хорошо вы это знаете,

136
00:05:36,320 --> 00:05:38,639
тогда мне было бы наплевать,

137
00:05:38,639 --> 00:05:41,199
ладно  так что в конечном итоге наша

138
00:05:41,199 --> 00:05:44,800
цель состоит в том, как лучше работать над

139
00:05:44,800 --> 00:05:48,720
созданием гм вычислительных систем,

140
00:05:48,720 --> 00:05:49,520
которые

141
00:05:49,520 --> 00:05:52,320
пытаются лучше угадывать, как их

142
00:05:52,320 --> 00:05:55,039
слова повлияют на других людей и что

143
00:05:55,039 --> 00:05:57,280
другие люди имеют в виду  По словам,

144
00:05:57,280 --> 00:06:00,400
которые они предпочитают говорить,

145
00:06:01,360 --> 00:06:04,560
так что интересная вещь о человеческом

146
00:06:04,560 --> 00:06:08,160
языке заключается в том, что это система,

147
00:06:08,160 --> 00:06:11,120
созданная людьми,

148
00:06:11,120 --> 00:06:11,919
ммм,

149
00:06:11,919 --> 00:06:15,039
и эта система была построена,

150
00:06:15,039 --> 00:06:18,639
вы знаете, в некотором смысле, относительно недавно,

151
00:06:18,639 --> 00:06:21,520
поэтому в обсуждениях искусственного

152
00:06:21,520 --> 00:06:24,880
интеллекта ммм  большую часть времени

153
00:06:24,880 --> 00:06:25,759
люди

154
00:06:25,759 --> 00:06:28,479
уделяют много внимания человеческому мозгу и

155
00:06:28,479 --> 00:06:32,240
нейронам, гудящим мимо, и этому разуму,

156
00:06:32,240 --> 00:06:34,400
который должен находиться в головах людей,

157
00:06:34,400 --> 00:06:36,639
но я просто хотел на

158
00:06:36,639 --> 00:06:40,000
мгновение сосредоточиться на роли языка,

159
00:06:40,000 --> 00:06:41,280
на самом деле

160
00:06:41,280 --> 00:06:43,680
вы знаете, что это своего рода  спорно,

161
00:06:43,680 --> 00:06:44,639
но

162
00:06:44,639 --> 00:06:46,560
вы знаете, что это не обязательно тот случай,

163
00:06:46,560 --> 00:06:48,960
когда люди намного

164
00:06:48,960 --> 00:06:51,120
умнее некоторых высших обезьян, таких как

165
00:06:51,120 --> 00:06:54,160
шимпанзе или бонобо, так что

166
00:06:54,160 --> 00:06:56,319
шимпанзе и бонобо, как было показано,

167
00:06:56,319 --> 00:06:59,199
могут использовать бассейны, чтобы строить планы,

168
00:06:59,199 --> 00:07:01,440
и на самом деле шимпанзе намного лучше

169
00:07:01,440 --> 00:07:05,360
коротких  -срочная память, чем человеческие существа,

170
00:07:05,360 --> 00:07:07,680
относительно того, что если вы посмотрите

171
00:07:07,680 --> 00:07:09,759
на историю жизни на Земле,

172
00:07:09,759 --> 00:07:13,120
люди действительно развивают язык в последнее

173
00:07:13,120 --> 00:07:15,840
время.  мы вроде как на самом деле

174
00:07:15,840 --> 00:07:17,440
не знаем, потому что вы знаете, что нет

175
00:07:17,440 --> 00:07:19,599
окаменелостей, которые говорят хорошо, вот язык,

176
00:07:19,599 --> 00:07:22,080
эм, но вы знаете, что

177
00:07:22,080 --> 00:07:25,599
большинство людей считает, что язык возник

178
00:07:25,599 --> 00:07:28,240
для людей вроде

179
00:07:28,240 --> 00:07:29,280
вы знаете

180
00:07:29,280 --> 00:07:31,199
где-то в диапазоне от ста

181
00:07:31,199 --> 00:07:33,759
тысяч до миллиона  лет назад, ладно,

182
00:07:33,759 --> 00:07:35,919
это было давно, но по сравнению с

183
00:07:35,919 --> 00:07:38,479
процессом эволюции жизни на Земле,

184
00:07:38,479 --> 00:07:41,520
это как бы моргает веком,

185
00:07:41,520 --> 00:07:44,800
ммм, но эта мощная коммуникация

186
00:07:44,800 --> 00:07:47,919
между людьми быстро привела к нашему

187
00:07:47,919 --> 00:07:50,560
превосходству над другими существами,

188
00:07:50,560 --> 00:07:52,960
так что это интересно, что

189
00:07:52,960 --> 00:07:55,919
окончательный  Оказалось, что сила - это

190
00:07:55,919 --> 00:07:58,560
не ядовитые клыки, не супербыстрая или сверхбольшая скорость,

191
00:07:58,560 --> 00:08:00,800
а способность общаться

192
00:08:02,240 --> 00:08:05,120
с другими членами вашего племени,

193
00:08:05,120 --> 00:08:07,039
гораздо позже

194
00:08:07,039 --> 00:08:09,680
люди снова разработали письмо, которое

195
00:08:09,680 --> 00:08:11,840
позволяло передавать знания на

196
00:08:11,840 --> 00:08:14,240
расстояниях во времени и времени.  пространство,

197
00:08:14,240 --> 00:08:16,240
и это всего лишь около

198
00:08:16,240 --> 00:08:19,360
5000 лет силе письма,

199
00:08:19,360 --> 00:08:22,080
так что всего за несколько тысяч

200
00:08:22,080 --> 00:08:24,000
лет способность сохранять и передавать

201
00:08:24,000 --> 00:08:26,879
Знание перенесло нас из бронзового века

202
00:08:26,879 --> 00:08:30,720
в современные смартфоны и планшеты,

203
00:08:30,720 --> 00:08:32,479
поэтому ключевой вопрос для искусственного

204
00:08:32,479 --> 00:08:34,320
интеллекта и взаимодействия человека с компьютером

205
00:08:34,320 --> 00:08:36,880
- как заставить компьютеры

206
00:08:36,880 --> 00:08:39,039
понимать информацию,

207
00:08:39,039 --> 00:08:41,360
передаваемую на человеческих языках

208
00:08:41,360 --> 00:08:43,839
одновременно. Искусственный интеллект

209
00:08:43,839 --> 00:08:46,160
требует компьютеров со знаниями

210
00:08:46,160 --> 00:08:47,519
людей,

211
00:08:47,519 --> 00:08:49,760
к счастью, теперь наши системы искусственного интеллекта

212
00:08:49,760 --> 00:08:52,880
могут извлечь выгоду из благотворного цикла, в котором нам

213
00:08:52,880 --> 00:08:54,800
нужны знания, чтобы хорошо понимать язык

214
00:08:54,800 --> 00:08:57,279
и людей, но также бывает и

215
00:08:57,279 --> 00:08:59,760
то, что большая часть этих знаний

216
00:08:59,760 --> 00:09:02,240
содержится в языке, распространенном

217
00:09:02,240 --> 00:09:04,399
по книгам и веб-страницам  мир, и

218
00:09:04,399 --> 00:09:05,760
это одна из вещей, которые мы собираемся

219
00:09:05,760 --> 00:09:08,000
рассмотреть в этом курсе, - это то, как мы

220
00:09:08,000 --> 00:09:11,680
можем как бы развить этот благотворный цикл,

221
00:09:11,680 --> 00:09:14,399
уже был достигнут большой прогресс,

222
00:09:14,399 --> 00:09:16,880
и я просто хочу очень быстро

223
00:09:16,880 --> 00:09:18,720
дать представление об этом

224
00:09:18,720 --> 00:09:19,839
поэтому

225
00:09:19,839 --> 00:09:21,680
в последнее

226
00:09:21,680 --> 00:09:23,760
десятилетие или около того, и особенно в последние

227
00:09:23,760 --> 00:09:26,320
несколько лет, с новыми методами машинного

228
00:09:26,320 --> 00:09:29,200
перевода мы сейчас находимся в пространстве, где

229
00:09:29,200 --> 00:09:32,399
машинный перевод  язык действительно работает

230
00:09:32,399 --> 00:09:34,880
умеренно хорошо, так что, опять же

231
00:09:34,880 --> 00:09:36,399
из мировой истории, это просто

232
00:09:36,399 --> 00:09:38,880
потрясающе, ведь тысячи лет

233
00:09:38,880 --> 00:09:41,680
изучение языков других людей было

234
00:09:41,680 --> 00:09:45,040
человеческой задачей, требующей больших

235
00:09:45,040 --> 00:09:48,000
усилий и концентрации, но теперь мы находимся

236
00:09:48,000 --> 00:09:49,760
в мире, где вы могли просто  Зайдите в

237
00:09:49,760 --> 00:09:52,800
свой веб-браузер и подумайте, о, мне интересно,

238
00:09:52,800 --> 00:09:55,200
какие новости в Кении сегодня, и вы

239
00:09:55,200 --> 00:09:57,680
можете отправиться на кенийский веб-сайт,

240
00:09:57,680 --> 00:09:59,760
и вы можете увидеть что-то вроде этого,

241
00:09:59,760 --> 00:10:02,480
и вы можете пойти, а, а затем попросить

242
00:10:02,480 --> 00:10:05,040
гугл эм перевести это для  Вы эм

243
00:10:05,040 --> 00:10:07,680
из суахили, и вы знаете, что

244
00:10:07,680 --> 00:10:10,399
перевод не совсем идеален, но

245
00:10:10,399 --> 00:10:12,399
вы знаете, что он достаточно хорош, поэтому

246
00:10:12,399 --> 00:10:15,040
газета tuko была проинформирована о том, что

247
00:10:15,040 --> 00:10:18,959
министр местного правительства Лингсан

248
00:10:18,959 --> 00:10:21,519
и его коллеги по транспорту в городе меня

249
00:10:21,519 --> 00:10:23,920
умерли в течение двух отдельных часов, так что вы

250
00:10:23,920 --> 00:10:25,680
знаете, в течение двух отдельных  часов

251
00:10:25,680 --> 00:10:28,160
немного неудобно, но, по сути, мы

252
00:10:28,160 --> 00:10:30,640
неплохо справляемся с получением информации

253
00:10:30,640 --> 00:10:33,360
с этой страницы, и это, ммм, довольно

254
00:10:33,360 --> 00:10:34,800
удивительно,

255
00:10:34,800 --> 00:10:36,800
единственная самая большая

256
00:10:36,800 --> 00:10:38,959
разработка в

257
00:10:38,959 --> 00:10:40,640
НЛП  или в прошлом году в

258
00:10:40,640 --> 00:10:43,200
популярных средствах массовой информации

259
00:10:43,200 --> 00:10:44,079
был

260
00:10:44,079 --> 00:10:45,600
gpt

261
00:10:45,600 --> 00:10:48,079
um, который был огромной новой моделью,

262
00:10:48,079 --> 00:10:51,120
выпущенной open ai um

263
00:10:51,120 --> 00:10:54,079
что такое gpt 3 и почему он великолепен,

264
00:10:54,079 --> 00:10:56,480
на самом деле немного тонко, и поэтому я не могу

265
00:10:56,480 --> 00:10:58,560
действительно пройти через все  подробности

266
00:10:58,560 --> 00:11:02,160
здесь, но это интересно, потому что

267
00:11:02,160 --> 00:11:04,079
кажется, что это первый шаг на

268
00:11:04,079 --> 00:11:06,320
пути к тому, что мы можем назвать универсальными

269
00:11:06,320 --> 00:11:09,120
моделями, где вы можете тренировать одну

270
00:11:09,120 --> 00:11:12,399
чрезвычайно большую модель на чем-то вроде

271
00:11:12,399 --> 00:11:14,880
той картинки библиотеки, которую я показывал ранее,

272
00:11:14,880 --> 00:11:16,320
и она просто

273
00:11:16,320 --> 00:11:18,480
знает  мировое знание

274
00:11:18,480 --> 00:11:21,040
человеческих языков знание того, как выполнять

275
00:11:21,040 --> 00:11:23,519
задачи, и затем вы можете применять его для выполнения

276
00:11:23,519 --> 00:11:26,640
самых разных задач, поэтому мы больше не

277
00:11:26,640 --> 00:11:29,440
строим модель для обнаружения спама, а

278
00:11:29,440 --> 00:11:32,240
затем модель для обнаружения порнографии, а затем

279
00:11:32,240 --> 00:11:34,720
модель для обнаружения

280
00:11:34,720 --> 00:11:36,959
чего угодно  контент на иностранном языке и

281
00:11:36,959 --> 00:11:38,240
просто построив все эти отдельные

282
00:11:38,240 --> 00:11:39,920
контролируемые классификаторы для каждой

283
00:11:39,920 --> 00:11:42,720
задачи, мы только что создали

284
00:11:42,720 --> 00:11:46,079
модель, которая так точно понимает, что

285
00:11:46,079 --> 00:11:48,640
она делает, просто предсказывает

286
00:11:48,640 --> 00:11:52,160
следующие слова и

287
00:11:52,160 --> 00:11:54,240
т. д.  он ушел,

288
00:11:54,240 --> 00:11:58,480
ему сказали написать об Илоне Маске

289
00:11:58,480 --> 00:12:00,720
в стиле

290
00:12:00,720 --> 00:12:02,160
доктора Сьюза,

291
00:12:02,160 --> 00:12:05,040
и он начался с некоторого текста, а

292
00:12:05,040 --> 00:12:07,040
затем он генерирует больше текста, и

293
00:12:07,040 --> 00:12:09,279
способ, которым он генерирует больше текста

294
00:12:09,279 --> 00:12:10,480
,

295
00:12:10,480 --> 00:12:13,279
буквально просто предсказывает одно слово за раз, когда

296
00:12:14,880 --> 00:12:16,880
приходят следующие слова  для завершения текста,

297
00:12:16,880 --> 00:12:20,399
но у этого есть очень мощное средство,

298
00:12:20,399 --> 00:12:23,600
потому что то, что вы можете сделать с

299
00:12:23,600 --> 00:12:25,839
3, - это дать ему несколько

300
00:12:25,839 --> 00:12:28,720
примеров того, что вы хотите, чтобы

301
00:12:28,720 --> 00:12:31,760
я мог дать ему текст и сказать, что я сломал

302
00:12:31,760 --> 00:12:32,959
изменение окна

303
00:12:32,959 --> 00:12:34,800
это вопрос, что я

304
00:12:34,800 --> 00:12:37,680
сломал, я изящно сохранил день, когда

305
00:12:37,680 --> 00:12:40,000
изменил его, на вопрос, что я

306
00:12:40,000 --> 00:12:43,440
изящно сохранил, поэтому это приглашение um сообщает

307
00:12:43,440 --> 00:12:45,519
gpt 3, что я хочу,

308
00:12:45,519 --> 00:12:48,000
чтобы он делал, а затем, если я

309
00:12:48,000 --> 00:12:49,920
дам ему другое утверждение, например, я  дал

310
00:12:49,920 --> 00:12:53,200
Джону Флауэрс, я могу сказать, что gpt-3

311
00:12:53,200 --> 00:12:55,760
предскажет, какие слова будут дальше, и он будет

312
00:12:55,760 --> 00:12:58,320
следовать моей подсказке и покажет, кому я

313
00:12:58,320 --> 00:13:01,200
подарил цветы или я могу сказать, что подарил

314
00:13:01,200 --> 00:13:04,079
ей розу и гитару, и он будет

315
00:13:04,079 --> 00:13:06,639
следовать идее  узор и делать кто я

316
00:13:06,639 --> 00:13:08,639
подарил розу и гитару

317
00:13:08,639 --> 00:13:11,200
и действовать  На самом деле эта одна модель может делать

318
00:13:11,200 --> 00:13:13,920
удивительный набор вещей, в том числе

319
00:13:13,920 --> 00:13:16,560
многие, что довольно удивительно,

320
00:13:16,560 --> 00:13:18,880
чтобы дать только один пример того, что

321
00:13:18,880 --> 00:13:21,200
еще одна вещь, которую вы можете сделать,

322
00:13:21,200 --> 00:13:23,040
- это заставить ее

323
00:13:23,040 --> 00:13:25,920
переводить предложения человеческого языка в

324
00:13:25,920 --> 00:13:28,560
sql, так что это может сделать это намного  легче

325
00:13:28,560 --> 00:13:30,560
сделать cs145,

326
00:13:30,560 --> 00:13:33,839
поэтому, дав ему пару

327
00:13:33,839 --> 00:13:37,839
примеров sql-перевода текста на человеческом языке,

328
00:13:37,839 --> 00:13:39,680
который я на этот раз не показываю,

329
00:13:39,680 --> 00:13:41,760
потому что он не помещается на моем слайде, я могу

330
00:13:41,760 --> 00:13:43,839
затем дать ему предложение, например, сколько

331
00:13:43,839 --> 00:13:45,680
пользователей подписались  с начала

332
00:13:45,680 --> 00:13:47,199
2020 года,

333
00:13:47,199 --> 00:13:49,920
и он превращает его в sql, или я могу дать

334
00:13:49,920 --> 00:13:51,839
ему другой запрос, каково среднее

335
00:13:51,839 --> 00:13:54,560
количество влияний, на которые каждый пользователь подписывается,

336
00:13:54,560 --> 00:13:58,480
и снова он затем преобразует это в

337
00:13:58,480 --> 00:14:00,079
sql,

338
00:14:00,079 --> 00:14:03,760
поэтому gpt gpt-3 много знает о

339
00:14:03,760 --> 00:14:05,839
значении языка  и значение

340
00:14:05,839 --> 00:14:08,880
других вещей, таких как sql, и может свободно

341
00:14:08,880 --> 00:14:11,760
манипулировать им,

342
00:14:13,040 --> 00:14:16,079
так что это приводит нас прямо к этому

343
00:14:16,079 --> 00:14:19,360
главному значению, и как мы

344
00:14:19,360 --> 00:14:22,160
хорошо представляем значение слова, что означает

345
00:14:22,160 --> 00:14:24,079
хорошо, мы могли бы найти что-то вроде

346
00:14:24,079 --> 00:14:27,760
webster d  словарный запас и скажите хорошо,

347
00:14:27,760 --> 00:14:30,480
идея представлена словом

348
00:14:30,480 --> 00:14:32,399
идея, которую человек хочет выразить,

349
00:14:32,399 --> 00:14:34,560
используя слова, знаки и т.

350
00:14:45,120 --> 00:14:47,920
так что они думают, что значение

351
00:14:47,920 --> 00:14:51,120
слова появляется между словом, которое

352
00:14:51,120 --> 00:14:54,160
является означающим или символом, и тем,

353
00:14:54,160 --> 00:14:56,720
что оно означает означаемую вещь,

354
00:14:56,720 --> 00:14:58,959
которая является идеей или вещью, так что

355
00:14:58,959 --> 00:15:01,360
значение слова стул

356
00:15:01,360 --> 00:15:04,480
- это совокупность вещей, которые являются стульями

357
00:15:04,480 --> 00:15:06,480
и это называется денотационной

358
00:15:06,480 --> 00:15:07,839
семантикой

359
00:15:07,839 --> 00:15:10,160
- термин, который также используется и аналогичным образом

360
00:15:10,160 --> 00:15:12,720
применяется для семантики языков программирования.

361
00:15:13,920 --> 00:15:18,240
Эта модель не очень глубоко

362
00:15:18,480 --> 00:15:20,240
реализуема, например,

363
00:15:20,240 --> 00:15:22,959
как мне отойти от идеи, что хорошо,

364
00:15:22,959 --> 00:15:24,560
стул означает набор стульев в

365
00:15:24,560 --> 00:15:26,800
мире просто что-то  я могу манипулировать

366
00:15:26,800 --> 00:15:29,920
смыслом на своих компьютерах

367
00:15:29,920 --> 00:15:32,880
так, как это

368
00:15:34,000 --> 00:15:35,680
обычно делается в системах обработки естественного языка.

369
00:15:35,680 --> 00:15:38,560
s использовать

370
00:15:38,560 --> 00:15:41,920
ресурсы, такие как словари и тезаурусы,

371
00:15:41,920 --> 00:15:44,560
в частности, популярным является wordnet,

372
00:15:44,560 --> 00:15:47,759
который объединяет слова и термины в

373
00:15:47,759 --> 00:15:50,639
оба синонима, устанавливает

374
00:15:50,639 --> 00:15:52,959
слова, которые могут означать одно и то же, и

375
00:15:52,959 --> 00:15:55,360
гиперонимы, которым соответствуют, - это

376
00:15:55,360 --> 00:15:58,160
отношения, и, таким образом, это

377
00:15:58,160 --> 00:15:59,839
отношения, которые вы  знаю, что мы можем как бы

378
00:15:59,839 --> 00:16:02,959
взглянуть на гипонимы панды и

379
00:16:02,959 --> 00:16:05,199
панды - это своего рода поступок,

380
00:16:05,199 --> 00:16:07,040
какими бы они ни были, я думаю, это, вероятно,

381
00:16:07,040 --> 00:16:09,920
с красными пандами, эм, это своего рода

382
00:16:09,920 --> 00:16:11,279
плотоядное животное,

383
00:16:11,279 --> 00:16:13,040
которое является своего рода плацентой, которая является

384
00:16:13,040 --> 00:16:15,040
своего рода млекопитающим, и вы вроде  из головы вверх по

385
00:16:15,040 --> 00:16:18,160
этой иерархии гипонимов,

386
00:16:18,160 --> 00:16:21,040
так что wordnet был большим

387
00:16:21,040 --> 00:16:23,839
ресурсом для nlp, но он также был

388
00:16:23,839 --> 00:16:26,240
крайне несовершенным, поэтому в

389
00:16:26,240 --> 00:16:28,639
нем не хватало множества нюансов,

390
00:16:28,639 --> 00:16:32,079
поэтому, например, в слове net proficient

391
00:16:32,079 --> 00:16:34,639
указано как синоним хорошего, но вы

392
00:16:34,639 --> 00:16:36,480
знаете, может быть, это иногда  правда, но

393
00:16:36,480 --> 00:16:38,480
кажется, что во многих контекстах это

394
00:16:38,480 --> 00:16:39,759
неправда, и вы имеете в виду нечто совершенно

395
00:16:39,759 --> 00:16:41,759
иное, когда говорите, что «опытный», а не «

396
00:16:41,759 --> 00:16:44,480
хороший», это ограничено, поскольку человеческий

397
00:16:44,480 --> 00:16:47,600
тезаурус создал

398
00:16:47,600 --> 00:16:49,279
в частности, там много слов и

399
00:16:49,279 --> 00:16:51,839
много употреблений слов, которых просто нет,

400
00:16:51,839 --> 00:16:54,399
в том числе вы знаете что-нибудь, эм,

401
00:16:54,399 --> 00:16:56,800
то есть вы знаете, что-

402
00:16:56,800 --> 00:17:00,560
то вроде более современной терминологии, например um

403
00:17:00,560 --> 00:17:02,639
wicked, существует для злой ведьмы, но

404
00:17:02,639 --> 00:17:06,559
не для более современного разговорного использования um

405
00:17:06,559 --> 00:17:09,039
ninja конечно  там нет

406
00:17:09,039 --> 00:17:10,799
такого описания программистов, которое некоторые люди делают,

407
00:17:10,799 --> 00:17:12,880
и его

408
00:17:12,880 --> 00:17:15,439
невозможно поддерживать в курсе,

409
00:17:15,439 --> 00:17:18,079
так что это требует много человеческого труда, но

410
00:17:18,079 --> 00:17:21,280
даже когда у вас есть это,

411
00:17:21,280 --> 00:17:23,919
вы знаете, что у него есть наборы синонимов, но нет

412
00:17:23,919 --> 00:17:25,919
''  У меня действительно хорошее чувство

413
00:17:25,919 --> 00:17:29,520
слов, которое означает нечто подобное, так

414
00:17:29,520 --> 00:17:33,120
что фантастическое и великое означает нечто

415
00:17:33,120 --> 00:17:35,720
похожее, но на самом деле не

416
00:17:35,720 --> 00:17:38,480
синонимы, и поэтому идея смысла

417
00:17:38,480 --> 00:17:40,559
сходства - это то, что было бы

418
00:17:40,559 --> 00:17:42,880
действительно полезно для достижения прогресса, и

419
00:17:42,880 --> 00:17:44,840
где модели глубокого обучения

420
00:17:44,840 --> 00:17:48,320
превосходны, так что  В чем проблема

421
00:17:48,320 --> 00:17:51,039
многих традиционных НЛП? Проблема

422
00:17:51,039 --> 00:17:54,400
с большим количеством традиционных НЛП

423
00:17:54,400 --> 00:17:57,200
заключается в том, что слова рассматриваются как отдельные

424
00:17:57,200 --> 00:18:00,080
символы, поэтому у нас есть символы, такие как

425
00:18:00,080 --> 00:18:03,120
конференция в отеле.  Наши слова,

426
00:18:03,120 --> 00:18:05,760
которые в глубоком обучении говорят, мы

427
00:18:05,760 --> 00:18:08,320
называем локалистическим представлением,

428
00:18:08,320 --> 00:18:10,400
и это потому, что

429
00:18:10,400 --> 00:18:12,080
если вы

430
00:18:12,080 --> 00:18:12,799
в

431
00:18:12,799 --> 00:18:16,000
системах статистического или машинного обучения

432
00:18:16,000 --> 00:18:18,559
хотите представить эти символы,

433
00:18:18,559 --> 00:18:21,280
каждый из них является отдельной вещью, поэтому

434
00:18:21,280 --> 00:18:23,039
стандартный способ их представления, и

435
00:18:23,039 --> 00:18:24,559
это то, что  вы

436
00:18:24,559 --> 00:18:27,520
делаете что-то вроде статистической модели, если

437
00:18:27,520 --> 00:18:29,039
вы строите модель логистической

438
00:18:29,039 --> 00:18:32,799
регрессии со словами в качестве функций, заключается в том, что вы

439
00:18:32,799 --> 00:18:35,360
представляете их как один горячий вектор, поэтому у вас

440
00:18:35,360 --> 00:18:37,760
есть измерение для каждого отдельного слова,

441
00:18:37,760 --> 00:18:40,320
поэтому, возможно, как в моем примере, вот мои

442
00:18:40,320 --> 00:18:43,520
представления как  векторы для мотелей и

443
00:18:43,520 --> 00:18:44,720
отелей,

444
00:18:45,760 --> 00:18:48,080
и это означает, что у нас должны быть

445
00:18:48,080 --> 00:18:50,799
огромные векторы, соответствующие

446
00:18:50,799 --> 00:18:53,120
количеству слов в нашем словарном запасе, так что

447
00:18:53,120 --> 00:18:54,320
если у вас был

448
00:18:54,320 --> 00:18:55,919
словарь английского языка в средней школе, в нем,

449
00:18:55,919 --> 00:18:59,360
вероятно, было около 250 000 слов,

450
00:18:59,360 --> 00:19:01,440
но там  на самом деле гораздо больше слов

451
00:19:01,440 --> 00:19:03,679
в языке, так что, возможно, мы, по крайней мере,

452
00:19:03,679 --> 00:19:06,480
хотим иметь вектор размерностью 500000 мкм,

453
00:19:06,480 --> 00:19:10,400
чтобы справиться с этим,

454
00:19:10,400 --> 00:19:13,280
хорошо, но более крупный ev  ru более серьезная

455
00:19:13,280 --> 00:19:15,840
проблема с дискретными символами заключается в

456
00:19:15,840 --> 00:19:17,919
том, что у нас нет этого понятия словесных

457
00:19:17,919 --> 00:19:20,480
отношений и сходства, поэтому,

458
00:19:20,480 --> 00:19:23,280
например, в веб-поиске, если пользователь

459
00:19:23,280 --> 00:19:25,760
ищет мотель в Сиэтле, мы также хотели бы

460
00:19:25,760 --> 00:19:28,160
сопоставить документы, содержащие отель Сиэтла,

461
00:19:29,200 --> 00:19:31,520
но наша проблема в том, что  у нас

462
00:19:31,520 --> 00:19:33,840
есть горячие векторы для разных слов,

463
00:19:33,840 --> 00:19:36,320
и поэтому в формальном математическом смысле

464
00:19:36,320 --> 00:19:38,720
эти два вектора ортогональны, так что

465
00:19:41,600 --> 00:19:43,600
между

466
00:19:43,600 --> 00:19:45,600
ними нет естественного понятия сходства.

467
00:19:45,600 --> 00:19:48,000
и люди

468
00:19:48,000 --> 00:19:50,640
делали с этим гм,

469
00:19:50,640 --> 00:19:53,919
до 2010 года мы могли сказать, что мы

470
00:19:53,919 --> 00:19:56,080
могли бы использовать синонимы word net, и мы считаем

471
00:19:56,080 --> 00:19:58,080
вещи, которые перечисляют синонимы, в

472
00:19:58,080 --> 00:20:00,880
любом случае похожи или эй, может быть, мы могли бы каким-то образом

473
00:20:00,880 --> 00:20:02,640
создать

474
00:20:02,640 --> 00:20:04,640
представления слов, которые имеют

475
00:20:04,640 --> 00:20:06,559
совпадающее значение, и людей  делали все

476
00:20:06,559 --> 00:20:09,520
эти вещи, но они имели тенденцию сильно терпеть неудачу

477
00:20:09,520 --> 00:20:12,080
из-за незавершенности, поэтому вместо

478
00:20:12,080 --> 00:20:15,039
этого я хочу представить сегодня

479
00:20:15,039 --> 00:20:16,080
современный

480
00:20:16,080 --> 00:20:18,240
метод глубокого обучения.

481
00:20:22,320 --> 00:20:25,440
Как мы можем это сделать? Мы

482
00:20:30,240 --> 00:20:32,080
используем эту идею, называемую

483
00:20:32,080 --> 00:20:34,799
распределительной семантикой,

484
00:20:34,799 --> 00:20:37,679
поэтому идея распределительной семантики

485
00:20:38,720 --> 00:20:40,960
снова становится чем-то, что, когда вы впервые видите

486
00:20:40,960 --> 00:20:41,840
это,

487
00:20:41,840 --> 00:20:44,720
может показаться немного  немного сумасшедший,

488
00:20:44,720 --> 00:20:46,240
потому что вместо того, чтобы иметь что-то

489
00:20:46,240 --> 00:20:48,799
вроде денотационной семантики,

490
00:20:48,799 --> 00:20:51,919
мы сейчас собираемся сказать, что значение слова

491
00:20:51,919 --> 00:20:54,559
будет дано

492
00:20:54,559 --> 00:20:58,960
словами, которые часто кажутся близкими к нему.

493
00:20:58,960 --> 00:21:02,400
Дж. Ферт был британским

494
00:21:02,400 --> 00:21:05,120
лингвистом середины прошлого века.  и один

495
00:21:05,120 --> 00:21:07,120
из его содержательных лозунгов, который все

496
00:21:07,120 --> 00:21:09,679
цитируют в данный момент: «Вы должны

497
00:21:09,679 --> 00:21:13,039
знать слово по компании, которую он держит,

498
00:21:13,039 --> 00:21:15,520
и поэтому идея о том, что вы можете

499
00:21:16,559 --> 00:21:20,559
представить смысл слов, означающих как представление о том, в

500
00:21:20,559 --> 00:21:24,400
каком контексте они появляются, оказалась

501
00:21:24,400 --> 00:21:27,280
очень успешной.  идея одна из самых

502
00:21:27,280 --> 00:21:30,320
успешных идей, которая используется в

503
00:21:30,320 --> 00:21:32,880
статистике и глубоком обучении nlp,

504
00:21:32,880 --> 00:21:36,000
это на самом деле интересная идея с

505
00:21:36,000 --> 00:21:37,760
более философской точки

506
00:21:38,480 --> 00:21:40,559
зрения, так что  Интересные

507
00:21:40,559 --> 00:21:42,480
связи, например, в

508
00:21:42,480 --> 00:21:44,799
более поздних работах Витгенштейна, он был

509
00:21:44,799 --> 00:21:47,520
очарован теорией употребления значения, и

510
00:21:47,520 --> 00:21:49,840
это в некотором смысле грех. Теория

511
00:21:49,840 --> 00:21:52,000
употребления значения, но знаете ли вы, что это

512
00:21:52,000 --> 00:21:53,840
окончательная теория семантики, это на

513
00:21:53,840 --> 00:21:56,159
самом деле все еще довольно спорно, но

514
00:21:56,159 --> 00:21:57,520
оказывается  чрезвычайно

515
00:21:57,520 --> 00:22:00,799
вычислительный смысл семантики,

516
00:22:00,799 --> 00:22:03,760
который только что привел к тому, что он

517
00:22:03,760 --> 00:22:06,480
очень успешно используется повсюду в системах глубокого обучения

518
00:22:06,480 --> 00:22:07,840
,

519
00:22:07,840 --> 00:22:11,440
поэтому, когда слово появляется в тексте, у

520
00:22:11,440 --> 00:22:13,440
него есть контекст, который представляет собой набор

521
00:22:13,440 --> 00:22:15,520
слов, которые появляются,

522
00:22:15,520 --> 00:22:18,559
и поэтому для конкретного слова мой пример

523
00:22:18,559 --> 00:22:20,159
вот банковское дело,

524
00:22:20,159 --> 00:22:22,880
мы найдем кучу мест, где

525
00:22:22,880 --> 00:22:24,720
банковское дело встречается

526
00:22:24,720 --> 00:22:27,200
в текстах, и мы соберем какие-то

527
00:22:27,200 --> 00:22:30,080
близкие слова в качестве контекстных слов, и мы увидим,

528
00:22:30,960 --> 00:22:32,880
что те слова, которые появляются

529
00:22:32,880 --> 00:22:35,280
таким мутным коричневым цветом вокруг

530
00:22:35,280 --> 00:22:38,480
банковского дела,  эти контекстные слова в

531
00:22:38,480 --> 00:22:41,520
некотором смысле будут представлять значение

532
00:22:41,520 --> 00:22:43,200
слова «банковский»,

533
00:22:43,200 --> 00:22:45,120
пока я здесь, позвольте мне упомянуть

534
00:22:45,120 --> 00:22:47,039
одно различие, которое будет регулярно появляться,

535
00:22:48,159 --> 00:22:51,280
когда мы  Говоря о слове um в

536
00:22:51,280 --> 00:22:54,080
нашем классе обработки естественного языка,

537
00:22:54,080 --> 00:22:57,200
у нас как бы есть два смысла слова,

538
00:22:57,200 --> 00:22:59,840
которые называются типами и

539
00:22:59,840 --> 00:23:03,039
токенами, так что есть конкретный пример

540
00:23:03,039 --> 00:23:05,760
для слова, так что в первом примере

541
00:23:05,760 --> 00:23:08,000
проблемы государственного долга превращаются

542
00:23:08,000 --> 00:23:10,000
в банковские кризисы.  банковское дело

543
00:23:10,000 --> 00:23:12,640
там, и это символ слова

544
00:23:12,640 --> 00:23:16,000
банковский, но затем я собрал

545
00:23:16,000 --> 00:23:18,880
кучу примеров цитат без кавычек слова

546
00:23:18,880 --> 00:23:21,200
банковское дело, и когда я говорю слово банковское дело

547
00:23:21,200 --> 00:23:23,760
и кучу его примеров, я тогда

548
00:23:23,760 --> 00:23:26,240
рассматриваю банковское дело как тип, который  относится

549
00:23:26,240 --> 00:23:29,039
к тому, что вы знаете, как используется и значение

550
00:23:29,039 --> 00:23:33,360
слова «банкинг» в разных случаях,

551
00:23:33,360 --> 00:23:34,880
хорошо, так

552
00:23:34,880 --> 00:23:37,360
что же, что мы собираемся делать

553
00:23:37,360 --> 00:23:38,400
с этими

554
00:23:40,720 --> 00:23:44,080
моделями распределения языка?

555
00:23:47,679 --> 00:23:51,120
в качестве векторов,

556
00:23:51,120 --> 00:23:53,520
которые мы хотим

557
00:23:53,520 --> 00:23:55,279
построить плотный

558
00:23:55,279 --> 00:23:57,679
вектор с действительными значениями

559
00:23:57,679 --> 00:24:00,960
для каждого слова, который в некотором смысле

560
00:24:00,960 --> 00:24:03,679
представляет значение этого слова,

561
00:24:03,679 --> 00:24:05,600
и способ, которым все это представляет значение

562
00:24:05,600 --> 00:24:07,679
этого слова, заключается в

563
00:24:07,679 --> 00:24:10,960
том, что  is vector будет полезен

564
00:24:10,960 --> 00:24:14,000
для предсказания других слов, которые встречаются в

565
00:24:15,520 --> 00:24:16,720
контексте этого

566
00:24:16,720 --> 00:24:19,600
um, поэтому в этом примере, чтобы держать его

567
00:24:19,600 --> 00:24:21,520
управляемым на слайдах,

568
00:24:21,520 --> 00:24:24,320
векторы только восьмимерные um,

569
00:24:24,320 --> 00:24:25,360
но

570
00:24:25,360 --> 00:24:27,600
на самом деле мы используем значительно большие

571
00:24:27,600 --> 00:24:29,360
векторы, поэтому очень распространенный размер на

572
00:24:29,360 --> 00:24:32,400
самом деле составляет 300  размерные векторы в

573
00:24:32,400 --> 00:24:35,919
порядке, поэтому для каждого слова, являющегося типом слова,

574
00:24:35,919 --> 00:24:38,720
у нас будет вектор слова,

575
00:24:38,720 --> 00:24:41,360
они также используются с другими

576
00:24:41,360 --> 00:24:42,480
именами,

577
00:24:42,480 --> 00:24:43,840
которые называются более новыми представлениями слов,

578
00:24:45,360 --> 00:24:47,520
или по какой-то причине они станут более ясными

579
00:24:47,520 --> 00:24:49,600
на следующем слайде.  называются

580
00:24:49,600 --> 00:24:52,000
вложениями слов, поэтому теперь это

581
00:24:52,000 --> 00:24:54,240
распределенное представление, а не

582
00:24:54,240 --> 00:24:56,559
локалистическое представление, потому что

583
00:24:56,559 --> 00:25:00,480
значение банка слов распределено

584
00:25:00,480 --> 00:25:04,320
по всем 300 измерениям вектора.

585
00:25:04,320 --> 00:25:06,799
Хорошо, это называется встраиванием слов,

586
00:25:06,799 --> 00:25:09,200
потому что эффективно, когда у нас есть целая

587
00:25:09,200 --> 00:25:11,039
куча слов

588
00:25:11,039 --> 00:25:14,240
эти представления помещают их все

589
00:25:14,240 --> 00:25:16,960
в многомерное векторное пространство,

590
00:25:16,960 --> 00:25:19,600
и поэтому они встроены в это пространство,

591
00:25:19,600 --> 00:25:22,640
теперь, к сожалению, люди очень

592
00:25:22,640 --> 00:25:25,760
плохо выглядят.  ng в 300-

593
00:25:25,760 --> 00:25:28,159
мерных векторных пространствах или даже в 8-

594
00:25:28,159 --> 00:25:30,720
мерных векторных пространствах, поэтому

595
00:25:30,720 --> 00:25:32,640
единственное, что я действительно могу показать вам

596
00:25:32,640 --> 00:25:35,120
здесь, - это двумерная проекция

597
00:25:35,120 --> 00:25:38,880
этого пространства, даже если это полезно,

598
00:25:38,880 --> 00:25:40,799
но также важно понимать, что когда

599
00:25:40,799 --> 00:25:42,480
вы делаете  двумерная

600
00:25:42,480 --> 00:25:45,200
проекция 300-мерного пространства,

601
00:25:45,200 --> 00:25:47,279
вы теряете почти всю

602
00:25:47,279 --> 00:25:49,520
информацию в этом пространстве, и многие

603
00:25:49,520 --> 00:25:51,520
вещи будут раздавлены вместе, что на

604
00:25:51,520 --> 00:25:53,919
самом деле не заслуживает улучшения.

605
00:25:56,640 --> 00:25:59,120
не могу увидеть

606
00:25:59,120 --> 00:26:01,039
ни одного из них вообще,

607
00:26:01,039 --> 00:26:04,240
но если я увеличу, а затем я увеличу

608
00:26:04,240 --> 00:26:07,360
еще больше, то вы уже увидите,

609
00:26:07,360 --> 00:26:09,360
что репрезентации, которые мы изучили

610
00:26:09,360 --> 00:26:11,440
распределенно,

611
00:26:11,440 --> 00:26:14,240
просто хорошо справляются с группировкой

612
00:26:14,240 --> 00:26:17,039
похожих

613
00:26:17,039 --> 00:26:20,000
слов, так что в этом  своего рода общая картина, которую

614
00:26:20,000 --> 00:26:22,640
я могу увеличить в одной части пространства, на

615
00:26:22,640 --> 00:26:25,200
самом деле это та часть, которая находится здесь, с этой

616
00:26:25,200 --> 00:26:28,240
точки зрения, и в ней есть слова для

617
00:26:28,240 --> 00:26:31,840
обозначения стран, так что не только страны

618
00:26:31,840 --> 00:26:34,240
обычно сгруппированы вместе.

619
00:26:34,240 --> 00:26:36,960
Различные подгруппы

620
00:26:36,960 --> 00:26:38,400
стран

621
00:26:38,400 --> 00:26:40,320
имеют определенный смысл, и

622
00:26:40,320 --> 00:26:43,279
здесь, внизу, у нас есть слова национальности, если

623
00:26:43,279 --> 00:26:45,279
мы перейдем в другую часть пространства, мы

624
00:26:45,279 --> 00:26:47,440
можем увидеть разные типы слов, так что здесь

625
00:26:47,440 --> 00:26:48,559
есть

626
00:26:48,559 --> 00:26:51,200
глаголы, и у нас есть такие, как приходить и уходить,

627
00:26:51,200 --> 00:26:53,600
очень похожи ммм

628
00:26:53,600 --> 00:26:56,159
Говорящие и думающие слова говорят думают

629
00:26:56,159 --> 00:26:59,120
ожидают чего-то похожего, и

630
00:27:00,640 --> 00:27:02,880
рядом в правом нижнем углу у нас есть своего рода

631
00:27:02,880 --> 00:27:05,440
глагольные вспомогательные средства и связки, поэтому у нас

632
00:27:05,440 --> 00:27:08,640
есть волоски формы глагола, чтобы быть,

633
00:27:08,640 --> 00:27:11,679
а некоторые содержательные глаголы похожи

634
00:27:11,679 --> 00:27:14,159
на глаголы связок, потому что они описывают

635
00:27:14,159 --> 00:27:17,200
заявляет, что вы знаете, что он по-прежнему злился, он

636
00:27:17,200 --> 00:27:19,279
разозлился, и поэтому они на самом деле

637
00:27:19,279 --> 00:27:21,840
затем сгруппированы рядом со словом, которым

638
00:27:21,840 --> 00:27:23,679
должен быть глагол, так что в этом пространстве есть много

639
00:27:23,679 --> 00:27:26,320
интересной структуры

640
00:27:26,320 --> 00:27:29,520
um, которая затем

641
00:27:29,520 --> 00:27:31,760
представляет значение слов, поэтому алгоритм

642
00:27:31,760 --> 00:27:33,279
i '  Я собираюсь

643
00:27:33,279 --> 00:27:36,640
представить вам тот, который называется word

644
00:27:36,640 --> 00:27:39,760
to vec, который был введен Тамашом

645
00:27:39,760 --> 00:27:42,880
Микуловым и его коллегами в 2013 году

646
00:27:42,880 --> 00:27:45,039
в качестве основы для изучения векторов слов,

647
00:27:45,039 --> 00:27:46,559
и это своего рода простой и легкий  y, чтобы

648
00:27:46,559 --> 00:27:48,480
понять, с чего начать,

649
00:27:48,480 --> 00:27:50,559
поэтому идея состоит в том,

650
00:27:50,559 --> 00:27:52,159
что у нас есть

651
00:27:52,159 --> 00:27:54,480
много текста откуда-то, что мы

652
00:27:54,480 --> 00:27:56,960
обычно называем корпусом текста

653
00:27:56,960 --> 00:27:59,440
Корпус - это просто латинское слово для тела,

654
00:27:59,440 --> 00:28:02,159
поэтому это основной текст,

655
00:28:02,159 --> 00:28:05,200
и поэтому мы выбираем фиксированный словарь

656
00:28:05,200 --> 00:28:07,520
которые, как правило, будут большими, но

657
00:28:07,520 --> 00:28:09,520
тем не менее усеченными, поэтому мы избавляемся от

658
00:28:09,520 --> 00:28:11,520
некоторых действительно редких слов, чтобы мы

659
00:28:11,520 --> 00:28:13,840
могли сказать размер словарного запаса в

660
00:28:13,840 --> 00:28:16,880
четыреста тысяч, а затем мы

661
00:28:16,880 --> 00:28:20,080
создаем для себя вектор для каждого

662
00:28:20,080 --> 00:28:21,679
слова,

663
00:28:21,679 --> 00:28:24,960
хорошо, поэтому мы делаем то, что мы хотим

664
00:28:24,960 --> 00:28:28,880
выяснить, какой вектор подходит для

665
00:28:28,880 --> 00:28:31,760
каждого слова, и действительно интересно

666
00:28:31,760 --> 00:28:35,120
то, что мы можем узнать эти

667
00:28:35,120 --> 00:28:38,559
векторы слов из большой кучи текста,

668
00:28:38,559 --> 00:28:41,279
выполнив задачу распределения подобия,

669
00:28:41,279 --> 00:28:44,159
чтобы иметь возможность хорошо предсказать, какие

670
00:28:44,159 --> 00:28:47,200
слова встречаются в контексте  другими

671
00:28:47,200 --> 00:28:49,840
словами, в частности, мы собираемся

672
00:28:49,840 --> 00:28:54,000
перебирать слова в тексте, и поэтому

673
00:28:54,000 --> 00:28:56,720
в любой момент у нас есть центральное слово um

674
00:28:56,720 --> 00:28:57,440
c

675
00:28:57,440 --> 00:28:59,919
и контекстные слова за его пределами, которые

676
00:28:59,919 --> 00:29:01,600
мы назовем o,

677
00:29:01,600 --> 00:29:02,799
а затем

678
00:29:02,799 --> 00:29:06,000
на основе текущего слова vecto  rs мы

679
00:29:06,000 --> 00:29:08,559
собираемся вычислить вероятность появления

680
00:29:08,559 --> 00:29:10,480
контекстного слова при

681
00:29:11,840 --> 00:29:14,799
данном центральном слове в соответствии с нашей

682
00:29:14,799 --> 00:29:16,240
текущей моделью,

683
00:29:16,240 --> 00:29:18,880
но тогда мы знаем, что определенные слова

684
00:29:18,880 --> 00:29:21,279
действительно встречались в контексте этого

685
00:29:21,279 --> 00:29:23,760
центрального слова, и поэтому мы

686
00:29:23,760 --> 00:29:26,480
хотим сохранить  корректировка векторов слов, чтобы

687
00:29:26,480 --> 00:29:29,840
максимизировать вероятность, которая присваивается

688
00:29:29,840 --> 00:29:32,799
словам, которые действительно встречаются в

689
00:29:32,799 --> 00:29:35,520
контексте центрального слова, когда мы проходим

690
00:29:35,520 --> 00:29:37,200
через эти тексты,

691
00:29:37,200 --> 00:29:38,720
чтобы начать делать это немного более

692
00:29:38,720 --> 00:29:42,480
конкретным, это то, что мы делаем,

693
00:29:42,480 --> 00:29:45,520
так что у нас есть  фрагмент текста, мы выбираем наше

694
00:29:45,520 --> 00:29:48,720
центральное слово, которое находится здесь, а затем

695
00:29:48,720 --> 00:29:51,919
говорим хорошо,

696
00:29:52,399 --> 00:29:55,200
если модель прогнозирования

697
00:29:55,200 --> 00:29:58,000
вероятности контекстных слов с учетом центрального слова,

698
00:29:58,000 --> 00:30:00,159
и эта модель придет в форму через минуту,

699
00:30:00,159 --> 00:30:02,320
но она определяется в терминах наших векторов слов,

700
00:30:03,360 --> 00:30:06,480
поэтому  давайте посмотрим, какую вероятность он дает

701
00:30:06,480 --> 00:30:08,640
словам, которые на самом деле встречаются

702
00:30:08,640 --> 00:30:12,480
в контексте этого слова,

703
00:30:12,480 --> 00:30:14,960
это дает им некоторую вероятность, но, может

704
00:30:14,960 --> 00:30:16,880
быть, было бы неплохо, если бы вероятность знака

705
00:30:16,880 --> 00:30:20,080
была выше, тогда как  мы меняем

706
00:30:20,080 --> 00:30:22,000
векторы слов, чтобы повысить эти

707
00:30:22,000 --> 00:30:24,880
вероятности, и поэтому мы сделаем некоторые

708
00:30:24,880 --> 00:30:27,360
вычисления, чтобы они были центральным

709
00:30:27,360 --> 00:30:29,440
словом, а затем мы просто перейдем к

710
00:30:29,440 --> 00:30:31,919
следующему слову, а затем мы сделаем такие

711
00:30:31,919 --> 00:30:34,240
же вычисления и продолжим

712
00:30:34,240 --> 00:30:35,440
разбивать

713
00:30:35,440 --> 00:30:38,320
Итак, большой вопрос в том, что

714
00:30:38,320 --> 00:30:40,320
мы делаем для определения

715
00:30:40,320 --> 00:30:43,440
вероятности того, что слово встречается в

716
00:30:43,440 --> 00:30:46,080
контексте центрального слова,

717
00:30:46,080 --> 00:30:48,240
и это центральная часть того, что

718
00:30:48,240 --> 00:30:51,840
мы разрабатываем как объект слова,

719
00:30:54,159 --> 00:30:56,000
так что это общая модель, которую мы

720
00:30:56,000 --> 00:30:58,480
хотим  чтобы использовать так

721
00:30:58,480 --> 00:31:01,200
для каждой позиции в нашем корпусе нашего

722
00:31:01,200 --> 00:31:02,399
тела текста,

723
00:31:02,399 --> 00:31:05,120
мы хотим предсказать контекстные слова

724
00:31:05,120 --> 00:31:07,279
в окне фиксированного размера m,

725
00:31:07,279 --> 00:31:10,000
учитывая центральное слово wj,

726
00:31:10,000 --> 00:31:12,720
и мы хотим научиться делать это,

727
00:31:12,720 --> 00:31:15,039
поэтому мы хотим дать высокую вероятность

728
00:31:15,039 --> 00:31:17,600
словам, которые  происходят в контексте,

729
00:31:17,600 --> 00:31:19,760
и поэтому мы

730
00:31:19,760 --> 00:31:21,519
собираемся выяснить, какова формальная

731
00:31:21,519 --> 00:31:25,120
вероятность данных относительно того, насколько хорошо мы

732
00:31:25,120 --> 00:31:27,360
выполняем работу по предсказанию слов в контексте

733
00:31:27,360 --> 00:31:28,960
других слов

734
00:31:28,960 --> 00:31:32,159
и так формально, что вероятность

735
00:31:32,159 --> 00:31:34,159
будет определено я  n членов наших векторов слов,

736
00:31:34,159 --> 00:31:36,000
поэтому они являются параметрами нашей

737
00:31:36,000 --> 00:31:39,600
модели, и она будет рассчитана как

738
00:31:39,600 --> 00:31:41,360
взятие произведения

739
00:31:41,360 --> 00:31:43,919
каждого слова в качестве центрального слова, а

740
00:31:43,919 --> 00:31:45,760
затем произведения каждого слова и

741
00:31:45,760 --> 00:31:48,880
окна вокруг вероятности

742
00:31:48,880 --> 00:31:51,120
предсказания  это контекстное слово

743
00:31:51,120 --> 00:31:54,080
в центральном слове,

744
00:31:54,080 --> 00:31:56,240
и поэтому для изучения этой модели у нас

745
00:31:56,240 --> 00:31:58,159
будет целевая функция, иногда

746
00:31:58,159 --> 00:32:00,559
также называемая затратами или потерями, которые мы

747
00:32:00,559 --> 00:32:03,200
хотим оптимизировать, и, по сути,

748
00:32:03,200 --> 00:32:06,799
мы хотим сделать, чтобы максимизировать

749
00:32:06,799 --> 00:32:09,279
вероятность  контекст, который мы видим вокруг

750
00:32:09,279 --> 00:32:10,720
центральных слов,

751
00:32:10,720 --> 00:32:13,200
но следуя стандартной практике, мы

752
00:32:13,200 --> 00:32:15,760
немного поправляемся,

753
00:32:15,760 --> 00:32:17,840
потому что вместо того, чтобы иметь дело с

754
00:32:17,840 --> 00:32:20,559
продуктами, легче иметь дело с суммами,

755
00:32:20,559 --> 00:32:23,200
и поэтому мы работаем с логической вероятностью, и как

756
00:32:23,200 --> 00:32:25,120
только мы берем логарифмическую вероятность, все наши

757
00:32:25,120 --> 00:32:26,080
продукты

758
00:32:26,080 --> 00:32:29,279
превращаются в суммы, мы также работаем  со

759
00:32:29,279 --> 00:32:31,919
средним логарифмическим правдоподобием, поэтому у нас есть

760
00:32:31,919 --> 00:32:34,480
термин один на t для количества

761
00:32:34,480 --> 00:32:37,440
слов в корпусе, и, наконец, без

762
00:32:37,440 --> 00:32:39,039
особой причины

763
00:32:39,039 --> 00:32:40,880
мы предпочитаем минимизировать нашу целевую

764
00:32:40,880 --> 00:32:43,200
функцию, а не t  Чтобы максимизировать ее, мы

765
00:32:43,200 --> 00:32:46,159
вставляем туда знак минус, а затем,

766
00:32:46,159 --> 00:32:49,519
минимизируя эту целевую функцию

767
00:32:49,519 --> 00:32:51,919
j теты, которая

768
00:32:51,919 --> 00:32:56,640
максимизирует нашу точность прогнозов,

769
00:32:56,640 --> 00:32:58,320
хорошо,

770
00:32:58,320 --> 00:33:01,120
так что это настройка, но мы все еще не

771
00:33:01,120 --> 00:33:05,039
добились никакого прогресса в том, как мы

772
00:33:05,039 --> 00:33:06,960
вычисляем вероятность  слово, встречающееся

773
00:33:06,960 --> 00:33:10,399
в контексте, заданном центральным словом, и,

774
00:33:10,399 --> 00:33:12,559
таким образом, мы на самом деле собираемся это сделать

775
00:33:12,559 --> 00:33:16,559
: у нас есть векторные представления

776
00:33:16,559 --> 00:33:19,679
для каждого слова, и мы собираемся вычислить

777
00:33:19,679 --> 00:33:21,440
вероятность

778
00:33:21,440 --> 00:33:24,480
просто в терминах векторов слов.

779
00:33:24,480 --> 00:33:26,320
Дело в небольшом техническом

780
00:33:26,320 --> 00:33:28,320
моменте, который мы на самом деле собираемся дать

781
00:33:28,320 --> 00:33:31,679
каждому слову два вектора слов,

782
00:33:31,679 --> 00:33:33,600
один вектор слова, когда он используется

783
00:33:33,600 --> 00:33:36,320
как центральное слово, и другой

784
00:33:36,320 --> 00:33:38,799
вектор слова, когда он используется как контекстное

785
00:33:38,799 --> 00:33:42,159
слово, это сделано, потому что это просто

786
00:33:42,159 --> 00:33:44,880
упрощает  математика и оптимизация,

787
00:33:44,880 --> 00:33:46,880
так что это кажется немного уродливым, но на

788
00:33:46,880 --> 00:33:47,919
самом деле

789
00:33:47,919 --> 00:33:51,039
значительно упрощает построение векторов слов,

790
00:33:51,039 --> 00:33:51,760
и на

791
00:33:51,760 --> 00:33:53,600
самом деле мы можем вернуться к этому и

792
00:33:53,600 --> 00:33:56,399
обсудить это позже, но это то, что это

793
00:33:56,399 --> 00:33:59,120
такое, и поэтому o  Поскольку у нас есть эти векторы слов,

794
00:34:02,320 --> 00:34:04,720
уравнение, которое мы собираемся использовать для

795
00:34:04,720 --> 00:34:06,880
определения вероятности

796
00:34:06,880 --> 00:34:08,879
появления контекстного слова с учетом

797
00:34:08,879 --> 00:34:10,800
центрального слова, состоит в том, что мы собираемся

798
00:34:10,800 --> 00:34:13,119
вычислить его, используя выражение в

799
00:34:13,119 --> 00:34:15,760
середине внизу моего слайда,

800
00:34:15,760 --> 00:34:17,119
поэтому

801
00:34:17,119 --> 00:34:18,639
давайте  как бы

802
00:34:18,639 --> 00:34:21,760
раздвинуть это

803
00:34:21,760 --> 00:34:22,960
немного дальше,

804
00:34:23,760 --> 00:34:25,280
так

805
00:34:25,280 --> 00:34:28,079
что то, что мы имеем здесь с этим выражением,

806
00:34:28,079 --> 00:34:31,199
таково для определенного центрального слова и

807
00:34:31,199 --> 00:34:32,560
определенного

808
00:34:32,560 --> 00:34:35,679
контекстного слова o мы собираемся

809
00:34:35,679 --> 00:34:38,239
найти векторное представление каждого слова,

810
00:34:38,239 --> 00:34:40,879
чтобы они были u  of o и v of c,

811
00:34:40,879 --> 00:34:42,719
и тогда мы просто

812
00:34:42,719 --> 00:34:46,239
возьмем скалярное произведение этих двух векторов, так что

813
00:34:46,239 --> 00:34:48,719
скалярное произведение является естественной мерой

814
00:34:48,719 --> 00:34:51,119
сходства между словами, потому что в любом

815
00:34:51,119 --> 00:34:52,320
конкретном

816
00:34:54,639 --> 00:34:57,359
измерении вы получите некоторый компонент,

817
00:34:57,359 --> 00:35:00,160
который добавляет  к скалярным произведениям um, если

818
00:35:00,160 --> 00:35:01,599
оба отрицательные, это добавит много к скалярному

819
00:35:01,599 --> 00:35:03,920
произведению; некоторые, если одно положительное, а другое

820
00:35:03,920 --> 00:35:07,440
отрицательное, um вычтут из этой

821
00:35:07,440 --> 00:35:09,760
меры сходства um, если оба они

822
00:35:09,760 --> 00:35:12,079
равны нулю, это не изменит сходства,

823
00:35:12,079 --> 00:35:14,640
поэтому  это вроде с  кажется правдоподобной

824
00:35:14,640 --> 00:35:17,040
идеей просто взять скалярное произведение и

825
00:35:17,040 --> 00:35:19,760
хорошо подумать, если два слова имеют более крупный

826
00:35:19,760 --> 00:35:22,400
скалярный продукт, что означает, что они более

827
00:35:22,400 --> 00:35:23,599
похожи,

828
00:35:23,599 --> 00:35:26,160
и после этого

829
00:35:26,160 --> 00:35:29,040
мы как бы действительно ничего не делаем,

830
00:35:29,040 --> 00:35:31,680
кроме как хорошо, мы хотим использовать  скалярные произведения, чтобы

831
00:35:31,680 --> 00:35:34,320
представить сходство слов, и теперь давайте

832
00:35:34,320 --> 00:35:37,599
сделаем самую глупую вещь, которую мы знаем,

833
00:35:37,599 --> 00:35:39,359
как превратить это в распределение вероятностей

834
00:35:40,640 --> 00:35:44,160
хорошо, что у нас хорошо получается, во-первых,

835
00:35:44,160 --> 00:35:46,800
хорошо, взяв скалярное произведение двух векторов,

836
00:35:46,800 --> 00:35:48,400
которые могут быть положительными или

837
00:35:48,400 --> 00:35:51,040
отрицательными, но хорошо, если  мы хотим иметь

838
00:35:51,040 --> 00:35:53,040
вероятности, у которых не может быть отрицательных

839
00:35:53,040 --> 00:35:55,359
вероятностей, поэтому простой способ избежать

840
00:35:55,359 --> 00:35:57,280
отрицательных вероятностей -

841
00:35:57,280 --> 00:35:59,839
возвести их в степень, потому что тогда мы знаем,

842
00:35:59,839 --> 00:36:02,720
что все положительно, и поэтому мы

843
00:36:02,720 --> 00:36:04,800
всегда получаем положительное число в

844
00:36:04,800 --> 00:36:06,480
числителе,

845
00:36:06,480 --> 00:36:09,440
но для вероятностей мы также  хотите,

846
00:36:09,440 --> 00:36:11,599
чтобы числа составляли единицу, чтобы у

847
00:36:11,599 --> 00:36:13,920
нас было распределение вероятностей, поэтому мы

848
00:36:13,920 --> 00:36:16,240
просто нормализуем очевидным способом,

849
00:36:16,240 --> 00:36:19,440
когда делим на

850
00:36:19,440 --> 00:36:21,680
сумму числителя количества для ea  ch

851
00:36:21,680 --> 00:36:23,920
другое слово и словарь, и

852
00:36:23,920 --> 00:36:26,000
поэтому обязательно это дает нам

853
00:36:26,000 --> 00:36:28,640
распределение вероятностей,

854
00:36:28,640 --> 00:36:30,480
поэтому все остальное, о чем я только что

855
00:36:30,480 --> 00:36:32,079
говорил, через

856
00:36:32,079 --> 00:36:33,680
то, что мы используем, это то, что

857
00:36:33,680 --> 00:36:36,400
называется функцией softmax, поэтому функция softmax

858
00:36:36,400 --> 00:36:39,200
будет принимать любой

859
00:36:39,200 --> 00:36:41,839
вектор rn и  превратить его в вещи

860
00:36:41,839 --> 00:36:46,000
между нулем и единицей,

861
00:36:46,000 --> 00:36:48,480
чтобы мы могли взять числа и пропустить их

862
00:36:48,480 --> 00:36:50,960
через этот мягкий максимум и превратить их в

863
00:36:50,960 --> 00:36:53,839
распределение вероятностей, так что

864
00:36:53,839 --> 00:36:55,920
название происходит от того факта, что это что-

865
00:36:55,920 --> 00:36:58,800
то вроде максимума, так что из-за того факта, что

866
00:36:58,800 --> 00:37:01,119
что мы возведем в степень, что действительно

867
00:37:01,119 --> 00:37:04,880
подчеркивает большое содержание в

868
00:37:04,880 --> 00:37:08,480
разных измерениях вычисления

869
00:37:08,480 --> 00:37:11,440
сходства, поэтому большая часть вероятности

870
00:37:11,440 --> 00:37:14,880
идет на самые похожие вещи, и

871
00:37:14,880 --> 00:37:17,200
это называется мягким, потому что он не

872
00:37:17,200 --> 00:37:19,760
делает этого абсолютно, он все равно

873
00:37:19,760 --> 00:37:23,359
дает некоторую вероятность всему

874
00:37:23,359 --> 00:37:26,079
это хоть немного похоже, я

875
00:37:26,079 --> 00:37:27,760
имею в виду, с другой стороны, это немного

876
00:37:27,760 --> 00:37:30,720
странное имя, потому что вы знаете, что max обычно

877
00:37:30,720 --> 00:37:33,920
берет набор вещей и просто возвращает

878
00:37:33,920 --> 00:37:36,079
один  самый большой из них, в то время как

879
00:37:36,079 --> 00:37:39,680
softmax принимает набор чисел

880
00:37:39,680 --> 00:37:42,079
и масштабирует их, но возвращает

881
00:37:42,079 --> 00:37:45,520
все распределение вероятностей,

882
00:37:45,520 --> 00:37:48,160
хорошо, так что теперь у нас есть все части

883
00:37:48,160 --> 00:37:51,839
нашей модели, и как нам

884
00:37:51,839 --> 00:37:54,320
сделать наши векторы слов

885
00:37:54,320 --> 00:37:57,119
хорошо, представление о том, что мы хотим  Чтобы сделать,

886
00:37:57,119 --> 00:37:59,440
мы хотим

887
00:37:59,440 --> 00:38:02,880
возиться с нашими векторами слов таким образом,

888
00:38:02,880 --> 00:38:05,440
чтобы мы минимизировали наши потери i, чтобы мы

889
00:38:05,440 --> 00:38:07,920
максимизировали вероятность слов,

890
00:38:07,920 --> 00:38:10,560
которые мы действительно видели в

891
00:38:10,560 --> 00:38:13,440
контексте центрального слова, и поэтому

892
00:38:13,440 --> 00:38:16,880
тета тета представляет все параметры нашей

893
00:38:16,880 --> 00:38:20,480
модели  в одном очень длинном векторе,

894
00:38:20,480 --> 00:38:22,800
поэтому для нашей модели здесь единственными

895
00:38:22,800 --> 00:38:26,320
параметрами являются наши векторы слов, поэтому

896
00:38:26,320 --> 00:38:28,800
для каждого слова у нас есть

897
00:38:28,800 --> 00:38:31,920
два вектора, это вектор контекста и его

898
00:38:31,920 --> 00:38:34,480
центральный вектор, и каждый из них является

899
00:38:34,480 --> 00:38:37,520
вектором размерности ad, где d может быть 300,

900
00:38:37,520 --> 00:38:40,640
и у нас есть v many  слов,

901
00:38:40,640 --> 00:38:43,440
так что мы получаем этот большой огромный

902
00:38:43,440 --> 00:38:45,920
вектор длиной 2 v,

903
00:38:45,920 --> 00:38:47,440
который,

904
00:38:47,440 --> 00:38:50,079
если у вас есть 500000 слов, умноженных на

905
00:38:50,079 --> 00:38:52,960
300

906
00:38:52,960 --> 00:38:55,760
измерений времени, маленький метод, который я могу сделать в своей

907
00:38:55,760 --> 00:38:57,040
голове, но у него есть миллионы и

908
00:38:57,040 --> 00:38:58,160
миллионы параметров, поэтому  У меня миллионы и

909
00:38:58,160 --> 00:38:59,680
миллионы параметров,

910
00:38:59,680 --> 00:39:03,520
и мы как-то хотим поиграть с ними все,

911
00:39:03,520 --> 00:39:05,839
чтобы

912
00:39:05,839 --> 00:39:08,400
максимизировать предсказание контекстных слов,

913
00:39:08,400 --> 00:39:10,400
и поэтому мы собираемся это сделать, а

914
00:39:10,400 --> 00:39:12,000
затем

915
00:39:12,000 --> 00:39:14,960
используем исчисление, поэтому мы хотим

916
00:39:14,960 --> 00:39:16,480
сделать это  что мы видели

917
00:39:16,480 --> 00:39:20,800
ранее и говорим, да ладно, с этой

918
00:39:20,800 --> 00:39:25,760
целевой функцией мы можем вычислить

919
00:39:25,760 --> 00:39:28,640
производные, и поэтому мы можем определить,

920
00:39:28,640 --> 00:39:31,680
где находится градиент, чтобы мы могли спуститься с

921
00:39:31,680 --> 00:39:34,400
холма, чтобы минимизировать потери, так что мы в

922
00:39:34,400 --> 00:39:37,440
какой-то момент и можем вычислить  выяснить,

923
00:39:37,440 --> 00:39:40,839
что идет под гору, и затем мы можем

924
00:39:40,839 --> 00:39:44,720
постепенно спускаться и улучшать

925
00:39:44,720 --> 00:39:47,119
нашу модель, и наша работа будет

926
00:39:47,119 --> 00:39:50,240
заключаться в вычислении всех этих векторных

927
00:39:50,240 --> 00:39:52,400
градиентов.

928
00:39:57,599 --> 00:40:00,800
подробнее о том, как мы

929
00:40:00,800 --> 00:40:05,280
можем это сделать,

930
00:40:05,280 --> 00:40:06,560
и

931
00:40:06,560 --> 00:40:09,280
еще пара слайдов здесь, но,

932
00:40:09,280 --> 00:40:11,440
может быть, я просто попробую

933
00:40:11,440 --> 00:40:14,800
снова перемешать вещи и перейду к своей

934
00:40:14,800 --> 00:40:17,839
интерактивной доске, что мы хотели сделать

935
00:40:18,640 --> 00:40:20,079
правильно, чтобы

936
00:40:20,079 --> 00:40:24,079
у нас было общее

937
00:40:24,079 --> 00:40:27,760
у нас было общее j theta  что

938
00:40:27,760 --> 00:40:30,720
мы хотели мин  представьте себе нашу среднюю

939
00:40:30,720 --> 00:40:32,880
вероятность неглогов таким образом, чтобы минус

940
00:40:32,880 --> 00:40:35,440
один на t

941
00:40:36,640 --> 00:40:39,839
суммы t равнялся единице к большому t, который был

942
00:40:39,839 --> 00:40:42,079
длиной нашего текста, а затем мы

943
00:40:42,079 --> 00:40:44,240
просматривали слова в каждом контексте, поэтому

944
00:40:44,240 --> 00:40:45,440
мы делаем

945
00:40:45,440 --> 00:40:46,480
j

946
00:40:46,480 --> 00:40:50,839
между m словами в каждом  сторона um кроме

947
00:40:50,839 --> 00:40:54,160
самого себя um, а затем мы хотели сделать то, что

948
00:40:54,160 --> 00:40:57,040
было в той стороне, где мы были тогда,

949
00:40:57,040 --> 00:40:58,720
мы вычисляли логарифмическую

950
00:40:58,720 --> 00:41:02,319
вероятность контекстного слова

951
00:41:02,319 --> 00:41:04,079
в этой позиции

952
00:41:04,079 --> 00:41:07,680
um, учитывая слово, которое находится в центральной

953
00:41:07,680 --> 00:41:09,040
позиции t,

954
00:41:09,040 --> 00:41:12,800
и затем мы преобразовали это

955
00:41:12,800 --> 00:41:13,760
в

956
00:41:15,359 --> 00:41:17,920
наши словесные векторы, сказав, что

957
00:41:17,920 --> 00:41:19,520
вероятность

958
00:41:20,480 --> 00:41:22,560
o при заданном

959
00:41:22,560 --> 00:41:24,720
c будет выражена как

960
00:41:25,520 --> 00:41:31,000
ум этот мягкий максимум скалярного произведения,

961
00:41:44,079 --> 00:41:47,520
хорошо, и теперь мы хотим

962
00:41:51,200 --> 00:41:55,119
разработать градиент в направлении спуска

963
00:41:55,119 --> 00:41:56,240
для

964
00:41:56,240 --> 00:41:57,920
этого

965
00:41:57,920 --> 00:42:00,720
последнего  gen, и поэтому мы делаем это

966
00:42:00,720 --> 00:42:02,160
так: мы вычисляем частную

967
00:42:02,160 --> 00:42:03,520
производную

968
00:42:04,319 --> 00:42:08,560
этого выражения

969
00:42:08,560 --> 00:42:12,160
по каждому параметру в модели, и

970
00:42:12,160 --> 00:42:15,119
все параметры в модели являются

971
00:42:15,119 --> 00:42:17,760
компонентами, размерностями

972
00:42:17,760 --> 00:42:21,280
векторов слов каждого слова  Итак, у нас есть

973
00:42:21,280 --> 00:42:24,079
векторы центрального слова и

974
00:42:24,079 --> 00:42:26,160
векторы внешних слов,

975
00:42:26,160 --> 00:42:29,760
так что я просто собираюсь сделать

976
00:42:30,800 --> 00:42:33,280
векторы центрального слова,

977
00:42:33,280 --> 00:42:35,280
но при выполнении домашнего задания по будущему домашнему

978
00:42:35,280 --> 00:42:37,040
заданию

979
00:42:37,040 --> 00:42:39,599
2 появятся векторы внешних слов,

980
00:42:39,599 --> 00:42:42,079
и они вроде как  аналогично, так что

981
00:42:42,079 --> 00:42:44,319
мы делаем то, что мы разрабатываем

982
00:42:44,319 --> 00:42:47,200
частную производную

983
00:42:47,200 --> 00:42:49,760
по отношению к нашему центральному вектору слова,

984
00:42:49,760 --> 00:42:51,280
который, как вы знаете, может быть 300-

985
00:42:51,280 --> 00:42:54,640
мерный вектор слова

986
00:42:56,560 --> 00:43:00,319
этой вероятности o дать c um, и

987
00:43:00,319 --> 00:43:02,079
поскольку мы используем логарифмические вероятности

988
00:43:02,079 --> 00:43:05,280
логарифм этой вероятности o при данном

989
00:43:05,280 --> 00:43:06,960
c этого x из

990
00:43:06,960 --> 00:43:09,440
u otvc

991
00:43:09,440 --> 00:43:10,800
по

992
00:43:10,800 --> 00:43:12,480
моему письму будет становиться все хуже и хуже,

993
00:43:12,480 --> 00:43:15,119
извините, я уже сделал ошибку,

994
00:43:15,119 --> 00:43:16,560
разве я не

995
00:43:16,560 --> 00:43:19,359
сумел сумма w равна единице

996
00:43:19,359 --> 00:43:21,800
словарю  эксперт

997
00:43:21,800 --> 00:43:23,440
uwt

998
00:43:23,440 --> 00:43:25,119
vc

999
00:43:25,119 --> 00:43:26,800
хорошо,

1000
00:43:26,800 --> 00:43:29,680
ну в этот момент все начинается

1001
00:43:29,680 --> 00:43:33,280
довольно легко, так что у нас есть

1002
00:43:33,280 --> 00:43:36,240
что-то, что логарифмически превышает b, так

1003
00:43:36,240 --> 00:43:39,280
что это легко, мы можем превратить это в журнал a

1004
00:43:39,280 --> 00:43:41,119
минус log b,

1005
00:43:41,119 --> 00:43:43,040
но прежде чем я пойду дальше, я просто  Сделайте

1006
00:43:43,040 --> 00:43:45,680
комментарий в этом месте,

1007
00:43:45,680 --> 00:43:49,680
вы знаете, что в этом письме  Моя аудитория делится на две части.

1008
00:43:51,839 --> 00:43:53,680
В аудитории есть несколько человек,

1009
00:43:53,680 --> 00:43:56,400
для которых, возможно, много

1010
00:43:56,400 --> 00:43:57,640
людей

1011
00:43:57,640 --> 00:43:58,880
[Музыка]

1012
00:43:58,880 --> 00:44:01,280
ах, это действительно элементарная математика,

1013
00:44:01,280 --> 00:44:03,280
я видел это миллион раз раньше,

1014
00:44:03,280 --> 00:44:04,880
и он даже не объясняет этого  очень

1015
00:44:04,880 --> 00:44:06,560
хорошо,

1016
00:44:06,560 --> 00:44:08,960
и если вы в этой группе, не

1017
00:44:08,960 --> 00:44:11,280
стесняйтесь смотреть свою электронную почту или

1018
00:44:11,280 --> 00:44:13,680
газету, или что-то еще, что лучше

1019
00:44:13,680 --> 00:44:16,640
всего подходит для вас, но я думаю, что

1020
00:44:16,640 --> 00:44:19,839
в классе также есть другие люди, которых

1021
00:44:19,839 --> 00:44:21,839
я видел в последний раз  исчисление было, когда я учился

1022
00:44:21,839 --> 00:44:23,359
в старшей школе,

1023
00:44:23,359 --> 00:44:25,680
для чего это не так, и поэтому я

1024
00:44:25,680 --> 00:44:28,240
хотел потратить несколько минут на

1025
00:44:30,960 --> 00:44:33,440
то, чтобы разобраться с этим немного конкретно, чтобы попытаться преодолеть идею,

1026
00:44:33,440 --> 00:44:35,599
которую вы знаете, хотя

1027
00:44:36,880 --> 00:44:38,960
большая часть глубокого обучения и даже  словесное

1028
00:44:38,960 --> 00:44:42,000
векторное обучение кажется магией,

1029
00:44:42,000 --> 00:44:45,839
что на самом деле это не волшебство, на самом деле

1030
00:44:45,839 --> 00:44:48,480
это просто математика, и одна из вещей, на которую мы

1031
00:44:48,480 --> 00:44:50,640
надеемся, это то, что вы действительно понимаете

1032
00:44:50,640 --> 00:44:53,119
эту математику, которая выполняется,

1033
00:44:53,119 --> 00:44:55,200
поэтому я буду продолжать и делать немного больше,

1034
00:44:55,200 --> 00:44:58,880
хорошо, так что  то у нас

1035
00:44:58,880 --> 00:45:01,520
есть как бы использовать этот w  ay

1036
00:45:01,520 --> 00:45:03,119
записи журнала,

1037
00:45:03,119 --> 00:45:05,359
и тогда мы можем сказать, что это

1038
00:45:05,359 --> 00:45:08,160
выражение выше равняется частным

1039
00:45:08,160 --> 00:45:10,960
производным с

1040
00:45:10,960 --> 00:45:12,400
of vc

1041
00:45:14,640 --> 00:45:17,119
журнала числителя

1042
00:45:17,119 --> 00:45:21,040
log xuotvc

1043
00:45:21,040 --> 00:45:22,480
минус

1044
00:45:24,160 --> 00:45:27,440
um частной

1045
00:45:28,880 --> 00:45:30,800
производной журнала

1046
00:45:32,599 --> 00:45:36,319
знаменателя, так что тогда

1047
00:45:36,319 --> 00:45:38,880
сумма w равна  От 1 до v

1048
00:45:38,880 --> 00:45:40,240
от

1049
00:45:40,240 --> 00:45:41,440
x uwtvc

1050
00:45:45,440 --> 00:45:48,800
хорошо, так что в этот момент у меня есть

1051
00:45:51,119 --> 00:45:54,480
мой числитель здесь и мой бывший

1052
00:45:54,480 --> 00:45:56,319
знаменатель там,

1053
00:45:56,319 --> 00:45:58,800
так что в этот момент

1054
00:45:58,800 --> 00:46:02,160
есть начало, первая часть - это

1055
00:46:02,160 --> 00:46:05,359
часть числителя, поэтому часть числителя

1056
00:46:05,359 --> 00:46:06,960
действительно очень проста,

1057
00:46:07,680 --> 00:46:10,800
поэтому мы  здесь

1058
00:46:10,800 --> 00:46:13,280
есть журнал и x, но просто инвертируют друг

1059
00:46:13,280 --> 00:46:16,720
друга, поэтому они просто уходят, так что это

1060
00:46:16,720 --> 00:46:17,839
становится

1061
00:46:21,280 --> 00:46:23,599
производной по

1062
00:46:23,599 --> 00:46:25,839
vc

1063
00:46:25,839 --> 00:46:27,359
от

1064
00:46:27,359 --> 00:46:30,079
того, что осталось позади, это

1065
00:46:30,079 --> 00:46:33,680
точечный продукт u0 и с vc,

1066
00:46:33,680 --> 00:46:37,280
хорошо, и поэтому то, о чем нужно знать

1067
00:46:37,280 --> 00:46:38,880
Вы знаете, что мы все еще делаем это

1068
00:46:38,880 --> 00:46:42,160
многомерное исчисление, поэтому то, что у нас есть,

1069
00:46:42,160 --> 00:46:44,640
это исчисление относительно

1070
00:46:44,640 --> 00:46:47,280
вектора, как, надеюсь, вы видели кое-что из

1071
00:46:47,280 --> 00:46:50,800
математики 51 или в другом месте, а не в средней

1072
00:46:50,800 --> 00:46:54,640
школе эм вычисление одной переменной  мы,

1073
00:46:54,640 --> 00:46:58,319
с другой стороны, вы знаете, насколько

1074
00:46:58,319 --> 00:47:01,040
вы и наполовину помните некоторые из этих вещей,

1075
00:47:01,040 --> 00:47:03,359
большую часть времени вы можете просто

1076
00:47:03,359 --> 00:47:05,680
отлично справляться, думая о том, что

1077
00:47:05,680 --> 00:47:08,640
происходит с одним

1078
00:47:08,640 --> 00:47:11,520
измерением за раз, и это

1079
00:47:11,520 --> 00:47:14,480
обобщает многомерное исчисление, поэтому, если  о

1080
00:47:14,480 --> 00:47:17,040
гм все, что вы помните об исчислении, это

1081
00:47:17,040 --> 00:47:21,760
то, что d dx of ax равно a, на

1082
00:47:21,760 --> 00:47:22,960
самом деле

1083
00:47:22,960 --> 00:47:24,640
это то же самое, что мы

1084
00:47:24,640 --> 00:47:29,599
собираемся использовать здесь, здесь у нас

1085
00:47:33,200 --> 00:47:35,440
есть внешнее слово

1086
00:47:35,440 --> 00:47:37,040
dot, созданное

1087
00:47:37,040 --> 00:47:40,160
с помощью vc well в конце дня

1088
00:47:40,160 --> 00:47:44,319
в нем будут термины типа компонента u0,

1089
00:47:44,319 --> 00:47:47,280
умноженного

1090
00:47:47,280 --> 00:47:50,960
на компонент 1 центрального слова, плюс

1091
00:47:50,960 --> 00:47:54,559
u um нулевой компонент два

1092
00:47:54,559 --> 00:47:55,680
плюс

1093
00:47:55,680 --> 00:47:58,000
um,

1094
00:47:58,000 --> 00:48:01,359
это был компонент два, и поэтому мы

1095
00:48:01,359 --> 00:48:04,000
вроде как используем этот бит здесь, и так

1096
00:48:04,000 --> 00:48:05,520
что мы будем  выход -

1097
00:48:05,520 --> 00:48:07,680
это u0,

1098
00:48:07,680 --> 00:48:11,200
u01 и u0 2,

1099
00:48:11,200 --> 00:48:13,760
так что это будет все, что останется по

1100
00:48:13,760 --> 00:48:16,160
отношению к vc1, когда мы возьмем его

1101
00:48:16,160 --> 00:48:18,720
производную по vc1, и этот

1102
00:48:18,720 --> 00:48:20,800
член будет единственным, что останется, когда мы

1103
00:48:20,800 --> 00:48:23,040
возьмем производную по

1104
00:48:23,040 --> 00:48:25,520
переменная um vc2,

1105
00:48:25,520 --> 00:48:29,440
так что конец re  В результате

1106
00:48:29,440 --> 00:48:34,319
взятия векторной производной от

1107
00:48:34,319 --> 00:48:37,520
скалярного произведения u0 и с vc просто

1108
00:48:37,520 --> 00:48:40,559
будет u0,

1109
00:48:40,559 --> 00:48:42,400
хорошо, отлично,

1110
00:48:42,400 --> 00:48:44,880
так что прогресс,

1111
00:48:44,880 --> 00:48:47,760
так что в этот момент

1112
00:48:47,760 --> 00:48:51,599
мы продолжаем и говорим, черт возьми, у нас все еще

1113
00:48:55,359 --> 00:48:57,760
есть знаменатель,

1114
00:48:57,760 --> 00:49:01,359
и это немного сложнее, но

1115
00:49:01,359 --> 00:49:03,440
не так уж и плохо, поэтому мы хотим взять

1116
00:49:03,440 --> 00:49:06,240
частные производные по vc

1117
00:49:06,240 --> 00:49:11,400
из журнала знаменателя,

1118
00:49:18,640 --> 00:49:22,880
хорошо, и тогда на этом этапе

1119
00:49:22,880 --> 00:49:25,119
единственный инструмент, который нам нужно знать и

1120
00:49:25,119 --> 00:49:29,599
помнить, - это как использовать правило цепочки,

1121
00:49:29,599 --> 00:49:31,760
чтобы цепочка  Правило - это когда вы хотите

1122
00:49:35,119 --> 00:49:38,480
разработать производные от композиций

1123
00:49:38,480 --> 00:49:41,920
функций, чтобы у нас было f of g

1124
00:49:43,680 --> 00:49:46,480
любого x, но здесь это будет vc,

1125
00:49:46,480 --> 00:49:50,079
и поэтому мы хотим сказать, хорошо, что у

1126
00:49:50,079 --> 00:49:52,559
нас есть, мы работаем  из

1127
00:49:52,559 --> 00:49:56,160
композиции функций, так что вот наш f

1128
00:49:56,160 --> 00:49:58,800
um, а вот

1129
00:49:58,800 --> 00:49:59,760
наш

1130
00:49:59,760 --> 00:50:00,800
x,

1131
00:50:00,800 --> 00:50:02,400
который является

1132
00:50:02,400 --> 00:50:04,960
g of vc, на

1133
00:50:04,960 --> 00:50:07,280
самом деле, возможно, мне не следует называть его x

1134
00:50:07,280 --> 00:50:09,040
um

1135
00:50:09,040 --> 00:50:11,359
oops,

1136
00:50:11,440 --> 00:50:12,800
может быть, я

1137
00:50:12,800 --> 00:50:14,319
был, вероятно, лучше называть его z или

1138
00:50:14,319 --> 00:50:17,040
что-то в этом роде, ну

1139
00:50:17,040 --> 00:50:19,599
ладно, поэтому, когда мы тогда

1140
00:50:19,599 --> 00:50:20,640
захотим  хорошо

1141
00:50:20,640 --> 00:50:22,640
отработать

1142
00:50:22,640 --> 00:50:25,839
правило цепочки, что мы делаем

1143
00:50:25,839 --> 00:50:29,200
мы берем производную f

1144
00:50:29,200 --> 00:50:31,599
в точке z,

1145
00:50:31,599 --> 00:50:33,920
и поэтому в этой точке мы должны фактически

1146
00:50:33,920 --> 00:50:35,440
вспомнить то, что мы должны помнить,

1147
00:50:35,440 --> 00:50:38,160
что производная log - это одна из

1148
00:50:38,160 --> 00:50:41,119
функции x, поэтому она будет равна

1149
00:50:43,599 --> 00:50:46,079
1 на x

1150
00:50:46,079 --> 00:50:47,440
для

1151
00:50:47,440 --> 00:50:50,319
z  так что тогда будет 1 по

1152
00:50:50,319 --> 00:50:53,520
сумме w, равной от 1 до v

1153
00:50:53,520 --> 00:50:54,960
из x

1154
00:50:54,960 --> 00:50:56,240
от

1155
00:50:56,240 --> 00:50:57,200
utvc,

1156
00:50:59,599 --> 00:51:02,240
умноженного на

1157
00:51:04,640 --> 00:51:07,520
производную внутренней функции,

1158
00:51:07,520 --> 00:51:10,319
так что производная

1159
00:51:12,000 --> 00:51:13,760
um - часть,

1160
00:51:13,760 --> 00:51:15,760
которая осталась,

1161
00:51:15,760 --> 00:51:18,480
я надеюсь, что я правильно понял сумму

1162
00:51:19,440 --> 00:51:21,440
о, и здесь есть один трюк, в этот

1163
00:51:21,440 --> 00:51:23,680
момент мы действительно хотим изменить

1164
00:51:23,680 --> 00:51:26,000
индекс, поэтому мы хотим сказать, что сумма x

1165
00:51:26,000 --> 00:51:27,599
равна 1 к v

1166
00:51:27,599 --> 00:51:29,040
из x

1167
00:51:29,040 --> 00:51:31,440
из u из xvc,

1168
00:51:32,880 --> 00:51:34,079
так как мы

1169
00:51:34,079 --> 00:51:37,520
можем столкнуться с проблемами, если не

1170
00:51:37,520 --> 00:51:40,480
изменим это  переменная um, чтобы использовать

1171
00:51:40,480 --> 00:51:43,119
другую,

1172
00:51:43,119 --> 00:51:46,400
хорошо, поэтому на этом этапе мы добиваемся некоторого

1173
00:51:46,400 --> 00:51:47,760
прогресса,

1174
00:51:47,760 --> 00:51:49,200
но мы все еще хотим выработать

1175
00:51:49,200 --> 00:51:51,680
производную от этой переменной, и поэтому мы

1176
00:51:51,680 --> 00:51:54,720
хотим еще раз применить правило цепочки,

1177
00:51:54,720 --> 00:51:55,760
так что теперь

1178
00:51:55,760 --> 00:52:00,000
вот наши f и  здесь наш новый z

1179
00:52:00,000 --> 00:52:03,359
равен g из vc,

1180
00:52:03,839 --> 00:52:06,559
поэтому

1181
00:52:06,559 --> 00:52:10,240
мы как бы повторяем, так что c

1182
00:52:10,240 --> 00:52:12,960
Переместите производную um

1183
00:52:12,960 --> 00:52:15,839
внутри uh some всегда,

1184
00:52:15,839 --> 00:52:20,640
поэтому мы берем производную

1185
00:52:22,960 --> 00:52:25,599
от этого,

1186
00:52:26,400 --> 00:52:27,440
и

1187
00:52:28,800 --> 00:52:31,200
тогда производная

1188
00:52:31,200 --> 00:52:32,559
x сама по себе в

1189
00:52:32,559 --> 00:52:35,359
порядке, поэтому мы собираемся просто иметь x, равное uxtvc,

1190
00:52:37,599 --> 00:52:39,040
умноженное на

1191
00:52:39,040 --> 00:52:40,480
um,

1192
00:52:40,480 --> 00:52:43,119
там сумма x равна 1, чтобы  v

1193
00:52:43,119 --> 00:52:44,800
умножить

1194
00:52:44,800 --> 00:52:47,839
на производную

1195
00:52:48,160 --> 00:52:50,240
um от uxtvc,

1196
00:52:52,319 --> 00:52:54,640
хорошо, и тогда это то, что мы

1197
00:52:54,640 --> 00:52:57,680
разработали, прежде чем

1198
00:52:58,960 --> 00:53:02,000
мы сможем просто переписать как ux,

1199
00:53:02,000 --> 00:53:02,880
хорошо,

1200
00:53:02,880 --> 00:53:05,200
теперь мы делаем успехи,

1201
00:53:06,240 --> 00:53:07,040
так что

1202
00:53:07,040 --> 00:53:10,800
если мы начнем собирать все это вместе,

1203
00:53:10,800 --> 00:53:13,040
то, что у нас есть,

1204
00:53:13,040 --> 00:53:15,839
производная

1205
00:53:15,839 --> 00:53:19,040
или частные производные с vc

1206
00:53:19,040 --> 00:53:22,319
этой логарифмической вероятности,

1207
00:53:22,960 --> 00:53:24,880
верно, у нас есть числитель, который был

1208
00:53:24,880 --> 00:53:26,640
просто u0

1209
00:53:26,640 --> 00:53:28,559
um минус

1210
00:53:28,559 --> 00:53:32,240
um, тогда у нас была сумма

1211
00:53:32,240 --> 00:53:35,200
суммы числителя по x, равной от

1212
00:53:35,200 --> 00:53:36,640
1 до v

1213
00:53:36,640 --> 00:53:38,000
от xuxt

1214
00:53:39,520 --> 00:53:42,720
dc, умноженного на u от x,

1215
00:53:42,720 --> 00:53:43,680
затем,

1216
00:53:43,680 --> 00:53:46,640
которая была умножена на  наш первый член,

1217
00:53:46,640 --> 00:53:48,880
который произошел от члена на x, который дает

1218
00:53:48,880 --> 00:53:53,280
вам сумму w, равную единице к v

1219
00:53:54,960 --> 00:53:58,079
x uwtvc,

1220
00:54:00,079 --> 00:54:01,920
и это тот факт, что мы изменили

1221
00:54:01,920 --> 00:54:05,680
переменные um, стало важным, и поэтому

1222
00:54:05,680 --> 00:54:09,680
просто переписав это немного um

1223
00:54:09,680 --> 00:54:12,240
мы можем получить, что это равно u  0

1224
00:54:12,240 --> 00:54:13,440
минус

1225
00:54:13,440 --> 00:54:14,800
um

1226
00:54:14,800 --> 00:54:19,280
сумма v равна ой, извините,

1227
00:54:23,680 --> 00:54:25,680
x хорошо, x равно

1228
00:54:25,680 --> 00:54:26,880
1 к

1229
00:54:26,880 --> 00:54:27,760
v

1230
00:54:28,640 --> 00:54:32,079
этого x.

1231
00:54:44,559 --> 00:54:47,760
случилось так, что

1232
00:54:47,760 --> 00:54:50,319
мы закончили тем, что получили

1233
00:54:50,319 --> 00:54:53,359
точно ту же вероятность формулы мягкого максимума,

1234
00:54:53,359 --> 00:54:54,799
которую мы видели,

1235
00:54:54,799 --> 00:54:58,000
когда мы начали,

1236
00:54:58,000 --> 00:54:59,839
мы можем просто переписать это для более

1237
00:54:59,839 --> 00:55:04,079
удобного использования, сказав, что это равно u0

1238
00:55:04,079 --> 00:55:08,079
минус сумма по x равна от 1 до

1239
00:55:08,079 --> 00:55:11,839
v вероятности x  учитывая c,

1240
00:55:11,839 --> 00:55:14,079
умноженное на ux,

1241
00:55:14,079 --> 00:55:16,880
и то, что у нас есть в

1242
00:55:16,880 --> 00:55:21,359
данный момент, это ожидание,

1243
00:55:21,359 --> 00:55:25,119
и поэтому это среднее значение по

1244
00:55:25,119 --> 00:55:27,839
всем векторам контекста, взвешенным по их

1245
00:55:27,839 --> 00:55:30,640
вероятности в соответствии с моделью, и

1246
00:55:30,640 --> 00:55:32,559
так всегда бывает

1247
00:55:32,559 --> 00:55:34,559
с этими моделями стиля softmax  что

1248
00:55:34,559 --> 00:55:36,720
то, что вы получаете для производных, это то, что

1249
00:55:36,720 --> 00:55:40,079
вы получаете наблюдаемое um

1250
00:55:41,040 --> 00:55:45,680
минус ожидаемое, поэтому наша модель хороша,

1251
00:55:45,680 --> 00:55:49,440
если наша модель в среднем предсказывает точно

1252
00:55:49,440 --> 00:55:53,440
um вектор слов, который мы на самом деле видим,

1253
00:55:53,440 --> 00:55:55,599
и поэтому мы собираемся попытаться скорректировать

1254
00:55:55,599 --> 00:55:57,520
номинал  параметры нашей модели,

1255
00:55:57,520 --> 00:55:58,559
так что

1256
00:55:58,559 --> 00:56:01,280
она делает именно это,

1257
00:56:01,280 --> 00:56:04,000
теперь я имею в виду, что

1258
00:56:04,000 --> 00:56:06,000
мы пытаемся сделать это как

1259
00:56:06,000 --> 00:56:08,960
можно больше, я имею в виду, конечно, поскольку вы обнаружите,

1260
00:56:08,960 --> 00:56:11,680
что никогда не сможете приблизиться, вы знаете,

1261
00:56:11,680 --> 00:56:14,000
если я просто скажу вам, хорошо

1262
00:56:14,000 --> 00:56:16,720
слово круассан, какие слова могут

1263
00:56:16,720 --> 00:56:19,040
встречаться в контексте

1264
00:56:19,040 --> 00:56:21,359
круассана, я имею в виду, что вы не можете ответить, что

1265
00:56:21,359 --> 00:56:22,960
есть всевозможные предложения, которые

1266
00:56:22,960 --> 00:56:24,799
вы могли бы сказать, которые включают слово

1267
00:56:24,799 --> 00:56:27,440
круассан, поэтому на самом деле наши конкретные

1268
00:56:27,440 --> 00:56:29,760
оценки вероятности будут

1269
00:56:29,760 --> 00:56:32,160
добрыми  маленьких, но,

1270
00:56:32,160 --> 00:56:34,880
тем не менее, мы хотим поиграть с

1271
00:56:34,880 --> 00:56:38,319
нашими векторами слов, чтобы попытаться сделать эти

1272
00:56:38,319 --> 00:56:41,520
оценки настолько высокими, насколько это возможно,

1273
00:56:41,520 --> 00:56:42,880
поэтому

1274
00:56:42,880 --> 00:56:45,920
я немного рассказал об этом,

1275
00:56:46,960 --> 00:56:49,119
но на самом деле не

1276
00:56:49,119 --> 00:56:51,839
показал вам ничего из того, что на самом деле

1277
00:56:51,839 --> 00:56:55,760
Случилось извините, я просто хочу быстро показать

1278
00:56:56,799 --> 00:56:58,559
вам,

1279
00:56:58,559 --> 00:57:00,640
что на самом деле происходит с векторами слов.

1280
00:57:10,000 --> 00:57:13,680
импортировать кучу вещей

1281
00:57:13,680 --> 00:57:16,880
гм, так что у нас есть numpy для наших векторов,

1282
00:57:16,880 --> 00:57:18,799
matplotlib

1283
00:57:18,799 --> 00:57:21,839
строит график, он изучает вид вашего

1284
00:57:21,839 --> 00:57:24,720
машинного обучения um swiss army knife

1285
00:57:24,720 --> 00:57:26,559
gensim - это пакет, который вы, возможно,

1286
00:57:26,559 --> 00:57:28,799
не видели, прежде чем это пакет,

1287
00:57:28,799 --> 00:57:30,880
который часто используется для векторов слов, для которых он на

1288
00:57:30,880 --> 00:57:32,559
самом деле не

1289
00:57:32,559 --> 00:57:34,400
используется  глубокое обучение, так что это

1290
00:57:34,400 --> 00:57:36,319
единственный раз, когда вы увидите его в классе, но

1291
00:57:36,319 --> 00:57:37,680
если вам просто нужен хороший пакет для

1292
00:57:37,680 --> 00:57:40,000
работы с векторами слов и какое-то другое

1293
00:57:40,000 --> 00:57:42,000
приложение,

1294
00:57:42,000 --> 00:57:43,760
хорошо бы знать о

1295
00:57:43,760 --> 00:57:47,760
порядке, поэтому во второй ячейке здесь

1296
00:57:47,760 --> 00:57:49,599
я  я

1297
00:57:49,599 --> 00:57:51,520
загружаю определенный набор векторов слов, так

1298
00:57:51,520 --> 00:57:54,240
что это наши векторы слов в перчатках, которые мы

1299
00:57:54,240 --> 00:57:57,440
сделали в Стэнфорде в 2014 году,

1300
00:57:57,440 --> 00:57:59,359
и я загружаю сотни размерных

1301
00:57:59,359 --> 00:58:02,000
векторов слов, ммм, чтобы все было

1302
00:58:02,000 --> 00:58:04,720
немного быстрее для меня, пока я

1303
00:58:04,720 --> 00:58:07,599
что-то делаю  вот вроде как сделать эту

1304
00:58:07,599 --> 00:58:11,119
модель хлеба и круассана, ну,

1305
00:58:11,119 --> 00:58:14,240
что я только что получил, это векторы слов,

1306
00:58:14,240 --> 00:58:16,960
так что я просто хотел как бы

1307
00:58:16,960 --> 00:58:21,680
показать вам, что

1308
00:58:27,520 --> 00:58:29,040
есть векторы слов

1309
00:58:29,040 --> 00:58:32,559
заранее хм

1310
00:58:32,960 --> 00:58:36,280
давай

1311
00:58:42,480 --> 00:58:46,000
хорошо, хорошо, я в деле,

1312
00:58:46,000 --> 00:58:49,040
хорошо, так хорошо, вот мои

1313
00:58:49,040 --> 00:58:53,599
векторы слов для хлеба и круассана,

1314
00:58:53,599 --> 00:58:55,280
и пока и вижу, что, возможно, эти

1315
00:58:55,280 --> 00:58:57,359
два слова немного похожи, поэтому оба

1316
00:58:57,359 --> 00:58:59,359
они отрицательны в первом измерении

1317
00:58:59,359 --> 00:59:00,960
положительно, а  второй отрицательный в

1318
00:59:00,960 --> 00:59:02,240
третьем

1319
00:59:02,240 --> 00:59:04,240
положительном, четвертый отрицательный и

1320
00:59:04,240 --> 00:59:06,160
пятый, так что похоже, что у них

1321
00:59:06,160 --> 00:59:07,760
может быть довольно много точечного произведения,

1322
00:59:07,760 --> 00:59:09,280
что отчасти то, что мы хотим, потому что

1323
00:59:09,280 --> 00:59:11,760
хлеб и круассан отчасти похожи,

1324
00:59:11,760 --> 00:59:14,319
но что мы можем сделать, так это  на самом деле

1325
00:59:14,319 --> 00:59:16,079
спросите модель, и это функции gen sim,

1326
00:59:16,079 --> 00:59:19,599
теперь вы знаете, какие слова наиболее

1327
00:59:19,599 --> 00:59:22,720
похожи, поэтому я могу спросить круассан,

1328
00:59:22,720 --> 00:59:24,160
какие

1329
00:59:24,160 --> 00:59:26,799
слова наиболее похожи на это, и

1330
00:59:26,799 --> 00:59:28,799
он скажет мне, что это такие вещи, как бриошь,

1331
00:59:28,799 --> 00:59:31,839
багет фокачча, так что это довольно хороший

1332
00:59:31,839 --> 00:59:33,359
пудинг  Возможно, немного более

1333
00:59:33,359 --> 00:59:34,640
сомнительно,

1334
00:59:34,640 --> 00:59:36,480
мы можем сказать, что

1335
00:59:36,480 --> 00:59:39,520
больше всего похож на США, и в нем говорится, что

1336
00:59:39,520 --> 00:59:42,640
Канада Америка США с периодами Соединенные

1337
00:59:42,640 --> 00:59:44,799
Штаты это довольно хорошо

1338
00:59:44,799 --> 00:59:46,799
больше всего похоже на банан

1339
00:59:46,799 --> 00:59:51,200
эм я достаю кокосы манго бананы

1340
00:59:51,200 --> 00:59:53,119
сорт  довольно тропический,

1341
00:59:53,119 --> 00:59:55,359
очень великий ум,

1342
00:59:55,359 --> 00:59:57,760
прежде чем закончить, хотя я хочу

1343
00:59:57,760 --> 01:00:00,000
показать вам кое-что немного большее, чем

1344
01:00:00,000 --> 01:00:01,920
просто сходство, которое является одной из

1345
01:00:01,920 --> 01:00:04,480
удивительных вещей, которые люди наблюдали с

1346
01:00:04,480 --> 01:00:07,440
этими векторами слов, и это означает, что

1347
01:00:07,440 --> 01:00:10,160
вы действительно можете выполнять арифметические операции

1348
01:00:10,160 --> 01:00:13,040
с этим вектором  пространство, которое имеет смысл,

1349
01:00:13,040 --> 01:00:15,040
и поэтому, в частности, люди предложили

1350
01:00:15,040 --> 01:00:18,000
эту задачу по аналогии, и поэтому идея задачи по

1351
01:00:18,000 --> 01:00:20,319
аналогии заключается в том, что вы должны уметь

1352
01:00:20,319 --> 01:00:22,880
начинать со слова, например, король, и у вас

1353
01:00:22,880 --> 01:00:24,960
должна быть возможность вычесть мужской

1354
01:00:24,960 --> 01:00:28,240
компонент из него и добавить обратно в  женский

1355
01:00:28,240 --> 01:00:30,480
компонент, и тогда вы сможете

1356
01:00:30,480 --> 01:00:34,000
хорошо спросить, какое слово здесь, и что

1357
01:00:34,000 --> 01:00:37,839
вы хотите, чтобы слово там

1358
01:00:37,839 --> 01:00:39,280
было королева

1359
01:00:48,720 --> 01:00:51,680
ммм  такая же самая похожая

1360
01:00:51,680 --> 01:00:54,880
функция, которая на самом деле больше, а

1361
01:00:54,880 --> 01:00:57,520
также имея положительные слова, вы можете

1362
01:00:57,520 --> 01:01:00,160
попросить самые похожие отрицательные слова, и

1363
01:01:00,160 --> 01:01:02,559
вы можете задаться вопросом, что наиболее отрицательно

1364
01:01:02,559 --> 01:01:04,480
похоже на банан, и вы можете

1365
01:01:04,480 --> 01:01:07,440
подумать о  это эм я не знаю

1366
01:01:07,440 --> 01:01:10,240
эм какое-то мясо или что-то на

1367
01:01:10,240 --> 01:01:12,160
самом деле, что само по себе не очень

1368
01:01:12,160 --> 01:01:13,760
полезно, потому что, когда вы можете просто попросить

1369
01:01:15,040 --> 01:01:17,200
наиболее негативно похожие вещи, вы, как правило,

1370
01:01:17,200 --> 01:01:18,000
получаете

1371
01:01:18,000 --> 01:01:19,680
сумасшедшие строки, которые были найдены в

1372
01:01:19,680 --> 01:01:20,880
наборе данных,

1373
01:01:20,880 --> 01:01:22,319
который вы используете  не знаю, что они означают,

1374
01:01:23,839 --> 01:01:26,480
но если мы сложим эти два вместе, мы сможем

1375
01:01:26,480 --> 01:01:28,240
использовать наиболее похожую функцию с

1376
01:01:28,240 --> 01:01:31,040
положительными и отрицательными значениями для проведения аналогий,

1377
01:01:31,040 --> 01:01:34,319
поэтому мы собираемся сказать, что хотим положительного

1378
01:01:34,319 --> 01:01:37,040
короля, мы хотим вычесть отрицательно

1379
01:01:37,040 --> 01:01:40,319
человека, которого мы хотим  затем добавить положительно

1380
01:01:40,319 --> 01:01:42,880
женщину и выяснить, что больше всего похоже

1381
01:01:42,880 --> 01:01:46,000
на эту точку в пространстве, так что моя

1382
01:01:46,000 --> 01:01:47,520
функция аналогии делает

1383
01:01:47,520 --> 01:01:50,079
именно это,

1384
01:01:50,079 --> 01:01:53,280
взяв пару наиболее похожих, а затем

1385
01:01:55,520 --> 01:01:57,920
вычтя отрицательную, и поэтому мы можем попробовать

1386
01:01:57,920 --> 01:02:00,640
эту аналогию  функция, так что я могу провести

1387
01:02:00,640 --> 01:02:03,520
аналогию, которую я показываю на картинке

1388
01:02:03,520 --> 01:02:04,400
с

1389
01:02:04,400 --> 01:02:07,599
мужчиной относительно короля, поскольку женщина

1390
01:02:07,599 --> 01:02:08,640
сражается,

1391
01:02:08,640 --> 01:02:11,039
извините, я не говорю это правильно, да,

1392
01:02:11,039 --> 01:02:14,480
мужчина - король, поскольку женщина слишком о,

1393
01:02:14,480 --> 01:02:16,880
извините, я не сделал свои клетки

1394
01:02:21,599 --> 01:02:23,920
человек король как  женщина как

1395
01:02:23,920 --> 01:02:28,319
королева, так что это здорово, и это

1396
01:02:28,319 --> 01:02:29,680
хорошо работает,

1397
01:02:29,680 --> 01:02:31,440
я имею в виду, и вы можете сделать это вроде

1398
01:02:31,440 --> 01:02:33,920
как король по-другому - мужчина как королева,

1399
01:02:33,920 --> 01:02:37,359
как женщина, если бы это сработало только для

1400
01:02:37,359 --> 01:02:40,880
этого причудливого примера, вы, может быть, не

1401
01:02:40,880 --> 01:02:43,520
могли бы  Не очень впечатлен, но вы

1402
01:02:43,520 --> 01:02:45,359
знаете, что на самом деле это не

1403
01:02:45,359 --> 01:02:47,520
идеально, но с этим можно проводить всевозможные забавные

1404
01:02:47,520 --> 01:02:50,000
аналогии, и они на самом деле

1405
01:02:50,000 --> 01:02:52,319
работают, так что вы знаете, я мог бы попросить

1406
01:02:52,319 --> 01:02:54,000
что-то

1407
01:02:54,000 --> 01:02:56,160
вроде аналогии,

1408
01:02:57,760 --> 01:02:59,680
о, вот хорошая

1409
01:03:02,559 --> 01:03:04,400
австралия  быть э-э,

1410
01:03:04,400 --> 01:03:05,520
как

1411
01:03:05,520 --> 01:03:09,119
франция к чему,

1412
01:03:09,119 --> 01:03:10,400
и вы можете подумать о том, что вы думаете,

1413
01:03:10,400 --> 01:03:12,240
что должно быть,

1414
01:03:12,240 --> 01:03:14,799
и получится

1415
01:03:14,799 --> 01:03:17,359
шампанское, что довольно хорошо, или я

1416
01:03:17,359 --> 01:03:20,880
мог бы попросить что-то вроде

1417
01:03:22,319 --> 01:03:23,839
карандаша по аналогии с

1418
01:03:23,839 --> 01:03:25,839
зарисовками,

1419
01:03:25,839 --> 01:03:28,160
как

1420
01:03:28,160 --> 01:03:30,559
камера для

1421
01:03:30,559 --> 01:03:31,440
того, что

1422
01:03:31,440 --> 01:03:33,760
эм  и он говорит, что фотографируя

1423
01:03:33,760 --> 01:03:36,079
ммм, вы также можете проводить аналогии с

1424
01:03:36,079 --> 01:03:39,280
людьми, на этом этапе я должен указать,

1425
01:03:39,280 --> 01:03:42,160
что эти данные были ммм, а модель

1426
01:03:42,160 --> 01:03:44,400
была построена в 2014 году,

1427
01:03:44,400 --> 01:03:47,119
поэтому вы не можете ничего спросить о

1428
01:03:47,119 --> 01:03:49,280
Дональде Трампе в этом хорошо, вы можете

1429
01:03:49,280 --> 01:03:51,200
Трамп там, но  не как президент, но я

1430
01:03:51,200 --> 01:03:54,720
мог бы спросить что-то вроде аналогии

1431
01:03:54,720 --> 01:03:56,839
Обамы с

1432
01:03:56,839 --> 01:04:01,520
Клинтоном, как Рейган

1433
01:04:04,960 --> 01:04:07,359
с чем, и вы можете

1434
01:04:07,359 --> 01:04:09,520
думать о том, что вы думаете, это правильная

1435
01:04:09,520 --> 01:04:11,440
аналогия

1436
01:04:11,440 --> 01:04:14,240
там аналогия, которую она возвращает, это Никсон,

1437
01:04:14,240 --> 01:04:15,599
так что я предполагаю, что это зависит от того, что вы

1438
01:04:15,599 --> 01:04:17,440
думаете о Билле Клинтоне  Что касается того, считаете ли вы,

1439
01:04:17,440 --> 01:04:19,680
что это хорошая аналогия или нет, вы

1440
01:04:19,680 --> 01:04:21,440
также

1441
01:04:21,440 --> 01:04:24,160
можете провести с ней какие-то лингвистические аналогии,

1442
01:04:24,160 --> 01:04:27,280
чтобы вы могли сделать что-то вроде аналогии:

1443
01:04:27,280 --> 01:04:28,559
высокий

1444
01:04:28,559 --> 01:04:31,039
- самый высокий,

1445
01:04:31,039 --> 01:04:32,319
до

1446
01:04:32,319 --> 01:04:33,599
тех пор,

1447
01:04:33,599 --> 01:04:36,400
пока он не является чем, и он действует дольше всего, поэтому он

1448
01:04:36,400 --> 01:04:37,839
действительно просто

1449
01:04:37,839 --> 01:04:40,000
знает  много о смысловом

1450
01:04:40,000 --> 01:04:43,520
поведении слов, и вы знаете, я думаю, когда эти

1451
01:04:44,720 --> 01:04:46,480
методы были впервые разработаны и,

1452
01:04:46,480 --> 01:04:48,799
надеюсь, для вас, вы знаете, что

1453
01:04:48,799 --> 01:04:51,359
люди были просто потрясены тем, насколько

1454
01:04:51,359 --> 01:04:55,520
хорошо это на самом деле работает при

1455
01:04:55,520 --> 01:04:57,839
захвате слов, и поэтому эти векторы слов затем распространились

1456
01:04:57,839 --> 01:05:00,960
повсюду как  новое представление,

1457
01:05:00,960 --> 01:05:03,520
которое было настолько мощным для определения

1458
01:05:03,520 --> 01:05:06,000
значения слов, и это наша отправная

1459
01:05:06,000 --> 01:05:08,400
точка для этого класса, и мы поговорим

1460
01:05:08,400 --> 01:05:10,480
о них немного больше в следующий раз, и они

1461
01:05:10,480 --> 01:05:12,880
также являются основой wh  когда вы смотрите

1462
01:05:12,880 --> 01:05:15,119
на первое задание, могу ли я задать

1463
01:05:15,119 --> 01:05:17,039
быстрый вопрос о различии

1464
01:05:17,039 --> 01:05:19,760
между двумя векторами на слово,

1465
01:05:19,760 --> 01:05:21,760
да,

1466
01:05:21,760 --> 01:05:23,760
я понимаю, что может быть

1467
01:05:23,760 --> 01:05:26,000
несколько контекстных слов на

1468
01:05:26,000 --> 01:05:27,920
одно слово в словаре, например, слово

1469
01:05:27,920 --> 01:05:30,640
в словаре, но  тогда, если есть

1470
01:05:30,640 --> 01:05:32,400
только два вектора, я вроде бы подумал, что

1471
01:05:32,400 --> 01:05:33,920
разница между ними в том, что

1472
01:05:33,920 --> 01:05:35,359
один похож на фактическое слово, а другой -

1473
01:05:35,359 --> 01:05:36,960
на контекстное слово, но несколько

1474
01:05:36,960 --> 01:05:39,440
контекстных слов, например, как вы, как вы

1475
01:05:39,440 --> 01:05:42,720
выбираете только два, тогда хорошо, так что мы '  делаю

1476
01:05:42,720 --> 01:05:45,520
все правильно, так

1477
01:05:45,520 --> 01:05:46,319
что,

1478
01:05:46,319 --> 01:05:48,640
может быть, я не вернусь к демонстрации экрана,

1479
01:05:48,640 --> 01:05:51,280
но вы знаете, что мы делали

1480
01:05:51,280 --> 01:05:53,599
в целевой функции, на вас была

1481
01:05:53,599 --> 01:05:54,559
сумма,

1482
01:05:55,599 --> 01:05:57,280
так что вы правильно знаете этот большой

1483
01:05:57,280 --> 01:05:59,440
корпус текста  Итак, вы берете

1484
01:05:59,440 --> 01:06:02,240
сумму по каждому слову, которое

1485
01:06:02,240 --> 01:06:04,480
появляется как центральное слово, а затем

1486
01:06:04,480 --> 01:06:07,200
внутри этого есть вторая сумма,

1487
01:06:07,200 --> 01:06:08,400
которая предназначена

1488
01:06:08,400 --> 01:06:10,640
для каждого слова в контексте, поэтому вы

1489
01:06:10,640 --> 01:06:13,200
собираетесь считать каждое слово как контекстное

1490
01:06:13,200 --> 01:06:16,400
слово, а затем для  один конкретный тер

1491
01:06:16,400 --> 01:06:18,400
m этой целевой функции у вас есть

1492
01:06:18,400 --> 01:06:21,520
конкретное контекстное слово и определенное

1493
01:06:21,520 --> 01:06:24,400
центральное слово um, но затем вы как бы

1494
01:06:24,400 --> 01:06:27,599
суммируете разные контекстные слова для

1495
01:06:27,599 --> 01:06:29,599
каждого центра слова, а затем

1496
01:06:29,599 --> 01:06:32,799
суммируете все решения

1497
01:06:32,799 --> 01:06:36,319
разные центральные слова и, скажем,

1498
01:06:36,319 --> 01:06:38,240
немного больше

1499
01:06:38,240 --> 01:06:41,280
о двух векторах, я имею в виду, что вы знаете, в

1500
01:06:41,280 --> 01:06:43,839
каком-то смысле это уродливая деталь, но

1501
01:06:43,839 --> 01:06:45,920
это было сделано для того, чтобы все было проще

1502
01:06:45,920 --> 01:06:50,079
и быстрее, чтобы вы знали, если

1503
01:06:50,079 --> 01:06:52,000
посмотрите

1504
01:06:52,000 --> 01:06:54,319
на математику  внимательно, если вы относитесь

1505
01:06:54,319 --> 01:06:56,160
к

1506
01:06:56,160 --> 01:06:58,720
этим двум векторам как к одному и тому же, поэтому, если вы

1507
01:06:58,720 --> 01:07:00,720
используете одни и те же векторы для центра и

1508
01:07:00,720 --> 01:07:02,319
контекста,

1509
01:07:02,319 --> 01:07:04,799
и вы говорите хорошо, давайте разберемся с

1510
01:07:04,799 --> 01:07:06,640
производными,

1511
01:07:06,640 --> 01:07:09,119
все становится уродливее, и причина, по которой

1512
01:07:09,119 --> 01:07:11,680
они становятся уродливее, -

1513
01:07:11,680 --> 01:07:14,319
это нормально, когда я повторяю  по

1514
01:07:14,319 --> 01:07:18,319
всем вариантам контекстного слова, о боже,

1515
01:07:18,319 --> 01:07:20,000
иногда контекстное слово

1516
01:07:20,000 --> 01:07:22,960
будет таким же, как и центральное слово, и

1517
01:07:22,960 --> 01:07:26,079
это мешает разрабатывать мои

1518
01:07:26,079 --> 01:07:27,599
производные, в

1519
01:07:27,599 --> 01:07:30,000
то время как, принимая их как отдельные

1520
01:07:30,000 --> 01:07:33,119
векторы, чего никогда не бывает.  о, это легко,

1521
01:07:33,119 --> 01:07:35,839
но интересно то, что

1522
01:07:35,839 --> 01:07:38,160
вы знаете, когда говорите, что у вас есть эти два

1523
01:07:38,160 --> 01:07:40,319
разных представления,

1524
01:07:40,319 --> 01:07:42,720
вроде как в конечном итоге действительно

1525
01:07:42,720 --> 01:07:46,000
не причиняют вреда, и я машу руками

1526
01:07:46,000 --> 01:07:48,880
аргумент в пользу этого, вы знаете, поскольку

1527
01:07:48,880 --> 01:07:51,520
мы как бы двигаемся  через каждую

1528
01:07:51,520 --> 01:07:54,799
позицию в корпусе одно за другим вы

1529
01:07:54,799 --> 01:07:57,200
кое-что знаете, слово, которое является центральным словом

1530
01:07:57,200 --> 01:07:59,920
в один момент, будет контекстным

1531
01:07:59,920 --> 01:08:02,400
словом в следующий момент, а слово,

1532
01:08:02,400 --> 01:08:04,640
которое было контекстным словом,

1533
01:08:04,640 --> 01:08:06,720
станет центральным словом, поэтому вы  мы как

1534
01:08:06,720 --> 01:08:09,039
бы выполняем

1535
01:08:09,039 --> 01:08:12,960
вычисления в обоих направлениях в каждом случае,

1536
01:08:12,960 --> 01:08:14,799
и поэтому вы должны быть в состоянии убедить

1537
01:08:14,799 --> 01:08:17,359
себя, что два представления

1538
01:08:17,359 --> 01:08:20,158
для слова в конечном итоге очень похожи,

1539
01:08:20,158 --> 01:08:22,560
и они не идентичны по

1540
01:08:22,560 --> 01:08:23,920
техническим причинам в концах

1541
01:08:23,920 --> 01:08:26,799
документов и  такие вещи, но очень

1542
01:08:26,799 --> 01:08:28,960
очень похожие

1543
01:08:28,960 --> 01:08:31,198
и так эффективно вы, как правило, получаете два

1544
01:08:31,198 --> 01:08:33,439
очень похожих представления для каждого

1545
01:08:33,439 --> 01:08:35,600
слова, и мы просто усредняем их и называем

1546
01:08:35,600 --> 01:08:37,920
это вектором слов, и поэтому, когда мы используем

1547
01:08:37,920 --> 01:08:40,399
векторы слов, у нас есть только один  вектор для

1548
01:08:40,399 --> 01:08:41,759
каждого слова,

1549
01:08:41,759 --> 01:08:44,560
которое имеет смысл, спасибо, у

1550
01:08:44,560 --> 01:08:46,799
меня вопрос чисто из любопытства, поэтому

1551
01:08:46,799 --> 01:08:49,279
мы начали с того, что спроецировали

1552
01:08:49,279 --> 01:08:51,439
векторы векторов слов на двумерную

1553
01:08:51,439 --> 01:08:53,679
поверхность, которые мы увидели как небольшие кластеры

1554
01:08:53,679 --> 01:08:55,120
слов, которые похожи друг на друга, а

1555
01:08:55,120 --> 01:08:57,198
затем мы увидели  что гм с

1556
01:08:57,198 --> 01:08:59,359
аналогиями, что мы как бы видим, что

1557
01:08:59,359 --> 01:09:01,198
есть эти направленные векторы, которые

1558
01:09:01,198 --> 01:09:03,439
как бы указывают на правителя

1559
01:09:03,439 --> 01:09:05,439
или генерального директора чего-то в этом роде, и поэтому

1560
01:09:05,439 --> 01:09:07,198
мне интересно, есть ли

1561
01:09:07,198 --> 01:09:09,359
отношения между

1562
01:09:09,359 --> 01:09:12,158
самими этими относительными векторами, например

1563
01:09:12,158 --> 01:09:14,880
правитель вектора вроде как похож

1564
01:09:14,880 --> 01:09:17,839
на генерального директора вектора, который сильно

1565
01:09:17,839 --> 01:09:20,238
отличается от того, что делает хороший

1566
01:09:20,238 --> 01:09:22,960
бутерброд с вектором,

1567
01:09:22,960 --> 01:09:25,920
есть ли какие-либо исследования по

1568
01:09:25,920 --> 01:09:29,759
этому поводу, это хороший вопрос,

1569
01:09:29,759 --> 01:09:31,279
как вы поставите меня в тупик уже на

1570
01:09:31,279 --> 01:09:35,560
первой лекции ах

1571
01:09:37,359 --> 01:09:38,640
я  означает, что

1572
01:09:38,640 --> 01:09:40,479
да, я действительно не могу придумать какое-то

1573
01:09:40,479 --> 01:09:42,479
исследование, поэтому я не уверен, что у меня есть

1574
01:09:42,479 --> 01:09:43,920
уверенность, и я не уверен, что у меня есть

1575
01:09:43,920 --> 01:09:46,238
уверенный ответ, я имею в виду, что кажется,

1576
01:09:46,238 --> 01:09:49,920
что это действительно легко  g, чтобы проверить,

1577
01:09:49,920 --> 01:09:52,158
сколько у вас есть одного из этих наборов

1578
01:09:52,158 --> 01:09:56,000
векторов слов um, которые кажутся вам похожими,

1579
01:09:56,000 --> 01:09:59,520
и для любых отношений, которые

1580
01:09:59,520 --> 01:10:02,000
достаточно хорошо представлены словом, вы

1581
01:10:02,000 --> 01:10:04,000
должны быть в состоянии увидеть, выходит ли оно

1582
01:10:04,000 --> 01:10:05,440
похожим на то, что

1583
01:10:07,760 --> 01:10:10,560
я имею в виду  Я не уверен, что мы сможем посмотреть

1584
01:10:10,560 --> 01:10:11,520
и увидеть

1585
01:10:11,520 --> 01:10:12,960
да, это совершенно нормально, просто

1586
01:10:12,960 --> 01:10:15,960
любопытно,

1587
01:10:17,040 --> 01:10:18,800
извините, я пропустил последнюю часть

1588
01:10:18,800 --> 01:10:21,120
вашего ответа на первый вопрос, поэтому, когда

1589
01:10:21,120 --> 01:10:22,960
вы хотели свернуть два вектора для

1590
01:10:22,960 --> 01:10:24,400
одного и того же слова, вы сказали  Обычно вы

1591
01:10:24,400 --> 01:10:26,640
берете среднее, ммм, разные люди

1592
01:10:26,640 --> 01:10:28,560
делали разные вещи, но наиболее

1593
01:10:28,560 --> 01:10:32,239
распространенная практика - это после того, как вы

1594
01:10:32,239 --> 01:10:34,080
знаете, я должен рассказать еще немного

1595
01:10:34,080 --> 01:10:36,800
о беге словесного погружения, что мы

1596
01:10:36,800 --> 01:10:38,400
действительно не прошли сегодня, так что я

1597
01:10:38,400 --> 01:10:39,840
все еще  В четверг у вас есть немного больше работы,

1598
01:10:39,840 --> 01:10:42,480
но вы знаете, как только вы запустите

1599
01:10:42,480 --> 01:10:44,400
свой алгоритм word to vec,

1600
01:10:44,400 --> 01:10:47,679
и вы сортируете свой вывод, это два

1601
01:10:47,679 --> 01:10:50,640
вектора для каждого слова и вроде как,

1602
01:10:50,640 --> 01:10:52,880
когда он в центре и когда это контекст,

1603
01:10:52,880 --> 01:10:53,679
и

1604
01:10:53,679 --> 01:10:56,080
поэтому обычно люди  просто усредни этих

1605
01:10:56,080 --> 01:10:59,120
двоих  векторов и скажите хорошо, что это

1606
01:10:59,120 --> 01:11:02,239
представление слова круассан, и это

1607
01:11:02,239 --> 01:11:04,800
то, что появляется в виде файла векторов слов,

1608
01:11:04,800 --> 01:11:08,719
подобного тому, который я загрузил,

1609
01:11:08,719 --> 01:11:11,199
что имеет смысл, спасибо,

1610
01:11:11,199 --> 01:11:14,080
спасибо, поэтому мой вопрос в том, может ли слово

1611
01:11:14,080 --> 01:11:16,239
иметь два разных значения или несколько

1612
01:11:16,239 --> 01:11:17,679
разных значений  мы по-прежнему

1613
01:11:17,679 --> 01:11:20,000
представляем его как один и тот же

1614
01:11:20,000 --> 01:11:21,600
единственный вектор,

1615
01:11:21,600 --> 01:11:24,640
да, это очень хороший вопрос, и на

1616
01:11:24,640 --> 01:11:26,320
самом деле

1617
01:11:26,320 --> 01:11:28,800
в лекции в четверг есть определенное содержание, так что я могу сказать больше

1618
01:11:28,800 --> 01:11:32,320
об этом, но да, первая

1619
01:11:32,320 --> 01:11:35,679
реакция - вам вроде как следует бояться,

1620
01:11:35,679 --> 01:11:38,159
потому что что-то  я вообще ничего не сказал,

1621
01:11:38,159 --> 01:11:40,800
вы знаете, что

1622
01:11:40,800 --> 01:11:43,840
большинство слов, особенно короткие общие слова,

1623
01:11:43,840 --> 01:11:46,640
имеют много значения, поэтому, если у вас есть такое

1624
01:11:46,640 --> 01:11:50,159
слово, как звезда, которое может быть астрономическим

1625
01:11:50,159 --> 01:11:52,800
объектом, или это может быть вы знаете

1626
01:11:52,800 --> 01:11:55,199
кинозвезду звезду Голливуда или это может быть  что-то

1627
01:11:55,199 --> 01:11:57,120
вроде золотых звезд, которые у вас есть в

1628
01:11:57,120 --> 01:11:58,719
начальной школе,

1629
01:11:58,719 --> 01:11:59,760
и

1630
01:11:59,760 --> 01:12:02,320
мы просто берем все эти

1631
01:12:02,320 --> 01:12:05,199
употребления слова звезда и сворачиваем

1632
01:12:05,199 --> 01:12:07,120
их вместе

1633
01:12:07,120 --> 01:12:10,320
в один вектор слова, ммм, и вы можете

1634
01:12:10,320 --> 01:12:13,840
подумать, что это ре  союзник сумасшедший и плохой ммм, но на

1635
01:12:13,840 --> 01:12:16,480
самом деле оказывается, что он работает довольно хорошо,

1636
01:12:17,600 --> 01:12:21,120
может, я не буду проходить через все это

1637
01:12:21,120 --> 01:12:22,560
прямо сейчас, потому что на самом деле

1638
01:12:22,560 --> 01:12:24,880
есть кое-что об этом на лекции в четверг,

1639
01:12:24,880 --> 01:12:27,679
о, я вижу, я думаю, вы можете поставить

1640
01:12:27,679 --> 01:12:32,080
вперед слайды для  в следующий раз, о, подождите,

1641
01:12:33,040 --> 01:12:36,920
я знаю это, давайте посмотрим

1642
01:12:39,690 --> 01:12:43,880
[Музыка],

1643
01:12:50,400 --> 01:12:51,520
мы

1644
01:12:51,520 --> 01:12:52,880
смотрим на то,

1645
01:12:52,880 --> 01:12:55,440
как реализовать, или мы смотрим

1646
01:12:55,440 --> 01:12:57,280
на стопку чего-то вроде alexa

1647
01:12:57,280 --> 01:13:00,400
или чего-то еще, обеспечивающего речь для

1648
01:13:00,400 --> 01:13:02,480
контекстных действий в этом курсе, было ли это

1649
01:13:02,480 --> 01:13:04,640
просто главным

1650
01:13:05,760 --> 01:13:08,960
пониманием  так что это необычный мошенник

1651
01:13:08,960 --> 01:13:11,440
необычный квартал,

1652
01:13:11,440 --> 01:13:14,159
но для этого квартала есть очень

1653
01:13:14,159 --> 01:13:16,480
четкий ответ, который

1654
01:13:16,480 --> 01:13:17,520
гм в

1655
01:13:17,520 --> 01:13:20,800
этом квартале есть также уроки речи,

1656
01:13:20,800 --> 01:13:24,800
которые преподаются CS 224

1657
01:13:24,800 --> 01:13:27,679
или уроки речи, которые преподает Эндрю

1658
01:13:27,679 --> 01:13:30,000
Марс, и вы знаете, что это  курс, который

1659
01:13:30,000 --> 01:13:32,800
предлагается более регулярно, иногда

1660
01:13:32,800 --> 01:13:35,040
предлагается только раз в три года,

1661
01:13:35,040 --> 01:13:37,920
но предлагается прямо сейчас, поэтому, если

1662
01:13:37,920 --> 01:13:40,000
вы хотите узнать о

1663
01:13:40,000 --> 01:13:43,840
распознавании речи и узнать о

1664
01:13:43,840 --> 01:13:47,360
некоторых методах построения диалога  системы,

1665
01:13:47,360 --> 01:13:48,520
вы должны выполнить

1666
01:13:48,520 --> 01:13:50,960
cs224 да,

1667
01:13:50,960 --> 01:13:54,640
так что вы знаете, что для этого класса в целом

1668
01:13:54,640 --> 01:13:59,120
большая часть этого класса работает

1669
01:13:59,120 --> 01:14:02,719
с текстом и выполняет различные виды

1670
01:14:02,719 --> 01:14:06,159
анализа и понимания текста, поэтому мы выполняем

1671
01:14:06,159 --> 01:14:08,000
задачи, подобные тем, которые я упомянул,

1672
01:14:08,000 --> 01:14:12,239
мы выполняем машинный перевод  мы

1673
01:14:12,239 --> 01:14:15,600
действительно отвечаем на вопросы эм мы смотрим на то, как

1674
01:14:15,600 --> 01:14:18,080
передать эту структуру предложений и

1675
01:14:18,080 --> 01:14:20,640
тому подобное, вы знаете, в другие годы

1676
01:14:20,640 --> 01:14:22,480
я иногда говорю немного о

1677
01:14:22,480 --> 01:14:24,320
речи эм, но

1678
01:14:24,320 --> 01:14:25,840
с этого квартала есть совсем

1679
01:14:25,840 --> 01:14:27,840
другой класс, который сосредоточен на речи,

1680
01:14:27,840 --> 01:14:30,960
которая кажется немного  немного глупо,

1681
01:14:31,120 --> 01:14:32,640
я думаю, у тебя

1682
01:14:32,640 --> 01:14:34,719
есть часть партнерства со своей

1683
01:14:34,719 --> 01:14:37,280
аудиторией.

1684
01:14:38,890 --> 01:14:40,239
[Музыка]

1685
01:14:40,239 --> 01:14:43,840
больше о речи

1686
01:14:47,760 --> 01:14:49,440
я сейчас получаю плохое эхо, я не уверен,

1687
01:14:49,440 --> 01:14:51,679
моя ли это вина или твоя, но в

1688
01:14:51,679 --> 01:14:54,000
любом случае эм, в

1689
01:14:54,000 --> 01:14:58,000
любом случае, ответь да, так что класс речи

1690
01:14:58,000 --> 01:15:01,040
делает смесь чего-то, поэтому я имею в виду, что

1691
01:15:01,040 --> 01:15:04,239
классические проблемы с чистой речью

1692
01:15:04,239 --> 01:15:07,360
были связаны с распознаванием речи, таким образом

1693
01:15:07,360 --> 01:15:10,159
переходя от речевого сигнала к тексту и

1694
01:15:10,159 --> 01:15:11,120
выполнением

1695
01:15:11,120 --> 01:15:15,120
преобразования текста в речь, переходя от текста к нам,

1696
01:15:15,120 --> 01:15:17,360
к речевому сигналу и  обе эти

1697
01:15:17,360 --> 01:15:19,600
проблемы теперь обычно решаются, в

1698
01:15:19,600 --> 01:15:21,040
том

1699
01:15:21,040 --> 01:15:22,719
числе сотовым телефоном, который находится в вашем

1700
01:15:22,719 --> 01:15:23,679
кармане,

1701
01:15:23,679 --> 01:15:26,560
с использованием нейронных сетей, и поэтому он охватывает

1702
01:15:26,560 --> 01:15:30,000
оба из них, но затем между ними

1703
01:15:30,000 --> 01:15:32,239
класс охватывает довольно много, и, в

1704
01:15:32,239 --> 01:15:35,040
частности, он начинается с

1705
01:15:35,040 --> 01:15:37,679
рассмотрения строительства  диалоговая система, так что

1706
01:15:37,679 --> 01:15:40,480
это что-то вроде alexa

1707
01:15:40,480 --> 01:15:43,520
google assistant siri,

1708
01:15:43,520 --> 01:15:45,760
если предположить, что у вас есть

1709
01:15:45,760 --> 01:15:49,040
распознавание речи система преобразования текста в речь,

1710
01:15:49,040 --> 01:15:52,080
тогда у вас действительно есть текст и текст,

1711
01:15:52,080 --> 01:15:54,320
какие способы люди

1712
01:15:54,320 --> 01:15:56,800
строят  гм

1713
01:15:56,800 --> 01:15:58,640
ммм

1714
01:15:58,640 --> 01:16:01,440
диалоговые системы, подобные тем, о которых я

1715
01:16:01,440 --> 01:16:04,159
только что упомянул, у

1716
01:16:04,239 --> 01:16:06,960
меня действительно был вопрос, поэтому

1717
01:16:06,960 --> 01:16:08,239
я думаю, что некоторые люди в

1718
01:16:08,239 --> 01:16:10,960
чате заметили, что эти противоположности

1719
01:16:10,960 --> 01:16:12,480
были действительно близко друг к другу, что было

1720
01:16:12,480 --> 01:16:15,600
немного странно, но мне также было интересно, ммм

1721
01:16:15,600 --> 01:16:18,320
как насчет положительной и отрицательной

1722
01:16:18,320 --> 01:16:20,880
валентности или подобного аффекта, эм,

1723
01:16:20,880 --> 01:16:24,239
это хорошо отражено в этом типе модели, или

1724
01:16:24,239 --> 01:16:26,480
это похоже на не очень хорошо, как хорошо,

1725
01:16:26,480 --> 01:16:27,679
как с противоположностями, как это

1726
01:16:27,679 --> 01:16:30,080
не было на самом деле  да, поэтому краткий ответ

1727
01:16:30,080 --> 01:16:32,640
предназначен для обоих, и поэтому это

1728
01:16:32,640 --> 01:16:35,520
хороший вопрос, хорошее наблюдение,

1729
01:16:35,520 --> 01:16:37,520
и короткий ответ: нет, оба они

1730
01:16:37,520 --> 01:16:40,320
улавливаются очень, очень плохо, я имею в виду,

1731
01:16:40,320 --> 01:16:42,719
что есть определение,

1732
01:16:43,520 --> 01:16:46,000
о, вы знаете, когда я говорю действительно очень

1733
01:16:46,000 --> 01:16:48,400
плохо  Я имею в виду,

1734
01:16:48,400 --> 01:16:51,199
что я имею в виду, что если это то,

1735
01:16:51,199 --> 01:16:54,320
на чем вы хотите сосредоточиться, у вас

1736
01:16:54,320 --> 01:16:55,760
проблемы, я имею в виду, что

1737
01:16:55,760 --> 01:16:57,840
алгоритм не работает так

1738
01:16:57,840 --> 01:17:01,040
точно, вы обнаруживаете,

1739
01:17:01,040 --> 01:17:04,080
что вы знаете, что антонимы обычно

1740
01:17:04,080 --> 01:17:06,239
встречаются в очень похожих темах, потому что вы

1741
01:17:06,239 --> 01:17:08,239
знаете  говорит ли это,

1742
01:17:08,239 --> 01:17:10,800
что вы знаете, что Джон действительно высокий, или

1743
01:17:10,800 --> 01:17:14,239
Джон действительно невысокий, или этот фильм был

1744
01:17:14,239 --> 01:17:16,880
фантастическим, или этот фильм был ужасным,

1745
01:17:16,880 --> 01:17:18,640
верно, вы получаете

1746
01:17:18,640 --> 01:17:20,880
антонимы, встречающиеся в одном контексте,

1747
01:17:20,880 --> 01:17:23,440
поэтому их векторы

1748
01:17:23,440 --> 01:17:26,000
очень похожи и одинаковы для своего рода

1749
01:17:26,000 --> 01:17:28,320
аффекта и  слова, основанные на сантиментах, ну,

1750
01:17:28,320 --> 01:17:32,320
например, отличный и ужасный пример,

1751
01:17:32,320 --> 01:17:35,360
их контексты схожи, они

1752
01:17:35,360 --> 01:17:36,640
на самом деле,

1753
01:17:36,640 --> 01:17:38,400
если вы просто изучаете

1754
01:17:38,400 --> 01:17:42,400
такие предсказательные слова и контекстные модели,

1755
01:17:42,400 --> 01:17:45,520
ну нет, это не captu  красный, это еще

1756
01:17:45,520 --> 01:17:48,400
не конец истории, я имею в виду, что вы знаете, что

1757
01:17:48,400 --> 01:17:50,719
абсолютно люди хотели использовать нейронные

1758
01:17:50,719 --> 01:17:53,920
сети для определения настроений и других

1759
01:17:53,920 --> 01:17:56,480
видов коннотационного эффекта, и

1760
01:17:56,480 --> 01:17:58,800
есть очень хорошие способы сделать это, но

1761
01:17:58,800 --> 01:18:01,360
каким-то образом вам нужно сделать что-то большее,

1762
01:18:01,360 --> 01:18:03,679
чем просто прогнозирование  слова в контексте,

1763
01:18:03,679 --> 01:18:06,239
потому что этого недостаточно, чтобы

1764
01:18:06,239 --> 01:18:08,080
охватывать это измерение,

1765
01:18:08,080 --> 01:18:12,040
а больше о последующих

1766
01:18:12,640 --> 01:18:14,400
прилагательных, таких как очень простые

1767
01:18:14,400 --> 01:18:17,120
прилагательные, как так и не нравится,

1768
01:18:17,120 --> 01:18:18,719
потому что они хотели бы появиться в

1769
01:18:18,719 --> 01:18:21,120
похожем контексте правильно,

1770
01:18:21,120 --> 01:18:23,600
что было вашим первым примером раньше, а не

1771
01:18:23,600 --> 01:18:26,640
как так  так здорово, так что

1772
01:18:26,640 --> 01:18:28,880
это на самом деле хороший вопрос, так

1773
01:18:29,679 --> 01:18:31,440
что да, есть эти очень распространенные

1774
01:18:31,440 --> 01:18:33,199
слова, которые лингвисты обычно называют

1775
01:18:33,199 --> 01:18:35,520
функциональными словами, которые, как вы

1776
01:18:35,520 --> 01:18:38,800
знаете, включают такие, как э-э,

1777
01:18:38,800 --> 01:18:42,159
а не другие, такие как и и

1778
01:18:42,159 --> 01:18:45,760
предлоги, как вы знаете  два, и

1779
01:18:46,640 --> 01:18:49,360
вы вроде бы могли подозревать, что словесные

1780
01:18:49,360 --> 01:18:51,440
векторы для них

1781
01:18:51,440 --> 01:18:54,560
не очень хорошо работают, потому что они встречаются во

1782
01:18:54,560 --> 01:18:57,040
всех видах разных контекстов и  г,

1783
01:18:59,040 --> 01:19:01,600
во многих случаях они не очень отличаются друг от друга, и в первом

1784
01:19:01,600 --> 01:19:03,920
приближении я думаю, что это правда, и

1785
01:19:03,920 --> 01:19:05,440
отчасти поэтому я не использовал их в качестве

1786
01:19:05,440 --> 01:19:09,840
примеров на своих слайдах. Да,

1787
01:19:10,000 --> 01:19:12,320
но вы знаете, в конце концов, что мы

1788
01:19:12,320 --> 01:19:14,560
создаем  векторные представления этих

1789
01:19:14,560 --> 01:19:17,360
слов тоже, и через несколько лекций вы увидите,

1790
01:19:17,360 --> 01:19:20,400
когда мы начнем строить

1791
01:19:20,400 --> 01:19:22,400
то, что мы называем языковыми моделями, что на

1792
01:19:22,400 --> 01:19:24,640
самом деле они отлично справляются с этими

1793
01:19:24,640 --> 01:19:26,880
словами, я хочу объяснить, что я имею в

1794
01:19:26,880 --> 01:19:29,280
виду здесь.  означает, что

1795
01:19:29,280 --> 01:19:30,480
вы знаете, что

1796
01:19:30,480 --> 01:19:31,679
еще одна

1797
01:19:31,679 --> 01:19:34,320
особенность модели word to vect заключается в

1798
01:19:34,320 --> 01:19:36,800
том, что на самом деле она игнорирует положение

1799
01:19:36,800 --> 01:19:39,840
слов, поэтому в нем говорилось, что я собираюсь

1800
01:19:39,840 --> 01:19:42,560
предсказать каждое слово вокруг центрального

1801
01:19:42,560 --> 01:19:45,360
слова, но вы знаете, что я прогнозирую его

1802
01:19:45,360 --> 01:19:47,600
так же, как и я  не предсказывая по-

1803
01:19:47,600 --> 01:19:49,920
разному слово передо мной, или

1804
01:19:49,920 --> 01:19:53,040
слово после меня, или слово, находящееся на расстоянии двух

1805
01:19:53,040 --> 01:19:54,640
в любом направлении, верно, все они

1806
01:19:54,640 --> 01:19:57,920
просто предсказываются одинаково с помощью этой

1807
01:19:57,920 --> 01:20:00,560
функции вероятности, и поэтому, если это

1808
01:20:00,560 --> 01:20:02,960
все, что у вас есть, это разрушает

1809
01:20:02,960 --> 01:20:06,480
ваш  умение делать хорошую работу в UM

1810
01:20:06,480 --> 01:20:09,199
Captur  используя такие общие, более

1811
01:20:09,199 --> 01:20:12,239
грамматические слова, такие как so not an и,

1812
01:20:12,239 --> 01:20:14,639
но мы строим немного разные модели,

1813
01:20:14,639 --> 01:20:16,800
которые более чувствительны к

1814
01:20:16,800 --> 01:20:18,639
структуре предложений, а затем мы начинаем

1815
01:20:18,639 --> 01:20:21,199
хорошо работать над ними,

1816
01:20:21,199 --> 01:20:23,920
хорошо, спасибо, у

1817
01:20:24,880 --> 01:20:26,560
меня был вопрос о

1818
01:20:26,560 --> 01:20:28,880
характеристике  слово в факт

1819
01:20:28,880 --> 01:20:32,880
гм, потому что II,

1820
01:20:36,400 --> 01:20:37,760
который немного отличался от того, как он

1821
01:20:37,760 --> 01:20:39,520
был представлен в микроволновке, так

1822
01:20:39,520 --> 01:20:43,120
это как две дополнительные причины,

1823
01:20:43,120 --> 01:20:45,120
да, так что

1824
01:20:45,120 --> 01:20:49,120
мне еще есть что сказать, так что я буду в

1825
01:20:49,120 --> 01:20:52,639
курсе в четверг, эм, чтобы узнать больше о векторах слов

1826
01:20:52,639 --> 01:20:55,760
эм, вы знаете, что

1827
01:20:55,760 --> 01:20:57,199
слово к спине

1828
01:20:57,199 --> 01:21:00,320
- это своего рода структура для построения векторов слов,

1829
01:21:00,320 --> 01:21:02,480
и что в рамках существует

1830
01:21:02,480 --> 01:21:05,440
несколько вариантов точных алгоритмов,

1831
01:21:05,440 --> 01:21:08,320
и

1832
01:21:08,320 --> 01:21:11,280
вы знаете, что один из них заключается в

1833
01:21:11,280 --> 01:21:12,960
том, как предсказываете

1834
01:21:12,960 --> 01:21:14,480
ли вы контекстные слова или

1835
01:21:14,480 --> 01:21:16,880
предсказывая центральное слово,

1836
01:21:16,880 --> 01:21:20,080
поэтому модель, которую я показал, предсказывала

1837
01:21:20,080 --> 01:21:23,199
контекстные слова, так что это была модель пропуска грамматики,

1838
01:21:23,199 --> 01:21:26,960
но затем есть какая-то деталь

1839
01:21:26,960 --> 01:21:30,840
о том, как, в частности, вы проводите

1840
01:21:30,840 --> 01:21:33,760
оптимизацию и что я  представленный

1841
01:21:33,760 --> 01:21:36,639
был своего рода самый простой способ сделать это, который

1842
01:21:36,639 --> 01:21:37,760
представляет собой

1843
01:21:37,760 --> 01:21:40,000
наивную оптимизацию

1844
01:21:40,000 --> 01:21:41,040
с

1845
01:21:41,040 --> 01:21:43,760
уравнением уравнение мягкого максимума для

1846
01:21:43,760 --> 01:21:47,440
векторов слов, а оказывается, что эта наивная

1847
01:21:47,440 --> 01:21:50,800
оптимизация излишне

1848
01:21:50,800 --> 01:21:53,920
затратна, и люди

1849
01:21:53,920 --> 01:21:57,040
придумали более быстрые способы сделать  это, в

1850
01:21:57,040 --> 01:21:59,280
частности, самая распространенная вещь, которую вы

1851
01:21:59,280 --> 01:22:00,880
видите, - это то, что называется пропустить грамм с

1852
01:22:00,880 --> 01:22:03,120
отрицательной выборкой, а отрицательная

1853
01:22:03,120 --> 01:22:05,520
выборка - это своего рода гораздо более

1854
01:22:05,520 --> 01:22:07,760
эффективный способ оценки вещей, и

1855
01:22:07,760 --> 01:22:10,400
я упомяну, что в четверг,

1856
01:22:10,400 --> 01:22:12,000
хорошо, хорошо,

1857
01:22:12,000 --> 01:22:12,960
спасибо,

1858
01:22:12,960 --> 01:22:16,639
кто запрашивает дополнительную информацию  о

1859
01:22:16,639 --> 01:22:19,199
том, как конструируются векторы слов,

1860
01:22:19,199 --> 01:22:21,360
помимо краткой информации о случайной

1861
01:22:21,360 --> 01:22:23,520
инициализации, а затем на

1862
01:22:23,520 --> 01:22:25,360
основе градиента,

1863
01:22:25,360 --> 01:22:28,639
итеративная оптимизация обновления, да,

1864
01:22:28,639 --> 01:22:30,880
так что я как бы сделаю немного больше,

1865
01:22:30,880 --> 01:22:33,199
связав это вместе, в

1866
01:22:33,199 --> 01:22:35,679
четверговой лекции, я думаю, такого рода

1867
01:22:35,679 --> 01:22:37,760
только так много, что можно  вписывается в первый

1868
01:22:37,760 --> 01:22:38,560
класс,

1869
01:22:38,560 --> 01:22:40,560
но картинка

1870
01:22:40,560 --> 01:22:43,199
на картинке - это, по сути, картинка, которую я

1871
01:22:43,199 --> 01:22:46,480
показал по частям, поэтому,

1872
01:22:46,480 --> 01:22:49,040
чтобы выучить векторы слов,

1873
01:22:49,040 --> 01:22:53,199
вы начинаете с Хавина  ga вектор для

1874
01:22:53,199 --> 01:22:54,159
каждого

1875
01:22:54,159 --> 01:22:58,320
типа слова как для контекста, так и за его пределами,

1876
01:22:58,320 --> 01:23:01,199
а также тех векторов, которые вы инициализируете

1877
01:23:01,199 --> 01:23:03,280
случайным

1878
01:23:03,280 --> 01:23:05,120
образом,

1879
01:23:05,120 --> 01:23:07,760
чтобы вы просто помещали небольшие числа,

1880
01:23:07,760 --> 01:23:10,000
которые случайным образом генерируются в каждом

1881
01:23:10,000 --> 01:23:12,239
векторном компоненте, и это только ваша

1882
01:23:12,239 --> 01:23:15,360
отправная точка, и с этого момента

1883
01:23:15,360 --> 01:23:18,000
вы используете итеративный  алгоритм, в

1884
01:23:18,000 --> 01:23:20,320
котором вы постепенно обновляете

1885
01:23:20,320 --> 01:23:23,199
эти векторы слов, чтобы они

1886
01:23:23,199 --> 01:23:26,320
лучше предсказывали, какие слова появляются

1887
01:23:26,320 --> 01:23:29,360
в контексте других слов, и способ,

1888
01:23:29,360 --> 01:23:33,120
которым мы собираемся это сделать, заключается в использовании

1889
01:23:33,120 --> 01:23:36,800
градиентов, которые я как бы

1890
01:23:36,800 --> 01:23:39,199
начал  чтобы показать, как рассчитать, а

1891
01:23:39,199 --> 01:23:41,440
затем вы знаете, что когда у вас есть градиент,

1892
01:23:41,440 --> 01:23:43,520
вы можете идти в направлении, противоположном

1893
01:23:43,520 --> 01:23:46,080
градиенту, а затем вы идете

1894
01:23:46,080 --> 01:23:49,679
вниз по склону, вы минимизируете свои потери,

1895
01:23:49,679 --> 01:23:52,080
и мы собираемся сделать многое из

1896
01:23:52,080 --> 01:23:54,560
этого  пока наши векторы слов не станут настолько хорошими,

1897
01:23:54,560 --> 01:23:57,199
насколько это возможно, чтобы вы знали,

1898
01:23:58,239 --> 01:24:00,960
что на самом деле это все математика, но в каком-то смысле

1899
01:24:00,960 --> 01:24:04,239
вы знаете, что изучение векторных слов - это своего рода

1900
01:24:04,239 --> 01:24:07,360
чудо, поскольку вы буквально просто

1901
01:24:07,360 --> 01:24:10,159
начинаете с совершенно случайных  векторы слов

1902
01:24:11,440 --> 01:24:12,159
и

1903
01:24:12,159 --> 01:24:15,120
запустите этот алгоритм предсказания слов

1904
01:24:15,120 --> 01:24:16,719
на долгое время,

1905
01:24:16,719 --> 01:24:19,760
и из ничего появляются эти векторы слов,

1906
01:24:19,760 --> 01:24:24,280
которые хорошо представляют значение

