1
00:00:04,960 --> 00:00:06,799
привет всем

2
00:00:06,799 --> 00:00:11,280
добро пожаловать в stanford cs224n, также

3
00:00:11,280 --> 00:00:13,759
известный как ling 284

4
00:00:13,759 --> 00:00:15,920
обработка естественного языка с глубоким

5
00:00:15,920 --> 00:00:18,000
обучением, я Кристофер Мэннинг, и

6
00:00:18,000 --> 00:00:22,000
я главный инструктор этого класса,

7
00:00:22,000 --> 00:00:22,800
поэтому

8
00:00:22,800 --> 00:00:25,439
то, что мы надеемся сделать сегодня,

9
00:00:25,439 --> 00:00:27,519
- это погрузиться прямо в него, так что я  Я собираюсь

10
00:00:27,519 --> 00:00:30,080
потратить около 10 минут, рассказывая о

11
00:00:30,080 --> 00:00:31,439
курсе,

12
00:00:31,439 --> 00:00:33,040
а затем мы перейдем прямо

13
00:00:33,040 --> 00:00:35,040
к содержанию, причины, которые я объясню

14
00:00:35,040 --> 00:00:37,600
через минуту, так что мы поговорим о

15
00:00:37,600 --> 00:00:39,840
человеческом языке и значении слов,

16
00:00:39,840 --> 00:00:42,559
затем я представлю  идеи алгоритма word to

17
00:00:42,559 --> 00:00:45,120
vec для изучения значения слова,

18
00:00:45,120 --> 00:00:47,280
а затем, исходя из этого, мы как бы

19
00:00:47,280 --> 00:00:49,920
конкретно проработаем, как вы можете

20
00:00:49,920 --> 00:00:52,320
разработать градиенты целевой функции по

21
00:00:52,320 --> 00:00:54,960
отношению к алгоритму word deveck,

22
00:00:54,960 --> 00:00:56,399
и немного расскажем о том, как

23
00:00:56,399 --> 00:00:59,120
работает оптимизация и затем прямо в

24
00:00:59,120 --> 00:01:01,440
конце урока я хочу потратить

25
00:01:01,440 --> 00:01:03,760
немного времени на то, чтобы дать вам представление о

26
00:01:03,760 --> 00:01:04,799
том, как

27
00:01:04,799 --> 00:01:07,200
работают эти векторы слов и что вы можете

28
00:01:07,200 --> 00:01:10,400
с ними делать, так что на самом деле ключевым уроком на

29
00:01:10,400 --> 00:01:13,600
сегодня является то, что я хочу дать вам смысл

30
00:01:13,600 --> 00:01:15,600
о том, как  удивительные

31
00:01:15,600 --> 00:01:18,159
векторы слов глубокого обучения, поэтому мы

32
00:01:18,159 --> 00:01:20,799
получили действительно удивительный результат, что

33
00:01:20,799 --> 00:01:23,040
значение слова может быть представлено не

34
00:01:23,040 --> 00:01:25,520
идеально, но довольно хорошо с помощью

35
00:01:25,520 --> 00:01:28,880
большого вектора действительных чисел, и вы

36
00:01:28,880 --> 00:01:30,640
знаете, что это своего рода обычное

37
00:01:30,640 --> 00:01:32,320
место в последнее десятилетие глубокого обучения.

38
00:01:32,320 --> 00:01:35,840
учиться, но это бросает вызов

39
00:01:35,840 --> 00:01:38,320
тысячелетней традиции, и это

40
00:01:38,320 --> 00:01:40,799
действительно довольно неожиданный результат -

41
00:01:40,799 --> 00:01:43,040
начать сосредотачиваться на том, что

42
00:01:43,040 --> 00:01:45,439
хорошо, так быстро, что мы надеемся

43
00:01:45,439 --> 00:01:48,320
научить в этом курсе, поэтому у нас есть три

44
00:01:48,320 --> 00:01:52,159
основные цели, первая - это  научить

45
00:01:52,159 --> 00:01:55,200
вас основам - хорошее глубокое

46
00:01:55,200 --> 00:01:57,439
понимание влияния современных

47
00:01:57,439 --> 00:02:00,079
методов глубокого обучения, применяемых к NLP,

48
00:02:00,079 --> 00:02:02,240
поэтому мы собираемся начать и пройтись

49
00:02:02,240 --> 00:02:05,200
по основам, а затем перейдем к ключевым

50
00:02:05,200 --> 00:02:07,759
методам, которые используются в рекуррентных

51
00:02:07,759 --> 00:02:10,160
сетях NLP, преобразователях внимания и

52
00:02:10,160 --> 00:02:11,840
тому подобном.  Таким образом,

53
00:02:11,840 --> 00:02:14,319
мы хотим сделать нечто большее, чем просто

54
00:02:14,319 --> 00:02:17,040
то, что мы также хотели бы дать вам некоторое

55
00:02:17,040 --> 00:02:19,360
представление об общей картине понимания

56
00:02:19,360 --> 00:02:21,920
человеческих языков и каковы причины того,

57
00:02:21,920 --> 00:02:24,000
почему они  На самом деле довольно

58
00:02:24,000 --> 00:02:26,319
сложно понять и воспроизвести, хотя

59
00:02:26,319 --> 00:02:27,840
люди, кажется,

60
00:02:27,840 --> 00:02:30,560
делают это сейчас легко, очевидно, если вы действительно

61
00:02:30,560 --> 00:02:32,480
хотите узнать много по этой теме, вам

62
00:02:32,480 --> 00:02:34,560
следует записаться и пойти и начать посещать

63
00:02:34,560 --> 00:02:36,000
некоторые классы на факультете лингвистики,

64
00:02:36,000 --> 00:02:38,400
но, тем не менее, для многих

65
00:02:38,400 --> 00:02:41,599
это единственный контент на человеческом языке, который

66
00:02:41,599 --> 00:02:43,519
вы увидите во время получения

67
00:02:43,519 --> 00:02:45,760
степени магистра или чего-то еще, и поэтому мы надеемся

68
00:02:45,760 --> 00:02:47,840
потратить на это немного времени,

69
00:02:47,840 --> 00:02:49,760
начиная с сегодняшнего дня,

70
00:02:49,760 --> 00:02:52,160
а затем, наконец, мы хотим дать вам

71
00:02:52,160 --> 00:02:54,160
понимание

72
00:02:54,160 --> 00:02:57,040
способности создавать системы в  пи-факел для некоторых

73
00:02:57,040 --> 00:02:59,360
из основных проблем в nlp, поэтому мы

74
00:02:59,360 --> 00:03:01,760
рассмотрим изучение значений слов, анализ зависимостей,

75
00:03:01,760 --> 00:03:04,239
машинный перевод,

76
00:03:04,239 --> 00:03:05,760
ответ на вопрос,

77
00:03:05,760 --> 00:03:10,400
давайте погрузимся в человеческий язык

78
00:03:10,400 --> 00:03:13,440
когда-то давно у меня было гораздо более длинное

79
00:03:13,440 --> 00:03:16,000
введение, в котором было много примеров

80
00:03:16,000 --> 00:03:18,480
того, как человек  языки могут быть

81
00:03:18,480 --> 00:03:21,200
непонятыми и сложными, я покажу

82
00:03:21,200 --> 00:03:23,040
некоторые из этих

83
00:03:23,040 --> 00:03:25,760
примеров в последующих лекциях,

84
00:03:25,760 --> 00:03:28,239
но, поскольку прямо сегодня

85
00:03:28,239 --> 00:03:30,000
мы собираемся сосредоточиться на значении слов,

86
00:03:30,000 --> 00:03:33,440
хотя  Я бы просто привел один

87
00:03:33,440 --> 00:03:36,159
пример, который взят из очень хорошего

88
00:03:36,159 --> 00:03:38,640
мультфильма xkcd,

89
00:03:38,640 --> 00:03:40,080
и

90
00:03:40,080 --> 00:03:42,480
это не похоже на какую-

91
00:03:42,480 --> 00:03:44,879
то синтаксическую двусмысленность

92
00:03:44,879 --> 00:03:47,599
предложений, но вместо этого он действительно

93
00:03:47,599 --> 00:03:50,000
подчеркивает важный момент, что

94
00:03:50,000 --> 00:03:52,879
язык - это построенная социальная система

95
00:03:52,879 --> 00:03:55,599
и интерпретируется людьми, и это

96
00:03:55,599 --> 00:03:58,480
часть того, как и оно меняется, когда люди

97
00:03:58,480 --> 00:04:00,480
решают

98
00:04:00,480 --> 00:04:02,480
адаптировать его конструкцию, и это

99
00:04:02,480 --> 00:04:05,360


100
00:04:05,360 --> 00:04:07,599
одна из причин, почему человеческие языки хороши как адаптивная система для

101
00:04:07,599 --> 00:04:08,959
людей,

102
00:04:08,959 --> 00:04:12,159
но трудны

103
00:04:12,159 --> 00:04:14,640
для понимания как система или наши компьютеры по сей

104
00:04:14,640 --> 00:04:15,439
день

105
00:04:15,439 --> 00:04:18,000
так что в этом разговоре между

106
00:04:18,000 --> 00:04:20,560
двумя женщинами одна говорит, что в любом случае мне наплевать,

107
00:04:20,560 --> 00:04:23,520
а другая говорит, что я думаю, вы имеете в виду, что

108
00:04:23,520 --> 00:04:25,280
вам наплевать,

109
00:04:25,280 --> 00:04:27,840
говоря, что вам наплевать, подразумевает, что вы

110
00:04:27,840 --> 00:04:29,759
хоть немного заботитесь,

111
00:04:29,759 --> 00:04:32,479
а другая говорит, что мне наплевать.  Я знаю,

112
00:04:32,479 --> 00:04:34,560
где эти невероятно сложные

113
00:04:34,560 --> 00:04:37,440
мозги плывут по пустоте,

114
00:04:37,440 --> 00:04:39,840
тщетно пытаясь соединиться друг с другом,

115
00:04:39,840 --> 00:04:41,840
слепо убегая слов в

116
00:04:41,840 --> 00:04:43,120
темноту,

117
00:04:43,120 --> 00:04:45,680
каждый выбор формулировки по буквам  g,

118
00:04:45,680 --> 00:04:48,800
тон и время несут бесчисленные

119
00:04:48,800 --> 00:04:51,759
сигналы, контексты, подтексты и

120
00:04:51,759 --> 00:04:52,800
многое другое,

121
00:04:52,800 --> 00:04:54,880
и каждый слушатель интерпретирует эти

122
00:04:54,880 --> 00:04:56,960
сигналы по-своему.

123
00:04:56,960 --> 00:04:59,520
язык не является формальной системой.

124
00:04:59,520 --> 00:05:01,840


125
00:05:01,840 --> 00:05:03,759


126
00:05:03,759 --> 00:05:06,000


127
00:05:06,000 --> 00:05:08,479
Что можно сделать, это попытаться

128
00:05:08,479 --> 00:05:11,199
научиться угадывать, как ваши слова влияют на людей, чтобы у

129
00:05:11,199 --> 00:05:13,120
вас был шанс найти

130
00:05:13,120 --> 00:05:15,199
те, которые заставят их почувствовать что-то

131
00:05:15,199 --> 00:05:17,360
вроде того, что вы хотите, чтобы они чувствовали

132
00:05:17,360 --> 00:05:19,520
все остальное бессмысленно,

133
00:05:19,520 --> 00:05:21,520
я полагаю, вы даете мне советы по поводу  как

134
00:05:21,520 --> 00:05:23,680
вы интерпретируете слова, потому что хотите, чтобы

135
00:05:23,680 --> 00:05:25,680
я чувствовал себя менее одиноким,

136
00:05:25,680 --> 00:05:29,600
если да, тогда спасибо, это очень много значит,

137
00:05:29,600 --> 00:05:31,919
но если вы просто пропускаете мои предложения

138
00:05:31,919 --> 00:05:34,320
мимо какого-то мысленного контрольного списка, чтобы вы могли

139
00:05:34,320 --> 00:05:36,320
показать, насколько хорошо вы это знаете,

140
00:05:36,320 --> 00:05:38,639
тогда мне было бы наплевать,

141
00:05:38,639 --> 00:05:41,199
ладно  так что в конечном итоге наша

142
00:05:41,199 --> 00:05:44,800
цель состоит в том, как лучше работать над

143
00:05:44,800 --> 00:05:48,720
созданием гм вычислительных систем,

144
00:05:48,720 --> 00:05:49,520
которые

145
00:05:49,520 --> 00:05:52,320
пытаются лучше угадывать, как их

146
00:05:52,320 --> 00:05:55,039
слова повлияют на других людей и что

147
00:05:55,039 --> 00:05:57,280
другие люди имеют в виду  По словам,

148
00:05:57,280 --> 00:06:00,400
которые они предпочитают говорить,

149
00:06:01,360 --> 00:06:04,560
так что интересная вещь о человеческом

150
00:06:04,560 --> 00:06:08,160
языке заключается в том, что это система,

151
00:06:08,160 --> 00:06:11,120
созданная людьми,

152
00:06:11,120 --> 00:06:11,919
ммм,

153
00:06:11,919 --> 00:06:15,039
и эта система была построена,

154
00:06:15,039 --> 00:06:18,639
вы знаете, в некотором смысле, относительно недавно,

155
00:06:18,639 --> 00:06:21,520
поэтому в обсуждениях искусственного

156
00:06:21,520 --> 00:06:24,880
интеллекта ммм  большую часть времени

157
00:06:24,880 --> 00:06:25,759
люди

158
00:06:25,759 --> 00:06:28,479
уделяют много внимания человеческому мозгу и

159
00:06:28,479 --> 00:06:32,240
нейронам, гудящим мимо, и этому разуму,

160
00:06:32,240 --> 00:06:34,400
который должен находиться в головах людей,

161
00:06:34,400 --> 00:06:36,639
но я просто хотел на

162
00:06:36,639 --> 00:06:40,000
мгновение сосредоточиться на роли языка,

163
00:06:40,000 --> 00:06:41,280
на самом деле

164
00:06:41,280 --> 00:06:43,680
вы знаете, что это своего рода  спорно,

165
00:06:43,680 --> 00:06:44,639
но

166
00:06:44,639 --> 00:06:46,560
вы знаете, что это не обязательно тот случай

167
00:06:46,560 --> 00:06:48,960
, когда люди намного

168
00:06:48,960 --> 00:06:51,120
умнее некоторых высших обезьян, таких как

169
00:06:51,120 --> 00:06:54,160
шимпанзе или бонобо, так что

170
00:06:54,160 --> 00:06:56,319
шимпанзе и бонобо, как было показано

171
00:06:56,319 --> 00:06:59,199
, могут использовать бассейны, чтобы строить планы,

172
00:06:59,199 --> 00:07:01,440
и на самом деле шимпанзе намного лучше

173
00:07:01,440 --> 00:07:05,360
коротких  -срочная память, чем человеческие существа

174
00:07:05,360 --> 00:07:07,680
, относительно того, что если вы посмотрите

175
00:07:07,680 --> 00:07:09,759
на историю жизни на Земле,

176
00:07:09,759 --> 00:07:13,120
люди действительно развивают язык в последнее

177
00:07:13,120 --> 00:07:15,840
время.  мы вроде как на самом деле

178
00:07:15,840 --> 00:07:17,440
не знаем, потому что вы знаете, что нет

179
00:07:17,440 --> 00:07:19,599
окаменелостей, которые говорят хорошо, вот язык,

180
00:07:19,599 --> 00:07:22,080
эм, но вы знаете, что

181
00:07:22,080 --> 00:07:25,599
большинство людей считает, что язык возник

182
00:07:25,599 --> 00:07:28,240
для людей вроде

183
00:07:28,240 --> 00:07:29,280
вы знаете

184
00:07:29,280 --> 00:07:31,199
где-то в диапазоне от ста

185
00:07:31,199 --> 00:07:33,759
тысяч до миллиона  лет назад, ладно

186
00:07:33,759 --> 00:07:35,919
, это было давно, но по сравнению с

187
00:07:35,919 --> 00:07:38,479
процессом эволюции жизни на Земле

188
00:07:38,479 --> 00:07:41,520
, это как бы моргает веком,

189
00:07:41,520 --> 00:07:44,800
ммм, но эта мощная коммуникация

190
00:07:44,800 --> 00:07:47,919
между людьми быстро привела к нашему

191
00:07:47,919 --> 00:07:50,560
превосходству над другими существами,

192
00:07:50,560 --> 00:07:52,960
так что это интересно, что

193
00:07:52,960 --> 00:07:55,919
окончательный  Оказалось, что сила - это

194
00:07:55,919 --> 00:07:58,560
не ядовитые клыки, не супербыстрая или сверхбольшая скорость,

195
00:07:58,560 --> 00:08:00,800
а способность общаться

196
00:08:00,800 --> 00:08:02,240


197
00:08:02,240 --> 00:08:05,120
с другими членами вашего племени

198
00:08:05,120 --> 00:08:07,039
, гораздо позже

199
00:08:07,039 --> 00:08:09,680
люди снова разработали письмо, которое

200
00:08:09,680 --> 00:08:11,840
позволяло передавать знания на

201
00:08:11,840 --> 00:08:14,240
расстояниях во времени и времени.  пространство,

202
00:08:14,240 --> 00:08:16,240
и это всего лишь около

203
00:08:16,240 --> 00:08:19,360
5000 лет силе письма,

204
00:08:19,360 --> 00:08:22,080
так что всего за несколько тысяч

205
00:08:22,080 --> 00:08:24,000
лет способность сохранять и передавать

206
00:08:24,000 --> 00:08:26,879
Знание перенесло нас из бронзового века

207
00:08:26,879 --> 00:08:30,720
в современные смартфоны и планшеты,

208
00:08:30,720 --> 00:08:32,479
поэтому ключевой вопрос для искусственного

209
00:08:32,479 --> 00:08:34,320
интеллекта и взаимодействия человека с компьютером

210
00:08:34,320 --> 00:08:36,880
- как заставить компьютеры

211
00:08:36,880 --> 00:08:39,039
понимать информацию,

212
00:08:39,039 --> 00:08:41,360
передаваемую на человеческих языках

213
00:08:41,360 --> 00:08:43,839
одновременно. Искусственный интеллект

214
00:08:43,839 --> 00:08:46,160
требует компьютеров со знаниями

215
00:08:46,160 --> 00:08:47,519
людей,

216
00:08:47,519 --> 00:08:49,760
к счастью, теперь наши системы искусственного интеллекта

217
00:08:49,760 --> 00:08:52,880
могут извлечь выгоду из благотворного цикла, в котором нам

218
00:08:52,880 --> 00:08:54,800
нужны знания, чтобы хорошо понимать язык

219
00:08:54,800 --> 00:08:57,279
и людей, но также бывает и

220
00:08:57,279 --> 00:08:59,760
то, что большая часть этих знаний

221
00:08:59,760 --> 00:09:02,240
содержится в языке, распространенном

222
00:09:02,240 --> 00:09:04,399
по книгам и веб-страницам  мир, и

223
00:09:04,399 --> 00:09:05,760
это одна из вещей, которые мы собираемся

224
00:09:05,760 --> 00:09:08,000
рассмотреть в этом курсе, - это то, как мы

225
00:09:08,000 --> 00:09:11,680
можем как бы развить этот благотворный цикл,

226
00:09:11,680 --> 00:09:14,399
уже был достигнут большой прогресс,

227
00:09:14,399 --> 00:09:16,880
и я просто хочу очень быстро

228
00:09:16,880 --> 00:09:18,720
дать представление об этом

229
00:09:18,720 --> 00:09:19,839
поэтому

230
00:09:19,839 --> 00:09:21,680
в последнее

231
00:09:21,680 --> 00:09:23,760
десятилетие или около того, и особенно в последние

232
00:09:23,760 --> 00:09:26,320
несколько лет, с новыми методами машинного

233
00:09:26,320 --> 00:09:29,200
перевода мы сейчас находимся в пространстве, где

234
00:09:29,200 --> 00:09:32,399
машинный перевод  язык действительно работает

235
00:09:32,399 --> 00:09:34,880
умеренно хорошо, так что, опять же

236
00:09:34,880 --> 00:09:36,399
из мировой истории, это просто

237
00:09:36,399 --> 00:09:38,880
потрясающе, ведь тысячи лет

238
00:09:38,880 --> 00:09:41,680
изучение языков других людей было

239
00:09:41,680 --> 00:09:45,040
человеческой задачей, требующей больших

240
00:09:45,040 --> 00:09:48,000
усилий и концентрации, но теперь мы находимся

241
00:09:48,000 --> 00:09:49,760
в мире, где вы могли просто  Зайдите в

242
00:09:49,760 --> 00:09:52,800
свой веб-браузер и подумайте, о, мне интересно,

243
00:09:52,800 --> 00:09:55,200
какие новости в Кении сегодня, и вы

244
00:09:55,200 --> 00:09:57,680
можете отправиться на кенийский веб-сайт,

245
00:09:57,680 --> 00:09:59,760
и вы можете увидеть что-то вроде этого,

246
00:09:59,760 --> 00:10:02,480
и вы можете пойти, а, а затем попросить

247
00:10:02,480 --> 00:10:05,040
гугл эм перевести это для  Вы эм

248
00:10:05,040 --> 00:10:07,680
из суахили, и вы знаете, что

249
00:10:07,680 --> 00:10:10,399
перевод не совсем идеален, но

250
00:10:10,399 --> 00:10:12,399
вы знаете, что он достаточно хорош, поэтому

251
00:10:12,399 --> 00:10:15,040
газета tuko была проинформирована о том, что

252
00:10:15,040 --> 00:10:18,959
министр местного правительства Лингсан

253
00:10:18,959 --> 00:10:21,519
и его коллеги по транспорту в городе меня

254
00:10:21,519 --> 00:10:23,920
умерли в течение двух отдельных часов, так что вы

255
00:10:23,920 --> 00:10:25,680
знаете, в течение двух отдельных  часов

256
00:10:25,680 --> 00:10:28,160
немного неудобно, но, по сути, мы

257
00:10:28,160 --> 00:10:30,640
неплохо справляемся с получением информации

258
00:10:30,640 --> 00:10:33,360
с этой страницы, и это, ммм, довольно

259
00:10:33,360 --> 00:10:34,800
удивительно,

260
00:10:34,800 --> 00:10:36,800
единственная самая большая

261
00:10:36,800 --> 00:10:38,959
разработка в

262
00:10:38,959 --> 00:10:40,640
НЛП  или в прошлом году в

263
00:10:40,640 --> 00:10:43,200
популярных средствах массовой информации

264
00:10:43,200 --> 00:10:44,079
был

265
00:10:44,079 --> 00:10:45,600
gpt

266
00:10:45,600 --> 00:10:48,079
um, который был огромной новой моделью,

267
00:10:48,079 --> 00:10:51,120
выпущенной open ai um

268
00:10:51,120 --> 00:10:54,079
что такое gpt 3 и почему он великолепен,

269
00:10:54,079 --> 00:10:56,480
на самом деле немного тонко, и поэтому я не могу

270
00:10:56,480 --> 00:10:58,560
действительно пройти через все  подробности

271
00:10:58,560 --> 00:11:02,160
здесь, но это интересно, потому что

272
00:11:02,160 --> 00:11:04,079
кажется, что это первый шаг на

273
00:11:04,079 --> 00:11:06,320
пути к тому, что мы можем назвать универсальными

274
00:11:06,320 --> 00:11:09,120
моделями, где вы можете тренировать одну

275
00:11:09,120 --> 00:11:12,399
чрезвычайно большую модель на чем-то вроде

276
00:11:12,399 --> 00:11:14,880
той картинки библиотеки, которую я показывал ранее,

277
00:11:14,880 --> 00:11:16,320
и она просто

278
00:11:16,320 --> 00:11:18,480
знает  мировое знание

279
00:11:18,480 --> 00:11:21,040
человеческих языков знание того, как выполнять

280
00:11:21,040 --> 00:11:23,519
задачи, и затем вы можете применять его для выполнения

281
00:11:23,519 --> 00:11:26,640
самых разных задач, поэтому мы больше не

282
00:11:26,640 --> 00:11:29,440
строим модель для обнаружения спама, а

283
00:11:29,440 --> 00:11:32,240
затем модель для обнаружения порнографии, а затем

284
00:11:32,240 --> 00:11:34,720
модель для обнаружения

285
00:11:34,720 --> 00:11:36,959
чего угодно  контент на иностранном языке и

286
00:11:36,959 --> 00:11:38,240
просто построив все эти отдельные

287
00:11:38,240 --> 00:11:39,920
контролируемые классификаторы для каждой

288
00:11:39,920 --> 00:11:42,720
задачи, мы только что создали

289
00:11:42,720 --> 00:11:46,079
модель, которая так точно понимает, что

290
00:11:46,079 --> 00:11:48,640
она делает, просто предсказывает

291
00:11:48,640 --> 00:11:52,160
следующие слова и

292
00:11:52,160 --> 00:11:54,240
т. д.  он ушел,

293
00:11:54,240 --> 00:11:58,480
ему сказали написать об Илоне Маске

294
00:11:58,480 --> 00:12:00,720
в стиле

295
00:12:00,720 --> 00:12:02,160
доктора Сьюза,

296
00:12:02,160 --> 00:12:05,040
и он начался с некоторого текста, а

297
00:12:05,040 --> 00:12:07,040
затем он генерирует больше текста, и

298
00:12:07,040 --> 00:12:09,279
способ, которым он генерирует больше текста

299
00:12:09,279 --> 00:12:10,480
,

300
00:12:10,480 --> 00:12:13,279
буквально просто предсказывает одно слово за раз, когда

301
00:12:13,279 --> 00:12:14,880


302
00:12:14,880 --> 00:12:16,880
приходят следующие слова  для завершения текста,

303
00:12:16,880 --> 00:12:20,399
но у этого есть очень мощное средство,

304
00:12:20,399 --> 00:12:23,600
потому что то, что вы можете сделать с

305
00:12:23,600 --> 00:12:25,839
3, - это дать ему несколько

306
00:12:25,839 --> 00:12:28,720
примеров того, что вы хотите, чтобы

307
00:12:28,720 --> 00:12:31,760
я мог дать ему текст и сказать, что я сломал

308
00:12:31,760 --> 00:12:32,959
изменение окна

309
00:12:32,959 --> 00:12:34,800
это вопрос, что я

310
00:12:34,800 --> 00:12:37,680
сломал, я изящно сохранил день, когда

311
00:12:37,680 --> 00:12:40,000
изменил его, на вопрос, что я

312
00:12:40,000 --> 00:12:43,440
изящно сохранил, поэтому это приглашение um сообщает

313
00:12:43,440 --> 00:12:45,519
gpt 3, что я хочу,

314
00:12:45,519 --> 00:12:48,000
чтобы он делал, а затем, если я

315
00:12:48,000 --> 00:12:49,920
дам ему другое утверждение, например, я  дал

316
00:12:49,920 --> 00:12:53,200
Джону Флауэрс, я могу сказать, что gpt-3

317
00:12:53,200 --> 00:12:55,760
предскажет, какие слова будут дальше, и он будет

318
00:12:55,760 --> 00:12:58,320
следовать моей подсказке и покажет, кому я

319
00:12:58,320 --> 00:13:01,200
подарил цветы или я могу сказать, что подарил

320
00:13:01,200 --> 00:13:04,079
ей розу и гитару, и он будет

321
00:13:04,079 --> 00:13:06,639
следовать идее  узор и делать кто я

322
00:13:06,639 --> 00:13:08,639
подарил розу и гитару

323
00:13:08,639 --> 00:13:11,200
и действовать  На самом деле эта одна модель может делать

324
00:13:11,200 --> 00:13:13,920
удивительный набор вещей, в том числе

325
00:13:13,920 --> 00:13:16,560
многие, что довольно удивительно,

326
00:13:16,560 --> 00:13:18,880
чтобы дать только один пример того, что

327
00:13:18,880 --> 00:13:21,200
еще одна вещь, которую вы можете сделать,

328
00:13:21,200 --> 00:13:23,040
- это заставить ее

329
00:13:23,040 --> 00:13:25,920
переводить предложения человеческого языка в

330
00:13:25,920 --> 00:13:28,560
sql, так что это может сделать это намного  легче

331
00:13:28,560 --> 00:13:30,560
сделать cs145,

332
00:13:30,560 --> 00:13:33,839
поэтому, дав ему пару

333
00:13:33,839 --> 00:13:37,839
примеров sql-перевода текста на человеческом языке,

334
00:13:37,839 --> 00:13:39,680
который я на этот раз не показываю,

335
00:13:39,680 --> 00:13:41,760
потому что он не помещается на моем слайде, я могу

336
00:13:41,760 --> 00:13:43,839
затем дать ему предложение, например, сколько

337
00:13:43,839 --> 00:13:45,680
пользователей подписались  с начала

338
00:13:45,680 --> 00:13:47,199
2020 года,

339
00:13:47,199 --> 00:13:49,920
и он превращает его в sql, или я могу дать

340
00:13:49,920 --> 00:13:51,839
ему другой запрос, каково среднее

341
00:13:51,839 --> 00:13:54,560
количество влияний, на которые каждый пользователь подписывается

342
00:13:54,560 --> 00:13:58,480
, и снова он затем преобразует это в

343
00:13:58,480 --> 00:14:00,079
sql,

344
00:14:00,079 --> 00:14:03,760
поэтому gpt gpt-3 много знает о

345
00:14:03,760 --> 00:14:05,839
значении языка  и значение

346
00:14:05,839 --> 00:14:08,880
других вещей, таких как sql, и может свободно

347
00:14:08,880 --> 00:14:11,760
манипулировать им,

348
00:14:13,040 --> 00:14:16,079
так что это приводит нас прямо к этому

349
00:14:16,079 --> 00:14:19,360
главному значению, и как мы

350
00:14:19,360 --> 00:14:22,160
хорошо представляем значение слова, что означает

351
00:14:22,160 --> 00:14:24,079
хорошо, мы могли бы найти что-то вроде

352
00:14:24,079 --> 00:14:27,760
webster d  словарный запас и скажите хорошо,

353
00:14:27,760 --> 00:14:30,480
идея представлена словом

354
00:14:30,480 --> 00:14:32,399
идея, которую человек хочет выразить,

355
00:14:32,399 --> 00:14:34,560
используя слова, знаки и т.

356
00:14:34,560 --> 00:14:35,600


357
00:14:35,600 --> 00:14:37,760


358
00:14:37,760 --> 00:14:40,160


359
00:14:40,160 --> 00:14:41,600


360
00:14:41,600 --> 00:14:44,079


361
00:14:44,079 --> 00:14:45,120


362
00:14:45,120 --> 00:14:47,920
так что они думают, что значение

363
00:14:47,920 --> 00:14:51,120
слова появляется между словом, которое

364
00:14:51,120 --> 00:14:54,160
является означающим или символом, и тем,

365
00:14:54,160 --> 00:14:56,720
что оно означает означаемую вещь,

366
00:14:56,720 --> 00:14:58,959
которая является идеей или вещью, так что

367
00:14:58,959 --> 00:15:01,360
значение слова стул

368
00:15:01,360 --> 00:15:04,480
- это совокупность вещей, которые являются стульями

369
00:15:04,480 --> 00:15:06,480
и это называется денотационной

370
00:15:06,480 --> 00:15:07,839
семантикой

371
00:15:07,839 --> 00:15:10,160
- термин, который также используется и аналогичным образом

372
00:15:10,160 --> 00:15:12,720
применяется для семантики языков программирования.

373
00:15:12,720 --> 00:15:13,920


374
00:15:13,920 --> 00:15:18,240
Эта модель не очень глубоко

375
00:15:18,480 --> 00:15:20,240
реализуема, например,

376
00:15:20,240 --> 00:15:22,959
как мне отойти от идеи, что хорошо,

377
00:15:22,959 --> 00:15:24,560
стул означает набор стульев в

378
00:15:24,560 --> 00:15:26,800
мире просто что-то  я могу манипулировать

379
00:15:26,800 --> 00:15:29,920
смыслом на своих компьютерах

380
00:15:29,920 --> 00:15:32,880
так, как это

381
00:15:32,880 --> 00:15:34,000


382
00:15:34,000 --> 00:15:35,680
обычно делается в системах обработки естественного языка.

383
00:15:35,680 --> 00:15:38,560
s использовать

384
00:15:38,560 --> 00:15:41,920
ресурсы, такие как словари и тезаурусы,

385
00:15:41,920 --> 00:15:44,560
в частности, популярным является wordnet,

386
00:15:44,560 --> 00:15:47,759
который объединяет слова и термины в

387
00:15:47,759 --> 00:15:50,639
оба синонима, устанавливает

388
00:15:50,639 --> 00:15:52,959
слова, которые могут означать одно и то же, и

389
00:15:52,959 --> 00:15:55,360
гиперонимы, которым соответствуют, - это

390
00:15:55,360 --> 00:15:58,160
отношения, и, таким образом, это

391
00:15:58,160 --> 00:15:59,839
отношения, которые вы  знаю, что мы можем как бы

392
00:15:59,839 --> 00:16:02,959
взглянуть на гипонимы панды и

393
00:16:02,959 --> 00:16:05,199
панды - это своего рода поступок,

394
00:16:05,199 --> 00:16:07,040
какими бы они ни были, я думаю, это, вероятно,

395
00:16:07,040 --> 00:16:09,920
с красными пандами, эм, это своего рода

396
00:16:09,920 --> 00:16:11,279
плотоядное животное,

397
00:16:11,279 --> 00:16:13,040
которое является своего рода плацентой, которая является

398
00:16:13,040 --> 00:16:15,040
своего рода млекопитающим, и вы вроде  из головы вверх по

399
00:16:15,040 --> 00:16:18,160
этой иерархии гипонимов,

400
00:16:18,160 --> 00:16:21,040
так что wordnet был большим

401
00:16:21,040 --> 00:16:23,839
ресурсом для nlp, но он также был

402
00:16:23,839 --> 00:16:26,240
крайне несовершенным, поэтому в

403
00:16:26,240 --> 00:16:28,639
нем не хватало множества нюансов,

404
00:16:28,639 --> 00:16:32,079
поэтому, например, в слове net proficient

405
00:16:32,079 --> 00:16:34,639
указано как синоним хорошего, но вы

406
00:16:34,639 --> 00:16:36,480
знаете, может быть, это иногда  правда, но

407
00:16:36,480 --> 00:16:38,480
кажется, что во многих контекстах это

408
00:16:38,480 --> 00:16:39,759
неправда, и вы имеете в виду нечто совершенно

409
00:16:39,759 --> 00:16:41,759
иное, когда говорите, что «опытный», а не «

410
00:16:41,759 --> 00:16:44,480
хороший», это ограничено, поскольку человеческий

411
00:16:44,480 --> 00:16:47,600
тезаурус создал

412
00:16:47,600 --> 00:16:49,279
в частности, там много слов и

413
00:16:49,279 --> 00:16:51,839
много употреблений слов, которых просто нет,

414
00:16:51,839 --> 00:16:54,399
в том числе вы знаете что-нибудь, эм,

415
00:16:54,399 --> 00:16:56,800
то есть вы знаете, что-

416
00:16:56,800 --> 00:17:00,560
то вроде более современной терминологии, например um

417
00:17:00,560 --> 00:17:02,639
wicked, существует для злой ведьмы, но

418
00:17:02,639 --> 00:17:06,559
не для более современного разговорного использования um

419
00:17:06,559 --> 00:17:09,039
ninja конечно  там нет

420
00:17:09,039 --> 00:17:10,799
такого описания программистов, которое некоторые люди делают,

421
00:17:10,799 --> 00:17:12,880
и его

422
00:17:12,880 --> 00:17:15,439
невозможно поддерживать в курсе,

423
00:17:15,439 --> 00:17:18,079
так что это требует много человеческого труда, но

424
00:17:18,079 --> 00:17:21,280
даже когда у вас есть это,

425
00:17:21,280 --> 00:17:23,919
вы знаете, что у него есть наборы синонимов, но нет

426
00:17:23,919 --> 00:17:25,919
''  У меня действительно хорошее чувство

427
00:17:25,919 --> 00:17:29,520
слов, которое означает нечто подобное, так

428
00:17:29,520 --> 00:17:33,120
что фантастическое и великое означает нечто

429
00:17:33,120 --> 00:17:35,720
похожее, но на самом деле не

430
00:17:35,720 --> 00:17:38,480
синонимы, и поэтому идея смысла

431
00:17:38,480 --> 00:17:40,559
сходства - это то, что было бы

432
00:17:40,559 --> 00:17:42,880
действительно полезно для достижения прогресса, и

433
00:17:42,880 --> 00:17:44,840
где модели глубокого обучения

434
00:17:44,840 --> 00:17:48,320
превосходны, так что  В чем проблема

435
00:17:48,320 --> 00:17:51,039
многих традиционных НЛП? Проблема

436
00:17:51,039 --> 00:17:54,400
с большим количеством традиционных НЛП

437
00:17:54,400 --> 00:17:57,200
заключается в том, что слова рассматриваются как отдельные

438
00:17:57,200 --> 00:18:00,080
символы, поэтому у нас есть символы, такие как

439
00:18:00,080 --> 00:18:03,120
конференция в отеле.  Наши слова,

440
00:18:03,120 --> 00:18:05,760
которые в глубоком обучении говорят, мы

441
00:18:05,760 --> 00:18:08,320
называем локалистическим представлением,

442
00:18:08,320 --> 00:18:10,400
и это потому, что

443
00:18:10,400 --> 00:18:12,080
если вы

444
00:18:12,080 --> 00:18:12,799
в

445
00:18:12,799 --> 00:18:16,000
системах статистического или машинного обучения

446
00:18:16,000 --> 00:18:18,559
хотите представить эти символы,

447
00:18:18,559 --> 00:18:21,280
каждый из них является отдельной вещью, поэтому

448
00:18:21,280 --> 00:18:23,039
стандартный способ их представления, и

449
00:18:23,039 --> 00:18:24,559
это то, что  вы

450
00:18:24,559 --> 00:18:27,520
делаете что-то вроде статистической модели, если

451
00:18:27,520 --> 00:18:29,039
вы строите модель логистической

452
00:18:29,039 --> 00:18:32,799
регрессии со словами в качестве функций, заключается в том, что вы

453
00:18:32,799 --> 00:18:35,360
представляете их как один горячий вектор, поэтому у вас

454
00:18:35,360 --> 00:18:37,760
есть измерение для каждого отдельного слова,

455
00:18:37,760 --> 00:18:40,320
поэтому, возможно, как в моем примере, вот мои

456
00:18:40,320 --> 00:18:43,520
представления как  векторы для мотелей и

457
00:18:43,520 --> 00:18:44,720
отелей,

458
00:18:44,720 --> 00:18:45,760


459
00:18:45,760 --> 00:18:48,080
и это означает, что у нас должны быть

460
00:18:48,080 --> 00:18:50,799
огромные векторы, соответствующие

461
00:18:50,799 --> 00:18:53,120
количеству слов в нашем словарном запасе, так что

462
00:18:53,120 --> 00:18:54,320
если у вас был

463
00:18:54,320 --> 00:18:55,919
словарь английского языка в средней школе, в нем,

464
00:18:55,919 --> 00:18:59,360
вероятно, было около 250 000 слов,

465
00:18:59,360 --> 00:19:01,440
но там  на самом деле гораздо больше слов

466
00:19:01,440 --> 00:19:03,679
в языке, так что, возможно, мы, по крайней мере,

467
00:19:03,679 --> 00:19:06,480
хотим иметь вектор размерностью 500000 мкм,

468
00:19:06,480 --> 00:19:10,400
чтобы справиться с этим,

469
00:19:10,400 --> 00:19:13,280
хорошо, но более крупный ev  ru более серьезная

470
00:19:13,280 --> 00:19:15,840
проблема с дискретными символами заключается в

471
00:19:15,840 --> 00:19:17,919
том, что у нас нет этого понятия словесных

472
00:19:17,919 --> 00:19:20,480
отношений и сходства, поэтому,

473
00:19:20,480 --> 00:19:23,280
например, в веб-поиске, если пользователь

474
00:19:23,280 --> 00:19:25,760
ищет мотель в Сиэтле, мы также хотели бы

475
00:19:25,760 --> 00:19:28,160
сопоставить документы, содержащие отель Сиэтла,

476
00:19:28,160 --> 00:19:29,200


477
00:19:29,200 --> 00:19:31,520
но наша проблема в том, что  у нас

478
00:19:31,520 --> 00:19:33,840
есть горячие векторы для разных слов,

479
00:19:33,840 --> 00:19:36,320
и поэтому в формальном математическом смысле

480
00:19:36,320 --> 00:19:38,720
эти два вектора ортогональны, так что

481
00:19:38,720 --> 00:19:41,600


482
00:19:41,600 --> 00:19:43,600
между

483
00:19:43,600 --> 00:19:45,600
ними нет естественного понятия сходства.

484
00:19:45,600 --> 00:19:48,000
и люди

485
00:19:48,000 --> 00:19:50,640
делали с этим гм

486
00:19:50,640 --> 00:19:53,919
, до 2010 года мы могли сказать, что мы

487
00:19:53,919 --> 00:19:56,080
могли бы использовать синонимы word net, и мы считаем

488
00:19:56,080 --> 00:19:58,080
вещи, которые перечисляют синонимы, в

489
00:19:58,080 --> 00:20:00,880
любом случае похожи или эй, может быть, мы могли бы каким-то образом

490
00:20:00,880 --> 00:20:02,640
создать

491
00:20:02,640 --> 00:20:04,640
представления слов, которые имеют

492
00:20:04,640 --> 00:20:06,559
совпадающее значение, и людей  делали все

493
00:20:06,559 --> 00:20:09,520
эти вещи, но они имели тенденцию сильно терпеть неудачу

494
00:20:09,520 --> 00:20:12,080
из-за незавершенности, поэтому вместо

495
00:20:12,080 --> 00:20:15,039
этого я хочу представить сегодня

496
00:20:15,039 --> 00:20:16,080
современный

497
00:20:16,080 --> 00:20:18,240
метод глубокого обучения.

498
00:20:18,240 --> 00:20:22,320


499
00:20:22,320 --> 00:20:25,440
Как мы можем это сделать? Мы

500
00:20:25,440 --> 00:20:27,200


501
00:20:27,200 --> 00:20:30,240


502
00:20:30,240 --> 00:20:32,080
используем эту идею, называемую

503
00:20:32,080 --> 00:20:34,799
распределительной семантикой,

504
00:20:34,799 --> 00:20:37,679
поэтому идея распределительной семантики

505
00:20:37,679 --> 00:20:38,720


506
00:20:38,720 --> 00:20:40,960
снова становится чем-то, что, когда вы впервые видите

507
00:20:40,960 --> 00:20:41,840
это,

508
00:20:41,840 --> 00:20:44,720
может показаться немного  немного сумасшедший,

509
00:20:44,720 --> 00:20:46,240
потому что вместо того, чтобы иметь что-то

510
00:20:46,240 --> 00:20:48,799
вроде денотационной семантики,

511
00:20:48,799 --> 00:20:51,919
мы сейчас собираемся сказать, что значение слова

512
00:20:51,919 --> 00:20:54,559
будет дано

513
00:20:54,559 --> 00:20:58,960
словами, которые часто кажутся близкими к нему.

514
00:20:58,960 --> 00:21:02,400
Дж. Ферт был британским

515
00:21:02,400 --> 00:21:05,120
лингвистом середины прошлого века.  и один

516
00:21:05,120 --> 00:21:07,120
из его содержательных лозунгов, который все

517
00:21:07,120 --> 00:21:09,679
цитируют в данный момент: «Вы должны

518
00:21:09,679 --> 00:21:13,039
знать слово по компании, которую он держит,

519
00:21:13,039 --> 00:21:15,520
и поэтому идея о том, что вы можете

520
00:21:15,520 --> 00:21:16,559


521
00:21:16,559 --> 00:21:20,559
представить смысл слов, означающих как представление о том, в

522
00:21:20,559 --> 00:21:24,400
каком контексте они появляются, оказалась

523
00:21:24,400 --> 00:21:27,280
очень успешной.  идея одна из самых

524
00:21:27,280 --> 00:21:30,320
успешных идей, которая используется в

525
00:21:30,320 --> 00:21:32,880
статистике и глубоком обучении nlp,

526
00:21:32,880 --> 00:21:36,000
это на самом деле интересная идея с

527
00:21:36,000 --> 00:21:37,760
более философской точки

528
00:21:37,760 --> 00:21:38,480


529
00:21:38,480 --> 00:21:40,559
зрения, так что  Интересные

530
00:21:40,559 --> 00:21:42,480
связи, например, в

531
00:21:42,480 --> 00:21:44,799
более поздних работах Витгенштейна, он был

532
00:21:44,799 --> 00:21:47,520
очарован теорией употребления значения, и

533
00:21:47,520 --> 00:21:49,840
это в некотором смысле грех. Теория

534
00:21:49,840 --> 00:21:52,000
употребления значения, но знаете ли вы, что это

535
00:21:52,000 --> 00:21:53,840
окончательная теория семантики, это на

536
00:21:53,840 --> 00:21:56,159
самом деле все еще довольно спорно, но

537
00:21:56,159 --> 00:21:57,520
оказывается  чрезвычайно

538
00:21:57,520 --> 00:22:00,799
вычислительный смысл семантики,

539
00:22:00,799 --> 00:22:03,760
который только что привел к тому, что он

540
00:22:03,760 --> 00:22:06,480
очень успешно используется повсюду в системах глубокого обучения

541
00:22:06,480 --> 00:22:07,840
,

542
00:22:07,840 --> 00:22:11,440
поэтому, когда слово появляется в тексте, у

543
00:22:11,440 --> 00:22:13,440
него есть контекст, который представляет собой набор

544
00:22:13,440 --> 00:22:15,520
слов, которые появляются,

545
00:22:15,520 --> 00:22:18,559
и поэтому для конкретного слова мой пример

546
00:22:18,559 --> 00:22:20,159
вот банковское дело,

547
00:22:20,159 --> 00:22:22,880
мы найдем кучу мест, где

548
00:22:22,880 --> 00:22:24,720
банковское дело встречается

549
00:22:24,720 --> 00:22:27,200
в текстах, и мы соберем какие-то

550
00:22:27,200 --> 00:22:30,080
близкие слова в качестве контекстных слов, и мы увидим

551
00:22:30,080 --> 00:22:30,960


552
00:22:30,960 --> 00:22:32,880
, что те слова, которые появляются

553
00:22:32,880 --> 00:22:35,280
таким мутным коричневым цветом вокруг

554
00:22:35,280 --> 00:22:38,480
банковского дела,  эти контекстные слова в

555
00:22:38,480 --> 00:22:41,520
некотором смысле будут представлять значение

556
00:22:41,520 --> 00:22:43,200
слова «банковский»,

557
00:22:43,200 --> 00:22:45,120
пока я здесь, позвольте мне упомянуть

558
00:22:45,120 --> 00:22:47,039
одно различие, которое будет регулярно появляться,

559
00:22:47,039 --> 00:22:48,159


560
00:22:48,159 --> 00:22:51,280
когда мы  Говоря о слове um в

561
00:22:51,280 --> 00:22:54,080
нашем классе обработки естественного языка,

562
00:22:54,080 --> 00:22:57,200
у нас как бы есть два смысла слова,

563
00:22:57,200 --> 00:22:59,840
которые называются типами и

564
00:22:59,840 --> 00:23:03,039
токенами, так что есть конкретный пример

565
00:23:03,039 --> 00:23:05,760
для слова, так что в первом примере

566
00:23:05,760 --> 00:23:08,000
проблемы государственного долга превращаются

567
00:23:08,000 --> 00:23:10,000
в банковские кризисы.  банковское дело

568
00:23:10,000 --> 00:23:12,640
там, и это символ слова

569
00:23:12,640 --> 00:23:16,000
банковский, но затем я собрал

570
00:23:16,000 --> 00:23:18,880
кучу примеров цитат без кавычек слова

571
00:23:18,880 --> 00:23:21,200
банковское дело, и когда я говорю слово банковское дело

572
00:23:21,200 --> 00:23:23,760
и кучу его примеров, я тогда

573
00:23:23,760 --> 00:23:26,240
рассматриваю банковское дело как тип, который  относится

574
00:23:26,240 --> 00:23:29,039
к тому, что вы знаете, как используется и значение

575
00:23:29,039 --> 00:23:33,360
слова «банкинг» в разных случаях,

576
00:23:33,360 --> 00:23:34,880
хорошо, так

577
00:23:34,880 --> 00:23:37,360
что же, что мы собираемся делать

578
00:23:37,360 --> 00:23:38,400
с этими

579
00:23:38,400 --> 00:23:40,720


580
00:23:40,720 --> 00:23:44,080
моделями распределения языка?

581
00:23:44,080 --> 00:23:44,960


582
00:23:44,960 --> 00:23:47,679


583
00:23:47,679 --> 00:23:51,120
в качестве векторов,

584
00:23:51,120 --> 00:23:53,520
которые мы хотим

585
00:23:53,520 --> 00:23:55,279
построить плотный

586
00:23:55,279 --> 00:23:57,679
вектор с действительными значениями

587
00:23:57,679 --> 00:24:00,960
для каждого слова, который в некотором смысле

588
00:24:00,960 --> 00:24:03,679
представляет значение этого слова,

589
00:24:03,679 --> 00:24:05,600
и способ, которым все это представляет значение

590
00:24:05,600 --> 00:24:07,679
этого слова, заключается в

591
00:24:07,679 --> 00:24:10,960
том, что  is vector будет полезен

592
00:24:10,960 --> 00:24:14,000
для предсказания других слов, которые встречаются в

593
00:24:14,000 --> 00:24:15,520


594
00:24:15,520 --> 00:24:16,720
контексте этого

595
00:24:16,720 --> 00:24:19,600
um, поэтому в этом примере, чтобы держать его

596
00:24:19,600 --> 00:24:21,520
управляемым на слайдах,

597
00:24:21,520 --> 00:24:24,320
векторы только восьмимерные um,

598
00:24:24,320 --> 00:24:25,360
но

599
00:24:25,360 --> 00:24:27,600
на самом деле мы используем значительно большие

600
00:24:27,600 --> 00:24:29,360
векторы, поэтому очень распространенный размер на

601
00:24:29,360 --> 00:24:32,400
самом деле составляет 300  размерные векторы в

602
00:24:32,400 --> 00:24:35,919
порядке, поэтому для каждого слова, являющегося типом слова,

603
00:24:35,919 --> 00:24:38,720
у нас будет вектор слова,

604
00:24:38,720 --> 00:24:41,360
они также используются с другими

605
00:24:41,360 --> 00:24:42,480
именами,

606
00:24:42,480 --> 00:24:43,840
которые называются более новыми представлениями слов,

607
00:24:43,840 --> 00:24:45,360


608
00:24:45,360 --> 00:24:47,520
или по какой-то причине они станут более ясными

609
00:24:47,520 --> 00:24:49,600
на следующем слайде.  называются

610
00:24:49,600 --> 00:24:52,000
вложениями слов, поэтому теперь это

611
00:24:52,000 --> 00:24:54,240
распределенное представление, а не

612
00:24:54,240 --> 00:24:56,559
локалистическое представление, потому что

613
00:24:56,559 --> 00:25:00,480
значение банка слов распределено

614
00:25:00,480 --> 00:25:04,320
по всем 300 измерениям вектора.

615
00:25:04,320 --> 00:25:06,799
Хорошо, это называется встраиванием слов,

616
00:25:06,799 --> 00:25:09,200
потому что эффективно, когда у нас есть целая

617
00:25:09,200 --> 00:25:11,039
куча слов

618
00:25:11,039 --> 00:25:14,240
эти представления помещают их все

619
00:25:14,240 --> 00:25:16,960
в многомерное векторное пространство,

620
00:25:16,960 --> 00:25:19,600
и поэтому они встроены в это пространство,

621
00:25:19,600 --> 00:25:22,640
теперь, к сожалению, люди очень

622
00:25:22,640 --> 00:25:25,760
плохо выглядят.  ng в 300-

623
00:25:25,760 --> 00:25:28,159
мерных векторных пространствах или даже в 8-

624
00:25:28,159 --> 00:25:30,720
мерных векторных пространствах, поэтому

625
00:25:30,720 --> 00:25:32,640
единственное, что я действительно могу показать вам

626
00:25:32,640 --> 00:25:35,120
здесь, - это двумерная проекция

627
00:25:35,120 --> 00:25:38,880
этого пространства, даже если это полезно,

628
00:25:38,880 --> 00:25:40,799
но также важно понимать, что когда

629
00:25:40,799 --> 00:25:42,480
вы делаете  двумерная

630
00:25:42,480 --> 00:25:45,200
проекция 300-мерного пространства,

631
00:25:45,200 --> 00:25:47,279
вы теряете почти всю

632
00:25:47,279 --> 00:25:49,520
информацию в этом пространстве, и многие

633
00:25:49,520 --> 00:25:51,520
вещи будут раздавлены вместе, что на

634
00:25:51,520 --> 00:25:53,919
самом деле не заслуживает улучшения.

635
00:25:53,919 --> 00:25:56,640


636
00:25:56,640 --> 00:25:59,120
не могу увидеть

637
00:25:59,120 --> 00:26:01,039
ни одного из них вообще,

638
00:26:01,039 --> 00:26:04,240
но если я увеличу, а затем я увеличу

639
00:26:04,240 --> 00:26:07,360
еще больше, то вы уже увидите,

640
00:26:07,360 --> 00:26:09,360
что репрезентации, которые мы изучили

641
00:26:09,360 --> 00:26:11,440
распределенно,

642
00:26:11,440 --> 00:26:14,240
просто хорошо справляются с группировкой

643
00:26:14,240 --> 00:26:17,039
похожих

644
00:26:17,039 --> 00:26:20,000
слов, так что в этом  своего рода общая картина, которую

645
00:26:20,000 --> 00:26:22,640
я могу увеличить в одной части пространства, на

646
00:26:22,640 --> 00:26:25,200
самом деле это та часть, которая находится здесь, с этой

647
00:26:25,200 --> 00:26:28,240
точки зрения, и в ней есть слова для

648
00:26:28,240 --> 00:26:31,840
обозначения стран, так что не только страны

649
00:26:31,840 --> 00:26:34,240
обычно сгруппированы вместе.

650
00:26:34,240 --> 00:26:36,960
Различные подгруппы

651
00:26:36,960 --> 00:26:38,400
стран

652
00:26:38,400 --> 00:26:40,320
имеют определенный смысл, и

653
00:26:40,320 --> 00:26:43,279
здесь, внизу, у нас есть слова национальности, если

654
00:26:43,279 --> 00:26:45,279
мы перейдем в другую часть пространства, мы

655
00:26:45,279 --> 00:26:47,440
можем увидеть разные типы слов, так что здесь

656
00:26:47,440 --> 00:26:48,559
есть

657
00:26:48,559 --> 00:26:51,200
глаголы, и у нас есть такие, как приходить и уходить

658
00:26:51,200 --> 00:26:53,600
, очень похожи ммм

659
00:26:53,600 --> 00:26:56,159
Говорящие и думающие слова говорят думают

660
00:26:56,159 --> 00:26:59,120
ожидают чего-то похожего, и

661
00:26:59,120 --> 00:27:00,640


662
00:27:00,640 --> 00:27:02,880
рядом в правом нижнем углу у нас есть своего рода

663
00:27:02,880 --> 00:27:05,440
глагольные вспомогательные средства и связки, поэтому у нас

664
00:27:05,440 --> 00:27:08,640
есть волоски формы глагола, чтобы быть,

665
00:27:08,640 --> 00:27:11,679
а некоторые содержательные глаголы похожи

666
00:27:11,679 --> 00:27:14,159
на глаголы связок, потому что они описывают

667
00:27:14,159 --> 00:27:17,200
заявляет, что вы знаете, что он по-прежнему злился, он

668
00:27:17,200 --> 00:27:19,279
разозлился, и поэтому они на самом деле

669
00:27:19,279 --> 00:27:21,840
затем сгруппированы рядом со словом, которым

670
00:27:21,840 --> 00:27:23,679
должен быть глагол, так что в этом пространстве есть много

671
00:27:23,679 --> 00:27:26,320
интересной структуры

672
00:27:26,320 --> 00:27:29,520
um, которая затем

673
00:27:29,520 --> 00:27:31,760
представляет значение слов, поэтому алгоритм

674
00:27:31,760 --> 00:27:33,279
i '  Я собираюсь

675
00:27:33,279 --> 00:27:36,640
представить вам тот, который называется word

676
00:27:36,640 --> 00:27:39,760
to vec, который был введен Тамашом

677
00:27:39,760 --> 00:27:42,880
Микуловым и его коллегами в 2013 году

678
00:27:42,880 --> 00:27:45,039
в качестве основы для изучения векторов слов,

679
00:27:45,039 --> 00:27:46,559
и это своего рода простой и легкий  y, чтобы

680
00:27:46,559 --> 00:27:48,480
понять, с чего начать,

681
00:27:48,480 --> 00:27:50,559
поэтому идея состоит в том

682
00:27:50,559 --> 00:27:52,159
, что у нас есть

683
00:27:52,159 --> 00:27:54,480
много текста откуда-то, что мы

684
00:27:54,480 --> 00:27:56,960
обычно называем корпусом текста

685
00:27:56,960 --> 00:27:59,440
Корпус - это просто латинское слово для тела,

686
00:27:59,440 --> 00:28:02,159
поэтому это основной текст,

687
00:28:02,159 --> 00:28:05,200
и поэтому мы выбираем фиксированный словарь

688
00:28:05,200 --> 00:28:07,520
которые, как правило, будут большими, но

689
00:28:07,520 --> 00:28:09,520
тем не менее усеченными, поэтому мы избавляемся от

690
00:28:09,520 --> 00:28:11,520
некоторых действительно редких слов, чтобы мы

691
00:28:11,520 --> 00:28:13,840
могли сказать размер словарного запаса в

692
00:28:13,840 --> 00:28:16,880
четыреста тысяч, а затем мы

693
00:28:16,880 --> 00:28:20,080
создаем для себя вектор для каждого

694
00:28:20,080 --> 00:28:21,679
слова,

695
00:28:21,679 --> 00:28:24,960
хорошо, поэтому мы делаем то, что мы хотим

696
00:28:24,960 --> 00:28:28,880
выяснить, какой вектор подходит для

697
00:28:28,880 --> 00:28:31,760
каждого слова, и действительно интересно

698
00:28:31,760 --> 00:28:35,120
то, что мы можем узнать эти

699
00:28:35,120 --> 00:28:38,559
векторы слов из большой кучи текста,

700
00:28:38,559 --> 00:28:41,279
выполнив задачу распределения подобия,

701
00:28:41,279 --> 00:28:44,159
чтобы иметь возможность хорошо предсказать, какие

702
00:28:44,159 --> 00:28:47,200
слова встречаются в контексте  другими

703
00:28:47,200 --> 00:28:49,840
словами, в частности, мы собираемся

704
00:28:49,840 --> 00:28:54,000
перебирать слова в тексте, и поэтому

705
00:28:54,000 --> 00:28:56,720
в любой момент у нас есть центральное слово um

706
00:28:56,720 --> 00:28:57,440
c

707
00:28:57,440 --> 00:28:59,919
и контекстные слова за его пределами, которые

708
00:28:59,919 --> 00:29:01,600
мы назовем o,

709
00:29:01,600 --> 00:29:02,799
а затем

710
00:29:02,799 --> 00:29:06,000
на основе текущего слова vecto  rs мы

711
00:29:06,000 --> 00:29:08,559
собираемся вычислить вероятность появления

712
00:29:08,559 --> 00:29:10,480
контекстного слова при

713
00:29:10,480 --> 00:29:11,840


714
00:29:11,840 --> 00:29:14,799
данном центральном слове в соответствии с нашей

715
00:29:14,799 --> 00:29:16,240
текущей моделью,

716
00:29:16,240 --> 00:29:18,880
но тогда мы знаем, что определенные слова

717
00:29:18,880 --> 00:29:21,279
действительно встречались в контексте этого

718
00:29:21,279 --> 00:29:23,760
центрального слова, и поэтому мы

719
00:29:23,760 --> 00:29:26,480
хотим сохранить  корректировка векторов слов, чтобы

720
00:29:26,480 --> 00:29:29,840
максимизировать вероятность, которая присваивается

721
00:29:29,840 --> 00:29:32,799
словам, которые действительно встречаются в

722
00:29:32,799 --> 00:29:35,520
контексте центрального слова, когда мы проходим

723
00:29:35,520 --> 00:29:37,200
через эти тексты

724
00:29:37,200 --> 00:29:38,720
, чтобы начать делать это немного более

725
00:29:38,720 --> 00:29:42,480
конкретным, это то, что мы делаем,

726
00:29:42,480 --> 00:29:45,520
так что у нас есть  фрагмент текста, мы выбираем наше

727
00:29:45,520 --> 00:29:48,720
центральное слово, которое находится здесь, а затем

728
00:29:48,720 --> 00:29:51,919
говорим хорошо,

729
00:29:52,399 --> 00:29:55,200
если модель прогнозирования

730
00:29:55,200 --> 00:29:58,000
вероятности контекстных слов с учетом центрального слова,

731
00:29:58,000 --> 00:30:00,159
и эта модель придет в форму через минуту,

732
00:30:00,159 --> 00:30:02,320
но она определяется в терминах наших векторов слов,

733
00:30:02,320 --> 00:30:03,360


734
00:30:03,360 --> 00:30:06,480
поэтому  давайте посмотрим, какую вероятность он дает

735
00:30:06,480 --> 00:30:08,640
словам, которые на самом деле встречаются

736
00:30:08,640 --> 00:30:12,480
в контексте этого слова,

737
00:30:12,480 --> 00:30:14,960
это дает им некоторую вероятность, но, может

738
00:30:14,960 --> 00:30:16,880
быть, было бы неплохо, если бы вероятность знака

739
00:30:16,880 --> 00:30:20,080
была выше, тогда как  мы меняем

740
00:30:20,080 --> 00:30:22,000
векторы слов, чтобы повысить эти

741
00:30:22,000 --> 00:30:24,880
вероятности, и поэтому мы сделаем некоторые

742
00:30:24,880 --> 00:30:27,360
вычисления, чтобы они были центральным

743
00:30:27,360 --> 00:30:29,440
словом, а затем мы просто перейдем к

744
00:30:29,440 --> 00:30:31,919
следующему слову, а затем мы сделаем такие

745
00:30:31,919 --> 00:30:34,240
же вычисления и продолжим

746
00:30:34,240 --> 00:30:35,440
разбивать

747
00:30:35,440 --> 00:30:38,320
Итак, большой вопрос в том, что

748
00:30:38,320 --> 00:30:40,320
мы делаем для определения

749
00:30:40,320 --> 00:30:43,440
вероятности того, что слово встречается в

750
00:30:43,440 --> 00:30:46,080
контексте центрального слова,

751
00:30:46,080 --> 00:30:48,240
и это центральная часть того, что

752
00:30:48,240 --> 00:30:51,840
мы разрабатываем как объект слова,

753
00:30:52,640 --> 00:30:54,159


754
00:30:54,159 --> 00:30:56,000
так что это общая модель, которую мы

755
00:30:56,000 --> 00:30:58,480
хотим  чтобы использовать так

756
00:30:58,480 --> 00:31:01,200
для каждой позиции в нашем корпусе нашего

757
00:31:01,200 --> 00:31:02,399
тела текста,

758
00:31:02,399 --> 00:31:05,120
мы хотим предсказать контекстные слова

759
00:31:05,120 --> 00:31:07,279
в окне фиксированного размера m,

760
00:31:07,279 --> 00:31:10,000
учитывая центральное слово wj,

761
00:31:10,000 --> 00:31:12,720
и мы хотим научиться делать это,

762
00:31:12,720 --> 00:31:15,039
поэтому мы хотим дать высокую вероятность

763
00:31:15,039 --> 00:31:17,600
словам, которые  происходят в контексте,

764
00:31:17,600 --> 00:31:19,760
и поэтому мы

765
00:31:19,760 --> 00:31:21,519
собираемся выяснить, какова формальная

766
00:31:21,519 --> 00:31:25,120
вероятность данных относительно того, насколько хорошо мы

767
00:31:25,120 --> 00:31:27,360
выполняем работу по предсказанию слов в контексте

768
00:31:27,360 --> 00:31:28,960
других слов

769
00:31:28,960 --> 00:31:32,159
и так формально, что вероятность

770
00:31:32,159 --> 00:31:34,159
будет определено я  n членов наших векторов слов,

771
00:31:34,159 --> 00:31:36,000
поэтому они являются параметрами нашей

772
00:31:36,000 --> 00:31:39,600
модели, и она будет рассчитана как

773
00:31:39,600 --> 00:31:41,360
взятие произведения

774
00:31:41,360 --> 00:31:43,919
каждого слова в качестве центрального слова, а

775
00:31:43,919 --> 00:31:45,760
затем произведения каждого слова и

776
00:31:45,760 --> 00:31:48,880
окна вокруг вероятности

777
00:31:48,880 --> 00:31:51,120
предсказания  это контекстное слово

778
00:31:51,120 --> 00:31:54,080
в центральном слове,

779
00:31:54,080 --> 00:31:56,240
и поэтому для изучения этой модели у нас

780
00:31:56,240 --> 00:31:58,159
будет целевая функция, иногда

781
00:31:58,159 --> 00:32:00,559
также называемая затратами или потерями, которые мы

782
00:32:00,559 --> 00:32:03,200
хотим оптимизировать, и, по сути,

783
00:32:03,200 --> 00:32:06,799
мы хотим сделать, чтобы максимизировать

784
00:32:06,799 --> 00:32:09,279
вероятность  контекст, который мы видим вокруг

785
00:32:09,279 --> 00:32:10,720
центральных слов,

786
00:32:10,720 --> 00:32:13,200
но следуя стандартной практике, мы

787
00:32:13,200 --> 00:32:15,760
немного поправляемся,

788
00:32:15,760 --> 00:32:17,840
потому что вместо того, чтобы иметь дело с

789
00:32:17,840 --> 00:32:20,559
продуктами, легче иметь дело с суммами,

790
00:32:20,559 --> 00:32:23,200
и поэтому мы работаем с логической вероятностью, и как

791
00:32:23,200 --> 00:32:25,120
только мы берем логарифмическую вероятность, все наши

792
00:32:25,120 --> 00:32:26,080
продукты

793
00:32:26,080 --> 00:32:29,279
превращаются в суммы, мы также работаем  со

794
00:32:29,279 --> 00:32:31,919
средним логарифмическим правдоподобием, поэтому у нас есть

795
00:32:31,919 --> 00:32:34,480
термин один на t для количества

796
00:32:34,480 --> 00:32:37,440
слов в корпусе, и, наконец, без

797
00:32:37,440 --> 00:32:39,039
особой причины

798
00:32:39,039 --> 00:32:40,880
мы предпочитаем минимизировать нашу целевую

799
00:32:40,880 --> 00:32:43,200
функцию, а не t  Чтобы максимизировать ее, мы

800
00:32:43,200 --> 00:32:46,159
вставляем туда знак минус, а затем

801
00:32:46,159 --> 00:32:49,519
, минимизируя эту целевую функцию

802
00:32:49,519 --> 00:32:51,919
j теты, которая

803
00:32:51,919 --> 00:32:56,640
максимизирует нашу точность прогнозов,

804
00:32:56,640 --> 00:32:58,320
хорошо,

805
00:32:58,320 --> 00:33:01,120
так что это настройка, но мы все еще не

806
00:33:01,120 --> 00:33:05,039
добились никакого прогресса в том, как мы

807
00:33:05,039 --> 00:33:06,960
вычисляем вероятность  слово, встречающееся

808
00:33:06,960 --> 00:33:10,399
в контексте, заданном центральным словом, и,

809
00:33:10,399 --> 00:33:12,559
таким образом, мы на самом деле собираемся это сделать

810
00:33:12,559 --> 00:33:16,559
: у нас есть векторные представления

811
00:33:16,559 --> 00:33:19,679
для каждого слова, и мы собираемся вычислить

812
00:33:19,679 --> 00:33:21,440
вероятность

813
00:33:21,440 --> 00:33:24,480
просто в терминах векторов слов.

814
00:33:24,480 --> 00:33:26,320
Дело в небольшом техническом

815
00:33:26,320 --> 00:33:28,320
моменте, который мы на самом деле собираемся дать

816
00:33:28,320 --> 00:33:31,679
каждому слову два вектора слов,

817
00:33:31,679 --> 00:33:33,600
один вектор слова, когда он используется

818
00:33:33,600 --> 00:33:36,320
как центральное слово, и другой

819
00:33:36,320 --> 00:33:38,799
вектор слова, когда он используется как контекстное

820
00:33:38,799 --> 00:33:42,159
слово, это сделано, потому что это просто

821
00:33:42,159 --> 00:33:44,880
упрощает  математика и оптимизация,

822
00:33:44,880 --> 00:33:46,880
так что это кажется немного уродливым, но на

823
00:33:46,880 --> 00:33:47,919
самом деле

824
00:33:47,919 --> 00:33:51,039
значительно упрощает построение векторов слов,

825
00:33:51,039 --> 00:33:51,760
и на

826
00:33:51,760 --> 00:33:53,600
самом деле мы можем вернуться к этому и

827
00:33:53,600 --> 00:33:56,399
обсудить это позже, но это то, что это

828
00:33:56,399 --> 00:33:59,120
такое, и поэтому o  Поскольку у нас есть эти векторы слов,

829
00:33:59,120 --> 00:34:02,320


830
00:34:02,320 --> 00:34:04,720
уравнение, которое мы собираемся использовать для

831
00:34:04,720 --> 00:34:06,880
определения вероятности

832
00:34:06,880 --> 00:34:08,879
появления контекстного слова с учетом

833
00:34:08,879 --> 00:34:10,800
центрального слова, состоит в том, что мы собираемся

834
00:34:10,800 --> 00:34:13,119
вычислить его, используя выражение в

835
00:34:13,119 --> 00:34:15,760
середине внизу моего слайда,

836
00:34:15,760 --> 00:34:17,119
поэтому

837
00:34:17,119 --> 00:34:18,639
давайте  как бы

838
00:34:18,639 --> 00:34:21,760
раздвинуть это

839
00:34:21,760 --> 00:34:22,960
немного дальше,

840
00:34:22,960 --> 00:34:23,760


841
00:34:23,760 --> 00:34:25,280
так

842
00:34:25,280 --> 00:34:28,079
что то, что мы имеем здесь с этим выражением,

843
00:34:28,079 --> 00:34:31,199
таково для определенного центрального слова и

844
00:34:31,199 --> 00:34:32,560
определенного

845
00:34:32,560 --> 00:34:35,679
контекстного слова o мы собираемся

846
00:34:35,679 --> 00:34:38,239
найти векторное представление каждого слова,

847
00:34:38,239 --> 00:34:40,879
чтобы они были u  of o и v of c,

848
00:34:40,879 --> 00:34:42,719
и тогда мы просто

849
00:34:42,719 --> 00:34:46,239
возьмем скалярное произведение этих двух векторов, так что

850
00:34:46,239 --> 00:34:48,719
скалярное произведение является естественной мерой

851
00:34:48,719 --> 00:34:51,119
сходства между словами, потому что в любом

852
00:34:51,119 --> 00:34:52,320
конкретном

853
00:34:52,320 --> 00:34:53,918


854
00:34:53,918 --> 00:34:54,639


855
00:34:54,639 --> 00:34:57,359
измерении вы получите некоторый компонент,

856
00:34:57,359 --> 00:35:00,160
который добавляет  к скалярным произведениям um, если

857
00:35:00,160 --> 00:35:01,599
оба отрицательные, это добавит много к скалярному

858
00:35:01,599 --> 00:35:03,920
произведению; некоторые, если одно положительное, а другое

859
00:35:03,920 --> 00:35:07,440
отрицательное, um вычтут из этой

860
00:35:07,440 --> 00:35:09,760
меры сходства um, если оба они

861
00:35:09,760 --> 00:35:12,079
равны нулю, это не изменит сходства,

862
00:35:12,079 --> 00:35:14,640
поэтому  это вроде с  кажется правдоподобной

863
00:35:14,640 --> 00:35:17,040
идеей просто взять скалярное произведение и

864
00:35:17,040 --> 00:35:19,760
хорошо подумать, если два слова имеют более крупный

865
00:35:19,760 --> 00:35:22,400
скалярный продукт, что означает, что они более

866
00:35:22,400 --> 00:35:23,599
похожи,

867
00:35:23,599 --> 00:35:26,160
и после этого

868
00:35:26,160 --> 00:35:29,040
мы как бы действительно ничего не делаем,

869
00:35:29,040 --> 00:35:31,680
кроме как хорошо, мы хотим использовать  скалярные произведения, чтобы

870
00:35:31,680 --> 00:35:34,320
представить сходство слов, и теперь давайте

871
00:35:34,320 --> 00:35:37,599
сделаем самую глупую вещь, которую мы знаем,

872
00:35:37,599 --> 00:35:39,359
как превратить это в распределение вероятностей

873
00:35:39,359 --> 00:35:40,640


874
00:35:40,640 --> 00:35:44,160
хорошо, что у нас хорошо получается, во-первых,

875
00:35:44,160 --> 00:35:46,800
хорошо, взяв скалярное произведение двух векторов,

876
00:35:46,800 --> 00:35:48,400
которые могут быть положительными или

877
00:35:48,400 --> 00:35:51,040
отрицательными, но хорошо, если  мы хотим иметь

878
00:35:51,040 --> 00:35:53,040
вероятности, у которых не может быть отрицательных

879
00:35:53,040 --> 00:35:55,359
вероятностей, поэтому простой способ избежать

880
00:35:55,359 --> 00:35:57,280
отрицательных вероятностей -

881
00:35:57,280 --> 00:35:59,839
возвести их в степень, потому что тогда мы знаем,

882
00:35:59,839 --> 00:36:02,720
что все положительно, и поэтому мы

883
00:36:02,720 --> 00:36:04,800
всегда получаем положительное число в

884
00:36:04,800 --> 00:36:06,480
числителе,

885
00:36:06,480 --> 00:36:09,440
но для вероятностей мы также  хотите,

886
00:36:09,440 --> 00:36:11,599
чтобы числа составляли единицу, чтобы у

887
00:36:11,599 --> 00:36:13,920
нас было распределение вероятностей, поэтому мы

888
00:36:13,920 --> 00:36:16,240
просто нормализуем очевидным способом,

889
00:36:16,240 --> 00:36:19,440
когда делим на

890
00:36:19,440 --> 00:36:21,680
сумму числителя количества для ea  ch

891
00:36:21,680 --> 00:36:23,920
другое слово и словарь, и

892
00:36:23,920 --> 00:36:26,000
поэтому обязательно это дает нам

893
00:36:26,000 --> 00:36:28,640
распределение вероятностей,

894
00:36:28,640 --> 00:36:30,480
поэтому все остальное, о чем я только что

895
00:36:30,480 --> 00:36:32,079
говорил, через

896
00:36:32,079 --> 00:36:33,680
то, что мы используем, это то, что

897
00:36:33,680 --> 00:36:36,400
называется функцией softmax, поэтому функция softmax

898
00:36:36,400 --> 00:36:39,200
будет принимать любой

899
00:36:39,200 --> 00:36:41,839
вектор rn и  превратить его в вещи

900
00:36:41,839 --> 00:36:46,000
между нулем и единицей,

901
00:36:46,000 --> 00:36:48,480
чтобы мы могли взять числа и пропустить их

902
00:36:48,480 --> 00:36:50,960
через этот мягкий максимум и превратить их в

903
00:36:50,960 --> 00:36:53,839
распределение вероятностей, так что

904
00:36:53,839 --> 00:36:55,920
название происходит от того факта, что это что-

905
00:36:55,920 --> 00:36:58,800
то вроде максимума, так что из-за того факта, что

906
00:36:58,800 --> 00:37:01,119
что мы возведем в степень, что действительно

907
00:37:01,119 --> 00:37:04,880
подчеркивает большое содержание в

908
00:37:04,880 --> 00:37:08,480
разных измерениях вычисления

909
00:37:08,480 --> 00:37:11,440
сходства, поэтому большая часть вероятности

910
00:37:11,440 --> 00:37:14,880
идет на самые похожие вещи, и

911
00:37:14,880 --> 00:37:17,200
это называется мягким, потому что он не

912
00:37:17,200 --> 00:37:19,760
делает этого абсолютно, он все равно

913
00:37:19,760 --> 00:37:23,359
дает некоторую вероятность всему

914
00:37:23,359 --> 00:37:26,079
это хоть немного похоже, я

915
00:37:26,079 --> 00:37:27,760
имею в виду, с другой стороны, это немного

916
00:37:27,760 --> 00:37:30,720
странное имя, потому что вы знаете, что max обычно

917
00:37:30,720 --> 00:37:33,920
берет набор вещей и просто возвращает

918
00:37:33,920 --> 00:37:36,079
один  самый большой из них, в то время как

919
00:37:36,079 --> 00:37:39,680
softmax принимает набор чисел

920
00:37:39,680 --> 00:37:42,079
и масштабирует их, но возвращает

921
00:37:42,079 --> 00:37:45,520
все распределение вероятностей,

922
00:37:45,520 --> 00:37:48,160
хорошо, так что теперь у нас есть все части

923
00:37:48,160 --> 00:37:51,839
нашей модели, и как нам

924
00:37:51,839 --> 00:37:54,320
сделать наши векторы слов

925
00:37:54,320 --> 00:37:57,119
хорошо, представление о том, что мы хотим  Чтобы сделать

926
00:37:57,119 --> 00:37:59,440
, мы хотим

927
00:37:59,440 --> 00:38:02,880
возиться с нашими векторами слов таким образом,

928
00:38:02,880 --> 00:38:05,440
чтобы мы минимизировали наши потери i, чтобы мы

929
00:38:05,440 --> 00:38:07,920
максимизировали вероятность слов,

930
00:38:07,920 --> 00:38:10,560
которые мы действительно видели в

931
00:38:10,560 --> 00:38:13,440
контексте центрального слова, и поэтому

932
00:38:13,440 --> 00:38:16,880
тета тета представляет все параметры нашей

933
00:38:16,880 --> 00:38:20,480
модели  в одном очень длинном векторе,

934
00:38:20,480 --> 00:38:22,800
поэтому для нашей модели здесь единственными

935
00:38:22,800 --> 00:38:26,320
параметрами являются наши векторы слов, поэтому

936
00:38:26,320 --> 00:38:28,800
для каждого слова у нас есть

937
00:38:28,800 --> 00:38:31,920
два вектора, это вектор контекста и его

938
00:38:31,920 --> 00:38:34,480
центральный вектор, и каждый из них является

939
00:38:34,480 --> 00:38:37,520
вектором размерности ad, где d может быть 300,

940
00:38:37,520 --> 00:38:40,640
и у нас есть v many  слов,

941
00:38:40,640 --> 00:38:43,440
так что мы получаем этот большой огромный

942
00:38:43,440 --> 00:38:45,920
вектор длиной 2 v,

943
00:38:45,920 --> 00:38:47,440
который,

944
00:38:47,440 --> 00:38:50,079
если у вас есть 500000 слов, умноженных на

945
00:38:50,079 --> 00:38:52,960
300

946
00:38:52,960 --> 00:38:55,760
измерений времени, маленький метод, который я могу сделать в своей

947
00:38:55,760 --> 00:38:57,040
голове, но у него есть миллионы и

948
00:38:57,040 --> 00:38:58,160
миллионы параметров, поэтому  У меня миллионы и

949
00:38:58,160 --> 00:38:59,680
миллионы параметров,

950
00:38:59,680 --> 00:39:03,520
и мы как-то хотим поиграть с ними все,

951
00:39:03,520 --> 00:39:05,839
чтобы

952
00:39:05,839 --> 00:39:08,400
максимизировать предсказание контекстных слов,

953
00:39:08,400 --> 00:39:10,400
и поэтому мы собираемся это сделать, а

954
00:39:10,400 --> 00:39:12,000
затем

955
00:39:12,000 --> 00:39:14,960
используем исчисление, поэтому мы хотим

956
00:39:14,960 --> 00:39:16,480
сделать это  что мы видели

957
00:39:16,480 --> 00:39:20,800
ранее и говорим, да ладно, с этой

958
00:39:20,800 --> 00:39:25,760
целевой функцией мы можем вычислить

959
00:39:25,760 --> 00:39:28,640
производные, и поэтому мы можем определить,

960
00:39:28,640 --> 00:39:31,680
где находится градиент, чтобы мы могли спуститься с

961
00:39:31,680 --> 00:39:34,400
холма, чтобы минимизировать потери, так что мы в

962
00:39:34,400 --> 00:39:37,440
какой-то момент и можем вычислить  выяснить,

963
00:39:37,440 --> 00:39:40,839
что идет под гору, и затем мы можем

964
00:39:40,839 --> 00:39:44,720
постепенно спускаться и улучшать

965
00:39:44,720 --> 00:39:47,119
нашу модель, и наша работа будет

966
00:39:47,119 --> 00:39:50,240
заключаться в вычислении всех этих векторных

967
00:39:50,240 --> 00:39:52,400
градиентов.

968
00:39:52,400 --> 00:39:54,480


969
00:39:54,480 --> 00:39:57,599


970
00:39:57,599 --> 00:40:00,800
подробнее о том, как мы

971
00:40:00,800 --> 00:40:05,280
можем это сделать,

972
00:40:05,280 --> 00:40:06,560
и

973
00:40:06,560 --> 00:40:09,280
еще пара слайдов здесь, но,

974
00:40:09,280 --> 00:40:11,440
может быть, я просто попробую

975
00:40:11,440 --> 00:40:14,800
снова перемешать вещи и перейду к своей

976
00:40:14,800 --> 00:40:17,839
интерактивной доске, что мы хотели сделать

977
00:40:17,839 --> 00:40:18,640


978
00:40:18,640 --> 00:40:20,079
правильно, чтобы

979
00:40:20,079 --> 00:40:24,079
у нас было общее

980
00:40:24,079 --> 00:40:27,760
у нас было общее j theta  что

981
00:40:27,760 --> 00:40:30,720
мы хотели мин  представьте себе нашу среднюю

982
00:40:30,720 --> 00:40:32,880
вероятность неглогов таким образом, чтобы минус

983
00:40:32,880 --> 00:40:35,440
один на t

984
00:40:35,440 --> 00:40:36,640


985
00:40:36,640 --> 00:40:39,839
суммы t равнялся единице к большому t, который был

986
00:40:39,839 --> 00:40:42,079
длиной нашего текста, а затем мы

987
00:40:42,079 --> 00:40:44,240
просматривали слова в каждом контексте, поэтому

988
00:40:44,240 --> 00:40:45,440
мы делаем

989
00:40:45,440 --> 00:40:46,480
j

990
00:40:46,480 --> 00:40:50,839
между m словами в каждом  сторона um кроме

991
00:40:50,839 --> 00:40:54,160
самого себя um, а затем мы хотели сделать то, что

992
00:40:54,160 --> 00:40:57,040
было в той стороне, где мы были тогда,

993
00:40:57,040 --> 00:40:58,720
мы вычисляли логарифмическую

994
00:40:58,720 --> 00:41:02,319
вероятность контекстного слова

995
00:41:02,319 --> 00:41:04,079
в этой позиции

996
00:41:04,079 --> 00:41:07,680
um, учитывая слово, которое находится в центральной

997
00:41:07,680 --> 00:41:09,040
позиции t,

998
00:41:09,040 --> 00:41:12,800
и затем мы преобразовали это

999
00:41:12,800 --> 00:41:13,760
в

1000
00:41:13,760 --> 00:41:15,359


1001
00:41:15,359 --> 00:41:17,920
наши словесные векторы, сказав, что

1002
00:41:17,920 --> 00:41:19,520
вероятность

1003
00:41:19,520 --> 00:41:20,480


1004
00:41:20,480 --> 00:41:22,560
o при заданном

1005
00:41:22,560 --> 00:41:24,720
c будет выражена как

1006
00:41:24,720 --> 00:41:25,520


1007
00:41:25,520 --> 00:41:31,000
ум этот мягкий максимум скалярного произведения,

1008
00:41:44,079 --> 00:41:47,520
хорошо, и теперь мы хотим

1009
00:41:47,520 --> 00:41:49,280


1010
00:41:49,280 --> 00:41:51,200


1011
00:41:51,200 --> 00:41:55,119
разработать градиент в направлении спуска

1012
00:41:55,119 --> 00:41:56,240
для

1013
00:41:56,240 --> 00:41:57,920
этого

1014
00:41:57,920 --> 00:42:00,720
последнего  gen, и поэтому мы делаем это

1015
00:42:00,720 --> 00:42:02,160
так: мы вычисляем частную

1016
00:42:02,160 --> 00:42:03,520
производную

1017
00:42:03,520 --> 00:42:04,319


1018
00:42:04,319 --> 00:42:08,560
этого выражения

1019
00:42:08,560 --> 00:42:12,160
по каждому параметру в модели, и

1020
00:42:12,160 --> 00:42:15,119
все параметры в модели являются

1021
00:42:15,119 --> 00:42:17,760
компонентами, размерностями

1022
00:42:17,760 --> 00:42:21,280
векторов слов каждого слова  Итак, у нас есть

1023
00:42:21,280 --> 00:42:24,079
векторы центрального слова и

1024
00:42:24,079 --> 00:42:26,160
векторы внешних слов,

1025
00:42:26,160 --> 00:42:29,760
так что я просто собираюсь сделать

1026
00:42:29,760 --> 00:42:30,800


1027
00:42:30,800 --> 00:42:33,280
векторы центрального слова,

1028
00:42:33,280 --> 00:42:35,280
но при выполнении домашнего задания по будущему домашнему

1029
00:42:35,280 --> 00:42:37,040
заданию

1030
00:42:37,040 --> 00:42:39,599
2 появятся векторы внешних слов,

1031
00:42:39,599 --> 00:42:42,079
и они вроде как  аналогично, так что

1032
00:42:42,079 --> 00:42:44,319
мы делаем то, что мы разрабатываем

1033
00:42:44,319 --> 00:42:47,200
частную производную

1034
00:42:47,200 --> 00:42:49,760
по отношению к нашему центральному вектору слова,

1035
00:42:49,760 --> 00:42:51,280
который, как вы знаете, может быть 300-

1036
00:42:51,280 --> 00:42:54,640
мерный вектор слова

1037
00:42:54,640 --> 00:42:56,560


1038
00:42:56,560 --> 00:43:00,319
этой вероятности o дать c um, и

1039
00:43:00,319 --> 00:43:02,079
поскольку мы используем логарифмические вероятности

1040
00:43:02,079 --> 00:43:05,280
логарифм этой вероятности o при данном

1041
00:43:05,280 --> 00:43:06,960
c этого x из

1042
00:43:06,960 --> 00:43:09,440
u otvc

1043
00:43:09,440 --> 00:43:10,800
по

1044
00:43:10,800 --> 00:43:12,480
моему письму будет становиться все хуже и хуже,

1045
00:43:12,480 --> 00:43:15,119
извините, я уже сделал ошибку

1046
00:43:15,119 --> 00:43:16,560
, разве я не

1047
00:43:16,560 --> 00:43:19,359
сумел сумма w равна единице

1048
00:43:19,359 --> 00:43:21,800
словарю  эксперт

1049
00:43:21,800 --> 00:43:23,440
uwt

1050
00:43:23,440 --> 00:43:25,119
vc

1051
00:43:25,119 --> 00:43:26,800
хорошо,

1052
00:43:26,800 --> 00:43:29,680
ну в этот момент все начинается

1053
00:43:29,680 --> 00:43:33,280
довольно легко, так что у нас есть

1054
00:43:33,280 --> 00:43:36,240
что-то, что логарифмически превышает b, так

1055
00:43:36,240 --> 00:43:39,280
что это легко, мы можем превратить это в журнал a

1056
00:43:39,280 --> 00:43:41,119
минус log b,

1057
00:43:41,119 --> 00:43:43,040
но прежде чем я пойду дальше, я просто  Сделайте

1058
00:43:43,040 --> 00:43:45,680
комментарий в этом месте,

1059
00:43:45,680 --> 00:43:49,680
вы знаете, что в этом письме  Моя аудитория делится на две части.

1060
00:43:49,680 --> 00:43:51,839


1061
00:43:51,839 --> 00:43:53,680
В аудитории есть несколько человек,

1062
00:43:53,680 --> 00:43:56,400
для которых, возможно, много

1063
00:43:56,400 --> 00:43:57,640
людей

1064
00:43:57,640 --> 00:43:58,880
[Музыка]

1065
00:43:58,880 --> 00:44:01,280
ах, это действительно элементарная математика,

1066
00:44:01,280 --> 00:44:03,280
я видел это миллион раз раньше,

1067
00:44:03,280 --> 00:44:04,880
и он даже не объясняет этого  очень

1068
00:44:04,880 --> 00:44:06,560
хорошо,

1069
00:44:06,560 --> 00:44:08,960
и если вы в этой группе, не

1070
00:44:08,960 --> 00:44:11,280
стесняйтесь смотреть свою электронную почту или

1071
00:44:11,280 --> 00:44:13,680
газету, или что-то еще, что лучше

1072
00:44:13,680 --> 00:44:16,640
всего подходит для вас, но я думаю, что

1073
00:44:16,640 --> 00:44:19,839
в классе также есть другие люди, которых

1074
00:44:19,839 --> 00:44:21,839
я видел в последний раз  исчисление было, когда я учился

1075
00:44:21,839 --> 00:44:23,359
в старшей школе,

1076
00:44:23,359 --> 00:44:25,680
для чего это не так, и поэтому я

1077
00:44:25,680 --> 00:44:28,240
хотел потратить несколько минут на

1078
00:44:28,240 --> 00:44:30,960


1079
00:44:30,960 --> 00:44:33,440
то, чтобы разобраться с этим немного конкретно, чтобы попытаться преодолеть идею,

1080
00:44:33,440 --> 00:44:35,599
которую вы знаете, хотя

1081
00:44:35,599 --> 00:44:36,880


1082
00:44:36,880 --> 00:44:38,960
большая часть глубокого обучения и даже  словесное

1083
00:44:38,960 --> 00:44:42,000
векторное обучение кажется магией,

1084
00:44:42,000 --> 00:44:45,839
что на самом деле это не волшебство, на самом деле

1085
00:44:45,839 --> 00:44:48,480
это просто математика, и одна из вещей, на которую мы

1086
00:44:48,480 --> 00:44:50,640
надеемся, это то, что вы действительно понимаете

1087
00:44:50,640 --> 00:44:53,119
эту математику, которая выполняется,

1088
00:44:53,119 --> 00:44:55,200
поэтому я буду продолжать и делать немного больше

1089
00:44:55,200 --> 00:44:58,880
, хорошо, так что  то у нас

1090
00:44:58,880 --> 00:45:01,520
есть как бы использовать этот w  ay

1091
00:45:01,520 --> 00:45:03,119
записи журнала,

1092
00:45:03,119 --> 00:45:05,359
и тогда мы можем сказать, что это

1093
00:45:05,359 --> 00:45:08,160
выражение выше равняется частным

1094
00:45:08,160 --> 00:45:10,960
производным с

1095
00:45:10,960 --> 00:45:12,400
of vc

1096
00:45:12,400 --> 00:45:13,839


1097
00:45:13,839 --> 00:45:14,640


1098
00:45:14,640 --> 00:45:17,119
журнала числителя

1099
00:45:17,119 --> 00:45:21,040
log xuotvc

1100
00:45:21,040 --> 00:45:22,480
минус

1101
00:45:22,480 --> 00:45:24,160


1102
00:45:24,160 --> 00:45:27,440
um частной

1103
00:45:28,880 --> 00:45:30,800
производной журнала

1104
00:45:30,800 --> 00:45:32,599


1105
00:45:32,599 --> 00:45:36,319
знаменателя, так что тогда

1106
00:45:36,319 --> 00:45:38,880
сумма w равна  От 1 до v

1107
00:45:38,880 --> 00:45:40,240
от

1108
00:45:40,240 --> 00:45:41,440
x uwtvc

1109
00:45:41,440 --> 00:45:43,520


1110
00:45:43,520 --> 00:45:44,480


1111
00:45:44,480 --> 00:45:45,440


1112
00:45:45,440 --> 00:45:48,800
хорошо, так что в этот момент у меня есть

1113
00:45:48,800 --> 00:45:51,119


1114
00:45:51,119 --> 00:45:54,480
мой числитель здесь и мой бывший

1115
00:45:54,480 --> 00:45:56,319
знаменатель там,

1116
00:45:56,319 --> 00:45:58,800
так что в этот момент

1117
00:45:58,800 --> 00:46:02,160
есть начало, первая часть - это

1118
00:46:02,160 --> 00:46:05,359
часть числителя, поэтому часть числителя

1119
00:46:05,359 --> 00:46:06,960
действительно очень проста,

1120
00:46:06,960 --> 00:46:07,680


1121
00:46:07,680 --> 00:46:10,800
поэтому мы  здесь

1122
00:46:10,800 --> 00:46:13,280
есть журнал и x, но просто инвертируют друг

1123
00:46:13,280 --> 00:46:16,720
друга, поэтому они просто уходят, так что это

1124
00:46:16,720 --> 00:46:17,839
становится

1125
00:46:17,839 --> 00:46:18,960


1126
00:46:18,960 --> 00:46:21,280


1127
00:46:21,280 --> 00:46:23,599
производной по

1128
00:46:23,599 --> 00:46:25,839
vc

1129
00:46:25,839 --> 00:46:27,359
от

1130
00:46:27,359 --> 00:46:30,079
того, что осталось позади, это

1131
00:46:30,079 --> 00:46:33,680
точечный продукт u0 и с vc,

1132
00:46:33,680 --> 00:46:37,280
хорошо, и поэтому то, о чем нужно знать

1133
00:46:37,280 --> 00:46:38,880
Вы знаете, что мы все еще делаем это

1134
00:46:38,880 --> 00:46:42,160
многомерное исчисление, поэтому то, что у нас есть,

1135
00:46:42,160 --> 00:46:44,640
это исчисление относительно

1136
00:46:44,640 --> 00:46:47,280
вектора, как, надеюсь, вы видели кое-что из

1137
00:46:47,280 --> 00:46:50,800
математики 51 или в другом месте, а не в средней

1138
00:46:50,800 --> 00:46:54,640
школе эм вычисление одной переменной  мы,

1139
00:46:54,640 --> 00:46:58,319
с другой стороны, вы знаете, насколько

1140
00:46:58,319 --> 00:47:01,040
вы и наполовину помните некоторые из этих вещей,

1141
00:47:01,040 --> 00:47:03,359
большую часть времени вы можете просто

1142
00:47:03,359 --> 00:47:05,680
отлично справляться, думая о том, что

1143
00:47:05,680 --> 00:47:08,640
происходит с одним

1144
00:47:08,640 --> 00:47:11,520
измерением за раз, и это

1145
00:47:11,520 --> 00:47:14,480
обобщает многомерное исчисление, поэтому, если  о

1146
00:47:14,480 --> 00:47:17,040
гм все, что вы помните об исчислении, это

1147
00:47:17,040 --> 00:47:21,760
то, что d dx of ax равно a, на

1148
00:47:21,760 --> 00:47:22,960
самом деле

1149
00:47:22,960 --> 00:47:24,640
это то же самое, что мы

1150
00:47:24,640 --> 00:47:29,599
собираемся использовать здесь, здесь у нас

1151
00:47:29,599 --> 00:47:31,760


1152
00:47:33,200 --> 00:47:35,440
есть внешнее слово

1153
00:47:35,440 --> 00:47:37,040
dot, созданное

1154
00:47:37,040 --> 00:47:40,160
с помощью vc well в конце дня

1155
00:47:40,160 --> 00:47:44,319
в нем будут термины типа компонента u0,

1156
00:47:44,319 --> 00:47:47,280
умноженного

1157
00:47:47,280 --> 00:47:50,960
на компонент 1 центрального слова, плюс

1158
00:47:50,960 --> 00:47:54,559
u um нулевой компонент два

1159
00:47:54,559 --> 00:47:55,680
плюс

1160
00:47:55,680 --> 00:47:58,000
um,

1161
00:47:58,000 --> 00:48:01,359
это был компонент два, и поэтому мы

1162
00:48:01,359 --> 00:48:04,000
вроде как используем этот бит здесь, и так

1163
00:48:04,000 --> 00:48:05,520
что мы будем  выход -

1164
00:48:05,520 --> 00:48:07,680
это u0,

1165
00:48:07,680 --> 00:48:11,200
u01 и u0 2,

1166
00:48:11,200 --> 00:48:13,760
так что это будет все, что останется по

1167
00:48:13,760 --> 00:48:16,160
отношению к vc1, когда мы возьмем его

1168
00:48:16,160 --> 00:48:18,720
производную по vc1, и этот

1169
00:48:18,720 --> 00:48:20,800
член будет единственным, что останется, когда мы

1170
00:48:20,800 --> 00:48:23,040
возьмем производную по

1171
00:48:23,040 --> 00:48:25,520
переменная um vc2,

1172
00:48:25,520 --> 00:48:29,440
так что конец re  В результате

1173
00:48:29,440 --> 00:48:34,319
взятия векторной производной от

1174
00:48:34,319 --> 00:48:37,520
скалярного произведения u0 и с vc просто

1175
00:48:37,520 --> 00:48:40,559
будет u0,

1176
00:48:40,559 --> 00:48:42,400
хорошо, отлично,

1177
00:48:42,400 --> 00:48:44,880
так что прогресс,

1178
00:48:44,880 --> 00:48:47,760
так что в этот момент

1179
00:48:47,760 --> 00:48:51,599
мы продолжаем и говорим, черт возьми, у нас все еще

1180
00:48:51,599 --> 00:48:52,800


1181
00:48:52,800 --> 00:48:54,960


1182
00:48:55,359 --> 00:48:57,760
есть знаменатель,

1183
00:48:57,760 --> 00:49:01,359
и это немного сложнее, но

1184
00:49:01,359 --> 00:49:03,440
не так уж и плохо, поэтому мы хотим взять

1185
00:49:03,440 --> 00:49:06,240
частные производные по vc

1186
00:49:06,240 --> 00:49:11,400
из журнала знаменателя,

1187
00:49:18,640 --> 00:49:22,880
хорошо, и тогда на этом этапе

1188
00:49:22,880 --> 00:49:25,119
единственный инструмент, который нам нужно знать и

1189
00:49:25,119 --> 00:49:29,599
помнить, - это как использовать правило цепочки,

1190
00:49:29,599 --> 00:49:31,760
чтобы цепочка  Правило - это когда вы хотите

1191
00:49:31,760 --> 00:49:34,960


1192
00:49:35,119 --> 00:49:38,480
разработать производные от композиций

1193
00:49:38,480 --> 00:49:41,920
функций, чтобы у нас было f of g

1194
00:49:41,920 --> 00:49:43,680


1195
00:49:43,680 --> 00:49:46,480
любого x, но здесь это будет vc,

1196
00:49:46,480 --> 00:49:50,079
и поэтому мы хотим сказать, хорошо, что у

1197
00:49:50,079 --> 00:49:52,559
нас есть, мы работаем  из

1198
00:49:52,559 --> 00:49:56,160
композиции функций, так что вот наш f

1199
00:49:56,160 --> 00:49:58,800
um, а вот

1200
00:49:58,800 --> 00:49:59,760
наш

1201
00:49:59,760 --> 00:50:00,800
x,

1202
00:50:00,800 --> 00:50:02,400
который является

1203
00:50:02,400 --> 00:50:04,960
g of vc, на

1204
00:50:04,960 --> 00:50:07,280
самом деле, возможно, мне не следует называть его x

1205
00:50:07,280 --> 00:50:09,040
um

1206
00:50:09,040 --> 00:50:11,359
oops,

1207
00:50:11,440 --> 00:50:12,800
может быть, я

1208
00:50:12,800 --> 00:50:14,319
был, вероятно, лучше называть его z или

1209
00:50:14,319 --> 00:50:17,040
что-то в этом роде, ну

1210
00:50:17,040 --> 00:50:19,599
ладно, поэтому, когда мы тогда

1211
00:50:19,599 --> 00:50:20,640
захотим  хорошо

1212
00:50:20,640 --> 00:50:22,640
отработать

1213
00:50:22,640 --> 00:50:25,839
правило цепочки, что мы делаем

1214
00:50:25,839 --> 00:50:29,200
мы берем производную f

1215
00:50:29,200 --> 00:50:31,599
в точке z,

1216
00:50:31,599 --> 00:50:33,920
и поэтому в этой точке мы должны фактически

1217
00:50:33,920 --> 00:50:35,440
вспомнить то, что мы должны помнить,

1218
00:50:35,440 --> 00:50:38,160
что производная log - это одна из

1219
00:50:38,160 --> 00:50:41,119
функции x, поэтому она будет равна

1220
00:50:41,119 --> 00:50:42,559


1221
00:50:42,559 --> 00:50:43,599


1222
00:50:43,599 --> 00:50:46,079
1 на x

1223
00:50:46,079 --> 00:50:47,440
для

1224
00:50:47,440 --> 00:50:50,319
z  так что тогда будет 1 по

1225
00:50:50,319 --> 00:50:53,520
сумме w, равной от 1 до v

1226
00:50:53,520 --> 00:50:54,960
из x

1227
00:50:54,960 --> 00:50:56,240
от

1228
00:50:56,240 --> 00:50:57,200
utvc,

1229
00:50:57,200 --> 00:50:58,079


1230
00:50:58,079 --> 00:50:59,599


1231
00:50:59,599 --> 00:51:02,240
умноженного на

1232
00:51:02,240 --> 00:51:03,760


1233
00:51:03,760 --> 00:51:04,640


1234
00:51:04,640 --> 00:51:07,520
производную внутренней функции,

1235
00:51:07,520 --> 00:51:10,319
так что производная

1236
00:51:10,319 --> 00:51:11,119


1237
00:51:11,119 --> 00:51:12,000


1238
00:51:12,000 --> 00:51:13,760
um - часть

1239
00:51:13,760 --> 00:51:15,760
, которая осталась,

1240
00:51:15,760 --> 00:51:18,480
я надеюсь, что я правильно понял сумму

1241
00:51:18,480 --> 00:51:19,440


1242
00:51:19,440 --> 00:51:21,440
о, и здесь есть один трюк, в этот

1243
00:51:21,440 --> 00:51:23,680
момент мы действительно хотим изменить

1244
00:51:23,680 --> 00:51:26,000
индекс, поэтому мы хотим сказать, что сумма x

1245
00:51:26,000 --> 00:51:27,599
равна 1 к v

1246
00:51:27,599 --> 00:51:29,040
из x

1247
00:51:29,040 --> 00:51:31,440
из u из xvc,

1248
00:51:31,440 --> 00:51:32,880


1249
00:51:32,880 --> 00:51:34,079
так как мы

1250
00:51:34,079 --> 00:51:37,520
можем столкнуться с проблемами, если не

1251
00:51:37,520 --> 00:51:40,480
изменим это  переменная um, чтобы использовать

1252
00:51:40,480 --> 00:51:43,119
другую,

1253
00:51:43,119 --> 00:51:46,400
хорошо, поэтому на этом этапе мы добиваемся некоторого

1254
00:51:46,400 --> 00:51:47,760
прогресса,

1255
00:51:47,760 --> 00:51:49,200
но мы все еще хотим выработать

1256
00:51:49,200 --> 00:51:51,680
производную от этой переменной, и поэтому мы

1257
00:51:51,680 --> 00:51:54,720
хотим еще раз применить правило цепочки,

1258
00:51:54,720 --> 00:51:55,760
так что теперь

1259
00:51:55,760 --> 00:52:00,000
вот наши f и  здесь наш новый z

1260
00:52:00,000 --> 00:52:03,359
равен g из vc

1261
00:52:03,839 --> 00:52:06,559
, поэтому

1262
00:52:06,559 --> 00:52:10,240
мы как бы повторяем, так что c

1263
00:52:10,240 --> 00:52:12,960
Переместите производную um

1264
00:52:12,960 --> 00:52:15,839
внутри uh some всегда,

1265
00:52:15,839 --> 00:52:20,640
поэтому мы берем производную

1266
00:52:22,960 --> 00:52:25,599
от этого,

1267
00:52:26,400 --> 00:52:27,440
и

1268
00:52:27,440 --> 00:52:28,800


1269
00:52:28,800 --> 00:52:31,200
тогда производная

1270
00:52:31,200 --> 00:52:32,559
x сама по себе в

1271
00:52:32,559 --> 00:52:35,359
порядке, поэтому мы собираемся просто иметь x, равное uxtvc,

1272
00:52:35,359 --> 00:52:37,599


1273
00:52:37,599 --> 00:52:39,040
умноженное на

1274
00:52:39,040 --> 00:52:40,480
um,

1275
00:52:40,480 --> 00:52:43,119
там сумма x равна 1, чтобы  v

1276
00:52:43,119 --> 00:52:44,800
умножить

1277
00:52:44,800 --> 00:52:47,839
на производную

1278
00:52:48,160 --> 00:52:50,240
um от uxtvc,

1279
00:52:50,240 --> 00:52:52,319


1280
00:52:52,319 --> 00:52:54,640
хорошо, и тогда это то, что мы

1281
00:52:54,640 --> 00:52:57,680
разработали, прежде чем

1282
00:52:58,960 --> 00:53:02,000
мы сможем просто переписать как ux,

1283
00:53:02,000 --> 00:53:02,880
хорошо

1284
00:53:02,880 --> 00:53:05,200
, теперь мы делаем успехи,

1285
00:53:05,200 --> 00:53:06,240


1286
00:53:06,240 --> 00:53:07,040
так что

1287
00:53:07,040 --> 00:53:10,800
если мы начнем собирать все это вместе,

1288
00:53:10,800 --> 00:53:13,040
то, что у нас есть,

1289
00:53:13,040 --> 00:53:15,839
производная

1290
00:53:15,839 --> 00:53:19,040
или частные производные с vc

1291
00:53:19,040 --> 00:53:22,319
этой логарифмической вероятности,

1292
00:53:22,960 --> 00:53:24,880
верно, у нас есть числитель, который был

1293
00:53:24,880 --> 00:53:26,640
просто u0

1294
00:53:26,640 --> 00:53:28,559
um минус

1295
00:53:28,559 --> 00:53:32,240
um, тогда у нас была сумма

1296
00:53:32,240 --> 00:53:35,200
суммы числителя по x, равной от

1297
00:53:35,200 --> 00:53:36,640
1 до v

1298
00:53:36,640 --> 00:53:38,000
от xuxt

1299
00:53:38,000 --> 00:53:39,520


1300
00:53:39,520 --> 00:53:42,720
dc, умноженного на u от x,

1301
00:53:42,720 --> 00:53:43,680
затем,

1302
00:53:43,680 --> 00:53:46,640
которая была умножена на  наш первый член,

1303
00:53:46,640 --> 00:53:48,880
который произошел от члена на x, который дает

1304
00:53:48,880 --> 00:53:53,280
вам сумму w, равную единице к v

1305
00:53:53,280 --> 00:53:54,960


1306
00:53:54,960 --> 00:53:58,079
x uwtvc,

1307
00:53:58,079 --> 00:54:00,079


1308
00:54:00,079 --> 00:54:01,920
и это тот факт, что мы изменили

1309
00:54:01,920 --> 00:54:05,680
переменные um, стало важным, и поэтому

1310
00:54:05,680 --> 00:54:09,680
просто переписав это немного um

1311
00:54:09,680 --> 00:54:12,240
мы можем получить, что это равно u  0

1312
00:54:12,240 --> 00:54:13,440
минус

1313
00:54:13,440 --> 00:54:14,800
um

1314
00:54:14,800 --> 00:54:19,280
сумма v равна ой, извините,

1315
00:54:23,680 --> 00:54:25,680
x хорошо, x равно

1316
00:54:25,680 --> 00:54:26,880
1 к

1317
00:54:26,880 --> 00:54:27,760
v

1318
00:54:27,760 --> 00:54:28,640


1319
00:54:28,640 --> 00:54:32,079
этого x.

1320
00:54:32,079 --> 00:54:33,040


1321
00:54:33,040 --> 00:54:34,079


1322
00:54:34,079 --> 00:54:36,480


1323
00:54:36,480 --> 00:54:40,720


1324
00:54:40,720 --> 00:54:42,960


1325
00:54:42,960 --> 00:54:44,559


1326
00:54:44,559 --> 00:54:47,760
случилось так, что

1327
00:54:47,760 --> 00:54:50,319
мы закончили тем, что получили

1328
00:54:50,319 --> 00:54:53,359
точно ту же вероятность формулы мягкого максимума,

1329
00:54:53,359 --> 00:54:54,799
которую мы видели,

1330
00:54:54,799 --> 00:54:58,000
когда мы начали,

1331
00:54:58,000 --> 00:54:59,839
мы можем просто переписать это для более

1332
00:54:59,839 --> 00:55:04,079
удобного использования, сказав, что это равно u0

1333
00:55:04,079 --> 00:55:08,079
минус сумма по x равна от 1 до

1334
00:55:08,079 --> 00:55:11,839
v вероятности x  учитывая c,

1335
00:55:11,839 --> 00:55:14,079
умноженное на ux,

1336
00:55:14,079 --> 00:55:16,880
и то, что у нас есть в

1337
00:55:16,880 --> 00:55:21,359
данный момент, это ожидание,

1338
00:55:21,359 --> 00:55:25,119
и поэтому это среднее значение по

1339
00:55:25,119 --> 00:55:27,839
всем векторам контекста, взвешенным по их

1340
00:55:27,839 --> 00:55:30,640
вероятности в соответствии с моделью, и

1341
00:55:30,640 --> 00:55:32,559
так всегда бывает

1342
00:55:32,559 --> 00:55:34,559
с этими моделями стиля softmax  что

1343
00:55:34,559 --> 00:55:36,720
то, что вы получаете для производных, это то, что

1344
00:55:36,720 --> 00:55:40,079
вы получаете наблюдаемое um

1345
00:55:40,079 --> 00:55:41,040


1346
00:55:41,040 --> 00:55:45,680
минус ожидаемое, поэтому наша модель хороша,

1347
00:55:45,680 --> 00:55:49,440
если наша модель в среднем предсказывает точно

1348
00:55:49,440 --> 00:55:53,440
um вектор слов, который мы на самом деле видим,

1349
00:55:53,440 --> 00:55:55,599
и поэтому мы собираемся попытаться скорректировать

1350
00:55:55,599 --> 00:55:57,520
номинал  параметры нашей модели,

1351
00:55:57,520 --> 00:55:58,559
так что

1352
00:55:58,559 --> 00:56:01,280
она делает именно это,

1353
00:56:01,280 --> 00:56:04,000
теперь я имею в виду, что

1354
00:56:04,000 --> 00:56:06,000
мы пытаемся сделать это как

1355
00:56:06,000 --> 00:56:08,960
можно больше, я имею в виду, конечно, поскольку вы обнаружите,

1356
00:56:08,960 --> 00:56:11,680
что никогда не сможете приблизиться, вы знаете,

1357
00:56:11,680 --> 00:56:14,000
если я просто скажу вам, хорошо

1358
00:56:14,000 --> 00:56:16,720
слово круассан, какие слова могут

1359
00:56:16,720 --> 00:56:19,040
встречаться в контексте

1360
00:56:19,040 --> 00:56:21,359
круассана, я имею в виду, что вы не можете ответить, что

1361
00:56:21,359 --> 00:56:22,960
есть всевозможные предложения, которые

1362
00:56:22,960 --> 00:56:24,799
вы могли бы сказать, которые включают слово

1363
00:56:24,799 --> 00:56:27,440
круассан, поэтому на самом деле наши конкретные

1364
00:56:27,440 --> 00:56:29,760
оценки вероятности будут

1365
00:56:29,760 --> 00:56:32,160
добрыми  маленьких, но,

1366
00:56:32,160 --> 00:56:34,880
тем не менее, мы хотим поиграть с

1367
00:56:34,880 --> 00:56:38,319
нашими векторами слов, чтобы попытаться сделать эти

1368
00:56:38,319 --> 00:56:41,520
оценки настолько высокими, насколько это возможно,

1369
00:56:41,520 --> 00:56:42,880
поэтому

1370
00:56:42,880 --> 00:56:45,920
я немного рассказал об этом,

1371
00:56:45,920 --> 00:56:46,960


1372
00:56:46,960 --> 00:56:49,119
но на самом деле не

1373
00:56:49,119 --> 00:56:51,839
показал вам ничего из того, что на самом деле

1374
00:56:51,839 --> 00:56:55,760
Случилось извините, я просто хочу быстро показать

1375
00:56:55,760 --> 00:56:56,799


1376
00:56:56,799 --> 00:56:58,559
вам,

1377
00:56:58,559 --> 00:57:00,640
что на самом деле происходит с векторами слов.

1378
00:57:00,640 --> 00:57:03,680


1379
00:57:03,680 --> 00:57:05,520


1380
00:57:05,520 --> 00:57:07,280


1381
00:57:07,280 --> 00:57:10,000


1382
00:57:10,000 --> 00:57:13,680
импортировать кучу вещей

1383
00:57:13,680 --> 00:57:16,880
гм, так что у нас есть numpy для наших векторов,

1384
00:57:16,880 --> 00:57:18,799
matplotlib

1385
00:57:18,799 --> 00:57:21,839
строит график, он изучает вид вашего

1386
00:57:21,839 --> 00:57:24,720
машинного обучения um swiss army knife

1387
00:57:24,720 --> 00:57:26,559
gensim - это пакет, который вы, возможно,

1388
00:57:26,559 --> 00:57:28,799
не видели, прежде чем это пакет,

1389
00:57:28,799 --> 00:57:30,880
который часто используется для векторов слов, для которых он на

1390
00:57:30,880 --> 00:57:32,559
самом деле не

1391
00:57:32,559 --> 00:57:34,400
используется  глубокое обучение, так что это

1392
00:57:34,400 --> 00:57:36,319
единственный раз, когда вы увидите его в классе, но

1393
00:57:36,319 --> 00:57:37,680
если вам просто нужен хороший пакет для

1394
00:57:37,680 --> 00:57:40,000
работы с векторами слов и какое-то другое

1395
00:57:40,000 --> 00:57:42,000
приложение,

1396
00:57:42,000 --> 00:57:43,760
хорошо бы знать о

1397
00:57:43,760 --> 00:57:47,760
порядке, поэтому во второй ячейке здесь

1398
00:57:47,760 --> 00:57:49,599
я  я

1399
00:57:49,599 --> 00:57:51,520
загружаю определенный набор векторов слов, так

1400
00:57:51,520 --> 00:57:54,240
что это наши векторы слов в перчатках, которые мы

1401
00:57:54,240 --> 00:57:57,440
сделали в Стэнфорде в 2014 году,

1402
00:57:57,440 --> 00:57:59,359
и я загружаю сотни размерных

1403
00:57:59,359 --> 00:58:02,000
векторов слов, ммм, чтобы все было

1404
00:58:02,000 --> 00:58:04,720
немного быстрее для меня, пока я

1405
00:58:04,720 --> 00:58:07,599
что-то делаю  вот вроде как сделать эту

1406
00:58:07,599 --> 00:58:11,119
модель хлеба и круассана, ну,

1407
00:58:11,119 --> 00:58:14,240
что я только что получил, это векторы слов,

1408
00:58:14,240 --> 00:58:16,960
так что я просто хотел как бы

1409
00:58:16,960 --> 00:58:21,680
показать вам, что

1410
00:58:21,680 --> 00:58:24,079


1411
00:58:25,440 --> 00:58:27,520


1412
00:58:27,520 --> 00:58:29,040
есть векторы слов

1413
00:58:29,040 --> 00:58:32,559
заранее хм

1414
00:58:32,960 --> 00:58:36,280
давай

1415
00:58:42,480 --> 00:58:46,000
хорошо, хорошо, я в деле,

1416
00:58:46,000 --> 00:58:49,040
хорошо, так хорошо, вот мои

1417
00:58:49,040 --> 00:58:53,599
векторы слов для хлеба и круассана,

1418
00:58:53,599 --> 00:58:55,280
и пока и вижу, что, возможно, эти

1419
00:58:55,280 --> 00:58:57,359
два слова немного похожи, поэтому оба

1420
00:58:57,359 --> 00:58:59,359
они отрицательны в первом измерении

1421
00:58:59,359 --> 00:59:00,960
положительно, а  второй отрицательный в

1422
00:59:00,960 --> 00:59:02,240
третьем

1423
00:59:02,240 --> 00:59:04,240
положительном, четвертый отрицательный и

1424
00:59:04,240 --> 00:59:06,160
пятый, так что похоже, что у них

1425
00:59:06,160 --> 00:59:07,760
может быть довольно много точечного произведения,

1426
00:59:07,760 --> 00:59:09,280
что отчасти то, что мы хотим, потому что

1427
00:59:09,280 --> 00:59:11,760
хлеб и круассан отчасти похожи,

1428
00:59:11,760 --> 00:59:14,319
но что мы можем сделать, так это  на самом деле

1429
00:59:14,319 --> 00:59:16,079
спросите модель, и это функции gen sim,

1430
00:59:16,079 --> 00:59:19,599
теперь вы знаете, какие слова наиболее

1431
00:59:19,599 --> 00:59:22,720
похожи, поэтому я могу спросить круассан,

1432
00:59:22,720 --> 00:59:24,160
какие

1433
00:59:24,160 --> 00:59:26,799
слова наиболее похожи на это, и

1434
00:59:26,799 --> 00:59:28,799
он скажет мне, что это такие вещи, как бриошь,

1435
00:59:28,799 --> 00:59:31,839
багет фокачча, так что это довольно хороший

1436
00:59:31,839 --> 00:59:33,359
пудинг  Возможно, немного более

1437
00:59:33,359 --> 00:59:34,640
сомнительно,

1438
00:59:34,640 --> 00:59:36,480
мы можем сказать, что

1439
00:59:36,480 --> 00:59:39,520
больше всего похож на США, и в нем говорится, что

1440
00:59:39,520 --> 00:59:42,640
Канада Америка США с периодами Соединенные

1441
00:59:42,640 --> 00:59:44,799
Штаты это довольно хорошо

1442
00:59:44,799 --> 00:59:46,799
больше всего похоже на банан

1443
00:59:46,799 --> 00:59:51,200
эм я достаю кокосы манго бананы

1444
00:59:51,200 --> 00:59:53,119
сорт  довольно тропический,

1445
00:59:53,119 --> 00:59:55,359
очень великий ум,

1446
00:59:55,359 --> 00:59:57,760
прежде чем закончить, хотя я хочу

1447
00:59:57,760 --> 01:00:00,000
показать вам кое-что немного большее, чем

1448
01:00:00,000 --> 01:00:01,920
просто сходство, которое является одной из

1449
01:00:01,920 --> 01:00:04,480
удивительных вещей, которые люди наблюдали с

1450
01:00:04,480 --> 01:00:07,440
этими векторами слов, и это означает, что

1451
01:00:07,440 --> 01:00:10,160
вы действительно можете выполнять арифметические операции

1452
01:00:10,160 --> 01:00:13,040
с этим вектором  пространство, которое имеет смысл,

1453
01:00:13,040 --> 01:00:15,040
и поэтому, в частности, люди предложили

1454
01:00:15,040 --> 01:00:18,000
эту задачу по аналогии, и поэтому идея задачи по

1455
01:00:18,000 --> 01:00:20,319
аналогии заключается в том, что вы должны уметь

1456
01:00:20,319 --> 01:00:22,880
начинать со слова, например, король, и у вас

1457
01:00:22,880 --> 01:00:24,960
должна быть возможность вычесть мужской

1458
01:00:24,960 --> 01:00:28,240
компонент из него и добавить обратно в  женский

1459
01:00:28,240 --> 01:00:30,480
компонент, и тогда вы сможете

1460
01:00:30,480 --> 01:00:34,000
хорошо спросить, какое слово здесь, и что

1461
01:00:34,000 --> 01:00:37,839
вы хотите, чтобы слово там

1462
01:00:37,839 --> 01:00:39,280
было королева

1463
01:00:39,280 --> 01:00:41,040


1464
01:00:41,040 --> 01:00:42,559


1465
01:00:42,559 --> 01:00:43,920


1466
01:00:43,920 --> 01:00:46,480


1467
01:00:46,480 --> 01:00:48,720


1468
01:00:48,720 --> 01:00:51,680
ммм  такая же самая похожая

1469
01:00:51,680 --> 01:00:54,880
функция, которая на самом деле больше, а

1470
01:00:54,880 --> 01:00:57,520
также имея положительные слова, вы можете

1471
01:00:57,520 --> 01:01:00,160
попросить самые похожие отрицательные слова, и

1472
01:01:00,160 --> 01:01:02,559
вы можете задаться вопросом, что наиболее отрицательно

1473
01:01:02,559 --> 01:01:04,480
похоже на банан, и вы можете

1474
01:01:04,480 --> 01:01:07,440
подумать о  это эм я не знаю

1475
01:01:07,440 --> 01:01:10,240
эм какое-то мясо или что-то на

1476
01:01:10,240 --> 01:01:12,160
самом деле, что само по себе не очень

1477
01:01:12,160 --> 01:01:13,760
полезно, потому что, когда вы можете просто попросить

1478
01:01:13,760 --> 01:01:15,040


1479
01:01:15,040 --> 01:01:17,200
наиболее негативно похожие вещи, вы, как правило

1480
01:01:17,200 --> 01:01:18,000
, получаете

1481
01:01:18,000 --> 01:01:19,680
сумасшедшие строки, которые были найдены в

1482
01:01:19,680 --> 01:01:20,880
наборе данных,

1483
01:01:20,880 --> 01:01:22,319
который вы используете  не знаю, что они означают,

1484
01:01:22,319 --> 01:01:23,839


1485
01:01:23,839 --> 01:01:26,480
но если мы сложим эти два вместе, мы сможем

1486
01:01:26,480 --> 01:01:28,240
использовать наиболее похожую функцию с

1487
01:01:28,240 --> 01:01:31,040
положительными и отрицательными значениями для проведения аналогий,

1488
01:01:31,040 --> 01:01:34,319
поэтому мы собираемся сказать, что хотим положительного

1489
01:01:34,319 --> 01:01:37,040
короля, мы хотим вычесть отрицательно

1490
01:01:37,040 --> 01:01:40,319
человека, которого мы хотим  затем добавить положительно

1491
01:01:40,319 --> 01:01:42,880
женщину и выяснить, что больше всего похоже

1492
01:01:42,880 --> 01:01:46,000
на эту точку в пространстве, так что моя

1493
01:01:46,000 --> 01:01:47,520
функция аналогии делает

1494
01:01:47,520 --> 01:01:50,079
именно это,

1495
01:01:50,079 --> 01:01:53,280
взяв пару наиболее похожих, а затем

1496
01:01:53,280 --> 01:01:55,520


1497
01:01:55,520 --> 01:01:57,920
вычтя отрицательную, и поэтому мы можем попробовать

1498
01:01:57,920 --> 01:02:00,640
эту аналогию  функция, так что я могу провести

1499
01:02:00,640 --> 01:02:03,520
аналогию, которую я показываю на картинке

1500
01:02:03,520 --> 01:02:04,400
с

1501
01:02:04,400 --> 01:02:07,599
мужчиной относительно короля, поскольку женщина

1502
01:02:07,599 --> 01:02:08,640
сражается,

1503
01:02:08,640 --> 01:02:11,039
извините, я не говорю это правильно, да,

1504
01:02:11,039 --> 01:02:14,480
мужчина - король, поскольку женщина слишком о,

1505
01:02:14,480 --> 01:02:16,880
извините, я не сделал свои клетки

1506
01:02:16,880 --> 01:02:19,880


1507
01:02:21,599 --> 01:02:23,920
человек король как  женщина как

1508
01:02:23,920 --> 01:02:28,319
королева, так что это здорово, и это

1509
01:02:28,319 --> 01:02:29,680
хорошо работает,

1510
01:02:29,680 --> 01:02:31,440
я имею в виду, и вы можете сделать это вроде

1511
01:02:31,440 --> 01:02:33,920
как король по-другому - мужчина как королева,

1512
01:02:33,920 --> 01:02:37,359
как женщина, если бы это сработало только для

1513
01:02:37,359 --> 01:02:40,880
этого причудливого примера, вы, может быть, не

1514
01:02:40,880 --> 01:02:43,520
могли бы  Не очень впечатлен, но вы

1515
01:02:43,520 --> 01:02:45,359
знаете, что на самом деле это не

1516
01:02:45,359 --> 01:02:47,520
идеально, но с этим можно проводить всевозможные забавные

1517
01:02:47,520 --> 01:02:50,000
аналогии, и они на самом деле

1518
01:02:50,000 --> 01:02:52,319
работают, так что вы знаете, я мог бы попросить

1519
01:02:52,319 --> 01:02:54,000
что-то

1520
01:02:54,000 --> 01:02:56,160
вроде аналогии,

1521
01:02:56,160 --> 01:02:57,760


1522
01:02:57,760 --> 01:02:59,680
о, вот хорошая

1523
01:02:59,680 --> 01:03:02,319


1524
01:03:02,559 --> 01:03:04,400
австралия  быть э-э,

1525
01:03:04,400 --> 01:03:05,520
как

1526
01:03:05,520 --> 01:03:09,119
франция к чему,

1527
01:03:09,119 --> 01:03:10,400
и вы можете подумать о том, что вы думаете,

1528
01:03:10,400 --> 01:03:12,240
что должно быть,

1529
01:03:12,240 --> 01:03:14,799
и получится

1530
01:03:14,799 --> 01:03:17,359
шампанское, что довольно хорошо, или я

1531
01:03:17,359 --> 01:03:20,880
мог бы попросить что-то вроде

1532
01:03:20,880 --> 01:03:22,319


1533
01:03:22,319 --> 01:03:23,839
карандаша по аналогии с

1534
01:03:23,839 --> 01:03:25,839
зарисовками,

1535
01:03:25,839 --> 01:03:28,160
как

1536
01:03:28,160 --> 01:03:30,559
камера для

1537
01:03:30,559 --> 01:03:31,440
того, что

1538
01:03:31,440 --> 01:03:33,760
эм  и он говорит, что фотографируя

1539
01:03:33,760 --> 01:03:36,079
ммм, вы также можете проводить аналогии с

1540
01:03:36,079 --> 01:03:39,280
людьми, на этом этапе я должен указать

1541
01:03:39,280 --> 01:03:42,160
, что эти данные были ммм, а модель

1542
01:03:42,160 --> 01:03:44,400
была построена в 2014 году,

1543
01:03:44,400 --> 01:03:47,119
поэтому вы не можете ничего спросить о

1544
01:03:47,119 --> 01:03:49,280
Дональде Трампе в этом хорошо, вы можете

1545
01:03:49,280 --> 01:03:51,200
Трамп там, но  не как президент, но я

1546
01:03:51,200 --> 01:03:54,720
мог бы спросить что-то вроде аналогии

1547
01:03:54,720 --> 01:03:56,839
Обамы с

1548
01:03:56,839 --> 01:04:01,520
Клинтоном, как Рейган

1549
01:04:02,720 --> 01:04:04,960


1550
01:04:04,960 --> 01:04:07,359
с чем, и вы можете

1551
01:04:07,359 --> 01:04:09,520
думать о том, что вы думаете, это правильная

1552
01:04:09,520 --> 01:04:11,440
аналогия

1553
01:04:11,440 --> 01:04:14,240
там аналогия, которую она возвращает, это Никсон,

1554
01:04:14,240 --> 01:04:15,599
так что я предполагаю, что это зависит от того, что вы

1555
01:04:15,599 --> 01:04:17,440
думаете о Билле Клинтоне  Что касается того, считаете ли вы,

1556
01:04:17,440 --> 01:04:19,680
что это хорошая аналогия или нет, вы

1557
01:04:19,680 --> 01:04:21,440
также

1558
01:04:21,440 --> 01:04:24,160
можете провести с ней какие-то лингвистические аналогии,

1559
01:04:24,160 --> 01:04:27,280
чтобы вы могли сделать что-то вроде аналогии:

1560
01:04:27,280 --> 01:04:28,559
высокий

1561
01:04:28,559 --> 01:04:31,039
- самый высокий,

1562
01:04:31,039 --> 01:04:32,319
до

1563
01:04:32,319 --> 01:04:33,599
тех пор,

1564
01:04:33,599 --> 01:04:36,400
пока он не является чем, и он действует дольше всего, поэтому он

1565
01:04:36,400 --> 01:04:37,839
действительно просто

1566
01:04:37,839 --> 01:04:40,000
знает  много о смысловом

1567
01:04:40,000 --> 01:04:43,520
поведении слов, и вы знаете, я думаю, когда эти

1568
01:04:43,520 --> 01:04:44,720


1569
01:04:44,720 --> 01:04:46,480
методы были впервые разработаны и,

1570
01:04:46,480 --> 01:04:48,799
надеюсь, для вас, вы знаете, что

1571
01:04:48,799 --> 01:04:51,359
люди были просто потрясены тем, насколько

1572
01:04:51,359 --> 01:04:55,520
хорошо это на самом деле работает при

1573
01:04:55,520 --> 01:04:57,839
захвате слов, и поэтому эти векторы слов затем распространились

1574
01:04:57,839 --> 01:05:00,960
повсюду как  новое представление,

1575
01:05:00,960 --> 01:05:03,520
которое было настолько мощным для определения

1576
01:05:03,520 --> 01:05:06,000
значения слов, и это наша отправная

1577
01:05:06,000 --> 01:05:08,400
точка для этого класса, и мы поговорим

1578
01:05:08,400 --> 01:05:10,480
о них немного больше в следующий раз, и они

1579
01:05:10,480 --> 01:05:12,880
также являются основой wh  когда вы смотрите

1580
01:05:12,880 --> 01:05:15,119
на первое задание, могу ли я задать

1581
01:05:15,119 --> 01:05:17,039
быстрый вопрос о различии

1582
01:05:17,039 --> 01:05:19,760
между двумя векторами на слово,

1583
01:05:19,760 --> 01:05:21,760
да,

1584
01:05:21,760 --> 01:05:23,760
я понимаю, что может быть

1585
01:05:23,760 --> 01:05:26,000
несколько контекстных слов на

1586
01:05:26,000 --> 01:05:27,920
одно слово в словаре, например, слово

1587
01:05:27,920 --> 01:05:30,640
в словаре, но  тогда, если есть

1588
01:05:30,640 --> 01:05:32,400
только два вектора, я вроде бы подумал, что

1589
01:05:32,400 --> 01:05:33,920
разница между ними в том, что

1590
01:05:33,920 --> 01:05:35,359
один похож на фактическое слово, а другой -

1591
01:05:35,359 --> 01:05:36,960
на контекстное слово, но несколько

1592
01:05:36,960 --> 01:05:39,440
контекстных слов, например, как вы, как вы

1593
01:05:39,440 --> 01:05:42,720
выбираете только два, тогда хорошо, так что мы '  делаю

1594
01:05:42,720 --> 01:05:45,520
все правильно, так

1595
01:05:45,520 --> 01:05:46,319
что,

1596
01:05:46,319 --> 01:05:48,640
может быть, я не вернусь к демонстрации экрана,

1597
01:05:48,640 --> 01:05:51,280
но вы знаете, что мы делали

1598
01:05:51,280 --> 01:05:53,599
в целевой функции, на вас была

1599
01:05:53,599 --> 01:05:54,559
сумма

1600
01:05:54,559 --> 01:05:55,599


1601
01:05:55,599 --> 01:05:57,280
, так что вы правильно знаете этот большой

1602
01:05:57,280 --> 01:05:59,440
корпус текста  Итак, вы берете

1603
01:05:59,440 --> 01:06:02,240
сумму по каждому слову, которое

1604
01:06:02,240 --> 01:06:04,480
появляется как центральное слово, а затем

1605
01:06:04,480 --> 01:06:07,200
внутри этого есть вторая сумма,

1606
01:06:07,200 --> 01:06:08,400
которая предназначена

1607
01:06:08,400 --> 01:06:10,640
для каждого слова в контексте, поэтому вы

1608
01:06:10,640 --> 01:06:13,200
собираетесь считать каждое слово как контекстное

1609
01:06:13,200 --> 01:06:16,400
слово, а затем для  один конкретный тер

1610
01:06:16,400 --> 01:06:18,400
m этой целевой функции у вас есть

1611
01:06:18,400 --> 01:06:21,520
конкретное контекстное слово и определенное

1612
01:06:21,520 --> 01:06:24,400
центральное слово um, но затем вы как бы

1613
01:06:24,400 --> 01:06:27,599
суммируете разные контекстные слова для

1614
01:06:27,599 --> 01:06:29,599
каждого центра слова, а затем

1615
01:06:29,599 --> 01:06:32,799
суммируете все решения

1616
01:06:32,799 --> 01:06:36,319
разные центральные слова и, скажем,

1617
01:06:36,319 --> 01:06:38,240
немного больше

1618
01:06:38,240 --> 01:06:41,280
о двух векторах, я имею в виду, что вы знаете, в

1619
01:06:41,280 --> 01:06:43,839
каком-то смысле это уродливая деталь, но

1620
01:06:43,839 --> 01:06:45,920
это было сделано для того, чтобы все было проще

1621
01:06:45,920 --> 01:06:50,079
и быстрее, чтобы вы знали, если

1622
01:06:50,079 --> 01:06:52,000
посмотрите

1623
01:06:52,000 --> 01:06:54,319
на математику  внимательно, если вы относитесь

1624
01:06:54,319 --> 01:06:56,160
к

1625
01:06:56,160 --> 01:06:58,720
этим двум векторам как к одному и тому же, поэтому, если вы

1626
01:06:58,720 --> 01:07:00,720
используете одни и те же векторы для центра и

1627
01:07:00,720 --> 01:07:02,319
контекста,

1628
01:07:02,319 --> 01:07:04,799
и вы говорите хорошо, давайте разберемся с

1629
01:07:04,799 --> 01:07:06,640
производными,

1630
01:07:06,640 --> 01:07:09,119
все становится уродливее, и причина, по которой

1631
01:07:09,119 --> 01:07:11,680
они становятся уродливее, -

1632
01:07:11,680 --> 01:07:14,319
это нормально, когда я повторяю  по

1633
01:07:14,319 --> 01:07:18,319
всем вариантам контекстного слова, о боже,

1634
01:07:18,319 --> 01:07:20,000
иногда контекстное слово

1635
01:07:20,000 --> 01:07:22,960
будет таким же, как и центральное слово, и

1636
01:07:22,960 --> 01:07:26,079
это мешает разрабатывать мои

1637
01:07:26,079 --> 01:07:27,599
производные, в

1638
01:07:27,599 --> 01:07:30,000
то время как, принимая их как отдельные

1639
01:07:30,000 --> 01:07:33,119
векторы, чего никогда не бывает.  о, это легко,

1640
01:07:33,119 --> 01:07:35,839
но интересно то, что

1641
01:07:35,839 --> 01:07:38,160
вы знаете, когда говорите, что у вас есть эти два

1642
01:07:38,160 --> 01:07:40,319
разных представления,

1643
01:07:40,319 --> 01:07:42,720
вроде как в конечном итоге действительно

1644
01:07:42,720 --> 01:07:46,000
не причиняют вреда, и я машу руками

1645
01:07:46,000 --> 01:07:48,880
аргумент в пользу этого, вы знаете, поскольку

1646
01:07:48,880 --> 01:07:51,520
мы как бы двигаемся  через каждую

1647
01:07:51,520 --> 01:07:54,799
позицию в корпусе одно за другим вы

1648
01:07:54,799 --> 01:07:57,200
кое-что знаете, слово, которое является центральным словом

1649
01:07:57,200 --> 01:07:59,920
в один момент, будет контекстным

1650
01:07:59,920 --> 01:08:02,400
словом в следующий момент, а слово,

1651
01:08:02,400 --> 01:08:04,640
которое было контекстным словом

1652
01:08:04,640 --> 01:08:06,720
, станет центральным словом, поэтому вы  мы как

1653
01:08:06,720 --> 01:08:09,039
бы выполняем

1654
01:08:09,039 --> 01:08:12,960
вычисления в обоих направлениях в каждом случае,

1655
01:08:12,960 --> 01:08:14,799
и поэтому вы должны быть в состоянии убедить

1656
01:08:14,799 --> 01:08:17,359
себя, что два представления

1657
01:08:17,359 --> 01:08:20,158
для слова в конечном итоге очень похожи,

1658
01:08:20,158 --> 01:08:22,560
и они не идентичны по

1659
01:08:22,560 --> 01:08:23,920
техническим причинам в концах

1660
01:08:23,920 --> 01:08:26,799
документов и  такие вещи, но очень

1661
01:08:26,799 --> 01:08:28,960
очень похожие

1662
01:08:28,960 --> 01:08:31,198
и так эффективно вы, как правило, получаете два

1663
01:08:31,198 --> 01:08:33,439
очень похожих представления для каждого

1664
01:08:33,439 --> 01:08:35,600
слова, и мы просто усредняем их и называем

1665
01:08:35,600 --> 01:08:37,920
это вектором слов, и поэтому, когда мы используем

1666
01:08:37,920 --> 01:08:40,399
векторы слов, у нас есть только один  вектор для

1667
01:08:40,399 --> 01:08:41,759
каждого слова,

1668
01:08:41,759 --> 01:08:44,560
которое имеет смысл, спасибо, у

1669
01:08:44,560 --> 01:08:46,799
меня вопрос чисто из любопытства, поэтому

1670
01:08:46,799 --> 01:08:49,279
мы начали с того, что спроецировали

1671
01:08:49,279 --> 01:08:51,439
векторы векторов слов на двумерную

1672
01:08:51,439 --> 01:08:53,679
поверхность, которые мы увидели как небольшие кластеры

1673
01:08:53,679 --> 01:08:55,120
слов, которые похожи друг на друга, а

1674
01:08:55,120 --> 01:08:57,198
затем мы увидели  что гм с

1675
01:08:57,198 --> 01:08:59,359
аналогиями, что мы как бы видим, что

1676
01:08:59,359 --> 01:09:01,198
есть эти направленные векторы, которые

1677
01:09:01,198 --> 01:09:03,439
как бы указывают на правителя

1678
01:09:03,439 --> 01:09:05,439
или генерального директора чего-то в этом роде, и поэтому

1679
01:09:05,439 --> 01:09:07,198
мне интересно, есть ли

1680
01:09:07,198 --> 01:09:09,359
отношения между

1681
01:09:09,359 --> 01:09:12,158
самими этими относительными векторами, например

1682
01:09:12,158 --> 01:09:14,880
правитель вектора вроде как похож

1683
01:09:14,880 --> 01:09:17,839
на генерального директора вектора, который сильно

1684
01:09:17,839 --> 01:09:20,238
отличается от того, что делает хороший

1685
01:09:20,238 --> 01:09:22,960
бутерброд с вектором,

1686
01:09:22,960 --> 01:09:25,920
есть ли какие-либо исследования по

1687
01:09:25,920 --> 01:09:29,759
этому поводу, это хороший вопрос,

1688
01:09:29,759 --> 01:09:31,279
как вы поставите меня в тупик уже на

1689
01:09:31,279 --> 01:09:35,560
первой лекции ах

1690
01:09:37,359 --> 01:09:38,640
я  означает, что

1691
01:09:38,640 --> 01:09:40,479
да, я действительно не могу придумать какое-то

1692
01:09:40,479 --> 01:09:42,479
исследование, поэтому я не уверен, что у меня есть

1693
01:09:42,479 --> 01:09:43,920
уверенность, и я не уверен, что у меня есть

1694
01:09:43,920 --> 01:09:46,238
уверенный ответ, я имею в виду, что кажется,

1695
01:09:46,238 --> 01:09:49,920
что это действительно легко  g, чтобы проверить,

1696
01:09:49,920 --> 01:09:52,158
сколько у вас есть одного из этих наборов

1697
01:09:52,158 --> 01:09:56,000
векторов слов um, которые кажутся вам похожими,

1698
01:09:56,000 --> 01:09:59,520
и для любых отношений, которые

1699
01:09:59,520 --> 01:10:02,000
достаточно хорошо представлены словом, вы

1700
01:10:02,000 --> 01:10:04,000
должны быть в состоянии увидеть, выходит ли оно

1701
01:10:04,000 --> 01:10:05,440
похожим на то, что

1702
01:10:05,440 --> 01:10:06,640


1703
01:10:06,640 --> 01:10:07,760


1704
01:10:07,760 --> 01:10:10,560
я имею в виду  Я не уверен, что мы сможем посмотреть

1705
01:10:10,560 --> 01:10:11,520
и увидеть

1706
01:10:11,520 --> 01:10:12,960
да, это совершенно нормально, просто

1707
01:10:12,960 --> 01:10:15,960
любопытно,

1708
01:10:17,040 --> 01:10:18,800
извините, я пропустил последнюю часть

1709
01:10:18,800 --> 01:10:21,120
вашего ответа на первый вопрос, поэтому, когда

1710
01:10:21,120 --> 01:10:22,960
вы хотели свернуть два вектора для

1711
01:10:22,960 --> 01:10:24,400
одного и того же слова, вы сказали  Обычно вы

1712
01:10:24,400 --> 01:10:26,640
берете среднее, ммм, разные люди

1713
01:10:26,640 --> 01:10:28,560
делали разные вещи, но наиболее

1714
01:10:28,560 --> 01:10:32,239
распространенная практика - это после того, как вы

1715
01:10:32,239 --> 01:10:34,080
знаете, я должен рассказать еще немного

1716
01:10:34,080 --> 01:10:36,800
о беге словесного погружения, что мы

1717
01:10:36,800 --> 01:10:38,400
действительно не прошли сегодня, так что я

1718
01:10:38,400 --> 01:10:39,840
все еще  В четверг у вас есть немного больше работы,

1719
01:10:39,840 --> 01:10:42,480
но вы знаете, как только вы запустите

1720
01:10:42,480 --> 01:10:44,400
свой алгоритм word to vec,

1721
01:10:44,400 --> 01:10:47,679
и вы сортируете свой вывод, это два

1722
01:10:47,679 --> 01:10:50,640
вектора для каждого слова и вроде как,

1723
01:10:50,640 --> 01:10:52,880
когда он в центре и когда это контекст,

1724
01:10:52,880 --> 01:10:53,679
и

1725
01:10:53,679 --> 01:10:56,080
поэтому обычно люди  просто усредни этих

1726
01:10:56,080 --> 01:10:59,120
двоих  векторов и скажите хорошо, что это

1727
01:10:59,120 --> 01:11:02,239
представление слова круассан, и это

1728
01:11:02,239 --> 01:11:04,800
то, что появляется в виде файла векторов слов,

1729
01:11:04,800 --> 01:11:08,719
подобного тому, который я загрузил,

1730
01:11:08,719 --> 01:11:11,199
что имеет смысл, спасибо,

1731
01:11:11,199 --> 01:11:14,080
спасибо, поэтому мой вопрос в том, может ли слово

1732
01:11:14,080 --> 01:11:16,239
иметь два разных значения или несколько

1733
01:11:16,239 --> 01:11:17,679
разных значений  мы по-прежнему

1734
01:11:17,679 --> 01:11:20,000
представляем его как один и тот же

1735
01:11:20,000 --> 01:11:21,600
единственный вектор,

1736
01:11:21,600 --> 01:11:24,640
да, это очень хороший вопрос, и на

1737
01:11:24,640 --> 01:11:26,320
самом деле

1738
01:11:26,320 --> 01:11:28,800
в лекции в четверг есть определенное содержание, так что я могу сказать больше

1739
01:11:28,800 --> 01:11:32,320
об этом, но да, первая

1740
01:11:32,320 --> 01:11:35,679
реакция - вам вроде как следует бояться,

1741
01:11:35,679 --> 01:11:38,159
потому что что-то  я вообще ничего не сказал,

1742
01:11:38,159 --> 01:11:40,800
вы знаете, что

1743
01:11:40,800 --> 01:11:43,840
большинство слов, особенно короткие общие слова,

1744
01:11:43,840 --> 01:11:46,640
имеют много значения, поэтому, если у вас есть такое

1745
01:11:46,640 --> 01:11:50,159
слово, как звезда, которое может быть астрономическим

1746
01:11:50,159 --> 01:11:52,800
объектом, или это может быть вы знаете

1747
01:11:52,800 --> 01:11:55,199
кинозвезду звезду Голливуда или это может быть  что-то

1748
01:11:55,199 --> 01:11:57,120
вроде золотых звезд, которые у вас есть в

1749
01:11:57,120 --> 01:11:58,719
начальной школе,

1750
01:11:58,719 --> 01:11:59,760
и

1751
01:11:59,760 --> 01:12:02,320
мы просто берем все эти

1752
01:12:02,320 --> 01:12:05,199
употребления слова звезда и сворачиваем

1753
01:12:05,199 --> 01:12:07,120
их вместе

1754
01:12:07,120 --> 01:12:10,320
в один вектор слова, ммм, и вы можете

1755
01:12:10,320 --> 01:12:13,840
подумать, что это ре  союзник сумасшедший и плохой ммм, но на

1756
01:12:13,840 --> 01:12:16,480
самом деле оказывается, что он работает довольно хорошо,

1757
01:12:16,480 --> 01:12:17,600


1758
01:12:17,600 --> 01:12:21,120
может, я не буду проходить через все это

1759
01:12:21,120 --> 01:12:22,560
прямо сейчас, потому что на самом деле

1760
01:12:22,560 --> 01:12:24,880
есть кое-что об этом на лекции в четверг,

1761
01:12:24,880 --> 01:12:27,679
о, я вижу, я думаю, вы можете поставить

1762
01:12:27,679 --> 01:12:32,080
вперед слайды для  в следующий раз, о, подождите,

1763
01:12:33,040 --> 01:12:36,920
я знаю это, давайте посмотрим

1764
01:12:39,690 --> 01:12:43,880
[Музыка]

1765
01:12:49,040 --> 01:12:50,400


1766
01:12:50,400 --> 01:12:51,520
, мы

1767
01:12:51,520 --> 01:12:52,880
смотрим на то,

1768
01:12:52,880 --> 01:12:55,440
как реализовать, или мы смотрим

1769
01:12:55,440 --> 01:12:57,280
на стопку чего-то вроде alexa

1770
01:12:57,280 --> 01:13:00,400
или чего-то еще, обеспечивающего речь для

1771
01:13:00,400 --> 01:13:02,480
контекстных действий в этом курсе, было ли это

1772
01:13:02,480 --> 01:13:04,640
просто главным

1773
01:13:04,640 --> 01:13:05,760


1774
01:13:05,760 --> 01:13:08,960
пониманием  так что это необычный мошенник

1775
01:13:08,960 --> 01:13:11,440
необычный квартал,

1776
01:13:11,440 --> 01:13:14,159
но для этого квартала есть очень

1777
01:13:14,159 --> 01:13:16,480
четкий ответ, который

1778
01:13:16,480 --> 01:13:17,520
гм в

1779
01:13:17,520 --> 01:13:20,800
этом квартале есть также уроки речи,

1780
01:13:20,800 --> 01:13:24,800
которые преподаются CS 224

1781
01:13:24,800 --> 01:13:27,679
или уроки речи, которые преподает Эндрю

1782
01:13:27,679 --> 01:13:30,000
Марс, и вы знаете, что это  курс, который

1783
01:13:30,000 --> 01:13:32,800
предлагается более регулярно, иногда

1784
01:13:32,800 --> 01:13:35,040
предлагается только раз в три года,

1785
01:13:35,040 --> 01:13:37,920
но предлагается прямо сейчас, поэтому, если

1786
01:13:37,920 --> 01:13:40,000
вы хотите узнать о

1787
01:13:40,000 --> 01:13:43,840
распознавании речи и узнать о

1788
01:13:43,840 --> 01:13:47,360
некоторых методах построения диалога  системы,

1789
01:13:47,360 --> 01:13:48,520
вы должны выполнить

1790
01:13:48,520 --> 01:13:50,960
cs224 да,

1791
01:13:50,960 --> 01:13:54,640
так что вы знаете, что для этого класса в целом

1792
01:13:54,640 --> 01:13:59,120
большая часть этого класса работает

1793
01:13:59,120 --> 01:14:02,719
с текстом и выполняет различные виды

1794
01:14:02,719 --> 01:14:06,159
анализа и понимания текста, поэтому мы выполняем

1795
01:14:06,159 --> 01:14:08,000
задачи, подобные тем, которые я упомянул,

1796
01:14:08,000 --> 01:14:12,239
мы выполняем машинный перевод  мы

1797
01:14:12,239 --> 01:14:15,600
действительно отвечаем на вопросы эм мы смотрим на то, как

1798
01:14:15,600 --> 01:14:18,080
передать эту структуру предложений и

1799
01:14:18,080 --> 01:14:20,640
тому подобное, вы знаете, в другие годы

1800
01:14:20,640 --> 01:14:22,480
я иногда говорю немного о

1801
01:14:22,480 --> 01:14:24,320
речи эм, но

1802
01:14:24,320 --> 01:14:25,840
с этого квартала есть совсем

1803
01:14:25,840 --> 01:14:27,840
другой класс, который сосредоточен на речи,

1804
01:14:27,840 --> 01:14:30,960
которая кажется немного  немного глупо,

1805
01:14:31,120 --> 01:14:32,640
я думаю, у тебя

1806
01:14:32,640 --> 01:14:34,719
есть часть партнерства со своей

1807
01:14:34,719 --> 01:14:37,280
аудиторией.

1808
01:14:38,890 --> 01:14:40,239
[Музыка]

1809
01:14:40,239 --> 01:14:43,840
больше о речи

1810
01:14:47,760 --> 01:14:49,440
я сейчас получаю плохое эхо, я не уверен

1811
01:14:49,440 --> 01:14:51,679
, моя ли это вина или твоя, но в

1812
01:14:51,679 --> 01:14:54,000
любом случае эм, в

1813
01:14:54,000 --> 01:14:58,000
любом случае, ответь да, так что класс речи

1814
01:14:58,000 --> 01:15:01,040
делает смесь чего-то, поэтому я имею в виду, что

1815
01:15:01,040 --> 01:15:04,239
классические проблемы с чистой речью

1816
01:15:04,239 --> 01:15:07,360
были связаны с распознаванием речи, таким образом

1817
01:15:07,360 --> 01:15:10,159
переходя от речевого сигнала к тексту и

1818
01:15:10,159 --> 01:15:11,120
выполнением

1819
01:15:11,120 --> 01:15:15,120
преобразования текста в речь, переходя от текста к нам,

1820
01:15:15,120 --> 01:15:17,360
к речевому сигналу и  обе эти

1821
01:15:17,360 --> 01:15:19,600
проблемы теперь обычно решаются, в

1822
01:15:19,600 --> 01:15:21,040
том

1823
01:15:21,040 --> 01:15:22,719
числе сотовым телефоном, который находится в вашем

1824
01:15:22,719 --> 01:15:23,679
кармане,

1825
01:15:23,679 --> 01:15:26,560
с использованием нейронных сетей, и поэтому он охватывает

1826
01:15:26,560 --> 01:15:30,000
оба из них, но затем между ними

1827
01:15:30,000 --> 01:15:32,239
класс охватывает довольно много, и, в

1828
01:15:32,239 --> 01:15:35,040
частности, он начинается с

1829
01:15:35,040 --> 01:15:37,679
рассмотрения строительства  диалоговая система, так что

1830
01:15:37,679 --> 01:15:40,480
это что-то вроде alexa

1831
01:15:40,480 --> 01:15:43,520
google assistant siri,

1832
01:15:43,520 --> 01:15:45,760
если предположить, что у вас есть

1833
01:15:45,760 --> 01:15:49,040
распознавание речи система преобразования текста в речь,

1834
01:15:49,040 --> 01:15:52,080
тогда у вас действительно есть текст и текст,

1835
01:15:52,080 --> 01:15:54,320
какие способы люди

1836
01:15:54,320 --> 01:15:56,800
строят  гм

1837
01:15:56,800 --> 01:15:58,640
ммм

1838
01:15:58,640 --> 01:16:01,440
диалоговые системы, подобные тем, о которых я

1839
01:16:01,440 --> 01:16:04,159
только что упомянул, у

1840
01:16:04,239 --> 01:16:06,960
меня действительно был вопрос, поэтому

1841
01:16:06,960 --> 01:16:08,239
я думаю, что некоторые люди в

1842
01:16:08,239 --> 01:16:10,960
чате заметили, что эти противоположности

1843
01:16:10,960 --> 01:16:12,480
были действительно близко друг к другу, что было

1844
01:16:12,480 --> 01:16:15,600
немного странно, но мне также было интересно, ммм

1845
01:16:15,600 --> 01:16:18,320
как насчет положительной и отрицательной

1846
01:16:18,320 --> 01:16:20,880
валентности или подобного аффекта, эм,

1847
01:16:20,880 --> 01:16:24,239
это хорошо отражено в этом типе модели, или

1848
01:16:24,239 --> 01:16:26,480
это похоже на не очень хорошо, как хорошо,

1849
01:16:26,480 --> 01:16:27,679
как с противоположностями, как это

1850
01:16:27,679 --> 01:16:30,080
не было на самом деле  да, поэтому краткий ответ

1851
01:16:30,080 --> 01:16:32,640
предназначен для обоих, и поэтому это

1852
01:16:32,640 --> 01:16:35,520
хороший вопрос, хорошее наблюдение,

1853
01:16:35,520 --> 01:16:37,520
и короткий ответ: нет, оба они

1854
01:16:37,520 --> 01:16:40,320
улавливаются очень, очень плохо, я имею в виду,

1855
01:16:40,320 --> 01:16:42,719
что есть определение,

1856
01:16:42,719 --> 01:16:43,520


1857
01:16:43,520 --> 01:16:46,000
о, вы знаете, когда я говорю действительно очень

1858
01:16:46,000 --> 01:16:48,400
плохо  Я имею в виду,

1859
01:16:48,400 --> 01:16:51,199
что я имею в виду, что если это то,

1860
01:16:51,199 --> 01:16:54,320
на чем вы хотите сосредоточиться, у вас

1861
01:16:54,320 --> 01:16:55,760
проблемы, я имею в виду, что

1862
01:16:55,760 --> 01:16:57,840
алгоритм не работает так

1863
01:16:57,840 --> 01:17:01,040
точно, вы обнаруживаете

1864
01:17:01,040 --> 01:17:04,080
, что вы знаете, что антонимы обычно

1865
01:17:04,080 --> 01:17:06,239
встречаются в очень похожих темах, потому что вы

1866
01:17:06,239 --> 01:17:08,239
знаете  говорит ли это,

1867
01:17:08,239 --> 01:17:10,800
что вы знаете, что Джон действительно высокий, или

1868
01:17:10,800 --> 01:17:14,239
Джон действительно невысокий, или этот фильм был

1869
01:17:14,239 --> 01:17:16,880
фантастическим, или этот фильм был ужасным,

1870
01:17:16,880 --> 01:17:18,640
верно, вы получаете

1871
01:17:18,640 --> 01:17:20,880
антонимы, встречающиеся в одном контексте,

1872
01:17:20,880 --> 01:17:23,440
поэтому их векторы

1873
01:17:23,440 --> 01:17:26,000
очень похожи и одинаковы для своего рода

1874
01:17:26,000 --> 01:17:28,320
аффекта и  слова, основанные на сантиментах, ну,

1875
01:17:28,320 --> 01:17:32,320
например, отличный и ужасный пример,

1876
01:17:32,320 --> 01:17:35,360
их контексты схожи, они

1877
01:17:35,360 --> 01:17:36,640
на самом деле

1878
01:17:36,640 --> 01:17:38,400
, если вы просто изучаете

1879
01:17:38,400 --> 01:17:42,400
такие предсказательные слова и контекстные модели,

1880
01:17:42,400 --> 01:17:45,520
ну нет, это не captu  красный, это еще

1881
01:17:45,520 --> 01:17:48,400
не конец истории, я имею в виду, что вы знаете, что

1882
01:17:48,400 --> 01:17:50,719
абсолютно люди хотели использовать нейронные

1883
01:17:50,719 --> 01:17:53,920
сети для определения настроений и других

1884
01:17:53,920 --> 01:17:56,480
видов коннотационного эффекта, и

1885
01:17:56,480 --> 01:17:58,800
есть очень хорошие способы сделать это, но

1886
01:17:58,800 --> 01:18:01,360
каким-то образом вам нужно сделать что-то большее,

1887
01:18:01,360 --> 01:18:03,679
чем просто прогнозирование  слова в контексте,

1888
01:18:03,679 --> 01:18:06,239
потому что этого недостаточно, чтобы

1889
01:18:06,239 --> 01:18:08,080
охватывать это измерение,

1890
01:18:08,080 --> 01:18:12,040
а больше о последующих

1891
01:18:12,640 --> 01:18:14,400
прилагательных, таких как очень простые

1892
01:18:14,400 --> 01:18:17,120
прилагательные, как так и не нравится,

1893
01:18:17,120 --> 01:18:18,719
потому что они хотели бы появиться в

1894
01:18:18,719 --> 01:18:21,120
похожем контексте правильно,

1895
01:18:21,120 --> 01:18:23,600
что было вашим первым примером раньше, а не

1896
01:18:23,600 --> 01:18:26,640
как так  так здорово, так что

1897
01:18:26,640 --> 01:18:28,880
это на самом деле хороший вопрос, так

1898
01:18:28,880 --> 01:18:29,679


1899
01:18:29,679 --> 01:18:31,440
что да, есть эти очень распространенные

1900
01:18:31,440 --> 01:18:33,199
слова, которые лингвисты обычно называют

1901
01:18:33,199 --> 01:18:35,520
функциональными словами, которые, как вы

1902
01:18:35,520 --> 01:18:38,800
знаете, включают такие, как э-э

1903
01:18:38,800 --> 01:18:42,159
, а не другие, такие как и и

1904
01:18:42,159 --> 01:18:45,760
предлоги, как вы знаете  два, и

1905
01:18:45,760 --> 01:18:46,640


1906
01:18:46,640 --> 01:18:49,360
вы вроде бы могли подозревать, что словесные

1907
01:18:49,360 --> 01:18:51,440
векторы для них

1908
01:18:51,440 --> 01:18:54,560
не очень хорошо работают, потому что они встречаются во

1909
01:18:54,560 --> 01:18:57,040
всех видах разных контекстов и  г,

1910
01:18:57,040 --> 01:18:59,040


1911
01:18:59,040 --> 01:19:01,600
во многих случаях они не очень отличаются друг от друга, и в первом

1912
01:19:01,600 --> 01:19:03,920
приближении я думаю, что это правда, и

1913
01:19:03,920 --> 01:19:05,440
отчасти поэтому я не использовал их в качестве

1914
01:19:05,440 --> 01:19:09,840
примеров на своих слайдах. Да,

1915
01:19:10,000 --> 01:19:12,320
но вы знаете, в конце концов, что мы

1916
01:19:12,320 --> 01:19:14,560
создаем  векторные представления этих

1917
01:19:14,560 --> 01:19:17,360
слов тоже, и через несколько лекций вы увидите,

1918
01:19:17,360 --> 01:19:20,400
когда мы начнем строить

1919
01:19:20,400 --> 01:19:22,400
то, что мы называем языковыми моделями, что на

1920
01:19:22,400 --> 01:19:24,640
самом деле они отлично справляются с этими

1921
01:19:24,640 --> 01:19:26,880
словами, я хочу объяснить, что я имею в

1922
01:19:26,880 --> 01:19:29,280
виду здесь.  означает, что

1923
01:19:29,280 --> 01:19:30,480
вы знаете, что

1924
01:19:30,480 --> 01:19:31,679
еще одна

1925
01:19:31,679 --> 01:19:34,320
особенность модели word to vect заключается в

1926
01:19:34,320 --> 01:19:36,800
том, что на самом деле она игнорирует положение

1927
01:19:36,800 --> 01:19:39,840
слов, поэтому в нем говорилось, что я собираюсь

1928
01:19:39,840 --> 01:19:42,560
предсказать каждое слово вокруг центрального

1929
01:19:42,560 --> 01:19:45,360
слова, но вы знаете, что я прогнозирую его

1930
01:19:45,360 --> 01:19:47,600
так же, как и я  не предсказывая по-

1931
01:19:47,600 --> 01:19:49,920
разному слово передо мной, или

1932
01:19:49,920 --> 01:19:53,040
слово после меня, или слово, находящееся на расстоянии двух

1933
01:19:53,040 --> 01:19:54,640
в любом направлении, верно, все они

1934
01:19:54,640 --> 01:19:57,920
просто предсказываются одинаково с помощью этой

1935
01:19:57,920 --> 01:20:00,560
функции вероятности, и поэтому, если это

1936
01:20:00,560 --> 01:20:02,960
все, что у вас есть, это разрушает

1937
01:20:02,960 --> 01:20:06,480
ваш  умение делать хорошую работу в UM

1938
01:20:06,480 --> 01:20:09,199
Captur  используя такие общие, более

1939
01:20:09,199 --> 01:20:12,239
грамматические слова, такие как so not an и,

1940
01:20:12,239 --> 01:20:14,639
но мы строим немного разные модели

1941
01:20:14,639 --> 01:20:16,800
, которые более чувствительны к

1942
01:20:16,800 --> 01:20:18,639
структуре предложений, а затем мы начинаем

1943
01:20:18,639 --> 01:20:21,199
хорошо работать над ними,

1944
01:20:21,199 --> 01:20:23,920
хорошо, спасибо, у

1945
01:20:24,880 --> 01:20:26,560
меня был вопрос о

1946
01:20:26,560 --> 01:20:28,880
характеристике  слово в факт

1947
01:20:28,880 --> 01:20:32,880
гм, потому что II,

1948
01:20:36,400 --> 01:20:37,760
который немного отличался от того, как он

1949
01:20:37,760 --> 01:20:39,520
был представлен в микроволновке, так

1950
01:20:39,520 --> 01:20:43,120
это как две дополнительные причины,

1951
01:20:43,120 --> 01:20:45,120
да, так что

1952
01:20:45,120 --> 01:20:49,120
мне еще есть что сказать, так что я буду в

1953
01:20:49,120 --> 01:20:52,639
курсе в четверг, эм, чтобы узнать больше о векторах слов

1954
01:20:52,639 --> 01:20:55,760
эм, вы знаете, что

1955
01:20:55,760 --> 01:20:57,199
слово к спине

1956
01:20:57,199 --> 01:21:00,320
- это своего рода структура для построения векторов слов,

1957
01:21:00,320 --> 01:21:02,480
и что в рамках существует

1958
01:21:02,480 --> 01:21:05,440
несколько вариантов точных алгоритмов,

1959
01:21:05,440 --> 01:21:08,320
и

1960
01:21:08,320 --> 01:21:11,280
вы знаете, что один из них заключается в

1961
01:21:11,280 --> 01:21:12,960
том, как предсказываете

1962
01:21:12,960 --> 01:21:14,480
ли вы контекстные слова или

1963
01:21:14,480 --> 01:21:16,880
предсказывая центральное слово,

1964
01:21:16,880 --> 01:21:20,080
поэтому модель, которую я показал, предсказывала

1965
01:21:20,080 --> 01:21:23,199
контекстные слова, так что это была модель пропуска грамматики,

1966
01:21:23,199 --> 01:21:26,960
но затем есть какая-то деталь

1967
01:21:26,960 --> 01:21:30,840
о том, как, в частности, вы проводите

1968
01:21:30,840 --> 01:21:33,760
оптимизацию и что я  представленный

1969
01:21:33,760 --> 01:21:36,639
был своего рода самый простой способ сделать это, который

1970
01:21:36,639 --> 01:21:37,760
представляет собой

1971
01:21:37,760 --> 01:21:40,000
наивную оптимизацию

1972
01:21:40,000 --> 01:21:41,040
с

1973
01:21:41,040 --> 01:21:43,760
уравнением уравнение мягкого максимума для

1974
01:21:43,760 --> 01:21:47,440
векторов слов, а оказывается, что эта наивная

1975
01:21:47,440 --> 01:21:50,800
оптимизация излишне

1976
01:21:50,800 --> 01:21:53,920
затратна, и люди

1977
01:21:53,920 --> 01:21:57,040
придумали более быстрые способы сделать  это, в

1978
01:21:57,040 --> 01:21:59,280
частности, самая распространенная вещь, которую вы

1979
01:21:59,280 --> 01:22:00,880
видите, - это то, что называется пропустить грамм с

1980
01:22:00,880 --> 01:22:03,120
отрицательной выборкой, а отрицательная

1981
01:22:03,120 --> 01:22:05,520
выборка - это своего рода гораздо более

1982
01:22:05,520 --> 01:22:07,760
эффективный способ оценки вещей, и

1983
01:22:07,760 --> 01:22:10,400
я упомяну, что в четверг,

1984
01:22:10,400 --> 01:22:12,000
хорошо, хорошо,

1985
01:22:12,000 --> 01:22:12,960
спасибо,

1986
01:22:12,960 --> 01:22:16,639
кто запрашивает дополнительную информацию  о

1987
01:22:16,639 --> 01:22:19,199
том, как конструируются векторы слов,

1988
01:22:19,199 --> 01:22:21,360
помимо краткой информации о случайной

1989
01:22:21,360 --> 01:22:23,520
инициализации, а затем на

1990
01:22:23,520 --> 01:22:25,360
основе градиента,

1991
01:22:25,360 --> 01:22:28,639
итеративная оптимизация обновления, да,

1992
01:22:28,639 --> 01:22:30,880
так что я как бы сделаю немного больше,

1993
01:22:30,880 --> 01:22:33,199
связав это вместе, в

1994
01:22:33,199 --> 01:22:35,679
четверговой лекции, я думаю, такого рода

1995
01:22:35,679 --> 01:22:37,760
только так много, что можно  вписывается в первый

1996
01:22:37,760 --> 01:22:38,560
класс,

1997
01:22:38,560 --> 01:22:40,560
но картинка

1998
01:22:40,560 --> 01:22:43,199
на картинке - это, по сути, картинка, которую я

1999
01:22:43,199 --> 01:22:46,480
показал по частям, поэтому,

2000
01:22:46,480 --> 01:22:49,040
чтобы выучить векторы слов,

2001
01:22:49,040 --> 01:22:53,199
вы начинаете с Хавина  ga вектор для

2002
01:22:53,199 --> 01:22:54,159
каждого

2003
01:22:54,159 --> 01:22:58,320
типа слова как для контекста, так и за его пределами,

2004
01:22:58,320 --> 01:23:01,199
а также тех векторов, которые вы инициализируете

2005
01:23:01,199 --> 01:23:03,280
случайным

2006
01:23:03,280 --> 01:23:05,120
образом,

2007
01:23:05,120 --> 01:23:07,760
чтобы вы просто помещали небольшие числа

2008
01:23:07,760 --> 01:23:10,000
, которые случайным образом генерируются в каждом

2009
01:23:10,000 --> 01:23:12,239
векторном компоненте, и это только ваша

2010
01:23:12,239 --> 01:23:15,360
отправная точка, и с этого момента

2011
01:23:15,360 --> 01:23:18,000
вы используете итеративный  алгоритм, в

2012
01:23:18,000 --> 01:23:20,320
котором вы постепенно обновляете

2013
01:23:20,320 --> 01:23:23,199
эти векторы слов, чтобы они

2014
01:23:23,199 --> 01:23:26,320
лучше предсказывали, какие слова появляются

2015
01:23:26,320 --> 01:23:29,360
в контексте других слов, и способ,

2016
01:23:29,360 --> 01:23:33,120
которым мы собираемся это сделать, заключается в использовании

2017
01:23:33,120 --> 01:23:36,800
градиентов, которые я как бы

2018
01:23:36,800 --> 01:23:39,199
начал  чтобы показать, как рассчитать, а

2019
01:23:39,199 --> 01:23:41,440
затем вы знаете, что когда у вас есть градиент,

2020
01:23:41,440 --> 01:23:43,520
вы можете идти в направлении, противоположном

2021
01:23:43,520 --> 01:23:46,080
градиенту, а затем вы идете

2022
01:23:46,080 --> 01:23:49,679
вниз по склону, вы минимизируете свои потери,

2023
01:23:49,679 --> 01:23:52,080
и мы собираемся сделать многое из

2024
01:23:52,080 --> 01:23:54,560
этого  пока наши векторы слов не станут настолько хорошими,

2025
01:23:54,560 --> 01:23:57,199
насколько это возможно, чтобы вы знали,

2026
01:23:57,199 --> 01:23:58,239


2027
01:23:58,239 --> 01:24:00,960
что на самом деле это все математика, но в каком-то смысле

2028
01:24:00,960 --> 01:24:04,239
вы знаете, что изучение векторных слов - это своего рода

2029
01:24:04,239 --> 01:24:07,360
чудо, поскольку вы буквально просто

2030
01:24:07,360 --> 01:24:10,159
начинаете с совершенно случайных  векторы слов

2031
01:24:10,159 --> 01:24:11,440


2032
01:24:11,440 --> 01:24:12,159
и

2033
01:24:12,159 --> 01:24:15,120
запустите этот алгоритм предсказания слов

2034
01:24:15,120 --> 01:24:16,719
на долгое время,

2035
01:24:16,719 --> 01:24:19,760
и из ничего появляются эти векторы слов,

2036
01:24:19,760 --> 01:24:24,280
которые хорошо представляют значение

