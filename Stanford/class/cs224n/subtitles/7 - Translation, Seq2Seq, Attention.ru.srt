1
00:00:05,520 --> 00:00:07,919
Привет всем и добро пожаловать на

2
00:00:07,919 --> 00:00:12,320
четвертую неделю, так что на четвертой неделе

3
00:00:12,320 --> 00:00:15,280
это будет две половины, так что сегодня

4
00:00:15,280 --> 00:00:17,199
я собираюсь поговорить о

5
00:00:17,199 --> 00:00:19,760
темах, связанных с машинным переводом, а затем

6
00:00:19,760 --> 00:00:22,720
во второй половине недели мы сделаем

7
00:00:22,720 --> 00:00:25,199
небольшой перерыв  от изучения все большего

8
00:00:25,199 --> 00:00:27,760
и большего количества тем о нейронных сетях и разговоров

9
00:00:27,760 --> 00:00:28,880
о

10
00:00:28,880 --> 00:00:31,359
финальных проектах, а также некоторых практических

11
00:00:31,359 --> 00:00:32,399
советов по

12
00:00:32,399 --> 00:00:34,880
построению ваших сетевых систем,

13
00:00:34,880 --> 00:00:37,120
поэтому для сегодняшней лекции

14
00:00:37,120 --> 00:00:40,320
это важный контент для лекции,

15
00:00:40,320 --> 00:00:42,399
поэтому в первую очередь я собираюсь представить

16
00:00:42,399 --> 00:00:43,760
новую задачу

17
00:00:43,760 --> 00:00:45,920
машинного перевода

18
00:00:45,920 --> 00:00:48,879
и  оказывается, что эта

19
00:00:48,879 --> 00:00:52,000
задача является основным вариантом использования нового

20
00:00:52,000 --> 00:00:54,320
архитектурного метода, чтобы научить

21
00:00:54,320 --> 00:00:56,719
вас глубокому обучению, которое представляет собой

22
00:00:56,719 --> 00:00:58,879
модели от последовательности к последовательности, и поэтому мы потратим

23
00:00:58,879 --> 00:01:01,840
на них много времени, а затем

24
00:01:01,840 --> 00:01:03,440
будет разработан важный

25
00:01:03,440 --> 00:01:05,680
способ улучшения  от последовательности к

26
00:01:05,680 --> 00:01:08,080
моделям последовательности, что является идеей

27
00:01:08,080 --> 00:01:10,640
внимания, и именно об этом я буду

28
00:01:10,640 --> 00:01:13,920
говорить в заключительной части урока,

29
00:01:13,920 --> 00:01:16,799
просто проверяя, все ли не отстают

30
00:01:16,799 --> 00:01:20,320
от того, что происходит, поэтому в первую очередь  все

31
00:01:20,320 --> 00:01:22,720
три задания

32
00:01:22,720 --> 00:01:24,960
должны быть выполнены сегодня, так что, надеюсь, вы

33
00:01:24,960 --> 00:01:27,280
все получили свою новую зависимость, проходит

34
00:01:27,280 --> 00:01:29,040
анализ текста хорошо,

35
00:01:29,040 --> 00:01:31,600
в то же время задание 4 выходит

36
00:01:31,600 --> 00:01:35,040
сегодня, и на самом деле сегодняшняя лекция является

37
00:01:35,040 --> 00:01:37,840
основным содержанием того, что вы будете использовать

38
00:01:37,840 --> 00:01:39,439
для создания своего задания  четыре

39
00:01:39,439 --> 00:01:41,119
системы

40
00:01:41,119 --> 00:01:43,119
переключают его ненадолго

41
00:01:43,119 --> 00:01:45,600
для задания четыре мы даем вам

42
00:01:45,600 --> 00:01:48,479
два могущественных дополнительных дня, так что у вас есть девять дней

43
00:01:48,479 --> 00:01:51,759
на это, и он должен быть в четверг,

44
00:01:51,759 --> 00:01:55,360
с другой стороны, пожалуйста, имейте в виду,

45
00:01:55,360 --> 00:01:59,520
что задание 4 больше и сложнее,

46
00:01:59,520 --> 00:02:02,159
чем предыдущее  задания, так что убедитесь,

47
00:02:02,159 --> 00:02:04,560
что вы приступили к этому рано,

48
00:02:04,560 --> 00:02:06,240
а затем, как я уже упоминал в четверг, я

49
00:02:06,240 --> 00:02:09,840
перейду к окончательным проектам,

50
00:02:10,239 --> 00:02:13,680
хорошо, так что давайте сразу перейдем к этому гм

51
00:02:13,680 --> 00:02:14,720
с

52
00:02:14,720 --> 00:02:17,599
машинным переводом, так что очень быстро я

53
00:02:17,599 --> 00:02:20,480
хотел рассказать вам немного о том, что

54
00:02:20,480 --> 00:02:21,760
вы знаете

55
00:02:21,760 --> 00:02:24,239
где  мы были и то, что мы делали,

56
00:02:24,239 --> 00:02:26,160
прежде чем мы перешли к нейронному машинному

57
00:02:26,160 --> 00:02:28,319
переводу, поэтому давайте сделаем

58
00:02:28,319 --> 00:02:31,599
предысторию машинного перевода, чтобы

59
00:02:31,599 --> 00:02:34,480
машинный перевод - это задача

60
00:02:34,480 --> 00:02:36,959
перевода предложения x с одного

61
00:02:36,959 --> 00:02:38,800
языка  который называется исходным

62
00:02:38,800 --> 00:02:41,519
языком для другого языка, целевой

63
00:02:41,519 --> 00:02:44,480
язык формирует предложение y,

64
00:02:44,480 --> 00:02:46,720
поэтому мы начинаем с предложения исходного языка

65
00:02:46,720 --> 00:02:48,239
x

66
00:02:48,239 --> 00:02:50,720
lo mein

67
00:02:50,720 --> 00:02:54,160
um, а затем переводим его и

68
00:02:54,160 --> 00:02:56,959
получаем перевод: человек рождается свободным, но

69
00:02:56,959 --> 00:02:59,280
везде он в цепях

70
00:02:59,280 --> 00:03:01,760
хорошо, так что есть наш машинный перевод,

71
00:03:01,760 --> 00:03:05,599
хорошо, так что в начале 1950-х они

72
00:03:05,599 --> 00:03:09,360
начали работать над машинным переводом, и

73
00:03:09,360 --> 00:03:11,040
так что на самом деле это связано с

74
00:03:11,040 --> 00:03:13,360
информатикой, если вы найдете вещи, в названии которых есть

75
00:03:13,360 --> 00:03:15,360
машина, большинство из них -

76
00:03:15,360 --> 00:03:17,120


77
00:03:17,120 --> 00:03:20,400
старые вещи  это действительно произошло

78
00:03:20,400 --> 00:03:23,360
в контексте США в контексте

79
00:03:23,360 --> 00:03:27,040
холодной войны, гм, поэтому было желание

80
00:03:27,040 --> 00:03:28,720
следить за тем, что делали россияне

81
00:03:28,720 --> 00:03:31,280
, и у людей возникла идея, что,

82
00:03:31,280 --> 00:03:33,519
поскольку некоторые из самых ранних компьютеров

83
00:03:33,519 --> 00:03:37,040
были настолько успешными, гм  занимаясь

84
00:03:37,040 --> 00:03:39,120
взломом кода во время Второй мировой войны,

85
00:03:39,120 --> 00:03:41,360
тогда, возможно, мы могли бы настроить первые

86
00:03:41,360 --> 00:03:43,599
компьютеры для работы

87
00:03:43,599 --> 00:03:47,200
во время холодной войны, чтобы делать перевод,

88
00:03:47,200 --> 00:03:48,720
и, надеюсь, это сыграет, и вы

89
00:03:48,720 --> 00:03:50,799
сможете это услышать  вот небольшой

90
00:03:50,799 --> 00:03:53,439
видеоклип, демонстрирующий некоторые из самых ранних

91
00:03:53,439 --> 00:03:56,239
работ по машинному переводу

92
00:03:56,239 --> 00:03:59,480
с 1954 года.

93
00:04:00,700 --> 00:04:02,239
[Музыка]

94
00:04:02,239 --> 00:04:04,319
они не учли двусмысленности,

95
00:04:04,319 --> 00:04:06,080
когда намеревались использовать компьютеры для

96
00:04:06,080 --> 00:04:07,760
перевода языков.

97
00:04:07,760 --> 00:04:11,120


98
00:04:11,120 --> 00:04:13,120


99
00:04:13,120 --> 00:04:15,760
с русского на английский

100
00:04:15,760 --> 00:04:18,000
вместо математического волшебства предложение на

101
00:04:18,000 --> 00:04:18,959
русском языке

102
00:04:18,959 --> 00:04:20,720
одно из первых нечисловых

103
00:04:20,720 --> 00:04:22,960
приложений компьютеров оно было разрекламировано

104
00:04:22,960 --> 00:04:24,560
как решение одержимости холодной войны

105
00:04:24,560 --> 00:04:26,400
следить за тем, что

106
00:04:26,400 --> 00:04:28,880
делали россияне.

107
00:04:28,880 --> 00:04:30,479


108
00:04:30,479 --> 00:04:32,560
переводчики-люди,

109
00:04:32,560 --> 00:04:33,600
конечно, вы просто находитесь на

110
00:04:33,600 --> 00:04:35,520
экспериментальной стадии, когда вы занимаетесь

111
00:04:35,520 --> 00:04:36,880
полномасштабным производством, какие

112
00:04:36,880 --> 00:04:39,280
мощности мы должны

113
00:04:39,280 --> 00:04:42,080
иметь с современным коммерческим компьютером,

114
00:04:42,080 --> 00:04:44,400
ну, около одного-двух миллионов

115
00:04:44,400 --> 00:04:46,639
часов, и это будет  вполне адекватная

116
00:04:46,639 --> 00:04:48,639
скорость, чтобы справиться со всей

117
00:04:48,639 --> 00:04:50,880
продукцией Советского Союза всего за несколько часов

118
00:04:50,880 --> 00:04:52,880
компьютерного времени в неделю, когда вы

119
00:04:52,880 --> 00:04:54,880
должны уметь  достичь скорости, если наши

120
00:04:54,880 --> 00:04:57,199
эксперименты пройдут хорошо, то, возможно, в течение

121
00:04:57,199 --> 00:04:59,440
пяти лет или около того, и, наконец, мистер

122
00:04:59,440 --> 00:05:02,160
Макдэниэл означает конец человеческим

123
00:05:02,160 --> 00:05:04,720
переводчикам. Это да для

124
00:05:04,720 --> 00:05:06,400
переводчиков научно-технических

125
00:05:06,400 --> 00:05:08,400
материалов, но что касается поэзии и

126
00:05:08,400 --> 00:05:10,080
романов, нет, я не  Думаю, мы когда-нибудь

127
00:05:10,080 --> 00:05:12,080
заменим переводчиков такого рода

128
00:05:12,080 --> 00:05:15,440
материалов. Мистер Макдэниел, большое спасибо,

129
00:05:15,440 --> 00:05:17,520
но, несмотря на шумиху, у него возникли серьезные

130
00:05:17,520 --> 00:05:19,840
проблемы.

131
00:05:20,320 --> 00:05:22,240


132
00:05:22,240 --> 00:05:25,280


133
00:05:25,280 --> 00:05:27,199


134
00:05:27,199 --> 00:05:30,000


135
00:05:30,000 --> 00:05:32,960
Ранние

136
00:05:32,960 --> 00:05:34,320


137
00:05:34,320 --> 00:05:36,960
работы не очень хорошо сработали, я имею в виду, что это было

138
00:05:36,960 --> 00:05:38,720
действительно начало

139
00:05:38,720 --> 00:05:41,360
компьютерной эры в 1950-х, но это

140
00:05:41,360 --> 00:05:44,400
также было началом, как вы знаете, люди

141
00:05:44,400 --> 00:05:46,320
начали понимать науку о

142
00:05:46,320 --> 00:05:48,560
человеческих языках, область лингвистики.

143
00:05:48,560 --> 00:05:50,720
люди плохо

144
00:05:50,720 --> 00:05:53,280
понимали ни одну из сторон

145
00:05:53,280 --> 00:05:56,479
происходящего, так что у вас было то, что люди

146
00:05:56,479 --> 00:05:58,960
пытаются писать системы на действительно

147
00:05:58,960 --> 00:06:01,840
невероятно примитивных компьютерах.

148
00:06:01,840 --> 00:06:04,720
вероятно, случай,

149
00:06:04,720 --> 00:06:07,680
если у вас есть блок питания USB, что

150
00:06:07,680 --> 00:06:09,759
у него больше вычислительных мощностей

151
00:06:09,759 --> 00:06:12,000
внутри, чем у компьютеров, которые

152
00:06:12,000 --> 00:06:15,280
они использовали для перевода um, и так

153
00:06:15,280 --> 00:06:16,960
эффективно то, что вы получали, были

154
00:06:16,960 --> 00:06:20,080
очень простыми системами на основе правил и

155
00:06:20,080 --> 00:06:22,479
поиском слов, поэтому  Было так похоже на словарь

156
00:06:22,479 --> 00:06:24,960
искать слово и получать его перевод,

157
00:06:24,960 --> 00:06:26,880
но это просто не сработало, потому что

158
00:06:26,880 --> 00:06:29,520
человеческие языки намного сложнее,

159
00:06:29,520 --> 00:06:32,319
чем то, что часто слова имеют много значений

160
00:06:32,319 --> 00:06:34,479
и разных смыслов, о чем мы как бы

161
00:06:34,479 --> 00:06:36,639


162
00:06:36,639 --> 00:06:38,639
часто обсуждали там  это идиомы, вам нужно

163
00:06:38,639 --> 00:06:40,240
понимать грамматику, чтобы переписывать

164
00:06:40,240 --> 00:06:43,759
предложения, поэтому по разным причинам это

165
00:06:43,759 --> 00:06:46,800
не сработало, и эта идея была в

166
00:06:46,800 --> 00:06:48,960
значительной степени консервирована, в частности,

167
00:06:48,960 --> 00:06:51,039
в середине 1960-х годов был известный отчет правительства США,

168
00:06:51,039 --> 00:06:54,479
отчет об альпаках, который в основном

169
00:06:54,479 --> 00:06:57,680
заключает это  не работал

170
00:06:57,680 --> 00:06:59,599
ой

171
00:06:59,599 --> 00:07:01,599
хорошо

172
00:07:01,599 --> 00:07:05,280
работа тогда действительно возродилась в искусственном интеллекте при выполнении

173
00:07:05,280 --> 00:07:06,880
основанных на правилах методов машинного

174
00:07:06,880 --> 00:07:10,560
перевода в 90-х гг.

175
00:07:10,560 --> 00:07:13,919


176
00:07:13,919 --> 00:07:17,039
в середине

177
00:07:17,039 --> 00:07:19,039
90-х и когда они были в периоде

178
00:07:19,039 --> 00:07:21,280
статистической НЛП, которую мы видели в других

179
00:07:21,280 --> 00:07:24,000
местах курса,

180
00:07:24,000 --> 00:07:27,840
и тогда возникла идея, можем ли мы начать

181
00:07:27,840 --> 00:07:31,360
с данных о переводе,

182
00:07:31,360 --> 00:07:34,880
то есть предложений и их переводов, и

183
00:07:34,880 --> 00:07:37,280
изучить вероятностную модель, которая может

184
00:07:37,280 --> 00:07:39,280
предсказывать  переводы свежих

185
00:07:39,280 --> 00:07:42,400
предложений, поэтому предположим, что мы переводим с

186
00:07:42,400 --> 00:07:45,440
французского на английский, поэтому мы

187
00:07:45,440 --> 00:07:48,240
хотим построить вероятностную модель,

188
00:07:48,240 --> 00:07:51,039
которая по французскому предложению может сказать,

189
00:07:51,039 --> 00:07:52,479
какова вероятность разных

190
00:07:52,479 --> 00:07:54,720
английских переводов, а затем мы

191
00:07:54,720 --> 00:07:56,840
выберем наиболее вероятный

192
00:07:56,840 --> 00:07:59,520
перевод  гм, тогда мы можем

193
00:07:59,520 --> 00:08:02,240
обнаружить, что удачно было обнаружено, что это было

194
00:08:02,240 --> 00:08:04,800
разбито на два

195
00:08:04,800 --> 00:08:08,000
компонента, просто изменив это с

196
00:08:08,000 --> 00:08:12,160
помощью правила Байеса, поэтому, если бы вместо этого у нас была

197
00:08:12,160 --> 00:08:13,599
вероятность

198
00:08:13,599 --> 00:08:14,639
по

199
00:08:14,639 --> 00:08:17,599
английским предложениям p из y

200
00:08:17,599 --> 00:08:21,199
мм, а затем вероятность французского

201
00:08:21,199 --> 00:08:24,160
предложения с учетом английского предложения  что

202
00:08:24,160 --> 00:08:26,080
люди смогли добиться большего прогресса,

203
00:08:26,080 --> 00:08:28,400
и не сразу понятно,

204
00:08:28,400 --> 00:08:30,240
почему это должно быть, потому что это

205
00:08:30,240 --> 00:08:32,559
просто банальное переписывание с помощью bay  Это

206
00:08:32,559 --> 00:08:34,799
правило позволило

207
00:08:34,799 --> 00:08:37,360
разделить проблему на две части, которые

208
00:08:37,360 --> 00:08:40,958
оказались более решаемыми, поэтому с левой стороны у

209
00:08:40,958 --> 00:08:43,679
вас фактически была модель перевода, в

210
00:08:43,679 --> 00:08:45,920
которой вы могли просто указать

211
00:08:45,920 --> 00:08:49,760
вероятность того, что слова или фразы будут

212
00:08:49,760 --> 00:08:52,959
переведены между двумя языками,

213
00:08:52,959 --> 00:08:54,720
не имея  чтобы позаботиться о

214
00:08:54,720 --> 00:08:57,519
структурном порядке слов в языках,

215
00:08:57,519 --> 00:08:59,680
а затем справа вы увидели

216
00:08:59,680 --> 00:09:02,640
именно то, с чем мы потратили много времени на

217
00:09:02,640 --> 00:09:04,800
прошлой неделе, а это всего лишь

218
00:09:04,800 --> 00:09:07,600
вероятностная языковая модель, поэтому, если у

219
00:09:07,600 --> 00:09:10,480
нас есть очень хорошая модель того, какой хороший

220
00:09:10,480 --> 00:09:13,360
свободный английский  предложения звучат

221
00:09:13,360 --> 00:09:14,959
так, как мы можем построить только из

222
00:09:14,959 --> 00:09:18,560
одноязычных данных, которые мы затем можем получить, чтобы

223
00:09:18,560 --> 00:09:20,800
убедиться, что мы создаем предложения, которые

224
00:09:20,800 --> 00:09:23,600
звучат хорошо, в то время как модель перевода,

225
00:09:23,600 --> 00:09:27,680
надеюсь, вставляет в них правильные слова,

226
00:09:27,920 --> 00:09:29,040
так как

227
00:09:29,040 --> 00:09:31,360
же нам изучить модель перевода, если

228
00:09:31,360 --> 00:09:33,760
у нас нет?  т охватил это, поэтому отправной

229
00:09:33,760 --> 00:09:36,160
точкой было получение большого количества

230
00:09:36,160 --> 00:09:39,200
параллельных данных, которые являются предложениями, переведенными человеком,

231
00:09:39,200 --> 00:09:42,160
и этот момент является обязательным,

232
00:09:42,160 --> 00:09:44,240
чтобы я показал конкретный  Это

233
00:09:44,240 --> 00:09:46,959
изображение розеттского камня, который является знаменитым

234
00:09:46,959 --> 00:09:50,240
оригинальным фрагментом параллельных данных, который

235
00:09:50,240 --> 00:09:51,279


236
00:09:51,279 --> 00:09:54,880
позволил расшифровать египетские иероглифы,

237
00:09:54,880 --> 00:09:57,519
потому что в нем был один и тот же фрагмент текста на

238
00:09:57,519 --> 00:10:00,880
разных языках в современном мире,

239
00:10:00,880 --> 00:10:02,800
которые, к счастью, для людей, которые

240
00:10:02,800 --> 00:10:04,720
создают системы обработки естественного языка,

241
00:10:04,720 --> 00:10:07,600
довольно  несколько мест, где

242
00:10:07,600 --> 00:10:10,160
параллельные данные производятся в больших

243
00:10:10,160 --> 00:10:13,040
количествах, поэтому европейский союз

244
00:10:13,040 --> 00:10:16,480
производит огромное количество параллельного текста

245
00:10:16,480 --> 00:10:19,200
на европейских языках,

246
00:10:19,200 --> 00:10:20,800


247
00:10:20,800 --> 00:10:23,279
извините, французском, а не французском Канадский

248
00:10:23,279 --> 00:10:25,760
парламент удобно производит

249
00:10:25,760 --> 00:10:27,600
параллельный текст на

250
00:10:27,600 --> 00:10:30,640
французском и английском языках и даже

251
00:10:30,640 --> 00:10:32,800
в ограниченном количестве в институте

252
00:10:32,800 --> 00:10:34,640
канадский

253
00:10:34,640 --> 00:10:36,560
эскимос,

254
00:10:36,560 --> 00:10:38,880
а затем парламент

255
00:10:38,880 --> 00:10:42,000
гонконга выпускает английский и китайский языки, поэтому

256
00:10:42,000 --> 00:10:44,079
есть достаточная доступность из

257
00:10:44,079 --> 00:10:46,560
разных источников, и мы можем использовать это для

258
00:10:46,560 --> 00:10:48,640
построения моделей,

259
00:10:48,640 --> 00:10:51,440
так как мы это делаем, хотя все, что у нас есть, это

260
00:10:51,440 --> 00:10:53,839
эти предложения, и не совсем

261
00:10:53,839 --> 00:10:56,079
очевидно, как построить  вероятностная

262
00:10:56,079 --> 00:10:57,680
модель из них,

263
00:10:57,680 --> 00:11:00,399
как и раньше, что мы хотим сделать, это

264
00:11:00,399 --> 00:11:04,320
сломать это  проблема вниз, поэтому в этом случае

265
00:11:04,320 --> 00:11:06,640
мы собираемся ввести

266
00:11:06,640 --> 00:11:09,600
дополнительную переменную, которая является переменной выравнивания,

267
00:11:09,600 --> 00:11:13,120
так что a - это переменная выравнивания,

268
00:11:13,120 --> 00:11:15,920
которая будет давать соответствие на уровне слова или

269
00:11:15,920 --> 00:11:18,959
иногда на уровне фразы

270
00:11:18,959 --> 00:11:21,680
между частями исходного предложения

271
00:11:21,680 --> 00:11:23,519
и  целевое предложение,

272
00:11:23,519 --> 00:11:27,040
так что это пример согласования,

273
00:11:27,040 --> 00:11:30,560
и поэтому, если бы мы могли вызвать это согласование

274
00:11:30,560 --> 00:11:32,560
между двумя

275
00:11:32,560 --> 00:11:35,440
предложениями, тогда у нас могли бы быть

276
00:11:35,440 --> 00:11:38,800
части вероятности того, насколько вероятно

277
00:11:38,800 --> 00:11:41,760
слово или короткая фраза

278
00:11:41,760 --> 00:11:44,720
переводится определенным образом,

279
00:11:45,200 --> 00:11:47,200
и

280
00:11:47,200 --> 00:11:50,079
в целом вы знаете

281
00:11:50,079 --> 00:11:52,880
выравнивание определяет соответствие между словами

282
00:11:52,880 --> 00:11:54,639
, которое

283
00:11:54,639 --> 00:11:56,959
фиксирует грамматические различия

284
00:11:56,959 --> 00:12:00,480
между языками, поэтому слова будут встречаться в

285
00:12:00,480 --> 00:12:02,560
разном порядке в разных языках в

286
00:12:02,560 --> 00:12:04,720
зависимости от того, в каком

287
00:12:04,720 --> 00:12:07,680
языке предмет ставится перед глаголом,

288
00:12:07,680 --> 00:12:10,160
или предмет после глагола или

289
00:12:10,160 --> 00:12:12,000
глагол перед  и субъект, и

290
00:12:12,000 --> 00:12:13,680
объект,

291
00:12:13,680 --> 00:12:15,600
и выравнивания также уловят

292
00:12:15,600 --> 00:12:17,760
кое-что о различиях в

293
00:12:17,760 --> 00:12:20,480
том, как слово «язык»  Мы делаем что-то, поэтому

294
00:12:20,480 --> 00:12:22,839
мы находим, что у нас есть все

295
00:12:22,839 --> 00:12:25,760
возможности того, как слова могут быть согласованы

296
00:12:25,760 --> 00:12:29,839
между языками, чтобы у вас могли быть слова,

297
00:12:29,839 --> 00:12:33,040
которые вообще не переводятся на

298
00:12:33,040 --> 00:12:34,560
другой язык,

299
00:12:34,560 --> 00:12:36,240
поэтому на французском языке

300
00:12:36,240 --> 00:12:39,040
вы помещаете определенную статью перед

301
00:12:39,040 --> 00:12:42,160
названиями стран, например  Япония, поэтому, когда это

302
00:12:42,160 --> 00:12:44,000
переводится на английский, вы просто получаете

303
00:12:44,000 --> 00:12:47,360
Japan, поэтому нет перевода,

304
00:12:47,360 --> 00:12:49,200
поэтому он просто уходит.

305
00:12:49,200 --> 00:12:52,720


306
00:12:52,720 --> 00:12:55,440


307
00:12:55,440 --> 00:12:57,040


308
00:12:57,040 --> 00:13:01,200


309
00:13:01,200 --> 00:13:02,800


310
00:13:02,800 --> 00:13:04,480
последнее французское слово

311
00:13:04,480 --> 00:13:07,200
переводится как аборигены как

312
00:13:07,200 --> 00:13:08,959
несколько слов,

313
00:13:08,959 --> 00:13:11,760
вы можете получить обратное, где у вас может

314
00:13:11,760 --> 00:13:14,880
быть несколько французских слов, которые

315
00:13:14,880 --> 00:13:18,440
переводятся как одно английское слово, поэтому в

316
00:13:18,440 --> 00:13:21,200
приложении Nissan реализованы переводчики,

317
00:13:21,200 --> 00:13:22,800


318
00:13:22,800 --> 00:13:24,720
и вы можете получить, но еще более

319
00:13:24,720 --> 00:13:28,480
сложный, мм  Итак, здесь у нас

320
00:13:28,480 --> 00:13:29,600
есть

321
00:13:29,600 --> 00:13:32,160
четыре английских слова, которые переводятся как

322
00:13:32,160 --> 00:13:34,959
два французских слова, но они на самом деле не

323
00:13:34,959 --> 00:13:37,680
ломаются и хорошо переводят друг друга,

324
00:13:37,680 --> 00:13:39,600
я имею в виду, что это не только  случаются в

325
00:13:39,600 --> 00:13:42,320
разных языках, они также случаются

326
00:13:42,320 --> 00:13:43,680
в языке, когда вы по-

327
00:13:43,680 --> 00:13:46,560
разному говорите одно и то же, так что другой

328
00:13:46,560 --> 00:13:47,360
способ, которым

329
00:13:47,360 --> 00:13:50,000
вы могли бы выразиться, э-э, бедные, у

330
00:13:50,000 --> 00:13:52,639
которых нет денег, - это сказать, что у бедных

331
00:13:52,639 --> 00:13:55,199
нет денег, и это гораздо больше

332
00:13:55,199 --> 00:13:58,240
похоже на то, как

333
00:13:58,240 --> 00:14:00,639
здесь отображается французский, поэтому даже с английского на

334
00:14:00,639 --> 00:14:02,240
английский у вас есть такая же

335
00:14:02,240 --> 00:14:05,279
проблема с выравниванием,

336
00:14:05,279 --> 00:14:07,600
поэтому в вероятностном или статистическом

337
00:14:07,600 --> 00:14:10,000
машинном переводе более широко

338
00:14:10,000 --> 00:14:11,279
известно,

339
00:14:11,279 --> 00:14:14,000
что мы хотели сделать, это изучить эти

340
00:14:14,000 --> 00:14:15,360
выравнивания,

341
00:14:15,360 --> 00:14:17,519
и есть куча источников

342
00:14:17,519 --> 00:14:19,440
информации, которые вы могли бы  используйте, если вы начинаете

343
00:14:19,440 --> 00:14:21,920
с параллельных предложений,

344
00:14:21,920 --> 00:14:24,320
вы можете увидеть, как часто слова и

345
00:14:24,320 --> 00:14:27,600
фразы встречаются в параллельных предложениях, вы можете

346
00:14:27,600 --> 00:14:30,959
посмотреть на их позиции в предложении

347
00:14:30,959 --> 00:14:31,760
и

348
00:14:31,760 --> 00:14:34,000
выяснить, какие правильные

349
00:14:34,000 --> 00:14:36,639
выравнивания, но выравнивания - это

350
00:14:36,639 --> 00:14:39,440
категориальная вещь, они не

351
00:14:39,440 --> 00:14:42,480
вероятностные и поэтому  они являются скрытыми

352
00:14:42,480 --> 00:14:44,560
переменными, поэтому вам необходимо использовать специальные

353
00:14:44,560 --> 00:14:46,639
алгоритмы обучения, такие как

354
00:14:46,639 --> 00:14:49,120
алгоритм максимизации ожидания, для

355
00:14:49,120 --> 00:14:51,839
изучения скрытых переменных va.  riables в былые

356
00:14:51,839 --> 00:14:54,480
дни cs224n, прежде чем мы начали делать все это

357
00:14:54,480 --> 00:14:56,839
с помощью глубокого обучения, мы потратили тонны

358
00:14:56,839 --> 00:14:58,959
cs224 на

359
00:14:58,959 --> 00:15:02,240
работу с алгоритмами скрытых переменных,

360
00:15:02,240 --> 00:15:03,920
но в наши дни мы вообще не рассматриваем это,

361
00:15:03,920 --> 00:15:05,519
и вам придется пойти

362
00:15:05,519 --> 00:15:08,639
и посмотреть  cs228, если вы хотите узнать больше

363
00:15:08,639 --> 00:15:10,560
об этом, и вы знаете, что мы на самом деле не

364
00:15:10,560 --> 00:15:12,639
ожидаем, что вы поймете детали

365
00:15:12,639 --> 00:15:15,839
здесь, но тогда я хотел сказать немного

366
00:15:15,839 --> 00:15:20,959
больше о том, как было выполнено декодирование

367
00:15:20,959 --> 00:15:23,600
в системе статистического машинного перевода,

368
00:15:23,600 --> 00:15:27,360
так что мы  хотел сделать, это

369
00:15:27,360 --> 00:15:29,839
сказать, что у нас есть модель перевода и

370
00:15:29,839 --> 00:15:33,600
языковая модель, и мы хотим

371
00:15:33,600 --> 00:15:36,160
выбрать наиболее вероятную причину

372
00:15:36,160 --> 00:15:38,959
перевода предложения и

373
00:15:38,959 --> 00:15:42,000
какой процесс мы могли бы использовать для этого ...

374
00:15:42,000 --> 00:15:44,320
ну,

375
00:15:44,320 --> 00:15:46,880
вы знаете, что наивная вещь  чтобы сказать хорошо,

376
00:15:46,880 --> 00:15:50,399
давайте просто перечислим все возможные y

377
00:15:50,399 --> 00:15:52,720
и вычислим их вероятность, но мы

378
00:15:52,720 --> 00:15:55,360
не можем этого сделать, потому что есть

379
00:15:55,360 --> 00:15:56,880
количество

380
00:15:56,880 --> 00:15:59,680
предложений перевода на целевом

381
00:15:59,680 --> 00:16:01,440
языке, которое экспоненциально зависит от

382
00:16:01,440 --> 00:16:04,000
длины предложения, так что это тоже

383
00:16:04,000 --> 00:16:07,839
e  x дорого, поэтому нам нужно иметь какой-то способ

384
00:16:07,839 --> 00:16:10,240
разбить его больше, и хорошо, у нас был

385
00:16:10,240 --> 00:16:13,600
простой способ для языковых моделей, мы

386
00:16:13,600 --> 00:16:17,120
просто генерировали слова по одному и

387
00:16:17,120 --> 00:16:19,920
выкладывали предложение, и поэтому это

388
00:16:19,920 --> 00:16:22,399
кажется разумным делом, но здесь нам

389
00:16:22,399 --> 00:16:24,399
нужно  чтобы справиться с тем фактом,

390
00:16:24,399 --> 00:16:25,440
что

391
00:16:25,440 --> 00:16:28,959
вещи происходят в разном порядке на

392
00:16:28,959 --> 00:16:33,399
исходных языках и в переводах,

393
00:16:33,839 --> 00:16:35,680
и поэтому мы действительно хотим разбить это на

394
00:16:35,680 --> 00:16:37,759
части с предположением независимости,

395
00:16:37,759 --> 00:16:40,240
как языковая модель, но тогда нам

396
00:16:40,240 --> 00:16:42,720
нужен способ разбить вещи на части и

397
00:16:42,720 --> 00:16:45,279
исследовать их в том, что  называется процесс декодирования,

398
00:16:45,279 --> 00:16:46,399


399
00:16:46,399 --> 00:16:49,279
так что именно так это и было сделано, поэтому мы

400
00:16:49,279 --> 00:16:51,519
начнем с исходного предложения, так что

401
00:16:51,519 --> 00:16:54,000
это немецкое предложение,

402
00:16:54,000 --> 00:16:56,000
и, как это стандартно

403
00:16:56,000 --> 00:16:57,360


404
00:16:57,360 --> 00:16:59,759
для немецкого языка, вы получаете

405
00:16:59,759 --> 00:17:02,560
этот глагол второй позиции,

406
00:17:02,560 --> 00:17:04,720
поэтому он, вероятно, не в правильном

407
00:17:04,720 --> 00:17:06,640
положении  где будет английский

408
00:17:06,640 --> 00:17:08,720
перевод, поэтому нам может

409
00:17:08,720 --> 00:17:12,559
потребоваться переставить слова так, чтобы то, что у нас

410
00:17:12,559 --> 00:17:16,079
было, было основано на модели перевода,

411
00:17:16,079 --> 00:17:17,039
у нас есть

412
00:17:17,039 --> 00:17:20,559
слова или фразы, которые с достаточной

413
00:17:20,559 --> 00:17:24,799
вероятностью являются переводами каждого немецкого языка

414
00:17:24,799 --> 00:17:28,079
слово или иногда немецкая фраза, так

415
00:17:28,079 --> 00:17:30,960
что это фактически части лего

416
00:17:30,960 --> 00:17:32,960
, из которых мы

417
00:17:32,960 --> 00:17:35,840
собираемся создать перевод,

418
00:17:35,840 --> 00:17:39,200
а затем внутри того, что мы собираемся

419
00:17:39,200 --> 00:17:42,559
использовать эти данные, мы собираемся

420
00:17:42,559 --> 00:17:45,679
сгенерировать часть перевода  по частям,

421
00:17:45,679 --> 00:17:47,840
как мы это делали с нашими новыми

422
00:17:47,840 --> 00:17:50,559
языковыми моделями, поэтому мы собираемся начать

423
00:17:50,559 --> 00:17:53,360
с пустого перевода, а затем мы

424
00:17:53,360 --> 00:17:56,480
собираемся сказать, что мы хотим использовать одну из

425
00:17:56,480 --> 00:17:59,840
этих частей лего, и чтобы мы могли

426
00:17:59,840 --> 00:18:02,000
изучить различные возможные, так что

427
00:18:02,000 --> 00:18:04,000
есть процесс поиска, но одна из

428
00:18:04,000 --> 00:18:06,799
возможных частей - мы могли бы переводить er

429
00:18:06,799 --> 00:18:08,320
с помощью he,

430
00:18:08,320 --> 00:18:10,559
или мы могли бы начать предложение

431
00:18:10,559 --> 00:18:13,600
с r, переводя второе слово, чтобы мы

432
00:18:13,600 --> 00:18:14,960
могли исследовать

433
00:18:14,960 --> 00:18:17,760
различные вероятные возможности, и если

434
00:18:17,760 --> 00:18:20,240
мы будем руководствоваться нашей языковой моделью, это,

435
00:18:20,240 --> 00:18:22,559
вероятно, намного больше  скорее всего, начнем

436
00:18:22,559 --> 00:18:24,880
предложение с он, чем начнем

437
00:18:24,880 --> 00:18:26,640
предложение с r, хотя r не

438
00:18:26,640 --> 00:18:28,880
невозможно, хорошо, а затем

439
00:18:28,880 --> 00:18:30,640
мы делаем еще кое-что, что мы делаем с этими маленькими

440
00:18:30,640 --> 00:18:32,480
черными пятнами вверху, которые мы как бы

441
00:18:32,480 --> 00:18:34,799
записываем, которые  немецкие слова, которые мы

442
00:18:34,799 --> 00:18:38,000
перевели, и поэтому мы продвигаемся вперед в

443
00:18:38,000 --> 00:18:39,120


444
00:18:39,120 --> 00:18:42,960
процессе перевода,

445
00:18:42,960 --> 00:18:45,200
и мы могли бы решить, что мы можем

446
00:18:45,200 --> 00:18:48,799
перевести следующим, когда идет второе слово,

447
00:18:48,799 --> 00:18:51,919
или мы можем перевести отрицание здесь

448
00:18:51,919 --> 00:18:54,480
и перевести его, что не происходит,

449
00:18:54,480 --> 00:18:57,679
когда мы исследуем различные продолжения

450
00:18:57,679 --> 00:18:59,280
и в процессе  Я расскажу

451
00:18:59,280 --> 00:19:01,360
более подробно позже, когда мы сделаем нейронный

452
00:19:01,360 --> 00:19:04,160
эквивалент, мы как бы делаем этот поиск,

453
00:19:04,160 --> 00:19:07,679
где мы исследуем вероятные переводы и

454
00:19:07,679 --> 00:19:10,240
сокращаем, и в конечном итоге мы

455
00:19:10,240 --> 00:19:12,640
перевели все входное предложение и

456
00:19:12,640 --> 00:19:15,039
разработали довольно вероятный перевод, который

457
00:19:15,039 --> 00:19:17,200
он не делает  иди домой, и это то, что

458
00:19:17,200 --> 00:19:20,720
мы будем использовать в качестве перевода,

459
00:19:20,960 --> 00:19:22,559
хорошо,

460
00:19:22,559 --> 00:19:26,960
так что в период с

461
00:19:27,240 --> 00:19:32,880
1997 по 2013

462
00:19:32,880 --> 00:19:35,840
год статистический машинный перевод был

463
00:19:35,840 --> 00:19:38,480
огромной областью исследований,

464
00:19:38,480 --> 00:19:41,679
лучшие системы были чрезвычайно сложными,

465
00:19:41,679 --> 00:19:44,080
и в них были сотни деталей, которых я

466
00:19:44,080 --> 00:19:46,240
определенно не знал ''  Здесь упоминалось, что

467
00:19:46,240 --> 00:19:48,880
системы имеют множество отдельно разработанных

468
00:19:48,880 --> 00:19:51,919
и построенных компонентов, поэтому я упомянул

469
00:19:51,919 --> 00:19:54,240
языковую модель и модель перевода,

470
00:19:54,240 --> 00:19:56,400
но у них было много других  Компоненты

471
00:19:56,400 --> 00:19:58,799
для переупорядочения моделей и моделей перегиба

472
00:19:58,799 --> 00:20:01,360
и прочего там было

473
00:20:01,360 --> 00:20:03,520
много инженерии функций,

474
00:20:03,520 --> 00:20:06,559
как правило, модели также использовали

475
00:20:06,559 --> 00:20:09,840
много дополнительных ресурсов, и

476
00:20:09,840 --> 00:20:13,440
было много человеческих усилий для поддержки,

477
00:20:13,440 --> 00:20:15,200
но, тем не менее, они уже были

478
00:20:15,200 --> 00:20:18,720
довольно успешными, так что google translate

479
00:20:18,720 --> 00:20:22,159
был запущен в середине 2000-х, и люди

480
00:20:22,159 --> 00:20:24,080
думали, что это потрясающе, вы можете

481
00:20:24,080 --> 00:20:26,960
начать получать полу-приличные

482
00:20:26,960 --> 00:20:29,840
автоматические переводы

483
00:20:29,840 --> 00:20:32,080
для разных веб-страниц,

484
00:20:32,080 --> 00:20:33,360
но

485
00:20:33,360 --> 00:20:36,480
это продвигалось достаточно хорошо, а

486
00:20:36,480 --> 00:20:39,520
затем мы добрались до 2014 года,

487
00:20:39,520 --> 00:20:43,120
и действительно с огромной неожиданностью.

488
00:20:43,120 --> 00:20:47,120
Затем люди разработали способы выполнения

489
00:20:47,120 --> 00:20:50,320
машинного перевода с использованием большой нейронной

490
00:20:50,320 --> 00:20:53,919
сети, и эти большие нейронные сети

491
00:20:53,919 --> 00:20:56,799
оказались просто чрезвычайно успешными

492
00:20:56,799 --> 00:20:59,360
и в значительной степени уничтожили все, что им

493
00:20:59,360 --> 00:21:02,000
предшествовало, поэтому в следующей большой

494
00:21:02,000 --> 00:21:04,960
части лекции я бы хотел сделать

495
00:21:04,960 --> 00:21:07,200
это говорит вам что-то о новом

496
00:21:07,200 --> 00:21:09,600
машинном переводе,

497
00:21:09,600 --> 00:21:12,159
нейронный машинный перевод,

498
00:21:12,159 --> 00:21:14,240
это означает, что вы используете новую сеть

499
00:21:14,240 --> 00:21:16,559
для выполнения машинного перевода.  ne перевод, но на

500
00:21:16,559 --> 00:21:19,039
практике это означает немного больше, чем

501
00:21:19,039 --> 00:21:22,400
то, что мы собираемся построить

502
00:21:22,400 --> 00:21:23,360


503
00:21:23,360 --> 00:21:26,799
одну очень большую нейронную сеть, которая

504
00:21:26,799 --> 00:21:29,919
полностью выполняет непрерывный перевод,

505
00:21:29,919 --> 00:21:31,520
поэтому у нас будет большая нейронная

506
00:21:31,520 --> 00:21:33,440
сеть, которую мы собираемся кормить  в

507
00:21:33,440 --> 00:21:35,919
исходном предложении во входные, и

508
00:21:35,919 --> 00:21:37,679
то, что выйдет на

509
00:21:37,679 --> 00:21:39,760
выходе нейронной сети,

510
00:21:39,760 --> 00:21:42,720
- это перевод предложения, которое мы

511
00:21:42,720 --> 00:21:45,360
собираемся обучить эту модель от начала до конца на

512
00:21:45,360 --> 00:21:49,039
параллельных предложениях, и это вся

513
00:21:49,039 --> 00:21:51,600
система, а не  много

514
00:21:51,600 --> 00:21:53,360
отдельных компонентов, как в

515
00:21:53,360 --> 00:21:56,159
старомодной системе машинного перевода,

516
00:21:56,159 --> 00:21:58,960
и мы увидим, что через некоторое время эти

517
00:21:58,960 --> 00:22:01,919
архитектуры нейронных сетей называются

518
00:22:01,919 --> 00:22:04,159
последовательными моделями или обычно

519
00:22:04,159 --> 00:22:07,919
сокращенно ищут гм и

520
00:22:07,919 --> 00:22:08,720
гм,

521
00:22:08,720 --> 00:22:10,080
они включают в себя

522
00:22:10,080 --> 00:22:12,240
две нейронные сети.  говорит, что два

523
00:22:12,240 --> 00:22:14,880
rnn версия, которую я представляю, теперь

524
00:22:14,880 --> 00:22:17,039
имеет два rnn, но в более общем плане они

525
00:22:17,039 --> 00:22:19,679
включают две нейронные сети, есть

526
00:22:19,679 --> 00:22:22,480
одна нейронная сеть, которая будет

527
00:22:22,480 --> 00:22:25,039
кодировать исходное предложение, поэтому, если у нас

528
00:22:25,039 --> 00:22:27,440
есть су  Здесь

529
00:22:27,440 --> 00:22:30,320
мы собираемся закодировать это предложение, и

530
00:22:30,320 --> 00:22:32,240
мы хорошо знаем способ, которым мы можем это сделать

531
00:22:32,240 --> 00:22:33,280
,

532
00:22:33,280 --> 00:22:36,559
поэтому, используя тип lstms, который мы видели в

533
00:22:36,559 --> 00:22:39,760
прошлом классе, мы можем начать с самого начала

534
00:22:39,760 --> 00:22:40,480
и

535
00:22:40,480 --> 00:22:42,960
пройти предложение и обновить

536
00:22:42,960 --> 00:22:45,600
скрытое состояние  каждый раз, и это

537
00:22:45,600 --> 00:22:49,120
даст нам представление о

538
00:22:49,120 --> 00:22:52,000
содержании исходного предложения, так

539
00:22:52,000 --> 00:22:54,799
что это первая модель последовательности

540
00:22:54,799 --> 00:22:58,400
um, которая кодирует исходное предложение, и

541
00:22:58,400 --> 00:23:01,520
мы будем использовать идею о том, что окончательное скрытое

542
00:23:01,520 --> 00:23:04,880
состояние кодировщика

543
00:23:04,880 --> 00:23:07,360
rnn будет

544
00:23:07,360 --> 00:23:10,240
в смысле представлять  эм исходное

545
00:23:10,240 --> 00:23:12,480
предложение, и мы собираемся передать его

546
00:23:12,480 --> 00:23:15,200
косвенно в качестве начального скрытого состояния

547
00:23:15,200 --> 00:23:18,240
для декодера rnn, поэтому на другой

548
00:23:18,240 --> 00:23:20,159
стороне изображения у нас есть наш декодер

549
00:23:20,159 --> 00:23:21,679
rnn,

550
00:23:21,679 --> 00:23:23,760
и это языковая модель, которая

551
00:23:23,760 --> 00:23:26,080
будет генерировать целевое предложение,

552
00:23:26,080 --> 00:23:28,720
обусловленное

553
00:23:28,720 --> 00:23:30,400


554
00:23:30,400 --> 00:23:34,240
окончательное скрытое состояние кодировщика rnn, поэтому

555
00:23:34,240 --> 00:23:36,400
мы собираемся начать с ввода

556
00:23:36,400 --> 00:23:38,799
начального символа, который мы собираемся передать в

557
00:23:38,799 --> 00:23:42,080
скрытом состоянии из кодировщика rnn, и

558
00:23:42,080 --> 00:23:45,039
теперь этот второй зеленый rnn

559
00:23:45,039 --> 00:23:47,120
имеет полностью отдельный па  параметры, которые я

560
00:23:47,120 --> 00:23:49,919
мог бы просто подчеркнуть, но мы выполняем те же

561
00:23:49,919 --> 00:23:54,000
вычисления lstm и генерируем

562
00:23:54,000 --> 00:23:56,320
первое слово предложения, которое он,

563
00:23:56,320 --> 00:24:00,480
а затем, выполняя генерацию lstm,

564
00:24:00,480 --> 00:24:03,200
как и в предыдущем классе, мы копируем это в качестве

565
00:24:03,200 --> 00:24:04,720
следующего ввода, который

566
00:24:04,720 --> 00:24:07,360
мы запускаем на следующем шаге lstm

567
00:24:07,360 --> 00:24:10,240
generate  другое слово нажмите, скопируйте его

568
00:24:10,240 --> 00:24:12,960
и продолжайте, и

569
00:24:12,960 --> 00:24:14,720
мы

570
00:24:14,720 --> 00:24:17,520
перевели предложение правильно, так что это

571
00:24:17,520 --> 00:24:21,919
показывает поведение во время тестирования,

572
00:24:21,919 --> 00:24:22,720
когда

573
00:24:22,720 --> 00:24:25,279
мы генерируем следующее предложение для

574
00:24:25,279 --> 00:24:28,159
поведения во время обучения, когда у нас есть

575
00:24:28,159 --> 00:24:31,520
параллельные предложения, которые мы все еще используем  тот

576
00:24:31,520 --> 00:24:34,720
же тип последовательности для модели последовательности,

577
00:24:34,720 --> 00:24:37,440
но мы делаем это с помощью части декодера,

578
00:24:37,440 --> 00:24:40,799
точно так же, как um обучение языковой модели,

579
00:24:40,799 --> 00:24:42,640
где мы хотим

580
00:24:42,640 --> 00:24:45,120
заставить учителя и предсказывать каждое слово,

581
00:24:45,120 --> 00:24:47,200
которое действительно встречается в предложении исходного

582
00:24:47,200 --> 00:24:50,080
языка

583
00:24:50,080 --> 00:24:53,360
um последовательностей моделей последовательностей  были

584
00:24:53,360 --> 00:24:54,880
невероятно

585
00:24:54,880 --> 00:24:56,080
мощной и

586
00:24:56,080 --> 00:25:00,240
широко используемой рабочей лошадкой в новых нейронных

587
00:25:00,240 --> 00:25:03,440
сетях для nlp, поэтому, хотя

588
00:25:03,440 --> 00:25:05,520
вы знаете, что исторически

589
00:25:05,520 --> 00:25:08,400
машинный перевод был их первым большим

590
00:25:08,400 --> 00:25:10,400
применением и в

591
00:25:10,400 --> 00:25:12,880
некотором роде  каноническое использование, они также используются

592
00:25:12,880 --> 00:25:15,279
повсюду,

593
00:25:15,279 --> 00:25:16,000
поэтому

594
00:25:16,000 --> 00:25:18,880
вы можете выполнять за них множество других задач nlp,

595
00:25:18,880 --> 00:25:21,039
чтобы вы могли делать резюмирование, вы можете

596
00:25:21,039 --> 00:25:23,360
думать о резюмировании текста как о

597
00:25:23,360 --> 00:25:26,400
переводе длинного текста в короткий

598
00:25:26,400 --> 00:25:27,840
текст,

599
00:25:27,840 --> 00:25:29,760
но вы можете использовать их для других вещей

600
00:25:29,760 --> 00:25:31,840
, которые  ни в коем случае не перевод,

601
00:25:31,840 --> 00:25:35,200
поэтому они обычно используются

602
00:25:35,200 --> 00:25:38,799
для нейронных диалоговых систем, поэтому

603
00:25:38,799 --> 00:25:40,480
кодировщик

604
00:25:40,480 --> 00:25:43,520
будет кодировать um, как говорят предыдущие два

605
00:25:43,520 --> 00:25:46,400
высказывания, а затем вы будете использовать

606
00:25:46,400 --> 00:25:49,679
декодер для генерации трендов шеи звезды

607
00:25:49,679 --> 00:25:50,480
um

608
00:25:50,480 --> 00:25:52,240
некоторые другие применения

609
00:25:52,240 --> 00:25:54,400
еще более причудливы,

610
00:25:54,400 --> 00:25:57,360
но доказали  чтобы быть довольно успешным,

611
00:25:57,360 --> 00:25:58,159


612
00:25:58,159 --> 00:25:58,960
так что

613
00:25:58,960 --> 00:26:02,960
если у вас есть способ представить

614
00:26:02,960 --> 00:26:05,840
пути предложения в виде строки,

615
00:26:05,840 --> 00:26:08,159
и если вы немного подумаете,

616
00:26:08,159 --> 00:26:11,200
довольно очевидно, как вы можете превратить

617
00:26:11,200 --> 00:26:13,600
пути предложения в строку,

618
00:26:13,600 --> 00:26:16,559
просто используя дополнительные  синтаксис, такой как

619
00:26:16,559 --> 00:26:20,480
круглые скобки, или вставка явных слов

620
00:26:20,480 --> 00:26:22,159
, которые говорят, что

621
00:26:22,159 --> 00:26:25,919
левая дуга правая дуга um

622
00:26:25,919 --> 00:26:28,080
смещается, как системы переходов, которые

623
00:26:28,080 --> 00:26:30,640
вы использовали для назначения три, тогда

624
00:26:30,640 --> 00:26:32,320
мы могли бы сказать,

625
00:26:32,320 --> 00:26:34,960
давайте использовать кодировщик

626
00:26:34,960 --> 00:26:37,600
f  передать входное предложение кодеру

627
00:26:37,600 --> 00:26:40,080
и позволить ему выводить последовательность переходов

628
00:26:40,080 --> 00:26:43,039
нашего анализатора зависимостей, и

629
00:26:43,039 --> 00:26:44,720
несколько удивительно, что на самом деле он

630
00:26:44,720 --> 00:26:47,840
работает хорошо как еще один способ создания

631
00:26:47,840 --> 00:26:50,159
синтаксического анализатора зависимостей или других видов

632
00:26:50,159 --> 00:26:52,559
синтаксического анализатора,

633
00:26:52,559 --> 00:26:54,400
эти модели также были применены не

634
00:26:54,400 --> 00:26:57,279
только к естественным языкам, но и  на

635
00:26:57,279 --> 00:26:59,360
другие языки,

636
00:26:59,360 --> 00:27:02,000
включая музыку, а также

637
00:27:02,000 --> 00:27:05,120
код языка программирования, чтобы вы могли

638
00:27:05,120 --> 00:27:08,559
обучить систему поиска искать, где она

639
00:27:08,559 --> 00:27:09,840
читается в

640
00:27:09,840 --> 00:27:12,720
псевдокоде на естественном языке и

641
00:27:12,720 --> 00:27:15,200
генерирует код Python, и если у

642
00:27:15,200 --> 00:27:16,720
вас достаточно хороший, он может выполнить

643
00:27:16,720 --> 00:27:19,520
задание за вас

644
00:27:19,520 --> 00:27:21,520


645
00:27:21,520 --> 00:27:24,799
Итак, существенная новая идея здесь с нашей

646
00:27:24,799 --> 00:27:27,200
последовательностью моделей последовательности состоит в том, что у нас

647
00:27:27,200 --> 00:27:29,679
есть пример условных языковых

648
00:27:29,679 --> 00:27:31,760
моделей, поэтому

649
00:27:31,760 --> 00:27:34,080
раньше

650
00:27:34,080 --> 00:27:35,919
мы просто начинали с

651
00:27:35,919 --> 00:27:37,919
начала предложения и

652
00:27:37,919 --> 00:27:40,159
генерировали предложение,

653
00:27:40,159 --> 00:27:43,600
основанное ни на чем, но здесь мы  иметь

654
00:27:43,600 --> 00:27:46,559
что-то, что будет определять или

655
00:27:46,559 --> 00:27:48,960
частично определять, что будет

656
00:27:48,960 --> 00:27:51,279
обусловливать то, что мы должны производить, поэтому у

657
00:27:51,279 --> 00:27:53,520
нас есть источник  предложение, и это

658
00:27:53,520 --> 00:27:56,480
будет строго определять, что является хорошим

659
00:27:56,480 --> 00:27:57,840
переводом,

660
00:27:57,840 --> 00:28:00,799
и поэтому для достижения этого

661
00:28:00,799 --> 00:28:04,000
мы собираемся иметь какой-то способ

662
00:28:04,000 --> 00:28:05,120


663
00:28:05,120 --> 00:28:08,240
передачи информации об

664
00:28:08,240 --> 00:28:11,679
исходном предложении от кодировщика, чтобы

665
00:28:11,679 --> 00:28:15,039
запустить то, что должен делать декодер,

666
00:28:15,039 --> 00:28:17,279
и два стандартных способа  для этого

667
00:28:17,279 --> 00:28:20,640
вы либо подаете в скрытое состояние в

668
00:28:20,640 --> 00:28:23,360
качестве начального скрытого состояния в декодер,

669
00:28:23,360 --> 00:28:26,399
либо иногда вы подаете что-то в

670
00:28:26,399 --> 00:28:29,200
качестве начального ввода в декодер,

671
00:28:29,200 --> 00:28:31,200
и поэтому

672
00:28:31,200 --> 00:28:34,480
в вашем основном переводе мы

673
00:28:34,480 --> 00:28:37,360
напрямую вычисляем эту условную

674
00:28:37,360 --> 00:28:40,080
вероятность модели.  предложения на целевом языке,

675
00:28:40,080 --> 00:28:43,440
заданного предложением исходного языка,

676
00:28:43,440 --> 00:28:46,320
и поэтому на каждом этапе, когда мы разбиваем

677
00:28:46,320 --> 00:28:49,600
генерацию слова за словом, мы

678
00:28:49,600 --> 00:28:52,559
обусловливаем не только предыдущие

679
00:28:52,559 --> 00:28:55,840
слова целевого языка, но и каждый

680
00:28:55,840 --> 00:28:59,520
раз предложение x исходного языка,

681
00:28:59,520 --> 00:29:02,080
поэтому мы  на самом деле знать тонну

682
00:29:02,080 --> 00:29:04,480
больше о том, каким должно быть наше предложение, которое мы

683
00:29:04,480 --> 00:29:06,000
генерируем,

684
00:29:06,000 --> 00:29:08,080
поэтому, если вы посмотрите

685
00:29:08,080 --> 00:29:10,960
на сложности такого рода

686
00:29:10,960 --> 00:29:13,360
условного языка  модели, которые вы

687
00:29:13,360 --> 00:29:15,600
найдете, и вам нравятся числа, которые я показал в прошлый

688
00:29:15,600 --> 00:29:18,880
раз, они обычно почти часто имеют

689
00:29:18,880 --> 00:29:21,360
низкие затруднения, что у вас будут

690
00:29:21,360 --> 00:29:23,600
модели с затруднениями, которые

691
00:29:23,600 --> 00:29:25,919
примерно четыре или даже меньше,

692
00:29:25,919 --> 00:29:28,720
иногда вы знаете 2,5, потому что вы получаете

693
00:29:28,720 --> 00:29:31,039
много информации о том, какие слова вам

694
00:29:31,039 --> 00:29:33,440
следует  генерировать

695
00:29:33,440 --> 00:29:34,880
нормально,

696
00:29:34,880 --> 00:29:37,279
поэтому у нас есть те же вопросы,

697
00:29:37,279 --> 00:29:39,679
что и для языковых моделей в целом, как

698
00:29:39,679 --> 00:29:42,320
обучить систему нейронного машинного перевода

699
00:29:42,320 --> 00:29:45,679
, а затем как использовать ее во время выполнения,

700
00:29:45,679 --> 00:29:48,240
поэтому давайте рассмотрим оба из них

701
00:29:48,240 --> 00:29:50,880
более подробно,

702
00:29:50,880 --> 00:29:54,399
поэтому первый шаг  у нас есть большой

703
00:29:54,399 --> 00:29:57,679
параллельный корпус, поэтому мы бежим, например, в

704
00:29:57,679 --> 00:29:59,360
Европейский союз,

705
00:29:59,360 --> 00:30:02,240
и мы берем много

706
00:30:02,240 --> 00:30:05,120
параллельных данных на английском и французском языках из

707
00:30:05,120 --> 00:30:07,679
заседаний Европейского парламента,

708
00:30:07,679 --> 00:30:09,679
поэтому, когда у нас будут наши параллельные

709
00:30:09,679 --> 00:30:13,679
предложения, что мы собираемся сделать, это

710
00:30:13,679 --> 00:30:15,120
взять

711
00:30:15,120 --> 00:30:19,120
um пакеты исходных предложений и

712
00:30:19,120 --> 00:30:20,960
целевых предложений

713
00:30:20,960 --> 00:30:24,080
будут кодировать исходное предложение с помощью нашего

714
00:30:24,080 --> 00:30:26,399
кодировщика lstm

715
00:30:26,399 --> 00:30:27,200
передаст

716
00:30:27,200 --> 00:30:29,440
его окончательное

717
00:30:29,440 --> 00:30:31,840
скрытое состояние

718
00:30:31,840 --> 00:30:33,440
в целевой

719
00:30:33,440 --> 00:30:37,679
lstm, а на этот раз мы  а теперь

720
00:30:37,679 --> 00:30:42,159
потренируемся слово за словом, сравнивая то, что

721
00:30:42,159 --> 00:30:45,520
он предсказывает, наиболее вероятное

722
00:30:45,520 --> 00:30:47,760
слово, которое будет произведено, с тем, что является

723
00:30:47,760 --> 00:30:50,240
фактическим первым словом, а затем фактическим

724
00:30:50,240 --> 00:30:52,799
вторым словом, и в той степени, в которой

725
00:30:52,799 --> 00:30:54,559
мы ошибаемся,

726
00:30:54,559 --> 00:30:57,600
мы будем страдать от некоторых  потеря, так что

727
00:30:57,600 --> 00:30:58,960
это будет отрицательная логарифмическая

728
00:30:58,960 --> 00:31:01,519
вероятность

729
00:31:01,519 --> 00:31:04,480
генерации правильного следующего слова он

730
00:31:04,480 --> 00:31:06,960
и так далее по предложению, и

731
00:31:06,960 --> 00:31:09,600
так же, как мы видели в прошлый раз для

732
00:31:09,600 --> 00:31:12,080
языковых моделей, мы можем вычислить наши

733
00:31:12,080 --> 00:31:13,679
общие

734
00:31:13,679 --> 00:31:16,240
потери для предложения, выполняя это

735
00:31:16,240 --> 00:31:18,880
стиль принуждения учителя генерирует одно слово за

736
00:31:18,880 --> 00:31:21,519
раз, вычисляет потерю относительно того

737
00:31:21,519 --> 00:31:25,919
слова, которое вы должны были произнести,

738
00:31:26,000 --> 00:31:28,640
и таким образом эта потеря дает нам

739
00:31:28,640 --> 00:31:31,840
информацию, которую мы можем распространить обратно

740
00:31:31,840 --> 00:31:34,399
по всей сети, и самое

741
00:31:34,399 --> 00:31:36,080
важное

742
00:31:36,080 --> 00:31:38,559
в этой последовательности - модели последовательности,

743
00:31:38,559 --> 00:31:40,960
которые имеют  сделали их чрезвычайно

744
00:31:40,960 --> 00:31:44,159
успешными на практике, так как

745
00:31:44,159 --> 00:31:47,200
все это оптимизировано как единая

746
00:31:47,200 --> 00:31:50,799
система от начала до конца, поэтому, начиная с наших

747
00:31:50,799 --> 00:31:52,640
окончательных потерь,

748
00:31:52,640 --> 00:31:55,440
мы распространяем их обратно прямо через

749
00:31:55,440 --> 00:31:58,720
систему.  o мы не только обновляем все

750
00:31:58,720 --> 00:32:02,240
параметры модели декодера, но мы

751
00:32:02,240 --> 00:32:04,720
также обновляем все

752
00:32:04,720 --> 00:32:07,519
параметры модели кодера, которые, в

753
00:32:07,519 --> 00:32:10,559
свою очередь, будут влиять на то, какие условия

754
00:32:10,559 --> 00:32:13,519
будут переданы от кодера к

755
00:32:13,519 --> 00:32:16,000
декодеру,

756
00:32:16,000 --> 00:32:17,039


757
00:32:17,039 --> 00:32:19,440
так что этот момент - хороший момент для меня

758
00:32:19,440 --> 00:32:22,720
чтобы вернуться к трем слайдам, которые

759
00:32:22,720 --> 00:32:25,120
я пропустил, у меня заканчивается время

760
00:32:25,120 --> 00:32:28,480
в конце прошлого раза, а именно, чтобы упомянуть

761
00:32:28,480 --> 00:32:33,679
многослойные сообщения ммм, так что значения, которые

762
00:32:33,679 --> 00:32:36,799
мы рассмотрели до сих пор, уже глубоко

763
00:32:36,799 --> 00:32:40,000
в одном измерении  затем развернуться по

764
00:32:40,000 --> 00:32:43,039
горизонтали на многих временных шагах,

765
00:32:43,039 --> 00:32:45,519
но они были неглубокими в том смысле, что

766
00:32:45,519 --> 00:32:48,159


767
00:32:48,159 --> 00:32:50,960
над нашими предложениями был только один слой повторяющейся структуры,

768
00:32:50,960 --> 00:32:52,880
мы также можем заставить их углубить другое

769
00:32:52,880 --> 00:32:56,960
измерение, применяя несколько

770
00:32:56,960 --> 00:32:59,279
rnn друг над другом, и это дает нам некоторые

771
00:32:59,279 --> 00:33:02,559
многослойный rnn uh часто также называется

772
00:33:02,559 --> 00:33:04,880
сложенным rnn,

773
00:33:04,880 --> 00:33:07,679
и наличие

774
00:33:07,679 --> 00:33:11,120
многослойного rnn позволяет нам

775
00:33:11,120 --> 00:33:15,120
вычислять более сложные представления, поэтому,

776
00:33:15,120 --> 00:33:16,640
проще говоря,

777
00:33:16,640 --> 00:33:19,039
более низкие rnns имеют тенденцию вычислять

778
00:33:19,039 --> 00:33:22,240
функции более низкого уровня, а более высокие  RNN

779
00:33:22,240 --> 00:33:24,880
должны вычислять функции более высокого уровня,

780
00:33:24,880 --> 00:33:27,760
и, как и в других нейронных сетях,

781
00:33:27,760 --> 00:33:30,399
будь то сети прямого распространения

782
00:33:30,399 --> 00:33:32,399
или те сети, которые вы видите в

783
00:33:32,399 --> 00:33:34,559
системах машинного зрения, вы получите

784
00:33:34,559 --> 00:33:38,320
гораздо большую мощность и успех,

785
00:33:38,320 --> 00:33:40,159
имея стек из

786
00:33:40,159 --> 00:33:42,480
нескольких слоев рекуррентных нейронных

787
00:33:42,480 --> 00:33:45,440
сетей.  мог бы подумать,

788
00:33:45,440 --> 00:33:47,679
что есть две вещи, которые я мог бы сделать, я

789
00:33:47,679 --> 00:33:50,159
мог бы иметь один lstm со скрытым

790
00:33:50,159 --> 00:33:53,600
состоянием размерности 2000 или у меня могло бы быть

791
00:33:53,600 --> 00:33:57,279
четыре слоя lstms со скрытым состоянием

792
00:33:57,279 --> 00:33:58,159


793
00:33:58,159 --> 00:34:00,799
500 каждый, и это не должно иметь никакого

794
00:34:00,799 --> 00:34:02,480
значения, потому что я '  У меня

795
00:34:02,480 --> 00:34:04,880
примерно одинаковое количество параметров, но

796
00:34:04,880 --> 00:34:07,600
на практике это не так, это имеет большое

797
00:34:07,600 --> 00:34:10,320
значение, и многослойные или

798
00:34:10,320 --> 00:34:13,199
составные РНН более эффективны,

799
00:34:13,199 --> 00:34:15,040
так

800
00:34:15,040 --> 00:34:17,520
что я могу спросить вас, эм,

801
00:34:17,520 --> 00:34:18,800
есть хороший вопрос студента

802
00:34:18,800 --> 00:34:20,399
о том, какой более низкий уровень или более высокий

803
00:34:20,399 --> 00:34:23,760
уровень  функции означают в этом контексте, конечно,

804
00:34:23,760 --> 00:34:27,199
да, поэтому я имею в виду, что в каком-то смысле

805
00:34:27,199 --> 00:34:31,599
это несколько хлипкие способы,

806
00:34:31,599 --> 00:34:32,879


807
00:34:32,879 --> 00:34:35,119
вы знаете,

808
00:34:35,119 --> 00:34:38,800
термины, это значение не совсем точное, но

809
00:34:38,800 --> 00:34:41,760
обычно то, что  Это означает

810
00:34:41,760 --> 00:34:44,639
, что особенности нижнего уровня и знание

811
00:34:44,639 --> 00:34:48,239
более простых вещей о словах

812
00:34:48,239 --> 00:34:49,040
и

813
00:34:49,040 --> 00:34:52,079
фразах, так что обычно могут быть вещи,

814
00:34:52,079 --> 00:34:56,159
например, какая часть речи является этим словом,

815
00:34:56,159 --> 00:34:59,280
или эти слова являются именем человека

816
00:34:59,280 --> 00:35:01,760
или названием компании,

817
00:35:01,760 --> 00:35:05,520
тогда как более высокий уровень  особенности относятся к

818
00:35:05,520 --> 00:35:08,079
вещам, которые находятся на более высоком семантическом

819
00:35:08,079 --> 00:35:10,960
уровне, поэтому знать больше об общей

820
00:35:10,960 --> 00:35:13,200
структуре предложения, знать

821
00:35:13,200 --> 00:35:15,920
кое-что о том, что это означает,

822
00:35:15,920 --> 00:35:18,359
имеет ли фраза положительные или отрицательные

823
00:35:18,359 --> 00:35:21,760
коннотации; какова

824
00:35:21,760 --> 00:35:24,079
ее семантика, когда вы объединяете

825
00:35:24,079 --> 00:35:26,640
несколько слов в идиоматику  фраза:

826
00:35:26,640 --> 00:35:28,800
ну, грубо говоря, вещи более высокого уровня,

827
00:35:28,800 --> 00:35:31,800


828
00:35:35,359 --> 00:35:37,760
ладно,

829
00:35:38,240 --> 00:35:40,960
забегаем вперед,

830
00:35:41,440 --> 00:35:42,720
ладно, поэтому,

831
00:35:42,720 --> 00:35:44,480
когда мы создаем

832
00:35:44,480 --> 00:35:45,280


833
00:35:45,280 --> 00:35:48,800
одну из этих сквозных

834
00:35:48,800 --> 00:35:51,920
систем нейронного машинного перевода, если мы

835
00:35:51,920 --> 00:35:55,520
хотим, чтобы они работали хорошо,

836
00:35:55,760 --> 00:35:58,400


837
00:35:58,400 --> 00:36:00,160
декодирование однослойного кодировщика lstm в ваших

838
00:36:00,160 --> 00:36:04,000
системах машинного перевода просто не  t работают хорошо,

839
00:36:04,000 --> 00:36:07,200
но вы можете построить что-то не

840
00:36:07,200 --> 00:36:09,200
более сложное, чем модель, которую я

841
00:36:09,200 --> 00:36:12,240
только что объяснил, которая действительно работает очень

842
00:36:12,240 --> 00:36:16,320
хорошо, сделав ее мю  lti-слойная

843
00:36:16,320 --> 00:36:19,760
система нейронного машинного перевода lstm,

844
00:36:19,760 --> 00:36:21,920


845
00:36:21,920 --> 00:36:23,920
поэтому изображение выглядит так, поэтому у нас есть

846
00:36:23,920 --> 00:36:27,280
этот многослойный lstm, который проходит

847
00:36:27,280 --> 00:36:30,320
через исходное предложение, и теперь

848
00:36:30,320 --> 00:36:33,760
в каждый момент времени мы вычисляем новое

849
00:36:33,760 --> 00:36:36,960
скрытое представление, которое вместо того, чтобы

850
00:36:36,960 --> 00:36:39,119
останавливать  там мы как бы подаем его как

851
00:36:39,119 --> 00:36:41,280
вход на другой уровень

852
00:36:41,280 --> 00:36:42,720
lstm,

853
00:36:42,720 --> 00:36:45,040
и мы вычисляем стандартным способом

854
00:36:45,040 --> 00:36:47,359
это новое скрытое представление, а его

855
00:36:47,359 --> 00:36:49,520
вывод мы подаем на третий

856
00:36:49,520 --> 00:36:53,119
уровень lstm, и поэтому мы запускаем его прямо вместе,

857
00:36:53,119 --> 00:36:54,599
и поэтому наше

858
00:36:54,599 --> 00:36:57,119
представление

859
00:36:57,119 --> 00:37:00,000
исходного предложения от нашего кодировщика -

860
00:37:00,000 --> 00:37:04,480
это стек из трех скрытых слоев,

861
00:37:04,480 --> 00:37:06,839
а затем мы

862
00:37:06,839 --> 00:37:09,680
используем um,

863
00:37:09,680 --> 00:37:11,119
чтобы затем

864
00:37:11,119 --> 00:37:15,280
вводить в качестве начального um в качестве начального

865
00:37:15,280 --> 00:37:17,599
скрытого слоя, чтобы затем генерировать

866
00:37:17,599 --> 00:37:19,079


867
00:37:19,079 --> 00:37:21,280
переводы или

868
00:37:21,280 --> 00:37:23,520
для обучения модели сравнения с

869
00:37:23,520 --> 00:37:25,520
потерями, поэтому  Это своего рода

870
00:37:25,520 --> 00:37:27,599
изображение

871
00:37:27,599 --> 00:37:30,240
декодера кодировщика lstm, на которое

872
00:37:30,240 --> 00:37:34,079
действительно похожа ваша система машинного перевода,

873
00:37:34,160 --> 00:37:37,359
поэтому, в частности,

874
00:37:37,359 --> 00:37:40,079
вы знаете, чтобы дать вам некоторое представление об этом,

875
00:37:40,079 --> 00:37:44,240
так что 2017  paper um от denny brits и

876
00:37:44,240 --> 00:37:47,200
других, что они обнаружили, что

877
00:37:47,200 --> 00:37:49,520
для кодировщика rnn

878
00:37:49,520 --> 00:37:52,560
он работал лучше всего, если бы он имел от двух до четырех

879
00:37:52,560 --> 00:37:55,280
слоев, а

880
00:37:55,280 --> 00:37:59,040
четыре слоя лучше всего подходили для декодера rnn,

881
00:37:59,040 --> 00:38:01,359
и детали здесь, как и для многих

882
00:38:01,359 --> 00:38:03,680
нейронных сетей, зависят так сильно  от того,

883
00:38:03,680 --> 00:38:06,160
что вы делаете и сколько данных у вас есть,

884
00:38:06,160 --> 00:38:08,880
и тому подобное, но вы знаете, что из практических

885
00:38:08,880 --> 00:38:11,520
правил, которые нужно иметь в голове,

886
00:38:11,520 --> 00:38:14,480
почти всегда бывает, что

887
00:38:14,480 --> 00:38:15,920
двухслойный

888
00:38:15,920 --> 00:38:18,960
lstm работает намного лучше, чем

889
00:38:18,960 --> 00:38:21,280
однослойный lstm

890
00:38:21,280 --> 00:38:24,240
после  что все становится намного менее ясным,

891
00:38:24,240 --> 00:38:25,280
вы знаете,

892
00:38:25,280 --> 00:38:27,359
что это не так уж и редко, что если вы попробуете

893
00:38:27,359 --> 00:38:29,760
три слоя, это будет на долю лучше, чем

894
00:38:29,760 --> 00:38:32,079
два, но не совсем, а если вы попробуете четыре

895
00:38:32,079 --> 00:38:34,160
слоя, на самом деле это снова станет хуже,

896
00:38:34,160 --> 00:38:36,800
вы знаете, это зависит от того, сколько данных и т. д. у

897
00:38:36,800 --> 00:38:40,720
вас есть на  в любом случае обычно

898
00:38:40,720 --> 00:38:42,960
очень сложно с той архитектурой модели,

899
00:38:42,960 --> 00:38:44,720
которую я только что показал

900
00:38:44,720 --> 00:38:48,079
здесь, получить лучшие результаты с более

901
00:38:48,079 --> 00:38:51,040
чем четырьмя уровнями lstm,

902
00:38:51,040 --> 00:38:52,880
как правило, для

903
00:38:52,880 --> 00:38:54,560
более

904
00:38:54,560 --> 00:38:57,040
глубоких моделей lstm и получения

905
00:38:57,040 --> 00:38:59,040
еще лучших результатов, которые вы должны

906
00:38:59,040 --> 00:39:01,440
добавление дополнительных пропускаемых соединений,

907
00:39:01,440 --> 00:39:04,400
о которых я говорил в самом

908
00:39:04,400 --> 00:39:05,200


909
00:39:05,200 --> 00:39:07,920
конце последнего урока на

910
00:39:07,920 --> 00:39:10,200
следующей неделе, Джон собирается говорить о

911
00:39:10,200 --> 00:39:12,480
сетях на основе трансформаторов,

912
00:39:12,480 --> 00:39:15,440
напротив, по довольно фундаментальным

913
00:39:15,440 --> 00:39:17,680
причинам они обычно

914
00:39:17,680 --> 00:39:18,640
намного

915
00:39:18,640 --> 00:39:20,960
глубже, но мы оставим их обсуждение

916
00:39:20,960 --> 00:39:22,079
пока мы не

917
00:39:22,079 --> 00:39:25,560
перейдем к следующему,

918
00:39:29,040 --> 00:39:31,280


919
00:39:31,359 --> 00:39:34,800
так что именно так мы обучили модель,

920
00:39:34,800 --> 00:39:37,680
так что давайте немного подробнее

921
00:39:37,680 --> 00:39:41,280
рассмотрим возможности декодирования и

922
00:39:41,280 --> 00:39:44,240
исследуем более сложную форму декодирования,

923
00:39:44,240 --> 00:39:46,960
чем мы рассматривали, самый простой способ

924
00:39:46,960 --> 00:39:49,440
декодирования - это  тот, который мы представили

925
00:39:49,440 --> 00:39:54,400
до сих пор, так что у нас есть наш lstm, мы начинаем

926
00:39:54,400 --> 00:39:56,800
генерировать скрытое состояние, у

927
00:39:56,800 --> 00:39:59,599
него есть распределение вероятностей по

928
00:39:59,599 --> 00:40:02,560
словам, и вы выбираете наиболее вероятное из

929
00:40:02,560 --> 00:40:05,200
них arg max, и вы говорите, что он, и вы

930
00:40:05,200 --> 00:40:07,920
копируете его, и повторяете снова  поэтому

931
00:40:07,920 --> 00:40:10,480
выполнение этого называется жадным

932
00:40:10,480 --> 00:40:13,599
декодированием, использующим наиболее вероятное слово

933
00:40:13,599 --> 00:40:16,720
на каждом этапе, и это своего рода

934
00:40:16,720 --> 00:40:18,560
очевидная вещь, которую нужно сделать,

935
00:40:18,560 --> 00:40:20,480
и не похоже, что это может быть плохо

936
00:40:20,480 --> 00:40:21,920
,

937
00:40:21,920 --> 00:40:22,720
но

938
00:40:22,720 --> 00:40:24,880
оказывается, что на самом деле это c  быть

939
00:40:24,880 --> 00:40:27,280
довольно проблематичным,

940
00:40:27,280 --> 00:40:30,800
и идея заключается в том, что вы знаете, что

941
00:40:30,800 --> 00:40:33,280
с жадным декодированием вы как бы

942
00:40:33,280 --> 00:40:35,599
берете локально то, что кажется лучшим

943
00:40:35,599 --> 00:40:38,079
выбором, а затем вы застряли в нем, и у

944
00:40:38,079 --> 00:40:41,599
вас нет возможности отменить решения,

945
00:40:41,599 --> 00:40:42,319
так что

946
00:40:42,319 --> 00:40:44,640
если в этих примерах использовалось

947
00:40:44,640 --> 00:40:47,680
это предложение о том, что он ударил меня пирогом,

948
00:40:47,680 --> 00:40:49,760
переходя от перевода с французского на

949
00:40:49,760 --> 00:40:52,640
английский, так что вы знаете, если вы начнете

950
00:40:52,640 --> 00:40:55,440
и скажете хорошо, плохо, первое слово в

951
00:40:55,440 --> 00:40:57,520
переводе должно быть он,

952
00:40:57,520 --> 00:41:01,839
мм, который выглядит хорошо, но тогда  вы эм,

953
00:41:01,839 --> 00:41:04,160
а затем вы говорите хорошо, я сгенерирую

954
00:41:04,160 --> 00:41:05,359
попадание,

955
00:41:05,359 --> 00:41:07,920
тогда как-то модель думает, что

956
00:41:07,920 --> 00:41:10,720
наиболее вероятное следующее слово, которое будет найдено после удара,

957
00:41:10,720 --> 00:41:12,560
- это r, и есть много причин, по которым она

958
00:41:12,560 --> 00:41:15,920
может так думать, потому что после удара чаще

959
00:41:15,920 --> 00:41:19,040
всего есть прямой объект  время от

960
00:41:19,040 --> 00:41:21,359
времени вы знаете, что он врезался в машину,

961
00:41:21,359 --> 00:41:23,359
он

962
00:41:23,359 --> 00:41:26,079
врезался в контрольно-пропускной пункт, так что это звучит

963
00:41:26,079 --> 00:41:28,560
довольно вероятно, но вы знаете, что после того, как вы его

964
00:41:28,560 --> 00:41:31,200
сгенерировали, нет возможности вернуться

965
00:41:31,200 --> 00:41:33,680
назад, и поэтому вам просто нужно

966
00:41:33,680 --> 00:41:36,160
продолжать движение оттуда, и вы можете не  иметь

967
00:41:36,160 --> 00:41:38,079
возможность генерировать t  Вы хотите перевести,

968
00:41:38,079 --> 00:41:40,240


969
00:41:40,240 --> 00:41:43,680
в лучшем случае, вы можете сгенерировать ммм, он попал

970
00:41:43,680 --> 00:41:44,960
в пирог,

971
00:41:44,960 --> 00:41:47,040
упс,

972
00:41:47,040 --> 00:41:49,760
что-то, поэтому мы хотели бы иметь возможность

973
00:41:49,760 --> 00:41:53,200
немного больше изучить при создании наших

974
00:41:53,200 --> 00:41:54,880
переводов

975
00:41:54,880 --> 00:41:58,319
, и вы знаете, что мы могли бы сделать

976
00:41:58,319 --> 00:42:00,640
хорошо, вы знаете, я вроде как упомянул  это

977
00:42:00,640 --> 00:42:02,800
прежде, чем рассматривать статистические модели mt в

978
00:42:02,800 --> 00:42:04,160


979
00:42:04,160 --> 00:42:06,160
целом, что мы хотели бы сделать, так

980
00:42:06,160 --> 00:42:07,760
это найти

981
00:42:07,760 --> 00:42:09,920
переводы,

982
00:42:09,920 --> 00:42:12,640
которые

983
00:42:12,640 --> 00:42:15,359
максимизируют вероятность y при заданном x, и, по

984
00:42:15,359 --> 00:42:18,240
крайней мере, если мы знаем, какова длина этого

985
00:42:18,240 --> 00:42:20,079
перевода,

986
00:42:20,079 --> 00:42:21,760
мы можем сделать это как результат

987
00:42:21,760 --> 00:42:24,160
генерации  слово за раз, и поэтому, чтобы

988
00:42:24,160 --> 00:42:26,000
иметь полную модель, мы также должны иметь

989
00:42:26,000 --> 00:42:28,560
распределение вероятностей по

990
00:42:28,560 --> 00:42:31,760
длине перевода,

991
00:42:31,760 --> 00:42:34,560
чтобы мы могли сказать, что это модель, и

992
00:42:34,560 --> 00:42:36,079
дадим вам знать,

993
00:42:36,079 --> 00:42:38,160
сгенерировать и оценить

994
00:42:38,160 --> 00:42:41,599
все возможные последовательности y,

995
00:42:41,599 --> 00:42:44,319
используя эту модель и  вот где

996
00:42:44,319 --> 00:42:47,520
тогда требуется генерировать экспоненциальное

997
00:42:47,520 --> 00:42:50,880
количество переводов и это

998
00:42:50,880 --> 00:42:52,720
слишком дорого,

999
00:42:52,720 --> 00:42:56,240
поэтому, помимо жадного декодирования,

1000
00:42:56,240 --> 00:42:58,960
самый важный метод, который используется,

1001
00:42:58,960 --> 00:43:01,200
и вы увидите, что во многих местах есть

1002
00:43:01,200 --> 00:43:03,760
что-то, что называется  декодирование поиска луча,

1003
00:43:03,760 --> 00:43:06,560
и поэтому это не то, что

1004
00:43:06,560 --> 00:43:08,640
нейронно хорошо, любой вид машинного

1005
00:43:08,640 --> 00:43:10,560
перевода - это одно место, где он

1006
00:43:10,560 --> 00:43:12,800
обычно используется, но это не

1007
00:43:12,800 --> 00:43:14,400
специфический для меня метод машинный

1008
00:43:14,400 --> 00:43:16,800
перевод, вы найдете множество других

1009
00:43:16,800 --> 00:43:19,359
мест, включая все другие виды

1010
00:43:19,359 --> 00:43:21,839
последовательности последовательностей  модели, это не

1011
00:43:21,839 --> 00:43:24,160
единственный другой метод декодирования, однажды, когда

1012
00:43:24,160 --> 00:43:26,240
мы перейдем к

1013
00:43:26,240 --> 00:43:28,160
классу генерации языка, мы увидим

1014
00:43:28,160 --> 00:43:30,240
еще пару, но это своего рода следующий метод,

1015
00:43:30,240 --> 00:43:32,560
о котором вы должны знать,

1016
00:43:32,560 --> 00:43:35,760
поэтому идея поиска луча заключается в том, что вы

1017
00:43:35,760 --> 00:43:37,040
собираетесь

1018
00:43:37,040 --> 00:43:37,920
сохранить

1019
00:43:37,920 --> 00:43:40,880
некоторые гипотезы, которые повышают вероятность того,

1020
00:43:40,880 --> 00:43:44,560
что вы найдете хорошее поколение, сохраняя при

1021
00:43:44,560 --> 00:43:47,359
этом управляемость поиска,

1022
00:43:47,359 --> 00:43:50,880
поэтому мы выбираем размер луча,

1023
00:43:50,880 --> 00:43:53,040
а для нейронной сети размер луча

1024
00:43:53,040 --> 00:43:55,040
обычно довольно мал, примерно от

1025
00:43:55,040 --> 00:43:57,920
пяти до десяти, и на каждом шаге

1026
00:43:57,920 --> 00:44:00,720
декодера мы собираемся отслеживать

1027
00:44:00,720 --> 00:44:04,480
k наиболее вероятных частичных переводов, поэтому

1028
00:44:04,480 --> 00:44:06,960
начальные подпоследовательности того, что мы

1029
00:44:06,960 --> 00:44:10,319
генерируем, которые мы называем гипотезами

1030
00:44:10,319 --> 00:44:13,359
um, так что гипотеза, которая затем является своего

1031
00:44:13,359 --> 00:44:17,359
рода префиксом at  ranslation имеет оценку,

1032
00:44:17,359 --> 00:44:19,680
которая представляет собой логарифмическую вероятность до

1033
00:44:19,680 --> 00:44:22,480
того, что было сгенерировано до сих пор, поэтому мы можем

1034
00:44:22,480 --> 00:44:24,000
сгенерировать это

1035
00:44:24,000 --> 00:44:25,920
обычным способом, используя нашу условную

1036
00:44:25,920 --> 00:44:27,280
языковую модель,

1037
00:44:27,280 --> 00:44:28,079
так

1038
00:44:28,079 --> 00:44:30,640
как записанные все оценки являются

1039
00:44:30,640 --> 00:44:33,680
отрицательными, и поэтому наименьший отрицательный

1040
00:44:33,680 --> 00:44:36,240
i - наибольшая вероятность  является лучшим

1041
00:44:36,240 --> 00:44:39,520
, поэтому мы хотим

1042
00:44:39,520 --> 00:44:43,680
искать гипотезы с высокой вероятностью,

1043
00:44:43,680 --> 00:44:47,040
так что это эвристический метод, который не

1044
00:44:47,040 --> 00:44:48,720
гарантирует нахождение

1045
00:44:48,720 --> 00:44:51,200
декодирования с самой высокой вероятностью, но, по крайней мере, он

1046
00:44:51,200 --> 00:44:53,440
дает вам больше шансов, чем просто

1047
00:44:53,440 --> 00:44:54,319
выполнение

1048
00:44:54,319 --> 00:44:56,800
жадного декодирования, поэтому давайте пройдемся

1049
00:44:56,800 --> 00:44:58,400
пример,

1050
00:44:58,400 --> 00:45:01,359
чтобы увидеть, как это работает,

1051
00:45:01,359 --> 00:45:02,960
так что

1052
00:45:02,960 --> 00:45:05,520
в этом случае, чтобы я мог разместить его на слайде,

1053
00:45:05,520 --> 00:45:08,319
размер нашего луча составляет всего два мкм,

1054
00:45:08,319 --> 00:45:10,480
хотя обычно он на самом деле был

1055
00:45:10,480 --> 00:45:12,640
бы немного больше, а синие

1056
00:45:12,640 --> 00:45:15,920
числа - это оценки префиксов

1057
00:45:15,920 --> 00:45:19,200
Итак, это логарифмические

1058
00:45:19,200 --> 00:45:22,480
вероятности префикса, поэтому мы начинаем с нашего начального

1059
00:45:22,480 --> 00:45:26,160
символа и собираемся сказать хорошо,

1060
00:45:26,160 --> 00:45:28,960
какие два слова с наибольшей вероятностью будут

1061
00:45:28,960 --> 00:45:31,599
сгенерированы первыми в соответствии с нашей языковой

1062
00:45:31,599 --> 00:45:34,079
моделью и s  o может быть, первые два наиболее

1063
00:45:34,079 --> 00:45:37,119
вероятных слова - это он и я,

1064
00:45:37,119 --> 00:45:40,480
и есть их логарифмические вероятности,

1065
00:45:40,480 --> 00:45:44,240
тогда что мы делаем дальше: для каждой

1066
00:45:44,240 --> 00:45:48,800
из этих k гипотез мы находим, какие

1067
00:45:48,800 --> 00:45:51,359
вероятные слова последуют за ними, в

1068
00:45:51,359 --> 00:45:53,839
частности, мы находим одно из k наиболее

1069
00:45:53,839 --> 00:45:57,440
вероятных слов  следовать за каждым из них, чтобы

1070
00:45:57,440 --> 00:46:01,760
мы могли сгенерировать он ударил он ударил меня

1071
00:46:01,760 --> 00:46:03,200


1072
00:46:03,200 --> 00:46:06,160
все в порядке, так что на данный момент похоже,

1073
00:46:06,160 --> 00:46:08,800
что мы движемся вниз, что снова превратится

1074
00:46:08,800 --> 00:46:11,760
в экспоненциальную древовидную структуру истинного размера,

1075
00:46:11,760 --> 00:46:15,119
но то, что мы делаем сейчас

1076
00:46:15,119 --> 00:46:18,720
,  мы вычисляем оценки каждой из

1077
00:46:18,720 --> 00:46:21,680
этих частичных гипотез, поэтому у нас есть

1078
00:46:21,680 --> 00:46:24,800
четыре частичные гипотезы, которые он поразил, он поразил

1079
00:46:24,800 --> 00:46:29,280
меня, я получил, и мы можем сделать это,

1080
00:46:29,280 --> 00:46:31,440
взяв предыдущую оценку, что у нас есть

1081
00:46:31,440 --> 00:46:35,440
частичная гипотеза, и добавив логарифмическую

1082
00:46:35,440 --> 00:46:37,760
вероятность генерации  следующее слово

1083
00:46:37,760 --> 00:46:41,280
вот здесь ударил, так что это дает нам баллы

1084
00:46:41,280 --> 00:46:44,319
для каждой гипотезы, а затем мы можем сказать,

1085
00:46:44,319 --> 00:46:47,359
какая из этих двух частичных гипотез,

1086
00:46:47,359 --> 00:46:50,079
потому что наш размер луча k равен двум,

1087
00:46:50,079 --> 00:46:54,400
имеет наивысший балл, так что они были я,

1088
00:46:54,400 --> 00:46:57,760
и он ударил, поэтому мы сохраняем эти две

1089
00:46:57,760 --> 00:46:59,920
и  игнорируем остальные,

1090
00:46:59,920 --> 00:47:02,400
и тогда для этих двух

1091
00:47:02,400 --> 00:47:05,680
мы собираемся сгенерировать um k гипотез

1092
00:47:05,680 --> 00:47:08,400
для наиболее вероятного следующего слова, которое

1093
00:47:08,400 --> 00:47:11,839
он ударил ээ он ударил меня я был поражен я был

1094
00:47:11,839 --> 00:47:14,960
поражен um и снова

1095
00:47:14,960 --> 00:47:18,560
теперь мы хотим найти k наиболее вероятных

1096
00:47:18,560 --> 00:47:20,240


1097
00:47:20,240 --> 00:47:22,880
гипотез  из этого полного набора, так что это

1098
00:47:22,880 --> 00:47:26,640
будет он ударил меня, и я был о, нет,

1099
00:47:26,640 --> 00:47:29,680
он ударил меня, и он ударил r

1100
00:47:29,680 --> 00:47:32,880
um, поэтому мы оставляем только эти,

1101
00:47:32,880 --> 00:47:36,559
а затем для каждого из них мы

1102
00:47:36,559 --> 00:47:40,000
генерируем k наиболее вероятных следующих слов

1103
00:47:40,000 --> 00:47:43,440
пирог с on  а затем снова мы

1104
00:47:43,440 --> 00:47:45,920
фильтруем обратно до размера k

1105
00:47:45,920 --> 00:47:47,599
, говоря:

1106
00:47:47,599 --> 00:47:50,400
хорошо, две наиболее вероятные вещи здесь - это

1107
00:47:50,400 --> 00:47:53,760
круговая диаграмма или ширина, поэтому мы продолжаем работать над

1108
00:47:53,760 --> 00:47:55,119
теми, которые

1109
00:47:55,119 --> 00:47:57,440
генерируют вещи,

1110
00:47:57,440 --> 00:47:59,680
обнаруживаем, что два наиболее вероятных

1111
00:47:59,680 --> 00:48:04,480
генерируют вещи, находят два наиболее вероятных,

1112
00:48:04,480 --> 00:48:08,240
и на этом этапе мы бы  сгенерируйте

1113
00:48:08,240 --> 00:48:11,839
конец строки и скажите, что у нас есть

1114
00:48:11,839 --> 00:48:15,839
полная гипотеза, он ударил меня кругом

1115
00:48:15,839 --> 00:48:16,960


1116
00:48:16,960 --> 00:48:21,119
um, и мы могли бы затем проследить um

1117
00:48:21,119 --> 00:48:24,319
через дерево um, чтобы получить полную

1118
00:48:24,319 --> 00:48:27,839
гипотезу для этого предложения,

1119
00:48:27,839 --> 00:48:30,640
так что в большей части алгоритма есть еще

1120
00:48:30,640 --> 00:48:32,880
одна деталь

1121
00:48:32,880 --> 00:48:36,079
который останавливает крик  terion, поэтому при

1122
00:48:36,079 --> 00:48:39,280
жадном декодировании мы обычно

1123
00:48:39,280 --> 00:48:42,079
декодируем до тех пор, пока модель не создаст n

1124
00:48:42,079 --> 00:48:44,880
токенов, и когда она создаст конечный токен,

1125
00:48:44,880 --> 00:48:48,000
мы говорим, что мы закончили

1126
00:48:48,000 --> 00:48:50,640
декодирование поиска луча, разные

1127
00:48:50,640 --> 00:48:54,240
гипотезы могут создавать n токенов на

1128
00:48:54,240 --> 00:48:57,440
разных временных шагах, и поэтому мы не

1129
00:48:57,440 --> 00:48:58,960
хотим  остановитесь,

1130
00:48:58,960 --> 00:49:01,520
как только один путь в

1131
00:49:01,520 --> 00:49:05,040
дереве поиска сгенерирует конец, потому что может

1132
00:49:05,040 --> 00:49:06,720
оказаться, что есть другой путь

1133
00:49:06,720 --> 00:49:09,440
через дерево поиска, который все

1134
00:49:09,440 --> 00:49:12,640
равно окажется лучше, поэтому то, что мы делаем,

1135
00:49:12,640 --> 00:49:15,359
вроде как откладываем его в сторону как полную

1136
00:49:15,359 --> 00:49:16,640
гипотезу

1137
00:49:16,640 --> 00:49:19,680
и  продолжаем исследовать другие гипотезы с

1138
00:49:19,680 --> 00:49:21,680
помощью нашего поиска луча,

1139
00:49:21,680 --> 00:49:25,680
и поэтому обычно мы останавливаемся,

1140
00:49:25,680 --> 00:49:26,960
когда мы

1141
00:49:26,960 --> 00:49:29,440
достигаем предельной длины,

1142
00:49:29,440 --> 00:49:32,640
или когда мы завершаем

1143
00:49:32,640 --> 00:49:34,000
n

1144
00:49:34,000 --> 00:49:36,400
полных гипотез,

1145
00:49:36,400 --> 00:49:38,400
а затем мы

1146
00:49:38,400 --> 00:49:41,680
просматриваем гипотезы, которые мы завершили, и

1147
00:49:41,680 --> 00:49:44,160
скажите, какая из них лучшая, и

1148
00:49:44,160 --> 00:49:47,359
это та, которую мы будем использовать,

1149
00:49:47,760 --> 00:49:50,160
хорошо, так что на этом этапе у нас есть наш

1150
00:49:50,160 --> 00:49:53,200
список завершенных гипотез,

1151
00:49:53,200 --> 00:49:56,079
и мы хотим выбрать лучшую из них

1152
00:49:56,079 --> 00:49:58,400
с наивысшим баллом хорошо, это именно

1153
00:49:58,400 --> 00:50:02,240
то, чем мы были  вычисление каждого из них имеет

1154
00:50:02,240 --> 00:50:03,200


1155
00:50:03,200 --> 00:50:05,119
вероятность,

1156
00:50:05,119 --> 00:50:07,599
которую мы разработали, но оказывается,

1157
00:50:07,599 --> 00:50:10,240
что мы, возможно, не захотим использовать это

1158
00:50:10,240 --> 00:50:13,040
так наивно, потому что оказывается

1159
00:50:13,040 --> 00:50:16,480
своего рода систематическая проблема, которую

1160
00:50:16,480 --> 00:50:19,920
вы знаете не как теорему, а в целом

1161
00:50:19,920 --> 00:50:23,280
более длинные гипотезы имеют более низкие оценки, поэтому,

1162
00:50:23,280 --> 00:50:25,839
если вы думаете об этом как о

1163
00:50:25,839 --> 00:50:28,960
вероятностях последовательного генерирования каждого слова,

1164
00:50:28,960 --> 00:50:31,280
которое в основном на каждом шаге вы

1165
00:50:31,280 --> 00:50:33,599
умножаете на другой шанс

1166
00:50:33,599 --> 00:50:36,400
создания вероятности следующего слова, и

1167
00:50:36,400 --> 00:50:38,480
обычно это может быть

1168
00:50:38,480 --> 00:50:41,040
10 минус три десятка  минус два, так

1169
00:50:41,040 --> 00:50:43,280
что, судя по длине предложения,

1170
00:50:43,280 --> 00:50:45,280
ваши вероятности становятся намного

1171
00:50:45,280 --> 00:50:47,680
ниже, чем дольше они продолжаются

1172
00:50:47,680 --> 00:50:51,119
, что кажется несправедливым, поскольку,

1173
00:50:51,119 --> 00:50:53,200
хотя в некотором смысле чрезвычайно длинные

1174
00:50:53,200 --> 00:50:55,680
предложения не так вероятны, как короткие,

1175
00:50:55,680 --> 00:50:58,319
они '  Не менее вероятно, что к

1176
00:50:58,319 --> 00:51:00,400
тому времени мы создаем длинные

1177
00:51:00,400 --> 00:51:03,359
предложения, поэтому, например, вы знаете, что в

1178
00:51:03,359 --> 00:51:05,680
газете

1179
00:51:05,680 --> 00:51:06,800


1180
00:51:06,800 --> 00:51:10,160
средняя длина предложений превышает 20, поэтому

1181
00:51:10,160 --> 00:51:12,000
вы не захотите иметь рекламу

1182
00:51:12,000 --> 00:51:14,400
Модель экодирования при переводе новостных

1183
00:51:14,400 --> 00:51:17,200
статей, которая вроде как говорит: «О,

1184
00:51:17,200 --> 00:51:19,280
просто генерируйте предложения из двух слов,

1185
00:51:19,280 --> 00:51:21,119
вероятность этого намного выше в соответствии с моей

1186
00:51:21,119 --> 00:51:23,599
языковой моделью, ммм.

1187
00:51:23,599 --> 00:51:26,000


1188
00:51:26,000 --> 00:51:29,119


1189
00:51:29,119 --> 00:51:32,480
работа в журнале вероятностей, что означает

1190
00:51:32,480 --> 00:51:34,880
деление на

1191
00:51:34,880 --> 00:51:37,280
длину предложения, а затем у вас есть оценка вероятности журнала для каждого

1192
00:51:37,280 --> 00:51:38,800
слова,

1193
00:51:38,800 --> 00:51:40,880


1194
00:51:40,880 --> 00:51:42,319
и вы знаете,

1195
00:51:42,319 --> 00:51:44,240
что можете утверждать, что это не совсем

1196
00:51:44,240 --> 00:51:45,119
верно

1197
00:51:45,119 --> 00:51:46,880
в некотором теоретическом смысле, но на

1198
00:51:46,880 --> 00:51:48,800
практике это работает довольно хорошо и  это

1199
00:51:48,800 --> 00:51:51,440
очень часто используемый

1200
00:51:51,440 --> 00:51:52,920
нейронный

1201
00:51:52,920 --> 00:51:54,480


1202
00:51:54,480 --> 00:51:56,559
перевод оказался

1203
00:51:56,559 --> 00:51:58,800
намного лучше, я покажу вам

1204
00:51:58,800 --> 00:52:02,240
пару статистических данных, и через мгновение

1205
00:52:02,240 --> 00:52:04,480
он имеет много преимуществ

1206
00:52:04,480 --> 00:52:07,359
эм, он дает лучшую производительность,

1207
00:52:07,359 --> 00:52:10,240
переводы лучше, в частности,

1208
00:52:10,240 --> 00:52:12,880
они более беглые, потому что  новые

1209
00:52:12,880 --> 00:52:15,680
языковые модели производят гораздо более плавные

1210
00:52:15,680 --> 00:52:17,119
предложения,

1211
00:52:17,119 --> 00:52:18,800
но

1212
00:52:18,800 --> 00:52:22,319
они также намного лучше используют контекст, потому что

1213
00:52:22,319 --> 00:52:24,640
нейронные языковые модели, включая

1214
00:52:24,640 --> 00:52:27,440
условные нейроязыковые модели, дают

1215
00:52:27,440 --> 00:52:30,079
это очень хороший способ обусловить

1216
00:52:30,079 --> 00:52:32,800
множество контекстов, в частности, мы можем просто

1217
00:52:32,800 --> 00:52:36,319
запустить длинный кодировщик и условие для

1218
00:52:36,319 --> 00:52:39,680
предыдущего предложения или мы можем

1219
00:52:39,680 --> 00:52:42,079
хорошо переводить слова в контексте, используя

1220
00:52:42,079 --> 00:52:44,800
нейронные модели нейронного контекста,

1221
00:52:44,800 --> 00:52:47,920
которые лучше понимают

1222
00:52:47,920 --> 00:52:50,079
сходство фраз и фразы, которые

1223
00:52:50,079 --> 00:52:54,079
означают примерно одно и то же,

1224
00:52:54,079 --> 00:52:55,119
а затем

1225
00:52:55,119 --> 00:52:56,000


1226
00:52:56,000 --> 00:52:59,040
метод оптимизации всех

1227
00:52:59,040 --> 00:53:01,359
параметров модели от начала до конца в

1228
00:53:01,359 --> 00:53:04,720
одной большой нейронной сети

1229
00:53:04,720 --> 00:53:07,680
оказался действительно мощной идеей, поэтому

1230
00:53:07,680 --> 00:53:10,160
раньше большую часть времени люди создавали

1231
00:53:10,160 --> 00:53:11,599


1232
00:53:11,599 --> 00:53:13,599
отдельные компоненты и настраивали их

1233
00:53:13,599 --> 00:53:16,079
по отдельности, что просто означало, что они

1234
00:53:16,079 --> 00:53:18,079
не были на самом деле оптимальными, когда их помещали в

1235
00:53:18,079 --> 00:53:20,400
гораздо большую систему,

1236
00:53:20,400 --> 00:53:22,720
так что действительно

1237
00:53:22,720 --> 00:53:25,920
очень мощная руководящая идея в мире нейронных

1238
00:53:25,920 --> 00:53:28,000
сетей заключается в том, что вы можете как бы построить

1239
00:53:28,000 --> 00:53:30,480
одну огромную сеть и просто оптимизировать

1240
00:53:30,480 --> 00:53:32,720
все это от начала до конца, что будет  дадут

1241
00:53:32,720 --> 00:53:34,400
вам гораздо лучшую производительность, а системы по

1242
00:53:34,400 --> 00:53:36,800
компонентам,

1243
00:53:36,800 --> 00:53:39,440
мы вернемся к затратам на это

1244
00:53:39,440 --> 00:53:42,079
позже в ходе курса,

1245
00:53:42,079 --> 00:53:44,000
модели также  на самом деле хороши

1246
00:53:44,000 --> 00:53:46,160
в других отношениях, они на самом деле требуют гораздо

1247
00:53:46,160 --> 00:53:48,319
меньше человеческих усилий для создания,

1248
00:53:48,319 --> 00:53:51,040
нет никакой разработки функций

1249
00:53:51,040 --> 00:53:52,079
,

1250
00:53:52,079 --> 00:53:54,319
в целом нет языковых

1251
00:53:54,319 --> 00:53:56,880
компонентов, которые вы используете один и тот же метод

1252
00:53:56,880 --> 00:53:59,040
для всех языковых пар,

1253
00:53:59,040 --> 00:54:01,119
конечно, редко что что-то может быть

1254
00:54:01,119 --> 00:54:03,359
идеальным во всех отношениях,

1255
00:54:03,359 --> 00:54:05,760
так что нейронные  У систем машинного перевода

1256
00:54:05,760 --> 00:54:08,800
также есть некоторые недостатки по сравнению

1257
00:54:08,800 --> 00:54:10,640
со старыми системами статистического машинного

1258
00:54:10,640 --> 00:54:12,880
перевода:

1259
00:54:12,880 --> 00:54:14,960
они менее интерпретируемы,

1260
00:54:14,960 --> 00:54:17,040
труднее понять, почему они делают то, что они

1261
00:54:17,040 --> 00:54:18,720
делают там, где вы, прежде чем вы

1262
00:54:18,720 --> 00:54:20,480
действительно сможете взглянуть на таблицы фраз, и они

1263
00:54:20,480 --> 00:54:21,920
были полезны,

1264
00:54:21,920 --> 00:54:24,400
поэтому они  их трудно отлаживать, их также довольно

1265
00:54:24,400 --> 00:54:27,280
сложно контролировать,

1266
00:54:27,280 --> 00:54:30,640
поэтому по сравнению с чем-либо вроде

1267
00:54:30,640 --> 00:54:34,160
правил написания, вы не можете дать много

1268
00:54:34,160 --> 00:54:37,200
подробностей, как если бы вы хотели сказать: о,

1269
00:54:37,200 --> 00:54:39,440
я бы хотел, чтобы мои переводы были более

1270
00:54:39,440 --> 00:54:41,920
случайный или что-то в

1271
00:54:41,920 --> 00:54:43,920
этом роде, трудно понять, что они будут генерировать,

1272
00:54:43,920 --> 00:54:47,760
поэтому существуют различные проблемы с безопасностью,

1273
00:54:47,760 --> 00:54:48,960


1274
00:54:48,960 --> 00:54:51,440
я покажу несколько примеров

1275
00:54:51,440 --> 00:54:54,000
всего через минуту, но сначала  делать

1276
00:54:54,000 --> 00:54:54,880
это

1277
00:54:54,880 --> 00:54:56,720
быстро, как мы

1278
00:54:56,720 --> 00:55:00,240
оцениваем машинный перевод? лучший

1279
00:55:00,240 --> 00:55:03,280
способ оценить машинный перевод -

1280
00:55:03,280 --> 00:55:06,880
это показать человеку, который свободно

1281
00:55:06,880 --> 00:55:09,200
владеет исходным и целевым языками,

1282
00:55:09,200 --> 00:55:12,799
предложения и заставить его вынести суждение

1283
00:55:12,799 --> 00:55:15,920
о том, насколько он хорош,

1284
00:55:15,920 --> 00:55:17,520
но это

1285
00:55:17,520 --> 00:55:19,359
дорого  сделать,

1286
00:55:19,359 --> 00:55:21,280
а может и не стать возможным, если

1287
00:55:21,280 --> 00:55:23,680
рядом нет нужных людей,

1288
00:55:23,680 --> 00:55:26,000
поэтому была проделана большая работа по поиску

1289
00:55:26,000 --> 00:55:28,160
автоматических методов оценки

1290
00:55:28,160 --> 00:55:31,040
переводов, которые были достаточно хороши,

1291
00:55:31,040 --> 00:55:33,520
и самый известный способ сделать это -

1292
00:55:33,520 --> 00:55:35,440
то, что называется синим

1293
00:55:35,440 --> 00:55:37,680
и  То, как вы делаете синий

1294
00:55:37,680 --> 00:55:39,760
- это то, что у вас

1295
00:55:39,760 --> 00:55:42,640
есть человеческий перевод или несколько человеческих

1296
00:55:42,640 --> 00:55:45,599
переводов исходного предложения, и

1297
00:55:45,599 --> 00:55:48,559
вы сравниваете машинный

1298
00:55:48,559 --> 00:55:51,920
перевод с заранее заданными человеческими

1299
00:55:51,920 --> 00:55:54,880
письменными переводами, и вы оцениваете

1300
00:55:54,880 --> 00:55:56,480
их схожесть

1301
00:55:56,480 --> 00:56:00,400
, вычисляя точность инграммы, то есть

1302
00:56:00,400 --> 00:56:02,480
слова, которые перекрываются

1303
00:56:02,480 --> 00:56:04,640
между компьютерными и человеческими

1304
00:56:04,640 --> 00:56:07,760
диаграммами письменного перевода триграммы и четыре

1305
00:56:07,760 --> 00:56:10,640
грамма, а затем

1306
00:56:10,640 --> 00:56:14,720
вычисление среднего геометрического между

1307
00:56:14,720 --> 00:56:17,280
наложениями  fn энграмм плюс штраф за

1308
00:56:17,280 --> 00:56:20,480
два коротких системных перевода, так что

1309
00:56:20,480 --> 00:56:23,599
синий оказался действительно полезной

1310
00:56:23,599 --> 00:56:25,839
мерой, но это несовершенная мера

1311
00:56:25,839 --> 00:56:28,000
, поскольку обычно существует много правильных

1312
00:56:28,000 --> 00:56:30,559
способов перевода предложения, и поэтому есть

1313
00:56:30,559 --> 00:56:33,680
некоторая удача в том,

1314
00:56:33,680 --> 00:56:34,880
что

1315
00:56:34,880 --> 00:56:37,119
человеческие письменные переводы вы

1316
00:56:37,119 --> 00:56:39,599
должны соответствовать тому, что

1317
00:56:39,599 --> 00:56:41,839
может быть хорошим переводом из

1318
00:56:41,839 --> 00:56:43,359
системы,

1319
00:56:43,359 --> 00:56:45,440
есть еще что сказать о деталях

1320
00:56:45,440 --> 00:56:47,359
синего

1321
00:56:47,359 --> 00:56:49,520
и о том, как он реализован, но

1322
00:56:49,520 --> 00:56:51,119
вы увидите все это, выполняя

1323
00:56:51,119 --> 00:56:52,880
задание 4,

1324
00:56:52,880 --> 00:56:54,880
потому что вы будете

1325
00:56:54,880 --> 00:56:57,040
создавать свой машинный перевод

1326
00:56:57,040 --> 00:57:00,079
системы и оценивая их с

1327
00:57:00,079 --> 00:57:00,880
помощью

1328
00:57:00,880 --> 00:57:03,359
синего алгоритма и их полную информацию

1329
00:57:03,359 --> 00:57:04,400
о

1330
00:57:04,400 --> 00:57:07,119
синем в раздаточном материале, но

1331
00:57:07,119 --> 00:57:08,960
в конце дня

1332
00:57:08,960 --> 00:57:11,680
синий дает оценку от нуля до

1333
00:57:11,680 --> 00:57:13,280
ста, где

1334
00:57:13,280 --> 00:57:15,520
ваша оценка равна ста, если вы

1335
00:57:15,520 --> 00:57:17,520
точно производите одну из

1336
00:57:17,520 --> 00:57:20,480
письменные переводы, написанные людьми, и ноль, если у вас

1337
00:57:20,480 --> 00:57:22,880
нет ни одной униграммы, которая

1338
00:57:22,880 --> 00:57:26,160
пересекается между ними

1339
00:57:26,160 --> 00:57:30,319
с тем довольно кратким вступлением, которое я хотел

1340
00:57:30,319 --> 00:57:32,160
показать y  Что-то вроде того, что произошло с

1341
00:57:32,160 --> 00:57:35,280
машинным переводом,

1342
00:57:35,280 --> 00:57:37,520
поэтому машинный перевод

1343
00:57:37,520 --> 00:57:39,680
со статистическими моделями, фразовый

1344
00:57:39,680 --> 00:57:41,680
статистический машинный перевод, который я

1345
00:57:41,680 --> 00:57:43,920
показал в начале урока,

1346
00:57:43,920 --> 00:57:45,599
продолжался

1347
00:57:45,599 --> 00:57:49,280
с середины 2000-х годов, и он

1348
00:57:49,280 --> 00:57:52,160
дал своего рода полу-хорошие

1349
00:57:52,160 --> 00:57:54,000
результаты  те, которые были в переводчике Google в

1350
00:57:54,000 --> 00:57:56,720
те дни, но к тому времени, когда вы вошли в

1351
00:57:56,720 --> 00:57:59,680
эм, 2010-е годы в

1352
00:57:59,680 --> 00:58:03,040
основном прогресс, и

1353
00:58:03,040 --> 00:58:05,200
статистический машинный перевод

1354
00:58:05,200 --> 00:58:06,960
застопорился,

1355
00:58:06,960 --> 00:58:09,839
и вы почти не получаете никакого увеличения с

1356
00:58:09,839 --> 00:58:12,559
течением времени, и большая часть увеличения

1357
00:58:12,559 --> 00:58:14,880
времени, которое вы получали с течением времени, было

1358
00:58:14,880 --> 00:58:16,480
просто потому, что вы тренируете свои

1359
00:58:16,480 --> 00:58:18,799
модели на большем количестве

1360
00:58:18,799 --> 00:58:21,040
данных в те годы,

1361
00:58:21,040 --> 00:58:24,920
примерно в начале 2010-х,

1362
00:58:25,839 --> 00:58:30,000
большая надежда, что большинство

1363
00:58:30,000 --> 00:58:31,760
людей спрашивали, что это за ось

1364
00:58:31,760 --> 00:58:34,079
Y, эта ось Y - это синяя оценка, о которой я

1365
00:58:34,079 --> 00:58:36,720
вам говорил на  предыдущий слайд

1366
00:58:36,720 --> 00:58:40,240
в начале 2010-х годов большая надежда, которую возлагала

1367
00:58:40,240 --> 00:58:42,160
большинство людей, работающих в сфере машинного перевода,

1368
00:58:42,160 --> 00:58:45,280
оправдалась, если мы построили более

1369
00:58:45,280 --> 00:58:47,200
сложную модель машинного перевода,

1370
00:58:47,200 --> 00:58:50,079
которая знает о  синтаксическая

1371
00:58:50,079 --> 00:58:52,640
структура языков, использующая

1372
00:58:52,640 --> 00:58:55,280
такие инструменты, как синтаксические анализаторы зависимостей,

1373
00:58:55,280 --> 00:58:57,920
сможет создавать гораздо лучшие переводы

1374
00:58:57,920 --> 00:59:00,799
, поэтому здесь представлены фиолетовые системы,

1375
00:59:00,799 --> 00:59:03,839
которые я вообще не описывал, но

1376
00:59:03,839 --> 00:59:06,640
с годами

1377
00:59:06,640 --> 00:59:08,640
это было довольно очевидно

1378
00:59:08,640 --> 00:59:12,480
казалось, что это едва ли помогло,

1379
00:59:12,480 --> 00:59:13,839
и поэтому

1380
00:59:13,839 --> 00:59:17,040
в середине 2000-х годов 2010-х годов,

1381
00:59:17,040 --> 00:59:17,960
так что в

1382
00:59:17,960 --> 00:59:21,200
2014 году была первая современная попытка

1383
00:59:21,200 --> 00:59:23,920
построить нейронную сеть для машинного

1384
00:59:23,920 --> 00:59:26,799
перевода и модель декодера кодировщика,

1385
00:59:26,799 --> 00:59:28,880
ммм, и к тому времени, когда она была вроде как

1386
00:59:28,880 --> 00:59:32,160
оценена в тестах в 2015 году,

1387
00:59:32,160 --> 00:59:34,000
она  было не так хорошо, как то, что было

1388
00:59:34,000 --> 00:59:36,240
создано за предыдущее десятилетие,

1389
00:59:36,240 --> 00:59:38,640
но оно уже становится довольно хорошим, но

1390
00:59:38,640 --> 00:59:41,680
было обнаружено, что эти новые

1391
00:59:41,680 --> 00:59:44,799
модели действительно открыли совершенно новый

1392
00:59:44,799 --> 00:59:47,440
путь к созданию гораздо

1393
00:59:47,440 --> 00:59:50,000
более совершенных систем машинного перевода, и

1394
00:59:50,000 --> 00:59:51,119
с тех пор

1395
00:59:51,119 --> 00:59:53,520
дела только начали развиваться, и с

1396
00:59:53,520 --> 00:59:56,160
каждым годом новые системы машинного перевода

1397
00:59:56,160 --> 00:59:59,119
становятся все лучше и

1398
00:59:59,119 --> 01:00:01,200
лучше, чем все, что у нас было до

1399
01:00:01,200 --> 01:00:02,559
этого.

1400
01:00:02,559 --> 01:00:04,000


1401
01:00:04,000 --> 01:00:07,839
по крайней мере, ранняя часть

1402
01:00:07,839 --> 01:00:10,160
применения нейронного машинного перевода глубокого обучения и

1403
01:00:10,160 --> 01:00:12,640
обработки естественного языка

1404
01:00:12,640 --> 01:00:15,839
была огромной

1405
01:00:15,839 --> 01:00:19,119
историей успеха за последние несколько

1406
01:00:19,119 --> 01:00:22,720
лет, когда у нас были такие модели, как gpt2

1407
01:00:22,720 --> 01:00:24,400
и gpt3,

1408
01:00:24,400 --> 01:00:28,160
и другие огромные нейронные модели, такие как птица,

1409
01:00:28,160 --> 01:00:30,000
улучшающие

1410
01:00:30,000 --> 01:00:31,760
поиск в Интернете  знаю, что это немного

1411
01:00:31,760 --> 01:00:34,720
сложнее, но это была первая область,

1412
01:00:34,720 --> 01:00:37,440
где была нейронная сеть, которая

1413
01:00:37,440 --> 01:00:41,200
была намного лучше, чем то, что она

1414
01:00:41,200 --> 01:00:44,400
существовала, и на самом деле решала практическую

1415
01:00:44,400 --> 01:00:46,799
проблему, в которой нуждалось множество людей в мире,

1416
01:00:46,799 --> 01:00:48,319


1417
01:00:48,319 --> 01:00:50,799
и это была

1418
01:00:50,799 --> 01:00:53,040
потрясающая скорость, с которой  успех был

1419
01:00:53,040 --> 01:00:54,559
достигнут,

1420
01:00:54,559 --> 01:00:55,680
так что

1421
01:00:55,680 --> 01:00:57,920
2014 год

1422
01:00:57,920 --> 01:01:00,319
был первым, что я называю здесь,

1423
01:01:00,319 --> 01:01:03,119
попытками исследования создать в

1424
01:01:03,119 --> 01:01:06,240
вашей системе машинного перевода, что означает,

1425
01:01:06,240 --> 01:01:09,119
что три или четыре человека, которые

1426
01:01:09,119 --> 01:01:12,240
работали над моделями нейронных сетей, подумали: «А

1427
01:01:12,240 --> 01:01:13,920
почему бы нам не посмотреть, можем ли мы использовать  один из

1428
01:01:13,920 --> 01:01:16,160
них переводить научиться переводить

1429
01:01:16,160 --> 01:01:18,160
предложения, где на самом деле они не были

1430
01:01:18,160 --> 01:01:19,680
людьми с опытом работы в машинном

1431
01:01:19,680 --> 01:01:22,960
переводе, но успех был

1432
01:01:22,960 --> 01:01:24,640
достигнут

1433
01:01:24,640 --> 01:01:29,119
так быстро  То, что в течение двух лет

1434
01:01:29,119 --> 01:01:31,280
Google переключился на использование нейронного

1435
01:01:31,280 --> 01:01:33,280
машинного перевода

1436
01:01:33,280 --> 01:01:34,640
для

1437
01:01:34,640 --> 01:01:37,520
большинства языков, а через пару

1438
01:01:37,520 --> 01:01:40,559
лет после этого практически любой, кто

1439
01:01:40,559 --> 01:01:42,799
занимается машинным переводом, теперь

1440
01:01:42,799 --> 01:01:45,839
развертывает живые системы нейронного машинного

1441
01:01:45,839 --> 01:01:49,200
перевода и получает

1442
01:01:49,200 --> 01:01:51,520
гораздо лучшие результаты, так что  был своего рода

1443
01:01:51,520 --> 01:01:54,640
удивительным технологическим переходом

1444
01:01:54,640 --> 01:01:55,520
,

1445
01:01:55,520 --> 01:01:57,760
поскольку за предыдущее десятилетие большие

1446
01:01:57,760 --> 01:02:00,240
системы статистического машинного перевода,

1447
01:02:00,240 --> 01:02:02,079
такие как предыдущее поколение Google

1448
01:02:02,079 --> 01:02:04,559
Translate, буквально создавались

1449
01:02:04,559 --> 01:02:06,400
сотнями инженеров

1450
01:02:06,400 --> 01:02:08,319
за годы,

1451
01:02:08,319 --> 01:02:13,039
но сравнительно небольшая группа людей,

1452
01:02:13,039 --> 01:02:16,319
занимающихся глубоким обучением, за несколько месяцев

1453
01:02:16,319 --> 01:02:18,960
с небольшим количеством кода,

1454
01:02:18,960 --> 01:02:20,640
и, надеюсь, вы даже почувствуете

1455
01:02:20,640 --> 01:02:23,359
это, выполняя задание 4, мы можем

1456
01:02:23,359 --> 01:02:24,240
создавать

1457
01:02:24,240 --> 01:02:26,960
новые системы машинного перевода, которые

1458
01:02:26,960 --> 01:02:30,240
работают намного лучше

1459
01:02:30,240 --> 01:02:32,079
, означает ли это,

1460
01:02:32,079 --> 01:02:35,520
что машинный перевод решен.

1461
01:02:35,520 --> 01:02:37,599
много разной легкости, над которой люди

1462
01:02:37,599 --> 01:02:40,799
продолжают очень активно работать, и вы можете узнать

1463
01:02:40,799 --> 01:02:42,720
больше о  он в небе, но сегодня

1464
01:02:42,720 --> 01:02:44,960
статья была связана внизу, но вы

1465
01:02:44,960 --> 01:02:46,559
знаете, что есть много проблем

1466
01:02:46,559 --> 01:02:48,799
без словарного запаса,

1467
01:02:48,799 --> 01:02:51,200
они несоответствия предметной области между

1468
01:02:51,200 --> 01:02:53,680
обучающими и тестовыми данными, поэтому его можно

1469
01:02:53,680 --> 01:02:56,000
обучать в основном на данных новостной ленты, но вы

1470
01:02:56,000 --> 01:02:57,359
хотите

1471
01:02:57,359 --> 01:03:00,640
переводить людей  сообщения facebook

1472
01:03:00,640 --> 01:03:02,079
по-прежнему существуют проблемы с

1473
01:03:02,079 --> 01:03:04,799
поддержанием контекста поверх более длинного текста, который

1474
01:03:04,799 --> 01:03:07,760
мы хотели бы перевести на языки, для

1475
01:03:07,760 --> 01:03:10,799
которых у нас мало данных, и поэтому

1476
01:03:10,799 --> 01:03:13,520
эти методы работают намного лучше, когда

1477
01:03:13,520 --> 01:03:17,440
у нас есть огромные объемы параллельных данных,

1478
01:03:17,440 --> 01:03:19,119
даже наши лучшие

1479
01:03:19,119 --> 01:03:22,640
мульти  -layer lstms не так хорош для

1480
01:03:22,640 --> 01:03:24,799
захвата предложения, что означает, что существуют

1481
01:03:24,799 --> 01:03:28,079
определенные проблемы, такие как интерпретация

1482
01:03:28,079 --> 01:03:31,119
того, к чему относятся местоимения, или в таких языках,

1483
01:03:31,119 --> 01:03:34,319
как китайский или японский,

1484
01:03:34,319 --> 01:03:36,880
где часто нет местоимения, но

1485
01:03:36,880 --> 01:03:38,880
есть подразумеваемая ссылка на

1486
01:03:38,880 --> 01:03:41,839
человека, который разрабатывает, как  переведите, что

1487
01:03:41,839 --> 01:03:43,440
для языков в законах есть много

1488
01:03:43,440 --> 01:03:45,680
флективных

1489
01:03:45,680 --> 01:03:47,920
форм существительных, глаголов и прилагательных, эти

1490
01:03:47,920 --> 01:03:50,160
системы часто ошибаются, поэтому все

1491
01:03:50,160 --> 01:03:52,960
еще существует множество  что нужно сделать

1492
01:03:52,960 --> 01:03:55,119
, вот просто

1493
01:03:55,119 --> 01:03:57,119
забавные примеры того, что

1494
01:03:57,119 --> 01:04:00,079
что-то идет не так, как надо, так что если вы

1495
01:04:00,079 --> 01:04:01,680
попросили перевести

1496
01:04:01,680 --> 01:04:04,160
бумажное варенье,

1497
01:04:04,160 --> 01:04:06,319
Google Translate решит, что

1498
01:04:06,319 --> 01:04:09,920
это разновидность варенья, точно так же, как

1499
01:04:09,920 --> 01:04:11,599
есть малиновое варенье

1500
01:04:11,599 --> 01:04:13,039
и

1501
01:04:13,039 --> 01:04:15,599
клубничное варенье  и это превращается в

1502
01:04:15,599 --> 01:04:16,799
застрявшую бумагу

1503
01:04:16,799 --> 01:04:18,559
эм,

1504
01:04:18,559 --> 01:04:20,559
есть

1505
01:04:20,559 --> 01:04:23,599
проблемы с согласованием и выбором,

1506
01:04:23,599 --> 01:04:27,119
поэтому, если у вас много языков, не делайте

1507
01:04:27,119 --> 01:04:30,720
различий по полу, и поэтому предложения,

1508
01:04:30,720 --> 01:04:34,720
э-э, нейтральные между вещами, являются мужскими

1509
01:04:34,720 --> 01:04:37,440
или женскими, так что малайский или турецкий - два

1510
01:04:37,440 --> 01:04:40,160
хорошо известных языка  такого рода,

1511
01:04:40,160 --> 01:04:41,760
но что происходит, когда это

1512
01:04:41,760 --> 01:04:43,920
переводится на английский с помощью Google

1513
01:04:43,920 --> 01:04:46,319
Translate, так это то, что

1514
01:04:46,319 --> 01:04:47,280


1515
01:04:47,280 --> 01:04:49,920
модель английского языка просто срабатывает и

1516
01:04:49,920 --> 01:04:53,200
применяет стереотипные предубеждения, и поэтому

1517
01:04:53,200 --> 01:04:55,839
эти гендерно-нейтральные предложения

1518
01:04:55,839 --> 01:04:58,640
переводятся на то, что она работает медсестрой,

1519
01:04:58,640 --> 01:05:01,280
он работает программистом, поэтому, если вы

1520
01:05:01,280 --> 01:05:04,079
хотите помочь решить эту проблему, вы все

1521
01:05:04,079 --> 01:05:07,039
можете помочь, используя единственное их число во

1522
01:05:07,039 --> 01:05:08,640
всех контекстах,

1523
01:05:08,640 --> 01:05:09,599
когда вы

1524
01:05:09,599 --> 01:05:11,839
размещаете материал в Интернете, и это может

1525
01:05:11,839 --> 01:05:13,599
затем изменить дистрибутив

1526
01:05:13,599 --> 01:05:14,880


1527
01:05:14,880 --> 01:05:16,960
Но люди также

1528
01:05:16,960 --> 01:05:18,960
работают над улучшением моделирования, чтобы попытаться

1529
01:05:18,960 --> 01:05:20,480
избежать этого.

1530
01:05:20,480 --> 01:05:22,960
Вот еще один забавный пример.

1531
01:05:22,960 --> 01:05:25,760


1532
01:05:25,760 --> 01:05:29,680
Пару лет назад люди заметили,

1533
01:05:29,680 --> 01:05:32,799
что если вы выберете один из более редких

1534
01:05:32,799 --> 01:05:34,240
языков, на

1535
01:05:34,240 --> 01:05:37,039
который Google будет переводить, например,

1536
01:05:37,039 --> 01:05:38,319
сомали.

1537
01:05:38,319 --> 01:05:39,680
гм

1538
01:05:39,680 --> 01:05:43,280
, и вы просто пишете в какой-

1539
01:05:43,280 --> 01:05:46,079
то ерунде, как кляп для геев, как ни

1540
01:05:46,079 --> 01:05:49,520
странно, это произвело бы из ниоткуда

1541
01:05:49,520 --> 01:05:52,559
пророческие и библейские тексты, поскольку

1542
01:05:52,559 --> 01:05:54,240
имя лорда было написано на иврите,

1543
01:05:54,240 --> 01:05:55,920
оно было написано на

1544
01:05:55,920 --> 01:05:57,920
языке еврейской нации,

1545
01:05:57,920 --> 01:06:01,839
что не делает  В общем,

1546
01:06:02,240 --> 01:06:04,240
мы собираемся узнать немного больше о том, почему

1547
01:06:04,240 --> 01:06:06,480
это происходит, ммм,

1548
01:06:06,480 --> 01:06:08,000
но

1549
01:06:08,000 --> 01:06:10,559
ах, это было немного тревожно,

1550
01:06:10,559 --> 01:06:12,720
насколько я понимаю, эта проблема

1551
01:06:12,720 --> 01:06:14,960
теперь исправлена в 2021 году, я не мог

1552
01:06:14,960 --> 01:06:17,200
получить переводчик Google  гм, чтобы больше создавать

1553
01:06:17,200 --> 01:06:21,280
подобные примеры,

1554
01:06:21,280 --> 01:06:23,680
но вы знаете, что есть много способов

1555
01:06:23,680 --> 01:06:26,880
продолжить исследования.

1556
01:06:26,880 --> 01:06:29,119


1557
01:06:29,119 --> 01:06:33,119


1558
01:06:33,119 --> 01:06:36,000


1559
01:06:36,000 --> 01:06:37,839
нововведения в

1560
01:06:37,839 --> 01:06:40,079
глубоком обучении nlp

1561
01:06:40,079 --> 01:06:42,480
были пионерами, и люди продолжают

1562
01:06:42,480 --> 01:06:45,039
упорно работать над этим люди нашли много-много

1563
01:06:45,039 --> 01:06:46,400
улучшений

1564
01:06:46,400 --> 01:06:49,359
эм, и на самом деле для последней части

1565
01:06:49,359 --> 01:06:51,760
урока в эту минуту я собираюсь

1566
01:06:51,760 --> 01:06:54,960
представить одно огромное улучшение, которое настолько

1567
01:06:54,960 --> 01:06:56,960
важно, что оно действительно произошло

1568
01:06:56,960 --> 01:06:59,520
доминировать над всей недавней областью

1569
01:06:59,520 --> 01:07:01,760


1570
01:07:01,760 --> 01:07:04,559
нейронных сетей Ньюэлла для НЛП, и это

1571
01:07:04,559 --> 01:07:06,400
идея внимания,

1572
01:07:06,400 --> 01:07:08,960
но прежде чем я перейду к вниманию, я хочу

1573
01:07:08,960 --> 01:07:10,240
потратить

1574
01:07:10,240 --> 01:07:13,920
три минуты на наше задание,

1575
01:07:13,920 --> 01:07:16,480
поэтому для четвертого задания в этом году

1576
01:07:16,480 --> 01:07:19,039
у нас есть новый  версия

1577
01:07:19,039 --> 01:07:21,839
задания, которое, как мы надеемся, будет

1578
01:07:21,839 --> 01:07:24,160
интересным, но это также настоящая

1579
01:07:24,160 --> 01:07:26,480
проблема, поэтому для четвертого задания в этом

1580
01:07:26,480 --> 01:07:27,680
году

1581
01:07:27,680 --> 01:07:30,160
мы решили сделать машинный перевод чероки на английский

1582
01:07:30,160 --> 01:07:32,079
язык,

1583
01:07:32,079 --> 01:07:34,400
так что чероки является вымирающим

1584
01:07:34,400 --> 01:07:36,640
языком коренных американцев, на нем

1585
01:07:36,640 --> 01:07:38,640
свободно говорят около 2000 человек

1586
01:07:38,640 --> 01:07:41,520
.  язык чрезвычайно мало ресурсов,

1587
01:07:41,520 --> 01:07:44,000
поэтому просто не так много

1588
01:07:44,000 --> 01:07:47,359
доступных письменных данных чероки,

1589
01:07:47,359 --> 01:07:50,079
и особенно не так много

1590
01:07:50,079 --> 01:07:52,960
параллельных предложений между чероки  и

1591
01:07:52,960 --> 01:07:54,079
английский,

1592
01:07:54,079 --> 01:07:56,960
и вот ответ на

1593
01:07:56,960 --> 01:08:00,400
причудливые пророческие переводы Google

1594
01:08:00,400 --> 01:08:04,079
для языков, для которых не так

1595
01:08:04,079 --> 01:08:08,079
много параллельных данных, как правило,

1596
01:08:08,079 --> 01:08:10,400
самое большое место, где вы можете получить

1597
01:08:10,400 --> 01:08:14,559
параллельные данные, - это переводы Библии,

1598
01:08:14,559 --> 01:08:16,479
поэтому вы можете

1599
01:08:16,479 --> 01:08:18,799
иметь свой личный выбор, где бы он ни находился

1600
01:08:18,799 --> 01:08:21,359


1601
01:08:21,359 --> 01:08:23,759
Вы не знаете, где вы находитесь в отношении религии,

1602
01:08:23,759 --> 01:08:25,920
но дело в том, что если вы

1603
01:08:25,920 --> 01:08:28,319
работаете над языками коренных народов, вы

1604
01:08:28,319 --> 01:08:30,000


1605
01:08:30,000 --> 01:08:33,439
очень быстро обнаруживаете, что большая часть

1606
01:08:33,439 --> 01:08:35,920
работы была проделана по сбору данных о

1607
01:08:35,920 --> 01:08:38,319
языках коренных народов и  много

1608
01:08:38,319 --> 01:08:41,040
материала, доступного в письменной

1609
01:08:41,040 --> 01:08:43,839
форме для многих языков коренных народов, - это

1610
01:08:43,839 --> 01:08:46,799
переводы Библии,

1611
01:08:46,799 --> 01:08:50,560
да ладно, так вот как выглядит чероки

1612
01:08:50,560 --> 01:08:52,960


1613
01:08:52,960 --> 01:08:54,399
, так что

1614
01:08:54,399 --> 01:08:56,399
вы можете видеть, что система письма

1615
01:08:56,399 --> 01:08:59,439
имеет смесь вещей, которые выглядят как

1616
01:08:59,439 --> 01:09:02,158
английские буквы, а затем все

1617
01:09:02,158 --> 01:09:04,719
письма, которых нет, и вот

1618
01:09:04,719 --> 01:09:05,759


1619
01:09:05,759 --> 01:09:09,040
начальная часть истории давным-давно о семи мальчиках,

1620
01:09:09,040 --> 01:09:11,279
которые проводили все свое время

1621
01:09:11,279 --> 01:09:13,040
у таунхауса, так что это  фрагмент

1622
01:09:13,040 --> 01:09:17,040
параллельных данных, который мы можем изучить, так

1623
01:09:17,040 --> 01:09:20,158
что система письма чероки состоит из 85

1624
01:09:20,158 --> 01:09:22,319
букв, и причина, по которой в ней так

1625
01:09:22,319 --> 01:09:25,198
много букв, заключается в том, что каждая из этих

1626
01:09:25,198 --> 01:09:28,319
букв на самом деле представляет собой слог,

1627
01:09:28,319 --> 01:09:31,279
поэтому многие языки мира

1628
01:09:31,279 --> 01:09:34,399
имеют строгую структуру согласных гласных слогов

1629
01:09:34,399 --> 01:09:37,040
так что у вас есть такие слова, как rata

1630
01:09:37,040 --> 01:09:39,359
per или что-то в этом роде,

1631
01:09:39,359 --> 01:09:41,600
или чероки

1632
01:09:41,600 --> 01:09:42,880
право,

1633
01:09:42,880 --> 01:09:44,640
и другой язык, такой как

1634
01:09:44,640 --> 01:09:46,640
гавайский,

1635
01:09:46,640 --> 01:09:49,040
и поэтому каждая из букв представляет собой

1636
01:09:49,040 --> 01:09:52,319
комбинацию согласной и гласной,

1637
01:09:52,319 --> 01:09:54,000
и гм,

1638
01:09:54,000 --> 01:09:57,360
это набор из них, а затем вы получите 17

1639
01:09:57,360 --> 01:10:00,640
на пять  дает вам 85 букв,

1640
01:10:00,640 --> 01:10:01,440


1641
01:10:01,440 --> 01:10:03,679
да, так что за возможность выполнить это задание

1642
01:10:03,679 --> 01:10:06,239
большое спасибо людям из

1643
01:10:06,239 --> 01:10:09,760
университета Северной Каролины, Чапел Хилл, которые

1644
01:10:09,760 --> 01:10:12,239
предоставили ресурсы, которые мы используем

1645
01:10:12,239 --> 01:10:14,800
для этого задания,

1646
01:10:14,800 --> 01:10:16,719
хотя вы можете говорить на довольно многих

1647
01:10:16,719 --> 01:10:18,960
языках на  google translate um

1648
01:10:18,960 --> 01:10:21,600
cherokee - это не язык, который Google

1649
01:10:21,600 --> 01:10:24,159
предлагает на google translate, поэтому мы можем видеть,

1650
01:10:24,159 --> 01:10:27,120
как далеко мы можем зайти, но мы должны

1651
01:10:27,120 --> 01:10:29,199
быть скромными в наших ожиданиях, потому

1652
01:10:29,199 --> 01:10:32,480
что это h  Мы стремимся создать очень хорошую MT-систему

1653
01:10:32,480 --> 01:10:34,960
с довольно ограниченным объемом

1654
01:10:34,960 --> 01:10:37,920
данных, поэтому мы увидим, как далеко мы можем зайти,

1655
01:10:37,920 --> 01:10:40,239
есть обратная сторона, которая для вас,

1656
01:10:40,239 --> 01:10:41,760
студентов, выполняющих

1657
01:10:41,760 --> 01:10:44,159
задание. Преимущество наличия не

1658
01:10:44,159 --> 01:10:46,400
слишком большого количества данных заключается в том, что вы  модели будут

1659
01:10:46,400 --> 01:10:48,560
обучаться относительно быстро, поэтому у нас на

1660
01:10:48,560 --> 01:10:50,800
самом деле будет меньше проблем, чем в

1661
01:10:50,800 --> 01:10:52,080
прошлом году,

1662
01:10:52,080 --> 01:10:54,080
когда модели людей потратят часы на

1663
01:10:54,080 --> 01:10:55,199
обучение, так

1664
01:10:55,199 --> 01:10:59,120
как крайний срок назначения подошел

1665
01:10:59,120 --> 01:11:01,199
к концу. Еще пара слов о

1666
01:11:01,199 --> 01:11:03,280
чероки, чтобы мы имели некоторое представление о том, о чем

1667
01:11:03,280 --> 01:11:04,719
говорим,

1668
01:11:04,719 --> 01:11:07,199
поэтому  чероки первоначально

1669
01:11:07,199 --> 01:11:10,400
жили в западной Северной Каролине и

1670
01:11:10,400 --> 01:11:13,440
теннисе в восточном Теннесси,

1671
01:11:13,440 --> 01:11:16,320
потом их как бы перевели на

1672
01:11:16,320 --> 01:11:19,040
юго-запад оттуда, а затем, в

1673
01:11:19,040 --> 01:11:20,400
частности,

1674
01:11:20,400 --> 01:11:22,239
для тех из вас, кто

1675
01:11:22,239 --> 01:11:24,960
ходил в американские средние школы и обращал

1676
01:11:24,960 --> 01:11:27,760
внимание на то, что вы, возможно, помните

1677
01:11:27,760 --> 01:11:30,480
обсуждение следа слез  когда

1678
01:11:30,480 --> 01:11:32,239
многие коренные американцы с

1679
01:11:32,239 --> 01:11:35,679
юго-востока США были

1680
01:11:35,679 --> 01:11:36,960
насильно вытеснены

1681
01:11:36,960 --> 01:11:40,719
далеко на запад, и поэтому большинство

1682
01:11:40,719 --> 01:11:43,600
чероки теперь живут в Оклахоме, хотя

1683
01:11:43,600 --> 01:11:46,080
есть и некоторые  в Северной

1684
01:11:46,080 --> 01:11:47,440
Каролине

1685
01:11:47,440 --> 01:11:50,239
система письма, которую я показал на этом

1686
01:11:50,239 --> 01:11:53,679
предыдущем слайде, была изобретена

1687
01:11:53,679 --> 01:11:54,880


1688
01:11:54,880 --> 01:11:57,520
секвойей человека чероки, это

1689
01:11:57,520 --> 01:11:59,280
его рисунок,

1690
01:11:59,280 --> 01:12:01,360
и это было на самом деле своего рода

1691
01:12:01,360 --> 01:12:04,960
невероятной вещью, поэтому он начал с

1692
01:12:04,960 --> 01:12:07,360
грамотности и

1693
01:12:07,360 --> 01:12:09,600
разработал, как

1694
01:12:09,600 --> 01:12:11,199
напишите или

1695
01:12:11,199 --> 01:12:13,199
создайте систему письма, которая была бы

1696
01:12:13,199 --> 01:12:16,480
хорошей для эм чероки, и, учитывая,

1697
01:12:16,480 --> 01:12:18,000
что у нее есть эта непревзойденная структура гласных,

1698
01:12:18,000 --> 01:12:21,440
он выбрал знаменитость, которая

1699
01:12:21,440 --> 01:12:24,480
оказалась хорошим выбором.

1700
01:12:24,480 --> 01:12:27,520


1701
01:12:27,520 --> 01:12:30,560
1840-е гг.

1702
01:12:30,560 --> 01:12:34,480
Процент чероки, которые были

1703
01:12:34,480 --> 01:12:36,159
грамотны

1704
01:12:36,159 --> 01:12:39,040
на языке чероки, написанном таким образом, был на

1705
01:12:39,040 --> 01:12:41,440
самом деле выше, чем процент

1706
01:12:41,440 --> 01:12:43,920
белых людей на юго-востоке

1707
01:12:43,920 --> 01:12:46,960
США в то время

1708
01:12:46,960 --> 01:12:48,400


1709
01:12:48,400 --> 01:12:51,199


1710
01:12:51,199 --> 01:12:53,360


1711
01:12:53,360 --> 01:12:55,920
эм, а потом мне нужно

1712
01:12:55,920 --> 01:12:58,480
будет сделать еще немного этого эм, мне придется

1713
01:12:58,480 --> 01:13:00,159
сделать еще немного в следующий раз, это

1714
01:13:00,159 --> 01:13:02,960
будет хорошо, так что последняя идея, которая

1715
01:13:02,960 --> 01:13:05,280
действительно важна для

1716
01:13:05,280 --> 01:13:07,679
секвенсора  Последовательность моделей - это

1717
01:13:07,679 --> 01:13:09,120
идея внимания,

1718
01:13:09,120 --> 01:13:13,440
и поэтому у нас была эта модель

1719
01:13:13,440 --> 01:13:14,239


1720
01:13:14,239 --> 01:13:15,199
выполнения

1721
01:13:15,199 --> 01:13:17,360
последовательности для моделей последовательности, таких как

1722
01:13:17,360 --> 01:13:19,600
нейронный машинный перевод,

1723
01:13:19,600 --> 01:13:23,360
и проблема с этой архитектурой

1724
01:13:23,360 --> 01:13:24,880
заключается в том, что

1725
01:13:24,880 --> 01:13:28,159
у нас есть одно скрытое состояние, которое

1726
01:13:28,159 --> 01:13:31,600
должно кодировать всю информацию о

1727
01:13:31,600 --> 01:13:34,560
исходное предложение, поэтому оно действует как своего

1728
01:13:34,560 --> 01:13:37,040
рода информационное узкое место, и это

1729
01:13:37,040 --> 01:13:39,840
вся информация, которой обусловлено поколение.

1730
01:13:39,840 --> 01:13:41,600


1731
01:13:41,600 --> 01:13:42,400


1732
01:13:42,400 --> 01:13:45,840


1733
01:13:45,840 --> 01:13:48,719


1734
01:13:48,719 --> 01:13:50,400


1735
01:13:50,400 --> 01:13:52,239
усреднить все векторы

1736
01:13:52,239 --> 01:13:54,800
источника, чтобы получить представление предложения,

1737
01:13:54,800 --> 01:13:57,120
но вы знаете, что этот метод оказывается

1738
01:13:57,120 --> 01:13:59,360
лучше для таких вещей, как анализ тональности,

1739
01:13:59,360 --> 01:14:00,400


1740
01:14:00,400 --> 01:14:03,040
и не очень хорош для машинного перевода,

1741
01:14:03,040 --> 01:14:05,199
где очень важно сохранять порядок слов,

1742
01:14:05,199 --> 01:14:07,040


1743
01:14:07,040 --> 01:14:10,800
поэтому кажется, что  нам было бы лучше,

1744
01:14:10,800 --> 01:14:11,600
если бы

1745
01:14:11,600 --> 01:14:14,560
мы каким-то образом могли получить больше информации

1746
01:14:14,560 --> 01:14:16,640
из исходного предложения,

1747
01:14:16,640 --> 01:14:20,080
пока мы генерируем перевод,

1748
01:14:20,080 --> 01:14:22,000
и в некотором смысле это просто соответствует  Если

1749
01:14:22,000 --> 01:14:24,640


1750
01:14:24,640 --> 01:14:27,360
вы переводчик-человек

1751
01:14:27,360 --> 01:14:29,600
, вы читаете предложение, которое хотите перевести,

1752
01:14:29,600 --> 01:14:31,760
и, возможно, начинаете переводить несколько

1753
01:14:31,760 --> 01:14:33,600
слов, но затем оглядываетесь на

1754
01:14:33,600 --> 01:14:35,600
исходное предложение, чтобы увидеть, что еще в

1755
01:14:35,600 --> 01:14:39,280
нем было  и перевести еще несколько слов, так что

1756
01:14:39,280 --> 01:14:41,199
очень быстро после появления первых нейронных

1757
01:14:41,199 --> 01:14:44,000
машинных систем перевода люди

1758
01:14:44,000 --> 01:14:46,800
пришли к идее, что, возможно, мы могли бы

1759
01:14:46,800 --> 01:14:49,520
построить лучшую нейронную МТ-модель,

1760
01:14:49,520 --> 01:14:52,880
которая сделала бы это, и это идея

1761
01:14:52,880 --> 01:14:54,880
напряжения,

1762
01:14:54,880 --> 01:14:57,920
поэтому основная идея находится на каждом этапе

1763
01:14:57,920 --> 01:15:02,159
декодера мы собираемся использовать прямую

1764
01:15:02,159 --> 01:15:06,239
связь между кодировщиком и декодером,

1765
01:15:06,239 --> 01:15:08,640
что позволит нам сосредоточиться на

1766
01:15:08,640 --> 01:15:10,320


1767
01:15:10,320 --> 01:15:12,400
конкретном слове или словах в исходной

1768
01:15:12,400 --> 01:15:16,960
последовательности и использовать его, чтобы помочь нам сгенерировать,

1769
01:15:16,960 --> 01:15:19,520
какие слова будут следующими.

1770
01:15:19,520 --> 01:15:22,400
просто пройдите сейчас, показывая вам

1771
01:15:22,400 --> 01:15:25,360
изображения того, что делает внимание, а затем

1772
01:15:25,360 --> 01:15:27,360
в начале следующего раза мы

1773
01:15:27,360 --> 01:15:31,600
рассмотрим уравнения более подробно,

1774
01:15:31,760 --> 01:15:35,440
поэтому мы генерируем, мы используем наш кодировщик,

1775
01:15:35,440 --> 01:15:37,440
как и раньше, и генерируем наш

1776
01:15:37,440 --> 01:15:39,600


1777
01:15:39,600 --> 01:15:42,560
фид представлений  в нашем кондиционировании, как и раньше, и

1778
01:15:42,560 --> 01:15:45,199
говорим, что мы начинаем наш перевод,

1779
01:15:45,199 --> 01:15:48,000
но на этом этапе мы берем это скрытое

1780
01:15:48,000 --> 01:15:49,600
представление

1781
01:15:49,600 --> 01:15:52,080
и говорим, что я собираюсь использовать это скрытое

1782
01:15:52,080 --> 01:15:54,640
представление, чтобы оглянуться на

1783
01:15:54,640 --> 01:15:57,600
источник, чтобы получить информацию непосредственно из

1784
01:15:57,600 --> 01:15:59,920
него, так что я буду делать

1785
01:15:59,920 --> 01:16:01,440
Я буду

1786
01:16:01,440 --> 01:16:03,280


1787
01:16:03,280 --> 01:16:06,320
сравнивать скрытое состояние декодера со

1788
01:16:06,320 --> 01:16:09,760
скрытым состоянием кодера в каждой

1789
01:16:09,760 --> 01:16:13,679
позиции и сгенерировать оценку внимания,

1790
01:16:13,679 --> 01:16:16,480
которая является своего рода оценкой сходства, такой

1791
01:16:16,480 --> 01:16:18,239
как скалярное произведение,

1792
01:16:18,239 --> 01:16:21,920
а затем на основе этих оценок внимания

1793
01:16:21,920 --> 01:16:24,560
я собираюсь вычислить  распределение вероятностей

1794
01:16:24,560 --> 01:16:26,080


1795
01:16:26,080 --> 01:16:28,560
um

1796
01:16:28,560 --> 01:16:32,640
относительно использования softmax, как обычно, чтобы сказать, какое

1797
01:16:32,640 --> 01:16:35,920
из этих состояний кодировщика

1798
01:16:35,920 --> 01:16:37,679
больше всего похоже на

1799
01:16:37,679 --> 01:16:41,120
мое состояние декодера, и поэтому мы будем

1800
01:16:41,120 --> 01:16:43,600
обучать модель здесь, чтобы она говорила

1801
01:16:43,600 --> 01:16:45,040
хорошо, вероятно, вам следует

1802
01:16:45,040 --> 01:16:47,280
сначала перевести первое слово предложения, поэтому

1803
01:16:47,280 --> 01:16:48,880
вот куда следует направить внимание,

1804
01:16:48,880 --> 01:16:50,080


1805
01:16:50,080 --> 01:16:52,400
поэтому на основе этого распределения внимания,

1806
01:16:52,400 --> 01:16:54,400
которое представляет собой распределение вероятностей,

1807
01:16:54,400 --> 01:16:56,960
выходящее из softmax,

1808
01:16:56,960 --> 01:16:58,400
мы собираемся

1809
01:16:58,400 --> 01:17:00,840
сгенерировать

1810
01:17:00,840 --> 01:17:05,760
новое внимание.  outport, поэтому

1811
01:17:05,760 --> 01:17:08,719
этот вывод внимания будет

1812
01:17:08,719 --> 01:17:10,480
средним из скрытых состояний

1813
01:17:10,480 --> 01:17:12,719
модели кодировщика, но он будет

1814
01:17:12,719 --> 01:17:16,880
средневзвешенным, основанным на нашем распределении внимания,

1815
01:17:16,880 --> 01:17:18,480


1816
01:17:18,480 --> 01:17:20,320
и поэтому мы собираемся взять этот

1817
01:17:20,320 --> 01:17:22,960
вывод внимания, объединить его со

1818
01:17:22,960 --> 01:17:26,560
скрытым  состояние декодера rnn

1819
01:17:26,560 --> 01:17:27,600
um,

1820
01:17:27,600 --> 01:17:29,600
и

1821
01:17:29,600 --> 01:17:33,199
вместе они затем будут

1822
01:17:33,199 --> 01:17:37,440
использоваться для прогнозирования soft max вируса,

1823
01:17:37,440 --> 01:17:40,320
какое слово генерировать первым, и мы

1824
01:17:40,320 --> 01:17:42,719
надеемся сгенерировать его,

1825
01:17:42,719 --> 01:17:45,120
а затем в этот момент мы как бы

1826
01:17:45,120 --> 01:17:49,199
пыхтем и продолжаем делать то же самое

1827
01:17:49,199 --> 01:17:52,560
вид вычислений в каждой позиции

1828
01:17:52,560 --> 01:17:54,719
здесь есть небольшое примечание, в котором

1829
01:17:54,719 --> 01:17:57,360
говорится, что иногда мы берем

1830
01:17:57,360 --> 01:17:59,760
выходной сигнал внимания с предыдущего шага, а также

1831
01:17:59,760 --> 01:18:02,320
передаем его в декодер вместе с

1832
01:18:02,320 --> 01:18:04,880
обычным входом декодера, поэтому мы отвлекаем это

1833
01:18:04,880 --> 01:18:06,719
внимание от фактической подачи его

1834
01:18:06,719 --> 01:18:08,080
вернуться

1835
01:18:08,080 --> 01:18:09,520
к

1836
01:18:09,520 --> 01:18:12,000
вычислению скрытого состояния, и это

1837
01:18:12,000 --> 01:18:14,239
иногда может улучшить производительность,

1838
01:18:14,239 --> 01:18:16,159
и у нас действительно есть этот трюк в

1839
01:18:16,159 --> 01:18:18,560
системе четырех назначений, и вы можете попробовать

1840
01:18:18,560 --> 01:18:20,239
его

1841
01:18:20,239 --> 01:18:23,280
хорошо, поэтому мы генерируем  И сгенерируйте

1842
01:18:23,280 --> 01:18:25,679
все наше предложение

1843
01:18:25,679 --> 01:18:27,440
таким образом,

1844
01:18:27,440 --> 01:18:29,040
и это

1845
01:18:29,040 --> 01:18:32,239
оказалось очень эффективным способом более гибкого получения

1846
01:18:32,239 --> 01:18:34,719
дополнительной информации из исходного

1847
01:18:34,719 --> 01:18:37,600
предложения, чтобы мы могли

1848
01:18:37,600 --> 01:18:38,640


1849
01:18:38,640 --> 01:18:40,719
сгенерировать хороший перевод,

1850
01:18:40,719 --> 01:18:43,440
я остановлюсь на этом сейчас и в

1851
01:18:43,440 --> 01:18:45,920
начале следующего раза.  Закончим это,

1852
01:18:45,920 --> 01:18:47,600
рассмотрев реальные уравнения

1853
01:18:47,600 --> 01:18:51,320
того, как на вас работает внимание.

1854
01:18:54,080 --> 01:18:56,159


