1
00:00:05,520 --> 00:00:07,919
Привет всем и добро пожаловать на

2
00:00:07,919 --> 00:00:12,320
четвертую неделю, так что на четвертой неделе

3
00:00:12,320 --> 00:00:15,280
это будет две половины, так что сегодня

4
00:00:15,280 --> 00:00:17,199
я собираюсь поговорить о

5
00:00:17,199 --> 00:00:19,760
темах, связанных с машинным переводом, а затем

6
00:00:19,760 --> 00:00:22,720
во второй половине недели мы сделаем

7
00:00:22,720 --> 00:00:25,199
небольшой перерыв  от изучения все большего

8
00:00:25,199 --> 00:00:27,760
и большего количества тем о нейронных сетях и разговоров

9
00:00:27,760 --> 00:00:28,880
о

10
00:00:28,880 --> 00:00:31,359
финальных проектах, а также некоторых практических

11
00:00:31,359 --> 00:00:32,399
советов по

12
00:00:32,399 --> 00:00:34,880
построению ваших сетевых систем,

13
00:00:34,880 --> 00:00:37,120
поэтому для сегодняшней лекции

14
00:00:37,120 --> 00:00:40,320
это важный контент для лекции,

15
00:00:40,320 --> 00:00:42,399
поэтому в первую очередь я собираюсь представить

16
00:00:42,399 --> 00:00:43,760
новую задачу

17
00:00:43,760 --> 00:00:45,920
машинного перевода

18
00:00:45,920 --> 00:00:48,879
и  оказывается, что эта

19
00:00:48,879 --> 00:00:52,000
задача является основным вариантом использования нового

20
00:00:52,000 --> 00:00:54,320
архитектурного метода, чтобы научить

21
00:00:54,320 --> 00:00:56,719
вас глубокому обучению, которое представляет собой

22
00:00:56,719 --> 00:00:58,879
модели от последовательности к последовательности, и поэтому мы потратим

23
00:00:58,879 --> 00:01:01,840
на них много времени, а затем

24
00:01:01,840 --> 00:01:03,440
будет разработан важный

25
00:01:03,440 --> 00:01:05,680
способ улучшения  от последовательности к

26
00:01:05,680 --> 00:01:08,080
моделям последовательности, что является идеей

27
00:01:08,080 --> 00:01:10,640
внимания, и именно об этом я буду

28
00:01:10,640 --> 00:01:13,920
говорить в заключительной части урока,

29
00:01:13,920 --> 00:01:16,799
просто проверяя, все ли не отстают

30
00:01:16,799 --> 00:01:20,320
от того, что происходит, поэтому в первую очередь  все

31
00:01:20,320 --> 00:01:22,720
три задания

32
00:01:22,720 --> 00:01:24,960
должны быть выполнены сегодня, так что, надеюсь, вы

33
00:01:24,960 --> 00:01:27,280
все получили свою новую зависимость, проходит

34
00:01:27,280 --> 00:01:29,040
анализ текста хорошо,

35
00:01:29,040 --> 00:01:31,600
в то же время задание 4 выходит

36
00:01:31,600 --> 00:01:35,040
сегодня, и на самом деле сегодняшняя лекция является

37
00:01:35,040 --> 00:01:37,840
основным содержанием того, что вы будете использовать

38
00:01:37,840 --> 00:01:39,439
для создания своего задания  четыре

39
00:01:39,439 --> 00:01:41,119
системы

40
00:01:41,119 --> 00:01:43,119
переключают его ненадолго

41
00:01:43,119 --> 00:01:45,600
для задания четыре мы даем вам

42
00:01:45,600 --> 00:01:48,479
два могущественных дополнительных дня, так что у вас есть девять дней

43
00:01:48,479 --> 00:01:51,759
на это, и он должен быть в четверг,

44
00:01:51,759 --> 00:01:55,360
с другой стороны, пожалуйста, имейте в виду,

45
00:01:55,360 --> 00:01:59,520
что задание 4 больше и сложнее,

46
00:01:59,520 --> 00:02:02,159
чем предыдущее  задания, так что убедитесь,

47
00:02:02,159 --> 00:02:04,560
что вы приступили к этому рано,

48
00:02:04,560 --> 00:02:06,240
а затем, как я уже упоминал в четверг, я

49
00:02:06,240 --> 00:02:09,840
перейду к окончательным проектам,

50
00:02:10,239 --> 00:02:13,680
хорошо, так что давайте сразу перейдем к этому гм

51
00:02:13,680 --> 00:02:14,720
с

52
00:02:14,720 --> 00:02:17,599
машинным переводом, так что очень быстро я

53
00:02:17,599 --> 00:02:20,480
хотел рассказать вам немного о том, что

54
00:02:20,480 --> 00:02:21,760
вы знаете

55
00:02:21,760 --> 00:02:24,239
где  мы были и то, что мы делали,

56
00:02:24,239 --> 00:02:26,160
прежде чем мы перешли к нейронному машинному

57
00:02:26,160 --> 00:02:28,319
переводу, поэтому давайте сделаем

58
00:02:28,319 --> 00:02:31,599
предысторию машинного перевода, чтобы

59
00:02:31,599 --> 00:02:34,480
машинный перевод - это задача

60
00:02:34,480 --> 00:02:36,959
перевода предложения x с одного

61
00:02:36,959 --> 00:02:38,800
языка  который называется исходным

62
00:02:38,800 --> 00:02:41,519
языком для другого языка, целевой

63
00:02:41,519 --> 00:02:44,480
язык формирует предложение y,

64
00:02:44,480 --> 00:02:46,720
поэтому мы начинаем с предложения исходного языка

65
00:02:46,720 --> 00:02:48,239
x

66
00:02:48,239 --> 00:02:50,720
lo mein

67
00:02:50,720 --> 00:02:54,160
um, а затем переводим его и

68
00:02:54,160 --> 00:02:56,959
получаем перевод: человек рождается свободным, но

69
00:02:56,959 --> 00:02:59,280
везде он в цепях

70
00:02:59,280 --> 00:03:01,760
хорошо, так что есть наш машинный перевод,

71
00:03:01,760 --> 00:03:05,599
хорошо, так что в начале 1950-х они

72
00:03:05,599 --> 00:03:09,360
начали работать над машинным переводом, и

73
00:03:09,360 --> 00:03:11,040
так что на самом деле это связано с

74
00:03:11,040 --> 00:03:13,360
информатикой, если вы найдете вещи, в названии которых есть

75
00:03:13,360 --> 00:03:15,360
машина, большинство из них -

76
00:03:17,120 --> 00:03:20,400
старые вещи  это действительно произошло

77
00:03:20,400 --> 00:03:23,360
в контексте США в контексте

78
00:03:23,360 --> 00:03:27,040
холодной войны, гм, поэтому было желание

79
00:03:27,040 --> 00:03:28,720
следить за тем, что делали россияне,

80
00:03:28,720 --> 00:03:31,280
и у людей возникла идея, что,

81
00:03:31,280 --> 00:03:33,519
поскольку некоторые из самых ранних компьютеров

82
00:03:33,519 --> 00:03:37,040
были настолько успешными, гм  занимаясь

83
00:03:37,040 --> 00:03:39,120
взломом кода во время Второй мировой войны,

84
00:03:39,120 --> 00:03:41,360
тогда, возможно, мы могли бы настроить первые

85
00:03:41,360 --> 00:03:43,599
компьютеры для работы

86
00:03:43,599 --> 00:03:47,200
во время холодной войны, чтобы делать перевод,

87
00:03:47,200 --> 00:03:48,720
и, надеюсь, это сыграет, и вы

88
00:03:48,720 --> 00:03:50,799
сможете это услышать  вот небольшой

89
00:03:50,799 --> 00:03:53,439
видеоклип, демонстрирующий некоторые из самых ранних

90
00:03:53,439 --> 00:03:56,239
работ по машинному переводу

91
00:03:56,239 --> 00:03:59,480
с 1954 года.

92
00:04:00,700 --> 00:04:02,239
[Музыка]

93
00:04:02,239 --> 00:04:04,319
они не учли двусмысленности,

94
00:04:04,319 --> 00:04:06,080
когда намеревались использовать компьютеры для

95
00:04:06,080 --> 00:04:07,760
перевода языков.

96
00:04:13,120 --> 00:04:15,760
с русского на английский

97
00:04:15,760 --> 00:04:18,000
вместо математического волшебства предложение на

98
00:04:18,000 --> 00:04:18,959
русском языке

99
00:04:18,959 --> 00:04:20,720
одно из первых нечисловых

100
00:04:20,720 --> 00:04:22,960
приложений компьютеров оно было разрекламировано

101
00:04:22,960 --> 00:04:24,560
как решение одержимости холодной войны

102
00:04:24,560 --> 00:04:26,400
следить за тем, что

103
00:04:26,400 --> 00:04:28,880
делали россияне.

104
00:04:30,479 --> 00:04:32,560
переводчики-люди,

105
00:04:32,560 --> 00:04:33,600
конечно, вы просто находитесь на

106
00:04:33,600 --> 00:04:35,520
экспериментальной стадии, когда вы занимаетесь

107
00:04:35,520 --> 00:04:36,880
полномасштабным производством, какие

108
00:04:36,880 --> 00:04:39,280
мощности мы должны

109
00:04:39,280 --> 00:04:42,080
иметь с современным коммерческим компьютером,

110
00:04:42,080 --> 00:04:44,400
ну, около одного-двух миллионов

111
00:04:44,400 --> 00:04:46,639
часов, и это будет  вполне адекватная

112
00:04:46,639 --> 00:04:48,639
скорость, чтобы справиться со всей

113
00:04:48,639 --> 00:04:50,880
продукцией Советского Союза всего за несколько часов

114
00:04:50,880 --> 00:04:52,880
компьютерного времени в неделю, когда вы

115
00:04:52,880 --> 00:04:54,880
должны уметь  достичь скорости, если наши

116
00:04:54,880 --> 00:04:57,199
эксперименты пройдут хорошо, то, возможно, в течение

117
00:04:57,199 --> 00:04:59,440
пяти лет или около того, и, наконец, мистер

118
00:04:59,440 --> 00:05:02,160
Макдэниэл означает конец человеческим

119
00:05:02,160 --> 00:05:04,720
переводчикам. Это да для

120
00:05:04,720 --> 00:05:06,400
переводчиков научно-технических

121
00:05:06,400 --> 00:05:08,400
материалов, но что касается поэзии и

122
00:05:08,400 --> 00:05:10,080
романов, нет, я не  Думаю, мы когда-нибудь

123
00:05:10,080 --> 00:05:12,080
заменим переводчиков такого рода

124
00:05:12,080 --> 00:05:15,440
материалов. Мистер Макдэниел, большое спасибо,

125
00:05:15,440 --> 00:05:17,520
но, несмотря на шумиху, у него возникли серьезные

126
00:05:17,520 --> 00:05:19,840
проблемы.

127
00:05:30,000 --> 00:05:32,960
Ранние

128
00:05:34,320 --> 00:05:36,960
работы не очень хорошо сработали, я имею в виду, что это было

129
00:05:36,960 --> 00:05:38,720
действительно начало

130
00:05:38,720 --> 00:05:41,360
компьютерной эры в 1950-х, но это

131
00:05:41,360 --> 00:05:44,400
также было началом, как вы знаете, люди

132
00:05:44,400 --> 00:05:46,320
начали понимать науку о

133
00:05:46,320 --> 00:05:48,560
человеческих языках, область лингвистики.

134
00:05:48,560 --> 00:05:50,720
люди плохо

135
00:05:50,720 --> 00:05:53,280
понимали ни одну из сторон

136
00:05:53,280 --> 00:05:56,479
происходящего, так что у вас было то, что люди

137
00:05:56,479 --> 00:05:58,960
пытаются писать системы на действительно

138
00:05:58,960 --> 00:06:01,840
невероятно примитивных компьютерах.

139
00:06:01,840 --> 00:06:04,720
вероятно, случай,

140
00:06:04,720 --> 00:06:07,680
если у вас есть блок питания USB, что

141
00:06:07,680 --> 00:06:09,759
у него больше вычислительных мощностей

142
00:06:09,759 --> 00:06:12,000
внутри, чем у компьютеров, которые

143
00:06:12,000 --> 00:06:15,280
они использовали для перевода um, и так

144
00:06:15,280 --> 00:06:16,960
эффективно то, что вы получали, были

145
00:06:16,960 --> 00:06:20,080
очень простыми системами на основе правил и

146
00:06:20,080 --> 00:06:22,479
поиском слов, поэтому  Было так похоже на словарь

147
00:06:22,479 --> 00:06:24,960
искать слово и получать его перевод,

148
00:06:24,960 --> 00:06:26,880
но это просто не сработало, потому что

149
00:06:26,880 --> 00:06:29,520
человеческие языки намного сложнее,

150
00:06:29,520 --> 00:06:32,319
чем то, что часто слова имеют много значений

151
00:06:32,319 --> 00:06:34,479
и разных смыслов, о чем мы как бы

152
00:06:36,639 --> 00:06:38,639
часто обсуждали там  это идиомы, вам нужно

153
00:06:38,639 --> 00:06:40,240
понимать грамматику, чтобы переписывать

154
00:06:40,240 --> 00:06:43,759
предложения, поэтому по разным причинам это

155
00:06:43,759 --> 00:06:46,800
не сработало, и эта идея была в

156
00:06:46,800 --> 00:06:48,960
значительной степени консервирована, в частности,

157
00:06:48,960 --> 00:06:51,039
в середине 1960-х годов был известный отчет правительства США,

158
00:06:51,039 --> 00:06:54,479
отчет об альпаках, который в основном

159
00:06:54,479 --> 00:06:57,680
заключает это  не работал

160
00:06:57,680 --> 00:06:59,599
ой

161
00:06:59,599 --> 00:07:01,599
хорошо

162
00:07:01,599 --> 00:07:05,280
работа тогда действительно возродилась в искусственном интеллекте при выполнении

163
00:07:05,280 --> 00:07:06,880
основанных на правилах методов машинного

164
00:07:06,880 --> 00:07:10,560
перевода в 90-х гг.

165
00:07:13,919 --> 00:07:17,039
в середине

166
00:07:17,039 --> 00:07:19,039
90-х и когда они были в периоде

167
00:07:19,039 --> 00:07:21,280
статистической НЛП, которую мы видели в других

168
00:07:21,280 --> 00:07:24,000
местах курса,

169
00:07:24,000 --> 00:07:27,840
и тогда возникла идея, можем ли мы начать

170
00:07:27,840 --> 00:07:31,360
с данных о переводе,

171
00:07:31,360 --> 00:07:34,880
то есть предложений и их переводов, и

172
00:07:34,880 --> 00:07:37,280
изучить вероятностную модель, которая может

173
00:07:37,280 --> 00:07:39,280
предсказывать  переводы свежих

174
00:07:39,280 --> 00:07:42,400
предложений, поэтому предположим, что мы переводим с

175
00:07:42,400 --> 00:07:45,440
французского на английский, поэтому мы

176
00:07:45,440 --> 00:07:48,240
хотим построить вероятностную модель,

177
00:07:48,240 --> 00:07:51,039
которая по французскому предложению может сказать,

178
00:07:51,039 --> 00:07:52,479
какова вероятность разных

179
00:07:52,479 --> 00:07:54,720
английских переводов, а затем мы

180
00:07:54,720 --> 00:07:56,840
выберем наиболее вероятный

181
00:07:56,840 --> 00:07:59,520
перевод  гм, тогда мы можем

182
00:07:59,520 --> 00:08:02,240
обнаружить, что удачно было обнаружено, что это было

183
00:08:02,240 --> 00:08:04,800
разбито на два

184
00:08:04,800 --> 00:08:08,000
компонента, просто изменив это с

185
00:08:08,000 --> 00:08:12,160
помощью правила Байеса, поэтому, если бы вместо этого у нас была

186
00:08:12,160 --> 00:08:13,599
вероятность

187
00:08:13,599 --> 00:08:14,639
по

188
00:08:14,639 --> 00:08:17,599
английским предложениям p из y

189
00:08:17,599 --> 00:08:21,199
мм, а затем вероятность французского

190
00:08:21,199 --> 00:08:24,160
предложения с учетом английского предложения  что

191
00:08:24,160 --> 00:08:26,080
люди смогли добиться большего прогресса,

192
00:08:26,080 --> 00:08:28,400
и не сразу понятно,

193
00:08:28,400 --> 00:08:30,240
почему это должно быть, потому что это

194
00:08:30,240 --> 00:08:32,559
просто банальное переписывание с помощью bay  Это

195
00:08:32,559 --> 00:08:34,799
правило позволило

196
00:08:34,799 --> 00:08:37,360
разделить проблему на две части, которые

197
00:08:37,360 --> 00:08:40,958
оказались более решаемыми, поэтому с левой стороны у

198
00:08:40,958 --> 00:08:43,679
вас фактически была модель перевода, в

199
00:08:43,679 --> 00:08:45,920
которой вы могли просто указать

200
00:08:45,920 --> 00:08:49,760
вероятность того, что слова или фразы будут

201
00:08:49,760 --> 00:08:52,959
переведены между двумя языками,

202
00:08:52,959 --> 00:08:54,720
не имея  чтобы позаботиться о

203
00:08:54,720 --> 00:08:57,519
структурном порядке слов в языках,

204
00:08:57,519 --> 00:08:59,680
а затем справа вы увидели

205
00:08:59,680 --> 00:09:02,640
именно то, с чем мы потратили много времени на

206
00:09:02,640 --> 00:09:04,800
прошлой неделе, а это всего лишь

207
00:09:04,800 --> 00:09:07,600
вероятностная языковая модель, поэтому, если у

208
00:09:07,600 --> 00:09:10,480
нас есть очень хорошая модель того, какой хороший

209
00:09:10,480 --> 00:09:13,360
свободный английский  предложения звучат

210
00:09:13,360 --> 00:09:14,959
так, как мы можем построить только из

211
00:09:14,959 --> 00:09:18,560
одноязычных данных, которые мы затем можем получить, чтобы

212
00:09:18,560 --> 00:09:20,800
убедиться, что мы создаем предложения, которые

213
00:09:20,800 --> 00:09:23,600
звучат хорошо, в то время как модель перевода,

214
00:09:23,600 --> 00:09:27,680
надеюсь, вставляет в них правильные слова,

215
00:09:27,920 --> 00:09:29,040
так как

216
00:09:29,040 --> 00:09:31,360
же нам изучить модель перевода, если

217
00:09:31,360 --> 00:09:33,760
у нас нет?  т охватил это, поэтому отправной

218
00:09:33,760 --> 00:09:36,160
точкой было получение большого количества

219
00:09:36,160 --> 00:09:39,200
параллельных данных, которые являются предложениями, переведенными человеком,

220
00:09:39,200 --> 00:09:42,160
и этот момент является обязательным,

221
00:09:42,160 --> 00:09:44,240
чтобы я показал конкретный  Это

222
00:09:44,240 --> 00:09:46,959
изображение розеттского камня, который является знаменитым

223
00:09:46,959 --> 00:09:50,240
оригинальным фрагментом параллельных данных, который

224
00:09:51,279 --> 00:09:54,880
позволил расшифровать египетские иероглифы,

225
00:09:54,880 --> 00:09:57,519
потому что в нем был один и тот же фрагмент текста на

226
00:09:57,519 --> 00:10:00,880
разных языках в современном мире,

227
00:10:00,880 --> 00:10:02,800
которые, к счастью, для людей, которые

228
00:10:02,800 --> 00:10:04,720
создают системы обработки естественного языка,

229
00:10:04,720 --> 00:10:07,600
довольно  несколько мест, где

230
00:10:07,600 --> 00:10:10,160
параллельные данные производятся в больших

231
00:10:10,160 --> 00:10:13,040
количествах, поэтому европейский союз

232
00:10:13,040 --> 00:10:16,480
производит огромное количество параллельного текста

233
00:10:16,480 --> 00:10:19,200
на европейских языках,

234
00:10:20,800 --> 00:10:23,279
извините, французском, а не французском Канадский

235
00:10:23,279 --> 00:10:25,760
парламент удобно производит

236
00:10:25,760 --> 00:10:27,600
параллельный текст на

237
00:10:27,600 --> 00:10:30,640
французском и английском языках и даже

238
00:10:30,640 --> 00:10:32,800
в ограниченном количестве в институте

239
00:10:32,800 --> 00:10:34,640
канадский

240
00:10:34,640 --> 00:10:36,560
эскимос,

241
00:10:36,560 --> 00:10:38,880
а затем парламент

242
00:10:38,880 --> 00:10:42,000
гонконга выпускает английский и китайский языки, поэтому

243
00:10:42,000 --> 00:10:44,079
есть достаточная доступность из

244
00:10:44,079 --> 00:10:46,560
разных источников, и мы можем использовать это для

245
00:10:46,560 --> 00:10:48,640
построения моделей,

246
00:10:48,640 --> 00:10:51,440
так как мы это делаем, хотя все, что у нас есть, это

247
00:10:51,440 --> 00:10:53,839
эти предложения, и не совсем

248
00:10:53,839 --> 00:10:56,079
очевидно, как построить  вероятностная

249
00:10:56,079 --> 00:10:57,680
модель из них,

250
00:10:57,680 --> 00:11:00,399
как и раньше, что мы хотим сделать, это

251
00:11:00,399 --> 00:11:04,320
сломать это  проблема вниз, поэтому в этом случае

252
00:11:04,320 --> 00:11:06,640
мы собираемся ввести

253
00:11:06,640 --> 00:11:09,600
дополнительную переменную, которая является переменной выравнивания,

254
00:11:09,600 --> 00:11:13,120
так что a - это переменная выравнивания,

255
00:11:13,120 --> 00:11:15,920
которая будет давать соответствие на уровне слова или

256
00:11:15,920 --> 00:11:18,959
иногда на уровне фразы

257
00:11:18,959 --> 00:11:21,680
между частями исходного предложения

258
00:11:21,680 --> 00:11:23,519
и  целевое предложение,

259
00:11:23,519 --> 00:11:27,040
так что это пример согласования,

260
00:11:27,040 --> 00:11:30,560
и поэтому, если бы мы могли вызвать это согласование

261
00:11:30,560 --> 00:11:32,560
между двумя

262
00:11:32,560 --> 00:11:35,440
предложениями, тогда у нас могли бы быть

263
00:11:35,440 --> 00:11:38,800
части вероятности того, насколько вероятно

264
00:11:38,800 --> 00:11:41,760
слово или короткая фраза

265
00:11:41,760 --> 00:11:44,720
переводится определенным образом,

266
00:11:45,200 --> 00:11:47,200
и

267
00:11:47,200 --> 00:11:50,079
в целом вы знаете

268
00:11:50,079 --> 00:11:52,880
выравнивание определяет соответствие между словами,

269
00:11:52,880 --> 00:11:54,639
которое

270
00:11:54,639 --> 00:11:56,959
фиксирует грамматические различия

271
00:11:56,959 --> 00:12:00,480
между языками, поэтому слова будут встречаться в

272
00:12:00,480 --> 00:12:02,560
разном порядке в разных языках в

273
00:12:02,560 --> 00:12:04,720
зависимости от того, в каком

274
00:12:04,720 --> 00:12:07,680
языке предмет ставится перед глаголом,

275
00:12:07,680 --> 00:12:10,160
или предмет после глагола или

276
00:12:10,160 --> 00:12:12,000
глагол перед  и субъект, и

277
00:12:12,000 --> 00:12:13,680
объект,

278
00:12:13,680 --> 00:12:15,600
и выравнивания также уловят

279
00:12:15,600 --> 00:12:17,760
кое-что о различиях в

280
00:12:17,760 --> 00:12:20,480
том, как слово «язык»  Мы делаем что-то, поэтому

281
00:12:20,480 --> 00:12:22,839
мы находим, что у нас есть все

282
00:12:22,839 --> 00:12:25,760
возможности того, как слова могут быть согласованы

283
00:12:25,760 --> 00:12:29,839
между языками, чтобы у вас могли быть слова,

284
00:12:29,839 --> 00:12:33,040
которые вообще не переводятся на

285
00:12:33,040 --> 00:12:34,560
другой язык,

286
00:12:34,560 --> 00:12:36,240
поэтому на французском языке

287
00:12:36,240 --> 00:12:39,040
вы помещаете определенную статью перед

288
00:12:39,040 --> 00:12:42,160
названиями стран, например  Япония, поэтому, когда это

289
00:12:42,160 --> 00:12:44,000
переводится на английский, вы просто получаете

290
00:12:44,000 --> 00:12:47,360
Japan, поэтому нет перевода,

291
00:12:47,360 --> 00:12:49,200
поэтому он просто уходит.

292
00:13:02,800 --> 00:13:04,480
последнее французское слово

293
00:13:04,480 --> 00:13:07,200
переводится как аборигены как

294
00:13:07,200 --> 00:13:08,959
несколько слов,

295
00:13:08,959 --> 00:13:11,760
вы можете получить обратное, где у вас может

296
00:13:11,760 --> 00:13:14,880
быть несколько французских слов, которые

297
00:13:14,880 --> 00:13:18,440
переводятся как одно английское слово, поэтому в

298
00:13:18,440 --> 00:13:21,200
приложении Nissan реализованы переводчики,

299
00:13:22,800 --> 00:13:24,720
и вы можете получить, но еще более

300
00:13:24,720 --> 00:13:28,480
сложный, мм  Итак, здесь у нас

301
00:13:28,480 --> 00:13:29,600
есть

302
00:13:29,600 --> 00:13:32,160
четыре английских слова, которые переводятся как

303
00:13:32,160 --> 00:13:34,959
два французских слова, но они на самом деле не

304
00:13:34,959 --> 00:13:37,680
ломаются и хорошо переводят друг друга,

305
00:13:37,680 --> 00:13:39,600
я имею в виду, что это не только  случаются в

306
00:13:39,600 --> 00:13:42,320
разных языках, они также случаются

307
00:13:42,320 --> 00:13:43,680
в языке, когда вы по-

308
00:13:43,680 --> 00:13:46,560
разному говорите одно и то же, так что другой

309
00:13:46,560 --> 00:13:47,360
способ, которым

310
00:13:47,360 --> 00:13:50,000
вы могли бы выразиться, э-э, бедные, у

311
00:13:50,000 --> 00:13:52,639
которых нет денег, - это сказать, что у бедных

312
00:13:52,639 --> 00:13:55,199
нет денег, и это гораздо больше

313
00:13:55,199 --> 00:13:58,240
похоже на то, как

314
00:13:58,240 --> 00:14:00,639
здесь отображается французский, поэтому даже с английского на

315
00:14:00,639 --> 00:14:02,240
английский у вас есть такая же

316
00:14:02,240 --> 00:14:05,279
проблема с выравниванием,

317
00:14:05,279 --> 00:14:07,600
поэтому в вероятностном или статистическом

318
00:14:07,600 --> 00:14:10,000
машинном переводе более широко

319
00:14:10,000 --> 00:14:11,279
известно,

320
00:14:11,279 --> 00:14:14,000
что мы хотели сделать, это изучить эти

321
00:14:14,000 --> 00:14:15,360
выравнивания,

322
00:14:15,360 --> 00:14:17,519
и есть куча источников

323
00:14:17,519 --> 00:14:19,440
информации, которые вы могли бы  используйте, если вы начинаете

324
00:14:19,440 --> 00:14:21,920
с параллельных предложений,

325
00:14:21,920 --> 00:14:24,320
вы можете увидеть, как часто слова и

326
00:14:24,320 --> 00:14:27,600
фразы встречаются в параллельных предложениях, вы можете

327
00:14:27,600 --> 00:14:30,959
посмотреть на их позиции в предложении

328
00:14:30,959 --> 00:14:31,760
и

329
00:14:31,760 --> 00:14:34,000
выяснить, какие правильные

330
00:14:34,000 --> 00:14:36,639
выравнивания, но выравнивания - это

331
00:14:36,639 --> 00:14:39,440
категориальная вещь, они не

332
00:14:39,440 --> 00:14:42,480
вероятностные и поэтому  они являются скрытыми

333
00:14:42,480 --> 00:14:44,560
переменными, поэтому вам необходимо использовать специальные

334
00:14:44,560 --> 00:14:46,639
алгоритмы обучения, такие как

335
00:14:46,639 --> 00:14:49,120
алгоритм максимизации ожидания, для

336
00:14:49,120 --> 00:14:51,839
изучения скрытых переменных va.  riables в былые

337
00:14:51,839 --> 00:14:54,480
дни cs224n, прежде чем мы начали делать все это

338
00:14:54,480 --> 00:14:56,839
с помощью глубокого обучения, мы потратили тонны

339
00:14:56,839 --> 00:14:58,959
cs224 на

340
00:14:58,959 --> 00:15:02,240
работу с алгоритмами скрытых переменных,

341
00:15:02,240 --> 00:15:03,920
но в наши дни мы вообще не рассматриваем это,

342
00:15:03,920 --> 00:15:05,519
и вам придется пойти

343
00:15:05,519 --> 00:15:08,639
и посмотреть  cs228, если вы хотите узнать больше

344
00:15:08,639 --> 00:15:10,560
об этом, и вы знаете, что мы на самом деле не

345
00:15:10,560 --> 00:15:12,639
ожидаем, что вы поймете детали

346
00:15:12,639 --> 00:15:15,839
здесь, но тогда я хотел сказать немного

347
00:15:15,839 --> 00:15:20,959
больше о том, как было выполнено декодирование

348
00:15:20,959 --> 00:15:23,600
в системе статистического машинного перевода,

349
00:15:23,600 --> 00:15:27,360
так что мы  хотел сделать, это

350
00:15:27,360 --> 00:15:29,839
сказать, что у нас есть модель перевода и

351
00:15:29,839 --> 00:15:33,600
языковая модель, и мы хотим

352
00:15:33,600 --> 00:15:36,160
выбрать наиболее вероятную причину

353
00:15:36,160 --> 00:15:38,959
перевода предложения и

354
00:15:38,959 --> 00:15:42,000
какой процесс мы могли бы использовать для этого ...

355
00:15:42,000 --> 00:15:44,320
ну,

356
00:15:44,320 --> 00:15:46,880
вы знаете, что наивная вещь  чтобы сказать хорошо,

357
00:15:46,880 --> 00:15:50,399
давайте просто перечислим все возможные y

358
00:15:50,399 --> 00:15:52,720
и вычислим их вероятность, но мы

359
00:15:52,720 --> 00:15:55,360
не можем этого сделать, потому что есть

360
00:15:55,360 --> 00:15:56,880
количество

361
00:15:56,880 --> 00:15:59,680
предложений перевода на целевом

362
00:15:59,680 --> 00:16:01,440
языке, которое экспоненциально зависит от

363
00:16:01,440 --> 00:16:04,000
длины предложения, так что это тоже

364
00:16:04,000 --> 00:16:07,839
e  x дорого, поэтому нам нужно иметь какой-то способ

365
00:16:07,839 --> 00:16:10,240
разбить его больше, и хорошо, у нас был

366
00:16:10,240 --> 00:16:13,600
простой способ для языковых моделей, мы

367
00:16:13,600 --> 00:16:17,120
просто генерировали слова по одному и

368
00:16:17,120 --> 00:16:19,920
выкладывали предложение, и поэтому это

369
00:16:19,920 --> 00:16:22,399
кажется разумным делом, но здесь нам

370
00:16:22,399 --> 00:16:24,399
нужно  чтобы справиться с тем фактом,

371
00:16:24,399 --> 00:16:25,440
что

372
00:16:25,440 --> 00:16:28,959
вещи происходят в разном порядке на

373
00:16:28,959 --> 00:16:33,399
исходных языках и в переводах,

374
00:16:33,839 --> 00:16:35,680
и поэтому мы действительно хотим разбить это на

375
00:16:35,680 --> 00:16:37,759
части с предположением независимости,

376
00:16:37,759 --> 00:16:40,240
как языковая модель, но тогда нам

377
00:16:40,240 --> 00:16:42,720
нужен способ разбить вещи на части и

378
00:16:42,720 --> 00:16:45,279
исследовать их в том, что  называется процесс декодирования,

379
00:16:46,399 --> 00:16:49,279
так что именно так это и было сделано, поэтому мы

380
00:16:49,279 --> 00:16:51,519
начнем с исходного предложения, так что

381
00:16:51,519 --> 00:16:54,000
это немецкое предложение,

382
00:16:54,000 --> 00:16:56,000
и, как это стандартно

383
00:16:57,360 --> 00:16:59,759
для немецкого языка, вы получаете

384
00:16:59,759 --> 00:17:02,560
этот глагол второй позиции,

385
00:17:02,560 --> 00:17:04,720
поэтому он, вероятно, не в правильном

386
00:17:04,720 --> 00:17:06,640
положении  где будет английский

387
00:17:06,640 --> 00:17:08,720
перевод, поэтому нам может

388
00:17:08,720 --> 00:17:12,559
потребоваться переставить слова так, чтобы то, что у нас

389
00:17:12,559 --> 00:17:16,079
было, было основано на модели перевода,

390
00:17:16,079 --> 00:17:17,039
у нас есть

391
00:17:17,039 --> 00:17:20,559
слова или фразы, которые с достаточной

392
00:17:20,559 --> 00:17:24,799
вероятностью являются переводами каждого немецкого языка

393
00:17:24,799 --> 00:17:28,079
слово или иногда немецкая фраза, так

394
00:17:28,079 --> 00:17:30,960
что это фактически части лего,

395
00:17:30,960 --> 00:17:32,960
из которых мы

396
00:17:32,960 --> 00:17:35,840
собираемся создать перевод,

397
00:17:35,840 --> 00:17:39,200
а затем внутри того, что мы собираемся

398
00:17:39,200 --> 00:17:42,559
использовать эти данные, мы собираемся

399
00:17:42,559 --> 00:17:45,679
сгенерировать часть перевода  по частям,

400
00:17:45,679 --> 00:17:47,840
как мы это делали с нашими новыми

401
00:17:47,840 --> 00:17:50,559
языковыми моделями, поэтому мы собираемся начать

402
00:17:50,559 --> 00:17:53,360
с пустого перевода, а затем мы

403
00:17:53,360 --> 00:17:56,480
собираемся сказать, что мы хотим использовать одну из

404
00:17:56,480 --> 00:17:59,840
этих частей лего, и чтобы мы могли

405
00:17:59,840 --> 00:18:02,000
изучить различные возможные, так что

406
00:18:02,000 --> 00:18:04,000
есть процесс поиска, но одна из

407
00:18:04,000 --> 00:18:06,799
возможных частей - мы могли бы переводить er

408
00:18:06,799 --> 00:18:08,320
с помощью he,

409
00:18:08,320 --> 00:18:10,559
или мы могли бы начать предложение

410
00:18:10,559 --> 00:18:13,600
с r, переводя второе слово, чтобы мы

411
00:18:13,600 --> 00:18:14,960
могли исследовать

412
00:18:14,960 --> 00:18:17,760
различные вероятные возможности, и если

413
00:18:17,760 --> 00:18:20,240
мы будем руководствоваться нашей языковой моделью, это,

414
00:18:20,240 --> 00:18:22,559
вероятно, намного больше  скорее всего, начнем

415
00:18:22,559 --> 00:18:24,880
предложение с он, чем начнем

416
00:18:24,880 --> 00:18:26,640
предложение с r, хотя r не

417
00:18:26,640 --> 00:18:28,880
невозможно, хорошо, а затем

418
00:18:28,880 --> 00:18:30,640
мы делаем еще кое-что, что мы делаем с этими маленькими

419
00:18:30,640 --> 00:18:32,480
черными пятнами вверху, которые мы как бы

420
00:18:32,480 --> 00:18:34,799
записываем, которые  немецкие слова, которые мы

421
00:18:34,799 --> 00:18:38,000
перевели, и поэтому мы продвигаемся вперед в

422
00:18:39,120 --> 00:18:42,960
процессе перевода,

423
00:18:42,960 --> 00:18:45,200
и мы могли бы решить, что мы можем

424
00:18:45,200 --> 00:18:48,799
перевести следующим, когда идет второе слово,

425
00:18:48,799 --> 00:18:51,919
или мы можем перевести отрицание здесь

426
00:18:51,919 --> 00:18:54,480
и перевести его, что не происходит,

427
00:18:54,480 --> 00:18:57,679
когда мы исследуем различные продолжения

428
00:18:57,679 --> 00:18:59,280
и в процессе  Я расскажу

429
00:18:59,280 --> 00:19:01,360
более подробно позже, когда мы сделаем нейронный

430
00:19:01,360 --> 00:19:04,160
эквивалент, мы как бы делаем этот поиск,

431
00:19:04,160 --> 00:19:07,679
где мы исследуем вероятные переводы и

432
00:19:07,679 --> 00:19:10,240
сокращаем, и в конечном итоге мы

433
00:19:10,240 --> 00:19:12,640
перевели все входное предложение и

434
00:19:12,640 --> 00:19:15,039
разработали довольно вероятный перевод, который

435
00:19:15,039 --> 00:19:17,200
он не делает  иди домой, и это то, что

436
00:19:17,200 --> 00:19:20,720
мы будем использовать в качестве перевода,

437
00:19:20,960 --> 00:19:22,559
хорошо,

438
00:19:22,559 --> 00:19:26,960
так что в период с

439
00:19:27,240 --> 00:19:32,880
1997 по 2013

440
00:19:32,880 --> 00:19:35,840
год статистический машинный перевод был

441
00:19:35,840 --> 00:19:38,480
огромной областью исследований,

442
00:19:38,480 --> 00:19:41,679
лучшие системы были чрезвычайно сложными,

443
00:19:41,679 --> 00:19:44,080
и в них были сотни деталей, которых я

444
00:19:44,080 --> 00:19:46,240
определенно не знал ''  Здесь упоминалось, что

445
00:19:46,240 --> 00:19:48,880
системы имеют множество отдельно разработанных

446
00:19:48,880 --> 00:19:51,919
и построенных компонентов, поэтому я упомянул

447
00:19:51,919 --> 00:19:54,240
языковую модель и модель перевода,

448
00:19:54,240 --> 00:19:56,400
но у них было много других  Компоненты

449
00:19:56,400 --> 00:19:58,799
для переупорядочения моделей и моделей перегиба

450
00:19:58,799 --> 00:20:01,360
и прочего там было

451
00:20:01,360 --> 00:20:03,520
много инженерии функций,

452
00:20:03,520 --> 00:20:06,559
как правило, модели также использовали

453
00:20:06,559 --> 00:20:09,840
много дополнительных ресурсов, и

454
00:20:09,840 --> 00:20:13,440
было много человеческих усилий для поддержки,

455
00:20:13,440 --> 00:20:15,200
но, тем не менее, они уже были

456
00:20:15,200 --> 00:20:18,720
довольно успешными, так что google translate

457
00:20:18,720 --> 00:20:22,159
был запущен в середине 2000-х, и люди

458
00:20:22,159 --> 00:20:24,080
думали, что это потрясающе, вы можете

459
00:20:24,080 --> 00:20:26,960
начать получать полу-приличные

460
00:20:26,960 --> 00:20:29,840
автоматические переводы

461
00:20:29,840 --> 00:20:32,080
для разных веб-страниц,

462
00:20:32,080 --> 00:20:33,360
но

463
00:20:33,360 --> 00:20:36,480
это продвигалось достаточно хорошо, а

464
00:20:36,480 --> 00:20:39,520
затем мы добрались до 2014 года,

465
00:20:39,520 --> 00:20:43,120
и действительно с огромной неожиданностью.

466
00:20:43,120 --> 00:20:47,120
Затем люди разработали способы выполнения

467
00:20:47,120 --> 00:20:50,320
машинного перевода с использованием большой нейронной

468
00:20:50,320 --> 00:20:53,919
сети, и эти большие нейронные сети

469
00:20:53,919 --> 00:20:56,799
оказались просто чрезвычайно успешными

470
00:20:56,799 --> 00:20:59,360
и в значительной степени уничтожили все, что им

471
00:20:59,360 --> 00:21:02,000
предшествовало, поэтому в следующей большой

472
00:21:02,000 --> 00:21:04,960
части лекции я бы хотел сделать

473
00:21:04,960 --> 00:21:07,200
это говорит вам что-то о новом

474
00:21:07,200 --> 00:21:09,600
машинном переводе,

475
00:21:09,600 --> 00:21:12,159
нейронный машинный перевод,

476
00:21:12,159 --> 00:21:14,240
это означает, что вы используете новую сеть

477
00:21:14,240 --> 00:21:16,559
для выполнения машинного перевода.  ne перевод, но на

478
00:21:16,559 --> 00:21:19,039
практике это означает немного больше, чем

479
00:21:19,039 --> 00:21:22,400
то, что мы собираемся построить

480
00:21:23,360 --> 00:21:26,799
одну очень большую нейронную сеть, которая

481
00:21:26,799 --> 00:21:29,919
полностью выполняет непрерывный перевод,

482
00:21:29,919 --> 00:21:31,520
поэтому у нас будет большая нейронная

483
00:21:31,520 --> 00:21:33,440
сеть, которую мы собираемся кормить  в

484
00:21:33,440 --> 00:21:35,919
исходном предложении во входные, и

485
00:21:35,919 --> 00:21:37,679
то, что выйдет на

486
00:21:37,679 --> 00:21:39,760
выходе нейронной сети,

487
00:21:39,760 --> 00:21:42,720
- это перевод предложения, которое мы

488
00:21:42,720 --> 00:21:45,360
собираемся обучить эту модель от начала до конца на

489
00:21:45,360 --> 00:21:49,039
параллельных предложениях, и это вся

490
00:21:49,039 --> 00:21:51,600
система, а не  много

491
00:21:51,600 --> 00:21:53,360
отдельных компонентов, как в

492
00:21:53,360 --> 00:21:56,159
старомодной системе машинного перевода,

493
00:21:56,159 --> 00:21:58,960
и мы увидим, что через некоторое время эти

494
00:21:58,960 --> 00:22:01,919
архитектуры нейронных сетей называются

495
00:22:01,919 --> 00:22:04,159
последовательными моделями или обычно

496
00:22:04,159 --> 00:22:07,919
сокращенно ищут гм и

497
00:22:07,919 --> 00:22:08,720
гм,

498
00:22:08,720 --> 00:22:10,080
они включают в себя

499
00:22:10,080 --> 00:22:12,240
две нейронные сети.  говорит, что два

500
00:22:12,240 --> 00:22:14,880
rnn версия, которую я представляю, теперь

501
00:22:14,880 --> 00:22:17,039
имеет два rnn, но в более общем плане они

502
00:22:17,039 --> 00:22:19,679
включают две нейронные сети, есть

503
00:22:19,679 --> 00:22:22,480
одна нейронная сеть, которая будет

504
00:22:22,480 --> 00:22:25,039
кодировать исходное предложение, поэтому, если у нас

505
00:22:25,039 --> 00:22:27,440
есть су  Здесь

506
00:22:27,440 --> 00:22:30,320
мы собираемся закодировать это предложение, и

507
00:22:30,320 --> 00:22:32,240
мы хорошо знаем способ, которым мы можем это сделать

508
00:22:32,240 --> 00:22:33,280
,

509
00:22:33,280 --> 00:22:36,559
поэтому, используя тип lstms, который мы видели в

510
00:22:36,559 --> 00:22:39,760
прошлом классе, мы можем начать с самого начала

511
00:22:39,760 --> 00:22:40,480
и

512
00:22:40,480 --> 00:22:42,960
пройти предложение и обновить

513
00:22:42,960 --> 00:22:45,600
скрытое состояние  каждый раз, и это

514
00:22:45,600 --> 00:22:49,120
даст нам представление о

515
00:22:49,120 --> 00:22:52,000
содержании исходного предложения, так

516
00:22:52,000 --> 00:22:54,799
что это первая модель последовательности

517
00:22:54,799 --> 00:22:58,400
um, которая кодирует исходное предложение, и

518
00:22:58,400 --> 00:23:01,520
мы будем использовать идею о том, что окончательное скрытое

519
00:23:01,520 --> 00:23:04,880
состояние кодировщика

520
00:23:04,880 --> 00:23:07,360
rnn будет

521
00:23:07,360 --> 00:23:10,240
в смысле представлять  эм исходное

522
00:23:10,240 --> 00:23:12,480
предложение, и мы собираемся передать его

523
00:23:12,480 --> 00:23:15,200
косвенно в качестве начального скрытого состояния

524
00:23:15,200 --> 00:23:18,240
для декодера rnn, поэтому на другой

525
00:23:18,240 --> 00:23:20,159
стороне изображения у нас есть наш декодер

526
00:23:20,159 --> 00:23:21,679
rnn,

527
00:23:21,679 --> 00:23:23,760
и это языковая модель, которая

528
00:23:23,760 --> 00:23:26,080
будет генерировать целевое предложение,

529
00:23:26,080 --> 00:23:28,720
обусловленное

530
00:23:30,400 --> 00:23:34,240
окончательное скрытое состояние кодировщика rnn, поэтому

531
00:23:34,240 --> 00:23:36,400
мы собираемся начать с ввода

532
00:23:36,400 --> 00:23:38,799
начального символа, который мы собираемся передать в

533
00:23:38,799 --> 00:23:42,080
скрытом состоянии из кодировщика rnn, и

534
00:23:42,080 --> 00:23:45,039
теперь этот второй зеленый rnn

535
00:23:45,039 --> 00:23:47,120
имеет полностью отдельный па  параметры, которые я

536
00:23:47,120 --> 00:23:49,919
мог бы просто подчеркнуть, но мы выполняем те же

537
00:23:49,919 --> 00:23:54,000
вычисления lstm и генерируем

538
00:23:54,000 --> 00:23:56,320
первое слово предложения, которое он,

539
00:23:56,320 --> 00:24:00,480
а затем, выполняя генерацию lstm,

540
00:24:00,480 --> 00:24:03,200
как и в предыдущем классе, мы копируем это в качестве

541
00:24:03,200 --> 00:24:04,720
следующего ввода, который

542
00:24:04,720 --> 00:24:07,360
мы запускаем на следующем шаге lstm

543
00:24:07,360 --> 00:24:10,240
generate  другое слово нажмите, скопируйте его

544
00:24:10,240 --> 00:24:12,960
и продолжайте, и

545
00:24:12,960 --> 00:24:14,720
мы

546
00:24:14,720 --> 00:24:17,520
перевели предложение правильно, так что это

547
00:24:17,520 --> 00:24:21,919
показывает поведение во время тестирования,

548
00:24:21,919 --> 00:24:22,720
когда

549
00:24:22,720 --> 00:24:25,279
мы генерируем следующее предложение для

550
00:24:25,279 --> 00:24:28,159
поведения во время обучения, когда у нас есть

551
00:24:28,159 --> 00:24:31,520
параллельные предложения, которые мы все еще используем  тот

552
00:24:31,520 --> 00:24:34,720
же тип последовательности для модели последовательности,

553
00:24:34,720 --> 00:24:37,440
но мы делаем это с помощью части декодера,

554
00:24:37,440 --> 00:24:40,799
точно так же, как um обучение языковой модели,

555
00:24:40,799 --> 00:24:42,640
где мы хотим

556
00:24:42,640 --> 00:24:45,120
заставить учителя и предсказывать каждое слово,

557
00:24:45,120 --> 00:24:47,200
которое действительно встречается в предложении исходного

558
00:24:47,200 --> 00:24:50,080
языка

559
00:24:50,080 --> 00:24:53,360
um последовательностей моделей последовательностей  были

560
00:24:53,360 --> 00:24:54,880
невероятно

561
00:24:54,880 --> 00:24:56,080
мощной и

562
00:24:56,080 --> 00:25:00,240
широко используемой рабочей лошадкой в новых нейронных

563
00:25:00,240 --> 00:25:03,440
сетях для nlp, поэтому, хотя

564
00:25:03,440 --> 00:25:05,520
вы знаете, что исторически

565
00:25:05,520 --> 00:25:08,400
машинный перевод был их первым большим

566
00:25:08,400 --> 00:25:10,400
применением и в

567
00:25:10,400 --> 00:25:12,880
некотором роде  каноническое использование, они также используются

568
00:25:12,880 --> 00:25:15,279
повсюду,

569
00:25:15,279 --> 00:25:16,000
поэтому

570
00:25:16,000 --> 00:25:18,880
вы можете выполнять за них множество других задач nlp,

571
00:25:18,880 --> 00:25:21,039
чтобы вы могли делать резюмирование, вы можете

572
00:25:21,039 --> 00:25:23,360
думать о резюмировании текста как о

573
00:25:23,360 --> 00:25:26,400
переводе длинного текста в короткий

574
00:25:26,400 --> 00:25:27,840
текст,

575
00:25:27,840 --> 00:25:29,760
но вы можете использовать их для других вещей,

576
00:25:29,760 --> 00:25:31,840
которые  ни в коем случае не перевод,

577
00:25:31,840 --> 00:25:35,200
поэтому они обычно используются

578
00:25:35,200 --> 00:25:38,799
для нейронных диалоговых систем, поэтому

579
00:25:38,799 --> 00:25:40,480
кодировщик

580
00:25:40,480 --> 00:25:43,520
будет кодировать um, как говорят предыдущие два

581
00:25:43,520 --> 00:25:46,400
высказывания, а затем вы будете использовать

582
00:25:46,400 --> 00:25:49,679
декодер для генерации трендов шеи звезды

583
00:25:49,679 --> 00:25:50,480
um

584
00:25:50,480 --> 00:25:52,240
некоторые другие применения

585
00:25:52,240 --> 00:25:54,400
еще более причудливы,

586
00:25:54,400 --> 00:25:57,360
но доказали  чтобы быть довольно успешным,

587
00:25:58,159 --> 00:25:58,960
так что

588
00:25:58,960 --> 00:26:02,960
если у вас есть способ представить

589
00:26:02,960 --> 00:26:05,840
пути предложения в виде строки,

590
00:26:05,840 --> 00:26:08,159
и если вы немного подумаете,

591
00:26:08,159 --> 00:26:11,200
довольно очевидно, как вы можете превратить

592
00:26:11,200 --> 00:26:13,600
пути предложения в строку,

593
00:26:13,600 --> 00:26:16,559
просто используя дополнительные  синтаксис, такой как

594
00:26:16,559 --> 00:26:20,480
круглые скобки, или вставка явных слов,

595
00:26:20,480 --> 00:26:22,159
которые говорят, что

596
00:26:22,159 --> 00:26:25,919
левая дуга правая дуга um

597
00:26:25,919 --> 00:26:28,080
смещается, как системы переходов, которые

598
00:26:28,080 --> 00:26:30,640
вы использовали для назначения три, тогда

599
00:26:30,640 --> 00:26:32,320
мы могли бы сказать,

600
00:26:32,320 --> 00:26:34,960
давайте использовать кодировщик

601
00:26:34,960 --> 00:26:37,600
f  передать входное предложение кодеру

602
00:26:37,600 --> 00:26:40,080
и позволить ему выводить последовательность переходов

603
00:26:40,080 --> 00:26:43,039
нашего анализатора зависимостей, и

604
00:26:43,039 --> 00:26:44,720
несколько удивительно, что на самом деле он

605
00:26:44,720 --> 00:26:47,840
работает хорошо как еще один способ создания

606
00:26:47,840 --> 00:26:50,159
синтаксического анализатора зависимостей или других видов

607
00:26:50,159 --> 00:26:52,559
синтаксического анализатора,

608
00:26:52,559 --> 00:26:54,400
эти модели также были применены не

609
00:26:54,400 --> 00:26:57,279
только к естественным языкам, но и  на

610
00:26:57,279 --> 00:26:59,360
другие языки,

611
00:26:59,360 --> 00:27:02,000
включая музыку, а также

612
00:27:02,000 --> 00:27:05,120
код языка программирования, чтобы вы могли

613
00:27:05,120 --> 00:27:08,559
обучить систему поиска искать, где она

614
00:27:08,559 --> 00:27:09,840
читается в

615
00:27:09,840 --> 00:27:12,720
псевдокоде на естественном языке и

616
00:27:12,720 --> 00:27:15,200
генерирует код Python, и если у

617
00:27:15,200 --> 00:27:16,720
вас достаточно хороший, он может выполнить

618
00:27:16,720 --> 00:27:19,520
задание за вас

619
00:27:21,520 --> 00:27:24,799
Итак, существенная новая идея здесь с нашей

620
00:27:24,799 --> 00:27:27,200
последовательностью моделей последовательности состоит в том, что у нас

621
00:27:27,200 --> 00:27:29,679
есть пример условных языковых

622
00:27:29,679 --> 00:27:31,760
моделей, поэтому

623
00:27:31,760 --> 00:27:34,080
раньше

624
00:27:34,080 --> 00:27:35,919
мы просто начинали с

625
00:27:35,919 --> 00:27:37,919
начала предложения и

626
00:27:37,919 --> 00:27:40,159
генерировали предложение,

627
00:27:40,159 --> 00:27:43,600
основанное ни на чем, но здесь мы  иметь

628
00:27:43,600 --> 00:27:46,559
что-то, что будет определять или

629
00:27:46,559 --> 00:27:48,960
частично определять, что будет

630
00:27:48,960 --> 00:27:51,279
обусловливать то, что мы должны производить, поэтому у

631
00:27:51,279 --> 00:27:53,520
нас есть источник  предложение, и это

632
00:27:53,520 --> 00:27:56,480
будет строго определять, что является хорошим

633
00:27:56,480 --> 00:27:57,840
переводом,

634
00:27:57,840 --> 00:28:00,799
и поэтому для достижения этого

635
00:28:00,799 --> 00:28:04,000
мы собираемся иметь какой-то способ

636
00:28:05,120 --> 00:28:08,240
передачи информации об

637
00:28:08,240 --> 00:28:11,679
исходном предложении от кодировщика, чтобы

638
00:28:11,679 --> 00:28:15,039
запустить то, что должен делать декодер,

639
00:28:15,039 --> 00:28:17,279
и два стандартных способа  для этого

640
00:28:17,279 --> 00:28:20,640
вы либо подаете в скрытое состояние в

641
00:28:20,640 --> 00:28:23,360
качестве начального скрытого состояния в декодер,

642
00:28:23,360 --> 00:28:26,399
либо иногда вы подаете что-то в

643
00:28:26,399 --> 00:28:29,200
качестве начального ввода в декодер,

644
00:28:29,200 --> 00:28:31,200
и поэтому

645
00:28:31,200 --> 00:28:34,480
в вашем основном переводе мы

646
00:28:34,480 --> 00:28:37,360
напрямую вычисляем эту условную

647
00:28:37,360 --> 00:28:40,080
вероятность модели.  предложения на целевом языке,

648
00:28:40,080 --> 00:28:43,440
заданного предложением исходного языка,

649
00:28:43,440 --> 00:28:46,320
и поэтому на каждом этапе, когда мы разбиваем

650
00:28:46,320 --> 00:28:49,600
генерацию слова за словом, мы

651
00:28:49,600 --> 00:28:52,559
обусловливаем не только предыдущие

652
00:28:52,559 --> 00:28:55,840
слова целевого языка, но и каждый

653
00:28:55,840 --> 00:28:59,520
раз предложение x исходного языка,

654
00:28:59,520 --> 00:29:02,080
поэтому мы  на самом деле знать тонну

655
00:29:02,080 --> 00:29:04,480
больше о том, каким должно быть наше предложение, которое мы

656
00:29:04,480 --> 00:29:06,000
генерируем,

657
00:29:06,000 --> 00:29:08,080
поэтому, если вы посмотрите

658
00:29:08,080 --> 00:29:10,960
на сложности такого рода

659
00:29:10,960 --> 00:29:13,360
условного языка  модели, которые вы

660
00:29:13,360 --> 00:29:15,600
найдете, и вам нравятся числа, которые я показал в прошлый

661
00:29:15,600 --> 00:29:18,880
раз, они обычно почти часто имеют

662
00:29:18,880 --> 00:29:21,360
низкие затруднения, что у вас будут

663
00:29:21,360 --> 00:29:23,600
модели с затруднениями, которые

664
00:29:23,600 --> 00:29:25,919
примерно четыре или даже меньше,

665
00:29:25,919 --> 00:29:28,720
иногда вы знаете 2,5, потому что вы получаете

666
00:29:28,720 --> 00:29:31,039
много информации о том, какие слова вам

667
00:29:31,039 --> 00:29:33,440
следует  генерировать

668
00:29:33,440 --> 00:29:34,880
нормально,

669
00:29:34,880 --> 00:29:37,279
поэтому у нас есть те же вопросы,

670
00:29:37,279 --> 00:29:39,679
что и для языковых моделей в целом, как

671
00:29:39,679 --> 00:29:42,320
обучить систему нейронного машинного перевода,

672
00:29:42,320 --> 00:29:45,679
а затем как использовать ее во время выполнения,

673
00:29:45,679 --> 00:29:48,240
поэтому давайте рассмотрим оба из них

674
00:29:48,240 --> 00:29:50,880
более подробно,

675
00:29:50,880 --> 00:29:54,399
поэтому первый шаг  у нас есть большой

676
00:29:54,399 --> 00:29:57,679
параллельный корпус, поэтому мы бежим, например, в

677
00:29:57,679 --> 00:29:59,360
Европейский союз,

678
00:29:59,360 --> 00:30:02,240
и мы берем много

679
00:30:02,240 --> 00:30:05,120
параллельных данных на английском и французском языках из

680
00:30:05,120 --> 00:30:07,679
заседаний Европейского парламента,

681
00:30:07,679 --> 00:30:09,679
поэтому, когда у нас будут наши параллельные

682
00:30:09,679 --> 00:30:13,679
предложения, что мы собираемся сделать, это

683
00:30:13,679 --> 00:30:15,120
взять

684
00:30:15,120 --> 00:30:19,120
um пакеты исходных предложений и

685
00:30:19,120 --> 00:30:20,960
целевых предложений

686
00:30:20,960 --> 00:30:24,080
будут кодировать исходное предложение с помощью нашего

687
00:30:24,080 --> 00:30:26,399
кодировщика lstm

688
00:30:26,399 --> 00:30:27,200
передаст

689
00:30:27,200 --> 00:30:29,440
его окончательное

690
00:30:29,440 --> 00:30:31,840
скрытое состояние

691
00:30:31,840 --> 00:30:33,440
в целевой

692
00:30:33,440 --> 00:30:37,679
lstm, а на этот раз мы  а теперь

693
00:30:37,679 --> 00:30:42,159
потренируемся слово за словом, сравнивая то, что

694
00:30:42,159 --> 00:30:45,520
он предсказывает, наиболее вероятное

695
00:30:45,520 --> 00:30:47,760
слово, которое будет произведено, с тем, что является

696
00:30:47,760 --> 00:30:50,240
фактическим первым словом, а затем фактическим

697
00:30:50,240 --> 00:30:52,799
вторым словом, и в той степени, в которой

698
00:30:52,799 --> 00:30:54,559
мы ошибаемся,

699
00:30:54,559 --> 00:30:57,600
мы будем страдать от некоторых  потеря, так что

700
00:30:57,600 --> 00:30:58,960
это будет отрицательная логарифмическая

701
00:30:58,960 --> 00:31:01,519
вероятность

702
00:31:01,519 --> 00:31:04,480
генерации правильного следующего слова он

703
00:31:04,480 --> 00:31:06,960
и так далее по предложению, и

704
00:31:06,960 --> 00:31:09,600
так же, как мы видели в прошлый раз для

705
00:31:09,600 --> 00:31:12,080
языковых моделей, мы можем вычислить наши

706
00:31:12,080 --> 00:31:13,679
общие

707
00:31:13,679 --> 00:31:16,240
потери для предложения, выполняя это

708
00:31:16,240 --> 00:31:18,880
стиль принуждения учителя генерирует одно слово за

709
00:31:18,880 --> 00:31:21,519
раз, вычисляет потерю относительно того

710
00:31:21,519 --> 00:31:25,919
слова, которое вы должны были произнести,

711
00:31:26,000 --> 00:31:28,640
и таким образом эта потеря дает нам

712
00:31:28,640 --> 00:31:31,840
информацию, которую мы можем распространить обратно

713
00:31:31,840 --> 00:31:34,399
по всей сети, и самое

714
00:31:34,399 --> 00:31:36,080
важное

715
00:31:36,080 --> 00:31:38,559
в этой последовательности - модели последовательности,

716
00:31:38,559 --> 00:31:40,960
которые имеют  сделали их чрезвычайно

717
00:31:40,960 --> 00:31:44,159
успешными на практике, так как

718
00:31:44,159 --> 00:31:47,200
все это оптимизировано как единая

719
00:31:47,200 --> 00:31:50,799
система от начала до конца, поэтому, начиная с наших

720
00:31:50,799 --> 00:31:52,640
окончательных потерь,

721
00:31:52,640 --> 00:31:55,440
мы распространяем их обратно прямо через

722
00:31:55,440 --> 00:31:58,720
систему.  o мы не только обновляем все

723
00:31:58,720 --> 00:32:02,240
параметры модели декодера, но мы

724
00:32:02,240 --> 00:32:04,720
также обновляем все

725
00:32:04,720 --> 00:32:07,519
параметры модели кодера, которые, в

726
00:32:07,519 --> 00:32:10,559
свою очередь, будут влиять на то, какие условия

727
00:32:10,559 --> 00:32:13,519
будут переданы от кодера к

728
00:32:13,519 --> 00:32:16,000
декодеру,

729
00:32:17,039 --> 00:32:19,440
так что этот момент - хороший момент для меня

730
00:32:19,440 --> 00:32:22,720
чтобы вернуться к трем слайдам, которые

731
00:32:22,720 --> 00:32:25,120
я пропустил, у меня заканчивается время

732
00:32:25,120 --> 00:32:28,480
в конце прошлого раза, а именно, чтобы упомянуть

733
00:32:28,480 --> 00:32:33,679
многослойные сообщения ммм, так что значения, которые

734
00:32:33,679 --> 00:32:36,799
мы рассмотрели до сих пор, уже глубоко

735
00:32:36,799 --> 00:32:40,000
в одном измерении  затем развернуться по

736
00:32:40,000 --> 00:32:43,039
горизонтали на многих временных шагах,

737
00:32:43,039 --> 00:32:45,519
но они были неглубокими в том смысле, что

738
00:32:48,159 --> 00:32:50,960
над нашими предложениями был только один слой повторяющейся структуры,

739
00:32:50,960 --> 00:32:52,880
мы также можем заставить их углубить другое

740
00:32:52,880 --> 00:32:56,960
измерение, применяя несколько

741
00:32:56,960 --> 00:32:59,279
rnn друг над другом, и это дает нам некоторые

742
00:32:59,279 --> 00:33:02,559
многослойный rnn uh часто также называется

743
00:33:02,559 --> 00:33:04,880
сложенным rnn,

744
00:33:04,880 --> 00:33:07,679
и наличие

745
00:33:07,679 --> 00:33:11,120
многослойного rnn позволяет нам

746
00:33:11,120 --> 00:33:15,120
вычислять более сложные представления, поэтому,

747
00:33:15,120 --> 00:33:16,640
проще говоря,

748
00:33:16,640 --> 00:33:19,039
более низкие rnns имеют тенденцию вычислять

749
00:33:19,039 --> 00:33:22,240
функции более низкого уровня, а более высокие  RNN

750
00:33:22,240 --> 00:33:24,880
должны вычислять функции более высокого уровня,

751
00:33:24,880 --> 00:33:27,760
и, как и в других нейронных сетях,

752
00:33:27,760 --> 00:33:30,399
будь то сети прямого распространения

753
00:33:30,399 --> 00:33:32,399
или те сети, которые вы видите в

754
00:33:32,399 --> 00:33:34,559
системах машинного зрения, вы получите

755
00:33:34,559 --> 00:33:38,320
гораздо большую мощность и успех,

756
00:33:38,320 --> 00:33:40,159
имея стек из

757
00:33:40,159 --> 00:33:42,480
нескольких слоев рекуррентных нейронных

758
00:33:42,480 --> 00:33:45,440
сетей.  мог бы подумать,

759
00:33:45,440 --> 00:33:47,679
что есть две вещи, которые я мог бы сделать, я

760
00:33:47,679 --> 00:33:50,159
мог бы иметь один lstm со скрытым

761
00:33:50,159 --> 00:33:53,600
состоянием размерности 2000 или у меня могло бы быть

762
00:33:53,600 --> 00:33:57,279
четыре слоя lstms со скрытым состоянием

763
00:33:58,159 --> 00:34:00,799
500 каждый, и это не должно иметь никакого

764
00:34:00,799 --> 00:34:02,480
значения, потому что я '  У меня

765
00:34:02,480 --> 00:34:04,880
примерно одинаковое количество параметров, но

766
00:34:04,880 --> 00:34:07,600
на практике это не так, это имеет большое

767
00:34:07,600 --> 00:34:10,320
значение, и многослойные или

768
00:34:10,320 --> 00:34:13,199
составные РНН более эффективны,

769
00:34:13,199 --> 00:34:15,040
так

770
00:34:15,040 --> 00:34:17,520
что я могу спросить вас, эм,

771
00:34:17,520 --> 00:34:18,800
есть хороший вопрос студента

772
00:34:18,800 --> 00:34:20,399
о том, какой более низкий уровень или более высокий

773
00:34:20,399 --> 00:34:23,760
уровень  функции означают в этом контексте, конечно,

774
00:34:23,760 --> 00:34:27,199
да, поэтому я имею в виду, что в каком-то смысле

775
00:34:27,199 --> 00:34:31,599
это несколько хлипкие способы,

776
00:34:32,879 --> 00:34:35,119
вы знаете,

777
00:34:35,119 --> 00:34:38,800
термины, это значение не совсем точное, но

778
00:34:38,800 --> 00:34:41,760
обычно то, что  Это означает,

779
00:34:41,760 --> 00:34:44,639
что особенности нижнего уровня и знание

780
00:34:44,639 --> 00:34:48,239
более простых вещей о словах

781
00:34:48,239 --> 00:34:49,040
и

782
00:34:49,040 --> 00:34:52,079
фразах, так что обычно могут быть вещи,

783
00:34:52,079 --> 00:34:56,159
например, какая часть речи является этим словом,

784
00:34:56,159 --> 00:34:59,280
или эти слова являются именем человека

785
00:34:59,280 --> 00:35:01,760
или названием компании,

786
00:35:01,760 --> 00:35:05,520
тогда как более высокий уровень  особенности относятся к

787
00:35:05,520 --> 00:35:08,079
вещам, которые находятся на более высоком семантическом

788
00:35:08,079 --> 00:35:10,960
уровне, поэтому знать больше об общей

789
00:35:10,960 --> 00:35:13,200
структуре предложения, знать

790
00:35:13,200 --> 00:35:15,920
кое-что о том, что это означает,

791
00:35:15,920 --> 00:35:18,359
имеет ли фраза положительные или отрицательные

792
00:35:18,359 --> 00:35:21,760
коннотации; какова

793
00:35:21,760 --> 00:35:24,079
ее семантика, когда вы объединяете

794
00:35:24,079 --> 00:35:26,640
несколько слов в идиоматику  фраза:

795
00:35:26,640 --> 00:35:28,800
ну, грубо говоря, вещи более высокого уровня,

796
00:35:35,359 --> 00:35:37,760
ладно,

797
00:35:38,240 --> 00:35:40,960
забегаем вперед,

798
00:35:41,440 --> 00:35:42,720
ладно, поэтому,

799
00:35:42,720 --> 00:35:44,480
когда мы создаем

800
00:35:45,280 --> 00:35:48,800
одну из этих сквозных

801
00:35:48,800 --> 00:35:51,920
систем нейронного машинного перевода, если мы

802
00:35:51,920 --> 00:35:55,520
хотим, чтобы они работали хорошо,

803
00:35:58,400 --> 00:36:00,160
декодирование однослойного кодировщика lstm в ваших

804
00:36:00,160 --> 00:36:04,000
системах машинного перевода просто не  t работают хорошо,

805
00:36:04,000 --> 00:36:07,200
но вы можете построить что-то не

806
00:36:07,200 --> 00:36:09,200
более сложное, чем модель, которую я

807
00:36:09,200 --> 00:36:12,240
только что объяснил, которая действительно работает очень

808
00:36:12,240 --> 00:36:16,320
хорошо, сделав ее мю  lti-слойная

809
00:36:16,320 --> 00:36:19,760
система нейронного машинного перевода lstm,

810
00:36:21,920 --> 00:36:23,920
поэтому изображение выглядит так, поэтому у нас есть

811
00:36:23,920 --> 00:36:27,280
этот многослойный lstm, который проходит

812
00:36:27,280 --> 00:36:30,320
через исходное предложение, и теперь

813
00:36:30,320 --> 00:36:33,760
в каждый момент времени мы вычисляем новое

814
00:36:33,760 --> 00:36:36,960
скрытое представление, которое вместо того, чтобы

815
00:36:36,960 --> 00:36:39,119
останавливать  там мы как бы подаем его как

816
00:36:39,119 --> 00:36:41,280
вход на другой уровень

817
00:36:41,280 --> 00:36:42,720
lstm,

818
00:36:42,720 --> 00:36:45,040
и мы вычисляем стандартным способом

819
00:36:45,040 --> 00:36:47,359
это новое скрытое представление, а его

820
00:36:47,359 --> 00:36:49,520
вывод мы подаем на третий

821
00:36:49,520 --> 00:36:53,119
уровень lstm, и поэтому мы запускаем его прямо вместе,

822
00:36:53,119 --> 00:36:54,599
и поэтому наше

823
00:36:54,599 --> 00:36:57,119
представление

824
00:36:57,119 --> 00:37:00,000
исходного предложения от нашего кодировщика -

825
00:37:00,000 --> 00:37:04,480
это стек из трех скрытых слоев,

826
00:37:04,480 --> 00:37:06,839
а затем мы

827
00:37:06,839 --> 00:37:09,680
используем um,

828
00:37:09,680 --> 00:37:11,119
чтобы затем

829
00:37:11,119 --> 00:37:15,280
вводить в качестве начального um в качестве начального

830
00:37:15,280 --> 00:37:17,599
скрытого слоя, чтобы затем генерировать

831
00:37:19,079 --> 00:37:21,280
переводы или

832
00:37:21,280 --> 00:37:23,520
для обучения модели сравнения с

833
00:37:23,520 --> 00:37:25,520
потерями, поэтому  Это своего рода

834
00:37:25,520 --> 00:37:27,599
изображение

835
00:37:27,599 --> 00:37:30,240
декодера кодировщика lstm, на которое

836
00:37:30,240 --> 00:37:34,079
действительно похожа ваша система машинного перевода,

837
00:37:34,160 --> 00:37:37,359
поэтому, в частности,

838
00:37:37,359 --> 00:37:40,079
вы знаете, чтобы дать вам некоторое представление об этом,

839
00:37:40,079 --> 00:37:44,240
так что 2017  paper um от denny brits и

840
00:37:44,240 --> 00:37:47,200
других, что они обнаружили, что

841
00:37:47,200 --> 00:37:49,520
для кодировщика rnn

842
00:37:49,520 --> 00:37:52,560
он работал лучше всего, если бы он имел от двух до четырех

843
00:37:52,560 --> 00:37:55,280
слоев, а

844
00:37:55,280 --> 00:37:59,040
четыре слоя лучше всего подходили для декодера rnn,

845
00:37:59,040 --> 00:38:01,359
и детали здесь, как и для многих

846
00:38:01,359 --> 00:38:03,680
нейронных сетей, зависят так сильно  от того,

847
00:38:03,680 --> 00:38:06,160
что вы делаете и сколько данных у вас есть,

848
00:38:06,160 --> 00:38:08,880
и тому подобное, но вы знаете, что из практических

849
00:38:08,880 --> 00:38:11,520
правил, которые нужно иметь в голове,

850
00:38:11,520 --> 00:38:14,480
почти всегда бывает, что

851
00:38:14,480 --> 00:38:15,920
двухслойный

852
00:38:15,920 --> 00:38:18,960
lstm работает намного лучше, чем

853
00:38:18,960 --> 00:38:21,280
однослойный lstm

854
00:38:21,280 --> 00:38:24,240
после  что все становится намного менее ясным,

855
00:38:24,240 --> 00:38:25,280
вы знаете,

856
00:38:25,280 --> 00:38:27,359
что это не так уж и редко, что если вы попробуете

857
00:38:27,359 --> 00:38:29,760
три слоя, это будет на долю лучше, чем

858
00:38:29,760 --> 00:38:32,079
два, но не совсем, а если вы попробуете четыре

859
00:38:32,079 --> 00:38:34,160
слоя, на самом деле это снова станет хуже,

860
00:38:34,160 --> 00:38:36,800
вы знаете, это зависит от того, сколько данных и т. д. у

861
00:38:36,800 --> 00:38:40,720
вас есть на  в любом случае обычно

862
00:38:40,720 --> 00:38:42,960
очень сложно с той архитектурой модели,

863
00:38:42,960 --> 00:38:44,720
которую я только что показал

864
00:38:44,720 --> 00:38:48,079
здесь, получить лучшие результаты с более

865
00:38:48,079 --> 00:38:51,040
чем четырьмя уровнями lstm,

866
00:38:51,040 --> 00:38:52,880
как правило, для

867
00:38:52,880 --> 00:38:54,560
более

868
00:38:54,560 --> 00:38:57,040
глубоких моделей lstm и получения

869
00:38:57,040 --> 00:38:59,040
еще лучших результатов, которые вы должны

870
00:38:59,040 --> 00:39:01,440
добавление дополнительных пропускаемых соединений,

871
00:39:01,440 --> 00:39:04,400
о которых я говорил в самом

872
00:39:05,200 --> 00:39:07,920
конце последнего урока на

873
00:39:07,920 --> 00:39:10,200
следующей неделе, Джон собирается говорить о

874
00:39:10,200 --> 00:39:12,480
сетях на основе трансформаторов,

875
00:39:12,480 --> 00:39:15,440
напротив, по довольно фундаментальным

876
00:39:15,440 --> 00:39:17,680
причинам они обычно

877
00:39:17,680 --> 00:39:18,640
намного

878
00:39:18,640 --> 00:39:20,960
глубже, но мы оставим их обсуждение

879
00:39:20,960 --> 00:39:22,079
пока мы не

880
00:39:22,079 --> 00:39:25,560
перейдем к следующему,

881
00:39:31,359 --> 00:39:34,800
так что именно так мы обучили модель,

882
00:39:34,800 --> 00:39:37,680
так что давайте немного подробнее

883
00:39:37,680 --> 00:39:41,280
рассмотрим возможности декодирования и

884
00:39:41,280 --> 00:39:44,240
исследуем более сложную форму декодирования,

885
00:39:44,240 --> 00:39:46,960
чем мы рассматривали, самый простой способ

886
00:39:46,960 --> 00:39:49,440
декодирования - это  тот, который мы представили

887
00:39:49,440 --> 00:39:54,400
до сих пор, так что у нас есть наш lstm, мы начинаем

888
00:39:54,400 --> 00:39:56,800
генерировать скрытое состояние, у

889
00:39:56,800 --> 00:39:59,599
него есть распределение вероятностей по

890
00:39:59,599 --> 00:40:02,560
словам, и вы выбираете наиболее вероятное из

891
00:40:02,560 --> 00:40:05,200
них arg max, и вы говорите, что он, и вы

892
00:40:05,200 --> 00:40:07,920
копируете его, и повторяете снова  поэтому

893
00:40:07,920 --> 00:40:10,480
выполнение этого называется жадным

894
00:40:10,480 --> 00:40:13,599
декодированием, использующим наиболее вероятное слово

895
00:40:13,599 --> 00:40:16,720
на каждом этапе, и это своего рода

896
00:40:16,720 --> 00:40:18,560
очевидная вещь, которую нужно сделать,

897
00:40:18,560 --> 00:40:20,480
и не похоже, что это может быть плохо

898
00:40:20,480 --> 00:40:21,920
,

899
00:40:21,920 --> 00:40:22,720
но

900
00:40:22,720 --> 00:40:24,880
оказывается, что на самом деле это c  быть

901
00:40:24,880 --> 00:40:27,280
довольно проблематичным,

902
00:40:27,280 --> 00:40:30,800
и идея заключается в том, что вы знаете, что

903
00:40:30,800 --> 00:40:33,280
с жадным декодированием вы как бы

904
00:40:33,280 --> 00:40:35,599
берете локально то, что кажется лучшим

905
00:40:35,599 --> 00:40:38,079
выбором, а затем вы застряли в нем, и у

906
00:40:38,079 --> 00:40:41,599
вас нет возможности отменить решения,

907
00:40:41,599 --> 00:40:42,319
так что

908
00:40:42,319 --> 00:40:44,640
если в этих примерах использовалось

909
00:40:44,640 --> 00:40:47,680
это предложение о том, что он ударил меня пирогом,

910
00:40:47,680 --> 00:40:49,760
переходя от перевода с французского на

911
00:40:49,760 --> 00:40:52,640
английский, так что вы знаете, если вы начнете

912
00:40:52,640 --> 00:40:55,440
и скажете хорошо, плохо, первое слово в

913
00:40:55,440 --> 00:40:57,520
переводе должно быть он,

914
00:40:57,520 --> 00:41:01,839
мм, который выглядит хорошо, но тогда  вы эм,

915
00:41:01,839 --> 00:41:04,160
а затем вы говорите хорошо, я сгенерирую

916
00:41:04,160 --> 00:41:05,359
попадание,

917
00:41:05,359 --> 00:41:07,920
тогда как-то модель думает, что

918
00:41:07,920 --> 00:41:10,720
наиболее вероятное следующее слово, которое будет найдено после удара,

919
00:41:10,720 --> 00:41:12,560
- это r, и есть много причин, по которым она

920
00:41:12,560 --> 00:41:15,920
может так думать, потому что после удара чаще

921
00:41:15,920 --> 00:41:19,040
всего есть прямой объект  время от

922
00:41:19,040 --> 00:41:21,359
времени вы знаете, что он врезался в машину,

923
00:41:21,359 --> 00:41:23,359
он

924
00:41:23,359 --> 00:41:26,079
врезался в контрольно-пропускной пункт, так что это звучит

925
00:41:26,079 --> 00:41:28,560
довольно вероятно, но вы знаете, что после того, как вы его

926
00:41:28,560 --> 00:41:31,200
сгенерировали, нет возможности вернуться

927
00:41:31,200 --> 00:41:33,680
назад, и поэтому вам просто нужно

928
00:41:33,680 --> 00:41:36,160
продолжать движение оттуда, и вы можете не  иметь

929
00:41:36,160 --> 00:41:38,079
возможность генерировать t  Вы хотите перевести,

930
00:41:40,240 --> 00:41:43,680
в лучшем случае, вы можете сгенерировать ммм, он попал

931
00:41:43,680 --> 00:41:44,960
в пирог,

932
00:41:44,960 --> 00:41:47,040
упс,

933
00:41:47,040 --> 00:41:49,760
что-то, поэтому мы хотели бы иметь возможность

934
00:41:49,760 --> 00:41:53,200
немного больше изучить при создании наших

935
00:41:53,200 --> 00:41:54,880
переводов,

936
00:41:54,880 --> 00:41:58,319
и вы знаете, что мы могли бы сделать

937
00:41:58,319 --> 00:42:00,640
хорошо, вы знаете, я вроде как упомянул  это

938
00:42:00,640 --> 00:42:02,800
прежде, чем рассматривать статистические модели mt в

939
00:42:04,160 --> 00:42:06,160
целом, что мы хотели бы сделать, так

940
00:42:06,160 --> 00:42:07,760
это найти

941
00:42:07,760 --> 00:42:09,920
переводы,

942
00:42:09,920 --> 00:42:12,640
которые

943
00:42:12,640 --> 00:42:15,359
максимизируют вероятность y при заданном x, и, по

944
00:42:15,359 --> 00:42:18,240
крайней мере, если мы знаем, какова длина этого

945
00:42:18,240 --> 00:42:20,079
перевода,

946
00:42:20,079 --> 00:42:21,760
мы можем сделать это как результат

947
00:42:21,760 --> 00:42:24,160
генерации  слово за раз, и поэтому, чтобы

948
00:42:24,160 --> 00:42:26,000
иметь полную модель, мы также должны иметь

949
00:42:26,000 --> 00:42:28,560
распределение вероятностей по

950
00:42:28,560 --> 00:42:31,760
длине перевода,

951
00:42:31,760 --> 00:42:34,560
чтобы мы могли сказать, что это модель, и

952
00:42:34,560 --> 00:42:36,079
дадим вам знать,

953
00:42:36,079 --> 00:42:38,160
сгенерировать и оценить

954
00:42:38,160 --> 00:42:41,599
все возможные последовательности y,

955
00:42:41,599 --> 00:42:44,319
используя эту модель и  вот где

956
00:42:44,319 --> 00:42:47,520
тогда требуется генерировать экспоненциальное

957
00:42:47,520 --> 00:42:50,880
количество переводов и это

958
00:42:50,880 --> 00:42:52,720
слишком дорого,

959
00:42:52,720 --> 00:42:56,240
поэтому, помимо жадного декодирования,

960
00:42:56,240 --> 00:42:58,960
самый важный метод, который используется,

961
00:42:58,960 --> 00:43:01,200
и вы увидите, что во многих местах есть

962
00:43:01,200 --> 00:43:03,760
что-то, что называется  декодирование поиска луча,

963
00:43:03,760 --> 00:43:06,560
и поэтому это не то, что

964
00:43:06,560 --> 00:43:08,640
нейронно хорошо, любой вид машинного

965
00:43:08,640 --> 00:43:10,560
перевода - это одно место, где он

966
00:43:10,560 --> 00:43:12,800
обычно используется, но это не

967
00:43:12,800 --> 00:43:14,400
специфический для меня метод машинный

968
00:43:14,400 --> 00:43:16,800
перевод, вы найдете множество других

969
00:43:16,800 --> 00:43:19,359
мест, включая все другие виды

970
00:43:19,359 --> 00:43:21,839
последовательности последовательностей  модели, это не

971
00:43:21,839 --> 00:43:24,160
единственный другой метод декодирования, однажды, когда

972
00:43:24,160 --> 00:43:26,240
мы перейдем к

973
00:43:26,240 --> 00:43:28,160
классу генерации языка, мы увидим

974
00:43:28,160 --> 00:43:30,240
еще пару, но это своего рода следующий метод,

975
00:43:30,240 --> 00:43:32,560
о котором вы должны знать,

976
00:43:32,560 --> 00:43:35,760
поэтому идея поиска луча заключается в том, что вы

977
00:43:35,760 --> 00:43:37,040
собираетесь

978
00:43:37,040 --> 00:43:37,920
сохранить

979
00:43:37,920 --> 00:43:40,880
некоторые гипотезы, которые повышают вероятность того,

980
00:43:40,880 --> 00:43:44,560
что вы найдете хорошее поколение, сохраняя при

981
00:43:44,560 --> 00:43:47,359
этом управляемость поиска,

982
00:43:47,359 --> 00:43:50,880
поэтому мы выбираем размер луча,

983
00:43:50,880 --> 00:43:53,040
а для нейронной сети размер луча

984
00:43:53,040 --> 00:43:55,040
обычно довольно мал, примерно от

985
00:43:55,040 --> 00:43:57,920
пяти до десяти, и на каждом шаге

986
00:43:57,920 --> 00:44:00,720
декодера мы собираемся отслеживать

987
00:44:00,720 --> 00:44:04,480
k наиболее вероятных частичных переводов, поэтому

988
00:44:04,480 --> 00:44:06,960
начальные подпоследовательности того, что мы

989
00:44:06,960 --> 00:44:10,319
генерируем, которые мы называем гипотезами

990
00:44:10,319 --> 00:44:13,359
um, так что гипотеза, которая затем является своего

991
00:44:13,359 --> 00:44:17,359
рода префиксом at  ranslation имеет оценку,

992
00:44:17,359 --> 00:44:19,680
которая представляет собой логарифмическую вероятность до

993
00:44:19,680 --> 00:44:22,480
того, что было сгенерировано до сих пор, поэтому мы можем

994
00:44:22,480 --> 00:44:24,000
сгенерировать это

995
00:44:24,000 --> 00:44:25,920
обычным способом, используя нашу условную

996
00:44:25,920 --> 00:44:27,280
языковую модель,

997
00:44:27,280 --> 00:44:28,079
так

998
00:44:28,079 --> 00:44:30,640
как записанные все оценки являются

999
00:44:30,640 --> 00:44:33,680
отрицательными, и поэтому наименьший отрицательный

1000
00:44:33,680 --> 00:44:36,240
i - наибольшая вероятность  является лучшим,

1001
00:44:36,240 --> 00:44:39,520
поэтому мы хотим

1002
00:44:39,520 --> 00:44:43,680
искать гипотезы с высокой вероятностью,

1003
00:44:43,680 --> 00:44:47,040
так что это эвристический метод, который не

1004
00:44:47,040 --> 00:44:48,720
гарантирует нахождение

1005
00:44:48,720 --> 00:44:51,200
декодирования с самой высокой вероятностью, но, по крайней мере, он

1006
00:44:51,200 --> 00:44:53,440
дает вам больше шансов, чем просто

1007
00:44:53,440 --> 00:44:54,319
выполнение

1008
00:44:54,319 --> 00:44:56,800
жадного декодирования, поэтому давайте пройдемся

1009
00:44:56,800 --> 00:44:58,400
пример,

1010
00:44:58,400 --> 00:45:01,359
чтобы увидеть, как это работает,

1011
00:45:01,359 --> 00:45:02,960
так что

1012
00:45:02,960 --> 00:45:05,520
в этом случае, чтобы я мог разместить его на слайде,

1013
00:45:05,520 --> 00:45:08,319
размер нашего луча составляет всего два мкм,

1014
00:45:08,319 --> 00:45:10,480
хотя обычно он на самом деле был

1015
00:45:10,480 --> 00:45:12,640
бы немного больше, а синие

1016
00:45:12,640 --> 00:45:15,920
числа - это оценки префиксов

1017
00:45:15,920 --> 00:45:19,200
Итак, это логарифмические

1018
00:45:19,200 --> 00:45:22,480
вероятности префикса, поэтому мы начинаем с нашего начального

1019
00:45:22,480 --> 00:45:26,160
символа и собираемся сказать хорошо,

1020
00:45:26,160 --> 00:45:28,960
какие два слова с наибольшей вероятностью будут

1021
00:45:28,960 --> 00:45:31,599
сгенерированы первыми в соответствии с нашей языковой

1022
00:45:31,599 --> 00:45:34,079
моделью и s  o может быть, первые два наиболее

1023
00:45:34,079 --> 00:45:37,119
вероятных слова - это он и я,

1024
00:45:37,119 --> 00:45:40,480
и есть их логарифмические вероятности,

1025
00:45:40,480 --> 00:45:44,240
тогда что мы делаем дальше: для каждой

1026
00:45:44,240 --> 00:45:48,800
из этих k гипотез мы находим, какие

1027
00:45:48,800 --> 00:45:51,359
вероятные слова последуют за ними, в

1028
00:45:51,359 --> 00:45:53,839
частности, мы находим одно из k наиболее

1029
00:45:53,839 --> 00:45:57,440
вероятных слов  следовать за каждым из них, чтобы

1030
00:45:57,440 --> 00:46:01,760
мы могли сгенерировать он ударил он ударил меня

1031
00:46:03,200 --> 00:46:06,160
все в порядке, так что на данный момент похоже,

1032
00:46:06,160 --> 00:46:08,800
что мы движемся вниз, что снова превратится

1033
00:46:08,800 --> 00:46:11,760
в экспоненциальную древовидную структуру истинного размера,

1034
00:46:11,760 --> 00:46:15,119
но то, что мы делаем сейчас,

1035
00:46:15,119 --> 00:46:18,720
 мы вычисляем оценки каждой из

1036
00:46:18,720 --> 00:46:21,680
этих частичных гипотез, поэтому у нас есть

1037
00:46:21,680 --> 00:46:24,800
четыре частичные гипотезы, которые он поразил, он поразил

1038
00:46:24,800 --> 00:46:29,280
меня, я получил, и мы можем сделать это,

1039
00:46:29,280 --> 00:46:31,440
взяв предыдущую оценку, что у нас есть

1040
00:46:31,440 --> 00:46:35,440
частичная гипотеза, и добавив логарифмическую

1041
00:46:35,440 --> 00:46:37,760
вероятность генерации  следующее слово

1042
00:46:37,760 --> 00:46:41,280
вот здесь ударил, так что это дает нам баллы

1043
00:46:41,280 --> 00:46:44,319
для каждой гипотезы, а затем мы можем сказать,

1044
00:46:44,319 --> 00:46:47,359
какая из этих двух частичных гипотез,

1045
00:46:47,359 --> 00:46:50,079
потому что наш размер луча k равен двум,

1046
00:46:50,079 --> 00:46:54,400
имеет наивысший балл, так что они были я,

1047
00:46:54,400 --> 00:46:57,760
и он ударил, поэтому мы сохраняем эти две

1048
00:46:57,760 --> 00:46:59,920
и  игнорируем остальные,

1049
00:46:59,920 --> 00:47:02,400
и тогда для этих двух

1050
00:47:02,400 --> 00:47:05,680
мы собираемся сгенерировать um k гипотез

1051
00:47:05,680 --> 00:47:08,400
для наиболее вероятного следующего слова, которое

1052
00:47:08,400 --> 00:47:11,839
он ударил ээ он ударил меня я был поражен я был

1053
00:47:11,839 --> 00:47:14,960
поражен um и снова

1054
00:47:14,960 --> 00:47:18,560
теперь мы хотим найти k наиболее вероятных

1055
00:47:20,240 --> 00:47:22,880
гипотез  из этого полного набора, так что это

1056
00:47:22,880 --> 00:47:26,640
будет он ударил меня, и я был о, нет,

1057
00:47:26,640 --> 00:47:29,680
он ударил меня, и он ударил r

1058
00:47:29,680 --> 00:47:32,880
um, поэтому мы оставляем только эти,

1059
00:47:32,880 --> 00:47:36,559
а затем для каждого из них мы

1060
00:47:36,559 --> 00:47:40,000
генерируем k наиболее вероятных следующих слов

1061
00:47:40,000 --> 00:47:43,440
пирог с on  а затем снова мы

1062
00:47:43,440 --> 00:47:45,920
фильтруем обратно до размера k,

1063
00:47:45,920 --> 00:47:47,599
говоря:

1064
00:47:47,599 --> 00:47:50,400
хорошо, две наиболее вероятные вещи здесь - это

1065
00:47:50,400 --> 00:47:53,760
круговая диаграмма или ширина, поэтому мы продолжаем работать над

1066
00:47:53,760 --> 00:47:55,119
теми, которые

1067
00:47:55,119 --> 00:47:57,440
генерируют вещи,

1068
00:47:57,440 --> 00:47:59,680
обнаруживаем, что два наиболее вероятных

1069
00:47:59,680 --> 00:48:04,480
генерируют вещи, находят два наиболее вероятных,

1070
00:48:04,480 --> 00:48:08,240
и на этом этапе мы бы  сгенерируйте

1071
00:48:08,240 --> 00:48:11,839
конец строки и скажите, что у нас есть

1072
00:48:11,839 --> 00:48:15,839
полная гипотеза, он ударил меня кругом

1073
00:48:16,960 --> 00:48:21,119
um, и мы могли бы затем проследить um

1074
00:48:21,119 --> 00:48:24,319
через дерево um, чтобы получить полную

1075
00:48:24,319 --> 00:48:27,839
гипотезу для этого предложения,

1076
00:48:27,839 --> 00:48:30,640
так что в большей части алгоритма есть еще

1077
00:48:30,640 --> 00:48:32,880
одна деталь

1078
00:48:32,880 --> 00:48:36,079
который останавливает крик  terion, поэтому при

1079
00:48:36,079 --> 00:48:39,280
жадном декодировании мы обычно

1080
00:48:39,280 --> 00:48:42,079
декодируем до тех пор, пока модель не создаст n

1081
00:48:42,079 --> 00:48:44,880
токенов, и когда она создаст конечный токен,

1082
00:48:44,880 --> 00:48:48,000
мы говорим, что мы закончили

1083
00:48:48,000 --> 00:48:50,640
декодирование поиска луча, разные

1084
00:48:50,640 --> 00:48:54,240
гипотезы могут создавать n токенов на

1085
00:48:54,240 --> 00:48:57,440
разных временных шагах, и поэтому мы не

1086
00:48:57,440 --> 00:48:58,960
хотим  остановитесь,

1087
00:48:58,960 --> 00:49:01,520
как только один путь в

1088
00:49:01,520 --> 00:49:05,040
дереве поиска сгенерирует конец, потому что может

1089
00:49:05,040 --> 00:49:06,720
оказаться, что есть другой путь

1090
00:49:06,720 --> 00:49:09,440
через дерево поиска, который все

1091
00:49:09,440 --> 00:49:12,640
равно окажется лучше, поэтому то, что мы делаем,

1092
00:49:12,640 --> 00:49:15,359
вроде как откладываем его в сторону как полную

1093
00:49:15,359 --> 00:49:16,640
гипотезу

1094
00:49:16,640 --> 00:49:19,680
и  продолжаем исследовать другие гипотезы с

1095
00:49:19,680 --> 00:49:21,680
помощью нашего поиска луча,

1096
00:49:21,680 --> 00:49:25,680
и поэтому обычно мы останавливаемся,

1097
00:49:25,680 --> 00:49:26,960
когда мы

1098
00:49:26,960 --> 00:49:29,440
достигаем предельной длины,

1099
00:49:29,440 --> 00:49:32,640
или когда мы завершаем

1100
00:49:32,640 --> 00:49:34,000
n

1101
00:49:34,000 --> 00:49:36,400
полных гипотез,

1102
00:49:36,400 --> 00:49:38,400
а затем мы

1103
00:49:38,400 --> 00:49:41,680
просматриваем гипотезы, которые мы завершили, и

1104
00:49:41,680 --> 00:49:44,160
скажите, какая из них лучшая, и

1105
00:49:44,160 --> 00:49:47,359
это та, которую мы будем использовать,

1106
00:49:47,760 --> 00:49:50,160
хорошо, так что на этом этапе у нас есть наш

1107
00:49:50,160 --> 00:49:53,200
список завершенных гипотез,

1108
00:49:53,200 --> 00:49:56,079
и мы хотим выбрать лучшую из них

1109
00:49:56,079 --> 00:49:58,400
с наивысшим баллом хорошо, это именно

1110
00:49:58,400 --> 00:50:02,240
то, чем мы были  вычисление каждого из них имеет

1111
00:50:03,200 --> 00:50:05,119
вероятность,

1112
00:50:05,119 --> 00:50:07,599
которую мы разработали, но оказывается,

1113
00:50:07,599 --> 00:50:10,240
что мы, возможно, не захотим использовать это

1114
00:50:10,240 --> 00:50:13,040
так наивно, потому что оказывается

1115
00:50:13,040 --> 00:50:16,480
своего рода систематическая проблема, которую

1116
00:50:16,480 --> 00:50:19,920
вы знаете не как теорему, а в целом

1117
00:50:19,920 --> 00:50:23,280
более длинные гипотезы имеют более низкие оценки, поэтому,

1118
00:50:23,280 --> 00:50:25,839
если вы думаете об этом как о

1119
00:50:25,839 --> 00:50:28,960
вероятностях последовательного генерирования каждого слова,

1120
00:50:28,960 --> 00:50:31,280
которое в основном на каждом шаге вы

1121
00:50:31,280 --> 00:50:33,599
умножаете на другой шанс

1122
00:50:33,599 --> 00:50:36,400
создания вероятности следующего слова, и

1123
00:50:36,400 --> 00:50:38,480
обычно это может быть

1124
00:50:38,480 --> 00:50:41,040
10 минус три десятка  минус два, так

1125
00:50:41,040 --> 00:50:43,280
что, судя по длине предложения,

1126
00:50:43,280 --> 00:50:45,280
ваши вероятности становятся намного

1127
00:50:45,280 --> 00:50:47,680
ниже, чем дольше они продолжаются,

1128
00:50:47,680 --> 00:50:51,119
что кажется несправедливым, поскольку,

1129
00:50:51,119 --> 00:50:53,200
хотя в некотором смысле чрезвычайно длинные

1130
00:50:53,200 --> 00:50:55,680
предложения не так вероятны, как короткие,

1131
00:50:55,680 --> 00:50:58,319
они '  Не менее вероятно, что к

1132
00:50:58,319 --> 00:51:00,400
тому времени мы создаем длинные

1133
00:51:00,400 --> 00:51:03,359
предложения, поэтому, например, вы знаете, что в

1134
00:51:03,359 --> 00:51:05,680
газете

1135
00:51:06,800 --> 00:51:10,160
средняя длина предложений превышает 20, поэтому

1136
00:51:10,160 --> 00:51:12,000
вы не захотите иметь рекламу

1137
00:51:12,000 --> 00:51:14,400
Модель экодирования при переводе новостных

1138
00:51:14,400 --> 00:51:17,200
статей, которая вроде как говорит: «О,

1139
00:51:17,200 --> 00:51:19,280
просто генерируйте предложения из двух слов,

1140
00:51:19,280 --> 00:51:21,119
вероятность этого намного выше в соответствии с моей

1141
00:51:21,119 --> 00:51:23,599
языковой моделью, ммм.

1142
00:51:29,119 --> 00:51:32,480
работа в журнале вероятностей, что означает

1143
00:51:32,480 --> 00:51:34,880
деление на

1144
00:51:34,880 --> 00:51:37,280
длину предложения, а затем у вас есть оценка вероятности журнала для каждого

1145
00:51:37,280 --> 00:51:38,800
слова,

1146
00:51:40,880 --> 00:51:42,319
и вы знаете,

1147
00:51:42,319 --> 00:51:44,240
что можете утверждать, что это не совсем

1148
00:51:44,240 --> 00:51:45,119
верно

1149
00:51:45,119 --> 00:51:46,880
в некотором теоретическом смысле, но на

1150
00:51:46,880 --> 00:51:48,800
практике это работает довольно хорошо и  это

1151
00:51:48,800 --> 00:51:51,440
очень часто используемый

1152
00:51:51,440 --> 00:51:52,920
нейронный

1153
00:51:54,480 --> 00:51:56,559
перевод оказался

1154
00:51:56,559 --> 00:51:58,800
намного лучше, я покажу вам

1155
00:51:58,800 --> 00:52:02,240
пару статистических данных, и через мгновение

1156
00:52:02,240 --> 00:52:04,480
он имеет много преимуществ

1157
00:52:04,480 --> 00:52:07,359
эм, он дает лучшую производительность,

1158
00:52:07,359 --> 00:52:10,240
переводы лучше, в частности,

1159
00:52:10,240 --> 00:52:12,880
они более беглые, потому что  новые

1160
00:52:12,880 --> 00:52:15,680
языковые модели производят гораздо более плавные

1161
00:52:15,680 --> 00:52:17,119
предложения,

1162
00:52:17,119 --> 00:52:18,800
но

1163
00:52:18,800 --> 00:52:22,319
они также намного лучше используют контекст, потому что

1164
00:52:22,319 --> 00:52:24,640
нейронные языковые модели, включая

1165
00:52:24,640 --> 00:52:27,440
условные нейроязыковые модели, дают

1166
00:52:27,440 --> 00:52:30,079
это очень хороший способ обусловить

1167
00:52:30,079 --> 00:52:32,800
множество контекстов, в частности, мы можем просто

1168
00:52:32,800 --> 00:52:36,319
запустить длинный кодировщик и условие для

1169
00:52:36,319 --> 00:52:39,680
предыдущего предложения или мы можем

1170
00:52:39,680 --> 00:52:42,079
хорошо переводить слова в контексте, используя

1171
00:52:42,079 --> 00:52:44,800
нейронные модели нейронного контекста,

1172
00:52:44,800 --> 00:52:47,920
которые лучше понимают

1173
00:52:47,920 --> 00:52:50,079
сходство фраз и фразы, которые

1174
00:52:50,079 --> 00:52:54,079
означают примерно одно и то же,

1175
00:52:54,079 --> 00:52:55,119
а затем

1176
00:52:56,000 --> 00:52:59,040
метод оптимизации всех

1177
00:52:59,040 --> 00:53:01,359
параметров модели от начала до конца в

1178
00:53:01,359 --> 00:53:04,720
одной большой нейронной сети

1179
00:53:04,720 --> 00:53:07,680
оказался действительно мощной идеей, поэтому

1180
00:53:07,680 --> 00:53:10,160
раньше большую часть времени люди создавали

1181
00:53:11,599 --> 00:53:13,599
отдельные компоненты и настраивали их

1182
00:53:13,599 --> 00:53:16,079
по отдельности, что просто означало, что они

1183
00:53:16,079 --> 00:53:18,079
не были на самом деле оптимальными, когда их помещали в

1184
00:53:18,079 --> 00:53:20,400
гораздо большую систему,

1185
00:53:20,400 --> 00:53:22,720
так что действительно

1186
00:53:22,720 --> 00:53:25,920
очень мощная руководящая идея в мире нейронных

1187
00:53:25,920 --> 00:53:28,000
сетей заключается в том, что вы можете как бы построить

1188
00:53:28,000 --> 00:53:30,480
одну огромную сеть и просто оптимизировать

1189
00:53:30,480 --> 00:53:32,720
все это от начала до конца, что будет  дадут

1190
00:53:32,720 --> 00:53:34,400
вам гораздо лучшую производительность, а системы по

1191
00:53:34,400 --> 00:53:36,800
компонентам,

1192
00:53:36,800 --> 00:53:39,440
мы вернемся к затратам на это

1193
00:53:39,440 --> 00:53:42,079
позже в ходе курса,

1194
00:53:42,079 --> 00:53:44,000
модели также  на самом деле хороши

1195
00:53:44,000 --> 00:53:46,160
в других отношениях, они на самом деле требуют гораздо

1196
00:53:46,160 --> 00:53:48,319
меньше человеческих усилий для создания,

1197
00:53:48,319 --> 00:53:51,040
нет никакой разработки функций

1198
00:53:51,040 --> 00:53:52,079
,

1199
00:53:52,079 --> 00:53:54,319
в целом нет языковых

1200
00:53:54,319 --> 00:53:56,880
компонентов, которые вы используете один и тот же метод

1201
00:53:56,880 --> 00:53:59,040
для всех языковых пар,

1202
00:53:59,040 --> 00:54:01,119
конечно, редко что что-то может быть

1203
00:54:01,119 --> 00:54:03,359
идеальным во всех отношениях,

1204
00:54:03,359 --> 00:54:05,760
так что нейронные  У систем машинного перевода

1205
00:54:05,760 --> 00:54:08,800
также есть некоторые недостатки по сравнению

1206
00:54:08,800 --> 00:54:10,640
со старыми системами статистического машинного

1207
00:54:10,640 --> 00:54:12,880
перевода:

1208
00:54:12,880 --> 00:54:14,960
они менее интерпретируемы,

1209
00:54:14,960 --> 00:54:17,040
труднее понять, почему они делают то, что они

1210
00:54:17,040 --> 00:54:18,720
делают там, где вы, прежде чем вы

1211
00:54:18,720 --> 00:54:20,480
действительно сможете взглянуть на таблицы фраз, и они

1212
00:54:20,480 --> 00:54:21,920
были полезны,

1213
00:54:21,920 --> 00:54:24,400
поэтому они  их трудно отлаживать, их также довольно

1214
00:54:24,400 --> 00:54:27,280
сложно контролировать,

1215
00:54:27,280 --> 00:54:30,640
поэтому по сравнению с чем-либо вроде

1216
00:54:30,640 --> 00:54:34,160
правил написания, вы не можете дать много

1217
00:54:34,160 --> 00:54:37,200
подробностей, как если бы вы хотели сказать: о,

1218
00:54:37,200 --> 00:54:39,440
я бы хотел, чтобы мои переводы были более

1219
00:54:39,440 --> 00:54:41,920
случайный или что-то в

1220
00:54:41,920 --> 00:54:43,920
этом роде, трудно понять, что они будут генерировать,

1221
00:54:43,920 --> 00:54:47,760
поэтому существуют различные проблемы с безопасностью,

1222
00:54:48,960 --> 00:54:51,440
я покажу несколько примеров

1223
00:54:51,440 --> 00:54:54,000
всего через минуту, но сначала  делать

1224
00:54:54,000 --> 00:54:54,880
это

1225
00:54:54,880 --> 00:54:56,720
быстро, как мы

1226
00:54:56,720 --> 00:55:00,240
оцениваем машинный перевод? лучший

1227
00:55:00,240 --> 00:55:03,280
способ оценить машинный перевод -

1228
00:55:03,280 --> 00:55:06,880
это показать человеку, который свободно

1229
00:55:06,880 --> 00:55:09,200
владеет исходным и целевым языками,

1230
00:55:09,200 --> 00:55:12,799
предложения и заставить его вынести суждение

1231
00:55:12,799 --> 00:55:15,920
о том, насколько он хорош,

1232
00:55:15,920 --> 00:55:17,520
но это

1233
00:55:17,520 --> 00:55:19,359
дорого  сделать,

1234
00:55:19,359 --> 00:55:21,280
а может и не стать возможным, если

1235
00:55:21,280 --> 00:55:23,680
рядом нет нужных людей,

1236
00:55:23,680 --> 00:55:26,000
поэтому была проделана большая работа по поиску

1237
00:55:26,000 --> 00:55:28,160
автоматических методов оценки

1238
00:55:28,160 --> 00:55:31,040
переводов, которые были достаточно хороши,

1239
00:55:31,040 --> 00:55:33,520
и самый известный способ сделать это -

1240
00:55:33,520 --> 00:55:35,440
то, что называется синим

1241
00:55:35,440 --> 00:55:37,680
и  То, как вы делаете синий

1242
00:55:37,680 --> 00:55:39,760
- это то, что у вас

1243
00:55:39,760 --> 00:55:42,640
есть человеческий перевод или несколько человеческих

1244
00:55:42,640 --> 00:55:45,599
переводов исходного предложения, и

1245
00:55:45,599 --> 00:55:48,559
вы сравниваете машинный

1246
00:55:48,559 --> 00:55:51,920
перевод с заранее заданными человеческими

1247
00:55:51,920 --> 00:55:54,880
письменными переводами, и вы оцениваете

1248
00:55:54,880 --> 00:55:56,480
их схожесть,

1249
00:55:56,480 --> 00:56:00,400
вычисляя точность инграммы, то есть

1250
00:56:00,400 --> 00:56:02,480
слова, которые перекрываются

1251
00:56:02,480 --> 00:56:04,640
между компьютерными и человеческими

1252
00:56:04,640 --> 00:56:07,760
диаграммами письменного перевода триграммы и четыре

1253
00:56:07,760 --> 00:56:10,640
грамма, а затем

1254
00:56:10,640 --> 00:56:14,720
вычисление среднего геометрического между

1255
00:56:14,720 --> 00:56:17,280
наложениями  fn энграмм плюс штраф за

1256
00:56:17,280 --> 00:56:20,480
два коротких системных перевода, так что

1257
00:56:20,480 --> 00:56:23,599
синий оказался действительно полезной

1258
00:56:23,599 --> 00:56:25,839
мерой, но это несовершенная мера,

1259
00:56:25,839 --> 00:56:28,000
поскольку обычно существует много правильных

1260
00:56:28,000 --> 00:56:30,559
способов перевода предложения, и поэтому есть

1261
00:56:30,559 --> 00:56:33,680
некоторая удача в том,

1262
00:56:33,680 --> 00:56:34,880
что

1263
00:56:34,880 --> 00:56:37,119
человеческие письменные переводы вы

1264
00:56:37,119 --> 00:56:39,599
должны соответствовать тому, что

1265
00:56:39,599 --> 00:56:41,839
может быть хорошим переводом из

1266
00:56:41,839 --> 00:56:43,359
системы,

1267
00:56:43,359 --> 00:56:45,440
есть еще что сказать о деталях

1268
00:56:45,440 --> 00:56:47,359
синего

1269
00:56:47,359 --> 00:56:49,520
и о том, как он реализован, но

1270
00:56:49,520 --> 00:56:51,119
вы увидите все это, выполняя

1271
00:56:51,119 --> 00:56:52,880
задание 4,

1272
00:56:52,880 --> 00:56:54,880
потому что вы будете

1273
00:56:54,880 --> 00:56:57,040
создавать свой машинный перевод

1274
00:56:57,040 --> 00:57:00,079
системы и оценивая их с

1275
00:57:00,079 --> 00:57:00,880
помощью

1276
00:57:00,880 --> 00:57:03,359
синего алгоритма и их полную информацию

1277
00:57:03,359 --> 00:57:04,400
о

1278
00:57:04,400 --> 00:57:07,119
синем в раздаточном материале, но

1279
00:57:07,119 --> 00:57:08,960
в конце дня

1280
00:57:08,960 --> 00:57:11,680
синий дает оценку от нуля до

1281
00:57:11,680 --> 00:57:13,280
ста, где

1282
00:57:13,280 --> 00:57:15,520
ваша оценка равна ста, если вы

1283
00:57:15,520 --> 00:57:17,520
точно производите одну из

1284
00:57:17,520 --> 00:57:20,480
письменные переводы, написанные людьми, и ноль, если у вас

1285
00:57:20,480 --> 00:57:22,880
нет ни одной униграммы, которая

1286
00:57:22,880 --> 00:57:26,160
пересекается между ними

1287
00:57:26,160 --> 00:57:30,319
с тем довольно кратким вступлением, которое я хотел

1288
00:57:30,319 --> 00:57:32,160
показать y  Что-то вроде того, что произошло с

1289
00:57:32,160 --> 00:57:35,280
машинным переводом,

1290
00:57:35,280 --> 00:57:37,520
поэтому машинный перевод

1291
00:57:37,520 --> 00:57:39,680
со статистическими моделями, фразовый

1292
00:57:39,680 --> 00:57:41,680
статистический машинный перевод, который я

1293
00:57:41,680 --> 00:57:43,920
показал в начале урока,

1294
00:57:43,920 --> 00:57:45,599
продолжался

1295
00:57:45,599 --> 00:57:49,280
с середины 2000-х годов, и он

1296
00:57:49,280 --> 00:57:52,160
дал своего рода полу-хорошие

1297
00:57:52,160 --> 00:57:54,000
результаты  те, которые были в переводчике Google в

1298
00:57:54,000 --> 00:57:56,720
те дни, но к тому времени, когда вы вошли в

1299
00:57:56,720 --> 00:57:59,680
эм, 2010-е годы в

1300
00:57:59,680 --> 00:58:03,040
основном прогресс, и

1301
00:58:03,040 --> 00:58:05,200
статистический машинный перевод

1302
00:58:05,200 --> 00:58:06,960
застопорился,

1303
00:58:06,960 --> 00:58:09,839
и вы почти не получаете никакого увеличения с

1304
00:58:09,839 --> 00:58:12,559
течением времени, и большая часть увеличения

1305
00:58:12,559 --> 00:58:14,880
времени, которое вы получали с течением времени, было

1306
00:58:14,880 --> 00:58:16,480
просто потому, что вы тренируете свои

1307
00:58:16,480 --> 00:58:18,799
модели на большем количестве

1308
00:58:18,799 --> 00:58:21,040
данных в те годы,

1309
00:58:21,040 --> 00:58:24,920
примерно в начале 2010-х,

1310
00:58:25,839 --> 00:58:30,000
большая надежда, что большинство

1311
00:58:30,000 --> 00:58:31,760
людей спрашивали, что это за ось

1312
00:58:31,760 --> 00:58:34,079
Y, эта ось Y - это синяя оценка, о которой я

1313
00:58:34,079 --> 00:58:36,720
вам говорил на  предыдущий слайд

1314
00:58:36,720 --> 00:58:40,240
в начале 2010-х годов большая надежда, которую возлагала

1315
00:58:40,240 --> 00:58:42,160
большинство людей, работающих в сфере машинного перевода,

1316
00:58:42,160 --> 00:58:45,280
оправдалась, если мы построили более

1317
00:58:45,280 --> 00:58:47,200
сложную модель машинного перевода,

1318
00:58:47,200 --> 00:58:50,079
которая знает о  синтаксическая

1319
00:58:50,079 --> 00:58:52,640
структура языков, использующая

1320
00:58:52,640 --> 00:58:55,280
такие инструменты, как синтаксические анализаторы зависимостей,

1321
00:58:55,280 --> 00:58:57,920
сможет создавать гораздо лучшие переводы,

1322
00:58:57,920 --> 00:59:00,799
поэтому здесь представлены фиолетовые системы,

1323
00:59:00,799 --> 00:59:03,839
которые я вообще не описывал, но

1324
00:59:03,839 --> 00:59:06,640
с годами

1325
00:59:06,640 --> 00:59:08,640
это было довольно очевидно

1326
00:59:08,640 --> 00:59:12,480
казалось, что это едва ли помогло,

1327
00:59:12,480 --> 00:59:13,839
и поэтому

1328
00:59:13,839 --> 00:59:17,040
в середине 2000-х годов 2010-х годов,

1329
00:59:17,040 --> 00:59:17,960
так что в

1330
00:59:17,960 --> 00:59:21,200
2014 году была первая современная попытка

1331
00:59:21,200 --> 00:59:23,920
построить нейронную сеть для машинного

1332
00:59:23,920 --> 00:59:26,799
перевода и модель декодера кодировщика,

1333
00:59:26,799 --> 00:59:28,880
ммм, и к тому времени, когда она была вроде как

1334
00:59:28,880 --> 00:59:32,160
оценена в тестах в 2015 году,

1335
00:59:32,160 --> 00:59:34,000
она  было не так хорошо, как то, что было

1336
00:59:34,000 --> 00:59:36,240
создано за предыдущее десятилетие,

1337
00:59:36,240 --> 00:59:38,640
но оно уже становится довольно хорошим, но

1338
00:59:38,640 --> 00:59:41,680
было обнаружено, что эти новые

1339
00:59:41,680 --> 00:59:44,799
модели действительно открыли совершенно новый

1340
00:59:44,799 --> 00:59:47,440
путь к созданию гораздо

1341
00:59:47,440 --> 00:59:50,000
более совершенных систем машинного перевода, и

1342
00:59:50,000 --> 00:59:51,119
с тех пор

1343
00:59:51,119 --> 00:59:53,520
дела только начали развиваться, и с

1344
00:59:53,520 --> 00:59:56,160
каждым годом новые системы машинного перевода

1345
00:59:56,160 --> 00:59:59,119
становятся все лучше и

1346
00:59:59,119 --> 01:00:01,200
лучше, чем все, что у нас было до

1347
01:00:01,200 --> 01:00:02,559
этого.

1348
01:00:04,000 --> 01:00:07,839
по крайней мере, ранняя часть

1349
01:00:07,839 --> 01:00:10,160
применения нейронного машинного перевода глубокого обучения и

1350
01:00:10,160 --> 01:00:12,640
обработки естественного языка

1351
01:00:12,640 --> 01:00:15,839
была огромной

1352
01:00:15,839 --> 01:00:19,119
историей успеха за последние несколько

1353
01:00:19,119 --> 01:00:22,720
лет, когда у нас были такие модели, как gpt2

1354
01:00:22,720 --> 01:00:24,400
и gpt3,

1355
01:00:24,400 --> 01:00:28,160
и другие огромные нейронные модели, такие как птица,

1356
01:00:28,160 --> 01:00:30,000
улучшающие

1357
01:00:30,000 --> 01:00:31,760
поиск в Интернете  знаю, что это немного

1358
01:00:31,760 --> 01:00:34,720
сложнее, но это была первая область,

1359
01:00:34,720 --> 01:00:37,440
где была нейронная сеть, которая

1360
01:00:37,440 --> 01:00:41,200
была намного лучше, чем то, что она

1361
01:00:41,200 --> 01:00:44,400
существовала, и на самом деле решала практическую

1362
01:00:44,400 --> 01:00:46,799
проблему, в которой нуждалось множество людей в мире,

1363
01:00:48,319 --> 01:00:50,799
и это была

1364
01:00:50,799 --> 01:00:53,040
потрясающая скорость, с которой  успех был

1365
01:00:53,040 --> 01:00:54,559
достигнут,

1366
01:00:54,559 --> 01:00:55,680
так что

1367
01:00:55,680 --> 01:00:57,920
2014 год

1368
01:00:57,920 --> 01:01:00,319
был первым, что я называю здесь,

1369
01:01:00,319 --> 01:01:03,119
попытками исследования создать в

1370
01:01:03,119 --> 01:01:06,240
вашей системе машинного перевода, что означает,

1371
01:01:06,240 --> 01:01:09,119
что три или четыре человека, которые

1372
01:01:09,119 --> 01:01:12,240
работали над моделями нейронных сетей, подумали: «А

1373
01:01:12,240 --> 01:01:13,920
почему бы нам не посмотреть, можем ли мы использовать  один из

1374
01:01:13,920 --> 01:01:16,160
них переводить научиться переводить

1375
01:01:16,160 --> 01:01:18,160
предложения, где на самом деле они не были

1376
01:01:18,160 --> 01:01:19,680
людьми с опытом работы в машинном

1377
01:01:19,680 --> 01:01:22,960
переводе, но успех был

1378
01:01:22,960 --> 01:01:24,640
достигнут

1379
01:01:24,640 --> 01:01:29,119
так быстро  То, что в течение двух лет

1380
01:01:29,119 --> 01:01:31,280
Google переключился на использование нейронного

1381
01:01:31,280 --> 01:01:33,280
машинного перевода

1382
01:01:33,280 --> 01:01:34,640
для

1383
01:01:34,640 --> 01:01:37,520
большинства языков, а через пару

1384
01:01:37,520 --> 01:01:40,559
лет после этого практически любой, кто

1385
01:01:40,559 --> 01:01:42,799
занимается машинным переводом, теперь

1386
01:01:42,799 --> 01:01:45,839
развертывает живые системы нейронного машинного

1387
01:01:45,839 --> 01:01:49,200
перевода и получает

1388
01:01:49,200 --> 01:01:51,520
гораздо лучшие результаты, так что  был своего рода

1389
01:01:51,520 --> 01:01:54,640
удивительным технологическим переходом

1390
01:01:54,640 --> 01:01:55,520
,

1391
01:01:55,520 --> 01:01:57,760
поскольку за предыдущее десятилетие большие

1392
01:01:57,760 --> 01:02:00,240
системы статистического машинного перевода,

1393
01:02:00,240 --> 01:02:02,079
такие как предыдущее поколение Google

1394
01:02:02,079 --> 01:02:04,559
Translate, буквально создавались

1395
01:02:04,559 --> 01:02:06,400
сотнями инженеров

1396
01:02:06,400 --> 01:02:08,319
за годы,

1397
01:02:08,319 --> 01:02:13,039
но сравнительно небольшая группа людей,

1398
01:02:13,039 --> 01:02:16,319
занимающихся глубоким обучением, за несколько месяцев

1399
01:02:16,319 --> 01:02:18,960
с небольшим количеством кода,

1400
01:02:18,960 --> 01:02:20,640
и, надеюсь, вы даже почувствуете

1401
01:02:20,640 --> 01:02:23,359
это, выполняя задание 4, мы можем

1402
01:02:23,359 --> 01:02:24,240
создавать

1403
01:02:24,240 --> 01:02:26,960
новые системы машинного перевода, которые

1404
01:02:26,960 --> 01:02:30,240
работают намного лучше,

1405
01:02:30,240 --> 01:02:32,079
означает ли это,

1406
01:02:32,079 --> 01:02:35,520
что машинный перевод решен.

1407
01:02:35,520 --> 01:02:37,599
много разной легкости, над которой люди

1408
01:02:37,599 --> 01:02:40,799
продолжают очень активно работать, и вы можете узнать

1409
01:02:40,799 --> 01:02:42,720
больше о  он в небе, но сегодня

1410
01:02:42,720 --> 01:02:44,960
статья была связана внизу, но вы

1411
01:02:44,960 --> 01:02:46,559
знаете, что есть много проблем

1412
01:02:46,559 --> 01:02:48,799
без словарного запаса,

1413
01:02:48,799 --> 01:02:51,200
они несоответствия предметной области между

1414
01:02:51,200 --> 01:02:53,680
обучающими и тестовыми данными, поэтому его можно

1415
01:02:53,680 --> 01:02:56,000
обучать в основном на данных новостной ленты, но вы

1416
01:02:56,000 --> 01:02:57,359
хотите

1417
01:02:57,359 --> 01:03:00,640
переводить людей  сообщения facebook

1418
01:03:00,640 --> 01:03:02,079
по-прежнему существуют проблемы с

1419
01:03:02,079 --> 01:03:04,799
поддержанием контекста поверх более длинного текста, который

1420
01:03:04,799 --> 01:03:07,760
мы хотели бы перевести на языки, для

1421
01:03:07,760 --> 01:03:10,799
которых у нас мало данных, и поэтому

1422
01:03:10,799 --> 01:03:13,520
эти методы работают намного лучше, когда

1423
01:03:13,520 --> 01:03:17,440
у нас есть огромные объемы параллельных данных,

1424
01:03:17,440 --> 01:03:19,119
даже наши лучшие

1425
01:03:19,119 --> 01:03:22,640
мульти  -layer lstms не так хорош для

1426
01:03:22,640 --> 01:03:24,799
захвата предложения, что означает, что существуют

1427
01:03:24,799 --> 01:03:28,079
определенные проблемы, такие как интерпретация

1428
01:03:28,079 --> 01:03:31,119
того, к чему относятся местоимения, или в таких языках,

1429
01:03:31,119 --> 01:03:34,319
как китайский или японский,

1430
01:03:34,319 --> 01:03:36,880
где часто нет местоимения, но

1431
01:03:36,880 --> 01:03:38,880
есть подразумеваемая ссылка на

1432
01:03:38,880 --> 01:03:41,839
человека, который разрабатывает, как  переведите, что

1433
01:03:41,839 --> 01:03:43,440
для языков в законах есть много

1434
01:03:43,440 --> 01:03:45,680
флективных

1435
01:03:45,680 --> 01:03:47,920
форм существительных, глаголов и прилагательных, эти

1436
01:03:47,920 --> 01:03:50,160
системы часто ошибаются, поэтому все

1437
01:03:50,160 --> 01:03:52,960
еще существует множество  что нужно сделать,

1438
01:03:52,960 --> 01:03:55,119
вот просто

1439
01:03:55,119 --> 01:03:57,119
забавные примеры того, что

1440
01:03:57,119 --> 01:04:00,079
что-то идет не так, как надо, так что если вы

1441
01:04:00,079 --> 01:04:01,680
попросили перевести

1442
01:04:01,680 --> 01:04:04,160
бумажное варенье,

1443
01:04:04,160 --> 01:04:06,319
Google Translate решит, что

1444
01:04:06,319 --> 01:04:09,920
это разновидность варенья, точно так же, как

1445
01:04:09,920 --> 01:04:11,599
есть малиновое варенье

1446
01:04:11,599 --> 01:04:13,039
и

1447
01:04:13,039 --> 01:04:15,599
клубничное варенье  и это превращается в

1448
01:04:15,599 --> 01:04:16,799
застрявшую бумагу

1449
01:04:16,799 --> 01:04:18,559
эм,

1450
01:04:18,559 --> 01:04:20,559
есть

1451
01:04:20,559 --> 01:04:23,599
проблемы с согласованием и выбором,

1452
01:04:23,599 --> 01:04:27,119
поэтому, если у вас много языков, не делайте

1453
01:04:27,119 --> 01:04:30,720
различий по полу, и поэтому предложения,

1454
01:04:30,720 --> 01:04:34,720
э-э, нейтральные между вещами, являются мужскими

1455
01:04:34,720 --> 01:04:37,440
или женскими, так что малайский или турецкий - два

1456
01:04:37,440 --> 01:04:40,160
хорошо известных языка  такого рода,

1457
01:04:40,160 --> 01:04:41,760
но что происходит, когда это

1458
01:04:41,760 --> 01:04:43,920
переводится на английский с помощью Google

1459
01:04:43,920 --> 01:04:46,319
Translate, так это то, что

1460
01:04:47,280 --> 01:04:49,920
модель английского языка просто срабатывает и

1461
01:04:49,920 --> 01:04:53,200
применяет стереотипные предубеждения, и поэтому

1462
01:04:53,200 --> 01:04:55,839
эти гендерно-нейтральные предложения

1463
01:04:55,839 --> 01:04:58,640
переводятся на то, что она работает медсестрой,

1464
01:04:58,640 --> 01:05:01,280
он работает программистом, поэтому, если вы

1465
01:05:01,280 --> 01:05:04,079
хотите помочь решить эту проблему, вы все

1466
01:05:04,079 --> 01:05:07,039
можете помочь, используя единственное их число во

1467
01:05:07,039 --> 01:05:08,640
всех контекстах,

1468
01:05:08,640 --> 01:05:09,599
когда вы

1469
01:05:09,599 --> 01:05:11,839
размещаете материал в Интернете, и это может

1470
01:05:11,839 --> 01:05:13,599
затем изменить дистрибутив

1471
01:05:14,880 --> 01:05:16,960
Но люди также

1472
01:05:16,960 --> 01:05:18,960
работают над улучшением моделирования, чтобы попытаться

1473
01:05:18,960 --> 01:05:20,480
избежать этого.

1474
01:05:20,480 --> 01:05:22,960
Вот еще один забавный пример.

1475
01:05:25,760 --> 01:05:29,680
Пару лет назад люди заметили,

1476
01:05:29,680 --> 01:05:32,799
что если вы выберете один из более редких

1477
01:05:32,799 --> 01:05:34,240
языков, на

1478
01:05:34,240 --> 01:05:37,039
который Google будет переводить, например,

1479
01:05:37,039 --> 01:05:38,319
сомали.

1480
01:05:38,319 --> 01:05:39,680
гм,

1481
01:05:39,680 --> 01:05:43,280
и вы просто пишете в какой-

1482
01:05:43,280 --> 01:05:46,079
то ерунде, как кляп для геев, как ни

1483
01:05:46,079 --> 01:05:49,520
странно, это произвело бы из ниоткуда

1484
01:05:49,520 --> 01:05:52,559
пророческие и библейские тексты, поскольку

1485
01:05:52,559 --> 01:05:54,240
имя лорда было написано на иврите,

1486
01:05:54,240 --> 01:05:55,920
оно было написано на

1487
01:05:55,920 --> 01:05:57,920
языке еврейской нации,

1488
01:05:57,920 --> 01:06:01,839
что не делает  В общем,

1489
01:06:02,240 --> 01:06:04,240
мы собираемся узнать немного больше о том, почему

1490
01:06:04,240 --> 01:06:06,480
это происходит, ммм,

1491
01:06:06,480 --> 01:06:08,000
но

1492
01:06:08,000 --> 01:06:10,559
ах, это было немного тревожно,

1493
01:06:10,559 --> 01:06:12,720
насколько я понимаю, эта проблема

1494
01:06:12,720 --> 01:06:14,960
теперь исправлена в 2021 году, я не мог

1495
01:06:14,960 --> 01:06:17,200
получить переводчик Google  гм, чтобы больше создавать

1496
01:06:17,200 --> 01:06:21,280
подобные примеры,

1497
01:06:21,280 --> 01:06:23,680
но вы знаете, что есть много способов

1498
01:06:23,680 --> 01:06:26,880
продолжить исследования.

1499
01:06:36,000 --> 01:06:37,839
нововведения в

1500
01:06:37,839 --> 01:06:40,079
глубоком обучении nlp

1501
01:06:40,079 --> 01:06:42,480
были пионерами, и люди продолжают

1502
01:06:42,480 --> 01:06:45,039
упорно работать над этим люди нашли много-много

1503
01:06:45,039 --> 01:06:46,400
улучшений

1504
01:06:46,400 --> 01:06:49,359
эм, и на самом деле для последней части

1505
01:06:49,359 --> 01:06:51,760
урока в эту минуту я собираюсь

1506
01:06:51,760 --> 01:06:54,960
представить одно огромное улучшение, которое настолько

1507
01:06:54,960 --> 01:06:56,960
важно, что оно действительно произошло

1508
01:06:56,960 --> 01:06:59,520
доминировать над всей недавней областью

1509
01:07:01,760 --> 01:07:04,559
нейронных сетей Ньюэлла для НЛП, и это

1510
01:07:04,559 --> 01:07:06,400
идея внимания,

1511
01:07:06,400 --> 01:07:08,960
но прежде чем я перейду к вниманию, я хочу

1512
01:07:08,960 --> 01:07:10,240
потратить

1513
01:07:10,240 --> 01:07:13,920
три минуты на наше задание,

1514
01:07:13,920 --> 01:07:16,480
поэтому для четвертого задания в этом году

1515
01:07:16,480 --> 01:07:19,039
у нас есть новый  версия

1516
01:07:19,039 --> 01:07:21,839
задания, которое, как мы надеемся, будет

1517
01:07:21,839 --> 01:07:24,160
интересным, но это также настоящая

1518
01:07:24,160 --> 01:07:26,480
проблема, поэтому для четвертого задания в этом

1519
01:07:26,480 --> 01:07:27,680
году

1520
01:07:27,680 --> 01:07:30,160
мы решили сделать машинный перевод чероки на английский

1521
01:07:30,160 --> 01:07:32,079
язык,

1522
01:07:32,079 --> 01:07:34,400
так что чероки является вымирающим

1523
01:07:34,400 --> 01:07:36,640
языком коренных американцев, на нем

1524
01:07:36,640 --> 01:07:38,640
свободно говорят около 2000 человек

1525
01:07:38,640 --> 01:07:41,520
.  язык чрезвычайно мало ресурсов,

1526
01:07:41,520 --> 01:07:44,000
поэтому просто не так много

1527
01:07:44,000 --> 01:07:47,359
доступных письменных данных чероки,

1528
01:07:47,359 --> 01:07:50,079
и особенно не так много

1529
01:07:50,079 --> 01:07:52,960
параллельных предложений между чероки  и

1530
01:07:52,960 --> 01:07:54,079
английский,

1531
01:07:54,079 --> 01:07:56,960
и вот ответ на

1532
01:07:56,960 --> 01:08:00,400
причудливые пророческие переводы Google

1533
01:08:00,400 --> 01:08:04,079
для языков, для которых не так

1534
01:08:04,079 --> 01:08:08,079
много параллельных данных, как правило,

1535
01:08:08,079 --> 01:08:10,400
самое большое место, где вы можете получить

1536
01:08:10,400 --> 01:08:14,559
параллельные данные, - это переводы Библии,

1537
01:08:14,559 --> 01:08:16,479
поэтому вы можете

1538
01:08:16,479 --> 01:08:18,799
иметь свой личный выбор, где бы он ни находился

1539
01:08:21,359 --> 01:08:23,759
Вы не знаете, где вы находитесь в отношении религии,

1540
01:08:23,759 --> 01:08:25,920
но дело в том, что если вы

1541
01:08:25,920 --> 01:08:28,319
работаете над языками коренных народов, вы

1542
01:08:30,000 --> 01:08:33,439
очень быстро обнаруживаете, что большая часть

1543
01:08:33,439 --> 01:08:35,920
работы была проделана по сбору данных о

1544
01:08:35,920 --> 01:08:38,319
языках коренных народов и  много

1545
01:08:38,319 --> 01:08:41,040
материала, доступного в письменной

1546
01:08:41,040 --> 01:08:43,839
форме для многих языков коренных народов, - это

1547
01:08:43,839 --> 01:08:46,799
переводы Библии,

1548
01:08:46,799 --> 01:08:50,560
да ладно, так вот как выглядит чероки,

1549
01:08:52,960 --> 01:08:54,399
так что

1550
01:08:54,399 --> 01:08:56,399
вы можете видеть, что система письма

1551
01:08:56,399 --> 01:08:59,439
имеет смесь вещей, которые выглядят как

1552
01:08:59,439 --> 01:09:02,158
английские буквы, а затем все

1553
01:09:02,158 --> 01:09:04,719
письма, которых нет, и вот

1554
01:09:05,759 --> 01:09:09,040
начальная часть истории давным-давно о семи мальчиках,

1555
01:09:09,040 --> 01:09:11,279
которые проводили все свое время

1556
01:09:11,279 --> 01:09:13,040
у таунхауса, так что это  фрагмент

1557
01:09:13,040 --> 01:09:17,040
параллельных данных, который мы можем изучить, так

1558
01:09:17,040 --> 01:09:20,158
что система письма чероки состоит из 85

1559
01:09:20,158 --> 01:09:22,319
букв, и причина, по которой в ней так

1560
01:09:22,319 --> 01:09:25,198
много букв, заключается в том, что каждая из этих

1561
01:09:25,198 --> 01:09:28,319
букв на самом деле представляет собой слог,

1562
01:09:28,319 --> 01:09:31,279
поэтому многие языки мира

1563
01:09:31,279 --> 01:09:34,399
имеют строгую структуру согласных гласных слогов

1564
01:09:34,399 --> 01:09:37,040
так что у вас есть такие слова, как rata

1565
01:09:37,040 --> 01:09:39,359
per или что-то в этом роде,

1566
01:09:39,359 --> 01:09:41,600
или чероки

1567
01:09:41,600 --> 01:09:42,880
право,

1568
01:09:42,880 --> 01:09:44,640
и другой язык, такой как

1569
01:09:44,640 --> 01:09:46,640
гавайский,

1570
01:09:46,640 --> 01:09:49,040
и поэтому каждая из букв представляет собой

1571
01:09:49,040 --> 01:09:52,319
комбинацию согласной и гласной,

1572
01:09:52,319 --> 01:09:54,000
и гм,

1573
01:09:54,000 --> 01:09:57,360
это набор из них, а затем вы получите 17

1574
01:09:57,360 --> 01:10:00,640
на пять  дает вам 85 букв,

1575
01:10:01,440 --> 01:10:03,679
да, так что за возможность выполнить это задание

1576
01:10:03,679 --> 01:10:06,239
большое спасибо людям из

1577
01:10:06,239 --> 01:10:09,760
университета Северной Каролины, Чапел Хилл, которые

1578
01:10:09,760 --> 01:10:12,239
предоставили ресурсы, которые мы используем

1579
01:10:12,239 --> 01:10:14,800
для этого задания,

1580
01:10:14,800 --> 01:10:16,719
хотя вы можете говорить на довольно многих

1581
01:10:16,719 --> 01:10:18,960
языках на  google translate um

1582
01:10:18,960 --> 01:10:21,600
cherokee - это не язык, который Google

1583
01:10:21,600 --> 01:10:24,159
предлагает на google translate, поэтому мы можем видеть,

1584
01:10:24,159 --> 01:10:27,120
как далеко мы можем зайти, но мы должны

1585
01:10:27,120 --> 01:10:29,199
быть скромными в наших ожиданиях, потому

1586
01:10:29,199 --> 01:10:32,480
что это h  Мы стремимся создать очень хорошую MT-систему

1587
01:10:32,480 --> 01:10:34,960
с довольно ограниченным объемом

1588
01:10:34,960 --> 01:10:37,920
данных, поэтому мы увидим, как далеко мы можем зайти,

1589
01:10:37,920 --> 01:10:40,239
есть обратная сторона, которая для вас,

1590
01:10:40,239 --> 01:10:41,760
студентов, выполняющих

1591
01:10:41,760 --> 01:10:44,159
задание. Преимущество наличия не

1592
01:10:44,159 --> 01:10:46,400
слишком большого количества данных заключается в том, что вы  модели будут

1593
01:10:46,400 --> 01:10:48,560
обучаться относительно быстро, поэтому у нас на

1594
01:10:48,560 --> 01:10:50,800
самом деле будет меньше проблем, чем в

1595
01:10:50,800 --> 01:10:52,080
прошлом году,

1596
01:10:52,080 --> 01:10:54,080
когда модели людей потратят часы на

1597
01:10:54,080 --> 01:10:55,199
обучение, так

1598
01:10:55,199 --> 01:10:59,120
как крайний срок назначения подошел

1599
01:10:59,120 --> 01:11:01,199
к концу. Еще пара слов о

1600
01:11:01,199 --> 01:11:03,280
чероки, чтобы мы имели некоторое представление о том, о чем

1601
01:11:03,280 --> 01:11:04,719
говорим,

1602
01:11:04,719 --> 01:11:07,199
поэтому  чероки первоначально

1603
01:11:07,199 --> 01:11:10,400
жили в западной Северной Каролине и

1604
01:11:10,400 --> 01:11:13,440
теннисе в восточном Теннесси,

1605
01:11:13,440 --> 01:11:16,320
потом их как бы перевели на

1606
01:11:16,320 --> 01:11:19,040
юго-запад оттуда, а затем, в

1607
01:11:19,040 --> 01:11:20,400
частности,

1608
01:11:20,400 --> 01:11:22,239
для тех из вас, кто

1609
01:11:22,239 --> 01:11:24,960
ходил в американские средние школы и обращал

1610
01:11:24,960 --> 01:11:27,760
внимание на то, что вы, возможно, помните

1611
01:11:27,760 --> 01:11:30,480
обсуждение следа слез  когда

1612
01:11:30,480 --> 01:11:32,239
многие коренные американцы с

1613
01:11:32,239 --> 01:11:35,679
юго-востока США были

1614
01:11:35,679 --> 01:11:36,960
насильно вытеснены

1615
01:11:36,960 --> 01:11:40,719
далеко на запад, и поэтому большинство

1616
01:11:40,719 --> 01:11:43,600
чероки теперь живут в Оклахоме, хотя

1617
01:11:43,600 --> 01:11:46,080
есть и некоторые  в Северной

1618
01:11:46,080 --> 01:11:47,440
Каролине

1619
01:11:47,440 --> 01:11:50,239
система письма, которую я показал на этом

1620
01:11:50,239 --> 01:11:53,679
предыдущем слайде, была изобретена

1621
01:11:54,880 --> 01:11:57,520
секвойей человека чероки, это

1622
01:11:57,520 --> 01:11:59,280
его рисунок,

1623
01:11:59,280 --> 01:12:01,360
и это было на самом деле своего рода

1624
01:12:01,360 --> 01:12:04,960
невероятной вещью, поэтому он начал с

1625
01:12:04,960 --> 01:12:07,360
грамотности и

1626
01:12:07,360 --> 01:12:09,600
разработал, как

1627
01:12:09,600 --> 01:12:11,199
напишите или

1628
01:12:11,199 --> 01:12:13,199
создайте систему письма, которая была бы

1629
01:12:13,199 --> 01:12:16,480
хорошей для эм чероки, и, учитывая,

1630
01:12:16,480 --> 01:12:18,000
что у нее есть эта непревзойденная структура гласных,

1631
01:12:18,000 --> 01:12:21,440
он выбрал знаменитость, которая

1632
01:12:21,440 --> 01:12:24,480
оказалась хорошим выбором.

1633
01:12:27,520 --> 01:12:30,560
1840-е гг.

1634
01:12:30,560 --> 01:12:34,480
Процент чероки, которые были

1635
01:12:34,480 --> 01:12:36,159
грамотны

1636
01:12:36,159 --> 01:12:39,040
на языке чероки, написанном таким образом, был на

1637
01:12:39,040 --> 01:12:41,440
самом деле выше, чем процент

1638
01:12:41,440 --> 01:12:43,920
белых людей на юго-востоке

1639
01:12:43,920 --> 01:12:46,960
США в то время

1640
01:12:53,360 --> 01:12:55,920
эм, а потом мне нужно

1641
01:12:55,920 --> 01:12:58,480
будет сделать еще немного этого эм, мне придется

1642
01:12:58,480 --> 01:13:00,159
сделать еще немного в следующий раз, это

1643
01:13:00,159 --> 01:13:02,960
будет хорошо, так что последняя идея, которая

1644
01:13:02,960 --> 01:13:05,280
действительно важна для

1645
01:13:05,280 --> 01:13:07,679
секвенсора  Последовательность моделей - это

1646
01:13:07,679 --> 01:13:09,120
идея внимания,

1647
01:13:09,120 --> 01:13:13,440
и поэтому у нас была эта модель

1648
01:13:14,239 --> 01:13:15,199
выполнения

1649
01:13:15,199 --> 01:13:17,360
последовательности для моделей последовательности, таких как

1650
01:13:17,360 --> 01:13:19,600
нейронный машинный перевод,

1651
01:13:19,600 --> 01:13:23,360
и проблема с этой архитектурой

1652
01:13:23,360 --> 01:13:24,880
заключается в том, что

1653
01:13:24,880 --> 01:13:28,159
у нас есть одно скрытое состояние, которое

1654
01:13:28,159 --> 01:13:31,600
должно кодировать всю информацию о

1655
01:13:31,600 --> 01:13:34,560
исходное предложение, поэтому оно действует как своего

1656
01:13:34,560 --> 01:13:37,040
рода информационное узкое место, и это

1657
01:13:37,040 --> 01:13:39,840
вся информация, которой обусловлено поколение.

1658
01:13:50,400 --> 01:13:52,239
усреднить все векторы

1659
01:13:52,239 --> 01:13:54,800
источника, чтобы получить представление предложения,

1660
01:13:54,800 --> 01:13:57,120
но вы знаете, что этот метод оказывается

1661
01:13:57,120 --> 01:13:59,360
лучше для таких вещей, как анализ тональности,

1662
01:14:00,400 --> 01:14:03,040
и не очень хорош для машинного перевода,

1663
01:14:03,040 --> 01:14:05,199
где очень важно сохранять порядок слов,

1664
01:14:07,040 --> 01:14:10,800
поэтому кажется, что  нам было бы лучше,

1665
01:14:10,800 --> 01:14:11,600
если бы

1666
01:14:11,600 --> 01:14:14,560
мы каким-то образом могли получить больше информации

1667
01:14:14,560 --> 01:14:16,640
из исходного предложения,

1668
01:14:16,640 --> 01:14:20,080
пока мы генерируем перевод,

1669
01:14:20,080 --> 01:14:22,000
и в некотором смысле это просто соответствует  Если

1670
01:14:24,640 --> 01:14:27,360
вы переводчик-человек,

1671
01:14:27,360 --> 01:14:29,600
вы читаете предложение, которое хотите перевести,

1672
01:14:29,600 --> 01:14:31,760
и, возможно, начинаете переводить несколько

1673
01:14:31,760 --> 01:14:33,600
слов, но затем оглядываетесь на

1674
01:14:33,600 --> 01:14:35,600
исходное предложение, чтобы увидеть, что еще в

1675
01:14:35,600 --> 01:14:39,280
нем было  и перевести еще несколько слов, так что

1676
01:14:39,280 --> 01:14:41,199
очень быстро после появления первых нейронных

1677
01:14:41,199 --> 01:14:44,000
машинных систем перевода люди

1678
01:14:44,000 --> 01:14:46,800
пришли к идее, что, возможно, мы могли бы

1679
01:14:46,800 --> 01:14:49,520
построить лучшую нейронную МТ-модель,

1680
01:14:49,520 --> 01:14:52,880
которая сделала бы это, и это идея

1681
01:14:52,880 --> 01:14:54,880
напряжения,

1682
01:14:54,880 --> 01:14:57,920
поэтому основная идея находится на каждом этапе

1683
01:14:57,920 --> 01:15:02,159
декодера мы собираемся использовать прямую

1684
01:15:02,159 --> 01:15:06,239
связь между кодировщиком и декодером,

1685
01:15:06,239 --> 01:15:08,640
что позволит нам сосредоточиться на

1686
01:15:10,320 --> 01:15:12,400
конкретном слове или словах в исходной

1687
01:15:12,400 --> 01:15:16,960
последовательности и использовать его, чтобы помочь нам сгенерировать,

1688
01:15:16,960 --> 01:15:19,520
какие слова будут следующими.

1689
01:15:19,520 --> 01:15:22,400
просто пройдите сейчас, показывая вам

1690
01:15:22,400 --> 01:15:25,360
изображения того, что делает внимание, а затем

1691
01:15:25,360 --> 01:15:27,360
в начале следующего раза мы

1692
01:15:27,360 --> 01:15:31,600
рассмотрим уравнения более подробно,

1693
01:15:31,760 --> 01:15:35,440
поэтому мы генерируем, мы используем наш кодировщик,

1694
01:15:35,440 --> 01:15:37,440
как и раньше, и генерируем наш

1695
01:15:39,600 --> 01:15:42,560
фид представлений  в нашем кондиционировании, как и раньше, и

1696
01:15:42,560 --> 01:15:45,199
говорим, что мы начинаем наш перевод,

1697
01:15:45,199 --> 01:15:48,000
но на этом этапе мы берем это скрытое

1698
01:15:48,000 --> 01:15:49,600
представление

1699
01:15:49,600 --> 01:15:52,080
и говорим, что я собираюсь использовать это скрытое

1700
01:15:52,080 --> 01:15:54,640
представление, чтобы оглянуться на

1701
01:15:54,640 --> 01:15:57,600
источник, чтобы получить информацию непосредственно из

1702
01:15:57,600 --> 01:15:59,920
него, так что я буду делать

1703
01:15:59,920 --> 01:16:01,440
Я буду

1704
01:16:03,280 --> 01:16:06,320
сравнивать скрытое состояние декодера со

1705
01:16:06,320 --> 01:16:09,760
скрытым состоянием кодера в каждой

1706
01:16:09,760 --> 01:16:13,679
позиции и сгенерировать оценку внимания,

1707
01:16:13,679 --> 01:16:16,480
которая является своего рода оценкой сходства, такой

1708
01:16:16,480 --> 01:16:18,239
как скалярное произведение,

1709
01:16:18,239 --> 01:16:21,920
а затем на основе этих оценок внимания

1710
01:16:21,920 --> 01:16:24,560
я собираюсь вычислить  распределение вероятностей

1711
01:16:26,080 --> 01:16:28,560
um

1712
01:16:28,560 --> 01:16:32,640
относительно использования softmax, как обычно, чтобы сказать, какое

1713
01:16:32,640 --> 01:16:35,920
из этих состояний кодировщика

1714
01:16:35,920 --> 01:16:37,679
больше всего похоже на

1715
01:16:37,679 --> 01:16:41,120
мое состояние декодера, и поэтому мы будем

1716
01:16:41,120 --> 01:16:43,600
обучать модель здесь, чтобы она говорила

1717
01:16:43,600 --> 01:16:45,040
хорошо, вероятно, вам следует

1718
01:16:45,040 --> 01:16:47,280
сначала перевести первое слово предложения, поэтому

1719
01:16:47,280 --> 01:16:48,880
вот куда следует направить внимание,

1720
01:16:50,080 --> 01:16:52,400
поэтому на основе этого распределения внимания,

1721
01:16:52,400 --> 01:16:54,400
которое представляет собой распределение вероятностей,

1722
01:16:54,400 --> 01:16:56,960
выходящее из softmax,

1723
01:16:56,960 --> 01:16:58,400
мы собираемся

1724
01:16:58,400 --> 01:17:00,840
сгенерировать

1725
01:17:00,840 --> 01:17:05,760
новое внимание.  outport, поэтому

1726
01:17:05,760 --> 01:17:08,719
этот вывод внимания будет

1727
01:17:08,719 --> 01:17:10,480
средним из скрытых состояний

1728
01:17:10,480 --> 01:17:12,719
модели кодировщика, но он будет

1729
01:17:12,719 --> 01:17:16,880
средневзвешенным, основанным на нашем распределении внимания,

1730
01:17:18,480 --> 01:17:20,320
и поэтому мы собираемся взять этот

1731
01:17:20,320 --> 01:17:22,960
вывод внимания, объединить его со

1732
01:17:22,960 --> 01:17:26,560
скрытым  состояние декодера rnn

1733
01:17:26,560 --> 01:17:27,600
um,

1734
01:17:27,600 --> 01:17:29,600
и

1735
01:17:29,600 --> 01:17:33,199
вместе они затем будут

1736
01:17:33,199 --> 01:17:37,440
использоваться для прогнозирования soft max вируса,

1737
01:17:37,440 --> 01:17:40,320
какое слово генерировать первым, и мы

1738
01:17:40,320 --> 01:17:42,719
надеемся сгенерировать его,

1739
01:17:42,719 --> 01:17:45,120
а затем в этот момент мы как бы

1740
01:17:45,120 --> 01:17:49,199
пыхтем и продолжаем делать то же самое

1741
01:17:49,199 --> 01:17:52,560
вид вычислений в каждой позиции

1742
01:17:52,560 --> 01:17:54,719
здесь есть небольшое примечание, в котором

1743
01:17:54,719 --> 01:17:57,360
говорится, что иногда мы берем

1744
01:17:57,360 --> 01:17:59,760
выходной сигнал внимания с предыдущего шага, а также

1745
01:17:59,760 --> 01:18:02,320
передаем его в декодер вместе с

1746
01:18:02,320 --> 01:18:04,880
обычным входом декодера, поэтому мы отвлекаем это

1747
01:18:04,880 --> 01:18:06,719
внимание от фактической подачи его

1748
01:18:06,719 --> 01:18:08,080
вернуться

1749
01:18:08,080 --> 01:18:09,520
к

1750
01:18:09,520 --> 01:18:12,000
вычислению скрытого состояния, и это

1751
01:18:12,000 --> 01:18:14,239
иногда может улучшить производительность,

1752
01:18:14,239 --> 01:18:16,159
и у нас действительно есть этот трюк в

1753
01:18:16,159 --> 01:18:18,560
системе четырех назначений, и вы можете попробовать

1754
01:18:18,560 --> 01:18:20,239
его

1755
01:18:20,239 --> 01:18:23,280
хорошо, поэтому мы генерируем  И сгенерируйте

1756
01:18:23,280 --> 01:18:25,679
все наше предложение

1757
01:18:25,679 --> 01:18:27,440
таким образом,

1758
01:18:27,440 --> 01:18:29,040
и это

1759
01:18:29,040 --> 01:18:32,239
оказалось очень эффективным способом более гибкого получения

1760
01:18:32,239 --> 01:18:34,719
дополнительной информации из исходного

1761
01:18:34,719 --> 01:18:37,600
предложения, чтобы мы могли

1762
01:18:38,640 --> 01:18:40,719
сгенерировать хороший перевод,

1763
01:18:40,719 --> 01:18:43,440
я остановлюсь на этом сейчас и в

1764
01:18:43,440 --> 01:18:45,920
начале следующего раза.  Закончим это,

1765
01:18:45,920 --> 01:18:47,600
рассмотрев реальные уравнения

1766
01:18:47,600 --> 01:18:51,320
того, как на вас работает внимание.

