1
00:00:05,200 --> 00:00:06,640
Итак, что мы собираемся делать на

2
00:00:06,640 --> 00:00:10,240
сегодня, так что основной контент на сегодня -

3
00:00:10,240 --> 00:00:11,519
это пройти

4
00:00:11,519 --> 00:00:12,320


5
00:00:12,320 --> 00:00:13,679


6
00:00:13,679 --> 00:00:16,560
еще кое-что о векторах слов,

7
00:00:16,560 --> 00:00:19,039
включая прикосновение к датчикам слов, а

8
00:00:19,039 --> 00:00:21,119
затем введение понятия

9
00:00:21,119 --> 00:00:23,840
классификаторов нейронных сетей, так что наша самая большая

10
00:00:23,840 --> 00:00:26,640
цель состоит в том, чтобы  В конце сегодняшнего урока

11
00:00:26,640 --> 00:00:28,080
вы должны почувствовать, что можете с

12
00:00:28,080 --> 00:00:30,160
уверенностью взглянуть на одну из

13
00:00:30,160 --> 00:00:32,479
документов для встраивания слов, такую как бумага Google

14
00:00:32,479 --> 00:00:35,760
word2vect, перчаточная бумага или

15
00:00:35,760 --> 00:00:37,600
бумага санджива авроры, к которой мы вернемся

16
00:00:37,600 --> 00:00:39,520
позже, и почувствуете, что да, я могу

17
00:00:39,520 --> 00:00:41,520
понять  это я знаю, что они

18
00:00:41,520 --> 00:00:43,760
делают, и это имеет смысл, поэтому давайте

19
00:00:43,760 --> 00:00:46,640
вернемся к тому месту, где мы были, так что

20
00:00:46,640 --> 00:00:48,640
это было своего рода введение этой

21
00:00:48,640 --> 00:00:51,120
модели слова devec, и

22
00:00:51,120 --> 00:00:52,320
третья

23
00:00:52,320 --> 00:00:55,199
строка, ваша идея заключалась в том, что мы начали со

24
00:00:55,199 --> 00:00:57,120
случайных векторов слов, а затем мы

25
00:00:57,120 --> 00:00:58,960
собираясь отсортировать это, у нас есть большой корпус

26
00:00:58,960 --> 00:01:00,879
текста, и мы собираемся перебирать

27
00:01:00,879 --> 00:01:03,199
каждое слово во всем корпусе, и для

28
00:01:03,199 --> 00:01:05,438
каждой позиции мы собираемся попытаться

29
00:01:05,438 --> 00:01:08,960
предсказать, какие слова окружают это наше

30
00:01:08,960 --> 00:01:11,040
центральное слово, и мы собираемся  к d  o что

31
00:01:11,040 --> 00:01:13,520
с распределением вероятностей, которое

32
00:01:13,520 --> 00:01:16,960
определяется в терминах скалярного произведения

33
00:01:16,960 --> 00:01:19,840
между векторами слов для центрального

34
00:01:19,840 --> 00:01:22,479
слова и контекстными словами

35
00:01:22,479 --> 00:01:24,400
um, и таким образом, это даст оценку вероятности

36
00:01:24,400 --> 00:01:26,159
появления слова в

37
00:01:26,159 --> 00:01:29,119
контексте в хорошо, что реальные слова действительно

38
00:01:29,119 --> 00:01:31,520
встречались в  контекст into в этом

39
00:01:31,520 --> 00:01:33,520
случае, поэтому мы

40
00:01:33,520 --> 00:01:36,560
хотим сделать более вероятным, что

41
00:01:36,560 --> 00:01:39,360
превращение проблем, связанных с банковским сектором и кризисами,

42
00:01:39,360 --> 00:01:42,159
в контексте в, и поэтому

43
00:01:42,159 --> 00:01:44,640
мы учимся обновлять векторы слов,

44
00:01:44,640 --> 00:01:46,560
чтобы они могли предсказывать  фактические

45
00:01:46,560 --> 00:01:48,720
окружающие слова лучше,

46
00:01:48,720 --> 00:01:49,520


47
00:01:49,520 --> 00:01:52,399
а затем почти волшебство

48
00:01:52,399 --> 00:01:54,799
заключается в том, что не более чем этот простой

49
00:01:54,799 --> 00:01:57,360
алгоритм позволяет нам изучать векторы слов,

50
00:01:57,360 --> 00:01:59,520
которые хорошо фиксируют

51
00:01:59,520 --> 00:02:02,399
сходство слов и значимые направления в

52
00:02:02,399 --> 00:02:04,880
пространстве слов,

53
00:02:04,880 --> 00:02:08,239
поэтому более точно подходят для этой

54
00:02:08,239 --> 00:02:11,360
модели единственные параметры  этой модели -

55
00:02:11,360 --> 00:02:14,080
векторы слов, поэтому у нас есть векторы внешних слов

56
00:02:14,080 --> 00:02:16,239
и векторы центральных слов для каждого

57
00:02:16,239 --> 00:02:18,800
слова, а затем мы берем их скалярное

58
00:02:18,800 --> 00:02:22,239
произведение um, чтобы получить  Вероятность хорошо,

59
00:02:22,239 --> 00:02:24,879
мы берем скалярное произведение, чтобы получить оценку

60
00:02:24,879 --> 00:02:27,520
того, насколько вероятно, что конкретное внешнее

61
00:02:27,520 --> 00:02:29,440
слово должно произойти с центральным словом, а

62
00:02:29,440 --> 00:02:30,879
затем мы используем мягкое преобразование максимума,

63
00:02:30,879 --> 00:02:33,280
чтобы преобразовать эти оценки

64
00:02:33,280 --> 00:02:35,840
в вероятности, как я обсуждал в прошлый

65
00:02:35,840 --> 00:02:38,319
раз, и я вроде  Я вернусь в

66
00:02:38,319 --> 00:02:40,160
конце, на этот раз,

67
00:02:40,160 --> 00:02:43,519
на пару вещей, на которые стоит обратить внимание. Эту

68
00:02:43,519 --> 00:02:47,040
модель мы называем nlp. Мешок слов

69
00:02:47,040 --> 00:02:49,760
моделей. Мешок слов моделей - это модели,

70
00:02:49,760 --> 00:02:52,319
которые на самом деле не обращают никакого внимания на

71
00:02:52,319 --> 00:02:54,400
порядок слов и не позиционируют его.  Неважно

72
00:02:54,400 --> 00:02:56,080
, рядом ли вы с центральным словом или

73
00:02:56,080 --> 00:02:58,560
немного дальше слева или справа,

74
00:02:58,560 --> 00:03:00,400
оценка вероятности будет такой

75
00:03:00,400 --> 00:03:01,920
же,

76
00:03:01,920 --> 00:03:04,480
и это похоже на очень грубую

77
00:03:04,480 --> 00:03:06,720
модель языка, которая оскорбит любого

78
00:03:06,720 --> 00:03:09,120
лингвиста, и это очень  грубая модель

79
00:03:09,120 --> 00:03:10,959
языка, и

80
00:03:10,959 --> 00:03:13,280
по мере продолжения мы перейдем к более совершенным моделям языка, но даже

81
00:03:13,280 --> 00:03:15,680
этой грубой модели языка достаточно,

82
00:03:15,680 --> 00:03:18,800
чтобы с большой долей вероятности выучить

83
00:03:18,800 --> 00:03:21,040
извинения о

84
00:03:21,040 --> 00:03:23,599
свойствах слов,

85
00:03:23,599 --> 00:03:25,599
а затем второе примечание

86
00:03:25,599 --> 00:03:28,640
хорошо ж  Используя эту модель,

87
00:03:28,640 --> 00:03:30,720
мы хотим, чтобы она давала

88
00:03:30,720 --> 00:03:33,440
достаточно высокие вероятности

89
00:03:33,440 --> 00:03:36,000
словам, которые действительно встречаются в

90
00:03:36,000 --> 00:03:38,480
контексте центрального слова, по крайней мере, если они делают это

91
00:03:38,480 --> 00:03:40,959
вообще часто, но очевидно, что

92
00:03:40,959 --> 00:03:43,360
может встречаться много разных слов, поэтому мы не

93
00:03:43,360 --> 00:03:45,680
говорим о вероятностях, подобных  0,3 и

94
00:03:45,680 --> 00:03:48,159
0,5, мы, скорее всего, будем

95
00:03:48,159 --> 00:03:50,799
говорить о вероятностях, таких как 0,01,

96
00:03:50,799 --> 00:03:53,360
и подобных числах,

97
00:03:53,360 --> 00:03:56,400
а также о том, как мы этого достигаем, и о том,

98
00:03:56,400 --> 00:03:58,959
как это достигается с помощью модели словесного дефекта,

99
00:03:58,959 --> 00:04:01,360
и это этап

100
00:04:01,360 --> 00:04:04,159
обучения модели, заключающийся в размещении

101
00:04:04,159 --> 00:04:06,959
слов  которые похожи по смыслу

102
00:04:06,959 --> 00:04:09,200
близко друг к другу в этом

103
00:04:09,200 --> 00:04:11,599
многомерном векторном пространстве, так что вы снова не

104
00:04:11,599 --> 00:04:13,840
можете прочитать это, но если мы перейдем

105
00:04:13,840 --> 00:04:15,760
к этому,

106
00:04:15,760 --> 00:04:18,160
мы увидим

107
00:04:18,160 --> 00:04:20,238
много похожих слов, означающих группу

108
00:04:20,238 --> 00:04:22,000
близко друг к другу в пространстве, так что вот

109
00:04:22,000 --> 00:04:24,160
дни недели, такие как вторник, четверг,

110
00:04:24,160 --> 00:04:27,520
воскресенье, а также рождество, а

111
00:04:27,520 --> 00:04:29,120


112
00:04:29,120 --> 00:04:31,759
что еще у нас есть,

113
00:04:31,759 --> 00:04:33,440
у нас есть

114
00:04:33,440 --> 00:04:36,240
samsung и nokia, это

115
00:04:36,240 --> 00:04:38,400
диаграмма, которую я сделал несколько лет назад, так

116
00:04:38,400 --> 00:04:40,720
что тогда Nokia все еще была важным

117
00:04:40,720 --> 00:04:43,199
производителем c  У нас есть

118
00:04:43,199 --> 00:04:44,960
разные области, такие как математика и

119
00:04:44,960 --> 00:04:46,400
экономика,

120
00:04:46,400 --> 00:04:49,120
поэтому мы группируем слова, которые

121
00:04:49,120 --> 00:04:51,919
схожи по значению, на

122
00:04:51,919 --> 00:04:53,520
самом деле еще одно замечание, которое я хотел сделать

123
00:04:53,520 --> 00:04:55,600
по этому поводу, я снова имею в виду, что это

124
00:04:55,600 --> 00:04:58,080
двухмерное изображение, которое все, что я

125
00:04:58,080 --> 00:05:01,039
могу вам показать  на слайде,

126
00:05:01,039 --> 00:05:02,560
и это делается с помощью

127
00:05:02,560 --> 00:05:04,880
проекции основных компонентов, которую вы также будете

128
00:05:04,880 --> 00:05:07,919
использовать в задании. Что-

129
00:05:07,919 --> 00:05:10,240
то важное, что нужно запомнить, но

130
00:05:10,240 --> 00:05:13,520
трудно запомнить, - это то, что многомерные

131
00:05:13,520 --> 00:05:16,320
пространства имеют очень разные свойства

132
00:05:16,320 --> 00:05:18,160
по сравнению с двухмерными пространствами, на которые мы можем

133
00:05:18,160 --> 00:05:21,120
смотреть и  так, в частности,

134
00:05:21,120 --> 00:05:21,840


135
00:05:21,840 --> 00:05:25,039
слово вектор может быть близко ко многим другим

136
00:05:25,039 --> 00:05:27,600
вещам в многомерном пространстве, но

137
00:05:27,600 --> 00:05:31,360
близко к ним в разных измерениях,

138
00:05:31,360 --> 00:05:34,240
хорошо, поэтому я упомянул об

139
00:05:34,240 --> 00:05:38,080
обучении, поэтому следующий вопрос

140
00:05:38,080 --> 00:05:41,600
: как мы можем выучить хорошие

141
00:05:41,600 --> 00:05:44,080
векторы слов и это  был бит, который я

142
00:05:44,080 --> 00:05:47,039
не совсем подключил в конце прошлого

143
00:05:47,039 --> 00:05:51,199
класса, поэтому какое-то время в последнем я сказал

144
00:05:51,199 --> 00:05:54,320
исчисление, и мы должны вычислить

145
00:05:54,320 --> 00:05:55,840
градиент функции потерь

146
00:05:55,840 --> 00:05:57,680
относительно  параметры, которые

147
00:05:57,680 --> 00:06:00,000
позволят нам добиться прогресса, но я

148
00:06:00,000 --> 00:06:01,520
не собирал их

149
00:06:01,520 --> 00:06:04,000
вместе, поэтому

150
00:06:04,000 --> 00:06:08,080
мы собираемся начать со случайных векторов слов, которые

151
00:06:08,080 --> 00:06:10,080
мы инициализируем небольшими

152
00:06:10,080 --> 00:06:12,880
числами, близкими к нулю в каждом измерении, которое

153
00:06:12,880 --> 00:06:15,039
мы '  Мы определили нашу

154
00:06:15,039 --> 00:06:17,680
функцию потерь j, которую мы рассматривали в прошлый

155
00:06:17,680 --> 00:06:19,919
раз, а затем мы собираемся использовать

156
00:06:19,919 --> 00:06:22,400
алгоритм градиентного спуска, который является

157
00:06:22,400 --> 00:06:24,960
итеративным итеративным алгоритмом, который

158
00:06:24,960 --> 00:06:27,600
учится максимизировать j тэты,

159
00:06:27,600 --> 00:06:30,560
изменяя тэту, и поэтому идея этого

160
00:06:30,560 --> 00:06:32,800
алгоритма заключается в том, что

161
00:06:32,800 --> 00:06:35,520
от  текущие значения теты вы

162
00:06:35,520 --> 00:06:38,400
вычисляете градиент j теты, а

163
00:06:38,400 --> 00:06:40,560
затем то, что вы собираетесь сделать, это сделать

164
00:06:40,560 --> 00:06:42,960
небольшой шаг в направлении

165
00:06:42,960 --> 00:06:45,280
отрицательного градиента, чтобы градиент был

166
00:06:45,280 --> 00:06:47,520
направлен вверх, и мы делаем

167
00:06:47,520 --> 00:06:49,520
небольшой шаг в направлении

168
00:06:49,520 --> 00:06:52,160
отрицательного градиента, чтобы постепенно

169
00:06:52,160 --> 00:06:54,960
опускаться к минимуму,

170
00:06:54,960 --> 00:06:57,360
и поэтому одним из параметров нейронных

171
00:06:57,360 --> 00:06:59,280
сетей, которые вы можете использовать в своем

172
00:06:59,280 --> 00:07:02,000
программном пакете, является размер шага,

173
00:07:02,000 --> 00:07:04,400
поэтому, если вы возьмете действительно очень немного

174
00:07:04,400 --> 00:07:08,080
шагу вам может потребоваться много времени,

175
00:07:08,080 --> 00:07:11,280
чтобы свести к минимуму функцию, вы выполняете много

176
00:07:11,280 --> 00:07:14,000
бесполезных вычислений, с другой стороны, если

177
00:07:14,000 --> 00:07:15,840
ваш размер шага

178
00:07:15,840 --> 00:07:18,240
слишком

179
00:07:18,240 --> 00:07:21,039
велик, вы можете действительно отклониться и

180
00:07:21,039 --> 00:07:24,080
начать идти в худшие места или даже если

181
00:07:24,080 --> 00:07:26,400
вы спускаетесь под гору  Немного о

182
00:07:26,400 --> 00:07:27,759
том, что произойдет, так это то, что

183
00:07:27,759 --> 00:07:29,759
вы в конечном итоге будете подпрыгивать взад и вперед,

184
00:07:29,759 --> 00:07:31,599
и вам понадобится гораздо больше времени, чтобы добраться

185
00:07:31,599 --> 00:07:33,280
до минимума

186
00:07:33,280 --> 00:07:36,880
хорошо на этой картинке у меня красивая

187
00:07:36,880 --> 00:07:38,400
квадратичная,

188
00:07:38,400 --> 00:07:40,880
и ее легко минимизировать что-то

189
00:07:40,880 --> 00:07:42,319
что вы, возможно, знаете о нейронных

190
00:07:42,319 --> 00:07:44,639
сетях, так это то, что в целом они не

191
00:07:44,639 --> 00:07:47,199
выпуклые, поэтому вы могли подумать,

192
00:07:47,199 --> 00:07:49,919
что все будет хорошо, но

193
00:07:49,919 --> 00:07:51,680
правда в том, что на практике жизнь

194
00:07:51,680 --> 00:07:52,720
идет хорошо,

195
00:07:52,720 --> 00:07:54,720
но я думаю, что я не пойму

196
00:07:54,720 --> 00:07:57,120
прямо сейчас и вернемся к этому um в

197
00:07:57,120 --> 00:07:59,120
более позднем классе,

198
00:07:59,120 --> 00:08:01,520
так что это наш градиентный спуск, поэтому у нас

199
00:08:01,520 --> 00:08:03,039
есть текущие значения

200
00:08:03,039 --> 00:08:07,120
параметров theta, мы затем

201
00:08:07,120 --> 00:08:08,879
немного пройдемся в отрицательном

202
00:08:08,879 --> 00:08:11,840
направлении градиента, используя нашу

203
00:08:11,840 --> 00:08:14,240
скорость обучения или шаг  размер  alpha, и

204
00:08:14,240 --> 00:08:17,680
это дает нам новые значения параметров, где

205
00:08:17,680 --> 00:08:19,440
это означает, что вы знаете, что это

206
00:08:19,440 --> 00:08:21,280
векторы, но для каждого отдельного

207
00:08:21,280 --> 00:08:24,240
параметра мы немного обновляем его,

208
00:08:24,240 --> 00:08:26,160
вычисляя частную

209
00:08:26,160 --> 00:08:28,960
производную j по этому

210
00:08:28,960 --> 00:08:31,599
параметру,

211
00:08:32,000 --> 00:08:34,080
так что простой алгоритм градиентного спуска

212
00:08:34,080 --> 00:08:36,880
никто  использует его, и вы

213
00:08:36,880 --> 00:08:38,958
не должны его

214
00:08:38,958 --> 00:08:42,719
использовать, проблема в том, что наш j является

215
00:08:42,719 --> 00:08:45,839
функцией всех окон в корпусе, помните,

216
00:08:45,839 --> 00:08:49,360
что мы делаем эту сумму по каждому центральному

217
00:08:49,360 --> 00:08:51,839
слову во всем корпусе, и у нас

218
00:08:51,839 --> 00:08:53,440
часто есть миллиарды слов в

219
00:08:53,440 --> 00:08:54,399
корпус,

220
00:08:54,399 --> 00:08:57,760
поэтому фактическая разработка j теты

221
00:08:57,760 --> 00:09:00,000
или градиента j теты будет

222
00:09:00,000 --> 00:09:02,160
чрезвычайно дорогостоящей, потому что мы

223
00:09:02,160 --> 00:09:04,399
должны перебирать весь наш корпус,

224
00:09:04,399 --> 00:09:06,720
поэтому вам придется очень долго ждать, прежде чем

225
00:09:06,720 --> 00:09:09,279
вы сделаете одно обновление градиента, и поэтому

226
00:09:09,279 --> 00:09:11,760
оптимизация будет чрезвычайно  медленный

227
00:09:11,760 --> 00:09:12,880
и поэтому в

228
00:09:12,880 --> 00:09:15,279
основном сто процентов времени

229
00:09:15,279 --> 00:09:17,760
в нейронных сетях мы не используем

230
00:09:17,760 --> 00:09:20,160
градиентный спуск, мы вместо этого используем то, что

231
00:09:20,160 --> 00:09:22,720
называется стохастическим градиентным спуском и

232
00:09:22,720 --> 00:09:25,200
стохастическим градиентным спуском  является очень

233
00:09:25,200 --> 00:09:27,760
простой модификацией этого, поэтому вместо того,

234
00:09:27,760 --> 00:09:31,040
чтобы вычислять

235
00:09:31,040 --> 00:09:34,399
оценку градиента на основе всего корпуса,

236
00:09:34,399 --> 00:09:37,440
вы просто берете одно центральное слово или

237
00:09:37,440 --> 00:09:40,720
небольшую партию, например 32 центральных слова, и

238
00:09:40,720 --> 00:09:42,959
вы вычисляете оценку градиента

239
00:09:42,959 --> 00:09:44,560
на основе них

240
00:09:44,560 --> 00:09:46,399
теперь, когда  оценка градиента будет

241
00:09:46,399 --> 00:09:47,120


242
00:09:47,120 --> 00:09:50,480
шумной и плохой, потому что вы просмотрели

243
00:09:50,480 --> 00:09:52,640
только небольшую часть корпуса, а

244
00:09:52,640 --> 00:09:55,120
не весь корпус, но, тем не менее,

245
00:09:55,120 --> 00:09:56,720
вы можете использовать эту оценку

246
00:09:56,720 --> 00:09:59,680
градиента для обновления ваших тета-

247
00:09:59,680 --> 00:10:02,480
параметров точно таким же образом, и поэтому это

248
00:10:02,480 --> 00:10:05,440
это алгоритм, который мы можем сделать, и поэтому,

249
00:10:05,440 --> 00:10:08,720
если у нас есть корпус из миллиарда слов, мы

250
00:10:08,720 --> 00:10:12,240
можем, если мы сделаем это для каждого центрального слова, мы

251
00:10:12,240 --> 00:10:13,920
сможем сделать миллиард обновлений

252
00:10:13,920 --> 00:10:16,480
параметров, которые мы передаем через корпус

253
00:10:16,480 --> 00:10:19,279
один раз, а не только сделать одно более

254
00:10:19,279 --> 00:10:20,800
точным

255
00:10:20,800 --> 00:10:22,959
обновите

256
00:10:22,959 --> 00:10:25,040
параметры сразу после того, как вы прошли через корпус,

257
00:10:25,040 --> 00:10:26,720
чтобы в целом

258
00:10:26,720 --> 00:10:28,000
мы могли изучить на

259
00:10:28,000 --> 00:10:30,800
несколько порядков быстрее,

260
00:10:30,800 --> 00:10:32,800
и поэтому это алгоритм, который вы

261
00:10:32,800 --> 00:10:34,800
будете использовать везде,

262
00:10:34,800 --> 00:10:37,600
включая вас  знать с

263
00:10:37,600 --> 00:10:40,800
самого начала из наших заданий

264
00:10:40,800 --> 00:10:42,720
эээ, еще

265
00:10:42,720 --> 00:10:45,120
раз просто дополнительный комментарий о более

266
00:10:45,120 --> 00:10:47,360
сложных вещах, к которым мы вернемся

267
00:10:47,360 --> 00:10:49,600
хорошо,

268
00:10:50,640 --> 00:10:52,160


269
00:10:52,160 --> 00:10:55,440
это градиентный спуск, это своего рода

270
00:10:55,440 --> 00:10:57,200
хак производительности, он позволяет вам учиться намного

271
00:10:57,200 --> 00:10:59,680
быстрее, оказывается, это не так

272
00:10:59,680 --> 00:11:02,160
только нейронные сети для взлома производительности имеют некоторые

273
00:11:02,160 --> 00:11:04,399
довольно противоречащие интуиции свойства um,

274
00:11:04,399 --> 00:11:07,680
и на самом деле тот факт, что

275
00:11:07,680 --> 00:11:10,240
стохастический градиентный спуск немного

276
00:11:10,240 --> 00:11:12,880
шумный и подпрыгивает, когда он делает свое

277
00:11:12,880 --> 00:11:15,839
дело, на самом деле означает, что в сложных

278
00:11:15,839 --> 00:11:19,760
сетях он изучает лучшие решения um,

279
00:11:19,760 --> 00:11:21,839
чем если бы вы  запускать

280
00:11:21,839 --> 00:11:24,480
простой градиентный спуск очень медленно, чтобы

281
00:11:24,480 --> 00:11:26,800
вы могли вычислять намного быстрее

282
00:11:26,800 --> 00:11:29,680
и лучше выполнять свою работу.

283
00:11:29,680 --> 00:11:30,640
Хорошо,

284
00:11:30,640 --> 00:11:32,959
одно последнее замечание по запуску стохастических

285
00:11:32,959 --> 00:11:35,120
градиентов с векторами слов, это своего

286
00:11:35,120 --> 00:11:36,880
рода отступление,

287
00:11:36,880 --> 00:11:39,120
но следует отметить, что если мы

288
00:11:39,120 --> 00:11:41,760
выполняем стохастический анализ  обновление градиента на

289
00:11:41,760 --> 00:11:45,360
основе одного окна, тогда на самом деле в этом

290
00:11:45,360 --> 00:11:47,600
окне мы не увидели почти ни одного из

291
00:11:47,600 --> 00:11:49,680
наших параметров, потому что если у нас есть

292
00:11:49,680 --> 00:11:51,680
окно чего-то вроде

293
00:11:51,680 --> 00:11:53,600
пяти слов  s по обе стороны от центрального

294
00:11:53,600 --> 00:11:57,440
слова мы видели не более 11 различных типов слов,

295
00:11:57,440 --> 00:11:59,120
поэтому

296
00:11:59,120 --> 00:12:01,040
у нас будет информация о градиенте для

297
00:12:01,040 --> 00:12:04,560
этих 11 слов, но для других 100000 нечетных

298
00:12:04,560 --> 00:12:06,880
слов теперь словарь не будет иметь

299
00:12:06,880 --> 00:12:09,680
информации об обновлении градиента, так что это

300
00:12:09,680 --> 00:12:13,680
будет очень-очень  разреженное обновление градиента,

301
00:12:13,680 --> 00:12:15,519
поэтому, если вы думаете только

302
00:12:15,519 --> 00:12:17,200
о математике,

303
00:12:17,200 --> 00:12:18,560
вы можете просто

304
00:12:18,560 --> 00:12:22,480
иметь весь свой градиент и использовать

305
00:12:22,480 --> 00:12:24,800
уравнение, которое я показал ранее,

306
00:12:24,800 --> 00:12:26,079
но если вы

307
00:12:26,079 --> 00:12:28,800
думаете об оптимизации системы,

308
00:12:28,800 --> 00:12:31,600
тогда вам нужно хорошо подумать, на самом деле я

309
00:12:31,600 --> 00:12:34,399
хочу только обновить

310
00:12:34,399 --> 00:12:37,120
параметры  для нескольких слов, и

311
00:12:37,120 --> 00:12:39,600
они должны быть, и есть гораздо более

312
00:12:39,600 --> 00:12:42,000
эффективные способы, которыми я мог бы это сделать,

313
00:12:42,000 --> 00:12:43,279


314
00:12:43,279 --> 00:12:45,200
и так

315
00:12:45,200 --> 00:12:47,680
вот, так что это еще одно в сторону, будет

316
00:12:47,680 --> 00:12:49,760
полезно для задания, поэтому я скажу

317
00:12:49,760 --> 00:12:50,639
это

318
00:12:50,639 --> 00:12:53,279
до сих пор, когда я представил

319
00:12:53,279 --> 00:12:56,000
векторы слов  Я представил их как векторы-столбцы,

320
00:12:56,000 --> 00:12:57,600


321
00:12:57,600 --> 00:13:00,160
и это имеет наибольший смысл, если вы

322
00:13:00,160 --> 00:13:03,519
думаете об этом как о части математики,

323
00:13:03,519 --> 00:13:05,680
тогда как на самом деле

324
00:13:05,680 --> 00:13:07,200
во всех

325
00:13:07,200 --> 00:13:09,519
распространенных пакетах глубокого обучения, включая

326
00:13:09,519 --> 00:13:11,680
pytorch, которые мы используем

327
00:13:11,680 --> 00:13:14,480
векторы слов, на самом деле воспроизводятся  представлены как

328
00:13:14,480 --> 00:13:18,000
векторы-строки, и если вы вспомните

329
00:13:18,000 --> 00:13:21,040
о представлении матриц и

330
00:13:21,040 --> 00:13:23,440
cs107 или что-то в этом роде, то вы

331
00:13:23,440 --> 00:13:25,200
знаете, что тогда это очевидно

332
00:13:25,200 --> 00:13:28,720
эффективно для представления слов,

333
00:13:28,720 --> 00:13:30,959
потому что тогда вы можете получить доступ ко всему

334
00:13:30,959 --> 00:13:32,959
вектору слова как к непрерывному диапазону

335
00:13:32,959 --> 00:13:34,560
памяти

336
00:13:34,560 --> 00:13:36,800
разные, если вы все равно находитесь в фортране, так что на

337
00:13:36,800 --> 00:13:38,639
самом деле наши

338
00:13:38,639 --> 00:13:41,839
векторы слов будут векторами-строками,

339
00:13:41,839 --> 00:13:44,240
когда вы посмотрите на эти мм внутри пи-

340
00:13:44,240 --> 00:13:46,639
факела,

341
00:13:46,800 --> 00:13:49,839
хорошо, теперь я хотел сказать немного больше

342
00:13:49,839 --> 00:13:52,880
о семействе алгоритмов word2vec,

343
00:13:52,880 --> 00:13:56,880
а также о том, что вы  собираюсь сделать

344
00:13:56,880 --> 00:13:59,360
в домашнем задании 2. эм так что если вы все еще

345
00:13:59,360 --> 00:14:01,199
должны работать над домашним заданием 1, которое

346
00:14:01,199 --> 00:14:04,639
помнит эм до следующего вторника, что на самом

347
00:14:04,639 --> 00:14:06,800
деле на самом деле с сегодняшним содержанием мы

348
00:14:06,800 --> 00:14:09,199
начинаем домашнее задание два, и я как бы пройдусь

349
00:14:09,199 --> 00:14:11,040
по первому  часть второго домашнего задания

350
00:14:11,040 --> 00:14:13,760
сегодня и другие вещи, которые вам

351
00:14:13,760 --> 00:14:16,240
нужно знать для второго домашнего задания, поэтому я

352
00:14:16,240 --> 00:14:18,800
кратко упомянул идею о том, что у нас есть два

353
00:14:18,800 --> 00:14:21,920
отдельных вектора для каждого типа слова:

354
00:14:21,920 --> 00:14:24,959
центральный вектор и внешние векторы,

355
00:14:24,959 --> 00:14:26,959
и мы просто усредняем  они оба в конце

356
00:14:26,959 --> 00:14:29,199
они похожи, но не идентичны по

357
00:14:29,199 --> 00:14:31,240
нескольким причинам, включая случайную

358
00:14:31,240 --> 00:14:33,360
инициализацию и стохастический

359
00:14:33,360 --> 00:14:35,839
градиентный спуск.

360
00:14:35,839 --> 00:14:37,040
Вы можете

361
00:14:37,040 --> 00:14:38,320
реализовать

362
00:14:38,320 --> 00:14:40,560
алгоритм дефекта слова только с одним

363
00:14:40,560 --> 00:14:43,519
вектором на слово, и на самом деле, если вы это сделаете,

364
00:14:43,519 --> 00:14:45,920
он будет работать немного лучше,

365
00:14:45,920 --> 00:14:48,079
но делает  алгоритм намного

366
00:14:48,079 --> 00:14:51,760
сложнее, и причина этого в том, что

367
00:14:51,760 --> 00:14:55,440
иногда у вас будет тот же тип слова,

368
00:14:55,440 --> 00:14:58,480
что и центральное слово и контекстное слово,

369
00:14:58,480 --> 00:15:01,120
а это означает, что когда вы выполняете

370
00:15:01,120 --> 00:15:03,600
свое вычисление в этой точке, вы

371
00:15:03,600 --> 00:15:06,079
получаете такого рода  беспорядочный случай, когда только

372
00:15:06,079 --> 00:15:08,079
для этого слова вы получаете термин x в

373
00:15:08,079 --> 00:15:10,959
квадрате, извините, скалярный продукт,

374
00:15:10,959 --> 00:15:12,959
вы получаете точечный продукт x, точка x,

375
00:15:12,959 --> 00:15:15,199
что делает его более беспорядочным

376
00:15:15,199 --> 00:15:17,120
для работы, и поэтому мы используем

377
00:15:17,120 --> 00:15:19,199
этот вид  простой оптимизации

378
00:15:19,199 --> 00:15:21,279
с использованием двух векторов на слово - это

379
00:15:21,279 --> 00:15:22,240
нормально,

380
00:15:22,240 --> 00:15:23,600
поэтому

381
00:15:23,600 --> 00:15:26,880
для модели слова-вектора,

382
00:15:26,880 --> 00:15:30,240
представленной в miklov в нашей статье в

383
00:15:30,240 --> 00:15:31,839
2013

384
00:15:31,839 --> 00:15:33,519
году, на самом деле это был не

385
00:15:33,519 --> 00:15:35,040
один

386
00:15:35,040 --> 00:15:38,240
алгоритм, а семейство алгоритмов,

387
00:15:38,240 --> 00:15:41,680
поэтому есть два основных  Варианты модели

388
00:15:41,680 --> 00:15:44,000
один назывался моделью пропуска грамматики,

389
00:15:44,000 --> 00:15:46,240
которая, как я вам объяснил

390
00:15:46,240 --> 00:15:48,480
,

391
00:15:49,600 --> 00:15:52,560
предсказывала положение внешних слов

392
00:15:52,560 --> 00:15:55,360
независимо от центрального слова

393
00:15:55,360 --> 00:15:57,519
в модели стиля мешка слов,

394
00:15:57,519 --> 00:15:59,519
другая называлась моделью непрерывного мешка

395
00:15:59,519 --> 00:16:02,560
слов sibo  и в этом вы

396
00:16:02,560 --> 00:16:04,560
предсказываете центральное слово из набора

397
00:16:04,560 --> 00:16:07,279
контекстных слов,

398
00:16:07,279 --> 00:16:10,160
оба из них дают схожие результаты,

399
00:16:10,160 --> 00:16:12,720
скипграмма более естественна во многих

400
00:16:12,720 --> 00:16:14,480
отношениях, так что это вроде того, к

401
00:16:14,480 --> 00:16:16,320
которому люди

402
00:16:16,320 --> 00:16:18,880
тяготеют в последующей работе,

403
00:16:18,880 --> 00:16:21,600
но потом, как  о том, как вы тренируете эту

404
00:16:21,600 --> 00:16:22,480
модель ...

405
00:16:22,480 --> 00:16:25,680
то, что я представил до сих пор, - это

406
00:16:25,680 --> 00:16:28,079
наивное уравнение softmax,

407
00:16:28,079 --> 00:16:29,680
которое

408
00:16:29,680 --> 00:16:32,320
является простым, но относительно дорогим

409
00:16:32,320 --> 00:16:35,759
методом обучения, и поэтому на самом деле это не то,

410
00:16:35,759 --> 00:16:38,079
что они предлагают использовать в вашей статье

411
00:16:38,079 --> 00:16:40,240
в статье, которую они предлагают использовать метод

412
00:16:40,240 --> 00:16:42,320
это называется отрицательной выборкой, поэтому

413
00:16:42,320 --> 00:16:45,839
вы иногда будете видеть аббревиатуру sgns,

414
00:16:45,839 --> 00:16:48,880
что означает пропустить грамм отрицательной выборки,

415
00:16:48,880 --> 00:16:52,079
поэтому позвольте мне сказать немного

416
00:16:52,079 --> 00:16:55,279
о том, что это такое, но на самом деле я

417
00:16:55,279 --> 00:16:56,800
делаю s  Модель cript gram с

418
00:16:56,800 --> 00:16:58,720
отрицательной выборкой также является частью

419
00:16:58,720 --> 00:17:00,480
домашнего задания, так что вы узнаете эту

420
00:17:00,480 --> 00:17:03,279
модель хорошо, так что суть в том, что если вы

421
00:17:03,279 --> 00:17:05,919
используете этот наивный softmax, вы знаете,

422
00:17:05,919 --> 00:17:08,240
хотя люди обычно используют этот наивный

423
00:17:08,240 --> 00:17:11,119
softmax в различных моделях нейронных сетей,

424
00:17:11,119 --> 00:17:13,839
которые  вычисление знаменателя

425
00:17:13,839 --> 00:17:16,160
довольно дорого, и это потому, что вам

426
00:17:16,160 --> 00:17:18,799
нужно перебирать каждое слово в

427
00:17:18,799 --> 00:17:21,599
словаре и вычислять

428
00:17:21,599 --> 00:17:23,439
эти скалярные произведения, поэтому, если у вас есть словарный запас в

429
00:17:23,439 --> 00:17:24,959
сто тысяч слов,

430
00:17:24,959 --> 00:17:26,000


431
00:17:26,000 --> 00:17:28,079
вам нужно сделать сто

432
00:17:28,079 --> 00:17:31,120
тысяч скалярных произведений, чтобы вычислить

433
00:17:31,120 --> 00:17:33,360
знаменатель, и это кажется

434
00:17:33,360 --> 00:17:36,160
немного стыдным, поэтому вместо этого

435
00:17:36,160 --> 00:17:38,880
идея отрицательной выборки заключается в том, что

436
00:17:38,880 --> 00:17:40,960
вместо использования

437
00:17:40,960 --> 00:17:45,600
этого мягкого максимума мы собираемся обучать

438
00:17:45,600 --> 00:17:48,559
модели бинарной логистической регрессии

439
00:17:48,559 --> 00:17:50,880
как для отряда, так

440
00:17:50,880 --> 00:17:54,559
и для истинной пары центрального слова

441
00:17:54,559 --> 00:17:56,720
и  контекстное слово

442
00:17:56,720 --> 00:17:58,160
против

443
00:17:58,160 --> 00:18:00,480
пар шума, где мы сохраняем истинное

444
00:18:00,480 --> 00:18:03,679
центральное слово, и мы просто случайным образом выбираем

445
00:18:03,679 --> 00:18:06,559
слова из словаря,

446
00:18:06,559 --> 00:18:09,280
чтобы, как представлено в документе,

447
00:18:09,280 --> 00:18:11,840
идея была такой, так что в целом, что w  Мы

448
00:18:11,840 --> 00:18:12,799
хотим

449
00:18:12,799 --> 00:18:15,600
оптимизировать по-

450
00:18:15,600 --> 00:18:18,160
прежнему среднее значение

451
00:18:18,160 --> 00:18:21,520
потерь для каждого отдельного центрального слова,

452
00:18:21,520 --> 00:18:23,360
но когда мы вычисляем потери для

453
00:18:23,360 --> 00:18:25,760
каждого конкретного центрального слова, мы

454
00:18:25,760 --> 00:18:28,720
собираемся вычислить, извините, потери для каждого

455
00:18:28,720 --> 00:18:30,160
конкретного центрального слова и каждого

456
00:18:30,160 --> 00:18:32,799
отдельного слова.  Мы собираемся

457
00:18:32,799 --> 00:18:35,520
взять скалярное произведение, как и раньше, из

458
00:18:35,520 --> 00:18:37,840
центрального слова

459
00:18:37,840 --> 00:18:40,640
и внешнего слова, и это своего рода

460
00:18:40,640 --> 00:18:43,440
основное количество, но теперь вместо использования

461
00:18:43,440 --> 00:18:45,600
этого внутри softmax мы собираемся пропустить

462
00:18:45,600 --> 00:18:46,960
его

463
00:18:46,960 --> 00:18:49,120
через логистическую функцию, которая иногда

464
00:18:49,120 --> 00:18:51,360
также часто также называется сигмоидной

465
00:18:51,360 --> 00:18:53,360
функцией, имя логистической является более

466
00:18:53,360 --> 00:18:55,520
точным, поэтому здесь эта функция,

467
00:18:55,520 --> 00:18:57,520
поэтому логистическая функция является удобной

468
00:18:57,520 --> 00:19:00,559
функцией, которая будет отображать любое действительное число

469
00:19:00,559 --> 00:19:02,320
с вероятностью

470
00:19:02,320 --> 00:19:05,360
между нулем и одним открытым интервалом, поэтому в

471
00:19:05,360 --> 00:19:09,120
основном, если скалярное произведение велико,

472
00:19:09,120 --> 00:19:11,520
логистика скалярного произведения будет

473
00:19:11,520 --> 00:19:13,039
практически

474
00:19:13,039 --> 00:19:16,559
приемлемой, поэтому мы хотим, чтобы она была большой, а

475
00:19:16,559 --> 00:19:19,679
затем, в среднем, мы

476
00:19:19,679 --> 00:19:22,160
хотели бы получить скалярное произведение между центральным

477
00:19:22,160 --> 00:19:24,799
словом и словами, которые мы оправдываем.  t выбрали

478
00:19:24,799 --> 00:19:27,679
случайным образом, т.е. они, скорее всего, на

479
00:19:27,679 --> 00:19:29,840
самом деле не встречались в контексте

480
00:19:29,840 --> 00:19:32,799
центрального слова, чтобы быть маленьким,

481
00:19:32,799 --> 00:19:35,840
и есть только одна небольшая хитрость,

482
00:19:35,840 --> 00:19:38,240
как это делается: эта

483
00:19:38,240 --> 00:19:40,080
сигмовидная функция

484
00:19:40,080 --> 00:19:43,520
является симметричной, и поэтому, если

485
00:19:43,520 --> 00:19:44,320


486
00:19:44,320 --> 00:19:46,320
мы хотим, чтобы

487
00:19:46,320 --> 00:19:49,120
эта вероятность была  small, мы можем

488
00:19:49,120 --> 00:19:52,000
взять отрицательное значение скалярного произведения, поэтому мы

489
00:19:52,000 --> 00:19:54,480
хотим, чтобы

490
00:19:54,480 --> 00:19:57,360
произведение скалярного произведения случайного слова

491
00:19:57,360 --> 00:20:00,720
в центральном слове было отрицательным числом

492
00:20:00,720 --> 00:20:02,799
, поэтому мы собираемся принять

493
00:20:02,799 --> 00:20:05,440
отрицание  это, а затем снова, как только

494
00:20:05,440 --> 00:20:07,440
мы пропустим это через сигмоид, мы хотели бы, чтобы

495
00:20:07,440 --> 00:20:08,720
большое число было

496
00:20:08,720 --> 00:20:10,640
хорошо, чтобы то, как они представляют

497
00:20:10,640 --> 00:20:13,360
вещи, на самом деле максимизирует это

498
00:20:13,360 --> 00:20:16,720
количество, но если я вернусь к тому, чтобы сделать его

499
00:20:16,720 --> 00:20:18,240
немного более похожим на то, как мы

500
00:20:18,240 --> 00:20:19,919
написал вещи, с

501
00:20:19,919 --> 00:20:22,000
которыми мы работали,

502
00:20:22,000 --> 00:20:25,120
минимизируя отрицательную логарифмическую вероятность

503
00:20:25,120 --> 00:20:26,960
um, так что

504
00:20:26,960 --> 00:20:29,520
это выглядит так, поэтому мы берем

505
00:20:29,520 --> 00:20:32,400
отрицательную логарифмическую вероятность

506
00:20:32,400 --> 00:20:35,520
этого сигмоида скалярного произведения um

507
00:20:35,520 --> 00:20:37,919
снова отрицательная логарифмическая вероятность, мы

508
00:20:37,919 --> 00:20:41,360
используем ту же точку отрицателя  продукт

509
00:20:41,360 --> 00:20:43,760
через  сигмоид, а затем мы собираемся вычислить

510
00:20:43,760 --> 00:20:46,840
это количество для небольшого

511
00:20:46,840 --> 00:20:49,360
числа брендов,

512
00:20:49,360 --> 00:20:53,760
мы k отрицательных образцов um, и насколько

513
00:20:53,760 --> 00:20:55,840
вероятно, что они будут выбирать слово, зависит

514
00:20:55,840 --> 00:20:59,120
от их вероятности и от того, где эта

515
00:20:59,120 --> 00:21:00,880
функция потерь будет

516
00:21:00,880 --> 00:21:04,640
минимизирована с учетом этого отрицания с помощью  делая

517
00:21:04,640 --> 00:21:07,760
эти скалярные произведения большими, а эти скалярные

518
00:21:07,760 --> 00:21:09,440
произведения

519
00:21:09,440 --> 00:21:12,799
уменьшают отрицательные,

520
00:21:14,000 --> 00:21:15,280
так

521
00:21:15,280 --> 00:21:16,960
что они просто еще

522
00:21:16,960 --> 00:21:19,039
один трюк, который они

523
00:21:19,039 --> 00:21:20,799
используют, на самом деле есть больше, чем один другой

524
00:21:20,799 --> 00:21:22,880
трюк, который используется в документе о дефектах слов,

525
00:21:22,880 --> 00:21:25,039
чтобы заставить его работать хорошо, но я

526
00:21:25,039 --> 00:21:26,799
только упомяну  одна из их других уловок,

527
00:21:26,799 --> 00:21:30,159
когда они выбирают слова, они

528
00:21:30,159 --> 00:21:33,919
не просто выбирают слова, основываясь

529
00:21:33,919 --> 00:21:37,120
на их вероятности появления

530
00:21:37,120 --> 00:21:40,240
в корпусе, или единообразно то, что они делают, -

531
00:21:40,240 --> 00:21:42,240
они начинают с того, что мы называем распределением слов по униграмме,

532
00:21:42,240 --> 00:21:45,679
так что  это то,

533
00:21:45,679 --> 00:21:48,240
как часто слова на самом деле встречаются в нашем

534
00:21:48,240 --> 00:21:50,799
большом корпусе, поэтому, если у вас есть корпус из миллиардов слов,

535
00:21:50,799 --> 00:21:54,559
и конкретное слово встречается

536
00:21:54,559 --> 00:21:57,200
в нем 90 раз, вы берете 90, деленное

537
00:21:57,200 --> 00:21:59,360
на миллиард, и это униграмма

538
00:21:59,360 --> 00:22:01,679
Вероятность слова, но

539
00:22:01,679 --> 00:22:02,880
затем

540
00:22:02,880 --> 00:22:04,400
они принимают это в

541
00:22:04,400 --> 00:22:06,720
степени трех четвертей и эффект

542
00:22:06,720 --> 00:22:08,799
этой степени в три четверти, который затем

543
00:22:08,799 --> 00:22:10,640
повторно нормализуется, чтобы получить распределение вероятности

544
00:22:10,640 --> 00:22:13,360
с z вроде того, что мы видели в

545
00:22:13,360 --> 00:22:15,520
прошлый раз с  мягкий максимум,

546
00:22:15,520 --> 00:22:18,159
взяв мощность в три четверти,

547
00:22:18,159 --> 00:22:20,960
которая имеет эффект сглаживания

548
00:22:20,960 --> 00:22:23,280
разницы между обычными и редкими словами,

549
00:22:23,280 --> 00:22:26,240
так что менее частые слова

550
00:22:26,240 --> 00:22:28,400
выбираются несколько чаще, но все же

551
00:22:28,400 --> 00:22:31,360
не так много, как

552
00:22:31,360 --> 00:22:33,520
если бы вы просто использовали что-то вроде  равномерное

553
00:22:33,520 --> 00:22:36,559
распределение по словарю,

554
00:22:36,559 --> 00:22:38,080
хорошо, так

555
00:22:38,080 --> 00:22:41,760
что это в основном все, что

556
00:22:41,760 --> 00:22:42,880
можно сказать

557
00:22:42,880 --> 00:22:45,120


558
00:22:45,120 --> 00:22:47,679
об основах того, как у нас есть этот очень

559
00:22:47,679 --> 00:22:49,039
простой

560
00:22:49,039 --> 00:22:51,120
алгоритм нейронной сети

561
00:22:51,120 --> 00:22:54,320
word2vec и как мы можем его обучить и

562
00:22:54,320 --> 00:22:56,480
изучить векторы слов,

563
00:22:56,480 --> 00:22:59,520
поэтому в следующем бите я хочу сделать

564
00:22:59,520 --> 00:23:01,600
сделайте шаг назад и скажите хорошо, вот

565
00:23:01,600 --> 00:23:04,080
алгоритм, который я вам показал, который отлично работает ...

566
00:23:04,080 --> 00:23:06,000


567
00:23:06,000 --> 00:23:08,320
что еще мы могли сделать и что

568
00:23:08,320 --> 00:23:11,840
мы можем сказать об этом, ммм и о первом

569
00:23:11,840 --> 00:23:14,320
том, что вы можете подумать

570
00:23:14,320 --> 00:23:15,919
вот этот

571
00:23:15,919 --> 00:23:18,240
забавный итеративный алгоритм,

572
00:23:18,240 --> 00:23:19,440
который дает вам

573
00:23:19,440 --> 00:23:21,120
векторы слов,

574
00:23:21,120 --> 00:23:25,200
вы знаете, если у нас много слов

575
00:23:25,200 --> 00:23:26,559
и корпус,

576
00:23:26,559 --> 00:23:28,320
кажется, более очевидная вещь, которую

577
00:23:28,320 --> 00:23:29,520
мы могли бы сделать,

578
00:23:29,520 --> 00:23:33,679
это просто посмотреть на количество слов, которые

579
00:23:33,679 --> 00:23:37,120
встречаются друг с другом  и построить

580
00:23:37,120 --> 00:23:39,280
матрицу подсчетов,

581
00:23:39,280 --> 00:23:41,600
матрицу совместной встречаемости, так что вот идея

582
00:23:41,600 --> 00:23:42,480


583
00:23:42,480 --> 00:23:45,200
матрицы совместной встречаемости, так что у меня есть

584
00:23:45,200 --> 00:23:47,520
крохотный корпус, мне нравится глубокое обучение, мне

585
00:23:47,520 --> 00:23:50,640
нравится nlp, мне нравится летать,

586
00:23:50,640 --> 00:23:53,440
и я могу определить размер окна, который я сделал своим

587
00:23:53,440 --> 00:23:56,720
окно просто размером один, чтобы

588
00:23:56,720 --> 00:23:57,840
упростить заполнение

589
00:23:57,840 --> 00:23:59,440
моей матрицы

590
00:23:59,440 --> 00:24:01,760
симметричной, как наш алгоритм word to back

591
00:24:01,760 --> 00:24:05,200
, и поэтому

592
00:24:05,200 --> 00:24:08,640
счетчики в этих ячейках просто показывают, как

593
00:24:08,640 --> 00:24:11,360
часто вещи, которые одновременно происходят в

594
00:24:11,360 --> 00:24:14,240
окне размера один, так что мне нравится,

595
00:24:14,240 --> 00:24:15,840
происходит дважды,

596
00:24:15,840 --> 00:24:18,080
поэтому  мы получаем двойки в этих ячейках, потому

597
00:24:18,080 --> 00:24:19,679
что это симметричное

598
00:24:19,679 --> 00:24:22,799
глубокое обучение происходит один, поэтому мы получаем

599
00:24:22,799 --> 00:24:25,279
один здесь, и многие другие вещи происходят с

600
00:24:25,279 --> 00:24:29,440
нулем, поэтому мы можем построить такую матрицу совместной встречаемости,

601
00:24:29,440 --> 00:24:32,400
как эта, и что на

602
00:24:32,400 --> 00:24:34,720
самом деле это дает нам

603
00:24:34,720 --> 00:24:36,640
представление слов как

604
00:24:36,640 --> 00:24:39,360
совместных  векторы возникновения  поэтому я могу взять

605
00:24:39,360 --> 00:24:40,559
слово i

606
00:24:40,559 --> 00:24:42,880
либо со строкой, либо с вектором-столбцом,

607
00:24:42,880 --> 00:24:45,840
поскольку оно симметрично, и сказать: хорошо, мое

608
00:24:45,840 --> 00:24:48,559
представление слова i

609
00:24:48,559 --> 00:24:50,400
- это вектор-строка,

610
00:24:50,400 --> 00:24:52,960
и это представление слова

611
00:24:52,960 --> 00:24:53,760
i,

612
00:24:53,760 --> 00:24:56,240
и я думаю, вы можете убедить

613
00:24:56,240 --> 00:24:58,720
себя, что  степень, в которой слова

614
00:24:58,720 --> 00:25:02,480
имеют схожее значение и использование, вы как

615
00:25:02,480 --> 00:25:04,080
бы ожидаете, что они будут иметь несколько

616
00:25:04,080 --> 00:25:06,480
схожие векторы, так что если бы у меня было

617
00:25:06,480 --> 00:25:09,200
слово u также в большем корпусе, вы

618
00:25:09,200 --> 00:25:11,679
могли бы ожидать, что i и u будут иметь похожие

619
00:25:11,679 --> 00:25:14,240
векторы, потому что мне нравится, что вы любите  Мне нравится, что

620
00:25:14,240 --> 00:25:17,279
вы и радость, вы бы увидели такие же

621
00:25:17,279 --> 00:25:20,159
возможности, эй, Крис, не могли бы вы

622
00:25:20,159 --> 00:25:22,080
продолжить поиск ответов на некоторые вопросы,

623
00:25:22,080 --> 00:25:23,120
конечно,

624
00:25:23,120 --> 00:25:24,720
хорошо, так что у нас есть несколько вопросов из

625
00:25:24,720 --> 00:25:26,320
негативных, э-э,

626
00:25:26,320 --> 00:25:28,799
слайдов с образцами негативных штампов,

627
00:25:28,799 --> 00:25:29,679


628
00:25:29,679 --> 00:25:31,120
в частности,

629
00:25:31,120 --> 00:25:32,080
э-э

630
00:25:32,080 --> 00:25:33,840
, как это может  вы даете некоторую интуицию

631
00:25:33,840 --> 00:25:35,039
для отрицательной выборки, что

632
00:25:35,039 --> 00:25:38,240
делает отрицательная выборка, и почему мы

633
00:25:38,240 --> 00:25:40,240
берем только один положительный пример, это

634
00:25:40,240 --> 00:25:41,440
два вопроса, на которые можно ответить

635
00:25:41,440 --> 00:25:42,400


636
00:25:42,400 --> 00:25:45,039
тандемно, хорошо, это хороший вопрос, хорошо  Я

637
00:25:45,039 --> 00:25:47,760
постараюсь дать больше интуиции, так

638
00:25:47,760 --> 00:25:50,720
что нужно разработать что-то вроде того, что

639
00:25:50,720 --> 00:25:53,520
softmax

640
00:25:53,840 --> 00:25:54,880


641
00:25:54,880 --> 00:25:57,120
сделал гораздо более

642
00:25:57,120 --> 00:25:59,360
эффективным способом,

643
00:25:59,360 --> 00:26:00,240


644
00:26:00,240 --> 00:26:02,799
так что в soft max well

645
00:26:02,799 --> 00:26:06,320
вы хотели дать высокую

646
00:26:06,320 --> 00:26:07,840


647
00:26:07,840 --> 00:26:10,720
вероятность предсказания контекста контекстному слову,

648
00:26:10,720 --> 00:26:12,480
которое действительно сделало  появляются с центральным

649
00:26:12,480 --> 00:26:16,880
словом um, и хорошо, как вы это делаете

650
00:26:16,880 --> 00:26:19,279
, если скалярное произведение между этими

651
00:26:19,279 --> 00:26:21,919
двумя словами должно быть как можно большим

652
00:26:21,919 --> 00:26:23,279
и является

653
00:26:23,279 --> 00:26:25,600
частью того, как, но вы знаете, что собираетесь

654
00:26:25,600 --> 00:26:28,320
быть чем-то большим, чем это, потому что

655
00:26:28,320 --> 00:26:30,880
в  знаменатель, вы также

656
00:26:30,880 --> 00:26:32,640
разрабатываете скалярное произведение с каждым другим

657
00:26:32,640 --> 00:26:34,960
словом в словаре, поэтому, помимо того,

658
00:26:34,960 --> 00:26:36,880


659
00:26:36,880 --> 00:26:39,679
что вы хотите, чтобы скалярное произведение с фактическим словом, которое вы видите в контексте, было

660
00:26:39,679 --> 00:26:44,159
большим, вы максимизируете свою вероятность,

661
00:26:44,159 --> 00:26:46,720
создавая скалярные произведения

662
00:26:46,720 --> 00:26:49,200
других слов  которые не были в контексте

663
00:26:49,200 --> 00:26:51,279
меньше, потому что это сужает ваш

664
00:26:51,279 --> 00:26:52,480
знаменатель,

665
00:26:52,480 --> 00:26:55,440
и, следовательно, у вас выходит большее

666
00:26:55,440 --> 00:26:57,760
число, и вы

667
00:26:57,760 --> 00:27:00,400
максимизируете потери, поэтому даже для softmax

668
00:27:00,400 --> 00:27:02,400
общая вещь, которую вы хотите сделать,

669
00:27:02,400 --> 00:27:04,720
чтобы максимизировать  ze это есть

670
00:27:04,720 --> 00:27:06,559
точечный продукт со словами действие

671
00:27:06,559 --> 00:27:08,240
контекстное большое

672
00:27:08,240 --> 00:27:10,000
точечное произведение со словами не в

673
00:27:10,000 --> 00:27:13,919
контексте быть маленьким, насколько это возможно,

674
00:27:13,919 --> 00:27:15,279
и, очевидно, вы должны

675
00:27:15,279 --> 00:27:17,120
усреднить это как можно лучше для всех

676
00:27:17,120 --> 00:27:18,640
видов разных контекстов, потому что

677
00:27:18,640 --> 00:27:20,320
иногда разные слова появляются в

678
00:27:20,320 --> 00:27:23,679
разных  контексты, очевидно,

679
00:27:23,679 --> 00:27:24,799
так

680
00:27:24,799 --> 00:27:26,640


681
00:27:26,640 --> 00:27:28,720
что отрицательная выборка - это способ,

682
00:27:28,720 --> 00:27:32,080
следовательно, попытаться максимизировать ту же

683
00:27:32,080 --> 00:27:35,760
цель, теперь вы знаете, что для

684
00:27:35,760 --> 00:27:36,880


685
00:27:36,880 --> 00:27:39,600
вас есть только один положительный термин, потому

686
00:27:39,600 --> 00:27:41,039
что вы на самом деле хотите использовать

687
00:27:41,039 --> 00:27:43,360
фактические данные, так что вы не ждете,

688
00:27:43,360 --> 00:27:46,000
желая  изобретать данные, чтобы для

689
00:27:46,000 --> 00:27:48,559
вычисления всего j мы

690
00:27:48,559 --> 00:27:51,440
действительно вычисляем это количество для каждого

691
00:27:51,440 --> 00:27:54,480
центрального слова и каждого контекстного слова, чтобы

692
00:27:54,480 --> 00:27:55,919
вы знали, что мы перебираем

693
00:27:55,919 --> 00:27:56,799
разные

694
00:27:56,799 --> 00:27:59,200
слова в контекстном окне, а затем

695
00:27:59,200 --> 00:28:00,960
мы перемещаемся по позициям в

696
00:28:00,960 --> 00:28:03,520
корпусе  поэтому мы делаем разные vcs, чтобы

697
00:28:03,520 --> 00:28:05,760
вы знали, что мы делаем это постепенно, но для

698
00:28:05,760 --> 00:28:07,600
одного конкретного центрального слова и одного

699
00:28:07,600 --> 00:28:10,320
конкретного контекстного слова у нас есть только один

700
00:28:10,320 --> 00:28:12,799
реальный фрагмент данных, который положителен  e, так

701
00:28:12,799 --> 00:28:14,960
что это все, что мы используем, потому что мы не знаем,

702
00:28:14,960 --> 00:28:17,520
какие еще слова

703
00:28:17,520 --> 00:28:20,159
следует считать положительными словами, теперь

704
00:28:20,159 --> 00:28:22,640
для отрицательных слов

705
00:28:22,640 --> 00:28:24,799
вы можете просто выбрать

706
00:28:24,799 --> 00:28:25,679
одно

707
00:28:25,679 --> 00:28:27,520
отрицательное слово, и

708
00:28:27,520 --> 00:28:29,679
это, вероятно, сработает, но если вы

709
00:28:29,679 --> 00:28:31,600
хотите немного

710
00:28:31,600 --> 00:28:34,640
лучшего, более стабильного

711
00:28:34,640 --> 00:28:36,640


712
00:28:36,640 --> 00:28:38,559
В общем, мы хотели бы, чтобы другие слова имели низкую

713
00:28:38,559 --> 00:28:40,799
вероятность; похоже, что вы

714
00:28:40,799 --> 00:28:43,120
могли бы получить более стабильные результаты,

715
00:28:43,120 --> 00:28:45,440
если вместо этого скажете: давайте возьмем

716
00:28:45,440 --> 00:28:48,399
10 или 15 образцов отрицательных слов, и

717
00:28:48,399 --> 00:28:51,679
действительно, это оказалось правдой,

718
00:28:51,679 --> 00:28:54,000
но  а для отрицательных слов

719
00:28:54,000 --> 00:28:55,760
легко выбрать любое количество случайных

720
00:28:55,760 --> 00:28:57,919
слов, которые вы хотите, и в этот момент это своего

721
00:28:57,919 --> 00:29:00,000
рода вероятностный аргумент:

722
00:29:00,000 --> 00:29:02,240
слова, которые вы отбираете,

723
00:29:02,240 --> 00:29:04,320
могут быть на самом деле не плохими словами, чтобы

724
00:29:04,320 --> 00:29:06,240
появиться в контексте,

725
00:29:06,240 --> 00:29:07,679
они могут быть другими  слова, которые

726
00:29:07,679 --> 00:29:10,720
находятся в контексте, но в 99,9 случаях

727
00:29:10,720 --> 00:29:13,200
они вряд ли

728
00:29:13,200 --> 00:29:15,279
будут встречаться в контексте

729
00:29:15,279 --> 00:29:18,399
, поэтому они хороши для использования, и да,

730
00:29:18,399 --> 00:29:21,440
вы выбираете только 10 или 15 из них, но

731
00:29:21,440 --> 00:29:25,039
этого достаточно, чтобы добиться прогресса  потому

732
00:29:25,039 --> 00:29:27,279
что центральное слово будет появляться в

733
00:29:27,279 --> 00:29:29,679
других случаях, и когда это произойдет, вы будете

734
00:29:29,679 --> 00:29:32,080
пробовать разные слова здесь, чтобы

735
00:29:32,080 --> 00:29:34,000
вы постепенно пробовали разные

736
00:29:34,000 --> 00:29:36,000
части пространства и начинали узнавать,

737
00:29:36,000 --> 00:29:39,840
что у нас была эта матрица совместной встречаемости, и она

738
00:29:39,840 --> 00:29:43,600
дает представление  слов как

739
00:29:43,600 --> 00:29:46,320
векторов совместной встречаемости

740
00:29:46,320 --> 00:29:49,600
и еще одно замечание о том, что я имею в виду,

741
00:29:49,600 --> 00:29:51,520
что на самом деле существует два способа, которыми

742
00:29:51,520 --> 00:29:53,200
люди обычно делают эти матрицы совместной встречаемости,

743
00:29:53,200 --> 00:29:54,399


744
00:29:54,399 --> 00:29:56,159
одна из которых соответствует тому, что мы уже видели

745
00:29:56,159 --> 00:29:58,640
, когда вы используете окно вокруг

746
00:29:58,640 --> 00:29:59,840
слова,

747
00:29:59,840 --> 00:30:02,480
которое является  похоже на слово в век,

748
00:30:02,480 --> 00:30:04,240
и это позволяет вам уловить некоторую

749
00:30:04,240 --> 00:30:06,320
локальность и некоторую

750
00:30:06,320 --> 00:30:08,960
синтаксическую и семантическую близость, которая

751
00:30:08,960 --> 00:30:11,520
более детализирована, в отличие от того, как

752
00:30:11,520 --> 00:30:14,520


753
00:30:15,760 --> 00:30:17,679
часто возникают эти коматричные заболевания, заключающиеся в том, что

754
00:30:17,679 --> 00:30:19,840
обычно документы имеют некоторую структуру,

755
00:30:19,840 --> 00:30:22,320
будь то абзацы или  эм, просто

756
00:30:22,320 --> 00:30:24,880
реальные веб-страницы сортируют размер документов,

757
00:30:24,880 --> 00:30:26,880
так что вы можете просто сделать размер своего окна

758
00:30:26,880 --> 00:30:30,080
абзацем или целой веб-страницей и

759
00:30:30,080 --> 00:30:32,480
подсчитать их совместное появление, и

760
00:30:32,480 --> 00:30:34,399
это своего рода мет  od, который часто

761
00:30:34,399 --> 00:30:36,960
используется при поиске информации в таких методах,

762
00:30:36,960 --> 00:30:40,720
как скрытый семантический анализ,

763
00:30:40,720 --> 00:30:42,640
хорошо, поэтому вопрос в

764
00:30:42,640 --> 00:30:46,320
том, хороши ли эти типы векторов подсчета

765
00:30:46,320 --> 00:30:47,840
слов

766
00:30:47,840 --> 00:30:52,000
для использования хорошо, что

767
00:30:52,000 --> 00:30:53,760
люди их использовали, они не

768
00:30:53,760 --> 00:30:54,799
ужасны,

769
00:30:54,799 --> 00:30:57,200
но у них есть определенные проблемы

770
00:30:57,200 --> 00:30:59,519
такого рода проблемы  что у них есть

771
00:30:59,519 --> 00:31:02,720
ну во-первых, они огромные, хотя и очень

772
00:31:02,720 --> 00:31:04,480
редкие, так что это то, что я сказал

773
00:31:04,480 --> 00:31:06,960
раньше, если бы у нас был словарный запас в

774
00:31:06,960 --> 00:31:10,080
полмиллиона слов, тогда у нас есть

775
00:31:10,080 --> 00:31:13,039
полмиллион мерный вектор для каждого слова,

776
00:31:13,039 --> 00:31:16,480
который намного больше, чем  слово

777
00:31:16,480 --> 00:31:19,039
векторы, которые мы обычно используем

778
00:31:19,039 --> 00:31:19,840
um,

779
00:31:19,840 --> 00:31:22,559
и это также означает, что, поскольку

780
00:31:22,559 --> 00:31:24,320
у нас есть эти очень многомерные

781
00:31:24,320 --> 00:31:27,840
векторы um, у нас много

782
00:31:27,840 --> 00:31:31,039
разреженности и много случайности, поэтому

783
00:31:31,039 --> 00:31:33,840
результаты, которые вы получаете, имеют тенденцию быть более шумными

784
00:31:33,840 --> 00:31:35,519
и менее надежными в зависимости от того, что

785
00:31:35,519 --> 00:31:38,559
конкретный материал был в корпусе,

786
00:31:38,559 --> 00:31:41,440
и поэтому в целом люди обнаружили, что

787
00:31:41,440 --> 00:31:43,840
вы можете получить гораздо лучшие результаты,

788
00:31:43,840 --> 00:31:46,880
работая с векторами низкой размерности,

789
00:31:46,880 --> 00:31:50,240
поэтому идея состоит в том, что мы можем хранить большую

790
00:31:50,240 --> 00:31:52,559
часть  важная информация о

791
00:31:52,559 --> 00:31:55,200
распределении слов в контексте

792
00:31:55,200 --> 00:31:57,919
других слов в фиксированном небольшом количестве

793
00:31:57,919 --> 00:32:00,880
измерений, дающих плотный вектор,

794
00:32:00,880 --> 00:32:02,960
и на практике

795
00:32:02,960 --> 00:32:05,360
размерность используемых векторов обычно находится

796
00:32:05,360 --> 00:32:08,159
где-то между 25 и тысячей,

797
00:32:08,159 --> 00:32:11,679
и поэтому в этот момент мы  нужно использовать два,

798
00:32:11,679 --> 00:32:13,919
нам нужно использовать какой-то способ уменьшить

799
00:32:13,919 --> 00:32:16,159
размерность наших

800
00:32:16,159 --> 00:32:19,120
векторов совпадения числа,

801
00:32:19,679 --> 00:32:20,640
поэтому,

802
00:32:20,640 --> 00:32:22,960
если у вас есть хорошая память

803
00:32:22,960 --> 00:32:26,159
из класса линейной алгебры,

804
00:32:26,159 --> 00:32:28,559
вы, надеюсь, видели разложение по сингулярным значениям,

805
00:32:28,559 --> 00:32:30,799
и

806
00:32:30,799 --> 00:32:33,360
оно имеет различные математические свойства,

807
00:32:33,360 --> 00:32:36,080
которые я не  Мы собираемся поговорить здесь

808
00:32:36,080 --> 00:32:39,440
о проекции единственного сингулярного значения,

809
00:32:39,440 --> 00:32:41,679
дающей вам оптимальный способ в соответствии с

810
00:32:41,679 --> 00:32:44,000
определенным определением оптимальности

811
00:32:44,000 --> 00:32:47,039
создания матрицы уменьшенной размерности,

812
00:32:47,039 --> 00:32:50,159
которая максимально или, к

813
00:32:50,159 --> 00:32:53,200
сожалению, пара матриц, которая максимально

814
00:32:53,200 --> 00:32:55,200
хорошо позволяет вам восстановить исходную

815
00:32:55,200 --> 00:32:56,399
матрицу,

816
00:32:56,399 --> 00:32:58,559
но идею сингулярного значения

817
00:32:58,559 --> 00:33:00,960
разложение - вы можете взять любую матрицу,

818
00:33:00,960 --> 00:33:02,720
такую как наша

819
00:33:02,720 --> 00:33:05,559
матрица подсчета, и

820
00:33:05,559 --> 00:33:08,240
разложить ее на

821
00:33:08,240 --> 00:33:10,960
три матрицы  es

822
00:33:10,960 --> 00:33:14,240
ua диагональная матрица sigma и av.

823
00:33:14,240 --> 00:33:16,240


824
00:33:16,240 --> 00:33:17,679


825
00:33:17,679 --> 00:33:20,799


826
00:33:20,799 --> 00:33:24,480


827
00:33:24,480 --> 00:33:26,799


828
00:33:26,799 --> 00:33:29,039


829
00:33:29,039 --> 00:33:32,720


830
00:33:32,720 --> 00:33:35,600


831
00:33:35,600 --> 00:33:38,720
вы хотите получить меньшие

832
00:33:38,720 --> 00:33:41,120
размерные представления,

833
00:33:41,120 --> 00:33:43,120
что вы делаете, это пользуетесь тем

834
00:33:43,120 --> 00:33:47,200
фактом, что сингулярные значения внутри

835
00:33:47,200 --> 00:33:50,399
диагональной сигма-матрицы упорядочены от

836
00:33:50,399 --> 00:33:53,440
наибольшего вниз к наименьшему, поэтому мы

837
00:33:53,440 --> 00:33:58,080
можем просто удалить большую часть

838
00:33:58,080 --> 00:34:01,519
матрицы удаления  out некоторые сингулярные значения,

839
00:34:01,519 --> 00:34:04,000
что фактически означает, что в этом

840
00:34:04,000 --> 00:34:05,279
произведении

841
00:34:05,279 --> 00:34:06,480
сумма u

842
00:34:06,480 --> 00:34:09,760
и сумма v также не используются, и

843
00:34:09,760 --> 00:34:12,560
поэтому в результате этого мы получаем

844
00:34:12,560 --> 00:34:15,280
представления um более низкой размерности

845
00:34:15,280 --> 00:34:16,079


846
00:34:16,079 --> 00:34:17,199
для

847
00:34:17,199 --> 00:34:19,359
наших слов, если мы хотим иметь векторы слов

848
00:34:19,359 --> 00:34:20,399


849
00:34:20,399 --> 00:34:24,000
которые по-прежнему выполняют максимально хорошую работу в

850
00:34:24,000 --> 00:34:26,879
пределах заданной размерности,

851
00:34:26,879 --> 00:34:30,639
позволяя вам восстановить исходную

852
00:34:30,639 --> 00:34:33,760
матрицу совместной встречаемости,

853
00:34:33,760 --> 00:34:36,960
поэтому из фона линейной алгебры um

854
00:34:36,960 --> 00:34:40,239
это  очевидная вещь, которую нужно использовать,

855
00:34:40,239 --> 00:34:42,399
так как это

856
00:34:42,399 --> 00:34:45,040
хорошо работает, если вы просто создаете

857
00:34:45,040 --> 00:34:46,000


858
00:34:46,000 --> 00:34:48,800
матрицу совпадения необработанных подсчетов

859
00:34:48,800 --> 00:34:52,079
и запускаете svd на этом и пытаетесь использовать

860
00:34:52,079 --> 00:34:55,119
их в качестве векторов слов, это на самом деле работает

861
00:34:55,119 --> 00:34:57,119
плохо, и

862
00:34:57,119 --> 00:34:59,040
это работает плохо,

863
00:34:59,040 --> 00:35:00,960
потому что, если вы попадете в  математические

864
00:35:00,960 --> 00:35:03,280
предположения svd, которые вы ожидаете

865
00:35:03,280 --> 00:35:04,079
иметь

866
00:35:04,079 --> 00:35:08,079
эти нормально распределенные ошибки, и

867
00:35:08,079 --> 00:35:10,720
то, что вы получаете с подсчетом слов,

868
00:35:10,720 --> 00:35:14,160
выглядело совсем не так,

869
00:35:14,880 --> 00:35:16,480
как что-то нормальное, чего вы не делали,

870
00:35:16,480 --> 00:35:18,640
потому что у вас есть чрезвычайно общие

871
00:35:18,640 --> 00:35:21,839
слова, такие как артур и и и у вас

872
00:35:21,839 --> 00:35:24,640
очень большой  количество редких слов, так что

873
00:35:24,640 --> 00:35:26,640
это не очень хорошо работает, но вы действительно

874
00:35:26,640 --> 00:35:29,599
получаете что-то, что работает намного лучше,

875
00:35:29,599 --> 00:35:32,480
если вы масштабируете счетчики в ячейках,

876
00:35:32,480 --> 00:35:33,200
поэтому

877
00:35:33,200 --> 00:35:35,200
для решения этой проблемы очень

878
00:35:35,200 --> 00:35:37,200
частых слов есть некоторые вещи, которые мы

879
00:35:37,200 --> 00:35:39,760
можем сделать, мы могли бы просто  возьмите журнал

880
00:35:39,760 --> 00:35:41,200
необработанных подсчетов,

881
00:35:41,200 --> 00:35:44,480
мы могли бы как бы ограничить максимальное количество, которое

882
00:35:44,480 --> 00:35:46,960
мы могли бы отбросить функциональные слова,

883
00:35:46,960 --> 00:35:50,240
и любая из этих идей позволит вам

884
00:35:50,240 --> 00:35:52,960
построить, а затем получить матрицу совместной встречаемости,

885
00:35:52,960 --> 00:35:55,280
которая станет более полезной  векторов слов

886
00:35:55,280 --> 00:35:58,400
от запуска чего-то вроде svd,

887
00:35:58,400 --> 00:36:00,480
и действительно, такие модели были

888
00:36:00,480 --> 00:36:04,200
исследованы в 1990-х и в

889
00:36:04,200 --> 00:36:08,560
2000-х, и, в частности, гм Дуг Роди

890
00:36:08,560 --> 00:36:11,119
исследовал ряд этих идей, а также то,

891
00:36:11,119 --> 00:36:14,160
как улучшить матрицу совместной встречаемости

892
00:36:14,160 --> 00:36:16,640
в модели, которую он  построенный, который назывался

893
00:36:16,640 --> 00:36:17,839
Коля,

894
00:36:17,839 --> 00:36:21,359
и вы знаете, что на самом деле в его модели Коля

895
00:36:21,359 --> 00:36:22,560


896
00:36:22,560 --> 00:36:26,560
он заметил тот факт, что вы можете

897
00:36:26,560 --> 00:36:29,440
получить такие же

898
00:36:29,440 --> 00:36:31,680
линейные компоненты, которые имеют семантические

899
00:36:31,680 --> 00:36:34,960
компоненты, которые мы видели вчера, когда

900
00:36:34,960 --> 00:36:36,320
говорили об

901
00:36:36,320 --> 00:36:39,760
аналогиях, так что, например, это

902
00:36:39,760 --> 00:36:42,480
фигура из его статьи и  вы можете видеть,

903
00:36:42,480 --> 00:36:45,680
что у нас, кажется, есть компонент значения,

904
00:36:45,680 --> 00:36:49,280
идущий от глагола к человеку, который произносит

905
00:36:49,280 --> 00:36:51,920
этот глагол, так побуждай к плаванию, чтобы

906
00:36:51,920 --> 00:36:54,320
пловец учил учителя жениться на

907
00:36:54,320 --> 00:36:56,880
священнике, и что эти

908
00:36:56,880 --> 00:36:59,920
компоненты вектора не идеальны,

909
00:36:59,920 --> 00:37:01,839
но примерно

910
00:37:01,839 --> 00:37:04,800
параллельны и примерно  того же размера, и

911
00:37:04,800 --> 00:37:07,280
поэтому у нас есть компонент значения,

912
00:37:07,280 --> 00:37:09,920
который мы могли бы добавить к другому слову

913
00:37:09,920 --> 00:37:12,240
так же, как мы делали ранее, для

914
00:37:12,240 --> 00:37:15,520
аналогий мы могли бы сказать драйверы водителю

915
00:37:15,520 --> 00:37:19,440
как mari  это то, к чему, и мы добавили бы на этот

916
00:37:19,440 --> 00:37:21,680
экранный векторный компонент, который примерно

917
00:37:21,680 --> 00:37:23,920
такой же, как этот, и мы бы сказали о

918
00:37:23,920 --> 00:37:26,240
священник, чтобы это пространство могло действительно

919
00:37:26,240 --> 00:37:28,800
получить некоторые

920
00:37:28,800 --> 00:37:30,560


921
00:37:30,560 --> 00:37:32,960
аналогии векторов слов,

922
00:37:32,960 --> 00:37:34,560
и поэтому это показалось

923
00:37:34,560 --> 00:37:37,760
нам действительно интересным  Примерно в то время

924
00:37:37,760 --> 00:37:40,320
word2vec появилось из-за желания

925
00:37:40,320 --> 00:37:42,160
лучше понять, что делает алгоритм итеративного

926
00:37:42,160 --> 00:37:44,880
обновления word2vec

927
00:37:44,880 --> 00:37:47,359
и как он соотносится с

928
00:37:47,359 --> 00:37:49,520
методами, более основанными на линейной алгебре, которые были

929
00:37:49,520 --> 00:37:50,400


930
00:37:50,400 --> 00:37:52,960
исследованы пару десятилетий назад, и

931
00:37:52,960 --> 00:37:55,520
поэтому для следующего бита i  хочу рассказать вам

932
00:37:55,520 --> 00:37:57,760
немного об алгоритме перчаток,

933
00:37:57,760 --> 00:37:59,359
который представлял собой алгоритм

934
00:37:59,359 --> 00:38:02,400
для векторов слов, который был

935
00:38:02,400 --> 00:38:05,119
разработан мной и Джеффри Пеннингтоном Ричардом Сохером

936
00:38:05,119 --> 00:38:07,119
в 2014 г.

937
00:38:07,119 --> 00:38:09,359


938
00:38:09,359 --> 00:38:12,079


939
00:38:12,079 --> 00:38:12,800


940
00:38:12,800 --> 00:38:14,800


941
00:38:14,800 --> 00:38:17,119
-матрицы встречаемости, такие как lsa и

942
00:38:17,119 --> 00:38:20,320
coles, с такими моделями, как skip grand

943
00:38:20,320 --> 00:38:22,960
sibo и другие их друзья, которые были

944
00:38:22,960 --> 00:38:26,400
алгоритмами итеративного нейронного обновления, так

945
00:38:26,400 --> 00:38:28,720
что, с одной стороны, вы

946
00:38:28,720 --> 00:38:30,720
знаете линейный a  На самом деле

947
00:38:30,720 --> 00:38:33,599
казалось, что у методов lgebra есть преимущества для быстрого

948
00:38:33,599 --> 00:38:34,960
обучения и эффективного использования

949
00:38:34,960 --> 00:38:36,400
статистики,

950
00:38:36,400 --> 00:38:37,760
но,

951
00:38:37,760 --> 00:38:39,119
несмотря на то, что проводилась работа по

952
00:38:39,119 --> 00:38:40,320
выявлению

953
00:38:40,320 --> 00:38:44,079
сходства слов с ними в целом,

954
00:38:44,079 --> 00:38:46,000
результаты были не такими хорошими, возможно,

955
00:38:46,000 --> 00:38:47,760
из-за непропорционального значения,

956
00:38:47,760 --> 00:38:49,920
придаваемого крупным счетам в основном.

957
00:38:49,920 --> 00:38:51,200
и наоборот,

958
00:38:51,200 --> 00:38:52,079


959
00:38:52,079 --> 00:38:53,920
модели,

960
00:38:53,920 --> 00:38:57,040
нейронные модели, кажется, что

961
00:38:57,040 --> 00:38:58,480
если вы просто выполняете эти

962
00:38:58,480 --> 00:39:01,280
обновления градиента в окнах, вы каким-то образом

963
00:39:01,280 --> 00:39:03,599
неэффективно используете статистику по сравнению с

964
00:39:03,599 --> 00:39:05,520
матрицей совместной встречаемости,

965
00:39:05,520 --> 00:39:07,520
но с другой стороны,

966
00:39:07,520 --> 00:39:09,839
на самом деле ее легче масштабировать до очень

967
00:39:09,839 --> 00:39:12,400
больших  корпус, обменивая

968
00:39:12,400 --> 00:39:15,680
время на пространство, но

969
00:39:15,680 --> 00:39:18,000
в то время казалось, что новые

970
00:39:18,000 --> 00:39:20,079
методы просто лучше работают для людей,

971
00:39:20,079 --> 00:39:22,320
что они повышают

972
00:39:22,320 --> 00:39:24,240
производительность многих задач не только из-за сходства слов,

973
00:39:24,240 --> 00:39:26,480
и что они могут улавливать

974
00:39:26,480 --> 00:39:29,760
сложные шаблоны, такие как аналогии

975
00:39:29,760 --> 00:39:33,280
, выходящие за рамки слов  сходство,

976
00:39:33,280 --> 00:39:35,200
и поэтому мы хотели

977
00:39:35,200 --> 00:39:37,839
немного больше понять, что вам нужно,

978
00:39:37,839 --> 00:39:40,839
какие свойства вам нужны  чтобы эти

979
00:39:40,839 --> 00:39:44,720
аналогии работали, как я показал в прошлый раз,

980
00:39:44,720 --> 00:39:45,920
и поэтому

981
00:39:45,920 --> 00:39:48,880
мы поняли, что если вы хотите

982
00:39:48,880 --> 00:39:50,240


983
00:39:50,240 --> 00:39:53,520
иметь такого рода векторные

984
00:39:53,520 --> 00:39:55,359
вычитания и

985
00:39:55,359 --> 00:39:56,880
сложения,

986
00:39:56,880 --> 00:39:59,920
работающие по аналогии, свойство, которое

987
00:39:59,920 --> 00:40:00,880
вы

988
00:40:00,880 --> 00:40:04,480
хотите, предназначено для обозначения компонентов, поэтому

989
00:40:04,480 --> 00:40:06,960
Смысловой компонент - это что-то вроде

990
00:40:06,960 --> 00:40:08,720
перехода от

991
00:40:08,720 --> 00:40:12,800
мужчины к королеве-женщине к королю или

992
00:40:12,800 --> 00:40:15,040
от

993
00:40:16,079 --> 00:40:19,520
его возраста и от грузовика к водителю, эм,

994
00:40:19,520 --> 00:40:21,440
эти значимые компоненты

995
00:40:21,440 --> 00:40:24,000
должны быть представлены как отношения

996
00:40:24,000 --> 00:40:25,920
вероятностей совместного появления,

997
00:40:25,920 --> 00:40:29,200
поэтому вот пример, который показывает, что

998
00:40:29,200 --> 00:40:32,079
хорошо, поэтому предположим, что значимый компонент

999
00:40:32,079 --> 00:40:35,040
то, что мы хотим получить, -

1000
00:40:35,040 --> 00:40:38,480
это спектр от твердого тела к газу, так как в

1001
00:40:38,480 --> 00:40:39,520
физике

1002
00:40:39,520 --> 00:40:40,319


1003
00:40:40,319 --> 00:40:41,920
вы могли бы подумать, что

1004
00:40:41,920 --> 00:40:44,560
вы можете получить твердую его часть,

1005
00:40:44,560 --> 00:40:47,200
возможно, сказав, встречается ли слово вместе

1006
00:40:47,200 --> 00:40:49,839
со льдом, а слово твердое тело встречается со

1007
00:40:49,839 --> 00:40:52,480
льдом, поэтому  это выглядит обнадеживающе, и газ

1008
00:40:52,480 --> 00:40:54,400
не возникает со льдом, так что это

1009
00:40:54,400 --> 00:40:56,960
выглядит обнадеживающим, но проблема в том, что

1010
00:40:56,960 --> 00:40:59,680
слово вода также часто встречается со

1011
00:40:59,680 --> 00:41:01,760
льдом, и если вы просто возьмете какое-то другое

1012
00:41:01,760 --> 00:41:04,079
случайное слово, например слово случайный, это

1013
00:41:04,079 --> 00:41:06,400
про  Возможно, со льдом не так

1014
00:41:06,400 --> 00:41:07,839
много,

1015
00:41:07,839 --> 00:41:10,640
по контрасту, если вы посмотрите на

1016
00:41:10,640 --> 00:41:13,599
слова, встречающиеся вместе с паром,

1017
00:41:13,599 --> 00:41:15,920
твердое тело не будет встречаться с паром много, но

1018
00:41:15,920 --> 00:41:17,119
газ будет,

1019
00:41:17,119 --> 00:41:19,680
но вода будет снова, и случайное значение будет

1020
00:41:19,680 --> 00:41:20,720
маленьким,

1021
00:41:20,720 --> 00:41:22,720
чтобы получить значимый компонент, который мы

1022
00:41:22,720 --> 00:41:24,560
хотим  При переходе от

1023
00:41:24,560 --> 00:41:27,200
газа к твердому телу действительно

1024
00:41:27,200 --> 00:41:29,839
полезно посмотреть на соотношение этих

1025
00:41:29,839 --> 00:41:32,480
вероятностей совместного возникновения,

1026
00:41:32,480 --> 00:41:35,280
потому что тогда мы получаем спектр

1027
00:41:35,280 --> 00:41:39,040
от большого к малому между твердым телом и газом,

1028
00:41:39,040 --> 00:41:41,520
тогда как для воды в случайном слове он в

1029
00:41:41,520 --> 00:41:45,119
основном сокращает и дает вам один

1030
00:41:45,119 --> 00:41:47,599
эм, я просто записал эти числа, но если

1031
00:41:47,599 --> 00:41:48,560
вы

1032
00:41:48,560 --> 00:41:51,280
пересчитаете их в большом корпусе, это в

1033
00:41:51,280 --> 00:41:53,440
основном то, что вы получите, так что вот

1034
00:41:53,440 --> 00:41:57,040
фактические вероятности совпадения и

1035
00:41:57,040 --> 00:41:58,240
что для

1036
00:41:58,240 --> 00:42:00,160
воды и моего случайного слова, которое

1037
00:42:00,160 --> 00:42:03,040
здесь было модным, это примерно один

1038
00:42:03,040 --> 00:42:04,960
мкм, тогда как для

1039
00:42:04,960 --> 00:42:07,200
отношение

1040
00:42:07,200 --> 00:42:09,920
вероятности совместного присутствия твердого тела

1041
00:42:09,920 --> 00:42:13,680
со льдом или паром составляет около десяти, а четыре

1042
00:42:13,680 --> 00:42:14,640
предполагают,

1043
00:42:14,640 --> 00:42:16,400
что это около десятой части,

1044
00:42:16,400 --> 00:42:18,400
так

1045
00:42:18,400 --> 00:42:19,920
как мы можем

1046
00:42:19,920 --> 00:42:22,560
зафиксировать эти отношения

1047
00:42:22,560 --> 00:42:25,040
вероятностей соекон как компоненты линейного значения,

1048
00:42:25,040 --> 00:42:27,520
чтобы  в нашем векторном

1049
00:42:27,520 --> 00:42:30,240
пространстве слов мы можем просто складывать и вычитать

1050
00:42:30,240 --> 00:42:32,400
компоненты линейного значения,

1051
00:42:32,400 --> 00:42:33,280


1052
00:42:33,280 --> 00:42:35,680
это похоже на то, как мы можем добиться

1053
00:42:35,680 --> 00:42:38,560
этого, если мы построим журнал по линейной

1054
00:42:38,560 --> 00:42:41,680
модели, так что скалярное произведение между

1055
00:42:41,680 --> 00:42:43,920
двумя векторами слов

1056
00:42:43,920 --> 00:42:46,480
пытается аппроксимировать журнал

1057
00:42:46,480 --> 00:42:48,640
вероятности  совпадения,

1058
00:42:48,640 --> 00:42:51,440
поэтому, если вы это сделаете, вы получите это

1059
00:42:51,440 --> 00:42:53,200
свойство, заключающееся в

1060
00:42:53,200 --> 00:42:55,119
том, что

1061
00:42:55,119 --> 00:42:58,319
разница между двумя векторами,

1062
00:42:58,319 --> 00:43:00,960
его сходство с другим словом

1063
00:43:00,960 --> 00:43:02,960
соответствует логарифму

1064
00:43:02,960 --> 00:43:05,520
отношения вероятностей, показанному на предыдущем

1065
00:43:05,520 --> 00:43:06,880
слайде,

1066
00:43:06,880 --> 00:43:10,640
поэтому модель перчатки хотела попытаться

1067
00:43:10,640 --> 00:43:11,599


1068
00:43:11,599 --> 00:43:14,560
объединить мышление  между

1069
00:43:14,560 --> 00:43:17,599
матричными моделями совместной встречаемости и

1070
00:43:17,599 --> 00:43:19,200
нейронными моделями

1071
00:43:19,200 --> 00:43:20,000
,

1072
00:43:20,000 --> 00:43:22,000
поскольку они в некотором роде похожи на более новую

1073
00:43:22,000 --> 00:43:25,839
модель, но фактически вычисляются поверх

1074
00:43:25,839 --> 00:43:26,800


1075
00:43:26,800 --> 00:43:29,200
текущего количества матриц,

1076
00:43:29,200 --> 00:43:32,640
поэтому у нас была явная функция потерь,

1077
00:43:32,640 --> 00:43:35,040
а наша явная функция потерь

1078
00:43:35,040 --> 00:43:38,160
заключается в том, что мы хотели скалярное произведение  чтобы быть

1079
00:43:38,160 --> 00:43:41,760
похожим на журнал совпадений, который

1080
00:43:41,760 --> 00:43:43,839
мы на самом деле добавили в некоторые термины смещения

1081
00:43:43,839 --> 00:43:45,280
здесь, но я проигнорирую их на

1082
00:43:45,280 --> 00:43:48,240
данный момент, и мы хотели не иметь очень

1083
00:43:48,240 --> 00:43:51,920
общего слова  s доминируют, и поэтому мы сохранили

1084
00:43:51,920 --> 00:43:54,560
эффект большого количества слов,

1085
00:43:54,560 --> 00:43:57,599
используя эту функцию f, которая показана здесь,

1086
00:43:57,599 --> 00:43:58,880
а затем мы могли

1087
00:43:58,880 --> 00:44:01,920
оптимизировать эту функцию j

1088
00:44:01,920 --> 00:44:04,800
непосредственно на

1089
00:44:04,800 --> 00:44:06,960
матрице подсчета совпадений, что дало нам быстрое

1090
00:44:06,960 --> 00:44:10,640
обучение, масштабируемое до огромных корпусов,

1091
00:44:10,640 --> 00:44:13,599
и поэтому этот алгоритм  работал

1092
00:44:13,599 --> 00:44:15,040
очень хорошо,

1093
00:44:15,040 --> 00:44:16,640
поэтому, если вы спросите

1094
00:44:16,640 --> 00:44:18,480
, запускаете ли вы этот алгоритм, спросите, какие

1095
00:44:18,480 --> 00:44:21,280
слова ближе всего к лягушке, вы получите лягушку,

1096
00:44:21,280 --> 00:44:23,520
жабу, а затем вы получите несколько сложных

1097
00:44:23,520 --> 00:44:25,920
слов, но оказывается, что все они

1098
00:44:25,920 --> 00:44:28,480
лягушки, ммм, пока вы не перейдете к ящерицам,

1099
00:44:28,480 --> 00:44:31,280
так что латоя такая милая  древесная лягушка,

1100
00:44:31,280 --> 00:44:32,079
гм,

1101
00:44:32,079 --> 00:44:34,079
и так что это на самом деле, казалось, сработало

1102
00:44:34,079 --> 00:44:35,839
довольно хорошо,

1103
00:44:35,839 --> 00:44:38,640
насколько хорошо это сработало, гм, чтобы обсудить

1104
00:44:38,640 --> 00:44:40,800
это еще немного, теперь я хочу

1105
00:44:40,800 --> 00:44:43,200
кое-что сказать о том, как мы оцениваем векторы слов,

1106
00:44:43,200 --> 00:44:45,680


1107
00:44:45,680 --> 00:44:47,040
годны ли мы до этого для

1108
00:44:47,040 --> 00:44:50,040
вопросов, которые

1109
00:44:50,079 --> 00:44:52,079
мы  У меня есть несколько вопросов, а что вы имеете в

1110
00:44:52,079 --> 00:44:54,000
виду, говоря о неэффективном использовании статистики

1111
00:44:54,000 --> 00:44:56,000
в качестве мошенничества для пропуска грамма,

1112
00:44:56,000 --> 00:44:56,800


1113
00:44:56,800 --> 00:44:58,400
я имею в виду,

1114
00:44:58,400 --> 00:44:59,760
что

1115
00:44:59,760 --> 00:45:00,880
вы

1116
00:45:00,880 --> 00:45:05,040
знаете слово, чтобы убрать, вы просто знаете,

1117
00:45:05,040 --> 00:45:08,560
глядя на одно центральное слово за раз и

1118
00:45:08,560 --> 00:45:11,760
ген  оцениваете несколько отрицательных образцов, и поэтому

1119
00:45:11,760 --> 00:45:14,800
кажется

1120
00:45:14,800 --> 00:45:17,119
, что там всегда делается что-то точное,

1121
00:45:17,119 --> 00:45:20,480
тогда как если вы выполняете

1122
00:45:20,480 --> 00:45:23,119
алгоритм оптимизации для всей

1123
00:45:23,119 --> 00:45:25,760
матрицы сразу, вы на самом деле знаете

1124
00:45:25,760 --> 00:45:27,440
все о матрице сразу,

1125
00:45:27,440 --> 00:45:30,160
вы не просто смотрите на то, что  слова,

1126
00:45:30,160 --> 00:45:32,800
какие еще слова произошли в этом одном

1127
00:45:32,800 --> 00:45:35,520
контексте центрального слова, у вас

1128
00:45:35,520 --> 00:45:37,920
есть весь вектор совместной встречаемости,

1129
00:45:37,920 --> 00:45:40,400
учитывает центральное слово и другое

1130
00:45:40,400 --> 00:45:43,280
слово, и поэтому вы можете гораздо более

1131
00:45:43,280 --> 00:45:46,400
эффективно и менее шумно

1132
00:45:46,400 --> 00:45:50,400
решить, как минимизировать свои потери

1133
00:45:50,400 --> 00:45:52,560
хорошо,

1134
00:45:52,560 --> 00:45:53,920
я продолжу,

1135
00:45:53,920 --> 00:45:55,680
хорошо, так что

1136
00:45:55,680 --> 00:45:57,680
я вроде как сказал, посмотрите на эти векторы слов,

1137
00:45:57,680 --> 00:45:59,760
они великолепны, и я вроде как

1138
00:45:59,760 --> 00:46:01,839
показал вам несколько вещей в

1139
00:46:01,839 --> 00:46:04,000
конце последнего урока, в которых утверждалось, что

1140
00:46:04,000 --> 00:46:06,480
это здорово, вы знаете, что они работают  Из

1141
00:46:06,480 --> 00:46:09,680
этих аналогий они показывают сходство

1142
00:46:09,680 --> 00:46:12,480
и такие вещи, как этот, мы хотим сделать

1143
00:46:12,480 --> 00:46:14,480
это немного точнее,

1144
00:46:14,480 --> 00:46:16,400
и действительно, для обработки естественного языка,

1145
00:46:16,400 --> 00:46:18,800
как и в других областях машинного

1146
00:46:18,800 --> 00:46:21,200
обучения, большая часть того, что люди

1147
00:46:21,200 --> 00:46:23,760
делают, работает  хорошие способы

1148
00:46:23,760 --> 00:46:27,040
оценки имеющихся знаний,

1149
00:46:27,040 --> 00:46:29,040
так как мы можем действительно оценивать векторы слов.

1150
00:46:29,040 --> 00:46:32,800
В общем, для оценки НЛП

1151
00:46:32,800 --> 00:46:35,760
люди говорят о двух способах оценки:

1152
00:46:35,760 --> 00:46:39,200
внутреннем и внешнем, так что внутренняя

1153
00:46:39,200 --> 00:46:41,119
оценка

1154
00:46:41,119 --> 00:46:45,119
означает, что вы оцениваете непосредственно

1155
00:46:45,119 --> 00:46:48,000
конкретные или промежуточные подзадачи, которые

1156
00:46:48,000 --> 00:46:50,160
вы выполняете.  я работал, поэтому мне нужна

1157
00:46:50,160 --> 00:46:52,480
мера, с помощью которой я могу напрямую оценить, насколько

1158
00:46:52,480 --> 00:46:55,599
хороши мои векторы слов, и обычно

1159
00:46:55,599 --> 00:46:58,560
внутренние оценки вычисляются быстро,

1160
00:46:58,560 --> 00:47:01,200
они помогли вам

1161
00:47:01,200 --> 00:47:03,760
понять компонент, над которым вы работали, но

1162
00:47:03,760 --> 00:47:04,960
часто

1163
00:47:04,960 --> 00:47:08,640
просто попытка оптимизировать этот компонент

1164
00:47:08,640 --> 00:47:11,280
может  или может не иметь очень большого хорошего

1165
00:47:11,280 --> 00:47:14,079
эффекта на общую систему, которую вы

1166
00:47:14,079 --> 00:47:16,319
пытаетесь построить,

1167
00:47:16,319 --> 00:47:19,440
так что люди также очень

1168
00:47:19,440 --> 00:47:22,240
интересовались внешними оценками,

1169
00:47:22,240 --> 00:47:25,520
поэтому внешняя оценка заключается в том, что вы берете на себя

1170
00:47:25,520 --> 00:47:28,160
какую-то реальную задачу, представляющую интерес для

1171
00:47:28,160 --> 00:47:30,400
людей, будь то  веб-поиск,

1172
00:47:30,400 --> 00:47:32,880
машинный перевод или что-то в этом роде

1173
00:47:32,880 --> 00:47:35,760
, и вы говорите, что ваша цель -

1174
00:47:35,760 --> 00:47:38,240
действительно улучшить производительность этой

1175
00:47:38,240 --> 00:47:42,319
задачи,  Это реальное доказательство того, что

1176
00:47:42,319 --> 00:47:45,280
это делает что-то полезное, так что в некотором

1177
00:47:45,280 --> 00:47:47,680
смысле это явно лучше,

1178
00:47:47,680 --> 00:47:50,319
но, с другой стороны, у него также есть некоторые

1179
00:47:50,319 --> 00:47:52,000
недостатки:

1180
00:47:52,000 --> 00:47:54,960
требуется намного больше времени для

1181
00:47:54,960 --> 00:47:57,680
оценки внешней задачи, потому

1182
00:47:57,680 --> 00:48:01,040
что это гораздо большая система, и иногда

1183
00:48:01,040 --> 00:48:03,760
вы знаете  когда вы меняете что-

1184
00:48:03,760 --> 00:48:06,240
то, неясно, был ли факт, что

1185
00:48:06,240 --> 00:48:09,040
числа снизились, потому что у вас теперь

1186
00:48:09,040 --> 00:48:11,839
есть худшие векторы слов, или

1187
00:48:11,839 --> 00:48:14,480
просто каким-то образом другие компоненты

1188
00:48:14,480 --> 00:48:16,400
системы

1189
00:48:16,400 --> 00:48:18,400
лучше взаимодействовали с вашими старыми векторами слов,

1190
00:48:18,400 --> 00:48:19,839
и если вы измените другие

1191
00:48:19,839 --> 00:48:22,400
компоненты, а также вещи

1192
00:48:22,400 --> 00:48:24,079
снова станет лучше, так что

1193
00:48:24,079 --> 00:48:26,319
в некотором смысле иногда может быть

1194
00:48:26,319 --> 00:48:29,040
сложнее увидеть, достигаете ли вы прогресса,

1195
00:48:29,040 --> 00:48:32,839
но я коснусь обоих этих методов

1196
00:48:32,839 --> 00:48:36,400
здесь, так что для внутренней оценки

1197
00:48:36,400 --> 00:48:37,839
векторов слов в

1198
00:48:37,839 --> 00:48:38,880
одну сторону,

1199
00:48:38,880 --> 00:48:42,800
о которой мы упоминали в прошлый раз, было это

1200
00:48:42,800 --> 00:48:45,280
аналогия вектора слова, чтобы мы могли просто

1201
00:48:45,280 --> 00:48:48,480
дать нашим моделям большой набор

1202
00:48:48,480 --> 00:48:50,880
задач аналогии вектора слова, чтобы мы могли сказать, что

1203
00:48:50,880 --> 00:48:54,079
мужчина - это женщина, поскольку король - это что, и

1204
00:48:54,079 --> 00:48:57,119
попросить модель определить  Найдите слово, которое

1205
00:48:57,119 --> 00:49:00,160
ближе всего, используя подобного рода вычисление аналогии со словами,

1206
00:49:00,160 --> 00:49:03,200
и надейтесь, что то, что выходит

1207
00:49:03,200 --> 00:49:05,680
, - королева,

1208
00:49:05,680 --> 00:49:07,760
и это то, что люди сделали

1209
00:49:07,760 --> 00:49:10,240
и выработали оценку точности того,

1210
00:49:10,240 --> 00:49:13,680
как часто вы правы

1211
00:49:13,920 --> 00:49:16,559
в этот момент, я просто должен упомянуть  одна

1212
00:49:16,559 --> 00:49:18,960
маленькая уловка из этих словесных векторных

1213
00:49:18,960 --> 00:49:22,079
аналогий, которую все используют, но не

1214
00:49:22,079 --> 00:49:24,559
все о ней много говорят в первую

1215
00:49:24,559 --> 00:49:26,720
очередь, я имею в виду,

1216
00:49:26,720 --> 00:49:28,319
что есть небольшая уловка, которую вы можете

1217
00:49:28,319 --> 00:49:30,880
найти в коде симулятора, если вы посмотрите на него,

1218
00:49:30,880 --> 00:49:34,240
что когда это действительно удается женщине как

1219
00:49:34,240 --> 00:49:35,920
королю

1220
00:49:35,920 --> 00:49:37,599


1221
00:49:37,599 --> 00:49:40,079


1222
00:49:40,079 --> 00:49:43,119
часто

1223
00:49:43,119 --> 00:49:45,760
случается то, что на самом деле слово, когда вы делаете

1224
00:49:45,760 --> 00:49:48,240
свои плюсы и минусы,

1225
00:49:48,240 --> 00:49:52,400
слово, которое на самом деле будет самым близким, по-

1226
00:49:52,400 --> 00:49:54,480
прежнему является королем,

1227
00:49:54,480 --> 00:49:55,200


1228
00:49:55,200 --> 00:49:57,440
поэтому люди всегда делают это так, что

1229
00:49:57,440 --> 00:50:00,079
они не позволяют ни одному из трех  введите

1230
00:50:00,079 --> 00:50:03,440
слова um в процессе выбора, поэтому

1231
00:50:03,440 --> 00:50:05,760
вы выбираете ближайшее слово,

1232
00:50:05,760 --> 00:50:09,200
которое не является одним из тех, что слова

1233
00:50:09,520 --> 00:50:11,200
um ok,

1234
00:50:11,200 --> 00:50:12,480
так как

1235
00:50:12,480 --> 00:50:14,400
um здесь показывает результаты из

1236
00:50:14,400 --> 00:50:17,200
векторов перчатки um, поэтому факторы перчатки

1237
00:50:17,200 --> 00:50:20,480
имеют этот штрих  ng свойство линейного компонента

1238
00:50:20,480 --> 00:50:24,000
точно так же, как я показал ранее um

1239
00:50:24,000 --> 00:50:24,800
для

1240
00:50:24,800 --> 00:50:25,680


1241
00:50:25,680 --> 00:50:28,000
угля um, так что это

1242
00:50:28,000 --> 00:50:31,280
для мужского и женского измерения, и

1243
00:50:31,280 --> 00:50:34,319
поэтому во многих случаях можно ожидать,

1244
00:50:34,319 --> 00:50:36,559
что аналогии слов будут работать, потому что я

1245
00:50:36,559 --> 00:50:38,880
могу взять векторную разность человека

1246
00:50:38,880 --> 00:50:41,520
и  женщина, а затем, если я добавлю эту векторную

1247
00:50:41,520 --> 00:50:44,079
разницу к брату, я ожидаю

1248
00:50:44,079 --> 00:50:45,359
получить сестру

1249
00:50:45,359 --> 00:50:47,760
и король-королеву и из любого из этих

1250
00:50:47,760 --> 00:50:50,400
примеров, но, конечно, они могут не

1251
00:50:50,400 --> 00:50:52,559
всегда работать правильно, потому что если я начну

1252
00:50:52,559 --> 00:50:55,040
с императора, это будет

1253
00:50:55,040 --> 00:50:57,599
скорее  и поэтому может оказаться, что

1254
00:50:57,599 --> 00:51:02,079
вместо этого выйдет графиня или герцогиня,

1255
00:51:02,240 --> 00:51:04,319
вы можете сделать это для различных

1256
00:51:04,319 --> 00:51:06,079
отношений, таких разных семантических

1257
00:51:06,079 --> 00:51:08,079
отношений, так что

1258
00:51:08,079 --> 00:51:09,520
эти типы словарных векторов на самом деле

1259
00:51:09,520 --> 00:51:10,960
узнают довольно много всего лишь мировых

1260
00:51:10,960 --> 00:51:14,240
знаний, так что вот генеральный директор компании

1261
00:51:14,240 --> 00:51:16,839
или это  является генеральным директором компании примерно с

1262
00:51:16,839 --> 00:51:20,160
2010 по 2014 год, когда данные были взяты

1263
00:51:20,160 --> 00:51:21,359
из

1264
00:51:21,359 --> 00:51:23,359
векторов слов,

1265
00:51:23,359 --> 00:51:25,680
и они, а также семантические или

1266
00:51:25,680 --> 00:51:27,839
прагматические вещи, подобные этому, также

1267
00:51:27,839 --> 00:51:30,000
изучают синтаксические вещи, поэтому вот

1268
00:51:30,000 --> 00:51:32,640
векторы для po  сравнительные и

1269
00:51:32,640 --> 00:51:35,520
превосходные формы прилагательных, и вы

1270
00:51:35,520 --> 00:51:38,800
можете видеть, что они также перемещаются и примерно

1271
00:51:38,800 --> 00:51:42,400
линейные компоненты, так что слово, чтобы

1272
00:51:42,400 --> 00:51:44,559
направить людей, построили набор данных

1273
00:51:44,559 --> 00:51:46,800
аналогий, чтобы вы могли оценивать

1274
00:51:46,800 --> 00:51:48,400
различные модели на предмет точности

1275
00:51:48,400 --> 00:51:50,559
их аналогий,

1276
00:51:50,559 --> 00:51:52,319
и

1277
00:51:52,319 --> 00:51:54,079
вот как вы  может это сделать, и это

1278
00:51:54,079 --> 00:51:56,400
дает некоторые числа, поэтому есть семантические

1279
00:51:56,400 --> 00:51:58,720
и синтаксические аналогии, я просто посмотрю

1280
00:51:58,720 --> 00:52:00,800
на итоги,

1281
00:52:00,800 --> 00:52:03,040
хорошо, так что я сказал раньше, если вы

1282
00:52:03,040 --> 00:52:06,079
просто используете немасштабированные

1283
00:52:06,079 --> 00:52:08,319
счетчики совпадений

1284
00:52:08,319 --> 00:52:11,040
и передаете их через svd, все работает

1285
00:52:11,040 --> 00:52:12,800
ужасно, и вы  видите, что вы

1286
00:52:12,800 --> 00:52:16,160
получаете только 7,3, но затем, как я также указал,

1287
00:52:16,160 --> 00:52:18,319
если вы выполните некоторое масштабирование, вы действительно можете

1288
00:52:18,319 --> 00:52:19,599
получить svd

1289
00:52:19,599 --> 00:52:22,400
для масштабированной матрицы подсчета, чтобы работать

1290
00:52:22,400 --> 00:52:26,640
достаточно хорошо, поэтому этот

1291
00:52:26,640 --> 00:52:29,040
spdl похож на модель Коля, и теперь

1292
00:52:29,040 --> 00:52:31,280
мы встаем  до 60,1, что на самом

1293
00:52:31,280 --> 00:52:33,040
деле неплохой результат, так что вы

1294
00:52:33,040 --> 00:52:34,880
действительно можете достойно работать без

1295
00:52:34,880 --> 00:52:37,839
нейронной сети, а затем вот

1296
00:52:37,839 --> 00:52:41,040
два варианта модели

1297
00:52:41,040 --> 00:52:43,839
um word2vec, и вот наши

1298
00:52:43,839 --> 00:52:44,880


1299
00:52:44,880 --> 00:52:46,800
результаты  модель перчатки и, конечно же, в

1300
00:52:46,800 --> 00:52:48,000


1301
00:52:48,000 --> 00:52:52,000
2014 году мы восприняли это как абсолютное доказательство того,

1302
00:52:52,000 --> 00:52:54,640
что наша модель была лучше, и наше более

1303
00:52:54,640 --> 00:52:57,440
эффективное использование статистики действительно

1304
00:52:57,440 --> 00:53:00,640
работало в нашу пользу эм с семью

1305
00:53:00,640 --> 00:53:03,280
годами ретроспективы, я думаю, что это не

1306
00:53:03,280 --> 00:53:05,839
совсем так, оказывается  Я думаю, что

1307
00:53:05,839 --> 00:53:08,480
основная причина того, почему мы набрали больше очков, заключается в

1308
00:53:08,480 --> 00:53:10,880
том, что у нас на самом деле были лучшие данные, и поэтому

1309
00:53:10,880 --> 00:53:13,760
на следующем слайде есть немного доказательств

1310
00:53:13,760 --> 00:53:16,720
этого, поэтому здесь рассматривается

1311
00:53:16,720 --> 00:53:17,920


1312
00:53:17,920 --> 00:53:20,400
семантическая синтаксическая и общая

1313
00:53:20,400 --> 00:53:24,319
производительность на словесных аналогиях моделей перчаток,

1314
00:53:24,319 --> 00:53:27,040
которые были обучены  на разных

1315
00:53:27,040 --> 00:53:30,079
подмножествах данных, поэтому, в частности, двое

1316
00:53:30,079 --> 00:53:34,319
слева обучаются в Википедии,

1317
00:53:34,319 --> 00:53:35,760
и вы можете видеть, что обучение в

1318
00:53:35,760 --> 00:53:38,800
Википедии помогает вам хорошо

1319
00:53:38,800 --> 00:53:41,520
разбираться в семантических аналогиях, что, возможно, имеет

1320
00:53:41,520 --> 00:53:43,520
смысл, потому что Википедия просто сообщает вам

1321
00:53:43,520 --> 00:53:45,280
много семантических фактов, я имею в виду, что  что-то вроде

1322
00:53:45,280 --> 00:53:45,920
того,

1323
00:53:45,920 --> 00:53:48,000
что делают энциклопедии,

1324
00:53:48,000 --> 00:53:50,640
и поэтому одним из наших больших преимуществ

1325
00:53:50,640 --> 00:53:54,240
была эта википедия

1326
00:53:54,240 --> 00:53:56,800
, в которой модель перчатки была частично обучена

1327
00:53:56,800 --> 00:53:59,280
в Википедии, а также в других тексах.  ts, в

1328
00:53:59,280 --> 00:54:01,119
то время как модель word2vec, которая была

1329
00:54:01,119 --> 00:54:03,440
выпущена, была обучена исключительно в

1330
00:54:03,440 --> 00:54:06,800
новостях Google, поэтому данные ленты новостей, и если вы

1331
00:54:06,800 --> 00:54:10,240
тренируетесь только на небольшом количестве

1332
00:54:10,240 --> 00:54:13,280
данных ленты новостей, вы можете видеть, что для

1333
00:54:13,280 --> 00:54:16,160
семантики это просто не так хорошо, как

1334
00:54:16,160 --> 00:54:17,520


1335
00:54:17,520 --> 00:54:20,240
даже четверть  от

1336
00:54:20,240 --> 00:54:23,040
размера данных Википедии, хотя, если вы получаете

1337
00:54:23,040 --> 00:54:25,119
много данных, вы можете компенсировать это, поэтому

1338
00:54:25,119 --> 00:54:27,599
здесь, справа, у вас

1339
00:54:27,599 --> 00:54:30,079
были общие веб-данные для сканирования, и поэтому,

1340
00:54:30,079 --> 00:54:32,400
когда веб-данных много, так что теперь 42

1341
00:54:32,400 --> 00:54:35,119
миллиарда слов гм  затем вы снова начинаете

1342
00:54:35,119 --> 00:54:37,280
добавлять хорошие оценки с семантической

1343
00:54:37,280 --> 00:54:39,440
стороны.

1344
00:54:39,440 --> 00:54:42,960
График справа показывает, насколько

1345
00:54:42,960 --> 00:54:45,440
хорошо вы делаете это при увеличении

1346
00:54:45,440 --> 00:54:49,200
размерности вектора, и вы можете видеть то,

1347
00:54:49,200 --> 00:54:51,680
что вы знаете, что 25-мерные

1348
00:54:51,680 --> 00:54:54,240
векторы не очень  хорошо, что они доходят до

1349
00:54:54,240 --> 00:54:57,440
50, а затем до 100, и поэтому 100 размерных

1350
00:54:57,440 --> 00:54:59,920
векторов уже работают достаточно хорошо

1351
00:54:59,920 --> 00:55:01,359
, поэтому я использовал сотни размерных

1352
00:55:01,359 --> 00:55:02,319
векторов,

1353
00:55:02,319 --> 00:55:06,960
когда показывал свой пример в классе, но

1354
00:55:06,960 --> 00:55:10,400
это сладкий запас, слишком долгая загрузка и

1355
00:55:10,400 --> 00:55:12,160
работа достаточно хорошо  Но вы все равно

1356
00:55:12,160 --> 00:55:14,480
получаете значительный прирост за 200, а это

1357
00:55:14,480 --> 00:55:17,119
несколько до 300, так что, по крайней мере,

1358
00:55:17,119 --> 00:55:20,240
в 2013–15 гг. все

1359
00:55:20,240 --> 00:55:22,480
тяготели к тому факту, что 300-

1360
00:55:22,480 --> 00:55:25,200
мерные векторы - это золотая середина, ммм

1361
00:55:25,200 --> 00:55:27,599
так почти часто, если вы

1362
00:55:27,599 --> 00:55:29,760
просматриваете самые известные наборы  векторов слов, которые

1363
00:55:29,760 --> 00:55:32,319
включают в себя векторы слов divec и

1364
00:55:32,319 --> 00:55:35,200
векторы перчаток, которые обычно

1365
00:55:35,200 --> 00:55:38,960
представляют собой 300-мерные векторы слов.

1366
00:55:38,960 --> 00:55:41,359
Это не единственная внутренняя

1367
00:55:41,359 --> 00:55:43,280
оценка, которую вы можете сделать.

1368
00:55:43,280 --> 00:55:45,599
Еще одна внутренняя оценка, которую вы можете сделать,

1369
00:55:45,599 --> 00:55:47,599
это посмотреть, как

1370
00:55:47,599 --> 00:55:50,720
эти модели моделируют человеческие суждения о

1371
00:55:50,720 --> 00:55:54,559
сходстве слов.  гм, поэтому психологи в течение

1372
00:55:54,559 --> 00:55:57,680
нескольких десятилетий фактически

1373
00:55:57,680 --> 00:56:00,559
принимали человеческие суждения о подобии слов,

1374
00:56:00,559 --> 00:56:03,920
когда буквально вы просите людей

1375
00:56:03,920 --> 00:56:06,720


1376
00:56:06,720 --> 00:56:09,280
называть пары слов, такие как профессор и доктор, чтобы дать им оценку сходства, которая

1377
00:56:09,280 --> 00:56:10,640
вроде бы измеряется как некоторая

1378
00:56:10,640 --> 00:56:13,280
непрерывная величина, дающая вам оценку

1379
00:56:13,280 --> 00:56:17,040
между, скажем,  0 и десять мкм, и поэтому

1380
00:56:17,040 --> 00:56:19,040
есть человеческие суждения, которые затем

1381
00:56:19,040 --> 00:56:21,280
усредняются по множеству человеческих суждений

1382
00:56:21,280 --> 00:56:24,079
о том, насколько похожи разные  Такие слова такие

1383
00:56:24,079 --> 00:56:26,640
тигр и кот очень похожи,

1384
00:56:26,640 --> 00:56:28,960
компьютер и интернет очень похожи,

1385
00:56:28,960 --> 00:56:31,680
самолет, а машина меньше похожа, и

1386
00:56:31,680 --> 00:56:34,480
компакт-диск совсем не похожи, но стоковая

1387
00:56:34,480 --> 00:56:38,319
и ягуар еще менее похожи,

1388
00:56:38,319 --> 00:56:39,920
так что мы могли бы сказать,

1389
00:56:39,920 --> 00:56:40,960


1390
00:56:40,960 --> 00:56:44,240
что наши модели делают  у них одинаковые

1391
00:56:44,240 --> 00:56:47,119
суждения о сходстве, и, в частности,

1392
00:56:47,119 --> 00:56:49,920
мы можем измерить коэффициент корреляции

1393
00:56:49,920 --> 00:56:51,599
того, дают ли они одинаковый порядок

1394
00:56:51,599 --> 00:56:52,319


1395
00:56:52,319 --> 00:56:54,960
суждений о сходстве, и тогда мы можем

1396
00:56:54,960 --> 00:56:57,359
получить данные для этого, и, таким образом, существуют

1397
00:56:57,359 --> 00:56:59,760
различные наборы данных о сходстве слов,

1398
00:56:59,760 --> 00:57:02,319
и мы можем оценивать разные

1399
00:57:02,319 --> 00:57:04,880
модели относительно того, насколько хорошо они справляются с

1400
00:57:04,880 --> 00:57:07,440
подобием, и снова

1401
00:57:07,440 --> 00:57:10,559
вы видите здесь, что простые svds

1402
00:57:10,559 --> 00:57:14,000
и здесь работают сравнительно лучше для

1403
00:57:14,000 --> 00:57:16,000
сходства, чем для аналогий,

1404
00:57:16,000 --> 00:57:17,520
вы знаете, что это не очень хорошо, но теперь это не

1405
00:57:17,520 --> 00:57:19,119
совсем ужасно,

1406
00:57:19,119 --> 00:57:20,960
потому что нам больше не нужно это линейное

1407
00:57:20,960 --> 00:57:23,760
свойство, но снова  масштабированные svds работают

1408
00:57:23,760 --> 00:57:25,760
намного лучше

1409
00:57:25,760 --> 00:57:28,400
word2vec работает немного лучше,

1410
00:57:28,400 --> 00:57:30,000
и мы получили некоторые из тех же

1411
00:57:30,000 --> 00:57:33,760
незначительных преимуществ от модели перчатки

1412
00:57:33,760 --> 00:57:35,440
эй chr  извините, что прерываю,

1413
00:57:35,440 --> 00:57:37,520
многие студенты спрашивали, можете ли вы

1414
00:57:37,520 --> 00:57:39,680
заново объяснить целевую функцию для

1415
00:57:39,680 --> 00:57:42,240
модели перчатки, а также что

1416
00:57:42,240 --> 00:57:45,119
означает логарифмическая билинейность.

1417
00:57:45,119 --> 00:57:46,160


1418
00:57:46,160 --> 00:57:47,359


1419
00:57:47,359 --> 00:57:49,760


1420
00:57:50,400 --> 00:57:52,160


1421
00:57:52,160 --> 00:57:55,920


1422
00:57:55,920 --> 00:57:58,640
так что один слайд

1423
00:57:58,640 --> 00:58:02,720
перед этим правым, так что свойство, которое

1424
00:58:02,720 --> 00:58:04,559
мы хотим,

1425
00:58:04,559 --> 00:58:08,640
состоит в том, что мы хотим, чтобы скалярное произведение

1426
00:58:08,640 --> 00:58:12,720
um представляло логарифмическую вероятность

1427
00:58:12,720 --> 00:58:14,319
совместной встречаемости,

1428
00:58:14,319 --> 00:58:15,200
поэтому

1429
00:58:15,200 --> 00:58:18,559
um, и это дает мне мой сложный

1430
00:58:18,559 --> 00:58:22,160
лог-билинейный, поэтому покупка заключается в том, что есть своего

1431
00:58:22,160 --> 00:58:24,640
рода wi  и wj, так что есть

1432
00:58:24,640 --> 00:58:26,640
как бы две

1433
00:58:26,640 --> 00:58:27,760
линейные

1434
00:58:27,760 --> 00:58:30,319
вещи, и это линейно в каждой из

1435
00:58:30,319 --> 00:58:33,920
них, так что это похоже на наличие и,

1436
00:58:33,920 --> 00:58:37,040
а не на своего рода топор, где у

1437
00:58:37,040 --> 00:58:39,280
вас просто есть что-то линейное

1438
00:58:39,280 --> 00:58:42,880
по x, а a - константа  это билинейно,

1439
00:58:42,880 --> 00:58:45,839
потому что у нас есть wiwj, и

1440
00:58:45,839 --> 00:58:48,160
в них обоих есть линейный, и это затем

1441
00:58:48,160 --> 00:58:51,200
связано с логарифмом вероятности, и

1442
00:58:51,200 --> 00:58:55,520
поэтому это дает нам журнал по линейной модели,

1443
00:58:56,799 --> 00:58:58,000
и так

1444
00:58:58,000 --> 00:59:01,200
как мы, поскольку мы хотим, чтобы эти вещи

1445
00:59:01,200 --> 00:59:03,200
были равны

1446
00:59:03,200 --> 00:59:05,440
тем, что мы  делаем ее  e, если вы проигнорируете

1447
00:59:05,440 --> 00:59:07,839
эти два центральных члена, это означает, что мы

1448
00:59:07,839 --> 00:59:11,359
хотим сказать, что разница между

1449
00:59:11,359 --> 00:59:12,319
этими

1450
00:59:12,319 --> 00:59:15,040
двумя минимальна, поэтому

1451
00:59:15,040 --> 00:59:16,799
мы берем эту разницу и

1452
00:59:16,799 --> 00:59:19,200
возводим ее в квадрат, так что она всегда положительна, и

1453
00:59:19,200 --> 00:59:20,720
мы хотим, чтобы этот

1454
00:59:20,720 --> 00:59:24,640
член в квадрате был  быть как можно меньше,

1455
00:59:24,640 --> 00:59:27,920
и вы знаете, что это 90 процентов, и

1456
00:59:27,920 --> 00:59:30,559
вы можете в основном остановиться на этом, но

1457
00:59:30,559 --> 00:59:32,480
другая часть, которая здесь присутствует,

1458
00:59:32,480 --> 00:59:34,319
- это много времени,

1459
00:59:34,319 --> 00:59:35,520
когда вы

1460
00:59:35,520 --> 00:59:39,040
строите модели, а не

1461
00:59:39,040 --> 00:59:42,559
просто имеете своего рода модель топора, кажется

1462
00:59:42,559 --> 00:59:45,839
полезно иметь термин смещения, который может

1463
00:59:45,839 --> 00:59:47,839
перемещать вещи вверх и вниз

1464
00:59:47,839 --> 00:59:48,640


1465
00:59:48,640 --> 00:59:51,920
для слова в целом, и поэтому мы добавляем его

1466
00:59:51,920 --> 00:59:54,240
в термин смещения модели, чтобы

1467
00:59:54,240 --> 00:59:57,680
для обоих слов был термин смещения, поэтому, если в

1468
00:59:57,680 --> 01:00:00,000
целом вероятность высока для

1469
01:00:00,000 --> 01:00:02,640
определенного слова, это смещение  термин может смоделировать

1470
01:00:02,640 --> 01:00:05,280
это, а для другого слова этот термин предвзятости,

1471
01:00:05,280 --> 01:00:07,280
затем смоделировать его,

1472
01:00:07,280 --> 01:00:08,720
хорошо,

1473
01:00:08,720 --> 01:00:10,559
так что теперь я вернусь,

1474
01:00:10,559 --> 01:00:12,240
и после того, как

1475
01:00:12,240 --> 01:00:13,440
э-э, на

1476
01:00:13,440 --> 01:00:16,240
самом деле, я только что увидел, как кто-то сказал, почему

1477
01:00:16,240 --> 01:00:19,440
умножение на f,

1478
01:00:19,440 --> 01:00:22,000
извините, я пропустил этот последний термин,

1479
01:00:22,000 --> 01:00:23,680


1480
01:00:23,680 --> 01:00:26,960
ладно, почему  модифицируя этим f элемента

1481
01:00:26,960 --> 01:00:30,960
xij, так что t  его последняя часть заключалась в том,

1482
01:00:30,960 --> 01:00:32,000
чтобы

1483
01:00:32,000 --> 01:00:36,720
масштабировать вещи в зависимости от частоты

1484
01:00:36,720 --> 01:00:40,319
употребления слова, потому что

1485
01:00:40,319 --> 01:00:41,520
вы хотите

1486
01:00:41,520 --> 01:00:43,760
уделять больше внимания

1487
01:00:43,760 --> 01:00:44,640


1488
01:00:44,640 --> 01:00:47,760
более распространенным словам или парам слов,

1489
01:00:47,760 --> 01:00:50,400
которые встречаются чаще, потому что вы знаете, что если

1490
01:00:50,400 --> 01:00:52,880
вы думаете об этом, вы понимаете,

1491
01:00:52,880 --> 01:00:55,040
что вы  видя, есть

1492
01:00:55,040 --> 01:00:57,119
ли у вещей счет совместного

1493
01:00:57,119 --> 01:00:59,359
возникновения 50 против

1494
01:00:59,359 --> 01:01:00,640
3,

1495
01:01:00,640 --> 01:01:03,680
вы хотите лучше

1496
01:01:03,680 --> 01:01:07,440
моделировать совместное возникновение вещей, которые

1497
01:01:07,440 --> 01:01:11,200
произошли вместе 50 раз,

1498
01:01:11,200 --> 01:01:12,400
и поэтому

1499
01:01:12,400 --> 01:01:14,640
вы хотите учитывать при подсчете

1500
01:01:14,640 --> 01:01:16,079
совпадения,

1501
01:01:16,079 --> 01:01:18,559
но тогда  аргумент состоит в том, что на

1502
01:01:18,559 --> 01:01:20,960
самом деле это сбивает вас с пути, когда у вас есть

1503
01:01:20,960 --> 01:01:23,119
чрезвычайно распространенные слова, такие как функциональные

1504
01:01:23,119 --> 01:01:26,319
слова, и вы так эффективно обращали больше

1505
01:01:26,319 --> 01:01:28,079
внимания

1506
01:01:28,079 --> 01:01:29,119
на

1507
01:01:29,119 --> 01:01:31,760
слова, которые сочетались вместе

1508
01:01:31,760 --> 01:01:34,160
до определенной точки, а затем кривая просто

1509
01:01:34,160 --> 01:01:36,480
сглаживалась, поэтому не имело значения, если  это

1510
01:01:36,480 --> 01:01:38,880
было чрезвычайно распространенное слово,

1511
01:01:38,880 --> 01:01:40,000
так что тогда

1512
01:01:40,000 --> 01:01:42,640
гм для

1513
01:01:42,640 --> 01:01:45,680
внешней оценки вектора слов, так что на

1514
01:01:45,680 --> 01:01:47,200
этом этапе

1515
01:01:47,200 --> 01:01:49,680
вы теперь хотите вроде как сказать,

1516
01:01:49,680 --> 01:01:53,119
можем ли мы встроить наши векторы слов в какую-то

1517
01:01:53,119 --> 01:01:57,680
задачу конечного пользователя, и помогают ли они

1518
01:01:57,680 --> 01:01:59,839
и  работают ли разные векторы слов

1519
01:01:59,839 --> 01:02:03,200
лучше или хуже, чем другие векторы слов,

1520
01:02:03,200 --> 01:02:05,280
так что это то, что мы увидим

1521
01:02:05,280 --> 01:02:07,839
много позже в классе, я имею в виду, в

1522
01:02:07,839 --> 01:02:09,680
частности,

1523
01:02:09,680 --> 01:02:11,520
когда вы перейдете к выполнению

1524
01:02:11,520 --> 01:02:13,920
третьего задания, которое вы получите для

1525
01:02:13,920 --> 01:02:16,400
создания синтаксических анализаторов зависимостей и  затем вы можете

1526
01:02:16,400 --> 01:02:18,079


1527
01:02:18,079 --> 01:02:19,920
использовать векторы слов в

1528
01:02:19,920 --> 01:02:22,559
анализаторе зависимостей и посмотреть, насколько они помогают, мы на

1529
01:02:22,559 --> 01:02:23,920
самом деле не заставляем вас тестировать

1530
01:02:23,920 --> 01:02:26,079
разные наборы векторов слов, но вы

1531
01:02:26,079 --> 01:02:28,240
могли бы

1532
01:02:28,240 --> 01:02:30,240
вот только один пример этого, чтобы дать

1533
01:02:30,240 --> 01:02:32,960
вам смысл, поэтому задача named

1534
01:02:32,960 --> 01:02:35,440
Распознавание сущностей проходит через фрагмент

1535
01:02:35,440 --> 01:02:38,400
текста и идентифицирует упоминания имени

1536
01:02:38,400 --> 01:02:40,960
человека или названия организации, например,

1537
01:02:40,960 --> 01:02:46,079
компании или местоположения, и поэтому,

1538
01:02:46,079 --> 01:02:49,760
если у вас есть хорошие векторы слов, ммм, они

1539
01:02:49,760 --> 01:02:52,240
помогают вам лучше распознавать именованные сущности,

1540
01:02:52,240 --> 01:02:55,039
и ответ на это  да, так что

1541
01:02:55,039 --> 01:02:57,680
если кто-то начинает с модели, которая

1542
01:02:57,680 --> 01:03:01,119
просто имеет дискретные функции, поэтому она использует

1543
01:03:01,119 --> 01:03:03,839
идентичность слов в качестве функций, вы можете

1544
01:03:03,839 --> 01:03:05,760
построить довольно хорошую именованную модель сущности, делая

1545
01:03:05,760 --> 01:03:08,960
это, но если вы добавите в нее векторы слов,

1546
01:03:08,960 --> 01:03:11,119
вы  получить лучшее представление о

1547
01:03:11,119 --> 01:03:13,440
значении слов, чтобы вы

1548
01:03:13,440 --> 01:03:16,480
могли немного увеличить числа, а

1549
01:03:16,480 --> 01:03:19,119
затем вы могли сравнивать разные модели, чтобы

1550
01:03:19,119 --> 01:03:21,760
увидеть, какой выигрыш они дают вам с точки

1551
01:03:21,760 --> 01:03:24,559
зрения этой внешней задачи,

1552
01:03:24,559 --> 01:03:26,640
так что забегая вперед,

1553
01:03:26,640 --> 01:03:28,400
это был вопрос  что после урока меня спросили,

1554
01:03:28,400 --> 01:03:30,559


1555
01:03:30,559 --> 01:03:31,599
что это за

1556
01:03:31,599 --> 01:03:35,039
слова, потому что до сих пор

1557
01:03:35,039 --> 01:03:36,799
у нас было только

1558
01:03:36,799 --> 01:03:37,760
одно

1559
01:03:37,760 --> 01:03:40,160
слово,

1560
01:03:40,160 --> 01:03:42,720
извините за одну конкретную строку, у нас

1561
01:03:42,720 --> 01:03:45,200
есть некоторый строковый дом, и мы собираемся

1562
01:03:45,200 --> 01:03:47,920
сказать для каждой из этих строк есть

1563
01:03:47,920 --> 01:03:49,359
вектор слов,

1564
01:03:49,359 --> 01:03:52,319
и если  вы думаете об этом немного больше,

1565
01:03:52,319 --> 01:03:54,319
что кажется

1566
01:03:54,319 --> 01:03:55,359
очень

1567
01:03:55,359 --> 01:03:59,599
странным, потому что на самом деле большинство слов,

1568
01:03:59,599 --> 01:04:01,920
особенно распространенные слова и особенно

1569
01:04:01,920 --> 01:04:04,480
слова, которые существуют в течение долгого времени, на

1570
01:04:04,480 --> 01:04:06,880
самом деле имеют много значений, которые

1571
01:04:06,880 --> 01:04:09,119
очень разные, так как это можно

1572
01:04:09,119 --> 01:04:11,039
уловить, если у вас есть только один

1573
01:04:11,039 --> 01:04:13,039
вектор слова для слова, потому что вы не можете на

1574
01:04:13,039 --> 01:04:15,119
самом деле уловить тот факт, что у вас

1575
01:04:15,119 --> 01:04:17,359
есть разные значения слова,

1576
01:04:17,359 --> 01:04:18,960
потому что ваше значение для слова - это

1577
01:04:18,960 --> 01:04:21,920
всего лишь одна точка в пространстве, один вектор

1578
01:04:21,920 --> 01:04:23,039
и так

1579
01:04:23,039 --> 01:04:24,799
, например,  об этом

1580
01:04:24,799 --> 01:04:27,599
вот слово труба

1581
01:04:27,599 --> 01:04:28,799
теперь

1582
01:04:28,799 --> 01:04:30,400
это на самом деле,

1583
01:04:30,400 --> 01:04:32,000
но это старое немецкое

1584
01:04:32,000 --> 01:04:33,680
слово хорошо, какое значение

1585
01:04:33,680 --> 01:04:35,200
имеет слово щука,

1586
01:04:35,200 --> 01:04:36,400


1587
01:04:36,400 --> 01:04:38,799
так что, может быть, вы можете просто подумать на минуту

1588
01:04:38,799 --> 01:04:41,440
и подумать,

1589
01:04:41,440 --> 01:04:46,480
какие значения имеет слово щука,

1590
01:04:46,559 --> 01:04:47,920
и оно на самом деле оказывается  как вы знаете, это

1591
01:04:47,920 --> 01:04:50,720
имеет много разных значений,

1592
01:04:50,720 --> 01:04:54,559
так что, возможно, самое основное значение - эм,

1593
01:04:54,559 --> 01:04:57,039
если вы играли в фэнтезийные игры или какое-то

1594
01:04:57,039 --> 01:05:00,160
средневековое оружие, эм, остроконечный

1595
01:05:00,160 --> 01:05:02,480
посох, есть пика, эм, но

1596
01:05:02,480 --> 01:05:04,400
есть рыба, которая имеет похожую

1597
01:05:04,400 --> 01:05:08,319
удлиненную форму  это щука, эм, она

1598
01:05:08,319 --> 01:05:09,520
использовалась для

1599
01:05:09,520 --> 01:05:13,039
железных дорог, эм, возможно, это употребление

1600
01:05:13,039 --> 01:05:15,119
больше не используется, но, безусловно, все еще

1601
01:05:15,119 --> 01:05:18,160
используется в отношении дорог, так что

1602
01:05:18,160 --> 01:05:20,559
это похоже на то, когда у вас есть магистрали, у нас есть

1603
01:05:20,559 --> 01:05:23,039
выражения, где щука означает будущее,

1604
01:05:23,039 --> 01:05:25,440
например, спуск по пике

1605
01:05:25,440 --> 01:05:27,680
это положение в дайвинге

1606
01:05:27,680 --> 01:05:30,079
, когда ныряльщики ловят щуку,

1607
01:05:30,079 --> 01:05:32,319
это все теперь петли,

1608
01:05:32,319 --> 01:05:34,799
они также используются словесно, так что вы можете поймать

1609
01:05:34,799 --> 01:05:37,599
кого-то своей щукой,

1610
01:05:37,599 --> 01:05:39,839
вы знаете, что в Австралии у разных видов использования может быть

1611
01:05:39,839 --> 01:05:41,359
другая валюта

1612
01:05:41,359 --> 01:05:43,520
вы  также можно использовать «

1613
01:05:43,520 --> 01:05:45,520
щуку», чтобы обозначить, что вы отказываетесь от выполнения

1614
01:05:45,520 --> 01:05:47,200
чего-то вроде, я думаю, он собирается

1615
01:05:47,200 --> 01:05:49,280
щучать,

1616
01:05:49,280 --> 01:05:50,880
я не думаю, что это употребление используется в

1617
01:05:50,880 --> 01:05:52,640
Америке, но много значений и, на

1618
01:05:52,640 --> 01:05:54,400
самом деле, для слов, которые встречаются чаще, если

1619
01:05:54,400 --> 01:05:56,799
вы начинаете думать о таких словах, как линия или

1620
01:05:56,799 --> 01:05:59,039
поле я имею в виду, что у них просто еще больше

1621
01:05:59,039 --> 01:06:01,200
значений, чем это, так

1622
01:06:01,200 --> 01:06:03,599
что мы на самом деле делаем только с одним

1623
01:06:03,599 --> 01:06:05,599
вектором для слова

1624
01:06:05,599 --> 01:06:06,559
, и что

1625
01:06:06,559 --> 01:06:07,359
ж,

1626
01:06:07,359 --> 01:06:10,160
один из способов, которым вы могли бы пойти, - это сказать хорошо,

1627
01:06:10,160 --> 01:06:12,559
до сих пор то, что мы делали, - это сумасшедшая

1628
01:06:12,559 --> 01:06:14,720
щука и другие слова  имеют все

1629
01:06:14,720 --> 01:06:17,200
эти разные значения, так что, возможно, нам

1630
01:06:17,200 --> 01:06:18,480


1631
01:06:18,480 --> 01:06:21,599
следует иметь разные векторы слов для

1632
01:06:21,599 --> 01:06:24,079
разных значений пики, чтобы у нас был

1633
01:06:24,079 --> 01:06:25,119
один

1634
01:06:25,119 --> 01:06:27,760
вектор слов для средневекового остроконечного

1635
01:06:27,760 --> 01:06:30,880
оружия, другой вектор слов для

1636
01:06:30,880 --> 01:06:33,680
вида рыбы, другой вектор слова для вида

1637
01:06:33,680 --> 01:06:36,240
дороги, так что тогда они будут векторами смыслового слова,

1638
01:06:36,240 --> 01:06:38,799


1639
01:06:39,280 --> 01:06:42,400
и вы можете это сделать, я имею в виду, что на самом деле

1640
01:06:42,400 --> 01:06:45,799
мы работали над этим в начале

1641
01:06:45,799 --> 01:06:48,960
2010-х, на самом деле, даже до того, как word2vec

1642
01:06:48,960 --> 01:06:50,559
появилось,

1643
01:06:50,559 --> 01:06:51,440
так что

1644
01:06:51,440 --> 01:06:54,000
эта картинка немного мала,

1645
01:06:54,000 --> 01:06:54,960
чтобы увидеть,

1646
01:06:54,960 --> 01:06:57,520
чем мы были  для

1647
01:06:57,520 --> 01:07:00,799
слов, которые мы работаем, кластеризовали

1648
01:07:00,799 --> 01:07:03,280
экземпляры слова в надежде, что эти

1649
01:07:03,280 --> 01:07:04,559
кластеры

1650
01:07:04,559 --> 01:07:07,119
будут кластеризовать словарные токены, надеясь, что

1651
01:07:07,119 --> 01:07:08,720
те кластеры, которые были похожи,

1652
01:07:08,720 --> 01:07:11,599
представляли сенсоры, а затем для

1653
01:07:11,599 --> 01:07:14,160
кластеров словесных токенов мы как бы

1654
01:07:14,160 --> 01:07:15,760
относились к ним, как к отдельным

1655
01:07:15,760 --> 01:07:18,079
словам, и изучая  вектор слова для

1656
01:07:18,079 --> 01:07:20,720
каждого, и вы знаете, что в основном

1657
01:07:20,720 --> 01:07:24,079
это работает, поэтому в зеленом цвете у нас есть два

1658
01:07:24,079 --> 01:07:26,559
смысла для слова банк, и поэтому есть

1659
01:07:26,559 --> 01:07:28,640
одно значение для слова банк, которое

1660
01:07:28,640 --> 01:07:30,880
здесь, где оно близко к таким словам, как

1661
01:07:30,880 --> 01:07:32,799
транзакция банковского финансирования и

1662
01:07:32,799 --> 01:07:34,799
отмывание, а затем у нас есть другое

1663
01:07:34,799 --> 01:07:36,960
смысл слова банк здесь, в

1664
01:07:36,960 --> 01:07:39,039
то время как близко к таким словам, как

1665
01:07:39,039 --> 01:07:41,440
территория границы плато, которая является

1666
01:07:41,440 --> 01:07:44,720
смыслом слова берег реки у берега реки, и

1667
01:07:44,720 --> 01:07:49,039
для слова ягуар, выделенного пурпурным цветом, ну, у

1668
01:07:49,039 --> 01:07:51,920
jq есть несколько датчиков, и поэтому

1669
01:07:51,920 --> 01:07:54,880
у нас есть и они, так что  это чувство

1670
01:07:54,880 --> 01:07:57,440
здесь близко к охотнику, так что

1671
01:07:57,440 --> 01:08:00,160
это чувство крупного зверя,

1672
01:08:00,160 --> 01:08:03,200
ягуара на вершине, здесь проявляется

1673
01:08:03,200 --> 01:08:05,440
рядом с роскошью и кабриолетом.  блин, это

1674
01:08:05,440 --> 01:08:07,599
чувство автомобиля ягуара,

1675
01:08:07,599 --> 01:08:11,839
тогда ягуар здесь рядом со струнной

1676
01:08:11,839 --> 01:08:14,160
клавиатурой и такими словами, так что ягуар

1677
01:08:14,160 --> 01:08:17,839
- это название своего рода клавиатуры, а

1678
01:08:17,839 --> 01:08:20,880
затем этот последний ягуар здесь

1679
01:08:20,880 --> 01:08:23,439
близок к программному обеспечению и Microsoft, а затем,

1680
01:08:23,439 --> 01:08:25,439
если вы  достаточно старый, вы помните

1681
01:08:25,439 --> 01:08:27,359
, что была старая версия mac os,

1682
01:08:27,359 --> 01:08:29,600
поэтому она называлась jaguar um, так что

1683
01:08:29,600 --> 01:08:32,080
это компьютерный смысл, так что в основном это

1684
01:08:32,080 --> 01:08:35,759
работает, и мы можем изучать векторы слов

1685
01:08:35,759 --> 01:08:38,560
для разных датчиков слова, но на

1686
01:08:38,560 --> 01:08:40,880
самом деле это не  в большинстве

1687
01:08:40,880 --> 01:08:44,719
случаев это было сделано на практике,

1688
01:08:44,719 --> 01:08:46,000
и для

1689
01:08:46,000 --> 01:08:48,000
этого есть несколько

1690
01:08:48,000 --> 01:08:51,279
причин, я имею в виду, что одна из них - это просто простота,

1691
01:08:51,279 --> 01:08:54,399
если вы делаете это,

1692
01:08:54,399 --> 01:08:56,479
это довольно сложно, потому что вам в

1693
01:08:56,479 --> 01:08:58,399
первую очередь нужно выучить значения слов, а

1694
01:08:58,399 --> 01:09:00,000
затем начать изучать векторы слов  с

1695
01:09:00,000 --> 01:09:02,399
точки зрения смысла слов,

1696
01:09:02,399 --> 01:09:04,799
но другая причина в том, что хотя эта

1697
01:09:04,799 --> 01:09:08,560
модель наличия датчиков слов

1698
01:09:08,560 --> 01:09:10,799
является традиционной, это то, что вы видите в

1699
01:09:10,799 --> 01:09:13,279
словарях, это обычно то, что

1700
01:09:13,279 --> 01:09:15,759
используется при обработке естественного языка, я

1701
01:09:15,759 --> 01:09:18,479
имею в виду, что это обычно  несовершенный по-своему

1702
01:09:18,479 --> 01:09:20,399
, потому что мы пытаемся взять все

1703
01:09:20,399 --> 01:09:22,799
варианты использования слова щука и как бы разрезать

1704
01:09:22,799 --> 01:09:27,759
их на ключевые различные датчики, где

1705
01:09:28,799 --> 01:09:29,839


1706
01:09:29,839 --> 01:09:32,238
разница в некотором роде перекрывается,

1707
01:09:32,238 --> 01:09:34,158
и часто неясно, какие из них

1708
01:09:34,158 --> 01:09:37,040
считать отдельными, например  здесь прямо

1709
01:09:37,040 --> 01:09:39,040
железнодорожная линия и тип дороги, ну

1710
01:09:39,040 --> 01:09:41,120
вроде как это то же самое чувство

1711
01:09:41,120 --> 01:09:42,479
щуки, просто это разные

1712
01:09:42,479 --> 01:09:44,640
виды транспорта, и поэтому вы знаете, что

1713
01:09:44,640 --> 01:09:46,399
это может быть вы знаете тип

1714
01:09:46,399 --> 01:09:48,399
транспортной линии и покрываете их обе,

1715
01:09:48,399 --> 01:09:51,279
так что  всегда очень непонятно,

1716
01:09:51,279 --> 01:09:54,560
как вы разрезаете значение слова на

1717
01:09:54,560 --> 01:09:56,719
разные датчики, и действительно, если вы посмотрите

1718
01:09:56,719 --> 01:09:58,640
на разные словари, все делают

1719
01:09:58,640 --> 01:10:01,520
это по-разному,

1720
01:10:01,520 --> 01:10:03,440
так

1721
01:10:03,440 --> 01:10:06,239
что на самом деле оказывается, что на практике

1722
01:10:06,239 --> 01:10:07,520
вы можете

1723
01:10:07,520 --> 01:10:11,600
неплохо справиться, просто имея один

1724
01:10:11,600 --> 01:10:14,239
вектор слова для каждого типа слова

1725
01:10:14,239 --> 01:10:17,440
и  что произойдет, если вы сделаете это

1726
01:10:17,440 --> 01:10:21,360
хорошо, вы обнаружите

1727
01:10:21,440 --> 01:10:22,239
,

1728
01:10:22,239 --> 01:10:24,000
что

1729
01:10:24,000 --> 01:10:26,719
то, что вы изучаете как вектор

1730
01:10:26,719 --> 01:10:27,440


1731
01:10:27,440 --> 01:10:30,400
слов, в модных разговорах называется

1732
01:10:30,400 --> 01:10:33,760


1733
01:10:33,760 --> 01:10:36,480
суперпозицией диф векторов слов для

1734
01:10:36,480 --> 01:10:39,360
ди  различные значения слова um, где

1735
01:10:39,360 --> 01:10:42,480
слово суперпозиция означает не больше и не меньше,

1736
01:10:42,480 --> 01:10:45,600
чем взвешенная сумма, поэтому наш вектор,

1737
01:10:45,600 --> 01:10:48,560
который мы выучили для пики, будет

1738
01:10:48,560 --> 01:10:50,560
средневзвешенным векторов, которые вы

1739
01:10:50,560 --> 01:10:52,719
выучили бы для средневекового

1740
01:10:52,719 --> 01:10:55,600
чувства оружия плюс чувство рыбы

1741
01:10:55,600 --> 01:10:57,440
плюс чувство дороги плюс любые другие

1742
01:10:57,440 --> 01:10:59,120
ощущения, которые у вас есть,

1743
01:10:59,120 --> 01:11:01,280
где вес, который дается

1744
01:11:01,280 --> 01:11:03,440
этим различным векторам чувств,

1745
01:11:03,440 --> 01:11:06,159
соответствует частотам

1746
01:11:06,159 --> 01:11:09,280
использования различных датчиков, поэтому мы получаем

1747
01:11:09,280 --> 01:11:10,400


1748
01:11:10,400 --> 01:11:14,320
слово um вектор для pike um, являющийся своего

1749
01:11:14,320 --> 01:11:16,560
рода средним  вектор,

1750
01:11:16,560 --> 01:11:18,560
и поэтому, если вы

1751
01:11:18,560 --> 01:11:20,480


1752
01:11:20,480 --> 01:11:21,760


1753
01:11:21,760 --> 01:11:23,920
говорите хорошо, вы только что сложили несколько

1754
01:11:23,920 --> 01:11:26,320
разных векторов в среднее, вы

1755
01:11:26,320 --> 01:11:29,520
можете подумать, что это бесполезно,

1756
01:11:29,520 --> 01:11:31,679
потому что вы знаете, что потеряли истинное

1757
01:11:31,679 --> 01:11:33,520
значение слова, и вы '  у нас только что есть

1758
01:11:33,520 --> 01:11:36,320
какой-то забавный средний вектор,

1759
01:11:36,320 --> 01:11:39,120
который находится между ними,

1760
01:11:39,120 --> 01:11:40,719
но на самом деле

1761
01:11:40,719 --> 01:11:42,719
оказывается, что если вы используете этот

1762
01:11:42,719 --> 01:11:44,400
средний вектор

1763
01:11:44,400 --> 01:11:48,600
в приложениях, он имеет тенденцию к

1764
01:11:48,600 --> 01:11:52,159
самоопределению, потому что если вы скажете, что

1765
01:11:52,159 --> 01:11:56,480
это слово p  Подобно слову

1766
01:11:56,480 --> 01:11:57,440
рыба,

1767
01:11:57,440 --> 01:12:01,040
часть этого вектора представляет рыбу,

1768
01:12:01,040 --> 01:12:03,920
чувство рыбы щуки, и поэтому в этих

1769
01:12:03,920 --> 01:12:06,239
компонентах он будет похож на

1770
01:12:06,239 --> 01:12:08,960
вектор рыбы, так что да, вы скажете,

1771
01:12:08,960 --> 01:12:10,320
что сущность

1772
01:12:10,320 --> 01:12:12,239
есть существенное

1773
01:12:12,239 --> 01:12:15,920
сходство, тогда как если  в другом

1774
01:12:15,920 --> 01:12:18,880
фрагменте текста, в котором говорится, что вы знаете, что люди

1775
01:12:18,880 --> 01:12:21,440
были нацелены, были вооружены пиками и

1776
01:12:21,440 --> 01:12:23,840
копьями, пиками и столами или любым

1777
01:12:23,840 --> 01:12:26,560
другим средневековым оружием, которое вы хорошо помните, на

1778
01:12:26,560 --> 01:12:28,080
самом деле

1779
01:12:28,080 --> 01:12:30,000
часть этого значения также присутствует в

1780
01:12:30,000 --> 01:12:32,320
векторе пики, и поэтому он скажет  да,

1781
01:12:32,320 --> 01:12:34,239
есть хорошее сходство

1782
01:12:34,239 --> 01:12:35,440
с

1783
01:12:35,440 --> 01:12:37,679
булавой, посохом и такими словами,

1784
01:12:37,679 --> 01:12:41,360
и на самом деле мы можем выяснить, какое

1785
01:12:41,360 --> 01:12:44,159
значение имеет смысл пики,

1786
01:12:44,159 --> 01:12:46,239
просто увидев, какие

1787
01:12:46,239 --> 01:12:48,320
компоненты похожи на другие слова

1788
01:12:48,320 --> 01:12:51,600
, которые используются в том же контексте,

1789
01:12:51,600 --> 01:12:54,080
и действительно есть  гораздо более

1790
01:12:54,080 --> 01:12:56,480
удивительный результат, чем это, и

1791
01:12:56,480 --> 01:12:59,040
это результат, который гм евреи - это санджив

1792
01:12:59,040 --> 01:13:00,239
аврора

1793
01:13:00,239 --> 01:13:02,560
тангума, который сейчас учится на нашем стэнфордском

1794
01:13:02,560 --> 01:13:06,080
факультете и других в 2018 году, и

1795
01:13:06,080 --> 01:13:07,520
это следующий результат, который я не знаю.

1796
01:13:07,520 --> 01:13:11,280
В общем, собираюсь объяснить,

1797
01:13:11,280 --> 01:13:14,560
но если вы думаете, что вектор для щуки

1798
01:13:14,560 --> 01:13:17,920
- это просто сумма векторов

1799
01:13:17,920 --> 01:13:20,719
для разных датчиков,

1800
01:13:20,719 --> 01:13:22,480


1801
01:13:22,480 --> 01:13:24,880
вы должны подумать, что просто

1802
01:13:24,880 --> 01:13:26,320


1803
01:13:26,320 --> 01:13:29,440
невозможно восстановить смысловые векторы

1804
01:13:29,440 --> 01:13:34,719
из вектора для гм  тип слова, потому что

1805
01:13:34,719 --> 01:13:38,080
обычно, если я говорю, что у меня два числа,

1806
01:13:38,080 --> 01:13:40,800
их сумма равна 17, у вас просто нет

1807
01:13:40,800 --> 01:13:42,719
информации о том, какие мои два числа

1808
01:13:42,719 --> 01:13:45,280
верны, вы не можете решить его, и

1809
01:13:45,280 --> 01:13:47,199
даже хуже, если я скажу вам, что у меня есть  три

1810
01:13:47,199 --> 01:13:50,560
числа, и их сумма составляет 17,

1811
01:13:50,560 --> 01:13:51,440
но

1812
01:13:51,440 --> 01:13:53,440
оказывается, что когда у нас есть эти

1813
01:13:53,440 --> 01:13:56,000
многомерные векторные пространства

1814
01:13:56,000 --> 01:13:56,880
,

1815
01:13:56,880 --> 01:13:59,280
в этих многомерных векторных пространствах вещи настолько редки,

1816
01:13:59,280 --> 01:14:01,840
что вы можете

1817
01:14:01,840 --> 01:14:05,600
использовать идеи разреженного кодирования для фактического

1818
01:14:05,600 --> 01:14:08,159
разделения различных датчиков при

1819
01:14:08,159 --> 01:14:11,360
условии, что они  относительно распространены,

1820
01:14:11,360 --> 01:14:13,840
поэтому они показывают в своей статье, что вы можете

1821
01:14:13,840 --> 01:14:16,560
начать с вектора, скажем, щука, и

1822
01:14:16,560 --> 01:14:19,440
фактически отделить компоненты этого

1823
01:14:19,440 --> 01:14:21,440
вектора, которые соответствуют различным

1824
01:14:21,440 --> 01:14:23,760
датчикам слова щука, и вот

1825
01:14:23,760 --> 01:14:26,000
пример в бо  В начале этого слайда,

1826
01:14:26,000 --> 01:14:29,520
который представляет собой слово,

1827
01:14:29,520 --> 01:14:31,840
он разделил этот вектор на пять

1828
01:14:31,840 --> 01:14:34,159
различных датчиков, и поэтому в одном

1829
01:14:34,159 --> 01:14:36,880
смысле он близок к словам брюки,

1830
01:14:36,880 --> 01:14:38,800
блузка, жилет, так что это вид

1831
01:14:38,800 --> 01:14:42,560
одежды, чувство галстука, другие датчики

1832
01:14:42,560 --> 01:14:44,480
, близкие к проводам, кабелям, проводке,

1833
01:14:44,480 --> 01:14:46,880
электричеству.  Так что это своего рода

1834
01:14:46,880 --> 01:14:48,719
тайский стиль одежды, используемый в

1835
01:14:48,719 --> 01:14:50,960
электрике, а затем у нас есть своего

1836
01:14:50,960 --> 01:14:53,760
рода уравнитель голов, так что

1837
01:14:53,760 --> 01:14:56,640
это ощущение ничьей в

1838
01:14:56,640 --> 01:14:59,280


1839
01:14:59,280 --> 01:15:02,080
спортивной игре.

1840
01:15:02,080 --> 01:15:03,920
вот, наконец, вот этот

1841
01:15:03,920 --> 01:15:06,480


1842
01:15:06,480 --> 01:15:08,239


1843
01:15:08,239 --> 01:15:09,840


1844
01:15:09,840 --> 01:15:11,840


1845
01:15:11,840 --> 01:15:15,159


