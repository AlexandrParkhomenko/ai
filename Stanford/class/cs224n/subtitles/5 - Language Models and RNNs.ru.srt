1
00:00:04,960 --> 00:00:08,160
Итак, мы начинаем гм на третьей неделе

2
00:00:08,160 --> 00:00:11,759
с пятой лекции,

3
00:00:11,759 --> 00:00:13,040
так что,

4
00:00:13,040 --> 00:00:15,839
к сожалению, на последнем

5
00:00:15,839 --> 00:00:17,840
занятии я думаю, я действительно отстал и пошел немного

6
00:00:17,840 --> 00:00:20,480
медленно, я думаю, мне просто нужно

7
00:00:20,480 --> 00:00:22,800
слишком много говорить о естественных языках, и поэтому

8
00:00:22,800 --> 00:00:24,960
я так и не получил  изюминка,

9
00:00:24,960 --> 00:00:26,480
показывающая, как вы могли бы делать хорошие вещи

10
00:00:26,480 --> 00:00:28,640
с парсером нейронных зависимостей, поэтому

11
00:00:28,640 --> 00:00:31,039
сегодня для первой части я в некотором

12
00:00:31,039 --> 00:00:32,880
смысле закончу содержание прошлого

13
00:00:32,880 --> 00:00:35,120
раза и расскажу о разборе нейронных зависимостей,

14
00:00:35,120 --> 00:00:37,120
который также дает нам

15
00:00:37,120 --> 00:00:40,480
возможность ввести  простой

16
00:00:40,480 --> 00:00:43,440
классификатор нейронных сетей с прямой связью

17
00:00:43,440 --> 00:00:46,399
, который затем приведет к небольшому количеству

18
00:00:46,399 --> 00:00:48,320
фоновых вещей, которые вам нужно

19
00:00:48,320 --> 00:00:50,239
знать о содержании нейронных сетей,

20
00:00:50,239 --> 00:00:51,680
потому что

21
00:00:51,680 --> 00:00:53,120
суть в том, что вам нужно знать много вещей

22
00:00:53,120 --> 00:00:55,760
о нейронных сетях и  затем, после того,

23
00:00:55,760 --> 00:00:58,000
как оба этих момента, я перейду к

24
00:00:58,000 --> 00:00:59,840
тому, что действительно должно быть темой

25
00:00:59,840 --> 00:01:02,559
сегодняшней лекции, которая рассматривает

26
00:01:02,559 --> 00:01:05,040
языковое моделирование и рекуррентные нейронные

27
00:01:05,040 --> 00:01:07,200
сети, а затем перейду к

28
00:01:07,200 --> 00:01:08,880


29
00:01:08,880 --> 00:01:11,760
Эти две вещи являются важными темами

30
00:01:11,760 --> 00:01:14,159
, о которых мы потом будем говорить на самом

31
00:01:14,159 --> 00:01:16,720
деле всю следующую неделю, а также

32
00:01:16,720 --> 00:01:18,400
есть пара напоминаний, прежде чем мы начнем.

33
00:01:18,400 --> 00:01:20,960
Во-первых

34
00:01:20,960 --> 00:01:22,240
, вы должны были сдать

35
00:01:22,240 --> 00:01:24,400
задание 2, прежде чем

36
00:01:24,400 --> 00:01:27,360
присоединиться к этому классу.  и, в свою очередь, задание

37
00:01:27,360 --> 00:01:29,439
три сегодня отсутствует, и это

38
00:01:29,439 --> 00:01:32,159
задание, в котором вы собираетесь

39
00:01:32,159 --> 00:01:34,560
создать по существу новый

40
00:01:34,560 --> 00:01:36,799
анализатор зависимостей, который я

41
00:01:36,799 --> 00:01:39,360
собираюсь представить в pytorch, поэтому часть

42
00:01:39,360 --> 00:01:42,079
роли этого задания на самом деле состоит в том, чтобы

43
00:01:42,079 --> 00:01:44,960
поднять вас  для ускорения работы с pytorch, поэтому это

44
00:01:44,960 --> 00:01:48,159
задание очень усложнено

45
00:01:48,159 --> 00:01:50,320
множеством комментариев и подсказок о том, что

46
00:01:50,320 --> 00:01:53,040
делать, и поэтому есть надежда, что

47
00:01:53,040 --> 00:01:54,960
к тому времени, когда вы подойдете к его концу,

48
00:01:54,960 --> 00:01:57,360
вы почувствуете себя довольно знакомым и

49
00:01:57,360 --> 00:02:00,000
комфортным с пирогом, не  Забудьте,

50
00:02:00,000 --> 00:02:02,000
на прошлой неделе был также учебник по пирогу,

51
00:02:02,000 --> 00:02:03,680
если вы не уловили его в

52
00:02:03,680 --> 00:02:05,759
то время, когда вы, возможно, захотите вернуться и

53
00:02:05,759 --> 00:02:08,399
посмотреть видео,

54
00:02:08,399 --> 00:02:09,840
еще одна вещь, которую следует упомянуть о

55
00:02:09,840 --> 00:02:12,800
заданиях, это то, что задание три

56
00:02:12,800 --> 00:02:15,920
является последним заданием.  Ent, где наша отличная

57
00:02:15,920 --> 00:02:18,959
команда экспертов будет рада взглянуть на ваш код

58
00:02:18,959 --> 00:02:21,440
и разобраться с вашими ошибками,

59
00:02:21,440 --> 00:02:23,920
так что, возможно, воспользуйтесь этим, но не

60
00:02:23,920 --> 00:02:25,120
слишком много,

61
00:02:25,120 --> 00:02:26,959
но начните задание четыре для

62
00:02:26,959 --> 00:02:28,640
заданий четыре пять и последний

63
00:02:28,640 --> 00:02:31,360
проект эм, участники очень счастливы  чтобы

64
00:02:31,360 --> 00:02:33,519
помочь в целом, но это просто

65
00:02:33,519 --> 00:02:35,760
не их работа, чтобы на самом деле

66
00:02:35,760 --> 00:02:37,599
разбирать ошибки для вас,

67
00:02:37,599 --> 00:02:39,680
вы должны смотреть на свой код и

68
00:02:39,680 --> 00:02:41,680
обсуждать идеи, концепции и

69
00:02:41,680 --> 00:02:43,519
причины, по которым что-то может с ними не работать,

70
00:02:43,519 --> 00:02:45,599


71
00:02:45,599 --> 00:02:47,280
хорошо, поэтому, если вы помните, где мы были

72
00:02:47,280 --> 00:02:48,959
в прошлый раз

73
00:02:48,959 --> 00:02:51,280
я представил идею

74
00:02:51,280 --> 00:02:54,879
синтаксических анализаторов зависимостей на основе переходов и что это

75
00:02:54,879 --> 00:02:58,000
был эффективный метод линейного времени

76
00:02:58,000 --> 00:03:00,239
для создания синтаксической структуры текста на

77
00:03:00,239 --> 00:03:02,159
естественном языке,

78
00:03:02,159 --> 00:03:03,200
и

79
00:03:03,200 --> 00:03:05,440
что они работали довольно хорошо до того, как

80
00:03:05,440 --> 00:03:07,840
появились нейронные сети и снова взяли на себя nlp,

81
00:03:07,840 --> 00:03:10,959
но они  имели некоторые недостатки,

82
00:03:10,959 --> 00:03:13,680
и их самый большой недостаток заключается в том, что,

83
00:03:13,680 --> 00:03:15,680
как и большинство моделей машинного обучения

84
00:03:15,680 --> 00:03:18,159
того времени, они работали с

85
00:03:18,159 --> 00:03:21,120
функциями индикаторов, что означает, что вы

86
00:03:21,120 --> 00:03:24,319
указываете  какое-то условие, а затем

87
00:03:24,319 --> 00:03:25,680
проверка, было ли оно верным для

88
00:03:25,680 --> 00:03:28,400
конфигурации, чтобы что-то вроде слова

89
00:03:28,400 --> 00:03:30,560
в верхней части стека было хорошим, и это

90
00:03:30,560 --> 00:03:32,480
часть речи была прилагательным,

91
00:03:32,480 --> 00:03:35,440
или следующее подходящее слово было

92
00:03:35,440 --> 00:03:37,200
личным местоимением, что это

93
00:03:37,200 --> 00:03:40,000
условия, которые будут функциями

94
00:03:40,000 --> 00:03:43,120
и обычный

95
00:03:43,120 --> 00:03:45,840
анализатор зависимостей на основе переходов um,

96
00:03:45,840 --> 00:03:48,400
и каковы проблемы с тем, чтобы сделать это

97
00:03:48,400 --> 00:03:51,519
хорошо? Одна проблема заключается в том, что эти

98
00:03:51,519 --> 00:03:53,439
функции очень

99
00:03:53,439 --> 00:03:56,159
редки.Второй проблемой являются неполные функции,

100
00:03:56,159 --> 00:03:59,360
что я имею в виду,

101
00:03:59,360 --> 00:04:00,400
зависит

102
00:04:00,400 --> 00:04:01,200
от

103
00:04:01,200 --> 00:04:02,080
того, какие

104
00:04:02,080 --> 00:04:04,560
слова и конфигурации произошли в  В

105
00:04:04,560 --> 00:04:07,599
обучающих данных есть определенные функции,

106
00:04:07,599 --> 00:04:10,000
которые будут существовать, потому что вы как бы

107
00:04:10,000 --> 00:04:12,560
видели определенное слово, предшествующее глаголу, и

108
00:04:12,560 --> 00:04:14,319
определенные функции, которых просто не будет,

109
00:04:14,319 --> 00:04:16,478
потому что это слово никогда не встречалось

110
00:04:16,478 --> 00:04:18,399
перед глаголом в данных обучения,

111
00:04:18,399 --> 00:04:20,560
но, возможно, самая большая проблема и

112
00:04:20,560 --> 00:04:22,000
возможность

113
00:04:22,000 --> 00:04:23,520
для  Лучше всего с нейронным

114
00:04:23,520 --> 00:04:26,479
анализатором зависимостей оказывается,

115
00:04:26,479 --> 00:04:29,759
что в анализаторе символьных зависимостей,

116
00:04:29,759 --> 00:04:31,840
вычисляющем все эти функции  просто

117
00:04:31,840 --> 00:04:34,000
оказывается на самом деле довольно дорого, потому что,

118
00:04:34,000 --> 00:04:35,919
хотя фактическая система перехода,

119
00:04:35,919 --> 00:04:39,199
которую я показал в прошлый раз, работает быстро и

120
00:04:39,199 --> 00:04:41,280
эффективно, вам на самом деле нужно

121
00:04:41,280 --> 00:04:44,479
вычислить все эти функции, и

122
00:04:44,479 --> 00:04:46,960
вы обнаружили, что примерно 95%

123
00:04:46,960 --> 00:04:50,000
времени анализа одного из них  models было

124
00:04:50,000 --> 00:04:52,639
потрачено только на вычисление всех

125
00:04:52,639 --> 00:04:54,840
функций каждой

126
00:04:54,840 --> 00:04:57,600
конфигурации, так что это говорит о том, что,

127
00:04:57,600 --> 00:04:59,520
возможно, мы можем добиться большего с помощью нейронного

128
00:04:59,520 --> 00:05:01,199
подхода, когда мы собираемся изучить

129
00:05:01,199 --> 00:05:03,919
плотное и компактное представление функций,

130
00:05:03,919 --> 00:05:05,440
и это то, что я хочу пройти

131
00:05:05,440 --> 00:05:06,880
сейчас,

132
00:05:06,880 --> 00:05:09,280
так что это  раз у нас все еще будет

133
00:05:09,280 --> 00:05:13,120
точно такая же

134
00:05:13,120 --> 00:05:15,280
конфигурация стека и буфера и будет запущена

135
00:05:15,280 --> 00:05:16,880
точно такая же

136
00:05:16,880 --> 00:05:20,080
последовательность переходов, за исключением этого раза,

137
00:05:20,080 --> 00:05:21,520
вместо того, чтобы представлять

138
00:05:21,520 --> 00:05:23,680
конфигурацию стека и буфера с

139
00:05:23,680 --> 00:05:26,000
помощью нескольких миллионов символических

140
00:05:26,000 --> 00:05:28,160
функций, которые мы вместо этого

141
00:05:28,160 --> 00:05:32,000
суммируем эту конфигурацию как плотный

142
00:05:32,000 --> 00:05:34,160
вектор размерности, возможно,

143
00:05:34,160 --> 00:05:36,960
приблизительно тысячу,

144
00:05:36,960 --> 00:05:38,720
и наш нейронный подход будет

145
00:05:38,720 --> 00:05:40,800
изучать  представляет собой плотное и компактное представление функций,

146
00:05:40,800 --> 00:05:42,560


147
00:05:42,560 --> 00:05:45,120
и поэтому довольно подробно то, что я собираюсь

148
00:05:45,120 --> 00:05:46,080


149
00:05:46,080 --> 00:05:48,960
вам сейчас вкратце показать, и то, что вы

150
00:05:48,960 --> 00:05:51,039
собираетесь реализовать,

151
00:05:51,039 --> 00:05:53,360
по сути является парсером нейронных зависимостей,

152
00:05:53,360 --> 00:05:55,840
который был разработан dante chen

153
00:05:55,840 --> 00:05:57,919
в 2014 году

154
00:05:57,919 --> 00:05:59,600
и позволяет

155
00:05:59,600 --> 00:06:01,360
перейти к рекламе прямо в

156
00:06:01,360 --> 00:06:04,720
Начиная с того, как это работает так хорошо,

157
00:06:04,720 --> 00:06:06,880
это те результаты, которые вы

158
00:06:06,880 --> 00:06:09,039
получили от этого, используя меры, которые я

159
00:06:09,039 --> 00:06:11,039
представил в последний раз:

160
00:06:11,039 --> 00:06:13,120
немаркированный показатель привязанности, правильно ли вы

161
00:06:13,120 --> 00:06:15,759
прикрепляете зависимости к

162
00:06:15,759 --> 00:06:18,800
правильному слову и помеченный

163
00:06:18,800 --> 00:06:20,880
балл привязанности относительно  правильно ли вы получаете

164
00:06:20,880 --> 00:06:22,639
тип грамматической связи этой

165
00:06:22,639 --> 00:06:24,319
зависимости,

166
00:06:24,319 --> 00:06:27,680
и поэтому, по сути, этот анализатор подбородка и

167
00:06:27,680 --> 00:06:31,919
укомплектования людьми дал нейронную версию

168
00:06:31,919 --> 00:06:34,080
чего-то вроде анализатора зависимостей на основе переходов,

169
00:06:34,080 --> 00:06:36,400
такого как синтаксический анализатор солода в

170
00:06:36,400 --> 00:06:37,360
желтом цвете,

171
00:06:37,360 --> 00:06:39,680
и интересно было

172
00:06:39,680 --> 00:06:42,800
то, что  нейронный классификатор

173
00:06:42,800 --> 00:06:44,639
таким образом, что я собираюсь

174
00:06:44,639 --> 00:06:45,440


175
00:06:45,440 --> 00:06:47,360
объяснить, что он может произвести

176
00:06:47,360 --> 00:06:49,599
что-то, что было примерно на два процента

177
00:06:49,599 --> 00:06:51,280
более точным  чем

178
00:06:51,280 --> 00:06:54,319
анализатор символьных зависимостей, и из-за

179
00:06:54,319 --> 00:06:55,680
того, что он не выполняет все

180
00:06:55,680 --> 00:06:58,880
вычисления символьных признаков, несмотря на

181
00:06:58,880 --> 00:07:00,720
то, что вы можете сначала подумать,

182
00:07:00,720 --> 00:07:02,960
что в парсере нейронных зависимостей много математики вещественных чисел и

183
00:07:02,960 --> 00:07:05,599
матричных векторных умножений,

184
00:07:05,599 --> 00:07:07,840
он действительно работал

185
00:07:07,840 --> 00:07:10,400
заметно быстрее  чем

186
00:07:10,400 --> 00:07:12,639
анализатор символьных зависимостей, потому что в нем не было

187
00:07:12,639 --> 00:07:14,840
вычислений всех функций.

188
00:07:14,840 --> 00:07:17,120
Другой важный подход к

189
00:07:17,120 --> 00:07:19,199
синтаксическому анализу зависимостей, который я также показываю

190
00:07:19,199 --> 00:07:22,400
здесь и к которому я вернусь в конце, - это

191
00:07:22,400 --> 00:07:24,080
то, что называется анализом зависимостей на основе графа

192
00:07:24,080 --> 00:07:26,240
и поэтому это

193
00:07:26,240 --> 00:07:28,880
другой подход к синтаксическому анализу зависимостей,

194
00:07:28,880 --> 00:07:31,280
и поэтому это два символьных

195
00:07:31,280 --> 00:07:34,479
анализатора зависимостей на основе графов, и

196
00:07:34,479 --> 00:07:36,560
в пре-нейронном мире они были

197
00:07:36,560 --> 00:07:38,560
несколько более точными,

198
00:07:38,560 --> 00:07:40,800
чем синтаксический анализ на основе переходов, как вы

199
00:07:40,800 --> 00:07:41,759
могли видеть,

200
00:07:41,759 --> 00:07:44,080
но с другой стороны они были близки к

201
00:07:44,080 --> 00:07:47,280
на два порядка медленнее, и поэтому, по

202
00:07:47,280 --> 00:07:48,879
сути, с

203
00:07:48,879 --> 00:07:51,520
синтаксическим анализатором chern manning мы смогли обеспечить то,

204
00:07:51,520 --> 00:07:53,440
что было в основном столь же точным  e как

205
00:07:53,440 --> 00:07:55,680
лучшие анализаторы зависимостей на основе графиков,

206
00:07:55,680 --> 00:07:58,160
которые были лучшими анализаторами зависимостей

207
00:07:58,160 --> 00:08:00,080
, работая примерно на два

208
00:08:00,080 --> 00:08:02,479
порядка быстрее,

209
00:08:02,479 --> 00:08:05,199
поэтому как мы это сделали, на самом деле это была

210
00:08:05,199 --> 00:08:08,240
очень простая

211
00:08:08,240 --> 00:08:10,080
реализация, которая является частью того, что

212
00:08:10,080 --> 00:08:11,680
делает его отличным

213
00:08:11,680 --> 00:08:14,160
для выполнения для назначения трех

214
00:08:14,160 --> 00:08:17,039
um  но именно так мы это сделали и получили

215
00:08:17,039 --> 00:08:19,199
победы, поэтому первая победа, о которой

216
00:08:19,199 --> 00:08:21,520
мы уже много говорили,

217
00:08:21,520 --> 00:08:24,000
начиная с первой недели, - это использование

218
00:08:24,000 --> 00:08:26,479
распределенных представлений, поэтому мы

219
00:08:26,479 --> 00:08:29,440
представляем каждое слово как встраивание слова,

220
00:08:29,440 --> 00:08:31,759
и у вас есть  у нас уже есть большой опыт в

221
00:08:31,759 --> 00:08:35,039
этом, и это означает, что

222
00:08:35,039 --> 00:08:37,200
когда слова не были замечены в конкретной

223
00:08:37,200 --> 00:08:39,679
конфигурации, мы все еще знаем, на что они

224
00:08:39,679 --> 00:08:41,760
похожи, потому что они будут, мы видели

225
00:08:41,760 --> 00:08:43,839
похожие слова в правильной

226
00:08:43,839 --> 00:08:45,600
конфигурации,

227
00:08:45,600 --> 00:08:48,560
но мы не  Не останавливаемся только на встраивании слов,

228
00:08:48,560 --> 00:08:50,240


229
00:08:50,240 --> 00:08:52,160
другие вещи, которые являются центральными для нашего

230
00:08:52,160 --> 00:08:54,560
синтаксического анализатора зависимостей, - это части

231
00:08:54,560 --> 00:08:57,120
речи слов и

232
00:08:57,120 --> 00:09:01,120
метки зависимостей, поэтому мы решили сделать так

233
00:09:01,120 --> 00:09:03,279
, чтобы  ch меньшие

234
00:09:03,279 --> 00:09:04,480
наборы,

235
00:09:04,480 --> 00:09:06,880
поэтому количество меток зависимостей составляет около 40

236
00:09:06,880 --> 00:09:09,360
, а части речи

237
00:09:09,360 --> 00:09:11,440
примерно того же порядка, иногда

238
00:09:11,440 --> 00:09:14,000
меньше, иногда больше, что даже внутри

239
00:09:14,000 --> 00:09:17,040
этих наборов категорий есть те

240
00:09:17,040 --> 00:09:19,600
, которые очень сильно связаны, поэтому мы

241
00:09:19,600 --> 00:09:22,399
также приняли распределенные

242
00:09:22,399 --> 00:09:25,040
представления um  для них, например,

243
00:09:25,040 --> 00:09:26,800
могут быть части речи для

244
00:09:26,800 --> 00:09:29,279
существительных в единственном числе и существительных во множественном числе, и в

245
00:09:29,279 --> 00:09:31,120
основном они ведут себя

246
00:09:31,120 --> 00:09:33,600
одинаково, и есть

247
00:09:33,600 --> 00:09:36,320
модификаторы прилагательных и числовые модификаторы, так

248
00:09:36,320 --> 00:09:38,000
что это просто числа, такие как три, четыре,

249
00:09:38,000 --> 00:09:40,560
пять и снова много времени  они

250
00:09:40,560 --> 00:09:43,200
ведут себя так же, как у вас есть и

251
00:09:43,200 --> 00:09:47,519
три коровы, и бурые коровы.

252
00:09:47,519 --> 00:09:49,839
Хорошо, поэтому все будет

253
00:09:49,839 --> 00:09:51,720
представлено в распределенном

254
00:09:51,720 --> 00:09:54,959
представлении, поэтому на этом этапе у нас есть

255
00:09:54,959 --> 00:09:58,000
точно такая же конфигурация, в

256
00:09:58,000 --> 00:10:01,040
которой у нас есть наш стек, наш буфер, и

257
00:10:01,040 --> 00:10:04,800
мы начали  построить несколько дуг, и поэтому

258
00:10:04,800 --> 00:10:05,920


259
00:10:05,920 --> 00:10:08,079
решения по классификации следующего

260
00:10:08,079 --> 00:10:10,720
перехода будут приниматься из

261
00:10:10,720 --> 00:10:12,880
нескольких элементов этой конфигурации.  ion, поэтому

262
00:10:12,880 --> 00:10:14,320
мы смотрим на верхний элемент в

263
00:10:14,320 --> 00:10:17,440
стеке, второй элемент в стеке,

264
00:10:17,440 --> 00:10:19,760
первое слово в буфере, а затем мы

265
00:10:19,760 --> 00:10:21,440
фактически добавили некоторые дополнительные

266
00:10:21,440 --> 00:10:23,920
функции, которые затем в той степени, в

267
00:10:23,920 --> 00:10:26,399
которой мы уже построили дуги для слов

268
00:10:26,399 --> 00:10:29,040
в стеке мы можем смотреть

269
00:10:29,040 --> 00:10:31,519
на зависимость слева и справа от

270
00:10:31,519 --> 00:10:33,680
тех слов, которые находятся в стеке,

271
00:10:33,680 --> 00:10:35,920
которые уже находятся в наборах дуг,

272
00:10:35,920 --> 00:10:38,880
и поэтому для каждой из этих вещей

273
00:10:38,880 --> 00:10:40,480
есть слово,

274
00:10:40,480 --> 00:10:42,880
есть часть речи

275
00:10:42,880 --> 00:10:45,440
и для некоторых из них существует

276
00:10:45,440 --> 00:10:48,320
зависимость um, где она уже

277
00:10:48,320 --> 00:10:51,120
подключена к чему-то еще,

278
00:10:51,120 --> 00:10:54,240
например, левый угол s2 здесь имеет

279
00:10:54,240 --> 00:10:56,959
подзависимость n обратно ко второму

280
00:10:56,959 --> 00:10:58,399
элементу в стеке,

281
00:10:58,399 --> 00:11:00,720
поэтому мы можем взять эти элементы

282
00:11:00,720 --> 00:11:03,760
конфигурации и  можем

283
00:11:03,760 --> 00:11:05,839
найти встраивание каждого из них, чтобы у нас

284
00:11:05,839 --> 00:11:07,279
были вложения слов, часть вложений речи

285
00:11:07,279 --> 00:11:09,360
и встраивания зависимостей, и

286
00:11:09,360 --> 00:11:11,839
просто объединить их все вместе,

287
00:11:11,839 --> 00:11:13,600
как мы делали раньше с оконным

288
00:11:13,600 --> 00:11:16,079
классификатором, и это даст нам более новое

289
00:11:16,079 --> 00:11:19,680
представление  конфигурации

290
00:11:19,680 --> 00:11:21,920
теперь есть вторая причина, по которой мы можем

291
00:11:21,920 --> 00:11:23,040
надеяться на победу

292
00:11:23,040 --> 00:11:25,600
, используя классификатор глубокого обучения для

293
00:11:25,600 --> 00:11:27,760
прогнозирования следующего перехода, и мы

294
00:11:27,760 --> 00:11:29,600
пока особо не говорили об этом,

295
00:11:29,600 --> 00:11:31,279
поэтому я просто хотел

296
00:11:31,279 --> 00:11:33,120
отклониться и сказать немного больше о

297
00:11:33,120 --> 00:11:36,079
что гм, так что простейший тип

298
00:11:36,079 --> 00:11:40,160
классификатора, который близок к тому, о чем

299
00:11:40,160 --> 00:11:41,680
мы говорили в нейронных

300
00:11:41,680 --> 00:11:45,279
моделях, - это мягкий классификатор максимума, так что

301
00:11:45,279 --> 00:11:48,320
если у нас есть d размерных векторов x

302
00:11:48,320 --> 00:11:51,600
и у нас есть классы y для назначения вещей

303
00:11:51,600 --> 00:11:52,560


304
00:11:52,560 --> 00:11:54,000
эм,

305
00:11:54,000 --> 00:11:57,600
извините, y  элемент набора классов um

306
00:11:57,600 --> 00:12:01,120
c для назначения вещей,

307
00:12:01,120 --> 00:12:03,600
затем мы можем построить классификатор softmax,

308
00:12:03,600 --> 00:12:05,440
используя распределение softmax, которое

309
00:12:05,440 --> 00:12:08,320
мы видели ранее, где мы выбираем

310
00:12:08,320 --> 00:12:11,040
классы на основе матрицы весов

311
00:12:11,040 --> 00:12:12,079


312
00:12:12,079 --> 00:12:13,839
c на d,

313
00:12:13,839 --> 00:12:16,880
и мы обучаемся на контролируемых данных

314
00:12:16,880 --> 00:12:19,440
значения этой весовой матрицы w для

315
00:12:19,440 --> 00:12:22,160
минимизации потери нашего отрицательного логарифмического

316
00:12:22,160 --> 00:12:24,639
правдоподобия, которую мы видели до того,

317
00:12:24,639 --> 00:12:26,800
как потеря также часто

318
00:12:26,800 --> 00:12:28,880
называется кросс-энтропийной потерей - термин, который вы

319
00:12:28,880 --> 00:12:32,000
увидите в пироге среди других мест,

320
00:12:32,000 --> 00:12:34,560
так что это  простой

321
00:12:34,560 --> 00:12:36,560
классификатор машинного обучения, и если вы выполнили

322
00:12:36,560 --> 00:12:40,320
229 и видели классификаторы

323
00:12:40,320 --> 00:12:43,839
softmax, ммм, но простой классификатор softmax,

324
00:12:43,839 --> 00:12:44,720
такой как этот, используется

325
00:12:44,720 --> 00:12:47,680
совместно с большинством традиционных

326
00:12:47,680 --> 00:12:50,320
классификаторов машинного обучения, поэтому модели включают

327
00:12:50,320 --> 00:12:52,240
наивные байесовские модели, поддерживающие

328
00:12:52,240 --> 00:12:54,800
логистическую регрессию векторных машин,

329
00:12:54,800 --> 00:12:57,200
которая  В конце дня это не

330
00:12:57,200 --> 00:12:59,519
очень мощные классификаторы, это

331
00:12:59,519 --> 00:13:01,519
классификаторы, которые дают только линейные

332
00:13:01,519 --> 00:13:04,000
границы решения, и поэтому это может быть

333
00:13:04,000 --> 00:13:06,240
довольно ограничивающим, поэтому, если у вас есть

334
00:13:06,240 --> 00:13:08,000
сложная проблема, подобная той, которую я

335
00:13:08,000 --> 00:13:09,200
показываю

336
00:13:09,200 --> 00:13:11,680
на картинке в нижнем левом углу

337
00:13:11,680 --> 00:13:13,920
просто невозможно отделить

338
00:13:13,920 --> 00:13:16,399
зеленые точки от красных,

339
00:13:16,399 --> 00:13:18,480
просто проведя прямую линию, так что у вас

340
00:13:18,480 --> 00:13:20,000
будет довольно несовершенный

341
00:13:20,000 --> 00:13:21,360
классификатор,

342
00:13:21,360 --> 00:13:24,320
поэтому вторая большая победа нейронных

343
00:13:24,320 --> 00:13:27,279
классификаторов заключается в том, что они могут быть намного

344
00:13:27,279 --> 00:13:29,760
более мощными, потому что они могут  обеспечить

345
00:13:29,760 --> 00:13:32,000
нелинейную классификацию,

346
00:13:32,000 --> 00:13:33,600
поэтому вместо того, чтобы делать

347
00:13:33,600 --> 00:13:36,079
что-то вроде того, что показано на левом рисунке, мы

348
00:13:36,079 --> 00:13:38,480
можем придумать классификаторы, которые что-то

349
00:13:38,480 --> 00:13:40,560
делают  ng, как на правом изображении, и

350
00:13:40,560 --> 00:13:43,279
поэтому может разделять зеленую и

351
00:13:43,279 --> 00:13:46,320
красную точки um, помимо этих

352
00:13:46,320 --> 00:13:48,800
снимков, которые я взял из

353
00:13:48,800 --> 00:13:51,519
программного обеспечения compnet js Андре Капарти, которое является своего рода

354
00:13:51,519 --> 00:13:53,680
забавным маленьким инструментом, с которым можно поиграть, если

355
00:13:53,680 --> 00:13:56,079
вы  у нас есть немного свободного времени,

356
00:13:56,079 --> 00:13:58,639
и поэтому здесь происходит что-то тонкое,

357
00:13:58,639 --> 00:14:03,279
потому что наши более мощные

358
00:14:03,279 --> 00:14:06,000
классификаторы нейронных сетей в конце

359
00:14:06,000 --> 00:14:08,240
дня то, что у них наверху, является

360
00:14:08,240 --> 00:14:09,839
слоем softmax,

361
00:14:09,839 --> 00:14:13,120
и поэтому этот слой softmax

362
00:14:13,120 --> 00:14:16,240
действительно линейный  classifier, и это по-

363
00:14:16,240 --> 00:14:18,880
прежнему линейный классификатор, но то, что у

364
00:14:18,880 --> 00:14:21,760
них ниже, - это другие уровни

365
00:14:21,760 --> 00:14:25,360
нейронной сети, и так эффективно

366
00:14:25,360 --> 00:14:28,000
происходит то, что

367
00:14:28,000 --> 00:14:30,560
решения по классификации линейны в том, что касается верхнего

368
00:14:30,560 --> 00:14:34,000
softmax, но

369
00:14:34,000 --> 00:14:36,639
так точно нелинейны в исходном пространстве представления

370
00:14:36,639 --> 00:14:39,120
то, что может сделать нейронная сеть, - это

371
00:14:39,120 --> 00:14:40,160


372
00:14:40,160 --> 00:14:42,399
деформировать пространство и перемещать

373
00:14:42,399 --> 00:14:45,600
представление точек данных, чтобы обеспечить

374
00:14:45,600 --> 00:14:47,920
то, что в конце дня

375
00:14:47,920 --> 00:14:51,440
можно классифицировать с помощью линейного классификатора,

376
00:14:51,440 --> 00:14:54,000
и вот что  простой

377
00:14:54,000 --> 00:14:56,639
многоклассовый классификатор нейронной сети с прямой связью

378
00:14:56,639 --> 00:14:59,560
делает так, что он начинается с входного

379
00:14:59,560 --> 00:15:02,480
представления, так что это некоторое

380
00:15:02,480 --> 00:15:05,279
плотное представление входных данных, которые

381
00:15:05,279 --> 00:15:08,240
он пропускает через скрытый слой h с

382
00:15:08,240 --> 00:15:09,519


383
00:15:09,519 --> 00:15:12,000
матричным умножением, за которым следует

384
00:15:12,000 --> 00:15:14,720
нелинейность, так что матричное умножение

385
00:15:14,720 --> 00:15:17,040
может преобразовать  пространство и отображать вещи

386
00:15:17,040 --> 00:15:20,160
вокруг, а затем результат этого мы

387
00:15:20,160 --> 00:15:23,040
можем поместить в слой мягкого максимума и

388
00:15:23,040 --> 00:15:25,839
получить вероятности мягкого максимума, из

389
00:15:25,839 --> 00:15:27,920
которых мы принимаем решения по классификации,

390
00:15:27,920 --> 00:15:29,920


391
00:15:29,920 --> 00:15:30,880
и

392
00:15:30,880 --> 00:15:32,800
в той степени, в которой наши вероятности

393
00:15:32,800 --> 00:15:35,279
не назначают одну из правильных  Затем мы

394
00:15:35,279 --> 00:15:37,839
получаем некоторую потерю журнала или ошибку кросс-энтропии,

395
00:15:37,839 --> 00:15:40,720
которую мы обратно распространяем

396
00:15:40,720 --> 00:15:42,480
на параметры и вложения нашей

397
00:15:42,480 --> 00:15:44,160
модели,

398
00:15:44,160 --> 00:15:47,199
и по мере обучения, которое происходит через

399
00:15:47,199 --> 00:15:50,480
обратное распространение, мы все лучше

400
00:15:50,480 --> 00:15:53,279
узнаем параметры этого скрытого

401
00:15:53,279 --> 00:15:56,079
слоя модели, которые учатся повторно  представляют

402
00:15:56,079 --> 00:15:59,360
собой входные данные, которые они перемещают в

403
00:15:59,360 --> 00:16:02,560
промежуточном скрытом векторном пространстве, поэтому

404
00:16:02,560 --> 00:16:05,279
его можно легко классифицировать по тому, что

405
00:16:05,279 --> 00:16:08,959
в конце дня является  linear softmax,

406
00:16:08,959 --> 00:16:11,920
так что это, по сути, весь

407
00:16:11,920 --> 00:16:14,079
простой поток для мультиклассового классификатора нейронной сети,

408
00:16:14,079 --> 00:16:17,519
но гм, и если бы у нас

409
00:16:17,519 --> 00:16:20,160
было что-то вроде визуального сигнала, мы

410
00:16:20,160 --> 00:16:22,639
просто вроде как передавали прямо здесь

411
00:16:22,639 --> 00:16:24,880
реальные числа, и мы закончили, но

412
00:16:24,880 --> 00:16:27,680
обычно с гм  материала человеческого языка

413
00:16:27,680 --> 00:16:30,399
у нас фактически есть еще один

414
00:16:30,399 --> 00:16:32,880
слой, который мы загружаем до этого,

415
00:16:32,880 --> 00:16:36,000
потому что на самом деле под этим плотным входным

416
00:16:36,000 --> 00:16:38,639
слоем у нас фактически есть один горячий вектор

417
00:16:38,639 --> 00:16:40,720
для того, какие слова или части речи были

418
00:16:40,720 --> 00:16:42,959
задействованы, а затем мы выполняем процесс поиска,

419
00:16:42,959 --> 00:16:45,360
который  вы можете представить себе еще одно

420
00:16:45,360 --> 00:16:47,920
матричное умножение, чтобы преобразовать одни

421
00:16:47,920 --> 00:16:52,000
горячие черты в наш плотный входной слой.

422
00:16:52,000 --> 00:16:54,160
Хорошо, на моей картинке здесь есть еще одна

423
00:16:54,160 --> 00:16:55,759
особенность: я

424
00:16:55,759 --> 00:16:58,320
ввел другую нелинейность

425
00:16:58,320 --> 00:17:00,639
в скрытый слой, которая является выпрямленной

426
00:17:00,639 --> 00:17:02,240
линейной.  unit,

427
00:17:02,240 --> 00:17:03,920
и это то, что мы будем использовать сейчас

428
00:17:03,920 --> 00:17:06,400
парсеры нейронных зависимостей, это

429
00:17:06,400 --> 00:17:08,240
похоже на картинку в правом нижнем углу, и

430
00:17:08,240 --> 00:17:10,559
я вернусь к ней через несколько

431
00:17:10,559 --> 00:17:12,160
минут, это одна из дополнительных нейронных сетей.

432
00:17:12,160 --> 00:17:15,599
чистые вещи, о которых нужно поговорить,

433
00:17:15,599 --> 00:17:19,520
хорошо, поэтому наша архитектура модели анализатора зависимостей нейронной сети, по

434
00:17:19,520 --> 00:17:22,000
сути,

435
00:17:22,000 --> 00:17:25,959
именно такая, но применяется к

436
00:17:25,959 --> 00:17:29,120
конфигурации нашего

437
00:17:29,120 --> 00:17:32,960
анализатора зависимостей на

438
00:17:32,960 --> 00:17:35,039
основе переходов, поэтому на основе нашей конфигурации анализатора зависимостей на основе переходов

439
00:17:35,039 --> 00:17:38,000
мы создаем

440
00:17:38,000 --> 00:17:40,640
встраиваемый входной слой, просматривая

441
00:17:40,640 --> 00:17:42,400
различные элементы, как я обсуждал

442
00:17:42,400 --> 00:17:45,679
ранее, а затем мы передаем его через

443
00:17:45,679 --> 00:17:47,039
этот

444
00:17:47,039 --> 00:17:50,320
скрытый слой на слой softmax,

445
00:17:50,320 --> 00:17:53,039
чтобы получить вероятности, из которых мы можем

446
00:17:53,039 --> 00:17:55,919
выбрать, какое следующее действие,

447
00:17:55,919 --> 00:17:58,840
и это не сложнее, чем

448
00:17:58,840 --> 00:18:03,840
это гм, но мы обнаружили, что

449
00:18:03,840 --> 00:18:04,640
просто

450
00:18:04,640 --> 00:18:06,720
просто

451
00:18:06,720 --> 00:18:08,240
вы знаете, что в некотором смысле использование

452
00:18:08,240 --> 00:18:11,039
простейшего

453
00:18:11,039 --> 00:18:15,280
нейронного классификатора с прямой связью может обеспечить

454
00:18:15,280 --> 00:18:16,000


455
00:18:16,000 --> 00:18:18,000
очень точный

456
00:18:18,000 --> 00:18:20,080
анализатор зависимостей,

457
00:18:20,080 --> 00:18:21,760
который определяет структуру

458
00:18:21,760 --> 00:18:23,760
предложений, поддерживающих

459
00:18:23,760 --> 00:18:25,760
интерпретацию смысла таким способом, который я

460
00:18:25,760 --> 00:18:28,000
предложил в прошлый раз,

461
00:18:28,000 --> 00:18:30,400
вы действительно знаете, несмотря на то,

462
00:18:30,400 --> 00:18:33,280
что он  была довольно простой архитектурой

463
00:18:33,280 --> 00:18:36,559
в 2014 году это была первая успешная

464
00:18:36,559 --> 00:18:40,039
нейронная зависимость  анализатор и особенно плотные

465
00:18:40,039 --> 00:18:42,480
представления,

466
00:18:42,480 --> 00:18:44,559
но также частично нелинейность

467
00:18:44,559 --> 00:18:46,000
классификатора

468
00:18:46,000 --> 00:18:47,760
дала нам такой хороший результат, что он

469
00:18:47,760 --> 00:18:49,679
мог превзойти

470
00:18:49,679 --> 00:18:52,960
символьные синтаксические анализаторы с точки зрения точности

471
00:18:52,960 --> 00:18:55,200
и он мог превзойти их с точки зрения

472
00:18:55,200 --> 00:18:57,600
скорости

473
00:18:57,600 --> 00:19:00,559
um, так что это был 2014 год,

474
00:19:00,559 --> 00:19:03,360
просто быстро здесь  еще пара слайдов

475
00:19:03,360 --> 00:19:04,160
о

476
00:19:04,160 --> 00:19:06,720
том, что произошло с тех пор,

477
00:19:06,720 --> 00:19:09,039
поэтому многие люди были в восторге от

478
00:19:09,039 --> 00:19:11,679
успеха этого нового парсера зависимостей,

479
00:19:11,679 --> 00:19:13,600
и многие люди, особенно в

480
00:19:13,600 --> 00:19:16,559
Google, а затем и другие, о создании

481
00:19:16,559 --> 00:19:18,559
более изящного

482
00:19:18,559 --> 00:19:20,400


483
00:19:20,400 --> 00:19:22,480
парсера нейронных зависимостей на основе переходов, чтобы они исследовали более глубокие

484
00:19:22,480 --> 00:19:24,160
сети, нет  причина иметь только

485
00:19:24,160 --> 00:19:25,600
один скрытый слой, у

486
00:19:25,600 --> 00:19:27,520
вас может быть два скрытых слоя, вы

487
00:19:27,520 --> 00:19:29,360
можете выполнить поиск луча, о котором я кратко

488
00:19:29,360 --> 00:19:31,280
упомянул в прошлый раз,

489
00:19:31,280 --> 00:19:32,799
еще одна вещь, о которой я не собираюсь сейчас говорить,

490
00:19:32,799 --> 00:19:34,640
- это добавление

491
00:19:34,640 --> 00:19:36,559
условного

492
00:19:36,559 --> 00:19:39,360
вывода стиля случайного поля к последовательностям решений,

493
00:19:39,360 --> 00:19:43,440
а затем  в 2016 году возглавил модель,

494
00:19:43,440 --> 00:19:46,960
которую они назвали парси макпа, лицо,

495
00:19:46,960 --> 00:19:48,480
которое трудно сказать с серьезным

496
00:19:48,480 --> 00:19:51,679
лицом, ммм  Тогда он был примерно на

497
00:19:51,679 --> 00:19:54,160
два с половиной три процента

498
00:19:54,160 --> 00:19:55,679
точнее, чем модель, которую мы

499
00:19:55,679 --> 00:19:58,480
создали, но все еще в основном в том же

500
00:19:58,480 --> 00:20:02,000
семействе парсеров на основе переходов с

501
00:20:02,000 --> 00:20:03,919
классификатором нейронной сети для выбора

502
00:20:03,919 --> 00:20:07,039
следующего перехода

503
00:20:07,039 --> 00:20:08,559


504
00:20:08,559 --> 00:20:10,720
um альтернатива анализаторам на основе переходов в

505
00:20:10,720 --> 00:20:13,039
виде графа  анализаторы зависимостей на основе

506
00:20:13,039 --> 00:20:15,840
и для парсера зависимостей на основе графа

507
00:20:15,840 --> 00:20:17,200


508
00:20:17,200 --> 00:20:18,880
то, что вы делаете, эффективно

509
00:20:18,880 --> 00:20:22,159
учитывает каждую пару слов и

510
00:20:22,159 --> 00:20:24,080
рассматривает слово как зависящее от

511
00:20:24,080 --> 00:20:27,280
корня, и вы придумываете оценку

512
00:20:27,280 --> 00:20:29,840
того, насколько вероятно, что он

513
00:20:29,840 --> 00:20:32,799
такой большой  зависит от корня или насколько

514
00:20:32,799 --> 00:20:34,400
велика вероятность быть

515
00:20:34,400 --> 00:20:36,720
зависимой от кота, и аналогично для любого

516
00:20:36,720 --> 00:20:39,679
другого слова для слова sat um насколько

517
00:20:39,679 --> 00:20:40,880
вероятно,

518
00:20:40,880 --> 00:20:43,039
что оно будет зависеть от корня или

519
00:20:43,039 --> 00:20:45,440
зависит от и т. д.,

520
00:20:45,440 --> 00:20:48,159
и хорошо, чтобы сделать это хорошо, вам нужно

521
00:20:48,159 --> 00:20:50,400
знать больше, чем просто два

522
00:20:50,400 --> 00:20:53,600
слова, которые задействованы, и

523
00:20:53,600 --> 00:20:56,559
поэтому вам нужно понимать контекст,

524
00:20:56,559 --> 00:20:58,480
чтобы иметь представление о

525
00:20:58,480 --> 00:21:00,880
контексте большого, что слева от

526
00:21:00,880 --> 00:21:02,960
того, что справа от него  чтобы понять,

527
00:21:02,960 --> 00:21:05,200
как вы могли бы подключить его к

528
00:21:05,200 --> 00:21:07,039
представлениям зависимостей

529
00:21:07,039 --> 00:21:08,159
предложения

530
00:21:08,159 --> 00:21:09,520
um,

531
00:21:09,520 --> 00:21:11,280
и поэтому, хотя ранее они работали

532
00:21:11,280 --> 00:21:13,600


533
00:21:13,600 --> 00:21:16,080
с анализом зависимостей на основе графиков, таким как анализатор mst, который я показал на слайде с более ранними

534
00:21:16,080 --> 00:21:18,080
результатами,

535
00:21:18,080 --> 00:21:20,000
казалось привлекательным,

536
00:21:20,000 --> 00:21:21,919
что мы могли придумать  гораздо лучшее

537
00:21:21,919 --> 00:21:25,039
представление контекста с использованием нейронных

538
00:21:25,039 --> 00:21:27,120
сетей, которые смотрят на контекст и то, как мы это делаем

539
00:21:27,120 --> 00:21:28,480
, - вот о чем я буду

540
00:21:28,480 --> 00:21:31,120
говорить в конце лекции, и

541
00:21:31,120 --> 00:21:35,520
поэтому в Стэнфорде

542
00:21:35,520 --> 00:21:37,760
мы заинтересовались попыткой понять, как это сделать.  вверх

543
00:21:37,760 --> 00:21:39,679
с лучшим анализатором зависимостей на основе графа

544
00:21:39,679 --> 00:21:42,400
с использованием контекста, извините, я забыл,

545
00:21:42,400 --> 00:21:45,280
это показывало, что если мы можем оценить

546
00:21:45,280 --> 00:21:47,440
каждую попарную зависимость, мы можем просто

547
00:21:47,440 --> 00:21:50,559
выбрать лучшую, поэтому мы можем сказать, что,

548
00:21:50,559 --> 00:21:53,840
вероятно, большой размер зависит от кота

549
00:21:53,840 --> 00:21:56,400
и первого  приближение, которое мы собираемся

550
00:21:56,400 --> 00:21:59,039
выбрать для каждого слова

551
00:21:59,039 --> 00:22:01,919
, которое зависит от слова,

552
00:22:01,919 --> 00:22:04,320
которое, скорее всего, будет зависимым, но

553
00:22:04,320 --> 00:22:06,240
мы хотим сделать это с некоторыми ограничениями,

554
00:22:06,240 --> 00:22:07,919
потому что мы хотим выйти  что-то,

555
00:22:07,919 --> 00:22:10,559
что является деревом с одним корнем, как я

556
00:22:10,559 --> 00:22:12,880
обсуждал в прошлый раз, и вы можете

557
00:22:12,880 --> 00:22:15,600
сделать это, используя алгоритм минимального связующего дерева

558
00:22:15,600 --> 00:22:18,640
, который использует эти оценки того, насколько

559
00:22:18,640 --> 00:22:21,840
вероятны разные зависимости в

560
00:22:21,840 --> 00:22:25,200
порядке, так что тогда в 2017 году

561
00:22:25,200 --> 00:22:27,919
другой набор учеников Тима и я тогда

562
00:22:27,919 --> 00:22:31,440
работал над тем, что хорошо сказал, можем ли мы теперь также

563
00:22:31,440 --> 00:22:33,440
создать гораздо лучший

564
00:22:33,440 --> 00:22:36,159
анализатор зависимостей на основе нейронного графа, и

565
00:22:36,159 --> 00:22:37,919
мы разработали новый

566
00:22:37,919 --> 00:22:40,480
метод для оценки анализа

567
00:22:40,480 --> 00:22:41,440


568
00:22:41,440 --> 00:22:43,440
зависимостей нейронной оценки и

569
00:22:43,440 --> 00:22:45,760
модель на основе графа, которую я не собираюсь

570
00:22:45,760 --> 00:22:48,559
вдаваться в подробности  сейчас, но это

571
00:22:48,559 --> 00:22:51,520
также дало очень хороший результат, потому

572
00:22:51,520 --> 00:22:53,520
что, вернувшись к синтаксическому анализу на основе графиков, мы

573
00:22:53,520 --> 00:22:55,679
могли бы затем создать синтаксический анализатор на основе графиков,

574
00:22:55,679 --> 00:22:58,159
который работал примерно на процент лучше,

575
00:22:58,159 --> 00:23:00,240
чем лучшие из

576
00:23:00,240 --> 00:23:03,280
новых анализаторов зависимостей на основе перехода Google,

577
00:23:03,280 --> 00:23:06,000
но я должен указать, что это  -

578
00:23:06,000 --> 00:23:08,720
смешанная победа, потому что, хотя его

579
00:23:08,720 --> 00:23:10,240
точность лучше,

580
00:23:10,240 --> 00:23:12,960
эти графические парсеры просто

581
00:23:12,960 --> 00:23:14,640
возведены в квадрат производительности, а не

582
00:23:14,640 --> 00:23:16,960
линейного времени, так что это похоже на ранние

583
00:23:16,960 --> 00:23:18,799
результаты, которые я показываю  ed

584
00:23:18,799 --> 00:23:21,120
они работают не так быстро,

585
00:23:21,120 --> 00:23:23,200
когда вы хотите

586
00:23:23,200 --> 00:23:26,960
передать большие объемы текста со сложными

587
00:23:26,960 --> 00:23:29,520
длинными предложениями,

588
00:23:29,520 --> 00:23:31,600
хорошо, так что это все, что вам нужно

589
00:23:31,600 --> 00:23:34,159
знать о синтаксических анализаторах зависимостей и

590
00:23:34,159 --> 00:23:36,320
выполнении третьего задания, так что возьмите его сегодня вечером

591
00:23:36,320 --> 00:23:39,840
и начните работать  но я хотел

592
00:23:39,840 --> 00:23:41,840


593
00:23:41,840 --> 00:23:45,440
кое-что сказать, прежде чем перейти к следующей теме, просто упомяну еще несколько вещей

594
00:23:45,440 --> 00:23:47,919
о нейронных сетях, так как некоторые из

595
00:23:47,919 --> 00:23:50,320
вас уже хорошо это знают, некоторые из вас

596
00:23:50,320 --> 00:23:52,159
видели меньше, но вы знаете, что есть

597
00:23:52,159 --> 00:23:53,760
просто куча  вещи, о которых вы

598
00:23:53,760 --> 00:23:57,039
должны знать um для создания новых сетей,

599
00:23:57,039 --> 00:23:59,760
теперь снова для третьего назначения, по

600
00:23:59,760 --> 00:24:01,679
сути, мы даем вам все, и

601
00:24:01,679 --> 00:24:04,080
если вы будете следовать рецепту, ваш парсер

602
00:24:04,080 --> 00:24:06,159
должен работать хорошо,

603
00:24:06,159 --> 00:24:07,760
но вы знаете,

604
00:24:07,760 --> 00:24:10,640
что вам следует делать как минимум, на самом деле

605
00:24:10,640 --> 00:24:12,960
вы знаете, что внимательно посмотрите на некоторые из

606
00:24:12,960 --> 00:24:14,159
то,

607
00:24:14,159 --> 00:24:17,039
что делает этот синтаксический анализатор, а именно вопросы,

608
00:24:17,039 --> 00:24:21,039
например, как мы инициализируем наши

609
00:24:21,039 --> 00:24:23,760
матрицы нашей нейронной сети,

610
00:24:23,760 --> 00:24:26,080
какие оптимизаторы мы используем, и

611
00:24:26,080 --> 00:24:28,559
тому подобное,

612
00:24:28,559 --> 00:24:30,320
потому что все это важные

613
00:24:30,320 --> 00:24:32,240
решения  Поэтому я хотел сказать

614
00:24:32,240 --> 00:24:35,360
несколько слов об этом,

615
00:24:35,360 --> 00:24:37,520
поэтому первое, что мы вообще не

616
00:24:37,520 --> 00:24:40,559
обсуждали, это концепция

617
00:24:40,559 --> 00:24:42,240
регуляризации,

618
00:24:42,240 --> 00:24:45,120
поэтому, когда мы строим эти нейронные сети,

619
00:24:45,120 --> 00:24:48,159
мы теперь строим модели с огромным

620
00:24:48,159 --> 00:24:50,400
числом  параметров,

621
00:24:50,400 --> 00:24:51,120
так что, по

622
00:24:51,120 --> 00:24:52,559
сути,

623
00:24:52,559 --> 00:24:55,279
почти все модели нейронных сетей, которые

624
00:24:55,279 --> 00:24:57,039
работают хорошо, на

625
00:24:57,039 --> 00:24:58,640
самом деле

626
00:24:58,640 --> 00:25:01,039
они полная функция потерь является

627
00:25:01,039 --> 00:25:05,679
регуляризованной функцией потерь, поэтому для этой

628
00:25:05,679 --> 00:25:09,120
функции потерь um здесь of j ну, эта часть

629
00:25:09,120 --> 00:25:11,679
здесь - та часть, которую мы видели раньше,

630
00:25:11,679 --> 00:25:15,520
где мы  мы используем мягкий классификатор максимума,

631
00:25:15,520 --> 00:25:17,520
а затем принимаем отрицательную

632
00:25:17,520 --> 00:25:19,360
потерю логарифмической вероятности, которую мы затем

633
00:25:19,360 --> 00:25:21,440
усредняем по различным примерам,

634
00:25:21,440 --> 00:25:24,159
но на самом деле мы затем придерживаемся конца

635
00:25:24,159 --> 00:25:25,200


636
00:25:25,200 --> 00:25:28,200
этого члена регуляризации, и поэтому этот

637
00:25:28,200 --> 00:25:30,240
член регуляризации

638
00:25:30,240 --> 00:25:33,520
суммирует квадрат каждого параметра в

639
00:25:33,520 --> 00:25:34,799
модель,

640
00:25:34,799 --> 00:25:39,039
и так что это эффективно говорит о том, что

641
00:25:39,039 --> 00:25:41,039
вы хотите сделать

642
00:25:41,039 --> 00:25:42,640
параметры

643
00:25:42,640 --> 00:25:46,080
ненулевыми, только если они действительно полезны,

644
00:25:46,080 --> 00:25:47,760
так что до той степени, в которой

645
00:25:47,760 --> 00:25:50,000
параметры не сильно помогают, вы

646
00:25:50,000 --> 00:25:53,039
просто наказываете здесь um by m  принимая их

647
00:25:53,039 --> 00:25:55,039
ненулевые, но в той степени, в которой

648
00:25:55,039 --> 00:25:57,600
параметры действительно помогают вам получить

649
00:25:57,600 --> 00:26:00,000
оценку вероятности, и, следовательно

650
00:26:00,000 --> 00:26:02,559
, для них нормально быть ненулевым, в

651
00:26:02,559 --> 00:26:05,520
частности, обратите внимание, что этот штраф

652
00:26:05,520 --> 00:26:08,400
оценивается только один раз для каждого параметра, он

653
00:26:08,400 --> 00:26:10,880
не  оценивается отдельно для каждого

654
00:26:10,880 --> 00:26:12,720
примера,

655
00:26:12,720 --> 00:26:14,120
хорошо, и наличие такого рода

656
00:26:14,120 --> 00:26:18,320
регуляризации важно для построения

657
00:26:18,320 --> 00:26:21,360
моделей нейронных сетей, которые хорошо регуляризируются,

658
00:26:21,360 --> 00:26:24,000
поэтому классическая проблема называется

659
00:26:24,000 --> 00:26:26,960
переобучением, а переобучение означает

660
00:26:26,960 --> 00:26:28,400
, что если у вас есть конкретный

661
00:26:28,400 --> 00:26:30,880
набор обучающих данных, и вы начинаете обучение

662
00:26:30,880 --> 00:26:32,480
ваша модель

663
00:26:32,480 --> 00:26:35,360
ваша ошибка снизится, потому что вы

664
00:26:35,360 --> 00:26:37,120
сместите параметры, чтобы они лучше

665
00:26:37,120 --> 00:26:39,039


666
00:26:39,039 --> 00:26:41,120
предсказывали правильный ответ

667
00:26:41,120 --> 00:26:43,600
для точек данных в модели, и вы можете

668
00:26:43,600 --> 00:26:46,559
продолжать это делать, и он начнет

669
00:26:46,559 --> 00:26:49,600
продолжать снижать частоту ошибок, но если

670
00:26:49,600 --> 00:26:52,480
вы затем  посмотрите на свой частично обученный

671
00:26:52,480 --> 00:26:55,760
классификатор и скажите, насколько хорошо этот

672
00:26:55,760 --> 00:26:58,640
классификатор классифицирует

673
00:26:58,640 --> 00:27:01,039
независимые данные различные

674
00:27:01,039 --> 00:27:03,039
тестовые данные, которые вы не обучали

675
00:27:03,039 --> 00:27:06,000
модель на том, что вы  Я обнаружу, что до

676
00:27:06,000 --> 00:27:08,080
определенного момента

677
00:27:08,080 --> 00:27:10,000
вы также научитесь лучше классифицировать

678
00:27:10,000 --> 00:27:13,520
примеры независимых тестов, и

679
00:27:13,520 --> 00:27:14,960
после этого

680
00:27:14,960 --> 00:27:16,640
обычно происходит то, что вы

681
00:27:16,640 --> 00:27:18,640
действительно начнете хуже

682
00:27:18,640 --> 00:27:22,000
классифицировать примеры независимых тестов,

683
00:27:22,000 --> 00:27:23,600
даже если вы продолжаете  научитесь

684
00:27:23,600 --> 00:27:25,039
лучше предсказывать обучающие

685
00:27:25,039 --> 00:27:27,600
примеры, и тогда это называлось

686
00:27:27,600 --> 00:27:30,080
тем, что вы

687
00:27:30,080 --> 00:27:32,000
переустанавливаете обучающие примеры, вы

688
00:27:32,000 --> 00:27:34,159
возитесь с параметрами модели, поэтому

689
00:27:34,159 --> 00:27:35,600
они действительно хороши в прогнозировании

690
00:27:35,600 --> 00:27:38,640
обучающих примеров, которые не являются полезными

691
00:27:38,640 --> 00:27:39,760
вещами

692
00:27:39,760 --> 00:27:43,600
который затем может предсказать гм на независимых

693
00:27:43,600 --> 00:27:47,279
примерах, к которым вы придете во время выполнения,

694
00:27:47,279 --> 00:27:48,720
хорошо,

695
00:27:48,720 --> 00:27:52,080
что классический взгляд на

696
00:27:52,080 --> 00:27:54,000
регуляризацию вроде как на самом деле

697
00:27:54,000 --> 00:27:56,880
устаревший и неправильный для современных нейронных

698
00:27:56,880 --> 00:27:58,799
сетей,

699
00:27:58,799 --> 00:28:00,240
так

700
00:28:00,240 --> 00:28:02,559
что правильный способ думать об этом для

701
00:28:02,559 --> 00:28:05,120
типа современных больших нейронных сетей, которые

702
00:28:05,120 --> 00:28:08,480
мы строим так, что

703
00:28:08,480 --> 00:28:11,279
переоснащение обучающих данных не является

704
00:28:11,279 --> 00:28:14,799
проблемой, но, тем не менее, вам нужна

705
00:28:14,799 --> 00:28:16,880
регуляризация,

706
00:28:16,880 --> 00:28:20,000
чтобы убедиться, что ваши модели

707
00:28:20,000 --> 00:28:23,760
хорошо обобщаются на независимые тестовые данные.  так что

708
00:28:23,760 --> 00:28:27,520
вы бы хотели, чтобы ваш график не выглядел так,

709
00:28:27,520 --> 00:28:28,720
как этот

710
00:28:28,720 --> 00:28:31,520
пример, когда тестовая ошибка начинает

711
00:28:31,520 --> 00:28:33,919
подниматься вверх, вы бы хотели, чтобы он

712
00:28:33,919 --> 00:28:36,000
в худшем случае был

713
00:28:36,000 --> 00:28:38,640
ровной линией, а в лучшем случае все еще

714
00:28:38,640 --> 00:28:40,880
постепенно снижался, он всегда будет

715
00:28:40,880 --> 00:28:43,679
выше, чем  ошибка обучения, но на

716
00:28:43,679 --> 00:28:46,320
самом деле она не показывает невозможность

717
00:28:46,320 --> 00:28:47,679
обобщения,

718
00:28:47,679 --> 00:28:49,600
поэтому, когда мы тренируем

719
00:28:49,600 --> 00:28:52,640
большие нейронные сети в наши дни,

720
00:28:52,640 --> 00:28:53,679
наши

721
00:28:53,679 --> 00:28:56,000
большие нейронные сети

722
00:28:56,000 --> 00:28:59,440
всегда переоснащаются обучающими данными, они

723
00:28:59,440 --> 00:29:02,080
очень переобучены обучающими данными.

724
00:29:02,080 --> 00:29:04,399
Фактически, во многих случаях наши нейронные

725
00:29:04,399 --> 00:29:06,880
сети имеют так много параметров, что  вы

726
00:29:06,880 --> 00:29:08,880
можете продолжать обучать их на

727
00:29:08,880 --> 00:29:10,799
обучающих данных до тех пор, пока ошибка

728
00:29:10,799 --> 00:29:13,279
обучающих данных не станет равна нулю, они получат каждый

729
00:29:13,279 --> 00:29:15,039
отдельный пример правильно, потому что они могут

730
00:29:15,039 --> 00:29:17,440
просто запомнить достаточно информации об этом, чтобы

731
00:29:17,440 --> 00:29:20,399
предсказать правильный ответ, но в целом при

732
00:29:20,399 --> 00:29:22,880
условии, что модели хорошо упорядочены,

733
00:29:22,880 --> 00:29:25,440
эти модели  будет по-прежнему

734
00:29:25,440 --> 00:29:27,919
хорошо обобщать и хорошо прогнозировать на

735
00:29:27,919 --> 00:29:30,480
независимых данных

736
00:29:30,480 --> 00:29:32,720
, поэтому часть того, что мы хотим сделать

737
00:29:32,720 --> 00:29:35,360
для этого, - это выяснить, насколько

738
00:29:35,360 --> 00:29:38,480
регуляризовать, и поэтому этот лямбда-параграф

739
00:29:38,480 --> 00:29:41,679
метр - это сила регуляризации,

740
00:29:41,679 --> 00:29:43,520
поэтому, если вы увеличиваете это лямбда-число,

741
00:29:43,520 --> 00:29:44,480


742
00:29:44,480 --> 00:29:47,520
вы получаете больше регуляризации, а

743
00:29:47,520 --> 00:29:48,720
если вы уменьшаете его, вы

744
00:29:48,720 --> 00:29:50,399
получаете меньше, и вы не хотите, чтобы

745
00:29:50,399 --> 00:29:52,640
оно было слишком большим, иначе  вы не будете хорошо соответствовать

746
00:29:52,640 --> 00:29:54,880
данным, и вы не хотите быть слишком

747
00:29:54,880 --> 00:29:56,080
маленькими

748
00:29:56,080 --> 00:29:57,279
, иначе у

749
00:29:57,279 --> 00:30:00,000
вас есть проблема, что вы

750
00:30:00,000 --> 00:30:01,600
плохо обобщаете,

751
00:30:01,600 --> 00:30:03,600
так что это классическая регуляризация l2,

752
00:30:03,600 --> 00:30:06,640
и это отправная точка,

753
00:30:06,640 --> 00:30:08,799
но наши большие нейронные сети достаточно

754
00:30:08,799 --> 00:30:10,480
сложны и имеют достаточно много

755
00:30:10,480 --> 00:30:13,200
параметров, поэтому регуляризация l2, по сути

756
00:30:13,200 --> 00:30:16,320
, не сокращает его, поэтому

757
00:30:16,320 --> 00:30:18,320
следующая вещь, о которой вы должны знать

758
00:30:18,320 --> 00:30:21,919
и которая является очень стандартной хорошей функцией

759
00:30:21,919 --> 00:30:23,919
для построения нейронных сетей, - это метод,

760
00:30:23,919 --> 00:30:26,880
называемый выпадением,

761
00:30:26,960 --> 00:30:29,919
поэтому

762
00:30:29,919 --> 00:30:32,960
выпадение обычно вводится как своего рода

763
00:30:32,960 --> 00:30:35,919
немного забавный процесс, который вы делаете во время

764
00:30:35,919 --> 00:30:37,120
обучения,

765
00:30:37,120 --> 00:30:39,320
чтобы избежать коадаптации функционального слоя,

766
00:30:39,320 --> 00:30:43,279
поэтому в случае отказа от обучения то, что вы делаете,

767
00:30:43,279 --> 00:30:46,720
- это то время, когда вы тренируете свою

768
00:30:46,720 --> 00:30:47,679
модель,

769
00:30:47,679 --> 00:30:49,760
что для каждого

770
00:30:49,760 --> 00:30:53,120
экземпляра или для каждой партии

771
00:30:53,120 --> 00:30:54,960
в вашем обучении, а

772
00:30:54,960 --> 00:30:58,320
затем для каждого  ch нейрон в модели

773
00:30:58,320 --> 00:31:01,679
вы отбрасываете 50 его входов, вы просто рассматриваете

774
00:31:01,679 --> 00:31:03,200
их как ноль,

775
00:31:03,200 --> 00:31:05,039
и это можно сделать, как бы

776
00:31:05,039 --> 00:31:09,440
обнуляя элементы um типа

777
00:31:09,440 --> 00:31:11,200
слоев

778
00:31:11,200 --> 00:31:12,080
um,

779
00:31:12,080 --> 00:31:12,880
а

780
00:31:12,880 --> 00:31:15,760
затем во время тестирования вы не отбрасываете ни одну

781
00:31:15,760 --> 00:31:18,640
из модели  веса, вы сохраняете их все, но на

782
00:31:18,640 --> 00:31:20,799
самом деле у вас есть все веса модели,

783
00:31:20,799 --> 00:31:22,960
потому что теперь вы храните в два раза больше

784
00:31:22,960 --> 00:31:25,679
вещей, чем вы бы использовали обучающие данные,

785
00:31:25,679 --> 00:31:26,480


786
00:31:26,480 --> 00:31:29,840
и так эффективно, что маленький

787
00:31:29,840 --> 00:31:32,159
рецепт предотвращает то, что называется совместной адаптацией функций,

788
00:31:32,159 --> 00:31:33,760


789
00:31:33,760 --> 00:31:34,480
так что

790
00:31:34,480 --> 00:31:35,279


791
00:31:35,279 --> 00:31:36,880
вы можете  У

792
00:31:36,880 --> 00:31:39,840
вас не может быть функций, которые полезны только

793
00:31:39,840 --> 00:31:42,159
при наличии определенных

794
00:31:42,159 --> 00:31:44,399
других функций, потому что модель не может

795
00:31:44,399 --> 00:31:45,600
гарантировать,

796
00:31:45,600 --> 00:31:47,120
какие функции будут присутствовать

797
00:31:47,120 --> 00:31:49,279
в разных примерах, потому что разные

798
00:31:49,279 --> 00:31:51,360
функции все время случайным образом удаляются,

799
00:31:51,360 --> 00:31:54,640
и поэтому

800
00:31:54,640 --> 00:31:56,399
Фактически отсев дает вам своего рода золотую

801
00:31:56,399 --> 00:31:58,159
середину между наивным байесовским методом и

802
00:31:58,159 --> 00:32:00,320
моделью логистической регрессии и наивными

803
00:32:00,320 --> 00:32:02,640
байесовскими моделями, все веса указаны

804
00:32:02,640 --> 00:32:04,559
независимо, в модели логистической

805
00:32:04,559 --> 00:32:06,480
регрессии все веса установлены в

806
00:32:06,480 --> 00:32:09,279
контекст всех других, и здесь

807
00:32:09,279 --> 00:32:11,120
вы знаете о других весах, но они могут

808
00:32:11,120 --> 00:32:13,919
случайным образом исчезнуть от вас, это также

809
00:32:13,919 --> 00:32:16,640
связано с моделями ансамбля, такими как

810
00:32:16,640 --> 00:32:18,240
упаковка моделей, потому что вы

811
00:32:18,240 --> 00:32:20,640
каждый раз используете разные подмножества функций,

812
00:32:20,640 --> 00:32:22,080


813
00:32:22,080 --> 00:32:23,519
но

814
00:32:23,519 --> 00:32:26,240
после всех этих объяснений есть

815
00:32:26,240 --> 00:32:28,480
на самом деле другой способ думать о

816
00:32:28,480 --> 00:32:30,720
отсеве, который был разработан

817
00:32:30,720 --> 00:32:32,799
здесь, в Стэнфорде, эта статья Перси

818
00:32:32,799 --> 00:32:36,399
Лян и студентами, гм, который состоит в том, чтобы утверждать,

819
00:32:36,399 --> 00:32:39,760
что на самом деле отсев дает вам

820
00:32:39,760 --> 00:32:42,640
сильный регуляризатор, который не является единообразным

821
00:32:42,640 --> 00:32:45,200
регуляризатором, таким как l2, который упорядочивает

822
00:32:45,200 --> 00:32:47,519
все с помощью  l2 последний, но может

823
00:32:47,519 --> 00:32:50,240
изучить регуляризацию, зависящую от функций, и

824
00:32:50,240 --> 00:32:52,640
поэтому этот отсев только что появился, как в

825
00:32:52,640 --> 00:32:54,799
целом лучший способ

826
00:32:54,799 --> 00:32:57,679
сделать регуляризацию для нейронных сетей,

827
00:32:57,679 --> 00:32:59,360
я думаю, вы уже видели и слышали

828
00:32:59,360 --> 00:33:02,000
это, но

829
00:33:02,000 --> 00:33:05,279
просто покажите его на моих слайдах один раз,

830
00:33:05,279 --> 00:33:07,200
если хотите  Чтобы ваши нейронные сети

831
00:33:07,200 --> 00:33:08,880
работали

832
00:33:08,880 --> 00:33:11,360
быстро, действительно важно, чтобы вы

833
00:33:11,360 --> 00:33:14,960
использовали тензоры векторных матриц и

834
00:33:14,960 --> 00:33:17,360
не делали ничего с циклами for, поэтому он

835
00:33:17,360 --> 00:33:19,120
Это крошечный пример, в

836
00:33:19,120 --> 00:33:20,240
котором я

837
00:33:20,240 --> 00:33:21,760
использую время,

838
00:33:21,760 --> 00:33:23,600
которое является полезной вещью, которую вы тоже можете использовать,

839
00:33:23,600 --> 00:33:25,519
чтобы увидеть, насколько быстро работают ваши нейронные

840
00:33:25,519 --> 00:33:27,600
сети, и различные способы записи

841
00:33:27,600 --> 00:33:28,640
этого,

842
00:33:28,640 --> 00:33:33,679
и поэтому, когда я делаю это, я

843
00:33:34,159 --> 00:33:35,360
делаю эти

844
00:33:35,360 --> 00:33:38,880
точечные продукты здесь  Я могу либо

845
00:33:38,880 --> 00:33:42,399
сделать это скалярное произведение в цикле for для

846
00:33:42,399 --> 00:33:45,519
каждого вектора слов, либо я могу сделать

847
00:33:45,519 --> 00:33:48,000
скалярное произведение с помощью векторной матрицы из одного слова,

848
00:33:48,000 --> 00:33:51,120
и если я делаю это в цикле for,

849
00:33:51,120 --> 00:33:52,799


850
00:33:52,799 --> 00:33:54,320
выполнение каждого

851
00:33:54,320 --> 00:33:56,080
цикла занимает у меня

852
00:33:56,080 --> 00:33:57,919
почти секунду,

853
00:33:57,919 --> 00:34:01,200
тогда как если я  сделайте это с

854
00:34:01,200 --> 00:34:03,919
умножением матриц, это займет у меня на

855
00:34:03,919 --> 00:34:07,039
порядок меньше времени, поэтому вы всегда

856
00:34:07,039 --> 00:34:09,040
должны стремиться использовать векторы и матрицы, а

857
00:34:09,040 --> 00:34:12,000
не циклы, и это ускорение

858
00:34:12,000 --> 00:34:14,320
примерно в 10 раз, когда вы делаете что-то

859
00:34:14,320 --> 00:34:16,159
на ЦП,

860
00:34:16,159 --> 00:34:18,560
движущемся вперед  мы собираемся использовать

861
00:34:18,560 --> 00:34:22,079
графические процессоры, и они только еще больше

862
00:34:22,079 --> 00:34:24,320
преувеличивают преимущества использования векторов и

863
00:34:24,320 --> 00:34:26,159
матриц, где вы обычно на два

864
00:34:26,159 --> 00:34:29,040
порядка ускоряетесь, делая

865
00:34:29,040 --> 00:34:30,719
что-то таким образом,

866
00:34:30,719 --> 00:34:35,199
да, поэтому для обратного прохода

867
00:34:35,199 --> 00:34:36,399
вы

868
00:34:36,399 --> 00:34:39,440
выполняете обратные проходы  раньше на

869
00:34:39,440 --> 00:34:41,440
дроппе

870
00:34:41,440 --> 00:34:43,440
приводил примеры правильно, поэтому

871
00:34:43,440 --> 00:34:46,000
для вещей, которые были выпадены,

872
00:34:46,000 --> 00:34:47,760
градиент не проходит через них, потому

873
00:34:47,760 --> 00:34:49,440
что они не присутствуют, они не

874
00:34:49,440 --> 00:34:51,520
влияют на вещи,

875
00:34:51,520 --> 00:34:53,918
поэтому в конкретной

876
00:34:53,918 --> 00:34:55,199
партии

877
00:34:55,199 --> 00:34:56,800
вы

878
00:34:56,800 --> 00:34:58,320
тренируете веса только для вещей,

879
00:34:58,320 --> 00:35:00,880
которые не выпали.  но затем, поскольку вы

880
00:35:00,880 --> 00:35:03,520
для каждой последующей партии um вы

881
00:35:03,520 --> 00:35:05,920
отбрасываете разные вещи, которые по

882
00:35:05,920 --> 00:35:08,000
кучке партий вы затем тренируете

883
00:35:08,000 --> 00:35:10,480
все веса модели

884
00:35:10,480 --> 00:35:14,560
um, и поэтому регуляризатор, зависящий от функции

885
00:35:14,560 --> 00:35:15,599
,

886
00:35:15,599 --> 00:35:16,640
означает,

887
00:35:16,640 --> 00:35:17,920
что

888
00:35:17,920 --> 00:35:20,640
насколько функцией

889
00:35:20,640 --> 00:35:22,240
могут быть различные функции

890
00:35:22,240 --> 00:35:26,400
регуляризовали различные количества

891
00:35:26,400 --> 00:35:27,680
для

892
00:35:27,680 --> 00:35:30,960
максимизации производительности, поэтому в этой

893
00:35:30,960 --> 00:35:32,079
модели

894
00:35:32,079 --> 00:35:33,040
каждая

895
00:35:33,040 --> 00:35:35,119
функция была

896
00:35:35,119 --> 00:35:38,240
просто наказана за счет взятия лямбда-

897
00:35:38,240 --> 00:35:40,240
значений в квадрате значений, так что это своего

898
00:35:40,240 --> 00:35:43,920
рода единообразная регуляризация,

899
00:35:43,920 --> 00:35:46,320
где конечным результатом этого

900
00:35:46,320 --> 00:35:49,200
обучения стилю исключения является то, что вы в конечном

901
00:35:49,200 --> 00:35:51,760
итоге получаете некоторые функции

902
00:35:51,760 --> 00:35:54,720
,  регуляризованы намного сильнее, а

903
00:35:54,720 --> 00:35:57,040
некоторые другие

904
00:35:57,040 --> 00:36:00,079
функции упорядочены менее сильно, и

905
00:36:00,079 --> 00:36:01,200
степень

906
00:36:01,200 --> 00:36:04,160
их регуляризации зависит от того, насколько

907
00:36:04,160 --> 00:36:05,359
они  используется, поэтому вы

908
00:36:05,359 --> 00:36:08,079
упорядочиваете больше функций,

909
00:36:08,079 --> 00:36:10,960
которые используются меньше, но

910
00:36:10,960 --> 00:36:12,240
я не собираюсь вдаваться в

911
00:36:12,240 --> 00:36:14,560
подробности того, как вы можете понять эту

912
00:36:14,560 --> 00:36:17,599
перспективу, мм, это, мм,

913
00:36:17,599 --> 00:36:19,280
вне контекста того, что я собираюсь

914
00:36:19,280 --> 00:36:21,280
чтобы пройти прямо сейчас,

915
00:36:21,280 --> 00:36:23,280
поэтому последний момент - я просто хотел

916
00:36:23,280 --> 00:36:26,800
немного взглянуть на

917
00:36:26,800 --> 00:36:30,000
нелинейности в наших нейронных сетях,

918
00:36:30,000 --> 00:36:31,280


919
00:36:31,280 --> 00:36:34,000
поэтому первое, что нужно запомнить, это

920
00:36:34,000 --> 00:36:36,560
иметь нелинейность, поэтому, если вы

921
00:36:36,560 --> 00:36:39,200
создаете мульти  -уровневая нейронная сеть, и

922
00:36:39,200 --> 00:36:42,160
вы только что узнали w1 x плюс b1,

923
00:36:42,160 --> 00:36:44,560
затем вы пропускаете ее через w к

924
00:36:44,560 --> 00:36:48,880
x плюс b2, а затем пропускаете через w3 um

925
00:36:48,880 --> 00:36:49,920
x

926
00:36:49,920 --> 00:36:51,760
ну, я думаю, это разные скрытые

927
00:36:51,760 --> 00:36:53,119
слои, поэтому я должен был сказать x, что они

928
00:36:53,119 --> 00:36:54,640
должны  быть скрытым одним скрытым двумя

929
00:36:54,640 --> 00:36:59,680
скрытыми тремя

930
00:36:59,680 --> 00:37:02,400


931
00:37:02,400 --> 00:37:04,960


932
00:37:04,960 --> 00:37:07,520


933
00:37:07,520 --> 00:37:11,920


934
00:37:11,920 --> 00:37:14,400


935
00:37:14,400 --> 00:37:17,839


936
00:37:17,839 --> 00:37:19,680
немного более длинная история там  потому что

937
00:37:19,680 --> 00:37:21,359
вы действительно получаете некоторые интересные

938
00:37:21,359 --> 00:37:22,800
эффекты обучения, но я не собираюсь

939
00:37:22,800 --> 00:37:24,880
сейчас об этом говорить,

940
00:37:24,880 --> 00:37:28,480
но обычно у нас должна быть какая

941
00:37:28,480 --> 00:37:29,720


942
00:37:29,720 --> 00:37:32,880
-то нелинейность, чтобы делать что-то

943
00:37:32,880 --> 00:37:35,599
интересное в глубокой нейронной сети,

944
00:37:35,599 --> 00:37:38,800
хорошо, так что это отправная

945
00:37:38,800 --> 00:37:41,839
точка, как  Самая классическая нелинейность

946
00:37:41,839 --> 00:37:44,160
- это логистика, которую часто называют

947
00:37:44,160 --> 00:37:46,880
нелинейностью сигмовидной формы um из-за

948
00:37:46,880 --> 00:37:49,760
ее формы um, которую мы видели ранее в

949
00:37:49,760 --> 00:37:52,880
предыдущих лекциях, так что это будет принимать любое

950
00:37:52,880 --> 00:37:55,440
действительное число и отображать его в

951
00:37:55,440 --> 00:37:58,800
диапазоне от нуля  один гм,

952
00:37:58,800 --> 00:38:00,720
и это было в основном то, что

953
00:38:00,720 --> 00:38:04,720
люди использовали в нейронных сетях 1980-х годов,

954
00:38:04,720 --> 00:38:07,359
теперь один из недостатков этой

955
00:38:07,359 --> 00:38:08,880
нелинейности

956
00:38:08,880 --> 00:38:09,760


957
00:38:09,760 --> 00:38:12,960
заключается в том, что она перемещает все в

958
00:38:12,960 --> 00:38:15,440
положительное пространство, потому что выход

959
00:38:15,440 --> 00:38:17,920
всегда находится между нулем и единицей, поэтому

960
00:38:17,920 --> 00:38:21,200
люди решили, что для  для многих целей

961
00:38:21,200 --> 00:38:24,160
было полезно иметь этот вариант сигмовидной

962
00:38:24,160 --> 00:38:25,599
формы

963
00:38:25,599 --> 00:38:28,320
гиперболического загара, который затем показан

964
00:38:28,320 --> 00:38:30,640
на втором рисунке.

965
00:38:30,640 --> 00:38:34,000
Теперь вы знаете, что логистический и гиперболический загар

966
00:38:34,000 --> 00:38:35,839
звучат так, как будто они верны.  y

967
00:38:35,839 --> 00:38:38,640
разные вещи, но на самом деле, как вы,

968
00:38:38,640 --> 00:38:40,160
возможно, помните из математического класса,

969
00:38:40,160 --> 00:38:42,480
гиперболический загар также может быть представлен в

970
00:38:42,480 --> 00:38:45,040
терминах экспонент, и если

971
00:38:45,040 --> 00:38:47,280
вы немного поработаете математикой, что, возможно, мы могли бы

972
00:38:47,280 --> 00:38:49,760
заставить вас сделать это при задании, на

973
00:38:49,760 --> 00:38:51,440
самом деле это тот случай, когда гиперлог

974
00:38:51,440 --> 00:38:54,320
тангенс - это просто измененная и смещенная

975
00:38:54,320 --> 00:38:56,560
версия логистики, так что на самом деле это

976
00:38:56,560 --> 00:38:58,800
точно такая же кривая, только

977
00:38:58,800 --> 00:39:01,440
немного сжатая, так что теперь она идет симметрично между

978
00:39:01,440 --> 00:39:03,040
минус единица и единица

979
00:39:03,040 --> 00:39:03,920
ммм

980
00:39:03,920 --> 00:39:04,880
ну

981
00:39:04,880 --> 00:39:05,680


982
00:39:05,680 --> 00:39:08,560
такие трансцендентные функции,

983
00:39:08,560 --> 00:39:11,440
такие как гиперболический тангенс, они отчасти

984
00:39:11,440 --> 00:39:13,839
медленные и  дорогие для вычисления прямо даже

985
00:39:13,839 --> 00:39:16,079
на наших быстрых компьютерах вычисление

986
00:39:16,079 --> 00:39:18,800
экспонент немного медленнее, поэтому

987
00:39:18,800 --> 00:39:21,599
люди заинтересовались тем, что хорошо,

988
00:39:21,599 --> 00:39:24,160
могли бы мы делать что-то с гораздо более простой

989
00:39:24,160 --> 00:39:27,359
нелинейностью, так что, если бы мы

990
00:39:27,359 --> 00:39:31,920
использовали так называемый жесткий tan h, чтобы жесткие 10h

991
00:39:31,920 --> 00:39:32,640
при

992
00:39:32,640 --> 00:39:34,880
в какой-то момент до некоторой точки это просто плоские

993
00:39:34,880 --> 00:39:36,960
линии в -1,

994
00:39:36,960 --> 00:39:39,200
тогда это

995
00:39:39,200 --> 00:39:40,800
y равно x

996
00:39:40,800 --> 00:39:43,599
до единицы, а затем снова просто плоские линии,

997
00:39:43,599 --> 00:39:44,560


998
00:39:44,560 --> 00:39:47,680
и вы знаете, что это кажется нам немного странным

999
00:39:47,680 --> 00:39:50,800
е, потому что, если ваш ввод

1000
00:39:50,800 --> 00:39:52,400
заканчивается

1001
00:39:52,400 --> 00:39:54,880
слева или справа,

1002
00:39:54,880 --> 00:39:56,320
вы не получаете никакой

1003
00:39:56,320 --> 00:39:58,800
дискриминации в отношении вещей,

1004
00:39:58,800 --> 00:40:00,800
дающих одинаковый результат,

1005
00:40:00,800 --> 00:40:03,280
но несколько удивительно, я имею в виду, что я был

1006
00:40:03,280 --> 00:40:05,599
удивлен, когда люди начали делать

1007
00:40:05,599 --> 00:40:07,280
это, э-э,

1008
00:40:07,280 --> 00:40:09,839
такого рода  модели um оказались

1009
00:40:09,839 --> 00:40:12,800
очень успешными, и это привело

1010
00:40:12,800 --> 00:40:13,680
к

1011
00:40:13,680 --> 00:40:16,079
тому, что оказалось наиболее

1012
00:40:16,079 --> 00:40:18,560
успешной и широко используемой

1013
00:40:18,560 --> 00:40:20,640
нелинейностью, и к большому количеству недавних работ по глубокому

1014
00:40:20,640 --> 00:40:23,040
обучению, которые

1015
00:40:23,040 --> 00:40:25,680
использовались um в

1016
00:40:25,680 --> 00:40:28,480
модели зависимостей i  Показано,

1017
00:40:28,480 --> 00:40:30,560
что это то, что называется выпрямленной линейной единицей

1018
00:40:30,560 --> 00:40:33,280
или значением, поэтому значение представляет собой

1019
00:40:33,280 --> 00:40:35,280
простейший вид нелинейности, который вы

1020
00:40:35,280 --> 00:40:38,319
можете себе представить, поэтому, если значение x

1021
00:40:38,319 --> 00:40:40,640
отрицательно, его значение равно нулю, так что

1022
00:40:40,640 --> 00:40:42,640
эффективно оно просто мертво, оно

1023
00:40:42,640 --> 00:40:45,680
ничего не делает в  вычисления, и если

1024
00:40:45,680 --> 00:40:50,240
его значение x больше нуля,

1025
00:40:50,240 --> 00:40:53,599
тогда просто y равно x значение

1026
00:40:53,599 --> 00:40:55,680
передается через

1027
00:40:55,680 --> 00:40:57,200
um,

1028
00:40:57,200 --> 00:40:59,359
и на первый взгляд это может показаться

1029
00:40:59,359 --> 00:41:01,359
действительно очень странным, и как это может

1030
00:41:01,359 --> 00:41:04,400
быть полезно в качестве  нелинейность, но если вы

1031
00:41:04,400 --> 00:41:06,640
немного подумаете о том, как можно очень точно

1032
00:41:06,640 --> 00:41:08,560
аппроксимировать вещи

1033
00:41:08,560 --> 00:41:10,800
с помощью кусочно-линейных функций,

1034
00:41:10,800 --> 00:41:13,200
вы можете как бы начать

1035
00:41:13,200 --> 00:41:15,440
видеть, как вы могли бы использовать это для

1036
00:41:15,440 --> 00:41:16,480
точной

1037
00:41:16,480 --> 00:41:19,200
аппроксимации функций с помощью кусочно-

1038
00:41:19,200 --> 00:41:22,160
линейных функций и какие

1039
00:41:22,160 --> 00:41:24,960
единицы измерения  было обнаружено, что они работают

1040
00:41:24,960 --> 00:41:28,480
чрезвычайно успешно, поэтому логистика

1041
00:41:28,480 --> 00:41:30,800
и tan h все еще используются в различных

1042
00:41:30,800 --> 00:41:33,200
местах, где вы используете логистику, когда вы на

1043
00:41:33,200 --> 00:41:36,160
выходе вероятности, мы снова увидим 10 часов

1044
00:41:36,160 --> 00:41:38,160
очень скоро, когда мы дойдем до повторяющихся

1045
00:41:38,160 --> 00:41:41,119
нейронных сетей, но они  больше

1046
00:41:41,119 --> 00:41:43,200
не используется по умолчанию при создании глубоких сетей,

1047
00:41:43,200 --> 00:41:45,359
что во многих местах первое, о чем

1048
00:41:45,359 --> 00:41:47,640
вы должны подумать, - это избегать

1049
00:41:47,640 --> 00:41:52,560
нелинейностей и, в частности, гм

1050
00:41:52,560 --> 00:41:54,160
,

1051
00:41:54,160 --> 00:41:56,400
почему они хороши отчасти в том, что

1052
00:41:56,400 --> 00:41:57,920
религия в

1053
00:41:57,920 --> 00:42:01,040
сетях обучается очень быстро, потому что  вы

1054
00:42:01,040 --> 00:42:03,200
получаете такого рода очень простой

1055
00:42:03,200 --> 00:42:05,280
градиентный обратный поток, потому что, предоставив вам его

1056
00:42:05,280 --> 00:42:08,480
с правой стороны, вы тогда

1057
00:42:08,480 --> 00:42:10,079
просто получите этот вид постоянного

1058
00:42:10,079 --> 00:42:13,119
градиентного обратного потока от t  наклон один, и

1059
00:42:13,119 --> 00:42:16,000
поэтому они тренируются очень быстро,

1060
00:42:16,000 --> 00:42:18,480
несколько удивительный факт заключается в том, что

1061
00:42:18,480 --> 00:42:20,240
своего рода почти простейшей

1062
00:42:20,240 --> 00:42:23,520
нелинейности, которую только можно вообразить, все еще достаточно,

1063
00:42:23,520 --> 00:42:26,319
чтобы иметь очень хорошую нейронную сеть,

1064
00:42:26,319 --> 00:42:28,880
но просто люди играли

1065
00:42:28,880 --> 00:42:31,359
с вариантами этого эм, так что люди

1066
00:42:31,359 --> 00:42:33,920
затем поигрались с использованием дырявых рельсов,

1067
00:42:33,920 --> 00:42:35,839
где вместо того,

1068
00:42:35,839 --> 00:42:38,480
чтобы левая сторона просто

1069
00:42:38,480 --> 00:42:43,119
полностью обнулялась, она немного отрицательна на,

1070
00:42:43,119 --> 00:42:44,480
но гораздо

1071
00:42:44,480 --> 00:42:46,640
более пологом склоне, а затем была

1072
00:42:46,640 --> 00:42:48,480
параметрическая перезагрузка, где у вас есть

1073
00:42:48,480 --> 00:42:50,640
дополнительный параметр, где вы изучаете

1074
00:42:50,640 --> 00:42:53,280
наклон  отрицательная часть,

1075
00:42:53,280 --> 00:42:55,680
еще одна вещь, которая использовалась в

1076
00:42:55,680 --> 00:42:58,000
последнее время, - это резкая нелинейность,

1077
00:42:58,000 --> 00:43:01,760
которая выглядит почти как значение, но

1078
00:43:01,760 --> 00:43:04,319
она как бы изгибается немного вниз

1079
00:43:04,319 --> 00:43:06,880
и начинает расти, я имею в виду, я думаю,

1080
00:43:06,880 --> 00:43:09,280
будет справедливо сказать, что вы знаете  ни один из

1081
00:43:09,280 --> 00:43:11,440
них на самом деле не зарекомендовал себя

1082
00:43:11,440 --> 00:43:13,760
значительно лучше, есть документы, в которых говорится, что

1083
00:43:13,760 --> 00:43:15,680
я могу получить лучшие результаты, используя один из

1084
00:43:15,680 --> 00:43:17,599
них, и, возможно, вы можете,

1085
00:43:17,599 --> 00:43:19,920
но вы знаете, что это не день и ночь

1086
00:43:19,920 --> 00:43:22,079
и  подавляющее большинство работ, которые вы видите

1087
00:43:22,079 --> 00:43:24,800
вокруг, по-прежнему просто используют значения во

1088
00:43:24,800 --> 00:43:27,680
многих местах,

1089
00:43:27,680 --> 00:43:29,040
хорошо,

1090
00:43:29,040 --> 00:43:31,119
еще пара вещей,

1091
00:43:31,119 --> 00:43:33,440
инициализация параметров,

1092
00:43:33,440 --> 00:43:34,240


1093
00:43:34,240 --> 00:43:38,839
поэтому почти во всех случаях вы должны должны

1094
00:43:38,839 --> 00:43:42,400
инициализировать матрицы ваших

1095
00:43:42,400 --> 00:43:46,480
нейронных сетей с небольшими случайными значениями, которые

1096
00:43:46,480 --> 00:43:49,280
нейронные сети просто не делают  работать, если вы

1097
00:43:49,280 --> 00:43:51,760
начинаете матрицы с нуля, потому что, по

1098
00:43:51,760 --> 00:43:54,480
сути, тогда все симметрично

1099
00:43:54,480 --> 00:43:56,000
, симметрично,

1100
00:43:56,000 --> 00:43:58,800
ничто не может специализироваться по-разному,

1101
00:43:58,800 --> 00:43:59,760


1102
00:43:59,760 --> 00:44:02,319
и тогда вы получите что-то вроде

1103
00:44:02,319 --> 00:44:03,599
э-э, у

1104
00:44:03,599 --> 00:44:05,200
вас просто нет способности

1105
00:44:05,200 --> 00:44:07,680
нейронной сети научиться, как вы это понимаете

1106
00:44:07,680 --> 00:44:09,839
дефектное решение,

1107
00:44:09,839 --> 00:44:10,880
поэтому

1108
00:44:10,880 --> 00:44:13,920
обычно вы используете какой-то метод, такой

1109
00:44:13,920 --> 00:44:14,800
как

1110
00:44:14,800 --> 00:44:17,680
равномерное рисование случайных чисел между

1111
00:44:17,680 --> 00:44:20,960
минус r и r для небольшого значения r и

1112
00:44:20,960 --> 00:44:22,800
просто заполнение всех параметров

1113
00:44:22,800 --> 00:44:26,640
, за исключением

1114
00:44:26,640 --> 00:44:28,960
весов смещения, можно установить веса смещения равными нулю

1115
00:44:28,960 --> 00:44:32,319
и  в каком-то смысле это лучше

1116
00:44:32,319 --> 00:44:37,200
с точки зрения выбора того, какое значение r по

1117
00:44:37,200 --> 00:44:40,800
существу для традиционных нейронных сетей,

1118
00:44:40,800 --> 00:44:44,079
что мы хотим установить, чтобы наш диапазон был

1119
00:44:44,079 --> 00:44:46,400
таким, чтобы числа i  В нашей нейронной

1120
00:44:46,400 --> 00:44:49,760
сети остаются разумного размера, они

1121
00:44:49,760 --> 00:44:52,480
не становятся слишком большими и не становятся слишком

1122
00:44:52,480 --> 00:44:54,240
маленькими, и

1123
00:44:54,240 --> 00:44:56,800
будут ли они взорваться или нет,

1124
00:44:56,800 --> 00:44:59,599
зависит от того, сколько

1125
00:44:59,599 --> 00:45:01,760
соединений в нейронной сети, поэтому посмотрите

1126
00:45:01,760 --> 00:45:04,640
на вентилятор в  и

1127
00:45:04,640 --> 00:45:07,119
разветвляют соединения в нейронной сети, и

1128
00:45:07,119 --> 00:45:10,079
поэтому очень распространенная

1129
00:45:10,839 --> 00:45:13,040
инициализация, которую вы увидите в пи-

1130
00:45:13,040 --> 00:45:15,240
факеле, - это так называемая инициализация Хавьера,

1131
00:45:15,240 --> 00:45:18,480
названная в честь человека,

1132
00:45:18,480 --> 00:45:20,400
который это предложил,

1133
00:45:20,400 --> 00:45:24,240
и он вырабатывает значение uh на

1134
00:45:24,240 --> 00:45:25,040
основе

1135
00:45:25,040 --> 00:45:26,480


1136
00:45:26,480 --> 00:45:28,960
этого вентилятора и  веером из слоев,

1137
00:45:28,960 --> 00:45:31,040
но вы можете просто попросить его, скажем,

1138
00:45:31,040 --> 00:45:33,200
инициализировать с этой инициализацией, и

1139
00:45:33,200 --> 00:45:34,560


1140
00:45:34,560 --> 00:45:36,319
это будет еще одна область, в которой

1141
00:45:36,319 --> 00:45:38,160
были некоторые последующие

1142
00:45:38,160 --> 00:45:41,119
разработки, поэтому примерно на

1143
00:45:41,119 --> 00:45:43,119
пятой неделе мы начнем говорить о

1144
00:45:43,119 --> 00:45:45,040
нормализации слоев, если вы используете

1145
00:45:45,040 --> 00:45:47,200
нормализация слоя, то это вроде как

1146
00:45:47,200 --> 00:45:48,560
не имеет значения, как вы

1147
00:45:48,560 --> 00:45:51,599
инициализируете веса,

1148
00:45:52,000 --> 00:45:53,680
поэтому, наконец,

1149
00:45:53,680 --> 00:45:56,640
мы должны обучить наши модели, и я

1150
00:45:56,640 --> 00:45:58,800
кратко представил идею

1151
00:45:58,800 --> 00:46:01,599
стохастического градиентного спуска, и вы знаете,

1152
00:46:01,599 --> 00:46:04,960
что хорошее  новость заключается в том, что в большинстве случаев

1153
00:46:04,960 --> 00:46:07,200
, если обучение ваших сетей с помощью

1154
00:46:07,200 --> 00:46:10,079
стохастического градиентного спуска работает

1155
00:46:10,079 --> 00:46:11,520
нормально,

1156
00:46:11,520 --> 00:46:15,839
используйте его, и вы получите хорошие результаты,

1157
00:46:16,160 --> 00:46:17,440
однако

1158
00:46:17,440 --> 00:46:19,440
часто это требует

1159
00:46:19,440 --> 00:46:21,359
выбора подходящей скорости обучения, что

1160
00:46:21,359 --> 00:46:22,800
является моим последним

1161
00:46:22,800 --> 00:46:25,440
слайдом советов на следующем слайде, но

1162
00:46:25,440 --> 00:46:27,920
было  огромный объем работы

1163
00:46:27,920 --> 00:46:31,520
по оптимизации нейронных сетей, и

1164
00:46:31,520 --> 00:46:34,160
люди придумали целую

1165
00:46:34,160 --> 00:46:37,920
серию более сложных оптимизаторов,

1166
00:46:37,920 --> 00:46:39,599
и я не собираюсь вдаваться в

1167
00:46:39,599 --> 00:46:42,000
детали оптимизации в этом классе,

1168
00:46:42,000 --> 00:46:44,960
но очень расплывчатая идея заключается в том, что эти

1169
00:46:44,960 --> 00:46:48,480
оптимизаторы являются адаптивными.  в том, что они могут как

1170
00:46:48,480 --> 00:46:51,520
бы отслеживать, какой был уклон,

1171
00:46:51,520 --> 00:46:54,400
какой градиент существует для

1172
00:46:54,400 --> 00:46:57,200
различных параметров, и, следовательно, на

1173
00:46:57,200 --> 00:46:59,839
основе этого принимать решения о том, насколько

1174
00:46:59,839 --> 00:47:01,680
корректировать веса

1175
00:47:01,680 --> 00:47:04,079
при обновлении градиента, а

1176
00:47:04,079 --> 00:47:06,319
не регулировать его на постоянную величину.

1177
00:47:06,319 --> 00:47:09,119
и поэтому в этом семействе методов

1178
00:47:09,119 --> 00:47:11,920
есть методы, которые включают edegrad

1179
00:47:11,920 --> 00:47:15,119
rms prop adam, а затем варианты

1180
00:47:15,119 --> 00:47:20,319
adam, включая sparse adam adam и т.

1181
00:47:20,319 --> 00:47:23,040
д.  Тот, который называется атомом, - довольно хорошее

1182
00:47:23,040 --> 00:47:25,520
место для начала, и в большинстве случаев

1183
00:47:25,520 --> 00:47:27,920
его можно использовать, и снова

1184
00:47:27,920 --> 00:47:30,319
с точки зрения пирог-факела, когда вы

1185
00:47:30,319 --> 00:47:32,880
инициализируете оптимизатор, вы можете просто

1186
00:47:32,880 --> 00:47:35,440
сказать, пожалуйста, используйте Адам, и вы не  На

1187
00:47:35,440 --> 00:47:37,839
самом деле не нужно знать об этом намного больше,

1188
00:47:37,839 --> 00:47:39,680
чем то, что

1189
00:47:39,680 --> 00:47:40,880
если вы

1190
00:47:40,880 --> 00:47:41,839


1191
00:47:41,839 --> 00:47:44,559
используете простой стохастический градиент,

1192
00:47:44,559 --> 00:47:46,640
вам нужно изменить выбор скорости обучения,

1193
00:47:46,640 --> 00:47:49,440
чтобы это было значение eta, на которое вы

1194
00:47:49,440 --> 00:47:50,800


1195
00:47:50,800 --> 00:47:53,359
умножили градиент, чтобы определить, на сколько нужно

1196
00:47:53,359 --> 00:47:55,920
настроить веса, и так  Я немного об этом говорил,

1197
00:47:55,920 --> 00:47:58,559
что вы не хотите, чтобы она была

1198
00:47:58,559 --> 00:48:01,280
слишком большой, или ваша модель могла расходиться или

1199
00:48:01,280 --> 00:48:03,599
подпрыгивать, вы не хотели, чтобы она была

1200
00:48:03,599 --> 00:48:08,079
слишком маленькой, иначе обучение может

1201
00:48:08,079 --> 00:48:10,079
происходить очень медленно, и вы

1202
00:48:10,079 --> 00:48:13,440
пропустите задание  крайний срок вы знаете, насколько

1203
00:48:13,440 --> 00:48:15,680
он должен быть большим, зависит от всех видов

1204
00:48:15,680 --> 00:48:17,839
деталей модели, и поэтому вы как бы

1205
00:48:17,839 --> 00:48:20,800
хотите попробовать несколько

1206
00:48:20,800 --> 00:48:23,760
чисел разного порядка, чтобы увидеть, какие

1207
00:48:23,760 --> 00:48:26,079
числа кажутся хорошо работающими для

1208
00:48:26,079 --> 00:48:28,000
стабильности, но достаточно

1209
00:48:28,000 --> 00:48:30,319
быстро что-то  Примерно от 10 до минус

1210
00:48:30,319 --> 00:48:32,480
3 или от 10 до минус 4 - это не сумасшедшее

1211
00:48:32,480 --> 00:48:34,480
место для начала,

1212
00:48:34,480 --> 00:48:36,640
в принципе, вы можете отлично справиться, просто

1213
00:48:36,640 --> 00:48:39,680
используя постоянную скорость обучения в sgd на

1214
00:48:39,680 --> 00:48:42,319
практике, люди обычно обнаруживают, что могут

1215
00:48:42,319 --> 00:48:44,800
добиться лучших результатов, уменьшив

1216
00:48:44,800 --> 00:48:47,760
скорость обучения по мере того, как  вы тренируетесь, поэтому очень

1217
00:48:47,760 --> 00:48:50,720
распространенный рецепт состоит в том, что вы уменьшаете

1218
00:48:50,720 --> 00:48:53,680
скорость обучения вдвое после каждых k эпох,

1219
00:48:53,680 --> 00:48:56,160
когда эпоха означает, что вы прошли

1220
00:48:56,160 --> 00:48:58,960
через весь набор обучающих данных,

1221
00:48:58,960 --> 00:49:01,040
поэтому, возможно, что-то вроде каждые три

1222
00:49:01,040 --> 00:49:04,960
эпохи у вас есть скорость обучения

1223
00:49:04,960 --> 00:49:07,359
и  заключительная небольшая заметка, выделенная фиолетовым,

1224
00:49:07,359 --> 00:49:09,200
- это когда вы проходите через данные,

1225
00:49:09,200 --> 00:49:11,040
которые вы не хотите

1226
00:49:11,040 --> 00:49:13,760
каждый раз проходить через элементы данных в одном и том же порядке,

1227
00:49:13,760 --> 00:49:16,720
потому что это приводит к тому, что у

1228
00:49:16,720 --> 00:49:19,040
вас есть своего рода шаблон

1229
00:49:19,040 --> 00:49:22,000
обучающих примеров, которые  модель будет как

1230
00:49:22,000 --> 00:49:24,000
бы попадать в эту периодичность

1231
00:49:24,000 --> 00:49:26,319
этих шаблонов, поэтому лучше

1232
00:49:26,319 --> 00:49:30,079
перетасовать данные перед каждым прохождением через нее,

1233
00:49:30,079 --> 00:49:32,000
хорошо, есть более сложные

1234
00:49:32,000 --> 00:49:35,839
способы установить скорость обучения, эм, и я

1235
00:49:35,839 --> 00:49:38,319
действительно не буду вдаваться в них сейчас

1236
00:49:38,319 --> 00:49:41,359
f  У более старых оптимизаторов, таких как Адам, также есть

1237
00:49:41,359 --> 00:49:42,800
скорость обучения, поэтому вам все равно нужно

1238
00:49:42,800 --> 00:49:45,359
выбрать значение скорости обучения, но на

1239
00:49:45,359 --> 00:49:47,200
самом деле это начальная скорость обучения,

1240
00:49:47,200 --> 00:49:49,280
которая обычно оптимизатор

1241
00:49:49,280 --> 00:49:52,160
сжимается при запуске, и поэтому вы

1242
00:49:52,160 --> 00:49:53,599
обычно хотите иметь число, с которого он

1243
00:49:53,599 --> 00:49:56,000
начинается, за пределами  больший размер,

1244
00:49:56,000 --> 00:49:58,079
потому что он будет уменьшаться по мере того, как все

1245
00:49:58,079 --> 00:50:00,480
будет в

1246
00:50:00,640 --> 00:50:02,800
порядке, так что это все в порядке

1247
00:50:02,800 --> 00:50:05,119
введения, и теперь я готов приступить

1248
00:50:05,119 --> 00:50:08,800
к языковым моделям и RNN, так что что такое

1249
00:50:08,800 --> 00:50:11,520
языковое моделирование, я имею в виду, как два слова

1250
00:50:11,520 --> 00:50:13,520
моделирования на английском языке могут означать

1251
00:50:13,520 --> 00:50:16,800
практически все, но в

1252
00:50:16,800 --> 00:50:19,200
литературе по обработке естественного языка

1253
00:50:19,200 --> 00:50:22,160
моделирование языка имеет очень точное техническое

1254
00:50:22,160 --> 00:50:24,240
определение, которое вы должны знать,

1255
00:50:24,240 --> 00:50:27,520
поэтому языковое моделирование - это задача

1256
00:50:27,520 --> 00:50:31,119
предсказания следующего слова,

1257
00:50:31,119 --> 00:50:34,960
так что если у вас есть какой-то контекст, например,

1258
00:50:34,960 --> 00:50:36,960
студенты открывают там,

1259
00:50:36,960 --> 00:50:39,200
вы хотите  уметь предугадывать, какие

1260
00:50:39,200 --> 00:50:42,240
слова появятся дальше, это их книги,

1261
00:50:42,240 --> 00:50:45,839
их ноутбуки, их экзамены, их разум,

1262
00:50:45,839 --> 00:50:48,559
и, в частности,

1263
00:50:48,559 --> 00:50:50,160
то, что вы хотите

1264
00:50:50,160 --> 00:50:52,839
делать, это быть  способность дать

1265
00:50:52,839 --> 00:50:56,079
вероятность того, что

1266
00:50:56,079 --> 00:50:59,680
в этом контексте будут встречаться разные слова, поэтому языковая

1267
00:50:59,680 --> 00:51:03,359
модель представляет собой распределение вероятностей для

1268
00:51:03,359 --> 00:51:09,240
следующих слов с учетом предыдущего контекста,

1269
00:51:10,160 --> 00:51:12,480
а система, которая делает это, называется

1270
00:51:12,480 --> 00:51:15,040
языковой моделью,

1271
00:51:15,040 --> 00:51:16,480
поэтому

1272
00:51:16,480 --> 00:51:18,400
в результате вы также можете думать

1273
00:51:18,400 --> 00:51:20,559
языковой модели как системы, которая

1274
00:51:20,559 --> 00:51:23,359
присваивает оценку вероятности

1275
00:51:23,359 --> 00:51:26,400
фрагменту текста, поэтому, если у нас есть фрагмент текста

1276
00:51:26,400 --> 00:51:28,160
, мы можем просто вычислить его

1277
00:51:28,160 --> 00:51:29,920
вероятность в соответствии с языковой

1278
00:51:29,920 --> 00:51:32,319
моделью, чтобы вероятность

1279
00:51:32,319 --> 00:51:36,319
последовательности токенов мы могли разложить с помощью  цепное

1280
00:51:36,319 --> 00:51:38,720
правило: вероятность первого раза

1281
00:51:38,720 --> 00:51:40,480
вероятность второго с учетом первого

1282
00:51:40,480 --> 00:51:44,240
и т. д., а затем мы можем решить это,

1283
00:51:44,240 --> 00:51:46,800
используя то, что наша языковая модель предоставляет

1284
00:51:46,800 --> 00:51:49,280
в качестве продукта каждой вероятности

1285
00:51:49,280 --> 00:51:52,559
предсказания следующего слова

1286
00:51:53,280 --> 00:51:56,000
хорошо, языковые модели действительно являются

1287
00:51:56,000 --> 00:52:00,480
краеугольным камнем человеческого языка  технологии

1288
00:52:00,480 --> 00:52:02,880
все, что вы делаете

1289
00:52:02,880 --> 00:52:05,359
с компьютерами, которые включают человеческий

1290
00:52:05,359 --> 00:52:09,200
язык, вы используете языковые модели,

1291
00:52:09,200 --> 00:52:12,000
поэтому, когда вы используете свой телефон, он

1292
00:52:12,000 --> 00:52:14,720
предлагает  хорошо или плохо,

1293
00:52:14,720 --> 00:52:16,480
какое следующее слово вы, вероятно, захотите

1294
00:52:16,480 --> 00:52:18,960
ввести, и это языковая модель,

1295
00:52:18,960 --> 00:52:21,440
которая пытается предсказать вероятные следующие

1296
00:52:21,440 --> 00:52:23,119
слова,

1297
00:52:23,119 --> 00:52:25,440
когда то же самое происходит в документе Google

1298
00:52:25,440 --> 00:52:28,160
и предлагает следующее слово или

1299
00:52:28,160 --> 00:52:31,839
несколько следующих слов, которые являются  языковая модель, которую

1300
00:52:31,839 --> 00:52:34,480
вы знаете, основная причина, по которой тот, который

1301
00:52:34,480 --> 00:52:36,559
в документах Google работает намного лучше,

1302
00:52:36,559 --> 00:52:39,119
чем тот, что на вашем телефоне, заключается в том, что для

1303
00:52:39,119 --> 00:52:41,119
моделей телефонов с клавиатурой они должны быть

1304
00:52:41,119 --> 00:52:44,240
очень компактными, чтобы они могли работать быстро,

1305
00:52:44,240 --> 00:52:45,920
и не много памяти,

1306
00:52:45,920 --> 00:52:48,000
поэтому они сортируются  только посредственных

1307
00:52:48,000 --> 00:52:49,920
языковых моделей, где что-то вроде

1308
00:52:49,920 --> 00:52:51,839
google docs может сделать намного лучше

1309
00:52:51,839 --> 00:52:54,480


1310
00:52:54,480 --> 00:52:56,880
завершение запроса задания языкового моделирования. То же самое, что и

1311
00:52:56,880 --> 00:52:59,040
языковая модель,

1312
00:52:59,040 --> 00:53:01,440
и тогда возникает вопрос, как

1313
00:53:01,440 --> 00:53:05,680
мы строим языковые модели, и поэтому я

1314
00:53:05,680 --> 00:53:08,800
кратко хотел сначала снова дать

1315
00:53:08,800 --> 00:53:11,680
традиционный ответ, так как вы

1316
00:53:11,680 --> 00:53:13,520
должны иметь хотя бы некоторое представление

1317
00:53:13,520 --> 00:53:14,480
о том, как

1318
00:53:14,480 --> 00:53:17,599
nlp выполнялась без нейронной сети,

1319
00:53:17,599 --> 00:53:21,200
и традиционный ответ, который использовался для

1320
00:53:21,200 --> 00:53:22,559
распознавания речи и других

1321
00:53:22,559 --> 00:53:25,440
приложений f  или, по крайней мере, два десятилетия

1322
00:53:25,440 --> 00:53:27,280
три десятилетия действительно

1323
00:53:27,280 --> 00:53:29,359
были тем, что называлось моделями языка инграмм,

1324
00:53:29,359 --> 00:53:32,880
и это была очень простая, но

1325
00:53:32,880 --> 00:53:35,440
все же довольно эффективная идея, поэтому мы хотим

1326
00:53:35,440 --> 00:53:39,599
дать вероятности следующих слов,

1327
00:53:39,839 --> 00:53:40,800
поэтому

1328
00:53:40,800 --> 00:53:42,720
мы собираемся работать с тем,

1329
00:53:42,720 --> 00:53:45,200
что называется n  граммов и так n

1330
00:53:45,200 --> 00:53:48,240
граммов - это просто кусок из n последовательных

1331
00:53:48,240 --> 00:53:49,359
слов

1332
00:53:49,359 --> 00:53:50,800
, которые обычно называют

1333
00:53:50,800 --> 00:53:54,000
униграммами, биграммами, триграммами, а затем четыре

1334
00:53:54,000 --> 00:53:55,839
грамма и пять граммов

1335
00:53:55,839 --> 00:53:58,400
- ужасный набор имен, который

1336
00:53:58,400 --> 00:54:00,480
оскорбил бы любого гуманиста,

1337
00:54:00,480 --> 00:54:03,520
но это то, что люди обычно говорят,

1338
00:54:03,520 --> 00:54:06,400
и так эффективно то, что мы  просто

1339
00:54:06,400 --> 00:54:08,800
собираем статистику о том, как часто

1340
00:54:08,800 --> 00:54:10,400
разные энграммы

1341
00:54:10,400 --> 00:54:13,440
встречаются в большом объеме текста, а затем

1342
00:54:13,440 --> 00:54:16,800
используем их для построения вероятностной модели,

1343
00:54:16,800 --> 00:54:19,280
поэтому первое, что мы делаем, - это то, что

1344
00:54:19,280 --> 00:54:21,040
называется марковским

1345
00:54:21,040 --> 00:54:22,960
предположением, поэтому они также

1346
00:54:22,960 --> 00:54:26,640
называются марковскими моделями.  и мы решаем, что

1347
00:54:26,640 --> 00:54:29,520
слово и позиция t плюс один

1348
00:54:29,520 --> 00:54:32,240
зависит только от предшествующих n минус одного

1349
00:54:32,240 --> 00:54:34,800
слова,

1350
00:54:35,680 --> 00:54:36,880
поэтому,

1351
00:54:36,880 --> 00:54:38,880
если мы хотим предсказать t плюс один,

1352
00:54:38,880 --> 00:54:40,319
учитывая весь

1353
00:54:40,319 --> 00:54:43,599
предыдущий текст, мы  на самом деле

1354
00:54:43,599 --> 00:54:45,920
отбросьте ранние слова и просто используйте

1355
00:54:45,920 --> 00:54:49,280
предыдущие n минус одно слово в качестве контекста, как

1356
00:54:49,280 --> 00:54:52,240
только мы сделаем это упрощение, мы

1357
00:54:52,240 --> 00:54:54,079
можем просто использовать определение

1358
00:54:54,079 --> 00:54:56,640
условной вероятности и сказать, что вся эта

1359
00:54:56,640 --> 00:54:58,720
условная вероятность - это

1360
00:54:58,720 --> 00:55:01,760
вероятность n слов,

1361
00:55:01,760 --> 00:55:03,760
деленная

1362
00:55:03,760 --> 00:55:07,280
на предыдущие n  минус одно слово, и, таким образом,

1363
00:55:07,280 --> 00:55:09,359
у нас есть вероятность n грамм

1364
00:55:09,359 --> 00:55:11,920
над вероятностью n минус один

1365
00:55:11,920 --> 00:55:14,000
грамм,

1366
00:55:14,000 --> 00:55:15,760
и тогда, как нам получить эти n грамм

1367
00:55:15,760 --> 00:55:18,240
и n минус один грамм вероятности, мы

1368
00:55:18,240 --> 00:55:21,200
просто берем большой объем текста на

1369
00:55:21,200 --> 00:55:24,160
каком-то языке и  мы подсчитываем, как часто

1370
00:55:24,160 --> 00:55:27,359
встречаются различные инграммы,

1371
00:55:27,359 --> 00:55:29,599
и поэтому наше грубое статистическое

1372
00:55:29,599 --> 00:55:32,480
приближение начинается с

1373
00:55:32,480 --> 00:55:35,520
подсчета инграммы по счету n минус

1374
00:55:35,520 --> 00:55:37,119
один грамм,

1375
00:55:37,119 --> 00:55:39,200
так что вот пример того, что предположим, что

1376
00:55:39,200 --> 00:55:41,200
мы изучаем языковую модель переднего плана,

1377
00:55:41,200 --> 00:55:44,319
хорошо, поэтому мы бросаем  убираем все слова,

1378
00:55:44,319 --> 00:55:46,960
кроме последних трех слов, и

1379
00:55:46,960 --> 00:55:48,880
они наша обусловленность.

1380
00:55:48,880 --> 00:55:51,359


1381
00:55:51,359 --> 00:55:53,599


1382
00:55:53,599 --> 00:55:56,160


1383
00:55:56,160 --> 00:55:59,040


1384
00:55:59,040 --> 00:56:01,839
учащиеся открывают свои книги, выясняется, как

1385
00:56:01,839 --> 00:56:03,760
часто учащиеся открывают свои умы,

1386
00:56:03,760 --> 00:56:04,720


1387
00:56:04,720 --> 00:56:06,880
и затем для каждого из этих подсчетов

1388
00:56:06,880 --> 00:56:08,960
мы делим на количество, как

1389
00:56:08,960 --> 00:56:11,520
часто учащиеся открывают свои книги, и

1390
00:56:11,520 --> 00:56:14,240
это дает нам наши оценки вероятности

1391
00:56:14,240 --> 00:56:15,680


1392
00:56:15,680 --> 00:56:19,040
, например, если в корпусе учащиеся

1393
00:56:19,040 --> 00:56:22,000
открывают  это происходило тысячу раз, когда

1394
00:56:22,000 --> 00:56:24,319
студенты открывали свои книги, происходило 400

1395
00:56:24,319 --> 00:56:25,440
раз,

1396
00:56:25,440 --> 00:56:27,920
мы бы получили оценку вероятности 0,4

1397
00:56:27,920 --> 00:56:30,559
для книг, если бы экзамены проводились 100 раз,

1398
00:56:30,559 --> 00:56:33,359
вы получили 0,1 за экзамены,

1399
00:56:33,359 --> 00:56:35,839
и мы уже здесь как бы видим

1400
00:56:35,839 --> 00:56:38,480
недостаток того, что мы сделали марковское

1401
00:56:38,480 --> 00:56:40,880
предположение и  избавился от всего

1402
00:56:40,880 --> 00:56:43,359
этого более раннего контекста,

1403
00:56:43,359 --> 00:56:45,280
который был бы полезен для того, чтобы помочь

1404
00:56:45,280 --> 00:56:46,720
нам предсказать

1405
00:56:46,720 --> 00:56:48,400


1406
00:56:48,400 --> 00:56:51,520
еще один момент, о котором я просто

1407
00:56:51,520 --> 00:56:54,880
упомяну, что я запутался в

1408
00:56:54,880 --> 00:56:58,240
этом количестве модели языка инграммы,

1409
00:56:58,240 --> 00:57:01,119
поэтому для модели языка переднего плана это

1410
00:57:01,119 --> 00:57:03,200
называется моделью языка переднего плана,

1411
00:57:03,200 --> 00:57:06,079
потому что при ее оценке вы используете

1412
00:57:06,079 --> 00:57:09,040
четыре грамма в числителе и

1413
00:57:09,040 --> 00:57:11,839
триграммы в знаменателе, поэтому вы

1414
00:57:11,839 --> 00:57:15,760
используете размер числа  erator, так что

1415
00:57:15,760 --> 00:57:17,920
терминология отличается от

1416
00:57:17,920 --> 00:57:20,880
терминологии, используемой в моделях Маркова,

1417
00:57:20,880 --> 00:57:23,280
поэтому, когда люди говорят о порядке

1418
00:57:23,280 --> 00:57:26,160
модели Маркова, который относится к

1419
00:57:26,160 --> 00:57:29,280
количеству используемого вами контекста, это будет

1420
00:57:29,280 --> 00:57:34,079
соответствовать модели разметки третьего порядка,

1421
00:57:34,160 --> 00:57:35,119


1422
00:57:35,119 --> 00:57:37,119
да, так что кто-то  Сказал, что это похоже

1423
00:57:37,119 --> 00:57:40,480
на наивную байесовскую модель,

1424
00:57:40,799 --> 00:57:42,079
вроде

1425
00:57:42,079 --> 00:57:44,559
наивных байесовских моделей, вы также оцениваете

1426
00:57:44,559 --> 00:57:47,760
вероятности, просто подсчитывая гм, так

1427
00:57:47,760 --> 00:57:48,640
что

1428
00:57:48,640 --> 00:57:51,040
они связаны,

1429
00:57:51,040 --> 00:57:53,200
и они в некотором смысле являются двумя

1430
00:57:53,200 --> 00:57:54,720
различиями,

1431
00:57:54,720 --> 00:57:55,520


1432
00:57:55,520 --> 00:57:59,280
первое отличие или специализация

1433
00:57:59,280 --> 00:58:03,599
заключается в том, что  наивные байесовские модели вычисляют

1434
00:58:03,599 --> 00:58:06,640
вероятности слов независимо от

1435
00:58:06,640 --> 00:58:09,920
их соседей, так что в одной части,

1436
00:58:09,920 --> 00:58:12,319
что наивная байесовская языковая модель является

1437
00:58:12,319 --> 00:58:14,640
моделью языка униграммы, вы просто

1438
00:58:14,640 --> 00:58:17,200
используете количество отдельных слов,

1439
00:58:17,200 --> 00:58:19,520
а другая часть наивной байесовской

1440
00:58:19,520 --> 00:58:22,720
модели - это  вы изучаете разный

1441
00:58:22,720 --> 00:58:26,319
набор подсчетов униграмм для каждого класса для

1442
00:58:26,319 --> 00:58:29,440
вашего классификатора,

1443
00:58:29,440 --> 00:58:31,440
и поэтому

1444
00:58:31,440 --> 00:58:33,680
у вас получилась такая эффективная

1445
00:58:33,680 --> 00:58:37,839
наивная байесовская модель, если у вас есть

1446
00:58:38,000 --> 00:58:43,359
спецификация класса  Языковые модели униграммы ific

1447
00:58:43,680 --> 00:58:45,280
Хорошо,

1448
00:58:45,280 --> 00:58:47,599
я дал это как простую статистическую

1449
00:58:47,599 --> 00:58:50,480
модель для оценки ваших вероятностей

1450
00:58:50,480 --> 00:58:52,880
с помощью модели инграммы, которую вы фактически не

1451
00:58:52,880 --> 00:58:55,040
можете избежать, просто сделав это, потому что у

1452
00:58:55,040 --> 00:58:57,280
вас есть проблемы с разреженностью,

1453
00:58:57,280 --> 00:59:00,319
поэтому вы знаете, что часто будет так, что

1454
00:59:00,319 --> 00:59:02,960
для многих студентов слов  открывать свои книги

1455
00:59:02,960 --> 00:59:06,000
или ученики открывали свои

1456
00:59:06,000 --> 00:59:08,240
рюкзаки, просто никогда не встречалось в

1457
00:59:08,240 --> 00:59:10,480
тренировочных данных, что, если вы подумаете об этом,

1458
00:59:10,480 --> 00:59:12,960
если у вас есть что-то вроде десяти-

1459
00:59:12,960 --> 00:59:15,599
пятого разных слов даже, и вы хотите,

1460
00:59:15,599 --> 00:59:18,400
чтобы тогда последовательность из четырех слов была

1461
00:59:18,400 --> 00:59:20,400
проблемой, и они '  В отношении 10 к пятой

1462
00:59:20,400 --> 00:59:21,839
каждой из них есть от

1463
00:59:21,839 --> 00:59:24,240
10 до 20 различных комбинаций, поэтому,

1464
00:59:24,240 --> 00:59:26,799
если вы не видите, и это действительно

1465
00:59:26,799 --> 00:59:29,599
астрономический объем данных, большинство прямых

1466
00:59:29,599 --> 00:59:32,559
последовательностей, которые вы никогда не видели, тогда

1467
00:59:32,559 --> 00:59:34,480
ваш числитель будет равен нулю, а ваша

1468
00:59:34,480 --> 00:59:36,880
оценка вероятности будет  ноль, и

1469
00:59:36,880 --> 00:59:39,119
это плохо, и поэтому самый распространенный способ

1470
00:59:39,119 --> 00:59:41,280
решения - просто добавить небольшую

1471
00:59:41,280 --> 00:59:43,760
дельту к каждому счету, а затем все

1472
00:59:43,760 --> 00:59:47,280
будет отличным от нуля, и это называется сглаживанием,

1473
00:59:47,280 --> 00:59:49,599
но  ну, иногда это хуже,

1474
00:59:49,599 --> 00:59:51,040
потому что иногда вы даже не

1475
00:59:51,040 --> 00:59:53,839
видели, чтобы ученики открывали свои, и это

1476
00:59:53,839 --> 00:59:55,599
более проблематично, потому что это означает, что наш

1477
00:59:55,599 --> 00:59:59,599
знаменатель здесь равен нулю, и поэтому

1478
00:59:59,599 --> 01:00:01,839
деление будет некорректным, и мы

1479
01:00:01,839 --> 01:00:03,680
не сможем с пользой вычислить какие-либо

1480
01:00:03,680 --> 01:00:06,000
вероятности в  контекст, который мы никогда

1481
01:00:06,000 --> 01:00:08,559
не видели, и поэтому стандартным решением

1482
01:00:08,559 --> 01:00:11,680
для этого является сокращение контекста,

1483
01:00:11,680 --> 01:00:14,559
который вызывается обратно, поэтому мы устанавливаем условие

1484
01:00:14,559 --> 01:00:18,240
только на открытие там, или, если мы все

1485
01:00:18,240 --> 01:00:20,480
еще не видели его открытым там, мы будем

1486
01:00:20,480 --> 01:00:23,520
условно только  или мы могли бы просто

1487
01:00:23,520 --> 01:00:25,839
забыть обо всех условных обозначениях и фактически использовать

1488
01:00:25,839 --> 01:00:30,079
модель униграммы для наших вероятностей,

1489
01:00:30,480 --> 01:00:31,359
да

1490
01:00:31,359 --> 01:00:32,640
гм,

1491
01:00:32,640 --> 01:00:33,920
и поэтому,

1492
01:00:33,920 --> 01:00:36,000
когда вы

1493
01:00:36,000 --> 01:00:38,559
увеличиваете порядок n модели языка инграмм,

1494
01:00:38,559 --> 01:00:40,640
эти проблемы разреженности

1495
01:00:40,640 --> 01:00:43,119
становятся все хуже и хуже, поэтому в первые

1496
01:00:43,119 --> 01:00:45,280
дни люди обычно работали с  триграммные

1497
01:00:45,280 --> 01:00:48,400
модели, поскольку стало легче собирать

1498
01:00:48,400 --> 01:00:50,400
миллиарды слов текста,

1499
01:00:50,400 --> 01:00:52,400
люди обычно переходили к пятиграммовым

1500
01:00:52,400 --> 01:00:54,319
моделям, но

1501
01:00:54,319 --> 01:00:57,520
каждый раз, когда вы повышаете порядок

1502
01:00:57,520 --> 01:00:59,200
кондиционирования, вам

1503
01:00:59,200 --> 01:01:01,839
необходимо эффективно собирать

1504
01:01:01,839 --> 01:01:03,760
Из-за

1505
01:01:03,760 --> 01:01:06,000
размера словарей человеческих

1506
01:01:06,000 --> 01:01:08,559


1507
01:01:08,960 --> 01:01:10,960
языков существует также проблема, заключающаяся в том, что эти

1508
01:01:10,960 --> 01:01:15,119
модели

1509
01:01:15,119 --> 01:01:17,680


1510
01:01:17,680 --> 01:01:20,319


1511
01:01:20,319 --> 01:01:22,559
огромны.  Я имею в виду, что это на

1512
01:01:22,559 --> 01:01:25,119
самом деле оказало большое влияние на то,

1513
01:01:25,119 --> 01:01:28,400
какие технологии доступны, так что в

1514
01:01:28,400 --> 01:01:29,200


1515
01:01:29,200 --> 01:01:32,079
десятилетие 2000-х годов до этого, когда

1516
01:01:32,079 --> 01:01:34,559
был 2014 год

1517
01:01:34,559 --> 01:01:37,680
, уже был Google Translate

1518
01:01:37,680 --> 01:01:39,520
с использованием

1519
01:01:39,520 --> 01:01:42,079
вероятностных моделей, включая

1520
01:01:42,079 --> 01:01:44,079
языковые модели типа модели языка инграммы,

1521
01:01:44,079 --> 01:01:46,319
но единственное  они

1522
01:01:46,319 --> 01:01:49,440
могли бы запускаться в облаке,

1523
01:01:49,440 --> 01:01:50,880
потому что

1524
01:01:50,880 --> 01:01:52,880
вам нужно было иметь эти огромные таблицы

1525
01:01:52,880 --> 01:01:54,400
вероятностей,

1526
01:01:54,400 --> 01:01:56,319
но теперь у нас есть нейронные сети, и вы можете

1527
01:01:56,319 --> 01:01:58,480
просто запустить Google Translate

1528
01:01:58,480 --> 01:02:00,640
на вашем телефоне, и это

1529
01:02:00,640 --> 01:02:02,480
возможно, потому что

1530
01:02:02,480 --> 01:02:04,640
модели нейронных сетей могут быть значительно более

1531
01:02:04,640 --> 01:02:07,440
компактными.  чем эти старые модели языка инграмм,

1532
01:02:07,440 --> 01:02:09,839


1533
01:02:11,839 --> 01:02:13,200
да,

1534
01:02:13,200 --> 01:02:14,799
но, тем не менее, прежде чем мы перейдем к

1535
01:02:14,799 --> 01:02:16,720
новым моделям,

1536
01:02:16,720 --> 01:02:20,400
давайте

1537
01:02:20,400 --> 01:02:23,520
просто взглянем на  Пример того, как они

1538
01:02:23,520 --> 01:02:26,880
работают, так что обучить модель языка инграммы тривиально,

1539
01:02:26,880 --> 01:02:28,559
потому что вы действительно просто

1540
01:02:28,559 --> 01:02:31,200
подсчитываете, как часто последовательности слов встречаются

1541
01:02:31,200 --> 01:02:33,520
в корпусе, и вы готовы к работе, чтобы эти

1542
01:02:33,520 --> 01:02:35,520
модели можно было обучить за секунды, это

1543
01:02:35,520 --> 01:02:37,680
действительно хорошо, это не похоже на сидение

1544
01:02:37,680 --> 01:02:40,480
для обучения нейронных сетей, поэтому, если я

1545
01:02:40,480 --> 01:02:44,319
тренирую на своем ноутбуке небольшую языковую

1546
01:02:44,319 --> 01:02:46,960
модель, вы знаете около

1547
01:02:46,960 --> 01:02:51,039
1,7 миллиона слов в качестве модели триграммы, я

1548
01:02:51,039 --> 01:02:53,440
могу попросить ее сгенерировать текст, если я

1549
01:02:53,440 --> 01:02:55,839
дам ему пару слов сегодня, хотя тогда я

1550
01:02:55,839 --> 01:02:56,799


1551
01:02:56,799 --> 01:02:58,240
могу его получить  чтобы как бы

1552
01:02:58,240 --> 01:03:00,799
предложить слово, которое может появиться следующим,

1553
01:03:00,799 --> 01:03:03,599
и способ, которым я это делаю, заключается в том, что языковая модель

1554
01:03:03,599 --> 01:03:06,240
знает распределение вероятностей того,

1555
01:03:06,240 --> 01:03:08,880
что может произойти дальше, эм, обратите внимание,

1556
01:03:08,880 --> 01:03:10,880
есть своего рода грубое распределение вероятностей,

1557
01:03:10,880 --> 01:03:14,079
я имею в виду, потому что эффективно

1558
01:03:14,079 --> 01:03:16,720
по этому относительно небольшому корпусу существует

1559
01:03:16,720 --> 01:03:19,520
были вещи, которые произошли когда-то в Италии

1560
01:03:19,520 --> 01:03:20,559
и эмирате, были вещи, которые

1561
01:03:20,559 --> 01:03:22,880
произошли вдвое дороже, были вещи,

1562
01:03:22,880 --> 01:03:25,359
которые произошли эм четыре раза компания и

1563
01:03:25,359 --> 01:03:27,680
банк эм, это довольно грубо и

1564
01:03:27,680 --> 01:03:30,240
грубо  но я, тем не менее, получаю оценки вероятности,

1565
01:03:30,240 --> 01:03:33,599
я могу

1566
01:03:33,599 --> 01:03:37,280
тогда сказать, хорошо, на основе этого, давайте возьмем

1567
01:03:37,280 --> 01:03:39,760
это распределение вероятностей, а затем

1568
01:03:39,760 --> 01:03:42,079
мы просто выберем следующее слово, так что

1569
01:03:42,079 --> 01:03:44,799
два, скорее всего, были образцом компании

1570
01:03:44,799 --> 01:03:47,920
или банка, но мы бросаем кости

1571
01:03:47,920 --> 01:03:49,839
и  мы можем получить любое из следующих слов,

1572
01:03:49,839 --> 01:03:53,680
так что, может быть, я пробую цену

1573
01:03:53,680 --> 01:03:57,520
сейчас, я буду ставить цену на цену

1574
01:03:57,520 --> 01:03:58,720
и искать

1575
01:03:58,720 --> 01:04:00,640
распределение вероятностей того, что

1576
01:04:00,640 --> 01:04:03,680
будет дальше, что наиболее вероятно,

1577
01:04:03,680 --> 01:04:06,160
и поэтому снова я пробую и  возможно, на этот

1578
01:04:06,160 --> 01:04:09,119
раз я возьму,

1579
01:04:09,119 --> 01:04:12,000
и тогда я теперь

1580
01:04:12,000 --> 01:04:15,280
буду определять цену, и я буду искать распределение вероятностей

1581
01:04:15,280 --> 01:04:18,160
слов, следующих за этим, и

1582
01:04:18,160 --> 01:04:21,280
я получу это распределение вероятностей,

1583
01:04:21,280 --> 01:04:23,680
и я выберу случайным образом какое-то слово из

1584
01:04:23,680 --> 01:04:26,559
него, и, возможно, на этот раз  я попробую

1585
01:04:26,559 --> 01:04:30,240
редкий, но возможный образец, такой как золото, и я

1586
01:04:30,240 --> 01:04:32,400
могу продолжить, и я выскажу

1587
01:04:32,400 --> 01:04:34,079
что-то подобное

1588
01:04:34,079 --> 01:04:36,640
сегодня цена золота за тонну при

1589
01:04:36,640 --> 01:04:38,559
производстве обуви и обувной

1590
01:04:38,559 --> 01:04:41,119
промышленности банк вмешался сразу после

1591
01:04:41,119 --> 01:04:43,599
того, как он рассмотрел и отклонил  удовлетворено требование

1592
01:04:43,599 --> 01:04:46,720
МВФ восстановить истощенные европейские запасы шаг

1593
01:04:46,720 --> 01:04:51,200
30 n первичные 76 центов на акцию, так что то,

1594
01:04:51,200 --> 01:04:54,319
что простая модель триграммы может произвести

1595
01:04:54,319 --> 01:04:57,119
поверх небольшого количества текста, на самом деле

1596
01:04:57,119 --> 01:04:59,520
уже довольно интересно, как будто это на

1597
01:04:59,520 --> 01:05:02,079
самом деле удивительно грамматически, тут

1598
01:05:02,079 --> 01:05:03,839
есть целые части  в то время как

1599
01:05:03,839 --> 01:05:05,839
производство обуви в последнюю очередь ощущается, что вы

1600
01:05:05,839 --> 01:05:08,319
производите собеседование с банком, вмешивается

1601
01:05:08,319 --> 01:05:10,480
сразу после того, как он может сесть и отклонить

1602
01:05:10,480 --> 01:05:12,480
требование МВФ, правда, это на самом деле довольно

1603
01:05:12,480 --> 01:05:15,200
хороший грамматический текст, так что это

1604
01:05:15,200 --> 01:05:17,119
просто удивительно, что эти простые модели инграмм на

1605
01:05:17,119 --> 01:05:20,319
самом деле могут моделировать множество людей

1606
01:05:20,319 --> 01:05:21,760
язык

1607
01:05:21,760 --> 01:05:23,839
с другой стороны, это не очень хороший

1608
01:05:23,839 --> 01:05:26,400
фрагмент текста, он совершенно бессвязен

1609
01:05:26,400 --> 01:05:28,960
и не имеет смысла,

1610
01:05:28,960 --> 01:05:32,240
и поэтому, чтобы действительно иметь возможность генерировать

1611
01:05:32,240 --> 01:05:35,280
текст, который кажется имеющим смысл,

1612
01:05:35,280 --> 01:05:37,280
нам понадобится значительно

1613
01:05:37,280 --> 01:05:39,119
лучшая языковая модель, и это

1614
01:05:39,119 --> 01:05:40,880
именно

1615
01:05:40,880 --> 01:05:42,880
то, что  модели нейронного языка позволили

1616
01:05:42,880 --> 01:05:46,240
нам построить, как мы увидим позже,

1617
01:05:46,240 --> 01:05:48,960
хорошо, так как же мы можем построить модель нейронного

1618
01:05:48,960 --> 01:05:51,760
языка, и поэтому,

1619
01:05:51,760 --> 01:05:53,920
собираюсь сделать простой,

1620
01:05:53,920 --> 01:05:55,039
а затем

1621
01:05:55,039 --> 01:05:57,359
мы посмотрим, что мы получим, но переход

1622
01:05:57,359 --> 01:05:59,119
в текущую нейронную сеть может по-прежнему увести

1623
01:05:59,119 --> 01:06:02,160
нас в следующий раз,

1624
01:06:02,319 --> 01:06:03,440
поэтому

1625
01:06:03,440 --> 01:06:05,359
будет входная последовательность слов,

1626
01:06:05,359 --> 01:06:07,760
и мы хотим, чтобы распределение вероятностей

1627
01:06:07,760 --> 01:06:10,880
для следующего слова хорошо  Самое простое

1628
01:06:10,880 --> 01:06:14,400
, что мы могли бы попробовать, - это сказать,

1629
01:06:14,400 --> 01:06:17,520
что единственный инструмент, который у нас есть до сих пор, -

1630
01:06:17,520 --> 01:06:21,200
это оконный классификатор, поэтому мы можем

1631
01:06:21,200 --> 01:06:24,960
сказать, что вы знаете, что мы делали ранее

1632
01:06:24,960 --> 01:06:27,599
либо для нашей именованной сущности, признанной

1633
01:06:27,599 --> 01:06:29,440
в лекции 3, либо для чего-то еще.  Я только что показал

1634
01:06:29,440 --> 01:06:31,920
вам для парсера зависимостей, у нас есть

1635
01:06:31,920 --> 01:06:34,559
какое-то контекстное окно, которое мы пропускаем через

1636
01:06:34,559 --> 01:06:37,039
нейронную сеть, и мы предсказываем что-то как

1637
01:06:37,039 --> 01:06:40,319
классификатор, поэтому до того, как мы

1638
01:06:40,319 --> 01:06:42,880
предсказывали местоположение, но, возможно,

1639
01:06:42,880 --> 01:06:46,000
вместо этого мы могли бы повторно использовать ту же

1640
01:06:46,000 --> 01:06:47,359
технологию

1641
01:06:47,359 --> 01:06:49,119
и сказать, что мы  у нас будет

1642
01:06:49,119 --> 01:06:51,200
оконный классификатор, поэтому мы

1643
01:06:51,200 --> 01:06:53,839
отбрасываем более удаленные слова так же,

1644
01:06:53,839 --> 01:06:55,920
как в

1645
01:06:55,920 --> 01:06:58,640
языковой модели инграмм, но мы будем передавать

1646
01:06:58,640 --> 01:07:02,720
это фиксированное окно в нейронную сеть,

1647
01:07:02,720 --> 01:07:05,599
чтобы мы объединяли вложения слов, которые

1648
01:07:05,599 --> 01:07:07,520
мы пропускаем через скрытую  слой,

1649
01:07:07,520 --> 01:07:11,680
а затем у нас есть классификатор softmax

1650
01:07:11,680 --> 01:07:12,720
по

1651
01:07:12,720 --> 01:07:14,640
нашему словарю,

1652
01:07:14,640 --> 01:07:16,559
и поэтому теперь вместо того, чтобы предсказывать

1653
01:07:16,559 --> 01:07:19,200
что-то вроде местоположения или

1654
01:07:19,200 --> 01:07:21,920
левой дуги в анализаторе зависимостей, мы

1655
01:07:21,920 --> 01:07:24,319
собираемся иметь мягкий максимум по всему

1656
01:07:24,319 --> 01:07:27,359
словарю вроде того, что мы сделали

1657
01:07:27,359 --> 01:07:29,280
с отрицательной выборкой skipgram

1658
01:07:29,280 --> 01:07:32,160
модель в первых двух лекциях, и поэтому

1659
01:07:32,160 --> 01:07:35,200
мы собираемся рассматривать этот выбор как

1660
01:07:35,200 --> 01:07:37,599
предсказание того, какое слово будет следующим,

1661
01:07:37,599 --> 01:07:41,039
будет ли оно производить книги умов ноутбуков и

1662
01:07:41,039 --> 01:07:43,440
т.д.

1663
01:07:43,440 --> 01:07:45,119


1664
01:07:45,119 --> 01:07:47,839


1665
01:07:47,839 --> 01:07:49,920


1666
01:07:49,920 --> 01:07:53,119


1667
01:07:53,119 --> 01:07:55,440


1668
01:07:55,440 --> 01:07:58,160
в использовании нейронных сетей для nlp-

1669
01:07:58,160 --> 01:08:01,520
приложений, так что сначала в

1670
01:08:01,520 --> 01:08:03,039
статье на конференции 2000 года, а затем в несколько

1671
01:08:03,039 --> 01:08:04,559
более поздней

1672
01:08:04,559 --> 01:08:06,880
журнальной статье Джошуа Бенжио и его

1673
01:08:06,880 --> 01:08:09,920
коллеги представили именно эту

1674
01:08:09,920 --> 01:08:12,640
модель как нейронную вероятностную

1675
01:08:12,640 --> 01:08:15,039
языковую модель, и они уже

1676
01:08:15,039 --> 01:08:16,960
смогли показать,

1677
01:08:16,960 --> 01:08:19,520
что это может дать интересную пользу

1678
01:08:19,520 --> 01:08:22,719
результаты для языкового моделирования, и поэтому

1679
01:08:22,719 --> 01:08:25,839
это не было отличным решением для

1680
01:08:25,839 --> 01:08:29,120
моделирования нейронного языка, но оно все еще имело ценность,

1681
01:08:29,120 --> 01:08:31,600
поэтому оно имело  не решает проблему,

1682
01:08:31,600 --> 01:08:34,158
позволяя нам иметь больший контекст для

1683
01:08:34,158 --> 01:08:35,839
предсказания того, какие слова появятся

1684
01:08:35,839 --> 01:08:39,839
дальше, он таким образом ограничен

1685
01:08:39,839 --> 01:08:43,120
точно так же, как и модель языка инграмм,

1686
01:08:43,120 --> 01:08:45,520
но имеет все преимущества

1687
01:08:45,520 --> 01:08:47,759
распределенных представлений,

1688
01:08:47,759 --> 01:08:49,759
поэтому вместо того, чтобы иметь

1689
01:08:49,759 --> 01:08:53,439
эти подсчеты  для последовательностей слов, которые

1690
01:08:53,439 --> 01:08:57,198
очень редки и очень грубые, мы можем использовать

1691
01:08:57,198 --> 01:08:59,600
распределенные распределенные

1692
01:08:59,600 --> 01:09:02,880
представления слов, которые затем делают прогнозы,

1693
01:09:02,880 --> 01:09:05,920
что семантически похожие слова должны

1694
01:09:05,920 --> 01:09:06,719
давать

1695
01:09:06,719 --> 01:09:09,359
схожие распределения вероятностей, поэтому

1696
01:09:09,359 --> 01:09:12,000
идея состоит в том, если мы используем здесь какое-то другое

1697
01:09:12,000 --> 01:09:15,439
слово, например, возможно, ученики открываются

1698
01:09:15,439 --> 01:09:19,040
там  ну, может быть, в наших тренировочных данных

1699
01:09:19,040 --> 01:09:21,839
мы видели предложения о студентах, но

1700
01:09:21,839 --> 01:09:24,399
мы никогда не видели предложений об учениках

1701
01:09:24,399 --> 01:09:26,560
, модель языка инграмм, тогда как

1702
01:09:26,560 --> 01:09:28,719
бы понятия не имела, какие вероятности

1703
01:09:28,719 --> 01:09:30,960
использовать, тогда

1704
01:09:30,960 --> 01:09:33,439
как новая языковая модель может сказать, что хорошие ученики

1705
01:09:33,439 --> 01:09:35,759
- это своего рода  похоже на студентов, поэтому

1706
01:09:35,759 --> 01:09:37,679
я могу предсказать аналогично тому, что я

1707
01:09:37,679 --> 01:09:41,040
бы предсказал для студентов,

1708
01:09:41,040 --> 01:09:42,238
хорошо,

1709
01:09:42,238 --> 01:09:45,520
так что теперь нет проблем с разреженностью  em нам

1710
01:09:45,520 --> 01:09:47,359
не нужно

1711
01:09:47,359 --> 01:09:49,359
хранить

1712
01:09:49,359 --> 01:09:52,399
миллиарды счетчиков инграмм, нам просто

1713
01:09:52,399 --> 01:09:54,000
нужно хранить

1714
01:09:54,000 --> 01:09:57,280
наши векторы слов и наши матрицы w и u,

1715
01:09:57,280 --> 01:09:59,120


1716
01:09:59,120 --> 01:10:01,360
но у нас все еще есть оставшиеся проблемы,

1717
01:10:01,360 --> 01:10:04,560
что наше фиксированное окно слишком мало,

1718
01:10:04,560 --> 01:10:07,600
мы можем попытаться увеличить окно, если

1719
01:10:07,600 --> 01:10:11,199
мы  Сделайте это, когда матрица w станет больше,

1720
01:10:11,199 --> 01:10:13,520
но это также указывает на другую проблему

1721
01:10:13,520 --> 01:10:15,440
с этой моделью:

1722
01:10:15,440 --> 01:10:17,360
не только окно никогда не будет

1723
01:10:17,360 --> 01:10:18,800
достаточно большим,

1724
01:10:18,800 --> 01:10:20,000
но

1725
01:10:20,000 --> 01:10:23,520
w - это просто обученная матрица, и

1726
01:10:23,520 --> 01:10:25,920
поэтому мы изучаем совершенно

1727
01:10:25,920 --> 01:10:28,640
разные веса для каждой позиции

1728
01:10:28,640 --> 01:10:31,440
контекста,  слово минус одна позиция

1729
01:10:31,440 --> 01:10:33,679
слово минус два, слово минус три

1730
01:10:33,679 --> 01:10:36,159
и слово минус четыре, так что модель не может

1731
01:10:36,159 --> 01:10:39,040
разделять то,

1732
01:10:39,040 --> 01:10:41,679
как она обрабатывает слова в разных

1733
01:10:41,679 --> 01:10:45,360
позициях, хотя в некотором смысле

1734
01:10:45,360 --> 01:10:48,080
они будут вносить семантические компоненты

1735
01:10:48,080 --> 01:10:50,800
, которые, по крайней мере, в некоторой степени  гм,

1736
01:10:50,800 --> 01:10:53,520
позиция независима, так что снова для тех, кто,

1737
01:10:53,520 --> 01:10:55,679
если вы как бы вспомните либо

1738
01:10:55,679 --> 01:10:58,800
наивную байесовскую модель, либо то, что мы видели

1739
01:10:58,800 --> 01:11:00,800
со словом to vect model в

1740
01:11:00,800 --> 01:11:02,880
начале, словом to vect model или

1741
01:11:02,880 --> 01:11:05,360
наивным  Модель Байеса полностью игнорирует

1742
01:11:05,360 --> 01:11:07,520
порядок слов, поэтому у нее есть один набор

1743
01:11:07,520 --> 01:11:09,920
параметров, независимо от того, в какой позиции

1744
01:11:09,920 --> 01:11:12,239
происходит что-то, что не подходит

1745
01:11:12,239 --> 01:11:14,640
для языкового моделирования, потому что порядок

1746
01:11:14,640 --> 01:11:16,800
слов действительно важен в языковом моделировании,

1747
01:11:16,800 --> 01:11:19,440
если последнее слово является действительно

1748
01:11:19,440 --> 01:11:21,040
хорошим предиктором  есть

1749
01:11:21,040 --> 01:11:23,199
прилагательное или существительное, после которого, если

1750
01:11:23,199 --> 01:11:26,800
слово «спина» - это эм, оно не дает

1751
01:11:26,800 --> 01:11:28,800
вам той же информации, поэтому вы действительно хотите

1752
01:11:28,800 --> 01:11:29,679
в

1753
01:11:29,679 --> 01:11:31,920
некоторой степени

1754
01:11:31,920 --> 01:11:35,120
использовать порядок слов, но эта модель находится

1755
01:11:35,120 --> 01:11:37,600
на противоположной крайности,

1756
01:11:37,600 --> 01:11:39,520
чем моделируется каждая позиция  полностью

1757
01:11:39,520 --> 01:11:42,159
независимо,

1758
01:11:42,159 --> 01:11:44,800
поэтому мы хотели бы иметь нейронную

1759
01:11:44,800 --> 01:11:48,000
архитектуру, которая может обрабатывать

1760
01:11:48,000 --> 01:11:51,360
произвольный объем контекста

1761
01:11:51,360 --> 01:11:54,320
и иметь больше общих параметров, но

1762
01:11:54,320 --> 01:11:58,320
при этом чувствительна к близости,

1763
01:11:58,320 --> 01:12:00,800
и это идея повторяющихся

1764
01:12:00,800 --> 01:12:03,520
нейронных сетей, и я скажу о  пять

1765
01:12:03,520 --> 01:12:05,040
минут об

1766
01:12:05,040 --> 01:12:07,280
этом сегодня, а затем в следующий раз мы

1767
01:12:07,280 --> 01:12:09,840
вернемся и сделаем больше о ваших

1768
01:12:09,840 --> 01:12:12,560
повторяющихся нейронных сетях, поэтому

1769
01:12:12,560 --> 01:12:15,280
для повторяющейся нейронной сети, а

1770
01:12:15,280 --> 01:12:19,760
не для havi  ng один скрытый слой внутри

1771
01:12:19,760 --> 01:12:23,520
нашего классификатора здесь, который мы вычисляем каждый

1772
01:12:23,520 --> 01:12:24,640
раз

1773
01:12:24,640 --> 01:12:27,760
для рекуррентной нейронной сети, у нас

1774
01:12:27,760 --> 01:12:30,000
есть скрытый слой, который часто

1775
01:12:30,000 --> 01:12:32,239
называют скрытым состоянием,

1776
01:12:32,239 --> 01:12:35,920
но мы поддерживаем его с течением времени, и мы

1777
01:12:35,920 --> 01:12:38,640
возвращаем его обратно в себя, так что вот что

1778
01:12:38,640 --> 01:12:40,560
слово «рекуррентный» означает, что

1779
01:12:40,560 --> 01:12:43,360
вы как бы «скармливаете» скрытый слой

1780
01:12:43,360 --> 01:12:45,199
обратно в себя,

1781
01:12:45,199 --> 01:12:47,120
поэтому то, что мы делаем

1782
01:12:47,120 --> 01:12:50,480
, основано на первом слове, которое

1783
01:12:50,480 --> 01:12:53,600
мы вычисляем в скрытом представлении,

1784
01:12:53,600 --> 01:12:56,000
подобном предыдущему, которое можно использовать для

1785
01:12:56,000 --> 01:12:58,640
предсказания следующего слова,

1786
01:12:58,640 --> 01:12:59,840
но затем

1787
01:12:59,840 --> 01:13:00,880
для того, когда

1788
01:13:00,880 --> 01:13:03,440
мы хотим предсказать, что будет после

1789
01:13:03,440 --> 01:13:06,159
второго слова, которое мы не только подаем во

1790
01:13:06,159 --> 01:13:10,560
втором слове, которое мы подаем в скрытом

1791
01:13:10,560 --> 01:13:13,679
слое из предыдущего слова,

1792
01:13:13,679 --> 01:13:16,560
чтобы оно помогло предсказать скрытый слой

1793
01:13:16,560 --> 01:13:19,199
над вторым словом и, таким образом, формально

1794
01:13:19,199 --> 01:13:21,600
то, как мы это делаем  мы

1795
01:13:21,600 --> 01:13:24,880
берем скрытый слой над первым словом,

1796
01:13:24,880 --> 01:13:28,320
умножая его на матрицу w,

1797
01:13:28,320 --> 01:13:30,800
а затем он будет идти

1798
01:13:30,800 --> 01:13:34,080
вместе с x2 для генерации следующего

1799
01:13:34,080 --> 01:13:35,440
скрытого шага,

1800
01:13:35,440 --> 01:13:37,679
и поэтому мы продолжаем делать это каждый

1801
01:13:37,679 --> 01:13:40,239
раз, когда ste  p, так что мы как бы

1802
01:13:40,239 --> 01:13:43,520
повторяем шаблон создания следующего

1803
01:13:43,520 --> 01:13:45,440
скрытого слоя

1804
01:13:45,440 --> 01:13:49,199
на основе следующего входного слова и

1805
01:13:49,199 --> 01:13:52,080
предыдущего скрытого состояния

1806
01:13:52,080 --> 01:13:54,960
, обновляя его путем умножения на матрицу.

1807
01:13:54,960 --> 01:13:56,880
четыре

1808
01:13:56,880 --> 01:13:58,719
слова контекста, потому что это хорошо для

1809
01:13:58,719 --> 01:14:00,800
моего слайда, но вы знаете, что

1810
01:14:00,800 --> 01:14:02,719
в принципе вы можете знать любое

1811
01:14:02,719 --> 01:14:05,920
количество слов контекста.

1812
01:14:05,920 --> 01:14:09,199


1813
01:14:09,199 --> 01:14:10,000


1814
01:14:10,000 --> 01:14:12,640


1815
01:14:12,640 --> 01:14:14,640


1816
01:14:14,640 --> 01:14:16,560
векторы, которые мы искали

1817
01:14:16,560 --> 01:14:18,080
для каждого слова,

1818
01:14:18,080 --> 01:14:19,840


1819
01:14:19,840 --> 01:14:20,640


1820
01:14:20,640 --> 01:14:22,719
извините, да, чтобы у нас были одни горячие

1821
01:14:22,719 --> 01:14:25,040
векторы для идентичности слова,

1822
01:14:25,040 --> 01:14:26,960
мы ищем наше вложение слов, поэтому

1823
01:14:26,960 --> 01:14:30,159
у нас есть вложения слов для каждого слова,

1824
01:14:30,159 --> 01:14:32,480
а затем мы хотим вычислить скрытые

1825
01:14:32,480 --> 01:14:34,960
состояния  поэтому нам нужно начать откуда-

1826
01:14:34,960 --> 01:14:38,560
то h0 - это начальное скрытое состояние,

1827
01:14:38,560 --> 01:14:41,920
а h0 обычно принимается как нулевой

1828
01:14:41,920 --> 01:14:43,600
вектор, поэтому на самом деле это просто

1829
01:14:43,600 --> 01:14:47,120
инициализация нулей, и поэтому для

1830
01:14:47,120 --> 01:14:50,159
определения первого скрытого состояния мы вычисляем

1831
01:14:50,159 --> 01:14:53,679
его на основе первого слова, которое оно

1832
01:14:53,679 --> 01:14:56,640
встраивает.  умножение этого  s встраивание

1833
01:14:56,640 --> 01:15:00,159
с помощью матрицы, которая дает нам

1834
01:15:00,159 --> 01:15:02,400
первое скрытое состояние,

1835
01:15:02,400 --> 01:15:03,520
но затем

1836
01:15:03,520 --> 01:15:06,719
вы знаете, что по мере продолжения мы хотим применить

1837
01:15:06,719 --> 01:15:10,239
ту же формулу снова, поэтому у нас есть

1838
01:15:10,239 --> 01:15:11,760
только две

1839
01:15:11,760 --> 01:15:14,000
матрицы параметров

1840
01:15:14,000 --> 01:15:16,880
в рекуррентной нейронной сети,

1841
01:15:16,880 --> 01:15:19,280
одна матрица для умножения входных

1842
01:15:19,280 --> 01:15:22,640
вложений и  одна матрица для

1843
01:15:22,640 --> 01:15:25,199
обновления скрытого состояния сети, и поэтому

1844
01:15:25,199 --> 01:15:27,600
для второго слова

1845
01:15:27,600 --> 01:15:30,880
из его вложения слова мы умножаем

1846
01:15:30,880 --> 01:15:32,000
его

1847
01:15:32,000 --> 01:15:35,440
на матрицу we, которую мы берем за предыдущие временные

1848
01:15:35,440 --> 01:15:38,159
шаги, скрытое состояние, и умножаем его

1849
01:15:38,159 --> 01:15:40,159
на матрицу wh,

1850
01:15:40,159 --> 01:15:43,760
и мы используем два из этих um  чтобы

1851
01:15:43,760 --> 01:15:45,840
сгенерировать новое скрытое состояние и

1852
01:15:45,840 --> 01:15:48,000
точно, как мы генерируем новое скрытое

1853
01:15:48,000 --> 01:15:51,199
состояние, затем показано в этом уравнении

1854
01:15:51,199 --> 01:15:52,400
слева,

1855
01:15:52,400 --> 01:15:54,640
поэтому мы берем предыдущее скрытое состояние,

1856
01:15:54,640 --> 01:15:57,840
умножаем его на то, что мы берем

1857
01:15:57,840 --> 01:16:00,719
входное вложение, умножаем на,

1858
01:16:00,719 --> 01:16:02,480
мы суммируем те два, которые

1859
01:16:02,480 --> 01:16:03,920
мы

1860
01:16:03,920 --> 01:16:08,080
добавляем вес смещения обучения, а затем мы

1861
01:16:08,080 --> 01:16:10,880
применяем это через нелинейность,

1862
01:16:10,880 --> 01:16:12,719
и хотя на этом слайде эта

1863
01:16:12,719 --> 01:16:15,679
нелинейность написана как сигма, на сегодняшний

1864
01:16:15,679 --> 01:16:18,239
день наиболее распространенной нелинейностью для использования

1865
01:16:18,239 --> 01:16:22,159
здесь на самом деле является загар  h нелинейность,

1866
01:16:22,159 --> 01:16:26,000
и поэтому это основное уравнение

1867
01:16:26,000 --> 01:16:28,800
для простой рекуррентной нейронной сети,

1868
01:16:28,800 --> 01:16:31,360
и для каждого последующего временного шага

1869
01:16:31,360 --> 01:16:33,760
мы просто продолжим применять это для

1870
01:16:33,760 --> 01:16:36,000
выработки скрытых состояний,

1871
01:16:36,000 --> 01:16:38,960
а затем из этих скрытых состояний мы можем

1872
01:16:38,960 --> 01:16:41,760
использовать их так же, как в  наш оконный

1873
01:16:41,760 --> 01:16:44,159
классификатор, чтобы предсказать, каким будет

1874
01:16:44,159 --> 01:16:48,000
следующее слово, поэтому в любой позиции мы можем взять

1875
01:16:48,000 --> 01:16:50,560
этот скрытый вектор,

1876
01:16:50,560 --> 01:16:52,800
пропустить его через мягкий максимальный слой, который

1877
01:16:52,800 --> 01:16:54,960
умножается на матрицу u и добавляет

1878
01:16:54,960 --> 01:16:57,360
другое смещение, а затем делает из этого мягкое максимальное

1879
01:16:57,360 --> 01:16:59,360
распределение и

1880
01:16:59,360 --> 01:17:00,719
это даст нам распределение вероятностей для

1881
01:17:00,719 --> 01:17:03,920
следующих слов,

1882
01:17:03,920 --> 01:17:07,280
что мы видели здесь,

1883
01:17:07,280 --> 01:17:10,480
это вся математика простой рекуррентной

1884
01:17:10,480 --> 01:17:13,199
нейронной сети, и в следующий раз я вернусь и расскажу

1885
01:17:13,199 --> 01:17:14,800


1886
01:17:14,800 --> 01:17:17,280
о них больше, но это

1887
01:17:17,280 --> 01:17:18,719
все,

1888
01:17:18,719 --> 01:17:21,199
что вам нужно  знать в некотором смысле

1889
01:17:21,199 --> 01:17:23,920
для вычисления прямой

1890
01:17:23,920 --> 01:17:26,480
модели простой рекуррентной нейронной сети, чтобы получить

1891
01:17:26,480 --> 01:17:28,719
преимущества, которые у нас есть сейчас, поскольку она может

1892
01:17:28,719 --> 01:17:34,320
обрабатывать ввод текста любой

1893
01:17:34,320 --> 01:17:36,239
длины теоретически, по крайней мере, она может использоваться

1894
01:17:36,239 --> 01:17:38,800
в  формирование из любого количества шагов

1895
01:17:38,800 --> 01:17:40,880
назад, мы поговорим на практике больше о

1896
01:17:40,880 --> 01:17:43,679
том, насколько хорошо это на самом деле работает,

1897
01:17:43,679 --> 01:17:45,840
размер модели фиксирован, не имеет

1898
01:17:45,840 --> 01:17:47,520
значения, сколько

1899
01:17:47,520 --> 01:17:50,320
из прошлого контекста есть, все, что у нас есть,

1900
01:17:50,320 --> 01:17:53,840
это наши wh и мы параметры

1901
01:17:53,840 --> 01:17:56,400
и на каждом  шаг времени

1902
01:17:56,400 --> 01:17:58,400
мы используем точно такие же веса для

1903
01:17:58,400 --> 01:18:01,120
обновления нашего скрытого состояния, так что есть

1904
01:18:01,120 --> 01:18:04,159
симметрия в том, как различные входные данные

1905
01:18:04,159 --> 01:18:07,600
обрабатываются при создании

1906
01:18:07,600 --> 01:18:10,400
наших прогнозов на практике, хотя эти

1907
01:18:10,400 --> 01:18:13,679
простые значения и практика не идеальны,

1908
01:18:13,679 --> 01:18:16,719
поэтому недостатком является то, что они  на

1909
01:18:16,719 --> 01:18:18,640
самом деле немного медленны, потому что с этим

1910
01:18:18,640 --> 01:18:21,120
повторяющимся вычислением

1911
01:18:21,120 --> 01:18:23,360
в некотором смысле мы как бы застряли с

1912
01:18:23,360 --> 01:18:25,360
необходимостью иметь снаружи цикла for,

1913
01:18:25,360 --> 01:18:27,920
поэтому мы можем выполнять

1914
01:18:27,920 --> 01:18:30,880
умножение векторной матрицы внутри здесь, но на самом деле

1915
01:18:30,880 --> 01:18:35,040
мы должны сделать, чтобы временной шаг был равен  От 1 до

1916
01:18:35,040 --> 01:18:36,239
n

1917
01:18:36,239 --> 01:18:39,120
вычисляются последовательные скрытые состояния,

1918
01:18:39,120 --> 01:18:41,520
и поэтому это не идеальная архитектура нейронной сети,

1919
01:18:41,520 --> 01:18:43,840
и мы

1920
01:18:43,840 --> 01:18:47,360
обсудим альтернативы этому позже,

1921
01:18:47,360 --> 01:18:50,080
и хотя теоретически эта модель может

1922
01:18:50,080 --> 01:18:52,400
получить доступ к информации за любое количество шагов.

1923
01:18:52,400 --> 01:18:55,040
возвращаясь к практике, мы обнаруживаем,

1924
01:18:55,040 --> 01:18:57,600
что это довольно несовершенно,

1925
01:18:57,600 --> 01:18:59,840
и это приведет к

1926
01:18:59,840 --> 01:19:01,760
более продвинутым формам рекуррентной нейронной

1927
01:19:01,760 --> 01:19:04,880
сети, о которых я расскажу в следующий раз

1928
01:19:04,880 --> 01:19:07,920
, которые могут более эффективно получать доступ к

1929
01:19:07,920 --> 01:19:09,920
прошлому контексту,

1930
01:19:09,920 --> 01:19:14,600
хорошо, я думаю, я остановлюсь на этом  На день

