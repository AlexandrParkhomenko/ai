1
00:00:05,359 --> 00:00:06,640
привет всем

2
00:00:06,640 --> 00:00:10,639
добро пожаловать на лекцию cs224n девять

3
00:00:10,639 --> 00:00:14,559
эээ самовнимание и трансформеры ээээ, если

4
00:00:14,559 --> 00:00:16,480
я не могу быть услышанным прямо сейчас,

5
00:00:16,480 --> 00:00:17,760
пожалуйста, отправьте сообщение в

6
00:00:17,760 --> 00:00:21,199
чате, потому что я никого не вижу

7
00:00:21,199 --> 00:00:24,000
эм ээээээээээ, я очень рад получить  в

8
00:00:24,000 --> 00:00:26,880
содержание на сегодня, мы будем говорить

9
00:00:26,880 --> 00:00:30,240
о самовнимании и трансформерах.

10
00:00:30,240 --> 00:00:32,960
Позвольте нам погрузиться в

11
00:00:32,960 --> 00:00:34,480
план лекции, и мы поговорим о

12
00:00:34,480 --> 00:00:36,800
каких-то делах для

13
00:00:36,800 --> 00:00:39,040
курса, так что мы начнем с  куда

14
00:00:39,040 --> 00:00:40,640
мы

15
00:00:40,640 --> 00:00:41,600
вернулись на

16
00:00:41,600 --> 00:00:43,840
прошлой неделе с рекуррентными рекуррентными

17
00:00:43,840 --> 00:00:45,760
нейронными сетями, и мы поговорим о

18
00:00:45,760 --> 00:00:47,520
движении от рецидивов к

19
00:00:47,520 --> 00:00:49,360
моделям НЛП, основанным на внимании,

20
00:00:49,360 --> 00:00:51,520
о которых мы говорили о внимании, и мы собираемся просто

21
00:00:51,520 --> 00:00:54,000
сосредоточить внимание на этом, мы

22
00:00:54,000 --> 00:00:56,079
представим трансформатор  модель, которая представляет

23
00:00:56,079 --> 00:00:58,000
собой особый тип модели, основанной на внимании,

24
00:00:58,000 --> 00:01:00,239
которая очень популярна,

25
00:01:00,239 --> 00:01:01,520
вам нужно знать, что вы собираетесь ее

26
00:01:01,520 --> 00:01:03,120
изучить, мы поговорим о некоторых отличных

27
00:01:03,120 --> 00:01:05,438
результатах с трансформаторами, а затем о некоторых

28
00:01:05,438 --> 00:01:07,760
недостатках и вариантах, а также о

29
00:01:07,760 --> 00:01:08,880
недавней

30
00:01:08,880 --> 00:01:11,360
работе по улучшению  их так некоторые напоминают

31
00:01:11,360 --> 00:01:13,040
Перед тем, как мы перейдем к

32
00:01:13,040 --> 00:01:15,680
заданию 4, необходимо

33
00:01:15,680 --> 00:01:18,080
провести опрос отзывов в середине квартала во

34
00:01:18,080 --> 00:01:20,000
вторник, 16 февраля,

35
00:01:20,000 --> 00:01:21,040
вы получите

36
00:01:21,040 --> 00:01:23,360
небольшое количество баллов за это,

37
00:01:23,360 --> 00:01:24,960
и мы очень ценим ваши

38
00:01:24,960 --> 00:01:26,640
отзывы о том, что мы сделали хорошо, что мы

39
00:01:26,640 --> 00:01:27,840
можем улучшить,

40
00:01:27,840 --> 00:01:29,280
а затем

41
00:01:29,280 --> 00:01:33,200
Окончательное проектное предложение также должно быть сделано.

42
00:01:33,200 --> 00:01:35,200
Одно примечание о предложениях.

43
00:01:35,200 --> 00:01:40,159
Цель предложения состоит в том, чтобы вы знаете,

44
00:01:40,240 --> 00:01:41,759
или я бы сказал, что основная часть

45
00:01:41,759 --> 00:01:44,479
цели предложения состоит в том, чтобы дать вам обратную связь

46
00:01:44,479 --> 00:01:47,520
по идее, которую вы представили.  и

47
00:01:47,520 --> 00:01:51,759
убедитесь, что

48
00:01:51,759 --> 00:01:55,360
это жизнеспособный вариант для окончательного проекта, и

49
00:01:55,360 --> 00:01:57,360
убедитесь, что мы как бы перецентрируем, если нет, и

50
00:01:57,360 --> 00:01:59,040
поэтому мы хотим очень быстро получить обратную связь с вами

51
00:01:59,040 --> 00:02:02,079
об этом, хорошо,

52
00:02:02,079 --> 00:02:04,479
так что давайте

53
00:02:04,479 --> 00:02:08,639
начнем с содержания  лекция на этой неделе,

54
00:02:08,639 --> 00:02:10,080
так что

55
00:02:10,080 --> 00:02:12,959
мы были в этом месте в nlp на прошлой

56
00:02:12,959 --> 00:02:14,720
неделе, где у нас были повторяющиеся нейронные

57
00:02:14,720 --> 00:02:16,800
сети, вроде многих вещей,

58
00:02:16,800 --> 00:02:18,640
которые вы хотели сделать,

59
00:02:18,640 --> 00:02:21,680
так что это примерно в 2016 году, и стратегия, если

60
00:02:21,680 --> 00:02:24,319
вы хотите построить сильную модель nlp,

61
00:02:24,319 --> 00:02:26,160
ты знаешь, что послал  что вам

62
00:02:26,160 --> 00:02:27,760
нужно кодировать, и у вас есть как

63
00:02:27,760 --> 00:02:31,519
двунаправленный lstm, скажите um, и вы знаете,

64
00:02:31,519 --> 00:02:32,640
может быть, это немного похоже на это

65
00:02:32,640 --> 00:02:34,239
пиктографически,

66
00:02:34,239 --> 00:02:36,080
и, возможно, это исходное предложение в

67
00:02:36,080 --> 00:02:37,680
переводе, например, мы видели машинный

68
00:02:37,680 --> 00:02:39,840
перевод, а затем вы определяете свой

69
00:02:39,840 --> 00:02:41,280
вывод, который может быть последовательностью

70
00:02:41,280 --> 00:02:43,360
слов, которая является целью, вы знаете

71
00:02:43,360 --> 00:02:44,319
перевод, который вы пытаетесь

72
00:02:44,319 --> 00:02:46,800
предсказать, или, может быть, это дерево синтаксического анализа или

73
00:02:46,800 --> 00:02:48,319
это резюме,

74
00:02:48,319 --> 00:02:49,519
и вы

75
00:02:49,519 --> 00:02:52,879
используете lstm с одним направлением

76
00:02:52,879 --> 00:02:54,640
для его генерации,

77
00:02:54,640 --> 00:02:56,959
и это действительно работает  ну, мы

78
00:02:56,959 --> 00:02:58,560
использовали эти архитектуры, чтобы делать

79
00:02:58,560 --> 00:03:00,879
всевозможные интересные вещи, но одна

80
00:03:00,879 --> 00:03:02,159
вещь, о которой мы говорили, мы говорили,

81
00:03:02,159 --> 00:03:04,239
это информационное узкое место, которое

82
00:03:04,239 --> 00:03:06,239
вы пытаетесь кодировать, возможно, очень

83
00:03:06,239 --> 00:03:08,239
длинную последовательность и вид самого последнего

84
00:03:08,239 --> 00:03:10,319
вектора  в вашем или в одном векторе в вашем

85
00:03:10,319 --> 00:03:12,800
кодировщике, и поэтому мы используем натяжение в качестве

86
00:03:12,800 --> 00:03:15,519
этого механизма, чтобы вы

87
00:03:15,519 --> 00:03:17,840
узнали представление из нашего декодера и

88
00:03:20,239 --> 00:03:22,480
как бы оглянулись назад и обработали закодированные представления как память

89
00:03:22,480 --> 00:03:24,480
Мы можем сослаться на то,

90
00:03:24,480 --> 00:03:26,080
что важно в любой момент времени, и как бы выбрать то, что важно,

91
00:03:27,040 --> 00:03:30,799
и это было внимание, и на этой неделе

92
00:03:30,799 --> 00:03:32,640
мы собираемся сделать что-то немного

93
00:03:32,640 --> 00:03:34,239
другое,

94
00:03:34,239 --> 00:03:36,000
поэтому мы узнали о моделях последовательности для

95
00:03:36,000 --> 00:03:38,400
последовательностей, способах декодирования кодировщика.

96
00:03:38,400 --> 00:03:41,200
думать о проблемах

97
00:03:41,200 --> 00:03:42,799
более или менее, чтобы справиться с этой

98
00:03:42,799 --> 00:03:44,879
идеей о том, что вы знаете, как построить

99
00:03:44,879 --> 00:03:46,400
систему машинного перевода, которая полностью

100
00:03:46,400 --> 00:03:47,840
дифференцируема,

101
00:03:47,840 --> 00:03:50,319
и так что это действительно

102
00:03:50,319 --> 00:03:51,519
интересный способ думать о

103
00:03:51,519 --> 00:03:53,920
проблемах, что мы будем делать на этой неделе

104
00:03:53,920 --> 00:03:55,519
отличается, мы не пытаемся

105
00:03:55,519 --> 00:03:58,159
мотивировать совершенно новый способ мышления

106
00:03:58,159 --> 00:03:59,360
о

107
00:03:59,360 --> 00:04:01,360
проблемах, таких как машинный перевод,

108
00:04:01,360 --> 00:04:03,120
вместо этого мы собираемся взять строительные

109
00:04:03,120 --> 00:04:05,360
блоки, которые мы использовали, ну, вы знаете,

110
00:04:05,360 --> 00:04:07,120
повторяющиеся нейронные сети, и мы

111
00:04:07,120 --> 00:04:09,680
собираемся потратить  много проб и ошибок

112
00:04:09,680 --> 00:04:11,439
в полевых условиях, пытаясь выяснить,

113
00:04:11,439 --> 00:04:13,680
есть ли строительные блоки, которые

114
00:04:13,680 --> 00:04:16,798
лучше работают с широким кругом проблем,

115
00:04:16,798 --> 00:04:19,199
мы собираемся

116
00:04:19,199 --> 00:04:21,358
вставить новую вещь для повторяющейся нейры  l

117
00:04:21,358 --> 00:04:23,680
и говорят, что вы знаете, вуаля, может быть, это

118
00:04:23,680 --> 00:04:25,520
работает лучше,

119
00:04:25,520 --> 00:04:26,880
и поэтому

120
00:04:26,880 --> 00:04:28,720
я хочу взять нас в такого рода

121
00:04:28,720 --> 00:04:30,560
путешествие

122
00:04:30,560 --> 00:04:31,520
к

123
00:04:31,520 --> 00:04:33,120
сетям самовнимания, и мы начнем

124
00:04:33,120 --> 00:04:34,639
с некоторых проблем с повторяющимися нейронными

125
00:04:34,639 --> 00:04:36,560
сетями, поэтому мы потратили немного времени,

126
00:04:36,560 --> 00:04:39,440
пытаясь убедить  вы,

127
00:04:39,680 --> 00:04:42,080
что наши текущие нейронные сети были

128
00:04:42,080 --> 00:04:43,600
очень полезны,

129
00:04:43,600 --> 00:04:45,280
теперь я собираюсь поговорить о причинах, по которым

130
00:04:45,280 --> 00:04:47,600
они могут быть улучшены,

131
00:04:47,600 --> 00:04:48,880
поэтому

132
00:04:48,880 --> 00:04:50,479
мы знаем, что рекуррентные нейронные сети

133
00:04:50,479 --> 00:04:52,800
регистрируются слева направо,

134
00:04:52,800 --> 00:04:54,400
цитата в кавычках, это может быть справа

135
00:04:54,400 --> 00:04:56,720
налево,

136
00:04:56,720 --> 00:04:58,320
так что что  Означает ли это, что

137
00:04:58,320 --> 00:05:00,240
наша текущая нейронная сеть

138
00:05:02,880 --> 00:05:06,000
правильно кодирует линейную локальность, поэтому, когда я смотрю на вкусное

139
00:05:06,000 --> 00:05:08,320
в этой фразе, я собираюсь посмотреть

140
00:05:08,320 --> 00:05:09,680
на пиццу, или если я иду в другом

141
00:05:09,680 --> 00:05:11,440
направлении, когда я смотрю на пиццу, я

142
00:05:11,440 --> 00:05:13,759
на вкус, и поэтому

143
00:05:13,759 --> 00:05:15,600
их значения, присутствующие

144
00:05:15,600 --> 00:05:17,440
в предложении, очень легко повлиять на значение, чтобы

145
00:05:17,440 --> 00:05:18,880
повлиять на представление другого

146
00:05:18,880 --> 00:05:20,479
слова um,

147
00:05:20,479 --> 00:05:22,160
и это на самом деле очень полезно,

148
00:05:22,160 --> 00:05:23,759
потому что соседние слова часто действительно

149
00:05:23,759 --> 00:05:25,440
влияют на каждое другое слово  r, это практически

150
00:05:25,440 --> 00:05:27,280
одна из вещей, о которых мы говорили с

151
00:05:27,280 --> 00:05:29,840
гипотезой распределения, которая

152
00:05:29,840 --> 00:05:32,320
закодирована чем-то вроде слова к спине,

153
00:05:32,320 --> 00:05:35,120
но если слова линейно удалены

154
00:05:36,400 --> 00:05:37,759
друг от друга, они все еще могут взаимодействовать друг с другом, это

155
00:05:37,759 --> 00:05:39,600
то, что мы видели при синтаксическом анализе зависимостей,

156
00:05:40,960 --> 00:05:43,840
поэтому, если я сказал  фраза шеф-повар

157
00:05:43,840 --> 00:05:46,000
замечает шеф-повар здесь жирным шрифтом я запускаю

158
00:05:46,000 --> 00:05:48,240
повторяющуюся нейронную сеть по этому,

159
00:05:48,240 --> 00:05:50,000
а затем вы знаете шеф-повара, который, а затем у

160
00:05:50,000 --> 00:05:52,240
меня есть эта длинная последовательность,

161
00:05:52,240 --> 00:05:55,919
которую я собираюсь закодировать,

162
00:05:55,919 --> 00:05:58,000
и тогда слово было

163
00:05:58,000 --> 00:06:00,000
правильным  может быть, это был шеф-повар,

164
00:06:00,000 --> 00:06:02,960
но между ними у меня есть

165
00:06:02,960 --> 00:06:04,880
длина последовательности, много шагов

166
00:06:04,880 --> 00:06:09,120
вычислений, которые мне нужно выполнить, прежде чем

167
00:06:09,120 --> 00:06:10,639
шеф-повар и waz

168
00:06:10,639 --> 00:06:12,240
смогут правильно взаимодействовать,

169
00:06:12,240 --> 00:06:14,160
и поэтому в середине

170
00:06:14,160 --> 00:06:16,400
все может пойти не так, как

171
00:06:16,400 --> 00:06:20,639
вы знаете, может быть трудно  изучайте

172
00:06:20,639 --> 00:06:22,319
вещи с помощью um,

173
00:06:22,319 --> 00:06:23,440
где они должны взаимодействовать, так что, в

174
00:06:23,440 --> 00:06:25,919
частности, может быть трудно изучить

175
00:06:25,919 --> 00:06:27,520
зависимости на больших расстояниях, потому что у нас

176
00:06:27,520 --> 00:06:29,919
есть проблемы с градиентом, мы видели, что lstms

177
00:06:29,919 --> 00:06:31,680
распространяет градиенты лучше, чем простые

178
00:06:31,680 --> 00:06:34,160
rnns, но  не идеально,

179
00:06:34,160 --> 00:06:36,319
и поэтому, если chef и waz очень далеки,

180
00:06:36,319 --> 00:06:37,919
становится трудно понять, что они должны

181
00:06:37,919 --> 00:06:40,800
взаимодействовать, а линейный порядок

182
00:06:40,800 --> 00:06:42,240
слов вроде бы встроен в модель, так что

183
00:06:42,240 --> 00:06:44,880
вам нужно развернуть rnn на

184
00:06:44,880 --> 00:06:46,400
протяжении всей последовательности,

185
00:06:46,400 --> 00:06:48,000
и это не совсем правильный путь

186
00:06:48,000 --> 00:06:49,360
думать о

187
00:06:49,360 --> 00:06:51,840
предложениях обязательно гм, по крайней мере, вы

188
00:06:51,840 --> 00:06:54,400
знаете, что линейный порядок - это не совсем то,

189
00:06:54,400 --> 00:06:58,000
насколько предложения структурированы,

190
00:06:58,880 --> 00:07:01,599
и вы знаете, что здесь у вас есть повар,

191
00:07:01,599 --> 00:07:03,120
а затем у вас есть все такого рода

192
00:07:03,120 --> 00:07:04,639
вычисления в середине, все эти

193
00:07:04,639 --> 00:07:05,919
применения рекуррентной

194
00:07:05,919 --> 00:07:08,639
матрицы весов до того, как вы позволите ей

195
00:07:08,639 --> 00:07:10,639
взаимодействовать, было, и снова вид

196
00:07:10,639 --> 00:07:14,000
зависимости - это длина последовательности,

197
00:07:14,000 --> 00:07:16,720
а затем вы поняли, что все в порядке,

198
00:07:16,720 --> 00:07:19,360
вторая проблема очень связана

199
00:07:19,360 --> 00:07:21,599
с отсутствием возможности распараллеливания,

200
00:07:21,599 --> 00:07:24,479
так что это происходит  Чтобы быть огромным рефреном

201
00:07:24,479 --> 00:07:25,440
сейчас, когда мы получили

202
00:07:25,440 --> 00:07:26,960
лекции по трансформерам, это возможность

203
00:07:26,960 --> 00:07:30,639
распараллеливания, это то, что вы получаете

204
00:07:30,639 --> 00:07:32,080
от своего графического процессора, и это то, что вы хотите

205
00:07:32,080 --> 00:07:34,160
использовать, так что

206
00:07:34,160 --> 00:07:36,319
когда вы запускаете rnn, у

207
00:07:36,319 --> 00:07:38,240
вас нет

208
00:07:38,240 --> 00:07:39,680
длина последовательности много

209
00:07:39,680 --> 00:07:41,840
непараллелизируемых

210
00:07:41,840 --> 00:07:43,120
операций,

211
00:07:43,120 --> 00:07:45,440
и поэтому, пока у вас есть графический процессор, они могут как

212
00:07:45,440 --> 00:07:46,720
бы фрагментировать сразу несколько

213
00:07:46,720 --> 00:07:49,360
независимых операций,

214
00:07:49,360 --> 00:07:51,520
вы не можете сортировать их все

215
00:07:51,520 --> 00:07:53,599
сразу, потому что у вас есть эта явная

216
00:07:53,599 --> 00:07:56,720
зависимость от времени в их текущих уравнениях,

217
00:07:56,720 --> 00:07:59,280
в частности  вы знаете, что будущее

218
00:07:59,280 --> 00:08:01,680
состояние rnn по строке не может быть вычислено,

219
00:08:01,680 --> 00:08:03,919
пока вы не вычислите то, что было раньше,

220
00:08:03,919 --> 00:08:05,599
и это препятствует обучению на очень

221
00:08:05,599 --> 00:08:07,840
больших наборах данных, поэтому давайте посмотрим на

222
00:08:07,840 --> 00:08:10,800
это при прокрутке rnn,

223
00:08:10,800 --> 00:08:13,120
если это, скажем, первый уровень

224
00:08:13,120 --> 00:08:15,199
слой rnn или lstm, возможно, он не

225
00:08:15,199 --> 00:08:16,400
зависит

226
00:08:16,400 --> 00:08:17,680
эффективно от чего-либо, вы можете просто

227
00:08:17,680 --> 00:08:19,840
вычислить его немедленно,

228
00:08:19,840 --> 00:08:22,000
а затем второй уровень, так что это, как

229
00:08:22,000 --> 00:08:24,319
вы знаете, сложенный набор из двух lstms,

230
00:08:24,319 --> 00:08:26,479
второй уровень зависит от первого слоя

231
00:08:26,479 --> 00:08:27,759
здесь

232
00:08:28,639 --> 00:08:31,680
во времени  измерение, хотя

233
00:08:31,680 --> 00:08:33,679
эта,

234
00:08:33,679 --> 00:08:34,479
эта

235
00:08:34,479 --> 00:08:36,479
ячейка здесь зависит от этого, так что у вас

236
00:08:36,479 --> 00:08:38,719
есть одна, а затем вы знаете, что это зависит от

237
00:08:38,719 --> 00:08:40,640
этого, так что у вас есть одна, так что вы знаете,

238
00:08:40,640 --> 00:08:42,958
извините, по крайней мере, две вещи, которые

239
00:08:42,958 --> 00:08:45,279
вам нужно вычислить  То есть здесь, прежде чем вы сможете

240
00:08:45,279 --> 00:08:47,760
вычислить значение этой ячейки,

241
00:08:47,760 --> 00:08:49,839
также три здесь, и с

242
00:08:49,839 --> 00:08:52,399
длиной последовательности она растет с o

243
00:08:52,399 --> 00:08:54,720
длины последовательности, поэтому здесь я не

244
00:08:54,720 --> 00:08:57,920
смог даже попытаться вычислить это значение,

245
00:08:57,920 --> 00:09:01,519
когда я приехал сюда, потому что мне пришлось

246
00:09:01,519 --> 00:09:03,200
сначала сделай все это, так что я не могу

247
00:09:03,200 --> 00:09:05,680
распараллелить по временному измерению,

248
00:09:05,680 --> 00:09:07,360
и это препятствует обучению на очень больших

249
00:09:07,360 --> 00:09:09,519
наборах данных,

250
00:09:09,519 --> 00:09:11,839
ну

251
00:09:12,720 --> 00:09:16,880
ладно, а затем я думаю, что Крис или Та

252
00:09:16,880 --> 00:09:18,640
могут остановить меня вопросом,

253
00:09:18,640 --> 00:09:20,080
если мне кажется, что это  правильно, что

254
00:09:20,080 --> 00:09:22,000
делать,

255
00:09:22,959 --> 00:09:24,320
хорошо, так что,

256
00:09:24,320 --> 00:09:25,519
и вы можете увидеть, как это связанная

257
00:09:25,519 --> 00:09:27,040
проблема, правильно, это действительно напрямую

258
00:09:27,040 --> 00:09:29,279
связано с повторением модели,

259
00:09:29,279 --> 00:09:30,720
а то, что мы думали, было действительно

260
00:09:30,720 --> 00:09:32,880
полезно сейчас,

261
00:09:32,880 --> 00:09:35,600
проблематично

262
00:09:37,920 --> 00:09:40,240
скажем с этим,

263
00:09:40,240 --> 00:09:42,480
мы, похоже, хотим заменить повторение

264
00:09:42,480 --> 00:09:44,720
как сам строительный блок, поэтому давайте рассмотрим

265
00:09:44,720 --> 00:09:46,720
некоторые альтернативы, и мы видели

266
00:09:46,720 --> 00:09:48,320
каждую из этих альтернатив в классе,

267
00:09:48,320 --> 00:09:51,279
пока что мы начнем с окна слова,

268
00:09:51,279 --> 00:09:53,519
типа строительных блоков для o  ur nlp

269
00:09:53,519 --> 00:09:55,040
модели, если мы хотим заменить наши

270
00:09:55,040 --> 00:09:57,519
кодировщики и наши декодеры чем-то,

271
00:09:57,519 --> 00:10:00,480
что соответствует той же

272
00:10:00,480 --> 00:10:03,519
цели, но имеет разные свойства,

273
00:10:03,519 --> 00:10:05,519
поэтому модель окна слов будет агрегировать

274
00:10:05,519 --> 00:10:07,760
локальный контекст, как мы видели это с нашими

275
00:10:07,760 --> 00:10:09,519
классификаторами окна слов, которые

276
00:10:09,519 --> 00:10:10,959
мы  уже построенный,

277
00:10:10,959 --> 00:10:13,040
вы берете локальный контекст слов, которые вы

278
00:10:13,040 --> 00:10:14,959
используете

279
00:10:14,959 --> 00:10:16,560
для представления информации о

280
00:10:16,560 --> 00:10:18,480
центральном слове, это также известно как

281
00:10:18,480 --> 00:10:20,720
одномерная свертка.

282
00:10:22,640 --> 00:10:24,720
word

283
00:10:24,720 --> 00:10:27,440
window uh контексты,

284
00:10:27,440 --> 00:10:29,440
поэтому количество непараллелизируемых

285
00:10:29,440 --> 00:10:31,839
операций с этими

286
00:10:31,839 --> 00:10:33,440
строительными блоками word window не растет с

287
00:10:33,440 --> 00:10:35,600
длиной последовательности, и вот что-то вроде

288
00:10:35,600 --> 00:10:37,920
картины того, что у вас есть слой встраивания,

289
00:10:37,920 --> 00:10:40,079
скажем так, что вы можете вставлять каждое слово

290
00:10:40,079 --> 00:10:41,680
независимо,

291
00:10:41,680 --> 00:10:43,040
так что вы не должны  Мне нужно знать

292
00:10:43,040 --> 00:10:44,480
другие слова, окружающие его, чтобы

293
00:10:44,480 --> 00:10:47,440
выбрать правильный размер вложения,

294
00:10:47,440 --> 00:10:49,440
и поэтому все они имеют своего рода нулевую

295
00:10:49,440 --> 00:10:51,519
зависимость в подобном

296
00:10:51,519 --> 00:10:53,360
волнистом представлении о том, как m  uch uh

297
00:10:53,360 --> 00:10:55,360
paralyzability есть,

298
00:10:55,360 --> 00:10:57,440
теперь вы можете пройти классификатор окна слова

299
00:10:57,440 --> 00:11:01,040
поверх каждого, чтобы построить

300
00:11:01,040 --> 00:11:03,120
представление слова, которое

301
00:11:03,120 --> 00:11:05,680
учитывает его локальный контекст,

302
00:11:05,680 --> 00:11:08,959
но для того, чтобы применить его к этому слову h1,

303
00:11:08,959 --> 00:11:11,600
мне не нужно знать  что-нибудь, извините, мне

304
00:11:11,600 --> 00:11:13,519
не нужно применять его к h1,

305
00:11:13,519 --> 00:11:15,760
чтобы применить его к h2 аналогично,

306
00:11:15,760 --> 00:11:17,200
чтобы применить контекстуализатор окна слова

307
00:11:17,200 --> 00:11:19,519
к ht,

308
00:11:19,519 --> 00:11:20,880
я могу просто смотреть на его локальное окно

309
00:11:20,880 --> 00:11:22,079
независимо,

310
00:11:22,079 --> 00:11:23,519
и поэтому снова

311
00:11:23,519 --> 00:11:25,839
ни один из них не имеет зависимости в  время,

312
00:11:25,839 --> 00:11:28,880
и я могу продолжать складывать слои, как это

313
00:11:28,880 --> 00:11:30,320
правильно, так что это может выглядеть

314
00:11:30,320 --> 00:11:32,880
как кодировщик справа кодировщик, как

315
00:11:32,880 --> 00:11:35,440
наши кодировщики lstm ммм, если бы я не позволил

316
00:11:35,440 --> 00:11:37,360
вам смотреть в будущее, просто

317
00:11:37,360 --> 00:11:38,959
отрезав окно, оно могло бы выглядеть

318
00:11:38,959 --> 00:11:41,279
как декодер для  языковые модели, и

319
00:11:41,279 --> 00:11:43,519
это хорошо,

320
00:11:43,519 --> 00:11:46,160
и мы получаем это красиво, знаете ли,

321
00:11:46,160 --> 00:11:48,399
о одной зависимости во времени, верно, никакой

322
00:11:48,399 --> 00:11:49,839
зависимости во временном измерении,

323
00:11:49,839 --> 00:11:51,519
это улучшение, но есть некоторые

324
00:11:51,519 --> 00:11:53,040
проблемы, так что насчет зависимостей на большом расстоянии,

325
00:11:53,040 --> 00:11:55,519
правильно это  вот почему

326
00:11:55,519 --> 00:11:57,040
мы сказали, что хотим использовать рекуррентные

327
00:11:57,040 --> 00:11:58,399
нейронные сети, потому что

328
00:11:58,399 --> 00:12:00,240
они лучше справятся с кодированием зависимостей на большом расстоянии,

329
00:12:02,399 --> 00:12:04,320
это проблема так же, как и

330
00:12:04,320 --> 00:12:06,560
раньше, но, складывая

331
00:12:06,560 --> 00:12:09,440
слои окна слов, мы можем получить более широкие и длинные

332
00:12:09,440 --> 00:12:10,800
контексты,

333
00:12:10,800 --> 00:12:13,760
поэтому, если у вас есть  какой-то размер окна,

334
00:12:13,760 --> 00:12:16,399
а затем вы складываете два слоя,

335
00:12:16,399 --> 00:12:19,600
так что красный указывает здесь интерактивное состояние вроде

336
00:12:19,600 --> 00:12:21,839
того, как вы можете посмотреть, как далеко вы можете

337
00:12:21,839 --> 00:12:24,240
смотреть, чтобы правильно кодировать hk,

338
00:12:24,240 --> 00:12:27,519
поэтому в слое встраивания у

339
00:12:27,519 --> 00:12:30,240
вас есть такие слова здесь, поэтому  так

340
00:12:30,240 --> 00:12:32,560
что это последний слой этот верхний слой

341
00:12:32,560 --> 00:12:34,480
классификатора окна слова здесь

342
00:12:34,480 --> 00:12:36,320
встраивание hk на выходе вашего

343
00:12:36,320 --> 00:12:38,000
кодировщика,

344
00:12:38,000 --> 00:12:39,839
и поэтому он смотрит на то, что вы знаете

345
00:12:39,839 --> 00:12:41,600
пять локальных слов, потому что это размер окна,

346
00:12:41,600 --> 00:12:44,399
а также вы знаете

347
00:12:44,399 --> 00:12:46,720
самое дальнее слово  здесь также

348
00:12:46,720 --> 00:12:48,639
сразу посмотрел на пару слов,

349
00:12:48,639 --> 00:12:49,920
поэтому, если вы сложите их, сложите

350
00:12:49,920 --> 00:12:51,279
их и сложите их,

351
00:12:51,279 --> 00:12:53,279
не увеличивая размер окна вообще

352
00:12:53,279 --> 00:12:55,120
на любом заданном слое,

353
00:12:55,120 --> 00:12:56,560
вы можете смотреть довольно далеко и на самом деле

354
00:12:56,560 --> 00:12:57,920
там  это уловки, которые вы можете использовать, чтобы смотреть

355
00:12:57,920 --> 00:12:59,839
еще дальше, но у вас все еще есть такая,

356
00:12:59,839 --> 00:13:02,560
по крайней мере в принципе, проблема,

357
00:13:02,560 --> 00:13:05,120
когда у вас есть такое слово, как это h1,

358
00:13:05,120 --> 00:13:07,440
и вы можете видеть его синим цветом,

359
00:13:07,440 --> 00:13:09,600
и вы знаете, с этими двумя

360
00:13:09,600 --> 00:13:10,800
слоями сети

361
00:13:10,800 --> 00:13:13,040
i  вообще ничего не знаю о h1,

362
00:13:13,040 --> 00:13:14,639
когда я создаю

363
00:13:14,639 --> 00:13:16,880
здесь представление hk,

364
00:13:16,880 --> 00:13:18,240
я мог бы решить это, добавив еще один

365
00:13:18,240 --> 00:13:20,480
глубинный слой, но вы знаете, что в принципе у

366
00:13:20,480 --> 00:13:22,959
вас всегда есть какое-то конечное поле,

367
00:13:24,399 --> 00:13:25,600
так что это

368
00:13:25,600 --> 00:13:27,920
вы знаете, на самом деле довольно  полезные э-э,

369
00:13:27,920 --> 00:13:29,600
вы знаете, э-э, слово оконные разновидности

370
00:13:29,600 --> 00:13:31,200
контекстуализаторов, и мы узнаем

371
00:13:31,200 --> 00:13:33,200
о них больше позже э-э, и есть вроде

372
00:13:33,200 --> 00:13:34,880
как много усилий, о которых я говорил

373
00:13:34,880 --> 00:13:36,079
в начале урока, на

374
00:13:36,079 --> 00:13:38,160
самом деле как бы частично решало, какое

375
00:13:38,160 --> 00:13:39,839
из когда слово

376
00:13:39,839 --> 00:13:41,279
сверточный материал окна называется вещью или

377
00:13:41,279 --> 00:13:43,360
вниманием, и внимание

378
00:13:43,360 --> 00:13:46,480
на данный момент выиграло эм, и да, а как

379
00:13:46,480 --> 00:13:49,120
насчет внимания, так почему оно может быть

380
00:13:49,120 --> 00:13:51,199
полезным в качестве фундаментального

381
00:13:51,199 --> 00:13:52,959
строительного блока вместо своего рода сахара

382
00:13:52,959 --> 00:13:55,120
на  top of our lstms

383
00:13:55,120 --> 00:13:56,880
um, так что просто чтобы

384
00:13:56,880 --> 00:13:58,800
вспомнить некоторые интуиции

385
00:13:58,800 --> 00:13:59,760
внимания,

386
00:13:59,760 --> 00:14:02,720
он обрабатывает представление слова как

387
00:14:02,720 --> 00:14:03,760
запрос,

388
00:14:03,760 --> 00:14:06,160
ищет где-то

389
00:14:06,160 --> 00:14:07,920
и пытается отсортировать информацию доступа

390
00:14:07,920 --> 00:14:10,079
из набора значений так, чтобы у нас было

391
00:14:10,079 --> 00:14:12,160
представление слова в нашем декодере в

392
00:14:12,160 --> 00:14:14,160
наши системы машинного перевода

393
00:14:14,160 --> 00:14:15,839
набор значений, в которых все кодировщики

394
00:14:15,839 --> 00:14:19,760
указывают исходное предложение,

395
00:14:19,760 --> 00:14:20,639
и

396
00:14:20,639 --> 00:14:22,079
сегодня мы будем думать о внимании, а не о

397
00:14:22,079 --> 00:14:23,920
внимании от декодера к

398
00:14:23,920 --> 00:14:26,000
кодировщику, мы будем думать о внимании

399
00:14:26,000 --> 00:14:28,720
в рамках одного предложения,

400
00:14:28,720 --> 00:14:31,279
так что просто очень быстрое изображение  у

401
00:14:31,279 --> 00:14:32,880
вас снова есть слой встраивания,

402
00:14:32,880 --> 00:14:34,639
я помещаю сюда счетчики вычислительных зависимостей,

403
00:14:34,639 --> 00:14:36,720
чтобы все это можно

404
00:14:36,720 --> 00:14:38,800
было делать параллельно для

405
00:14:38,800 --> 00:14:40,000
слоя встраивания снова,

406
00:14:40,000 --> 00:14:42,079
и теперь вы делаете напряжение правильно,

407
00:14:42,079 --> 00:14:43,600
так что вы  я как бы смотрю на

408
00:14:43,600 --> 00:14:46,160
каждое слово в слое встраивания, чтобы

409
00:14:46,160 --> 00:14:47,839
обратить внимание на это слово,

410
00:14:47,839 --> 00:14:50,480
и я испускаю кучу стрелок,

411
00:14:50,480 --> 00:14:52,720
так что это все стрелки, все слова

412
00:14:52,720 --> 00:14:54,240
взаимодействуют со всеми словами, и мы

413
00:14:54,240 --> 00:14:55,920
углубимся в это  сегодня я обещаю, но я

414
00:14:55,920 --> 00:14:57,519
просто хотел сделать этот график

415
00:14:57,519 --> 00:14:58,560
немного

416
00:14:58,560 --> 00:15:00,959
менее плотным, а затем,

417
00:15:00,959 --> 00:15:03,760
чтобы во втором слое снова все

418
00:15:03,760 --> 00:15:05,920
пары слов взаимодействовали, и все

419
00:15:05,920 --> 00:15:08,000
это можно распараллелить, поэтому вы не можете

420
00:15:08,000 --> 00:15:10,320
распараллелить глубоко правильно  потому что вам нужно

421
00:15:10,320 --> 00:15:12,800
закодировать этот слой, прежде чем вы сможете сделать этот

422
00:15:12,800 --> 00:15:15,440
слой, но со временем он становится распараллеливаемым,

423
00:15:15,440 --> 00:15:18,240
поэтому он ставит этот флажок,

424
00:15:18,240 --> 00:15:21,279
поэтому снова у нас есть один вид

425
00:15:21,279 --> 00:15:22,959
вычислительной

426
00:15:22,959 --> 00:15:24,320
зависимости,

427
00:15:24,320 --> 00:15:26,000
вы знаете количество непараллелизируемых

428
00:15:26,000 --> 00:15:27,600
операций как функцию длины последовательности

429
00:15:27,600 --> 00:15:31,680
и ну, как  дополнительное преимущество право,

430
00:15:31,680 --> 00:15:33,519
расстояние взаимодействия между

431
00:15:33,519 --> 00:15:34,880
словами

432
00:15:34,880 --> 00:15:36,639
также равно единице,

433
00:15:36,639 --> 00:15:38,240
так что раньше у нас были повторяющиеся

434
00:15:38,240 --> 00:15:40,800
сети, где, если вы находитесь далеко,

435
00:15:40,800 --> 00:15:42,959
это последнее слово в предложении, у

436
00:15:42,959 --> 00:15:45,120
вас может быть o из t

437
00:15:45,120 --> 00:15:47,120
операций между вами и далеким

438
00:15:47,120 --> 00:15:49,519
словом  с вниманием вы

439
00:15:49,519 --> 00:15:51,680
немедленно взаимодействуете, это самый первый уровень, на котором вы

440
00:15:51,680 --> 00:15:53,519
можете увидеть свое далекое слово, и так

441
00:15:53,519 --> 00:15:56,800
что это ноль из одного, и это в конечном итоге

442
00:15:56,800 --> 00:15:59,440
кажется захватывающе мощным, и

443
00:15:59,440 --> 00:16:01,600
мы войдем во многие  подробности

444
00:16:01,600 --> 00:16:04,000
сегодня в

445
00:16:04,000 --> 00:16:05,279
порядке,

446
00:16:05,279 --> 00:16:08,160
вот почему внимание

447
00:16:08,160 --> 00:16:10,079
решает две проблемы, которые мы

448
00:16:10,079 --> 00:16:11,839
подняли с помощью повторяющихся нейронных сетей, но

449
00:16:11,839 --> 00:16:14,560
с нашими эмпирическими шляпами на

450
00:16:14,560 --> 00:16:16,320
нем еще не должно быть доказательством того, что вы знаете, что

451
00:16:16,320 --> 00:16:17,600
это должен быть хороший строительный блок, и

452
00:16:17,600 --> 00:16:19,440
на самом деле это  нужно немного

453
00:16:19,440 --> 00:16:21,040
подумать, чтобы подумать о том, как превратить

454
00:16:21,040 --> 00:16:22,639
внимание в строительный блок, как у

455
00:16:22,639 --> 00:16:24,800
rnn,

456
00:16:24,800 --> 00:16:26,560
так что давайте начнем с

457
00:16:26,560 --> 00:16:29,199
копания прямо в уравнения

458
00:16:29,199 --> 00:16:30,800
для самовнимания, которое снова является

459
00:16:30,800 --> 00:16:32,720
вниманием к тому, куда

460
00:16:32,720 --> 00:16:33,920
все смотрит внутри себя,

461
00:16:33,920 --> 00:16:36,720
мы формализуем  это для вас, поэтому сегодня мы

462
00:16:36,720 --> 00:16:38,240
будем говорить всю лекцию

463
00:16:38,240 --> 00:16:42,000
о ключах и значениях

464
00:16:42,800 --> 00:16:44,800
запросов, наши запросы будут набором из t

465
00:16:44,800 --> 00:16:47,040
запросов, каждый запрос - это вектор в

466
00:16:47,040 --> 00:16:48,800
измерении d,

467
00:16:48,800 --> 00:16:50,959
вы можете просто думать о них как о тех

468
00:16:50,959 --> 00:16:52,480
векторах прямо сейчас

469
00:16:52,480 --> 00:16:53,839
не беспокоясь о том, откуда

470
00:16:53,839 --> 00:16:55,120
они

471
00:16:55,120 --> 00:16:58,560
взялись, у нас есть набор ключей k1, kt,

472
00:16:58,560 --> 00:17:01,680
опять же, каждый вектор k имеет размерность

473
00:17:01,680 --> 00:17:02,560
d,

474
00:17:02,560 --> 00:17:04,959
и у нас есть некоторые значения, каждое

475
00:17:04,959 --> 00:17:06,160
значение будет

476
00:17:06,160 --> 00:17:09,280
также в размерности d an  d на данный момент

477
00:17:09,280 --> 00:17:11,280
мы собираемся предположить, что у нас

478
00:17:11,280 --> 00:17:13,119
их всех одинаковое количество,

479
00:17:13,119 --> 00:17:15,599
что не обязательно произойдет позже, поэтому

480
00:17:15,599 --> 00:17:17,119
при самовнимании

481
00:17:17,119 --> 00:17:19,359
при самовнимании ключи, запросы и

482
00:17:19,359 --> 00:17:20,959
значения поступают из одного и того же источника

483
00:17:20,959 --> 00:17:22,240
информации,

484
00:17:22,240 --> 00:17:24,480
одно и то же предложение  например,

485
00:17:24,480 --> 00:17:27,199
да, на практике, когда все они

486
00:17:27,199 --> 00:17:28,799
происходят из одного и того же предложения,

487
00:17:28,799 --> 00:17:30,160
их всех будет одинаковое количество,

488
00:17:30,160 --> 00:17:32,720
это все будет на

489
00:17:32,720 --> 00:17:35,760
практике, у вас могут быть разные числа,

490
00:17:35,760 --> 00:17:37,919
так откуда они берутся мы '  Я

491
00:17:37,919 --> 00:17:39,919
подробно расскажу об этом позже,

492
00:17:39,919 --> 00:17:41,679
а пока подумайте о выходе

493
00:17:41,679 --> 00:17:44,559
предыдущего слоя, так что представьте, что на выходе

494
00:17:44,559 --> 00:17:46,240
вы знаете, что у вас есть как раз такой слой встраивания,

495
00:17:47,120 --> 00:17:48,320
и это вход для чего-то,

496
00:17:48,320 --> 00:17:50,240
что будет заниматься самовниманием, подумайте

497
00:17:50,240 --> 00:17:51,919
обо всем  из этих выходов

498
00:17:51,919 --> 00:17:55,440
вложений в виде некоторых векторов

499
00:17:55,440 --> 00:17:57,679
xi,

500
00:17:57,679 --> 00:18:00,080
и теперь мы можем просто сказать, что значение

501
00:18:00,080 --> 00:18:02,080
равно ключу q,

502
00:18:02,080 --> 00:18:04,640
равно как запрос равен этому

503
00:18:04,640 --> 00:18:05,919
xi, поэтому мы собираемся использовать одни и те же векторы

504
00:18:05,919 --> 00:18:08,240
для всех из них  но называть их

505
00:18:08,240 --> 00:18:10,640
Ключевые запросы и значения, которые я обещаю,

506
00:18:10,640 --> 00:18:12,320
будут очень полезны в том, как мы как бы думаем

507
00:18:12,320 --> 00:18:14,960
о том, что происходит, и как мы смотрим

508
00:18:14,960 --> 00:18:17,600
на уравнения, которые реализуют это,

509
00:18:17,600 --> 00:18:18,320
так что

510
00:18:19,679 --> 00:18:22,080
самовнимание в целом,

511
00:18:22,080 --> 00:18:24,080
но с этим точечным продуктом, так что развивайте

512
00:18:24,080 --> 00:18:25,520
собственное внимание продукта здесь просто  математическая

513
00:18:26,480 --> 00:18:29,919
математика заключается в том, что вы вычисляете сродства ключевых запросов,

514
00:18:29,919 --> 00:18:32,640
а бит скалярного произведения - это тот факт, что

515
00:18:32,640 --> 00:18:33,760
вы используете здесь функцию скалярного произведения,

516
00:18:34,720 --> 00:18:37,280
поэтому вы берете скалярное произведение для всех пар

517
00:18:37,280 --> 00:18:38,559
i и

518
00:18:38,559 --> 00:18:39,840
j qi

519
00:18:39,840 --> 00:18:41,840
с точками k,

520
00:18:41,840 --> 00:18:45,280
так что это на t  матрица заглавная t

521
00:18:45,280 --> 00:18:48,240
справа на t матрица аффинностей,

522
00:18:48,240 --> 00:18:50,160
которые являются скалярными значениями,

523
00:18:50,160 --> 00:18:53,120
не ограниченными по размеру,

524
00:18:53,120 --> 00:18:54,480
затем вы вычисляете веса внимания, которые

525
00:18:54,480 --> 00:18:56,480
мы также видели, используя функцию softmax, которую

526
00:18:56,480 --> 00:18:57,440
я только что написал

527
00:18:57,440 --> 00:18:59,360
здесь, функцию softmax,

528
00:18:59,360 --> 00:19:01,440
чтобы вы знали, что вы возводите в степень

529
00:19:01,440 --> 00:19:04,320
аффинность и  тогда вы суммируете

530
00:19:04,320 --> 00:19:05,840
в этом случае правильно, вы суммируете

531
00:19:05,840 --> 00:19:07,360
все

532
00:19:08,559 --> 00:19:10,400
ключи uh, поэтому у вас есть данный запрос,

533
00:19:10,400 --> 00:19:12,000
и вы суммируете все ключи

534
00:19:12,000 --> 00:19:13,840
для нормализации, так где

535
00:19:13,840 --> 00:19:15,679
этот запрос должен искать re  член, у вас

536
00:19:15,679 --> 00:19:18,160
есть t разных запросов, которые мы делаем

537
00:19:18,160 --> 00:19:21,840
для этого э-э, здесь um, и поэтому для данного

538
00:19:21,840 --> 00:19:23,760
запроса вы суммируете все ключи,

539
00:19:23,760 --> 00:19:25,919
чтобы нормализовать нормированную константу,

540
00:19:25,919 --> 00:19:29,520
которая дает вам распределение по um

541
00:19:29,520 --> 00:19:31,760
по длине последовательности t, поэтому  теперь у вас

542
00:19:31,760 --> 00:19:33,200
есть своего рода вес для всех индексов

543
00:19:33,200 --> 00:19:34,320
последовательности,

544
00:19:35,840 --> 00:19:38,720
и снова мы делаем наше средневзвешенное значение

545
00:19:38,720 --> 00:19:40,400
правильно, так что у нас есть наши

546
00:19:40,400 --> 00:19:42,559
веса для нашего среднего, а затем

547
00:19:42,559 --> 00:19:43,919
выход прямо там будет один

548
00:19:43,919 --> 00:19:46,080
выход для каждого запроса,

549
00:19:46,080 --> 00:19:48,480
выход - это веса для  который

550
00:19:48,480 --> 00:19:51,120
умножается на векторы значений

551
00:19:51,120 --> 00:19:53,760
справа um, так что снова, если вы установите ключи,

552
00:19:53,760 --> 00:19:56,080
все значения будут x,

553
00:19:56,080 --> 00:19:59,440
это имеет смысл, но приятно

554
00:19:59,440 --> 00:20:02,240
иметь реплики и k, чтобы знать,

555
00:20:02,240 --> 00:20:03,600
какая вещь делает то, что вы можете думать

556
00:20:03,600 --> 00:20:05,440
о  запрос как своего рода

557
00:20:05,440 --> 00:20:07,919
поиск информации где-то там, где ключ, который вы

558
00:20:07,919 --> 00:20:10,159
знаете, взаимодействует с запросом, а

559
00:20:10,159 --> 00:20:11,919
затем значение - это то, что вы знаете, что на

560
00:20:11,919 --> 00:20:13,520
самом деле собираетесь узнать вес

561
00:20:13,520 --> 00:20:16,000
в среднем и вывести

562
00:20:16,000 --> 00:20:18,880
Джону вопрос, который вам может понравиться  nswer,

563
00:20:18,880 --> 00:20:22,080
так что, если теперь мы подключаем все

564
00:20:22,080 --> 00:20:24,000
ко всему, чем это отличается от

565
00:20:24,000 --> 00:20:26,480
использования полностью подключенного слоя,

566
00:20:26,480 --> 00:20:28,400
это здорово, это отличный вопрос,

567
00:20:29,360 --> 00:20:32,240
пара причин, а первая заключается в том, что, в

568
00:20:32,240 --> 00:20:34,480
отличие от полностью подключенного слоя, вы можете

569
00:20:36,080 --> 00:20:37,919
узнать веса взаимодействия

570
00:20:37,919 --> 00:20:40,080
ну, веса взаимодействия являются

571
00:20:40,080 --> 00:20:43,679
динамическими в зависимости от того, какие фактические

572
00:20:43,679 --> 00:20:45,679
значения здесь верны, поэтому на полностью

573
00:20:45,679 --> 00:20:47,440
подключенном слое у вас есть эти веса,

574
00:20:47,440 --> 00:20:49,360
которые вы медленно изучаете в

575
00:20:49,360 --> 00:20:51,440
ходе обучения вашей сети,

576
00:20:51,440 --> 00:20:53,039
что позволяет вам сказать, какой вид

577
00:20:53,039 --> 00:20:55,120
скрытые блоки, на которые вы должны обратить

578
00:20:55,120 --> 00:20:57,600
внимание, это фактические взаимодействия

579
00:20:57,600 --> 00:21:00,159
между ключом и

580
00:21:00,159 --> 00:21:01,840
векторами запроса um, которые зависят от

581
00:21:01,840 --> 00:21:04,159
фактического содержимого, которое может

582
00:21:04,159 --> 00:21:07,120
изменяться во времени, и поэтому фактические сильные стороны

583
00:21:07,120 --> 00:21:08,720
всех взаимодействий всех видов

584
00:21:08,720 --> 00:21:10,559
весов внимания, которые,

585
00:21:10,559 --> 00:21:12,799
как вы понимаете, связаны

586
00:21:12,799 --> 00:21:14,159
с весами в полностью

587
00:21:14,159 --> 00:21:16,880
подключенном слое, могут изменяться

588
00:21:16,880 --> 00:21:19,360
в зависимости от входных данных.  оценка

589
00:21:19,360 --> 00:21:20,960
заключается в том, что параметризация сильно

590
00:21:20,960 --> 00:21:22,240
отличается, поэтому вы не изучаете

591
00:21:22,240 --> 00:21:23,919
независимый вес соединения для всех

592
00:21:23,919 --> 00:21:28,000
пар вещей, вместо этого вам разрешено

593
00:21:30,159 --> 00:21:32,799
параметризовать внимание, поскольку

594
00:21:32,799 --> 00:21:34,960
вы знаете такого рода функции точечного произведения

595
00:21:34,960 --> 00:21:36,400
между векторами, которые  являются

596
00:21:36,400 --> 00:21:38,480
представлениями, и вы в конечном итоге

597
00:21:38,480 --> 00:21:40,159
знаете,

598
00:21:40,159 --> 00:21:42,799
что параметры работают

599
00:21:42,799 --> 00:21:44,320
лучше, что мы увидим позже,

600
00:21:44,320 --> 00:21:45,120
мы еще не вдавались в то, как мы

601
00:21:45,120 --> 00:21:47,120
параметризуем эти функции, так

602
00:21:47,120 --> 00:21:49,039
что это два ответа, которые я бы сказал, один из

603
00:21:49,039 --> 00:21:50,240
них  у вас есть своего рода динамическое

604
00:21:50,240 --> 00:21:53,039
соединение, и во-вторых, вы знаете, что

605
00:21:53,039 --> 00:21:55,200
у вас его нет, просто у него есть индуктивное

606
00:21:55,200 --> 00:21:57,200
смещение, которое не просто связывает все

607
00:21:57,200 --> 00:22:00,400
со всем, прямая связь

608
00:22:02,320 --> 00:22:03,360
отлично,

609
00:22:03,360 --> 00:22:05,120
хорошо, я думаю, что это очень интересный

610
00:22:05,120 --> 00:22:06,799
вопрос,

611
00:22:06,799 --> 00:22:09,200
да, так что я рад, что вы спросили

612
00:22:09,200 --> 00:22:10,720
хорошо, так

613
00:22:10,720 --> 00:22:12,240
что мы поговорили о самовнимании сейчас,

614
00:22:12,240 --> 00:22:13,039
уравнениях, которые входят в

615
00:22:13,039 --> 00:22:15,679
самовнимание, но мы можем просто

616
00:22:15,679 --> 00:22:17,600
использовать это как строительный блок, я имею в виду, что вы

617
00:22:17,600 --> 00:22:19,440
знаете, возьмите все свои lstm, выбросьте их,

618
00:22:19,440 --> 00:22:20,799
используйте s  эльфийское внимание, которое мы

619
00:22:20,799 --> 00:22:22,960
только что определили, а почему не хорошо,

620
00:22:22,960 --> 00:22:26,799
вот пара причин, почему так смотри на

621
00:22:26,799 --> 00:22:28,559
самовнимание как на строительный блок, чтобы у нас

622
00:22:28,559 --> 00:22:30,960
было несколько слов в предложении повар,

623
00:22:30,960 --> 00:22:32,640
который

624
00:22:32,640 --> 00:22:34,799
кое-что длинное предложение

625
00:22:34,799 --> 00:22:36,720
еда - последнее слово  предложения

626
00:22:36,720 --> 00:22:38,559
хорошо,

627
00:22:38,559 --> 00:22:40,559
и вы знаете, что у них есть встраивание, и

628
00:22:40,559 --> 00:22:42,159
из этого вы получаете свой ключевой запрос и

629
00:22:42,159 --> 00:22:44,720
значение, о которых мы говорили до сих пор, что на

630
00:22:44,720 --> 00:22:46,880
самом деле тот же вектор, но вы знаете, что ключ

631
00:22:46,880 --> 00:22:48,640
значение запроса ключ значение запроса ключ создать

632
00:22:48,640 --> 00:22:49,520
значение

633
00:22:49,520 --> 00:22:51,919
мм, и вы знаете  мы могли бы складывать их, как

634
00:22:51,919 --> 00:22:53,919
слои lstm, чтобы у вас было значение keqree,

635
00:22:53,919 --> 00:22:56,000
выполняйте самовнимание по

636
00:22:56,000 --> 00:22:57,360
запросам и значениям ключей, как мы сказали,

637
00:22:57,360 --> 00:22:59,120
самовнимание - это функция для

638
00:22:59,120 --> 00:23:00,640
запросов и значений ключей, поэтому

639
00:23:00,640 --> 00:23:02,480
выполняйте самовнимание теперь, когда у вас есть эти

640
00:23:02,480 --> 00:23:04,720
запросы, получайте новые ключи  значений,

641
00:23:04,720 --> 00:23:06,960
а затем снова выполните

642
00:23:06,960 --> 00:23:09,280
самовнимание, вы знаете,

643
00:23:09,280 --> 00:23:12,400
это очень похоже на складывание lstms,

644
00:23:12,400 --> 00:23:13,760
но на самом деле у него есть несколько проблем в его

645
00:23:13,760 --> 00:23:15,120
нынешнем виде, поэтому нам нужно будет отправиться в

646
00:23:15,120 --> 00:23:17,280
путешествие, чтобы определить, чего не хватает в

647
00:23:17,280 --> 00:23:19,360
нашем  себя  -внимание, и первое,

648
00:23:19,360 --> 00:23:22,080
что самовнимание - это операция

649
00:23:22,080 --> 00:23:24,960
на множествах,

650
00:23:24,960 --> 00:23:26,559
хорошо, поэтому для уравнений, которые у нас были

651
00:23:26,559 --> 00:23:29,120
до этого, уравнение самовнимания

652
00:23:29,120 --> 00:23:31,520
никогда не ссылалось на

653
00:23:31,520 --> 00:23:33,280
индексы kq или v,

654
00:23:33,280 --> 00:23:35,120
кроме как для того, чтобы сказать, какие

655
00:23:35,120 --> 00:23:36,640
пары взаимодействуют  друг с другом он

656
00:23:36,640 --> 00:23:38,240
не знает, каков порядок вашего

657
00:23:38,240 --> 00:23:40,159
предложения, когда он вычисляет,

658
00:23:40,159 --> 00:23:42,400
хотя веса он не знает,

659
00:23:42,400 --> 00:23:44,480
и поэтому, если бы я ввел это предложение,

660
00:23:44,480 --> 00:23:46,080
повар, который готовил,

661
00:23:46,080 --> 00:23:48,240
это было бы так же, как если бы я просто

662
00:23:48,240 --> 00:23:50,400
поменял местами с  chef, а затем переключился

663
00:23:50,400 --> 00:23:52,960
на, и он просто не имел бы

664
00:23:52,960 --> 00:23:53,919
понятия,

665
00:23:53,919 --> 00:23:55,440
так что это уже не сработает,

666
00:23:55,440 --> 00:23:57,679
потому что порядок, в котором слова появляются

667
00:23:57,679 --> 00:24:01,039
в предложениях, имеет значение,

668
00:24:01,039 --> 00:24:02,320
так что вот первая проблема, с которой нам нужно

669
00:24:02,320 --> 00:24:03,919
работать, поэтому у меня будет

670
00:24:03,919 --> 00:24:06,080
список препятствий.

671
00:24:15,200 --> 00:24:18,159
знаю, какой приказ

672
00:24:19,440 --> 00:24:20,960
Если мы не собираемся изменять

673
00:24:20,960 --> 00:24:23,120
сами уравнения самовнимания, нам

674
00:24:23,120 --> 00:24:24,400
нужно

675
00:24:24,400 --> 00:24:27,600
закодировать порядок в ключах, запросах и

676
00:24:27,600 --> 00:24:28,960
значениях и позволить сети как

677
00:24:28,960 --> 00:24:32,559
бы разобраться в этом самостоятельно, так что  подумайте

678
00:24:32,559 --> 00:24:35,760
об этом, у нас есть t индексов последовательности,

679
00:24:35,760 --> 00:24:37,520
и мы собираемся связать t с некоторой

680
00:24:37,520 --> 00:24:39,120
конечной константой,

681
00:24:39,120 --> 00:24:40,400
так что t никогда не будет для нас больше,

682
00:24:40,400 --> 00:24:42,480
чем что-то,

683
00:24:42,480 --> 00:24:44,080
и мы называем это t,

684
00:24:44,080 --> 00:24:45,120
и теперь мы собираемся представить

685
00:24:45,120 --> 00:24:47,760
индекс последовательности как  вектор, поэтому pi

686
00:24:47,760 --> 00:24:49,679
будет вектором, представляющим

687
00:24:49,679 --> 00:24:51,600
индекс i, и он будет иметь

688
00:24:51,600 --> 00:24:53,120
размерность d, как наши ключевые

689
00:24:53,120 --> 00:24:54,799
запросы и значения,

690
00:24:54,799 --> 00:24:56,240
и поэтому у нас будет один из них

691
00:24:56,240 --> 00:24:58,480
для uh от 1 до t,

692
00:24:58,480 --> 00:25:00,240
поэтому не делайте этого  Пока не беспокойтесь о том, как

693
00:25:00,240 --> 00:25:02,080
выглядят числа Пи, как они устроены,

694
00:25:02,080 --> 00:25:04,000
мы сразу перейдем к этому, но подумайте

695
00:25:04,000 --> 00:25:06,080
об этом, легко включить эту

696
00:25:06,080 --> 00:25:08,400
информацию в наши строительные

697
00:25:08,400 --> 00:25:11,039
блоки внимания на первом уровне, если вы позволите

698
00:25:11,039 --> 00:25:14,320
tilda v tilde k tilde q  быть нашими старыми

699
00:25:14,320 --> 00:25:17,120
значениями, ключами и запросами, которые

700
00:25:17,120 --> 00:25:19,279
мы можем просто добавить, мы могли бы  делаем и другие вещи,

701
00:25:20,080 --> 00:25:22,480
но на практике мы просто добавляем так, что vi

702
00:25:22,480 --> 00:25:25,120
равно v тильде i, нашему вектору значений без порядка

703
00:25:25,120 --> 00:25:27,440
плюс пи, так что это может быть

704
00:25:27,440 --> 00:25:29,919
ваше вложение, вы знаете

705
00:25:29,919 --> 00:25:32,480
свой вектор встраивания, а затем вы

706
00:25:32,480 --> 00:25:35,360
добавляете индекс, который он находится, к его вектору

707
00:25:35,360 --> 00:25:37,279
эм, и вы знаете, что можете сделать это только

708
00:25:37,279 --> 00:25:38,720
на первом уровне сети,

709
00:25:38,720 --> 00:25:40,159
например, поэтому вы делаете то же самое для

710
00:25:40,159 --> 00:25:41,760
запроса и ключа, так что это то,

711
00:25:41,760 --> 00:25:43,440
что вы могли бы сделать

712
00:25:43,440 --> 00:25:44,480
на практике, мы делаем что-то немного

713
00:25:44,480 --> 00:25:45,440
другое,

714
00:25:45,440 --> 00:25:47,440
но

715
00:25:47,440 --> 00:25:48,799
это то, что  теперь вы знаете, что он

716
00:25:48,799 --> 00:25:52,159
знает порядок последовательности, потому что,

717
00:25:52,159 --> 00:25:55,360
если эти pis вы как-то правильно установите,

718
00:25:55,360 --> 00:25:57,200
тогда теперь сеть сможет

719
00:25:57,200 --> 00:25:59,120
выяснить, что с этим делать, так что один из

720
00:25:59,120 --> 00:26:02,799
способов на самом деле сделать это,

721
00:26:04,159 --> 00:26:06,480
один из способов сделать это -

722
00:26:06,480 --> 00:26:09,440
через  конкатенация синусоид, и это

723
00:26:09,440 --> 00:26:11,120
был интересный прием, когда

724
00:26:11,120 --> 00:26:12,960
вышла первая статья о трансформаторах,

725
00:26:12,960 --> 00:26:14,960
они использовали этот метод, поэтому давайте

726
00:26:14,960 --> 00:26:18,000
углубимся в него, ммм, чтобы у вас были

727
00:26:18,000 --> 00:26:19,919
разные длины волн синусоидальных

728
00:26:19,919 --> 00:26:21,039
функций

729
00:26:21,039 --> 00:26:23,120
в ea  ch ваших измерений, поэтому в

730
00:26:23,120 --> 00:26:24,880
первом измерении, если у вас есть эта

731
00:26:24,880 --> 00:26:26,799
функция синуса с заданным периодом,

732
00:26:26,799 --> 00:26:28,080
а затем эта функция косинуса с

733
00:26:28,080 --> 00:26:30,400
заданным периодом, а затем своего рода точка точка

734
00:26:30,400 --> 00:26:31,360
точка,

735
00:26:31,360 --> 00:26:33,360
вы как бы меняете периоды, пока не

736
00:26:33,360 --> 00:26:35,679
дойдете до очень разных периодов и

737
00:26:35,679 --> 00:26:36,720
как это выглядит, так

738
00:26:36,720 --> 00:26:37,679
что

739
00:26:37,679 --> 00:26:39,600
представьте, что здесь,

740
00:26:39,600 --> 00:26:41,919
на вертикальной оси, у нас есть

741
00:26:41,919 --> 00:26:45,039
размерность сети, так

742
00:26:45,039 --> 00:26:46,640
что это d,

743
00:26:46,640 --> 00:26:48,720
а затем это длина последовательности,

744
00:26:48,720 --> 00:26:50,960
и просто указав, что вы знаете, и каждая

745
00:26:50,960 --> 00:26:53,120
строка является своего рода одним из  эти знаки

746
00:26:53,120 --> 00:26:56,799
с разными частотами

747
00:26:56,799 --> 00:26:57,520
правильно,

748
00:26:57,520 --> 00:26:58,799
и вы можете увидеть, как это

749
00:26:58,799 --> 00:27:00,640
кодирует позицию, эти вещи имеют

750
00:27:00,640 --> 00:27:02,000
разные значения

751
00:27:02,000 --> 00:27:03,679
в разных индексах,

752
00:27:03,679 --> 00:27:06,240
и это довольно круто, я действительно не

753
00:27:06,240 --> 00:27:08,400
знаю, как они сразу об этом подумали,

754
00:27:08,400 --> 00:27:10,320
но одна классная вещь  об

755
00:27:10,320 --> 00:27:12,480
этом понятие периодичности верно тот факт,

756
00:27:12,480 --> 00:27:14,159
что синусоиды имеют периоды, которые

757
00:27:14,159 --> 00:27:16,720
могут быть меньше длины последовательности,

758
00:27:16,720 --> 00:27:18,320
указывает на то, что, возможно, абсолютная

759
00:27:18,320 --> 00:27:21,200
позиция слова не так важна

760
00:27:21,200 --> 00:27:23,520
Правильно, потому что вы, если период

761
00:27:23,520 --> 00:27:24,960
меньше длины последовательности, вы теряете

762
00:27:24,960 --> 00:27:26,960
информацию, может быть, о том, где вы находитесь,

763
00:27:26,960 --> 00:27:28,080
конечно, у вас есть конкатенация

764
00:27:28,080 --> 00:27:29,679
многих из них,

765
00:27:29,679 --> 00:27:31,520
так что это профессионал,

766
00:27:31,520 --> 00:27:32,960
может быть, его можно экстраполировать на более длинные

767
00:27:32,960 --> 00:27:34,880
последовательности, потому что снова вы вроде как

768
00:27:34,880 --> 00:27:38,000
правильное повторение значений, потому

769
00:27:38,000 --> 00:27:39,200
что периоды будут,

770
00:27:39,200 --> 00:27:40,480
когда они завершатся, вы увидите,

771
00:27:40,480 --> 00:27:42,799
что это значение снова, минусы в том, что

772
00:27:42,799 --> 00:27:44,399
это не обучается, я имею в виду, что это круто,

773
00:27:44,399 --> 00:27:46,240
но вы не можете

774
00:27:46,240 --> 00:27:48,480
в любом из этого нет обучаемых параметров, а также

775
00:27:48,480 --> 00:27:50,320
экстраполяция на самом деле не работает,

776
00:27:50,320 --> 00:27:51,840
так что это неинтересно и определенно

777
00:27:51,840 --> 00:27:53,039
все еще выполняется,

778
00:27:53,039 --> 00:27:55,840
но то, что делается сейчас чаще, - это

779
00:27:55,840 --> 00:27:57,520
мы, вы знаете, что мы делаем, мы изучаем

780
00:27:57,520 --> 00:28:01,279
представления позиций с нуля, так

781
00:28:01,279 --> 00:28:03,600
что у нас есть э-э,

782
00:28:03,600 --> 00:28:05,200
мы собираемся изучить их из  нацарапайте,

783
00:28:05,200 --> 00:28:06,960
так что пусть все пи будут просто

784
00:28:06,960 --> 00:28:08,559
параметрами, которые можно изучить, так что

785
00:28:08,559 --> 00:28:10,880
мы собираемся получить матрицу p,

786
00:28:10,880 --> 00:28:13,360
которая будет иметь размерность

787
00:28:13,360 --> 00:28:14,880
d размерность нашей сети снова

788
00:28:14,880 --> 00:28:16,480
с помощью последовательности  length, так что это просто

789
00:28:16,480 --> 00:28:18,480
большая матрица справа

790
00:28:18,480 --> 00:28:20,080
от размера

791
00:28:20,080 --> 00:28:22,399
здесь этого размера, фактически d по

792
00:28:22,399 --> 00:28:24,080
длине последовательности,

793
00:28:24,080 --> 00:28:26,240
но каждое отдельное значение в этой матрице является

794
00:28:26,240 --> 00:28:28,720
просто обучаемым параметром.

795
00:28:28,720 --> 00:28:30,799
Плюсы гибкости, теперь вы можете узнать,

796
00:28:30,799 --> 00:28:32,399
какие позиции вроде как должны

797
00:28:32,399 --> 00:28:34,559
означать в соответствии с  ваши данные от начала до

798
00:28:34,559 --> 00:28:36,640
конца, так что это крутые

799
00:28:36,640 --> 00:28:37,600
минусы, которые

800
00:28:37,600 --> 00:28:39,360
вы определенно не можете экстраполировать на

801
00:28:39,360 --> 00:28:41,440
индексы за пределами одного, чтобы отлично, потому

802
00:28:41,440 --> 00:28:43,120
что вы устанавливаете размер этой

803
00:28:43,120 --> 00:28:44,799
матрицы параметров в начале, и вы изучили

804
00:28:44,799 --> 00:28:46,480
их все сейчас, если хотите выйти за пределы

805
00:28:46,480 --> 00:28:48,000
позиции t

806
00:28:48,000 --> 00:28:49,840
вы знаете, что просто у вас нет возможности

807
00:28:49,840 --> 00:28:51,760
представить его эффективно,

808
00:28:51,760 --> 00:28:53,600
но большинство систем используют это, это очень

809
00:28:53,600 --> 00:28:54,640
полезно,

810
00:28:54,640 --> 00:28:55,440
и

811
00:28:55,440 --> 00:28:56,960
иногда люди пробуют более гибкие

812
00:28:56,960 --> 00:28:58,480
представления позиции, потому что

813
00:28:58,480 --> 00:29:01,279
снова абсолютный индекс слова

814
00:29:01,279 --> 00:29:03,520
не является своего рода естественным

815
00:29:03,520 --> 00:29:06,640
представлением его позиции в

816
00:29:06,640 --> 00:29:08,640
предложения, и поэтому люди смотрели

817
00:29:08,640 --> 00:29:09,919
на вид относительного положения

818
00:29:09,919 --> 00:29:12,080
между словами, а также на представления положения,

819
00:29:12,080 --> 00:29:14,080
которые зависят от

820
00:29:14,080 --> 00:29:15,679
синтаксиса, но мы не собираемся  в состоянии

821
00:29:15,679 --> 00:29:18,720
зайти слишком далеко в эти вопросы сегодня,

822
00:29:18,720 --> 00:29:20,720
ладно, так что это была проблема первая,

823
00:29:20,720 --> 00:29:22,399
верно, мы просто, что бы мы ни делали, если бы

824
00:29:22,399 --> 00:29:23,520
у нас не было представления о

825
00:29:23,520 --> 00:29:25,120
позиции, мы не могли бы использовать

826
00:29:25,120 --> 00:29:27,279
самовнимание в качестве нашего нового строительного блока,

827
00:29:27,279 --> 00:29:28,480
и мы '  Я решил это с позиционными

828
00:29:28,480 --> 00:29:29,919
представлениями, которые мы просто как бы добавляем

829
00:29:29,919 --> 00:29:32,559
к входам в

830
00:29:32,880 --> 00:29:34,720
следующем эм,

831
00:29:34,720 --> 00:29:36,000
мы увидим эту проблему, что у вас

832
00:29:36,000 --> 00:29:37,919
нет нелинейностей, о которых вы знаете, даже

833
00:29:37,919 --> 00:29:40,240
говоря, что вы знаете нелинейности, вы знаете

834
00:29:40,240 --> 00:29:42,320
абстрактные функции, которые они  отличное глубокое

835
00:29:42,320 --> 00:29:44,480
обучение, вы знаете, что сквозное

836
00:29:44,480 --> 00:29:46,720
изучение представлений - это здорово, но

837
00:29:46,720 --> 00:29:49,120
сейчас мы просто делаем средневзвешенные данные,

838
00:29:49,120 --> 00:29:51,840
и каким будет наше решение,

839
00:29:51,840 --> 00:29:54,240
я имею в виду, что оно не будет таким

840
00:29:54,240 --> 00:29:55,520
сложным,

841
00:29:55,520 --> 00:29:56,880
так что все мы '  Прямо сейчас мы делаем

842
00:29:56,880 --> 00:29:59,919
повторное усреднение векторов, так что у вас здесь,

843
00:29:59,919 --> 00:30:01,840
э-э, самовнимание, и

844
00:30:01,840 --> 00:30:03,679
если вы просто сложили еще один, вы просто

845
00:30:03,679 --> 00:30:06,640
сохраняете своего рода усредняющие проекции

846
00:30:06,640 --> 00:30:08,159
векторов, но что, если мы просто

847
00:30:08,159 --> 00:30:09,840
добавим прямую связь  сеть

848
00:30:09,840 --> 00:30:12,000
для каждого отдельного слова так внутри th  является

849
00:30:12,000 --> 00:30:13,520
слоем,

850
00:30:13,520 --> 00:30:15,279
каждая из этих нейронных сетей с прямой связью

851
00:30:15,279 --> 00:30:18,000
имеет общие параметры,

852
00:30:18,000 --> 00:30:19,600
но он получает только результат

853
00:30:19,600 --> 00:30:21,520
самовнимания для этого слова, как мы

854
00:30:21,520 --> 00:30:24,240
определили, он обрабатывает его,

855
00:30:24,240 --> 00:30:27,200
и вы знаете, что излучение испускает что-то еще,

856
00:30:27,200 --> 00:30:29,120
и поэтому вы знаете, что у вас есть вывод i от себя.

857
00:30:29,120 --> 00:30:32,000
внимание, которое мы видели на слайдах назад,

858
00:30:32,000 --> 00:30:34,159
применяем вы знаете слой с прямой связью,

859
00:30:34,159 --> 00:30:35,840
где вы берете результат, умноженный

860
00:30:35,840 --> 00:30:38,720
на матрицу, которую вы знаете, нелинейность, другая

861
00:30:38,720 --> 00:30:41,360
матрица, и интуиция, о которой вы можете

862
00:30:41,360 --> 00:30:43,919
подумать, по крайней мере, хорошо, что вы знаете

863
00:30:43,919 --> 00:30:45,919
что-то вроде сети с прямой связью

864
00:30:45,919 --> 00:30:49,360
обрабатывает результат внимания

865
00:30:49,360 --> 00:30:50,960
для каждой вещи, но

866
00:30:50,960 --> 00:30:52,799
более фундаментально верно, вам нужна была какая

867
00:30:52,799 --> 00:30:55,039
-то нелинейность, и вы

868
00:30:55,039 --> 00:30:56,880
знаете, что сеть с прямой связью

869
00:30:56,880 --> 00:30:59,279
хорошо справится, так что это еще одна

870
00:30:59,279 --> 00:31:01,440
проблема, решенная легко исправить

871
00:31:01,440 --> 00:31:02,880
добавить канал  -передовая сеть получает вашу

872
00:31:02,880 --> 00:31:05,840
нелинейность, а теперь ваш

873
00:31:05,840 --> 00:31:07,279
вывод самовнимания, вы можете вроде как обработать его

874
00:31:07,279 --> 00:31:09,200
глубину, увеличивающуюся по мере

875
00:31:09,200 --> 00:31:11,120
увеличения слоев сети, что, как мы

876
00:31:11,120 --> 00:31:13,679
знаем,

877
00:31:13,679 --> 00:31:16,399
полезно  проблема хорошо, так что терпите меня в

878
00:31:16,399 --> 00:31:17,440
этом,

879
00:31:17,440 --> 00:31:19,600
мы не хотим смотреть в будущее, когда

880
00:31:19,600 --> 00:31:21,519
мы делаем языковое моделирование

881
00:31:21,519 --> 00:31:22,960
правильно, поэтому языковое моделирование вы

882
00:31:22,960 --> 00:31:25,919
пытаетесь предсказать слова в будущем,

883
00:31:25,919 --> 00:31:28,000
и с повторяющейся моделью это очень

884
00:31:28,000 --> 00:31:29,760
естественно, как  вы просто

885
00:31:29,760 --> 00:31:32,320
не разворачиваете его дальше, как только вы

886
00:31:32,320 --> 00:31:33,840
развернули слово, чтобы

887
00:31:33,840 --> 00:31:36,159
развернуть свой lstm до данного слова,

888
00:31:36,159 --> 00:31:38,480
нет никакого способа

889
00:31:38,480 --> 00:31:40,640
передать его и следующему слову, но при

890
00:31:40,640 --> 00:31:42,080
самовнимании мы увидим, что это

891
00:31:42,080 --> 00:31:44,480
немного сложнее, так что мы не можем обмануть

892
00:31:44,480 --> 00:31:45,760
и посмотреть на вещи, которые мы пытаемся

893
00:31:45,760 --> 00:31:47,039
прогнозировать, потому что тогда мы будем обучать

894
00:31:47,039 --> 00:31:48,880
сети, которые были совершенно бесполезны, так

895
00:31:48,880 --> 00:31:50,240
что

896
00:31:50,240 --> 00:31:52,000
мы собираемся замаскировать

897
00:31:52,000 --> 00:31:53,120
маскировку - это слово, которое

898
00:31:53,120 --> 00:31:55,039
мы собираемся замаскировать будущее

899
00:32:00,960 --> 00:32:03,039
самовниманием, поэтому, в частности, это важно, когда у нас есть правильные декодеры, одна из причин, по которой мы могли использовать двунаправленные lstms в

900
00:32:03,039 --> 00:32:04,960
наших кодировщиках, заключалась в том, что мы могли видеть

901
00:32:04,960 --> 00:32:06,559
все  исходное предложение в нейронном машинном

902
00:32:06,559 --> 00:32:08,159
переводе, но когда  мы

903
00:32:08,159 --> 00:32:10,480
предсказываем выходное предложение правильно, мы не можем

904
00:32:10,480 --> 00:32:12,320
видеть будущее, если мы хотим обучить модель

905
00:32:12,320 --> 00:32:14,880
делать фактическое предсказание, поэтому, чтобы использовать

906
00:32:14,880 --> 00:32:16,799
самовнимание в декодере, вам нужно

907
00:32:16,799 --> 00:32:18,480
замаскировать будущее.

908
00:32:18,480 --> 00:32:20,399
мог бы

909
00:32:20,399 --> 00:32:22,720
просто каждый раз, когда вы вычисляете внимание,

910
00:32:22,720 --> 00:32:25,840
вы меняете набор ключей

911
00:32:25,840 --> 00:32:28,159
и значений, это должны быть ключи и

912
00:32:28,159 --> 00:32:30,480
значения, чтобы включать только прошлые слова, чтобы

913
00:32:30,480 --> 00:32:32,399
вы как бы динамически изменяли

914
00:32:32,399 --> 00:32:34,559
материал, который вы посещаете, но

915
00:32:34,559 --> 00:32:35,919
это не позволяет нам делать  как мы увидим, так и с

916
00:32:35,919 --> 00:32:38,960
тензорами, а также с возможностью распараллеливания

917
00:32:38,960 --> 00:32:40,159
,

918
00:32:40,159 --> 00:32:42,720
поэтому мы не хотим этого делать, вместо этого

919
00:32:42,720 --> 00:32:44,960
мы собираемся замаскировать будущие слова с

920
00:32:44,960 --> 00:32:46,880
помощью самих весов внимания,

921
00:32:46,880 --> 00:32:48,799
поэтому в математике не волнуйтесь, мы дойдем до

922
00:32:48,799 --> 00:32:50,799
своего рода диаграмма, но в математике у нас были эти

923
00:32:50,799 --> 00:32:53,120
оценки внимания,

924
00:32:53,120 --> 00:32:55,039
и они были равны только этому скалярному

925
00:32:55,039 --> 00:32:58,320
произведению раньше для всех пар,

926
00:32:59,120 --> 00:33:01,039
но теперь,

927
00:33:01,039 --> 00:33:03,440
только если

928
00:33:03,440 --> 00:33:05,919
ключ строго меньше, чем индекс

929
00:33:05,919 --> 00:33:08,000
ключа строго меньше, чем um, это должно

930
00:33:08,000 --> 00:33:10,159
быть i, только если  индекс ключа строго

931
00:33:10,159 --> 00:33:12,320
меньше t  измените индекс запроса, чтобы он

932
00:33:12,320 --> 00:33:13,519
был на j

933
00:33:13,519 --> 00:33:17,200
меньше, чем я, если мы позволим сети

934
00:33:17,200 --> 00:33:18,320
смотреть на слово, и оно должно быть

935
00:33:18,320 --> 00:33:20,320
отрицательной бесконечностью, иначе мы не

936
00:33:20,320 --> 00:33:23,039
позволим вам смотреть на результат, поэтому давайте перейдем

937
00:33:23,039 --> 00:33:24,960
к картинке для кодирования слов

938
00:33:24,960 --> 00:33:26,240
что мы увидим здесь, так что, возможно, у вас есть

939
00:33:26,240 --> 00:33:28,880
начальный токен, который

940
00:33:29,120 --> 00:33:30,960
вы хотите решить, это все ваше

941
00:33:30,960 --> 00:33:32,399
предложение, теперь вы хотите решить, какие

942
00:33:32,399 --> 00:33:34,159
слова в предложении

943
00:33:34,159 --> 00:33:35,679
вам разрешено смотреть при

944
00:33:35,679 --> 00:33:37,519
составлении прогнозов,

945
00:33:37,519 --> 00:33:39,200
поэтому я хочу предварительно  чтобы предсказать

946
00:33:39,200 --> 00:33:41,039
первое слово

947
00:33:41,039 --> 00:33:43,919
и э-э, чтобы предсказать, что мне не

948
00:33:43,919 --> 00:33:46,240
разрешено смотреть на слово, мне также

949
00:33:46,240 --> 00:33:47,919
не разрешено смотреть ни на одно из будущих

950
00:33:47,919 --> 00:33:48,960
слов,

951
00:33:48,960 --> 00:33:50,799
мне разрешено смотреть на начало слова,

952
00:33:50,799 --> 00:33:54,000
так что такого рода  блок здесь не заштрихован,

953
00:33:56,000 --> 00:33:58,080
чтобы поставить слово повар, которое я могу посмотреть

954
00:33:58,080 --> 00:34:01,840
в начале, и правильно начать, но не

955
00:34:01,840 --> 00:34:04,399
повар, естественно, или слово, которое идет

956
00:34:04,399 --> 00:34:06,080
после него, а также для других

957
00:34:06,080 --> 00:34:09,040
слов, чтобы вы могли видеть

958
00:34:09,040 --> 00:34:11,440
эту матрицу  здесь правильно, поэтому мы просто

959
00:34:11,440 --> 00:34:13,839
хотим убедиться, что наши натяжные

960
00:34:13,839 --> 00:34:15,599
веса  вы везде знаете ноль,

961
00:34:15,599 --> 00:34:17,918
поэтому при

962
00:34:17,918 --> 00:34:21,280
вычислении сродства мы добавляем отрицательную бесконечность

963
00:34:21,280 --> 00:34:24,480
ко всем этим в этой большой матрице,

964
00:34:24,480 --> 00:34:26,960
и это гарантирует, что мы не сможем хорошо смотреть

965
00:34:26,960 --> 00:34:29,199
в будущее,

966
00:34:29,199 --> 00:34:31,520
поэтому теперь мы можем выполнять большие матричные

967
00:34:31,520 --> 00:34:33,520
умножения, чтобы вычислить наше внимание,

968
00:34:33,520 --> 00:34:34,639
когда мы  увидим

969
00:34:34,639 --> 00:34:35,599
эм,

970
00:34:35,599 --> 00:34:37,440
и мы вроде как не беспокоимся о том, чтобы смотреть

971
00:34:37,440 --> 00:34:38,800
в будущее, потому что мы добавили эти

972
00:34:38,800 --> 00:34:40,800
отрицательные бесконечности,

973
00:34:40,800 --> 00:34:42,560
и это последнее, что последняя

974
00:34:42,560 --> 00:34:44,480
проблема с самовниманием типа,

975
00:34:44,480 --> 00:34:45,280
которая

976
00:34:45,280 --> 00:34:47,040
возникает принципиально, как то, что

977
00:34:47,040 --> 00:34:50,320
нам нужно для этого  строительный блок,

978
00:34:50,800 --> 00:34:52,639
который у вас есть, у вас не было неотъемлемого

979
00:34:52,639 --> 00:34:54,399
понятия порядка теперь у вас есть хорошее

980
00:34:54,399 --> 00:34:56,079
представление о порядке или, по крайней мере, что-то

981
00:34:56,079 --> 00:34:57,839
вроде понятия порядка у вас не было

982
00:34:57,839 --> 00:34:59,680
нелинейностей добавить

983
00:34:59,680 --> 00:35:01,119
сети с прямой связью,

984
00:35:01,119 --> 00:35:02,800
а затем вы

985
00:35:02,800 --> 00:35:04,880
не хотели  Чтобы заглянуть в будущее, вы

986
00:35:04,880 --> 00:35:08,640
добавляете маски для декодеров,

987
00:35:10,560 --> 00:35:12,560
чтобы знать, что самовнимание - это

988
00:35:12,560 --> 00:35:13,920
основа любого строительного

989
00:35:13,920 --> 00:35:15,200
блока, основанного на самовнимании.

990
00:35:15,200 --> 00:35:17,359
Представления позиции приветствия полезны.

991
00:35:19,440 --> 00:35:21,520
сеть с прямой связью,

992
00:35:21,520 --> 00:35:23,200
как будто вы могли бы просто сделать

993
00:35:23,200 --> 00:35:25,119
другие вещи, я думаю, но вы знаете, что на

994
00:35:25,119 --> 00:35:26,480
практике очень легко

995
00:35:26,480 --> 00:35:28,160
распараллелить эти сети с прямой связью,

996
00:35:28,160 --> 00:35:30,880
так что мы в конечном итоге делаем это, а

997
00:35:30,880 --> 00:35:32,800
затем маскируем,

998
00:35:32,800 --> 00:35:34,320
вы знаете, да, вы не делаете  Я не хочу, чтобы

999
00:35:34,320 --> 00:35:35,359
информация просачивалась

1000
00:35:35,359 --> 00:35:36,960
из будущего в прошлое в вашем

1001
00:35:36,960 --> 00:35:39,040
декодере,

1002
00:35:39,040 --> 00:35:41,839
поэтому позвольте мне прояснить, что мы еще не говорили

1003
00:35:41,839 --> 00:35:44,560
о трансформаторе,

1004
00:35:44,560 --> 00:35:46,560
но это все, что вам нужно, если

1005
00:35:46,560 --> 00:35:48,079
вы думаете, как черт возьми, что мне нужно,

1006
00:35:48,079 --> 00:35:49,280
чтобы построить

1007
00:35:49,280 --> 00:35:50,800
Блок моего самовнимания, мы увидим, что

1008
00:35:50,800 --> 00:35:52,480
в трансформаторе есть гораздо больше деталей,

1009
00:35:52,480 --> 00:35:53,760
которые мы собираемся провести

1010
00:35:53,760 --> 00:35:55,839
остаток лекции, э-э

1011
00:35:55,839 --> 00:35:57,520
и я,

1012
00:35:57,520 --> 00:36:00,000
но я хочу, чтобы вы вроде как, по крайней мере, э-э, как

1013
00:36:00,000 --> 00:36:01,359
вы »  думаю о том, что

1014
00:36:01,359 --> 00:36:03,440
будет дальше после трансформатора и как

1015
00:36:03,440 --> 00:36:05,680
вы собираетесь его изобрести ... подумайте

1016
00:36:05,680 --> 00:36:07,760
о том, что это были те вещи, которые

1017
00:36:07,760 --> 00:36:09,760
были необходимы ммм, а затем другие

1018
00:36:09,760 --> 00:36:11,359
вещи в конечном итоге оказываются очень, очень важными,

1019
00:36:11,359 --> 00:36:14,800
оказывается, ммм, но  ты знаешь, что есть

1020
00:36:14,800 --> 00:36:16,320
здесь много места для дизайна, которое

1021
00:36:16,320 --> 00:36:19,119
еще не исследовано,

1022
00:36:19,280 --> 00:36:21,119
хорошо, так что давайте поговорим о модели трансформатора,

1023
00:36:21,119 --> 00:36:22,800
и я собираюсь сделать паузу, если

1024
00:36:22,800 --> 00:36:24,320
есть какие-то, это хороший вопрос, я могу

1025
00:36:24,320 --> 00:36:27,839
принять, я могу принять его сейчас,

1026
00:36:30,880 --> 00:36:32,240
хорошо,

1027
00:36:32,240 --> 00:36:34,640
так что трансформаторы

1028
00:36:34,640 --> 00:36:36,160
давайте  к нему,

1029
00:36:37,200 --> 00:36:39,280
давайте сначала посмотрим на

1030
00:36:39,280 --> 00:36:41,520
блоки декодера кодировщика трансформатора на высоком уровне,

1031
00:36:41,520 --> 00:36:43,200
это должно быть очень похоже на декодеры кодировщика,

1032
00:36:43,200 --> 00:36:45,280
которые мы видели

1033
00:36:45,280 --> 00:36:47,839
в системах

1034
00:36:49,040 --> 00:36:50,320
машинного перевода рекуррентных нейронных сетей,

1035
00:36:50,320 --> 00:36:52,720
которые мы видели хорошо, поэтому

1036
00:36:52,720 --> 00:36:54,800
у нас есть вложения слов,

1037
00:36:54,800 --> 00:36:56,000
которые мы  собираясь добавить в наши представления положения,

1038
00:36:56,000 --> 00:36:58,320
мы видели это,

1039
00:36:58,320 --> 00:37:00,560
и это из нашей входной последовательности у нас

1040
00:37:00,560 --> 00:37:02,480
будет последовательность блоков кодировщика, каждый

1041
00:37:02,480 --> 00:37:05,359
из которых называется кодировщиком преобразователя,

1042
00:37:05,359 --> 00:37:07,040
и тогда вы знаете, что у нас есть наше

1043
00:37:07,040 --> 00:37:08,800
слово выходной последовательности, встраивающее

1044
00:37:08,800 --> 00:37:10,400
представление положения снова

1045
00:37:10,400 --> 00:37:14,160
у нас есть  трансформаторный декодер,

1046
00:37:14,160 --> 00:37:16,480
каждый и последний уровень кодировщиков

1047
00:37:17,760 --> 00:37:20,320
будет использоваться на каждом уровне трансформаторного

1048
00:37:20,320 --> 00:37:22,079
декодера,

1049
00:37:22,079 --> 00:37:23,760
а затем мы получим некоторые выходные данные, некоторые

1050
00:37:23,760 --> 00:37:25,359
прогнозы хорошо, так что это выглядит красиво

1051
00:37:25,359 --> 00:37:27,280
почти то же самое на очень высоком уровне, возможно,

1052
00:37:27,280 --> 00:37:28,640
за вычетом того факта, что теперь нам нужно

1053
00:37:28,640 --> 00:37:30,320
добавить представление позиции

1054
00:37:30,320 --> 00:37:32,960
в самом начале,

1055
00:37:32,960 --> 00:37:34,400
поэтому теперь давайте посмотрим на сами эти блоки,

1056
00:37:34,400 --> 00:37:36,640
чтобы кодер и декодер

1057
00:37:36,640 --> 00:37:38,320
блокировали то, что осталось, что мы не

1058
00:37:38,320 --> 00:37:40,160
охватили правильно, потому что  мы могли бы просто

1059
00:37:40,160 --> 00:37:41,599
поместить строительные блоки, которые мы только что

1060
00:37:41,599 --> 00:37:44,400
придумали в первой части класса, в эти

1061
00:37:44,400 --> 00:37:46,560
вещи правильные кодеры нам нужно

1062
00:37:46,560 --> 00:37:48,400
наше внимание наши наши сети прямой

1063
00:37:48,400 --> 00:37:50,079
связи

1064
00:37:50,079 --> 00:37:51,680
у нас есть свои тюремные представления мы

1065
00:37:51,680 --> 00:37:53,680
получаем маскировку для декодеров правильно

1066
00:37:53,680 --> 00:37:55,440
мы можем  просто вставьте их, но

1067
00:37:55,440 --> 00:37:57,200
оказывается, что они не будут работать так

1068
00:37:57,200 --> 00:37:58,800
хорошо по сравнению с трансформаторами, поэтому что

1069
00:37:58,800 --> 00:37:59,599
осталось,

1070
00:37:59,599 --> 00:38:01,520
так что первое, что нужно, - это внимание к ключевому значению запроса,

1071
00:38:02,960 --> 00:38:04,880
это особый способ получить

1072
00:38:04,880 --> 00:38:07,920
векторы kq и v из одного слова

1073
00:38:07,920 --> 00:38:10,560
правильное внедрение, поэтому вместо того, чтобы иметь kq

1074
00:38:10,560 --> 00:38:13,599
и v, равные x, как на выходе

1075
00:38:13,599 --> 00:38:14,800
из последнего слоя, мы собираемся сделать

1076
00:38:14,800 --> 00:38:16,640
что-то немного большее, а

1077
00:38:16,640 --> 00:38:18,160
затем - многоголовое внимание, которое мы

1078
00:38:18,160 --> 00:38:20,480
собираемся уделить mul  расположите места в

1079
00:38:20,480 --> 00:38:22,320
одном слое, и мы увидим, что это

1080
00:38:22,320 --> 00:38:25,119
даст нам что-то вроде

1081
00:38:25,119 --> 00:38:27,599
интересного в домашнем задании позже,

1082
00:38:27,599 --> 00:38:28,720
но мы поговорим об этом немного

1083
00:38:28,720 --> 00:38:31,040
сегодня, а затем будет куча вещей,

1084
00:38:31,040 --> 00:38:32,960
которые просто помогут с  обучение

1085
00:38:32,960 --> 00:38:34,560
казалось, что их было очень сложно обучить

1086
00:38:34,560 --> 00:38:35,920
сначала многие из этих приемов очень

1087
00:38:35,920 --> 00:38:37,200
полезны, поэтому мы поговорим о

1088
00:38:37,200 --> 00:38:39,200
нормализации слоя остаточных связей и

1089
00:38:39,200 --> 00:38:41,040
масштабировании скалярного произведения все, что указано в

1090
00:38:41,040 --> 00:38:43,440
третьем маркированном

1091
00:38:43,440 --> 00:38:44,960
списке, здесь приемы, которые помогут с обучением, не

1092
00:38:44,960 --> 00:38:47,680
улучшаются  на что способна модель,

1093
00:38:47,680 --> 00:38:49,359
но они имеют решающее значение в том, что они улучшают

1094
00:38:49,359 --> 00:38:50,880
процесс обучения,

1095
00:38:50,880 --> 00:38:53,359
поэтому моделирование улучшений обоих

1096
00:38:53,359 --> 00:38:56,240
видов действительно очень важно, так что вы

1097
00:38:56,240 --> 00:38:57,200
знаете, что хорошо, что мы используем

1098
00:38:57,200 --> 00:38:58,640
самовнимание, которое является этой классной вещью,

1099
00:38:58,640 --> 00:39:00,160
которая  эти свойства, но если бы мы

1100
00:39:00,160 --> 00:39:02,480
не могли его обучить, это было бы бесполезно,

1101
00:39:02,480 --> 00:39:04,800
хорошо,

1102
00:39:04,880 --> 00:39:06,800
поэтому вот как преобразователь строит

1103
00:39:06,800 --> 00:39:10,800
ключевые векторы запроса и значений,

1104
00:39:12,480 --> 00:39:16,800
у нас есть uh x1 для xt входных векторов в

1105
00:39:16,800 --> 00:39:19,680
наш слой преобразователя, хорошо, и мы

1106
00:39:19,680 --> 00:39:21,520
пытаясь избежать здесь кодировщика трансформатора,

1107
00:39:22,480 --> 00:39:23,680
чтобы мы

1108
00:39:23,680 --> 00:39:25,520
знали, что вы знаете один из этих векторов на каждое слово, которое

1109
00:39:25,520 --> 00:39:27,920
вы можете сказать, и снова каждый xi

1110
00:39:27,920 --> 00:39:30,400
будет вектором в размерности d,

1111
00:39:30,400 --> 00:39:31,839
и вот как мы вычисляем

1112
00:39:31,839 --> 00:39:33,520
запросы ключей и значения,

1113
00:39:33,520 --> 00:39:36,560
которые мы собираемся  пусть каждый ключ ki, который

1114
00:39:36,560 --> 00:39:37,760
мы видели ранее,

1115
00:39:37,760 --> 00:39:40,560
равен некоторой матрице k, умноженной на xi,

1116
00:39:40,560 --> 00:39:42,560
где k равно d на d

1117
00:39:42,560 --> 00:39:44,480
правильно, так что это преобразование от

1118
00:39:44,480 --> 00:39:47,760
размерности d к размерности d,

1119
00:39:47,760 --> 00:39:49,200
мы собираемся называть это ключевой матрицей

1120
00:39:49,200 --> 00:39:50,000
k,

1121
00:39:50,000 --> 00:39:51,040
и мы собираемся  сделать то же самое

1122
00:39:51,040 --> 00:39:53,119
для запросов, хорошо, поэтому мы

1123
00:39:53,119 --> 00:39:55,920
возьмем xi, умножим его на матрицу, получим

1124
00:39:55,920 --> 00:39:57,359
вектор запроса,

1125
00:39:57,359 --> 00:39:59,599
и мы сделаем то же самое для v, хорошо,

1126
00:39:59,599 --> 00:40:00,720
так что

1127
00:40:00,720 --> 00:40:02,240
вы можете просто подключить это прямо сейчас,

1128
00:40:02,240 --> 00:40:04,319
вместо того, чтобы говорить  что все kq

1129
00:40:04,319 --> 00:40:06,480
и v все такие же, как x, все они

1130
00:40:06,480 --> 00:40:08,000
немного отличаются, потому что вы применяете

1131
00:40:08,000 --> 00:40:10,000
линейное преобразование, что это делает

1132
00:40:10,000 --> 00:40:11,359
хорошо, вы знаете,

1133
00:40:11,359 --> 00:40:13,040
вы можете думать об этом так же хорошо, как вы

1134
00:40:13,040 --> 00:40:15,920
знаете, что матрицы kq и v могут быть очень

1135
00:40:15,920 --> 00:40:17,920
разными  друг от друга прямо, и поэтому

1136
00:40:17,920 --> 00:40:20,079
они вроде гм

1137
00:40:20,079 --> 00:40:22,079
подчеркивать или разрешать использование различных аспектов

1138
00:40:22,079 --> 00:40:24,880
векторов x в каждой из

1139
00:40:24,880 --> 00:40:27,440
трех ролей, поэтому мы выписали

1140
00:40:27,440 --> 00:40:29,040
уравнения самовнимания с тремя

1141
00:40:29,040 --> 00:40:30,480
ролями, чтобы указать,

1142
00:40:30,480 --> 00:40:32,000
что с каждой из них выполняются разные вещи, так что,

1143
00:40:32,000 --> 00:40:34,800
возможно, вы знаете k  и q помогают вам

1144
00:40:34,800 --> 00:40:36,560
выяснить, где искать,

1145
00:40:36,560 --> 00:40:38,480
и поэтому они должны быть определенным образом, они

1146
00:40:38,480 --> 00:40:40,079
должны смотреть на разные части x, а

1147
00:40:40,079 --> 00:40:42,079
затем v значение,

1148
00:40:42,079 --> 00:40:44,000
возможно, вы хотите передать

1149
00:40:44,000 --> 00:40:45,359
другую информацию, чем то,

1150
00:40:45,359 --> 00:40:46,800
что на самом деле помогает вам

1151
00:40:46,800 --> 00:40:48,720
получить доступ к этому  информация,

1152
00:40:50,000 --> 00:40:52,000
так что это важно, как мы это делаем

1153
00:40:52,000 --> 00:40:53,839
на практике, мы вычисляем это с действительно

1154
00:40:53,839 --> 00:40:57,040
большими тензорами, так что у нас были наши x-векторы, о

1155
00:40:57,040 --> 00:40:58,400
которых мы говорили, вроде

1156
00:40:58,400 --> 00:41:00,160
слово за словами, где у вас была последовательность от

1157
00:41:00,160 --> 00:41:02,000
xix 1 до xt

1158
00:41:02,000 --> 00:41:03,760
um  теперь мы собираемся представить их все

1159
00:41:03,760 --> 00:41:06,960
как матрицу x, которая находится в нашей последовательности

1160
00:41:06,960 --> 00:41:09,200
по размерности, поэтому

1161
00:41:09,200 --> 00:41:11,040
длина последовательности на d

1162
00:41:11,040 --> 00:41:13,040
заглавная t на d,

1163
00:41:13,040 --> 00:41:15,760
и теперь, если у нас есть матрица для каждого

1164
00:41:15,760 --> 00:41:17,920
из наших ключевых запросов и значений, мы

1165
00:41:17,920 --> 00:41:19,680
идем  приложить  y, как будто мы собираемся посмотреть

1166
00:41:19,680 --> 00:41:23,119
на эти вещи xkxq и xv, которые

1167
00:41:23,119 --> 00:41:25,359
имеют ту же размерность, что и x,

1168
00:41:25,359 --> 00:41:28,319
из-за преобразований d на d,

1169
00:41:28,319 --> 00:41:29,760
поэтому как мы вычисляем

1170
00:41:29,760 --> 00:41:32,560
самовнимание, у нас есть выходной тензор,

1171
00:41:32,560 --> 00:41:34,400
который имеет ту же размерность, что и

1172
00:41:34,400 --> 00:41:35,599
вход

1173
00:41:35,599 --> 00:41:37,200
x будет равен soft max,

1174
00:41:37,200 --> 00:41:40,000
там softmax этого умножения матриц,

1175
00:41:40,000 --> 00:41:41,359
который мы получим в умножении

1176
00:41:41,359 --> 00:41:43,520
на векторы значений, поэтому

1177
00:41:43,520 --> 00:41:45,200
умножение матриц здесь вычисляет

1178
00:41:45,200 --> 00:41:46,720
сродство между ключами и запросами, которые

1179
00:41:46,720 --> 00:41:48,960
мы увидим, а затем наше усреднение,

1180
00:41:48,960 --> 00:41:51,440
что делает  которые выглядят как наглядно,

1181
00:41:51,440 --> 00:41:53,280
поэтому вы берете ключевые точечные произведения запроса,

1182
00:41:53,280 --> 00:41:56,079
поэтому этот термин здесь xq

1183
00:41:56,079 --> 00:41:58,240
xk transpose

1184
00:41:58,240 --> 00:42:00,160
дает вам все точечные произведения

1185
00:42:00,160 --> 00:42:02,640
всех t на t пар оценок внимания, поэтому

1186
00:42:02,640 --> 00:42:05,280
наши eij находятся в этой матрице прямо здесь,

1187
00:42:05,280 --> 00:42:07,280
это t на t, и это  просто большое

1188
00:42:07,280 --> 00:42:09,119
матричное умножение, поэтому вы выполняете

1189
00:42:09,119 --> 00:42:11,680
матричное умножение xq, а затем xk,

1190
00:42:11,680 --> 00:42:14,480
а затем вы получаете все скалярные произведения

1191
00:42:14,480 --> 00:42:16,560
с помощью этого хорошо, так что теперь у вас есть

1192
00:42:16,560 --> 00:42:18,640
этот большой набор оценок t на t, это то,

1193
00:42:18,640 --> 00:42:21,440
что мы хотели  m, и теперь вы можете soft

1194
00:42:21,440 --> 00:42:24,800
max, который непосредственно в качестве матрицы,

1195
00:42:24,800 --> 00:42:26,560
а затем вы знаете, что здесь выполняется

1196
00:42:26,560 --> 00:42:28,800
умножение матрицы с xv, чтобы

1197
00:42:28,800 --> 00:42:30,400
дать свой выходной вектор, так что на

1198
00:42:30,400 --> 00:42:32,240
самом деле это средневзвешенное значение,

1199
00:42:32,240 --> 00:42:33,599
которое мы видели в начале

1200
00:42:33,599 --> 00:42:34,560
класса,

1201
00:42:34,560 --> 00:42:36,160
и это вы  знаю, что здесь нет циклов for,

1202
00:42:36,160 --> 00:42:37,920
это действительно красиво, что мы говорим в

1203
00:42:37,920 --> 00:42:40,640
векторизованном формате um, и это дает нам наш результат,

1204
00:42:40,640 --> 00:42:42,720
который снова запоминает ту же размерность

1205
00:42:42,720 --> 00:42:44,800
t на d,

1206
00:42:44,800 --> 00:42:46,880
хорошо, поэтому все периоды внимания

1207
00:42:46,880 --> 00:42:49,119
затем вычисляют средние,

1208
00:42:49,119 --> 00:42:51,280
применяя оценку softmax оценок к

1209
00:42:52,160 --> 00:42:54,800
xv

1210
00:42:56,839 --> 00:42:59,760
matrix um, так что это все для внимания к ключевому

1211
00:42:59,760 --> 00:43:01,440
значению запроса, вот как вы

1212
00:43:01,440 --> 00:43:04,079
знаете, что мы реализуем это с помощью тензоров, а

1213
00:43:04,079 --> 00:43:06,000
затем мы рассмотрим вид следующего типа

1214
00:43:06,000 --> 00:43:06,960
um,

1215
00:43:06,960 --> 00:43:08,400
который в конечном итоге будет очень важен

1216
00:43:08,400 --> 00:43:09,760
для обучения трансформаторов на практике,

1217
00:43:09,760 --> 00:43:11,599
который является мульти  -головое внимание так

1218
00:43:11,599 --> 00:43:13,040
трансформирует кодировщика многоголовое

1219
00:43:13,040 --> 00:43:16,240
внимание, так что вопрос в том,

1220
00:43:16,240 --> 00:43:17,680
что, если мы хотим смотреть сразу на несколько

1221
00:43:17,680 --> 00:43:20,560
мест в предложении,

1222
00:43:20,560 --> 00:43:22,720
можно сделать то, что вы знаете  с

1223
00:43:22,720 --> 00:43:23,839
самовниманием с нормальным

1224
00:43:23,839 --> 00:43:26,000
самовниманием, но подумайте об этом,

1225
00:43:26,000 --> 00:43:27,599
где вы в конечном итоге смотрите на

1226
00:43:27,599 --> 00:43:29,119
себя,

1227
00:43:29,119 --> 00:43:31,280
вы в конечном итоге смотрите, где скалярные

1228
00:43:31,280 --> 00:43:34,880
произведения xi, ваша q-матрица, транспонирует

1229
00:43:34,880 --> 00:43:37,200
вашу ключевую матрицу, xj высока, так что это

1230
00:43:37,200 --> 00:43:39,760
те, которые отсортированы  из пар xij извините,

1231
00:43:39,760 --> 00:43:41,760
это пары ij, которые в конечном итоге

1232
00:43:41,760 --> 00:43:44,400
взаимодействуют друг с другом,

1233
00:43:44,400 --> 00:43:46,720
но, возможно, для какого-то запроса для какого-то слова

1234
00:43:46,720 --> 00:43:48,960
в выходном для какого-то слова вы хотите

1235
00:43:48,960 --> 00:43:50,400
сосредоточиться на других других словах в

1236
00:43:50,400 --> 00:43:52,400
предложении по разным

1237
00:43:52,400 --> 00:43:54,319
причинам, как вы  может кодировать это,

1238
00:43:54,319 --> 00:43:55,359
имея

1239
00:43:55,359 --> 00:43:58,079
несколько матриц ключей и значений запроса,

1240
00:43:58,079 --> 00:44:00,319
которые все кодируют разные вещи

1241
00:44:00,319 --> 00:44:02,400
о xi, которые они все переводят, изучать разные

1242
00:44:02,400 --> 00:44:04,560
преобразования, поэтому вместо одной

1243
00:44:04,560 --> 00:44:06,800
очереди мы получаем один k и один v, что мы

1244
00:44:06,800 --> 00:44:11,280
получаем, это aq sub lk sub lv sub l

1245
00:44:11,280 --> 00:44:13,760
все другой размерности теперь, поэтому

1246
00:44:13,760 --> 00:44:16,480
по их размерности d на d над h,

1247
00:44:16,480 --> 00:44:18,160
где h - количество головок, поэтому

1248
00:44:18,160 --> 00:44:20,560
они все еще будут применяться к матрице x,

1249
00:44:20,560 --> 00:44:22,640
но они собираются преобразовать ее

1250
00:44:22,640 --> 00:44:26,640
в меньшие размеры  Иональность d за h

1251
00:44:26,640 --> 00:44:28,880
um, а затем каждая голова внимания будет

1252
00:44:28,880 --> 00:44:30,560
выполнять натяжение независимо,

1253
00:44:30,560 --> 00:44:31,680
как будто вы только что делали это целую кучу

1254
00:44:31,680 --> 00:44:32,640
раз

1255
00:44:32,640 --> 00:44:34,880
правильно, и поэтому выход l равен soft

1256
00:44:34,880 --> 00:44:37,760
softmax, вы знаете, вот ваше qk, но

1257
00:44:37,760 --> 00:44:42,640
теперь оно в форме l  умножить на xvl, и теперь у

1258
00:44:42,640 --> 00:44:45,119
вас есть вид этих проиндексированных

1259
00:44:45,119 --> 00:44:46,560
выходов,

1260
00:44:46,560 --> 00:44:48,400
и для того, чтобы вроде бы

1261
00:44:48,400 --> 00:44:49,920
размерность выхода была равна

1262
00:44:49,920 --> 00:44:51,599
размерности входа и типа смешивания вещей,

1263
00:44:53,040 --> 00:44:54,319
объедините всю информацию из

1264
00:44:54,319 --> 00:44:56,720
разных головок, которые вы объединяете,

1265
00:44:56,720 --> 00:44:58,319
чтобы вывести  от одного до

1266
00:44:58,319 --> 00:45:00,800
вывода h складываем их вместе, теперь

1267
00:45:00,800 --> 00:45:03,119
размерность этого снова равна

1268
00:45:03,119 --> 00:45:05,040
размерности x,

1269
00:45:05,040 --> 00:45:07,200
а затем мы используем выученную матрицу y,

1270
00:45:07,200 --> 00:45:10,400
чтобы вроде как смешивать uh y is d

1271
00:45:10,400 --> 00:45:11,839
by d, и это результат

1272
00:45:11,839 --> 00:45:13,760
многоголовой  внимание

1273
00:45:13,760 --> 00:45:16,400
многоголовое самовнимание

1274
00:45:16,960 --> 00:45:19,280
и так потому что разные, так что каждая голова

1275
00:45:19,280 --> 00:45:21,200
может смотреть на разные вещи правильно,

1276
00:45:21,200 --> 00:45:23,359
потому что все они могут типа эээ,

1277
00:45:23,359 --> 00:45:25,200
линейные преобразования, можно сказать,

1278
00:45:25,200 --> 00:45:26,720
сосредоточиться на разных частях  векторов x

1279
00:45:26,720 --> 00:45:29,599
и um векторы значений также

1280
00:45:29,599 --> 00:45:32,880
должны быть разными,

1281
00:45:32,880 --> 00:45:34,640
так что наглядно это то, что у нас было

1282
00:45:34,640 --> 00:45:37,920
до того, как одноголовое внимание у вас было x,

1283
00:45:37,920 --> 00:45:41,760
умноженное на q, чтобы получить xq,

1284
00:45:41,760 --> 00:45:43,200
и что интересно, и вы можете видеть

1285
00:45:43,200 --> 00:45:44,800
это, вы знаете,

1286
00:45:44,800 --> 00:45:46,160
что можете  Видите это из этой диаграммы, я

1287
00:45:46,160 --> 00:45:47,920
думаю, что многоголовое внимание

1288
00:45:47,920 --> 00:45:49,920
не обязательно должно быть больше,

1289
00:45:49,920 --> 00:45:53,200
больше работы, верно, мы видели, что векторы qk и

1290
00:45:53,200 --> 00:45:55,920
l uh, извините, матрицы qk и v

1291
00:45:55,920 --> 00:45:58,560
при многоголовом натяжении имеют

1292
00:45:58,560 --> 00:46:01,119
более низкий выход  размерность, так что вот

1293
00:46:01,119 --> 00:46:03,760
два из них прямо здесь, вот q1 и q2

1294
00:46:03,760 --> 00:46:06,480
того же размера, что и q,

1295
00:46:06,480 --> 00:46:09,839
а затем вы нажимаете выходы xq1 и xq2,

1296
00:46:09,839 --> 00:46:11,119
и поэтому вы фактически выполняете тот же

1297
00:46:11,119 --> 00:46:13,040
объем вычислений, что и раньше,

1298
00:46:13,040 --> 00:46:14,640
но теперь вы вроде как делаете, у вас

1299
00:46:14,640 --> 00:46:15,920
разные  распределения задержек для

1300
00:46:15,920 --> 00:46:17,280
каждой из разных голов, так что это

1301
00:46:17,280 --> 00:46:19,680
довольно круто,

1302
00:46:19,680 --> 00:46:21,760
хорошо,

1303
00:46:23,200 --> 00:46:25,040
так что это основные

1304
00:46:25,040 --> 00:46:27,200
различия в моделировании, правильно мы сделали

1305
00:46:27,200 --> 00:46:29,040
внимание к значению ключевого запроса,

1306
00:46:31,359 --> 00:46:34,319
вот как мы

1307
00:46:34,319 --> 00:46:36,960
получили ключевые запросы и значения из

1308
00:46:36,960 --> 00:46:37,920
вектора x  s,

1309
00:46:37,920 --> 00:46:40,000
и мы увидели, как реализовать это в

1310
00:46:40,000 --> 00:46:42,880
матрицах, на которые мы смотрим,

1311
00:46:42,880 --> 00:46:44,319
а затем мы

1312
00:46:44,319 --> 00:46:45,440
посмотрели

1313
00:46:45,440 --> 00:46:47,440
на многогранное внимание, которое позволяет

1314
00:46:47,440 --> 00:46:48,640
нам смотреть в разные места

1315
00:46:49,760 --> 00:46:51,760
в последовательности, чтобы

1316
00:46:51,760 --> 00:46:53,119
теперь иметь больше гибкости в пределах

1317
00:46:53,119 --> 00:46:54,240
данного слоя.

1318
00:46:54,240 --> 00:46:55,280
мы собираемся поговорить о наших

1319
00:46:55,280 --> 00:46:56,480
тренировочных приемах,

1320
00:46:56,480 --> 00:46:58,400
они действительно важны, оказывается,

1321
00:46:58,400 --> 00:47:00,079
и так

1322
00:47:00,079 --> 00:47:01,440
что да,

1323
00:47:01,440 --> 00:47:03,440
подумав о них, я думаю, это

1324
00:47:03,440 --> 00:47:04,800
то, чего мы недостаточно делаем

1325
00:47:04,800 --> 00:47:06,400
в полевых условиях, и поэтому давайте действительно пройдемся

1326
00:47:06,400 --> 00:47:08,400
по ним, так что остаточные связи

1327
00:47:08,400 --> 00:47:10,720
остаточные соединения были вокруг

1328
00:47:10,720 --> 00:47:12,480
остаточных соединений, вы можете думать о

1329
00:47:12,480 --> 00:47:14,720
них как о помощи в обучении модели

1330
00:47:14,720 --> 00:47:16,720
по ряду причин, давайте

1331
00:47:16,720 --> 00:47:18,319
сначала посмотрим, что они делают, наше

1332
00:47:18,319 --> 00:47:19,520
остаточное соединение выглядит так, поэтому у

1333
00:47:19,520 --> 00:47:21,680
вас есть нормальный слой

1334
00:47:21,680 --> 00:47:22,640
x

1335
00:47:22,640 --> 00:47:24,720
в некотором слое ii  представляет собой

1336
00:47:24,720 --> 00:47:26,240
вид слоя и глубины

1337
00:47:26,240 --> 00:47:28,640
в сети,

1338
00:47:28,640 --> 00:47:31,680
так что xi равно некоторому слою xi

1339
00:47:31,680 --> 00:47:33,599
минус один, так что вы были правы, вы имели, я

1340
00:47:33,599 --> 00:47:35,200
не знаю, что этот слой делает

1341
00:47:35,200 --> 00:47:36,480
обязательно,

1342
00:47:36,480 --> 00:47:38,720
но этот слой  эр - это функция

1343
00:47:38,720 --> 00:47:40,960
предыдущего слоя, хорошо,

1344
00:47:40,960 --> 00:47:43,680
и у вас это получилось, поэтому я снова

1345
00:47:43,680 --> 00:47:45,280
хочу абстрагироваться от того, что

1346
00:47:45,280 --> 00:47:47,520
делает слой, но вы просто передаете его

1347
00:47:47,520 --> 00:47:49,760
через остаточное соединение, делает что-то

1348
00:47:49,760 --> 00:47:51,520
очень простое, он говорит,

1349
00:47:51,520 --> 00:47:54,240
хорошо, я иду  чтобы взять функцию, которую

1350
00:47:54,240 --> 00:47:55,680
я вычислял на моем предыдущем уровне

1351
00:47:55,680 --> 00:47:57,280
раньше,

1352
00:47:57,280 --> 00:47:58,880
и я собираюсь добавить ее к предыдущему

1353
00:47:58,880 --> 00:48:00,720
слою, поэтому теперь

1354
00:48:00,720 --> 00:48:02,880
xi не равно слою xi минус 1,

1355
00:48:02,880 --> 00:48:06,400
он равен xi минус 1 плюс слой

1356
00:48:06,400 --> 00:48:08,720
xi минус 1. this  Это

1357
00:48:08,720 --> 00:48:10,559
остаточные связи,

1358
00:48:10,559 --> 00:48:13,680
гм, и интуиция подсказывает, что, как и до того, как

1359
00:48:13,680 --> 00:48:16,160
вы начали изучать что-

1360
00:48:16,160 --> 00:48:18,319
то вроде, у вас есть такое представление, что вы

1361
00:48:18,319 --> 00:48:21,760
должны изучать только то, как слой i

1362
00:48:21,760 --> 00:48:23,520
должен отличаться от слоя i минус

1363
00:48:23,520 --> 00:48:25,839
один,

1364
00:48:26,319 --> 00:48:27,599
вместо того, чтобы изучать с нуля, что он

1365
00:48:27,599 --> 00:48:30,000
должен  выглядит так, что это значение здесь,

1366
00:48:30,000 --> 00:48:31,920
слой xi минус один,

1367
00:48:31,920 --> 00:48:34,000
должно быть чем-то в некотором смысле, и

1368
00:48:34,000 --> 00:48:35,440
вам нужно узнать, чем оно отличается

1369
00:48:35,440 --> 00:48:37,599
от предыдущего слоя, это своего

1370
00:48:37,599 --> 00:48:40,640
рода хорошее индуктивное смещение, поэтому здесь вы можете

1371
00:48:40,640 --> 00:48:42,079
представить его как yo  у вас этот

1372
00:48:42,079 --> 00:48:45,359
слой x минус 1 идет на слой,

1373
00:48:45,359 --> 00:48:47,040
он также проходит и просто

1374
00:48:47,040 --> 00:48:49,599
добавляется теперь подумайте о градиентах, правильно

1375
00:48:49,599 --> 00:48:51,920
мы говорим об исчезающих градиентах,

1376
00:48:51,920 --> 00:48:53,280
это проблема,

1377
00:48:54,160 --> 00:48:56,160
вы знаете, что градиент этой

1378
00:48:56,160 --> 00:48:58,240
связи здесь прекрасен, верно  даже

1379
00:48:58,240 --> 00:49:00,079
если все насыщает

1380
00:49:00,079 --> 00:49:02,000
э-э, все ваши сигмоиды, как вы знаете, насыщаются

1381
00:49:02,000 --> 00:49:03,760
или все ваши рельсы

1382
00:49:03,760 --> 00:49:05,680
отрицательны, поэтому градиенты все равны нулю,

1383
00:49:05,680 --> 00:49:07,440
правильно, вы получаете градиенты,

1384
00:49:07,440 --> 00:49:08,800
распространяющиеся обратно через остальную часть

1385
00:49:08,800 --> 00:49:10,240
сети, в любом случае через это соединение

1386
00:49:10,240 --> 00:49:12,240
здесь, это довольно круто,

1387
00:49:12,240 --> 00:49:15,599
получается  чтобы быть очень полезным

1388
00:49:15,599 --> 00:49:18,160
и просто для быстрой визуализации,

1389
00:49:18,160 --> 00:49:19,760
этот сюжет никогда не перестает выглядеть

1390
00:49:19,760 --> 00:49:22,800
действительно очень интересно, вот это

1391
00:49:22,800 --> 00:49:24,800
своего рода

1392
00:49:24,800 --> 00:49:27,599
визуализация потерянного пейзажа, так что

1393
00:49:27,599 --> 00:49:30,319
каждая точка на 2-й плоскости

1394
00:49:30,319 --> 00:49:32,319
похожа на гм,

1395
00:49:32,319 --> 00:49:34,960
это своего рода  настройки

1396
00:49:34,960 --> 00:49:36,559
параметров вашей сети, а затем

1397
00:49:36,559 --> 00:49:39,680
ось z - это потеря сети, для

1398
00:49:39,680 --> 00:49:41,760
которой она оптимизируется, и

1399
00:49:41,760 --> 00:49:43,839
вот сетевое примечание без r  esiduals,

1400
00:49:43,839 --> 00:49:45,920
и вы стохастический градиентный спуск,

1401
00:49:45,920 --> 00:49:47,200
и вам как бы нужно найти локальный

1402
00:49:47,200 --> 00:49:48,640
минимум, и действительно сложно найти

1403
00:49:48,640 --> 00:49:50,079
хороший локальный минимум

1404
00:49:50,079 --> 00:49:53,119
um, а затем с остаточной сетью,

1405
00:49:53,119 --> 00:49:54,960
вы знаете,

1406
00:49:54,960 --> 00:49:56,400
это намного более плавно, поэтому вы можете представить,

1407
00:49:56,400 --> 00:49:57,920
как сортируется стохастический градиентный спуск

1408
00:49:57,920 --> 00:50:00,720
спуститься сюда к этому хорошему

1409
00:50:00,720 --> 00:50:03,440
очень низкому локальному минимуму, это

1410
00:50:03,440 --> 00:50:05,599
статья, в которой пытались объяснить, почему

1411
00:50:05,599 --> 00:50:08,720
остаточные связи так полезны,

1412
00:50:08,720 --> 00:50:10,160
так что это может быть интуиция, которая может

1413
00:50:10,160 --> 00:50:12,720
быть вам полезна, так что вы знаете, что

1414
00:50:12,720 --> 00:50:14,800
это так называемый  ландшафт потерь,

1415
00:50:14,800 --> 00:50:16,480
так что это остаточные

1416
00:50:16,480 --> 00:50:17,839
связи,

1417
00:50:18,720 --> 00:50:21,040
и они кажутся простыми, но многие

1418
00:50:21,040 --> 00:50:22,640
простые идеи в конечном итоге оказываются очень полезными

1419
00:50:22,640 --> 00:50:24,319
в глубоком обучении,

1420
00:50:24,319 --> 00:50:25,040
поэтому

1421
00:50:25,040 --> 00:50:27,839
в нормализации

1422
00:50:27,839 --> 00:50:30,079
слоев мы делаем что-то

1423
00:50:30,079 --> 00:50:32,000
похожее, мы пытаемся помочь сети

1424
00:50:32,000 --> 00:50:34,720
лучше обучаться  гм, но мы делаем это с помощью

1425
00:50:34,720 --> 00:50:36,880
совершенно другой интуиции,

1426
00:50:36,880 --> 00:50:38,160
поэтому

1427
00:50:38,160 --> 00:50:40,720
считается, что нормализация уровня говорит

1428
00:50:40,720 --> 00:50:42,800
в разное время в моей сети, когда

1429
00:50:42,800 --> 00:50:44,400
я ее тренирую, я делаю прямой

1430
00:50:44,400 --> 00:50:46,720
проход, там много  вариаций

1431
00:50:46,720 --> 00:50:48,800
в том, как выглядит прямой проход,

1432
00:50:48,800 --> 00:50:50,640
и многие из них неинформативны,

1433
00:50:50,640 --> 00:50:53,440
и это может повредить обучению,

1434
00:50:53,440 --> 00:50:56,960
но если мы нормализуем внутри слоя

1435
00:50:56,960 --> 00:51:00,079
до единичного среднего значения и стандартного

1436
00:51:00,079 --> 00:51:02,240
отклонения, то такой вид сокращает

1437
00:51:02,240 --> 00:51:03,760
все это на  вся эта

1438
00:51:03,760 --> 00:51:06,400
неинформативная вариация

1439
00:51:06,400 --> 00:51:08,000
и информативная вариация того,

1440
00:51:08,000 --> 00:51:09,200
как единицы отличались друг от

1441
00:51:09,200 --> 00:51:11,520
друга, сохраняется,

1442
00:51:11,520 --> 00:51:13,920
ммм, так что также думают, что

1443
00:51:13,920 --> 00:51:15,280
успех Лары Норман был

1444
00:51:15,280 --> 00:51:17,280
большим успехом нормы уровня был на

1445
00:51:17,280 --> 00:51:19,520
самом деле благодаря

1446
00:51:19,520 --> 00:51:21,839
помощи  нормализовать градиенты каждого

1447
00:51:21,839 --> 00:51:24,079
слоя, это недавняя работа,

1448
00:51:24,079 --> 00:51:26,319
поэтому давайте поговорим о том, как она реализована,

1449
00:51:26,319 --> 00:51:28,400
поэтому мы вернемся к x и не

1450
00:51:28,400 --> 00:51:30,480
будем индексировать его здесь, поэтому просто x - это некоторый

1451
00:51:30,480 --> 00:51:32,800
вектор, какой-то вектор слова в нашем

1452
00:51:32,800 --> 00:51:35,359
преобразователе.

1453
00:51:35,359 --> 00:51:36,559
собираюсь вычислить

1454
00:51:36,559 --> 00:51:37,920
оценку среднего

1455
00:51:37,920 --> 00:51:40,319
хорошо, просто суммируя скрытые

1456
00:51:40,319 --> 00:51:42,480
единицы, мы собираемся вычислить

1457
00:51:42,480 --> 00:51:44,960
оценку стандартного отклонения

1458
00:51:44,960 --> 00:51:46,640
аналогично, так, как если бы вы взяли один

1459
00:51:46,640 --> 00:51:49,119
rd-вектор, вы просто суммируете их  вы

1460
00:51:49,119 --> 00:51:51,440
вычисляете среднее значение, которое вы вычисляете, хорошо

1461
00:51:51,440 --> 00:51:52,640
ли вы оцениваете среднее значение, которое вы оцениваете

1462
00:51:52,640 --> 00:51:54,079
стандартное отклонение,

1463
00:51:54,079 --> 00:51:56,000
хорошо,

1464
00:51:56,000 --> 00:51:57,599
теперь

1465
00:51:57,599 --> 00:51:59,520
вы

1466
00:51:59,520 --> 00:52:02,240
также потенциально, и это необязательный

1467
00:52:02,240 --> 00:52:05,119
элемент изучения элементов поэлементного усиления и параметров смещения,

1468
00:52:05,119 --> 00:52:07,359
чтобы попытаться изменить масштаб,

1469
00:52:07,359 --> 00:52:09,520
если определенные скрытые единицы вроде

1470
00:52:09,520 --> 00:52:12,960
должен иметь большее значение в целом или

1471
00:52:12,960 --> 00:52:15,280
если вы знаете, что он должен быть мультипликативно

1472
00:52:15,280 --> 00:52:17,520
больше в общем, так что это

1473
00:52:17,520 --> 00:52:19,839
векторы в rd, так же как x был вектором

1474
00:52:19,839 --> 00:52:21,599
в rd,

1475
00:52:21,599 --> 00:52:23,119
а затем вот, что вычисляет нормализация уровня, у

1476
00:52:24,319 --> 00:52:26,240
вас есть ваш результат, который

1477
00:52:26,240 --> 00:52:27,680
будет просто rd

1478
00:52:27,680 --> 00:52:30,800
как ваш ввод в порядке, и вы берете

1479
00:52:30,800 --> 00:52:33,520
свой вектор x, вы

1480
00:52:33,520 --> 00:52:35,920
вычитаете среднее значение из всех из них, которые вы

1481
00:52:35,920 --> 00:52:37,920
делите на

1482
00:52:37,920 --> 00:52:40,240
стандартное отклонение,

1483
00:52:40,240 --> 00:52:41,920
о, да, извините, это не должен быть

1484
00:52:41,920 --> 00:52:44,160
квадратный корень um, а затем вы добавляете

1485
00:52:44,160 --> 00:52:46,400
небольшой эпсилон,

1486
00:52:46,400 --> 00:52:48,240
если стандарт  отклонение

1487
00:52:48,240 --> 00:52:49,839
становится очень маленьким, вы не хотите,

1488
00:52:49,839 --> 00:52:52,480
чтобы знаменатель стал

1489
00:52:52,480 --> 00:52:54,240
слишком маленьким, потому что тогда вы получаете огромные

1490
00:52:54,240 --> 00:52:55,920
числа, а затем ваша сеть

1491
00:52:55,920 --> 00:52:58,640
переходит в nan и не трогает  ну хорошо,

1492
00:52:58,640 --> 00:53:01,760
так что у вас есть какая-то толерантность тут

1493
00:53:01,760 --> 00:53:03,920
и тогда, поэтому вы нормализуете здесь и затем

1494
00:53:03,920 --> 00:53:06,079
наши поэлементные прирост и смещение теперь

1495
00:53:06,079 --> 00:53:07,839
помните, что эта дробь

1496
00:53:07,839 --> 00:53:09,680
x - вектор, все делается

1497
00:53:09,680 --> 00:53:11,680
здесь поэлементно, так что

1498
00:53:11,680 --> 00:53:12,880
это наш  d,

1499
00:53:12,880 --> 00:53:14,079
а затем у вас есть это поэлементное

1500
00:53:14,079 --> 00:53:16,079
умножение этого произведения Хадамара

1501
00:53:16,079 --> 00:53:17,839
на ваше усиление,

1502
00:53:17,839 --> 00:53:19,599
затем вы добавляете смещение,

1503
00:53:19,599 --> 00:53:21,839
необходимы ли усиление и смещение,

1504
00:53:22,880 --> 00:53:24,800
неясно, эта статья здесь предполагает, что

1505
00:53:24,800 --> 00:53:26,480
они бесполезны,

1506
00:53:26,480 --> 00:53:28,559
но они часто используются, так

1507
00:53:28,559 --> 00:53:30,960
что вроде как  инженерный вопрос на

1508
00:53:30,960 --> 00:53:32,319
этом этапе и научный вопрос,

1509
00:53:32,319 --> 00:53:34,400
можем ли мы понять, почему в

1510
00:53:34,400 --> 00:53:36,400
целом эм,

1511
00:53:36,400 --> 00:53:38,960
но да, это нормализация слоев, и в

1512
00:53:38,960 --> 00:53:41,520
конечном итоге это очень важно в

1513
00:53:41,520 --> 00:53:44,079
трансформаторах, вы удалите это, и они

1514
00:53:44,079 --> 00:53:46,720
действительно не очень хорошо тренируются,

1515
00:53:46,720 --> 00:53:49,200
хорошо, так что  это наш второй, наш

1516
00:53:49,200 --> 00:53:50,720
второй трюк,

1517
00:53:50,720 --> 00:53:53,920
третий трюк,

1518
00:53:54,079 --> 00:53:55,839
вероятно, самый простой, но это

1519
00:53:55,839 --> 00:53:58,400
полезно знать, и это просто

1520
00:53:58,400 --> 00:54:00,079
вы можете назвать это масштабируемым скалярным произведением

1521
00:54:00,079 --> 00:54:01,920
внимания,

1522
00:54:01,920 --> 00:54:03,440
потому что мы собираемся масштабировать  точечные

1523
00:54:03,440 --> 00:54:06,160
продукты вроде так хорошо, так

1524
00:54:06,160 --> 00:54:07,440
что мы собираемся сделать это у нас будет

1525
00:54:07,440 --> 00:54:09,599
эта интуиция,

1526
00:54:09,599 --> 00:54:12,079
что наша размерность d

1527
00:54:12,079 --> 00:54:14,160
в действительно больших нейронных сетях

1528
00:54:14,160 --> 00:54:16,240
станет очень большой,

1529
00:54:16,240 --> 00:54:17,680
так что, возможно, наш скрытый слой в  наш

1530
00:54:17,680 --> 00:54:19,520
трансформатор - тысяча, две

1531
00:54:19,520 --> 00:54:21,119
тысячи или три тысячи, в любом случае

1532
00:54:21,119 --> 00:54:23,520
он становится большим,

1533
00:54:23,520 --> 00:54:25,440
и когда размерность становится

1534
00:54:25,440 --> 00:54:27,440
большой, точечные произведения между векторами

1535
00:54:27,440 --> 00:54:29,440
имеют тенденцию становиться большими,

1536
00:54:29,440 --> 00:54:31,520
поэтому, например, если вы знаете, возьмите

1537
00:54:31,520 --> 00:54:33,920
скалярное произведение между двумя случайными векторами

1538
00:54:33,920 --> 00:54:35,839
в rd

1539
00:54:35,839 --> 00:54:37,680
он растет довольно быстро, их скалярное произведение

1540
00:54:37,680 --> 00:54:39,680
растет довольно быстро, теперь векторы

1541
00:54:39,680 --> 00:54:41,839
случайны в преобразователях, ну, они не являются

1542
00:54:41,839 --> 00:54:43,599
однородными, случайными, но вы знаете, что можете

1543
00:54:43,599 --> 00:54:44,559
себе представить, что существует много

1544
00:54:44,559 --> 00:54:46,480
вариаций, и в целом, поскольку

1545
00:54:46,480 --> 00:54:48,079
размерность растет, все эти скалярные

1546
00:54:48,079 --> 00:54:50,000
произведения  становится довольно большим,

1547
00:54:50,000 --> 00:54:52,000
и это может стать проблемой по

1548
00:54:52,000 --> 00:54:54,000
следующей причине: мы берем все

1549
00:54:54,000 --> 00:54:56,559
эти точечные продукты напрямую и

1550
00:54:56,559 --> 00:54:58,960
помещаем их в softmax,

1551
00:54:58,960 --> 00:55:01,040
так что если есть  вариации

1552
00:55:01,040 --> 00:55:02,319
в скалярных произведениях, а некоторые из них

1553
00:55:02,319 --> 00:55:03,520
очень велики,

1554
00:55:03,520 --> 00:55:06,559
тогда softmax может стать очень пиковым,

1555
00:55:06,559 --> 00:55:08,720
вкладывая большую часть или вы знаете, да,

1556
00:55:08,720 --> 00:55:10,960
большую часть его вероятностной массы на небольшое

1557
00:55:10,960 --> 00:55:12,480
количество вещей, что делает

1558
00:55:12,480 --> 00:55:14,400
градиенты маленькими для всего остального,

1559
00:55:14,400 --> 00:55:16,640
эффективно  правильно, потому что

1560
00:55:16,640 --> 00:55:18,799
softmax пытается быть в порядке, это мягкий аргумент max,

1561
00:55:18,799 --> 00:55:20,480
так что он вроде говорит, какой

1562
00:55:20,480 --> 00:55:23,040
из них похож на max, или вы знаете

1563
00:55:23,040 --> 00:55:25,200
вес этого типа относительно того, насколько

1564
00:55:25,200 --> 00:55:26,319
они близки к максимальному значению

1565
00:55:26,319 --> 00:55:28,720
функции um и  поэтому, если некоторые из них

1566
00:55:28,720 --> 00:55:30,880
очень-очень большие, вы как бы просто

1567
00:55:30,880 --> 00:55:32,720
обнуляете связи со всем, на что

1568
00:55:32,720 --> 00:55:34,240
не обращают внимания, что имеет низкое

1569
00:55:34,240 --> 00:55:36,470
распределение вероятностей и гм

1570
00:55:36,470 --> 00:55:37,920
[Музыка],

1571
00:55:37,920 --> 00:55:40,160
и тогда они не получают градиентов,

1572
00:55:40,160 --> 00:55:41,680
и вот размер напряжения

1573
00:55:41,680 --> 00:55:43,839
операция, которую мы видели, хорошо, я взял

1574
00:55:43,839 --> 00:55:45,440
это вариант с несколькими головами прямо здесь,

1575
00:55:45,440 --> 00:55:46,799
потому что у нас есть индексы на

1576
00:55:46,799 --> 00:55:51,440
выходе у меня есть индексы на qk и v,

1577
00:55:51,440 --> 00:55:53,440
и все, что я собираюсь сделать, это я  собираюсь

1578
00:55:53,440 --> 00:55:55,359
сказать хорошо ты

1579
00:55:55,359 --> 00:55:56,480
знаешь й  Я собираюсь расставить точки

1580
00:55:56,480 --> 00:55:58,880
вместе, знаете ли вы, что векторы

1581
00:55:58,880 --> 00:56:02,079
размерности d над h,

1582
00:56:02,079 --> 00:56:03,280
из-за

1583
00:56:03,280 --> 00:56:06,000
снова многоголого внимания,

1584
00:56:06,000 --> 00:56:08,400
и, чтобы не дать им вырасти,

1585
00:56:08,400 --> 00:56:10,160
скалярные произведения станут слишком большими,

1586
00:56:10,160 --> 00:56:11,920
я просто собираюсь  чтобы разделить

1587
00:56:11,920 --> 00:56:13,680
все мои оценки,

1588
00:56:13,680 --> 00:56:18,319
так что помните, здесь xqk top x top находится

1589
00:56:18,319 --> 00:56:20,400
на t матрице оценок, вы собираетесь

1590
00:56:20,400 --> 00:56:23,359
разделить их все на d на h,

1591
00:56:23,359 --> 00:56:27,119
и по мере того, как d растет, d на h растет вправо,

1592
00:56:27,119 --> 00:56:29,359
и поэтому ваши точечные продукты не  не

1593
00:56:29,359 --> 00:56:32,000
растут, и это в конечном итоге оказывается также полезным.

1594
00:56:35,680 --> 00:56:38,000
Хорошо,

1595
00:56:38,559 --> 00:56:41,200
любые вопросы,

1596
00:56:43,040 --> 00:56:45,200
да, Джон, не могли бы

1597
00:56:45,200 --> 00:56:47,040
вы задать интересный вопрос,

1598
00:56:47,040 --> 00:56:49,359
когда вы играете аккорд b.

1599
00:56:52,799 --> 00:56:54,720
первый слой, или вы делаете

1600
00:56:54,720 --> 00:56:57,200
маскировку также в заметках слоя,

1601
00:56:57,200 --> 00:56:59,599
да, хорошо, хорошо, так что

1602
00:56:59,599 --> 00:57:02,079
если бы мы

1603
00:57:03,920 --> 00:57:06,880
выполняли маскировку только на первом слое, мы

1604
00:57:06,880 --> 00:57:08,559
бы получили утечку информации в более

1605
00:57:08,559 --> 00:57:09,920
поздних слоях,

1606
00:57:09,920 --> 00:57:12,720
поэтому, если мы посмотрим на

1607
00:57:12,720 --> 00:57:15,520
этот

1608
00:57:15,520 --> 00:57:16,880
возглас,

1609
00:57:16,880 --> 00:57:18,400
мы, если мы  должны были снова взглянуть на эту диаграмму,

1610
00:57:18,400 --> 00:57:20,160
так что вот первый

1611
00:57:20,160 --> 00:57:22,079
слоя декодера, и мы сказали, что есть

1612
00:57:22,079 --> 00:57:23,839
правильное маскирование, и вы можете смотреть на

1613
00:57:23,839 --> 00:57:25,599
любое из состояний кодировщика, и вы

1614
00:57:25,599 --> 00:57:28,000
можете смотреть только на предыдущие слова

1615
00:57:28,000 --> 00:57:30,880
в декодере на втором слое, если

1616
00:57:30,880 --> 00:57:32,240
я  внезапно позволил взглянуть на

1617
00:57:32,240 --> 00:57:34,400
все будущие слова сейчас,

1618
00:57:34,400 --> 00:57:35,680
эй, хотя я этого не делал в первом

1619
00:57:35,680 --> 00:57:37,119
слое, это так же хорошо, что я могу

1620
00:57:37,119 --> 00:57:38,880
во втором слое, и поэтому я могу просто научиться

1621
00:57:38,880 --> 00:57:40,799
смотреть прямо на то, что

1622
00:57:40,799 --> 00:57:42,319
должно быть мое слово  так что каждый отдельный

1623
00:57:42,319 --> 00:57:45,040
слой декодера должен иметь эту маскировку, иначе

1624
00:57:45,040 --> 00:57:46,079
это будет

1625
00:57:46,079 --> 00:57:47,520
спорным, как будто вы не замаскировали его

1626
00:57:47,520 --> 00:57:50,319
вообще эффективно,

1627
00:57:51,839 --> 00:57:54,839
спасибо,

1628
00:58:03,760 --> 00:58:07,520
хорошо, так что масштабируйте точечный продукт в сумке, которую

1629
00:58:07,520 --> 00:58:09,920
мы получили, так что давайте вернемся к нашему

1630
00:58:09,920 --> 00:58:12,559
Полная структура декодера кодировщика трансформатора

1631
00:58:12,559 --> 00:58:14,160
мы рассмотрели

1632
00:58:14,160 --> 00:58:15,839
сами блоки

1633
00:58:15,839 --> 00:58:18,640
кодировщика, так что давайте немного расширим один из

1634
00:58:18,640 --> 00:58:21,040
этих масштабов и улучшим, и

1635
00:58:21,040 --> 00:58:22,400
у нас есть наши представления позиций встраивания слов,

1636
00:58:22,400 --> 00:58:25,040
и сначала мы рассмотрим

1637
00:58:25,040 --> 00:58:27,119
это многоголовым вниманием,

1638
00:58:27,119 --> 00:58:28,559
так что  мы видели,

1639
00:58:28,559 --> 00:58:30,880
что пропускаем его через остаточный слой  и

1640
00:58:30,880 --> 00:58:32,640
норма слоя,

1641
00:58:32,640 --> 00:58:34,640
так что у вас есть слово, встраивающее

1642
00:58:34,640 --> 00:58:36,799
представления поршня, проходящие через

1643
00:58:36,799 --> 00:58:39,040
остаточное соединение здесь,

1644
00:58:39,040 --> 00:58:40,319
а также проходящие через многоголовое

1645
00:58:40,319 --> 00:58:41,359
задержание,

1646
00:58:41,359 --> 00:58:43,440
добавьте их норму уровня,

1647
00:58:43,440 --> 00:58:45,839
затем вы поместите результат этого

1648
00:58:45,839 --> 00:58:48,000
через сеть с прямой

1649
00:58:48,000 --> 00:58:49,599
связью, а там должен быть  стрелка между

1650
00:58:49,599 --> 00:58:52,480
прямой связью и следующим остаточным слоем,

1651
00:58:52,480 --> 00:58:54,000
но выход этого остатка и

1652
00:58:54,000 --> 00:58:56,559
нормы слоя добавляется к этому остатку

1653
00:58:56,559 --> 00:58:58,079
и норме слоя вместе с

1654
00:58:58,079 --> 00:59:00,000
выходом прямой связи,

1655
00:59:00,000 --> 00:59:01,599
а затем выход этого остатка и

1656
00:59:01,599 --> 00:59:03,280
нормы слоя является выходом

1657
00:59:03,280 --> 00:59:05,200
блок кодировщика трансформатора,

1658
00:59:05,200 --> 00:59:06,640
поэтому, когда у нас был

1659
00:59:06,640 --> 00:59:08,640
каждый из этих кодировщиков

1660
00:59:08,640 --> 00:59:10,079
внутри, каждый из них был именно

1661
00:59:10,079 --> 00:59:11,440
таким, и мы видели все эти строительные

1662
00:59:11,440 --> 00:59:13,280
блоки раньше,

1663
00:59:13,280 --> 00:59:16,400
и это многоголовое

1664
00:59:16,720 --> 00:59:18,880
масштабируемое скалярное произведение. Внимание, я опускаю

1665
00:59:18,880 --> 00:59:20,960
масштабированное слово,

1666
00:59:20,960 --> 00:59:22,880
но да, так что это  это блок,

1667
00:59:22,880 --> 00:59:24,720
и вы любопытно замечаете, как вы

1668
00:59:24,720 --> 00:59:26,799
выполняете остаточную норму и норму слоев после

1669
00:59:26,799 --> 00:59:28,240
начального мультиголовка напряжения, а

1670
00:59:28,240 --> 00:59:31,960
также после прямой связи.

1671
00:59:32,000 --> 00:59:34,720
Итак, каждый из них просто

1672
00:59:34,720 --> 00:59:37,040
идентичен, разные параметры

1673
00:59:37,040 --> 00:59:39,440
для разных слоев, но те же самые

1674
00:59:39,440 --> 00:59:41,040
вещи, которые мы видели

1675
00:59:41,040 --> 00:59:42,559
сейчас, давайте посмотрим на блок декодера трансформатора,

1676
00:59:45,200 --> 00:59:48,000
так что это на самом деле более сложный, в

1677
00:59:48,000 --> 00:59:49,920
частности, у вас есть эта замаскированная

1678
00:59:49,920 --> 00:59:51,760
многоголовое самовнимание,

1679
00:59:51,760 --> 00:59:53,119
и теперь помните, что это не только

1680
00:59:53,119 --> 00:59:54,240
для первого, это для всех

1681
00:59:54,240 --> 00:59:55,599
блоков трансформатора, поэтому у нас есть массовое

1682
00:59:55,599 --> 00:59:57,040
многоголовое самовнимание, когда мы

1683
00:59:57,040 --> 00:59:58,720
не можем смотреть в будущее, потому что мы

1684
00:59:58,720 --> 01:00:01,200
добавили отрицательную бесконечность к отрицательной

1685
01:00:01,200 --> 01:00:04,160
бесконечности к остаточной оценке сродства

1686
01:00:04,160 --> 01:00:06,240
и норме слоя, как мы это делали для

1687
01:00:06,240 --> 01:00:08,319
э-э для кодировщика,

1688
01:00:08,319 --> 01:00:10,400
теперь у нас есть перекрестное внимание с несколькими головками,

1689
01:00:10,400 --> 01:00:11,839
поэтому это соединение с кодировщиком трансформатора

1690
01:00:12,880 --> 01:00:14,480
это на самом деле очень похоже на то, что мы видели

1691
01:00:14,480 --> 01:00:15,920
вы знаете, что

1692
01:00:16,559 --> 01:00:18,240
до сих пор во внимании мы обращаемся

1693
01:00:18,240 --> 01:00:21,760
от декодера к кодеру,

1694
01:00:21,760 --> 01:00:23,440
поэтому на самом деле у нас в каждом

1695
01:00:23,440 --> 01:00:26,079
блоке декодера трансформатора у нас есть две разные

1696
01:00:26,079 --> 01:00:28,480
функции внимания,

1697
01:00:28,480 --> 01:00:31,200
так что мы делаем перекрестное внимание,

1698
01:00:31,200 --> 01:00:32,799
мы добавляем resu  lt остатка и

1699
01:00:32,799 --> 01:00:34,960
нормы слоя

1700
01:00:34,960 --> 01:00:37,280
до следующего остатка и нормы уровня uh

1701
01:00:37,280 --> 01:00:38,480
вместе с нормой перекрестного внимания с несколькими головками.

1702
01:00:48,240 --> 01:00:50,480
приближается, так что вы

1703
01:00:50,480 --> 01:00:52,559
знаете, что xi минус единица - это остаточная близкая к

1704
01:00:52,559 --> 01:00:54,880
норме, здесь переходит в эту вместе

1705
01:00:54,880 --> 01:00:57,040
с прямой связью, и поэтому вы можете думать об

1706
01:00:57,040 --> 01:00:58,480
остатке, и их норма приходит

1707
01:00:58,480 --> 01:00:59,680
после каждого из интересных вещей, которые

1708
01:00:59,680 --> 01:01:00,799
мы делаем правильно, мы '  повторяя одну

1709
01:01:00,799 --> 01:01:02,480
интересную вещь здесь, вы знаете, что

1710
01:01:02,480 --> 01:01:04,640
многоголовая маска самовнимание

1711
01:01:04,640 --> 01:01:06,319
перекрестное внимание после каждого из них делает

1712
01:01:06,319 --> 01:01:07,839
остаток и норма слоя помогает

1713
01:01:07,839 --> 01:01:11,440
градиентам проходить

1714
01:01:15,520 --> 01:01:18,240
и т.  Единственное, чего

1715
01:01:18,240 --> 01:01:20,799
мы пока не видели в этой лекции,

1716
01:01:20,799 --> 01:01:24,480
- это перекрестное внимание с несколькими головами,

1717
01:01:24,480 --> 01:01:28,079
и я хочу, чтобы я хотел пройтись по нему эээ,

1718
01:01:28,079 --> 01:01:30,160
это те же уравнения,

1719
01:01:30,160 --> 01:01:33,040
что и эм,

1720
01:01:33,040 --> 01:01:34,640
как и

1721
01:01:34,640 --> 01:01:36,400
многоголовый я  -внимание, но

1722
01:01:36,400 --> 01:01:37,760
входные данные поступают из разных мест,

1723
01:01:37,760 --> 01:01:40,559
и поэтому я хочу быть точным,

1724
01:01:40,559 --> 01:01:42,720
поэтому давайте рассмотрим

1725
01:01:42,720 --> 01:01:45,920
детали перекрестного внимания, так что

1726
01:01:45,920 --> 01:01:47,680
правильное напоминание о самом внимании заключается в том, что когда

1727
01:01:47,680 --> 01:01:49,599
мы берем ключи, запросы

1728
01:01:49,599 --> 01:01:50,799
и  значения внимания из того

1729
01:01:50,799 --> 01:01:53,520
же источника информации, как, например, одно и то же

1730
01:01:53,520 --> 01:01:55,680
предложение,

1731
01:01:55,680 --> 01:01:57,760
и на прошлой неделе мы видели, как внимание

1732
01:01:57,760 --> 01:01:59,839
от декодера переходит к кодировщику, так что

1733
01:01:59,839 --> 01:02:01,280
это будет похоже,

1734
01:02:01,280 --> 01:02:03,119
давайте сделаем несколько других обозначений, так что

1735
01:02:03,119 --> 01:02:05,200
у нас будет h1 для

1736
01:02:05,200 --> 01:02:07,920
ht  выходные векторы

1737
01:02:07,920 --> 01:02:11,039
от кодировщика бывшего трансформатора,

1738
01:02:11,039 --> 01:02:13,440
которые все xi в rd, теперь помните, что

1739
01:02:13,440 --> 01:02:16,079
это последний кодировщик трансформатора здесь,

1740
01:02:16,079 --> 01:02:18,480
вы никогда не обращаетесь к средним

1741
01:02:18,480 --> 01:02:20,000
блокам кодировщика, это выход

1742
01:02:20,000 --> 01:02:21,520
последнего блока кодировщика,

1743
01:02:21,520 --> 01:02:22,960
поэтому эти выходные векторы от

1744
01:02:22,960 --> 01:02:25,680
последнего кодировщика трансформатора uh  block, и

1745
01:02:25,680 --> 01:02:29,039
теперь у нас есть z1 для zt входных векторов

1746
01:02:29,039 --> 01:02:31,119
из декодера трансформатора, так

1747
01:02:31,119 --> 01:02:34,960
что, возможно, вы знаете, что вход -

1748
01:02:34,960 --> 01:02:36,240
это вложения слов плюс есть

1749
01:02:36,240 --> 01:02:38,079
представления положения

1750
01:02:38,079 --> 01:02:40,240
uh или r  Право, это на самом деле

1751
01:02:40,240 --> 01:02:42,480
выход предыдущего декодера трансформатора, так что

1752
01:02:42,480 --> 01:02:43,680
мы будем входами для

1753
01:02:43,680 --> 01:02:46,079
следующего,

1754
01:02:46,960 --> 01:02:49,440
так что да, у нас есть z один на zt,

1755
01:02:49,440 --> 01:02:50,559
и мы снова позволяем им иметь одинаковую

1756
01:02:50,559 --> 01:02:52,880
длину последовательности t и t  просто для

1757
01:02:52,880 --> 01:02:54,160
простоты,

1758
01:02:54,160 --> 01:02:57,520
это также векторы zi в нашем d,

1759
01:02:57,520 --> 01:02:59,440
а затем ключи и запрос, извините,

1760
01:02:59,440 --> 01:03:01,680
что ключи и значения все взяты

1761
01:03:01,680 --> 01:03:03,200
из кодировщика

1762
01:03:03,200 --> 01:03:04,960
правильно, поэтому, когда мы говорим о

1763
01:03:04,960 --> 01:03:06,319
самовнимании, когда мы говорим о

1764
01:03:06,319 --> 01:03:08,400
внимании как  позволяя нам сортировать

1765
01:03:08,400 --> 01:03:10,400
доступ к памяти

1766
01:03:10,400 --> 01:03:12,880
прямо, память uh - это

1767
01:03:12,880 --> 01:03:14,319
своего рода то, что кодируют векторы значений,

1768
01:03:14,319 --> 01:03:17,760
и способ, которым значения

1769
01:03:17,760 --> 01:03:19,920
вроде как индексируются или могут быть

1770
01:03:19,920 --> 01:03:21,920
доступны, через ключи,

1771
01:03:21,920 --> 01:03:24,400
а затем значение, а затем

1772
01:03:24,400 --> 01:03:26,319
запросы  вы знаете,

1773
01:03:26,319 --> 01:03:27,440
что вы используете, чтобы попытаться найти

1774
01:03:27,440 --> 01:03:29,280
что-то правильное, поэтому мы смотрим

1775
01:03:29,280 --> 01:03:31,359
на кодировщик как на память, и мы используем

1776
01:03:31,359 --> 01:03:33,359
ключи от декодера, чтобы выяснить,

1777
01:03:33,359 --> 01:03:35,920
где искать каждый,

1778
01:03:35,920 --> 01:03:38,400
так что наглядно

1779
01:03:38,400 --> 01:03:40,400
снова мы можем посмотреть, как перекрестное внимание

1780
01:03:40,400 --> 01:03:42,000
вычисляется в матрицах, как мы делали для

1781
01:03:42,000 --> 01:03:43,359
самовнимания,

1782
01:03:43,359 --> 01:03:45,039
поэтому у нас здесь то же самое, прежде чем

1783
01:03:45,039 --> 01:03:47,200
у нас было x, теперь у нас есть h, это

1784
01:03:47,200 --> 01:03:49,760
векторы кодировщика, это будет rt

1785
01:03:49,760 --> 01:03:52,000
на d,

1786
01:03:52,000 --> 01:03:54,079
аналогично у нас есть z,

1787
01:03:54,079 --> 01:03:55,280
обратите внимание, у нас есть два из них  раньше,

1788
01:03:55,280 --> 01:03:57,200
раньше у нас было только x, у нас было x,

1789
01:03:57,200 --> 01:03:59,280
потому что x должен был быть для ключей,

1790
01:03:59,280 --> 01:04:00,960
запросы и значения, теперь у нас есть h

1791
01:04:00,960 --> 01:04:04,400
и z, оба находятся в rt через d,

1792
01:04:04,400 --> 01:04:06,720
и

1793
01:04:06,720 --> 01:04:09,440
выход будет хорошо, вы возьмете свой

1794
01:04:09,440 --> 01:04:10,160
z

1795
01:04:10,160 --> 01:04:11,520
для

1796
01:04:11,520 --> 01:04:13,200
запросы вправо zs, которые мы умножаем на

1797
01:04:13,200 --> 01:04:16,240
запросы, которые вы берете с h для ключей

1798
01:04:16,240 --> 01:04:19,039
и с h для v,

1799
01:04:19,039 --> 01:04:21,599
поэтому вы пытаетесь взять ключ,

1800
01:04:21,599 --> 01:04:23,760
ключевые точечные произведения запроса все t возведены в

1801
01:04:23,760 --> 01:04:26,720
квадрат um в одном умножении матрицы,

1802
01:04:26,720 --> 01:04:29,119
поэтому фиолетовый говорит  это исходит от

1803
01:04:29,839 --> 01:04:33,599
декодера, коричневый говорит или ish

1804
01:04:33,599 --> 01:04:35,359
говорит, что он исходит от

1805
01:04:35,359 --> 01:04:36,960
кодировщика,

1806
01:04:36,960 --> 01:04:39,599
теперь у вас есть точечные продукты,

1807
01:04:39,599 --> 01:04:41,680
soft max, как и раньше, и теперь

1808
01:04:41,680 --> 01:04:43,599
ваши значения также поступают из

1809
01:04:43,599 --> 01:04:45,440
кодировщика, так что

1810
01:04:45,440 --> 01:04:47,920
снова та же операция  разные источники

1811
01:04:47,920 --> 01:04:50,319
для входов,

1812
01:04:50,319 --> 01:04:51,599
и теперь у вас есть лет  ur вывод, который

1813
01:04:51,599 --> 01:04:53,839
снова является просто средним векторов значений

1814
01:04:53,839 --> 01:04:56,720
от кодировщика.

1815
01:04:58,000 --> 01:04:59,520
hv среднее значение определяется вашими

1816
01:04:59,520 --> 01:05:01,839
весами.

1817
01:05:02,000 --> 01:05:04,559
Хорошо, поэтому результаты с трансформаторами.

1818
01:05:04,559 --> 01:05:06,880
Результаты с трансформаторами.

1819
01:05:13,359 --> 01:05:16,559
и вы знаете,

1820
01:05:16,559 --> 01:05:18,640
как это работает, это работает очень хорошо,

1821
01:05:18,640 --> 01:05:19,839
так

1822
01:05:19,839 --> 01:05:21,440
что это куча систем машинного перевода,

1823
01:05:21,440 --> 01:05:23,039
которых не было, когда исходное

1824
01:05:23,039 --> 01:05:24,559
внимание - это все, что вам нужно,

1825
01:05:24,559 --> 01:05:26,000
бумага для трансформаторов вышла,

1826
01:05:26,000 --> 01:05:26,960
и

1827
01:05:26,960 --> 01:05:29,119
сначала вы увидели, что трансформаторы

1828
01:05:29,119 --> 01:05:30,720
получают действительно хорошие синие оценки, так что это

1829
01:05:30,720 --> 01:05:32,240
находится на семинаре по машинному

1830
01:05:32,240 --> 01:05:35,119
переводу 2014

1831
01:05:36,640 --> 01:05:40,319
наборы тестов для английского немецкого и английского французского, вы получите более высокие баллы синего цвета,

1832
01:05:40,319 --> 01:05:41,920
что означает лучший перевод. Обратите

1833
01:05:41,920 --> 01:05:43,280
внимание, как наши баллы синего цвета в этом

1834
01:05:43,280 --> 01:05:45,599
выше, чем в задании,

1835
01:05:45,599 --> 01:05:48,160
например, здесь на четыре намного больше обучающих данных,

1836
01:05:48,160 --> 01:05:49,599
но не только  Вы получаете лучшие

1837
01:05:49,599 --> 01:05:52,160
оценки синих эм, у вас также были более

1838
01:05:52,160 --> 01:05:53,680
эффективные тренировки,

1839
01:05:53,680 --> 01:05:56,000
и у нас было много трюков  ks,

1840
01:05:56,000 --> 01:05:57,680
которые пошли на то, чтобы обучение работало

1841
01:05:57,680 --> 01:05:59,280
лучше, так что у вас здесь более эффективное

1842
01:05:59,280 --> 01:06:01,520
обучение,

1843
01:06:01,520 --> 01:06:03,200
хорошо, так что это хороший результат, который был

1844
01:06:03,200 --> 01:06:04,640
в исходной статье,

1845
01:06:04,640 --> 01:06:07,839
вы знаете, что в прошлом есть

1846
01:06:07,839 --> 01:06:10,000
ряд интересных результатов,

1847
01:06:10,000 --> 01:06:11,520
резюмирование - один из них, так что

1848
01:06:11,520 --> 01:06:13,920
вот результат резюмирования,

1849
01:06:13,920 --> 01:06:15,520
это своего рода часть более крупной

1850
01:06:15,520 --> 01:06:17,760
системы реферирования, но вы знаете,

1851
01:06:17,760 --> 01:06:19,440
что мне понравилась эта таблица, потому что у вас есть

1852
01:06:19,440 --> 01:06:20,640
своего рода стремление к поиску с вниманием,

1853
01:06:20,640 --> 01:06:22,559
которое мы видели раньше, и в ней уменьшалось

1854
01:06:22,559 --> 01:06:24,000
недоумение, тем лучше, если

1855
01:06:24,000 --> 01:06:26,640
недоумение выше  лучше с румянцем

1856
01:06:26,640 --> 01:06:29,440
в этом наборе данных суммы вики, а затем вроде

1857
01:06:31,280 --> 01:06:33,440
как кучка моделей трансформаторов, которые они пробовали,

1858
01:06:33,440 --> 01:06:35,039
и в какой-то момент они

1859
01:06:35,039 --> 01:06:37,200
становятся трансформаторами полностью вниз,

1860
01:06:37,200 --> 01:06:38,960
и своего рода старый стандарт rnn вроде как

1861
01:06:38,960 --> 01:06:40,640
выпадает из практики

1862
01:06:40,640 --> 01:06:43,039
гм, и на самом деле очень скоро правые

1863
01:06:43,039 --> 01:06:44,720
трансформаторы стали доминирующими по

1864
01:06:44,720 --> 01:06:46,559
совершенно другой причине, которая

1865
01:06:46,559 --> 01:06:48,960
больше связана с их параллелизуемостью,

1866
01:06:48,960 --> 01:06:51,200
потому что они позволяют вам предварительно тренироваться на

1867
01:06:51,200 --> 01:06:53,200
ю  Очень быстро собрать тонну данных,

1868
01:06:54,880 --> 01:06:57,280
и это сделало их стандартом де-факто,

1869
01:06:57,280 --> 01:06:59,359
поэтому в последнее время появилось много результатов

1870
01:06:59,359 --> 01:07:01,359
с трансформаторами, включая

1871
01:07:01,359 --> 01:07:02,720
предварительное обучение, и я вроде как

1872
01:07:02,720 --> 01:07:04,319
намеренно исключаю их

1873
01:07:04,319 --> 01:07:06,000
из этой лекции, чтобы вы перешли

1874
01:07:06,000 --> 01:07:07,920
к следующей.  читать лекции, говорить и узнавать

1875
01:07:07,920 --> 01:07:09,280
о предварительном обучении,

1876
01:07:09,280 --> 01:07:10,720
но есть популярный совокупный

1877
01:07:10,720 --> 01:07:12,160
тест, который включает в себя кучу очень

1878
01:07:12,160 --> 01:07:14,480
сложных задач и говорит, что

1879
01:07:14,480 --> 01:07:15,920
вы хорошо справляетесь со всеми из них, если хотите получить

1880
01:07:15,920 --> 01:07:18,400
высокие баллы в нашей таблице лидеров, и

1881
01:07:18,400 --> 01:07:19,599
вы знаете названия этих моделей  вы

1882
01:07:19,599 --> 01:07:21,359
можете поискать, если вам интересно, но

1883
01:07:21,359 --> 01:07:22,960
все они основаны на трансформаторе после

1884
01:07:22,960 --> 01:07:24,480
определенных точек тест называется

1885
01:07:24,480 --> 01:07:26,480
клей, у него есть преемник, называемый

1886
01:07:26,480 --> 01:07:29,359
суперклей, все это просто трансформаторы

1887
01:07:29,359 --> 01:07:31,520
после определенного периода времени

1888
01:07:32,799 --> 01:07:35,119
отчасти из-за их предтренировочных

1889
01:07:35,119 --> 01:07:37,039
способностей

1890
01:07:37,039 --> 01:07:39,359
хорошо,

1891
01:07:39,599 --> 01:07:42,240
отлично, так что мы обсудим предварительные тренировки больше

1892
01:07:42,240 --> 01:07:43,039
в

1893
01:07:43,039 --> 01:07:44,880
четверг,

1894
01:07:46,079 --> 01:07:48,559
и наш наш трансформатор -

1895
01:07:48,559 --> 01:07:50,799
это как то, как мы описали

1896
01:07:50,799 --> 01:07:52,880
внимание, это все, что вам нужно, бумага, так что

1897
01:07:52,880 --> 01:07:54,880
преобразование  r кодировщик декодер, который мы видели, был взят

1898
01:07:54,880 --> 01:07:59,680
из этой статьи, и в какой-то момент

1899
01:07:59,680 --> 01:08:01,280
вы знаете, что мы хотим создать новые системы,

1900
01:08:01,280 --> 01:08:02,799
какие есть некоторые недостатки, и мы

1901
01:08:02,799 --> 01:08:03,920
уже начали, люди уже

1902
01:08:03,920 --> 01:08:04,960
начали создавать варианты

1903
01:08:04,960 --> 01:08:07,039
трансформаторов, в которые вы войдете сегодня,

1904
01:08:07,039 --> 01:08:07,920
и

1905
01:08:07,920 --> 01:08:09,520
вы знаете  у него определенно есть проблемы, над которыми

1906
01:08:09,520 --> 01:08:12,799
мы можем попытаться поработать, поэтому

1907
01:08:13,760 --> 01:08:15,599
я также могу задать вопрос, если кто-то

1908
01:08:15,599 --> 01:08:18,480
захочет задать один,

1909
01:08:18,719 --> 01:08:20,880
я имею в виду, что это немного назад, что то,

1910
01:08:20,880 --> 01:08:23,600
о чем было несколько вопросов,

1911
01:08:23,600 --> 01:08:26,238
было скалярным произведением масштаба,

1912
01:08:26,238 --> 01:08:29,279
и вопросы включали,

1913
01:08:29,279 --> 01:08:32,479
почему квадратный корень  d, разделенного на h,

1914
01:08:32,479 --> 01:08:35,679
в отличие от просто d, разделенного на h, или любой

1915
01:08:35,679 --> 01:08:39,120
другой функции d, разделенной на h, а

1916
01:08:39,120 --> 01:08:41,839
еще одна была

1917
01:08:41,839 --> 01:08:43,120
um

1918
01:08:43,120 --> 01:08:44,080
that

1919
01:08:44,080 --> 01:08:45,839
um

1920
01:08:45,839 --> 01:08:48,479
like, почему вам это вообще нужно, учитывая,

1921
01:08:48,479 --> 01:08:50,799
что позже вы собираетесь использовать норму слоя

1922
01:08:53,198 --> 01:08:54,479
wow  второй вопрос действительно

1923
01:08:54,479 --> 01:08:55,600
интересен и не тот, о котором я

1924
01:08:55,600 --> 01:08:58,080
думал раньше,

1925
01:08:59,920 --> 01:09:01,359
так что даже если отдельные

1926
01:09:02,479 --> 01:09:04,479
компоненты маленькие, давайте начнем со второго

1927
01:09:04,479 --> 01:09:06,080
вопроса, почему это имеет значение, даже если

1928
01:09:06,080 --> 01:09:08,399
вы собираетесь использовать норму слоя um

1929
01:09:08,399 --> 01:09:10,319
you k  теперь, если lara normal усредняет

1930
01:09:10,319 --> 01:09:12,080
все, скажем, сделайте это единичным стандартным

1931
01:09:12,080 --> 01:09:15,198
отклонением и, и, ну, и значит,

1932
01:09:15,198 --> 01:09:17,279
тогда на самом деле ничего

1933
01:09:17,279 --> 01:09:19,439
не будет слишком маленьким в этих векторах,

1934
01:09:19,439 --> 01:09:22,238
поэтому, когда у вас очень очень большой

1935
01:09:22,238 --> 01:09:25,120
вектор, все с вещами, которые не

1936
01:09:25,120 --> 01:09:26,950
слишком маленький да

1937
01:09:26,950 --> 01:09:29,120
[Музыка] у

1938
01:09:29,120 --> 01:09:31,198
вас все еще будет

1939
01:09:32,238 --> 01:09:33,520
норма

1940
01:09:33,520 --> 01:09:35,759
увеличения точечных продуктов,

1941
01:09:35,759 --> 01:09:36,799
я думаю,

1942
01:09:36,799 --> 01:09:38,000
я думаю, что это хороший вопрос, я не

1943
01:09:38,000 --> 01:09:40,080
думал об этом слишком много, это

1944
01:09:40,080 --> 01:09:41,359
мой нестандартный ответ,

1945
01:09:41,359 --> 01:09:43,600
но о нем стоит подумать подробнее

1946
01:09:43,600 --> 01:09:45,759
i  Думаю, ответ заключается в том,

1947
01:09:45,759 --> 01:09:47,920
что вы получаете эффект потери

1948
01:09:47,920 --> 01:09:51,439
динамического диапазона по мере того, как вещи становятся длиннее,

1949
01:09:51,439 --> 01:09:54,560
которые в любом случае должны произойти,

1950
01:09:54,560 --> 01:09:57,679
и leia norm не может исправить, что это

1951
01:09:57,679 --> 01:10:00,000
как бы происходит слишком поздно,

1952
01:10:00,000 --> 01:10:00,960
и,

1953
01:10:00,960 --> 01:10:04,159
следовательно, вы получаете, выполняя масштабирование,

1954
01:10:04,159 --> 01:10:05,600
я думаю  так что,

1955
01:10:05,600 --> 01:10:07,040
но я думаю, что да, я думаю,

1956
01:10:07,040 --> 01:10:09,040
стоит подумать больше, почему квадратный

1957
01:10:09,040 --> 01:10:10,880
корень ммм

1958
01:10:10,880 --> 01:10:13,440
хорошо, давайте посмотрим, что нормы скалярного

1959
01:10:13,440 --> 01:10:15,520
произведения растут с

1960
01:10:15,520 --> 01:10:17,120
o из d,

1961
01:10:17,120 --> 01:10:19,600
и поэтому, когда вы извлекаете квадратный корень из единицы,

1962
01:10:19,600 --> 01:10:21,600
нет, я думаю, квадратный корень масштабируется с

1963
01:10:21,600 --> 01:10:23,120
корнем o  ди  не могу вспомнить, есть

1964
01:10:23,120 --> 01:10:24,400
небольшая заметка во внимании - это все, что вам

1965
01:10:24,400 --> 01:10:27,040
нужно в бумаге о том, почему это корень d, но я на

1966
01:10:27,040 --> 01:10:28,640
самом деле не могу снять это с моей

1967
01:10:28,640 --> 01:10:31,199
головы, так что,

1968
01:10:31,199 --> 01:10:34,239
но это в этой статье,

1969
01:10:37,920 --> 01:10:40,159
хорошо,

1970
01:10:40,320 --> 01:10:44,280
что-нибудь еще, прежде чем продолжить

1971
01:10:44,960 --> 01:10:46,719
отлично

1972
01:10:46,719 --> 01:10:47,110
ммм

1973
01:10:47,110 --> 01:10:48,400
[музыка

1974
01:10:48,400 --> 01:10:50,400
] хорошо, так что бы вы хотели исправить,

1975
01:10:50,400 --> 01:10:51,520
ммм,

1976
01:10:51,520 --> 01:10:53,360
вы знаете, что то, что

1977
01:10:53,360 --> 01:10:55,199
чаще всего проявляется в качестве болевой точки в

1978
01:10:55,199 --> 01:10:57,280
трансформерах, на самом деле похоже на

1979
01:10:57,280 --> 01:10:59,679
квадратичные вычисления в

1980
01:10:59,679 --> 01:11:02,719
самом самовнимании.  все пары

1981
01:11:02,719 --> 01:11:04,480
взаимодействий у нас была матрица t на t,

1982
01:11:04,480 --> 01:11:06,800
которая была вычислена путем взятия этих скалярных

1983
01:11:06,800 --> 01:11:09,520
произведений между всеми парами векторов слов,

1984
01:11:09,520 --> 01:11:12,239
и поэтому, несмотря на то, что в начале класса мы утверждали,

1985
01:11:12,239 --> 01:11:13,760
что у нас

1986
01:11:13,760 --> 01:11:16,080
нет такой временной зависимости

1987
01:11:16,080 --> 01:11:18,080
в  граф вычислений, который мешает нам

1988
01:11:18,080 --> 01:11:20,159
распараллеливать то, что нам все еще нужно для выполнения

1989
01:11:20,159 --> 01:11:21,679
всех этих вычислений, и который растет

1990
01:11:21,679 --> 01:11:24,320
квадратично для рекуррентных моделей, верно,

1991
01:11:24,320 --> 01:11:26,080
он только линейно увеличивался каждый раз, когда

1992
01:11:26,080 --> 01:11:28,880
вы применяли ячейку rnn.

1993
01:11:28,880 --> 01:11:30,640
g

1994
01:11:30,640 --> 01:11:32,239
квадратично к объему работы, которую вы

1995
01:11:32,239 --> 01:11:33,920
должны выполнить, когда вы переходите к более длинным

1996
01:11:33,920 --> 01:11:35,280
последовательностям,

1997
01:11:35,280 --> 01:11:36,880
отдельно позиционным представлениям, я

1998
01:11:36,880 --> 01:11:38,640
имею в виду, что абсолютная позиция слова

1999
01:11:38,640 --> 01:11:40,719
просто не

2000
01:11:40,719 --> 01:11:43,040
может быть, возможно, не лучшим способом представить

2001
01:11:43,040 --> 01:11:45,840
э-э, вы знаете структуру предложения,

2002
01:11:45,840 --> 01:11:48,159
и поэтому  были эти два ээээ,

2003
01:11:48,159 --> 01:11:50,719
вы знаете, среди других достижений в

2004
01:11:50,719 --> 01:11:52,800
том, что я не

2005
01:11:52,800 --> 01:11:54,000
смогу сегодня рассказать, но вы можете взглянуть на

2006
01:11:54,000 --> 01:11:55,440
эти документы и статьи, которые их цитируют,

2007
01:11:55,440 --> 01:11:56,880
есть другие способы представить

2008
01:11:56,880 --> 01:11:59,040
позицию  люди работают над этим, но сегодня я

2009
01:11:59,040 --> 01:12:02,640
хочу больше сосредоточиться на

2010
01:12:02,640 --> 01:12:06,080
проблеме квадратичных вычислений, так

2011
01:12:06,080 --> 01:12:08,400
как мы можем решить

2012
01:12:08,400 --> 01:12:10,880
эту проблему,

2013
01:12:12,320 --> 01:12:14,480
почему это проблема?  операций у нас есть t в квадрате,

2014
01:12:14,480 --> 01:12:16,400
это длина последовательности, а затем d

2015
01:12:16,400 --> 01:12:18,560
- размерность, и поэтому при вычислении

2016
01:12:18,560 --> 01:12:20,880
этой матрицы у нас есть o из t возведенных в квадрат d

2017
01:12:20,880 --> 01:12:23,040
вычислений, которые наш графический процессор должен разбить на

2018
01:12:23,840 --> 01:12:25,840
um, если мы думаем о d, это примерно

2019
01:12:25,840 --> 01:12:29,520
тысяча  d, две или три тысячи um, если бы

2020
01:12:29,520 --> 01:12:32,560
у нас были какие-то отдельные короткие предложения,

2021
01:12:32,560 --> 01:12:34,320
но тогда, возможно, t было бы где-то тридцать, а

2022
01:12:34,320 --> 01:12:36,640
затем t в квадрате равно 900, так что это как ах,

2023
01:12:36,640 --> 01:12:38,239
на самом деле это не так уж важно, но

2024
01:12:38,239 --> 01:12:40,000
вы знаете, и на практике вы знаете, что

2025
01:12:40,000 --> 01:12:42,000
для многих моделей мы установим фактическую границу,

2026
01:12:42,000 --> 01:12:43,840
например 512, так что если ваш

2027
01:12:43,840 --> 01:12:46,080
документ длиннее 512 слов, вы знаете, что вам

2028
01:12:46,080 --> 01:12:47,679
не повезло, вы усечены или

2029
01:12:47,679 --> 01:12:50,239
что-то в этом роде, но что, если мы хотим работать

2030
01:12:50,239 --> 01:12:52,080
над документами, которые  Знаете ли вы, что десять

2031
01:12:52,080 --> 01:12:54,320
тысяч слов или больше десять тысяч в

2032
01:12:54,320 --> 01:12:56,239
квадрате

2033
01:12:56,239 --> 01:12:58,320
невозможно, поэтому нам нужно как-то

2034
01:12:58,320 --> 01:13:00,880
устранить зависимость от t в квадрате, если

2035
01:13:00,880 --> 01:13:03,280
мы собираемся работать с ними,

2036
01:13:03,280 --> 01:13:04,800
есть несколько способов, которые

2037
01:13:04,800 --> 01:13:06,080
были придуманы для этого  все это

2038
01:13:06,080 --> 01:13:07,520
очень недавняя работа, и это лишь

2039
01:13:07,520 --> 01:13:09,679
малая часть усилий, которые

2040
01:13:09,679 --> 01:13:12,320
возникли, поэтому вопрос в том, можем ли мы построить

2041
01:13:12,320 --> 01:13:14,480
модели, такие как трансформаторы, которые

2042
01:13:14,480 --> 01:13:17,120
обходятся без квадрата o или t в квадрате всех

2043
01:13:17,120 --> 01:13:20,159
парных взаимодействий.

2044
01:13:20,159 --> 01:13:22,480
Одним из примеров является линформер и

2045
01:13:22,480 --> 01:13:24,400
идея здесь в том, что

2046
01:13:24,400 --> 01:13:26,719
ты  собираемся фактически сопоставить

2047
01:13:26,719 --> 01:13:29,120
размерность длины последовательности с

2048
01:13:29,120 --> 01:13:31,520
пространством более низкой размерности

2049
01:13:31,520 --> 01:13:34,080
для значений и ключей, чтобы у вас были

2050
01:13:34,080 --> 01:13:35,600
ключи значений и запросы,

2051
01:13:35,600 --> 01:13:37,280
и у вас были обычные линейные слои,

2052
01:13:37,280 --> 01:13:38,640
теперь вы собираетесь проецировать гораздо

2053
01:13:38,640 --> 01:13:41,199
меньшее измерение, чем длина последовательности

2054
01:13:41,199 --> 01:13:43,600
и  Поступая так правильно,

2055
01:13:43,600 --> 01:13:46,400
вы как бы избавляетесь от этого t,

2056
01:13:46,400 --> 01:13:48,159
сопоставляя его с чем-то маленьким, о котором вы

2057
01:13:48,159 --> 01:13:49,760
говорите, как будто вы просто знаете, объедините

2058
01:13:49,760 --> 01:13:50,960
всю информацию из всех этих временных

2059
01:13:50,960 --> 01:13:52,560
шагов во что-то более низкое

2060
01:13:52,560 --> 01:13:54,960
измерение, и поэтому на этом графике из

2061
01:13:54,960 --> 01:13:56,719
документ, который вы знаете, поскольку длина последовательности

2062
01:13:56,719 --> 01:14:00,480
идет от 512, размер пакета 128 до

2063
01:14:00,480 --> 01:14:03,040
длины последовательности 65000 с

2064
01:14:03,040 --> 01:14:04,640
размером пакета один, вы получаете

2065
01:14:04,640 --> 01:14:07,360
время вывода трансформатора, вроде

2066
01:14:07,360 --> 01:14:09,040
как очень большой, а затем

2067
01:14:09,040 --> 01:14:12,239
линформер с различными типами

2068
01:14:12,239 --> 01:14:15,520
узких мест размерностей ks  128 256

2069
01:14:15,520 --> 01:14:18,239
они работают намного лучше,

2070
01:14:18,239 --> 01:14:21,199
отдельный вариант заключался в том, чтобы

2071
01:14:22,880 --> 01:14:25,199
взглянуть на совершенно другой взгляд,

2072
01:14:25,199 --> 01:14:26,800
можем ли мы обойтись без всех этих парных

2073
01:14:26,800 --> 01:14:29,679
взаимодействий, что является следующим  крыло,

2074
01:14:29,679 --> 01:14:32,159
нам нужно даже попытаться вычислить все

2075
01:14:32,159 --> 01:14:34,159
пары взаимодействий, если мы сможем сделать

2076
01:14:34,159 --> 01:14:35,760
кучу других вещей, которые

2077
01:14:35,760 --> 01:14:38,320
будут более эффективными для вычислений,

2078
01:14:38,320 --> 01:14:40,560
например, смотреть на локальные окна, которые мы знаем,

2079
01:14:40,560 --> 01:14:42,480
что это полезно, но в некотором

2080
01:14:42,480 --> 01:14:45,040
смысле недостаточно  во всем, так что если бы

2081
01:14:45,040 --> 01:14:46,560
вы просто взяли среднее значение

2082
01:14:46,560 --> 01:14:48,320
векторов, просто все усреднение векторов,

2083
01:14:48,320 --> 01:14:49,600
вам не нужно было бы вычислять взаимодействия

2084
01:14:49,600 --> 01:14:50,560
для этого,

2085
01:14:50,560 --> 01:14:52,560
и если вы посмотрите на какие-то случайные пары,

2086
01:14:52,560 --> 01:14:54,800
вам не нужно знать, что вы все это знаете.

2087
01:14:54,800 --> 01:14:56,960
много времени, чтобы вычислить и это, и поэтому

2088
01:14:56,960 --> 01:14:59,199
то, что сделала эта статья, они сделали все из

2089
01:14:59,199 --> 01:15:02,239
них, так что у вас есть случайное внимание, у вас

2090
01:15:02,239 --> 01:15:03,840
есть внимание окна слова, где

2091
01:15:03,840 --> 01:15:05,360
вы смотрите на своих местных соседей,

2092
01:15:05,360 --> 01:15:07,199
и у вас есть своего рода глобальное внимание

2093
01:15:07,199 --> 01:15:09,199
там, где вы ''  если вы знаете, что вы посещаете, но

2094
01:15:09,199 --> 01:15:11,440
не взаимодействуете с вещами, которые присутствуют в

2095
01:15:11,440 --> 01:15:13,360
целом на протяжении всей последовательности, вы делаете

2096
01:15:13,360 --> 01:15:16,400
целую кучу всего этого правильно, и в конечном итоге вы

2097
01:15:16,400 --> 01:15:18,000
можете приблизиться к множеству хороших

2098
01:15:18,000 --> 01:15:19,600
вещей,

2099
01:15:19,600 --> 01:15:21,440
которые вы не знаете, обязательно

2100
01:15:21,440 --> 01:15:23,280
ответ  Вариант с обычным трансформатором

2101
01:15:23,280 --> 01:15:25,840
в настоящее время является наиболее популярным, но

2102
01:15:25,840 --> 01:15:28,400
это интересный вопрос, который стоит изучить,

2103
01:15:28,400 --> 01:15:30,719
поэтому теперь, когда

2104
01:15:30,719 --> 01:15:31,679
время

2105
01:15:31,679 --> 01:15:33,920
более или менее истекает, я скажу, что мы

2106
01:15:33,920 --> 01:15:35,679
работаем над предварительной тренировкой в четверг,

2107
01:15:35,679 --> 01:15:38,000
удачи в задании 4 и не забудьте

2108
01:15:38,000 --> 01:15:39,840
поработайте над своим проектным предложением, я думаю, у нас

2109
01:15:39,840 --> 01:15:42,159
есть время для последнего вопроса, если кто-то

2110
01:15:42,159 --> 01:15:44,560
захочет,

2111
01:15:56,640 --> 01:15:58,719
это хороший вопрос,

2112
01:15:58,719 --> 01:16:00,239
да, я

2113
01:16:00,239 --> 01:16:02,159
имею в виду, что

2114
01:16:02,159 --> 01:16:04,480
я верю, что в обучении с подкреплением все еще есть места,

2115
01:16:06,640 --> 01:16:08,400
я имею в виду места, где повторяющееся

2116
01:16:08,400 --> 01:16:10,719
индуктивное смещение четко

2117
01:16:10,719 --> 01:16:13,040
определено или полезно  был

2118
01:16:13,040 --> 01:16:15,760
разговор вроде,

2119
01:16:16,000 --> 01:16:18,880
но я не знаю мест в nlp, где

2120
01:16:18,880 --> 01:16:21,199
люди все еще широко используют rnns, какое-то время

2121
01:16:21,199 --> 01:16:22,239
считалось, что

2122
01:16:22,239 --> 01:16:23,840
трансформаторам требуется намного больше данных для

2123
01:16:23,840 --> 01:16:25,360
обучения, чем rnn, и поэтому вы вроде как

2124
01:16:25,360 --> 01:16:27,520
должны использовать rnns для небольших проблем с данными

2125
01:16:27,520 --> 01:16:29,199
но с предварительным обучением я не уверен,

2126
01:16:29,199 --> 01:16:31,360
что это так,

2127
01:16:31,360 --> 01:16:33,360
я думаю, ответ - да,

2128
01:16:33,360 --> 01:16:35,120
все еще есть варианты использования,

2129
01:16:35,120 --> 01:16:37,679
но это должно быть там, где повторение

2130
01:16:37,679 --> 01:16:39,679
кажется действительно

2131
01:16:39,679 --> 01:16:41,199
тем, что выигрывает у

2132
01:16:41,199 --> 01:16:44,080
вас  Для чего-то, а не вроде,

2133
01:16:44,080 --> 01:16:45,840
может потребоваться больше данных или

2134
01:16:45,840 --> 01:16:47,120
трансформаторов, потому что кажется, что это

2135
01:16:47,120 --> 01:16:48,400
может быть не так,

2136
01:16:48,400 --> 01:16:52,920
хотя мы думали об этом еще в 2017 году.

