1
00:00:05,359 --> 00:00:06,640
привет всем

2
00:00:06,640 --> 00:00:10,639
добро пожаловать на лекцию cs224n девять

3
00:00:10,639 --> 00:00:14,559
эээ самовнимание и трансформеры ээээ, если

4
00:00:14,559 --> 00:00:16,480
я не могу быть услышанным прямо сейчас,

5
00:00:16,480 --> 00:00:17,760
пожалуйста, отправьте сообщение в

6
00:00:17,760 --> 00:00:21,199
чате, потому что я никого не вижу

7
00:00:21,199 --> 00:00:24,000
эм ээээээээээ, я очень рад получить  в

8
00:00:24,000 --> 00:00:26,880
содержание на сегодня, мы будем говорить

9
00:00:26,880 --> 00:00:30,240
о самовнимании и трансформерах.

10
00:00:30,240 --> 00:00:32,960
Позвольте нам погрузиться в

11
00:00:32,960 --> 00:00:34,480
план лекции, и мы поговорим о

12
00:00:34,480 --> 00:00:36,800
каких-то делах для

13
00:00:36,800 --> 00:00:39,040
курса, так что мы начнем с  куда

14
00:00:39,040 --> 00:00:40,640
мы

15
00:00:40,640 --> 00:00:41,600
вернулись на

16
00:00:41,600 --> 00:00:43,840
прошлой неделе с рекуррентными рекуррентными

17
00:00:43,840 --> 00:00:45,760
нейронными сетями, и мы поговорим о

18
00:00:45,760 --> 00:00:47,520
движении от рецидивов к

19
00:00:47,520 --> 00:00:49,360
моделям НЛП, основанным на внимании,

20
00:00:49,360 --> 00:00:51,520
о которых мы говорили о внимании, и мы собираемся просто

21
00:00:51,520 --> 00:00:54,000
сосредоточить внимание на этом, мы

22
00:00:54,000 --> 00:00:56,079
представим трансформатор  модель, которая представляет

23
00:00:56,079 --> 00:00:58,000
собой особый тип модели, основанной на внимании,

24
00:00:58,000 --> 00:01:00,239
которая очень популярна,

25
00:01:00,239 --> 00:01:01,520
вам нужно знать, что вы собираетесь ее

26
00:01:01,520 --> 00:01:03,120
изучить, мы поговорим о некоторых отличных

27
00:01:03,120 --> 00:01:05,438
результатах с трансформаторами, а затем о некоторых

28
00:01:05,438 --> 00:01:07,760
недостатках и вариантах, а также о

29
00:01:07,760 --> 00:01:08,880
недавней

30
00:01:08,880 --> 00:01:11,360
работе по улучшению  их так некоторые напоминают

31
00:01:11,360 --> 00:01:13,040
Перед тем, как мы перейдем к

32
00:01:13,040 --> 00:01:15,680
заданию 4, необходимо

33
00:01:15,680 --> 00:01:18,080
провести опрос отзывов в середине квартала во

34
00:01:18,080 --> 00:01:20,000
вторник, 16 февраля,

35
00:01:20,000 --> 00:01:21,040
вы получите

36
00:01:21,040 --> 00:01:23,360
небольшое количество баллов за это

37
00:01:23,360 --> 00:01:24,960
, и мы очень ценим ваши

38
00:01:24,960 --> 00:01:26,640
отзывы о том, что мы сделали хорошо, что мы

39
00:01:26,640 --> 00:01:27,840
можем улучшить,

40
00:01:27,840 --> 00:01:29,280
а затем

41
00:01:29,280 --> 00:01:33,200
Окончательное проектное предложение также должно быть сделано.

42
00:01:33,200 --> 00:01:35,200
Одно примечание о предложениях.

43
00:01:35,200 --> 00:01:40,159
Цель предложения состоит в том, чтобы вы знаете,

44
00:01:40,240 --> 00:01:41,759
или я бы сказал, что основная часть

45
00:01:41,759 --> 00:01:44,479
цели предложения состоит в том, чтобы дать вам обратную связь

46
00:01:44,479 --> 00:01:47,520
по идее, которую вы представили.  и

47
00:01:47,520 --> 00:01:51,759
убедитесь, что

48
00:01:51,759 --> 00:01:55,360
это жизнеспособный вариант для окончательного проекта, и

49
00:01:55,360 --> 00:01:57,360
убедитесь, что мы как бы перецентрируем, если нет, и

50
00:01:57,360 --> 00:01:59,040
поэтому мы хотим очень быстро получить обратную связь с вами

51
00:01:59,040 --> 00:02:02,079
об этом, хорошо,

52
00:02:02,079 --> 00:02:04,479
так что давайте

53
00:02:04,479 --> 00:02:08,639
начнем с содержания  лекция на этой неделе,

54
00:02:08,639 --> 00:02:10,080
так что

55
00:02:10,080 --> 00:02:12,959
мы были в этом месте в nlp на прошлой

56
00:02:12,959 --> 00:02:14,720
неделе, где у нас были повторяющиеся нейронные

57
00:02:14,720 --> 00:02:16,800
сети, вроде многих вещей,

58
00:02:16,800 --> 00:02:18,640
которые вы хотели сделать,

59
00:02:18,640 --> 00:02:21,680
так что это примерно в 2016 году, и стратегия, если

60
00:02:21,680 --> 00:02:24,319
вы хотите построить сильную модель nlp,

61
00:02:24,319 --> 00:02:26,160
ты знаешь, что послал  что вам

62
00:02:26,160 --> 00:02:27,760
нужно кодировать, и у вас есть как

63
00:02:27,760 --> 00:02:31,519
двунаправленный lstm, скажите um, и вы знаете,

64
00:02:31,519 --> 00:02:32,640
может быть, это немного похоже на это

65
00:02:32,640 --> 00:02:34,239
пиктографически,

66
00:02:34,239 --> 00:02:36,080
и, возможно, это исходное предложение в

67
00:02:36,080 --> 00:02:37,680
переводе, например, мы видели машинный

68
00:02:37,680 --> 00:02:39,840
перевод, а затем вы определяете свой

69
00:02:39,840 --> 00:02:41,280
вывод, который может быть последовательностью

70
00:02:41,280 --> 00:02:43,360
слов, которая является целью, вы знаете

71
00:02:43,360 --> 00:02:44,319
перевод, который вы пытаетесь

72
00:02:44,319 --> 00:02:46,800
предсказать, или, может быть, это дерево синтаксического анализа или

73
00:02:46,800 --> 00:02:48,319
это резюме,

74
00:02:48,319 --> 00:02:49,519
и вы

75
00:02:49,519 --> 00:02:52,879
используете lstm с одним направлением

76
00:02:52,879 --> 00:02:54,640
для его генерации,

77
00:02:54,640 --> 00:02:56,959
и это действительно работает  ну, мы

78
00:02:56,959 --> 00:02:58,560
использовали эти архитектуры, чтобы делать

79
00:02:58,560 --> 00:03:00,879
всевозможные интересные вещи, но одна

80
00:03:00,879 --> 00:03:02,159
вещь, о которой мы говорили, мы говорили,

81
00:03:02,159 --> 00:03:04,239
это информационное узкое место, которое

82
00:03:04,239 --> 00:03:06,239
вы пытаетесь кодировать, возможно, очень

83
00:03:06,239 --> 00:03:08,239
длинную последовательность и вид самого последнего

84
00:03:08,239 --> 00:03:10,319
вектора  в вашем или в одном векторе в вашем

85
00:03:10,319 --> 00:03:12,800
кодировщике, и поэтому мы используем натяжение в качестве

86
00:03:12,800 --> 00:03:15,519
этого механизма, чтобы вы

87
00:03:15,519 --> 00:03:17,840
узнали представление из нашего декодера и

88
00:03:17,840 --> 00:03:20,239


89
00:03:20,239 --> 00:03:22,480
как бы оглянулись назад и обработали закодированные представления как память

90
00:03:22,480 --> 00:03:24,480
Мы можем сослаться на то

91
00:03:24,480 --> 00:03:26,080
, что важно в любой момент времени, и как бы выбрать то, что важно,

92
00:03:26,080 --> 00:03:27,040


93
00:03:27,040 --> 00:03:30,799
и это было внимание, и на этой неделе

94
00:03:30,799 --> 00:03:32,640
мы собираемся сделать что-то немного

95
00:03:32,640 --> 00:03:34,239
другое,

96
00:03:34,239 --> 00:03:36,000
поэтому мы узнали о моделях последовательности для

97
00:03:36,000 --> 00:03:38,400
последовательностей, способах декодирования кодировщика.

98
00:03:38,400 --> 00:03:41,200
думать о проблемах

99
00:03:41,200 --> 00:03:42,799
более или менее, чтобы справиться с этой

100
00:03:42,799 --> 00:03:44,879
идеей о том, что вы знаете, как построить

101
00:03:44,879 --> 00:03:46,400
систему машинного перевода, которая полностью

102
00:03:46,400 --> 00:03:47,840
дифференцируема,

103
00:03:47,840 --> 00:03:50,319
и так что это действительно

104
00:03:50,319 --> 00:03:51,519
интересный способ думать о

105
00:03:51,519 --> 00:03:53,920
проблемах, что мы будем делать на этой неделе

106
00:03:53,920 --> 00:03:55,519
отличается, мы не пытаемся

107
00:03:55,519 --> 00:03:58,159
мотивировать совершенно новый способ мышления

108
00:03:58,159 --> 00:03:59,360
о

109
00:03:59,360 --> 00:04:01,360
проблемах, таких как машинный перевод,

110
00:04:01,360 --> 00:04:03,120
вместо этого мы собираемся взять строительные

111
00:04:03,120 --> 00:04:05,360
блоки, которые мы использовали, ну, вы знаете,

112
00:04:05,360 --> 00:04:07,120
повторяющиеся нейронные сети, и мы

113
00:04:07,120 --> 00:04:09,680
собираемся потратить  много проб и ошибок

114
00:04:09,680 --> 00:04:11,439
в полевых условиях, пытаясь выяснить,

115
00:04:11,439 --> 00:04:13,680
есть ли строительные блоки, которые

116
00:04:13,680 --> 00:04:16,798
лучше работают с широким кругом проблем,

117
00:04:16,798 --> 00:04:19,199
мы собираемся

118
00:04:19,199 --> 00:04:21,358
вставить новую вещь для повторяющейся нейры  l

119
00:04:21,358 --> 00:04:23,680
и говорят, что вы знаете, вуаля, может быть, это

120
00:04:23,680 --> 00:04:25,520
работает лучше,

121
00:04:25,520 --> 00:04:26,880
и поэтому

122
00:04:26,880 --> 00:04:28,720
я хочу взять нас в такого рода

123
00:04:28,720 --> 00:04:30,560
путешествие

124
00:04:30,560 --> 00:04:31,520
к

125
00:04:31,520 --> 00:04:33,120
сетям самовнимания, и мы начнем

126
00:04:33,120 --> 00:04:34,639
с некоторых проблем с повторяющимися нейронными

127
00:04:34,639 --> 00:04:36,560
сетями, поэтому мы потратили немного времени,

128
00:04:36,560 --> 00:04:39,440
пытаясь убедить  вы,

129
00:04:39,680 --> 00:04:42,080
что наши текущие нейронные сети были

130
00:04:42,080 --> 00:04:43,600
очень полезны,

131
00:04:43,600 --> 00:04:45,280
теперь я собираюсь поговорить о причинах, по которым

132
00:04:45,280 --> 00:04:47,600
они могут быть улучшены,

133
00:04:47,600 --> 00:04:48,880
поэтому

134
00:04:48,880 --> 00:04:50,479
мы знаем, что рекуррентные нейронные сети

135
00:04:50,479 --> 00:04:52,800
регистрируются слева направо,

136
00:04:52,800 --> 00:04:54,400
цитата в кавычках, это может быть справа

137
00:04:54,400 --> 00:04:56,720
налево,

138
00:04:56,720 --> 00:04:58,320
так что что  Означает ли это, что

139
00:04:58,320 --> 00:05:00,240
наша текущая нейронная сеть

140
00:05:00,240 --> 00:05:02,880


141
00:05:02,880 --> 00:05:06,000
правильно кодирует линейную локальность, поэтому, когда я смотрю на вкусное

142
00:05:06,000 --> 00:05:08,320
в этой фразе, я собираюсь посмотреть

143
00:05:08,320 --> 00:05:09,680
на пиццу, или если я иду в другом

144
00:05:09,680 --> 00:05:11,440
направлении, когда я смотрю на пиццу, я

145
00:05:11,440 --> 00:05:13,759
на вкус, и поэтому

146
00:05:13,759 --> 00:05:15,600
их значения, присутствующие

147
00:05:15,600 --> 00:05:17,440
в предложении, очень легко повлиять на значение, чтобы

148
00:05:17,440 --> 00:05:18,880
повлиять на представление другого

149
00:05:18,880 --> 00:05:20,479
слова um,

150
00:05:20,479 --> 00:05:22,160
и это на самом деле очень полезно,

151
00:05:22,160 --> 00:05:23,759
потому что соседние слова часто действительно

152
00:05:23,759 --> 00:05:25,440
влияют на каждое другое слово  r, это практически

153
00:05:25,440 --> 00:05:27,280
одна из вещей, о которых мы говорили с

154
00:05:27,280 --> 00:05:29,840
гипотезой распределения, которая

155
00:05:29,840 --> 00:05:32,320
закодирована чем-то вроде слова к спине,

156
00:05:32,320 --> 00:05:35,120
но если слова линейно удалены

157
00:05:35,120 --> 00:05:36,400


158
00:05:36,400 --> 00:05:37,759
друг от друга, они все еще могут взаимодействовать друг с другом, это

159
00:05:37,759 --> 00:05:39,600
то, что мы видели при синтаксическом анализе зависимостей,

160
00:05:39,600 --> 00:05:40,960


161
00:05:40,960 --> 00:05:43,840
поэтому, если я сказал  фраза шеф-повар

162
00:05:43,840 --> 00:05:46,000
замечает шеф-повар здесь жирным шрифтом я запускаю

163
00:05:46,000 --> 00:05:48,240
повторяющуюся нейронную сеть по этому,

164
00:05:48,240 --> 00:05:50,000
а затем вы знаете шеф-повара, который, а затем у

165
00:05:50,000 --> 00:05:52,240
меня есть эта длинная последовательность,

166
00:05:52,240 --> 00:05:55,919
которую я собираюсь закодировать,

167
00:05:55,919 --> 00:05:58,000
и тогда слово было

168
00:05:58,000 --> 00:06:00,000
правильным  может быть, это был шеф-повар

169
00:06:00,000 --> 00:06:02,960
, но между ними у меня есть

170
00:06:02,960 --> 00:06:04,880
длина последовательности, много шагов

171
00:06:04,880 --> 00:06:09,120
вычислений, которые мне нужно выполнить, прежде чем

172
00:06:09,120 --> 00:06:10,639
шеф-повар и waz

173
00:06:10,639 --> 00:06:12,240
смогут правильно взаимодействовать,

174
00:06:12,240 --> 00:06:14,160
и поэтому в середине

175
00:06:14,160 --> 00:06:16,400
все может пойти не так, как

176
00:06:16,400 --> 00:06:20,639
вы знаете, может быть трудно  изучайте

177
00:06:20,639 --> 00:06:22,319
вещи с помощью um,

178
00:06:22,319 --> 00:06:23,440
где они должны взаимодействовать, так что, в

179
00:06:23,440 --> 00:06:25,919
частности, может быть трудно изучить

180
00:06:25,919 --> 00:06:27,520
зависимости на больших расстояниях, потому что у нас

181
00:06:27,520 --> 00:06:29,919
есть проблемы с градиентом, мы видели, что lstms

182
00:06:29,919 --> 00:06:31,680
распространяет градиенты лучше, чем простые

183
00:06:31,680 --> 00:06:34,160
rnns, но  не идеально,

184
00:06:34,160 --> 00:06:36,319
и поэтому, если chef и waz очень далеки,

185
00:06:36,319 --> 00:06:37,919
становится трудно понять, что они должны

186
00:06:37,919 --> 00:06:40,800
взаимодействовать, а линейный порядок

187
00:06:40,800 --> 00:06:42,240
слов вроде бы встроен в модель, так что

188
00:06:42,240 --> 00:06:44,880
вам нужно развернуть rnn на

189
00:06:44,880 --> 00:06:46,400
протяжении всей последовательности,

190
00:06:46,400 --> 00:06:48,000
и это не совсем правильный путь

191
00:06:48,000 --> 00:06:49,360
думать о

192
00:06:49,360 --> 00:06:51,840
предложениях обязательно гм, по крайней мере, вы

193
00:06:51,840 --> 00:06:54,400
знаете, что линейный порядок - это не совсем то,

194
00:06:54,400 --> 00:06:58,000
насколько предложения структурированы,

195
00:06:58,000 --> 00:06:58,880


196
00:06:58,880 --> 00:07:01,599
и вы знаете, что здесь у вас есть повар,

197
00:07:01,599 --> 00:07:03,120
а затем у вас есть все такого рода

198
00:07:03,120 --> 00:07:04,639
вычисления в середине, все эти

199
00:07:04,639 --> 00:07:05,919
применения рекуррентной

200
00:07:05,919 --> 00:07:08,639
матрицы весов до того, как вы позволите ей

201
00:07:08,639 --> 00:07:10,639
взаимодействовать, было, и снова вид

202
00:07:10,639 --> 00:07:14,000
зависимости - это длина последовательности,

203
00:07:14,000 --> 00:07:16,720
а затем вы поняли, что все в порядке,

204
00:07:16,720 --> 00:07:19,360
вторая проблема очень связана

205
00:07:19,360 --> 00:07:21,599
с отсутствием возможности распараллеливания,

206
00:07:21,599 --> 00:07:24,479
так что это происходит  Чтобы быть огромным рефреном

207
00:07:24,479 --> 00:07:25,440
сейчас, когда мы получили

208
00:07:25,440 --> 00:07:26,960
лекции по трансформерам, это возможность

209
00:07:26,960 --> 00:07:30,639
распараллеливания, это то, что вы получаете

210
00:07:30,639 --> 00:07:32,080
от своего графического процессора, и это то, что вы хотите

211
00:07:32,080 --> 00:07:34,160
использовать, так что

212
00:07:34,160 --> 00:07:36,319
когда вы запускаете rnn, у

213
00:07:36,319 --> 00:07:38,240
вас нет

214
00:07:38,240 --> 00:07:39,680
длина последовательности много

215
00:07:39,680 --> 00:07:41,840
непараллелизируемых

216
00:07:41,840 --> 00:07:43,120
операций,

217
00:07:43,120 --> 00:07:45,440
и поэтому, пока у вас есть графический процессор, они могут как

218
00:07:45,440 --> 00:07:46,720
бы фрагментировать сразу несколько

219
00:07:46,720 --> 00:07:49,360
независимых операций,

220
00:07:49,360 --> 00:07:51,520
вы не можете сортировать их все

221
00:07:51,520 --> 00:07:53,599
сразу, потому что у вас есть эта явная

222
00:07:53,599 --> 00:07:56,720
зависимость от времени в их текущих уравнениях,

223
00:07:56,720 --> 00:07:59,280
в частности  вы знаете, что будущее

224
00:07:59,280 --> 00:08:01,680
состояние rnn по строке не может быть вычислено,

225
00:08:01,680 --> 00:08:03,919
пока вы не вычислите то, что было раньше

226
00:08:03,919 --> 00:08:05,599
, и это препятствует обучению на очень

227
00:08:05,599 --> 00:08:07,840
больших наборах данных, поэтому давайте посмотрим на

228
00:08:07,840 --> 00:08:10,800
это при прокрутке rnn,

229
00:08:10,800 --> 00:08:13,120
если это, скажем, первый уровень

230
00:08:13,120 --> 00:08:15,199
слой rnn или lstm, возможно, он не

231
00:08:15,199 --> 00:08:16,400
зависит

232
00:08:16,400 --> 00:08:17,680
эффективно от чего-либо, вы можете просто

233
00:08:17,680 --> 00:08:19,840
вычислить его немедленно,

234
00:08:19,840 --> 00:08:22,000
а затем второй уровень, так что это, как

235
00:08:22,000 --> 00:08:24,319
вы знаете, сложенный набор из двух lstms,

236
00:08:24,319 --> 00:08:26,479
второй уровень зависит от первого слоя

237
00:08:26,479 --> 00:08:27,759
здесь

238
00:08:27,759 --> 00:08:28,639


239
00:08:28,639 --> 00:08:31,680
во времени  измерение, хотя

240
00:08:31,680 --> 00:08:33,679
эта,

241
00:08:33,679 --> 00:08:34,479
эта

242
00:08:34,479 --> 00:08:36,479
ячейка здесь зависит от этого, так что у вас

243
00:08:36,479 --> 00:08:38,719
есть одна, а затем вы знаете, что это зависит от

244
00:08:38,719 --> 00:08:40,640
этого, так что у вас есть одна, так что вы знаете,

245
00:08:40,640 --> 00:08:42,958
извините, по крайней мере, две вещи, которые

246
00:08:42,958 --> 00:08:45,279
вам нужно вычислить  То есть здесь, прежде чем вы сможете

247
00:08:45,279 --> 00:08:47,760
вычислить значение этой ячейки,

248
00:08:47,760 --> 00:08:49,839
также три здесь, и с

249
00:08:49,839 --> 00:08:52,399
длиной последовательности она растет с o

250
00:08:52,399 --> 00:08:54,720
длины последовательности, поэтому здесь я не

251
00:08:54,720 --> 00:08:57,920
смог даже попытаться вычислить это значение,

252
00:08:57,920 --> 00:09:01,519
когда я приехал сюда, потому что мне пришлось

253
00:09:01,519 --> 00:09:03,200
сначала сделай все это, так что я не могу

254
00:09:03,200 --> 00:09:05,680
распараллелить по временному измерению,

255
00:09:05,680 --> 00:09:07,360
и это препятствует обучению на очень больших

256
00:09:07,360 --> 00:09:09,519
наборах данных,

257
00:09:09,519 --> 00:09:11,839
ну

258
00:09:12,720 --> 00:09:16,880
ладно, а затем я думаю, что Крис или Та

259
00:09:16,880 --> 00:09:18,640
могут остановить меня вопросом,

260
00:09:18,640 --> 00:09:20,080
если мне кажется, что это  правильно, что

261
00:09:20,080 --> 00:09:22,000
делать,

262
00:09:22,000 --> 00:09:22,959


263
00:09:22,959 --> 00:09:24,320
хорошо, так что,

264
00:09:24,320 --> 00:09:25,519
и вы можете увидеть, как это связанная

265
00:09:25,519 --> 00:09:27,040
проблема, правильно, это действительно напрямую

266
00:09:27,040 --> 00:09:29,279
связано с повторением модели,

267
00:09:29,279 --> 00:09:30,720
а то, что мы думали, было действительно

268
00:09:30,720 --> 00:09:32,880
полезно сейчас,

269
00:09:32,880 --> 00:09:35,600
проблематично

270
00:09:35,600 --> 00:09:37,920


271
00:09:37,920 --> 00:09:40,240
скажем с этим,

272
00:09:40,240 --> 00:09:42,480
мы, похоже, хотим заменить повторение

273
00:09:42,480 --> 00:09:44,720
как сам строительный блок, поэтому давайте рассмотрим

274
00:09:44,720 --> 00:09:46,720
некоторые альтернативы, и мы видели

275
00:09:46,720 --> 00:09:48,320
каждую из этих альтернатив в классе

276
00:09:48,320 --> 00:09:51,279
, пока что мы начнем с окна слова,

277
00:09:51,279 --> 00:09:53,519
типа строительных блоков для o  ur nlp

278
00:09:53,519 --> 00:09:55,040
модели, если мы хотим заменить наши

279
00:09:55,040 --> 00:09:57,519
кодировщики и наши декодеры чем-то,

280
00:09:57,519 --> 00:10:00,480
что соответствует той же

281
00:10:00,480 --> 00:10:03,519
цели, но имеет разные свойства,

282
00:10:03,519 --> 00:10:05,519
поэтому модель окна слов будет агрегировать

283
00:10:05,519 --> 00:10:07,760
локальный контекст, как мы видели это с нашими

284
00:10:07,760 --> 00:10:09,519
классификаторами окна слов, которые

285
00:10:09,519 --> 00:10:10,959
мы  уже построенный,

286
00:10:10,959 --> 00:10:13,040
вы берете локальный контекст слов, которые вы

287
00:10:13,040 --> 00:10:14,959
используете

288
00:10:14,959 --> 00:10:16,560
для представления информации о

289
00:10:16,560 --> 00:10:18,480
центральном слове, это также известно как

290
00:10:18,480 --> 00:10:20,720
одномерная свертка.

291
00:10:20,720 --> 00:10:22,640


292
00:10:22,640 --> 00:10:24,720
word

293
00:10:24,720 --> 00:10:27,440
window uh контексты,

294
00:10:27,440 --> 00:10:29,440
поэтому количество непараллелизируемых

295
00:10:29,440 --> 00:10:31,839
операций с этими

296
00:10:31,839 --> 00:10:33,440
строительными блоками word window не растет с

297
00:10:33,440 --> 00:10:35,600
длиной последовательности, и вот что-то вроде

298
00:10:35,600 --> 00:10:37,920
картины того, что у вас есть слой встраивания,

299
00:10:37,920 --> 00:10:40,079
скажем так, что вы можете вставлять каждое слово

300
00:10:40,079 --> 00:10:41,680
независимо,

301
00:10:41,680 --> 00:10:43,040
так что вы не должны  Мне нужно знать

302
00:10:43,040 --> 00:10:44,480
другие слова, окружающие его, чтобы

303
00:10:44,480 --> 00:10:47,440
выбрать правильный размер вложения,

304
00:10:47,440 --> 00:10:49,440
и поэтому все они имеют своего рода нулевую

305
00:10:49,440 --> 00:10:51,519
зависимость в подобном

306
00:10:51,519 --> 00:10:53,360
волнистом представлении о том, как m  uch uh

307
00:10:53,360 --> 00:10:55,360
paralyzability есть,

308
00:10:55,360 --> 00:10:57,440
теперь вы можете пройти классификатор окна слова

309
00:10:57,440 --> 00:11:01,040
поверх каждого, чтобы построить

310
00:11:01,040 --> 00:11:03,120
представление слова, которое

311
00:11:03,120 --> 00:11:05,680
учитывает его локальный контекст,

312
00:11:05,680 --> 00:11:08,959
но для того, чтобы применить его к этому слову h1,

313
00:11:08,959 --> 00:11:11,600
мне не нужно знать  что-нибудь, извините, мне

314
00:11:11,600 --> 00:11:13,519
не нужно применять его к h1

315
00:11:13,519 --> 00:11:15,760
, чтобы применить его к h2 аналогично

316
00:11:15,760 --> 00:11:17,200
, чтобы применить контекстуализатор окна слова

317
00:11:17,200 --> 00:11:19,519
к ht,

318
00:11:19,519 --> 00:11:20,880
я могу просто смотреть на его локальное окно

319
00:11:20,880 --> 00:11:22,079
независимо,

320
00:11:22,079 --> 00:11:23,519
и поэтому снова

321
00:11:23,519 --> 00:11:25,839
ни один из них не имеет зависимости в  время,

322
00:11:25,839 --> 00:11:28,880
и я могу продолжать складывать слои, как это

323
00:11:28,880 --> 00:11:30,320
правильно, так что это может выглядеть

324
00:11:30,320 --> 00:11:32,880
как кодировщик справа кодировщик, как

325
00:11:32,880 --> 00:11:35,440
наши кодировщики lstm ммм, если бы я не позволил

326
00:11:35,440 --> 00:11:37,360
вам смотреть в будущее, просто

327
00:11:37,360 --> 00:11:38,959
отрезав окно, оно могло бы выглядеть

328
00:11:38,959 --> 00:11:41,279
как декодер для  языковые модели, и

329
00:11:41,279 --> 00:11:43,519
это хорошо,

330
00:11:43,519 --> 00:11:46,160
и мы получаем это красиво, знаете ли,

331
00:11:46,160 --> 00:11:48,399
о одной зависимости во времени, верно, никакой

332
00:11:48,399 --> 00:11:49,839
зависимости во временном измерении

333
00:11:49,839 --> 00:11:51,519
, это улучшение, но есть некоторые

334
00:11:51,519 --> 00:11:53,040
проблемы, так что насчет зависимостей на большом расстоянии,

335
00:11:53,040 --> 00:11:55,519
правильно это  вот почему

336
00:11:55,519 --> 00:11:57,040
мы сказали, что хотим использовать рекуррентные

337
00:11:57,040 --> 00:11:58,399
нейронные сети, потому что

338
00:11:58,399 --> 00:12:00,240
они лучше справятся с кодированием зависимостей на большом расстоянии

339
00:12:00,240 --> 00:12:02,399


340
00:12:02,399 --> 00:12:04,320
, это проблема так же, как и

341
00:12:04,320 --> 00:12:06,560
раньше, но, складывая

342
00:12:06,560 --> 00:12:09,440
слои окна слов, мы можем получить более широкие и длинные

343
00:12:09,440 --> 00:12:10,800
контексты,

344
00:12:10,800 --> 00:12:13,760
поэтому, если у вас есть  какой-то размер окна,

345
00:12:13,760 --> 00:12:16,399
а затем вы складываете два слоя,

346
00:12:16,399 --> 00:12:19,600
так что красный указывает здесь интерактивное состояние вроде

347
00:12:19,600 --> 00:12:21,839
того, как вы можете посмотреть, как далеко вы можете

348
00:12:21,839 --> 00:12:24,240
смотреть, чтобы правильно кодировать hk,

349
00:12:24,240 --> 00:12:27,519
поэтому в слое встраивания у

350
00:12:27,519 --> 00:12:30,240
вас есть такие слова здесь, поэтому  так

351
00:12:30,240 --> 00:12:32,560
что это последний слой этот верхний слой

352
00:12:32,560 --> 00:12:34,480
классификатора окна слова здесь

353
00:12:34,480 --> 00:12:36,320
встраивание hk на выходе вашего

354
00:12:36,320 --> 00:12:38,000
кодировщика,

355
00:12:38,000 --> 00:12:39,839
и поэтому он смотрит на то, что вы знаете

356
00:12:39,839 --> 00:12:41,600
пять локальных слов, потому что это размер окна

357
00:12:41,600 --> 00:12:44,399
, а также вы знаете

358
00:12:44,399 --> 00:12:46,720
самое дальнее слово  здесь также

359
00:12:46,720 --> 00:12:48,639
сразу посмотрел на пару слов,

360
00:12:48,639 --> 00:12:49,920
поэтому, если вы сложите их, сложите

361
00:12:49,920 --> 00:12:51,279
их и сложите их,

362
00:12:51,279 --> 00:12:53,279
не увеличивая размер окна вообще

363
00:12:53,279 --> 00:12:55,120
на любом заданном слое,

364
00:12:55,120 --> 00:12:56,560
вы можете смотреть довольно далеко и на самом деле

365
00:12:56,560 --> 00:12:57,920
там  это уловки, которые вы можете использовать, чтобы смотреть

366
00:12:57,920 --> 00:12:59,839
еще дальше, но у вас все еще есть такая,

367
00:12:59,839 --> 00:13:02,560
по крайней мере в принципе, проблема,

368
00:13:02,560 --> 00:13:05,120
когда у вас есть такое слово, как это h1,

369
00:13:05,120 --> 00:13:07,440
и вы можете видеть его синим цветом,

370
00:13:07,440 --> 00:13:09,600
и вы знаете, с этими двумя

371
00:13:09,600 --> 00:13:10,800
слоями сети

372
00:13:10,800 --> 00:13:13,040
i  вообще ничего не знаю о h1,

373
00:13:13,040 --> 00:13:14,639
когда я создаю

374
00:13:14,639 --> 00:13:16,880
здесь представление hk,

375
00:13:16,880 --> 00:13:18,240
я мог бы решить это, добавив еще один

376
00:13:18,240 --> 00:13:20,480
глубинный слой, но вы знаете, что в принципе у

377
00:13:20,480 --> 00:13:22,959
вас всегда есть какое-то конечное поле,

378
00:13:22,959 --> 00:13:24,399


379
00:13:24,399 --> 00:13:25,600
так что это

380
00:13:25,600 --> 00:13:27,920
вы знаете, на самом деле довольно  полезные э-э,

381
00:13:27,920 --> 00:13:29,600
вы знаете, э-э, слово оконные разновидности

382
00:13:29,600 --> 00:13:31,200
контекстуализаторов, и мы узнаем

383
00:13:31,200 --> 00:13:33,200
о них больше позже э-э, и есть вроде

384
00:13:33,200 --> 00:13:34,880
как много усилий, о которых я говорил

385
00:13:34,880 --> 00:13:36,079
в начале урока, на

386
00:13:36,079 --> 00:13:38,160
самом деле как бы частично решало, какое

387
00:13:38,160 --> 00:13:39,839
из когда слово

388
00:13:39,839 --> 00:13:41,279
сверточный материал окна называется вещью или

389
00:13:41,279 --> 00:13:43,360
вниманием, и внимание

390
00:13:43,360 --> 00:13:46,480
на данный момент выиграло эм, и да, а как

391
00:13:46,480 --> 00:13:49,120
насчет внимания, так почему оно может быть

392
00:13:49,120 --> 00:13:51,199
полезным в качестве фундаментального

393
00:13:51,199 --> 00:13:52,959
строительного блока вместо своего рода сахара

394
00:13:52,959 --> 00:13:55,120
на  top of our lstms

395
00:13:55,120 --> 00:13:56,880
um, так что просто чтобы

396
00:13:56,880 --> 00:13:58,800
вспомнить некоторые интуиции

397
00:13:58,800 --> 00:13:59,760
внимания,

398
00:13:59,760 --> 00:14:02,720
он обрабатывает представление слова как

399
00:14:02,720 --> 00:14:03,760
запрос

400
00:14:03,760 --> 00:14:06,160
, ищет где-то

401
00:14:06,160 --> 00:14:07,920
и пытается отсортировать информацию доступа

402
00:14:07,920 --> 00:14:10,079
из набора значений так, чтобы у нас было

403
00:14:10,079 --> 00:14:12,160
представление слова в нашем декодере в

404
00:14:12,160 --> 00:14:14,160
наши системы машинного перевода

405
00:14:14,160 --> 00:14:15,839
набор значений, в которых все кодировщики

406
00:14:15,839 --> 00:14:19,760
указывают исходное предложение,

407
00:14:19,760 --> 00:14:20,639
и

408
00:14:20,639 --> 00:14:22,079
сегодня мы будем думать о внимании, а не о

409
00:14:22,079 --> 00:14:23,920
внимании от декодера к

410
00:14:23,920 --> 00:14:26,000
кодировщику, мы будем думать о внимании

411
00:14:26,000 --> 00:14:28,720
в рамках одного предложения,

412
00:14:28,720 --> 00:14:31,279
так что просто очень быстрое изображение  у

413
00:14:31,279 --> 00:14:32,880
вас снова есть слой встраивания,

414
00:14:32,880 --> 00:14:34,639
я помещаю сюда счетчики вычислительных зависимостей,

415
00:14:34,639 --> 00:14:36,720
чтобы все это можно

416
00:14:36,720 --> 00:14:38,800
было делать параллельно для

417
00:14:38,800 --> 00:14:40,000
слоя встраивания снова,

418
00:14:40,000 --> 00:14:42,079
и теперь вы делаете напряжение правильно,

419
00:14:42,079 --> 00:14:43,600
так что вы  я как бы смотрю на

420
00:14:43,600 --> 00:14:46,160
каждое слово в слое встраивания, чтобы

421
00:14:46,160 --> 00:14:47,839
обратить внимание на это слово,

422
00:14:47,839 --> 00:14:50,480
и я испускаю кучу стрелок,

423
00:14:50,480 --> 00:14:52,720
так что это все стрелки, все слова

424
00:14:52,720 --> 00:14:54,240
взаимодействуют со всеми словами, и мы

425
00:14:54,240 --> 00:14:55,920
углубимся в это  сегодня я обещаю, но я

426
00:14:55,920 --> 00:14:57,519
просто хотел сделать этот график

427
00:14:57,519 --> 00:14:58,560
немного

428
00:14:58,560 --> 00:15:00,959
менее плотным, а затем,

429
00:15:00,959 --> 00:15:03,760
чтобы во втором слое снова все

430
00:15:03,760 --> 00:15:05,920
пары слов взаимодействовали, и все

431
00:15:05,920 --> 00:15:08,000
это можно распараллелить, поэтому вы не можете

432
00:15:08,000 --> 00:15:10,320
распараллелить глубоко правильно  потому что вам нужно

433
00:15:10,320 --> 00:15:12,800
закодировать этот слой, прежде чем вы сможете сделать этот

434
00:15:12,800 --> 00:15:15,440
слой, но со временем он становится распараллеливаемым,

435
00:15:15,440 --> 00:15:18,240
поэтому он ставит этот флажок,

436
00:15:18,240 --> 00:15:21,279
поэтому снова у нас есть один вид

437
00:15:21,279 --> 00:15:22,959
вычислительной

438
00:15:22,959 --> 00:15:24,320
зависимости,

439
00:15:24,320 --> 00:15:26,000
вы знаете количество непараллелизируемых

440
00:15:26,000 --> 00:15:27,600
операций как функцию длины последовательности

441
00:15:27,600 --> 00:15:31,680
и ну, как  дополнительное преимущество право,

442
00:15:31,680 --> 00:15:33,519
расстояние взаимодействия между

443
00:15:33,519 --> 00:15:34,880
словами

444
00:15:34,880 --> 00:15:36,639
также равно единице,

445
00:15:36,639 --> 00:15:38,240
так что раньше у нас были повторяющиеся

446
00:15:38,240 --> 00:15:40,800
сети, где, если вы находитесь далеко,

447
00:15:40,800 --> 00:15:42,959
это последнее слово в предложении, у

448
00:15:42,959 --> 00:15:45,120
вас может быть o из t

449
00:15:45,120 --> 00:15:47,120
операций между вами и далеким

450
00:15:47,120 --> 00:15:49,519
словом  с вниманием вы

451
00:15:49,519 --> 00:15:51,680
немедленно взаимодействуете, это самый первый уровень, на котором вы

452
00:15:51,680 --> 00:15:53,519
можете увидеть свое далекое слово, и так

453
00:15:53,519 --> 00:15:56,800
что это ноль из одного, и это в конечном итоге

454
00:15:56,800 --> 00:15:59,440
кажется захватывающе мощным, и

455
00:15:59,440 --> 00:16:01,600
мы войдем во многие  подробности

456
00:16:01,600 --> 00:16:04,000
сегодня в

457
00:16:04,000 --> 00:16:05,279
порядке

458
00:16:05,279 --> 00:16:08,160
, вот почему внимание

459
00:16:08,160 --> 00:16:10,079
решает две проблемы, которые мы

460
00:16:10,079 --> 00:16:11,839
подняли с помощью повторяющихся нейронных сетей, но

461
00:16:11,839 --> 00:16:14,560
с нашими эмпирическими шляпами на

462
00:16:14,560 --> 00:16:16,320
нем еще не должно быть доказательством того, что вы знаете, что

463
00:16:16,320 --> 00:16:17,600
это должен быть хороший строительный блок, и

464
00:16:17,600 --> 00:16:19,440
на самом деле это  нужно немного

465
00:16:19,440 --> 00:16:21,040
подумать, чтобы подумать о том, как превратить

466
00:16:21,040 --> 00:16:22,639
внимание в строительный блок, как у

467
00:16:22,639 --> 00:16:24,800
rnn,

468
00:16:24,800 --> 00:16:26,560
так что давайте начнем с

469
00:16:26,560 --> 00:16:29,199
копания прямо в уравнения

470
00:16:29,199 --> 00:16:30,800
для самовнимания, которое снова является

471
00:16:30,800 --> 00:16:32,720
вниманием к тому, куда

472
00:16:32,720 --> 00:16:33,920
все смотрит внутри себя,

473
00:16:33,920 --> 00:16:36,720
мы формализуем  это для вас, поэтому сегодня мы

474
00:16:36,720 --> 00:16:38,240
будем говорить всю лекцию

475
00:16:38,240 --> 00:16:42,000
о ключах и значениях

476
00:16:42,800 --> 00:16:44,800
запросов, наши запросы будут набором из t

477
00:16:44,800 --> 00:16:47,040
запросов, каждый запрос - это вектор в

478
00:16:47,040 --> 00:16:48,800
измерении d,

479
00:16:48,800 --> 00:16:50,959
вы можете просто думать о них как о тех

480
00:16:50,959 --> 00:16:52,480
векторах прямо сейчас

481
00:16:52,480 --> 00:16:53,839
не беспокоясь о том, откуда

482
00:16:53,839 --> 00:16:55,120
они

483
00:16:55,120 --> 00:16:58,560
взялись, у нас есть набор ключей k1, kt,

484
00:16:58,560 --> 00:17:01,680
опять же, каждый вектор k имеет размерность

485
00:17:01,680 --> 00:17:02,560
d,

486
00:17:02,560 --> 00:17:04,959
и у нас есть некоторые значения, каждое

487
00:17:04,959 --> 00:17:06,160
значение будет

488
00:17:06,160 --> 00:17:09,280
также в размерности d an  d на данный момент

489
00:17:09,280 --> 00:17:11,280
мы собираемся предположить, что у нас

490
00:17:11,280 --> 00:17:13,119
их всех одинаковое количество,

491
00:17:13,119 --> 00:17:15,599
что не обязательно произойдет позже, поэтому

492
00:17:15,599 --> 00:17:17,119
при самовнимании

493
00:17:17,119 --> 00:17:19,359
при самовнимании ключи, запросы и

494
00:17:19,359 --> 00:17:20,959
значения поступают из одного и того же источника

495
00:17:20,959 --> 00:17:22,240
информации,

496
00:17:22,240 --> 00:17:24,480
одно и то же предложение  например,

497
00:17:24,480 --> 00:17:27,199
да, на практике, когда все они

498
00:17:27,199 --> 00:17:28,799
происходят из одного и того же предложения,

499
00:17:28,799 --> 00:17:30,160
их всех будет одинаковое количество,

500
00:17:30,160 --> 00:17:32,720
это все будет на

501
00:17:32,720 --> 00:17:35,760
практике, у вас могут быть разные числа,

502
00:17:35,760 --> 00:17:37,919
так откуда они берутся мы '  Я

503
00:17:37,919 --> 00:17:39,919
подробно расскажу об этом позже,

504
00:17:39,919 --> 00:17:41,679
а пока подумайте о выходе

505
00:17:41,679 --> 00:17:44,559
предыдущего слоя, так что представьте, что на выходе

506
00:17:44,559 --> 00:17:46,240
вы знаете, что у вас есть как раз такой слой встраивания,

507
00:17:46,240 --> 00:17:47,120


508
00:17:47,120 --> 00:17:48,320
и это вход для чего-то,

509
00:17:48,320 --> 00:17:50,240
что будет заниматься самовниманием, подумайте

510
00:17:50,240 --> 00:17:51,919
обо всем  из этих выходов

511
00:17:51,919 --> 00:17:55,440
вложений в виде некоторых векторов

512
00:17:55,440 --> 00:17:57,679
xi,

513
00:17:57,679 --> 00:18:00,080
и теперь мы можем просто сказать, что значение

514
00:18:00,080 --> 00:18:02,080
равно ключу q,

515
00:18:02,080 --> 00:18:04,640
равно как запрос равен этому

516
00:18:04,640 --> 00:18:05,919
xi, поэтому мы собираемся использовать одни и те же векторы

517
00:18:05,919 --> 00:18:08,240
для всех из них  но называть их

518
00:18:08,240 --> 00:18:10,640
Ключевые запросы и значения, которые я обещаю,

519
00:18:10,640 --> 00:18:12,320
будут очень полезны в том, как мы как бы думаем

520
00:18:12,320 --> 00:18:14,960
о том, что происходит, и как мы смотрим

521
00:18:14,960 --> 00:18:17,600
на уравнения, которые реализуют это,

522
00:18:17,600 --> 00:18:18,320
так что

523
00:18:18,320 --> 00:18:19,679


524
00:18:19,679 --> 00:18:22,080
самовнимание в целом,

525
00:18:22,080 --> 00:18:24,080
но с этим точечным продуктом, так что развивайте

526
00:18:24,080 --> 00:18:25,520
собственное внимание продукта здесь просто  математическая

527
00:18:25,520 --> 00:18:26,480


528
00:18:26,480 --> 00:18:29,919
математика заключается в том, что вы вычисляете сродства ключевых запросов,

529
00:18:29,919 --> 00:18:32,640
а бит скалярного произведения - это тот факт, что

530
00:18:32,640 --> 00:18:33,760
вы используете здесь функцию скалярного произведения,

531
00:18:33,760 --> 00:18:34,720


532
00:18:34,720 --> 00:18:37,280
поэтому вы берете скалярное произведение для всех пар

533
00:18:37,280 --> 00:18:38,559
i и

534
00:18:38,559 --> 00:18:39,840
j qi

535
00:18:39,840 --> 00:18:41,840
с точками k,

536
00:18:41,840 --> 00:18:45,280
так что это на t  матрица заглавная t

537
00:18:45,280 --> 00:18:48,240
справа на t матрица аффинностей,

538
00:18:48,240 --> 00:18:50,160
которые являются скалярными значениями,

539
00:18:50,160 --> 00:18:53,120
не ограниченными по размеру,

540
00:18:53,120 --> 00:18:54,480
затем вы вычисляете веса внимания, которые

541
00:18:54,480 --> 00:18:56,480
мы также видели, используя функцию softmax, которую

542
00:18:56,480 --> 00:18:57,440
я только что написал

543
00:18:57,440 --> 00:18:59,360
здесь, функцию softmax,

544
00:18:59,360 --> 00:19:01,440
чтобы вы знали, что вы возводите в степень

545
00:19:01,440 --> 00:19:04,320
аффинность и  тогда вы суммируете

546
00:19:04,320 --> 00:19:05,840
в этом случае правильно, вы суммируете

547
00:19:05,840 --> 00:19:07,360
все

548
00:19:07,360 --> 00:19:08,559


549
00:19:08,559 --> 00:19:10,400
ключи uh, поэтому у вас есть данный запрос,

550
00:19:10,400 --> 00:19:12,000
и вы суммируете все ключи

551
00:19:12,000 --> 00:19:13,840
для нормализации, так где

552
00:19:13,840 --> 00:19:15,679
этот запрос должен искать re  член, у вас

553
00:19:15,679 --> 00:19:18,160
есть t разных запросов, которые мы делаем

554
00:19:18,160 --> 00:19:21,840
для этого э-э, здесь um, и поэтому для данного

555
00:19:21,840 --> 00:19:23,760
запроса вы суммируете все ключи,

556
00:19:23,760 --> 00:19:25,919
чтобы нормализовать нормированную константу

557
00:19:25,919 --> 00:19:29,520
, которая дает вам распределение по um

558
00:19:29,520 --> 00:19:31,760
по длине последовательности t, поэтому  теперь у вас

559
00:19:31,760 --> 00:19:33,200
есть своего рода вес для всех индексов

560
00:19:33,200 --> 00:19:34,320
последовательности,

561
00:19:34,320 --> 00:19:35,840


562
00:19:35,840 --> 00:19:38,720
и снова мы делаем наше средневзвешенное значение

563
00:19:38,720 --> 00:19:40,400
правильно, так что у нас есть наши

564
00:19:40,400 --> 00:19:42,559
веса для нашего среднего, а затем

565
00:19:42,559 --> 00:19:43,919
выход прямо там будет один

566
00:19:43,919 --> 00:19:46,080
выход для каждого запроса,

567
00:19:46,080 --> 00:19:48,480
выход - это веса для  который

568
00:19:48,480 --> 00:19:51,120
умножается на векторы значений

569
00:19:51,120 --> 00:19:53,760
справа um, так что снова, если вы установите ключи,

570
00:19:53,760 --> 00:19:56,080
все значения будут x,

571
00:19:56,080 --> 00:19:59,440
это имеет смысл, но приятно

572
00:19:59,440 --> 00:20:02,240
иметь реплики и k, чтобы знать,

573
00:20:02,240 --> 00:20:03,600
какая вещь делает то, что вы можете думать

574
00:20:03,600 --> 00:20:05,440
о  запрос как своего рода

575
00:20:05,440 --> 00:20:07,919
поиск информации где-то там, где ключ, который вы

576
00:20:07,919 --> 00:20:10,159
знаете, взаимодействует с запросом, а

577
00:20:10,159 --> 00:20:11,919
затем значение - это то, что вы знаете, что на

578
00:20:11,919 --> 00:20:13,520
самом деле собираетесь узнать вес

579
00:20:13,520 --> 00:20:16,000
в среднем и вывести

580
00:20:16,000 --> 00:20:18,880
Джону вопрос, который вам может понравиться  nswer

581
00:20:18,880 --> 00:20:22,080
, так что, если теперь мы подключаем все

582
00:20:22,080 --> 00:20:24,000
ко всему, чем это отличается от

583
00:20:24,000 --> 00:20:26,480
использования полностью подключенного слоя

584
00:20:26,480 --> 00:20:28,400
, это здорово, это отличный вопрос,

585
00:20:28,400 --> 00:20:29,360


586
00:20:29,360 --> 00:20:32,240
пара причин, а первая заключается в том, что, в

587
00:20:32,240 --> 00:20:34,480
отличие от полностью подключенного слоя, вы можете

588
00:20:34,480 --> 00:20:36,080


589
00:20:36,080 --> 00:20:37,919
узнать веса взаимодействия

590
00:20:37,919 --> 00:20:40,080
ну, веса взаимодействия являются

591
00:20:40,080 --> 00:20:43,679
динамическими в зависимости от того, какие фактические

592
00:20:43,679 --> 00:20:45,679
значения здесь верны, поэтому на полностью

593
00:20:45,679 --> 00:20:47,440
подключенном слое у вас есть эти веса,

594
00:20:47,440 --> 00:20:49,360
которые вы медленно изучаете в

595
00:20:49,360 --> 00:20:51,440
ходе обучения вашей сети,

596
00:20:51,440 --> 00:20:53,039
что позволяет вам сказать, какой вид

597
00:20:53,039 --> 00:20:55,120
скрытые блоки, на которые вы должны обратить

598
00:20:55,120 --> 00:20:57,600
внимание, это фактические взаимодействия

599
00:20:57,600 --> 00:21:00,159
между ключом и

600
00:21:00,159 --> 00:21:01,840
векторами запроса um, которые зависят от

601
00:21:01,840 --> 00:21:04,159
фактического содержимого, которое может

602
00:21:04,159 --> 00:21:07,120
изменяться во времени, и поэтому фактические сильные стороны

603
00:21:07,120 --> 00:21:08,720
всех взаимодействий всех видов

604
00:21:08,720 --> 00:21:10,559
весов внимания, которые,

605
00:21:10,559 --> 00:21:12,799
как вы понимаете, связаны

606
00:21:12,799 --> 00:21:14,159
с весами в полностью

607
00:21:14,159 --> 00:21:16,880
подключенном слое, могут изменяться

608
00:21:16,880 --> 00:21:19,360
в зависимости от входных данных.  оценка

609
00:21:19,360 --> 00:21:20,960
заключается в том, что параметризация сильно

610
00:21:20,960 --> 00:21:22,240
отличается, поэтому вы не изучаете

611
00:21:22,240 --> 00:21:23,919
независимый вес соединения для всех

612
00:21:23,919 --> 00:21:28,000
пар вещей, вместо этого вам разрешено

613
00:21:28,000 --> 00:21:30,159


614
00:21:30,159 --> 00:21:32,799
параметризовать внимание, поскольку

615
00:21:32,799 --> 00:21:34,960
вы знаете такого рода функции точечного произведения

616
00:21:34,960 --> 00:21:36,400
между векторами, которые  являются

617
00:21:36,400 --> 00:21:38,480
представлениями, и вы в конечном итоге

618
00:21:38,480 --> 00:21:40,159
знаете,

619
00:21:40,159 --> 00:21:42,799
что параметры работают

620
00:21:42,799 --> 00:21:44,320
лучше, что мы увидим позже,

621
00:21:44,320 --> 00:21:45,120
мы еще не вдавались в то, как мы

622
00:21:45,120 --> 00:21:47,120
параметризуем эти функции, так

623
00:21:47,120 --> 00:21:49,039
что это два ответа, которые я бы сказал, один из

624
00:21:49,039 --> 00:21:50,240
них  у вас есть своего рода динамическое

625
00:21:50,240 --> 00:21:53,039
соединение, и во-вторых, вы знаете, что

626
00:21:53,039 --> 00:21:55,200
у вас его нет, просто у него есть индуктивное

627
00:21:55,200 --> 00:21:57,200
смещение, которое не просто связывает все

628
00:21:57,200 --> 00:22:00,400
со всем, прямая связь

629
00:22:02,320 --> 00:22:03,360
отлично,

630
00:22:03,360 --> 00:22:05,120
хорошо, я думаю, что это очень интересный

631
00:22:05,120 --> 00:22:06,799
вопрос,

632
00:22:06,799 --> 00:22:09,200
да, так что я рад, что вы спросили

633
00:22:09,200 --> 00:22:10,720
хорошо, так

634
00:22:10,720 --> 00:22:12,240
что мы поговорили о самовнимании сейчас,

635
00:22:12,240 --> 00:22:13,039
уравнениях, которые входят в

636
00:22:13,039 --> 00:22:15,679
самовнимание, но мы можем просто

637
00:22:15,679 --> 00:22:17,600
использовать это как строительный блок, я имею в виду, что вы

638
00:22:17,600 --> 00:22:19,440
знаете, возьмите все свои lstm, выбросьте их

639
00:22:19,440 --> 00:22:20,799
, используйте s  эльфийское внимание, которое мы

640
00:22:20,799 --> 00:22:22,960
только что определили, а почему не хорошо,

641
00:22:22,960 --> 00:22:26,799
вот пара причин, почему так смотри на

642
00:22:26,799 --> 00:22:28,559
самовнимание как на строительный блок, чтобы у нас

643
00:22:28,559 --> 00:22:30,960
было несколько слов в предложении повар,

644
00:22:30,960 --> 00:22:32,640
который

645
00:22:32,640 --> 00:22:34,799
кое-что длинное предложение

646
00:22:34,799 --> 00:22:36,720
еда - последнее слово  предложения

647
00:22:36,720 --> 00:22:38,559
хорошо,

648
00:22:38,559 --> 00:22:40,559
и вы знаете, что у них есть встраивание, и

649
00:22:40,559 --> 00:22:42,159
из этого вы получаете свой ключевой запрос и

650
00:22:42,159 --> 00:22:44,720
значение, о которых мы говорили до сих пор, что на

651
00:22:44,720 --> 00:22:46,880
самом деле тот же вектор, но вы знаете, что ключ

652
00:22:46,880 --> 00:22:48,640
значение запроса ключ значение запроса ключ создать

653
00:22:48,640 --> 00:22:49,520
значение

654
00:22:49,520 --> 00:22:51,919
мм, и вы знаете  мы могли бы складывать их, как

655
00:22:51,919 --> 00:22:53,919
слои lstm, чтобы у вас было значение keqree,

656
00:22:53,919 --> 00:22:56,000
выполняйте самовнимание по

657
00:22:56,000 --> 00:22:57,360
запросам и значениям ключей, как мы сказали,

658
00:22:57,360 --> 00:22:59,120
самовнимание - это функция для

659
00:22:59,120 --> 00:23:00,640
запросов и значений ключей, поэтому

660
00:23:00,640 --> 00:23:02,480
выполняйте самовнимание теперь, когда у вас есть эти

661
00:23:02,480 --> 00:23:04,720
запросы, получайте новые ключи  значений,

662
00:23:04,720 --> 00:23:06,960
а затем снова выполните

663
00:23:06,960 --> 00:23:09,280
самовнимание, вы знаете,

664
00:23:09,280 --> 00:23:12,400
это очень похоже на складывание lstms,

665
00:23:12,400 --> 00:23:13,760
но на самом деле у него есть несколько проблем в его

666
00:23:13,760 --> 00:23:15,120
нынешнем виде, поэтому нам нужно будет отправиться в

667
00:23:15,120 --> 00:23:17,280
путешествие, чтобы определить, чего не хватает в

668
00:23:17,280 --> 00:23:19,360
нашем  себя  -внимание, и первое,

669
00:23:19,360 --> 00:23:22,080
что самовнимание - это операция

670
00:23:22,080 --> 00:23:24,960
на множествах,

671
00:23:24,960 --> 00:23:26,559
хорошо, поэтому для уравнений, которые у нас были

672
00:23:26,559 --> 00:23:29,120
до этого, уравнение самовнимания

673
00:23:29,120 --> 00:23:31,520
никогда не ссылалось на

674
00:23:31,520 --> 00:23:33,280
индексы kq или v,

675
00:23:33,280 --> 00:23:35,120
кроме как для того, чтобы сказать, какие

676
00:23:35,120 --> 00:23:36,640
пары взаимодействуют  друг с другом он

677
00:23:36,640 --> 00:23:38,240
не знает, каков порядок вашего

678
00:23:38,240 --> 00:23:40,159
предложения, когда он вычисляет,

679
00:23:40,159 --> 00:23:42,400
хотя веса он не знает,

680
00:23:42,400 --> 00:23:44,480
и поэтому, если бы я ввел это предложение,

681
00:23:44,480 --> 00:23:46,080
повар, который готовил,

682
00:23:46,080 --> 00:23:48,240
это было бы так же, как если бы я просто

683
00:23:48,240 --> 00:23:50,400
поменял местами с  chef, а затем переключился

684
00:23:50,400 --> 00:23:52,960
на, и он просто не имел бы

685
00:23:52,960 --> 00:23:53,919
понятия,

686
00:23:53,919 --> 00:23:55,440
так что это уже не сработает,

687
00:23:55,440 --> 00:23:57,679
потому что порядок, в котором слова появляются

688
00:23:57,679 --> 00:24:01,039
в предложениях, имеет значение,

689
00:24:01,039 --> 00:24:02,320
так что вот первая проблема, с которой нам нужно

690
00:24:02,320 --> 00:24:03,919
работать, поэтому у меня будет

691
00:24:03,919 --> 00:24:06,080
список препятствий.

692
00:24:06,080 --> 00:24:07,440


693
00:24:07,440 --> 00:24:09,360


694
00:24:09,360 --> 00:24:10,960


695
00:24:10,960 --> 00:24:13,440


696
00:24:13,440 --> 00:24:15,200


697
00:24:15,200 --> 00:24:18,159
знаю, какой приказ

698
00:24:18,159 --> 00:24:19,440


699
00:24:19,440 --> 00:24:20,960
Если мы не собираемся изменять

700
00:24:20,960 --> 00:24:23,120
сами уравнения самовнимания, нам

701
00:24:23,120 --> 00:24:24,400
нужно

702
00:24:24,400 --> 00:24:27,600
закодировать порядок в ключах, запросах и

703
00:24:27,600 --> 00:24:28,960
значениях и позволить сети как

704
00:24:28,960 --> 00:24:32,559
бы разобраться в этом самостоятельно, так что  подумайте

705
00:24:32,559 --> 00:24:35,760
об этом, у нас есть t индексов последовательности,

706
00:24:35,760 --> 00:24:37,520
и мы собираемся связать t с некоторой

707
00:24:37,520 --> 00:24:39,120
конечной константой,

708
00:24:39,120 --> 00:24:40,400
так что t никогда не будет для нас больше,

709
00:24:40,400 --> 00:24:42,480
чем что-то,

710
00:24:42,480 --> 00:24:44,080
и мы называем это t,

711
00:24:44,080 --> 00:24:45,120
и теперь мы собираемся представить

712
00:24:45,120 --> 00:24:47,760
индекс последовательности как  вектор, поэтому pi

713
00:24:47,760 --> 00:24:49,679
будет вектором, представляющим

714
00:24:49,679 --> 00:24:51,600
индекс i, и он будет иметь

715
00:24:51,600 --> 00:24:53,120
размерность d, как наши ключевые

716
00:24:53,120 --> 00:24:54,799
запросы и значения,

717
00:24:54,799 --> 00:24:56,240
и поэтому у нас будет один из них

718
00:24:56,240 --> 00:24:58,480
для uh от 1 до t,

719
00:24:58,480 --> 00:25:00,240
поэтому не делайте этого  Пока не беспокойтесь о том, как

720
00:25:00,240 --> 00:25:02,080
выглядят числа Пи, как они устроены,

721
00:25:02,080 --> 00:25:04,000
мы сразу перейдем к этому, но подумайте

722
00:25:04,000 --> 00:25:06,080
об этом, легко включить эту

723
00:25:06,080 --> 00:25:08,400
информацию в наши строительные

724
00:25:08,400 --> 00:25:11,039
блоки внимания на первом уровне, если вы позволите

725
00:25:11,039 --> 00:25:14,320
tilda v tilde k tilde q  быть нашими старыми

726
00:25:14,320 --> 00:25:17,120
значениями, ключами и запросами, которые

727
00:25:17,120 --> 00:25:19,279
мы можем просто добавить, мы могли бы  делаем и другие вещи,

728
00:25:19,279 --> 00:25:20,080


729
00:25:20,080 --> 00:25:22,480
но на практике мы просто добавляем так, что vi

730
00:25:22,480 --> 00:25:25,120
равно v тильде i, нашему вектору значений без порядка

731
00:25:25,120 --> 00:25:27,440
плюс пи, так что это может быть

732
00:25:27,440 --> 00:25:29,919
ваше вложение, вы знаете

733
00:25:29,919 --> 00:25:32,480
свой вектор встраивания, а затем вы

734
00:25:32,480 --> 00:25:35,360
добавляете индекс, который он находится, к его вектору

735
00:25:35,360 --> 00:25:37,279
эм, и вы знаете, что можете сделать это только

736
00:25:37,279 --> 00:25:38,720
на первом уровне сети,

737
00:25:38,720 --> 00:25:40,159
например, поэтому вы делаете то же самое для

738
00:25:40,159 --> 00:25:41,760
запроса и ключа, так что это то,

739
00:25:41,760 --> 00:25:43,440
что вы могли бы сделать

740
00:25:43,440 --> 00:25:44,480
на практике, мы делаем что-то немного

741
00:25:44,480 --> 00:25:45,440
другое,

742
00:25:45,440 --> 00:25:47,440
но

743
00:25:47,440 --> 00:25:48,799
это то, что  теперь вы знаете, что он

744
00:25:48,799 --> 00:25:52,159
знает порядок последовательности, потому что,

745
00:25:52,159 --> 00:25:55,360
если эти pis вы как-то правильно установите,

746
00:25:55,360 --> 00:25:57,200
тогда теперь сеть сможет

747
00:25:57,200 --> 00:25:59,120
выяснить, что с этим делать, так что один из

748
00:25:59,120 --> 00:26:02,799
способов на самом деле сделать это,

749
00:26:04,159 --> 00:26:06,480
один из способов сделать это -

750
00:26:06,480 --> 00:26:09,440
через  конкатенация синусоид, и это

751
00:26:09,440 --> 00:26:11,120
был интересный прием, когда

752
00:26:11,120 --> 00:26:12,960
вышла первая статья о трансформаторах,

753
00:26:12,960 --> 00:26:14,960
они использовали этот метод, поэтому давайте

754
00:26:14,960 --> 00:26:18,000
углубимся в него, ммм, чтобы у вас были

755
00:26:18,000 --> 00:26:19,919
разные длины волн синусоидальных

756
00:26:19,919 --> 00:26:21,039
функций

757
00:26:21,039 --> 00:26:23,120
в ea  ch ваших измерений, поэтому в

758
00:26:23,120 --> 00:26:24,880
первом измерении, если у вас есть эта

759
00:26:24,880 --> 00:26:26,799
функция синуса с заданным периодом,

760
00:26:26,799 --> 00:26:28,080
а затем эта функция косинуса с

761
00:26:28,080 --> 00:26:30,400
заданным периодом, а затем своего рода точка точка

762
00:26:30,400 --> 00:26:31,360
точка,

763
00:26:31,360 --> 00:26:33,360
вы как бы меняете периоды, пока не

764
00:26:33,360 --> 00:26:35,679
дойдете до очень разных периодов и

765
00:26:35,679 --> 00:26:36,720
как это выглядит, так

766
00:26:36,720 --> 00:26:37,679
что

767
00:26:37,679 --> 00:26:39,600
представьте, что здесь,

768
00:26:39,600 --> 00:26:41,919
на вертикальной оси, у нас есть

769
00:26:41,919 --> 00:26:45,039
размерность сети, так

770
00:26:45,039 --> 00:26:46,640
что это d,

771
00:26:46,640 --> 00:26:48,720
а затем это длина последовательности,

772
00:26:48,720 --> 00:26:50,960
и просто указав, что вы знаете, и каждая

773
00:26:50,960 --> 00:26:53,120
строка является своего рода одним из  эти знаки

774
00:26:53,120 --> 00:26:56,799
с разными частотами

775
00:26:56,799 --> 00:26:57,520
правильно,

776
00:26:57,520 --> 00:26:58,799
и вы можете увидеть, как это

777
00:26:58,799 --> 00:27:00,640
кодирует позицию, эти вещи имеют

778
00:27:00,640 --> 00:27:02,000
разные значения

779
00:27:02,000 --> 00:27:03,679
в разных индексах,

780
00:27:03,679 --> 00:27:06,240
и это довольно круто, я действительно не

781
00:27:06,240 --> 00:27:08,400
знаю, как они сразу об этом подумали,

782
00:27:08,400 --> 00:27:10,320
но одна классная вещь  об

783
00:27:10,320 --> 00:27:12,480
этом понятие периодичности верно тот факт,

784
00:27:12,480 --> 00:27:14,159
что синусоиды имеют периоды, которые

785
00:27:14,159 --> 00:27:16,720
могут быть меньше длины последовательности,

786
00:27:16,720 --> 00:27:18,320
указывает на то, что, возможно, абсолютная

787
00:27:18,320 --> 00:27:21,200
позиция слова не так важна

788
00:27:21,200 --> 00:27:23,520
Правильно, потому что вы, если период

789
00:27:23,520 --> 00:27:24,960
меньше длины последовательности, вы теряете

790
00:27:24,960 --> 00:27:26,960
информацию, может быть, о том, где вы находитесь,

791
00:27:26,960 --> 00:27:28,080
конечно, у вас есть конкатенация

792
00:27:28,080 --> 00:27:29,679
многих из них,

793
00:27:29,679 --> 00:27:31,520
так что это профессионал,

794
00:27:31,520 --> 00:27:32,960
может быть, его можно экстраполировать на более длинные

795
00:27:32,960 --> 00:27:34,880
последовательности, потому что снова вы вроде как

796
00:27:34,880 --> 00:27:38,000
правильное повторение значений, потому

797
00:27:38,000 --> 00:27:39,200
что периоды будут,

798
00:27:39,200 --> 00:27:40,480
когда они завершатся, вы увидите,

799
00:27:40,480 --> 00:27:42,799
что это значение снова, минусы в том, что

800
00:27:42,799 --> 00:27:44,399
это не обучается, я имею в виду, что это круто,

801
00:27:44,399 --> 00:27:46,240
но вы не можете

802
00:27:46,240 --> 00:27:48,480
в любом из этого нет обучаемых параметров, а также

803
00:27:48,480 --> 00:27:50,320
экстраполяция на самом деле не работает,

804
00:27:50,320 --> 00:27:51,840
так что это неинтересно и определенно

805
00:27:51,840 --> 00:27:53,039
все еще выполняется,

806
00:27:53,039 --> 00:27:55,840
но то, что делается сейчас чаще, - это

807
00:27:55,840 --> 00:27:57,520
мы, вы знаете, что мы делаем, мы изучаем

808
00:27:57,520 --> 00:28:01,279
представления позиций с нуля, так

809
00:28:01,279 --> 00:28:03,600
что у нас есть э-э,

810
00:28:03,600 --> 00:28:05,200
мы собираемся изучить их из  нацарапайте,

811
00:28:05,200 --> 00:28:06,960
так что пусть все пи будут просто

812
00:28:06,960 --> 00:28:08,559
параметрами, которые можно изучить, так что

813
00:28:08,559 --> 00:28:10,880
мы собираемся получить матрицу p,

814
00:28:10,880 --> 00:28:13,360
которая будет иметь размерность

815
00:28:13,360 --> 00:28:14,880
d размерность нашей сети снова

816
00:28:14,880 --> 00:28:16,480
с помощью последовательности  length, так что это просто

817
00:28:16,480 --> 00:28:18,480
большая матрица справа

818
00:28:18,480 --> 00:28:20,080
от размера

819
00:28:20,080 --> 00:28:22,399
здесь этого размера, фактически d по

820
00:28:22,399 --> 00:28:24,080
длине последовательности,

821
00:28:24,080 --> 00:28:26,240
но каждое отдельное значение в этой матрице является

822
00:28:26,240 --> 00:28:28,720
просто обучаемым параметром.

823
00:28:28,720 --> 00:28:30,799
Плюсы гибкости, теперь вы можете узнать,

824
00:28:30,799 --> 00:28:32,399
какие позиции вроде как должны

825
00:28:32,399 --> 00:28:34,559
означать в соответствии с  ваши данные от начала до

826
00:28:34,559 --> 00:28:36,640
конца, так что это крутые

827
00:28:36,640 --> 00:28:37,600
минусы, которые

828
00:28:37,600 --> 00:28:39,360
вы определенно не можете экстраполировать на

829
00:28:39,360 --> 00:28:41,440
индексы за пределами одного, чтобы отлично, потому

830
00:28:41,440 --> 00:28:43,120
что вы устанавливаете размер этой

831
00:28:43,120 --> 00:28:44,799
матрицы параметров в начале, и вы изучили

832
00:28:44,799 --> 00:28:46,480
их все сейчас, если хотите выйти за пределы

833
00:28:46,480 --> 00:28:48,000
позиции t

834
00:28:48,000 --> 00:28:49,840
вы знаете, что просто у вас нет возможности

835
00:28:49,840 --> 00:28:51,760
представить его эффективно,

836
00:28:51,760 --> 00:28:53,600
но большинство систем используют это, это очень

837
00:28:53,600 --> 00:28:54,640
полезно,

838
00:28:54,640 --> 00:28:55,440
и

839
00:28:55,440 --> 00:28:56,960
иногда люди пробуют более гибкие

840
00:28:56,960 --> 00:28:58,480
представления позиции, потому что

841
00:28:58,480 --> 00:29:01,279
снова абсолютный индекс слова

842
00:29:01,279 --> 00:29:03,520
не является своего рода естественным

843
00:29:03,520 --> 00:29:06,640
представлением его позиции в

844
00:29:06,640 --> 00:29:08,640
предложения, и поэтому люди смотрели

845
00:29:08,640 --> 00:29:09,919
на вид относительного положения

846
00:29:09,919 --> 00:29:12,080
между словами, а также на представления положения,

847
00:29:12,080 --> 00:29:14,080
которые зависят от

848
00:29:14,080 --> 00:29:15,679
синтаксиса, но мы не собираемся  в состоянии

849
00:29:15,679 --> 00:29:18,720
зайти слишком далеко в эти вопросы сегодня,

850
00:29:18,720 --> 00:29:20,720
ладно, так что это была проблема первая,

851
00:29:20,720 --> 00:29:22,399
верно, мы просто, что бы мы ни делали, если бы

852
00:29:22,399 --> 00:29:23,520
у нас не было представления о

853
00:29:23,520 --> 00:29:25,120
позиции, мы не могли бы использовать

854
00:29:25,120 --> 00:29:27,279
самовнимание в качестве нашего нового строительного блока,

855
00:29:27,279 --> 00:29:28,480
и мы '  Я решил это с позиционными

856
00:29:28,480 --> 00:29:29,919
представлениями, которые мы просто как бы добавляем

857
00:29:29,919 --> 00:29:32,559
к входам в

858
00:29:32,880 --> 00:29:34,720
следующем эм,

859
00:29:34,720 --> 00:29:36,000
мы увидим эту проблему, что у вас

860
00:29:36,000 --> 00:29:37,919
нет нелинейностей, о которых вы знаете, даже

861
00:29:37,919 --> 00:29:40,240
говоря, что вы знаете нелинейности, вы знаете

862
00:29:40,240 --> 00:29:42,320
абстрактные функции, которые они  отличное глубокое

863
00:29:42,320 --> 00:29:44,480
обучение, вы знаете, что сквозное

864
00:29:44,480 --> 00:29:46,720
изучение представлений - это здорово, но

865
00:29:46,720 --> 00:29:49,120
сейчас мы просто делаем средневзвешенные данные,

866
00:29:49,120 --> 00:29:51,840
и каким будет наше решение

867
00:29:51,840 --> 00:29:54,240
, я имею в виду, что оно не будет таким

868
00:29:54,240 --> 00:29:55,520
сложным,

869
00:29:55,520 --> 00:29:56,880
так что все мы '  Прямо сейчас мы делаем

870
00:29:56,880 --> 00:29:59,919
повторное усреднение векторов, так что у вас здесь,

871
00:29:59,919 --> 00:30:01,840
э-э, самовнимание, и

872
00:30:01,840 --> 00:30:03,679
если вы просто сложили еще один, вы просто

873
00:30:03,679 --> 00:30:06,640
сохраняете своего рода усредняющие проекции

874
00:30:06,640 --> 00:30:08,159
векторов, но что, если мы просто

875
00:30:08,159 --> 00:30:09,840
добавим прямую связь  сеть

876
00:30:09,840 --> 00:30:12,000
для каждого отдельного слова так внутри th  является

877
00:30:12,000 --> 00:30:13,520
слоем,

878
00:30:13,520 --> 00:30:15,279
каждая из этих нейронных сетей с прямой связью

879
00:30:15,279 --> 00:30:18,000
имеет общие параметры,

880
00:30:18,000 --> 00:30:19,600
но он получает только результат

881
00:30:19,600 --> 00:30:21,520
самовнимания для этого слова, как мы

882
00:30:21,520 --> 00:30:24,240
определили, он обрабатывает его,

883
00:30:24,240 --> 00:30:27,200
и вы знаете, что излучение испускает что-то еще,

884
00:30:27,200 --> 00:30:29,120
и поэтому вы знаете, что у вас есть вывод i от себя.

885
00:30:29,120 --> 00:30:32,000
внимание, которое мы видели на слайдах назад,

886
00:30:32,000 --> 00:30:34,159
применяем вы знаете слой с прямой связью,

887
00:30:34,159 --> 00:30:35,840
где вы берете результат, умноженный

888
00:30:35,840 --> 00:30:38,720
на матрицу, которую вы знаете, нелинейность, другая

889
00:30:38,720 --> 00:30:41,360
матрица, и интуиция, о которой вы можете

890
00:30:41,360 --> 00:30:43,919
подумать, по крайней мере, хорошо, что вы знаете

891
00:30:43,919 --> 00:30:45,919
что-то вроде сети с прямой связью

892
00:30:45,919 --> 00:30:49,360
обрабатывает результат внимания

893
00:30:49,360 --> 00:30:50,960
для каждой вещи, но

894
00:30:50,960 --> 00:30:52,799
более фундаментально верно, вам нужна была какая

895
00:30:52,799 --> 00:30:55,039
-то нелинейность, и вы

896
00:30:55,039 --> 00:30:56,880
знаете, что сеть с прямой связью

897
00:30:56,880 --> 00:30:59,279
хорошо справится, так что это еще одна

898
00:30:59,279 --> 00:31:01,440
проблема, решенная легко исправить

899
00:31:01,440 --> 00:31:02,880
добавить канал  -передовая сеть получает вашу

900
00:31:02,880 --> 00:31:05,840
нелинейность, а теперь ваш

901
00:31:05,840 --> 00:31:07,279
вывод самовнимания, вы можете вроде как обработать его

902
00:31:07,279 --> 00:31:09,200
глубину, увеличивающуюся по мере

903
00:31:09,200 --> 00:31:11,120
увеличения слоев сети, что, как мы

904
00:31:11,120 --> 00:31:13,679
знаем,

905
00:31:13,679 --> 00:31:16,399
полезно  проблема хорошо, так что терпите меня в

906
00:31:16,399 --> 00:31:17,440
этом,

907
00:31:17,440 --> 00:31:19,600
мы не хотим смотреть в будущее, когда

908
00:31:19,600 --> 00:31:21,519
мы делаем языковое моделирование

909
00:31:21,519 --> 00:31:22,960
правильно, поэтому языковое моделирование вы

910
00:31:22,960 --> 00:31:25,919
пытаетесь предсказать слова в будущем,

911
00:31:25,919 --> 00:31:28,000
и с повторяющейся моделью это очень

912
00:31:28,000 --> 00:31:29,760
естественно, как  вы просто

913
00:31:29,760 --> 00:31:32,320
не разворачиваете его дальше, как только вы

914
00:31:32,320 --> 00:31:33,840
развернули слово, чтобы

915
00:31:33,840 --> 00:31:36,159
развернуть свой lstm до данного слова,

916
00:31:36,159 --> 00:31:38,480
нет никакого способа

917
00:31:38,480 --> 00:31:40,640
передать его и следующему слову, но при

918
00:31:40,640 --> 00:31:42,080
самовнимании мы увидим, что это

919
00:31:42,080 --> 00:31:44,480
немного сложнее, так что мы не можем обмануть

920
00:31:44,480 --> 00:31:45,760
и посмотреть на вещи, которые мы пытаемся

921
00:31:45,760 --> 00:31:47,039
прогнозировать, потому что тогда мы будем обучать

922
00:31:47,039 --> 00:31:48,880
сети, которые были совершенно бесполезны, так

923
00:31:48,880 --> 00:31:50,240
что

924
00:31:50,240 --> 00:31:52,000
мы собираемся замаскировать

925
00:31:52,000 --> 00:31:53,120
маскировку - это слово, которое

926
00:31:53,120 --> 00:31:55,039
мы собираемся замаскировать будущее

927
00:31:55,039 --> 00:31:56,640


928
00:31:56,640 --> 00:31:57,440


929
00:31:57,440 --> 00:31:58,960


930
00:31:58,960 --> 00:32:00,960


931
00:32:00,960 --> 00:32:03,039
самовниманием, поэтому, в частности, это важно, когда у нас есть правильные декодеры, одна из причин, по которой мы могли использовать двунаправленные lstms в

932
00:32:03,039 --> 00:32:04,960
наших кодировщиках, заключалась в том, что мы могли видеть

933
00:32:04,960 --> 00:32:06,559
все  исходное предложение в нейронном машинном

934
00:32:06,559 --> 00:32:08,159
переводе, но когда  мы

935
00:32:08,159 --> 00:32:10,480
предсказываем выходное предложение правильно, мы не можем

936
00:32:10,480 --> 00:32:12,320
видеть будущее, если мы хотим обучить модель

937
00:32:12,320 --> 00:32:14,880
делать фактическое предсказание, поэтому, чтобы использовать

938
00:32:14,880 --> 00:32:16,799
самовнимание в декодере, вам нужно

939
00:32:16,799 --> 00:32:18,480
замаскировать будущее.

940
00:32:18,480 --> 00:32:20,399
мог бы

941
00:32:20,399 --> 00:32:22,720
просто каждый раз, когда вы вычисляете внимание,

942
00:32:22,720 --> 00:32:25,840
вы меняете набор ключей

943
00:32:25,840 --> 00:32:28,159
и значений, это должны быть ключи и

944
00:32:28,159 --> 00:32:30,480
значения, чтобы включать только прошлые слова, чтобы

945
00:32:30,480 --> 00:32:32,399
вы как бы динамически изменяли

946
00:32:32,399 --> 00:32:34,559
материал, который вы посещаете, но

947
00:32:34,559 --> 00:32:35,919
это не позволяет нам делать  как мы увидим, так и с

948
00:32:35,919 --> 00:32:38,960
тензорами, а также с возможностью распараллеливания

949
00:32:38,960 --> 00:32:40,159
,

950
00:32:40,159 --> 00:32:42,720
поэтому мы не хотим этого делать, вместо этого

951
00:32:42,720 --> 00:32:44,960
мы собираемся замаскировать будущие слова с

952
00:32:44,960 --> 00:32:46,880
помощью самих весов внимания,

953
00:32:46,880 --> 00:32:48,799
поэтому в математике не волнуйтесь, мы дойдем до

954
00:32:48,799 --> 00:32:50,799
своего рода диаграмма, но в математике у нас были эти

955
00:32:50,799 --> 00:32:53,120
оценки внимания,

956
00:32:53,120 --> 00:32:55,039
и они были равны только этому скалярному

957
00:32:55,039 --> 00:32:58,320
произведению раньше для всех пар,

958
00:32:58,320 --> 00:32:59,120


959
00:32:59,120 --> 00:33:01,039
но теперь,

960
00:33:01,039 --> 00:33:03,440
только если

961
00:33:03,440 --> 00:33:05,919
ключ строго меньше, чем индекс

962
00:33:05,919 --> 00:33:08,000
ключа строго меньше, чем um, это должно

963
00:33:08,000 --> 00:33:10,159
быть i, только если  индекс ключа строго

964
00:33:10,159 --> 00:33:12,320
меньше t  измените индекс запроса, чтобы он

965
00:33:12,320 --> 00:33:13,519
был на j

966
00:33:13,519 --> 00:33:17,200
меньше, чем я, если мы позволим сети

967
00:33:17,200 --> 00:33:18,320
смотреть на слово, и оно должно быть

968
00:33:18,320 --> 00:33:20,320
отрицательной бесконечностью, иначе мы не

969
00:33:20,320 --> 00:33:23,039
позволим вам смотреть на результат, поэтому давайте перейдем

970
00:33:23,039 --> 00:33:24,960
к картинке для кодирования слов

971
00:33:24,960 --> 00:33:26,240
что мы увидим здесь, так что, возможно, у вас есть

972
00:33:26,240 --> 00:33:28,880
начальный токен, который

973
00:33:29,120 --> 00:33:30,960
вы хотите решить, это все ваше

974
00:33:30,960 --> 00:33:32,399
предложение, теперь вы хотите решить, какие

975
00:33:32,399 --> 00:33:34,159
слова в предложении

976
00:33:34,159 --> 00:33:35,679
вам разрешено смотреть при

977
00:33:35,679 --> 00:33:37,519
составлении прогнозов,

978
00:33:37,519 --> 00:33:39,200
поэтому я хочу предварительно  чтобы предсказать

979
00:33:39,200 --> 00:33:41,039
первое слово

980
00:33:41,039 --> 00:33:43,919
и э-э, чтобы предсказать, что мне не

981
00:33:43,919 --> 00:33:46,240
разрешено смотреть на слово, мне также

982
00:33:46,240 --> 00:33:47,919
не разрешено смотреть ни на одно из будущих

983
00:33:47,919 --> 00:33:48,960
слов,

984
00:33:48,960 --> 00:33:50,799
мне разрешено смотреть на начало слова,

985
00:33:50,799 --> 00:33:54,000
так что такого рода  блок здесь не заштрихован

986
00:33:54,000 --> 00:33:56,000


987
00:33:56,000 --> 00:33:58,080
, чтобы поставить слово повар, которое я могу посмотреть

988
00:33:58,080 --> 00:34:01,840
в начале, и правильно начать, но не

989
00:34:01,840 --> 00:34:04,399
повар, естественно, или слово, которое идет

990
00:34:04,399 --> 00:34:06,080
после него, а также для других

991
00:34:06,080 --> 00:34:09,040
слов, чтобы вы могли видеть

992
00:34:09,040 --> 00:34:11,440
эту матрицу  здесь правильно, поэтому мы просто

993
00:34:11,440 --> 00:34:13,839
хотим убедиться, что наши натяжные

994
00:34:13,839 --> 00:34:15,599
веса  вы везде знаете ноль

995
00:34:15,599 --> 00:34:17,918
, поэтому при

996
00:34:17,918 --> 00:34:21,280
вычислении сродства мы добавляем отрицательную бесконечность

997
00:34:21,280 --> 00:34:24,480
ко всем этим в этой большой матрице,

998
00:34:24,480 --> 00:34:26,960
и это гарантирует, что мы не сможем хорошо смотреть

999
00:34:26,960 --> 00:34:29,199
в будущее,

1000
00:34:29,199 --> 00:34:31,520
поэтому теперь мы можем выполнять большие матричные

1001
00:34:31,520 --> 00:34:33,520
умножения, чтобы вычислить наше внимание,

1002
00:34:33,520 --> 00:34:34,639
когда мы  увидим

1003
00:34:34,639 --> 00:34:35,599
эм,

1004
00:34:35,599 --> 00:34:37,440
и мы вроде как не беспокоимся о том, чтобы смотреть

1005
00:34:37,440 --> 00:34:38,800
в будущее, потому что мы добавили эти

1006
00:34:38,800 --> 00:34:40,800
отрицательные бесконечности,

1007
00:34:40,800 --> 00:34:42,560
и это последнее, что последняя

1008
00:34:42,560 --> 00:34:44,480
проблема с самовниманием типа,

1009
00:34:44,480 --> 00:34:45,280
которая

1010
00:34:45,280 --> 00:34:47,040
возникает принципиально, как то, что

1011
00:34:47,040 --> 00:34:50,320
нам нужно для этого  строительный блок,

1012
00:34:50,800 --> 00:34:52,639
который у вас есть, у вас не было неотъемлемого

1013
00:34:52,639 --> 00:34:54,399
понятия порядка теперь у вас есть хорошее

1014
00:34:54,399 --> 00:34:56,079
представление о порядке или, по крайней мере, что-то

1015
00:34:56,079 --> 00:34:57,839
вроде понятия порядка у вас не было

1016
00:34:57,839 --> 00:34:59,680
нелинейностей добавить

1017
00:34:59,680 --> 00:35:01,119
сети с прямой связью,

1018
00:35:01,119 --> 00:35:02,800
а затем вы

1019
00:35:02,800 --> 00:35:04,880
не хотели  Чтобы заглянуть в будущее, вы

1020
00:35:04,880 --> 00:35:08,640
добавляете маски для декодеров,

1021
00:35:10,560 --> 00:35:12,560
чтобы знать, что самовнимание - это

1022
00:35:12,560 --> 00:35:13,920
основа любого строительного

1023
00:35:13,920 --> 00:35:15,200
блока, основанного на самовнимании.

1024
00:35:15,200 --> 00:35:17,359
Представления позиции приветствия полезны.

1025
00:35:17,359 --> 00:35:19,440


1026
00:35:19,440 --> 00:35:21,520
сеть с прямой связью,

1027
00:35:21,520 --> 00:35:23,200
как будто вы могли бы просто сделать

1028
00:35:23,200 --> 00:35:25,119
другие вещи, я думаю, но вы знаете, что на

1029
00:35:25,119 --> 00:35:26,480
практике очень легко

1030
00:35:26,480 --> 00:35:28,160
распараллелить эти сети с прямой связью,

1031
00:35:28,160 --> 00:35:30,880
так что мы в конечном итоге делаем это, а

1032
00:35:30,880 --> 00:35:32,800
затем маскируем,

1033
00:35:32,800 --> 00:35:34,320
вы знаете, да, вы не делаете  Я не хочу, чтобы

1034
00:35:34,320 --> 00:35:35,359
информация просачивалась

1035
00:35:35,359 --> 00:35:36,960
из будущего в прошлое в вашем

1036
00:35:36,960 --> 00:35:39,040
декодере,

1037
00:35:39,040 --> 00:35:41,839
поэтому позвольте мне прояснить, что мы еще не говорили

1038
00:35:41,839 --> 00:35:44,560
о трансформаторе,

1039
00:35:44,560 --> 00:35:46,560
но это все, что вам нужно, если

1040
00:35:46,560 --> 00:35:48,079
вы думаете, как черт возьми, что мне нужно

1041
00:35:48,079 --> 00:35:49,280
, чтобы построить

1042
00:35:49,280 --> 00:35:50,800
Блок моего самовнимания, мы увидим, что

1043
00:35:50,800 --> 00:35:52,480
в трансформаторе есть гораздо больше деталей,

1044
00:35:52,480 --> 00:35:53,760
которые мы собираемся провести

1045
00:35:53,760 --> 00:35:55,839
остаток лекции, э-э

1046
00:35:55,839 --> 00:35:57,520
и я,

1047
00:35:57,520 --> 00:36:00,000
но я хочу, чтобы вы вроде как, по крайней мере, э-э, как

1048
00:36:00,000 --> 00:36:01,359
вы »  думаю о том, что

1049
00:36:01,359 --> 00:36:03,440
будет дальше после трансформатора и как

1050
00:36:03,440 --> 00:36:05,680
вы собираетесь его изобрести ... подумайте

1051
00:36:05,680 --> 00:36:07,760
о том, что это были те вещи, которые

1052
00:36:07,760 --> 00:36:09,760
были необходимы ммм, а затем другие

1053
00:36:09,760 --> 00:36:11,359
вещи в конечном итоге оказываются очень, очень важными

1054
00:36:11,359 --> 00:36:14,800
, оказывается, ммм, но  ты знаешь, что есть

1055
00:36:14,800 --> 00:36:16,320
здесь много места для дизайна, которое

1056
00:36:16,320 --> 00:36:19,119
еще не исследовано,

1057
00:36:19,280 --> 00:36:21,119
хорошо, так что давайте поговорим о модели трансформатора,

1058
00:36:21,119 --> 00:36:22,800
и я собираюсь сделать паузу, если

1059
00:36:22,800 --> 00:36:24,320
есть какие-то, это хороший вопрос, я могу

1060
00:36:24,320 --> 00:36:27,839
принять, я могу принять его сейчас,

1061
00:36:30,880 --> 00:36:32,240
хорошо,

1062
00:36:32,240 --> 00:36:34,640
так что трансформаторы

1063
00:36:34,640 --> 00:36:36,160
давайте  к нему,

1064
00:36:36,160 --> 00:36:37,200


1065
00:36:37,200 --> 00:36:39,280
давайте сначала посмотрим на

1066
00:36:39,280 --> 00:36:41,520
блоки декодера кодировщика трансформатора на высоком уровне,

1067
00:36:41,520 --> 00:36:43,200
это должно быть очень похоже на декодеры кодировщика,

1068
00:36:43,200 --> 00:36:45,280
которые мы видели

1069
00:36:45,280 --> 00:36:47,839
в системах

1070
00:36:47,839 --> 00:36:49,040


1071
00:36:49,040 --> 00:36:50,320
машинного перевода рекуррентных нейронных сетей

1072
00:36:50,320 --> 00:36:52,720
, которые мы видели хорошо, поэтому

1073
00:36:52,720 --> 00:36:54,800
у нас есть вложения слов,

1074
00:36:54,800 --> 00:36:56,000
которые мы  собираясь добавить в наши представления положения,

1075
00:36:56,000 --> 00:36:58,320
мы видели это,

1076
00:36:58,320 --> 00:37:00,560
и это из нашей входной последовательности у нас

1077
00:37:00,560 --> 00:37:02,480
будет последовательность блоков кодировщика, каждый

1078
00:37:02,480 --> 00:37:05,359
из которых называется кодировщиком преобразователя,

1079
00:37:05,359 --> 00:37:07,040
и тогда вы знаете, что у нас есть наше

1080
00:37:07,040 --> 00:37:08,800
слово выходной последовательности, встраивающее

1081
00:37:08,800 --> 00:37:10,400
представление положения снова

1082
00:37:10,400 --> 00:37:14,160
у нас есть  трансформаторный декодер,

1083
00:37:14,160 --> 00:37:16,480
каждый и последний уровень кодировщиков

1084
00:37:16,480 --> 00:37:17,760


1085
00:37:17,760 --> 00:37:20,320
будет использоваться на каждом уровне трансформаторного

1086
00:37:20,320 --> 00:37:22,079
декодера,

1087
00:37:22,079 --> 00:37:23,760
а затем мы получим некоторые выходные данные, некоторые

1088
00:37:23,760 --> 00:37:25,359
прогнозы хорошо, так что это выглядит красиво

1089
00:37:25,359 --> 00:37:27,280
почти то же самое на очень высоком уровне, возможно,

1090
00:37:27,280 --> 00:37:28,640
за вычетом того факта, что теперь нам нужно

1091
00:37:28,640 --> 00:37:30,320
добавить представление позиции

1092
00:37:30,320 --> 00:37:32,960
в самом начале,

1093
00:37:32,960 --> 00:37:34,400
поэтому теперь давайте посмотрим на сами эти блоки,

1094
00:37:34,400 --> 00:37:36,640
чтобы кодер и декодер

1095
00:37:36,640 --> 00:37:38,320
блокировали то, что осталось, что мы не

1096
00:37:38,320 --> 00:37:40,160
охватили правильно, потому что  мы могли бы просто

1097
00:37:40,160 --> 00:37:41,599
поместить строительные блоки, которые мы только что

1098
00:37:41,599 --> 00:37:44,400
придумали в первой части класса, в эти

1099
00:37:44,400 --> 00:37:46,560
вещи правильные кодеры нам нужно

1100
00:37:46,560 --> 00:37:48,400
наше внимание наши наши сети прямой

1101
00:37:48,400 --> 00:37:50,079
связи

1102
00:37:50,079 --> 00:37:51,680
у нас есть свои тюремные представления мы

1103
00:37:51,680 --> 00:37:53,680
получаем маскировку для декодеров правильно

1104
00:37:53,680 --> 00:37:55,440
мы можем  просто вставьте их, но

1105
00:37:55,440 --> 00:37:57,200
оказывается, что они не будут работать так

1106
00:37:57,200 --> 00:37:58,800
хорошо по сравнению с трансформаторами, поэтому что

1107
00:37:58,800 --> 00:37:59,599
осталось,

1108
00:37:59,599 --> 00:38:01,520
так что первое, что нужно, - это внимание к ключевому значению запроса,

1109
00:38:01,520 --> 00:38:02,960


1110
00:38:02,960 --> 00:38:04,880
это особый способ получить

1111
00:38:04,880 --> 00:38:07,920
векторы kq и v из одного слова

1112
00:38:07,920 --> 00:38:10,560
правильное внедрение, поэтому вместо того, чтобы иметь kq

1113
00:38:10,560 --> 00:38:13,599
и v, равные x, как на выходе

1114
00:38:13,599 --> 00:38:14,800
из последнего слоя, мы собираемся сделать

1115
00:38:14,800 --> 00:38:16,640
что-то немного большее, а

1116
00:38:16,640 --> 00:38:18,160
затем - многоголовое внимание, которое мы

1117
00:38:18,160 --> 00:38:20,480
собираемся уделить mul  расположите места в

1118
00:38:20,480 --> 00:38:22,320
одном слое, и мы увидим, что это

1119
00:38:22,320 --> 00:38:25,119
даст нам что-то вроде

1120
00:38:25,119 --> 00:38:27,599
интересного в домашнем задании позже,

1121
00:38:27,599 --> 00:38:28,720
но мы поговорим об этом немного

1122
00:38:28,720 --> 00:38:31,040
сегодня, а затем будет куча вещей,

1123
00:38:31,040 --> 00:38:32,960
которые просто помогут с  обучение

1124
00:38:32,960 --> 00:38:34,560
казалось, что их было очень сложно обучить

1125
00:38:34,560 --> 00:38:35,920
сначала многие из этих приемов очень

1126
00:38:35,920 --> 00:38:37,200
полезны, поэтому мы поговорим о

1127
00:38:37,200 --> 00:38:39,200
нормализации слоя остаточных связей и

1128
00:38:39,200 --> 00:38:41,040
масштабировании скалярного произведения все, что указано в

1129
00:38:41,040 --> 00:38:43,440
третьем маркированном

1130
00:38:43,440 --> 00:38:44,960
списке, здесь приемы, которые помогут с обучением, не

1131
00:38:44,960 --> 00:38:47,680
улучшаются  на что способна модель,

1132
00:38:47,680 --> 00:38:49,359
но они имеют решающее значение в том, что они улучшают

1133
00:38:49,359 --> 00:38:50,880
процесс обучения,

1134
00:38:50,880 --> 00:38:53,359
поэтому моделирование улучшений обоих

1135
00:38:53,359 --> 00:38:56,240
видов действительно очень важно, так что вы

1136
00:38:56,240 --> 00:38:57,200
знаете, что хорошо, что мы используем

1137
00:38:57,200 --> 00:38:58,640
самовнимание, которое является этой классной вещью

1138
00:38:58,640 --> 00:39:00,160
, которая  эти свойства, но если бы мы

1139
00:39:00,160 --> 00:39:02,480
не могли его обучить, это было бы бесполезно,

1140
00:39:02,480 --> 00:39:04,800
хорошо,

1141
00:39:04,880 --> 00:39:06,800
поэтому вот как преобразователь строит

1142
00:39:06,800 --> 00:39:10,800
ключевые векторы запроса и значений,

1143
00:39:12,480 --> 00:39:16,800
у нас есть uh x1 для xt входных векторов в

1144
00:39:16,800 --> 00:39:19,680
наш слой преобразователя, хорошо, и мы

1145
00:39:19,680 --> 00:39:21,520
пытаясь избежать здесь кодировщика трансформатора,

1146
00:39:21,520 --> 00:39:22,480


1147
00:39:22,480 --> 00:39:23,680
чтобы мы

1148
00:39:23,680 --> 00:39:25,520
знали, что вы знаете один из этих векторов на каждое слово, которое

1149
00:39:25,520 --> 00:39:27,920
вы можете сказать, и снова каждый xi

1150
00:39:27,920 --> 00:39:30,400
будет вектором в размерности d,

1151
00:39:30,400 --> 00:39:31,839
и вот как мы вычисляем

1152
00:39:31,839 --> 00:39:33,520
запросы ключей и значения,

1153
00:39:33,520 --> 00:39:36,560
которые мы собираемся  пусть каждый ключ ki, который

1154
00:39:36,560 --> 00:39:37,760
мы видели ранее

1155
00:39:37,760 --> 00:39:40,560
, равен некоторой матрице k, умноженной на xi,

1156
00:39:40,560 --> 00:39:42,560
где k равно d на d

1157
00:39:42,560 --> 00:39:44,480
правильно, так что это преобразование от

1158
00:39:44,480 --> 00:39:47,760
размерности d к размерности d,

1159
00:39:47,760 --> 00:39:49,200
мы собираемся называть это ключевой матрицей

1160
00:39:49,200 --> 00:39:50,000
k,

1161
00:39:50,000 --> 00:39:51,040
и мы собираемся  сделать то же самое

1162
00:39:51,040 --> 00:39:53,119
для запросов, хорошо, поэтому мы

1163
00:39:53,119 --> 00:39:55,920
возьмем xi, умножим его на матрицу, получим

1164
00:39:55,920 --> 00:39:57,359
вектор запроса,

1165
00:39:57,359 --> 00:39:59,599
и мы сделаем то же самое для v, хорошо,

1166
00:39:59,599 --> 00:40:00,720
так что

1167
00:40:00,720 --> 00:40:02,240
вы можете просто подключить это прямо сейчас,

1168
00:40:02,240 --> 00:40:04,319
вместо того, чтобы говорить  что все kq

1169
00:40:04,319 --> 00:40:06,480
и v все такие же, как x, все они

1170
00:40:06,480 --> 00:40:08,000
немного отличаются, потому что вы применяете

1171
00:40:08,000 --> 00:40:10,000
линейное преобразование, что это делает

1172
00:40:10,000 --> 00:40:11,359
хорошо, вы знаете,

1173
00:40:11,359 --> 00:40:13,040
вы можете думать об этом так же хорошо, как вы

1174
00:40:13,040 --> 00:40:15,920
знаете, что матрицы kq и v могут быть очень

1175
00:40:15,920 --> 00:40:17,920
разными  друг от друга прямо, и поэтому

1176
00:40:17,920 --> 00:40:20,079
они вроде гм

1177
00:40:20,079 --> 00:40:22,079
подчеркивать или разрешать использование различных аспектов

1178
00:40:22,079 --> 00:40:24,880
векторов x в каждой из

1179
00:40:24,880 --> 00:40:27,440
трех ролей, поэтому мы выписали

1180
00:40:27,440 --> 00:40:29,040
уравнения самовнимания с тремя

1181
00:40:29,040 --> 00:40:30,480
ролями, чтобы указать

1182
00:40:30,480 --> 00:40:32,000
, что с каждой из них выполняются разные вещи, так что,

1183
00:40:32,000 --> 00:40:34,800
возможно, вы знаете k  и q помогают вам

1184
00:40:34,800 --> 00:40:36,560
выяснить, где искать,

1185
00:40:36,560 --> 00:40:38,480
и поэтому они должны быть определенным образом, они

1186
00:40:38,480 --> 00:40:40,079
должны смотреть на разные части x, а

1187
00:40:40,079 --> 00:40:42,079
затем v значение,

1188
00:40:42,079 --> 00:40:44,000
возможно, вы хотите передать

1189
00:40:44,000 --> 00:40:45,359
другую информацию, чем то,

1190
00:40:45,359 --> 00:40:46,800
что на самом деле помогает вам

1191
00:40:46,800 --> 00:40:48,720
получить доступ к этому  информация,

1192
00:40:48,720 --> 00:40:50,000


1193
00:40:50,000 --> 00:40:52,000
так что это важно, как мы это делаем

1194
00:40:52,000 --> 00:40:53,839
на практике, мы вычисляем это с действительно

1195
00:40:53,839 --> 00:40:57,040
большими тензорами, так что у нас были наши x-векторы, о

1196
00:40:57,040 --> 00:40:58,400
которых мы говорили, вроде

1197
00:40:58,400 --> 00:41:00,160
слово за словами, где у вас была последовательность от

1198
00:41:00,160 --> 00:41:02,000
xix 1 до xt

1199
00:41:02,000 --> 00:41:03,760
um  теперь мы собираемся представить их все

1200
00:41:03,760 --> 00:41:06,960
как матрицу x, которая находится в нашей последовательности

1201
00:41:06,960 --> 00:41:09,200
по размерности, поэтому

1202
00:41:09,200 --> 00:41:11,040
длина последовательности на d

1203
00:41:11,040 --> 00:41:13,040
заглавная t на d,

1204
00:41:13,040 --> 00:41:15,760
и теперь, если у нас есть матрица для каждого

1205
00:41:15,760 --> 00:41:17,920
из наших ключевых запросов и значений, мы

1206
00:41:17,920 --> 00:41:19,680
идем  приложить  y, как будто мы собираемся посмотреть

1207
00:41:19,680 --> 00:41:23,119
на эти вещи xkxq и xv, которые

1208
00:41:23,119 --> 00:41:25,359
имеют ту же размерность, что и x,

1209
00:41:25,359 --> 00:41:28,319
из-за преобразований d на d,

1210
00:41:28,319 --> 00:41:29,760
поэтому как мы вычисляем

1211
00:41:29,760 --> 00:41:32,560
самовнимание, у нас есть выходной тензор,

1212
00:41:32,560 --> 00:41:34,400
который имеет ту же размерность, что и

1213
00:41:34,400 --> 00:41:35,599
вход

1214
00:41:35,599 --> 00:41:37,200
x будет равен soft max,

1215
00:41:37,200 --> 00:41:40,000
там softmax этого умножения матриц,

1216
00:41:40,000 --> 00:41:41,359
который мы получим в умножении

1217
00:41:41,359 --> 00:41:43,520
на векторы значений, поэтому

1218
00:41:43,520 --> 00:41:45,200
умножение матриц здесь вычисляет

1219
00:41:45,200 --> 00:41:46,720
сродство между ключами и запросами, которые

1220
00:41:46,720 --> 00:41:48,960
мы увидим, а затем наше усреднение,

1221
00:41:48,960 --> 00:41:51,440
что делает  которые выглядят как наглядно,

1222
00:41:51,440 --> 00:41:53,280
поэтому вы берете ключевые точечные произведения запроса,

1223
00:41:53,280 --> 00:41:56,079
поэтому этот термин здесь xq

1224
00:41:56,079 --> 00:41:58,240
xk transpose

1225
00:41:58,240 --> 00:42:00,160
дает вам все точечные произведения

1226
00:42:00,160 --> 00:42:02,640
всех t на t пар оценок внимания, поэтому

1227
00:42:02,640 --> 00:42:05,280
наши eij находятся в этой матрице прямо здесь,

1228
00:42:05,280 --> 00:42:07,280
это t на t, и это  просто большое

1229
00:42:07,280 --> 00:42:09,119
матричное умножение, поэтому вы выполняете

1230
00:42:09,119 --> 00:42:11,680
матричное умножение xq, а затем xk,

1231
00:42:11,680 --> 00:42:14,480
а затем вы получаете все скалярные произведения

1232
00:42:14,480 --> 00:42:16,560
с помощью этого хорошо, так что теперь у вас есть

1233
00:42:16,560 --> 00:42:18,640
этот большой набор оценок t на t, это то,

1234
00:42:18,640 --> 00:42:21,440
что мы хотели  m, и теперь вы можете soft

1235
00:42:21,440 --> 00:42:24,800
max, который непосредственно в качестве матрицы,

1236
00:42:24,800 --> 00:42:26,560
а затем вы знаете, что здесь выполняется

1237
00:42:26,560 --> 00:42:28,800
умножение матрицы с xv, чтобы

1238
00:42:28,800 --> 00:42:30,400
дать свой выходной вектор, так что на

1239
00:42:30,400 --> 00:42:32,240
самом деле это средневзвешенное значение,

1240
00:42:32,240 --> 00:42:33,599
которое мы видели в начале

1241
00:42:33,599 --> 00:42:34,560
класса,

1242
00:42:34,560 --> 00:42:36,160
и это вы  знаю, что здесь нет циклов for,

1243
00:42:36,160 --> 00:42:37,920
это действительно красиво, что мы говорим в

1244
00:42:37,920 --> 00:42:40,640
векторизованном формате um, и это дает нам наш результат,

1245
00:42:40,640 --> 00:42:42,720
который снова запоминает ту же размерность

1246
00:42:42,720 --> 00:42:44,800
t на d,

1247
00:42:44,800 --> 00:42:46,880
хорошо, поэтому все периоды внимания

1248
00:42:46,880 --> 00:42:49,119
затем вычисляют средние,

1249
00:42:49,119 --> 00:42:51,280
применяя оценку softmax оценок к

1250
00:42:51,280 --> 00:42:52,160


1251
00:42:52,160 --> 00:42:54,800
xv

1252
00:42:56,839 --> 00:42:59,760
matrix um, так что это все для внимания к ключевому

1253
00:42:59,760 --> 00:43:01,440
значению запроса, вот как вы

1254
00:43:01,440 --> 00:43:04,079
знаете, что мы реализуем это с помощью тензоров, а

1255
00:43:04,079 --> 00:43:06,000
затем мы рассмотрим вид следующего типа

1256
00:43:06,000 --> 00:43:06,960
um

1257
00:43:06,960 --> 00:43:08,400
, который в конечном итоге будет очень важен

1258
00:43:08,400 --> 00:43:09,760
для обучения трансформаторов на практике,

1259
00:43:09,760 --> 00:43:11,599
который является мульти  -головое внимание так

1260
00:43:11,599 --> 00:43:13,040
трансформирует кодировщика многоголовое

1261
00:43:13,040 --> 00:43:16,240
внимание, так что вопрос в том,

1262
00:43:16,240 --> 00:43:17,680
что, если мы хотим смотреть сразу на несколько

1263
00:43:17,680 --> 00:43:20,560
мест в предложении

1264
00:43:20,560 --> 00:43:22,720
, можно сделать то, что вы знаете  с

1265
00:43:22,720 --> 00:43:23,839
самовниманием с нормальным

1266
00:43:23,839 --> 00:43:26,000
самовниманием, но подумайте об этом,

1267
00:43:26,000 --> 00:43:27,599
где вы в конечном итоге смотрите на

1268
00:43:27,599 --> 00:43:29,119
себя,

1269
00:43:29,119 --> 00:43:31,280
вы в конечном итоге смотрите, где скалярные

1270
00:43:31,280 --> 00:43:34,880
произведения xi, ваша q-матрица, транспонирует

1271
00:43:34,880 --> 00:43:37,200
вашу ключевую матрицу, xj высока, так что это

1272
00:43:37,200 --> 00:43:39,760
те, которые отсортированы  из пар xij извините,

1273
00:43:39,760 --> 00:43:41,760
это пары ij, которые в конечном итоге

1274
00:43:41,760 --> 00:43:44,400
взаимодействуют друг с другом,

1275
00:43:44,400 --> 00:43:46,720
но, возможно, для какого-то запроса для какого-то слова

1276
00:43:46,720 --> 00:43:48,960
в выходном для какого-то слова вы хотите

1277
00:43:48,960 --> 00:43:50,400
сосредоточиться на других других словах в

1278
00:43:50,400 --> 00:43:52,400
предложении по разным

1279
00:43:52,400 --> 00:43:54,319
причинам, как вы  может кодировать это,

1280
00:43:54,319 --> 00:43:55,359
имея

1281
00:43:55,359 --> 00:43:58,079
несколько матриц ключей и значений запроса,

1282
00:43:58,079 --> 00:44:00,319
которые все кодируют разные вещи

1283
00:44:00,319 --> 00:44:02,400
о xi, которые они все переводят, изучать разные

1284
00:44:02,400 --> 00:44:04,560
преобразования, поэтому вместо одной

1285
00:44:04,560 --> 00:44:06,800
очереди мы получаем один k и один v, что мы

1286
00:44:06,800 --> 00:44:11,280
получаем, это aq sub lk sub lv sub l

1287
00:44:11,280 --> 00:44:13,760
все другой размерности теперь, поэтому

1288
00:44:13,760 --> 00:44:16,480
по их размерности d на d над h,

1289
00:44:16,480 --> 00:44:18,160
где h - количество головок, поэтому

1290
00:44:18,160 --> 00:44:20,560
они все еще будут применяться к матрице x,

1291
00:44:20,560 --> 00:44:22,640
но они собираются преобразовать ее

1292
00:44:22,640 --> 00:44:26,640
в меньшие размеры  Иональность d за h

1293
00:44:26,640 --> 00:44:28,880
um, а затем каждая голова внимания будет

1294
00:44:28,880 --> 00:44:30,560
выполнять натяжение независимо,

1295
00:44:30,560 --> 00:44:31,680
как будто вы только что делали это целую кучу

1296
00:44:31,680 --> 00:44:32,640
раз

1297
00:44:32,640 --> 00:44:34,880
правильно, и поэтому выход l равен soft

1298
00:44:34,880 --> 00:44:37,760
softmax, вы знаете, вот ваше qk, но

1299
00:44:37,760 --> 00:44:42,640
теперь оно в форме l  умножить на xvl, и теперь у

1300
00:44:42,640 --> 00:44:45,119
вас есть вид этих проиндексированных

1301
00:44:45,119 --> 00:44:46,560
выходов,

1302
00:44:46,560 --> 00:44:48,400
и для того, чтобы вроде бы

1303
00:44:48,400 --> 00:44:49,920
размерность выхода была равна

1304
00:44:49,920 --> 00:44:51,599
размерности входа и типа смешивания вещей,

1305
00:44:51,599 --> 00:44:53,040


1306
00:44:53,040 --> 00:44:54,319
объедините всю информацию из

1307
00:44:54,319 --> 00:44:56,720
разных головок, которые вы объединяете,

1308
00:44:56,720 --> 00:44:58,319
чтобы вывести  от одного до

1309
00:44:58,319 --> 00:45:00,800
вывода h складываем их вместе, теперь

1310
00:45:00,800 --> 00:45:03,119
размерность этого снова равна

1311
00:45:03,119 --> 00:45:05,040
размерности x,

1312
00:45:05,040 --> 00:45:07,200
а затем мы используем выученную матрицу y

1313
00:45:07,200 --> 00:45:10,400
, чтобы вроде как смешивать uh y is d

1314
00:45:10,400 --> 00:45:11,839
by d, и это результат

1315
00:45:11,839 --> 00:45:13,760
многоголовой  внимание

1316
00:45:13,760 --> 00:45:16,400
многоголовое самовнимание

1317
00:45:16,960 --> 00:45:19,280
и так потому что разные, так что каждая голова

1318
00:45:19,280 --> 00:45:21,200
может смотреть на разные вещи правильно,

1319
00:45:21,200 --> 00:45:23,359
потому что все они могут типа эээ,

1320
00:45:23,359 --> 00:45:25,200
линейные преобразования, можно сказать,

1321
00:45:25,200 --> 00:45:26,720
сосредоточиться на разных частях  векторов x

1322
00:45:26,720 --> 00:45:29,599
и um векторы значений также

1323
00:45:29,599 --> 00:45:32,880
должны быть разными,

1324
00:45:32,880 --> 00:45:34,640
так что наглядно это то, что у нас было

1325
00:45:34,640 --> 00:45:37,920
до того, как одноголовое внимание у вас было x,

1326
00:45:37,920 --> 00:45:41,760
умноженное на q, чтобы получить xq,

1327
00:45:41,760 --> 00:45:43,200
и что интересно, и вы можете видеть

1328
00:45:43,200 --> 00:45:44,800
это, вы знаете,

1329
00:45:44,800 --> 00:45:46,160
что можете  Видите это из этой диаграммы, я

1330
00:45:46,160 --> 00:45:47,920
думаю, что многоголовое внимание

1331
00:45:47,920 --> 00:45:49,920
не обязательно должно быть больше,

1332
00:45:49,920 --> 00:45:53,200
больше работы, верно, мы видели, что векторы qk и

1333
00:45:53,200 --> 00:45:55,920
l uh, извините, матрицы qk и v

1334
00:45:55,920 --> 00:45:58,560
при многоголовом натяжении имеют

1335
00:45:58,560 --> 00:46:01,119
более низкий выход  размерность, так что вот

1336
00:46:01,119 --> 00:46:03,760
два из них прямо здесь, вот q1 и q2

1337
00:46:03,760 --> 00:46:06,480
того же размера, что и q,

1338
00:46:06,480 --> 00:46:09,839
а затем вы нажимаете выходы xq1 и xq2,

1339
00:46:09,839 --> 00:46:11,119
и поэтому вы фактически выполняете тот же

1340
00:46:11,119 --> 00:46:13,040
объем вычислений, что и раньше,

1341
00:46:13,040 --> 00:46:14,640
но теперь вы вроде как делаете, у вас

1342
00:46:14,640 --> 00:46:15,920
разные  распределения задержек для

1343
00:46:15,920 --> 00:46:17,280
каждой из разных голов, так что это

1344
00:46:17,280 --> 00:46:19,680
довольно круто,

1345
00:46:19,680 --> 00:46:21,760
хорошо,

1346
00:46:21,760 --> 00:46:23,200


1347
00:46:23,200 --> 00:46:25,040
так что это основные

1348
00:46:25,040 --> 00:46:27,200
различия в моделировании, правильно мы сделали

1349
00:46:27,200 --> 00:46:29,040
внимание к значению ключевого запроса

1350
00:46:29,040 --> 00:46:31,359


1351
00:46:31,359 --> 00:46:34,319
, вот как мы

1352
00:46:34,319 --> 00:46:36,960
получили ключевые запросы и значения из

1353
00:46:36,960 --> 00:46:37,920
вектора x  s,

1354
00:46:37,920 --> 00:46:40,000
и мы увидели, как реализовать это в

1355
00:46:40,000 --> 00:46:42,880
матрицах, на которые мы смотрим,

1356
00:46:42,880 --> 00:46:44,319
а затем мы

1357
00:46:44,319 --> 00:46:45,440
посмотрели

1358
00:46:45,440 --> 00:46:47,440
на многогранное внимание, которое позволяет

1359
00:46:47,440 --> 00:46:48,640
нам смотреть в разные места

1360
00:46:48,640 --> 00:46:49,760


1361
00:46:49,760 --> 00:46:51,760
в последовательности, чтобы

1362
00:46:51,760 --> 00:46:53,119
теперь иметь больше гибкости в пределах

1363
00:46:53,119 --> 00:46:54,240
данного слоя.

1364
00:46:54,240 --> 00:46:55,280
мы собираемся поговорить о наших

1365
00:46:55,280 --> 00:46:56,480
тренировочных приемах,

1366
00:46:56,480 --> 00:46:58,400
они действительно важны, оказывается,

1367
00:46:58,400 --> 00:47:00,079
и так

1368
00:47:00,079 --> 00:47:01,440
что да,

1369
00:47:01,440 --> 00:47:03,440
подумав о них, я думаю, это

1370
00:47:03,440 --> 00:47:04,800
то, чего мы недостаточно делаем

1371
00:47:04,800 --> 00:47:06,400
в полевых условиях, и поэтому давайте действительно пройдемся

1372
00:47:06,400 --> 00:47:08,400
по ним, так что остаточные связи

1373
00:47:08,400 --> 00:47:10,720
остаточные соединения были вокруг

1374
00:47:10,720 --> 00:47:12,480
остаточных соединений, вы можете думать о

1375
00:47:12,480 --> 00:47:14,720
них как о помощи в обучении модели

1376
00:47:14,720 --> 00:47:16,720
по ряду причин, давайте

1377
00:47:16,720 --> 00:47:18,319
сначала посмотрим, что они делают, наше

1378
00:47:18,319 --> 00:47:19,520
остаточное соединение выглядит так, поэтому у

1379
00:47:19,520 --> 00:47:21,680
вас есть нормальный слой

1380
00:47:21,680 --> 00:47:22,640
x

1381
00:47:22,640 --> 00:47:24,720
в некотором слое ii  представляет собой

1382
00:47:24,720 --> 00:47:26,240
вид слоя и глубины

1383
00:47:26,240 --> 00:47:28,640
в сети,

1384
00:47:28,640 --> 00:47:31,680
так что xi равно некоторому слою xi

1385
00:47:31,680 --> 00:47:33,599
минус один, так что вы были правы, вы имели, я

1386
00:47:33,599 --> 00:47:35,200
не знаю, что этот слой делает

1387
00:47:35,200 --> 00:47:36,480
обязательно,

1388
00:47:36,480 --> 00:47:38,720
но этот слой  эр - это функция

1389
00:47:38,720 --> 00:47:40,960
предыдущего слоя, хорошо,

1390
00:47:40,960 --> 00:47:43,680
и у вас это получилось, поэтому я снова

1391
00:47:43,680 --> 00:47:45,280
хочу абстрагироваться от того, что

1392
00:47:45,280 --> 00:47:47,520
делает слой, но вы просто передаете его

1393
00:47:47,520 --> 00:47:49,760
через остаточное соединение, делает что-то

1394
00:47:49,760 --> 00:47:51,520
очень простое, он говорит,

1395
00:47:51,520 --> 00:47:54,240
хорошо, я иду  чтобы взять функцию, которую

1396
00:47:54,240 --> 00:47:55,680
я вычислял на моем предыдущем уровне

1397
00:47:55,680 --> 00:47:57,280
раньше,

1398
00:47:57,280 --> 00:47:58,880
и я собираюсь добавить ее к предыдущему

1399
00:47:58,880 --> 00:48:00,720
слою, поэтому теперь

1400
00:48:00,720 --> 00:48:02,880
xi не равно слою xi минус 1,

1401
00:48:02,880 --> 00:48:06,400
он равен xi минус 1 плюс слой

1402
00:48:06,400 --> 00:48:08,720
xi минус 1. this  Это

1403
00:48:08,720 --> 00:48:10,559
остаточные связи,

1404
00:48:10,559 --> 00:48:13,680
гм, и интуиция подсказывает, что, как и до того, как

1405
00:48:13,680 --> 00:48:16,160
вы начали изучать что-

1406
00:48:16,160 --> 00:48:18,319
то вроде, у вас есть такое представление, что вы

1407
00:48:18,319 --> 00:48:21,760
должны изучать только то, как слой i

1408
00:48:21,760 --> 00:48:23,520
должен отличаться от слоя i минус

1409
00:48:23,520 --> 00:48:25,839
один,

1410
00:48:26,319 --> 00:48:27,599
вместо того, чтобы изучать с нуля, что он

1411
00:48:27,599 --> 00:48:30,000
должен  выглядит так, что это значение здесь,

1412
00:48:30,000 --> 00:48:31,920
слой xi минус один,

1413
00:48:31,920 --> 00:48:34,000
должно быть чем-то в некотором смысле, и

1414
00:48:34,000 --> 00:48:35,440
вам нужно узнать, чем оно отличается

1415
00:48:35,440 --> 00:48:37,599
от предыдущего слоя, это своего

1416
00:48:37,599 --> 00:48:40,640
рода хорошее индуктивное смещение, поэтому здесь вы можете

1417
00:48:40,640 --> 00:48:42,079
представить его как yo  у вас этот

1418
00:48:42,079 --> 00:48:45,359
слой x минус 1 идет на слой,

1419
00:48:45,359 --> 00:48:47,040
он также проходит и просто

1420
00:48:47,040 --> 00:48:49,599
добавляется теперь подумайте о градиентах, правильно

1421
00:48:49,599 --> 00:48:51,920
мы говорим об исчезающих градиентах

1422
00:48:51,920 --> 00:48:53,280
, это проблема,

1423
00:48:53,280 --> 00:48:54,160


1424
00:48:54,160 --> 00:48:56,160
вы знаете, что градиент этой

1425
00:48:56,160 --> 00:48:58,240
связи здесь прекрасен, верно  даже

1426
00:48:58,240 --> 00:49:00,079
если все насыщает

1427
00:49:00,079 --> 00:49:02,000
э-э, все ваши сигмоиды, как вы знаете, насыщаются

1428
00:49:02,000 --> 00:49:03,760
или все ваши рельсы

1429
00:49:03,760 --> 00:49:05,680
отрицательны, поэтому градиенты все равны нулю,

1430
00:49:05,680 --> 00:49:07,440
правильно, вы получаете градиенты,

1431
00:49:07,440 --> 00:49:08,800
распространяющиеся обратно через остальную часть

1432
00:49:08,800 --> 00:49:10,240
сети, в любом случае через это соединение

1433
00:49:10,240 --> 00:49:12,240
здесь, это довольно круто,

1434
00:49:12,240 --> 00:49:15,599
получается  чтобы быть очень полезным

1435
00:49:15,599 --> 00:49:18,160
и просто для быстрой визуализации,

1436
00:49:18,160 --> 00:49:19,760
этот сюжет никогда не перестает выглядеть

1437
00:49:19,760 --> 00:49:22,800
действительно очень интересно, вот это

1438
00:49:22,800 --> 00:49:24,800
своего рода

1439
00:49:24,800 --> 00:49:27,599
визуализация потерянного пейзажа, так что

1440
00:49:27,599 --> 00:49:30,319
каждая точка на 2-й плоскости

1441
00:49:30,319 --> 00:49:32,319
похожа на гм,

1442
00:49:32,319 --> 00:49:34,960
это своего рода  настройки

1443
00:49:34,960 --> 00:49:36,559
параметров вашей сети, а затем

1444
00:49:36,559 --> 00:49:39,680
ось z - это потеря сети, для

1445
00:49:39,680 --> 00:49:41,760
которой она оптимизируется, и

1446
00:49:41,760 --> 00:49:43,839
вот сетевое примечание без r  esiduals,

1447
00:49:43,839 --> 00:49:45,920
и вы стохастический градиентный спуск,

1448
00:49:45,920 --> 00:49:47,200
и вам как бы нужно найти локальный

1449
00:49:47,200 --> 00:49:48,640
минимум, и действительно сложно найти

1450
00:49:48,640 --> 00:49:50,079
хороший локальный минимум

1451
00:49:50,079 --> 00:49:53,119
um, а затем с остаточной сетью,

1452
00:49:53,119 --> 00:49:54,960
вы знаете,

1453
00:49:54,960 --> 00:49:56,400
это намного более плавно, поэтому вы можете представить,

1454
00:49:56,400 --> 00:49:57,920
как сортируется стохастический градиентный спуск

1455
00:49:57,920 --> 00:50:00,720
спуститься сюда к этому хорошему

1456
00:50:00,720 --> 00:50:03,440
очень низкому локальному минимуму, это

1457
00:50:03,440 --> 00:50:05,599
статья, в которой пытались объяснить, почему

1458
00:50:05,599 --> 00:50:08,720
остаточные связи так полезны,

1459
00:50:08,720 --> 00:50:10,160
так что это может быть интуиция, которая может

1460
00:50:10,160 --> 00:50:12,720
быть вам полезна, так что вы знаете, что

1461
00:50:12,720 --> 00:50:14,800
это так называемый  ландшафт потерь,

1462
00:50:14,800 --> 00:50:16,480
так что это остаточные

1463
00:50:16,480 --> 00:50:17,839
связи,

1464
00:50:17,839 --> 00:50:18,720


1465
00:50:18,720 --> 00:50:21,040
и они кажутся простыми, но многие

1466
00:50:21,040 --> 00:50:22,640
простые идеи в конечном итоге оказываются очень полезными

1467
00:50:22,640 --> 00:50:24,319
в глубоком обучении,

1468
00:50:24,319 --> 00:50:25,040
поэтому

1469
00:50:25,040 --> 00:50:27,839
в нормализации

1470
00:50:27,839 --> 00:50:30,079
слоев мы делаем что-то

1471
00:50:30,079 --> 00:50:32,000
похожее, мы пытаемся помочь сети

1472
00:50:32,000 --> 00:50:34,720
лучше обучаться  гм, но мы делаем это с помощью

1473
00:50:34,720 --> 00:50:36,880
совершенно другой интуиции,

1474
00:50:36,880 --> 00:50:38,160
поэтому

1475
00:50:38,160 --> 00:50:40,720
считается, что нормализация уровня говорит

1476
00:50:40,720 --> 00:50:42,800
в разное время в моей сети, когда

1477
00:50:42,800 --> 00:50:44,400
я ее тренирую, я делаю прямой

1478
00:50:44,400 --> 00:50:46,720
проход, там много  вариаций

1479
00:50:46,720 --> 00:50:48,800
в том, как выглядит прямой проход,

1480
00:50:48,800 --> 00:50:50,640
и многие из них неинформативны

1481
00:50:50,640 --> 00:50:53,440
, и это может повредить обучению,

1482
00:50:53,440 --> 00:50:56,960
но если мы нормализуем внутри слоя

1483
00:50:56,960 --> 00:51:00,079
до единичного среднего значения и стандартного

1484
00:51:00,079 --> 00:51:02,240
отклонения, то такой вид сокращает

1485
00:51:02,240 --> 00:51:03,760
все это на  вся эта

1486
00:51:03,760 --> 00:51:06,400
неинформативная вариация

1487
00:51:06,400 --> 00:51:08,000
и информативная вариация того,

1488
00:51:08,000 --> 00:51:09,200
как единицы отличались друг от

1489
00:51:09,200 --> 00:51:11,520
друга, сохраняется,

1490
00:51:11,520 --> 00:51:13,920
ммм, так что также думают, что

1491
00:51:13,920 --> 00:51:15,280
успех Лары Норман был

1492
00:51:15,280 --> 00:51:17,280
большим успехом нормы уровня был на

1493
00:51:17,280 --> 00:51:19,520
самом деле благодаря

1494
00:51:19,520 --> 00:51:21,839
помощи  нормализовать градиенты каждого

1495
00:51:21,839 --> 00:51:24,079
слоя, это недавняя работа,

1496
00:51:24,079 --> 00:51:26,319
поэтому давайте поговорим о том, как она реализована,

1497
00:51:26,319 --> 00:51:28,400
поэтому мы вернемся к x и не

1498
00:51:28,400 --> 00:51:30,480
будем индексировать его здесь, поэтому просто x - это некоторый

1499
00:51:30,480 --> 00:51:32,800
вектор, какой-то вектор слова в нашем

1500
00:51:32,800 --> 00:51:35,359
преобразователе.

1501
00:51:35,359 --> 00:51:36,559
собираюсь вычислить

1502
00:51:36,559 --> 00:51:37,920
оценку среднего

1503
00:51:37,920 --> 00:51:40,319
хорошо, просто суммируя скрытые

1504
00:51:40,319 --> 00:51:42,480
единицы, мы собираемся вычислить

1505
00:51:42,480 --> 00:51:44,960
оценку стандартного отклонения

1506
00:51:44,960 --> 00:51:46,640
аналогично, так, как если бы вы взяли один

1507
00:51:46,640 --> 00:51:49,119
rd-вектор, вы просто суммируете их  вы

1508
00:51:49,119 --> 00:51:51,440
вычисляете среднее значение, которое вы вычисляете, хорошо

1509
00:51:51,440 --> 00:51:52,640
ли вы оцениваете среднее значение, которое вы оцениваете

1510
00:51:52,640 --> 00:51:54,079
стандартное отклонение,

1511
00:51:54,079 --> 00:51:56,000
хорошо,

1512
00:51:56,000 --> 00:51:57,599
теперь

1513
00:51:57,599 --> 00:51:59,520
вы

1514
00:51:59,520 --> 00:52:02,240
также потенциально, и это необязательный

1515
00:52:02,240 --> 00:52:05,119
элемент изучения элементов поэлементного усиления и параметров смещения,

1516
00:52:05,119 --> 00:52:07,359
чтобы попытаться изменить масштаб,

1517
00:52:07,359 --> 00:52:09,520
если определенные скрытые единицы вроде

1518
00:52:09,520 --> 00:52:12,960
должен иметь большее значение в целом или

1519
00:52:12,960 --> 00:52:15,280
если вы знаете, что он должен быть мультипликативно

1520
00:52:15,280 --> 00:52:17,520
больше в общем, так что это

1521
00:52:17,520 --> 00:52:19,839
векторы в rd, так же как x был вектором

1522
00:52:19,839 --> 00:52:21,599
в rd,

1523
00:52:21,599 --> 00:52:23,119
а затем вот, что вычисляет нормализация уровня, у

1524
00:52:23,119 --> 00:52:24,319


1525
00:52:24,319 --> 00:52:26,240
вас есть ваш результат, который

1526
00:52:26,240 --> 00:52:27,680
будет просто rd

1527
00:52:27,680 --> 00:52:30,800
как ваш ввод в порядке, и вы берете

1528
00:52:30,800 --> 00:52:33,520
свой вектор x, вы

1529
00:52:33,520 --> 00:52:35,920
вычитаете среднее значение из всех из них, которые вы

1530
00:52:35,920 --> 00:52:37,920
делите на

1531
00:52:37,920 --> 00:52:40,240
стандартное отклонение,

1532
00:52:40,240 --> 00:52:41,920
о, да, извините, это не должен быть

1533
00:52:41,920 --> 00:52:44,160
квадратный корень um, а затем вы добавляете

1534
00:52:44,160 --> 00:52:46,400
небольшой эпсилон,

1535
00:52:46,400 --> 00:52:48,240
если стандарт  отклонение

1536
00:52:48,240 --> 00:52:49,839
становится очень маленьким, вы не хотите,

1537
00:52:49,839 --> 00:52:52,480
чтобы знаменатель стал

1538
00:52:52,480 --> 00:52:54,240
слишком маленьким, потому что тогда вы получаете огромные

1539
00:52:54,240 --> 00:52:55,920
числа, а затем ваша сеть

1540
00:52:55,920 --> 00:52:58,640
переходит в nan и не трогает  ну хорошо,

1541
00:52:58,640 --> 00:53:01,760
так что у вас есть какая-то толерантность тут

1542
00:53:01,760 --> 00:53:03,920
и тогда, поэтому вы нормализуете здесь и затем

1543
00:53:03,920 --> 00:53:06,079
наши поэлементные прирост и смещение теперь

1544
00:53:06,079 --> 00:53:07,839
помните, что эта дробь

1545
00:53:07,839 --> 00:53:09,680
x - вектор, все делается

1546
00:53:09,680 --> 00:53:11,680
здесь поэлементно, так что

1547
00:53:11,680 --> 00:53:12,880
это наш  d,

1548
00:53:12,880 --> 00:53:14,079
а затем у вас есть это поэлементное

1549
00:53:14,079 --> 00:53:16,079
умножение этого произведения Хадамара

1550
00:53:16,079 --> 00:53:17,839
на ваше усиление,

1551
00:53:17,839 --> 00:53:19,599
затем вы добавляете смещение,

1552
00:53:19,599 --> 00:53:21,839
необходимы ли усиление и смещение

1553
00:53:21,839 --> 00:53:22,880


1554
00:53:22,880 --> 00:53:24,800
, неясно, эта статья здесь предполагает, что

1555
00:53:24,800 --> 00:53:26,480
они бесполезны,

1556
00:53:26,480 --> 00:53:28,559
но они часто используются, так

1557
00:53:28,559 --> 00:53:30,960
что вроде как  инженерный вопрос на

1558
00:53:30,960 --> 00:53:32,319
этом этапе и научный вопрос

1559
00:53:32,319 --> 00:53:34,400
, можем ли мы понять, почему в

1560
00:53:34,400 --> 00:53:36,400
целом эм,

1561
00:53:36,400 --> 00:53:38,960
но да, это нормализация слоев, и в

1562
00:53:38,960 --> 00:53:41,520
конечном итоге это очень важно в

1563
00:53:41,520 --> 00:53:44,079
трансформаторах, вы удалите это, и они

1564
00:53:44,079 --> 00:53:46,720
действительно не очень хорошо тренируются,

1565
00:53:46,720 --> 00:53:49,200
хорошо, так что  это наш второй, наш

1566
00:53:49,200 --> 00:53:50,720
второй трюк,

1567
00:53:50,720 --> 00:53:53,920
третий трюк,

1568
00:53:54,079 --> 00:53:55,839
вероятно, самый простой, но это

1569
00:53:55,839 --> 00:53:58,400
полезно знать, и это просто

1570
00:53:58,400 --> 00:54:00,079
вы можете назвать это масштабируемым скалярным произведением

1571
00:54:00,079 --> 00:54:01,920
внимания,

1572
00:54:01,920 --> 00:54:03,440
потому что мы собираемся масштабировать  точечные

1573
00:54:03,440 --> 00:54:06,160
продукты вроде так хорошо, так

1574
00:54:06,160 --> 00:54:07,440
что мы собираемся сделать это у нас будет

1575
00:54:07,440 --> 00:54:09,599
эта интуиция,

1576
00:54:09,599 --> 00:54:12,079
что наша размерность d

1577
00:54:12,079 --> 00:54:14,160
в действительно больших нейронных сетях

1578
00:54:14,160 --> 00:54:16,240
станет очень большой,

1579
00:54:16,240 --> 00:54:17,680
так что, возможно, наш скрытый слой в  наш

1580
00:54:17,680 --> 00:54:19,520
трансформатор - тысяча, две

1581
00:54:19,520 --> 00:54:21,119
тысячи или три тысячи, в любом случае

1582
00:54:21,119 --> 00:54:23,520
он становится большим,

1583
00:54:23,520 --> 00:54:25,440
и когда размерность становится

1584
00:54:25,440 --> 00:54:27,440
большой, точечные произведения между векторами

1585
00:54:27,440 --> 00:54:29,440
имеют тенденцию становиться большими,

1586
00:54:29,440 --> 00:54:31,520
поэтому, например, если вы знаете, возьмите

1587
00:54:31,520 --> 00:54:33,920
скалярное произведение между двумя случайными векторами

1588
00:54:33,920 --> 00:54:35,839
в rd

1589
00:54:35,839 --> 00:54:37,680
он растет довольно быстро, их скалярное произведение

1590
00:54:37,680 --> 00:54:39,680
растет довольно быстро, теперь векторы

1591
00:54:39,680 --> 00:54:41,839
случайны в преобразователях, ну, они не являются

1592
00:54:41,839 --> 00:54:43,599
однородными, случайными, но вы знаете, что можете

1593
00:54:43,599 --> 00:54:44,559
себе представить, что существует много

1594
00:54:44,559 --> 00:54:46,480
вариаций, и в целом, поскольку

1595
00:54:46,480 --> 00:54:48,079
размерность растет, все эти скалярные

1596
00:54:48,079 --> 00:54:50,000
произведения  становится довольно большим,

1597
00:54:50,000 --> 00:54:52,000
и это может стать проблемой по

1598
00:54:52,000 --> 00:54:54,000
следующей причине: мы берем все

1599
00:54:54,000 --> 00:54:56,559
эти точечные продукты напрямую и

1600
00:54:56,559 --> 00:54:58,960
помещаем их в softmax,

1601
00:54:58,960 --> 00:55:01,040
так что если есть  вариации

1602
00:55:01,040 --> 00:55:02,319
в скалярных произведениях, а некоторые из них

1603
00:55:02,319 --> 00:55:03,520
очень велики,

1604
00:55:03,520 --> 00:55:06,559
тогда softmax может стать очень пиковым,

1605
00:55:06,559 --> 00:55:08,720
вкладывая большую часть или вы знаете, да,

1606
00:55:08,720 --> 00:55:10,960
большую часть его вероятностной массы на небольшое

1607
00:55:10,960 --> 00:55:12,480
количество вещей, что делает

1608
00:55:12,480 --> 00:55:14,400
градиенты маленькими для всего остального,

1609
00:55:14,400 --> 00:55:16,640
эффективно  правильно, потому что

1610
00:55:16,640 --> 00:55:18,799
softmax пытается быть в порядке, это мягкий аргумент max,

1611
00:55:18,799 --> 00:55:20,480
так что он вроде говорит, какой

1612
00:55:20,480 --> 00:55:23,040
из них похож на max, или вы знаете

1613
00:55:23,040 --> 00:55:25,200
вес этого типа относительно того, насколько

1614
00:55:25,200 --> 00:55:26,319
они близки к максимальному значению

1615
00:55:26,319 --> 00:55:28,720
функции um и  поэтому, если некоторые из них

1616
00:55:28,720 --> 00:55:30,880
очень-очень большие, вы как бы просто

1617
00:55:30,880 --> 00:55:32,720
обнуляете связи со всем, на что

1618
00:55:32,720 --> 00:55:34,240
не обращают внимания, что имеет низкое

1619
00:55:34,240 --> 00:55:36,470
распределение вероятностей и гм

1620
00:55:36,470 --> 00:55:37,920
[Музыка],

1621
00:55:37,920 --> 00:55:40,160
и тогда они не получают градиентов,

1622
00:55:40,160 --> 00:55:41,680
и вот размер напряжения

1623
00:55:41,680 --> 00:55:43,839
операция, которую мы видели, хорошо, я взял

1624
00:55:43,839 --> 00:55:45,440
это вариант с несколькими головами прямо здесь,

1625
00:55:45,440 --> 00:55:46,799
потому что у нас есть индексы на

1626
00:55:46,799 --> 00:55:51,440
выходе у меня есть индексы на qk и v,

1627
00:55:51,440 --> 00:55:53,440
и все, что я собираюсь сделать, это я  собираюсь

1628
00:55:53,440 --> 00:55:55,359
сказать хорошо ты

1629
00:55:55,359 --> 00:55:56,480
знаешь й  Я собираюсь расставить точки

1630
00:55:56,480 --> 00:55:58,880
вместе, знаете ли вы, что векторы

1631
00:55:58,880 --> 00:56:02,079
размерности d над h,

1632
00:56:02,079 --> 00:56:03,280
из-за

1633
00:56:03,280 --> 00:56:06,000
снова многоголого внимания,

1634
00:56:06,000 --> 00:56:08,400
и, чтобы не дать им вырасти,

1635
00:56:08,400 --> 00:56:10,160
скалярные произведения станут слишком большими,

1636
00:56:10,160 --> 00:56:11,920
я просто собираюсь  чтобы разделить

1637
00:56:11,920 --> 00:56:13,680
все мои оценки,

1638
00:56:13,680 --> 00:56:18,319
так что помните, здесь xqk top x top находится

1639
00:56:18,319 --> 00:56:20,400
на t матрице оценок, вы собираетесь

1640
00:56:20,400 --> 00:56:23,359
разделить их все на d на h,

1641
00:56:23,359 --> 00:56:27,119
и по мере того, как d растет, d на h растет вправо,

1642
00:56:27,119 --> 00:56:29,359
и поэтому ваши точечные продукты не  не

1643
00:56:29,359 --> 00:56:32,000
растут, и это в конечном итоге оказывается также полезным.

1644
00:56:32,000 --> 00:56:34,160


1645
00:56:35,680 --> 00:56:38,000
Хорошо,

1646
00:56:38,559 --> 00:56:41,200
любые вопросы,

1647
00:56:43,040 --> 00:56:45,200
да, Джон, не могли бы

1648
00:56:45,200 --> 00:56:47,040
вы задать интересный вопрос,

1649
00:56:47,040 --> 00:56:49,359
когда вы играете аккорд b.

1650
00:56:49,359 --> 00:56:51,760


1651
00:56:51,760 --> 00:56:52,799


1652
00:56:52,799 --> 00:56:54,720
первый слой, или вы делаете

1653
00:56:54,720 --> 00:56:57,200
маскировку также в заметках слоя,

1654
00:56:57,200 --> 00:56:59,599
да, хорошо, хорошо, так что

1655
00:56:59,599 --> 00:57:02,079
если бы мы

1656
00:57:02,079 --> 00:57:03,920


1657
00:57:03,920 --> 00:57:06,880
выполняли маскировку только на первом слое, мы

1658
00:57:06,880 --> 00:57:08,559
бы получили утечку информации в более

1659
00:57:08,559 --> 00:57:09,920
поздних слоях,

1660
00:57:09,920 --> 00:57:12,720
поэтому, если мы посмотрим на

1661
00:57:12,720 --> 00:57:15,520
этот

1662
00:57:15,520 --> 00:57:16,880
возглас,

1663
00:57:16,880 --> 00:57:18,400
мы, если мы  должны были снова взглянуть на эту диаграмму,

1664
00:57:18,400 --> 00:57:20,160
так что вот первый

1665
00:57:20,160 --> 00:57:22,079
слоя декодера, и мы сказали, что есть

1666
00:57:22,079 --> 00:57:23,839
правильное маскирование, и вы можете смотреть на

1667
00:57:23,839 --> 00:57:25,599
любое из состояний кодировщика, и вы

1668
00:57:25,599 --> 00:57:28,000
можете смотреть только на предыдущие слова

1669
00:57:28,000 --> 00:57:30,880
в декодере на втором слое, если

1670
00:57:30,880 --> 00:57:32,240
я  внезапно позволил взглянуть на

1671
00:57:32,240 --> 00:57:34,400
все будущие слова сейчас,

1672
00:57:34,400 --> 00:57:35,680
эй, хотя я этого не делал в первом

1673
00:57:35,680 --> 00:57:37,119
слое, это так же хорошо, что я могу

1674
00:57:37,119 --> 00:57:38,880
во втором слое, и поэтому я могу просто научиться

1675
00:57:38,880 --> 00:57:40,799
смотреть прямо на то, что

1676
00:57:40,799 --> 00:57:42,319
должно быть мое слово  так что каждый отдельный

1677
00:57:42,319 --> 00:57:45,040
слой декодера должен иметь эту маскировку, иначе

1678
00:57:45,040 --> 00:57:46,079
это будет

1679
00:57:46,079 --> 00:57:47,520
спорным, как будто вы не замаскировали его

1680
00:57:47,520 --> 00:57:50,319
вообще эффективно,

1681
00:57:51,839 --> 00:57:54,839
спасибо,

1682
00:58:03,760 --> 00:58:07,520
хорошо, так что масштабируйте точечный продукт в сумке, которую

1683
00:58:07,520 --> 00:58:09,920
мы получили, так что давайте вернемся к нашему

1684
00:58:09,920 --> 00:58:12,559
Полная структура декодера кодировщика трансформатора

1685
00:58:12,559 --> 00:58:14,160
мы рассмотрели

1686
00:58:14,160 --> 00:58:15,839
сами блоки

1687
00:58:15,839 --> 00:58:18,640
кодировщика, так что давайте немного расширим один из

1688
00:58:18,640 --> 00:58:21,040
этих масштабов и улучшим, и

1689
00:58:21,040 --> 00:58:22,400
у нас есть наши представления позиций встраивания слов,

1690
00:58:22,400 --> 00:58:25,040
и сначала мы рассмотрим

1691
00:58:25,040 --> 00:58:27,119
это многоголовым вниманием,

1692
00:58:27,119 --> 00:58:28,559
так что  мы видели,

1693
00:58:28,559 --> 00:58:30,880
что пропускаем его через остаточный слой  и

1694
00:58:30,880 --> 00:58:32,640
норма слоя,

1695
00:58:32,640 --> 00:58:34,640
так что у вас есть слово, встраивающее

1696
00:58:34,640 --> 00:58:36,799
представления поршня, проходящие через

1697
00:58:36,799 --> 00:58:39,040
остаточное соединение здесь,

1698
00:58:39,040 --> 00:58:40,319
а также проходящие через многоголовое

1699
00:58:40,319 --> 00:58:41,359
задержание,

1700
00:58:41,359 --> 00:58:43,440
добавьте их норму уровня,

1701
00:58:43,440 --> 00:58:45,839
затем вы поместите результат этого

1702
00:58:45,839 --> 00:58:48,000
через сеть с прямой

1703
00:58:48,000 --> 00:58:49,599
связью, а там должен быть  стрелка между

1704
00:58:49,599 --> 00:58:52,480
прямой связью и следующим остаточным слоем,

1705
00:58:52,480 --> 00:58:54,000
но выход этого остатка и

1706
00:58:54,000 --> 00:58:56,559
нормы слоя добавляется к этому остатку

1707
00:58:56,559 --> 00:58:58,079
и норме слоя вместе с

1708
00:58:58,079 --> 00:59:00,000
выходом прямой связи,

1709
00:59:00,000 --> 00:59:01,599
а затем выход этого остатка и

1710
00:59:01,599 --> 00:59:03,280
нормы слоя является выходом

1711
00:59:03,280 --> 00:59:05,200
блок кодировщика трансформатора,

1712
00:59:05,200 --> 00:59:06,640
поэтому, когда у нас был

1713
00:59:06,640 --> 00:59:08,640
каждый из этих кодировщиков

1714
00:59:08,640 --> 00:59:10,079
внутри, каждый из них был именно

1715
00:59:10,079 --> 00:59:11,440
таким, и мы видели все эти строительные

1716
00:59:11,440 --> 00:59:13,280
блоки раньше,

1717
00:59:13,280 --> 00:59:16,400
и это многоголовое

1718
00:59:16,720 --> 00:59:18,880
масштабируемое скалярное произведение. Внимание, я опускаю

1719
00:59:18,880 --> 00:59:20,960
масштабированное слово,

1720
00:59:20,960 --> 00:59:22,880
но да, так что это  это блок,

1721
00:59:22,880 --> 00:59:24,720
и вы любопытно замечаете, как вы

1722
00:59:24,720 --> 00:59:26,799
выполняете остаточную норму и норму слоев после

1723
00:59:26,799 --> 00:59:28,240
начального мультиголовка напряжения, а

1724
00:59:28,240 --> 00:59:31,960
также после прямой связи.

1725
00:59:32,000 --> 00:59:34,720
Итак, каждый из них просто

1726
00:59:34,720 --> 00:59:37,040
идентичен, разные параметры

1727
00:59:37,040 --> 00:59:39,440
для разных слоев, но те же самые

1728
00:59:39,440 --> 00:59:41,040
вещи, которые мы видели

1729
00:59:41,040 --> 00:59:42,559
сейчас, давайте посмотрим на блок декодера трансформатора,

1730
00:59:42,559 --> 00:59:45,200


1731
00:59:45,200 --> 00:59:48,000
так что это на самом деле более сложный, в

1732
00:59:48,000 --> 00:59:49,920
частности, у вас есть эта замаскированная

1733
00:59:49,920 --> 00:59:51,760
многоголовое самовнимание,

1734
00:59:51,760 --> 00:59:53,119
и теперь помните, что это не только

1735
00:59:53,119 --> 00:59:54,240
для первого, это для всех

1736
00:59:54,240 --> 00:59:55,599
блоков трансформатора, поэтому у нас есть массовое

1737
00:59:55,599 --> 00:59:57,040
многоголовое самовнимание, когда мы

1738
00:59:57,040 --> 00:59:58,720
не можем смотреть в будущее, потому что мы

1739
00:59:58,720 --> 01:00:01,200
добавили отрицательную бесконечность к отрицательной

1740
01:00:01,200 --> 01:00:04,160
бесконечности к остаточной оценке сродства

1741
01:00:04,160 --> 01:00:06,240
и норме слоя, как мы это делали для

1742
01:00:06,240 --> 01:00:08,319
э-э для кодировщика,

1743
01:00:08,319 --> 01:00:10,400
теперь у нас есть перекрестное внимание с несколькими головками,

1744
01:00:10,400 --> 01:00:11,839
поэтому это соединение с кодировщиком трансформатора

1745
01:00:11,839 --> 01:00:12,880


1746
01:00:12,880 --> 01:00:14,480
это на самом деле очень похоже на то, что мы видели

1747
01:00:14,480 --> 01:00:15,920
вы знаете, что

1748
01:00:15,920 --> 01:00:16,559


1749
01:00:16,559 --> 01:00:18,240
до сих пор во внимании мы обращаемся

1750
01:00:18,240 --> 01:00:21,760
от декодера к кодеру,

1751
01:00:21,760 --> 01:00:23,440
поэтому на самом деле у нас в каждом

1752
01:00:23,440 --> 01:00:26,079
блоке декодера трансформатора у нас есть две разные

1753
01:00:26,079 --> 01:00:28,480
функции внимания,

1754
01:00:28,480 --> 01:00:31,200
так что мы делаем перекрестное внимание,

1755
01:00:31,200 --> 01:00:32,799
мы добавляем resu  lt остатка и

1756
01:00:32,799 --> 01:00:34,960
нормы слоя

1757
01:00:34,960 --> 01:00:37,280
до следующего остатка и нормы уровня uh

1758
01:00:37,280 --> 01:00:38,480
вместе с нормой перекрестного внимания с несколькими головками.

1759
01:00:38,480 --> 01:00:40,000


1760
01:00:40,000 --> 01:00:42,319


1761
01:00:42,319 --> 01:00:44,640


1762
01:00:44,640 --> 01:00:48,240


1763
01:00:48,240 --> 01:00:50,480
приближается, так что вы

1764
01:00:50,480 --> 01:00:52,559
знаете, что xi минус единица - это остаточная близкая к

1765
01:00:52,559 --> 01:00:54,880
норме, здесь переходит в эту вместе

1766
01:00:54,880 --> 01:00:57,040
с прямой связью, и поэтому вы можете думать об

1767
01:00:57,040 --> 01:00:58,480
остатке, и их норма приходит

1768
01:00:58,480 --> 01:00:59,680
после каждого из интересных вещей, которые

1769
01:00:59,680 --> 01:01:00,799
мы делаем правильно, мы '  повторяя одну

1770
01:01:00,799 --> 01:01:02,480
интересную вещь здесь, вы знаете, что

1771
01:01:02,480 --> 01:01:04,640
многоголовая маска самовнимание

1772
01:01:04,640 --> 01:01:06,319
перекрестное внимание после каждого из них делает

1773
01:01:06,319 --> 01:01:07,839
остаток и норма слоя помогает

1774
01:01:07,839 --> 01:01:11,440
градиентам проходить

1775
01:01:11,440 --> 01:01:12,960


1776
01:01:12,960 --> 01:01:13,839


1777
01:01:13,839 --> 01:01:15,520


1778
01:01:15,520 --> 01:01:18,240
и т.  Единственное, чего

1779
01:01:18,240 --> 01:01:20,799
мы пока не видели в этой лекции,

1780
01:01:20,799 --> 01:01:24,480
- это перекрестное внимание с несколькими головами,

1781
01:01:24,480 --> 01:01:28,079
и я хочу, чтобы я хотел пройтись по нему эээ,

1782
01:01:28,079 --> 01:01:30,160
это те же уравнения,

1783
01:01:30,160 --> 01:01:33,040
что и эм,

1784
01:01:33,040 --> 01:01:34,640
как и

1785
01:01:34,640 --> 01:01:36,400
многоголовый я  -внимание, но

1786
01:01:36,400 --> 01:01:37,760
входные данные поступают из разных мест,

1787
01:01:37,760 --> 01:01:40,559
и поэтому я хочу быть точным,

1788
01:01:40,559 --> 01:01:42,720
поэтому давайте рассмотрим

1789
01:01:42,720 --> 01:01:45,920
детали перекрестного внимания, так что

1790
01:01:45,920 --> 01:01:47,680
правильное напоминание о самом внимании заключается в том, что когда

1791
01:01:47,680 --> 01:01:49,599
мы берем ключи, запросы

1792
01:01:49,599 --> 01:01:50,799
и  значения внимания из того

1793
01:01:50,799 --> 01:01:53,520
же источника информации, как, например, одно и то же

1794
01:01:53,520 --> 01:01:55,680
предложение,

1795
01:01:55,680 --> 01:01:57,760
и на прошлой неделе мы видели, как внимание

1796
01:01:57,760 --> 01:01:59,839
от декодера переходит к кодировщику, так что

1797
01:01:59,839 --> 01:02:01,280
это будет похоже,

1798
01:02:01,280 --> 01:02:03,119
давайте сделаем несколько других обозначений, так что

1799
01:02:03,119 --> 01:02:05,200
у нас будет h1 для

1800
01:02:05,200 --> 01:02:07,920
ht  выходные векторы

1801
01:02:07,920 --> 01:02:11,039
от кодировщика бывшего трансформатора,

1802
01:02:11,039 --> 01:02:13,440
которые все xi в rd, теперь помните, что

1803
01:02:13,440 --> 01:02:16,079
это последний кодировщик трансформатора здесь,

1804
01:02:16,079 --> 01:02:18,480
вы никогда не обращаетесь к средним

1805
01:02:18,480 --> 01:02:20,000
блокам кодировщика, это выход

1806
01:02:20,000 --> 01:02:21,520
последнего блока кодировщика,

1807
01:02:21,520 --> 01:02:22,960
поэтому эти выходные векторы от

1808
01:02:22,960 --> 01:02:25,680
последнего кодировщика трансформатора uh  block, и

1809
01:02:25,680 --> 01:02:29,039
теперь у нас есть z1 для zt входных векторов

1810
01:02:29,039 --> 01:02:31,119
из декодера трансформатора, так

1811
01:02:31,119 --> 01:02:34,960
что, возможно, вы знаете, что вход -

1812
01:02:34,960 --> 01:02:36,240
это вложения слов плюс есть

1813
01:02:36,240 --> 01:02:38,079
представления положения

1814
01:02:38,079 --> 01:02:40,240
uh или r  Право, это на самом деле

1815
01:02:40,240 --> 01:02:42,480
выход предыдущего декодера трансформатора, так что

1816
01:02:42,480 --> 01:02:43,680
мы будем входами для

1817
01:02:43,680 --> 01:02:46,079
следующего,

1818
01:02:46,079 --> 01:02:46,960


1819
01:02:46,960 --> 01:02:49,440
так что да, у нас есть z один на zt,

1820
01:02:49,440 --> 01:02:50,559
и мы снова позволяем им иметь одинаковую

1821
01:02:50,559 --> 01:02:52,880
длину последовательности t и t  просто для

1822
01:02:52,880 --> 01:02:54,160
простоты,

1823
01:02:54,160 --> 01:02:57,520
это также векторы zi в нашем d,

1824
01:02:57,520 --> 01:02:59,440
а затем ключи и запрос, извините,

1825
01:02:59,440 --> 01:03:01,680
что ключи и значения все взяты

1826
01:03:01,680 --> 01:03:03,200
из кодировщика

1827
01:03:03,200 --> 01:03:04,960
правильно, поэтому, когда мы говорим о

1828
01:03:04,960 --> 01:03:06,319
самовнимании, когда мы говорим о

1829
01:03:06,319 --> 01:03:08,400
внимании как  позволяя нам сортировать

1830
01:03:08,400 --> 01:03:10,400
доступ к памяти

1831
01:03:10,400 --> 01:03:12,880
прямо, память uh - это

1832
01:03:12,880 --> 01:03:14,319
своего рода то, что кодируют векторы значений,

1833
01:03:14,319 --> 01:03:17,760
и способ, которым значения

1834
01:03:17,760 --> 01:03:19,920
вроде как индексируются или могут быть

1835
01:03:19,920 --> 01:03:21,920
доступны, через ключи,

1836
01:03:21,920 --> 01:03:24,400
а затем значение, а затем

1837
01:03:24,400 --> 01:03:26,319
запросы  вы знаете,

1838
01:03:26,319 --> 01:03:27,440
что вы используете, чтобы попытаться найти

1839
01:03:27,440 --> 01:03:29,280
что-то правильное, поэтому мы смотрим

1840
01:03:29,280 --> 01:03:31,359
на кодировщик как на память, и мы используем

1841
01:03:31,359 --> 01:03:33,359
ключи от декодера, чтобы выяснить,

1842
01:03:33,359 --> 01:03:35,920
где искать каждый,

1843
01:03:35,920 --> 01:03:38,400
так что наглядно

1844
01:03:38,400 --> 01:03:40,400
снова мы можем посмотреть, как перекрестное внимание

1845
01:03:40,400 --> 01:03:42,000
вычисляется в матрицах, как мы делали для

1846
01:03:42,000 --> 01:03:43,359
самовнимания,

1847
01:03:43,359 --> 01:03:45,039
поэтому у нас здесь то же самое, прежде чем

1848
01:03:45,039 --> 01:03:47,200
у нас было x, теперь у нас есть h, это

1849
01:03:47,200 --> 01:03:49,760
векторы кодировщика, это будет rt

1850
01:03:49,760 --> 01:03:52,000
на d,

1851
01:03:52,000 --> 01:03:54,079
аналогично у нас есть z,

1852
01:03:54,079 --> 01:03:55,280
обратите внимание, у нас есть два из них  раньше,

1853
01:03:55,280 --> 01:03:57,200
раньше у нас было только x, у нас было x,

1854
01:03:57,200 --> 01:03:59,280
потому что x должен был быть для ключей,

1855
01:03:59,280 --> 01:04:00,960
запросы и значения, теперь у нас есть h

1856
01:04:00,960 --> 01:04:04,400
и z, оба находятся в rt через d,

1857
01:04:04,400 --> 01:04:06,720
и

1858
01:04:06,720 --> 01:04:09,440
выход будет хорошо, вы возьмете свой

1859
01:04:09,440 --> 01:04:10,160
z

1860
01:04:10,160 --> 01:04:11,520
для

1861
01:04:11,520 --> 01:04:13,200
запросы вправо zs, которые мы умножаем на

1862
01:04:13,200 --> 01:04:16,240
запросы, которые вы берете с h для ключей

1863
01:04:16,240 --> 01:04:19,039
и с h для v,

1864
01:04:19,039 --> 01:04:21,599
поэтому вы пытаетесь взять ключ,

1865
01:04:21,599 --> 01:04:23,760
ключевые точечные произведения запроса все t возведены в

1866
01:04:23,760 --> 01:04:26,720
квадрат um в одном умножении матрицы,

1867
01:04:26,720 --> 01:04:29,119
поэтому фиолетовый говорит  это исходит от

1868
01:04:29,119 --> 01:04:29,839


1869
01:04:29,839 --> 01:04:33,599
декодера, коричневый говорит или ish

1870
01:04:33,599 --> 01:04:35,359
говорит, что он исходит от

1871
01:04:35,359 --> 01:04:36,960
кодировщика,

1872
01:04:36,960 --> 01:04:39,599
теперь у вас есть точечные продукты,

1873
01:04:39,599 --> 01:04:41,680
soft max, как и раньше, и теперь

1874
01:04:41,680 --> 01:04:43,599
ваши значения также поступают из

1875
01:04:43,599 --> 01:04:45,440
кодировщика, так что

1876
01:04:45,440 --> 01:04:47,920
снова та же операция  разные источники

1877
01:04:47,920 --> 01:04:50,319
для входов,

1878
01:04:50,319 --> 01:04:51,599
и теперь у вас есть лет  ur вывод, который

1879
01:04:51,599 --> 01:04:53,839
снова является просто средним векторов значений

1880
01:04:53,839 --> 01:04:56,720
от кодировщика.

1881
01:04:56,720 --> 01:04:58,000


1882
01:04:58,000 --> 01:04:59,520
hv среднее значение определяется вашими

1883
01:04:59,520 --> 01:05:01,839
весами.

1884
01:05:02,000 --> 01:05:04,559
Хорошо, поэтому результаты с трансформаторами.

1885
01:05:04,559 --> 01:05:06,880
Результаты с трансформаторами.

1886
01:05:06,880 --> 01:05:11,039


1887
01:05:11,039 --> 01:05:13,359


1888
01:05:13,359 --> 01:05:16,559
и вы знаете,

1889
01:05:16,559 --> 01:05:18,640
как это работает, это работает очень хорошо,

1890
01:05:18,640 --> 01:05:19,839
так

1891
01:05:19,839 --> 01:05:21,440
что это куча систем машинного перевода

1892
01:05:21,440 --> 01:05:23,039
, которых не было, когда исходное

1893
01:05:23,039 --> 01:05:24,559
внимание - это все, что вам нужно,

1894
01:05:24,559 --> 01:05:26,000
бумага для трансформаторов вышла,

1895
01:05:26,000 --> 01:05:26,960
и

1896
01:05:26,960 --> 01:05:29,119
сначала вы увидели, что трансформаторы

1897
01:05:29,119 --> 01:05:30,720
получают действительно хорошие синие оценки, так что это

1898
01:05:30,720 --> 01:05:32,240
находится на семинаре по машинному

1899
01:05:32,240 --> 01:05:35,119
переводу 2014

1900
01:05:35,119 --> 01:05:36,640


1901
01:05:36,640 --> 01:05:40,319
наборы тестов для английского немецкого и английского французского, вы получите более высокие баллы синего цвета,

1902
01:05:40,319 --> 01:05:41,920
что означает лучший перевод. Обратите

1903
01:05:41,920 --> 01:05:43,280
внимание, как наши баллы синего цвета в этом

1904
01:05:43,280 --> 01:05:45,599
выше, чем в задании,

1905
01:05:45,599 --> 01:05:48,160
например, здесь на четыре намного больше обучающих данных,

1906
01:05:48,160 --> 01:05:49,599
но не только  Вы получаете лучшие

1907
01:05:49,599 --> 01:05:52,160
оценки синих эм, у вас также были более

1908
01:05:52,160 --> 01:05:53,680
эффективные тренировки,

1909
01:05:53,680 --> 01:05:56,000
и у нас было много трюков  ks,

1910
01:05:56,000 --> 01:05:57,680
которые пошли на то, чтобы обучение работало

1911
01:05:57,680 --> 01:05:59,280
лучше, так что у вас здесь более эффективное

1912
01:05:59,280 --> 01:06:01,520
обучение,

1913
01:06:01,520 --> 01:06:03,200
хорошо, так что это хороший результат, который был

1914
01:06:03,200 --> 01:06:04,640
в исходной статье,

1915
01:06:04,640 --> 01:06:07,839
вы знаете, что в прошлом есть

1916
01:06:07,839 --> 01:06:10,000
ряд интересных результатов,

1917
01:06:10,000 --> 01:06:11,520
резюмирование - один из них, так что

1918
01:06:11,520 --> 01:06:13,920
вот результат резюмирования,

1919
01:06:13,920 --> 01:06:15,520
это своего рода часть более крупной

1920
01:06:15,520 --> 01:06:17,760
системы реферирования, но вы знаете,

1921
01:06:17,760 --> 01:06:19,440
что мне понравилась эта таблица, потому что у вас есть

1922
01:06:19,440 --> 01:06:20,640
своего рода стремление к поиску с вниманием,

1923
01:06:20,640 --> 01:06:22,559
которое мы видели раньше, и в ней уменьшалось

1924
01:06:22,559 --> 01:06:24,000
недоумение, тем лучше, если

1925
01:06:24,000 --> 01:06:26,640
недоумение выше  лучше с румянцем

1926
01:06:26,640 --> 01:06:29,440
в этом наборе данных суммы вики, а затем вроде

1927
01:06:29,440 --> 01:06:31,280


1928
01:06:31,280 --> 01:06:33,440
как кучка моделей трансформаторов, которые они пробовали,

1929
01:06:33,440 --> 01:06:35,039
и в какой-то момент они

1930
01:06:35,039 --> 01:06:37,200
становятся трансформаторами полностью вниз,

1931
01:06:37,200 --> 01:06:38,960
и своего рода старый стандарт rnn вроде как

1932
01:06:38,960 --> 01:06:40,640
выпадает из практики

1933
01:06:40,640 --> 01:06:43,039
гм, и на самом деле очень скоро правые

1934
01:06:43,039 --> 01:06:44,720
трансформаторы стали доминирующими по

1935
01:06:44,720 --> 01:06:46,559
совершенно другой причине, которая

1936
01:06:46,559 --> 01:06:48,960
больше связана с их параллелизуемостью,

1937
01:06:48,960 --> 01:06:51,200
потому что они позволяют вам предварительно тренироваться на

1938
01:06:51,200 --> 01:06:53,200
ю  Очень быстро собрать тонну данных,

1939
01:06:53,200 --> 01:06:54,880


1940
01:06:54,880 --> 01:06:57,280
и это сделало их стандартом де-факто,

1941
01:06:57,280 --> 01:06:59,359
поэтому в последнее время появилось много результатов

1942
01:06:59,359 --> 01:07:01,359
с трансформаторами, включая

1943
01:07:01,359 --> 01:07:02,720
предварительное обучение, и я вроде как

1944
01:07:02,720 --> 01:07:04,319
намеренно исключаю их

1945
01:07:04,319 --> 01:07:06,000
из этой лекции, чтобы вы перешли

1946
01:07:06,000 --> 01:07:07,920
к следующей.  читать лекции, говорить и узнавать

1947
01:07:07,920 --> 01:07:09,280
о предварительном обучении,

1948
01:07:09,280 --> 01:07:10,720
но есть популярный совокупный

1949
01:07:10,720 --> 01:07:12,160
тест, который включает в себя кучу очень

1950
01:07:12,160 --> 01:07:14,480
сложных задач и говорит, что

1951
01:07:14,480 --> 01:07:15,920
вы хорошо справляетесь со всеми из них, если хотите получить

1952
01:07:15,920 --> 01:07:18,400
высокие баллы в нашей таблице лидеров, и

1953
01:07:18,400 --> 01:07:19,599
вы знаете названия этих моделей  вы

1954
01:07:19,599 --> 01:07:21,359
можете поискать, если вам интересно, но

1955
01:07:21,359 --> 01:07:22,960
все они основаны на трансформаторе после

1956
01:07:22,960 --> 01:07:24,480
определенных точек тест называется

1957
01:07:24,480 --> 01:07:26,480
клей, у него есть преемник, называемый

1958
01:07:26,480 --> 01:07:29,359
суперклей, все это просто трансформаторы

1959
01:07:29,359 --> 01:07:31,520
после определенного периода времени

1960
01:07:31,520 --> 01:07:32,799


1961
01:07:32,799 --> 01:07:35,119
отчасти из-за их предтренировочных

1962
01:07:35,119 --> 01:07:37,039
способностей

1963
01:07:37,039 --> 01:07:39,359
хорошо,

1964
01:07:39,599 --> 01:07:42,240
отлично, так что мы обсудим предварительные тренировки больше

1965
01:07:42,240 --> 01:07:43,039
в

1966
01:07:43,039 --> 01:07:44,880
четверг,

1967
01:07:44,880 --> 01:07:46,079


1968
01:07:46,079 --> 01:07:48,559
и наш наш трансформатор -

1969
01:07:48,559 --> 01:07:50,799
это как то, как мы описали

1970
01:07:50,799 --> 01:07:52,880
внимание, это все, что вам нужно, бумага, так что

1971
01:07:52,880 --> 01:07:54,880
преобразование  r кодировщик декодер, который мы видели, был взят

1972
01:07:54,880 --> 01:07:59,680
из этой статьи, и в какой-то момент

1973
01:07:59,680 --> 01:08:01,280
вы знаете, что мы хотим создать новые системы,

1974
01:08:01,280 --> 01:08:02,799
какие есть некоторые недостатки, и мы

1975
01:08:02,799 --> 01:08:03,920
уже начали, люди уже

1976
01:08:03,920 --> 01:08:04,960
начали создавать варианты

1977
01:08:04,960 --> 01:08:07,039
трансформаторов, в которые вы войдете сегодня,

1978
01:08:07,039 --> 01:08:07,920
и

1979
01:08:07,920 --> 01:08:09,520
вы знаете  у него определенно есть проблемы, над которыми

1980
01:08:09,520 --> 01:08:12,799
мы можем попытаться поработать, поэтому

1981
01:08:13,760 --> 01:08:15,599
я также могу задать вопрос, если кто-то

1982
01:08:15,599 --> 01:08:18,480
захочет задать один,

1983
01:08:18,719 --> 01:08:20,880
я имею в виду, что это немного назад, что то,

1984
01:08:20,880 --> 01:08:23,600
о чем было несколько вопросов,

1985
01:08:23,600 --> 01:08:26,238
было скалярным произведением масштаба,

1986
01:08:26,238 --> 01:08:29,279
и вопросы включали,

1987
01:08:29,279 --> 01:08:32,479
почему квадратный корень  d, разделенного на h,

1988
01:08:32,479 --> 01:08:35,679
в отличие от просто d, разделенного на h, или любой

1989
01:08:35,679 --> 01:08:39,120
другой функции d, разделенной на h, а

1990
01:08:39,120 --> 01:08:41,839
еще одна была

1991
01:08:41,839 --> 01:08:43,120
um

1992
01:08:43,120 --> 01:08:44,080
that

1993
01:08:44,080 --> 01:08:45,839
um

1994
01:08:45,839 --> 01:08:48,479
like, почему вам это вообще нужно, учитывая,

1995
01:08:48,479 --> 01:08:50,799
что позже вы собираетесь использовать норму слоя

1996
01:08:50,799 --> 01:08:53,198


1997
01:08:53,198 --> 01:08:54,479
wow  второй вопрос действительно

1998
01:08:54,479 --> 01:08:55,600
интересен и не тот, о котором я

1999
01:08:55,600 --> 01:08:58,080
думал раньше,

2000
01:08:58,080 --> 01:08:59,920


2001
01:08:59,920 --> 01:09:01,359
так что даже если отдельные

2002
01:09:01,359 --> 01:09:02,479


2003
01:09:02,479 --> 01:09:04,479
компоненты маленькие, давайте начнем со второго

2004
01:09:04,479 --> 01:09:06,080
вопроса, почему это имеет значение, даже если

2005
01:09:06,080 --> 01:09:08,399
вы собираетесь использовать норму слоя um

2006
01:09:08,399 --> 01:09:10,319
you k  теперь, если lara normal усредняет

2007
01:09:10,319 --> 01:09:12,080
все, скажем, сделайте это единичным стандартным

2008
01:09:12,080 --> 01:09:15,198
отклонением и, и, ну, и значит,

2009
01:09:15,198 --> 01:09:17,279
тогда на самом деле ничего

2010
01:09:17,279 --> 01:09:19,439
не будет слишком маленьким в этих векторах,

2011
01:09:19,439 --> 01:09:22,238
поэтому, когда у вас очень очень большой

2012
01:09:22,238 --> 01:09:25,120
вектор, все с вещами, которые не

2013
01:09:25,120 --> 01:09:26,950
слишком маленький да

2014
01:09:26,950 --> 01:09:29,120
[Музыка] у

2015
01:09:29,120 --> 01:09:31,198
вас все еще будет

2016
01:09:31,198 --> 01:09:32,238


2017
01:09:32,238 --> 01:09:33,520
норма

2018
01:09:33,520 --> 01:09:35,759
увеличения точечных продуктов,

2019
01:09:35,759 --> 01:09:36,799
я думаю,

2020
01:09:36,799 --> 01:09:38,000
я думаю, что это хороший вопрос, я не

2021
01:09:38,000 --> 01:09:40,080
думал об этом слишком много, это

2022
01:09:40,080 --> 01:09:41,359
мой нестандартный ответ,

2023
01:09:41,359 --> 01:09:43,600
но о нем стоит подумать подробнее

2024
01:09:43,600 --> 01:09:45,759
i  Думаю, ответ заключается в том,

2025
01:09:45,759 --> 01:09:47,920
что вы получаете эффект потери

2026
01:09:47,920 --> 01:09:51,439
динамического диапазона по мере того, как вещи становятся длиннее

2027
01:09:51,439 --> 01:09:54,560
, которые в любом случае должны произойти,

2028
01:09:54,560 --> 01:09:57,679
и leia norm не может исправить, что это

2029
01:09:57,679 --> 01:10:00,000
как бы происходит слишком поздно,

2030
01:10:00,000 --> 01:10:00,960
и,

2031
01:10:00,960 --> 01:10:04,159
следовательно, вы получаете, выполняя масштабирование,

2032
01:10:04,159 --> 01:10:05,600
я думаю  так что,

2033
01:10:05,600 --> 01:10:07,040
но я думаю, что да, я думаю,

2034
01:10:07,040 --> 01:10:09,040
стоит подумать больше, почему квадратный

2035
01:10:09,040 --> 01:10:10,880
корень ммм

2036
01:10:10,880 --> 01:10:13,440
хорошо, давайте посмотрим, что нормы скалярного

2037
01:10:13,440 --> 01:10:15,520
произведения растут с

2038
01:10:15,520 --> 01:10:17,120
o из d,

2039
01:10:17,120 --> 01:10:19,600
и поэтому, когда вы извлекаете квадратный корень из единицы,

2040
01:10:19,600 --> 01:10:21,600
нет, я думаю, квадратный корень масштабируется с

2041
01:10:21,600 --> 01:10:23,120
корнем o  ди  не могу вспомнить, есть

2042
01:10:23,120 --> 01:10:24,400
небольшая заметка во внимании - это все, что вам

2043
01:10:24,400 --> 01:10:27,040
нужно в бумаге о том, почему это корень d, но я на

2044
01:10:27,040 --> 01:10:28,640
самом деле не могу снять это с моей

2045
01:10:28,640 --> 01:10:31,199
головы, так что,

2046
01:10:31,199 --> 01:10:34,239
но это в этой статье,

2047
01:10:37,920 --> 01:10:40,159
хорошо,

2048
01:10:40,320 --> 01:10:44,280
что-нибудь еще, прежде чем продолжить

2049
01:10:44,960 --> 01:10:46,719
отлично

2050
01:10:46,719 --> 01:10:47,110
ммм

2051
01:10:47,110 --> 01:10:48,400
[музыка

2052
01:10:48,400 --> 01:10:50,400
] хорошо, так что бы вы хотели исправить,

2053
01:10:50,400 --> 01:10:51,520
ммм,

2054
01:10:51,520 --> 01:10:53,360
вы знаете, что то, что

2055
01:10:53,360 --> 01:10:55,199
чаще всего проявляется в качестве болевой точки в

2056
01:10:55,199 --> 01:10:57,280
трансформерах, на самом деле похоже на

2057
01:10:57,280 --> 01:10:59,679
квадратичные вычисления в

2058
01:10:59,679 --> 01:11:02,719
самом самовнимании.  все пары

2059
01:11:02,719 --> 01:11:04,480
взаимодействий у нас была матрица t на t,

2060
01:11:04,480 --> 01:11:06,800
которая была вычислена путем взятия этих скалярных

2061
01:11:06,800 --> 01:11:09,520
произведений между всеми парами векторов слов,

2062
01:11:09,520 --> 01:11:12,239
и поэтому, несмотря на то, что в начале класса мы утверждали,

2063
01:11:12,239 --> 01:11:13,760
что у нас

2064
01:11:13,760 --> 01:11:16,080
нет такой временной зависимости

2065
01:11:16,080 --> 01:11:18,080
в  граф вычислений, который мешает нам

2066
01:11:18,080 --> 01:11:20,159
распараллеливать то, что нам все еще нужно для выполнения

2067
01:11:20,159 --> 01:11:21,679
всех этих вычислений, и который растет

2068
01:11:21,679 --> 01:11:24,320
квадратично для рекуррентных моделей, верно,

2069
01:11:24,320 --> 01:11:26,080
он только линейно увеличивался каждый раз, когда

2070
01:11:26,080 --> 01:11:28,880
вы применяли ячейку rnn.

2071
01:11:28,880 --> 01:11:30,640
g

2072
01:11:30,640 --> 01:11:32,239
квадратично к объему работы, которую вы

2073
01:11:32,239 --> 01:11:33,920
должны выполнить, когда вы переходите к более длинным

2074
01:11:33,920 --> 01:11:35,280
последовательностям,

2075
01:11:35,280 --> 01:11:36,880
отдельно позиционным представлениям, я

2076
01:11:36,880 --> 01:11:38,640
имею в виду, что абсолютная позиция слова

2077
01:11:38,640 --> 01:11:40,719
просто не

2078
01:11:40,719 --> 01:11:43,040
может быть, возможно, не лучшим способом представить

2079
01:11:43,040 --> 01:11:45,840
э-э, вы знаете структуру предложения,

2080
01:11:45,840 --> 01:11:48,159
и поэтому  были эти два ээээ,

2081
01:11:48,159 --> 01:11:50,719
вы знаете, среди других достижений в

2082
01:11:50,719 --> 01:11:52,800
том, что я не

2083
01:11:52,800 --> 01:11:54,000
смогу сегодня рассказать, но вы можете взглянуть на

2084
01:11:54,000 --> 01:11:55,440
эти документы и статьи, которые их цитируют,

2085
01:11:55,440 --> 01:11:56,880
есть другие способы представить

2086
01:11:56,880 --> 01:11:59,040
позицию  люди работают над этим, но сегодня я

2087
01:11:59,040 --> 01:12:02,640
хочу больше сосредоточиться на

2088
01:12:02,640 --> 01:12:06,080
проблеме квадратичных вычислений, так

2089
01:12:06,080 --> 01:12:08,400
как мы можем решить

2090
01:12:08,400 --> 01:12:10,880
эту проблему,

2091
01:12:10,880 --> 01:12:12,320


2092
01:12:12,320 --> 01:12:14,480
почему это проблема?  операций у нас есть t в квадрате

2093
01:12:14,480 --> 01:12:16,400
, это длина последовательности, а затем d

2094
01:12:16,400 --> 01:12:18,560
- размерность, и поэтому при вычислении

2095
01:12:18,560 --> 01:12:20,880
этой матрицы у нас есть o из t возведенных в квадрат d

2096
01:12:20,880 --> 01:12:23,040
вычислений, которые наш графический процессор должен разбить на

2097
01:12:23,040 --> 01:12:23,840


2098
01:12:23,840 --> 01:12:25,840
um, если мы думаем о d, это примерно

2099
01:12:25,840 --> 01:12:29,520
тысяча  d, две или три тысячи um, если бы

2100
01:12:29,520 --> 01:12:32,560
у нас были какие-то отдельные короткие предложения,

2101
01:12:32,560 --> 01:12:34,320
но тогда, возможно, t было бы где-то тридцать, а

2102
01:12:34,320 --> 01:12:36,640
затем t в квадрате равно 900, так что это как ах,

2103
01:12:36,640 --> 01:12:38,239
на самом деле это не так уж важно, но

2104
01:12:38,239 --> 01:12:40,000
вы знаете, и на практике вы знаете, что

2105
01:12:40,000 --> 01:12:42,000
для многих моделей мы установим фактическую границу,

2106
01:12:42,000 --> 01:12:43,840
например 512, так что если ваш

2107
01:12:43,840 --> 01:12:46,080
документ длиннее 512 слов, вы знаете, что вам

2108
01:12:46,080 --> 01:12:47,679
не повезло, вы усечены или

2109
01:12:47,679 --> 01:12:50,239
что-то в этом роде, но что, если мы хотим работать

2110
01:12:50,239 --> 01:12:52,080
над документами, которые  Знаете ли вы, что десять

2111
01:12:52,080 --> 01:12:54,320
тысяч слов или больше десять тысяч в

2112
01:12:54,320 --> 01:12:56,239
квадрате

2113
01:12:56,239 --> 01:12:58,320
невозможно, поэтому нам нужно как-то

2114
01:12:58,320 --> 01:13:00,880
устранить зависимость от t в квадрате, если

2115
01:13:00,880 --> 01:13:03,280
мы собираемся работать с ними,

2116
01:13:03,280 --> 01:13:04,800
есть несколько способов, которые

2117
01:13:04,800 --> 01:13:06,080
были придуманы для этого  все это

2118
01:13:06,080 --> 01:13:07,520
очень недавняя работа, и это лишь

2119
01:13:07,520 --> 01:13:09,679
малая часть усилий, которые

2120
01:13:09,679 --> 01:13:12,320
возникли, поэтому вопрос в том, можем ли мы построить

2121
01:13:12,320 --> 01:13:14,480
модели, такие как трансформаторы, которые

2122
01:13:14,480 --> 01:13:17,120
обходятся без квадрата o или t в квадрате всех

2123
01:13:17,120 --> 01:13:20,159
парных взаимодействий.

2124
01:13:20,159 --> 01:13:22,480
Одним из примеров является линформер и

2125
01:13:22,480 --> 01:13:24,400
идея здесь в том, что

2126
01:13:24,400 --> 01:13:26,719
ты  собираемся фактически сопоставить

2127
01:13:26,719 --> 01:13:29,120
размерность длины последовательности с

2128
01:13:29,120 --> 01:13:31,520
пространством более низкой размерности

2129
01:13:31,520 --> 01:13:34,080
для значений и ключей, чтобы у вас были

2130
01:13:34,080 --> 01:13:35,600
ключи значений и запросы,

2131
01:13:35,600 --> 01:13:37,280
и у вас были обычные линейные слои,

2132
01:13:37,280 --> 01:13:38,640
теперь вы собираетесь проецировать гораздо

2133
01:13:38,640 --> 01:13:41,199
меньшее измерение, чем длина последовательности

2134
01:13:41,199 --> 01:13:43,600
и  Поступая так правильно,

2135
01:13:43,600 --> 01:13:46,400
вы как бы избавляетесь от этого t

2136
01:13:46,400 --> 01:13:48,159
, сопоставляя его с чем-то маленьким, о котором вы

2137
01:13:48,159 --> 01:13:49,760
говорите, как будто вы просто знаете, объедините

2138
01:13:49,760 --> 01:13:50,960
всю информацию из всех этих временных

2139
01:13:50,960 --> 01:13:52,560
шагов во что-то более низкое

2140
01:13:52,560 --> 01:13:54,960
измерение, и поэтому на этом графике из

2141
01:13:54,960 --> 01:13:56,719
документ, который вы знаете, поскольку длина последовательности

2142
01:13:56,719 --> 01:14:00,480
идет от 512, размер пакета 128 до

2143
01:14:00,480 --> 01:14:03,040
длины последовательности 65000 с

2144
01:14:03,040 --> 01:14:04,640
размером пакета один, вы получаете

2145
01:14:04,640 --> 01:14:07,360
время вывода трансформатора, вроде

2146
01:14:07,360 --> 01:14:09,040
как очень большой, а затем

2147
01:14:09,040 --> 01:14:12,239
линформер с различными типами

2148
01:14:12,239 --> 01:14:15,520
узких мест размерностей ks  128 256

2149
01:14:15,520 --> 01:14:18,239
они работают намного лучше,

2150
01:14:18,239 --> 01:14:21,199
отдельный вариант заключался в том, чтобы

2151
01:14:21,199 --> 01:14:22,080


2152
01:14:22,080 --> 01:14:22,880


2153
01:14:22,880 --> 01:14:25,199
взглянуть на совершенно другой взгляд,

2154
01:14:25,199 --> 01:14:26,800
можем ли мы обойтись без всех этих парных

2155
01:14:26,800 --> 01:14:29,679
взаимодействий, что является следующим  крыло

2156
01:14:29,679 --> 01:14:32,159
, нам нужно даже попытаться вычислить все

2157
01:14:32,159 --> 01:14:34,159
пары взаимодействий, если мы сможем сделать

2158
01:14:34,159 --> 01:14:35,760
кучу других вещей, которые

2159
01:14:35,760 --> 01:14:38,320
будут более эффективными для вычислений

2160
01:14:38,320 --> 01:14:40,560
, например, смотреть на локальные окна, которые мы знаем,

2161
01:14:40,560 --> 01:14:42,480
что это полезно, но в некотором

2162
01:14:42,480 --> 01:14:45,040
смысле недостаточно  во всем, так что если бы

2163
01:14:45,040 --> 01:14:46,560
вы просто взяли среднее значение

2164
01:14:46,560 --> 01:14:48,320
векторов, просто все усреднение векторов,

2165
01:14:48,320 --> 01:14:49,600
вам не нужно было бы вычислять взаимодействия

2166
01:14:49,600 --> 01:14:50,560
для этого,

2167
01:14:50,560 --> 01:14:52,560
и если вы посмотрите на какие-то случайные пары,

2168
01:14:52,560 --> 01:14:54,800
вам не нужно знать, что вы все это знаете.

2169
01:14:54,800 --> 01:14:56,960
много времени, чтобы вычислить и это, и поэтому

2170
01:14:56,960 --> 01:14:59,199
то, что сделала эта статья, они сделали все из

2171
01:14:59,199 --> 01:15:02,239
них, так что у вас есть случайное внимание, у вас

2172
01:15:02,239 --> 01:15:03,840
есть внимание окна слова, где

2173
01:15:03,840 --> 01:15:05,360
вы смотрите на своих местных соседей,

2174
01:15:05,360 --> 01:15:07,199
и у вас есть своего рода глобальное внимание

2175
01:15:07,199 --> 01:15:09,199
там, где вы ''  если вы знаете, что вы посещаете, но

2176
01:15:09,199 --> 01:15:11,440
не взаимодействуете с вещами, которые присутствуют в

2177
01:15:11,440 --> 01:15:13,360
целом на протяжении всей последовательности, вы делаете

2178
01:15:13,360 --> 01:15:16,400
целую кучу всего этого правильно, и в конечном итоге вы

2179
01:15:16,400 --> 01:15:18,000
можете приблизиться к множеству хороших

2180
01:15:18,000 --> 01:15:19,600
вещей,

2181
01:15:19,600 --> 01:15:21,440
которые вы не знаете, обязательно

2182
01:15:21,440 --> 01:15:23,280
ответ  Вариант с обычным трансформатором

2183
01:15:23,280 --> 01:15:25,840
в настоящее время является наиболее популярным, но

2184
01:15:25,840 --> 01:15:28,400
это интересный вопрос, который стоит изучить,

2185
01:15:28,400 --> 01:15:30,719
поэтому теперь, когда

2186
01:15:30,719 --> 01:15:31,679
время

2187
01:15:31,679 --> 01:15:33,920
более или менее истекает, я скажу, что мы

2188
01:15:33,920 --> 01:15:35,679
работаем над предварительной тренировкой в четверг,

2189
01:15:35,679 --> 01:15:38,000
удачи в задании 4 и не забудьте

2190
01:15:38,000 --> 01:15:39,840
поработайте над своим проектным предложением, я думаю, у нас

2191
01:15:39,840 --> 01:15:42,159
есть время для последнего вопроса, если кто-то

2192
01:15:42,159 --> 01:15:44,560
захочет

2193
01:15:56,640 --> 01:15:58,719
, это хороший вопрос,

2194
01:15:58,719 --> 01:16:00,239
да, я

2195
01:16:00,239 --> 01:16:02,159
имею в виду, что

2196
01:16:02,159 --> 01:16:04,480
я верю, что в обучении с подкреплением все еще есть места,

2197
01:16:04,480 --> 01:16:06,640


2198
01:16:06,640 --> 01:16:08,400
я имею в виду места, где повторяющееся

2199
01:16:08,400 --> 01:16:10,719
индуктивное смещение четко

2200
01:16:10,719 --> 01:16:13,040
определено или полезно  был

2201
01:16:13,040 --> 01:16:15,760
разговор вроде,

2202
01:16:16,000 --> 01:16:18,880
но я не знаю мест в nlp, где

2203
01:16:18,880 --> 01:16:21,199
люди все еще широко используют rnns, какое-то время

2204
01:16:21,199 --> 01:16:22,239
считалось, что

2205
01:16:22,239 --> 01:16:23,840
трансформаторам требуется намного больше данных для

2206
01:16:23,840 --> 01:16:25,360
обучения, чем rnn, и поэтому вы вроде как

2207
01:16:25,360 --> 01:16:27,520
должны использовать rnns для небольших проблем с данными

2208
01:16:27,520 --> 01:16:29,199
но с предварительным обучением я не уверен

2209
01:16:29,199 --> 01:16:31,360
, что это так,

2210
01:16:31,360 --> 01:16:33,360
я думаю, ответ - да,

2211
01:16:33,360 --> 01:16:35,120
все еще есть варианты использования,

2212
01:16:35,120 --> 01:16:37,679
но это должно быть там, где повторение

2213
01:16:37,679 --> 01:16:39,679
кажется действительно

2214
01:16:39,679 --> 01:16:41,199
тем, что выигрывает у

2215
01:16:41,199 --> 01:16:44,080
вас  Для чего-то, а не вроде,

2216
01:16:44,080 --> 01:16:45,840
может потребоваться больше данных или

2217
01:16:45,840 --> 01:16:47,120
трансформаторов, потому что кажется, что это

2218
01:16:47,120 --> 01:16:48,400
может быть не так,

2219
01:16:48,400 --> 01:16:52,920
хотя мы думали об этом еще в 2017 году.

