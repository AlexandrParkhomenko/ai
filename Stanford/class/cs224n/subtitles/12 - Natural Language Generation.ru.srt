1
00:00:05,520 --> 00:00:09,599
привет всем, добро пожаловать обратно в cs224n эм,

2
00:00:09,599 --> 00:00:12,639
первое, всего пара объявлений,

3
00:00:12,639 --> 00:00:14,320
эм изначально это должен был быть

4
00:00:14,320 --> 00:00:17,039
день, когда должно было выполняться задание 5, но эм, как

5
00:00:17,039 --> 00:00:18,240
вы видели,

6
00:00:18,240 --> 00:00:20,160
мы даем вам еще один дополнительный день, так что

7
00:00:20,160 --> 00:00:23,680
теперь он должен быть в пятницу в 4  30. эм, мы понимаем,

8
00:00:23,680 --> 00:00:26,480
что задание 5 было

9
00:00:26,480 --> 00:00:28,400
трудным испытанием для многих людей, хотя

10
00:00:28,400 --> 00:00:30,720
мы пытались помочь людям в свободное время и в

11
00:00:30,720 --> 00:00:33,200
рабочее время, а в остальном, я надеюсь,

12
00:00:33,200 --> 00:00:34,719
в конце дня это будет выглядеть так, как будто это

13
00:00:34,719 --> 00:00:37,120
было  действительно хороший опыт обучения,

14
00:00:37,120 --> 00:00:39,840
чтобы действительно получить гораздо более

15
00:00:39,840 --> 00:00:42,239
пристальный практический взгляд на то, как работают трансформаторы,

16
00:00:42,239 --> 00:00:45,120
а не просто

17
00:00:45,120 --> 00:00:47,039
загрузка трансформатора в виде черного

18
00:00:47,039 --> 00:00:49,760
таинственного ящика

19
00:00:49,760 --> 00:00:52,559
после пятницы, я думаю, нет отдыха,

20
00:00:52,559 --> 00:00:53,920
так как

21
00:00:53,920 --> 00:00:56,079
мы действительно надеемся, что  вы можете

22
00:00:56,079 --> 00:00:58,559
практически сразу перейти к

23
00:00:58,559 --> 00:01:00,559
работе над финальными проектами, так как у нас в

24
00:01:00,559 --> 00:01:02,800
основном четыре недели до финальных

25
00:01:02,800 --> 00:01:05,040
проектов, и, в частности, мы надеемся

26
00:01:05,040 --> 00:01:07,520
получить отзывы о ваших проектных

27
00:01:07,520 --> 00:01:10,799
предложениях к следующему вторнику, чтобы помочь в

28
00:01:10,799 --> 00:01:13,119
этом процессе или  почитай людей, мы должны

29
00:01:13,119 --> 00:01:16,320
приступить к ним в ближайшее время,

30
00:01:16,320 --> 00:01:17,920
и вы знаете

31
00:01:17,920 --> 00:01:18,960


32
00:01:18,960 --> 00:01:21,280
, что, может быть, это просто хороший

33
00:01:21,280 --> 00:01:23,119
момент, чтобы сказать, что вы знаете, мы действительно

34
00:01:23,119 --> 00:01:24,560
ценим всех людей, которые

35
00:01:24,560 --> 00:01:26,000
приложили массу усилий для выполнения этих

36
00:01:26,000 --> 00:01:29,520
заданий, и нам это нравится.

37
00:01:29,520 --> 00:01:30,960
Ключевые слова, которые мы наблюдаем у

38
00:01:30,960 --> 00:01:33,280
студентов, в порядке, так что с этим в

39
00:01:33,280 --> 00:01:36,880
сторону, сегодня я рад,

40
00:01:36,880 --> 00:01:39,280
что прочитал сегодняшнюю лекцию о

41
00:01:39,280 --> 00:01:41,680
генерации нейронного языка босс антран Лу,

42
00:01:41,680 --> 00:01:44,560
который в настоящее время постдок в Стэнфорде,

43
00:01:44,560 --> 00:01:46,880
он тот, кто много сделал  работы над

44
00:01:46,880 --> 00:01:49,119
генерацией естественного языка в его

45
00:01:49,119 --> 00:01:50,799
предыдущей жизни,

46
00:01:50,799 --> 00:01:53,280
когда он был аспирантом Вашингтонского

47
00:01:53,280 --> 00:01:55,439
университета, а в следующем году он собирается занять

48
00:01:55,439 --> 00:01:56,719


49
00:01:56,719 --> 00:01:57,439


50
00:01:57,439 --> 00:01:59,439
должность профессора

51
00:01:59,439 --> 00:02:03,439
в Швейцарии, хорошо, так что добро пожаловать и трон,

52
00:02:03,439 --> 00:02:06,000
спасибо, Крис, это очень доброе

53
00:02:06,000 --> 00:02:07,280
введение

54
00:02:07,280 --> 00:02:10,318
.  Приятно быть здесь, прочтением

55
00:02:10,318 --> 00:02:12,879
этой лекции,

56
00:02:12,879 --> 00:02:14,720
cs224n,

57
00:02:14,720 --> 00:02:17,280
особенно по одной из моих

58
00:02:17,280 --> 00:02:19,920
любимых тем в глубоком обучении

59
00:02:19,920 --> 00:02:22,800
для nlp, э-э, естественного языка,

60
00:02:22,800 --> 00:02:24,000


61
00:02:24,000 --> 00:02:25,200
и, надеюсь, со стороны en  d этой

62
00:02:25,200 --> 00:02:27,120
лекции, эм, большинство из вас хотя

63
00:02:27,120 --> 00:02:29,920
бы немного узнают об LG с

64
00:02:29,920 --> 00:02:32,160
глубоким обучением и, надеюсь, у вас появится мотивация

65
00:02:32,160 --> 00:02:34,480
начать исследования по этому

66
00:02:34,480 --> 00:02:38,080
поводу или запустить стартап в NLG или, возможно,

67
00:02:38,080 --> 00:02:42,080
поработать над ним в более крупной организации

68
00:02:42,080 --> 00:02:43,280
хорошо

69
00:02:43,280 --> 00:02:44,160


70
00:02:44,160 --> 00:02:45,840
, для начала, я думаю, было бы действительно

71
00:02:45,840 --> 00:02:48,000
полезно определить то, что мы встречаем на

72
00:02:48,000 --> 00:02:49,760
высоком уровне, когда мы говорим о

73
00:02:49,760 --> 00:02:52,239
генерации естественного языка, потому что

74
00:02:52,239 --> 00:02:53,680
вы знаете, что за последние несколько лет

75
00:02:53,680 --> 00:02:55,920
определение фактически изменилось

76
00:02:55,920 --> 00:02:58,239
и действительно выросло как  a в качестве подполя,

77
00:02:58,239 --> 00:02:58,959


78
00:02:58,959 --> 00:03:00,080
чтобы действительно

79
00:03:00,080 --> 00:03:02,400
инкапсулировать любую часть nlp, которая

80
00:03:02,400 --> 00:03:04,800
включает в себя создание письменной

81
00:03:04,800 --> 00:03:06,720
или устной речи,

82
00:03:06,720 --> 00:03:08,400
другими словами, если вам предоставлены некоторые

83
00:03:08,400 --> 00:03:11,040
данные, и ваша цель - создать

84
00:03:11,040 --> 00:03:13,760
текст, который вы знаете, описать, ответить,

85
00:03:13,760 --> 00:03:15,680
перевести или обобщить это  кусок

86
00:03:15,680 --> 00:03:16,640
текста

87
00:03:16,640 --> 00:03:17,599
ммм, как

88
00:03:17,599 --> 00:03:19,519
вы знаете, nlg действительно фокусируется на том, как вы

89
00:03:19,519 --> 00:03:21,200
можете на самом деле создать систему,

90
00:03:21,200 --> 00:03:23,280
которая может автоматически создавать

91
00:03:23,280 --> 00:03:26,400
последовательный и полезный письменный

92
00:03:26,400 --> 00:03:28,319
фрагмент текста для этого человека

93
00:03:28,319 --> 00:03:30,159
для человека  mption

94
00:03:30,159 --> 00:03:32,400
um, и раньше это была гораздо более ограниченная

95
00:03:32,400 --> 00:03:34,560
область исследования, потому что многие задачи, которые

96
00:03:34,560 --> 00:03:36,640
мы теперь рассматриваем как nlg, на

97
00:03:36,640 --> 00:03:39,360
самом деле не включали в себя много текста

98
00:03:39,360 --> 00:03:41,680
до появления нейронных сетей, но теперь

99
00:03:41,680 --> 00:03:44,000
эта область значительно расширилась, и

100
00:03:44,000 --> 00:03:46,799
у нас есть это  это гораздо более обширная область

101
00:03:46,799 --> 00:03:49,200
для своего рода работы,

102
00:03:49,200 --> 00:03:50,799
к сожалению, мы вам еще не знакомы

103
00:03:50,799 --> 00:03:52,560
на

104
00:03:52,560 --> 00:03:54,640
уровне типов инструментов aai и lg,

105
00:03:54,640 --> 00:03:56,400
которые мы видели в поп-культуре и

106
00:03:56,400 --> 00:03:58,799
которые мы хотели бы представить, но  мы

107
00:03:58,799 --> 00:04:01,040
начинаем видеть многие области, в которых

108
00:04:01,040 --> 00:04:05,040
инструменты nlg имеют огромное влияние.

109
00:04:05,040 --> 00:04:06,720
Вы знаете, что начать с машинного

110
00:04:06,720 --> 00:04:08,799
перевода - это своего рода классический

111
00:04:08,799 --> 00:04:13,519
пример задачи nlg.

112
00:04:13,519 --> 00:04:15,840


113
00:04:15,840 --> 00:04:18,079
перешел на нейронные сети в нейронные сети

114
00:04:18,079 --> 00:04:22,000
и среду, отличную от LG, примерно в 2014 году,

115
00:04:22,000 --> 00:04:24,080
и теперь мы наблюдаем быстрое

116
00:04:24,080 --> 00:04:25,520
улучшение качества и применимости

117
00:04:25,520 --> 00:04:27,759
систем перевода, и на самом деле вы

118
00:04:27,759 --> 00:04:30,400
часто можете использовать Google Translate для большей части

119
00:04:30,400 --> 00:04:33,919
вашего вида розничной торговли  переводить  Иону нужен

120
00:04:33,919 --> 00:04:34,960


121
00:04:34,960 --> 00:04:37,680
как хорошая отправная точка.

122
00:04:37,680 --> 00:04:40,639
Точно так же технологии NLG действительно

123
00:04:40,639 --> 00:04:42,479
лежат в основе некоторых диалоговых систем, с

124
00:04:42,479 --> 00:04:44,560
которыми вы могли бы взаимодействовать ежедневно.

125
00:04:44,560 --> 00:04:47,440


126
00:04:47,440 --> 00:04:50,800


127
00:04:50,800 --> 00:04:52,240


128
00:04:52,240 --> 00:04:54,240
Диалоговая система других крупных компаний, есть

129
00:04:54,240 --> 00:04:55,840
большая вероятность, что

130
00:04:55,840 --> 00:04:58,400
в эту систему встроен нейронный компонент nlg, который

131
00:04:58,400 --> 00:04:59,840
участвует в предоставлении вам

132
00:04:59,840 --> 00:05:01,520
ответа на ваш запрос,

133
00:05:01,520 --> 00:05:02,639
и

134
00:05:02,639 --> 00:05:04,560
в этой области действительно еще предстоит многое сделать, и

135
00:05:04,560 --> 00:05:06,400
это привело к некоторым  крупные компании, которые на

136
00:05:06,400 --> 00:05:08,240
самом деле используют технологии чат-ботов

137
00:05:08,240 --> 00:05:10,080
у исследователей и

138
00:05:10,080 --> 00:05:11,680
студентов, таких как вы

139
00:05:11,680 --> 00:05:13,520
, чтобы продолжать пытаться добиться больших

140
00:05:13,520 --> 00:05:16,320
успехов в этой области

141
00:05:16,320 --> 00:05:18,639
, мы также знаем, что видим много

142
00:05:18,639 --> 00:05:21,759
технологий NLG, используемых в

143
00:05:21,759 --> 00:05:24,479
таких областях, как обобщение, где

144
00:05:24,479 --> 00:05:27,840
системам часто приходится объединять

145
00:05:27,840 --> 00:05:30,000
информацию из потенциально нескольких

146
00:05:30,000 --> 00:05:32,479
источников и

147
00:05:32,479 --> 00:05:35,680
перефразировать наиболее заметное содержание в

148
00:05:35,680 --> 00:05:38,720
сокращенном, но  до очень увлекательного способа, в

149
00:05:38,720 --> 00:05:40,080
то время как наш пример

150
00:05:40,080 --> 00:05:42,320
резюмирования обычно связан с

151
00:05:42,320 --> 00:05:44,720
, скажем, генерацией основных новостных моментов.

152
00:05:44,720 --> 00:05:46,639
Системы реферирования, как вы

153
00:05:46,639 --> 00:05:48,880
знаете, достигли широкого применения во

154
00:05:48,880 --> 00:05:51,120
многих областях, где мы принимаем контент, такой

155
00:05:51,120 --> 00:05:53,919
как подведение итогов электронных писем или подведение итогов

156
00:05:53,919 --> 00:05:56,160
стенограмм собраний

157
00:05:56,160 --> 00:05:57,680
и  на самом деле есть еще много областей, не

158
00:05:57,680 --> 00:05:59,600
перечисленных здесь, я на самом деле не говорил об этом

159
00:05:59,600 --> 00:06:01,919
на слайде, но несколько месяцев назад инструмент,

160
00:06:01,919 --> 00:06:03,360
называемый семантическим исследователем, фактически

161
00:06:03,360 --> 00:06:05,199
разработал нейронную систему для создания

162
00:06:05,199 --> 00:06:07,199
резюме научных статей

163
00:06:07,199 --> 00:06:08,800
, что я лично в конечном

164
00:06:08,800 --> 00:06:11,440
итоге использую  немного в качестве примера того,

165
00:06:11,440 --> 00:06:12,800
как люди могут

166
00:06:12,800 --> 00:06:15,600
взаимодействовать с этими технологиями,

167
00:06:15,600 --> 00:06:17,280
но эти методы на самом деле не

168
00:06:17,280 --> 00:06:20,240
ограничиваются вводом или выводом текста, поэтому на

169
00:06:20,240 --> 00:06:22,400
самом деле классическая область nlg, о которой я

170
00:06:22,400 --> 00:06:24,560
упоминал ранее, заключается в том, как задача, которая

171
00:06:24,560 --> 00:06:26,240
раньше была оформлена, на самом деле была вокруг

172
00:06:26,240 --> 00:06:28,560
того, что  Теперь мы вызываем данные для генерации текста,

173
00:06:28,560 --> 00:06:30,880
так что вы можете научиться компилировать или,

174
00:06:30,880 --> 00:06:32,880
скажем так, обобщать самые интересные факты

175
00:06:32,880 --> 00:06:35,600
из таблицы или графа знаний

176
00:06:35,600 --> 00:06:38,080
или какого-либо типа потока данных,

177
00:06:38,080 --> 00:06:39,840
таким образом люди могут получить наиболее

178
00:06:39,840 --> 00:06:41,360
интересный и заметный контент

179
00:06:41,360 --> 00:06:44,160
, который представлен

180
00:06:44,160 --> 00:06:46,400
в этих структурах данных быстро и

181
00:06:46,400 --> 00:06:48,240
в более легком для восприятия формате, чем

182
00:06:48,240 --> 00:06:49,280
необходимость просматривать структуры

183
00:06:49,280 --> 00:06:51,199


184
00:06:51,199 --> 00:06:54,000
мы также видели много недавних работ

185
00:06:54,000 --> 00:06:55,680
по визуальному описанию,

186
00:06:55,680 --> 00:06:57,759
которые пытаются использовать язык для

187
00:06:57,759 --> 00:07:00,000
описания контента и изображений или видео,

188
00:07:00,000 --> 00:07:02,319
поэтому примерно в 2014 году мы начинаем видеть

189
00:07:02,319 --> 00:07:04,800
первые нейронные системы

190
00:07:04,800 --> 00:07:06,960
NLG в пространстве, и они  на самом деле, вы

191
00:07:06,960 --> 00:07:09,280
знаете, продолжали развиваться в течение последних шести

192
00:07:09,280 --> 00:07:11,039
лет, и теперь мы фактически занимаемся гораздо

193
00:07:11,039 --> 00:07:13,360
более сложными задачами описания, такими

194
00:07:13,360 --> 00:07:15,199
как создание полных описательных

195
00:07:15,199 --> 00:07:18,240
абзацев сцен или

196
00:07:18,240 --> 00:07:20,319
создание потоков визуальных

197
00:07:20,319 --> 00:07:21,840
описаний для потоков визуального

198
00:07:21,840 --> 00:07:24,400
контента, например, в  видео с

199
00:07:24,400 --> 00:07:26,160
субтитрами, и эти инструменты

200
00:07:26,160 --> 00:07:28,479
действительно имеют широкое

201
00:07:28,479 --> 00:07:31,360
применение в различных областях

202
00:07:31,360 --> 00:07:33,360
искусственного интеллекта и, наконец, последний вид

203
00:07:33,360 --> 00:07:34,800
приложений  Я как бы хочу упомянуть,

204
00:07:34,800 --> 00:07:36,479
что мы также начали наблюдать, как lg-

205
00:07:36,479 --> 00:07:38,800
системы разрабатываются в более

206
00:07:38,800 --> 00:07:40,639
творческих приложениях, таких как

207
00:07:40,639 --> 00:07:43,199
генерация историй, где системы искусственного интеллекта теперь могут

208
00:07:43,199 --> 00:07:45,440
помочь людям писать короткие рассказы в

209
00:07:45,440 --> 00:07:48,400
блогах или даже полные книги в некоторых случаях,

210
00:07:48,400 --> 00:07:51,039
как  помощники по творческому письму в

211
00:07:51,039 --> 00:07:52,479
другой области, такой как создание дерева поз,

212
00:07:52,479 --> 00:07:53,599


213
00:07:53,599 --> 00:07:55,199
у нас могут быть полностью

214
00:07:55,199 --> 00:07:56,639
автоматизированные настройки, в которых, как вы знаете,

215
00:07:56,639 --> 00:07:58,560
могут быть агенты искусственного интеллекта, которые могут генерировать

216
00:07:58,560 --> 00:08:00,479
что-то вроде сонета и фактически

217
00:08:00,479 --> 00:08:03,039
обусловливать выполнение многих требований,

218
00:08:03,039 --> 00:08:04,479
которые выполняются.  дается через пользовательский

219
00:08:04,479 --> 00:08:06,479
интерфейс,

220
00:08:06,479 --> 00:08:08,160
поэтому вы знаете, что я надеюсь, что к этому моменту

221
00:08:08,160 --> 00:08:10,879
я действительно дал вам взгляд на

222
00:08:10,879 --> 00:08:13,919
широту nlg-приложений

223
00:08:13,919 --> 00:08:16,319
и на то, как они вроде бы охватывают любую

224
00:08:16,319 --> 00:08:17,440
задачу,

225
00:08:17,440 --> 00:08:19,520
которая, как вы могли подумать, включает в себя

226
00:08:19,520 --> 00:08:21,840
создание текста

227
00:08:21,840 --> 00:08:23,360
и каждого  Для решения этих задач, которые вы знаете, действительно

228
00:08:23,360 --> 00:08:25,120
необходимо знать разные алгоритмы

229
00:08:25,120 --> 00:08:26,800
и разные модели, а также другой

230
00:08:26,800 --> 00:08:28,879
способ проектирования системы,

231
00:08:28,879 --> 00:08:30,400
но их объединяет то, что

232
00:08:30,400 --> 00:08:32,080
многие из них основаны на

233
00:08:32,080 --> 00:08:35,839
достижениях следующего поколения в области глубокого

234
00:08:35,839 --> 00:08:39,120
обучения, для nlg,

235
00:08:39,120 --> 00:08:41,440
и цель сегодняшнего дня - действительно

236
00:08:41,440 --> 00:08:44,159
дать вам введение в эти

237
00:08:44,159 --> 00:08:45,680
темы, что действительно позволит вам внести

238
00:08:45,680 --> 00:08:47,360
свой вклад в следующую эру этих тем.

239
00:08:47,360 --> 00:08:49,519
технологии и разработка систем глубокого обучения

240
00:08:49,519 --> 00:08:52,640
для nlg,

241
00:08:52,640 --> 00:08:54,000
и поэтому я думаю, что для начала вы знаете, что

242
00:08:54,000 --> 00:08:56,640
может быть интересно сделать, это

243
00:08:56,640 --> 00:08:59,200
быстро перечислить темы, которые вы, возможно,

244
00:08:59,200 --> 00:09:00,880
видели на предыдущих лекциях, но которые

245
00:09:00,880 --> 00:09:04,560
будут очень актуальны сегодня,

246
00:09:04,560 --> 00:09:06,800
когда мы  пытаемся разработать систему nlg

247
00:09:06,800 --> 00:09:08,560
, и то, что мы эффективно

248
00:09:08,560 --> 00:09:10,320
пытаемся сделать в настройках, - это, как вы знаете, взять

249
00:09:10,320 --> 00:09:12,720
последовательность токенов в качестве входных

250
00:09:12,720 --> 00:09:14,480
данных и создать новый текст, который

251
00:09:14,480 --> 00:09:16,399
обусловлен этим входом, а затем то, что

252
00:09:16,399 --> 00:09:18,080
мы обычно называем автоматическим

253
00:09:18,080 --> 00:09:20,880
регрессивная настройка, которая является наиболее распространенной

254
00:09:20,880 --> 00:09:23,360
настройкой генерации текста,

255
00:09:23,360 --> 00:09:25,120
мы берем эти созданные токены текста

256
00:09:25,120 --> 00:09:27,120
и возвращаем их в нашу модель, чтобы

257
00:09:27,120 --> 00:09:29,279
сгенерировать следующий токен в последовательности,

258
00:09:29,279 --> 00:09:32,720
которую мы хотим создать  есть,

259
00:09:32,720 --> 00:09:34,480
но чтобы по-настоящему понять, что

260
00:09:34,480 --> 00:09:36,399
происходит в авторегрессивной энергетической

261
00:09:36,399 --> 00:09:38,560
системе, нам действительно нужно начать

262
00:09:38,560 --> 00:09:40,160
делать, это посмотреть, что происходит при

263
00:09:40,160 --> 00:09:42,240
генерации отдельного токена, поскольку

264
00:09:42,240 --> 00:09:44,320
дальнейшие этапы действительно зависят от того,

265
00:09:44,320 --> 00:09:46,320
чтобы взять этот сгенерированный токен и передать его обратно

266
00:09:46,320 --> 00:09:48,640
в качестве входных данных и делает то же самое

267
00:09:48,640 --> 00:09:50,399
, что происходит на низком уровне, так это то, что

268
00:09:50,399 --> 00:09:52,080
ваша модель принимает эту

269
00:09:52,080 --> 00:09:55,120
последовательность входных данных, поэтому эти y, и она

270
00:09:55,120 --> 00:09:57,279
вычисляет вектор оценок, используя саму

271
00:09:57,279 --> 00:09:58,640


272
00:09:58,640 --> 00:10:00,560
модель и каждый индекс и этот вектор

273
00:10:00,560 --> 00:10:02,720
соответствует баллу для токена в

274
00:10:02,720 --> 00:10:04,880
вашем словаре, поэтому единственные токены, которые

275
00:10:04,880 --> 00:10:06,720
действительно разрешено генерировать вашей модели,

276
00:10:06,720 --> 00:10:08,079


277
00:10:08,079 --> 00:10:09,440
а затем то, что вы делаете, - это то, что вы вычисляете

278
00:10:09,440 --> 00:10:11,200
распределение вероятностей по этим

279
00:10:11,200 --> 00:10:13,680
баллам, используя то, что мы называем функцией мягкого максимума,

280
00:10:13,680 --> 00:10:15,760
для вычисления оценки вероятности

281
00:10:15,760 --> 00:10:17,040


282
00:10:17,040 --> 00:10:20,399
для каждого токена в вашем словаре,

283
00:10:20,399 --> 00:10:24,399
учитывая контекст, который ему предшествует,

284
00:10:24,399 --> 00:10:26,399
и в качестве сокращения я просто упомяну,

285
00:10:26,399 --> 00:10:28,560
что иногда я удаляю w из

286
00:10:28,560 --> 00:10:30,320
этого уравнения вероятности

287
00:10:30,320 --> 00:10:31,680
но просто знайте, что когда я записываю

288
00:10:31,680 --> 00:10:34,320
вероятность токена y в момент t,

289
00:10:34,320 --> 00:10:36,959
я имею в виду, что это вероятность того, что y из t

290
00:10:36,959 --> 00:10:39,040
uh является конкретным словом, так что

291
00:10:39,040 --> 00:10:42,160
это своего рода присваивание переменной

292
00:10:42,160 --> 00:10:43,360
um,

293
00:10:43,360 --> 00:10:45,680
но так что на самом деле является выходом  того,

294
00:10:45,680 --> 00:10:47,440
что мы обычно называем моделью генерации текста

295
00:10:47,440 --> 00:10:49,040
на данный момент, на самом деле является этим

296
00:10:49,040 --> 00:10:50,800
вектором оценок, а затем этот вектор

297
00:10:50,800 --> 00:10:52,560
передается в функцию softmax, чтобы

298
00:10:52,560 --> 00:10:54,720
дать нам распределение вероятностей

299
00:10:54,720 --> 00:10:58,399
по набору токенов в словаре,

300
00:10:58,399 --> 00:11:00,160
а затем фактически сгенерировать токен  мы

301
00:11:00,160 --> 00:11:02,000
можем определить то, что мы называем алгоритмом декодирования

302
00:11:02,000 --> 00:11:04,000
, который представляет собой функцию, которая

303
00:11:04,000 --> 00:11:06,720
принимает это распределение uh p по

304
00:11:06,720 --> 00:11:08,880
всем токенам словаря, и вы

305
00:11:08,880 --> 00:11:11,440
знаете, что он определяет функцию для

306
00:11:11,440 --> 00:11:14,480
выбора токена uh из этого распределения в качестве

307
00:11:14,480 --> 00:11:16,640
следующего токена, который создается  э-э, нашей

308
00:11:16,640 --> 00:11:17,600


309
00:11:17,600 --> 00:11:18,720
системой nlg

310
00:11:18,720 --> 00:11:20,240
и для того, чтобы это распределение было

311
00:11:20,240 --> 00:11:22,079
откалибровано таким образом, чтобы это означало

312
00:11:22,079 --> 00:11:24,160
все, что нам нужно для обучения модели,

313
00:11:24,160 --> 00:11:26,959
чтобы она действительно могла выполнять эту задачу,

314
00:11:26,959 --> 00:11:28,240
так что самый

315
00:11:28,240 --> 00:11:29,920
распространенный способ t  дождь из моделей генерации текста,

316
00:11:29,920 --> 00:11:30,959


317
00:11:30,959 --> 00:11:34,399
э-э, заключается в использовании обучения с максимальным правдоподобием,

318
00:11:34,399 --> 00:11:36,240
и, несмотря на его название, мы на самом деле не

319
00:11:36,240 --> 00:11:37,760
максимизируем правдоподобия, мы фактически

320
00:11:37,760 --> 00:11:40,240
минимизировали негативные логарифмические правдоподобия,

321
00:11:40,240 --> 00:11:42,240
и на самом деле это просто

322
00:11:42,240 --> 00:11:44,399
мультиклассовая задача классификации, где

323
00:11:44,399 --> 00:11:46,560
каждое слово в нашем словаре э-э является

324
00:11:46,560 --> 00:11:48,640
класс, который может быть предсказан

325
00:11:48,640 --> 00:11:50,160
моделью, и поэтому на каждом этапе

326
00:11:50,160 --> 00:11:52,000
последовательности мы на самом деле пытаемся

327
00:11:52,000 --> 00:11:53,920
предсказать класс,

328
00:11:53,920 --> 00:11:55,839
который соответствует следующему слову в

329
00:11:55,839 --> 00:11:57,440
последовательности текста, который мы пытаемся

330
00:11:57,440 --> 00:11:58,399
обучить,

331
00:11:58,399 --> 00:12:00,560
и  это слово часто называют золотым

332
00:12:00,560 --> 00:12:02,800
или наземным токеном истины, как просто своего рода

333
00:12:02,800 --> 00:12:05,200
взаимозаменяемый словарь, который мы используем

334
00:12:05,200 --> 00:12:08,480
, а еще один термин для этого

335
00:12:08,480 --> 00:12:11,040
алгоритма обучения - это принуждение учителя, поэтому

336
00:12:11,040 --> 00:12:12,560
вы можете увидеть, что эти выражения как бы

337
00:12:12,560 --> 00:12:15,040
взаимозаменяемы, если вы читаете статьи

338
00:12:15,040 --> 00:12:16,880
по этой теме

339
00:12:16,880 --> 00:12:18,320
но поэтому на каждом шаге вы действительно

340
00:12:18,320 --> 00:12:20,160
вычисляете термин потерь, который представляет собой

341
00:12:20,160 --> 00:12:22,000
отрицательную логарифмическую вероятность предсказания

342
00:12:22,000 --> 00:12:25,040
этого золотого жетона y1 на каждом шаге, поэтому на

343
00:12:25,040 --> 00:12:26,320
этих слайдах, когда  когда вы видите

344
00:12:26,320 --> 00:12:28,639
звездочку рядом с ay,

345
00:12:28,639 --> 00:12:30,240
это означает, что это золотой токен,

346
00:12:30,240 --> 00:12:32,160
который поступает из обучающей последовательности,

347
00:12:32,160 --> 00:12:33,760
и вы можете сделать это за несколько шагов,

348
00:12:33,760 --> 00:12:35,680
сложив логарифмические вероятности

349
00:12:35,680 --> 00:12:38,079
по пути, в

350
00:12:38,079 --> 00:12:39,760
конечном итоге вы придете к

351
00:12:39,760 --> 00:12:41,839
концу  ваша золотая последовательность, и

352
00:12:41,839 --> 00:12:43,440
вы сможете вычислить градиенты по

353
00:12:43,440 --> 00:12:45,839
отношению к этому суммированному члену потерь

354
00:12:45,839 --> 00:12:47,839
для каждого параметра в вашей модели, что

355
00:12:47,839 --> 00:12:49,440
позволит вам обновить его

356
00:12:49,440 --> 00:12:50,720
так, чтобы в следующий раз, когда вы

357
00:12:50,720 --> 00:12:52,480
увидите последовательность, ваша модель будет более

358
00:12:52,480 --> 00:12:54,480
уверенной в  вероятность

359
00:12:54,480 --> 00:12:56,639
того, что последовательность - правильная последовательность,

360
00:12:56,639 --> 00:12:58,240
учитывая тот же контекст, который она видела

361
00:12:58,240 --> 00:13:00,000
раньше,

362
00:13:00,000 --> 00:13:01,680
но большая часть этого должна быть просто резюме

363
00:13:01,680 --> 00:13:03,360
из предыдущих лекций по языковому

364
00:13:03,360 --> 00:13:07,200
моделированию и машинному переводу,

365
00:13:07,200 --> 00:13:08,800
ммм, но теперь, когда у нас это есть

366
00:13:08,800 --> 00:13:11,920
, давайте  перейдем к интересной части и

367
00:13:11,920 --> 00:13:14,079
поговорим о некоторых новых темах,

368
00:13:14,079 --> 00:13:15,839
первая из которых - декодирование, которое на

369
00:13:15,839 --> 00:13:17,760
самом деле является одной из моих любимых тем

370
00:13:17,760 --> 00:13:20,959
в исследованиях генерации естественного языка,

371
00:13:20,959 --> 00:13:23,360
поэтому, если вы помните свои алгоритмы декодирования  thm

372
00:13:23,360 --> 00:13:24,800
- это действительно функция, которая принимает

373
00:13:24,800 --> 00:13:26,160
это индуцированное

374
00:13:26,160 --> 00:13:27,760
распределение вероятностей uh

375
00:13:27,760 --> 00:13:30,000
из вашей модели по следующим возможным

376
00:13:30,000 --> 00:13:32,320
токенам, которые могут быть сгенерированы, и выбирает

377
00:13:32,320 --> 00:13:34,079
, какой из этих токенов должен быть

378
00:13:34,079 --> 00:13:36,160
выведен,

379
00:13:36,160 --> 00:13:38,160
поэтому после обучения вашей модели это

380
00:13:38,160 --> 00:13:40,399
распределение должно быть значимым,

381
00:13:40,399 --> 00:13:42,079
и вы хотите  чтобы иметь возможность сгенерировать

382
00:13:42,079 --> 00:13:44,639
разумный следующий токен,

383
00:13:44,639 --> 00:13:45,920
а затем вы можете использовать эти сгенерированные

384
00:13:45,920 --> 00:13:48,959
следующие токены, чтобы синяя y-шляпа была здесь в

385
00:13:48,959 --> 00:13:50,959
качестве входных данных на следующем шаге модели,

386
00:13:50,959 --> 00:13:52,639
которая позволяет вам пересчитать новое

387
00:13:52,639 --> 00:13:55,199
распределение, декодировать новый токен,

388
00:13:55,199 --> 00:13:57,199
повторить процесс и  в конечном итоге получится

389
00:13:57,199 --> 00:13:59,199
полная последовательность, которую ваша модель теперь

390
00:13:59,199 --> 00:14:01,279
сгенерировала с фиксированной начальной

391
00:14:01,279 --> 00:14:03,839
последовательностью текста,

392
00:14:03,839 --> 00:14:05,760
и поэтому давайте немного поговорим об

393
00:14:05,760 --> 00:14:07,199
алгоритмах, которые мы можем использовать для декодирования

394
00:14:07,199 --> 00:14:10,800
токенов из этого распределения,

395
00:14:10,880 --> 00:14:12,800
чтобы вы на самом деле уже видели некоторые из

396
00:14:12,800 --> 00:14:14,720
них  в предыдущей лекции о нейронном

397
00:14:14,720 --> 00:14:17,279
машинном переводе, я полагаю,

398
00:14:17,279 --> 00:14:19,519
вы начали с того, что увидели

399
00:14:19,519 --> 00:14:21,199
относительно простой алгоритм декодирования,

400
00:14:21,199 --> 00:14:24,079
который, тем не менее, оставил  ins очень популярное

401
00:14:24,079 --> 00:14:26,000
декодирование arg max и с

402
00:14:26,000 --> 00:14:27,600
декодированием ardmax вы в значительной степени просто берете

403
00:14:27,600 --> 00:14:30,480
токен с наибольшей вероятностью um из вашего

404
00:14:30,480 --> 00:14:32,720
дистрибутива в качестве декодированного токена и

405
00:14:32,720 --> 00:14:35,199
отправляете его обратно в модель, чтобы

406
00:14:35,199 --> 00:14:36,720
получить распределение следующего шага,

407
00:14:36,720 --> 00:14:38,639
и вы продолжаете повторять это  процесс, и

408
00:14:38,639 --> 00:14:41,279
это очень приятно, и это очень удобно,

409
00:14:41,279 --> 00:14:43,199
и вы также, я думаю, узнали о

410
00:14:43,199 --> 00:14:45,279
поиске лучей, где вы можете масштабировать

411
00:14:45,279 --> 00:14:47,440
эти жадные методы,

412
00:14:47,440 --> 00:14:49,839
выполнив более широкий поиск по этому набору

413
00:14:49,839 --> 00:14:52,000
токенов, которые следуют за набором наиболее

414
00:14:52,000 --> 00:14:54,079
вероятных  токены, которые следуют, чтобы

415
00:14:54,079 --> 00:14:56,800
попытаться найти подпоследовательность, которая является более низкой

416
00:14:56,800 --> 00:14:58,800
общей

417
00:14:58,800 --> 00:15:00,959
вероятностью отрицательного журнала, даже если

418
00:15:00,959 --> 00:15:02,399
на промежуточном этапе

419
00:15:02,399 --> 00:15:04,079
она имеет тенденцию быть выше, чем то, что было

420
00:15:04,079 --> 00:15:07,040
бы декодированным токеном ardmax,

421
00:15:07,040 --> 00:15:08,800
и хотя эти жадные методы

422
00:15:08,800 --> 00:15:10,399
отлично подходят для

423
00:15:10,399 --> 00:15:12,880
машинного перевода и  в других

424
00:15:12,880 --> 00:15:15,440
тестах, а также, таких как суммирование,

425
00:15:15,440 --> 00:15:17,600
они, как правило, проблематичны во многих

426
00:15:17,600 --> 00:15:19,760
других задачах генерации текста, особенно в

427
00:15:19,760 --> 00:15:22,639
тех, которые в конечном итоге становятся более открытыми.

428
00:15:22,639 --> 00:15:24,240
o одна из этих больших проблем, которые

429
00:15:24,240 --> 00:15:26,000
у них есть, заключается в том, что они часто в конечном итоге

430
00:15:26,000 --> 00:15:27,600
повторяются,

431
00:15:27,600 --> 00:15:29,120
поэтому здесь, в этом примере от

432
00:15:29,120 --> 00:15:31,680
holtzman all 2020, мы можем видеть, что

433
00:15:31,680 --> 00:15:35,120
после примерно 20 или извините, но 60 токенов

434
00:15:35,120 --> 00:15:37,120
или около того,

435
00:15:37,120 --> 00:15:39,040
модель действительно превращается в  просто

436
00:15:39,040 --> 00:15:40,800
повторять одно и то же снова и

437
00:15:40,800 --> 00:15:43,040
снова, это на самом деле часто происходит

438
00:15:43,040 --> 00:15:45,680
в системах генерации текста.

439
00:15:45,680 --> 00:15:47,040
Повторение было на самом деле одной из самых

440
00:15:47,040 --> 00:15:48,720
больших проблем, которую мы

441
00:15:48,720 --> 00:15:50,959
пытались решить в процессе генерации текста в течение многих

442
00:15:50,959 --> 00:15:54,639
лет и с которой сталкиваемся по сей день.  эм, и

443
00:15:54,639 --> 00:15:56,079
вы знаете, я думаю, что стоит

444
00:15:56,079 --> 00:15:59,360
взглянуть на то, почему повторение происходит

445
00:15:59,360 --> 00:16:01,199
немного более аналитически,

446
00:16:01,199 --> 00:16:03,199
чтобы вы могли лучше понять

447
00:16:03,199 --> 00:16:05,680
взаимодействие между вашей моделью и

448
00:16:05,680 --> 00:16:07,360
вашим алгоритмом декодирования,

449
00:16:07,360 --> 00:16:08,959
так что просто в качестве небольшой визуальной

450
00:16:08,959 --> 00:16:10,480
демонстрации

451
00:16:10,480 --> 00:16:12,480
здесь я показываю  вы пошагово

452
00:16:12,480 --> 00:16:14,000
оцениваете вероятность отрицательного журнала из двух

453
00:16:14,000 --> 00:16:16,079
разных языковых моделей, одна основана

454
00:16:16,079 --> 00:16:17,839
на рекуррентных нейронных сетях, а другая -

455
00:16:17,839 --> 00:16:20,320
на языковой модели-преобразователе,

456
00:16:20,320 --> 00:16:23,920
называемой g  пт э, и я показываю этот

457
00:16:23,920 --> 00:16:26,000
сюжет для конкретной фразы, которую я не

458
00:16:26,000 --> 00:16:28,160
знаю, э-э, которая для любого, кто работал с

459
00:16:28,160 --> 00:16:30,959
чат-ботами, наверное, видел э-э, много

460
00:16:30,959 --> 00:16:31,839
раз

461
00:16:31,839 --> 00:16:34,720
потенциально, даже в кошмарах, э-э,

462
00:16:34,720 --> 00:16:36,399
это не очень интересный сюжет, э-э,

463
00:16:36,399 --> 00:16:37,839
хотя вы знаете  что мы действительно видим, так это то,

464
00:16:37,839 --> 00:16:39,440
что модель трансформатора имеет тенденцию быть

465
00:16:39,440 --> 00:16:41,759
немного менее уверенной в вероятности

466
00:16:41,759 --> 00:16:43,519
каждого слова, чем рекуррентная нейронная

467
00:16:43,519 --> 00:16:44,639
сеть,

468
00:16:44,639 --> 00:16:46,079
что более интересно, хотя вот

469
00:16:46,079 --> 00:16:47,920
что произойдет, если я повторю эту же

470
00:16:47,920 --> 00:16:50,480
фразу несколько раз подряд

471
00:16:50,480 --> 00:16:51,759
и  Из того, что мы здесь замечаем,

472
00:16:51,759 --> 00:16:53,920
является то, что повторение этой

473
00:16:53,920 --> 00:16:54,800
фразы

474
00:16:54,800 --> 00:16:57,519
фактически приводит к тому, что

475
00:16:57,519 --> 00:16:59,920
вероятность отрицательного логарифма уровня токена становится все ниже

476
00:16:59,920 --> 00:17:02,000
и ниже для каждого из этих токенов, что на

477
00:17:02,000 --> 00:17:03,199
самом деле означает, что модель

478
00:17:03,199 --> 00:17:05,119
становится более уверенной в том, что

479
00:17:05,119 --> 00:17:07,280
это правильные токены и  что они,

480
00:17:07,280 --> 00:17:08,880
вероятно, должны следовать предыдущему

481
00:17:08,880 --> 00:17:11,520
контексту, поскольку мы генерируем его больше раз,

482
00:17:11,520 --> 00:17:13,119
и это на самом деле не утихает, поскольку

483
00:17:13,119 --> 00:17:15,760
последовательность становится все длиннее, а вы

484
00:17:15,760 --> 00:17:18,640
знаете, если и как  по мере того, как вы повторяете

485
00:17:18,640 --> 00:17:20,799
одни и те же фразы снова и снова,

486
00:17:20,799 --> 00:17:21,919
модель становится все более и более

487
00:17:21,919 --> 00:17:23,919
уверенной, а в следующий раз она

488
00:17:23,919 --> 00:17:26,799
должна сказать то же самое,

489
00:17:26,799 --> 00:17:28,640
и вы знаете, в то время как это действительно

490
00:17:28,640 --> 00:17:30,799
имеет смысл, и если вы знаете,

491
00:17:30,799 --> 00:17:32,960
скажите  фраза, скажем, я устал 15

492
00:17:32,960 --> 00:17:33,919


493
00:17:33,919 --> 00:17:35,919
раз, справедливо поспорить, что в 16-й раз

494
00:17:35,919 --> 00:17:37,360
вы на самом деле собираетесь повторить это снова,

495
00:17:37,360 --> 00:17:38,960
это не обязательно поведение, которое

496
00:17:38,960 --> 00:17:41,520
мы хотим, чтобы наши системы генерации

497
00:17:41,520 --> 00:17:43,120
застряли в

498
00:17:43,120 --> 00:17:44,480
другой интересной вещи, чтобы отметить здесь,

499
00:17:44,480 --> 00:17:46,880
как  кроме того, это поведение на

500
00:17:46,880 --> 00:17:48,720
самом деле менее проблематично в рекуррентных

501
00:17:48,720 --> 00:17:50,640
нейронных сетях, чем в

502
00:17:50,640 --> 00:17:52,559
моделях языка трансформера, так что вы можете видеть, что

503
00:17:52,559 --> 00:17:55,360
для lstm кривая сглаживается

504
00:17:55,360 --> 00:17:57,520
после определенной точки, и поэтому, если вы

505
00:17:57,520 --> 00:17:59,200
помните, возможно, в предыдущей

506
00:17:59,200 --> 00:18:01,440
лекции по  почему нам может понравиться трансформация

507
00:18:01,440 --> 00:18:03,600
языковых моделей, одно из их преимуществ

508
00:18:03,600 --> 00:18:05,120
состоит в том, что у них нет временного

509
00:18:05,120 --> 00:18:07,120
узкого места, связанного с отслеживанием состояния,

510
00:18:07,120 --> 00:18:09,600
которое, как

511
00:18:09,600 --> 00:18:11,600
правило, имеет рекуррентная нейронная сеть, и поэтому удаление  из этого

512
00:18:11,600 --> 00:18:13,360
узкого места на самом деле в конечном итоге делает их

513
00:18:13,360 --> 00:18:15,440
более склонными к повторяющемуся поведению, когда

514
00:18:15,440 --> 00:18:18,400
вы используете жадные алгоритмы для декодирования

515
00:18:18,400 --> 00:18:20,080
эм, но так что мы можем на самом деле сделать, чтобы

516
00:18:20,080 --> 00:18:21,919
уменьшить повторение, поскольку это довольно

517
00:18:21,919 --> 00:18:23,760
большая проблема в этих системах, ну

518
00:18:23,760 --> 00:18:25,440
, на самом деле есть довольно много

519
00:18:25,440 --> 00:18:26,960
предлагаемых подходов  за последние несколько

520
00:18:26,960 --> 00:18:29,520
лет некоторые из них, которые я обобщу здесь,

521
00:18:29,520 --> 00:18:31,520
которые, как вы знаете, варьируются от

522
00:18:31,520 --> 00:18:33,760
вида хакерских, но удивительно эффективных

523
00:18:33,760 --> 00:18:35,760
, не повторяют никаких инграмм во время вывода,

524
00:18:35,760 --> 00:18:37,520


525
00:18:37,520 --> 00:18:38,960
но также были подходы времени обучения

526
00:18:38,960 --> 00:18:40,880
для этого, такие как наличие

527
00:18:40,880 --> 00:18:42,400
функции потерь, которая  минимизирует

528
00:18:42,400 --> 00:18:45,039
сходство между скрытыми активациями

529
00:18:45,039 --> 00:18:47,520
на разных временных шагах или потерю покрытия,

530
00:18:47,520 --> 00:18:49,360
которая наказывает использование

531
00:18:49,360 --> 00:18:51,679
одних и тех же токенов с течением времени, поэтому, если вы

532
00:18:51,679 --> 00:18:53,280
знаете, что измените входные данные, на которых вашей

533
00:18:53,280 --> 00:18:54,720
модели разрешено сосредоточиться, это, естественно,

534
00:18:54,720 --> 00:18:56,559
будет производить другой текст

535
00:18:56,559 --> 00:18:58,400
или больше  в последнее время это маловероятная

536
00:18:58,400 --> 00:18:59,760
цель, которая фактически наказывает

537
00:18:59,760 --> 00:19:01,360
вывод тех же слов, о

538
00:19:01,360 --> 00:19:05,360
которых мы поговорим немного позже,

539
00:19:05,360 --> 00:19:06,960
но правда  заключается в том, что проблема

540
00:19:06,960 --> 00:19:08,320
здесь на

541
00:19:08,320 --> 00:19:10,799
самом деле заключается в использовании жадных алгоритмов,

542
00:19:10,799 --> 00:19:13,440
в первую очередь, во многих

543
00:19:13,440 --> 00:19:16,400
приложениях, которые, как вы знаете, человеческий язык,

544
00:19:16,400 --> 00:19:17,919
люди на самом деле не говорят с

545
00:19:17,919 --> 00:19:20,799
максимальной вероятностью, так что если вы

546
00:19:20,799 --> 00:19:22,480
посмотрите на этот график

547
00:19:22,480 --> 00:19:24,799
Хольцмана за весь 2020 год  он показывает

548
00:19:24,799 --> 00:19:27,360
вероятность написания текста, написанного человеком,

549
00:19:27,360 --> 00:19:29,520
оранжевым цветом, а декодированный текст при поиске луча -

550
00:19:29,520 --> 00:19:32,080
синим на том же графике, и

551
00:19:32,080 --> 00:19:33,440
вы можете видеть, что

552
00:19:33,440 --> 00:19:35,200
декодированный текст при поиске луча имеет тенденцию к очень высокой

553
00:19:35,200 --> 00:19:37,360
вероятности с небольшими отклонениями во

554
00:19:37,360 --> 00:19:39,520
времени, и это  вы знаете, имеет большой смысл,

555
00:19:39,520 --> 00:19:40,880
потому что он буквально пытается

556
00:19:40,880 --> 00:19:42,400
максимизировать вероятность

557
00:19:42,400 --> 00:19:44,720
последовательностей, которые он создает, и большая

558
00:19:44,720 --> 00:19:46,640
часть этого максимизирует известные вам

559
00:19:46,640 --> 00:19:48,720
пошаговые вероятности токенов,

560
00:19:48,720 --> 00:19:50,640
которые он использует, но пока мы можем  Видите

561
00:19:50,640 --> 00:19:52,160
, письменный текст, написанный человеком, гораздо более

562
00:19:52,160 --> 00:19:54,000
изменчив, часто фактически погружаясь в территорию с

563
00:19:54,000 --> 00:19:56,320
очень низкой вероятностью.

564
00:19:56,320 --> 00:19:57,760


565
00:19:57,760 --> 00:19:59,679


566
00:19:59,679 --> 00:20:01,760
h вероятность э-э, вы знаете, что на

567
00:20:01,760 --> 00:20:03,919
самом деле нет причин

568
00:20:03,919 --> 00:20:05,360
слушать друг друга, потому что мы бы знали, что

569
00:20:05,360 --> 00:20:08,000
собирались сказать,

570
00:20:08,000 --> 00:20:09,360
но в конечном итоге то, что мы хотим

571
00:20:09,360 --> 00:20:11,280
сделать, - это сопоставить

572
00:20:11,280 --> 00:20:13,600
неопределенность человеческих языковых моделей в  как мы

573
00:20:13,600 --> 00:20:15,520
декодируем текст

574
00:20:15,520 --> 00:20:17,440
, вот почему во многих приложениях,

575
00:20:17,440 --> 00:20:18,799
которые имеют тенденцию к этой более высокой

576
00:20:18,799 --> 00:20:20,320
вариативности

577
00:20:20,320 --> 00:20:22,320
выборка из этих распределений,

578
00:20:22,320 --> 00:20:25,919
стала своего рода методом декодирования,

579
00:20:25,919 --> 00:20:27,919
особенно в задачах творческой генерации,

580
00:20:27,919 --> 00:20:29,120


581
00:20:29,120 --> 00:20:31,520
и поэтому с помощью выборки мы берем

582
00:20:31,520 --> 00:20:33,760
распределение по токенам, которые

583
00:20:33,760 --> 00:20:36,000
произведенный нашей моделью, и мы сгенерировали, мы

584
00:20:36,000 --> 00:20:38,880
генерируем токен случайным образом в

585
00:20:38,880 --> 00:20:40,640
соответствии с вероятностной массой,

586
00:20:40,640 --> 00:20:42,799
которая назначена каждому потенциальному варианту,

587
00:20:42,799 --> 00:20:45,039
поэтому вместо того, чтобы делать какие-либо жадные

588
00:20:45,039 --> 00:20:46,880
шаги, мы используем вероятность для каждого

589
00:20:46,880 --> 00:20:48,640
токена, чтобы дать нам шанс, что этот

590
00:20:48,640 --> 00:20:51,600
токен  сгенерированный

591
00:20:52,559 --> 00:20:53,520
um,

592
00:20:53,520 --> 00:20:56,000
и поэтому это позволяет нам получить

593
00:20:56,000 --> 00:20:57,360
гораздо больше,

594
00:20:57,360 --> 00:20:59,039
вы знаете о стохастичности и типах

595
00:20:59,039 --> 00:21:01,120
генерируемых токенов,

596
00:21:01,120 --> 00:21:02,880
но возникает проблема  в том,

597
00:21:02,880 --> 00:21:05,600
что эти распределения, как правило, охватывают

598
00:21:05,600 --> 00:21:08,320
очень большой словарный запас, и поэтому даже если

599
00:21:08,320 --> 00:21:10,640
явно есть токены, у которых есть более

600
00:21:10,640 --> 00:21:12,799
высокие шансы быть сгенерированными,

601
00:21:12,799 --> 00:21:14,799
хвост распределения часто может

602
00:21:14,799 --> 00:21:17,520
быть распределен по гораздо большему количеству

603
00:21:17,520 --> 00:21:19,679
возможных токенов,

604
00:21:19,679 --> 00:21:21,360
и поэтому это становится  небольшая проблема,

605
00:21:21,360 --> 00:21:24,320
потому что эти токены в длинном хвосте

606
00:21:24,320 --> 00:21:26,480
, вероятно, совершенно не имеют отношения

607
00:21:26,480 --> 00:21:28,320
к текущему контексту, поэтому у них не должно

608
00:21:28,320 --> 00:21:30,159
быть никаких шансов быть выбранными

609
00:21:30,159 --> 00:21:31,360
индивидуально,

610
00:21:31,360 --> 00:21:33,200
но как группа это заканчивается тем, что есть

611
00:21:33,200 --> 00:21:34,880
приличный шанс, что вы можете вывести

612
00:21:34,880 --> 00:21:38,159
полностью  нерелевантный токен, даже если

613
00:21:38,159 --> 00:21:40,000
90 вашей вероятностной массы находится на соответствующих

614
00:21:40,000 --> 00:21:41,919
токенах, это означает, что у вас есть один

615
00:21:41,919 --> 00:21:43,440
шанс из 10 вывести что-то, что

616
00:21:43,440 --> 00:21:45,840
полностью отбрасывает весь ваш

617
00:21:45,840 --> 00:21:47,919
конвейер генерации текста и является совершенно

618
00:21:47,919 --> 00:21:49,120
бессмысленным,

619
00:21:49,120 --> 00:21:50,880
поэтому для смягчения этого поля

620
00:21:50,880 --> 00:21:53,039
разработано

621
00:21:53,039 --> 00:21:55,039
новое  набор алгоритмов, который пытается

622
00:21:55,039 --> 00:21:56,799
сократить эти распределения во время вывода

623
00:21:56,799 --> 00:21:57,600


624
00:21:57,600 --> 00:22:00,080
um, и поэтому выборка верхнего случая uh является своего

625
00:22:00,080 --> 00:22:02,240
рода наиболее очевидным способом t  o сделать это, так что

626
00:22:02,240 --> 00:22:04,320
здесь мы понимаем, что вы знаете, что

627
00:22:04,320 --> 00:22:06,880
большинство токенов в нашем словаре вообще

628
00:22:06,880 --> 00:22:09,120
не должны иметь вероятности быть выбранными,

629
00:22:09,120 --> 00:22:11,120
поэтому мы просто усекаем набор токенов,

630
00:22:11,120 --> 00:22:12,960
которые нам разрешено выбирать из uh, чтобы

631
00:22:12,960 --> 00:22:16,159
быть k токенами  э-э, вы

632
00:22:16,159 --> 00:22:17,919
знаете наибольшую вероятностную

633
00:22:17,919 --> 00:22:19,919
массу распределений,

634
00:22:19,919 --> 00:22:21,600
и вы знаете, что общие значения k

635
00:22:21,600 --> 00:22:25,280
часто составляют 5 10 20 иногда до 100,

636
00:22:25,280 --> 00:22:26,720
но на самом деле это гиперпараметр, который

637
00:22:26,720 --> 00:22:28,720
вы в конечном итоге устанавливаете как разработчик

638
00:22:28,720 --> 00:22:30,080
этой системы,

639
00:22:30,080 --> 00:22:31,679
в общем  хотя важно

640
00:22:31,679 --> 00:22:33,840
отметить, что чем выше вы сделаете k,

641
00:22:33,840 --> 00:22:35,600
тем больше вы сможете генерировать

642
00:22:35,600 --> 00:22:37,520
разнообразные результаты, что хорошо, потому

643
00:22:37,520 --> 00:22:38,960
что это то, что мы пытаемся сделать, но

644
00:22:38,960 --> 00:22:41,120
вы также собираетесь увеличить, вы

645
00:22:41,120 --> 00:22:43,039
знаете, шанс позволить  этот длинный

646
00:22:43,039 --> 00:22:44,960
хвост просачивается внутрь и генерирует что-то,

647
00:22:44,960 --> 00:22:46,400
что совершенно не имеет отношения к

648
00:22:46,400 --> 00:22:48,159
текущему контексту,

649
00:22:48,159 --> 00:22:50,400
о, извините, тем временем, если вы уменьшите k,

650
00:22:50,400 --> 00:22:52,240
ваши результаты будут в большей безопасности от

651
00:22:52,240 --> 00:22:54,240
этих эффектов длинного хвоста, но ваш текст

652
00:22:54,240 --> 00:22:56,559
может оказаться скучным и общим

653
00:22:56,559 --> 00:22:58,080
потому что ваш алгоритм выборки

654
00:22:58,080 --> 00:22:59,679
начинает выглядеть намного более жадным

655
00:22:59,679 --> 00:23:01,679
по своей природе

656
00:23:01,679 --> 00:23:03,600
um, и этот вид показывает проблему

657
00:23:03,600 --> 00:23:06,559
наличия фиксированного k как количества токенов,

658
00:23:06,559 --> 00:23:07,840
которые вы можете сгенерировать из своего

659
00:23:07,840 --> 00:23:10,559
дистрибутива um, если в вашем распределении

660
00:23:10,559 --> 00:23:12,400
определенная точка довольно плоская, например  в

661
00:23:12,400 --> 00:23:14,720
этом примере она сказала, что я никогда не

662
00:23:14,720 --> 00:23:16,559
пропускаю, вы можете не захотеть усекать

663
00:23:16,559 --> 00:23:18,559
много интересных вариантов, которые вы знаете,

664
00:23:18,559 --> 00:23:20,400
используя небольшое значение k, когда есть так

665
00:23:20,400 --> 00:23:22,880
много хороших вариантов, которые могли бы вписаться

666
00:23:22,880 --> 00:23:25,840
в это в этом потенциальном контексте,

667
00:23:25,840 --> 00:23:27,200
вы знаете, наоборот  в другом

668
00:23:27,200 --> 00:23:28,159


669
00:23:28,159 --> 00:23:30,320
примере вы можете захотеть отрезать гораздо больше,

670
00:23:30,320 --> 00:23:32,720
чем, скажем, ваши минимальные k вариантов,

671
00:23:32,720 --> 00:23:34,320
потому что только часть из них в конечном итоге

672
00:23:34,320 --> 00:23:36,240
будет вполне подходящей, и вы знаете, что большее

673
00:23:36,240 --> 00:23:37,919
k, чем действительно необходимо, позволяет этому

674
00:23:37,919 --> 00:23:39,919
длинному хвосту просачиваться, и, возможно, вы

675
00:23:39,919 --> 00:23:42,559
знаете, разрушьте свое поколение,

676
00:23:42,559 --> 00:23:44,960
и вы знаете, что в ответ на этот

677
00:23:44,960 --> 00:23:47,440
верхний p или выборку ядра - это способ

678
00:23:47,440 --> 00:23:49,520
обойти эту проблему, поэтому здесь вместо

679
00:23:49,520 --> 00:23:51,200
выборки из фиксированного количества токенов

680
00:23:51,200 --> 00:23:53,520
на каждом шаге вы  достаточно от фиксированного

681
00:23:53,520 --> 00:23:55,679
количества вероятностной массы,

682
00:23:55,679 --> 00:23:58,400
и поэтому, в зависимости от равномерности вашего

683
00:23:58,400 --> 00:24:02,240
распределения, вы в конечном итоге включаете

684
00:24:02,240 --> 00:24:04,480
переменное количество токенов

685
00:24:04,480 --> 00:24:06,080
, которое как бы

686
00:24:06,080 --> 00:24:08,400
динамически изменяется в зависимости от того

687
00:24:08,400 --> 00:24:10,240
, как вы знаете, как эта вероятностная масса распределяется

688
00:24:10,240 --> 00:24:12,400
по распределению,

689
00:24:12,400 --> 00:24:14,240
и так  вы знаете, как описать это

690
00:24:14,240 --> 00:24:16,159
визуально, если у вас есть три

691
00:24:16,159 --> 00:24:18,559
разных дистрибутива на определенном

692
00:24:18,559 --> 00:24:21,120
этапе для генерации определенного токена

693
00:24:21,120 --> 00:24:22,559
, каждый из которых собирается отсечь разное

694
00:24:22,559 --> 00:24:25,360
количество токенов из доступного набора

695
00:24:25,360 --> 00:24:27,279
, из которого вы можете выбрать образец в

696
00:24:27,279 --> 00:24:29,919
зависимости от того, что  значение p

697
00:24:29,919 --> 00:24:31,760
и какова пикообразность этого

698
00:24:31,760 --> 00:24:35,360
распределения на самом деле в конечном итоге оказывается,

699
00:24:37,679 --> 00:24:40,159
так что вы знаете, я продолжаю говорить об этой

700
00:24:40,159 --> 00:24:43,039
концепции плоского распределения, а это

701
00:24:43,039 --> 00:24:45,039
как бы критически важно для понимания

702
00:24:45,039 --> 00:24:47,120
того, из скольких токенов мы можем фактически провести

703
00:24:47,120 --> 00:24:49,120
выборку из

704
00:24:49,120 --> 00:24:51,120
и в  Фактически, когда мы пытаемся использовать

705
00:24:51,120 --> 00:24:52,880
алгоритмы выборки, мы можем обнаружить, что модель,

706
00:24:52,880 --> 00:24:54,960
которую мы изучили,

707
00:24:54,960 --> 00:24:56,559
может на самом деле не производить

708
00:24:56,559 --> 00:24:58,159
распределение вероятностей  s, которые

709
00:24:58,159 --> 00:24:59,760
очень хорошо подходят для использования этих

710
00:24:59,760 --> 00:25:01,200
типов алгоритмов выборки, вы знаете,

711
00:25:01,200 --> 00:25:03,120
что распределения могут быть слишком плоскими, они

712
00:25:03,120 --> 00:25:04,880
могут быть слишком острыми,

713
00:25:04,880 --> 00:25:06,400
и на самом деле мы, возможно, захотим

714
00:25:06,400 --> 00:25:08,240
изменить масштаб этих распределений, чтобы они лучше

715
00:25:08,240 --> 00:25:10,080
соответствовали алгоритму декодирования,

716
00:25:10,080 --> 00:25:12,240
который мы могли бы захотеть  использовать,

717
00:25:12,240 --> 00:25:14,480
и мы можем сделать это с помощью метода,

718
00:25:14,480 --> 00:25:16,480
который, как вы знаете, имеет множество

719
00:25:16,480 --> 00:25:18,080
разных имен, которые я называю температурным

720
00:25:18,080 --> 00:25:20,159
масштабированием, и здесь вы

721
00:25:20,159 --> 00:25:22,720
применяете линейный коэффициент

722
00:25:22,720 --> 00:25:25,200
к каждому баллу для каждого токена,

723
00:25:25,200 --> 00:25:26,559
прежде чем передать его

724
00:25:26,559 --> 00:25:28,640
softmax этот температурный

725
00:25:28,640 --> 00:25:30,159
коэффициент одинаков для каждого токена, поэтому он не

726
00:25:30,159 --> 00:25:31,600
меняется динамически среди вашего

727
00:25:31,600 --> 00:25:35,360
словаря, он остается неизменным,

728
00:25:35,360 --> 00:25:37,360
но происходит то, что это

729
00:25:37,360 --> 00:25:39,840
изменение в конечном итоге усиливается

730
00:25:39,840 --> 00:25:41,360
функцией softmax,

731
00:25:41,360 --> 00:25:42,720
и в конечном итоге происходит то, что  если

732
00:25:42,720 --> 00:25:44,640
ваш температурный коэффициент

733
00:25:44,640 --> 00:25:46,159
больше единицы, вы на самом деле собираетесь сделать

734
00:25:46,159 --> 00:25:48,400
свое распределение вероятностей гораздо

735
00:25:48,400 --> 00:25:50,559
более однородным

736
00:25:50,559 --> 00:25:51,679
, другими словами, вы собираетесь  тем временем, чтобы сделать

737
00:25:51,679 --> 00:25:53,200
его более плоским,

738
00:25:53,200 --> 00:25:54,559
если ваш температурный

739
00:25:54,559 --> 00:25:56,400
коэффициент меньше единицы, баллы

740
00:25:56,400 --> 00:25:57,919
будут увеличиваться, что

741
00:25:57,919 --> 00:26:00,400
сделает ваши распределения более резкими и

742
00:26:00,400 --> 00:26:02,000


743
00:26:02,000 --> 00:26:03,919
приведет к тому, что масса вероятности будет отодвигаться к наиболее вероятным

744
00:26:03,919 --> 00:26:06,320
токенам.

745
00:26:06,559 --> 00:26:08,320


746
00:26:08,320 --> 00:26:09,840
что на самом деле это не

747
00:26:09,840 --> 00:26:12,240
алгоритм декодирования, это просто способ

748
00:26:12,240 --> 00:26:14,720
перебалансировать ваше распределение вероятностей, так что на

749
00:26:14,720 --> 00:26:16,320
самом деле он может быть применен ко всем

750
00:26:16,320 --> 00:26:18,799
алгоритмам выборки, которые я описал ранее,

751
00:26:18,799 --> 00:26:21,360
а также к некоторым жадным алгоритмам декодирования, а

752
00:26:21,360 --> 00:26:23,520
также к единственному, на поведение которого это

753
00:26:23,520 --> 00:26:26,080
не влияет.  с помощью мягкого масштабирования максимальной температуры

754
00:26:26,080 --> 00:26:28,720
- это декодирование ardmax,

755
00:26:28,720 --> 00:26:30,159
потому что даже если вы изменяете

756
00:26:30,159 --> 00:26:32,480
относительные величины

757
00:26:32,480 --> 00:26:34,320
вероятностной массы в своем распределении,

758
00:26:34,320 --> 00:26:36,000
вы фактически не меняете относительный

759
00:26:36,000 --> 00:26:38,320
рейтинг среди токенов в этом

760
00:26:38,320 --> 00:26:40,320
распределении, поэтому декодирование art max

761
00:26:40,320 --> 00:26:44,640
даст вам то же самое  вывод, как и раньше,

762
00:26:46,240 --> 00:26:47,840
но теперь, когда мы думаем о том, как мы

763
00:26:47,840 --> 00:26:50,559
могли бы узнать об изменении дистрибутива,

764
00:26:50,559 --> 00:26:52,320
который является профессиональным  под влиянием нашей модели мы могли бы

765
00:26:52,320 --> 00:26:53,600
понять, что мы могли бы изменить

766
00:26:53,600 --> 00:26:55,760
больше, чем относительные величины, которые я

767
00:26:55,760 --> 00:26:58,240
упомянул um, а также вместо этого изменить

768
00:26:58,240 --> 00:26:59,520
их ранжирование по отношению друг к

769
00:26:59,520 --> 00:27:01,600
другу, возможно, на самом деле вы знаете, что наша

770
00:27:01,600 --> 00:27:04,880
модель не является идеальным um

771
00:27:04,880 --> 00:27:07,120
приближением к вам  знать, каким

772
00:27:07,120 --> 00:27:08,880
должно быть распределение по токенам; вы

773
00:27:08,880 --> 00:27:10,400
знаете, возможно, обучение было выполнено

774
00:27:10,400 --> 00:27:12,000
неправильно или у нас не было достаточно

775
00:27:12,000 --> 00:27:13,840
данных для обучения, чтобы на самом деле сделать его хорошо

776
00:27:13,840 --> 00:27:16,240
откалиброванным, и поэтому, если мы решим, что наша

777
00:27:16,240 --> 00:27:18,080
модель недостаточно хорошо откалибрована для задачи

778
00:27:18,080 --> 00:27:20,080
что мы делаем, мы можем захотеть ввести

779
00:27:20,080 --> 00:27:22,000
внешнюю информацию

780
00:27:22,000 --> 00:27:23,679
во время декодирования,

781
00:27:23,679 --> 00:27:25,360
и поэтому здесь я хочу немного поговорить

782
00:27:25,360 --> 00:27:28,320
о новых классах методов, которые позволяют нам

783
00:27:28,320 --> 00:27:29,360
изменять

784
00:27:29,360 --> 00:27:31,440
распределения прогнозирования нашей модели во

785
00:27:31,440 --> 00:27:34,000
время вывода, а не полагаться

786
00:27:34,000 --> 00:27:36,559
на фиксированный  Э-э, статическая модель, которая

787
00:27:36,559 --> 00:27:38,399
обучалась только один раз,

788
00:27:38,399 --> 00:27:40,000
а крутой способ сделать это,

789
00:27:40,000 --> 00:27:42,880
появившийся в прошлом году, - это фактически использовать

790
00:27:42,880 --> 00:27:45,360
э-э, языковые модели ближайшего соседа, э-э,

791
00:27:45,360 --> 00:27:47,440
которые позволяют вам повторно откалибровать ваш

792
00:27:47,440 --> 00:27:49,679
outpu  t распределение вероятностей

793
00:27:49,679 --> 00:27:52,559
с использованием статистики фраз из,

794
00:27:52,559 --> 00:27:56,159
скажем, гораздо большего корпуса,

795
00:27:56,159 --> 00:27:57,440
и поэтому вы знаете, что вы делаете в этом

796
00:27:57,440 --> 00:28:00,320
методе, так это то, что вы инициализируете большую

797
00:28:00,320 --> 00:28:03,440
базу данных фраз вместе с

798
00:28:03,440 --> 00:28:06,960
векторными представлениями для этих фраз,

799
00:28:06,960 --> 00:28:09,039
а затем во время декодирования

800
00:28:09,039 --> 00:28:10,960
вы можете искать  наиболее похожие

801
00:28:10,960 --> 00:28:13,039
фразы в базе данных, и поэтому

802
00:28:13,039 --> 00:28:14,640
вы знаете, что берете

803
00:28:14,640 --> 00:28:16,240


804
00:28:16,240 --> 00:28:17,840
представление контекста, которое у вас есть, из своей

805
00:28:17,840 --> 00:28:20,159
модели, и вы вычисляете функцию подобия

806
00:28:20,159 --> 00:28:21,760
со всеми

807
00:28:21,760 --> 00:28:24,240
представлениями фраз, которые вы сохранили, и

808
00:28:24,240 --> 00:28:25,600
вы знаете  основываясь на относительных

809
00:28:25,600 --> 00:28:26,880
различиях между этими разными

810
00:28:26,880 --> 00:28:28,880
фразами в вашем текущем контексте,

811
00:28:28,880 --> 00:28:31,919
вы можете вычислить распределение по

812
00:28:31,919 --> 00:28:34,320
этим наиболее похожим фразам, а затем вы

813
00:28:34,320 --> 00:28:36,880
можете взять следующие токены, следующие за

814
00:28:36,880 --> 00:28:38,559
этими фразами,

815
00:28:38,559 --> 00:28:40,640
и добавить статистику по этим

816
00:28:40,640 --> 00:28:42,559
фразам к распределению

817
00:28:42,559 --> 00:28:44,960
из вашей модели и  так что это позволяет

818
00:28:44,960 --> 00:28:46,640
вам действительно сбалансировать распределение вероятностей,

819
00:28:46,640 --> 00:28:48,640
которое ваша модель

820
00:28:48,640 --> 00:28:51,919
дала вам, с помощью этого  это индуцированное

821
00:28:51,919 --> 00:28:53,600
распределение по фразам и

822
00:28:53,600 --> 00:28:55,600
их интерполяция вместе, чтобы получить

823
00:28:55,600 --> 00:28:57,279
другую оценку того, насколько вероятно, что определенные

824
00:28:57,279 --> 00:29:00,399
слова являются

825
00:29:01,840 --> 00:29:04,399
одним вопросом прямо сейчас, гм, откуда вы

826
00:29:04,399 --> 00:29:07,679
знаете, что держать в клетке наличными,

827
00:29:07,679 --> 00:29:10,840
да, так что это действительно хороший вопрос,

828
00:29:10,840 --> 00:29:12,640


829
00:29:12,640 --> 00:29:14,320
я думаю, вы знаете  ответ заключается в

830
00:29:14,320 --> 00:29:16,240
том, что вы, вероятно, знаете, что решите

831
00:29:16,240 --> 00:29:18,320
, какой набор фраз может быть наиболее заметным, в

832
00:29:18,320 --> 00:29:20,080
зависимости от, скажем, названных

833
00:29:20,080 --> 00:29:22,159
объектов, которые могут вас заинтересовать,

834
00:29:22,159 --> 00:29:24,159
или фраз, которые вы знаете, что

835
00:29:24,159 --> 00:29:26,159
распределение вашей модели не очень хорошо обрабатывается,

836
00:29:26,159 --> 00:29:27,279


837
00:29:27,279 --> 00:29:29,679
но я '  Я почти уверен, что в этой работе

838
00:29:29,679 --> 00:29:32,240
они взяли каждую фразу

839
00:29:32,240 --> 00:29:34,480
в своем обучающем корпусе, кэшировали ее, а

840
00:29:34,480 --> 00:29:36,480
затем использовали очень эффективные алгоритмы

841
00:29:36,480 --> 00:29:39,360
для выполнения этого поиска, ну, сверх

842
00:29:39,360 --> 00:29:41,520
схожести представлений, чтобы на самом деле

843
00:29:41,520 --> 00:29:43,919
найти наиболее вероятные, хотя

844
00:29:43,919 --> 00:29:45,840
они и сократили  количество фраз, которые

845
00:29:45,840 --> 00:29:47,679
они на самом деле использовали для создания этого

846
00:29:47,679 --> 00:29:50,080
распределения, э-э, распределения фраз,

847
00:29:50,080 --> 00:29:55,000
которые не были охарактеризованы по всему корпусу,

848
00:29:58,799 --> 00:29:59,919
ммм,

849
00:29:59,919 --> 00:30:02,240
так что это фантастика, что w

850
00:30:02,240 --> 00:30:04,240
Теперь мы можем перебалансировать эти распределения, если мы

851
00:30:04,240 --> 00:30:06,640
обнаружим, что наша модель работает плохо

852
00:30:06,640 --> 00:30:08,559
для э-э, вы знаете, в частности, вы знаете, что

853
00:30:08,559 --> 00:30:09,919
это может быть актуально, если, скажем,

854
00:30:09,919 --> 00:30:12,159
мы прыгаем в новый домен, поэтому

855
00:30:12,159 --> 00:30:14,720
мы обучили хорошую большую модель генерации

856
00:30:14,720 --> 00:30:17,520
to to в тексте википедии, и теперь

857
00:30:17,520 --> 00:30:19,679
мы переходим к тому, что вы

858
00:30:19,679 --> 00:30:22,880
знаете больше, более связанным с историями, которые, как вы

859
00:30:22,880 --> 00:30:24,320
знаете, мы могли бы захотеть использовать этот тип

860
00:30:24,320 --> 00:30:26,640
системы, чтобы получать оттуда распределения из фраз,

861
00:30:26,640 --> 00:30:27,840


862
00:30:27,840 --> 00:30:29,440
но вы знаете, что также возможно, что мы

863
00:30:29,440 --> 00:30:31,279
можем  не всегда есть хорошая база данных

864
00:30:31,279 --> 00:30:33,440
фраз, чтобы помочь нам откалибровать выходные

865
00:30:33,440 --> 00:30:35,600
распределения для всех типов

866
00:30:35,600 --> 00:30:37,840
текстов, которые мы хотим сгенерировать,

867
00:30:37,840 --> 00:30:39,600
и, к счастью, в прошлом

868
00:30:39,600 --> 00:30:41,360
году также были, ну, вы знаете, новые

869
00:30:41,360 --> 00:30:43,200
подходы, которые смотрят на то, чтобы делать это в

870
00:30:43,200 --> 00:30:45,440
градиенте.  основанный на способе,

871
00:30:45,440 --> 00:30:47,600
и поэтому идея здесь состоит в том, что вы можете

872
00:30:47,600 --> 00:30:49,760
фактически определить некоторый тип внешней

873
00:30:49,760 --> 00:30:50,880
цели,

874
00:30:50,880 --> 00:30:53,919
используя классификатор, который мы обычно

875
00:30:53,919 --> 00:30:56,720
называем дискриминатором, но на этом рисунке

876
00:30:56,720 --> 00:30:58,240
из предложенной статьи

877
00:30:58,240 --> 00:31:00,080
они называют  ed модель атрибута,

878
00:31:00,080 --> 00:31:02,399
и то, что делает этот классификатор, заключается в том, что он

879
00:31:02,399 --> 00:31:04,640
приближает некоторое свойство, которое вы

880
00:31:04,640 --> 00:31:06,799
хотели бы стимулировать, чтобы ваш текст

881
00:31:06,799 --> 00:31:08,799
отображался при декодировании, так что, возможно, это

882
00:31:08,799 --> 00:31:10,880
классификатор настроений, потому что вы

883
00:31:10,880 --> 00:31:13,279
работаете над диалоговой моделью, а это и

884
00:31:13,279 --> 00:31:15,600
вы хотите  поощряйте позитивно звучащие

885
00:31:15,600 --> 00:31:17,120
комментарии,

886
00:31:17,120 --> 00:31:18,399
так что тогда, когда вы

887
00:31:18,399 --> 00:31:20,640
генерируете текст, вы вводите вывод

888
00:31:20,640 --> 00:31:23,120
своей модели генерации текста в

889
00:31:23,120 --> 00:31:24,640
эту модель атрибутов, и есть

890
00:31:24,640 --> 00:31:26,720
некоторые уловки о том, как вы должны это делать,

891
00:31:26,720 --> 00:31:29,519
чтобы на самом деле, э  это

892
00:31:29,519 --> 00:31:31,200
не дискретный токен, который вы

893
00:31:31,200 --> 00:31:33,279
предоставляете модели, а вместо этого распределение

894
00:31:33,279 --> 00:31:34,880
по токенам,

895
00:31:34,880 --> 00:31:36,559
но важно то, что вы

896
00:31:36,559 --> 00:31:38,480
знаете, правильно ли вы делаете это,

897
00:31:38,480 --> 00:31:40,399
используя мягкое распределение токенов в качестве

898
00:31:40,399 --> 00:31:42,640
входных данных для модели атрибутов.  он

899
00:31:42,640 --> 00:31:44,399
сможет вычислить оценку

900
00:31:44,399 --> 00:31:46,720
для последовательности, которую он получает,

901
00:31:46,720 --> 00:31:47,760
чтобы вы знали, является ли это классификатором настроений,

902
00:31:47,760 --> 00:31:50,159
он может оценить, насколько

903
00:31:50,159 --> 00:31:52,880
положительна последовательность, которую вы ему предоставили.

904
00:31:52,880 --> 00:31:55,039
m, а затем то, что вы можете сделать, это то, что вы

905
00:31:55,039 --> 00:31:56,960
можете вычислить градиенты относительно

906
00:31:56,960 --> 00:31:59,360
этого свойства uh и обратно распространить

907
00:31:59,360 --> 00:32:00,720
эти градиенты обратно на вашу

908
00:32:00,720 --> 00:32:03,519
модель генерации текста uh напрямую

909
00:32:03,519 --> 00:32:04,399
um

910
00:32:04,399 --> 00:32:06,399
и uh, но вместо обновления

911
00:32:06,399 --> 00:32:07,760
параметров, которые вы бы сделали

912
00:32:07,760 --> 00:32:09,679
во время обучения  вместо этого обновите

913
00:32:09,679 --> 00:32:11,760
промежуточные активации

914
00:32:11,760 --> 00:32:13,440
на каждом уровне вашей модели, которые вы

915
00:32:13,440 --> 00:32:15,760
затем можете распространить, чтобы вычислить

916
00:32:15,760 --> 00:32:18,559
новое распределение по наборам токенов,

917
00:32:18,559 --> 00:32:20,159
и поэтому это аккуратный трюк, который

918
00:32:20,159 --> 00:32:22,000
позволяет вам выполнять обновление распределения в реальном времени

919
00:32:22,000 --> 00:32:23,760
на основе некоторого внешнего

920
00:32:23,760 --> 00:32:24,960
дискриминатора,

921
00:32:24,960 --> 00:32:26,960
который  позволяя вам обновлять ваши

922
00:32:26,960 --> 00:32:28,720
внутренние представления последовательности

923
00:32:28,720 --> 00:32:30,240
так, чтобы она, надеюсь, генерировала

924
00:32:30,240 --> 00:32:32,399
что-то более положительное на выходе в

925
00:32:32,399 --> 00:32:34,880
этом случае,

926
00:32:36,960 --> 00:32:38,640
так что эти методы перебалансировки распределения,

927
00:32:38,640 --> 00:32:40,320
которые вы знаете,

928
00:32:40,320 --> 00:32:42,480
основаны на поиске ближайших соседей или

929
00:32:42,480 --> 00:32:45,760
на использовании какого-либо типа дискриминатора.

930
00:32:45,760 --> 00:32:47,760
довольно многообещающе и интересно,

931
00:32:47,760 --> 00:32:49,039
но в конечном итоге они

932
00:32:49,039 --> 00:32:51,440
требуют больших вычислительных затрат в

933
00:32:51,440 --> 00:32:52,960
В первом случае вы, по сути, проводите

934
00:32:52,960 --> 00:32:54,320
поиск по тому,

935
00:32:54,320 --> 00:32:56,080
что знаете тысячи фраз, чтобы

936
00:32:56,080 --> 00:32:58,000
перебалансировать свой дистрибутив, а во

937
00:32:58,000 --> 00:32:59,519
втором случае вы знаете несколько

938
00:32:59,519 --> 00:33:01,039
прямых и обратных проходов на каждом

939
00:33:01,039 --> 00:33:02,880
этапе, чтобы попытаться заставить токены

940
00:33:02,880 --> 00:33:05,519
демонстрировать определенное поведение более ммм и

941
00:33:05,519 --> 00:33:07,039
к сожалению, ни один из них на самом деле не

942
00:33:07,039 --> 00:33:10,399
мешает вам декодировать плохие последовательности.

943
00:33:10,399 --> 00:33:12,399


944
00:33:12,399 --> 00:33:14,080


945
00:33:14,080 --> 00:33:16,399


946
00:33:16,399 --> 00:33:18,240


947
00:33:18,240 --> 00:33:20,000


948
00:33:20,000 --> 00:33:21,600


949
00:33:21,600 --> 00:33:23,600
называются повторными ранжаторами, и

950
00:33:23,600 --> 00:33:24,880
поэтому мы на самом деле

951
00:33:24,880 --> 00:33:27,360
декодируем несколько последовательностей, возможно, используя

952
00:33:27,360 --> 00:33:30,960
выборку или более широкий жадный поиск,

953
00:33:30,960 --> 00:33:32,720
скажем, может быть, десять,

954
00:33:32,720 --> 00:33:33,919
а затем мы можем

955
00:33:33,919 --> 00:33:36,799
инициализировать оценку для оценки

956
00:33:36,799 --> 00:33:39,279
последовательностей, которые мы  произвести и перераспределить эти

957
00:33:39,279 --> 00:33:42,240
последовательности в соответствии со счетом,

958
00:33:42,240 --> 00:33:44,240
ммм, и самое простое, что мы можем

959
00:33:44,240 --> 00:33:46,399
сделать, это на самом деле просто оценить их, ммм,

960
00:33:46,399 --> 00:33:48,799
по вероятности,

961
00:33:48,799 --> 00:33:50,960
заданной моделью, например, вы знаете,

962
00:33:50,960 --> 00:33:52,240
особенно если мы используем алгоритм выборки

963
00:33:52,240 --> 00:33:54,000
, который мы могли бы захотеть знать,

964
00:33:54,000 --> 00:33:55,360
убедитесь, что мы не сгенерировали что-то,

965
00:33:55,360 --> 00:33:57,519
что, как вы знаете, полностью отклоняется от

966
00:33:57,519 --> 00:33:59,039
хорошего текста, который  имеют тенденцию иметь

967
00:33:59,039 --> 00:34:02,159
очень высокий уровень недоумения.

968
00:34:02,159 --> 00:34:03,679


969
00:34:03,679 --> 00:34:05,919


970
00:34:05,919 --> 00:34:07,679


971
00:34:07,679 --> 00:34:09,679


972
00:34:09,679 --> 00:34:12,399


973
00:34:12,399 --> 00:34:14,560
недоумение, вы, вероятно,

974
00:34:14,560 --> 00:34:15,760
просто сгенерируете то, чего

975
00:34:15,760 --> 00:34:18,639
пытались избежать в первом случае,

976
00:34:18,639 --> 00:34:19,679


977
00:34:19,679 --> 00:34:21,918
но вы знаете, что мы также можем заставить наших

978
00:34:21,918 --> 00:34:23,918
повторно ранжеров оценивать более сложное

979
00:34:23,918 --> 00:34:25,359
поведение так же, как мы

980
00:34:25,359 --> 00:34:27,119
могли бы использовать методы на основе градиента для

981
00:34:27,119 --> 00:34:29,119
обновить наши дистрибутивы, чтобы они отображали более

982
00:34:29,119 --> 00:34:31,040
сложное поведение, которое мы действительно можем

983
00:34:31,040 --> 00:34:32,639
просто знать, взять те же

984
00:34:32,639 --> 00:34:34,960
модели атрибутов и использовать их в качестве средств повторного ранжирования для

985
00:34:34,960 --> 00:34:36,800
повторного ранжирования фиксированного набора последовательностей, а

986
00:34:36,800 --> 00:34:38,639
не для их обратного распространения  использовали

987
00:34:38,639 --> 00:34:39,918
градиенты для основной

988
00:34:39,918 --> 00:34:40,879
модели,

989
00:34:40,879 --> 00:34:42,560
и поэтому мы можем использовать их для ранжирования

990
00:34:42,560 --> 00:34:44,639
таких вещей, как стиль дискурса, фактологичность,

991
00:34:44,639 --> 00:34:46,399
логическая согласованность,

992
00:34:46,399 --> 00:34:48,879
вы знаете, но вы знаете, просто будьте осторожны,

993
00:34:48,879 --> 00:34:50,879
если ваш повторный ранжер окажется плохо

994
00:34:50,879 --> 00:34:53,119
откалиброванным, вы знаете, только потому, что

995
00:34:53,119 --> 00:34:54,879
вы  обучение классификатора предсказанию

996
00:34:54,879 --> 00:34:56,320
того, является ли предложение фактическим

997
00:34:56,320 --> 00:34:58,480
утверждением, на самом деле не означает, что он

998
00:34:58,480 --> 00:34:59,839
будет хорошо ранжировать различные

999
00:34:59,839 --> 00:35:02,000
фактические утверждения по отношению друг к

1000
00:35:02,000 --> 00:35:03,920
другу,

1001
00:35:03,920 --> 00:35:05,119
и, наконец, хорошая вещь о

1002
00:35:05,119 --> 00:35:06,640
повторном ранжировании заключается в том, что вы можете использовать

1003
00:35:06,640 --> 00:35:08,800
несколько повторных оценок.  -rankers и parallel, если

1004
00:35:08,800 --> 00:35:10,720
есть несколько свойств, которые вы хотите оценить

1005
00:35:10,720 --> 00:35:11,680


1006
00:35:11,680 --> 00:35:13,119
и придумать, скажем,

1007
00:35:13,119 --> 00:35:15,280
средневзвешенное значение различных рейтинговых оценок,

1008
00:35:15,280 --> 00:35:17,040
чтобы решить, какая последовательность может быть лучшей в

1009
00:35:17,040 --> 00:35:18,400
соответствии с различными

1010
00:35:18,400 --> 00:35:20,960
свойствами,

1011
00:35:21,359 --> 00:35:22,400


1012
00:35:22,400 --> 00:35:23,920
но чтобы вы знали, что мы

1013
00:35:23,920 --> 00:35:25,520
говорилось с точки зрения декодирования, вы

1014
00:35:25,520 --> 00:35:26,960
знаете, я просто хочу упомянуть, что

1015
00:35:26,960 --> 00:35:29,119
декодирование по-прежнему является очень сложной

1016
00:35:29,119 --> 00:35:31,520
проблемой в генерации естественного языка,

1017
00:35:31,520 --> 00:35:33,680
которую мы на самом деле не знаем.  Однако

1018
00:35:33,680 --> 00:35:34,480


1019
00:35:34,480 --> 00:35:36,000
наши алгоритмы до сих пор не знаете,

1020
00:35:36,000 --> 00:35:37,680
действительно ли они отражают способ, которым люди

1021
00:35:37,680 --> 00:35:39,680
выбирают слова, когда они говорят,

1022
00:35:39,680 --> 00:35:41,200
и наши лучшие подходы, которые, как вы знаете, в

1023
00:35:41,200 --> 00:35:42,880
настоящее время основаны на попытке калибровки

1024
00:35:42,880 --> 00:35:44,560
распределений вероятностей, созданных

1025
00:35:44,560 --> 00:35:47,359
моделями, чтобы, возможно, более точно

1026
00:35:47,359 --> 00:35:49,680
отражать человеческую вероятность,

1027
00:35:49,680 --> 00:35:50,800
но  правда в том, что вы знаете, что

1028
00:35:50,800 --> 00:35:53,760
распределение человеческого языка довольно шумно

1029
00:35:53,760 --> 00:35:55,119
и не отражает простые

1030
00:35:55,119 --> 00:35:56,800
свойства, которые наши алгоритмы декодирования

1031
00:35:56,800 --> 00:35:59,280
часто фиксируют, такие как максимизация вероятности,

1032
00:35:59,280 --> 00:36:02,160


1033
00:36:02,240 --> 00:36:03,040
но

1034
00:36:03,040 --> 00:36:04,640
различные алгоритмы декодирования действительно позволяют

1035
00:36:04,640 --> 00:36:06,720
нам, возможно, вносить предубеждения, которые

1036
00:36:06,720 --> 00:36:08,000
поощряют различные свойства

1037
00:36:08,000 --> 00:36:10,079
когерентной генерации естественного языка

1038
00:36:10,079 --> 00:36:11,920
это позволило нам сделать многообещающие

1039
00:36:11,920 --> 00:36:13,920
улучшения и в этой области,

1040
00:36:13,920 --> 00:36:15,359
и на самом деле некоторые из самых значительных

1041
00:36:15,359 --> 00:36:17,440
достижений в nlg за последние несколько

1042
00:36:17,440 --> 00:36:19,599
лет действительно стали результатом простых, но

1043
00:36:19,599 --> 00:36:21,680
очень эффективных модификаций

1044
00:36:21,680 --> 00:36:24,240
алгоритмов декодирования, потому что вы часто можете

1045
00:36:24,240 --> 00:36:26,160
повлиять на  очень большое количество

1046
00:36:26,160 --> 00:36:27,920
задач, внося хорошие изменения в

1047
00:36:27,920 --> 00:36:29,920
де  алгоритм кодирования,

1048
00:36:29,920 --> 00:36:31,040
но на самом деле в этой области

1049
00:36:31,040 --> 00:36:32,800
еще много работы,

1050
00:36:32,800 --> 00:36:34,800
и, надеюсь, вы знаете, что многие из вас

1051
00:36:34,800 --> 00:36:36,079
сделают эти следующие

1052
00:36:36,079 --> 00:36:38,000
прорывы,

1053
00:36:38,000 --> 00:36:39,520
я рад ответить на вопросы на этом

1054
00:36:39,520 --> 00:36:41,760
этапе, если они возникли  Хорошо, вот

1055
00:36:41,760 --> 00:36:44,400
один вопрос, как вы

1056
00:36:44,400 --> 00:36:46,000
оцениваете, как вы определяете,

1057
00:36:46,000 --> 00:36:48,560
лучше ли распределение ребалансировки, и если

1058
00:36:48,560 --> 00:36:50,960
я сделаю оттуда редакционную статью, я полагаю, вы

1059
00:36:50,960 --> 00:36:53,119
признаете на этом слайде, что вы не можете

1060
00:36:53,119 --> 00:36:56,240
просто смотреть на вероятность,

1061
00:36:56,240 --> 00:36:58,880
да, да, так что да, вы можете

1062
00:36:58,880 --> 00:37:01,359
Я имею в виду, что существует

1063
00:37:01,359 --> 00:37:03,839
определенная степень доверия, которая возникает

1064
00:37:03,839 --> 00:37:05,920
, если вы не верите, что ваш

1065
00:37:05,920 --> 00:37:08,720
ранжер дает вам лучшую оценку того,

1066
00:37:08,720 --> 00:37:10,400
подготовили ли вы качественный

1067
00:37:10,400 --> 00:37:12,400
фрагмент текста, возможно, вам не следует  Не используйте

1068
00:37:12,400 --> 00:37:14,079
это изменение ранжирования здесь в первую очередь, и

1069
00:37:14,079 --> 00:37:15,359
вы знаете, что, надеюсь, вы

1070
00:37:15,359 --> 00:37:17,359
действительно имели в виду протестировать это средство повторного ранжирования, чтобы на

1071
00:37:17,359 --> 00:37:19,440
самом деле показать, что в нем есть э-э, что он

1072
00:37:19,440 --> 00:37:21,440
улучшает качество текста, но мы

1073
00:37:21,440 --> 00:37:22,960
поговорим  намного больше о том, как вы можете

1074
00:37:22,960 --> 00:37:25,280
на самом деле оцените качество текста

1075
00:37:25,280 --> 00:37:27,280
позже, хотя я должен предупредить вас

1076
00:37:27,280 --> 00:37:29,760
заранее, что ответы не

1077
00:37:29,760 --> 00:37:31,520
такие, ну,

1078
00:37:31,520 --> 00:37:34,000
не такие прямые и полные, как вы

1079
00:37:34,000 --> 00:37:35,440
хотели бы, и на самом деле

1080
00:37:35,440 --> 00:37:37,040
есть много места для

1081
00:37:37,040 --> 00:37:39,119
интерпретации того, как  вы бы на самом деле

1082
00:37:39,119 --> 00:37:40,560
сделали это,

1083
00:37:40,560 --> 00:37:42,480
возможно, вам стоит продолжить об этом позже,

1084
00:37:42,480 --> 00:37:44,240
но я думаю, что люди озадачены этим,

1085
00:37:44,240 --> 00:37:46,079
потому что есть еще один вопрос,

1086
00:37:46,079 --> 00:37:49,040
который, да, задается,

1087
00:37:49,040 --> 00:37:52,000
ммм, я говорил, что вы сказали, что не

1088
00:37:52,000 --> 00:37:53,920
знаете, как заставить модель выбирать такие слова,

1089
00:37:53,920 --> 00:37:55,280
как  человек

1090
00:37:55,280 --> 00:37:56,320
ммм,

1091
00:37:56,320 --> 00:37:58,720
как мы моделируем разных людей

1092
00:37:58,720 --> 00:37:59,520
ммм

1093
00:37:59,520 --> 00:38:03,800
с разным происхождением и т. д.,

1094
00:38:04,320 --> 00:38:06,160


1095
00:38:06,160 --> 00:38:07,520
да, это действительно хороший

1096
00:38:07,520 --> 00:38:09,520
вопрос,

1097
00:38:09,520 --> 00:38:11,599
ответ на этот вопрос состоит в том, что вы могли бы

1098
00:38:11,599 --> 00:38:14,160
попытаться узнать, хотите ли вы что-

1099
00:38:14,160 --> 00:38:16,079


1100
00:38:16,079 --> 00:38:18,560
то вроде тонкой настройки

1101
00:38:18,560 --> 00:38:20,480
языкового распределения  конкретного

1102
00:38:20,480 --> 00:38:22,079
человека, начиная с, скажем,

1103
00:38:22,079 --> 00:38:23,599
предварительно обученной языковой модели, поскольку у вас,

1104
00:38:23,599 --> 00:38:25,680
вероятно, никогда не будет достаточно данных, чтобы

1105
00:38:25,680 --> 00:38:28,400
использовать только результаты одного человека, или

1106
00:38:28,400 --> 00:38:29,520
вы могли бы попытаться выполнить некоторые из этих

1107
00:38:29,520 --> 00:38:31,200
перебалансировок  методы, о которых мы говорили,

1108
00:38:31,200 --> 00:38:33,760
чтобы, возможно,

1109
00:38:33,760 --> 00:38:35,680
использовать их для более точного

1110
00:38:35,680 --> 00:38:37,839
приближения ваших дистрибутивов к определенному

1111
00:38:37,839 --> 00:38:39,359
человеческому языковому распределению,

1112
00:38:39,359 --> 00:38:41,440
поэтому в методах на основе градиента,

1113
00:38:41,440 --> 00:38:43,280
которые я описал, я мог бы, возможно, обучить

1114
00:38:43,280 --> 00:38:45,839
единую языковую модель только на моем типе

1115
00:38:45,839 --> 00:38:46,960
языка,

1116
00:38:46,960 --> 00:38:48,480
даже если я  есть гораздо больший, который

1117
00:38:48,480 --> 00:38:50,160
обучен на гораздо большем корпусе

1118
00:38:50,160 --> 00:38:52,160
языков от разных носителей,

1119
00:38:52,160 --> 00:38:54,880
но затем попытайтесь сделать так, чтобы моя модель

1120
00:38:54,880 --> 00:38:58,880
ранжировала выходы основной модели, у

1121
00:38:58,960 --> 00:39:00,720
меня есть вопрос,

1122
00:39:00,720 --> 00:39:01,520
да,

1123
00:39:01,520 --> 00:39:04,560
так что выборка ядра и выборка верхнего регистра

1124
00:39:04,560 --> 00:39:06,000
действительно эффективен на

1125
00:39:06,000 --> 00:39:08,000
практике, и вы утверждали, что

1126
00:39:08,000 --> 00:39:09,280
есть все эти маленькие

1127
00:39:09,280 --> 00:39:10,960
вещи с очень малой вероятностной массой,

1128
00:39:10,960 --> 00:39:13,760
но она суммируется с большей вероятностной массой,

1129
00:39:13,760 --> 00:39:14,640
но

1130
00:39:14,640 --> 00:39:16,480
если она суммируется с большей вероятностной массой,

1131
00:39:16,480 --> 00:39:17,920
чем они на самом деле

1132
00:39:17,920 --> 00:39:20,320
должны иметь при реальном

1133
00:39:20,320 --> 00:39:22,079
распределении  человеческому языку не следовало бы обучать наши модели таким образом

1134
00:39:22,079 --> 00:39:24,079
, чтобы они давали им меньшую

1135
00:39:24,079 --> 00:39:26,240
вероятностную массу, и

1136
00:39:26,240 --> 00:39:28,400
почему бы нам и нам, почему бы нам не сделать наши языковые

1137
00:39:28,400 --> 00:39:30,160
модели лучше в  в этом случае, например, почему у

1138
00:39:30,160 --> 00:39:32,240
нас есть эта проблема, если они на

1139
00:39:32,240 --> 00:39:33,359
самом деле получают больше вероятности,

1140
00:39:33,359 --> 00:39:35,920
чем должны.

1141
00:39:37,680 --> 00:39:39,040


1142
00:39:39,040 --> 00:39:40,880
Да, это действительно хороший вопрос,

1143
00:39:40,880 --> 00:39:43,599
вы знаете, я думаю, что ответ на него

1144
00:39:43,599 --> 00:39:45,119
заключается в том, как мы их обучаем, к

1145
00:39:45,119 --> 00:39:47,680
чему я доберусь.  in a bit на самом деле

1146
00:39:47,680 --> 00:39:48,960
пытается

1147
00:39:48,960 --> 00:39:50,400
смоделировать

1148
00:39:50,400 --> 00:39:51,839


1149
00:39:51,839 --> 00:39:53,520
распространение человеческого языка, как он

1150
00:39:53,520 --> 00:39:55,200
видит его в своем учебном корпусе, и на

1151
00:39:55,200 --> 00:39:58,000
самом деле он удивительно эффективен в

1152
00:39:58,000 --> 00:39:59,839
этом,

1153
00:39:59,839 --> 00:40:01,040


1154
00:40:01,040 --> 00:40:03,040
но в то же время, когда мы действительно

1155
00:40:03,040 --> 00:40:04,480
начинаем использовать эти  языковые модели

1156
00:40:04,480 --> 00:40:06,240
из коробки в nlg-задачах, с которыми мы работаем, во-

1157
00:40:06,240 --> 00:40:07,920
первых,

1158
00:40:07,920 --> 00:40:09,760
всегда есть небольшие отклонения в

1159
00:40:09,760 --> 00:40:11,359
распределении текста, который мы на самом деле

1160
00:40:11,359 --> 00:40:13,119
пытаемся смоделировать для задачи, которую мы делаем,

1161
00:40:13,119 --> 00:40:14,800
и того, что большой корпус  Мы тренировались

1162
00:40:14,800 --> 00:40:16,800
, в первую очередь, что может сделать

1163
00:40:16,800 --> 00:40:18,720
эти алгоритмы декодирования менее

1164
00:40:18,720 --> 00:40:19,760
эффективными,

1165
00:40:19,760 --> 00:40:21,280
и вы знаете, второе, что я хотел бы

1166
00:40:21,280 --> 00:40:22,960
отметить, это то, что даже при том, что эти

1167
00:40:22,960 --> 00:40:24,720
алгоритмы декодирования довольно

1168
00:40:24,720 --> 00:40:27,119
эффективны  на практике

1169
00:40:27,119 --> 00:40:29,520
они не делают того, что делают люди, когда мы

1170
00:40:29,520 --> 00:40:32,000
говорим, мы не находимся в точке, когда человек

1171
00:40:32,000 --> 00:40:33,599
говорит, мы потенциально пытаемся

1172
00:40:33,599 --> 00:40:35,599
максимизировать вероятность потенциального

1173
00:40:35,599 --> 00:40:38,240
следующего токена или случайным образом выбираем слово

1174
00:40:38,240 --> 00:40:39,920
из набора токенов, которые мы в конечном итоге

1175
00:40:39,920 --> 00:40:41,920
имеем  мировые модели, которые определяют то, как мы

1176
00:40:41,920 --> 00:40:44,079
выбираем токены, которые мы выбираем,

1177
00:40:44,079 --> 00:40:46,079
чтобы выразить нашу точку зрения,

1178
00:40:46,079 --> 00:40:47,599
и это сильно отличается от того, что мы

1179
00:40:47,599 --> 00:40:50,720
получаем в вероятностных языковых моделях,

1180
00:40:50,720 --> 00:40:52,079
теперь вы знаете, должны ли мы отбрасывать

1181
00:40:52,079 --> 00:40:53,920
вероятностные языковые модели

1182
00:40:53,920 --> 00:40:55,680
?  в конечном итоге работают

1183
00:40:55,680 --> 00:40:58,079
довольно хорошо, но в какой-то момент нам также

1184
00:40:58,079 --> 00:40:59,680
нужно смягчить эти различия

1185
00:40:59,680 --> 00:41:01,280
между тем, как люди в конечном итоге говорят, и тем,

1186
00:41:01,280 --> 00:41:02,800
как языковые модели в конечном итоге моделируют

1187
00:41:02,800 --> 00:41:05,200
язык,

1188
00:41:05,839 --> 00:41:08,079
спасибо,

1189
00:41:09,280 --> 00:41:10,240
эм

1190
00:41:10,240 --> 00:41:11,920
, теперь вы знаете, что теперь, когда Джон

1191
00:41:11,920 --> 00:41:14,319
отлично подготовил нас к тому, что будет дальше

1192
00:41:14,319 --> 00:41:17,040
эм, вы знаете, давайте вернемся

1193
00:41:17,040 --> 00:41:18,960
к обучению этих моделей.

1194
00:41:18,960 --> 00:41:21,280


1195
00:41:21,280 --> 00:41:23,040


1196
00:41:23,040 --> 00:41:24,480
вы выбираете свой

1197
00:41:24,480 --> 00:41:26,560
алгоритм декодирования в зависимости от

1198
00:41:26,560 --> 00:41:28,240
свойств, которые вас

1199
00:41:28,240 --> 00:41:30,640
интересуют, это здорово, но правда в том,

1200
00:41:30,640 --> 00:41:32,960
что между вашей

1201
00:41:32,960 --> 00:41:34,319
моделью декодирования и вашим

1202
00:41:34,319 --> 00:41:35,359
алгоритмом обучения есть взаимодействия, о которых вы, возможно, захотите

1203
00:41:35,359 --> 00:41:37,680
подумать во время обучения, что на

1204
00:41:37,680 --> 00:41:40,240
самом деле не то, что мы '  повторюсь прямо сейчас,

1205
00:41:40,240 --> 00:41:42,160
и поэтому, если вы помните, что

1206
00:41:42,160 --> 00:41:43,520
алгоритм обучения, который мы предложили

1207
00:41:43,520 --> 00:41:44,800
до этого момента,

1208
00:41:44,800 --> 00:41:46,560
- это тот, в котором мы просто пытаемся минимизировать

1209
00:41:46,560 --> 00:41:48,400
отрицательную логарифмическую

1210
00:41:48,400 --> 00:41:50,480
вероятность следующего токена в последовательности с

1211
00:41:50,480 --> 00:41:53,119
учетом предыдущих э-э на каждом

1212
00:41:53,119 --> 00:41:55,680
шаге  эм, и вы знаете, как я уже упоминал, это на

1213
00:41:55,680 --> 00:41:57,920
самом деле очень хорошо работает для обучения

1214
00:41:57,920 --> 00:42:01,119
авторегрессивных моделей человеческого языка,

1215
00:42:01,119 --> 00:42:03,359
но на самом деле это вызывает несколько

1216
00:42:03,359 --> 00:42:06,560
проблем, на которые Джон, мм, намекнул,

1217
00:42:06,560 --> 00:42:08,000
так что на следующих нескольких слайдах я собираюсь

1218
00:42:08,000 --> 00:42:09,440
Я немного расскажу об этих проблемах, а

1219
00:42:09,440 --> 00:42:10,720
затем выделю некоторые обучающие

1220
00:42:10,720 --> 00:42:12,960
решения этих проблем, которые мне

1221
00:42:12,960 --> 00:42:15,200
показались интересными или которые я считаю

1222
00:42:15,200 --> 00:42:17,520
важными за последние несколько

1223
00:42:17,520 --> 00:42:18,800
лет

1224
00:42:18,800 --> 00:42:20,240
, так что первый вопрос  ue на самом деле тот, на

1225
00:42:20,240 --> 00:42:22,079
который я намекал в последнем разделе, а

1226
00:42:22,079 --> 00:42:24,240
именно, что тренировка с максимальной

1227
00:42:24,240 --> 00:42:27,119
вероятностью, мм, имеет тенденцию препятствовать

1228
00:42:27,119 --> 00:42:29,680
текстовому разнообразию, и я показал эту

1229
00:42:29,680 --> 00:42:32,000
последовательность на слайде ранее в качестве

1230
00:42:32,000 --> 00:42:34,240
примера жадных алгоритмов, склонных

1231
00:42:34,240 --> 00:42:36,640
к генерации повторяющихся  Последовательности,

1232
00:42:36,640 --> 00:42:37,920
которые, как вы знаете, представляют собой

1233
00:42:37,920 --> 00:42:40,000
наихудшую форму разнообразия,

1234
00:42:40,000 --> 00:42:41,680
которую вы могли бы получить,

1235
00:42:41,680 --> 00:42:43,839
но вы знаете, что жадные алгоритмы

1236
00:42:43,839 --> 00:42:45,760
просто пытаются максимизировать вероятность создаваемых

1237
00:42:45,760 --> 00:42:47,839
ими последовательностей, поэтому на

1238
00:42:47,839 --> 00:42:49,839
самом деле они могут быть только склонны к повторению

1239
00:42:49,839 --> 00:42:52,960
и неразборчивые фразы, э-э, если эти

1240
00:42:52,960 --> 00:42:55,280
фразы высоко оцениваются моделью,

1241
00:42:55,280 --> 00:42:56,880
э-э, для начала,

1242
00:42:56,880 --> 00:42:58,400
и это в конечном итоге становится одной из проблем

1243
00:42:58,400 --> 00:43:00,640
с обучением с максимальной вероятностью в том, что

1244
00:43:00,640 --> 00:43:02,480
оно имеет тенденцию в конечном итоге отдавать предпочтение общим

1245
00:43:02,480 --> 00:43:04,160
выражениям, потому что это те

1246
00:43:04,160 --> 00:43:06,640
, которые вы часто знаете  скорее всего,

1247
00:43:06,640 --> 00:43:09,119
на человеческом языке,

1248
00:43:09,119 --> 00:43:10,960
но вы знаете, как мы все знаем, и как я уже

1249
00:43:10,960 --> 00:43:12,480
упоминал ранее,

1250
00:43:12,480 --> 00:43:14,640
производство человеческого языка не связано с максимизацией

1251
00:43:14,640 --> 00:43:16,400
Вероятность слов, которые мы производим,

1252
00:43:16,400 --> 00:43:18,079
поэтому, даже если мы можем производить общие

1253
00:43:18,079 --> 00:43:19,839
фразы чаще, чем необобщенные

1254
00:43:19,839 --> 00:43:21,200
фразы, это не та цель, которую мы

1255
00:43:21,200 --> 00:43:23,520
ставим перед собой, когда мы говорим, есть

1256
00:43:23,520 --> 00:43:25,440
гораздо больше в общении, которое на самом

1257
00:43:25,440 --> 00:43:27,040
деле не синтезируется обучением

1258
00:43:27,040 --> 00:43:29,680
цель, которая пытается максимизировать гм, вы

1259
00:43:29,680 --> 00:43:31,920
знаете, вероятность по сравнению с человеческим языком

1260
00:43:31,920 --> 00:43:33,680
, который читается, ну так как мы можем в конечном

1261
00:43:33,680 --> 00:43:35,839
итоге смягчить эту проблему, мм, чтобы

1262
00:43:35,839 --> 00:43:37,280


1263
00:43:37,280 --> 00:43:38,560
вы знали, что интересный подход, который мне

1264
00:43:38,560 --> 00:43:41,040
действительно нравится, который появился в прошлом году, на

1265
00:43:41,040 --> 00:43:43,280
самом деле был, как вы знаете, предложен  by

1266
00:43:43,280 --> 00:43:44,480
welllick,

1267
00:43:44,480 --> 00:43:47,599
что называлось обучением невероятности,

1268
00:43:47,599 --> 00:43:49,119
и поэтому здесь вы

1269
00:43:49,119 --> 00:43:51,520
фактически препятствуете производству

1270
00:43:51,520 --> 00:43:53,920
определенных токенов моделью в

1271
00:43:53,920 --> 00:43:55,920
определенных контекстах, и что происходит

1272
00:43:55,920 --> 00:43:58,160
, когда этот термин потерь здесь уменьшается по мере

1273
00:43:58,160 --> 00:44:00,960
того, как вероятность токенов y-neg

1274
00:44:00,960 --> 00:44:02,720
уменьшается, поэтому для любого токена, который вы

1275
00:44:02,720 --> 00:44:03,839
не хотите генерировать, поскольку

1276
00:44:03,839 --> 00:44:05,040
вероятность генерации этого токена

1277
00:44:05,040 --> 00:44:07,520
снижается, так же как и срок потерь,

1278
00:44:07,520 --> 00:44:09,200
что означает, что yo  на самом деле вы не

1279
00:44:09,200 --> 00:44:11,920
так сильно обновляете модель

1280
00:44:11,920 --> 00:44:13,920
для этого конкретного поведения,

1281
00:44:13,920 --> 00:44:15,040
но что важно, так это то, что у вас

1282
00:44:15,040 --> 00:44:16,079
все еще есть

1283
00:44:16,079 --> 00:44:18,240
цель форсирования

1284
00:44:18,240 --> 00:44:19,680
вашей

1285
00:44:19,680 --> 00:44:21,200
футболки.  распределение

1286
00:44:21,200 --> 00:44:23,200
языка из учебного корпуса, которое

1287
00:44:23,200 --> 00:44:24,960
ему нужно сделать, чтобы

1288
00:44:24,960 --> 00:44:26,640
научиться генерировать текст,

1289
00:44:26,640 --> 00:44:28,480
но также он научится не

1290
00:44:28,480 --> 00:44:30,960
произносить определенные слова, которые вы, возможно,

1291
00:44:30,960 --> 00:44:33,040
не захотите,

1292
00:44:33,040 --> 00:44:33,920


1293
00:44:33,920 --> 00:44:35,119
а затем то, что вы можете сделать, это то, что вы можете

1294
00:44:35,119 --> 00:44:36,960
установить  этот список слов, которые вы не

1295
00:44:36,960 --> 00:44:39,119
хотите, чтобы модель генерировала, на

1296
00:44:39,119 --> 00:44:40,880
самом деле это слова, которые вы уже

1297
00:44:40,880 --> 00:44:42,960
сгенерировали раньше, и, по сути

1298
00:44:42,960 --> 00:44:44,960
, вы учите модель не

1299
00:44:44,960 --> 00:44:47,119
повторять одно и то же снова, и

1300
00:44:47,119 --> 00:44:48,640
это естественно  собираетесь ограничить

1301
00:44:48,640 --> 00:44:50,400
количество повторений, которые ваша модель

1302
00:44:50,400 --> 00:44:51,839
будет в состоянии выплевывать,

1303
00:44:51,839 --> 00:44:53,280
и в результате вы также сможете генерировать

1304
00:44:53,280 --> 00:44:57,920
более разнообразные тексты,

1305
00:45:00,160 --> 00:45:02,720
но вторая и очень важная проблема,

1306
00:45:02,720 --> 00:45:04,079
которая возникает при обучении с

1307
00:45:04,079 --> 00:45:06,720
максимой  гм, вероятно, вы знаете, что

1308
00:45:06,720 --> 00:45:09,280
мы часто называем предвзятым отношением к разоблачению

1309
00:45:09,280 --> 00:45:11,280
: контекст, который мы тренируем

1310
00:45:11,280 --> 00:45:13,839
для генерации следующего токена,

1311
00:45:13,839 --> 00:45:15,680
будет отличаться от тех, которые

1312
00:45:15,680 --> 00:45:18,160
мы видим во время генерации, и почему это

1313
00:45:18,160 --> 00:45:19,760
может быть

1314
00:45:19,760 --> 00:45:22,160
хм ну так что  Во время

1315
00:45:22,160 --> 00:45:24,560
обучения мы всегда получаем токен из

1316
00:45:24,560 --> 00:45:27,839
золотого документа или человеческого текста, поскольку он помещает

1317
00:45:27,839 --> 00:45:30,240
его в последовательность золота, как мы это называем,

1318
00:45:30,240 --> 00:45:32,640
но затем во время генерации мы возвращаем

1319
00:45:32,640 --> 00:45:34,960
наши ранее сгенерированные токены

1320
00:45:34,960 --> 00:45:37,760
обратно в модель в качестве входных данных um,

1321
00:45:37,760 --> 00:45:39,440
а не  эти жетоны силы учителя

1322
00:45:39,440 --> 00:45:41,680
, которые взяты из

1323
00:45:41,680 --> 00:45:43,200
золотых документов,

1324
00:45:43,200 --> 00:45:44,960
и поэтому на набор токенов на самом деле

1325
00:45:44,960 --> 00:45:46,560
сильно влияют такие вещи, как

1326
00:45:46,560 --> 00:45:48,640
распределения, производимые нашей моделью,

1327
00:45:48,640 --> 00:45:50,960
и алгоритм декодирования, который мы используем для

1328
00:45:50,960 --> 00:45:53,119
получения токенов, и так что вы знаете  может ли это

1329
00:45:53,119 --> 00:45:55,440
стать проблемой, ну да,

1330
00:45:55,440 --> 00:45:57,760
потому что, как мы уже видели ранее,

1331
00:45:57,760 --> 00:46:00,560
типы текста, которые генерирует наша модель,

1332
00:46:00,560 --> 00:46:02,640
часто вы знаете, не очень близкое

1333
00:46:02,640 --> 00:46:04,800
приближение к шаблонам человеческого языка

1334
00:46:04,800 --> 00:46:06,160
в траектории  ning,

1335
00:46:06,160 --> 00:46:07,680
и поэтому будет дисбаланс

1336
00:46:07,680 --> 00:46:09,200
между типом текста, который наша

1337
00:46:09,200 --> 00:46:11,760
модель научилась предсказывать, научиться

1338
00:46:11,760 --> 00:46:14,240
предсказывать и ожидать увидеть, и

1339
00:46:14,240 --> 00:46:16,960
типом текста, который она увидит, как только мы

1340
00:46:16,960 --> 00:46:19,200
начнем декодирование, и так однажды  ваша

1341
00:46:19,200 --> 00:46:21,760
модель начинает получать свои собственные входные данные,

1342
00:46:21,760 --> 00:46:22,960
которые будут отклоняться от

1343
00:46:22,960 --> 00:46:24,960
распределения текста, и ожидается, что

1344
00:46:24,960 --> 00:46:26,319
для нее будет очень

1345
00:46:26,319 --> 00:46:27,920
сложно создавать связные тексты в

1346
00:46:27,920 --> 00:46:29,760
будущем, потому что она не будет действительно

1347
00:46:29,760 --> 00:46:31,040
знать, как синтезировать свою собственную

1348
00:46:31,040 --> 00:46:33,680
информацию, которая  это сгенерировано

1349
00:46:33,680 --> 00:46:35,440
ммм, и поэтому есть множество способов

1350
00:46:35,440 --> 00:46:37,440
попытаться противостоять этой проблеме смещения экспозиции,

1351
00:46:37,440 --> 00:46:39,839
и многие другие, которые

1352
00:46:39,839 --> 00:46:42,160
продолжают появляться, ммм, вы знаете, к сожалению,

1353
00:46:42,160 --> 00:46:43,280
на самом деле не хватает времени, чтобы поговорить

1354
00:46:43,280 --> 00:46:44,960
обо всех из них, поэтому я добавил слайды

1355
00:46:44,960 --> 00:46:46,880
чтобы обсудить два из них, которые основаны

1356
00:46:46,880 --> 00:46:49,280
на полууправляемом обучении здесь,

1357
00:46:49,280 --> 00:46:50,160


1358
00:46:50,160 --> 00:46:52,480
но я действительно хочу сосредоточиться на двух

1359
00:46:52,480 --> 00:46:53,280
других

1360
00:46:53,280 --> 00:46:55,280
подходах, которые лично мне кажутся очень

1361
00:46:55,280 --> 00:46:58,160
интересными. Первый называется

1362
00:46:58,160 --> 00:46:59,680
se

1363
00:46:59,680 --> 00:47:01,520
Вы знаете, что в этой настройке ваша

1364
00:47:01,520 --> 00:47:03,760
модель сначала научилась извлекать

1365
00:47:03,760 --> 00:47:04,960
последовательность

1366
00:47:04,960 --> 00:47:08,000
из существующей базы данных

1367
00:47:08,000 --> 00:47:08,839


1368
00:47:08,839 --> 00:47:11,040
прототипов, написанных человеком, так что это похоже на

1369
00:47:11,040 --> 00:47:12,640
декодеры наших ближайших соседей ранее, где

1370
00:47:12,640 --> 00:47:14,640
мы кэшировали кучу фраз, здесь вы кешируете

1371
00:47:14,640 --> 00:47:16,640
кучу последовательностей, которые  может быть

1372
00:47:16,640 --> 00:47:18,000
похож на тот, который вы должны

1373
00:47:18,000 --> 00:47:21,200
создать для этой новой ситуации,

1374
00:47:21,200 --> 00:47:22,800
и тогда мы делаем то, что, как только мы берем

1375
00:47:22,800 --> 00:47:24,559
эту последовательность и извлекаем ее, мы

1376
00:47:24,559 --> 00:47:26,559
учимся редактировать ее, э-э, делая такие вещи, как

1377
00:47:26,559 --> 00:47:29,920
добавление или изменение токенов,

1378
00:47:29,920 --> 00:47:31,520
э-э,  более точно отражать

1379
00:47:31,520 --> 00:47:33,520
контекст, который нам на самом деле дан, а

1380
00:47:33,520 --> 00:47:36,000
не тот, для которого изначально была разработана эта исходная последовательность,

1381
00:47:36,000 --> 00:47:37,599


1382
00:47:37,599 --> 00:47:38,800


1383
00:47:38,800 --> 00:47:40,319
и поэтому мы все еще можем использовать здесь алгоритм,

1384
00:47:40,319 --> 00:47:41,920
который пытается максимизировать вероятность

1385
00:47:41,920 --> 00:47:43,760
для обучения, но потому что есть этот

1386
00:47:43,760 --> 00:47:45,440
вид  скрытой переменной

1387
00:47:45,440 --> 00:47:47,599
получения правильного прототипа, который задействован,

1388
00:47:47,599 --> 00:47:49,280
это снижает вероятность того, что

1389
00:47:49,280 --> 00:47:50,800
наш сгенерированный текст в конечном итоге будет страдать

1390
00:47:50,800 --> 00:47:52,640
от предвзятости экспозиции.  Потому что вы

1391
00:47:52,640 --> 00:47:54,160
уже начинаете с чего-то, что

1392
00:47:54,160 --> 00:47:55,680
больше похоже на обучающую последовательность, которую

1393
00:47:55,680 --> 00:47:58,240
вы, возможно, видели.

1394
00:47:58,240 --> 00:48:01,280
Другой общий класс

1395
00:48:01,280 --> 00:48:02,960
возможностей, которые мы можем сделать, - это позволить нашей

1396
00:48:02,960 --> 00:48:04,480
модели научиться

1397
00:48:04,480 --> 00:48:06,400
генерировать текст, изучая ее собственные

1398
00:48:06,400 --> 00:48:07,440
образцы,

1399
00:48:07,440 --> 00:48:09,359
и вы знаете  это естественно

1400
00:48:09,359 --> 00:48:11,119
хорошо соотносится с обучением с подкреплением,

1401
00:48:11,119 --> 00:48:12,640
которое на самом деле является одним из моих

1402
00:48:12,640 --> 00:48:15,359
любимых способов научиться

1403
00:48:15,359 --> 00:48:18,000
генерировать текст в этой настройке.

1404
00:48:18,000 --> 00:48:19,599


1405
00:48:19,599 --> 00:48:22,319


1406
00:48:22,319 --> 00:48:24,400
- это

1407
00:48:24,400 --> 00:48:26,559
представление модели предыдущего контекста, в

1408
00:48:26,559 --> 00:48:27,599
котором вы видите

1409
00:48:27,599 --> 00:48:29,280
свои действия; - это слова, которые можно

1410
00:48:29,280 --> 00:48:30,640
сгенерировать;

1411
00:48:30,640 --> 00:48:32,800
ваша политика - это декодер,

1412
00:48:32,800 --> 00:48:34,559
и ваши награды обеспечиваются

1413
00:48:34,559 --> 00:48:36,800
некоторым типом внешней оценки

1414
00:48:36,800 --> 00:48:38,559
; здесь вы можете изучить множество

1415
00:48:38,559 --> 00:48:39,839
различных вариантов поведения для своих

1416
00:48:39,839 --> 00:48:41,119
модель генерации текста

1417
00:48:41,119 --> 00:48:43,359
, вознаграждая ее, когда она демонстрирует такое

1418
00:48:43,359 --> 00:48:45,760
поведение,

1419
00:48:45,760 --> 00:48:47,440
и, таким образом, чтобы быстро присоединиться к этому

1420
00:48:47,440 --> 00:48:49,119
обрамлению с точки зрения

1421
00:48:49,119 --> 00:48:50,640
модели генерации текста  s, что мы видели

1422
00:48:50,640 --> 00:48:52,960
до сих пор, вы собираетесь предпринимать

1423
00:48:52,960 --> 00:48:56,000
действия, выбирая слова

1424
00:48:56,000 --> 00:48:58,800
um hat y из дистрибутива,

1425
00:48:58,800 --> 00:48:59,920
а затем вы собираетесь скармливать их обратно

1426
00:48:59,920 --> 00:49:02,319
во вход, чтобы получить новое состояние,

1427
00:49:02,319 --> 00:49:04,079
которое вы  знаете, что мы делали в

1428
00:49:04,079 --> 00:49:05,200
каждой точке,

1429
00:49:05,200 --> 00:49:06,880
но что отличает его, так это то, что когда

1430
00:49:06,880 --> 00:49:08,800
вы генерируете текст, вы используете некоторую

1431
00:49:08,800 --> 00:49:11,040
внешнюю функцию вознаграждения, чтобы вычислить

1432
00:49:11,040 --> 00:49:12,880
вознаграждение для каждого токена, который вы генерируете,

1433
00:49:12,880 --> 00:49:14,480
поэтому вы награждаете каждое действие, которое

1434
00:49:14,480 --> 00:49:15,920
вы предпринимаете,

1435
00:49:15,920 --> 00:49:18,400
а затем  вы масштабируете потерю

1436
00:49:18,400 --> 00:49:20,480
выборки на этом конкретном токене, который

1437
00:49:20,480 --> 00:49:23,200
вы генерируете этим вознаграждением,

1438
00:49:23,200 --> 00:49:24,400
которое будет побуждать модель

1439
00:49:24,400 --> 00:49:26,640
генерировать последовательность в аналогичном контексте,

1440
00:49:26,640 --> 00:49:29,280
если награда высока,

1441
00:49:29,280 --> 00:49:31,440
чтобы очень четко выразить это, вы

1442
00:49:31,440 --> 00:49:33,280
минимизируете  отрицательная логарифмическая

1443
00:49:33,280 --> 00:49:35,200
вероятность вашего образца токена, так что здесь это не

1444
00:49:35,200 --> 00:49:37,359
золотой токен, обратите внимание на шляпу, которая находится на

1445
00:49:37,359 --> 00:49:40,000
выражении y в функции вознаграждения

1446
00:49:40,000 --> 00:49:41,520
um,

1447
00:49:41,520 --> 00:49:43,839
а затем вы собираетесь вычислить

1448
00:49:43,839 --> 00:49:45,680
вознаграждение для этого токена и масштабировать эту

1449
00:49:45,680 --> 00:49:47,680
отрицательную логарифмическую вероятность с помощью этого  Рева

1450
00:49:47,680 --> 00:49:49,440
Поэтому, если награда высока, модель

1451
00:49:49,440 --> 00:49:51,119
с большей вероятностью будет генерировать ту

1452
00:49:51,119 --> 00:49:52,960
же последовательность в аналогичном контексте

1453
00:49:52,960 --> 00:49:55,440
в будущем, если награда будет низкой, это

1454
00:49:55,440 --> 00:49:58,559
будет менее вероятно,

1455
00:49:58,559 --> 00:49:59,839
но этот вид естественного вопроса вызывает естественный

1456
00:49:59,839 --> 00:50:01,440
вопрос: вы знаете, что может  мы фактически

1457
00:50:01,440 --> 00:50:03,119
используем в качестве награды, чтобы поощрять

1458
00:50:03,119 --> 00:50:04,640
поведение, которое мы хотим в этой системе генерации текста,

1459
00:50:04,640 --> 00:50:06,319


1460
00:50:06,319 --> 00:50:07,760
что действительно зависит от вас, когда вы разрабатываете

1461
00:50:07,760 --> 00:50:09,920
этот конвейер генерации,

1462
00:50:09,920 --> 00:50:11,359
обычная

1463
00:50:11,359 --> 00:50:13,520
практика в первые дни использования rl

1464
00:50:13,520 --> 00:50:15,839
для генерации текста э-э, заключалась в том, чтобы установить

1465
00:50:15,839 --> 00:50:17,920
награду в размере  окончательная метрика оценки,

1466
00:50:17,920 --> 00:50:20,240
которую вы собирались оценивать,

1467
00:50:20,240 --> 00:50:22,800
и поэтому здесь вместо того, чтобы иметь уникальное

1468
00:50:22,800 --> 00:50:24,640
вознаграждение для каждого сгенерированного токена на каждом

1469
00:50:24,640 --> 00:50:26,400
временном шаге, вы просто возьмете окончательную

1470
00:50:26,400 --> 00:50:28,800
оценку последовательности, которую вы получаете, и вознаграждаете

1471
00:50:28,800 --> 00:50:30,960
каждый токен в сгенерированной

1472
00:50:30,960 --> 00:50:32,559
последовательности этим значением

1473
00:50:32,559 --> 00:50:33,520
и

1474
00:50:33,520 --> 00:50:36,079
это было абсолютно волшебно,

1475
00:50:36,079 --> 00:50:38,000
вы бы установили свою метрику оценки в

1476
00:50:38,000 --> 00:50:39,760
качестве награды, и в конечном итоге вы бы научились

1477
00:50:39,760 --> 00:50:41,680
получать больше вознаграждения, потому что это то, что

1478
00:50:41,680 --> 00:50:44,400
делают алгоритмы rl, что, в свою очередь, означает, что

1479
00:50:44,400 --> 00:50:46,000
вы  учились генерировать последовательности,

1480
00:50:46,000 --> 00:50:48,640
которые лучше подходят для вашей метрики оценки,

1481
00:50:48,640 --> 00:50:50,400
поэтому результаты тестов lg взлетали до небес,

1482
00:50:50,400 --> 00:50:51,520


1483
00:50:51,520 --> 00:50:54,480
мы добивались реального прогресса, но на

1484
00:50:54,480 --> 00:50:56,960
самом деле это была ложь, вы знаете,

1485
00:50:56,960 --> 00:50:59,359
оказывается, поскольку я расскажу о более поздних метриках оценки,

1486
00:50:59,359 --> 00:51:01,520
особенно для  генерация текста

1487
00:51:01,520 --> 00:51:03,520
- это всего лишь приближения,

1488
00:51:03,520 --> 00:51:04,800
и не всегда ясно, что

1489
00:51:04,800 --> 00:51:07,119
оптимизация этих

1490
00:51:07,119 --> 00:51:09,920
приближений приведет к большему и лучшему,

1491
00:51:09,920 --> 00:51:12,240
когерентному генерации текста, а вместо этого

1492
00:51:12,240 --> 00:51:13,760
часто в конечном итоге происходит

1493
00:51:13,760 --> 00:51:16,000
то, что он просто учится использовать шум

1494
00:51:16,000 --> 00:51:18,000
в метрике оценки

1495
00:51:18,000 --> 00:51:20,800
и в  Фактически, в своей большой

1496
00:51:20,800 --> 00:51:22,240
работе, где они представили

1497
00:51:22,240 --> 00:51:23,839
систему нейронного машинного перевода Google в

1498
00:51:23,839 --> 00:51:26,480
2016 году, вы знаете, исследователи Google в

1499
00:51:26,480 --> 00:51:28,960
целом обнаружили, что обучение

1500
00:51:28,960 --> 00:51:30,960
моделей машинного перевода с оценками rl и синими в

1501
00:51:30,960 --> 00:51:32,480
качестве награды

1502
00:51:32,480 --> 00:51:33,839
, на самом деле, вообще не улучшило

1503
00:51:33,839 --> 00:51:36,559
качество перевода, даже если оно

1504
00:51:36,559 --> 00:51:40,160
действительно привело к более высоким показателям синих баллов,

1505
00:51:41,119 --> 00:51:43,520
но разработка вашей функции вознаграждения

1506
00:51:43,520 --> 00:51:46,559
- это очень важная проблема.

1507
00:51:46,559 --> 00:51:48,880
nrl для фактического изучения поведения,

1508
00:51:48,880 --> 00:51:51,680
которое вы хотите, и я перечислил

1509
00:51:51,680 --> 00:51:53,520
некоторые интересные работы здесь о том, как вы можете на

1510
00:51:53,520 --> 00:51:55,680
самом деле научиться связывать довольно сложные

1511
00:51:55,680 --> 00:51:58,079
поведения с функциями вознаграждения,

1512
00:51:58,079 --> 00:52:00,640
фактически инициализируя оценки, которые они

1513
00:52:00,640 --> 00:52:02,960
используют в качестве вознаграждения в качестве нейронных сетей

1514
00:52:02,960 --> 00:52:04,400
вы знаете, что заранее обучены вспомогательной

1515
00:52:04,400 --> 00:52:06,400
задаче, но затем могут быть использованы

1516
00:52:06,400 --> 00:52:08,640
для предоставления оценок в качестве вознаграждения

1517
00:52:08,640 --> 00:52:10,800
системе, которую вы создаете

1518
00:52:10,800 --> 00:52:12,480


1519
00:52:12,480 --> 00:52:13,680


1520
00:52:13,680 --> 00:52:15,839


1521
00:52:15,839 --> 00:52:17,599
.  говорит о положительных вещах, вы можете

1522
00:52:17,599 --> 00:52:19,440
использовать классификатор настроений,

1523
00:52:19,440 --> 00:52:21,680
чтобы получить вознаграждение за

1524
00:52:21,680 --> 00:52:24,319
генерируемые вами последовательности,

1525
00:52:24,559 --> 00:52:27,520
и, к сожалению, это очень весело,

1526
00:52:28,079 --> 00:52:29,359
несмотря на все удовольствие, которое

1527
00:52:29,359 --> 00:52:30,880
вы можете получить с помощью rl для обучения

1528
00:52:30,880 --> 00:52:32,400
движков генерации текста, есть немного

1529
00:52:32,400 --> 00:52:34,960
темной стороны  а это то, что

1530
00:52:34,960 --> 00:52:37,280
алгоритмы обучения с подкреплением могут

1531
00:52:37,280 --> 00:52:40,000
быть заведомо нестабильными,

1532
00:52:40,000 --> 00:52:41,760
и поэтому, чтобы заставить эти системы генерации текста

1533
00:52:41,760 --> 00:52:43,520
учиться с помощью rl, вам часто

1534
00:52:43,520 --> 00:52:45,280
приходится тщательно

1535
00:52:45,280 --> 00:52:47,119
настраивать  разные циферблаты в вашей модели

1536
00:52:47,119 --> 00:52:48,079


1537
00:52:48,079 --> 00:52:49,440


1538
00:52:49,440 --> 00:52:50,880
точно настроены, многие из них, два из них, которые, я

1539
00:52:50,880 --> 00:52:52,240
думаю, стоит упомянуть, или один, который

1540
00:52:52,240 --> 00:52:53,520
вам всегда нужно предварительно обучать с помощью

1541
00:52:53,520 --> 00:52:55,440
учителя,

1542
00:52:55,440 --> 00:52:56,720
вы обычно не можете тренироваться с

1543
00:52:56,720 --> 00:52:58,880
обучением с подкреплением с нуля,

1544
00:52:58,880 --> 00:53:00,400
а также вам необходимо предоставить  какой-то

1545
00:53:00,400 --> 00:53:02,240
тип базового вознаграждения

1546
00:53:02,240 --> 00:53:04,160
, которого должна достигать ваша модель

1547
00:53:04,160 --> 00:53:05,680
, например, синий балл, который я

1548
00:53:05,680 --> 00:53:07,599
описал ранее, всегда является положительным

1549
00:53:07,599 --> 00:53:08,559
значением,

1550
00:53:08,559 --> 00:53:10,480
если он не равен нулю, но это означает, что если

1551
00:53:10,480 --> 00:53:12,160
вы используете его отдельно в качестве награды, каждая

1552
00:53:12,160 --> 00:53:14,800
отдельная последовательность, которую вы пробуете, заканчивается,

1553
00:53:14,800 --> 00:53:16,319
вы знаете  вас поощряют в

1554
00:53:16,319 --> 00:53:18,000
будущем, так что вы хотите иметь какой-то

1555
00:53:18,000 --> 00:53:21,119
базовый уровень, который представляет собой ожидание

1556
00:53:21,119 --> 00:53:23,200
того, сколько вознаграждения вы должны получить,

1557
00:53:23,200 --> 00:53:25,440
которое можно вычесть из вознаграждения,

1558
00:53:25,440 --> 00:53:26,559
которое вы на самом деле получаете, чтобы вы также могли

1559
00:53:26,559 --> 00:53:29,760
препятствовать определенному поведению.

1560
00:53:29,760 --> 00:53:32,319
Последнее замечание по этому поводу заключается в том, что

1561
00:53:32,319 --> 00:53:34,000
нейронные сети довольно хорошо умеют

1562
00:53:34,000 --> 00:53:35,520
находить самый простой способ чему-то научиться,

1563
00:53:35,520 --> 00:53:36,720


1564
00:53:36,720 --> 00:53:38,400
чтобы вы знали, есть ли способ использовать

1565
00:53:38,400 --> 00:53:40,079
свою функцию вознаграждения.

1566
00:53:40,079 --> 00:53:41,839
э-э, он найдет способ сделать это,

1567
00:53:41,839 --> 00:53:43,200
особенно это проще, чем

1568
00:53:43,200 --> 00:53:45,359
изучать поведение, которое вы хотите, чтобы он

1569
00:53:45,359 --> 00:53:46,640
изучил, так

1570
00:53:46,640 --> 00:53:48,319
что кое-что, что нужно запомнить, это отчасти

1571
00:53:48,319 --> 00:53:49,280
важно, если вы попытаетесь использовать

1572
00:53:49,280 --> 00:53:50,559
обучение с подкреплением для систем генерации текста,

1573
00:53:50,559 --> 00:53:52,240


1574
00:53:52,240 --> 00:53:53,440
особенно потому, что есть такой

1575
00:53:53,440 --> 00:53:56,880
большое пространство действия с акцентом для

1576
00:53:56,880 --> 00:53:58,400
слов, которые он может генерировать, чтобы попытаться

1577
00:53:58,400 --> 00:54:01,520
выполнить определенное поведение,

1578
00:54:02,160 --> 00:54:04,240
эм, чтобы вы знали, чтобы закончить этот раздел, я

1579
00:54:04,240 --> 00:54:05,599
просто хочу начать с того, что в

1580
00:54:05,599 --> 00:54:07,680
целом мы все еще используем принуждение

1581
00:54:07,680 --> 00:54:09,599
учителя в качестве основного средства обучения  для

1582
00:54:09,599 --> 00:54:10,559
создания

1583
00:54:10,559 --> 00:54:12,000
связного текста у

1584
00:54:12,000 --> 00:54:14,160
него есть проблемы с разнообразием, но он все же

1585
00:54:14,160 --> 00:54:15,040
позволяет

1586
00:54:15,040 --> 00:54:16,960
нам изучать модель с приличными возможностями генерации текста,

1587
00:54:16,960 --> 00:54:18,720


1588
00:54:18,720 --> 00:54:19,920


1589
00:54:19,920 --> 00:54:21,520
одна вещь, на которой я не особо сосредоточился

1590
00:54:21,520 --> 00:54:23,760
в этой лекции, - это тип модели,

1591
00:54:23,760 --> 00:54:25,839
которую вы можете использовать для фактического создания

1592
00:54:25,839 --> 00:54:27,760
текста  потому что они имеют тенденцию быть менее

1593
00:54:27,760 --> 00:54:30,079
универсальными и в большей степени предназначены для

1594
00:54:30,079 --> 00:54:32,319
очень конкретных конечных задач,

1595
00:54:32,319 --> 00:54:34,160
но в целом общий подход в nlg

1596
00:54:34,160 --> 00:54:35,359
- попытаться создать нейронную

1597
00:54:35,359 --> 00:54:38,720
архитектуру, которая позволяет

1598
00:54:38,720 --> 00:54:41,040
ваша модель должна быть, возможно, менее

1599
00:54:41,040 --> 00:54:42,559
чувствительна к проблемам

1600
00:54:42,559 --> 00:54:43,920
принуждения учителя или решать их с

1601
00:54:43,920 --> 00:54:45,520
дополнительными условиями потерь, которые, возможно, связаны с

1602
00:54:45,520 --> 00:54:47,520
конкретной задачей

1603
00:54:47,520 --> 00:54:49,359
смещение экспозиции, хотя это проблема

1604
00:54:49,359 --> 00:54:51,359
везде, в значительной степени независимо

1605
00:54:51,359 --> 00:54:54,160
от вашей нейронной архитектуры эм и

1606
00:54:54,160 --> 00:54:55,839
вас  знать, чтобы смягчить его, вы

1607
00:54:55,839 --> 00:54:57,359
можете либо обучить свою модель, чтобы она была более

1608
00:54:57,359 --> 00:54:59,440
устойчивой к изменениям ее собственного распределения с

1609
00:54:59,440 --> 00:55:00,480
помощью таких вещей, как

1610
00:55:00,480 --> 00:55:03,119
полу-контролируемое обучение, либо вы

1611
00:55:03,119 --> 00:55:05,599
можете изменить свой конвейер,

1612
00:55:05,599 --> 00:55:06,559
чтобы вы учились вносить

1613
00:55:06,559 --> 00:55:08,400
изменения  к существующей последовательности,

1614
00:55:08,400 --> 00:55:09,920
которую вы извлекаете из своего обучающего набора,

1615
00:55:09,920 --> 00:55:11,359
вместо того, чтобы пытаться научиться

1616
00:55:11,359 --> 00:55:14,559
создавать последовательности с нуля, ммм,

1617
00:55:14,559 --> 00:55:16,079
предостережение в том, что вы знаете,

1618
00:55:16,079 --> 00:55:17,440
что тип текста, который вы генерируете,

1619
00:55:17,440 --> 00:55:19,440
становится все длиннее и длиннее.

1620
00:55:19,440 --> 00:55:21,520
вид поиска и редактирования во многих случаях

1621
00:55:21,520 --> 00:55:23,119
становится столь же сложным, как

1622
00:55:23,119 --> 00:55:26,240
создание с нуля,

1623
00:55:26,240 --> 00:55:28,160
и, наконец, вы можете использовать

1624
00:55:28,160 --> 00:55:30,160
обучение с подкреплением как еще одно средство

1625
00:55:30,160 --> 00:55:32,079
Изучение на ваших собственных примерах,

1626
00:55:32,079 --> 00:55:33,839
и вы знаете, и, по сути, вы также можете

1627
00:55:33,839 --> 00:55:35,599
использовать его для поощрения различных моделей поведения, а

1628
00:55:35,599 --> 00:55:38,160
не просто максимизации вероятности.

1629
00:55:38,160 --> 00:55:39,760


1630
00:55:39,760 --> 00:55:41,920


1631
00:55:41,920 --> 00:55:43,839
часто можно

1632
00:55:43,839 --> 00:55:45,599
научиться использовать это,

1633
00:55:45,599 --> 00:55:49,040
есть ли там

1634
00:55:49,040 --> 00:55:51,920
онлайн-симуляторы языка, где вы

1635
00:55:51,920 --> 00:55:54,000
можете тренироваться

1636
00:55:54,000 --> 00:55:56,319
с РЛ онлайн, или вы говорите только

1637
00:55:56,319 --> 00:55:59,960
об обучении в автономном режиме

1638
00:56:02,000 --> 00:56:04,240
эм нет, в этой настройке мы обычно

1639
00:56:04,240 --> 00:56:06,160
говорим об обучении в автономном режиме, так что

1640
00:56:06,160 --> 00:56:07,680
вы вы тренируете все это заранее  раз, а

1641
00:56:07,680 --> 00:56:10,240
затем вы используете свою модель, так как она была

1642
00:56:10,240 --> 00:56:12,400
обучена в первый раз,

1643
00:56:12,400 --> 00:56:14,960
хорошо, да, теперь мы

1644
00:56:14,960 --> 00:56:16,480
наконец достигли раздела, на который я

1645
00:56:16,480 --> 00:56:18,559
намекал ранее, и по очень

1646
00:56:18,559 --> 00:56:22,400
важной теме, оценка,

1647
00:56:22,400 --> 00:56:23,920
которую вы знаете  честно говоря, это то

1648
00:56:23,920 --> 00:56:25,760
, о чем мы должны подумать,

1649
00:56:25,760 --> 00:56:27,839
прежде чем мы даже начнем проектировать модель

1650
00:56:27,839 --> 00:56:29,599
или или алгоритм обучения, или или

1651
00:56:29,599 --> 00:56:31,280
алгоритм декодирования, вы знаете, как

1652
00:56:31,280 --> 00:56:33,440
мы можем на самом деле проверить, что наш встретился

1653
00:56:33,440 --> 00:56:34,880
Как мы можем действовать, как мы на самом деле

1654
00:56:34,880 --> 00:56:36,000
собираемся оценить, что наш метод

1655
00:56:36,000 --> 00:56:37,280
вообще работает,

1656
00:56:37,280 --> 00:56:39,280
и сегодня я хочу немного поговорить о

1657
00:56:39,280 --> 00:56:41,680
трех типах оценочных метрик, так что

1658
00:56:41,680 --> 00:56:43,760
сначала мы поговорим об автоматических оценочных

1659
00:56:43,760 --> 00:56:44,799
метриках,

1660
00:56:44,799 --> 00:56:46,480
потому что обычно вам нужны  чтобы иметь возможность

1661
00:56:46,480 --> 00:56:48,319
быстро создавать прототип и диагностировать отказы

1662
00:56:48,319 --> 00:56:49,920
в вашей модели, поэтому очень важно иметь

1663
00:56:49,920 --> 00:56:51,440
возможность получать эту быструю обратную связь,

1664
00:56:51,440 --> 00:56:54,079
даже если она очень грубая

1665
00:56:54,079 --> 00:56:56,480
и в автоматических оценочных показателях.

1666
00:56:56,480 --> 00:56:58,720
Вы знаете, что мы традиционно

1667
00:56:58,720 --> 00:57:00,480
использовали то, что я  вызов метрик перекрытия контента,

1668
00:57:00,480 --> 00:57:02,960
которые фокусируются на том, насколько

1669
00:57:02,960 --> 00:57:04,960
последовательность явно похожа на другую

1670
00:57:04,960 --> 00:57:07,119
последовательность, обычно с точки зрения соответствия слов или

1671
00:57:07,119 --> 00:57:09,040
фраз,

1672
00:57:09,040 --> 00:57:10,880
а в последнее время также появились новые

1673
00:57:10,880 --> 00:57:12,559
автоматические оценки,

1674
00:57:12,559 --> 00:57:14,720
основанные на моделях, где мы пытаемся использовать достижения

1675
00:57:14,720 --> 00:57:16,720
во встраиваниях и нейронных моделях  чтобы

1676
00:57:16,720 --> 00:57:19,359
определить более неявные меры сходства

1677
00:57:19,359 --> 00:57:20,880
между последовательностями,

1678
00:57:20,880 --> 00:57:22,319
и, наконец, мы поговорим немного о человеческих

1679
00:57:22,319 --> 00:57:23,920
оценках, которые являются своего рода золотым

1680
00:57:23,920 --> 00:57:25,520
стандартом

1681
00:57:25,520 --> 00:57:28,480
оценки текста  поколение

1682
00:57:28,480 --> 00:57:30,480
э-э, но у них также, э-э, есть недостатки

1683
00:57:30,480 --> 00:57:33,359
, к которым мы тоже доберемся, и

1684
00:57:33,359 --> 00:57:34,480
я просто хочу отметить, что некоторые из этих

1685
00:57:34,480 --> 00:57:36,400
слайдов здесь на самом деле перепрофилированы из

1686
00:57:36,400 --> 00:57:38,480
слайдов из спящего,

1687
00:57:38,480 --> 00:57:42,880
который является ведущим экспертом по оценке nlg,

1688
00:57:42,880 --> 00:57:45,680
но так что давайте  перескочите, так что

1689
00:57:45,680 --> 00:57:47,760
метрики перекрытия контента обычно вычисляют

1690
00:57:47,760 --> 00:57:49,599
явную оценку сходства между двумя

1691
00:57:49,599 --> 00:57:51,760
последовательностями, которые были

1692
00:57:51,760 --> 00:57:53,839
сгенерированы вашей моделью, и некоторой

1693
00:57:53,839 --> 00:57:56,400
эталонной последовательностью золотого стандарта,

1694
00:57:56,400 --> 00:57:58,640
которая была прикреплена к входам,

1695
00:57:58,640 --> 00:58:00,319
которые у вас были, другими словами,

1696
00:58:00,319 --> 00:58:01,520
последовательность, которая  вы знаете, было

1697
00:58:01,520 --> 00:58:04,079
подходящее поколение для входных данных,

1698
00:58:04,079 --> 00:58:04,880
у

1699
00:58:04,880 --> 00:58:07,280
вас были,

1700
00:58:07,280 --> 00:58:08,880
и эти метрики часто являются

1701
00:58:08,880 --> 00:58:10,400
популярной отправной точкой, потому что они

1702
00:58:10,400 --> 00:58:12,880
быстрые и очень эффективные, что является

1703
00:58:12,880 --> 00:58:14,400
их основным преимуществом, если у вас есть

1704
00:58:14,400 --> 00:58:16,400
эталонная последовательность для сравнения  ну,

1705
00:58:16,400 --> 00:58:17,680
вы знаете, вычислите эти оценки

1706
00:58:17,680 --> 00:58:19,440
быстро, чтобы получить обратную связь,

1707
00:58:19,440 --> 00:58:21,040
и я собираюсь разделить их на

1708
00:58:21,040 --> 00:58:23,119
две зоны, здесь сначала инграмма перекрывает

1709
00:58:23,119 --> 00:58:24,880
метрики, которые вычисляют разные функции

1710
00:58:24,880 --> 00:58:27,280
количество слов и слов, таких как

1711
00:58:27,280 --> 00:58:28,960
метрики перекрытия и семантического перекрытия, которые

1712
00:58:28,960 --> 00:58:31,280
включают в себя более сложные функции перекрытия,

1713
00:58:31,280 --> 00:58:34,160
основанные на семантических структурах,

1714
00:58:34,160 --> 00:58:35,599
к сожалению, помимо того, что они быстрые и

1715
00:58:35,599 --> 00:58:36,480
эффективные,

1716
00:58:36,480 --> 00:58:39,760
большинство показателей перекрытия инграмм на

1717
00:58:39,760 --> 00:58:41,040
самом деле не дают вам

1718
00:58:41,040 --> 00:58:44,000
хорошего приближения к

1719
00:58:44,000 --> 00:58:46,640
качеству последовательности  раз, ммм, они

1720
00:58:46,640 --> 00:58:48,480
уже, как вы знаете, не идеальны для чего-то

1721
00:58:48,480 --> 00:58:50,240
вроде машинного перевода, где может

1722
00:58:50,240 --> 00:58:52,160
быть несколько способов перевода одной и той же

1723
00:58:52,160 --> 00:58:55,359
последовательности с такими вещами, как синонимы,

1724
00:58:55,359 --> 00:58:56,960
и они постепенно становятся намного

1725
00:58:56,960 --> 00:58:59,680
хуже для задач, которые являются более открытыми,

1726
00:58:59,680 --> 00:59:01,119
чем mt,

1727
00:59:01,119 --> 00:59:03,200
поэтому  например, при резюмировании

1728
00:59:03,200 --> 00:59:05,440
вы знаете, что более длинные выходные тексты,

1729
00:59:05,440 --> 00:59:07,040
естественно, затрудняют измерение чего-либо

1730
00:59:07,040 --> 00:59:09,119
с помощью соответствия слов

1731
00:59:09,119 --> 00:59:11,119
um, и это что-то вроде диалога, он

1732
00:59:11,119 --> 00:59:13,040
невероятно открытый, и на самом деле у вас

1733
00:59:13,040 --> 00:59:13,920
может быть

1734
00:59:13,920 --> 00:59:15,920
несколько ответов

1735
00:59:15,920 --> 00:59:18,319
на конкретное высказывание, которое, как вы знаете,

1736
00:59:18,319 --> 00:59:20,400
означает то же самое  вещь, но не используйте

1737
00:59:20,400 --> 00:59:21,839
общеупотребительные слова

1738
00:59:21,839 --> 00:59:23,280
, поэтому мы можем проиллюстрировать это на

1739
00:59:23,280 --> 00:59:25,920
простом, довольно надуманном примере.

1740
00:59:25,920 --> 00:59:27,680
Если вы знаете, что у вас есть контекстное высказывание диалога,

1741
00:59:27,680 --> 00:59:29,440
которое задает вопрос, такой как

1742
00:59:29,440 --> 00:59:30,799
вы знаете, собираетесь ли вы посетить

1743
00:59:30,799 --> 00:59:33,599
невероятный cs224 Антуана и читать

1744
00:59:33,599 --> 00:59:35,920
лекцию полностью беспристрастный справочный текст

1745
00:59:35,920 --> 00:59:38,240
, полученный от человека,

1746
00:59:38,240 --> 00:59:40,079
а затем агент диалога, который выплевывает

1747
00:59:40,079 --> 00:59:42,400
разные ответы, такие как  как да, который

1748
00:59:42,400 --> 00:59:44,799
получает довольно высокий балл по нашим метрикам перекрытия инграмм,

1749
00:59:44,799 --> 00:59:46,400


1750
00:59:46,400 --> 00:59:48,240
или вы знаете его, который уже оценивается

1751
00:59:48,240 --> 00:59:50,559
намного ниже, несмотря на то, что вы изображаете

1752
00:59:50,559 --> 00:59:52,880
ту же самую идею, или да, который на самом деле

1753
00:59:52,880 --> 00:59:54,480
получает нулевой балл,

1754
00:59:54,480 --> 00:59:56,079
в то

1755
00:59:56,079 --> 00:59:57,760
время как совершенно неверный ответ,

1756
00:59:57,760 --> 01:00:00,480
черт возьми, нет  наивысший балл

1757
01:00:00,480 --> 01:00:02,079
из всех, который указывает

1758
01:00:02,079 --> 01:00:03,520
на проблему, которую вы знаете, когда вы используете

1759
01:00:03,520 --> 01:00:06,480
меры перекрытия инграммы во многих

1760
01:00:06,480 --> 01:00:08,160
приложениях, вам будет не

1761
01:00:08,160 --> 01:00:09,520
хватать основных элементов того, что

1762
01:00:09,520 --> 01:00:11,440
сгенерированная последовательность должна фиксировать, и

1763
01:00:11,440 --> 01:00:14,640
вместо этого вы знаете  получить стилистическое э-э ...

1764
01:00:14,640 --> 01:00:16,559
сходство между текстами, даже если вы

1765
01:00:16,559 --> 01:00:18,880
упускаете самый важный контекст,

1766
01:00:18,880 --> 01:00:21,040
и если вы предпочитаете эмпирические

1767
01:00:21,040 --> 01:00:23,200
подтверждения надуманным примерам, э-э '  На самом деле

1768
01:00:23,200 --> 01:00:25,280
было показано, что многие показатели оценки диалогов

1769
01:00:25,280 --> 01:00:26,880
вообще не коррелируют с

1770
01:00:26,880 --> 01:00:29,280
человеческими суждениями,

1771
01:00:29,280 --> 01:00:32,880
и это ухудшается по мере увеличения длины вашей последовательности,

1772
01:00:32,880 --> 01:00:34,640
поэтому с

1773
01:00:34,640 --> 01:00:37,280
открытой задачей, такой как создание истории,

1774
01:00:37,280 --> 01:00:38,319
вы можете

1775
01:00:38,319 --> 01:00:40,160
получить улучшенные оценки, просто сопоставив

1776
01:00:40,160 --> 01:00:41,440
множество стоп-слов, которые

1777
01:00:41,440 --> 01:00:42,880
не имеют ничего общего с содержанием

1778
01:00:42,880 --> 01:00:45,599
самой истории,

1779
01:00:46,720 --> 01:00:48,480
у нас также есть другая категория метрик перекрытия,

1780
01:00:48,480 --> 01:00:50,400
которую я назову метриками семантического перекрытия,

1781
01:00:50,400 --> 01:00:52,160
потому что они не обязательно

1782
01:00:52,160 --> 01:00:54,720
напрямую связаны со словами, которые вы использовали, а

1783
01:00:54,720 --> 01:00:56,319
вместо этого пытаетесь  вместо этого создайте концептуальные

1784
01:00:56,319 --> 01:00:58,400
представления сгенерированных и

1785
01:00:58,400 --> 01:01:00,640
эталонных выходных данных.

1786
01:01:00,640 --> 01:01:02,480


1787
01:01:02,480 --> 01:01:06,240


1788
01:01:06,240 --> 01:01:08,559


1789
01:01:08,559 --> 01:01:09,359


1790
01:01:09,359 --> 01:01:11,920


1791
01:01:11,920 --> 01:01:14,000


1792
01:01:14,000 --> 01:01:17,280
представление в конечном итоге оказывается

1793
01:01:17,280 --> 01:01:18,319
гм,

1794
01:01:18,319 --> 01:01:19,680
но очевидно, что есть некоторые

1795
01:01:19,680 --> 01:01:22,000
ограничения на то, насколько хорошо

1796
01:01:22,000 --> 01:01:24,000
могут работать явные метрики перекрытия контента, особенно когда

1797
01:01:24,000 --> 01:01:26,319
мы начинаем думать о более открытых

1798
01:01:26,319 --> 01:01:28,400
задачах, поэтому в последние

1799
01:01:28,400 --> 01:01:30,160
годы в ответ было сосредоточено внимание на использовании

1800
01:01:30,160 --> 01:01:31,920
метрик на основе моделей, представления которых

1801
01:01:31,920 --> 01:01:33,359


1802
01:01:33,359 --> 01:01:35,520
исходят из моделей машинного

1803
01:01:35,520 --> 01:01:37,440
обучения, и где их можно

1804
01:01:37,440 --> 01:01:39,680
использовать для реальной оценки

1805
01:01:39,680 --> 01:01:42,319
точности сгенерированных  текст,

1806
01:01:42,319 --> 01:01:43,760
я имею в виду, что они хороши, потому что больше нет

1807
01:01:43,760 --> 01:01:46,319
необходимости в явных совпадениях между

1808
01:01:46,319 --> 01:01:47,920
словами в вашей ссылке и сгенерированным

1809
01:01:47,920 --> 01:01:50,400
текстом, вместо этого вы можете полагаться на гораздо более

1810
01:01:50,400 --> 01:01:52,480
неявные понятия сходства,

1811
01:01:52,480 --> 01:01:55,760
которые вы получаете из встраивания слов

1812
01:01:55,920 --> 01:01:58,079
um, и поэтому вы знаете, что было

1813
01:01:58,079 --> 01:02:00,720
много моделей  эээ, разработать в этой области

1814
01:02:00,720 --> 01:02:02,079
некоторые из оригинальных,

1815
01:02:02,079 --> 01:02:04,240
сфокусированных на том, что вы знаете, определение

1816
01:02:04,240 --> 01:02:07,599
функций композиции по

1817
01:02:07,599 --> 01:02:09,440
встраиванию слов в вашу сгенерированную

1818
01:02:09,440 --> 01:02:10,960
и ссылочную последовательности, а затем

1819
01:02:10,960 --> 01:02:12,559
вычисление

1820
01:02:12,559 --> 01:02:14,720
расстояния между композициями

1821
01:02:14,720 --> 01:02:17,520
двух последовательностей

1822
01:02:17,599 --> 01:02:21,200
еще немного, вы знаете, что  некоторые более

1823
01:02:21,200 --> 01:02:23,039
сложные подходы к этой идее - это такие вещи,

1824
01:02:23,039 --> 01:02:25,359
как расстояние между переносчиками слов, где вы на

1825
01:02:25,359 --> 01:02:27,599
самом деле пытаетесь сопоставить векторы слов в боте  h

1826
01:02:27,599 --> 01:02:30,079
ваши сгенерированные и ссылочные последовательности

1827
01:02:30,079 --> 01:02:32,640
на пары друг с другом, чтобы

1828
01:02:32,640 --> 01:02:34,640
каждый вектор слов, который вы знаете, связан с

1829
01:02:34,640 --> 01:02:35,920
другим рабочим вектором в противоположной

1830
01:02:35,920 --> 01:02:37,599
последовательности, и расстояние между

1831
01:02:37,599 --> 01:02:40,799
ними вычисляется, а затем вы позволяете

1832
01:02:40,799 --> 01:02:42,240
метрике оценки фактически

1833
01:02:42,240 --> 01:02:44,240
вычислять оптимальную  сопоставления между

1834
01:02:44,240 --> 01:02:46,079
этими парами слов, такие как общее

1835
01:02:46,079 --> 01:02:48,240
расстояние, сведены к минимуму

1836
01:02:48,240 --> 01:02:50,400
и оценка птицы, которая

1837
01:02:50,400 --> 01:02:52,880
стала довольно популярной за последний

1838
01:02:52,880 --> 01:02:55,200
год или около того, поскольку метрика оценки - это в

1839
01:02:55,200 --> 01:02:57,039
значительной степени просто расстояние перемещений слов,

1840
01:02:57,039 --> 01:02:58,799
но с использованием контекстуализированных вложений птиц и

1841
01:02:58,799 --> 01:03:02,880
птиц для их вычисления  расстояние

1842
01:03:02,880 --> 01:03:05,359
um сходство движителей предложений - это своего рода

1843
01:03:05,359 --> 01:03:07,839
еще одно расширение движителей слов uh

1844
01:03:07,839 --> 01:03:10,400
подобие, которое добавляет вложения предложений

1845
01:03:10,400 --> 01:03:11,920
из рекуррентных нейронных сетей к этой

1846
01:03:11,920 --> 01:03:14,000
оптимизации расстояния и

1847
01:03:14,000 --> 01:03:15,920
позволяет более эффективно оценивать

1848
01:03:15,920 --> 01:03:18,799
, скажем, длинный текст, состоящий из нескольких предложений,

1849
01:03:18,799 --> 01:03:20,559
а не только один-единственный

1850
01:03:20,559 --> 01:03:22,559
предложения

1851
01:03:22,559 --> 01:03:25,760
и, наконец, в прошлом году появилась

1852
01:03:25,760 --> 01:03:28,240
новая модель под названием Blurt, которая на самом деле

1853
01:03:28,240 --> 01:03:30,640
ya регрессионная модель, основанная на птицах,

1854
01:03:30,640 --> 01:03:33,359
и здесь она берет пару предложений,

1855
01:03:33,359 --> 01:03:34,799
ссылка на сгенерированную, и

1856
01:03:34,799 --> 01:03:36,640
возвращает оценку, которая указывает,

1857
01:03:36,640 --> 01:03:38,960
насколько кандидат грамматичен, и

1858
01:03:38,960 --> 01:03:41,839
передает значение справочного

1859
01:03:41,839 --> 01:03:44,839
текста,

1860
01:03:45,280 --> 01:03:47,839
так что вы знаете, что мы можем  может говорить

1861
01:03:47,839 --> 01:03:49,839
о гораздо большем количестве оценочных метрик, которые

1862
01:03:49,839 --> 01:03:51,440
вычисляются автоматически, и вы знаете,

1863
01:03:51,440 --> 01:03:53,280
что их гораздо больше, чем я на

1864
01:03:53,280 --> 01:03:54,559
самом деле упомянул, хотя они, как правило

1865
01:03:54,559 --> 01:03:56,240
, вписываются в те две категории, которые я

1866
01:03:56,240 --> 01:03:57,520
описал,

1867
01:03:57,520 --> 01:03:59,039
но важно помнить, что

1868
01:03:59,039 --> 01:04:00,319
в конце  день истинным признаком

1869
01:04:00,319 --> 01:04:02,559
производительности системы LG является то

1870
01:04:02,559 --> 01:04:04,240
, ценна ли она для человека-пользователя, который должен

1871
01:04:04,240 --> 01:04:06,160
с ней взаимодействовать, или читать текст

1872
01:04:06,160 --> 01:04:07,760
, созданный из нее,

1873
01:04:07,760 --> 01:04:09,520
и, к сожалению, автоматические метрики, как правило

1874
01:04:09,520 --> 01:04:12,160
, не могут воспроизводить человеческое

1875
01:04:12,160 --> 01:04:14,640
мнение о качестве созданного

1876
01:04:14,640 --> 01:04:15,599
текст,

1877
01:04:15,599 --> 01:04:17,599
и именно поэтому вы знаете, что по этой

1878
01:04:17,599 --> 01:04:20,640
причине человеческие оценки

1879
01:04:20,640 --> 01:04:22,319
рассматриваются как наиболее важная форма

1880
01:04:22,319 --> 01:04:24,720
оценки для системы генерации

1881
01:04:24,720 --> 01:04:25,839
текста  ems

1882
01:04:25,839 --> 01:04:27,839
почти вся работа в nlg

1883
01:04:27,839 --> 01:04:29,599
обычно включает некоторую форму человеческой

1884
01:04:29,599 --> 01:04:31,760
оценки, особенно если задача

1885
01:04:31,760 --> 01:04:33,359
более открытая,

1886
01:04:33,359 --> 01:04:36,000
и если она не очень хорошо известна, позвольте

1887
01:04:36,000 --> 01:04:37,760
мне быть откровенным, вам, вероятно, следует

1888
01:04:37,760 --> 01:04:39,680
скептически относиться к любым утверждениям, которые

1889
01:04:39,680 --> 01:04:40,880
делаются этой работой

1890
01:04:40,880 --> 01:04:42,559
хм,

1891
01:04:42,559 --> 01:04:44,960
и, наконец, вы знаете, что еще одно использование

1892
01:04:44,960 --> 01:04:47,119
человеческих оценок заключается в том, что в дополнение

1893
01:04:47,119 --> 01:04:49,599
к оценке производительности модели

1894
01:04:49,599 --> 01:04:51,760
вы также можете использовать их для фактического обучения

1895
01:04:51,760 --> 01:04:54,000
новых моделей машинного обучения, которые

1896
01:04:54,000 --> 01:04:56,079
предназначены для оценки, которые предназначены

1897
01:04:56,079 --> 01:04:58,160
для использования в качестве оценочных функций.

1898
01:04:58,160 --> 01:05:00,880
сами

1899
01:05:01,119 --> 01:05:03,280
ээээээ, я думаю, вы знаете, что

1900
01:05:03,280 --> 01:05:05,280
главное упомянуть о человеческих оценках,

1901
01:05:05,280 --> 01:05:06,799
э-э, поскольку вы знаете, что мы могли бы поговорить о

1902
01:05:06,799 --> 01:05:08,960
них некоторое время, э-э, они оба

1903
01:05:08,960 --> 01:05:11,359
очень простые, но также очень трудные, э-э

1904
01:05:11,359 --> 01:05:12,640


1905
01:05:12,640 --> 01:05:14,000
, вы знаете, что они '  Это просто, потому что вам

1906
01:05:14,000 --> 01:05:15,680
обычно просто нужно найти судей и

1907
01:05:15,680 --> 01:05:17,680
попросить их оценить внутреннее

1908
01:05:17,680 --> 01:05:20,319
измерение качества вашего сгенерированного текста,

1909
01:05:20,319 --> 01:05:22,240
поэтому вам нужно определить набор критериев,

1910
01:05:22,240 --> 01:05:23,920
которые, по вашему мнению, важны.  или

1911
01:05:23,920 --> 01:05:26,000
задача, для которой вы разрабатываете систему,

1912
01:05:26,000 --> 01:05:27,920
и это могут быть такие вещи, как беглость речи,

1913
01:05:27,920 --> 01:05:29,440
когда вы просто знаете, что измеряете такие вещи,

1914
01:05:29,440 --> 01:05:31,920
как грамматика, выбор орфографических слов, действительно ли

1915
01:05:31,920 --> 01:05:33,920
это похоже на человеческий язык,

1916
01:05:33,920 --> 01:05:36,319
эээ, на самом деле текст точно

1917
01:05:36,319 --> 01:05:37,920
отражает факты, которые, как вы знаете,

1918
01:05:37,920 --> 01:05:40,240
описаны  в контексте, вы знаете,

1919
01:05:40,240 --> 01:05:41,920
здравый смысл соответствует ли он

1920
01:05:41,920 --> 01:05:44,319
логическим правилам мира, которых мы могли

1921
01:05:44,319 --> 01:05:46,319
бы ожидать,

1922
01:05:46,319 --> 01:05:47,760
но как только вы определили

1923
01:05:47,760 --> 01:05:49,200
эти критерии, вы можете попросить людей

1924
01:05:49,200 --> 01:05:51,440
оценить сгенерированный текст на предмет того, насколько

1925
01:05:51,440 --> 01:05:54,559
хорошо он на самом деле  создает текст,

1926
01:05:54,559 --> 01:05:55,760


1927
01:05:55,760 --> 01:05:57,440
который удовлетворяет этим конкретным

1928
01:05:57,440 --> 01:05:59,039
критериям.

1929
01:05:59,039 --> 01:06:00,559
Следует отметить, что

1930
01:06:00,559 --> 01:06:02,400
вы знаете, хотя эти размеры являются

1931
01:06:02,400 --> 01:06:04,079
общими и повторяются в разных

1932
01:06:04,079 --> 01:06:06,640
оценках.

1933
01:06:06,640 --> 01:06:08,480


1934
01:06:08,480 --> 01:06:11,119
объяснять оценщикам в

1935
01:06:11,119 --> 01:06:12,400
разных

1936
01:06:12,400 --> 01:06:14,319
терминах и оценивать по-разному, и

1937
01:06:14,319 --> 01:06:15,599
на самом деле одна из проблем с человеческими

1938
01:06:15,599 --> 01:06:17,520
оценками заключается в том, что в разных работах они

1939
01:06:17,520 --> 01:06:20,079
склонны  быть очень нестандартным, что

1940
01:06:20,079 --> 01:06:21,440
может сделать воспроизведение

1941
01:06:21,440 --> 01:06:23,839
результатов, полученных человеком, довольно сложным, и

1942
01:06:23,839 --> 01:06:25,760
поэтому, когда вы читаете статьи по генерации текста,

1943
01:06:25,760 --> 01:06:27,440
вы редко видите сравнение

1944
01:06:27,440 --> 01:06:29,359
оценок людей между двумя разными

1945
01:06:29,359 --> 01:06:30,319
исследованиями,

1946
01:06:30,319 --> 01:06:31,680
даже если они оценивали одни и те же

1947
01:06:31,680 --> 01:06:34,160
параметры,

1948
01:06:34,160 --> 01:06:35,359


1949
01:06:35,359 --> 01:06:36,799
но вы знаете

1950
01:06:36,799 --> 01:06:38,160
другой набор  проблем с человеческими

1951
01:06:38,160 --> 01:06:40,319
оценками, помимо того факта, что

1952
01:06:40,319 --> 01:06:41,760
они медленные, дорогие и

1953
01:06:41,760 --> 01:06:43,920
нестандартные, заключается в том, что вы знаете, что

1954
01:06:43,920 --> 01:06:47,520
сами люди на самом деле не идеальны,

1955
01:06:47,520 --> 01:06:48,640


1956
01:06:48,640 --> 01:06:50,160
вы знаете, я думаю, что есть несколько отрицательных моментов,

1957
01:06:50,160 --> 01:06:52,160
которые я могу сказать о людях, вы знаете,

1958
01:06:52,160 --> 01:06:53,039
хотя

1959
01:06:53,039 --> 01:06:54,720
мы все люди, что мы, как правило,

1960
01:06:54,720 --> 01:06:56,640
не очень последовательны люди

1961
01:06:56,640 --> 01:06:58,319
э-э, вы знаете, мы часто меняем свое мнение

1962
01:06:58,319 --> 01:07:00,400
о том, как мы рассматриваем что-то э-э, в зависимости

1963
01:07:00,400 --> 01:07:02,559
от чего-то, как вы знаете тривиально,

1964
01:07:02,559 --> 01:07:03,839
как время суток

1965
01:07:03,839 --> 01:07:06,480
эм, вы знаете, мы не всегда рассуждаем  в

1966
01:07:06,480 --> 01:07:08,640
том смысле, что мы ожидаем, когда

1967
01:07:08,640 --> 01:07:10,319
перед нами стоит такая задача, как

1968
01:07:10,319 --> 01:07:11,839
оценка чего-то, что мы можем потерять.

1969
01:07:11,839 --> 01:07:14,240


1970
01:07:14,240 --> 01:07:16,240
что мы делаем,

1971
01:07:16,240 --> 01:07:18,319
и вы знаете, что мы часто можем неверно истолковать

1972
01:07:18,319 --> 01:07:20,400
эм, что, скажем, человеческая оценка

1973
01:07:20,400 --> 01:07:22,559
просит нас сделать, так что мы привносим наши

1974
01:07:22,559 --> 01:07:24,559
собственные предубеждения в задачу

1975
01:07:24,559 --> 01:07:26,400
эм, и вы знаете, помимо всего того,

1976
01:07:26,400 --> 01:07:27,599
что вы знаете, когда мы проводим человеческие

1977
01:07:27,599 --> 01:07:29,119
оценки  мы также имеем дело с тем

1978
01:07:29,119 --> 01:07:30,640
фактом, что одним из главных мотиваторов

1979
01:07:30,640 --> 01:07:32,880
наших судей-людей является выполнение

1980
01:07:32,880 --> 01:07:34,880
задачи как можно быстрее, а

1981
01:07:34,880 --> 01:07:36,400
это не лучший вариант, особенно если мы

1982
01:07:36,400 --> 01:07:38,240
хотим, чтобы они действительно давали нам высокое

1983
01:07:38,240 --> 01:07:41,520
качество.  э-э рейтинги

1984
01:07:41,520 --> 01:07:43,119
э-э, но вы знаете, что люди - это

1985
01:07:43,119 --> 01:07:44,880
лучшее из того, что мы должны на

1986
01:07:44,880 --> 01:07:46,480
самом деле дать нам наиболее точные

1987
01:07:46,480 --> 01:07:48,160
оценки того, хорошо ли работают системы генерации текста.

1988
01:07:48,160 --> 01:07:50,720


1989
01:07:50,720 --> 01:07:53,280


1990
01:07:53,280 --> 01:07:55,359
Я на самом деле собираюсь пропустить этот слайд,

1991
01:07:55,359 --> 01:07:56,640
но я уже упоминал ранее, что вы знаете, что

1992
01:07:56,640 --> 01:07:58,079
одна из вещей, которые мы также можем сделать, - это

1993
01:07:58,079 --> 01:08:01,119
использовать человеческие рейтинги для обучения моделей,

1994
01:08:01,119 --> 01:08:03,920
а, на самом деле, предсказывать оценки для

1995
01:08:03,920 --> 01:08:07,440
самого текста, и так что две системы

1996
01:08:07,440 --> 01:08:09,119
, которые делают  некоторые вещи в этом

1997
01:08:09,119 --> 01:08:11,280
ли  Кроме того, я привел сюда цитаты,

1998
01:08:11,280 --> 01:08:12,400
чтобы вы могли взглянуть на них, если

1999
01:08:12,400 --> 01:08:15,359
вам интересно позже,

2000
01:08:15,599 --> 01:08:16,799


2001
01:08:16,799 --> 01:08:18,799
но так что вы знаете, какие выводы я как

2002
01:08:18,799 --> 01:08:20,799
бы хочу, чтобы вы вынесли из этого раздела,

2003
01:08:20,799 --> 01:08:22,560
ну, вы знаете, что оценка - это  довольно

2004
01:08:22,560 --> 01:08:25,198
сложно, особенно при генерации текста,

2005
01:08:25,198 --> 01:08:26,479


2006
01:08:26,479 --> 01:08:28,799
поэтому метрики перекрытия контента действительно являются

2007
01:08:28,799 --> 01:08:30,640
хорошей отправной точкой для

2008
01:08:30,640 --> 01:08:33,198
оценки качества сгенерированного текста, вы

2009
01:08:33,198 --> 01:08:35,120
знаете, если вы запускаете эти метрики перекрытия дзен-граммы,

2010
01:08:35,120 --> 01:08:36,960
и они показывают оценки

2011
01:08:36,960 --> 01:08:38,560
хуже, чем они должны быть.

2012
01:08:38,560 --> 01:08:40,238
первый признак того, что у вас есть

2013
01:08:40,238 --> 01:08:42,158
проблема, но они, как правило,

2014
01:08:42,158 --> 01:08:44,719
недостаточно хороши, по их собственным

2015
01:08:44,719 --> 01:08:47,040
меркам, основанным на моделях, как правило,

2016
01:08:47,040 --> 01:08:48,399
больше коррелируют с человеческими

2017
01:08:48,399 --> 01:08:50,158
суждениями, чем контент перекрывается один раз,

2018
01:08:50,158 --> 01:08:52,640
особенно когда задач становится больше

2019
01:08:52,640 --> 01:08:54,799
открытый э-э, например, вы знаете диалоги

2020
01:08:54,799 --> 01:08:56,719
и рассказывание историй, но недостатком

2021
01:08:56,719 --> 01:08:58,238
является то, что они не очень

2022
01:08:58,238 --> 01:09:00,319
интерпретируемы, ну вы знаете, в отличие от

2023
01:09:00,319 --> 01:09:02,000
метрики перекрытия контента, где вы можете

2024
01:09:02,000 --> 01:09:04,080
точно сказать, что это  почему эта

2025
01:09:04,080 --> 01:09:05,520
оценка такая? Потому что эти

2026
01:09:05,520 --> 01:09:07,198
слова совпадают с этими словами с

2027
01:09:07,198 --> 01:09:09,520
метрикой, основанной на модели, вы получаете гораздо

2028
01:09:09,520 --> 01:09:10,880
более

2029
01:09:10,880 --> 01:09:12,640
неявное определение сходства, которое, как вы

2030
01:09:12,640 --> 01:09:14,640
знаете, хотя и полезно, также менее, ммм,

2031
01:09:14,640 --> 01:09:16,719
интерпретируемое,

2032
01:09:16,719 --> 01:09:18,319
человеческие суждения абсолютно

2033
01:09:18,319 --> 01:09:20,158
важны,

2034
01:09:20,158 --> 01:09:21,439
потому что

2035
01:09:21,439 --> 01:09:23,198
даже если  они непоследовательны

2036
01:09:23,198 --> 01:09:24,640
и иногда не выполняют те задачи, которые

2037
01:09:24,640 --> 01:09:26,719
вы хотите, чтобы люди действительно

2038
01:09:26,719 --> 01:09:28,479
могли внутренне оценивать

2039
01:09:28,479 --> 01:09:30,080
измерения, которые мы,

2040
01:09:30,080 --> 01:09:31,759
которые мы даже не знаем, как сформулировать,

2041
01:09:31,759 --> 01:09:34,880
используя любой тип автоматической метрики,

2042
01:09:34,880 --> 01:09:37,040
но, наконец, немного не связанные с тем, что

2043
01:09:37,040 --> 01:09:38,319
Я уже говорил об этом разделе, вы знаете,

2044
01:09:38,319 --> 01:09:39,520
я просто хочу сказать, что

2045
01:09:39,520 --> 01:09:41,920
оценщиком номер один любой системы nlg,

2046
01:09:41,920 --> 01:09:43,920
которую вы создаете, на самом деле должен

2047
01:09:43,920 --> 01:09:46,238
быть вы, знаете ли, посмотрите на результаты своей

2048
01:09:46,238 --> 01:09:48,719
модели, как, возможно, вы делаете

2049
01:09:48,719 --> 01:09:51,040
проект  который включает в себя nlg,

2050
01:09:51,040 --> 01:09:52,880
может действительно стоить дней недель, а

2051
01:09:52,880 --> 01:09:54,158
иногда и месяцев изучения

2052
01:09:54,158 --> 01:09:55,679
показателей оценки, которые,

2053
01:09:55,679 --> 01:09:57,360
возможно, немного неинформативны,

2054
01:09:57,360 --> 01:09:59,280
поэтому, если вы разрабатываете систему lg, сделайте s  Мы

2055
01:09:59,280 --> 01:10:01,520
должны очень последовательно оценивать свои собственные поколения,

2056
01:10:01,520 --> 01:10:04,159


2057
01:10:05,920 --> 01:10:07,760


2058
01:10:07,760 --> 01:10:10,080
поэтому в этом последнем разделе

2059
01:10:10,080 --> 01:10:11,920
я думаю, что очень важно поговорить

2060
01:10:11,920 --> 01:10:12,719
об

2061
01:10:12,719 --> 01:10:14,400
этических темах в

2062
01:10:14,400 --> 01:10:16,320
генерации естественного языка,

2063
01:10:16,320 --> 01:10:19,520
потому что в конечном итоге вы знаете, в то время как NLG

2064
01:10:19,520 --> 01:10:21,120
действительно позволяет нам заниматься новыми и

2065
01:10:21,120 --> 01:10:23,679
интересными приложениями, вы знаете, если

2066
01:10:23,679 --> 01:10:24,960
мы не способны, мы можем в конечном итоге

2067
01:10:24,960 --> 01:10:27,280
развернуть довольно опасные и

2068
01:10:27,280 --> 01:10:29,440
вредоносные системы, и в качестве предупреждения я просто

2069
01:10:29,440 --> 01:10:30,880
хочу сказать, что вы знаете, что часть контента

2070
01:10:30,880 --> 01:10:33,040
на следующих нескольких слайдах может быть,

2071
01:10:33,040 --> 01:10:34,560
вы знаете, потенциально может быть довольно

2072
01:10:34,560 --> 01:10:36,480
неудобной, но  Я думаю, что

2073
01:10:36,480 --> 01:10:38,239
важно очень четко прояснить, как эти

2074
01:10:38,239 --> 01:10:41,040
системы могут пойти очень и очень неправильно,

2075
01:10:41,040 --> 01:10:42,719
и, не выбирая конкретный

2076
01:10:42,719 --> 01:10:45,760
пример, я, возможно, думаю, что вы

2077
01:10:45,760 --> 01:10:47,920
знаете, а вы знаете, что одним из самых

2078
01:10:47,920 --> 01:10:49,760
известных примеров этого был тайный

2079
01:10:49,760 --> 01:10:51,440
диалог  чат-боты, которые были выпущены

2080
01:10:51,440 --> 01:10:54,880
в твиттере в 2016 году. Хм, и в

2081
01:10:54,880 --> 01:10:57,199
течение 24 часов он начал

2082
01:10:57,199 --> 01:10:59,679
делать несколько очень неприятных комментариев,

2083
01:10:59,679 --> 01:11:01,679
которые демонстрировали расистские сексистские антипатии.

2084
01:11:01,679 --> 01:11:04,159
-семит, вы знаете, сторонник превосходства белой расы,

2085
01:11:04,159 --> 01:11:06,400
э-э, склонность к превосходству, которая, как вы знаете, в конечном итоге,

2086
01:11:06,400 --> 01:11:08,000
вероятно, не то, что

2087
01:11:08,000 --> 01:11:10,640
имели в виду дизайнеры Тай, так что на самом деле в итоге

2088
01:11:10,640 --> 01:11:12,560
пошло

2089
01:11:12,560 --> 01:11:14,239


2090
01:11:14,239 --> 01:11:16,320


2091
01:11:16,320 --> 01:11:18,400


2092
01:11:18,400 --> 01:11:19,280
не так.

2093
01:11:19,280 --> 01:11:21,199
он был разработан, чтобы научиться демонстрировать

2094
01:11:21,199 --> 01:11:22,960
разговорные шаблоны пользователей, с

2095
01:11:22,960 --> 01:11:24,960
которыми он взаимодействовал,

2096
01:11:24,960 --> 01:11:26,800
и он сделал то, что, как вы знаете, модели nlg

2097
01:11:26,800 --> 01:11:28,560
очень хороши в захвате языкового

2098
01:11:28,560 --> 01:11:30,080
распределения их обучающих примеров

2099
01:11:30,080 --> 01:11:31,600
, это было единственной вещью, в которой

2100
01:11:31,600 --> 01:11:34,640
мы были замечательно последовательны  ну

2101
01:11:34,640 --> 01:11:36,159
вы знаете, за последние несколько лет, и

2102
01:11:36,159 --> 01:11:37,600
оказывается, что если они, как учебные

2103
01:11:37,600 --> 01:11:40,000
примеры, в конечном итоге имеют токсичный контент,

2104
01:11:40,000 --> 01:11:42,880
они научатся повторять этот контент,

2105
01:11:42,880 --> 01:11:44,480
и это, возможно, не яснее, чем если

2106
01:11:44,480 --> 01:11:46,320
вы посмотрите на то, что есть у предварительно обученных языковых

2107
01:11:46,320 --> 01:11:48,320
моделей

2108
01:11:48,320 --> 01:11:50,880
чтобы сказать о различных демографических данных,

2109
01:11:50,880 --> 01:11:52,480
так что если вы помните, что знаете большие

2110
01:11:52,480 --> 01:11:54,400
предварительно обученные языковые модели, которые

2111
01:11:54,400 --> 01:11:57,520
лежат в основе многих современных систем LG, на которых

2112
01:11:57,520 --> 01:11:59,199
они обучаются  массивные корпоративные

2113
01:11:59,199 --> 01:12:01,280
тексты, которые часто непрозрачны и сканируются

2114
01:12:01,280 --> 01:12:03,120
с онлайн-ресурсов,

2115
01:12:03,120 --> 01:12:05,120
если окажется, что эти корпуса содержат

2116
01:12:05,120 --> 01:12:07,199
токсичный контент, языковые модели

2117
01:12:07,199 --> 01:12:08,719
будут изучать его и фактически сделают его

2118
01:12:08,719 --> 01:12:10,000
еще хуже,

2119
01:12:10,000 --> 01:12:11,280
а затем окажется, что если вы предложите

2120
01:12:11,280 --> 01:12:12,880
эти языковые модели для  для определенных

2121
01:12:12,880 --> 01:12:14,239
частей информации

2122
01:12:14,239 --> 01:12:16,560
он может выплюнуть этот токсичный контент, который, как

2123
01:12:16,560 --> 01:12:18,480
вы знаете, показывает, что вы знаете очень

2124
01:12:18,480 --> 01:12:20,719
разные мнения, вы знаете, гендерные

2125
01:12:20,719 --> 01:12:23,040
расы, сексуальную ориентацию,

2126
01:12:23,040 --> 01:12:24,719
эм теперь вы знаете, я вижу, что на

2127
01:12:24,719 --> 01:12:26,640
самом деле было бы редко попросить языковую

2128
01:12:26,640 --> 01:12:28,320
модель взвесить с  их мнения по этому

2129
01:12:28,320 --> 01:12:29,199
поводу,

2130
01:12:29,199 --> 01:12:30,880
но вы должны спросить себя, закодирован ли этот

2131
01:12:30,880 --> 01:12:32,480
тип информации в

2132
01:12:32,480 --> 01:12:34,480
модели каким-либо образом, и какими еще способами

2133
01:12:34,480 --> 01:12:36,000
эти изученные шаблоны в конечном итоге могут

2134
01:12:36,000 --> 01:12:37,760
быть отражены в этой модели после того, как она

2135
01:12:37,760 --> 01:12:40,719
фактически развернута на практике,

2136
01:12:40,719 --> 01:12:42,800
и такого рода  приводит нас ко второй

2137
01:12:42,800 --> 01:12:45,199
проблеме с этими языковыми моделями,

2138
01:12:45,199 --> 01:12:46,640
мы на самом деле не знаем, как

2139
01:12:46,640 --> 01:12:48,239
информация в конечном итоге изучается и

2140
01:12:48,239 --> 01:12:49,679
кодируется ими, что означает  у нас

2141
01:12:49,679 --> 01:12:51,120
нет четкого понимания того,

2142
01:12:51,120 --> 01:12:52,560
какие типы входов будут

2143
01:12:52,560 --> 01:12:55,040
запускать какие типы выходов,

2144
01:12:55,040 --> 01:12:56,640
и на самом деле Уоллес все показал в

2145
01:12:56,640 --> 01:12:59,520
своей работе emnlp 2019, что это была

2146
01:12:59,520 --> 01:13:01,440
большая проблема, потому что если вы начнете эти

2147
01:13:01,440 --> 01:13:03,120
модели с особенно враждебными

2148
01:13:03,120 --> 01:13:05,040
входами  они, как правило,

2149
01:13:05,040 --> 01:13:07,199
немедленно превращались в создание очень токсичного

2150
01:13:07,199 --> 01:13:08,239
контента

2151
01:13:08,239 --> 01:13:10,320
, другими словами, то, что потребовалось 24 часа, чтобы

2152
01:13:10,320 --> 01:13:12,239
научиться делать, эти системы могут

2153
01:13:12,239 --> 01:13:14,000
делать из коробки, если они содержат

2154
01:13:14,000 --> 01:13:16,719
неправильные примеры,

2155
01:13:16,960 --> 01:13:19,360
и, к сожалению, неправильные примеры

2156
01:13:19,360 --> 01:13:20,880
um вы знаете, в

2157
01:13:20,880 --> 01:13:22,719
конечном итоге  знаете, что быть намного менее неприятным,

2158
01:13:22,719 --> 01:13:24,320
чем мы могли ожидать, намного менее

2159
01:13:24,320 --> 01:13:25,600
неприятным, чем те, что на предыдущем

2160
01:13:25,600 --> 01:13:28,560
слайде, ну, по крайней мере, но

2161
01:13:28,560 --> 01:13:31,760
вы знаете, что в прошлогодней работе на em nlp результаты

2162
01:13:31,760 --> 01:13:33,760


2163
01:13:33,760 --> 01:13:35,280
вы знаете, что исследовательская группа показала, что

2164
01:13:35,280 --> 01:13:36,960
вы знаете, что гораздо более безобидно выглядящие

2165
01:13:36,960 --> 01:13:38,159
вводные могут фактически заставить эти

2166
01:13:38,159 --> 01:13:39,920
языковые модели продолжать довольно ядовитые

2167
01:13:39,920 --> 01:13:42,640
диатрибы, э-э, как раньше,

2168
01:13:42,640 --> 01:13:44,640
это было не так последовательно, как

2169
01:13:44,640 --> 01:13:46,400
в предыдущей работе, но это было  до тех пор, пока

2170
01:13:46,400 --> 01:13:49,199
достаточно часто,

2171
01:13:49,199 --> 01:13:51,199
и эти примеры действительно показывают,

2172
01:13:51,199 --> 01:13:52,560
что нам нужно быть осторожными с тем,

2173
01:13:52,560 --> 01:13:54,719
как эти системы развертываются,

2174
01:13:54,719 --> 01:13:56,480
если у вас есть система nlg, вам нужны

2175
01:13:56,480 --> 01:13:58,080
меры предосторожности, чтобы она не выводила

2176
01:13:58,080 --> 01:13:59,520
вредоносный контент,

2177
01:13:59,520 --> 01:14:01,120
и это выходит за рамки примеров

2178
01:14:01,120 --> 01:14:03,520
токсичного и  биотоксичность и устройство, которое я

2179
01:14:03,520 --> 01:14:05,440
показал сегодня, вы знаете, что модель, которая может быть

2180
01:14:05,440 --> 01:14:07,280
настроена для генерации неверной или

2181
01:14:07,280 --> 01:14:09,040
неточной информации, также может быть

2182
01:14:09,040 --> 01:14:11,600
довольно опасной,

2183
01:14:11,600 --> 01:14:13,360
а также модели nlg не должны

2184
01:14:13,360 --> 01:14:15,280
развертываться без понимания того, кем

2185
01:14:15,280 --> 01:14:16,800
будут ее пользователи,

2186
01:14:16,800 --> 01:14:18,159
и есть  всегда будут

2187
01:14:18,159 --> 01:14:20,719
враждебно настроенными пользователями для любой модели, которую вы

2188
01:14:20,719 --> 01:14:22,719
создаете, даже если вы

2189
01:14:22,719 --> 01:14:26,000
не можете думать о них в данный момент,

2190
01:14:27,199 --> 01:14:29,120
и это оставляет нам последний пункт,

2191
01:14:29,120 --> 01:14:30,960
который заключается в том, что вы знаете, что достижения в

2192
01:14:30,960 --> 01:14:32,560
NLG действительно

2193
01:14:32,560 --> 01:14:34,560
позволили

2194
01:14:34,560 --> 01:14:36,320
нам  создавать системы производства текста для

2195
01:14:36,320 --> 01:14:39,120
многих новых приложений, вы знаете, как мы

2196
01:14:39,120 --> 01:14:40,640
это делаем, хотя важно

2197
01:14:40,640 --> 01:14:42,880
спросить вас, знает ли контент, который мы создаем,

2198
01:14:42,880 --> 01:14:44,800
систему для автоматической

2199
01:14:44,800 --> 01:14:47,040
генерации, вы знаете, для  r для облегчения

2200
01:14:47,040 --> 01:14:49,280
восприятия человеком эм, вы знаете, действительно ли

2201
01:14:49,280 --> 01:14:50,400
это нужно генерировать

2202
01:14:50,400 --> 01:14:52,320
автоматически,

2203
01:14:52,320 --> 01:14:54,880
и я думаю, что хорошим примером этого

2204
01:14:54,880 --> 01:14:57,520
является работа zellers вообще на nurbs 2019,

2205
01:14:57,520 --> 01:14:59,120
которая, как вы знаете, продемонстрировала потенциальную

2206
01:14:59,120 --> 01:15:01,679
опасность генераторов фальшивых новостей.

2207
01:15:01,679 --> 01:15:04,159
- обученные языковые модели. Я на

2208
01:15:04,159 --> 01:15:05,360
самом деле подумал, что это отличная работа,

2209
01:15:05,360 --> 01:15:07,040
и это выдвинуло на первый план вы знаете многие

2210
01:15:07,040 --> 01:15:09,679
средства защиты, которые вы могли бы разработать

2211
01:15:09,679 --> 01:15:12,159
против системы генерации фальшивых новостей,

2212
01:15:12,159 --> 01:15:13,840


2213
01:15:13,840 --> 01:15:15,679
но вы знаете, что дело больше в том,

2214
01:15:15,679 --> 01:15:17,440
что вы всегда должны вообразить,

2215
01:15:17,440 --> 01:15:19,280
что любая  Инструмент, который вы создаете, может быть

2216
01:15:19,280 --> 01:15:21,120
использован в негативном ключе,

2217
01:15:21,120 --> 01:15:22,960
поэтому рассказывание историй

2218
01:15:22,960 --> 01:15:25,760
в системе LG также потенциально может быть

2219
01:15:25,760 --> 01:15:28,320
перепрофилировано для создания фальшивых новостей, гм,

2220
01:15:28,320 --> 01:15:30,560


2221
01:15:30,560 --> 01:15:31,760
и вы действительно всегда должны спрашивать

2222
01:15:31,760 --> 01:15:32,880
себя, перевешивают ли положительные

2223
01:15:32,880 --> 01:15:36,080
применения той или иной технологии

2224
01:15:36,080 --> 01:15:38,159
потенциал  отрицательные ммм,

2225
01:15:38,159 --> 01:15:40,000
и это часто оказывается

2226
01:15:40,000 --> 01:15:42,640
непростым вопросом,

2227
01:15:43,280 --> 01:15:44,159


2228
01:15:44,159 --> 01:15:46,239
так что я думаю, в качестве заключительных мыслей на

2229
01:15:46,239 --> 01:15:47,760
сегодня

2230
01:15:47,760 --> 01:15:49,760
ммм, вы знаете, я просто хочу

2231
01:15:49,760 --> 01:15:51,280


2232
01:15:51,280 --> 01:15:52,560
Вы знаете, что

2233
01:15:52,560 --> 01:15:54,159
если вы начнете взаимодействовать с

2234
01:15:54,159 --> 01:15:56,320
системами NLG на практике,

2235
01:15:56,320 --> 01:15:58,400
вы быстро увидите довольно

2236
01:15:58,400 --> 01:16:01,040
большие ограничения, которые они

2237
01:16:01,040 --> 01:16:03,440
имеют тенденцию иметь даже в задачах,

2238
01:16:03,440 --> 01:16:05,280
которые, как вы знаете, достигли

2239
01:16:05,280 --> 01:16:07,120
больший прогресс

2240
01:16:07,120 --> 01:16:09,360
в создании систем, которые могут

2241
01:16:09,360 --> 01:16:11,440
достаточно хорошо справляться с этой задачей, есть еще много

2242
01:16:11,440 --> 01:16:13,520
улучшений, которые можно сделать, чтобы сделать

2243
01:16:13,520 --> 01:16:16,400
их еще

2244
01:16:16,640 --> 01:16:18,560
лучше практически в любой задаче nlg, в то же

2245
01:16:18,560 --> 01:16:21,920
время ее эффективная оценка остается

2246
01:16:21,920 --> 01:16:23,920
огромной проблемой, которую мы часто  приходится полагаться

2247
01:16:23,920 --> 01:16:26,080
на людей, чтобы дать нам наилучшие оценки

2248
01:16:26,080 --> 01:16:28,239
того, насколько хорошо наша система работает,

2249
01:16:28,239 --> 01:16:31,520
и, таким образом, область, в которой большое

2250
01:16:31,520 --> 01:16:33,679
улучшение, ну, действительно ли вы знаете,

2251
01:16:33,679 --> 01:16:35,440
типа начальной загрузки больших улучшений

2252
01:16:35,440 --> 01:16:37,199
и многих других областей nlg, можно было бы

2253
01:16:37,199 --> 01:16:40,360
найти лучше  автоматическая оценка для

2254
01:16:40,360 --> 01:16:42,640
чужих систем,

2255
01:16:42,640 --> 01:16:44,400
с другой стороны, на очень

2256
01:16:44,400 --> 01:16:45,840
оптимистичной ноте, я хочу сказать, что

2257
01:16:45,840 --> 01:16:47,520
с появлением крупномасштабных языковых

2258
01:16:47,520 --> 01:16:48,480
моделей,

2259
01:16:48,480 --> 01:16:50,400
вы знаете, глубокие исследования энергии

2260
01:16:50,400 --> 01:16:52,480
еще не  был сброшен, но никогда не было так просто

2261
01:16:52,480 --> 01:16:54,320
прыгнуть в космос и

2262
01:16:54,320 --> 01:16:56,000
начать играть с системами

2263
01:16:56,000 --> 01:16:56,800
и

2264
01:16:56,800 --> 01:16:59,199
э-э, и разрабатывать крутые новые инструменты, э-э,

2265
01:16:59,199 --> 01:17:00,960
которые могут, что вы можете знать, помогают

2266
01:17:00,960 --> 01:17:03,760
людям воспринимать контент и информацию э-э,

2267
01:17:03,760 --> 01:17:06,400
быстрее и эффективнее э-э

2268
01:17:06,400 --> 01:17:07,600
и в результате я думаю, что вы

2269
01:17:07,600 --> 01:17:09,360
знаете одну из самых захватывающих областей

2270
01:17:09,360 --> 01:17:12,320
НЛП, в которой можно работать, и я думаю,

2271
01:17:12,320 --> 01:17:14,159
что вы знаете, что если вы начнете работать в ней

2272
01:17:14,159 --> 01:17:15,679
, вы почувствуете то же самое, и я

2273
01:17:15,679 --> 01:17:17,679
бы поддержал  Вам это нужно,

2274
01:17:17,679 --> 01:17:19,679
поэтому большое спасибо за то, что пригласили меня сегодня,

2275
01:17:19,679 --> 01:17:23,239
это было действительно захватывающе

