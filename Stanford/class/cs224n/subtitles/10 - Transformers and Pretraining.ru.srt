1
00:00:05,359 --> 00:00:07,120
Привет всем,

2
00:00:07,120 --> 00:00:09,519
добро пожаловать на

3
00:00:09,519 --> 00:00:11,280
лекцию 10 по cs224n. Это будет в

4
00:00:11,280 --> 00:00:13,120
первую очередь предварительное обучение, но мы

5
00:00:13,120 --> 00:00:15,040
также

6
00:00:15,040 --> 00:00:17,600
немного обсудим модели вложенных слов и рассмотрим

7
00:00:17,600 --> 00:00:19,439
трансформаторы,

8
00:00:19,439 --> 00:00:21,359
хорошо, так что у нас есть много интересных вещей, которыми нужно

9
00:00:23,119 --> 00:00:26,880
заняться сегодня, но некоторые напоминания о

10
00:00:26,880 --> 00:00:30,640
задание класса 5

11
00:00:30,640 --> 00:00:32,320
выходит сегодня,

12
00:00:32,320 --> 00:00:35,360
задание 4 должно было быть сдано минуту назад, так что

13
00:00:35,360 --> 00:00:36,800
если вы закончили с этим поздравлением,

14
00:00:36,800 --> 00:00:38,480
если нет, я надеюсь,

15
00:00:38,480 --> 00:00:40,719
что последние дни пройдут хорошо,

16
00:00:40,719 --> 00:00:43,280
задание 5

17
00:00:43,280 --> 00:00:46,800
касается предварительного обучения и трансформеров, так что

18
00:00:46,800 --> 00:00:47,840
эти лекции продолжаются  чтобы быть очень

19
00:00:47,840 --> 00:00:49,440
полезным для вас, и он ничего не

20
00:00:49,440 --> 00:00:52,000
касается после этих лекций,

21
00:00:53,039 --> 00:00:56,960
ладно, так что сегодня давайте немного взглянем на то,

22
00:00:56,960 --> 00:00:58,640
каким

23
00:00:58,640 --> 00:01:00,960
будет план. Мы еще не говорили о

24
00:01:00,960 --> 00:01:02,719
моделировании вложенных слов и  вроде бы у нас

25
00:01:02,719 --> 00:01:04,799
должно быть э-э, и поэтому мы собираемся

26
00:01:04,799 --> 00:01:06,240
немного поговорить о дополнительных словах, которые вы

27
00:01:06,240 --> 00:01:09,520
видели в задании для э-э, все, что

28
00:01:09,520 --> 00:01:11,680
вы знаете, как данные, которые мы предоставили

29
00:01:11,680 --> 00:01:13,680
вам с вашей системой машинного перевода,

30
00:01:13,680 --> 00:01:15,119
но мы собираемся  поговорим немного

31
00:01:15,119 --> 00:01:17,520
о том, почему они настолько распространены в НЛП,

32
00:01:17,520 --> 00:01:20,720
потому что они используются в предварительно обученных

33
00:01:20,720 --> 00:01:23,040
моделях, я имею в виду, что они используются в

34
00:01:23,040 --> 00:01:24,880
нескольких разных моделях, но

35
00:01:24,880 --> 00:01:26,400
когда мы обсуждаем предварительное обучение,

36
00:01:26,400 --> 00:01:28,320
важно знать, что  второстепенные слова являются

37
00:01:28,320 --> 00:01:30,640
его частью, тогда мы как бы

38
00:01:30,640 --> 00:01:32,079
мотивируем, мы отправимся в другое путешествие по

39
00:01:32,079 --> 00:01:34,560
мотивации предварительного обучения мотивирующей модели

40
00:01:34,560 --> 00:01:36,079
из встраивания слов, так

41
00:01:36,079 --> 00:01:38,159
что мы уже видели предварительное обучение в некотором

42
00:01:38,159 --> 00:01:39,840
смысле в самом первом  лекцию по этому

43
00:01:39,840 --> 00:01:41,119
курсу,

44
00:01:41,119 --> 00:01:43,439
потому что мы предварительно обучили отдельные вложения слов,

45
00:01:43,439 --> 00:01:44,960
которые не принимают во внимание

46
00:01:44,960 --> 00:01:47,040
их контекст на очень больших текстовых

47
00:01:47,040 --> 00:01:48,479
корпусах, и увидели, что они могут

48
00:01:48,479 --> 00:01:51,200
закодировать много полезных вещей о

49
00:01:51,200 --> 00:01:53,040
языке,

50
00:01:53,040 --> 00:01:54,799
поэтому после того, как мы сделаем мотивацию,

51
00:01:54,799 --> 00:01:56,479
мы '  Я пройду через предварительное обучение модели

52
00:01:56,479 --> 00:01:58,399
тремя способами, и мы собираемся дать вам

53
00:01:58,399 --> 00:01:59,600
ссылку на лекцию во

54
00:01:59,600 --> 00:02:00,640
вторник, поэтому мы

55
00:02:00,640 --> 00:02:02,479
рассмотрим немного материала о трансформаторах,

56
00:02:02,479 --> 00:02:03,680
мы поговорим о

57
00:02:03,680 --> 00:02:05,680
предварительном обучении модели и декодерах  как

58
00:02:05,680 --> 00:02:07,200
декодер трансформатора  На прошлой неделе мы видели

59
00:02:07,200 --> 00:02:09,598
кодеры, а затем декодеры кодировщиков,

60
00:02:09,598 --> 00:02:11,200
в каждом из этих трех случаев

61
00:02:11,200 --> 00:02:13,040
мы собираемся немного поговорить о

62
00:02:13,040 --> 00:02:14,800
том, что вы могли бы делать,

63
00:02:14,800 --> 00:02:17,280
а затем о популярных моделях, которые используются

64
00:02:17,280 --> 00:02:20,160
в исследованиях и в отрасли, а

65
00:02:20,160 --> 00:02:21,520
затем мы  мы собираемся поговорить немного

66
00:02:21,520 --> 00:02:22,720
о том, что вы знаете, что, по нашему мнению,

67
00:02:22,720 --> 00:02:24,879
преподает предварительная подготовка, это

68
00:02:24,879 --> 00:02:26,560
будет очень кратко, на самом деле большая

69
00:02:26,560 --> 00:02:28,000
часть лекций по интерпретируемости и анализу

70
00:02:28,000 --> 00:02:29,840
за две недели будет больше говорить

71
00:02:29,840 --> 00:02:32,319
о своего рода  загадка

72
00:02:32,319 --> 00:02:34,720
и научная проблема выяснения того,

73
00:02:34,720 --> 00:02:36,640
что эти модели изучают о

74
00:02:36,640 --> 00:02:37,760
языке с

75
00:02:37,760 --> 00:02:39,280
помощью предварительных целей обучения, но

76
00:02:39,280 --> 00:02:40,959
мы как бы взглянем, а затем

77
00:02:40,959 --> 00:02:42,560
поговорим об очень больших моделях и

78
00:02:42,560 --> 00:02:44,160
обучении в контексте, так что, если вы  слышали

79
00:02:44,160 --> 00:02:45,280
о

80
00:02:45,280 --> 00:02:47,599
gpt-3, например, мы собираемся

81
00:02:47,599 --> 00:02:49,440
кратко коснуться этого здесь, и я думаю, что

82
00:02:49,440 --> 00:02:51,280
мы обсудим это больше в

83
00:02:51,280 --> 00:02:53,440
курсе позже,

84
00:02:53,440 --> 00:02:55,440
хорошо, у нас много работы, давайте сразу перейдем

85
00:02:57,280 --> 00:03:00,480
к структуре слов  и модели Subwood,

86
00:03:00,480 --> 00:03:01,760
давай подумаем  Это своего рода

87
00:03:01,760 --> 00:03:03,120
предположения, которые мы делали в этом

88
00:03:03,120 --> 00:03:04,480
курсе до сих пор, когда мы даем вам

89
00:03:04,480 --> 00:03:06,560
задание, когда говорим об

90
00:03:06,560 --> 00:03:08,959
обучении слов для vec, например, мы

91
00:03:08,959 --> 00:03:10,480
сделали это предположение о

92
00:03:10,480 --> 00:03:12,720
словарном запасе языка, в частности, мы сделали

93
00:03:12,720 --> 00:03:14,640
предположение, что  имеет фиксированный словарный запас

94
00:03:14,640 --> 00:03:16,159
что-то вроде десятков тысяч,

95
00:03:16,159 --> 00:03:18,159
может быть, сотен тысяч, я не знаю

96
00:03:18,159 --> 00:03:19,440
количества, да,

97
00:03:19,440 --> 00:03:21,360
некоторых, относительно большого, кажется,

98
00:03:21,360 --> 00:03:23,360
количества слов, и это кажется

99
00:03:23,360 --> 00:03:26,640
вроде довольно неплохим, эм, пока, по крайней мере, в

100
00:03:26,640 --> 00:03:28,560
том, что у нас есть  готово, и мы создаем этот

101
00:03:28,560 --> 00:03:30,400
словарь из набора, который мы обучаем,

102
00:03:30,400 --> 00:03:32,159
скажите слово для проверки,

103
00:03:32,159 --> 00:03:34,000
а затем вот что важно: любое

104
00:03:34,000 --> 00:03:36,400
новое слово, любое слово, которое вы не видели

105
00:03:36,400 --> 00:03:38,080
во время обучения,

106
00:03:38,080 --> 00:03:41,280
как бы сопоставлено с одним

107
00:03:41,280 --> 00:03:43,519
токеном unk, есть другие способы  справиться с

108
00:03:43,519 --> 00:03:45,200
этим, но вы вроде как должны что-то делать,

109
00:03:45,200 --> 00:03:47,599
и частый метод заключается в том, чтобы

110
00:03:47,599 --> 00:03:50,000
вы знали, сопоставьте их все с unk, поэтому давайте рассмотрим,

111
00:03:50,000 --> 00:03:51,680
что такого рода

112
00:03:51,680 --> 00:03:53,040
средства на английском языке

113
00:03:53,040 --> 00:03:55,120
вы изучаете вложения, вы сопоставляете их все

114
00:03:55,120 --> 00:03:57,280
работает, тогда у вас есть  вари  на

115
00:03:57,280 --> 00:04:01,599
слове, например, вкусно, с кучей

116
00:04:01,599 --> 00:04:02,959
правильных букв, и ваша модель недостаточно умна,

117
00:04:02,959 --> 00:04:05,439
чтобы знать, что такого рода средства вроде

118
00:04:05,439 --> 00:04:08,879
очень вкусный, может быть, а, и поэтому она отображает его в

119
00:04:08,879 --> 00:04:11,040
unk, потому что это просто поиск в словаре,

120
00:04:11,040 --> 00:04:11,920
промах,

121
00:04:11,920 --> 00:04:14,959
а затем вы  есть опечатка, такая как обучение,

122
00:04:14,959 --> 00:04:17,040
и которая также отображает unk, ну,

123
00:04:17,040 --> 00:04:18,399
потенциально, если этого не было в вашем

124
00:04:18,399 --> 00:04:20,160
тренировочном наборе, вы знаете, что некоторые люди делают

125
00:04:20,160 --> 00:04:21,918
опечатки, но не все из них будут

126
00:04:21,918 --> 00:04:23,520
видны во время обучения, и тогда у вас

127
00:04:23,520 --> 00:04:25,199
будут новые предметы правильно  так что это может

128
00:04:25,199 --> 00:04:27,280
быть первый раз, когда вы когда-либо видели вас

129
00:04:27,280 --> 00:04:29,440
в качестве студента в 2224n, когда вы видели

130
00:04:29,440 --> 00:04:31,440
слово преобразовать

131
00:04:31,440 --> 00:04:32,800
эм, но

132
00:04:32,800 --> 00:04:34,720
я чувствую, что вы

133
00:04:34,720 --> 00:04:36,240
как бы понимаете, что оно должно означать, например,

134
00:04:36,240 --> 00:04:38,800
может быть, добавить трансформаторы или повернуть

135
00:04:38,800 --> 00:04:40,479
использовать трансформаторы или превратить в

136
00:04:40,479 --> 00:04:42,160
трансформатор или что-то в этом роде, и

137
00:04:42,160 --> 00:04:43,759
это также будет отображаться в unk,

138
00:04:43,759 --> 00:04:46,000
даже если вы видели трансформатор, и

139
00:04:46,000 --> 00:04:48,080
если я в

140
00:04:48,080 --> 00:04:51,040
порядке, и поэтому каким-то образом мы

141
00:04:51,040 --> 00:04:53,600
должны прийти к выводу, что смотрим на слова

142
00:04:53,600 --> 00:04:56,400
как на просто  как отдельные последовательности

143
00:04:56,400 --> 00:04:59,040
символов, которые вы знаете, однозначно идентифицирует

144
00:04:59,040 --> 00:05:00,320
это слово, и это своего рода то, как мы

145
00:05:00,320 --> 00:05:01,600
должны параметризовать вещи, это просто

146
00:05:01,600 --> 00:05:03,440
неправильно,

147
00:05:03,440 --> 00:05:05,759
и так

148
00:05:05,759 --> 00:05:07,440
что это верно не только для английского, но и для

149
00:05:07,440 --> 00:05:09,199
многих языков, это предположение о ограниченном словарном запасе

150
00:05:09,199 --> 00:05:10,960
имеет еще меньше смысла, поэтому

151
00:05:10,960 --> 00:05:12,880
уже оно не имеет  смысл в английском,

152
00:05:12,880 --> 00:05:15,840
но английский - это не худшее для

153
00:05:15,840 --> 00:05:17,199
английского, поэтому

154
00:05:17,199 --> 00:05:19,840
морфология -

155
00:05:19,840 --> 00:05:22,080
это изучение структуры слов, а

156
00:05:22,080 --> 00:05:23,600
английский язык, как известно, имеет довольно простую

157
00:05:23,600 --> 00:05:26,160
морфологию в

158
00:05:26,160 --> 00:05:29,280
некотором смысле, а когда

159
00:05:29,280 --> 00:05:31,360
языки имеют сложную морфологию, это

160
00:05:31,360 --> 00:05:32,479
означает  у вас есть

161
00:05:32,479 --> 00:05:34,720
более длинные слова, более сложные слова, которые

162
00:05:34,720 --> 00:05:37,199
изменяются больше, и каждое из них

163
00:05:37,199 --> 00:05:39,199
встречается реже, и это должно

164
00:05:39,199 --> 00:05:40,560
звучать как проблема.

165
00:05:46,240 --> 00:05:47,759
он не будет отображаться в вашем

166
00:05:47,759 --> 00:05:49,039
наборе тестов, никогда в вашем наборе обучения теперь

167
00:05:49,039 --> 00:05:50,400
он сопоставлен с unk, и вы не знаете,

168
00:05:50,400 --> 00:05:51,600
что делать,

169
00:05:51,600 --> 00:05:54,000
например, вы знаете, что глаголы на суахили могут

170
00:05:54,000 --> 00:05:58,160
иметь сотни спряжений  Таким образом,

171
00:05:58,160 --> 00:06:00,400
каждое спряжение кодирует важную

172
00:06:00,400 --> 00:06:02,479
информацию о предложении, которое на

173
00:06:02,479 --> 00:06:04,720
английском языке может быть представлено с помощью

174
00:06:04,720 --> 00:06:07,360
дополнительных слов, но в суахили оно отображается

175
00:06:07,360 --> 00:06:10,000
на глагол, поскольку вы знаете префиксы,

176
00:06:10,000 --> 00:06:11,280
суффиксы и тому подобное, это называется

177
00:06:11,280 --> 00:06:13,280
флективной морфологией, и поэтому у вас могут

178
00:06:13,280 --> 00:06:14,639
быть сотни  спряжения, я просто как

179
00:06:14,639 --> 00:06:17,039
бы вставил этот

180
00:06:17,039 --> 00:06:20,080
блок викисловаря, просто чтобы дать вам небольшую выборку

181
00:06:20,080 --> 00:06:21,600
из огромного количества

182
00:06:21,600 --> 00:06:23,680
спряжения, и поэтому пытаться запомнить

183
00:06:23,680 --> 00:06:25,280
независимо значение каждого из

184
00:06:25,280 --> 00:06:27,039
этих слов - просто неправильный

185
00:06:27,039 --> 00:06:29,360
ответ

186
00:06:31,840 --> 00:06:33,039
Итак,

187
00:06:33,039 --> 00:06:34,160
это будет очень краткий

188
00:06:34,160 --> 00:06:35,919
обзор, и что мы собираемся сделать,

189
00:06:35,919 --> 00:06:37,919
это взять один,

190
00:06:37,919 --> 00:06:40,560
скажем так, класс

191
00:06:40,560 --> 00:06:43,440
алгоритмов для моделирования

192
00:06:43,440 --> 00:06:45,360
подслов, которые были

193
00:06:45,360 --> 00:06:48,960
разработаны, чтобы попытаться найти золотую середину

194
00:06:48,960 --> 00:06:52,479
между двумя вариантами, как говорит один из вариантов

195
00:06:52,479 --> 00:06:54,720
все как отдельные слова,

196
00:06:54,720 --> 00:06:56,240
либо я знаю слово, и я видел его во

197
00:06:56,240 --> 00:06:58,080
время тренировки, либо я не знаю слово,

198
00:06:58,080 --> 00:06:59,919
и это похоже на unk, а затем

199
00:06:59,919 --> 00:07:01,840
еще одна крайность  вариант - сказать, что это

200
00:07:01,840 --> 00:07:03,759
просто символы,

201
00:07:03,759 --> 00:07:05,120
так, как будто я получаю последовательность

202
00:07:05,120 --> 00:07:07,360
символов, а затем моя нейронная сеть

203
00:07:07,360 --> 00:07:09,520
поверх моей последовательности символов должна

204
00:07:09,520 --> 00:07:10,319
выучить

205
00:07:10,319 --> 00:07:12,240
все, должна научиться комбинировать

206
00:07:12,240 --> 00:07:14,800
слова и прочее, поэтому модели подслов в

207
00:07:14,800 --> 00:07:16,880
целом просто означают просмотр  своего

208
00:07:16,880 --> 00:07:18,479
рода внутренняя структура слов, которая каким-то образом

209
00:07:18,479 --> 00:07:20,639
выглядит ниже уровня слов, но эта

210
00:07:22,080 --> 00:07:24,319
группа моделей будет пытаться найти

211
00:07:24,319 --> 00:07:28,000
золотую середину, поэтому при кодировании пар байтов

212
00:07:28,160 --> 00:07:30,880
то, что мы собираемся сделать, это

213
00:07:30,880 --> 00:07:33,199
выучить словарь из  набор обучающих данных

214
00:07:33,199 --> 00:07:34,639
снова, так что теперь у нас есть набор обучающих данных

215
00:07:34,639 --> 00:07:36,639
вместо того, чтобы просто говорить о, все,

216
00:07:36,639 --> 00:07:39,120
что было разделено моим эвристическим разделителем слов,

217
00:07:39,120 --> 00:07:40,400
например

218
00:07:40,400 --> 00:07:42,720
пробелы в английском языке, например,

219
00:07:42,720 --> 00:07:45,599
будет словом в моем словаре, мы собираемся

220
00:07:46,720 --> 00:07:49,360
выучить словарь  используя жадный

221
00:07:49,360 --> 00:07:51,039
алгоритм в этом случае, вот что

222
00:07:51,039 --> 00:07:53,039
мы собираемся сделать, мы начнем со

223
00:07:53,039 --> 00:07:55,360
словаря, содержащего только символы, так

224
00:07:55,360 --> 00:07:57,520
что это наша крайняя правая, это наша,

225
00:07:57,520 --> 00:07:59,280
по крайней мере, если вы видели все

226
00:07:59,280 --> 00:08:00,720
символы

227
00:08:00,720 --> 00:08:02,879
Тогда вы знаете, что у вас никогда не

228
00:08:02,879 --> 00:08:04,720
будет права unk, потому что вы видите слово,

229
00:08:04,720 --> 00:08:05,919
которое никогда не видели, прежде чем вы просто

230
00:08:05,919 --> 00:08:07,759
разделите его на его символы, а затем

231
00:08:07,759 --> 00:08:09,360
вы попытаетесь увидеть, как вы знаете, как поступить с ним

232
00:08:09,360 --> 00:08:11,360
таким образом,

233
00:08:11,360 --> 00:08:13,840
а затем также  символ конца слова,

234
00:08:13,840 --> 00:08:15,039
а затем мы будем перебирать этот

235
00:08:15,039 --> 00:08:16,960
алгоритм, мы скажем, что используйте корпус

236
00:08:16,960 --> 00:08:20,400
текста, найдите общие смежные буквы, поэтому,

237
00:08:20,400 --> 00:08:22,080
возможно, a и b очень часто являются

238
00:08:22,080 --> 00:08:23,919
смежными,

239
00:08:24,800 --> 00:08:26,720
добавьте их пару вместе

240
00:08:26,720 --> 00:08:28,879
как одно подслово в свой

241
00:08:28,879 --> 00:08:30,240
словарь

242
00:08:30,240 --> 00:08:32,000
теперь замените экземпляры этой

243
00:08:32,000 --> 00:08:33,760
пары символов новым вложенным словом, повторяйте

244
00:08:33,760 --> 00:08:35,760
до желаемого размера словарного запаса, так что, возможно,

245
00:08:35,760 --> 00:08:38,080
вы начнете с небольшого словаря символов,

246
00:08:38,080 --> 00:08:40,880
а затем получите тот

247
00:08:40,880 --> 00:08:43,760
же небольшой словарь символов плюс

248
00:08:43,760 --> 00:08:46,640
кучу целых слов или

249
00:08:46,640 --> 00:08:48,560
частей слов  так что обратите внимание, как яблоко целое

250
00:08:48,560 --> 00:08:51,200
слово выглядит как яблоко, но тогда приложение, может быть,

251
00:08:51,200 --> 00:08:53,200
это своего рода первая часть,

252
00:08:53,200 --> 00:08:56,959
первое подслово приложения или

253
00:08:56,959 --> 00:08:59,120
да,

254
00:08:59,120 --> 00:09:00,880
а затем ли,

255
00:09:00,880 --> 00:09:03,680
я думаю, я не должен был помещать

256
00:09:03,680 --> 00:09:05,760
туда хеш,

257
00:09:05,760 --> 00:09:07,680
но вы знаете, может быть  вы выучили lee как,

258
00:09:07,680 --> 00:09:10,480
например, конец слова,

259
00:09:10,480 --> 00:09:13,440
и в итоге

260
00:09:13,440 --> 00:09:16,000
вы получите словарный запас, в котором общие вещи

261
00:09:16,000 --> 00:09:18,560
вы можете сопоставить сами с собой, а затем

262
00:09:18,560 --> 00:09:20,399
редкие последовательности символов, которые вы как бы

263
00:09:20,399 --> 00:09:22,800
разделите как можно меньше,

264
00:09:22,800 --> 00:09:24,399
и это  не всегда заканчивается так хорошо,

265
00:09:24,399 --> 00:09:26,399
что вы изучаете морфологически

266
00:09:26,399 --> 00:09:28,720
релевантные суффиксы, такие как lee,

267
00:09:28,720 --> 00:09:31,600
но вы знаете, что пытаетесь разбить вещи

268
00:09:31,600 --> 00:09:33,440
несколько разумно, и, если у вас

269
00:09:33,440 --> 00:09:35,920
достаточно данных, вспомогательный словарь, который вы изучаете,

270
00:09:35,920 --> 00:09:37,760
имеет тенденцию быть нормальным, поэтому он изначально

271
00:09:37,760 --> 00:09:40,640
использовался в  машинный перевод, а теперь

272
00:09:40,640 --> 00:09:43,040
похожий фрагмент слова метода, который

273
00:09:43,040 --> 00:09:45,040
мы не будем рассматривать в этой лекции, используется в

274
00:09:45,040 --> 00:09:46,880
предварительно обученных моделях, но вы знаете, что

275
00:09:46,880 --> 00:09:48,480
идея фактически такая же, и в итоге вы

276
00:09:48,480 --> 00:09:50,080
получаете словари, которые очень похожи на

277
00:09:50,080 --> 00:09:52,730
это, так что  если мы вернемся к нашей эм

278
00:09:52,730 --> 00:09:54,560
[музыке],

279
00:09:54,560 --> 00:09:56,880
если мы вернемся к нашим примерам, когда

280
00:09:56,880 --> 00:10:00,959
вы знаете, что уровень слов nlp подводил нас,

281
00:10:02,720 --> 00:10:04,800
тогда у вас есть отображение шляпы в шляпу,

282
00:10:04,800 --> 00:10:06,800
хорошо, у вас есть отображение шляпы в шляпу,

283
00:10:06,800 --> 00:10:08,240
потому что это было обычным  Eno  тьфу

284
00:10:08,240 --> 00:10:10,079
последовательность символов, которая была

285
00:10:10,079 --> 00:10:11,760
фактически включена в наш

286
00:10:11,760 --> 00:10:13,200
словарь

287
00:10:13,200 --> 00:10:14,560
подслов, а затем вы должны научиться отображать, чтобы

288
00:10:14,560 --> 00:10:16,640
выучить такие общие слова хорошо,

289
00:10:16,640 --> 00:10:18,240
а это означает, что модель нейронной

290
00:10:18,240 --> 00:10:19,680
сети, с которой вы собираетесь обрабатывать

291
00:10:19,680 --> 00:10:23,200
этот текст, не нуждается  сказать,

292
00:10:23,200 --> 00:10:25,360
объединить буквы обучения и шляпы,

293
00:10:25,360 --> 00:10:27,680
чтобы попытаться понравиться, получить

294
00:10:27,680 --> 00:10:29,680
значение этих слов из

295
00:10:29,680 --> 00:10:31,040
букв, потому что вы можете себе представить, что это

296
00:10:31,040 --> 00:10:33,440
может быть сложно, но затем, когда вы

297
00:10:33,440 --> 00:10:36,399
получаете слово, которое вы не видели

298
00:10:36,399 --> 00:10:39,200
раньше,  в состоянии разложить его на части,

299
00:10:39,200 --> 00:10:41,600
и поэтому, если вы видели вкусное с разным

300
00:10:41,600 --> 00:10:45,200
количеством знаков a во время торговли,

301
00:10:45,200 --> 00:10:47,279
вы знаете, может быть, вы действительно получаете некоторые из

302
00:10:47,279 --> 00:10:49,680
тех же дополнительных слов или похожих дополнительных слов, на

303
00:10:49,680 --> 00:10:51,200
которые вы разбиваете его во

304
00:10:51,200 --> 00:10:52,720
время оценки, поэтому мы никогда не думали

305
00:10:52,720 --> 00:10:54,640
достаточно вкусно, чтобы понравиться вам, знаете,

306
00:10:54,640 --> 00:10:56,640
сколько а, чтобы добавить его в

307
00:10:56,640 --> 00:10:58,959
наш словарь подслов, эм, но мы все

308
00:10:58,959 --> 00:11:00,880
еще можем разделить его на вещи, а

309
00:11:00,880 --> 00:11:02,560
затем нейронную сеть, которая работает

310
00:11:02,560 --> 00:11:05,120
поверх этих субвложений  можно было бы

311
00:11:05,120 --> 00:11:06,880
в некотором роде внушить, что о да, это

312
00:11:06,880 --> 00:11:08,880
одна из тех вещей, где такие люди, как

313
00:11:08,880 --> 00:11:09,839
вы, знают, что

314
00:11:09,839 --> 00:11:11,600
цепные буквы соединяются в цепочку гласных

315
00:11:11,600 --> 00:11:15,040
в английском для акцента,

316
00:11:15,040 --> 00:11:17,360
поэтому орфографические ошибки все еще в значительной степени мешают

317
00:11:17,360 --> 00:11:20,000
вам, так что теперь учиться

318
00:11:20,000 --> 00:11:21,519
с этой ошибкой может быть сопоставлено

319
00:11:21,519 --> 00:11:23,279
два дополнительных слова, но если вы видели

320
00:11:23,279 --> 00:11:25,360
такие орфографические ошибки достаточно часто,

321
00:11:25,360 --> 00:11:27,760
возможно, вы могли бы научиться как-то справляться с

322
00:11:27,760 --> 00:11:30,079
этим, это все еще портит модель,

323
00:11:30,079 --> 00:11:31,760
и, ммм,

324
00:11:31,760 --> 00:11:33,200
но, по крайней мере, это не просто

325
00:11:33,200 --> 00:11:34,959
недоработка, это кажется явно лучше, чем

326
00:11:34,959 --> 00:11:37,760
это, а затем  трансформировать, возможно,

327
00:11:37,760 --> 00:11:40,160
в лучшем случае, это своего рода оптимизм, но,

328
00:11:40,160 --> 00:11:42,160
может быть, в лучшем случае правильно, вы

329
00:11:42,160 --> 00:11:44,560
смогли сказать ах, да, это трансформер,

330
00:11:44,560 --> 00:11:46,800
и если я

331
00:11:46,800 --> 00:11:49,120
снова эм, дополнительные слова, которые вы изучаете,

332
00:11:49,120 --> 00:11:50,639
на самом деле, как правило, не так хорошо

333
00:11:50,639 --> 00:11:52,800
морфологически мотивированы.  думаю, что если

334
00:11:52,800 --> 00:11:55,120
я похож на суффикс clear like

335
00:11:55,120 --> 00:11:57,279
в английском языке, который имеет очень распространенное

336
00:11:57,279 --> 00:11:59,120
и воспроизводимое значение, когда вы применяете его

337
00:11:59,120 --> 00:12:01,120
к существительным, это производная

338
00:12:01,120 --> 00:12:04,000
морфология, но вы знаете, что

339
00:12:04,000 --> 00:12:06,079
можете сортировать  of comb составить слово

340
00:12:06,079 --> 00:12:08,079
tr значение transformerify, возможно,

341
00:12:08,079 --> 00:12:10,800
из его двух составляющих подслова,

342
00:12:10,800 --> 00:12:12,800
и поэтому, когда мы говорим о словах,

343
00:12:12,800 --> 00:12:14,959
вводимых в модели трансформаторов, предварительно обученные

344
00:12:14,959 --> 00:12:16,399
модели трансформаторов на протяжении

345
00:12:16,399 --> 00:12:18,240
всей этой лекции мы будем

346
00:12:18,240 --> 00:12:21,360
говорить о подсловах, так что  Я мог бы сказать

347
00:12:21,360 --> 00:12:24,000
слово э-э, и я имею в виду, что вы знаете,

348
00:12:24,000 --> 00:12:27,040
возможно, полное слово, а также, возможно, дополнительное

349
00:12:27,040 --> 00:12:28,880
слово, хорошо, поэтому, когда мы произносим последовательность

350
00:12:28,880 --> 00:12:30,320
слов, трансформатор, предварительно обученный

351
00:12:30,320 --> 00:12:33,040
трансформатор не имеет никакого представления о том,

352
00:12:33,040 --> 00:12:36,320
имеет ли он дело со словами или под-  слова,

353
00:12:36,320 --> 00:12:38,240
эм, когда он выполняет операции с вниманием,

354
00:12:40,079 --> 00:12:41,920
и это может быть проблемой, которую вы можете

355
00:12:41,920 --> 00:12:43,920
себе представить, если у вас действительно странные

356
00:12:43,920 --> 00:12:45,920
последовательности символов, у вас действительно может

357
00:12:45,920 --> 00:12:48,639
быть отдельное слово, сопоставленное с

358
00:12:48,639 --> 00:12:51,279
таким количеством подслов, сколько в нем символов,

359
00:12:51,279 --> 00:12:52,800
которые могут быть проблемой  потому что внезапно

360
00:12:52,800 --> 00:12:55,120
вы знаете, что у вас есть предложение из 10 слов, но

361
00:12:55,120 --> 00:12:57,360
одно из слов сопоставлено с вы знаете

362
00:12:57,360 --> 00:12:59,680
20 дополнительных слов, теперь у вас есть

363
00:12:59,680 --> 00:13:01,519
предложение из 30 слов, где 20 из 30 слов - это

364
00:13:01,519 --> 00:13:04,320
всего лишь одно э-э, настоящее  слово um, так что имейте это в

365
00:13:04,320 --> 00:13:06,000
виду, но

366
00:13:06,000 --> 00:13:08,000
вы знаете, я думаю, что это важно для своего

367
00:13:08,000 --> 00:13:09,760
рода предположения об открытом словарном запасе, это

368
00:13:09,760 --> 00:13:11,040
важно на английском языке, и это даже более

369
00:13:11,040 --> 00:13:14,399
важно на многих других языках

370
00:13:14,959 --> 00:13:16,480
и фактическом алгоритме, я имею в виду, что вы можете

371
00:13:16,480 --> 00:13:17,680
войти в фактические алгоритмы, которые у них есть

372
00:13:17,680 --> 00:13:18,959
сделано для этой

373
00:13:18,959 --> 00:13:20,560
пары укусов - это своего рода мой

374
00:13:20,560 --> 00:13:23,279
любимый вариант для краткого обзора части слова,

375
00:13:23,279 --> 00:13:26,880
вы также можете взглянуть на

376
00:13:27,040 --> 00:13:28,160
хорошо,

377
00:13:28,160 --> 00:13:30,320
любые вопросы по подсловам,

378
00:13:30,320 --> 00:13:34,200
я думаю, джон эм,

379
00:13:34,639 --> 00:13:37,040
о, отличный замечательный момент, так что это означает, что

380
00:13:37,040 --> 00:13:39,519
вы должны комбинировать эту подзаголовок, чтобы это

381
00:13:39,519 --> 00:13:41,760
Подслово не является концом слова,

382
00:13:41,760 --> 00:13:43,760
таа

383
00:13:43,760 --> 00:13:45,760
является своего рода сообщением модели, поэтому, если у меня было

384
00:13:45,760 --> 00:13:48,480
таа без хешей,

385
00:13:48,480 --> 00:13:50,399
это отдельное подслово, что означает, что

386
00:13:50,399 --> 00:13:52,560
есть целое слово, которое является

387
00:13:52,560 --> 00:13:54,240
та, или, по крайней мере, это не

388
00:13:54,240 --> 00:13:56,399
конец слова, см.  как здесь у меня

389
00:13:56,399 --> 00:13:58,320
нет хэшей в конце, потому что

390
00:13:58,320 --> 00:13:59,760
это указывает на то, что это в

391
00:13:59,760 --> 00:14:02,320
конце слова, разные схемы слов, э-э,

392
00:14:02,320 --> 00:14:03,680
различаются, следует ли вам ставить

393
00:14:03,680 --> 00:14:05,040
что-то в начале слова,

394
00:14:05,040 --> 00:14:07,600
если это действительно похоже на начало слова  слово или если вы

395
00:14:07,600 --> 00:14:08,560
должны поставить что-то в конце

396
00:14:08,560 --> 00:14:11,120
слова, если оно не заканчивает слово правильно, поэтому,

397
00:14:11,120 --> 00:14:12,800
когда токенизатор обрабатывает ваши

398
00:14:12,800 --> 00:14:14,399
данные, поэтому у вас есть что-то, что

399
00:14:14,399 --> 00:14:17,519
токенизирует, вы знаете это предложение в

400
00:14:17,519 --> 00:14:18,880
худшем случае,

401
00:14:18,880 --> 00:14:22,800
о да  в худшем случае

402
00:14:22,800 --> 00:14:24,800
он говорит как n, это целое слово, дайте

403
00:14:24,800 --> 00:14:27,040
ему только слово без хешей, это

404
00:14:27,040 --> 00:14:29,199
целое слово, дайте ему только слово без

405
00:14:29,199 --> 00:14:31,120
хешей, а затем,

406
00:14:31,120 --> 00:14:32,000
может быть,

407
00:14:32,000 --> 00:14:34,880
здесь, в дополнительных словах, правильно, у нас есть

408
00:14:34,880 --> 00:14:36,959
это странное слово, вложенные слова  и он разбивает

409
00:14:36,959 --> 00:14:40,560
его на под-слово и слова, и поэтому под-

410
00:14:40,560 --> 00:14:42,320
он собирается дать ему под-слово с

411
00:14:42,320 --> 00:14:44,720
под-хешем,

412
00:14:44,720 --> 00:14:46,399
чтобы указать, что это часть этого

413
00:14:46,399 --> 00:14:47,760
большего

414
00:14:47,760 --> 00:14:50,720
под-слова слова, в отличие от слова

415
00:14:50,720 --> 00:14:53,040
под-как подводная лодка, которое было бы

416
00:14:53,040 --> 00:14:54,160
другим,

417
00:14:54,160 --> 00:14:57,800
да, это здорово  вопрос,

418
00:15:02,160 --> 00:15:04,560
хорошо,

419
00:15:04,720 --> 00:15:07,279
здорово, так что это была наша заметка о

420
00:15:07,279 --> 00:15:10,000
моделировании дополнительных слов, и вы знаете, что

421
00:15:10,000 --> 00:15:12,800
дополнительные слова важны, например,

422
00:15:13,680 --> 00:15:15,199
вы знаете много приложений для перевода,

423
00:15:15,199 --> 00:15:16,800
поэтому мы дали вам дополнительные

424
00:15:16,800 --> 00:15:18,399
слова в задании на машинный перевод

425
00:15:18,399 --> 00:15:20,560
.  говорить о тебе  t моделируйте

426
00:15:20,560 --> 00:15:23,040
предварительное обучение и встраивание слов, так что

427
00:15:23,040 --> 00:15:24,959
мне очень нравится, что я могу перейти к этому

428
00:15:24,959 --> 00:15:27,120
слайду, поэтому мы видели эту цитату в

429
00:15:27,120 --> 00:15:28,560
начале урока, вы должны знать

430
00:15:28,560 --> 00:15:30,720
слово компании, которую он держит, и это

431
00:15:30,720 --> 00:15:32,160
было своего рода одним из  вещи, которые мы

432
00:15:32,160 --> 00:15:34,079
использовали для обобщения семантики распределения,

433
00:15:34,079 --> 00:15:36,399
эта идея о том, что слово для вектора

434
00:15:36,399 --> 00:15:38,560
была в некотором роде хорошо мотивирована,

435
00:15:38,560 --> 00:15:40,320
потому что значение слова можно

436
00:15:40,320 --> 00:15:43,120
рассматривать как производное от

437
00:15:43,120 --> 00:15:44,959
вида статистики совпадения

438
00:15:44,959 --> 00:15:48,560
слов, которые вместе встречаются вокруг

439
00:15:48,560 --> 00:15:50,720
это было просто потрясающе эффективным, я

440
00:15:50,720 --> 00:15:51,839
думаю,

441
00:15:51,839 --> 00:15:53,600
но есть еще одна цитата

442
00:15:53,600 --> 00:15:55,279
от того же человека, так что у нас есть jr

443
00:15:55,279 --> 00:15:58,639
firth um 1935 по сравнению с нашей предыдущей цитатой

444
00:15:58,639 --> 00:16:01,440
из 1957 года, а вторая цитата

445
00:16:01,440 --> 00:16:03,759
говорит, что полное значение слова

446
00:16:03,759 --> 00:16:05,360
всегда контекстно

447
00:16:05,360 --> 00:16:07,040
и нет  к изучению значения отдельно от

448
00:16:07,040 --> 00:16:10,320
полного контекста можно отнестись серьезно,

449
00:16:10,320 --> 00:16:12,160
теперь снова это просто вещи, о которых мы

450
00:16:12,160 --> 00:16:14,560
можем как бы подумать и разобрать,

451
00:16:14,560 --> 00:16:17,600
но это сразу приходит на ум,

452
00:16:17,600 --> 00:16:19,519
когда вы вставляете слова со словом в век

453
00:16:19,519 --> 00:16:21,440
одна из проблем заключается в том, что вы на

454
00:16:21,440 --> 00:16:23,440
самом деле не смотрите на его соседей, когда

455
00:16:23,440 --> 00:16:25,680
даете и встраиваете,

456
00:16:25,680 --> 00:16:28,880
поэтому, если у меня есть предложение, я записываю

457
00:16:28,880 --> 00:16:31,440
запись, вы знаете, что два экземпляра

458
00:16:31,440 --> 00:16:33,120
записи

459
00:16:33,120 --> 00:16:34,800
означают разные вещи, но им

460
00:16:34,800 --> 00:16:37,279
дается  одно и то же слово для поддержки встраивания

461
00:16:37,279 --> 00:16:38,800
правильно, потому что в word to vect вы

462
00:16:38,800 --> 00:16:40,720
берете строку, которую вы сопоставляете с ней, о, я видел

463
00:16:40,720 --> 00:16:42,800
запись слова, прежде чем вы получите

464
00:16:42,800 --> 00:16:46,000
такой вектор uh из вашей изученной матрицы,

465
00:16:46,000 --> 00:16:47,199
и вы дадите ему одно и то же в обоих

466
00:16:47,199 --> 00:16:49,279
случаях

467
00:16:49,279 --> 00:16:50,639
и  так что то, что мы собираемся делать

468
00:16:50,639 --> 00:16:53,920
сегодня, на самом деле концептуально не так уж и

469
00:16:53,920 --> 00:16:56,240
отличается от обучения слова до

470
00:16:56,240 --> 00:16:58,399
обучения вектору слова, которое вы можете рассматривать как

471
00:16:58,399 --> 00:17:00,720
предварительное обучение, просто очень простую модель,

472
00:17:00,720 --> 00:17:03,120
которая только назначает индивидуальный

473
00:17:03,120 --> 00:17:05,599
вектор каждому уникальному типу слова, каждый уникальный

474
00:17:05,599 --> 00:17:07,679
элемент в вашем словарном запасе сегодня мы пойдем

475
00:17:07,679 --> 00:17:10,079
намного дальше, но

476
00:17:10,079 --> 00:17:11,839
идея очень похожа,

477
00:17:11,839 --> 00:17:14,400
так что, как вы знаете, в 2017 году мы бы начали

478
00:17:14,400 --> 00:17:16,160
с предварительно обученных встраиваний

479
00:17:16,160 --> 00:17:18,400
слов и снова не запомните никакого контекста,

480
00:17:18,400 --> 00:17:20,319
поэтому вы дадите слово и  набивать  dding

481
00:17:20,319 --> 00:17:22,079
независимо от контекста, в котором он

482
00:17:22,079 --> 00:17:24,240
появляется, а затем вы узнаете, как

483
00:17:24,240 --> 00:17:25,679
включить контекст, это не похоже на то, что

484
00:17:25,679 --> 00:17:28,400
наши модели nlp никогда не использовали контекст правильно,

485
00:17:28,400 --> 00:17:30,080
вместо этого вы научились бы

486
00:17:30,080 --> 00:17:32,559
включать контекст, используя вы знаете свой lstm или,

487
00:17:32,559 --> 00:17:34,640
если это позже в 2017 году, вы знаете  ваш

488
00:17:34,640 --> 00:17:36,880
трансформатор,

489
00:17:36,880 --> 00:17:38,480
и вы научитесь включать

490
00:17:38,480 --> 00:17:41,520
контекст во время обучения задачам,

491
00:17:41,520 --> 00:17:43,760
чтобы у вас был некоторый надзор, возможно, это надзор за

492
00:17:43,760 --> 00:17:45,360
машинным переводом, может быть,

493
00:17:45,360 --> 00:17:47,440
сантименты, может быть, отвечая на вопрос, и

494
00:17:47,440 --> 00:17:48,640
вы бы узнали, как включить

495
00:17:48,640 --> 00:17:50,480
контекст в свой в свой lstm или

496
00:17:50,480 --> 00:17:51,600
иначе

497
00:17:51,600 --> 00:17:54,240
через сигнал  обучения

498
00:17:54,240 --> 00:17:55,679
вместо того, чтобы говорить через слово к сигналу x,

499
00:17:55,679 --> 00:17:57,600
и, таким образом, вы знаете, как

500
00:17:57,600 --> 00:17:59,600
пиктографически, у вас есть эти

501
00:17:59,600 --> 00:18:01,440
вложения слов здесь, поэтому красный цвет - это своего

502
00:18:01,440 --> 00:18:02,960
рода ваше слово для векторных ставок,

503
00:18:02,960 --> 00:18:04,559
и они предварительно обучены,

504
00:18:04,559 --> 00:18:06,400
а те, которые занимают некоторые из

505
00:18:06,400 --> 00:18:08,559
параметры вашей сети, а затем

506
00:18:08,559 --> 00:18:10,000
у вас есть контекстуализация, теперь

507
00:18:10,000 --> 00:18:11,360
это выглядит как lstm, но это может быть

508
00:18:11,360 --> 00:18:12,960
что угодно, поэтому это может  быть

509
00:18:12,960 --> 00:18:14,960
двунаправленным, вы знаете, что кодировщик

510
00:18:14,960 --> 00:18:17,039
здесь не предварительно обучен, и теперь

511
00:18:17,039 --> 00:18:18,400
это много параметров, которые

512
00:18:18,400 --> 00:18:20,240
предварительно не обучены, а затем, возможно, у вас есть какая-

513
00:18:20,240 --> 00:18:22,400
то функция считывания в конце

514
00:18:22,400 --> 00:18:24,480
справа, чтобы предсказать, что вы

515
00:18:24,480 --> 00:18:25,679
пытаетесь  чтобы предсказать еще раз, может быть, это

516
00:18:25,679 --> 00:18:28,160
сантименты, может быть, вы делаете, я не

517
00:18:28,160 --> 00:18:30,559
знаю темы, обозначающие все, что вы хотите

518
00:18:30,559 --> 00:18:32,400
сделать, это своего рода парадигма, как будто вы

519
00:18:32,400 --> 00:18:33,840
устанавливаете какую-то архитектуру, и вы

520
00:18:33,840 --> 00:18:36,559
только предварительно обучаете вложения слов,

521
00:18:36,559 --> 00:18:38,880
и поэтому это не  на самом деле

522
00:18:38,880 --> 00:18:40,960
концептуально обязательно самая большая

523
00:18:40,960 --> 00:18:43,360
проблема,

524
00:18:43,360 --> 00:18:44,960
потому что вы знаете, что

525
00:18:44,960 --> 00:18:47,039
мы любим думать о вещах глубокого обучения,

526
00:18:47,039 --> 00:18:48,960
что у нас есть много обучающих

527
00:18:48,960 --> 00:18:50,880
данных для наших целей, я имею в виду

528
00:18:50,880 --> 00:18:52,320
одну из вещей, которые мы

529
00:18:52,320 --> 00:18:55,520
мотивировали, вы знаете, очень глубоко  нейронные

530
00:18:55,520 --> 00:18:56,960
сети в том, что они могут принимать

531
00:18:56,960 --> 00:18:58,720
много данных и изучать из них шаблоны,

532
00:18:58,720 --> 00:19:00,960
но это накладывает бремя на наши

533
00:19:00,960 --> 00:19:05,120
последующие данные, чтобы их было достаточно

534
00:19:05,120 --> 00:19:06,880
для обучения контекстным аспектам

535
00:19:06,880 --> 00:19:09,360
языка, так что вы можете представить, если вы  только у

536
00:19:09,360 --> 00:19:11,360
вас есть немного, что вы знаете помеченные

537
00:19:11,360 --> 00:19:13,360
данные для точной настройки, вы придаете

538
00:19:13,360 --> 00:19:16,240
этим данным довольно большую роль, чтобы сказать: эй,

539
00:19:16,240 --> 00:19:17,760
может быть, вот несколько предварительно обученных встраиваний,

540
00:19:17,760 --> 00:19:19,520
но например, как вы обрабатываете подобные предложения

541
00:19:19,520 --> 00:19:21,440
и как они составляются и все такое

542
00:19:21,440 --> 00:19:23,280
это зависит от вас,

543
00:19:23,280 --> 00:19:24,640
поэтому, если у вас нет большого количества помеченных

544
00:19:24,640 --> 00:19:26,720
данных для вашей последующей задачи, вы

545
00:19:26,720 --> 00:19:29,600
просите его сделать много с помощью, вы знаете,

546
00:19:29,600 --> 00:19:30,880
большое количество параметров, которые

547
00:19:30,880 --> 00:19:34,080
были инициализированы случайным образом,

548
00:19:34,320 --> 00:19:35,919
хорошо, например, небольшая часть

549
00:19:35,919 --> 00:19:39,039
параметры были предварительно обучены,

550
00:19:39,039 --> 00:19:40,000
хорошо,

551
00:19:40,000 --> 00:19:42,720
поэтому мы идем предварительно обучать

552
00:19:42,720 --> 00:19:44,559
целые модели, я имею в виду, что концептуально вы

553
00:19:44,559 --> 00:19:46,960
знаете, что мы довольно близки к этому,

554
00:19:46,960 --> 00:19:47,840
поэтому в

555
00:19:47,840 --> 00:19:50,400
настоящее время почти все

556
00:19:50,400 --> 00:19:52,640
параметры в вашей нейронной сети и,

557
00:19:52,640 --> 00:19:54,400
скажем, много  настройки исследований и

558
00:19:54,400 --> 00:19:56,480
все чаще в промышленности инициализируются

559
00:19:56,480 --> 00:19:58,160
через предварительное обучение,

560
00:19:58,160 --> 00:19:59,600
так же как были инициализированы параметры

561
00:19:59,600 --> 00:20:02,400
последовательного соединения, а методы предварительного обучения, как

562
00:20:02,400 --> 00:20:03,679
правило,

563
00:20:03,679 --> 00:20:06,880
скрывают части входных данных

564
00:20:06,880 --> 00:20:08,640
от самой модели,

565
00:20:08,640 --> 00:20:10,400
а затем обучают модель восстанавливать

566
00:20:10,400 --> 00:20:11,520
эти части

567
00:20:11,520 --> 00:20:14,799
Как это

568
00:20:14,799 --> 00:20:16,400
связано с словом в век в слово для вектора? Вы знаете, что люди обычно не

569
00:20:16,400 --> 00:20:18,000
делают эту связь, но

570
00:20:18,000 --> 00:20:19,840
это следующее: у вас есть отдельное

571
00:20:19,840 --> 00:20:21,760
слово,

572
00:20:21,760 --> 00:20:24,159
и оно знает себя правильно, потому что у вас

573
00:20:24,159 --> 00:20:26,000
есть вложение для центрального слова

574
00:20:26,000 --> 00:20:27,919
прямо из задания  два, у вас есть

575
00:20:27,919 --> 00:20:29,200
вложение для центра, где он знает

576
00:20:29,200 --> 00:20:30,240
себя,

577
00:20:30,240 --> 00:20:32,159
и вы замаскировали всех его

578
00:20:32,159 --> 00:20:33,280
соседей,

579
00:20:33,280 --> 00:20:34,880
вы спрятали от него всех его соседей,

580
00:20:34,880 --> 00:20:36,080
а также всех его соседей по окну, которые

581
00:20:36,080 --> 00:20:37,840
вы скрыли от него, вы

582
00:20:37,840 --> 00:20:40,640
спрашиваете центральное слово  чтобы правильно предсказать своих соседей,

583
00:20:41,520 --> 00:20:43,600
и поэтому

584
00:20:43,600 --> 00:20:45,520
это подпадает под категорию

585
00:20:45,520 --> 00:20:47,120
предварительного обучения,

586
00:20:47,120 --> 00:20:48,559
все эти методы выглядят одинаково, вы

587
00:20:48,559 --> 00:20:50,799
скрываете части входных данных из модели

588
00:20:50,799 --> 00:20:52,480
и обучаете модель восстанавливать эти

589
00:20:53,919 --> 00:20:55,440
части различия с полным

590
00:20:55,440 --> 00:20:57,760
предварительным обучением модели  заключается в том, что вы не даете

591
00:20:57,760 --> 00:20:59,440
модели только отдельное слово и не

592
00:20:59,440 --> 00:21:01,039
заставляете ее выучить это слово на свадьбе, вы

593
00:21:01,039 --> 00:21:02,960
даете ей гораздо больше последовательности,

594
00:21:02,960 --> 00:21:05,280
и она предсказывает, вы знаете, протянутые

595
00:21:05,280 --> 00:21:06,799
части последовательности  мы углубимся

596
00:21:06,799 --> 00:21:08,960
в детали, но вы знаете,

597
00:21:08,960 --> 00:21:11,360
что вывод состоит в том, что все здесь

598
00:21:11,360 --> 00:21:13,039
предварительно обучено совместно,

599
00:21:13,039 --> 00:21:15,360
возможно, за исключением самого

600
00:21:15,360 --> 00:21:19,280
последнего уровня, который предсказывает, что метка в

601
00:21:19,280 --> 00:21:21,039
порядке, и это было

602
00:21:21,039 --> 00:21:23,360
исключительно эффективно при построении

603
00:21:23,360 --> 00:21:25,919
представлений языка,

604
00:21:25,919 --> 00:21:28,559
которые  просто сопоставьте похожие вещи на языке

605
00:21:28,559 --> 00:21:30,240
с аналогичными представлениями в этих

606
00:21:30,240 --> 00:21:32,960
кодировщиках, точно так же, как word to vect сопоставить

607
00:21:32,960 --> 00:21:35,280
похожие слова с аналогичными векторами,

608
00:21:35,280 --> 00:21:37,120
это было исключительно эффективно

609
00:21:37,120 --> 00:21:38,880
при инициализации параметров, когда

610
00:21:38,880 --> 00:21:40,799
вы начинаете с этих параметров,

611
00:21:40,799 --> 00:21:43,039
которые были предварительно предварительно обучены, а затем вы в

612
00:21:43,039 --> 00:21:45,600
порядке  -Настройте их на ваших помеченных данных,

613
00:21:45,600 --> 00:21:46,960
и, в-третьих, они были

614
00:21:46,960 --> 00:21:48,400
исключительно эффективны при определении

615
00:21:48,400 --> 00:21:50,559
распределений вероятностей по языку,

616
00:21:50,559 --> 00:21:52,320
например, в языковом моделировании, которые на

617
00:21:52,320 --> 00:21:53,200
самом деле

618
00:21:53,200 --> 00:21:55,120
действительно полезны для выборки в определенных

619
00:21:55,120 --> 00:21:56,320
случаях,

620
00:21:56,320 --> 00:21:57,679
так что это три способа, которыми мы

621
00:21:57,679 --> 00:21:59,840
взаимодействуем с предварительно обученными  модели мы

622
00:21:59,840 --> 00:22:01,280
используем представления только для вычисления

623
00:22:01,280 --> 00:22:03,280
сходства, мы используем их для

624
00:22:03,280 --> 00:22:05,120
инициализации параметров  ионы, и мы на самом деле просто используем

625
00:22:05,120 --> 00:22:07,679
их как вероятностные распределения,

626
00:22:07,679 --> 00:22:10,799
вроде того, как мы их тренировали,

627
00:22:12,559 --> 00:22:14,559
так что давайте, мы собираемся здесь перейти к некоторым

628
00:22:14,559 --> 00:22:16,640
техническим частям, но я как бы хочу

629
00:22:17,520 --> 00:22:19,360
обдумать общие мысли о том,

630
00:22:19,360 --> 00:22:21,280
что мы могли бы сделать с предварительным обучением.  -тренинг и

631
00:22:21,280 --> 00:22:22,720
какие вещи мы могли бы ожидать

632
00:22:22,720 --> 00:22:24,159
потенциально узнать

633
00:22:24,159 --> 00:22:26,799
из этого общего метода скрытия

634
00:22:26,799 --> 00:22:29,200
части ввода, а затем просмотра других

635
00:22:29,200 --> 00:22:30,400
частей ввода, а затем попытаться предсказать те

636
00:22:30,400 --> 00:22:31,679
части, которые вы спрятали,

637
00:22:31,679 --> 00:22:33,520
хорошо, так что

638
00:22:33,520 --> 00:22:35,440
университет Стэнфорда находится в пустом месте

639
00:22:35,440 --> 00:22:36,640
Калифорния,

640
00:22:36,640 --> 00:22:38,240
если бы мы дали модели все, что

641
00:22:38,240 --> 00:22:39,440
здесь не было вычеркнуто, и попросили бы

642
00:22:39,440 --> 00:22:40,720
предсказать среднюю

643
00:22:40,720 --> 00:22:43,840
правую часть, функция потерь научила бы

644
00:22:43,840 --> 00:22:46,159
модель предсказывать пало-

645
00:22:46,159 --> 00:22:49,120
альто здесь, я ожидаю

646
00:22:49,120 --> 00:22:52,559
хорошо, так что это пример чего-то,

647
00:22:52,559 --> 00:22:53,600
что вы можете себе представить

648
00:22:53,600 --> 00:22:55,039
цель предварительной тренировки, которую вы берете в

649
00:22:55,039 --> 00:22:57,440
предложении, вы удаляете ее часть и

650
00:22:57,440 --> 00:22:59,679
говорите воссоздать ту часть, которую я удалил, и

651
00:22:59,679 --> 00:23:01,200
в этом случае, если я просто привел кучу

652
00:23:01,200 --> 00:23:03,120
примеров, которые выглядели так, это могло бы

653
00:23:03,120 --> 00:23:04,559
узнай, что

654
00:23:04,559 --> 00:23:08,159
ты знаешь, вроде мелочи здесь

655
00:23:08,159 --> 00:23:10,880
хорошо, вот еще одна я положил пустую

656
00:23:10,880 --> 00:23:13,039
вилку на стол, эта не

657
00:23:13,039 --> 00:23:15,760
указана, ну правильно, так что это может быть

658
00:23:15,760 --> 00:23:18,320
вилка моя вилка

659
00:23:18,320 --> 00:23:21,679
его вилка ее вилка если вы знаете

660
00:23:21,679 --> 00:23:22,720
какую-то

661
00:23:22,720 --> 00:23:25,440
искру да вилку так что это  вы знаете, как

662
00:23:25,440 --> 00:23:28,400
указать виды синтаксических

663
00:23:28,400 --> 00:23:30,080
категорий вещей, которые могут

664
00:23:30,080 --> 00:23:32,720
появиться в этом контексте, ммм, так что это еще

665
00:23:32,720 --> 00:23:33,919
одна вещь, которую вы могли бы

666
00:23:33,919 --> 00:23:37,200
извлечь из такой цели

667
00:23:37,200 --> 00:23:38,240
здесь,

668
00:23:38,240 --> 00:23:39,360
так что вы можете попросить женщину перейти

669
00:23:39,360 --> 00:23:40,880
улицу, проверяя наличие пробок  пустое

670
00:23:40,880 --> 00:23:42,080
плечо

671
00:23:42,080 --> 00:23:43,279
одна из вещей, которые могут быть

672
00:23:43,279 --> 00:23:46,240
здесь затронуты, - это ее заявление со ссылкой,

673
00:23:47,279 --> 00:23:48,720
чтобы вы могли узнать о каких-то связях

674
00:23:48,720 --> 00:23:52,480
между сущностями в тексте, где

675
00:23:52,480 --> 00:23:55,120
одно слово женщина также может совместно ссылаться на

676
00:23:55,120 --> 00:23:57,679
одну и ту же сущность в мире как  это слово ээ

677
00:23:57,679 --> 00:24:00,559
это местоимение она

678
00:24:01,120 --> 00:24:02,960
здесь, о котором вы могли подумать, вы знаете, я

679
00:24:02,960 --> 00:24:04,559
пошел в океан, чтобы посмотреть на тюленей рыб-

680
00:24:04,559 --> 00:24:07,279
черепах, и теперь пусто, я не

681
00:24:07,279 --> 00:24:08,880
думаю, что есть единственно правильный ответ относительно

682
00:24:08,880 --> 00:24:10,400
того, что мы могли увидеть в этом

683
00:24:10,400 --> 00:24:11,520
бла  нк,

684
00:24:11,520 --> 00:24:13,120
но модель могла бы изучить

685
00:24:13,120 --> 00:24:14,880
распределение тех вещей, о которых люди могли

686
00:24:14,880 --> 00:24:17,440
бы говорить, когда один идет в

687
00:24:17,440 --> 00:24:19,840
океан, а двое взволнованы, чтобы увидеть морскую

688
00:24:19,840 --> 00:24:21,600
жизнь, ну правильно, так что это своего рода

689
00:24:21,600 --> 00:24:23,360
семантическая категория лексико-семантическая

690
00:24:23,360 --> 00:24:25,760
категория вещей  гм, это может быть похоже

691
00:24:25,760 --> 00:24:29,039
на тот же набор интересов, что и рыбные

692
00:24:29,039 --> 00:24:32,240
черепахи и тюлени, в контексте того, что я

693
00:24:32,240 --> 00:24:34,400
пошел в океан,

694
00:24:34,400 --> 00:24:36,000
хорошо,

695
00:24:36,000 --> 00:24:37,919
так что, вы знаете, я не знаю, я ожидаю,

696
00:24:37,919 --> 00:24:39,600
что будут примеры этого

697
00:24:39,600 --> 00:24:41,679
в большом корпусе  текст, может быть,

698
00:24:41,679 --> 00:24:43,919
книга,

699
00:24:44,159 --> 00:24:46,400
хорошо, вот еще один пример, а в

700
00:24:46,400 --> 00:24:48,400
целом ценность, которую я получил

701
00:24:48,400 --> 00:24:50,480
за два часа просмотра, была

702
00:24:50,480 --> 00:24:53,440
сумма попкорна и напитка,

703
00:24:53,440 --> 00:24:55,600
фильм был пустым,

704
00:24:55,600 --> 00:24:56,880
и именно тогда я бы

705
00:24:56,880 --> 00:24:59,120
как бы посмотрел в  аудитория и скажите,

706
00:24:59,120 --> 00:25:01,600
был ли фильм плохим или хорошим, но фильм

707
00:25:01,600 --> 00:25:03,200
был плохим,

708
00:25:03,200 --> 00:25:05,600
это мое предсказание здесь верно, и поэтому

709
00:25:05,600 --> 00:25:07,600
это учит вас кое-что о

710
00:25:07,600 --> 00:25:09,360
чувствах, о том, как люди выражают

711
00:25:09,360 --> 00:25:12,240
чувства на языке, и это

712
00:25:12,240 --> 00:25:14,080
даже

713
00:25:14,080 --> 00:25:16,159
выглядит как сама задача, как сделать

714
00:25:16,159 --> 00:25:18,080
анализ настроений - это то, что вам

715
00:25:18,080 --> 00:25:19,679
нужно сделать, чтобы выяснить, был

716
00:25:19,679 --> 00:25:21,600
ли фильм плохим или хорошим или,

717
00:25:21,600 --> 00:25:22,799
может

718
00:25:22,799 --> 00:25:24,000
быть, слово не плохое или хорошее,

719
00:25:24,000 --> 00:25:25,520
фильм закончился или что-то в этом роде,

720
00:25:25,520 --> 00:25:27,360
но как если бы вам пришлось, если бы вам

721
00:25:27,360 --> 00:25:29,440
пришлось  выбрать между плохим или хорошим, скорее

722
00:25:29,440 --> 00:25:31,120
всего, правильно, вам нужно понять

723
00:25:31,120 --> 00:25:33,039
смысл текста,

724
00:25:33,039 --> 00:25:34,240
а

725
00:25:34,240 --> 00:25:37,120
теперь это действительно увлекательно,

726
00:25:37,120 --> 00:25:37,919
хорошо,

727
00:25:37,919 --> 00:25:40,240
вот еще один ум ира пошла на

728
00:25:40,240 --> 00:25:42,480
кухню, чтобы приготовить чай,

729
00:25:42,480 --> 00:25:45,120
стоя рядом с иро зуко, размышляя о своей

730
00:25:45,120 --> 00:25:46,159
судьбе,

731
00:25:46,159 --> 00:25:48,880
оставил зуко  пусто

732
00:25:48,880 --> 00:25:50,640
хорошо, так что это немного проще, потому что

733
00:25:50,640 --> 00:25:52,559
мы действительно

734
00:25:52,559 --> 00:25:54,159
показываем только одно место, я думаю, у нас есть другое

735
00:25:54,159 --> 00:25:56,640
сейчас в судьбе, но это своего

736
00:25:56,640 --> 00:25:58,080
рода рассуждения о пространственном

737
00:25:58,080 --> 00:26:00,080
расположении и перемещении неких

738
00:26:00,080 --> 00:26:02,640
агентов в воображаемом мире, который мы  можно

739
00:26:02,640 --> 00:26:05,600
представить текст, в котором есть такие строки, как этот

740
00:26:05,600 --> 00:26:07,200
человек вошел в это место и был рядом

741
00:26:07,200 --> 00:26:08,960
с тем-то и таким-то, кто ушел, и сделал это, и

742
00:26:08,960 --> 00:26:10,640
не сделал, и поэтому у вас есть эти похожие

743
00:26:10,640 --> 00:26:13,679
отношения, ну вот, зуко покинул

744
00:26:13,679 --> 00:26:15,919
кухню, наиболее вероятно  вещь, которая, как я

745
00:26:15,919 --> 00:26:18,000
думаю, могла бы пойти сюда, и это как бы

746
00:26:18,000 --> 00:26:20,880
указывает на то, что для того, чтобы модель

747
00:26:20,880 --> 00:26:23,520
научилась выполнять эту

748
00:26:23,520 --> 00:26:25,679
задачу заполнения недостающей части,

749
00:26:25,679 --> 00:26:27,200
ей может потребоваться

750
00:26:27,200 --> 00:26:28,640
в целом

751
00:26:28,640 --> 00:26:31,360
выяснить, где

752
00:26:31,360 --> 00:26:33,360
находятся вещи, и

753
00:26:33,360 --> 00:26:36,559
означают ли утверждения или подразумевают эту местность

754
00:26:36,559 --> 00:26:38,480
так что стоя рядом с

755
00:26:38,480 --> 00:26:40,640
ирой пошел на кухню, теперь ира

756
00:26:40,640 --> 00:26:41,919
на кухне,

757
00:26:41,919 --> 00:26:43,760
а затем стоя рядом с иро означает, что

758
00:26:43,760 --> 00:26:45,919
зуко сейчас на кухне, а потом вы

759
00:26:45,919 --> 00:26:46,720
знаете, что

760
00:26:46,720 --> 00:26:48,960
зуко теперь уходит с того места, где он был на

761
00:26:48,960 --> 00:26:50,960
кухне раньше, так что это своего рода очень

762
00:26:50,960 --> 00:26:53,760
базовый смысл рассуждений,

763
00:26:54,880 --> 00:26:56,880
вот это предложение, которое я

764
00:26:56,880 --> 00:26:58,400
думал о последовательности, которая идет 1

765
00:26:58,400 --> 00:27:04,640
1 2 3 5 8 13 21 мм пусто, так что

766
00:27:04,640 --> 00:27:06,320
я не знаю, я могу представить, что люди

767
00:27:06,320 --> 00:27:07,760
пишут что-то, так что это

768
00:27:07,760 --> 00:27:10,240
последовательность Фибоначчи и  как бы вы знаете, вы суммируете

769
00:27:10,240 --> 00:27:11,600
эти два, чтобы получить следующий некоторые из

770
00:27:11,600 --> 00:27:12,960
этих двух к следующему, некоторые из этих

771
00:27:12,960 --> 00:27:15,120
двух верны, и вы знаете, что у вас есть эта

772
00:27:15,120 --> 00:27:17,360
текущая сумма, это известная последовательность, она

773
00:27:17,360 --> 00:27:18,559
появляется во многих текстах в

774
00:27:18,559 --> 00:27:20,000
Интернете

775
00:27:20,000 --> 00:27:21,600
вы знаете и в целом  l вам нужно

776
00:27:21,600 --> 00:27:23,360
изучить алгоритм

777
00:27:23,360 --> 00:27:25,600
или просто формулу, которая, я полагаю,

778
00:27:25,600 --> 00:27:27,200
определяет последовательность Фибоначчи,

779
00:27:27,200 --> 00:27:29,360
чтобы продолжать

780
00:27:29,360 --> 00:27:31,919
работу, модели изучают это на практике,

781
00:27:31,919 --> 00:27:34,480
подождите и узнайте, но вам нужно

782
00:27:34,480 --> 00:27:36,559
будет изучить это, чтобы узнать

783
00:27:36,559 --> 00:27:37,919
последовательность  чтобы продолжать работать и идти и

784
00:27:37,919 --> 00:27:40,240
идти

785
00:27:40,640 --> 00:27:43,120
хорошо, поэтому мы собираемся перейти к

786
00:27:43,120 --> 00:27:45,600
конкретным предварительно обученным моделям, конкретным

787
00:27:45,600 --> 00:27:47,279
методам предварительного обучения сейчас,

788
00:27:47,279 --> 00:27:49,360
поэтому я собираюсь пройти краткий

789
00:27:49,360 --> 00:27:53,840
обзор декодеров трансформаторных кодеров

790
00:27:53,840 --> 00:27:55,919
и декодеров кодеров,

791
00:27:55,919 --> 00:27:57,520
потому что мы '  Сейчас мы собираемся вникнуть

792
00:27:57,520 --> 00:27:59,600
в технические детали, поэтому, прежде чем я это сделаю,

793
00:27:59,600 --> 00:28:01,679
я сделаю паузу,

794
00:28:01,679 --> 00:28:04,880
есть ли какие-нибудь вопросы

795
00:28:05,520 --> 00:28:06,960
? Да, есть интересный

796
00:28:06,960 --> 00:28:08,960
вопрос о том, как

797
00:28:08,960 --> 00:28:10,799
получить нашу модель на

798
00:28:10,799 --> 00:28:13,520
наших входных обучающих данных заранее,

799
00:28:13,520 --> 00:28:14,720
и, возможно, вы также можете  ответьте на этот

800
00:28:14,720 --> 00:28:17,039
вопрос в

801
00:28:18,080 --> 00:28:20,159
настоящее время,

802
00:28:20,159 --> 00:28:22,240
извините, первая часть этого вопроса

803
00:28:22,240 --> 00:28:24,399
заключалась в том, что мы слишком подогнали наши модели к

804
00:28:24,399 --> 00:28:26,640
тому,

805
00:28:29,200 --> 00:28:31,520
что они делают, да, обучение

806
00:28:31,520 --> 00:28:33,200
получилось,

807
00:28:33,200 --> 00:28:34,880
да, это хороший момент, поэтому мы используем

808
00:28:34,880 --> 00:28:36,399
очень большие модели,

809
00:28:36,399 --> 00:28:38,240
и мы  Я могу представить себе, что есть

810
00:28:38,240 --> 00:28:39,919
риск переобучения,

811
00:28:39,919 --> 00:28:42,640
и на практике да, это действительно

812
00:28:42,640 --> 00:28:44,559
одна из наиболее важных вещей, которые нужно сделать для

813
00:28:44,559 --> 00:28:46,159
предварительной подготовки,

814
00:28:46,159 --> 00:28:47,679
поэтому оказывается, что вам нужно иметь

815
00:28:47,679 --> 00:28:50,080
много много данных, например много

816
00:28:50,080 --> 00:28:51,039
данных

817
00:28:51,039 --> 00:28:53,600
и на самом деле мы покажем результаты позже,

818
00:28:53,600 --> 00:28:54,720
когда люди

819
00:28:54,720 --> 00:28:56,559
построили предварительно обученную модель, предварительно обучив ее

820
00:28:56,559 --> 00:28:58,880
на большом количестве данных, а затем, как через шесть

821
00:28:58,880 --> 00:29:01,039
месяцев, появился кто-то другой, это было

822
00:29:01,039 --> 00:29:02,480
типа эй, если вы предварительно обучили ее через 10

823
00:29:02,480 --> 00:29:04,240
месяцев  и почти ничего не изменил,

824
00:29:04,240 --> 00:29:06,159
сейчас все пошло бы еще лучше, если бы

825
00:29:06,159 --> 00:29:08,559
это было переоснащением, я имею в виду, что

826
00:29:08,559 --> 00:29:10,240
вы можете вроде как

827
00:29:10,240 --> 00:29:12,480
протянуть какой-то текст во время предварительного обучения

828
00:29:12,480 --> 00:29:14,159
и как бы оценить

829
00:29:14,159 --> 00:29:15,760
затруднения, правильно лингвистическое

830
00:29:15,760 --> 00:29:18,559
моделирование на этом протянувшемся тексте, и это

831
00:29:18,559 --> 00:29:20,159
имеет тенденцию быть так, что на самом деле эти

832
00:29:20,159 --> 00:29:22,480
модели недостаточно подходят, что нам

833
00:29:22,480 --> 00:29:25,600
нужны все более и более крупные модели для

834
00:29:25,600 --> 00:29:27,600
выражения сложных взаимодействий, которые

835
00:29:27,600 --> 00:29:30,559
позволяют нам лучше соответствовать этим наборам данных,

836
00:29:30,559 --> 00:29:31,919
и поэтому мы поговорим об этом, когда будем

837
00:29:31,919 --> 00:29:33,279
говорить о bert

838
00:29:33,279 --> 00:29:34,399
и  один  из действительно интересных

839
00:29:34,399 --> 00:29:36,559
результатов то, что Burt не

840
00:29:36,559 --> 00:29:38,799
подходит, а не слишком подходит, но в принципе да,

841
00:29:38,799 --> 00:29:41,200
это проблема, потенциально проблема с

842
00:29:41,200 --> 00:29:43,440
переоборудованием, но в конечном итоге у нас есть тонна

843
00:29:43,440 --> 00:29:45,520
текста на английском языке, по крайней мере, хотя и не на

844
00:29:45,520 --> 00:29:48,320
всех языках, и так

845
00:29:48,320 --> 00:29:49,840
что да  важно масштабировать их, но в

846
00:29:49,840 --> 00:29:51,600
настоящее время наши модели не кажутся слишком подходящими

847
00:29:51,600 --> 00:29:55,320
для текста предварительного обучения,

848
00:29:58,720 --> 00:30:02,240
хорошо, любые другие

849
00:30:03,600 --> 00:30:05,440
вопросы хорошо,

850
00:30:05,440 --> 00:30:07,679
поэтому мы видели этот рисунок раньше, прямо здесь

851
00:30:07,679 --> 00:30:09,679
мы видели этот рисунок

852
00:30:09,679 --> 00:30:11,600
декодера кодировщика трансформатора из этого документа

853
00:30:11,600 --> 00:30:15,679
внимание, все  тебе нужно э-э, э-э,

854
00:30:15,679 --> 00:30:17,600
так что у нас есть пара вещей, которые мы не

855
00:30:17,600 --> 00:30:19,679
собираемся

856
00:30:19,679 --> 00:30:21,679
снова обсуждать сегодня, потому что нам нужно многое обсудить,

857
00:30:21,679 --> 00:30:23,679
но я рад поболтать об этом больше

858
00:30:23,679 --> 00:30:25,360
на э-э, э-э,

859
00:30:25,360 --> 00:30:27,679
но так что  в нашем кодировщике у нас есть некоторая

860
00:30:27,679 --> 00:30:29,200
входная последовательность, помните, что есть

861
00:30:29,200 --> 00:30:31,440
последовательность подслов, теперь

862
00:30:31,440 --> 00:30:34,080
каждое подслово получает встраивание слова,

863
00:30:34,080 --> 00:30:36,399
а каждый индекс в преобразователе

864
00:30:36,399 --> 00:30:38,080
получает встраивание позиции

865
00:30:38,080 --> 00:30:39,520
теперь помните, что у нас есть конечная

866
00:30:39,520 --> 00:30:42,080
длина, которую может иметь наша последовательность uh  bly

867
00:30:42,080 --> 00:30:44,240
будет похоже на 512

868
00:30:44,240 --> 00:30:46,159
токенов, это была та заглавная буква t

869
00:30:46,159 --> 00:30:47,919
из последней лекции, поэтому у вас есть некоторая

870
00:30:47,919 --> 00:30:50,480
конечная длина, поэтому у вас есть одно

871
00:30:50,480 --> 00:30:52,559
встраивание позиции для каждого

872
00:30:52,559 --> 00:30:54,880
индекса для всех 512 индексов,

873
00:30:54,880 --> 00:30:55,919
а затем у

874
00:30:55,919 --> 00:30:57,440
вас есть все вложения ваших слов, а

875
00:30:57,440 --> 00:30:59,760
затем кодировщик преобразователя был

876
00:30:59,760 --> 00:31:01,919
эта комбинация

877
00:31:01,919 --> 00:31:04,720
субмодулей, которые мы

878
00:31:04,720 --> 00:31:08,080
проходили строка за строкой во вторник, правильно,

879
00:31:08,080 --> 00:31:09,600
многоголовое внимание было своего рода

880
00:31:09,600 --> 00:31:12,159
основным строительным блоком,

881
00:31:12,159 --> 00:31:14,480
а затем у нас были права на остаточную и слойную норму,

882
00:31:14,480 --> 00:31:16,320
чтобы помочь с прохождением градиентов

883
00:31:16,320 --> 00:31:17,919
и помочь в обучении  лучше и

884
00:31:17,919 --> 00:31:21,760
быстрее у нас был этот слой прямого распространения,

885
00:31:21,760 --> 00:31:22,799
чтобы

886
00:31:22,799 --> 00:31:24,799
обработать вид

887
00:31:24,799 --> 00:31:26,480
результата многоголового внимания, другой

888
00:31:26,480 --> 00:31:28,799
остаток и норму уровня, а затем перейти к тому

889
00:31:28,799 --> 00:31:30,799
же блоку кодировщика трансформатора

890
00:31:30,799 --> 00:31:32,880
здесь, и они будут сложены, я

891
00:31:32,880 --> 00:31:34,399
увижу несколько разных  конфигурации

892
00:31:34,399 --> 00:31:37,840
здесь, но я думаю, что вы знаете шесть

893
00:31:37,840 --> 00:31:39,679
12 таких сложенных вместе,

894
00:31:39,679 --> 00:31:40,640
хорошо,

895
00:31:40,640 --> 00:31:42,240
так что это кодировщик трансформатора, и сегодня

896
00:31:42,240 --> 00:31:44,720
мы собираемся увидеть целые модели

897
00:31:44,720 --> 00:31:48,399
это просто трансформаторные кодеры,

898
00:31:48,399 --> 00:31:50,000
хорошо, поэтому мы, когда мы говорили о машинном

899
00:31:50,000 --> 00:31:51,200
переводе, когда мы говорили о

900
00:31:51,200 --> 00:31:53,600
самом трансформаторе, декодере трансформаторного кодера,

901
00:31:53,600 --> 00:31:55,679
мы говорили обо всем этом,

902
00:31:55,679 --> 00:31:57,039
но на самом деле вы могли бы просто иметь этот

903
00:31:57,039 --> 00:31:59,039
левый столбец, вы могли бы фактически просто иметь

904
00:31:59,039 --> 00:32:01,440
этот правый столбец,

905
00:32:01,440 --> 00:32:03,120
ммм  хотя правый столбец немного изменится,

906
00:32:03,120 --> 00:32:04,720
если он у вас просто есть, так что

907
00:32:04,720 --> 00:32:07,360
помните, что в правом столбце у нас было это

908
00:32:07,360 --> 00:32:09,840
замаскированное самовнимание с несколькими головами,

909
00:32:09,840 --> 00:32:11,200
так что вы не можете смотреть в

910
00:32:11,200 --> 00:32:13,440
будущее,

911
00:32:13,440 --> 00:32:15,600
и кто-то спросил на самом деле о том, как

912
00:32:15,600 --> 00:32:18,159
мы декодируем из трансформаторов  учитывая, что у

913
00:32:18,159 --> 00:32:19,360
вас есть такая большая

914
00:32:19,360 --> 00:32:21,039
операция по фрагментации, это отличный вопрос, я не

915
00:32:21,039 --> 00:32:23,039
смогу подробно разбираться в нем сегодня,

916
00:32:23,039 --> 00:32:25,360
но вам нужно запускать его один раз во время

917
00:32:25,360 --> 00:32:28,080
процесса декодирования каждый раз, когда вы

918
00:32:28,080 --> 00:32:31,279
декодируете, чтобы вроде как предсказать следующий  слово

919
00:32:31,279 --> 00:32:33,440
эм, я напишу что-нибудь по

920
00:32:33,440 --> 00:32:35,039
этому

921
00:32:35,039 --> 00:32:36,880
поводу, так что в массовом самовнушении с несколькими головами

922
00:32:36,880 --> 00:32:38,159
вам не разрешено смотреть в будущее,

923
00:32:38,159 --> 00:32:40,320
чтобы у вас было что-то вроде

924
00:32:40,320 --> 00:32:42,159
четко определенного  цель попытки выполнить

925
00:32:42,159 --> 00:32:44,399
языковое моделирование um тогда у нас есть

926
00:32:44,399 --> 00:32:46,399
остаток и норма уровня, перекрестное внимание с несколькими головками

927
00:32:46,399 --> 00:32:48,000
помните, что оно возвращается

928
00:32:48,000 --> 00:32:49,919
к последнему уровню

929
00:32:49,919 --> 00:32:52,159
кодировщика трансформатора или к последнему блоку кодера трансформатора,

930
00:32:53,279 --> 00:32:54,720
а затем к более остаточной норме уровня

931
00:32:54,720 --> 00:32:56,399
другого уровня прямой связи

932
00:32:56,399 --> 00:32:58,480
или  если у

933
00:32:58,480 --> 00:33:00,720
нас нет кодировщика прямо здесь,

934
00:33:00,720 --> 00:33:02,880
тогда мы избавляемся от перекрестного

935
00:33:02,880 --> 00:33:04,559
внимания, остатка и нормы уровня

936
00:33:04,559 --> 00:33:06,240
прямо здесь, поэтому, если бы у нас не было этого

937
00:33:06,240 --> 00:33:08,320
стека кодировщиков, декодеры стали бы

938
00:33:08,320 --> 00:33:09,760
проще, потому что вы не  им не нужно уделять внимание,

939
00:33:10,640 --> 00:33:12,080
но опять же, у вас также есть эти

940
00:33:12,080 --> 00:33:13,760
вложения слов внизу и позиционные

941
00:33:13,760 --> 00:33:17,519
представления для выходной последовательности,

942
00:33:17,519 --> 00:33:21,120
хорошо, так что это было рассмотрено, давай поговорим

943
00:33:21,120 --> 00:33:22,399
о предварительном обучении с помощью языкового

944
00:33:22,399 --> 00:33:24,640
моделирования, так что мы действительно поговорили, может быть,

945
00:33:24,640 --> 00:33:25,840
немного  немного об этом раньше, и мы

946
00:33:25,840 --> 00:33:28,880
видели языковое моделирование в контексте,

947
00:33:28,880 --> 00:33:30,880
возможно, просто желания сделать это априори, поэтому

948
00:33:30,880 --> 00:33:32,799
языковые модели были полезны, например,

949
00:33:32,799 --> 00:33:35,039
в системе автоматического распознавания речи.  Тем не менее,

950
00:33:35,039 --> 00:33:36,720
они были полезны в системах статистического машинного

951
00:33:36,720 --> 00:33:38,960
перевода, поэтому

952
00:33:38,960 --> 00:33:41,919
давайте вспомним задачу моделирования языка,

953
00:33:41,919 --> 00:33:44,399
вы можете сказать, что она определяется как моделирование

954
00:33:44,399 --> 00:33:47,200
вероятности слова при заданном индексе

955
00:33:47,200 --> 00:33:49,440
t любого слова в любом заданном индексе с учетом

956
00:33:49,440 --> 00:33:51,600
всех слов перед ним,

957
00:33:51,600 --> 00:33:54,080
и это распределение

958
00:33:54,080 --> 00:33:55,840
вероятностей  это распределение слов с учетом

959
00:33:55,840 --> 00:33:58,320
их прошлого контекста,

960
00:33:59,360 --> 00:34:01,039
и поэтому это просто говорит о том, что вы

961
00:34:01,039 --> 00:34:02,960
знаете для любого префикса здесь

962
00:34:02,960 --> 00:34:04,880
iro делает, чтобы

963
00:34:04,880 --> 00:34:06,320
я хотел, чтобы вероятность того, что

964
00:34:06,320 --> 00:34:08,320
должно быть следующее слово, так что наблюдаемое следующее

965
00:34:08,320 --> 00:34:10,800
слово будет вкусным, но вы знаете

966
00:34:10,800 --> 00:34:13,359
может быть, там идет приготовить чай,

967
00:34:13,359 --> 00:34:15,918
идет приготовить горячую воду и т. д. вы можете

968
00:34:15,918 --> 00:34:17,839
распределить, какое следующее слово

969
00:34:17,839 --> 00:34:20,239
должно быть в этом декодере, и помните,

970
00:34:20,239 --> 00:34:21,520
что из-за замаскированного

971
00:34:21,520 --> 00:34:24,159
самовнимания make может оглянуться на

972
00:34:24,159 --> 00:34:27,359
слово или идет или iro  но он не может

973
00:34:27,359 --> 00:34:30,560
рассчитывать на вкусное,

974
00:34:30,560 --> 00:34:31,760
поэтому для этого есть много данных, у

975
00:34:31,760 --> 00:34:33,119
вас просто есть текст,

976
00:34:33,119 --> 00:34:35,440
и, как вуаля, у вас есть

977
00:34:35,440 --> 00:34:38,239
данные языкового моделирования, это бесплатно, хорошо, когда у

978
00:34:38,239 --> 00:34:40,239
вас есть текст, он свободен  вы знаете, что

979
00:34:40,239 --> 00:34:42,320
доступно, вам не нужно его маркировать,

980
00:34:42,320 --> 00:34:43,599
и на английском у вас его много,

981
00:34:43,599 --> 00:34:45,918
это верно не для всех

982
00:34:45,918 --> 00:34:47,918
языков, но в английском языке у вас есть

983
00:34:47,918 --> 00:34:51,839
много данных до обучения,

984
00:34:51,839 --> 00:34:52,879
и

985
00:34:52,879 --> 00:34:54,879
поэтому вы знаете  простая вещь о

986
00:34:54,879 --> 00:34:56,239
некотором предварительном обучении - это

987
00:34:56,239 --> 00:34:57,920
хорошо, что мы собираемся сделать, это

988
00:34:57,920 --> 00:34:59,280
обучить нейронную сеть выполнять

989
00:34:59,280 --> 00:35:01,119
языковое моделирование на большом объеме

990
00:35:01,119 --> 00:35:02,960
текста, и мы просто сохраним вашу известную

991
00:35:02,960 --> 00:35:04,560
сеть в параметрах  нашей обученной

992
00:35:04,560 --> 00:35:06,480
сети на диск,

993
00:35:06,480 --> 00:35:08,640
так что концептуально это на самом деле не

994
00:35:08,640 --> 00:35:09,760
отличается от того, что мы

995
00:35:09,760 --> 00:35:12,079
делали до этого, это просто своего рода намерение,

996
00:35:12,079 --> 00:35:13,760
правильно мы обучаем эти параметры, чтобы

997
00:35:13,760 --> 00:35:15,760
начать использовать их для чего-то еще в

998
00:35:15,760 --> 00:35:17,359
дальнейшем, но

999
00:35:17,359 --> 00:35:19,920
моделирование языка  сам по себе не

1000
00:35:19,920 --> 00:35:22,079
меняет декодер здесь не меняется правильно,

1001
00:35:22,079 --> 00:35:24,720
это преобразователь в предварительно обученных моделях

1002
00:35:24,720 --> 00:35:26,480
в современном, потому что это своего рода

1003
00:35:26,480 --> 00:35:30,160
новая популярная концепция,

1004
00:35:30,160 --> 00:35:32,000
хотя вы знаете, что еще в 2015 году было что-то вроде

1005
00:35:32,000 --> 00:35:34,640
того, когда это, я думаю, было  сначала вы

1006
00:35:34,640 --> 00:35:36,800
знаете эффективно  d out и получил некоторые

1007
00:35:36,800 --> 00:35:38,800
интересные результаты

1008
00:35:38,800 --> 00:35:41,359
ммм, но вы знаете, что сегодня здесь может быть что угодно,

1009
00:35:42,240 --> 00:35:43,359
в основном это будут

1010
00:35:43,359 --> 00:35:44,800
трансформаторы и модели, которые мы на

1011
00:35:44,800 --> 00:35:47,440
самом деле наблюдаем,

1012
00:35:47,440 --> 00:35:49,040
хорошо,

1013
00:35:49,040 --> 00:35:50,240
поэтому, когда у вас есть ваша предварительно обученная

1014
00:35:50,240 --> 00:35:52,079
сеть, что

1015
00:35:52,079 --> 00:35:54,560
вы делаете по умолчанию?  Примите, чтобы использовать это правильно, и если

1016
00:35:54,560 --> 00:35:56,240
вы извлечете из этой лекции что-нибудь

1017
00:35:56,240 --> 00:35:57,760
с точки зрения тех же

1018
00:35:57,760 --> 00:35:59,200
инженерных практик, которые будут

1019
00:35:59,200 --> 00:36:01,920
вам в целом полезны, когда вы начнете

1020
00:36:01,920 --> 00:36:04,880
строить и изучать вещи, возможно, вы

1021
00:36:04,880 --> 00:36:07,680
знаете, как инженер по машинному обучению

1022
00:36:07,680 --> 00:36:09,760
или вычислительный  социолог или

1023
00:36:09,760 --> 00:36:11,839
и т. д. эм, вы знаете, что люди

1024
00:36:11,839 --> 00:36:14,160
обычно делают, это вы предварительно обучаете свою сеть

1025
00:36:14,160 --> 00:36:16,560
только на большом количестве данных, много текста, изучаете

1026
00:36:16,560 --> 00:36:19,520
очень общие вещи, а затем

1027
00:36:19,520 --> 00:36:21,839
вы адаптируете сеть к тому, что вы

1028
00:36:21,839 --> 00:36:23,680
хотели сделать, так что у нас была куча

1029
00:36:23,680 --> 00:36:25,760
данных до обучения, а затем, возможно, это

1030
00:36:25,760 --> 00:36:27,280
обзор фильма, который мы берем здесь в качестве

1031
00:36:27,280 --> 00:36:30,320
входных данных, и мы просто применяем

1032
00:36:30,320 --> 00:36:31,839
э-э, применяем

1033
00:36:31,839 --> 00:36:33,520
декодер, который мы как бы предварительно обучили,

1034
00:36:33,520 --> 00:36:36,160
запускаем параметры там,

1035
00:36:36,160 --> 00:36:39,200
а затем настраиваем его  что бы мы ни

1036
00:36:40,720 --> 00:36:42,480
хотели сделать, может быть, это задача анализа настроений,

1037
00:36:42,480 --> 00:36:45,280
поэтому мы прогоняем всю

1038
00:36:45,280 --> 00:36:47,760
последовательность через декодер, чтобы получить

1039
00:36:47,760 --> 00:36:49,359
скрытое состояние в конце в самом последнем,

1040
00:36:49,359 --> 00:36:51,040
а затем мы прогнозируем, что вы знаете, может быть,

1041
00:36:51,040 --> 00:36:53,280
плюс или минус  настроения,

1042
00:36:53,280 --> 00:36:54,560
и это своего рода адаптация

1043
00:36:54,560 --> 00:36:56,240
предварительно обученной сети к задачам, эта

1044
00:36:56,240 --> 00:37:00,240
предварительно обученная парадигма тонкой настройки дико

1045
00:37:00,240 --> 00:37:03,119
успешна, и вам действительно стоит попробовать ее

1046
00:37:03,119 --> 00:37:05,599
всякий раз, когда вы эффективно выполняете какие-либо задачи nlp в

1047
00:37:05,599 --> 00:37:08,800
настоящее время,

1048
00:37:08,800 --> 00:37:11,119
потому что это, как правило, то, что  какой-то

1049
00:37:11,119 --> 00:37:12,720
вариант этого, как правило, лучше всего работает

1050
00:37:14,800 --> 00:37:17,440
хорошо, так что теперь у нас есть техническая заметка,

1051
00:37:19,760 --> 00:37:20,560
так что,

1052
00:37:20,560 --> 00:37:22,640
если вам не нравится думать об

1053
00:37:22,640 --> 00:37:26,000
оптимизации или градиентном спуске, вы

1054
00:37:26,000 --> 00:37:27,680
знаете, может быть, пропустите этот слайд, но

1055
00:37:27,680 --> 00:37:29,760
я рекомендую вам  просто подумайте, подумайте

1056
00:37:29,760 --> 00:37:32,320
на секунду о том, почему это должно помочь

1057
00:37:32,320 --> 00:37:34,320
вам знать

1058
00:37:34,320 --> 00:37:36,240
обучающие нейронные сети, мы

1059
00:37:36,240 --> 00:37:38,400
используем градиентный спуск, чтобы попытаться найти

1060
00:37:38,400 --> 00:37:40,560
какое-то глобальное минимальное право этой

1061
00:37:40,560 --> 00:37:43,440
оптимизации этой функции потерь,

1062
00:37:43,440 --> 00:37:44,240
и

1063
00:37:44,240 --> 00:37:45,920
вы знаете, что мы  Мы как бы делаем это в

1064
00:37:45,920 --> 00:37:48,640
два этапа, и первый шаг - вы знаете,

1065
00:37:48,640 --> 00:37:51,760
что у нас есть некоторые параметры, тета-шляпа,

1066
00:37:51,760 --> 00:37:53,839
аппроксимируя

1067
00:37:53,839 --> 00:37:56,480
min по нашим, извините, тета - это

1068
00:37:56,480 --> 00:37:58,000
параметры нейронной сети, поэтому все

1069
00:37:59,359 --> 00:38:01,359
векторы kqv в нашем преобразователе записывают

1070
00:38:01,359 --> 00:38:03,040
word встраивает позиции, встраивает

1071
00:38:03,040 --> 00:38:04,480
это просто

1072
00:38:04,480 --> 00:38:06,800
все параметры нашей нейронной сети um,

1073
00:38:06,800 --> 00:38:08,720
и поэтому мы делаем min по всем

1074
00:38:08,720 --> 00:38:10,160
параметрам нашей теты, которые мы пытаемся

1075
00:38:10,160 --> 00:38:11,839
аппроксимировать min по параметрам

1076
00:38:11,839 --> 00:38:13,599
нашей нейронной сети наших потерь до обучения,

1077
00:38:13,599 --> 00:38:16,240
которые  здесь было языковое

1078
00:38:16,240 --> 00:38:18,960
моделирование наших наших параметров, и

1079
00:38:18,960 --> 00:38:20,720
давайте просто получим такую

1080
00:38:20,720 --> 00:38:23,680
оценку некоторых параметров тета-шляпа,

1081
00:38:23,680 --> 00:38:26,640
а затем мы точно настроим, аппроксимируя

1082
00:38:26,640 --> 00:38:30,000
эту минуту

1083
00:38:30,000 --> 00:38:32,240
по тэте тонкой настройки потери, может быть, это

1084
00:38:32,240 --> 00:38:34,000
правильное начало  в тэта-шляпе, поэтому мы

1085
00:38:34,000 --> 00:38:36,079
инициализируем наш градиентный спуск в тэта-

1086
00:38:36,079 --> 00:38:37,280
шляпе, а затем мы просто вроде как позволяем ей

1087
00:38:37,280 --> 00:38:38,480
делать то, что они хотят,

1088
00:38:38,480 --> 00:38:40,079
и как будто

1089
00:38:40,079 --> 00:38:43,119
это просто работает, а отчасти

1090
00:38:43,119 --> 00:38:45,839
это должно быть потому, что вы k  Теперь

1091
00:38:45,839 --> 00:38:47,839
кое-что о том, с чего мы начинаем, так

1092
00:38:47,839 --> 00:38:50,800
важно не только с точки зрения своего рода

1093
00:38:50,800 --> 00:38:52,400
градиентного потока, хотя это большая его

1094
00:38:52,400 --> 00:38:55,440
часть, но также похоже, что вы

1095
00:38:55,440 --> 00:38:57,920
знаете, что стохастический градиентный спуск придерживается

1096
00:38:57,920 --> 00:38:59,599
относительно близко

1097
00:38:59,599 --> 00:39:00,560
к

1098
00:39:00,560 --> 00:39:02,720
этой предварительной инициализации во время

1099
00:39:02,720 --> 00:39:04,800
точной настройки этого  это то, что мы,

1100
00:39:04,800 --> 00:39:06,800
кажется, правильно наблюдаем на практике,

1101
00:39:06,800 --> 00:39:08,560
что

1102
00:39:08,560 --> 00:39:10,400
каким-то образом локальность стохастического

1103
00:39:10,400 --> 00:39:12,079
градиентного спуска обнаруживает локальные минимумы,

1104
00:39:12,079 --> 00:39:13,839
близкие к этой тэта-шляпе, что

1105
00:39:13,839 --> 00:39:15,920
было хорошо для такой общей проблемы

1106
00:39:15,920 --> 00:39:17,280
языкового моделирования,

1107
00:39:17,280 --> 00:39:19,760
кажется, да, локальные

1108
00:39:19,760 --> 00:39:21,760
минимумы  потеря тонкой настройки, потому что мы не

1109
00:39:21,760 --> 00:39:23,440
находим, или да, локальный минимум

1110
00:39:23,440 --> 00:39:26,320
потери тонкой настройки имеет тенденцию хорошо обобщаться,

1111
00:39:26,320 --> 00:39:28,000
когда они находятся рядом с этой тета-шляпой, которую

1112
00:39:28,000 --> 00:39:29,200
мы предварительно обучили, и это своего рода

1113
00:39:29,200 --> 00:39:30,240
загадка, что мы  все еще пытаюсь

1114
00:39:30,240 --> 00:39:31,760
выяснить больше об

1115
00:39:31,760 --> 00:39:33,599
um, а затем также да, может быть,

1116
00:39:33,599 --> 00:39:35,040
градиенты прямо градиенты

1117
00:39:35,040 --> 00:39:36,720
потери точной настройки около тета распространяются

1118
00:39:36,720 --> 00:39:38,720
хорошо, поэтому наше сетевое обучение проходит

1119
00:39:38,720 --> 00:39:41,040
очень хорошо, хорошо,

1120
00:39:41,040 --> 00:39:43,760
хорошо, так что  это то, что нужно пережевывать,

1121
00:39:43,760 --> 00:39:46,000
но на практике это работает, я думаю, это все

1122
00:39:46,000 --> 00:39:49,359
еще интересно, что он работает

1123
00:39:49,440 --> 00:39:52,400
нормально, так

1124
00:39:52,400 --> 00:39:54,320
что мы говорили в основном о

1125
00:39:54,320 --> 00:39:56,480
декодере кодировщика трансформатора,

1126
00:39:56,480 --> 00:39:58,079
и

1127
00:39:58,079 --> 00:40:00,240
на самом деле я сказал, что мы могли бы

1128
00:40:00,240 --> 00:40:02,880
просто  левосторонние кодеры, которые

1129
00:40:02,880 --> 00:40:05,040
должны быть предварительно обучены, или просто

1130
00:40:05,040 --> 00:40:07,440
декодеры для предварительного обучения, или декодеры кодировщиков,

1131
00:40:07,440 --> 00:40:09,760
и на самом деле есть действительно

1132
00:40:09,760 --> 00:40:11,839
популярные типы известных моделей в каждой из

1133
00:40:11,839 --> 00:40:13,839
этих трех категорий, виды

1134
00:40:13,839 --> 00:40:15,680
предварительного обучения, которые вы можете выполнять,

1135
00:40:15,680 --> 00:40:18,319
и  виды

1136
00:40:18,319 --> 00:40:20,560
приложений или использование этих

1137
00:40:20,560 --> 00:40:23,200
предварительно обученных моделей, которые наиболее естественны, на

1138
00:40:23,200 --> 00:40:25,119
самом деле сильно зависят от того,

1139
00:40:25,119 --> 00:40:28,319
выберете ли вы предварительное обучение кодировщика декодера

1140
00:40:28,319 --> 00:40:31,280
или кодировщика-декодера,

1141
00:40:31,280 --> 00:40:32,720
и поэтому я думаю, что это полезно, когда мы рассмотрим

1142
00:40:32,720 --> 00:40:35,680
некоторые из этих популярных

1143
00:40:35,680 --> 00:40:37,280
своего рода названия моделей, которые вам нужно

1144
00:40:37,280 --> 00:40:40,079
знать, и что они собой представляют, каковы были их

1145
00:40:40,079 --> 00:40:41,680
инновации, чтобы на самом деле разделить их

1146
00:40:41,680 --> 00:40:44,079
на эти категории,

1147
00:40:44,079 --> 00:40:46,560
так что мы уже так вот что,

1148
00:40:46,560 --> 00:40:48,319
мы собираемся пройти через эти три,

1149
00:40:48,319 --> 00:40:50,640
и все они  есть своего рода преимущества и,

1150
00:40:50,640 --> 00:40:53,119
в некотором смысле, недостатки, так что

1151
00:40:53,119 --> 00:40:54,880
декодеры правы,

1152
00:40:54,880 --> 00:40:56,160
на самом

1153
00:40:56,160 --> 00:40:57,520
деле то, о чем мы здесь говорим, в

1154
00:40:57,520 --> 00:40:59,119
основном, это языковые модели, и мы видели

1155
00:40:59,119 --> 00:41:00,800
это до сих пор, мы говорили о

1156
00:41:00,800 --> 00:41:03,119
предварительно обученных декодерах, и это

1157
00:41:03,119 --> 00:41:04,640
приятно генерировать

1158
00:41:04,640 --> 00:41:06,400
справа, так что вы можете просто взять образец из своей

1159
00:41:06,400 --> 00:41:08,160
предварительно обученной языковой модели и получить

1160
00:41:08,160 --> 00:41:09,599
вещи, которые выглядят как текст, который

1161
00:41:09,599 --> 00:41:11,119
вы предварительно тренировали на

1162
00:41:11,119 --> 00:41:13,280
um, но вы знаете, что одна проблема заключается в том, что вы

1163
00:41:13,280 --> 00:41:15,280
не можете правильно определять будущие слова, поэтому

1164
00:41:15,280 --> 00:41:16,400
мы упоминали

1165
00:41:16,400 --> 00:41:19,440
в нашем моделировании с помощью lstms,

1166
00:41:19,440 --> 00:41:21,520
что вместо этого, если бы вы могли, когда вы могли

1167
00:41:21,520 --> 00:41:22,480
это сделать,

1168
00:41:22,480 --> 00:41:24,480
мы сказали, что наличие двунаправленного

1169
00:41:24,480 --> 00:41:27,280
lstm на самом деле намного полезнее,

1170
00:41:27,280 --> 00:41:29,200
чем наличие однонаправленного stm, ну,

1171
00:41:29,200 --> 00:41:31,119
это в некотором роде верно и для

1172
00:41:31,119 --> 00:41:32,720
трансформаторов  Итак,

1173
00:41:32,720 --> 00:41:34,319
если вы видите, как стрелки

1174
00:41:34,319 --> 00:41:36,640
указывают здесь, стрелки указывают

1175
00:41:36,640 --> 00:41:38,079
вверх, в то, что вы знаете справа, так что

1176
00:41:38,079 --> 00:41:40,240
это слово как бы оглядывается на его

1177
00:41:40,240 --> 00:41:42,560
прошлую историю,

1178
00:41:42,560 --> 00:41:46,960
но вы знаете, что это слово, э-э, не видите,

1179
00:41:46,960 --> 00:41:48,400
вы можете ''  t контекстуализировать с футу  re,

1180
00:41:48,400 --> 00:41:50,079
тогда как в блоке кодировщика здесь,

1181
00:41:50,079 --> 00:41:52,319
выделенном синим цветом, чуть ниже, у вас как бы есть все

1182
00:41:52,319 --> 00:41:54,800
пары взаимодействий, и поэтому вы знаете, что

1183
00:41:54,800 --> 00:41:56,160
когда вы создаете представления, на

1184
00:41:56,160 --> 00:41:57,440
самом деле может быть очень полезно знать,

1185
00:41:57,440 --> 00:41:59,280
каковы будут будущие слова, так

1186
00:41:59,280 --> 00:42:00,800
что кодировщики дают вам  правильно, вы получаете

1187
00:42:00,800 --> 00:42:02,640
двунаправленный контекст,

1188
00:42:02,640 --> 00:42:04,160
чтобы вы могли ориентироваться на будущее, возможно,

1189
00:42:04,160 --> 00:42:05,200
это поможет вам создать лучшие

1190
00:42:05,200 --> 00:42:07,359
представления о языке,

1191
00:42:07,359 --> 00:42:09,119
но вопрос, который мы на самом деле рассмотрим

1192
00:42:09,119 --> 00:42:10,560
здесь, хорошо, как вы

1193
00:42:10,560 --> 00:42:11,839
их предварительно обучите,

1194
00:42:11,839 --> 00:42:13,280
вы не можете заранее  торгуйте ими как языковыми

1195
00:42:13,280 --> 00:42:14,720
моделями, потому что у вас есть доступ к

1196
00:42:14,720 --> 00:42:17,440
будущему, поэтому, если вы попытаетесь сделать это,

1197
00:42:17,440 --> 00:42:19,520
потери сразу же будут равны нулю, потому что

1198
00:42:19,520 --> 00:42:20,880
вы можете просто увидеть, какое

1199
00:42:20,880 --> 00:42:23,119
будущее будет бесполезным, а затем

1200
00:42:23,119 --> 00:42:24,560
мы поговорим о предварительном  обученные кодировщики-

1201
00:42:24,560 --> 00:42:26,000
декодеры,

1202
00:42:26,000 --> 00:42:27,040
которым нравится,

1203
00:42:27,040 --> 00:42:29,760
может быть, лучшее из обоих миров, но

1204
00:42:29,760 --> 00:42:31,680
также, возможно, неясно, как лучше всего

1205
00:42:31,680 --> 00:42:33,040
их предварительно обучить, у

1206
00:42:33,040 --> 00:42:36,000
них определенно есть преимущества для

1207
00:42:36,000 --> 00:42:37,280
обоих, так что

1208
00:42:37,280 --> 00:42:40,160
давайте перейдем к некоторой общей

1209
00:42:40,160 --> 00:42:40,960
вершине,

1210
00:42:40,960 --> 00:42:42,560
например, более да,

1211
00:42:42,560 --> 00:42:43,920
сначала займемся декодерами, мы пройдем через все

1212
00:42:43,920 --> 00:42:45,920
три,

1213
00:42:45,920 --> 00:42:48,560
хорошо, так что,

1214
00:42:50,240 --> 00:42:52,480
когда мы предварительно обучаем языковую модель,

1215
00:42:52,480 --> 00:42:53,920
мы создаем ее для этой

1216
00:42:53,920 --> 00:42:55,440
цели, мы пытаемся

1217
00:42:55,440 --> 00:42:57,839
приблизить эту вероятность  слово,

1218
00:42:57,839 --> 00:43:00,240
учитывая все его предыдущие слова,

1219
00:43:01,119 --> 00:43:02,720
что мы в итоге делаем, и я показал

1220
00:43:02,720 --> 00:43:04,160
это пиктографически, давайте добавим немного

1221
00:43:04,160 --> 00:43:06,400
математики правильно, мы получим скрытое состояние от

1222
00:43:06,400 --> 00:43:10,240
h1 до ht для каждого из слов на

1223
00:43:10,240 --> 00:43:12,560
входе от w1 до wt теперь запомните слова снова

1224
00:43:12,560 --> 00:43:14,240
означает  дополнительные слова здесь,

1225
00:43:14,240 --> 00:43:16,160
хорошо, а затем, когда мы настраиваем

1226
00:43:16,160 --> 00:43:17,839
это правильно, мы можем взять

1227
00:43:17,839 --> 00:43:20,800
представление, которое должно быть htaht

1228
00:43:20,800 --> 00:43:22,480
плюс b,

1229
00:43:22,480 --> 00:43:24,079
а затем изображение здесь прямо

1230
00:43:24,079 --> 00:43:27,599
здесь ht это самое последнее

1231
00:43:27,599 --> 00:43:28,640
состояние

1232
00:43:28,640 --> 00:43:31,280
кодировщика, а теперь это вроде как  Представьте, он

1233
00:43:31,280 --> 00:43:33,119
видел всю свою историю

1234
00:43:33,119 --> 00:43:35,520
правильно, и поэтому

1235
00:43:35,520 --> 00:43:38,480
вы можете применить здесь линейный слой, возможно,

1236
00:43:38,480 --> 00:43:40,400
умножив его на некоторые параметры a и

1237
00:43:40,400 --> 00:43:42,800
b, которые не были предварительно обучены,

1238
00:43:42,800 --> 00:43:44,079
а затем вы прогнозируете настроение,

1239
00:43:44,079 --> 00:43:46,560
может быть, вы знаете, что положительное или отрицательное настроение,

1240
00:43:46,560 --> 00:43:48,960
а, возможно, и  ну ты знаешь, посмотри на

1241
00:43:48,960 --> 00:43:50,240
красный и серый, поэтому большинство

1242
00:43:50,240 --> 00:43:51,760
параметров в моей нейронной сети теперь

1243
00:43:51,760 --> 00:43:54,400
были предварительно обучены, самый последний уровень,

1244
00:43:54,400 --> 00:43:56,560
который изучает настроения,

1245
00:43:56,560 --> 00:43:58,240
скажем,

1246
00:43:58,240 --> 00:44:00,240
решение, это не было предварительно обучено, поэтому

1247
00:44:00,240 --> 00:44:02,079
они были инициализированы случайным образом, и

1248
00:44:02,079 --> 00:44:03,839
когда вы, когда  вы принимаете потерю

1249
00:44:03,839 --> 00:44:06,319
потери настроения, вы тренируете здесь не

1250
00:44:06,319 --> 00:44:08,319
только линейный слой, но вы фактически

1251
00:44:08,319 --> 00:44:10,480
обратно распространяете градиенты

1252
00:44:10,480 --> 00:44:12,960
по всей предварительно обученной сети

1253
00:44:12,960 --> 00:44:16,000
и точно настраиваете все эти параметры

1254
00:44:16,000 --> 00:44:17,200
правильно, так что это не похоже на то, что вы  просто

1255
00:44:17,200 --> 00:44:19,200
тренируя это во время точной настройки этого

1256
00:44:19,200 --> 00:44:20,960
линейного слоя, вы тренируете всю

1257
00:44:20,960 --> 00:44:22,400
сеть в зависимости от этой

1258
00:44:22,400 --> 00:44:25,280
потери точной настройки,

1259
00:44:25,520 --> 00:44:27,440
и вы знаете, что, возможно, это плохо, что,

1260
00:44:27,440 --> 00:44:29,920
как и линейный слой, не был предварительно обучен

1261
00:44:29,920 --> 00:44:31,440
по общей схеме вещей, он  не

1262
00:44:31,440 --> 00:44:32,960
так много параметров,

1263
00:44:34,400 --> 00:44:36,079
так что это полезно, это всего лишь один

1264
00:44:36,079 --> 00:44:38,240
способ правильно взаимодействовать с предварительно обученными моделями,

1265
00:44:38,240 --> 00:44:39,520
и поэтому

1266
00:44:39,520 --> 00:44:40,880
я хочу, чтобы вы вынесли из этого

1267
00:44:40,880 --> 00:44:42,640
то, что у нас был контракт

1268
00:44:42,640 --> 00:44:44,560
с исходной моделью.

1269
00:44:44,560 --> 00:44:46,240
Контракт заключался в том, что он определял

1270
00:44:46,240 --> 00:44:48,400
распределения вероятностей,

1271
00:44:48,400 --> 00:44:49,760
но когда мы тонко настраиваем, когда мы

1272
00:44:49,760 --> 00:44:51,440
взаимодействуем с предварительно обученной моделью,

1273
00:44:51,440 --> 00:44:53,200
то, что у нас также есть, точно такие же, как

1274
00:44:53,200 --> 00:44:54,480
обученные веса и сетевая

1275
00:44:54,480 --> 00:44:56,560
архитектура, которые нам не нужно использовать  это

1276
00:44:56,560 --> 00:44:58,000
как языковая модель, нам не нужно использовать ее

1277
00:44:58,000 --> 00:44:59,839
в качестве распределения вероятностей, когда мы на

1278
00:44:59,839 --> 00:45:01,760
самом деле ее настраиваем, мы на самом деле

1279
00:45:01,760 --> 00:45:04,160
просто используем ее для инициализации

1280
00:45:04,160 --> 00:45:05,680
ее параметров и говорим, что это

1281
00:45:06,560 --> 00:45:08,640
просто декодер-преобразователь, который был

1282
00:45:08,640 --> 00:45:10,400
предварительно обученный,

1283
00:45:10,400 --> 00:45:12,400
и это действительно здорово в

1284
00:45:12,400 --> 00:45:14,319
том смысле, что, когда вы обнаруживаете тунца на некоторых из

1285
00:45:14,319 --> 00:45:15,760
некоторых данных настроения, он действительно

1286
00:45:17,680 --> 00:45:18,880
хорошо работает, но есть второй способ

1287
00:45:18,880 --> 00:45:20,560
взаимодействия с декодером с

1288
00:45:20,560 --> 00:45:22,240
предварительно обученными декодерами, который есть в некоторых

1289
00:45:22,240 --> 00:45:24,800
В смысле даже более естественным, на самом деле это

1290
00:45:24,800 --> 00:45:27,280
ближе к контракту, с которого мы начали,

1291
00:45:27,280 --> 00:45:30,079
поэтому нам не нужно просто игнорировать тот

1292
00:45:30,079 --> 00:45:31,119
факт, что это было распределение вероятностей,

1293
00:45:31,119 --> 00:45:33,920
мы можем использовать

1294
00:45:33,920 --> 00:45:35,520
его, все еще настраивая его, так что вот

1295
00:45:35,520 --> 00:45:37,119
что мы  собираюсь т  для

1296
00:45:37,119 --> 00:45:39,359
этого мы можем использовать их в качестве генератора во время

1297
00:45:39,359 --> 00:45:41,599
точной настройки генератором, я имею в

1298
00:45:41,599 --> 00:45:43,440
виду, что вы узнаете, что определите это

1299
00:45:43,440 --> 00:45:45,119
распределение слов с учетом их

1300
00:45:45,119 --> 00:45:47,119
контекста,

1301
00:45:47,119 --> 00:45:49,280
а затем мы на самом деле просто точно настроим

1302
00:45:49,280 --> 00:45:51,680
это распределение вероятностей,

1303
00:45:51,680 --> 00:45:54,480
поэтому в такой задаче, как di  как какой-то

1304
00:45:54,480 --> 00:45:57,200
пошаговый диалог, мы можем закодировать

1305
00:45:57,200 --> 00:46:01,440
историю диалога как ваш прошлый контекст,

1306
00:46:01,440 --> 00:46:03,359
чтобы вы знали, что у вас есть

1307
00:46:03,359 --> 00:46:05,359
история диалога некоторых

1308
00:46:05,359 --> 00:46:06,480
вещей, которые люди говорят

1309
00:46:06,480 --> 00:46:08,000
друг другу, вы кодируете это как

1310
00:46:08,000 --> 00:46:10,000
слова и пытаетесь предсказать  следующие

1311
00:46:10,000 --> 00:46:11,680
слова в диалоге

1312
00:46:11,680 --> 00:46:12,880
правильно и, возможно, ваша цель перед тренировкой

1313
00:46:12,880 --> 00:46:14,800
вы просмотрели текст очень общего

1314
00:46:14,800 --> 00:46:17,040
назначения из я не знаю википедии,

1315
00:46:17,040 --> 00:46:18,560
книг или чего-то еще, и вы

1316
00:46:18,560 --> 00:46:20,720
настраиваете его как языковую модель, но у

1317
00:46:20,720 --> 00:46:21,839
вас все в порядке ...  настройка его в качестве языковой

1318
00:46:21,839 --> 00:46:23,359
модели

1319
00:46:23,359 --> 00:46:25,760
для такого рода предметно-ориентированного

1320
00:46:25,760 --> 00:46:27,839
распределения текста, такого как диалог или,

1321
00:46:27,839 --> 00:46:29,839
возможно, резюмирование, когда вы

1322
00:46:29,839 --> 00:46:33,040
вставляете весь документ, а затем

1323
00:46:33,040 --> 00:46:35,839
произносите определенное слово, а затем резюме и

1324
00:46:35,839 --> 00:46:37,920
говорите предсказать сумму  Мэри, и так,

1325
00:46:37,920 --> 00:46:39,839
как это выглядит, снова время точной

1326
00:46:39,839 --> 00:46:42,079
настройки здесь у

1327
00:46:42,079 --> 00:46:44,319
вас есть ваш h1 to ht, равный

1328
00:46:44,319 --> 00:46:47,359
декодеру слов, а затем у вас

1329
00:46:47,359 --> 00:46:49,040
есть этот дистрибутив, который вы

1330
00:46:49,040 --> 00:46:53,200
настраиваете wt's ah - это снова опечатка

1331
00:46:53,200 --> 00:46:56,880
ht  минус 1 плюс b, так что теперь каждый раз, когда у меня

1332
00:46:56,880 --> 00:46:58,000
есть это

1333
00:46:58,000 --> 00:47:00,079
um, я предсказываю эти слова по первому слову,

1334
00:47:00,079 --> 00:47:01,920
я предсказываю слово два или два, предсказываю

1335
00:47:01,920 --> 00:47:05,040
слово три и т.д.

1336
00:47:08,880 --> 00:47:10,960
-подготовлен, но я все еще тонко

1337
00:47:10,960 --> 00:47:12,160
настраиваю все

1338
00:47:12,160 --> 00:47:14,880
правильно, поэтому a и b здесь отображают, чтобы

1339
00:47:14,880 --> 00:47:16,640
вроде как распределение вероятностей по

1340
00:47:16,640 --> 00:47:18,400
моему словарю

1341
00:47:18,400 --> 00:47:19,520
или логитам распределения вероятностей,

1342
00:47:19,520 --> 00:47:22,079
и гм, и я просто могу

1343
00:47:22,079 --> 00:47:23,920
вроде как настроить их сейчас

1344
00:47:23,920 --> 00:47:25,440
чтобы дистрибутив, который я собираюсь

1345
00:47:25,440 --> 00:47:28,400
использовать, отражал такую вещь, как диалог, который я

1346
00:47:28,400 --> 00:47:31,280
хочу, чтобы он отражал

1347
00:47:32,480 --> 00:47:33,760
нормально, так что это два способа

1348
00:47:33,760 --> 00:47:36,720
взаимодействия с предварительно обученным декодером,

1349
00:47:36,720 --> 00:47:39,359
вот пример того, что в

1350
00:47:39,359 --> 00:47:42,240
итоге оказалось первым  линии безумного

1351
00:47:43,440 --> 00:47:46,559
успеха  Я или, по крайней мере, говорил о

1352
00:47:46,559 --> 00:47:48,880
предварительно обученных декодерах,

1353
00:47:48,880 --> 00:47:51,359
поэтому генеративный предварительно обученный декодер или

1354
00:47:51,359 --> 00:47:53,839
gpt

1355
00:47:54,400 --> 00:47:56,160
имел огромный успех

1356
00:47:56,160 --> 00:47:58,480
в каком-то смысле, или, по крайней мере, он получил

1357
00:47:58,480 --> 00:48:00,640
много шума, так что это

1358
00:48:00,640 --> 00:48:03,680
трансформаторный декодер без кодировщика с 12

1359
00:48:03,680 --> 00:48:05,760
слоями  Я даю вам детали, чтобы вы

1360
00:48:05,760 --> 00:48:07,839
могли начать

1361
00:48:07,839 --> 00:48:09,760
понимать, как размер вещей

1362
00:48:09,760 --> 00:48:12,240
меняется с годами, поскольку мы продолжим

1363
00:48:12,240 --> 00:48:13,359
прогрессировать здесь,

1364
00:48:13,359 --> 00:48:15,040
если каждое из наших скрытых

1365
00:48:15,040 --> 00:48:18,000
состояний имеет размерность 768.

1366
00:48:18,000 --> 00:48:19,520
Итак, если вы  Вспомните, в прошлой лекции

1367
00:48:19,520 --> 00:48:21,839
у нас был термин d, который был нашей

1368
00:48:21,839 --> 00:48:24,559
размерностью, поэтому d равно

1369
00:48:24,559 --> 00:48:25,839
768. А затем

1370
00:48:25,839 --> 00:48:27,119
интересное утверждение, которое вы должны

1371
00:48:27,119 --> 00:48:29,200
иметь в виду для тех, кто разбирается в технике,

1372
00:48:29,200 --> 00:48:31,040
заключается в том, что на самом деле

1373
00:48:31,040 --> 00:48:32,319
слои прямой связи у вас есть  скрытый слой

1374
00:48:32,319 --> 00:48:33,920
в фиде извините меня в

1375
00:48:33,920 --> 00:48:35,760
слое с прямой связью, и он был на самом деле очень

1376
00:48:35,760 --> 00:48:37,599
большим, поэтому у вас были такие похожие по

1377
00:48:37,599 --> 00:48:40,000
положению слои с прямой связью справа

1378
00:48:40,000 --> 00:48:41,200
и гм,

1379
00:48:41,200 --> 00:48:43,119
а слой с прямой связью принял

1380
00:48:43,119 --> 00:48:45,520
бы 768-мерный векторный вид вроде как

1381
00:48:45,520 --> 00:48:48,480
проецировать его  до 3000 дн.  пространственное пространство создает

1382
00:48:48,480 --> 00:48:50,319
своего рода нелинейность, а затем

1383
00:48:50,319 --> 00:48:52,640
проецирует ее обратно на 768. Это в конечном

1384
00:48:52,640 --> 00:48:55,040
итоге потому, что вы можете сжать гораздо

1385
00:48:55,040 --> 00:48:57,119
больше параметров для не слишком большого количества

1386
00:48:57,119 --> 00:48:59,040
вычислений таким образом, но это

1387
00:48:59,040 --> 00:49:01,119
любопытно,

1388
00:49:01,119 --> 00:49:03,680
хорошо, тогда, а затем  путем парного кодирования

1389
00:49:03,680 --> 00:49:05,040
это на самом деле

1390
00:49:05,040 --> 00:49:06,240
было это кодирование печати было

1391
00:49:06,240 --> 00:49:09,119
словарём подслов с 40 000 слияний, поэтому

1392
00:49:09,119 --> 00:49:10,880
40 000 слияний, так что это не

1393
00:49:10,880 --> 00:49:13,040
размер словаря, потому что вы начали

1394
00:49:13,040 --> 00:49:15,040
с кучи символов, я не помню, с

1395
00:49:15,040 --> 00:49:17,119
какого количества символов они начали

1396
00:49:17,119 --> 00:49:18,640
но так что это относительно небольшой

1397
00:49:18,640 --> 00:49:22,160
словарный запас, который вы можете увидеть правильно, э-э,

1398
00:49:22,160 --> 00:49:24,000
по сравнению с тем, если бы вы пытались сказать, чтобы

1399
00:49:24,000 --> 00:49:25,760
каждое слово имело уникальное

1400
00:49:25,760 --> 00:49:27,520
представление, теперь оно будет

1401
00:49:27,520 --> 00:49:30,000
обучено на книгах, корпорациях, у него есть 000

1402
00:49:30,000 --> 00:49:31,520
уникальных книг,

1403
00:49:31,520 --> 00:49:33,839
и он содержит длинные промежутки смежных

1404
00:49:33,839 --> 00:49:36,720
text, поэтому вместо того, чтобы говорить,

1405
00:49:36,720 --> 00:49:38,559
тренировать его на отдельных предложениях, просто на

1406
00:49:38,559 --> 00:49:41,200
небольших коротких предложениях, модель

1407
00:49:41,200 --> 00:49:43,599
может изучать зависимости на большом расстоянии,

1408
00:49:43,599 --> 00:49:45,920
потому что вы не разбили, как книга

1409
00:49:45,920 --> 00:49:47,680
int  o случайные предложения и перемешивание их

1410
00:49:47,680 --> 00:49:49,280
всех вокруг, где вы были, вы как бы

1411
00:49:49,280 --> 00:49:51,599
сохраняли его непрерывным, чтобы он мог иметь

1412
00:49:51,599 --> 00:49:53,760
такую последовательность,

1413
00:49:53,760 --> 00:49:55,520
а затем немного, э-э, здесь

1414
00:49:55,520 --> 00:49:57,359
да, так что gpt никогда не появлялся в

1415
00:49:57,359 --> 00:49:59,599
исходной статье или исходном блоге  пост

1416
00:49:59,599 --> 00:50:01,520
как аббревиатура, и на самом деле это может

1417
00:50:01,520 --> 00:50:03,599
относиться к генеративному

1418
00:50:03,599 --> 00:50:05,599
предварительному обучению, что вроде того,

1419
00:50:05,599 --> 00:50:07,119
что предлагает название статьи, или

1420
00:50:07,119 --> 00:50:09,200
генеративному предварительно обученному преобразователю,

1421
00:50:09,200 --> 00:50:10,640
и я вроде как решил сказать

1422
00:50:10,640 --> 00:50:12,000
генеративный предварительно обученный трансформатор

1423
00:50:12,000 --> 00:50:15,119
потому что это казалось слишком общим,

1424
00:50:15,119 --> 00:50:17,599
так что

1425
00:50:17,599 --> 00:50:20,720
ладно, так что они предварительно обучили

1426
00:50:20,720 --> 00:50:22,160
этот огромный преобразователь языковой модели, этот огромный

1427
00:50:22,160 --> 00:50:25,280
декодер-преобразователь всего на 7000 книг,

1428
00:50:25,280 --> 00:50:27,040
и они настроили его для ряда

1429
00:50:27,040 --> 00:50:28,319
различных задач, и я хочу немного поговорить

1430
00:50:28,319 --> 00:50:29,520
о деталях того, как

1431
00:50:29,520 --> 00:50:31,440
они его настраивали,

1432
00:50:31,440 --> 00:50:32,240
и

1433
00:50:32,240 --> 00:50:33,920
поэтому они настраивали его для одной конкретной

1434
00:50:33,920 --> 00:50:34,880
задачи

1435
00:50:34,880 --> 00:50:36,319
или семейства задач, называемых

1436
00:50:36,319 --> 00:50:38,000
логическим выводом

1437
00:50:38,720 --> 00:50:39,920
на естественном языке, поэтому при логическом выводе на естественном языке мы

1438
00:50:39,920 --> 00:50:42,559
маркируем пары предложений  они влекут за собой

1439
00:50:42,559 --> 00:50:43,839
или противоречат друг другу в

1440
00:50:43,839 --> 00:50:46,000
нейтральном смысле, поэтому у вас есть предпосылка, и вы

1441
00:50:46,000 --> 00:50:48,400
придерживаетесь этой предпосылки как своего рода правду, что человек

1442
00:50:48,400 --> 00:50:49,920
находится в дверном проеме,

1443
00:50:49,920 --> 00:50:52,319
и у вас есть гипотеза, что человек находится

1444
00:50:52,319 --> 00:50:53,760
рядом с дверью,

1445
00:50:53,760 --> 00:50:56,800
если этот человек имеет в виду этого человека

1446
00:50:56,800 --> 00:50:58,800
э-э  тогда вы знаете, что это вроде как о

1447
00:50:58,800 --> 00:51:01,520
да, это как бы влечет за собой, потому что

1448
00:51:01,520 --> 00:51:02,880
есть человек, потому что мужчина - это

1449
00:51:02,880 --> 00:51:04,880
личность, и они в дверном проеме, тогда

1450
00:51:04,880 --> 00:51:06,319
они рядом с дверью,

1451
00:51:06,319 --> 00:51:08,319
так что у вас есть такого рода логические

1452
00:51:08,319 --> 00:51:10,880
рассуждения, что вы  делаю э-э, или вы

1453
00:51:10,880 --> 00:51:12,720
должны уметь делать, и

1454
00:51:12,720 --> 00:51:14,160
вы маркируете эти предложения, так что

1455
00:51:14,160 --> 00:51:15,359
это помеченная

1456
00:51:15,359 --> 00:51:17,839
задача, у вас есть своего рода ввод, который

1457
00:51:17,839 --> 00:51:19,760
разрезан на две части, а затем один из трех

1458
00:51:19,760 --> 00:51:20,960
выходов

1459
00:51:20,960 --> 00:51:22,400
хорошо, так

1460
00:51:22,400 --> 00:51:24,880
что gpt  paper оценивает эту задачу,

1461
00:51:24,880 --> 00:51:26,319
но то, что у них есть, это

1462
00:51:26,319 --> 00:51:28,559
декодер-преобразователь, так что они делают, что

1463
00:51:28,559 --> 00:51:30,400
они делают ...

1464
00:51:39,280 --> 00:51:41,440
какую задачу ты

1465
00:51:42,400 --> 00:51:45,040
делаешь  Мы собираемся просто отформатировать задачу

1466
00:51:45,040 --> 00:51:47,040
как кучу токенов

1467
00:51:47,040 --> 00:51:49,520
и не менять вашу архитектуру,

1468
00:51:49,520 --> 00:51:51,839
потому что предварительное обучение было настолько полезным,

1469
00:51:51,839 --> 00:51:53,040
что, вероятно, лучше сохранить

1470
00:51:53,040 --> 00:51:56,160
фиксированную архитектуру перед обучением, а затем

1471
00:51:56,160 --> 00:51:58,960
изменить спецификацию задачи, чтобы она

1472
00:51:58,960 --> 00:52:00,480
соответствовала предварительному  -обученная архитектура, так что

1473
00:52:00,480 --> 00:52:03,200
они сделали правильно, они положили этот токен на начало,

1474
00:52:03,200 --> 00:52:04,880
это особый токен,

1475
00:52:04,880 --> 00:52:08,319
человек находится в дверном проеме, какой-то

1476
00:52:08,319 --> 00:52:09,760
токен-разделитель справа, так что это просто линейная

1477
00:52:09,760 --> 00:52:11,599
последовательность токенов, которую мы даем как

1478
00:52:11,599 --> 00:52:14,160
один большой префикс для gpt

1479
00:52:14,160 --> 00:52:17,119
um и  затем человек находится возле двери,

1480
00:52:17,119 --> 00:52:19,680
а затем какой-то дополнительный токен здесь справа,

1481
00:52:19,680 --> 00:52:21,359
извлеките,

1482
00:52:21,359 --> 00:52:23,599
а затем вы знаете линейный классификатор,

1483
00:52:23,599 --> 00:52:25,359
о котором мы говорили, и своего рода

1484
00:52:25,359 --> 00:52:28,000
первый способ взаимодействия с

1485
00:52:28,000 --> 00:52:30,880
моделями с моделями декодера, который применяется

1486
00:52:30,880 --> 00:52:33,760
к представлению

1487
00:52:33,760 --> 00:52:35,599
токена извлечения  Итак, у вас есть последнее

1488
00:52:35,599 --> 00:52:38,079
скрытое состояние поверх извлечения, а затем

1489
00:52:38,079 --> 00:52:39,839
вы настраиваете всю сеть, чтобы

1490
00:52:39,839 --> 00:52:42,079
правильно предсказать эти метки, и поэтому этот

1491
00:52:42,079 --> 00:52:45,599
вид форматирования ввода все

1492
00:52:45,599 --> 00:52:48,000
чаще используется  d, чтобы

1493
00:52:48,000 --> 00:52:50,240
архитектура модели оставалась неизменной и

1494
00:52:50,240 --> 00:52:51,839
позволяла решать множество различных проблем,

1495
00:52:51,839 --> 00:52:53,839
с ней

1496
00:52:53,839 --> 00:52:55,599
все в порядке, и так ли это работало с

1497
00:52:55,599 --> 00:52:58,240
логическим выводом на естественном языке, ответ - да,

1498
00:52:58,240 --> 00:52:59,520
так что здесь есть несколько разных чисел,

1499
00:52:59,520 --> 00:53:01,440
я бы не стал беспокоиться  слишком много об

1500
00:53:01,440 --> 00:53:02,960
этом отлаженная языковая модель-трансформер

1501
00:53:02,960 --> 00:53:05,280
- это своего рода то, на что вы должны обратить

1502
00:53:05,280 --> 00:53:07,440
внимание э-э, есть много усилий,

1503
00:53:07,440 --> 00:53:09,440
которые были правильно вложены в другие модели,

1504
00:53:09,440 --> 00:53:10,640
и это как бы история о том, как

1505
00:53:10,640 --> 00:53:12,400
люди, прошедшие предварительное обучение, приложили много усилий

1506
00:53:12,400 --> 00:53:13,920
в модели, которые делают различного рода

1507
00:53:13,920 --> 00:53:16,240
осторожные вещи, а затем вы берете один-

1508
00:53:16,240 --> 00:53:18,960
единственный трансформатор и говорите, что

1509
00:53:18,960 --> 00:53:20,400
я собираюсь предварительно обучить его на тонне

1510
00:53:20,400 --> 00:53:22,319
текста и не слишком беспокоиться

1511
00:53:22,319 --> 00:53:24,160
ни о чем другом, просто настройте его, и

1512
00:53:24,160 --> 00:53:27,680
вы закончите  делать супер супер хорошо

1513
00:53:27,760 --> 00:53:30,160
иногда не намного лучше в

1514
00:53:30,160 --> 00:53:31,680
случае gpt, чем некоторые

1515
00:53:31,680 --> 00:53:34,319
из самых известных современных методов, но

1516
00:53:34,319 --> 00:53:36,400
обычно немного лучше и

1517
00:53:36,400 --> 00:53:38,160
снова количество усилий количество

1518
00:53:38,160 --> 00:53:39,680
конкретных усилий, которые вы должны  вложить

1519
00:53:39,680 --> 00:53:42,559
в это очень  низкий,

1520
00:53:42,559 --> 00:53:44,960
хорошо, а как насчет другого способа

1521
00:53:44,960 --> 00:53:46,400
взаимодействия с декодерами? Итак,

1522
00:53:46,400 --> 00:53:47,599
мы сказали, что мы можем взаимодействовать

1523
00:53:47,599 --> 00:53:49,359
с кодерами, просто выбирая из них,

1524
00:53:49,359 --> 00:53:50,720
просто говоря, что это вероятностные

1525
00:53:50,720 --> 00:53:53,280
распределения, так что мы можем использовать их в

1526
00:53:53,280 --> 00:53:55,520
их качестве как  языковые модели и

1527
00:53:55,520 --> 00:53:58,720
gpt2, на самом деле это просто более

1528
00:53:58,720 --> 00:54:00,400
крупный

1529
00:54:00,400 --> 00:54:02,079
gpt, не беспокойтесь об этом

1530
00:54:02,079 --> 00:54:04,960
с большими скрытыми единицами, большим количеством слоев,

1531
00:54:04,960 --> 00:54:06,800
когда он был обучен на большем количестве данных, было

1532
00:54:06,800 --> 00:54:09,839
показано, что он создает своего рода относительно

1533
00:54:09,839 --> 00:54:11,760
убедительные образцы естественного языка,

1534
00:54:11,760 --> 00:54:13,680
так что это  это то, что часто ходило в

1535
00:54:13,680 --> 00:54:15,119
твиттере, так что у вас есть

1536
00:54:15,119 --> 00:54:18,079
такой надуманный пример, который, вероятно,

1537
00:54:18,079 --> 00:54:19,440
не отображался

1538
00:54:19,440 --> 00:54:21,760
в данных обучения, где вы знаете, что

1539
00:54:21,760 --> 00:54:24,640
ученый обнаруживает стадо единорогов,

1540
00:54:24,640 --> 00:54:27,760
а затем они вроде как образец из

1541
00:54:28,720 --> 00:54:29,839
э-э,

1542
00:54:29,839 --> 00:54:31,440
почти  распределение модели

1543
00:54:31,440 --> 00:54:34,480
они вроде как придают модели немного ээээ,

1544
00:54:34,480 --> 00:54:36,720
некоторую заслугу здесь

1545
00:54:36,720 --> 00:54:38,640
они делают что-то, называемое усечением

1546
00:54:38,640 --> 00:54:40,400
распределения языковых моделей, чтобы

1547
00:54:40,400 --> 00:54:41,680
своего рода вырезать

1548
00:54:41,680 --> 00:54:45,280
шум  в gpt2, так что это не

1549
00:54:45,280 --> 00:54:49,040
совсем идеальный образец, но более или менее gpt2

1550
00:54:49,040 --> 00:54:51,839
сгенерировал это,

1551
00:54:51,839 --> 00:54:53,040
и поэтому у вас есть ученые,

1552
00:54:53,040 --> 00:54:55,599
открывающие единорогов, а затем вы знаете,

1553
00:54:55,599 --> 00:54:57,599
что у вас есть эта последовательность, хорошо,

1554
00:54:57,599 --> 00:54:58,960
есть ученый,

1555
00:54:58,960 --> 00:55:01,599
а вы знаете, что они дают ему

1556
00:55:01,599 --> 00:55:05,200
имя ммм вы  Вы обращаетесь к

1557
00:55:05,200 --> 00:55:07,200
этому, ну

1558
00:55:08,000 --> 00:55:09,680
да, вы ссылаетесь на имя ученого, у

1559
00:55:09,680 --> 00:55:11,920
вас вроде есть такие

1560
00:55:11,920 --> 00:55:14,640
вещи, как согласованность темы, а также

1561
00:55:14,640 --> 00:55:17,200
синтаксис действительно хорош, ну, похоже, вы знаете

1562
00:55:17,200 --> 00:55:18,559
смутно как английский,

1563
00:55:18,559 --> 00:55:20,000
и поэтому

1564
00:55:20,000 --> 00:55:21,280
тенденция по мере того, как мы становимся все больше и больше

1565
00:55:21,280 --> 00:55:22,880
языковых моделей, которые мы фактически выбираем из

1566
00:55:22,880 --> 00:55:24,720
них, даже когда мы даем им подсказки, которые

1567
00:55:24,720 --> 00:55:27,280
выглядят немного странно, и они кажутся

1568
00:55:27,280 --> 00:55:29,680
все более убедительными,

1569
00:55:29,680 --> 00:55:31,440
хорошо,

1570
00:55:31,440 --> 00:55:33,040
так что

1571
00:55:33,040 --> 00:55:35,440
кодировщики с предварительным обучением

1572
00:55:35,440 --> 00:55:38,480
хорошо, кодеры с предварительным обучением давайте

1573
00:55:38,480 --> 00:55:39,920
возьмем еще секунду  потому что мне нужно

1574
00:55:39,920 --> 00:55:41,920
немного воды здесь,

1575
00:55:41,920 --> 00:55:46,040
если есть другой вопрос, дайте мне знать,

1576
00:55:50,160 --> 00:55:52,400
хорошо,

1577
00:55:53,200 --> 00:55:55,920
так что преимущество кодировщиков,

1578
00:55:55,920 --> 00:55:57,359
о которых мы говорили, заключалось в том, что они получают

1579
00:55:57,359 --> 00:55:59,520
этот двунаправленный контекст  так что вы можете,

1580
00:55:59,520 --> 00:56:00,839
пока вы создаете

1581
00:56:00,839 --> 00:56:03,280
представления своего

1582
00:56:03,280 --> 00:56:04,880
предложения из своих частей предложений, вы

1583
00:56:04,880 --> 00:56:06,640
можете смотреть в будущее, и это может помочь

1584
00:56:06,640 --> 00:56:08,160
вам построить лучшее представление

1585
00:56:08,160 --> 00:56:10,079
слова, на которое вы смотрите прямо сейчас,

1586
00:56:10,079 --> 00:56:11,920
но большая проблема в том, что мы  Сейчас мы не можем заниматься

1587
00:56:11,920 --> 00:56:13,280
языковым моделированием, поэтому мы в значительной

1588
00:56:13,280 --> 00:56:15,760
степени только сказали, что полагались на эту

1589
00:56:15,760 --> 00:56:17,680
задачу, что мы уже знали о языковом

1590
00:56:17,680 --> 00:56:19,599
моделировании, чтобы провести предварительное обучение, но теперь

1591
00:56:19,599 --> 00:56:22,640
мы хотим предварительно обучить кодировщики, и поэтому мы

1592
00:56:22,640 --> 00:56:23,920
можем '  Мы не можем его использовать, так что мы

1593
00:56:23,920 --> 00:56:25,680
собираемся делать

1594
00:56:26,960 --> 00:56:28,640
вот это решение, которое было предложено

1595
00:56:28,640 --> 00:56:30,480
в

1596
00:56:30,480 --> 00:56:32,160
статье, в которой был представлен языковой мод

1597
00:56:33,119 --> 00:56:34,640
модели под названием burt,

1598
00:56:34,640 --> 00:56:37,119
это называется моделированием замаскированного языка, так что

1599
00:56:38,079 --> 00:56:40,559
вот идея, которую мы получаем  предложение,

1600
00:56:40,559 --> 00:56:42,160
а затем мы просто берем часть

1601
00:56:42,160 --> 00:56:44,960
слов и заменяем их своего рода

1602
00:56:44,960 --> 00:56:48,000
токеном маски, токеном, который

1603
00:56:48,000 --> 00:56:49,920
означает, что вы не знаете, что это

1604
00:56:49,920 --> 00:56:52,720
сейчас, а затем вы предсказываете эти слова

1605
00:56:52,720 --> 00:56:54,000
некоторые детали, которые мы  перейти на следующем

1606
00:56:54,000 --> 00:56:56,000
слайде, но так вот почему  Похоже,

1607
00:56:56,000 --> 00:56:59,040
что у нас есть предложение, которое я маскирую

1608
00:56:59,040 --> 00:57:01,040
в маску,

1609
00:57:01,040 --> 00:57:02,559
мы получаем некоторые скрытые состояния для

1610
00:57:02,559 --> 00:57:03,920
всех правильно, поэтому мы не тренировались,

1611
00:57:03,920 --> 00:57:05,200
мы вообще не

1612
00:57:05,200 --> 00:57:08,319
меняли кодировщик трансформатора,

1613
00:57:08,319 --> 00:57:10,319
мы просто сказали хорошо, вот как эта

1614
00:57:10,319 --> 00:57:11,839
последовательность  вы увидите все правильно,

1615
00:57:11,839 --> 00:57:13,599
посмотрите на все стрелки, идущие повсюду,

1616
00:57:13,599 --> 00:57:15,520
но тогда, гм,

1617
00:57:15,520 --> 00:57:17,520
у нас есть этот слой прогнозирования,

1618
00:57:17,520 --> 00:57:19,040
который был бы, если бы

1619
00:57:19,040 --> 00:57:21,040
мы правильно тренировались и использовали его, у нас

1620
00:57:21,040 --> 00:57:23,119
есть потери только

1621
00:57:23,119 --> 00:57:26,319
в тех словах, где мы  были маски здесь, так что

1622
00:57:26,319 --> 00:57:28,240
я замаскировал это, а затем я должен

1623
00:57:28,240 --> 00:57:30,559
предсказать, что это был wendt, который пошел сюда,

1624
00:57:30,559 --> 00:57:32,559
и store, который пошел сюда,

1625
00:57:32,559 --> 00:57:34,400
и теперь это очень похоже на языковое

1626
00:57:34,400 --> 00:57:36,000
моделирование, вы можете сказать,

1627
00:57:36,000 --> 00:57:37,440
но теперь вам не нужно иметь такого

1628
00:57:37,440 --> 00:57:39,520
рода  разложение слева направо,

1629
00:57:39,520 --> 00:57:40,559
вы говорите, что

1630
00:57:40,559 --> 00:57:42,000
я собираюсь удалить некоторые слова,

1631
00:57:42,000 --> 00:57:43,920
и вы должны предсказать, что они

1632
00:57:43,920 --> 00:57:45,839
собой представляют.

1633
00:57:50,079 --> 00:57:52,640
ммм

1634
00:57:52,640 --> 00:57:54,319
так что они на самом деле  Они

1635
00:57:54,319 --> 00:57:56,400
предложили моделирование массового языка, и

1636
00:57:56,400 --> 00:57:57,760
они выпустили веса

1637
00:57:57,760 --> 00:57:59,520
этого предварительно обученного преобразователя с

1638
00:57:59,520 --> 00:58:01,599
немного большей сложностью, чтобы заставить

1639
00:58:01,599 --> 00:58:03,359
моделирование массового языка работать,

1640
00:58:03,359 --> 00:58:04,960
поэтому

1641
00:58:04,960 --> 00:58:07,200
вы собираетесь взять случайные 15

1642
00:58:07,200 --> 00:58:08,960
токенов подслов,

1643
00:58:08,960 --> 00:58:11,119
чтобы было  это было правдой, но вы не

1644
00:58:11,119 --> 00:58:14,160
всегда собираетесь заменять их маской,

1645
00:58:14,160 --> 00:58:15,839
вы можете думать об этом так, как

1646
00:58:15,839 --> 00:58:18,319
если бы модель видит маркер маски,

1647
00:58:18,319 --> 00:58:19,760
она получает гарантию, что ей нужно

1648
00:58:19,760 --> 00:58:21,440
что-то предсказать,

1649
00:58:21,440 --> 00:58:23,119
и если модель не видит маркер маски,

1650
00:58:23,119 --> 00:58:25,440
это  получает гарантию, что ему

1651
00:58:25,440 --> 00:58:27,280
не нужно ничего предсказывать, так

1652
00:58:27,280 --> 00:58:28,720
зачем ему создавать сильные

1653
00:58:28,720 --> 00:58:31,200
представления слов, которые не

1654
00:58:31,200 --> 00:58:32,720
замаскированы,

1655
00:58:32,720 --> 00:58:34,160
и я хочу, чтобы моя модель строила сильные

1656
00:58:34,160 --> 00:58:36,640
представления всего, поэтому мы

1657
00:58:36,640 --> 00:58:38,400
собираемся добавить некоторую неопределенность

1658
00:58:38,400 --> 00:58:39,920
к модели, поэтому мы собираемся

1659
00:58:39,920 --> 00:58:41,359
использовать эти пятнадцать процентов токенов в

1660
00:58:41,359 --> 00:58:42,880
восьмидесяти процентах случаев, когда мы собираемся

1661
00:58:42,880 --> 00:58:45,040
заменить ее маской, которая была нашей

1662
00:58:45,040 --> 00:58:47,760
первоначальной идеей моделирования массового языка.

1663
00:58:47,760 --> 00:58:49,760
мы на

1664
00:58:49,760 --> 00:58:51,040
самом деле собираемся заменить слово

1665
00:58:51,040 --> 00:58:53,040
просто случайным токеном, просто случайным

1666
00:58:53,040 --> 00:58:54,720
словарным элементом,

1667
00:58:54,720 --> 00:58:56,079
это может быть что угодно,

1668
00:58:56,079 --> 00:58:57,440
а затем в остальные 10 случаев мы

1669
00:58:57,440 --> 00:58:59,520
собираемся оставить слово без изменений,

1670
00:58:59,520 --> 00:59:02,880
поэтому теперь он сразу видит слово,

1671
00:59:02,880 --> 00:59:05,440
которое может быть  случайный токен, или он может

1672
00:59:05,440 --> 00:59:07,040
быть неизменным,

1673
00:59:07,040 --> 00:59:09,119
и если я увижу маску, я знаю, что мне нужно ее

1674
00:59:09,119 --> 00:59:10,640
предсказать,

1675
00:59:10,640 --> 00:59:12,960
так что эти две вещи здесь говорят, что

1676
00:59:12,960 --> 00:59:15,200
вы должны как бы делать это, что

1677
00:59:15,200 --> 00:59:17,119
вы должны быть начеку для каждого

1678
00:59:17,119 --> 00:59:18,799
слова  в вашем в вашем представлении, так что

1679
00:59:18,799 --> 00:59:21,680
вот я пицца с маской

1680
00:59:21,680 --> 00:59:23,280
правильно, оказывается, и модель этого не

1681
00:59:23,280 --> 00:59:25,839
знала, но она получает три

1682
00:59:25,839 --> 00:59:28,400
терма для предложения, у него только

1683
00:59:28,400 --> 00:59:30,960
одна маска, но он будет наказан

1684
00:59:30,960 --> 00:59:32,559
за предсказание трех разных вещей  ему

1685
00:59:32,559 --> 00:59:33,839
нужно предсказать, что это слово на

1686
00:59:33,839 --> 00:59:35,359
самом деле ушло,

1687
00:59:35,359 --> 00:59:37,440
поэтому я заменил это

1688
00:59:37,440 --> 00:59:39,359
ему нужно предсказать, что это слово

1689
00:59:39,359 --> 00:59:41,599
на самом деле является словом to,

1690
00:59:41,599 --> 00:59:43,040
а затем ему нужно предсказать, что это

1691
00:59:43,040 --> 00:59:44,839
слово фактически

1692
00:59:44,839 --> 00:59:47,520
хранится сейчас в качестве короткой интерлюдии, которую вы могли бы

1693
00:59:47,520 --> 00:59:49,200
Думаю, ты думаешь, Джон

1694
00:59:49,200 --> 00:59:50,720
модель никак не могла знать об этом,

1695
00:59:50,720 --> 00:59:54,000
это настолько недооценено, вы знаете,

1696
00:59:54,000 --> 00:59:56,240
хорошо, я понимаю, что пицца немного странная, я признаю,

1697
00:59:56,240 --> 00:59:57,440
но просто нет способа узнать, что

1698
00:59:57,440 --> 00:59:58,880
есть магазин, в который я вошел, я имею в виду, что

1699
00:59:58,880 --> 01:00:00,160
то же самое верно и в отношении языкового

1700
01:00:00,160 --> 01:00:02,319
моделирования, так что  в конечном итоге он

1701
01:00:02,319 --> 01:00:03,920
изучит эти средние статистические данные о

1702
01:00:03,920 --> 01:00:05,520
том, что обычно происходит в данном

1703
01:00:05,520 --> 01:00:08,240
контексте, и вы узнаете, как

1704
01:00:08,240 --> 01:00:09,760
хеджировать свои ставки и попытаетесь построить

1705
01:00:09,760 --> 01:00:11,359
распределение того, что может

1706
01:00:11,359 --> 01:00:13,119
там появиться, для людей, которые думают,

1707
01:00:13,119 --> 01:00:14,640
что  если есть что-то, о чем вы

1708
01:00:14,640 --> 01:00:15,920
должны подумать, он должен

1709
01:00:15,920 --> 01:00:17,920
знать, какие вещи окажутся в

1710
01:00:17,920 --> 01:00:20,079
этих слотах, у него есть другая неопределенность,

1711
01:00:20,079 --> 01:00:21,520
потому что он не может быть уверен, что любое

1712
01:00:21,520 --> 01:00:24,079
из других слов обязательно

1713
01:00:24,079 --> 01:00:25,119
правильное,

1714
01:00:25,119 --> 01:00:26,319
а затем  это вы знаете, что он хорошо

1715
01:00:26,319 --> 01:00:29,680
предсказывает эти три слова,

1716
01:00:29,680 --> 01:00:31,280
и поэтому

1717
01:00:33,119 --> 01:00:34,559
вы можете понять, почему важно не

1718
01:00:34,559 --> 01:00:36,559
просто иметь маски потенциально, чтобы иметь

1719
01:00:36,559 --> 01:00:38,160
такие вещи вроде рандомизации токенов,

1720
01:00:38,160 --> 01:00:40,079
потому что мы снова на самом деле не

1721
01:00:40,079 --> 01:00:41,839
заботиться о его способности правильно предсказывать

1722
01:00:41,839 --> 01:00:44,319
маски, как будто я не собираюсь

1723
01:00:44,319 --> 01:00:46,799
обычно я не собираюсь на самом деле отбирать

1724
01:00:46,799 --> 01:00:48,480
из распределения модели то, что

1725
01:00:48,480 --> 01:00:50,480
должно быть здесь,

1726
01:00:50,480 --> 01:00:53,599
вместо этого я собираюсь, чтобы вы знали, используйте

1727
01:00:53,599 --> 01:00:55,280
параметры нейронной сети и

1728
01:00:55,280 --> 01:00:56,799
ожидайте  что он построил сильные

1729
01:00:56,799 --> 01:00:58,799
представления языка, поэтому я не

1730
01:00:58,799 --> 01:01:00,400
хочу, чтобы он думал, что у него есть бесплатный пропуск

1731
01:01:00,400 --> 01:01:02,319
для представления чего-то, если у него

1732
01:01:02,319 --> 01:01:05,200
нет маски,

1733
01:01:05,599 --> 01:01:07,520
ладно, так что, гм,

1734
01:01:07,520 --> 01:01:09,280
была одна дополнительная вещь с

1735
01:01:09,280 --> 01:01:11,760
предварительной тренировкой

1736
01:01:11,760 --> 01:01:12,880
отрыжки,

1737
01:01:12,880 --> 01:01:14,880
которая  - это цель прогнозирования следующего предложения,

1738
01:01:14,880 --> 01:01:16,960
поэтому ввод для bert выглядит так,

1739
01:01:16,960 --> 01:01:18,240
как будто это прямо из бумаги отрыжки,

1740
01:01:18,240 --> 01:01:21,200
которую у вас есть, у вас есть метка здесь

1741
01:01:21,200 --> 01:01:23,599
перед вашим первым предложением, а затем

1742
01:01:23,599 --> 01:01:25,359
разделение, а затем второе предложение, поэтому у

1743
01:01:25,359 --> 01:01:27,839
вас всегда были два смежных фрагмента  текста

1744
01:01:28,960 --> 01:01:31,599
у вас был первый фрагмент текста здесь

1745
01:01:31,599 --> 01:01:33,920
моя собака симпатичная, а затем вторая

1746
01:01:33,920 --> 01:01:36,240
проверка текста, который ему нравится играть

1747
01:01:36,240 --> 01:01:39,599
правильно, вы можете увидеть подписку там, и

1748
01:01:39,599 --> 01:01:41,280
теперь они на самом деле будут намного

1749
01:01:41,280 --> 01:01:43,680
длиннее, так что все это  Это

1750
01:01:43,680 --> 01:01:46,720
будет 512 слов, это будет примерно половина,

1751
01:01:46,720 --> 01:01:48,079
а это будет примерно половина, и они

1752
01:01:48,079 --> 01:01:51,599
будут непрерывными фрагментами текста,

1753
01:01:51,599 --> 01:01:53,359
но вот сделка, которую они хотели

1754
01:01:53,359 --> 01:01:54,400
сделать,

1755
01:01:54,400 --> 01:01:56,079
это попытаться научить

1756
01:01:56,079 --> 01:01:57,920
систему понимать

1757
01:01:57,920 --> 01:02:00,000
отношения между  различные целые

1758
01:02:00,000 --> 01:02:01,520
фрагменты текста,

1759
01:02:01,520 --> 01:02:03,200
чтобы лучше подготовиться к

1760
01:02:03,200 --> 01:02:05,200
целям для последующих приложений,

1761
01:02:05,200 --> 01:02:07,200
таких как ответы на вопросы, где у вас есть

1762
01:02:07,200 --> 01:02:10,079
два довольно разных фрагмента текста, и

1763
01:02:10,079 --> 01:02:11,599
вам нужно знать, как они соотносятся друг с

1764
01:02:11,599 --> 01:02:13,440
другом, поэтому цель, которую они придумали,

1765
01:02:13,440 --> 01:02:15,680
заключалась в том, что вы должны  предварительно вы должны

1766
01:02:15,680 --> 01:02:18,559
иногда, чтобы второй фрагмент текста

1767
01:02:18,559 --> 01:02:19,760
был

1768
01:02:19,760 --> 01:02:22,079
фактическим фрагментом текста, который непосредственно

1769
01:02:22,079 --> 01:02:26,079
следует за первым в вашем наборе данных,

1770
01:02:26,079 --> 01:02:27,760
а иногда вторая проверка

1771
01:02:27,760 --> 01:02:29,760
текста

1772
01:02:29,760 --> 01:02:31,520
случайным образом выбирается из другого,

1773
01:02:31,520 --> 01:02:32,720
не связанного с этим места,

1774
01:02:32,720 --> 01:02:35,039
и модель должна предсказать,

1775
01:02:35,039 --> 01:02:36,400
будет ли он первым  случай или

1776
01:02:36,400 --> 01:02:38,960
второй, чтобы снова как-то

1777
01:02:38,960 --> 01:02:40,400
рассуждать о взаимосвязях

1778
01:02:40,400 --> 01:02:42,240
между двумя фрагментами текста,

1779
01:02:42,240 --> 01:02:44,640
так что это предсказание следующего предложения, я

1780
01:02:44,640 --> 01:02:45,760
думаю, что я  Об этом важно подумать,

1781
01:02:45,760 --> 01:02:47,920
потому что опять же, это совсем другая идея

1782
01:02:47,920 --> 01:02:50,400
о цели предварительной подготовки, чем языковое

1783
01:02:50,400 --> 01:02:53,520
моделирование и моделирование замаскированного языка,

1784
01:02:53,520 --> 01:02:55,520
хотя в более поздних работах вроде бы утверждалось,

1785
01:02:55,520 --> 01:02:57,359
что в случае с Bert это не

1786
01:02:57,359 --> 01:03:00,160
нужно или полезно, и один из

1787
01:03:00,160 --> 01:03:02,559
аргументов на самом деле  потому

1788
01:03:02,559 --> 01:03:05,280
что на самом деле лучше

1789
01:03:05,280 --> 01:03:07,359
иметь один контекст, который вдвое

1790
01:03:07,359 --> 01:03:09,680
длиннее, чтобы вы могли изучать

1791
01:03:09,680 --> 01:03:12,160
зависимости и вещи на еще более дальнем расстоянии, и поэтому

1792
01:03:12,160 --> 01:03:13,760
была бы полезна сама цель,

1793
01:03:13,760 --> 01:03:15,680
если бы вы всегда могли просто удвоить

1794
01:03:15,680 --> 01:03:17,119
размер контекста, я не уверен, что

1795
01:03:17,119 --> 01:03:18,960
кто-то проводил исследования по этому поводу, но, опять же,

1796
01:03:18,960 --> 01:03:20,720
это похоже на другую цель,

1797
01:03:20,720 --> 01:03:22,640
и он все еще что-то шумит

1798
01:03:22,640 --> 01:03:24,880
о вводе. Правильно, ввод был

1799
01:03:24,880 --> 01:03:27,119
этим большим фрагментом текста, и вы

1800
01:03:27,119 --> 01:03:28,720
произнесли его, чтобы сказать, что теперь вы не знаете,

1801
01:03:28,720 --> 01:03:30,160
действительно ли это было  это или

1802
01:03:30,160 --> 01:03:31,680
вы как бы заменили его кучей

1803
01:03:31,680 --> 01:03:32,720
мусора

1804
01:03:32,720 --> 01:03:35,119
вот такая вторая часть здесь была

1805
01:03:35,119 --> 01:03:36,559
ли вторая часть

1806
01:03:36,559 --> 01:03:39,680
заменена чем-то, да, была

1807
01:03:39,680 --> 01:03:41,119
заменена  с чем-то, что на

1808
01:03:41,119 --> 01:03:45,000
самом деле происходило не из той же последовательности,

1809
01:03:46,240 --> 01:03:47,920
хорошо, поэтому давайте поговорим о некоторых деталях о

1810
01:03:47,920 --> 01:03:48,720
bert

1811
01:03:48,720 --> 01:03:51,839
um, поэтому у берта было 12 или 24 слоя в зависимости

1812
01:03:51,839 --> 01:03:53,440
от основания или большого размера, вы,

1813
01:03:53,440 --> 01:03:55,039
вероятно, будете использовать одну из этих моделей или

1814
01:03:55,039 --> 01:03:56,319
одну из  потомки этих

1815
01:03:56,319 --> 01:03:58,640
моделей, если вы просто решите сделать

1816
01:03:58,640 --> 01:04:00,160
что-то с настраиваемым окончательным проектом

1817
01:04:00,160 --> 01:04:03,599
потенциально или если вы выберете

1818
01:04:03,599 --> 01:04:06,079
версию проекта файла по умолчанию,

1819
01:04:06,079 --> 01:04:08,880
и вы знаете, что скрытое измерение 600 или тысячи

1820
01:04:08,880 --> 01:04:10,640
измерений вызывает

1821
01:04:10,640 --> 01:04:11,599
кучу внимания, так что это  это

1822
01:04:11,599 --> 01:04:13,599
многогранное внимание, помните, но

1823
01:04:13,599 --> 01:04:16,079
их куча, так что вы разбиваете

1824
01:04:16,079 --> 01:04:18,240
все свои измерения на эти 16 голов,

1825
01:04:18,240 --> 01:04:20,559
и мы говорим о порядке

1826
01:04:20,559 --> 01:04:22,960
пары сотен миллионов параметров

1827
01:04:22,960 --> 01:04:25,440
в то время, прямо в 2018 году, мы были как

1828
01:04:25,440 --> 01:04:28,319
эй, это  много параметров,

1829
01:04:28,319 --> 01:04:29,760
как вы, это много

1830
01:04:29,760 --> 01:04:33,839
параметров, а теперь модели

1831
01:04:33,839 --> 01:04:36,079
намного больше, поэтому давайте отслеживать

1832
01:04:36,079 --> 01:04:37,839
размеры моделей, когда мы проходим

1833
01:04:37,839 --> 01:04:40,400
через это, и давайте теперь вернемся

1834
01:04:40,400 --> 01:04:42,160
к  размеры корпуса, так что

1835
01:04:42,160 --> 01:04:44,480
у нас есть корпус книг, это количество

1836
01:04:44,480 --> 01:04:45,680
слов, это то же самое, что

1837
01:04:45,680 --> 01:04:48,799
gpt uh gpt one был обучен на 800

1838
01:04:48,799 --> 01:04:50,799
миллионах слов, теперь мы собираемся тренироваться

1839
01:04:51,599 --> 01:04:53,599
также на английской википедии,

1840
01:04:53,599 --> 01:04:56,720
это 250, извините, это 2500 миллионов, так что

1841
01:04:56,720 --> 01:04:59,520
это 2,5 миллиарда слов,

1842
01:04:59,520 --> 01:05:01,520
и еще

1843
01:05:01,520 --> 01:05:03,280
раз, чтобы дать вам представление о том, что

1844
01:05:03,280 --> 01:05:05,680
делается на практике, правильное предварительное обучение

1845
01:05:05,680 --> 01:05:09,039
дорого и непрактично для большинства

1846
01:05:09,039 --> 01:05:10,880
пользователей, скажем

1847
01:05:10,880 --> 01:05:11,760
так,

1848
01:05:11,760 --> 01:05:13,839
если вы исследователь с

1849
01:05:13,839 --> 01:05:16,240
графическим процессором или пятью графическими процессорами или что-то в этом роде,

1850
01:05:16,240 --> 01:05:17,839
вы склонны  чтобы на самом деле не проводить предварительное обучение

1851
01:05:17,839 --> 01:05:19,920
всей вашей собственной модели Bert, если вы не

1852
01:05:19,920 --> 01:05:22,160
готовы тратить на это много времени,

1853
01:05:22,160 --> 01:05:24,799
сам Burt был предварительно обучен с 64

1854
01:05:24,799 --> 01:05:27,039
чипами TPU. TPU - это особый вид

1855
01:05:27,039 --> 01:05:29,599
аппаратного ускорителя, который эффективно ускоряет

1856
01:05:30,400 --> 01:05:32,079
тензорные операции.

1857
01:05:32,079 --> 01:05:34,799
разработан Google,

1858
01:05:34,799 --> 01:05:36,960
так что tpus просто

1859
01:05:36,960 --> 01:05:37,920
быстрые

1860
01:05:37,920 --> 01:05:39,680
и могут вместить много,

1861
01:05:39,680 --> 01:05:41,760
и в течение четырех дней у них было 64 чипа,

1862
01:05:41,760 --> 01:05:43,520
поэтому, если у вас есть один gpu, который вы можете

1863
01:05:43,520 --> 01:05:45,839
думать как меньше, чем один tpu,

1864
01:05:45,839 --> 01:05:47,359
вы будете ждать

1865
01:05:47,359 --> 01:05:49,280
долго  к pre-t  дождь, но

1866
01:05:49,280 --> 01:05:51,920
настройка на самом деле точная настройка настолько быстра, что

1867
01:05:51,920 --> 01:05:54,079
она настолько быстра и практична, что это обычное дело

1868
01:05:54,079 --> 01:05:56,319
для одного графического процессора, вы увидите, насколько более

1869
01:05:56,319 --> 01:05:58,400
быстрая точная настройка, чем предварительная тренировка

1870
01:05:58,400 --> 01:06:02,480
в задании пять мкм, и поэтому

1871
01:06:02,480 --> 01:06:04,000
я думаю, что это становится рефреном

1872
01:06:04,000 --> 01:06:06,000
области  вы проходили предварительное обучение один или

1873
01:06:06,000 --> 01:06:07,520
несколько раз, точно так же,

1874
01:06:07,520 --> 01:06:08,960
как пара людей выпустила большие

1875
01:06:08,960 --> 01:06:10,480
предварительно обученные модели, а затем вы

1876
01:06:10,480 --> 01:06:12,480
много раз настраиваете правильно, так что вы

1877
01:06:12,480 --> 01:06:14,400
сохраняете эти параметры от предварительного обучения

1878
01:06:14,400 --> 01:06:16,240
и настраиваете все виды

1879
01:06:16,240 --> 01:06:19,559
различные проблемы,

1880
01:06:20,480 --> 01:06:22,160
и что эта парадигма верна, беря

1881
01:06:22,160 --> 01:06:24,160
что-то вроде bert или что-то еще, что является лучшим

1882
01:06:24,160 --> 01:06:27,520
потомком bert, и беря его

1883
01:06:27,520 --> 01:06:29,119
предварительно обученным, а затем настраивая его на

1884
01:06:29,119 --> 01:06:31,039
то, что вы хотите,

1885
01:06:31,039 --> 01:06:32,400
довольно близко к тому, что

1886
01:06:32,400 --> 01:06:35,119
вы знаете, это очень, очень сильная

1887
01:06:35,119 --> 01:06:38,480
основа в  nlp прямо сейчас,

1888
01:06:38,480 --> 01:06:40,640
и простота довольно увлекательна,

1889
01:06:40,640 --> 01:06:43,200
и есть одна база кода,

1890
01:06:43,200 --> 01:06:45,119
называемая трансформаторами, от компании

1891
01:06:45,119 --> 01:06:47,599
под названием hugging face, которая делает это

1892
01:06:47,599 --> 01:06:50,079
действительно просто парой строк

1893
01:06:50,079 --> 01:06:52,000
python, чтобы попробовать, так что это  как

1894
01:06:52,000 --> 01:06:53,280
бы открываются

1895
01:06:53,280 --> 01:06:56,000
очень сильные басовые партии без

1896
01:06:56,000 --> 01:06:59,039
особых усилий для множества задач,

1897
01:06:59,039 --> 01:07:01,359
ладно, так что давайте поговорим об оценке,

1898
01:07:01,359 --> 01:07:03,920
поэтому предварительная подготовка требует

1899
01:07:03,920 --> 01:07:05,119
всего этого разного понимания языка,

1900
01:07:05,119 --> 01:07:08,559
а область - это

1901
01:07:08,559 --> 01:07:11,520
область нлп.  трудно проводить оценку,

1902
01:07:11,520 --> 01:07:13,200
но мы стараемся изо всех сил и создаем наборы данных,

1903
01:07:13,200 --> 01:07:14,880
которые, по нашему мнению, являются сложными по разным

1904
01:07:14,880 --> 01:07:16,400
причинам, потому что они требуют от вас

1905
01:07:16,400 --> 01:07:18,079
знаний о языке, о мире

1906
01:07:18,079 --> 01:07:20,480
и о рассуждениях, и поэтому, когда мы

1907
01:07:20,480 --> 01:07:22,559
оцениваем, помогает ли вам предварительная

1908
01:07:22,559 --> 01:07:24,720
подготовка  много своего рода общих знаний

1909
01:07:24,720 --> 01:07:26,319
эм

1910
01:07:26,319 --> 01:07:29,520
мы о мальчик эм мы оцениваем по многим из

1911
01:07:29,520 --> 01:07:32,559
этих задач, поэтому мы оцениваем такие вещи,

1912
01:07:32,559 --> 01:07:35,680
как обнаружение перефразирования в вопросах кворуса,

1913
01:07:36,880 --> 01:07:38,960
логический вывод естественного языка, который мы видели,

1914
01:07:38,960 --> 01:07:40,799
у нас есть наборы данных для жесткого анализа настроений

1915
01:07:40,799 --> 01:07:42,319
или того, что было жестким

1916
01:07:42,319 --> 01:07:45,200
анализом настроений  наборы данных пару лет назад эм на

1917
01:07:45,200 --> 01:07:47,200
самом деле выяснить, являются ли предложения

1918
01:07:47,200 --> 01:07:49,440
грамматическими, бывает сложно

1919
01:07:49,440 --> 01:07:51,359
определить текстовое сходство

1920
01:07:51,359 --> 01:07:53,599
семантическое сходство текста может быть h

1921
01:07:53,599 --> 01:07:55,680
Снова перефразируя

1922
01:07:55,680 --> 01:07:57,599
логический вывод естественного языка на очень-очень маленьком наборе данных,

1923
01:07:57,599 --> 01:07:59,520
так что это действительно предварительное обучение, помогающее

1924
01:07:59,520 --> 01:08:01,119
вам тренироваться на небольших наборах данных,

1925
01:08:01,119 --> 01:08:04,960
ответ - да, вроде того, и так гм,

1926
01:08:04,960 --> 01:08:06,799
так что ребята из bert выпустили свою

1927
01:08:06,799 --> 01:08:09,760
статью после того, как gpt был выпущен  и

1928
01:08:09,760 --> 01:08:11,280
было много современных

1929
01:08:11,280 --> 01:08:13,359
результатов, которые были получены из различных вещей,

1930
01:08:13,359 --> 01:08:15,680
которые вы должны были делать,

1931
01:08:15,680 --> 01:08:17,359
и

1932
01:08:17,359 --> 01:08:19,600
вы знаете результаты, которые вы получаете

1933
01:08:19,600 --> 01:08:21,120
от предварительной подготовки, так что вот

1934
01:08:21,120 --> 01:08:23,359
openai gpt вот  bert base и

1935
01:08:23,359 --> 01:08:25,600
large последние три ряда все предварительно обучены,

1936
01:08:25,600 --> 01:08:28,799
elmo вроде как посередине, а это как

1937
01:08:28,799 --> 01:08:29,759
бы посередине между

1938
01:08:29,759 --> 01:08:31,198
предварительным обучением всей модели и

1939
01:08:31,198 --> 01:08:32,799
просто встраиванием слов, вот что это

1940
01:08:32,799 --> 01:08:34,000
такое,

1941
01:08:34,000 --> 01:08:36,080
и числами, которые вы получаете  просто

1942
01:08:36,080 --> 01:08:37,600
я думаю, что эта область была довольно

1943
01:08:37,600 --> 01:08:39,600
поразительной, на самом деле, мы все были

1944
01:08:39,600 --> 01:08:40,960
удивлены, что осталось так много всего, что нужно

1945
01:08:40,960 --> 01:08:43,198
было даже получить по некоторым из этих наборов данных,

1946
01:08:43,198 --> 01:08:46,158
и вы знаете, беря здесь, так что

1947
01:08:46,158 --> 01:08:48,479
эта строка в таблице не отмечена, но

1948
01:08:48,479 --> 01:08:49,359
на самом деле это  количество обучающих

1949
01:08:49,359 --> 01:08:52,238
примеров, этот набор данных содержит 2,5 000

1950
01:08:52,238 --> 01:08:55,279
обучающих примеров, и до того, как

1951
01:08:55,279 --> 01:08:57,839
появились большие трансформаторы, у нас была

1952
01:08:57,839 --> 01:09:00,719
точность 60, мы запускаем на нем трансформаторы,

1953
01:09:00,719 --> 01:09:03,359
мы получаем 10 баллов только за предварительное обучение,

1954
01:09:03,359 --> 01:09:05,439
и это была тенденция, которая  только что

1955
01:09:05,439 --> 01:09:07,040
продолжил,

1956
01:09:07,040 --> 01:09:09,679
так зачем делать что-либо, кроме кодировщиков с предварительным обучением,

1957
01:09:09,679 --> 01:09:12,158
так как мы знаем, что кодировщики

1958
01:09:12,158 --> 01:09:14,158
хороши, и нам нам нравится тот факт, что у вас

1959
01:09:14,158 --> 01:09:15,839
есть двунаправленный контекст, мы также видели,

1960
01:09:15,839 --> 01:09:18,080
что bert справился лучше, чем gpt

1961
01:09:18,080 --> 01:09:20,640
um, но вы знаете, что это так, если хотите

1962
01:09:20,640 --> 01:09:21,839
на самом деле

1963
01:09:21,839 --> 01:09:24,399
заставить его делать все правильно,

1964
01:09:24,399 --> 01:09:26,238
вы не можете просто сгенерировать его,

1965
01:09:26,238 --> 01:09:27,759
генерировать последовательности из него так же, как

1966
01:09:27,759 --> 01:09:30,399
вы бы из модели, такой как gpt,

1967
01:09:30,399 --> 01:09:32,719
предварительно обученный декодер, вы можете

1968
01:09:32,719 --> 01:09:34,080
отсортировать образец того, что должно быть в

1969
01:09:34,080 --> 01:09:35,920
маске, так что вот  mask

1970
01:09:35,920 --> 01:09:37,759
вы можете поместить маску где-нибудь, образец

1971
01:09:37,759 --> 01:09:39,679
слов, которые должны быть там, но если вы

1972
01:09:39,679 --> 01:09:41,198
хотите выбрать весь контекст правильно, если

1973
01:09:41,198 --> 01:09:42,238
вы хотите получить эту историю о

1974
01:09:42,238 --> 01:09:44,960
единорогах, например, кодировщик - это

1975
01:09:44,960 --> 01:09:46,799
не то, что вы хотите сделать, поэтому у них

1976
01:09:46,799 --> 01:09:49,679
есть  Типа различных контрактов, которые

1977
01:09:49,679 --> 01:09:50,799
могут быть использованы

1978
01:09:50,799 --> 01:09:54,319
естественным образом, по крайней мере, по-разному,

1979
01:09:54,960 --> 01:09:56,560
хорошо, так что давайте очень кратко поговорим о

1980
01:09:56,560 --> 01:09:58,239
расширениях bert, так что есть сожженные

1981
01:09:58,239 --> 01:10:00,800
варианты, такие как roberta и spanbert, и

1982
01:10:00,800 --> 01:10:02,320
есть просто куча документов со

1983
01:10:02,320 --> 01:10:03,920
словом burt в названии, которые сделали  разные

1984
01:10:03,920 --> 01:10:07,600
вещи два очень важных вывода roberta

1985
01:10:07,600 --> 01:10:09,760
train bert long bert is under fit

1986
01:10:09,760 --> 01:10:11,840
обучайте его на большем количестве данных обучайте его для большего количества

1987
01:10:11,840 --> 01:10:14,480
шагов spanbert

1988
01:10:14,480 --> 01:10:18,480
uh mask смежные промежутки дополнительных слов

1989
01:10:18,480 --> 01:10:20,400
uh слов усложняют более полезную

1990
01:10:20,400 --> 01:10:22,080
предварительную тренировку, так что идея заключается в том,

1991
01:10:22,080 --> 01:10:23,760
что  мы можем придумать лучшие способы

1992
01:10:23,760 --> 01:10:25,920
зашумить ввод, скрыть материал

1993
01:10:25,920 --> 01:10:27,840
на входе или сломать материал на входе

1994
01:10:27,840 --> 01:10:31,040
для нашей модели, чтобы исправить um, чтобы вы знали,

1995
01:10:31,040 --> 01:10:33,040
например, если у вас есть маска с предложением

1996
01:10:33,040 --> 01:10:35,040
маска сопротивления ушей

1997
01:10:35,040 --> 01:10:38,560
хорошо, это просто не так сложно

1998
01:10:38,560 --> 01:10:41,440
знать, что это неотразимо

1999
01:10:41,440 --> 01:10:42,800
правильно, потому что, например, что это

2000
01:10:42,800 --> 01:10:44,560
могло быть после этих дополнительных слов, так

2001
01:10:44,560 --> 01:10:46,719
что это сопротивление уху,

2002
01:10:46,719 --> 01:10:48,640
вы знаете, что что-

2003
01:10:48,640 --> 01:10:49,840
то должно произойти здесь, и это,

2004
01:10:49,840 --> 01:10:52,640
вероятно, конец этого  слово, тогда как если

2005
01:10:52,640 --> 01:10:55,280
вы сейчас замаскируете длинную последовательность вещей,

2006
01:10:55,280 --> 01:10:57,280
это намного сложнее, и на самом деле

2007
01:10:57,280 --> 01:10:59,199
вы получаете полезный сигнал, который

2008
01:10:59,199 --> 01:11:01,280
непреодолимо хорош, и вам как бы

2009
01:11:01,280 --> 01:11:03,600
нужно замаскировать все из них, чтобы сделать задачу

2010
01:11:03,600 --> 01:11:05,679
интересной, поэтому

2011
01:11:05,679 --> 01:11:07,679
Спанберт был похож на вас  должен сделать это,

2012
01:11:07,679 --> 01:11:09,920
так что это было очень полезно,

2013
01:11:09,920 --> 01:11:11,679
так что Роберта просто чтобы указать вам на тот

2014
01:11:11,679 --> 01:11:13,600
факт, что Роберта показала, что Берт был не

2015
01:11:13,600 --> 01:11:17,040
в форме, вы знаете, он сказал, что Берт был

2016
01:11:17,040 --> 01:11:19,679
обучен примерно на 13 гигабайтах текста, он

2017
01:11:19,679 --> 01:11:22,400
получил некоторую точность, которую вы можете получить

2018
01:11:22,400 --> 01:11:24,640
выше  Удивительные результаты: наберите четыре

2019
01:11:24,640 --> 01:11:26,000
дополнительных очка

2020
01:11:26,000 --> 01:11:28,719
или около того здесь, просто взяв

2021
01:11:28,719 --> 01:11:31,199
идентичную модель и обучив ее на большем количестве

2022
01:11:31,199 --> 01:11:34,880
данных с большим размером пакета в

2023
01:11:34,880 --> 01:11:36,320
течение длительного времени,

2024
01:11:36,320 --> 01:11:38,800
и если вы тренируете ее еще дольше,

2025
01:11:38,800 --> 01:11:40,400
без каких-либо дополнительных данных, которые

2026
01:11:40,400 --> 01:11:43,520
вы не делаете

2027
01:11:44,960 --> 01:11:46,640
очень кратко хорошо,

2028
01:11:46,640 --> 01:11:49,520
очень кратко о декодерах кодировщика, чтобы

2029
01:11:49,520 --> 01:11:50,719
мы увидели, что

2030
01:11:50,719 --> 01:11:52,560
декодеры могут быть хорошими, потому что мы можем

2031
01:11:52,560 --> 01:11:53,760
играть с контрактами, которые они нам дают,

2032
01:11:53,760 --> 01:11:55,040
мы можем играть с ними, как

2033
01:11:55,040 --> 01:11:56,640
кодеры языковых моделей дают нам  этот

2034
01:11:56,640 --> 01:11:59,280
двунаправленный контекст, так что кодировщики-

2035
01:11:59,280 --> 01:12:01,199
декодеры, может быть, мы получим и то, и

2036
01:12:01,199 --> 01:12:02,960
другое на практике, они на самом деле да,

2037
01:12:02,960 --> 01:12:06,159
довольно сильны, так что у

2038
01:12:08,400 --> 01:12:10,080
нас было право, но я думаю, что один из

2039
01:12:10,080 --> 01:12:11,600
вопросов - что мы делаем, чтобы

2040
01:12:11,600 --> 01:12:12,880
предварительно  обучить их,

2041
01:12:12,880 --> 01:12:14,640
чтобы мы могли делать что-то вроде языкового

2042
01:12:14,640 --> 01:12:16,719
моделирования прямо там, где мы берем

2043
01:12:16,719 --> 01:12:22,080
последовательность слов слово один uh на слово uh

2044
01:12:22,080 --> 01:12:24,880
two t вместо t

2045
01:12:24,880 --> 01:12:26,960
right, и так как у меня есть слово один здесь точка

2046
01:12:26,960 --> 01:12:29,280
точка слово t мы предоставляем все это нашему

2047
01:12:29,280 --> 01:12:31,920
кодировщику и  мы не предсказываем ни одного из них,

2048
01:12:31,920 --> 01:12:33,760
и тогда у нас есть слово t плюс одно к слову

2049
01:12:33,760 --> 01:12:36,480
два t здесь, в нашем декодере

2050
01:12:36,480 --> 01:12:38,400
справа, и мы прогнозируем по ним, поэтому мы

2051
01:12:38,400 --> 01:12:39,920
делаем языковое моделирование на половине

2052
01:12:39,920 --> 01:12:42,320
последовательности, а другую половину мы взяли,

2053
01:12:42,320 --> 01:12:45,040
чтобы  двунаправленный кодировщик

2054
01:12:45,040 --> 01:12:46,000
правильно, поэтому мы строим сильные

2055
01:12:46,000 --> 01:12:49,360
представления на стороне кодировщика,

2056
01:12:49,360 --> 01:12:51,199
не прогнозируя языковое моделирование ни на что

2057
01:12:51,199 --> 01:12:53,199
из этого, а затем мы на другой

2058
01:12:53,199 --> 01:12:54,880
половине токенов, которые, как мы предполагаем, вы знаете, как

2059
01:12:54,880 --> 01:12:57,120
языковая модель,

2060
01:12:57,120 --> 01:12:58,480
и надеемся, что  ты вроде как

2061
01:12:58,480 --> 01:13:00,480
подготовительный бот  h из них хорошо из-за

2062
01:13:00,480 --> 01:13:04,000
потери моделирования на одном языке здесь,

2063
01:13:04,000 --> 01:13:05,520
и это на самом деле так, что это работает

2064
01:13:05,520 --> 01:13:07,600
довольно хорошо, кодер извлекает выгоду из

2065
01:13:07,600 --> 01:13:09,520
двунаправленности декодера, который вы можете

2066
01:13:09,520 --> 01:13:12,159
использовать для обучения модели, но что гм, эта

2067
01:13:12,159 --> 01:13:15,040
статья

2068
01:13:15,120 --> 01:13:17,040
показала, что представила модель t5

2069
01:13:17,040 --> 01:13:19,440
ruffle  Все это, как оказалось, работает лучше всего, на

2070
01:13:19,440 --> 01:13:21,280
самом деле была очень или, по крайней мере, несколько

2071
01:13:21,280 --> 01:13:23,360
другой целью, и это должно иметь

2072
01:13:23,360 --> 01:13:24,880
в виду, что у нас есть

2073
01:13:24,880 --> 01:13:26,800
разные способы определения

2074
01:13:26,800 --> 01:13:28,719
целей предварительной подготовки, и они

2075
01:13:28,719 --> 01:13:30,159
действительно будут работать по-разному,

2076
01:13:30,159 --> 01:13:31,760
поэтому  то, что они сказали, допустим, у вас есть

2077
01:13:31,760 --> 01:13:34,000
оригинальный текст, подобный этому, спасибо, что

2078
01:13:34,000 --> 01:13:37,600
пригласили меня на вечеринку на прошлой неделе,

2079
01:13:37,600 --> 01:13:39,760
мы собираемся определить промежутки переменной длины

2080
01:13:39,760 --> 01:13:42,960
в тексте, чтобы заменить их

2081
01:13:42,960 --> 01:13:44,719
уникальным символом, который говорит, что

2082
01:13:44,719 --> 01:13:47,280
здесь чего-то не хватает, а затем мы '  Я заменим, а

2083
01:13:47,280 --> 01:13:49,360
затем мы удалим это, так что теперь наш

2084
01:13:49,360 --> 01:13:52,560
вход в наш кодировщик - это символ благодарности

2085
01:13:53,679 --> 01:13:54,560
один

2086
01:13:54,560 --> 01:13:57,280
мне к символу вашей партии слабый

2087
01:13:57,280 --> 01:13:59,199
справа, поэтому мы зашумили вход, который мы

2088
01:13:59,199 --> 01:14:00,800
скрыли во входных данных

2089
01:14:00,800 --> 01:14:02,400
также действительно интересно, правда, это

2090
01:14:02,400 --> 01:14:04,000
не говорит, как долго это должно

2091
01:14:04,000 --> 01:14:05,120
быть,

2092
01:14:05,120 --> 01:14:07,600
это отличается от Берта справа Берт

2093
01:14:07,600 --> 01:14:10,480
сказал, что вы замаскировали столько

2094
01:14:13,920 --> 01:14:15,199
подслов  Я даже не знаю,

2095
01:14:15,199 --> 01:14:18,159
сколько это подслов, а затем у

2096
01:14:18,159 --> 01:14:19,360
вас есть это в вашем кодировщике, а затем

2097
01:14:19,360 --> 01:14:23,520
ваш декодер предсказывает первое

2098
01:14:24,239 --> 01:14:27,520
специальное слово это x здесь,

2099
01:14:27,520 --> 01:14:29,280
а затем то, что не хватало

2100
01:14:29,280 --> 01:14:32,000
для приглашения, так что спасибо xx за

2101
01:14:32,000 --> 01:14:34,560
приглашение, а затем  он предсказывает y, вот

2102
01:14:34,560 --> 01:14:36,480
этот y здесь, а затем то, что отсутствовало

2103
01:14:36,480 --> 01:14:40,159
в y на прошлой неделе,

2104
01:14:40,159 --> 01:14:42,480
это называется повреждением диапазона, и это

2105
01:14:42,480 --> 01:14:44,080
действительно интересно для меня, потому

2106
01:14:44,080 --> 01:14:46,000
что с точки зрения фактического

2107
01:14:46,000 --> 01:14:48,080
декодера кодировщика нам не нужно его менять

2108
01:14:48,080 --> 01:14:49,440
по сравнению с тем,  мы, если бы мы просто

2109
01:14:49,440 --> 01:14:51,199
выполняли предварительную подготовку по языковому моделированию,

2110
01:14:51,199 --> 01:14:53,360
потому что я просто занимаюсь языковым моделированием

2111
01:14:53,360 --> 01:14:55,520
всех этих вещей, я просто предсказываю эти

2112
01:14:55,520 --> 01:14:57,600
слова, как будто я языковая модель, я

2113
01:14:57,600 --> 01:14:59,920
только что выполнил этап предварительной обработки текста

2114
01:15:00,880 --> 01:15:02,480
правильно,

2115
01:15:02,480 --> 01:15:04,640
поэтому фактический  Я только что предварительно обработал

2116
01:15:04,640 --> 01:15:06,719
текст, чтобы он выглядел так,

2117
01:15:06,719 --> 01:15:08,480
чтобы он выглядел так, чтобы он выглядел так, а затем сделайте

2118
01:15:08,480 --> 01:15:10,640
вывод, который выглядит так, наверху,

2119
01:15:10,640 --> 01:15:11,679
и

2120
01:15:11,679 --> 01:15:13,440
модель получает, что вы знаете, делайте то, что

2121
01:15:13,440 --> 01:15:14,640
эффективно языковое моделирование, но на

2122
01:15:14,640 --> 01:15:16,320
самом деле это работает  лучше, так что есть

2123
01:15:16,320 --> 01:15:19,040
много чисел, я понимаю, но взгляните

2124
01:15:19,040 --> 01:15:21,440
на звезду здесь, этот кодировщик-декодер

2125
01:15:21,440 --> 01:15:23,679
с целью шумоподавления, который имеет тенденцию

2126
01:15:23,679 --> 01:15:26,159
работать лучше всего, и они пробовали аналогичные

2127
01:15:26,159 --> 01:15:28,880
модели, такие как модель языка

2128
01:15:28,880 --> 01:15:30,960
префиксов, мм, это была своего рода первая

2129
01:15:31,760 --> 01:15:33,600
попытка  что мы определили цель предварительного обучения

2130
01:15:33,600 --> 01:15:35,199
для языковых моделей, извините за

2131
01:15:35,199 --> 01:15:36,960
кодировщики-декодеры,

2132
01:15:38,400 --> 01:15:40,080
а затем у них был еще ряд

2133
01:15:40,080 --> 01:15:41,440
других вариантов, но что лучше всего сработало

2134
01:15:41,440 --> 01:15:43,199
для кодировщиков-декодеров,

2135
01:15:43,199 --> 01:15:45,040
и одна из увлекательных вещей в

2136
01:15:45,040 --> 01:15:47,520
t5 - это то, что вы можете предварительно обучить  и

2137
01:15:47,520 --> 01:15:50,159
настроить его на такие вопросы, как, когда

2138
01:15:50,159 --> 01:15:52,960
родился Франклин Друзвельт, и

2139
01:15:52,960 --> 01:15:55,600
настроить его, чтобы дать ответ,

2140
01:15:55,600 --> 01:15:57,440
и затем вы могли бы задать ему новые вопросы

2141
01:15:57,440 --> 01:15:59,760
во время теста, а затем он будет

2142
01:15:59,760 --> 01:16:01,440
получать ответ из своего  параметры с некоторой

2143
01:16:01,440 --> 01:16:04,320
точностью, и он будет делать это относительно

2144
01:16:04,320 --> 01:16:06,960
хорошо на самом деле, и он будет делать это, может быть, в

2145
01:16:06,960 --> 01:16:08,800
25 случаях на некоторых из этих

2146
01:16:08,800 --> 01:16:11,520
наборов данных с 220 миллионами параметров, а затем при

2147
01:16:11,520 --> 01:16:13,760
11 миллиардах параметров, это намного

2148
01:16:13,760 --> 01:16:15,920
больше, чем bert large, он будет делать это

2149
01:16:15,920 --> 01:16:18,640
даже  иногда лучше даже делать так же хорошо,

2150
01:16:18,640 --> 01:16:21,120
как и системы, которым было разрешено смотреть на

2151
01:16:21,120 --> 01:16:22,560
вещи, отличные от их собственных параметров, так что

2152
01:16:22,560 --> 01:16:24,880
снова это просто заставляет этот ответ

2153
01:16:24,880 --> 01:16:27,440
исходить из его параметров,

2154
01:16:28,320 --> 01:16:31,199
да, мне придется пропустить это, так что

2155
01:16:31,199 --> 01:16:33,280
если вы посмотрите на это  слайд после

2156
01:16:33,280 --> 01:16:35,520
урока у меня есть каждый из примеров того,

2157
01:16:35,520 --> 01:16:36,800
чему мы могли бы научиться

2158
01:16:36,800 --> 01:16:39,040
на предварительном обучении, с пометкой того, что

2159
01:16:39,040 --> 01:16:40,560
вы, возможно, изучаете, так что этот пример:

2160
01:16:40,560 --> 01:16:42,719
Стэнфордский университет расположен пустым,

2161
01:16:42,719 --> 01:16:44,640
вы можете узнать мелочи во всех этих

2162
01:16:44,640 --> 01:16:46,080
случаях есть все  эти вещи вы можете

2163
01:16:46,080 --> 01:16:48,159
узнать одно, я скажу, что

2164
01:16:48,159 --> 01:16:50,080
модели также учатся

2165
01:16:50,080 --> 01:16:52,000
и могут усугубить

2166
01:16:52,000 --> 01:16:54,560
расизм сексизм всевозможные плохие предубеждения,

2167
01:16:54,560 --> 01:16:56,960
которые закодированы в нашем тексте, когда я говорю, что

2168
01:16:56,960 --> 01:16:58,880
вы знаете, да,

2169
01:16:58,880 --> 01:17:00,800
они  сделайте это, и поэтому мы узнаем

2170
01:17:00,800 --> 01:17:02,080
об этом больше в наших последующих лекциях, но

2171
01:17:02,080 --> 01:17:04,080
важно помнить, что когда

2172
01:17:04,080 --> 01:17:05,040
вы делаете предварительную тренировку, вы

2173
01:17:05,040 --> 01:17:06,800
изучаете много вещей, и не все из

2174
01:17:06,800 --> 01:17:09,280
них хороши,

2175
01:17:09,600 --> 01:17:12,480
поэтому с gpt three  Последнее, что здесь

2176
01:17:14,400 --> 01:17:15,760
есть, - это третий способ взаимодействия

2177
01:17:15,760 --> 01:17:17,920
с моделями, связанный с обращением с

2178
01:17:17,920 --> 01:17:20,880
ними как с языковыми моделями, так что

2179
01:17:20,880 --> 01:17:23,360
gpt-3 - это очень большая модель,

2180
01:17:23,360 --> 01:17:25,040
выпущенная с помощью открытого ИИ,

2181
01:17:25,040 --> 01:17:27,360
и, похоже, она может учиться

2182
01:17:27,360 --> 01:17:29,679
на примерах.  в их контексте их

2183
01:17:29,679 --> 01:17:32,719
контексты декодера без шагов градиента,

2184
01:17:32,719 --> 01:17:35,280
просто просматривая их

2185
01:17:35,280 --> 01:17:36,400
историю,

2186
01:17:36,400 --> 01:17:39,920
и теперь gpt3 имеет 175 миллиардов параметров,

2187
01:17:39,920 --> 01:17:42,000
а последняя модель t5, которую мы видели, была 11

2188
01:17:42,000 --> 01:17:43,520
миллиардов параметров,

2189
01:17:43,520 --> 01:17:46,159
и, похоже, это своего

2190
01:17:46,159 --> 01:17:48,159
рода канонический пример этого  работает,

2191
01:17:48,159 --> 01:17:50,320
и как это выглядит, вы даете его

2192
01:17:50,320 --> 01:17:52,960
как часть его префикса спасибо

2193
01:17:52,960 --> 01:17:55,920
merci привет идет к mint идет вправо,

2194
01:17:55,920 --> 01:17:57,760
так что у вас есть эти примеры перевода,

2195
01:17:57,760 --> 01:18:00,080
вы просите его последний, и он

2196
01:18:00,080 --> 01:18:03,040
приходит с правильным переводом  ion, по-

2197
01:18:03,040 --> 01:18:04,560
видимому, потому, что он узнал что-то

2198
01:18:04,560 --> 01:18:05,840
о задаче, которую вы как бы

2199
01:18:05,840 --> 01:18:08,480
говорите ему с помощью префикса, и

2200
01:18:08,480 --> 01:18:09,840
поэтому вы можете сделать то же самое с

2201
01:18:09,840 --> 01:18:12,080
добавлением, поэтому что-то поместит что-то 5

2202
01:18:12,080 --> 01:18:14,719
плюс 8 равно 13. дайте ему дополнительные примеры, которые

2203
01:18:14,719 --> 01:18:16,400
он может выполнить  следующий пример дополнения

2204
01:18:16,400 --> 01:18:20,400
для вас, эм или, может быть, вы

2205
01:18:20,400 --> 01:18:21,920
пытаетесь выяснить грамматические или

2206
01:18:21,920 --> 01:18:24,320
орфографические ошибки, например, и вот пример

2207
01:18:24,320 --> 01:18:27,199
французского на

2208
01:18:27,199 --> 01:18:28,880
английский, английский на французский

2209
01:18:28,880 --> 01:18:31,679
регистр, так что снова вы учитесь просто делать

2210
01:18:31,679 --> 01:18:32,880
предварительную тренировку,

2211
01:18:32,880 --> 01:18:35,440
но когда вы '  переоценивая его, вы

2212
01:18:35,440 --> 01:18:36,880
даже не настраиваете модель, которую вы просто

2213
01:18:36,880 --> 01:18:39,120
предоставляете префиксы,

2214
01:18:39,120 --> 01:18:41,760
и поэтому это особенно не очень хорошо

2215
01:18:41,760 --> 01:18:42,960
понимается,

2216
01:18:42,960 --> 01:18:44,800
и есть много исследований, посвященных

2217
01:18:44,800 --> 01:18:46,800
тому, каковы

2218
01:18:46,800 --> 01:18:48,080
ограничения этого

2219
01:18:48,080 --> 01:18:49,920
так называемого контекстного обучения  но

2220
01:18:49,920 --> 01:18:52,159
это увлекательное направление для

2221
01:18:52,159 --> 01:18:54,800
будущей работы. В целом, эти модели

2222
01:18:54,800 --> 01:18:56,719
не очень хорошо изучены,

2223
01:18:56,719 --> 01:18:59,440
однако маленькие, маленькие в воздухе модели,

2224
01:18:59,440 --> 01:19:01,360
такие как Burt, стали общими инструментами в

2225
01:19:01,360 --> 01:19:03,280
широком диапазоне настроек, и у них действительно есть

2226
01:19:03,280 --> 01:19:04,800
эти i  вопросы об изучении всех этих

2227
01:19:04,800 --> 01:19:06,719
предубеждений в отношении мира, в который они войдут,

2228
01:19:06,719 --> 01:19:09,440
и дальнейших лекциях в этом курсе, ммм,

2229
01:19:10,400 --> 01:19:12,080
и так что да, то, что вы узнали на этой

2230
01:19:12,080 --> 01:19:14,239
неделе, трансформеры и предварительная подготовка

2231
01:19:14,239 --> 01:19:17,120
составляют основу или, по крайней мере, исходные данные

2232
01:19:17,120 --> 01:19:18,800
для большей части естественных  языковая обработка

2233
01:19:18,800 --> 01:19:19,679
сегодня

2234
01:19:19,679 --> 01:19:21,120
и задание пять вышло, и вы

2235
01:19:21,120 --> 01:19:24,239
сможете подробнее изучить его,

2236
01:19:24,719 --> 01:19:27,760
и со временем все в порядке,

2237
01:19:31,920 --> 01:19:33,199
да,

2238
01:19:33,199 --> 01:19:34,880
я думаю, я могу ответить на вопрос, если есть,

2239
01:19:34,880 --> 01:19:38,320
если есть, но люди тоже могут начать,

2240
01:19:45,600 --> 01:19:47,280
так что  Я думаю, я думаю, что есть вопрос

2241
01:19:47,280 --> 01:19:50,640
о дизайне, который был

2242
01:20:01,440 --> 01:20:02,840
x или

2243
01:20:02,840 --> 01:20:07,600
y, это иерархия предсказания x или y,

2244
01:20:09,120 --> 01:20:10,880
я думаю, он не указывает такого рода

2245
01:20:10,880 --> 01:20:13,120
смысл, откуда он знает, что он в

2246
01:20:13,120 --> 01:20:14,960
настоящее время будет использоваться,

2247
01:20:14,960 --> 01:20:16,719
хорошо, да, это имеет смысл  так что

2248
01:20:16,719 --> 01:20:19,600
же он сделал то, что он делает правильно, чтобы он знал

2249
01:20:19,600 --> 01:20:22,400
от кодировщика, что он должен в какой-то момент

2250
01:20:22,400 --> 01:20:25,360
предсказать x, и в какой-то момент предсказать, почему,

2251
01:20:25,360 --> 01:20:26,719
потому что кодировщик может просто

2252
01:20:26,719 --> 01:20:28,239
вспомнить, что о да, там не хватает двух вещей,

2253
01:20:28,239 --> 01:20:30,239
и если там было

2254
01:20:30,239 --> 01:20:32,719
заменено больше интервалов  будет  z, а затем

2255
01:20:32,719 --> 01:20:34,239
a, а затем ab, и вы знаете все, что угодно,

2256
01:20:34,239 --> 01:20:36,639
просто кучу уникальных ну,

2257
01:20:36,639 --> 01:20:40,560
вы знаете идентификаторы, а затем здесь, мм,

2258
01:20:40,560 --> 01:20:44,239
он должен сказать, хорошо, у меня есть напряжение,

2259
01:20:44,239 --> 01:20:45,920
я полагаю, я могу посмотреть, и я знаю, что

2260
01:20:45,920 --> 01:20:47,280
сначала я должен предсказать  это вы знаете,

2261
01:20:47,280 --> 01:20:49,280
первая замаскированная вещь, поэтому я собираюсь

2262
01:20:49,280 --> 01:20:51,360
сгенерировать это в моем декодере, а затем он, как

2263
01:20:51,360 --> 01:20:52,880
вы знаете, получает этот символ правильно, поэтому

2264
01:20:52,880 --> 01:20:54,639
мы проводим обучение, давая ему правильный

2265
01:20:54,639 --> 01:20:56,800
символ, теперь он получает этот x и говорит:

2266
01:20:56,800 --> 01:20:59,120
хорошо, я  предсказание x сейчас

2267
01:20:59,120 --> 01:21:00,880
правильное, и теперь он может предсказать предиктор

2268
01:21:00,880 --> 01:21:02,719
предсказать предсказать тогда он получит y, поэтому

2269
01:21:02,719 --> 01:21:04,080
мы проводим тренинг по принудительному обучению учителя,

2270
01:21:04,080 --> 01:21:05,679
где мы даем ему правильный ответ после

2271
01:21:05,679 --> 01:21:08,080
наказания, если он неправильный сейчас он понимает

2272
01:21:08,080 --> 01:21:09,760
это y

2273
01:21:09,760 --> 01:21:11,040
правильно и говорит хорошо, теперь у меня есть

2274
01:21:11,040 --> 01:21:12,480
чтобы предсказать, что должно происходить и почему, и это

2275
01:21:12,480 --> 01:21:14,800
может присутствовать, вы знаете, в естественных

2276
01:21:14,800 --> 01:21:16,400
частях этого, а также в том, что

2277
01:21:16,400 --> 01:21:18,159
уже предсказано здесь, потому что у

2278
01:21:18,159 --> 01:21:21,040
декодера есть напряжение внутри себя,

2279
01:21:21,040 --> 01:21:22,480
и он может видеть, что там должно происходить, поэтому

2280
01:21:22,480 --> 01:21:23,679
то, что здесь увлекательно, - это вы ''  ты делаешь

2281
01:21:23,679 --> 01:21:25,760
что-то вроде языка мо  deling, но

2282
01:21:25,760 --> 01:21:27,360
когда вы предсказываете, почему правильно,

2283
01:21:27,360 --> 01:21:29,679
вы видите, что было после этого, и я

2284
01:21:29,679 --> 01:21:30,960
думаю, что это одно из преимуществ коррупции диапазона,

2285
01:21:30,960 --> 01:21:32,320
поэтому вы делаете это,

2286
01:21:32,320 --> 01:21:33,600
когда не знаете, как долго вы

2287
01:21:33,600 --> 01:21:36,159
должны прогнозировать подобный язык  моделирование,

2288
01:21:36,159 --> 01:21:38,719
но вы узнаете, что было после

2289
01:21:38,719 --> 01:21:42,280
того, что вам не хватало

