1
00:00:05,359 --> 00:00:07,120
Привет всем,

2
00:00:07,120 --> 00:00:09,519
добро пожаловать на

3
00:00:09,519 --> 00:00:11,280
лекцию 10 по cs224n. Это будет в

4
00:00:11,280 --> 00:00:13,120
первую очередь предварительное обучение, но мы

5
00:00:13,120 --> 00:00:15,040
также

6
00:00:15,040 --> 00:00:17,600
немного обсудим модели вложенных слов и рассмотрим

7
00:00:17,600 --> 00:00:19,439
трансформаторы,

8
00:00:19,439 --> 00:00:21,359
хорошо, так что у нас есть много интересных вещей, которыми нужно

9
00:00:21,359 --> 00:00:23,119


10
00:00:23,119 --> 00:00:26,880
заняться сегодня, но некоторые напоминания о

11
00:00:26,880 --> 00:00:30,640
задание класса 5

12
00:00:30,640 --> 00:00:32,320
выходит сегодня,

13
00:00:32,320 --> 00:00:35,360
задание 4 должно было быть сдано минуту назад, так что

14
00:00:35,360 --> 00:00:36,800
если вы закончили с этим поздравлением,

15
00:00:36,800 --> 00:00:38,480
если нет, я надеюсь,

16
00:00:38,480 --> 00:00:40,719
что последние дни пройдут хорошо,

17
00:00:40,719 --> 00:00:43,280
задание 5

18
00:00:43,280 --> 00:00:46,800
касается предварительного обучения и трансформеров, так что

19
00:00:46,800 --> 00:00:47,840
эти лекции продолжаются  чтобы быть очень

20
00:00:47,840 --> 00:00:49,440
полезным для вас, и он ничего не

21
00:00:49,440 --> 00:00:52,000
касается после этих лекций,

22
00:00:52,000 --> 00:00:53,039


23
00:00:53,039 --> 00:00:56,960
ладно, так что сегодня давайте немного взглянем на то,

24
00:00:56,960 --> 00:00:58,640
каким

25
00:00:58,640 --> 00:01:00,960
будет план. Мы еще не говорили о

26
00:01:00,960 --> 00:01:02,719
моделировании вложенных слов и  вроде бы у нас

27
00:01:02,719 --> 00:01:04,799
должно быть э-э, и поэтому мы собираемся

28
00:01:04,799 --> 00:01:06,240
немного поговорить о дополнительных словах, которые вы

29
00:01:06,240 --> 00:01:09,520
видели в задании для э-э, все, что

30
00:01:09,520 --> 00:01:11,680
вы знаете, как данные, которые мы предоставили

31
00:01:11,680 --> 00:01:13,680
вам с вашей системой машинного перевода,

32
00:01:13,680 --> 00:01:15,119
но мы собираемся  поговорим немного

33
00:01:15,119 --> 00:01:17,520
о том, почему они настолько распространены в НЛП,

34
00:01:17,520 --> 00:01:20,720
потому что они используются в предварительно обученных

35
00:01:20,720 --> 00:01:23,040
моделях, я имею в виду, что они используются в

36
00:01:23,040 --> 00:01:24,880
нескольких разных моделях, но

37
00:01:24,880 --> 00:01:26,400
когда мы обсуждаем предварительное обучение,

38
00:01:26,400 --> 00:01:28,320
важно знать, что  второстепенные слова являются

39
00:01:28,320 --> 00:01:30,640
его частью, тогда мы как бы

40
00:01:30,640 --> 00:01:32,079
мотивируем, мы отправимся в другое путешествие по

41
00:01:32,079 --> 00:01:34,560
мотивации предварительного обучения мотивирующей модели

42
00:01:34,560 --> 00:01:36,079
из встраивания слов, так

43
00:01:36,079 --> 00:01:38,159
что мы уже видели предварительное обучение в некотором

44
00:01:38,159 --> 00:01:39,840
смысле в самом первом  лекцию по этому

45
00:01:39,840 --> 00:01:41,119
курсу,

46
00:01:41,119 --> 00:01:43,439
потому что мы предварительно обучили отдельные вложения слов,

47
00:01:43,439 --> 00:01:44,960
которые не принимают во внимание

48
00:01:44,960 --> 00:01:47,040
их контекст на очень больших текстовых

49
00:01:47,040 --> 00:01:48,479
корпусах, и увидели, что они могут

50
00:01:48,479 --> 00:01:51,200
закодировать много полезных вещей о

51
00:01:51,200 --> 00:01:53,040
языке,

52
00:01:53,040 --> 00:01:54,799
поэтому после того, как мы сделаем мотивацию,

53
00:01:54,799 --> 00:01:56,479
мы '  Я пройду через предварительное обучение модели

54
00:01:56,479 --> 00:01:58,399
тремя способами, и мы собираемся дать вам

55
00:01:58,399 --> 00:01:59,600
ссылку на лекцию во

56
00:01:59,600 --> 00:02:00,640
вторник, поэтому мы

57
00:02:00,640 --> 00:02:02,479
рассмотрим немного материала о трансформаторах,

58
00:02:02,479 --> 00:02:03,680
мы поговорим о

59
00:02:03,680 --> 00:02:05,680
предварительном обучении модели и декодерах  как

60
00:02:05,680 --> 00:02:07,200
декодер трансформатора  На прошлой неделе мы видели

61
00:02:07,200 --> 00:02:09,598
кодеры, а затем декодеры кодировщиков,

62
00:02:09,598 --> 00:02:11,200
в каждом из этих трех случаев

63
00:02:11,200 --> 00:02:13,040
мы собираемся немного поговорить о

64
00:02:13,040 --> 00:02:14,800
том, что вы могли бы делать,

65
00:02:14,800 --> 00:02:17,280
а затем о популярных моделях, которые используются

66
00:02:17,280 --> 00:02:20,160
в исследованиях и в отрасли, а

67
00:02:20,160 --> 00:02:21,520
затем мы  мы собираемся поговорить немного

68
00:02:21,520 --> 00:02:22,720
о том, что вы знаете, что, по нашему мнению,

69
00:02:22,720 --> 00:02:24,879
преподает предварительная подготовка, это

70
00:02:24,879 --> 00:02:26,560
будет очень кратко, на самом деле большая

71
00:02:26,560 --> 00:02:28,000
часть лекций по интерпретируемости и анализу

72
00:02:28,000 --> 00:02:29,840
за две недели будет больше говорить

73
00:02:29,840 --> 00:02:32,319
о своего рода  загадка

74
00:02:32,319 --> 00:02:34,720
и научная проблема выяснения того,

75
00:02:34,720 --> 00:02:36,640
что эти модели изучают о

76
00:02:36,640 --> 00:02:37,760
языке с

77
00:02:37,760 --> 00:02:39,280
помощью предварительных целей обучения, но

78
00:02:39,280 --> 00:02:40,959
мы как бы взглянем, а затем

79
00:02:40,959 --> 00:02:42,560
поговорим об очень больших моделях и

80
00:02:42,560 --> 00:02:44,160
обучении в контексте, так что, если вы  слышали

81
00:02:44,160 --> 00:02:45,280
о

82
00:02:45,280 --> 00:02:47,599
gpt-3, например, мы собираемся

83
00:02:47,599 --> 00:02:49,440
кратко коснуться этого здесь, и я думаю, что

84
00:02:49,440 --> 00:02:51,280
мы обсудим это больше в

85
00:02:51,280 --> 00:02:53,440
курсе позже,

86
00:02:53,440 --> 00:02:55,440
хорошо, у нас много работы, давайте сразу перейдем

87
00:02:55,440 --> 00:02:57,280


88
00:02:57,280 --> 00:03:00,480
к структуре слов  и модели Subwood,

89
00:03:00,480 --> 00:03:01,760
давай подумаем  Это своего рода

90
00:03:01,760 --> 00:03:03,120
предположения, которые мы делали в этом

91
00:03:03,120 --> 00:03:04,480
курсе до сих пор, когда мы даем вам

92
00:03:04,480 --> 00:03:06,560
задание, когда говорим об

93
00:03:06,560 --> 00:03:08,959
обучении слов для vec, например, мы

94
00:03:08,959 --> 00:03:10,480
сделали это предположение о

95
00:03:10,480 --> 00:03:12,720
словарном запасе языка, в частности, мы сделали

96
00:03:12,720 --> 00:03:14,640
предположение, что  имеет фиксированный словарный запас

97
00:03:14,640 --> 00:03:16,159
что-то вроде десятков тысяч,

98
00:03:16,159 --> 00:03:18,159
может быть, сотен тысяч, я не знаю

99
00:03:18,159 --> 00:03:19,440
количества, да,

100
00:03:19,440 --> 00:03:21,360
некоторых, относительно большого, кажется,

101
00:03:21,360 --> 00:03:23,360
количества слов, и это кажется

102
00:03:23,360 --> 00:03:26,640
вроде довольно неплохим, эм, пока, по крайней мере, в

103
00:03:26,640 --> 00:03:28,560
том, что у нас есть  готово, и мы создаем этот

104
00:03:28,560 --> 00:03:30,400
словарь из набора, который мы обучаем,

105
00:03:30,400 --> 00:03:32,159
скажите слово для проверки,

106
00:03:32,159 --> 00:03:34,000
а затем вот что важно: любое

107
00:03:34,000 --> 00:03:36,400
новое слово, любое слово, которое вы не видели

108
00:03:36,400 --> 00:03:38,080
во время обучения,

109
00:03:38,080 --> 00:03:41,280
как бы сопоставлено с одним

110
00:03:41,280 --> 00:03:43,519
токеном unk, есть другие способы  справиться с

111
00:03:43,519 --> 00:03:45,200
этим, но вы вроде как должны что-то делать,

112
00:03:45,200 --> 00:03:47,599
и частый метод заключается в том, чтобы

113
00:03:47,599 --> 00:03:50,000
вы знали, сопоставьте их все с unk, поэтому давайте рассмотрим,

114
00:03:50,000 --> 00:03:51,680
что такого рода

115
00:03:51,680 --> 00:03:53,040
средства на английском языке

116
00:03:53,040 --> 00:03:55,120
вы изучаете вложения, вы сопоставляете их все

117
00:03:55,120 --> 00:03:57,280
работает, тогда у вас есть  вари  на

118
00:03:57,280 --> 00:04:01,599
слове, например, вкусно, с кучей

119
00:04:01,599 --> 00:04:02,959
правильных букв, и ваша модель недостаточно умна,

120
00:04:02,959 --> 00:04:05,439
чтобы знать, что такого рода средства вроде

121
00:04:05,439 --> 00:04:08,879
очень вкусный, может быть, а, и поэтому она отображает его в

122
00:04:08,879 --> 00:04:11,040
unk, потому что это просто поиск в словаре

123
00:04:11,040 --> 00:04:11,920
, промах,

124
00:04:11,920 --> 00:04:14,959
а затем вы  есть опечатка, такая как обучение,

125
00:04:14,959 --> 00:04:17,040
и которая также отображает unk, ну,

126
00:04:17,040 --> 00:04:18,399
потенциально, если этого не было в вашем

127
00:04:18,399 --> 00:04:20,160
тренировочном наборе, вы знаете, что некоторые люди делают

128
00:04:20,160 --> 00:04:21,918
опечатки, но не все из них будут

129
00:04:21,918 --> 00:04:23,520
видны во время обучения, и тогда у вас

130
00:04:23,520 --> 00:04:25,199
будут новые предметы правильно  так что это может

131
00:04:25,199 --> 00:04:27,280
быть первый раз, когда вы когда-либо видели вас

132
00:04:27,280 --> 00:04:29,440
в качестве студента в 2224n, когда вы видели

133
00:04:29,440 --> 00:04:31,440
слово преобразовать

134
00:04:31,440 --> 00:04:32,800
эм, но

135
00:04:32,800 --> 00:04:34,720
я чувствую, что вы

136
00:04:34,720 --> 00:04:36,240
как бы понимаете, что оно должно означать, например,

137
00:04:36,240 --> 00:04:38,800
может быть, добавить трансформаторы или повернуть

138
00:04:38,800 --> 00:04:40,479
использовать трансформаторы или превратить в

139
00:04:40,479 --> 00:04:42,160
трансформатор или что-то в этом роде, и

140
00:04:42,160 --> 00:04:43,759
это также будет отображаться в unk,

141
00:04:43,759 --> 00:04:46,000
даже если вы видели трансформатор, и

142
00:04:46,000 --> 00:04:48,080
если я в

143
00:04:48,080 --> 00:04:51,040
порядке, и поэтому каким-то образом мы

144
00:04:51,040 --> 00:04:53,600
должны прийти к выводу, что смотрим на слова

145
00:04:53,600 --> 00:04:56,400
как на просто  как отдельные последовательности

146
00:04:56,400 --> 00:04:59,040
символов, которые вы знаете, однозначно идентифицирует

147
00:04:59,040 --> 00:05:00,320
это слово, и это своего рода то, как мы

148
00:05:00,320 --> 00:05:01,600
должны параметризовать вещи, это просто

149
00:05:01,600 --> 00:05:03,440
неправильно,

150
00:05:03,440 --> 00:05:05,759
и так

151
00:05:05,759 --> 00:05:07,440
что это верно не только для английского, но и для

152
00:05:07,440 --> 00:05:09,199
многих языков, это предположение о ограниченном словарном запасе

153
00:05:09,199 --> 00:05:10,960
имеет еще меньше смысла, поэтому

154
00:05:10,960 --> 00:05:12,880
уже оно не имеет  смысл в английском,

155
00:05:12,880 --> 00:05:15,840
но английский - это не худшее для

156
00:05:15,840 --> 00:05:17,199
английского, поэтому

157
00:05:17,199 --> 00:05:19,840
морфология -

158
00:05:19,840 --> 00:05:22,080
это изучение структуры слов, а

159
00:05:22,080 --> 00:05:23,600
английский язык, как известно, имеет довольно простую

160
00:05:23,600 --> 00:05:26,160
морфологию в

161
00:05:26,160 --> 00:05:29,280
некотором смысле, а когда

162
00:05:29,280 --> 00:05:31,360
языки имеют сложную морфологию, это

163
00:05:31,360 --> 00:05:32,479
означает  у вас есть

164
00:05:32,479 --> 00:05:34,720
более длинные слова, более сложные слова, которые

165
00:05:34,720 --> 00:05:37,199
изменяются больше, и каждое из них

166
00:05:37,199 --> 00:05:39,199
встречается реже, и это должно

167
00:05:39,199 --> 00:05:40,560
звучать как проблема.

168
00:05:40,560 --> 00:05:42,400


169
00:05:42,400 --> 00:05:46,240


170
00:05:46,240 --> 00:05:47,759
он не будет отображаться в вашем

171
00:05:47,759 --> 00:05:49,039
наборе тестов, никогда в вашем наборе обучения теперь

172
00:05:49,039 --> 00:05:50,400
он сопоставлен с unk, и вы не знаете,

173
00:05:50,400 --> 00:05:51,600
что делать

174
00:05:51,600 --> 00:05:54,000
, например, вы знаете, что глаголы на суахили могут

175
00:05:54,000 --> 00:05:58,160
иметь сотни спряжений  Таким образом,

176
00:05:58,160 --> 00:06:00,400
каждое спряжение кодирует важную

177
00:06:00,400 --> 00:06:02,479
информацию о предложении, которое на

178
00:06:02,479 --> 00:06:04,720
английском языке может быть представлено с помощью

179
00:06:04,720 --> 00:06:07,360
дополнительных слов, но в суахили оно отображается

180
00:06:07,360 --> 00:06:10,000
на глагол, поскольку вы знаете префиксы,

181
00:06:10,000 --> 00:06:11,280
суффиксы и тому подобное, это называется

182
00:06:11,280 --> 00:06:13,280
флективной морфологией, и поэтому у вас могут

183
00:06:13,280 --> 00:06:14,639
быть сотни  спряжения, я просто как

184
00:06:14,639 --> 00:06:17,039
бы вставил этот

185
00:06:17,039 --> 00:06:20,080
блок викисловаря, просто чтобы дать вам небольшую выборку

186
00:06:20,080 --> 00:06:21,600
из огромного количества

187
00:06:21,600 --> 00:06:23,680
спряжения, и поэтому пытаться запомнить

188
00:06:23,680 --> 00:06:25,280
независимо значение каждого из

189
00:06:25,280 --> 00:06:27,039
этих слов - просто неправильный

190
00:06:27,039 --> 00:06:29,360
ответ

191
00:06:31,840 --> 00:06:33,039
Итак,

192
00:06:33,039 --> 00:06:34,160
это будет очень краткий

193
00:06:34,160 --> 00:06:35,919
обзор, и что мы собираемся сделать,

194
00:06:35,919 --> 00:06:37,919
это взять один

195
00:06:37,919 --> 00:06:40,560
, скажем так, класс

196
00:06:40,560 --> 00:06:43,440
алгоритмов для моделирования

197
00:06:43,440 --> 00:06:45,360
подслов, которые были

198
00:06:45,360 --> 00:06:48,960
разработаны, чтобы попытаться найти золотую середину

199
00:06:48,960 --> 00:06:52,479
между двумя вариантами, как говорит один из вариантов

200
00:06:52,479 --> 00:06:54,720
все как отдельные слова,

201
00:06:54,720 --> 00:06:56,240
либо я знаю слово, и я видел его во

202
00:06:56,240 --> 00:06:58,080
время тренировки, либо я не знаю слово,

203
00:06:58,080 --> 00:06:59,919
и это похоже на unk, а затем

204
00:06:59,919 --> 00:07:01,840
еще одна крайность  вариант - сказать, что это

205
00:07:01,840 --> 00:07:03,759
просто символы,

206
00:07:03,759 --> 00:07:05,120
так, как будто я получаю последовательность

207
00:07:05,120 --> 00:07:07,360
символов, а затем моя нейронная сеть

208
00:07:07,360 --> 00:07:09,520
поверх моей последовательности символов должна

209
00:07:09,520 --> 00:07:10,319
выучить

210
00:07:10,319 --> 00:07:12,240
все, должна научиться комбинировать

211
00:07:12,240 --> 00:07:14,800
слова и прочее, поэтому модели подслов в

212
00:07:14,800 --> 00:07:16,880
целом просто означают просмотр  своего

213
00:07:16,880 --> 00:07:18,479
рода внутренняя структура слов, которая каким-то образом

214
00:07:18,479 --> 00:07:20,639
выглядит ниже уровня слов, но эта

215
00:07:20,639 --> 00:07:22,080


216
00:07:22,080 --> 00:07:24,319
группа моделей будет пытаться найти

217
00:07:24,319 --> 00:07:28,000
золотую середину, поэтому при кодировании пар байтов

218
00:07:28,160 --> 00:07:30,880
то, что мы собираемся сделать, это

219
00:07:30,880 --> 00:07:33,199
выучить словарь из  набор обучающих данных

220
00:07:33,199 --> 00:07:34,639
снова, так что теперь у нас есть набор обучающих данных

221
00:07:34,639 --> 00:07:36,639
вместо того, чтобы просто говорить о, все,

222
00:07:36,639 --> 00:07:39,120
что было разделено моим эвристическим разделителем слов,

223
00:07:39,120 --> 00:07:40,400
например

224
00:07:40,400 --> 00:07:42,720
пробелы в английском языке, например,

225
00:07:42,720 --> 00:07:45,599
будет словом в моем словаре, мы собираемся

226
00:07:45,599 --> 00:07:46,720


227
00:07:46,720 --> 00:07:49,360
выучить словарь  используя жадный

228
00:07:49,360 --> 00:07:51,039
алгоритм в этом случае, вот что

229
00:07:51,039 --> 00:07:53,039
мы собираемся сделать, мы начнем со

230
00:07:53,039 --> 00:07:55,360
словаря, содержащего только символы, так

231
00:07:55,360 --> 00:07:57,520
что это наша крайняя правая, это наша,

232
00:07:57,520 --> 00:07:59,280
по крайней мере, если вы видели все

233
00:07:59,280 --> 00:08:00,720
символы

234
00:08:00,720 --> 00:08:02,879
Тогда вы знаете, что у вас никогда не

235
00:08:02,879 --> 00:08:04,720
будет права unk, потому что вы видите слово

236
00:08:04,720 --> 00:08:05,919
, которое никогда не видели, прежде чем вы просто

237
00:08:05,919 --> 00:08:07,759
разделите его на его символы, а затем

238
00:08:07,759 --> 00:08:09,360
вы попытаетесь увидеть, как вы знаете, как поступить с ним

239
00:08:09,360 --> 00:08:11,360
таким образом,

240
00:08:11,360 --> 00:08:13,840
а затем также  символ конца слова,

241
00:08:13,840 --> 00:08:15,039
а затем мы будем перебирать этот

242
00:08:15,039 --> 00:08:16,960
алгоритм, мы скажем, что используйте корпус

243
00:08:16,960 --> 00:08:20,400
текста, найдите общие смежные буквы, поэтому,

244
00:08:20,400 --> 00:08:22,080
возможно, a и b очень часто являются

245
00:08:22,080 --> 00:08:23,919
смежными,

246
00:08:23,919 --> 00:08:24,800


247
00:08:24,800 --> 00:08:26,720
добавьте их пару вместе

248
00:08:26,720 --> 00:08:28,879
как одно подслово в свой

249
00:08:28,879 --> 00:08:30,240
словарь

250
00:08:30,240 --> 00:08:32,000
теперь замените экземпляры этой

251
00:08:32,000 --> 00:08:33,760
пары символов новым вложенным словом, повторяйте

252
00:08:33,760 --> 00:08:35,760
до желаемого размера словарного запаса, так что, возможно,

253
00:08:35,760 --> 00:08:38,080
вы начнете с небольшого словаря символов,

254
00:08:38,080 --> 00:08:40,880
а затем получите тот

255
00:08:40,880 --> 00:08:43,760
же небольшой словарь символов плюс

256
00:08:43,760 --> 00:08:46,640
кучу целых слов или

257
00:08:46,640 --> 00:08:48,560
частей слов  так что обратите внимание, как яблоко целое

258
00:08:48,560 --> 00:08:51,200
слово выглядит как яблоко, но тогда приложение, может быть,

259
00:08:51,200 --> 00:08:53,200
это своего рода первая часть,

260
00:08:53,200 --> 00:08:56,959
первое подслово приложения или

261
00:08:56,959 --> 00:08:59,120
да,

262
00:08:59,120 --> 00:09:00,880
а затем ли,

263
00:09:00,880 --> 00:09:03,680
я думаю, я не должен был помещать

264
00:09:03,680 --> 00:09:05,760
туда хеш,

265
00:09:05,760 --> 00:09:07,680
но вы знаете, может быть  вы выучили lee как,

266
00:09:07,680 --> 00:09:10,480
например, конец слова,

267
00:09:10,480 --> 00:09:13,440
и в итоге

268
00:09:13,440 --> 00:09:16,000
вы получите словарный запас, в котором общие вещи

269
00:09:16,000 --> 00:09:18,560
вы можете сопоставить сами с собой, а затем

270
00:09:18,560 --> 00:09:20,399
редкие последовательности символов, которые вы как бы

271
00:09:20,399 --> 00:09:22,800
разделите как можно меньше,

272
00:09:22,800 --> 00:09:24,399
и это  не всегда заканчивается так хорошо,

273
00:09:24,399 --> 00:09:26,399
что вы изучаете морфологически

274
00:09:26,399 --> 00:09:28,720
релевантные суффиксы, такие как lee,

275
00:09:28,720 --> 00:09:31,600
но вы знаете, что пытаетесь разбить вещи

276
00:09:31,600 --> 00:09:33,440
несколько разумно, и, если у вас

277
00:09:33,440 --> 00:09:35,920
достаточно данных, вспомогательный словарь, который вы изучаете,

278
00:09:35,920 --> 00:09:37,760
имеет тенденцию быть нормальным, поэтому он изначально

279
00:09:37,760 --> 00:09:40,640
использовался в  машинный перевод, а теперь

280
00:09:40,640 --> 00:09:43,040
похожий фрагмент слова метода, который

281
00:09:43,040 --> 00:09:45,040
мы не будем рассматривать в этой лекции, используется в

282
00:09:45,040 --> 00:09:46,880
предварительно обученных моделях, но вы знаете, что

283
00:09:46,880 --> 00:09:48,480
идея фактически такая же, и в итоге вы

284
00:09:48,480 --> 00:09:50,080
получаете словари, которые очень похожи на

285
00:09:50,080 --> 00:09:52,730
это, так что  если мы вернемся к нашей эм

286
00:09:52,730 --> 00:09:54,560
[музыке],

287
00:09:54,560 --> 00:09:56,880
если мы вернемся к нашим примерам, когда

288
00:09:56,880 --> 00:10:00,959
вы знаете, что уровень слов nlp подводил нас,

289
00:10:00,959 --> 00:10:02,720


290
00:10:02,720 --> 00:10:04,800
тогда у вас есть отображение шляпы в шляпу,

291
00:10:04,800 --> 00:10:06,800
хорошо, у вас есть отображение шляпы в шляпу,

292
00:10:06,800 --> 00:10:08,240
потому что это было обычным  Eno  тьфу

293
00:10:08,240 --> 00:10:10,079
последовательность символов, которая была

294
00:10:10,079 --> 00:10:11,760
фактически включена в наш

295
00:10:11,760 --> 00:10:13,200
словарь

296
00:10:13,200 --> 00:10:14,560
подслов, а затем вы должны научиться отображать, чтобы

297
00:10:14,560 --> 00:10:16,640
выучить такие общие слова хорошо,

298
00:10:16,640 --> 00:10:18,240
а это означает, что модель нейронной

299
00:10:18,240 --> 00:10:19,680
сети, с которой вы собираетесь обрабатывать

300
00:10:19,680 --> 00:10:23,200
этот текст, не нуждается  сказать,

301
00:10:23,200 --> 00:10:25,360
объединить буквы обучения и шляпы

302
00:10:25,360 --> 00:10:27,680
, чтобы попытаться понравиться, получить

303
00:10:27,680 --> 00:10:29,680
значение этих слов из

304
00:10:29,680 --> 00:10:31,040
букв, потому что вы можете себе представить, что это

305
00:10:31,040 --> 00:10:33,440
может быть сложно, но затем, когда вы

306
00:10:33,440 --> 00:10:36,399
получаете слово, которое вы не видели

307
00:10:36,399 --> 00:10:39,200
раньше,  в состоянии разложить его на части,

308
00:10:39,200 --> 00:10:41,600
и поэтому, если вы видели вкусное с разным

309
00:10:41,600 --> 00:10:45,200
количеством знаков a во время торговли,

310
00:10:45,200 --> 00:10:47,279
вы знаете, может быть, вы действительно получаете некоторые из

311
00:10:47,279 --> 00:10:49,680
тех же дополнительных слов или похожих дополнительных слов, на

312
00:10:49,680 --> 00:10:51,200
которые вы разбиваете его во

313
00:10:51,200 --> 00:10:52,720
время оценки, поэтому мы никогда не думали

314
00:10:52,720 --> 00:10:54,640
достаточно вкусно, чтобы понравиться вам, знаете,

315
00:10:54,640 --> 00:10:56,640
сколько а, чтобы добавить его в

316
00:10:56,640 --> 00:10:58,959
наш словарь подслов, эм, но мы все

317
00:10:58,959 --> 00:11:00,880
еще можем разделить его на вещи, а

318
00:11:00,880 --> 00:11:02,560
затем нейронную сеть, которая работает

319
00:11:02,560 --> 00:11:05,120
поверх этих субвложений  можно было бы

320
00:11:05,120 --> 00:11:06,880
в некотором роде внушить, что о да, это

321
00:11:06,880 --> 00:11:08,880
одна из тех вещей, где такие люди, как

322
00:11:08,880 --> 00:11:09,839
вы, знают, что

323
00:11:09,839 --> 00:11:11,600
цепные буквы соединяются в цепочку гласных

324
00:11:11,600 --> 00:11:15,040
в английском для акцента,

325
00:11:15,040 --> 00:11:17,360
поэтому орфографические ошибки все еще в значительной степени мешают

326
00:11:17,360 --> 00:11:20,000
вам, так что теперь учиться

327
00:11:20,000 --> 00:11:21,519
с этой ошибкой может быть сопоставлено

328
00:11:21,519 --> 00:11:23,279
два дополнительных слова, но если вы видели

329
00:11:23,279 --> 00:11:25,360
такие орфографические ошибки достаточно часто,

330
00:11:25,360 --> 00:11:27,760
возможно, вы могли бы научиться как-то справляться с

331
00:11:27,760 --> 00:11:30,079
этим, это все еще портит модель,

332
00:11:30,079 --> 00:11:31,760
и, ммм,

333
00:11:31,760 --> 00:11:33,200
но, по крайней мере, это не просто

334
00:11:33,200 --> 00:11:34,959
недоработка, это кажется явно лучше, чем

335
00:11:34,959 --> 00:11:37,760
это, а затем  трансформировать, возможно,

336
00:11:37,760 --> 00:11:40,160
в лучшем случае, это своего рода оптимизм, но,

337
00:11:40,160 --> 00:11:42,160
может быть, в лучшем случае правильно, вы

338
00:11:42,160 --> 00:11:44,560
смогли сказать ах, да, это трансформер,

339
00:11:44,560 --> 00:11:46,800
и если я

340
00:11:46,800 --> 00:11:49,120
снова эм, дополнительные слова, которые вы изучаете

341
00:11:49,120 --> 00:11:50,639
, на самом деле, как правило, не так хорошо

342
00:11:50,639 --> 00:11:52,800
морфологически мотивированы.  думаю, что если

343
00:11:52,800 --> 00:11:55,120
я похож на суффикс clear like

344
00:11:55,120 --> 00:11:57,279
в английском языке, который имеет очень распространенное

345
00:11:57,279 --> 00:11:59,120
и воспроизводимое значение, когда вы применяете его

346
00:11:59,120 --> 00:12:01,120
к существительным, это производная

347
00:12:01,120 --> 00:12:04,000
морфология, но вы знаете, что

348
00:12:04,000 --> 00:12:06,079
можете сортировать  of comb составить слово

349
00:12:06,079 --> 00:12:08,079
tr значение transformerify, возможно,

350
00:12:08,079 --> 00:12:10,800
из его двух составляющих подслова,

351
00:12:10,800 --> 00:12:12,800
и поэтому, когда мы говорим о словах,

352
00:12:12,800 --> 00:12:14,959
вводимых в модели трансформаторов, предварительно обученные

353
00:12:14,959 --> 00:12:16,399
модели трансформаторов на протяжении

354
00:12:16,399 --> 00:12:18,240
всей этой лекции мы будем

355
00:12:18,240 --> 00:12:21,360
говорить о подсловах, так что  Я мог бы сказать

356
00:12:21,360 --> 00:12:24,000
слово э-э, и я имею в виду, что вы знаете,

357
00:12:24,000 --> 00:12:27,040
возможно, полное слово, а также, возможно, дополнительное

358
00:12:27,040 --> 00:12:28,880
слово, хорошо, поэтому, когда мы произносим последовательность

359
00:12:28,880 --> 00:12:30,320
слов, трансформатор, предварительно обученный

360
00:12:30,320 --> 00:12:33,040
трансформатор не имеет никакого представления о том

361
00:12:33,040 --> 00:12:36,320
, имеет ли он дело со словами или под-  слова,

362
00:12:36,320 --> 00:12:38,240
эм, когда он выполняет операции с вниманием,

363
00:12:38,240 --> 00:12:40,079


364
00:12:40,079 --> 00:12:41,920
и это может быть проблемой, которую вы можете

365
00:12:41,920 --> 00:12:43,920
себе представить, если у вас действительно странные

366
00:12:43,920 --> 00:12:45,920
последовательности символов, у вас действительно может

367
00:12:45,920 --> 00:12:48,639
быть отдельное слово, сопоставленное с

368
00:12:48,639 --> 00:12:51,279
таким количеством подслов, сколько в нем символов,

369
00:12:51,279 --> 00:12:52,800
которые могут быть проблемой  потому что внезапно

370
00:12:52,800 --> 00:12:55,120
вы знаете, что у вас есть предложение из 10 слов, но

371
00:12:55,120 --> 00:12:57,360
одно из слов сопоставлено с вы знаете

372
00:12:57,360 --> 00:12:59,680
20 дополнительных слов, теперь у вас есть

373
00:12:59,680 --> 00:13:01,519
предложение из 30 слов, где 20 из 30 слов - это

374
00:13:01,519 --> 00:13:04,320
всего лишь одно э-э, настоящее  слово um, так что имейте это в

375
00:13:04,320 --> 00:13:06,000
виду, но

376
00:13:06,000 --> 00:13:08,000
вы знаете, я думаю, что это важно для своего

377
00:13:08,000 --> 00:13:09,760
рода предположения об открытом словарном запасе, это

378
00:13:09,760 --> 00:13:11,040
важно на английском языке, и это даже более

379
00:13:11,040 --> 00:13:14,399
важно на многих других языках

380
00:13:14,959 --> 00:13:16,480
и фактическом алгоритме, я имею в виду, что вы можете

381
00:13:16,480 --> 00:13:17,680
войти в фактические алгоритмы, которые у них есть

382
00:13:17,680 --> 00:13:18,959
сделано для этой

383
00:13:18,959 --> 00:13:20,560
пары укусов - это своего рода мой

384
00:13:20,560 --> 00:13:23,279
любимый вариант для краткого обзора части слова,

385
00:13:23,279 --> 00:13:26,880
вы также можете взглянуть на

386
00:13:27,040 --> 00:13:28,160
хорошо,

387
00:13:28,160 --> 00:13:30,320
любые вопросы по подсловам,

388
00:13:30,320 --> 00:13:34,200
я думаю, джон эм,

389
00:13:34,639 --> 00:13:37,040
о, отличный замечательный момент, так что это означает, что

390
00:13:37,040 --> 00:13:39,519
вы должны комбинировать эту подзаголовок, чтобы это

391
00:13:39,519 --> 00:13:41,760
Подслово не является концом слова,

392
00:13:41,760 --> 00:13:43,760
таа

393
00:13:43,760 --> 00:13:45,760
является своего рода сообщением модели, поэтому, если у меня было

394
00:13:45,760 --> 00:13:48,480
таа без хешей

395
00:13:48,480 --> 00:13:50,399
, это отдельное подслово, что означает, что

396
00:13:50,399 --> 00:13:52,560
есть целое слово, которое является

397
00:13:52,560 --> 00:13:54,240
та, или, по крайней мере, это не

398
00:13:54,240 --> 00:13:56,399
конец слова, см.  как здесь у меня

399
00:13:56,399 --> 00:13:58,320
нет хэшей в конце, потому что

400
00:13:58,320 --> 00:13:59,760
это указывает на то, что это в

401
00:13:59,760 --> 00:14:02,320
конце слова, разные схемы слов, э-э,

402
00:14:02,320 --> 00:14:03,680
различаются, следует ли вам ставить

403
00:14:03,680 --> 00:14:05,040
что-то в начале слова,

404
00:14:05,040 --> 00:14:07,600
если это действительно похоже на начало слова  слово или если вы

405
00:14:07,600 --> 00:14:08,560
должны поставить что-то в конце

406
00:14:08,560 --> 00:14:11,120
слова, если оно не заканчивает слово правильно, поэтому,

407
00:14:11,120 --> 00:14:12,800
когда токенизатор обрабатывает ваши

408
00:14:12,800 --> 00:14:14,399
данные, поэтому у вас есть что-то, что

409
00:14:14,399 --> 00:14:17,519
токенизирует, вы знаете это предложение в

410
00:14:17,519 --> 00:14:18,880
худшем случае,

411
00:14:18,880 --> 00:14:22,800
о да  в худшем случае

412
00:14:22,800 --> 00:14:24,800
он говорит как n, это целое слово, дайте

413
00:14:24,800 --> 00:14:27,040
ему только слово без хешей, это

414
00:14:27,040 --> 00:14:29,199
целое слово, дайте ему только слово без

415
00:14:29,199 --> 00:14:31,120
хешей, а затем,

416
00:14:31,120 --> 00:14:32,000
может быть,

417
00:14:32,000 --> 00:14:34,880
здесь, в дополнительных словах, правильно, у нас есть

418
00:14:34,880 --> 00:14:36,959
это странное слово, вложенные слова  и он разбивает

419
00:14:36,959 --> 00:14:40,560
его на под-слово и слова, и поэтому под-

420
00:14:40,560 --> 00:14:42,320
он собирается дать ему под-слово с

421
00:14:42,320 --> 00:14:44,720
под-хешем,

422
00:14:44,720 --> 00:14:46,399
чтобы указать, что это часть этого

423
00:14:46,399 --> 00:14:47,760
большего

424
00:14:47,760 --> 00:14:50,720
под-слова слова, в отличие от слова

425
00:14:50,720 --> 00:14:53,040
под-как подводная лодка, которое было бы

426
00:14:53,040 --> 00:14:54,160
другим,

427
00:14:54,160 --> 00:14:57,800
да, это здорово  вопрос,

428
00:15:02,160 --> 00:15:04,560
хорошо,

429
00:15:04,720 --> 00:15:07,279
здорово, так что это была наша заметка о

430
00:15:07,279 --> 00:15:10,000
моделировании дополнительных слов, и вы знаете, что

431
00:15:10,000 --> 00:15:12,800
дополнительные слова важны, например

432
00:15:12,800 --> 00:15:13,680


433
00:15:13,680 --> 00:15:15,199
, вы знаете много приложений для перевода

434
00:15:15,199 --> 00:15:16,800
, поэтому мы дали вам дополнительные

435
00:15:16,800 --> 00:15:18,399
слова в задании на машинный перевод

436
00:15:18,399 --> 00:15:20,560
.  говорить о тебе  t моделируйте

437
00:15:20,560 --> 00:15:23,040
предварительное обучение и встраивание слов, так что

438
00:15:23,040 --> 00:15:24,959
мне очень нравится, что я могу перейти к этому

439
00:15:24,959 --> 00:15:27,120
слайду, поэтому мы видели эту цитату в

440
00:15:27,120 --> 00:15:28,560
начале урока, вы должны знать

441
00:15:28,560 --> 00:15:30,720
слово компании, которую он держит, и это

442
00:15:30,720 --> 00:15:32,160
было своего рода одним из  вещи, которые мы

443
00:15:32,160 --> 00:15:34,079
использовали для обобщения семантики распределения,

444
00:15:34,079 --> 00:15:36,399
эта идея о том, что слово для вектора

445
00:15:36,399 --> 00:15:38,560
была в некотором роде хорошо мотивирована,

446
00:15:38,560 --> 00:15:40,320
потому что значение слова можно

447
00:15:40,320 --> 00:15:43,120
рассматривать как производное от

448
00:15:43,120 --> 00:15:44,959
вида статистики совпадения

449
00:15:44,959 --> 00:15:48,560
слов, которые вместе встречаются вокруг

450
00:15:48,560 --> 00:15:50,720
это было просто потрясающе эффективным, я

451
00:15:50,720 --> 00:15:51,839
думаю,

452
00:15:51,839 --> 00:15:53,600
но есть еще одна цитата

453
00:15:53,600 --> 00:15:55,279
от того же человека, так что у нас есть jr

454
00:15:55,279 --> 00:15:58,639
firth um 1935 по сравнению с нашей предыдущей цитатой

455
00:15:58,639 --> 00:16:01,440
из 1957 года, а вторая цитата

456
00:16:01,440 --> 00:16:03,759
говорит, что полное значение слова

457
00:16:03,759 --> 00:16:05,360
всегда контекстно

458
00:16:05,360 --> 00:16:07,040
и нет  к изучению значения отдельно от

459
00:16:07,040 --> 00:16:10,320
полного контекста можно отнестись серьезно,

460
00:16:10,320 --> 00:16:12,160
теперь снова это просто вещи, о которых мы

461
00:16:12,160 --> 00:16:14,560
можем как бы подумать и разобрать,

462
00:16:14,560 --> 00:16:17,600
но это сразу приходит на ум,

463
00:16:17,600 --> 00:16:19,519
когда вы вставляете слова со словом в век

464
00:16:19,519 --> 00:16:21,440
одна из проблем заключается в том, что вы на

465
00:16:21,440 --> 00:16:23,440
самом деле не смотрите на его соседей, когда

466
00:16:23,440 --> 00:16:25,680
даете и встраиваете,

467
00:16:25,680 --> 00:16:28,880
поэтому, если у меня есть предложение, я записываю

468
00:16:28,880 --> 00:16:31,440
запись, вы знаете, что два экземпляра

469
00:16:31,440 --> 00:16:33,120
записи

470
00:16:33,120 --> 00:16:34,800
означают разные вещи, но им

471
00:16:34,800 --> 00:16:37,279
дается  одно и то же слово для поддержки встраивания

472
00:16:37,279 --> 00:16:38,800
правильно, потому что в word to vect вы

473
00:16:38,800 --> 00:16:40,720
берете строку, которую вы сопоставляете с ней, о, я видел

474
00:16:40,720 --> 00:16:42,800
запись слова, прежде чем вы получите

475
00:16:42,800 --> 00:16:46,000
такой вектор uh из вашей изученной матрицы,

476
00:16:46,000 --> 00:16:47,199
и вы дадите ему одно и то же в обоих

477
00:16:47,199 --> 00:16:49,279
случаях

478
00:16:49,279 --> 00:16:50,639
и  так что то, что мы собираемся делать

479
00:16:50,639 --> 00:16:53,920
сегодня, на самом деле концептуально не так уж и

480
00:16:53,920 --> 00:16:56,240
отличается от обучения слова до

481
00:16:56,240 --> 00:16:58,399
обучения вектору слова, которое вы можете рассматривать как

482
00:16:58,399 --> 00:17:00,720
предварительное обучение, просто очень простую модель,

483
00:17:00,720 --> 00:17:03,120
которая только назначает индивидуальный

484
00:17:03,120 --> 00:17:05,599
вектор каждому уникальному типу слова, каждый уникальный

485
00:17:05,599 --> 00:17:07,679
элемент в вашем словарном запасе сегодня мы пойдем

486
00:17:07,679 --> 00:17:10,079
намного дальше, но

487
00:17:10,079 --> 00:17:11,839
идея очень похожа,

488
00:17:11,839 --> 00:17:14,400
так что, как вы знаете, в 2017 году мы бы начали

489
00:17:14,400 --> 00:17:16,160
с предварительно обученных встраиваний

490
00:17:16,160 --> 00:17:18,400
слов и снова не запомните никакого контекста,

491
00:17:18,400 --> 00:17:20,319
поэтому вы дадите слово и  набивать  dding

492
00:17:20,319 --> 00:17:22,079
независимо от контекста, в котором он

493
00:17:22,079 --> 00:17:24,240
появляется, а затем вы узнаете, как

494
00:17:24,240 --> 00:17:25,679
включить контекст, это не похоже на то, что

495
00:17:25,679 --> 00:17:28,400
наши модели nlp никогда не использовали контекст правильно,

496
00:17:28,400 --> 00:17:30,080
вместо этого вы научились бы

497
00:17:30,080 --> 00:17:32,559
включать контекст, используя вы знаете свой lstm или,

498
00:17:32,559 --> 00:17:34,640
если это позже в 2017 году, вы знаете  ваш

499
00:17:34,640 --> 00:17:36,880
трансформатор

500
00:17:36,880 --> 00:17:38,480
, и вы научитесь включать

501
00:17:38,480 --> 00:17:41,520
контекст во время обучения задачам,

502
00:17:41,520 --> 00:17:43,760
чтобы у вас был некоторый надзор, возможно, это надзор за

503
00:17:43,760 --> 00:17:45,360
машинным переводом, может быть,

504
00:17:45,360 --> 00:17:47,440
сантименты, может быть, отвечая на вопрос, и

505
00:17:47,440 --> 00:17:48,640
вы бы узнали, как включить

506
00:17:48,640 --> 00:17:50,480
контекст в свой в свой lstm или

507
00:17:50,480 --> 00:17:51,600
иначе

508
00:17:51,600 --> 00:17:54,240
через сигнал  обучения

509
00:17:54,240 --> 00:17:55,679
вместо того, чтобы говорить через слово к сигналу x,

510
00:17:55,679 --> 00:17:57,600
и, таким образом, вы знаете, как

511
00:17:57,600 --> 00:17:59,600
пиктографически, у вас есть эти

512
00:17:59,600 --> 00:18:01,440
вложения слов здесь, поэтому красный цвет - это своего

513
00:18:01,440 --> 00:18:02,960
рода ваше слово для векторных ставок,

514
00:18:02,960 --> 00:18:04,559
и они предварительно обучены

515
00:18:04,559 --> 00:18:06,400
, а те, которые занимают некоторые из

516
00:18:06,400 --> 00:18:08,559
параметры вашей сети, а затем

517
00:18:08,559 --> 00:18:10,000
у вас есть контекстуализация, теперь

518
00:18:10,000 --> 00:18:11,360
это выглядит как lstm, но это может быть

519
00:18:11,360 --> 00:18:12,960
что угодно, поэтому это может  быть

520
00:18:12,960 --> 00:18:14,960
двунаправленным, вы знаете, что кодировщик

521
00:18:14,960 --> 00:18:17,039
здесь не предварительно обучен, и теперь

522
00:18:17,039 --> 00:18:18,400
это много параметров, которые

523
00:18:18,400 --> 00:18:20,240
предварительно не обучены, а затем, возможно, у вас есть какая-

524
00:18:20,240 --> 00:18:22,400
то функция считывания в конце

525
00:18:22,400 --> 00:18:24,480
справа, чтобы предсказать, что вы

526
00:18:24,480 --> 00:18:25,679
пытаетесь  чтобы предсказать еще раз, может быть, это

527
00:18:25,679 --> 00:18:28,160
сантименты, может быть, вы делаете, я не

528
00:18:28,160 --> 00:18:30,559
знаю темы, обозначающие все, что вы хотите

529
00:18:30,559 --> 00:18:32,400
сделать, это своего рода парадигма, как будто вы

530
00:18:32,400 --> 00:18:33,840
устанавливаете какую-то архитектуру, и вы

531
00:18:33,840 --> 00:18:36,559
только предварительно обучаете вложения слов,

532
00:18:36,559 --> 00:18:38,880
и поэтому это не  на самом деле

533
00:18:38,880 --> 00:18:40,960
концептуально обязательно самая большая

534
00:18:40,960 --> 00:18:43,360
проблема,

535
00:18:43,360 --> 00:18:44,960
потому что вы знаете, что

536
00:18:44,960 --> 00:18:47,039
мы любим думать о вещах глубокого обучения,

537
00:18:47,039 --> 00:18:48,960
что у нас есть много обучающих

538
00:18:48,960 --> 00:18:50,880
данных для наших целей, я имею в виду

539
00:18:50,880 --> 00:18:52,320
одну из вещей, которые мы

540
00:18:52,320 --> 00:18:55,520
мотивировали, вы знаете, очень глубоко  нейронные

541
00:18:55,520 --> 00:18:56,960
сети в том, что они могут принимать

542
00:18:56,960 --> 00:18:58,720
много данных и изучать из них шаблоны,

543
00:18:58,720 --> 00:19:00,960
но это накладывает бремя на наши

544
00:19:00,960 --> 00:19:05,120
последующие данные, чтобы их было достаточно

545
00:19:05,120 --> 00:19:06,880
для обучения контекстным аспектам

546
00:19:06,880 --> 00:19:09,360
языка, так что вы можете представить, если вы  только у

547
00:19:09,360 --> 00:19:11,360
вас есть немного, что вы знаете помеченные

548
00:19:11,360 --> 00:19:13,360
данные для точной настройки, вы придаете

549
00:19:13,360 --> 00:19:16,240
этим данным довольно большую роль, чтобы сказать: эй,

550
00:19:16,240 --> 00:19:17,760
может быть, вот несколько предварительно обученных встраиваний,

551
00:19:17,760 --> 00:19:19,520
но например, как вы обрабатываете подобные предложения

552
00:19:19,520 --> 00:19:21,440
и как они составляются и все такое

553
00:19:21,440 --> 00:19:23,280
это зависит от вас,

554
00:19:23,280 --> 00:19:24,640
поэтому, если у вас нет большого количества помеченных

555
00:19:24,640 --> 00:19:26,720
данных для вашей последующей задачи, вы

556
00:19:26,720 --> 00:19:29,600
просите его сделать много с помощью, вы знаете,

557
00:19:29,600 --> 00:19:30,880
большое количество параметров, которые

558
00:19:30,880 --> 00:19:34,080
были инициализированы случайным образом,

559
00:19:34,320 --> 00:19:35,919
хорошо, например, небольшая часть

560
00:19:35,919 --> 00:19:39,039
параметры были предварительно обучены,

561
00:19:39,039 --> 00:19:40,000
хорошо,

562
00:19:40,000 --> 00:19:42,720
поэтому мы идем предварительно обучать

563
00:19:42,720 --> 00:19:44,559
целые модели, я имею в виду, что концептуально вы

564
00:19:44,559 --> 00:19:46,960
знаете, что мы довольно близки к этому,

565
00:19:46,960 --> 00:19:47,840
поэтому в

566
00:19:47,840 --> 00:19:50,400
настоящее время почти все

567
00:19:50,400 --> 00:19:52,640
параметры в вашей нейронной сети и

568
00:19:52,640 --> 00:19:54,400
, скажем, много  настройки исследований и

569
00:19:54,400 --> 00:19:56,480
все чаще в промышленности инициализируются

570
00:19:56,480 --> 00:19:58,160
через предварительное обучение,

571
00:19:58,160 --> 00:19:59,600
так же как были инициализированы параметры

572
00:19:59,600 --> 00:20:02,400
последовательного соединения, а методы предварительного обучения, как

573
00:20:02,400 --> 00:20:03,679
правило,

574
00:20:03,679 --> 00:20:06,880
скрывают части входных данных

575
00:20:06,880 --> 00:20:08,640
от самой модели,

576
00:20:08,640 --> 00:20:10,400
а затем обучают модель восстанавливать

577
00:20:10,400 --> 00:20:11,520
эти части

578
00:20:11,520 --> 00:20:14,799
Как это

579
00:20:14,799 --> 00:20:16,400
связано с словом в век в слово для вектора? Вы знаете, что люди обычно не

580
00:20:16,400 --> 00:20:18,000
делают эту связь, но

581
00:20:18,000 --> 00:20:19,840
это следующее: у вас есть отдельное

582
00:20:19,840 --> 00:20:21,760
слово,

583
00:20:21,760 --> 00:20:24,159
и оно знает себя правильно, потому что у вас

584
00:20:24,159 --> 00:20:26,000
есть вложение для центрального слова

585
00:20:26,000 --> 00:20:27,919
прямо из задания  два, у вас есть

586
00:20:27,919 --> 00:20:29,200
вложение для центра, где он знает

587
00:20:29,200 --> 00:20:30,240
себя,

588
00:20:30,240 --> 00:20:32,159
и вы замаскировали всех его

589
00:20:32,159 --> 00:20:33,280
соседей,

590
00:20:33,280 --> 00:20:34,880
вы спрятали от него всех его соседей,

591
00:20:34,880 --> 00:20:36,080
а также всех его соседей по окну, которые

592
00:20:36,080 --> 00:20:37,840
вы скрыли от него, вы

593
00:20:37,840 --> 00:20:40,640
спрашиваете центральное слово  чтобы правильно предсказать своих соседей,

594
00:20:40,640 --> 00:20:41,520


595
00:20:41,520 --> 00:20:43,600
и поэтому

596
00:20:43,600 --> 00:20:45,520
это подпадает под категорию

597
00:20:45,520 --> 00:20:47,120
предварительного обучения,

598
00:20:47,120 --> 00:20:48,559
все эти методы выглядят одинаково, вы

599
00:20:48,559 --> 00:20:50,799
скрываете части входных данных из модели

600
00:20:50,799 --> 00:20:52,480
и обучаете модель восстанавливать эти

601
00:20:52,480 --> 00:20:53,919


602
00:20:53,919 --> 00:20:55,440
части различия с полным

603
00:20:55,440 --> 00:20:57,760
предварительным обучением модели  заключается в том, что вы не даете

604
00:20:57,760 --> 00:20:59,440
модели только отдельное слово и не

605
00:20:59,440 --> 00:21:01,039
заставляете ее выучить это слово на свадьбе, вы

606
00:21:01,039 --> 00:21:02,960
даете ей гораздо больше последовательности,

607
00:21:02,960 --> 00:21:05,280
и она предсказывает, вы знаете, протянутые

608
00:21:05,280 --> 00:21:06,799
части последовательности  мы углубимся

609
00:21:06,799 --> 00:21:08,960
в детали, но вы знаете,

610
00:21:08,960 --> 00:21:11,360
что вывод состоит в том, что все здесь

611
00:21:11,360 --> 00:21:13,039
предварительно обучено совместно,

612
00:21:13,039 --> 00:21:15,360
возможно, за исключением самого

613
00:21:15,360 --> 00:21:19,280
последнего уровня, который предсказывает, что метка в

614
00:21:19,280 --> 00:21:21,039
порядке, и это было

615
00:21:21,039 --> 00:21:23,360
исключительно эффективно при построении

616
00:21:23,360 --> 00:21:25,919
представлений языка,

617
00:21:25,919 --> 00:21:28,559
которые  просто сопоставьте похожие вещи на языке

618
00:21:28,559 --> 00:21:30,240
с аналогичными представлениями в этих

619
00:21:30,240 --> 00:21:32,960
кодировщиках, точно так же, как word to vect сопоставить

620
00:21:32,960 --> 00:21:35,280
похожие слова с аналогичными векторами,

621
00:21:35,280 --> 00:21:37,120
это было исключительно эффективно

622
00:21:37,120 --> 00:21:38,880
при инициализации параметров, когда

623
00:21:38,880 --> 00:21:40,799
вы начинаете с этих параметров,

624
00:21:40,799 --> 00:21:43,039
которые были предварительно предварительно обучены, а затем вы в

625
00:21:43,039 --> 00:21:45,600
порядке  -Настройте их на ваших помеченных данных,

626
00:21:45,600 --> 00:21:46,960
и, в-третьих, они были

627
00:21:46,960 --> 00:21:48,400
исключительно эффективны при определении

628
00:21:48,400 --> 00:21:50,559
распределений вероятностей по языку,

629
00:21:50,559 --> 00:21:52,320
например, в языковом моделировании, которые на

630
00:21:52,320 --> 00:21:53,200
самом деле

631
00:21:53,200 --> 00:21:55,120
действительно полезны для выборки в определенных

632
00:21:55,120 --> 00:21:56,320
случаях,

633
00:21:56,320 --> 00:21:57,679
так что это три способа, которыми мы

634
00:21:57,679 --> 00:21:59,840
взаимодействуем с предварительно обученными  модели мы

635
00:21:59,840 --> 00:22:01,280
используем представления только для вычисления

636
00:22:01,280 --> 00:22:03,280
сходства, мы используем их для

637
00:22:03,280 --> 00:22:05,120
инициализации параметров  ионы, и мы на самом деле просто используем

638
00:22:05,120 --> 00:22:07,679
их как вероятностные распределения,

639
00:22:07,679 --> 00:22:10,799
вроде того, как мы их тренировали,

640
00:22:10,960 --> 00:22:12,559


641
00:22:12,559 --> 00:22:14,559
так что давайте, мы собираемся здесь перейти к некоторым

642
00:22:14,559 --> 00:22:16,640
техническим частям, но я как бы хочу

643
00:22:16,640 --> 00:22:17,520


644
00:22:17,520 --> 00:22:19,360
обдумать общие мысли о том,

645
00:22:19,360 --> 00:22:21,280
что мы могли бы сделать с предварительным обучением.  -тренинг и

646
00:22:21,280 --> 00:22:22,720
какие вещи мы могли бы ожидать

647
00:22:22,720 --> 00:22:24,159
потенциально узнать

648
00:22:24,159 --> 00:22:26,799
из этого общего метода скрытия

649
00:22:26,799 --> 00:22:29,200
части ввода, а затем просмотра других

650
00:22:29,200 --> 00:22:30,400
частей ввода, а затем попытаться предсказать те

651
00:22:30,400 --> 00:22:31,679
части, которые вы спрятали,

652
00:22:31,679 --> 00:22:33,520
хорошо, так что

653
00:22:33,520 --> 00:22:35,440
университет Стэнфорда находится в пустом месте

654
00:22:35,440 --> 00:22:36,640
Калифорния,

655
00:22:36,640 --> 00:22:38,240
если бы мы дали модели все, что

656
00:22:38,240 --> 00:22:39,440
здесь не было вычеркнуто, и попросили бы

657
00:22:39,440 --> 00:22:40,720
предсказать среднюю

658
00:22:40,720 --> 00:22:43,840
правую часть, функция потерь научила бы

659
00:22:43,840 --> 00:22:46,159
модель предсказывать пало-

660
00:22:46,159 --> 00:22:49,120
альто здесь, я ожидаю

661
00:22:49,120 --> 00:22:52,559
хорошо, так что это пример чего-то,

662
00:22:52,559 --> 00:22:53,600
что вы можете себе представить

663
00:22:53,600 --> 00:22:55,039
цель предварительной тренировки, которую вы берете в

664
00:22:55,039 --> 00:22:57,440
предложении, вы удаляете ее часть и

665
00:22:57,440 --> 00:22:59,679
говорите воссоздать ту часть, которую я удалил, и

666
00:22:59,679 --> 00:23:01,200
в этом случае, если я просто привел кучу

667
00:23:01,200 --> 00:23:03,120
примеров, которые выглядели так, это могло бы

668
00:23:03,120 --> 00:23:04,559
узнай, что

669
00:23:04,559 --> 00:23:08,159
ты знаешь, вроде мелочи здесь

670
00:23:08,159 --> 00:23:10,880
хорошо, вот еще одна я положил пустую

671
00:23:10,880 --> 00:23:13,039
вилку на стол, эта не

672
00:23:13,039 --> 00:23:15,760
указана, ну правильно, так что это может быть

673
00:23:15,760 --> 00:23:18,320
вилка моя вилка

674
00:23:18,320 --> 00:23:21,679
его вилка ее вилка если вы знаете

675
00:23:21,679 --> 00:23:22,720
какую-то

676
00:23:22,720 --> 00:23:25,440
искру да вилку так что это  вы знаете, как

677
00:23:25,440 --> 00:23:28,400
указать виды синтаксических

678
00:23:28,400 --> 00:23:30,080
категорий вещей, которые могут

679
00:23:30,080 --> 00:23:32,720
появиться в этом контексте, ммм, так что это еще

680
00:23:32,720 --> 00:23:33,919
одна вещь, которую вы могли бы

681
00:23:33,919 --> 00:23:37,200
извлечь из такой цели

682
00:23:37,200 --> 00:23:38,240
здесь,

683
00:23:38,240 --> 00:23:39,360
так что вы можете попросить женщину перейти

684
00:23:39,360 --> 00:23:40,880
улицу, проверяя наличие пробок  пустое

685
00:23:40,880 --> 00:23:42,080
плечо

686
00:23:42,080 --> 00:23:43,279
одна из вещей, которые могут быть

687
00:23:43,279 --> 00:23:46,240
здесь затронуты, - это ее заявление со ссылкой,

688
00:23:46,240 --> 00:23:47,279


689
00:23:47,279 --> 00:23:48,720
чтобы вы могли узнать о каких-то связях

690
00:23:48,720 --> 00:23:52,480
между сущностями в тексте, где

691
00:23:52,480 --> 00:23:55,120
одно слово женщина также может совместно ссылаться на

692
00:23:55,120 --> 00:23:57,679
одну и ту же сущность в мире как  это слово ээ

693
00:23:57,679 --> 00:24:00,559
это местоимение она

694
00:24:01,120 --> 00:24:02,960
здесь, о котором вы могли подумать, вы знаете, я

695
00:24:02,960 --> 00:24:04,559
пошел в океан, чтобы посмотреть на тюленей рыб-

696
00:24:04,559 --> 00:24:07,279
черепах, и теперь пусто, я не

697
00:24:07,279 --> 00:24:08,880
думаю, что есть единственно правильный ответ относительно

698
00:24:08,880 --> 00:24:10,400
того, что мы могли увидеть в этом

699
00:24:10,400 --> 00:24:11,520
бла  нк,

700
00:24:11,520 --> 00:24:13,120
но модель могла бы изучить

701
00:24:13,120 --> 00:24:14,880
распределение тех вещей, о которых люди могли

702
00:24:14,880 --> 00:24:17,440
бы говорить, когда один идет в

703
00:24:17,440 --> 00:24:19,840
океан, а двое взволнованы, чтобы увидеть морскую

704
00:24:19,840 --> 00:24:21,600
жизнь, ну правильно, так что это своего рода

705
00:24:21,600 --> 00:24:23,360
семантическая категория лексико-семантическая

706
00:24:23,360 --> 00:24:25,760
категория вещей  гм, это может быть похоже

707
00:24:25,760 --> 00:24:29,039
на тот же набор интересов, что и рыбные

708
00:24:29,039 --> 00:24:32,240
черепахи и тюлени, в контексте того, что я

709
00:24:32,240 --> 00:24:34,400
пошел в океан,

710
00:24:34,400 --> 00:24:36,000
хорошо,

711
00:24:36,000 --> 00:24:37,919
так что, вы знаете, я не знаю, я ожидаю

712
00:24:37,919 --> 00:24:39,600
, что будут примеры этого

713
00:24:39,600 --> 00:24:41,679
в большом корпусе  текст, может быть,

714
00:24:41,679 --> 00:24:43,919
книга,

715
00:24:44,159 --> 00:24:46,400
хорошо, вот еще один пример, а в

716
00:24:46,400 --> 00:24:48,400
целом ценность, которую я получил

717
00:24:48,400 --> 00:24:50,480
за два часа просмотра, была

718
00:24:50,480 --> 00:24:53,440
сумма попкорна и напитка,

719
00:24:53,440 --> 00:24:55,600
фильм был пустым,

720
00:24:55,600 --> 00:24:56,880
и именно тогда я бы

721
00:24:56,880 --> 00:24:59,120
как бы посмотрел в  аудитория и скажите,

722
00:24:59,120 --> 00:25:01,600
был ли фильм плохим или хорошим, но фильм

723
00:25:01,600 --> 00:25:03,200
был плохим,

724
00:25:03,200 --> 00:25:05,600
это мое предсказание здесь верно, и поэтому

725
00:25:05,600 --> 00:25:07,600
это учит вас кое-что о

726
00:25:07,600 --> 00:25:09,360
чувствах, о том, как люди выражают

727
00:25:09,360 --> 00:25:12,240
чувства на языке, и это

728
00:25:12,240 --> 00:25:14,080
даже

729
00:25:14,080 --> 00:25:16,159
выглядит как сама задача, как сделать

730
00:25:16,159 --> 00:25:18,080
анализ настроений - это то, что вам

731
00:25:18,080 --> 00:25:19,679
нужно сделать, чтобы выяснить, был

732
00:25:19,679 --> 00:25:21,600
ли фильм плохим или хорошим или,

733
00:25:21,600 --> 00:25:22,799
может

734
00:25:22,799 --> 00:25:24,000
быть, слово не плохое или хорошее,

735
00:25:24,000 --> 00:25:25,520
фильм закончился или что-то в этом роде

736
00:25:25,520 --> 00:25:27,360
, но как если бы вам пришлось, если бы вам

737
00:25:27,360 --> 00:25:29,440
пришлось  выбрать между плохим или хорошим, скорее

738
00:25:29,440 --> 00:25:31,120
всего, правильно, вам нужно понять

739
00:25:31,120 --> 00:25:33,039
смысл текста,

740
00:25:33,039 --> 00:25:34,240
а

741
00:25:34,240 --> 00:25:37,120
теперь это действительно увлекательно,

742
00:25:37,120 --> 00:25:37,919
хорошо,

743
00:25:37,919 --> 00:25:40,240
вот еще один ум ира пошла на

744
00:25:40,240 --> 00:25:42,480
кухню, чтобы приготовить чай,

745
00:25:42,480 --> 00:25:45,120
стоя рядом с иро зуко, размышляя о своей

746
00:25:45,120 --> 00:25:46,159
судьбе,

747
00:25:46,159 --> 00:25:48,880
оставил зуко  пусто

748
00:25:48,880 --> 00:25:50,640
хорошо, так что это немного проще, потому что

749
00:25:50,640 --> 00:25:52,559
мы действительно

750
00:25:52,559 --> 00:25:54,159
показываем только одно место, я думаю, у нас есть другое

751
00:25:54,159 --> 00:25:56,640
сейчас в судьбе, но это своего

752
00:25:56,640 --> 00:25:58,080
рода рассуждения о пространственном

753
00:25:58,080 --> 00:26:00,080
расположении и перемещении неких

754
00:26:00,080 --> 00:26:02,640
агентов в воображаемом мире, который мы  можно

755
00:26:02,640 --> 00:26:05,600
представить текст, в котором есть такие строки, как этот

756
00:26:05,600 --> 00:26:07,200
человек вошел в это место и был рядом

757
00:26:07,200 --> 00:26:08,960
с тем-то и таким-то, кто ушел, и сделал это, и

758
00:26:08,960 --> 00:26:10,640
не сделал, и поэтому у вас есть эти похожие

759
00:26:10,640 --> 00:26:13,679
отношения, ну вот, зуко покинул

760
00:26:13,679 --> 00:26:15,919
кухню, наиболее вероятно  вещь, которая, как я

761
00:26:15,919 --> 00:26:18,000
думаю, могла бы пойти сюда, и это как бы

762
00:26:18,000 --> 00:26:20,880
указывает на то, что для того, чтобы модель

763
00:26:20,880 --> 00:26:23,520
научилась выполнять эту

764
00:26:23,520 --> 00:26:25,679
задачу заполнения недостающей части,

765
00:26:25,679 --> 00:26:27,200
ей может потребоваться

766
00:26:27,200 --> 00:26:28,640
в целом

767
00:26:28,640 --> 00:26:31,360
выяснить, где

768
00:26:31,360 --> 00:26:33,360
находятся вещи, и

769
00:26:33,360 --> 00:26:36,559
означают ли утверждения или подразумевают эту местность

770
00:26:36,559 --> 00:26:38,480
так что стоя рядом с

771
00:26:38,480 --> 00:26:40,640
ирой пошел на кухню, теперь ира

772
00:26:40,640 --> 00:26:41,919
на кухне,

773
00:26:41,919 --> 00:26:43,760
а затем стоя рядом с иро означает, что

774
00:26:43,760 --> 00:26:45,919
зуко сейчас на кухне, а потом вы

775
00:26:45,919 --> 00:26:46,720
знаете, что

776
00:26:46,720 --> 00:26:48,960
зуко теперь уходит с того места, где он был на

777
00:26:48,960 --> 00:26:50,960
кухне раньше, так что это своего рода очень

778
00:26:50,960 --> 00:26:53,760
базовый смысл рассуждений,

779
00:26:53,760 --> 00:26:54,880


780
00:26:54,880 --> 00:26:56,880
вот это предложение, которое я

781
00:26:56,880 --> 00:26:58,400
думал о последовательности, которая идет 1

782
00:26:58,400 --> 00:27:04,640
1 2 3 5 8 13 21 мм пусто, так что

783
00:27:04,640 --> 00:27:06,320
я не знаю, я могу представить, что люди

784
00:27:06,320 --> 00:27:07,760
пишут что-то, так что это

785
00:27:07,760 --> 00:27:10,240
последовательность Фибоначчи и  как бы вы знаете, вы суммируете

786
00:27:10,240 --> 00:27:11,600
эти два, чтобы получить следующий некоторые из

787
00:27:11,600 --> 00:27:12,960
этих двух к следующему, некоторые из этих

788
00:27:12,960 --> 00:27:15,120
двух верны, и вы знаете, что у вас есть эта

789
00:27:15,120 --> 00:27:17,360
текущая сумма, это известная последовательность, она

790
00:27:17,360 --> 00:27:18,559
появляется во многих текстах в

791
00:27:18,559 --> 00:27:20,000
Интернете

792
00:27:20,000 --> 00:27:21,600
вы знаете и в целом  l вам нужно

793
00:27:21,600 --> 00:27:23,360
изучить алгоритм

794
00:27:23,360 --> 00:27:25,600
или просто формулу, которая, я полагаю,

795
00:27:25,600 --> 00:27:27,200
определяет последовательность Фибоначчи,

796
00:27:27,200 --> 00:27:29,360
чтобы продолжать

797
00:27:29,360 --> 00:27:31,919
работу, модели изучают это на практике,

798
00:27:31,919 --> 00:27:34,480
подождите и узнайте, но вам нужно

799
00:27:34,480 --> 00:27:36,559
будет изучить это, чтобы узнать

800
00:27:36,559 --> 00:27:37,919
последовательность  чтобы продолжать работать и идти и

801
00:27:37,919 --> 00:27:40,240
идти

802
00:27:40,640 --> 00:27:43,120
хорошо, поэтому мы собираемся перейти к

803
00:27:43,120 --> 00:27:45,600
конкретным предварительно обученным моделям, конкретным

804
00:27:45,600 --> 00:27:47,279
методам предварительного обучения сейчас,

805
00:27:47,279 --> 00:27:49,360
поэтому я собираюсь пройти краткий

806
00:27:49,360 --> 00:27:53,840
обзор декодеров трансформаторных кодеров

807
00:27:53,840 --> 00:27:55,919
и декодеров кодеров,

808
00:27:55,919 --> 00:27:57,520
потому что мы '  Сейчас мы собираемся вникнуть

809
00:27:57,520 --> 00:27:59,600
в технические детали, поэтому, прежде чем я это сделаю

810
00:27:59,600 --> 00:28:01,679
, я сделаю паузу

811
00:28:01,679 --> 00:28:04,880
, есть ли какие-нибудь вопросы

812
00:28:05,520 --> 00:28:06,960
? Да, есть интересный

813
00:28:06,960 --> 00:28:08,960
вопрос о том, как

814
00:28:08,960 --> 00:28:10,799
получить нашу модель на

815
00:28:10,799 --> 00:28:13,520
наших входных обучающих данных заранее,

816
00:28:13,520 --> 00:28:14,720
и, возможно, вы также можете  ответьте на этот

817
00:28:14,720 --> 00:28:17,039
вопрос в

818
00:28:18,080 --> 00:28:20,159
настоящее время,

819
00:28:20,159 --> 00:28:22,240
извините, первая часть этого вопроса

820
00:28:22,240 --> 00:28:24,399
заключалась в том, что мы слишком подогнали наши модели к

821
00:28:24,399 --> 00:28:26,640
тому,

822
00:28:29,200 --> 00:28:31,520
что они делают, да, обучение

823
00:28:31,520 --> 00:28:33,200
получилось,

824
00:28:33,200 --> 00:28:34,880
да, это хороший момент, поэтому мы используем

825
00:28:34,880 --> 00:28:36,399
очень большие модели,

826
00:28:36,399 --> 00:28:38,240
и мы  Я могу представить себе, что есть

827
00:28:38,240 --> 00:28:39,919
риск переобучения,

828
00:28:39,919 --> 00:28:42,640
и на практике да, это действительно

829
00:28:42,640 --> 00:28:44,559
одна из наиболее важных вещей, которые нужно сделать для

830
00:28:44,559 --> 00:28:46,159
предварительной подготовки,

831
00:28:46,159 --> 00:28:47,679
поэтому оказывается, что вам нужно иметь

832
00:28:47,679 --> 00:28:50,080
много много данных, например много

833
00:28:50,080 --> 00:28:51,039
данных

834
00:28:51,039 --> 00:28:53,600
и на самом деле мы покажем результаты позже,

835
00:28:53,600 --> 00:28:54,720
когда люди

836
00:28:54,720 --> 00:28:56,559
построили предварительно обученную модель, предварительно обучив ее

837
00:28:56,559 --> 00:28:58,880
на большом количестве данных, а затем, как через шесть

838
00:28:58,880 --> 00:29:01,039
месяцев, появился кто-то другой, это было

839
00:29:01,039 --> 00:29:02,480
типа эй, если вы предварительно обучили ее через 10

840
00:29:02,480 --> 00:29:04,240
месяцев  и почти ничего не изменил,

841
00:29:04,240 --> 00:29:06,159
сейчас все пошло бы еще лучше, если бы

842
00:29:06,159 --> 00:29:08,559
это было переоснащением, я имею в виду, что

843
00:29:08,559 --> 00:29:10,240
вы можете вроде как

844
00:29:10,240 --> 00:29:12,480
протянуть какой-то текст во время предварительного обучения

845
00:29:12,480 --> 00:29:14,159
и как бы оценить

846
00:29:14,159 --> 00:29:15,760
затруднения, правильно лингвистическое

847
00:29:15,760 --> 00:29:18,559
моделирование на этом протянувшемся тексте, и это

848
00:29:18,559 --> 00:29:20,159
имеет тенденцию быть так, что на самом деле эти

849
00:29:20,159 --> 00:29:22,480
модели недостаточно подходят, что нам

850
00:29:22,480 --> 00:29:25,600
нужны все более и более крупные модели для

851
00:29:25,600 --> 00:29:27,600
выражения сложных взаимодействий, которые

852
00:29:27,600 --> 00:29:30,559
позволяют нам лучше соответствовать этим наборам данных,

853
00:29:30,559 --> 00:29:31,919
и поэтому мы поговорим об этом, когда будем

854
00:29:31,919 --> 00:29:33,279
говорить о bert

855
00:29:33,279 --> 00:29:34,399
и  один  из действительно интересных

856
00:29:34,399 --> 00:29:36,559
результатов то, что Burt не

857
00:29:36,559 --> 00:29:38,799
подходит, а не слишком подходит, но в принципе да,

858
00:29:38,799 --> 00:29:41,200
это проблема, потенциально проблема с

859
00:29:41,200 --> 00:29:43,440
переоборудованием, но в конечном итоге у нас есть тонна

860
00:29:43,440 --> 00:29:45,520
текста на английском языке, по крайней мере, хотя и не на

861
00:29:45,520 --> 00:29:48,320
всех языках, и так

862
00:29:48,320 --> 00:29:49,840
что да  важно масштабировать их, но в

863
00:29:49,840 --> 00:29:51,600
настоящее время наши модели не кажутся слишком подходящими

864
00:29:51,600 --> 00:29:55,320
для текста предварительного обучения,

865
00:29:58,720 --> 00:30:02,240
хорошо, любые другие

866
00:30:03,600 --> 00:30:05,440
вопросы хорошо,

867
00:30:05,440 --> 00:30:07,679
поэтому мы видели этот рисунок раньше, прямо здесь

868
00:30:07,679 --> 00:30:09,679
мы видели этот рисунок

869
00:30:09,679 --> 00:30:11,600
декодера кодировщика трансформатора из этого документа

870
00:30:11,600 --> 00:30:15,679
внимание, все  тебе нужно э-э, э-э,

871
00:30:15,679 --> 00:30:17,600
так что у нас есть пара вещей, которые мы не

872
00:30:17,600 --> 00:30:19,679
собираемся

873
00:30:19,679 --> 00:30:21,679
снова обсуждать сегодня, потому что нам нужно многое обсудить,

874
00:30:21,679 --> 00:30:23,679
но я рад поболтать об этом больше

875
00:30:23,679 --> 00:30:25,360
на э-э, э-э,

876
00:30:25,360 --> 00:30:27,679
но так что  в нашем кодировщике у нас есть некоторая

877
00:30:27,679 --> 00:30:29,200
входная последовательность, помните, что есть

878
00:30:29,200 --> 00:30:31,440
последовательность подслов, теперь

879
00:30:31,440 --> 00:30:34,080
каждое подслово получает встраивание слова,

880
00:30:34,080 --> 00:30:36,399
а каждый индекс в преобразователе

881
00:30:36,399 --> 00:30:38,080
получает встраивание позиции

882
00:30:38,080 --> 00:30:39,520
теперь помните, что у нас есть конечная

883
00:30:39,520 --> 00:30:42,080
длина, которую может иметь наша последовательность uh  bly

884
00:30:42,080 --> 00:30:44,240
будет похоже на 512

885
00:30:44,240 --> 00:30:46,159
токенов, это была та заглавная буква t

886
00:30:46,159 --> 00:30:47,919
из последней лекции, поэтому у вас есть некоторая

887
00:30:47,919 --> 00:30:50,480
конечная длина, поэтому у вас есть одно

888
00:30:50,480 --> 00:30:52,559
встраивание позиции для каждого

889
00:30:52,559 --> 00:30:54,880
индекса для всех 512 индексов,

890
00:30:54,880 --> 00:30:55,919
а затем у

891
00:30:55,919 --> 00:30:57,440
вас есть все вложения ваших слов, а

892
00:30:57,440 --> 00:30:59,760
затем кодировщик преобразователя был

893
00:30:59,760 --> 00:31:01,919
эта комбинация

894
00:31:01,919 --> 00:31:04,720
субмодулей, которые мы

895
00:31:04,720 --> 00:31:08,080
проходили строка за строкой во вторник, правильно,

896
00:31:08,080 --> 00:31:09,600
многоголовое внимание было своего рода

897
00:31:09,600 --> 00:31:12,159
основным строительным блоком,

898
00:31:12,159 --> 00:31:14,480
а затем у нас были права на остаточную и слойную норму,

899
00:31:14,480 --> 00:31:16,320
чтобы помочь с прохождением градиентов

900
00:31:16,320 --> 00:31:17,919
и помочь в обучении  лучше и

901
00:31:17,919 --> 00:31:21,760
быстрее у нас был этот слой прямого распространения,

902
00:31:21,760 --> 00:31:22,799
чтобы

903
00:31:22,799 --> 00:31:24,799
обработать вид

904
00:31:24,799 --> 00:31:26,480
результата многоголового внимания, другой

905
00:31:26,480 --> 00:31:28,799
остаток и норму уровня, а затем перейти к тому

906
00:31:28,799 --> 00:31:30,799
же блоку кодировщика трансформатора

907
00:31:30,799 --> 00:31:32,880
здесь, и они будут сложены, я

908
00:31:32,880 --> 00:31:34,399
увижу несколько разных  конфигурации

909
00:31:34,399 --> 00:31:37,840
здесь, но я думаю, что вы знаете шесть

910
00:31:37,840 --> 00:31:39,679
12 таких сложенных вместе,

911
00:31:39,679 --> 00:31:40,640
хорошо,

912
00:31:40,640 --> 00:31:42,240
так что это кодировщик трансформатора, и сегодня

913
00:31:42,240 --> 00:31:44,720
мы собираемся увидеть целые модели

914
00:31:44,720 --> 00:31:48,399
это просто трансформаторные кодеры,

915
00:31:48,399 --> 00:31:50,000
хорошо, поэтому мы, когда мы говорили о машинном

916
00:31:50,000 --> 00:31:51,200
переводе, когда мы говорили о

917
00:31:51,200 --> 00:31:53,600
самом трансформаторе, декодере трансформаторного кодера,

918
00:31:53,600 --> 00:31:55,679
мы говорили обо всем этом,

919
00:31:55,679 --> 00:31:57,039
но на самом деле вы могли бы просто иметь этот

920
00:31:57,039 --> 00:31:59,039
левый столбец, вы могли бы фактически просто иметь

921
00:31:59,039 --> 00:32:01,440
этот правый столбец,

922
00:32:01,440 --> 00:32:03,120
ммм  хотя правый столбец немного изменится,

923
00:32:03,120 --> 00:32:04,720
если он у вас просто есть, так что

924
00:32:04,720 --> 00:32:07,360
помните, что в правом столбце у нас было это

925
00:32:07,360 --> 00:32:09,840
замаскированное самовнимание с несколькими головами,

926
00:32:09,840 --> 00:32:11,200
так что вы не можете смотреть в

927
00:32:11,200 --> 00:32:13,440
будущее,

928
00:32:13,440 --> 00:32:15,600
и кто-то спросил на самом деле о том, как

929
00:32:15,600 --> 00:32:18,159
мы декодируем из трансформаторов  учитывая, что у

930
00:32:18,159 --> 00:32:19,360
вас есть такая большая

931
00:32:19,360 --> 00:32:21,039
операция по фрагментации, это отличный вопрос, я не

932
00:32:21,039 --> 00:32:23,039
смогу подробно разбираться в нем сегодня,

933
00:32:23,039 --> 00:32:25,360
но вам нужно запускать его один раз во время

934
00:32:25,360 --> 00:32:28,080
процесса декодирования каждый раз, когда вы

935
00:32:28,080 --> 00:32:31,279
декодируете, чтобы вроде как предсказать следующий  слово

936
00:32:31,279 --> 00:32:33,440
эм, я напишу что-нибудь по

937
00:32:33,440 --> 00:32:35,039
этому

938
00:32:35,039 --> 00:32:36,880
поводу, так что в массовом самовнушении с несколькими головами

939
00:32:36,880 --> 00:32:38,159
вам не разрешено смотреть в будущее,

940
00:32:38,159 --> 00:32:40,320
чтобы у вас было что-то вроде

941
00:32:40,320 --> 00:32:42,159
четко определенного  цель попытки выполнить

942
00:32:42,159 --> 00:32:44,399
языковое моделирование um тогда у нас есть

943
00:32:44,399 --> 00:32:46,399
остаток и норма уровня, перекрестное внимание с несколькими головками

944
00:32:46,399 --> 00:32:48,000
помните, что оно возвращается

945
00:32:48,000 --> 00:32:49,919
к последнему уровню

946
00:32:49,919 --> 00:32:52,159
кодировщика трансформатора или к последнему блоку кодера трансформатора,

947
00:32:52,159 --> 00:32:53,279


948
00:32:53,279 --> 00:32:54,720
а затем к более остаточной норме уровня

949
00:32:54,720 --> 00:32:56,399
другого уровня прямой связи

950
00:32:56,399 --> 00:32:58,480
или  если у

951
00:32:58,480 --> 00:33:00,720
нас нет кодировщика прямо здесь,

952
00:33:00,720 --> 00:33:02,880
тогда мы избавляемся от перекрестного

953
00:33:02,880 --> 00:33:04,559
внимания, остатка и нормы уровня

954
00:33:04,559 --> 00:33:06,240
прямо здесь, поэтому, если бы у нас не было этого

955
00:33:06,240 --> 00:33:08,320
стека кодировщиков, декодеры стали бы

956
00:33:08,320 --> 00:33:09,760
проще, потому что вы не  им не нужно уделять внимание,

957
00:33:09,760 --> 00:33:10,640


958
00:33:10,640 --> 00:33:12,080
но опять же, у вас также есть эти

959
00:33:12,080 --> 00:33:13,760
вложения слов внизу и позиционные

960
00:33:13,760 --> 00:33:17,519
представления для выходной последовательности,

961
00:33:17,519 --> 00:33:21,120
хорошо, так что это было рассмотрено, давай поговорим

962
00:33:21,120 --> 00:33:22,399
о предварительном обучении с помощью языкового

963
00:33:22,399 --> 00:33:24,640
моделирования, так что мы действительно поговорили, может быть,

964
00:33:24,640 --> 00:33:25,840
немного  немного об этом раньше, и мы

965
00:33:25,840 --> 00:33:28,880
видели языковое моделирование в контексте,

966
00:33:28,880 --> 00:33:30,880
возможно, просто желания сделать это априори, поэтому

967
00:33:30,880 --> 00:33:32,799
языковые модели были полезны, например,

968
00:33:32,799 --> 00:33:35,039
в системе автоматического распознавания речи.  Тем не менее,

969
00:33:35,039 --> 00:33:36,720
они были полезны в системах статистического машинного

970
00:33:36,720 --> 00:33:38,960
перевода, поэтому

971
00:33:38,960 --> 00:33:41,919
давайте вспомним задачу моделирования языка,

972
00:33:41,919 --> 00:33:44,399
вы можете сказать, что она определяется как моделирование

973
00:33:44,399 --> 00:33:47,200
вероятности слова при заданном индексе

974
00:33:47,200 --> 00:33:49,440
t любого слова в любом заданном индексе с учетом

975
00:33:49,440 --> 00:33:51,600
всех слов перед ним,

976
00:33:51,600 --> 00:33:54,080
и это распределение

977
00:33:54,080 --> 00:33:55,840
вероятностей  это распределение слов с учетом

978
00:33:55,840 --> 00:33:58,320
их прошлого контекста,

979
00:33:58,320 --> 00:33:59,360


980
00:33:59,360 --> 00:34:01,039
и поэтому это просто говорит о том, что вы

981
00:34:01,039 --> 00:34:02,960
знаете для любого префикса здесь

982
00:34:02,960 --> 00:34:04,880
iro делает, чтобы

983
00:34:04,880 --> 00:34:06,320
я хотел, чтобы вероятность того, что

984
00:34:06,320 --> 00:34:08,320
должно быть следующее слово, так что наблюдаемое следующее

985
00:34:08,320 --> 00:34:10,800
слово будет вкусным, но вы знаете

986
00:34:10,800 --> 00:34:13,359
может быть, там идет приготовить чай,

987
00:34:13,359 --> 00:34:15,918
идет приготовить горячую воду и т. д. вы можете

988
00:34:15,918 --> 00:34:17,839
распределить, какое следующее слово

989
00:34:17,839 --> 00:34:20,239
должно быть в этом декодере, и помните

990
00:34:20,239 --> 00:34:21,520
, что из-за замаскированного

991
00:34:21,520 --> 00:34:24,159
самовнимания make может оглянуться на

992
00:34:24,159 --> 00:34:27,359
слово или идет или iro  но он не может

993
00:34:27,359 --> 00:34:30,560
рассчитывать на вкусное,

994
00:34:30,560 --> 00:34:31,760
поэтому для этого есть много данных, у

995
00:34:31,760 --> 00:34:33,119
вас просто есть текст,

996
00:34:33,119 --> 00:34:35,440
и, как вуаля, у вас есть

997
00:34:35,440 --> 00:34:38,239
данные языкового моделирования, это бесплатно, хорошо, когда у

998
00:34:38,239 --> 00:34:40,239
вас есть текст, он свободен  вы знаете, что

999
00:34:40,239 --> 00:34:42,320
доступно, вам не нужно его маркировать,

1000
00:34:42,320 --> 00:34:43,599
и на английском у вас его много,

1001
00:34:43,599 --> 00:34:45,918
это верно не для всех

1002
00:34:45,918 --> 00:34:47,918
языков, но в английском языке у вас есть

1003
00:34:47,918 --> 00:34:51,839
много данных до обучения,

1004
00:34:51,839 --> 00:34:52,879
и

1005
00:34:52,879 --> 00:34:54,879
поэтому вы знаете  простая вещь о

1006
00:34:54,879 --> 00:34:56,239
некотором предварительном обучении - это

1007
00:34:56,239 --> 00:34:57,920
хорошо, что мы собираемся сделать, это

1008
00:34:57,920 --> 00:34:59,280
обучить нейронную сеть выполнять

1009
00:34:59,280 --> 00:35:01,119
языковое моделирование на большом объеме

1010
00:35:01,119 --> 00:35:02,960
текста, и мы просто сохраним вашу известную

1011
00:35:02,960 --> 00:35:04,560
сеть в параметрах  нашей обученной

1012
00:35:04,560 --> 00:35:06,480
сети на диск,

1013
00:35:06,480 --> 00:35:08,640
так что концептуально это на самом деле не

1014
00:35:08,640 --> 00:35:09,760
отличается от того, что мы

1015
00:35:09,760 --> 00:35:12,079
делали до этого, это просто своего рода намерение,

1016
00:35:12,079 --> 00:35:13,760
правильно мы обучаем эти параметры, чтобы

1017
00:35:13,760 --> 00:35:15,760
начать использовать их для чего-то еще в

1018
00:35:15,760 --> 00:35:17,359
дальнейшем, но

1019
00:35:17,359 --> 00:35:19,920
моделирование языка  сам по себе не

1020
00:35:19,920 --> 00:35:22,079
меняет декодер здесь не меняется правильно

1021
00:35:22,079 --> 00:35:24,720
, это преобразователь в предварительно обученных моделях

1022
00:35:24,720 --> 00:35:26,480
в современном, потому что это своего рода

1023
00:35:26,480 --> 00:35:30,160
новая популярная концепция,

1024
00:35:30,160 --> 00:35:32,000
хотя вы знаете, что еще в 2015 году было что-то вроде

1025
00:35:32,000 --> 00:35:34,640
того, когда это, я думаю, было  сначала вы

1026
00:35:34,640 --> 00:35:36,800
знаете эффективно  d out и получил некоторые

1027
00:35:36,800 --> 00:35:38,800
интересные результаты

1028
00:35:38,800 --> 00:35:41,359
ммм, но вы знаете, что сегодня здесь может быть что угодно,

1029
00:35:41,359 --> 00:35:42,240


1030
00:35:42,240 --> 00:35:43,359
в основном это будут

1031
00:35:43,359 --> 00:35:44,800
трансформаторы и модели, которые мы на

1032
00:35:44,800 --> 00:35:47,440
самом деле наблюдаем,

1033
00:35:47,440 --> 00:35:49,040
хорошо,

1034
00:35:49,040 --> 00:35:50,240
поэтому, когда у вас есть ваша предварительно обученная

1035
00:35:50,240 --> 00:35:52,079
сеть, что

1036
00:35:52,079 --> 00:35:54,560
вы делаете по умолчанию?  Примите, чтобы использовать это правильно, и если

1037
00:35:54,560 --> 00:35:56,240
вы извлечете из этой лекции что-нибудь

1038
00:35:56,240 --> 00:35:57,760
с точки зрения тех же

1039
00:35:57,760 --> 00:35:59,200
инженерных практик, которые будут

1040
00:35:59,200 --> 00:36:01,920
вам в целом полезны, когда вы начнете

1041
00:36:01,920 --> 00:36:04,880
строить и изучать вещи, возможно, вы

1042
00:36:04,880 --> 00:36:07,680
знаете, как инженер по машинному обучению

1043
00:36:07,680 --> 00:36:09,760
или вычислительный  социолог или

1044
00:36:09,760 --> 00:36:11,839
и т. д. эм, вы знаете, что люди

1045
00:36:11,839 --> 00:36:14,160
обычно делают, это вы предварительно обучаете свою сеть

1046
00:36:14,160 --> 00:36:16,560
только на большом количестве данных, много текста, изучаете

1047
00:36:16,560 --> 00:36:19,520
очень общие вещи, а затем

1048
00:36:19,520 --> 00:36:21,839
вы адаптируете сеть к тому, что вы

1049
00:36:21,839 --> 00:36:23,680
хотели сделать, так что у нас была куча

1050
00:36:23,680 --> 00:36:25,760
данных до обучения, а затем, возможно, это

1051
00:36:25,760 --> 00:36:27,280
обзор фильма, который мы берем здесь в качестве

1052
00:36:27,280 --> 00:36:30,320
входных данных, и мы просто применяем

1053
00:36:30,320 --> 00:36:31,839
э-э, применяем

1054
00:36:31,839 --> 00:36:33,520
декодер, который мы как бы предварительно обучили,

1055
00:36:33,520 --> 00:36:36,160
запускаем параметры там,

1056
00:36:36,160 --> 00:36:39,200
а затем настраиваем его  что бы мы ни

1057
00:36:39,200 --> 00:36:40,720


1058
00:36:40,720 --> 00:36:42,480
хотели сделать, может быть, это задача анализа настроений,

1059
00:36:42,480 --> 00:36:45,280
поэтому мы прогоняем всю

1060
00:36:45,280 --> 00:36:47,760
последовательность через декодер, чтобы получить

1061
00:36:47,760 --> 00:36:49,359
скрытое состояние в конце в самом последнем

1062
00:36:49,359 --> 00:36:51,040
, а затем мы прогнозируем, что вы знаете, может быть,

1063
00:36:51,040 --> 00:36:53,280
плюс или минус  настроения,

1064
00:36:53,280 --> 00:36:54,560
и это своего рода адаптация

1065
00:36:54,560 --> 00:36:56,240
предварительно обученной сети к задачам, эта

1066
00:36:56,240 --> 00:37:00,240
предварительно обученная парадигма тонкой настройки дико

1067
00:37:00,240 --> 00:37:03,119
успешна, и вам действительно стоит попробовать ее

1068
00:37:03,119 --> 00:37:05,599
всякий раз, когда вы эффективно выполняете какие-либо задачи nlp в

1069
00:37:05,599 --> 00:37:08,800
настоящее время,

1070
00:37:08,800 --> 00:37:11,119
потому что это, как правило, то, что  какой-то

1071
00:37:11,119 --> 00:37:12,720
вариант этого, как правило, лучше всего работает

1072
00:37:12,720 --> 00:37:14,800


1073
00:37:14,800 --> 00:37:17,440
хорошо, так что теперь у нас есть техническая заметка,

1074
00:37:17,440 --> 00:37:19,760


1075
00:37:19,760 --> 00:37:20,560
так что,

1076
00:37:20,560 --> 00:37:22,640
если вам не нравится думать об

1077
00:37:22,640 --> 00:37:26,000
оптимизации или градиентном спуске, вы

1078
00:37:26,000 --> 00:37:27,680
знаете, может быть, пропустите этот слайд, но

1079
00:37:27,680 --> 00:37:29,760
я рекомендую вам  просто подумайте, подумайте

1080
00:37:29,760 --> 00:37:32,320
на секунду о том, почему это должно помочь

1081
00:37:32,320 --> 00:37:34,320
вам знать

1082
00:37:34,320 --> 00:37:36,240
обучающие нейронные сети, мы

1083
00:37:36,240 --> 00:37:38,400
используем градиентный спуск, чтобы попытаться найти

1084
00:37:38,400 --> 00:37:40,560
какое-то глобальное минимальное право этой

1085
00:37:40,560 --> 00:37:43,440
оптимизации этой функции потерь,

1086
00:37:43,440 --> 00:37:44,240
и

1087
00:37:44,240 --> 00:37:45,920
вы знаете, что мы  Мы как бы делаем это в

1088
00:37:45,920 --> 00:37:48,640
два этапа, и первый шаг - вы знаете,

1089
00:37:48,640 --> 00:37:51,760
что у нас есть некоторые параметры, тета-шляпа

1090
00:37:51,760 --> 00:37:53,839
, аппроксимируя

1091
00:37:53,839 --> 00:37:56,480
min по нашим, извините, тета - это

1092
00:37:56,480 --> 00:37:58,000
параметры нейронной сети, поэтому все

1093
00:37:58,000 --> 00:37:59,359


1094
00:37:59,359 --> 00:38:01,359
векторы kqv в нашем преобразователе записывают

1095
00:38:01,359 --> 00:38:03,040
word встраивает позиции, встраивает

1096
00:38:03,040 --> 00:38:04,480
это просто

1097
00:38:04,480 --> 00:38:06,800
все параметры нашей нейронной сети um,

1098
00:38:06,800 --> 00:38:08,720
и поэтому мы делаем min по всем

1099
00:38:08,720 --> 00:38:10,160
параметрам нашей теты, которые мы пытаемся

1100
00:38:10,160 --> 00:38:11,839
аппроксимировать min по параметрам

1101
00:38:11,839 --> 00:38:13,599
нашей нейронной сети наших потерь до обучения,

1102
00:38:13,599 --> 00:38:16,240
которые  здесь было языковое

1103
00:38:16,240 --> 00:38:18,960
моделирование наших наших параметров, и

1104
00:38:18,960 --> 00:38:20,720
давайте просто получим такую

1105
00:38:20,720 --> 00:38:23,680
оценку некоторых параметров тета-шляпа,

1106
00:38:23,680 --> 00:38:26,640
а затем мы точно настроим, аппроксимируя

1107
00:38:26,640 --> 00:38:30,000
эту минуту

1108
00:38:30,000 --> 00:38:32,240
по тэте тонкой настройки потери, может быть, это

1109
00:38:32,240 --> 00:38:34,000
правильное начало  в тэта-шляпе, поэтому мы

1110
00:38:34,000 --> 00:38:36,079
инициализируем наш градиентный спуск в тэта-

1111
00:38:36,079 --> 00:38:37,280
шляпе, а затем мы просто вроде как позволяем ей

1112
00:38:37,280 --> 00:38:38,480
делать то, что они хотят,

1113
00:38:38,480 --> 00:38:40,079
и как будто

1114
00:38:40,079 --> 00:38:43,119
это просто работает, а отчасти

1115
00:38:43,119 --> 00:38:45,839
это должно быть потому, что вы k  Теперь

1116
00:38:45,839 --> 00:38:47,839
кое-что о том, с чего мы начинаем, так

1117
00:38:47,839 --> 00:38:50,800
важно не только с точки зрения своего рода

1118
00:38:50,800 --> 00:38:52,400
градиентного потока, хотя это большая его

1119
00:38:52,400 --> 00:38:55,440
часть, но также похоже, что вы

1120
00:38:55,440 --> 00:38:57,920
знаете, что стохастический градиентный спуск придерживается

1121
00:38:57,920 --> 00:38:59,599
относительно близко

1122
00:38:59,599 --> 00:39:00,560
к

1123
00:39:00,560 --> 00:39:02,720
этой предварительной инициализации во время

1124
00:39:02,720 --> 00:39:04,800
точной настройки этого  это то, что мы,

1125
00:39:04,800 --> 00:39:06,800
кажется, правильно наблюдаем на практике

1126
00:39:06,800 --> 00:39:08,560
, что

1127
00:39:08,560 --> 00:39:10,400
каким-то образом локальность стохастического

1128
00:39:10,400 --> 00:39:12,079
градиентного спуска обнаруживает локальные минимумы

1129
00:39:12,079 --> 00:39:13,839
, близкие к этой тэта-шляпе, что

1130
00:39:13,839 --> 00:39:15,920
было хорошо для такой общей проблемы

1131
00:39:15,920 --> 00:39:17,280
языкового моделирования,

1132
00:39:17,280 --> 00:39:19,760
кажется, да, локальные

1133
00:39:19,760 --> 00:39:21,760
минимумы  потеря тонкой настройки, потому что мы не

1134
00:39:21,760 --> 00:39:23,440
находим, или да, локальный минимум

1135
00:39:23,440 --> 00:39:26,320
потери тонкой настройки имеет тенденцию хорошо обобщаться,

1136
00:39:26,320 --> 00:39:28,000
когда они находятся рядом с этой тета-шляпой, которую

1137
00:39:28,000 --> 00:39:29,200
мы предварительно обучили, и это своего рода

1138
00:39:29,200 --> 00:39:30,240
загадка, что мы  все еще пытаюсь

1139
00:39:30,240 --> 00:39:31,760
выяснить больше об

1140
00:39:31,760 --> 00:39:33,599
um, а затем также да, может быть,

1141
00:39:33,599 --> 00:39:35,040
градиенты прямо градиенты

1142
00:39:35,040 --> 00:39:36,720
потери точной настройки около тета распространяются

1143
00:39:36,720 --> 00:39:38,720
хорошо, поэтому наше сетевое обучение проходит

1144
00:39:38,720 --> 00:39:41,040
очень хорошо, хорошо,

1145
00:39:41,040 --> 00:39:43,760
хорошо, так что  это то, что нужно пережевывать,

1146
00:39:43,760 --> 00:39:46,000
но на практике это работает, я думаю, это все

1147
00:39:46,000 --> 00:39:49,359
еще интересно, что он работает

1148
00:39:49,440 --> 00:39:52,400
нормально, так

1149
00:39:52,400 --> 00:39:54,320
что мы говорили в основном о

1150
00:39:54,320 --> 00:39:56,480
декодере кодировщика трансформатора,

1151
00:39:56,480 --> 00:39:58,079
и

1152
00:39:58,079 --> 00:40:00,240
на самом деле я сказал, что мы могли бы

1153
00:40:00,240 --> 00:40:02,880
просто  левосторонние кодеры, которые

1154
00:40:02,880 --> 00:40:05,040
должны быть предварительно обучены, или просто

1155
00:40:05,040 --> 00:40:07,440
декодеры для предварительного обучения, или декодеры кодировщиков,

1156
00:40:07,440 --> 00:40:09,760
и на самом деле есть действительно

1157
00:40:09,760 --> 00:40:11,839
популярные типы известных моделей в каждой из

1158
00:40:11,839 --> 00:40:13,839
этих трех категорий, виды

1159
00:40:13,839 --> 00:40:15,680
предварительного обучения, которые вы можете выполнять,

1160
00:40:15,680 --> 00:40:18,319
и  виды

1161
00:40:18,319 --> 00:40:20,560
приложений или использование этих

1162
00:40:20,560 --> 00:40:23,200
предварительно обученных моделей, которые наиболее естественны, на

1163
00:40:23,200 --> 00:40:25,119
самом деле сильно зависят от того,

1164
00:40:25,119 --> 00:40:28,319
выберете ли вы предварительное обучение кодировщика декодера

1165
00:40:28,319 --> 00:40:31,280
или кодировщика-декодера,

1166
00:40:31,280 --> 00:40:32,720
и поэтому я думаю, что это полезно, когда мы рассмотрим

1167
00:40:32,720 --> 00:40:35,680
некоторые из этих популярных

1168
00:40:35,680 --> 00:40:37,280
своего рода названия моделей, которые вам нужно

1169
00:40:37,280 --> 00:40:40,079
знать, и что они собой представляют, каковы были их

1170
00:40:40,079 --> 00:40:41,680
инновации, чтобы на самом деле разделить их

1171
00:40:41,680 --> 00:40:44,079
на эти категории,

1172
00:40:44,079 --> 00:40:46,560
так что мы уже так вот что,

1173
00:40:46,560 --> 00:40:48,319
мы собираемся пройти через эти три,

1174
00:40:48,319 --> 00:40:50,640
и все они  есть своего рода преимущества и

1175
00:40:50,640 --> 00:40:53,119
, в некотором смысле, недостатки, так что

1176
00:40:53,119 --> 00:40:54,880
декодеры правы,

1177
00:40:54,880 --> 00:40:56,160
на самом

1178
00:40:56,160 --> 00:40:57,520
деле то, о чем мы здесь говорим, в

1179
00:40:57,520 --> 00:40:59,119
основном, это языковые модели, и мы видели

1180
00:40:59,119 --> 00:41:00,800
это до сих пор, мы говорили о

1181
00:41:00,800 --> 00:41:03,119
предварительно обученных декодерах, и это

1182
00:41:03,119 --> 00:41:04,640
приятно генерировать

1183
00:41:04,640 --> 00:41:06,400
справа, так что вы можете просто взять образец из своей

1184
00:41:06,400 --> 00:41:08,160
предварительно обученной языковой модели и получить

1185
00:41:08,160 --> 00:41:09,599
вещи, которые выглядят как текст, который

1186
00:41:09,599 --> 00:41:11,119
вы предварительно тренировали на

1187
00:41:11,119 --> 00:41:13,280
um, но вы знаете, что одна проблема заключается в том, что вы

1188
00:41:13,280 --> 00:41:15,280
не можете правильно определять будущие слова, поэтому

1189
00:41:15,280 --> 00:41:16,400
мы упоминали

1190
00:41:16,400 --> 00:41:19,440
в нашем моделировании с помощью lstms,

1191
00:41:19,440 --> 00:41:21,520
что вместо этого, если бы вы могли, когда вы могли

1192
00:41:21,520 --> 00:41:22,480
это сделать,

1193
00:41:22,480 --> 00:41:24,480
мы сказали, что наличие двунаправленного

1194
00:41:24,480 --> 00:41:27,280
lstm на самом деле намного полезнее,

1195
00:41:27,280 --> 00:41:29,200
чем наличие однонаправленного stm, ну,

1196
00:41:29,200 --> 00:41:31,119
это в некотором роде верно и для

1197
00:41:31,119 --> 00:41:32,720
трансформаторов  Итак,

1198
00:41:32,720 --> 00:41:34,319
если вы видите, как стрелки

1199
00:41:34,319 --> 00:41:36,640
указывают здесь, стрелки указывают

1200
00:41:36,640 --> 00:41:38,079
вверх, в то, что вы знаете справа, так что

1201
00:41:38,079 --> 00:41:40,240
это слово как бы оглядывается на его

1202
00:41:40,240 --> 00:41:42,560
прошлую историю,

1203
00:41:42,560 --> 00:41:46,960
но вы знаете, что это слово, э-э, не видите,

1204
00:41:46,960 --> 00:41:48,400
вы можете ''  t контекстуализировать с футу  re,

1205
00:41:48,400 --> 00:41:50,079
тогда как в блоке кодировщика здесь,

1206
00:41:50,079 --> 00:41:52,319
выделенном синим цветом, чуть ниже, у вас как бы есть все

1207
00:41:52,319 --> 00:41:54,800
пары взаимодействий, и поэтому вы знаете, что

1208
00:41:54,800 --> 00:41:56,160
когда вы создаете представления, на

1209
00:41:56,160 --> 00:41:57,440
самом деле может быть очень полезно знать,

1210
00:41:57,440 --> 00:41:59,280
каковы будут будущие слова, так

1211
00:41:59,280 --> 00:42:00,800
что кодировщики дают вам  правильно, вы получаете

1212
00:42:00,800 --> 00:42:02,640
двунаправленный контекст,

1213
00:42:02,640 --> 00:42:04,160
чтобы вы могли ориентироваться на будущее, возможно,

1214
00:42:04,160 --> 00:42:05,200
это поможет вам создать лучшие

1215
00:42:05,200 --> 00:42:07,359
представления о языке,

1216
00:42:07,359 --> 00:42:09,119
но вопрос, который мы на самом деле рассмотрим

1217
00:42:09,119 --> 00:42:10,560
здесь, хорошо, как вы

1218
00:42:10,560 --> 00:42:11,839
их предварительно обучите,

1219
00:42:11,839 --> 00:42:13,280
вы не можете заранее  торгуйте ими как языковыми

1220
00:42:13,280 --> 00:42:14,720
моделями, потому что у вас есть доступ к

1221
00:42:14,720 --> 00:42:17,440
будущему, поэтому, если вы попытаетесь сделать это,

1222
00:42:17,440 --> 00:42:19,520
потери сразу же будут равны нулю, потому что

1223
00:42:19,520 --> 00:42:20,880
вы можете просто увидеть, какое

1224
00:42:20,880 --> 00:42:23,119
будущее будет бесполезным, а затем

1225
00:42:23,119 --> 00:42:24,560
мы поговорим о предварительном  обученные кодировщики-

1226
00:42:24,560 --> 00:42:26,000
декодеры,

1227
00:42:26,000 --> 00:42:27,040
которым нравится,

1228
00:42:27,040 --> 00:42:29,760
может быть, лучшее из обоих миров, но

1229
00:42:29,760 --> 00:42:31,680
также, возможно, неясно, как лучше всего

1230
00:42:31,680 --> 00:42:33,040
их предварительно обучить, у

1231
00:42:33,040 --> 00:42:36,000
них определенно есть преимущества для

1232
00:42:36,000 --> 00:42:37,280
обоих, так что

1233
00:42:37,280 --> 00:42:40,160
давайте перейдем к некоторой общей

1234
00:42:40,160 --> 00:42:40,960
вершине,

1235
00:42:40,960 --> 00:42:42,560
например, более да,

1236
00:42:42,560 --> 00:42:43,920
сначала займемся декодерами, мы пройдем через все

1237
00:42:43,920 --> 00:42:45,920
три,

1238
00:42:45,920 --> 00:42:48,560
хорошо, так что,

1239
00:42:48,720 --> 00:42:50,240


1240
00:42:50,240 --> 00:42:52,480
когда мы предварительно обучаем языковую модель

1241
00:42:52,480 --> 00:42:53,920
, мы создаем ее для этой

1242
00:42:53,920 --> 00:42:55,440
цели, мы пытаемся

1243
00:42:55,440 --> 00:42:57,839
приблизить эту вероятность  слово,

1244
00:42:57,839 --> 00:43:00,240
учитывая все его предыдущие слова,

1245
00:43:00,240 --> 00:43:01,119


1246
00:43:01,119 --> 00:43:02,720
что мы в итоге делаем, и я показал

1247
00:43:02,720 --> 00:43:04,160
это пиктографически, давайте добавим немного

1248
00:43:04,160 --> 00:43:06,400
математики правильно, мы получим скрытое состояние от

1249
00:43:06,400 --> 00:43:10,240
h1 до ht для каждого из слов на

1250
00:43:10,240 --> 00:43:12,560
входе от w1 до wt теперь запомните слова снова

1251
00:43:12,560 --> 00:43:14,240
означает  дополнительные слова здесь,

1252
00:43:14,240 --> 00:43:16,160
хорошо, а затем, когда мы настраиваем

1253
00:43:16,160 --> 00:43:17,839
это правильно, мы можем взять

1254
00:43:17,839 --> 00:43:20,800
представление, которое должно быть htaht

1255
00:43:20,800 --> 00:43:22,480
плюс b,

1256
00:43:22,480 --> 00:43:24,079
а затем изображение здесь прямо

1257
00:43:24,079 --> 00:43:27,599
здесь ht это самое последнее

1258
00:43:27,599 --> 00:43:28,640
состояние

1259
00:43:28,640 --> 00:43:31,280
кодировщика, а теперь это вроде как  Представьте, он

1260
00:43:31,280 --> 00:43:33,119
видел всю свою историю

1261
00:43:33,119 --> 00:43:35,520
правильно, и поэтому

1262
00:43:35,520 --> 00:43:38,480
вы можете применить здесь линейный слой, возможно,

1263
00:43:38,480 --> 00:43:40,400
умножив его на некоторые параметры a и

1264
00:43:40,400 --> 00:43:42,800
b, которые не были предварительно обучены,

1265
00:43:42,800 --> 00:43:44,079
а затем вы прогнозируете настроение,

1266
00:43:44,079 --> 00:43:46,560
может быть, вы знаете, что положительное или отрицательное настроение,

1267
00:43:46,560 --> 00:43:48,960
а, возможно, и  ну ты знаешь, посмотри на

1268
00:43:48,960 --> 00:43:50,240
красный и серый, поэтому большинство

1269
00:43:50,240 --> 00:43:51,760
параметров в моей нейронной сети теперь

1270
00:43:51,760 --> 00:43:54,400
были предварительно обучены, самый последний уровень,

1271
00:43:54,400 --> 00:43:56,560
который изучает настроения,

1272
00:43:56,560 --> 00:43:58,240
скажем,

1273
00:43:58,240 --> 00:44:00,240
решение, это не было предварительно обучено, поэтому

1274
00:44:00,240 --> 00:44:02,079
они были инициализированы случайным образом, и

1275
00:44:02,079 --> 00:44:03,839
когда вы, когда  вы принимаете потерю

1276
00:44:03,839 --> 00:44:06,319
потери настроения, вы тренируете здесь не

1277
00:44:06,319 --> 00:44:08,319
только линейный слой, но вы фактически

1278
00:44:08,319 --> 00:44:10,480
обратно распространяете градиенты

1279
00:44:10,480 --> 00:44:12,960
по всей предварительно обученной сети

1280
00:44:12,960 --> 00:44:16,000
и точно настраиваете все эти параметры

1281
00:44:16,000 --> 00:44:17,200
правильно, так что это не похоже на то, что вы  просто

1282
00:44:17,200 --> 00:44:19,200
тренируя это во время точной настройки этого

1283
00:44:19,200 --> 00:44:20,960
линейного слоя, вы тренируете всю

1284
00:44:20,960 --> 00:44:22,400
сеть в зависимости от этой

1285
00:44:22,400 --> 00:44:25,280
потери точной настройки,

1286
00:44:25,520 --> 00:44:27,440
и вы знаете, что, возможно, это плохо, что,

1287
00:44:27,440 --> 00:44:29,920
как и линейный слой, не был предварительно обучен

1288
00:44:29,920 --> 00:44:31,440
по общей схеме вещей, он  не

1289
00:44:31,440 --> 00:44:32,960
так много параметров,

1290
00:44:32,960 --> 00:44:34,400


1291
00:44:34,400 --> 00:44:36,079
так что это полезно, это всего лишь один

1292
00:44:36,079 --> 00:44:38,240
способ правильно взаимодействовать с предварительно обученными моделями,

1293
00:44:38,240 --> 00:44:39,520
и поэтому

1294
00:44:39,520 --> 00:44:40,880
я хочу, чтобы вы вынесли из этого

1295
00:44:40,880 --> 00:44:42,640
то, что у нас был контракт

1296
00:44:42,640 --> 00:44:44,560
с исходной моделью.

1297
00:44:44,560 --> 00:44:46,240
Контракт заключался в том, что он определял

1298
00:44:46,240 --> 00:44:48,400
распределения вероятностей,

1299
00:44:48,400 --> 00:44:49,760
но когда мы тонко настраиваем, когда мы

1300
00:44:49,760 --> 00:44:51,440
взаимодействуем с предварительно обученной моделью,

1301
00:44:51,440 --> 00:44:53,200
то, что у нас также есть, точно такие же, как

1302
00:44:53,200 --> 00:44:54,480
обученные веса и сетевая

1303
00:44:54,480 --> 00:44:56,560
архитектура, которые нам не нужно использовать  это

1304
00:44:56,560 --> 00:44:58,000
как языковая модель, нам не нужно использовать ее

1305
00:44:58,000 --> 00:44:59,839
в качестве распределения вероятностей, когда мы на

1306
00:44:59,839 --> 00:45:01,760
самом деле ее настраиваем, мы на самом деле

1307
00:45:01,760 --> 00:45:04,160
просто используем ее для инициализации

1308
00:45:04,160 --> 00:45:05,680
ее параметров и говорим, что это

1309
00:45:05,680 --> 00:45:06,560


1310
00:45:06,560 --> 00:45:08,640
просто декодер-преобразователь, который был

1311
00:45:08,640 --> 00:45:10,400
предварительно обученный,

1312
00:45:10,400 --> 00:45:12,400
и это действительно здорово в

1313
00:45:12,400 --> 00:45:14,319
том смысле, что, когда вы обнаруживаете тунца на некоторых из

1314
00:45:14,319 --> 00:45:15,760
некоторых данных настроения, он действительно

1315
00:45:15,760 --> 00:45:17,680


1316
00:45:17,680 --> 00:45:18,880
хорошо работает, но есть второй способ

1317
00:45:18,880 --> 00:45:20,560
взаимодействия с декодером с

1318
00:45:20,560 --> 00:45:22,240
предварительно обученными декодерами, который есть в некоторых

1319
00:45:22,240 --> 00:45:24,800
В смысле даже более естественным, на самом деле это

1320
00:45:24,800 --> 00:45:27,280
ближе к контракту, с которого мы начали

1321
00:45:27,280 --> 00:45:30,079
, поэтому нам не нужно просто игнорировать тот

1322
00:45:30,079 --> 00:45:31,119
факт, что это было распределение вероятностей,

1323
00:45:31,119 --> 00:45:33,920
мы можем использовать

1324
00:45:33,920 --> 00:45:35,520
его, все еще настраивая его, так что вот

1325
00:45:35,520 --> 00:45:37,119
что мы  собираюсь т  для

1326
00:45:37,119 --> 00:45:39,359
этого мы можем использовать их в качестве генератора во время

1327
00:45:39,359 --> 00:45:41,599
точной настройки генератором, я имею в

1328
00:45:41,599 --> 00:45:43,440
виду, что вы узнаете, что определите это

1329
00:45:43,440 --> 00:45:45,119
распределение слов с учетом их

1330
00:45:45,119 --> 00:45:47,119
контекста,

1331
00:45:47,119 --> 00:45:49,280
а затем мы на самом деле просто точно настроим

1332
00:45:49,280 --> 00:45:51,680
это распределение вероятностей,

1333
00:45:51,680 --> 00:45:54,480
поэтому в такой задаче, как di  как какой-то

1334
00:45:54,480 --> 00:45:57,200
пошаговый диалог, мы можем закодировать

1335
00:45:57,200 --> 00:46:01,440
историю диалога как ваш прошлый контекст,

1336
00:46:01,440 --> 00:46:03,359
чтобы вы знали, что у вас есть

1337
00:46:03,359 --> 00:46:05,359
история диалога некоторых

1338
00:46:05,359 --> 00:46:06,480
вещей, которые люди говорят

1339
00:46:06,480 --> 00:46:08,000
друг другу, вы кодируете это как

1340
00:46:08,000 --> 00:46:10,000
слова и пытаетесь предсказать  следующие

1341
00:46:10,000 --> 00:46:11,680
слова в диалоге

1342
00:46:11,680 --> 00:46:12,880
правильно и, возможно, ваша цель перед тренировкой

1343
00:46:12,880 --> 00:46:14,800
вы просмотрели текст очень общего

1344
00:46:14,800 --> 00:46:17,040
назначения из я не знаю википедии,

1345
00:46:17,040 --> 00:46:18,560
книг или чего-то еще, и вы

1346
00:46:18,560 --> 00:46:20,720
настраиваете его как языковую модель, но у

1347
00:46:20,720 --> 00:46:21,839
вас все в порядке ...  настройка его в качестве языковой

1348
00:46:21,839 --> 00:46:23,359
модели

1349
00:46:23,359 --> 00:46:25,760
для такого рода предметно-ориентированного

1350
00:46:25,760 --> 00:46:27,839
распределения текста, такого как диалог или,

1351
00:46:27,839 --> 00:46:29,839
возможно, резюмирование, когда вы

1352
00:46:29,839 --> 00:46:33,040
вставляете весь документ, а затем

1353
00:46:33,040 --> 00:46:35,839
произносите определенное слово, а затем резюме и

1354
00:46:35,839 --> 00:46:37,920
говорите предсказать сумму  Мэри, и так,

1355
00:46:37,920 --> 00:46:39,839
как это выглядит, снова время точной

1356
00:46:39,839 --> 00:46:42,079
настройки здесь у

1357
00:46:42,079 --> 00:46:44,319
вас есть ваш h1 to ht, равный

1358
00:46:44,319 --> 00:46:47,359
декодеру слов, а затем у вас

1359
00:46:47,359 --> 00:46:49,040
есть этот дистрибутив, который вы

1360
00:46:49,040 --> 00:46:53,200
настраиваете wt's ah - это снова опечатка

1361
00:46:53,200 --> 00:46:56,880
ht  минус 1 плюс b, так что теперь каждый раз, когда у меня

1362
00:46:56,880 --> 00:46:58,000
есть это

1363
00:46:58,000 --> 00:47:00,079
um, я предсказываю эти слова по первому слову

1364
00:47:00,079 --> 00:47:01,920
, я предсказываю слово два или два, предсказываю

1365
00:47:01,920 --> 00:47:05,040
слово три и т.д.

1366
00:47:05,040 --> 00:47:07,440


1367
00:47:07,440 --> 00:47:08,880


1368
00:47:08,880 --> 00:47:10,960
-подготовлен, но я все еще тонко

1369
00:47:10,960 --> 00:47:12,160
настраиваю все

1370
00:47:12,160 --> 00:47:14,880
правильно, поэтому a и b здесь отображают, чтобы

1371
00:47:14,880 --> 00:47:16,640
вроде как распределение вероятностей по

1372
00:47:16,640 --> 00:47:18,400
моему словарю

1373
00:47:18,400 --> 00:47:19,520
или логитам распределения вероятностей,

1374
00:47:19,520 --> 00:47:22,079
и гм, и я просто могу

1375
00:47:22,079 --> 00:47:23,920
вроде как настроить их сейчас

1376
00:47:23,920 --> 00:47:25,440
чтобы дистрибутив, который я собираюсь

1377
00:47:25,440 --> 00:47:28,400
использовать, отражал такую вещь, как диалог, который я

1378
00:47:28,400 --> 00:47:31,280
хочу, чтобы он отражал

1379
00:47:32,480 --> 00:47:33,760
нормально, так что это два способа

1380
00:47:33,760 --> 00:47:36,720
взаимодействия с предварительно обученным декодером,

1381
00:47:36,720 --> 00:47:39,359
вот пример того, что в

1382
00:47:39,359 --> 00:47:42,240
итоге оказалось первым  линии безумного

1383
00:47:42,240 --> 00:47:43,440


1384
00:47:43,440 --> 00:47:46,559
успеха  Я или, по крайней мере, говорил о

1385
00:47:46,559 --> 00:47:48,880
предварительно обученных декодерах,

1386
00:47:48,880 --> 00:47:51,359
поэтому генеративный предварительно обученный декодер или

1387
00:47:51,359 --> 00:47:53,839
gpt

1388
00:47:54,400 --> 00:47:56,160
имел огромный успех

1389
00:47:56,160 --> 00:47:58,480
в каком-то смысле, или, по крайней мере, он получил

1390
00:47:58,480 --> 00:48:00,640
много шума, так что это

1391
00:48:00,640 --> 00:48:03,680
трансформаторный декодер без кодировщика с 12

1392
00:48:03,680 --> 00:48:05,760
слоями  Я даю вам детали, чтобы вы

1393
00:48:05,760 --> 00:48:07,839
могли начать

1394
00:48:07,839 --> 00:48:09,760
понимать, как размер вещей

1395
00:48:09,760 --> 00:48:12,240
меняется с годами, поскольку мы продолжим

1396
00:48:12,240 --> 00:48:13,359
прогрессировать здесь,

1397
00:48:13,359 --> 00:48:15,040
если каждое из наших скрытых

1398
00:48:15,040 --> 00:48:18,000
состояний имеет размерность 768.

1399
00:48:18,000 --> 00:48:19,520
Итак, если вы  Вспомните, в прошлой лекции

1400
00:48:19,520 --> 00:48:21,839
у нас был термин d, который был нашей

1401
00:48:21,839 --> 00:48:24,559
размерностью, поэтому d равно

1402
00:48:24,559 --> 00:48:25,839
768. А затем

1403
00:48:25,839 --> 00:48:27,119
интересное утверждение, которое вы должны

1404
00:48:27,119 --> 00:48:29,200
иметь в виду для тех, кто разбирается в технике,

1405
00:48:29,200 --> 00:48:31,040
заключается в том, что на самом деле

1406
00:48:31,040 --> 00:48:32,319
слои прямой связи у вас есть  скрытый слой

1407
00:48:32,319 --> 00:48:33,920
в фиде извините меня в

1408
00:48:33,920 --> 00:48:35,760
слое с прямой связью, и он был на самом деле очень

1409
00:48:35,760 --> 00:48:37,599
большим, поэтому у вас были такие похожие по

1410
00:48:37,599 --> 00:48:40,000
положению слои с прямой связью справа

1411
00:48:40,000 --> 00:48:41,200
и гм,

1412
00:48:41,200 --> 00:48:43,119
а слой с прямой связью принял

1413
00:48:43,119 --> 00:48:45,520
бы 768-мерный векторный вид вроде как

1414
00:48:45,520 --> 00:48:48,480
проецировать его  до 3000 дн.  пространственное пространство создает

1415
00:48:48,480 --> 00:48:50,319
своего рода нелинейность, а затем

1416
00:48:50,319 --> 00:48:52,640
проецирует ее обратно на 768. Это в конечном

1417
00:48:52,640 --> 00:48:55,040
итоге потому, что вы можете сжать гораздо

1418
00:48:55,040 --> 00:48:57,119
больше параметров для не слишком большого количества

1419
00:48:57,119 --> 00:48:59,040
вычислений таким образом, но это

1420
00:48:59,040 --> 00:49:01,119
любопытно,

1421
00:49:01,119 --> 00:49:03,680
хорошо, тогда, а затем  путем парного кодирования

1422
00:49:03,680 --> 00:49:05,040
это на самом деле

1423
00:49:05,040 --> 00:49:06,240
было это кодирование печати было

1424
00:49:06,240 --> 00:49:09,119
словарём подслов с 40 000 слияний, поэтому

1425
00:49:09,119 --> 00:49:10,880
40 000 слияний, так что это не

1426
00:49:10,880 --> 00:49:13,040
размер словаря, потому что вы начали

1427
00:49:13,040 --> 00:49:15,040
с кучи символов, я не помню, с

1428
00:49:15,040 --> 00:49:17,119
какого количества символов они начали

1429
00:49:17,119 --> 00:49:18,640
но так что это относительно небольшой

1430
00:49:18,640 --> 00:49:22,160
словарный запас, который вы можете увидеть правильно, э-э,

1431
00:49:22,160 --> 00:49:24,000
по сравнению с тем, если бы вы пытались сказать, чтобы

1432
00:49:24,000 --> 00:49:25,760
каждое слово имело уникальное

1433
00:49:25,760 --> 00:49:27,520
представление, теперь оно будет

1434
00:49:27,520 --> 00:49:30,000
обучено на книгах, корпорациях, у него есть 000

1435
00:49:30,000 --> 00:49:31,520
уникальных книг,

1436
00:49:31,520 --> 00:49:33,839
и он содержит длинные промежутки смежных

1437
00:49:33,839 --> 00:49:36,720
text, поэтому вместо того, чтобы говорить,

1438
00:49:36,720 --> 00:49:38,559
тренировать его на отдельных предложениях, просто на

1439
00:49:38,559 --> 00:49:41,200
небольших коротких предложениях, модель

1440
00:49:41,200 --> 00:49:43,599
может изучать зависимости на большом расстоянии,

1441
00:49:43,599 --> 00:49:45,920
потому что вы не разбили, как книга

1442
00:49:45,920 --> 00:49:47,680
int  o случайные предложения и перемешивание их

1443
00:49:47,680 --> 00:49:49,280
всех вокруг, где вы были, вы как бы

1444
00:49:49,280 --> 00:49:51,599
сохраняли его непрерывным, чтобы он мог иметь

1445
00:49:51,599 --> 00:49:53,760
такую последовательность,

1446
00:49:53,760 --> 00:49:55,520
а затем немного, э-э, здесь

1447
00:49:55,520 --> 00:49:57,359
да, так что gpt никогда не появлялся в

1448
00:49:57,359 --> 00:49:59,599
исходной статье или исходном блоге  пост

1449
00:49:59,599 --> 00:50:01,520
как аббревиатура, и на самом деле это может

1450
00:50:01,520 --> 00:50:03,599
относиться к генеративному

1451
00:50:03,599 --> 00:50:05,599
предварительному обучению, что вроде того,

1452
00:50:05,599 --> 00:50:07,119
что предлагает название статьи, или

1453
00:50:07,119 --> 00:50:09,200
генеративному предварительно обученному преобразователю,

1454
00:50:09,200 --> 00:50:10,640
и я вроде как решил сказать

1455
00:50:10,640 --> 00:50:12,000
генеративный предварительно обученный трансформатор

1456
00:50:12,000 --> 00:50:15,119
потому что это казалось слишком общим,

1457
00:50:15,119 --> 00:50:17,599
так что

1458
00:50:17,599 --> 00:50:20,720
ладно, так что они предварительно обучили

1459
00:50:20,720 --> 00:50:22,160
этот огромный преобразователь языковой модели, этот огромный

1460
00:50:22,160 --> 00:50:25,280
декодер-преобразователь всего на 7000 книг,

1461
00:50:25,280 --> 00:50:27,040
и они настроили его для ряда

1462
00:50:27,040 --> 00:50:28,319
различных задач, и я хочу немного поговорить

1463
00:50:28,319 --> 00:50:29,520
о деталях того, как

1464
00:50:29,520 --> 00:50:31,440
они его настраивали,

1465
00:50:31,440 --> 00:50:32,240
и

1466
00:50:32,240 --> 00:50:33,920
поэтому они настраивали его для одной конкретной

1467
00:50:33,920 --> 00:50:34,880
задачи

1468
00:50:34,880 --> 00:50:36,319
или семейства задач, называемых

1469
00:50:36,319 --> 00:50:38,000
логическим выводом

1470
00:50:38,000 --> 00:50:38,720


1471
00:50:38,720 --> 00:50:39,920
на естественном языке, поэтому при логическом выводе на естественном языке мы

1472
00:50:39,920 --> 00:50:42,559
маркируем пары предложений  они влекут за собой

1473
00:50:42,559 --> 00:50:43,839
или противоречат друг другу в

1474
00:50:43,839 --> 00:50:46,000
нейтральном смысле, поэтому у вас есть предпосылка, и вы

1475
00:50:46,000 --> 00:50:48,400
придерживаетесь этой предпосылки как своего рода правду, что человек

1476
00:50:48,400 --> 00:50:49,920
находится в дверном проеме,

1477
00:50:49,920 --> 00:50:52,319
и у вас есть гипотеза, что человек находится

1478
00:50:52,319 --> 00:50:53,760
рядом с дверью,

1479
00:50:53,760 --> 00:50:56,800
если этот человек имеет в виду этого человека

1480
00:50:56,800 --> 00:50:58,800
э-э  тогда вы знаете, что это вроде как о

1481
00:50:58,800 --> 00:51:01,520
да, это как бы влечет за собой, потому что

1482
00:51:01,520 --> 00:51:02,880
есть человек, потому что мужчина - это

1483
00:51:02,880 --> 00:51:04,880
личность, и они в дверном проеме, тогда

1484
00:51:04,880 --> 00:51:06,319
они рядом с дверью,

1485
00:51:06,319 --> 00:51:08,319
так что у вас есть такого рода логические

1486
00:51:08,319 --> 00:51:10,880
рассуждения, что вы  делаю э-э, или вы

1487
00:51:10,880 --> 00:51:12,720
должны уметь делать, и

1488
00:51:12,720 --> 00:51:14,160
вы маркируете эти предложения, так что

1489
00:51:14,160 --> 00:51:15,359
это помеченная

1490
00:51:15,359 --> 00:51:17,839
задача, у вас есть своего рода ввод, который

1491
00:51:17,839 --> 00:51:19,760
разрезан на две части, а затем один из трех

1492
00:51:19,760 --> 00:51:20,960
выходов

1493
00:51:20,960 --> 00:51:22,400
хорошо, так

1494
00:51:22,400 --> 00:51:24,880
что gpt  paper оценивает эту задачу,

1495
00:51:24,880 --> 00:51:26,319
но то, что у них есть, это

1496
00:51:26,319 --> 00:51:28,559
декодер-преобразователь, так что они делают, что

1497
00:51:28,559 --> 00:51:30,400
они делают ...

1498
00:51:30,400 --> 00:51:32,559


1499
00:51:32,559 --> 00:51:36,000


1500
00:51:36,000 --> 00:51:37,760


1501
00:51:37,760 --> 00:51:39,280


1502
00:51:39,280 --> 00:51:41,440
какую задачу ты

1503
00:51:41,440 --> 00:51:42,400


1504
00:51:42,400 --> 00:51:45,040
делаешь  Мы собираемся просто отформатировать задачу

1505
00:51:45,040 --> 00:51:47,040
как кучу токенов

1506
00:51:47,040 --> 00:51:49,520
и не менять вашу архитектуру,

1507
00:51:49,520 --> 00:51:51,839
потому что предварительное обучение было настолько полезным,

1508
00:51:51,839 --> 00:51:53,040
что, вероятно, лучше сохранить

1509
00:51:53,040 --> 00:51:56,160
фиксированную архитектуру перед обучением, а затем

1510
00:51:56,160 --> 00:51:58,960
изменить спецификацию задачи, чтобы она

1511
00:51:58,960 --> 00:52:00,480
соответствовала предварительному  -обученная архитектура, так что

1512
00:52:00,480 --> 00:52:03,200
они сделали правильно, они положили этот токен на начало,

1513
00:52:03,200 --> 00:52:04,880
это особый токен,

1514
00:52:04,880 --> 00:52:08,319
человек находится в дверном проеме, какой-то

1515
00:52:08,319 --> 00:52:09,760
токен-разделитель справа, так что это просто линейная

1516
00:52:09,760 --> 00:52:11,599
последовательность токенов, которую мы даем как

1517
00:52:11,599 --> 00:52:14,160
один большой префикс для gpt

1518
00:52:14,160 --> 00:52:17,119
um и  затем человек находится возле двери,

1519
00:52:17,119 --> 00:52:19,680
а затем какой-то дополнительный токен здесь справа,

1520
00:52:19,680 --> 00:52:21,359
извлеките,

1521
00:52:21,359 --> 00:52:23,599
а затем вы знаете линейный классификатор

1522
00:52:23,599 --> 00:52:25,359
, о котором мы говорили, и своего рода

1523
00:52:25,359 --> 00:52:28,000
первый способ взаимодействия с

1524
00:52:28,000 --> 00:52:30,880
моделями с моделями декодера, который применяется

1525
00:52:30,880 --> 00:52:33,760
к представлению

1526
00:52:33,760 --> 00:52:35,599
токена извлечения  Итак, у вас есть последнее

1527
00:52:35,599 --> 00:52:38,079
скрытое состояние поверх извлечения, а затем

1528
00:52:38,079 --> 00:52:39,839
вы настраиваете всю сеть, чтобы

1529
00:52:39,839 --> 00:52:42,079
правильно предсказать эти метки, и поэтому этот

1530
00:52:42,079 --> 00:52:45,599
вид форматирования ввода все

1531
00:52:45,599 --> 00:52:48,000
чаще используется  d, чтобы

1532
00:52:48,000 --> 00:52:50,240
архитектура модели оставалась неизменной и

1533
00:52:50,240 --> 00:52:51,839
позволяла решать множество различных проблем,

1534
00:52:51,839 --> 00:52:53,839
с ней

1535
00:52:53,839 --> 00:52:55,599
все в порядке, и так ли это работало с

1536
00:52:55,599 --> 00:52:58,240
логическим выводом на естественном языке, ответ - да,

1537
00:52:58,240 --> 00:52:59,520
так что здесь есть несколько разных чисел,

1538
00:52:59,520 --> 00:53:01,440
я бы не стал беспокоиться  слишком много об

1539
00:53:01,440 --> 00:53:02,960
этом отлаженная языковая модель-трансформер

1540
00:53:02,960 --> 00:53:05,280
- это своего рода то, на что вы должны обратить

1541
00:53:05,280 --> 00:53:07,440
внимание э-э, есть много усилий,

1542
00:53:07,440 --> 00:53:09,440
которые были правильно вложены в другие модели,

1543
00:53:09,440 --> 00:53:10,640
и это как бы история о том, как

1544
00:53:10,640 --> 00:53:12,400
люди, прошедшие предварительное обучение, приложили много усилий

1545
00:53:12,400 --> 00:53:13,920
в модели, которые делают различного рода

1546
00:53:13,920 --> 00:53:16,240
осторожные вещи, а затем вы берете один-

1547
00:53:16,240 --> 00:53:18,960
единственный трансформатор и говорите, что

1548
00:53:18,960 --> 00:53:20,400
я собираюсь предварительно обучить его на тонне

1549
00:53:20,400 --> 00:53:22,319
текста и не слишком беспокоиться

1550
00:53:22,319 --> 00:53:24,160
ни о чем другом, просто настройте его, и

1551
00:53:24,160 --> 00:53:27,680
вы закончите  делать супер супер хорошо

1552
00:53:27,760 --> 00:53:30,160
иногда не намного лучше в

1553
00:53:30,160 --> 00:53:31,680
случае gpt, чем некоторые

1554
00:53:31,680 --> 00:53:34,319
из самых известных современных методов, но

1555
00:53:34,319 --> 00:53:36,400
обычно немного лучше и

1556
00:53:36,400 --> 00:53:38,160
снова количество усилий количество

1557
00:53:38,160 --> 00:53:39,680
конкретных усилий, которые вы должны  вложить

1558
00:53:39,680 --> 00:53:42,559
в это очень  низкий,

1559
00:53:42,559 --> 00:53:44,960
хорошо, а как насчет другого способа

1560
00:53:44,960 --> 00:53:46,400
взаимодействия с декодерами? Итак,

1561
00:53:46,400 --> 00:53:47,599
мы сказали, что мы можем взаимодействовать

1562
00:53:47,599 --> 00:53:49,359
с кодерами, просто выбирая из них,

1563
00:53:49,359 --> 00:53:50,720
просто говоря, что это вероятностные

1564
00:53:50,720 --> 00:53:53,280
распределения, так что мы можем использовать их в

1565
00:53:53,280 --> 00:53:55,520
их качестве как  языковые модели и

1566
00:53:55,520 --> 00:53:58,720
gpt2, на самом деле это просто более

1567
00:53:58,720 --> 00:54:00,400
крупный

1568
00:54:00,400 --> 00:54:02,079
gpt, не беспокойтесь об этом

1569
00:54:02,079 --> 00:54:04,960
с большими скрытыми единицами, большим количеством слоев,

1570
00:54:04,960 --> 00:54:06,800
когда он был обучен на большем количестве данных, было

1571
00:54:06,800 --> 00:54:09,839
показано, что он создает своего рода относительно

1572
00:54:09,839 --> 00:54:11,760
убедительные образцы естественного языка,

1573
00:54:11,760 --> 00:54:13,680
так что это  это то, что часто ходило в

1574
00:54:13,680 --> 00:54:15,119
твиттере, так что у вас есть

1575
00:54:15,119 --> 00:54:18,079
такой надуманный пример, который, вероятно

1576
00:54:18,079 --> 00:54:19,440
, не отображался

1577
00:54:19,440 --> 00:54:21,760
в данных обучения, где вы знаете, что

1578
00:54:21,760 --> 00:54:24,640
ученый обнаруживает стадо единорогов,

1579
00:54:24,640 --> 00:54:27,760
а затем они вроде как образец из

1580
00:54:27,760 --> 00:54:28,720


1581
00:54:28,720 --> 00:54:29,839
э-э,

1582
00:54:29,839 --> 00:54:31,440
почти  распределение модели

1583
00:54:31,440 --> 00:54:34,480
они вроде как придают модели немного ээээ,

1584
00:54:34,480 --> 00:54:36,720
некоторую заслугу здесь

1585
00:54:36,720 --> 00:54:38,640
они делают что-то, называемое усечением

1586
00:54:38,640 --> 00:54:40,400
распределения языковых моделей, чтобы

1587
00:54:40,400 --> 00:54:41,680
своего рода вырезать

1588
00:54:41,680 --> 00:54:45,280
шум  в gpt2, так что это не

1589
00:54:45,280 --> 00:54:49,040
совсем идеальный образец, но более или менее gpt2

1590
00:54:49,040 --> 00:54:51,839
сгенерировал это,

1591
00:54:51,839 --> 00:54:53,040
и поэтому у вас есть ученые,

1592
00:54:53,040 --> 00:54:55,599
открывающие единорогов, а затем вы знаете,

1593
00:54:55,599 --> 00:54:57,599
что у вас есть эта последовательность, хорошо,

1594
00:54:57,599 --> 00:54:58,960
есть ученый,

1595
00:54:58,960 --> 00:55:01,599
а вы знаете, что они дают ему

1596
00:55:01,599 --> 00:55:05,200
имя ммм вы  Вы обращаетесь к

1597
00:55:05,200 --> 00:55:07,200
этому, ну

1598
00:55:07,200 --> 00:55:08,000


1599
00:55:08,000 --> 00:55:09,680
да, вы ссылаетесь на имя ученого, у

1600
00:55:09,680 --> 00:55:11,920
вас вроде есть такие

1601
00:55:11,920 --> 00:55:14,640
вещи, как согласованность темы, а также

1602
00:55:14,640 --> 00:55:17,200
синтаксис действительно хорош, ну, похоже, вы знаете

1603
00:55:17,200 --> 00:55:18,559
смутно как английский,

1604
00:55:18,559 --> 00:55:20,000
и поэтому

1605
00:55:20,000 --> 00:55:21,280
тенденция по мере того, как мы становимся все больше и больше

1606
00:55:21,280 --> 00:55:22,880
языковых моделей, которые мы фактически выбираем из

1607
00:55:22,880 --> 00:55:24,720
них, даже когда мы даем им подсказки, которые

1608
00:55:24,720 --> 00:55:27,280
выглядят немного странно, и они кажутся

1609
00:55:27,280 --> 00:55:29,680
все более убедительными,

1610
00:55:29,680 --> 00:55:31,440
хорошо,

1611
00:55:31,440 --> 00:55:33,040
так что

1612
00:55:33,040 --> 00:55:35,440
кодировщики с предварительным обучением

1613
00:55:35,440 --> 00:55:38,480
хорошо, кодеры с предварительным обучением давайте

1614
00:55:38,480 --> 00:55:39,920
возьмем еще секунду  потому что мне нужно

1615
00:55:39,920 --> 00:55:41,920
немного воды здесь,

1616
00:55:41,920 --> 00:55:46,040
если есть другой вопрос, дайте мне знать,

1617
00:55:50,160 --> 00:55:52,400
хорошо,

1618
00:55:53,200 --> 00:55:55,920
так что преимущество кодировщиков

1619
00:55:55,920 --> 00:55:57,359
, о которых мы говорили, заключалось в том, что они получают

1620
00:55:57,359 --> 00:55:59,520
этот двунаправленный контекст  так что вы можете,

1621
00:55:59,520 --> 00:56:00,839
пока вы создаете

1622
00:56:00,839 --> 00:56:03,280
представления своего

1623
00:56:03,280 --> 00:56:04,880
предложения из своих частей предложений, вы

1624
00:56:04,880 --> 00:56:06,640
можете смотреть в будущее, и это может помочь

1625
00:56:06,640 --> 00:56:08,160
вам построить лучшее представление

1626
00:56:08,160 --> 00:56:10,079
слова, на которое вы смотрите прямо сейчас,

1627
00:56:10,079 --> 00:56:11,920
но большая проблема в том, что мы  Сейчас мы не можем заниматься

1628
00:56:11,920 --> 00:56:13,280
языковым моделированием, поэтому мы в значительной

1629
00:56:13,280 --> 00:56:15,760
степени только сказали, что полагались на эту

1630
00:56:15,760 --> 00:56:17,680
задачу, что мы уже знали о языковом

1631
00:56:17,680 --> 00:56:19,599
моделировании, чтобы провести предварительное обучение, но теперь

1632
00:56:19,599 --> 00:56:22,640
мы хотим предварительно обучить кодировщики, и поэтому мы

1633
00:56:22,640 --> 00:56:23,920
можем '  Мы не можем его использовать, так что мы

1634
00:56:23,920 --> 00:56:25,680
собираемся делать

1635
00:56:25,680 --> 00:56:26,960


1636
00:56:26,960 --> 00:56:28,640
вот это решение, которое было предложено

1637
00:56:28,640 --> 00:56:30,480
в

1638
00:56:30,480 --> 00:56:32,160
статье, в которой был представлен языковой мод

1639
00:56:32,160 --> 00:56:33,119


1640
00:56:33,119 --> 00:56:34,640
модели под названием burt,

1641
00:56:34,640 --> 00:56:37,119
это называется моделированием замаскированного языка, так что

1642
00:56:37,119 --> 00:56:38,079


1643
00:56:38,079 --> 00:56:40,559
вот идея, которую мы получаем  предложение,

1644
00:56:40,559 --> 00:56:42,160
а затем мы просто берем часть

1645
00:56:42,160 --> 00:56:44,960
слов и заменяем их своего рода

1646
00:56:44,960 --> 00:56:48,000
токеном маски, токеном, который

1647
00:56:48,000 --> 00:56:49,920
означает, что вы не знаете, что это

1648
00:56:49,920 --> 00:56:52,720
сейчас, а затем вы предсказываете эти слова

1649
00:56:52,720 --> 00:56:54,000
некоторые детали, которые мы  перейти на следующем

1650
00:56:54,000 --> 00:56:56,000
слайде, но так вот почему  Похоже,

1651
00:56:56,000 --> 00:56:59,040
что у нас есть предложение, которое я маскирую

1652
00:56:59,040 --> 00:57:01,040
в маску,

1653
00:57:01,040 --> 00:57:02,559
мы получаем некоторые скрытые состояния для

1654
00:57:02,559 --> 00:57:03,920
всех правильно, поэтому мы не тренировались,

1655
00:57:03,920 --> 00:57:05,200
мы вообще не

1656
00:57:05,200 --> 00:57:08,319
меняли кодировщик трансформатора,

1657
00:57:08,319 --> 00:57:10,319
мы просто сказали хорошо, вот как эта

1658
00:57:10,319 --> 00:57:11,839
последовательность  вы увидите все правильно,

1659
00:57:11,839 --> 00:57:13,599
посмотрите на все стрелки, идущие повсюду,

1660
00:57:13,599 --> 00:57:15,520
но тогда, гм,

1661
00:57:15,520 --> 00:57:17,520
у нас есть этот слой прогнозирования,

1662
00:57:17,520 --> 00:57:19,040
который был бы, если бы

1663
00:57:19,040 --> 00:57:21,040
мы правильно тренировались и использовали его, у нас

1664
00:57:21,040 --> 00:57:23,119
есть потери только

1665
00:57:23,119 --> 00:57:26,319
в тех словах, где мы  были маски здесь, так что

1666
00:57:26,319 --> 00:57:28,240
я замаскировал это, а затем я должен

1667
00:57:28,240 --> 00:57:30,559
предсказать, что это был wendt, который пошел сюда,

1668
00:57:30,559 --> 00:57:32,559
и store, который пошел сюда,

1669
00:57:32,559 --> 00:57:34,400
и теперь это очень похоже на языковое

1670
00:57:34,400 --> 00:57:36,000
моделирование, вы можете сказать,

1671
00:57:36,000 --> 00:57:37,440
но теперь вам не нужно иметь такого

1672
00:57:37,440 --> 00:57:39,520
рода  разложение слева направо,

1673
00:57:39,520 --> 00:57:40,559
вы говорите, что

1674
00:57:40,559 --> 00:57:42,000
я собираюсь удалить некоторые слова,

1675
00:57:42,000 --> 00:57:43,920
и вы должны предсказать, что они

1676
00:57:43,920 --> 00:57:45,839
собой представляют.

1677
00:57:45,839 --> 00:57:48,319


1678
00:57:48,319 --> 00:57:50,079


1679
00:57:50,079 --> 00:57:52,640
ммм

1680
00:57:52,640 --> 00:57:54,319
так что они на самом деле  Они

1681
00:57:54,319 --> 00:57:56,400
предложили моделирование массового языка, и

1682
00:57:56,400 --> 00:57:57,760
они выпустили веса

1683
00:57:57,760 --> 00:57:59,520
этого предварительно обученного преобразователя с

1684
00:57:59,520 --> 00:58:01,599
немного большей сложностью, чтобы заставить

1685
00:58:01,599 --> 00:58:03,359
моделирование массового языка работать,

1686
00:58:03,359 --> 00:58:04,960
поэтому

1687
00:58:04,960 --> 00:58:07,200
вы собираетесь взять случайные 15

1688
00:58:07,200 --> 00:58:08,960
токенов подслов,

1689
00:58:08,960 --> 00:58:11,119
чтобы было  это было правдой, но вы не

1690
00:58:11,119 --> 00:58:14,160
всегда собираетесь заменять их маской,

1691
00:58:14,160 --> 00:58:15,839
вы можете думать об этом так, как

1692
00:58:15,839 --> 00:58:18,319
если бы модель видит маркер маски,

1693
00:58:18,319 --> 00:58:19,760
она получает гарантию, что ей нужно

1694
00:58:19,760 --> 00:58:21,440
что-то предсказать,

1695
00:58:21,440 --> 00:58:23,119
и если модель не видит маркер маски,

1696
00:58:23,119 --> 00:58:25,440
это  получает гарантию, что ему

1697
00:58:25,440 --> 00:58:27,280
не нужно ничего предсказывать, так

1698
00:58:27,280 --> 00:58:28,720
зачем ему создавать сильные

1699
00:58:28,720 --> 00:58:31,200
представления слов, которые не

1700
00:58:31,200 --> 00:58:32,720
замаскированы,

1701
00:58:32,720 --> 00:58:34,160
и я хочу, чтобы моя модель строила сильные

1702
00:58:34,160 --> 00:58:36,640
представления всего, поэтому мы

1703
00:58:36,640 --> 00:58:38,400
собираемся добавить некоторую неопределенность

1704
00:58:38,400 --> 00:58:39,920
к модели, поэтому мы собираемся

1705
00:58:39,920 --> 00:58:41,359
использовать эти пятнадцать процентов токенов в

1706
00:58:41,359 --> 00:58:42,880
восьмидесяти процентах случаев, когда мы собираемся

1707
00:58:42,880 --> 00:58:45,040
заменить ее маской, которая была нашей

1708
00:58:45,040 --> 00:58:47,760
первоначальной идеей моделирования массового языка.

1709
00:58:47,760 --> 00:58:49,760
мы на

1710
00:58:49,760 --> 00:58:51,040
самом деле собираемся заменить слово

1711
00:58:51,040 --> 00:58:53,040
просто случайным токеном, просто случайным

1712
00:58:53,040 --> 00:58:54,720
словарным элементом,

1713
00:58:54,720 --> 00:58:56,079
это может быть что угодно,

1714
00:58:56,079 --> 00:58:57,440
а затем в остальные 10 случаев мы

1715
00:58:57,440 --> 00:58:59,520
собираемся оставить слово без изменений,

1716
00:58:59,520 --> 00:59:02,880
поэтому теперь он сразу видит слово,

1717
00:59:02,880 --> 00:59:05,440
которое может быть  случайный токен, или он может

1718
00:59:05,440 --> 00:59:07,040
быть неизменным,

1719
00:59:07,040 --> 00:59:09,119
и если я увижу маску, я знаю, что мне нужно ее

1720
00:59:09,119 --> 00:59:10,640
предсказать,

1721
00:59:10,640 --> 00:59:12,960
так что эти две вещи здесь говорят, что

1722
00:59:12,960 --> 00:59:15,200
вы должны как бы делать это, что

1723
00:59:15,200 --> 00:59:17,119
вы должны быть начеку для каждого

1724
00:59:17,119 --> 00:59:18,799
слова  в вашем в вашем представлении, так что

1725
00:59:18,799 --> 00:59:21,680
вот я пицца с маской

1726
00:59:21,680 --> 00:59:23,280
правильно, оказывается, и модель этого не

1727
00:59:23,280 --> 00:59:25,839
знала, но она получает три

1728
00:59:25,839 --> 00:59:28,400
терма для предложения, у него только

1729
00:59:28,400 --> 00:59:30,960
одна маска, но он будет наказан

1730
00:59:30,960 --> 00:59:32,559
за предсказание трех разных вещей  ему

1731
00:59:32,559 --> 00:59:33,839
нужно предсказать, что это слово на

1732
00:59:33,839 --> 00:59:35,359
самом деле ушло,

1733
00:59:35,359 --> 00:59:37,440
поэтому я заменил это

1734
00:59:37,440 --> 00:59:39,359
ему нужно предсказать, что это слово

1735
00:59:39,359 --> 00:59:41,599
на самом деле является словом to,

1736
00:59:41,599 --> 00:59:43,040
а затем ему нужно предсказать, что это

1737
00:59:43,040 --> 00:59:44,839
слово фактически

1738
00:59:44,839 --> 00:59:47,520
хранится сейчас в качестве короткой интерлюдии, которую вы могли бы

1739
00:59:47,520 --> 00:59:49,200
Думаю, ты думаешь, Джон

1740
00:59:49,200 --> 00:59:50,720
модель никак не могла знать об этом,

1741
00:59:50,720 --> 00:59:54,000
это настолько недооценено, вы знаете,

1742
00:59:54,000 --> 00:59:56,240
хорошо, я понимаю, что пицца немного странная, я признаю,

1743
00:59:56,240 --> 00:59:57,440
но просто нет способа узнать, что

1744
00:59:57,440 --> 00:59:58,880
есть магазин, в который я вошел, я имею в виду, что

1745
00:59:58,880 --> 01:00:00,160
то же самое верно и в отношении языкового

1746
01:00:00,160 --> 01:00:02,319
моделирования, так что  в конечном итоге он

1747
01:00:02,319 --> 01:00:03,920
изучит эти средние статистические данные о

1748
01:00:03,920 --> 01:00:05,520
том, что обычно происходит в данном

1749
01:00:05,520 --> 01:00:08,240
контексте, и вы узнаете, как

1750
01:00:08,240 --> 01:00:09,760
хеджировать свои ставки и попытаетесь построить

1751
01:00:09,760 --> 01:00:11,359
распределение того, что может

1752
01:00:11,359 --> 01:00:13,119
там появиться, для людей, которые думают,

1753
01:00:13,119 --> 01:00:14,640
что  если есть что-то, о чем вы

1754
01:00:14,640 --> 01:00:15,920
должны подумать, он должен

1755
01:00:15,920 --> 01:00:17,920
знать, какие вещи окажутся в

1756
01:00:17,920 --> 01:00:20,079
этих слотах, у него есть другая неопределенность,

1757
01:00:20,079 --> 01:00:21,520
потому что он не может быть уверен, что любое

1758
01:00:21,520 --> 01:00:24,079
из других слов обязательно

1759
01:00:24,079 --> 01:00:25,119
правильное,

1760
01:00:25,119 --> 01:00:26,319
а затем  это вы знаете, что он хорошо

1761
01:00:26,319 --> 01:00:29,680
предсказывает эти три слова,

1762
01:00:29,680 --> 01:00:31,280
и поэтому

1763
01:00:31,280 --> 01:00:33,119


1764
01:00:33,119 --> 01:00:34,559
вы можете понять, почему важно не

1765
01:00:34,559 --> 01:00:36,559
просто иметь маски потенциально, чтобы иметь

1766
01:00:36,559 --> 01:00:38,160
такие вещи вроде рандомизации токенов,

1767
01:00:38,160 --> 01:00:40,079
потому что мы снова на самом деле не

1768
01:00:40,079 --> 01:00:41,839
заботиться о его способности правильно предсказывать

1769
01:00:41,839 --> 01:00:44,319
маски, как будто я не собираюсь

1770
01:00:44,319 --> 01:00:46,799
обычно я не собираюсь на самом деле отбирать

1771
01:00:46,799 --> 01:00:48,480
из распределения модели то, что

1772
01:00:48,480 --> 01:00:50,480
должно быть здесь,

1773
01:00:50,480 --> 01:00:53,599
вместо этого я собираюсь, чтобы вы знали, используйте

1774
01:00:53,599 --> 01:00:55,280
параметры нейронной сети и

1775
01:00:55,280 --> 01:00:56,799
ожидайте  что он построил сильные

1776
01:00:56,799 --> 01:00:58,799
представления языка, поэтому я не

1777
01:00:58,799 --> 01:01:00,400
хочу, чтобы он думал, что у него есть бесплатный пропуск

1778
01:01:00,400 --> 01:01:02,319
для представления чего-то, если у него

1779
01:01:02,319 --> 01:01:05,200
нет маски,

1780
01:01:05,599 --> 01:01:07,520
ладно, так что, гм,

1781
01:01:07,520 --> 01:01:09,280
была одна дополнительная вещь с

1782
01:01:09,280 --> 01:01:11,760
предварительной тренировкой

1783
01:01:11,760 --> 01:01:12,880
отрыжки,

1784
01:01:12,880 --> 01:01:14,880
которая  - это цель прогнозирования следующего предложения,

1785
01:01:14,880 --> 01:01:16,960
поэтому ввод для bert выглядит так,

1786
01:01:16,960 --> 01:01:18,240
как будто это прямо из бумаги отрыжки,

1787
01:01:18,240 --> 01:01:21,200
которую у вас есть, у вас есть метка здесь

1788
01:01:21,200 --> 01:01:23,599
перед вашим первым предложением, а затем

1789
01:01:23,599 --> 01:01:25,359
разделение, а затем второе предложение, поэтому у

1790
01:01:25,359 --> 01:01:27,839
вас всегда были два смежных фрагмента  текста

1791
01:01:27,839 --> 01:01:28,960


1792
01:01:28,960 --> 01:01:31,599
у вас был первый фрагмент текста здесь

1793
01:01:31,599 --> 01:01:33,920
моя собака симпатичная, а затем вторая

1794
01:01:33,920 --> 01:01:36,240
проверка текста, который ему нравится играть

1795
01:01:36,240 --> 01:01:39,599
правильно, вы можете увидеть подписку там, и

1796
01:01:39,599 --> 01:01:41,280
теперь они на самом деле будут намного

1797
01:01:41,280 --> 01:01:43,680
длиннее, так что все это  Это

1798
01:01:43,680 --> 01:01:46,720
будет 512 слов, это будет примерно половина,

1799
01:01:46,720 --> 01:01:48,079
а это будет примерно половина, и они

1800
01:01:48,079 --> 01:01:51,599
будут непрерывными фрагментами текста,

1801
01:01:51,599 --> 01:01:53,359
но вот сделка, которую они хотели

1802
01:01:53,359 --> 01:01:54,400
сделать,

1803
01:01:54,400 --> 01:01:56,079
это попытаться научить

1804
01:01:56,079 --> 01:01:57,920
систему понимать

1805
01:01:57,920 --> 01:02:00,000
отношения между  различные целые

1806
01:02:00,000 --> 01:02:01,520
фрагменты текста

1807
01:02:01,520 --> 01:02:03,200
, чтобы лучше подготовиться к

1808
01:02:03,200 --> 01:02:05,200
целям для последующих приложений,

1809
01:02:05,200 --> 01:02:07,200
таких как ответы на вопросы, где у вас есть

1810
01:02:07,200 --> 01:02:10,079
два довольно разных фрагмента текста, и

1811
01:02:10,079 --> 01:02:11,599
вам нужно знать, как они соотносятся друг с

1812
01:02:11,599 --> 01:02:13,440
другом, поэтому цель, которую они придумали,

1813
01:02:13,440 --> 01:02:15,680
заключалась в том, что вы должны  предварительно вы должны

1814
01:02:15,680 --> 01:02:18,559
иногда, чтобы второй фрагмент текста

1815
01:02:18,559 --> 01:02:19,760
был

1816
01:02:19,760 --> 01:02:22,079
фактическим фрагментом текста, который непосредственно

1817
01:02:22,079 --> 01:02:26,079
следует за первым в вашем наборе данных,

1818
01:02:26,079 --> 01:02:27,760
а иногда вторая проверка

1819
01:02:27,760 --> 01:02:29,760
текста

1820
01:02:29,760 --> 01:02:31,520
случайным образом выбирается из другого,

1821
01:02:31,520 --> 01:02:32,720
не связанного с этим места,

1822
01:02:32,720 --> 01:02:35,039
и модель должна предсказать,

1823
01:02:35,039 --> 01:02:36,400
будет ли он первым  случай или

1824
01:02:36,400 --> 01:02:38,960
второй, чтобы снова как-то

1825
01:02:38,960 --> 01:02:40,400
рассуждать о взаимосвязях

1826
01:02:40,400 --> 01:02:42,240
между двумя фрагментами текста,

1827
01:02:42,240 --> 01:02:44,640
так что это предсказание следующего предложения, я

1828
01:02:44,640 --> 01:02:45,760
думаю, что я  Об этом важно подумать,

1829
01:02:45,760 --> 01:02:47,920
потому что опять же, это совсем другая идея

1830
01:02:47,920 --> 01:02:50,400
о цели предварительной подготовки, чем языковое

1831
01:02:50,400 --> 01:02:53,520
моделирование и моделирование замаскированного языка,

1832
01:02:53,520 --> 01:02:55,520
хотя в более поздних работах вроде бы утверждалось,

1833
01:02:55,520 --> 01:02:57,359
что в случае с Bert это не

1834
01:02:57,359 --> 01:03:00,160
нужно или полезно, и один из

1835
01:03:00,160 --> 01:03:02,559
аргументов на самом деле  потому

1836
01:03:02,559 --> 01:03:05,280
что на самом деле лучше

1837
01:03:05,280 --> 01:03:07,359
иметь один контекст, который вдвое

1838
01:03:07,359 --> 01:03:09,680
длиннее, чтобы вы могли изучать

1839
01:03:09,680 --> 01:03:12,160
зависимости и вещи на еще более дальнем расстоянии, и поэтому

1840
01:03:12,160 --> 01:03:13,760
была бы полезна сама цель,

1841
01:03:13,760 --> 01:03:15,680
если бы вы всегда могли просто удвоить

1842
01:03:15,680 --> 01:03:17,119
размер контекста, я не уверен, что

1843
01:03:17,119 --> 01:03:18,960
кто-то проводил исследования по этому поводу, но, опять же,

1844
01:03:18,960 --> 01:03:20,720
это похоже на другую цель,

1845
01:03:20,720 --> 01:03:22,640
и он все еще что-то шумит

1846
01:03:22,640 --> 01:03:24,880
о вводе. Правильно, ввод был

1847
01:03:24,880 --> 01:03:27,119
этим большим фрагментом текста, и вы

1848
01:03:27,119 --> 01:03:28,720
произнесли его, чтобы сказать, что теперь вы не знаете

1849
01:03:28,720 --> 01:03:30,160
, действительно ли это было  это или

1850
01:03:30,160 --> 01:03:31,680
вы как бы заменили его кучей

1851
01:03:31,680 --> 01:03:32,720
мусора

1852
01:03:32,720 --> 01:03:35,119
вот такая вторая часть здесь была

1853
01:03:35,119 --> 01:03:36,559
ли вторая часть

1854
01:03:36,559 --> 01:03:39,680
заменена чем-то, да, была

1855
01:03:39,680 --> 01:03:41,119
заменена  с чем-то, что на

1856
01:03:41,119 --> 01:03:45,000
самом деле происходило не из той же последовательности,

1857
01:03:46,240 --> 01:03:47,920
хорошо, поэтому давайте поговорим о некоторых деталях о

1858
01:03:47,920 --> 01:03:48,720
bert

1859
01:03:48,720 --> 01:03:51,839
um, поэтому у берта было 12 или 24 слоя в зависимости

1860
01:03:51,839 --> 01:03:53,440
от основания или большого размера, вы,

1861
01:03:53,440 --> 01:03:55,039
вероятно, будете использовать одну из этих моделей или

1862
01:03:55,039 --> 01:03:56,319
одну из  потомки этих

1863
01:03:56,319 --> 01:03:58,640
моделей, если вы просто решите сделать

1864
01:03:58,640 --> 01:04:00,160
что-то с настраиваемым окончательным проектом

1865
01:04:00,160 --> 01:04:03,599
потенциально или если вы выберете

1866
01:04:03,599 --> 01:04:06,079
версию проекта файла по умолчанию,

1867
01:04:06,079 --> 01:04:08,880
и вы знаете, что скрытое измерение 600 или тысячи

1868
01:04:08,880 --> 01:04:10,640
измерений вызывает

1869
01:04:10,640 --> 01:04:11,599
кучу внимания, так что это  это

1870
01:04:11,599 --> 01:04:13,599
многогранное внимание, помните, но

1871
01:04:13,599 --> 01:04:16,079
их куча, так что вы разбиваете

1872
01:04:16,079 --> 01:04:18,240
все свои измерения на эти 16 голов,

1873
01:04:18,240 --> 01:04:20,559
и мы говорим о порядке

1874
01:04:20,559 --> 01:04:22,960
пары сотен миллионов параметров

1875
01:04:22,960 --> 01:04:25,440
в то время, прямо в 2018 году, мы были как

1876
01:04:25,440 --> 01:04:28,319
эй, это  много параметров,

1877
01:04:28,319 --> 01:04:29,760
как вы, это много

1878
01:04:29,760 --> 01:04:33,839
параметров, а теперь модели

1879
01:04:33,839 --> 01:04:36,079
намного больше, поэтому давайте отслеживать

1880
01:04:36,079 --> 01:04:37,839
размеры моделей, когда мы проходим

1881
01:04:37,839 --> 01:04:40,400
через это, и давайте теперь вернемся

1882
01:04:40,400 --> 01:04:42,160
к  размеры корпуса, так что

1883
01:04:42,160 --> 01:04:44,480
у нас есть корпус книг, это количество

1884
01:04:44,480 --> 01:04:45,680
слов, это то же самое, что

1885
01:04:45,680 --> 01:04:48,799
gpt uh gpt one был обучен на 800

1886
01:04:48,799 --> 01:04:50,799
миллионах слов, теперь мы собираемся тренироваться

1887
01:04:50,799 --> 01:04:51,599


1888
01:04:51,599 --> 01:04:53,599
также на английской википедии,

1889
01:04:53,599 --> 01:04:56,720
это 250, извините, это 2500 миллионов, так что

1890
01:04:56,720 --> 01:04:59,520
это 2,5 миллиарда слов,

1891
01:04:59,520 --> 01:05:01,520
и еще

1892
01:05:01,520 --> 01:05:03,280
раз, чтобы дать вам представление о том, что

1893
01:05:03,280 --> 01:05:05,680
делается на практике, правильное предварительное обучение

1894
01:05:05,680 --> 01:05:09,039
дорого и непрактично для большинства

1895
01:05:09,039 --> 01:05:10,880
пользователей, скажем

1896
01:05:10,880 --> 01:05:11,760
так,

1897
01:05:11,760 --> 01:05:13,839
если вы исследователь с

1898
01:05:13,839 --> 01:05:16,240
графическим процессором или пятью графическими процессорами или что-то в этом роде,

1899
01:05:16,240 --> 01:05:17,839
вы склонны  чтобы на самом деле не проводить предварительное обучение

1900
01:05:17,839 --> 01:05:19,920
всей вашей собственной модели Bert, если вы не

1901
01:05:19,920 --> 01:05:22,160
готовы тратить на это много времени,

1902
01:05:22,160 --> 01:05:24,799
сам Burt был предварительно обучен с 64

1903
01:05:24,799 --> 01:05:27,039
чипами TPU. TPU - это особый вид

1904
01:05:27,039 --> 01:05:29,599
аппаратного ускорителя, который эффективно ускоряет

1905
01:05:29,599 --> 01:05:30,400


1906
01:05:30,400 --> 01:05:32,079
тензорные операции.

1907
01:05:32,079 --> 01:05:34,799
разработан Google,

1908
01:05:34,799 --> 01:05:36,960
так что tpus просто

1909
01:05:36,960 --> 01:05:37,920
быстрые

1910
01:05:37,920 --> 01:05:39,680
и могут вместить много,

1911
01:05:39,680 --> 01:05:41,760
и в течение четырех дней у них было 64 чипа,

1912
01:05:41,760 --> 01:05:43,520
поэтому, если у вас есть один gpu, который вы можете

1913
01:05:43,520 --> 01:05:45,839
думать как меньше, чем один tpu,

1914
01:05:45,839 --> 01:05:47,359
вы будете ждать

1915
01:05:47,359 --> 01:05:49,280
долго  к pre-t  дождь, но

1916
01:05:49,280 --> 01:05:51,920
настройка на самом деле точная настройка настолько быстра, что

1917
01:05:51,920 --> 01:05:54,079
она настолько быстра и практична, что это обычное дело

1918
01:05:54,079 --> 01:05:56,319
для одного графического процессора, вы увидите, насколько более

1919
01:05:56,319 --> 01:05:58,400
быстрая точная настройка, чем предварительная тренировка

1920
01:05:58,400 --> 01:06:02,480
в задании пять мкм, и поэтому

1921
01:06:02,480 --> 01:06:04,000
я думаю, что это становится рефреном

1922
01:06:04,000 --> 01:06:06,000
области  вы проходили предварительное обучение один или

1923
01:06:06,000 --> 01:06:07,520
несколько раз, точно так же,

1924
01:06:07,520 --> 01:06:08,960
как пара людей выпустила большие

1925
01:06:08,960 --> 01:06:10,480
предварительно обученные модели, а затем вы

1926
01:06:10,480 --> 01:06:12,480
много раз настраиваете правильно, так что вы

1927
01:06:12,480 --> 01:06:14,400
сохраняете эти параметры от предварительного обучения

1928
01:06:14,400 --> 01:06:16,240
и настраиваете все виды

1929
01:06:16,240 --> 01:06:19,559
различные проблемы,

1930
01:06:20,480 --> 01:06:22,160
и что эта парадигма верна, беря

1931
01:06:22,160 --> 01:06:24,160
что-то вроде bert или что-то еще, что является лучшим

1932
01:06:24,160 --> 01:06:27,520
потомком bert, и беря его

1933
01:06:27,520 --> 01:06:29,119
предварительно обученным, а затем настраивая его на

1934
01:06:29,119 --> 01:06:31,039
то, что вы хотите,

1935
01:06:31,039 --> 01:06:32,400
довольно близко к тому, что

1936
01:06:32,400 --> 01:06:35,119
вы знаете, это очень, очень сильная

1937
01:06:35,119 --> 01:06:38,480
основа в  nlp прямо сейчас,

1938
01:06:38,480 --> 01:06:40,640
и простота довольно увлекательна,

1939
01:06:40,640 --> 01:06:43,200
и есть одна база кода,

1940
01:06:43,200 --> 01:06:45,119
называемая трансформаторами, от компании

1941
01:06:45,119 --> 01:06:47,599
под названием hugging face, которая делает это

1942
01:06:47,599 --> 01:06:50,079
действительно просто парой строк

1943
01:06:50,079 --> 01:06:52,000
python, чтобы попробовать, так что это  как

1944
01:06:52,000 --> 01:06:53,280
бы открываются

1945
01:06:53,280 --> 01:06:56,000
очень сильные басовые партии без

1946
01:06:56,000 --> 01:06:59,039
особых усилий для множества задач,

1947
01:06:59,039 --> 01:07:01,359
ладно, так что давайте поговорим об оценке,

1948
01:07:01,359 --> 01:07:03,920
поэтому предварительная подготовка требует

1949
01:07:03,920 --> 01:07:05,119
всего этого разного понимания языка,

1950
01:07:05,119 --> 01:07:08,559
а область - это

1951
01:07:08,559 --> 01:07:11,520
область нлп.  трудно проводить оценку,

1952
01:07:11,520 --> 01:07:13,200
но мы стараемся изо всех сил и создаем наборы данных,

1953
01:07:13,200 --> 01:07:14,880
которые, по нашему мнению, являются сложными по разным

1954
01:07:14,880 --> 01:07:16,400
причинам, потому что они требуют от вас

1955
01:07:16,400 --> 01:07:18,079
знаний о языке, о мире

1956
01:07:18,079 --> 01:07:20,480
и о рассуждениях, и поэтому, когда мы

1957
01:07:20,480 --> 01:07:22,559
оцениваем, помогает ли вам предварительная

1958
01:07:22,559 --> 01:07:24,720
подготовка  много своего рода общих знаний

1959
01:07:24,720 --> 01:07:26,319
эм

1960
01:07:26,319 --> 01:07:29,520
мы о мальчик эм мы оцениваем по многим из

1961
01:07:29,520 --> 01:07:32,559
этих задач, поэтому мы оцениваем такие вещи,

1962
01:07:32,559 --> 01:07:35,680
как обнаружение перефразирования в вопросах кворуса,

1963
01:07:35,680 --> 01:07:36,880


1964
01:07:36,880 --> 01:07:38,960
логический вывод естественного языка, который мы видели,

1965
01:07:38,960 --> 01:07:40,799
у нас есть наборы данных для жесткого анализа настроений

1966
01:07:40,799 --> 01:07:42,319
или того, что было жестким

1967
01:07:42,319 --> 01:07:45,200
анализом настроений  наборы данных пару лет назад эм на

1968
01:07:45,200 --> 01:07:47,200
самом деле выяснить, являются ли предложения

1969
01:07:47,200 --> 01:07:49,440
грамматическими, бывает сложно

1970
01:07:49,440 --> 01:07:51,359
определить текстовое сходство

1971
01:07:51,359 --> 01:07:53,599
семантическое сходство текста может быть h

1972
01:07:53,599 --> 01:07:55,680
Снова перефразируя

1973
01:07:55,680 --> 01:07:57,599
логический вывод естественного языка на очень-очень маленьком наборе данных,

1974
01:07:57,599 --> 01:07:59,520
так что это действительно предварительное обучение, помогающее

1975
01:07:59,520 --> 01:08:01,119
вам тренироваться на небольших наборах данных,

1976
01:08:01,119 --> 01:08:04,960
ответ - да, вроде того, и так гм,

1977
01:08:04,960 --> 01:08:06,799
так что ребята из bert выпустили свою

1978
01:08:06,799 --> 01:08:09,760
статью после того, как gpt был выпущен  и

1979
01:08:09,760 --> 01:08:11,280
было много современных

1980
01:08:11,280 --> 01:08:13,359
результатов, которые были получены из различных вещей,

1981
01:08:13,359 --> 01:08:15,680
которые вы должны были делать,

1982
01:08:15,680 --> 01:08:17,359
и

1983
01:08:17,359 --> 01:08:19,600
вы знаете результаты, которые вы получаете

1984
01:08:19,600 --> 01:08:21,120
от предварительной подготовки, так что вот

1985
01:08:21,120 --> 01:08:23,359
openai gpt вот  bert base и

1986
01:08:23,359 --> 01:08:25,600
large последние три ряда все предварительно обучены,

1987
01:08:25,600 --> 01:08:28,799
elmo вроде как посередине, а это как

1988
01:08:28,799 --> 01:08:29,759
бы посередине между

1989
01:08:29,759 --> 01:08:31,198
предварительным обучением всей модели и

1990
01:08:31,198 --> 01:08:32,799
просто встраиванием слов, вот что это

1991
01:08:32,799 --> 01:08:34,000
такое,

1992
01:08:34,000 --> 01:08:36,080
и числами, которые вы получаете  просто

1993
01:08:36,080 --> 01:08:37,600
я думаю, что эта область была довольно

1994
01:08:37,600 --> 01:08:39,600
поразительной, на самом деле, мы все были

1995
01:08:39,600 --> 01:08:40,960
удивлены, что осталось так много всего, что нужно

1996
01:08:40,960 --> 01:08:43,198
было даже получить по некоторым из этих наборов данных,

1997
01:08:43,198 --> 01:08:46,158
и вы знаете, беря здесь, так что

1998
01:08:46,158 --> 01:08:48,479
эта строка в таблице не отмечена, но

1999
01:08:48,479 --> 01:08:49,359
на самом деле это  количество обучающих

2000
01:08:49,359 --> 01:08:52,238
примеров, этот набор данных содержит 2,5 000

2001
01:08:52,238 --> 01:08:55,279
обучающих примеров, и до того, как

2002
01:08:55,279 --> 01:08:57,839
появились большие трансформаторы, у нас была

2003
01:08:57,839 --> 01:09:00,719
точность 60, мы запускаем на нем трансформаторы,

2004
01:09:00,719 --> 01:09:03,359
мы получаем 10 баллов только за предварительное обучение,

2005
01:09:03,359 --> 01:09:05,439
и это была тенденция, которая  только что

2006
01:09:05,439 --> 01:09:07,040
продолжил,

2007
01:09:07,040 --> 01:09:09,679
так зачем делать что-либо, кроме кодировщиков с предварительным обучением,

2008
01:09:09,679 --> 01:09:12,158
так как мы знаем, что кодировщики

2009
01:09:12,158 --> 01:09:14,158
хороши, и нам нам нравится тот факт, что у вас

2010
01:09:14,158 --> 01:09:15,839
есть двунаправленный контекст, мы также видели,

2011
01:09:15,839 --> 01:09:18,080
что bert справился лучше, чем gpt

2012
01:09:18,080 --> 01:09:20,640
um, но вы знаете, что это так, если хотите

2013
01:09:20,640 --> 01:09:21,839
на самом деле

2014
01:09:21,839 --> 01:09:24,399
заставить его делать все правильно,

2015
01:09:24,399 --> 01:09:26,238
вы не можете просто сгенерировать его,

2016
01:09:26,238 --> 01:09:27,759
генерировать последовательности из него так же, как

2017
01:09:27,759 --> 01:09:30,399
вы бы из модели, такой как gpt,

2018
01:09:30,399 --> 01:09:32,719
предварительно обученный декодер, вы можете

2019
01:09:32,719 --> 01:09:34,080
отсортировать образец того, что должно быть в

2020
01:09:34,080 --> 01:09:35,920
маске, так что вот  mask

2021
01:09:35,920 --> 01:09:37,759
вы можете поместить маску где-нибудь, образец

2022
01:09:37,759 --> 01:09:39,679
слов, которые должны быть там, но если вы

2023
01:09:39,679 --> 01:09:41,198
хотите выбрать весь контекст правильно, если

2024
01:09:41,198 --> 01:09:42,238
вы хотите получить эту историю о

2025
01:09:42,238 --> 01:09:44,960
единорогах, например, кодировщик - это

2026
01:09:44,960 --> 01:09:46,799
не то, что вы хотите сделать, поэтому у них

2027
01:09:46,799 --> 01:09:49,679
есть  Типа различных контрактов, которые

2028
01:09:49,679 --> 01:09:50,799
могут быть использованы

2029
01:09:50,799 --> 01:09:54,319
естественным образом, по крайней мере, по-разному,

2030
01:09:54,960 --> 01:09:56,560
хорошо, так что давайте очень кратко поговорим о

2031
01:09:56,560 --> 01:09:58,239
расширениях bert, так что есть сожженные

2032
01:09:58,239 --> 01:10:00,800
варианты, такие как roberta и spanbert, и

2033
01:10:00,800 --> 01:10:02,320
есть просто куча документов со

2034
01:10:02,320 --> 01:10:03,920
словом burt в названии, которые сделали  разные

2035
01:10:03,920 --> 01:10:07,600
вещи два очень важных вывода roberta

2036
01:10:07,600 --> 01:10:09,760
train bert long bert is under fit

2037
01:10:09,760 --> 01:10:11,840
обучайте его на большем количестве данных обучайте его для большего количества

2038
01:10:11,840 --> 01:10:14,480
шагов spanbert

2039
01:10:14,480 --> 01:10:18,480
uh mask смежные промежутки дополнительных слов

2040
01:10:18,480 --> 01:10:20,400
uh слов усложняют более полезную

2041
01:10:20,400 --> 01:10:22,080
предварительную тренировку, так что идея заключается в том,

2042
01:10:22,080 --> 01:10:23,760
что  мы можем придумать лучшие способы

2043
01:10:23,760 --> 01:10:25,920
зашумить ввод, скрыть материал

2044
01:10:25,920 --> 01:10:27,840
на входе или сломать материал на входе

2045
01:10:27,840 --> 01:10:31,040
для нашей модели, чтобы исправить um, чтобы вы знали,

2046
01:10:31,040 --> 01:10:33,040
например, если у вас есть маска с предложением

2047
01:10:33,040 --> 01:10:35,040
маска сопротивления ушей

2048
01:10:35,040 --> 01:10:38,560
хорошо, это просто не так сложно

2049
01:10:38,560 --> 01:10:41,440
знать, что это неотразимо

2050
01:10:41,440 --> 01:10:42,800
правильно, потому что, например, что это

2051
01:10:42,800 --> 01:10:44,560
могло быть после этих дополнительных слов, так

2052
01:10:44,560 --> 01:10:46,719
что это сопротивление уху,

2053
01:10:46,719 --> 01:10:48,640
вы знаете, что что-

2054
01:10:48,640 --> 01:10:49,840
то должно произойти здесь, и это,

2055
01:10:49,840 --> 01:10:52,640
вероятно, конец этого  слово, тогда как если

2056
01:10:52,640 --> 01:10:55,280
вы сейчас замаскируете длинную последовательность вещей,

2057
01:10:55,280 --> 01:10:57,280
это намного сложнее, и на самом деле

2058
01:10:57,280 --> 01:10:59,199
вы получаете полезный сигнал, который

2059
01:10:59,199 --> 01:11:01,280
непреодолимо хорош, и вам как бы

2060
01:11:01,280 --> 01:11:03,600
нужно замаскировать все из них, чтобы сделать задачу

2061
01:11:03,600 --> 01:11:05,679
интересной, поэтому

2062
01:11:05,679 --> 01:11:07,679
Спанберт был похож на вас  должен сделать это,

2063
01:11:07,679 --> 01:11:09,920
так что это было очень полезно,

2064
01:11:09,920 --> 01:11:11,679
так что Роберта просто чтобы указать вам на тот

2065
01:11:11,679 --> 01:11:13,600
факт, что Роберта показала, что Берт был не

2066
01:11:13,600 --> 01:11:17,040
в форме, вы знаете, он сказал, что Берт был

2067
01:11:17,040 --> 01:11:19,679
обучен примерно на 13 гигабайтах текста, он

2068
01:11:19,679 --> 01:11:22,400
получил некоторую точность, которую вы можете получить

2069
01:11:22,400 --> 01:11:24,640
выше  Удивительные результаты: наберите четыре

2070
01:11:24,640 --> 01:11:26,000
дополнительных очка

2071
01:11:26,000 --> 01:11:28,719
или около того здесь, просто взяв

2072
01:11:28,719 --> 01:11:31,199
идентичную модель и обучив ее на большем количестве

2073
01:11:31,199 --> 01:11:34,880
данных с большим размером пакета в

2074
01:11:34,880 --> 01:11:36,320
течение длительного времени,

2075
01:11:36,320 --> 01:11:38,800
и если вы тренируете ее еще дольше,

2076
01:11:38,800 --> 01:11:40,400
без каких-либо дополнительных данных, которые

2077
01:11:40,400 --> 01:11:43,520
вы не делаете

2078
01:11:44,960 --> 01:11:46,640
очень кратко хорошо,

2079
01:11:46,640 --> 01:11:49,520
очень кратко о декодерах кодировщика, чтобы

2080
01:11:49,520 --> 01:11:50,719
мы увидели, что

2081
01:11:50,719 --> 01:11:52,560
декодеры могут быть хорошими, потому что мы можем

2082
01:11:52,560 --> 01:11:53,760
играть с контрактами, которые они нам дают,

2083
01:11:53,760 --> 01:11:55,040
мы можем играть с ними, как

2084
01:11:55,040 --> 01:11:56,640
кодеры языковых моделей дают нам  этот

2085
01:11:56,640 --> 01:11:59,280
двунаправленный контекст, так что кодировщики-

2086
01:11:59,280 --> 01:12:01,199
декодеры, может быть, мы получим и то, и

2087
01:12:01,199 --> 01:12:02,960
другое на практике, они на самом деле да,

2088
01:12:02,960 --> 01:12:06,159
довольно сильны, так что у

2089
01:12:06,159 --> 01:12:08,400


2090
01:12:08,400 --> 01:12:10,080
нас было право, но я думаю, что один из

2091
01:12:10,080 --> 01:12:11,600
вопросов - что мы делаем, чтобы

2092
01:12:11,600 --> 01:12:12,880
предварительно  обучить их,

2093
01:12:12,880 --> 01:12:14,640
чтобы мы могли делать что-то вроде языкового

2094
01:12:14,640 --> 01:12:16,719
моделирования прямо там, где мы берем

2095
01:12:16,719 --> 01:12:22,080
последовательность слов слово один uh на слово uh

2096
01:12:22,080 --> 01:12:24,880
two t вместо t

2097
01:12:24,880 --> 01:12:26,960
right, и так как у меня есть слово один здесь точка

2098
01:12:26,960 --> 01:12:29,280
точка слово t мы предоставляем все это нашему

2099
01:12:29,280 --> 01:12:31,920
кодировщику и  мы не предсказываем ни одного из них,

2100
01:12:31,920 --> 01:12:33,760
и тогда у нас есть слово t плюс одно к слову

2101
01:12:33,760 --> 01:12:36,480
два t здесь, в нашем декодере

2102
01:12:36,480 --> 01:12:38,400
справа, и мы прогнозируем по ним, поэтому мы

2103
01:12:38,400 --> 01:12:39,920
делаем языковое моделирование на половине

2104
01:12:39,920 --> 01:12:42,320
последовательности, а другую половину мы взяли,

2105
01:12:42,320 --> 01:12:45,040
чтобы  двунаправленный кодировщик

2106
01:12:45,040 --> 01:12:46,000
правильно, поэтому мы строим сильные

2107
01:12:46,000 --> 01:12:49,360
представления на стороне кодировщика,

2108
01:12:49,360 --> 01:12:51,199
не прогнозируя языковое моделирование ни на что

2109
01:12:51,199 --> 01:12:53,199
из этого, а затем мы на другой

2110
01:12:53,199 --> 01:12:54,880
половине токенов, которые, как мы предполагаем, вы знаете, как

2111
01:12:54,880 --> 01:12:57,120
языковая модель,

2112
01:12:57,120 --> 01:12:58,480
и надеемся, что  ты вроде как

2113
01:12:58,480 --> 01:13:00,480
подготовительный бот  h из них хорошо из-за

2114
01:13:00,480 --> 01:13:04,000
потери моделирования на одном языке здесь,

2115
01:13:04,000 --> 01:13:05,520
и это на самом деле так, что это работает

2116
01:13:05,520 --> 01:13:07,600
довольно хорошо, кодер извлекает выгоду из

2117
01:13:07,600 --> 01:13:09,520
двунаправленности декодера, который вы можете

2118
01:13:09,520 --> 01:13:12,159
использовать для обучения модели, но что гм, эта

2119
01:13:12,159 --> 01:13:15,040
статья

2120
01:13:15,120 --> 01:13:17,040
показала, что представила модель t5

2121
01:13:17,040 --> 01:13:19,440
ruffle  Все это, как оказалось, работает лучше всего, на

2122
01:13:19,440 --> 01:13:21,280
самом деле была очень или, по крайней мере, несколько

2123
01:13:21,280 --> 01:13:23,360
другой целью, и это должно иметь

2124
01:13:23,360 --> 01:13:24,880
в виду, что у нас есть

2125
01:13:24,880 --> 01:13:26,800
разные способы определения

2126
01:13:26,800 --> 01:13:28,719
целей предварительной подготовки, и они

2127
01:13:28,719 --> 01:13:30,159
действительно будут работать по-разному,

2128
01:13:30,159 --> 01:13:31,760
поэтому  то, что они сказали, допустим, у вас есть

2129
01:13:31,760 --> 01:13:34,000
оригинальный текст, подобный этому, спасибо, что

2130
01:13:34,000 --> 01:13:37,600
пригласили меня на вечеринку на прошлой неделе,

2131
01:13:37,600 --> 01:13:39,760
мы собираемся определить промежутки переменной длины

2132
01:13:39,760 --> 01:13:42,960
в тексте, чтобы заменить их

2133
01:13:42,960 --> 01:13:44,719
уникальным символом, который говорит, что

2134
01:13:44,719 --> 01:13:47,280
здесь чего-то не хватает, а затем мы '  Я заменим, а

2135
01:13:47,280 --> 01:13:49,360
затем мы удалим это, так что теперь наш

2136
01:13:49,360 --> 01:13:52,560
вход в наш кодировщик - это символ благодарности

2137
01:13:52,560 --> 01:13:53,679


2138
01:13:53,679 --> 01:13:54,560
один

2139
01:13:54,560 --> 01:13:57,280
мне к символу вашей партии слабый

2140
01:13:57,280 --> 01:13:59,199
справа, поэтому мы зашумили вход, который мы

2141
01:13:59,199 --> 01:14:00,800
скрыли во входных данных

2142
01:14:00,800 --> 01:14:02,400
также действительно интересно, правда, это

2143
01:14:02,400 --> 01:14:04,000
не говорит, как долго это должно

2144
01:14:04,000 --> 01:14:05,120
быть

2145
01:14:05,120 --> 01:14:07,600
, это отличается от Берта справа Берт

2146
01:14:07,600 --> 01:14:10,480
сказал, что вы замаскировали столько

2147
01:14:10,480 --> 01:14:12,239


2148
01:14:12,239 --> 01:14:13,920


2149
01:14:13,920 --> 01:14:15,199
подслов  Я даже не знаю,

2150
01:14:15,199 --> 01:14:18,159
сколько это подслов, а затем у

2151
01:14:18,159 --> 01:14:19,360
вас есть это в вашем кодировщике, а затем

2152
01:14:19,360 --> 01:14:23,520
ваш декодер предсказывает первое

2153
01:14:24,239 --> 01:14:27,520
специальное слово это x здесь,

2154
01:14:27,520 --> 01:14:29,280
а затем то, что не хватало

2155
01:14:29,280 --> 01:14:32,000
для приглашения, так что спасибо xx за

2156
01:14:32,000 --> 01:14:34,560
приглашение, а затем  он предсказывает y, вот

2157
01:14:34,560 --> 01:14:36,480
этот y здесь, а затем то, что отсутствовало

2158
01:14:36,480 --> 01:14:40,159
в y на прошлой неделе,

2159
01:14:40,159 --> 01:14:42,480
это называется повреждением диапазона, и это

2160
01:14:42,480 --> 01:14:44,080
действительно интересно для меня, потому

2161
01:14:44,080 --> 01:14:46,000
что с точки зрения фактического

2162
01:14:46,000 --> 01:14:48,080
декодера кодировщика нам не нужно его менять

2163
01:14:48,080 --> 01:14:49,440
по сравнению с тем,  мы, если бы мы просто

2164
01:14:49,440 --> 01:14:51,199
выполняли предварительную подготовку по языковому моделированию,

2165
01:14:51,199 --> 01:14:53,360
потому что я просто занимаюсь языковым моделированием

2166
01:14:53,360 --> 01:14:55,520
всех этих вещей, я просто предсказываю эти

2167
01:14:55,520 --> 01:14:57,600
слова, как будто я языковая модель, я

2168
01:14:57,600 --> 01:14:59,920
только что выполнил этап предварительной обработки текста

2169
01:14:59,920 --> 01:15:00,880


2170
01:15:00,880 --> 01:15:02,480
правильно,

2171
01:15:02,480 --> 01:15:04,640
поэтому фактический  Я только что предварительно обработал

2172
01:15:04,640 --> 01:15:06,719
текст, чтобы он выглядел так,

2173
01:15:06,719 --> 01:15:08,480
чтобы он выглядел так, чтобы он выглядел так, а затем сделайте

2174
01:15:08,480 --> 01:15:10,640
вывод, который выглядит так, наверху,

2175
01:15:10,640 --> 01:15:11,679
и

2176
01:15:11,679 --> 01:15:13,440
модель получает, что вы знаете, делайте то, что

2177
01:15:13,440 --> 01:15:14,640
эффективно языковое моделирование, но на

2178
01:15:14,640 --> 01:15:16,320
самом деле это работает  лучше, так что есть

2179
01:15:16,320 --> 01:15:19,040
много чисел, я понимаю, но взгляните

2180
01:15:19,040 --> 01:15:21,440
на звезду здесь, этот кодировщик-декодер

2181
01:15:21,440 --> 01:15:23,679
с целью шумоподавления, который имеет тенденцию

2182
01:15:23,679 --> 01:15:26,159
работать лучше всего, и они пробовали аналогичные

2183
01:15:26,159 --> 01:15:28,880
модели, такие как модель языка

2184
01:15:28,880 --> 01:15:30,960
префиксов, мм, это была своего рода первая

2185
01:15:30,960 --> 01:15:31,760


2186
01:15:31,760 --> 01:15:33,600
попытка  что мы определили цель предварительного обучения

2187
01:15:33,600 --> 01:15:35,199
для языковых моделей, извините за

2188
01:15:35,199 --> 01:15:36,960
кодировщики-декодеры,

2189
01:15:36,960 --> 01:15:38,400


2190
01:15:38,400 --> 01:15:40,080
а затем у них был еще ряд

2191
01:15:40,080 --> 01:15:41,440
других вариантов, но что лучше всего сработало

2192
01:15:41,440 --> 01:15:43,199
для кодировщиков-декодеров,

2193
01:15:43,199 --> 01:15:45,040
и одна из увлекательных вещей в

2194
01:15:45,040 --> 01:15:47,520
t5 - это то, что вы можете предварительно обучить  и

2195
01:15:47,520 --> 01:15:50,159
настроить его на такие вопросы, как, когда

2196
01:15:50,159 --> 01:15:52,960
родился Франклин Друзвельт, и

2197
01:15:52,960 --> 01:15:55,600
настроить его, чтобы дать ответ,

2198
01:15:55,600 --> 01:15:57,440
и затем вы могли бы задать ему новые вопросы

2199
01:15:57,440 --> 01:15:59,760
во время теста, а затем он будет

2200
01:15:59,760 --> 01:16:01,440
получать ответ из своего  параметры с некоторой

2201
01:16:01,440 --> 01:16:04,320
точностью, и он будет делать это относительно

2202
01:16:04,320 --> 01:16:06,960
хорошо на самом деле, и он будет делать это, может быть, в

2203
01:16:06,960 --> 01:16:08,800
25 случаях на некоторых из этих

2204
01:16:08,800 --> 01:16:11,520
наборов данных с 220 миллионами параметров, а затем при

2205
01:16:11,520 --> 01:16:13,760
11 миллиардах параметров, это намного

2206
01:16:13,760 --> 01:16:15,920
больше, чем bert large, он будет делать это

2207
01:16:15,920 --> 01:16:18,640
даже  иногда лучше даже делать так же хорошо,

2208
01:16:18,640 --> 01:16:21,120
как и системы, которым было разрешено смотреть на

2209
01:16:21,120 --> 01:16:22,560
вещи, отличные от их собственных параметров, так что

2210
01:16:22,560 --> 01:16:24,880
снова это просто заставляет этот ответ

2211
01:16:24,880 --> 01:16:27,440
исходить из его параметров,

2212
01:16:27,440 --> 01:16:28,320


2213
01:16:28,320 --> 01:16:31,199
да, мне придется пропустить это, так что

2214
01:16:31,199 --> 01:16:33,280
если вы посмотрите на это  слайд после

2215
01:16:33,280 --> 01:16:35,520
урока у меня есть каждый из примеров того,

2216
01:16:35,520 --> 01:16:36,800
чему мы могли бы научиться

2217
01:16:36,800 --> 01:16:39,040
на предварительном обучении, с пометкой того, что

2218
01:16:39,040 --> 01:16:40,560
вы, возможно, изучаете, так что этот пример:

2219
01:16:40,560 --> 01:16:42,719
Стэнфордский университет расположен пустым,

2220
01:16:42,719 --> 01:16:44,640
вы можете узнать мелочи во всех этих

2221
01:16:44,640 --> 01:16:46,080
случаях есть все  эти вещи вы можете

2222
01:16:46,080 --> 01:16:48,159
узнать одно, я скажу, что

2223
01:16:48,159 --> 01:16:50,080
модели также учатся

2224
01:16:50,080 --> 01:16:52,000
и могут усугубить

2225
01:16:52,000 --> 01:16:54,560
расизм сексизм всевозможные плохие предубеждения

2226
01:16:54,560 --> 01:16:56,960
, которые закодированы в нашем тексте, когда я говорю, что

2227
01:16:56,960 --> 01:16:58,880
вы знаете, да,

2228
01:16:58,880 --> 01:17:00,800
они  сделайте это, и поэтому мы узнаем

2229
01:17:00,800 --> 01:17:02,080
об этом больше в наших последующих лекциях, но

2230
01:17:02,080 --> 01:17:04,080
важно помнить, что когда

2231
01:17:04,080 --> 01:17:05,040
вы делаете предварительную тренировку, вы

2232
01:17:05,040 --> 01:17:06,800
изучаете много вещей, и не все из

2233
01:17:06,800 --> 01:17:09,280
них хороши,

2234
01:17:09,600 --> 01:17:12,480
поэтому с gpt three  Последнее, что здесь

2235
01:17:12,480 --> 01:17:14,400


2236
01:17:14,400 --> 01:17:15,760
есть, - это третий способ взаимодействия

2237
01:17:15,760 --> 01:17:17,920
с моделями, связанный с обращением с

2238
01:17:17,920 --> 01:17:20,880
ними как с языковыми моделями, так что

2239
01:17:20,880 --> 01:17:23,360
gpt-3 - это очень большая модель

2240
01:17:23,360 --> 01:17:25,040
, выпущенная с помощью открытого ИИ,

2241
01:17:25,040 --> 01:17:27,360
и, похоже, она может учиться

2242
01:17:27,360 --> 01:17:29,679
на примерах.  в их контексте их

2243
01:17:29,679 --> 01:17:32,719
контексты декодера без шагов градиента,

2244
01:17:32,719 --> 01:17:35,280
просто просматривая их

2245
01:17:35,280 --> 01:17:36,400
историю,

2246
01:17:36,400 --> 01:17:39,920
и теперь gpt3 имеет 175 миллиардов параметров,

2247
01:17:39,920 --> 01:17:42,000
а последняя модель t5, которую мы видели, была 11

2248
01:17:42,000 --> 01:17:43,520
миллиардов параметров

2249
01:17:43,520 --> 01:17:46,159
, и, похоже, это своего

2250
01:17:46,159 --> 01:17:48,159
рода канонический пример этого  работает,

2251
01:17:48,159 --> 01:17:50,320
и как это выглядит, вы даете его

2252
01:17:50,320 --> 01:17:52,960
как часть его префикса спасибо

2253
01:17:52,960 --> 01:17:55,920
merci привет идет к mint идет вправо,

2254
01:17:55,920 --> 01:17:57,760
так что у вас есть эти примеры перевода,

2255
01:17:57,760 --> 01:18:00,080
вы просите его последний, и он

2256
01:18:00,080 --> 01:18:03,040
приходит с правильным переводом  ion, по-

2257
01:18:03,040 --> 01:18:04,560
видимому, потому, что он узнал что-то

2258
01:18:04,560 --> 01:18:05,840
о задаче, которую вы как бы

2259
01:18:05,840 --> 01:18:08,480
говорите ему с помощью префикса, и

2260
01:18:08,480 --> 01:18:09,840
поэтому вы можете сделать то же самое с

2261
01:18:09,840 --> 01:18:12,080
добавлением, поэтому что-то поместит что-то 5

2262
01:18:12,080 --> 01:18:14,719
плюс 8 равно 13. дайте ему дополнительные примеры, которые

2263
01:18:14,719 --> 01:18:16,400
он может выполнить  следующий пример дополнения

2264
01:18:16,400 --> 01:18:20,400
для вас, эм или, может быть, вы

2265
01:18:20,400 --> 01:18:21,920
пытаетесь выяснить грамматические или

2266
01:18:21,920 --> 01:18:24,320
орфографические ошибки, например, и вот пример

2267
01:18:24,320 --> 01:18:27,199
французского на

2268
01:18:27,199 --> 01:18:28,880
английский, английский на французский

2269
01:18:28,880 --> 01:18:31,679
регистр, так что снова вы учитесь просто делать

2270
01:18:31,679 --> 01:18:32,880
предварительную тренировку,

2271
01:18:32,880 --> 01:18:35,440
но когда вы '  переоценивая его, вы

2272
01:18:35,440 --> 01:18:36,880
даже не настраиваете модель, которую вы просто

2273
01:18:36,880 --> 01:18:39,120
предоставляете префиксы,

2274
01:18:39,120 --> 01:18:41,760
и поэтому это особенно не очень хорошо

2275
01:18:41,760 --> 01:18:42,960
понимается,

2276
01:18:42,960 --> 01:18:44,800
и есть много исследований, посвященных

2277
01:18:44,800 --> 01:18:46,800
тому, каковы

2278
01:18:46,800 --> 01:18:48,080
ограничения этого

2279
01:18:48,080 --> 01:18:49,920
так называемого контекстного обучения  но

2280
01:18:49,920 --> 01:18:52,159
это увлекательное направление для

2281
01:18:52,159 --> 01:18:54,800
будущей работы. В целом, эти модели

2282
01:18:54,800 --> 01:18:56,719
не очень хорошо изучены,

2283
01:18:56,719 --> 01:18:59,440
однако маленькие, маленькие в воздухе модели,

2284
01:18:59,440 --> 01:19:01,360
такие как Burt, стали общими инструментами в

2285
01:19:01,360 --> 01:19:03,280
широком диапазоне настроек, и у них действительно есть

2286
01:19:03,280 --> 01:19:04,800
эти i  вопросы об изучении всех этих

2287
01:19:04,800 --> 01:19:06,719
предубеждений в отношении мира, в который они войдут,

2288
01:19:06,719 --> 01:19:09,440
и дальнейших лекциях в этом курсе, ммм,

2289
01:19:09,440 --> 01:19:10,400


2290
01:19:10,400 --> 01:19:12,080
и так что да, то, что вы узнали на этой

2291
01:19:12,080 --> 01:19:14,239
неделе, трансформеры и предварительная подготовка

2292
01:19:14,239 --> 01:19:17,120
составляют основу или, по крайней мере, исходные данные

2293
01:19:17,120 --> 01:19:18,800
для большей части естественных  языковая обработка

2294
01:19:18,800 --> 01:19:19,679
сегодня

2295
01:19:19,679 --> 01:19:21,120
и задание пять вышло, и вы

2296
01:19:21,120 --> 01:19:24,239
сможете подробнее изучить его,

2297
01:19:24,719 --> 01:19:27,760
и со временем все в порядке,

2298
01:19:31,920 --> 01:19:33,199
да,

2299
01:19:33,199 --> 01:19:34,880
я думаю, я могу ответить на вопрос, если есть

2300
01:19:34,880 --> 01:19:38,320
, если есть, но люди тоже могут начать,

2301
01:19:38,320 --> 01:19:40,880


2302
01:19:45,600 --> 01:19:47,280
так что  Я думаю, я думаю, что есть вопрос

2303
01:19:47,280 --> 01:19:50,640
о дизайне, который был

2304
01:20:01,440 --> 01:20:02,840
x или

2305
01:20:02,840 --> 01:20:07,600
y, это иерархия предсказания x или y,

2306
01:20:07,600 --> 01:20:09,120


2307
01:20:09,120 --> 01:20:10,880
я думаю, он не указывает такого рода

2308
01:20:10,880 --> 01:20:13,120
смысл, откуда он знает, что он в

2309
01:20:13,120 --> 01:20:14,960
настоящее время будет использоваться,

2310
01:20:14,960 --> 01:20:16,719
хорошо, да, это имеет смысл  так что

2311
01:20:16,719 --> 01:20:19,600
же он сделал то, что он делает правильно, чтобы он знал

2312
01:20:19,600 --> 01:20:22,400
от кодировщика, что он должен в какой-то момент

2313
01:20:22,400 --> 01:20:25,360
предсказать x, и в какой-то момент предсказать, почему,

2314
01:20:25,360 --> 01:20:26,719
потому что кодировщик может просто

2315
01:20:26,719 --> 01:20:28,239
вспомнить, что о да, там не хватает двух вещей,

2316
01:20:28,239 --> 01:20:30,239
и если там было

2317
01:20:30,239 --> 01:20:32,719
заменено больше интервалов  будет  z, а затем

2318
01:20:32,719 --> 01:20:34,239
a, а затем ab, и вы знаете все, что угодно,

2319
01:20:34,239 --> 01:20:36,639
просто кучу уникальных ну,

2320
01:20:36,639 --> 01:20:40,560
вы знаете идентификаторы, а затем здесь, мм,

2321
01:20:40,560 --> 01:20:44,239
он должен сказать, хорошо, у меня есть напряжение,

2322
01:20:44,239 --> 01:20:45,920
я полагаю, я могу посмотреть, и я знаю, что

2323
01:20:45,920 --> 01:20:47,280
сначала я должен предсказать  это вы знаете,

2324
01:20:47,280 --> 01:20:49,280
первая замаскированная вещь, поэтому я собираюсь

2325
01:20:49,280 --> 01:20:51,360
сгенерировать это в моем декодере, а затем он, как

2326
01:20:51,360 --> 01:20:52,880
вы знаете, получает этот символ правильно, поэтому

2327
01:20:52,880 --> 01:20:54,639
мы проводим обучение, давая ему правильный

2328
01:20:54,639 --> 01:20:56,800
символ, теперь он получает этот x и говорит:

2329
01:20:56,800 --> 01:20:59,120
хорошо, я  предсказание x сейчас

2330
01:20:59,120 --> 01:21:00,880
правильное, и теперь он может предсказать предиктор

2331
01:21:00,880 --> 01:21:02,719
предсказать предсказать тогда он получит y, поэтому

2332
01:21:02,719 --> 01:21:04,080
мы проводим тренинг по принудительному обучению учителя,

2333
01:21:04,080 --> 01:21:05,679
где мы даем ему правильный ответ после

2334
01:21:05,679 --> 01:21:08,080
наказания, если он неправильный сейчас он понимает

2335
01:21:08,080 --> 01:21:09,760
это y

2336
01:21:09,760 --> 01:21:11,040
правильно и говорит хорошо, теперь у меня есть

2337
01:21:11,040 --> 01:21:12,480
чтобы предсказать, что должно происходить и почему, и это

2338
01:21:12,480 --> 01:21:14,800
может присутствовать, вы знаете, в естественных

2339
01:21:14,800 --> 01:21:16,400
частях этого, а также в том, что

2340
01:21:16,400 --> 01:21:18,159
уже предсказано здесь, потому что у

2341
01:21:18,159 --> 01:21:21,040
декодера есть напряжение внутри себя,

2342
01:21:21,040 --> 01:21:22,480
и он может видеть, что там должно происходить, поэтому

2343
01:21:22,480 --> 01:21:23,679
то, что здесь увлекательно, - это вы ''  ты делаешь

2344
01:21:23,679 --> 01:21:25,760
что-то вроде языка мо  deling, но

2345
01:21:25,760 --> 01:21:27,360
когда вы предсказываете, почему правильно,

2346
01:21:27,360 --> 01:21:29,679
вы видите, что было после этого, и я

2347
01:21:29,679 --> 01:21:30,960
думаю, что это одно из преимуществ коррупции диапазона,

2348
01:21:30,960 --> 01:21:32,320
поэтому вы делаете это,

2349
01:21:32,320 --> 01:21:33,600
когда не знаете, как долго вы

2350
01:21:33,600 --> 01:21:36,159
должны прогнозировать подобный язык  моделирование,

2351
01:21:36,159 --> 01:21:38,719
но вы узнаете, что было после

2352
01:21:38,719 --> 01:21:42,280
того, что вам не хватало

2353
01:21:45,040 --> 01:21:47,120


