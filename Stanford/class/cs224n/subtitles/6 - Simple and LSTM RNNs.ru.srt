1
00:00:05,600 --> 00:00:10,800
Хорошо, привет всем, добро пожаловать обратно в cs224n,

2
00:00:10,800 --> 00:00:13,840
так что сегодня это очень ключевая лекция, в которой

3
00:00:13,840 --> 00:00:16,400
мы рассмотрим ряд важных

4
00:00:16,400 --> 00:00:19,359
тем для нейронных сетей, особенно

5
00:00:19,359 --> 00:00:21,600
применительно к обработке естественного языка,

6
00:00:21,600 --> 00:00:23,519
поэтому в самом конце прошлого раза я

7
00:00:23,519 --> 00:00:26,000
начал с текущих нейронных сетей, поэтому

8
00:00:26,000 --> 00:00:27,920
мы '  Я подробно расскажу о текущих

9
00:00:27,920 --> 00:00:30,160
нейронных сетях в первой части

10
00:00:30,160 --> 00:00:32,079
класса, и

11
00:00:32,079 --> 00:00:34,719
мы выделим языковые модели, но

12
00:00:34,719 --> 00:00:37,520
затем также выйдем за рамки этого, а

13
00:00:37,520 --> 00:00:40,160
затем посмотрим на более продвинутые виды

14
00:00:40,160 --> 00:00:42,239
рекуррентных нейронных сетей ближе к

15
00:00:42,239 --> 00:00:44,879
конечной части класса.

16
00:00:44,879 --> 00:00:47,520
эм, я просто хотел сказать пару слов

17
00:00:47,520 --> 00:00:50,000
перед тем, как приступить к окончательному

18
00:00:50,000 --> 00:00:52,320
проекту, так что, надеюсь, к настоящему времени вы

19
00:00:52,320 --> 00:00:54,879
начали смотреть на третье задание,

20
00:00:54,879 --> 00:00:56,320
которое является серединой из пяти

21
00:00:56,320 --> 00:00:57,920
заданий для первой половины

22
00:00:57,920 --> 00:01:00,320
курса, а затем - для второй.

23
00:01:00,320 --> 00:01:02,320
Конечно, большая часть ваших усилий уходит

24
00:01:02,320 --> 00:01:04,080
на финальный проект,

25
00:01:04,080 --> 00:01:06,640
поэтому на следующей неделе в четверг лекция

26
00:01:06,640 --> 00:01:08,479
будет посвящена финальным проектам,

27
00:01:08,479 --> 00:01:10,479
выбору финального проекта и советам по

28
00:01:10,479 --> 00:01:13,360
окончательной оценке.  проектов и т.

29
00:01:13,360 --> 00:01:14,479


30
00:01:14,479 --> 00:01:16,479


31
00:01:16,479 --> 00:01:18,880


32
00:01:18,880 --> 00:01:20,720


33
00:01:20,720 --> 00:01:22,799


34
00:01:22,799 --> 00:01:25,200


35
00:01:25,200 --> 00:01:26,880
думают о финальных проектах, вы можете

36
00:01:26,880 --> 00:01:29,840
найти некоторую информацию на веб-сайте, но

37
00:01:29,840 --> 00:01:31,439
обратите внимание, что информация, которая есть на

38
00:01:31,439 --> 00:01:34,400
данный момент, все еще является прошлогодней

39
00:01:34,400 --> 00:01:36,240
и будет обновляться в течение

40
00:01:36,240 --> 00:01:37,840
следующей недели,

41
00:01:37,840 --> 00:01:40,000
мы также поговорим о наставниках проекта, если

42
00:01:40,000 --> 00:01:42,479
вы '  У меня есть идеи о людях, которые

43
00:01:42,479 --> 00:01:44,640
самостоятельно могут стать наставниками, и

44
00:01:44,640 --> 00:01:46,799
сейчас самое время спросить их об этом, и

45
00:01:46,799 --> 00:01:48,159
мы как бы поговорим о том, какие

46
00:01:48,159 --> 00:01:51,040
альтернативы

47
00:01:51,040 --> 00:01:52,960
приемлемы, так что на

48
00:01:52,960 --> 00:01:54,399
прошлой лекции

49
00:01:54,399 --> 00:01:57,360
я представил идею языка  модели

50
00:01:57,360 --> 00:01:59,840
настолько вероятностные модели, которые предсказывают

51
00:01:59,840 --> 00:02:02,240
вероятность следующих слов после последовательности слов

52
00:02:02,240 --> 00:02:04,640
, а затем мы посмотрели на

53
00:02:04,640 --> 00:02:06,719
модели языка инграмм и начали с

54
00:02:06,719 --> 00:02:09,520
текущих моделей нейронных сетей, поэтому сегодня

55
00:02:09,520 --> 00:02:11,520
мы собираемся поговорить больше о

56
00:02:11,520 --> 00:02:14,400
простых RNN, которые мы видели ранее  мы говорим об

57
00:02:14,400 --> 00:02:17,280
обучении RNN и использовании RNN, но затем

58
00:02:17,280 --> 00:02:19,680
мы также рассмотрим проблемы,

59
00:02:19,680 --> 00:02:22,319
которые возникают с RNN, и способы

60
00:02:22,319 --> 00:02:24,720
их решения, которые будут мотивировать более

61
00:02:24,720 --> 00:02:26,879
сложную архитектуру RNN под названием

62
00:02:26,879 --> 00:02:29,520
lstms, и мы поговорим о других более

63
00:02:29,520 --> 00:02:33,040
сложных параметрах RNN.  -направленные РНН

64
00:02:33,040 --> 00:02:35,680
и многослойные РНН,

65
00:02:35,680 --> 00:02:38,000
то в следующий вторник

66
00:02:38,000 --> 00:02:39,840
мы, по сути, собираемся

67
00:02:39,840 --> 00:02:43,040
использовать дальнейшие эксплойты и построим архитектуры на основе RNN,

68
00:02:43,040 --> 00:02:45,120
которые мы

69
00:02:45,120 --> 00:02:47,200
рассматривали, чтобы обсудить, как построить

70
00:02:47,200 --> 00:02:49,760
новую систему машинного перевода

71
00:02:49,760 --> 00:02:52,160
с последовательностью в последовательность и

72
00:02:52,160 --> 00:02:55,200
внимательно и эффективно, поскольку эта

73
00:02:55,200 --> 00:02:57,840
модель - это то, что вы будете использовать в задании,

74
00:02:57,840 --> 00:02:59,599
но это также означает, что вы будете использовать

75
00:02:59,599 --> 00:03:00,879
все, о чем мы

76
00:03:00,879 --> 00:03:03,360
говорим сегодня,

77
00:03:03,360 --> 00:03:05,680
хорошо, так что, если вы помните из прошлого раза,

78
00:03:05,680 --> 00:03:08,640
это был  идея простой рекуррентной

79
00:03:08,640 --> 00:03:11,760
языковой модели нейронной сети, поэтому у нас

80
00:03:11,760 --> 00:03:15,200
была последовательность слов в качестве нашего контекста, для

81
00:03:15,200 --> 00:03:17,599
которого мы искали вложения слов,

82
00:03:17,599 --> 00:03:19,360
а затем их текущая модель нейронной сети

83
00:03:19,360 --> 00:03:22,800
запускала этот повторяющийся слой,

84
00:03:22,800 --> 00:03:25,920
где  в каждой точке у нас есть предыдущее

85
00:03:25,920 --> 00:03:28,879
скрытое состояние, которое может быть просто нулем

86
00:03:28,879 --> 00:03:31,120
в начале последовательности,

87
00:03:31,120 --> 00:03:34,239
и вы вводите его

88
00:03:34,239 --> 00:03:36,319
в следующее скрытое состояние, предыдущее

89
00:03:36,319 --> 00:03:39,360
скрытое состояние и кодирование

90
00:03:39,360 --> 00:03:42,480
и преобразование кодирования слова

91
00:03:42,480 --> 00:03:44,640
с помощью этого рекуррентного уравнения нейронной сети

92
00:03:44,640 --> 00:03:46,799
что у меня слева, это

93
00:03:46,799 --> 00:03:49,040
очень центральное место, и на его основе вы

94
00:03:49,040 --> 00:03:52,239
вычисляете новое скрытое представление

95
00:03:52,239 --> 00:03:54,159
для следующего временного шага,

96
00:03:54,159 --> 00:03:56,879
и вы можете повторять это для

97
00:03:56,879 --> 00:03:59,120
последовательных временных шагов.Теперь

98
00:03:59,120 --> 00:04:00,400
мы также

99
00:04:00,400 --> 00:04:02,640
обычно хотим, чтобы наши рекуррентные нейронные

100
00:04:02,640 --> 00:04:05,760
сети производили выходные данные, поэтому

101
00:04:05,760 --> 00:04:08,400
я показываю только  это в конце здесь, но на

102
00:04:08,400 --> 00:04:11,120
каждом временном шаге мы также собираемся

103
00:04:11,120 --> 00:04:13,680
генерировать вывод, и для этого

104
00:04:13,680 --> 00:04:16,880
мы загружаем скрытый слой в

105
00:04:16,880 --> 00:04:18,720
мягкий максимальный слой, поэтому мы делаем еще одну

106
00:04:18,720 --> 00:04:21,440
матрицу, умножаем, добавляем на смещение, положив  это

107
00:04:21,440 --> 00:04:23,680
через уравнение мягкого максимума, и это

108
00:04:23,680 --> 00:04:25,280
затем даст распределение вероятностей

109
00:04:25,280 --> 00:04:28,080
по словам, и мы можем использовать

110
00:04:28,080 --> 00:04:30,320
это, чтобы предсказать, насколько вероятно, что

111
00:04:30,320 --> 00:04:33,040
разные слова будут встречаться после того,

112
00:04:33,040 --> 00:04:36,560
как ученики откроют их

113
00:04:36,800 --> 00:04:39,520
хорошо, я не представил эту

114
00:04:39,520 --> 00:04:41,120
модель, но я действительно не разбирался

115
00:04:41,120 --> 00:04:43,680
в особенностях того, как мы обучаем эту

116
00:04:43,680 --> 00:04:46,479
модель, как мы ее используем, и оцениваем ее, так что

117
00:04:46,479 --> 00:04:50,240
позвольте мне пройти через это сейчас,

118
00:04:50,240 --> 00:04:53,360
так вот как мы обучаем  В языковой

119
00:04:53,360 --> 00:04:55,919
модели rnn мы получаем большой корпус текста, просто

120
00:04:55,919 --> 00:04:58,400
много текста, и поэтому мы можем рассматривать это как

121
00:04:58,400 --> 00:05:02,880
просто длинную последовательность слов от x1 до xt,

122
00:05:02,880 --> 00:05:05,120
и то, что мы собираемся сделать, это

123
00:05:05,120 --> 00:05:07,520
передать ее в rnnlm,

124
00:05:07,520 --> 00:05:10,240
поэтому для каждой позиции мы  мы собираемся взять

125
00:05:10,240 --> 00:05:14,080
префиксы этой последовательности и на основе

126
00:05:14,080 --> 00:05:16,560
каждого префикса мы собираемся

127
00:05:16,560 --> 00:05:17,680
предсказать

128
00:05:17,680 --> 00:05:20,400
распределение вероятностей для

129
00:05:20,400 --> 00:05:23,520
следующего слова,

130
00:05:23,520 --> 00:05:26,720
а затем мы собираемся обучить нашу модель

131
00:05:26,720 --> 00:05:29,360
, оценивая, насколько хорошо мы выполняем свою работу.

132
00:05:29,360 --> 00:05:32,479
что и поэтому функция потерь, которую мы используем,

133
00:05:32,479 --> 00:05:34,960
является функцией потерь, обычно

134
00:05:34,960 --> 00:05:36,960
называемой кросс-энтропийной потерей в

135
00:05:36,960 --> 00:05:39,360
литературе, которая заключается в том, что это отрицательное логарифмическое

136
00:05:39,360 --> 00:05:42,080
правдоподобие длится, поэтому мы собираемся

137
00:05:42,080 --> 00:05:43,120
предсказать

138
00:05:43,120 --> 00:05:46,639
какое-то слово, которое будет следующим, хорошо, у нас есть

139
00:05:46,639 --> 00:05:48,800
распределение вероятностей по

140
00:05:48,800 --> 00:05:51,280
предсказаниям  какое слово будет следующим и на

141
00:05:51,280 --> 00:05:53,039
самом деле там ва  это фактическое следующее слово

142
00:05:53,039 --> 00:05:55,600
в тексте, и поэтому мы хорошо говорим, какую

143
00:05:55,600 --> 00:05:57,440
вероятность вы дали этому слову,

144
00:05:57,440 --> 00:05:59,039
и, возможно, мы дали ему оценку вероятности

145
00:05:59,039 --> 00:06:01,759
0,01 хорошо, было бы

146
00:06:01,759 --> 00:06:03,440
здорово, если бы мы дали оценку вероятности

147
00:06:03,440 --> 00:06:05,840
почти в единицу, потому что это

148
00:06:05,840 --> 00:06:08,160
означало, что мы почти уверены, что то, что

149
00:06:08,160 --> 00:06:09,440
произошло дальше

150
00:06:09,440 --> 00:06:11,840
в нашей модели, и поэтому мы

151
00:06:11,840 --> 00:06:14,000
возьмем убыток в той степени, в которой мы

152
00:06:14,000 --> 00:06:17,360
даем фактическому следующему слову прогнозируемую

153
00:06:17,360 --> 00:06:20,720
вероятность меньше единицы,

154
00:06:20,720 --> 00:06:22,960
чтобы затем получить представление о том, насколько хорошо мы

155
00:06:22,960 --> 00:06:24,319


156
00:06:24,319 --> 00:06:28,560
мы работаем над всем корпусом um, мы вычисляем эту потерю

157
00:06:28,560 --> 00:06:31,120
в каждой позиции, а затем мы вычисляем

158
00:06:31,120 --> 00:06:33,600
среднюю потерю всего обучающего

159
00:06:33,600 --> 00:06:34,560
набора,

160
00:06:34,560 --> 00:06:36,639
поэтому давайте просто рассмотрим это еще раз

161
00:06:36,639 --> 00:06:38,720
более наглядно на следующих

162
00:06:38,720 --> 00:06:41,039
двух слайдах, так что внизу

163
00:06:41,039 --> 00:06:44,319
внизу наши  корпус текста,

164
00:06:44,319 --> 00:06:46,080
мы пропускаем его через

165
00:06:46,080 --> 00:06:48,560
нашу простую рекуррентную нейронную сеть, и

166
00:06:48,560 --> 00:06:50,560
в каждой позиции

167
00:06:50,560 --> 00:06:52,479
мы прогнозируем распределение вероятностей для

168
00:06:52,479 --> 00:06:55,039
слов, которые

169
00:06:55,039 --> 00:06:56,080
мы затем

170
00:06:56,080 --> 00:06:56,880


171
00:06:56,880 --> 00:06:58,960
хорошо произносим, на самом деле

172
00:06:58,960 --> 00:07:00,960
в каждой позиции мы знаем, какое слово на

173
00:07:00,960 --> 00:07:03,680
самом деле будет следующим, поэтому, когда мы находимся на временном шаге

174
00:07:03,680 --> 00:07:04,800


175
00:07:04,800 --> 00:07:07,199
Фактически следующее слово - студенты, потому что

176
00:07:07,199 --> 00:07:08,880
мы можем видеть его здесь справа от нас,

177
00:07:08,880 --> 00:07:10,960
и мы говорим, какую

178
00:07:10,960 --> 00:07:13,120
оценку вероятности вы дали студентам, и в

179
00:07:13,120 --> 00:07:16,080
той степени, в которой она не высока, это не

180
00:07:16,080 --> 00:07:18,960
та, которую мы принимаем на себя, а затем мы продолжаем

181
00:07:18,960 --> 00:07:22,319
ко второму временному шагу, и мы говорим, что на

182
00:07:22,319 --> 00:07:24,400
втором временном шаге вы предсказали

183
00:07:24,400 --> 00:07:26,560
распределение вероятностей по словам,

184
00:07:26,560 --> 00:07:28,960
фактическое следующее слово открывается так, что в той

185
00:07:28,960 --> 00:07:30,479
степени, в которой вы не дали высокой

186
00:07:30,479 --> 00:07:33,680
вероятности открытия, вы понесете убыток, а

187
00:07:33,680 --> 00:07:36,800
затем это повторяется со временем  Шаг третий,

188
00:07:36,800 --> 00:07:39,199
мы надеемся, что модель предсказывает там, на

189
00:07:39,199 --> 00:07:41,680
временном шаге четыре, мы надеемся, что модель

190
00:07:41,680 --> 00:07:44,240
будет предсказывать экзамены, а затем, чтобы вычислить

191
00:07:44,240 --> 00:07:46,400
нашу общую потерю,

192
00:07:46,400 --> 00:07:47,840
мы затем

193
00:07:47,840 --> 00:07:51,599
усредняем потерю за каждый временной шаг,

194
00:07:51,599 --> 00:07:52,560
так что

195
00:07:52,560 --> 00:07:54,720
в некотором смысле это довольно  очевидная вещь, которую

196
00:07:54,720 --> 00:07:57,680
нужно сделать, но обратите внимание, что здесь есть небольшая

197
00:07:57,680 --> 00:08:00,879
тонкость, и, в частности, этот

198
00:08:00,879 --> 00:08:02,400
алгоритм упоминается в

199
00:08:02,400 --> 00:08:04,879
литературе как принудительное принуждение учителя,

200
00:08:04,879 --> 00:08:07,680
и что это значит?

201
00:08:07,680 --> 00:08:10,240


202
00:08:10,240 --> 00:08:13,039


203
00:08:13,039 --> 00:08:15,680
Ok  ай, просто начни генерировать, может быть,

204
00:08:15,680 --> 00:08:17,520
я подскажу, с чего начать,

205
00:08:17,520 --> 00:08:19,199
я скажу, что предложение запускает

206
00:08:19,199 --> 00:08:21,599
учеников, а затем

207
00:08:21,599 --> 00:08:23,440
дайте ему поработать и посмотрите, что оно генерирует в

208
00:08:23,440 --> 00:08:26,400
следующем эм, он может начать говорить,

209
00:08:26,400 --> 00:08:28,879
что ученики заблокированы

210
00:08:28,879 --> 00:08:32,320
классная комната или что-то в этом роде, гм,

211
00:08:32,320 --> 00:08:34,159
и мы могли бы сказать, что это не

212
00:08:34,159 --> 00:08:36,320
очень близко к тому, что говорит настоящий текст,

213
00:08:36,320 --> 00:08:38,479
и каким-то образом мы хотим извлечь уроки из этого,

214
00:08:38,479 --> 00:08:40,479
и если вы пойдете в этом направлении,

215
00:08:40,479 --> 00:08:42,240
есть пространство вещей, которые вы можете сделать,

216
00:08:42,240 --> 00:08:44,640
что приведет  в более сложные алгоритмы,

217
00:08:44,640 --> 00:08:47,360
такие как обучение с подкреплением, эм, но

218
00:08:47,360 --> 00:08:50,160
с точки зрения обучения этих

219
00:08:50,160 --> 00:08:54,240
нейронных моделей это чрезмерно сложное и

220
00:08:54,240 --> 00:08:57,519
ненужное, поэтому у нас есть очень простой

221
00:08:57,519 --> 00:09:00,720
способ делать вещи, которые мы делаем

222
00:09:00,720 --> 00:09:03,680
, просто прогнозируем на один шаг вперед, поэтому

223
00:09:03,680 --> 00:09:07,279
мы говорим, что знаем  что префикс - это

224
00:09:07,279 --> 00:09:09,360
предсказание распределения вероятности

225
00:09:09,360 --> 00:09:11,760
по следующему слову, это хорошо,

226
00:09:11,760 --> 00:09:14,160
если вы даете мастер вероятности открытому

227
00:09:14,160 --> 00:09:16,320
хорошо, теперь префикс - это открытый ученик,

228
00:09:16,320 --> 00:09:17,920
предсказать

229
00:09:17,920 --> 00:09:20,240
распределение вероятности  Ион над следующим

230
00:09:20,240 --> 00:09:22,560
словом, это хорошо в той мере, в какой вы

231
00:09:22,560 --> 00:09:25,760
указываете там мастер вероятностей, и так

232
00:09:25,760 --> 00:09:29,440
эффективно на каждом этапе мы сбрасываем

233
00:09:29,440 --> 00:09:31,440
то, что было на самом деле в корпусе, чтобы

234
00:09:31,440 --> 00:09:32,720
вы знали, что это возможно после того, как

235
00:09:32,720 --> 00:09:35,360
студенты открыли модель, которая считает, что,

236
00:09:35,360 --> 00:09:38,000
безусловно, наиболее  вероятная вещь, которая будет дальше

237
00:09:38,000 --> 00:09:40,880
, ах или так сказать, я имею в виду, что мы на

238
00:09:40,880 --> 00:09:43,839
самом деле не используем то, что предлагала модель, мы

239
00:09:43,839 --> 00:09:45,440
наказываем модель за то, что она не

240
00:09:45,440 --> 00:09:47,760
предлагала там, но затем мы просто идем с

241
00:09:47,760 --> 00:09:49,920
тем, что на самом деле находится в корпусе, и просим ее

242
00:09:49,920 --> 00:09:52,800
снова предсказать, что

243
00:09:55,600 --> 00:09:58,080
это  просто небольшая

244
00:09:58,080 --> 00:10:00,800
побочная вещь, но это важная часть, чтобы

245
00:10:00,800 --> 00:10:01,680
знать

246
00:10:01,680 --> 00:10:03,279
, действительно ли вы тренируете свою собственную

247
00:10:03,279 --> 00:10:05,120
нейронную языковую модель, которую я

248
00:10:05,120 --> 00:10:07,440
как бы представлен как один огромный

249
00:10:07,440 --> 00:10:10,480
корпус, который мы пихаем, но на

250
00:10:10,480 --> 00:10:13,200
практике мы не прогоняем весь

251
00:10:13,200 --> 00:10:16,640
корпус ни на один шаг за  время то, что мы делаем,

252
00:10:16,640 --> 00:10:19,600
мы разрезаем весь корпус на более короткие

253
00:10:19,600 --> 00:10:22,800
части, которые обычно могут быть предложениями

254
00:10:22,800 --> 00:10:24,640
или документами, а иногда они

255
00:10:24,640 --> 00:10:26,880
буквально просто кусочки, которые нарезаны

256
00:10:26,880 --> 00:10:28,720
правильно, чтобы вы вспомнили этот

257
00:10:28,720 --> 00:10:30,959
запах стохастического градиента  позволяет нам вычислять

258
00:10:30,959 --> 00:10:33,120
потери и градиенты из небольшого фрагмента

259
00:10:33,120 --> 00:10:36,160
данных и обновлять, поэтому мы берем

260
00:10:36,160 --> 00:10:39,200
эти небольшие фрагменты, вычисляем градиенты

261
00:10:39,200 --> 00:10:41,839
из них, обновляем веса и повторяем,

262
00:10:41,839 --> 00:10:43,600
и, в частности,

263
00:10:43,600 --> 00:10:46,000
мы получаем намного больше скорости, эффективности

264
00:10:46,000 --> 00:10:48,640
и обучения, если мы  на самом деле

265
00:10:48,640 --> 00:10:51,360
не обновляют только одно предложение

266
00:10:51,360 --> 00:10:54,560
за раз, а на самом деле пакет предложений,

267
00:10:54,560 --> 00:10:56,959
поэтому обычно то, что мы на самом деле делаем, это

268
00:10:56,959 --> 00:11:00,079
мы загружаем модели 32 предложения, скажем,

269
00:11:00,079 --> 00:11:02,560
одинаковой длины, в то же время

270
00:11:02,560 --> 00:11:05,440
вычисляя градиенты для  они обновляют

271
00:11:05,440 --> 00:11:07,440
веса, а затем получают еще одну партию

272
00:11:07,440 --> 00:11:10,720
предложений для обучения,

273
00:11:11,040 --> 00:11:13,440
как мы тренируемся, я вроде как не

274
00:11:13,440 --> 00:11:16,880
вдавался в подробности этого, я имею в виду, что

275
00:11:16,880 --> 00:11:19,120
в определенном смысле ответ такой же, как мы

276
00:11:19,120 --> 00:11:21,760
говорили в лекции три, которые мы используем

277
00:11:21,760 --> 00:11:23,680
обратное распространение для получения градиентов и

278
00:11:23,680 --> 00:11:26,000
обновления параметров um, но давайте потратим хотя

279
00:11:26,000 --> 00:11:28,560
бы минуту, чтобы пройти через

280
00:11:28,560 --> 00:11:30,800
различия и тонкости

281
00:11:30,800 --> 00:11:34,000
текущего случая нейронной сети um и

282
00:11:34,000 --> 00:11:36,880
центральную вещь, которую вы немного знаете, как и

283
00:11:36,880 --> 00:11:40,000
прежде, мы собираемся t  возьмем нашу потерю, и

284
00:11:40,000 --> 00:11:42,959
мы собираемся распространить ее обратно на

285
00:11:42,959 --> 00:11:45,040
все параметры сети,

286
00:11:45,040 --> 00:11:46,880
от встраивания слов до

287
00:11:46,880 --> 00:11:50,399
смещений и т. д., но центральный бит, который

288
00:11:50,399 --> 00:11:52,480
немного отличается и более

289
00:11:52,480 --> 00:11:55,120
сложен, заключается в

290
00:11:55,120 --> 00:11:58,399
том, что у нас есть эта матрица wh, которая работает

291
00:11:58,399 --> 00:12:00,720
вместе  последовательность, которую мы продолжаем

292
00:12:00,720 --> 00:12:05,040
применять для обновления нашего скрытого состояния,

293
00:12:05,040 --> 00:12:09,200
так какова производная jt от теты

294
00:12:09,200 --> 00:12:11,040
по отношению к повторяющейся

295
00:12:11,040 --> 00:12:13,120
матрице весов wh

296
00:12:13,120 --> 00:12:16,720
, и ответ на это - то,

297
00:12:16,720 --> 00:12:18,800
что мы делаем, мы

298
00:12:18,800 --> 00:12:22,480
смотрим на нее в каждой позиции

299
00:12:22,480 --> 00:12:27,680
и  выяснить, какие частичные элементы имеют gt

300
00:12:27,680 --> 00:12:31,920
по отношению к wh в позиции один или

301
00:12:31,920 --> 00:12:34,320
позиции два, позиции три, позиции

302
00:12:34,320 --> 00:12:36,800
четыре и т. д. прямо вдоль последовательности,

303
00:12:36,800 --> 00:12:39,680
и мы просто суммируем все эти частичные,

304
00:12:39,680 --> 00:12:43,680
и это дает нам частичное значение для jt

305
00:12:43,680 --> 00:12:46,480
относительно wh в целом

306
00:12:46,480 --> 00:12:48,000
um

307
00:12:48,000 --> 00:12:51,200
поэтому ответ для текущей нейронной сети

308
00:12:51,200 --> 00:12:53,120
- это градиент по отношению к

309
00:12:53,120 --> 00:12:55,760
повторяющемуся весу в нашей текущей сети,

310
00:12:55,760 --> 00:12:58,079
это сумма градиента по отношению

311
00:12:58,079 --> 00:13:00,240
к каждому разу, когда он появляется

312
00:13:00,240 --> 00:13:01,279
um,

313
00:13:01,279 --> 00:13:02,880
и позвольте мне просто

314
00:13:02,880 --> 00:13:06,320
пройти через  тьфу, немного, почему это так,

315
00:13:06,320 --> 00:13:09,600
но прежде чем я это сделаю, позвольте мне просто

316
00:13:09,600 --> 00:13:12,000
отметить одну ошибку, я имею в виду,

317
00:13:12,000 --> 00:13:14,800
что это просто не тот случай, что это означает, что

318
00:13:14,800 --> 00:13:20,320
она равна t раз частичному jt по

319
00:13:20,320 --> 00:13:23,600
отношению к wh, потому что мы используем wh

320
00:13:23,600 --> 00:13:26,160
здесь, здесь, здесь  здесь, здесь, через

321
00:13:26,160 --> 00:13:28,880
последовательность, и для каждого из мест, где мы

322
00:13:28,880 --> 00:13:31,279
его используем, есть свой восходящий

323
00:13:31,279 --> 00:13:33,519
градиент, который подается в него, поэтому

324
00:13:33,519 --> 00:13:36,000
каждое из значений в этой сумме будет

325
00:13:36,000 --> 00:13:39,680
полностью отличаться друг от друга,

326
00:13:39,839 --> 00:13:40,880


327
00:13:40,880 --> 00:13:45,040
поэтому мы получаем этот ответ, по сути, является

328
00:13:45,040 --> 00:13:47,519
следствием  то, о чем мы говорили

329
00:13:47,519 --> 00:13:49,360
в третьей лекции

330
00:13:49,360 --> 00:13:51,920
, чтобы взять простейший случай этого

331
00:13:51,920 --> 00:13:54,639
права: если у вас есть функция с несколькими переменными

332
00:13:54,639 --> 00:13:57,519
f от xy,

333
00:13:57,519 --> 00:13:59,279
и у вас есть две

334
00:13:59,279 --> 00:14:02,000
функции с одной переменной x от t и y

335
00:14:02,000 --> 00:14:02,959
от t,

336
00:14:02,959 --> 00:14:05,440
которые хорошо подаются на один вход

337
00:14:05,440 --> 00:14:08,320
t  тогда

338
00:14:08,320 --> 00:14:11,279
простой вариант вычисления

339
00:14:11,279 --> 00:14:14,160
производной этой функции состоит в том, что

340
00:14:14,160 --> 00:14:16,800
вы берете производную по одному пути, а

341
00:14:16,800 --> 00:14:19,360
производную по другому пути,

342
00:14:19,360 --> 00:14:21,839
и поэтому на слайдах в

343
00:14:21,839 --> 00:14:24,240
третьей лекции это было то, что было

344
00:14:24,240 --> 00:14:26,639
резюмировано на  пара слайдов по

345
00:14:26,639 --> 00:14:30,240
лозунгу градиентная сумма на внешних ветвях,

346
00:14:30,240 --> 00:14:33,760
так что у t есть внешние ветки, и поэтому вы

347
00:14:33,760 --> 00:14:36,800
берете градиент здесь, на левом градиенте

348
00:14:36,800 --> 00:14:39,519
справа, и вы суммируете их вместе,

349
00:14:39,519 --> 00:14:41,680
и поэтому на самом деле то, что происходит с

350
00:14:41,680 --> 00:14:44,800
рекуррентной нейронной сетью, - это просто

351
00:14:44,800 --> 00:14:47,920
обобщение множества частей  это, поэтому у нас

352
00:14:47,920 --> 00:14:51,600
есть одна матрица wh, и мы используем ее, чтобы

353
00:14:51,600 --> 00:14:54,160
продолжать обновлять скрытое состояние во

354
00:14:54,160 --> 00:14:57,680
время 1, 2, 3, прямо через время

355
00:14:57,680 --> 00:14:58,880
t,

356
00:14:58,880 --> 00:15:02,639
и поэтому мы собираемся получить, что у

357
00:15:02,639 --> 00:15:06,560
нее много внешних

358
00:15:06,560 --> 00:15:09,120
ветвей  и мы собираемся

359
00:15:09,120 --> 00:15:12,880
просуммировать путь градиента для каждого из них,

360
00:15:12,880 --> 00:15:13,760
но

361
00:15:13,760 --> 00:15:15,600
что это за

362
00:15:15,600 --> 00:15:18,000
путь градиента здесь, он как бы идет здесь,

363
00:15:18,000 --> 00:15:20,399
а затем идет вниз, но вы знаете, что на

364
00:15:20,399 --> 00:15:23,199
самом деле нижняя часть заключается в том, что мы

365
00:15:23,199 --> 00:15:27,199
просто используем wh для каждого  позиции, поэтому у нас

366
00:15:27,199 --> 00:15:30,480
есть партиал wh, используемый в позиции

367
00:15:30,480 --> 00:15:34,320
i по отношению к партиалу wh,

368
00:15:34,320 --> 00:15:36,480
который является просто нашей матрицей весов для нашей

369
00:15:36,480 --> 00:15:38,880
текущей нейронной сети, так что это только

370
00:15:38,880 --> 00:15:41,120
один, потому что вы знаете, что мы просто используем

371
00:15:41,120 --> 00:15:44,240
одну и ту же матрицу везде, и поэтому мы

372
00:15:44,240 --> 00:15:45,279
именно тогда 

373
00:15:45,279 --> 00:15:47,360


374
00:15:47,360 --> 00:15:49,360
суммируя частички в каждой позиции, которую мы

375
00:15:49,360 --> 00:15:51,839
используем,

376
00:15:52,480 --> 00:15:55,440
хорошо, на практике, что это значит

377
00:15:55,440 --> 00:15:59,519
с точки зрения того, как вы хорошо вычисляете это,

378
00:15:59,519 --> 00:16:02,079
если вы делаете это вручную, что

379
00:16:02,079 --> 00:16:05,199
происходит, вы начинаете с конца,

380
00:16:05,199 --> 00:16:07,920
как в этой общей лекции три  story

381
00:16:07,920 --> 00:16:10,399
вы разрабатываете

382
00:16:10,399 --> 00:16:12,480
производные

383
00:16:12,480 --> 00:16:14,639
по отношению к скрытому слою, а

384
00:16:14,639 --> 00:16:18,320
затем по отношению к wh на последнем временном

385
00:16:18,320 --> 00:16:20,880
шаге, и это дает вам

386
00:16:20,880 --> 00:16:24,160
одно обновление для wh, но затем вы продолжаете

387
00:16:24,160 --> 00:16:26,880
передавать градиент обратно к t минус

388
00:16:26,880 --> 00:16:29,440
один временной шаг и через пару  больше

389
00:16:29,440 --> 00:16:32,639
шагов правила цепочки вы получаете еще одно

390
00:16:32,639 --> 00:16:35,680
обновление для wh, и вы просто суммируете это с

391
00:16:35,680 --> 00:16:38,399
вашим предыдущим обновлением для wh, а

392
00:16:38,399 --> 00:16:39,600
затем вы переходите к

393
00:16:39,600 --> 00:16:43,199
ht минус 2, вы получаете еще одно обновление для wh,

394
00:16:43,199 --> 00:16:45,759
и вы суммируете это со своим обновлением для wh,

395
00:16:45,759 --> 00:16:48,160
и вы возвращаетесь назад  все время,

396
00:16:48,160 --> 00:16:50,480
и вы суммируете градиенты по мере

397
00:16:50,480 --> 00:16:53,920
продвижения, и это дает вам полное обновление

398
00:16:53,920 --> 00:16:55,360
um для чего,

399
00:16:55,360 --> 00:16:58,880
и поэтому здесь есть два трюка, и

400
00:16:58,880 --> 00:17:01,759
я просто упомяну два трюка, которые вы

401
00:17:01,759 --> 00:17:03,680
должны как бы отдельно суммировать

402
00:17:03,680 --> 00:17:06,079
обновления  для чего, а потом еще раз  вы

403
00:17:06,079 --> 00:17:09,119
закончили применять их все сразу, вы

404
00:17:09,119 --> 00:17:10,640
не хотите на самом деле изменять

405
00:17:10,640 --> 00:17:13,760
матрицу wh по мере продвижения, потому что тогда это

406
00:17:13,760 --> 00:17:16,000
недействительно, потому что прямые

407
00:17:16,000 --> 00:17:18,880
вычисления были выполнены с константой

408
00:17:18,880 --> 00:17:22,559
wh, которая была у вас из предыдущего

409
00:17:22,559 --> 00:17:25,599
состояния по всей сети

410
00:17:25,599 --> 00:17:28,160
второй трюк хорош, если вы делаете

411
00:17:28,160 --> 00:17:30,880
это для предложений, вы обычно можете просто

412
00:17:30,880 --> 00:17:33,440
вернуться к началу предложения,

413
00:17:33,440 --> 00:17:36,320
но если у вас очень длинные последовательности,

414
00:17:36,320 --> 00:17:38,400
это может действительно замедлить вас, если

415
00:17:38,400 --> 00:17:40,080
вам нужно как бы запустить это  алгоритм

416
00:17:40,080 --> 00:17:43,679
обратно в течение огромного количества времени,

417
00:17:43,679 --> 00:17:45,760
поэтому то, что люди обычно делают, это

418
00:17:45,760 --> 00:17:48,559
то, что называется усеченным обратным распространением

419
00:17:48,559 --> 00:17:50,640
во времени, где вы выбираете некоторую

420
00:17:50,640 --> 00:17:53,440
константу, скажем 20, и вы говорите хорошо, я

421
00:17:53,440 --> 00:17:56,160
просто собираюсь запустить это обратное распространение

422
00:17:56,160 --> 00:17:59,840
для 20 временных шагов, суммируйте эти 20 градиентов

423
00:17:59,840 --> 00:18:03,039
а затем я просто сделал это то, чем я

424
00:18:03,039 --> 00:18:06,240
обновлю матрицу wh,

425
00:18:06,240 --> 00:18:09,840
и это работает нормально,

426
00:18:09,919 --> 00:18:13,280
хорошо, поэтому теперь, учитывая корпус,

427
00:18:13,280 --> 00:18:14,720
мы можем обучить

428
00:18:14,720 --> 00:18:16,559


429
00:18:16,559 --> 00:18:20,240
простой rnn, и это хороший прогресс,

430
00:18:20,240 --> 00:18:21,840
но

431
00:18:21,840 --> 00:18:23,919
это модель, которая также может генерировать

432
00:18:23,919 --> 00:18:26,559
t  ext в целом, так как же нам

433
00:18:26,559 --> 00:18:29,679
хорошо сгенерировать текст точно так же, как в нашей

434
00:18:29,679 --> 00:18:31,360
языковой модели инграмм, мы собираемся генерировать

435
00:18:31,360 --> 00:18:34,640
текст путем повторной выборки, поэтому мы собираемся

436
00:18:34,640 --> 00:18:38,799
начать с начального состояния,

437
00:18:39,760 --> 00:18:40,840
и

438
00:18:40,840 --> 00:18:44,720
да, этот слайд несовершенен,

439
00:18:44,720 --> 00:18:46,799
так что начальный  состояние для скрытого

440
00:18:46,799 --> 00:18:48,480
состояния

441
00:18:48,480 --> 00:18:51,280
um обычно просто берется как нулевой

442
00:18:51,280 --> 00:18:53,520
вектор, и тогда нам нужно что-

443
00:18:53,520 --> 00:18:56,240
то для первого ввода, и на этом

444
00:18:56,240 --> 00:18:58,080
слайде первый ввод отображается как

445
00:18:58,080 --> 00:19:00,559
первое слово my, и если вы хотите

446
00:19:00,559 --> 00:19:03,760
указать начальную точку  вы могли бы скормить мне, но

447
00:19:03,760 --> 00:19:05,840
большую часть времени вы хотели бы сгенерировать

448
00:19:05,840 --> 00:19:07,840
предложение из ничего, и если вы хотите

449
00:19:07,840 --> 00:19:10,080
сделать это, то обычно нужно

450
00:19:10,080 --> 00:19:12,080
дополнительно иметь

451
00:19:12,080 --> 00:19:14,720
токен начала последовательности, который является специальным токеном,

452
00:19:14,720 --> 00:19:16,160
поэтому вы будете кормить в  начало

453
00:19:16,160 --> 00:19:18,400
токена последовательности в начале в

454
00:19:18,400 --> 00:19:21,360
качестве первого токена, у которого есть встраивание, а

455
00:19:21,360 --> 00:19:23,600
затем вы используете

456
00:19:23,600 --> 00:19:26,400
обновление rnn, а затем вы генерируете,

457
00:19:26,400 --> 00:19:29,600
используя softmax и следующее слово, и,

458
00:19:29,600 --> 00:19:32,080
ну, вы генерируете

459
00:19:32,080 --> 00:19:34,640
вероятность распределения вероятностей по следующим словам

460
00:19:34,640 --> 00:19:36,720
и t  в этот момент вы пробуете из

461
00:19:36,720 --> 00:19:39,200
этого, и он выбирает какое-то слово, например

462
00:19:39,200 --> 00:19:42,320
избранное, и тогда трюк состоит в том, чтобы

463
00:19:42,320 --> 00:19:44,960
выполнить генерацию, вы берете это слово,

464
00:19:44,960 --> 00:19:47,520
которое вы выбрали, и копируете его

465
00:19:47,520 --> 00:19:50,160
обратно на вход, а затем вы вводите

466
00:19:50,160 --> 00:19:52,720
его как импорт  следующий шаг, если вы

467
00:19:52,720 --> 00:19:53,520


468
00:19:53,520 --> 00:19:56,240
образец n из softmax, возьмите другое слово

469
00:19:56,240 --> 00:19:58,559
и просто продолжайте повторять это снова и

470
00:19:58,559 --> 00:20:01,520
снова, и вы начинаете генерировать

471
00:20:01,520 --> 00:20:02,559
текст,

472
00:20:02,559 --> 00:20:05,760
и как вы заканчиваете, а также имея

473
00:20:05,760 --> 00:20:07,520
начало последовательности

474
00:20:07,520 --> 00:20:10,159
um специальный символ, у вас обычно есть

475
00:20:10,159 --> 00:20:12,640
конец  специального символа последовательности, и в

476
00:20:12,640 --> 00:20:14,799
какой-то момент рекуррентная нейронная

477
00:20:14,799 --> 00:20:17,520
сеть сгенерирует конец

478
00:20:17,520 --> 00:20:20,720
символа последовательности um, а затем вы говорите, хорошо,

479
00:20:20,720 --> 00:20:25,200
я закончил, я закончил генерировать текст,

480
00:20:25,200 --> 00:20:27,440
поэтому, прежде чем переходить к

481
00:20:27,440 --> 00:20:29,120
более

482
00:20:29,120 --> 00:20:31,440
сложному содержанию лекции  мы можем

483
00:20:31,440 --> 00:20:33,520
просто немного повеселиться с этим

484
00:20:33,520 --> 00:20:34,960
и попробовать

485
00:20:34,960 --> 00:20:37,679
потренироваться и сгенерировать текст с нашей

486
00:20:37,679 --> 00:20:40,000
текущей моделью нейронной сети, чтобы вы могли

487
00:20:40,000 --> 00:20:42,480
сгенерировать, вы можете обучать rnn на любом

488
00:20:42,480 --> 00:20:44,960
типе текста, и это означает, что это одна

489
00:20:44,960 --> 00:20:47,039
из забавных вещей  Вы можете

490
00:20:47,039 --> 00:20:48,720
создавать текст

491
00:20:48,720 --> 00:20:50,799
в разных стилях в зависимости от того, что вы

492
00:20:50,799 --> 00:20:53,760
могли бы использовать для его обучения,

493
00:20:54,400 --> 00:20:56,320
так что здесь

494
00:20:56,320 --> 00:20:58,799
есть большой объем

495
00:20:58,799 --> 00:21:01,520
текста, так что вы можете обучать

496
00:21:01,520 --> 00:21:03,919
rnn lm на книгах о Гарри Поттере, а затем

497
00:21:03,919 --> 00:21:06,559
сказать dolph  и сгенерируйте какой-то текст, и

498
00:21:06,559 --> 00:21:09,360
он сгенерирует такой текст, извините, как

499
00:21:09,360 --> 00:21:11,679
Гарри кричал в панике, я оставлю эти

500
00:21:11,679 --> 00:21:13,919
метлы в Лондоне, неужели они

501
00:21:13,919 --> 00:21:15,840
не догадывались, сказали, что почти обезглавленный Ник

502
00:21:15,840 --> 00:21:18,320
низко бросается рядом с Седриком, несущим

503
00:21:18,320 --> 00:21:20,320
последний кусочек патоки с

504
00:21:20,320 --> 00:21:22,400
плеча Гарри и на  ответьте ему,

505
00:21:22,400 --> 00:21:24,960
гостиная толкнула его, четыре руки держали

506
00:21:24,960 --> 00:21:27,200
блестящую ручку, когда паук не

507
00:21:27,200 --> 00:21:30,720
чувствовал, что казалось, что он слишком хорошо добрался до команд,

508
00:21:30,720 --> 00:21:34,159
так что, с одной стороны, это все еще

509
00:21:34,159 --> 00:21:37,200
немного бессвязно, как рассказ,

510
00:21:37,200 --> 00:21:39,520
с другой стороны  это вроде как звучит как

511
00:21:39,520 --> 00:21:41,679
гарри поттер и, конечно, из тех,

512
00:21:41,679 --> 00:21:44,159
кто знает лексику и конструкции в

513
00:21:44,159 --> 00:21:47,679
пользователях, и я думаю, вы согласитесь, что

514
00:21:47,679 --> 00:21:49,360
знаете, даже если это становится своего рода

515
00:21:49,360 --> 00:21:52,559
бессвязным, это вроде более связно,

516
00:21:52,559 --> 00:21:54,880
чем то, что мы получили от ингра.  m языковая

517
00:21:54,880 --> 00:21:57,600
модель эм, когда я показал поколение

518
00:21:57,600 --> 00:21:58,799
на прошлой

519
00:21:58,799 --> 00:22:00,559
лекции,

520
00:22:00,559 --> 00:22:03,520
вы можете выбрать совсем другой стиль

521
00:22:03,520 --> 00:22:04,640
текста,

522
00:22:04,640 --> 00:22:06,559
чтобы вместо этого вы могли

523
00:22:06,559 --> 00:22:09,600
обучить модель на кучке кулинарных книг,

524
00:22:09,600 --> 00:22:11,919
и если вы это сделаете, вы можете затем сказать

525
00:22:11,919 --> 00:22:13,200
генерировать

526
00:22:13,200 --> 00:22:14,640
на основе того, что вы  узнал о

527
00:22:14,640 --> 00:22:16,000
поваренных книгах,

528
00:22:16,000 --> 00:22:17,919
и он просто сгенерирует рецепт, так что

529
00:22:17,919 --> 00:22:19,760
вот рецепт

530
00:22:19,760 --> 00:22:22,960
шоколадное ранчо барбекю

531
00:22:22,960 --> 00:22:25,520
категории дают шесть порций

532
00:22:25,520 --> 00:22:28,080
две столовые ложки нарезанного сыра пармезан

533
00:22:28,080 --> 00:22:31,039
и одна чашка кокосового молока

534
00:22:31,039 --> 00:22:32,880
три взбитых яйца

535
00:22:32,880 --> 00:22:35,760
поместите каждую пасту поверх слоев

536
00:22:35,760 --> 00:22:38,000
смеси в форме комков в умеренную духовку

537
00:22:38,000 --> 00:22:41,200
и тушить, пока не станет твердой подача. Горячие воплощенные

538
00:22:41,200 --> 00:22:44,080
свежие горчицы, апельсин и сыр.

539
00:22:44,080 --> 00:22:46,799
Смешайте сыр и соль.

540
00:22:46,799 --> 00:22:48,960
Тесто в большой сковороде. Добавьте

541
00:22:48,960 --> 00:22:51,120
ингредиенты и перемешайте шоколад

542
00:22:51,120 --> 00:22:52,880
и перец.

543
00:22:52,880 --> 00:22:56,240


544
00:22:56,240 --> 00:22:58,000


545
00:22:58,000 --> 00:22:59,679
на самом деле даже нет

546
00:22:59,679 --> 00:23:01,760
опасности, что вы попробуете приготовить это

547
00:23:01,760 --> 00:23:04,159
дома, но вы знаете, что кое-что

548
00:23:04,159 --> 00:23:07,200
интересное, хотя вы знаете, что это

549
00:23:07,200 --> 00:23:08,559
действительно просто

550
00:23:08,559 --> 00:23:11,440
не  Рецепт и то, что

551
00:23:11,440 --> 00:23:14,240
делается в инструкциях, не имеет

552
00:23:14,240 --> 00:23:17,919
отношения к ингредиентам.

553
00:23:17,919 --> 00:23:19,919
Интересно то, что он

554
00:23:19,919 --> 00:23:22,240
узнал, это рекуррентная модель нейронной сети,

555
00:23:22,240 --> 00:23:25,200
что она действительно освоила

556
00:23:25,200 --> 00:23:27,600
общую структуру рецепта, который, как он знает,

557
00:23:27,600 --> 00:23:30,559
рецепт имеет  название, которое он часто говорит

558
00:23:30,559 --> 00:23:33,360
вам о том, сколько людей он обслуживает, он

559
00:23:33,360 --> 00:23:35,280
перечисляет ингредиенты, а затем в нем есть

560
00:23:35,280 --> 00:23:37,679
инструкции, ммм, чтобы сделать это так, что

561
00:23:37,679 --> 00:23:40,559
это довольно впечатляюще в некотором смысле

562
00:23:40,559 --> 00:23:43,919
для высокоуровневого структурирования текста,

563
00:23:43,919 --> 00:23:46,559
поэтому еще одна вещь, которую я хотел

564
00:23:46,559 --> 00:23:48,400
упомянуть, это

565
00:23:48,400 --> 00:23:50,960
когда я говорю, что вы можете обучить языковую модель rnn

566
00:23:50,960 --> 00:23:53,039
в любом виде текста, другое

567
00:23:53,039 --> 00:23:54,880
отличие от того, где мы были в

568
00:23:54,880 --> 00:23:57,279
моделях грамматического языка, было в языковых

569
00:23:57,279 --> 00:23:59,760
моделях инграмм, что означало просто подсчет инграмм

570
00:23:59,760 --> 00:24:01,840
и означало, что это заняло

571
00:24:01,840 --> 00:24:04,240
две минуты или даже большой корпус

572
00:24:04,240 --> 00:24:06,559
при любом современном компьютерном обучении ваш

573
00:24:06,559 --> 00:24:09,039
rnn lm на самом деле может быть

574
00:24:09,039 --> 00:24:11,200
занятием, требующим много времени, и вы можете тратить на

575
00:24:11,200 --> 00:24:13,679
это часы, как вы можете обнаружить на следующей

576
00:24:13,679 --> 00:24:15,520
неделе, когда будете тренировать

577
00:24:15,520 --> 00:24:18,400
ма.  китайские модели перевода

578
00:24:18,400 --> 00:24:19,760
хорошо,

579
00:24:19,760 --> 00:24:22,799
как нам решить, хороши наши модели

580
00:24:22,799 --> 00:24:24,159
или нет,

581
00:24:24,159 --> 00:24:26,080


582
00:24:26,080 --> 00:24:29,520
так что стандартная метрика оценки для

583
00:24:29,520 --> 00:24:31,679
языковых моделей - это то, что называется

584
00:24:31,679 --> 00:24:33,679
недоумением,

585
00:24:33,679 --> 00:24:36,960
а то, что такое недоумение,

586
00:24:36,960 --> 00:24:37,840


587
00:24:37,840 --> 00:24:40,880
похоже на то, как когда вы

588
00:24:40,880 --> 00:24:43,840
тренируете свою модель, вы используете учитель,

589
00:24:43,840 --> 00:24:45,039


590
00:24:45,039 --> 00:24:47,679
заставляющий кусок  текста, который представляет собой другой

591
00:24:47,679 --> 00:24:50,720
фрагмент тестового текста, который не является текстом, который

592
00:24:50,720 --> 00:24:52,960
был в обучающих данных, и вы хорошо говорите,

593
00:24:52,960 --> 00:24:57,440
учитывая последовательность из t слов,

594
00:24:57,440 --> 00:24:59,360
какую вероятность вы даете

595
00:24:59,360 --> 00:25:02,480
фактическому t плюс одно слово, и вы

596
00:25:02,480 --> 00:25:04,960
повторяете это в каждой позиции

597
00:25:04,960 --> 00:25:07,200
и  затем вы берете обратную

598
00:25:07,200 --> 00:25:10,000
вероятность этой вероятности и увеличиваете ее до

599
00:25:10,000 --> 00:25:13,039
единицы на t для длины вашего тестового

600
00:25:13,039 --> 00:25:14,400
образца текста,

601
00:25:14,400 --> 00:25:17,360
и это число является недоумением, так

602
00:25:17,360 --> 00:25:20,480
что это среднее геометрическое обратных

603
00:25:20,480 --> 00:25:22,159
вероятностей

604
00:25:22,159 --> 00:25:25,360
теперь после этого объяснения, возможно, более

605
00:25:25,360 --> 00:25:28,240
простой способ думать  из-за того, что

606
00:25:28,240 --> 00:25:29,919
недоумение

607
00:25:29,919 --> 00:25:32,240
- это

608
00:25:32,240 --> 00:25:34,480
просто потеря кросс-энтропии, которую я ввел

609
00:25:34,480 --> 00:25:38,080
перед возведением в степень,

610
00:25:38,080 --> 00:25:40,000
так

611
00:25:40,000 --> 00:25:41,679
что теперь вы знаете, что сейчас все наоборот,

612
00:25:41,679 --> 00:25:42,720


613
00:25:42,720 --> 00:25:46,159
так что низкая недоумение лучше, так

614
00:25:46,159 --> 00:25:47,679
что  На самом деле это интересная история

615
00:25:47,679 --> 00:25:51,360
об этих затруднениях, гм, так что известным

616
00:25:51,360 --> 00:25:53,760
деятелем в разработке

617
00:25:53,760 --> 00:25:55,360
вероятностных подходов и подходов машинного обучения

618
00:25:55,360 --> 00:25:56,960
к обработке естественного языка

619
00:25:56,960 --> 00:25:59,679
является Фред Гелланек, который умер

620
00:25:59,679 --> 00:26:01,600
несколько лет назад

621
00:26:01,600 --> 00:26:05,520
и пытался

622
00:26:06,000 --> 00:26:08,880
заинтересовать людей идеей использования

623
00:26:08,880 --> 00:26:11,279
вероятностных моделей.  и машинное обучение

624
00:26:11,279 --> 00:26:14,000
для обработки естественного языка в

625
00:26:14,000 --> 00:26:18,480
то время. Это 1970-е и начало 1980-х годов,

626
00:26:18,480 --> 00:26:22,720
когда почти все в области искусственного интеллекта

627
00:26:22,720 --> 00:26:25,679
все еще были в плену логических

628
00:26:25,679 --> 00:26:28,720
моделей, архитектур классной доски и

629
00:26:28,720 --> 00:26:30,159
тому подобного для

630
00:26:30,159 --> 00:26:32,480
систем искусственного интеллекта и т. д.

631
00:26:32,480 --> 00:26:35,039
Гелларнек на самом деле был теоретиком информации

632
00:26:35,039 --> 00:26:39,360
по фону э-э, а

633
00:26:39,360 --> 00:26:41,600
затем заинтересовался работой с

634
00:26:41,600 --> 00:26:44,559
речью, а затем с языковыми данными,

635
00:26:44,559 --> 00:26:47,919
так что в то время вещи, которые были такого

636
00:26:47,919 --> 00:26:50,640
рода экспоненциальными или использующими

637
00:26:50,640 --> 00:26:53,039
кросс-энтропийные потери, были полностью

638
00:26:53,039 --> 00:26:55,919
хлебом с маслом для Фреда Джелинека, но

639
00:26:55,919 --> 00:26:58,720
он  обнаружил, что никто из искусственного интеллекта не может

640
00:26:58,720 --> 00:27:01,520
понять нижнюю половину слайда

641
00:27:01,520 --> 00:27:03,039
, поэтому он решил придумать

642
00:27:03,039 --> 00:27:05,520
что-то простое, что люди в то

643
00:27:05,520 --> 00:27:08,640
время могли понять, а недоумение имеет

644
00:27:08,640 --> 00:27:10,799
своего рода простую

645
00:27:10,799 --> 00:27:13,840
интерпретацию, которую вы можете сказать людям, поэтому, если

646
00:27:13,840 --> 00:27:16,840
вы получаете недоумение

647
00:27:16,840 --> 00:27:21,039
53, это означает, что ваша

648
00:27:21,039 --> 00:27:23,760
неуверенность в следующем слове эквивалентна

649
00:27:23,760 --> 00:27:26,919
неуверенности в том, что вы ''  перебрасывает

650
00:27:26,919 --> 00:27:30,880
53-гранный кубик, и это становится

651
00:27:30,880 --> 00:27:34,559
правильным, так что это была легкая

652
00:27:34,559 --> 00:27:37,760
простая метрика, и поэтому он представил

653
00:27:37,760 --> 00:27:39,840
эту идею,

654
00:27:39,840 --> 00:27:42,880
но вы знаете, я думаю, что все остается, и

655
00:27:42,880 --> 00:27:46,480
по сей день все оценивают свои

656
00:27:46,480 --> 00:27:48,720
языковые модели  предоставляя числа недоумения,

657
00:27:48,720 --> 00:27:50,720
и вот некоторые числа недоумения,

658
00:27:50,720 --> 00:27:52,080


659
00:27:52,080 --> 00:27:55,039
так что традиционные модели языка инграмм

660
00:27:55,039 --> 00:27:58,080
обычно имели затруднения более сотни,

661
00:27:58,080 --> 00:27:59,919
но если вы сделали их действительно большими и

662
00:27:59,919 --> 00:28:02,399
очень осторожными, вы могли

663
00:28:02,399 --> 00:28:06,240
бы их уменьшить до числа, например, 67, поскольку

664
00:28:06,240 --> 00:28:09,039
люди начали строить больше  продвинутые

665
00:28:09,039 --> 00:28:12,159
рекуррентные нейронные сети, особенно по мере

666
00:28:12,159 --> 00:28:14,559
того, как они вышли за рамки простых

667
00:28:14,559 --> 00:28:16,880
rnn, которые я вам показал до сих пор,

668
00:28:16,880 --> 00:28:19,440
какая из них находится во второй

669
00:28:19,440 --> 00:28:21,720
строке сайта слайда  е в

670
00:28:21,720 --> 00:28:25,360
lstms, о которых я расскажу позже в этом

671
00:28:25,360 --> 00:28:28,480
курсе, что люди начали создавать

672
00:28:28,480 --> 00:28:31,360
гораздо более серьезные затруднения, а здесь мы

673
00:28:31,360 --> 00:28:34,640
получаем количество затруднений до 30, и

674
00:28:34,640 --> 00:28:37,520
это результат, фактически полученный несколько лет назад,

675
00:28:37,520 --> 00:28:39,679
так что в настоящее время люди получают затруднения

676
00:28:39,679 --> 00:28:42,720
даже ниже 30. um  вы должны быть

677
00:28:42,720 --> 00:28:44,960
реалистами в том, чего вы можете ожидать правильно,

678
00:28:44,960 --> 00:28:46,480
потому что, если вы просто обычно

679
00:28:46,480 --> 00:28:49,360
создаете текст, некоторые слова почти

680
00:28:49,360 --> 00:28:52,640
определены, так что вы знаете, если это что-

681
00:28:52,640 --> 00:28:55,440
то вроде,

682
00:28:55,440 --> 00:28:58,399
вы знаете, зум дал человеку салфетку, которую он

683
00:28:58,399 --> 00:28:59,679
сказал, спасибо

684
00:28:59,679 --> 00:29:01,520
, в основном  на сто процентов вы

685
00:29:01,520 --> 00:29:02,880
должны быть в состоянии сказать следующее слово

686
00:29:02,880 --> 00:29:06,080
- это вы, э-э, и чтобы вы могли

687
00:29:06,080 --> 00:29:09,279
очень хорошо предсказать, но э-э, вы знаете, если

688
00:29:09,279 --> 00:29:12,000
это много других предложений, например, э-э, он

689
00:29:12,000 --> 00:29:14,960
выглянул в окно и увидел

690
00:29:14,960 --> 00:29:17,200
что-то правильное без всякой вероятности в

691
00:29:17,200 --> 00:29:19,840
модельная модель в мире может дать очень

692
00:29:19,840 --> 00:29:21,360
хорошую оценку того, что на самом деле

693
00:29:21,360 --> 00:29:23,679
будет дальше от этой точки, и, таким образом,

694
00:29:23,679 --> 00:29:26,080
это дает нам своего рода остаточную

695
00:29:26,080 --> 00:29:28,240
неопределенность, которая приводит

696
00:29:28,240 --> 00:29:30,799
к затруднениям, которые  ярость может быть около 20

697
00:29:30,799 --> 00:29:33,279
или что-то в этом роде,

698
00:29:33,279 --> 00:29:34,159


699
00:29:34,159 --> 00:29:35,279


700
00:29:35,279 --> 00:29:37,520
так что мы много говорили о языковых

701
00:29:37,520 --> 00:29:40,399
моделях, почему мы должны заботиться о

702
00:29:40,399 --> 00:29:42,480
языковом моделировании, вы хорошо знаете, есть своего

703
00:29:42,480 --> 00:29:43,520
рода

704
00:29:43,520 --> 00:29:47,279
интеллектуальный научный ответ, который говорит, что

705
00:29:47,279 --> 00:29:50,080
это контрольная задача, если мы

706
00:29:50,080 --> 00:29:52,399
то, что хотим  нам предстоит создать

707
00:29:52,399 --> 00:29:54,799
модели машинного обучения языка и нашу

708
00:29:54,799 --> 00:29:56,559
способность предсказывать, какое слово будет

709
00:29:56,559 --> 00:29:59,600
следующим, в контексте, который показывает, насколько хорошо

710
00:29:59,600 --> 00:30:02,080
мы понимаем и структуру

711
00:30:02,080 --> 00:30:04,640
языка, и структуру человеческого

712
00:30:04,640 --> 00:30:05,840
мира,

713
00:30:05,840 --> 00:30:08,720
о котором говорит язык, но есть

714
00:30:08,720 --> 00:30:11,600
гораздо больше  практический ответ, чем тот эм,

715
00:30:11,600 --> 00:30:14,720
который, как вы знаете, языковые модели на

716
00:30:14,720 --> 00:30:17,039
самом деле являются секретным инструментом обработки естественного

717
00:30:17,039 --> 00:30:20,399
языка, поэтому, если вы

718
00:30:20,399 --> 00:30:24,720
разговариваете с любым человеком из НЛП и у вас есть

719
00:30:24,720 --> 00:30:27,360
почти любая

720
00:30:27,360 --> 00:30:30,399
задача, вполне вероятно, что они скажут: о, я

721
00:30:30,399 --> 00:30:32,240
уверен, мы могли бы  использовать для этого языковую модель

722
00:30:32,240 --> 00:30:35,360
, и поэтому языковые модели как бы

723
00:30:35,360 --> 00:30:38,960
используются не как полное решение, а как

724
00:30:38,960 --> 00:30:42,159
часть почти любой задачи, любой задачи, которая

725
00:30:42,159 --> 00:30:44,960
включает в себя создание или оценку

726
00:30:44,960 --> 00:30:47,360
вероятности te  xt, чтобы вы могли использовать его

727
00:30:47,360 --> 00:30:50,880
для прогнозирующего набора текста, исправления грамматики распознавания речи,

728
00:30:50,880 --> 00:30:53,600
определения авторов,

729
00:30:53,600 --> 00:30:55,760


730
00:30:55,760 --> 00:30:57,840
диалога резюмирования машинного перевода, практически все, что вы делаете с

731
00:30:57,840 --> 00:30:59,440
естественным языком, включает языковые

732
00:30:59,440 --> 00:31:02,640
модели, и мы увидим примеры этого

733
00:31:02,640 --> 00:31:04,799
на следующих занятиях, включая следующий

734
00:31:04,799 --> 00:31:06,720
вторник, где мы будем использовать язык

735
00:31:06,720 --> 00:31:09,919
модели для машинного перевода

736
00:31:09,919 --> 00:31:12,159
хорошо, поэтому языковая модель

737
00:31:12,159 --> 00:31:14,640
- это просто система, которая предсказывает следующее

738
00:31:14,640 --> 00:31:18,640
слово um рекуррентная нейронная сеть - это

739
00:31:18,640 --> 00:31:22,159
семейство нейронных сетей, которые могут

740
00:31:22,159 --> 00:31:25,120
принимать последовательные входные данные любой длины,

741
00:31:25,120 --> 00:31:28,240
они повторно используют одни и те же веса для

742
00:31:28,240 --> 00:31:30,799
генерации скрытого состояния и, возможно, но

743
00:31:30,799 --> 00:31:33,760
обычно вывод на каждом шаге,

744
00:31:33,760 --> 00:31:37,200
обратите внимание, что эти две вещи разные,

745
00:31:37,200 --> 00:31:38,640


746
00:31:38,640 --> 00:31:40,320
поэтому мы говорили о двух способах

747
00:31:40,320 --> 00:31:42,640
построения языковых моделей, но один из

748
00:31:42,640 --> 00:31:46,000
них - это отличный способ rnn, но rnns

749
00:31:46,000 --> 00:31:47,679
также можно использовать для многих других

750
00:31:47,679 --> 00:31:50,159
вещей, поэтому  позвольте мне быстро просмотреть

751
00:31:50,159 --> 00:31:53,279
несколько вещей, которые вы можете делать с помощью rnns,

752
00:31:53,279 --> 00:31:55,279
так что есть много задач, которые люди

753
00:31:55,279 --> 00:31:57,919
хотят выполнять в nlp, которые

754
00:31:57,919 --> 00:32:00,799
называются задачами маркировки последовательностей, когда мы

755
00:32:00,799 --> 00:32:04,320
хотели бы взять слова из текста и провести

756
00:32:04,320 --> 00:32:06,320
некоторую классификацию по

757
00:32:06,320 --> 00:32:09,840
последовательности, поэтому одна простая распространенная задача -

758
00:32:09,840 --> 00:32:12,480
дать словам части речи, которая является

759
00:32:12,480 --> 00:32:14,960
определяющим фактором, испугавшись, поскольку прилагательное кошка

760
00:32:14,960 --> 00:32:18,240
- это существительное  knocked как глагол um, и

761
00:32:18,240 --> 00:32:20,640
вы можете сделать это напрямую,

762
00:32:20,640 --> 00:32:24,320
используя текущую нейронную сеть в качестве

763
00:32:24,320 --> 00:32:26,799
последовательного классификатора, где теперь она

764
00:32:26,799 --> 00:32:29,360
будет генерировать части речи, а

765
00:32:29,360 --> 00:32:31,840
не следующее слово,

766
00:32:31,840 --> 00:32:33,919
вы можете использовать рекуррентную нейронную сеть,

767
00:32:33,919 --> 00:32:36,159
классификацию настроений, на

768
00:32:36,159 --> 00:32:37,120


769
00:32:37,120 --> 00:32:38,799
этот раз мы  на самом деле не хотим

770
00:32:38,799 --> 00:32:42,080
генерировать um и выводить каждое слово

771
00:32:42,080 --> 00:32:44,000
обязательно, но мы хотим знать, как

772
00:32:44,000 --> 00:32:46,799
выглядит общее настроение, поэтому каким-

773
00:32:46,799 --> 00:32:49,279
то образом мы хотим получить кодировку предложения,

774
00:32:49,279 --> 00:32:51,760
которую мы, возможно, можем пропустить через другой

775
00:32:51,760 --> 00:32:54,080
уровень нейронной сети, чтобы определить, является

776
00:32:54,080 --> 00:32:56,399
ли предложение  положительный

777
00:32:56,399 --> 00:32:59,440
или отрицательный, самый простой способ сделать

778
00:32:59,440 --> 00:33:01,519
это - хорошо подумать

779
00:33:01,519 --> 00:33:02,960
после того, как

780
00:33:02,960 --> 00:33:03,840
я

781
00:33:03,840 --> 00:33:05,200
пропустил свой

782
00:33:05,200 --> 00:33:06,720
lstm

783
00:33:06,720 --> 00:33:09,440
через все предложение, на самом деле это

784
00:33:09,440 --> 00:33:11,919
окончательное скрытое состояние, это  закодировал

785
00:33:11,919 --> 00:33:13,440
все предложение, потому что помните, я

786
00:33:13,440 --> 00:33:15,679
обновил это скрытое состояние на основе каждого

787
00:33:15,679 --> 00:33:18,320
предыдущего слова, и поэтому вы могли бы сказать, что

788
00:33:18,320 --> 00:33:19,679
это все значение

789
00:33:19,679 --> 00:33:22,480
предложения, поэтому давайте просто скажем, что это

790
00:33:22,480 --> 00:33:24,480
кодировка предложения,

791
00:33:24,480 --> 00:33:26,880
а затем поместите на него дополнительный

792
00:33:26,880 --> 00:33:28,960
слой классификатора с чем-то

793
00:33:28,960 --> 00:33:32,000
как классификатор softmax, этот

794
00:33:32,000 --> 00:33:34,000
метод использовался, и он действительно работает

795
00:33:34,000 --> 00:33:36,480
достаточно хорошо, и если вы как бы хорошо обучите

796
00:33:36,480 --> 00:33:38,720
эту модель от начала до конца, на самом деле это

797
00:33:38,720 --> 00:33:42,159
будет мотивировано для сохранения

798
00:33:42,159 --> 00:33:44,240
информации о настроениях в скрытом состоянии

799
00:33:44,240 --> 00:33:46,159
текущей нейронной сети, потому что это

800
00:33:46,159 --> 00:33:48,640
позволит ей  чтобы лучше предсказать

801
00:33:48,640 --> 00:33:50,399
тональность всего предложения,

802
00:33:50,399 --> 00:33:53,440
которое является последней задачей и, следовательно, функцией потерь,

803
00:33:53,440 --> 00:33:55,600
которую мы передаем сети,

804
00:33:55,600 --> 00:33:57,760
но оказывается, что вы обычно можете

805
00:33:57,760 --> 00:34:00,399
добиться большего, фактически выполняя

806
00:34:00,399 --> 00:34:03,519
такие вещи, как ввод всех скрытых состояний

807
00:34:03,519 --> 00:34:05,919
в предложение и  кодирование, возможно, путем

808
00:34:05,919 --> 00:34:08,639
создания предложения,

809
00:34:08,639 --> 00:34:11,440
кодирующего поэлементный максимум или

810
00:34:11,440 --> 00:34:14,399
поэлементное среднее всех скрытых состояний,

811
00:34:14,399 --> 00:34:17,040
потому что это больше s  ymmetrically

812
00:34:17,040 --> 00:34:18,399


813
00:34:18,399 --> 00:34:22,800
кодирует скрытое состояние на каждом временном шаге,

814
00:34:24,639 --> 00:34:26,719
еще одно большое использование рекуррентных нейронных

815
00:34:26,719 --> 00:34:28,879
сетей - это

816
00:34:28,879 --> 00:34:32,000
то, что я назову модулем языкового кодирования,

817
00:34:32,000 --> 00:34:36,000
поэтому каждый раз, когда у вас есть

818
00:34:36,000 --> 00:34:38,079
текст, например, здесь, у нас есть вопрос

819
00:34:38,079 --> 00:34:41,599
о том, какой национальности был Бетховен,

820
00:34:41,599 --> 00:34:44,239
мы бы хотели построить  какое-то

821
00:34:44,239 --> 00:34:47,199
более новое представление этого, поэтому один из способов

822
00:34:47,199 --> 00:34:50,480
сделать это -

823
00:34:50,480 --> 00:34:52,800
запустить текущую нейронную сеть поверх нее, а

824
00:34:52,800 --> 00:34:55,199
затем, как и в прошлый раз

825
00:34:55,199 --> 00:34:57,520
, либо принять окончательное скрытое состояние, либо

826
00:34:57,520 --> 00:34:59,599
взять какую-то

827
00:34:59,599 --> 00:35:01,839
функцию всех скрытых состояний и

828
00:35:01,839 --> 00:35:05,040
сказать, что  представление предложения,

829
00:35:05,040 --> 00:35:07,200
и мы могли бы сделать то же самое

830
00:35:07,200 --> 00:35:09,760
для контекста, поэтому для

831
00:35:09,760 --> 00:35:11,440
ответа на вопрос мы собираемся построить дополнительную

832
00:35:11,440 --> 00:35:13,119
структуру нейронной

833
00:35:13,119 --> 00:35:14,880
сети поверх этого, и мы узнаем больше

834
00:35:14,880 --> 00:35:17,520
об этом через пару недель, когда

835
00:35:17,520 --> 00:35:19,680
у нас будет вопрос  отвечая на лекцию,

836
00:35:19,680 --> 00:35:20,400


837
00:35:20,400 --> 00:35:23,280
но ключевым моментом является то, что мы построили до сих пор, мы

838
00:35:23,280 --> 00:35:26,079
используем для получения представления предложения, поэтому

839
00:35:26,079 --> 00:35:28,720
это модуль языкового кодирования,

840
00:35:28,720 --> 00:35:31,520
так что это была часть языкового кодирования,

841
00:35:31,520 --> 00:35:35,680
мы также можем использовать rnns для декодирования i  на

842
00:35:35,680 --> 00:35:38,079
язык, и это обычно используется в

843
00:35:38,079 --> 00:35:40,640


844
00:35:40,640 --> 00:35:43,040
обобщении машинного перевода для распознавания речи, поэтому, если у нас есть распознаватель речи,

845
00:35:43,040 --> 00:35:46,480
входом является аудиосигнал,

846
00:35:46,480 --> 00:35:48,880
и мы хотим хорошо декодировать его

847
00:35:48,880 --> 00:35:51,760
на язык, что мы могли бы сделать, это

848
00:35:51,760 --> 00:35:54,560
использовать некоторую функцию ввода, которая является

849
00:35:54,560 --> 00:35:56,800
вероятно, сама будет нейронной сетью

850
00:35:56,800 --> 00:35:58,560
в качестве

851
00:35:58,560 --> 00:36:00,400
начального

852
00:36:00,400 --> 00:36:02,960
скрытого состояния

853
00:36:02,960 --> 00:36:04,880
нашего rnn lm,

854
00:36:04,880 --> 00:36:08,320
а затем мы говорим начать генерацию текста

855
00:36:08,320 --> 00:36:11,520
на основе этого, и поэтому мы должны затем

856
00:36:11,520 --> 00:36:13,760
генерировать слово за раз с помощью метода,

857
00:36:13,760 --> 00:36:15,359
который мы только что рассмотрели,

858
00:36:15,359 --> 00:36:18,079
мы превращаем речь в  текст,

859
00:36:18,079 --> 00:36:20,320
так что это пример модели условного

860
00:36:20,320 --> 00:36:22,160
языка, потому что теперь мы

861
00:36:22,160 --> 00:36:24,640
генерируем текст, обусловленный

862
00:36:24,640 --> 00:36:28,240
речевым сигналом, и большую часть времени вы

863
00:36:28,240 --> 00:36:29,119
можете делать

864
00:36:29,119 --> 00:36:31,040
интересные более сложные вещи с

865
00:36:31,040 --> 00:36:33,599
рекуррентными нейронными сетями, создавая

866
00:36:33,599 --> 00:36:36,560
модели условного языка в другом

867
00:36:36,560 --> 00:36:38,400
месте, которое вы можете использовать  условные языковые

868
00:36:38,400 --> 00:36:41,920
модели предназначены для задач классификации текста

869
00:36:41,920 --> 00:36:44,720
и включают классификацию тональности,

870
00:36:44,720 --> 00:36:47,520
поэтому, если вы можете обусловить

871
00:36:47,520 --> 00:36:48,640
свою

872
00:36:48,640 --> 00:36:50,560
языковую модель на основе своего рода

873
00:36:50,560 --> 00:36:52,079
настроения, вы можете создать своего рода

874
00:36:52,079 --> 00:36:54,400
классификатор для этого и другого использования, которое

875
00:36:54,400 --> 00:36:56,480
мы увидим много в следующем классе, для

876
00:36:56,480 --> 00:36:59,280
машинного перевода,

877
00:36:59,280 --> 00:37:00,160
хорошо,

878
00:37:00,160 --> 00:37:03,040
так что это конец вступления

879
00:37:03,040 --> 00:37:05,280
к работе

880
00:37:06,079 --> 00:37:07,760
с

881
00:37:07,760 --> 00:37:09,520
повторяющимися нейронными сетями и языковыми

882
00:37:09,520 --> 00:37:13,200
моделями, теперь я хочу переместить  и расскажем

883
00:37:13,200 --> 00:37:15,920
вам о том факте, что все

884
00:37:15,920 --> 00:37:18,960
не идеально, и у этих повторяющихся нейронных

885
00:37:18,960 --> 00:37:21,359
сетей, как правило, есть несколько

886
00:37:21,359 --> 00:37:22,560
проблем,

887
00:37:22,560 --> 00:37:25,760
и мы поговорим о них, а затем

888
00:37:25,760 --> 00:37:28,079
частично, что затем будет мотивировать

889
00:37:28,079 --> 00:37:29,680
придумывать более продвинутую архитектуру рекуррентной нейронной

890
00:37:29,680 --> 00:37:31,599
сети.

891
00:37:31,599 --> 00:37:34,800
Итак, первая проблема, которую следует упомянуть, -

892
00:37:34,800 --> 00:37:37,680
это идея того, что называется исчезающими

893
00:37:37,680 --> 00:37:38,960
градиентами,

894
00:37:38,960 --> 00:37:42,000
и что это значит. Что ж, в

895
00:37:42,000 --> 00:37:46,079
конце нашей последовательности мы имеем некоторую общую

896
00:37:46,079 --> 00:37:49,040
потерю, которую мы вычисляем, и

897
00:37:49,040 --> 00:37:51,920
что мы хотим сделать, это распространить

898
00:37:51,920 --> 00:37:53,760
эту потерю в обратном направлении

899
00:37:53,760 --> 00:37:55,920
и  мы хотим распространить его в обратном направлении прямо

900
00:37:55,920 --> 00:37:58,480
по последовательности, поэтому мы

901
00:37:58,480 --> 00:38:02,240
прорабатываем частичные значения j4

902
00:38:02,240 --> 00:38:04,480
относительно скрытого состояния во время

903
00:38:04,480 --> 00:38:06,560
один, и когда у нас будет более длинная последовательность,

904
00:38:06,560 --> 00:38:09,440
мы будем работать над  но частичные значения j20

905
00:38:09,440 --> 00:38:11,760
относительно скрытого состояния в момент времени

906
00:38:11,760 --> 00:38:13,200
один, и

907
00:38:13,200 --> 00:38:17,200
как мы это делаем хорошо, как мы это делаем, так это

908
00:38:17,200 --> 00:38:20,079
по композиции и правилу цепочки, у нас

909
00:38:20,079 --> 00:38:23,040
есть большая длинная цепочка, прокручивающаяся по

910
00:38:23,040 --> 00:38:24,800
всей последовательности,

911
00:38:24,800 --> 00:38:25,760
ну

912
00:38:25,760 --> 00:38:28,800
хорошо, если мы '  Делая это,

913
00:38:28,800 --> 00:38:31,520
вы знаете, что мы умножаем тонны

914
00:38:31,520 --> 00:38:33,680
вещей вместе,

915
00:38:33,680 --> 00:38:36,560
и поэтому опасность того, что имеет тенденцию

916
00:38:36,560 --> 00:38:37,599
происходить,

917
00:38:37,599 --> 00:38:40,200
состоит в том, что по мере того, как мы делаем

918
00:38:40,200 --> 00:38:43,040
это умножение,

919
00:38:43,040 --> 00:38:45,760
эти частичные

920
00:38:45,760 --> 00:38:49,119
последовательные скрытые состояния становятся небольшими,

921
00:38:49,119 --> 00:38:52,160
и поэтому происходит то, что мы идем дальше

922
00:38:52,160 --> 00:38:54,800
градиент становится все меньше и меньше

923
00:38:54,800 --> 00:38:58,720
и меньше и начинает исчезать,

924
00:38:58,720 --> 00:39:01,920
и в той степени, в которой он исчезает

925
00:39:01,920 --> 00:39:04,320
хорошо, у нас как бы нет восходящего

926
00:39:04,320 --> 00:39:06,560
градиента, и поэтому мы вообще не будем

927
00:39:06,560 --> 00:39:09,520
изменять параметры, и это

928
00:39:09,520 --> 00:39:12,839
оказывается  довольно проблематично,

929
00:39:12,839 --> 00:39:14,800


930
00:39:14,800 --> 00:39:17,760
так что следующая пара слайдов как

931
00:39:17,760 --> 00:39:22,320
бы немного расскажет о том, почему и как

932
00:39:22,320 --> 00:39:24,320
это происходит,

933
00:39:24,320 --> 00:39:27,440
то, что представлено здесь, является своего рода лишь

934
00:39:27,440 --> 00:39:29,119
полуформальным

935
00:39:29,119 --> 00:39:30,240


936
00:39:30,240 --> 00:39:32,320
взмахом руки над проблемами

937
00:39:32,320 --> 00:39:34,960
, которых вы могли бы ожидать, если действительно

938
00:39:34,960 --> 00:39:37,359
хотите  так  Чтобы вникнуть во все

939
00:39:37,359 --> 00:39:39,280
детали этого эм, вы должны взглянуть на

940
00:39:39,280 --> 00:39:41,599
пару статей, которые упомянуты мелким

941
00:39:41,599 --> 00:39:44,400
шрифтом внизу слайда, но в

942
00:39:44,400 --> 00:39:46,079
любом случае, если вы помните, что это

943
00:39:46,079 --> 00:39:49,040
наше основное уравнение рекуррентной нейронной сети,

944
00:39:49,040 --> 00:39:51,599
давайте рассмотрим  простой

945
00:39:51,599 --> 00:39:55,920
случай, предположим, что мы как бы избавляемся от нашей

946
00:39:55,920 --> 00:39:58,720
нелинейности и просто предполагаем, что это

947
00:39:58,720 --> 00:40:00,640
функция

948
00:40:00,640 --> 00:40:02,960
идентичности, и тогда, когда мы

949
00:40:02,960 --> 00:40:04,720
прорабатываем части скрытого состояния по

950
00:40:04,720 --> 00:40:07,680
отношению к предыдущему скрытому состоянию,

951
00:40:07,680 --> 00:40:10,800
мы можем обработать их в  обычным способом в

952
00:40:10,800 --> 00:40:13,119
соответствии с правилом цепочки,

953
00:40:13,119 --> 00:40:15,440
а затем, если

954
00:40:15,440 --> 00:40:16,480
сигма

955
00:40:16,480 --> 00:40:17,920
- это

956
00:40:17,920 --> 00:40:20,880
просто функция идентичности,

957
00:40:20,880 --> 00:40:22,800
тогда все становится

958
00:40:22,800 --> 00:40:24,400
для нас действительно легко, поэтому

959
00:40:24,400 --> 00:40:25,920


960
00:40:25,920 --> 00:40:28,960
только сигма просто уходит, и только

961
00:40:28,960 --> 00:40:30,319
первый член

962
00:40:30,319 --> 00:40:34,160
включает um h в момент времени t минус один, поэтому более

963
00:40:34,160 --> 00:40:37,680
поздние члены  уйти, и поэтому наш

964
00:40:37,680 --> 00:40:40,960
градиент заканчивается так,

965
00:40:40,960 --> 00:40:43,599
что он делает это только для одного временного

966
00:40:43,599 --> 00:40:45,920
шага, что происходит, когда вы хотите

967
00:40:45,920 --> 00:40:48,560
проработать эти частичные шаги несколько раз,

968
00:40:48,560 --> 00:40:50,319
так что

969
00:40:50,319 --> 00:40:53,920
мы хотим отработать его часть

970
00:40:53,920 --> 00:40:57,760
времени  шаг i в отношении j

971
00:40:57,760 --> 00:40:59,040
um

972
00:40:59,040 --> 00:41:02,880
хорошо то, что мы в итоге получаем, является

973
00:41:02,880 --> 00:41:06,880
продуктом частичных последовательных временных шагов,

974
00:41:06,880 --> 00:41:10,400
и хотя каждый из

975
00:41:10,400 --> 00:41:15,520
них выходит как wh, и поэтому мы в конечном итоге

976
00:41:15,520 --> 00:41:21,280
получаем wh возведены в силу эльфов

977
00:41:21,280 --> 00:41:24,240
и, что ж, наша потенциальная проблема  состоит в том, что

978
00:41:24,240 --> 00:41:28,400
если wh в некотором смысле мало, то этот

979
00:41:28,400 --> 00:41:31,040
член становится экспоненциально проблематичным, и он

980
00:41:31,040 --> 00:41:33,839
становится исчезающе малым, поскольку

981
00:41:33,839 --> 00:41:36,480
длина нашей последовательности становится большой,

982
00:41:36,480 --> 00:41:39,920
что мы можем понимать под маленьким колодцем,

983
00:41:39,920 --> 00:41:41,839
матрица мала,

984
00:41:41,839 --> 00:41:44,960
если все ее собственные значения меньше единицы,

985
00:41:44,960 --> 00:41:47,680
поэтому мы можем переписать  что происходит с

986
00:41:47,680 --> 00:41:50,319
этим успехом умножения с использованием

987
00:41:50,319 --> 00:41:53,040
собственных значений и собственных векторов

988
00:41:53,040 --> 00:41:55,520
um и я должен сказать, что все, что я могу

989
00:41:55,520 --> 00:41:57,599
векторные значения меньше единицы, является

990
00:41:57,599 --> 00:41:59,520
достаточным, но не необходимым условием

991
00:41:59,520 --> 00:42:02,720
для того, что я собираюсь сказать um right, поэтому мы

992
00:42:02,720 --> 00:42:04,319
можем переписать

993
00:42:04,319 --> 00:42:08,160
вещи, используя собственные векторы  в качестве основы,

994
00:42:08,160 --> 00:42:11,280
и если мы это сделаем,

995
00:42:11,280 --> 00:42:13,440
мы получим

996
00:42:13,440 --> 00:42:14,440


997
00:42:14,440 --> 00:42:16,880
собственные значения, возведенные в восьмую

998
00:42:16,880 --> 00:42:20,960
степень, и поэтому, если все наши собственные

999
00:42:20,960 --> 00:42:22,960
значения меньше единицы, если мы берем

1000
00:42:22,960 --> 00:42:25,200
число меньше единицы, а затем  возведя его

1001
00:42:25,200 --> 00:42:27,119
в восьмую степень, которая будет

1002
00:42:27,119 --> 00:42:30,160
приближаться к нулю по мере увеличения длины последовательности,

1003
00:42:30,160 --> 00:42:33,680
и поэтому градиент исчезает,

1004
00:42:33,680 --> 00:42:36,160
хорошо, теперь реальность более сложна,

1005
00:42:36,160 --> 00:42:38,319
чем это, потому что на самом деле мы всегда

1006
00:42:38,319 --> 00:42:40,880
используем нелинейную сигму активации, но вы

1007
00:42:40,880 --> 00:42:43,520
знаете, что в принципе это своего рода

1008
00:42:43,520 --> 00:42:46,480
То же самое, кроме того, что мы должны

1009
00:42:46,480 --> 00:42:48,400
учитывать влияние нелинейной

1010
00:42:48,400 --> 00:42:50,960
активации,

1011
00:42:50,960 --> 00:42:53,920
хорошо, так почему это проблема, что

1012
00:42:53,920 --> 00:42:55,680
градиенты хорошо исчезают?

1013
00:42:55,680 --> 00:42:56,640


1014
00:42:56,640 --> 00:42:58,480
Предположим, мы хотим посмотреть на

1015
00:42:58,480 --> 00:43:01,359
влияние временных шагов в

1016
00:43:01,359 --> 00:43:02,560
будущем

1017
00:43:02,560 --> 00:43:03,760


1018
00:43:03,760 --> 00:43:06,319
на представления  мы хотим, чтобы в

1019
00:43:06,319 --> 00:43:10,000
начале предложения было хорошо

1020
00:43:10,000 --> 00:43:12,480
то, что происходит в конце предложения,

1021
00:43:12,480 --> 00:43:14,400
просто не будет давать много

1022
00:43:14,400 --> 00:43:17,920
информации о том, что мы должны

1023
00:43:17,920 --> 00:43:21,680
хранить в векторе h в момент времени 1,

1024
00:43:21,680 --> 00:43:24,400
тогда как, с другой стороны, потеря на

1025
00:43:24,400 --> 00:43:26,800
временном шаге два равна  будет давать

1026
00:43:26,800 --> 00:43:29,760
много информации о том, что

1027
00:43:29,760 --> 00:43:31,760
должно храниться в скрытом векторе на

1028
00:43:31,760 --> 00:43:35,520


1029
00:43:35,520 --> 00:43:39,599


1030
00:43:39,599 --> 00:43:44,079
первом временном шаге, поэтому конечный результат заключается в том, что эти простые rnn очень хороши при моделировании вблизи  y,

1031
00:43:44,079 --> 00:43:45,200


1032
00:43:45,200 --> 00:43:48,000
но они совсем не годятся для моделирования

1033
00:43:48,000 --> 00:43:50,800
долгосрочных эффектов, потому что сигнал градиента

1034
00:43:50,800 --> 00:43:53,760
издалека просто теряется слишком

1035
00:43:53,760 --> 00:43:54,480
сильно,

1036
00:43:54,480 --> 00:43:57,280
и поэтому модель никогда не сможет

1037
00:43:57,280 --> 00:43:59,920
эффективно узнать,

1038
00:43:59,920 --> 00:44:03,040
какую информацию издалека было

1039
00:44:03,040 --> 00:44:05,920
бы полезно сохранить в  Будущее,

1040
00:44:05,920 --> 00:44:08,640
так что давайте рассмотрим это конкретно на

1041
00:44:08,640 --> 00:44:10,000


1042
00:44:10,000 --> 00:44:12,720
примере языковых моделей, над которыми мы работали,

1043
00:44:12,720 --> 00:44:16,160
так что вот фрагмент текста ммм, когда она

1044
00:44:16,160 --> 00:44:18,000
пыталась распечатать свои билеты, она обнаружила,

1045
00:44:18,000 --> 00:44:20,480
что в принтере закончился тонер, она

1046
00:44:20,480 --> 00:44:22,560
пошла в магазин канцелярских товаров, чтобы купить  больше

1047
00:44:22,560 --> 00:44:25,440
тонера цена на него была очень завышена после

1048
00:44:25,440 --> 00:44:27,200
установки тонера в принтер,

1049
00:44:27,200 --> 00:44:29,440
она наконец напечатала ее

1050
00:44:29,440 --> 00:44:32,240
, и вы все умные люди, я

1051
00:44:32,240 --> 00:44:34,480
надеюсь, вы все можете догадаться, какое

1052
00:44:34,480 --> 00:44:38,319
слово будет дальше, это должны быть билеты,

1053
00:44:38,319 --> 00:44:41,760
но проблема в том, что для  rnn,

1054
00:44:41,760 --> 00:44:44,720
чтобы начать изучать такие случаи, ему

1055
00:44:44,720 --> 00:44:46,880
пришлось бы в своем

1056
00:44:46,880 --> 00:44:49,920
скрытом состоянии передать память словарных

1057
00:44:49,920 --> 00:44:52,800
билетов для чего-то вроде того, что это около

1058
00:44:52,800 --> 00:44:55,680
30 скрытых обновлений состояния,

1059
00:44:55,680 --> 00:44:56,720
и

1060
00:44:56,720 --> 00:45:00,400
мы будем тренироваться на этом  um пример, и поэтому

1061
00:45:00,400 --> 00:45:03,440
мы захотим, чтобы он предсказывал, что билеты

1062
00:45:03,440 --> 00:45:06,000
- это следующее слово, и поэтому обновление градиента

1063
00:45:06,000 --> 00:45:08,400
будет отправлено обратно

1064
00:45:08,400 --> 00:45:11,440
через скрытые состояния lstm,

1065
00:45:11,440 --> 00:45:13,920
соответствующие этому предложению,

1066
00:45:13,920 --> 00:45:15,280
и

1067
00:45:15,280 --> 00:45:17,680
это должно сообщить модели,

1068
00:45:17,680 --> 00:45:19,680
что хорошо сохранять информацию о

1069
00:45:19,680 --> 00:45:21,359
тикеры слов, потому что это может быть

1070
00:45:21,359 --> 00:45:23,680
полезно в будущем, здесь это было полезно

1071
00:45:23,680 --> 00:45:25,280
в будущем,

1072
00:45:25,280 --> 00:45:27,119
но проблема в том, что

1073
00:45:27,119 --> 00:45:29,839
сигнал градиента станет

1074
00:45:29,839 --> 00:45:33,200
слишком слабым после набора слов,

1075
00:45:33,200 --> 00:45:36,640
и он просто никогда не узнает эту зависимость

1076
00:45:36,640 --> 00:45:40,160
и, следовательно, то, что мы  найти на практике,

1077
00:45:40,160 --> 00:45:42,560
модель просто неспособна предсказать аналогичные

1078
00:45:42,560 --> 00:45:45,520
зависимости на большом расстоянии во время тестирования

1079
00:45:45,520 --> 00:45:48,240
Я потратил довольно много времени на

1080
00:45:48,240 --> 00:45:50,240
исчезающие градиенты, а затем действительно

1081
00:45:50,240 --> 00:45:53,359
исчезающие градиенты являются большой проблемой

1082
00:45:53,359 --> 00:45:57,839
на практике с использованием

1083
00:45:57,839 --> 00:45:59,680
рекуррентных нейронных сетей на длинных

1084
00:45:59,680 --> 00:46:01,440
последовательностях

1085
00:46:01,440 --> 00:46:03,920
но вы знаете, что я должен отдать должное тому

1086
00:46:03,920 --> 00:46:05,359
факту, что на самом деле у вас может быть и

1087
00:46:05,359 --> 00:46:07,599
противоположная проблема, у вас также могут быть

1088
00:46:07,599 --> 00:46:09,520
взрывающиеся градиенты,

1089
00:46:09,520 --> 00:46:10,319
поэтому,

1090
00:46:10,319 --> 00:46:14,240
если градиент становится слишком би  g, это

1091
00:46:14,240 --> 00:46:17,280
также проблема, и это проблема,

1092
00:46:17,280 --> 00:46:20,079
потому что

1093
00:46:20,079 --> 00:46:23,200
шаг обновления стохастического градиента становится слишком большим, так

1094
00:46:23,200 --> 00:46:26,880
что помните, что наше обновление параметров

1095
00:46:26,880 --> 00:46:29,359
основано на произведении скорости обучения

1096
00:46:29,359 --> 00:46:31,440
и градиента, поэтому, если ваш

1097
00:46:31,440 --> 00:46:34,079
градиент огромен, вы правильно рассчитали,

1098
00:46:34,079 --> 00:46:36,319
о, это  здесь большой уклон, это

1099
00:46:36,319 --> 00:46:38,960
имеет уклон 10000,

1100
00:46:38,960 --> 00:46:40,880
тогда ваше

1101
00:46:40,880 --> 00:46:43,680
обновление параметров может быть произвольно

1102
00:46:43,680 --> 00:46:45,440
большим,

1103
00:46:45,440 --> 00:46:47,760
и это потенциально проблематично, что

1104
00:46:47,760 --> 00:46:50,000
может вызвать плохое обновление, когда вы делаете

1105
00:46:50,000 --> 00:46:53,839
огромный шаг, и вы в конечном итоге получаете странную и

1106
00:46:53,839 --> 00:46:54,720
плохую

1107
00:46:54,720 --> 00:46:57,119
конфигурацию параметров, поэтому вы  Вы вроде как

1108
00:46:57,119 --> 00:46:59,040
думаете, что вы подходите

1109
00:46:59,040 --> 00:47:01,599
к крутому холму, чтобы подняться, и вы

1110
00:47:01,599 --> 00:47:03,680
хотите подняться на холм с высокой

1111
00:47:03,680 --> 00:47:05,920
вероятностью, но на самом деле градиент

1112
00:47:05,920 --> 00:47:10,640
настолько крутой, что вы делаете огромное

1113
00:47:10,640 --> 00:47:12,880
обновление, а затем внезапно ваши

1114
00:47:12,880 --> 00:47:14,960
параметры заканчиваются  Айова, и вы вообще потеряли свой

1115
00:47:14,960 --> 00:47:16,880
холм, есть также

1116
00:47:16,880 --> 00:47:18,560
практическая трудность, заключающаяся в том, что у нас сейчас только

1117
00:47:18,560 --> 00:47:20,559
такое большое разрешение чисел с плавающей запятой,

1118
00:47:20,559 --> 00:47:21,680


1119
00:47:21,680 --> 00:47:22,640
поэтому,

1120
00:47:22,640 --> 00:47:25,359
если ваш градиент становится слишком крутым, вы

1121
00:47:25,359 --> 00:47:26,720
начинаете получать  Не говоря уже

1122
00:47:26,720 --> 00:47:29,040
о числах в ваших расчетах, которые

1123
00:47:29,040 --> 00:47:31,920
разрушают всю вашу тяжелую тренировочную работу,

1124
00:47:31,920 --> 00:47:35,280
мы используем своего рода простое решение,

1125
00:47:35,280 --> 00:47:38,079
которое называется градиентной обрезкой, то

1126
00:47:38,079 --> 00:47:41,599
есть мы выбираем какое-то разумное

1127
00:47:41,599 --> 00:47:44,160
число и говорим, что просто не собираемся

1128
00:47:44,160 --> 00:47:45,760
иметь дело с градиентами  которые больше,

1129
00:47:45,760 --> 00:47:47,839
чем это число.

1130
00:47:47,839 --> 00:47:50,400


1131
00:47:50,400 --> 00:47:52,160


1132
00:47:52,160 --> 00:47:54,000


1133
00:47:54,000 --> 00:47:58,319


1134
00:47:58,319 --> 00:48:01,040
градиент

1135
00:48:01,040 --> 00:48:03,599
больше, чем этот порог, мы

1136
00:48:03,599 --> 00:48:06,400
просто уменьшаем его, что означает

1137
00:48:06,400 --> 00:48:09,200
, что затем мы делаем меньшее обновление градиента,

1138
00:48:09,200 --> 00:48:12,400
поэтому мы все еще движемся в

1139
00:48:12,400 --> 00:48:13,680
том же

1140
00:48:13,680 --> 00:48:16,079
направлении, но мы делаем меньший

1141
00:48:16,079 --> 00:48:17,839
шаг,

1142
00:48:17,839 --> 00:48:19,839
поэтому выполнение этого градиентного отсечения

1143
00:48:19,839 --> 00:48:21,920
важно, как

1144
00:48:21,920 --> 00:48:23,760
вы знаете  но эту проблему легко

1145
00:48:23,760 --> 00:48:26,079
решить,

1146
00:48:27,040 --> 00:48:30,160
ладно, эм, поэтому

1147
00:48:30,160 --> 00:48:34,720
нам еще предстоит решить, как на самом деле решить

1148
00:48:34,720 --> 00:48:38,240
эту проблему исчезающих градиентов.

1149
00:48:38,240 --> 00:48:39,280


1150
00:48:39,280 --> 00:48:43,040


1151
00:48:43,040 --> 00:48:45,680


1152
00:48:45,680 --> 00:48:47,280
s,

1153
00:48:47,280 --> 00:48:48,800
и

1154
00:48:48,800 --> 00:48:51,920
один из способов подумать об этом интуитивно

1155
00:48:51,920 --> 00:48:55,280
: на каждом временном шаге у нас есть скрытое

1156
00:48:55,280 --> 00:48:56,400
состояние,

1157
00:48:56,400 --> 00:49:00,160
и скрытое состояние полностью

1158
00:49:00,160 --> 00:49:03,520
изменяется на каждом временном шаге, и оно

1159
00:49:03,520 --> 00:49:06,800
изменяется мультипликативным образом

1160
00:49:06,800 --> 00:49:09,200
путем умножения на wh, а затем

1161
00:49:09,200 --> 00:49:10,079
через

1162
00:49:10,079 --> 00:49:11,200
um

1163
00:49:11,200 --> 00:49:13,599
и  нелинейность,

1164
00:49:13,599 --> 00:49:16,000
например, мы могли бы добиться большего

1165
00:49:16,000 --> 00:49:17,839
прогресса,

1166
00:49:17,839 --> 00:49:20,960
если бы могли более гибко

1167
00:49:20,960 --> 00:49:24,319
поддерживать память в нашей рекуррентной

1168
00:49:24,319 --> 00:49:25,920
нейронной сети,

1169
00:49:25,920 --> 00:49:27,319
которой мы могли бы

1170
00:49:27,319 --> 00:49:30,800
манипулировать более гибко,

1171
00:49:30,800 --> 00:49:33,440
что позволяет нам легче сохранять

1172
00:49:33,440 --> 00:49:34,800
информацию,

1173
00:49:34,800 --> 00:49:36,480
и поэтому эту

1174
00:49:36,480 --> 00:49:39,599
идею начали  думали,

1175
00:49:39,599 --> 00:49:41,440
и на самом деле они начали думать об

1176
00:49:41,440 --> 00:49:43,680
этом очень давно,

1177
00:49:43,680 --> 00:49:46,290
в конце 1990-х

1178
00:49:46,290 --> 00:49:48,559
[Музыка],

1179
00:49:48,559 --> 00:49:51,040
и Хак Райт и Шмидт Гувер

1180
00:49:51,040 --> 00:49:52,880
придумали эту идею,

1181
00:49:52,880 --> 00:49:55,920
которую назвали долгосрочной краткосрочной памятью

1182
00:49:55,920 --> 00:49:57,359


1183
00:49:57,359 --> 00:49:59,440
как решение проблемы

1184
00:49:59,440 --> 00:50:01,119
исчезающих градиентов,

1185
00:50:01,119 --> 00:50:04,640
я имею в виду  так что эта статья 1997 года - это статья, которую

1186
00:50:04,640 --> 00:50:07,680
вы всегда видите процитированной для lstms, но вы

1187
00:50:07,680 --> 00:50:09,440
знаете, что на самом деле

1188
00:50:09,440 --> 00:50:11,680
с точки зрения того, что мы теперь понимаем как

1189
00:50:11,680 --> 00:50:13,200
lstm, в

1190
00:50:13,200 --> 00:50:16,240
ней отсутствовала часть  Фактически

1191
00:50:16,240 --> 00:50:18,319
отсутствует то, что в ретроспективе

1192
00:50:18,319 --> 00:50:21,520
оказалось самой важной частью

1193
00:50:21,520 --> 00:50:22,480


1194
00:50:22,480 --> 00:50:25,920
современного lstm, так что в некотором смысле

1195
00:50:25,920 --> 00:50:29,359
настоящая бумага, из-за которой появился современный lstm,

1196
00:50:29,359 --> 00:50:32,960
- это немного более поздняя статья Герша,

1197
00:50:32,960 --> 00:50:34,880
все еще Schmidt hoover and cummins from

1198
00:50:34,880 --> 00:50:37,680
2000 um, которая  дополнительно вводит

1199
00:50:37,680 --> 00:50:40,000
ворота забывания, которые я объясню через

1200
00:50:40,000 --> 00:50:42,400
минуту,

1201
00:50:42,400 --> 00:50:45,839
да, так что это был очень умный

1202
00:50:45,839 --> 00:50:49,359
материал, который был представлен, и позже выяснилось

1203
00:50:49,359 --> 00:50:52,720
, что он окажет огромное влияние,

1204
00:50:52,720 --> 00:50:54,319
если я просто

1205
00:50:54,319 --> 00:50:56,559
отклонюсь от технической части еще на

1206
00:50:56,559 --> 00:50:58,400
один момент

1207
00:50:58,400 --> 00:51:00,880
это вы знаете для тех из вас, кто в наши

1208
00:51:00,880 --> 00:51:04,319
дни думает, что освоение ваших

1209
00:51:04,319 --> 00:51:08,640
сетей - это путь к славе и богатству,

1210
00:51:08,640 --> 00:51:11,119
что забавно, вы знаете, что в то время

1211
00:51:11,119 --> 00:51:12,960
, когда эта работа была сделана

1212
00:51:12,960 --> 00:51:16,559
, это было неправдой.

1213
00:51:16,559 --> 00:51:18,720


1214
00:51:18,720 --> 00:51:22,240
сетей, и хотя долгосрочные краткосрочные

1215
00:51:22,240 --> 00:51:24,079
воспоминания оказались одной

1216
00:51:24,079 --> 00:51:26,079
из самых важных успешных и

1217
00:51:26,079 --> 00:51:29,359
влиятельных идей в нейронных сетях на

1218
00:51:29,359 --> 00:51:30,240


1219
00:51:30,240 --> 00:51:32,960
последующие 25 лет, на

1220
00:51:32,960 --> 00:51:35,520
самом деле оригинальные авторы  не получил

1221
00:51:35,520 --> 00:51:38,079
признания за это, поэтому они оба

1222
00:51:38,079 --> 00:51:41,280
теперь профессора в немецких университетах,

1223
00:51:41,280 --> 00:51:43,599
но ястреб-писатель

1224
00:51:43,599 --> 00:51:46,240
занялся биоинформатикой,

1225
00:51:46,240 --> 00:51:47,280


1226
00:51:47,280 --> 00:51:49,040
чтобы найти

1227
00:51:49,040 --> 00:51:52,720
что-то, чем им заняться, и предположить, что на самом деле

1228
00:51:52,720 --> 00:51:56,319
занимается своего рода мультимедийными исследованиями,

1229
00:51:56,319 --> 00:52:00,400
так что это судьбы истории

1230
00:52:00,400 --> 00:52:01,599


1231
00:52:01,599 --> 00:52:05,440
Итак, что такое lstm, так

1232
00:52:05,440 --> 00:52:08,720
что ключевое нововведение lstm - это

1233
00:52:08,720 --> 00:52:12,079
сказать хорошо, а не просто иметь один

1234
00:52:12,079 --> 00:52:13,839
скрытый вектор

1235
00:52:13,839 --> 00:52:17,760
в рекуррентной модели, мы собираемся

1236
00:52:17,760 --> 00:52:19,760
построить модель с

1237
00:52:19,760 --> 00:52:21,599
двумя

1238
00:52:21,599 --> 00:52:25,200
скрытыми векторами на каждом временном шаге, один из

1239
00:52:25,200 --> 00:52:28,079
которых все еще  называется скрытым состоянием h,

1240
00:52:28,079 --> 00:52:30,319
а другое из которых называется

1241
00:52:30,319 --> 00:52:32,880
состоянием ячейки,

1242
00:52:32,880 --> 00:52:35,760
теперь вы знаете, что, возможно, ретроспективно

1243
00:52:35,760 --> 00:52:37,760
они были названы неправильно, потому что, как

1244
00:52:37,760 --> 00:52:39,200
вы увидите, когда мы рассмотрим более

1245
00:52:39,200 --> 00:52:42,160
подробно, в некотором смысле ячейка более

1246
00:52:42,160 --> 00:52:44,400
эквивалентна скрытому состоянию

1247
00:52:44,400 --> 00:52:47,359
простого rnn тогда наоборот, но

1248
00:52:47,359 --> 00:52:49,280
мы просто используем имена, которые все

1249
00:52:49,280 --> 00:52:51,920
используют, так что оба они являются векторами

1250
00:52:51,920 --> 00:52:53,200
длины n

1251
00:52:53,200 --> 00:52:55,280
um, и это будет ячейка, которая

1252
00:52:55,280 --> 00:52:58,800
хранит долгосрочную информацию,

1253
00:52:58,800 --> 00:53:00,800
и поэтому мы  Чтобы иметь что-то, что

1254
00:53:00,800 --> 00:53:03,599
больше похоже на память, так что

1255
00:53:03,599 --> 00:53:06,480
значение, такое как барабан и компьютер, так

1256
00:53:06,480 --> 00:53:09,599
что ячейка спроектирована так, чтобы вы могли читать

1257
00:53:09,599 --> 00:53:12,160
из нее, вы можете стирать ее части, и

1258
00:53:12,160 --> 00:53:14,160
вы можете записывать новую информацию в

1259
00:53:14,160 --> 00:53:15,440


1260
00:53:15,440 --> 00:53:18,880
ячейку и интересную часть

1261
00:53:18,880 --> 00:53:22,240
lstm - это управляющие структуры, которые

1262
00:53:22,240 --> 00:53:25,440
решают, как вы это делаете, поэтому

1263
00:53:25,440 --> 00:53:28,160
выбор информации для стирания записи и

1264
00:53:28,160 --> 00:53:30,800
чтения контролируется вероятностными

1265
00:53:30,800 --> 00:53:31,760
вентилями,

1266
00:53:31,760 --> 00:53:34,400
поэтому вентили также являются векторами длины

1267
00:53:34,400 --> 00:53:35,200
n,

1268
00:53:35,200 --> 00:53:39,200
и на каждом временном шаге um мы определяем

1269
00:53:39,200 --> 00:53:41,440
состояние для  векторов вентилей, поэтому каждый

1270
00:53:41,440 --> 00:53:43,520
элемент векторов вентилей является

1271
00:53:43,520 --> 00:53:45,440
вероятностью, поэтому они могут иметь вероятность открытия,

1272
00:53:45,440 --> 00:53:48,720
одну близкую вероятность, ноль

1273
00:53:48,720 --> 00:53:52,000
или где-то посередине, и их значение

1274
00:53:52,000 --> 00:53:54,880
будет говорить, сколько вы стираете,

1275
00:53:54,880 --> 00:53:58,640
сколько вы пишете, сколько вы читаете

1276
00:53:58,640 --> 00:54:00,960
и так  это динамические ворота со

1277
00:54:00,960 --> 00:54:03,280
значением, которое вычисляется на основе

1278
00:54:03,280 --> 00:54:06,319
текущего контекста.

1279
00:54:06,400 --> 00:54:09,119
Хорошо, поэтому в следующем слайде мы

1280
00:54:09,119 --> 00:54:11,920
рассмотрим уравнения lstm, но после

1281
00:54:11,920 --> 00:54:14,079
этого есть еще несколько графических слайдов,

1282
00:54:14,079 --> 00:54:17,040
которые будут про  возможно, будет легче усвоить

1283
00:54:17,040 --> 00:54:19,680
правильно, поэтому мы снова, как и прежде, чем это

1284
00:54:19,680 --> 00:54:21,839
наша текущая нейронная сеть, у нас есть

1285
00:54:21,839 --> 00:54:25,280
последовательность входных данных x и t,

1286
00:54:25,280 --> 00:54:27,280
и мы собираемся на каждом временном шаге

1287
00:54:27,280 --> 00:54:30,480
вычислять состояние ячейки и скрытое состояние,

1288
00:54:30,480 --> 00:54:33,839
так как мы это делаем?  поэтому сначала мы

1289
00:54:33,839 --> 00:54:36,839
собираемся вычислить значения трех

1290
00:54:36,839 --> 00:54:39,839
вентилей, и поэтому мы вычисляем значения вентилей,

1291
00:54:39,839 --> 00:54:42,799
используя уравнение, которое

1292
00:54:42,799 --> 00:54:46,160
идентично уравнению

1293
00:54:46,160 --> 00:54:49,839
для простой рекуррентной нейронной сети,

1294
00:54:49,839 --> 00:54:53,280
но, в частности, ой ой, извините, я

1295
00:54:53,280 --> 00:54:55,200
просто скажу, что  ворота являются первыми,

1296
00:54:55,200 --> 00:54:58,319
поэтому есть ворота забвения um, с помощью которых мы

1297
00:54:58,319 --> 00:55:00,880
можем контролировать то, что хранится

1298
00:55:00,880 --> 00:55:03,520
в ячейке на следующем временном шаге, по сравнению с

1299
00:55:03,520 --> 00:55:06,480
тем, что забыто, есть входной шлюз,

1300
00:55:06,480 --> 00:55:09,520
который будет определять, какие

1301
00:55:09,520 --> 00:55:12,160
части вычисленного нового содержимого ячейки будут

1302
00:55:12,160 --> 00:55:14,319
записаны в  ячейка памяти, и

1303
00:55:14,319 --> 00:55:16,880
есть выходной вентиль, который будет контролировать,

1304
00:55:16,880 --> 00:55:19,599
какие части ячейки памяти

1305
00:55:19,599 --> 00:55:21,760
перемещаются в скрытое состояние,

1306
00:55:21,760 --> 00:55:24,640
и поэтому каждый из них использует

1307
00:55:24,640 --> 00:55:27,200
логистическую функцию, потому что мы хотим, чтобы

1308
00:55:27,200 --> 00:55:30,640
они были в каждом элементе этого вектора.  ra

1309
00:55:30,640 --> 00:55:33,599
вероятность, которая скажет, следует ли

1310
00:55:33,599 --> 00:55:36,960
полностью забыть, частично забыть или

1311
00:55:36,960 --> 00:55:39,200
полностью вспомнить,

1312
00:55:39,200 --> 00:55:41,680
да, и уравнение для каждого из них

1313
00:55:41,680 --> 00:55:43,760
в точности похоже на простое уравнение r и n,

1314
00:55:43,760 --> 00:55:46,160
но, конечно, обратите внимание, что у нас

1315
00:55:46,160 --> 00:55:48,480
есть разные параметры для каждого из них, поэтому

1316
00:55:48,480 --> 00:55:52,160
мы  получили матрицу весов забвения w

1317
00:55:52,160 --> 00:55:55,119
со смещением

1318
00:55:55,119 --> 00:55:57,040
забвения и множителем забвения

1319
00:55:57,040 --> 00:56:00,559
входных данных,

1320
00:56:00,960 --> 00:56:04,079
хорошо, так что тогда у нас есть другие уравнения,

1321
00:56:04,079 --> 00:56:07,839
которые действительно являются механикой

1322
00:56:07,839 --> 00:56:09,280
lstm,

1323
00:56:09,280 --> 00:56:12,799
поэтому у нас есть что-то, что будет

1324
00:56:12,799 --> 00:56:15,440
вычислять новое содержимое ячейки, так что это наше

1325
00:56:15,440 --> 00:56:18,160
обновление кандидата  и поэтому

1326
00:56:18,160 --> 00:56:20,720
для вычисления кандидата обновления

1327
00:56:20,720 --> 00:56:23,440
мы снова, по сути, используем точно

1328
00:56:23,440 --> 00:56:26,880
такое же простое уравнение rnn, за исключением того, что

1329
00:56:26,880 --> 00:56:29,839
теперь обычно используется tanh, поэтому вы получаете

1330
00:56:29,839 --> 00:56:32,480
что-то, что, как обсуждалось в прошлый раз,

1331
00:56:32,480 --> 00:56:34,720
сбалансировано около нуля,

1332
00:56:34,720 --> 00:56:38,240
хорошо, поэтому для фактического обновления вещей

1333
00:56:38,240 --> 00:56:42,000
мы используем наши  ворота, поэтому для нашего нового

1334
00:56:42,000 --> 00:56:45,119
содержимого ячейки идея состоит в том, что мы

1335
00:56:45,119 --> 00:56:49,040
хотим запомнить некоторые, но, вероятно, не все,

1336
00:56:49,040 --> 00:56:51,040
что у нас было в ячейке с предыдущих

1337
00:56:51,040 --> 00:56:54,640
временных шагов, и мы хотим, чтобы  o

1338
00:56:54,640 --> 00:56:55,920
сохранить

1339
00:56:55,920 --> 00:56:59,040
некоторые, но, вероятно, не все значения,

1340
00:56:59,040 --> 00:57:02,960
которые мы вычислили в качестве обновления новой

1341
00:57:02,960 --> 00:57:03,839
ячейки,

1342
00:57:03,839 --> 00:57:07,839
и поэтому мы делаем это так: мы

1343
00:57:07,839 --> 00:57:11,359
берем предыдущее содержимое ячейки, а

1344
00:57:11,359 --> 00:57:14,640
затем берем его продукт хадамара

1345
00:57:14,640 --> 00:57:17,520
с вектором забывания,

1346
00:57:17,520 --> 00:57:20,400
а затем добавляем  к нему произведение хадамара

1347
00:57:20,400 --> 00:57:24,559
входных ворот, умноженное на

1348
00:57:24,559 --> 00:57:27,839
обновление ячейки-кандидата,

1349
00:57:30,160 --> 00:57:32,720
а затем для разработки нового скрытого

1350
00:57:32,720 --> 00:57:36,079
состояния мы затем решаем, какие части

1351
00:57:36,079 --> 00:57:38,079


1352
00:57:38,079 --> 00:57:42,319
ячейки отображать в скрытом состоянии, и поэтому

1353
00:57:42,319 --> 00:57:45,440
после преобразования tan h

1354
00:57:45,440 --> 00:57:48,160
ячейки мы  затем возьмите продукт хадамара

1355
00:57:48,160 --> 00:57:50,400
с выходным вентилем, и это дает нам

1356
00:57:50,400 --> 00:57:52,799
наше скрытое представление и в качестве этого

1357
00:57:52,799 --> 00:57:54,400
скрытого представления,

1358
00:57:54,400 --> 00:57:55,680
которое мы затем

1359
00:57:55,680 --> 00:57:58,000
пропускаем через слой soft soft max для

1360
00:57:58,000 --> 00:58:01,520
генерации следующего вывода нашей lstm

1361
00:58:01,520 --> 00:58:03,119
или текущей нейронной сети.

1362
00:58:03,119 --> 00:58:04,400


1363
00:58:04,400 --> 00:58:07,040


1364
00:58:07,040 --> 00:58:10,000
вещи, которые они

1365
00:58:10,000 --> 00:58:13,359
помещают, являются векторами размера n, и что

1366
00:58:13,359 --> 00:58:15,599
мы делаем, мы берем каждый

1367
00:58:15,599 --> 00:58:17,920
из них и умножаем их

1368
00:58:17,920 --> 00:58:19,040
поэлементно,

1369
00:58:19,040 --> 00:58:22,079
чтобы выработать новый вектор, а затем мы получаем

1370
00:58:22,079 --> 00:58:23,440
два вектора, и что мы  сложить

1371
00:58:23,440 --> 00:58:27,040
вместе, так что этот способ делать что

1372
00:58:27,040 --> 00:58:29,440
-то поэлементно, что вы вроде как действительно не

1373
00:58:29,440 --> 00:58:32,880
видите в стандартном курсе линейной алгебры,

1374
00:58:32,880 --> 00:58:35,920
он называется продуктом Хадамара,

1375
00:58:35,920 --> 00:58:38,799
он представлен каким-то кругом,

1376
00:58:38,799 --> 00:58:40,559
я имею в виду, что на самом деле в более современной работе это

1377
00:58:40,559 --> 00:58:42,880
было больше  обычно представлял это с помощью

1378
00:58:42,880 --> 00:58:44,720
этого немного большего круга с

1379
00:58:44,720 --> 00:58:47,280
точкой посередине в качестве символа продукта Хадамар,

1380
00:58:47,280 --> 00:58:48,799


1381
00:58:48,799 --> 00:58:50,960
и когда-нибудь я изменю эти слайды, чтобы они

1382
00:58:50,960 --> 00:58:53,440
были такими, но я был ленив и

1383
00:58:53,440 --> 00:58:55,839
переделал уравнения, но другая запись, которую

1384
00:58:55,839 --> 00:58:58,240
вы видите довольно часто, это  просто используя тот

1385
00:58:58,240 --> 00:59:00,240
же маленький кружок, который вы используете для

1386
00:59:00,240 --> 00:59:02,319
композиции функций для представления

1387
00:59:02,319 --> 00:59:04,400
продукта хадама,

1388
00:59:04,400 --> 00:59:07,040
хорошо, поэтому все эти вещи

1389
00:59:07,040 --> 00:59:10,720
выполняются как векторы одинаковой длины n,

1390
00:59:10,720 --> 00:59:12,799
а другая вещь, которую вы можете

1391
00:59:12,799 --> 00:59:16,799
заметить, это то, что обновление кандидата

1392
00:59:16,799 --> 00:59:19,359
и забыть

1393
00:59:19,359 --> 00:59:22,480
импорт и  Все выходные ворота имеют очень

1394
00:59:22,480 --> 00:59:25,040
похожую форму, единственное отличие состоит в том, что

1395
00:59:25,040 --> 00:59:28,720
три логических элемента и один tanh, и ни один из

1396
00:59:28,720 --> 00:59:31,520
них не зависит друг от друга, поэтому все четыре из

1397
00:59:31,520 --> 00:59:34,559
них могут быть рассчитаны параллельно.  И

1398
00:59:34,559 --> 00:59:36,880
если вы хотите иметь эффективную

1399
00:59:36,880 --> 00:59:39,599
реализацию lstm, это то, что вы делаете,

1400
00:59:39,599 --> 00:59:42,480
хорошо, так что вот более графическое

1401
00:59:42,480 --> 00:59:45,280
представление этого, поэтому эти изображения

1402
00:59:45,280 --> 00:59:48,160
взяты от Криса Ола, и я думаю, он проделал

1403
00:59:48,160 --> 00:59:50,720
такую хорошую работу по созданию изображений

1404
00:59:50,720 --> 00:59:54,480
для lstms, что почти все их используют

1405
00:59:54,480 --> 00:59:57,359
дней, и поэтому этот вид

1406
00:59:57,359 --> 00:59:58,960


1407
00:59:58,960 --> 01:00:02,559
разбивает график вычислений модуля lstm, так что,

1408
01:00:02,559 --> 01:00:04,960
взорвав его,

1409
01:00:04,960 --> 01:00:08,319
вы получили с предыдущего временного шага

1410
01:00:08,319 --> 01:00:11,520
и свою ячейку, и скрытые

1411
01:00:11,520 --> 01:00:12,839
рекуррентные

1412
01:00:12,839 --> 01:00:17,920
векторы, и поэтому вы скармливаете скрытый

1413
01:00:18,400 --> 01:00:21,040
вектор из предыдущего временного шага

1414
01:00:21,040 --> 01:00:24,640
и новый  введите xt в

1415
01:00:24,640 --> 01:00:26,640
вычисление ворот, которое происходит

1416
01:00:26,640 --> 01:00:29,680
внизу, так что вы вычисляете ворота забывания,

1417
01:00:29,680 --> 01:00:32,240
а затем вы используете ворота

1418
01:00:32,240 --> 01:00:34,720
забывания в продукте Хадамара, здесь нарисованном как

1419
01:00:34,720 --> 01:00:35,920
фактический

1420
01:00:35,920 --> 01:00:39,440
символ времени, чтобы забыть некоторое содержимое ячейки,

1421
01:00:39,440 --> 01:00:42,240
вы разрабатываете входные ворота  а затем,

1422
01:00:42,240 --> 01:00:46,720
используя входной вентиль и обычную

1423
01:00:46,720 --> 01:00:48,160
рекуррентную нейронную сеть, такую как

1424
01:00:48,160 --> 01:00:51,200
вычисления, вы можете вычислить

1425
01:00:51,200 --> 01:00:54,000
содержание новой ячейки кандидата,

1426
01:00:54,000 --> 01:00:58,240
и затем вы складываете эти два вместе,

1427
01:00:58,240 --> 01:01:01,440
чтобы получить новую ячейку  содержимое, которое затем

1428
01:01:01,440 --> 01:01:04,079
выводится как новое содержимое ячейки в

1429
01:01:04,079 --> 01:01:05,440
момент времени t,

1430
01:01:05,440 --> 01:01:07,280
но затем вы

1431
01:01:07,280 --> 01:01:10,799
также разработали выходной вентиль, и

1432
01:01:10,799 --> 01:01:14,079
затем вы берете содержимое ячейки,

1433
01:01:14,079 --> 01:01:18,480
пропускаете его через другой нелинейный и

1434
01:01:18,480 --> 01:01:20,880
мультиадамарный продукт с выходным

1435
01:01:20,880 --> 01:01:23,200
вентилем, и это затем дает  Вы новое

1436
01:01:23,200 --> 01:01:25,680
скрытое состояние,

1437
01:01:25,680 --> 01:01:29,359
так что это все довольно сложно,

1438
01:01:29,359 --> 01:01:32,880
но что касается понимания того, почему

1439
01:01:32,880 --> 01:01:35,359
здесь происходит что-то другое, следует

1440
01:01:35,359 --> 01:01:39,520
отметить, что состояние ячейки от t

1441
01:01:39,520 --> 01:01:41,040
минус 1

1442
01:01:41,040 --> 01:01:44,559
проходит прямо через это, чтобы быть

1443
01:01:44,559 --> 01:01:46,480
состоянием ячейки в момент времени t

1444
01:01:46,480 --> 01:01:50,000
с этим ничего не происходит, поэтому

1445
01:01:50,000 --> 01:01:52,400
некоторые из них

1446
01:01:52,400 --> 01:01:55,200
удаляются шлюзом забывания,

1447
01:01:55,200 --> 01:01:58,799
а затем в него записывается какой-то новый материал

1448
01:01:58,799 --> 01:02:01,839
в результате

1449
01:02:01,839 --> 01:02:04,799
использования этого кандидата нового содержимого ячейки,

1450
01:02:04,799 --> 01:02:06,480
но настоящий секрет

1451
01:02:06,480 --> 01:02:07,839


1452
01:02:07,839 --> 01:02:09,599
um

1453
01:02:09,599 --> 01:02:11,039
lstm

1454
01:02:11,039 --> 01:02:14,559
заключается в том, что новый материал  просто добавляется

1455
01:02:14,559 --> 01:02:17,520
в ячейку с правом сложения, поэтому

1456
01:02:17,520 --> 01:02:21,200
в простом rnn на каждом последующем шаге

1457
01:02:21,200 --> 01:02:23,440
вы выполняете умножение,

1458
01:02:23,440 --> 01:02:26,720
и это делает невероятно

1459
01:02:26,720 --> 01:02:29,760
трудным научиться сохранять информацию в

1460
01:02:29,760 --> 01:02:32,960
скрытом состоянии ov  в течение длительного периода времени

1461
01:02:32,960 --> 01:02:35,280
это не совсем невозможно, но

1462
01:02:35,280 --> 01:02:37,760
это очень сложная вещь для изучения,

1463
01:02:37,760 --> 01:02:41,440
тогда как с этой новой архитектурой

1464
01:02:41,440 --> 01:02:43,920
lstm тривиально сохранять информацию в

1465
01:02:43,920 --> 01:02:46,799
ячейке от одного временного шага до следующего, вы

1466
01:02:46,799 --> 01:02:49,280
просто не забываете об этом,

1467
01:02:49,280 --> 01:02:51,520
и она будет  довести до конца,

1468
01:02:51,520 --> 01:02:54,480
возможно, добавив что-то новое, чтобы

1469
01:02:54,480 --> 01:02:57,200
также запомнить, и поэтому в этом смысле

1470
01:02:57,200 --> 01:02:58,480


1471
01:02:58,480 --> 01:03:01,760
ячейка ведет себя больше как барана в

1472
01:03:01,760 --> 01:03:03,839
обычном компьютере, который хранит

1473
01:03:03,839 --> 01:03:07,039
материал, и в него можно хранить дополнительные данные,

1474
01:03:07,039 --> 01:03:08,960
а другие материалы можно удалять  из

1475
01:03:08,960 --> 01:03:12,319
этого, когда вы продвигаетесь

1476
01:03:12,720 --> 01:03:15,839
хорошо, поэтому архитектура lstm

1477
01:03:15,839 --> 01:03:19,119
значительно упрощает сохранение информации из

1478
01:03:19,119 --> 01:03:22,240
многих временных шагов,

1479
01:03:22,480 --> 01:03:24,799
поэтому в частности,

1480
01:03:24,799 --> 01:03:28,160
стандартная практика с lstms заключается в

1481
01:03:28,160 --> 01:03:30,720
инициализации шлюза забывания одним

1482
01:03:30,720 --> 01:03:33,119
вектором, который просто так, что

1483
01:03:33,119 --> 01:03:35,440
отправной точкой является  скажем сохранить

1484
01:03:35,440 --> 01:03:39,280
все из предыдущих временных шагов,

1485
01:03:39,280 --> 01:03:40,319
а

1486
01:03:40,319 --> 01:03:42,000
затем он учится, когда

1487
01:03:42,000 --> 01:03:44,640
уместно забыть вещи,

1488
01:03:44,640 --> 01:03:48,400
и, наоборот, очень трудно получить

1489
01:03:48,400 --> 01:03:51,599
или простой rnn для сохранения  в течение

1490
01:03:51,599 --> 01:03:54,400
очень долгого времени я имею в виду, что это на

1491
01:03:54,400 --> 01:03:56,319
самом деле означает

1492
01:03:56,319 --> 01:03:58,559
хорошо, вы знаете, я записал здесь некоторые цифры,

1493
01:03:58,559 --> 01:04:00,240
я имею в виду, что

1494
01:04:00,240 --> 01:04:03,039
вы знаете, как то, что вы получаете на практике,

1495
01:04:03,039 --> 01:04:05,039
вы знаете, зависит от миллиона вещей, это

1496
01:04:05,039 --> 01:04:07,119
зависит от характера ваших данных и

1497
01:04:07,119 --> 01:04:08,960
сколько у вас данных и какой

1498
01:04:08,960 --> 01:04:11,280
размерности ваши

1499
01:04:11,280 --> 01:04:14,240
скрытые состояния являются размытыми кровавыми пятнами, но

1500
01:04:14,240 --> 01:04:17,359
просто чтобы дать вам некоторое представление о том, что

1501
01:04:17,359 --> 01:04:20,720
происходит, обычно, если вы тренируете

1502
01:04:20,720 --> 01:04:23,599
простую рекуррентную нейронную сеть, ее

1503
01:04:23,599 --> 01:04:26,000
эффект памяти ее способность иметь

1504
01:04:26,000 --> 01:04:28,000
возможность использовать вещи в  прошлое, чтобы

1505
01:04:28,000 --> 01:04:30,640
обусловить будущее, занимает около семи временных

1506
01:04:30,640 --> 01:04:32,799
шагов, вы просто не можете заставить его

1507
01:04:32,799 --> 01:04:35,119
вспомнить вещи более

1508
01:04:35,119 --> 01:04:37,680


1509
01:04:37,680 --> 01:04:40,559
ранние, чем это, тогда как для lstm

1510
01:04:40,559 --> 01:04:42,880
это не полная магия, это не

1511
01:04:42,880 --> 01:04:45,200
работает вечно, но вы знаете, что это

1512
01:04:45,200 --> 01:04:48,319
эффективно запоминать и использовать

1513
01:04:48,319 --> 01:04:50,799
вещи из гораздо более

1514
01:04:50,799 --> 01:04:54,079
далекого прошлого, поэтому обычно вы обнаруживаете, что с lstm вы

1515
01:04:54,079 --> 01:04:56,720
можете эффективно запоминать и использовать вещи

1516
01:04:56,720 --> 01:04:59,039
примерно в сто раз назад, и

1517
01:04:59,039 --> 01:05:01,920
это просто намного полезнее для

1518
01:05:01,920 --> 01:05:03,200
многие задачи по пониманию естественного языка,

1519
01:05:03,200 --> 01:05:05,680
которые

1520
01:05:05,680 --> 01:05:08,079
мы хотим выполнить,

1521
01:05:08,079 --> 01:05:11,119
и поэтому именно для этого

1522
01:05:11,119 --> 01:05:14,000
был разработан lstm, и я имею в виду, в

1523
01:05:14,000 --> 01:05:16,720
частности, просто возвращаясь к его названию, я

1524
01:05:16,720 --> 01:05:19,680
довольно много людей пропустили его название,

1525
01:05:19,680 --> 01:05:22,400
идею его  Это была

1526
01:05:22,400 --> 01:05:24,480
концепция кратковременной памяти, пришедшая

1527
01:05:24,480 --> 01:05:26,559
из психологии, и было

1528
01:05:26,559 --> 01:05:29,680
предложено для простых МНН, что

1529
01:05:29,680 --> 01:05:31,760
скрытое состояние РНН

1530
01:05:31,760 --> 01:05:33,280
могло быть моделью

1531
01:05:33,280 --> 01:05:36,319
кратковременной памяти человека, и тогда было

1532
01:05:36,319 --> 01:05:38,720
бы что-то еще, что

1533
01:05:38,720 --> 01:05:41,200
могло бы иметь дело.  с человеческой долговременной памятью,

1534
01:05:41,200 --> 01:05:43,680
но люди обнаружили, что это

1535
01:05:43,680 --> 01:05:46,480
дает вам только очень короткую кратковременную память,

1536
01:05:46,480 --> 01:05:49,680
так что гм Хок Райт и Шмидт

1537
01:05:49,680 --> 01:05:52,559
Убер интересовались, как мы могли бы

1538
01:05:52,559 --> 01:05:54,000


1539
01:05:54,000 --> 01:05:57,039
создать модели с долговременной краткосрочной

1540
01:05:57,039 --> 01:05:59,359
памятью и т. д.  который затем дал нам это

1541
01:05:59,359 --> 01:06:02,240
имя lstm

1542
01:06:02,240 --> 01:06:05,359
um lstms не гарантирует, что

1543
01:06:05,359 --> 01:06:08,160
нет исчезающих или взрывающихся градиентов, но

1544
01:06:08,160 --> 01:06:10,880
на практике они обеспечивают,

1545
01:06:10,880 --> 01:06:13,839
что они не имеют тенденции снова взорваться

1546
01:06:13,839 --> 01:06:16,400
почти так же, как знак

1547
01:06:16,400 --> 01:06:18,720
плюса cr

1548
01:06:18,720 --> 01:06:21,039
они являются гораздо более эффективным

1549
01:06:21,039 --> 01:06:25,039
способом изучения зависимостей на большом расстоянии.

1550
01:06:25,039 --> 01:06:26,400


1551
01:06:26,400 --> 01:06:29,280


1552
01:06:29,280 --> 01:06:30,640


1553
01:06:30,640 --> 01:06:32,640


1554
01:06:32,640 --> 01:06:34,000


1555
01:06:34,000 --> 01:06:38,079


1556
01:06:38,079 --> 01:06:40,480


1557
01:06:40,480 --> 01:06:42,880
они были успешными, так что это было

1558
01:06:42,880 --> 01:06:46,480
примерно с 2013 по 2015 год

1559
01:06:46,480 --> 01:06:50,000
, когда lstms своего рода поразил мир,

1560
01:06:50,000 --> 01:06:52,160
достигнув самых современных результатов по

1561
01:06:52,160 --> 01:06:54,319
всем видам проблем. Одна из

1562
01:06:54,319 --> 01:06:56,160
первых крупных демонстраций была для

1563
01:06:56,160 --> 01:06:58,880
распознавания рукописного ввода, затем распознавания речи,

1564
01:06:58,880 --> 01:07:01,680
но затем  к

1565
01:07:01,680 --> 01:07:04,799
множеству задач естественного языка, включая

1566
01:07:04,799 --> 01:07:07,760
машинный перевод, синтаксический анализ видения

1567
01:07:07,760 --> 01:07:10,079
и языковые задачи, такие как создание подписей к изображениям,

1568
01:07:10,079 --> 01:07:11,760
а также, конечно, их использование для

1569
01:07:11,760 --> 01:07:15,440
языковых моделей, и примерно в эти годы

1570
01:07:15,440 --> 01:07:18,400
lstms стал доминирующим подходом для

1571
01:07:18,400 --> 01:07:20,960
большинства задач

1572
01:07:20,960 --> 01:07:23,920
nlp.  сильная модель заключалась в том, чтобы подойти к

1573
01:07:23,920 --> 01:07:26,480
проблеме с lstm,

1574
01:07:26,480 --> 01:07:30,319
поэтому теперь, в 2021 году, фактически lstms

1575
01:07:30,319 --> 01:07:32,480
начинают вытесняться или были

1576
01:07:32,480 --> 01:07:34,480
вытеснены другими подходами  В

1577
01:07:34,480 --> 01:07:37,599
частности, модели трансформеров, к которым

1578
01:07:37,599 --> 01:07:39,359
мы доберемся в классе через пару

1579
01:07:39,359 --> 01:07:40,720
недель,

1580
01:07:40,720 --> 01:07:42,480
так что это та картина, которую вы можете

1581
01:07:42,480 --> 01:07:45,200
видеть, так что в течение многих лет проводилась

1582
01:07:45,200 --> 01:07:47,599
конференция по машинному переводу и своего

1583
01:07:47,599 --> 01:07:50,240
рода соревнование по запеканию, называемое

1584
01:07:50,240 --> 01:07:52,960
мастерской wmt on  машинный перевод,

1585
01:07:52,960 --> 01:07:55,520
так что если вы посмотрите на историю того, что в

1586
01:07:55,520 --> 01:07:57,650
wmt 2014

1587
01:07:57,650 --> 01:07:58,799
[Музыка]

1588
01:07:58,799 --> 01:08:00,960


1589
01:08:00,960 --> 01:08:03,480
на конкурсе не было систем нейронного машинного перевода,

1590
01:08:03,480 --> 01:08:06,640
2014 год был фактически первым годом

1591
01:08:06,640 --> 01:08:09,200
, когда успех

1592
01:08:09,200 --> 01:08:12,799
um lstms для машинного перевода был

1593
01:08:12,799 --> 01:08:15,760
доказан в документе конференции, но

1594
01:08:15,760 --> 01:08:18,640
в этом соревновании ничего не произошло

1595
01:08:18,640 --> 01:08:20,180
к 2016 году

1596
01:08:20,180 --> 01:08:22,080
[Музыка]

1597
01:08:22,080 --> 01:08:25,520
все запрыгали на ls dms работает

1598
01:08:25,520 --> 01:08:28,560
отлично, и многие люди,

1599
01:08:28,560 --> 01:08:30,399
включая победителя конкурса, использовали

1600
01:08:30,399 --> 01:08:33,920
модель lstm ммм

1601
01:08:33,920 --> 01:08:35,839


1602
01:08:35,839 --> 01:08:39,679


1603
01:08:39,679 --> 01:08:42,000
lstms, и подавляющее большинство

1604
01:08:42,000 --> 01:08:44,319
людей сейчас используют трансформаторы,

1605
01:08:44,319 --> 01:08:46,640
поэтому в вашей сети все быстро меняется,

1606
01:08:46,640 --> 01:08:49,120
и мне постоянно приходится переписывать

1607
01:08:49,120 --> 01:08:51,520
эти лекции,

1608
01:08:51,520 --> 01:08:52,799
эм

1609
01:08:52,799 --> 01:08:55,359
так быстро  Другое примечание об исчезающих и

1610
01:08:55,359 --> 01:08:58,000
взрывающихся градиентах - это проблема только

1611
01:08:58,000 --> 01:09:00,960
с повторяющимися нейронными сетями, это не

1612
01:09:00,960 --> 01:09:04,000
на самом деле проблема, которая также возникает

1613
01:09:04,000 --> 01:09:06,479
везде, где у вас есть большая глубина,

1614
01:09:06,479 --> 01:09:08,719
включая прямую связь и сверточные

1615
01:09:08,719 --> 01:09:10,560
нейронные сети,

1616
01:09:10,560 --> 01:09:11,359
как в

1617
01:09:11,359 --> 01:09:13,759
любое время, когда у вас есть длинные последовательности

1618
01:09:13,759 --> 01:09:14,479


1619
01:09:14,479 --> 01:09:16,520
правил цепочки, которые дают вам

1620
01:09:16,520 --> 01:09:19,359
умножения, градиент может стать

1621
01:09:19,359 --> 01:09:22,080
исчезающе маленьким по мере того, как он распространяется обратно

1622
01:09:22,080 --> 01:09:24,238
um, и поэтому

1623
01:09:24,238 --> 01:09:26,319
обычно нижние уровни

1624
01:09:26,319 --> 01:09:28,479
изучаются очень медленно и их трудно

1625
01:09:28,479 --> 01:09:31,439
обучать, поэтому было много усилий в

1626
01:09:31,439 --> 01:09:33,359
других местах,

1627
01:09:33,359 --> 01:09:36,799
чтобы придумать  различные архитектуры,

1628
01:09:36,799 --> 01:09:38,238
которые позволяют вам

1629
01:09:38,238 --> 01:09:41,120
более эффективно учиться в более глубоких

1630
01:09:41,120 --> 01:09:43,759
сетях, и самый распространенный способ сделать

1631
01:09:43,759 --> 01:09:44,640
это

1632
01:09:44,640 --> 01:09:47,279
- добавить больше прямых соединений, которые

1633
01:09:47,279 --> 01:09:50,799
позволяют течь градиенту, так что большой

1634
01:09:50,799 --> 01:09:52,799
вещью и видением в последние несколько

1635
01:09:52,799 --> 01:09:55,040
лет были пересети, где res

1636
01:09:55,040 --> 01:09:58,080
означает  остаточные соединения и то, как

1637
01:09:58,080 --> 01:09:59,760
они сделаны, эта

1638
01:09:59,760 --> 01:10:01,920
картинка перевернута, так что ввод находится

1639
01:10:01,920 --> 01:10:03,760
наверху, гм

1640
01:10:03,760 --> 01:10:05,520
, у вас

1641
01:10:05,520 --> 01:10:07,840
есть эти виды  f два пути, которые

1642
01:10:07,840 --> 01:10:10,239
суммируются, один путь - это просто

1643
01:10:10,239 --> 01:10:12,640
путь идентичности, а другой проходит

1644
01:10:12,640 --> 01:10:14,880
через некоторые уровни нейронной сети,

1645
01:10:14,880 --> 01:10:17,600
поэтому его поведение по умолчанию -

1646
01:10:17,600 --> 01:10:20,880
просто сохранить входной um,

1647
01:10:20,880 --> 01:10:22,960
что может немного походить на то, что

1648
01:10:22,960 --> 01:10:26,080
мы только что видели для lstms  гм, есть и другие

1649
01:10:26,080 --> 01:10:28,239
методы, которые тогда были плотными сетями, где

1650
01:10:28,239 --> 01:10:30,719
вы добавляете пропускающие соединения вперед к

1651
01:10:30,719 --> 01:10:33,199
каждому лазерному слою

1652
01:10:33,199 --> 01:10:34,719
сети шоссе,

1653
01:10:34,719 --> 01:10:36,239
также были фактически разработаны

1654
01:10:36,239 --> 01:10:37,760
schmidhuber

1655
01:10:37,760 --> 01:10:39,760
и вроде как напоминают то, что было

1656
01:10:39,760 --> 01:10:42,480
сделано с lstms, так что вместо того, чтобы просто

1657
01:10:42,480 --> 01:10:45,440
иметь соединение с идентификацией в качестве

1658
01:10:45,440 --> 01:10:49,199
resnet  он вводит дополнительный вентиль,

1659
01:10:49,199 --> 01:10:52,320
поэтому он больше похож на lstm, который говорит,

1660
01:10:52,320 --> 01:10:54,560
сколько нужно отправить вход через

1661
01:10:54,560 --> 01:10:56,320
магистраль по

1662
01:10:56,320 --> 01:10:59,280
сравнению с тем, сколько мкм, чтобы пропустить его через

1663
01:10:59,280 --> 01:11:01,760
слой нейронной сети, и эти два затем

1664
01:11:01,760 --> 01:11:05,040
объединяются в выход,

1665
01:11:05,040 --> 01:11:06,159


1666
01:11:06,159 --> 01:11:07,120
так что, по

1667
01:11:07,120 --> 01:11:08,800
сути,

1668
01:11:08,800 --> 01:11:11,440
эта проблема  происходит где угодно, когда у вас

1669
01:11:11,440 --> 01:11:13,520
есть большая глубина в ваших

1670
01:11:13,520 --> 01:11:15,679
слоях нейронной сети,

1671
01:11:15,679 --> 01:11:16,640
но

1672
01:11:16,640 --> 01:11:19,040
она впервые возникла и оказывается

1673
01:11:19,040 --> 01:11:21,520
особенно проблематичной

1674
01:11:21,520 --> 01:11:23,760
с повторяющимися нейронными сетями.  сетей, они

1675
01:11:23,760 --> 01:11:26,239
особенно нестабильны из-за

1676
01:11:26,239 --> 01:11:28,800
того, что у вас есть одна

1677
01:11:28,800 --> 01:11:30,480
матрица весов, которую вы

1678
01:11:30,480 --> 01:11:32,960
постоянно используете во временной

1679
01:11:32,960 --> 01:11:35,520
последовательности.

1680
01:11:35,600 --> 01:11:38,600


1681
01:11:40,080 --> 01:11:40,880


1682
01:11:40,880 --> 01:11:42,400


1683
01:11:42,400 --> 01:11:44,080


1684
01:11:44,080 --> 01:11:45,440
или менее о том, захотите ли вы

1685
01:11:45,440 --> 01:11:47,840
когда-нибудь использовать rn, например, простой rnn,

1686
01:11:47,840 --> 01:11:49,679
вместо lstm,

1687
01:11:49,679 --> 01:11:52,159
как lstm узнает, что делать со

1688
01:11:52,159 --> 01:11:53,520
своими воротами,

1689
01:11:53,520 --> 01:11:55,840
вы можете высказать свое мнение об этих вещах,

1690
01:11:55,840 --> 01:11:56,880
конечно,

1691
01:11:56,880 --> 01:11:57,679


1692
01:11:57,679 --> 01:12:01,360
так что я думаю, в основном ответ: ммм,

1693
01:12:01,360 --> 01:12:03,679
вы никогда не должны  используйте простой rnn, в наши

1694
01:12:03,679 --> 01:12:06,960
дни вы всегда должны использовать lstm, я

1695
01:12:06,960 --> 01:12:09,280
имею в виду, что вы, очевидно, знаете, что это зависит от

1696
01:12:09,280 --> 01:12:10,640
того, что вы делаете, если вы хотите

1697
01:12:10,640 --> 01:12:13,040
сделать какую-то аналитическую статью или

1698
01:12:13,040 --> 01:12:17,040
что-то, что вы можете предпочесть простому rnn,

1699
01:12:17,040 --> 01:12:20,719
и это так  что вы

1700
01:12:20,719 --> 01:12:22,960
действительно можете получить достойные результаты с простыми

1701
01:12:22,960 --> 01:12:25,600
РНН, при условии, что вы очень внимательно

1702
01:12:25,600 --> 01:12:28,880
следите за тем, чтобы вещи не взрывались и

1703
01:12:28,880 --> 01:12:32,000
не исчезали,

1704
01:12:32,880 --> 01:12:36,320
но на практике вы знаете, что заставить простые

1705
01:12:36,320 --> 01:12:39,440
РНН работать и сохранять длинный

1706
01:12:39,440 --> 01:12:41,760
контекст невероятно сложно там, где вы можете

1707
01:12:41,760 --> 01:12:44,800
тренировать lst  мс, и они будут просто работать,

1708
01:12:44,800 --> 01:12:46,880
так что на самом деле вы всегда должны просто использовать

1709
01:12:46,880 --> 01:12:49,520
lstm,

1710
01:12:49,520 --> 01:12:52,480
теперь подождите, второй вопрос был:

1711
01:12:52,480 --> 01:12:54,400
эм, я думаю, есть небольшая путаница,

1712
01:12:54,400 --> 01:12:55,840
например

1713
01:12:55,840 --> 01:12:57,600
, узнаются ли ворота по-

1714
01:12:57,600 --> 01:13:01,040
другому.

1715
01:13:01,040 --> 01:13:04,800
мы

1716
01:13:04,800 --> 01:13:07,120
возвращаемся к этим уравнениям,

1717
01:13:07,120 --> 01:13:08,880


1718
01:13:08,880 --> 01:13:11,679
вы знаете, что это полная модель, и

1719
01:13:11,679 --> 01:13:14,080
когда мы обучаем модель,

1720
01:13:14,080 --> 01:13:17,280
каждый из этих параметров, поэтому все

1721
01:13:17,280 --> 01:13:20,560
эти wu и b,

1722
01:13:20,560 --> 01:13:23,199
все одновременно

1723
01:13:23,199 --> 01:13:25,600
обучаются с помощью обратного распространения,

1724
01:13:25,600 --> 01:13:26,400
так

1725
01:13:26,400 --> 01:13:27,760
что то, на

1726
01:13:27,760 --> 01:13:30,800
что вы надеетесь, и действительно работает

1727
01:13:30,800 --> 01:13:33,840
модель изучает, какие вещи я должен

1728
01:13:33,840 --> 01:13:36,400
запоминать надолго, а какие -

1729
01:13:36,400 --> 01:13:38,159
забыть,

1730
01:13:38,159 --> 01:13:40,320
какие вещи во входных данных важны,

1731
01:13:40,320 --> 01:13:42,560
а какие во входных данных на самом деле не имеют

1732
01:13:42,560 --> 01:13:44,960
значения, поэтому она может изучать такие вещи,

1733
01:13:44,960 --> 01:13:48,080
как наши функциональные слова, такие как

1734
01:13:48,080 --> 01:13:49,920
don '  это действительно имеет значение, даже если все

1735
01:13:49,920 --> 01:13:51,440
используют их на английском языке,

1736
01:13:51,440 --> 01:13:54,239
поэтому вы можете просто не беспокоиться о них,

1737
01:13:54,239 --> 01:13:56,960
поэтому все это изучено, и модели

1738
01:13:56,960 --> 01:13:59,120
действительно успешно узнают

1739
01:13:59,120 --> 01:14:01,840
значения ворот о том, какая информация  полезно

1740
01:14:01,840 --> 01:14:04,400
для сохранения в долгосрочной перспективе по сравнению с тем, какая

1741
01:14:04,400 --> 01:14:05,840
информация

1742
01:14:05,840 --> 01:14:08,159
на самом деле полезна только в краткосрочной перспективе для

1743
01:14:08,159 --> 01:14:11,679
предсказания следующих одного или двух слов,

1744
01:14:11,679 --> 01:14:12,840
наконец,

1745
01:14:12,840 --> 01:14:16,480
э-э, улучшения градиента из-

1746
01:14:16,480 --> 01:14:18,480
за того, что вы сказали, что добавление

1747
01:14:18,480 --> 01:14:20,080
действительно важно между новым

1748
01:14:20,080 --> 01:14:21,920
кандидатом в ячейку и состоянием ячейки  Я не

1749
01:14:21,920 --> 01:14:23,520
думаю, что по крайней мере пара студентов

1750
01:14:23,520 --> 01:14:24,560
задалась этим

1751
01:14:24,560 --> 01:14:26,159
вопросом, поэтому, если вы хотите

1752
01:14:26,159 --> 01:14:28,880
повторить это еще раз, это может быть полезно,

1753
01:14:28,880 --> 01:14:30,640
конечно,

1754
01:14:30,640 --> 01:14:32,640
ммм,

1755
01:14:32,640 --> 01:14:37,120
так что нам бы хотелось, чтобы это был простой способ

1756
01:14:37,120 --> 01:14:40,880
сохранить память на долгое время

1757
01:14:40,880 --> 01:14:43,199
ммм, а

1758
01:14:43,199 --> 01:14:44,239
вы  Знайте, что

1759
01:14:44,239 --> 01:14:46,719
один способ, который используют ресети, - это

1760
01:14:46,719 --> 01:14:49,280
просто как бы полностью иметь прямой

1761
01:14:49,280 --> 01:14:52,960
путь от ct минус один до ct и

1762
01:14:52,960 --> 01:14:54,239


1763
01:14:54,239 --> 01:14:56,719
полностью сохранять историю, так что это своего

1764
01:14:56,719 --> 01:15:00,320
рода действие по умолчанию для сохранения

1765
01:15:00,320 --> 01:15:03,840
информации um о прошлом долгосрочном

1766
01:15:03,840 --> 01:15:08,000
lstms не  Это вполне возможно, но они

1767
01:15:08,000 --> 01:15:10,960
позволяют упростить эту функцию, поэтому

1768
01:15:10,960 --> 01:15:12,960
вы начинаете с предыдущего

1769
01:15:12,960 --> 01:15:16,080
состояния ячейки и можете забыть некоторые из них с

1770
01:15:16,080 --> 01:15:18,159
помощью ворот забывания, чтобы вы могли удалить

1771
01:15:18,159 --> 01:15:19,520
из своей памяти то, что используется  Полная

1772
01:15:19,520 --> 01:15:21,280
операция,

1773
01:15:21,280 --> 01:15:22,640
а затем, пока вы

1774
01:15:22,640 --> 01:15:25,440
сможете обновить содержимое ячейки с помощью

1775
01:15:25,440 --> 01:15:26,800
этой

1776
01:15:26,800 --> 01:15:27,600


1777
01:15:27,600 --> 01:15:30,400
правильной операции, которая происходит в плюсе,

1778
01:15:30,400 --> 01:15:33,199
где в зависимости от входного шлюза

1779
01:15:33,199 --> 01:15:35,360
будут добавлены некоторые части того, что находится в ячейке,

1780
01:15:35,360 --> 01:15:36,560


1781
01:15:36,560 --> 01:15:38,719
но вы можете  подумайте об этом добавлении как

1782
01:15:38,719 --> 01:15:41,920
наложении дополнительной информации, все,

1783
01:15:41,920 --> 01:15:43,600
что было в ячейке, что не было

1784
01:15:43,600 --> 01:15:46,719
забыто, по-прежнему продолжается до

1785
01:15:46,719 --> 01:15:49,840
следующего временного шага

1786
01:15:50,000 --> 01:15:52,239
и, в частности, когда вы

1787
01:15:52,239 --> 01:15:54,880
выполняете обратное распространение во времени

1788
01:15:54,880 --> 01:15:57,360
, которого нет,

1789
01:15:57,360 --> 01:15:58,239


1790
01:15:58,239 --> 01:15:59,280
я хочу  чтобы сказать, что нет

1791
01:15:59,280 --> 01:16:02,159
умножения между ct и ct минус

1792
01:16:02,159 --> 01:16:05,840
один, и здесь есть этот неудачный символ времени um,

1793
01:16:05,840 --> 01:16:07,360
но помните, что это

1794
01:16:07,360 --> 01:16:09,920
произведение Хадамара, которое обнуляет его

1795
01:16:09,920 --> 01:16:12,159
часть с помощью элемента забывания, это

1796
01:16:12,159 --> 01:16:14,800
не умножение на матрицу,

1797
01:16:14,800 --> 01:16:18,600
как в простом  пнн,

1798
01:16:22,400 --> 01:16:24,880
я надеюсь, что это хорошо, ну

1799
01:16:24,880 --> 01:16:27,040
ладно, есть еще пара

1800
01:16:27,040 --> 01:16:29,440
вещей, которые я хотел бы сделать

1801
01:16:29,440 --> 01:16:31,360
до конца, я думаю, у меня не

1802
01:16:31,360 --> 01:16:33,199
будет времени сделать и то, и другое, я думаю, что

1803
01:16:33,199 --> 01:16:35,440
сделаю последнее  вероятно  В следующий раз,

1804
01:16:35,440 --> 01:16:37,920
так что это на самом деле простые и легкие

1805
01:16:37,920 --> 01:16:39,760
вещи,

1806
01:16:39,760 --> 01:16:43,199
но они дополняют нашу картину,

1807
01:16:43,199 --> 01:16:45,760
так что я как бы кратко сослался на этот

1808
01:16:45,760 --> 01:16:48,080
пример классификации настроений,

1809
01:16:48,080 --> 01:16:51,600
где то, что мы могли бы сделать, это запустить rnn,

1810
01:16:51,600 --> 01:16:55,120
может быть, lstm над предложением

1811
01:16:55,120 --> 01:16:58,400
назовем это нашим представлением  предложения

1812
01:16:58,400 --> 01:16:59,920


1813
01:16:59,920 --> 01:17:01,040
и

1814
01:17:01,040 --> 01:17:04,400
передать его в мягкий классификатор максимума, чтобы

1815
01:17:04,400 --> 01:17:06,480
классифицировать его по

1816
01:17:06,480 --> 01:17:09,199
настроениям, так что на самом деле мы

1817
01:17:09,199 --> 01:17:11,840
говорим, что мы можем рассматривать скрытое

1818
01:17:11,840 --> 01:17:16,000
состояние как представление слова в

1819
01:17:16,000 --> 01:17:17,600
контексте,

1820
01:17:17,600 --> 01:17:18,400
который

1821
01:17:18,400 --> 01:17:20,960
ниже у нас есть просто вектор слов

1822
01:17:20,960 --> 01:17:23,920
для  ужасно, но затем мы

1823
01:17:23,920 --> 01:17:27,199
посмотрели на наш контекст и сказали, что теперь мы

1824
01:17:27,199 --> 01:17:28,000


1825
01:17:28,000 --> 01:17:31,199
создали представление скрытого состояния

1826
01:17:31,199 --> 01:17:34,159
для слова, ужасно в

1827
01:17:34,159 --> 01:17:37,360
контексте фильма, и это оказалось

1828
01:17:37,360 --> 01:17:40,080
действительно полезной идеей, потому что слова имеют

1829
01:17:40,080 --> 01:17:43,520
разные значения в разных контекстах,

1830
01:17:43,520 --> 01:17:45,360
но это  похоже, что в

1831
01:17:45,360 --> 01:17:47,920
том, что мы здесь сделали, есть дефект, потому что

1832
01:17:47,920 --> 01:17:51,199
наш контекст содержит информацию

1833
01:17:51,199 --> 01:17:53,920
только слева, а что насчет правого контекста,

1834
01:17:53,920 --> 01:17:56,560
конечно, также было бы полезно иметь

1835
01:17:56,560 --> 01:17:58,400
значение t  ошибочно

1836
01:17:58,400 --> 01:18:02,000
зависят от захватывающего, потому что часто слова

1837
01:18:02,000 --> 01:18:04,000
означают разные вещи в зависимости от того, что

1838
01:18:04,000 --> 01:18:05,760
следует за ними,

1839
01:18:05,760 --> 01:18:08,080
поэтому вы знаете, что если у вас есть что-то вроде

1840
01:18:08,080 --> 01:18:10,480
красного вина, это означает что-то совсем

1841
01:18:10,480 --> 01:18:14,159
иное, чем красный свет,

1842
01:18:14,159 --> 01:18:16,880
так как мы можем справиться с этим колодцем?

1843
01:18:16,880 --> 01:18:19,360


1844
01:18:19,360 --> 01:18:22,320
хорошо сказать, если мы просто хотим

1845
01:18:22,320 --> 01:18:24,880
придумать нейронное кодирование предложения, мы могли

1846
01:18:24,880 --> 01:18:27,679
бы получить второй rnn с полностью

1847
01:18:27,679 --> 01:18:30,560
отдельными изученными параметрами, и мы могли бы

1848
01:18:30,560 --> 01:18:33,440
запустить его в обратном направлении по предложению,

1849
01:18:33,440 --> 01:18:35,840
чтобы получить обратное представление каждого

1850
01:18:35,840 --> 01:18:38,480
слова, а затем мы можем  получить общее

1851
01:18:38,480 --> 01:18:41,280
представление каждого слова в контексте

1852
01:18:41,280 --> 01:18:43,280
, просто объединив эти два

1853
01:18:43,280 --> 01:18:44,880
представления,

1854
01:18:44,880 --> 01:18:46,800
и теперь у нас есть представление

1855
01:18:46,800 --> 01:18:50,400
ужасно, которое имеет как левый, так и правый

1856
01:18:50,400 --> 01:18:52,800
контекст,

1857
01:18:52,800 --> 01:18:56,640
поэтому мы просто запускаем вперед rnn,

1858
01:18:56,640 --> 01:18:58,800
и когда я говорю здесь rnn,

1859
01:18:58,800 --> 01:19:00,560
это просто означает  любой тип рекуррентной

1860
01:19:00,560 --> 01:19:02,560
нейронной сети, так что обычно это будет

1861
01:19:02,560 --> 01:19:04,640
lstm

1862
01:19:04,640 --> 01:19:06,320
и обратная,

1863
01:19:06,320 --> 01:19:08,040
а затем на каждом временном шаге мы просто

1864
01:19:08,040 --> 01:19:11,360
объединяем их представления

1865
01:19:11,360 --> 01:19:14,320
um с каждым из  они имеют разные

1866
01:19:14,320 --> 01:19:16,640
веса, и поэтому мы рассматриваем эту

1867
01:19:16,640 --> 01:19:19,520
объединенную вещь как скрытое состояние,

1868
01:19:19,520 --> 01:19:22,239
контекстное представление токена

1869
01:19:22,239 --> 01:19:24,640
в конкретное время, которое мы передаем

1870
01:19:24,640 --> 01:19:26,080
вперед,

1871
01:19:26,080 --> 01:19:29,520
это настолько распространено, что люди используют

1872
01:19:29,520 --> 01:19:30,960
ярлык,

1873
01:19:30,960 --> 01:19:32,880
чтобы обозначить это, и они просто нарисуют

1874
01:19:32,880 --> 01:19:36,000
эту картинку  с двусторонними стрелками, и

1875
01:19:36,000 --> 01:19:38,400
когда вы видите это изображение с двусторонними

1876
01:19:38,400 --> 01:19:42,400
стрелками, это означает, что вы запускаете два

1877
01:19:42,400 --> 01:19:44,239
um

1878
01:19:44,239 --> 01:19:47,120
rnn по одному в каждом направлении, а затем

1879
01:19:47,120 --> 01:19:49,440
объединяете их результаты на каждом временном

1880
01:19:49,440 --> 01:19:51,679
шаге, и это то, что вы собираетесь использовать

1881
01:19:51,679 --> 01:19:54,480
позже в  модель в

1882
01:19:54,480 --> 01:19:56,800
порядке, но

1883
01:19:56,800 --> 01:19:58,640
если

1884
01:19:58,640 --> 01:20:01,040
вы решаете проблему с кодированием, например, для

1885
01:20:01,040 --> 01:20:03,199
классификации настроений или ответов на вопросы

1886
01:20:03,199 --> 01:20:05,840


1887
01:20:06,080 --> 01:20:08,400
с использованием двунаправленных rnns, это отличная

1888
01:20:08,400 --> 01:20:10,080
вещь,

1889
01:20:10,080 --> 01:20:12,320
но они применимы только в том случае, если у вас есть

1890
01:20:12,320 --> 01:20:15,679
доступ ко всей входной последовательности,

1891
01:20:15,679 --> 01:20:17,600
они  неприменимо

1892
01:20:17,600 --> 01:20:20,239
к языковому моделированию, потому что в

1893
01:20:20,239 --> 01:20:22,320
языковой модели вам обязательно нужно

1894
01:20:22,320 --> 01:20:24,960
сгенерировать следующее слово на основе только

1895
01:20:24,960 --> 01:20:27,600
предыдущего контекста,

1896
01:20:27,600 --> 01:20:29,440
но если у вас есть вся входная

1897
01:20:29,440 --> 01:20:31,679
последовательность, которая двунаправленная  Иональность дает

1898
01:20:31,679 --> 01:20:35,280
вам большую мощность, и это действительно

1899
01:20:35,280 --> 01:20:37,840
была идея, которую люди развили в

1900
01:20:37,840 --> 01:20:41,280
последующей работе, поэтому, когда мы дойдем до

1901
01:20:41,280 --> 01:20:43,600
трансформаторов через пару недель,

1902
01:20:43,600 --> 01:20:46,000
мы потратим много времени на модель Берта,

1903
01:20:46,000 --> 01:20:47,040


1904
01:20:47,040 --> 01:20:49,120
где этот акроним означает

1905
01:20:49,120 --> 01:20:51,679
двух-  представления направленного кодировщика

1906
01:20:51,679 --> 01:20:55,440
от трансформаторов, поэтому часть того, что

1907
01:20:55,440 --> 01:20:57,440
важно в этой модели, - это

1908
01:20:57,440 --> 01:21:00,480
трансформатор, но на самом деле центральным

1909
01:21:00,480 --> 01:21:03,280
моментом статьи было сказать, что вы можете

1910
01:21:03,280 --> 01:21:05,520
построить более мощные модели, используя

1911
01:21:05,520 --> 01:21:08,239
трансформаторы, снова

1912
01:21:08,239 --> 01:21:10,040
используя

1913
01:21:10,040 --> 01:21:11,760
двунаправленность,

1914
01:21:11,760 --> 01:21:12,880


1915
01:21:12,880 --> 01:21:15,600
хорошо, на rnn остался один крошечный бит  но

1916
01:21:15,600 --> 01:21:18,159
я пробираюсь в следующий класс, и я

1917
01:21:18,159 --> 01:21:20,800
назову это концом на сегодня, и если

1918
01:21:20,800 --> 01:21:22,719
есть другие вещи, о которых вы хотели бы задать

1919
01:21:22,719 --> 01:21:24,159
вопросы,

1920
01:21:24,159 --> 01:21:27,600
вы можете снова найти меня в укромных уголках и

1921
01:21:27,600 --> 01:21:29,120
всего через минуту,

1922
01:21:29,120 --> 01:21:34,280
хорошо, так что увидимся  снова в следующий вторник

