1
00:00:05,359 --> 00:00:08,440
хорошо, всем привет, добро пожаловать обратно на

2
00:00:08,440 --> 00:00:12,160
cs224n, так что сегодня у нас будет

3
00:00:12,160 --> 00:00:14,240
второй из наших приглашенных спикеров в этом

4
00:00:14,240 --> 00:00:16,400
квартале, и, учитывая весь

5
00:00:16,400 --> 00:00:18,240
ажиотаж, связанный с

6
00:00:18,240 --> 00:00:20,480
предварительно обученными языковыми моделями трансформера,

7
00:00:20,480 --> 00:00:22,720
и все замечательные вещи  мы

8
00:00:22,720 --> 00:00:25,119
закончили с ними, мы особенно

9
00:00:25,119 --> 00:00:27,359
рады сегодня приветствовать

10
00:00:27,359 --> 00:00:30,000
Колина Рафаэля, который был одним из ключевых людей,

11
00:00:30,000 --> 00:00:32,238
которые продвигали исследование

12
00:00:32,238 --> 00:00:34,399
больших предварительно обученных языковых моделей, в

13
00:00:34,399 --> 00:00:37,520
частности, он был очень заинтересован

14
00:00:37,520 --> 00:00:39,680
в разработке  языковой модели t5, о

15
00:00:39,680 --> 00:00:41,520
которой мы вам

16
00:00:41,520 --> 00:00:43,600
сегодня много расскажем, но расскажу вам

17
00:00:43,600 --> 00:00:46,800
еще несколько предложений, чтобы ммм Колин

18
00:00:46,800 --> 00:00:48,559
проработал несколько лет

19
00:00:48,559 --> 00:00:50,559
в Google Brain, включая работу с

20
00:00:50,559 --> 00:00:52,960
Джеффом Хинтоном над капсульными сетями,

21
00:00:52,960 --> 00:00:55,199
затем он заинтересовался

22
00:00:55,199 --> 00:00:58,239
эффективностью  о переводе с использованием

23
00:00:58,239 --> 00:01:00,320
предварительно подготовленных языковых моделей и как

24
00:01:00,320 --> 00:01:02,800
часть работы, которую он начал

25
00:01:02,800 --> 00:01:04,720
вместе с другими людьми по

26
00:01:04,720 --> 00:01:07,280
созданию еще более крупных предварительно обученных

27
00:01:07,280 --> 00:01:08,799
языковых моделей и  проводил много

28
00:01:08,799 --> 00:01:11,680
исследований по тем эм, которые привели к

29
00:01:11,680 --> 00:01:13,680
эм, документы по t5, о которых вы

30
00:01:13,680 --> 00:01:16,560
услышите сегодня, так что добро пожаловать, Колин,

31
00:01:16,560 --> 00:01:17,920
здорово, да, большое спасибо за

32
00:01:17,920 --> 00:01:19,439
введение и за то, что меня пригласили, для меня

33
00:01:19,439 --> 00:01:20,880
определенно большая честь выступать на

34
00:01:20,880 --> 00:01:22,119
легендарном

35
00:01:22,119 --> 00:01:25,360
классе cs224n  так что да, я собираюсь

36
00:01:25,360 --> 00:01:27,759
сегодня говорить о больших языковых

37
00:01:27,759 --> 00:01:30,320
моделях в целом, но

38
00:01:30,320 --> 00:01:33,040
сосредоточившись конкретно на этой модели t5,

39
00:01:33,040 --> 00:01:34,960
которую мы выпустили около полутора лет

40
00:01:34,960 --> 00:01:37,200
назад, я представлю

41
00:01:37,200 --> 00:01:39,680
пять или около того документов, которые должны

42
00:01:39,680 --> 00:01:41,600
представлять  полный спектр хороших

43
00:01:41,600 --> 00:01:43,920
вещей, плохих и уродливых вещей о

44
00:01:43,920 --> 00:01:45,920
больших языковых моделях, и на самом деле я

45
00:01:45,920 --> 00:01:47,680
немного расскажу о статье, которая только что

46
00:01:47,680 --> 00:01:50,000
появилась в архиве вчера вечером, поэтому,

47
00:01:50,000 --> 00:01:52,320
надеюсь, каждый узнает что-то

48
00:01:52,320 --> 00:01:54,399
новое сегодня, даже если даже если вы

49
00:01:54,399 --> 00:01:55,600
уже  знаком с некоторыми из этих

50
00:01:55,600 --> 00:01:57,680
документов,

51
00:01:57,680 --> 00:01:59,680
поэтому, чтобы дать вам представление о том, что я

52
00:01:59,680 --> 00:02:01,439
буду освещать в этом выступлении, я как

53
00:02:01,439 --> 00:02:03,520
бы отвечу на каждый из этих вопросов по

54
00:02:03,520 --> 00:02:05,439
очереди

55
00:02:06,399 --> 00:02:08,479
в ходе презентации

56
00:02:08,479 --> 00:02:10,560
э-э, и я должен упомянуть, что это,

57
00:02:10,560 --> 00:02:11,280
поскольку

58
00:02:11,280 --> 00:02:12,319
некоторые из этих

59
00:02:12,319 --> 00:02:13,920
документов являются новыми, а некоторые из этих материалов

60
00:02:13,920 --> 00:02:15,760
являются новыми, это первый раз, когда я буду

61
00:02:15,760 --> 00:02:17,440
представлять эти слайды, поэтому, если что-то

62
00:02:17,440 --> 00:02:19,440
сбивает с толку, я понимаю,

63
00:02:19,440 --> 00:02:21,520
что вы можете пинговать вопросы  меня

64
00:02:21,520 --> 00:02:23,760
через хостов и не стесняйтесь спрашивать

65
00:02:23,760 --> 00:02:25,520
обо всем, что может

66
00:02:25,520 --> 00:02:26,720
сбить с толку,

67
00:02:26,720 --> 00:02:28,720
так что да, первый вопрос, на который я

68
00:02:28,720 --> 00:02:30,000
попытаюсь ответить,

69
00:02:30,000 --> 00:02:32,800
это своего рода ответ, на котором мы

70
00:02:32,800 --> 00:02:37,040
стремились сосредоточиться в статье t5, а именно вы

71
00:02:37,040 --> 00:02:38,400
знать, какой из методов трансферного обучения,

72
00:02:38,400 --> 00:02:40,239
предложенных до сих пор, работает

73
00:02:40,239 --> 00:02:42,400
лучше всего и что происходит, когда мы масштабируем

74
00:02:42,400 --> 00:02:43,280
их,

75
00:02:43,280 --> 00:02:46,400
а затем после статьи t5 мы

76
00:02:46,400 --> 00:02:48,319
решили исследовать

77
00:02:48,319 --> 00:02:50,480
неанглоязычные предварительно обученные языковые модели

78
00:02:50,480 --> 00:02:52,560
t5 - это модель только для английского языка, их

79
00:02:52,560 --> 00:02:54,400
много  языков, на которых говорят в мире, и

80
00:02:54,400 --> 00:02:56,959
что происходит, когда мы модифицируем t5 так, чтобы

81
00:02:56,959 --> 00:02:59,680
это была массово многоязычная модель,

82
00:02:59,680 --> 00:03:01,200
а затем я расскажу о другой статье, в

83
00:03:01,200 --> 00:03:03,200
которой мы попытаемся исследовать, какие

84
00:03:03,200 --> 00:03:05,280
виды знаний и сколько знаний

85
00:03:05,280 --> 00:03:06,879
собирает модель  в ходе

86
00:03:06,879 --> 00:03:08,080
предварительного обучения,

87
00:03:08,080 --> 00:03:10,319
а затем, в связи с этим, в последующей работе

88
00:03:10,319 --> 00:03:12,239
мы попытались выяснить, действительно ли модель

89
00:03:12,239 --> 00:03:14,400
запоминает данные обучения во время

90
00:03:14,400 --> 00:03:16,560
предварительного обучения, если это на самом деле, если мы можем

91
00:03:16,560 --> 00:03:18,560
заставить ее выдавать дословные

92
00:03:18,560 --> 00:03:20,640
записи из набора данных обучения

93
00:03:20,640 --> 00:03:22,400
после обучения,

94
00:03:22,400 --> 00:03:24,080
а затем, наконец, я расскажу об очень

95
00:03:24,080 --> 00:03:26,319
недавней работе, которая по духу похожа

96
00:03:26,319 --> 00:03:29,120
на статью t5, где цель состоит в том, чтобы ответить

97
00:03:29,120 --> 00:03:31,040
не на то, какие методы трансферного обучения работают

98
00:03:31,040 --> 00:03:33,120
лучше всего, а на то, какие изменения в

99
00:03:33,120 --> 00:03:35,040
архитектуре трансформатора,

100
00:03:35,040 --> 00:03:38,000
которые предложили люди, работают лучше всего

101
00:03:38,000 --> 00:03:40,159
так что просто чтобы мотивировать это, я на самом деле

102
00:03:40,159 --> 00:03:41,760
просмотрел некоторые из лекций, которые

103
00:03:41,760 --> 00:03:43,360
вы все уже читали, просто чтобы получить

104
00:03:43,360 --> 00:03:45,360
представление о том, что вы уже узнали, и

105
00:03:45,360 --> 00:03:47,360
я знаю, что вы все уже

106
00:03:47,360 --> 00:03:48,959
хорошо знакомы с этой

107
00:03:48,959 --> 00:03:51,040
парадигмой трансферного обучения, которая имеет  как бы штурмом

108
00:03:51,040 --> 00:03:52,879
занялись обработкой естественного языка,

109
00:03:52,879 --> 00:03:56,080
но для того, чтобы

110
00:03:56,080 --> 00:03:58,720
освежить этот стиль передачи,

111
00:03:58,720 --> 00:04:00,159
мы обычно берем кучу

112
00:04:00,159 --> 00:04:02,080
немаркированных текстовых

113
00:04:02,080 --> 00:04:03,519
данных и  d мы

114
00:04:03,519 --> 00:04:05,840
применяем некоторую неконтролируемую цель, или вы

115
00:04:05,840 --> 00:04:07,840
могли бы сказать самоконтролируемую задачу,

116
00:04:07,840 --> 00:04:09,519
когда вы делаете что-то вроде того, что вы маскируете

117
00:04:09,519 --> 00:04:11,120
слова наугад, а затем вы обучаете

118
00:04:11,120 --> 00:04:12,799
модель предсказывать пропущенные слова, чтобы

119
00:04:12,799 --> 00:04:14,879
вы могли видеть эти пробелы в

120
00:04:14,879 --> 00:04:17,120
блоке текста в  зеленый и пропущенные

121
00:04:17,120 --> 00:04:18,720
слова, которые

122
00:04:18,720 --> 00:04:20,560
будет обучена предсказывать языковая модель в желтом

123
00:04:20,560 --> 00:04:22,639
поле внизу, а затем, после того, как мы проведем

124
00:04:22,639 --> 00:04:24,400
это предварительное обучение в течение некоторого времени, мы

125
00:04:24,400 --> 00:04:26,400
тонко настраиваем модель для некоторой последующей

126
00:04:26,400 --> 00:04:28,960
контролируемой задачи

127
00:04:28,960 --> 00:04:30,880
в этом примере i  Я показываю

128
00:04:30,880 --> 00:04:33,360
задачу анализа настроений для обзоров фильмов,

129
00:04:33,360 --> 00:04:35,199
и результатом всего этого является то, что

130
00:04:35,199 --> 00:04:36,880
выполнение этого первого неконтролируемого

131
00:04:36,880 --> 00:04:39,120
предтренировочного шага просто смехотворно

132
00:04:39,120 --> 00:04:40,720
полезно, не только

133
00:04:40,720 --> 00:04:42,720
обычно оно улучшает производительность, но и часто дает вам

134
00:04:42,720 --> 00:04:44,400
очень хорошую производительность при относительно

135
00:04:44,400 --> 00:04:46,720
небольших затратах.  э-э, точная настройка данных по сравнению с

136
00:04:46,720 --> 00:04:48,880
обучением с нуля, так что это

137
00:04:48,880 --> 00:04:51,280
действительно очень распространенный способ де-факто

138
00:04:51,280 --> 00:04:53,120
стандартного способа атаковать многие

139
00:04:53,120 --> 00:04:56,080
проблемы обработки естественного языка сейчас,

140
00:04:56,080 --> 00:04:58,560
и я  Я думаю, вы уже рассмотрели

141
00:04:58,560 --> 00:05:01,919
некоторые из этих методов, но из-за того, насколько

142
00:05:01,919 --> 00:05:03,919
эффективен рецепт трансферного обучения,

143
00:05:03,919 --> 00:05:05,600
действительно произошел своего рода

144
00:05:05,600 --> 00:05:08,000
взрыв работы по трансферному обучению,

145
00:05:08,000 --> 00:05:10,400
начиная, может быть, с 2018 года или около того,

146
00:05:10,400 --> 00:05:12,639
есть некоторая предшествующая работа над другими

147
00:05:12,639 --> 00:05:14,320
подходами к осуществлению передачи  обучение,

148
00:05:14,320 --> 00:05:16,639
как словесные векторы, такие как слово, для поддержки

149
00:05:16,639 --> 00:05:18,240
какой-то предварительной работы, которая

150
00:05:18,240 --> 00:05:19,840
предлагала рецепт, который я только что

151
00:05:19,840 --> 00:05:21,759
описал, под названием полу-контролируемое

152
00:05:21,759 --> 00:05:23,680
последовательное обучение, э-э, некоторая работа, которая

153
00:05:23,680 --> 00:05:25,120
предполагает, что такого рода вещи могут

154
00:05:25,120 --> 00:05:26,720
быть действительно возможны, как

155
00:05:26,720 --> 00:05:28,479
бумага для неконтролируемого нейрона настроения,

156
00:05:28,479 --> 00:05:30,880
но я бы сказал  Примерно в 2018 году

157
00:05:30,880 --> 00:05:32,560
появилась серия документов, которые

158
00:05:32,560 --> 00:05:34,880
положили начало ажиотажу в этой области, в

159
00:05:34,880 --> 00:05:36,639
том числе статья о тонкой настройке универсальной языковой модели,

160
00:05:36,639 --> 00:05:39,199
в которой содержалась статья Аламо,

161
00:05:39,199 --> 00:05:41,680
которую мы сейчас называем gpt-1, и, конечно же,

162
00:05:41,680 --> 00:05:44,160
статья Берта в конце 2018 года,

163
00:05:44,160 --> 00:05:46,160
а затем  начиная с 2019 года действительно

164
00:05:46,160 --> 00:05:48,080
произошел просто невероятный взрыв

165
00:05:48,080 --> 00:05:50,400
различных методов трансферного

166
00:05:50,400 --> 00:05:52,479
обучения, включая новое трансферное обучение.

167
00:05:52,479 --> 00:05:55,360
извините, новые цели перед обучением новые

168
00:05:55,360 --> 00:05:58,160
данные устанавливают новые способы выполнения тонкой настройки

169
00:05:58,160 --> 00:05:59,520
и так далее,

170
00:05:59,520 --> 00:06:01,120
и

171
00:06:01,120 --> 00:06:03,280
мы начали работать над проектом t5 в

172
00:06:03,280 --> 00:06:06,080
конце 2018 года, и мы заметили, что по мере

173
00:06:06,080 --> 00:06:07,759
появления всех этих методов

174
00:06:07,759 --> 00:06:09,199
становилось все труднее и труднее понять

175
00:06:09,199 --> 00:06:12,319
выяснить, что на самом деле работает лучше всего, и

176
00:06:13,360 --> 00:06:14,720
отчасти причина этого заключалась в том,

177
00:06:14,720 --> 00:06:16,240
что было так много методов,

178
00:06:16,240 --> 00:06:17,759
которые предлагались

179
00:06:17,759 --> 00:06:20,000
одновременно, и когда это происходит,

180
00:06:20,000 --> 00:06:22,080
даже когда все работают

181
00:06:22,080 --> 00:06:24,479
всерьез, вы знаете, с доброй волей, что у

182
00:06:24,479 --> 00:06:25,919
вас может быть  ситуации, подобные этой,

183
00:06:25,919 --> 00:06:28,000
когда вы знаете, что бумага, одна статья приходит

184
00:06:28,000 --> 00:06:29,919
вместе с бумагой, которая предлагает новую

185
00:06:29,919 --> 00:06:31,680
неконтролируемую предварительную подготовку, например, технику

186
00:06:31,680 --> 00:06:34,000
под названием fancy learn, другая бумага приходит,

187
00:06:34,000 --> 00:06:36,080
возможно, примерно в то же время с

188
00:06:36,080 --> 00:06:37,520
предварительной техникой, называемой фантазией или

189
00:06:37,520 --> 00:06:39,520
обучением и бумажной предварительной подготовкой.  тренируется в

190
00:06:39,520 --> 00:06:41,840
Википедии для немаркированных данных, в то время как в статье

191
00:06:41,840 --> 00:06:43,840
b используется Википедия и корпус книг Торонто,

192
00:06:43,840 --> 00:06:45,600
который представляет собой собрание нового

193
00:06:45,600 --> 00:06:46,800
текста,

194
00:06:46,800 --> 00:06:48,720
и тогда вопрос, очевидно, будет

195
00:06:48,720 --> 00:06:50,800
интереснее l  зарабатывать лучше, чем на воображение,

196
00:06:50,800 --> 00:06:52,319
хорошо учиться, трудно сказать, потому что они используют

197
00:06:52,319 --> 00:06:54,479
разные источники данных для предварительного обучения,

198
00:06:54,479 --> 00:06:56,000
вы можете представить, что, возможно, они используют

199
00:06:56,000 --> 00:06:57,919
модели разных размеров,

200
00:06:57,919 --> 00:06:59,440
возможно, они предварительно обучаются в течение разного

201
00:06:59,440 --> 00:07:00,720
времени,

202
00:07:00,720 --> 00:07:02,560
они используют разные оптимизаторы, есть

203
00:07:02,560 --> 00:07:04,400
тонны дизайна  решения, которые здесь вступают

204
00:07:04,400 --> 00:07:06,479
в игру,

205
00:07:06,479 --> 00:07:08,000
и поэтому,

206
00:07:08,000 --> 00:07:10,000
учитывая, что эти дизайнерские решения могут

207
00:07:10,000 --> 00:07:11,599
затруднить определение того, что работает

208
00:07:11,599 --> 00:07:14,240
лучше всего, наша цель в документе t5 была своего

209
00:07:14,240 --> 00:07:16,560
рода отступить и просто сказать,

210
00:07:16,560 --> 00:07:18,800
учитывая текущий ландшафт трансферного

211
00:07:18,800 --> 00:07:20,000
обучения, вы знаете все  методы,

212
00:07:20,000 --> 00:07:22,000
которые люди предложили, что на самом деле

213
00:07:22,000 --> 00:07:24,080
работает лучше всего, когда мы сравниваем их в одной и

214
00:07:24,080 --> 00:07:26,000
той же конкретной обстановке,

215
00:07:26,000 --> 00:07:28,560
и когда мы узнаем, что работает лучше всего,

216
00:07:28,560 --> 00:07:30,560
насколько далеко мы можем продвинуть эти инструменты, которые у нас

217
00:07:30,560 --> 00:07:31,680
уже есть,

218
00:07:31,680 --> 00:07:32,720
и

219
00:07:32,720 --> 00:07:34,479
насколько мы можем изучить пределы и

220
00:07:34,479 --> 00:07:36,000
выяснить  насколько хорошо эти вещи работают в

221
00:07:36,000 --> 00:07:37,599
масштабе,

222
00:07:37,599 --> 00:07:40,000
и поэтому для решения этой проблемы мы были

223
00:07:40,000 --> 00:07:41,759
единственной вещью, которую мы

224
00:07:41,759 --> 00:07:43,759
ввели, поскольку мы снова как бы

225
00:07:43,759 --> 00:07:45,919
изучаем существующие методы,

226
00:07:45,919 --> 00:07:48,160
это  идея

227
00:07:48,160 --> 00:07:50,879
обрабатывать все текстовые проблемы в одном

228
00:07:50,879 --> 00:07:53,840
формате и такой

229
00:07:53,840 --> 00:07:57,039
подход, эта догма обработки каждой

230
00:07:57,039 --> 00:07:59,039
текстовой проблемы в одном и том же формате дает

231
00:07:59,039 --> 00:08:00,400
начало нашей модели, которую мы называем

232
00:08:00,400 --> 00:08:02,560
преобразователем преобразования текста в текст

233
00:08:02,560 --> 00:08:04,960
и, таким образом, объясняем этот формат

234
00:08:04,960 --> 00:08:07,840
Основная идея заключается в том, что мы представляем каждую

235
00:08:07,840 --> 00:08:11,199
текстовую проблему NLP как задачу преобразования текста в текст,

236
00:08:11,199 --> 00:08:13,280
и под этим я подразумеваю, что модель

237
00:08:13,280 --> 00:08:15,440
принимает текст в качестве входных данных и производит текст в

238
00:08:15,440 --> 00:08:17,919
качестве выходных данных, поэтому в таких вещах, как

239
00:08:17,919 --> 00:08:19,680
перевод с английского на немецкий, это  довольно

240
00:08:19,680 --> 00:08:22,000
типично, мы вводим

241
00:08:22,000 --> 00:08:24,000
английское предложение на входе, и мы

242
00:08:24,000 --> 00:08:25,280
обучаем модель предсказывать немецкое

243
00:08:25,280 --> 00:08:27,599
предложение на выходе,

244
00:08:27,599 --> 00:08:28,879
и вы заметите, что в нашем случае мы

245
00:08:28,879 --> 00:08:30,800
фактически также подаем то, что мы называем префиксом задачи,

246
00:08:30,800 --> 00:08:33,200
переводим английский на немецкий язык, который

247
00:08:33,200 --> 00:08:34,719
просто  сообщает модели, что мы хотели

248
00:08:34,719 --> 00:08:36,559
сделать с вводом, потому что, особенно

249
00:08:36,559 --> 00:08:38,479
если мы торгуем многозадачной моделью, если

250
00:08:38,479 --> 00:08:40,399
вы просто скармливаете модель, которая хороша,

251
00:08:40,399 --> 00:08:41,760
модель не знает, что с ней делать, она

252
00:08:41,760 --> 00:08:42,799
не знает,  Вы пытаетесь провести

253
00:08:42,799 --> 00:08:44,399
анализ настроений  это или

254
00:08:44,399 --> 00:08:46,720
английский перевод на немецкий язык, или что-то в

255
00:08:48,080 --> 00:08:49,760
этом роде, это не должно быть таким удивительным,

256
00:08:49,760 --> 00:08:52,000
пока вы, вероятно, узнали о

257
00:08:52,000 --> 00:08:53,519
моделях кодировщика-декодера, последовательность к

258
00:08:53,519 --> 00:08:55,120
моделям последовательности, и это в основном все,

259
00:08:55,120 --> 00:08:56,560
что мы здесь делаем,

260
00:08:56,560 --> 00:08:58,240
возможно, это станет немного более необычным, когда

261
00:08:58,240 --> 00:09:00,160
мы начнем  для решения таких вещей, как

262
00:09:00,160 --> 00:09:02,399
задачи классификации текста, так что это

263
00:09:02,399 --> 00:09:04,320
пример из теста cola,

264
00:09:04,320 --> 00:09:05,760
который является корпусом

265
00:09:05,760 --> 00:09:08,160
лингвистической приемлемости, и цель

266
00:09:08,160 --> 00:09:10,320
здесь состоит в том, чтобы взять предложение и определить, является

267
00:09:10,320 --> 00:09:12,080
ли предложение приемлемым цитатой без кавычек,

268
00:09:12,080 --> 00:09:14,240
что означает, является ли

269
00:09:14,240 --> 00:09:16,240
оно грамматически правильным и  также, если

270
00:09:16,240 --> 00:09:18,720
это чушь или нет, и в этом случае

271
00:09:19,760 --> 00:09:21,680
предложение гласит, что курс хорошо прыгает,

272
00:09:21,680 --> 00:09:23,680
и, конечно, курсы не могут прыгать, поэтому это

273
00:09:23,680 --> 00:09:25,519
предложение неприемлемо,

274
00:09:25,519 --> 00:09:27,600
но вместо обучения нашей модели

275
00:09:27,600 --> 00:09:29,600
через слой классификатора, который выводит

276
00:09:29,600 --> 00:09:31,440
метку класса или вероятность

277
00:09:31,440 --> 00:09:32,880
распределение по

278
00:09:32,880 --> 00:09:34,800
индексам классов мы фактически обучаем

279
00:09:34,800 --> 00:09:36,959
модель выводить буквальный текст

280
00:09:36,959 --> 00:09:39,440
неприемлемо, поэтому она выводит эту

281
00:09:39,440 --> 00:09:42,480
текстовую строку  токен за токеном,

282
00:09:42,480 --> 00:09:44,080
и это может стать немного даже немного

283
00:09:44,080 --> 00:09:45,839
страннее, мы можем атаковать такие вещи, как

284
00:09:45,839 --> 00:09:48,320
проблемы регрессии, где

285
00:09:48,320 --> 00:09:49,839
эффективно модель должна выводить значение с

286
00:09:49,839 --> 00:09:51,360
плавающей запятой,

287
00:09:51,360 --> 00:09:53,680
и мы фактически просто делаем это, беря

288
00:09:53,680 --> 00:09:55,200
числа с плавающей запятой и

289
00:09:55,200 --> 00:09:57,200
преобразовывая их в строки  и

290
00:09:57,200 --> 00:09:59,200
обучая модель просто предсказывать строку,

291
00:09:59,200 --> 00:10:00,800
и оказывается, что по крайней мере для этой

292
00:10:00,800 --> 00:10:03,600
конкретной задачи, которая является эталонным тестом stsb,

293
00:10:03,600 --> 00:10:05,920
он работает отлично, и в

294
00:10:05,920 --> 00:10:07,760
конечном итоге мы действительно получили современное

295
00:10:07,760 --> 00:10:09,760
состояние этого теста, поэтому делаем этот

296
00:10:09,760 --> 00:10:11,200
тип своего рода

297
00:10:11,200 --> 00:10:12,880
float  в строковое преобразование с небольшим

298
00:10:12,880 --> 00:10:14,640
количеством квантования, оказывается, отлично работает

299
00:10:14,640 --> 00:10:17,200
для проблем регрессии,

300
00:10:17,200 --> 00:10:19,279
и, наконец, вы знаете, что основная

301
00:10:19,279 --> 00:10:20,560
суть этого заключается в том, что существует

302
00:10:20,560 --> 00:10:22,079
множество действительно тонны и тонны проблем,

303
00:10:22,079 --> 00:10:23,680
которые мы можем преобразовать в этот формат, поэтому

304
00:10:23,680 --> 00:10:25,279
вот  пример абстрактного

305
00:10:25,279 --> 00:10:27,200
резюмирования, который мы подаем в новостной статье

306
00:10:27,200 --> 00:10:29,120
слева и предсказываем, что в

307
00:10:29,120 --> 00:10:30,480
сводке справа

308
00:10:30,480 --> 00:10:32,560
и снова мы действительно сможем атаковать все

309
00:10:32,560 --> 00:10:34,880
эти проблемы точно такие же,

310
00:10:34,880 --> 00:10:36,720
поэтому мы используем одну и ту же

311
00:10:36,720 --> 00:10:39,200
цель во время обучения и точно

312
00:10:39,200 --> 00:10:41,360
такую же процедуру декодирования

313
00:10:41,360 --> 00:10:44,560
во время тестирования, чтобы атаковать огромное количество

314
00:10:44,560 --> 00:10:46,480
проблем обработки естественного языка,

315
00:10:46,480 --> 00:10:48,079
и хорошая сторона в этом заключается в

316
00:10:48,079 --> 00:10:50,000
том, что до тех пор, пока

317
00:10:50,000 --> 00:10:51,680
Улучшение передачи обучения

318
00:10:51,680 --> 00:10:53,920
применимо к нашей модели, и к этому

319
00:10:53,920 --> 00:10:55,680
текстовому формату

320
00:10:55,680 --> 00:10:57,600
мы можем опробовать его на огромном наборе

321
00:10:57,600 --> 00:10:59,200
последующих

322
00:10:59,200 --> 00:11:01,519
задач, используя точно такую же модель,

323
00:11:01,519 --> 00:11:03,920
точно такую же процедуру обучения оптимизатора скорости

324
00:11:03,920 --> 00:11:05,839
обучения, точно такую же

325
00:11:05,839 --> 00:11:07,839
процедуру вывода, чтобы мы могли получить  Избавьтесь

326
00:11:07,839 --> 00:11:09,200
от тонны путаницы, о которых я

327
00:11:09,200 --> 00:11:11,200
упоминал ранее,

328
00:11:11,200 --> 00:11:12,320
они звонят,

329
00:11:12,320 --> 00:11:15,200
да, вы можете взять вторую эм,

330
00:11:15,200 --> 00:11:17,200
поскольку мы обсуждаем здесь форматирование,

331
00:11:17,200 --> 00:11:20,160
чтобы поговорить о том, как

332
00:11:20,160 --> 00:11:22,320
работает пример регрессии типа 3.8, и

333
00:11:22,320 --> 00:11:24,480
да, просто пройдите  что еще раз для

334
00:11:24,480 --> 00:11:26,160
студентов, да, абсолютно

335
00:11:26,160 --> 00:11:29,120
точно, поэтому в этой конкретной задаче stsb цель состоит в том,

336
00:11:29,120 --> 00:11:31,360
чтобы взять два предложения и предсказать число с

337
00:11:31,360 --> 00:11:33,120
плавающей запятой, которое обозначает, как

338
00:11:33,120 --> 00:11:35,360
s  похожи на эти два предложения, и это

339
00:11:35,360 --> 00:11:38,320
число с плавающей запятой находится в диапазоне от 1,0

340
00:11:38,320 --> 00:11:39,920
до 5,0,

341
00:11:39,920 --> 00:11:41,680
поэтому мы в основном взяли базовые

342
00:11:41,680 --> 00:11:44,320
аннотированные значения uh, которые мы

343
00:11:44,320 --> 00:11:46,240
должны регрессировать, мы квантовали их до

344
00:11:46,240 --> 00:11:49,839
ближайшего 0,2 и преобразовали его в строку,

345
00:11:49,839 --> 00:11:51,920
и теперь у нас есть  строка вроде три

346
00:11:51,920 --> 00:11:54,800
три периода восемь эээ 3.8 но вы можете

347
00:11:54,800 --> 00:11:56,320
думать об этом как три периода восемь или

348
00:11:56,320 --> 00:11:59,120
четыре периода ноль ээ, и мы просто

349
00:11:59,120 --> 00:12:01,600
обучаем модель предсказывать эту строку, которая в

350
00:12:01,600 --> 00:12:02,959
конечном итоге является строкой, вы знаете, что

351
00:12:02,959 --> 00:12:04,399
это больше не число, которое мы просто

352
00:12:04,399 --> 00:12:06,639
предсказываем  токен за токеном

353
00:12:06,639 --> 00:12:08,800
um, так что в некотором смысле вы можете думать

354
00:12:08,800 --> 00:12:10,320
об этом как о преобразовании проблемы регрессии в

355
00:12:10,320 --> 00:12:12,320
проблему классификации,

356
00:12:12,320 --> 00:12:14,240
потому что вы выполняете это квантование,

357
00:12:14,240 --> 00:12:16,320
но вы также в более широком смысле можете просто думать

358
00:12:16,320 --> 00:12:17,600
об этом как о преобразовании проблемы регрессии

359
00:12:17,600 --> 00:12:19,519
в текст  -to-text проблема,

360
00:12:19,519 --> 00:12:21,839
которую мы делаем,

361
00:12:21,839 --> 00:12:22,800
спасибо,

362
00:12:22,800 --> 00:12:23,600
да,

363
00:12:23,600 --> 00:12:25,600
это немного круто, но я обещаю, что это

364
00:12:25,600 --> 00:12:27,279
работает,

365
00:12:28,399 --> 00:12:30,560
и отлично, так что еще раз приятно

366
00:12:30,560 --> 00:12:32,160
использовать такую последовательность для

367
00:12:32,160 --> 00:12:34,160
преобразования текста в текст fo  rmat заключается в том, что на

368
00:12:34,160 --> 00:12:36,399
самом деле мы можем просто использовать оригинальный ванильный

369
00:12:36,399 --> 00:12:38,399
преобразователь, как это было предложено, потому что,

370
00:12:38,399 --> 00:12:40,480
если вы помните, что преобразователь на

371
00:12:40,480 --> 00:12:42,959
самом деле был предложен для английского языка, он

372
00:12:42,959 --> 00:12:44,560
был предложен в первую очередь для машинного перевода,

373
00:12:44,560 --> 00:12:46,639
который представляет собой задачу последовательности последовательности, которую

374
00:12:46,639 --> 00:12:48,959
вы знаете, принимая язык и

375
00:12:48,959 --> 00:12:51,360
предложение  на одном языке в качестве входных данных и

376
00:12:51,360 --> 00:12:52,959
создание соответствующего предложения на

377
00:12:52,959 --> 00:12:54,959
другом языке, поэтому я не буду

378
00:12:54,959 --> 00:12:56,560
много говорить о модели, которую мы используем. В

379
00:12:58,399 --> 00:13:00,320
стандартную архитектуру трансформатора,

380
00:13:00,320 --> 00:13:02,480
как было предложено изначально, мы внесли относительно мало изменений, и

381
00:13:02,480 --> 00:13:04,079
внимание - это все, что вы  нужно

382
00:13:04,079 --> 00:13:07,920
э-э, при построении нашей модели t5

383
00:13:07,920 --> 00:13:09,279
я буду ближе к концу разговора

384
00:13:09,279 --> 00:13:11,040
обсудить множество архитектурных

385
00:13:11,040 --> 00:13:12,480
модификаций, которые люди имели

386
00:13:12,480 --> 00:13:15,279
смысл, но для t5 для статьи t5 мы

387
00:13:15,279 --> 00:13:19,040
действительно в основном придерживались основ,

388
00:13:19,760 --> 00:13:21,279
так что следующий большой вопрос, когда вы

389
00:13:21,279 --> 00:13:23,920
атакуете  проблема трансферного обучения - это

390
00:13:23,920 --> 00:13:26,720
то, каким должен быть мой набор данных до обучения,

391
00:13:26,720 --> 00:13:28,959
и из-за Интернета есть

392
00:13:28,959 --> 00:13:31,040
много возможных кислых  массивы немаркированных

393
00:13:31,040 --> 00:13:33,680
текстовых данных.Один общий источник - это Википедия.

394
00:13:33,680 --> 00:13:35,200
Я показываю тонну статей из Википедии

395
00:13:35,200 --> 00:13:36,959
на экране,

396
00:13:36,959 --> 00:13:39,120
и

397
00:13:39,120 --> 00:13:41,120
при выполнении этого проекта одним из

398
00:13:41,120 --> 00:13:43,760
факторов, которые мы хотели изучить, было

399
00:13:43,760 --> 00:13:45,440
влияние самого набора данных до обучения

400
00:13:45,440 --> 00:13:47,680
и т.  мы фактически

401
00:13:47,680 --> 00:13:49,199
создали новый набор данных для предварительного обучения, который

402
00:13:49,199 --> 00:13:51,839
позволит нам изменять размер на многие

403
00:13:51,839 --> 00:13:54,560
порядки величины, а также имеет

404
00:13:54,560 --> 00:13:56,160
конвейер фильтрации, который позволит

405
00:13:56,160 --> 00:13:58,160
нам контролировать качество и тип данных,

406
00:13:58,160 --> 00:14:00,160
которые были предварительно обучены, и i '  Я

407
00:14:00,160 --> 00:14:03,680
опишу, как мы построили этот набор данных сейчас,

408
00:14:03,680 --> 00:14:06,240
поэтому первое, что мы сделали, это мы хотели

409
00:14:06,240 --> 00:14:07,279
получить

410
00:14:07,279 --> 00:14:08,959
наши данные из

411
00:14:08,959 --> 00:14:10,639
общедоступного источника, мы не хотели использовать

412
00:14:10,639 --> 00:14:12,320
какой-то внутренний веб-парсинг Google, который мы

413
00:14:12,320 --> 00:14:13,600
не могли выпустить,

414
00:14:13,600 --> 00:14:16,079
поэтому мы использовали  эти поисковые запросы, выполняемые

415
00:14:16,079 --> 00:14:17,920
некоммерческой организацией под названием common

416
00:14:17,920 --> 00:14:20,240
crawl, которая на самом деле является просто

417
00:14:20,240 --> 00:14:22,240
организацией, которая

418
00:14:22,240 --> 00:14:23,839
рассылает поисковые роботы через

419
00:14:23,839 --> 00:14:25,600
Интернет и загружает столько текста, сколько

420
00:14:25,600 --> 00:14:28,320
они могут, и каждый месяц они выгружают то,

421
00:14:28,320 --> 00:14:30,720
что они называют веб-электронной  xtracted текст, то

422
00:14:30,720 --> 00:14:32,720
есть вы можете думать о них как о веб-сайтах

423
00:14:32,720 --> 00:14:35,440
со всеми идеально удаленными HTML и javascript,

424
00:14:36,959 --> 00:14:38,800
и это создает

425
00:14:38,800 --> 00:14:40,560
текст с довольно приличным количеством

426
00:14:40,560 --> 00:14:42,560
естественного языка в нем, а также много

427
00:14:42,560 --> 00:14:45,360
шаблонного

428
00:14:45,360 --> 00:14:47,199
текста и текста меню, а также немного

429
00:14:47,199 --> 00:14:48,240
бред,

430
00:14:48,240 --> 00:14:49,760
но в целом это хорошая

431
00:14:49,760 --> 00:14:51,760
отправная точка для создания

432
00:14:51,760 --> 00:14:54,399
этих предварительно обученных наборов данных,

433
00:14:54,399 --> 00:14:56,320
а затем мы предприняли несколько шагов, чтобы

434
00:14:56,320 --> 00:14:58,560
попытаться сделать так, чтобы этот набор данных был

435
00:14:58,560 --> 00:15:00,560
немного чище, поэтому первое, что мы сделали,

436
00:15:00,560 --> 00:15:02,880
это  мы удалили все строки, которые не

437
00:15:02,880 --> 00:15:05,760
заканчивались знаком пунктуации терминала; мы использовали

438
00:15:05,760 --> 00:15:07,760
языковой классификатор, чтобы сохранить только

439
00:15:07,760 --> 00:15:09,360
английский текст;

440
00:15:09,360 --> 00:15:11,680
мы удалили все, что выглядело как

441
00:15:11,680 --> 00:15:13,839
текст-заполнитель, например текст laura mipson

442
00:15:13,839 --> 00:15:15,519
справа; мы удалили все, что

443
00:15:15,519 --> 00:15:18,320
выглядело как код, который мы дедуплицировали.  вещи

444
00:15:18,320 --> 00:15:21,120
на уровне предложения, поэтому в любое время, когда любой

445
00:15:21,120 --> 00:15:23,040
фрагмент текста появлялся на нескольких страницах,

446
00:15:23,040 --> 00:15:25,279
мы сохраняли его только на одной из страниц

447
00:15:25,279 --> 00:15:27,680
и так далее, и в конечном итоге эти

448
00:15:27,680 --> 00:15:29,839
эвристики были относительно простыми, но

449
00:15:29,839 --> 00:15:32,399
производимыми  ced достаточно чистый текст, и

450
00:15:32,399 --> 00:15:34,320
позже я буду обсуждать влияние

451
00:15:34,320 --> 00:15:36,320
этих вариантов очистки, которая была одним

452
00:15:36,320 --> 00:15:39,279
из экспериментов, которые мы проводили,

453
00:15:39,279 --> 00:15:41,360
и поэтому после этого мы создали этот

454
00:15:41,360 --> 00:15:43,680
набор данных под названием c4, который представляет собой колоссальный

455
00:15:43,680 --> 00:15:45,600
корпус чистого сканирования,

456
00:15:45,600 --> 00:15:47,600
и он доступен в тензорном потоке  наборы данных,

457
00:15:47,600 --> 00:15:48,560
которые

458
00:15:48,560 --> 00:15:50,000
вам на самом деле нужно

459
00:15:50,000 --> 00:15:51,759
обрабатывать самостоятельно, что довольно

460
00:15:51,759 --> 00:15:53,759
дорого с вычислительной точки зрения,

461
00:15:53,759 --> 00:15:55,120
но, тем не менее,

462
00:15:55,120 --> 00:15:57,680
это вполне возможно и дает

463
00:15:57,680 --> 00:16:00,800
около 750 гигабайт

464
00:16:00,800 --> 00:16:04,639
достаточно чистых естественных текстовых данных.

465
00:16:08,959 --> 00:16:10,320
нужна наша предтренировочная

466
00:16:10,320 --> 00:16:13,440
цель, что мы собираемся делать,

467
00:16:13,440 --> 00:16:15,839
чтобы обучить модель на нашем немаркированном тексте,

468
00:16:15,839 --> 00:16:16,720
и чтобы

469
00:16:16,720 --> 00:16:19,040
просто объяснить цель, которую мы

470
00:16:19,040 --> 00:16:20,399
выбрали для нашей базовой

471
00:16:20,399 --> 00:16:22,079
экспериментальной процедуры, которую мы снова

472
00:16:22,079 --> 00:16:23,279
будем экспериментировать с другими

473
00:16:23,279 --> 00:16:25,279
предтренировочными целями позже

474
00:16:25,279 --> 00:16:27,519
представьте, что у вас есть какое-то оригинальное

475
00:16:27,519 --> 00:16:28,800
текстовое предложение, например, спасибо, что

476
00:16:28,800 --> 00:16:31,199
пригласили меня на вечеринку на прошлой неделе,

477
00:16:31,199 --> 00:16:33,120
и что мы делаем, это мы ба  Обычно мы выбираем

478
00:16:33,120 --> 00:16:35,040
некоторые слова наугад, и технически

479
00:16:35,040 --> 00:16:37,440
мы выбираем токены наугад,

480
00:16:37,440 --> 00:16:39,040
но пока давайте просто предположим, что токены

481
00:16:39,040 --> 00:16:40,160
- это слова,

482
00:16:40,160 --> 00:16:42,639
и мы собираемся выбросить эти токены,

483
00:16:42,639 --> 00:16:44,240
и в итоге мы получим что-то

484
00:16:44,240 --> 00:16:46,720
похожее на это мы для каждого  последовательный

485
00:16:46,720 --> 00:16:48,240
диапазон выпавших токенов,

486
00:16:48,240 --> 00:16:50,399
мы заменяем его на сторожевой маркер,

487
00:16:50,399 --> 00:16:52,800
и каждый сторожевой маркер получает уникальный

488
00:16:52,800 --> 00:16:54,959
индекс, поэтому один из них, который мы называем, назовем

489
00:16:54,959 --> 00:16:56,560
его дозорным x, а другой

490
00:16:56,560 --> 00:16:58,959
будет сторожевым y, и вы увидите

491
00:16:58,959 --> 00:17:01,120
что, поскольку слова для и приглашения

492
00:17:01,120 --> 00:17:03,040
- это следующие слова,

493
00:17:03,040 --> 00:17:04,880
которые мы решили замаскировать случайным образом

494
00:17:04,880 --> 00:17:06,400
маскирующими словами,

495
00:17:06,400 --> 00:17:07,599
мы собираемся заменить оба этих

496
00:17:07,599 --> 00:17:09,599
слова одним единственным

497
00:17:09,599 --> 00:17:11,520
уникальным сигнальным маркером,

498
00:17:11,520 --> 00:17:13,439
и тогда цель модели будет заключаться в том,

499
00:17:13,439 --> 00:17:15,039
чтобы  заполните пробелы, и поэтому, если вы

500
00:17:15,039 --> 00:17:16,799
знакомы с целью предварительной тренировки

501
00:17:16,799 --> 00:17:19,119
Bert, это в некоторой степени похоже на тот

502
00:17:19,119 --> 00:17:21,039
факт, что мы сворачиваем последующие

503
00:17:21,039 --> 00:17:23,599
токены в промежуток, который мы

504
00:17:23,599 --> 00:17:25,439
собираемся заменить слова f  rom немного

505
00:17:25,439 --> 00:17:26,640
отличается,

506
00:17:26,640 --> 00:17:28,079
и тот факт, что мы реконструируем

507
00:17:28,079 --> 00:17:29,440
только недостающие слова, а не

508
00:17:29,440 --> 00:17:31,039
всю последовательность, возможно,

509
00:17:31,039 --> 00:17:33,679
тоже немного отличается,

510
00:17:33,679 --> 00:17:34,960
хорошо, так что

511
00:17:34,960 --> 00:17:36,400
теперь я расскажу о нашей

512
00:17:36,400 --> 00:17:38,000
базовой экспериментальной процедуре, которую

513
00:17:38,000 --> 00:17:39,120
мы собираемся использовать.

514
00:17:39,120 --> 00:17:41,120
со временем

515
00:17:41,120 --> 00:17:44,240
и по разным осям, чтобы

516
00:17:44,240 --> 00:17:46,640
исследовать ландшафт трансферного обучения, по

517
00:17:46,640 --> 00:17:49,200
крайней мере, примерно, когда вышла эта статья,

518
00:17:49,200 --> 00:17:50,880
поэтому для предварительного обучения модели мы собираемся

519
00:17:50,880 --> 00:17:53,600
взять модель, которая имеет кодировщик размера на основе

520
00:17:53,600 --> 00:17:55,760
Берта  и декодер, так что технически имеет

521
00:17:55,760 --> 00:17:57,919
вдвое больше параметров, чем база bert,

522
00:17:57,919 --> 00:17:59,200
потому что есть кодировщик размера на основе птицы

523
00:17:59,200 --> 00:18:01,520
и декодер базового размера,

524
00:18:01,520 --> 00:18:02,960
мы собираемся использовать цель шумоподавления,

525
00:18:02,960 --> 00:18:04,400
своего рода цель моделирования массового языка,

526
00:18:04,400 --> 00:18:06,880
которую я только что описал,

527
00:18:06,880 --> 00:18:08,400
и мы собираемся  чтобы применить его к

528
00:18:08,400 --> 00:18:10,400
набору данных c4, о котором я упоминал ранее, мы

529
00:18:10,400 --> 00:18:11,840
собираемся предварительно

530
00:18:11,840 --> 00:18:14,320
обучить около 34 миллиардов токенов, что составляет

531
00:18:14,320 --> 00:18:16,400
примерно четверть времени, пока база burt была

532
00:18:16,400 --> 00:18:17,919
обучена, так что это не тонна

533
00:18:17,919 --> 00:18:19,520
предтренировочного времени, но потому что  Поскольку мы

534
00:18:19,520 --> 00:18:20,880
тренируемся, мы проводим так много

535
00:18:20,880 --> 00:18:22,559
экспериментов, что нам нужно было немного сократить его,

536
00:18:24,160 --> 00:18:25,919
мы собираемся использовать график скорости обучения с обратным квадратом,

537
00:18:25,919 --> 00:18:27,440
который, как оказалось,

538
00:18:27,440 --> 00:18:28,960
достаточно хорошо работает в наших условиях,

539
00:18:28,960 --> 00:18:30,720
но это не так уж важно  проектное

540
00:18:30,720 --> 00:18:32,640
решение,

541
00:18:32,640 --> 00:18:35,200
а затем мы доработаем различные

542
00:18:35,200 --> 00:18:36,799
последующие задачи, тип задач, о которых

543
00:18:36,799 --> 00:18:38,640
люди очень заботились в то время, когда

544
00:18:38,640 --> 00:18:40,160
есть тест Glue, который является своего

545
00:18:40,160 --> 00:18:42,640
рода мета-тестом для многих отдельных

546
00:18:42,640 --> 00:18:45,280
последующих задач, таких как cola и stsb, которые

547
00:18:45,280 --> 00:18:47,520
я  уже упоминалось, что это то, что некоторые

548
00:18:47,520 --> 00:18:48,960
люди могут назвать задачами понимания естественного языка,

549
00:18:48,960 --> 00:18:51,440
но

550
00:18:51,440 --> 00:18:52,720
по большей части вы можете думать о них

551
00:18:52,720 --> 00:18:55,120
как о классификации предложений, задачах классификации пар предложений

552
00:18:55,120 --> 00:18:58,880
или регрессии,

553
00:18:58,880 --> 00:19:00,640
мы также рассматриваем корпус абстрактивного суммирования ежедневной почты cnn,

554
00:19:00,640 --> 00:19:02,720
это

555
00:19:02,720 --> 00:19:04,080
проблема последовательности для последовательности, где

556
00:19:04,080 --> 00:19:05,840
вам дается новостная статья, и вы должны

557
00:19:05,840 --> 00:19:07,360
вывести сводку

558
00:19:07,360 --> 00:19:09,360
теста ответов на вопросы отряда,

559
00:19:09,360 --> 00:19:10,640
который является тестом понимания прочитанного,

560
00:19:10,640 --> 00:19:13,039
где y  Вам дан абзац,

561
00:19:13,039 --> 00:19:14,320
и вы должны ответить на вопрос

562
00:19:14,320 --> 00:19:16,240
о абзаце, который вы можете атаковать

563
00:19:16,240 --> 00:19:18,160
в экстрактивной настройке, где вы

564
00:19:18,160 --> 00:19:20,080
извлекаете ответ из абзаца, или

565
00:19:20,080 --> 00:19:21,840
в абстрактной настройке, где вы просто

566
00:19:21,840 --> 00:19:23,360
выводите ответ, который

567
00:19:23,360 --> 00:19:25,919
мы используем в абстрактной форме, потому что он

568
00:19:25,919 --> 00:19:27,760
текст в текст,

569
00:19:27,760 --> 00:19:29,120
мы также включили

570
00:19:29,120 --> 00:19:30,720
тест Super Glue, который был новым тестом в

571
00:19:30,720 --> 00:19:32,240
то время, когда он был разработан

572
00:19:32,240 --> 00:19:33,840
как более сложная версия

573
00:19:33,840 --> 00:19:36,000
теста Glue, у него есть новый набор задач, которые

574
00:19:36,000 --> 00:19:38,799
были трудными для существующих моделей,

575
00:19:38,799 --> 00:19:40,160
а затем, наконец,  мы включили три

576
00:19:40,160 --> 00:19:42,559
набора данных о переводе с английского на немецкий с

577
00:19:42,559 --> 00:19:43,600
английского на французский и с английского на

578
00:19:43,600 --> 00:19:45,679
румынский перевод uh с английского на

579
00:19:45,679 --> 00:19:48,400
французский является самым большим

580
00:19:48,400 --> 00:19:49,919
набором данных с английского на румынский, который был очень большим набором данных с английского на

581
00:19:49,919 --> 00:19:51,840
румынский b он был на много

582
00:19:51,840 --> 00:19:54,000
порядков меньше,

583
00:19:54,000 --> 00:19:55,840
и мы собираемся в порядке  настраиваться на каждую из

584
00:19:55,840 --> 00:19:57,919
этих задач индивидуально и по отдельности,

585
00:19:57,919 --> 00:19:59,360
поэтому мы берем предварительно обученную модель и

586
00:19:59,360 --> 00:20:00,799
отдельно настраиваем ее для каждой из этих

587
00:20:00,799 --> 00:20:02,480
последующих задач и w  Мы собираемся выполнить

588
00:20:02,480 --> 00:20:05,679
точную настройку до 17 миллиардов токенов,

589
00:20:05,679 --> 00:20:06,960
но мы собираемся сохранять контрольные точки

590
00:20:06,960 --> 00:20:09,039
по пути, оценивать каждую контрольную точку

591
00:20:09,039 --> 00:20:10,640
в наборе проверки и сообщать о

592
00:20:10,640 --> 00:20:12,799
производительности на лучшей, э, лучшей

593
00:20:12,799 --> 00:20:16,320
контрольной точке, и обратите внимание, что это не

594
00:20:16,320 --> 00:20:18,240
экспериментально допустимый способ сообщить о своей

595
00:20:18,240 --> 00:20:19,520
производительности, потому что вы в основном

596
00:20:19,520 --> 00:20:21,280
выбираете модель для набора данных, в

597
00:20:21,280 --> 00:20:24,240
котором вы являетесь разделением данных, которое вы представляете.

598
00:20:28,240 --> 00:20:29,760
хороший способ для сравнения

599
00:20:29,760 --> 00:20:31,360
различных методов, но для сравнения внутри

600
00:20:31,360 --> 00:20:34,400
методов, это э-э, нам достаточно

601
00:20:34,400 --> 00:20:37,200
удобно делать это,

602
00:20:37,520 --> 00:20:39,840
поэтому теперь я собираюсь дать вам

603
00:20:39,840 --> 00:20:41,919
очень высокий обзор некоторых

604
00:20:41,919 --> 00:20:43,840
экспериментальных результатов в этой статье.

605
00:20:43,840 --> 00:20:45,280
бумага

606
00:20:45,280 --> 00:20:47,440
довольно огромна с точки зрения

607
00:20:47,440 --> 00:20:49,760
количества экспериментов, которые мы проводили, и поэтому, если бы

608
00:20:49,760 --> 00:20:52,159
я действительно углубился в эту статью, это,

609
00:20:52,159 --> 00:20:53,360
вероятно, заняло бы у меня все время,

610
00:20:53,360 --> 00:20:54,720
но есть и другие забавные вещи, о которых я хочу

611
00:20:54,720 --> 00:20:56,240
вам рассказать, поэтому

612
00:20:56,240 --> 00:20:58,240
но в любом случае суть в том, что я буду

613
00:20:58,240 --> 00:21:00,240
показывать вам множество таблиц, подобных этой,

614
00:21:00,240 --> 00:21:02,480
и поэтому в этих таблицах в столбцах у

615
00:21:02,480 --> 00:21:03,840
вас есть производительность по различным

616
00:21:03,840 --> 00:21:05,919
последующим задачам, а в строках у вас

617
00:21:05,919 --> 00:21:07,200
есть разные экспериментальные настройки,

618
00:21:07,200 --> 00:21:08,880
которые мы считали

619
00:21:08,880 --> 00:21:10,559
так, чтобы дать вам  Пример здесь - это своего

620
00:21:10,559 --> 00:21:13,679
рода оценки, которые мы получили

621
00:21:13,679 --> 00:21:16,400
от нашей базовой линии, которая является точной

622
00:21:16,400 --> 00:21:17,840
экспериментальной процедурой, которую я должен

623
00:21:17,840 --> 00:21:21,120
описать, которую я только что описал, и я

624
00:21:21,120 --> 00:21:22,960
также запускал эту базовую линию 10 раз и

625
00:21:22,960 --> 00:21:24,400
сообщал о стандартном отклонении во

626
00:21:24,400 --> 00:21:26,400
второй строке, а затем в  в этой последней

627
00:21:26,400 --> 00:21:28,960
строке мы сообщаем о производительности

628
00:21:28,960 --> 00:21:31,520
той же модели без какого-либо предварительного обучения,

629
00:21:31,520 --> 00:21:33,440
просто в основном только обученные отдельно

630
00:21:33,440 --> 00:21:34,799
с наблюдением за всеми этими

631
00:21:34,799 --> 00:21:36,720
последующими задачами,

632
00:21:36,720 --> 00:21:38,480
и

633
00:21:38,480 --> 00:21:40,080
просто чтобы указать на пару вещей в

634
00:21:40,080 --> 00:21:41,679
этой таблице, первая очевидная вещь заключается в

635
00:21:41,679 --> 00:21:44,720
том, что  в большинстве случаев проповедь о том, что

636
00:21:44,720 --> 00:21:46,400
предварительная подготовка отсутствует, значительно

637
00:21:46,400 --> 00:21:48,559
хуже, поэтому на самом деле трансферное обучение,

638
00:21:48,559 --> 00:21:51,760
как правило, полезно там, где

639
00:21:51,760 --> 00:21:53,280
это не так,  На самом деле в этой

640
00:21:53,280 --> 00:21:55,280
задаче перевода разницы на английский язык, и

641
00:21:55,280 --> 00:21:56,799
это, вероятно, потому, что это настолько большая

642
00:21:56,799 --> 00:21:58,240
задача, что вам на самом деле не

643
00:21:58,240 --> 00:22:00,799
нужно предварительное обучение, чтобы делать э-э, чтобы преуспеть

644
00:22:00,799 --> 00:22:02,799
в ней, мы хотели включить это, потому что,

645
00:22:02,799 --> 00:22:04,559
если производительность упадет в этой

646
00:22:04,559 --> 00:22:06,240
задаче  то мы

647
00:22:06,240 --> 00:22:08,559
должны побеспокоиться

648
00:22:08,559 --> 00:22:10,559
о следующей особенности этой таблицы, которую следует заметить,

649
00:22:10,559 --> 00:22:12,799
- появлении этой маленькой звездочки, которая

650
00:22:12,799 --> 00:22:14,640
будет появляться каждый раз, когда в таблице есть строка,

651
00:22:14,640 --> 00:22:15,840
эквивалентная нашей

652
00:22:15,840 --> 00:22:17,200
базовой линии,

653
00:22:17,200 --> 00:22:19,520
и еще одна мелочь, которую следует отметить, может быть,

654
00:22:19,520 --> 00:22:21,679
если вы  знаком с историей,

655
00:22:21,679 --> 00:22:23,520
оценка, которую мы получили за клей и отряд, была

656
00:22:23,520 --> 00:22:25,120
достаточно сопоставима с Bert, так что это

657
00:22:25,120 --> 00:22:26,880
достойная проверка здравомыслия, вы знаете, что у нас есть

658
00:22:26,880 --> 00:22:30,000
модель с большим количеством параметров,

659
00:22:30,000 --> 00:22:32,400
но она обучается только

660
00:22:32,400 --> 00:22:33,600
четверть времени

661
00:22:33,600 --> 00:22:35,840
и, тем не менее, имеет

662
00:22:35,840 --> 00:22:37,760
сопоставимую производительность  которые используют

663
00:22:37,760 --> 00:22:39,200
аналогичную цель, поэтому мы не должны слишком

664
00:22:39,200 --> 00:22:40,480
удивляться этому,

665
00:22:40,480 --> 00:22:41,679
а затем последнее, что нужно упомянуть, это

666
00:22:41,679 --> 00:22:43,039
то, что мы собираемся использовать это стандартное

667
00:22:43,039 --> 00:22:45,440
отклонение снова и снова.  r снова, чтобы мы

668
00:22:45,440 --> 00:22:46,960
могли выделить записи в таблице, выделенные жирным шрифтом, когда

669
00:22:46,960 --> 00:22:48,320
они находятся в пределах одного стандартного отклонения

670
00:22:48,320 --> 00:22:50,159
от максимального значения для этого набора данных

671
00:22:50,159 --> 00:22:51,760
в таблице,

672
00:22:51,760 --> 00:22:53,280
поэтому теперь я просто сделаю большой отказ от ответственности,

673
00:22:53,280 --> 00:22:55,039
и мы собираемся сравнить множество

674
00:22:55,039 --> 00:22:56,400
разные вещи, мы собираемся провести

675
00:22:56,400 --> 00:22:58,240
множество экспериментов, но мы не собираемся

676
00:22:58,240 --> 00:23:00,240
настраивать какие-либо гиперпараметры, потому что, если бы мы

677
00:23:00,240 --> 00:23:01,039
действительно

678
00:23:01,039 --> 00:23:03,039
хотели изменить скорость обучения или

679
00:23:03,039 --> 00:23:04,480
что-то еще,

680
00:23:04,480 --> 00:23:06,159
это было бы слишком затратно в вычислительном отношении,

681
00:23:06,159 --> 00:23:07,679
чтобы сделать это для каждого отдельного

682
00:23:07,679 --> 00:23:10,400
метода, мы надеемся, что  что это нормально,

683
00:23:10,400 --> 00:23:13,440
потому что мы обрабатываем все проблемы в

684
00:23:13,440 --> 00:23:15,679
одной и той же среде,

685
00:23:15,679 --> 00:23:17,679
мы всегда проводим обучение максимальной вероятности текста в текст,

686
00:23:17,679 --> 00:23:19,760
поэтому, надеюсь, мы сможем

687
00:23:19,760 --> 00:23:21,679
сохранить фиксированные гиперпараметры и, возможно,

688
00:23:21,679 --> 00:23:22,960
если вы предложите новый метод, который

689
00:23:22,960 --> 00:23:24,559
требует обширной настройки гиперпериметра,

690
00:23:24,559 --> 00:23:26,480
он  не очень полезный метод для

691
00:23:26,480 --> 00:23:28,000
практиков,

692
00:23:28,000 --> 00:23:29,120
и мы поговорим об этом

693
00:23:29,120 --> 00:23:30,159
чуть позже, когда я буду говорить и об

694
00:23:30,159 --> 00:23:32,720
архитектурных модификациях.

695
00:23:35,840 --> 00:23:37,520
Мы не можем быть исчерпывающими только

696
00:23:37,520 --> 00:23:39,600
потому, что было так

697
00:23:39,600 --> 00:23:42,000
много методов, а включение или

698
00:23:42,000 --> 00:23:44,080
исключение одного конкретного метода

699
00:23:44,080 --> 00:23:46,640
не означает суждение о его качестве,

700
00:23:46,640 --> 00:23:48,960
это просто, ммм, это то, что мы смогли  с

701
00:23:48,960 --> 00:23:50,880
учетом ограничений, с которыми мы

702
00:23:50,880 --> 00:23:53,200
работали,

703
00:23:53,200 --> 00:23:55,760
поэтому первая серия экспериментов, которые мы

704
00:23:55,760 --> 00:23:57,840
провели, заключалась в сравнении различных структур моделей,

705
00:23:57,840 --> 00:24:01,440
поэтому, как я упоминал ранее,

706
00:24:01,440 --> 00:24:03,679
основная базовая модель t5 - это

707
00:24:03,679 --> 00:24:05,600
модель декодера кодировщика, и в этом случае у вас

708
00:24:05,600 --> 00:24:07,440
есть отдельный стек слоев

709
00:24:07,440 --> 00:24:09,600
для  который кодирует последовательность и

710
00:24:09,600 --> 00:24:11,360
отдельный стек слоев, который декодирует

711
00:24:11,360 --> 00:24:13,600
целевую последовательность, в основном он

712
00:24:13,600 --> 00:24:16,000
генерирует целевую последовательность по одному токену за токеном,

713
00:24:16,000 --> 00:24:18,159
обращаясь к выходным данным кодера,

714
00:24:18,159 --> 00:24:19,760
чтобы выяснить, что он должен

715
00:24:21,039 --> 00:24:22,480
обусловить следующей

716
00:24:22,480 --> 00:24:24,240
настройкой, которую мы рассматривали, это декодер кодера

717
00:24:24,240 --> 00:24:26,000
модель, за исключением того, что все

718
00:24:26,000 --> 00:24:27,440
соответствующие параметры в

719
00:24:27,440 --> 00:24:30,080
декодере кодировщика являются общими, поэтому их в

720
00:24:30,080 --> 00:24:32,240
основном вдвое меньше, а

721
00:24:32,240 --> 00:24:34,000
затем окончательно  Другой вариант, который мы

722
00:24:34,000 --> 00:24:35,520
рассмотрели, - это модель декодера кодировщика, в

723
00:24:35,520 --> 00:24:37,520
которой кодер и декодер имеют

724
00:24:37,520 --> 00:24:39,279
вдвое меньше слоев, чем в

725
00:24:39,279 --> 00:24:41,600
базовой линии, и это потому, что мы также

726
00:24:41,600 --> 00:24:44,000
рассматриваем

727
00:24:44,000 --> 00:24:45,600
модели языка с одним стеком и то, что мы

728
00:24:45,600 --> 00:24:47,840
называем префиксной языковой моделью.  языковая

729
00:24:47,840 --> 00:24:50,400
модель - это модель, которая моделирует

730
00:24:50,400 --> 00:24:52,080
последовательность строго слева направо

731
00:24:52,080 --> 00:24:54,080
причинным образом, она в основном

732
00:24:54,080 --> 00:24:56,480
просто принимает токены по одному и

733
00:24:56,480 --> 00:24:58,480
предсказывает следующий токен,

734
00:24:58,480 --> 00:25:00,320
и вы можете фактически применить их

735
00:25:00,320 --> 00:25:02,320
к тексту для проблем с текстом,  в основном

736
00:25:02,320 --> 00:25:04,159
вводят входные данные в качестве префикса, прежде чем вы начнете

737
00:25:04,159 --> 00:25:05,840
что-либо предсказывать

738
00:25:05,840 --> 00:25:08,000
сейчас, если вы просто используете языковую модель в

739
00:25:08,000 --> 00:25:10,880
ее строгом формате, тогда вам все равно нужно

740
00:25:10,880 --> 00:25:13,279
иметь то, что мы бы назвали причинной маской, так

741
00:25:13,279 --> 00:25:16,159
что шаблон причинного внимания на префиксе,

742
00:25:16,159 --> 00:25:18,080
и именно так  Модели

743
00:25:18,080 --> 00:25:19,760
серии gpt обрабатывают

744
00:25:19,760 --> 00:25:22,720
все свои проблемы, но поскольку мы

745
00:25:22,720 --> 00:25:25,200
явно обозначаем часть последовательности

746
00:25:25,200 --> 00:25:26,720
как входную, а остальную часть последовательности

747
00:25:26,720 --> 00:25:28,640
как цель, мы фактически c

748
00:25:28,640 --> 00:25:30,559
Чтобы модель имела полную видимость, вы знаете

749
00:25:30,559 --> 00:25:32,640
непричинную маску во входной

750
00:25:32,640 --> 00:25:34,320
области последовательности, и когда мы вносим это

751
00:25:34,320 --> 00:25:36,000
изменение, мы называем это префиксной языковой

752
00:25:36,000 --> 00:25:37,679
моделью,

753
00:25:37,679 --> 00:25:39,360
и теперь результатом всего этого действительно

754
00:25:39,360 --> 00:25:41,200
является то, что модель декодера кодировщика  поскольку

755
00:25:41,200 --> 00:25:43,760
наша структура работает лучше всего, вы

756
00:25:43,760 --> 00:25:44,960
можете видеть, что когда мы делимся

757
00:25:44,960 --> 00:25:46,559
параметрами, это

758
00:25:46,559 --> 00:25:47,919
немного ухудшает производительность, но, возможно, немного меньше, чем

759
00:25:47,919 --> 00:25:49,360
вы могли ожидать,

760
00:25:49,360 --> 00:25:51,840
модель языка префиксов дает

761
00:25:51,840 --> 00:25:53,360
немного худшую производительность, но

762
00:25:53,360 --> 00:25:54,960
значительно лучшую производительность, чем

763
00:25:54,960 --> 00:25:56,720
выполнение строго причинно-следственной связи  правильного

764
00:25:56,720 --> 00:25:58,559
языкового моделирования, которое

765
00:25:58,559 --> 00:26:00,480
вы видите здесь в четвертой строке, и,

766
00:26:00,480 --> 00:26:02,240
наконец, наличие количества параметров

767
00:26:02,240 --> 00:26:04,080
в кодировщике и декодере

768
00:26:04,080 --> 00:26:07,520
значительно снижает производительность,

769
00:26:07,520 --> 00:26:09,520
следует отметить, что во всех

770
00:26:09,520 --> 00:26:11,679
этих случаях мы обрабатываем одну и ту же

771
00:26:11,679 --> 00:26:13,360
сумму  длина последовательности - это та же

772
00:26:13,360 --> 00:26:14,720
входная последовательность в той же целевой

773
00:26:14,720 --> 00:26:17,440
последовательности, поэтому в большинстве этих случаев

774
00:26:17,440 --> 00:26:18,960
общее количество флопов, необходимых для

775
00:26:18,960 --> 00:26:21,120
обработки последовательности  ce то же самое,

776
00:26:21,120 --> 00:26:22,400
даже несмотря на то, что количество параметров в

777
00:26:22,400 --> 00:26:26,159
два раза больше в базовой модели,

778
00:26:26,159 --> 00:26:27,600
поэтому следующее, что мы рассмотрели, были

779
00:26:27,600 --> 00:26:29,039
разные варианты нашей предтренировочной

780
00:26:29,039 --> 00:26:31,520
задачи, поэтому первое, что мы сделали, это как

781
00:26:31,520 --> 00:26:33,120
бы сравнить разные подходы высокого уровня.

782
00:26:33,120 --> 00:26:35,120
может быть, просто обучить

783
00:26:35,120 --> 00:26:36,640
модель предсказывать следующий токен по одному

784
00:26:36,640 --> 00:26:37,919
токену за раз, это своего рода

785
00:26:37,919 --> 00:26:40,080
задача языкового моделирования,

786
00:26:40,080 --> 00:26:42,000
другая - взять входную последовательность,

787
00:26:42,000 --> 00:26:43,679
перетасовать ее и обучить модель

788
00:26:43,679 --> 00:26:46,400
предсказывать последовательность без перетасовки или

789
00:26:46,400 --> 00:26:48,720
рассмотреть модель массового языка

790
00:26:48,720 --> 00:26:50,320
стиль цели в стиле берта, подобный той, о

791
00:26:50,320 --> 00:26:52,559
которой мы упоминали ранее,

792
00:26:52,559 --> 00:26:54,720
и теперь на втором этапе, который я

793
00:26:54,720 --> 00:26:56,799
показываю здесь, результаты для, мы

794
00:26:56,799 --> 00:26:59,039
считали, цель стиля птицы,

795
00:26:59,039 --> 00:27:00,640
где модель обучена предсказывать

796
00:27:00,640 --> 00:27:02,559
всю

797
00:27:02,559 --> 00:27:04,880
исходную неповрежденную входную последовательность

798
00:27:04,880 --> 00:27:06,720
цель массового стиля, которая очень

799
00:27:06,720 --> 00:27:08,480
похожа,

800
00:27:08,480 --> 00:27:10,400
а затем цель замены поврежденных промежутков,

801
00:27:10,400 --> 00:27:11,919
которая похожа на ту, которую я

802
00:27:11,919 --> 00:27:13,279
описал в начале, которую мы

803
00:27:13,279 --> 00:27:15,440
используем в нашей  r базовая модель и, наконец,

804
00:27:15,440 --> 00:27:17,520
вариант, в котором вместо того, чтобы заменять

805
00:27:17,520 --> 00:27:20,320
каждый жетон э-э на уникальный сторожевой

806
00:27:20,320 --> 00:27:22,320
жетон, мы просто полностью отбрасываем массовые

807
00:27:22,320 --> 00:27:23,919
жетоны и обучаем модель

808
00:27:23,919 --> 00:27:26,000
предсказывать выпавшие жетоны,

809
00:27:26,000 --> 00:27:28,399
и вы можете видеть, что последние два

810
00:27:28,399 --> 00:27:30,320
варианта работают примерно так же, как и один

811
00:27:30,320 --> 00:27:32,399
еще одно, но еще одно существенное

812
00:27:32,399 --> 00:27:34,480
различие между этими

813
00:27:34,480 --> 00:27:36,559
наборами целей состоит в том, что первые два

814
00:27:36,559 --> 00:27:38,399
включают прогнозирование всей входной

815
00:27:38,399 --> 00:27:41,039
последовательности, а последние два в основном

816
00:27:41,039 --> 00:27:43,760
включают прогнозирование собранных токенов,

817
00:27:43,760 --> 00:27:45,440
а когда вы прогнозируете только токены талисмана, у

818
00:27:45,440 --> 00:27:47,120
вас гораздо более короткая целевая

819
00:27:47,120 --> 00:27:49,120
последовательность  Таким образом, общая

820
00:27:49,120 --> 00:27:51,919
стоимость предварительного обучения значительно ниже, поэтому

821
00:27:51,919 --> 00:27:53,279
мы решили, что это лучший

822
00:27:53,279 --> 00:27:55,840
подход, а затем мы рассмотрели другие

823
00:27:55,840 --> 00:27:58,640
гиперпараметры в нашей стратегии маскирования,

824
00:27:58,640 --> 00:28:03,600
такие как количество токенов, которые нужно замаскировать,

825
00:28:03,600 --> 00:28:05,360
поэтому следующее, что мы рассмотрели, были

826
00:28:05,360 --> 00:28:07,039
разные варианты  набор данных до обучения

827
00:28:08,399 --> 00:28:10,480
в нашей базовой линии мы использовали набор данных c4,

828
00:28:10,480 --> 00:28:12,240
который я предложил

829
00:28:12,240 --> 00:28:13,840
в начале выступления,

830
00:28:13,840 --> 00:28:16,080
мы также c  по сравнению с предварительным обучением только

831
00:28:16,080 --> 00:28:18,799
на неотфильтрованных данных из c4, поэтому вместо того,

832
00:28:18,799 --> 00:28:20,320
чтобы выполнять все эти шаги эвристической фильтрации,

833
00:28:20,320 --> 00:28:22,320
мы просто берем необработанный

834
00:28:22,320 --> 00:28:24,720
текст, извлеченный из Интернета из c4, и предварительно обучаем его, и

835
00:28:24,720 --> 00:28:26,320
вы можете видеть, что это работает равномерно

836
00:28:26,320 --> 00:28:28,320
хуже, так что это  действительно кажется правдой,

837
00:28:28,320 --> 00:28:29,600
что эти шаги по очистке, которые мы

838
00:28:29,600 --> 00:28:31,279
делаем, на самом деле

839
00:28:31,279 --> 00:28:33,279
полезны следующие четыре набора данных были нашей попыткой

840
00:28:34,080 --> 00:28:35,840
предварительно обучиться на аналогичных наборах данных,

841
00:28:35,840 --> 00:28:38,080
которые использовались в пароле, настоящий набор данных новостей

842
00:28:38,080 --> 00:28:39,760
взят из газеты Гровера.

843
00:28:39,760 --> 00:28:40,960
По сути,

844
00:28:40,960 --> 00:28:42,799
предварительное обучение только на данных с новостных

845
00:28:42,799 --> 00:28:45,520
сайтов. Веб-текст - это набор данных, который

846
00:28:45,520 --> 00:28:47,679
использовался в документе gpt2, где вы

847
00:28:47,679 --> 00:28:50,320
тренируетесь только на веб-тексте, который был связан с Reddit и

848
00:28:50,320 --> 00:28:52,399
получил достаточно высокий балл,

849
00:28:53,279 --> 00:28:55,039
а затем два последних варианта

850
00:28:55,039 --> 00:28:57,520
либо  только wikipedia или как использовалось

851
00:28:57,520 --> 00:28:59,679
в wiki в wikipedia burp paper

852
00:28:59,679 --> 00:29:02,000
с корпусом toronto books,

853
00:29:02,000 --> 00:29:03,600
и

854
00:29:03,600 --> 00:29:05,200
вы могли бы заметить, что некоторые из

855
00:29:05,200 --> 00:29:07,120
этих более специализированных наборов данных мы получаем

856
00:29:07,120 --> 00:29:09,440
лучшую производительность, поэтому, например, вы

857
00:29:09,440 --> 00:29:11,200
можете  е, что в Википедии и

858
00:29:11,200 --> 00:29:13,279
корпусе книг Торонто в нижнем ряду

859
00:29:13,279 --> 00:29:15,600
мы на самом деле намного лучше справляемся с суперклеем

860
00:29:15,600 --> 00:29:18,000
с оценкой чуть больше 73

861
00:29:18,000 --> 00:29:20,480
по сравнению с предварительным обучением на c4, и,

862
00:29:20,480 --> 00:29:22,880
оказывается, это потому, что мы или мы

863
00:29:22,880 --> 00:29:24,960
предполагаем, что это  это потому, что

864
00:29:24,960 --> 00:29:27,039
суперклей содержит задачу под названием

865
00:29:27,039 --> 00:29:28,640
multi-rc, которая представляет собой

866
00:29:28,640 --> 00:29:30,480
задачу понимания прочитанного

867
00:29:30,480 --> 00:29:34,080
в вики-новостях, извините, в

868
00:29:34,080 --> 00:29:36,960
статьях из энциклопедии и в романах, поэтому основной

869
00:29:36,960 --> 00:29:38,720
вывод здесь заключается в том, что когда вы предварительно тренируетесь

870
00:29:38,720 --> 00:29:41,120
на данных, которые похожи на вашу

871
00:29:41,120 --> 00:29:42,640
последующую задачу  у которого есть похожий

872
00:29:42,640 --> 00:29:44,720
домен, вы часто получаете большой импульс в этой

873
00:29:44,720 --> 00:29:46,399
последующей задаче, и это действительно то, что

874
00:29:46,399 --> 00:29:48,080
произошло здесь, что

875
00:29:48,080 --> 00:29:49,440
интересно, вы также можете увидеть

876
00:29:49,440 --> 00:29:51,600
противоположный эффект, поэтому, если вы посмотрите

877
00:29:51,600 --> 00:29:54,399
википедию во второй и последней строке,

878
00:29:54,399 --> 00:29:56,399
если вы только предварительно тренируетесь на  Википедия, вы в

879
00:29:56,399 --> 00:29:58,399
конечном итоге намного хуже справляетесь с колой, которая

880
00:29:58,399 --> 00:30:00,399
является корпусом задач лингвистической приемлемости,

881
00:30:00,399 --> 00:30:02,559
о которых я упоминал ранее, и мы

882
00:30:02,559 --> 00:30:04,320
предполагаем, что это потому, что в

883
00:30:04,320 --> 00:30:06,640
википедии очень мало неприемлемого

884
00:30:06,640 --> 00:30:08,720
текста, который вы базовый  lly только предварительное обучение

885
00:30:08,720 --> 00:30:10,960
на чистом тексте, тогда как в c4 есть некоторый

886
00:30:10,960 --> 00:30:13,200
неграмотный текст, в нем есть какая-то ерунда,

887
00:30:13,200 --> 00:30:14,480
и так что это действительно может немного повысить вашу

888
00:30:14,480 --> 00:30:16,559
производительность на коле,

889
00:30:16,559 --> 00:30:18,159
последнее, что следует отметить, это то, что, хотя вы

890
00:30:18,159 --> 00:30:20,080
иногда видите некоторый выигрыш при использовании этих

891
00:30:20,080 --> 00:30:22,080
меньших данных  Эти наборы данных

892
00:30:22,080 --> 00:30:23,679
примерно на порядок меньше, чем

893
00:30:23,679 --> 00:30:26,399
c4, поэтому возникает естественный вопрос:

894
00:30:26,399 --> 00:30:28,399
действительно ли вам больно предварительно тренироваться на

895
00:30:28,399 --> 00:30:30,080
меньшем наборе данных,

896
00:30:30,080 --> 00:30:31,840
поэтому для ответа на этот вопрос мы

897
00:30:31,840 --> 00:30:34,480
в основном взяли c4 и

898
00:30:34,480 --> 00:30:36,799
искусственно уменьшили его  так что это повторялось

899
00:30:36,799 --> 00:30:38,640
в ходе предварительного обучения,

900
00:30:38,640 --> 00:30:40,960
и вы можете видеть здесь, что когда вы

901
00:30:40,960 --> 00:30:44,080
повторяете набор данных 64 раза,

902
00:30:44,080 --> 00:30:47,120
получается 34 миллиарда, разделенные на 64 токена,

903
00:30:47,120 --> 00:30:48,399
потому что это то количество предварительных тренировок, которое мы

904
00:30:48,399 --> 00:30:49,600
провели,

905
00:30:49,600 --> 00:30:51,120
вы на самом деле не жертвуете часами

906
00:30:51,120 --> 00:30:53,360
производительность производительность примерно

907
00:30:53,360 --> 00:30:54,559
такая же,

908
00:30:54,559 --> 00:30:57,440
но если вы повторите набор данных 256

909
00:30:57,440 --> 00:30:59,679
раз 1024 или более раз, вы действительно начнете

910
00:30:59,679 --> 00:31:01,440
видеть деградацию, и причина, по которой мы

911
00:31:01,440 --> 00:31:02,960
думаем, что это происходит, заключается в том, что

912
00:31:02,960 --> 00:31:04,559
вы базовый  Во время

913
00:31:04,559 --> 00:31:06,960
предварительной подготовки вы можете понять,

914
00:31:06,960 --> 00:31:08,320
правда это или нет,

915
00:31:08,320 --> 00:31:09,679
просто посмотрев на потери при обучении,

916
00:31:09,679 --> 00:31:11,519
вы увидите, что модель

917
00:31:11,519 --> 00:31:13,600
достигает гораздо меньших потерь при обучении, когда вы

918
00:31:13,600 --> 00:31:15,600
повторяете набор данных все больше и больше.  раз,

919
00:31:15,600 --> 00:31:17,440
поэтому результатом этого является то, что ваш

920
00:31:17,440 --> 00:31:19,360
набор данных должен быть как минимум настолько большим, чтобы вы

921
00:31:19,360 --> 00:31:21,600
не видели значительного переобучения во время

922
00:31:21,600 --> 00:31:23,360
предварительного обучения

923
00:31:23,360 --> 00:31:24,159
и

924
00:31:24,159 --> 00:31:26,159
позже, когда мы масштабируем эти модели

925
00:31:26,159 --> 00:31:28,320
и предварительно обучаем их на гораздо большем количестве данных, которые мы

926
00:31:28,320 --> 00:31:29,360
будем делать

927
00:31:29,360 --> 00:31:31,440
достаточное количество повторений тех меньших типов

928
00:31:31,440 --> 00:31:33,519
наборов данных, более специфичных для предметной области, которые, как мы

929
00:31:33,519 --> 00:31:37,840
думаем, мы увидим вредные эффекты.

930
00:31:38,159 --> 00:31:40,880
Следующим, с чем мы экспериментировали, были

931
00:31:40,880 --> 00:31:43,039
стратегии многозадачного обучения, поэтому, когда

932
00:31:43,039 --> 00:31:44,399
вы выполняете многозадачное обучение, вы, по

933
00:31:44,399 --> 00:31:46,399
сути, тренируете модель на

934
00:31:46,399 --> 00:31:49,200
несколько задач одновременно, и

935
00:31:49,200 --> 00:31:50,799
в большинстве экспериментов, которые я показываю

936
00:31:50,799 --> 00:31:53,200
на этом слайде, на

937
00:31:53,200 --> 00:31:55,200
самом деле мы тренируемся по каждой отдельной задаче

938
00:31:55,200 --> 00:31:57,440
одновременно, поэтому предварительная тренировочная задача и

939
00:31:57,440 --> 00:31:59,600
все последующие задачи вместе

940
00:31:59,600 --> 00:32:01,279
и наиболее уместный вопрос, когда

941
00:32:01,279 --> 00:32:02,640
вы выполняете многозадачное обучение, как

942
00:32:02,640 --> 00:32:04,960
это, заключается в том, как часто я должен выполнять выборку данных

943
00:32:04,960 --> 00:32:06,320
из каждой задачи,

944
00:32:06,320 --> 00:32:08,799
поэтому один подход - просто выборка данных

945
00:32:08,799 --> 00:32:11,120
с одинаковой скоростью для всех

946
00:32:11,120 --> 00:32:12,240
задач,

947
00:32:12,240 --> 00:32:15,440
другой случай - в основном  сделайте вид,

948
00:32:15,440 --> 00:32:16,799
будто вы только что объединили все

949
00:32:16,799 --> 00:32:18,480
наборы данных, которые мы называем примерами

950
00:32:18,480 --> 00:32:19,679
пропорционального смешивания, потому что это

951
00:32:19,679 --> 00:32:21,840
эквивалентно выборке из набора данных

952
00:32:21,840 --> 00:32:23,519
в соответствии с тем, сколько примеров

953
00:32:23,519 --> 00:32:24,960
есть в наборе данных.

954
00:32:26,320 --> 00:32:28,480
набор данных настолько велик,

955
00:32:28,480 --> 00:32:30,880
что его доля будет

956
00:32:30,880 --> 00:32:32,559
намного больше, чем у каждой последующей

957
00:32:32,559 --> 00:32:34,240
задачи, и мы, по сути, никогда не будем тренироваться ни

958
00:32:34,240 --> 00:32:36,480
на одном из последующих данных, поэтому мы

959
00:32:36,480 --> 00:32:38,960
ввели этот гиперпараметр k,

960
00:32:38,960 --> 00:32:41,919
который является константой, которая в основном

961
00:32:41,919 --> 00:32:43,519
определяет размер, который мы должны  Представьте, что

962
00:32:43,519 --> 00:32:45,679
набор данных перед обучением -

963
00:32:45,679 --> 00:32:48,720
это последнее, что вы можете сделать, это взять

964
00:32:48,720 --> 00:32:50,399
количество примеров в каждом наборе данных и

965
00:32:50,399 --> 00:32:52,159
масштабировать его по температуре,

966
00:32:52,159 --> 00:32:53,919
чем больше температура, тем ближе

967
00:32:53,919 --> 00:32:56,399
вы получаете равное смешивание и единообразную

968
00:32:56,399 --> 00:32:59,440
выборку из каждого набора данных,

969
00:32:59,440 --> 00:33:01,679
но в любом случае главный вывод из

970
00:33:01,679 --> 00:33:03,360
этой таблицы заключается в том, что

971
00:33:03,360 --> 00:33:05,039
вы можете довольно близко подойти к

972
00:33:05,039 --> 00:33:07,760
производительности отдельного предварительного обучения и

973
00:33:07,760 --> 00:33:09,840
точной настройки, как мы это делаем на нашем базовом уровне,

974
00:33:09,840 --> 00:33:12,000
если вы получите  стратегия смешивания верна, но в

975
00:33:12,000 --> 00:33:14,000
конечном итоге мы обнаружили, что вы склонны

976
00:33:14,000 --> 00:33:15,760
жертвовать некоторой производительностью при

977
00:33:15,760 --> 00:33:18,080
выполнении многозадачного обучения, и, по крайней мере, по

978
00:33:18,080 --> 00:33:20,480
некоторым задачам и колин, было

979
00:33:20,480 --> 00:33:22,640
много вопросов по выбору

980
00:33:22,640 --> 00:33:24,559
на слайде с различными наборами данных

981
00:33:24,559 --> 00:33:28,000
один показывает настоящие новости и c4 и так далее,

982
00:33:28,000 --> 00:33:29,840
вы хотите взять пару,

983
00:33:29,840 --> 00:33:33,039
абсолютно да, во-первых, эм,

984
00:33:33,039 --> 00:33:34,720
если вы просто посмотрите на это,

985
00:33:34,720 --> 00:33:38,240
он все еще выглядит так, как будто вы

986
00:33:38,240 --> 00:33:41,039
можете получить отличные результаты с

987
00:33:41,039 --> 00:33:43,039
более чем на порядок меньше

988
00:33:43,039 --> 00:33:46,000
текста, чем  c4 и да, похоже,

989
00:33:46,000 --> 00:33:47,600
что это не то сообщение, которое вы

990
00:33:47,600 --> 00:33:50,240
хотели выдвинуть, да, так что здесь есть

991
00:33:50,240 --> 00:33:51,840
небольшой нюанс, который в основном заключается в

992
00:33:51,840 --> 00:33:53,360
том, что в нашей базовой линии в этих

993
00:33:53,360 --> 00:33:54,640
экспериментах, которые я проводил до

994
00:33:54,640 --> 00:33:56,720
сих пор, мы действительны  Я не занимаюсь предварительной подготовкой

995
00:33:56,720 --> 00:33:58,640
так долго, так как, как я уже упоминал ранее,

996
00:33:58,640 --> 00:33:59,840
мы на самом деле предварительно тренируемся на

997
00:33:59,840 --> 00:34:02,720
четверть до тех пор, пока не эээ, а на самом деле

998
00:34:02,720 --> 00:34:04,240
для нас

999
00:34:04,240 --> 00:34:06,960
я верю одну 256-ю часть

1000
00:34:06,960 --> 00:34:11,040
до тех пор, пока э-э, например, нетто, поэтому

1001
00:34:11,040 --> 00:34:13,359
и когда позже  документ, который мы собираемся

1002
00:34:13,359 --> 00:34:15,679
подготовить намного дольше, и

1003
00:34:15,679 --> 00:34:17,599
в этом случае мы бы в конечном итоге повторили

1004
00:34:17,599 --> 00:34:20,000
эти наборы данных много раз в

1005
00:34:20,000 --> 00:34:21,440
течение предварительного обучения, и мы начали бы

1006
00:34:21,440 --> 00:34:22,719
видеть эти негативные эффекты, которые я

1007
00:34:22,719 --> 00:34:24,399
объяснил на  следующий слайд, ну, вот

1008
00:34:24,399 --> 00:34:26,239
почему вы затем повторяете, хорошо,

1009
00:34:26,239 --> 00:34:28,000
кто-то потом спросил об этом, почему

1010
00:34:28,000 --> 00:34:29,679
вы тренируетесь на одном и том же снова

1011
00:34:29,679 --> 00:34:31,839
и снова, это тест

1012
00:34:36,480 --> 00:34:37,918
так

1013
00:34:37,918 --> 00:34:40,480
что в первом приближении c4 содержит

1014
00:34:40,480 --> 00:34:44,399
википедию или, возможно, многие

1015
00:34:44,399 --> 00:34:46,159
страницы википедии, но не всю

1016
00:34:46,159 --> 00:34:49,359
википедию, ну, вы знаете, что обычное

1017
00:34:49,359 --> 00:34:53,440
сканирование выполняется, как будто это своего рода

1018
00:34:53,440 --> 00:34:55,520
веб-сканирование, переходя по ссылкам с определенным

1019
00:34:55,520 --> 00:34:58,160
приоритетом, и в конечном итоге это  не охватывал

1020
00:34:58,160 --> 00:34:59,920
всю Википедию  Я на самом деле не

1021
00:34:59,920 --> 00:35:01,680
знаю точную долю википедии, которая

1022
00:35:01,680 --> 00:35:05,200
включена в c4, но определенно,

1023
00:35:05,200 --> 00:35:07,440
когда вы тренируетесь на c4, вы увидите какой-то

1024
00:35:07,440 --> 00:35:09,680
текст википедии, ну, он будет в

1025
00:35:09,680 --> 00:35:11,599
относительно низкой пропорции по сравнению со

1026
00:35:11,599 --> 00:35:14,160
всеми другими данными, которые вы будете

1027
00:35:14,160 --> 00:35:16,160
источниками  данные, которые вы обязательно увидите, а затем

1028
00:35:16,160 --> 00:35:18,640
кто-то не совсем убедился в вашем

1029
00:35:18,640 --> 00:35:21,520
аргументе, что хорошее качество

1030
00:35:21,520 --> 00:35:23,280
Википедии объясняет худшую

1031
00:35:23,280 --> 00:35:26,160
производительность колы, потому что они наверняка хорошо думали, что

1032
00:35:28,640 --> 00:35:31,520
настоящие новости, в основном хорошо отредактированный

1033
00:35:31,520 --> 00:35:33,920
текст, и все же, похоже,  работают

1034
00:35:33,920 --> 00:35:35,040
нормально,

1035
00:35:35,040 --> 00:35:37,119
да, это хороший момент, я не

1036
00:35:37,119 --> 00:35:38,720
уверен, что вы знаете, что это могут быть настоящие новости,

1037
00:35:38,720 --> 00:35:40,800
потому что в настоящих новостях есть цитаты, или,

1038
00:35:40,800 --> 00:35:43,280
может быть, в настоящих новостях есть

1039
00:35:43,280 --> 00:35:44,880
контент из общих разделов

1040
00:35:44,880 --> 00:35:45,839
сайтов,

1041
00:35:45,839 --> 00:35:47,839
я должен сказать, что когда  i причина того,

1042
00:35:47,839 --> 00:35:49,760
что это настоящие новости и веб-текст,

1043
00:35:49,760 --> 00:35:51,839
заключается в том, что это наши собственные репродукции,

1044
00:35:51,839 --> 00:35:53,440
поэтому они могут не быть в точности такими

1045
00:35:53,440 --> 00:35:55,760
же, как первоначально предложенные варианты,

1046
00:35:55,760 --> 00:35:57,440
потому что веб-текст, например, никогда не

1047
00:35:57,440 --> 00:35:59,839
публиковался  Эд

1048
00:36:00,400 --> 00:36:01,520
да, но это интересный момент, и

1049
00:36:01,520 --> 00:36:03,680
именно поэтому я бы сказал, что

1050
00:36:03,680 --> 00:36:05,280
это предположение, что вы не знаете,

1051
00:36:05,280 --> 00:36:06,560
что я могу сделать строгое

1052
00:36:06,560 --> 00:36:08,000
заявление о

1053
00:36:08,000 --> 00:36:10,160
да ладно, может быть, мы должны позволить вам

1054
00:36:10,160 --> 00:36:11,200
продолжить сейчас

1055
00:36:11,200 --> 00:36:12,480
большое спасибо да спасибо за тех

1056
00:36:12,480 --> 00:36:13,520
вопросы,

1057
00:36:14,480 --> 00:36:16,640
так что следующее, что нужно сделать после

1058
00:36:16,640 --> 00:36:18,160
изучения этих различных стратегий многозадачного обучения,

1059
00:36:20,079 --> 00:36:22,000
- это посмотреть, есть ли у нас какой-либо способ

1060
00:36:22,000 --> 00:36:24,640
сократить разрыв между многозадачным обучением

1061
00:36:24,640 --> 00:36:26,800
и этим предварительным обучением, за которым следует

1062
00:36:26,800 --> 00:36:29,359
отдельная тонкая настройка, и мы экспериментировали

1063
00:36:29,359 --> 00:36:30,800
со многими  здесь разные стратегии, в

1064
00:36:30,800 --> 00:36:33,520
основном строгое многозадачное обучение,

1065
00:36:33,520 --> 00:36:35,359
многозадачное обучение с последующей

1066
00:36:35,359 --> 00:36:37,680
тонкой настройкой индивидуальных задач,

1067
00:36:37,680 --> 00:36:39,520
многозадачное обучение, но без

1068
00:36:39,520 --> 00:36:41,760
каких-либо неконтролируемых данных, и действительно

1069
00:36:41,760 --> 00:36:43,200
главный вывод из всех этих

1070
00:36:43,200 --> 00:36:45,200
экспериментов заключался в том, что если вы выполняете

1071
00:36:45,200 --> 00:36:48,160
многозадачность  сначала обучение, включая

1072
00:36:48,160 --> 00:36:49,760
неконтролируемую задачу,

1073
00:36:49,760 --> 00:36:52,000
а затем вы настраиваете модель для каждой

1074
00:36:52,000 --> 00:36:53,680
задачи отдельно,

1075
00:36:53,680 --> 00:36:55,359
что является третьей строкой здесь, которую вы на самом деле

1076
00:36:55,359 --> 00:36:57,200
не на самом деле  жертвовать большой производительностью

1077
00:36:57,200 --> 00:36:59,599
вообще вы не в конечном итоге с

1078
00:36:59,599 --> 00:37:01,680
многозадачной моделью, потому что вы

1079
00:37:01,680 --> 00:37:03,839
настраиваете каждую задачу индивидуально,

1080
00:37:03,839 --> 00:37:06,079
но хорошая вещь в этом подходе

1081
00:37:06,079 --> 00:37:08,800
заключается в том, что вы можете контролировать

1082
00:37:08,800 --> 00:37:11,119
производительность своих последующих задач, пока вы

1083
00:37:11,119 --> 00:37:12,880
выполняете предварительную тренировку, и вы не

1084
00:37:12,880 --> 00:37:16,160
жертвуете большой производительностью,

1085
00:37:16,160 --> 00:37:18,079
один параметр, который мы не рассматривали,

1086
00:37:18,079 --> 00:37:20,079
- это предварительная подготовка без учителя, за

1087
00:37:20,079 --> 00:37:22,640
которой следует контролируемое многозадачное обучение.

1088
00:37:27,599 --> 00:37:29,200
вроде последней серии

1089
00:37:29,200 --> 00:37:31,280
экспериментов, которые мы провели, попробуйте ответить на

1090
00:37:31,280 --> 00:37:33,200
следующий вопрос: допустим,

1091
00:37:33,200 --> 00:37:34,880
кто-то приходит и внезапно

1092
00:37:34,880 --> 00:37:36,960
дает вам в четыре раза больше вычислений,

1093
00:37:36,960 --> 00:37:38,800
что вы должны с этим делать, и поэтому

1094
00:37:38,800 --> 00:37:40,240
есть ряд вещей, которые вы могли бы сделать  вы

1095
00:37:40,240 --> 00:37:41,760
можете увеличить количество

1096
00:37:41,760 --> 00:37:43,680
шагов обучения в четыре раза вы можете

1097
00:37:43,680 --> 00:37:45,280
увеличить размер пакета в

1098
00:37:45,280 --> 00:37:47,280
четыре раза вы можете сделать свою модель в два раза

1099
00:37:47,280 --> 00:37:49,040
больше и тренироваться в два раза дольше вы можете

1100
00:37:49,040 --> 00:37:51,040
сделать свою модель в четыре раза больше, чем вы

1101
00:37:51,040 --> 00:37:53,599
могли  обучите четыре модели по отдельности и

1102
00:37:53,599 --> 00:37:55,920
объедините их, или вы могли бы сделать последнюю

1103
00:37:55,920 --> 00:37:57,200
вещь, которая на самом деле не требует в четыре

1104
00:37:57,200 --> 00:37:58,880
раза больше вычислений, когда вы

1105
00:37:58,880 --> 00:38:01,200
предварительно обучаете одну модель и настраиваете ее

1106
00:38:01,200 --> 00:38:02,880
четыре раза по отдельности, а затем объединяете

1107
00:38:02,880 --> 00:38:06,320
их и основной вывод здесь  заключается в том, что

1108
00:38:06,320 --> 00:38:08,720
масштабирование помогает, это, как вы знаете, очень

1109
00:38:08,720 --> 00:38:11,839
неудивительно, особенно в 2021 году,

1110
00:38:11,839 --> 00:38:13,119
но, что

1111
00:38:13,119 --> 00:38:15,599
интересно, вы получите значительный

1112
00:38:15,599 --> 00:38:16,560
выигрыш,

1113
00:38:16,560 --> 00:38:18,720
если просто увеличите время обучения

1114
00:38:18,720 --> 00:38:21,119
или увеличите размер,

1115
00:38:21,119 --> 00:38:23,680
чтобы вы могли видеть, что по обеим этим

1116
00:38:23,680 --> 00:38:25,280
осям мы получаем значительные улучшения производительности,

1117
00:38:25,280 --> 00:38:26,640
хотя

1118
00:38:26,640 --> 00:38:28,160
Улучшение производительности становится более значительным, когда мы

1119
00:38:28,160 --> 00:38:30,240
увеличиваем размер, и, в частности, вы

1120
00:38:30,240 --> 00:38:32,400
можете видеть, что мы перешли с

1121
00:38:32,400 --> 00:38:35,359
71 балла на суперклей до 78, просто

1122
00:38:35,359 --> 00:38:38,880
увеличив модель в четыре раза.

1123
00:38:40,640 --> 00:38:42,880
повторить все это, а затем использовать это

1124
00:38:42,880 --> 00:38:44,720
резюме, чтобы объяснить дизайнерские решения,

1125
00:38:44,720 --> 00:38:46,560
которые вошли в окончательный вид моделей t5.

1126
00:38:47,599 --> 00:38:49,599
Первое, что мы собираемся

1127
00:38:49,599 --> 00:38:51,280
выбрать  архитектура декодера кодера,

1128
00:38:51,280 --> 00:38:52,800
потому что это, казалось, лучше всего работает в нашем

1129
00:38:52,800 --> 00:38:54,400
формате текста в текст,

1130
00:38:54,400 --> 00:38:55,520
следующее: мы собираемся

1131
00:38:55,520 --> 00:38:57,440
использовать цель прогнозирования диапазона, которая

1132
00:38:57,440 --> 00:38:58,800
в конечном итоге очень похожа на

1133
00:38:58,800 --> 00:39:00,320
цель базовой линии, которую я описал

1134
00:39:00,320 --> 00:39:01,359
ранее,

1135
00:39:01,359 --> 00:39:03,440
мы будем использовать c4  набор данных, потому что он

1136
00:39:03,440 --> 00:39:05,760
действительно достиг разумной производительности, но

1137
00:39:05,760 --> 00:39:07,359
был достаточно большим, чтобы нам не нужно было

1138
00:39:07,359 --> 00:39:08,480
беспокоиться о

1139
00:39:08,480 --> 00:39:09,680
повторении данных и обнаружении

1140
00:39:09,680 --> 00:39:11,200
пагубного переобучения во время

1141
00:39:11,200 --> 00:39:12,640
предварительного обучения,

1142
00:39:12,640 --> 00:39:14,079
когда мы увеличиваем количество

1143
00:39:14,079 --> 00:39:16,400
этапов предварительного обучения, которые мы фактически

1144
00:39:16,400 --> 00:39:18,160
решили выполнить несколько раз.  предварительное обучение задач, потому что мы

1145
00:39:18,160 --> 00:39:19,760
будем увеличивать объем

1146
00:39:19,760 --> 00:39:21,839
предварительной подготовки, наши самые длительные тренировки

1147
00:39:21,839 --> 00:39:23,359
заняли около месяца, и мы хотели иметь

1148
00:39:23,359 --> 00:39:24,800
возможность отслеживать производительность в

1149
00:39:24,800 --> 00:39:26,400
ходе предварительной подготовки, не выполняя

1150
00:39:26,400 --> 00:39:28,320
тонкую настройку, поэтому мы собираемся  выполнить

1151
00:39:28,320 --> 00:39:30,160
это предварительное многозадачное обучение с последующей

1152
00:39:30,160 --> 00:39:32,000
тонкой настройкой, а затем,

1153
00:39:32,000 --> 00:39:33,520
конечно же, мы собираемся обучать более крупные

1154
00:39:33,520 --> 00:39:35,520
модели на более длительный срок

1155
00:39:35,520 --> 00:39:37,680
и, в частности, размеры моделей, которые мы в

1156
00:39:37,680 --> 00:39:38,880
итоге

1157
00:39:38,880 --> 00:39:41,599
изменили  лизинг мы называем малый бас большой 3b

1158
00:39:41,599 --> 00:39:43,040
и

1159
00:39:43,040 --> 00:39:44,800
11b малая модель имеет 60 миллионов

1160
00:39:44,800 --> 00:39:46,720
параметров, она примерно на четверть

1161
00:39:46,720 --> 00:39:48,960
меньше нашей базовой линии, которая снова была

1162
00:39:48,960 --> 00:39:51,760
кодировщиком размера на основе рождения декодером базового размера мы

1163
00:39:51,760 --> 00:39:53,520
также обучили модель, которая была

1164
00:39:53,520 --> 00:39:56,480
кодировщиком большого размера декодером большого размера  а затем

1165
00:39:56,480 --> 00:39:59,040
мы создали два больших варианта, просто

1166
00:39:59,040 --> 00:40:01,040
увеличив размер трансформатора с прямой связью

1167
00:40:01,040 --> 00:40:03,280
и количество

1168
00:40:03,280 --> 00:40:05,119
головок внимания в трансформаторе, как вы

1169
00:40:05,119 --> 00:40:06,720
можете видеть, наша самая большая модель на самом деле имела

1170
00:40:06,720 --> 00:40:10,319
скрытую размерность 65 000 мкм

1171
00:40:10,319 --> 00:40:12,240
в слоях с прямой связью.  то, что

1172
00:40:12,240 --> 00:40:13,839
мы сделали такой необычный способ

1173
00:40:13,839 --> 00:40:15,359
увеличения количества параметров, просто

1174
00:40:15,359 --> 00:40:16,240
потому,

1175
00:40:17,119 --> 00:40:18,400
что слои с прямой связью - это просто

1176
00:40:18,400 --> 00:40:20,560
гигантские матричные умножения, и

1177
00:40:20,560 --> 00:40:22,079
это лучший способ использовать аппаратные

1178
00:40:22,079 --> 00:40:24,000
ускорители.

1179
00:40:27,200 --> 00:40:29,920
спрашивать,

1180
00:40:30,800 --> 00:40:32,800
как вы выполняли

1181
00:40:32,800 --> 00:40:36,000
многозадачное обучение, так что

1182
00:40:41,680 --> 00:40:43,760
в нашем случае просто приклеили простой классификатор soft max для каждой задачи или около того, потому что  так как мы используем этот

1183
00:40:43,760 --> 00:40:46,400
формат текста в текст, в основном мы

1184
00:40:46,400 --> 00:40:48,800
обучаем именно этому, это точно такая

1185
00:40:48,800 --> 00:40:50,640
же модель, нет новых классификационных заголовков

1186
00:40:50,640 --> 00:40:52,240
для каждой задачи, единственная разница в том,

1187
00:40:52,240 --> 00:40:54,640
что каждая задача получает свой собственный префикс задачи,

1188
00:40:54,640 --> 00:40:56,000
поэтому, если вы все помните  вернувшись

1189
00:40:56,000 --> 00:40:57,520
в начало, мы говорим, что вы знаете, что переводите с

1190
00:40:57,520 --> 00:40:59,839
английского на немецкий, английское предложение с двоеточием,

1191
00:40:59,839 --> 00:41:02,400
или вы знаете, что суммируете английский

1192
00:41:02,400 --> 00:41:04,079
абзац с двоеточием, и это говорит модели, что

1193
00:41:04,079 --> 00:41:05,839
она должна делать, а затем вы просто тренируете

1194
00:41:05,839 --> 00:41:07,119
модель, чтобы предсказать соответствующую

1195
00:41:07,119 --> 00:41:09,520
цель,

1196
00:41:11,599 --> 00:41:14,560
так что последний  уместная деталь

1197
00:41:14,560 --> 00:41:17,040
опять же заключается в том, что мы увеличили

1198
00:41:17,040 --> 00:41:18,960
объем предварительной подготовки, которую мы закончили предварительной подготовкой

1199
00:41:18,960 --> 00:41:20,960
на триллионе токенов данных, а не на

1200
00:41:20,960 --> 00:41:23,520
34 миллиардах токенов, так что это, э-э,

1201
00:41:23,520 --> 00:41:25,359
это намного больше предварительной подготовки, хотя

1202
00:41:25,359 --> 00:41:27,040
она еще менее предварительная.  обучение, которое

1203
00:41:27,040 --> 00:41:29,040
использовалось в Excel net, я думаю, в два раза,

1204
00:41:29,040 --> 00:41:32,560
если я правильно помню,

1205
00:41:32,640 --> 00:41:34,400
так что вот результаты, которые

1206
00:41:34,400 --> 00:41:35,760
и это были своего рода тем, как

1207
00:41:35,760 --> 00:41:36,960
все было в то время, когда

1208
00:41:36,960 --> 00:41:39,280
мы выпустили статью.  Мы

1209
00:41:39,280 --> 00:41:41,839
получили самые современные результаты

1210
00:41:41,839 --> 00:41:44,960
в тесте glue meta Benchmark cnn daily mail,

1211
00:41:46,960 --> 00:41:48,640
и мы были очень

1212
00:41:48,640 --> 00:41:50,640
взволнованы, увидев, насколько хорошо мы справились с

1213
00:41:50,640 --> 00:41:51,920
суперклеем.

1214
00:41:53,520 --> 00:41:56,000
89.8,

1215
00:41:56,000 --> 00:41:58,079
и мы работали значительно лучше,

1216
00:41:58,079 --> 00:41:59,440
чем

1217
00:41:59,440 --> 00:42:01,680
суперклей roberta, оказалось, что это тест,

1218
00:42:01,680 --> 00:42:03,920
который сильно выигрывает от больших моделей,

1219
00:42:03,920 --> 00:42:06,319
и поэтому вы действительно можете увидеть резкое

1220
00:42:06,319 --> 00:42:07,839
увеличение производительности модели, когда

1221
00:42:07,839 --> 00:42:10,319
мы масштабируем модель,

1222
00:42:10,319 --> 00:42:12,240
с другой стороны, мы не получили

1223
00:42:12,240 --> 00:42:13,839
состояние  - самые современные результаты по любому из

1224
00:42:13,839 --> 00:42:15,599
наборов данных перевода, и причина, по

1225
00:42:15,599 --> 00:42:17,040
которой мы думаем, что это правда, заключается в том,

1226
00:42:17,040 --> 00:42:18,640
что все современные

1227
00:42:18,640 --> 00:42:20,319
результаты в то время для этих

1228
00:42:20,319 --> 00:42:22,880
наборов данных перевода использовали обратный перевод, и если

1229
00:42:22,880 --> 00:42:24,880
Вы помните, что

1230
00:42:24,880 --> 00:42:28,160
в нашей модели мы проводили предварительное обучение только английскому языку, и мы ожидаем,

1231
00:42:28,160 --> 00:42:30,800
что с точки зрения использования

1232
00:42:30,800 --> 00:42:32,960
немаркированных данных более эффективно

1233
00:42:32,960 --> 00:42:34,480
использовать обратный перевод для

1234
00:42:34,480 --> 00:42:36,079
задач машинного перевода.  Если мы использовали этот

1235
00:42:36,079 --> 00:42:39,040
английский только для предварительной тренировки,

1236
00:42:39,040 --> 00:42:40,640
я должен упомянуть, конечно, что эти

1237
00:42:40,640 --> 00:42:42,640
результаты сейчас немного устарели, и некоторые

1238
00:42:42,640 --> 00:42:44,480
из этих результатов были

1239
00:42:44,480 --> 00:42:47,599
побиты последующими моделями,

1240
00:42:47,920 --> 00:42:48,720
поэтому

1241
00:42:48,720 --> 00:42:50,319
теперь я просто быстро сделаю пробку, которая  вы

1242
00:42:50,319 --> 00:42:51,680
знаете, что весь наш код выпущен, наши

1243
00:42:51,680 --> 00:42:53,599
предварительно обученные модели были выпущены,

1244
00:42:53,599 --> 00:42:54,880
вы можете использовать их в нашей кодовой

1245
00:42:54,880 --> 00:42:56,560
базе, они также, конечно, в

1246
00:42:56,560 --> 00:42:59,280
кодовой базе обнимающих лиц трансформеров,

1247
00:42:59,280 --> 00:43:02,319
мы сделали совместную работу в то время, которая показывает

1248
00:43:03,599 --> 00:43:06,160
довольно простая демонстрация того, как взять одну

1249
00:43:06,160 --> 00:43:08,160
из наших предварительно обученных моделей и в основном

1250
00:43:08,160 --> 00:43:11,680
обучить ее на tsv-файле входных данных и

1251
00:43:11,680 --> 00:43:13,839
целей, поэтому, поскольку все проблемы являются

1252
00:43:13,839 --> 00:43:15,200
проблемами преобразования текста в текст, вам просто нужно

1253
00:43:15,200 --> 00:43:17,119
дать модели некоторый входной текст и  какой-то

1254
00:43:17,119 --> 00:43:19,040
целевой текст, и это все, что вам нужно для

1255
00:43:19,040 --> 00:43:20,720
точной настройки модели,

1256
00:43:20,720 --> 00:43:22,160
и вы можете сделать так, чтобы вы могли действительно

1257
00:43:22,160 --> 00:43:23,680
точно настроить модель с тремя миллиардами

1258
00:43:23,680 --> 00:43:26,720
параметров на бесплатном совместном tpu,

1259
00:43:26,720 --> 00:43:30,640
используя ссылку внизу здесь

1260
00:43:30,880 --> 00:43:33,040
отлично, пока что мы '  я говорил

1261
00:43:33,040 --> 00:43:34,640
о предварительной тренировке только на английском  Я

1262
00:43:34,640 --> 00:43:36,480
имею в виду, что мы применили ее к машинному

1263
00:43:36,480 --> 00:43:38,800
переводу и последующим задачам, поэтому

1264
00:43:38,800 --> 00:43:40,319
возникает естественный вопрос: вы знаете,

1265
00:43:40,319 --> 00:43:42,720
что насчет всех других языков,

1266
00:43:42,720 --> 00:43:45,200
почему бы не обучить многоязычную модель, и

1267
00:43:45,200 --> 00:43:46,720
это то, что мы сделали

1268
00:43:46,720 --> 00:43:48,240
совсем недавно, и на самом деле позволили мне просто  пауза,

1269
00:43:48,240 --> 00:43:49,760
потому что я видел пару вопросов,

1270
00:43:49,760 --> 00:43:51,599
которые выходят, я хочу убедиться, что

1271
00:43:51,599 --> 00:43:53,920
я никого не оставлю, когда я перейду

1272
00:43:53,920 --> 00:43:55,359
к следующему

1273
00:43:55,359 --> 00:43:57,680
разделу,

1274
00:43:59,119 --> 00:44:00,720
конечно, поэтому

1275
00:44:00,720 --> 00:44:03,359
один из вопросов касается

1276
00:44:03,359 --> 00:44:05,920
настройки многозадачности, если вы

1277
00:44:05,920 --> 00:44:07,760
включаете неизвестную задачу  prefix делает

1278
00:44:07,760 --> 00:44:09,520
что-то интересное, и если вы

1279
00:44:09,520 --> 00:44:11,440
не включаете префикс, что случилось, что

1280
00:44:11,440 --> 00:44:13,040
это делает,

1281
00:44:14,400 --> 00:44:17,599
если вы включаете префикс неизвестной задачи

1282
00:44:17,599 --> 00:44:19,520
или если вы вообще не включаете префикс,

1283
00:44:19,520 --> 00:44:21,839
что он, вероятно, будет делать  это применить

1284
00:44:21,839 --> 00:44:24,079
неконтролируемую цель, потому что мы на

1285
00:44:24,079 --> 00:44:26,079
самом деле не использовали префикс задачи

1286
00:44:26,079 --> 00:44:29,200
для неконтролируемой цели, ну,

1287
00:44:29,200 --> 00:44:31,839
я думаю, я должен сказать,

1288
00:44:31,839 --> 00:44:32,560
что

1289
00:44:32,560 --> 00:44:34,079
это не совсем так, это не совсем так,

1290
00:44:34,079 --> 00:44:35,599
потому что в inpu не будет никаких

1291
00:44:35,599 --> 00:44:36,480
маркеров

1292
00:44:36,480 --> 00:44:38,880
дозорных  Так что мы на самом деле видим, что

1293
00:44:38,880 --> 00:44:40,800
это обычно происходит, так это то, что он выводит какие-

1294
00:44:40,800 --> 00:44:42,240
то связанные слова и некоторые другие

1295
00:44:42,240 --> 00:44:45,200
маркеры дозорных и тарабарщину, эм,

1296
00:44:45,200 --> 00:44:47,200
это не очень полезно, я думаю, в

1297
00:44:47,200 --> 00:44:49,040
результате

1298
00:44:49,040 --> 00:44:50,960
у нас есть вопросы по обратному

1299
00:44:50,960 --> 00:44:52,160
переводу, я не думаю, что они  слышал

1300
00:44:52,160 --> 00:44:53,280
об обратном переводе в остальной

1301
00:44:53,280 --> 00:44:55,200
части курса,

1302
00:44:55,200 --> 00:44:56,560
поэтому обратный перевод - довольно

1303
00:44:56,560 --> 00:44:58,240
простой метод, основная идея заключается в

1304
00:44:58,240 --> 00:45:00,160
том, что если у меня есть немаркированные текстовые данные на

1305
00:45:00,160 --> 00:45:02,640
одном языке, я использую свою текущую модель для

1306
00:45:02,640 --> 00:45:04,400
перевода этих данных

1307
00:45:04,400 --> 00:45:05,359
на

1308
00:45:05,359 --> 00:45:08,640
какой-то конкретный язык, и я использую

1309
00:45:08,640 --> 00:45:11,200
это  как обучающие данные, то впоследствии

1310
00:45:11,200 --> 00:45:13,119
для моей модели это похоже на самообучение,

1311
00:45:13,119 --> 00:45:14,640
если вы знакомы с этим, в

1312
00:45:14,640 --> 00:45:16,079
основном вы делаете прогнозы на

1313
00:45:16,079 --> 00:45:17,680
немаркированных данных, а затем используете эти

1314
00:45:17,680 --> 00:45:19,839
прогнозы для обучения модели,

1315
00:45:19,839 --> 00:45:22,160
это оказывается полезным

1316
00:45:22,160 --> 00:45:24,319
ага, а затем один э-э  один

1317
00:45:25,440 --> 00:45:27,359
подробный вопрос о максимальной длине ввода

1318
00:45:27,359 --> 00:45:28,800
и максимальной длине вывода, как вы

1319
00:45:28,800 --> 00:45:30,480
их выбрали, вы тоже изучали это

1320
00:45:31,359 --> 00:45:33,359
да, поэтому большинство задач, которые мы рассматривали,

1321
00:45:33,359 --> 00:45:35,280
не были  ave входная длина

1322
00:45:35,280 --> 00:45:37,599
значительно превышает 512 токенов

1323
00:45:37,599 --> 00:45:39,520
большую часть времени с использованием

1324
00:45:39,520 --> 00:45:42,960
стратегии токенизации, которую мы использовали,

1325
00:45:42,960 --> 00:45:45,359
и поэтому мы использовали максимальную длину ввода

1326
00:45:45,359 --> 00:45:46,400
512,

1327
00:45:46,400 --> 00:45:48,400
но мы использовали схему кодирования позиции,

1328
00:45:48,400 --> 00:45:50,880
которая допускает произвольную длину ввода, и

1329
00:45:50,880 --> 00:45:53,280
у нас действительно есть uh  в последующей работе

1330
00:45:53,280 --> 00:45:57,040
доработал t5 для последовательностей

1331
00:45:57,040 --> 00:45:59,440
длины 2048. кроме этого вы начинаете

1332
00:45:59,440 --> 00:46:01,599
попадать в проблемы с памятью из-за

1333
00:46:01,599 --> 00:46:03,520
квадратичной сложности памяти, внимание,

1334
00:46:03,520 --> 00:46:06,079
но, в принципе, вы

1335
00:46:06,079 --> 00:46:08,400
можете применить его к длинным последовательностям,

1336
00:46:08,400 --> 00:46:09,680
а затем, возможно, к одному  последнее, что, если у вас

1337
00:46:09,680 --> 00:46:11,599
есть секунда, - это я

1338
00:46:11,599 --> 00:46:12,880
снова говорю о том, что

1339
00:46:12,880 --> 00:46:16,160
перевод снова кажется

1340
00:46:16,160 --> 00:46:18,480
не лучшим приложением для этого, и

1341
00:46:19,440 --> 00:46:21,200
какова ваша интуиция относительно того, почему

1342
00:46:21,200 --> 00:46:22,960
такие переводы делают это,

1343
00:46:22,960 --> 00:46:24,400
но не получают выгоды от бесплатного

1344
00:46:24,400 --> 00:46:26,560
обучения тому, что  Причина, по которой

1345
00:46:26,560 --> 00:46:28,960
я не знаю, я действительно не уверен, я

1346
00:46:31,839 --> 00:46:34,800
могу только догадываться, я думаю,

1347
00:46:34,800 --> 00:46:37,119
что предварительное обучение помогает модели

1348
00:46:37,119 --> 00:46:38,640
узнать значение слов,

1349
00:46:38,640 --> 00:46:40,319
это помогает модели выучить некоторые вещи  ld

1350
00:46:40,319 --> 00:46:41,760
знания, о которых я расскажу

1351
00:46:41,760 --> 00:46:43,200
немного позже, и это очень

1352
00:46:43,200 --> 00:46:44,480
расплывчатая концепция,

1353
00:46:45,359 --> 00:46:48,000
я думаю, что для изучения перевода

1354
00:46:48,000 --> 00:46:50,319
мировые знания не очень полезны,

1355
00:46:50,319 --> 00:46:52,240
потому что все знания,

1356
00:46:52,240 --> 00:46:55,040
необходимые для перевода предложения,

1357
00:46:55,040 --> 00:46:58,560
по большей части  находится в предложении,

1358
00:46:58,560 --> 00:46:59,520
и

1359
00:46:59,520 --> 00:47:01,119
поэтому в основном вся

1360
00:47:01,119 --> 00:47:02,960
информация о стиле контекстных знаний,

1361
00:47:02,960 --> 00:47:04,560
которая вам нужна для создания немецкого

1362
00:47:04,560 --> 00:47:06,800
предложения, находится во входном предложении, поэтому

1363
00:47:06,800 --> 00:47:08,000
получение мировых знаний во время

1364
00:47:08,000 --> 00:47:10,480
предварительного обучения не очень полезно,

1365
00:47:10,480 --> 00:47:11,760
конечно, полезно знать, что означают слова

1366
00:47:12,640 --> 00:47:15,040
но в какой-то степени

1367
00:47:15,040 --> 00:47:16,560
это самый простой сигнал, который можно уловить

1368
00:47:16,560 --> 00:47:19,040
во время тренировки, и я полагаю,

1369
00:47:19,040 --> 00:47:21,200
что это было бы моим предположением, у меня нет никаких

1370
00:47:21,200 --> 00:47:22,839
строгих доказательств

1371
00:47:22,839 --> 00:47:25,280
этого, спасибо,

1372
00:47:26,160 --> 00:47:29,920
отлично, так что как будто я говорил, что мы

1373
00:47:29,920 --> 00:47:31,760
тренировали это  модель только на английском языке, и мы

1374
00:47:31,760 --> 00:47:32,800
хотели

1375
00:47:32,800 --> 00:47:35,520
устранить основной недостаток, который

1376
00:47:35,520 --> 00:47:38,000
действительно может говорить только на одном языке, поэтому мы

1377
00:47:38,000 --> 00:47:39,520
представили модель под названием mt5

1378
00:47:39,520 --> 00:47:42,079
многоязычный t5 и действительно по большей

1379
00:47:42,079 --> 00:47:44,240
части if  вы помните одну вещь о

1380
00:47:44,240 --> 00:47:46,720
mt5, это в основном то, что это точно такая

1381
00:47:46,720 --> 00:47:48,880
же модель, но обученная на многоязычном

1382
00:47:48,880 --> 00:47:51,760
корпусе, а текстовый текстовый формат такой

1383
00:47:51,760 --> 00:47:54,400
же, как вы знаете, мы подаем префиксы задач,

1384
00:47:54,400 --> 00:47:56,480
но мы можем подавать контент на

1385
00:47:56,480 --> 00:47:58,400
разных языках, и мы можем проводить

1386
00:47:58,400 --> 00:48:00,559
классификацию  задачи, которые мы можем выполнять,

1387
00:48:00,559 --> 00:48:03,040
отвечая на вопросы с помощью mt5

1388
00:48:03,040 --> 00:48:04,480
точно так же, как мы можем делать с

1389
00:48:04,480 --> 00:48:06,079
t5,

1390
00:48:06,079 --> 00:48:08,319
поэтому, как я уже сказал, уместная вещь о

1391
00:48:08,319 --> 00:48:10,720
mt5 заключалась в создании многоязычного

1392
00:48:10,720 --> 00:48:13,359
варианта c4, в целом процесс очень

1393
00:48:13,359 --> 00:48:15,920
похож на процесс, который мы использовали для c4

1394
00:48:15,920 --> 00:48:18,960
за исключением того, что он включает 101 язык,

1395
00:48:18,960 --> 00:48:21,520
который мы обнаружили с помощью детектора языков с открытым исходным кодом,

1396
00:48:23,200 --> 00:48:24,480
мы также

1397
00:48:24,480 --> 00:48:26,319
извлекли данные из более распространенных дампов сканирования,

1398
00:48:26,319 --> 00:48:27,680
потому что, особенно для языков с низким уровнем

1399
00:48:27,680 --> 00:48:29,280
ресурсов, было трудно получить

1400
00:48:29,280 --> 00:48:30,640
достаточно данных

1401
00:48:30,640 --> 00:48:32,880
только из одного общего дампа сканирования, и вы

1402
00:48:32,880 --> 00:48:34,240
можете увидеть список  из языков, которые мы

1403
00:48:34,240 --> 00:48:36,079
здесь включаем, в конечном итоге набор данных

1404
00:48:36,079 --> 00:48:38,000
оказался размером около 27 терабайт,

1405
00:48:40,079 --> 00:48:42,000
так что вот распределение

1406
00:48:42,000 --> 00:48:44,960
количества страниц в mc4  набор данных по обучению

1407
00:48:44,960 --> 00:48:46,720
для различных языков, вы можете увидеть

1408
00:48:46,720 --> 00:48:48,640
наш самый высокий ресурсный язык -

1409
00:48:48,640 --> 00:48:50,400
английский, где у нас около

1410
00:48:50,400 --> 00:48:52,160
трех миллиардов страниц с

1411
00:48:52,160 --> 00:48:54,319
общим количеством триллионов токенов. Самый низкий ресурсный

1412
00:48:54,319 --> 00:48:56,319
язык - йоруба, всего около пятидесяти

1413
00:48:56,319 --> 00:48:58,559
тысяч страниц, так что вы можете это увидеть.

1414
00:48:58,559 --> 00:48:59,839
объем данных, которые мы получили для каждого

1415
00:48:59,839 --> 00:49:01,760
языка, варьируется на много порядков

1416
00:49:01,760 --> 00:49:04,240
величины,

1417
00:49:04,240 --> 00:49:06,160
потому что общая стратегия заключается в

1418
00:49:06,160 --> 00:49:07,760
использовании такого рода температурного масштабирования,

1419
00:49:07,760 --> 00:49:09,359
о котором я упоминал ранее, когда в основном

1420
00:49:09,359 --> 00:49:11,440
вы выбираете данные на определенном

1421
00:49:11,440 --> 00:49:12,640
языке,

1422
00:49:12,640 --> 00:49:15,760
используя масштабирование  количество

1423
00:49:15,760 --> 00:49:17,520
примеров на этом языке по

1424
00:49:17,520 --> 00:49:20,800
температуре и как и я прошу прощения,

1425
00:49:20,800 --> 00:49:23,119
что здесь температура на один

1426
00:49:23,119 --> 00:49:24,720
выше, на один выше температуры,

1427
00:49:24,720 --> 00:49:26,720
которую я описал ранее, поэтому в этом

1428
00:49:26,720 --> 00:49:28,559
случае, когда температура становится все меньше и

1429
00:49:28,559 --> 00:49:30,240
меньше, вы подходите все ближе и ближе  к

1430
00:49:30,240 --> 00:49:32,240
равномерному распределению,

1431
00:49:32,240 --> 00:49:34,480
чистый эффект этого заключается в том, что при очень

1432
00:49:34,480 --> 00:49:36,240
низких температурах вы, как правило, лучше справляетесь

1433
00:49:36,240 --> 00:49:38,000
с последующими задачами с низким уровнем ресурсов

1434
00:49:38,000 --> 00:49:40,160
языки, такие как урду, но по мере того, как вы

1435
00:49:40,160 --> 00:49:41,920
увеличиваете температуру, так что вы в

1436
00:49:41,920 --> 00:49:43,119
основном делаете примеры

1437
00:49:43,119 --> 00:49:44,480
пропорционального смешивания разных

1438
00:49:44,480 --> 00:49:46,480
языков, которые вы лучше делаете на языках с высокими ресурсами,

1439
00:49:46,480 --> 00:49:48,880
таких как русский,

1440
00:49:48,880 --> 00:49:50,319
поэтому мы взяли

1441
00:49:51,040 --> 00:49:54,800
mt, мы взяли mc4, мы снова предварительно обучили mt5, в

1442
00:49:54,800 --> 00:49:56,720
основном все было  Мы сделали то же самое,

1443
00:49:56,720 --> 00:49:58,720
мы немного увеличили словарный запас, чтобы

1444
00:49:58,720 --> 00:50:00,559
приспособить его ко всем различным языкам,

1445
00:50:00,559 --> 00:50:02,480
но в целом объем предварительного обучения,

1446
00:50:02,480 --> 00:50:04,960
размеры моделей и т. д. в основном те

1447
00:50:04,960 --> 00:50:07,359
же, и в конечном итоге мы

1448
00:50:07,359 --> 00:50:08,800
получили самое современное решение некоторых

1449
00:50:08,800 --> 00:50:10,800
задач в экстремальном тесте.  Я

1450
00:50:10,800 --> 00:50:12,319
заметил, что мы не сообщаем о результатах для

1451
00:50:12,319 --> 00:50:14,720
некоторых из этих задач, частично

1452
00:50:14,720 --> 00:50:17,680
потому, что extreme разработан для кодировщиков предложений,

1453
00:50:17,680 --> 00:50:21,839
таких как bert uh t5 is a mt5, наши

1454
00:50:21,839 --> 00:50:24,240
модели кодировщика-декодера, которые мы не

1455
00:50:24,240 --> 00:50:26,160
экспериментировали с использованием кодировщика

1456
00:50:26,160 --> 00:50:28,319
самостоятельно, но для того, чтобы  атаковать некоторые из этих

1457
00:50:28,319 --> 00:50:29,760
проблем, например проблемы поиска предложений,

1458
00:50:29,760 --> 00:50:31,520
вам нужна модель, которая может

1459
00:50:31,520 --> 00:50:33,359
выводить единственное

1460
00:50:33,359 --> 00:50:35,920
векторное представление вашей последовательности,

1461
00:50:35,920 --> 00:50:37,920
а у нас нет этой  в t5, поэтому мы

1462
00:50:37,920 --> 00:50:41,839
не применяли его к этим задачам,

1463
00:50:42,000 --> 00:50:43,920
один интересный вывод из этой статьи,

1464
00:50:43,920 --> 00:50:45,839
который я просто кратко упомяну здесь, заключается в

1465
00:50:45,839 --> 00:50:46,640
том, что

1466
00:50:46,640 --> 00:50:48,400
в

1467
00:50:48,400 --> 00:50:49,920
основном есть

1468
00:50:49,920 --> 00:50:51,920
несколько настроек, которые люди считают

1469
00:50:51,920 --> 00:50:54,640
многоязычными тестами, а один -

1470
00:50:54,640 --> 00:50:56,800
случай, когда ноль  выстрел, и в этом случае

1471
00:50:56,800 --> 00:50:59,040
вы не проводите никакого предварительного обучения на

1472
00:50:59,040 --> 00:51:01,520
языке, извините, у вас нет

1473
00:51:01,520 --> 00:51:03,680
данных для точной настройки на каждом конкретном

1474
00:51:03,680 --> 00:51:05,359
языке, у вас есть только данные предварительного обучения

1475
00:51:05,359 --> 00:51:07,200
на этих языках, поэтому вы можете выполнить точную настройку

1476
00:51:07,200 --> 00:51:09,119
допустим, только на английском языке, а затем вы

1477
00:51:09,119 --> 00:51:10,720
вводите в модель какой-то текст на другом

1478
00:51:10,720 --> 00:51:12,160
языке и смотрите, дает ли она

1479
00:51:12,160 --> 00:51:13,599
правильные прогнозы,

1480
00:51:13,599 --> 00:51:15,440
следующая настройка - это настройка поезда перевода,

1481
00:51:15,440 --> 00:51:16,640
в которой вы берете модель машинного

1482
00:51:16,640 --> 00:51:19,040
перевода и переводите данные

1483
00:51:19,040 --> 00:51:21,040
в тонкой настройке на английский  корпус на

1484
00:51:21,040 --> 00:51:22,960
разные языки, а затем последняя

1485
00:51:22,960 --> 00:51:24,800
настройка - это настройка многозадачности на языке,

1486
00:51:24,800 --> 00:51:26,640
это настройка, при которой вы

1487
00:51:26,640 --> 00:51:28,240
предполагаете, что у вас есть

1488
00:51:28,240 --> 00:51:30,720
базовые данные золотого стандарта на каждом языке, на котором

1489
00:51:30,720 --> 00:51:32,400
вы хотите использовать модель.  чтобы иметь возможность

1490
00:51:32,400 --> 00:51:33,920
обрабатывать данные,

1491
00:51:33,920 --> 00:51:36,559
и вывод здесь на самом деле заключается в том, что

1492
00:51:37,920 --> 00:51:40,079
разница в производительности между небольшими

1493
00:51:40,079 --> 00:51:42,559
моделями и нашей самой большой моделью, когда мы идем

1494
00:51:42,559 --> 00:51:46,240
по оси x, намного больше

1495
00:51:46,240 --> 00:51:48,079
для нулевого выстрела и трансляции

1496
00:51:48,079 --> 00:51:49,839
настроек поезда, чем для  настройка многозадачности на языке,

1497
00:51:49,839 --> 00:51:52,000
так что

1498
00:51:52,000 --> 00:51:55,440
в основном это говорит нам о том, что модель

1499
00:51:56,480 --> 00:51:58,880
изучает гораздо более широкое распределение языков,

1500
00:51:58,880 --> 00:52:00,480
если у нее гораздо большее количество

1501
00:52:00,480 --> 00:52:01,839
параметров,

1502
00:52:01,839 --> 00:52:04,160
и она может выполнять такие

1503
00:52:04,160 --> 00:52:07,119
задачи, как обучение с нулевым выстрелом, многоязычное

1504
00:52:07,119 --> 00:52:09,280
обучение задачам  Намного лучше, когда у него

1505
00:52:09,280 --> 00:52:12,000
больше параметров,

1506
00:52:12,559 --> 00:52:14,480
поэтому вы знаете, что

1507
00:52:14,480 --> 00:52:16,480
более крупные модели могут соответствовать большему количеству

1508
00:52:16,480 --> 00:52:18,480
знаний о большем количестве языков,

1509
00:52:18,480 --> 00:52:20,160
у нас был другой документ, в котором мы в основном

1510
00:52:20,160 --> 00:52:22,000
пытались ответить на вопрос, который вы знаете,

1511
00:52:22,000 --> 00:52:23,920
сколько и какие знания

1512
00:52:23,920 --> 00:52:26,960
выбирает модель  во время предварительного обучения,

1513
00:52:26,960 --> 00:52:30,240
и поэтому, чтобы ответить на этот вопрос, мы в

1514
00:52:30,240 --> 00:52:32,079
основном ввели новый вариант задачи

1515
00:52:32,079 --> 00:52:34,000
с ответом на вопрос, поэтому задача с ответом на

1516
00:52:34,000 --> 00:52:36,000
вопрос  f поставляется

1517
00:52:36,000 --> 00:52:37,680
в нескольких различных вариантах, самый

1518
00:52:37,680 --> 00:52:39,040
простой вариант, о котором я уже упоминал,

1519
00:52:39,040 --> 00:52:41,280
- понимание прочитанного, и в

1520
00:52:41,280 --> 00:52:43,359
этом случае модели в основном дается

1521
00:52:43,359 --> 00:52:45,359
абзац или статья, а затем

1522
00:52:45,359 --> 00:52:47,040
задается вопрос об абзаце или

1523
00:52:47,040 --> 00:52:49,599
статье, и она должна в основном

1524
00:52:49,599 --> 00:52:51,680
извлеките ответ, чтобы вы могли видеть, спрашивают ли его,

1525
00:52:51,680 --> 00:52:53,280
какого цвета лимон, он должен

1526
00:52:53,280 --> 00:52:56,000
посмотреть в э-э в абзаце,

1527
00:52:56,000 --> 00:52:58,240
который он видел, и увидеть, что у

1528
00:52:58,240 --> 00:52:59,920
лимона есть желтый фрукт, и выведите

1529
00:52:59,920 --> 00:53:01,520
слово желтый, так что это любезно  из

1530
00:53:01,520 --> 00:53:03,040
простейшей формы задачи ответа на вопрос

1531
00:53:04,000 --> 00:53:05,680
более сложная форма - это то, что люди

1532
00:53:05,680 --> 00:53:07,760
называют ответом на вопрос открытой области, и

1533
00:53:07,760 --> 00:53:09,200
в этом случае вы предполагаете, что модели

1534
00:53:09,200 --> 00:53:11,440
задается вопрос и имеет доступ к

1535
00:53:11,440 --> 00:53:13,839
большой внешней базе данных знаний,

1536
00:53:13,839 --> 00:53:16,480
возможно, всей Википедии, поэтому  модель

1537
00:53:16,480 --> 00:53:18,880
должна сделать две вещи: сначала она должна

1538
00:53:18,880 --> 00:53:21,359
найти статью или фрагмент текста, который

1539
00:53:21,359 --> 00:53:22,880
содержит ответ

1540
00:53:22,880 --> 00:53:24,880
в базе данных, а затем

1541
00:53:24,880 --> 00:53:27,760
извлечь ответ из статьи и

1542
00:53:27,760 --> 00:53:29,359
так далее.  Этот дополнительный шаг поиска,

1543
00:53:29,359 --> 00:53:30,720
который немного усложняет задачу,

1544
00:53:31,760 --> 00:53:33,599
но мы представили своего рода третий

1545
00:53:33,599 --> 00:53:34,880
вариант ответа на вопрос, который мы

1546
00:53:34,880 --> 00:53:36,880
называем вопросом закрытой книги. Ответ на это

1547
00:53:36,880 --> 00:53:38,720
имя черпает вдохновение из закрытых книжных

1548
00:53:38,720 --> 00:53:39,920
экзаменов.

1549
00:53:42,000 --> 00:53:43,839
вопрос у него нет

1550
00:53:43,839 --> 00:53:45,440
доступа к внешнему источнику знаний

1551
00:53:45,440 --> 00:53:47,599
он не может искать информацию где-либо

1552
00:53:47,599 --> 00:53:49,680
он может ответить на вопрос только на

1553
00:53:49,680 --> 00:53:51,200
основе знаний, полученных

1554
00:53:51,200 --> 00:53:52,960
во время предварительного обучения, поэтому, если вы зададите

1555
00:53:52,960 --> 00:53:55,200
модели вопрос, какой цвет у нее лимон

1556
00:53:55,200 --> 00:53:56,640
правильно вывести модель желтым,

1557
00:53:56,640 --> 00:53:58,960
потому что она,

1558
00:53:58,960 --> 00:54:01,040
так сказать, знает, что лимоны

1559
00:54:01,040 --> 00:54:02,079
желтые,

1560
00:54:02,079 --> 00:54:04,079
так что это хороший способ аргументировать

1561
00:54:04,079 --> 00:54:06,480
проверку знаний количество

1562
00:54:06,480 --> 00:54:08,480
знаний, хранящихся в модели во время

1563
00:54:08,480 --> 00:54:10,880
предварительного обучения,

1564
00:54:10,880 --> 00:54:12,880
так почему мы можем ожидать, что это сработает?  ну,

1565
00:54:12,880 --> 00:54:14,400
вы можете себе представить, что мы

1566
00:54:14,400 --> 00:54:17,119
делаем нашу обычную предварительную подготовку к t5,

1567
00:54:17,119 --> 00:54:18,640
мы маскируем слова и обучаем

1568
00:54:18,640 --> 00:54:20,880
модель предсказывать массивные интервалы

1569
00:54:20,880 --> 00:54:22,160
слов

1570
00:54:22,160 --> 00:54:24,720
a  И вы можете представить, что где-то

1571
00:54:24,720 --> 00:54:26,240
во время предварительного обучения он видит предложение, в

1572
00:54:26,240 --> 00:54:28,319
котором говорится, что президент франклин бланк родился

1573
00:54:28,319 --> 00:54:31,040
пустой январь 1882 г., цель здесь

1574
00:54:31,040 --> 00:54:34,800
состоит в том, чтобы вывести д рузвельт было пустым,

1575
00:54:34,800 --> 00:54:37,040
а затем во время точной настройки мы

1576
00:54:37,040 --> 00:54:40,079
обучили модель предсказывать, когда наступит

1577
00:54:40,079 --> 00:54:42,559
год.  1882 г., когда был задан вопрос,

1578
00:54:42,559 --> 00:54:44,960
когда родился Франклин д Рузвельт, и

1579
00:54:44,960 --> 00:54:47,119
вы можете надеяться, что он как бы

1580
00:54:47,119 --> 00:54:50,880
возвращается к своей предтренировочной задаче и

1581
00:54:50,880 --> 00:54:52,480
вспоминает некоторые знания, полученные им

1582
00:54:52,480 --> 00:54:53,920
во время предтренировки, чтобы правильно ответить на

1583
00:54:53,920 --> 00:54:56,880
этот вопрос,

1584
00:54:57,040 --> 00:55:00,079
поэтому мы  взял некоторые стандартные открытые вопросы домена,

1585
00:55:00,079 --> 00:55:01,760
отвечая на данные, устанавливает естественные

1586
00:55:01,760 --> 00:55:04,799
вопросы, веб-вопросы и мелочи qa,

1587
00:55:04,799 --> 00:55:07,839
и в основном удалил весь контекст

1588
00:55:07,839 --> 00:55:10,559
и обучил нашу модель предсказывать

1589
00:55:10,559 --> 00:55:12,400
правильный ответ, когда задается какой-то

1590
00:55:12,400 --> 00:55:14,559
конкретный вопрос, а затем оценил

1591
00:55:14,559 --> 00:55:16,160
ее производительность на тестовом наборе для каждого

1592
00:55:16,160 --> 00:55:17,520
из них  задач,

1593
00:55:17,520 --> 00:55:19,440
и в этой таблице мы сравниваем

1594
00:55:19,440 --> 00:55:20,880
современные результаты для открытой

1595
00:55:20,880 --> 00:55:22,640
доменной системы. Это системы, которые

1596
00:55:22,640 --> 00:55:24,319
явно извлекают знания  уступ от

1597
00:55:24,319 --> 00:55:28,559
внешнего источника знаний по сравнению с t5,

1598
00:55:28,559 --> 00:55:29,920
когда он был обучен в этой закрытой

1599
00:55:29,920 --> 00:55:31,599
книжной настройке, вы заметите, что на

1600
00:55:31,599 --> 00:55:32,960
самом деле мы используем немного другую

1601
00:55:32,960 --> 00:55:36,079
версию t5, здесь мы используем t5.1.1,

1602
00:55:36,079 --> 00:55:37,880
уместное отличие состоит только в том, что

1603
00:55:37,880 --> 00:55:40,960
t5.  1.1 не был многозадачным предварительно обученным,

1604
00:55:40,960 --> 00:55:42,880
он был предварительно обучен с использованием

1605
00:55:42,880 --> 00:55:44,960
неконтролируемой цели, потому что мы сделали

1606
00:55:44,960 --> 00:55:46,559
это снова, потому что мы хотим

1607
00:55:46,559 --> 00:55:47,839
измерить объем знаний, которые модель

1608
00:55:47,839 --> 00:55:50,400
получила во время предварительного обучения,

1609
00:55:50,400 --> 00:55:52,160
и вы можете видеть, что мы  на самом деле вы знаете, что

1610
00:55:52,160 --> 00:55:54,000
достаточно высокая производительность, возможно,

1611
00:55:54,000 --> 00:55:57,920
достойная производительность,

1612
00:56:00,640 --> 00:56:01,760
я не хочу снова использовать производительность,

1613
00:56:01,760 --> 00:56:03,359
снова слово производительность,

1614
00:56:03,359 --> 00:56:05,280
точность в основном для каждого из этих

1615
00:56:05,280 --> 00:56:07,359
наборов данных увеличивается по мере увеличения размера модели,

1616
00:56:07,359 --> 00:56:10,079
что, возможно, в некоторой степени

1617
00:56:10,079 --> 00:56:11,599
предполагает, что  более крупные модели

1618
00:56:11,599 --> 00:56:12,880
получили больше знаний во время

1619
00:56:12,880 --> 00:56:14,960
предварительного обучения, но в конечном итоге мы

1620
00:56:14,960 --> 00:56:16,640
отстали от современных результатов для

1621
00:56:16,640 --> 00:56:18,480
систем с открытой предметной областью, которые явно

1622
00:56:18,480 --> 00:56:20,480
извлекают знания,

1623
00:56:20,480 --> 00:56:22,880
чтобы  попытаться восполнить этот пробел,

1624
00:56:22,880 --> 00:56:25,680
мы использовали эту цель, называемую

1625
00:56:25,680 --> 00:56:28,400
маскированием заметного промежутка, из статьи, называемой

1626
00:56:29,839 --> 00:56:32,480
предварительным обучением модели расширенного языка поиска, и маскирование промежутка плавания

1627
00:56:32,480 --> 00:56:34,319
- очень простая идея,

1628
00:56:34,319 --> 00:56:36,079
идея состоит в том, что вместо того, чтобы маскировать

1629
00:56:36,079 --> 00:56:37,920
слова наугад в вашем предварительном

1630
00:56:37,920 --> 00:56:41,040
-обучение вы фактически маскируете

1631
00:56:41,040 --> 00:56:43,520
сущности явным образом, чтобы

1632
00:56:43,520 --> 00:56:46,640
имена людей помещали даты и т. д.,

1633
00:56:46,640 --> 00:56:49,280
и вы в основном просто используете готовый с

1634
00:56:49,280 --> 00:56:51,359
именем конец вашего распознавателя, чтобы

1635
00:56:51,359 --> 00:56:52,880
выяснить, какие сущности находятся в вашем

1636
00:56:52,880 --> 00:56:55,200
наборе данных перед обучением, и вы обучаете

1637
00:56:55,200 --> 00:56:56,960
модель для заполнения  эм,

1638
00:56:56,960 --> 00:56:59,599
выступающие интервалы вместо случайных интервалов,

1639
00:56:59,599 --> 00:57:02,400
поэтому мы взяли t 5.1.1 после

1640
00:57:02,400 --> 00:57:04,960
того, как он был предварительно обучен, и продолжили

1641
00:57:04,960 --> 00:57:07,359
предварительное обучение по маскированию интервала физиологического раствора, а

1642
00:57:07,359 --> 00:57:08,720
затем измерили производительность в наших

1643
00:57:08,720 --> 00:57:10,799
последующих задачах после точной настройки, и

1644
00:57:10,799 --> 00:57:12,559
вы можете  увидим, что чем более заметным

1645
00:57:12,559 --> 00:57:14,400
мы сделали предварительную тренировку по маске диапазона,

1646
00:57:14,400 --> 00:57:16,240
тем лучше извините меня, тем лучше и

1647
00:57:16,240 --> 00:57:18,640
лучше получилась производительность, когда мы

1648
00:57:18,640 --> 00:57:20,880
отрегулировали последующие задачи и в

1649
00:57:20,880 --> 00:57:22,640
конечном итоге смогли  чтобы значительно закрыть некоторые из

1650
00:57:22,640 --> 00:57:25,040
этих пробелов и фактически

1651
00:57:25,040 --> 00:57:27,760
превзойти лучшую систему с открытым доменом

1652
00:57:27,760 --> 00:57:30,160
по веб-вопросам, добавив

1653
00:57:30,160 --> 00:57:34,559
маскировку заметности веера к uh к t5.1.1,

1654
00:57:34,559 --> 00:57:37,359
так что это просто сообщение, чтобы сказать вам,

1655
00:57:37,359 --> 00:57:40,079
что цель имеет значение, и

1656
00:57:40,079 --> 00:57:42,400
делают такие люди  в то время

1657
00:57:42,400 --> 00:57:43,760
люди не называли это так, но теперь

1658
00:57:43,760 --> 00:57:45,520
люди называют этот домен адаптивным или адаптивным к задаче

1659
00:57:45,520 --> 00:57:47,280
предварительное обучение,

1660
00:57:47,280 --> 00:57:48,960
и это хороший способ

1661
00:57:48,960 --> 00:57:50,400
повысить производительность в ваших последующих

1662
00:57:50,400 --> 00:57:52,559
задачах,

1663
00:57:52,559 --> 00:57:55,440
поэтому у меня здесь есть хороший набор вопросов.

1664
00:57:55,440 --> 00:57:58,079
если сейчас хорошее время,

1665
00:57:58,079 --> 00:57:59,680
да, это было бы здорово,

1666
00:57:59,680 --> 00:58:02,000
так что для некоторого контекста ученики в

1667
00:58:02,000 --> 00:58:03,760
их последнем задании

1668
00:58:03,760 --> 00:58:07,280
должны были эффективно создать мини-t5,

1669
00:58:07,280 --> 00:58:08,720
были единственными вопросами, которые были

1670
00:58:08,720 --> 00:58:10,319
заданы из простой предметной области, чтобы они

1671
00:58:10,319 --> 00:58:12,960
могли предварительно потренироваться  на одном графическом

1672
00:58:12,960 --> 00:58:15,200
процессоре, и один из вопросов, который у них есть, - а как

1673
00:58:15,200 --> 00:58:17,440
мы можем быть уверены, что ответ, полученный

1674
00:58:17,440 --> 00:58:19,200
от модели, не придуман,

1675
00:58:19,200 --> 00:58:20,720
им также задавали это в задании,

1676
00:58:20,720 --> 00:58:23,040
я думаю,

1677
00:58:25,440 --> 00:58:27,760
что вы, если у вас нет  доступ к

1678
00:58:27,760 --> 00:58:29,280
На самом деле очень

1679
00:58:29,280 --> 00:58:31,599
сложно понять,

1680
00:58:31,599 --> 00:58:33,680
что после этой статьи вышла хорошая статья, которая называлась:

1681
00:58:33,680 --> 00:58:35,680
как мы можем узнать, знают ли языковые модели,

1682
00:58:35,680 --> 00:58:38,000
и какова цель этой статьи -

1683
00:58:38,000 --> 00:58:40,160
сделать так, чтобы t5 был

1684
00:58:40,160 --> 00:58:42,400
тем, что мы называем хорошо откалиброванным и  когда

1685
00:58:42,400 --> 00:58:43,920
модель хорошо откалибрована, это означает, что,

1686
00:58:43,920 --> 00:58:45,920
когда она не знает ответа, она

1687
00:58:45,920 --> 00:58:47,440
не дает высоконадежных

1688
00:58:47,440 --> 00:58:49,920
прогнозов, и в этой статье были изучены

1689
00:58:49,920 --> 00:58:52,799
различные способы калибровки t5 для близких,

1690
00:58:52,799 --> 00:58:54,160
но ответных вопросов, и они в

1691
00:58:54,160 --> 00:58:55,599
конечном итоге обнаружили, что, когда модель

1692
00:58:55,599 --> 00:58:56,640
не  знаете ответ, когда он

1693
00:58:56,640 --> 00:58:59,520
выводит вы знаете что-то выдуманное,

1694
00:58:59,520 --> 00:59:01,520
что они могли бы эффективно сделать

1695
00:59:01,520 --> 00:59:02,640
его очень

1696
00:59:02,640 --> 00:59:06,400
неуверенным в своих прогнозах,

1697
00:59:06,640 --> 00:59:09,599
так что это один из способов сделать это,

1698
00:59:11,200 --> 00:59:12,960
я думаю, вы на самом деле приглушены

1699
00:59:12,960 --> 00:59:14,160
Джон

1700
00:59:14,160 --> 00:59:15,760
большое

1701
00:59:15,760 --> 00:59:17,200
спасибо, а затем еще один вопрос -

1702
00:59:17,200 --> 00:59:18,880
знание  это необходимо для выполнения

1703
00:59:18,880 --> 00:59:21,680
этой тонкой настройки, например, qa на

1704
00:59:21,680 --> 00:59:23,520
этих наборах данных для тонкой настройки,

1705
00:59:24,400 --> 00:59:26,480
это все знания, которые

1706
00:59:26,480 --> 00:59:28,880
присутствуют во время предварительной тренировки,

1707
00:59:28,880 --> 00:59:30,559
да, это тоже кое-что  ng,

1708
00:59:30,559 --> 00:59:32,960
который был исследован в следующей статье,

1709
00:59:32,960 --> 00:59:34,160
где было показано, что на самом деле

1710
00:59:34,160 --> 00:59:35,760
существует приличное количество перекрытий обучения и тестирования

1711
00:59:35,760 --> 00:59:37,599
с точки зрения знаний в этих

1712
00:59:37,599 --> 00:59:40,079
наборах данных, поэтому в

1713
00:59:40,079 --> 00:59:42,480
этих случаях определенно возможно, что модель

1714
00:59:42,480 --> 00:59:44,400
собирает знания во время прекрасного  настройка,

1715
00:59:44,400 --> 00:59:48,240
а не предварительная подготовка, однако,

1716
00:59:48,559 --> 00:59:51,760
в качестве побочного экспериментального примечания,

1717
00:59:51,760 --> 00:59:53,440
мы обнаруживаем,

1718
00:59:53,440 --> 00:59:57,280
что производительность t5 фактически находится на плато,

1719
00:59:57,280 --> 00:59:59,440
прежде чем он сделает один проход по

1720
00:59:59,440 --> 01:00:01,680
набору данных точной настройки, поэтому в основном

1721
01:00:01,680 --> 01:00:03,359
t5 очень очень быстро поймет,

1722
01:00:03,359 --> 01:00:04,799
что вы, черт возьми,  пытаемся заставить его это сделать,

1723
01:00:04,799 --> 01:00:06,160
ему даже не нужно видеть полный

1724
01:00:06,160 --> 01:00:08,480
обучающий набор, прежде чем он получит в основном

1725
01:00:08,480 --> 01:00:10,400
свою максимальную производительность на тестовом наборе,

1726
01:00:10,400 --> 01:00:12,400
поэтому мы на самом деле не думаем,

1727
01:00:12,400 --> 01:00:14,160
что это основной фактор для этих

1728
01:00:14,160 --> 01:00:15,200
результатов

1729
01:00:15,200 --> 01:00:17,359
отлично, а затем последний вопрос, как это получилось,

1730
01:00:17,359 --> 01:00:19,200
если вы изучили его, как многозадачная

1731
01:00:19,200 --> 01:00:20,960
модель работает ...

1732
01:00:22,079 --> 01:00:23,520
да, эти результаты в статье,

1733
01:00:23,520 --> 01:00:25,280
результаты почти точно такие же,

1734
01:00:25,280 --> 01:00:26,960
просто немного легче объяснить

1735
01:00:29,440 --> 01:00:32,960
Когда вы говорите о t5.1.1

1736
01:00:32,960 --> 01:00:35,200
в интересах экономии времени, я могу

1737
01:00:35,200 --> 01:00:37,599
пропустить эти следующие три слайда, которые

1738
01:00:37,599 --> 01:00:40,079
являются кратким изложением этих слайдов.

1739
01:00:40,079 --> 01:00:42,160
Просто процедура оценки, которую

1740
01:00:42,160 --> 01:00:44,559
мы используем, несправедливо наказывается как закрытая

1741
01:00:44,559 --> 01:00:46,400
книжные системы ответов на вопросы, если вы

1742
01:00:46,400 --> 01:00:47,680
хотите узнать немного больше о том, что вы

1743
01:00:47,680 --> 01:00:49,359
можете немного ткнуть в бумагу, на

1744
01:00:49,359 --> 01:00:50,960
самом деле это не

1745
01:00:50,960 --> 01:00:52,640
поддерживает основной момент, который я пытаюсь

1746
01:00:54,400 --> 01:00:56,640
выразить каким-либо значимым образом, поэтому и я хочу

1747
01:00:56,640 --> 01:00:58,880
получить  к некоторым из более свежих статей,

1748
01:00:58,880 --> 01:01:00,480
а это, и у меня должно быть время,

1749
01:01:00,480 --> 01:01:02,640
чтобы сделать это,

1750
01:01:02,640 --> 01:01:04,799
круто, поэтому мы как бы ответили на этот

1751
01:01:04,799 --> 01:01:07,440
вопрос, вы знаете, как это,

1752
01:01:07,440 --> 01:01:09,359
сколько знаний модель приобретает во время

1753
01:01:09,359 --> 01:01:11,119
предварительного обучения, ответ, возможно, является

1754
01:01:11,920 --> 01:01:14,000
Так что напрашивается следующий вопрос:

1755
01:01:14,000 --> 01:01:15,760
запоминает ли модель эти

1756
01:01:15,760 --> 01:01:17,440
знания правильно, но запоминает ли она также,

1757
01:01:17,440 --> 01:01:19,280
запоминают ли также большие языковые модели

1758
01:01:19,280 --> 01:01:21,440
то, что мы не хотим, чтобы

1759
01:01:21,440 --> 01:01:23,599
они запоминали конкретно, например, личные

1760
01:01:23,599 --> 01:01:25,599
данные, как вы могли бы я?  Представим,

1761
01:01:25,599 --> 01:01:28,160
что где-то в c4 находится чей-то номер социального

1762
01:01:28,160 --> 01:01:29,520
страхования,

1763
01:01:29,520 --> 01:01:31,040
мы, вероятно, не хотим, чтобы наша модель

1764
01:01:31,040 --> 01:01:33,760
запоминала его и выдавала его, когда мы

1765
01:01:33,760 --> 01:01:35,520
декодируем его,

1766
01:01:35,520 --> 01:01:37,359
и мы, конечно, не хотим, чтобы это происходило

1767
01:01:37,359 --> 01:01:39,359
для безусловных моделей, таких как gbt2 или

1768
01:01:39,359 --> 01:01:40,880
gbt3,

1769
01:01:40,880 --> 01:01:43,440
поэтому в этой следующей работе мы попытаемся ответить на

1770
01:01:43,440 --> 01:01:45,040
этот вопрос, который, как вы знаете, запоминают ли большие языковые

1771
01:01:45,040 --> 01:01:46,720
модели данные из своего

1772
01:01:46,720 --> 01:01:48,720
набора данных до обучения,

1773
01:01:48,720 --> 01:01:50,000
и

1774
01:01:50,000 --> 01:01:52,079
мы можем сначала фактически обратиться к экспертам

1775
01:01:52,079 --> 01:01:54,960
и посмотреть, что думают эксперты, и вот

1776
01:01:54,960 --> 01:01:57,920
два утверждения, сделанные eff и  открытые

1777
01:01:57,920 --> 01:02:00,319
ИИ, которые были отправлены в ведомство по патентам и

1778
01:02:00,319 --> 01:02:01,920
товарным знакам США, когда был призыв

1779
01:02:01,920 --> 01:02:03,680
дать комментарии по существу именно по

1780
01:02:03,680 --> 01:02:04,799
этому вопросу,

1781
01:02:04,799 --> 01:02:06,559
и вы можете видеть, что в обоих случаях эти,

1782
01:02:06,559 --> 01:02:09,520
эти организации в основном говорят, что вы

1783
01:02:09,520 --> 01:02:10,319
знаете,

1784
01:02:10,319 --> 01:02:12,799
что в основном нет оснований полагать,

1785
01:02:12,799 --> 01:02:15,359
что большой  языковая модель

1786
01:02:15,359 --> 01:02:17,200
будет копировать данные из своего

1787
01:02:17,200 --> 01:02:20,000
набора обучающих данных.

1788
01:02:24,079 --> 01:02:25,200
прочтите их заявление

1789
01:02:25,200 --> 01:02:26,960
немного дольше, они вроде как говорят, что вы знаете,

1790
01:02:26,960 --> 01:02:28,319
если вы построите систему

1791
01:02:32,240 --> 01:02:34,240
искусственного интеллекта надлежащим образом, система искусственного интеллекта не будет чрезмерно соответствовать набору обучающих данных, и если она не будет чрезмерной, мы не ожидаем, что они на

1792
01:02:34,240 --> 01:02:36,319
самом деле выведут свой э-э-любой из

1793
01:02:36,319 --> 01:02:38,720
их тренировочный набор любым нетривиальным

1794
01:02:38,720 --> 01:02:40,960
образом,

1795
01:02:41,839 --> 01:02:42,880
так

1796
01:02:42,880 --> 01:02:44,960
что это своего рода утверждения, которые были

1797
01:02:44,960 --> 01:02:47,280
догадками, и в этой работе мы попытались

1798
01:02:47,280 --> 01:02:49,280
более тщательно исследовать,

1799
01:02:49,280 --> 01:02:50,720
были ли они правдой,

1800
01:02:50,720 --> 01:02:51,920
и способ, которым мы это делали, в

1801
01:02:51,920 --> 01:02:53,520
основном заключался в использовании предварительно обученного

1802
01:02:53,520 --> 01:02:56,640
gpt-2  модель и вводит ей префиксы, чтобы

1803
01:02:56,640 --> 01:02:59,520
вы могли представить, что вы берете

1804
01:02:59,520 --> 01:03:01,839
причинно-следственную языковую модель, такую как gpt2, которая

1805
01:03:01,839 --> 01:03:04,000
просто автоматически предсказывает токены,

1806
01:03:04,000 --> 01:03:05,680
вы вводите ей префикс, а затем просите его в

1807
01:03:05,680 --> 01:03:07,839
основном предсказать, что будет дальше,

1808
01:03:07,839 --> 01:03:10,000
и мы показываем здесь такие странные

1809
01:03:10,000 --> 01:03:12,559
префикс East Stroudsburg Stroudsburg, но

1810
01:03:12,559 --> 01:03:14,160
мы обнаружили, что когда мы вводили этот конкретный

1811
01:03:14,160 --> 01:03:16,960
префикс в gpg2, он фактически выводил

1812
01:03:16,960 --> 01:03:20,480
дословно имя адреса, адрес электронной почты,

1813
01:03:20,480 --> 01:03:22,000
номер телефона и номер факса реального

1814
01:03:22,000 --> 01:03:24,319
человека, который появляется в интерактивном  rnet

1815
01:03:24,319 --> 01:03:26,640
этот пример на самом деле появляется только шесть

1816
01:03:26,640 --> 01:03:29,119
раз во всем общедоступном Интернете, поэтому

1817
01:03:29,119 --> 01:03:32,640
маловероятно, что gbg2 видел этот адрес

1818
01:03:32,640 --> 01:03:35,280
очень много раз, и основной смысл

1819
01:03:35,280 --> 01:03:36,799
этой работы в том, что

1820
01:03:36,799 --> 01:03:40,000
да, похоже, что gpt2 по крайней мере

1821
01:03:40,000 --> 01:03:41,599
запомнил значительное количество

1822
01:03:41,599 --> 01:03:43,599
не  тривиальная информация из его

1823
01:03:43,599 --> 01:03:45,920
набора данных до обучения,

1824
01:03:45,920 --> 01:03:48,400
поэтому, как мы проводили это исследование,

1825
01:03:48,400 --> 01:03:50,480
мы использовали эту процедуру,

1826
01:03:50,480 --> 01:03:53,200
которая показана на экране здесь,

1827
01:03:53,200 --> 01:03:55,440
мы в основном рассматриваем три различных

1828
01:03:55,440 --> 01:03:58,319
способа выборки данных из gpg2,

1829
01:03:58,319 --> 01:03:59,680
первый - это просто

1830
01:03:59,680 --> 01:04:02,000
автоагрессивная выборка из  следующее -

1831
01:04:02,000 --> 01:04:03,680
это автоагрессивная выборка, но с

1832
01:04:03,680 --> 01:04:05,680
понижающейся температурой это в основном

1833
01:04:05,680 --> 01:04:08,319
означает, что вы хотите, чтобы модель становилась

1834
01:04:08,319 --> 01:04:10,079
все более и более уверенной в своих

1835
01:04:10,079 --> 01:04:13,280
прогнозах в процессе выборки,

1836
01:04:13,280 --> 01:04:15,200
и последний вариант - взять случайный

1837
01:04:15,200 --> 01:04:17,119
текст из Интернета и  используйте это как

1838
01:04:17,119 --> 01:04:19,520
условие для gpt2, прежде чем просить его

1839
01:04:19,520 --> 01:04:21,280
сгенерировать то, что будет дальше,

1840
01:04:21,280 --> 01:04:23,839
так что теперь для каждого из этих sam для каждого из

1841
01:04:23,839 --> 01:04:26,640
этих поколений каждое из этих 200000

1842
01:04:26,640 --> 01:04:28,160
поколений  Поскольку мы сделали для каждого из этих

1843
01:04:28,160 --> 01:04:30,720
методов выборки, нам нужен

1844
01:04:30,720 --> 01:04:32,640
способ попытаться предсказать, может ли он быть

1845
01:04:32,640 --> 01:04:34,319
запомнен или нет,

1846
01:04:34,319 --> 01:04:36,079
и поэтому мы придумали шесть показателей, чтобы

1847
01:04:36,079 --> 01:04:39,280
использовать их, чтобы дать нам представление о том, может ли

1848
01:04:39,280 --> 01:04:41,359
конкретный образец из gpt2 быть

1849
01:04:41,359 --> 01:04:42,480
запомнил, что

1850
01:04:42,480 --> 01:04:44,319
все эти различные метрики в

1851
01:04:44,319 --> 01:04:46,960
основном используют недоумение gpt2 для

1852
01:04:46,960 --> 01:04:50,000
образца, недоумение, как я думаю, вы,

1853
01:04:50,000 --> 01:04:51,280
вероятно, узнали,

1854
01:04:51,280 --> 01:04:54,400
в основном является мерой того, насколько уверенно

1855
01:04:54,400 --> 01:04:56,640
gpt2 генерировал этот конкретный

1856
01:04:56,640 --> 01:04:58,559
образец,

1857
01:04:58,559 --> 01:05:00,079
вы также можете думать об этом как о мере

1858
01:05:00,079 --> 01:05:02,079
сжатия

1859
01:05:02,079 --> 01:05:03,680
так что

1860
01:05:03,680 --> 01:05:05,280
все эти показатели используют

1861
01:05:05,280 --> 01:05:06,559
недоумение,

1862
01:05:06,559 --> 01:05:08,799
либо мы просто измеряем недоумение gbg2

1863
01:05:08,799 --> 01:05:11,839
для того, что он сгенерировал, либо мы

1864
01:05:11,839 --> 01:05:15,680
вычисляем отношение недоумения gpt2

1865
01:05:15,680 --> 01:05:17,680
к недоумению для другого варианта

1866
01:05:17,680 --> 01:05:20,000
gpt, либо с библиотекой сжатия текста

1867
01:05:20,000 --> 01:05:22,000
под названием zlib, которую

1868
01:05:22,000 --> 01:05:24,319
мы также сравнивали с помощью нашей  соотносить

1869
01:05:24,319 --> 01:05:26,319
недоумение исходного образца с

1870
01:05:26,319 --> 01:05:28,319
версией образца в нижнем регистре, а

1871
01:05:28,319 --> 01:05:30,400
также недоумение с оконным управлением, когда мы

1872
01:05:30,400 --> 01:05:32,079
вычисляем недоумение только для небольшого

1873
01:05:32,079 --> 01:05:33,680
Окно выборки вместо

1874
01:05:33,680 --> 01:05:35,520
всего, что

1875
01:05:35,520 --> 01:05:36,960
мы

1876
01:05:36,960 --> 01:05:38,880
берем наверху, мы

1877
01:05:38,880 --> 01:05:40,799
делаем некоторую дедупликацию для поколения,

1878
01:05:40,799 --> 01:05:42,720
а затем выбираем 100 лучших поколений в

1879
01:05:42,720 --> 01:05:43,839
соответствии с

1880
01:05:43,839 --> 01:05:45,200
каждой из этих метрик, что в

1881
01:05:45,200 --> 01:05:47,119
конечном итоге даст нам 600 возможных

1882
01:05:47,119 --> 01:05:49,039
запомненных поколений, а затем мы

1883
01:05:49,039 --> 01:05:51,039
фактически  просто сделайте простое извинение,

1884
01:05:51,039 --> 01:05:53,280
простейший поиск в Google, чтобы увидеть, сможем ли мы

1885
01:05:53,280 --> 01:05:56,720
найти этот текст, который gpt2 сгенерировал

1886
01:05:56,720 --> 01:05:58,960
где-то в Интернете, и если он, если мы

1887
01:05:58,960 --> 01:06:00,480
действительно найдем его где-нибудь в Интернете, мы

1888
01:06:00,480 --> 01:06:03,039
спросим авторов gpg2, было ли это в

1889
01:06:03,039 --> 01:06:05,200
обучающем наборе или нет  и они проверили

1890
01:06:05,200 --> 01:06:06,640
все примеры, которые мы нашли, и

1891
01:06:06,640 --> 01:06:09,280
сообщили нам, действительно ли gbt2 выплевывает

1892
01:06:09,280 --> 01:06:12,240
что-то из своего набора обучающих данных,

1893
01:06:12,240 --> 01:06:13,920
чтобы просто дать вам представление о том, что

1894
01:06:13,920 --> 01:06:15,680
это за метрики и почему они могут

1895
01:06:15,680 --> 01:06:17,039
быть полезны,

1896
01:06:17,039 --> 01:06:19,280
этот график рассеивания показывает

1897
01:06:19,280 --> 01:06:22,079
недоумение, назначенное gpt2, и

1898
01:06:22,079 --> 01:06:25,119
недоумение, назначенное zlib для

1899
01:06:25,119 --> 01:06:28,480
200000 выборок, сгенерированных gpt2, и

1900
01:06:28,480 --> 01:06:30,079
вы можете видеть, что большинство из них как бы

1901
01:06:30,079 --> 01:06:32,400
попадают в эту строку, их большая группа

1902
01:06:33,599 --> 01:06:35,520
на r  Здесь серым цветом,

1903
01:06:35,520 --> 01:06:36,400
но

1904
01:06:36,400 --> 01:06:39,039
выделены здесь красным и синим цветом, показаны

1905
01:06:39,039 --> 01:06:40,160
образцы,

1906
01:06:40,160 --> 01:06:43,039
которые, как вы знаете, являются выбросами, которые, как вы знаете,

1907
01:06:43,039 --> 01:06:46,079
gpg2 придает zlib гораздо меньшее затруднение,

1908
01:06:46,079 --> 01:06:49,039
извините за эти образцы, чем zlib,

1909
01:06:49,039 --> 01:06:51,200
и это означает, что gpg2

1910
01:06:51,200 --> 01:06:53,200
очень хорошо предсказывает, что  Далее идет

1911
01:06:53,200 --> 01:06:55,760
в этих примерах, а zlib - это не

1912
01:06:55,760 --> 01:06:58,559
zlib, это своего рода

1913
01:06:58,559 --> 01:07:00,880
непредвзятый источник, правда, он на самом деле не

1914
01:07:00,880 --> 01:07:02,880
предварительно обучен на кучу данных, это своего

1915
01:07:02,880 --> 01:07:05,440
рода агностик данных, поэтому может случиться так,

1916
01:07:05,440 --> 01:07:07,520
что если gpt2 очень хорош в предсказании

1917
01:07:07,520 --> 01:07:09,440
того, что будет дальше  в заданной последовательности, но

1918
01:07:09,440 --> 01:07:12,400
zlib не означает, что gpt-2 запомнил

1919
01:07:12,400 --> 01:07:14,240
эти образцы, поэтому все эти вещи

1920
01:07:14,240 --> 01:07:15,520
вроде как в верхнем левом углу есть

1921
01:07:15,520 --> 01:07:17,760
возможные запомненные образцы, и на самом деле

1922
01:07:17,760 --> 01:07:19,680
мы отметили синим цветом те, которые, как

1923
01:07:19,680 --> 01:07:21,680
оказалось, действительно были в обучающих

1924
01:07:21,680 --> 01:07:25,920
данных  set и что gpt2 запомнил в

1925
01:07:26,240 --> 01:07:29,680
целом, мы нашли много примеров

1926
01:07:29,680 --> 01:07:31,440
дословного текста, запомненного из

1927
01:07:31,440 --> 01:07:34,160
набора обучающих данных, который включал в себя файлы журнала новостей,

1928
01:07:34,160 --> 01:07:36,720
лицензии, вы знаете страницы из

1929
01:07:36,720 --> 01:07:41,599
URL-адресов википедии, и мы выделяем два

1930
01:07:41,599 --> 01:07:44,160
типа da  Здесь мы обнаружили, что

1931
01:07:44,160 --> 01:07:46,480
gpt2 запомнил, что,

1932
01:07:46,480 --> 01:07:48,079
по нашему мнению, представляет собой личную

1933
01:07:48,079 --> 01:07:50,400
информацию, такую как имена людей из

1934
01:07:50,400 --> 01:07:52,400
не новых образцов или контактную информацию,

1935
01:07:52,400 --> 01:07:56,319
как в примере, который я показал ранее,

1936
01:07:56,319 --> 01:07:58,240
и поэтому вы можете спросить, хорошо, вы знаете, может быть,

1937
01:07:58,240 --> 01:08:00,160
это не так уж удивительно, что gpt2

1938
01:08:00,160 --> 01:08:01,920
запомнил  новостная статья, если эта новостная

1939
01:08:01,920 --> 01:08:03,520
статья появляется в Интернете сотни раз,

1940
01:08:03,520 --> 01:08:05,920
нам на самом деле повезло,

1941
01:08:05,920 --> 01:08:08,079
потому что оказалось, что было

1942
01:08:08,079 --> 01:08:10,160
множество примеров запомненных данных, которые

1943
01:08:10,160 --> 01:08:13,039
появлялись только в одном документе во всем

1944
01:08:13,039 --> 01:08:16,158
общедоступном Интернете, это была в основном

1945
01:08:16,158 --> 01:08:18,799
паста, похожая на  вставьте кучу URL-адресов

1946
01:08:18,799 --> 01:08:20,799
из Reddit из спорного

1947
01:08:20,799 --> 01:08:22,640
субреддита, называемого дональдом,

1948
01:08:22,640 --> 01:08:24,719
и все эти URL-адреса имели точно такую

1949
01:08:24,719 --> 01:08:27,120
же форму, как вы знаете, http двоеточие

1950
01:08:27,120 --> 01:08:30,319
косая черта reddit.com

1951
01:08:35,359 --> 01:08:37,600
Теперь эта часть потока случайных чисел и букв

1952
01:08:37,600 --> 01:08:40,158
хороша, потому что

1953
01:08:40,158 --> 01:08:42,158
во всех случаях одинаково сложно предсказать, что это в основном

1954
01:08:42,158 --> 01:08:44,560
случайный хэш, поэтому мы знаем, что

1955
01:08:44,560 --> 01:08:46,238
Часть последовательности должна быть одинаково сложной

1956
01:08:46,238 --> 01:08:48,719
для запоминания любой модели,

1957
01:08:48,719 --> 01:08:50,880
и это означает, что мы можем

1958
01:08:50,880 --> 01:08:52,479
запомнить, мы можем измерить,

1959
01:08:52,479 --> 01:08:54,238
сколько раз

1960
01:08:54,238 --> 01:08:56,960
конкретный URL-адрес должен появиться в

1961
01:08:56,960 --> 01:08:58,719
этом списке URL-адресов, потому что

1962
01:08:58,719 --> 01:09:01,359
в списке были повторения по порядку  для

1963
01:09:01,359 --> 01:09:05,040
одной из конкретных моделей размера gb2,

1964
01:09:05,040 --> 01:09:07,120
чтобы запомнить его,

1965
01:09:07,120 --> 01:09:09,040
и мы обнаружили, что самый большой

1966
01:09:09,040 --> 01:09:12,158
вариант gpg2 gb2xl

1967
01:09:13,520 --> 01:09:17,439
запомнил URL-адрес, который появлялся 33 раза

1968
01:09:17,439 --> 01:09:19,439
в этом конкретном документе, но

1969
01:09:19,439 --> 01:09:22,880
не URL-адреса, которые появлялись в 17 раз или меньше,

1970
01:09:22,880 --> 01:09:25,279
модель среднего размера была  только действительно

1971
01:09:25,279 --> 01:09:27,679
может полностью запомнить URL-адрес, который

1972
01:09:27,679 --> 01:09:29,600
появлялся 56 раз, а маленькая модель

1973
01:09:29,600 --> 01:09:31,839
действительно не запоминала ничего. Половина в

1974
01:09:31,839 --> 01:09:33,198
основном означает, что мы могли бы заставить его

1975
01:09:33,198 --> 01:09:34,719
выплюнуть URL-адрес, если бы мы дали ему

1976
01:09:34,719 --> 01:09:36,880
дополнительные подсказки, мы в основном намекнули

1977
01:09:36,880 --> 01:09:39,439
на то, что некоторые  чисел было гм, и

1978
01:09:39,439 --> 01:09:41,120
вывод из этого состоит в том,

1979
01:09:41,120 --> 01:09:43,120
что мы на самом деле случайно, потому

1980
01:09:43,120 --> 01:09:44,640
что есть этот конкретный документ с

1981
01:09:44,640 --> 01:09:46,719
такой структурой, мы могли сказать

1982
01:09:46,719 --> 01:09:49,040
достаточно уверенно  То, что более крупные

1983
01:09:49,040 --> 01:09:51,120
модели, как правило, запоминают больше данных, если

1984
01:09:51,120 --> 01:09:53,198
им нужно увидеть пример, им

1985
01:09:53,198 --> 01:09:55,520
нужно видеть конкретные примеры реже,

1986
01:09:55,520 --> 01:09:57,920
чтобы запоминать их, что, как мы

1987
01:09:57,920 --> 01:10:00,800
думали, было интересным

1988
01:10:00,840 --> 01:10:03,120
открытием,

1989
01:10:03,120 --> 01:10:05,120
пока я как бы в

1990
01:10:05,120 --> 01:10:06,800
основном говорил о преимуществах

1991
01:10:06,800 --> 01:10:09,280
больших моделей правильно, потому что большие

1992
01:10:09,280 --> 01:10:11,040
модели лучше справились с суперклеем, большие

1993
01:10:11,040 --> 01:10:12,800
модели лучше справились с закрытой книгой,

1994
01:10:12,800 --> 01:10:14,640
отвечая на вопрос, конечно,

1995
01:10:14,640 --> 01:10:16,480
есть одно предостережение, что более крупные модели также, кажется,

1996
01:10:16,480 --> 01:10:17,920
лучше запоминают свой

1997
01:10:17,920 --> 01:10:20,640
обучающий набор данных, но большие

1998
01:10:20,640 --> 01:10:24,080
модели также неудобны.

1999
01:10:24,080 --> 01:10:25,679
они более затратны в вычислительном отношении,

2000
01:10:25,679 --> 01:10:27,840
они потребляют больше энергии и

2001
01:10:27,840 --> 01:10:30,320
не подходят, например, t511b не подходит для одного графического процессора,

2002
01:10:32,159 --> 01:10:34,159
если вы не используете

2003
01:10:34,159 --> 01:10:37,040
какие-то умные методы,

2004
01:10:37,920 --> 01:10:39,840
поэтому последняя статья, которую я буду обсуждать,

2005
01:10:39,840 --> 01:10:42,239
является супер недавней работой

2006
01:10:42,239 --> 01:10:44,000
можем ли мы в основном сократить разрыв в производительности

2007
01:10:44,000 --> 01:10:46,000
между большими и маленькими моделями за

2008
01:10:46,000 --> 01:10:47,600
счет улучшений в архитектуре трансформатора,

2009
01:10:47,600 --> 01:10:49,679
поэтому в этой работе мы в

2010
01:10:49,679 --> 01:10:51,280
основном берем  Та же стратегия, которую мы использовали

2011
01:10:51,280 --> 01:10:52,880
в документе t5,

2012
01:10:52,880 --> 01:10:55,040
где мы взяли своего рода ландшафт

2013
01:10:55,040 --> 01:10:56,400
существующих модификаций

2014
01:10:56,400 --> 01:10:58,640
архитектуры трансформатора и оценили

2015
01:10:58,640 --> 01:11:00,800
их в тех же точных настройках,

2016
01:11:00,800 --> 01:11:02,560
и было много

2017
01:11:02,560 --> 01:11:04,080
вариантов, предложенных для

2018
01:11:04,080 --> 01:11:06,400
архитектуры трансформатора в t5, мы используем в основном

2019
01:11:06,400 --> 01:11:08,400
стандартная архитектура декодера кодера

2020
01:11:08,400 --> 01:11:10,480
из внимания - это все, что вам нужно,

2021
01:11:10,480 --> 01:11:12,320
чтобы визуализировать здесь, но

2022
01:11:12,320 --> 01:11:14,239
было много и много модификаций,

2023
01:11:14,239 --> 01:11:15,440
которые были предложены с тех пор, как

2024
01:11:15,440 --> 01:11:18,239
преобразователь был выпущен в 2017 году.

2025
01:11:18,239 --> 01:11:19,520
например,

2026
01:11:19,520 --> 01:11:21,120
возможно, люди предложили

2027
01:11:21,120 --> 01:11:23,440
вам факторизовать свою матрицу встраивания.

2028
01:11:23,440 --> 01:11:25,120
должны совместно использовать матрицу внедрения и

2029
01:11:25,120 --> 01:11:27,440
выходной слой softmax, вы должны использовать

2030
01:11:27,440 --> 01:11:29,360
разные формы softmax, такие как

2031
01:11:29,360 --> 01:11:31,040
смесь softmax или адаптивный

2032
01:11:31,040 --> 01:11:32,320
softmax,

2033
01:11:32,320 --> 01:11:34,080
разные способы нормализации или

2034
01:11:34,080 --> 01:11:36,000
инициализации модели,

2035
01:11:36,000 --> 01:11:37,679
возможно, разные

2036
01:11:37,679 --> 01:11:40,080
механизмы внимания, альтернативы

2037
01:11:40,080 --> 01:11:41,520
механизмам внимания, такие как легкие

2038
01:11:41,520 --> 01:11:43,679
и динамические свертки,

2039
01:11:43,679 --> 01:11:45,440
разные нелинейности в  корм

2040
01:11:45,440 --> 01:11:47,040
для  Уорд накладывает разные структуры

2041
01:11:47,040 --> 01:11:48,480
на слои с прямой связью, такие как смесь

2042
01:11:48,480 --> 01:11:51,199
экспертов или переключающий трансформатор,

2043
01:11:51,199 --> 01:11:52,719
совершенно разные архитектуры, которые

2044
01:11:52,719 --> 01:11:54,480
были вдохновлены трансформатором, например,

2045
01:11:54,480 --> 01:11:55,760
воронкообразный трансформатор, развитый

2046
01:11:55,760 --> 01:11:57,840
трансформатор, универсальный трансформатор

2047
01:11:57,840 --> 01:11:59,360
и так далее, на самом деле их было просто

2048
01:11:59,360 --> 01:12:01,760
тонны, тонны и тонны  и

2049
01:12:01,760 --> 01:12:03,440
снова цель в этой статье состояла в том, чтобы взять

2050
01:12:03,440 --> 01:12:05,679
кучу этих модификаций и применить

2051
01:12:05,679 --> 01:12:07,600
ту же базовую методологию из статьи t5,

2052
01:12:07,600 --> 01:12:09,280
где мы тестируем их в тех же

2053
01:12:09,280 --> 01:12:10,880
экспериментальных условиях, в

2054
01:12:10,880 --> 01:12:12,960
частности, мы в основном тестировали их

2055
01:12:12,960 --> 01:12:16,000
точно в настройке t5, которую я описал

2056
01:12:16,000 --> 01:12:17,360
в начале  лекции, в которой мы

2057
01:12:17,360 --> 01:12:20,320
предварительно обучили

2058
01:12:20,320 --> 01:12:23,040
модель размера на основе t5 на c4, а затем точно

2059
01:12:23,040 --> 01:12:26,159
настроили ее для нескольких последующих задач,

2060
01:12:26,159 --> 01:12:27,679
я не буду обсуждать это слишком подробно,

2061
01:12:27,679 --> 01:12:28,880
потому что я дал довольно подробное

2062
01:12:28,880 --> 01:12:30,159
введение в нее в

2063
01:12:30,159 --> 01:12:32,159
начале выступления

2064
01:12:32,159 --> 01:12:33,280
и  Итак,

2065
01:12:33,280 --> 01:12:36,080
вот что-то вроде первого набора результатов,

2066
01:12:36,080 --> 01:12:37,600
которые я покажу вам

2067
01:12:37,600 --> 01:12:39,040
по оси x, это разные

2068
01:12:39,040 --> 01:12:40,719
модификации трансформатора, я не

2069
01:12:40,719 --> 01:12:42,400
лаб  ling, какой из них, потому что я, как

2070
01:12:42,400 --> 01:12:44,000
вы знаете, не хотите вызывать какую-либо

2071
01:12:44,000 --> 01:12:46,640
конкретную модификацию,

2072
01:12:46,640 --> 01:12:49,520
это потеря валидации,

2073
01:12:49,520 --> 01:12:50,400
достигаемая моделью

2074
01:12:50,400 --> 01:12:52,239
для цели предварительного обучения, поэтому, когда

2075
01:12:52,239 --> 01:12:54,159
мы проводим предварительное обучение на c4, мы сохраняем

2076
01:12:54,159 --> 01:12:56,480
некоторые данные из c4  а затем мы в основном

2077
01:12:56,480 --> 01:12:59,120
измеряем потерю проверки на имеющихся

2078
01:12:59,120 --> 01:13:01,520
данных, так что чем меньше, тем лучше в этом

2079
01:13:01,520 --> 01:13:03,679
случае, и вы можете видеть, что эта пунктирная

2080
01:13:03,679 --> 01:13:05,199
черная линия - это производительность

2081
01:13:05,199 --> 01:13:07,520
базовой модели без каких-либо

2082
01:13:07,520 --> 01:13:09,199
модификаций трансформатора, вы знаете, что в основном

2083
01:13:09,199 --> 01:13:11,199
ванильный трансформатор, и вы  Можно видеть, что на

2084
01:13:11,199 --> 01:13:12,400
самом деле некоторые модификации трансформатора

2085
01:13:12,400 --> 01:13:14,320
достигли лучшей

2086
01:13:14,320 --> 01:13:16,480
производительности, что было хорошо,

2087
01:13:16,480 --> 01:13:18,239
но многие из них этого не сделали, и многие

2088
01:13:18,239 --> 01:13:19,920
из них действительно получили значительно худшую

2089
01:13:19,920 --> 01:13:21,280
производительность,

2090
01:13:21,280 --> 01:13:23,440
и, возможно, даже хуже, некоторые из

2091
01:13:23,440 --> 01:13:24,719
этих

2092
01:13:24,719 --> 01:13:26,159
вариантов  трансформатор, который

2093
01:13:26,159 --> 01:13:27,920
достиг лучшей производительности,

2094
01:13:27,920 --> 01:13:29,920
претерпел довольно незначительные изменения, такие как, например, просто

2095
01:13:29,920 --> 01:13:32,000
взятие relu в плотном действительно

2096
01:13:32,000 --> 01:13:34,239
плотном слое и замена его другим

2097
01:13:34,239 --> 01:13:36,320
нелинейным  y, так что это довольно незначительное

2098
01:13:37,520 --> 01:13:39,199
изменение, а некоторые из других

2099
01:13:39,199 --> 01:13:41,040
действительно высокопроизводительных моделей были в

2100
01:13:41,040 --> 01:13:43,120
конечном итоге более дорогими моделями, как вы

2101
01:13:43,120 --> 01:13:44,800
знаете, мы использовали ту же базовую модель, это

2102
01:13:44,800 --> 01:13:46,719
была та же модель размера на основе t5,

2103
01:13:48,000 --> 01:13:50,560
но некоторые из этих методов, например

2104
01:13:50,560 --> 01:13:53,040
Переключатель трансформатора, например,

2105
01:13:53,040 --> 01:13:55,280
резко увеличивает количество параметров, поэтому

2106
01:13:55,280 --> 01:13:56,880
он дороже с точки зрения

2107
01:13:56,880 --> 01:13:59,280
памяти, некоторые другие методы как

2108
01:13:59,280 --> 01:14:00,719
бы случайно совпадают,

2109
01:14:00,719 --> 01:14:02,320
возможно, если вы сделаете модель глубже, она

2110
01:14:02,320 --> 01:14:05,360
не сможет использовать ее,

2111
01:14:05,360 --> 01:14:08,080
использовать ускоритель как  эффективно и, таким образом,

2112
01:14:08,080 --> 01:14:09,360
это

2113
01:14:09,360 --> 01:14:11,199
делает время обучения и время вывода

2114
01:14:11,199 --> 01:14:12,719
немного дороже,

2115
01:14:12,719 --> 01:14:15,040
поэтому, если вы исключите очень простые

2116
01:14:15,040 --> 01:14:17,120
изменения в трансформаторе и те,

2117
01:14:17,120 --> 01:14:18,640
которые в конечном итоге сделали модель более

2118
01:14:18,640 --> 01:14:20,800
дорогой по какой-либо оси, на самом деле

2119
01:14:20,800 --> 01:14:23,440
было очень мало, если вообще какие-либо модификации  это значительно

2120
01:14:23,440 --> 01:14:25,600
улучшило производительность,

2121
01:14:26,840 --> 01:14:29,040
и это верно для

2122
01:14:29,040 --> 01:14:31,679
задачи перед обучением, это также верно и для

2123
01:14:31,679 --> 01:14:33,679
последующих задач, которые мы рассмотрели, так что

2124
01:14:33,679 --> 01:14:35,520
это  Rouge two score - это просто

2125
01:14:35,520 --> 01:14:37,440
одна из метрик, которые люди используют

2126
01:14:37,440 --> 01:14:40,080
в задаче x-sum xm. Вы можете

2127
01:14:40,080 --> 01:14:41,760
думать об этом как о более сложной версии

2128
01:14:41,760 --> 01:14:44,080
ежедневной почты cnn через задачу суммирования,

2129
01:14:44,080 --> 01:14:46,400
и вы можете видеть, что варианты модели

2130
01:14:46,400 --> 01:14:49,520
которые достигли лучшего результата валидации,

2131
01:14:49,520 --> 01:14:52,320
как правило, также получали лучший

2132
01:14:52,320 --> 01:14:55,520
результат x-sum rouge 2, но опять же, почти

2133
01:14:55,520 --> 01:14:57,360
все варианты, которые мы пробовали, снижали

2134
01:14:57,360 --> 01:14:59,920
производительность,

2135
01:14:59,920 --> 01:15:02,320
и, кроме того, я как бы немного упомянул

2136
01:15:02,320 --> 01:15:04,000
об этом, существует

2137
01:15:04,000 --> 01:15:05,679
достаточно хорошая корреляция  между

2138
01:15:05,679 --> 01:15:08,320
потерей валидации и оценкой суперклея,

2139
01:15:08,320 --> 01:15:10,560
гм, хотя я просто отмечу

2140
01:15:10,560 --> 01:15:12,800
пару интересных моментов, один из них - это

2141
01:15:12,800 --> 01:15:15,199
метод, называемый прозрачным вниманием, он

2142
01:15:15,199 --> 01:15:17,040
восстановил довольно хорошую потерю валидации,

2143
01:15:17,040 --> 01:15:19,199
но в конечном итоге очень плохую

2144
01:15:19,199 --> 01:15:21,679
оценку суперклея, что было удивительно для  нас гм,

2145
01:15:21,679 --> 01:15:22,960
коммутирующий трансформатор, который я

2146
01:15:22,960 --> 01:15:25,040
выделю здесь, достиг лучших

2147
01:15:25,040 --> 01:15:26,960
потерь при проверке, но он не получил

2148
01:15:26,960 --> 01:15:29,600
наилучшего результата суперклея в закрытой

2149
01:15:29,600 --> 01:15:31,760
книге вариантов веб-

2150
01:15:31,760 --> 01:15:34,239
вопросов переключатель tran  sformer на самом деле добился

2151
01:15:34,239 --> 01:15:37,360
просто наилучшей проверки и точности,

2152
01:15:37,360 --> 01:15:40,000
и мы, такого рода, поддерживаем слабую

2153
01:15:40,000 --> 01:15:43,040
гипотезу в этой области, что

2154
01:15:43,040 --> 01:15:44,960
увеличение числа параметров

2155
01:15:44,960 --> 01:15:46,400
улучшает объем знаний,

2156
01:15:46,400 --> 01:15:48,719
которые модель может усвоить, но это не

2157
01:15:48,719 --> 01:15:51,600
помогает модельной причине, так что

2158
01:15:51,600 --> 01:15:53,520
очень, очень грубо говоря, опять же,

2159
01:15:53,520 --> 01:15:55,760
это своего рода предположение, что суперклей

2160
01:15:55,760 --> 01:15:58,719
требует эм-глубоких возможностей рассуждения.

2161
01:15:58,719 --> 01:16:01,280
Веб-закрытая книга. Веб-вопросы

2162
01:16:01,280 --> 01:16:03,360
требуют интеллектуальных возможностей, и поэтому

2163
01:16:03,360 --> 01:16:05,440
переключающий трансформатор, который только увеличивает

2164
01:16:05,440 --> 01:16:07,520
количество параметров без

2165
01:16:07,520 --> 01:16:08,960
масштабирования обработки,

2166
01:16:08,960 --> 01:16:10,800
может быть, лучше справляется с этим на  Задача веб-

2167
01:16:10,800 --> 01:16:13,199
вопросов,

2168
01:16:13,199 --> 01:16:14,000
поэтому

2169
01:16:14,000 --> 01:16:15,600
такая цифра должна

2170
01:16:15,600 --> 01:16:17,679
вызывать у вас красные флажки,

2171
01:16:17,679 --> 01:16:19,280
потому что это довольно смелое заявление о том, что

2172
01:16:19,280 --> 01:16:20,480
большинство из этих вещей на

2173
01:16:20,480 --> 01:16:21,920
самом деле не так сильно помогают,

2174
01:16:21,920 --> 01:16:24,480
и есть несколько возможных

2175
01:16:24,480 --> 01:16:26,400
причин, по которым  это может быть случай,

2176
01:16:26,400 --> 01:16:27,840
когда наша кодовая база просто очень

2177
01:16:27,840 --> 01:16:29,920
необычна и нестандартна,

2178
01:16:29,920 --> 01:16:31,840
мы не думаем, что это так, потому

2179
01:16:31,840 --> 01:16:33,600
что  Кодовая база, которую мы использовали, на самом деле была

2180
01:16:33,600 --> 01:16:36,239
разработана одним из людей, которые

2181
01:16:36,239 --> 01:16:38,719
изобрели трансформатор gnome shazier,

2182
01:16:38,719 --> 01:16:41,360
и он использовался много, он

2183
01:16:41,360 --> 01:16:44,320
использовался во многих различных документах, это в

2184
01:16:44,320 --> 01:16:45,760
основном то же самое, что и тензор для

2185
01:16:45,760 --> 01:16:48,400
тензорной базы кода, и поэтому мы думаем  что,

2186
01:16:48,400 --> 01:16:49,840
возможно, наша кодовая база и

2187
01:16:49,840 --> 01:16:51,120
детали реализации должны быть

2188
01:16:51,120 --> 01:16:52,640
достаточно стандартными.

2189
01:17:02,239 --> 01:17:04,880
у них есть

2190
01:17:04,880 --> 01:17:06,800
современные результаты от трансформаторов, и на

2191
01:17:06,800 --> 01:17:08,880
самом деле мы включили отдельно

2192
01:17:08,880 --> 01:17:11,760
контролируемое обучение только на английском

2193
01:17:11,760 --> 01:17:13,760
немецком языке, что было задачей, для которой

2194
01:17:13,760 --> 01:17:15,840
был фактически предложен трансформатор,

2195
01:17:17,679 --> 01:17:19,199
возможно, нам нужна дополнительная настройка гиперпараметров,

2196
01:17:20,239 --> 01:17:21,679
потому что мы снова этого не сделали  провести

2197
01:17:21,679 --> 01:17:23,040
значительную настройку гиперпараметров для

2198
01:17:23,040 --> 01:17:25,040
каждого из этих методов, чтобы проверить, насколько это верно,

2199
01:17:25,040 --> 01:17:26,320
мы на самом деле использовали один из

2200
01:17:26,320 --> 01:17:27,520
методов,

2201
01:17:27,520 --> 01:17:29,600
которые мы значительно выполнили  Намного хуже,

2202
01:17:29,600 --> 01:17:32,000
чем мы ожидали, и мы провели, может быть,

2203
01:17:32,000 --> 01:17:33,679
пару сотен попыток оптимизации гиперпараметров.

2204
01:17:33,679 --> 01:17:35,520
Один из исследователей

2205
01:17:35,520 --> 01:17:36,960
в этой статье потратил много времени,

2206
01:17:36,960 --> 01:17:38,400
пытаясь получить правильные гиперпараметры, чтобы заставить его

2207
01:17:38,400 --> 01:17:41,120
работать, и в конечном итоге он никогда не работал

2208
01:17:41,120 --> 01:17:44,320
так же хорошо, как базовый уровень  метод следующая

2209
01:17:44,320 --> 01:17:45,679
возможность заключается в том, что мы

2210
01:17:45,679 --> 01:17:47,280
неправильно реализовали эти модификации для

2211
01:17:47,280 --> 01:17:49,440
проверки работоспособности, мы фактически отправили электронное

2212
01:17:49,440 --> 01:17:50,719
письмо авторам всех различных

2213
01:17:50,719 --> 01:17:52,239
модификаций и попросили их проверить

2214
01:17:52,239 --> 01:17:54,320
нашу реализацию, все те, которые

2215
01:17:54,320 --> 01:17:55,600
вернулись к нам, сказали, что это выглядело

2216
01:17:55,600 --> 01:17:56,880
правильным для  их,

2217
01:17:56,880 --> 01:17:58,400
а затем, наконец, последний вариант состоит в том, что,

2218
01:17:58,400 --> 01:17:59,760
возможно, эти модификации

2219
01:17:59,760 --> 01:18:01,679
преобразователя на самом деле не являются своего рода

2220
01:18:01,679 --> 01:18:03,440
передачей, они не передаются между

2221
01:18:03,440 --> 01:18:05,600
базами кода, реализациями и

2222
01:18:05,600 --> 01:18:08,480
приложениями, и нам, по крайней мере, на

2223
01:18:08,480 --> 01:18:10,159
основании доказательств того, что у нас есть это

2224
01:18:10,159 --> 01:18:12,880
правдоподобная возможность, на мой взгляд,

2225
01:18:12,880 --> 01:18:15,040
лучший способ контролировать это - если

2226
01:18:15,040 --> 01:18:16,719
вы предлагаете новую

2227
01:18:16,719 --> 01:18:19,120
модификацию трансформатора, попробуйте применить ее к

2228
01:18:19,120 --> 01:18:21,600
много кодовых баз и задач, как вы можете

2229
01:18:21,600 --> 01:18:23,840
без настройки гиперпараметров, и если

2230
01:18:23,840 --> 01:18:25,679
он работает во всех этих настройках, тогда

2231
01:18:25,679 --> 01:18:27,360
вы золотой, и ваша вещь, вероятно,

2232
01:18:27,360 --> 01:18:29,280
будет перенесена, и мы думаем, что это,

2233
01:18:29,280 --> 01:18:30,560
вероятно, тот случай, когда более простые

2234
01:18:30,560 --> 01:18:32,159
модификации, такие как изменение

2235
01:18:32,159 --> 01:18:33,920
нелинейности,

2236
01:18:33,920 --> 01:18:36,159
являются  не так сильно зависят от гиперпараметров

2237
01:18:36,159 --> 01:18:38,480
и деталей реализации, поэтому

2238
01:18:38,480 --> 01:18:39,600
они могут быть теми, которые с большей

2239
01:18:39,600 --> 01:18:43,440
вероятностью будут перенесены, так сказать,

2240
01:18:43,440 --> 01:18:45,040
так что это все, что я буду обсуждать в этом выступлении,

2241
01:18:45,040 --> 01:18:46,880
я понимаю, что это был своего рода вихревой

2242
01:18:46,880 --> 01:18:48,640
тур, поэтому я связал  все документы,

2243
01:18:48,640 --> 01:18:51,440
которые я обсуждал на этом слайде,

2244
01:18:51,440 --> 01:18:54,000
конечно, это была работа, проделанная огромной

2245
01:18:54,000 --> 01:18:56,159
и поистине удивительной группой сотрудников

2246
01:18:56,159 --> 01:18:58,159
над этими пятью документами,

2247
01:18:58,159 --> 01:19:00,239
которые я перечислил на экране здесь, и

2248
01:19:00,239 --> 01:19:02,560
да, я счастлив  чтобы ответить на любые

2249
01:19:02,560 --> 01:19:04,480
дополнительные вопросы, которые у вас все в

2250
01:19:04,480 --> 01:19:06,800
порядке, так что большое спасибо, Колин, за эту

2251
01:19:06,800 --> 01:19:08,719
прекрасную беседу и было немного

2252
01:19:08,719 --> 01:19:12,159
информации, эм, я понял, также

2253
01:19:12,159 --> 01:19:13,760
есть одна вещь, которую я забыл сказать

2254
01:19:13,760 --> 01:19:15,280
во введении  так что, я думаю, мне нужно

2255
01:19:15,280 --> 01:19:18,480
иметь и после этого, а именно,

2256
01:19:18,480 --> 01:19:21,760
что ммм Колин теперь начал работать

2257
01:19:21,760 --> 01:19:23,760
профессором в университете Северной

2258
01:19:23,760 --> 01:19:25,760
Каролины, так что

2259
01:19:25,760 --> 01:19:27,760
университет Северной Каролины играет большую роль

2260
01:19:27,760 --> 01:19:30,080
в этом курсе, потому что он также был

2261
01:19:30,080 --> 01:19:32,560
источником  данных чероки, которые мы используем

2262
01:19:32,560 --> 01:19:34,560
для задания 4 для перевода чероки на

2263
01:19:34,560 --> 01:19:38,320
английский язык, так что давай, смоляные каблуки,

2264
01:19:38,320 --> 01:19:41,040
но мм,

2265
01:19:41,040 --> 01:19:44,239
да, так что, да, Колин рад остаться

2266
01:19:44,239 --> 01:19:46,800
и ответить на некоторые вопросы, поэтому, если вы

2267
01:19:46,800 --> 01:19:49,520
хотите задать больше вопросов, используйте

2268
01:19:49,520 --> 01:19:51,679
луч  руки, и мы затем как бы пригласим

2269
01:19:51,679 --> 01:19:54,800
вас в эм, где люди могут видеть

2270
01:19:54,800 --> 01:19:56,560
друг друга, эм,

2271
01:19:56,560 --> 01:19:57,760
комната масштабирования,

2272
01:19:57,760 --> 01:19:59,440
и вы знаете, если вы готовы к этому, было бы

2273
01:19:59,440 --> 01:20:02,320
даже неплохо включить ваше видео, чтобы

2274
01:20:02,320 --> 01:20:04,480
люди могли видеть, кто они  разговариваю с

2275
01:20:04,480 --> 01:20:06,080
ммм, и

2276
01:20:06,080 --> 01:20:08,239
да,

2277
01:20:08,239 --> 01:20:09,679
может быть, в первую очередь вам следует

2278
01:20:09,679 --> 01:20:12,880
прекратить показывать экран, и да, если

2279
01:20:12,880 --> 01:20:14,320
есть что-то, что вы хотите показать снова,

2280
01:20:14,320 --> 01:20:16,639
вы можете включить его снова

2281
01:20:16,639 --> 01:20:18,400
да, может быть, я просто скажу,

2282
01:20:18,400 --> 01:20:21,440
пока есть люди  гм

2283
01:20:21,440 --> 01:20:23,120
в том смысле, что я профессор  в

2284
01:20:23,120 --> 01:20:24,880
unc в случае, если

2285
01:20:24,880 --> 01:20:26,719
в аудитории есть магистры или студенты бакалавриата,

2286
01:20:26,719 --> 01:20:28,480
которые подают заявки на программы PhD,

2287
01:20:28,480 --> 01:20:30,560
крайний срок подачи заявок

2288
01:20:30,560 --> 01:20:33,280
на unc фактически еще не наступил,

2289
01:20:33,280 --> 01:20:35,920
поэтому, если вы, возможно, хотите подать заявление в

2290
01:20:35,920 --> 01:20:37,199
другую школу, у вас есть другой

2291
01:20:37,199 --> 01:20:38,480
вариант  в восторге от работы, которую я

2292
01:20:38,480 --> 01:20:40,960
представил, вы все еще можете подать заявку в unc, у

2293
01:20:40,960 --> 01:20:42,719
нас очень поздний

2294
01:20:42,719 --> 01:20:45,120
срок подачи заявок, так что просто вставьте вилку на случай, если

2295
01:20:45,120 --> 01:20:49,040
есть кто-то, кто ищет докторскую степень,

2296
01:20:49,040 --> 01:20:51,520
и unc является старейшим государственным университетом

2297
01:20:51,520 --> 01:20:52,960
в стране, и

2298
01:20:52,960 --> 01:20:56,000
позвольте мне  полная реклама usc, и

2299
01:20:56,000 --> 01:20:58,080
я думаю, у нас есть второй старейший отдел CS,

2300
01:20:58,080 --> 01:21:00,000
который, да, он

2301
01:21:00,000 --> 01:21:01,199
существует уже давно, он довольно

2302
01:21:01,199 --> 01:21:03,840
маленький, это всего лишь около 50 преподавателей,

2303
01:21:06,159 --> 01:21:08,239
так что пока мы ждем, пока кто-то

2304
01:21:08,239 --> 01:21:09,360
присоединится,

2305
01:21:09,360 --> 01:21:11,199
мы делаем  у меня уже есть один вопрос на самом деле

2306
01:21:11,199 --> 01:21:14,199
от

2307
01:21:14,320 --> 01:21:16,719
да привет спасибо за лекцию

2308
01:21:16,719 --> 01:21:18,080
эм, о которой

2309
01:21:18,080 --> 01:21:20,239
я не спрашивал раньше, эм,

2310
01:21:20,239 --> 01:21:22,320
когда вы обсуждаете, как

2311
01:21:22,320 --> 01:21:23,679
э-э,

2312
01:21:23,679 --> 01:21:26,960
задержка на t5 и вроде того, сколько

2313
01:21:26,960 --> 01:21:28,560
проходов

2314
01:21:28,560 --> 01:21:31,280
потребовалось для этого t  о переобучении, поэтому я уверен, что

2315
01:21:32,400 --> 01:21:34,639
если вы думаете, что некоторые из более крупных

2316
01:21:34,639 --> 01:21:36,560
моделей, такие как три миллиарда одиннадцать

2317
01:21:37,600 --> 01:21:39,679
миллиардов, хотели бы масштабироваться до того, как

2318
01:21:39,679 --> 01:21:40,880
игроки

2319
01:21:50,320 --> 01:21:52,400
перестанут быть

2320
01:21:54,239 --> 01:21:56,159
сравнивая потери

2321
01:21:56,159 --> 01:21:59,199
данных обучения с потерями данных проверки,

2322
01:21:59,199 --> 01:22:01,120
мы видим, что даже в этом случае в очень

2323
01:22:01,120 --> 01:22:03,440
очень больших моделях это примерно то же самое,

2324
01:22:03,440 --> 01:22:04,719
что предполагает своего рода в

2325
01:22:04,719 --> 01:22:06,159
традиционном смысле, что нет

2326
01:22:06,159 --> 01:22:08,239
переоснащения, одна из причин этого заключается в том, что

2327
01:22:08,239 --> 01:22:10,639
c4 на самом деле  Достаточно большой, чтобы мы выполняли

2328
01:22:10,639 --> 01:22:13,600
только один проход,

2329
01:22:13,600 --> 01:22:16,159
когда мы тренируемся на триллион токенов, и

2330
01:22:16,159 --> 01:22:17,520
вы знаете, что можете надеяться, что увидите

2331
01:22:17,520 --> 01:22:19,679
ограниченное переоснащение, когда вы видите

2332
01:22:19,679 --> 01:22:21,840
каждую часть данных только один раз в

2333
01:22:21,840 --> 01:22:23,440
течение курса обучения, так что это вроде

2334
01:22:23,440 --> 01:22:24,880
как и каждый раз, когда вы видите данные, это

2335
01:22:24,880 --> 01:22:26,320
новые данные, поэтому нет большой

2336
01:22:26,320 --> 01:22:27,520
разницы между данными в

2337
01:22:27,520 --> 01:22:29,440
обучающем наборе в проверочном наборе,

2338
01:22:29,440 --> 01:22:31,280
конечно, есть также понятие

2339
01:22:31,280 --> 01:22:32,800
переобучения, которое похоже на то, что

2340
01:22:32,800 --> 01:22:35,280
Переобучение случая, которое

2341
01:22:35,280 --> 01:22:37,280
связано с работой по запоминанию, о которой я упоминал,

2342
01:22:37,280 --> 01:22:39,600
кажется, что

2343
01:22:39,600 --> 01:22:42,080
языковые модели могут запоминать данные,

2344
01:22:42,080 --> 01:22:44,080
даже если они делают относительно мало проходов

2345
01:22:44,080 --> 01:22:46,000
по набору обучающих данных, и вы не

2346
01:22:46,000 --> 01:22:48,239
видите переоснащения среднего случая при

2347
01:22:48,239 --> 01:22:49,760
сравнении обучения

2348
01:22:49,760 --> 01:22:53,800
потеря и потеря проверки

2349
01:23:00,800 --> 01:23:02,800
конечно, извините, я пытался включить звук в моем

2350
01:23:02,800 --> 01:23:04,719
видео, но я не могу сделать это по

2351
01:23:04,719 --> 01:23:06,960
какой-то причине, в

2352
01:23:06,960 --> 01:23:08,719
первую очередь, Колин, большое спасибо,

2353
01:23:08,719 --> 01:23:10,080
это фантастическая лекция, мне она очень

2354
01:23:10,080 --> 01:23:12,159
понравилась.

2355
01:23:12,960 --> 01:23:14,159
что

2356
01:23:14,159 --> 01:23:16,560
мне особенно понравилось, это работа,

2357
01:23:16,560 --> 01:23:18,159
которую вы, ребята, проделали

2358
01:23:18,159 --> 01:23:20,480
над вашей атакой по извлечению тренировочных данных,

2359
01:23:20,480 --> 01:23:23,679
пытаясь идентифицировать, ну, действительно,

2360
01:23:23,679 --> 01:23:26,639
проверьте эту догадку на открытом ИИ и эффекте,

2361
01:23:26,639 --> 01:23:27,679
что

2362
01:23:27,679 --> 01:23:29,679
эти модели на самом деле не запоминают

2363
01:23:29,679 --> 01:23:32,000
тренировочные данные.  интересно, у меня на

2364
01:23:32,000 --> 01:23:34,800
самом деле есть два вопроса, у одного есть

2365
01:23:34,800 --> 01:23:37,600
openai и eff с тех пор, как они изменили тон

2366
01:23:43,199 --> 01:23:45,840
что вы знаете, что хорошо

2367
01:23:45,840 --> 01:23:47,920
сконструированные модели могут действительно это делать,

2368
01:23:47,920 --> 01:23:49,120
и во-вторых,

2369
01:23:49,120 --> 01:23:50,719
будет ли этот подход действительно работать для

2370
01:23:50,719 --> 01:23:52,000
обнаружения

2371
01:23:52,000 --> 01:23:54,000
других, скажем,

2372
01:23:54,000 --> 01:23:57,840
случайно закодированных предубеждений в

2373
01:23:57,840 --> 01:23:59,440
сторону крайних предубеждений, которые преобладают

2374
01:23:59,440 --> 01:24:01,040
в некоторых языковых моделях, например, если бы вы

2375
01:24:01,040 --> 01:24:03,120
могли создавать пакеты,

2376
01:24:03,120 --> 01:24:04,800
которые могли бы имитировать такие виды

2377
01:24:04,800 --> 01:24:06,560
атак  на этих моделях, а затем

2378
01:24:06,560 --> 01:24:08,719
определить с некоторой степенью точности,

2379
01:24:08,719 --> 01:24:11,360
насколько эти предубеждения на самом деле

2380
01:24:11,360 --> 01:24:12,800
присутствуют,

2381
01:24:12,800 --> 01:24:13,920
да, поэтому что

2382
01:24:13,920 --> 01:24:15,840
касается первого вопроса, я

2383
01:24:15,840 --> 01:24:17,920
не знаю никаких официальных заявлений,

2384
01:24:17,920 --> 01:24:19,760
которые были сделаны кем-либо, но я

2385
01:24:19,760 --> 01:24:21,679
скажу, что на самом деле  документ по запоминанию,

2386
01:24:21,679 --> 01:24:23,920
у нас было несколько соавторов из

2387
01:24:23,920 --> 01:24:25,679
открытого ИИ, так что это было очень много

2388
01:24:25,679 --> 01:24:28,000
сотрудничества с ними, я имею в виду, что вы знаете,

2389
01:24:28,000 --> 01:24:30,320
что мы все ученые, и мы все, что вы

2390
01:24:30,320 --> 01:24:32,159
знаете, мы все как бы выдвигаем гипотезы, которые

2391
01:24:32,159 --> 01:24:33,520
иногда оказываются правильными и

2392
01:24:33,520 --> 01:24:36,480
неправильными  гм, и поэтому я думаю, что

2393
01:24:36,480 --> 01:24:38,560
открытые глаза определенно

2394
01:24:38,560 --> 01:24:41,120
осведомлены о том факте, что

2395
01:24:41,120 --> 01:24:44,639
да, возможно, эти модели

2396
01:24:44,639 --> 01:24:46,639
могут запоминать данные, даже когда  Они не

2397
01:24:48,159 --> 01:24:49,760
демонстрируют традиционные признаки

2398
01:24:49,760 --> 01:24:52,320
переобучения

2399
01:24:52,400 --> 01:24:54,000
по второму пункту.

2400
01:24:55,679 --> 01:24:56,880
То, как люди

2401
01:24:56,880 --> 01:24:59,280
измеряли это произвольно, -

2402
01:24:59,280 --> 01:25:01,360
это вводить в модель префикс о

2403
01:25:01,360 --> 01:25:03,920
конкретной демографической группе или

2404
01:25:03,920 --> 01:25:05,040
типе человека

2405
01:25:05,040 --> 01:25:07,199
и видеть  что модель говорит

2406
01:25:07,199 --> 01:25:08,560
об этом человеке,

2407
01:25:08,560 --> 01:25:11,600
и я думаю, я думаю, что в принципе вы

2408
01:25:11,600 --> 01:25:14,560
можете думать о нашем, нашем подходе,

2409
01:25:14,560 --> 01:25:16,719
как о связанном с этим, за исключением того, что у нас есть

2410
01:25:16,719 --> 01:25:18,400
этот дополнительный шаг, который

2411
01:25:18,400 --> 01:25:21,040
измеряет, генерирует ли модель

2412
01:25:21,040 --> 01:25:23,840
этот текст  потому что он видел

2413
01:25:23,840 --> 01:25:26,080
это в своих обучающих данных в основном

2414
01:25:26,080 --> 01:25:29,120
потому, что недоумение

2415
01:25:29,120 --> 01:25:31,920
для некоторого продолжения слишком мало по сравнению

2416
01:25:31,920 --> 01:25:33,360
с моделью, которая не была обучена на тех же

2417
01:25:33,360 --> 01:25:35,840
данных, ммм, так что это может быть интересно,

2418
01:25:35,840 --> 01:25:38,159
например, если вы введете в модель

2419
01:25:38,159 --> 01:25:41,600
префикс, который  вы просите его

2420
01:25:41,600 --> 01:25:43,199
заполнить некоторую оскорбительную информацию о какой-то

2421
01:25:43,199 --> 01:25:45,520
демографической группе, чтобы проверить, значительно ли

2422
01:25:45,520 --> 01:25:47,920
ниже сложность модели для ее

2423
01:25:47,920 --> 01:25:50,719
продолжения,

2424
01:25:50,719 --> 01:25:53,040
чем вы знаете, например, zlib и  в

2425
01:25:53,040 --> 01:25:54,880
этом случае вы можете подумать, что предвзятость на

2426
01:25:54,880 --> 01:25:57,360
самом деле, может быть, эта предвзятость,

2427
01:25:57,360 --> 01:25:59,280
которую подхватила модель, связана с тем, что она

2428
01:25:59,280 --> 01:26:01,679
увидела какое-то предложение, которое выглядит

2429
01:26:01,679 --> 01:26:03,760
так же, как это обучающие данные или

2430
01:26:05,840 --> 01:26:08,639
что  модель

2431
01:26:08,639 --> 01:26:11,760
усвоилась в процессе обучения

2432
01:26:11,760 --> 01:26:13,840
фантастически

2433
01:26:13,840 --> 01:26:15,600
большое спасибо за то, что поделились да спасибо

2434
01:26:15,600 --> 01:26:16,880
за вопросы

2435
01:26:16,880 --> 01:26:20,159
хорошо, так что следующая остановка, я думаю,

2436
01:26:20,800 --> 01:26:23,920
здорово эм спасибо за беседу, которая была

2437
01:26:23,920 --> 01:26:28,080
супер интересной, так что мой вопрос

2438
01:26:28,080 --> 01:26:31,280
вроде как у вас  мысли о том, чтобы

2439
01:26:31,280 --> 01:26:34,480
потенциально сделать несколько

2440
01:26:34,480 --> 01:26:37,199
раундов предварительного обучения,

2441
01:26:37,199 --> 01:26:40,000
чтобы сделать его более конкретным, эм, вы знаете,

2442
01:26:40,000 --> 01:26:42,320
например, допустим, у вас есть задача где-

2443
01:26:42,320 --> 01:26:45,280
то вроде генерации ответов,

2444
01:26:45,280 --> 01:26:48,320
и вы очень привязаны к

2445
01:26:48,320 --> 01:26:50,239
конкретному набору данных генерации ответов,

2446
01:26:50,239 --> 01:26:52,480
который вы используете, но потенциально  вы хотите как

2447
01:26:52,480 --> 01:26:55,679
бы приукрасить это, введя

2448
01:26:55,679 --> 01:26:57,840
некоторый общий набор диалоговых данных, который

2449
01:26:57,840 --> 01:27:00,960
состоит из натуралистических человеческих данных,

2450
01:27:00,960 --> 01:27:02,639
так что мне интересно, есть ли у вас какие-

2451
01:27:02,639 --> 01:27:04,960
нибудь  догадки или интуиция о том, насколько

2452
01:27:04,960 --> 01:27:07,840
это эффективно, может быть, начать с

2453
01:27:08,719 --> 01:27:10,880
того, что вы знаете общий Интернет, а затем

2454
01:27:10,880 --> 01:27:13,679
точно настроить этот

2455
01:27:13,679 --> 01:27:16,239
набор данных неструктурированного диалога, а затем

2456
01:27:16,239 --> 01:27:19,440
точно настроить, может быть, более

2457
01:27:19,440 --> 01:27:21,760
узкий набор данных генерации ответов

2458
01:27:22,960 --> 01:27:24,960
да, так  так что техника, которую вы

2459
01:27:24,960 --> 01:27:27,360
описываете, звучит

2460
01:27:27,360 --> 01:27:29,760
очень похоже на этот действительно отличный

2461
01:27:30,800 --> 01:27:32,560
подход, который люди теперь называют

2462
01:27:32,560 --> 01:27:34,400
адаптивным к предметной области предварительным обучением или адаптивным к задачам

2463
01:27:34,400 --> 01:27:36,320
предварительным обучением. Я был представлен в статье

2464
01:27:36,320 --> 01:27:39,199
под названием не прекращайте предварительное обучение, а

2465
01:27:39,199 --> 01:27:42,000
затем  есть менее запоминающийся подзаголовок

2466
01:27:42,000 --> 01:27:43,360
эм,

2467
01:27:43,360 --> 01:27:45,679
и идея очень похожа на то, что

2468
01:27:45,679 --> 01:27:47,120
вы предложили, в основном вы берете

2469
01:27:47,120 --> 01:27:48,560
предварительно обученную модель, которая была обучена

2470
01:27:48,560 --> 01:27:51,040
вы знаете общий текст, вы делаете

2471
01:27:51,040 --> 01:27:52,719
то, что вы могли бы назвать промежуточным

2472
01:27:52,719 --> 01:27:54,639
обучением заданию, или вы продолжаете

2473
01:27:54,639 --> 01:27:56,800
предварительное обучение  на данных, зависящих от предметной области, а

2474
01:27:56,800 --> 01:27:59,199
затем, наконец, вы выполняете тонкую настройку вашего

2475
01:27:59,199 --> 01:28:01,520
конкретного набора данных тонкой настройки, в их

2476
01:28:01,520 --> 01:28:03,600
случае они рассматривают такие вещи,

2477
01:28:03,600 --> 01:28:06,159
как вы знаете, например, создание научного текста

2478
01:28:06,159 --> 01:28:08,080
classifi  катионный или биомедицинский анализ текста,

2479
01:28:08,080 --> 01:28:09,760
и когда они выполняют

2480
01:28:09,760 --> 01:28:11,920
промежуточный этап предварительной подготовки на

2481
01:28:11,920 --> 01:28:14,960
данных в домене или даже просто выполняют

2482
01:28:14,960 --> 01:28:17,280
задачу предварительной подготовки на данных

2483
01:28:17,280 --> 01:28:19,199
из задачи, это определенно помогает

2484
01:28:19,199 --> 01:28:21,840
значительно э-э, э-э, и да, так

2485
01:28:21,840 --> 01:28:23,679
что это очень хорошо

2486
01:28:23,679 --> 01:28:26,239
интуиция и гм, и я думаю, что это

2487
01:28:26,239 --> 01:28:28,320
метод, наиболее похожий на то,

2488
01:28:28,320 --> 01:28:30,560
что вы описываете, эм, это как

2489
01:28:30,560 --> 01:28:32,719
бы поднимает ясный вопрос, который, я не

2490
01:28:32,719 --> 01:28:34,639
думаю, был адресован э-э, насколько мне

2491
01:28:34,639 --> 01:28:36,400
известно, в литературе, а именно вы

2492
01:28:36,400 --> 01:28:37,679
мы обычно думаем о переносном

2493
01:28:37,679 --> 01:28:40,320
обучении как о предварительном обучении, а затем о точной настройке,

2494
01:28:40,320 --> 01:28:41,760
и теперь мы делаем что-то вроде

2495
01:28:41,760 --> 01:28:43,840
предварительного обучения, а затем, возможно, еще какое-то

2496
01:28:43,840 --> 01:28:45,920
предварительное обучение, а затем тонкую настройку, и

2497
01:28:45,920 --> 01:28:47,360
есть другие методы, которые

2498
01:28:47,360 --> 01:28:50,560
другие шаги на этом пути гм, и

2499
01:28:50,560 --> 01:28:52,159
поэтому возникает естественный вопрос, например,

2500
01:28:52,159 --> 01:28:54,480
каким должен быть учебный план задач,

2501
01:28:54,480 --> 01:28:56,639
вы знаете,

2502
01:28:56,639 --> 01:28:58,639
сколько должно быть промежуточных шагов,

2503
01:28:58,639 --> 01:29:00,239
какими должны быть промежуточные

2504
01:29:00,239 --> 01:29:02,800
шаги, какова польза от одного

2505
01:29:02,800 --> 01:29:05,120
дома  в сравнении с другими, сколько там сдвига домена,

2506
01:29:05,120 --> 01:29:07,120
и каковы

2507
01:29:07,120 --> 01:29:08,880
соответствующие преимущества и так далее, и я

2508
01:29:08,880 --> 01:29:10,159
думаю, что будет

2509
01:29:10,159 --> 01:29:11,760
интересное направление работы, которое было бы

2510
01:29:12,800 --> 01:29:14,159
лучше, в основном, лучше отвечая на эти

2511
01:29:14,159 --> 01:29:16,560
вопросы,

2512
01:29:16,639 --> 01:29:18,480
и что было, что было

2513
01:29:18,480 --> 01:29:21,280
аббревиатура эм или

2514
01:29:21,280 --> 01:29:23,760
да, так это называется э-э-э-э-э-э-э-э-э-э-э-э-э-дэппинг или тэппинг

2515
01:29:23,760 --> 01:29:24,639
э-э,

2516
01:29:24,639 --> 01:29:26,960
адаптивная предварительная тренировка домена или адаптивная предварительная подготовка к задаче

2517
01:29:26,960 --> 01:29:28,719
о да, статья

2518
01:29:28,719 --> 01:29:30,880
называется не прекращайте предварительную тренировку,

2519
01:29:30,880 --> 01:29:33,120
которую легко запомнить, если вам

2520
01:29:33,120 --> 01:29:35,280
нравится песня, не надо  не перестаю верить, что

2521
01:29:35,280 --> 01:29:38,080
я, как я, я не знаю,

2522
01:29:38,080 --> 01:29:40,239
преднамеренно ли это ссылка на эту песню, я

2523
01:29:40,239 --> 01:29:42,239
полагаю, это должно быть я думаю, что они должны

2524
01:29:42,239 --> 01:29:44,239
были сделать не останавливайся, предварительно

2525
01:29:44,239 --> 01:29:45,760
обучившись апострофу, если они действительно  хотел

2526
01:29:45,760 --> 01:29:47,440
отвезти его домой, но знаете, может быть,

2527
01:29:47,440 --> 01:29:48,719
это было бы слишком глупо, но в любом случае

2528
01:29:48,719 --> 01:29:50,000
да, бумага называется, не прекращайте

2529
01:29:50,000 --> 01:29:51,840
предварительную тренировку,

2530
01:29:51,840 --> 01:29:54,320
это здорово, большое спасибо,

2531
01:29:54,320 --> 01:29:56,000
да, абсолютно

2532
01:29:56,000 --> 01:29:58,639
хорошо, следующий вопрос: эм,

2533
01:29:58,639 --> 01:30:00,000
я не совсем уверен, что  они знают,

2534
01:30:00,000 --> 01:30:03,040
соответствует  тогда

2535
01:30:03,040 --> 01:30:05,920
да ладно спасибо за этот тако,

2536
01:30:05,920 --> 01:30:07,120
действительно интересный

2537
01:30:07,120 --> 01:30:09,520
эм, у меня есть открытый вопрос,

2538
01:30:09,520 --> 01:30:11,440
я действительно ищу здесь несколько советов,

2539
01:30:11,440 --> 01:30:13,199
так что кажется, что недавние

2540
01:30:13,199 --> 01:30:15,040
успехи в

2541
01:30:15,040 --> 01:30:17,199
индустрии НЛП по захвату заголовков были достигнуты благодаря

2542
01:30:17,199 --> 01:30:20,000
построение этих массивных моделей, таких как gpg3

2543
01:30:20,000 --> 01:30:21,440
um с миллиардами параметров,

2544
01:30:21,440 --> 01:30:22,880
обучение которых, как вы часто знаете, обходится в миллионы

2545
01:30:22,880 --> 01:30:24,239
долларов,

2546
01:30:24,239 --> 01:30:25,760
и вы знаете, что эти достижения

2547
01:30:25,760 --> 01:30:27,679
финансируются такими же крупными организациями, как

2548
01:30:27,679 --> 01:30:30,159
google facebook open ai, у которых более или

2549
01:30:30,159 --> 01:30:32,080
менее бесконечные ресурсы в некотором роде,

2550
01:30:32,080 --> 01:30:34,880
так что мой  вопрос в том, что вы знаете, как

2551
01:30:34,880 --> 01:30:37,760
практикующий специалист с ограниченными ресурсами, но

2552
01:30:37,760 --> 01:30:40,159
с бесконечным аппетитом к обучению,

2553
01:30:40,159 --> 01:30:41,520
каким образом я могу

2554
01:30:41,520 --> 01:30:43,840
участвовать в этих достижениях, и

2555
01:30:44,719 --> 01:30:46,159
вы знаете, просто участвуйте в том, что

2556
01:30:46,159 --> 01:30:48,320
происходит в отрасли, да,

2557
01:30:48,320 --> 01:30:50,719
конечно, я имею в виду, что я действительно полностью

2558
01:30:50,719 --> 01:30:52,719
сочувствую и согласен с

2559
01:30:52,719 --> 01:30:54,400
вами в том смысле, что вы знаете, что большая

2560
01:30:54,400 --> 01:30:55,760
часть разработки этих моделей

2561
01:30:55,760 --> 01:30:58,639
происходит небольшими  группы за

2562
01:30:58,639 --> 01:31:01,360
закрытыми дверями в крупных корпорациях, и

2563
01:31:01,360 --> 01:31:03,600
обычно мне не нравится видеть, как вы

2564
01:31:03,600 --> 01:31:05,440
знаете, что наука развивалась, мне нравится рассматривать это

2565
01:31:05,440 --> 01:31:07,520
как усилие сообщества, в котором участвуют

2566
01:31:07,520 --> 01:31:09,120
все виды заинтересованных сторон с

2567
01:31:09,120 --> 01:31:11,040
разным объемом ресурсов, а мы

2568
01:31:11,040 --> 01:31:12,639
не совсем  на этом этапе с этим

2569
01:31:12,639 --> 01:31:14,560
с этой работой, но

2570
01:31:14,560 --> 01:31:17,360
я действительно думаю, что в той степени, в которой

2571
01:31:17,360 --> 01:31:19,199
люди все еще выпускают предварительно обученные

2572
01:31:19,199 --> 01:31:22,320
модели, что верно, например, для t5,

2573
01:31:22,320 --> 01:31:25,280
но не для gpt-3, предстоит много

2574
01:31:25,280 --> 01:31:28,719
работы над  в основном анализируйте

2575
01:31:28,719 --> 01:31:30,080
некоторые из вещей, которые мы

2576
01:31:30,080 --> 01:31:32,480
обсуждали ранее, вы знаете, даже работа по

2577
01:31:32,480 --> 01:31:34,960
запоминанию - это в основном я

2578
01:31:34,960 --> 01:31:36,800
бы сказал, что это похоже на аналитическую работу.

2579
01:31:42,400 --> 01:31:44,480
настолько мало, что мы на самом деле знаем

2580
01:31:44,480 --> 01:31:47,280
о том, как эти модели работают

2581
01:31:47,280 --> 01:31:50,159
и что делает их полезными в масштабе,

2582
01:31:52,880 --> 01:31:54,480
что есть много места

2583
01:31:54,480 --> 01:31:55,920
для интересной аналитической работы, которая

2584
01:31:55,920 --> 01:31:59,840
требует  требует значительно меньше вычислений,

2585
01:32:00,639 --> 01:32:02,560
я думаю, я бы сказал

2586
01:32:02,560 --> 01:32:05,600
еще пару вещей, во-первых, я

2587
01:32:05,600 --> 01:32:07,520
действительно надеюсь, что эта область будет больше двигаться в

2588
01:32:07,520 --> 01:32:09,199
сторону разработки моделей сообщества и

2589
01:32:09,199 --> 01:32:11,120
движется к фреймворкам мобов,

2590
01:32:11,120 --> 01:32:12,719
которые позволяют людям

2591
01:32:12,719 --> 01:32:14,719
совместно тренировать модель,

2592
01:32:14,719 --> 01:32:16,960
например, как в распределенном режиме.

2593
01:32:16,960 --> 01:32:18,320
думаю, что это невероятно захватывающее

2594
01:32:18,320 --> 01:32:19,440
направление исследований, это то, над чем

2595
01:32:19,440 --> 01:32:21,840
я сейчас работаю с моими студентами в моей

2596
01:32:21,840 --> 01:32:24,560
лаборатории в UNC,

2597
01:32:24,560 --> 01:32:27,120
и последнее, что я скажу, ну

2598
01:32:27,120 --> 01:32:29,199
и обычно мне не нравится это говорить,

2599
01:32:29,199 --> 01:32:31,520
но я  я все равно скажу это, я действительно

2600
01:32:31,520 --> 01:32:33,440
думаю, что

2601
01:32:33,440 --> 01:32:36,080
наше поле часто подвергается своего рода

2602
01:32:36,080 --> 01:32:38,400
шаблону тик-так, когда мы показываем,

2603
01:32:38,400 --> 01:32:40,320
что что-то возможно в масштабе, а затем

2604
01:32:40,320 --> 01:32:41,840
мы показываем, что масштаб не является необходимым

2605
01:32:41,840 --> 01:32:44,080
для достижения того же самого, и в некоторой

2606
01:32:44,080 --> 01:32:45,280
степени вы могли бы  утверждают, что это

2607
01:32:45,280 --> 01:32:47,679
уже произошло с gpg3 в том смысле,

2608
01:32:47,679 --> 01:32:49,760
что мы видели, как gpt3 получил

2609
01:32:49,760 --> 01:32:51,679
выдающиеся результаты, например, на

2610
01:32:51,679 --> 01:32:54,239
суперклее, всего с 32 примерами на класс, а

2611
01:32:54,239 --> 01:32:56,080
затем была статья, в которой

2612
01:32:56,080 --> 01:32:58,159
предлагалось  его метод под названием ipet,

2613
01:32:58,159 --> 01:33:00,400
который, как мне кажется, является интерактивным

2614
01:33:00,400 --> 01:33:03,440
итеративным шаблоном, использующим обучение,

2615
01:33:03,440 --> 01:33:04,880
которое дает практически сопоставимую

2616
01:33:04,880 --> 01:33:06,719
производительность в значительно меньшей

2617
01:33:06,719 --> 01:33:09,360
модели с тем же объемом данных,

2618
01:33:10,960 --> 01:33:13,360
и вы знаете, вы можете указать

2619
01:33:13,360 --> 01:33:15,679
на другие примеры, а я лично хотел

2620
01:33:15,679 --> 01:33:17,840
бы  приписывают историю

2621
01:33:17,840 --> 01:33:20,159
изобретения внимания тому факту, что исследователи

2622
01:33:20,159 --> 01:33:21,760
из монреальского института обучения

2623
01:33:21,760 --> 01:33:24,560
алгоритмов не могли позволить себе машину

2624
01:33:24,560 --> 01:33:26,400
с графическим процессором, поэтому они не могли запускать гигантский lstm

2625
01:33:26,400 --> 01:33:28,159
в последовательности документов, поэтому им

2626
01:33:28,159 --> 01:33:30,159
нужно было изобрести что-то, что работало бы

2627
01:33:30,159 --> 01:33:31,440
лучше, но  не требовали такой большой

2628
01:33:31,440 --> 01:33:34,000
модели, поэтому они изобрели внимание,

2629
01:33:34,000 --> 01:33:35,920
конечно, это не лучший совет -

2630
01:33:35,920 --> 01:33:37,440
говорить кому-то, что они должны просто пойти

2631
01:33:37,440 --> 01:33:39,520
изобрести что-то меньшее, но я, по

2632
01:33:39,520 --> 01:33:40,960
крайней мере, надеюсь, что некоторые из этих вещей,

2633
01:33:40,960 --> 01:33:43,440
которые мы показали, являются  возможно в масштабе

2634
01:33:43,440 --> 01:33:45,920
а также возможны в гораздо меньшем

2635
01:33:45,920 --> 01:33:48,000
масштабе

2636
01:33:48,000 --> 01:33:49,040
спасибо

2637
01:33:49,040 --> 01:33:51,840
да

2638
01:33:52,000 --> 01:33:53,840
ладно я думаю,

2639
01:33:53,840 --> 01:33:56,080
что нет никого, кто бы поддержал

2640
01:33:56,080 --> 01:33:57,679
в данный момент может быть сейчас  настал момент для

2641
01:33:57,679 --> 01:33:59,840
Джона задать свой вопрос, но если у кого-

2642
01:33:59,840 --> 01:34:02,960
то есть вопросы, сейчас

2643
01:34:02,960 --> 01:34:05,520
хороший момент, чтобы перейти к делу,

2644
01:34:05,520 --> 01:34:07,199
я просто собирался задать вопрос от

2645
01:34:07,199 --> 01:34:12,320
QA, и я вошел и спросил его в прямом эфире, так

2646
01:34:12,400 --> 01:34:13,760
что да ладно, я скажу еще кое-что

2647
01:34:13,760 --> 01:34:16,840
просто быстро, а это значит, что вы знаете, эм

2648
01:34:16,840 --> 01:34:19,199
t5, я был очень похож на то, что я сказал, я был очень

2649
01:34:19,199 --> 01:34:20,880
взволнован, что мы достигли почти человеческих

2650
01:34:20,880 --> 01:34:23,520
характеристик на суперклее.

2651
01:34:28,960 --> 01:34:30,719
меньше с точки зрения количества параметров,

2652
01:34:30,719 --> 01:34:32,320
так что это как еще один

2653
01:34:32,320 --> 01:34:34,639
разумный пример, я имею в виду, что он все еще

2654
01:34:34,639 --> 01:34:38,000
довольно большой, но, по крайней мере, вы знаете,

2655
01:34:38,000 --> 01:34:39,840
когда делаете алгоритмические и

2656
01:34:39,840 --> 01:34:41,840
архитектурные улучшения, иногда

2657
01:34:41,840 --> 01:34:44,000
вы можете закрыть эти пробелы,

2658
01:34:44,000 --> 01:34:46,719
хорошо, спасибо, Колин, и позвольте  Вы,

2659
01:34:46,719 --> 01:34:48,800
гм, что угодно, выпейте пива и ложитесь спать или

2660
01:34:48,800 --> 01:34:49,240
что-то в этом роде

2661
01:34:49,240 --> 01:34:51,040
[Смех]

2662
01:34:51,040 --> 01:34:53,280
Да, да, звучит здорово, да, еще

2663
01:34:53,280 --> 01:34:54,880
раз спасибо за то, что пригласили меня, это такое

2664
01:34:54,880 --> 01:34:56,800
удовольствие, так что я должен сказать, если у

2665
01:34:56,800 --> 01:34:58,400
кого-то есть какие-то дополнительные вопросы, которые они

2666
01:34:58,400 --> 01:35:00,480
думают о  е позже я всегда рад

2667
01:35:00,480 --> 01:35:02,000
получать электронные письма о таких вещах,

2668
01:35:02,000 --> 01:35:03,679
вы знаете, это то, над чем мне нравится работать,

2669
01:35:03,679 --> 01:35:04,719
так

2670
01:35:04,719 --> 01:35:06,560
что да, еще раз спасибо за отличную

2671
01:35:06,560 --> 01:35:10,199
информативную беседу

