1
00:00:05,359 --> 00:00:08,440
хорошо, всем привет, добро пожаловать обратно на

2
00:00:08,440 --> 00:00:12,160
cs224n, так что сегодня у нас будет

3
00:00:12,160 --> 00:00:14,240
второй из наших приглашенных спикеров в этом

4
00:00:14,240 --> 00:00:16,400
квартале, и, учитывая весь

5
00:00:16,400 --> 00:00:18,240
ажиотаж, связанный с

6
00:00:18,240 --> 00:00:20,480
предварительно обученными языковыми моделями трансформера,

7
00:00:20,480 --> 00:00:22,720
и все замечательные вещи  мы

8
00:00:22,720 --> 00:00:25,119
закончили с ними, мы особенно

9
00:00:25,119 --> 00:00:27,359
рады сегодня приветствовать

10
00:00:27,359 --> 00:00:30,000
Колина Рафаэля, который был одним из ключевых людей

11
00:00:30,000 --> 00:00:32,238
, которые продвигали исследование

12
00:00:32,238 --> 00:00:34,399
больших предварительно обученных языковых моделей, в

13
00:00:34,399 --> 00:00:37,520
частности, он был очень заинтересован

14
00:00:37,520 --> 00:00:39,680
в разработке  языковой модели t5, о

15
00:00:39,680 --> 00:00:41,520
которой мы вам

16
00:00:41,520 --> 00:00:43,600
сегодня много расскажем, но расскажу вам

17
00:00:43,600 --> 00:00:46,800
еще несколько предложений, чтобы ммм Колин

18
00:00:46,800 --> 00:00:48,559
проработал несколько лет

19
00:00:48,559 --> 00:00:50,559
в Google Brain, включая работу с

20
00:00:50,559 --> 00:00:52,960
Джеффом Хинтоном над капсульными сетями,

21
00:00:52,960 --> 00:00:55,199
затем он заинтересовался

22
00:00:55,199 --> 00:00:58,239
эффективностью  о переводе с использованием

23
00:00:58,239 --> 00:01:00,320
предварительно подготовленных языковых моделей и как

24
00:01:00,320 --> 00:01:02,800
часть работы, которую он начал

25
00:01:02,800 --> 00:01:04,720
вместе с другими людьми по

26
00:01:04,720 --> 00:01:07,280
созданию еще более крупных предварительно обученных

27
00:01:07,280 --> 00:01:08,799
языковых моделей и  проводил много

28
00:01:08,799 --> 00:01:11,680
исследований по тем эм, которые привели к

29
00:01:11,680 --> 00:01:13,680
эм, документы по t5, о которых вы

30
00:01:13,680 --> 00:01:16,560
услышите сегодня, так что добро пожаловать, Колин,

31
00:01:16,560 --> 00:01:17,920
здорово, да, большое спасибо за

32
00:01:17,920 --> 00:01:19,439
введение и за то, что меня пригласили, для меня

33
00:01:19,439 --> 00:01:20,880
определенно большая честь выступать на

34
00:01:20,880 --> 00:01:22,119
легендарном

35
00:01:22,119 --> 00:01:25,360
классе cs224n  так что да, я собираюсь

36
00:01:25,360 --> 00:01:27,759
сегодня говорить о больших языковых

37
00:01:27,759 --> 00:01:30,320
моделях в целом, но

38
00:01:30,320 --> 00:01:33,040
сосредоточившись конкретно на этой модели t5,

39
00:01:33,040 --> 00:01:34,960
которую мы выпустили около полутора лет

40
00:01:34,960 --> 00:01:37,200
назад, я представлю

41
00:01:37,200 --> 00:01:39,680
пять или около того документов, которые должны

42
00:01:39,680 --> 00:01:41,600
представлять  полный спектр хороших

43
00:01:41,600 --> 00:01:43,920
вещей, плохих и уродливых вещей о

44
00:01:43,920 --> 00:01:45,920
больших языковых моделях, и на самом деле я

45
00:01:45,920 --> 00:01:47,680
немного расскажу о статье, которая только что

46
00:01:47,680 --> 00:01:50,000
появилась в архиве вчера вечером, поэтому,

47
00:01:50,000 --> 00:01:52,320
надеюсь, каждый узнает что-то

48
00:01:52,320 --> 00:01:54,399
новое сегодня, даже если даже если вы

49
00:01:54,399 --> 00:01:55,600
уже  знаком с некоторыми из этих

50
00:01:55,600 --> 00:01:57,680
документов,

51
00:01:57,680 --> 00:01:59,680
поэтому, чтобы дать вам представление о том, что я

52
00:01:59,680 --> 00:02:01,439
буду освещать в этом выступлении, я как

53
00:02:01,439 --> 00:02:03,520
бы отвечу на каждый из этих вопросов по

54
00:02:03,520 --> 00:02:05,439
очереди

55
00:02:05,439 --> 00:02:06,399


56
00:02:06,399 --> 00:02:08,479
в ходе презентации

57
00:02:08,479 --> 00:02:10,560
э-э, и я должен упомянуть, что это,

58
00:02:10,560 --> 00:02:11,280
поскольку

59
00:02:11,280 --> 00:02:12,319
некоторые из этих

60
00:02:12,319 --> 00:02:13,920
документов являются новыми, а некоторые из этих материалов

61
00:02:13,920 --> 00:02:15,760
являются новыми, это первый раз, когда я буду

62
00:02:15,760 --> 00:02:17,440
представлять эти слайды, поэтому, если что-то

63
00:02:17,440 --> 00:02:19,440
сбивает с толку, я понимаю,

64
00:02:19,440 --> 00:02:21,520
что вы можете пинговать вопросы  меня

65
00:02:21,520 --> 00:02:23,760
через хостов и не стесняйтесь спрашивать

66
00:02:23,760 --> 00:02:25,520
обо всем, что может

67
00:02:25,520 --> 00:02:26,720
сбить с толку,

68
00:02:26,720 --> 00:02:28,720
так что да, первый вопрос, на который я

69
00:02:28,720 --> 00:02:30,000
попытаюсь ответить,

70
00:02:30,000 --> 00:02:32,800
это своего рода ответ, на котором мы

71
00:02:32,800 --> 00:02:37,040
стремились сосредоточиться в статье t5, а именно вы

72
00:02:37,040 --> 00:02:38,400
знать, какой из методов трансферного обучения

73
00:02:38,400 --> 00:02:40,239
, предложенных до сих пор, работает

74
00:02:40,239 --> 00:02:42,400
лучше всего и что происходит, когда мы масштабируем

75
00:02:42,400 --> 00:02:43,280
их,

76
00:02:43,280 --> 00:02:46,400
а затем после статьи t5 мы

77
00:02:46,400 --> 00:02:48,319
решили исследовать

78
00:02:48,319 --> 00:02:50,480
неанглоязычные предварительно обученные языковые модели

79
00:02:50,480 --> 00:02:52,560
t5 - это модель только для английского языка, их

80
00:02:52,560 --> 00:02:54,400
много  языков, на которых говорят в мире, и

81
00:02:54,400 --> 00:02:56,959
что происходит, когда мы модифицируем t5 так, чтобы

82
00:02:56,959 --> 00:02:59,680
это была массово многоязычная модель,

83
00:02:59,680 --> 00:03:01,200
а затем я расскажу о другой статье, в

84
00:03:01,200 --> 00:03:03,200
которой мы попытаемся исследовать, какие

85
00:03:03,200 --> 00:03:05,280
виды знаний и сколько знаний

86
00:03:05,280 --> 00:03:06,879
собирает модель  в ходе

87
00:03:06,879 --> 00:03:08,080
предварительного обучения,

88
00:03:08,080 --> 00:03:10,319
а затем, в связи с этим, в последующей работе

89
00:03:10,319 --> 00:03:12,239
мы попытались выяснить, действительно ли модель

90
00:03:12,239 --> 00:03:14,400
запоминает данные обучения во время

91
00:03:14,400 --> 00:03:16,560
предварительного обучения, если это на самом деле, если мы можем

92
00:03:16,560 --> 00:03:18,560
заставить ее выдавать дословные

93
00:03:18,560 --> 00:03:20,640
записи из набора данных обучения

94
00:03:20,640 --> 00:03:22,400
после обучения,

95
00:03:22,400 --> 00:03:24,080
а затем, наконец, я расскажу об очень

96
00:03:24,080 --> 00:03:26,319
недавней работе, которая по духу похожа

97
00:03:26,319 --> 00:03:29,120
на статью t5, где цель состоит в том, чтобы ответить

98
00:03:29,120 --> 00:03:31,040
не на то, какие методы трансферного обучения работают

99
00:03:31,040 --> 00:03:33,120
лучше всего, а на то, какие изменения в

100
00:03:33,120 --> 00:03:35,040
архитектуре трансформатора,

101
00:03:35,040 --> 00:03:38,000
которые предложили люди, работают лучше всего

102
00:03:38,000 --> 00:03:40,159
так что просто чтобы мотивировать это, я на самом деле

103
00:03:40,159 --> 00:03:41,760
просмотрел некоторые из лекций, которые

104
00:03:41,760 --> 00:03:43,360
вы все уже читали, просто чтобы получить

105
00:03:43,360 --> 00:03:45,360
представление о том, что вы уже узнали, и

106
00:03:45,360 --> 00:03:47,360
я знаю, что вы все уже

107
00:03:47,360 --> 00:03:48,959
хорошо знакомы с этой

108
00:03:48,959 --> 00:03:51,040
парадигмой трансферного обучения, которая имеет  как бы штурмом

109
00:03:51,040 --> 00:03:52,879
занялись обработкой естественного языка

110
00:03:52,879 --> 00:03:56,080
, но для того, чтобы

111
00:03:56,080 --> 00:03:58,720
освежить этот стиль передачи,

112
00:03:58,720 --> 00:04:00,159
мы обычно берем кучу

113
00:04:00,159 --> 00:04:02,080
немаркированных текстовых

114
00:04:02,080 --> 00:04:03,519
данных и  d мы

115
00:04:03,519 --> 00:04:05,840
применяем некоторую неконтролируемую цель, или вы

116
00:04:05,840 --> 00:04:07,840
могли бы сказать самоконтролируемую задачу,

117
00:04:07,840 --> 00:04:09,519
когда вы делаете что-то вроде того, что вы маскируете

118
00:04:09,519 --> 00:04:11,120
слова наугад, а затем вы обучаете

119
00:04:11,120 --> 00:04:12,799
модель предсказывать пропущенные слова, чтобы

120
00:04:12,799 --> 00:04:14,879
вы могли видеть эти пробелы в

121
00:04:14,879 --> 00:04:17,120
блоке текста в  зеленый и пропущенные

122
00:04:17,120 --> 00:04:18,720
слова, которые

123
00:04:18,720 --> 00:04:20,560
будет обучена предсказывать языковая модель в желтом

124
00:04:20,560 --> 00:04:22,639
поле внизу, а затем, после того, как мы проведем

125
00:04:22,639 --> 00:04:24,400
это предварительное обучение в течение некоторого времени, мы

126
00:04:24,400 --> 00:04:26,400
тонко настраиваем модель для некоторой последующей

127
00:04:26,400 --> 00:04:28,960
контролируемой задачи

128
00:04:28,960 --> 00:04:30,880
в этом примере i  Я показываю

129
00:04:30,880 --> 00:04:33,360
задачу анализа настроений для обзоров фильмов,

130
00:04:33,360 --> 00:04:35,199
и результатом всего этого является то, что

131
00:04:35,199 --> 00:04:36,880
выполнение этого первого неконтролируемого

132
00:04:36,880 --> 00:04:39,120
предтренировочного шага просто смехотворно

133
00:04:39,120 --> 00:04:40,720
полезно, не только

134
00:04:40,720 --> 00:04:42,720
обычно оно улучшает производительность, но и часто дает вам

135
00:04:42,720 --> 00:04:44,400
очень хорошую производительность при относительно

136
00:04:44,400 --> 00:04:46,720
небольших затратах.  э-э, точная настройка данных по сравнению с

137
00:04:46,720 --> 00:04:48,880
обучением с нуля, так что это

138
00:04:48,880 --> 00:04:51,280
действительно очень распространенный способ де-факто

139
00:04:51,280 --> 00:04:53,120
стандартного способа атаковать многие

140
00:04:53,120 --> 00:04:56,080
проблемы обработки естественного языка сейчас,

141
00:04:56,080 --> 00:04:58,560
и я  Я думаю, вы уже рассмотрели

142
00:04:58,560 --> 00:05:01,919
некоторые из этих методов, но из-за того, насколько

143
00:05:01,919 --> 00:05:03,919
эффективен рецепт трансферного обучения,

144
00:05:03,919 --> 00:05:05,600
действительно произошел своего рода

145
00:05:05,600 --> 00:05:08,000
взрыв работы по трансферному обучению,

146
00:05:08,000 --> 00:05:10,400
начиная, может быть, с 2018 года или около того,

147
00:05:10,400 --> 00:05:12,639
есть некоторая предшествующая работа над другими

148
00:05:12,639 --> 00:05:14,320
подходами к осуществлению передачи  обучение,

149
00:05:14,320 --> 00:05:16,639
как словесные векторы, такие как слово, для поддержки

150
00:05:16,639 --> 00:05:18,240
какой-то предварительной работы, которая

151
00:05:18,240 --> 00:05:19,840
предлагала рецепт, который я только что

152
00:05:19,840 --> 00:05:21,759
описал, под названием полу-контролируемое

153
00:05:21,759 --> 00:05:23,680
последовательное обучение, э-э, некоторая работа, которая

154
00:05:23,680 --> 00:05:25,120
предполагает, что такого рода вещи могут

155
00:05:25,120 --> 00:05:26,720
быть действительно возможны, как

156
00:05:26,720 --> 00:05:28,479
бумага для неконтролируемого нейрона настроения,

157
00:05:28,479 --> 00:05:30,880
но я бы сказал  Примерно в 2018 году

158
00:05:30,880 --> 00:05:32,560
появилась серия документов, которые

159
00:05:32,560 --> 00:05:34,880
положили начало ажиотажу в этой области, в

160
00:05:34,880 --> 00:05:36,639
том числе статья о тонкой настройке универсальной языковой модели,

161
00:05:36,639 --> 00:05:39,199
в которой содержалась статья Аламо,

162
00:05:39,199 --> 00:05:41,680
которую мы сейчас называем gpt-1, и, конечно же,

163
00:05:41,680 --> 00:05:44,160
статья Берта в конце 2018 года,

164
00:05:44,160 --> 00:05:46,160
а затем  начиная с 2019 года действительно

165
00:05:46,160 --> 00:05:48,080
произошел просто невероятный взрыв

166
00:05:48,080 --> 00:05:50,400
различных методов трансферного

167
00:05:50,400 --> 00:05:52,479
обучения, включая новое трансферное обучение.

168
00:05:52,479 --> 00:05:55,360
извините, новые цели перед обучением новые

169
00:05:55,360 --> 00:05:58,160
данные устанавливают новые способы выполнения тонкой настройки

170
00:05:58,160 --> 00:05:59,520
и так далее,

171
00:05:59,520 --> 00:06:01,120
и

172
00:06:01,120 --> 00:06:03,280
мы начали работать над проектом t5 в

173
00:06:03,280 --> 00:06:06,080
конце 2018 года, и мы заметили, что по мере

174
00:06:06,080 --> 00:06:07,759
появления всех этих методов

175
00:06:07,759 --> 00:06:09,199
становилось все труднее и труднее понять

176
00:06:09,199 --> 00:06:12,319
выяснить, что на самом деле работает лучше всего, и

177
00:06:12,319 --> 00:06:13,360


178
00:06:13,360 --> 00:06:14,720
отчасти причина этого заключалась в том,

179
00:06:14,720 --> 00:06:16,240
что было так много методов,

180
00:06:16,240 --> 00:06:17,759
которые предлагались

181
00:06:17,759 --> 00:06:20,000
одновременно, и когда это происходит,

182
00:06:20,000 --> 00:06:22,080
даже когда все работают

183
00:06:22,080 --> 00:06:24,479
всерьез, вы знаете, с доброй волей, что у

184
00:06:24,479 --> 00:06:25,919
вас может быть  ситуации, подобные этой,

185
00:06:25,919 --> 00:06:28,000
когда вы знаете, что бумага, одна статья приходит

186
00:06:28,000 --> 00:06:29,919
вместе с бумагой, которая предлагает новую

187
00:06:29,919 --> 00:06:31,680
неконтролируемую предварительную подготовку, например, технику

188
00:06:31,680 --> 00:06:34,000
под названием fancy learn, другая бумага приходит,

189
00:06:34,000 --> 00:06:36,080
возможно, примерно в то же время с

190
00:06:36,080 --> 00:06:37,520
предварительной техникой, называемой фантазией или

191
00:06:37,520 --> 00:06:39,520
обучением и бумажной предварительной подготовкой.  тренируется в

192
00:06:39,520 --> 00:06:41,840
Википедии для немаркированных данных, в то время как в статье

193
00:06:41,840 --> 00:06:43,840
b используется Википедия и корпус книг Торонто,

194
00:06:43,840 --> 00:06:45,600
который представляет собой собрание нового

195
00:06:45,600 --> 00:06:46,800
текста,

196
00:06:46,800 --> 00:06:48,720
и тогда вопрос, очевидно, будет

197
00:06:48,720 --> 00:06:50,800
интереснее l  зарабатывать лучше, чем на воображение,

198
00:06:50,800 --> 00:06:52,319
хорошо учиться, трудно сказать, потому что они используют

199
00:06:52,319 --> 00:06:54,479
разные источники данных для предварительного обучения,

200
00:06:54,479 --> 00:06:56,000
вы можете представить, что, возможно, они используют

201
00:06:56,000 --> 00:06:57,919
модели разных размеров,

202
00:06:57,919 --> 00:06:59,440
возможно, они предварительно обучаются в течение разного

203
00:06:59,440 --> 00:07:00,720
времени,

204
00:07:00,720 --> 00:07:02,560
они используют разные оптимизаторы, есть

205
00:07:02,560 --> 00:07:04,400
тонны дизайна  решения, которые здесь вступают

206
00:07:04,400 --> 00:07:06,479
в игру,

207
00:07:06,479 --> 00:07:08,000
и поэтому,

208
00:07:08,000 --> 00:07:10,000
учитывая, что эти дизайнерские решения могут

209
00:07:10,000 --> 00:07:11,599
затруднить определение того, что работает

210
00:07:11,599 --> 00:07:14,240
лучше всего, наша цель в документе t5 была своего

211
00:07:14,240 --> 00:07:16,560
рода отступить и просто сказать,

212
00:07:16,560 --> 00:07:18,800
учитывая текущий ландшафт трансферного

213
00:07:18,800 --> 00:07:20,000
обучения, вы знаете все  методы,

214
00:07:20,000 --> 00:07:22,000
которые люди предложили, что на самом деле

215
00:07:22,000 --> 00:07:24,080
работает лучше всего, когда мы сравниваем их в одной и

216
00:07:24,080 --> 00:07:26,000
той же конкретной обстановке,

217
00:07:26,000 --> 00:07:28,560
и когда мы узнаем, что работает лучше всего,

218
00:07:28,560 --> 00:07:30,560
насколько далеко мы можем продвинуть эти инструменты, которые у нас

219
00:07:30,560 --> 00:07:31,680
уже есть

220
00:07:31,680 --> 00:07:32,720
, и

221
00:07:32,720 --> 00:07:34,479
насколько мы можем изучить пределы и

222
00:07:34,479 --> 00:07:36,000
выяснить  насколько хорошо эти вещи работают в

223
00:07:36,000 --> 00:07:37,599
масштабе,

224
00:07:37,599 --> 00:07:40,000
и поэтому для решения этой проблемы мы были

225
00:07:40,000 --> 00:07:41,759
единственной вещью, которую мы

226
00:07:41,759 --> 00:07:43,759
ввели, поскольку мы снова как бы

227
00:07:43,759 --> 00:07:45,919
изучаем существующие методы

228
00:07:45,919 --> 00:07:48,160
, это  идея

229
00:07:48,160 --> 00:07:50,879
обрабатывать все текстовые проблемы в одном

230
00:07:50,879 --> 00:07:53,840
формате и такой

231
00:07:53,840 --> 00:07:57,039
подход, эта догма обработки каждой

232
00:07:57,039 --> 00:07:59,039
текстовой проблемы в одном и том же формате дает

233
00:07:59,039 --> 00:08:00,400
начало нашей модели, которую мы называем

234
00:08:00,400 --> 00:08:02,560
преобразователем преобразования текста в текст

235
00:08:02,560 --> 00:08:04,960
и, таким образом, объясняем этот формат

236
00:08:04,960 --> 00:08:07,840
Основная идея заключается в том, что мы представляем каждую

237
00:08:07,840 --> 00:08:11,199
текстовую проблему NLP как задачу преобразования текста в текст,

238
00:08:11,199 --> 00:08:13,280
и под этим я подразумеваю, что модель

239
00:08:13,280 --> 00:08:15,440
принимает текст в качестве входных данных и производит текст в

240
00:08:15,440 --> 00:08:17,919
качестве выходных данных, поэтому в таких вещах, как

241
00:08:17,919 --> 00:08:19,680
перевод с английского на немецкий, это  довольно

242
00:08:19,680 --> 00:08:22,000
типично, мы вводим

243
00:08:22,000 --> 00:08:24,000
английское предложение на входе, и мы

244
00:08:24,000 --> 00:08:25,280
обучаем модель предсказывать немецкое

245
00:08:25,280 --> 00:08:27,599
предложение на выходе,

246
00:08:27,599 --> 00:08:28,879
и вы заметите, что в нашем случае мы

247
00:08:28,879 --> 00:08:30,800
фактически также подаем то, что мы называем префиксом задачи,

248
00:08:30,800 --> 00:08:33,200
переводим английский на немецкий язык, который

249
00:08:33,200 --> 00:08:34,719
просто  сообщает модели, что мы хотели

250
00:08:34,719 --> 00:08:36,559
сделать с вводом, потому что, особенно

251
00:08:36,559 --> 00:08:38,479
если мы торгуем многозадачной моделью, если

252
00:08:38,479 --> 00:08:40,399
вы просто скармливаете модель, которая хороша,

253
00:08:40,399 --> 00:08:41,760
модель не знает, что с ней делать, она

254
00:08:41,760 --> 00:08:42,799
не знает,  Вы пытаетесь провести

255
00:08:42,799 --> 00:08:44,399
анализ настроений  это или

256
00:08:44,399 --> 00:08:46,720
английский перевод на немецкий язык, или что-то в

257
00:08:46,720 --> 00:08:48,080


258
00:08:48,080 --> 00:08:49,760
этом роде, это не должно быть таким удивительным

259
00:08:49,760 --> 00:08:52,000
, пока вы, вероятно, узнали о

260
00:08:52,000 --> 00:08:53,519
моделях кодировщика-декодера, последовательность к

261
00:08:53,519 --> 00:08:55,120
моделям последовательности, и это в основном все,

262
00:08:55,120 --> 00:08:56,560
что мы здесь делаем,

263
00:08:56,560 --> 00:08:58,240
возможно, это станет немного более необычным, когда

264
00:08:58,240 --> 00:09:00,160
мы начнем  для решения таких вещей, как

265
00:09:00,160 --> 00:09:02,399
задачи классификации текста, так что это

266
00:09:02,399 --> 00:09:04,320
пример из теста cola,

267
00:09:04,320 --> 00:09:05,760
который является корпусом

268
00:09:05,760 --> 00:09:08,160
лингвистической приемлемости, и цель

269
00:09:08,160 --> 00:09:10,320
здесь состоит в том, чтобы взять предложение и определить, является

270
00:09:10,320 --> 00:09:12,080
ли предложение приемлемым цитатой без кавычек,

271
00:09:12,080 --> 00:09:14,240
что означает, является ли

272
00:09:14,240 --> 00:09:16,240
оно грамматически правильным и  также, если

273
00:09:16,240 --> 00:09:18,720
это чушь или нет, и в этом случае

274
00:09:18,720 --> 00:09:19,760


275
00:09:19,760 --> 00:09:21,680
предложение гласит, что курс хорошо прыгает,

276
00:09:21,680 --> 00:09:23,680
и, конечно, курсы не могут прыгать, поэтому это

277
00:09:23,680 --> 00:09:25,519
предложение неприемлемо,

278
00:09:25,519 --> 00:09:27,600
но вместо обучения нашей модели

279
00:09:27,600 --> 00:09:29,600
через слой классификатора, который выводит

280
00:09:29,600 --> 00:09:31,440
метку класса или вероятность

281
00:09:31,440 --> 00:09:32,880
распределение по

282
00:09:32,880 --> 00:09:34,800
индексам классов мы фактически обучаем

283
00:09:34,800 --> 00:09:36,959
модель выводить буквальный текст

284
00:09:36,959 --> 00:09:39,440
неприемлемо, поэтому она выводит эту

285
00:09:39,440 --> 00:09:42,480
текстовую строку  токен за токеном,

286
00:09:42,480 --> 00:09:44,080
и это может стать немного даже немного

287
00:09:44,080 --> 00:09:45,839
страннее, мы можем атаковать такие вещи, как

288
00:09:45,839 --> 00:09:48,320
проблемы регрессии, где

289
00:09:48,320 --> 00:09:49,839
эффективно модель должна выводить значение с

290
00:09:49,839 --> 00:09:51,360
плавающей запятой,

291
00:09:51,360 --> 00:09:53,680
и мы фактически просто делаем это, беря

292
00:09:53,680 --> 00:09:55,200
числа с плавающей запятой и

293
00:09:55,200 --> 00:09:57,200
преобразовывая их в строки  и

294
00:09:57,200 --> 00:09:59,200
обучая модель просто предсказывать строку,

295
00:09:59,200 --> 00:10:00,800
и оказывается, что по крайней мере для этой

296
00:10:00,800 --> 00:10:03,600
конкретной задачи, которая является эталонным тестом stsb,

297
00:10:03,600 --> 00:10:05,920
он работает отлично, и в

298
00:10:05,920 --> 00:10:07,760
конечном итоге мы действительно получили современное

299
00:10:07,760 --> 00:10:09,760
состояние этого теста, поэтому делаем этот

300
00:10:09,760 --> 00:10:11,200
тип своего рода

301
00:10:11,200 --> 00:10:12,880
float  в строковое преобразование с небольшим

302
00:10:12,880 --> 00:10:14,640
количеством квантования, оказывается, отлично работает

303
00:10:14,640 --> 00:10:17,200
для проблем регрессии,

304
00:10:17,200 --> 00:10:19,279
и, наконец, вы знаете, что основная

305
00:10:19,279 --> 00:10:20,560
суть этого заключается в том, что существует

306
00:10:20,560 --> 00:10:22,079
множество действительно тонны и тонны проблем,

307
00:10:22,079 --> 00:10:23,680
которые мы можем преобразовать в этот формат, поэтому

308
00:10:23,680 --> 00:10:25,279
вот  пример абстрактного

309
00:10:25,279 --> 00:10:27,200
резюмирования, который мы подаем в новостной статье

310
00:10:27,200 --> 00:10:29,120
слева и предсказываем, что в

311
00:10:29,120 --> 00:10:30,480
сводке справа

312
00:10:30,480 --> 00:10:32,560
и снова мы действительно сможем атаковать все

313
00:10:32,560 --> 00:10:34,880
эти проблемы точно такие же,

314
00:10:34,880 --> 00:10:36,720
поэтому мы используем одну и ту же

315
00:10:36,720 --> 00:10:39,200
цель во время обучения и точно

316
00:10:39,200 --> 00:10:41,360
такую же процедуру декодирования

317
00:10:41,360 --> 00:10:44,560
во время тестирования, чтобы атаковать огромное количество

318
00:10:44,560 --> 00:10:46,480
проблем обработки естественного языка,

319
00:10:46,480 --> 00:10:48,079
и хорошая сторона в этом заключается в

320
00:10:48,079 --> 00:10:50,000
том, что до тех пор, пока

321
00:10:50,000 --> 00:10:51,680
Улучшение передачи обучения

322
00:10:51,680 --> 00:10:53,920
применимо к нашей модели, и к этому

323
00:10:53,920 --> 00:10:55,680
текстовому формату

324
00:10:55,680 --> 00:10:57,600
мы можем опробовать его на огромном наборе

325
00:10:57,600 --> 00:10:59,200
последующих

326
00:10:59,200 --> 00:11:01,519
задач, используя точно такую же модель,

327
00:11:01,519 --> 00:11:03,920
точно такую же процедуру обучения оптимизатора скорости

328
00:11:03,920 --> 00:11:05,839
обучения, точно такую же

329
00:11:05,839 --> 00:11:07,839
процедуру вывода, чтобы мы могли получить  Избавьтесь

330
00:11:07,839 --> 00:11:09,200
от тонны путаницы, о которых я

331
00:11:09,200 --> 00:11:11,200
упоминал ранее,

332
00:11:11,200 --> 00:11:12,320
они звонят,

333
00:11:12,320 --> 00:11:15,200
да, вы можете взять вторую эм,

334
00:11:15,200 --> 00:11:17,200
поскольку мы обсуждаем здесь форматирование,

335
00:11:17,200 --> 00:11:20,160
чтобы поговорить о том, как

336
00:11:20,160 --> 00:11:22,320
работает пример регрессии типа 3.8, и

337
00:11:22,320 --> 00:11:24,480
да, просто пройдите  что еще раз для

338
00:11:24,480 --> 00:11:26,160
студентов, да, абсолютно

339
00:11:26,160 --> 00:11:29,120
точно, поэтому в этой конкретной задаче stsb цель состоит в том,

340
00:11:29,120 --> 00:11:31,360
чтобы взять два предложения и предсказать число с

341
00:11:31,360 --> 00:11:33,120
плавающей запятой, которое обозначает, как

342
00:11:33,120 --> 00:11:35,360
s  похожи на эти два предложения, и это

343
00:11:35,360 --> 00:11:38,320
число с плавающей запятой находится в диапазоне от 1,0

344
00:11:38,320 --> 00:11:39,920
до 5,0,

345
00:11:39,920 --> 00:11:41,680
поэтому мы в основном взяли базовые

346
00:11:41,680 --> 00:11:44,320
аннотированные значения uh, которые мы

347
00:11:44,320 --> 00:11:46,240
должны регрессировать, мы квантовали их до

348
00:11:46,240 --> 00:11:49,839
ближайшего 0,2 и преобразовали его в строку,

349
00:11:49,839 --> 00:11:51,920
и теперь у нас есть  строка вроде три

350
00:11:51,920 --> 00:11:54,800
три периода восемь эээ 3.8 но вы можете

351
00:11:54,800 --> 00:11:56,320
думать об этом как три периода восемь или

352
00:11:56,320 --> 00:11:59,120
четыре периода ноль ээ, и мы просто

353
00:11:59,120 --> 00:12:01,600
обучаем модель предсказывать эту строку, которая в

354
00:12:01,600 --> 00:12:02,959
конечном итоге является строкой, вы знаете, что

355
00:12:02,959 --> 00:12:04,399
это больше не число, которое мы просто

356
00:12:04,399 --> 00:12:06,639
предсказываем  токен за токеном

357
00:12:06,639 --> 00:12:08,800
um, так что в некотором смысле вы можете думать

358
00:12:08,800 --> 00:12:10,320
об этом как о преобразовании проблемы регрессии в

359
00:12:10,320 --> 00:12:12,320
проблему классификации,

360
00:12:12,320 --> 00:12:14,240
потому что вы выполняете это квантование,

361
00:12:14,240 --> 00:12:16,320
но вы также в более широком смысле можете просто думать

362
00:12:16,320 --> 00:12:17,600
об этом как о преобразовании проблемы регрессии

363
00:12:17,600 --> 00:12:19,519
в текст  -to-text проблема,

364
00:12:19,519 --> 00:12:21,839
которую мы делаем,

365
00:12:21,839 --> 00:12:22,800
спасибо,

366
00:12:22,800 --> 00:12:23,600
да

367
00:12:23,600 --> 00:12:25,600
, это немного круто, но я обещаю, что это

368
00:12:25,600 --> 00:12:27,279
работает,

369
00:12:27,279 --> 00:12:28,399


370
00:12:28,399 --> 00:12:30,560
и отлично, так что еще раз приятно

371
00:12:30,560 --> 00:12:32,160
использовать такую последовательность для

372
00:12:32,160 --> 00:12:34,160
преобразования текста в текст fo  rmat заключается в том, что на

373
00:12:34,160 --> 00:12:36,399
самом деле мы можем просто использовать оригинальный ванильный

374
00:12:36,399 --> 00:12:38,399
преобразователь, как это было предложено, потому что,

375
00:12:38,399 --> 00:12:40,480
если вы помните, что преобразователь на

376
00:12:40,480 --> 00:12:42,959
самом деле был предложен для английского языка, он

377
00:12:42,959 --> 00:12:44,560
был предложен в первую очередь для машинного перевода,

378
00:12:44,560 --> 00:12:46,639
который представляет собой задачу последовательности последовательности, которую

379
00:12:46,639 --> 00:12:48,959
вы знаете, принимая язык и

380
00:12:48,959 --> 00:12:51,360
предложение  на одном языке в качестве входных данных и

381
00:12:51,360 --> 00:12:52,959
создание соответствующего предложения на

382
00:12:52,959 --> 00:12:54,959
другом языке, поэтому я не буду

383
00:12:54,959 --> 00:12:56,560
много говорить о модели, которую мы используем. В

384
00:12:56,560 --> 00:12:58,399


385
00:12:58,399 --> 00:13:00,320
стандартную архитектуру трансформатора,

386
00:13:00,320 --> 00:13:02,480
как было предложено изначально, мы внесли относительно мало изменений, и

387
00:13:02,480 --> 00:13:04,079
внимание - это все, что вы  нужно

388
00:13:04,079 --> 00:13:07,920
э-э, при построении нашей модели t5

389
00:13:07,920 --> 00:13:09,279
я буду ближе к концу разговора

390
00:13:09,279 --> 00:13:11,040
обсудить множество архитектурных

391
00:13:11,040 --> 00:13:12,480
модификаций, которые люди имели

392
00:13:12,480 --> 00:13:15,279
смысл, но для t5 для статьи t5 мы

393
00:13:15,279 --> 00:13:19,040
действительно в основном придерживались основ,

394
00:13:19,760 --> 00:13:21,279
так что следующий большой вопрос, когда вы

395
00:13:21,279 --> 00:13:23,920
атакуете  проблема трансферного обучения - это

396
00:13:23,920 --> 00:13:26,720
то, каким должен быть мой набор данных до обучения,

397
00:13:26,720 --> 00:13:28,959
и из-за Интернета есть

398
00:13:28,959 --> 00:13:31,040
много возможных кислых  массивы немаркированных

399
00:13:31,040 --> 00:13:33,680
текстовых данных.Один общий источник - это Википедия.

400
00:13:33,680 --> 00:13:35,200
Я показываю тонну статей из Википедии

401
00:13:35,200 --> 00:13:36,959
на экране,

402
00:13:36,959 --> 00:13:39,120
и

403
00:13:39,120 --> 00:13:41,120
при выполнении этого проекта одним из

404
00:13:41,120 --> 00:13:43,760
факторов, которые мы хотели изучить, было

405
00:13:43,760 --> 00:13:45,440
влияние самого набора данных до обучения

406
00:13:45,440 --> 00:13:47,680
и т.  мы фактически

407
00:13:47,680 --> 00:13:49,199
создали новый набор данных для предварительного обучения, который

408
00:13:49,199 --> 00:13:51,839
позволит нам изменять размер на многие

409
00:13:51,839 --> 00:13:54,560
порядки величины, а также имеет

410
00:13:54,560 --> 00:13:56,160
конвейер фильтрации, который позволит

411
00:13:56,160 --> 00:13:58,160
нам контролировать качество и тип данных,

412
00:13:58,160 --> 00:14:00,160
которые были предварительно обучены, и i '  Я

413
00:14:00,160 --> 00:14:03,680
опишу, как мы построили этот набор данных сейчас,

414
00:14:03,680 --> 00:14:06,240
поэтому первое, что мы сделали, это мы хотели

415
00:14:06,240 --> 00:14:07,279
получить

416
00:14:07,279 --> 00:14:08,959
наши данные из

417
00:14:08,959 --> 00:14:10,639
общедоступного источника, мы не хотели использовать

418
00:14:10,639 --> 00:14:12,320
какой-то внутренний веб-парсинг Google, который мы

419
00:14:12,320 --> 00:14:13,600
не могли выпустить,

420
00:14:13,600 --> 00:14:16,079
поэтому мы использовали  эти поисковые запросы, выполняемые

421
00:14:16,079 --> 00:14:17,920
некоммерческой организацией под названием common

422
00:14:17,920 --> 00:14:20,240
crawl, которая на самом деле является просто

423
00:14:20,240 --> 00:14:22,240
организацией, которая

424
00:14:22,240 --> 00:14:23,839
рассылает поисковые роботы через

425
00:14:23,839 --> 00:14:25,600
Интернет и загружает столько текста, сколько

426
00:14:25,600 --> 00:14:28,320
они могут, и каждый месяц они выгружают то,

427
00:14:28,320 --> 00:14:30,720
что они называют веб-электронной  xtracted текст, то

428
00:14:30,720 --> 00:14:32,720
есть вы можете думать о них как о веб-сайтах

429
00:14:32,720 --> 00:14:35,440
со всеми идеально удаленными HTML и javascript,

430
00:14:35,440 --> 00:14:36,959


431
00:14:36,959 --> 00:14:38,800
и это создает

432
00:14:38,800 --> 00:14:40,560
текст с довольно приличным количеством

433
00:14:40,560 --> 00:14:42,560
естественного языка в нем, а также много

434
00:14:42,560 --> 00:14:45,360
шаблонного

435
00:14:45,360 --> 00:14:47,199
текста и текста меню, а также немного

436
00:14:47,199 --> 00:14:48,240
бред,

437
00:14:48,240 --> 00:14:49,760
но в целом это хорошая

438
00:14:49,760 --> 00:14:51,760
отправная точка для создания

439
00:14:51,760 --> 00:14:54,399
этих предварительно обученных наборов данных,

440
00:14:54,399 --> 00:14:56,320
а затем мы предприняли несколько шагов, чтобы

441
00:14:56,320 --> 00:14:58,560
попытаться сделать так, чтобы этот набор данных был

442
00:14:58,560 --> 00:15:00,560
немного чище, поэтому первое, что мы сделали,

443
00:15:00,560 --> 00:15:02,880
это  мы удалили все строки, которые не

444
00:15:02,880 --> 00:15:05,760
заканчивались знаком пунктуации терминала; мы использовали

445
00:15:05,760 --> 00:15:07,760
языковой классификатор, чтобы сохранить только

446
00:15:07,760 --> 00:15:09,360
английский текст;

447
00:15:09,360 --> 00:15:11,680
мы удалили все, что выглядело как

448
00:15:11,680 --> 00:15:13,839
текст-заполнитель, например текст laura mipson

449
00:15:13,839 --> 00:15:15,519
справа; мы удалили все, что

450
00:15:15,519 --> 00:15:18,320
выглядело как код, который мы дедуплицировали.  вещи

451
00:15:18,320 --> 00:15:21,120
на уровне предложения, поэтому в любое время, когда любой

452
00:15:21,120 --> 00:15:23,040
фрагмент текста появлялся на нескольких страницах,

453
00:15:23,040 --> 00:15:25,279
мы сохраняли его только на одной из страниц

454
00:15:25,279 --> 00:15:27,680
и так далее, и в конечном итоге эти

455
00:15:27,680 --> 00:15:29,839
эвристики были относительно простыми, но

456
00:15:29,839 --> 00:15:32,399
производимыми  ced достаточно чистый текст, и

457
00:15:32,399 --> 00:15:34,320
позже я буду обсуждать влияние

458
00:15:34,320 --> 00:15:36,320
этих вариантов очистки, которая была одним

459
00:15:36,320 --> 00:15:39,279
из экспериментов, которые мы проводили,

460
00:15:39,279 --> 00:15:41,360
и поэтому после этого мы создали этот

461
00:15:41,360 --> 00:15:43,680
набор данных под названием c4, который представляет собой колоссальный

462
00:15:43,680 --> 00:15:45,600
корпус чистого сканирования,

463
00:15:45,600 --> 00:15:47,600
и он доступен в тензорном потоке  наборы данных,

464
00:15:47,600 --> 00:15:48,560
которые

465
00:15:48,560 --> 00:15:50,000
вам на самом деле нужно

466
00:15:50,000 --> 00:15:51,759
обрабатывать самостоятельно, что довольно

467
00:15:51,759 --> 00:15:53,759
дорого с вычислительной точки зрения,

468
00:15:53,759 --> 00:15:55,120
но, тем не менее,

469
00:15:55,120 --> 00:15:57,680
это вполне возможно и дает

470
00:15:57,680 --> 00:16:00,800
около 750 гигабайт

471
00:16:00,800 --> 00:16:04,639
достаточно чистых естественных текстовых данных.

472
00:16:04,800 --> 00:16:06,720


473
00:16:06,720 --> 00:16:08,959


474
00:16:08,959 --> 00:16:10,320
нужна наша предтренировочная

475
00:16:10,320 --> 00:16:13,440
цель, что мы собираемся делать,

476
00:16:13,440 --> 00:16:15,839
чтобы обучить модель на нашем немаркированном тексте,

477
00:16:15,839 --> 00:16:16,720
и чтобы

478
00:16:16,720 --> 00:16:19,040
просто объяснить цель, которую мы

479
00:16:19,040 --> 00:16:20,399
выбрали для нашей базовой

480
00:16:20,399 --> 00:16:22,079
экспериментальной процедуры, которую мы снова

481
00:16:22,079 --> 00:16:23,279
будем экспериментировать с другими

482
00:16:23,279 --> 00:16:25,279
предтренировочными целями позже

483
00:16:25,279 --> 00:16:27,519
представьте, что у вас есть какое-то оригинальное

484
00:16:27,519 --> 00:16:28,800
текстовое предложение, например, спасибо, что

485
00:16:28,800 --> 00:16:31,199
пригласили меня на вечеринку на прошлой неделе,

486
00:16:31,199 --> 00:16:33,120
и что мы делаем, это мы ба  Обычно мы выбираем

487
00:16:33,120 --> 00:16:35,040
некоторые слова наугад, и технически

488
00:16:35,040 --> 00:16:37,440
мы выбираем токены наугад,

489
00:16:37,440 --> 00:16:39,040
но пока давайте просто предположим, что токены

490
00:16:39,040 --> 00:16:40,160
- это слова,

491
00:16:40,160 --> 00:16:42,639
и мы собираемся выбросить эти токены,

492
00:16:42,639 --> 00:16:44,240
и в итоге мы получим что-то

493
00:16:44,240 --> 00:16:46,720
похожее на это мы для каждого  последовательный

494
00:16:46,720 --> 00:16:48,240
диапазон выпавших токенов,

495
00:16:48,240 --> 00:16:50,399
мы заменяем его на сторожевой маркер,

496
00:16:50,399 --> 00:16:52,800
и каждый сторожевой маркер получает уникальный

497
00:16:52,800 --> 00:16:54,959
индекс, поэтому один из них, который мы называем, назовем

498
00:16:54,959 --> 00:16:56,560
его дозорным x, а другой

499
00:16:56,560 --> 00:16:58,959
будет сторожевым y, и вы увидите

500
00:16:58,959 --> 00:17:01,120
что, поскольку слова для и приглашения

501
00:17:01,120 --> 00:17:03,040
- это следующие слова,

502
00:17:03,040 --> 00:17:04,880
которые мы решили замаскировать случайным образом

503
00:17:04,880 --> 00:17:06,400
маскирующими словами,

504
00:17:06,400 --> 00:17:07,599
мы собираемся заменить оба этих

505
00:17:07,599 --> 00:17:09,599
слова одним единственным

506
00:17:09,599 --> 00:17:11,520
уникальным сигнальным маркером,

507
00:17:11,520 --> 00:17:13,439
и тогда цель модели будет заключаться в том,

508
00:17:13,439 --> 00:17:15,039
чтобы  заполните пробелы, и поэтому, если вы

509
00:17:15,039 --> 00:17:16,799
знакомы с целью предварительной тренировки

510
00:17:16,799 --> 00:17:19,119
Bert, это в некоторой степени похоже на тот

511
00:17:19,119 --> 00:17:21,039
факт, что мы сворачиваем последующие

512
00:17:21,039 --> 00:17:23,599
токены в промежуток, который мы

513
00:17:23,599 --> 00:17:25,439
собираемся заменить слова f  rom немного

514
00:17:25,439 --> 00:17:26,640
отличается,

515
00:17:26,640 --> 00:17:28,079
и тот факт, что мы реконструируем

516
00:17:28,079 --> 00:17:29,440
только недостающие слова, а не

517
00:17:29,440 --> 00:17:31,039
всю последовательность, возможно,

518
00:17:31,039 --> 00:17:33,679
тоже немного отличается,

519
00:17:33,679 --> 00:17:34,960
хорошо, так что

520
00:17:34,960 --> 00:17:36,400
теперь я расскажу о нашей

521
00:17:36,400 --> 00:17:38,000
базовой экспериментальной процедуре, которую

522
00:17:38,000 --> 00:17:39,120
мы собираемся использовать.

523
00:17:39,120 --> 00:17:41,120
со временем

524
00:17:41,120 --> 00:17:44,240
и по разным осям, чтобы

525
00:17:44,240 --> 00:17:46,640
исследовать ландшафт трансферного обучения, по

526
00:17:46,640 --> 00:17:49,200
крайней мере, примерно, когда вышла эта статья,

527
00:17:49,200 --> 00:17:50,880
поэтому для предварительного обучения модели мы собираемся

528
00:17:50,880 --> 00:17:53,600
взять модель, которая имеет кодировщик размера на основе

529
00:17:53,600 --> 00:17:55,760
Берта  и декодер, так что технически имеет

530
00:17:55,760 --> 00:17:57,919
вдвое больше параметров, чем база bert,

531
00:17:57,919 --> 00:17:59,200
потому что есть кодировщик размера на основе птицы

532
00:17:59,200 --> 00:18:01,520
и декодер базового размера,

533
00:18:01,520 --> 00:18:02,960
мы собираемся использовать цель шумоподавления,

534
00:18:02,960 --> 00:18:04,400
своего рода цель моделирования массового языка,

535
00:18:04,400 --> 00:18:06,880
которую я только что описал,

536
00:18:06,880 --> 00:18:08,400
и мы собираемся  чтобы применить его к

537
00:18:08,400 --> 00:18:10,400
набору данных c4, о котором я упоминал ранее, мы

538
00:18:10,400 --> 00:18:11,840
собираемся предварительно

539
00:18:11,840 --> 00:18:14,320
обучить около 34 миллиардов токенов, что составляет

540
00:18:14,320 --> 00:18:16,400
примерно четверть времени, пока база burt была

541
00:18:16,400 --> 00:18:17,919
обучена, так что это не тонна

542
00:18:17,919 --> 00:18:19,520
предтренировочного времени, но потому что  Поскольку мы

543
00:18:19,520 --> 00:18:20,880
тренируемся, мы проводим так много

544
00:18:20,880 --> 00:18:22,559
экспериментов, что нам нужно было немного сократить его,

545
00:18:22,559 --> 00:18:24,160


546
00:18:24,160 --> 00:18:25,919
мы собираемся использовать график скорости обучения с обратным квадратом,

547
00:18:25,919 --> 00:18:27,440
который, как оказалось,

548
00:18:27,440 --> 00:18:28,960
достаточно хорошо работает в наших условиях,

549
00:18:28,960 --> 00:18:30,720
но это не так уж важно  проектное

550
00:18:30,720 --> 00:18:32,640
решение,

551
00:18:32,640 --> 00:18:35,200
а затем мы доработаем различные

552
00:18:35,200 --> 00:18:36,799
последующие задачи, тип задач, о которых

553
00:18:36,799 --> 00:18:38,640
люди очень заботились в то время, когда

554
00:18:38,640 --> 00:18:40,160
есть тест Glue, который является своего

555
00:18:40,160 --> 00:18:42,640
рода мета-тестом для многих отдельных

556
00:18:42,640 --> 00:18:45,280
последующих задач, таких как cola и stsb, которые

557
00:18:45,280 --> 00:18:47,520
я  уже упоминалось, что это то, что некоторые

558
00:18:47,520 --> 00:18:48,960
люди могут назвать задачами понимания естественного языка,

559
00:18:48,960 --> 00:18:51,440
но

560
00:18:51,440 --> 00:18:52,720
по большей части вы можете думать о них

561
00:18:52,720 --> 00:18:55,120
как о классификации предложений, задачах классификации пар предложений

562
00:18:55,120 --> 00:18:58,880
или регрессии,

563
00:18:58,880 --> 00:19:00,640
мы также рассматриваем корпус абстрактивного суммирования ежедневной почты cnn,

564
00:19:00,640 --> 00:19:02,720
это

565
00:19:02,720 --> 00:19:04,080
проблема последовательности для последовательности, где

566
00:19:04,080 --> 00:19:05,840
вам дается новостная статья, и вы должны

567
00:19:05,840 --> 00:19:07,360
вывести сводку

568
00:19:07,360 --> 00:19:09,360
теста ответов на вопросы отряда,

569
00:19:09,360 --> 00:19:10,640
который является тестом понимания прочитанного,

570
00:19:10,640 --> 00:19:13,039
где y  Вам дан абзац,

571
00:19:13,039 --> 00:19:14,320
и вы должны ответить на вопрос

572
00:19:14,320 --> 00:19:16,240
о абзаце, который вы можете атаковать

573
00:19:16,240 --> 00:19:18,160
в экстрактивной настройке, где вы

574
00:19:18,160 --> 00:19:20,080
извлекаете ответ из абзаца, или

575
00:19:20,080 --> 00:19:21,840
в абстрактной настройке, где вы просто

576
00:19:21,840 --> 00:19:23,360
выводите ответ, который

577
00:19:23,360 --> 00:19:25,919
мы используем в абстрактной форме, потому что он

578
00:19:25,919 --> 00:19:27,760
текст в текст,

579
00:19:27,760 --> 00:19:29,120
мы также включили

580
00:19:29,120 --> 00:19:30,720
тест Super Glue, который был новым тестом в

581
00:19:30,720 --> 00:19:32,240
то время, когда он был разработан

582
00:19:32,240 --> 00:19:33,840
как более сложная версия

583
00:19:33,840 --> 00:19:36,000
теста Glue, у него есть новый набор задач, которые

584
00:19:36,000 --> 00:19:38,799
были трудными для существующих моделей,

585
00:19:38,799 --> 00:19:40,160
а затем, наконец,  мы включили три

586
00:19:40,160 --> 00:19:42,559
набора данных о переводе с английского на немецкий с

587
00:19:42,559 --> 00:19:43,600
английского на французский и с английского на

588
00:19:43,600 --> 00:19:45,679
румынский перевод uh с английского на

589
00:19:45,679 --> 00:19:48,400
французский является самым большим

590
00:19:48,400 --> 00:19:49,919
набором данных с английского на румынский, который был очень большим набором данных с английского на

591
00:19:49,919 --> 00:19:51,840
румынский b он был на много

592
00:19:51,840 --> 00:19:54,000
порядков меньше,

593
00:19:54,000 --> 00:19:55,840
и мы собираемся в порядке  настраиваться на каждую из

594
00:19:55,840 --> 00:19:57,919
этих задач индивидуально и по отдельности,

595
00:19:57,919 --> 00:19:59,360
поэтому мы берем предварительно обученную модель и

596
00:19:59,360 --> 00:20:00,799
отдельно настраиваем ее для каждой из этих

597
00:20:00,799 --> 00:20:02,480
последующих задач и w  Мы собираемся выполнить

598
00:20:02,480 --> 00:20:05,679
точную настройку до 17 миллиардов токенов,

599
00:20:05,679 --> 00:20:06,960
но мы собираемся сохранять контрольные точки

600
00:20:06,960 --> 00:20:09,039
по пути, оценивать каждую контрольную точку

601
00:20:09,039 --> 00:20:10,640
в наборе проверки и сообщать о

602
00:20:10,640 --> 00:20:12,799
производительности на лучшей, э, лучшей

603
00:20:12,799 --> 00:20:16,320
контрольной точке, и обратите внимание, что это не

604
00:20:16,320 --> 00:20:18,240
экспериментально допустимый способ сообщить о своей

605
00:20:18,240 --> 00:20:19,520
производительности, потому что вы в основном

606
00:20:19,520 --> 00:20:21,280
выбираете модель для набора данных, в

607
00:20:21,280 --> 00:20:24,240
котором вы являетесь разделением данных, которое вы представляете.

608
00:20:24,240 --> 00:20:25,760


609
00:20:25,760 --> 00:20:26,880


610
00:20:26,880 --> 00:20:28,240


611
00:20:28,240 --> 00:20:29,760
хороший способ для сравнения

612
00:20:29,760 --> 00:20:31,360
различных методов, но для сравнения внутри

613
00:20:31,360 --> 00:20:34,400
методов, это э-э, нам достаточно

614
00:20:34,400 --> 00:20:37,200
удобно делать это,

615
00:20:37,520 --> 00:20:39,840
поэтому теперь я собираюсь дать вам

616
00:20:39,840 --> 00:20:41,919
очень высокий обзор некоторых

617
00:20:41,919 --> 00:20:43,840
экспериментальных результатов в этой статье.

618
00:20:43,840 --> 00:20:45,280
бумага

619
00:20:45,280 --> 00:20:47,440
довольно огромна с точки зрения

620
00:20:47,440 --> 00:20:49,760
количества экспериментов, которые мы проводили, и поэтому, если бы

621
00:20:49,760 --> 00:20:52,159
я действительно углубился в эту статью, это,

622
00:20:52,159 --> 00:20:53,360
вероятно, заняло бы у меня все время,

623
00:20:53,360 --> 00:20:54,720
но есть и другие забавные вещи, о которых я хочу

624
00:20:54,720 --> 00:20:56,240
вам рассказать, поэтому

625
00:20:56,240 --> 00:20:58,240
но в любом случае суть в том, что я буду

626
00:20:58,240 --> 00:21:00,240
показывать вам множество таблиц, подобных этой,

627
00:21:00,240 --> 00:21:02,480
и поэтому в этих таблицах в столбцах у

628
00:21:02,480 --> 00:21:03,840
вас есть производительность по различным

629
00:21:03,840 --> 00:21:05,919
последующим задачам, а в строках у вас

630
00:21:05,919 --> 00:21:07,200
есть разные экспериментальные настройки,

631
00:21:07,200 --> 00:21:08,880
которые мы считали

632
00:21:08,880 --> 00:21:10,559
так, чтобы дать вам  Пример здесь - это своего

633
00:21:10,559 --> 00:21:13,679
рода оценки, которые мы получили

634
00:21:13,679 --> 00:21:16,400
от нашей базовой линии, которая является точной

635
00:21:16,400 --> 00:21:17,840
экспериментальной процедурой, которую я должен

636
00:21:17,840 --> 00:21:21,120
описать, которую я только что описал, и я

637
00:21:21,120 --> 00:21:22,960
также запускал эту базовую линию 10 раз и

638
00:21:22,960 --> 00:21:24,400
сообщал о стандартном отклонении во

639
00:21:24,400 --> 00:21:26,400
второй строке, а затем в  в этой последней

640
00:21:26,400 --> 00:21:28,960
строке мы сообщаем о производительности

641
00:21:28,960 --> 00:21:31,520
той же модели без какого-либо предварительного обучения,

642
00:21:31,520 --> 00:21:33,440
просто в основном только обученные отдельно

643
00:21:33,440 --> 00:21:34,799
с наблюдением за всеми этими

644
00:21:34,799 --> 00:21:36,720
последующими задачами,

645
00:21:36,720 --> 00:21:38,480
и

646
00:21:38,480 --> 00:21:40,080
просто чтобы указать на пару вещей в

647
00:21:40,080 --> 00:21:41,679
этой таблице, первая очевидная вещь заключается в

648
00:21:41,679 --> 00:21:44,720
том, что  в большинстве случаев проповедь о том, что

649
00:21:44,720 --> 00:21:46,400
предварительная подготовка отсутствует, значительно

650
00:21:46,400 --> 00:21:48,559
хуже, поэтому на самом деле трансферное обучение,

651
00:21:48,559 --> 00:21:51,760
как правило, полезно там, где

652
00:21:51,760 --> 00:21:53,280
это не так,  На самом деле в этой

653
00:21:53,280 --> 00:21:55,280
задаче перевода разницы на английский язык, и

654
00:21:55,280 --> 00:21:56,799
это, вероятно, потому, что это настолько большая

655
00:21:56,799 --> 00:21:58,240
задача, что вам на самом деле не

656
00:21:58,240 --> 00:22:00,799
нужно предварительное обучение, чтобы делать э-э, чтобы преуспеть

657
00:22:00,799 --> 00:22:02,799
в ней, мы хотели включить это, потому что,

658
00:22:02,799 --> 00:22:04,559
если производительность упадет в этой

659
00:22:04,559 --> 00:22:06,240
задаче  то мы

660
00:22:06,240 --> 00:22:08,559
должны побеспокоиться

661
00:22:08,559 --> 00:22:10,559
о следующей особенности этой таблицы, которую следует заметить,

662
00:22:10,559 --> 00:22:12,799
- появлении этой маленькой звездочки, которая

663
00:22:12,799 --> 00:22:14,640
будет появляться каждый раз, когда в таблице есть строка

664
00:22:14,640 --> 00:22:15,840
, эквивалентная нашей

665
00:22:15,840 --> 00:22:17,200
базовой линии,

666
00:22:17,200 --> 00:22:19,520
и еще одна мелочь, которую следует отметить, может быть,

667
00:22:19,520 --> 00:22:21,679
если вы  знаком с историей,

668
00:22:21,679 --> 00:22:23,520
оценка, которую мы получили за клей и отряд, была

669
00:22:23,520 --> 00:22:25,120
достаточно сопоставима с Bert, так что это

670
00:22:25,120 --> 00:22:26,880
достойная проверка здравомыслия, вы знаете, что у нас есть

671
00:22:26,880 --> 00:22:30,000
модель с большим количеством параметров

672
00:22:30,000 --> 00:22:32,400
, но она обучается только

673
00:22:32,400 --> 00:22:33,600
четверть времени

674
00:22:33,600 --> 00:22:35,840
и, тем не менее, имеет

675
00:22:35,840 --> 00:22:37,760
сопоставимую производительность  которые используют

676
00:22:37,760 --> 00:22:39,200
аналогичную цель, поэтому мы не должны слишком

677
00:22:39,200 --> 00:22:40,480
удивляться этому,

678
00:22:40,480 --> 00:22:41,679
а затем последнее, что нужно упомянуть, это

679
00:22:41,679 --> 00:22:43,039
то, что мы собираемся использовать это стандартное

680
00:22:43,039 --> 00:22:45,440
отклонение снова и снова.  r снова, чтобы мы

681
00:22:45,440 --> 00:22:46,960
могли выделить записи в таблице, выделенные жирным шрифтом, когда

682
00:22:46,960 --> 00:22:48,320
они находятся в пределах одного стандартного отклонения

683
00:22:48,320 --> 00:22:50,159
от максимального значения для этого набора данных

684
00:22:50,159 --> 00:22:51,760
в таблице,

685
00:22:51,760 --> 00:22:53,280
поэтому теперь я просто сделаю большой отказ от ответственности

686
00:22:53,280 --> 00:22:55,039
, и мы собираемся сравнить множество

687
00:22:55,039 --> 00:22:56,400
разные вещи, мы собираемся провести

688
00:22:56,400 --> 00:22:58,240
множество экспериментов, но мы не собираемся

689
00:22:58,240 --> 00:23:00,240
настраивать какие-либо гиперпараметры, потому что, если бы мы

690
00:23:00,240 --> 00:23:01,039
действительно

691
00:23:01,039 --> 00:23:03,039
хотели изменить скорость обучения или

692
00:23:03,039 --> 00:23:04,480
что-то еще,

693
00:23:04,480 --> 00:23:06,159
это было бы слишком затратно в вычислительном отношении,

694
00:23:06,159 --> 00:23:07,679
чтобы сделать это для каждого отдельного

695
00:23:07,679 --> 00:23:10,400
метода, мы надеемся, что  что это нормально,

696
00:23:10,400 --> 00:23:13,440
потому что мы обрабатываем все проблемы в

697
00:23:13,440 --> 00:23:15,679
одной и той же среде,

698
00:23:15,679 --> 00:23:17,679
мы всегда проводим обучение максимальной вероятности текста в текст,

699
00:23:17,679 --> 00:23:19,760
поэтому, надеюсь, мы сможем

700
00:23:19,760 --> 00:23:21,679
сохранить фиксированные гиперпараметры и, возможно,

701
00:23:21,679 --> 00:23:22,960
если вы предложите новый метод, который

702
00:23:22,960 --> 00:23:24,559
требует обширной настройки гиперпериметра,

703
00:23:24,559 --> 00:23:26,480
он  не очень полезный метод для

704
00:23:26,480 --> 00:23:28,000
практиков,

705
00:23:28,000 --> 00:23:29,120
и мы поговорим об этом

706
00:23:29,120 --> 00:23:30,159
чуть позже, когда я буду говорить и об

707
00:23:30,159 --> 00:23:32,720
архитектурных модификациях.

708
00:23:32,720 --> 00:23:34,320


709
00:23:34,320 --> 00:23:35,840


710
00:23:35,840 --> 00:23:37,520
Мы не можем быть исчерпывающими только

711
00:23:37,520 --> 00:23:39,600
потому, что было так

712
00:23:39,600 --> 00:23:42,000
много методов, а включение или

713
00:23:42,000 --> 00:23:44,080
исключение одного конкретного метода

714
00:23:44,080 --> 00:23:46,640
не означает суждение о его качестве,

715
00:23:46,640 --> 00:23:48,960
это просто, ммм, это то, что мы смогли  с

716
00:23:48,960 --> 00:23:50,880
учетом ограничений, с которыми мы

717
00:23:50,880 --> 00:23:53,200
работали,

718
00:23:53,200 --> 00:23:55,760
поэтому первая серия экспериментов, которые мы

719
00:23:55,760 --> 00:23:57,840
провели, заключалась в сравнении различных структур моделей,

720
00:23:57,840 --> 00:24:01,440
поэтому, как я упоминал ранее,

721
00:24:01,440 --> 00:24:03,679
основная базовая модель t5 - это

722
00:24:03,679 --> 00:24:05,600
модель декодера кодировщика, и в этом случае у вас

723
00:24:05,600 --> 00:24:07,440
есть отдельный стек слоев

724
00:24:07,440 --> 00:24:09,600
для  который кодирует последовательность и

725
00:24:09,600 --> 00:24:11,360
отдельный стек слоев, который декодирует

726
00:24:11,360 --> 00:24:13,600
целевую последовательность, в основном он

727
00:24:13,600 --> 00:24:16,000
генерирует целевую последовательность по одному токену за токеном

728
00:24:16,000 --> 00:24:18,159
, обращаясь к выходным данным кодера,

729
00:24:18,159 --> 00:24:19,760
чтобы выяснить, что он должен

730
00:24:19,760 --> 00:24:21,039


731
00:24:21,039 --> 00:24:22,480
обусловить следующей

732
00:24:22,480 --> 00:24:24,240
настройкой, которую мы рассматривали, это декодер кодера

733
00:24:24,240 --> 00:24:26,000
модель, за исключением того, что все

734
00:24:26,000 --> 00:24:27,440
соответствующие параметры в

735
00:24:27,440 --> 00:24:30,080
декодере кодировщика являются общими, поэтому их в

736
00:24:30,080 --> 00:24:32,240
основном вдвое меньше, а

737
00:24:32,240 --> 00:24:34,000
затем окончательно  Другой вариант, который мы

738
00:24:34,000 --> 00:24:35,520
рассмотрели, - это модель декодера кодировщика, в

739
00:24:35,520 --> 00:24:37,520
которой кодер и декодер имеют

740
00:24:37,520 --> 00:24:39,279
вдвое меньше слоев, чем в

741
00:24:39,279 --> 00:24:41,600
базовой линии, и это потому, что мы также

742
00:24:41,600 --> 00:24:44,000
рассматриваем

743
00:24:44,000 --> 00:24:45,600
модели языка с одним стеком и то, что мы

744
00:24:45,600 --> 00:24:47,840
называем префиксной языковой моделью.  языковая

745
00:24:47,840 --> 00:24:50,400
модель - это модель, которая моделирует

746
00:24:50,400 --> 00:24:52,080
последовательность строго слева направо

747
00:24:52,080 --> 00:24:54,080
причинным образом, она в основном

748
00:24:54,080 --> 00:24:56,480
просто принимает токены по одному и

749
00:24:56,480 --> 00:24:58,480
предсказывает следующий токен,

750
00:24:58,480 --> 00:25:00,320
и вы можете фактически применить их

751
00:25:00,320 --> 00:25:02,320
к тексту для проблем с текстом,  в основном

752
00:25:02,320 --> 00:25:04,159
вводят входные данные в качестве префикса, прежде чем вы начнете

753
00:25:04,159 --> 00:25:05,840
что-либо предсказывать

754
00:25:05,840 --> 00:25:08,000
сейчас, если вы просто используете языковую модель в

755
00:25:08,000 --> 00:25:10,880
ее строгом формате, тогда вам все равно нужно

756
00:25:10,880 --> 00:25:13,279
иметь то, что мы бы назвали причинной маской, так

757
00:25:13,279 --> 00:25:16,159
что шаблон причинного внимания на префиксе,

758
00:25:16,159 --> 00:25:18,080
и именно так  Модели

759
00:25:18,080 --> 00:25:19,760
серии gpt обрабатывают

760
00:25:19,760 --> 00:25:22,720
все свои проблемы, но поскольку мы

761
00:25:22,720 --> 00:25:25,200
явно обозначаем часть последовательности

762
00:25:25,200 --> 00:25:26,720
как входную, а остальную часть последовательности

763
00:25:26,720 --> 00:25:28,640
как цель, мы фактически c

764
00:25:28,640 --> 00:25:30,559
Чтобы модель имела полную видимость, вы знаете

765
00:25:30,559 --> 00:25:32,640
непричинную маску во входной

766
00:25:32,640 --> 00:25:34,320
области последовательности, и когда мы вносим это

767
00:25:34,320 --> 00:25:36,000
изменение, мы называем это префиксной языковой

768
00:25:36,000 --> 00:25:37,679
моделью,

769
00:25:37,679 --> 00:25:39,360
и теперь результатом всего этого действительно

770
00:25:39,360 --> 00:25:41,200
является то, что модель декодера кодировщика  поскольку

771
00:25:41,200 --> 00:25:43,760
наша структура работает лучше всего, вы

772
00:25:43,760 --> 00:25:44,960
можете видеть, что когда мы делимся

773
00:25:44,960 --> 00:25:46,559
параметрами, это

774
00:25:46,559 --> 00:25:47,919
немного ухудшает производительность, но, возможно, немного меньше, чем

775
00:25:47,919 --> 00:25:49,360
вы могли ожидать,

776
00:25:49,360 --> 00:25:51,840
модель языка префиксов дает

777
00:25:51,840 --> 00:25:53,360
немного худшую производительность, но

778
00:25:53,360 --> 00:25:54,960
значительно лучшую производительность, чем

779
00:25:54,960 --> 00:25:56,720
выполнение строго причинно-следственной связи  правильного

780
00:25:56,720 --> 00:25:58,559
языкового моделирования, которое

781
00:25:58,559 --> 00:26:00,480
вы видите здесь в четвертой строке, и,

782
00:26:00,480 --> 00:26:02,240
наконец, наличие количества параметров

783
00:26:02,240 --> 00:26:04,080
в кодировщике и декодере

784
00:26:04,080 --> 00:26:07,520
значительно снижает производительность

785
00:26:07,520 --> 00:26:09,520
, следует отметить, что во всех

786
00:26:09,520 --> 00:26:11,679
этих случаях мы обрабатываем одну и ту же

787
00:26:11,679 --> 00:26:13,360
сумму  длина последовательности - это та же

788
00:26:13,360 --> 00:26:14,720
входная последовательность в той же целевой

789
00:26:14,720 --> 00:26:17,440
последовательности, поэтому в большинстве этих случаев

790
00:26:17,440 --> 00:26:18,960
общее количество флопов, необходимых для

791
00:26:18,960 --> 00:26:21,120
обработки последовательности  ce то же самое,

792
00:26:21,120 --> 00:26:22,400
даже несмотря на то, что количество параметров в

793
00:26:22,400 --> 00:26:26,159
два раза больше в базовой модели,

794
00:26:26,159 --> 00:26:27,600
поэтому следующее, что мы рассмотрели, были

795
00:26:27,600 --> 00:26:29,039
разные варианты нашей предтренировочной

796
00:26:29,039 --> 00:26:31,520
задачи, поэтому первое, что мы сделали, это как

797
00:26:31,520 --> 00:26:33,120
бы сравнить разные подходы высокого уровня.

798
00:26:33,120 --> 00:26:35,120
может быть, просто обучить

799
00:26:35,120 --> 00:26:36,640
модель предсказывать следующий токен по одному

800
00:26:36,640 --> 00:26:37,919
токену за раз, это своего рода

801
00:26:37,919 --> 00:26:40,080
задача языкового моделирования,

802
00:26:40,080 --> 00:26:42,000
другая - взять входную последовательность,

803
00:26:42,000 --> 00:26:43,679
перетасовать ее и обучить модель

804
00:26:43,679 --> 00:26:46,400
предсказывать последовательность без перетасовки или

805
00:26:46,400 --> 00:26:48,720
рассмотреть модель массового языка

806
00:26:48,720 --> 00:26:50,320
стиль цели в стиле берта, подобный той, о

807
00:26:50,320 --> 00:26:52,559
которой мы упоминали ранее,

808
00:26:52,559 --> 00:26:54,720
и теперь на втором этапе, который я

809
00:26:54,720 --> 00:26:56,799
показываю здесь, результаты для, мы

810
00:26:56,799 --> 00:26:59,039
считали, цель стиля птицы,

811
00:26:59,039 --> 00:27:00,640
где модель обучена предсказывать

812
00:27:00,640 --> 00:27:02,559
всю

813
00:27:02,559 --> 00:27:04,880
исходную неповрежденную входную последовательность

814
00:27:04,880 --> 00:27:06,720
цель массового стиля, которая очень

815
00:27:06,720 --> 00:27:08,480
похожа,

816
00:27:08,480 --> 00:27:10,400
а затем цель замены поврежденных промежутков,

817
00:27:10,400 --> 00:27:11,919
которая похожа на ту, которую я

818
00:27:11,919 --> 00:27:13,279
описал в начале, которую мы

819
00:27:13,279 --> 00:27:15,440
используем в нашей  r базовая модель и, наконец,

820
00:27:15,440 --> 00:27:17,520
вариант, в котором вместо того, чтобы заменять

821
00:27:17,520 --> 00:27:20,320
каждый жетон э-э на уникальный сторожевой

822
00:27:20,320 --> 00:27:22,320
жетон, мы просто полностью отбрасываем массовые

823
00:27:22,320 --> 00:27:23,919
жетоны и обучаем модель

824
00:27:23,919 --> 00:27:26,000
предсказывать выпавшие жетоны,

825
00:27:26,000 --> 00:27:28,399
и вы можете видеть, что последние два

826
00:27:28,399 --> 00:27:30,320
варианта работают примерно так же, как и один

827
00:27:30,320 --> 00:27:32,399
еще одно, но еще одно существенное

828
00:27:32,399 --> 00:27:34,480
различие между этими

829
00:27:34,480 --> 00:27:36,559
наборами целей состоит в том, что первые два

830
00:27:36,559 --> 00:27:38,399
включают прогнозирование всей входной

831
00:27:38,399 --> 00:27:41,039
последовательности, а последние два в основном

832
00:27:41,039 --> 00:27:43,760
включают прогнозирование собранных токенов,

833
00:27:43,760 --> 00:27:45,440
а когда вы прогнозируете только токены талисмана, у

834
00:27:45,440 --> 00:27:47,120
вас гораздо более короткая целевая

835
00:27:47,120 --> 00:27:49,120
последовательность  Таким образом, общая

836
00:27:49,120 --> 00:27:51,919
стоимость предварительного обучения значительно ниже, поэтому

837
00:27:51,919 --> 00:27:53,279
мы решили, что это лучший

838
00:27:53,279 --> 00:27:55,840
подход, а затем мы рассмотрели другие

839
00:27:55,840 --> 00:27:58,640
гиперпараметры в нашей стратегии маскирования,

840
00:27:58,640 --> 00:28:03,600
такие как количество токенов, которые нужно замаскировать,

841
00:28:03,600 --> 00:28:05,360
поэтому следующее, что мы рассмотрели, были

842
00:28:05,360 --> 00:28:07,039
разные варианты  набор данных до обучения

843
00:28:07,039 --> 00:28:08,399


844
00:28:08,399 --> 00:28:10,480
в нашей базовой линии мы использовали набор данных c4,

845
00:28:10,480 --> 00:28:12,240
который я предложил

846
00:28:12,240 --> 00:28:13,840
в начале выступления,

847
00:28:13,840 --> 00:28:16,080
мы также c  по сравнению с предварительным обучением только

848
00:28:16,080 --> 00:28:18,799
на неотфильтрованных данных из c4, поэтому вместо того,

849
00:28:18,799 --> 00:28:20,320
чтобы выполнять все эти шаги эвристической фильтрации,

850
00:28:20,320 --> 00:28:22,320
мы просто берем необработанный

851
00:28:22,320 --> 00:28:24,720
текст, извлеченный из Интернета из c4, и предварительно обучаем его, и

852
00:28:24,720 --> 00:28:26,320
вы можете видеть, что это работает равномерно

853
00:28:26,320 --> 00:28:28,320
хуже, так что это  действительно кажется правдой,

854
00:28:28,320 --> 00:28:29,600
что эти шаги по очистке, которые мы

855
00:28:29,600 --> 00:28:31,279
делаем, на самом деле

856
00:28:31,279 --> 00:28:33,279
полезны следующие четыре набора данных были нашей попыткой

857
00:28:33,279 --> 00:28:34,080


858
00:28:34,080 --> 00:28:35,840
предварительно обучиться на аналогичных наборах данных,

859
00:28:35,840 --> 00:28:38,080
которые использовались в пароле, настоящий набор данных новостей

860
00:28:38,080 --> 00:28:39,760
взят из газеты Гровера.

861
00:28:39,760 --> 00:28:40,960
По сути,

862
00:28:40,960 --> 00:28:42,799
предварительное обучение только на данных с новостных

863
00:28:42,799 --> 00:28:45,520
сайтов. Веб-текст - это набор данных, который

864
00:28:45,520 --> 00:28:47,679
использовался в документе gpt2, где вы

865
00:28:47,679 --> 00:28:50,320
тренируетесь только на веб-тексте, который был связан с Reddit и

866
00:28:50,320 --> 00:28:52,399
получил достаточно высокий балл,

867
00:28:52,399 --> 00:28:53,279


868
00:28:53,279 --> 00:28:55,039
а затем два последних варианта

869
00:28:55,039 --> 00:28:57,520
либо  только wikipedia или как использовалось

870
00:28:57,520 --> 00:28:59,679
в wiki в wikipedia burp paper

871
00:28:59,679 --> 00:29:02,000
с корпусом toronto books,

872
00:29:02,000 --> 00:29:03,600
и

873
00:29:03,600 --> 00:29:05,200
вы могли бы заметить, что некоторые из

874
00:29:05,200 --> 00:29:07,120
этих более специализированных наборов данных мы получаем

875
00:29:07,120 --> 00:29:09,440
лучшую производительность, поэтому, например, вы

876
00:29:09,440 --> 00:29:11,200
можете  е, что в Википедии и

877
00:29:11,200 --> 00:29:13,279
корпусе книг Торонто в нижнем ряду

878
00:29:13,279 --> 00:29:15,600
мы на самом деле намного лучше справляемся с суперклеем

879
00:29:15,600 --> 00:29:18,000
с оценкой чуть больше 73

880
00:29:18,000 --> 00:29:20,480
по сравнению с предварительным обучением на c4, и,

881
00:29:20,480 --> 00:29:22,880
оказывается, это потому, что мы или мы

882
00:29:22,880 --> 00:29:24,960
предполагаем, что это  это потому, что

883
00:29:24,960 --> 00:29:27,039
суперклей содержит задачу под названием

884
00:29:27,039 --> 00:29:28,640
multi-rc, которая представляет собой

885
00:29:28,640 --> 00:29:30,480
задачу понимания прочитанного

886
00:29:30,480 --> 00:29:34,080
в вики-новостях, извините, в

887
00:29:34,080 --> 00:29:36,960
статьях из энциклопедии и в романах, поэтому основной

888
00:29:36,960 --> 00:29:38,720
вывод здесь заключается в том, что когда вы предварительно тренируетесь

889
00:29:38,720 --> 00:29:41,120
на данных, которые похожи на вашу

890
00:29:41,120 --> 00:29:42,640
последующую задачу  у которого есть похожий

891
00:29:42,640 --> 00:29:44,720
домен, вы часто получаете большой импульс в этой

892
00:29:44,720 --> 00:29:46,399
последующей задаче, и это действительно то, что

893
00:29:46,399 --> 00:29:48,080
произошло здесь, что

894
00:29:48,080 --> 00:29:49,440
интересно, вы также можете увидеть

895
00:29:49,440 --> 00:29:51,600
противоположный эффект, поэтому, если вы посмотрите

896
00:29:51,600 --> 00:29:54,399
википедию во второй и последней строке,

897
00:29:54,399 --> 00:29:56,399
если вы только предварительно тренируетесь на  Википедия, вы в

898
00:29:56,399 --> 00:29:58,399
конечном итоге намного хуже справляетесь с колой, которая

899
00:29:58,399 --> 00:30:00,399
является корпусом задач лингвистической приемлемости,

900
00:30:00,399 --> 00:30:02,559
о которых я упоминал ранее, и мы

901
00:30:02,559 --> 00:30:04,320
предполагаем, что это потому, что в

902
00:30:04,320 --> 00:30:06,640
википедии очень мало неприемлемого

903
00:30:06,640 --> 00:30:08,720
текста, который вы базовый  lly только предварительное обучение

904
00:30:08,720 --> 00:30:10,960
на чистом тексте, тогда как в c4 есть некоторый

905
00:30:10,960 --> 00:30:13,200
неграмотный текст, в нем есть какая-то ерунда,

906
00:30:13,200 --> 00:30:14,480
и так что это действительно может немного повысить вашу

907
00:30:14,480 --> 00:30:16,559
производительность на коле,

908
00:30:16,559 --> 00:30:18,159
последнее, что следует отметить, это то, что, хотя вы

909
00:30:18,159 --> 00:30:20,080
иногда видите некоторый выигрыш при использовании этих

910
00:30:20,080 --> 00:30:22,080
меньших данных  Эти наборы данных

911
00:30:22,080 --> 00:30:23,679
примерно на порядок меньше, чем

912
00:30:23,679 --> 00:30:26,399
c4, поэтому возникает естественный вопрос:

913
00:30:26,399 --> 00:30:28,399
действительно ли вам больно предварительно тренироваться на

914
00:30:28,399 --> 00:30:30,080
меньшем наборе данных,

915
00:30:30,080 --> 00:30:31,840
поэтому для ответа на этот вопрос мы

916
00:30:31,840 --> 00:30:34,480
в основном взяли c4 и

917
00:30:34,480 --> 00:30:36,799
искусственно уменьшили его  так что это повторялось

918
00:30:36,799 --> 00:30:38,640
в ходе предварительного обучения,

919
00:30:38,640 --> 00:30:40,960
и вы можете видеть здесь, что когда вы

920
00:30:40,960 --> 00:30:44,080
повторяете набор данных 64 раза

921
00:30:44,080 --> 00:30:47,120
, получается 34 миллиарда, разделенные на 64 токена,

922
00:30:47,120 --> 00:30:48,399
потому что это то количество предварительных тренировок, которое мы

923
00:30:48,399 --> 00:30:49,600
провели,

924
00:30:49,600 --> 00:30:51,120
вы на самом деле не жертвуете часами

925
00:30:51,120 --> 00:30:53,360
производительность производительность примерно

926
00:30:53,360 --> 00:30:54,559
такая же,

927
00:30:54,559 --> 00:30:57,440
но если вы повторите набор данных 256

928
00:30:57,440 --> 00:30:59,679
раз 1024 или более раз, вы действительно начнете

929
00:30:59,679 --> 00:31:01,440
видеть деградацию, и причина, по которой мы

930
00:31:01,440 --> 00:31:02,960
думаем, что это происходит, заключается в том, что

931
00:31:02,960 --> 00:31:04,559
вы базовый  Во время

932
00:31:04,559 --> 00:31:06,960
предварительной подготовки вы можете понять

933
00:31:06,960 --> 00:31:08,320
, правда это или нет,

934
00:31:08,320 --> 00:31:09,679
просто посмотрев на потери при обучении,

935
00:31:09,679 --> 00:31:11,519
вы увидите, что модель

936
00:31:11,519 --> 00:31:13,600
достигает гораздо меньших потерь при обучении, когда вы

937
00:31:13,600 --> 00:31:15,600
повторяете набор данных все больше и больше.  раз,

938
00:31:15,600 --> 00:31:17,440
поэтому результатом этого является то, что ваш

939
00:31:17,440 --> 00:31:19,360
набор данных должен быть как минимум настолько большим, чтобы вы

940
00:31:19,360 --> 00:31:21,600
не видели значительного переобучения во время

941
00:31:21,600 --> 00:31:23,360
предварительного обучения

942
00:31:23,360 --> 00:31:24,159
и

943
00:31:24,159 --> 00:31:26,159
позже, когда мы масштабируем эти модели

944
00:31:26,159 --> 00:31:28,320
и предварительно обучаем их на гораздо большем количестве данных, которые мы

945
00:31:28,320 --> 00:31:29,360
будем делать

946
00:31:29,360 --> 00:31:31,440
достаточное количество повторений тех меньших типов

947
00:31:31,440 --> 00:31:33,519
наборов данных, более специфичных для предметной области, которые, как мы

948
00:31:33,519 --> 00:31:37,840
думаем, мы увидим вредные эффекты.

949
00:31:38,159 --> 00:31:40,880
Следующим, с чем мы экспериментировали, были

950
00:31:40,880 --> 00:31:43,039
стратегии многозадачного обучения, поэтому, когда

951
00:31:43,039 --> 00:31:44,399
вы выполняете многозадачное обучение, вы, по

952
00:31:44,399 --> 00:31:46,399
сути, тренируете модель на

953
00:31:46,399 --> 00:31:49,200
несколько задач одновременно, и

954
00:31:49,200 --> 00:31:50,799
в большинстве экспериментов, которые я показываю

955
00:31:50,799 --> 00:31:53,200
на этом слайде, на

956
00:31:53,200 --> 00:31:55,200
самом деле мы тренируемся по каждой отдельной задаче

957
00:31:55,200 --> 00:31:57,440
одновременно, поэтому предварительная тренировочная задача и

958
00:31:57,440 --> 00:31:59,600
все последующие задачи вместе

959
00:31:59,600 --> 00:32:01,279
и наиболее уместный вопрос, когда

960
00:32:01,279 --> 00:32:02,640
вы выполняете многозадачное обучение, как

961
00:32:02,640 --> 00:32:04,960
это, заключается в том, как часто я должен выполнять выборку данных

962
00:32:04,960 --> 00:32:06,320
из каждой задачи,

963
00:32:06,320 --> 00:32:08,799
поэтому один подход - просто выборка данных

964
00:32:08,799 --> 00:32:11,120
с одинаковой скоростью для всех

965
00:32:11,120 --> 00:32:12,240
задач,

966
00:32:12,240 --> 00:32:15,440
другой случай - в основном  сделайте вид,

967
00:32:15,440 --> 00:32:16,799
будто вы только что объединили все

968
00:32:16,799 --> 00:32:18,480
наборы данных, которые мы называем примерами

969
00:32:18,480 --> 00:32:19,679
пропорционального смешивания, потому что это

970
00:32:19,679 --> 00:32:21,840
эквивалентно выборке из набора данных

971
00:32:21,840 --> 00:32:23,519
в соответствии с тем, сколько примеров

972
00:32:23,519 --> 00:32:24,960
есть в наборе данных.

973
00:32:24,960 --> 00:32:26,320


974
00:32:26,320 --> 00:32:28,480
набор данных настолько велик,

975
00:32:28,480 --> 00:32:30,880
что его доля будет

976
00:32:30,880 --> 00:32:32,559
намного больше, чем у каждой последующей

977
00:32:32,559 --> 00:32:34,240
задачи, и мы, по сути, никогда не будем тренироваться ни

978
00:32:34,240 --> 00:32:36,480
на одном из последующих данных, поэтому мы

979
00:32:36,480 --> 00:32:38,960
ввели этот гиперпараметр k,

980
00:32:38,960 --> 00:32:41,919
который является константой, которая в основном

981
00:32:41,919 --> 00:32:43,519
определяет размер, который мы должны  Представьте, что

982
00:32:43,519 --> 00:32:45,679
набор данных перед обучением -

983
00:32:45,679 --> 00:32:48,720
это последнее, что вы можете сделать, это взять

984
00:32:48,720 --> 00:32:50,399
количество примеров в каждом наборе данных и

985
00:32:50,399 --> 00:32:52,159
масштабировать его по температуре,

986
00:32:52,159 --> 00:32:53,919
чем больше температура, тем ближе

987
00:32:53,919 --> 00:32:56,399
вы получаете равное смешивание и единообразную

988
00:32:56,399 --> 00:32:59,440
выборку из каждого набора данных,

989
00:32:59,440 --> 00:33:01,679
но в любом случае главный вывод из

990
00:33:01,679 --> 00:33:03,360
этой таблицы заключается в том, что

991
00:33:03,360 --> 00:33:05,039
вы можете довольно близко подойти к

992
00:33:05,039 --> 00:33:07,760
производительности отдельного предварительного обучения и

993
00:33:07,760 --> 00:33:09,840
точной настройки, как мы это делаем на нашем базовом уровне,

994
00:33:09,840 --> 00:33:12,000
если вы получите  стратегия смешивания верна, но в

995
00:33:12,000 --> 00:33:14,000
конечном итоге мы обнаружили, что вы склонны

996
00:33:14,000 --> 00:33:15,760
жертвовать некоторой производительностью при

997
00:33:15,760 --> 00:33:18,080
выполнении многозадачного обучения, и, по крайней мере, по

998
00:33:18,080 --> 00:33:20,480
некоторым задачам и колин, было

999
00:33:20,480 --> 00:33:22,640
много вопросов по выбору

1000
00:33:22,640 --> 00:33:24,559
на слайде с различными наборами данных

1001
00:33:24,559 --> 00:33:28,000
один показывает настоящие новости и c4 и так далее

1002
00:33:28,000 --> 00:33:29,840
, вы хотите взять пару,

1003
00:33:29,840 --> 00:33:33,039
абсолютно да, во-первых, эм,

1004
00:33:33,039 --> 00:33:34,720
если вы просто посмотрите на это,

1005
00:33:34,720 --> 00:33:38,240
он все еще выглядит так, как будто вы

1006
00:33:38,240 --> 00:33:41,039
можете получить отличные результаты с

1007
00:33:41,039 --> 00:33:43,039
более чем на порядок меньше

1008
00:33:43,039 --> 00:33:46,000
текста, чем  c4 и да, похоже,

1009
00:33:46,000 --> 00:33:47,600
что это не то сообщение, которое вы

1010
00:33:47,600 --> 00:33:50,240
хотели выдвинуть, да, так что здесь есть

1011
00:33:50,240 --> 00:33:51,840
небольшой нюанс, который в основном заключается в

1012
00:33:51,840 --> 00:33:53,360
том, что в нашей базовой линии в этих

1013
00:33:53,360 --> 00:33:54,640
экспериментах, которые я проводил до

1014
00:33:54,640 --> 00:33:56,720
сих пор, мы действительны  Я не занимаюсь предварительной подготовкой

1015
00:33:56,720 --> 00:33:58,640
так долго, так как, как я уже упоминал ранее,

1016
00:33:58,640 --> 00:33:59,840
мы на самом деле предварительно тренируемся на

1017
00:33:59,840 --> 00:34:02,720
четверть до тех пор, пока не эээ, а на самом деле

1018
00:34:02,720 --> 00:34:04,240
для нас

1019
00:34:04,240 --> 00:34:06,960
я верю одну 256-ю часть

1020
00:34:06,960 --> 00:34:11,040
до тех пор, пока э-э, например, нетто, поэтому

1021
00:34:11,040 --> 00:34:13,359
и когда позже  документ, который мы собираемся

1022
00:34:13,359 --> 00:34:15,679
подготовить намного дольше, и

1023
00:34:15,679 --> 00:34:17,599
в этом случае мы бы в конечном итоге повторили

1024
00:34:17,599 --> 00:34:20,000
эти наборы данных много раз в

1025
00:34:20,000 --> 00:34:21,440
течение предварительного обучения, и мы начали бы

1026
00:34:21,440 --> 00:34:22,719
видеть эти негативные эффекты, которые я

1027
00:34:22,719 --> 00:34:24,399
объяснил на  следующий слайд, ну, вот

1028
00:34:24,399 --> 00:34:26,239
почему вы затем повторяете, хорошо,

1029
00:34:26,239 --> 00:34:28,000
кто-то потом спросил об этом, почему

1030
00:34:28,000 --> 00:34:29,679
вы тренируетесь на одном и том же снова

1031
00:34:29,679 --> 00:34:31,839
и снова, это тест

1032
00:34:31,839 --> 00:34:32,719


1033
00:34:32,719 --> 00:34:35,440


1034
00:34:35,440 --> 00:34:36,480


1035
00:34:36,480 --> 00:34:37,918
так

1036
00:34:37,918 --> 00:34:40,480
что в первом приближении c4 содержит

1037
00:34:40,480 --> 00:34:44,399
википедию или, возможно, многие

1038
00:34:44,399 --> 00:34:46,159
страницы википедии, но не всю

1039
00:34:46,159 --> 00:34:49,359
википедию, ну, вы знаете, что обычное

1040
00:34:49,359 --> 00:34:53,440
сканирование выполняется, как будто это своего рода

1041
00:34:53,440 --> 00:34:55,520
веб-сканирование, переходя по ссылкам с определенным

1042
00:34:55,520 --> 00:34:58,160
приоритетом, и в конечном итоге это  не охватывал

1043
00:34:58,160 --> 00:34:59,920
всю Википедию  Я на самом деле не

1044
00:34:59,920 --> 00:35:01,680
знаю точную долю википедии, которая

1045
00:35:01,680 --> 00:35:05,200
включена в c4, но определенно,

1046
00:35:05,200 --> 00:35:07,440
когда вы тренируетесь на c4, вы увидите какой-то

1047
00:35:07,440 --> 00:35:09,680
текст википедии, ну, он будет в

1048
00:35:09,680 --> 00:35:11,599
относительно низкой пропорции по сравнению со

1049
00:35:11,599 --> 00:35:14,160
всеми другими данными, которые вы будете

1050
00:35:14,160 --> 00:35:16,160
источниками  данные, которые вы обязательно увидите, а затем

1051
00:35:16,160 --> 00:35:18,640
кто-то не совсем убедился в вашем

1052
00:35:18,640 --> 00:35:21,520
аргументе, что хорошее качество

1053
00:35:21,520 --> 00:35:23,280
Википедии объясняет худшую

1054
00:35:23,280 --> 00:35:26,160
производительность колы, потому что они наверняка хорошо думали, что

1055
00:35:26,160 --> 00:35:28,640


1056
00:35:28,640 --> 00:35:31,520
настоящие новости, в основном хорошо отредактированный

1057
00:35:31,520 --> 00:35:33,920
текст, и все же, похоже,  работают

1058
00:35:33,920 --> 00:35:35,040
нормально,

1059
00:35:35,040 --> 00:35:37,119
да, это хороший момент, я не

1060
00:35:37,119 --> 00:35:38,720
уверен, что вы знаете, что это могут быть настоящие новости,

1061
00:35:38,720 --> 00:35:40,800
потому что в настоящих новостях есть цитаты, или,

1062
00:35:40,800 --> 00:35:43,280
может быть, в настоящих новостях есть

1063
00:35:43,280 --> 00:35:44,880
контент из общих разделов

1064
00:35:44,880 --> 00:35:45,839
сайтов,

1065
00:35:45,839 --> 00:35:47,839
я должен сказать, что когда  i причина того,

1066
00:35:47,839 --> 00:35:49,760
что это настоящие новости и веб-текст,

1067
00:35:49,760 --> 00:35:51,839
заключается в том, что это наши собственные репродукции,

1068
00:35:51,839 --> 00:35:53,440
поэтому они могут не быть в точности такими

1069
00:35:53,440 --> 00:35:55,760
же, как первоначально предложенные варианты,

1070
00:35:55,760 --> 00:35:57,440
потому что веб-текст, например, никогда не

1071
00:35:57,440 --> 00:35:59,839
публиковался  Эд

1072
00:36:00,400 --> 00:36:01,520
да, но это интересный момент, и

1073
00:36:01,520 --> 00:36:03,680
именно поэтому я бы сказал, что

1074
00:36:03,680 --> 00:36:05,280
это предположение, что вы не знаете,

1075
00:36:05,280 --> 00:36:06,560
что я могу сделать строгое

1076
00:36:06,560 --> 00:36:08,000
заявление о

1077
00:36:08,000 --> 00:36:10,160
да ладно, может быть, мы должны позволить вам

1078
00:36:10,160 --> 00:36:11,200
продолжить сейчас

1079
00:36:11,200 --> 00:36:12,480
большое спасибо да спасибо за тех

1080
00:36:12,480 --> 00:36:13,520
вопросы,

1081
00:36:13,520 --> 00:36:14,480


1082
00:36:14,480 --> 00:36:16,640
так что следующее, что нужно сделать после

1083
00:36:16,640 --> 00:36:18,160
изучения этих различных стратегий многозадачного обучения,

1084
00:36:18,160 --> 00:36:20,079


1085
00:36:20,079 --> 00:36:22,000
- это посмотреть, есть ли у нас какой-либо способ

1086
00:36:22,000 --> 00:36:24,640
сократить разрыв между многозадачным обучением

1087
00:36:24,640 --> 00:36:26,800
и этим предварительным обучением, за которым следует

1088
00:36:26,800 --> 00:36:29,359
отдельная тонкая настройка, и мы экспериментировали

1089
00:36:29,359 --> 00:36:30,800
со многими  здесь разные стратегии, в

1090
00:36:30,800 --> 00:36:33,520
основном строгое многозадачное обучение

1091
00:36:33,520 --> 00:36:35,359
, многозадачное обучение с последующей

1092
00:36:35,359 --> 00:36:37,680
тонкой настройкой индивидуальных задач,

1093
00:36:37,680 --> 00:36:39,520
многозадачное обучение, но без

1094
00:36:39,520 --> 00:36:41,760
каких-либо неконтролируемых данных, и действительно

1095
00:36:41,760 --> 00:36:43,200
главный вывод из всех этих

1096
00:36:43,200 --> 00:36:45,200
экспериментов заключался в том, что если вы выполняете

1097
00:36:45,200 --> 00:36:48,160
многозадачность  сначала обучение, включая

1098
00:36:48,160 --> 00:36:49,760
неконтролируемую задачу,

1099
00:36:49,760 --> 00:36:52,000
а затем вы настраиваете модель для каждой

1100
00:36:52,000 --> 00:36:53,680
задачи отдельно,

1101
00:36:53,680 --> 00:36:55,359
что является третьей строкой здесь, которую вы на самом деле

1102
00:36:55,359 --> 00:36:57,200
не на самом деле  жертвовать большой производительностью

1103
00:36:57,200 --> 00:36:59,599
вообще вы не в конечном итоге с

1104
00:36:59,599 --> 00:37:01,680
многозадачной моделью, потому что вы

1105
00:37:01,680 --> 00:37:03,839
настраиваете каждую задачу индивидуально,

1106
00:37:03,839 --> 00:37:06,079
но хорошая вещь в этом подходе

1107
00:37:06,079 --> 00:37:08,800
заключается в том, что вы можете контролировать

1108
00:37:08,800 --> 00:37:11,119
производительность своих последующих задач, пока вы

1109
00:37:11,119 --> 00:37:12,880
выполняете предварительную тренировку, и вы не

1110
00:37:12,880 --> 00:37:16,160
жертвуете большой производительностью,

1111
00:37:16,160 --> 00:37:18,079
один параметр, который мы не рассматривали,

1112
00:37:18,079 --> 00:37:20,079
- это предварительная подготовка без учителя, за

1113
00:37:20,079 --> 00:37:22,640
которой следует контролируемое многозадачное обучение.

1114
00:37:22,640 --> 00:37:24,560


1115
00:37:24,560 --> 00:37:26,320


1116
00:37:26,320 --> 00:37:27,599


1117
00:37:27,599 --> 00:37:29,200
вроде последней серии

1118
00:37:29,200 --> 00:37:31,280
экспериментов, которые мы провели, попробуйте ответить на

1119
00:37:31,280 --> 00:37:33,200
следующий вопрос: допустим,

1120
00:37:33,200 --> 00:37:34,880
кто-то приходит и внезапно

1121
00:37:34,880 --> 00:37:36,960
дает вам в четыре раза больше вычислений,

1122
00:37:36,960 --> 00:37:38,800
что вы должны с этим делать, и поэтому

1123
00:37:38,800 --> 00:37:40,240
есть ряд вещей, которые вы могли бы сделать  вы

1124
00:37:40,240 --> 00:37:41,760
можете увеличить количество

1125
00:37:41,760 --> 00:37:43,680
шагов обучения в четыре раза вы можете

1126
00:37:43,680 --> 00:37:45,280
увеличить размер пакета в

1127
00:37:45,280 --> 00:37:47,280
четыре раза вы можете сделать свою модель в два раза

1128
00:37:47,280 --> 00:37:49,040
больше и тренироваться в два раза дольше вы можете

1129
00:37:49,040 --> 00:37:51,040
сделать свою модель в четыре раза больше, чем вы

1130
00:37:51,040 --> 00:37:53,599
могли  обучите четыре модели по отдельности и

1131
00:37:53,599 --> 00:37:55,920
объедините их, или вы могли бы сделать последнюю

1132
00:37:55,920 --> 00:37:57,200
вещь, которая на самом деле не требует в четыре

1133
00:37:57,200 --> 00:37:58,880
раза больше вычислений, когда вы

1134
00:37:58,880 --> 00:38:01,200
предварительно обучаете одну модель и настраиваете ее

1135
00:38:01,200 --> 00:38:02,880
четыре раза по отдельности, а затем объединяете

1136
00:38:02,880 --> 00:38:06,320
их и основной вывод здесь  заключается в том, что

1137
00:38:06,320 --> 00:38:08,720
масштабирование помогает, это, как вы знаете, очень

1138
00:38:08,720 --> 00:38:11,839
неудивительно, особенно в 2021 году,

1139
00:38:11,839 --> 00:38:13,119
но, что

1140
00:38:13,119 --> 00:38:15,599
интересно, вы получите значительный

1141
00:38:15,599 --> 00:38:16,560
выигрыш

1142
00:38:16,560 --> 00:38:18,720
, если просто увеличите время обучения

1143
00:38:18,720 --> 00:38:21,119
или увеличите размер,

1144
00:38:21,119 --> 00:38:23,680
чтобы вы могли видеть, что по обеим этим

1145
00:38:23,680 --> 00:38:25,280
осям мы получаем значительные улучшения производительности,

1146
00:38:25,280 --> 00:38:26,640
хотя

1147
00:38:26,640 --> 00:38:28,160
Улучшение производительности становится более значительным, когда мы

1148
00:38:28,160 --> 00:38:30,240
увеличиваем размер, и, в частности, вы

1149
00:38:30,240 --> 00:38:32,400
можете видеть, что мы перешли с

1150
00:38:32,400 --> 00:38:35,359
71 балла на суперклей до 78, просто

1151
00:38:35,359 --> 00:38:38,880
увеличив модель в четыре раза.

1152
00:38:38,880 --> 00:38:40,640


1153
00:38:40,640 --> 00:38:42,880
повторить все это, а затем использовать это

1154
00:38:42,880 --> 00:38:44,720
резюме, чтобы объяснить дизайнерские решения,

1155
00:38:44,720 --> 00:38:46,560
которые вошли в окончательный вид моделей t5.

1156
00:38:46,560 --> 00:38:47,599


1157
00:38:47,599 --> 00:38:49,599
Первое, что мы собираемся

1158
00:38:49,599 --> 00:38:51,280
выбрать  архитектура декодера кодера,

1159
00:38:51,280 --> 00:38:52,800
потому что это, казалось, лучше всего работает в нашем

1160
00:38:52,800 --> 00:38:54,400
формате текста в текст,

1161
00:38:54,400 --> 00:38:55,520
следующее: мы собираемся

1162
00:38:55,520 --> 00:38:57,440
использовать цель прогнозирования диапазона, которая

1163
00:38:57,440 --> 00:38:58,800
в конечном итоге очень похожа на

1164
00:38:58,800 --> 00:39:00,320
цель базовой линии, которую я описал

1165
00:39:00,320 --> 00:39:01,359
ранее,

1166
00:39:01,359 --> 00:39:03,440
мы будем использовать c4  набор данных, потому что он

1167
00:39:03,440 --> 00:39:05,760
действительно достиг разумной производительности, но

1168
00:39:05,760 --> 00:39:07,359
был достаточно большим, чтобы нам не нужно было

1169
00:39:07,359 --> 00:39:08,480
беспокоиться о

1170
00:39:08,480 --> 00:39:09,680
повторении данных и обнаружении

1171
00:39:09,680 --> 00:39:11,200
пагубного переобучения во время

1172
00:39:11,200 --> 00:39:12,640
предварительного обучения,

1173
00:39:12,640 --> 00:39:14,079
когда мы увеличиваем количество

1174
00:39:14,079 --> 00:39:16,400
этапов предварительного обучения, которые мы фактически

1175
00:39:16,400 --> 00:39:18,160
решили выполнить несколько раз.  предварительное обучение задач, потому что мы

1176
00:39:18,160 --> 00:39:19,760
будем увеличивать объем

1177
00:39:19,760 --> 00:39:21,839
предварительной подготовки, наши самые длительные тренировки

1178
00:39:21,839 --> 00:39:23,359
заняли около месяца, и мы хотели иметь

1179
00:39:23,359 --> 00:39:24,800
возможность отслеживать производительность в

1180
00:39:24,800 --> 00:39:26,400
ходе предварительной подготовки, не выполняя

1181
00:39:26,400 --> 00:39:28,320
тонкую настройку, поэтому мы собираемся  выполнить

1182
00:39:28,320 --> 00:39:30,160
это предварительное многозадачное обучение с последующей

1183
00:39:30,160 --> 00:39:32,000
тонкой настройкой, а затем,

1184
00:39:32,000 --> 00:39:33,520
конечно же, мы собираемся обучать более крупные

1185
00:39:33,520 --> 00:39:35,520
модели на более длительный срок

1186
00:39:35,520 --> 00:39:37,680
и, в частности, размеры моделей, которые мы в

1187
00:39:37,680 --> 00:39:38,880
итоге

1188
00:39:38,880 --> 00:39:41,599
изменили  лизинг мы называем малый бас большой 3b

1189
00:39:41,599 --> 00:39:43,040
и

1190
00:39:43,040 --> 00:39:44,800
11b малая модель имеет 60 миллионов

1191
00:39:44,800 --> 00:39:46,720
параметров, она примерно на четверть

1192
00:39:46,720 --> 00:39:48,960
меньше нашей базовой линии, которая снова была

1193
00:39:48,960 --> 00:39:51,760
кодировщиком размера на основе рождения декодером базового размера мы

1194
00:39:51,760 --> 00:39:53,520
также обучили модель, которая была

1195
00:39:53,520 --> 00:39:56,480
кодировщиком большого размера декодером большого размера  а затем

1196
00:39:56,480 --> 00:39:59,040
мы создали два больших варианта, просто

1197
00:39:59,040 --> 00:40:01,040
увеличив размер трансформатора с прямой связью

1198
00:40:01,040 --> 00:40:03,280
и количество

1199
00:40:03,280 --> 00:40:05,119
головок внимания в трансформаторе, как вы

1200
00:40:05,119 --> 00:40:06,720
можете видеть, наша самая большая модель на самом деле имела

1201
00:40:06,720 --> 00:40:10,319
скрытую размерность 65 000 мкм

1202
00:40:10,319 --> 00:40:12,240
в слоях с прямой связью.  то, что

1203
00:40:12,240 --> 00:40:13,839
мы сделали такой необычный способ

1204
00:40:13,839 --> 00:40:15,359
увеличения количества параметров, просто

1205
00:40:15,359 --> 00:40:16,240
потому,

1206
00:40:16,240 --> 00:40:17,119


1207
00:40:17,119 --> 00:40:18,400
что слои с прямой связью - это просто

1208
00:40:18,400 --> 00:40:20,560
гигантские матричные умножения, и

1209
00:40:20,560 --> 00:40:22,079
это лучший способ использовать аппаратные

1210
00:40:22,079 --> 00:40:24,000
ускорители.

1211
00:40:24,000 --> 00:40:24,960


1212
00:40:24,960 --> 00:40:27,200


1213
00:40:27,200 --> 00:40:29,920
спрашивать,

1214
00:40:29,920 --> 00:40:30,800


1215
00:40:30,800 --> 00:40:32,800
как вы выполняли

1216
00:40:32,800 --> 00:40:36,000
многозадачное обучение, так что

1217
00:40:36,000 --> 00:40:37,119


1218
00:40:37,119 --> 00:40:39,920


1219
00:40:39,920 --> 00:40:41,680


1220
00:40:41,680 --> 00:40:43,760
в нашем случае просто приклеили простой классификатор soft max для каждой задачи или около того, потому что  так как мы используем этот

1221
00:40:43,760 --> 00:40:46,400
формат текста в текст, в основном мы

1222
00:40:46,400 --> 00:40:48,800
обучаем именно этому, это точно такая

1223
00:40:48,800 --> 00:40:50,640
же модель, нет новых классификационных заголовков

1224
00:40:50,640 --> 00:40:52,240
для каждой задачи, единственная разница в том,

1225
00:40:52,240 --> 00:40:54,640
что каждая задача получает свой собственный префикс задачи,

1226
00:40:54,640 --> 00:40:56,000
поэтому, если вы все помните  вернувшись

1227
00:40:56,000 --> 00:40:57,520
в начало, мы говорим, что вы знаете, что переводите с

1228
00:40:57,520 --> 00:40:59,839
английского на немецкий, английское предложение с двоеточием,

1229
00:40:59,839 --> 00:41:02,400
или вы знаете, что суммируете английский

1230
00:41:02,400 --> 00:41:04,079
абзац с двоеточием, и это говорит модели, что

1231
00:41:04,079 --> 00:41:05,839
она должна делать, а затем вы просто тренируете

1232
00:41:05,839 --> 00:41:07,119
модель, чтобы предсказать соответствующую

1233
00:41:07,119 --> 00:41:09,520
цель,

1234
00:41:11,599 --> 00:41:14,560
так что последний  уместная деталь

1235
00:41:14,560 --> 00:41:17,040
опять же заключается в том, что мы увеличили

1236
00:41:17,040 --> 00:41:18,960
объем предварительной подготовки, которую мы закончили предварительной подготовкой

1237
00:41:18,960 --> 00:41:20,960
на триллионе токенов данных, а не на

1238
00:41:20,960 --> 00:41:23,520
34 миллиардах токенов, так что это, э-э,

1239
00:41:23,520 --> 00:41:25,359
это намного больше предварительной подготовки, хотя

1240
00:41:25,359 --> 00:41:27,040
она еще менее предварительная.  обучение, которое

1241
00:41:27,040 --> 00:41:29,040
использовалось в Excel net, я думаю, в два раза,

1242
00:41:29,040 --> 00:41:32,560
если я правильно помню,

1243
00:41:32,640 --> 00:41:34,400
так что вот результаты, которые

1244
00:41:34,400 --> 00:41:35,760
и это были своего рода тем, как

1245
00:41:35,760 --> 00:41:36,960
все было в то время, когда

1246
00:41:36,960 --> 00:41:39,280
мы выпустили статью.  Мы

1247
00:41:39,280 --> 00:41:41,839
получили самые современные результаты

1248
00:41:41,839 --> 00:41:44,960
в тесте glue meta Benchmark cnn daily mail,

1249
00:41:44,960 --> 00:41:46,960


1250
00:41:46,960 --> 00:41:48,640
и мы были очень

1251
00:41:48,640 --> 00:41:50,640
взволнованы, увидев, насколько хорошо мы справились с

1252
00:41:50,640 --> 00:41:51,920
суперклеем.

1253
00:41:51,920 --> 00:41:53,520


1254
00:41:53,520 --> 00:41:56,000
89.8,

1255
00:41:56,000 --> 00:41:58,079
и мы работали значительно лучше,

1256
00:41:58,079 --> 00:41:59,440
чем

1257
00:41:59,440 --> 00:42:01,680
суперклей roberta, оказалось, что это тест,

1258
00:42:01,680 --> 00:42:03,920
который сильно выигрывает от больших моделей,

1259
00:42:03,920 --> 00:42:06,319
и поэтому вы действительно можете увидеть резкое

1260
00:42:06,319 --> 00:42:07,839
увеличение производительности модели, когда

1261
00:42:07,839 --> 00:42:10,319
мы масштабируем модель,

1262
00:42:10,319 --> 00:42:12,240
с другой стороны, мы не получили

1263
00:42:12,240 --> 00:42:13,839
состояние  - самые современные результаты по любому из

1264
00:42:13,839 --> 00:42:15,599
наборов данных перевода, и причина, по

1265
00:42:15,599 --> 00:42:17,040
которой мы думаем, что это правда, заключается в том,

1266
00:42:17,040 --> 00:42:18,640
что все современные

1267
00:42:18,640 --> 00:42:20,319
результаты в то время для этих

1268
00:42:20,319 --> 00:42:22,880
наборов данных перевода использовали обратный перевод, и если

1269
00:42:22,880 --> 00:42:24,880
Вы помните, что

1270
00:42:24,880 --> 00:42:28,160
в нашей модели мы проводили предварительное обучение только английскому языку, и мы ожидаем,

1271
00:42:28,160 --> 00:42:30,800
что с точки зрения использования

1272
00:42:30,800 --> 00:42:32,960
немаркированных данных более эффективно

1273
00:42:32,960 --> 00:42:34,480
использовать обратный перевод для

1274
00:42:34,480 --> 00:42:36,079
задач машинного перевода.  Если мы использовали этот

1275
00:42:36,079 --> 00:42:39,040
английский только для предварительной тренировки,

1276
00:42:39,040 --> 00:42:40,640
я должен упомянуть, конечно, что эти

1277
00:42:40,640 --> 00:42:42,640
результаты сейчас немного устарели, и некоторые

1278
00:42:42,640 --> 00:42:44,480
из этих результатов были

1279
00:42:44,480 --> 00:42:47,599
побиты последующими моделями,

1280
00:42:47,920 --> 00:42:48,720
поэтому

1281
00:42:48,720 --> 00:42:50,319
теперь я просто быстро сделаю пробку, которая  вы

1282
00:42:50,319 --> 00:42:51,680
знаете, что весь наш код выпущен, наши

1283
00:42:51,680 --> 00:42:53,599
предварительно обученные модели были выпущены,

1284
00:42:53,599 --> 00:42:54,880
вы можете использовать их в нашей кодовой

1285
00:42:54,880 --> 00:42:56,560
базе, они также, конечно, в

1286
00:42:56,560 --> 00:42:59,280
кодовой базе обнимающих лиц трансформеров,

1287
00:42:59,280 --> 00:43:02,319
мы сделали совместную работу в то время, которая показывает

1288
00:43:02,319 --> 00:43:03,599


1289
00:43:03,599 --> 00:43:06,160
довольно простая демонстрация того, как взять одну

1290
00:43:06,160 --> 00:43:08,160
из наших предварительно обученных моделей и в основном

1291
00:43:08,160 --> 00:43:11,680
обучить ее на tsv-файле входных данных и

1292
00:43:11,680 --> 00:43:13,839
целей, поэтому, поскольку все проблемы являются

1293
00:43:13,839 --> 00:43:15,200
проблемами преобразования текста в текст, вам просто нужно

1294
00:43:15,200 --> 00:43:17,119
дать модели некоторый входной текст и  какой-то

1295
00:43:17,119 --> 00:43:19,040
целевой текст, и это все, что вам нужно для

1296
00:43:19,040 --> 00:43:20,720
точной настройки модели

1297
00:43:20,720 --> 00:43:22,160
, и вы можете сделать так, чтобы вы могли действительно

1298
00:43:22,160 --> 00:43:23,680
точно настроить модель с тремя миллиардами

1299
00:43:23,680 --> 00:43:26,720
параметров на бесплатном совместном tpu,

1300
00:43:26,720 --> 00:43:30,640
используя ссылку внизу здесь

1301
00:43:30,880 --> 00:43:33,040
отлично, пока что мы '  я говорил

1302
00:43:33,040 --> 00:43:34,640
о предварительной тренировке только на английском  Я

1303
00:43:34,640 --> 00:43:36,480
имею в виду, что мы применили ее к машинному

1304
00:43:36,480 --> 00:43:38,800
переводу и последующим задачам, поэтому

1305
00:43:38,800 --> 00:43:40,319
возникает естественный вопрос: вы знаете,

1306
00:43:40,319 --> 00:43:42,720
что насчет всех других языков,

1307
00:43:42,720 --> 00:43:45,200
почему бы не обучить многоязычную модель, и

1308
00:43:45,200 --> 00:43:46,720
это то, что мы сделали

1309
00:43:46,720 --> 00:43:48,240
совсем недавно, и на самом деле позволили мне просто  пауза,

1310
00:43:48,240 --> 00:43:49,760
потому что я видел пару вопросов,

1311
00:43:49,760 --> 00:43:51,599
которые выходят, я хочу убедиться, что

1312
00:43:51,599 --> 00:43:53,920
я никого не оставлю, когда я перейду

1313
00:43:53,920 --> 00:43:55,359
к следующему

1314
00:43:55,359 --> 00:43:57,680
разделу,

1315
00:43:59,119 --> 00:44:00,720
конечно, поэтому

1316
00:44:00,720 --> 00:44:03,359
один из вопросов касается

1317
00:44:03,359 --> 00:44:05,920
настройки многозадачности, если вы

1318
00:44:05,920 --> 00:44:07,760
включаете неизвестную задачу  prefix делает

1319
00:44:07,760 --> 00:44:09,520
что-то интересное, и если вы

1320
00:44:09,520 --> 00:44:11,440
не включаете префикс, что случилось, что

1321
00:44:11,440 --> 00:44:13,040
это делает,

1322
00:44:13,040 --> 00:44:14,400


1323
00:44:14,400 --> 00:44:17,599
если вы включаете префикс неизвестной задачи

1324
00:44:17,599 --> 00:44:19,520
или если вы вообще не включаете префикс,

1325
00:44:19,520 --> 00:44:21,839
что он, вероятно, будет делать  это применить

1326
00:44:21,839 --> 00:44:24,079
неконтролируемую цель, потому что мы на

1327
00:44:24,079 --> 00:44:26,079
самом деле не использовали префикс задачи

1328
00:44:26,079 --> 00:44:29,200
для неконтролируемой цели, ну,

1329
00:44:29,200 --> 00:44:31,839
я думаю, я должен сказать,

1330
00:44:31,839 --> 00:44:32,560
что

1331
00:44:32,560 --> 00:44:34,079
это не совсем так, это не совсем так,

1332
00:44:34,079 --> 00:44:35,599
потому что в inpu не будет никаких

1333
00:44:35,599 --> 00:44:36,480
маркеров

1334
00:44:36,480 --> 00:44:38,880
дозорных  Так что мы на самом деле видим, что

1335
00:44:38,880 --> 00:44:40,800
это обычно происходит, так это то, что он выводит какие-

1336
00:44:40,800 --> 00:44:42,240
то связанные слова и некоторые другие

1337
00:44:42,240 --> 00:44:45,200
маркеры дозорных и тарабарщину, эм,

1338
00:44:45,200 --> 00:44:47,200
это не очень полезно, я думаю, в

1339
00:44:47,200 --> 00:44:49,040
результате

1340
00:44:49,040 --> 00:44:50,960
у нас есть вопросы по обратному

1341
00:44:50,960 --> 00:44:52,160
переводу, я не думаю, что они  слышал

1342
00:44:52,160 --> 00:44:53,280
об обратном переводе в остальной

1343
00:44:53,280 --> 00:44:55,200
части курса,

1344
00:44:55,200 --> 00:44:56,560
поэтому обратный перевод - довольно

1345
00:44:56,560 --> 00:44:58,240
простой метод, основная идея заключается в

1346
00:44:58,240 --> 00:45:00,160
том, что если у меня есть немаркированные текстовые данные на

1347
00:45:00,160 --> 00:45:02,640
одном языке, я использую свою текущую модель для

1348
00:45:02,640 --> 00:45:04,400
перевода этих данных

1349
00:45:04,400 --> 00:45:05,359
на

1350
00:45:05,359 --> 00:45:08,640
какой-то конкретный язык, и я использую

1351
00:45:08,640 --> 00:45:11,200
это  как обучающие данные, то впоследствии

1352
00:45:11,200 --> 00:45:13,119
для моей модели это похоже на самообучение,

1353
00:45:13,119 --> 00:45:14,640
если вы знакомы с этим, в

1354
00:45:14,640 --> 00:45:16,079
основном вы делаете прогнозы на

1355
00:45:16,079 --> 00:45:17,680
немаркированных данных, а затем используете эти

1356
00:45:17,680 --> 00:45:19,839
прогнозы для обучения модели,

1357
00:45:19,839 --> 00:45:22,160
это оказывается полезным

1358
00:45:22,160 --> 00:45:24,319
ага, а затем один э-э  один

1359
00:45:24,319 --> 00:45:25,440


1360
00:45:25,440 --> 00:45:27,359
подробный вопрос о максимальной длине ввода

1361
00:45:27,359 --> 00:45:28,800
и максимальной длине вывода, как вы

1362
00:45:28,800 --> 00:45:30,480
их выбрали, вы тоже изучали это

1363
00:45:30,480 --> 00:45:31,359


1364
00:45:31,359 --> 00:45:33,359
да, поэтому большинство задач, которые мы рассматривали

1365
00:45:33,359 --> 00:45:35,280
, не были  ave входная длина

1366
00:45:35,280 --> 00:45:37,599
значительно превышает 512 токенов

1367
00:45:37,599 --> 00:45:39,520
большую часть времени с использованием

1368
00:45:39,520 --> 00:45:42,960
стратегии токенизации, которую мы использовали,

1369
00:45:42,960 --> 00:45:45,359
и поэтому мы использовали максимальную длину ввода

1370
00:45:45,359 --> 00:45:46,400
512,

1371
00:45:46,400 --> 00:45:48,400
но мы использовали схему кодирования позиции,

1372
00:45:48,400 --> 00:45:50,880
которая допускает произвольную длину ввода, и

1373
00:45:50,880 --> 00:45:53,280
у нас действительно есть uh  в последующей работе

1374
00:45:53,280 --> 00:45:57,040
доработал t5 для последовательностей

1375
00:45:57,040 --> 00:45:59,440
длины 2048. кроме этого вы начинаете

1376
00:45:59,440 --> 00:46:01,599
попадать в проблемы с памятью из-за

1377
00:46:01,599 --> 00:46:03,520
квадратичной сложности памяти, внимание,

1378
00:46:03,520 --> 00:46:06,079
но, в принципе, вы

1379
00:46:06,079 --> 00:46:08,400
можете применить его к длинным последовательностям,

1380
00:46:08,400 --> 00:46:09,680
а затем, возможно, к одному  последнее, что, если у вас

1381
00:46:09,680 --> 00:46:11,599
есть секунда, - это я

1382
00:46:11,599 --> 00:46:12,880
снова говорю о том, что

1383
00:46:12,880 --> 00:46:16,160
перевод снова кажется

1384
00:46:16,160 --> 00:46:18,480
не лучшим приложением для этого, и

1385
00:46:18,480 --> 00:46:19,440


1386
00:46:19,440 --> 00:46:21,200
какова ваша интуиция относительно того, почему

1387
00:46:21,200 --> 00:46:22,960
такие переводы делают это,

1388
00:46:22,960 --> 00:46:24,400
но не получают выгоды от бесплатного

1389
00:46:24,400 --> 00:46:26,560
обучения тому, что  Причина, по которой

1390
00:46:26,560 --> 00:46:28,960
я не знаю, я действительно не уверен, я

1391
00:46:28,960 --> 00:46:30,880


1392
00:46:30,880 --> 00:46:31,839


1393
00:46:31,839 --> 00:46:34,800
могу только догадываться, я думаю,

1394
00:46:34,800 --> 00:46:37,119
что предварительное обучение помогает модели

1395
00:46:37,119 --> 00:46:38,640
узнать значение слов,

1396
00:46:38,640 --> 00:46:40,319
это помогает модели выучить некоторые вещи  ld

1397
00:46:40,319 --> 00:46:41,760
знания, о которых я расскажу

1398
00:46:41,760 --> 00:46:43,200
немного позже, и это очень

1399
00:46:43,200 --> 00:46:44,480
расплывчатая концепция,

1400
00:46:44,480 --> 00:46:45,359


1401
00:46:45,359 --> 00:46:48,000
я думаю, что для изучения перевода

1402
00:46:48,000 --> 00:46:50,319
мировые знания не очень полезны,

1403
00:46:50,319 --> 00:46:52,240
потому что все знания

1404
00:46:52,240 --> 00:46:55,040
, необходимые для перевода предложения,

1405
00:46:55,040 --> 00:46:58,560
по большей части  находится в предложении,

1406
00:46:58,560 --> 00:46:59,520
и

1407
00:46:59,520 --> 00:47:01,119
поэтому в основном вся

1408
00:47:01,119 --> 00:47:02,960
информация о стиле контекстных знаний,

1409
00:47:02,960 --> 00:47:04,560
которая вам нужна для создания немецкого

1410
00:47:04,560 --> 00:47:06,800
предложения, находится во входном предложении, поэтому

1411
00:47:06,800 --> 00:47:08,000
получение мировых знаний во время

1412
00:47:08,000 --> 00:47:10,480
предварительного обучения не очень полезно,

1413
00:47:10,480 --> 00:47:11,760
конечно, полезно знать, что означают слова

1414
00:47:11,760 --> 00:47:12,640


1415
00:47:12,640 --> 00:47:15,040
но в какой-то степени

1416
00:47:15,040 --> 00:47:16,560
это самый простой сигнал, который можно уловить

1417
00:47:16,560 --> 00:47:19,040
во время тренировки, и я полагаю,

1418
00:47:19,040 --> 00:47:21,200
что это было бы моим предположением, у меня нет никаких

1419
00:47:21,200 --> 00:47:22,839
строгих доказательств

1420
00:47:22,839 --> 00:47:25,280
этого, спасибо,

1421
00:47:25,280 --> 00:47:26,160


1422
00:47:26,160 --> 00:47:29,920
отлично, так что как будто я говорил, что мы

1423
00:47:29,920 --> 00:47:31,760
тренировали это  модель только на английском языке, и мы

1424
00:47:31,760 --> 00:47:32,800
хотели

1425
00:47:32,800 --> 00:47:35,520
устранить основной недостаток, который

1426
00:47:35,520 --> 00:47:38,000
действительно может говорить только на одном языке, поэтому мы

1427
00:47:38,000 --> 00:47:39,520
представили модель под названием mt5

1428
00:47:39,520 --> 00:47:42,079
многоязычный t5 и действительно по большей

1429
00:47:42,079 --> 00:47:44,240
части if  вы помните одну вещь о

1430
00:47:44,240 --> 00:47:46,720
mt5, это в основном то, что это точно такая

1431
00:47:46,720 --> 00:47:48,880
же модель, но обученная на многоязычном

1432
00:47:48,880 --> 00:47:51,760
корпусе, а текстовый текстовый формат такой

1433
00:47:51,760 --> 00:47:54,400
же, как вы знаете, мы подаем префиксы задач,

1434
00:47:54,400 --> 00:47:56,480
но мы можем подавать контент на

1435
00:47:56,480 --> 00:47:58,400
разных языках, и мы можем проводить

1436
00:47:58,400 --> 00:48:00,559
классификацию  задачи, которые мы можем выполнять,

1437
00:48:00,559 --> 00:48:03,040
отвечая на вопросы с помощью mt5

1438
00:48:03,040 --> 00:48:04,480
точно так же, как мы можем делать с

1439
00:48:04,480 --> 00:48:06,079
t5,

1440
00:48:06,079 --> 00:48:08,319
поэтому, как я уже сказал, уместная вещь о

1441
00:48:08,319 --> 00:48:10,720
mt5 заключалась в создании многоязычного

1442
00:48:10,720 --> 00:48:13,359
варианта c4, в целом процесс очень

1443
00:48:13,359 --> 00:48:15,920
похож на процесс, который мы использовали для c4

1444
00:48:15,920 --> 00:48:18,960
за исключением того, что он включает 101 язык,

1445
00:48:18,960 --> 00:48:21,520
который мы обнаружили с помощью детектора языков с открытым исходным кодом,

1446
00:48:21,520 --> 00:48:23,200


1447
00:48:23,200 --> 00:48:24,480
мы также

1448
00:48:24,480 --> 00:48:26,319
извлекли данные из более распространенных дампов сканирования,

1449
00:48:26,319 --> 00:48:27,680
потому что, особенно для языков с низким уровнем

1450
00:48:27,680 --> 00:48:29,280
ресурсов, было трудно получить

1451
00:48:29,280 --> 00:48:30,640
достаточно данных

1452
00:48:30,640 --> 00:48:32,880
только из одного общего дампа сканирования, и вы

1453
00:48:32,880 --> 00:48:34,240
можете увидеть список  из языков, которые мы

1454
00:48:34,240 --> 00:48:36,079
здесь включаем, в конечном итоге набор данных

1455
00:48:36,079 --> 00:48:38,000
оказался размером около 27 терабайт,

1456
00:48:38,000 --> 00:48:40,079


1457
00:48:40,079 --> 00:48:42,000
так что вот распределение

1458
00:48:42,000 --> 00:48:44,960
количества страниц в mc4  набор данных по обучению

1459
00:48:44,960 --> 00:48:46,720
для различных языков, вы можете увидеть

1460
00:48:46,720 --> 00:48:48,640
наш самый высокий ресурсный язык -

1461
00:48:48,640 --> 00:48:50,400
английский, где у нас около

1462
00:48:50,400 --> 00:48:52,160
трех миллиардов страниц с

1463
00:48:52,160 --> 00:48:54,319
общим количеством триллионов токенов. Самый низкий ресурсный

1464
00:48:54,319 --> 00:48:56,319
язык - йоруба, всего около пятидесяти

1465
00:48:56,319 --> 00:48:58,559
тысяч страниц, так что вы можете это увидеть.

1466
00:48:58,559 --> 00:48:59,839
объем данных, которые мы получили для каждого

1467
00:48:59,839 --> 00:49:01,760
языка, варьируется на много порядков

1468
00:49:01,760 --> 00:49:04,240
величины,

1469
00:49:04,240 --> 00:49:06,160
потому что общая стратегия заключается в

1470
00:49:06,160 --> 00:49:07,760
использовании такого рода температурного масштабирования

1471
00:49:07,760 --> 00:49:09,359
, о котором я упоминал ранее, когда в основном

1472
00:49:09,359 --> 00:49:11,440
вы выбираете данные на определенном

1473
00:49:11,440 --> 00:49:12,640
языке

1474
00:49:12,640 --> 00:49:15,760
, используя масштабирование  количество

1475
00:49:15,760 --> 00:49:17,520
примеров на этом языке по

1476
00:49:17,520 --> 00:49:20,800
температуре и как и я прошу прощения,

1477
00:49:20,800 --> 00:49:23,119
что здесь температура на один

1478
00:49:23,119 --> 00:49:24,720
выше, на один выше температуры,

1479
00:49:24,720 --> 00:49:26,720
которую я описал ранее, поэтому в этом

1480
00:49:26,720 --> 00:49:28,559
случае, когда температура становится все меньше и

1481
00:49:28,559 --> 00:49:30,240
меньше, вы подходите все ближе и ближе  к

1482
00:49:30,240 --> 00:49:32,240
равномерному распределению,

1483
00:49:32,240 --> 00:49:34,480
чистый эффект этого заключается в том, что при очень

1484
00:49:34,480 --> 00:49:36,240
низких температурах вы, как правило, лучше справляетесь

1485
00:49:36,240 --> 00:49:38,000
с последующими задачами с низким уровнем ресурсов

1486
00:49:38,000 --> 00:49:40,160
языки, такие как урду, но по мере того, как вы

1487
00:49:40,160 --> 00:49:41,920
увеличиваете температуру, так что вы в

1488
00:49:41,920 --> 00:49:43,119
основном делаете примеры

1489
00:49:43,119 --> 00:49:44,480
пропорционального смешивания разных

1490
00:49:44,480 --> 00:49:46,480
языков, которые вы лучше делаете на языках с высокими ресурсами,

1491
00:49:46,480 --> 00:49:48,880
таких как русский,

1492
00:49:48,880 --> 00:49:50,319
поэтому мы взяли

1493
00:49:50,319 --> 00:49:51,040


1494
00:49:51,040 --> 00:49:54,800
mt, мы взяли mc4, мы снова предварительно обучили mt5, в

1495
00:49:54,800 --> 00:49:56,720
основном все было  Мы сделали то же самое,

1496
00:49:56,720 --> 00:49:58,720
мы немного увеличили словарный запас, чтобы

1497
00:49:58,720 --> 00:50:00,559
приспособить его ко всем различным языкам,

1498
00:50:00,559 --> 00:50:02,480
но в целом объем предварительного обучения,

1499
00:50:02,480 --> 00:50:04,960
размеры моделей и т. д. в основном те

1500
00:50:04,960 --> 00:50:07,359
же, и в конечном итоге мы

1501
00:50:07,359 --> 00:50:08,800
получили самое современное решение некоторых

1502
00:50:08,800 --> 00:50:10,800
задач в экстремальном тесте.  Я

1503
00:50:10,800 --> 00:50:12,319
заметил, что мы не сообщаем о результатах для

1504
00:50:12,319 --> 00:50:14,720
некоторых из этих задач, частично

1505
00:50:14,720 --> 00:50:17,680
потому, что extreme разработан для кодировщиков предложений,

1506
00:50:17,680 --> 00:50:21,839
таких как bert uh t5 is a mt5, наши

1507
00:50:21,839 --> 00:50:24,240
модели кодировщика-декодера, которые мы не

1508
00:50:24,240 --> 00:50:26,160
экспериментировали с использованием кодировщика

1509
00:50:26,160 --> 00:50:28,319
самостоятельно, но для того, чтобы  атаковать некоторые из этих

1510
00:50:28,319 --> 00:50:29,760
проблем, например проблемы поиска предложений,

1511
00:50:29,760 --> 00:50:31,520
вам нужна модель, которая может

1512
00:50:31,520 --> 00:50:33,359
выводить единственное

1513
00:50:33,359 --> 00:50:35,920
векторное представление вашей последовательности,

1514
00:50:35,920 --> 00:50:37,920
а у нас нет этой  в t5, поэтому мы

1515
00:50:37,920 --> 00:50:41,839
не применяли его к этим задачам,

1516
00:50:42,000 --> 00:50:43,920
один интересный вывод из этой статьи,

1517
00:50:43,920 --> 00:50:45,839
который я просто кратко упомяну здесь, заключается в

1518
00:50:45,839 --> 00:50:46,640
том, что

1519
00:50:46,640 --> 00:50:48,400
в

1520
00:50:48,400 --> 00:50:49,920
основном есть

1521
00:50:49,920 --> 00:50:51,920
несколько настроек, которые люди считают

1522
00:50:51,920 --> 00:50:54,640
многоязычными тестами, а один -

1523
00:50:54,640 --> 00:50:56,800
случай, когда ноль  выстрел, и в этом случае

1524
00:50:56,800 --> 00:50:59,040
вы не проводите никакого предварительного обучения на

1525
00:50:59,040 --> 00:51:01,520
языке, извините, у вас нет

1526
00:51:01,520 --> 00:51:03,680
данных для точной настройки на каждом конкретном

1527
00:51:03,680 --> 00:51:05,359
языке, у вас есть только данные предварительного обучения

1528
00:51:05,359 --> 00:51:07,200
на этих языках, поэтому вы можете выполнить точную настройку

1529
00:51:07,200 --> 00:51:09,119
допустим, только на английском языке, а затем вы

1530
00:51:09,119 --> 00:51:10,720
вводите в модель какой-то текст на другом

1531
00:51:10,720 --> 00:51:12,160
языке и смотрите, дает ли она

1532
00:51:12,160 --> 00:51:13,599
правильные прогнозы,

1533
00:51:13,599 --> 00:51:15,440
следующая настройка - это настройка поезда перевода

1534
00:51:15,440 --> 00:51:16,640
, в которой вы берете модель машинного

1535
00:51:16,640 --> 00:51:19,040
перевода и переводите данные

1536
00:51:19,040 --> 00:51:21,040
в тонкой настройке на английский  корпус на

1537
00:51:21,040 --> 00:51:22,960
разные языки, а затем последняя

1538
00:51:22,960 --> 00:51:24,800
настройка - это настройка многозадачности на языке

1539
00:51:24,800 --> 00:51:26,640
, это настройка, при которой вы

1540
00:51:26,640 --> 00:51:28,240
предполагаете, что у вас есть

1541
00:51:28,240 --> 00:51:30,720
базовые данные золотого стандарта на каждом языке, на котором

1542
00:51:30,720 --> 00:51:32,400
вы хотите использовать модель.  чтобы иметь возможность

1543
00:51:32,400 --> 00:51:33,920
обрабатывать данные,

1544
00:51:33,920 --> 00:51:36,559
и вывод здесь на самом деле заключается в том, что

1545
00:51:36,559 --> 00:51:37,920


1546
00:51:37,920 --> 00:51:40,079
разница в производительности между небольшими

1547
00:51:40,079 --> 00:51:42,559
моделями и нашей самой большой моделью, когда мы идем

1548
00:51:42,559 --> 00:51:46,240
по оси x, намного больше

1549
00:51:46,240 --> 00:51:48,079
для нулевого выстрела и трансляции

1550
00:51:48,079 --> 00:51:49,839
настроек поезда, чем для  настройка многозадачности на языке,

1551
00:51:49,839 --> 00:51:52,000
так что

1552
00:51:52,000 --> 00:51:55,440
в основном это говорит нам о том, что модель

1553
00:51:55,440 --> 00:51:56,480


1554
00:51:56,480 --> 00:51:58,880
изучает гораздо более широкое распределение языков,

1555
00:51:58,880 --> 00:52:00,480
если у нее гораздо большее количество

1556
00:52:00,480 --> 00:52:01,839
параметров,

1557
00:52:01,839 --> 00:52:04,160
и она может выполнять такие

1558
00:52:04,160 --> 00:52:07,119
задачи, как обучение с нулевым выстрелом, многоязычное

1559
00:52:07,119 --> 00:52:09,280
обучение задачам  Намного лучше, когда у него

1560
00:52:09,280 --> 00:52:12,000
больше параметров,

1561
00:52:12,559 --> 00:52:14,480
поэтому вы знаете, что

1562
00:52:14,480 --> 00:52:16,480
более крупные модели могут соответствовать большему количеству

1563
00:52:16,480 --> 00:52:18,480
знаний о большем количестве языков,

1564
00:52:18,480 --> 00:52:20,160
у нас был другой документ, в котором мы в основном

1565
00:52:20,160 --> 00:52:22,000
пытались ответить на вопрос, который вы знаете,

1566
00:52:22,000 --> 00:52:23,920
сколько и какие знания

1567
00:52:23,920 --> 00:52:26,960
выбирает модель  во время предварительного обучения,

1568
00:52:26,960 --> 00:52:30,240
и поэтому, чтобы ответить на этот вопрос, мы в

1569
00:52:30,240 --> 00:52:32,079
основном ввели новый вариант задачи

1570
00:52:32,079 --> 00:52:34,000
с ответом на вопрос, поэтому задача с ответом на

1571
00:52:34,000 --> 00:52:36,000
вопрос  f поставляется

1572
00:52:36,000 --> 00:52:37,680
в нескольких различных вариантах, самый

1573
00:52:37,680 --> 00:52:39,040
простой вариант, о котором я уже упоминал,

1574
00:52:39,040 --> 00:52:41,280
- понимание прочитанного, и в

1575
00:52:41,280 --> 00:52:43,359
этом случае модели в основном дается

1576
00:52:43,359 --> 00:52:45,359
абзац или статья, а затем

1577
00:52:45,359 --> 00:52:47,040
задается вопрос об абзаце или

1578
00:52:47,040 --> 00:52:49,599
статье, и она должна в основном

1579
00:52:49,599 --> 00:52:51,680
извлеките ответ, чтобы вы могли видеть, спрашивают ли его,

1580
00:52:51,680 --> 00:52:53,280
какого цвета лимон, он должен

1581
00:52:53,280 --> 00:52:56,000
посмотреть в э-э в абзаце,

1582
00:52:56,000 --> 00:52:58,240
который он видел, и увидеть, что у

1583
00:52:58,240 --> 00:52:59,920
лимона есть желтый фрукт, и выведите

1584
00:52:59,920 --> 00:53:01,520
слово желтый, так что это любезно  из

1585
00:53:01,520 --> 00:53:03,040
простейшей формы задачи ответа на вопрос

1586
00:53:03,040 --> 00:53:04,000


1587
00:53:04,000 --> 00:53:05,680
более сложная форма - это то, что люди

1588
00:53:05,680 --> 00:53:07,760
называют ответом на вопрос открытой области, и

1589
00:53:07,760 --> 00:53:09,200
в этом случае вы предполагаете, что модели

1590
00:53:09,200 --> 00:53:11,440
задается вопрос и имеет доступ к

1591
00:53:11,440 --> 00:53:13,839
большой внешней базе данных знаний,

1592
00:53:13,839 --> 00:53:16,480
возможно, всей Википедии, поэтому  модель

1593
00:53:16,480 --> 00:53:18,880
должна сделать две вещи: сначала она должна

1594
00:53:18,880 --> 00:53:21,359
найти статью или фрагмент текста, который

1595
00:53:21,359 --> 00:53:22,880
содержит ответ

1596
00:53:22,880 --> 00:53:24,880
в базе данных, а затем

1597
00:53:24,880 --> 00:53:27,760
извлечь ответ из статьи и

1598
00:53:27,760 --> 00:53:29,359
так далее.  Этот дополнительный шаг поиска,

1599
00:53:29,359 --> 00:53:30,720
который немного усложняет задачу,

1600
00:53:30,720 --> 00:53:31,760


1601
00:53:31,760 --> 00:53:33,599
но мы представили своего рода третий

1602
00:53:33,599 --> 00:53:34,880
вариант ответа на вопрос, который мы

1603
00:53:34,880 --> 00:53:36,880
называем вопросом закрытой книги. Ответ на это

1604
00:53:36,880 --> 00:53:38,720
имя черпает вдохновение из закрытых книжных

1605
00:53:38,720 --> 00:53:39,920
экзаменов.

1606
00:53:39,920 --> 00:53:42,000


1607
00:53:42,000 --> 00:53:43,839
вопрос у него нет

1608
00:53:43,839 --> 00:53:45,440
доступа к внешнему источнику знаний

1609
00:53:45,440 --> 00:53:47,599
он не может искать информацию где-либо

1610
00:53:47,599 --> 00:53:49,680
он может ответить на вопрос только на

1611
00:53:49,680 --> 00:53:51,200
основе знаний, полученных

1612
00:53:51,200 --> 00:53:52,960
во время предварительного обучения, поэтому, если вы зададите

1613
00:53:52,960 --> 00:53:55,200
модели вопрос, какой цвет у нее лимон

1614
00:53:55,200 --> 00:53:56,640
правильно вывести модель желтым,

1615
00:53:56,640 --> 00:53:58,960
потому что она,

1616
00:53:58,960 --> 00:54:01,040
так сказать, знает, что лимоны

1617
00:54:01,040 --> 00:54:02,079
желтые,

1618
00:54:02,079 --> 00:54:04,079
так что это хороший способ аргументировать

1619
00:54:04,079 --> 00:54:06,480
проверку знаний количество

1620
00:54:06,480 --> 00:54:08,480
знаний, хранящихся в модели во время

1621
00:54:08,480 --> 00:54:10,880
предварительного обучения,

1622
00:54:10,880 --> 00:54:12,880
так почему мы можем ожидать, что это сработает?  ну,

1623
00:54:12,880 --> 00:54:14,400
вы можете себе представить, что мы

1624
00:54:14,400 --> 00:54:17,119
делаем нашу обычную предварительную подготовку к t5,

1625
00:54:17,119 --> 00:54:18,640
мы маскируем слова и обучаем

1626
00:54:18,640 --> 00:54:20,880
модель предсказывать массивные интервалы

1627
00:54:20,880 --> 00:54:22,160
слов

1628
00:54:22,160 --> 00:54:24,720
a  И вы можете представить, что где-то

1629
00:54:24,720 --> 00:54:26,240
во время предварительного обучения он видит предложение, в

1630
00:54:26,240 --> 00:54:28,319
котором говорится, что президент франклин бланк родился

1631
00:54:28,319 --> 00:54:31,040
пустой январь 1882 г., цель здесь

1632
00:54:31,040 --> 00:54:34,800
состоит в том, чтобы вывести д рузвельт было пустым,

1633
00:54:34,800 --> 00:54:37,040
а затем во время точной настройки мы

1634
00:54:37,040 --> 00:54:40,079
обучили модель предсказывать, когда наступит

1635
00:54:40,079 --> 00:54:42,559
год.  1882 г., когда был задан вопрос,

1636
00:54:42,559 --> 00:54:44,960
когда родился Франклин д Рузвельт, и

1637
00:54:44,960 --> 00:54:47,119
вы можете надеяться, что он как бы

1638
00:54:47,119 --> 00:54:50,880
возвращается к своей предтренировочной задаче и

1639
00:54:50,880 --> 00:54:52,480
вспоминает некоторые знания, полученные им

1640
00:54:52,480 --> 00:54:53,920
во время предтренировки, чтобы правильно ответить на

1641
00:54:53,920 --> 00:54:56,880
этот вопрос,

1642
00:54:57,040 --> 00:55:00,079
поэтому мы  взял некоторые стандартные открытые вопросы домена,

1643
00:55:00,079 --> 00:55:01,760
отвечая на данные, устанавливает естественные

1644
00:55:01,760 --> 00:55:04,799
вопросы, веб-вопросы и мелочи qa,

1645
00:55:04,799 --> 00:55:07,839
и в основном удалил весь контекст

1646
00:55:07,839 --> 00:55:10,559
и обучил нашу модель предсказывать

1647
00:55:10,559 --> 00:55:12,400
правильный ответ, когда задается какой-то

1648
00:55:12,400 --> 00:55:14,559
конкретный вопрос, а затем оценил

1649
00:55:14,559 --> 00:55:16,160
ее производительность на тестовом наборе для каждого

1650
00:55:16,160 --> 00:55:17,520
из них  задач,

1651
00:55:17,520 --> 00:55:19,440
и в этой таблице мы сравниваем

1652
00:55:19,440 --> 00:55:20,880
современные результаты для открытой

1653
00:55:20,880 --> 00:55:22,640
доменной системы. Это системы, которые

1654
00:55:22,640 --> 00:55:24,319
явно извлекают знания  уступ от

1655
00:55:24,319 --> 00:55:28,559
внешнего источника знаний по сравнению с t5,

1656
00:55:28,559 --> 00:55:29,920
когда он был обучен в этой закрытой

1657
00:55:29,920 --> 00:55:31,599
книжной настройке, вы заметите, что на

1658
00:55:31,599 --> 00:55:32,960
самом деле мы используем немного другую

1659
00:55:32,960 --> 00:55:36,079
версию t5, здесь мы используем t5.1.1,

1660
00:55:36,079 --> 00:55:37,880
уместное отличие состоит только в том, что

1661
00:55:37,880 --> 00:55:40,960
t5.  1.1 не был многозадачным предварительно обученным,

1662
00:55:40,960 --> 00:55:42,880
он был предварительно обучен с использованием

1663
00:55:42,880 --> 00:55:44,960
неконтролируемой цели, потому что мы сделали

1664
00:55:44,960 --> 00:55:46,559
это снова, потому что мы хотим

1665
00:55:46,559 --> 00:55:47,839
измерить объем знаний, которые модель

1666
00:55:47,839 --> 00:55:50,400
получила во время предварительного обучения,

1667
00:55:50,400 --> 00:55:52,160
и вы можете видеть, что мы  на самом деле вы знаете, что

1668
00:55:52,160 --> 00:55:54,000
достаточно высокая производительность, возможно,

1669
00:55:54,000 --> 00:55:57,920
достойная производительность,

1670
00:56:00,640 --> 00:56:01,760
я не хочу снова использовать производительность,

1671
00:56:01,760 --> 00:56:03,359
снова слово производительность,

1672
00:56:03,359 --> 00:56:05,280
точность в основном для каждого из этих

1673
00:56:05,280 --> 00:56:07,359
наборов данных увеличивается по мере увеличения размера модели,

1674
00:56:07,359 --> 00:56:10,079
что, возможно, в некоторой степени

1675
00:56:10,079 --> 00:56:11,599
предполагает, что  более крупные модели

1676
00:56:11,599 --> 00:56:12,880
получили больше знаний во время

1677
00:56:12,880 --> 00:56:14,960
предварительного обучения, но в конечном итоге мы

1678
00:56:14,960 --> 00:56:16,640
отстали от современных результатов для

1679
00:56:16,640 --> 00:56:18,480
систем с открытой предметной областью, которые явно

1680
00:56:18,480 --> 00:56:20,480
извлекают знания,

1681
00:56:20,480 --> 00:56:22,880
чтобы  попытаться восполнить этот пробел,

1682
00:56:22,880 --> 00:56:25,680
мы использовали эту цель, называемую

1683
00:56:25,680 --> 00:56:28,400
маскированием заметного промежутка, из статьи, называемой

1684
00:56:28,400 --> 00:56:29,839


1685
00:56:29,839 --> 00:56:32,480
предварительным обучением модели расширенного языка поиска, и маскирование промежутка плавания

1686
00:56:32,480 --> 00:56:34,319
- очень простая идея,

1687
00:56:34,319 --> 00:56:36,079
идея состоит в том, что вместо того, чтобы маскировать

1688
00:56:36,079 --> 00:56:37,920
слова наугад в вашем предварительном

1689
00:56:37,920 --> 00:56:41,040
-обучение вы фактически маскируете

1690
00:56:41,040 --> 00:56:43,520
сущности явным образом, чтобы

1691
00:56:43,520 --> 00:56:46,640
имена людей помещали даты и т. д.,

1692
00:56:46,640 --> 00:56:49,280
и вы в основном просто используете готовый с

1693
00:56:49,280 --> 00:56:51,359
именем конец вашего распознавателя, чтобы

1694
00:56:51,359 --> 00:56:52,880
выяснить, какие сущности находятся в вашем

1695
00:56:52,880 --> 00:56:55,200
наборе данных перед обучением, и вы обучаете

1696
00:56:55,200 --> 00:56:56,960
модель для заполнения  эм,

1697
00:56:56,960 --> 00:56:59,599
выступающие интервалы вместо случайных интервалов,

1698
00:56:59,599 --> 00:57:02,400
поэтому мы взяли t 5.1.1 после

1699
00:57:02,400 --> 00:57:04,960
того, как он был предварительно обучен, и продолжили

1700
00:57:04,960 --> 00:57:07,359
предварительное обучение по маскированию интервала физиологического раствора, а

1701
00:57:07,359 --> 00:57:08,720
затем измерили производительность в наших

1702
00:57:08,720 --> 00:57:10,799
последующих задачах после точной настройки, и

1703
00:57:10,799 --> 00:57:12,559
вы можете  увидим, что чем более заметным

1704
00:57:12,559 --> 00:57:14,400
мы сделали предварительную тренировку по маске диапазона,

1705
00:57:14,400 --> 00:57:16,240
тем лучше извините меня, тем лучше и

1706
00:57:16,240 --> 00:57:18,640
лучше получилась производительность, когда мы

1707
00:57:18,640 --> 00:57:20,880
отрегулировали последующие задачи и в

1708
00:57:20,880 --> 00:57:22,640
конечном итоге смогли  чтобы значительно закрыть некоторые из

1709
00:57:22,640 --> 00:57:25,040
этих пробелов и фактически

1710
00:57:25,040 --> 00:57:27,760
превзойти лучшую систему с открытым доменом

1711
00:57:27,760 --> 00:57:30,160
по веб-вопросам, добавив

1712
00:57:30,160 --> 00:57:34,559
маскировку заметности веера к uh к t5.1.1,

1713
00:57:34,559 --> 00:57:37,359
так что это просто сообщение, чтобы сказать вам,

1714
00:57:37,359 --> 00:57:40,079
что цель имеет значение, и

1715
00:57:40,079 --> 00:57:42,400
делают такие люди  в то время

1716
00:57:42,400 --> 00:57:43,760
люди не называли это так, но теперь

1717
00:57:43,760 --> 00:57:45,520
люди называют этот домен адаптивным или адаптивным к задаче

1718
00:57:45,520 --> 00:57:47,280
предварительное обучение

1719
00:57:47,280 --> 00:57:48,960
, и это хороший способ

1720
00:57:48,960 --> 00:57:50,400
повысить производительность в ваших последующих

1721
00:57:50,400 --> 00:57:52,559
задачах,

1722
00:57:52,559 --> 00:57:55,440
поэтому у меня здесь есть хороший набор вопросов.

1723
00:57:55,440 --> 00:57:58,079
если сейчас хорошее время,

1724
00:57:58,079 --> 00:57:59,680
да, это было бы здорово,

1725
00:57:59,680 --> 00:58:02,000
так что для некоторого контекста ученики в

1726
00:58:02,000 --> 00:58:03,760
их последнем задании

1727
00:58:03,760 --> 00:58:07,280
должны были эффективно создать мини-t5,

1728
00:58:07,280 --> 00:58:08,720
были единственными вопросами, которые были

1729
00:58:08,720 --> 00:58:10,319
заданы из простой предметной области, чтобы они

1730
00:58:10,319 --> 00:58:12,960
могли предварительно потренироваться  на одном графическом

1731
00:58:12,960 --> 00:58:15,200
процессоре, и один из вопросов, который у них есть, - а как

1732
00:58:15,200 --> 00:58:17,440
мы можем быть уверены, что ответ, полученный

1733
00:58:17,440 --> 00:58:19,200
от модели, не придуман,

1734
00:58:19,200 --> 00:58:20,720
им также задавали это в задании

1735
00:58:20,720 --> 00:58:23,040
, я думаю,

1736
00:58:23,040 --> 00:58:23,920


1737
00:58:23,920 --> 00:58:25,440


1738
00:58:25,440 --> 00:58:27,760
что вы, если у вас нет  доступ к

1739
00:58:27,760 --> 00:58:29,280
На самом деле очень

1740
00:58:29,280 --> 00:58:31,599
сложно понять,

1741
00:58:31,599 --> 00:58:33,680
что после этой статьи вышла хорошая статья, которая называлась:

1742
00:58:33,680 --> 00:58:35,680
как мы можем узнать, знают ли языковые модели

1743
00:58:35,680 --> 00:58:38,000
, и какова цель этой статьи -

1744
00:58:38,000 --> 00:58:40,160
сделать так, чтобы t5 был

1745
00:58:40,160 --> 00:58:42,400
тем, что мы называем хорошо откалиброванным и  когда

1746
00:58:42,400 --> 00:58:43,920
модель хорошо откалибрована, это означает, что,

1747
00:58:43,920 --> 00:58:45,920
когда она не знает ответа, она

1748
00:58:45,920 --> 00:58:47,440
не дает высоконадежных

1749
00:58:47,440 --> 00:58:49,920
прогнозов, и в этой статье были изучены

1750
00:58:49,920 --> 00:58:52,799
различные способы калибровки t5 для близких,

1751
00:58:52,799 --> 00:58:54,160
но ответных вопросов, и они в

1752
00:58:54,160 --> 00:58:55,599
конечном итоге обнаружили, что, когда модель

1753
00:58:55,599 --> 00:58:56,640
не  знаете ответ, когда он

1754
00:58:56,640 --> 00:58:59,520
выводит вы знаете что-то выдуманное,

1755
00:58:59,520 --> 00:59:01,520
что они могли бы эффективно сделать

1756
00:59:01,520 --> 00:59:02,640
его очень

1757
00:59:02,640 --> 00:59:06,400
неуверенным в своих прогнозах,

1758
00:59:06,640 --> 00:59:09,599
так что это один из способов сделать это,

1759
00:59:11,200 --> 00:59:12,960
я думаю, вы на самом деле приглушены

1760
00:59:12,960 --> 00:59:14,160
Джон

1761
00:59:14,160 --> 00:59:15,760
большое

1762
00:59:15,760 --> 00:59:17,200
спасибо, а затем еще один вопрос -

1763
00:59:17,200 --> 00:59:18,880
знание  это необходимо для выполнения

1764
00:59:18,880 --> 00:59:21,680
этой тонкой настройки, например, qa на

1765
00:59:21,680 --> 00:59:23,520
этих наборах данных для тонкой настройки,

1766
00:59:23,520 --> 00:59:24,400


1767
00:59:24,400 --> 00:59:26,480
это все знания, которые

1768
00:59:26,480 --> 00:59:28,880
присутствуют во время предварительной тренировки,

1769
00:59:28,880 --> 00:59:30,559
да, это тоже кое-что  ng,

1770
00:59:30,559 --> 00:59:32,960
который был исследован в следующей статье,

1771
00:59:32,960 --> 00:59:34,160
где было показано, что на самом деле

1772
00:59:34,160 --> 00:59:35,760
существует приличное количество перекрытий обучения и тестирования

1773
00:59:35,760 --> 00:59:37,599
с точки зрения знаний в этих

1774
00:59:37,599 --> 00:59:40,079
наборах данных, поэтому в

1775
00:59:40,079 --> 00:59:42,480
этих случаях определенно возможно, что модель

1776
00:59:42,480 --> 00:59:44,400
собирает знания во время прекрасного  настройка,

1777
00:59:44,400 --> 00:59:48,240
а не предварительная подготовка, однако,

1778
00:59:48,559 --> 00:59:51,760
в качестве побочного экспериментального примечания,

1779
00:59:51,760 --> 00:59:53,440
мы обнаруживаем,

1780
00:59:53,440 --> 00:59:57,280
что производительность t5 фактически находится на плато,

1781
00:59:57,280 --> 00:59:59,440
прежде чем он сделает один проход по

1782
00:59:59,440 --> 01:00:01,680
набору данных точной настройки, поэтому в основном

1783
01:00:01,680 --> 01:00:03,359
t5 очень очень быстро поймет,

1784
01:00:03,359 --> 01:00:04,799
что вы, черт возьми,  пытаемся заставить его это сделать,

1785
01:00:04,799 --> 01:00:06,160
ему даже не нужно видеть полный

1786
01:00:06,160 --> 01:00:08,480
обучающий набор, прежде чем он получит в основном

1787
01:00:08,480 --> 01:00:10,400
свою максимальную производительность на тестовом наборе,

1788
01:00:10,400 --> 01:00:12,400
поэтому мы на самом деле не думаем,

1789
01:00:12,400 --> 01:00:14,160
что это основной фактор для этих

1790
01:00:14,160 --> 01:00:15,200
результатов

1791
01:00:15,200 --> 01:00:17,359
отлично, а затем последний вопрос, как это получилось,

1792
01:00:17,359 --> 01:00:19,200
если вы изучили его, как многозадачная

1793
01:00:19,200 --> 01:00:20,960
модель работает ...

1794
01:00:20,960 --> 01:00:22,079


1795
01:00:22,079 --> 01:00:23,520
да, эти результаты в статье,

1796
01:00:23,520 --> 01:00:25,280
результаты почти точно такие же,

1797
01:00:25,280 --> 01:00:26,960
просто немного легче объяснить

1798
01:00:26,960 --> 01:00:29,440


1799
01:00:29,440 --> 01:00:32,960
Когда вы говорите о t5.1.1

1800
01:00:32,960 --> 01:00:35,200
в интересах экономии времени, я могу

1801
01:00:35,200 --> 01:00:37,599
пропустить эти следующие три слайда, которые

1802
01:00:37,599 --> 01:00:40,079
являются кратким изложением этих слайдов.

1803
01:00:40,079 --> 01:00:42,160
Просто процедура оценки, которую

1804
01:00:42,160 --> 01:00:44,559
мы используем, несправедливо наказывается как закрытая

1805
01:00:44,559 --> 01:00:46,400
книжные системы ответов на вопросы, если вы

1806
01:00:46,400 --> 01:00:47,680
хотите узнать немного больше о том, что вы

1807
01:00:47,680 --> 01:00:49,359
можете немного ткнуть в бумагу, на

1808
01:00:49,359 --> 01:00:50,960
самом деле это не

1809
01:00:50,960 --> 01:00:52,640
поддерживает основной момент, который я пытаюсь

1810
01:00:52,640 --> 01:00:54,400


1811
01:00:54,400 --> 01:00:56,640
выразить каким-либо значимым образом, поэтому и я хочу

1812
01:00:56,640 --> 01:00:58,880
получить  к некоторым из более свежих статей

1813
01:00:58,880 --> 01:01:00,480
, а это, и у меня должно быть время,

1814
01:01:00,480 --> 01:01:02,640
чтобы сделать это,

1815
01:01:02,640 --> 01:01:04,799
круто, поэтому мы как бы ответили на этот

1816
01:01:04,799 --> 01:01:07,440
вопрос, вы знаете, как это,

1817
01:01:07,440 --> 01:01:09,359
сколько знаний модель приобретает во время

1818
01:01:09,359 --> 01:01:11,119
предварительного обучения, ответ, возможно, является

1819
01:01:11,119 --> 01:01:11,920


1820
01:01:11,920 --> 01:01:14,000
Так что напрашивается следующий вопрос:

1821
01:01:14,000 --> 01:01:15,760
запоминает ли модель эти

1822
01:01:15,760 --> 01:01:17,440
знания правильно, но запоминает ли она также,

1823
01:01:17,440 --> 01:01:19,280
запоминают ли также большие языковые модели

1824
01:01:19,280 --> 01:01:21,440
то, что мы не хотим, чтобы

1825
01:01:21,440 --> 01:01:23,599
они запоминали конкретно, например, личные

1826
01:01:23,599 --> 01:01:25,599
данные, как вы могли бы я?  Представим,

1827
01:01:25,599 --> 01:01:28,160
что где-то в c4 находится чей-то номер социального

1828
01:01:28,160 --> 01:01:29,520
страхования,

1829
01:01:29,520 --> 01:01:31,040
мы, вероятно, не хотим, чтобы наша модель

1830
01:01:31,040 --> 01:01:33,760
запоминала его и выдавала его, когда мы

1831
01:01:33,760 --> 01:01:35,520
декодируем его,

1832
01:01:35,520 --> 01:01:37,359
и мы, конечно, не хотим, чтобы это происходило

1833
01:01:37,359 --> 01:01:39,359
для безусловных моделей, таких как gbt2 или

1834
01:01:39,359 --> 01:01:40,880
gbt3,

1835
01:01:40,880 --> 01:01:43,440
поэтому в этой следующей работе мы попытаемся ответить на

1836
01:01:43,440 --> 01:01:45,040
этот вопрос, который, как вы знаете, запоминают ли большие языковые

1837
01:01:45,040 --> 01:01:46,720
модели данные из своего

1838
01:01:46,720 --> 01:01:48,720
набора данных до обучения,

1839
01:01:48,720 --> 01:01:50,000
и

1840
01:01:50,000 --> 01:01:52,079
мы можем сначала фактически обратиться к экспертам

1841
01:01:52,079 --> 01:01:54,960
и посмотреть, что думают эксперты, и вот

1842
01:01:54,960 --> 01:01:57,920
два утверждения, сделанные eff и  открытые

1843
01:01:57,920 --> 01:02:00,319
ИИ, которые были отправлены в ведомство по патентам и

1844
01:02:00,319 --> 01:02:01,920
товарным знакам США, когда был призыв

1845
01:02:01,920 --> 01:02:03,680
дать комментарии по существу именно по

1846
01:02:03,680 --> 01:02:04,799
этому вопросу,

1847
01:02:04,799 --> 01:02:06,559
и вы можете видеть, что в обоих случаях эти,

1848
01:02:06,559 --> 01:02:09,520
эти организации в основном говорят, что вы

1849
01:02:09,520 --> 01:02:10,319
знаете,

1850
01:02:10,319 --> 01:02:12,799
что в основном нет оснований полагать,

1851
01:02:12,799 --> 01:02:15,359
что большой  языковая модель

1852
01:02:15,359 --> 01:02:17,200
будет копировать данные из своего

1853
01:02:17,200 --> 01:02:20,000
набора обучающих данных.

1854
01:02:20,000 --> 01:02:22,240


1855
01:02:22,240 --> 01:02:24,079


1856
01:02:24,079 --> 01:02:25,200
прочтите их заявление

1857
01:02:25,200 --> 01:02:26,960
немного дольше, они вроде как говорят, что вы знаете,

1858
01:02:26,960 --> 01:02:28,319
если вы построите систему

1859
01:02:28,319 --> 01:02:30,559


1860
01:02:30,559 --> 01:02:32,240


1861
01:02:32,240 --> 01:02:34,240
искусственного интеллекта надлежащим образом, система искусственного интеллекта не будет чрезмерно соответствовать набору обучающих данных, и если она не будет чрезмерной, мы не ожидаем, что они на

1862
01:02:34,240 --> 01:02:36,319
самом деле выведут свой э-э-любой из

1863
01:02:36,319 --> 01:02:38,720
их тренировочный набор любым нетривиальным

1864
01:02:38,720 --> 01:02:40,960
образом,

1865
01:02:41,839 --> 01:02:42,880
так

1866
01:02:42,880 --> 01:02:44,960
что это своего рода утверждения, которые были

1867
01:02:44,960 --> 01:02:47,280
догадками, и в этой работе мы попытались

1868
01:02:47,280 --> 01:02:49,280
более тщательно исследовать,

1869
01:02:49,280 --> 01:02:50,720
были ли они правдой,

1870
01:02:50,720 --> 01:02:51,920
и способ, которым мы это делали, в

1871
01:02:51,920 --> 01:02:53,520
основном заключался в использовании предварительно обученного

1872
01:02:53,520 --> 01:02:56,640
gpt-2  модель и вводит ей префиксы, чтобы

1873
01:02:56,640 --> 01:02:59,520
вы могли представить, что вы берете

1874
01:02:59,520 --> 01:03:01,839
причинно-следственную языковую модель, такую как gpt2, которая

1875
01:03:01,839 --> 01:03:04,000
просто автоматически предсказывает токены,

1876
01:03:04,000 --> 01:03:05,680
вы вводите ей префикс, а затем просите его в

1877
01:03:05,680 --> 01:03:07,839
основном предсказать, что будет дальше,

1878
01:03:07,839 --> 01:03:10,000
и мы показываем здесь такие странные

1879
01:03:10,000 --> 01:03:12,559
префикс East Stroudsburg Stroudsburg, но

1880
01:03:12,559 --> 01:03:14,160
мы обнаружили, что когда мы вводили этот конкретный

1881
01:03:14,160 --> 01:03:16,960
префикс в gpg2, он фактически выводил

1882
01:03:16,960 --> 01:03:20,480
дословно имя адреса, адрес электронной почты,

1883
01:03:20,480 --> 01:03:22,000
номер телефона и номер факса реального

1884
01:03:22,000 --> 01:03:24,319
человека, который появляется в интерактивном  rnet

1885
01:03:24,319 --> 01:03:26,640
этот пример на самом деле появляется только шесть

1886
01:03:26,640 --> 01:03:29,119
раз во всем общедоступном Интернете, поэтому

1887
01:03:29,119 --> 01:03:32,640
маловероятно, что gbg2 видел этот адрес

1888
01:03:32,640 --> 01:03:35,280
очень много раз, и основной смысл

1889
01:03:35,280 --> 01:03:36,799
этой работы в том, что

1890
01:03:36,799 --> 01:03:40,000
да, похоже, что gpt2 по крайней мере

1891
01:03:40,000 --> 01:03:41,599
запомнил значительное количество

1892
01:03:41,599 --> 01:03:43,599
не  тривиальная информация из его

1893
01:03:43,599 --> 01:03:45,920
набора данных до обучения,

1894
01:03:45,920 --> 01:03:48,400
поэтому, как мы проводили это исследование,

1895
01:03:48,400 --> 01:03:50,480
мы использовали эту процедуру,

1896
01:03:50,480 --> 01:03:53,200
которая показана на экране здесь,

1897
01:03:53,200 --> 01:03:55,440
мы в основном рассматриваем три различных

1898
01:03:55,440 --> 01:03:58,319
способа выборки данных из gpg2,

1899
01:03:58,319 --> 01:03:59,680
первый - это просто

1900
01:03:59,680 --> 01:04:02,000
автоагрессивная выборка из  следующее -

1901
01:04:02,000 --> 01:04:03,680
это автоагрессивная выборка, но с

1902
01:04:03,680 --> 01:04:05,680
понижающейся температурой это в основном

1903
01:04:05,680 --> 01:04:08,319
означает, что вы хотите, чтобы модель становилась

1904
01:04:08,319 --> 01:04:10,079
все более и более уверенной в своих

1905
01:04:10,079 --> 01:04:13,280
прогнозах в процессе выборки,

1906
01:04:13,280 --> 01:04:15,200
и последний вариант - взять случайный

1907
01:04:15,200 --> 01:04:17,119
текст из Интернета и  используйте это как

1908
01:04:17,119 --> 01:04:19,520
условие для gpt2, прежде чем просить его

1909
01:04:19,520 --> 01:04:21,280
сгенерировать то, что будет дальше,

1910
01:04:21,280 --> 01:04:23,839
так что теперь для каждого из этих sam для каждого из

1911
01:04:23,839 --> 01:04:26,640
этих поколений каждое из этих 200000

1912
01:04:26,640 --> 01:04:28,160
поколений  Поскольку мы сделали для каждого из этих

1913
01:04:28,160 --> 01:04:30,720
методов выборки, нам нужен

1914
01:04:30,720 --> 01:04:32,640
способ попытаться предсказать, может ли он быть

1915
01:04:32,640 --> 01:04:34,319
запомнен или нет,

1916
01:04:34,319 --> 01:04:36,079
и поэтому мы придумали шесть показателей, чтобы

1917
01:04:36,079 --> 01:04:39,280
использовать их, чтобы дать нам представление о том, может ли

1918
01:04:39,280 --> 01:04:41,359
конкретный образец из gpt2 быть

1919
01:04:41,359 --> 01:04:42,480
запомнил, что

1920
01:04:42,480 --> 01:04:44,319
все эти различные метрики в

1921
01:04:44,319 --> 01:04:46,960
основном используют недоумение gpt2 для

1922
01:04:46,960 --> 01:04:50,000
образца, недоумение, как я думаю, вы,

1923
01:04:50,000 --> 01:04:51,280
вероятно, узнали,

1924
01:04:51,280 --> 01:04:54,400
в основном является мерой того, насколько уверенно

1925
01:04:54,400 --> 01:04:56,640
gpt2 генерировал этот конкретный

1926
01:04:56,640 --> 01:04:58,559
образец,

1927
01:04:58,559 --> 01:05:00,079
вы также можете думать об этом как о мере

1928
01:05:00,079 --> 01:05:02,079
сжатия

1929
01:05:02,079 --> 01:05:03,680
так что

1930
01:05:03,680 --> 01:05:05,280
все эти показатели используют

1931
01:05:05,280 --> 01:05:06,559
недоумение,

1932
01:05:06,559 --> 01:05:08,799
либо мы просто измеряем недоумение gbg2

1933
01:05:08,799 --> 01:05:11,839
для того, что он сгенерировал, либо мы

1934
01:05:11,839 --> 01:05:15,680
вычисляем отношение недоумения gpt2

1935
01:05:15,680 --> 01:05:17,680
к недоумению для другого варианта

1936
01:05:17,680 --> 01:05:20,000
gpt, либо с библиотекой сжатия текста

1937
01:05:20,000 --> 01:05:22,000
под названием zlib, которую

1938
01:05:22,000 --> 01:05:24,319
мы также сравнивали с помощью нашей  соотносить

1939
01:05:24,319 --> 01:05:26,319
недоумение исходного образца с

1940
01:05:26,319 --> 01:05:28,319
версией образца в нижнем регистре, а

1941
01:05:28,319 --> 01:05:30,400
также недоумение с оконным управлением, когда мы

1942
01:05:30,400 --> 01:05:32,079
вычисляем недоумение только для небольшого

1943
01:05:32,079 --> 01:05:33,680
Окно выборки вместо

1944
01:05:33,680 --> 01:05:35,520
всего, что

1945
01:05:35,520 --> 01:05:36,960
мы

1946
01:05:36,960 --> 01:05:38,880
берем наверху, мы

1947
01:05:38,880 --> 01:05:40,799
делаем некоторую дедупликацию для поколения,

1948
01:05:40,799 --> 01:05:42,720
а затем выбираем 100 лучших поколений в

1949
01:05:42,720 --> 01:05:43,839
соответствии с

1950
01:05:43,839 --> 01:05:45,200
каждой из этих метрик, что в

1951
01:05:45,200 --> 01:05:47,119
конечном итоге даст нам 600 возможных

1952
01:05:47,119 --> 01:05:49,039
запомненных поколений, а затем мы

1953
01:05:49,039 --> 01:05:51,039
фактически  просто сделайте простое извинение,

1954
01:05:51,039 --> 01:05:53,280
простейший поиск в Google, чтобы увидеть, сможем ли мы

1955
01:05:53,280 --> 01:05:56,720
найти этот текст, который gpt2 сгенерировал

1956
01:05:56,720 --> 01:05:58,960
где-то в Интернете, и если он, если мы

1957
01:05:58,960 --> 01:06:00,480
действительно найдем его где-нибудь в Интернете, мы

1958
01:06:00,480 --> 01:06:03,039
спросим авторов gpg2, было ли это в

1959
01:06:03,039 --> 01:06:05,200
обучающем наборе или нет  и они проверили

1960
01:06:05,200 --> 01:06:06,640
все примеры, которые мы нашли, и

1961
01:06:06,640 --> 01:06:09,280
сообщили нам, действительно ли gbt2 выплевывает

1962
01:06:09,280 --> 01:06:12,240
что-то из своего набора обучающих данных,

1963
01:06:12,240 --> 01:06:13,920
чтобы просто дать вам представление о том, что

1964
01:06:13,920 --> 01:06:15,680
это за метрики и почему они могут

1965
01:06:15,680 --> 01:06:17,039
быть полезны,

1966
01:06:17,039 --> 01:06:19,280
этот график рассеивания показывает

1967
01:06:19,280 --> 01:06:22,079
недоумение, назначенное gpt2, и

1968
01:06:22,079 --> 01:06:25,119
недоумение, назначенное zlib для

1969
01:06:25,119 --> 01:06:28,480
200000 выборок, сгенерированных gpt2, и

1970
01:06:28,480 --> 01:06:30,079
вы можете видеть, что большинство из них как бы

1971
01:06:30,079 --> 01:06:32,400
попадают в эту строку, их большая группа

1972
01:06:32,400 --> 01:06:33,599


1973
01:06:33,599 --> 01:06:35,520
на r  Здесь серым цветом,

1974
01:06:35,520 --> 01:06:36,400
но

1975
01:06:36,400 --> 01:06:39,039
выделены здесь красным и синим цветом, показаны

1976
01:06:39,039 --> 01:06:40,160
образцы,

1977
01:06:40,160 --> 01:06:43,039
которые, как вы знаете, являются выбросами, которые, как вы знаете,

1978
01:06:43,039 --> 01:06:46,079
gpg2 придает zlib гораздо меньшее затруднение,

1979
01:06:46,079 --> 01:06:49,039
извините за эти образцы, чем zlib,

1980
01:06:49,039 --> 01:06:51,200
и это означает, что gpg2

1981
01:06:51,200 --> 01:06:53,200
очень хорошо предсказывает, что  Далее идет

1982
01:06:53,200 --> 01:06:55,760
в этих примерах, а zlib - это не

1983
01:06:55,760 --> 01:06:58,559
zlib, это своего рода

1984
01:06:58,559 --> 01:07:00,880
непредвзятый источник, правда, он на самом деле не

1985
01:07:00,880 --> 01:07:02,880
предварительно обучен на кучу данных, это своего

1986
01:07:02,880 --> 01:07:05,440
рода агностик данных, поэтому может случиться так,

1987
01:07:05,440 --> 01:07:07,520
что если gpt2 очень хорош в предсказании

1988
01:07:07,520 --> 01:07:09,440
того, что будет дальше  в заданной последовательности, но

1989
01:07:09,440 --> 01:07:12,400
zlib не означает, что gpt-2 запомнил

1990
01:07:12,400 --> 01:07:14,240
эти образцы, поэтому все эти вещи

1991
01:07:14,240 --> 01:07:15,520
вроде как в верхнем левом углу есть

1992
01:07:15,520 --> 01:07:17,760
возможные запомненные образцы, и на самом деле

1993
01:07:17,760 --> 01:07:19,680
мы отметили синим цветом те, которые, как

1994
01:07:19,680 --> 01:07:21,680
оказалось, действительно были в обучающих

1995
01:07:21,680 --> 01:07:25,920
данных  set и что gpt2 запомнил в

1996
01:07:26,240 --> 01:07:29,680
целом, мы нашли много примеров

1997
01:07:29,680 --> 01:07:31,440
дословного текста, запомненного из

1998
01:07:31,440 --> 01:07:34,160
набора обучающих данных, который включал в себя файлы журнала новостей,

1999
01:07:34,160 --> 01:07:36,720
лицензии, вы знаете страницы из

2000
01:07:36,720 --> 01:07:41,599
URL-адресов википедии, и мы выделяем два

2001
01:07:41,599 --> 01:07:44,160
типа da  Здесь мы обнаружили, что

2002
01:07:44,160 --> 01:07:46,480
gpt2 запомнил, что,

2003
01:07:46,480 --> 01:07:48,079
по нашему мнению, представляет собой личную

2004
01:07:48,079 --> 01:07:50,400
информацию, такую как имена людей из

2005
01:07:50,400 --> 01:07:52,400
не новых образцов или контактную информацию,

2006
01:07:52,400 --> 01:07:56,319
как в примере, который я показал ранее,

2007
01:07:56,319 --> 01:07:58,240
и поэтому вы можете спросить, хорошо, вы знаете, может быть,

2008
01:07:58,240 --> 01:08:00,160
это не так уж удивительно, что gpt2

2009
01:08:00,160 --> 01:08:01,920
запомнил  новостная статья, если эта новостная

2010
01:08:01,920 --> 01:08:03,520
статья появляется в Интернете сотни раз,

2011
01:08:03,520 --> 01:08:05,920
нам на самом деле повезло,

2012
01:08:05,920 --> 01:08:08,079
потому что оказалось, что было

2013
01:08:08,079 --> 01:08:10,160
множество примеров запомненных данных, которые

2014
01:08:10,160 --> 01:08:13,039
появлялись только в одном документе во всем

2015
01:08:13,039 --> 01:08:16,158
общедоступном Интернете, это была в основном

2016
01:08:16,158 --> 01:08:18,799
паста, похожая на  вставьте кучу URL-адресов

2017
01:08:18,799 --> 01:08:20,799
из Reddit из спорного

2018
01:08:20,799 --> 01:08:22,640
субреддита, называемого дональдом,

2019
01:08:22,640 --> 01:08:24,719
и все эти URL-адреса имели точно такую

2020
01:08:24,719 --> 01:08:27,120
же форму, как вы знаете, http двоеточие

2021
01:08:27,120 --> 01:08:30,319
косая черта reddit.com

2022
01:08:30,319 --> 01:08:32,560


2023
01:08:32,560 --> 01:08:35,359


2024
01:08:35,359 --> 01:08:37,600
Теперь эта часть потока случайных чисел и букв

2025
01:08:37,600 --> 01:08:40,158
хороша, потому что

2026
01:08:40,158 --> 01:08:42,158
во всех случаях одинаково сложно предсказать, что это в основном

2027
01:08:42,158 --> 01:08:44,560
случайный хэш, поэтому мы знаем, что

2028
01:08:44,560 --> 01:08:46,238
Часть последовательности должна быть одинаково сложной

2029
01:08:46,238 --> 01:08:48,719
для запоминания любой модели,

2030
01:08:48,719 --> 01:08:50,880
и это означает, что мы можем

2031
01:08:50,880 --> 01:08:52,479
запомнить, мы можем измерить,

2032
01:08:52,479 --> 01:08:54,238
сколько раз

2033
01:08:54,238 --> 01:08:56,960
конкретный URL-адрес должен появиться в

2034
01:08:56,960 --> 01:08:58,719
этом списке URL-адресов, потому что

2035
01:08:58,719 --> 01:09:01,359
в списке были повторения по порядку  для

2036
01:09:01,359 --> 01:09:05,040
одной из конкретных моделей размера gb2,

2037
01:09:05,040 --> 01:09:07,120
чтобы запомнить его,

2038
01:09:07,120 --> 01:09:09,040
и мы обнаружили, что самый большой

2039
01:09:09,040 --> 01:09:12,158
вариант gpg2 gb2xl

2040
01:09:12,158 --> 01:09:13,520


2041
01:09:13,520 --> 01:09:17,439
запомнил URL-адрес, который появлялся 33 раза

2042
01:09:17,439 --> 01:09:19,439
в этом конкретном документе, но

2043
01:09:19,439 --> 01:09:22,880
не URL-адреса, которые появлялись в 17 раз или меньше,

2044
01:09:22,880 --> 01:09:25,279
модель среднего размера была  только действительно

2045
01:09:25,279 --> 01:09:27,679
может полностью запомнить URL-адрес, который

2046
01:09:27,679 --> 01:09:29,600
появлялся 56 раз, а маленькая модель

2047
01:09:29,600 --> 01:09:31,839
действительно не запоминала ничего. Половина в

2048
01:09:31,839 --> 01:09:33,198
основном означает, что мы могли бы заставить его

2049
01:09:33,198 --> 01:09:34,719
выплюнуть URL-адрес, если бы мы дали ему

2050
01:09:34,719 --> 01:09:36,880
дополнительные подсказки, мы в основном намекнули

2051
01:09:36,880 --> 01:09:39,439
на то, что некоторые  чисел было гм, и

2052
01:09:39,439 --> 01:09:41,120
вывод из этого состоит в том,

2053
01:09:41,120 --> 01:09:43,120
что мы на самом деле случайно, потому

2054
01:09:43,120 --> 01:09:44,640
что есть этот конкретный документ с

2055
01:09:44,640 --> 01:09:46,719
такой структурой, мы могли сказать

2056
01:09:46,719 --> 01:09:49,040
достаточно уверенно  То, что более крупные

2057
01:09:49,040 --> 01:09:51,120
модели, как правило, запоминают больше данных, если

2058
01:09:51,120 --> 01:09:53,198
им нужно увидеть пример, им

2059
01:09:53,198 --> 01:09:55,520
нужно видеть конкретные примеры реже

2060
01:09:55,520 --> 01:09:57,920
, чтобы запоминать их, что, как мы

2061
01:09:57,920 --> 01:10:00,800
думали, было интересным

2062
01:10:00,840 --> 01:10:03,120
открытием,

2063
01:10:03,120 --> 01:10:05,120
пока я как бы в

2064
01:10:05,120 --> 01:10:06,800
основном говорил о преимуществах

2065
01:10:06,800 --> 01:10:09,280
больших моделей правильно, потому что большие

2066
01:10:09,280 --> 01:10:11,040
модели лучше справились с суперклеем, большие

2067
01:10:11,040 --> 01:10:12,800
модели лучше справились с закрытой книгой,

2068
01:10:12,800 --> 01:10:14,640
отвечая на вопрос, конечно,

2069
01:10:14,640 --> 01:10:16,480
есть одно предостережение, что более крупные модели также, кажется,

2070
01:10:16,480 --> 01:10:17,920
лучше запоминают свой

2071
01:10:17,920 --> 01:10:20,640
обучающий набор данных, но большие

2072
01:10:20,640 --> 01:10:24,080
модели также неудобны.

2073
01:10:24,080 --> 01:10:25,679
они более затратны в вычислительном отношении,

2074
01:10:25,679 --> 01:10:27,840
они потребляют больше энергии и

2075
01:10:27,840 --> 01:10:30,320
не подходят, например, t511b не подходит для одного графического процессора,

2076
01:10:30,320 --> 01:10:32,159


2077
01:10:32,159 --> 01:10:34,159
если вы не используете

2078
01:10:34,159 --> 01:10:37,040
какие-то умные методы,

2079
01:10:37,040 --> 01:10:37,920


2080
01:10:37,920 --> 01:10:39,840
поэтому последняя статья, которую я буду обсуждать,

2081
01:10:39,840 --> 01:10:42,239
является супер недавней работой

2082
01:10:42,239 --> 01:10:44,000
можем ли мы в основном сократить разрыв в производительности

2083
01:10:44,000 --> 01:10:46,000
между большими и маленькими моделями за

2084
01:10:46,000 --> 01:10:47,600
счет улучшений в архитектуре трансформатора,

2085
01:10:47,600 --> 01:10:49,679
поэтому в этой работе мы в

2086
01:10:49,679 --> 01:10:51,280
основном берем  Та же стратегия, которую мы использовали

2087
01:10:51,280 --> 01:10:52,880
в документе t5,

2088
01:10:52,880 --> 01:10:55,040
где мы взяли своего рода ландшафт

2089
01:10:55,040 --> 01:10:56,400
существующих модификаций

2090
01:10:56,400 --> 01:10:58,640
архитектуры трансформатора и оценили

2091
01:10:58,640 --> 01:11:00,800
их в тех же точных настройках,

2092
01:11:00,800 --> 01:11:02,560
и было много

2093
01:11:02,560 --> 01:11:04,080
вариантов, предложенных для

2094
01:11:04,080 --> 01:11:06,400
архитектуры трансформатора в t5, мы используем в основном

2095
01:11:06,400 --> 01:11:08,400
стандартная архитектура декодера кодера

2096
01:11:08,400 --> 01:11:10,480
из внимания - это все, что вам нужно,

2097
01:11:10,480 --> 01:11:12,320
чтобы визуализировать здесь, но

2098
01:11:12,320 --> 01:11:14,239
было много и много модификаций

2099
01:11:14,239 --> 01:11:15,440
, которые были предложены с тех пор, как

2100
01:11:15,440 --> 01:11:18,239
преобразователь был выпущен в 2017 году.

2101
01:11:18,239 --> 01:11:19,520
например,

2102
01:11:19,520 --> 01:11:21,120
возможно, люди предложили

2103
01:11:21,120 --> 01:11:23,440
вам факторизовать свою матрицу встраивания.

2104
01:11:23,440 --> 01:11:25,120
должны совместно использовать матрицу внедрения и

2105
01:11:25,120 --> 01:11:27,440
выходной слой softmax, вы должны использовать

2106
01:11:27,440 --> 01:11:29,360
разные формы softmax, такие как

2107
01:11:29,360 --> 01:11:31,040
смесь softmax или адаптивный

2108
01:11:31,040 --> 01:11:32,320
softmax,

2109
01:11:32,320 --> 01:11:34,080
разные способы нормализации или

2110
01:11:34,080 --> 01:11:36,000
инициализации модели,

2111
01:11:36,000 --> 01:11:37,679
возможно, разные

2112
01:11:37,679 --> 01:11:40,080
механизмы внимания, альтернативы

2113
01:11:40,080 --> 01:11:41,520
механизмам внимания, такие как легкие

2114
01:11:41,520 --> 01:11:43,679
и динамические свертки,

2115
01:11:43,679 --> 01:11:45,440
разные нелинейности в  корм

2116
01:11:45,440 --> 01:11:47,040
для  Уорд накладывает разные структуры

2117
01:11:47,040 --> 01:11:48,480
на слои с прямой связью, такие как смесь

2118
01:11:48,480 --> 01:11:51,199
экспертов или переключающий трансформатор,

2119
01:11:51,199 --> 01:11:52,719
совершенно разные архитектуры, которые

2120
01:11:52,719 --> 01:11:54,480
были вдохновлены трансформатором, например,

2121
01:11:54,480 --> 01:11:55,760
воронкообразный трансформатор, развитый

2122
01:11:55,760 --> 01:11:57,840
трансформатор, универсальный трансформатор

2123
01:11:57,840 --> 01:11:59,360
и так далее, на самом деле их было просто

2124
01:11:59,360 --> 01:12:01,760
тонны, тонны и тонны  и

2125
01:12:01,760 --> 01:12:03,440
снова цель в этой статье состояла в том, чтобы взять

2126
01:12:03,440 --> 01:12:05,679
кучу этих модификаций и применить

2127
01:12:05,679 --> 01:12:07,600
ту же базовую методологию из статьи t5,

2128
01:12:07,600 --> 01:12:09,280
где мы тестируем их в тех же

2129
01:12:09,280 --> 01:12:10,880
экспериментальных условиях, в

2130
01:12:10,880 --> 01:12:12,960
частности, мы в основном тестировали их

2131
01:12:12,960 --> 01:12:16,000
точно в настройке t5, которую я описал

2132
01:12:16,000 --> 01:12:17,360
в начале  лекции, в которой мы

2133
01:12:17,360 --> 01:12:20,320
предварительно обучили

2134
01:12:20,320 --> 01:12:23,040
модель размера на основе t5 на c4, а затем точно

2135
01:12:23,040 --> 01:12:26,159
настроили ее для нескольких последующих задач,

2136
01:12:26,159 --> 01:12:27,679
я не буду обсуждать это слишком подробно,

2137
01:12:27,679 --> 01:12:28,880
потому что я дал довольно подробное

2138
01:12:28,880 --> 01:12:30,159
введение в нее в

2139
01:12:30,159 --> 01:12:32,159
начале выступления

2140
01:12:32,159 --> 01:12:33,280
и  Итак,

2141
01:12:33,280 --> 01:12:36,080
вот что-то вроде первого набора результатов,

2142
01:12:36,080 --> 01:12:37,600
которые я покажу вам

2143
01:12:37,600 --> 01:12:39,040
по оси x, это разные

2144
01:12:39,040 --> 01:12:40,719
модификации трансформатора, я не

2145
01:12:40,719 --> 01:12:42,400
лаб  ling, какой из них, потому что я, как

2146
01:12:42,400 --> 01:12:44,000
вы знаете, не хотите вызывать какую-либо

2147
01:12:44,000 --> 01:12:46,640
конкретную модификацию,

2148
01:12:46,640 --> 01:12:49,520
это потеря валидации,

2149
01:12:49,520 --> 01:12:50,400
достигаемая моделью

2150
01:12:50,400 --> 01:12:52,239
для цели предварительного обучения, поэтому, когда

2151
01:12:52,239 --> 01:12:54,159
мы проводим предварительное обучение на c4, мы сохраняем

2152
01:12:54,159 --> 01:12:56,480
некоторые данные из c4  а затем мы в основном

2153
01:12:56,480 --> 01:12:59,120
измеряем потерю проверки на имеющихся

2154
01:12:59,120 --> 01:13:01,520
данных, так что чем меньше, тем лучше в этом

2155
01:13:01,520 --> 01:13:03,679
случае, и вы можете видеть, что эта пунктирная

2156
01:13:03,679 --> 01:13:05,199
черная линия - это производительность

2157
01:13:05,199 --> 01:13:07,520
базовой модели без каких-либо

2158
01:13:07,520 --> 01:13:09,199
модификаций трансформатора, вы знаете, что в основном

2159
01:13:09,199 --> 01:13:11,199
ванильный трансформатор, и вы  Можно видеть, что на

2160
01:13:11,199 --> 01:13:12,400
самом деле некоторые модификации трансформатора

2161
01:13:12,400 --> 01:13:14,320
достигли лучшей

2162
01:13:14,320 --> 01:13:16,480
производительности, что было хорошо,

2163
01:13:16,480 --> 01:13:18,239
но многие из них этого не сделали, и многие

2164
01:13:18,239 --> 01:13:19,920
из них действительно получили значительно худшую

2165
01:13:19,920 --> 01:13:21,280
производительность

2166
01:13:21,280 --> 01:13:23,440
, и, возможно, даже хуже, некоторые из

2167
01:13:23,440 --> 01:13:24,719
этих

2168
01:13:24,719 --> 01:13:26,159
вариантов  трансформатор, который

2169
01:13:26,159 --> 01:13:27,920
достиг лучшей производительности,

2170
01:13:27,920 --> 01:13:29,920
претерпел довольно незначительные изменения, такие как, например, просто

2171
01:13:29,920 --> 01:13:32,000
взятие relu в плотном действительно

2172
01:13:32,000 --> 01:13:34,239
плотном слое и замена его другим

2173
01:13:34,239 --> 01:13:36,320
нелинейным  y, так что это довольно незначительное

2174
01:13:36,320 --> 01:13:37,520


2175
01:13:37,520 --> 01:13:39,199
изменение, а некоторые из других

2176
01:13:39,199 --> 01:13:41,040
действительно высокопроизводительных моделей были в

2177
01:13:41,040 --> 01:13:43,120
конечном итоге более дорогими моделями, как вы

2178
01:13:43,120 --> 01:13:44,800
знаете, мы использовали ту же базовую модель, это

2179
01:13:44,800 --> 01:13:46,719
была та же модель размера на основе t5,

2180
01:13:46,719 --> 01:13:48,000


2181
01:13:48,000 --> 01:13:50,560
но некоторые из этих методов, например

2182
01:13:50,560 --> 01:13:53,040
Переключатель трансформатора, например,

2183
01:13:53,040 --> 01:13:55,280
резко увеличивает количество параметров, поэтому

2184
01:13:55,280 --> 01:13:56,880
он дороже с точки зрения

2185
01:13:56,880 --> 01:13:59,280
памяти, некоторые другие методы как

2186
01:13:59,280 --> 01:14:00,719
бы случайно совпадают,

2187
01:14:00,719 --> 01:14:02,320
возможно, если вы сделаете модель глубже, она

2188
01:14:02,320 --> 01:14:05,360
не сможет использовать ее,

2189
01:14:05,360 --> 01:14:08,080
использовать ускоритель как  эффективно и, таким образом,

2190
01:14:08,080 --> 01:14:09,360
это

2191
01:14:09,360 --> 01:14:11,199
делает время обучения и время вывода

2192
01:14:11,199 --> 01:14:12,719
немного дороже,

2193
01:14:12,719 --> 01:14:15,040
поэтому, если вы исключите очень простые

2194
01:14:15,040 --> 01:14:17,120
изменения в трансформаторе и те,

2195
01:14:17,120 --> 01:14:18,640
которые в конечном итоге сделали модель более

2196
01:14:18,640 --> 01:14:20,800
дорогой по какой-либо оси, на самом деле

2197
01:14:20,800 --> 01:14:23,440
было очень мало, если вообще какие-либо модификации  это значительно

2198
01:14:23,440 --> 01:14:25,600
улучшило производительность,

2199
01:14:25,600 --> 01:14:26,840


2200
01:14:26,840 --> 01:14:29,040
и это верно для

2201
01:14:29,040 --> 01:14:31,679
задачи перед обучением, это также верно и для

2202
01:14:31,679 --> 01:14:33,679
последующих задач, которые мы рассмотрели, так что

2203
01:14:33,679 --> 01:14:35,520
это  Rouge two score - это просто

2204
01:14:35,520 --> 01:14:37,440
одна из метрик, которые люди используют

2205
01:14:37,440 --> 01:14:40,080
в задаче x-sum xm. Вы можете

2206
01:14:40,080 --> 01:14:41,760
думать об этом как о более сложной версии

2207
01:14:41,760 --> 01:14:44,080
ежедневной почты cnn через задачу суммирования,

2208
01:14:44,080 --> 01:14:46,400
и вы можете видеть, что варианты модели

2209
01:14:46,400 --> 01:14:49,520
которые достигли лучшего результата валидации,

2210
01:14:49,520 --> 01:14:52,320
как правило, также получали лучший

2211
01:14:52,320 --> 01:14:55,520
результат x-sum rouge 2, но опять же, почти

2212
01:14:55,520 --> 01:14:57,360
все варианты, которые мы пробовали, снижали

2213
01:14:57,360 --> 01:14:59,920
производительность,

2214
01:14:59,920 --> 01:15:02,320
и, кроме того, я как бы немного упомянул

2215
01:15:02,320 --> 01:15:04,000
об этом, существует

2216
01:15:04,000 --> 01:15:05,679
достаточно хорошая корреляция  между

2217
01:15:05,679 --> 01:15:08,320
потерей валидации и оценкой суперклея,

2218
01:15:08,320 --> 01:15:10,560
гм, хотя я просто отмечу

2219
01:15:10,560 --> 01:15:12,800
пару интересных моментов, один из них - это

2220
01:15:12,800 --> 01:15:15,199
метод, называемый прозрачным вниманием, он

2221
01:15:15,199 --> 01:15:17,040
восстановил довольно хорошую потерю валидации,

2222
01:15:17,040 --> 01:15:19,199
но в конечном итоге очень плохую

2223
01:15:19,199 --> 01:15:21,679
оценку суперклея, что было удивительно для  нас гм,

2224
01:15:21,679 --> 01:15:22,960
коммутирующий трансформатор, который я

2225
01:15:22,960 --> 01:15:25,040
выделю здесь, достиг лучших

2226
01:15:25,040 --> 01:15:26,960
потерь при проверке, но он не получил

2227
01:15:26,960 --> 01:15:29,600
наилучшего результата суперклея в закрытой

2228
01:15:29,600 --> 01:15:31,760
книге вариантов веб-

2229
01:15:31,760 --> 01:15:34,239
вопросов переключатель tran  sformer на самом деле добился

2230
01:15:34,239 --> 01:15:37,360
просто наилучшей проверки и точности,

2231
01:15:37,360 --> 01:15:40,000
и мы, такого рода, поддерживаем слабую

2232
01:15:40,000 --> 01:15:43,040
гипотезу в этой области, что

2233
01:15:43,040 --> 01:15:44,960
увеличение числа параметров

2234
01:15:44,960 --> 01:15:46,400
улучшает объем знаний,

2235
01:15:46,400 --> 01:15:48,719
которые модель может усвоить, но это не

2236
01:15:48,719 --> 01:15:51,600
помогает модельной причине, так что

2237
01:15:51,600 --> 01:15:53,520
очень, очень грубо говоря, опять же,

2238
01:15:53,520 --> 01:15:55,760
это своего рода предположение, что суперклей

2239
01:15:55,760 --> 01:15:58,719
требует эм-глубоких возможностей рассуждения.

2240
01:15:58,719 --> 01:16:01,280
Веб-закрытая книга. Веб-вопросы

2241
01:16:01,280 --> 01:16:03,360
требуют интеллектуальных возможностей, и поэтому

2242
01:16:03,360 --> 01:16:05,440
переключающий трансформатор, который только увеличивает

2243
01:16:05,440 --> 01:16:07,520
количество параметров без

2244
01:16:07,520 --> 01:16:08,960
масштабирования обработки,

2245
01:16:08,960 --> 01:16:10,800
может быть, лучше справляется с этим на  Задача веб-

2246
01:16:10,800 --> 01:16:13,199
вопросов,

2247
01:16:13,199 --> 01:16:14,000
поэтому

2248
01:16:14,000 --> 01:16:15,600
такая цифра должна

2249
01:16:15,600 --> 01:16:17,679
вызывать у вас красные флажки,

2250
01:16:17,679 --> 01:16:19,280
потому что это довольно смелое заявление о том, что

2251
01:16:19,280 --> 01:16:20,480
большинство из этих вещей на

2252
01:16:20,480 --> 01:16:21,920
самом деле не так сильно помогают,

2253
01:16:21,920 --> 01:16:24,480
и есть несколько возможных

2254
01:16:24,480 --> 01:16:26,400
причин, по которым  это может быть случай

2255
01:16:26,400 --> 01:16:27,840
, когда наша кодовая база просто очень

2256
01:16:27,840 --> 01:16:29,920
необычна и нестандартна,

2257
01:16:29,920 --> 01:16:31,840
мы не думаем, что это так, потому

2258
01:16:31,840 --> 01:16:33,600
что  Кодовая база, которую мы использовали, на самом деле была

2259
01:16:33,600 --> 01:16:36,239
разработана одним из людей, которые

2260
01:16:36,239 --> 01:16:38,719
изобрели трансформатор gnome shazier,

2261
01:16:38,719 --> 01:16:41,360
и он использовался много, он

2262
01:16:41,360 --> 01:16:44,320
использовался во многих различных документах, это в

2263
01:16:44,320 --> 01:16:45,760
основном то же самое, что и тензор для

2264
01:16:45,760 --> 01:16:48,400
тензорной базы кода, и поэтому мы думаем  что,

2265
01:16:48,400 --> 01:16:49,840
возможно, наша кодовая база и

2266
01:16:49,840 --> 01:16:51,120
детали реализации должны быть

2267
01:16:51,120 --> 01:16:52,640
достаточно стандартными.

2268
01:16:52,640 --> 01:16:54,239


2269
01:16:54,239 --> 01:16:56,480


2270
01:16:56,480 --> 01:16:58,239


2271
01:16:58,239 --> 01:17:00,239


2272
01:17:00,239 --> 01:17:02,239


2273
01:17:02,239 --> 01:17:04,880
у них есть

2274
01:17:04,880 --> 01:17:06,800
современные результаты от трансформаторов, и на

2275
01:17:06,800 --> 01:17:08,880
самом деле мы включили отдельно

2276
01:17:08,880 --> 01:17:11,760
контролируемое обучение только на английском

2277
01:17:11,760 --> 01:17:13,760
немецком языке, что было задачей, для которой

2278
01:17:13,760 --> 01:17:15,840
был фактически предложен трансформатор,

2279
01:17:15,840 --> 01:17:17,679


2280
01:17:17,679 --> 01:17:19,199
возможно, нам нужна дополнительная настройка гиперпараметров,

2281
01:17:19,199 --> 01:17:20,239


2282
01:17:20,239 --> 01:17:21,679
потому что мы снова этого не сделали  провести

2283
01:17:21,679 --> 01:17:23,040
значительную настройку гиперпараметров для

2284
01:17:23,040 --> 01:17:25,040
каждого из этих методов, чтобы проверить, насколько это верно,

2285
01:17:25,040 --> 01:17:26,320
мы на самом деле использовали один из

2286
01:17:26,320 --> 01:17:27,520
методов,

2287
01:17:27,520 --> 01:17:29,600
которые мы значительно выполнили  Намного хуже,

2288
01:17:29,600 --> 01:17:32,000
чем мы ожидали, и мы провели, может быть,

2289
01:17:32,000 --> 01:17:33,679
пару сотен попыток оптимизации гиперпараметров.

2290
01:17:33,679 --> 01:17:35,520
Один из исследователей

2291
01:17:35,520 --> 01:17:36,960
в этой статье потратил много времени,

2292
01:17:36,960 --> 01:17:38,400
пытаясь получить правильные гиперпараметры, чтобы заставить его

2293
01:17:38,400 --> 01:17:41,120
работать, и в конечном итоге он никогда не работал

2294
01:17:41,120 --> 01:17:44,320
так же хорошо, как базовый уровень  метод следующая

2295
01:17:44,320 --> 01:17:45,679
возможность заключается в том, что мы

2296
01:17:45,679 --> 01:17:47,280
неправильно реализовали эти модификации для

2297
01:17:47,280 --> 01:17:49,440
проверки работоспособности, мы фактически отправили электронное

2298
01:17:49,440 --> 01:17:50,719
письмо авторам всех различных

2299
01:17:50,719 --> 01:17:52,239
модификаций и попросили их проверить

2300
01:17:52,239 --> 01:17:54,320
нашу реализацию, все те, которые

2301
01:17:54,320 --> 01:17:55,600
вернулись к нам, сказали, что это выглядело

2302
01:17:55,600 --> 01:17:56,880
правильным для  их,

2303
01:17:56,880 --> 01:17:58,400
а затем, наконец, последний вариант состоит в том, что,

2304
01:17:58,400 --> 01:17:59,760
возможно, эти модификации

2305
01:17:59,760 --> 01:18:01,679
преобразователя на самом деле не являются своего рода

2306
01:18:01,679 --> 01:18:03,440
передачей, они не передаются между

2307
01:18:03,440 --> 01:18:05,600
базами кода, реализациями и

2308
01:18:05,600 --> 01:18:08,480
приложениями, и нам, по крайней мере, на

2309
01:18:08,480 --> 01:18:10,159
основании доказательств того, что у нас есть это

2310
01:18:10,159 --> 01:18:12,880
правдоподобная возможность, на мой взгляд,

2311
01:18:12,880 --> 01:18:15,040
лучший способ контролировать это - если

2312
01:18:15,040 --> 01:18:16,719
вы предлагаете новую

2313
01:18:16,719 --> 01:18:19,120
модификацию трансформатора, попробуйте применить ее к

2314
01:18:19,120 --> 01:18:21,600
много кодовых баз и задач, как вы можете

2315
01:18:21,600 --> 01:18:23,840
без настройки гиперпараметров, и если

2316
01:18:23,840 --> 01:18:25,679
он работает во всех этих настройках, тогда

2317
01:18:25,679 --> 01:18:27,360
вы золотой, и ваша вещь, вероятно,

2318
01:18:27,360 --> 01:18:29,280
будет перенесена, и мы думаем, что это,

2319
01:18:29,280 --> 01:18:30,560
вероятно, тот случай, когда более простые

2320
01:18:30,560 --> 01:18:32,159
модификации, такие как изменение

2321
01:18:32,159 --> 01:18:33,920
нелинейности,

2322
01:18:33,920 --> 01:18:36,159
являются  не так сильно зависят от гиперпараметров

2323
01:18:36,159 --> 01:18:38,480
и деталей реализации, поэтому

2324
01:18:38,480 --> 01:18:39,600
они могут быть теми, которые с большей

2325
01:18:39,600 --> 01:18:43,440
вероятностью будут перенесены, так сказать,

2326
01:18:43,440 --> 01:18:45,040
так что это все, что я буду обсуждать в этом выступлении,

2327
01:18:45,040 --> 01:18:46,880
я понимаю, что это был своего рода вихревой

2328
01:18:46,880 --> 01:18:48,640
тур, поэтому я связал  все документы,

2329
01:18:48,640 --> 01:18:51,440
которые я обсуждал на этом слайде,

2330
01:18:51,440 --> 01:18:54,000
конечно, это была работа, проделанная огромной

2331
01:18:54,000 --> 01:18:56,159
и поистине удивительной группой сотрудников

2332
01:18:56,159 --> 01:18:58,159
над этими пятью документами,

2333
01:18:58,159 --> 01:19:00,239
которые я перечислил на экране здесь, и

2334
01:19:00,239 --> 01:19:02,560
да, я счастлив  чтобы ответить на любые

2335
01:19:02,560 --> 01:19:04,480
дополнительные вопросы, которые у вас все в

2336
01:19:04,480 --> 01:19:06,800
порядке, так что большое спасибо, Колин, за эту

2337
01:19:06,800 --> 01:19:08,719
прекрасную беседу и было немного

2338
01:19:08,719 --> 01:19:12,159
информации, эм, я понял, также

2339
01:19:12,159 --> 01:19:13,760
есть одна вещь, которую я забыл сказать

2340
01:19:13,760 --> 01:19:15,280
во введении  так что, я думаю, мне нужно

2341
01:19:15,280 --> 01:19:18,480
иметь и после этого, а именно,

2342
01:19:18,480 --> 01:19:21,760
что ммм Колин теперь начал работать

2343
01:19:21,760 --> 01:19:23,760
профессором в университете Северной

2344
01:19:23,760 --> 01:19:25,760
Каролины, так что

2345
01:19:25,760 --> 01:19:27,760
университет Северной Каролины играет большую роль

2346
01:19:27,760 --> 01:19:30,080
в этом курсе, потому что он также был

2347
01:19:30,080 --> 01:19:32,560
источником  данных чероки, которые мы используем

2348
01:19:32,560 --> 01:19:34,560
для задания 4 для перевода чероки на

2349
01:19:34,560 --> 01:19:38,320
английский язык, так что давай, смоляные каблуки,

2350
01:19:38,320 --> 01:19:41,040
но мм,

2351
01:19:41,040 --> 01:19:44,239
да, так что, да, Колин рад остаться

2352
01:19:44,239 --> 01:19:46,800
и ответить на некоторые вопросы, поэтому, если вы

2353
01:19:46,800 --> 01:19:49,520
хотите задать больше вопросов, используйте

2354
01:19:49,520 --> 01:19:51,679
луч  руки, и мы затем как бы пригласим

2355
01:19:51,679 --> 01:19:54,800
вас в эм, где люди могут видеть

2356
01:19:54,800 --> 01:19:56,560
друг друга, эм,

2357
01:19:56,560 --> 01:19:57,760
комната масштабирования,

2358
01:19:57,760 --> 01:19:59,440
и вы знаете, если вы готовы к этому, было бы

2359
01:19:59,440 --> 01:20:02,320
даже неплохо включить ваше видео, чтобы

2360
01:20:02,320 --> 01:20:04,480
люди могли видеть, кто они  разговариваю с

2361
01:20:04,480 --> 01:20:06,080
ммм, и

2362
01:20:06,080 --> 01:20:08,239
да,

2363
01:20:08,239 --> 01:20:09,679
может быть, в первую очередь вам следует

2364
01:20:09,679 --> 01:20:12,880
прекратить показывать экран, и да, если

2365
01:20:12,880 --> 01:20:14,320
есть что-то, что вы хотите показать снова,

2366
01:20:14,320 --> 01:20:16,639
вы можете включить его снова

2367
01:20:16,639 --> 01:20:18,400
да, может быть, я просто скажу,

2368
01:20:18,400 --> 01:20:21,440
пока есть люди  гм

2369
01:20:21,440 --> 01:20:23,120
в том смысле, что я профессор  в

2370
01:20:23,120 --> 01:20:24,880
unc в случае, если

2371
01:20:24,880 --> 01:20:26,719
в аудитории есть магистры или студенты бакалавриата,

2372
01:20:26,719 --> 01:20:28,480
которые подают заявки на программы PhD,

2373
01:20:28,480 --> 01:20:30,560
крайний срок подачи заявок

2374
01:20:30,560 --> 01:20:33,280
на unc фактически еще не наступил,

2375
01:20:33,280 --> 01:20:35,920
поэтому, если вы, возможно, хотите подать заявление в

2376
01:20:35,920 --> 01:20:37,199
другую школу, у вас есть другой

2377
01:20:37,199 --> 01:20:38,480
вариант  в восторге от работы, которую я

2378
01:20:38,480 --> 01:20:40,960
представил, вы все еще можете подать заявку в unc, у

2379
01:20:40,960 --> 01:20:42,719
нас очень поздний

2380
01:20:42,719 --> 01:20:45,120
срок подачи заявок, так что просто вставьте вилку на случай, если

2381
01:20:45,120 --> 01:20:49,040
есть кто-то, кто ищет докторскую степень,

2382
01:20:49,040 --> 01:20:51,520
и unc является старейшим государственным университетом

2383
01:20:51,520 --> 01:20:52,960
в стране, и

2384
01:20:52,960 --> 01:20:56,000
позвольте мне  полная реклама usc, и

2385
01:20:56,000 --> 01:20:58,080
я думаю, у нас есть второй старейший отдел CS,

2386
01:20:58,080 --> 01:21:00,000
который, да, он

2387
01:21:00,000 --> 01:21:01,199
существует уже давно, он довольно

2388
01:21:01,199 --> 01:21:03,840
маленький, это всего лишь около 50 преподавателей,

2389
01:21:03,840 --> 01:21:06,000


2390
01:21:06,159 --> 01:21:08,239
так что пока мы ждем, пока кто-то

2391
01:21:08,239 --> 01:21:09,360
присоединится,

2392
01:21:09,360 --> 01:21:11,199
мы делаем  у меня уже есть один вопрос на самом деле

2393
01:21:11,199 --> 01:21:14,199
от

2394
01:21:14,320 --> 01:21:16,719
да привет спасибо за лекцию

2395
01:21:16,719 --> 01:21:18,080
эм, о которой

2396
01:21:18,080 --> 01:21:20,239
я не спрашивал раньше, эм,

2397
01:21:20,239 --> 01:21:22,320
когда вы обсуждаете, как

2398
01:21:22,320 --> 01:21:23,679
э-э,

2399
01:21:23,679 --> 01:21:26,960
задержка на t5 и вроде того, сколько

2400
01:21:26,960 --> 01:21:28,560
проходов

2401
01:21:28,560 --> 01:21:31,280
потребовалось для этого t  о переобучении, поэтому я уверен, что

2402
01:21:31,280 --> 01:21:32,400


2403
01:21:32,400 --> 01:21:34,639
если вы думаете, что некоторые из более крупных

2404
01:21:34,639 --> 01:21:36,560
моделей, такие как три миллиарда одиннадцать

2405
01:21:36,560 --> 01:21:37,600


2406
01:21:37,600 --> 01:21:39,679
миллиардов, хотели бы масштабироваться до того, как

2407
01:21:39,679 --> 01:21:40,880
игроки

2408
01:21:40,880 --> 01:21:43,120


2409
01:21:50,320 --> 01:21:52,400
перестанут быть

2410
01:21:52,400 --> 01:21:54,239


2411
01:21:54,239 --> 01:21:56,159
сравнивая потери

2412
01:21:56,159 --> 01:21:59,199
данных обучения с потерями данных проверки,

2413
01:21:59,199 --> 01:22:01,120
мы видим, что даже в этом случае в очень

2414
01:22:01,120 --> 01:22:03,440
очень больших моделях это примерно то же самое,

2415
01:22:03,440 --> 01:22:04,719
что предполагает своего рода в

2416
01:22:04,719 --> 01:22:06,159
традиционном смысле, что нет

2417
01:22:06,159 --> 01:22:08,239
переоснащения, одна из причин этого заключается в том, что

2418
01:22:08,239 --> 01:22:10,639
c4 на самом деле  Достаточно большой, чтобы мы выполняли

2419
01:22:10,639 --> 01:22:13,600
только один проход,

2420
01:22:13,600 --> 01:22:16,159
когда мы тренируемся на триллион токенов, и

2421
01:22:16,159 --> 01:22:17,520
вы знаете, что можете надеяться, что увидите

2422
01:22:17,520 --> 01:22:19,679
ограниченное переоснащение, когда вы видите

2423
01:22:19,679 --> 01:22:21,840
каждую часть данных только один раз в

2424
01:22:21,840 --> 01:22:23,440
течение курса обучения, так что это вроде

2425
01:22:23,440 --> 01:22:24,880
как и каждый раз, когда вы видите данные, это

2426
01:22:24,880 --> 01:22:26,320
новые данные, поэтому нет большой

2427
01:22:26,320 --> 01:22:27,520
разницы между данными в

2428
01:22:27,520 --> 01:22:29,440
обучающем наборе в проверочном наборе,

2429
01:22:29,440 --> 01:22:31,280
конечно, есть также понятие

2430
01:22:31,280 --> 01:22:32,800
переобучения, которое похоже на то, что

2431
01:22:32,800 --> 01:22:35,280
Переобучение случая, которое

2432
01:22:35,280 --> 01:22:37,280
связано с работой по запоминанию, о которой я упоминал

2433
01:22:37,280 --> 01:22:39,600
, кажется, что

2434
01:22:39,600 --> 01:22:42,080
языковые модели могут запоминать данные,

2435
01:22:42,080 --> 01:22:44,080
даже если они делают относительно мало проходов

2436
01:22:44,080 --> 01:22:46,000
по набору обучающих данных, и вы не

2437
01:22:46,000 --> 01:22:48,239
видите переоснащения среднего случая при

2438
01:22:48,239 --> 01:22:49,760
сравнении обучения

2439
01:22:49,760 --> 01:22:53,800
потеря и потеря проверки

2440
01:23:00,800 --> 01:23:02,800
конечно, извините, я пытался включить звук в моем

2441
01:23:02,800 --> 01:23:04,719
видео, но я не могу сделать это по

2442
01:23:04,719 --> 01:23:06,960
какой-то причине, в

2443
01:23:06,960 --> 01:23:08,719
первую очередь, Колин, большое спасибо,

2444
01:23:08,719 --> 01:23:10,080
это фантастическая лекция, мне она очень

2445
01:23:10,080 --> 01:23:12,159
понравилась.

2446
01:23:12,159 --> 01:23:12,960


2447
01:23:12,960 --> 01:23:14,159
что

2448
01:23:14,159 --> 01:23:16,560
мне особенно понравилось, это работа,

2449
01:23:16,560 --> 01:23:18,159
которую вы, ребята, проделали

2450
01:23:18,159 --> 01:23:20,480
над вашей атакой по извлечению тренировочных данных,

2451
01:23:20,480 --> 01:23:23,679
пытаясь идентифицировать, ну, действительно,

2452
01:23:23,679 --> 01:23:26,639
проверьте эту догадку на открытом ИИ и эффекте,

2453
01:23:26,639 --> 01:23:27,679
что

2454
01:23:27,679 --> 01:23:29,679
эти модели на самом деле не запоминают

2455
01:23:29,679 --> 01:23:32,000
тренировочные данные.  интересно, у меня на

2456
01:23:32,000 --> 01:23:34,800
самом деле есть два вопроса, у одного есть

2457
01:23:34,800 --> 01:23:37,600
openai и eff с тех пор, как они изменили тон

2458
01:23:37,600 --> 01:23:39,280


2459
01:23:39,280 --> 01:23:41,199


2460
01:23:41,199 --> 01:23:43,199


2461
01:23:43,199 --> 01:23:45,840
что вы знаете, что хорошо

2462
01:23:45,840 --> 01:23:47,920
сконструированные модели могут действительно это делать,

2463
01:23:47,920 --> 01:23:49,120
и во-вторых,

2464
01:23:49,120 --> 01:23:50,719
будет ли этот подход действительно работать для

2465
01:23:50,719 --> 01:23:52,000
обнаружения

2466
01:23:52,000 --> 01:23:54,000
других, скажем,

2467
01:23:54,000 --> 01:23:57,840
случайно закодированных предубеждений в

2468
01:23:57,840 --> 01:23:59,440
сторону крайних предубеждений, которые преобладают

2469
01:23:59,440 --> 01:24:01,040
в некоторых языковых моделях, например, если бы вы

2470
01:24:01,040 --> 01:24:03,120
могли создавать пакеты,

2471
01:24:03,120 --> 01:24:04,800
которые могли бы имитировать такие виды

2472
01:24:04,800 --> 01:24:06,560
атак  на этих моделях, а затем

2473
01:24:06,560 --> 01:24:08,719
определить с некоторой степенью точности,

2474
01:24:08,719 --> 01:24:11,360
насколько эти предубеждения на самом деле

2475
01:24:11,360 --> 01:24:12,800
присутствуют,

2476
01:24:12,800 --> 01:24:13,920
да, поэтому что

2477
01:24:13,920 --> 01:24:15,840
касается первого вопроса, я

2478
01:24:15,840 --> 01:24:17,920
не знаю никаких официальных заявлений

2479
01:24:17,920 --> 01:24:19,760
, которые были сделаны кем-либо, но я

2480
01:24:19,760 --> 01:24:21,679
скажу, что на самом деле  документ по запоминанию,

2481
01:24:21,679 --> 01:24:23,920
у нас было несколько соавторов из

2482
01:24:23,920 --> 01:24:25,679
открытого ИИ, так что это было очень много

2483
01:24:25,679 --> 01:24:28,000
сотрудничества с ними, я имею в виду, что вы знаете,

2484
01:24:28,000 --> 01:24:30,320
что мы все ученые, и мы все, что вы

2485
01:24:30,320 --> 01:24:32,159
знаете, мы все как бы выдвигаем гипотезы, которые

2486
01:24:32,159 --> 01:24:33,520
иногда оказываются правильными и

2487
01:24:33,520 --> 01:24:36,480
неправильными  гм, и поэтому я думаю, что

2488
01:24:36,480 --> 01:24:38,560
открытые глаза определенно

2489
01:24:38,560 --> 01:24:41,120
осведомлены о том факте, что

2490
01:24:41,120 --> 01:24:44,639
да, возможно, эти модели

2491
01:24:44,639 --> 01:24:46,639
могут запоминать данные, даже когда  Они не

2492
01:24:46,639 --> 01:24:48,159


2493
01:24:48,159 --> 01:24:49,760
демонстрируют традиционные признаки

2494
01:24:49,760 --> 01:24:52,320
переобучения

2495
01:24:52,400 --> 01:24:54,000
по второму пункту.

2496
01:24:54,000 --> 01:24:55,679


2497
01:24:55,679 --> 01:24:56,880
То, как люди

2498
01:24:56,880 --> 01:24:59,280
измеряли это произвольно, -

2499
01:24:59,280 --> 01:25:01,360
это вводить в модель префикс о

2500
01:25:01,360 --> 01:25:03,920
конкретной демографической группе или

2501
01:25:03,920 --> 01:25:05,040
типе человека

2502
01:25:05,040 --> 01:25:07,199
и видеть  что модель говорит

2503
01:25:07,199 --> 01:25:08,560
об этом человеке,

2504
01:25:08,560 --> 01:25:11,600
и я думаю, я думаю, что в принципе вы

2505
01:25:11,600 --> 01:25:14,560
можете думать о нашем, нашем подходе,

2506
01:25:14,560 --> 01:25:16,719
как о связанном с этим, за исключением того, что у нас есть

2507
01:25:16,719 --> 01:25:18,400
этот дополнительный шаг, который

2508
01:25:18,400 --> 01:25:21,040
измеряет, генерирует ли модель

2509
01:25:21,040 --> 01:25:23,840
этот текст  потому что он видел

2510
01:25:23,840 --> 01:25:26,080
это в своих обучающих данных в основном

2511
01:25:26,080 --> 01:25:29,120
потому, что недоумение

2512
01:25:29,120 --> 01:25:31,920
для некоторого продолжения слишком мало по сравнению

2513
01:25:31,920 --> 01:25:33,360
с моделью, которая не была обучена на тех же

2514
01:25:33,360 --> 01:25:35,840
данных, ммм, так что это может быть интересно,

2515
01:25:35,840 --> 01:25:38,159
например, если вы введете в модель

2516
01:25:38,159 --> 01:25:41,600
префикс, который  вы просите его

2517
01:25:41,600 --> 01:25:43,199
заполнить некоторую оскорбительную информацию о какой-то

2518
01:25:43,199 --> 01:25:45,520
демографической группе, чтобы проверить, значительно ли

2519
01:25:45,520 --> 01:25:47,920
ниже сложность модели для ее

2520
01:25:47,920 --> 01:25:50,719
продолжения,

2521
01:25:50,719 --> 01:25:53,040
чем вы знаете, например, zlib и  в

2522
01:25:53,040 --> 01:25:54,880
этом случае вы можете подумать, что предвзятость на

2523
01:25:54,880 --> 01:25:57,360
самом деле, может быть, эта предвзятость,

2524
01:25:57,360 --> 01:25:59,280
которую подхватила модель, связана с тем, что она

2525
01:25:59,280 --> 01:26:01,679
увидела какое-то предложение, которое выглядит

2526
01:26:01,679 --> 01:26:03,760
так же, как это обучающие данные или

2527
01:26:03,760 --> 01:26:05,840


2528
01:26:05,840 --> 01:26:08,639
что  модель

2529
01:26:08,639 --> 01:26:11,760
усвоилась в процессе обучения

2530
01:26:11,760 --> 01:26:13,840
фантастически

2531
01:26:13,840 --> 01:26:15,600
большое спасибо за то, что поделились да спасибо

2532
01:26:15,600 --> 01:26:16,880
за вопросы

2533
01:26:16,880 --> 01:26:20,159
хорошо, так что следующая остановка, я думаю,

2534
01:26:20,800 --> 01:26:23,920
здорово эм спасибо за беседу, которая была

2535
01:26:23,920 --> 01:26:28,080
супер интересной, так что мой вопрос

2536
01:26:28,080 --> 01:26:31,280
вроде как у вас  мысли о том, чтобы

2537
01:26:31,280 --> 01:26:34,480
потенциально сделать несколько

2538
01:26:34,480 --> 01:26:37,199
раундов предварительного обучения,

2539
01:26:37,199 --> 01:26:40,000
чтобы сделать его более конкретным, эм, вы знаете,

2540
01:26:40,000 --> 01:26:42,320
например, допустим, у вас есть задача где-

2541
01:26:42,320 --> 01:26:45,280
то вроде генерации ответов,

2542
01:26:45,280 --> 01:26:48,320
и вы очень привязаны к

2543
01:26:48,320 --> 01:26:50,239
конкретному набору данных генерации ответов,

2544
01:26:50,239 --> 01:26:52,480
который вы используете, но потенциально  вы хотите как

2545
01:26:52,480 --> 01:26:55,679
бы приукрасить это, введя

2546
01:26:55,679 --> 01:26:57,840
некоторый общий набор диалоговых данных, который

2547
01:26:57,840 --> 01:27:00,960
состоит из натуралистических человеческих данных,

2548
01:27:00,960 --> 01:27:02,639
так что мне интересно, есть ли у вас какие-

2549
01:27:02,639 --> 01:27:04,960
нибудь  догадки или интуиция о том, насколько

2550
01:27:04,960 --> 01:27:07,840
это эффективно, может быть, начать с

2551
01:27:07,840 --> 01:27:08,719


2552
01:27:08,719 --> 01:27:10,880
того, что вы знаете общий Интернет, а затем

2553
01:27:10,880 --> 01:27:13,679
точно настроить этот

2554
01:27:13,679 --> 01:27:16,239
набор данных неструктурированного диалога, а затем

2555
01:27:16,239 --> 01:27:19,440
точно настроить, может быть, более

2556
01:27:19,440 --> 01:27:21,760
узкий набор данных генерации ответов

2557
01:27:21,760 --> 01:27:22,960


2558
01:27:22,960 --> 01:27:24,960
да, так  так что техника, которую вы

2559
01:27:24,960 --> 01:27:27,360
описываете, звучит

2560
01:27:27,360 --> 01:27:29,760
очень похоже на этот действительно отличный

2561
01:27:29,760 --> 01:27:30,800


2562
01:27:30,800 --> 01:27:32,560
подход, который люди теперь называют

2563
01:27:32,560 --> 01:27:34,400
адаптивным к предметной области предварительным обучением или адаптивным к задачам

2564
01:27:34,400 --> 01:27:36,320
предварительным обучением. Я был представлен в статье

2565
01:27:36,320 --> 01:27:39,199
под названием не прекращайте предварительное обучение, а

2566
01:27:39,199 --> 01:27:42,000
затем  есть менее запоминающийся подзаголовок

2567
01:27:42,000 --> 01:27:43,360
эм

2568
01:27:43,360 --> 01:27:45,679
, и идея очень похожа на то, что

2569
01:27:45,679 --> 01:27:47,120
вы предложили, в основном вы берете

2570
01:27:47,120 --> 01:27:48,560
предварительно обученную модель, которая была обучена

2571
01:27:48,560 --> 01:27:51,040
вы знаете общий текст, вы делаете

2572
01:27:51,040 --> 01:27:52,719
то, что вы могли бы назвать промежуточным

2573
01:27:52,719 --> 01:27:54,639
обучением заданию, или вы продолжаете

2574
01:27:54,639 --> 01:27:56,800
предварительное обучение  на данных, зависящих от предметной области, а

2575
01:27:56,800 --> 01:27:59,199
затем, наконец, вы выполняете тонкую настройку вашего

2576
01:27:59,199 --> 01:28:01,520
конкретного набора данных тонкой настройки, в их

2577
01:28:01,520 --> 01:28:03,600
случае они рассматривают такие вещи,

2578
01:28:03,600 --> 01:28:06,159
как вы знаете, например, создание научного текста

2579
01:28:06,159 --> 01:28:08,080
classifi  катионный или биомедицинский анализ текста,

2580
01:28:08,080 --> 01:28:09,760
и когда они выполняют

2581
01:28:09,760 --> 01:28:11,920
промежуточный этап предварительной подготовки на

2582
01:28:11,920 --> 01:28:14,960
данных в домене или даже просто выполняют

2583
01:28:14,960 --> 01:28:17,280
задачу предварительной подготовки на данных

2584
01:28:17,280 --> 01:28:19,199
из задачи, это определенно помогает

2585
01:28:19,199 --> 01:28:21,840
значительно э-э, э-э, и да, так

2586
01:28:21,840 --> 01:28:23,679
что это очень хорошо

2587
01:28:23,679 --> 01:28:26,239
интуиция и гм, и я думаю, что это

2588
01:28:26,239 --> 01:28:28,320
метод, наиболее похожий на то,

2589
01:28:28,320 --> 01:28:30,560
что вы описываете, эм, это как

2590
01:28:30,560 --> 01:28:32,719
бы поднимает ясный вопрос, который, я не

2591
01:28:32,719 --> 01:28:34,639
думаю, был адресован э-э, насколько мне

2592
01:28:34,639 --> 01:28:36,400
известно, в литературе, а именно вы

2593
01:28:36,400 --> 01:28:37,679
мы обычно думаем о переносном

2594
01:28:37,679 --> 01:28:40,320
обучении как о предварительном обучении, а затем о точной настройке,

2595
01:28:40,320 --> 01:28:41,760
и теперь мы делаем что-то вроде

2596
01:28:41,760 --> 01:28:43,840
предварительного обучения, а затем, возможно, еще какое-то

2597
01:28:43,840 --> 01:28:45,920
предварительное обучение, а затем тонкую настройку, и

2598
01:28:45,920 --> 01:28:47,360
есть другие методы, которые

2599
01:28:47,360 --> 01:28:50,560
другие шаги на этом пути гм, и

2600
01:28:50,560 --> 01:28:52,159
поэтому возникает естественный вопрос, например,

2601
01:28:52,159 --> 01:28:54,480
каким должен быть учебный план задач,

2602
01:28:54,480 --> 01:28:56,639
вы знаете,

2603
01:28:56,639 --> 01:28:58,639
сколько должно быть промежуточных шагов

2604
01:28:58,639 --> 01:29:00,239
, какими должны быть промежуточные

2605
01:29:00,239 --> 01:29:02,800
шаги, какова польза от одного

2606
01:29:02,800 --> 01:29:05,120
дома  в сравнении с другими, сколько там сдвига домена,

2607
01:29:05,120 --> 01:29:07,120
и каковы

2608
01:29:07,120 --> 01:29:08,880
соответствующие преимущества и так далее, и я

2609
01:29:08,880 --> 01:29:10,159
думаю, что будет

2610
01:29:10,159 --> 01:29:11,760
интересное направление работы, которое было бы

2611
01:29:11,760 --> 01:29:12,800


2612
01:29:12,800 --> 01:29:14,159
лучше, в основном, лучше отвечая на эти

2613
01:29:14,159 --> 01:29:16,560
вопросы,

2614
01:29:16,639 --> 01:29:18,480
и что было, что было

2615
01:29:18,480 --> 01:29:21,280
аббревиатура эм или

2616
01:29:21,280 --> 01:29:23,760
да, так это называется э-э-э-э-э-э-э-э-э-э-э-э-э-дэппинг или тэппинг

2617
01:29:23,760 --> 01:29:24,639
э-э,

2618
01:29:24,639 --> 01:29:26,960
адаптивная предварительная тренировка домена или адаптивная предварительная подготовка к задаче

2619
01:29:26,960 --> 01:29:28,719
о да, статья

2620
01:29:28,719 --> 01:29:30,880
называется не прекращайте предварительную тренировку,

2621
01:29:30,880 --> 01:29:33,120
которую легко запомнить, если вам

2622
01:29:33,120 --> 01:29:35,280
нравится песня, не надо  не перестаю верить, что

2623
01:29:35,280 --> 01:29:38,080
я, как я, я не знаю,

2624
01:29:38,080 --> 01:29:40,239
преднамеренно ли это ссылка на эту песню, я

2625
01:29:40,239 --> 01:29:42,239
полагаю, это должно быть я думаю, что они должны

2626
01:29:42,239 --> 01:29:44,239
были сделать не останавливайся, предварительно

2627
01:29:44,239 --> 01:29:45,760
обучившись апострофу, если они действительно  хотел

2628
01:29:45,760 --> 01:29:47,440
отвезти его домой, но знаете, может быть,

2629
01:29:47,440 --> 01:29:48,719
это было бы слишком глупо, но в любом случае

2630
01:29:48,719 --> 01:29:50,000
да, бумага называется, не прекращайте

2631
01:29:50,000 --> 01:29:51,840
предварительную тренировку

2632
01:29:51,840 --> 01:29:54,320
, это здорово, большое спасибо,

2633
01:29:54,320 --> 01:29:56,000
да, абсолютно

2634
01:29:56,000 --> 01:29:58,639
хорошо, следующий вопрос: эм,

2635
01:29:58,639 --> 01:30:00,000
я не совсем уверен, что  они знают,

2636
01:30:00,000 --> 01:30:03,040
соответствует  тогда

2637
01:30:03,040 --> 01:30:05,920
да ладно спасибо за этот тако,

2638
01:30:05,920 --> 01:30:07,120
действительно интересный

2639
01:30:07,120 --> 01:30:09,520
эм, у меня есть открытый вопрос,

2640
01:30:09,520 --> 01:30:11,440
я действительно ищу здесь несколько советов,

2641
01:30:11,440 --> 01:30:13,199
так что кажется, что недавние

2642
01:30:13,199 --> 01:30:15,040
успехи в

2643
01:30:15,040 --> 01:30:17,199
индустрии НЛП по захвату заголовков были достигнуты благодаря

2644
01:30:17,199 --> 01:30:20,000
построение этих массивных моделей, таких как gpg3

2645
01:30:20,000 --> 01:30:21,440
um с миллиардами параметров,

2646
01:30:21,440 --> 01:30:22,880
обучение которых, как вы часто знаете, обходится в миллионы

2647
01:30:22,880 --> 01:30:24,239
долларов,

2648
01:30:24,239 --> 01:30:25,760
и вы знаете, что эти достижения

2649
01:30:25,760 --> 01:30:27,679
финансируются такими же крупными организациями, как

2650
01:30:27,679 --> 01:30:30,159
google facebook open ai, у которых более или

2651
01:30:30,159 --> 01:30:32,080
менее бесконечные ресурсы в некотором роде,

2652
01:30:32,080 --> 01:30:34,880
так что мой  вопрос в том, что вы знаете, как

2653
01:30:34,880 --> 01:30:37,760
практикующий специалист с ограниченными ресурсами, но

2654
01:30:37,760 --> 01:30:40,159
с бесконечным аппетитом к обучению

2655
01:30:40,159 --> 01:30:41,520
, каким образом я могу

2656
01:30:41,520 --> 01:30:43,840
участвовать в этих достижениях, и

2657
01:30:43,840 --> 01:30:44,719


2658
01:30:44,719 --> 01:30:46,159
вы знаете, просто участвуйте в том, что

2659
01:30:46,159 --> 01:30:48,320
происходит в отрасли, да,

2660
01:30:48,320 --> 01:30:50,719
конечно, я имею в виду, что я действительно полностью

2661
01:30:50,719 --> 01:30:52,719
сочувствую и согласен с

2662
01:30:52,719 --> 01:30:54,400
вами в том смысле, что вы знаете, что большая

2663
01:30:54,400 --> 01:30:55,760
часть разработки этих моделей

2664
01:30:55,760 --> 01:30:58,639
происходит небольшими  группы за

2665
01:30:58,639 --> 01:31:01,360
закрытыми дверями в крупных корпорациях, и

2666
01:31:01,360 --> 01:31:03,600
обычно мне не нравится видеть, как вы

2667
01:31:03,600 --> 01:31:05,440
знаете, что наука развивалась, мне нравится рассматривать это

2668
01:31:05,440 --> 01:31:07,520
как усилие сообщества, в котором участвуют

2669
01:31:07,520 --> 01:31:09,120
все виды заинтересованных сторон с

2670
01:31:09,120 --> 01:31:11,040
разным объемом ресурсов, а мы

2671
01:31:11,040 --> 01:31:12,639
не совсем  на этом этапе с этим

2672
01:31:12,639 --> 01:31:14,560
с этой работой, но

2673
01:31:14,560 --> 01:31:17,360
я действительно думаю, что в той степени, в которой

2674
01:31:17,360 --> 01:31:19,199
люди все еще выпускают предварительно обученные

2675
01:31:19,199 --> 01:31:22,320
модели, что верно, например, для t5,

2676
01:31:22,320 --> 01:31:25,280
но не для gpt-3, предстоит много

2677
01:31:25,280 --> 01:31:28,719
работы над  в основном анализируйте

2678
01:31:28,719 --> 01:31:30,080
некоторые из вещей, которые мы

2679
01:31:30,080 --> 01:31:32,480
обсуждали ранее, вы знаете, даже работа по

2680
01:31:32,480 --> 01:31:34,960
запоминанию - это в основном я

2681
01:31:34,960 --> 01:31:36,800
бы сказал, что это похоже на аналитическую работу.

2682
01:31:36,800 --> 01:31:38,480


2683
01:31:38,480 --> 01:31:40,639


2684
01:31:40,639 --> 01:31:42,400


2685
01:31:42,400 --> 01:31:44,480
настолько мало, что мы на самом деле знаем

2686
01:31:44,480 --> 01:31:47,280
о том, как эти модели работают

2687
01:31:47,280 --> 01:31:50,159
и что делает их полезными в масштабе,

2688
01:31:50,159 --> 01:31:51,679


2689
01:31:51,679 --> 01:31:52,880


2690
01:31:52,880 --> 01:31:54,480
что есть много места

2691
01:31:54,480 --> 01:31:55,920
для интересной аналитической работы, которая

2692
01:31:55,920 --> 01:31:59,840
требует  требует значительно меньше вычислений,

2693
01:32:00,639 --> 01:32:02,560
я думаю, я бы сказал

2694
01:32:02,560 --> 01:32:05,600
еще пару вещей, во-первых, я

2695
01:32:05,600 --> 01:32:07,520
действительно надеюсь, что эта область будет больше двигаться в

2696
01:32:07,520 --> 01:32:09,199
сторону разработки моделей сообщества и

2697
01:32:09,199 --> 01:32:11,120
движется к фреймворкам мобов,

2698
01:32:11,120 --> 01:32:12,719
которые позволяют людям

2699
01:32:12,719 --> 01:32:14,719
совместно тренировать модель,

2700
01:32:14,719 --> 01:32:16,960
например, как в распределенном режиме.

2701
01:32:16,960 --> 01:32:18,320
думаю, что это невероятно захватывающее

2702
01:32:18,320 --> 01:32:19,440
направление исследований, это то, над чем

2703
01:32:19,440 --> 01:32:21,840
я сейчас работаю с моими студентами в моей

2704
01:32:21,840 --> 01:32:24,560
лаборатории в UNC,

2705
01:32:24,560 --> 01:32:27,120
и последнее, что я скажу, ну

2706
01:32:27,120 --> 01:32:29,199
и обычно мне не нравится это говорить,

2707
01:32:29,199 --> 01:32:31,520
но я  я все равно скажу это, я действительно

2708
01:32:31,520 --> 01:32:33,440
думаю, что

2709
01:32:33,440 --> 01:32:36,080
наше поле часто подвергается своего рода

2710
01:32:36,080 --> 01:32:38,400
шаблону тик-так, когда мы показываем,

2711
01:32:38,400 --> 01:32:40,320
что что-то возможно в масштабе, а затем

2712
01:32:40,320 --> 01:32:41,840
мы показываем, что масштаб не является необходимым

2713
01:32:41,840 --> 01:32:44,080
для достижения того же самого, и в некоторой

2714
01:32:44,080 --> 01:32:45,280
степени вы могли бы  утверждают, что это

2715
01:32:45,280 --> 01:32:47,679
уже произошло с gpg3 в том смысле,

2716
01:32:47,679 --> 01:32:49,760
что мы видели, как gpt3 получил

2717
01:32:49,760 --> 01:32:51,679
выдающиеся результаты, например, на

2718
01:32:51,679 --> 01:32:54,239
суперклее, всего с 32 примерами на класс, а

2719
01:32:54,239 --> 01:32:56,080
затем была статья, в которой

2720
01:32:56,080 --> 01:32:58,159
предлагалось  его метод под названием ipet,

2721
01:32:58,159 --> 01:33:00,400
который, как мне кажется, является интерактивным

2722
01:33:00,400 --> 01:33:03,440
итеративным шаблоном, использующим обучение,

2723
01:33:03,440 --> 01:33:04,880
которое дает практически сопоставимую

2724
01:33:04,880 --> 01:33:06,719
производительность в значительно меньшей

2725
01:33:06,719 --> 01:33:09,360
модели с тем же объемом данных,

2726
01:33:09,360 --> 01:33:10,960


2727
01:33:10,960 --> 01:33:13,360
и вы знаете, вы можете указать

2728
01:33:13,360 --> 01:33:15,679
на другие примеры, а я лично хотел

2729
01:33:15,679 --> 01:33:17,840
бы  приписывают историю

2730
01:33:17,840 --> 01:33:20,159
изобретения внимания тому факту, что исследователи

2731
01:33:20,159 --> 01:33:21,760
из монреальского института обучения

2732
01:33:21,760 --> 01:33:24,560
алгоритмов не могли позволить себе машину

2733
01:33:24,560 --> 01:33:26,400
с графическим процессором, поэтому они не могли запускать гигантский lstm

2734
01:33:26,400 --> 01:33:28,159
в последовательности документов, поэтому им

2735
01:33:28,159 --> 01:33:30,159
нужно было изобрести что-то, что работало бы

2736
01:33:30,159 --> 01:33:31,440
лучше, но  не требовали такой большой

2737
01:33:31,440 --> 01:33:34,000
модели, поэтому они изобрели внимание,

2738
01:33:34,000 --> 01:33:35,920
конечно, это не лучший совет -

2739
01:33:35,920 --> 01:33:37,440
говорить кому-то, что они должны просто пойти

2740
01:33:37,440 --> 01:33:39,520
изобрести что-то меньшее, но я, по

2741
01:33:39,520 --> 01:33:40,960
крайней мере, надеюсь, что некоторые из этих вещей,

2742
01:33:40,960 --> 01:33:43,440
которые мы показали, являются  возможно в масштабе

2743
01:33:43,440 --> 01:33:45,920
а также возможны в гораздо меньшем

2744
01:33:45,920 --> 01:33:48,000
масштабе

2745
01:33:48,000 --> 01:33:49,040
спасибо

2746
01:33:49,040 --> 01:33:51,840
да

2747
01:33:52,000 --> 01:33:53,840
ладно я думаю,

2748
01:33:53,840 --> 01:33:56,080
что нет никого, кто бы поддержал

2749
01:33:56,080 --> 01:33:57,679
в данный момент может быть сейчас  настал момент для

2750
01:33:57,679 --> 01:33:59,840
Джона задать свой вопрос, но если у кого-

2751
01:33:59,840 --> 01:34:02,960
то есть вопросы, сейчас

2752
01:34:02,960 --> 01:34:05,520
хороший момент, чтобы перейти к делу,

2753
01:34:05,520 --> 01:34:07,199
я просто собирался задать вопрос от

2754
01:34:07,199 --> 01:34:12,320
QA, и я вошел и спросил его в прямом эфире, так

2755
01:34:12,400 --> 01:34:13,760
что да ладно, я скажу еще кое-что

2756
01:34:13,760 --> 01:34:16,840
просто быстро, а это значит, что вы знаете, эм

2757
01:34:16,840 --> 01:34:19,199
t5, я был очень похож на то, что я сказал, я был очень

2758
01:34:19,199 --> 01:34:20,880
взволнован, что мы достигли почти человеческих

2759
01:34:20,880 --> 01:34:23,520
характеристик на суперклее.

2760
01:34:23,520 --> 01:34:26,400


2761
01:34:26,400 --> 01:34:28,960


2762
01:34:28,960 --> 01:34:30,719
меньше с точки зрения количества параметров,

2763
01:34:30,719 --> 01:34:32,320
так что это как еще один

2764
01:34:32,320 --> 01:34:34,639
разумный пример, я имею в виду, что он все еще

2765
01:34:34,639 --> 01:34:38,000
довольно большой, но, по крайней мере, вы знаете,

2766
01:34:38,000 --> 01:34:39,840
когда делаете алгоритмические и

2767
01:34:39,840 --> 01:34:41,840
архитектурные улучшения, иногда

2768
01:34:41,840 --> 01:34:44,000
вы можете закрыть эти пробелы,

2769
01:34:44,000 --> 01:34:46,719
хорошо, спасибо, Колин, и позвольте  Вы,

2770
01:34:46,719 --> 01:34:48,800
гм, что угодно, выпейте пива и ложитесь спать или

2771
01:34:48,800 --> 01:34:49,240
что-то в этом роде

2772
01:34:49,240 --> 01:34:51,040
[Смех]

2773
01:34:51,040 --> 01:34:53,280
Да, да, звучит здорово, да, еще

2774
01:34:53,280 --> 01:34:54,880
раз спасибо за то, что пригласили меня, это такое

2775
01:34:54,880 --> 01:34:56,800
удовольствие, так что я должен сказать, если у

2776
01:34:56,800 --> 01:34:58,400
кого-то есть какие-то дополнительные вопросы, которые они

2777
01:34:58,400 --> 01:35:00,480
думают о  е позже я всегда рад

2778
01:35:00,480 --> 01:35:02,000
получать электронные письма о таких вещах,

2779
01:35:02,000 --> 01:35:03,679
вы знаете, это то, над чем мне нравится работать

2780
01:35:03,679 --> 01:35:04,719
, так

2781
01:35:04,719 --> 01:35:06,560
что да, еще раз спасибо за отличную

2782
01:35:06,560 --> 01:35:10,199
информативную беседу

