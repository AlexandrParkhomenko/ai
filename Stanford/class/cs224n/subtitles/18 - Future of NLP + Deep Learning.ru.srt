1
00:00:04,880 --> 00:00:07,440
Добрый день, ребята, добро пожаловать на

2
00:00:07,440 --> 00:00:09,920
лекцию 18. Сегодня мы поговорим о

3
00:00:09,920 --> 00:00:11,519
некоторых из последних и величайших

4
00:00:11,519 --> 00:00:13,759
достижений в нейронной НЛП,

5
00:00:13,759 --> 00:00:16,400
куда мы пришли и куда мы направляемся,

6
00:00:16,400 --> 00:00:18,720
Крис, просто чтобы быть уверенным, что

7
00:00:18,720 --> 00:00:19,760


8
00:00:19,760 --> 00:00:21,359
это мой подарок и что видно  с

9
00:00:21,359 --> 00:00:23,680
этой части все в порядке,

10
00:00:23,680 --> 00:00:25,119
вы

11
00:00:25,119 --> 00:00:28,080
видите хорошо, но никто из моих докладчиков не прав

12
00:00:28,080 --> 00:00:30,960


13
00:00:30,960 --> 00:00:31,760


14
00:00:31,760 --> 00:00:34,079


15
00:00:34,079 --> 00:00:36,079


16
00:00:36,079 --> 00:00:38,879


17
00:00:38,879 --> 00:00:41,040
правильно  вы должны

18
00:00:41,040 --> 00:00:42,879
были получить обратную связь сейчас, если не

19
00:00:42,879 --> 00:00:45,360
свяжитесь с коллегой, я думаю, вы знаете,

20
00:00:45,360 --> 00:00:48,160
что у нас были некоторые проблемы в последнюю минуту, но если

21
00:00:48,160 --> 00:00:51,920
это не решено, пожалуйста, свяжитесь с нами,

22
00:00:51,920 --> 00:00:53,600
наконец, отчеты по проекту должны быть представлены

23
00:00:53,600 --> 00:00:55,920
очень скоро, 16 марта, что является

24
00:00:55,920 --> 00:00:58,160
На следующей неделе на ed будет один вопрос

25
00:00:58,160 --> 00:00:59,120
о

26
00:00:59,120 --> 00:01:02,239
таблице лидеров, и последний день для

27
00:01:02,239 --> 00:01:05,199
призыва в таблицу лидеров - 19 марта,

28
00:01:05,199 --> 00:01:06,240


29
00:01:06,240 --> 00:01:08,320


30
00:01:08,320 --> 00:01:10,799
хорошо, поэтому сегодня мы начнем с разговора

31
00:01:10,799 --> 00:01:13,040
об очень больших языковых моделях

32
00:01:13,040 --> 00:01:15,600
и gpd3, которые имеют rec  После того, как мы приобрели

33
00:01:15,600 --> 00:01:18,240
большую популярность

34
00:01:18,240 --> 00:01:20,080
, мы более подробно рассмотрим

35
00:01:20,080 --> 00:01:22,080
композицию и обобщение

36
00:01:22,080 --> 00:01:25,360
этих нейронных моделей, в

37
00:01:25,360 --> 00:01:27,680
то время как модели-трансформеры, такие как bird

38
00:01:27,680 --> 00:01:29,680
и gpt, имеют действительно высокую производительность во

39
00:01:29,680 --> 00:01:32,079
всех тестах, они по-прежнему терпят неудачу при развертывании по-настоящему

40
00:01:32,079 --> 00:01:34,320
удивительным образом, как мы можем

41
00:01:34,320 --> 00:01:36,079
укрепить  наше понимание

42
00:01:36,079 --> 00:01:38,320
оценки этих моделей, чтобы они более

43
00:01:38,320 --> 00:01:41,439
точно отражали выполнение задач

44
00:01:41,439 --> 00:01:44,000
в реальном мире,

45
00:01:44,000 --> 00:01:45,920
и затем мы закончим разговором о том, как мы

46
00:01:45,920 --> 00:01:47,520
можем выйти за рамки этой действительно ограниченной

47
00:01:47,520 --> 00:01:49,840
парадигмы обучения моделей языку

48
00:01:49,840 --> 00:01:52,000
только через текст, и взглянем на языковую

49
00:01:52,000 --> 00:01:54,479
борьбу,

50
00:01:54,479 --> 00:01:56,399
наконец, я дам  несколько практических советов о

51
00:01:56,399 --> 00:01:58,640
том, как продвигаться вперед в ваших нейронных исследованиях НЛП,

52
00:01:58,640 --> 00:02:01,280
и это будет включать несколько

53
00:02:01,280 --> 00:02:04,000
практических советов для финального проекта,

54
00:02:04,000 --> 00:02:05,680


55
00:02:05,680 --> 00:02:06,799
хорошо,

56
00:02:06,799 --> 00:02:09,119
так что вы знаете, что этот луч действительно как

57
00:02:09,119 --> 00:02:11,760
бы захватывает,

58
00:02:11,760 --> 00:02:13,280
вы знаете, что на самом деле происходит в

59
00:02:13,280 --> 00:02:15,840
поле и  просто наша

60
00:02:15,840 --> 00:02:17,760
способность использовать немаркированные данные

61
00:02:17,760 --> 00:02:20,080
значительно увеличилась за последние несколько лет,

62
00:02:20,080 --> 00:02:22,080
и это было сделано  возможно благодаря

63
00:02:22,080 --> 00:02:24,800
достижениям не только в оборудовании, но и в

64
00:02:24,800 --> 00:02:27,840
системах, и нашем понимании

65
00:02:27,840 --> 00:02:30,239
подобного самоконтролируемого обучения, поэтому мы можем

66
00:02:30,239 --> 00:02:33,360
использовать много-много непредставленных данных,

67
00:02:33,360 --> 00:02:34,400


68
00:02:34,400 --> 00:02:36,800
так что на основе этого вот общий

69
00:02:36,800 --> 00:02:39,040
рецепт обучения представлению, который просто

70
00:02:39,040 --> 00:02:41,680
работает для вас  знать все, в основном, большинство

71
00:02:41,680 --> 00:02:42,879
модальностей,

72
00:02:42,879 --> 00:02:46,400
поэтому рецепт в основном

73
00:02:46,400 --> 00:02:48,879
таков, поэтому конвертируйте ваши данные, если они

74
00:02:48,879 --> 00:02:51,599
преобразованы в изображения

75
00:02:51,599 --> 00:02:54,560
или как будто это не

76
00:02:54,560 --> 00:02:56,480
так, это действительно агностик модальности, поэтому

77
00:02:56,480 --> 00:02:58,480
вы берете свои данные, если это изображения, текст

78
00:02:58,480 --> 00:02:59,760
или видео, и конвертируете их в

79
00:02:59,760 --> 00:03:02,000
последовательность целых чисел,

80
00:03:02,000 --> 00:03:03,519
и на втором шаге мы определяем функцию потерь,

81
00:03:03,519 --> 00:03:06,000
чтобы максимизировать вероятность данных или

82
00:03:06,000 --> 00:03:09,280
создать потерю шумоподавляющего автокодировщика,

83
00:03:09,280 --> 00:03:11,360
наконец, на третьем шаге тренируйтесь на больших и больших

84
00:03:11,360 --> 00:03:13,599
объемах данных,

85
00:03:13,599 --> 00:03:14,400


86
00:03:14,400 --> 00:03:16,319
некоторые свойства проявляются только тогда, когда мы

87
00:03:16,319 --> 00:03:17,840
увеличиваем размер модели, и это

88
00:03:17,840 --> 00:03:20,400
действительно удивительно  Факты о масштабе,

89
00:03:20,400 --> 00:03:23,200
поэтому, чтобы дать несколько примеров этого рецепта

90
00:03:23,200 --> 00:03:26,480
в действии, вот gpd3, который может научиться решать

91
00:03:26,480 --> 00:03:28,480
действительно нетривиальную

92
00:03:28,480 --> 00:03:30,879
задачу классификации с помощью всего двух демонстрационных  рационов,

93
00:03:30,879 --> 00:03:33,280
и мы скоро поговорим об этом подробнее.

94
00:03:33,280 --> 00:03:34,239


95
00:03:34,239 --> 00:03:36,400
Еще один пример, как мы видели в лекции 14,

96
00:03:36,400 --> 00:03:39,040
- это d5, который действительно эффективно закрывает

97
00:03:39,040 --> 00:03:40,799
книгу qa, сохраняя знания в

98
00:03:40,799 --> 00:03:43,440
параметрах,

99
00:03:43,680 --> 00:03:45,760
ну, наконец, просто поэтому я рассмотрел еще одну

100
00:03:45,760 --> 00:03:48,319
модальность, вот недавняя

101
00:03:48,319 --> 00:03:50,400


102
00:03:50,400 --> 00:03:52,480
модель генерации текста в изображение с  действительно впечатляющее

103
00:03:52,480 --> 00:03:55,840
обобщение с нулевым выстрелом,

104
00:03:56,400 --> 00:03:59,439
хорошо, так что теперь давайте поговорим о gpd3,

105
00:03:59,439 --> 00:04:01,680
так насколько на самом деле велики эти модели.

106
00:04:01,680 --> 00:04:03,439
Эта таблица представляет некоторые цифры,

107
00:04:03,439 --> 00:04:06,239
чтобы представить вещи в перспективе.

108
00:04:06,239 --> 00:04:08,400


109
00:04:08,400 --> 00:04:11,040


110
00:04:11,040 --> 00:04:13,840
своего рода основной продукт в nlp до 2016

111
00:04:13,840 --> 00:04:14,799


112
00:04:14,799 --> 00:04:16,720
года, вплоть до людей, у которых было 100

113
00:04:16,720 --> 00:04:19,120
триллионов синапсов, а некоторые из них посередине,

114
00:04:19,120 --> 00:04:20,880
у нас есть gbt2 с более чем миллиардом

115
00:04:20,880 --> 00:04:24,240
параметров и gpt3 с более чем 150

116
00:04:24,240 --> 00:04:26,320
миллиардами параметров,

117
00:04:26,320 --> 00:04:28,160
и это превышает количество синаптических

118
00:04:28,160 --> 00:04:31,600
соединений у пчелы  мозг

119
00:04:31,600 --> 00:04:33,600
так очевидно, что любой с небольшим

120
00:04:33,600 --> 00:04:35,600
познанием в нейробиологии и знает, что

121
00:04:35,600 --> 00:04:36,960
это не яблоки и апельсины

122
00:04:36,960 --> 00:04:38,479
сравнения эээ, это яблочный

123
00:04:38,479 --> 00:04:40,479
магазин в этом компе  arison, но

124
00:04:40,479 --> 00:04:42,160
дело в том, что масштаб этих

125
00:04:42,160 --> 00:04:44,000
моделей действительно начинает достигать астрономических

126
00:04:44,000 --> 00:04:45,360
цифр,

127
00:04:45,360 --> 00:04:46,240


128
00:04:46,240 --> 00:04:49,040
так что вот некоторые факты о gbt3

129
00:04:49,040 --> 00:04:51,440
, например, это большой трансформатор с 96

130
00:04:51,440 --> 00:04:52,960
слоями.

131
00:04:52,960 --> 00:04:53,759


132
00:04:53,759 --> 00:04:55,120


133
00:04:55,120 --> 00:04:57,360


134
00:04:57,360 --> 00:04:59,520
за исключением того, что для увеличения

135
00:04:59,520 --> 00:05:02,240
вычисления внимания он использует эти

136
00:05:02,240 --> 00:05:04,320
локально разбитые шаблоны редкого внимания,

137
00:05:04,320 --> 00:05:05,520
и я действительно рекомендую вам взглянуть

138
00:05:05,520 --> 00:05:07,199
на документ, чтобы понять детали.Причина, которую

139
00:05:07,199 --> 00:05:09,120
мы упомянули здесь, заключается в том, что

140
00:05:09,120 --> 00:05:10,720
это своего рода подчеркивает, что увеличение масштаба

141
00:05:10,720 --> 00:05:12,160
просто не просто меняет  гиперпараметры,

142
00:05:12,160 --> 00:05:14,240
как многие могут подумать, и это

143
00:05:14,240 --> 00:05:16,240
включает в себя действительно нетривиальную инженерию

144
00:05:16,240 --> 00:05:17,919
и алгоритмы, чтобы сделать вычисления

145
00:05:17,919 --> 00:05:20,400
эффективными,

146
00:05:20,479 --> 00:05:22,720
наконец, все это обучено на 500

147
00:05:22,720 --> 00:05:24,880
миллиардах токенов, взятых из общей

148
00:05:24,880 --> 00:05:28,010
википедии

149
00:05:28,010 --> 00:05:29,840
[Музыка],

150
00:05:29,840 --> 00:05:32,400
так что что нового в gpp3, так что давайте

151
00:05:32,400 --> 00:05:33,919
Давайте сначала посмотрим на некоторые результаты в

152
00:05:33,919 --> 00:05:36,160
документе, так что очевидно, что он лучше подходит

153
00:05:36,160 --> 00:05:37,840
для языкового моделирования и

154
00:05:37,840 --> 00:05:39,199
проблемы завершения текста  lems,

155
00:05:39,199 --> 00:05:41,759
как вы можете видеть из этой таблицы, он

156
00:05:41,759 --> 00:05:43,680
лучше, чем gpt2, при языковом моделировании

157
00:05:43,680 --> 00:05:46,000
в банке pentree, а также лучше при

158
00:05:46,000 --> 00:05:47,840
завершении истории в наборе данных завершения истории,

159
00:05:47,840 --> 00:05:50,160
называемом

160
00:05:50,160 --> 00:05:52,320
ambada, чтобы дать представление о том, что будет дальше,

161
00:05:52,320 --> 00:05:53,680
давайте подробнее рассмотрим

162
00:05:53,680 --> 00:05:55,919
набор данных о завершении этой истории Limbaugh,

163
00:05:55,919 --> 00:05:57,520
поэтому задача здесь состоит в том, чтобы нам дали

164
00:05:57,520 --> 00:05:59,440
короткий рассказ, и мы должны были

165
00:05:59,440 --> 00:06:00,639
заполнить последнее слово

166
00:06:00,639 --> 00:06:04,000
um, удовлетворяющее ограничениям

167
00:06:04,000 --> 00:06:05,360
uh задачи, может быть сложно для

168
00:06:05,360 --> 00:06:07,120
языковой модели, которая могла бы генерировать

169
00:06:07,120 --> 00:06:08,960
много-  Завершение слов

170
00:06:08,960 --> 00:06:11,120
с помощью gpd3. По-настоящему новинка заключается в том, что

171
00:06:11,120 --> 00:06:12,479
мы можем просто привести несколько примеров в качестве

172
00:06:12,479 --> 00:06:14,319
подсказок и как бы передать

173
00:06:14,319 --> 00:06:16,560
спецификацию задачи модели, и теперь gpt3

174
00:06:16,560 --> 00:06:18,080
знает, как завершение должно быть

175
00:06:18,080 --> 00:06:20,240
одним словом, это очень мощная

176
00:06:20,240 --> 00:06:22,319
парадигма, и мы  приведите еще несколько

177
00:06:22,319 --> 00:06:24,800
примеров этого в контексте обучения на еще

178
00:06:24,800 --> 00:06:27,280
нескольких слайдах,

179
00:06:27,280 --> 00:06:29,360
так что помимо языкового моделирования он

180
00:06:29,360 --> 00:06:31,759
действительно хорош в таких наукоемких

181
00:06:31,759 --> 00:06:34,960
задачах, как закрытая книга qa, а также

182
00:06:34,960 --> 00:06:36,560
понимание прочитанного  и здесь мы

183
00:06:36,560 --> 00:06:38,080
наблюдаем, что масштабирование наших параметров

184
00:06:38,080 --> 00:06:39,520
приводит к значительному повышению

185
00:06:39,520 --> 00:06:42,080
производительности,

186
00:06:42,160 --> 00:06:43,919
поэтому теперь давайте поговорим о контекстном

187
00:06:43,919 --> 00:06:47,280
обучении. uh gbt3 демонстрирует некоторый

188
00:06:47,280 --> 00:06:49,120
уровень быстрой адаптации к совершенно новым

189
00:06:49,120 --> 00:06:50,080
задачам.

190
00:06:50,080 --> 00:06:51,919
Это происходит с помощью того, что называется

191
00:06:51,919 --> 00:06:53,520
контекстным обучением,

192
00:06:53,520 --> 00:06:54,960
как показано на рисунке.

193
00:06:54,960 --> 00:06:56,960
обучение модели можно охарактеризовать как наличие

194
00:06:56,960 --> 00:06:58,319
внешнего цикла,

195
00:06:58,319 --> 00:07:00,240
который изучает набор параметров, которые

196
00:07:00,240 --> 00:07:02,960
делают изучение внутреннего цикла

197
00:07:02,960 --> 00:07:06,000
максимально эффективным, и, имея

198
00:07:06,000 --> 00:07:07,440
в виду такую структуру, мы действительно можем увидеть,

199
00:07:07,440 --> 00:07:09,520
как хорошая языковая модель может также

200
00:07:09,520 --> 00:07:13,039
служить  Хорошие несколько коротких учеников,

201
00:07:13,759 --> 00:07:15,680
поэтому в этом сегменте мы немного повеселимся

202
00:07:15,680 --> 00:07:17,440
с gpd3 и посмотрим на некоторые

203
00:07:17,440 --> 00:07:19,759
демонстрации этого в контексте

204
00:07:19,759 --> 00:07:20,720
обучения

205
00:07:20,720 --> 00:07:21,000
um

206
00:07:21,000 --> 00:07:22,319
[Music],

207
00:07:22,319 --> 00:07:24,800
так что для начала вот пример,

208
00:07:24,800 --> 00:07:26,240
когда кто-то пытается создать

209
00:07:26,240 --> 00:07:28,479
приложение, которое преобразует язык

210
00:07:28,479 --> 00:07:31,199
в язык  описание uh для bash одного

211
00:07:31,199 --> 00:07:33,520
языка первые три примера - это

212
00:07:33,520 --> 00:07:35,759
подсказки, за которыми следуют сгенерированные примеры

213
00:07:35,759 --> 00:07:37,599
из gpd3

214
00:07:37,599 --> 00:07:39,840
uh, поэтому он получает список запущенных

215
00:07:39,840 --> 00:07:41,919
это просто, вероятно,

216
00:07:41,919 --> 00:07:43,759
просто включает в себя просмотр хеш-таблицы,

217
00:07:43,759 --> 00:07:45,280
некоторые из наиболее сложных, которые

218
00:07:45,280 --> 00:07:47,039
включают копирование,

219
00:07:47,039 --> 00:07:49,680
вы знаете, что некоторые интервалы

220
00:07:49,680 --> 00:07:52,000
из текста, такие как пример scp

221
00:07:52,000 --> 00:07:54,080
, отчасти интересны, а также

222
00:07:54,080 --> 00:07:57,120
сложнее разобрать  Пример scp часто

223
00:07:57,120 --> 00:07:59,919
возникает в рабочие часы, поэтому

224
00:07:59,919 --> 00:08:03,440
gpd3 знает, как это сделать,

225
00:08:03,599 --> 00:08:05,360
вот несколько более сложный пример,

226
00:08:05,360 --> 00:08:06,879
когда модели дается

227
00:08:06,879 --> 00:08:09,039
описание базы данных на естественном языке,

228
00:08:09,039 --> 00:08:12,319
и она начинает имитировать это поведение,

229
00:08:12,319 --> 00:08:14,560
поэтому текст, выделенный жирным шрифтом, сортируется

230
00:08:14,560 --> 00:08:16,800
подсказки, предоставленной модели, подсказка

231
00:08:16,800 --> 00:08:20,000
включает в себя некоторую

232
00:08:20,000 --> 00:08:21,360
функциональную спецификацию функции того, что такое

233
00:08:21,360 --> 00:08:22,720
база данных,

234
00:08:22,720 --> 00:08:24,240
поэтому в нем говорится, что база данных начинает

235
00:08:24,240 --> 00:08:25,440
ничего не знать,

236
00:08:25,440 --> 00:08:26,879
база данных знает все, что

237
00:08:26,879 --> 00:08:28,879
к ней добавлено, база данных не знает

238
00:08:28,879 --> 00:08:30,000
ничего другого,

239
00:08:30,000 --> 00:08:31,360
и когда вы спрашиваете  вопрос к

240
00:08:31,360 --> 00:08:33,039
базе данных, если ответ есть в

241
00:08:33,039 --> 00:08:34,559
базе данных, база данных должна вернуть

242
00:08:34,559 --> 00:08:36,640
ответ, иначе она должна сказать, что

243
00:08:36,640 --> 00:08:38,159
не знает ответа,

244
00:08:38,159 --> 00:08:42,240
поэтому это  очень новый и очень мощный um,

245
00:08:42,240 --> 00:08:43,760
и вы знаете, что приглашение также включает

246
00:08:43,760 --> 00:08:46,399
некоторые примеры использования, поэтому, когда вы спрашиваете два

247
00:08:46,399 --> 00:08:48,640
плюс два, база данных не знает, вы

248
00:08:48,640 --> 00:08:50,160
спрашиваете столицу Франции, которую база данных

249
00:08:50,160 --> 00:08:52,800
не знает, а затем вы добавляете факт,

250
00:08:52,800 --> 00:08:55,040
что Том  20 лет базе данных,

251
00:08:55,040 --> 00:08:56,320
и теперь вы можете начать задавать ему

252
00:08:56,320 --> 00:08:58,800
вопросы, например, где живет Том,

253
00:08:58,800 --> 00:09:00,880
и, как и ожидалось, он говорит, что

254
00:09:00,880 --> 00:09:02,640
база данных не знает,

255
00:09:02,640 --> 00:09:05,519
но теперь, если вы спросите его, сколько лет тому

256
00:09:05,519 --> 00:09:07,200
Тому, база данных говорит, что Тому 20

257
00:09:07,200 --> 00:09:09,680
лет и  если вы спросите, сколько мне лет,

258
00:09:09,680 --> 00:09:11,279
база данных говорит,

259
00:09:11,279 --> 00:09:12,560
что в основном она не знает, потому

260
00:09:12,560 --> 00:09:14,000
что это не было добавлено, так что это действительно

261
00:09:14,000 --> 00:09:15,360
мощно,

262
00:09:15,360 --> 00:09:17,279
вот еще один,

263
00:09:17,279 --> 00:09:20,160
а теперь, в этом примере, модель

264
00:09:20,160 --> 00:09:22,720
просят объединить концепции вместе, и поэтому

265
00:09:22,720 --> 00:09:24,160
есть определение того, что  означает ли это

266
00:09:24,160 --> 00:09:26,560
смешивание концепций, поэтому, если вы возьмете

267
00:09:26,560 --> 00:09:28,240
самолет и автомобиль, вы можете смешать это с

268
00:09:28,240 --> 00:09:30,800
вашим летающим автомобилем

269
00:09:30,800 --> 00:09:32,080
, по сути,

270
00:09:32,080 --> 00:09:33,440
вы знаете, что в Википедии есть определение

271
00:09:33,440 --> 00:09:35,040
того, что концепция смешивания это

272
00:09:35,040 --> 00:09:36,399
концепция смешивания вместе с некоторыми

273
00:09:36,399 --> 00:09:38,000
электронными  xamples,

274
00:09:38,000 --> 00:09:39,839
а теперь давайте посмотрим на

275
00:09:39,839 --> 00:09:42,080
некоторые проблемы, за которыми следуют ответы gp3,

276
00:09:42,080 --> 00:09:43,519


277
00:09:43,519 --> 00:09:45,040
так что первое - это

278
00:09:45,040 --> 00:09:47,040
прямое двумерное пространство,

279
00:09:47,040 --> 00:09:49,279
смешанное с трехмерным пространством, дает 2,5-

280
00:09:49,279 --> 00:09:50,880
мерное пространство,

281
00:09:50,880 --> 00:09:53,200
одно, что несколько интересно,

282
00:09:53,200 --> 00:09:55,760
старое, а новое дает переработанное,

283
00:09:55,760 --> 00:09:57,519
мм

284
00:09:57,519 --> 00:09:59,680
затем треугольный квадрат дает трапецию,

285
00:09:59,680 --> 00:10:01,839
которая также

286
00:10:01,839 --> 00:10:03,839
интересна.Те, что действительно нетривиально,

287
00:10:03,839 --> 00:10:06,720
- это геология плюс неврология, используемая для

288
00:10:06,720 --> 00:10:08,480
седиментационной неврологии, и я понятия не имел,

289
00:10:08,480 --> 00:10:11,519
что это было, очевидно, правильно,

290
00:10:11,519 --> 00:10:14,079
так ясно, что он может делать эти очень

291
00:10:14,079 --> 00:10:16,000
гибкие вещи просто из  просто из

292
00:10:16,000 --> 00:10:18,399
подсказки,

293
00:10:18,399 --> 00:10:21,200
так что вот еще один класс

294
00:10:21,200 --> 00:10:23,040
примеров, который gbt3, как

295
00:10:23,040 --> 00:10:25,680
вы знаете, несколько прав, и

296
00:10:25,680 --> 00:10:27,839
это проблемы аналогии с подражанием,

297
00:10:27,839 --> 00:10:29,600
которые действительно хорошо

298
00:10:29,600 --> 00:10:31,920
изучены в когнитивной науке,

299
00:10:31,920 --> 00:10:33,440
и то, как это работает, заключается в том, что я

300
00:10:33,440 --> 00:10:35,920
собираюсь  дать вам несколько примеров, а затем попросить

301
00:10:35,920 --> 00:10:36,800
вас,

302
00:10:36,800 --> 00:10:38,320
как вы знаете, ввести функцию из

303
00:10:38,320 --> 00:10:40,399
этих примеров и применить ее к вам, применив ее

304
00:10:40,399 --> 00:10:42,959
к новым запросам, поэтому, если  bc меняет на

305
00:10:42,959 --> 00:10:45,680
abt то, что pqr меняется на well pqr

306
00:10:45,680 --> 00:10:47,680
должен измениться на pqs, потому что функция, которую

307
00:10:47,680 --> 00:10:49,360
мы узнали, заключается в том, что последняя буква

308
00:10:49,360 --> 00:10:51,360
должна быть увеличена на единицу

309
00:10:51,360 --> 00:10:53,760
, и эту функцию теперь люди могут

310
00:10:53,760 --> 00:10:56,240
применять к примерам таких, как вы знаете

311
00:10:56,240 --> 00:10:58,720
разные типы, поэтому  как uh p повторяется

312
00:10:58,720 --> 00:11:00,880
дважды q повторяется дважды r повторяется дважды

313
00:11:00,880 --> 00:11:02,959
много изменений нужно повторять дважды q

314
00:11:02,959 --> 00:11:05,760
повторять дважды и s повторять дважды ммм,

315
00:11:05,760 --> 00:11:08,160
и кажется, что gpd3 может

316
00:11:08,160 --> 00:11:10,640
исправить их более или менее,

317
00:11:10,640 --> 00:11:13,839
но проблема в том, что если вы, если вы

318
00:11:13,839 --> 00:11:16,480
попросите его обобщить, ну вы знаете

319
00:11:16,480 --> 00:11:19,360
примеры, которые имеют увеличивающееся количество

320
00:11:19,360 --> 00:11:21,519
повторений, которые были замечены в приглашении,

321
00:11:21,519 --> 00:11:23,760
он не может этого сделать, поэтому в этой

322
00:11:23,760 --> 00:11:26,240
ситуации вы спрашиваете его,

323
00:11:26,240 --> 00:11:27,120
вы знаете,

324
00:11:27,120 --> 00:11:29,440
проведите аналогию, где

325
00:11:29,440 --> 00:11:31,279
буквы повторяются четыре

326
00:11:31,279 --> 00:11:32,880
раз, и никогда не было видно, чтобы раньше он

327
00:11:32,880 --> 00:11:34,399
не знал, что делать,

328
00:11:34,399 --> 00:11:36,880
и поэтому он все это неправильно

329
00:11:36,880 --> 00:11:38,160
понимает, поэтому вы знаете, что здесь есть смысл сказать

330
00:11:38,160 --> 00:11:39,680
об э-э,

331
00:11:39,680 --> 00:11:41,600
точно так же, как, может быть, этих подсказок

332
00:11:41,600 --> 00:11:44,000
недостаточно, чтобы передать э-э, вы знаете

333
00:11:44,000 --> 00:11:45,360
функцию  модель должна изучать

334
00:11:45,360 --> 00:11:46,640
и, возможно, даже больше примеров, которые вы

335
00:11:46,640 --> 00:11:49,120
можете изучить, но дело в том, что

336
00:11:49,120 --> 00:11:51,760
она, вероятно, не эм, вероятно, она,

337
00:11:51,760 --> 00:11:53,120
вероятно, не имеет такого же рода

338
00:11:53,120 --> 00:11:55,200
обобщения, как у людей,

339
00:11:55,200 --> 00:11:56,959
и это подводит нас к своего рода

340
00:11:56,959 --> 00:11:59,440
ограничению  эти модули и

341
00:11:59,440 --> 00:12:01,680
некоторые открытые вопросы, поэтому, просто глядя

342
00:12:01,680 --> 00:12:03,200
на статью и понимаете, проходя

343
00:12:03,200 --> 00:12:04,560
через результаты,

344
00:12:04,560 --> 00:12:06,639
кажется, что модель плохо

345
00:12:06,639 --> 00:12:08,079


346
00:12:08,079 --> 00:12:09,519


347
00:12:09,519 --> 00:12:11,680
справляется с логико-логическим и математическим рассуждением всего, что включает в себя выполнение нескольких

348
00:12:11,680 --> 00:12:14,399
шагов рассуждения, и это объясняет,

349
00:12:14,399 --> 00:12:16,480
почему она плохо справляется с  арифметика, почему она плохо

350
00:12:16,480 --> 00:12:18,800
справляется с проблемами работы, почему она не очень хороша для

351
00:12:18,800 --> 00:12:21,680
создания аналогий и даже как традиционные

352
00:12:21,680 --> 00:12:23,360
текстовые наборы данных, которые, кажется

353
00:12:23,360 --> 00:12:27,440
, требуют логических рассуждений, таких как rte,

354
00:12:27,519 --> 00:12:29,839
поэтому второй наиболее тонкий момент заключается в том, что

355
00:12:29,839 --> 00:12:32,160
неясно, как мы можем вносить постоянные

356
00:12:32,160 --> 00:12:33,600
обновления в модель, например  возможно, если я

357
00:12:33,600 --> 00:12:35,519
захочу научить модель новой концепции

358
00:12:35,519 --> 00:12:37,600
, это можно сделать,

359
00:12:37,600 --> 00:12:39,040
пока я взаимодействую с системой,

360
00:12:39,040 --> 00:12:41,040
но после того, как взаимодействие закончится

361
00:12:41,040 --> 00:12:42,800
он как бы перезапускается и не имеет

362
00:12:42,800 --> 00:12:44,880
понятия знания, и дело не в том, что

363
00:12:44,880 --> 00:12:46,480
это то, что модель не может

364
00:12:46,480 --> 00:12:48,399
сделать в принципе, а просто в

365
00:12:48,399 --> 00:12:49,750
том, что на самом деле не исследовалось

366
00:12:49,750 --> 00:12:51,279
[Музыка]

367
00:12:51,279 --> 00:12:53,440
эм, похоже, что это не демонстрирует человеческих

368
00:12:53,440 --> 00:12:54,880
обобщений, которые  часто называют

369
00:12:54,880 --> 00:12:57,120
систематичностью, и я буду говорить об этом гораздо

370
00:12:57,120 --> 00:12:58,480
больше,

371
00:12:58,480 --> 00:13:00,560
и, наконец, язык определен, и

372
00:13:00,560 --> 00:13:02,959
gpt3 просто учится на основе текста, и

373
00:13:02,959 --> 00:13:04,639
нет никакого взаимодействия с другими модальностями

374
00:13:04,639 --> 00:13:07,200
, нет взаимодействия, поэтому, возможно,

375
00:13:07,200 --> 00:13:08,800
аспекты значения, которые он требует,

376
00:13:08,800 --> 00:13:10,480
как бы несколько ограничены и  возможно, нам

377
00:13:10,480 --> 00:13:12,079
следует изучить, как мы можем ввести другие

378
00:13:12,079 --> 00:13:13,440
методы,

379
00:13:13,440 --> 00:13:16,079
поэтому мы поговорим намного больше об этих

380
00:13:16,079 --> 00:13:19,120
последних, многих последних ограничениях в

381
00:13:19,120 --> 00:13:20,399
оставшейся части лекции,

382
00:13:20,399 --> 00:13:22,320
но, возможно, я могу задать несколько вопросов,

383
00:13:22,320 --> 00:13:23,600
если есть

384
00:13:23,600 --> 00:13:26,669
[Музыка]

385
00:13:32,839 --> 00:13:35,440
какие-то i  не думаю, что есть большой

386
00:13:35,440 --> 00:13:38,800
нерешенный вопрос, но я имею в виду, что я думаю, что

387
00:13:38,800 --> 00:13:42,399
некоторые люди не совсем

388
00:13:42,399 --> 00:13:45,440
понимают, что вы знаете, несколько настроек выстрела и подсказки по

389
00:13:45,440 --> 00:13:47,279
сравнению с обучением, и я думаю, что на

390
00:13:47,279 --> 00:13:48,800
самом деле было бы хорошо объяснить  в этом немного

391
00:13:48,800 --> 00:13:49,600
больше,

392
00:13:49,600 --> 00:13:51,440
хорошо,

393
00:13:51,440 --> 00:13:52,800


394
00:13:52,800 --> 00:13:55,440
так что, может быть,

395
00:13:55,440 --> 00:13:59,199
позвольте мне выбрать простой пример,

396
00:14:00,839 --> 00:14:04,480
позвольте мне выбрать этот пример здесь,

397
00:14:04,480 --> 00:14:08,639
так что подсказка просто означает, что gpd3,

398
00:14:08,639 --> 00:14:10,079
как если бы вы вернулись к первым принципам,

399
00:14:10,079 --> 00:14:11,920
правильно gbt3 - это в основном просто языковая

400
00:14:11,920 --> 00:14:15,279
модель и  что это означает, э-э, учитывая

401
00:14:15,279 --> 00:14:16,880
контекст, он расскажет вам, какова

402
00:14:16,880 --> 00:14:20,720
вероятность следующего слова, поэтому,

403
00:14:20,720 --> 00:14:22,800
если я дам ему контекст, от

404
00:14:22,800 --> 00:14:26,160
w1 до wk uh gpd3 скажет мне,

405
00:14:26,160 --> 00:14:27,920
какова вероятность

406
00:14:27,920 --> 00:14:31,440
того, что для вас откроется w uh k плюс один

407
00:14:31,440 --> 00:14:33,760
словарный запас,

408
00:14:33,760 --> 00:14:35,680
вот что такое языковая модель,

409
00:14:35,680 --> 00:14:37,760
приглашение - это, по сути, контекст

410
00:14:37,760 --> 00:14:40,160
, который предварительно изменяется до того, как gt3 может

411
00:14:40,160 --> 00:14:42,639
начать создание,

412
00:14:42,639 --> 00:14:44,560
а то, что происходит в обучении контекста,

413
00:14:44,560 --> 00:14:46,560
- это

414
00:14:46,560 --> 00:14:48,720
контекст, который вы добавляете, что

415
00:14:48,720 --> 00:14:50,959
вы, что вы  pre-pen для gp3

416
00:14:50,959 --> 00:14:53,600
- это в основном примеры xy

417
00:14:53,600 --> 00:14:54,639
um,

418
00:14:54,639 --> 00:14:57,360
так что это подсказка, и

419
00:14:57,360 --> 00:14:59,760
причина, по которой это также, uh,

420
00:14:59,760 --> 00:15:01,600
это эквивалентно небольшому количеству короткого обучения, заключается в том,

421
00:15:01,600 --> 00:15:04,160
что вы предварительно сгибаете небольшое количество

422
00:15:04,160 --> 00:15:07,040
xy примеров, поэтому в этом случае, если я просто

423
00:15:07,040 --> 00:15:09,279
добавлю это uh  этот один пример, который

424
00:15:09,279 --> 00:15:11,839
выделен фиолетовым цветом, то это, по

425
00:15:11,839 --> 00:15:13,360
сути, одноразовое обучение, потому что я

426
00:15:13,360 --> 00:15:16,560
просто даю ему один пример в качестве контекста,

427
00:15:16,560 --> 00:15:18,959
и теперь, как дано, вы знаете, с

428
00:15:18,959 --> 00:15:21,360
этим запросом, который также добавляется из-

429
00:15:21,360 --> 00:15:23,519
за модели, которую он должен сделать для прогноза,

430
00:15:23,519 --> 00:15:24,399


431
00:15:24,399 --> 00:15:25,680


432
00:15:25,680 --> 00:15:27,760
так что  формат ввода-вывода такой же,

433
00:15:27,760 --> 00:15:30,639
как и в случае с несколькими учащимися,

434
00:15:30,639 --> 00:15:34,000
но поскольку это языковая модель,

435
00:15:34,000 --> 00:15:35,920
набор обучающих данных по существу

436
00:15:35,920 --> 00:15:40,360
представлен в виде контекста,

437
00:15:43,519 --> 00:15:45,360
поэтому кто-то все еще спрашивает, можете ли вы

438
00:15:45,360 --> 00:15:47,279
более конкретно рассказать о настройках обучения в контексте,

439
00:15:47,279 --> 00:15:50,720
что  это задача

440
00:15:50,720 --> 00:15:53,519
правильная,

441
00:15:53,519 --> 00:15:57,199
так что давай посмотрим, может, я смогу перейти к

442
00:15:57,199 --> 00:15:59,440
эм,

443
00:16:01,360 --> 00:16:03,680
да, может, я смогу перейти к этому слайду,

444
00:16:03,680 --> 00:16:06,160
так что задача

445
00:16:06,160 --> 00:16:08,560
просто в том, что я это языковая модель, поэтому

446
00:16:08,560 --> 00:16:10,079
она получает контекст,

447
00:16:10,079 --> 00:16:13,440
который представляет собой просто последовательность токенов и

448
00:16:13,440 --> 00:16:16,079
задача состоит в том, чтобы вы знали, что у

449
00:16:16,079 --> 00:16:18,639
вас есть последовательность токенов, а

450
00:16:18,639 --> 00:16:19,360


451
00:16:19,360 --> 00:16:21,519
затем модель должна сгенерировать заданную

452
00:16:21,519 --> 00:16:23,040
последовательность токенов,

453
00:16:23,040 --> 00:16:24,959
и способ, которым вы можете преобразовать это в

454
00:16:24,959 --> 00:16:26,720
реальную проблему классификации машинного обучения,

455
00:16:26,720 --> 00:16:27,680


456
00:16:27,680 --> 00:16:29,440
заключается в том, что ну

457
00:16:29,440 --> 00:16:31,680
так  для этого примера, возможно, вы дадите ему 5

458
00:16:31,680 --> 00:16:33,600
плюс 8 равно 13

459
00:16:33,600 --> 00:16:36,639
7 плюс 2 равно 9, а затем один плюс ноль

460
00:16:36,639 --> 00:16:39,680
равно, и теперь gpd3 может заполнить,

461
00:16:39,680 --> 00:16:42,000
вы знаете число, так что как

462
00:16:42,000 --> 00:16:44,000
вы конвертируете его в проблему классификации,

463
00:16:44,000 --> 00:16:46,240
контекст здесь будет  эти

464
00:16:46,240 --> 00:16:48,000
два примера

465
00:16:48,000 --> 00:16:50,079
арифметики, например, пять плюс восемь

466
00:16:50,079 --> 00:16:51,440
равны тринадцати и семь плюс два

467
00:16:51,440 --> 00:16:53,600
равны девяти, а затем запрос - один

468
00:16:53,600 --> 00:16:55,920
плюс ноль равно, а затем модель,

469
00:16:55,920 --> 00:16:57,600
поскольку это просто языковая модель, должна

470
00:16:57,600 --> 00:16:59,680
заполнить единицу плюс ноль равно знак вопроса,

471
00:16:59,680 --> 00:17:01,360
чтобы она  заполняет что-то, что

472
00:17:01,360 --> 00:17:02,720
не должно заполнять числа, он может

473
00:17:02,720 --> 00:17:05,439
заполнить что угодно, и но если он

474
00:17:05,439 --> 00:17:09,359
заполняет единицу, вы знаете, что он выполняет правильную работу,

475
00:17:09,359 --> 00:17:11,119
так что вы можете взять, как

476
00:17:11,119 --> 00:17:13,119
языковую модель, и сделать с ней несколько тренировок

477
00:17:13,119 --> 00:17:14,880


478
00:17:14,880 --> 00:17:17,359
Я буду продолжать отвечать на эти вопросы, чем

479
00:17:17,359 --> 00:17:19,520
контекстное обучение отличается от трансферного

480
00:17:19,520 --> 00:17:21,919
обучения,

481
00:17:23,919 --> 00:17:26,079
так что я предполагаю, что

482
00:17:26,079 --> 00:17:27,359


483
00:17:27,359 --> 00:17:29,360
в контексте обучения вы можете

484
00:17:29,360 --> 00:17:31,280
думать о контекстном обучении как о

485
00:17:31,280 --> 00:17:33,760
разновидности трансферного обучения, но как о

486
00:17:33,760 --> 00:17:35,600
переносном обучении  не указывать

487
00:17:35,600 --> 00:17:37,520
механизм, через который будет происходить передача

488
00:17:37,520 --> 00:17:40,480
в контексте обучения.Механизм обучения

489
00:17:40,480 --> 00:17:43,200
заключается в том,

490
00:17:43,200 --> 00:17:44,960
что обучающие примеры как бы

491
00:17:44,960 --> 00:17:46,559
добавляются к модели,

492
00:17:46,559 --> 00:17:49,520
которая является языковой моделью, просто, вы

493
00:17:49,520 --> 00:17:50,640
знаете,

494
00:17:50,640 --> 00:17:53,360
по порядку, так что допустим, у вас есть

495
00:17:53,360 --> 00:17:56,000
xyx one y one x  два и два, и

496
00:17:56,000 --> 00:17:58,320
они просто добавляются непосредственно к модели,

497
00:17:58,320 --> 00:18:00,720
и теперь он делает прогноз на основе того, что вы знаете

498
00:18:00,720 --> 00:18:02,720
какой-то запрос,

499
00:18:02,720 --> 00:18:04,080
некоторые запросы, которые взяты из

500
00:18:04,080 --> 00:18:07,200
этого набора данных, так что да, это

501
00:18:07,200 --> 00:18:09,039
подкатегория трансферного обучения, но

502
00:18:09,039 --> 00:18:11,280
переносного обучения  не указывает

503
00:18:11,280 --> 00:18:13,600
точно, как достигается это переносное обучение,

504
00:18:13,600 --> 00:18:15,360
но в контексте обучения очень

505
00:18:15,360 --> 00:18:17,919
специфично и говорится, что для языковых

506
00:18:17,919 --> 00:18:20,240
моделей вы можете по существу объединить

507
00:18:20,240 --> 00:18:22,160
набор обучающих данных, а затем представить

508
00:18:22,160 --> 00:18:25,200
это языковой модели,

509
00:18:25,360 --> 00:18:28,160
люди все еще недостаточно понимают

510
00:18:28,160 --> 00:18:31,039
, что такое  или не происходит с

511
00:18:31,039 --> 00:18:32,160
обучением

512
00:18:32,160 --> 00:18:33,760
и подсказками,

513
00:18:33,760 --> 00:18:35,760
поэтому вы знаете, что другой вопрос заключается в том, что в

514
00:18:35,760 --> 00:18:38,160
контексте обучения все еще требуется точная настройка

515
00:18:38,160 --> 00:18:41,039
вопросительного знака, нам нужно обучить gpt 3 to

516
00:18:41,039 --> 00:18:43,039
d  o в контексте обучения

517
00:18:43,039 --> 00:18:44,480
вопросительный знак

518
00:18:44,480 --> 00:18:46,320
правильно,

519
00:18:46,320 --> 00:18:47,679


520
00:18:47,679 --> 00:18:49,520
так что есть две части этого вопроса

521
00:18:49,520 --> 00:18:50,960
правильно,

522
00:18:50,960 --> 00:18:53,600
так что ответ - да и нет, поэтому,

523
00:18:53,600 --> 00:18:56,240
конечно, модель является языковой моделью,

524
00:18:56,240 --> 00:18:58,320
поэтому ее необходимо обучить, поэтому вы начинаете

525
00:18:58,320 --> 00:19:00,720
с некоторых случайных параметров  и вам нужно

526
00:19:00,720 --> 00:19:03,200
обучить их, но модель правильно обучена

527
00:19:03,200 --> 00:19:04,880
как языковая модель,

528
00:19:04,880 --> 00:19:07,520
и как только модель обучена

529
00:19:07,520 --> 00:19:10,000
, теперь вы можете использовать ее для передачи

530
00:19:10,000 --> 00:19:10,960
обучения,

531
00:19:10,960 --> 00:19:13,760
а параметры модели в контексте

532
00:19:13,760 --> 00:19:15,600
обучения исправлены, вы не обновляете

533
00:19:15,600 --> 00:19:17,520
параметры модели

534
00:19:17,520 --> 00:19:20,400
все  вы делаете это, вы даете ему эти,

535
00:19:20,400 --> 00:19:22,880
вы знаете, небольшой обучающий набор для модели,

536
00:19:22,880 --> 00:19:24,720
который просто добавляется к модели в качестве

537
00:19:24,720 --> 00:19:25,760
контекста,

538
00:19:25,760 --> 00:19:27,679
и теперь модель может начать генерировать

539
00:19:27,679 --> 00:19:29,520
с этой точки,

540
00:19:29,520 --> 00:19:31,440
поэтому в этом примере,

541
00:19:31,440 --> 00:19:33,919
если 5 минус 8 равно 13 и 7 плюс  2

542
00:19:33,919 --> 00:19:37,760
равно 9 - это два примера xy

543
00:19:37,760 --> 00:19:39,120
в

544
00:19:39,120 --> 00:19:40,480
ванильном обучении передачи данных, что вы бы

545
00:19:40,480 --> 00:19:42,000
сделали, так это то, что вы бы сделали несколько больших

546
00:19:42,000 --> 00:19:44,080
шагов, обновите параметры вашей модели, а

547
00:19:44,080 --> 00:19:45,919
затем сделайте прогноз на единицу плюс ноль,

548
00:19:45,919 --> 00:19:48,320
равный тому, что правильно, но в контексте

549
00:19:48,320 --> 00:19:50,840
l  зарабатывая все, что вы делаете, вы просто

550
00:19:50,840 --> 00:19:52,720
объединяете

551
00:19:52,720 --> 00:19:55,919
5 плюс 8 равно 13 и 7 плюс 2 равно 9

552
00:19:55,919 --> 00:19:58,320
с окном контекста модели, а затем

553
00:19:58,320 --> 00:20:01,600
заставляете его предсказать, какой один плюс 0

554
00:20:01,600 --> 00:20:05,240
должен быть

555
00:20:08,720 --> 00:20:10,159


556
00:20:10,159 --> 00:20:13,360
равен  Вопрос общей картины:

557
00:20:13,360 --> 00:20:15,360
знаете ли вы о каких-либо исследованиях, сочетающих

558
00:20:15,360 --> 00:20:17,360
эти модели с обучением с подкреплением

559
00:20:17,360 --> 00:20:20,240
для более сложных задач рассуждений,

560
00:20:20,240 --> 00:20:22,400
так что это отличный вопрос, а

561
00:20:22,400 --> 00:20:25,520
есть недавняя работа по

562
00:20:25,520 --> 00:20:27,360
попыткам согласовать

563
00:20:27,360 --> 00:20:29,280
языковые модели с человеческими

564
00:20:29,280 --> 00:20:31,120
предпочтениями, где

565
00:20:31,120 --> 00:20:32,880
да  это похоже на то, что вы знаете некоторое

566
00:20:32,880 --> 00:20:35,280
количество тонкой настройки

567
00:20:35,280 --> 00:20:36,720
с обучением с подкреплением на основе

568
00:20:36,720 --> 00:20:38,720
подобных предпочтений людей, поэтому,

569
00:20:38,720 --> 00:20:40,159
возможно, вы хотите провести лето, когда вы

570
00:20:40,159 --> 00:20:42,480
хотите решить задачу с обобщением с помощью gbd3,

571
00:20:42,480 --> 00:20:43,840
и модель создает несколько

572
00:20:43,840 --> 00:20:46,559
резюме, и для каждого резюме, возможно, у вас

573
00:20:46,559 --> 00:20:48,960
есть  награда, которая, по сути, является

574
00:20:48,960 --> 00:20:50,559
человеческим предпочтением, например, может быть, я хочу

575
00:20:50,559 --> 00:20:51,919
включить некоторые факты и не хочу

576
00:20:51,919 --> 00:20:54,320
включать вы знаете некоторые другие эээ

577
00:20:54,320 --> 00:20:56,240
неважные факты и  Я могу

578
00:20:56,240 --> 00:20:58,400
построить вознаграждение из этого, и я могу

579
00:20:58,400 --> 00:21:00,799
точно настроить параметры моей

580
00:21:00,799 --> 00:21:03,840
языковой модели, в основном используя обучение с

581
00:21:03,840 --> 00:21:05,919
подкреплением, основанное на этой

582
00:21:05,919 --> 00:21:07,440
награде, которая, по сути, является человеческими

583
00:21:07,440 --> 00:21:09,200
предпочтениями, так что есть совсем

584
00:21:09,200 --> 00:21:11,120
недавняя работа, которая пытается сделать это, но

585
00:21:11,120 --> 00:21:13,200
я '  Я не уверен, да, я не знаю

586
00:21:13,200 --> 00:21:14,880
ни одной работы, в которой пытались бы использовать

587
00:21:14,880 --> 00:21:17,200
обучение с подкреплением, чтобы научить ее рассуждать. Я использовал

588
00:21:17,200 --> 00:21:19,120
эти модели, но я думаю, что это

589
00:21:19,120 --> 00:21:23,039
интересное направление будущего для исследования,

590
00:21:28,720 --> 00:21:31,120
может, вам стоит продолжить на этом этапе,

591
00:21:31,120 --> 00:21:33,360
хорошо,

592
00:21:37,280 --> 00:21:39,600
хорошо, так что мы »  Я расскажу немного больше об

593
00:21:39,600 --> 00:21:41,360
этих последних двух пунктах, так что

594
00:21:41,360 --> 00:21:42,880
систематичность

595
00:21:42,880 --> 00:21:44,240
и

596
00:21:44,240 --> 00:21:46,400
языковое основание,

597
00:21:46,400 --> 00:21:48,000
ммм,

598
00:21:48,000 --> 00:21:50,400
так что для начала, как вы

599
00:21:50,400 --> 00:21:52,240
определяете систематичность, так что на самом деле

600
00:21:52,240 --> 00:21:53,760
определение состоит в

601
00:21:53,760 --> 00:21:55,600
том, что существует определенный и предсказуемый

602
00:21:55,600 --> 00:21:58,000
образец среди предложений, которые

603
00:21:58,000 --> 00:22:00,080
носители языка  понять,

604
00:22:00,080 --> 00:22:02,480
и поэтому есть систематический образец

605
00:22:02,480 --> 00:22:04,240
среди предложений, что мы понимаем,

606
00:22:04,240 --> 00:22:06,320
что это означает, допустим, есть

607
00:22:06,320 --> 00:22:08,240
предложение, например, Джон любит меня

608
00:22:08,240 --> 00:22:09,919
правильно, и если носитель языка

609
00:22:09,919 --> 00:22:11,120
понимает предложение, тогда они

610
00:22:11,120 --> 00:22:12,799
также должны быть в состоянии понять

611
00:22:12,799 --> 00:22:16,159
предложение, которое Мэри любит Джона,

612
00:22:16,720 --> 00:22:18,240
и с этой идеей

613
00:22:18,240 --> 00:22:19,919
систематичности тесно связан принцип

614
00:22:19,919 --> 00:22:22,480
композиционности, и сейчас я собираюсь,

615
00:22:22,480 --> 00:22:23,200


616
00:22:23,200 --> 00:22:24,799
вы знаете, игнорировать определение

617
00:22:24,799 --> 00:22:26,240
Монтагю и просто взглянуть на  грубое

618
00:22:26,240 --> 00:22:27,840
определение, а затем мы можем вернуться к

619
00:22:27,840 --> 00:22:30,320
этому другому, как к более конкретному определению,

620
00:22:30,320 --> 00:22:31,919
грубое определение по сути состоит в том,

621
00:22:31,919 --> 00:22:33,600
что значение выражения

622
00:22:33,600 --> 00:22:35,120
является функцией значения его

623
00:22:35,120 --> 00:22:37,520
частей,

624
00:22:37,600 --> 00:22:39,600
поэтому мы подходим к вопросу,

625
00:22:39,600 --> 00:22:42,320
действительно ли человеческие языки являются композиционными, и

626
00:22:42,320 --> 00:22:44,320
вот  некоторые примеры, которые

627
00:22:44,320 --> 00:22:46,400
вы знаете, заставляют нас думать, что, может быть,

628
00:22:46,400 --> 00:22:49,760
да, например, если вы посмотрите,

629
00:22:49,760 --> 00:22:51,919
что означает существительная фраза коричневая корова,

630
00:22:51,919 --> 00:22:54,799
так что она состоит из значения

631
00:22:54,799 --> 00:22:58,400
прилагательного коричневый и существительного корова,

632
00:22:58,400 --> 00:22:59,919
так что все, что есть  Коричневый и

633
00:22:59,919 --> 00:23:01,840
все это корова, возьмите перекресток

634
00:23:01,840 --> 00:23:04,080
и получите коричневых коров, похожих на красных кроликов,

635
00:23:04,080 --> 00:23:05,280
так что все, что красное, все

636
00:23:05,280 --> 00:23:07,520
вещи, скорее объединяющие их, становятся красными.

637
00:23:07,520 --> 00:23:09,120
а затем ударить по мячу эту рабочую фразу

638
00:23:09,120 --> 00:23:11,520
можно понять, поскольку у вас есть агент,

639
00:23:11,520 --> 00:23:13,679
который, как вы знаете, выполняет

640
00:23:13,679 --> 00:23:15,760
аналогичную операцию по мячу ногой,

641
00:23:15,760 --> 00:23:17,840
но это не всегда тот случай

642
00:23:17,840 --> 00:23:20,559
, когда вы можете понять

643
00:23:20,559 --> 00:23:23,120
смысл всего, комбинируя значения

644
00:23:23,120 --> 00:23:24,720
частей, так что здесь у нас есть несколько встречных

645
00:23:24,720 --> 00:23:27,200
примеров, которые люди часто используют, так что

646
00:23:27,200 --> 00:23:28,960
отвлекающий маневр не означает все,

647
00:23:28,960 --> 00:23:30,080
что красное, и все, что

648
00:23:30,080 --> 00:23:32,480
движется и пинает ведро,

649
00:23:32,480 --> 00:23:33,840
определенно не означает, что есть

650
00:23:33,840 --> 00:23:36,480
агент, который пинает ведро, так что э-э

651
00:23:36,480 --> 00:23:38,559
хотя такие примеры, как

652
00:23:38,559 --> 00:23:40,559
предполагается, должны быть провокационными, поскольку мы думаем,

653
00:23:40,559 --> 00:23:41,840
что язык подобен в основном

654
00:23:41,840 --> 00:23:44,000
композиционному, есть много исключений,

655
00:23:44,000 --> 00:23:44,880
но

656
00:23:44,880 --> 00:23:46,799
для подавляющего большинства предложений, которые

657
00:23:46,799 --> 00:23:48,720
мы никогда не слышали, мы можем

658
00:23:48,720 --> 00:23:50,159
понять, что они означают, собирая по кусочкам

659
00:23:50,159 --> 00:23:52,480
слова, которые  предложение

660
00:23:52,480 --> 00:23:53,760
состоит из,

661
00:23:53,760 --> 00:23:55,520
и это означает, что, возможно,

662
00:23:55,520 --> 00:23:57,120
композиционность представлений

663
00:23:57,120 --> 00:23:58,960
полезна до того, что может привести к

664
00:23:58,960 --> 00:24:01,200
систематичности в поведении

665
00:24:01,200 --> 00:24:02,240


666
00:24:02,240 --> 00:24:03,919
и т.  Это подводит нас к вопросам, которые

667
00:24:03,919 --> 00:24:05,919
мы задаем в этом сегменте, являются нейронными

668
00:24:05,919 --> 00:24:08,000
репрезентациями композиционными, и

669
00:24:08,000 --> 00:24:09,760
второй вопрос заключается в

670
00:24:09,760 --> 00:24:13,279
том, систематически ли они обобщают,

671
00:24:13,279 --> 00:24:14,320


672
00:24:14,320 --> 00:24:15,200
так

673
00:24:15,200 --> 00:24:16,799
как вы вообще можете

674
00:24:16,799 --> 00:24:19,200
измерить, демонстрируют ли репрезентации, которые

675
00:24:19,200 --> 00:24:20,480
обучаются в вашей сети,

676
00:24:20,480 --> 00:24:22,880
композиционность,

677
00:24:22,880 --> 00:24:25,200
так что давайте, давай вернемся назад  к этому

678
00:24:25,200 --> 00:24:27,039
определению из логики u,

679
00:24:27,039 --> 00:24:29,279
которая говорит, что композиционность

680
00:24:29,279 --> 00:24:31,440
связана с существованием гомоморфизма

681
00:24:31,440 --> 00:24:33,840
от синтаксиса к чему-то

682
00:24:33,840 --> 00:24:36,799
um, и чтобы посмотреть, что у нас есть этот

683
00:24:36,799 --> 00:24:39,200
пример, который является lisa is not

684
00:24:39,200 --> 00:24:42,080
skateboard, и у нас есть синтаксическое дерево uh,

685
00:24:42,080 --> 00:24:44,240
соответствующее этому примеру

686
00:24:44,240 --> 00:24:46,640
и  значение предложения может быть

687
00:24:46,640 --> 00:24:47,840
составлено

688
00:24:47,840 --> 00:24:50,320
в

689
00:24:50,320 --> 00:24:51,360
соответствии со структурой, которая

690
00:24:51,360 --> 00:24:54,080
определяется синтаксисом, поэтому значение слова Лиза

691
00:24:54,080 --> 00:24:55,600
не скейтборд, это

692
00:24:55,600 --> 00:24:57,760
функция значения Лизы и не

693
00:24:57,760 --> 00:24:59,600
скейтборд значение слова не катание,

694
00:24:59,600 --> 00:25:01,120
а функция  не

695
00:25:01,120 --> 00:25:03,360
скейтборд значение слова не скейтборд

696
00:25:03,360 --> 00:25:05,520
является функцией севера и скейтборда,

697
00:25:05,520 --> 00:25:08,320
так что это хорошо, и это дает

698
00:25:08,320 --> 00:25:10,400
нам  Способ формализации того, как мы можем

699
00:25:10,400 --> 00:25:12,480
измерить композиционность в нейронных

700
00:25:12,480 --> 00:25:14,960
представлениях, и поэтому

701
00:25:14,960 --> 00:25:16,960
композиционность представлений

702
00:25:16,960 --> 00:25:18,720
можно рассматривать как то, насколько хорошо

703
00:25:18,720 --> 00:25:20,960
представление аппроксимирует

704
00:25:20,960 --> 00:25:23,120
явно гомоморфную гомоморфную

705
00:25:23,120 --> 00:25:24,640
функцию и изучено в большом

706
00:25:24,640 --> 00:25:26,960
пространстве представлений, поэтому то, что мы

707
00:25:26,960 --> 00:25:29,679
собираемся сделать, по сути, будет измерять, если  мы

708
00:25:29,679 --> 00:25:32,080
должны были построить нейронную сеть

709
00:25:32,080 --> 00:25:34,000
, вычисления которой основаны

710
00:25:34,000 --> 00:25:36,000
именно на этих деревьях синтаксического анализа,

711
00:25:36,000 --> 00:25:38,000
насколько далеки представления нашей

712
00:25:38,000 --> 00:25:39,200
изученной модели

713
00:25:39,200 --> 00:25:42,320
от этого явно композиционного

714
00:25:42,320 --> 00:25:43,760
представления,

715
00:25:43,760 --> 00:25:46,000
и это дало нам некоторое представление о

716
00:25:46,000 --> 00:25:47,679
том, насколько композиционными являются представления нейронных сетей на

717
00:25:47,679 --> 00:25:49,840
самом деле.

718
00:25:49,840 --> 00:25:52,960
чтобы немного распаковать это

719
00:25:52,960 --> 00:25:55,279
вместо того, чтобы иметь э-э,

720
00:25:55,279 --> 00:25:57,120
да, так что вместо того, чтобы иметь э-э-

721
00:25:57,120 --> 00:26:00,320
обозначения, у нас есть э-

722
00:26:00,320 --> 00:26:03,279
э-э-э-нота в узле э-э,

723
00:26:03,279 --> 00:26:04,000
и,

724
00:26:04,000 --> 00:26:06,000
чтобы быть более конкретным

725
00:26:06,000 --> 00:26:06,720
,

726
00:26:06,720 --> 00:26:08,720
мы сначала начинаем с выбора функции расстояния,

727
00:26:08,720 --> 00:26:11,039
которая сообщает  нам, как далеко друг от друга два

728
00:26:11,039 --> 00:26:13,120
представления, а затем мы также

729
00:26:13,120 --> 00:26:15,279
нужен способ составить вместе две

730
00:26:15,279 --> 00:26:18,559
составляющие, чтобы дать нам, э-э,

731
00:26:18,559 --> 00:26:21,120
смысл целого,

732
00:26:21,120 --> 00:26:23,760
и но как только у нас есть это, мы можем начать

733
00:26:23,760 --> 00:26:26,240
с э-э, мы можем создать как явно

734
00:26:26,240 --> 00:26:28,159
композиционную функцию, так что то, что мы

735
00:26:28,159 --> 00:26:30,960
делаем, это у нас есть эти

736
00:26:30,960 --> 00:26:33,760
э-э  у нас есть эти представления

737
00:26:33,760 --> 00:26:37,279
на листьях, которые инициализируются случайным образом,

738
00:26:37,279 --> 00:26:38,880
и функция композиции, которая также

739
00:26:38,880 --> 00:26:41,520
инициализируется случайным образом, а затем прямой

740
00:26:41,520 --> 00:26:44,000
проход в соответствии с этим синтаксисом используется для

741
00:26:44,000 --> 00:26:45,840
вычисления

742
00:26:45,840 --> 00:26:48,240
представления Лизы не скейтборд, и теперь, когда у

743
00:26:48,240 --> 00:26:50,240
вас есть это представление, вы можете

744
00:26:50,240 --> 00:26:52,480
создать  функция потерь, и эта

745
00:26:52,480 --> 00:26:54,400
функция потерь измеряет, насколько далеко

746
00:26:54,400 --> 00:26:57,120
представления моей нейронной сети

747
00:26:57,120 --> 00:27:00,000
от этой второй прокси нейронной

748
00:27:00,000 --> 00:27:01,919
сети, которую я создал,

749
00:27:01,919 --> 00:27:03,840
а затем я могу в

750
00:27:03,840 --> 00:27:06,480
основном оптимизировать как функцию композиции, так

751
00:27:06,480 --> 00:27:07,440


752
00:27:07,440 --> 00:27:09,919
и встраивание листьев,

753
00:27:09,919 --> 00:27:11,520
а затем один раз  оптимизация

754
00:27:11,520 --> 00:27:14,400
завершена, я могу измерить, насколько далеко

755
00:27:14,400 --> 00:27:16,960
было представление моей нейронной

756
00:27:16,960 --> 00:27:19,760
сети от этой явно композиционной

757
00:27:19,760 --> 00:27:20,880
сети

758
00:27:20,880 --> 00:27:23,120
на удерживаемом снаружи, и это затем говорит мне, было

759
00:27:23,120 --> 00:27:24,720
ли представление моей

760
00:27:24,720 --> 00:27:26,480
изученной нейронной сети на самом деле композиционным

761
00:27:26,480 --> 00:27:28,480
или нет,

762
00:27:28,480 --> 00:27:31,279
так что, чтобы увидеть, насколько хорошо это работает, давайте

763
00:27:31,279 --> 00:27:33,200
посмотрим на сюжет,

764
00:27:33,200 --> 00:27:37,120
и это относительно сложно, но

765
00:27:37,120 --> 00:27:39,679
просто чтобы немного распечатать это  немного, ээ,

766
00:27:39,679 --> 00:27:41,120


767
00:27:41,120 --> 00:27:43,200
он отображает взаимную информацию

768
00:27:43,200 --> 00:27:44,080
между

769
00:27:44,080 --> 00:27:47,200
входными данными, которые получает нейронная сеть, в

770
00:27:47,200 --> 00:27:48,480


771
00:27:48,480 --> 00:27:50,640
сравнении с представлением на

772
00:27:50,640 --> 00:27:52,799
фоне этой ошибки реконструкции дерева,

773
00:27:52,799 --> 00:27:55,360
о которой мы говорили,

774
00:27:55,360 --> 00:27:57,520
и чтобы дать дополнительную информацию о

775
00:27:57,520 --> 00:27:58,880
том, что должно произойти,

776
00:27:58,880 --> 00:28:01,360
а есть теория

777
00:28:01,360 --> 00:28:02,880
это называется

778
00:28:02,880 --> 00:28:06,240
теорией информационного узкого места, которая гласит, что

779
00:28:06,240 --> 00:28:08,320
когда нейронная сеть тренируется,

780
00:28:08,320 --> 00:28:09,840
она сначала

781
00:28:09,840 --> 00:28:12,799
пытается максимизировать взаимную информацию

782
00:28:12,799 --> 00:28:15,120
между представлением и вводом

783
00:28:15,120 --> 00:28:16,880
в попытке запомнить весь

784
00:28:16,880 --> 00:28:18,480
набор данных,

785
00:28:18,480 --> 00:28:20,559
и это называется

786
00:28:20,559 --> 00:28:23,120
, это наше запоминание  фаза, а затем,

787
00:28:23,120 --> 00:28:25,520
когда запоминание завершено,

788
00:28:25,520 --> 00:28:27,520
наступает фаза обучения или сжатия,

789
00:28:27,520 --> 00:28:30,000
когда эта взаимная информация начинает

790
00:28:30,000 --> 00:28:31,840
уменьшаться, и модель  по сути,

791
00:28:31,840 --> 00:28:34,000
пытается сжать данные

792
00:28:34,000 --> 00:28:36,000
или объединить знания в данных

793
00:28:36,000 --> 00:28:37,840
в их параметры,

794
00:28:37,840 --> 00:28:40,480
и мы видим здесь то, что по мере того, как

795
00:28:40,480 --> 00:28:42,799
модель учится, что характеризуется

796
00:28:42,799 --> 00:28:44,880
уменьшением основной информации,

797
00:28:44,880 --> 00:28:46,640
мы видим, что

798
00:28:46,640 --> 00:28:48,159
сами представления становятся все более и более

799
00:28:48,159 --> 00:28:50,240
композиционными

800
00:28:50,240 --> 00:28:52,640
и общими  мы наблюдаем, что обучение

801
00:28:52,640 --> 00:28:53,760
коррелирует с повышенной

802
00:28:53,760 --> 00:28:55,919
композиционностью, измеряемой этой

803
00:28:55,919 --> 00:28:57,760
ошибкой реконструкции дерева,

804
00:28:57,760 --> 00:29:00,960
так что это действительно обнадеживает,

805
00:29:01,679 --> 00:29:04,320
так что теперь, когда у нас есть метод

806
00:29:04,320 --> 00:29:06,399
измерения

807
00:29:06,399 --> 00:29:08,240
композиционности представлений в этих нейронных

808
00:29:08,240 --> 00:29:09,279
сетях,

809
00:29:09,279 --> 00:29:11,520
как вы знаете, начните создавать

810
00:29:11,520 --> 00:29:12,799
тесты

811
00:29:12,799 --> 00:29:14,399
теперь, когда вы  Знайте, что давайте посмотрим, обобщают ли они

812
00:29:14,399 --> 00:29:17,760
систематически или нет

813
00:29:17,760 --> 00:29:20,880
, чтобы сделать это, вот метод для

814
00:29:20,880 --> 00:29:22,960
взятия любого набора данных и его

815
00:29:22,960 --> 00:29:25,120
разделения на обученный тестовый раздел, который

816
00:29:25,120 --> 00:29:27,039
явно проверяет

817
00:29:27,039 --> 00:29:30,399
этот вид обобщения,

818
00:29:31,600 --> 00:29:33,600
поэтому мы

819
00:29:33,600 --> 00:29:36,000
используем этот принцип, называемый  максимизируя

820
00:29:36,000 --> 00:29:37,760
составную дивергенцию,

821
00:29:37,760 --> 00:29:39,760
и чтобы проиллюстрировать, как работает этот принцип,

822
00:29:39,760 --> 00:29:41,919
мы

823
00:29:41,919 --> 00:29:44,159
посмотрите на этот игрушечный пример, так что в этом игрушечном

824
00:29:44,159 --> 00:29:46,399
примере у нас есть обучающий набор данных,

825
00:29:46,399 --> 00:29:48,000
который состоит всего из двух примеров, и

826
00:29:48,000 --> 00:29:50,640
набор тестовых данных, состоящий только из двух примеров.

827
00:29:50,640 --> 00:29:51,600


828
00:29:51,600 --> 00:29:53,600
Атомы, которые определены

829
00:29:53,600 --> 00:29:56,640
как примитивные элементы.

830
00:29:56,640 --> 00:29:59,760
в

831
00:29:59,760 --> 00:30:01,200
этом игрушечном примере

832
00:30:01,200 --> 00:30:03,200
Голдфингер Кристофер Нолан:

833
00:30:03,200 --> 00:30:05,520
все это так, что примитивные элементы и

834
00:30:05,520 --> 00:30:07,440
соединения являются составами этих

835
00:30:07,440 --> 00:30:09,760
примитивных элементов, так что управляющая

836
00:30:09,760 --> 00:30:11,600
сущность будет составом

837
00:30:11,600 --> 00:30:14,640
типа вопроса сделал ли x предикат y

838
00:30:14,640 --> 00:30:17,760
и предикат прямым,

839
00:30:18,880 --> 00:30:20,399
так что вот базовый механизм для

840
00:30:20,399 --> 00:30:22,320
создание сложных по составу

841
00:30:22,320 --> 00:30:23,360
расщеплений,

842
00:30:23,360 --> 00:30:26,320
так что давайте начнем с введения двух

843
00:30:26,320 --> 00:30:29,360
распределений, первое распределение -

844
00:30:29,360 --> 00:30:31,600
это нормализованное частотное

845
00:30:31,600 --> 00:30:34,640
распределение атомов, поэтому с учетом любого набора данных, если мы

846
00:30:34,640 --> 00:30:37,360
знаем, что такое атомы, мы можем в

847
00:30:37,360 --> 00:30:40,159
основном вычислить частоту

848
00:30:40,159 --> 00:30:42,480
всех атомов, а затем  нормализовать это

849
00:30:42,480 --> 00:30:44,000
по общему количеству, и это даст

850
00:30:44,000 --> 00:30:47,360
нам одно распределение, и мы можем

851
00:30:47,360 --> 00:30:49,600
повторить  то же самое для соединений,

852
00:30:49,600 --> 00:30:51,679
и это даст нам второе

853
00:30:51,679 --> 00:30:53,520
частотное распределение,

854
00:30:53,520 --> 00:30:55,360
поэтому обратите внимание, что это всего лишь два

855
00:30:55,360 --> 00:30:57,600
распределения вероятностей,

856
00:30:57,600 --> 00:31:00,240
и как только у нас есть эти два распределения,

857
00:31:00,240 --> 00:31:02,720
мы можем по существу

858
00:31:02,720 --> 00:31:05,200
определить расхождение атома и соединения

859
00:31:05,200 --> 00:31:08,480
просто как эта величина здесь

860
00:31:08,480 --> 00:31:09,679
и

861
00:31:09,679 --> 00:31:11,120
где

862
00:31:11,120 --> 00:31:12,960
есть коэффициент журнала между

863
00:31:12,960 --> 00:31:15,279
двумя категориальными распределениями,

864
00:31:15,279 --> 00:31:17,600
отток базиса коэффициентов в основном измеряет,

865
00:31:17,600 --> 00:31:20,240
насколько далеки два категориальных распределения

866
00:31:20,240 --> 00:31:22,000
, так что просто чтобы получить немного больше интуиции

867
00:31:22,000 --> 00:31:23,039
об этом

868
00:31:23,039 --> 00:31:25,679
эм, если мы установим p равным q,

869
00:31:25,679 --> 00:31:27,360
тогда коэффициент выключения будет таким,

870
00:31:27,360 --> 00:31:28,960
что означает

871
00:31:28,960 --> 00:31:31,120
эти представления максимально

872
00:31:31,120 --> 00:31:34,159
похожи, и тогда, если p не равно нулю

873
00:31:34,159 --> 00:31:38,399
всюду, q равно нулю um, или если или если p

874
00:31:38,399 --> 00:31:40,640
равно нулю во всех местах, где q равно

875
00:31:40,640 --> 00:31:42,799
нулю, то коэффициент канала

876
00:31:42,799 --> 00:31:45,440
точно равен uh равен нулю, что означает,

877
00:31:45,440 --> 00:31:46,960
что эти  два распределения находятся

878
00:31:46,960 --> 00:31:48,880
максимально далеко,

879
00:31:48,880 --> 00:31:51,039
и общая цель

880
00:31:51,039 --> 00:31:53,120
, описывая

881
00:31:53,120 --> 00:31:55,760
эту цель, состоит в том, что

882
00:31:55,760 --> 00:31:58,080
эта цель потерь заключается в том, что

883
00:31:58,080 --> 00:32:00,640
мы  Мы собираемся максимизировать составную

884
00:32:00,640 --> 00:32:02,480
дивергенцию и минимизировать расходимость атомов,

885
00:32:02,480 --> 00:32:04,799
и что интуиция

886
00:32:04,799 --> 00:32:07,120
стоит за этим, так что мы

887
00:32:07,120 --> 00:32:09,200
хотим, чтобы распределение униграмм

888
00:32:09,200 --> 00:32:10,720


889
00:32:10,720 --> 00:32:12,399
в некотором смысле было постоянным между

890
00:32:12,399 --> 00:32:15,279
поездом и тестовым разбиением, чтобы это было

891
00:32:15,279 --> 00:32:17,279
так  что модель не встречает

892
00:32:17,279 --> 00:32:19,919
никаких новых слов, но мы хотим, чтобы составная

893
00:32:19,919 --> 00:32:21,519
дивергенция была

894
00:32:21,519 --> 00:32:23,840
очень высокой, что означает, что те же самые

895
00:32:23,840 --> 00:32:26,480
слова, которые модель видела много раз,

896
00:32:26,480 --> 00:32:28,720
должны появиться в новых комбинациях, что

897
00:32:28,720 --> 00:32:30,159
означает, что мы проверяем

898
00:32:30,159 --> 00:32:33,039
систематичность,

899
00:32:33,120 --> 00:32:35,679
и поэтому, если вы это сделаете  эээ, если вы будете следовать этой

900
00:32:35,679 --> 00:32:36,720
процедуре

901
00:32:36,720 --> 00:32:38,880
для семантического набора данных передачи,

902
00:32:38,880 --> 00:32:39,760
скажем,

903
00:32:39,760 --> 00:32:42,960
что мы видим э-э, так это то, что когда вы

904
00:32:42,960 --> 00:32:45,760
увеличиваете масштаб, мы видим, что меньший просто

905
00:32:45,760 --> 00:32:47,279
лучше и лучше подходит

906
00:32:47,279 --> 00:32:49,679
для композиционного обобщения,

907
00:32:49,679 --> 00:32:51,360
но э-э, просто вытаскивая цитату из

908
00:32:51,360 --> 00:32:53,279
этой статьи

909
00:32:53,279 --> 00:32:54,640
предварительное обучение помогает для обобщения композиции,

910
00:32:54,640 --> 00:32:56,320
но не решает его полностью,

911
00:32:56,320 --> 00:32:58,720
и это означает, что, возможно, по мере

912
00:32:58,720 --> 00:33:00,320
того, как вы продолжаете увеличивать масштаб этих моделей, вы

913
00:33:00,320 --> 00:33:02,799
увидите лучше и  лучшая производительность

914
00:33:02,799 --> 00:33:04,720
или, может быть, в какой-то момент он начинает насыщаться,

915
00:33:04,720 --> 00:33:06,720


916
00:33:06,720 --> 00:33:08,080
в любом случае нам, вероятно, следует

917
00:33:08,080 --> 00:33:10,159
больше думать об этой проблеме, вместо

918
00:33:10,159 --> 00:33:13,440
того, чтобы просто пытаться ее использовать грубой силой,

919
00:33:14,000 --> 00:33:16,480
так что теперь этот сегмент как бы говорит нам,

920
00:33:16,480 --> 00:33:18,480
что способ, которым мы разделяем набор данных, вы

921
00:33:18,480 --> 00:33:20,480
знаете  мы можем измерить разные

922
00:33:20,480 --> 00:33:22,559
типы эм, которые

923
00:33:22,559 --> 00:33:24,480
мы можем измерить, как разные

924
00:33:24,480 --> 00:33:26,159
поведения модели, и это говорит нам, что,

925
00:33:26,159 --> 00:33:27,440
возможно, нам следует более

926
00:33:27,440 --> 00:33:28,960
критически относиться к тому, как мы оцениваем

927
00:33:28,960 --> 00:33:31,360
модели в nlp в целом,

928
00:33:31,360 --> 00:33:32,159
чтобы

929
00:33:32,159 --> 00:33:34,320
вы знали, что произошла революция

930
00:33:34,320 --> 00:33:36,000
в основном за последние несколько лет в тех

931
00:33:36,000 --> 00:33:37,760
областях, где мы видим, что все эти

932
00:33:37,760 --> 00:33:39,440
модели больших преобразований являются всеми нашими

933
00:33:39,440 --> 00:33:41,919
тестами, в то же время,

934
00:33:41,919 --> 00:33:43,120
вы знаете, все еще

935
00:33:43,120 --> 00:33:45,440
нет полной уверенности в том, что после того, как мы

936
00:33:45,440 --> 00:33:47,039
развернем эти системы в реальном мире,

937
00:33:47,039 --> 00:33:49,440
они  чтобы вы знали, как будто

938
00:33:49,440 --> 00:33:50,480
они собираются поддерживать свою

939
00:33:50,480 --> 00:33:51,679
производительность

940
00:33:51,679 --> 00:33:53,200
, поэтому неясно, являются ли эти преимущества

941
00:33:53,200 --> 00:33:54,720
результатом ложных корреляций или

942
00:33:54,720 --> 00:33:56,960
какого-то реального понимания задачи,

943
00:33:56,960 --> 00:33:59,200
и как мы проектируем  тесты, которые

944
00:33:59,200 --> 00:34:01,600
точно говорят нам, насколько хорошо эта

945
00:34:01,600 --> 00:34:03,600
модель будет работать в реальном мире,

946
00:34:03,600 --> 00:34:05,919
и поэтому я собираюсь привести один пример

947
00:34:05,919 --> 00:34:07,679
работ, которые пытаются это сделать,

948
00:34:07,679 --> 00:34:09,440
и это идея динамических

949
00:34:09,440 --> 00:34:10,800
тестов

950
00:34:10,800 --> 00:34:11,679
и

951
00:34:11,679 --> 00:34:13,359
то, что динамическое, что идея

952
00:34:13,359 --> 00:34:15,199
динамического  Тесты в основном говорят,

953
00:34:15,199 --> 00:34:17,520
что вместо тестирования наших моделей на

954
00:34:17,520 --> 00:34:20,560
статических, на статических наборах тестов, мы должны

955
00:34:20,560 --> 00:34:22,320
оценивать их на постоянно меняющемся

956
00:34:22,320 --> 00:34:24,079
динамическом тесте,

957
00:34:24,079 --> 00:34:26,719
и есть много недавних примеров этого

958
00:34:26,719 --> 00:34:29,280
, и эта идея восходит к семинару 2017 года

959
00:34:29,280 --> 00:34:30,320


960
00:34:30,320 --> 00:34:33,040
в emlp

961
00:34:33,040 --> 00:34:34,639
и т.  общая схема выглядит

962
00:34:34,639 --> 00:34:35,839
примерно так,

963
00:34:35,839 --> 00:34:38,320
что мы начинаем с набора обучающих данных

964
00:34:38,320 --> 00:34:40,239
и набора тестовых данных, который представляет собой статический,

965
00:34:40,239 --> 00:34:42,159
э-э, статический порядок, который

966
00:34:42,159 --> 00:34:44,239
мы обучаем модели на этом, а затем,

967
00:34:44,239 --> 00:34:45,599
когда модель обучена,

968
00:34:45,599 --> 00:34:48,320
мы развертываем это, а затем люди

969
00:34:48,320 --> 00:34:50,239
создают новые  примеры, которые модель не

970
00:34:50,239 --> 00:34:51,839
может классифицировать,

971
00:34:51,839 --> 00:34:52,719
и,

972
00:34:52,719 --> 00:34:54,719
что очень важно, мы ищем примеры, которые

973
00:34:54,719 --> 00:34:56,560
модель не проверяется, но у людей

974
00:34:56,560 --> 00:34:59,599
нет проблем с выяснением ответа,

975
00:34:59,599 --> 00:35:01,440
поэтому, играя в эту игру в удар  Если

976
00:35:01,440 --> 00:35:02,400


977
00:35:02,400 --> 00:35:03,440
вы знаете, что мы,

978
00:35:03,440 --> 00:35:06,160
люди, выясняем, что это за

979
00:35:06,160 --> 00:35:08,240
дыры в понимании модели,

980
00:35:08,240 --> 00:35:09,839
а затем добавляем это обратно в обучающие

981
00:35:09,839 --> 00:35:10,960
данные,

982
00:35:10,960 --> 00:35:12,800
переобучаем модель, развернем ее снова, чтобы

983
00:35:12,800 --> 00:35:15,040
люди создавали новые примеры, мы, по

984
00:35:15,040 --> 00:35:17,520
сути, можем построить эти бесконечные,

985
00:35:17,520 --> 00:35:20,160
вы знаете, данные  установите этот бесконечный

986
00:35:20,160 --> 00:35:21,200
набор тестов,

987
00:35:21,200 --> 00:35:23,440
который, надеюсь, может

988
00:35:23,440 --> 00:35:25,520
быть лучшим

989
00:35:25,520 --> 00:35:27,920
показателем для оценки реальной производительности,

990
00:35:27,920 --> 00:35:30,000


991
00:35:30,000 --> 00:35:30,880


992
00:35:30,880 --> 00:35:32,640
так что это действительно передовое

993
00:35:32,640 --> 00:35:34,640
исследование, и одна из основных проблем,

994
00:35:34,640 --> 00:35:35,920
с которыми вы знаете этот

995
00:35:35,920 --> 00:35:38,640
класс работ, заключается в том, что неясно, как

996
00:35:38,640 --> 00:35:40,720
это может масштабироваться, потому что,

997
00:35:40,720 --> 00:35:42,720
может быть,

998
00:35:42,720 --> 00:35:45,359
после нескольких итераций этого

999
00:35:45,359 --> 00:35:47,119
удара-молота люди просто

1000
00:35:47,119 --> 00:35:49,359
фундаментально ограничены творчеством, так

1001
00:35:49,359 --> 00:35:51,599
что выяснение того, как

1002
00:35:51,599 --> 00:35:52,800
с этим справиться,

1003
00:35:52,800 --> 00:35:55,280
это действительно открытая проблема и своего рода

1004
00:35:55,280 --> 00:35:57,040
подходы  просто используйте примеры из других

1005
00:35:57,040 --> 00:35:59,599
наборов данных, чтобы побудить людей

1006
00:35:59,599 --> 00:36:01,040
мыслить более творчески,

1007
00:36:01,040 --> 00:36:03,040
но, возможно, мы сможем придумать

1008
00:36:03,040 --> 00:36:04,800
более автоматизированные

1009
00:36:04,800 --> 00:36:07,760
методы для этого,

1010
00:36:08,000 --> 00:36:10,320
так что э-э  это подводит нас к

1011
00:36:10,320 --> 00:36:12,880
заключительному сегменту

1012
00:36:12,880 --> 00:36:14,800
или на самом деле позвольте мне остановиться для вопросов на

1013
00:36:14,800 --> 00:36:16,560
этом этапе и посмотреть, есть ли у

1014
00:36:16,560 --> 00:36:20,119
людей вопросы

1015
00:36:28,240 --> 00:36:31,200
вот вопрос с динамическим эталонным

1016
00:36:31,200 --> 00:36:33,599


1017
00:36:33,599 --> 00:36:36,000


1018
00:36:36,000 --> 00:36:38,079
тестом, не означает ли это, что создателю модели также нужно будет постоянно тестировать косую черту для оценки моделей  в

1019
00:36:38,079 --> 00:36:41,599
новых тестах новые данные тест новые данные

1020
00:36:41,599 --> 00:36:43,200
состояния

1021
00:36:43,200 --> 00:36:46,400
э-э, подождите секунду,

1022
00:36:49,760 --> 00:36:51,040
извините,

1023
00:36:51,040 --> 00:36:54,000
да, с динамическими тестами, да,

1024
00:36:54,000 --> 00:36:56,400
это абсолютно верно, что вам

1025
00:36:56,400 --> 00:36:57,920
придется постоянно обучать свою

1026
00:36:57,920 --> 00:37:00,640
модель, и это просто для того, чтобы

1027
00:37:00,640 --> 00:37:02,000


1028
00:37:02,000 --> 00:37:04,160
вы знали причину своего  модель

1029
00:37:04,160 --> 00:37:06,160
не очень хорошо работает на тестовом наборе

1030
00:37:06,160 --> 00:37:07,839
, не имеет отношения к подобному несоответствию предметной области,

1031
00:37:07,839 --> 00:37:09,040


1032
00:37:09,040 --> 00:37:11,119
и то, что мы действительно пытаемся сделать, это

1033
00:37:11,119 --> 00:37:11,839
как будто

1034
00:37:11,839 --> 00:37:13,440
вы знаете,

1035
00:37:13,440 --> 00:37:15,680
измерить, как просто придумать

1036
00:37:15,680 --> 00:37:17,760
лучшую оценку производительности модели

1037
00:37:17,760 --> 00:37:19,839
для  общая задача

1038
00:37:19,839 --> 00:37:20,720
и

1039
00:37:20,720 --> 00:37:22,160
просто пытаюсь получить все больше и больше

1040
00:37:22,160 --> 00:37:24,320
данных, так что да, чтобы ответить, чтобы ответить на ваш

1041
00:37:24,320 --> 00:37:25,920
вопрос, да, нам нужно продолжать

1042
00:37:25,920 --> 00:37:27,280
обучение модели снова и снова, но

1043
00:37:27,280 --> 00:37:29,119
это можно автоматизировать,

1044
00:37:29,119 --> 00:37:33,359
хорошо, так что  ээ, я перейду к некоторой разновидности

1045
00:37:33,359 --> 00:37:35,599
э-э-языка,

1046
00:37:35,599 --> 00:37:37,920
так что э-э, в этом заключительном шаге я

1047
00:37:37,920 --> 00:37:40,240
расскажу о том, как мы можем выйти за рамки простого

1048
00:37:40,240 --> 00:37:41,599
обучения моделей,

1049
00:37:41,599 --> 00:37:44,079
э-э, только на тексте

1050
00:37:44,079 --> 00:37:45,359
э-э,

1051
00:37:45,359 --> 00:37:46,240


1052
00:37:46,240 --> 00:37:48,079
многие заявили

1053
00:37:48,079 --> 00:37:50,400
о необходимости использования других модальностей, кроме

1054
00:37:50,400 --> 00:37:52,960
своих бывших  если мы когда-нибудь захотим достичь

1055
00:37:52,960 --> 00:37:54,880
реального понимания языка,

1056
00:37:54,880 --> 00:37:57,839
и знаете ли вы об этом с тех пор, как

1057
00:37:57,839 --> 00:37:59,839
у нас появились подобные большие языковые модели,

1058
00:37:59,839 --> 00:38:01,520
вы знаете, что это было своего рода

1059
00:38:01,520 --> 00:38:03,599
возрождение этих дебатов, и недавно

1060
00:38:03,599 --> 00:38:06,240
было несколько статей на эту тему и

1061
00:38:06,240 --> 00:38:08,880
Итак, в прошлом году на ACL была опубликована статья, в

1062
00:38:08,880 --> 00:38:11,280
которой с помощью нескольких мысленных

1063
00:38:11,280 --> 00:38:12,400
экспериментов утверждается, что на самом деле

1064
00:38:12,400 --> 00:38:14,160
невозможно получить

1065
00:38:14,160 --> 00:38:16,880
значение только из формы, где значение

1066
00:38:16,880 --> 00:38:19,040
относится к коммуникативному намерению

1067
00:38:19,040 --> 00:38:21,520
говорящего, а форма относится к

1068
00:38:21,520 --> 00:38:24,320
текстовым или речевым сигналам,

1069
00:38:24,320 --> 00:38:25,280


1070
00:38:25,280 --> 00:38:27,280
а в более модифицированной версии  это было

1071
00:38:27,280 --> 00:38:30,079
выдвинуто во второй статье, в которой

1072
00:38:30,079 --> 00:38:32,880
говорится, что обучение только на данных веб-масштаба как

1073
00:38:32,880 --> 00:38:34,800
бы ограничивает мировой объем моделей

1074
00:38:34,800 --> 00:38:37,200
и как бы ограничивает аспекты

1075
00:38:37,200 --> 00:38:38,960
значений, которые мод  Эль действительно может

1076
00:38:38,960 --> 00:38:40,320
получить

1077
00:38:40,320 --> 00:38:41,119
эм,

1078
00:38:41,119 --> 00:38:43,040
и вот что-то вроде диаграммы, которую

1079
00:38:43,040 --> 00:38:44,880
я позаимствовал из статьи, и

1080
00:38:44,880 --> 00:38:47,839
они говорят, что э-э ээ эра, когда мы

1081
00:38:47,839 --> 00:38:50,160
тренировали модернизм, как контролируемые наборы данных.

1082
00:38:50,160 --> 00:38:52,640


1083
00:38:52,640 --> 00:38:55,599
Мы перешли

1084
00:38:55,599 --> 00:38:58,320
к эксплуатации, например, немаркированных данных, которые сейчас находятся

1085
00:38:58,320 --> 00:39:00,480
в области 2, где модели просто

1086
00:39:00,480 --> 00:39:01,920
имеют строго больше

1087
00:39:01,920 --> 00:39:03,920
сигналов, чтобы получить больше аспектов

1088
00:39:03,920 --> 00:39:05,119
минимума,

1089
00:39:05,119 --> 00:39:07,440
если вы добавите в это дополнительные методы,

1090
00:39:07,440 --> 00:39:09,599
так что, возможно, вы сделаете несколько видеороликов и,

1091
00:39:09,599 --> 00:39:11,440
возможно, сделаете несколько изображений

1092
00:39:11,440 --> 00:39:12,880
затем это

1093
00:39:12,880 --> 00:39:14,560
расширяет кругозор модели

1094
00:39:14,560 --> 00:39:16,880
еще больше, и теперь, возможно, он может получить

1095
00:39:16,880 --> 00:39:19,040
больше аспектов значения, так что

1096
00:39:19,040 --> 00:39:21,520
теперь он знает, что

1097
00:39:21,520 --> 00:39:24,800
лексический элемент, выделенный красным, относится к вам, возможно, вы знаете

1098
00:39:24,800 --> 00:39:26,880
красные изображения,

1099
00:39:26,880 --> 00:39:28,800
а затем, если вы выйдете за пределы этого, вы можете

1100
00:39:28,800 --> 00:39:30,240
получить  модель

1101
00:39:30,240 --> 00:39:32,240
, которая воплощена, и она на самом деле

1102
00:39:32,240 --> 00:39:33,680
живет в среде, где она может

1103
00:39:33,680 --> 00:39:34,720


1104
00:39:34,720 --> 00:39:37,040
взаимодействовать со своими данными, проводить

1105
00:39:37,040 --> 00:39:39,359
вмешательство и эксперименты,

1106
00:39:39,359 --> 00:39:41,359
а затем, если вы выйдете за пределы

1107
00:39:41,359 --> 00:39:43,760
этого, вы можете иметь модели, которые живут  е в

1108
00:39:43,760 --> 00:39:45,280
социальном мире, где они могут взаимодействовать

1109
00:39:45,280 --> 00:39:47,359
с другими моделями, потому что, в конце концов,

1110
00:39:47,359 --> 00:39:49,280
цель языка - общение,

1111
00:39:49,280 --> 00:39:51,599
и поэтому у нее может быть как социальный мир,

1112
00:39:51,599 --> 00:39:53,760
где

1113
00:39:53,760 --> 00:39:55,359
модели могут общаться с другими моделями,

1114
00:39:55,359 --> 00:39:58,320
что как бы расширяет аспекты

1115
00:39:58,320 --> 00:39:58,750
смысла

1116
00:39:58,750 --> 00:40:00,000
[Музыка]

1117
00:40:00,000 --> 00:40:04,079
и поэтому gpd3 находится в очереди мировой области видимости,

1118
00:40:04,079 --> 00:40:06,400
поэтому в этом пространстве много открытых вопросов,

1119
00:40:06,400 --> 00:40:07,359


1120
00:40:07,359 --> 00:40:09,200
поэтому, учитывая, что есть все эти

1121
00:40:09,200 --> 00:40:11,119
хорошие аргументы о том, как нам нужно выйти

1122
00:40:11,119 --> 00:40:13,359
за пределы текста, как лучше всего сделать

1123
00:40:13,359 --> 00:40:14,960
это в масштабе, который

1124
00:40:14,960 --> 00:40:16,160


1125
00:40:16,160 --> 00:40:18,000
мы знаем  что вы знаете, что младенцы не могут

1126
00:40:18,000 --> 00:40:20,800
выучить язык, например, смотря телевизор в одиночку,

1127
00:40:20,800 --> 00:40:24,160
поэтому должны быть какие-то

1128
00:40:24,160 --> 00:40:25,359
вмешательства и

1129
00:40:25,359 --> 00:40:26,480
взаимодействия,

1130
00:40:26,480 --> 00:40:28,079
которые должны произойти,

1131
00:40:28,079 --> 00:40:30,079
но в то же время вопрос в том, как

1132
00:40:30,079 --> 00:40:31,839
далеко могут зайти модели

1133
00:40:31,839 --> 00:40:33,920
, просто обучаясь на

1134
00:40:33,920 --> 00:40:35,359
статических данных

1135
00:40:35,359 --> 00:40:37,440
как  пока у нас есть дополнительные возможности,

1136
00:40:37,440 --> 00:40:38,880
особенно когда мы объединяем их с

1137
00:40:38,880 --> 00:40:40,480
масштабом,

1138
00:40:40,480 --> 00:40:42,480
и если взаимодействие с окружающей

1139
00:40:42,480 --> 00:40:44,800
средой действительно необходимо, как мы собираем

1140
00:40:44,800 --> 00:40:46,960
данные и проектируем системы, которые взаимодействуют

1141
00:40:46,960 --> 00:40:50,000
минимально?

1142
00:40:50,000 --> 00:40:51,760


1143
00:40:51,760 --> 00:40:54,720


1144
00:40:54,720 --> 00:40:55,520


1145
00:40:55,520 --> 00:40:58,720


1146
00:40:58,720 --> 00:41:00,960


1147
00:41:00,960 --> 00:41:03,599


1148
00:41:03,599 --> 00:41:05,680
г в экономически эффективным способом, а затем, наконец, поставить предварительный тренинг по тексту все еще быть полезным, если любой из этих других эм, если любой из этих других, как гм любой из этих направлений исследований эм стать более эффективной выборки, так что если вы заинтересованы в обучении  подробнее

1149
00:41:05,680 --> 00:41:06,400
об

1150
00:41:06,400 --> 00:41:08,240
этой теме, я настоятельно рекомендую вам

1151
00:41:08,240 --> 00:41:10,960
взять cs224u, который предлагается весной,

1152
00:41:10,960 --> 00:41:12,800
у них есть несколько лекций только на

1153
00:41:12,800 --> 00:41:15,280
языке,

1154
00:41:18,960 --> 00:41:21,760
хорошо, поэтому в этом заключительном сегменте я собираюсь

1155
00:41:21,760 --> 00:41:23,520
рассказать немного больше о том, как вы можете

1156
00:41:23,520 --> 00:41:24,880
принять участие

1157
00:41:24,880 --> 00:41:26,880
в нлп  и

1158
00:41:26,880 --> 00:41:29,440
исследования в области глубокого обучения, и как вы знаете, как

1159
00:41:29,440 --> 00:41:32,000
добиться большего прогресса,

1160
00:41:32,000 --> 00:41:34,720
так что вот некоторые общие принципы

1161
00:41:34,720 --> 00:41:36,960
того, как добиться прогресса в английском и

1162
00:41:36,960 --> 00:41:39,119
исследованиях, поэтому я думаю, что самое важное

1163
00:41:39,119 --> 00:41:41,200
- просто читать широко,

1164
00:41:41,200 --> 00:41:43,280
что означает, что не просто читать  последние и

1165
00:41:43,280 --> 00:41:46,079
лучшие статьи в архиве, но также читаются

1166
00:41:46,079 --> 00:41:48,960
как статистические данные НЛП до 2010 года. Узнайте

1167
00:41:48,960 --> 00:41:50,000


1168
00:41:50,000 --> 00:41:52,480
о математических

1169
00:41:52,480 --> 00:41:54,720
основах машинного обучения, чтобы понять, как

1170
00:41:54,720 --> 00:41:58,240
работает обобщение.

1171
00:41:58,240 --> 00:42:00,720
П больше о языке, что означает

1172
00:42:00,720 --> 00:42:02,480
,

1173
00:42:02,480 --> 00:42:03,920
в частности, занятия на факультете лингвистики, я бы

1174
00:42:03,920 --> 00:42:07,119
порекомендовал вселенную 138, а также взять

1175
00:42:07,119 --> 00:42:09,359
cs224u

1176
00:42:09,359 --> 00:42:12,160
и, наконец, если вы хотите, э-э, если вы

1177
00:42:12,160 --> 00:42:13,920
хотите черпать вдохновение в том, как учатся младенцы,

1178
00:42:13,920 --> 00:42:16,319
то обязательно

1179
00:42:16,319 --> 00:42:18,079
прочтите литературу по изучению языка для детей, это

1180
00:42:18,079 --> 00:42:20,480
увлекательно

1181
00:42:20,480 --> 00:42:22,560
наконец-то узнайте, как

1182
00:42:22,560 --> 00:42:24,880
выучить свои программные инструменты, которые

1183
00:42:24,880 --> 00:42:26,880
включают в себя инструменты сценариев.

1184
00:42:26,880 --> 00:42:28,880
Преодоление данных управления версиями.

1185
00:42:28,880 --> 00:42:31,200
Обучение

1186
00:42:31,200 --> 00:42:34,079
быстрой визуализации с помощью блокнотов jupiter.

1187
00:42:34,079 --> 00:42:36,880
Глубокое обучение часто включает в себя

1188
00:42:36,880 --> 00:42:38,560
проведение нескольких экспериментов с

1189
00:42:38,560 --> 00:42:40,000
разными гиперпараметрами и разными

1190
00:42:40,000 --> 00:42:42,079
идеями параллельно, а иногда

1191
00:42:42,079 --> 00:42:43,520
может быть очень сложно отслеживать

1192
00:42:43,520 --> 00:42:45,040
все, поэтому научитесь использовать

1193
00:42:45,040 --> 00:42:46,640
инструменты управления экспериментом, такие как веса

1194
00:42:46,640 --> 00:42:49,280
и смещения,

1195
00:42:50,560 --> 00:42:53,839
и, наконец, я расскажу о некоторых

1196
00:42:53,839 --> 00:42:56,319
действительно быстрых окончательных советах по проекту,

1197
00:42:56,319 --> 00:42:57,200


1198
00:42:57,200 --> 00:42:59,040
так что сначала давайте просто начнем с того,

1199
00:42:59,040 --> 00:43:00,640
что если ваш подход не  Кажется,

1200
00:43:00,640 --> 00:43:02,720
работает, пожалуйста, не паникуйте,

1201
00:43:02,720 --> 00:43:05,599
вставьте утверждения утверждения  повсюду и

1202
00:43:05,599 --> 00:43:07,040
проверьте, правильны ли вычисления, которые вы

1203
00:43:07,040 --> 00:43:08,400
делаете,

1204
00:43:08,400 --> 00:43:10,240
широко используйте точки останова, и я

1205
00:43:10,240 --> 00:43:12,560
расскажу немного больше об этой

1206
00:43:12,560 --> 00:43:14,319
проверке, правильна ли реализованная вами функция потерь,

1207
00:43:14,319 --> 00:43:16,400


1208
00:43:16,400 --> 00:43:19,280
и об одном из способов отладки, который заключается в том, чтобы увидеть,

1209
00:43:19,280 --> 00:43:21,280
что, мм  начальные значения верны,

1210
00:43:21,280 --> 00:43:22,800
поэтому, если вы решаете задачу классификации kva,

1211
00:43:22,800 --> 00:43:24,560
тогда начальная потеря должна

1212
00:43:24,560 --> 00:43:27,040
быть натуральным логарифмом k

1213
00:43:27,040 --> 00:43:29,040
всегда всегда всегда начинается с создания

1214
00:43:29,040 --> 00:43:31,040
небольшого набора обучающих данных, который имеет от

1215
00:43:31,040 --> 00:43:32,960
пяти до десяти примеров, и посмотрите, может ли ваша

1216
00:43:32,960 --> 00:43:35,200
модель  полностью переоснащить, если

1217
00:43:35,200 --> 00:43:36,319
нет проблемы с вашим циклом обучения

1218
00:43:36,319 --> 00:43:37,359


1219
00:43:37,359 --> 00:43:38,400
um

1220
00:43:38,400 --> 00:43:40,400
проверьте насыщающие активации и

1221
00:43:40,400 --> 00:43:43,119
мертвые значения, и часто это может быть

1222
00:43:43,119 --> 00:43:44,000
исправлено,

1223
00:43:44,000 --> 00:43:45,040
как вы знаете, может быть, есть некоторые

1224
00:43:45,040 --> 00:43:46,400
проблемы с градиентом, поэтому, возможно, есть

1225
00:43:46,400 --> 00:43:48,160
некоторые проблемы с инициализацией,

1226
00:43:48,160 --> 00:43:50,240
которые подводят меня к следующему  точка проверьте

1227
00:43:50,240 --> 00:43:51,680
свои значения градиента, посмотрите, не слишком ли они

1228
00:43:51,680 --> 00:43:53,520
малы, что означает, что, возможно, вам

1229
00:43:53,520 --> 00:43:56,400
следует использовать остаточные соединения или lstms,

1230
00:43:56,400 --> 00:43:57,920
или если они слишком велики, вам следует

1231
00:43:57,920 --> 00:44:00,079
использовать отсечение градиента на самом деле всегда использовать

1232
00:44:00,079 --> 00:44:02,079
отсечение градиента um в

1233
00:44:02,079 --> 00:44:04,319
целом будьте методичны, если ваш подход

1234
00:44:04,319 --> 00:44:07,040
не работает, выдвигайте гипотезы или

1235
00:44:07,040 --> 00:44:08,720
почему это может быть случай, дизайн

1236
00:44:08,720 --> 00:44:10,880
экспериментов оракула для его отладки посмотрите на

1237
00:44:10,880 --> 00:44:12,960
свои данные и посмотрите на ошибки, которые

1238
00:44:12,960 --> 00:44:14,800
он делает  и просто попытайтесь быть

1239
00:44:14,800 --> 00:44:17,520
систематическим во всем,

1240
00:44:17,520 --> 00:44:20,319
так что я просто скажу немного больше

1241
00:44:20,319 --> 00:44:23,680
о точках останова, так что есть эта

1242
00:44:23,680 --> 00:44:26,079
отличная библиотека под названием pdb, она похожа на gdp,

1243
00:44:26,079 --> 00:44:28,560
но для python, поэтому pdb

1244
00:44:28,560 --> 00:44:31,520
um to create для создания точки останова просто

1245
00:44:31,520 --> 00:44:34,640
добавьте  строка import pdb pdb set trace

1246
00:44:34,640 --> 00:44:37,520
перед строкой, которую вы хотите проверить, поэтому

1247
00:44:37,520 --> 00:44:39,200
ранее сегодня я пытался

1248
00:44:39,200 --> 00:44:41,680
поиграть с библиотекой трансформаторов,

1249
00:44:41,680 --> 00:44:42,720


1250
00:44:42,720 --> 00:44:44,400
поэтому я пытался

1251
00:44:44,400 --> 00:44:46,800
отвечать на вопросы, поэтому у меня действительно небольшой

1252
00:44:46,800 --> 00:44:49,119
учебный корпус и контекст  однажды

1253
00:44:49,119 --> 00:44:51,839
утром я застрелил слона в своей пижаме,

1254
00:44:51,839 --> 00:44:54,800
как он попал в мою пижаму, я не знаю,

1255
00:44:54,800 --> 00:44:57,359
и вопрос в том, что я снимал

1256
00:44:57,359 --> 00:44:59,440
и что делать, чтобы решить эту проблему, я в

1257
00:44:59,440 --> 00:45:01,599
основном импортировал токенизатор

1258
00:45:01,599 --> 00:45:03,200
и рождение  h model

1259
00:45:03,200 --> 00:45:05,119
um, и я, вы знаете, инициализируйте мой

1260
00:45:05,119 --> 00:45:06,720
токенизатор, инициализируйте мою модель, например,

1261
00:45:06,720 --> 00:45:09,359
токенизируйте мой ввод, я установил свою модель в

1262
00:45:09,359 --> 00:45:12,160
режим eval, и я пытаюсь посмотреть на

1263
00:45:12,160 --> 00:45:13,599
иллюстрацию,

1264
00:45:13,599 --> 00:45:15,920
но я получаю эту ошибку,

1265
00:45:15,920 --> 00:45:17,599
и мне очень грустно, что непонятно, что

1266
00:45:17,599 --> 00:45:19,119
вызывает это  error,

1267
00:45:19,119 --> 00:45:21,200
и поэтому лучший способ посмотреть, что

1268
00:45:21,200 --> 00:45:23,040
вызывает эту ошибку, - это на самом деле поставить

1269
00:45:23,040 --> 00:45:24,160
точку останова

1270
00:45:24,160 --> 00:45:26,720
um, поэтому сразу после модульного eval я поставил

1271
00:45:26,720 --> 00:45:29,040
точку останова, потому что я знаю, что это

1272
00:45:29,040 --> 00:45:30,720
где проблема, поэтому проблема

1273
00:45:30,720 --> 00:45:35,280
в 21, поэтому я поставил точку останова на  строка 21,

1274
00:45:35,280 --> 00:45:37,760
и теперь, как только я поставлю эту точку останова, я могу

1275
00:45:37,760 --> 00:45:38,720
просто

1276
00:45:38,720 --> 00:45:41,520
снова запустить свой скрипт, и он остановится перед

1277
00:45:41,520 --> 00:45:43,920
выполнением строки 21, и в этот момент я

1278
00:45:43,920 --> 00:45:46,800
могу проверить все свои переменные, чтобы я мог

1279
00:45:46,800 --> 00:45:48,400
смотреть на токен как на ввод, потому что, возможно

1280
00:45:48,400 --> 00:45:50,480
, в этом проблема  и, о

1281
00:45:50,480 --> 00:45:53,440
чудо, я вижу, что на самом деле

1282
00:45:53,440 --> 00:45:56,160
это список, так что это словарь списков,

1283
00:45:56,160 --> 00:45:58,560
тогда как модули обычно ожидают тензор уклонения,

1284
00:45:58,560 --> 00:46:01,119
поэтому теперь я знаю, в чем проблема,

1285
00:46:01,119 --> 00:46:02,800
и это означает, что я могу быстро пойти

1286
00:46:02,800 --> 00:46:04,000
и исправить ее,

1287
00:46:04,000 --> 00:46:06,319
и все просто работает, так что это

1288
00:46:06,319 --> 00:46:07,440
просто 

1289
00:46:07,440 --> 00:46:09,440


1290
00:46:09,440 --> 00:46:11,599


1291
00:46:11,599 --> 00:46:15,280


1292
00:46:15,280 --> 00:46:18,079


1293
00:46:18,079 --> 00:46:20,240


1294
00:46:20,240 --> 00:46:22,160
показывает, что вы должны использовать контрольные точки всюду эм, если ваш код не работает, и он может так же, как помощь в отладке очень быстро гм хорошо так ээ, наконец, я скажу, что если вы хотите принять участие в NLP и глубоких исследований обучения, и если вы на самом деле  как и

1295
00:46:22,160 --> 00:46:24,880
в последнем проекте, у нас есть программа клипов

1296
00:46:24,880 --> 00:46:28,000
в Стэнфорде, и это способ

1297
00:46:28,000 --> 00:46:30,800
для студентов магистратуры и докторов наук

1298
00:46:30,800 --> 00:46:33,119
, которые заинтересованы в исследованиях НЛП

1299
00:46:33,119 --> 00:46:34,800
и хотят принять участие в

1300
00:46:34,800 --> 00:46:37,359
группе НЛП, поэтому мы настоятельно рекомендуем вам

1301
00:46:37,359 --> 00:46:38,960
подать заявку на участие в клипах.

1302
00:46:38,960 --> 00:46:40,240


1303
00:46:40,240 --> 00:46:43,119
и так что да, так что я в заключение,

1304
00:46:43,119 --> 00:46:45,680
продолжу сегодняшнюю приправу класса, что, как вы

1305
00:46:45,680 --> 00:46:47,760
знаете, мы добились большого прогресса

1306
00:46:47,760 --> 00:46:50,319
за последнее десятилетие, и это в основном

1307
00:46:50,319 --> 00:46:52,240
из-за того, что вы знаете умное понимание

1308
00:46:52,240 --> 00:46:54,400
аппаратного обеспечения данных нейронных сетей, все

1309
00:46:54,400 --> 00:46:56,560
это в сочетании с масштабом  у нас есть несколько

1310
00:46:56,560 --> 00:46:58,240
действительно удивительных технологий, которые могут делать

1311
00:46:58,240 --> 00:47:00,240
действительно захватывающие вещи, и мы видели некоторые

1312
00:47:00,240 --> 00:47:01,839
примеры этого сегодня,

1313
00:47:01,839 --> 00:47:02,960


1314
00:47:02,960 --> 00:47:05,040
в краткосрочной перспективе, я ожидаю, что мы

1315
00:47:05,040 --> 00:47:07,280
увидим больше масштабирования, потому что это,

1316
00:47:07,280 --> 00:47:09,359
кажется, просто помогает  возможно, даже более крупные

1317
00:47:09,359 --> 00:47:10,560
модели,

1318
00:47:10,560 --> 00:47:13,599
но это нетривиально, так что вы знаете,

1319
00:47:13,599 --> 00:47:15,280
я сказал, что раньше, и я просто повторю это

1320
00:47:15,280 --> 00:47:16,880
снова, масштабирование требует действительно

1321
00:47:16,880 --> 00:47:18,800
нетривиальных инженерных усилий, и

1322
00:47:18,800 --> 00:47:20,480
иногда даже вы знаете умные

1323
00:47:20,480 --> 00:47:22,880
алгоритмы, и поэтому у нас есть много

1324
00:47:22,880 --> 00:47:25,280
интересных систем.  нужно сделать здесь,

1325
00:47:25,280 --> 00:47:26,720
но в долгосрочной

1326
00:47:26,720 --> 00:47:28,400
перспективе нам действительно нужно больше думать

1327
00:47:28,400 --> 00:47:30,480
об этих более серьезных проблемах, таких как

1328
00:47:30,480 --> 00:47:33,040
обобщение систематичности, как мы можем

1329
00:47:33,040 --> 00:47:35,440
сделать наши модели, которые вы знаете,

1330
00:47:35,440 --> 00:47:37,280
очень быстро изучить новую концепцию, чтобы это быстрая

1331
00:47:37,280 --> 00:47:38,480
адаптация,

1332
00:47:38,480 --> 00:47:40,400
а затем нам также нужно  вы знаете, что

1333
00:47:40,400 --> 00:47:42,000
создаете эталонные тесты, которым мы действительно можем

1334
00:47:42,000 --> 00:47:44,160
доверять, поэтому, если моя модель имеет некоторую

1335
00:47:44,160 --> 00:47:46,400
производительность на некотором

1336
00:47:46,400 --> 00:47:48,400
наборе данных анализа настроений и развернута в реальном мире,

1337
00:47:48,400 --> 00:47:50,640
это должно быть отражено в количестве,

1338
00:47:50,640 --> 00:47:52,160
которое я получаю из эталонного теста, поэтому нам нужно

1339
00:47:52,160 --> 00:47:53,440
добиться

1340
00:47:53,440 --> 00:47:55,839
прогресса в  то, как мы оцениваем модели,

1341
00:47:55,839 --> 00:47:57,119
а затем

1342
00:47:57,119 --> 00:47:59,599
выясняем, как выйти за рамки текста

1343
00:47:59,599 --> 00:48:01,680
в более сговорчивой форме, это тоже

1344
00:48:01,680 --> 00:48:03,680
очень важно,

1345
00:48:03,680 --> 00:48:05,839
так что да, вот и все, удачи

1346
00:48:05,839 --> 00:48:07,599
тебе  ваши последние проекты,

1347
00:48:07,599 --> 00:48:11,920
я могу ответить на дополнительные вопросы на этом этапе,

1348
00:48:12,800 --> 00:48:13,680
поэтому

1349
00:48:13,680 --> 00:48:15,520
я ответил на вопрос ранее, что на

1350
00:48:15,520 --> 00:48:17,440
самом деле я думаю, что вы

1351
00:48:17,440 --> 00:48:18,720


1352
00:48:18,720 --> 00:48:20,559
также можете найти в мм, это был

1353
00:48:20,559 --> 00:48:22,240
вопрос о том, есть ли у вас большая

1354
00:48:22,240 --> 00:48:24,160
модель, которая предварительно обучена языку, если

1355
00:48:24,160 --> 00:48:26,800
это действительно поможет  вы в других

1356
00:48:26,800 --> 00:48:28,559
областях, как вы применяете его к вещам для зрения,

1357
00:48:28,559 --> 00:48:29,839


1358
00:48:29,839 --> 00:48:31,280


1359
00:48:31,280 --> 00:48:32,800
да,

1360
00:48:32,800 --> 00:48:36,240
да, поэтому я думаю, что ответ на

1361
00:48:36,240 --> 00:48:37,760
самом деле да, как будто недавно была опубликована статья, которая

1362
00:48:37,760 --> 00:48:39,359
вышла совсем недавно,

1363
00:48:39,359 --> 00:48:42,160
всего два дня назад, которая просто занимает,

1364
00:48:42,160 --> 00:48:44,319
я думаю, это тоже было gpt

1365
00:48:44,319 --> 00:48:45,760
я  Я не уверен, что это похоже на одну большую

1366
00:48:45,760 --> 00:48:47,280
модель-трансформер, которая изображена в

1367
00:48:47,280 --> 00:48:48,640
тексте,

1368
00:48:48,640 --> 00:48:51,599
и подобные другие диалоги

1369
00:48:51,599 --> 00:48:53,680
определенно применимы к изображениям, и я думаю, что

1370
00:48:53,680 --> 00:48:55,520
они применимы к

1371
00:48:55,520 --> 00:48:56,319


1372
00:48:56,319 --> 00:48:58,480
математическим задачам и некоторым другим модальностям

1373
00:48:58,480 --> 00:48:59,839
и показывают, что это действительно

1374
00:48:59,839 --> 00:49:02,559
эффективно при подобном переносе, поэтому, если вы предварительно

1375
00:49:02,559 --> 00:49:03,839
тренируетесь по тексту, а затем переходите к

1376
00:49:03,839 --> 00:49:05,680
другой модальности, которая помогает мне думать, что

1377
00:49:05,680 --> 00:49:08,079
отчасти причина этого просто в том, что

1378
00:49:08,079 --> 00:49:09,680
вы знаете, что в разных модальностях

1379
00:49:09,680 --> 00:49:11,599
существует много общей автоагрессивной структуры

1380
00:49:11,599 --> 00:49:12,559


1381
00:49:12,559 --> 00:49:13,359


1382
00:49:13,359 --> 00:49:15,920
гм и я думаю, что одна из причин этого в том,

1383
00:49:15,920 --> 00:49:16,720
что

1384
00:49:16,720 --> 00:49:18,640
язык действительно относится к

1385
00:49:18,640 --> 00:49:20,960
окружающему миру, и поэтому вы можете ожидать,

1386
00:49:20,960 --> 00:49:21,760
что

1387
00:49:21,760 --> 00:49:23,839
есть, вы знаете,

1388
00:49:23,839 --> 00:49:25,839
есть какое-то соответствие, которое

1389
00:49:25,839 --> 00:49:28,000
выходит за рамки авторегрессионной структуры,

1390
00:49:28,000 --> 00:49:30,480
поэтому есть также работы, которые показывают, что  э-э,

1391
00:49:30,480 --> 00:49:32,240
если у вас есть

1392
00:49:32,240 --> 00:49:34,079
только текстовые представления и представления только

1393
00:49:34,079 --> 00:49:36,319
изображения, вы действительно можете изучить

1394
00:49:36,319 --> 00:49:38,079
простой линейный классификатор, который может

1395
00:49:38,079 --> 00:49:40,160
научиться выравнивать оба этих представления,

1396
00:49:40,160 --> 00:49:41,440
и все эти работы просто показывают,

1397
00:49:41,440 --> 00:49:42,960
что на самом деле между смертностями намного больше общего,

1398
00:49:42,960 --> 00:49:45,599
чем мы думали в

1399
00:49:45,599 --> 00:49:46,640
начало

1400
00:49:46,640 --> 00:49:48,480
ну так что да, я думаю

1401
00:49:48,480 --> 00:49:51,280
, что можно создать текст,

1402
00:49:51,280 --> 00:49:52,240
а затем

1403
00:49:52,240 --> 00:49:54,720
точно настроить его на интересующую вас модальность,

1404
00:49:54,720 --> 00:49:55,839
и

1405
00:49:55,839 --> 00:49:58,000
это, вероятно, должно быть эффективным,

1406
00:49:58,000 --> 00:49:59,599
конечно, в зависимости от того, что такое модальность,

1407
00:49:59,599 --> 00:50:02,240
но да, для подобных изображений и видео

1408
00:50:02,240 --> 00:50:06,359
это определенно эффективно

1409
00:50:13,680 --> 00:50:16,480
больше вопросов

1410
00:50:20,000 --> 00:50:22,720
хорошо, появилось несколько вопросов,

1411
00:50:22,720 --> 00:50:24,400


1412
00:50:24,400 --> 00:50:26,960
один - в чем разница между cs

1413
00:50:26,960 --> 00:50:30,079
224 you и этим классом в te  среднеквадратичное значение

1414
00:50:30,079 --> 00:50:32,160
затронутых тем и фокус

1415
00:50:32,160 --> 00:50:33,839
вы хотите ответить на этот шакар или

1416
00:50:33,839 --> 00:50:36,480
мне стоит попытаться ответить на него,

1417
00:50:36,480 --> 00:50:39,040
возможно, вам стоит ответить на этот вопрос, хорошо, так что в

1418
00:50:39,040 --> 00:50:42,720
следующем квартале um cs224u

1419
00:50:42,720 --> 00:50:44,880
понимание естественного языка будет

1420
00:50:44,880 --> 00:50:48,559
совместно преподавать крис поттс ммм и билл

1421
00:50:48,559 --> 00:50:50,559
Маккартни,

1422
00:50:50,559 --> 00:50:51,440
так что

1423
00:50:51,440 --> 00:50:52,800
вы знаете,

1424
00:50:52,800 --> 00:50:56,960
по сути, это должно отличаться

1425
00:50:56,960 --> 00:50:59,359
от понимания того, что понимание естественного языка

1426
00:50:59,359 --> 00:51:03,040
фокусируется на том, как его зовут, типа того,

1427
00:51:03,040 --> 00:51:05,599
как построить компьютерные системы, которые

1428
00:51:05,599 --> 00:51:07,520
понимают предложения естественного

1429
00:51:07,520 --> 00:51:09,280
языка.

1430
00:51:09,280 --> 00:51:11,760


1431
00:51:11,760 --> 00:51:15,760
эм, мы также

1432
00:51:15,760 --> 00:51:18,640
понимаем естественный язык в этом

1433
00:51:18,640 --> 00:51:20,559
классе, и, конечно же, для

1434
00:51:20,559 --> 00:51:22,240
людей, которые выполняют окончательный проект по умолчанию, на

1435
00:51:22,240 --> 00:51:24,720
вопрос, отвечающий хорошо

1436
00:51:24,720 --> 00:51:26,880
, это абсолютно задача понимания естественного языка,

1437
00:51:26,880 --> 00:51:28,960


1438
00:51:28,960 --> 00:51:32,240
но различие заключается в том,

1439
00:51:32,240 --> 00:51:35,119
что вы знаете хотя бы много  то, что мы

1440
00:51:35,119 --> 00:51:37,280
делаем в этом классе,

1441
00:51:37,280 --> 00:51:40,000
например, вы знаете парсер зависимостей задания 3

1442
00:51:40,000 --> 00:51:42,559


1443
00:51:42,559 --> 00:51:44,960
или строите систему машинного перевода

1444
00:51:44,960 --> 00:51:47,119
в

1445
00:51:47,119 --> 00:51:49,760
задании 4, которую они  в каком-то смысле

1446
00:51:49,760 --> 00:51:51,760


1447
00:51:51,760 --> 00:51:54,240
задачи обработки естественного языка, где вы знаете, что обработка может означать

1448
00:51:54,240 --> 00:51:56,960
что угодно, но обычно означает, что вы делаете

1449
00:51:56,960 --> 00:51:58,800
полезные

1450
00:51:58,800 --> 00:52:01,680
полезные интеллектуальные вещи с вводом человеческого

1451
00:52:01,680 --> 00:52:03,920
языка, но вы не

1452
00:52:03,920 --> 00:52:06,800
обязательно глубоко понимаете его, поэтому

1453
00:52:06,800 --> 00:52:08,240
есть некоторое

1454
00:52:08,240 --> 00:52:10,640
перекрытие в классах  если вы выполните

1455
00:52:10,640 --> 00:52:14,160
cs224u, вы обязательно снова увидите векторы слов

1456
00:52:14,160 --> 00:52:18,000
и преобразователи, но упор

1457
00:52:18,000 --> 00:52:20,480
делается на то, чтобы делать гораздо больше с

1458
00:52:20,480 --> 00:52:23,359
задачами понимания естественного языка, и поэтому это

1459
00:52:23,359 --> 00:52:24,559
включает в себя

1460
00:52:24,559 --> 00:52:28,000
такие вещи, как создание семантических парсеров, чтобы

1461
00:52:28,000 --> 00:52:31,280
они были такими устройствами, которые

1462
00:52:31,280 --> 00:52:33,520
вы знаете  отвечать на вопросы и

1463
00:52:33,520 --> 00:52:36,559
команды, такие как alexa или google

1464
00:52:36,559 --> 00:52:40,000
assistant, будут

1465
00:52:40,000 --> 00:52:42,800
строить системы извлечения отношений, которые извлекают

1466
00:52:42,800 --> 00:52:45,040
конкретные факты из

1467
00:52:45,040 --> 00:52:49,040
фрагмента текста всего этого человека,

1468
00:52:49,040 --> 00:52:52,480
занимающего эту должность в этой компании,

1469
00:52:52,480 --> 00:52:55,359
смотрящего на обоснованное изучение языка

1470
00:52:55,359 --> 00:52:57,359
и обоснованное понимание языка,

1471
00:52:57,359 --> 00:52:59,280
где  вы используете не только

1472
00:52:59,280 --> 00:53:01,440
язык, но и мировой контекст для

1473
00:53:01,440 --> 00:53:04,240
получения информации

1474
00:53:04,240 --> 00:53:06,400
и других задач такого рода, я имею в виду, что я гу  эсс,

1475
00:53:06,400 --> 00:53:08,880
вы можете посмотреть веб-сайт, чтобы получить

1476
00:53:08,880 --> 00:53:11,520
более подробную информацию, я имею в виду, что

1477
00:53:11,520 --> 00:53:14,079
вы знаете, что относится к этому классу, я имею в виду, что

1478
00:53:14,079 --> 00:53:16,480
многие люди также находят

1479
00:53:16,480 --> 00:53:19,920
возможность просто продвинуться дальше в выполнении

1480
00:53:19,920 --> 00:53:20,800


1481
00:53:20,800 --> 00:53:22,240
проекта

1482
00:53:22,240 --> 00:53:23,839
в области обработки естественного языка

1483
00:53:23,839 --> 00:53:26,319
такого рода  по

1484
00:53:26,319 --> 00:53:28,640
характеру структуры класса, поскольку вы

1485
00:53:28,640 --> 00:53:31,440
знаете, что он больше предполагает, что люди знают,

1486
00:53:31,440 --> 00:53:32,960
как создавать системы естественного языка с глубоким обучением

1487
00:53:32,960 --> 00:53:34,160


1488
00:53:34,160 --> 00:53:36,000
в начале, что вместо

1489
00:53:36,000 --> 00:53:37,359
того,

1490
00:53:37,359 --> 00:53:39,920
чтобы большой процент класса переходил

1491
00:53:39,920 --> 00:53:41,760
в норму, вы должны выполнить все эти

1492
00:53:41,760 --> 00:53:43,760
задания, хотя  ранее были небольшие

1493
00:53:43,760 --> 00:53:46,000
задания, чтобы было

1494
00:53:46,000 --> 00:53:48,720
больше времени для работы над проектом

1495
00:53:48,720 --> 00:53:51,280
на квартал.

1496
00:53:54,079 --> 00:53:56,079
Хорошо, вот еще один вопрос, который, возможно,

1497
00:53:56,079 --> 00:53:57,440
мог бы решить Чикаго

1498
00:53:57,440 --> 00:53:59,520
, знаете ли вы о попытках краудсорсинга

1499
00:53:59,520 --> 00:54:02,160
динамических тестов, например,

1500
00:54:02,160 --> 00:54:05,280
использует загрузку состязательных примеров для

1501
00:54:05,280 --> 00:54:09,040
оценки или онлайн-обучения

1502
00:54:09,599 --> 00:54:12,240
да, на самом

1503
00:54:12,240 --> 00:54:14,000
деле основная

1504
00:54:14,000 --> 00:54:16,400
идея заключается в том, чтобы правильно использовать краудсорсинг,

1505
00:54:16,400 --> 00:54:19,119
так что на самом деле есть эта скамейка, ну, вот

1506
00:54:19,119 --> 00:54:21,359
эта платформа, которая была

1507
00:54:21,359 --> 00:54:24,800
создана  Ted by Bear, это называется динамометрическим стендом,

1508
00:54:24,800 --> 00:54:27,040
и цель состоит в том,

1509
00:54:27,040 --> 00:54:29,359
чтобы построить этот небольшой динамически

1510
00:54:29,359 --> 00:54:30,640
развивающийся

1511
00:54:30,640 --> 00:54:32,720
эталонный тест, мы просто собираемся

1512
00:54:32,720 --> 00:54:35,119
разгрузить его, чтобы вы знали пользователей этой

1513
00:54:35,119 --> 00:54:36,160
платформы,

1514
00:54:36,160 --> 00:54:37,839
и вы знаете, что он по существу

1515
00:54:37,839 --> 00:54:40,000
дает вам утилиты для вроде

1516
00:54:40,000 --> 00:54:42,319
развертывания  ваша модель, а затем, э-э,

1517
00:54:42,319 --> 00:54:44,400
вы знаете, что люди вроде пытаются обмануть

1518
00:54:44,400 --> 00:54:45,359
модель,

1519
00:54:45,359 --> 00:54:46,480


1520
00:54:46,480 --> 00:54:48,480
да, так что это похоже на то,

1521
00:54:48,480 --> 00:54:49,599


1522
00:54:49,599 --> 00:54:51,839
как динамическая оценка динамической

1523
00:54:51,839 --> 00:54:54,160
эталонной

1524
00:54:54,160 --> 00:54:56,640
коллекции на самом деле работает так, как будто вы

1525
00:54:56,640 --> 00:54:58,000
развертываете модель

1526
00:54:58,000 --> 00:54:58,880


1527
00:54:58,880 --> 00:55:00,960
на какой-то платформе, а затем  вы заставляете людей

1528
00:55:00,960 --> 00:55:03,520
любить дурачить систему.

1529
00:55:03,520 --> 00:55:06,520


1530
00:55:18,640 --> 00:55:20,319


1531
00:55:20,319 --> 00:55:23,040


1532
00:55:23,040 --> 00:55:25,359


1533
00:55:25,359 --> 00:55:27,280


1534
00:55:27,280 --> 00:55:29,920


1535
00:55:29,920 --> 00:55:31,920


1536
00:55:31,920 --> 00:55:35,280


1537
00:55:35,280 --> 00:55:37,359
Правильно,

1538
00:55:37,359 --> 00:55:38,799
этот вид пытался масштабировать

1539
00:55:38,799 --> 00:55:40,960
трансформеры, чтобы они нравились действительно большой, э-э,

1540
00:55:40,960 --> 00:55:43,359
линза контекста, эм, один из них, как

1541
00:55:43,359 --> 00:55:44,720
реформатор,

1542
00:55:44,720 --> 00:55:46,720
эм, а также как трансформер

1543
00:55:46,720 --> 00:55:48,880
excel  я думаю, что это был первый, кто

1544
00:55:48,880 --> 00:55:50,240
попытался это сделать,

1545
00:55:50,240 --> 00:55:51,599


1546
00:55:51,599 --> 00:55:56,640
я думаю, что неясно,

1547
00:55:56,640 --> 00:55:59,440
можете ли вы совместить это с масштабом

1548
00:55:59,440 --> 00:56:00,839
этих gpt-подобных

1549
00:56:00,839 --> 00:56:03,760
моделей, и если вы увидите качественно

1550
00:56:03,760 --> 00:56:06,160
разные вещи, когда сделаете это,

1551
00:56:06,160 --> 00:56:08,640
например, и часть  просто

1552
00:56:08,640 --> 00:56:10,960
все это похоже на недавний правый,

1553
00:56:10,960 --> 00:56:12,640
но да, я думаю, что открытый вопрос

1554
00:56:12,640 --> 00:56:14,240
заключается в том, что

1555
00:56:14,240 --> 00:56:15,520
вы знаете, можете ли вы принять их как действительно

1556
00:56:15,520 --> 00:56:17,920
длинные преобразователи контекста,

1557
00:56:17,920 --> 00:56:20,559
которые могут работать в длинном контексте,

1558
00:56:20,559 --> 00:56:23,200
объединить это с масштабом gpd3,

1559
00:56:23,200 --> 00:56:25,599
а затем получить  модели, которые действительно могут

1560
00:56:25,599 --> 00:56:27,119
рассуждать о них, как действительно большие

1561
00:56:27,119 --> 00:56:28,160
контексты,

1562
00:56:28,160 --> 00:56:29,200


1563
00:56:29,200 --> 00:56:31,119
потому что я предполагаю, что гипотеза масштаба

1564
00:56:31,119 --> 00:56:32,799
состоит в том, что как только вы тренируете языковые модели

1565
00:56:32,799 --> 00:56:35,280
в масштабе, они могут начать делать эти

1566
00:56:35,280 --> 00:56:36,079
вещи,

1567
00:56:36,079 --> 00:56:39,040
и поэтому для этого в длинном контексте нам

1568
00:56:39,040 --> 00:56:40,640
действительно нужно нравиться

1569
00:56:40,640 --> 00:56:42,799
у вас есть длинные преобразователи контекста, которые

1570
00:56:42,799 --> 00:56:44,960
обучены в масштабе, и я думаю, что

1571
00:56:44,960 --> 00:56:48,000
люди еще этого не сделали,

1572
00:56:55,440 --> 00:56:57,440
поэтому я вижу этот другой вопрос об

1573
00:56:57,440 --> 00:56:58,710
овладении языком

1574
00:56:58,710 --> 00:57:00,079
[Музыка],

1575
00:57:00,079 --> 00:57:01,119
потому что у вас есть какие-то мысли по

1576
00:57:01,119 --> 00:57:02,400
этому поводу?

1577
00:57:02,400 --> 00:57:06,480
Может быть, я смогу что-то сделать с этим,

1578
00:57:08,319 --> 00:57:10,319


1579
00:57:10,319 --> 00:57:13,359
да, поэтому вопрос в том, что, по вашему

1580
00:57:13,359 --> 00:57:15,599
мнению, мы можем извлечь из детского овладения языком,

1581
00:57:15,599 --> 00:57:18,079
можем ли мы построить языковую

1582
00:57:18,079 --> 00:57:20,319
модель более интерактивным способом, например,

1583
00:57:20,319 --> 00:57:23,119
обучением с подкреплением, знаете ли вы

1584
00:57:23,119 --> 00:57:25,440
какие-либо из этих попыток,

1585
00:57:25,440 --> 00:57:28,480
о, это  это большой, огромный вопрос, и

1586
00:57:28,480 --> 00:57:30,880
вы знаете, я думаю, что короткий

1587
00:57:30,880 --> 00:57:32,880
бесполезный ответ заключается в том, что

1588
00:57:32,880 --> 00:57:35,440
на данный момент нет ответов, как вы

1589
00:57:35,440 --> 00:57:38,559
знаете, люди, безусловно, пытались делать

1590
00:57:38,559 --> 00:57:41,359
что-то в различных масштабах, но вы знаете, что у нас

1591
00:57:41,359 --> 00:57:42,559
просто

1592
00:57:42,559 --> 00:57:45,839
нет технологии, которая  наименее

1593
00:57:45,839 --> 00:57:48,640
убедителен для того, чтобы

1594
00:57:48,640 --> 00:57:51,119
воспроизвести способность человеческого ребенка к изучению языка,

1595
00:57:51,119 --> 00:57:54,720
но после этого

1596
00:57:54,720 --> 00:57:58,319
пролога я могу сказать, что да

1597
00:57:58,319 --> 00:58:01,359
, у вас в голове определенно есть идеи,

1598
00:58:01,359 --> 00:58:04,319
чтобы вы знали, что есть какие-то

1599
00:58:04,319 --> 00:58:08,160
четкие результаты, которые  что

1600
00:58:08,160 --> 00:58:10,240
маленькие дети не учатся, просматривая

1601
00:58:10,240 --> 00:58:14,559
видео, поэтому кажется, что взаимодействие

1602
00:58:14,559 --> 00:58:16,640
является ключевым моментом

1603
00:58:16,640 --> 00:58:20,559
эм маленькие дети учатся не только на языке,

1604
00:58:20,559 --> 00:58:22,400
они в очень

1605
00:58:22,400 --> 00:58:25,520
богатой среде, где люди вроде как

1606
00:58:25,520 --> 00:58:27,280


1607
00:58:27,280 --> 00:58:30,640


1608
00:58:30,640 --> 00:58:33,280


1609
00:58:33,280 --> 00:58:36,319


1610
00:58:36,319 --> 00:58:38,960


1611
00:58:38,960 --> 00:58:42,000


1612
00:58:42,000 --> 00:58:45,280


1613
00:58:45,280 --> 00:58:47,760
обоихи обучения вещи из окружающей среды в целом и, в частности, вы знаете, что они учатся много от того, что риск приобретения языка гм исследователи называют как внимание, которое отличается то, что мы имеем в виду внимания, но это означает, что попечитель будет смотреть на  объект, который находится в

1614
00:58:47,760 --> 00:58:49,839
центре внимания, и вы обычно знаете и другие

1615
00:58:49,839 --> 00:58:51,599
вещи, например, вроде того, что вы знаете, как

1616
00:58:51,599 --> 00:58:53,119
поднять его и поднести к

1617
00:58:53,119 --> 00:58:57,280
ребенку, и все такие вещи, эм,

1618
00:58:57,280 --> 00:58:59,680
и вы знаете, что

1619
00:58:59,680 --> 00:59:02,400
младенцы и маленькие дети могут

1620
00:59:02,400 --> 00:59:04,480
много экспериментировать, так что независимо от

1621
00:59:04,480 --> 00:59:07,040
о том, узнает ли он, что происходит,

1622
00:59:07,040 --> 00:59:09,839
когда у вас есть какие-то блоки, которые вы

1623
00:59:09,839 --> 00:59:13,760
складываете и играете с ними, или вы

1624
00:59:13,760 --> 00:59:16,640
изучаете язык, вы своего рода экспериментируете

1625
00:59:16,640 --> 00:59:18,880
, пробуя некоторые вещи и

1626
00:59:18,880 --> 00:59:20,880
смотрите, какой ответ вы получите, и

1627
00:59:20,880 --> 00:59:23,359
опять же, это, по сути,

1628
00:59:23,359 --> 00:59:24,400
основано

1629
00:59:24,400 --> 00:59:26,559
на интерактивности  из того, что вы

1630
00:59:26,559 --> 00:59:29,359
получаете какой-то ответ на любые

1631
00:59:29,359 --> 00:59:32,079
ваши возражения, и вы знаете, что это

1632
00:59:32,079 --> 00:59:33,680
то, что как бы горячо

1633
00:59:33,680 --> 00:59:35,520
обсуждается в языковых приобретениях.  по

1634
00:59:35,520 --> 00:59:39,040
литературе, так что традиционная позиция чавкания

1635
00:59:39,040 --> 00:59:41,520


1636
00:59:41,520 --> 00:59:43,680
- это то, что вы знаете, что

1637
00:59:43,680 --> 00:59:47,920
люди не получают эффективной

1638
00:59:47,920 --> 00:59:51,119
обратной связи, вы знаете контролируемые ярлыки, когда

1639
00:59:51,119 --> 00:59:53,920
они говорят, и вы знаете, что в некотором очень

1640
00:59:53,920 --> 00:59:56,160
узком смысле это правда, верно, это

1641
00:59:56,160 --> 00:59:57,920
просто не тот случай, когда ребенок

1642
00:59:57,920 --> 01:00:00,240
пытается  скажите что-нибудь, что они получают

1643
01:00:00,240 --> 01:00:02,960
обратную связь, вы знаете, синтаксическая ошибка на

1644
01:00:02,960 --> 01:00:05,680
английском языке в четвертом слове,

1645
01:00:05,680 --> 01:00:07,119
или им

1646
01:00:07,119 --> 01:00:10,160
дается вот семантическая форма, которую я взял

1647
01:00:10,160 --> 01:00:12,160
из вашего высказывания,

1648
01:00:12,160 --> 01:00:13,119


1649
01:00:13,119 --> 01:00:15,680
но более косвенным образом они явно получают

1650
01:00:15,680 --> 01:00:17,920
огромную обратную связь, они могут видеть,

1651
01:00:17,920 --> 01:00:20,960
какой ответ они  получать от их

1652
01:00:20,960 --> 01:00:25,599
опекуна на каждом углу, и так, как

1653
01:00:25,599 --> 01:00:27,040
в своем вопросе,

1654
01:00:27,040 --> 01:00:30,160
вы предлагали каким-

1655
01:00:30,160 --> 01:00:32,319
то

1656
01:00:32,319 --> 01:00:34,240
образом использовать обучение с подкреплением,

1657
01:00:34,240 --> 01:00:35,839
потому что у нас есть что-то вроде сигнала вознаграждения,

1658
01:00:35,839 --> 01:00:38,880
и вы знаете, что в

1659
01:00:38,880 --> 01:00:42,640
целом я бы  Сказать привет да я согласен

1660
01:00:42,640 --> 01:00:45,520
с точки зрения гораздо более конкретного способа

1661
01:00:45,520 --> 01:00:47,440
, а как мы можем заставить это

1662
01:00:47,440 --> 01:00:49,200
работать, чтобы выучить что-то с

1663
01:00:49,200 --> 01:00:52,559
богатством человеческого языка я

1664
01:00:52,559 --> 01:00:53,680
вы знаете,

1665
01:00:53,680 --> 01:00:57,440
я худой  k, у нас нет большого представления, но вы

1666
01:00:57,440 --> 01:00:59,599
знаете, что началась некоторая работа,

1667
01:00:59,599 --> 01:01:01,599
поэтому

1668
01:01:01,599 --> 01:01:04,319
люди вроде как создают виртуальные

1669
01:01:04,319 --> 01:01:06,799
среды, в которых, как вы

1670
01:01:06,799 --> 01:01:08,880
знаете, есть ваш

1671
01:01:08,880 --> 01:01:11,359
аватар, которые можно манипулировать в

1672
01:01:11,359 --> 01:01:12,960
виртуальной среде, и есть

1673
01:01:12,960 --> 01:01:15,599
лингвистический ввод и  он может преуспеть в

1674
01:01:15,599 --> 01:01:18,079
получении вознаграждения за своего рода выполнение

1675
01:01:18,079 --> 01:01:19,760
команды, где команда может быть

1676
01:01:19,760 --> 01:01:21,359
чем-то вроде того, что вы знаете, поднимите

1677
01:01:21,359 --> 01:01:24,480
оранжевый блок или что-то в этом роде,

1678
01:01:24,480 --> 01:01:25,440
и

1679
01:01:25,440 --> 01:01:27,440
вы знаете,

1680
01:01:27,440 --> 01:01:29,920
что в небольшой степени люди

1681
01:01:29,920 --> 01:01:33,200
смогли создать вещи, которые работают, я имею в виду,

1682
01:01:33,200 --> 01:01:36,240
как я  так как вы, возможно, забираете, я имею в виду, я

1683
01:01:36,240 --> 01:01:37,680
думаю, что

1684
01:01:37,680 --> 01:01:40,079
до сих пор, по крайней мере, я просто был

1685
01:01:40,079 --> 01:01:42,240
не в восторге, потому что кажется, что

1686
01:01:42,240 --> 01:01:44,960
сложность того, чего достигли

1687
01:01:44,960 --> 01:01:46,480
люди, вроде как

1688
01:01:46,480 --> 01:01:48,799
вы знаете, настолько примитивна по сравнению

1689
01:01:48,799 --> 01:01:51,040
с полной сложной сложностью языка

1690
01:01:51,040 --> 01:01:53,200
да, вы знаете, какие языки

1691
01:01:53,200 --> 01:01:55,200
, на которых люди

1692
01:01:55,200 --> 01:01:58,000
смогли выучить системы, - это те, которые могут

1693
01:01:58,000 --> 01:02:00,559
подбирать команды, на которых они могут выучить,

1694
01:02:00,559 --> 01:02:03,520
вы знаете, синий куб или оранжевая

1695
01:02:03,520 --> 01:02:06,160
сфера, и это  или о том, как

1696
01:02:06,160 --> 01:02:08,880
далеко продвинулись люди, и о

1697
01:02:08,880 --> 01:02:12,480
таком крохотном уголке того, что связано

1698
01:02:12,480 --> 01:02:16,319
с изучением человеческого языка

1699
01:02:16,960 --> 01:02:19,440
, я просто добавлю к этому одну вещь: я

1700
01:02:19,440 --> 01:02:23,039
думаю, что есть некоторые

1701
01:02:23,039 --> 01:02:26,079
принципы обучения детей, которые люди пытались

1702
01:02:26,079 --> 01:02:28,000
применительно к глубокому обучению, и один

1703
01:02:28,000 --> 01:02:30,000
пример, который приходит на ум, - это обучение по учебной программе,

1704
01:02:30,000 --> 01:02:31,039


1705
01:02:31,039 --> 01:02:31,920


1706
01:02:31,920 --> 01:02:33,200
где есть много литературы,

1707
01:02:33,200 --> 01:02:35,839
которая показывает, что вы знаете детей, а они

1708
01:02:35,839 --> 01:02:37,599
склонны обращать внимание на вещи,

1709
01:02:37,599 --> 01:02:38,960


1710
01:02:38,960 --> 01:02:40,880
которые для них просто немного сложны,

1711
01:02:40,880 --> 01:02:42,400
и они не делают этого.  не обращают внимания на

1712
01:02:42,400 --> 01:02:44,160
чрезвычайно сложные задачи,

1713
01:02:44,160 --> 01:02:45,680
а также не обращают внимания на те вещи,

1714
01:02:45,680 --> 01:02:47,280
которые они умеют решать,

1715
01:02:47,280 --> 01:02:49,599
и многие исследователи действительно

1716
01:02:49,599 --> 01:02:51,920
пытались заставить обучение по учебной программе работать.

1717
01:02:51,920 --> 01:02:53,039


1718
01:02:53,039 --> 01:02:56,160


1719
01:02:56,160 --> 01:02:57,680
работать, когда вы находитесь, как

1720
01:02:57,680 --> 01:03:00,000
настройки обучения с подкреплением, но

1721
01:03:00,000 --> 01:03:01,920
неясно, будет ли это работать как

1722
01:03:01,920 --> 01:03:03,920
настройки контролируемого обучения, но я все еще

1723
01:03:03,920 --> 01:03:05,520
думаю, что это как малоизученные, и,

1724
01:03:05,520 --> 01:03:06,400
возможно,

1725
01:03:06,400 --> 01:03:08,000
вы знаете  Должно

1726
01:03:08,000 --> 01:03:10,720
быть больше попыток посмотреть, можем ли мы

1727
01:03:10,720 --> 01:03:11,760


1728
01:03:11,760 --> 01:03:13,839
добавить в учебный план, и если это

1729
01:03:13,839 --> 01:03:16,240
улучшит что-нибудь?

1730
01:03:16,240 --> 01:03:17,680
Да,

1731
01:03:17,680 --> 01:03:19,440
я согласен, изучение учебного плана -

1732
01:03:19,440 --> 01:03:22,000
важная идея, о которой мы на самом деле не

1733
01:03:22,000 --> 01:03:24,720
говорили, но кажется, что это,

1734
01:03:24,720 --> 01:03:27,520
безусловно, важно для  человеческое обучение гм,

1735
01:03:27,520 --> 01:03:29,440
и в мире машинного обучения с ним были некоторые незначительные успехи,

1736
01:03:29,440 --> 01:03:32,000


1737
01:03:32,000 --> 01:03:34,720
но похоже, что это идея, с которой

1738
01:03:34,720 --> 01:03:36,240
вы сможете сделать гораздо больше

1739
01:03:36,240 --> 01:03:39,520
в будущем, поскольку вы переходите от

1740
01:03:39,520 --> 01:03:42,319
моделей, которые просто делают одну узкую

1741
01:03:42,319 --> 01:03:44,720
задача, которая пытается сделать более общий

1742
01:03:44,720 --> 01:03:47,359


1743
01:03:47,359 --> 01:03:49,359
процесс овладения языком,

1744
01:03:49,359 --> 01:03:51,280
если я попытаюсь ответить на следующий вопрос,

1745
01:03:51,280 --> 01:03:54,079
хорошо, следующий вопрос заключается в том,

1746
01:03:54,079 --> 01:03:56,480
почему люди изучают языки лучше

1747
01:03:56,480 --> 01:03:58,480
только потому, что мы предварительно обучены за

1748
01:03:58,480 --> 01:04:01,280
миллионы лет моделирования физики,

1749
01:04:01,280 --> 01:04:04,559
может быть, нам следует заранее  - тренируйте модель таким

1750
01:04:04,559 --> 01:04:07,520
же образом, я имею в виду, я предполагаю, что то, что вы

1751
01:04:07,520 --> 01:04:10,400
говорите, является симуляцией физики, эм, вы

1752
01:04:10,400 --> 01:04:12,960
вызываете эволюцию, когда говорите

1753
01:04:12,960 --> 01:04:16,319
о миллионах лет, так что вы знаете, что  это

1754
01:04:16,319 --> 01:04:17,200


1755
01:04:17,200 --> 01:04:19,039


1756
01:04:19,039 --> 01:04:23,200
спорный, обсуждаемый большой вопрос,

1757
01:04:23,200 --> 01:04:24,799
так что

1758
01:04:24,799 --> 01:04:26,400
вы знаете еще раз,

1759
01:04:26,400 --> 01:04:28,720
если я снова воспользуюсь хомским, так что

1760
01:04:28,720 --> 01:04:30,319
Ноам Хомский - своего рода

1761
01:04:30,319 --> 01:04:36,079
самый известный гм лингвист в мире,

1762
01:04:36,079 --> 01:04:38,880
и вы знаете, что карьера Ноама Хомского,

1763
01:04:38,880 --> 01:04:42,079
начиная с 1950-х годов, построена

1764
01:04:42,079 --> 01:04:45,119
на идее, что

1765
01:04:45,119 --> 01:04:46,559
мало  дети

1766
01:04:46,559 --> 01:04:47,440
получают

1767
01:04:47,440 --> 01:04:48,400
такой

1768
01:04:48,400 --> 01:04:51,599
сомнительный лингвистический ввод, потому что вы

1769
01:04:51,599 --> 01:04:53,599
знаете, что они слышат кучу случайных вещей,

1770
01:04:53,599 --> 01:04:55,359
они не получают особой обратной связи о том,

1771
01:04:55,359 --> 01:05:00,000
что они говорят, и т. д.

1772
01:05:00,000 --> 01:05:01,200


1773
01:05:01,200 --> 01:05:04,000


1774
01:05:04,000 --> 01:05:06,000


1775
01:05:06,000 --> 01:05:09,119


1776
01:05:09,119 --> 01:05:13,760
значительная часть человеческого языка,

1777
01:05:13,760 --> 01:05:14,799
ээээ,

1778
01:05:14,799 --> 01:05:17,520
врожденная или в виде человеческого генома,

1779
01:05:17,520 --> 01:05:19,520
младенцы рождаются с этим, и это

1780
01:05:19,520 --> 01:05:22,079
объясняет чудо, благодаря которому очень

1781
01:05:22,079 --> 01:05:24,240
маленькие люди

1782
01:05:24,240 --> 01:05:27,440
удивительно быстро учатся тому, как работают человеческие языки.

1783
01:05:27,440 --> 01:05:28,400


1784
01:05:28,400 --> 01:05:29,200


1785
01:05:29,200 --> 01:05:32,079


1786
01:05:32,079 --> 01:05:36,240
вы, кто не был рядом, эм,

1787
01:05:36,240 --> 01:05:39,359
маленькие дети, я имею в виду, я думаю,

1788
01:05:39,359 --> 01:05:42,240
нужно просто признать, что вы знаете

1789
01:05:42,240 --> 01:05:45,039
, что

1790
01:05:45,039 --> 01:05:47,920
живые маленькие дети овладевают человеческим языком у меня  и это действительно

1791
01:05:47,920 --> 01:05:50,240
кажется чудесным, но вы проходите

1792
01:05:50,240 --> 01:05:53,200
через такую медленную фазу в течение

1793
01:05:53,200 --> 01:05:56,799
нескольких лет, когда вы знаете,

1794
01:05:56,799 --> 01:05:59,280
что дети вроде гуся и болтают несколько

1795
01:05:59,280 --> 01:06:01,599
слогов, а затем довольно длительный

1796
01:06:01,599 --> 01:06:04,160
период, когда они подбирают несколько слов

1797
01:06:04,160 --> 01:06:07,200
и они могут сказать сок сок, эм, когда

1798
01:06:07,200 --> 01:06:08,960
они хотят выпить немного сока и

1799
01:06:08,960 --> 01:06:11,200
ничего больше, и тогда

1800
01:06:11,200 --> 01:06:13,599
кажется, что есть фаза,

1801
01:06:13,599 --> 01:06:16,720
когда дети внезапно понимают, подождите,

1802
01:06:16,720 --> 01:06:18,880
это продуктивная генеративная

1803
01:06:18,880 --> 01:06:21,520
система предложений, я могу произносить целые предложения, а

1804
01:06:21,520 --> 01:06:23,520
затем в  невероятно

1805
01:06:23,520 --> 01:06:25,440
короткий период они вроде как

1806
01:06:25,440 --> 01:06:28,000
переходят от произнесения одного и двух

1807
01:06:28,000 --> 01:06:32,000
слов к внезапно они могут сказать, что ты

1808
01:06:32,000 --> 01:06:33,440
знаешь,

1809
01:06:33,440 --> 01:06:36,000
папа пришел домой в гараж

1810
01:06:36,000 --> 01:06:39,920
эм пудинг байк в гараже, и ты удивляешься,

1811
01:06:39,920 --> 01:06:42,079
как они внезапно открывают для себя язык

1812
01:06:42,079 --> 01:06:45,280
хм, так что ты это знаешь

1813
01:06:45,280 --> 01:06:48,480
это отчасти удивительно, но

1814
01:06:48,480 --> 01:06:51,440
лично для меня, по крайней мере,

1815
01:06:51,440 --> 01:06:52,240
вы знаете,

1816
01:06:52,240 --> 01:06:55,200
я просто никогда не верил в сильные

1817
01:06:55,200 --> 01:06:58,640
версии гипотезы о том, что у

1818
01:06:58,640 --> 01:07:00,880
людей есть

1819
01:07:00,880 --> 01:07:04,640
много специфических языковых

1820
01:07:04,640 --> 01:07:07,039
знаний  или структура в их мозгу,

1821
01:07:07,039 --> 01:07:10,079
которая происходит от генетической наследственности, например,

1822
01:07:10,079 --> 01:07:11,200
очевидно, что у

1823
01:07:11,200 --> 01:07:14,319
людей действительно есть эти очень умные мозги,

1824
01:07:14,319 --> 01:07:17,119
и если мы находимся на уровне того, чтобы сказать,

1825
01:07:17,119 --> 01:07:20,559
что способность думать или способность

1826
01:07:20,559 --> 01:07:23,359
интерпретировать визуальный мир

1827
01:07:23,359 --> 01:07:26,880
, это вещи, которые развивались за

1828
01:07:26,880 --> 01:07:29,200
десятки миллионов  лет

1829
01:07:29,200 --> 01:07:31,839
и эволюция

1830
01:07:31,839 --> 01:07:34,559
могут быть значительной частью объяснения,

1831
01:07:34,559 --> 01:07:35,440
и

1832
01:07:35,440 --> 01:07:37,039
люди

1833
01:07:37,039 --> 01:07:39,680
явно рождаются с большим количеством

1834
01:07:39,680 --> 01:07:42,640
аппаратного обеспечения, специфичного для зрения, в их мозгу, как и

1835
01:07:42,640 --> 01:07:44,400
многие другие существа,

1836
01:07:44,400 --> 01:07:48,240
но когда вы переходите к языку, вы знаете, что

1837
01:07:48,240 --> 01:07:49,680


1838
01:07:49,680 --> 01:07:52,960
никто не знает, когда язык был  в

1839
01:07:52,960 --> 01:07:54,960
некой современной форме, которая

1840
01:07:54,960 --> 01:07:57,280
впервые стала доступной, потому что вы знаете,

1841
01:07:57,280 --> 01:07:59,839
что нет никаких окаменелостей людей,

1842
01:07:59,839 --> 01:08:00,799
говорящих, что

1843
01:08:00,799 --> 01:08:03,520
вы знаете слово um spear или что-

1844
01:08:03,520 --> 01:08:06,240
то в этом роде, но вы знаете,

1845
01:08:06,240 --> 01:08:08,799
что есть оценки, основанные на сортировке,

1846
01:08:08,799 --> 01:08:11,119
так что вы можете  увидеть

1847
01:08:11,119 --> 01:08:12,319
распространение

1848
01:08:12,319 --> 01:08:13,520
эм-

1849
01:08:13,520 --> 01:08:16,000
прото-людей и

1850
01:08:16,000 --> 01:08:19,040
их вид очевидных социальных структур, исходя

1851
01:08:19,040 --> 01:08:21,839
из того, что вы можете найти в окаменелостях, как вы

1852
01:08:21,839 --> 01:08:24,640
знаете, большинство людей предполагают, что язык составляет

1853
01:08:24,640 --> 01:08:28,640
не более миллиона лет.  старый, и вы знаете,

1854
01:08:28,640 --> 01:08:32,238
что это слишком короткое время для любого

1855
01:08:32,238 --> 01:08:35,040
значительного накануне эволюции, чтобы

1856
01:08:35,040 --> 01:08:37,679
построить какую-либо значительную структуру в

1857
01:08:37,679 --> 01:08:39,920
человеческом мозгу, специфичную для языка,

1858
01:08:39,920 --> 01:08:41,120
поэтому

1859
01:08:41,120 --> 01:08:43,839
я как бы думаю, что

1860
01:08:43,839 --> 01:08:46,319
рабочее предположение должно быть таким, что

1861
01:08:46,319 --> 01:08:49,040
просто нет ничего

1862
01:08:49,040 --> 01:08:51,759
относится к языку и человеческому мозгу,

1863
01:08:51,759 --> 01:08:54,319
и вы знаете наиболее правдоподобную

1864
01:08:54,319 --> 01:08:56,799
гипотезу, хотя я не очень много знаю

1865
01:08:56,799 --> 01:08:59,040
о нейробиологии, когда дело доходит до

1866
01:08:59,040 --> 01:09:02,640
того, что люди могли

1867
01:09:02,640 --> 01:09:05,679
перепрофилировать оборудование, которое изначально было

1868
01:09:05,679 --> 01:09:08,399
создано для других целей, таких как визуальная

1869
01:09:08,399 --> 01:09:11,920
интерпретация сцены и память и

1870
01:09:11,920 --> 01:09:14,640
это дало основу для своего рода наличия

1871
01:09:14,640 --> 01:09:16,719
всего этого умного оборудования, которое вы

1872
01:09:16,719 --> 01:09:18,399
затем могли бы использовать для языка, так что вы знаете, что это похоже на

1873
01:09:18,399 --> 01:09:20,880
то, как будто графические процессоры были изобретены для

1874
01:09:20,880 --> 01:09:23,279
компьютерных игр, и мы

1875
01:09:23,279 --> 01:09:25,439
смогли перепрофилировать это оборудование для глубокого

1876
01:09:25,439 --> 01:09:28,439
обучения, которое

1877
01:09:34,080 --> 01:09:35,040
у нас есть

1878
01:09:35,040 --> 01:09:38,238
многие из них вышли в конце,

1879
01:09:38,238 --> 01:09:41,759
хорошо, так что на этот ответят в прямом эфире,

1880
01:09:41,759 --> 01:09:42,799
давай посмотрим,

1881
01:09:42,799 --> 01:09:45,120
да, если бы ты мог назвать, я думаю, это

1882
01:09:45,120 --> 01:09:48,640
для любого из вас  основное узкое место в

1883
01:09:48,640 --> 01:09:50,158
отношении

1884
01:09:50,158 --> 01:09:52,319
эээ, если бы мы могли эффективно обеспечивать обратную связь

1885
01:09:52,319 --> 01:09:54,960
с нашими системами, например, младенцам

1886
01:09:54,960 --> 01:09:57,199
дают обратную связь, какое узкое место

1887
01:09:57,199 --> 01:10:00,719
остается в

1888
01:10:00,719 --> 01:10:03,840
попытках овладеть языком, более похожим на человеческий,

1889
01:10:03,840 --> 01:10:06,840


1890
01:10:20,400 --> 01:10:22,320
эм, я имею в виду, что я вроде как

1891
01:10:22,320 --> 01:10:24,000
не могу найти об этом снова или хотел бы  вы

1892
01:10:24,000 --> 01:10:25,840
начинаете выпускать что-то

1893
01:10:25,840 --> 01:10:27,679
да, я просто хотел сказать, что я думаю,

1894
01:10:27,679 --> 01:10:30,239
что все немного

1895
01:10:30,239 --> 01:10:31,520
правильно, как

1896
01:10:31,520 --> 01:10:33,760
я думаю в терминах моделей

1897
01:10:33,760 --> 01:10:35,760
, я скажу одно, что мы знаем

1898
01:10:35,760 --> 01:10:38,239
, что в мозгу больше обратных связей

1899
01:10:38,239 --> 01:10:40,960
и обратных связей

1900
01:10:40,960 --> 01:10:44,000
мы на самом деле не придумали

1901
01:10:44,000 --> 01:10:44,960
способ своего

1902
01:10:44,960 --> 01:10:45,920
рода

1903
01:10:45,920 --> 01:10:49,120
э-э, так что вы знаете, конечно, у нас были rnns эм,

1904
01:10:49,120 --> 01:10:51,040
вы знаете, какой тип реализации, как

1905
01:10:51,040 --> 01:10:52,159
вы знаете, вам может понравиться, просмотреть

1906
01:10:52,159 --> 01:10:54,000
элемент, который вроде реализует цикл обратной связи,

1907
01:10:54,000 --> 01:10:55,280
но

1908
01:10:55,280 --> 01:10:57,360
мы все еще не знаем '  Я действительно понял,

1909
01:10:57,360 --> 01:10:58,719
как узнать,

1910
01:10:58,719 --> 01:11:00,239
использовать это знание о том, что мозг имеет

1911
01:11:00,239 --> 01:11:01,760
множество обратных связей, а затем

1912
01:11:01,760 --> 01:11:03,040
применить это к

1913
01:11:03,040 --> 01:11:05,840
э-э, как практические практические системы, я

1914
01:11:05,840 --> 01:11:07,280
думаю о моделировании и, может быть

1915
01:11:07,280 --> 01:11:09,040
, это одна из проблем  эм

1916
01:11:09,040 --> 01:11:10,560
эм

1917
01:11:10,560 --> 01:11:12,719
есть вроде да, я думаю, что

1918
01:11:12,719 --> 01:11:15,120
изучение учебной программы может быть одним из них,

1919
01:11:15,120 --> 01:11:16,800
но я думаю, что тот, который, вероятно,

1920
01:11:16,800 --> 01:11:19,440
принесет наибольшую отдачу, на самом деле

1921
01:11:19,440 --> 01:11:22,320
выясняет, как мы можем выйти за рамки текста,

1922
01:11:22,320 --> 01:11:24,560
я думаю, есть так много

1923
01:11:24,560 --> 01:11:26,800
дополнительной информации, которая доступна  что

1924
01:11:26,800 --> 01:11:28,480
мы просто не используем,

1925
01:11:28,480 --> 01:11:30,880
и поэтому я думаю, что большая часть

1926
01:11:30,880 --> 01:11:32,560
прогресса может быть связана с выяснением того

1927
01:11:32,560 --> 01:11:34,320
, что наиболее

1928
01:11:34,320 --> 01:11:36,000
практично для выхода за рамки текста,

1929
01:11:36,000 --> 01:11:36,960


1930
01:11:36,960 --> 01:11:40,760
это то, что я думаю,

1931
01:11:45,120 --> 01:11:48,000
хорошо,

1932
01:11:48,000 --> 01:11:49,760
давай посмотрим

1933
01:11:49,760 --> 01:11:52,239
, какие важные темы

1934
01:11:52,239 --> 01:11:55,840
nlp мы  не охвачены в этом классе,

1935
01:12:00,080 --> 01:12:02,159
я делаю то, что

1936
01:12:02,159 --> 01:12:04,320
вы хорошо знаете, вроде один ответ - это

1937
01:12:04,320 --> 01:12:06,000
множество тем, которые рассматриваются в

1938
01:12:06,000 --> 01:12:07,600
cs224u,

1939
01:12:07,600 --> 01:12:09,679
потому что вы знаете, что мы прилагаем некоторые

1940
01:12:09,679 --> 01:12:11,920
усилия, чтобы они не пересекались, они не совсем

1941
01:12:11,920 --> 01:12:13,679


1942
01:12:13,679 --> 01:12:16,719
правы  так что есть много тем

1943
01:12:16,719 --> 01:12:19,440
в понимании языка, которые

1944
01:12:19,440 --> 01:12:20,400
мы не

1945
01:12:20,400 --> 01:12:23,840
рассмотрели правильно, поэтому, если вы хотите

1946
01:12:23,840 --> 01:12:25,840
сделать

1947
01:12:25,840 --> 01:12:29,679
голосового помощника, такого как alexa siri или

1948
01:12:29,679 --> 01:12:31,679
помощник Google, вам нужно как

1949
01:12:31,679 --> 01:12:35,199
бы иметь возможность взаимодействовать с системным

1950
01:12:35,199 --> 01:12:37,920
API  at может делать такие вещи, как удалять вашу

1951
01:12:37,920 --> 01:12:40,960
почту или покупать вам билеты на концерт, и поэтому

1952
01:12:40,960 --> 01:12:42,320
вам нужно иметь возможность преобразовывать

1953
01:12:42,320 --> 01:12:45,520
язык в явную семантическую форму,

1954
01:12:45,520 --> 01:12:47,600
которая может взаимодействовать с

1955
01:12:47,600 --> 01:12:50,239
системами мира, о которых мы

1956
01:12:50,239 --> 01:12:52,719
вообще не говорили.  много

1957
01:12:52,719 --> 01:12:55,520
вещей для понимания языка, есть также

1958
01:12:55,520 --> 01:12:56,719
много вещей для

1959
01:12:56,719 --> 01:12:58,800
генерации языка,

1960
01:12:58,800 --> 01:13:01,520
так что вы эффективно знаете для

1961
01:13:01,520 --> 01:13:04,400
генерации языка все, что мы сделали,

1962
01:13:04,400 --> 01:13:07,360
это нейронные языковые модели, они великолепны,

1963
01:13:07,360 --> 01:13:09,120
эм, управляйте ими, и они будут генерировать

1964
01:13:09,120 --> 01:13:12,000
язык, и вы знаете, в определенном смысле

1965
01:13:12,000 --> 01:13:14,960
это правда, так как это просто  классная

1966
01:13:14,960 --> 01:13:18,080
генерация, которую вы можете создать

1967
01:13:18,080 --> 01:13:19,120
с помощью

1968
01:13:19,120 --> 01:13:22,320
таких вещей, как gpt two или three,

1969
01:13:22,320 --> 01:13:23,520
но вы знаете,

1970
01:13:23,520 --> 01:13:25,040
где этого не

1971
01:13:25,040 --> 01:13:26,159
хватает

1972
01:13:26,159 --> 01:13:28,239
, это на самом деле только дает вам

1973
01:13:28,239 --> 01:13:31,679
возможность создавать

1974
01:13:31,679 --> 01:13:35,199
беглый текст, в то время как кролики часто

1975
01:13:35,199 --> 01:13:37,440
создают беглый текст,

1976
01:13:37,440 --> 01:13:39,600
который, если вы действительно хотели иметь

1977
01:13:39,600 --> 01:13:42,480
хороший естественный язык  система генерации

1978
01:13:42,480 --> 01:13:45,199
вы также должны иметь более высокий уровень

1979
01:13:45,199 --> 01:13:49,120
планирования того, о чем вы собираетесь

1980
01:13:49,120 --> 01:13:52,400
говорить и как вы собираетесь выразить

1981
01:13:52,400 --> 01:13:56,080
это правильно, чтобы  в большинстве ситуаций на

1982
01:13:56,080 --> 01:13:58,800
естественном языке вы хорошо думаете, я

1983
01:13:58,800 --> 01:14:01,440
хочу объяснить людям

1984
01:14:01,440 --> 01:14:03,360
кое-что о том, почему так важно заниматься

1985
01:14:03,360 --> 01:14:06,320
математикой в колледже, позвольте мне подумать, как

1986
01:14:06,320 --> 01:14:08,880
это организовать, возможно, мне стоит поговорить

1987
01:14:08,880 --> 01:14:10,719
о некоторых различных приложениях, в

1988
01:14:10,719 --> 01:14:13,760
которых математика появляется и как  это

1989
01:14:13,760 --> 01:14:16,000
действительно хорошее основание, вы знаете, что

1990
01:14:16,000 --> 01:14:18,159
бы вы ни планировали, вот как я могу

1991
01:14:18,159 --> 01:14:20,640
правильно представить некоторые идеи и такого рода

1992
01:14:20,640 --> 01:14:23,440
генерацию естественного языка

1993
01:14:23,440 --> 01:14:27,360
ммм мы не делаем

1994
01:14:27,360 --> 01:14:29,440


1995
01:14:29,440 --> 01:14:32,400


1996
01:14:32,400 --> 01:14:34,080
говоря больше понимания,

1997
01:14:34,080 --> 01:14:37,120
большее поколение, которое составляет большую часть nlp, вы

1998
01:14:37,120 --> 01:14:39,760
можете сказать, я имею в виду, очевидно, что есть какие-

1999
01:14:39,760 --> 01:14:42,640
то особые задачи, о которых мы можем

2000
01:14:42,640 --> 01:14:44,880
говорить, которые мы либо решаем,

2001
01:14:44,880 --> 01:14:49,199
либо не решаем явно

2002
01:14:52,960 --> 01:14:55,280
хорошо

2003
01:14:55,280 --> 01:14:57,360
, была ли какая-то работа по

2004
01:14:57,360 --> 01:14:58,960
включению языка  модели в

2005
01:14:58,960 --> 01:15:00,560
среду, в которой они могут

2006
01:15:00,560 --> 01:15:03,120
общаться для достижения задачи,

2007
01:15:03,120 --> 01:15:05,440
и как вы думаете, это поможет с

2008
01:15:05,440 --> 01:15:09,239
обучением без учителя,

2009
01:15:11,920 --> 01:15:13,920
так что снова я предполагаю, что было много

2010
01:15:13,920 --> 01:15:17,520
работы по погружению  общение,

2011
01:15:17,520 --> 01:15:19,280
а также самостоятельная игра,

2012
01:15:19,280 --> 01:15:22,719
где у вас есть разные

2013
01:15:22,719 --> 01:15:23,440


2014
01:15:23,440 --> 01:15:25,760
модели, которые инициализированы как языковые

2015
01:15:25,760 --> 01:15:28,080
модели, которые пытаются общаться

2016
01:15:28,080 --> 01:15:31,040
друг с другом для решения некоторых задач, а затем

2017
01:15:31,040 --> 01:15:33,040
вы знаете, что в конце у вас есть награда

2018
01:15:33,040 --> 01:15:35,040
, смогли ли они  закончить

2019
01:15:35,040 --> 01:15:36,400
задание или нет, а затем, основываясь на этой

2020
01:15:36,400 --> 01:15:37,840
награде, вы пытаетесь изучить как стратегию

2021
01:15:37,840 --> 01:15:39,040
общения

2022
01:15:39,040 --> 01:15:40,239
,

2023
01:15:40,239 --> 01:15:42,800
и это началось как возникающее

2024
01:15:42,800 --> 01:15:44,640
общение и самостоятельная игра, а

2025
01:15:44,640 --> 01:15:46,000
затем была как недавняя работа, я думаю, это

2026
01:15:46,000 --> 01:15:48,000
было похоже на то, что я прошел в прошлом году или в году

2027
01:15:48,000 --> 01:15:49,760
до этого, где они показали, что

2028
01:15:49,760 --> 01:15:52,159
если вы инициализируете эти модели, например,

2029
01:15:52,159 --> 01:15:54,719
с помощью предварительного обучения языковой модели,

2030
01:15:54,719 --> 01:15:56,400
вы в

2031
01:15:56,400 --> 01:15:58,480
основном предотвращаете эту проблему, например,

2032
01:15:58,480 --> 01:15:59,920
языковой дрейф,

2033
01:15:59,920 --> 01:16:00,800


2034
01:16:00,800 --> 01:16:02,640
когда язык, который или протокол связи

2035
01:16:02,640 --> 01:16:04,320
, который в конечном итоге изучают ваши модели

2036
01:16:04,320 --> 01:16:06,239
, не имеет ничего общего с  как

2037
01:16:06,239 --> 01:16:07,600
настоящий язык

2038
01:16:07,600 --> 01:16:08,719
ммм,

2039
01:16:08,719 --> 01:16:10,560
и так что да, я имею в виду, что в этом

2040
01:16:10,560 --> 01:16:13,520
смысле была некоторая работа, но это похоже на очень

2041
01:16:13,520 --> 01:16:14,960
ограниченную, я думаю, есть группы,

2042
01:16:14,960 --> 01:16:16,800
которые пытаются  изучите это, но не более

2043
01:16:16,800 --> 01:16:19,800
того,

2044
01:16:23,199 --> 01:16:26,159
хорошо, я имею в виду, что последние два вопроса

2045
01:16:26,159 --> 01:16:29,280
касаются генов

2046
01:16:31,120 --> 01:16:32,560
, а также один вопрос о том, имеет ли

2047
01:16:32,560 --> 01:16:34,800
ген

2048
01:16:34,800 --> 01:16:36,560
некоторые корреляции с социальными сигналами системой,

2049
01:16:36,560 --> 01:16:38,400
основанной на вознаграждении, я не знаю, есть

2050
01:16:38,400 --> 01:16:40,480
ли у кого-то из вас мнение об этом,

2051
01:16:40,480 --> 01:16:41,360


2052
01:16:41,360 --> 01:16:44,000
но если

2053
01:16:45,600 --> 01:16:48,239
да, я имею в виду, что мне нечего сказать особо

2054
01:16:48,239 --> 01:16:50,400
по этому вопросу, так что речь идет

2055
01:16:50,400 --> 01:16:53,280
о важности социальных сигналов в

2056
01:16:53,280 --> 01:16:55,520
отличие от систем, основанных исключительно на вознаграждении,

2057
01:16:55,520 --> 01:16:59,360
ну, я имею в виду в некотором смысле

2058
01:16:59,360 --> 01:17:02,239
социальный сигнал, который вы также можете рассматривать как

2059
01:17:02,239 --> 01:17:06,320
награду,  люди, которых вы знаете, любят,

2060
01:17:06,320 --> 01:17:08,239
когда другие люди улыбаются,

2061
01:17:08,239 --> 01:17:11,199
когда вы что-то говорите, но вы

2062
01:17:11,199 --> 01:17:14,960
знаете, я действительно думаю, в целом, вы не знаете,

2063
01:17:14,960 --> 01:17:16,960
когда люди говорят, что мы

2064
01:17:16,960 --> 01:17:18,239
не освещали,

2065
01:17:18,239 --> 01:17:20,640
еще одна вещь, которую мы едва охватили,

2066
01:17:20,640 --> 01:17:23,360
- это  социальная сторона языка, поэтому вы

2067
01:17:23,360 --> 01:17:26,480
знаете очень много

2068
01:17:26,480 --> 01:17:29,120
интересного о

2069
01:17:29,120 --> 01:17:31,920
языке, так это то, что он имеет очень динамичный большой

2070
01:17:31,920 --> 01:17:34,640
динамический диапазон, поэтому, с одной стороны, вы можете

2071
01:17:34,640 --> 01:17:37,280
говорить об очень точных вещах в

2072
01:17:37,280 --> 01:17:39,360
языке, чтобы вы могли как бы говорить о

2073
01:17:39,360 --> 01:17:42,000
математике для  мулы и шаги в доказательстве и

2074
01:17:42,000 --> 01:17:44,159
тому подобное, чтобы было

2075
01:17:44,159 --> 01:17:46,560
много точности и языка, но вы знаете,

2076
01:17:46,560 --> 01:17:49,520
с другой стороны, вы можете просто как бы

2077
01:17:49,520 --> 01:17:51,760
решительно бормотать какие-то

2078
01:17:51,760 --> 01:17:54,080
слова вообще, и вы на самом деле ничего не говорите

2079
01:17:54,080 --> 01:17:56,480
в

2080
01:17:56,480 --> 01:17:59,280
способ пропозиционального содержания эм то, что вы на

2081
01:17:59,280 --> 01:18:02,159
самом деле пытаетесь передать, это вы знаете,

2082
01:18:02,159 --> 01:18:04,560
я о, я думаю о вас прямо сейчас,

2083
01:18:04,560 --> 01:18:07,679
и о, меня беспокоит, как вы

2084
01:18:07,679 --> 01:18:09,080
себя чувствуете или что бы там ни было в

2085
01:18:09,080 --> 01:18:12,080
сложившихся обстоятельствах  так что огромная

2086
01:18:12,080 --> 01:18:14,080
часть использования языка

2087
01:18:14,080 --> 01:18:14,880
находится

2088
01:18:14,880 --> 01:18:15,920
в

2089
01:18:15,920 --> 01:18:18,719
формах своего рода социального общения

2090
01:18:18,719 --> 01:18:22,159
между людьми, и вы знаете, что это

2091
01:18:22,159 --> 01:18:23,199
еще одна

2092
01:18:23,199 --> 01:18:26,800
большая часть фактического построения успешных

2093
01:18:26,800 --> 01:18:28,400


2094
01:18:28,400 --> 01:18:31,199
систем естественного языка, так что,

2095
01:18:31,199 --> 01:18:33,679
если вы знаете, думаете ли вы о

2096
01:18:33,679 --> 01:18:35,280
чем-то негативном, например, о виртуальных помощниках

2097
01:18:35,280 --> 01:18:37,840
я много чего возвращаюсь назад, вы

2098
01:18:37,840 --> 01:18:40,800
знаете, что у них практически нет

2099
01:18:40,800 --> 01:18:42,800
способностей как

2100
01:18:42,800 --> 01:18:45,840
пользователей социальных языков, так что сейчас мы

2101
01:18:45,840 --> 01:18:48,880
обучаем поколение маленьких детей

2102
01:18:48,880 --> 01:18:52,080
тому, что вы должны делать, это вроде как лаять

2103
01:18:52,080 --> 01:18:53,760
о  ut командует,

2104
01:18:53,760 --> 01:18:57,040
как будто вы знаете, что служите в

2105
01:18:57,040 --> 01:18:59,679
немецкой армии во время Второй мировой войны или что-то в этом роде,

2106
01:18:59,679 --> 01:19:01,199
и гм,

2107
01:19:01,199 --> 01:19:04,320
что нет никакой социальной

2108
01:19:04,320 --> 01:19:08,640
части того, как вы знаете, как использовать язык,

2109
01:19:08,640 --> 01:19:11,080
чтобы

2110
01:19:11,080 --> 01:19:13,440
удовлетворительно общаться с людьми и

2111
01:19:13,440 --> 01:19:15,920
поддерживать социальную  система и что вы

2112
01:19:15,920 --> 01:19:18,159
знаете, что это огромная часть использования человеческого

2113
01:19:18,159 --> 01:19:21,520
языка, которую дети должны выучить и

2114
01:19:21,520 --> 01:19:23,760
научиться успешно использовать.

2115
01:19:23,760 --> 01:19:26,159


2116
01:19:26,159 --> 01:19:26,960


2117
01:19:26,960 --> 01:19:28,000


2118
01:19:28,000 --> 01:19:29,760


2119
01:19:29,760 --> 01:19:32,320
что

2120
01:19:32,320 --> 01:19:34,000
есть хорошие способы

2121
01:19:34,000 --> 01:19:36,880
спросить их об этом, вы знаете, что некоторые из них

2122
01:19:36,880 --> 01:19:39,360
выбирают, как представлять аргументы,

2123
01:19:39,360 --> 01:19:42,560
но вы знаете, что некоторые из них заключаются в установлении

2124
01:19:42,560 --> 01:19:45,440
социального взаимопонимания, вежливых и

2125
01:19:45,440 --> 01:19:48,239
разумных просьбах и создании видимости, что

2126
01:19:48,239 --> 01:19:50,320
вы милый человек, который  другие люди

2127
01:19:50,320 --> 01:19:52,560
должны что-то делать, и вы знаете, что

2128
01:19:52,560 --> 01:19:54,800
люди очень хороши в этом,

2129
01:19:54,800 --> 01:19:57,280
и это действительно важный

2130
01:19:57,280 --> 01:19:58,880
навык для умения хорошо ориентироваться в

2131
01:19:58,880 --> 01:20:02,199
мире.

2132
01:20:04,800 --> 01:20:06,880


