
using LinearAlgebra
using Distributions
using LightGraphs
using JuMP
using GLPK
using Ipopt

# import IterTools: subsets
function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_ma
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end
Base.argmax(f::Function, xs) = findmax(f, xs)[2]

Base.Dict{Symbol,V}(a::NamedTuple) where V =
          Dict{Symbol,V}(n => v for (n, v) in zip(keys(a), values(a)))

Base.convert(::Type{Dict{Symbol,V}}, a::NamedTuple) where V =
          Dict{Symbol,V}(a)

Base.isequal(a::Dict{Symbol,<:Any}, nt::NamedTuple) =
          length(a) == length(nt) &&
          all(a[n] == v for (n, v) in zip(keys(nt), values(nt)))

struct Variable
    name::Symbol
    m::Int # number of possible values
end

const Assignment = Dict{Symbol,Int}
const FactorTable = Dict{Assignment,Float64}

struct Factor
    vars::Vector{Variable}
    table::FactorTable
end

variablenames(Ï•::Factor) = [var.name for var in Ï•.vars]
select(a::Assignment, varnames::Vector{Symbol}) =
                Assignment(n => a[n] for n in varnames)

function assignments(vars::AbstractVector{Variable})
    names = [var.name for var in vars]
    return vec([Assignment(n => v for (n, v) in zip(names, values))
                for values in product((1:v.m for v in vars)...)])
end

function normalize!(Ï•::Factor)
    z = sum(p for (a, p) in Ï•.table)
    for (a, p) in Ï•.table
        Ï•.table[a] = p / z
    end
    return Ï•
end

struct BayesianNetwork
    vars::Vector{Variable}
    factors::Vector{Factor}
    graph::SimpleDiGraph{Int64}
end

B = Variable(:b, 2); S = Variable(:s, 2)
E = Variable(:e, 2)
D = Variable(:d, 2); C = Variable(:c, 2)
vars = [B, S, E, D, C]
factors = [
    Factor([B], FactorTable((b = 1,) => 0.99, (b = 2,) => 0.01)),
    Factor([S], FactorTable((s = 1,) => 0.98, (s = 2,) => 0.02)),
    Factor([E,B,S], FactorTable(
        (e = 1, b = 1, s = 1) => 0.90, (e = 1, b = 1, s = 2) => 0.04,
        (e = 1, b = 2, s = 1) => 0.05, (e = 1, b = 2, s = 2) => 0.01,
        (e = 2, b = 1, s = 1) => 0.10, (e = 2, b = 1, s = 2) => 0.96,
        (e = 2, b = 2, s = 1) => 0.95, (e = 2, b = 2, s = 2) => 0.99)),
    Factor([D, E], FactorTable(
        (d = 1, e = 1) => 0.96, (d = 1, e = 2) => 0.03,
        (d = 2, e = 1) => 0.04, (d = 2, e = 2) => 0.97)),
    Factor([C, E], FactorTable(
        (c = 1, e = 1) => 0.98, (c = 1, e = 2) => 0.01,
        (c = 2, e = 1) => 0.02, (c = 2, e = 2) => 0.99))
]
graph = SimpleDiGraph(5)
add_edge!(graph, 1, 3); add_edge!(graph, 2, 3)
add_edge!(graph, 3, 4); add_edge!(graph, 3, 5)
bn = BayesianNetwork(vars, factors, graph)

function probability(bn::BayesianNetwork, assignment)
    subassignment(Ï•) = select(assignment, variablenames(Ï•))
    probability(Ï•) = get(Ï•.table, subassignment(Ï•), 0.0)
    return prod(probability(Ï•) for Ï• in bn.factors)
end

function Base.:*(Ï•::Factor, Ïˆ::Factor)
    Ï•names = variablenames(Ï•)
    Ïˆnames = variablenames(Ïˆ)
    Ïˆonly = setdiff(Ïˆ.vars, Ï•.vars)
    table = FactorTable()
    for (Ï•a, Ï•p) in Ï•.table
        for a in assignments(Ïˆonly)
            a = merge(Ï•a, a)
            Ïˆa = select(a, Ïˆnames)
            table[a] = Ï•p * get(Ïˆ.table, Ïˆa, 0.0)
        end
    end
    vars = vcat(Ï•.vars, Ïˆonly)
    return Factor(vars, table)
end

function marginalize(Ï•::Factor, name)
    table = FactorTable()
    for (a, p) in Ï•.table
        aâ€² = delete!(copy(a), name)
        table[aâ€²] = get(table, aâ€², 0.0) + p
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end

in_scope(name, Ï•) = any(name == v.name for v in Ï•.vars)

function condition(Ï•::Factor, name, value)
    if !in_scope(name, Ï•)
        return Ï•
    end
    table = FactorTable()
    for (a, p) in Ï•.table
        if a[name] == value
            table[delete!(copy(a), name)] = p
        end
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end
function condition(Ï•::Factor, evidence)
    for (name, value) in pairs(evidence)
        Ï• = condition(Ï•, name, value)
    end
    return Ï•
end

struct ExactInference end
function infer(M::ExactInference, bn, query, evidence)
    Ï• = prod(bn.factors)
    Ï• = condition(Ï•, evidence)
    for name in setdiff(variablenames(Ï•), query)
    Ï• = marginalize(Ï•, name)
        end
    return normalize!(Ï•)
end

struct VariableElimination
    ordering # array of variable indices
end
function infer(M::VariableElimination, bn, query, evidence)
    Î¦ = [condition(Ï•, evidence) for Ï• in bn.factors]
    for i in M.ordering
        name = bn.vars[i].name
        if name âˆ‰ query
            inds = findall(Ï• -> in_scope(name, Ï•), Î¦)
            if !isempty(inds)
                Ï• = prod(Î¦[inds])
                deleteat!(Î¦, inds)
                Ï• = marginalize(Ï•, name)
                push!(Î¦, Ï•)
            end
        end
    end
    return normalize!(prod(Î¦))
end

function Base.rand(Ï•::Factor)
    tot, p, w = 0.0, rand(), sum(values(Ï•.table))
    for (a, v) in Ï•.table
        tot += v / w
        if tot >= p
            return a
        end
    end
    return Assignment()
end

function Base.rand(bn::BayesianNetwork)
    a = Assignment()
    for i in topological_sort(bn.graph)
        name, Ï• = bn.vars[i].name, bn.factors[i]
        a[name] = rand(condition(Ï•, a))[name]
    end
    return a
end

struct DirectSampling
    m # number of samples
end
function infer(M::DirectSampling, bn, query, evidence)
    table = FactorTable()
    for i in 1:(M.m)
        a = rand(bn)
        if all(a[k] == v for (k, v) in pairs(evidence))
            b = select(a, query)
            table[b] = get(table, b, 0) + 1
        end
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

struct LikelihoodWeightedSampling
    m # number of samples
end
function infer(M::LikelihoodWeightedSampling, bn, query, evidence)
    table = FactorTable()
    ordering = topological_sort(bn.graph)
    for i in 1:(M.m)
        a, w = Assignment(), 1.0
        for j in ordering
            name, Ï• = bn.vars[j].name, bn.factors[j]
            if haskey(evidence, name)
                a[name] = evidence[name]
                w *= Ï•.table[select(a, variablenames(Ï•))]
            else
                a[name] = rand(condition(Ï•, a))[name]
            end
        end
        b = select(a, query)
        table[b] = get(table, b, 0) + w
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function blanket(bn, a, i)
    name = bn.vars[i].name
    val = a[name]
    a = delete!(copy(a), name)
    Î¦ = filter(Ï• -> in_scope(name, Ï•), bn.factors)
    Ï• = prod(condition(Ï•, a) for Ï• in Î¦)
    return normalize!(Ï•)
end

function update_gibbs_sample!(a, bn, evidence, ordering)
    for i in ordering
        name = bn.vars[i].name
        if !haskey(evidence, name)
            b = blanket(bn, a, i)
            a[name] = rand(b)[name]
    end
end
end

function gibbs_sample!(a, bn, evidence, ordering, m)
    for j in 1:m
        update_gibbs_sample!(a, bn, evidence, ordering)
end
end
struct GibbsSampling
    m_samples # number of samples to use
    m_burnin # number of samples to discard during burn-in
    m_skip # number of samples to skip for thinning
    ordering # array of variable indices
end

function infer(M::GibbsSampling, bn, query, evidence)
    table = FactorTable()
    a = merge(rand(bn), evidence)
    gibbs_sample!(a, bn, evidence, M.ordering, M.m_burnin)
    for i in 1:(M.m_samples)
        gibbs_sample!(a, bn, evidence, M.ordering, M.m_skip)
        b = select(a, query)
        table[b] = get(table, b, 0) + 1
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function infer(D::MvNormal, query, evidencevars, evidence)
    Î¼, Î£ = D.Î¼, D.Î£.mat
    b, Î¼a, Î¼b = evidence, Î¼[query], Î¼[evidencevars]
    A = Î£[query,query]
    B = Î£[evidencevars,evidencevars]
    C = Î£[query,evidencevars]
    Î¼ = Î¼[query] + C * (B \ (b - Î¼b))
    Î£ = A - C * (B \ C')
    return MvNormal(Î¼, Î£)
end

function sub2ind(siz, x)
    k = vcat(1, cumprod(siz[1:end - 1]))
    return dot(k, x .- 1) + 1
end
function statistics(vars, G, D::Matrix{Int})
    n = size(D, 1)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    M = [zeros(q[i], r[i]) for i in 1:n]
    for o in eachcol(D)
        for i in 1:n
            k = o[i]
            parents = inneighbors(G, i)
            j = 1
            if !isempty(parents)
                j = sub2ind(r[parents], o[parents])
            end
            M[i][j,k] += 1.0
        end
    end
    return M
end
    
# G = SimpleDiGraph(3)
# add_edge!(G, 1, 2)
# add_edge!(G, 3, 2)
# vars = [Variable(:A,2), Variable(:B,2), Variable(:C,2)]
# D = [1 2 2 1; 1 2 2 1; 2 2 2 2]
# M = statistics(vars, G, D)

# Î¸ = [mapslices(x->normalize(x,1), Mi, dims=2) for Mi in M]

function prior(vars, G)
    n = length(vars)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    return [ones(q[i], r[i]) for i in 1:n]
end

gaussian_kernel(b) = x -> pdf(Normal(0, b), x)

function kernel_density_estimate(Ï•, O)
    return x -> sum([Ï•(x - o) for o in O]) / length(O)
end

function bayesian_score_component(M, Î±)
    p = sum(loggamma.(Î± + M))
    p -= sum(loggamma.(Î±))
    p += sum(loggamma.(sum(Î±, dims=2)))
    p -= sum(loggamma.(sum(Î±, dims=2) + sum(M, dims=2)))
    return p
end
function bayesian_score(vars, G, D)
    n = length(vars)
    M = statistics(vars, G, D)
    Î± = prior(vars, G)
    return sum(bayesian_score_component(M[i], Î±[i]) for i in 1:n)
end

struct K2Search
    ordering::Vector{Int} # variable ordering
end
function fit(method::K2Search, vars, D)
    G = SimpleDiGraph(length(vars))
    for (k, i) in enumerate(method.ordering[2:end])
        y = bayesian_score(vars, G, D)
        while true
            y_best, j_best = -Inf, 0
            for j in method.ordering[1:k]
                if !has_edge(G, j, i)
                add_edge!(G, j, i)
                yâ€² = bayesian_score(vars, G, D)
                    if yâ€² > y_best
                        y_best, j_best = yâ€², j
                    end
                rem_edge!(G, j, i)
                end
            end
            if y_best > y
                y = y_best
                add_edge!(G, j_best, i)
            else
                break
            end
        end
    end
    return G
end

struct LocalDirectedGraphSearch
    G # initial graph
    k_max # number of iterations
end
function rand_graph_neighbor(G)
    n = nv(G)
    i = rand(1:n)
    j = mod1(i + rand(2:n) - 1, n)
    Gâ€² = copy(G)
    has_edge(G, i, j) ? rem_edge!(Gâ€², i, j) : add_edge!(Gâ€², i, j)
    return Gâ€²
end
function fit(method::LocalDirectedGraphSearch, vars, D)
    G = method.G
    y = bayesian_score(vars, G, D)
    for k in 1:method.k_max
        Gâ€² = rand_graph_neighbor(G)
        yâ€² = is_cyclic(Gâ€²) ? -Inf : bayesian_score(vars, Gâ€², D)
        if yâ€² > y
            y, G = yâ€², Gâ€²
        end
    end
    return G
end

### HERE
function are_markov_equivalent(G, H)
    if nv(G) != nv(H) || ne(G) != ne(H) ||
        !all(has_edge(H, e) ||
        has_edge(H, reverse(e))
            for e in edges(G))
            return false
    end
    for c in 1:nv(G)
        parents = inneighbors(G, c)
        for (a, b) in subsets(parents, 2)
            if !has_edge(G, a, b) && !has_edge(G, b, a) &&
                !(has_edge(H, a, c) && has_edge(H, b, c))
                    return false
            end
        end
    end
    return true
end

struct SimpleProblem
    bn::BayesianNetwork
    chance_vars::Vector{Variable}
    decision_vars::Vector{Variable}
    utility_vars::Vector{Variable}
    utilities::Dict{Symbol,Vector{Float64}}
end
function solve(ğ’«::SimpleProblem, evidence, M)
    query = [var.name for var in ğ’«.utility_vars]
    U(a) = sum(ğ’«.utilities[uname][a[uname]] for uname in query)
    best = (a = nothing, u = -Inf)
    for assignment in assignments(ğ’«.decision_vars)
        evidence = merge(evidence, assignment)
        Ï• = infer(M, ğ’«.bn, query, evidence)
        u = sum(p * U(a) for (a, p) in Ï•.table)
        if u > best.u
            best = (a = assignment, u = u)
        end
    end
    return best
end

function value_of_information(ğ’«, query, evidence, M)
    Ï• = infer(M, ğ’«.bn, query, evidence)
    voi = -solve(ğ’«, evidence, M).u
    query_vars = filter(v -> v.name âˆˆ query, ğ’«.chance_vars)
    for oâ€² in assignments(query_vars)
        ooâ€² = merge(evidence, oâ€²)
        p = Ï•.table[oâ€²]
        voi += p * solve(ğ’«, ooâ€², M).u
    end
    return voi
end

struct MDP
    Î³  # discount factor
    ğ’®  # state space
    ğ’œ # action space
    T  # transition function
    R  # reward function
    TR # sample transition and reward
end

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U(sâ€²) for sâ€² in ğ’®)
end
function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U[i] for (i, sâ€²) in enumerate(ğ’®))
end

function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end

function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³ * Tâ€²) \ Râ€²
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end
function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a -> lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a = a, u = u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a

struct PolicyIteration
    Ï€ # initial policy
    k_max # maximum number of iterations
end
function solve(M::PolicyIteration, ğ’«::MDP)
    Ï€, ğ’® = M.Ï€, ğ’«.ğ’®
    for k = 1:M.k_max
        U = policy_evaluation(ğ’«, Ï€)
        Ï€â€² = ValueFunctionPolicy(ğ’«, U)
        if all(Ï€(s) == Ï€â€²(s) for s in ğ’®)
            break
        end
        Ï€ = Ï€â€²
    end
    return Ï€
end

function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end

struct ValueIteration
    k_max # maximum number of iterations
    end
function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end

struct GaussSeidelValueIteration
    k_max # maximum number of iterations
end
function solve(M::GaussSeidelValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’®]
    for k = 1:M.k_max
        for (s, i) in enumerate(ğ’«.ğ’®)
            U[i] = backup(ğ’«, U, s)
        end
    end
    return ValueFunctionPolicy(ğ’«, U)
end

struct LinearProgramFormulation end
function tensorform(ğ’«::MDP)
    ğ’®, ğ’œ, R, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s,a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    return ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end
solve(ğ’«::MDP) = solve(LinearProgramFormulation(), ğ’«)
function solve(M::LinearProgramFormulation, ğ’«::MDP)
    ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ğ’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s=ğ’®,a=ğ’œ], U[s] â‰¥ R[s,a] + ğ’«.Î³*T[s,a,:]â‹…U)
    optimize!(model)
    return ValueFunctionPolicy(ğ’«, value.(U))
end

struct LinearQuadraticProblem
    Ts # transition matrix with respect to state
    Ta # transition matrix with respect to action
    Rs # reward matrix with respect to state (negative semidefinite)
    Ra # reward matrix with respect to action (negative definite)
    h_max # horizon
end
function solve(ğ’«::LinearQuadraticProblem)
    Ts, Ta, Rs, Ra, h_max = ğ’«.Ts, ğ’«.Ta, ğ’«.Rs, ğ’«.Ra, ğ’«.h_max
    V = zeros(size(Rs))
    Ï€s = Any[s -> zeros(size(Ta, 2))]
    for h in 2:h_max
        V = Ts'*(V - V*Ta*((Ta'*V*Ta + Ra) \ Ta'*V))*Ts + Rs
        L = -(Ta'*V*Ta + Ra) \ Ta' * V * Ts
        push!(Ï€s, s -> L*s)
    end
    return Ï€s
end

struct ApproximateValueIteration
    UÎ¸    # initial parameterized value function that supports fit!
    S     # set of discrete states for performing backups
    k_max # maximum number of iterations
end
function solve(M::ApproximateValueIteration, ğ’«::MDP)
    UÎ¸, S, k_max = M.UÎ¸, M.S, M.k_max
    for k in 1:k_max
        U = [backup(ğ’«, UÎ¸, s) for s in S]
        fit!(UÎ¸, S, U)
    end
    return ValueFunctionPolicy(ğ’«, UÎ¸)
end

mutable struct NearestNeighborValueFunction
    k # number of neighbors
    d # distance function d(s, sâ€²)
    S # set of discrete states
    Î¸ # vector of values at states in S
end

function (UÎ¸::NearestNeighborValueFunction)(s)
    dists = [UÎ¸.d(s,sâ€²) for sâ€² in UÎ¸.S]
    ind = sortperm(dists)[1:UÎ¸.k]
    return mean(UÎ¸.Î¸[i] for i in ind)
end
function fit!(UÎ¸::NearestNeighborValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct LocallyWeightedValueFunction
    k # kernel function k(s, sâ€²)
    S # set of discrete states
    Î¸ # vector of values at states in S
end

function (UÎ¸::LocallyWeightedValueFunction)(s)
    w = normalize([UÎ¸.k(s,sâ€²) for sâ€² in UÎ¸.S], 1)
    return UÎ¸.Î¸ â‹… w
end
function fit!(UÎ¸::LocallyWeightedValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct MultilinearValueFunction
    o # position of lower-left corner
    Î´ # vector of widths
    Î¸ # vector of values at states in S
end

function (UÎ¸::MultilinearValueFunction)(s)
    o, Î´, Î¸ = UÎ¸.o, UÎ¸.Î´, UÎ¸.Î¸
    Î” = (s - o)./Î´
    # Multidimensional index of lower-left cell
    i = min.(floor.(Int, Î”) .+ 1, size(Î¸) .- 1)
    vertex_index = similar(i)
    d = length(s)
    u = 0.0
    for vertex in 0:2^d-1
        weight = 1.0
        for j in 1:d
            # Check whether jth bit is set
            if vertex & (1 << (j-1)) > 0
                vertex_index[j] = i[j] + 1
                weight *= Î”[j] - i[j] + 1
            else
                vertex_index[j] = i[j]
                weight *= i[j] - Î”[j]
            end
        end
        u += Î¸[vertex_index...]*weight
    end
    return u
end
function fit!(UÎ¸::MultilinearValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct SimplexValueFunction
    o # position of lower-left corner
    Î´ # vector of widths
    Î¸ # vector of values at states in S
end

function (UÎ¸::SimplexValueFunction)(s)
    Î” = (s - UÎ¸.o)./UÎ¸.Î´
    # Multidimensional index of upper-right cell
    i = min.(floor.(Int, Î”) .+ 1, size(UÎ¸.Î¸) .- 1) .+ 1
    u = 0.0
    sâ€² = (s - (UÎ¸.o + UÎ¸.Î´.*(i.-2))) ./ UÎ¸.Î´
    p = sortperm(sâ€²) # increasing order
    w_tot = 0.0
    for j in p
        w = sâ€²[j] - w_tot
        u += w*UÎ¸.Î¸[i...]
        i[j] -= 1
        w_tot += w
    end
    u += (1 - w_tot)*UÎ¸.Î¸[i...]
    return u
end

function fit!(UÎ¸::SimplexValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct LinearRegressionValueFunction
    Î² # basis vector function
    Î¸ # vector of parameters
end
function (UÎ¸::LinearRegressionValueFunction)(s)
    return UÎ¸.Î²(s) â‹… UÎ¸.Î¸
end
function fit!(UÎ¸::LinearRegressionValueFunction, S, U)
    X = hcat([UÎ¸.Î²(s) for s in S]...)'
    UÎ¸.Î¸ = pinv(X)*U
    return UÎ¸
end


struct RolloutLookahead
    ğ’« # problem
    Ï€ # rollout policy
    d # depth
end
randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)
function rollout(ğ’«, s, Ï€, d)
    ret = 0.0
    for t in 1:d
        a = Ï€(s)
        s, r = randstep(ğ’«, s, a)
        ret += ğ’«.Î³^(t-1) * r
    end
    return ret
 end
function (Ï€::RolloutLookahead)(s)
    U(s) = rollout(Ï€.ğ’«, s, Ï€.Ï€, Ï€.d)
    return greedy(Ï€.ğ’«, U, s).a
end

struct ForwardSearch
    ğ’« # problem
    d # depth
    U # value function at depth d
end
function forward_search(ğ’«, s, d, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end
    best = (a=nothing, u=-Inf)
    Uâ€²(s) = forward_search(ğ’«, s, d-1, U).u
    for a in ğ’«.ğ’œ
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::ForwardSearch)(s) = forward_search(Ï€.ğ’«, s, Ï€.d, Ï€.U).a

struct BranchAndBound
    ğ’«  # problem
    d   # depth
    Ulo # lower bound on value function at depth d
    Qhi # upper bound on action value function
end
function branch_and_bound(ğ’«, s, d, Ulo, Qhi)
    if d â‰¤ 0
        return (a=nothing, u=Ulo(s))
    end
    Uâ€²(s) = branch_and_bound(ğ’«, s, d-1, Ulo, Qhi).u
    best = (a=nothing, u=-Inf)
    for a in sort(ğ’«.ğ’œ, by=a->Qhi(s,a), rev=true)
        if Qhi(s, a) < best.u
            return best # safe to prune
        end
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::BranchAndBound)(s) = branch_and_bound(Ï€.ğ’«, s, Ï€.d, Ï€.Ulo, Ï€.Qhi).a

struct SparseSampling
    ğ’« # problem
    d # depth
    m # number of samples
    U # value function at depth d
end
function sparse_sampling(ğ’«, s, d, m, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end
    best = (a=nothing, u=-Inf)
    for a in ğ’«.ğ’œ
        u = 0.0
        for i in 1:m
            sâ€², r = randstep(ğ’«, s, a)
            aâ€², uâ€² = sparse_sampling(ğ’«, sâ€², d-1, m, U)
            u += (r + ğ’«.Î³*uâ€²) / m
        end
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::SparseSampling)(s) = sparse_sampling(Ï€.ğ’«, s, Ï€.d, Ï€.m, Ï€.U).a

struct MonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    U # value function estimate
end
function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a->Ï€.Q[(s,a)], Ï€.ğ’«.ğ’œ)
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s,a)
    q = r + Î³*simulate!(Ï€, sâ€², d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)
function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end


struct HeuristicSearch
    ğ’«   # problem
    Uhi # upper bound on value function
    d   # depth
    m   # number of simulations
end
function simulate!(Ï€::HeuristicSearch, U, s)
    ğ’« = Ï€.ğ’«
    for d in 1:Ï€.d
        a, u = greedy(ğ’«, U, s)
        U[s] = u
        s = rand(ğ’«.T(s, a))
    end
end
function (Ï€::HeuristicSearch)(s)
    U = [Ï€.Uhi(s) for s in Ï€.ğ’«.ğ’®]
    for i in 1:m
        simulate!(Ï€, U, s)
    end
    return greedy(Ï€.ğ’«, U, s).a
end

struct LabeledHeuristicSearch
    ğ’«   # problem
    Uhi # upper bound on value function
    d   # depth
    Î´   # gap threshold
end
function (Ï€::LabeledHeuristicSearch)(s)
    U, solved = [Ï€.Uhi(s) for s in ğ’«.ğ’®], Set()
    while s âˆ‰ solved
        simulate!(Ï€, U, solved, s)
    end
    return greedy(Ï€.ğ’«, U, s).a
end

function simulate!(Ï€::LabeledHeuristicSearch, U, solved, s)
    visited = []
    for d in 1:Ï€.d
    if s âˆˆ solved
        break
    end
    push!(visited, s)
    a, u = greedy(Ï€.ğ’«, U, s)
    U[s] = u
    s = rand(Ï€.ğ’«.T(s, a))
    end
    while !isempty(visited)
        if label!(Ï€, U, solved, pop!(visited))
            break
        end
    end
end


function expand(Ï€::LabeledHeuristicSearch, U, solved, s)
    ğ’«, Î´ = Ï€.ğ’«, Ï€.Î´
    ğ’®, ğ’œ, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T
    found, toexpand, envelope = false, Set(s), []
    while !isempty(toexpand)
        s = pop!(toexpand)
        push!(envelope, s)
        a, u = greedy(ğ’«, U, s)
        if abs(U[s] - u) > Î´
        found = true
        else
            for sâ€² in ğ’®
                if T(s,a,sâ€²) > 0 && sâ€² âˆ‰ (solved âˆª envelope)
                    push!(toexpand, sâ€²)
                end
            end
        end
    end
    return (found, envelope)
end

function label!(Ï€::LabeledHeuristicSearch, U, solved, s)
    if s âˆˆ solved
        return false
    end
    found, envelope = expand(Ï€, U, solved, s)
    if found
        for s âˆˆ reverse(envelope)
            U[s] = greedy(Ï€.ğ’«, U, s).u
        end
    else
        union!(solved, envelope)
    end
    return found
end

#=
model = Model(Ipopt.Optimizer)
d = 10
current_state = zeros(4)
goal = [10,10,0,0]
obstacle = [3,4]
@variables model begin
s[1:4, 1:d]
-1 â‰¤ a[1:2,1:d] â‰¤ 1
end
# velocity update
@constraint(model, [i=2:d,j=1:2], s[2+j,i] == s[2+j,i-1] + a[j,i-1])
# position update
@constraint(model, [i=2:d,j=1:2], s[j,i] == s[j,i-1] + s[2+j,i-1])
# initial condition
@constraint(model, s[:,1] .== current_state)
# obstacle
@constraint(model, [i=1:d], sum((s[1:2,i] - obstacle).^2) â‰¥ 4)
@objective(model, Min, 100*sum((s[:,d] - goal).^2) + sum(a.^2))
optimize!(model)
action = value.(a[:,1])
# EXIT: Optimal Solution Found.
# 2-element Vector{Float64}:
#  0.8304708931331188
#  0.32812069979351177
=#

struct MonteCarloPolicyEvaluation
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
end

function (U::MonteCarloPolicyEvaluation)(Ï€)
    R(Ï€) = rollout(U.ğ’«, rand(U.b), Ï€, U.d)
    return mean(R(Ï€) for i = 1:U.m)
end

(U::MonteCarloPolicyEvaluation)(Ï€, Î¸) = U(s->Ï€(Î¸, s))


struct HookeJeevesPolicySearch
    Î¸ # initial parameterization
    Î± # step size
    c # step size reduction factor
    Ïµ # termination step size
end
function optimize(M::HookeJeevesPolicySearch, Ï€, U)
    Î¸, Î¸â€², Î±, c, Ïµ = copy(M.Î¸), similar(M.Î¸), M.Î±, M.c, M.Ïµ
    u, n = U(Ï€, Î¸), length(Î¸)
    while Î± > Ïµ
        copyto!(Î¸â€², Î¸)
        best = (i=0, sgn=0, u=u)
        for i in 1:n
            for sgn in (-1,1)
                Î¸â€²[i] = Î¸[i] + sgn*Î±
                uâ€² = U(Ï€, Î¸â€²)
                if uâ€² > best.u
                    best = (i=i, sgn=sgn, u=uâ€²)
                end
            end
            Î¸â€²[i] = Î¸[i]
        end
        if best.i != 0
            Î¸[best.i] += best.sgn*Î±
            u = best.u
        else
            Î± *= c
        end
    end
    return Î¸
end

function Ï€(Î¸, s)
    return rand(Normal(Î¸[1]*s, abs(Î¸[2]) + 0.00001))
end

#=
ğ’« = #nothing
b, d, n_rollouts = Normal(0.3,0.1), 10, 3
U = MonteCarloPolicyEvaluation(ğ’«, b, d, n_rollouts)
Î¸, Î±, c, Ïµ = [0.0,1.0], 0.75, 0.75, 0.01
M = HookeJeevesPolicySearch(Î¸, Î±, c, Ïµ)
Î¸ = optimize(M, Ï€, U)
=#

struct GeneticPolicySearch
    Î¸s      # initial population
    Ïƒ       # initial standard devidation
    m_elite # number of elite samples
    k_max   # number of iterations
end
function optimize(M::GeneticPolicySearch, Ï€, U)
    Î¸s, Ïƒ = M.Î¸s, M.Ïƒ
    n, m = length(first(Î¸s)), length(Î¸s)
    for k in 1:M.k_max
        us = [U(Ï€, Î¸) for Î¸ in Î¸s]
        sp = sortperm(us, rev=true)
        Î¸_best = Î¸s[sp[1]]
        rand_elite() = Î¸s[sp[rand(1:M.m_elite)]]
        Î¸s = [rand_elite() + Ïƒ.*randn(n) for i in 1:(m-1)]
        push!(Î¸s, Î¸_best)
    end
    return last(Î¸s)
end


struct CrossEntropyPolicySearch
    p # initial distribution
    m # number of samples
    m_elite # number of elite samples
    k_max   # number of iterations
end

function optimize_dist(M::CrossEntropyPolicySearch, Ï€, U)
    p, m, m_elite, k_max = M.p, M.m, M.m_elite, M.k_max
    for k in 1:k_max
        Î¸s = rand(p, m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        Î¸_elite = Î¸s[:,sortperm(us)[(m-m_elite+1):m]]
        p = Distributions.fit(typeof(p), Î¸_elite)
    end
    return p
end

function optimize(M, Ï€, U)
    return Distributions.mode(optimize_dist(M, Ï€, U))
end


struct EvolutionStrategies
    D     # distribution constructor
    Ïˆ     # initial distribution parameterization
    âˆ‡logp # log search likelihood gradient
    m     # number of samples
    Î±     # step factor
    k_max # number of iterations
end
function evolution_strategy_weights(m)
    ws = [max(0, log(m/2+1) - log(i)) for i in 1:m]
    ws ./= sum(ws)
    ws .-= 1/m
    return ws
end
function optimize_dist(M::EvolutionStrategies, Ï€, U)
    D, Ïˆ, m, âˆ‡logp, Î± = M.D, M.Ïˆ, M.m, M.âˆ‡logp, M.Î±
    ws = evolution_strategy_weights(m)
    for k in 1:M.k_max
        Î¸s = rand(D(Ïˆ), m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*âˆ‡logp(Ïˆ, Î¸s[:,i]) for (w,i) in zip(ws,sp))
        Ïˆ += Î±.*âˆ‡
    end
    return D(Ïˆ)
end

struct IsotropicEvolutionStrategies
    Ïˆ     # initial mean
    Ïƒ     # initial standard devidation
    m     # number of samples
    Î±     # step factor
    k_max # number of iterations
end
function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
    Ïˆ, Ïƒ, m, Î±, k_max = M.Ïˆ, M.Ïƒ, M.m, M.Î±, M.k_max
    n = length(Ïˆ)
    ws = evolution_strategy_weights(2*div(m,2))
    for k in 1:k_max
        Ïµs = [randn(n) for i in 1:div(m,2)]
        append!(Ïµs, -Ïµs) # weight mirroring
        us = [U(Ï€, Ïˆ + Ïƒ.*Ïµ) for Ïµ in Ïµs]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*Ïµs[i] for (w,i) in zip(ws,sp)) / Ïƒ
        Ïˆ += Î±.*âˆ‡
    end
    return MvNormal(Ïˆ, Ïƒ)
end


struct IsotropicEvolutionStrategies
    Ïˆ # initial mean
    Ïƒ # initial standard devidation
    m # number of samples
    Î± # step factor
    k_max # number of iterations
end
function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
    Ïˆ, Ïƒ, m, Î±, k_max = M.Ïˆ, M.Ïƒ, M.m, M.Î±, M.k_max
    n = length(Ïˆ)
    ws = evolution_strategy_weights(2*div(m,2))
    for k in 1:k_max
        Ïµs = [randn(n) for i in 1:div(m,2)]
        append!(Ïµs, -Ïµs) # weight mirroring
        us = [U(Ï€, Ïˆ + Ïƒ.*Ïµ) for Ïµ in Ïµs]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*Ïµs[i] for (w,i) in zip(ws,sp)) / Ïƒ
        Ïˆ += Î±.*âˆ‡
    end
    return MvNormal(Ïˆ, Ïƒ)
end


function simulate(ğ’«::MDP, s, Ï€, d)
    Ï„ = []
    for i = 1:d
        a = Ï€(s)
        sâ€², r = ğ’«.TR(s,a)
        push!(Ï„, (s,a,r))
        s = sâ€²
    end
    return Ï„
end

struct FiniteDifferenceGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Î´ # step size
end

function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)
    ğ’«, b, d, m, Î´, Î³, n = M.ğ’«, M.b, M.d, M.m, M.Î´, M.ğ’«.Î³, length(Î¸)
    Î”Î¸(i) = [i == k ? Î´ : 0.0 for k in 1:n]
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    U(Î¸) = mean(R(simulate(ğ’«, rand(b), s->Ï€(Î¸, s), d)) for i in 1:m)
    Î”U = [U(Î¸ + Î”Î¸(i)) - U(Î¸) for i in 1:n]
    return Î”U ./ Î´
end

#=
f(x) = x^2 + 1e-2*randn()
m = 20
Î´ = 1e-2
Î”X = [Î´.*randn() for i = 1:m]
x0 = 2.0
Î”F = [f(x0 + Î”x) - f(x0) for Î”x in Î”X]
pinv(Î”X) * Î”F
=#

struct LikelihoodRatioGradient
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::LikelihoodRatioGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)*R(Ï„)
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end


struct RewardToGoGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::RewardToGoGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s)*R(Ï„,j) for (j, (s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end


struct BaselineSubtractionGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::BaselineSubtractionGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    â„“(a, s, k) = âˆ‡logÏ€(Î¸, a, s)*Î³^(k-1)
    R(Ï„, k) = sum(r*Î³^(j-1) for (j,(s,a,r)) in enumerate(Ï„[k:end]))
    numer(Ï„) = sum(â„“(a,s,k).^2*R(Ï„,k) for (k,(s,a,r)) in enumerate(Ï„))
    denom(Ï„) = sum(â„“(a,s,k).^2 for (k,(s,a)) in enumerate(Ï„))
    base(Ï„) = numer(Ï„) ./ denom(Ï„)
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    rbase = mean(base(Ï„) for Ï„ in trajs)
    âˆ‡U(Ï„) = sum(â„“(a,s,k).*(R(Ï„,k).-rbase) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡U(Ï„) for Ï„ in trajs)
end

struct PolicyGradientUpdate
    âˆ‡U # policy gradient estimate
    Î± # step factor
end
function update(M::PolicyGradientUpdate, Î¸)
    return Î¸ + M.Î± * M.âˆ‡U(Î¸)
end

scale_gradient(âˆ‡, L2_max) = min(L2_max/norm(âˆ‡), 1)*âˆ‡
clip_gradient(âˆ‡, a, b) = clamp.(âˆ‡, a, b)

struct RestrictedPolicyUpdate
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
    Ï€     # policy
    Ïµ     # divergence bound
end
function update(M::RestrictedPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    u = mean(âˆ‡U(Ï„) for Ï„ in Ï„s)
    return Î¸ + u*sqrt(2*M.Ïµ/dot(u,u))
end


struct NaturalPolicyUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
    Ï€  # policy
    Ïµ  # divergence bound
end
function natural_update(Î¸, âˆ‡f, F, Ïµ, Ï„s)
    âˆ‡fÎ¸ = mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
    u = mean(F(Ï„) for Ï„ in Ï„s) \ âˆ‡fÎ¸
    return Î¸ + u*sqrt(2Ïµ/dot(âˆ‡fÎ¸,u))
end
function update(M::NaturalPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    F(Ï„) = âˆ‡log(Ï„)*âˆ‡log(Ï„)'
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return natural_update(Î¸, âˆ‡U, F, M.Ïµ, Ï„s)
end


struct TrustRegionUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Ï€ # policy Ï€(s)
    p # policy likelihood p(Î¸, a, s)
    âˆ‡logÏ€ # log likelihood gradient
    KL # KL divergence KL(Î¸, Î¸â€², s)
    Ïµ # divergence bound
    Î± # line search reduction factor (e.g. 0.5)
end
function surrogate_objective(M::TrustRegionUpdate, Î¸, Î¸â€², Ï„s)
    d, p, Î³ = M.d, M.p, M.ğ’«.Î³
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    w(a,s) = p(Î¸â€²,a,s) / p(Î¸,a,s)
    f(Ï„) = mean(w(a,s)*R(Ï„,k) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(f(Ï„) for Ï„ in Ï„s)
end
function surrogate_constraint(M::TrustRegionUpdate, Î¸, Î¸â€², Ï„s)
    Î³ = M.ğ’«.Î³
    KL(Ï„) = mean(M.KL(Î¸, Î¸â€², s)*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(KL(Ï„) for Ï„ in Ï„s)
end

function linesearch(M::TrustRegionUpdate, f, g, Î¸, Î¸â€²)
    fÎ¸ = f(Î¸)
    while g(Î¸â€²) > M.Ïµ || f(Î¸â€²) â‰¤ fÎ¸
        Î¸â€² = Î¸ + M.Î±*(Î¸â€² - Î¸)
    end
    return Î¸â€²
end
function update(M::TrustRegionUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    F(Ï„) = âˆ‡log(Ï„)*âˆ‡log(Ï„)'
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    Î¸â€² = natural_update(Î¸, âˆ‡U, F, M.Ïµ, Ï„s)
    f(Î¸â€²) = surrogate_objective(M, Î¸, Î¸â€², Ï„s)
    g(Î¸â€²) = surrogate_constraint(M, Î¸, Î¸â€², Ï„s)
    return linesearch(M, f, g, Î¸, Î¸â€²)
end


struct ClampedSurrogateUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of trajectories
    Ï€ # policy
    p # policy likelihood
    âˆ‡Ï€ # policy likelihood gradient
    Ïµ # divergence bound
    Î± # step size
    k_max # number of iterations per update
end

function clamped_gradient(M::ClampedSurrogateUpdate, Î¸, Î¸â€², Ï„s)
    d, p, âˆ‡Ï€, Ïµ, Î³ = M.d, M.p, M.âˆ‡Ï€, M.Ïµ, M.ğ’«.Î³
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    âˆ‡f(a,s,r_togo) = begin
        P = p(Î¸, a,s)
        w = p(Î¸â€²,a,s) / P
        if (r_togo > 0 && w > 1+Ïµ) || (r_togo < 0 && w < 1-Ïµ)
            return zeros(length(Î¸))
        end
        return âˆ‡Ï€(Î¸â€², a, s) * r_togo / P
    end
    âˆ‡f(Ï„) = mean(âˆ‡f(a,s,R(Ï„,k)) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
end
function update(M::ClampedSurrogateUpdate, Î¸)
    ğ’«, b, d, m, Ï€, Î±, k_max= M.ğ’«, M.b, M.d, M.m, M.Ï€, M.Î±, M.k_max
    Ï€Î¸(s) = Ï€(Î¸, s)
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    Î¸â€² = copy(Î¸)
    for k in 1:k_max
        Î¸â€² += Î±*clamped_gradient(M, Î¸, Î¸â€², Ï„s)
    end
    return Î¸â€²
end


struct ActorCritic
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U # parameterized value function U(Ï•, s)
    âˆ‡U # gradient of value function âˆ‡U(Ï•,s)
end
function gradient(M::ActorCritic, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³ = M.U, M.âˆ‡U, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    A(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1) for (j, (s,a,r))
             in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s) for (j, (s,a,r))
             in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end


struct GeneralizedAdvantageEstimation
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U     # parameterized value function U(Ï•, s)
    âˆ‡U    # gradient of value function âˆ‡U(Ï•,s)
    Î»     # weight âˆˆ [0,1]
end
function gradient(M::GeneralizedAdvantageEstimation, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³, Î» = M.U, M.âˆ‡U, M.ğ’«.Î³, M.Î»
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    Î´(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    A(Ï„,j) = sum((Î³*Î»)^(â„“-1)*Î´(Ï„, j+â„“-1) for â„“ in 1:d-j)
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1)
             for (j, (s,a,r)) in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s)
             for (j, (s,a,r)) in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end


struct DeterministicPolicyGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡Ï€ # gradient of deterministic policy Ï€(Î¸, s)
    Q # parameterized value function Q(Ï•,s,a)
    âˆ‡QÏ• # gradient of value function with respect to Ï•
    âˆ‡Qa # gradient of value function with respect to a
    Ïƒ   # policy noise
end
function gradient(M::DeterministicPolicyGradient, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡Ï€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡Ï€
    Q, âˆ‡QÏ•, âˆ‡Qa, Ïƒ, Î³ = M.Q, M.âˆ‡QÏ•, M.âˆ‡Qa, M.Ïƒ, M.ğ’«.Î³
    Ï€_rand(s) = Ï€(Î¸, s) + Ïƒ*randn()*I
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡Ï€(Î¸,s)*âˆ‡Qa(Ï•,s,Ï€(Î¸,s))*Î³^(j-1) for (j,(s,a,r))
    in enumerate(Ï„))
    âˆ‡â„“Ï•(Ï„,j) = begin
        s, a, r = Ï„[j]
        sâ€² = Ï„[j+1][1]
        aâ€² = Ï€(Î¸,sâ€²)
        Î´ = r + Î³*Q(Ï•,sâ€²,aâ€²) - Q(Ï•,s,a)
        return Î´*(Î³*âˆ‡QÏ•(Ï•,sâ€²,aâ€²) - âˆ‡QÏ•(Ï•,s,a))
    end
    âˆ‡â„“Ï•(Ï„) = sum(âˆ‡â„“Ï•(Ï„,j) for j in 1:length(Ï„)-1)
    trajs = [simulate(ğ’«, rand(b), Ï€_rand, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end



function adversarial(ğ’«::MDP, Ï€, Î»)
    ğ’®, ğ’œ, T, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    ğ’®â€² = ğ’œâ€² = ğ’®
    Râ€² = zeros(length(ğ’®â€²), length(ğ’œâ€²))
    Tâ€² = zeros(length(ğ’®â€²), length(ğ’œâ€²), length(ğ’®â€²))
    for s in ğ’®â€²
        for a in ğ’œâ€²
            Râ€²[s,a] = -R(s, Ï€(s)) + Î»*log(T(s, Ï€(s), a))
            Tâ€²[s,a,a] = 1
        end
    end
    return MDP(Tâ€², Râ€², Î³)
end



struct BanditProblem
    Î¸ # vector of payoff probabilities
    R # reward sampler
end
function BanditProblem(Î¸)
    R(a) = rand() < Î¸[a] ? 1 : 0
    return BanditProblem(Î¸, R)
end
function simulate(ğ’«::BanditProblem, model, Ï€, h)
    for i in 1:h
        a = Ï€(model)
        r = ğ’«.R(a)
        update!(model, a, r)
    end
end

struct BanditModel
    B # vector of beta distributions
end
function update!(model::BanditModel, a, r)
    Î±, Î² = StatsBase.params(model.B[a])
    model.B[a] = Beta(Î± + r, Î² + (1-r))
    return model
end


mutable struct EpsilonGreedyExploration
    Ïµ # probability of random arm
    Î± # exploration decay factor
end
function (Ï€::EpsilonGreedyExploration)(model::BanditModel)
    if rand() < Ï€.Ïµ
        Ï€.Ïµ *= Ï€.Î±
        return rand(eachindex(model.B))
    else
        return argmax(mean.(model.B))
    end
end

#=
model(fill(Beta(),2)) #// ACHTUNG UndefVarError: model not defined
Ï€ = EpsilonGreedyExploration(0.3, 0.99)

update!(model, 1, 0)
=#

mutable struct ExploreThenCommitExploration
    k # pulls remaining until commitment
end
function (Ï€::ExploreThenCommitExploration)(model::BanditModel)
    if Ï€.k > 0
        Ï€.k -= 1
        return rand(eachindex(model.B))
    end
    return argmax(mean.(model.B))
end

mutable struct SoftmaxExploration
    Î» # precision parameter
    Î± # precision factor
end
function (Ï€::SoftmaxExploration)(model::BanditModel)
    weights = exp.(Ï€.Î» * mean.(model.B))
    Ï€.Î» *= Ï€.Î±
    return rand(Categorical(normalize(weights, 1)))
end

mutable struct QuantileExploration
    Î± # quantile (e.g. 0.95)
end

function (Ï€::QuantileExploration)(model::BanditModel)
    return argmax([quantile(B, Ï€.Î±) for B in model.B])
end


mutable struct UCB1Exploration
    c # exploration constant
end
function bonus(Ï€::UCB1Exploration, B, a)
    N = sum(b.Î± + b.Î² for b in B)
    Na = B[a].Î± + B[a].Î²
    return Ï€.c * sqrt(log(N)/Na)
end
function (Ï€::UCB1Exploration)(model::BanditModel)
    B = model.B
    Ï = mean.(B)
    u = Ï .+ [bonus(Ï€, B, a) for a in eachindex(B)]
    return argmax(u)
end

struct PosteriorSamplingExploration end

(Ï€::PosteriorSamplingExploration)(model::BanditModel) =
argmax(rand.(model.B))

function simulate(ğ’«::MDP, model, Ï€, h, s)
    for i in 1:h
        a = Ï€(model, s)
        sâ€², r = ğ’«.TR(s, a)
        update!(model, s, a, r, sâ€²)
        s = sâ€²
    end
end


mutable struct MaximumLikelihoodMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
end

function lookahead(model::MaximumLikelihoodMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s,a,:])
    if n == 0
        return 0.0
    end
    r = model.Ï[s, a] / n
    T(s,a,sâ€²) = model.N[s,a,sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function backup(model::MaximumLikelihoodMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::MaximumLikelihoodMDP, s, a, r, sâ€²)
    model.N[s,a,sâ€²] += 1
    model.Ï[s,a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end

function MDP(model::MaximumLikelihoodMDP)
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R = similar(N), similar(Ï)
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s,a,:])
            if n == 0
                T[s,a,:] .= 0.0
                R[s,a] = 0.0
            else
                T[s,a,:] = N[s,a,:] / n
                R[s,a] = Ï[s,a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end

struct FullUpdate end
function update!(planner::FullUpdate, model, s, a, r, sâ€²)
    ğ’« = MDP(model)
    U = solve(ğ’«).U
    copy!(model.U, U)
    return planner
end

struct RandomizedUpdate
    m # number of updates
end
function update!(planner::RandomizedUpdate, model, s, a, r, sâ€²)
    U = model.U
    U[s] = backup(model, U, s)
    for i in 1:planner.m
        s = rand(model.ğ’®)
        U[s] = backup(model, U, s)
    end
    return planner
end


struct PrioritizedUpdate
    m # number of updates
    pq # priority queue
end
function update!(planner::PrioritizedUpdate, model, s)
    N, U, pq = model.N, model.U, planner.pq
    ğ’®, ğ’œ = model.ğ’®, model.ğ’œ
    u = U[s]
    U[s] = backup(model, U, s)
    for sâ» in ğ’®
        for aâ» in ğ’œ
            n_sa = sum(N[sâ»,aâ»,sâ€²] for sâ€² in ğ’®)
            if n_sa > 0
                T = N[sâ»,aâ»,s] / n_sa
                priority = T * abs(U[s] - u)
                pq[sâ»] = max(get(pq, sâ», -Inf), priority)
            end
        end
    end
    return planner
end

function update!(planner::PrioritizedUpdate, model, s, a, r, sâ€²)
    planner.pq[s] = Inf
    for i in 1:planner.m
        if isempty(planner.pq)
        break
        end
        update!(planner, model, dequeue!(planner.pq))
    end
    return planner
end

function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ = model.ğ’œ, Ï€.Ïµ
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), ğ’œ)
end


mutable struct RmaxMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
    m # count threshold
    rmax # maximum reward
end

function lookahead(model::RmaxMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s,a,:])
    if n < model.m
        return model.rmax / (1-Î³)
    end
    r = model.Ï[s, a] / n
    T(s,a,sâ€²) = model.N[s,a,sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function backup(model::RmaxMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::RmaxMDP, s, a, r, sâ€²)
    model.N[s,a,sâ€²] += 1
    model.Ï[s,a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end
function MDP(model::RmaxMDP)
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R, m, rmax = similar(N), similar(Ï), model.m, model.rmax
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s,a,:])
            if n < m
                T[s,a,:] .= 0.0
                T[s,a,s] = 1.0
                R[s,a] = rmax
            else
                T[s,a,:] = N[s,a,:] / n
                R[s,a] = Ï[s,a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end

#=
N = zeros(length(ğ’®), length(ğ’œ), length(ğ’®)) # ğ’® not defined
Ï = zeros(length(ğ’®), length(ğ’œ))
U = zeros(length(ğ’®))
planner = FullUpdate()
model = MaximumLikelihoodMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner)
Ï€ = EpsilonGreedyExploration(0.1, 1)
simulate(ğ’«, model, Ï€, 100)

rmax = maximum(ğ’«.R(s,a) for s in ğ’®, a in ğ’œ)
m = 3
model = RmaxMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner, m, rmax)
Ï€ = EpsilonGreedyExploration(0, 1)
simulate(ğ’«, model, Ï€, 100)
=#

mutable struct BayesianMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    D # Dirichlet distributions D[s,a]
    R # reward function as matrix (not estimated)
    Î³ # discount
    U # value function
    planner
end
function lookahead(model::BayesianMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.D[s,a].alpha)
    if n == 0
        return 0.0
    end
    r = model.R(s,a)
    T(s,a,sâ€²) = model.D[s,a].alpha[sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function update!(model::BayesianMDP, s, a, r, sâ€²)
    Î± = model.D[s,a].alpha
    Î±[sâ€²] += 1
    model.D[s,a] = Dirichlet(Î±)
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end


struct PosteriorSamplingUpdate end
function Base.rand(model::BayesianMDP)
    ğ’®, ğ’œ = model.ğ’®, model.ğ’œ
    T = zeros(length(ğ’®), length(ğ’œ), length(ğ’®))
    for s in ğ’®
        for a in ğ’œ
            T[s,a,:] = rand(model.D[s,a])
        end
    end
    return MDP(T, model.R, model.Î³)
end
function update!(planner::PosteriorSamplingUpdate, model, s, a, r, sâ€²)
    ğ’« = rand(model)
    U = solve(ğ’«).U
    copy!(model.U, U)
end


mutable struct IncrementalEstimate
    Î¼ # mean estimate
    Î± # learning rate
    m # number of updates
end

function update!(model::IncrementalEstimate, x)
    model.m += 1
    model.Î¼ += model.Î±(model.m) * (x - model.Î¼)
    return model
end

mutable struct QLearning
    ğ’®  # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  #  discount
    Q  #  action value function
    Î±  #  learning rate
end 

lookahead(model::QLearning, s, a) = model.Q[s,a]
function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

#=
Q = zeros(length(ğ’«.ğ’®), length(ğ’«.ğ’œ))
Î± = 0.2 # learning rate
model = QLearning(ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, Q, Î±)
Ïµ = 0.1 # probability of random action
Î± = 1.0 # exploration decay factor
Ï€ = EpsilonGreedyExploration(Ïµ, Î±)
k = 20 # number of steps to simulate
s = 1 # initial state
simulate(ğ’«, model, Ï€, k, s)
=#


mutable struct Sarsa
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # action value function
    Î±  # learning rate
    â„“  # most recent experience tuple (s,a,r)
end

lookahead(model::Sarsa, s, a) = model.Q[s,a]
function update!(model::Sarsa, s, a, r, sâ€²)
    if model.â„“ != nothing
        Î³, Q, Î±, â„“ = model.Î³, model.Q, model.Î±, model.â„“
        model.Q[â„“.s,â„“.a] += Î±*(â„“.r + Î³*Q[s,a] - Q[â„“.s,â„“.a])
    end
    model.â„“ = (s=s, a=a, r=r)
    return model
end


mutable struct SarsaLambda
    ğ’®  # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # action value function
    N  # trace
    Î±  # learning rate
    Î»  # trace decay rate
    â„“  # most recent experience tuple (s,a,r)
end

lookahead(model::SarsaLambda, s, a) = model.Q[s,a]

function update!(model::SarsaLambda, s, a, r, sâ€²)
    if model.â„“ != nothing
        Î³, Î», Q, Î±, â„“ = model.Î³, model.Î», model.Q, model.Î±, model.â„“
        model.N[â„“.s,â„“.a] += 1
        Î´ = â„“.r + Î³*Q[s,a] - Q[â„“.s,â„“.a]
        for s in model.ğ’®
            for a in model.ğ’œ
                model.Q[s,a] += Î±*Î´*model.N[s,a]
                model.N[s,a] *= Î³*Î»
            end
        end
    else
        model.N[:,:] .= 0.0
    end
    model.â„“ = (s=s, a=a, r=r)
    return model
end


struct GradientQLearning
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # parameterized action value function Q(Î¸,s,a)
    âˆ‡Q # gradient of action value function
    Î¸  # action value function parameter
    Î±  # learning rate
end 

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    u = maximum(Q(Î¸,sâ€²,aâ€²) for aâ€² in ğ’œ)
    Î” = (r + Î³*u - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
    Î¸[:] += Î±*scale_gradient(Î”, 1)
    return model
end

#=
Î²(s,a) = [s,s^2,a,a^2,1]
Q(Î¸,s,a) = dot(Î¸,Î²(s,a))
âˆ‡Q(Î¸,s,a) = Î²(s,a)
Î¸ = [0.1,0.2,0.3,0.4,0.5] # initial parameter vector
Î± = 0.5 # learning rate
model = GradientQLearning(ğ’«.ğ’œ, ğ’«.Î³, Q, âˆ‡Q, Î¸, Î±)
Ïµ = 0.1 # probability of random action
Î± = 1.0 # exploration decay factor
Ï€ = EpsilonGreedyExploration(Ïµ, Î±)
k = 20 # number of steps to simulate
s = 0.0 # initial state
simulate(ğ’«, model, Ï€, k, s)
=#

struct ReplayGradientQLearning
    ğ’œ     # action space (assumes 1:nactions)
    Î³      # discount
    Q      # parameterized action value funciton Q(Î¸,s,a)
    âˆ‡Q     # gradient of action value function
    Î¸      # action value function parameter
    Î±      # learning rate
    buffer # circular memory buffer
    m      # number of steps between gradient updates
    m_grad # batch size
end

function lookahead(model::ReplayGradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::ReplayGradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    buffer, m, m_grad = model.buffer, model.m, model.m_grad
    if isfull(buffer)
        U(s) = maximum(Q(Î¸,s,a) for a in ğ’œ)
        âˆ‡Q(s,a,r,sâ€²) = (r + Î³*U(sâ€²) - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
        Î” = mean(âˆ‡Q(s,a,r,sâ€²) for (s,a,r,sâ€²) in rand(buffer, m_grad))
        Î¸[:] += Î±*scale_gradient(Î”, 1)
        for i in 1:m # discard oldest experiences
            popfirst!(buffer)
        end
    else
        push!(buffer, (s,a,r,sâ€²))
    end
    return model
end

#=
capacity = 100 # maximum size of the replay buffer
ExperienceTuple = Tuple{Float64,Float64,Float64,Float64}
M = CircularBuffer{ExperienceTuple}(capacity) # replay buffer
m_grad = 20 # batch size
model = ReplayGradientQLearning(ğ’«.ğ’œ, ğ’«.Î³, Q, âˆ‡Q, Î¸, Î±, M, m, m_grad)
=#

struct BehavioralCloning
    Î±     # step size
    k_max # number of iterations
    âˆ‡logÏ€ # log likelihood gradient
end
function optimize(M::BehavioralCloning, D, Î¸)
    Î±, k_max, âˆ‡logÏ€ = M.Î±, M.k_max, M.âˆ‡logÏ€
    for k in 1:k_max
        âˆ‡ = mean(âˆ‡logÏ€(Î¸, a, s) for (s,a) in D)
        Î¸ += Î±*âˆ‡
    end
    return Î¸
end

struct DatasetAggregation
    ğ’« # problem with unknown reward function
    bc # behavioral cloning struct
    k_max # number of iterations
    m # number of rollouts per iteration
    d # rollout depth
    b # initial state distribution
    Ï€E # expert
    Ï€Î¸ # parameterized policy
end

function optimize(M::DatasetAggregation, D, Î¸)
    ğ’«, bc, k_max, m = M.ğ’«, M.bc, M.k_max, M.m
    d, b, Ï€E, Ï€Î¸ = M.d, M.b, M.Ï€E, M.Ï€Î¸
    Î¸ = optimize(bc, D, Î¸)
    for k in 2:k_max
        for i in 1:m
            s = rand(b)
            for j in 1:d
                push!(D, (s, Ï€E(s)))
                a = rand(Ï€Î¸(Î¸, s))
                s = rand(ğ’«.T(s, a))
            end
        end
        Î¸ = optimize(bc, D, Î¸)
    end
    return Î¸
end


struct SMILe
    ğ’« # problem with unknown reward
    bc # Behavioral cloning struct
    k_max # number of iterations
    m # number of rollouts per iteration
    d # rollout depth
    b # initial state distribution
    Î² # mixing scalar (e.g., d^-3)
    Ï€E # expert policy
    Ï€Î¸ # parameterized policy
end

function optimize(M::SMILe, Î¸)
    ğ’«, bc, k_max, m = M.ğ’«, M.bc, M.k_max, M.m
    d, b, Î², Ï€E, Ï€Î¸ = M.d, M.b, M.Î², M.Ï€E, M.Ï€Î¸
    ğ’œ, T = ğ’«.ğ’œ, ğ’«.T
    Î¸s = []
    Ï€ = s -> Ï€E(s)
    for k in 1:k_max
        # execute latest Ï€ to get new dataset D
        D = []
        for i in 1:m
            s = rand(b)
            for j in 1:d
                push!(D, (s, Ï€E(s)))
                a = Ï€(s)
                s = rand(T(s, a))
            end
        end
        # train new policy classifier
        Î¸ = optimize(bc, D, Î¸)
        push!(Î¸s, Î¸)
        # compute a new policy mixture
        PÏ€ = Categorical(normalize([(1-Î²)^(i-1) for i in 1:k],1))
        Ï€ = s -> begin
            if rand() < (1-Î²)^(k-1)
                return Ï€E(s)
            else
                return rand(Categorical(Ï€Î¸(Î¸s[rand(PÏ€)], s)))
            end
        end
    end
    Ps = normalize([(1-Î²)^(i-1) for i in 1:k_max],1)
    return Ps, Î¸s
end

struct InverseReinforcementLearning
    ğ’«  # problem
    b  # initial state distribution
    d  # depth
    m  # number of samples
    Ï€  # parameterized policy
    Î²  # binary feature mapping
    Î¼E # expert feature expectations
    RL # reinforcement learning method
    Ïµ  # tolerance
end

function feature_expectations(M::InverseReinforcementLearning, Ï€)
    ğ’«, b, m, d, Î², Î³ = M.ğ’«, M.b, M.m, M.d, M.Î², M.ğ’«.Î³
    Î¼(Ï„) = sum(Î³^(k-1)*Î²(s, a) for (k,(s,a)) in enumerate(Ï„))
    Ï„s = [simulate(ğ’«, rand(b), Ï€, d) for i in 1:m]
    return mean(Î¼(Ï„) for Ï„ in Ï„s)
end

function calc_weighting(M::InverseReinforcementLearning, Î¼s)
    Î¼E = M.Î¼E
    k = length(Î¼E)
    model = Model(Ipopt.Optimizer)
    @variable(model, t)
    @variable(model, Ï•[1:k] â‰¥ 0)
    @objective(model, Max, t)
    for Î¼ in Î¼s
        @constraint(model, Ï•â‹…Î¼E â‰¥ Ï•â‹…Î¼ + t)
    end
    @constraint(model, Ï•â‹…Ï• â‰¤ 1)
    optimize!(model)
    return (value(t), value.(Ï•))
end

function calc_policy_mixture(M::InverseReinforcementLearning, Î¼s)
    Î¼E = M.Î¼E
    k = length(Î¼s)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î»[1:k] â‰¥ 0)
    @objective(model, Min, (Î¼E - sum(Î»[i]*Î¼s[i] for i in 1:k))â‹…
                           (Î¼E - sum(Î»[i]*Î¼s[i] for i in 1:k)))
    @constraint(model, sum(Î») == 1)
    optimize!(model)
    return value.(Î»)
end

function optimize(M::InverseReinforcementLearning, Î¸)
    Ï€, Ïµ, RL = M.Ï€, M.Ïµ, M.RL
    Î¸s = [Î¸]
    Î¼s = [feature_expectations(M, s->Ï€(Î¸,s))]
    while true
        t, Ï• = calc_weighting(M, Î¼s)
        if t â‰¤ Ïµ
            break
        end
        copyto!(RL.Ï•, Ï•) # R(s,a) = Ï•â‹…Î²(s,a)
        Î¸ = optimize(RL, Ï€, Î¸)
        push!(Î¸s, Î¸)
        push!(Î¼s, feature_expectations(M, s->Ï€(Î¸,s)))
    end
    Î» = calc_policy_mixture(M, Î¼s)
    return Î», Î¸s
end


struct MaximumEntropyIRL
    ğ’« # problem
    b # initial state distribution
    d # depth
    Ï€ # parameterized policy Ï€(Î¸,s)
    PÏ€ # parameterized policy likelihood Ï€(Î¸, a, s)
    âˆ‡R # reward function gradient
    RL # reinforcement learning method
    Î± # step size
    k_max # number of iterations
end

function discounted_state_visitations(M::MaximumEntropyIRL, Î¸)
    ğ’«, b, d, PÏ€ = M.ğ’«, M.b, M.d, M.PÏ€
    ğ’®, ğ’œ, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.Î³
    b_sk = zeros(length(ğ’«.ğ’®), d)
    b_sk[:,1] = [pdf(b, s) for s in ğ’®]
    for k in 2:d
        for (siâ€², sâ€²) in enumerate(ğ’®)
            b_sk[siâ€²,k] = Î³*sum(
                sum(b_sk[si,k-1]*PÏ€(Î¸, a, s)*T(s, a, sâ€²)
                for (si,s) in enumerate(ğ’®))
                    for a in ğ’œ)
        end
    end
    return normalize!(vec(mean(b_sk, dims=2)),1)
end
function optimize(M::MaximumEntropyIRL, D, Ï•, Î¸)
    ğ’«, Ï€, PÏ€, âˆ‡R, RL, Î±, k_max = M.ğ’«, M.Ï€, M.PÏ€, M.âˆ‡R, M.RL, M.Î±, M.k_max
    ğ’®, ğ’œ, Î³, nD = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, length(D)
    for k in 1:k_max
        copyto!(RL.Ï•, Ï•) # update parameters
        Î¸ = optimize(RL, Ï€, Î¸)
        b = discounted_state_visitations(M, Î¸)
        âˆ‡RÏ„ = Ï„ -> sum(Î³^(i-1)*âˆ‡R(Ï•,s,a) for (i,(s,a)) in enumerate(Ï„))
        âˆ‡f = sum(âˆ‡RÏ„(Ï„) for Ï„ in D) - nD*sum(
            b[si]*sum(PÏ€(Î¸,a,s)*âˆ‡R(Ï•,s,a)
                for (ai,a) in enumerate(ğ’œ))
            for (si, s) in enumerate(ğ’®))
        Ï• += Î±*âˆ‡f
    end
    return Ï•, Î¸
end

struct POMDP
    Î³ # discount factor
    ğ’® # state space
    ğ’œ # action space
    ğ’ª # observation space
    T # transition function
    R # reward function
    O # observation function
    TRO # sample transition, reward, and observation
end

function update(b::Vector{Float64}, ğ’«, a, o)
    ğ’®, T, O = ğ’«.ğ’®, ğ’«.T, ğ’«.O
    bâ€² = similar(b)
    for (iâ€², sâ€²) in enumerate(ğ’®)
        po = O(a, sâ€², o)
        bâ€²[iâ€²] = po * sum(T(s, a, sâ€²) * b[i] for (i, s) in enumerate(ğ’®))
    end
    if sum(bâ€²) â‰ˆ 0.0
        fill!(bâ€², 1)
    end
    return normalize!(bâ€², 1)
end

struct KalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
end

function update(b::KalmanFilter, ğ’«, a, o)
    Î¼b, Î£b = b.Î¼b, b.Î£b
    Ts, Ta, Os = ğ’«.Ts, ğ’«.Ta, ğ’«.Os
    Î£s, Î£o = ğ’«.Î£s, ğ’«.Î£o
    # predict
    Î¼p = Ts*Î¼b + Ta*a
    Î£p = Ts*Î£b*Ts' + Î£s
    # update
    K = Î£p*Os'/(Os*Î£p*Os' + Î£o)
    Î¼bâ€² = Î¼p + K*(o - Os*Î¼p)
    Î£bâ€² = (I - K*Os)*Î£p
    return KalmanFilter(Î¼bâ€², Î£bâ€²)
end


struct ExtendedKalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
end

import ForwardDiff: jacobian
function update(b::ExtendedKalmanFilter, ğ’«, a, o)
    Î¼b, Î£b = b.Î¼b, b.Î£b
    fT, fO = ğ’«.fT, ğ’«.fO
    Î£s, Î£o = ğ’«.Î£s, ğ’«.Î£o
    # predict
    Î¼p = fT(Î¼b, a)
    Ts = jacobian(s->fT(s, a), Î¼b)
    Os = jacobian(fO, Î¼p)
    Î£p = Ts*Î£b*Ts' + Î£s
    # update
    K = Î£p*Os'/(Os*Î£p*Os' + Î£o)
    Î¼bâ€² = Î¼p + K*(o - fO(Î¼p))
    Î£bâ€² = (I - K*Os)*Î£p
    return ExtendedKalmanFilter(Î¼bâ€², Î£bâ€²)
end


struct UnscentedKalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
    Î»  # spread parameter
end

function unscented_transform(Î¼, Î£, f, Î», ws)
    n = length(Î¼)
    Î” = cholesky((n + Î») * Î£).L
    S = [Î¼]
    for i in 1:n
        push!(S, Î¼ + Î”[:,i])
        push!(S, Î¼ - Î”[:,i])
    end
    Sâ€² = f.(S)
    Î¼â€² = sum(w*s for (w,s) in zip(ws, Sâ€²))
    Î£â€² = sum(w*(s - Î¼â€²)*(s - Î¼â€²)' for (w,s) in zip(ws, Sâ€²))
    return (Î¼â€², Î£â€², S, Sâ€²)
end

function update(b::UnscentedKalmanFilter, ğ’«, a, o)
    Î¼b, Î£b, Î» = b.Î¼b, b.Î£b, b.Î»
    fT, fO = ğ’«.fT, ğ’«.fO
    n = length(Î¼b)
    ws = [Î» / (n + Î»); fill(1/(2(n + Î»)), 2n)]
    # predict
    Î¼p, Î£p, Sp, Spâ€² = unscented_transform(Î¼b, Î£b, s->fT(s,a), Î», ws)
    Î£p += ğ’«.Î£s
    # update
    Î¼o, Î£o, So, Soâ€² = unscented_transform(Î¼p, Î£p, fO, Î», ws)
    Î£o += ğ’«.Î£o
    Î£po = sum(w*(s - Î¼p)*(sâ€² - Î¼o)' for (w,s,sâ€²) in zip(ws, So, Soâ€²))
    K = Î£po / Î£o
    Î¼bâ€² = Î¼p + K*(o - Î¼o)
    Î£bâ€² = Î£p - K*Î£o*K'
    return UnscentedKalmanFilter(Î¼bâ€², Î£bâ€², Î»)
end

struct ParticleFilter
    states # vector of state samples
end
function update(b::ParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    D = SetCategorical(states, weights)
    return ParticleFilter(rand(D, length(states)))
end

struct RejectionParticleFilter
    states # vector of state samples
end
function update(b::RejectionParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    states = similar(b.states)
    i = 1
    while i â‰¤ length(states)
        s = rand(b.states)
        sâ€² = rand(T(s,a))
        if rand(O(a,sâ€²)) == o
            states[i] = sâ€²
            i += 1
        end
    end
    return RejectionParticleFilter(states)
end

struct InjectionParticleFilter
    states # vector of state samples
    m_inject # number of samples to inject
    D_inject # injection distribution
end

function update(b::InjectionParticleFilter, ğ’«, a, o)
    T, O, m_inject, D_inject = ğ’«.T, ğ’«.O, b.m_inject, b.D_inject
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    D = SetCategorical(states, weights)
    m = length(states)
    states = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))
    return InjectionParticleFilter(states, m_inject, D_inject)
end

mutable struct AdaptiveInjectionParticleFilter
    states   # vector of state samples
    w_slow   # slow moving average
    w_fast   # fast moving average
    Î±_slow   # slow moving average parameter
    Î±_fast   # fast moving average parameter
    Î½        # injection parameter
    D_inject # injection distribution
end

function update(b::AdaptiveInjectionParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    w_slow, w_fast, Î±_slow, Î±_fast, Î½, D_inject =
        b.w_slow, b.w_fast, b.Î±_slow, b.Î±_fast, b.Î½, b.D_inject
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    w_mean = mean(weights)
    w_slow += Î±_slow*(w_mean - w_slow)
    w_fast += Î±_fast*(w_mean - w_fast)
    m = length(states)
    m_inject = round(Int, m * max(0, 1.0 - Î½*w_fast / w_slow))
    D = SetCategorical(states, weights) # ACHTUNG not found
    states = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))
    b.w_slow, b.w_fast = w_slow, w_fast
    return AdaptiveInjectionParticleFilter(states,
        w_slow, w_fast, Î±_slow, Î±_fast, Î½, D_inject)
end

struct ConditionalPlan
    a        # action to take at root
    subplans # dictionary mapping observations to subplans
end
ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]


function lookahead(ğ’«::POMDP, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s,a) + Î³*uâ€²
end
function evaluate_plan(ğ’«::POMDP, Ï€::ConditionalPlan, s)
    U(o,sâ€²) = evaluate_plan(ğ’«, Ï€(o), sâ€²)
    return isempty(Ï€.subplans) ? ğ’«.R(s,Ï€()) : lookahead(ğ’«, U, s, Ï€())
end

function alphavector(ğ’«::POMDP, Ï€::ConditionalPlan)
    return [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
end


struct AlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
    a # actions associated with alpha vectors
end
function utility(Ï€::AlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end
function (Ï€::AlphaVectorPolicy)(b)
    i = argmax([Î±â‹…b for Î± in Ï€.Î“])
    return Ï€.a[i]
end


function lookahead(ğ’«::POMDP, U, b::Vector, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    r = sum(R(s,a)*b[i] for (i,s) in enumerate(ğ’®))
    Posa(o,s,a) = sum(O(a,sâ€²,o)*T(s,a,sâ€²) for sâ€² in ğ’®)
    Poba(o,b,a) = sum(b[i]*Posa(o,s,a) for (i,s) in enumerate(ğ’®))
    return r + Î³*sum(Poba(o,b,a)*U(update(b, ğ’«, a, o)) for o in ğ’ª)
end

function greedy(ğ’«::POMDP, U, b::Vector)
    u, a = findmax(a->lookahead(ğ’«, U, b, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end
struct LookaheadAlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
end
function utility(Ï€::LookaheadAlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end
function greedy(Ï€, b)
    U(b) = utility(Ï€, b)
    return greedy(Ï€.ğ’«, U, b)
end
(Ï€::LookaheadAlphaVectorPolicy)(b) = greedy(Ï€, b).a


function find_maximal_belief(Î±, Î“)
    m = length(Î±)
    if isempty(Î“)
        return fill(1/m, m) # arbitrary belief
    end
    model = Model(GLPK.Optimizer)
    @variable(model, Î´)
    @variable(model, b[i=1:m] â‰¥ 0)
    @constraint(model, sum(b) == 1.0)
    for a in Î“
        @constraint(model, (Î±-a)â‹…b â‰¥ Î´)
    end
    @objective(model, Max, Î´)
    optimize!(model)
    return value(Î´) > 0 ? value.(b) : nothing
end


function find_dominating(Î“)
    n = length(Î“)
    candidates, dominating = trues(n), falses(n)
    while any(candidates)
        i = findfirst(candidates)
        b = find_maximal_belief(Î“[i], Î“[dominating])
        if b === nothing
            candidates[i] = false
        else
            k = argmax([candidates[j] ? bâ‹…Î“[j] : -Inf for j in 1:n])
            candidates[k], dominating[k] = false, true
        end
    end
    return dominating
end
function prune(plans, Î“)
    d = find_dominating(Î“)
    return (plans[d], Î“[d])
end


function value_iteration(ğ’«::POMDP, k_max)
    ğ’®, ğ’œ, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    plans = [ConditionalPlan(a) for a in ğ’œ]
    Î“ = [[R(s,a) for s in ğ’®] for a in ğ’œ]
    plans, Î“ = prune(plans, Î“)
    for k in 2:k_max
        plans, Î“ = expand(plans, Î“, ğ’«)
        plans, Î“ = prune(plans, Î“)
    end
    return (plans, Î“)
end

function solve(M::ValueIteration, ğ’«::POMDP)
    plans, Î“ = value_iteration(ğ’«, M.k_max)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end












































































