
# Mykel J. Kochenderfer, Tim A. Wheeler, Kyle H. Wray (2022) Algorithms for Decision Making

# using IJulia

using LinearAlgebra
using Distributions
using LightGraphs
using JuMP
using GLPK
using Ipopt

# using GraphRecipes
using Plots
using IterTools

# import IterTools: subsets
function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_ma
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end
Base.argmax(f::Function, xs) = findmax(f, xs)[2]

Base.Dict{Symbol,V}(a::NamedTuple) where V =
          Dict{Symbol,V}(n => v for (n, v) in zip(keys(a), values(a)))

Base.convert(::Type{Dict{Symbol,V}}, a::NamedTuple) where V =
          Dict{Symbol,V}(a)

Base.isequal(a::Dict{Symbol,<:Any}, nt::NamedTuple) =
          length(a) == length(nt) &&
          all(a[n] == v for (n, v) in zip(keys(nt), values(nt)))

struct Variable
    name::Symbol
    m::Int # number of possible values
end

const Assignment = Dict{Symbol,Int}
const FactorTable = Dict{Assignment,Float64}

struct Factor
    vars::Vector{Variable}
    table::FactorTable
end

variablenames(Ï•::Factor) = [var.name for var in Ï•.vars]
select(a::Assignment, varnames::Vector{Symbol}) =
                Assignment(n => a[n] for n in varnames)

function assignments(vars::AbstractVector{Variable})
    names = [var.name for var in vars]
    return vec([Assignment(n => v for (n, v) in zip(names, values))
                for values in product((1:v.m for v in vars)...)])
end

function normalize!(Ï•::Factor)
    z = sum(p for (a, p) in Ï•.table)
    for (a, p) in Ï•.table
        Ï•.table[a] = p / z
    end
    return Ï•
end

struct BayesianNetwork
    vars::Vector{Variable}
    factors::Vector{Factor}
    graph::SimpleDiGraph{Int64}
end

B = Variable(:b, 2); S = Variable(:s, 2)
E = Variable(:e, 2)
D = Variable(:d, 2); C = Variable(:c, 2)
vars = [B, S, E, D, C]
factors = [
    Factor([B], FactorTable((b = 1,) => 0.99, (b = 2,) => 0.01)),
    Factor([S], FactorTable((s = 1,) => 0.98, (s = 2,) => 0.02)),
    Factor([E,B,S], FactorTable(
        (e = 1, b = 1, s = 1) => 0.90, (e = 1, b = 1, s = 2) => 0.04,
        (e = 1, b = 2, s = 1) => 0.05, (e = 1, b = 2, s = 2) => 0.01,
        (e = 2, b = 1, s = 1) => 0.10, (e = 2, b = 1, s = 2) => 0.96,
        (e = 2, b = 2, s = 1) => 0.95, (e = 2, b = 2, s = 2) => 0.99)),
    Factor([D, E], FactorTable(
        (d = 1, e = 1) => 0.96, (d = 1, e = 2) => 0.03,
        (d = 2, e = 1) => 0.04, (d = 2, e = 2) => 0.97)),
    Factor([C, E], FactorTable(
        (c = 1, e = 1) => 0.98, (c = 1, e = 2) => 0.01,
        (c = 2, e = 1) => 0.02, (c = 2, e = 2) => 0.99))
]
graph = SimpleDiGraph(5)
add_edge!(graph, 1, 3); add_edge!(graph, 2, 3)
add_edge!(graph, 3, 4); add_edge!(graph, 3, 5)
bn = BayesianNetwork(vars, factors, graph)

function probability(bn::BayesianNetwork, assignment)
    subassignment(Ï•) = select(assignment, variablenames(Ï•))
    probability(Ï•) = get(Ï•.table, subassignment(Ï•), 0.0)
    return prod(probability(Ï•) for Ï• in bn.factors)
end

function Base.:*(Ï•::Factor, Ïˆ::Factor)
    Ï•names = variablenames(Ï•)
    Ïˆnames = variablenames(Ïˆ)
    Ïˆonly = setdiff(Ïˆ.vars, Ï•.vars)
    table = FactorTable()
    for (Ï•a, Ï•p) in Ï•.table
        for a in assignments(Ïˆonly)
            a = merge(Ï•a, a)
            Ïˆa = select(a, Ïˆnames)
            table[a] = Ï•p * get(Ïˆ.table, Ïˆa, 0.0)
        end
    end
    vars = vcat(Ï•.vars, Ïˆonly)
    return Factor(vars, table)
end

function marginalize(Ï•::Factor, name)
    table = FactorTable()
    for (a, p) in Ï•.table
        aâ€² = delete!(copy(a), name)
        table[aâ€²] = get(table, aâ€², 0.0) + p
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end

in_scope(name, Ï•) = any(name == v.name for v in Ï•.vars)

function condition(Ï•::Factor, name, value)
    if !in_scope(name, Ï•)
        return Ï•
    end
    table = FactorTable()
    for (a, p) in Ï•.table
        if a[name] == value
            table[delete!(copy(a), name)] = p
        end
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end
function condition(Ï•::Factor, evidence)
    for (name, value) in pairs(evidence)
        Ï• = condition(Ï•, name, value)
    end
    return Ï•
end

struct ExactInference end
function infer(M::ExactInference, bn, query, evidence)
    Ï• = prod(bn.factors)
    Ï• = condition(Ï•, evidence)
    for name in setdiff(variablenames(Ï•), query)
    Ï• = marginalize(Ï•, name)
        end
    return normalize!(Ï•)
end

struct VariableElimination
    ordering # array of variable indices
end
function infer(M::VariableElimination, bn, query, evidence)
    Î¦ = [condition(Ï•, evidence) for Ï• in bn.factors]
    for i in M.ordering
        name = bn.vars[i].name
        if name âˆ‰ query
            inds = findall(Ï• -> in_scope(name, Ï•), Î¦)
            if !isempty(inds)
                Ï• = prod(Î¦[inds])
                deleteat!(Î¦, inds)
                Ï• = marginalize(Ï•, name)
                push!(Î¦, Ï•)
            end
        end
    end
    return normalize!(prod(Î¦))
end

function Base.rand(Ï•::Factor)
    tot, p, w = 0.0, rand(), sum(values(Ï•.table))
    for (a, v) in Ï•.table
        tot += v / w
        if tot >= p
            return a
        end
    end
    return Assignment()
end

function Base.rand(bn::BayesianNetwork)
    a = Assignment()
    for i in topological_sort(bn.graph)
        name, Ï• = bn.vars[i].name, bn.factors[i]
        a[name] = rand(condition(Ï•, a))[name]
    end
    return a
end

struct DirectSampling
    m # number of samples
end
function infer(M::DirectSampling, bn, query, evidence)
    table = FactorTable()
    for i in 1:(M.m)
        a = rand(bn)
        if all(a[k] == v for (k, v) in pairs(evidence))
            b = select(a, query)
            table[b] = get(table, b, 0) + 1
        end
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

struct LikelihoodWeightedSampling
    m # number of samples
end
function infer(M::LikelihoodWeightedSampling, bn, query, evidence)
    table = FactorTable()
    ordering = topological_sort(bn.graph)
    for i in 1:(M.m)
        a, w = Assignment(), 1.0
        for j in ordering
            name, Ï• = bn.vars[j].name, bn.factors[j]
            if haskey(evidence, name)
                a[name] = evidence[name]
                w *= Ï•.table[select(a, variablenames(Ï•))]
            else
                a[name] = rand(condition(Ï•, a))[name]
            end
        end
        b = select(a, query)
        table[b] = get(table, b, 0) + w
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function blanket(bn, a, i)
    name = bn.vars[i].name
    val = a[name]
    a = delete!(copy(a), name)
    Î¦ = filter(Ï• -> in_scope(name, Ï•), bn.factors)
    Ï• = prod(condition(Ï•, a) for Ï• in Î¦)
    return normalize!(Ï•)
end

function update_gibbs_sample!(a, bn, evidence, ordering)
    for i in ordering
        name = bn.vars[i].name
        if !haskey(evidence, name)
            b = blanket(bn, a, i)
            a[name] = rand(b)[name]
    end
end
end

function gibbs_sample!(a, bn, evidence, ordering, m)
    for j in 1:m
        update_gibbs_sample!(a, bn, evidence, ordering)
end
end
struct GibbsSampling
    m_samples # number of samples to use
    m_burnin # number of samples to discard during burn-in
    m_skip # number of samples to skip for thinning
    ordering # array of variable indices
end

function infer(M::GibbsSampling, bn, query, evidence)
    table = FactorTable()
    a = merge(rand(bn), evidence)
    gibbs_sample!(a, bn, evidence, M.ordering, M.m_burnin)
    for i in 1:(M.m_samples)
        gibbs_sample!(a, bn, evidence, M.ordering, M.m_skip)
        b = select(a, query)
        table[b] = get(table, b, 0) + 1
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function infer(D::MvNormal, query, evidencevars, evidence)
    Î¼, Î£ = D.Î¼, D.Î£.mat
    b, Î¼a, Î¼b = evidence, Î¼[query], Î¼[evidencevars]
    A = Î£[query,query]
    B = Î£[evidencevars,evidencevars]
    C = Î£[query,evidencevars]
    Î¼ = Î¼[query] + C * (B \ (b - Î¼b))
    Î£ = A - C * (B \ C')
    return MvNormal(Î¼, Î£)
end

function sub2ind(siz, x)
    k = vcat(1, cumprod(siz[1:end - 1]))
    return dot(k, x .- 1) + 1
end
function statistics(vars, G, D::Matrix{Int})
    n = size(D, 1)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    M = [zeros(q[i], r[i]) for i in 1:n]
    for o in eachcol(D)
        for i in 1:n
            k = o[i]
            parents = inneighbors(G, i)
            j = 1
            if !isempty(parents)
                j = sub2ind(r[parents], o[parents])
            end
            M[i][j,k] += 1.0
        end
    end
    return M
end
    
# G = SimpleDiGraph(3)
# add_edge!(G, 1, 2)
# add_edge!(G, 3, 2)
# vars = [Variable(:A,2), Variable(:B,2), Variable(:C,2)]
# D = [1 2 2 1; 1 2 2 1; 2 2 2 2]
# M = statistics(vars, G, D)

# Î¸ = [mapslices(x->normalize(x,1), Mi, dims=2) for Mi in M]

function prior(vars, G)
    n = length(vars)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    return [ones(q[i], r[i]) for i in 1:n]
end

gaussian_kernel(b) = x -> pdf(Normal(0, b), x)

function kernel_density_estimate(Ï•, O)
    return x -> sum([Ï•(x - o) for o in O]) / length(O)
end

function bayesian_score_component(M, Î±)
    p = sum(loggamma.(Î± + M))
    p -= sum(loggamma.(Î±))
    p += sum(loggamma.(sum(Î±, dims=2)))
    p -= sum(loggamma.(sum(Î±, dims=2) + sum(M, dims=2)))
    return p
end
function bayesian_score(vars, G, D)
    n = length(vars)
    M = statistics(vars, G, D)
    Î± = prior(vars, G)
    return sum(bayesian_score_component(M[i], Î±[i]) for i in 1:n)
end

struct K2Search
    ordering::Vector{Int} # variable ordering
end
function fit(method::K2Search, vars, D)
    G = SimpleDiGraph(length(vars))
    for (k, i) in enumerate(method.ordering[2:end])
        y = bayesian_score(vars, G, D)
        while true
            y_best, j_best = -Inf, 0
            for j in method.ordering[1:k]
                if !has_edge(G, j, i)
                add_edge!(G, j, i)
                yâ€² = bayesian_score(vars, G, D)
                    if yâ€² > y_best
                        y_best, j_best = yâ€², j
                    end
                rem_edge!(G, j, i)
                end
            end
            if y_best > y
                y = y_best
                add_edge!(G, j_best, i)
            else
                break
            end
        end
    end
    return G
end

struct LocalDirectedGraphSearch
    G # initial graph
    k_max # number of iterations
end
function rand_graph_neighbor(G)
    n = nv(G)
    i = rand(1:n)
    j = mod1(i + rand(2:n) - 1, n)
    Gâ€² = copy(G)
    has_edge(G, i, j) ? rem_edge!(Gâ€², i, j) : add_edge!(Gâ€², i, j)
    return Gâ€²
end
function fit(method::LocalDirectedGraphSearch, vars, D)
    G = method.G
    y = bayesian_score(vars, G, D)
    for k in 1:method.k_max
        Gâ€² = rand_graph_neighbor(G)
        yâ€² = is_cyclic(Gâ€²) ? -Inf : bayesian_score(vars, Gâ€², D)
        if yâ€² > y
            y, G = yâ€², Gâ€²
        end
    end
    return G
end


function are_markov_equivalent(G, H)
    if nv(G) != nv(H) || ne(G) != ne(H) ||
        !all(has_edge(H, e) ||
        has_edge(H, reverse(e))
            for e in edges(G))
            return false
    end
    for c in 1:nv(G)
        parents = inneighbors(G, c)
        for (a, b) in subsets(parents, 2)
            if !has_edge(G, a, b) && !has_edge(G, b, a) &&
                !(has_edge(H, a, c) && has_edge(H, b, c))
                    return false
            end
        end
    end
    return true
end

struct SimpleProblem
    bn::BayesianNetwork
    chance_vars::Vector{Variable}
    decision_vars::Vector{Variable}
    utility_vars::Vector{Variable}
    utilities::Dict{Symbol,Vector{Float64}}
end
function solve(ğ’«::SimpleProblem, evidence, M)
    query = [var.name for var in ğ’«.utility_vars]
    U(a) = sum(ğ’«.utilities[uname][a[uname]] for uname in query)
    best = (a = nothing, u = -Inf)
    for assignment in assignments(ğ’«.decision_vars)
        evidence = merge(evidence, assignment)
        Ï• = infer(M, ğ’«.bn, query, evidence)
        u = sum(p * U(a) for (a, p) in Ï•.table)
        if u > best.u
            best = (a = assignment, u = u)
        end
    end
    return best
end

function value_of_information(ğ’«, query, evidence, M)
    Ï• = infer(M, ğ’«.bn, query, evidence)
    voi = -solve(ğ’«, evidence, M).u
    query_vars = filter(v -> v.name âˆˆ query, ğ’«.chance_vars)
    for oâ€² in assignments(query_vars)
        ooâ€² = merge(evidence, oâ€²)
        p = Ï•.table[oâ€²]
        voi += p * solve(ğ’«, ooâ€², M).u
    end
    return voi
end

struct MDP
    Î³  # discount factor
    ğ’®  # state space
    ğ’œ # action space
    T  # transition function
    R  # reward function
    TR # sample transition and reward
end

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U(sâ€²) for sâ€² in ğ’®)
end
function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s, a) + Î³ * sum(T(s, a, sâ€²) * U[i] for (i, sâ€²) in enumerate(ğ’®))
end

function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end

function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³ * Tâ€²) \ Râ€²
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end
function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a -> lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a = a, u = u)
end

(Ï€::ValueFunctionPolicy)(s) = greedy(Ï€.ğ’«, Ï€.U, s).a

struct PolicyIteration
    Ï€ # initial policy
    k_max # maximum number of iterations
end
function solve(M::PolicyIteration, ğ’«::MDP)
    Ï€, ğ’® = M.Ï€, ğ’«.ğ’®
    for k = 1:M.k_max
        U = policy_evaluation(ğ’«, Ï€)
        Ï€â€² = ValueFunctionPolicy(ğ’«, U)
        if all(Ï€(s) == Ï€â€²(s) for s in ğ’®)
            break
        end
        Ï€ = Ï€â€²
    end
    return Ï€
end

function backup(ğ’«::MDP, U, s)
    return maximum(lookahead(ğ’«, U, s, a) for a in ğ’«.ğ’œ)
end

struct ValueIteration
    k_max # maximum number of iterations
    end
function solve(M::ValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’«.ğ’®]
    for k = 1:M.k_max
        U = [backup(ğ’«, U, s) for s in ğ’«.ğ’®]
    end
    return ValueFunctionPolicy(ğ’«, U)
end

struct GaussSeidelValueIteration
    k_max # maximum number of iterations
end
function solve(M::GaussSeidelValueIteration, ğ’«::MDP)
    U = [0.0 for s in ğ’®]
    for k = 1:M.k_max
        for (s, i) in enumerate(ğ’«.ğ’®)
            U[i] = backup(ğ’«, U, s)
        end
    end
    return ValueFunctionPolicy(ğ’«, U)
end

struct LinearProgramFormulation end
function tensorform(ğ’«::MDP)
    ğ’®, ğ’œ, R, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    Râ€² = [R(s,a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    return ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end
solve(ğ’«::MDP) = solve(LinearProgramFormulation(), ğ’«)
function solve(M::LinearProgramFormulation, ğ’«::MDP)
    ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    model = Model(GLPK.Optimizer)
    @variable(model, U[ğ’®])
    @objective(model, Min, sum(U))
    @constraint(model, [s=ğ’®,a=ğ’œ], U[s] â‰¥ R[s,a] + ğ’«.Î³*T[s,a,:]â‹…U)
    optimize!(model)
    return ValueFunctionPolicy(ğ’«, value.(U))
end

struct LinearQuadraticProblem
    Ts # transition matrix with respect to state
    Ta # transition matrix with respect to action
    Rs # reward matrix with respect to state (negative semidefinite)
    Ra # reward matrix with respect to action (negative definite)
    h_max # horizon
end
function solve(ğ’«::LinearQuadraticProblem)
    Ts, Ta, Rs, Ra, h_max = ğ’«.Ts, ğ’«.Ta, ğ’«.Rs, ğ’«.Ra, ğ’«.h_max
    V = zeros(size(Rs))
    Ï€s = Any[s -> zeros(size(Ta, 2))]
    for h in 2:h_max
        V = Ts'*(V - V*Ta*((Ta'*V*Ta + Ra) \ Ta'*V))*Ts + Rs
        L = -(Ta'*V*Ta + Ra) \ Ta' * V * Ts
        push!(Ï€s, s -> L*s)
    end
    return Ï€s
end

struct ApproximateValueIteration
    UÎ¸    # initial parameterized value function that supports fit!
    S     # set of discrete states for performing backups
    k_max # maximum number of iterations
end
function solve(M::ApproximateValueIteration, ğ’«::MDP)
    UÎ¸, S, k_max = M.UÎ¸, M.S, M.k_max
    for k in 1:k_max
        U = [backup(ğ’«, UÎ¸, s) for s in S]
        fit!(UÎ¸, S, U)
    end
    return ValueFunctionPolicy(ğ’«, UÎ¸)
end

mutable struct NearestNeighborValueFunction
    k # number of neighbors
    d # distance function d(s, sâ€²)
    S # set of discrete states
    Î¸ # vector of values at states in S
end

function (UÎ¸::NearestNeighborValueFunction)(s)
    dists = [UÎ¸.d(s,sâ€²) for sâ€² in UÎ¸.S]
    ind = sortperm(dists)[1:UÎ¸.k]
    return mean(UÎ¸.Î¸[i] for i in ind)
end
function fit!(UÎ¸::NearestNeighborValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct LocallyWeightedValueFunction
    k # kernel function k(s, sâ€²)
    S # set of discrete states
    Î¸ # vector of values at states in S
end

function (UÎ¸::LocallyWeightedValueFunction)(s)
    w = normalize([UÎ¸.k(s,sâ€²) for sâ€² in UÎ¸.S], 1)
    return UÎ¸.Î¸ â‹… w
end
function fit!(UÎ¸::LocallyWeightedValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct MultilinearValueFunction
    o # position of lower-left corner
    Î´ # vector of widths
    Î¸ # vector of values at states in S
end

function (UÎ¸::MultilinearValueFunction)(s)
    o, Î´, Î¸ = UÎ¸.o, UÎ¸.Î´, UÎ¸.Î¸
    Î” = (s - o)./Î´
    # Multidimensional index of lower-left cell
    i = min.(floor.(Int, Î”) .+ 1, size(Î¸) .- 1)
    vertex_index = similar(i)
    d = length(s)
    u = 0.0
    for vertex in 0:2^d-1
        weight = 1.0
        for j in 1:d
            # Check whether jth bit is set
            if vertex & (1 << (j-1)) > 0
                vertex_index[j] = i[j] + 1
                weight *= Î”[j] - i[j] + 1
            else
                vertex_index[j] = i[j]
                weight *= i[j] - Î”[j]
            end
        end
        u += Î¸[vertex_index...]*weight
    end
    return u
end
function fit!(UÎ¸::MultilinearValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct SimplexValueFunction
    o # position of lower-left corner
    Î´ # vector of widths
    Î¸ # vector of values at states in S
end

function (UÎ¸::SimplexValueFunction)(s)
    Î” = (s - UÎ¸.o)./UÎ¸.Î´
    # Multidimensional index of upper-right cell
    i = min.(floor.(Int, Î”) .+ 1, size(UÎ¸.Î¸) .- 1) .+ 1
    u = 0.0
    sâ€² = (s - (UÎ¸.o + UÎ¸.Î´.*(i.-2))) ./ UÎ¸.Î´
    p = sortperm(sâ€²) # increasing order
    w_tot = 0.0
    for j in p
        w = sâ€²[j] - w_tot
        u += w*UÎ¸.Î¸[i...]
        i[j] -= 1
        w_tot += w
    end
    u += (1 - w_tot)*UÎ¸.Î¸[i...]
    return u
end

function fit!(UÎ¸::SimplexValueFunction, S, U)
    UÎ¸.Î¸ = U
    return UÎ¸
end

mutable struct LinearRegressionValueFunction
    Î² # basis vector function
    Î¸ # vector of parameters
end
function (UÎ¸::LinearRegressionValueFunction)(s)
    return UÎ¸.Î²(s) â‹… UÎ¸.Î¸
end
function fit!(UÎ¸::LinearRegressionValueFunction, S, U)
    X = hcat([UÎ¸.Î²(s) for s in S]...)'
    UÎ¸.Î¸ = pinv(X)*U
    return UÎ¸
end


struct RolloutLookahead
    ğ’« # problem
    Ï€ # rollout policy
    d # depth
end
randstep(ğ’«::MDP, s, a) = ğ’«.TR(s, a)
function rollout(ğ’«, s, Ï€, d)
    ret = 0.0
    for t in 1:d
        a = Ï€(s)
        s, r = randstep(ğ’«, s, a)
        ret += ğ’«.Î³^(t-1) * r
    end
    return ret
 end
function (Ï€::RolloutLookahead)(s)
    U(s) = rollout(Ï€.ğ’«, s, Ï€.Ï€, Ï€.d)
    return greedy(Ï€.ğ’«, U, s).a
end

struct ForwardSearch
    ğ’« # problem
    d # depth
    U # value function at depth d
end
function forward_search(ğ’«, s, d, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end
    best = (a=nothing, u=-Inf)
    Uâ€²(s) = forward_search(ğ’«, s, d-1, U).u
    for a in ğ’«.ğ’œ
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::ForwardSearch)(s) = forward_search(Ï€.ğ’«, s, Ï€.d, Ï€.U).a

struct BranchAndBound
    ğ’«  # problem
    d   # depth
    Ulo # lower bound on value function at depth d
    Qhi # upper bound on action value function
end
function branch_and_bound(ğ’«, s, d, Ulo, Qhi)
    if d â‰¤ 0
        return (a=nothing, u=Ulo(s))
    end
    Uâ€²(s) = branch_and_bound(ğ’«, s, d-1, Ulo, Qhi).u
    best = (a=nothing, u=-Inf)
    for a in sort(ğ’«.ğ’œ, by=a->Qhi(s,a), rev=true)
        if Qhi(s, a) < best.u
            return best # safe to prune
        end
        u = lookahead(ğ’«, Uâ€², s, a)
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::BranchAndBound)(s) = branch_and_bound(Ï€.ğ’«, s, Ï€.d, Ï€.Ulo, Ï€.Qhi).a

struct SparseSampling
    ğ’« # problem
    d # depth
    m # number of samples
    U # value function at depth d
end
function sparse_sampling(ğ’«, s, d, m, U)
    if d â‰¤ 0
        return (a=nothing, u=U(s))
    end
    best = (a=nothing, u=-Inf)
    for a in ğ’«.ğ’œ
        u = 0.0
        for i in 1:m
            sâ€², r = randstep(ğ’«, s, a)
            aâ€², uâ€² = sparse_sampling(ğ’«, sâ€², d-1, m, U)
            u += (r + ğ’«.Î³*uâ€²) / m
        end
        if u > best.u
            best = (a=a, u=u)
        end
    end
    return best
end
(Ï€::SparseSampling)(s) = sparse_sampling(Ï€.ğ’«, s, Ï€.d, Ï€.m, Ï€.U).a

struct MonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    U # value function estimate
end
function (Ï€::MonteCarloTreeSearch)(s)
    for k in 1:Ï€.m
        simulate!(Ï€, s)
    end
    return argmax(a->Ï€.Q[(s,a)], Ï€.ğ’«.ğ’œ)
end

function simulate!(Ï€::MonteCarloTreeSearch, s, d=Ï€.d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’œ, TR, Î³ = ğ’«.ğ’œ, ğ’«.TR, ğ’«.Î³
    if !haskey(N, (s, first(ğ’œ)))
        for a in ğ’œ
            N[(s,a)] = 0
            Q[(s,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, s)
    sâ€², r = TR(s,a)
    q = r + Î³*simulate!(Ï€, sâ€², d-1)
    N[(s,a)] += 1
    Q[(s,a)] += (q-Q[(s,a)])/N[(s,a)]
    return q
end

bonus(Nsa, Ns) = Nsa == 0 ? Inf : sqrt(log(Ns)/Nsa)
function explore(Ï€::MonteCarloTreeSearch, s)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Ns = sum(N[(s,a)] for a in ğ’œ)
    return argmax(a->Q[(s,a)] + c*bonus(N[(s,a)], Ns), ğ’œ)
end


struct HeuristicSearch
    ğ’«   # problem
    Uhi # upper bound on value function
    d   # depth
    m   # number of simulations
end
function simulate!(Ï€::HeuristicSearch, U, s)
    ğ’« = Ï€.ğ’«
    for d in 1:Ï€.d
        a, u = greedy(ğ’«, U, s)
        U[s] = u
        s = rand(ğ’«.T(s, a))
    end
end
function (Ï€::HeuristicSearch)(s)
    U = [Ï€.Uhi(s) for s in Ï€.ğ’«.ğ’®]
    for i in 1:m
        simulate!(Ï€, U, s)
    end
    return greedy(Ï€.ğ’«, U, s).a
end

struct LabeledHeuristicSearch
    ğ’«   # problem
    Uhi # upper bound on value function
    d   # depth
    Î´   # gap threshold
end
function (Ï€::LabeledHeuristicSearch)(s)
    U, solved = [Ï€.Uhi(s) for s in ğ’«.ğ’®], Set()
    while s âˆ‰ solved
        simulate!(Ï€, U, solved, s)
    end
    return greedy(Ï€.ğ’«, U, s).a
end

function simulate!(Ï€::LabeledHeuristicSearch, U, solved, s)
    visited = []
    for d in 1:Ï€.d
    if s âˆˆ solved
        break
    end
    push!(visited, s)
    a, u = greedy(Ï€.ğ’«, U, s)
    U[s] = u
    s = rand(Ï€.ğ’«.T(s, a))
    end
    while !isempty(visited)
        if label!(Ï€, U, solved, pop!(visited))
            break
        end
    end
end


function expand(Ï€::LabeledHeuristicSearch, U, solved, s)
    ğ’«, Î´ = Ï€.ğ’«, Ï€.Î´
    ğ’®, ğ’œ, T = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T
    found, toexpand, envelope = false, Set(s), []
    while !isempty(toexpand)
        s = pop!(toexpand)
        push!(envelope, s)
        a, u = greedy(ğ’«, U, s)
        if abs(U[s] - u) > Î´
        found = true
        else
            for sâ€² in ğ’®
                if T(s,a,sâ€²) > 0 && sâ€² âˆ‰ (solved âˆª envelope)
                    push!(toexpand, sâ€²)
                end
            end
        end
    end
    return (found, envelope)
end

function label!(Ï€::LabeledHeuristicSearch, U, solved, s)
    if s âˆˆ solved
        return false
    end
    found, envelope = expand(Ï€, U, solved, s)
    if found
        for s âˆˆ reverse(envelope)
            U[s] = greedy(Ï€.ğ’«, U, s).u
        end
    else
        union!(solved, envelope)
    end
    return found
end

#=
model = Model(Ipopt.Optimizer)
d = 10
current_state = zeros(4)
goal = [10,10,0,0]
obstacle = [3,4]
@variables model begin
s[1:4, 1:d]
-1 â‰¤ a[1:2,1:d] â‰¤ 1
end
# velocity update
@constraint(model, [i=2:d,j=1:2], s[2+j,i] == s[2+j,i-1] + a[j,i-1])
# position update
@constraint(model, [i=2:d,j=1:2], s[j,i] == s[j,i-1] + s[2+j,i-1])
# initial condition
@constraint(model, s[:,1] .== current_state)
# obstacle
@constraint(model, [i=1:d], sum((s[1:2,i] - obstacle).^2) â‰¥ 4)
@objective(model, Min, 100*sum((s[:,d] - goal).^2) + sum(a.^2))
optimize!(model)
action = value.(a[:,1])
# EXIT: Optimal Solution Found.
# 2-element Vector{Float64}:
#  0.8304708931331188
#  0.32812069979351177
=#

struct MonteCarloPolicyEvaluation
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
end

function (U::MonteCarloPolicyEvaluation)(Ï€)
    R(Ï€) = rollout(U.ğ’«, rand(U.b), Ï€, U.d)
    return mean(R(Ï€) for i = 1:U.m)
end

(U::MonteCarloPolicyEvaluation)(Ï€, Î¸) = U(s->Ï€(Î¸, s))


struct HookeJeevesPolicySearch
    Î¸ # initial parameterization
    Î± # step size
    c # step size reduction factor
    Ïµ # termination step size
end
function optimize(M::HookeJeevesPolicySearch, Ï€, U)
    Î¸, Î¸â€², Î±, c, Ïµ = copy(M.Î¸), similar(M.Î¸), M.Î±, M.c, M.Ïµ
    u, n = U(Ï€, Î¸), length(Î¸)
    while Î± > Ïµ
        copyto!(Î¸â€², Î¸)
        best = (i=0, sgn=0, u=u)
        for i in 1:n
            for sgn in (-1,1)
                Î¸â€²[i] = Î¸[i] + sgn*Î±
                uâ€² = U(Ï€, Î¸â€²)
                if uâ€² > best.u
                    best = (i=i, sgn=sgn, u=uâ€²)
                end
            end
            Î¸â€²[i] = Î¸[i]
        end
        if best.i != 0
            Î¸[best.i] += best.sgn*Î±
            u = best.u
        else
            Î± *= c
        end
    end
    return Î¸
end

function Ï€(Î¸, s)
    return rand(Normal(Î¸[1]*s, abs(Î¸[2]) + 0.00001))
end

#=
ğ’« = #nothing
b, d, n_rollouts = Normal(0.3,0.1), 10, 3
U = MonteCarloPolicyEvaluation(ğ’«, b, d, n_rollouts)
Î¸, Î±, c, Ïµ = [0.0,1.0], 0.75, 0.75, 0.01
M = HookeJeevesPolicySearch(Î¸, Î±, c, Ïµ)
Î¸ = optimize(M, Ï€, U)
=#

struct GeneticPolicySearch
    Î¸s      # initial population
    Ïƒ       # initial standard devidation
    m_elite # number of elite samples
    k_max   # number of iterations
end
function optimize(M::GeneticPolicySearch, Ï€, U)
    Î¸s, Ïƒ = M.Î¸s, M.Ïƒ
    n, m = length(first(Î¸s)), length(Î¸s)
    for k in 1:M.k_max
        us = [U(Ï€, Î¸) for Î¸ in Î¸s]
        sp = sortperm(us, rev=true)
        Î¸_best = Î¸s[sp[1]]
        rand_elite() = Î¸s[sp[rand(1:M.m_elite)]]
        Î¸s = [rand_elite() + Ïƒ.*randn(n) for i in 1:(m-1)]
        push!(Î¸s, Î¸_best)
    end
    return last(Î¸s)
end


struct CrossEntropyPolicySearch
    p # initial distribution
    m # number of samples
    m_elite # number of elite samples
    k_max   # number of iterations
end

function optimize_dist(M::CrossEntropyPolicySearch, Ï€, U)
    p, m, m_elite, k_max = M.p, M.m, M.m_elite, M.k_max
    for k in 1:k_max
        Î¸s = rand(p, m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        Î¸_elite = Î¸s[:,sortperm(us)[(m-m_elite+1):m]]
        p = Distributions.fit(typeof(p), Î¸_elite)
    end
    return p
end

function optimize(M, Ï€, U)
    return Distributions.mode(optimize_dist(M, Ï€, U))
end


struct EvolutionStrategies
    D     # distribution constructor
    Ïˆ     # initial distribution parameterization
    âˆ‡logp # log search likelihood gradient
    m     # number of samples
    Î±     # step factor
    k_max # number of iterations
end
function evolution_strategy_weights(m)
    ws = [max(0, log(m/2+1) - log(i)) for i in 1:m]
    ws ./= sum(ws)
    ws .-= 1/m
    return ws
end
function optimize_dist(M::EvolutionStrategies, Ï€, U)
    D, Ïˆ, m, âˆ‡logp, Î± = M.D, M.Ïˆ, M.m, M.âˆ‡logp, M.Î±
    ws = evolution_strategy_weights(m)
    for k in 1:M.k_max
        Î¸s = rand(D(Ïˆ), m)
        us = [U(Ï€, Î¸s[:,i]) for i in 1:m]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*âˆ‡logp(Ïˆ, Î¸s[:,i]) for (w,i) in zip(ws,sp))
        Ïˆ += Î±.*âˆ‡
    end
    return D(Ïˆ)
end

struct IsotropicEvolutionStrategies
    Ïˆ     # initial mean
    Ïƒ     # initial standard devidation
    m     # number of samples
    Î±     # step factor
    k_max # number of iterations
end
function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
    Ïˆ, Ïƒ, m, Î±, k_max = M.Ïˆ, M.Ïƒ, M.m, M.Î±, M.k_max
    n = length(Ïˆ)
    ws = evolution_strategy_weights(2*div(m,2))
    for k in 1:k_max
        Ïµs = [randn(n) for i in 1:div(m,2)]
        append!(Ïµs, -Ïµs) # weight mirroring
        us = [U(Ï€, Ïˆ + Ïƒ.*Ïµ) for Ïµ in Ïµs]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*Ïµs[i] for (w,i) in zip(ws,sp)) / Ïƒ
        Ïˆ += Î±.*âˆ‡
    end
    return MvNormal(Ïˆ, Ïƒ)
end


struct IsotropicEvolutionStrategies
    Ïˆ # initial mean
    Ïƒ # initial standard devidation
    m # number of samples
    Î± # step factor
    k_max # number of iterations
end
function optimize_dist(M::IsotropicEvolutionStrategies, Ï€, U)
    Ïˆ, Ïƒ, m, Î±, k_max = M.Ïˆ, M.Ïƒ, M.m, M.Î±, M.k_max
    n = length(Ïˆ)
    ws = evolution_strategy_weights(2*div(m,2))
    for k in 1:k_max
        Ïµs = [randn(n) for i in 1:div(m,2)]
        append!(Ïµs, -Ïµs) # weight mirroring
        us = [U(Ï€, Ïˆ + Ïƒ.*Ïµ) for Ïµ in Ïµs]
        sp = sortperm(us, rev=true)
        âˆ‡ = sum(w.*Ïµs[i] for (w,i) in zip(ws,sp)) / Ïƒ
        Ïˆ += Î±.*âˆ‡
    end
    return MvNormal(Ïˆ, Ïƒ)
end


function simulate(ğ’«::MDP, s, Ï€, d)
    Ï„ = []
    for i = 1:d
        a = Ï€(s)
        sâ€², r = ğ’«.TR(s,a)
        push!(Ï„, (s,a,r))
        s = sâ€²
    end
    return Ï„
end

struct FiniteDifferenceGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Î´ # step size
end

function gradient(M::FiniteDifferenceGradient, Ï€, Î¸)
    ğ’«, b, d, m, Î´, Î³, n = M.ğ’«, M.b, M.d, M.m, M.Î´, M.ğ’«.Î³, length(Î¸)
    Î”Î¸(i) = [i == k ? Î´ : 0.0 for k in 1:n]
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    U(Î¸) = mean(R(simulate(ğ’«, rand(b), s->Ï€(Î¸, s), d)) for i in 1:m)
    Î”U = [U(Î¸ + Î”Î¸(i)) - U(Î¸) for i in 1:n]
    return Î”U ./ Î´
end

#=
f(x) = x^2 + 1e-2*randn()
m = 20
Î´ = 1e-2
Î”X = [Î´.*randn() for i = 1:m]
x0 = 2.0
Î”F = [f(x0 + Î”x) - f(x0) for Î”x in Î”X]
pinv(Î”X) * Î”F
=#

struct LikelihoodRatioGradient
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::LikelihoodRatioGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)*R(Ï„)
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end


struct RewardToGoGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::RewardToGoGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    âˆ‡U(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s)*R(Ï„,j) for (j, (s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡U(simulate(ğ’«, rand(b), Ï€Î¸, d)) for i in 1:m)
end


struct BaselineSubtractionGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
end
function gradient(M::BaselineSubtractionGradient, Ï€, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    â„“(a, s, k) = âˆ‡logÏ€(Î¸, a, s)*Î³^(k-1)
    R(Ï„, k) = sum(r*Î³^(j-1) for (j,(s,a,r)) in enumerate(Ï„[k:end]))
    numer(Ï„) = sum(â„“(a,s,k).^2*R(Ï„,k) for (k,(s,a,r)) in enumerate(Ï„))
    denom(Ï„) = sum(â„“(a,s,k).^2 for (k,(s,a)) in enumerate(Ï„))
    base(Ï„) = numer(Ï„) ./ denom(Ï„)
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    rbase = mean(base(Ï„) for Ï„ in trajs)
    âˆ‡U(Ï„) = sum(â„“(a,s,k).*(R(Ï„,k).-rbase) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡U(Ï„) for Ï„ in trajs)
end

struct PolicyGradientUpdate
    âˆ‡U # policy gradient estimate
    Î± # step factor
end
function update(M::PolicyGradientUpdate, Î¸)
    return Î¸ + M.Î± * M.âˆ‡U(Î¸)
end

scale_gradient(âˆ‡, L2_max) = min(L2_max/norm(âˆ‡), 1)*âˆ‡
clip_gradient(âˆ‡, a, b) = clamp.(âˆ‡, a, b)

struct RestrictedPolicyUpdate
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
    Ï€     # policy
    Ïµ     # divergence bound
end
function update(M::RestrictedPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    u = mean(âˆ‡U(Ï„) for Ï„ in Ï„s)
    return Î¸ + u*sqrt(2*M.Ïµ/dot(u,u))
end


struct NaturalPolicyUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood
    Ï€  # policy
    Ïµ  # divergence bound
end
function natural_update(Î¸, âˆ‡f, F, Ïµ, Ï„s)
    âˆ‡fÎ¸ = mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
    u = mean(F(Ï„) for Ï„ in Ï„s) \ âˆ‡fÎ¸
    return Î¸ + u*sqrt(2Ïµ/dot(âˆ‡fÎ¸,u))
end
function update(M::NaturalPolicyUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    F(Ï„) = âˆ‡log(Ï„)*âˆ‡log(Ï„)'
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return natural_update(Î¸, âˆ‡U, F, M.Ïµ, Ï„s)
end


struct TrustRegionUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    Ï€ # policy Ï€(s)
    p # policy likelihood p(Î¸, a, s)
    âˆ‡logÏ€ # log likelihood gradient
    KL # KL divergence KL(Î¸, Î¸â€², s)
    Ïµ # divergence bound
    Î± # line search reduction factor (e.g. 0.5)
end
function surrogate_objective(M::TrustRegionUpdate, Î¸, Î¸â€², Ï„s)
    d, p, Î³ = M.d, M.p, M.ğ’«.Î³
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    w(a,s) = p(Î¸â€²,a,s) / p(Î¸,a,s)
    f(Ï„) = mean(w(a,s)*R(Ï„,k) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(f(Ï„) for Ï„ in Ï„s)
end
function surrogate_constraint(M::TrustRegionUpdate, Î¸, Î¸â€², Ï„s)
    Î³ = M.ğ’«.Î³
    KL(Ï„) = mean(M.KL(Î¸, Î¸â€², s)*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(KL(Ï„) for Ï„ in Ï„s)
end

function linesearch(M::TrustRegionUpdate, f, g, Î¸, Î¸â€²)
    fÎ¸ = f(Î¸)
    while g(Î¸â€²) > M.Ïµ || f(Î¸â€²) â‰¤ fÎ¸
        Î¸â€² = Î¸ + M.Î±*(Î¸â€² - Î¸)
    end
    return Î¸â€²
end
function update(M::TrustRegionUpdate, Î¸)
    ğ’«, b, d, m, âˆ‡logÏ€, Ï€, Î³ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€, M.Ï€, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„) = sum(r*Î³^(k-1) for (k, (s,a,r)) in enumerate(Ï„))
    âˆ‡log(Ï„) = sum(âˆ‡logÏ€(Î¸, a, s) for (s,a) in Ï„)
    âˆ‡U(Ï„) = âˆ‡log(Ï„)*R(Ï„)
    F(Ï„) = âˆ‡log(Ï„)*âˆ‡log(Ï„)'
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    Î¸â€² = natural_update(Î¸, âˆ‡U, F, M.Ïµ, Ï„s)
    f(Î¸â€²) = surrogate_objective(M, Î¸, Î¸â€², Ï„s)
    g(Î¸â€²) = surrogate_constraint(M, Î¸, Î¸â€², Ï„s)
    return linesearch(M, f, g, Î¸, Î¸â€²)
end


struct ClampedSurrogateUpdate
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of trajectories
    Ï€ # policy
    p # policy likelihood
    âˆ‡Ï€ # policy likelihood gradient
    Ïµ # divergence bound
    Î± # step size
    k_max # number of iterations per update
end

function clamped_gradient(M::ClampedSurrogateUpdate, Î¸, Î¸â€², Ï„s)
    d, p, âˆ‡Ï€, Ïµ, Î³ = M.d, M.p, M.âˆ‡Ï€, M.Ïµ, M.ğ’«.Î³
    R(Ï„, j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in zip(j:d, Ï„[j:end]))
    âˆ‡f(a,s,r_togo) = begin
        P = p(Î¸, a,s)
        w = p(Î¸â€²,a,s) / P
        if (r_togo > 0 && w > 1+Ïµ) || (r_togo < 0 && w < 1-Ïµ)
            return zeros(length(Î¸))
        end
        return âˆ‡Ï€(Î¸â€², a, s) * r_togo / P
    end
    âˆ‡f(Ï„) = mean(âˆ‡f(a,s,R(Ï„,k)) for (k,(s,a,r)) in enumerate(Ï„))
    return mean(âˆ‡f(Ï„) for Ï„ in Ï„s)
end
function update(M::ClampedSurrogateUpdate, Î¸)
    ğ’«, b, d, m, Ï€, Î±, k_max= M.ğ’«, M.b, M.d, M.m, M.Ï€, M.Î±, M.k_max
    Ï€Î¸(s) = Ï€(Î¸, s)
    Ï„s = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    Î¸â€² = copy(Î¸)
    for k in 1:k_max
        Î¸â€² += Î±*clamped_gradient(M, Î¸, Î¸â€², Ï„s)
    end
    return Î¸â€²
end


struct ActorCritic
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U # parameterized value function U(Ï•, s)
    âˆ‡U # gradient of value function âˆ‡U(Ï•,s)
end
function gradient(M::ActorCritic, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³ = M.U, M.âˆ‡U, M.ğ’«.Î³
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    A(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1) for (j, (s,a,r))
             in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s) for (j, (s,a,r))
             in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end


struct GeneralizedAdvantageEstimation
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    m     # number of samples
    âˆ‡logÏ€ # gradient of log likelihood âˆ‡logÏ€(Î¸,a,s)
    U     # parameterized value function U(Ï•, s)
    âˆ‡U    # gradient of value function âˆ‡U(Ï•,s)
    Î»     # weight âˆˆ [0,1]
end
function gradient(M::GeneralizedAdvantageEstimation, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡logÏ€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡logÏ€
    U, âˆ‡U, Î³, Î» = M.U, M.âˆ‡U, M.ğ’«.Î³, M.Î»
    Ï€Î¸(s) = Ï€(Î¸, s)
    R(Ï„,j) = sum(r*Î³^(k-1) for (k,(s,a,r)) in enumerate(Ï„[j:end]))
    Î´(Ï„,j) = Ï„[j][3] + Î³*U(Ï•,Ï„[j+1][1]) - U(Ï•,Ï„[j][1])
    A(Ï„,j) = sum((Î³*Î»)^(â„“-1)*Î´(Ï„, j+â„“-1) for â„“ in 1:d-j)
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡logÏ€(Î¸,a,s)*A(Ï„,j)*Î³^(j-1)
             for (j, (s,a,r)) in enumerate(Ï„[1:end-1]))
    âˆ‡â„“Ï•(Ï„) = sum((U(Ï•,s) - R(Ï„,j))*âˆ‡U(Ï•,s)
             for (j, (s,a,r)) in enumerate(Ï„))
    trajs = [simulate(ğ’«, rand(b), Ï€Î¸, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end


struct DeterministicPolicyGradient
    ğ’« # problem
    b # initial state distribution
    d # depth
    m # number of samples
    âˆ‡Ï€ # gradient of deterministic policy Ï€(Î¸, s)
    Q # parameterized value function Q(Ï•,s,a)
    âˆ‡QÏ• # gradient of value function with respect to Ï•
    âˆ‡Qa # gradient of value function with respect to a
    Ïƒ   # policy noise
end
function gradient(M::DeterministicPolicyGradient, Ï€, Î¸, Ï•)
    ğ’«, b, d, m, âˆ‡Ï€ = M.ğ’«, M.b, M.d, M.m, M.âˆ‡Ï€
    Q, âˆ‡QÏ•, âˆ‡Qa, Ïƒ, Î³ = M.Q, M.âˆ‡QÏ•, M.âˆ‡Qa, M.Ïƒ, M.ğ’«.Î³
    Ï€_rand(s) = Ï€(Î¸, s) + Ïƒ*randn()*I
    âˆ‡UÎ¸(Ï„) = sum(âˆ‡Ï€(Î¸,s)*âˆ‡Qa(Ï•,s,Ï€(Î¸,s))*Î³^(j-1) for (j,(s,a,r))
    in enumerate(Ï„))
    âˆ‡â„“Ï•(Ï„,j) = begin
        s, a, r = Ï„[j]
        sâ€² = Ï„[j+1][1]
        aâ€² = Ï€(Î¸,sâ€²)
        Î´ = r + Î³*Q(Ï•,sâ€²,aâ€²) - Q(Ï•,s,a)
        return Î´*(Î³*âˆ‡QÏ•(Ï•,sâ€²,aâ€²) - âˆ‡QÏ•(Ï•,s,a))
    end
    âˆ‡â„“Ï•(Ï„) = sum(âˆ‡â„“Ï•(Ï„,j) for j in 1:length(Ï„)-1)
    trajs = [simulate(ğ’«, rand(b), Ï€_rand, d) for i in 1:m]
    return mean(âˆ‡UÎ¸(Ï„) for Ï„ in trajs), mean(âˆ‡â„“Ï•(Ï„) for Ï„ in trajs)
end



function adversarial(ğ’«::MDP, Ï€, Î»)
    ğ’®, ğ’œ, T, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    ğ’®â€² = ğ’œâ€² = ğ’®
    Râ€² = zeros(length(ğ’®â€²), length(ğ’œâ€²))
    Tâ€² = zeros(length(ğ’®â€²), length(ğ’œâ€²), length(ğ’®â€²))
    for s in ğ’®â€²
        for a in ğ’œâ€²
            Râ€²[s,a] = -R(s, Ï€(s)) + Î»*log(T(s, Ï€(s), a))
            Tâ€²[s,a,a] = 1
        end
    end
    return MDP(Tâ€², Râ€², Î³)
end



struct BanditProblem
    Î¸ # vector of payoff probabilities
    R # reward sampler
end
function BanditProblem(Î¸)
    R(a) = rand() < Î¸[a] ? 1 : 0
    return BanditProblem(Î¸, R)
end
function simulate(ğ’«::BanditProblem, model, Ï€, h)
    for i in 1:h
        a = Ï€(model)
        r = ğ’«.R(a)
        update!(model, a, r)
    end
end

struct BanditModel
    B # vector of beta distributions
end
function update!(model::BanditModel, a, r)
    Î±, Î² = StatsBase.params(model.B[a])
    model.B[a] = Beta(Î± + r, Î² + (1-r))
    return model
end


mutable struct EpsilonGreedyExploration
    Ïµ # probability of random arm
    Î± # exploration decay factor
end
function (Ï€::EpsilonGreedyExploration)(model::BanditModel)
    if rand() < Ï€.Ïµ
        Ï€.Ïµ *= Ï€.Î±
        return rand(eachindex(model.B))
    else
        return argmax(mean.(model.B))
    end
end

#=
model(fill(Beta(),2)) #// ACHTUNG UndefVarError: model not defined
Ï€ = EpsilonGreedyExploration(0.3, 0.99)

update!(model, 1, 0)
=#

mutable struct ExploreThenCommitExploration
    k # pulls remaining until commitment
end
function (Ï€::ExploreThenCommitExploration)(model::BanditModel)
    if Ï€.k > 0
        Ï€.k -= 1
        return rand(eachindex(model.B))
    end
    return argmax(mean.(model.B))
end

mutable struct SoftmaxExploration
    Î» # precision parameter
    Î± # precision factor
end
function (Ï€::SoftmaxExploration)(model::BanditModel)
    weights = exp.(Ï€.Î» * mean.(model.B))
    Ï€.Î» *= Ï€.Î±
    return rand(Categorical(normalize(weights, 1)))
end

mutable struct QuantileExploration
    Î± # quantile (e.g. 0.95)
end

function (Ï€::QuantileExploration)(model::BanditModel)
    return argmax([quantile(B, Ï€.Î±) for B in model.B])
end


mutable struct UCB1Exploration
    c # exploration constant
end
function bonus(Ï€::UCB1Exploration, B, a)
    N = sum(b.Î± + b.Î² for b in B)
    Na = B[a].Î± + B[a].Î²
    return Ï€.c * sqrt(log(N)/Na)
end
function (Ï€::UCB1Exploration)(model::BanditModel)
    B = model.B
    Ï = mean.(B)
    u = Ï .+ [bonus(Ï€, B, a) for a in eachindex(B)]
    return argmax(u)
end

struct PosteriorSamplingExploration end

(Ï€::PosteriorSamplingExploration)(model::BanditModel) =
argmax(rand.(model.B))

function simulate(ğ’«::MDP, model, Ï€, h, s)
    for i in 1:h
        a = Ï€(model, s)
        sâ€², r = ğ’«.TR(s, a)
        update!(model, s, a, r, sâ€²)
        s = sâ€²
    end
end


mutable struct MaximumLikelihoodMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
end

function lookahead(model::MaximumLikelihoodMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s,a,:])
    if n == 0
        return 0.0
    end
    r = model.Ï[s, a] / n
    T(s,a,sâ€²) = model.N[s,a,sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function backup(model::MaximumLikelihoodMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::MaximumLikelihoodMDP, s, a, r, sâ€²)
    model.N[s,a,sâ€²] += 1
    model.Ï[s,a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end

function MDP(model::MaximumLikelihoodMDP)
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R = similar(N), similar(Ï)
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s,a,:])
            if n == 0
                T[s,a,:] .= 0.0
                R[s,a] = 0.0
            else
                T[s,a,:] = N[s,a,:] / n
                R[s,a] = Ï[s,a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end

struct FullUpdate end
function update!(planner::FullUpdate, model, s, a, r, sâ€²)
    ğ’« = MDP(model)
    U = solve(ğ’«).U
    copy!(model.U, U)
    return planner
end

struct RandomizedUpdate
    m # number of updates
end
function update!(planner::RandomizedUpdate, model, s, a, r, sâ€²)
    U = model.U
    U[s] = backup(model, U, s)
    for i in 1:planner.m
        s = rand(model.ğ’®)
        U[s] = backup(model, U, s)
    end
    return planner
end


struct PrioritizedUpdate
    m # number of updates
    pq # priority queue
end
function update!(planner::PrioritizedUpdate, model, s)
    N, U, pq = model.N, model.U, planner.pq
    ğ’®, ğ’œ = model.ğ’®, model.ğ’œ
    u = U[s]
    U[s] = backup(model, U, s)
    for sâ» in ğ’®
        for aâ» in ğ’œ
            n_sa = sum(N[sâ»,aâ»,sâ€²] for sâ€² in ğ’®)
            if n_sa > 0
                T = N[sâ»,aâ»,s] / n_sa
                priority = T * abs(U[s] - u)
                pq[sâ»] = max(get(pq, sâ», -Inf), priority)
            end
        end
    end
    return planner
end

function update!(planner::PrioritizedUpdate, model, s, a, r, sâ€²)
    planner.pq[s] = Inf
    for i in 1:planner.m
        if isempty(planner.pq)
        break
        end
        update!(planner, model, dequeue!(planner.pq))
    end
    return planner
end

function (Ï€::EpsilonGreedyExploration)(model, s)
    ğ’œ, Ïµ = model.ğ’œ, Ï€.Ïµ
    if rand() < Ïµ
        return rand(ğ’œ)
    end
    Q(s,a) = lookahead(model, s, a)
    return argmax(a->Q(s,a), ğ’œ)
end


mutable struct RmaxMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    N # transition count N(s,a,sâ€²)
    Ï # reward sum Ï(s, a)
    Î³ # discount
    U # value function
    planner
    m # count threshold
    rmax # maximum reward
end

function lookahead(model::RmaxMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.N[s,a,:])
    if n < model.m
        return model.rmax / (1-Î³)
    end
    r = model.Ï[s, a] / n
    T(s,a,sâ€²) = model.N[s,a,sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function backup(model::RmaxMDP, U, s)
    return maximum(lookahead(model, s, a) for a in model.ğ’œ)
end
function update!(model::RmaxMDP, s, a, r, sâ€²)
    model.N[s,a,sâ€²] += 1
    model.Ï[s,a] += r
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end
function MDP(model::RmaxMDP)
    N, Ï, ğ’®, ğ’œ, Î³ = model.N, model.Ï, model.ğ’®, model.ğ’œ, model.Î³
    T, R, m, rmax = similar(N), similar(Ï), model.m, model.rmax
    for s in ğ’®
        for a in ğ’œ
            n = sum(N[s,a,:])
            if n < m
                T[s,a,:] .= 0.0
                T[s,a,s] = 1.0
                R[s,a] = rmax
            else
                T[s,a,:] = N[s,a,:] / n
                R[s,a] = Ï[s,a] / n
            end
        end
    end
    return MDP(T, R, Î³)
end

#=
N = zeros(length(ğ’®), length(ğ’œ), length(ğ’®)) # ğ’® not defined
Ï = zeros(length(ğ’®), length(ğ’œ))
U = zeros(length(ğ’®))
planner = FullUpdate()
model = MaximumLikelihoodMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner)
Ï€ = EpsilonGreedyExploration(0.1, 1)
simulate(ğ’«, model, Ï€, 100)

rmax = maximum(ğ’«.R(s,a) for s in ğ’®, a in ğ’œ)
m = 3
model = RmaxMDP(ğ’®, ğ’œ, N, Ï, Î³, U, planner, m, rmax)
Ï€ = EpsilonGreedyExploration(0, 1)
simulate(ğ’«, model, Ï€, 100)
=#

mutable struct BayesianMDP
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    D # Dirichlet distributions D[s,a]
    R # reward function as matrix (not estimated)
    Î³ # discount
    U # value function
    planner
end
function lookahead(model::BayesianMDP, s, a)
    ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
    n = sum(model.D[s,a].alpha)
    if n == 0
        return 0.0
    end
    r = model.R(s,a)
    T(s,a,sâ€²) = model.D[s,a].alpha[sâ€²] / n
    return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
end

function update!(model::BayesianMDP, s, a, r, sâ€²)
    Î± = model.D[s,a].alpha
    Î±[sâ€²] += 1
    model.D[s,a] = Dirichlet(Î±)
    update!(model.planner, model, s, a, r, sâ€²)
    return model
end


struct PosteriorSamplingUpdate end
function Base.rand(model::BayesianMDP)
    ğ’®, ğ’œ = model.ğ’®, model.ğ’œ
    T = zeros(length(ğ’®), length(ğ’œ), length(ğ’®))
    for s in ğ’®
        for a in ğ’œ
            T[s,a,:] = rand(model.D[s,a])
        end
    end
    return MDP(T, model.R, model.Î³)
end
function update!(planner::PosteriorSamplingUpdate, model, s, a, r, sâ€²)
    ğ’« = rand(model)
    U = solve(ğ’«).U
    copy!(model.U, U)
end


mutable struct IncrementalEstimate
    Î¼ # mean estimate
    Î± # learning rate
    m # number of updates
end

function update!(model::IncrementalEstimate, x)
    model.m += 1
    model.Î¼ += model.Î±(model.m) * (x - model.Î¼)
    return model
end

mutable struct QLearning
    ğ’®  # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  #  discount
    Q  #  action value function
    Î±  #  learning rate
end 

lookahead(model::QLearning, s, a) = model.Q[s,a]
function update!(model::QLearning, s, a, r, sâ€²)
    Î³, Q, Î± = model.Î³, model.Q, model.Î±
    Q[s,a] += Î±*(r + Î³*maximum(Q[sâ€²,:]) - Q[s,a])
    return model
end

#=
Q = zeros(length(ğ’«.ğ’®), length(ğ’«.ğ’œ))
Î± = 0.2 # learning rate
model = QLearning(ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, Q, Î±)
Ïµ = 0.1 # probability of random action
Î± = 1.0 # exploration decay factor
Ï€ = EpsilonGreedyExploration(Ïµ, Î±)
k = 20 # number of steps to simulate
s = 1 # initial state
simulate(ğ’«, model, Ï€, k, s)
=#


mutable struct Sarsa
    ğ’® # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # action value function
    Î±  # learning rate
    â„“  # most recent experience tuple (s,a,r)
end

lookahead(model::Sarsa, s, a) = model.Q[s,a]
function update!(model::Sarsa, s, a, r, sâ€²)
    if model.â„“ != nothing
        Î³, Q, Î±, â„“ = model.Î³, model.Q, model.Î±, model.â„“
        model.Q[â„“.s,â„“.a] += Î±*(â„“.r + Î³*Q[s,a] - Q[â„“.s,â„“.a])
    end
    model.â„“ = (s=s, a=a, r=r)
    return model
end


mutable struct SarsaLambda
    ğ’®  # state space (assumes 1:nstates)
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # action value function
    N  # trace
    Î±  # learning rate
    Î»  # trace decay rate
    â„“  # most recent experience tuple (s,a,r)
end

lookahead(model::SarsaLambda, s, a) = model.Q[s,a]

function update!(model::SarsaLambda, s, a, r, sâ€²)
    if model.â„“ != nothing
        Î³, Î», Q, Î±, â„“ = model.Î³, model.Î», model.Q, model.Î±, model.â„“
        model.N[â„“.s,â„“.a] += 1
        Î´ = â„“.r + Î³*Q[s,a] - Q[â„“.s,â„“.a]
        for s in model.ğ’®
            for a in model.ğ’œ
                model.Q[s,a] += Î±*Î´*model.N[s,a]
                model.N[s,a] *= Î³*Î»
            end
        end
    else
        model.N[:,:] .= 0.0
    end
    model.â„“ = (s=s, a=a, r=r)
    return model
end


struct GradientQLearning
    ğ’œ # action space (assumes 1:nactions)
    Î³  # discount
    Q  # parameterized action value function Q(Î¸,s,a)
    âˆ‡Q # gradient of action value function
    Î¸  # action value function parameter
    Î±  # learning rate
end 

function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    u = maximum(Q(Î¸,sâ€²,aâ€²) for aâ€² in ğ’œ)
    Î” = (r + Î³*u - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
    Î¸[:] += Î±*scale_gradient(Î”, 1)
    return model
end

#=
Î²(s,a) = [s,s^2,a,a^2,1]
Q(Î¸,s,a) = dot(Î¸,Î²(s,a))
âˆ‡Q(Î¸,s,a) = Î²(s,a)
Î¸ = [0.1,0.2,0.3,0.4,0.5] # initial parameter vector
Î± = 0.5 # learning rate
model = GradientQLearning(ğ’«.ğ’œ, ğ’«.Î³, Q, âˆ‡Q, Î¸, Î±)
Ïµ = 0.1 # probability of random action
Î± = 1.0 # exploration decay factor
Ï€ = EpsilonGreedyExploration(Ïµ, Î±)
k = 20 # number of steps to simulate
s = 0.0 # initial state
simulate(ğ’«, model, Ï€, k, s)
=#

struct ReplayGradientQLearning
    ğ’œ     # action space (assumes 1:nactions)
    Î³      # discount
    Q      # parameterized action value funciton Q(Î¸,s,a)
    âˆ‡Q     # gradient of action value function
    Î¸      # action value function parameter
    Î±      # learning rate
    buffer # circular memory buffer
    m      # number of steps between gradient updates
    m_grad # batch size
end

function lookahead(model::ReplayGradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end

function update!(model::ReplayGradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î±
    buffer, m, m_grad = model.buffer, model.m, model.m_grad
    if isfull(buffer)
        U(s) = maximum(Q(Î¸,s,a) for a in ğ’œ)
        âˆ‡Q(s,a,r,sâ€²) = (r + Î³*U(sâ€²) - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
        Î” = mean(âˆ‡Q(s,a,r,sâ€²) for (s,a,r,sâ€²) in rand(buffer, m_grad))
        Î¸[:] += Î±*scale_gradient(Î”, 1)
        for i in 1:m # discard oldest experiences
            popfirst!(buffer)
        end
    else
        push!(buffer, (s,a,r,sâ€²))
    end
    return model
end

#=
capacity = 100 # maximum size of the replay buffer
ExperienceTuple = Tuple{Float64,Float64,Float64,Float64}
M = CircularBuffer{ExperienceTuple}(capacity) # replay buffer
m_grad = 20 # batch size
model = ReplayGradientQLearning(ğ’«.ğ’œ, ğ’«.Î³, Q, âˆ‡Q, Î¸, Î±, M, m, m_grad)
=#

struct BehavioralCloning
    Î±     # step size
    k_max # number of iterations
    âˆ‡logÏ€ # log likelihood gradient
end
function optimize(M::BehavioralCloning, D, Î¸)
    Î±, k_max, âˆ‡logÏ€ = M.Î±, M.k_max, M.âˆ‡logÏ€
    for k in 1:k_max
        âˆ‡ = mean(âˆ‡logÏ€(Î¸, a, s) for (s,a) in D)
        Î¸ += Î±*âˆ‡
    end
    return Î¸
end

struct DatasetAggregation
    ğ’«     # problem with unknown reward function
    bc    # behavioral cloning struct
    k_max # number of iterations
    m     # number of rollouts per iteration
    d     # rollout depth
    b     # initial state distribution
    Ï€E    # expert
    Ï€Î¸    # parameterized policy
end

function optimize(M::DatasetAggregation, D, Î¸)
    ğ’«, bc, k_max, m = M.ğ’«, M.bc, M.k_max, M.m
    d, b, Ï€E, Ï€Î¸ = M.d, M.b, M.Ï€E, M.Ï€Î¸
    Î¸ = optimize(bc, D, Î¸)
    for k in 2:k_max
        for i in 1:m
            s = rand(b)
            for j in 1:d
                push!(D, (s, Ï€E(s)))
                a = rand(Ï€Î¸(Î¸, s))
                s = rand(ğ’«.T(s, a))
            end
        end
        Î¸ = optimize(bc, D, Î¸)
    end
    return Î¸
end


struct SMILe
    ğ’«     # problem with unknown reward
    bc    # Behavioral cloning struct
    k_max # number of iterations
    m     # number of rollouts per iteration
    d     # rollout depth
    b     # initial state distribution
    Î²     # mixing scalar (e.g., d^-3)
    Ï€E    # expert policy
    Ï€Î¸    # parameterized policy
end

function optimize(M::SMILe, Î¸)
    ğ’«, bc, k_max, m = M.ğ’«, M.bc, M.k_max, M.m
    d, b, Î², Ï€E, Ï€Î¸ = M.d, M.b, M.Î², M.Ï€E, M.Ï€Î¸
    ğ’œ, T = ğ’«.ğ’œ, ğ’«.T
    Î¸s = []
    Ï€ = s -> Ï€E(s)
    for k in 1:k_max
        # execute latest Ï€ to get new dataset D
        D = []
        for i in 1:m
            s = rand(b)
            for j in 1:d
                push!(D, (s, Ï€E(s)))
                a = Ï€(s)
                s = rand(T(s, a))
            end
        end
        # train new policy classifier
        Î¸ = optimize(bc, D, Î¸)
        push!(Î¸s, Î¸)
        # compute a new policy mixture
        PÏ€ = Categorical(normalize([(1-Î²)^(i-1) for i in 1:k],1))
        Ï€ = s -> begin
            if rand() < (1-Î²)^(k-1)
                return Ï€E(s)
            else
                return rand(Categorical(Ï€Î¸(Î¸s[rand(PÏ€)], s)))
            end
        end
    end
    Ps = normalize([(1-Î²)^(i-1) for i in 1:k_max],1)
    return Ps, Î¸s
end

struct InverseReinforcementLearning
    ğ’«  # problem
    b  # initial state distribution
    d  # depth
    m  # number of samples
    Ï€  # parameterized policy
    Î²  # binary feature mapping
    Î¼E # expert feature expectations
    RL # reinforcement learning method
    Ïµ  # tolerance
end

function feature_expectations(M::InverseReinforcementLearning, Ï€)
    ğ’«, b, m, d, Î², Î³ = M.ğ’«, M.b, M.m, M.d, M.Î², M.ğ’«.Î³
    Î¼(Ï„) = sum(Î³^(k-1)*Î²(s, a) for (k,(s,a)) in enumerate(Ï„))
    Ï„s = [simulate(ğ’«, rand(b), Ï€, d) for i in 1:m]
    return mean(Î¼(Ï„) for Ï„ in Ï„s)
end

function calc_weighting(M::InverseReinforcementLearning, Î¼s)
    Î¼E = M.Î¼E
    k = length(Î¼E)
    model = Model(Ipopt.Optimizer)
    @variable(model, t)
    @variable(model, Ï•[1:k] â‰¥ 0)
    @objective(model, Max, t)
    for Î¼ in Î¼s
        @constraint(model, Ï•â‹…Î¼E â‰¥ Ï•â‹…Î¼ + t)
    end
    @constraint(model, Ï•â‹…Ï• â‰¤ 1)
    optimize!(model)
    return (value(t), value.(Ï•))
end

function calc_policy_mixture(M::InverseReinforcementLearning, Î¼s)
    Î¼E = M.Î¼E
    k = length(Î¼s)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î»[1:k] â‰¥ 0)
    @objective(model, Min, (Î¼E - sum(Î»[i]*Î¼s[i] for i in 1:k))â‹…
                           (Î¼E - sum(Î»[i]*Î¼s[i] for i in 1:k)))
    @constraint(model, sum(Î») == 1)
    optimize!(model)
    return value.(Î»)
end

function optimize(M::InverseReinforcementLearning, Î¸)
    Ï€, Ïµ, RL = M.Ï€, M.Ïµ, M.RL
    Î¸s = [Î¸]
    Î¼s = [feature_expectations(M, s->Ï€(Î¸,s))]
    while true
        t, Ï• = calc_weighting(M, Î¼s)
        if t â‰¤ Ïµ
            break
        end
        copyto!(RL.Ï•, Ï•) # R(s,a) = Ï•â‹…Î²(s,a)
        Î¸ = optimize(RL, Ï€, Î¸)
        push!(Î¸s, Î¸)
        push!(Î¼s, feature_expectations(M, s->Ï€(Î¸,s)))
    end
    Î» = calc_policy_mixture(M, Î¼s)
    return Î», Î¸s
end


struct MaximumEntropyIRL
    ğ’«     # problem
    b     # initial state distribution
    d     # depth
    Ï€     # parameterized policy Ï€(Î¸,s)
    PÏ€    # parameterized policy likelihood Ï€(Î¸, a, s)
    âˆ‡R    # reward function gradient
    RL    # reinforcement learning method
    Î±     # step size
    k_max # number of iterations
end

function discounted_state_visitations(M::MaximumEntropyIRL, Î¸)
    ğ’«, b, d, PÏ€ = M.ğ’«, M.b, M.d, M.PÏ€
    ğ’®, ğ’œ, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.Î³
    b_sk = zeros(length(ğ’«.ğ’®), d)
    b_sk[:,1] = [pdf(b, s) for s in ğ’®]
    for k in 2:d
        for (siâ€², sâ€²) in enumerate(ğ’®)
            b_sk[siâ€²,k] = Î³*sum(
                sum(b_sk[si,k-1]*PÏ€(Î¸, a, s)*T(s, a, sâ€²)
                for (si,s) in enumerate(ğ’®))
                    for a in ğ’œ)
        end
    end
    return normalize!(vec(mean(b_sk, dims=2)),1)
end
function optimize(M::MaximumEntropyIRL, D, Ï•, Î¸)
    ğ’«, Ï€, PÏ€, âˆ‡R, RL, Î±, k_max = M.ğ’«, M.Ï€, M.PÏ€, M.âˆ‡R, M.RL, M.Î±, M.k_max
    ğ’®, ğ’œ, Î³, nD = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³, length(D)
    for k in 1:k_max
        copyto!(RL.Ï•, Ï•) # update parameters
        Î¸ = optimize(RL, Ï€, Î¸)
        b = discounted_state_visitations(M, Î¸)
        âˆ‡RÏ„ = Ï„ -> sum(Î³^(i-1)*âˆ‡R(Ï•,s,a) for (i,(s,a)) in enumerate(Ï„))
        âˆ‡f = sum(âˆ‡RÏ„(Ï„) for Ï„ in D) - nD*sum(
            b[si]*sum(PÏ€(Î¸,a,s)*âˆ‡R(Ï•,s,a)
                for (ai,a) in enumerate(ğ’œ))
            for (si, s) in enumerate(ğ’®))
        Ï• += Î±*âˆ‡f
    end
    return Ï•, Î¸
end

struct POMDP
    Î³   # discount factor
    ğ’®   # state space
    ğ’œ   # action space
    ğ’ª   # observation space
    T   # transition function
    R   # reward function
    O   # observation function
    TRO # sample transition, reward, and observation
end

function update(b::Vector{Float64}, ğ’«, a, o)
    ğ’®, T, O = ğ’«.ğ’®, ğ’«.T, ğ’«.O
    bâ€² = similar(b)
    for (iâ€², sâ€²) in enumerate(ğ’®)
        po = O(a, sâ€², o)
        bâ€²[iâ€²] = po * sum(T(s, a, sâ€²) * b[i] for (i, s) in enumerate(ğ’®))
    end
    if sum(bâ€²) â‰ˆ 0.0
        fill!(bâ€², 1)
    end
    return normalize!(bâ€², 1)
end

struct KalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
end

function update(b::KalmanFilter, ğ’«, a, o)
    Î¼b, Î£b = b.Î¼b, b.Î£b
    Ts, Ta, Os = ğ’«.Ts, ğ’«.Ta, ğ’«.Os
    Î£s, Î£o = ğ’«.Î£s, ğ’«.Î£o
    # predict
    Î¼p = Ts*Î¼b + Ta*a
    Î£p = Ts*Î£b*Ts' + Î£s
    # update
    K = Î£p*Os'/(Os*Î£p*Os' + Î£o)
    Î¼bâ€² = Î¼p + K*(o - Os*Î¼p)
    Î£bâ€² = (I - K*Os)*Î£p
    return KalmanFilter(Î¼bâ€², Î£bâ€²)
end


struct ExtendedKalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
end

import ForwardDiff: jacobian
function update(b::ExtendedKalmanFilter, ğ’«, a, o)
    Î¼b, Î£b = b.Î¼b, b.Î£b
    fT, fO = ğ’«.fT, ğ’«.fO
    Î£s, Î£o = ğ’«.Î£s, ğ’«.Î£o
    # predict
    Î¼p = fT(Î¼b, a)
    Ts = jacobian(s->fT(s, a), Î¼b)
    Os = jacobian(fO, Î¼p)
    Î£p = Ts*Î£b*Ts' + Î£s
    # update
    K = Î£p*Os'/(Os*Î£p*Os' + Î£o)
    Î¼bâ€² = Î¼p + K*(o - fO(Î¼p))
    Î£bâ€² = (I - K*Os)*Î£p
    return ExtendedKalmanFilter(Î¼bâ€², Î£bâ€²)
end


struct UnscentedKalmanFilter
    Î¼b # mean vector
    Î£b # covariance matrix
    Î»  # spread parameter
end

function unscented_transform(Î¼, Î£, f, Î», ws)
    n = length(Î¼)
    Î” = cholesky((n + Î») * Î£).L
    S = [Î¼]
    for i in 1:n
        push!(S, Î¼ + Î”[:,i])
        push!(S, Î¼ - Î”[:,i])
    end
    Sâ€² = f.(S)
    Î¼â€² = sum(w*s for (w,s) in zip(ws, Sâ€²))
    Î£â€² = sum(w*(s - Î¼â€²)*(s - Î¼â€²)' for (w,s) in zip(ws, Sâ€²))
    return (Î¼â€², Î£â€², S, Sâ€²)
end

function update(b::UnscentedKalmanFilter, ğ’«, a, o)
    Î¼b, Î£b, Î» = b.Î¼b, b.Î£b, b.Î»
    fT, fO = ğ’«.fT, ğ’«.fO
    n = length(Î¼b)
    ws = [Î» / (n + Î»); fill(1/(2(n + Î»)), 2n)]
    # predict
    Î¼p, Î£p, Sp, Spâ€² = unscented_transform(Î¼b, Î£b, s->fT(s,a), Î», ws)
    Î£p += ğ’«.Î£s
    # update
    Î¼o, Î£o, So, Soâ€² = unscented_transform(Î¼p, Î£p, fO, Î», ws)
    Î£o += ğ’«.Î£o
    Î£po = sum(w*(s - Î¼p)*(sâ€² - Î¼o)' for (w,s,sâ€²) in zip(ws, So, Soâ€²))
    K = Î£po / Î£o
    Î¼bâ€² = Î¼p + K*(o - Î¼o)
    Î£bâ€² = Î£p - K*Î£o*K'
    return UnscentedKalmanFilter(Î¼bâ€², Î£bâ€², Î»)
end

struct ParticleFilter
    states # vector of state samples
end
function update(b::ParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    D = SetCategorical(states, weights)
    return ParticleFilter(rand(D, length(states)))
end

struct RejectionParticleFilter
    states # vector of state samples
end
function update(b::RejectionParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    states = similar(b.states)
    i = 1
    while i â‰¤ length(states)
        s = rand(b.states)
        sâ€² = rand(T(s,a))
        if rand(O(a,sâ€²)) == o
            states[i] = sâ€²
            i += 1
        end
    end
    return RejectionParticleFilter(states)
end

struct InjectionParticleFilter
    states   # vector of state samples
    m_inject # number of samples to inject
    D_inject # injection distribution
end

function update(b::InjectionParticleFilter, ğ’«, a, o)
    T, O, m_inject, D_inject = ğ’«.T, ğ’«.O, b.m_inject, b.D_inject
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    D = SetCategorical(states, weights)
    m = length(states)
    states = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))
    return InjectionParticleFilter(states, m_inject, D_inject)
end

mutable struct AdaptiveInjectionParticleFilter
    states   # vector of state samples
    w_slow   # slow moving average
    w_fast   # fast moving average
    Î±_slow   # slow moving average parameter
    Î±_fast   # fast moving average parameter
    Î½        # injection parameter
    D_inject # injection distribution
end

function update(b::AdaptiveInjectionParticleFilter, ğ’«, a, o)
    T, O = ğ’«.T, ğ’«.O
    w_slow, w_fast, Î±_slow, Î±_fast, Î½, D_inject =
        b.w_slow, b.w_fast, b.Î±_slow, b.Î±_fast, b.Î½, b.D_inject
    states = [rand(T(s, a)) for s in b.states]
    weights = [O(a, sâ€², o) for sâ€² in states]
    w_mean = mean(weights)
    w_slow += Î±_slow*(w_mean - w_slow)
    w_fast += Î±_fast*(w_mean - w_fast)
    m = length(states)
    m_inject = round(Int, m * max(0, 1.0 - Î½*w_fast / w_slow))
    D = SetCategorical(states, weights) # ACHTUNG not found
    states = vcat(rand(D, m - m_inject), rand(D_inject, m_inject))
    b.w_slow, b.w_fast = w_slow, w_fast
    return AdaptiveInjectionParticleFilter(states,
        w_slow, w_fast, Î±_slow, Î±_fast, Î½, D_inject)
end

struct ConditionalPlan
    a        # action to take at root
    subplans # dictionary mapping observations to subplans
end
ConditionalPlan(a) = ConditionalPlan(a, Dict())
(Ï€::ConditionalPlan)() = Ï€.a
(Ï€::ConditionalPlan)(o) = Ï€.subplans[o]


function lookahead(ğ’«::POMDP, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s,a) + Î³*uâ€²
end
function evaluate_plan(ğ’«::POMDP, Ï€::ConditionalPlan, s)
    U(o,sâ€²) = evaluate_plan(ğ’«, Ï€(o), sâ€²)
    return isempty(Ï€.subplans) ? ğ’«.R(s,Ï€()) : lookahead(ğ’«, U, s, Ï€())
end

function alphavector(ğ’«::POMDP, Ï€::ConditionalPlan)
    return [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
end


struct AlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
    a # actions associated with alpha vectors
end
function utility(Ï€::AlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end
function (Ï€::AlphaVectorPolicy)(b)
    i = argmax([Î±â‹…b for Î± in Ï€.Î“])
    return Ï€.a[i]
end


function lookahead(ğ’«::POMDP, U, b::Vector, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    r = sum(R(s,a)*b[i] for (i,s) in enumerate(ğ’®))
    Posa(o,s,a) = sum(O(a,sâ€²,o)*T(s,a,sâ€²) for sâ€² in ğ’®)
    Poba(o,b,a) = sum(b[i]*Posa(o,s,a) for (i,s) in enumerate(ğ’®))
    return r + Î³*sum(Poba(o,b,a)*U(update(b, ğ’«, a, o)) for o in ğ’ª)
end

function greedy(ğ’«::POMDP, U, b::Vector)
    u, a = findmax(a->lookahead(ğ’«, U, b, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end
struct LookaheadAlphaVectorPolicy
    ğ’« # POMDP problem
    Î“ # alpha vectors
end
function utility(Ï€::LookaheadAlphaVectorPolicy, b)
    return maximum(Î±â‹…b for Î± in Ï€.Î“)
end
function greedy(Ï€, b)
    U(b) = utility(Ï€, b)
    return greedy(Ï€.ğ’«, U, b)
end
(Ï€::LookaheadAlphaVectorPolicy)(b) = greedy(Ï€, b).a


function find_maximal_belief(Î±, Î“)
    m = length(Î±)
    if isempty(Î“)
        return fill(1/m, m) # arbitrary belief
    end
    model = Model(GLPK.Optimizer)
    @variable(model, Î´)
    @variable(model, b[i=1:m] â‰¥ 0)
    @constraint(model, sum(b) == 1.0)
    for a in Î“
        @constraint(model, (Î±-a)â‹…b â‰¥ Î´)
    end
    @objective(model, Max, Î´)
    optimize!(model)
    return value(Î´) > 0 ? value.(b) : nothing
end


function find_dominating(Î“)
    n = length(Î“)
    candidates, dominating = trues(n), falses(n)
    while any(candidates)
        i = findfirst(candidates)
        b = find_maximal_belief(Î“[i], Î“[dominating])
        if b === nothing
            candidates[i] = false
        else
            k = argmax([candidates[j] ? bâ‹…Î“[j] : -Inf for j in 1:n])
            candidates[k], dominating[k] = false, true
        end
    end
    return dominating
end
function prune(plans, Î“)
    d = find_dominating(Î“)
    return (plans[d], Î“[d])
end


function value_iteration(ğ’«::POMDP, k_max)
    ğ’®, ğ’œ, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    plans = [ConditionalPlan(a) for a in ğ’œ]
    Î“ = [[R(s,a) for s in ğ’®] for a in ğ’œ]
    plans, Î“ = prune(plans, Î“)
    for k in 2:k_max
        plans, Î“ = expand(plans, Î“, ğ’«)
        plans, Î“ = prune(plans, Î“)
    end
    return (plans, Î“)
end

function solve(M::ValueIteration, ğ’«::POMDP)
    plans, Î“ = value_iteration(ğ’«, M.k_max)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end


function ConditionalPlan(ğ’«::POMDP, a, plans)
    subplans = Dict(o=>Ï€ for (o, Ï€) in zip(ğ’«.ğ’ª, plans))
    return ConditionalPlan(a, subplans)
end
function combine_lookahead(ğ’«::POMDP, s, a, Î“o)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    Uâ€²(sâ€²,i) = sum(O(a,sâ€²,o)*Î±[i] for (o,Î±) in zip(ğ’ª,Î“o))
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*Uâ€²(sâ€²,i) for (i,sâ€²) in enumerate(ğ’®))
end
function combine_alphavector(ğ’«::POMDP, a, Î“o)
    return [combine_lookahead(ğ’«, s, a, Î“o) for s in ğ’«.ğ’®]
end
function expand(plans, Î“, ğ’«)
    ğ’®, ğ’œ, ğ’ª, T, O, R = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R
    plansâ€², Î“â€² = [], []
    for a in ğ’œ
        # iterate over all possible mappings from observations to plans
        for inds in product([eachindex(plans) for o in ğ’ª]...)
            Ï€o = plans[[inds...]]
            Î“o = Î“[[inds...]]
            Ï€ = ConditionalPlan(ğ’«, a, Ï€o)
            Î± = combine_alphavector(ğ’«, a, Î“o)
            push!(plansâ€², Ï€)
            push!(Î“â€², Î±)
        end
    end
    return (plansâ€², Î“â€²)
end


# Offline Belief State Planning

function alphavector_iteration(ğ’«::POMDP, M, Î“)
    for k in 1:M.k_max
        Î“ = update(ğ’«, M, Î“)
    end
    return Î“
end

struct QMDP
    k_max # maximum number of iterations
end
function update(ğ’«::POMDP, M::QMDP, Î“)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Î“â€² = [[R(s,a) + Î³*sum(T(s,a,sâ€²)*maximum(Î±â€²[j] for Î±â€² in Î“)
        for (j,sâ€²) in enumerate(ğ’®)) for s in ğ’®] for a in ğ’œ]
    return Î“â€²
end
function solve(M::QMDP, ğ’«::POMDP)
    Î“ = [zeros(length(ğ’«.ğ’®)) for a in ğ’«.ğ’œ]
    Î“ = alphavector_iteration(ğ’«, M, Î“)
    return AlphaVectorPolicy(ğ’«, Î“, ğ’«.ğ’œ)
end


struct FastInformedBound
    k_max # maximum number of iterations
end
function update(ğ’«::POMDP, M::FastInformedBound, Î“)
    ğ’®, ğ’œ, ğ’ª, R, T, O, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.R, ğ’«.T, ğ’«.O, ğ’«.Î³
    Î“â€² = [[R(s, a) + Î³*sum(maximum(sum(O(a,sâ€²,o)*T(s,a,sâ€²)*Î±â€²[j]
        for (j,sâ€²) in enumerate(ğ’®)) for Î±â€² in Î“) for o in ğ’ª)
        for s in ğ’®] for a in ğ’œ]
    return Î“â€²
end
function solve(M::FastInformedBound, ğ’«::POMDP)
    Î“ = [zeros(length(ğ’«.ğ’®)) for a in ğ’«.ğ’œ]
    Î“ = alphavector_iteration(ğ’«, M, Î“)
    return AlphaVectorPolicy(ğ’«, Î“, ğ’«.ğ’œ)
end

function baws_lowerbound(ğ’«::POMDP)
    ğ’®, ğ’œ, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³
    r = maximum(minimum(R(s, a) for s in ğ’®) for a in ğ’œ) / (1-Î³)
    Î± = fill(r, length(ğ’®))
    return Î±
end

function blind_lowerbound(ğ’«, k_max)
    ğ’®, ğ’œ, T, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    Q(s,a,Î±) = R(s,a) + Î³*sum(T(s,a,sâ€²)*Î±[j] for (j,sâ€²) in enumerate(ğ’®))
    Î“ = [baws_lowerbound(ğ’«) for a in ğ’œ]
    for k in 1:k_max
        Î“ = [[Q(s,a,Î±) for s in ğ’®] for (Î±,a) in zip(Î“, ğ’œ)]
    end
    return Î“
end

function backup(ğ’«::POMDP, Î“, b)
    ğ’®, ğ’œ, ğ’ª, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.Î³
    R, T, O = ğ’«.R, ğ’«.T, ğ’«.O
    Î“a = []
    for a in ğ’œ
        Î“ao = []
        for o in ğ’ª
            bâ€² = update(b, ğ’«, a, o)
            push!(Î“ao, argmax(Î±->Î±â‹…bâ€², Î“))
        end
        Î± = [R(s, a) + Î³*sum(sum(T(s, a, sâ€²)*O(a, sâ€², o)*Î“ao[i][j]
            for (j,sâ€²) in enumerate(ğ’®)) for (i,o) in enumerate(ğ’ª))
            for s in ğ’®]
        push!(Î“a, Î±)
    end
    return argmax(Î±->Î±â‹…b, Î“a)
end

struct PointBasedValueIteration
    B     # set of belief points
    k_max # maximum number of iterations
end
function update(ğ’«::POMDP, M::PointBasedValueIteration, Î“)
    return [backup(ğ’«, Î“, b) for b in M.B]
end
function solve(M::PointBasedValueIteration, ğ’«)
    Î“ = fill(baws_lowerbound(ğ’«), length(ğ’«.ğ’œ))
    Î“ = alphavector_iteration(ğ’«, M, Î“)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end


struct RandomizedPointBasedValueIteration
    B     # set of belief points
    k_max # maximum number of iterations
end
function update(ğ’«::POMDP, M::RandomizedPointBasedValueIteration, Î“)
    Î“â€², Bâ€² = [], copy(M.B)
    while !isempty(Bâ€²)
        b = rand(Bâ€²)
        Î± = argmax(Î±->Î±â‹…b, Î“)
        Î±â€² = backup(ğ’«, Î“, b)
        if Î±â€²â‹…b â‰¥ Î±â‹…b
            push!(Î“â€², Î±â€²)
        else
            push!(Î“â€², Î±)
        end
        filter!(b->maximum(Î±â‹…b for Î± in Î“â€²) <
            maximum(Î±â‹…b for Î± in Î“), Bâ€²)
    end
    return Î“â€²
end
function solve(M::RandomizedPointBasedValueIteration, ğ’«)
    Î“ = [baws_lowerbound(ğ’«)]
    Î“ = alphavector_iteration(ğ’«, M, Î“)
    return LookaheadAlphaVectorPolicy(ğ’«, Î“)
end


struct SawtoothPolicy
    ğ’« # POMDP problem
    V # dictionary mapping beliefs to utilities
end
function basis(ğ’«)
    n = length(ğ’«.ğ’®)
    e(i) = [j == i ? 1.0 : 0.0 for j in 1:n]
    return [e(i) for i in 1:n]
end
function utility(Ï€::SawtoothPolicy, b)
    ğ’«, V = Ï€.ğ’«, Ï€.V
    if haskey(V, b)
        return V[b]
    end
    n = length(ğ’«.ğ’®)
    E = basis(ğ’«)
    u = sum(V[E[i]] * b[i] for i in 1:n)
    for (bâ€², uâ€²) in V
        if bâ€² âˆ‰ E
            i = argmax([norm(b-e, 1) - norm(bâ€²-e, 1) for e in E])
            w = [norm(b - e, 1) for e in E]
            w[i] = norm(b - bâ€², 1)
            w /= sum(w)
            w = [1 - wi for wi in w]
            Î± = [V[e] for e in E]
            Î±[i] = uâ€²
            u = min(u, wâ‹…Î±)
        end
    end
    return u
end
(Ï€::SawtoothPolicy)(b) = greedy(Ï€, b).a


struct SawtoothIteration
    V     # initial mapping from beliefs to utilities
    B     # beliefs to compute values including those in V map
    k_max # maximum number of iterations
end
function solve(M::SawtoothIteration, ğ’«::POMDP)
    E = basis(ğ’«)
    Ï€ = SawtoothPolicy(ğ’«, M.V)
    for k in 1:M.k_max
        V = Dict(b => (b âˆˆ E ? M.V[b] : greedy(Ï€, b).u) for b in M.B)
        Ï€ = SawtoothPolicy(ğ’«, V)
    end
    return Ï€
end

#=
n = length(ğ’«.ğ’®)
Ï€fib = solve(FastInformedBound(1), ğ’«)
V = Dict(e => utility(Ï€fib, e) for e in basis(ğ’«))
B = [[p, 1 - p] for p in 0.0:0.2:1.0]
Ï€ = solve(SawtoothIteration(V, B, 2), ğ’«)
=#

function randstep(ğ’«::POMDP, b, a)
    s = rand(SetCategorical(ğ’«.ğ’®, b))
    sâ€², r, o = ğ’«.TRO(s, a)
    bâ€² = update(b, ğ’«, a, o)
    return bâ€², r
end

function random_belief_expansion(ğ’«, B)
    Bâ€² = copy(B)
    for b in B
        a = rand(ğ’«.ğ’œ)
        bâ€², r = randstep(ğ’«, b, a)
        push!(Bâ€², bâ€²)
    end
    return unique!(Bâ€²)
end

function exploratory_belief_expansion(ğ’«, B)
    Bâ€² = copy(B)
    for b in B
        best = (b=copy(b), d=0.0)
        for a in ğ’«.ğ’œ
            bâ€², r = randstep(ğ’«, b, a)
            d = minimum(norm(b - bâ€², 1) for b in Bâ€²)
            if d > best.d
                best = (b=bâ€², d=d)
            end
        end
        push!(Bâ€², best.b)
    end
    return unique!(Bâ€²)
end


struct SawtoothHeuristicSearch
    b     # initial belief
    Î´     # gap threshold
    d     # depth
    k_max # maximum number of iterations
    k_fib # number of iterations for fast informed bound
end
function explore!(M::SawtoothHeuristicSearch, ğ’«, Ï€hi, Ï€lo, b, d=0)
    ğ’®, ğ’œ, ğ’ª, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.Î³
    Ïµ(bâ€²) = utility(Ï€hi, bâ€²) - utility(Ï€lo, bâ€²)
    if d â‰¥ M.d || Ïµ(b) â‰¤ M.Î´ / Î³^d
        return
    end
    a = Ï€hi(b)
    o = argmax(o -> Ïµ(update(b, ğ’«, a, o)), ğ’ª)
    bâ€² = update(b, ğ’«, a, o)
    explore!(M, ğ’«, Ï€hi, Ï€lo, bâ€², d+1)
    if bâ€² âˆ‰ basis(ğ’«)
        Ï€hi.V[bâ€²] = greedy(Ï€hi, bâ€²).u
    end
    push!(Ï€lo.Î“, backup(ğ’«, Ï€lo.Î“, bâ€²))
end
function solve(M::SawtoothHeuristicSearch, ğ’«::POMDP)
    Ï€fib = solve(FastInformedBound(M.k_fib), ğ’«)
    Vhi = Dict(e => utility(Ï€fib, e) for e in basis(ğ’«))
    Ï€hi = SawtoothPolicy(ğ’«, Vhi)
    Ï€lo = LookaheadAlphaVectorPolicy(ğ’«, [baws_lowerbound(ğ’«)])
    for i in 1:M.k_max
        explore!(M, ğ’«, Ï€hi, Ï€lo, M.b)
        if utility(Ï€hi, M.b) - utility(Ï€lo, M.b) < M.Î´
            break
        end
    end
    return Ï€lo
end


struct TriangulatedPolicy
    ğ’« # POMDP problem
    V # dictionary mapping beliefs to utilities
    B # beliefs
    T # Freudenthal triangulation
end
function TriangulatedPolicy(ğ’«::POMDP, m)
    T = FreudenthalTriangulation(length(ğ’«.ğ’®), m)
    B = belief_vertices(T)
    V = Dict(b => 0.0 for b in B)
    return TriangulatedPolicy(ğ’«, V, B, T)
end
function utility(Ï€::TriangulatedPolicy, b)
    B, Î» = belief_simplex(Ï€.T, b)
    return sum(Î»i*Ï€.V[b] for (Î»i, b) in zip(Î», B))
end
(Ï€::TriangulatedPolicy)(b) = greedy(Ï€, b).a


struct TriangulatedIteration
    m # granulatiry
    k_max # maximum number of iterations
end
function solve(M::TriangulatedIteration, ğ’«)
    Ï€ = TriangulatedPolicy(ğ’«, M.m)
    U(b) = utility(Ï€, b)
    for k in 1:M.k_max
        Uâ€² = [greedy(ğ’«, U, b).u for b in Ï€.B]
        for (b, uâ€²) in zip(Ï€.B, Uâ€²)
            Ï€.V[b] = uâ€²
        end
    end
    return Ï€
end


#  Online Belief State Planning

#=
k_max = 10 # maximum number of iterations of QMDP
Ï€QMDP = solve(QMDP(k_max), ğ’«)
d = 5 # depth
U(b) = utility(Ï€QMDP, b)
Ï€ = ForwardSearch(ğ’«, d, U)
Ï€([0.5,0.2,0.3])

k_max = 10 # maximum number of iterations for bounds
Ï€FIB = solve(FastInformedBound(k_max), ğ’«)
d = 5 # depth
Ulo(b) = utility(Ï€FIB, b)
B = [[p, 1 - p] for p in 0.0:0.2:1.0]
Ï€PBVI = solve(PointBasedValueIteration(B, k_max), ğ’«)
Uhi(b) = utility(Ï€PBVI, b)
Qhi(b,a) = lookahead(ğ’«, Uhi, b, a)
Ï€ = BranchAndBound(ğ’«, d, Ulo, Qhi)
Ï€([0.4,0.6])
=#

struct HistoryMonteCarloTreeSearch
    ğ’« # problem
    N # visit counts
    Q # action value estimates
    d # depth
    m # number of simulations
    c # exploration constant
    U # value function estimate
end
function explore(Ï€::HistoryMonteCarloTreeSearch, h)
    ğ’œ, N, Q, c = Ï€.ğ’«.ğ’œ, Ï€.N, Ï€.Q, Ï€.c
    Nh = sum(get(N, (h,a), 0) for a in ğ’œ)
    return argmax(a->Q[(h,a)] + c*bonus(N[(h,a)], Nh), ğ’œ)
end
function simulate(Ï€::HistoryMonteCarloTreeSearch, s, h, d)
    if d â‰¤ 0
        return Ï€.U(s)
    end
    ğ’«, N, Q, c = Ï€.ğ’«, Ï€.N, Ï€.Q, Ï€.c
    ğ’®, ğ’œ, TRO, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.TRO, ğ’«.Î³
    if !haskey(N, (h, first(ğ’œ)))
        for a in ğ’œ
            N[(h,a)] = 0
            Q[(h,a)] = 0.0
        end
        return Ï€.U(s)
    end
    a = explore(Ï€, h)
    sâ€², r, o = TRO(s,a)
    q = r + Î³*simulate(Ï€, sâ€², vcat(h, (a,o)), d-1)
    N[(h,a)] += 1
    Q[(h,a)] += (q-Q[(h,a)])/N[(h,a)]
    return q
end
function (Ï€::HistoryMonteCarloTreeSearch)(b, h=[])
    for i in 1:Ï€.m
        s = rand(SetCategorical(Ï€.ğ’«.ğ’®, b))
        simulate(Ï€, s, h, Ï€.d)
    end
    return argmax(a->Ï€.Q[(h,a)], Ï€.ğ’«.ğ’œ)
end


struct DeterminizedParticle
    s # state
    i # scenario index
    j # depth index
end
function successor(ğ’«, Î¦, Ï•, a)
    ğ’®, ğ’ª, T, O = ğ’«.ğ’®, ğ’«.ğ’ª, ğ’«.T, ğ’«.O
    p = 0.0
    for (sâ€², o) in product(ğ’®, ğ’ª)
        p += T(Ï•.s, a, sâ€²) * O(a, sâ€², o)
        if p â‰¥ Î¦[Ï•.i, Ï•.j]
            return (sâ€², o)
        end
    end
    return last(ğ’®), last(ğ’ª)
end
function possible_observations(ğ’«, Î¦, b, a)
    ğ’ª = []
    for Ï• in b
        sâ€², o = successor(ğ’«, Î¦, Ï•, a)
        push!(ğ’ª, o)
    end
    return unique(ğ’ª)
end
function update(b, Î¦, ğ’«, a, o)
    bâ€² = []
    for Ï• in b
        sâ€², oâ€² = successor(ğ’«, Î¦, Ï•, a)
        if o == oâ€²
            push!(bâ€², DeterminizedParticle(sâ€², Ï•.i, Ï•.j + 1))
        end
    end
    return bâ€²
end


struct DeterminizedSparseTreeSearch
    ğ’« # problem
    d # depth
    Î¦ # mÃ—d determinizing matrix
    U # value function to use at leaf nodes
end
function determinized_sparse_tree_search(ğ’«, b, d, Î¦, U)
    ğ’®, ğ’œ, ğ’ª, T, R, O, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.R, ğ’«.O, ğ’«.Î³
    if d == 0
        return (a=nothing, u=U(b))
    end
    best = (a=nothing, u=-Inf)
    for a in ğ’œ
    u = sum(R(Ï•.s, a) for Ï• in b) / length(b)
    for o in possible_observations(ğ’«, Î¦, b, a)
        Poba = sum(sum(O(a,sâ€²,o)*T(Ï•.s,a,sâ€²) for sâ€² in ğ’®)
            for Ï• in b) / length(b)
        bâ€² = update(b, Î¦, ğ’«, a, o)
        uâ€² = determinized_sparse_tree_search(ğ’«,bâ€²,d-1,Î¦,U).u
        u += Î³*Poba*uâ€²
    end
    if u > best.u
        best = (a=a, u=u)
    end
    end
    return best
end
function determized_approximate_belief(b, ğ’«, m)
    particles = []
    for i in 1:m
        s = rand(SetCategorical(ğ’«.ğ’®, b))
        push!(particles, DeterminizedParticle(s, i, 1))
    end
    return particles
end
function (Ï€::DeterminizedSparseTreeSearch)(b)
    particles = determized_approximate_belief(b, Ï€.ğ’«, size(Ï€.Î¦,1))
    return determinized_sparse_tree_search(Ï€.ğ’«,particles,Ï€.d,Ï€.Î¦,Ï€.U).a
end


struct GapHeuristicSearch
    ğ’«     # problem
    Uhi   # upper bound on value function
    Ulo   # lower bound on value function
    Ï€     # rollout policy
    Î´     # gap threshold
    k_max # maximum number of simulations
    d_max # maximum depth
end
function heuristic_search(Ï€::GapHeuristicSearch, b, d)
    ğ’«, Uhi, Ulo, Î´ = Ï€.ğ’«, Ï€.Uhi, Ï€.Ulo, Ï€.Î´
    ğ’®, ğ’œ, ğ’ª, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.R, ğ’«.Î³
    B = Dict((a,o)=>update(b,ğ’«,a,o) for (a,o) in product(ğ’œ,ğ’ª))
    B = merge(B, Dict(()=>copy(b)))
    Rmax = maximum(R(s,a) for (s,a) in product(ğ’®,ğ’œ))
    for (ao, bâ€²) in B
        if !haskey(Uhi, bâ€²)
            Uhi[bâ€²], Ulo[bâ€²] = Rmax/(1.0-Î³), rollout(ğ’«,bâ€²,Ï€.Ï€,d)
        end
    end
    if d == 0 || Uhi[b] - Ulo[b] â‰¤ Î´
        return
    end
    a = argmax(a -> lookahead(ğ’«,bâ€²->Uhi[bâ€²],b,a), ğ’œ)
    o = argmax(o -> Uhi[B[(a, o)]] - Ulo[B[(a, o)]], ğ’ª)
    bâ€² = update(b,ğ’«,a,o)
    heuristic_search(Ï€,bâ€²,d-1)
    Uhi[b] = maximum(lookahead(ğ’«,bâ€²->Uhi[bâ€²],b,a) for a in ğ’œ)
    Ulo[b] = maximum(lookahead(ğ’«,bâ€²->Ulo[bâ€²],b,a) for a in ğ’œ)
end
function (Ï€::GapHeuristicSearch)(b)
    Uhi, Ulo, k_max, d_max, Î´ = Ï€.Uhi, Ï€.Ulo, Ï€.k_max, Ï€.d_max, M.Î´
    for i in 1:k_max
        heuristic_search(Ï€, b, d_max)
    if Uhi[b] - Ulo[b] < Î´
        break
    end
    end
    return argmax(a -> lookahead(ğ’«,bâ€²->Ulo[bâ€²],b,a), ğ’«.ğ’œ)
end


# Controller Abstractions

mutable struct ControllerPolicy
    ğ’« # problem
    X # set of controller nodes
    Ïˆ # action selection distribution
    Î· # successor selection distribution
end
function (Ï€::ControllerPolicy)(x)
    ğ’œ, Ïˆ = Ï€.ğ’«.ğ’œ, Ï€.Ïˆ
    dist = [Ïˆ[x, a] for a in ğ’œ]
    return rand(SetCategorical(ğ’œ, dist))
end
function update(Ï€::ControllerPolicy, x, a, o)
    X, Î· = Ï€.X, Ï€.Î·
    dist = [Î·[x, a, o, xâ€²] for xâ€² in X]
    return rand(SetCategorical(X, dist))
end


function utility(Ï€::ControllerPolicy, U, x, s)
    ğ’®, ğ’œ, ğ’ª = Ï€.ğ’«.ğ’®, Ï€.ğ’«.ğ’œ, Ï€.ğ’«.ğ’ª
    T, O, R, Î³ = Ï€.ğ’«.T, Ï€.ğ’«.O, Ï€.ğ’«.R, Ï€.ğ’«.Î³
    X, Ïˆ, Î· = Ï€.X, Ï€.Ïˆ, Ï€.Î·
    Uâ€²(a,sâ€²,o) = sum(Î·[x,a,o,xâ€²]*U[xâ€²,sâ€²] for xâ€² in X)
    Uâ€²(a,sâ€²) = T(s,a,sâ€²)*sum(O(a,sâ€²,o)*Uâ€²(a,sâ€²,o) for o in ğ’ª)
    Uâ€²(a) = R(s,a) + Î³*sum(Uâ€²(a,sâ€²) for sâ€² in ğ’®)
    return sum(Ïˆ[x,a]*Uâ€²(a) for a in ğ’œ)
end
function iterative_policy_evaluation(Ï€::ControllerPolicy, k_max)
    ğ’®, X = Ï€.ğ’«.ğ’®, Ï€.X
    U = Dict((x, s) => 0.0 for x in X, s in ğ’®)
    for k in 1:k_max
        U = Dict((x, s) => utility(Ï€, U, x, s) for x in X, s in ğ’®)
    end
    return U
end


struct ControllerPolicyIteration
    k_max # number of iterations
    eval_max # number of evaluation iterations
end
function solve(M::ControllerPolicyIteration, ğ’«::POMDP)
    ğ’œ, ğ’ª, k_max, eval_max = ğ’«.ğ’œ, ğ’«.ğ’ª, M.k_max, M.eval_max
    X = [1]
    Ïˆ = Dict((x, a) => 1.0 / length(ğ’œ) for x in X, a in ğ’œ)
    Î· = Dict((x, a, o, xâ€²) => 1.0 for x in X, a in ğ’œ, o in ğ’ª, xâ€² in X)
    Ï€ = ControllerPolicy(ğ’«, X, Ïˆ, Î·)
    for i in 1:k_max
    prevX = copy(Ï€.X)
    U = iterative_policy_evaluation(Ï€, eval_max)
    policy_improvement!(Ï€, U, prevX)
    prune!(Ï€, U, prevX)
    end
    return Ï€
end
function policy_improvement!(Ï€::ControllerPolicy, U, prevX)
    ğ’®, ğ’œ, ğ’ª = Ï€.ğ’«.ğ’®, Ï€.ğ’«.ğ’œ, Ï€.ğ’«.ğ’ª
    X, Ïˆ, Î· = Ï€.X, Ï€.Ïˆ, Ï€.Î·
    repeatXğ’ª = fill(X, length(ğ’ª))
    assignğ’œXâ€² = vec(collect(product(ğ’œ, repeatXğ’ª...)))
    for axâ€² in assignğ’œXâ€²
        x, a = maximum(X) + 1, axâ€²[1]
        push!(X, x)
        successor(o) = axâ€²[findfirst(isequal(o), ğ’ª) + 1]
        Uâ€²(o,sâ€²) = U[successor(o), sâ€²]
        for s in ğ’®
            U[x, s] = lookahead(Ï€.ğ’«, Uâ€², s, a)
        end
        for aâ€² in ğ’œ
            Ïˆ[x, aâ€²] = aâ€² == a ? 1.0 : 0.0
            for (o, xâ€²) in product(ğ’ª, prevX)
                Î·[x, aâ€², o, xâ€²] = xâ€² == successor(o) ? 1.0 : 0.0
            end
        end
    end
    for (x, a, o, xâ€²) in product(X, ğ’œ, ğ’ª, X)
        if !haskey(Î·, (x, a, o, xâ€²))
            Î·[x, a, o, xâ€²] = 0.0
        end
    end
end


function prune!(Ï€::ControllerPolicy, U, prevX)
    ğ’®, ğ’œ, ğ’ª, X, Ïˆ, Î· = Ï€.ğ’«.ğ’®, Ï€.ğ’«.ğ’œ, Ï€.ğ’«.ğ’ª, Ï€.X, Ï€.Ïˆ, Ï€.Î·
    newX, removeX = setdiff(X, prevX), []
    # prune dominated from previous nodes
    dominated(x,xâ€²) = all(U[x,s] â‰¤ U[xâ€²,s] for s in ğ’®)
    for (x,xâ€²) in product(prevX, newX)
        if xâ€² âˆ‰ removeX && dominated(x, xâ€²)
            for s in ğ’®
                U[x,s] = U[xâ€²,s]
            end
            for a in ğ’œ
                Ïˆ[x,a] = Ïˆ[xâ€²,a]
                for (o,xâ€²â€²) in product(ğ’ª, X)
                    Î·[x,a,o,xâ€²â€²] = Î·[xâ€²,a,o,xâ€²â€²]
                end
            end
            push!(removeX, xâ€²)
        end
    end
    # prune identical from previous nodes
    identical_action(x,xâ€²) = all(Ïˆ[x,a] â‰ˆ Ïˆ[xâ€²,a] for a in ğ’œ)
    identical_successor(x,xâ€²) = all(Î·[x,a,o,xâ€²â€²] â‰ˆ Î·[xâ€²,a,o,xâ€²â€²]
        for a in ğ’œ, o in ğ’ª, xâ€²â€² in X)
    identical(x,xâ€²) = identical_action(x,xâ€²) && identical_successor(x,xâ€²)
    for (x,xâ€²) in product(prevX, newX)
        if xâ€² âˆ‰ removeX && identical(x,xâ€²)
            push!(removeX, xâ€²)
        end
    end
    # prune dominated from new nodes
    for (x,xâ€²) in product(X, newX)
        if xâ€² âˆ‰ removeX && dominated(xâ€²,x) && x â‰  xâ€²
            push!(removeX, xâ€²)
        end
    end
    # update controller
    Ï€.X = setdiff(X, removeX)
    Ï€.Ïˆ = Dict(k => v for (k,v) in Ïˆ if k[1] âˆ‰ removeX)
    Ï€.Î· = Dict(k => v for (k,v) in Î· if k[1] âˆ‰ removeX)
end


struct NonlinearProgramming
    b # initial belief
    â„“ # number of nodes
end
function tensorform(ğ’«::POMDP)
    ğ’®, ğ’œ, ğ’ª, R, T, O = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.R, ğ’«.T, ğ’«.O
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = eachindex(ğ’œ)
    ğ’ªâ€² = eachindex(ğ’ª)
    Râ€² = [R(s,a) for s in ğ’®, a in ğ’œ]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in ğ’œ, sâ€² in ğ’®]
    Oâ€² = [O(a,sâ€²,o) for a in ğ’œ, sâ€² in ğ’®, o in ğ’ª]
    return ğ’®â€², ğ’œâ€², ğ’ªâ€², Râ€², Tâ€², Oâ€²
end
function solve(M::NonlinearProgramming, ğ’«::POMDP)
    x1, X = 1, collect(1:M.â„“)
    ğ’«, Î³, b = ğ’«, ğ’«.Î³, M.b
    ğ’®, ğ’œ, ğ’ª, R, T, O = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[X,ğ’®])
    @variable(model, Ïˆ[X,ğ’œ] â‰¥ 0)
    @variable(model, Î·[X,ğ’œ,ğ’ª,X] â‰¥ 0)
    @objective(model, Max, bâ‹…U[x1,:])
    @NLconstraint(model, [x=X,s=ğ’®],
        U[x,s] == (sum(Ïˆ[x,a]*(R[s,a] + Î³*sum(T[s,a,sâ€²]*sum(O[a,sâ€²,o]
        *sum(Î·[x,a,o,xâ€²]*U[xâ€²,sâ€²] for xâ€² in X)
        for o in ğ’ª) for sâ€² in ğ’®)) for a in ğ’œ)))
    @constraint(model, [x=X], sum(Ïˆ[x,:]) == 1)
    @constraint(model, [x=X,a=ğ’œ,o=ğ’ª], sum(Î·[x,a,o,:]) == 1)
    optimize!(model)
    Ïˆâ€², Î·â€² = value.(Ïˆ), value.(Î·)
    return ControllerPolicy(ğ’«, X,
        Dict((x, ğ’«.ğ’œ[a]) => Ïˆâ€²[x, a] for x in X, a in ğ’œ),
        Dict((x, ğ’«.ğ’œ[a], ğ’«.ğ’ª[o], xâ€²) => Î·â€²[x, a, o, xâ€²]
            for x in X, a in ğ’œ, o in ğ’ª, xâ€² in X))
end


struct ControllerGradient
    b     # initial belief
    â„“     # number of nodes
    Î±     # gradient step
    k_max # maximum iterations
end
function solve(M::ControllerGradient, ğ’«::POMDP)
    ğ’œ, ğ’ª, â„“, k_max = ğ’«.ğ’œ, ğ’«.ğ’ª, M.â„“, M.k_max
    X = collect(1:â„“)
    Ïˆ = Dict((x, a) => rand() for x in X, a in ğ’œ)
    Î· = Dict((x, a, o, xâ€²) => rand() for x in X, a in ğ’œ, o in ğ’ª, xâ€² in X)
    Ï€ = ControllerPolicy(ğ’«, X, Ïˆ, Î·)
    for i in 1:k_max
        improve!(Ï€, M, ğ’«)
    end
    return Ï€
end
function improve!(Ï€::ControllerPolicy, M::ControllerGradient, ğ’«::POMDP)
    ğ’®, ğ’œ, ğ’ª, X, x1, Ïˆ, Î· = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, Ï€.X, 1, Ï€.Ïˆ, Ï€.Î·
    n, m, z, b, â„“, Î± = length(ğ’®), length(ğ’œ), length(ğ’ª), M.b, M.â„“, M.Î±
    âˆ‚Uâ€²âˆ‚Ïˆ, âˆ‚Uâ€²âˆ‚Î· = gradient(Ï€, M, ğ’«)
    UIndex(x, s) = (s - 1) * â„“ + (x - 1) + 1
    E(U, x1, b) = sum(b[s]*U[UIndex(x1,s)] for s in 1:n)
    Ïˆâ€² = Dict((x, a) => 0.0 for x in X, a in ğ’œ)
    Î·â€² = Dict((x, a, o, xâ€²) => 0.0 for x in X, a in ğ’œ, o in ğ’ª, xâ€² in X)
    for x in X
        Ïˆâ€²x = [Ïˆ[x, a] + Î± * E(âˆ‚Uâ€²âˆ‚Ïˆ(x, a), x1, b) for a in ğ’œ]
        Ïˆâ€²x = project_to_simplex(Ïˆâ€²x)
        for (aIndex, a) in enumerate(ğ’œ)
            Ïˆâ€²[x, a] = Ïˆâ€²x[aIndex]
        end
        for (a, o) in product(ğ’œ, ğ’ª)
            Î·â€²x = [(Î·[x, a, o, xâ€²] +
                Î± * E(âˆ‚Uâ€²âˆ‚Î·(x, a, o, xâ€²), x1, b)) for xâ€² in X]
            Î·â€²x = project_to_simplex(Î·â€²x)
            for (xâ€²Index, xâ€²) in enumerate(X)
                Î·â€²[x, a, o, xâ€²] = Î·â€²x[xâ€²Index]
            end
        end
    end
    Ï€.Ïˆ, Ï€.Î· = Ïˆâ€², Î·â€²
end
function project_to_simplex(y)
    u = sort(copy(y), rev=true)
    i = maximum([j for j in eachindex(u)
        if u[j] + (1 - sum(u[1:j])) / j > 0.0])
    Î´ = (1 - sum(u[j] for j = 1:i)) / i
    return [max(y[j] + Î´, 0.0) for j in eachindex(u)]
end


function gradient(Ï€::ControllerPolicy, M::ControllerGradient, ğ’«::POMDP)
    ğ’®, ğ’œ, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    X, x1, Ïˆ, Î· = Ï€.X, 1, Ï€.Ïˆ, Ï€.Î·
    n, m, z = length(ğ’®), length(ğ’œ), length(ğ’ª)
    Xğ’® = vec(collect(product(X, ğ’®)))
    Tâ€² = [sum(Ïˆ[x, a] * T(s, a, sâ€²) * sum(O(a, sâ€², o) * Î·[x, a, o, xâ€²]
        for o in ğ’ª) for a in ğ’œ) for (x, s) in Xğ’®, (xâ€², sâ€²) in Xğ’®]
    Râ€² = [sum(Ïˆ[x, a] * R(s, a) for a in ğ’œ) for (x, s) in Xğ’®]
    Z = 1.0I(length(Xğ’®)) - Î³ * Tâ€²
    invZ = inv(Z)
    âˆ‚Zâˆ‚Ïˆ(hx, ha) = [x == hx ? (-Î³ * T(s, ha, sâ€²)
        * sum(O(ha, sâ€², o) * Î·[hx, ha, o, xâ€²]
        for o in ğ’ª)) : 0.0
            for (x, s) in Xğ’®, (xâ€², sâ€²) in Xğ’®]
    âˆ‚Zâˆ‚Î·(hx, ha, ho, hxâ€²) = [x == hx && xâ€² == hxâ€² ? (-Î³ * Ïˆ[hx, ha]
        * T(s, ha, sâ€²) * O(ha, sâ€², ho)) : 0.0
        for (x, s) in Xğ’®, (xâ€², sâ€²) in Xğ’®]
    âˆ‚Râ€²âˆ‚Ïˆ(hx, ha) = [x == hx ? R(s, ha) : 0.0 for (x, s) in Xğ’®]
    âˆ‚Râ€²âˆ‚Î·(hx, ha, ho, hxâ€²) = [0.0 for (x, s) in Xğ’®]
    âˆ‚Uâ€²âˆ‚Ïˆ(hx, ha) = invZ * (âˆ‚Râ€²âˆ‚Ïˆ(hx, ha) - âˆ‚Zâˆ‚Ïˆ(hx, ha) * invZ * Râ€²)
    âˆ‚Uâ€²âˆ‚Î·(hx, ha, ho, hxâ€²) = invZ * (âˆ‚Râ€²âˆ‚Î·(hx, ha, ho, hxâ€²)
        - âˆ‚Zâˆ‚Î·(hx, ha, ho, hxâ€²) * invZ * Râ€²)
    return âˆ‚Uâ€²âˆ‚Ïˆ, âˆ‚Uâ€²âˆ‚Î·
end

# 24 Multiagent Reasoning

struct SimpleGame
    Î³ # discount factor
    â„ # agents
    ğ’œ # joint action space
    R # joint reward function
end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end
    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
    return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end

(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)
function (Ï€i::SimpleGamePolicy)()
    D = SetCategorical(collect(keys(Ï€i.p)), collect(values(Ï€i.p)))
    return rand(D)
end
joint(X) = vec(collect(product(X...)))
joint(Ï€, Ï€i, i) = [i == j ? Ï€i : Ï€j for (j, Ï€j) in enumerate(Ï€)]

function utility(ğ’«::SimpleGame, Ï€, i)
    ğ’œ, R = ğ’«.ğ’œ, ğ’«.R
    p(a) = prod(Ï€j(aj) for (Ï€j, aj) in zip(Ï€, a))
    return sum(R(a)[i]*p(a) for a in joint(ğ’œ))
end
function best_response(ğ’«::SimpleGame, Ï€, i)
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    ai = argmax(U, ğ’«.ğ’œ[i])
    return SimpleGamePolicy(ai)
end

function softmax_response(ğ’«::SimpleGame, Ï€, i, Î»)
    ğ’œi = ğ’«.ğ’œ[i]
    U(ai) = utility(ğ’«, joint(Ï€, SimpleGamePolicy(ai), i), i)
    return SimpleGamePolicy(ai => exp(Î»*U(ai)) for ai in ğ’œi)
end

struct NashEquilibrium end
function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end

function solve(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
            for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
        U[i] â‰¥ sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
            * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

mutable struct JointCorrelatedPolicy
    p # dictionary mapping from joint actions to probabilities
    JointCorrelatedPolicy(p::Base.Generator) = new(Dict(p))
end
(Ï€::JointCorrelatedPolicy)(a) = get(Ï€.p, a, 0.0)
function (Ï€::JointCorrelatedPolicy)()
    D = SetCategorical(collect(keys(Ï€.p)), collect(values(Ï€.p)))
    return rand(D)
end

struct CorrelatedEquilibrium end
function solve(M::CorrelatedEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    model = Model(Ipopt.Optimizer)
    @variable(model, Ï€[joint(ğ’œ)] â‰¥ 0)
    @objective(model, Max, sum(sum(Ï€[a]*R(a) for a in joint(ğ’œ))))
    @constraint(model, [i=â„, ai=ğ’œ[i], aiâ€²=ğ’œ[i]],
        sum(R(a)[i]*Ï€[a] for a in joint(ğ’œ) if a[i]==ai)
        â‰¥ sum(R(joint(a,aiâ€²,i))[i]*Ï€[a] for a in joint(ğ’œ) if a[i]==ai))
    @constraint(model, sum(Ï€) == 1)
    optimize!(model)
    return JointCorrelatedPolicy(a => value(Ï€[a]) for a in joint(ğ’œ))
end

struct IteratedBestResponse
    k_max # number of iterations
    Ï€     # initial policy
end
function IteratedBestResponse(ğ’«::SimpleGame, k_max)
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
    return IteratedBestResponse(k_max, Ï€)
end
function solve(M::IteratedBestResponse, ğ’«)
    Ï€ = M.Ï€
    for k in 1:M.k_max
        Ï€ = [best_response(ğ’«, Ï€, i) for i in ğ’«.â„]
    end
    return Ï€
end

struct HierarchicalSoftmax
    Î» # precision parameter
    k # level
    Ï€ # initial policy
end
function HierarchicalSoftmax(ğ’«::SimpleGame, Î», k)
    Ï€ = [SimpleGamePolicy(ai => 1.0 for ai in ğ’œi) for ğ’œi in ğ’«.ğ’œ]
    return HierarchicalSoftmax(Î», k, Ï€)
end

function solve(M::HierarchicalSoftmax, ğ’«)
    Ï€ = M.Ï€
    for k in 1:M.k
        Ï€ = [softmax_response(ğ’«, Ï€, i, M.Î») for i in ğ’«.â„]
    end
    return Ï€
end

function simulate(ğ’«::SimpleGame, Ï€, k_max)
    for k = 1:k_max
        a = [Ï€i() for Ï€i in Ï€]
        for Ï€i in Ï€
            update!(Ï€i, a)
        end
    end
    return Ï€
end



mutable struct FictitiousPlay
    ğ’«  # simple game
    i  # agent index
    N  # array of action count dictionaries
    Ï€i # current policy
end
function FictitiousPlay(ğ’«::SimpleGame, i)
    N = [Dict(aj => 1 for aj in ğ’«.ğ’œ[j]) for j in ğ’«.â„]
    Ï€i = SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])
    return FictitiousPlay(ğ’«, i, N, Ï€i)
end

(Ï€i::FictitiousPlay)() = Ï€i.Ï€i()
(Ï€i::FictitiousPlay)(ai) = Ï€i.Ï€i(ai)
function update!(Ï€i::FictitiousPlay, a)
    N, ğ’«, â„, i = Ï€i.N, Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.i
    for (j, aj) in enumerate(a)
        N[j][aj] += 1
    end
    p(j) = SimpleGamePolicy(aj => u/sum(values(N[j])) for (aj, u) in N[j])
    Ï€ = [p(j) for j in â„]
    Ï€i.Ï€i = best_response(ğ’«, Ï€, i)
end


mutable struct GradientAscent
    ğ’« # simple game
    i # agent index
    t # time step
    Ï€i # current policy
end
function GradientAscent(ğ’«::SimpleGame, i)
    uniform() = SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])
    return GradientAscent(ğ’«, i, 1, uniform())
end
(Ï€i::GradientAscent)() = Ï€i.Ï€i()
(Ï€i::GradientAscent)(ai) = Ï€i.Ï€i(ai)
function update!(Ï€i::GradientAscent, a)
    ğ’«, â„, ğ’œi, i, t = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’œ[Ï€i.i], Ï€i.i, Ï€i.t
    jointÏ€(ai) = [SimpleGamePolicy(j == i ? ai : a[j]) for j in â„]
    r = [utility(ğ’«, jointÏ€(ai), i) for ai in ğ’œi]
    Ï€â€² = [Ï€i.Ï€i(ai) for ai in ğ’œi]
    Ï€ = project_to_simplex(Ï€â€² + r / sqrt(t))
    Ï€i.t = t + 1
    Ï€i.Ï€i = SimpleGamePolicy(ai => p for (ai, p) in zip(ğ’œi, Ï€))
end

# 25 Sequential Problems

struct MG
    Î³ # discount factor
    â„ # agents
    ğ’® # state space
    ğ’œ # joint action space
    T # transition function
    R # joint reward function
end

struct MGPolicy
    p # dictionary mapping states to simple game policies
    MGPolicy(p::Base.Generator) = new(Dict(p))
end

(Ï€i::MGPolicy)(s, ai) = Ï€i.p[s](ai)
(Ï€i::SimpleGamePolicy)(s, ai) = Ï€i(ai)

probability(ğ’«::MG, s, Ï€, a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
reward(ğ’«::MG, s, Ï€, i) =
    sum(ğ’«.R(s,a)[i]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’«.ğ’œ))
transition(ğ’«::MG, s, Ï€, sâ€²) =
    sum(ğ’«.T(s,a,sâ€²)*probability(ğ’«,s,Ï€,a) for a in joint(ğ’«.ğ’œ))

function policy_evaluation(ğ’«::MG, Ï€, i)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    p(s,a) = prod(Ï€j(s, aj) for (Ï€j, aj) in zip(Ï€, a))
    Râ€² = [sum(R(s,a)[i]*p(s,a) for a in joint(ğ’œ)) for s in ğ’®]
    Tâ€² = [sum(T(s,a,sâ€²)*p(s,a) for a in joint(ğ’œ)) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³*Tâ€²)\Râ€²
end

function best_response(ğ’«::MG, Ï€, i)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Tâ€²(s,ai,sâ€²) = transition(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), sâ€²)
    Râ€²(s,ai) = reward(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), i)
    Ï€i = solve(MDP(Î³, ğ’®, ğ’œ[i], Tâ€², Râ€²))
    return MGPolicy(s => SimpleGamePolicy(Ï€i(s)) for s in ğ’®)
end

function softmax_response(ğ’«::MG, Ï€, i, Î»)
    ğ’®, ğ’œ, R, T, Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T, ğ’«.Î³
    Tâ€²(s,ai,sâ€²) = transition(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), sâ€²)
    Râ€²(s,ai) = reward(ğ’«, s, joint(Ï€, SimpleGamePolicy(ai), i), i)
    mdp = MDP(Î³, ğ’®, joint(ğ’œ), Tâ€², Râ€²)
    Ï€i = solve(mdp)
    Q(s,a) = lookahead(mdp, Ï€i.U, s, a)
    p(s) = SimpleGamePolicy(a => exp(Î»*Q(s,a)) for a in ğ’œ[i])
    return MGPolicy(s => p(s) for s in ğ’®)
end


function tensorform(ğ’«::MG)
    â„, ğ’®, ğ’œ, R, T = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.T
    â„â€² = eachindex(â„)
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(s,a) for s in ğ’®, a in joint(ğ’œ)]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in joint(ğ’œ), sâ€² in ğ’®]
    return â„â€², ğ’®â€², ğ’œâ€², Râ€², Tâ€²
end

function solve(M::NashEquilibrium, ğ’«::MG)
    â„, ğ’®, ğ’œ, R, T = tensorform(ğ’«)
    ğ’®â€², ğ’œâ€², Î³ = ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.Î³
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„, ğ’®])
    @variable(model, Ï€[i=â„, ğ’®, ai=ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
        sum(U[i,s] - sum(prod(Ï€[j,s,a[j]] for j in â„)
            * (R[s,y][i] + Î³*sum(T[s,y,sâ€²]*U[i,sâ€²] for sâ€² in ğ’®))
            for (y,a) in enumerate(joint(ğ’œ))) for i in â„, s in ğ’®))
    @NLconstraint(model, [i=â„, s=ğ’®, ai=ğ’œ[i]],
        U[i,s] â‰¥ sum(
            prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,s,a[j]] for j in â„)
            * (R[s,y][i] + Î³*sum(T[s,y,sâ€²]*U[i,sâ€²] for sâ€² in ğ’®))
            for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„, s=ğ’®], sum(Ï€[i,s,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€â€² = value.(Ï€)
    Ï€iâ€²(i,s) = SimpleGamePolicy(ğ’œâ€²[i][ai] => Ï€â€²[i,s,ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(ğ’®â€²[s] => Ï€iâ€²(i,s) for s in ğ’®)
    return [Ï€iâ€²(i) for i in â„]
end

function randstep(ğ’«::MG, s, a)
    sâ€² = rand(SetCategorical(ğ’«.ğ’®, [ğ’«.T(s, a, sâ€²) for sâ€² in ğ’«.ğ’®]))
    r = ğ’«.R(s,a)
    return sâ€², r
end
function simulate(ğ’«::MG, Ï€, k_max, b)
    s = rand(b)
    for k = 1:k_max
        a = Tuple(Ï€i(s)() for Ï€i in Ï€)
        sâ€², r = randstep(ğ’«, s, a)
        for Ï€i in Ï€
            update!(Ï€i, s, a, sâ€²)
        end
        s = sâ€²
    end
    return Ï€
end


mutable struct MGFictitiousPlay
    ğ’« # Markov game
    i # agent index
    Qi # state-action value estimates
    Ni # state-action counts
end
function MGFictitiousPlay(ğ’«::MG, i)
    â„, ğ’®, ğ’œ, R = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R
    Qi = Dict((s, a) => R(s, a)[i] for s in ğ’® for a in joint(ğ’œ))
    Ni = Dict((j, s, aj) => 1.0 for j in â„ for s in ğ’® for aj in ğ’œ[j])
    return MGFictitiousPlay(ğ’«, i, Qi, Ni)
end

function (Ï€i::MGFictitiousPlay)(s)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    Ï€iâ€²(i,s) = SimpleGamePolicy(ai => Ï€i.Ni[i,s,ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i,s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(s,Ï€) = sum(Ï€i.Qi[s,a]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’œ))
    Q(s,Ï€) = reward(ğ’«,s,Ï€,i) + Î³*sum(transition(ğ’«,s,Ï€,sâ€²)*U(sâ€²,Ï€)
        for sâ€² in ğ’®)
    Q(ai) = Q(s, joint(Ï€, SimpleGamePolicy(ai), i))
    ai = argmax(Q, ğ’«.ğ’œ[Ï€i.i])
    return SimpleGamePolicy(ai)
end

function update!(Ï€i::MGFictitiousPlay, s, a, sâ€²)
    ğ’«, i, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.Qi
    â„, ğ’®, ğ’œ, T, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.T, ğ’«.R, ğ’«.Î³
    for (j,aj) in enumerate(a)
        Ï€i.Ni[j,s,aj] += 1
    end
    Ï€iâ€²(i,s) = SimpleGamePolicy(ai => Ï€i.Ni[i,s,ai] for ai in ğ’œ[i])
    Ï€iâ€²(i) = MGPolicy(s => Ï€iâ€²(i,s) for s in ğ’®)
    Ï€ = [Ï€iâ€²(i) for i in â„]
    U(Ï€,s) = sum(Ï€i.Qi[s,a]*probability(ğ’«,s,Ï€,a) for a in joint(ğ’œ))
    Q(s,a) = R(s,a)[i] + Î³*sum(T(s,a,sâ€²)*U(Ï€,sâ€²) for sâ€² in ğ’®)
    for a in joint(ğ’œ)
        Ï€i.Qi[s,a] = Q(s,a)
    end
end


mutable struct MGGradientAscent
    ğ’« # Markov game
    i # agent index
    t # time step
    Qi # state-action value estimates
    Ï€i # current policy
end
function MGGradientAscent(ğ’«::MG, i)
    â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
    Qi = Dict((s, a) => 0.0 for s in ğ’®, a in joint(ğ’œ))
    uniform() = Dict(s => SimpleGamePolicy(ai => 1.0 for ai in ğ’«.ğ’œ[i])
        for s in ğ’®)
    return MGGradientAscent(ğ’«, i, 1, Qi, uniform())
end

function (Ï€i::MGGradientAscent)(s)
    ğ’œi, t = Ï€i.ğ’«.ğ’œ[Ï€i.i], Ï€i.t
    Ïµ = 1 / sqrt(t)
    Ï€iâ€²(ai) = Ïµ/length(ğ’œi) + (1-Ïµ)*Ï€i.Ï€i[s](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end
function update!(Ï€i::MGGradientAscent, s, a, sâ€²)
    ğ’«, i, t, Qi = Ï€i.ğ’«, Ï€i.i, Ï€i.t, Ï€i.Qi
    â„, ğ’®, ğ’œi, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ[Ï€i.i], ğ’«.R, ğ’«.Î³
    jointÏ€(ai) = Tuple(j == i ? ai : a[j] for j in â„)
    Î± = 1 / sqrt(t)
    Qmax = maximum(Qi[sâ€², jointÏ€(ai)] for ai in ğ’œi)
    Ï€i.Qi[s, a] += Î± * (R(s, a)[i] + Î³ * Qmax - Qi[s, a])
    u = [Qi[s, jointÏ€(ai)] for ai in ğ’œi]
    Ï€â€² = [Ï€i.Ï€i[s](ai) for ai in ğ’œi]
    Ï€ = project_to_simplex(Ï€â€² + u / sqrt(t))
    Ï€i.t = t + 1
    Ï€i.Ï€i[s] = SimpleGamePolicy(ai => p for (ai, p) in zip(ğ’œi, Ï€))
end


mutable struct NashQLearning
    ğ’« # Markov game
    i # agent index
    Q # state-action value estimates
    N # history of actions performed
end

function NashQLearning(ğ’«::MG, i)
    â„, ğ’®, ğ’œ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ
    Q = Dict((j, s, a) => 0.0 for j in â„, s in ğ’®, a in joint(ğ’œ))
    N = Dict((s, a) => 1.0 for s in ğ’®, a in joint(ğ’œ))
    return NashQLearning(ğ’«, i, Q, N)
end

function (Ï€i::NashQLearning)(s)
    ğ’«, i, Q, N = Ï€i.ğ’«, Ï€i.i, Ï€i.Q, Ï€i.N
    â„, ğ’®, ğ’œ, ğ’œi, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’œ[Ï€i.i], ğ’«.Î³
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, a -> [Q[j, s, a] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ïµ = 1 / sum(N[s, a] for a in joint(ğ’œ))
    Ï€iâ€²(ai) = Ïµ/length(ğ’œi) + (1-Ïµ)*Ï€[i](ai)
    return SimpleGamePolicy(ai => Ï€iâ€²(ai) for ai in ğ’œi)
end

function update!(Ï€i::NashQLearning, s, a, sâ€²)
    ğ’«, â„, ğ’®, ğ’œ, R, Î³ = Ï€i.ğ’«, Ï€i.ğ’«.â„, Ï€i.ğ’«.ğ’®, Ï€i.ğ’«.ğ’œ, Ï€i.ğ’«.R, Ï€i.ğ’«.Î³
    i, Q, N = Ï€i.i, Ï€i.Q, Ï€i.N
    M = NashEquilibrium()
    ğ’¢ = SimpleGame(Î³, â„, ğ’œ, aâ€² -> [Q[j, sâ€², aâ€²] for j in â„])
    Ï€ = solve(M, ğ’¢)
    Ï€i.N[s, a] += 1
    Î± = 1 / sqrt(N[s, a])
    for j in â„
        Ï€i.Q[j,s,a] += Î±*(R(s,a)[j] + Î³*utility(ğ’¢,Ï€,j) - Q[j,s,a])
    end
end

# 26 State Uncertainty

struct POMG
    Î³ # discount factor
    â„ # agents
    ğ’® # state space
    ğ’œ # joint action space
    ğ’ª # joint observation space
    T # transition function
    O # joint observation function
    R # joint reward function
end

function lookahead(ğ’«::POMG, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s,a) + Î³*uâ€²
end
function evaluate_plan(ğ’«::POMG, Ï€, s)
    a = Tuple(Ï€i() for Ï€i in Ï€)
    U(o,sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€,o)], sâ€²)
    return isempty(first(Ï€).subplans) ? ğ’«.R(s,a) : lookahead(ğ’«, U, s, a)
end
function utility(ğ’«::POMG, b, Ï€)
    u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
    return sum(bs * us for (bs, us) in zip(b, u))
end


struct POMGNashEquilibrium
    b # initial belief
    d # depth of conditional plans
end
function create_conditional_plans(ğ’«, d)
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
    end
    return Î 
end

function expand_conditional_plans(ğ’«, Î )
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i]))
        for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end
function solve(M::POMGNashEquilibrium, ğ’«::POMG)
    â„, Î³, b, d = ğ’«.â„, ğ’«.Î³, M.b, M.d
    Î  = create_conditional_plans(ğ’«, d)
    U = Dict(Ï€ => utility(ğ’«, b, Ï€) for Ï€ in joint(Î ))
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> U[Ï€])
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end


struct POMGDynamicProgramming
    b # initial belief
    d # depth of conditional plans
end

function solve(M::POMGDynamicProgramming, ğ’«::POMG)
    â„, ğ’®, ğ’œ, R, Î³, b, d = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³, M.b, M.d
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
        prune_dominated!(Î , ğ’«)
    end
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> utility(ğ’«, b, Ï€))
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

function prune_dominated!(Î , ğ’«::POMG)
    done = false
    while !done
        done = true
        for i in shuffle(ğ’«.â„)
            for Ï€i in shuffle(Î [i])
                if length(Î [i]) > 1 && is_dominated(ğ’«, Î , i, Ï€i)
                    filter!(Ï€iâ€² -> Ï€iâ€² â‰  Ï€i, Î [i])
                    done = false
                    break
                end
            end
        end
    end
end

function is_dominated(ğ’«::POMG, Î , i, Ï€i)
    â„, ğ’® = ğ’«.â„, ğ’«.ğ’®
    jointÎ noti = joint([Î [j] for j in â„ if j â‰  i])
    Ï€(Ï€iâ€², Ï€noti) = [j==i ? Ï€iâ€² : Ï€noti[j>i ? j-1 : j] for j in â„]
    Ui = Dict((Ï€iâ€², Ï€noti, s) => evaluate_plan(ğ’«, Ï€(Ï€iâ€², Ï€noti), s)[i]
        for Ï€iâ€² in Î [i], Ï€noti in jointÎ noti, s in ğ’®)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î´)
    @variable(model, b[jointÎ noti, ğ’®] â‰¥ 0)
    @objective(model, Max, Î´)
    @constraint(model, [Ï€iâ€²=Î [i]],
        sum(b[Ï€noti, s] * (Ui[Ï€iâ€², Ï€noti, s] - Ui[Ï€i, Ï€noti, s])
        for Ï€noti in jointÎ noti for s in ğ’®) â‰¥ Î´)
    @constraint(model, sum(b) == 1)
    optimize!(model)
    return value(Î´) â‰¥ 0
end

# 27 Collaborative Agents

struct DecPOMDP
    Î³ # discount factor
    â„ # agents
    ğ’® # state space
    ğ’œ # joint action space
    ğ’ª # joint observation space
    T # transition function
    O # joint observation function
    R # reward function
end

struct DecPOMDPDynamicProgramming
    b # initial belief
    d # depth of conditional plans
end
function solve(M::DecPOMDPDynamicProgramming, ğ’«::DecPOMDP)
    â„, ğ’®, ğ’œ, ğ’ª, T, O, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    Râ€²(s, a) = [R(s, a) for i in â„]
    ğ’«â€² = POMG(Î³, â„, ğ’®, ğ’œ, ğ’ª, T, O, Râ€²)
    Mâ€² = POMGDynamicProgramming(M.b, M.d)
    return solve(Mâ€², ğ’«â€²)
end


struct DecPOMDPIteratedBestResponse
    b # initial belief
    d # depth of conditional plans
    k_max # number of iterations
end

function solve(M::DecPOMDPIteratedBestResponse, ğ’«::DecPOMDP)
    â„, ğ’®, ğ’œ, ğ’ª, T, O, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    b, d, k_max = M.b, M.d, M.k_max
    Râ€²(s, a) = [R(s, a) for i in â„]
    ğ’«â€² = POMG(Î³, â„, ğ’®, ğ’œ, ğ’ª, T, O, Râ€²)
    Î  = create_conditional_plans(ğ’«, d)
    Ï€ = [rand(Î [i]) for i in â„]
    for k in 1:k_max
        for i in shuffle(â„)
            Ï€â€²(Ï€i) = Tuple(j == i ? Ï€i : Ï€[j] for j in â„)
            Ui(Ï€i) = utility(ğ’«â€², b, Ï€â€²(Ï€i))[i]
            Ï€[i] = argmax(Ui, Î [i])
        end
    end
    return Tuple(Ï€)
end


struct DecPOMDPHeuristicSearch
    b # initial belief
    d # depth of conditional plans
    Ï€_max # number of policies
end

function solve(M::DecPOMDPHeuristicSearch, ğ’«::DecPOMDP)
    â„, ğ’®, ğ’œ, ğ’ª, T, O, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    b, d, Ï€_max = M.b, M.d, M.Ï€_max
    Râ€²(s, a) = [R(s, a) for i in â„]
    ğ’«â€² = POMG(Î³, â„, ğ’®, ğ’œ, ğ’ª, T, O, Râ€²)
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        allÎ  = expand_conditional_plans(ğ’«, Î )
        Î  = [[] for i in â„]
        for z in 1:Ï€_max
            bâ€² = explore(M, ğ’«, t)
            Ï€ = argmax(Ï€ -> first(utility(ğ’«â€², bâ€², Ï€)), joint(allÎ ))
            for i in â„
                push!(Î [i], Ï€[i])
                filter!(Ï€i -> Ï€i != Ï€[i], allÎ [i])
            end
        end
    end
    return argmax(Ï€ -> first(utility(ğ’«â€², b, Ï€)), joint(Î ))
end

function explore(M::DecPOMDPHeuristicSearch, ğ’«::DecPOMDP, t)
    â„, ğ’®, ğ’œ, ğ’ª, T, O, R, Î³ = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    b = copy(M.b)
    bâ€² = similar(b)
    s = rand(SetCategorical(ğ’®, b))
    for Ï„ in 1:t
        a = Tuple(rand(ğ’œi) for ğ’œi in ğ’œ)
        sâ€² = rand(SetCategorical(ğ’®, [T(s,a,sâ€²) for sâ€² in ğ’®]))
        o = rand(SetCategorical(joint(ğ’ª), [O(a,sâ€²,o) for o in joint(ğ’ª)]))
        for (iâ€², sâ€²) in enumerate(ğ’®)
            po = O(a, sâ€², o)
            bâ€²[iâ€²] = po*sum(T(s,a,sâ€²)*b[i] for (i,s) in enumerate(ğ’®))
        end
        normalize!(bâ€², 1)
        b, s = bâ€², sâ€²
    end
    return bâ€²
end

struct DecPOMDPNonlinearProgramming
    b # initial belief
    â„“ # number of nodes for each agent
end
function tensorform(ğ’«::DecPOMDP)
    â„, ğ’®, ğ’œ, ğ’ª, R, T, O = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.ğ’ª, ğ’«.R, ğ’«.T, ğ’«.O
    â„â€² = eachindex(â„)
    ğ’®â€² = eachindex(ğ’®)
    ğ’œâ€² = [eachindex(ğ’œi) for ğ’œi in ğ’œ]
    ğ’ªâ€² = [eachindex(ğ’ªi) for ğ’ªi in ğ’ª]
    Râ€² = [R(s,a) for s in ğ’®, a in joint(ğ’œ)]
    Tâ€² = [T(s,a,sâ€²) for s in ğ’®, a in joint(ğ’œ), sâ€² in ğ’®]
    Oâ€² = [O(a,sâ€²,o) for a in joint(ğ’œ), sâ€² in ğ’®, o in joint(ğ’ª)]
    return â„â€², ğ’®â€², ğ’œâ€², ğ’ªâ€², Râ€², Tâ€², Oâ€²
end

function solve(M::DecPOMDPNonlinearProgramming, ğ’«::DecPOMDP)
    ğ’«, Î³, b = ğ’«, ğ’«.Î³, M.b
    â„, ğ’®, ğ’œ, ğ’ª, R, T, O = tensorform(ğ’«)
    X = [collect(1:M.â„“) for i in â„]
    jointX, jointğ’œ, jointğ’ª = joint(X), joint(ğ’œ), joint(ğ’ª)
    x1 = jointX[1]
    model = Model(Ipopt.Optimizer)
    @variable(model, U[jointX,ğ’®])
    @variable(model, Ïˆ[i=â„,X[i],ğ’œ[i]] â‰¥ 0)
    @variable(model, Î·[i=â„,X[i],ğ’œ[i],ğ’ª[i],X[i]] â‰¥ 0)
    @objective(model, Max, bâ‹…U[x1,:])
    @NLconstraint(model, [x=jointX,s=ğ’®],
        U[x,s] == (sum(prod(Ïˆ[i,x[i],a[i]] for i in â„)
            *(R[s,y] + Î³*sum(T[s,y,sâ€²]*sum(O[y,sâ€²,z]
                *sum(prod(Î·[i,x[i],a[i],o[i],xâ€²[i]] for i in â„)
                    *U[xâ€²,sâ€²] for xâ€² in jointX)
                for (z, o) in enumerate(jointğ’ª)) for sâ€² in ğ’®))
            for (y, a) in enumerate(jointğ’œ))))
    @constraint(model, [i=â„,xi=X[i]],
        sum(Ïˆ[i,xi,ai] for ai in ğ’œ[i]) == 1)
    @constraint(model, [i=â„,xi=X[i],ai=ğ’œ[i],oi=ğ’ª[i]],
        sum(Î·[i,xi,ai,oi,xiâ€²] for xiâ€² in X[i]) == 1)
    optimize!(model)
    Ïˆâ€², Î·â€² = value.(Ïˆ), value.(Î·)
    return [ControllerPolicy(ğ’«, X[i],
            Dict((xi,ğ’«.ğ’œ[i][ai]) => Ïˆâ€²[i,xi,ai]
                for xi in X[i], ai in ğ’œ[i]),
            Dict((xi,ğ’«.ğ’œ[i][ai],ğ’«.ğ’ª[i][oi],xiâ€²) => Î·â€²[i,xi,ai,oi,xiâ€²]
                for xi in X[i], ai in ğ’œ[i], oi in ğ’ª[i], xiâ€² in X[i]))
        for i in â„]
end

# EOF
