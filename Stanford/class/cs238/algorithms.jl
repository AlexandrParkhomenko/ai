
using LinearAlgebra
using Distributions
using LightGraphs
using JuMP
using GLPK

#import IterTools: subsets
function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_ma
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end
Base.argmax(f::Function, xs) = findmax(f, xs)[2]

Base.Dict{Symbol,V}(a::NamedTuple) where V =
          Dict{Symbol,V}(n => v for (n, v) in zip(keys(a), values(a)))

Base.convert(::Type{Dict{Symbol,V}}, a::NamedTuple) where V =
          Dict{Symbol,V}(a)

Base.isequal(a::Dict{Symbol,<:Any}, nt::NamedTuple) =
          length(a) == length(nt) &&
          all(a[n] == v for (n, v) in zip(keys(nt), values(nt)))

struct Variable
    name::Symbol
    m::Int # number of possible values
end

const Assignment = Dict{Symbol,Int}
const FactorTable = Dict{Assignment,Float64}

struct Factor
    vars::Vector{Variable}
    table::FactorTable
end

variablenames(Ï•::Factor) = [var.name for var in Ï•.vars]
select(a::Assignment, varnames::Vector{Symbol}) =
                Assignment(n => a[n] for n in varnames)

function assignments(vars::AbstractVector{Variable})
    names = [var.name for var in vars]
    return vec([Assignment(n => v for (n, v) in zip(names, values))
                for values in product((1:v.m for v in vars)...)])
end

function normalize!(Ï•::Factor)
    z = sum(p for (a, p) in Ï•.table)
    for (a, p) in Ï•.table
        Ï•.table[a] = p / z
    end
    return Ï•
end

struct BayesianNetwork
    vars::Vector{Variable}
    factors::Vector{Factor}
    graph::SimpleDiGraph{Int64}
end

B = Variable(:b, 2); S = Variable(:s, 2)
E = Variable(:e, 2)
D = Variable(:d, 2); C = Variable(:c, 2)
vars = [B, S, E, D, C]
factors = [
    Factor([B], FactorTable((b = 1,) => 0.99, (b = 2,) => 0.01)),
    Factor([S], FactorTable((s = 1,) => 0.98, (s = 2,) => 0.02)),
    Factor([E,B,S], FactorTable(
        (e = 1, b = 1, s = 1) => 0.90, (e = 1, b = 1, s = 2) => 0.04,
        (e = 1, b = 2, s = 1) => 0.05, (e = 1, b = 2, s = 2) => 0.01,
        (e = 2, b = 1, s = 1) => 0.10, (e = 2, b = 1, s = 2) => 0.96,
        (e = 2, b = 2, s = 1) => 0.95, (e = 2, b = 2, s = 2) => 0.99)),
    Factor([D, E], FactorTable(
        (d = 1, e = 1) => 0.96, (d = 1, e = 2) => 0.03,
        (d = 2, e = 1) => 0.04, (d = 2, e = 2) => 0.97)),
    Factor([C, E], FactorTable(
        (c = 1, e = 1) => 0.98, (c = 1, e = 2) => 0.01,
        (c = 2, e = 1) => 0.02, (c = 2, e = 2) => 0.99))
]
graph = SimpleDiGraph(5)
add_edge!(graph, 1, 3); add_edge!(graph, 2, 3)
add_edge!(graph, 3, 4); add_edge!(graph, 3, 5)
bn = BayesianNetwork(vars, factors, graph)

function probability(bn::BayesianNetwork, assignment)
    subassignment(Ï•) = select(assignment, variablenames(Ï•))
    probability(Ï•) = get(Ï•.table, subassignment(Ï•), 0.0)
    return prod(probability(Ï•) for Ï• in bn.factors)
end

function Base.:*(Ï•::Factor, Ïˆ::Factor)
    Ï•names = variablenames(Ï•)
    Ïˆnames = variablenames(Ïˆ)
    Ïˆonly = setdiff(Ïˆ.vars, Ï•.vars)
    table = FactorTable()
    for (Ï•a, Ï•p) in Ï•.table
        for a in assignments(Ïˆonly)
            a = merge(Ï•a, a)
            Ïˆa = select(a, Ïˆnames)
            table[a] = Ï•p * get(Ïˆ.table, Ïˆa, 0.0)
        end
    end
    vars = vcat(Ï•.vars, Ïˆonly)
    return Factor(vars, table)
end

function marginalize(Ï•::Factor, name)
    table = FactorTable()
    for (a, p) in Ï•.table
        aâ€² = delete!(copy(a), name)
        table[aâ€²] = get(table, aâ€², 0.0) + p
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end

in_scope(name, Ï•) = any(name == v.name for v in Ï•.vars)

function condition(Ï•::Factor, name, value)
    if !in_scope(name, Ï•)
        return Ï•
    end
    table = FactorTable()
    for (a, p) in Ï•.table
        if a[name] == value
            table[delete!(copy(a), name)] = p
        end
    end
    vars = filter(v -> v.name != name, Ï•.vars)
    return Factor(vars, table)
end
function condition(Ï•::Factor, evidence)
    for (name, value) in pairs(evidence)
        Ï• = condition(Ï•, name, value)
    end
    return Ï•
end

struct ExactInference end
function infer(M::ExactInference, bn, query, evidence)
    Ï• = prod(bn.factors)
    Ï• = condition(Ï•, evidence)
    for name in setdiff(variablenames(Ï•), query)
    Ï• = marginalize(Ï•, name)
        end
    return normalize!(Ï•)
end

struct VariableElimination
    ordering # array of variable indices
end
function infer(M::VariableElimination, bn, query, evidence)
    Î¦ = [condition(Ï•, evidence) for Ï• in bn.factors]
    for i in M.ordering
        name = bn.vars[i].name
        if name âˆ‰ query
            inds = findall(Ï• -> in_scope(name, Ï•), Î¦)
            if !isempty(inds)
                Ï• = prod(Î¦[inds])
                deleteat!(Î¦, inds)
                Ï• = marginalize(Ï•, name)
                push!(Î¦, Ï•)
            end
        end
    end
    return normalize!(prod(Î¦))
end

function Base.rand(Ï•::Factor)
    tot, p, w = 0.0, rand(), sum(values(Ï•.table))
    for (a, v) in Ï•.table
        tot += v / w
        if tot >= p
            return a
        end
    end
    return Assignment()
end

function Base.rand(bn::BayesianNetwork)
    a = Assignment()
    for i in topological_sort(bn.graph)
        name, Ï• = bn.vars[i].name, bn.factors[i]
        a[name] = rand(condition(Ï•, a))[name]
    end
    return a
end

struct DirectSampling
    m # number of samples
end
function infer(M::DirectSampling, bn, query, evidence)
    table = FactorTable()
    for i in 1:(M.m)
        a = rand(bn)
        if all(a[k] == v for (k, v) in pairs(evidence))
            b = select(a, query)
            table[b] = get(table, b, 0) + 1
        end
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

struct LikelihoodWeightedSampling
    m # number of samples
end
function infer(M::LikelihoodWeightedSampling, bn, query, evidence)
    table = FactorTable()
    ordering = topological_sort(bn.graph)
    for i in 1:(M.m)
        a, w = Assignment(), 1.0
        for j in ordering
            name, Ï• = bn.vars[j].name, bn.factors[j]
            if haskey(evidence, name)
                a[name] = evidence[name]
                w *= Ï•.table[select(a, variablenames(Ï•))]
            else
                a[name] = rand(condition(Ï•, a))[name]
            end
        end
        b = select(a, query)
        table[b] = get(table, b, 0) + w
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function blanket(bn, a, i)
    name = bn.vars[i].name
    val = a[name]
    a = delete!(copy(a), name)
    Î¦ = filter(Ï• -> in_scope(name, Ï•), bn.factors)
    Ï• = prod(condition(Ï•, a) for Ï• in Î¦)
    return normalize!(Ï•)
end

function update_gibbs_sample!(a, bn, evidence, ordering)
    for i in ordering
        name = bn.vars[i].name
        if !haskey(evidence, name)
            b = blanket(bn, a, i)
            a[name] = rand(b)[name]
        end
end
end

function gibbs_sample!(a, bn, evidence, ordering, m)
    for j in 1:m
        update_gibbs_sample!(a, bn, evidence, ordering)
end
end
struct GibbsSampling
    m_samples # number of samples to use
    m_burnin # number of samples to discard during burn-in
    m_skip # number of samples to skip for thinning
    ordering # array of variable indices
end

function infer(M::GibbsSampling, bn, query, evidence)
    table = FactorTable()
    a = merge(rand(bn), evidence)
    gibbs_sample!(a, bn, evidence, M.ordering, M.m_burnin)
    for i in 1:(M.m_samples)
        gibbs_sample!(a, bn, evidence, M.ordering, M.m_skip)
        b = select(a, query)
        table[b] = get(table, b, 0) + 1
    end
    vars = filter(v -> v.name âˆˆ query, bn.vars)
    return normalize!(Factor(vars, table))
end

function infer(D::MvNormal, query, evidencevars, evidence)
    Î¼, Î£ = D.Î¼, D.Î£.mat
    b, Î¼a, Î¼b = evidence, Î¼[query], Î¼[evidencevars]
    A = Î£[query,query]
    B = Î£[evidencevars,evidencevars]
    C = Î£[query,evidencevars]
    Î¼ = Î¼[query] + C * (B \ (b - Î¼b))
    Î£ = A - C * (B \ C')
    return MvNormal(Î¼, Î£)
end

function sub2ind(siz, x)
    k = vcat(1, cumprod(siz[1:end - 1]))
    return dot(k, x .- 1) + 1
end
function statistics(vars, G, D::Matrix{Int})
    n = size(D, 1)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    M = [zeros(q[i], r[i]) for i in 1:n]
    for o in eachcol(D)
        for i in 1:n
            k = o[i]
            parents = inneighbors(G, i)
            j = 1
            if !isempty(parents)
                j = sub2ind(r[parents], o[parents])
            end
            M[i][j,k] += 1.0
        end
    end
    return M
end
    
# G = SimpleDiGraph(3)
# add_edge!(G, 1, 2)
# add_edge!(G, 3, 2)
# vars = [Variable(:A,2), Variable(:B,2), Variable(:C,2)]
# D = [1 2 2 1; 1 2 2 1; 2 2 2 2]
# M = statistics(vars, G, D)

# Î¸ = [mapslices(x->normalize(x,1), Mi, dims=2) for Mi in M]

function prior(vars, G)
    n = length(vars)
    r = [vars[i].m for i in 1:n]
    q = [prod([r[j] for j in inneighbors(G, i)]) for i in 1:n]
    return [ones(q[i], r[i]) for i in 1:n]
end

gaussian_kernel(b) = x -> pdf(Normal(0, b), x)

function kernel_density_estimate(Ï•, O)
    return x -> sum([Ï•(x - o) for o in O]) / length(O)
end

function bayesian_score_component(M, Î±)
    p = sum(loggamma.(Î± + M))
    p -= sum(loggamma.(Î±))
    p += sum(loggamma.(sum(Î±, dims=2)))
    p -= sum(loggamma.(sum(Î±, dims=2) + sum(M, dims=2)))
    return p
end
function bayesian_score(vars, G, D)
    n = length(vars)
    M = statistics(vars, G, D)
    Î± = prior(vars, G)
    return sum(bayesian_score_component(M[i], Î±[i]) for i in 1:n)
end

struct K2Search
    ordering::Vector{Int} # variable ordering
end
function fit(method::K2Search, vars, D)
    G = SimpleDiGraph(length(vars))
    for (k, i) in enumerate(method.ordering[2:end])
        y = bayesian_score(vars, G, D)
        while true
            y_best, j_best = -Inf, 0
            for j in method.ordering[1:k]
                if !has_edge(G, j, i)
                add_edge!(G, j, i)
                yâ€² = bayesian_score(vars, G, D)
                    if yâ€² > y_best
                        y_best, j_best = yâ€², j
                    end
                rem_edge!(G, j, i)
                end
            end
            if y_best > y
                y = y_best
                add_edge!(G, j_best, i)
            else
                break
            end
        end
    end
    return G
end

struct LocalDirectedGraphSearch
    # initial graph
    G
    k_max # number of iterations
end
function rand_graph_neighbor(G)
    n = nv(G)
    i = rand(1:n)
    j = mod1(i + rand(2:n) - 1, n)
    Gâ€² = copy(G)
    has_edge(G, i, j) ? rem_edge!(Gâ€², i, j) : add_edge!(Gâ€², i, j)
    return Gâ€²
end
function fit(method::LocalDirectedGraphSearch, vars, D)
    G = method.G
    y = bayesian_score(vars, G, D)
    for k in 1:method.k_max
        Gâ€² = rand_graph_neighbor(G)
        yâ€² = is_cyclic(Gâ€²) ? -Inf : bayesian_score(vars, Gâ€², D)
        if yâ€² > y
            y, G = yâ€², Gâ€²
        end
    end
    return G
end

### HERE
function are_markov_equivalent(G, H)
    if  nv(G) != nv(H) || ne(G) != ne(H) ||
        !all(has_edge(H, e) ||
        has_edge(H, reverse(e))
            for e in edges(G))
            return false
    end
    for c in 1:nv(G)
        parents = inneighbors(G, c)
        for (a, b) in subsets(parents, 2)
            if  !has_edge(G, a, b) && !has_edge(G, b, a) &&
                !(has_edge(H, a, c) && has_edge(H, b, c))
                    return false
            end
        end
    end
    return true
end

struct SimpleProblem
    bn::BayesianNetwork
    chance_vars::Vector{Variable}
    decision_vars::Vector{Variable}
    utility_vars::Vector{Variable}
    utilities::Dict{Symbol, Vector{Float64}}
end
function solve(ğ’«::SimpleProblem, evidence, M)
    query = [var.name for var in ğ’«.utility_vars]
    U(a) = sum(ğ’«.utilities[uname][a[uname]] for uname in query)
    best = (a=nothing, u=-Inf)
    for assignment in assignments(ğ’«.decision_vars)
        evidence = merge(evidence, assignment)
        Ï• = infer(M, ğ’«.bn, query, evidence)
        u = sum(p*U(a) for (a, p) in Ï•.table)
        if u > best.u
            best = (a=assignment, u=u)
        end
    end
    return best
end

function value_of_information(ğ’«, query, evidence, M)
    Ï• = infer(M, ğ’«.bn, query, evidence)
    voi = -solve(ğ’«, evidence, M).u
    query_vars = filter(v->v.name âˆˆ query, ğ’«.chance_vars)
    for oâ€² in assignments(query_vars)
        ooâ€² = merge(evidence, oâ€²)
        p = Ï•.table[oâ€²]
        voi += p*solve(ğ’«, ooâ€², M).u
    end
    return voi
end

struct MDP
    Î³  # discount factor
    ğ’®  # state space
    ğ’œ # action space
    T  # transition function
    R  # reward function
    TR # sample transition and reward
end

function lookahead(ğ’«::MDP, U, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U(sâ€²) for sâ€² in ğ’®)
end
function lookahead(ğ’«::MDP, U::Vector, s, a)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    return R(s,a) + Î³*sum(T(s,a,sâ€²)*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

function iterative_policy_evaluation(ğ’«::MDP, Ï€, k_max)
    ğ’®, T, R, Î³ = ğ’«.ğ’®, ğ’«.T, ğ’«.R, ğ’«.Î³
    U = [0.0 for s in ğ’®]
    for k in 1:k_max
        U = [lookahead(ğ’«, U, s, Ï€(s)) for s in ğ’®]
    end
    return U
end

function policy_evaluation(ğ’«::MDP, Ï€)
    ğ’®, R, T, Î³ = ğ’«.ğ’®, ğ’«.R, ğ’«.T, ğ’«.Î³
    Râ€² = [R(s, Ï€(s)) for s in ğ’®]
    Tâ€² = [T(s, Ï€(s), sâ€²) for s in ğ’®, sâ€² in ğ’®]
    return (I - Î³*Tâ€²)\Râ€²
end

struct ValueFunctionPolicy
    ğ’« # problem
    U # utility function
end
function greedy(ğ’«::MDP, U, s)
    u, a = findmax(a->lookahead(ğ’«, U, s, a), ğ’«.ğ’œ)
    return (a=a, u=u)
end











