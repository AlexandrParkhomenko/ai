1
00:00:00,000 --> 00:00:04,090


2
00:00:04,090 --> 00:00:05,840
CHRISTOPHER POTTS:
Welcome back, everyone.

3
00:00:05,840 --> 00:00:07,810
This is part three
in our series on NLI.

4
00:00:07,810 --> 00:00:10,480
This is our chance to start
getting a little introspective,

5
00:00:10,480 --> 00:00:13,240
and think more about developing
truly robust systems.

6
00:00:13,240 --> 00:00:16,149
So our topic is going
to be Dataset Artifacts

7
00:00:16,149 --> 00:00:18,620
And Adversarial Testing.

8
00:00:18,620 --> 00:00:21,800
I'm pleased to report that a lot
of the discussion in this area

9
00:00:21,800 --> 00:00:24,320
actually traces,
in the NLI context

10
00:00:24,320 --> 00:00:26,210
to a course project
for this class.

11
00:00:26,210 --> 00:00:29,300
In 2016, Leonid
Keselman observed

12
00:00:29,300 --> 00:00:31,850
that for NLI,
hypothesis-only models

13
00:00:31,850 --> 00:00:33,540
were surprisingly strong.

14
00:00:33,540 --> 00:00:35,407
What I mean by
hypothesis-only model

15
00:00:35,407 --> 00:00:37,490
is that these are models
that literally throw away

16
00:00:37,490 --> 00:00:40,160
the premise text,
and reason entirely

17
00:00:40,160 --> 00:00:42,120
in terms of the hypothesis.

18
00:00:42,120 --> 00:00:44,330
So to the extent the
models can succeed,

19
00:00:44,330 --> 00:00:46,940
despite having no information
about the premise.

20
00:00:46,940 --> 00:00:49,100
We might really worry
about whether we're

21
00:00:49,100 --> 00:00:50,930
solving the task we
think we're solving.

22
00:00:50,930 --> 00:00:53,060
Because after all
NLI is supposed

23
00:00:53,060 --> 00:00:55,760
to be about the reasoning
relationship between premise

24
00:00:55,760 --> 00:00:57,020
and hypothesis.

25
00:00:57,020 --> 00:00:59,690
And you would not expect
it to be successful if you

26
00:00:59,690 --> 00:01:01,530
were given only the hypothesis.

27
00:01:01,530 --> 00:01:04,310
But what Leonid observed,
is that these models

28
00:01:04,310 --> 00:01:06,530
were surprisingly strong.

29
00:01:06,530 --> 00:01:08,630
Subsequently, and I think
partly independently,

30
00:01:08,630 --> 00:01:11,930
a number of other groups made
the same sort of observation

31
00:01:11,930 --> 00:01:14,480
about a variety
of NLI benchmarks.

32
00:01:14,480 --> 00:01:16,850
And that leads us to the
conclusion that at least

33
00:01:16,850 --> 00:01:20,900
for SNLI, averaging across a
whole lot of different systems.

34
00:01:20,900 --> 00:01:23,330
Hypothesis-only
baselines are typically

35
00:01:23,330 --> 00:01:27,410
operating in the range of
about 65% to 70% accuracy.

36
00:01:27,410 --> 00:01:29,960
Again, that's eye opening
because chance performance

37
00:01:29,960 --> 00:01:31,550
would be 33%.

38
00:01:31,550 --> 00:01:33,260
And this is showing
us that there

39
00:01:33,260 --> 00:01:36,620
is some unusual bias
in the hypothesis that

40
00:01:36,620 --> 00:01:38,930
is allowing us to
neglect the premise,

41
00:01:38,930 --> 00:01:42,640
and still have a lot
of predictive capacity.

42
00:01:42,640 --> 00:01:45,340
The reason for this
is likely due to what

43
00:01:45,340 --> 00:01:47,560
we're going to call
artifacts in these datasets.

44
00:01:47,560 --> 00:01:49,510
And just for a
few examples here,

45
00:01:49,510 --> 00:01:51,490
we can observe that
specific claims

46
00:01:51,490 --> 00:01:54,040
are likely to be premises
in entailment cases.

47
00:01:54,040 --> 00:01:55,750
And correspondingly
general claims

48
00:01:55,750 --> 00:01:58,870
are likely to be hypotheses
in entailment cases.

49
00:01:58,870 --> 00:02:01,090
Just think of the
lexical level if you have

50
00:02:01,090 --> 00:02:03,700
a pair like turtle and animal.

51
00:02:03,700 --> 00:02:05,920
Where you have a very
specific term in the premise,

52
00:02:05,920 --> 00:02:08,288
and a very general
term in the hypothesis.

53
00:02:08,288 --> 00:02:10,059
That's an entailment
relation, and if you

54
00:02:10,060 --> 00:02:12,670
did just drop off the
premise and looked only

55
00:02:12,670 --> 00:02:15,010
at the hypothesis
"animal" you might still

56
00:02:15,010 --> 00:02:16,510
have a pretty good
guess that that's

57
00:02:16,510 --> 00:02:18,190
going to be an
entailment case in virtue

58
00:02:18,190 --> 00:02:21,370
of the generality
of the second term.

59
00:02:21,370 --> 00:02:23,290
And relatedly
specific claims are

60
00:02:23,290 --> 00:02:24,760
likely to lead to contradiction.

61
00:02:24,760 --> 00:02:28,160
The common strategy for
creating a contradiction pair,

62
00:02:28,160 --> 00:02:30,160
is to just make sure you
have two sentences that

63
00:02:30,160 --> 00:02:34,360
exclude each other, in virtue
of being very specific.

64
00:02:34,360 --> 00:02:37,420
So it's in virtue of patterns
like this, that a system denied

65
00:02:37,420 --> 00:02:39,340
the premise, can
nonetheless succeed

66
00:02:39,340 --> 00:02:44,590
at about 65% to 70% accuracy.

67
00:02:44,590 --> 00:02:46,340
Let's get a little
more precise about what

68
00:02:46,340 --> 00:02:47,715
we mean by an
artifact, because I

69
00:02:47,715 --> 00:02:50,060
think we need to think
about this in a nuanced way.

70
00:02:50,060 --> 00:02:54,470
So my definition is
that a dataset artifact

71
00:02:54,470 --> 00:02:56,720
is a bias that
would make a system

72
00:02:56,720 --> 00:02:59,060
susceptible to
adversarial attack.

73
00:02:59,060 --> 00:03:01,922
Even if the bias is
linguistically motivated.

74
00:03:01,922 --> 00:03:03,380
And let me give
you an example that

75
00:03:03,380 --> 00:03:05,090
brings out the nuance there.

76
00:03:05,090 --> 00:03:07,700
Consider negated
hypotheses tending

77
00:03:07,700 --> 00:03:10,010
to signal contradiction.

78
00:03:10,010 --> 00:03:12,320
This is a very natural
thing linguistically

79
00:03:12,320 --> 00:03:14,990
if you give me a sentence
like; the dog barked.

80
00:03:14,990 --> 00:03:17,540
And you asked me to construct
a contradictory sentence,

81
00:03:17,540 --> 00:03:19,920
it's very natural for me
to say the dog didn't bark,

82
00:03:19,920 --> 00:03:22,070
by simply inserting a negation.

83
00:03:22,070 --> 00:03:23,900
So it's not a surprise
that this happens,

84
00:03:23,900 --> 00:03:25,370
it's linguistically motivated.

85
00:03:25,370 --> 00:03:27,870
Negation is our best way of
establishing the relevant kind

86
00:03:27,870 --> 00:03:29,730
of connection.

87
00:03:29,730 --> 00:03:32,330
However, here's the reason
that this is an artifact.

88
00:03:32,330 --> 00:03:35,840
We could easily curate a
data set, in which negation

89
00:03:35,840 --> 00:03:38,120
correlated with
the other labels,

90
00:03:38,120 --> 00:03:40,670
but nonetheless, this led
to no human confusion.

91
00:03:40,670 --> 00:03:43,460
Because humans are not really
operating at this general level

92
00:03:43,460 --> 00:03:44,960
of a dataset bias.

93
00:03:44,960 --> 00:03:46,938
They are thinking about
individual examples,

94
00:03:46,938 --> 00:03:47,730
and what they mean.

95
00:03:47,730 --> 00:03:49,940
But we know that our
systems are going

96
00:03:49,940 --> 00:03:52,460
to be very sensitive to
the distributions of things

97
00:03:52,460 --> 00:03:53,810
in their training data.

98
00:03:53,810 --> 00:03:57,050
And that's the sense in which we
can be adversarial in this way.

99
00:03:57,050 --> 00:04:00,170
And expose that a system has
overfit to a certain kind

100
00:04:00,170 --> 00:04:02,450
of regularity.

101
00:04:02,450 --> 00:04:03,960
Here are some known artifacts.

102
00:04:03,960 --> 00:04:05,510
This is by no means
exhaustive, but I

103
00:04:05,510 --> 00:04:08,177
think this will give you a sense
for the kind of things that you

104
00:04:08,177 --> 00:04:10,130
want to look out for
in an NLI context

105
00:04:10,130 --> 00:04:14,088
but also generally in
dealing with problems in NLU.

106
00:04:14,088 --> 00:04:16,130
So it's been observed that
these datasets contain

107
00:04:16,130 --> 00:04:18,230
words whose appearance
nearly perfectly

108
00:04:18,230 --> 00:04:19,640
correlates with specific labels.

109
00:04:19,640 --> 00:04:21,980
And what I mean here is
just randomly chosen words

110
00:04:21,980 --> 00:04:23,570
like cat and dog.

111
00:04:23,570 --> 00:04:26,030
The reason for this is probably
that crowd workers were

112
00:04:26,030 --> 00:04:27,740
producing a lot of examples.

113
00:04:27,740 --> 00:04:29,540
And they fell into
a pattern of making

114
00:04:29,540 --> 00:04:31,370
specific lexical choices.

115
00:04:31,370 --> 00:04:35,820
And that created a spurious
bias for one label or another.

116
00:04:35,820 --> 00:04:38,070
Entailment hypotheses
overrepresent

117
00:04:38,070 --> 00:04:39,480
general and approximating words.

118
00:04:39,480 --> 00:04:41,760
We've seen that that's
systematic in terms

119
00:04:41,760 --> 00:04:43,200
of the linguistic patterns.

120
00:04:43,200 --> 00:04:45,210
But it's an artifact
because the world

121
00:04:45,210 --> 00:04:47,880
needn't be this way
for humans to succeed,

122
00:04:47,880 --> 00:04:50,100
in making their predictions.

123
00:04:50,100 --> 00:04:52,680
Neutral hypotheses often
introduce modifiers.

124
00:04:52,680 --> 00:04:54,510
This was a way
that workers found

125
00:04:54,510 --> 00:04:57,300
to create statements
that excluded each other

126
00:04:57,300 --> 00:04:59,960
with simple modifications.

127
00:04:59,960 --> 00:05:02,600
Contradiction hypotheses
overrepresent negation,

128
00:05:02,600 --> 00:05:04,670
we've seen that and
that makes sense.

129
00:05:04,670 --> 00:05:06,680
And neutral hypotheses
tend to be longer.

130
00:05:06,680 --> 00:05:08,600
And that last one is
yet again another case

131
00:05:08,600 --> 00:05:10,820
where that's the
sort of regularity

132
00:05:10,820 --> 00:05:13,940
that our systems are going to
be very good at picking up on.

133
00:05:13,940 --> 00:05:15,470
But that humans
will probably not

134
00:05:15,470 --> 00:05:18,020
make direct use of in making
predictions of their own.

135
00:05:18,020 --> 00:05:21,830
And in that way we can easily
leverage this observation

136
00:05:21,830 --> 00:05:24,920
to create an adversarial
setting for one of our models

137
00:05:24,920 --> 00:05:28,520
where humans succeed at the
task but our models suffer.

138
00:05:28,520 --> 00:05:31,520
Because they're cued into the
wrong aspects of the underlying

139
00:05:31,520 --> 00:05:33,248
problem.

140
00:05:33,248 --> 00:05:35,040
To close this out, I
just want to emphasize

141
00:05:35,040 --> 00:05:38,290
that artifacts are discussed
a lot in the context of NLI.

142
00:05:38,290 --> 00:05:41,190
I think that's because a bunch
of big benchmarks appeared.

143
00:05:41,190 --> 00:05:44,190
And people did a lot of probing
work to understand them.

144
00:05:44,190 --> 00:05:47,070
But it does not follow that
NLI is the only task where

145
00:05:47,070 --> 00:05:48,390
the datasets have artifacts.

146
00:05:48,390 --> 00:05:51,360
In fact, I would venture that
every task that we work on,

147
00:05:51,360 --> 00:05:54,150
has associated datasets
that suffer from artifacts.

148
00:05:54,150 --> 00:05:57,750
Here, I've given you a sample
from prominent NLU problems

149
00:05:57,750 --> 00:05:59,250
where people have
found artifacts

150
00:05:59,250 --> 00:06:02,040
that are similar to the ones
that I just covered for NLI.

151
00:06:02,040 --> 00:06:04,803
And the overall lesson is
clear, whatever problem

152
00:06:04,803 --> 00:06:06,720
you're working on, you
should think critically

153
00:06:06,720 --> 00:06:10,170
about your data, and how
idiosyncrasies in that data

154
00:06:10,170 --> 00:06:12,330
might be affecting
system performance.

155
00:06:12,330 --> 00:06:14,640
And creating a distance
between what you think you're

156
00:06:14,640 --> 00:06:16,410
doing, in terms of
problem solving.

157
00:06:16,410 --> 00:06:19,050
And what your system
is actually doing.

158
00:06:19,050 --> 00:06:21,690
And one way we can expose
this is via efforts

159
00:06:21,690 --> 00:06:23,403
involving adversarial testing.

160
00:06:23,403 --> 00:06:25,320
We're going to have a
whole discussion of this

161
00:06:25,320 --> 00:06:26,200
later in the quarter.

162
00:06:26,200 --> 00:06:28,290
But I do want to
plant the idea here.

163
00:06:28,290 --> 00:06:29,760
In adversarial
testing we're going

164
00:06:29,760 --> 00:06:32,400
to take standard NLI
examples like this

165
00:06:32,400 --> 00:06:35,550
and modify them in ways
that expose that systems

166
00:06:35,550 --> 00:06:37,440
have surprising gaps.

167
00:06:37,440 --> 00:06:39,330
So a prominent early
example of this,

168
00:06:39,330 --> 00:06:41,220
is this wonderful paper
by Glockner et al.

169
00:06:41,220 --> 00:06:43,022
called Breaking NLI.

170
00:06:43,022 --> 00:06:44,480
What they did is
very simple, we're

171
00:06:44,480 --> 00:06:46,968
going to operate with
actual SNLI examples.

172
00:06:46,968 --> 00:06:48,510
Here we would start
with the premise;

173
00:06:48,510 --> 00:06:50,950
A little girl is kneeling
in the dirt crying.

174
00:06:50,950 --> 00:06:52,710
And: A little girl
is very sad, which

175
00:06:52,710 --> 00:06:56,160
is an actual SNLI example
with the entailment relation.

176
00:06:56,160 --> 00:06:57,660
We'll fix the
premise, what they did

177
00:06:57,660 --> 00:07:00,120
to create their adversarial
dataset is simply swap

178
00:07:00,120 --> 00:07:01,410
out "sad" for "Unhappy".

179
00:07:01,410 --> 00:07:04,230
That is to create pairs
that differ only according

180
00:07:04,230 --> 00:07:06,450
to the synonyms that they use.

181
00:07:06,450 --> 00:07:08,760
This is only mildly
adversarial, but you can see

182
00:07:08,760 --> 00:07:09,990
why this might be difficult.

183
00:07:09,990 --> 00:07:13,530
Unhappy contains a negation,
and we might have a hypothesis

184
00:07:13,530 --> 00:07:15,510
that systems will then
predict contradiction

185
00:07:15,510 --> 00:07:18,750
because they've overfit to
the association of negation

186
00:07:18,750 --> 00:07:19,980
with contradiction.

187
00:07:19,980 --> 00:07:21,660
And that's exactly
the kind of pattern

188
00:07:21,660 --> 00:07:23,250
that Glockner et al. found.

189
00:07:23,250 --> 00:07:26,520
And the overall takeaway here
is that these systems that we've

190
00:07:26,520 --> 00:07:29,123
developed are not
behaving systematically

191
00:07:29,123 --> 00:07:30,540
in the sense of
cognitive science,

192
00:07:30,540 --> 00:07:33,540
they're not behaving according
to the kinds of underlying

193
00:07:33,540 --> 00:07:36,090
intuitions that human
language users have.

194
00:07:36,090 --> 00:07:37,500
Their patterns of
errors are more

195
00:07:37,500 --> 00:07:39,900
surprising and
idiosyncratic, leading

196
00:07:39,900 --> 00:07:43,350
us to worry about their
true capacity to generalize.

197
00:07:43,350 --> 00:07:44,770
Let me show you
one other example.

198
00:07:44,770 --> 00:07:47,280
This is from Nie et al. and it
involves syntactic variation.

199
00:07:47,280 --> 00:07:49,830
So in this case, we'll
take actual examples

200
00:07:49,830 --> 00:07:53,208
and fix the hypothesis,
but now vary the premise.

201
00:07:53,208 --> 00:07:54,750
So this is one of
their manipulations

202
00:07:54,750 --> 00:07:57,750
where they simply swap
the subject and object.

203
00:07:57,750 --> 00:08:00,210
Our original premise is;
A woman is pulling a child

204
00:08:00,210 --> 00:08:01,650
on a sled in the snow.

205
00:08:01,650 --> 00:08:04,590
That entails a child is sitting
on a sled in the snow, that's

206
00:08:04,590 --> 00:08:05,340
good.

207
00:08:05,340 --> 00:08:07,830
We would expect that if
we modify the premise so

208
00:08:07,830 --> 00:08:11,100
that the subject is a child,
and the object is a woman.

209
00:08:11,100 --> 00:08:13,200
This would create the
neutral label with respect

210
00:08:13,200 --> 00:08:15,090
to our fixed hypothesis.

211
00:08:15,090 --> 00:08:17,310
But perhaps
unsurprisingly, Nie et al.

212
00:08:17,310 --> 00:08:20,190
found that many systems were
not sensitive to this change

213
00:08:20,190 --> 00:08:21,150
in the premise.

214
00:08:21,150 --> 00:08:24,660
Revealing that they were mainly
making use of the bag of words,

215
00:08:24,660 --> 00:08:25,590
so to speak.

216
00:08:25,590 --> 00:08:28,560
And not truly attuned to
the syntactic structure.

217
00:08:28,560 --> 00:08:31,680
And that's still productive
because that specific lesson

218
00:08:31,680 --> 00:08:34,320
might lead us to go
looking for systems that

219
00:08:34,320 --> 00:08:36,000
are more sensitive to syntax.

220
00:08:36,000 --> 00:08:38,460
And therefore, would be not
susceptible to this kind

221
00:08:38,460 --> 00:08:39,480
of adversary.

222
00:08:39,480 --> 00:08:41,770
And in that kind of
productive back and forth,

223
00:08:41,770 --> 00:08:44,520
I think we can triangulate
on even better systems

224
00:08:44,520 --> 00:08:47,180
for the next generation.

225
00:08:47,180 --> 00:08:51,000


