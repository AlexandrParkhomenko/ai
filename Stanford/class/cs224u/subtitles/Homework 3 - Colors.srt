1
00:00:00,000 --> 00:00:04,800


2
00:00:04,800 --> 00:00:06,550
CHRISTOPHER POTTS:
Welcome back, everyone.

3
00:00:06,550 --> 00:00:09,720
This screencast is an overview
of the homework and bakeoff

4
00:00:09,720 --> 00:00:11,970
associated with our unit
on grounded language

5
00:00:11,970 --> 00:00:13,000
understanding.

6
00:00:13,000 --> 00:00:14,625
More than any of the
other assignments,

7
00:00:14,625 --> 00:00:16,170
what we're asking
you to do here,

8
00:00:16,170 --> 00:00:19,200
is essentially develop a
fully integrated system,

9
00:00:19,200 --> 00:00:21,460
that addresses our task.

10
00:00:21,460 --> 00:00:23,130
So the distinction
between homework

11
00:00:23,130 --> 00:00:25,230
questions and original
system questions

12
00:00:25,230 --> 00:00:27,900
is kind of getting blurred here
in the interest of having you

13
00:00:27,900 --> 00:00:29,730
devote all your
energy to developing

14
00:00:29,730 --> 00:00:32,439
a cool original system
for this problem.

15
00:00:32,439 --> 00:00:34,410
So because of that
I'm going to use

16
00:00:34,410 --> 00:00:37,320
some slides to give you an
overview of the entire problem.

17
00:00:37,320 --> 00:00:39,510
And how we're thinking
about evaluation

18
00:00:39,510 --> 00:00:43,160
and how the questions feed
into these overall goals.

19
00:00:43,160 --> 00:00:46,510
So recall that our core task is
the Stanford colors and context

20
00:00:46,510 --> 00:00:47,320
task.

21
00:00:47,320 --> 00:00:49,480
And we're going to take
the speaker's perspective

22
00:00:49,480 --> 00:00:49,980
primarily.

23
00:00:49,980 --> 00:00:52,570
And what that means is that
the inputs to our model

24
00:00:52,570 --> 00:00:54,370
are sequences of
three color patches.

25
00:00:54,370 --> 00:00:56,590
One of them designated
as the target,

26
00:00:56,590 --> 00:01:00,160
and the task is to generate
a description of the target,

27
00:01:00,160 --> 00:01:03,790
in that particular context.

28
00:01:03,790 --> 00:01:05,660
The core model
that will be using,

29
00:01:05,660 --> 00:01:08,140
which is in
torch_color_describer

30
00:01:08,140 --> 00:01:10,270
is an Encoder/Decoder
architecture.

31
00:01:10,270 --> 00:01:12,640
And the way it works
is on the encoder side

32
00:01:12,640 --> 00:01:14,440
you have a sequence
of three colors.

33
00:01:14,440 --> 00:01:18,825
And we always put the target
color in the third position.

34
00:01:18,825 --> 00:01:20,950
So those are the inputs,
and then the decoding step

35
00:01:20,950 --> 00:01:24,680
is essentially to describe
the target in that context.

36
00:01:24,680 --> 00:01:26,770
So that's the natural
language generation part

37
00:01:26,770 --> 00:01:29,080
and we've covered
this core architecture

38
00:01:29,080 --> 00:01:30,460
in previous screencasts.

39
00:01:30,460 --> 00:01:32,680
And I'll return to some
of the modifications

40
00:01:32,680 --> 00:01:37,110
that you see here in the
context of question four.

41
00:01:37,110 --> 00:01:39,405
There's a separate notebook
called Colors Overview

42
00:01:39,405 --> 00:01:40,530
that you should start with.

43
00:01:40,530 --> 00:01:42,613
It gives you a sense for
what the dataset is like,

44
00:01:42,613 --> 00:01:44,880
and also what our
modeling code is like.

45
00:01:44,880 --> 00:01:47,430
Here you can see that I've
loaded in the corpus itself,

46
00:01:47,430 --> 00:01:50,460
it's got about 47,000
examples in it.

47
00:01:50,460 --> 00:01:52,050
And each one of
those examples has

48
00:01:52,050 --> 00:01:54,880
a number of different attributes
that you should be aware of.

49
00:01:54,880 --> 00:01:58,260
So here's a typical example,
the first one in the corpus.

50
00:01:58,260 --> 00:02:00,490
Fundamentally, you have
these three color patches.

51
00:02:00,490 --> 00:02:03,660
And you can see this display is
marking out the target, as well

52
00:02:03,660 --> 00:02:05,280
as an utterance.

53
00:02:05,280 --> 00:02:08,580
Each one of these colors is
encoded as a triple and HSV

54
00:02:08,580 --> 00:02:09,240
values.

55
00:02:09,240 --> 00:02:11,430
That is a sequence
of three floats,

56
00:02:11,430 --> 00:02:16,280
and you can see here that you
can also access the utterance.

57
00:02:16,280 --> 00:02:18,860
There are three conditions
in the underlying corpus

58
00:02:18,860 --> 00:02:20,690
that vary in their difficulty.

59
00:02:20,690 --> 00:02:22,980
In the far condition,
all three of the colors

60
00:02:22,980 --> 00:02:24,480
are quite different
from each other.

61
00:02:24,480 --> 00:02:26,390
And so the task of
identifying the target

62
00:02:26,390 --> 00:02:28,700
is typically pretty
easy, here the person

63
00:02:28,700 --> 00:02:31,140
just had to say purple.

64
00:02:31,140 --> 00:02:33,450
In the split condition,
two of the colors

65
00:02:33,450 --> 00:02:35,030
are highly confusable.

66
00:02:35,030 --> 00:02:37,620
So you can see here that
we have two green colors.

67
00:02:37,620 --> 00:02:39,220
And that pushed the
speaker to choose

68
00:02:39,220 --> 00:02:43,440
a kind of more specified
form of green in saying lime.

69
00:02:43,440 --> 00:02:45,750
And the hardest condition
is the close condition,

70
00:02:45,750 --> 00:02:47,580
and that's where all
three of the colors

71
00:02:47,580 --> 00:02:49,630
are highly similar
to each other.

72
00:02:49,630 --> 00:02:51,877
This tends to lead to
the longest descriptions.

73
00:02:51,877 --> 00:02:53,460
You can see here
that the speaker even

74
00:02:53,460 --> 00:02:56,790
took two turns, as indicated
by this boundary marker.

75
00:02:56,790 --> 00:02:59,310
To try to give their full
description; medium pink,

76
00:02:59,310 --> 00:03:00,270
the medium dark one.

77
00:03:00,270 --> 00:03:02,330
Because these colors
are so confusable.

78
00:03:02,330 --> 00:03:04,830
So you should be aware of this
difference in the conditions.

79
00:03:04,830 --> 00:03:07,247
And it might affect how you
do different kinds of modeling

80
00:03:07,247 --> 00:03:11,330
based on what the
color sequence is like.

81
00:03:11,330 --> 00:03:14,090
Now evaluation for natural
language generation systems

82
00:03:14,090 --> 00:03:15,080
is always challenging.

83
00:03:15,080 --> 00:03:16,730
And there are some
automatic metrics

84
00:03:16,730 --> 00:03:17,990
that we can use as guideposts.

85
00:03:17,990 --> 00:03:21,080
In fact we're going to use
BLEU in various places.

86
00:03:21,080 --> 00:03:23,690
But our primary
evaluation metric

87
00:03:23,690 --> 00:03:27,200
will be this task oriented
one, which brings in a listener

88
00:03:27,200 --> 00:03:28,590
perspective.

89
00:03:28,590 --> 00:03:31,400
So at a mechanical level here's
how we'll make predictions.

90
00:03:31,400 --> 00:03:35,300
For a given context, c,
consisting of three colors.

91
00:03:35,300 --> 00:03:37,910
Capital C here is
all the permutations

92
00:03:37,910 --> 00:03:39,860
of those three colors.

93
00:03:39,860 --> 00:03:42,480
Suppose that you have trained
a speaker model, PS here,

94
00:03:42,480 --> 00:03:44,485
it's a probabilistic agent.

95
00:03:44,485 --> 00:03:45,860
We're going to
think about how it

96
00:03:45,860 --> 00:03:49,040
makes predictions for all of
those different permutations.

97
00:03:49,040 --> 00:03:51,080
And take as its
prediction the level

98
00:03:51,080 --> 00:03:54,860
of a full sequence, the sequence
that it assigns the highest

99
00:03:54,860 --> 00:03:56,150
probability to.

100
00:03:56,150 --> 00:03:59,245
Given the message that
your system produced.

101
00:03:59,245 --> 00:04:00,620
And then we say
that a speaker is

102
00:04:00,620 --> 00:04:04,310
accurate in its prediction
about some context c.

103
00:04:04,310 --> 00:04:07,550
Just in case the best
sequence that it predicts,

104
00:04:07,550 --> 00:04:09,770
the highest probability
one, has the target

105
00:04:09,770 --> 00:04:11,120
in the final position.

106
00:04:11,120 --> 00:04:13,467
As designated by
our model structure.

107
00:04:13,467 --> 00:04:15,050
So in a little bit
more detail, here's

108
00:04:15,050 --> 00:04:16,410
how this works with an example.

109
00:04:16,410 --> 00:04:18,329
Suppose that our
context looks like this,

110
00:04:18,329 --> 00:04:19,980
it has these three
color patches.

111
00:04:19,980 --> 00:04:22,430
The target is always
in third position,

112
00:04:22,430 --> 00:04:24,810
and our message was blue.

113
00:04:24,810 --> 00:04:26,970
Here on the right we
have all the permutations

114
00:04:26,970 --> 00:04:28,203
of these three colors.

115
00:04:28,203 --> 00:04:30,370
And we're going to say that
your system was correct,

116
00:04:30,370 --> 00:04:33,780
if its highest probability
context given that message

117
00:04:33,780 --> 00:04:34,770
was one of these two.

118
00:04:34,770 --> 00:04:38,220
That is one of the two that has
the target in final position.

119
00:04:38,220 --> 00:04:40,380
And the system is
inaccurate to the extent

120
00:04:40,380 --> 00:04:42,390
that it stands higher
probability to one

121
00:04:42,390 --> 00:04:43,860
of these other sequences.

122
00:04:43,860 --> 00:04:45,390
Essentially, we're
saying that it's

123
00:04:45,390 --> 00:04:48,540
assigning higher probability
to some other target.

124
00:04:48,540 --> 00:04:52,507
But we do operate at the
level of these full sequences.

125
00:04:52,507 --> 00:04:54,590
All right, now let's move
into the questions here,

126
00:04:54,590 --> 00:04:56,780
and we first start
with the tokenizer.

127
00:04:56,780 --> 00:04:59,320
You're unconstrained in how
you design your tokenizer.

128
00:04:59,320 --> 00:05:01,278
You should just make sure
that you have a start

129
00:05:01,278 --> 00:05:02,510
symbol and an end symbol.

130
00:05:02,510 --> 00:05:04,820
The start symbol is important
conditioning context

131
00:05:04,820 --> 00:05:05,720
for the model.

132
00:05:05,720 --> 00:05:07,550
And the end symbol
is the crucial signal

133
00:05:07,550 --> 00:05:10,520
that your model will actually
stop producing tokens.

134
00:05:10,520 --> 00:05:13,040
So don't forget those pieces,
but in terms of what else

135
00:05:13,040 --> 00:05:15,110
you do in there,
it's unconstrained.

136
00:05:15,110 --> 00:05:17,540
And I think you can see
from the Monroe et al. work

137
00:05:17,540 --> 00:05:20,090
that making smart choices
about tokenization

138
00:05:20,090 --> 00:05:22,773
might be really meaningful.

139
00:05:22,773 --> 00:05:24,440
Question two asks you
to think about how

140
00:05:24,440 --> 00:05:25,565
you're representing colors.

141
00:05:25,565 --> 00:05:28,760
By default they're just going
to be those three float values,

142
00:05:28,760 --> 00:05:31,130
but that's probably not optimal.

143
00:05:31,130 --> 00:05:34,160
In the Monroe et al. paper, we
explored a Fourier transform

144
00:05:34,160 --> 00:05:35,870
as a way of embedding colors.

145
00:05:35,870 --> 00:05:38,060
And I've given you a
little recipe for that

146
00:05:38,060 --> 00:05:39,770
in the context of the notebook.

147
00:05:39,770 --> 00:05:42,330
In case you want to explore
that it is highly effective,

148
00:05:42,330 --> 00:05:43,740
but this is optional.

149
00:05:43,740 --> 00:05:45,710
And there might be other
representation schemes

150
00:05:45,710 --> 00:05:49,330
that are even better
and worth exploring.

151
00:05:49,330 --> 00:05:52,420
Question three asks you to
think about rich initialization

152
00:05:52,420 --> 00:05:53,830
or pre-training for your model.

153
00:05:53,830 --> 00:05:56,142
We've worked a lot with
pre-trained GloVe embeddings.

154
00:05:56,142 --> 00:05:58,600
And this is a chance for you
to bring those into your model

155
00:05:58,600 --> 00:05:59,860
and see how well they do.

156
00:05:59,860 --> 00:06:01,420
You should be aware
that this step

157
00:06:01,420 --> 00:06:03,820
is going to interact
in non-trivial ways,

158
00:06:03,820 --> 00:06:06,700
with choices you make
for your tokenizer.

159
00:06:06,700 --> 00:06:08,560
And question four is
the most involved,

160
00:06:08,560 --> 00:06:11,110
it involves some real
PyTorch wrangling.

161
00:06:11,110 --> 00:06:13,150
Conceptually, what
we're asking you to do

162
00:06:13,150 --> 00:06:16,210
is borrow a trick from
the Monroe et al. paper.

163
00:06:16,210 --> 00:06:17,980
What we found in
that work is that it

164
00:06:17,980 --> 00:06:20,470
helped to remind the
model, during decoding,

165
00:06:20,470 --> 00:06:22,750
of which of the three
colors was its target.

166
00:06:22,750 --> 00:06:24,730
And the way we did
that, essentially,

167
00:06:24,730 --> 00:06:27,790
was by taking the color
embedding for the target.

168
00:06:27,790 --> 00:06:29,500
And appending it
to the embedding

169
00:06:29,500 --> 00:06:32,140
of each one of the tokens
that it was producing,

170
00:06:32,140 --> 00:06:34,630
as a kind of reminder.

171
00:06:34,630 --> 00:06:36,630
In terms of how that works
at the level of code,

172
00:06:36,630 --> 00:06:39,900
there is a decoder class,
and you should modify it

173
00:06:39,900 --> 00:06:42,960
so that the input vector to
the model at each timestep

174
00:06:42,960 --> 00:06:45,150
is not just the token embedding.

175
00:06:45,150 --> 00:06:47,070
But the concatenation
of that embedding

176
00:06:47,070 --> 00:06:50,160
with the representation
of the target code.

177
00:06:50,160 --> 00:06:52,440
Then you need to modify
the Encoder/Decoder

178
00:06:52,440 --> 00:06:54,540
class to extract
the target colors,

179
00:06:54,540 --> 00:06:57,210
and feed them to
that decoder class.

180
00:06:57,210 --> 00:07:00,330
And then finally here, this
is the interface that you use.

181
00:07:00,330 --> 00:07:03,670
Modify that interface so that it
uses your decoder and encoder,

182
00:07:03,670 --> 00:07:06,200
and that's a pretty
mechanical step.

183
00:07:06,200 --> 00:07:09,560
When you're developing on
this problem use toy datasets.

184
00:07:09,560 --> 00:07:11,420
Because you don't want
to wait around as you

185
00:07:11,420 --> 00:07:13,790
process the entire
colors corpus, only

186
00:07:13,790 --> 00:07:15,860
to find out that you
have a low level bug.

187
00:07:15,860 --> 00:07:18,170
And I also encourage
you to lean on the tests

188
00:07:18,170 --> 00:07:19,970
that we have included
in the notebook

189
00:07:19,970 --> 00:07:22,610
as a way of ensuring that you
have exactly the right data

190
00:07:22,610 --> 00:07:23,570
structures.

191
00:07:23,570 --> 00:07:26,127
And assuming all those
pieces fall into place,

192
00:07:26,127 --> 00:07:27,710
I think you'll find
that the resulting

193
00:07:27,710 --> 00:07:30,365
models are substantially
better for our task.

194
00:07:30,365 --> 00:07:32,940


195
00:07:32,940 --> 00:07:34,715
That brings us to
the original system.

196
00:07:34,715 --> 00:07:36,090
And here's just
some expectations

197
00:07:36,090 --> 00:07:38,610
about how we think you
might work on this problem.

198
00:07:38,610 --> 00:07:40,740
You could iteratively
improve your answers

199
00:07:40,740 --> 00:07:43,350
to the assignment questions as
part of the original system.

200
00:07:43,350 --> 00:07:47,022
Modify the tokenizer, think
about your GloVe embeddings,

201
00:07:47,022 --> 00:07:48,480
think about how
you're representing

202
00:07:48,480 --> 00:07:52,170
colors, and kind of how all
those pieces are interacting.

203
00:07:52,170 --> 00:07:54,840
You might want to extend
the modified Encoder/Decoder

204
00:07:54,840 --> 00:07:57,030
classes to do new and
interesting things.

205
00:07:57,030 --> 00:07:58,530
And I have provided
guidance on how

206
00:07:58,530 --> 00:08:00,300
to do that at a
mechanical level,

207
00:08:00,300 --> 00:08:04,060
in the colors overview notebook.

208
00:08:04,060 --> 00:08:06,910
Any data that you can
find is fine to bring in

209
00:08:06,910 --> 00:08:10,420
for development, and for
training your original system.

210
00:08:10,420 --> 00:08:12,628
The bake-off involves
a new test set,

211
00:08:12,628 --> 00:08:14,420
that's never been
released anywhere before,

212
00:08:14,420 --> 00:08:16,540
it's just used in this context.

213
00:08:16,540 --> 00:08:18,940
It's got the same
kinds of color context

214
00:08:18,940 --> 00:08:20,980
as in the released corpus.

215
00:08:20,980 --> 00:08:23,590
But it was one-off games
rather than iterated games.

216
00:08:23,590 --> 00:08:26,410
And I do think that makes this
test set a little bit easier

217
00:08:26,410 --> 00:08:28,842
than the training set.

218
00:08:28,842 --> 00:08:30,800
And all the items have
been listener-validated.

219
00:08:30,800 --> 00:08:32,258
So I think all the
descriptions are

220
00:08:32,258 --> 00:08:34,460
in principle good
descriptions at a human level.

221
00:08:34,460 --> 00:08:38,380
And so it should be a
good basis for evaluation.

222
00:08:38,380 --> 00:08:42,000


