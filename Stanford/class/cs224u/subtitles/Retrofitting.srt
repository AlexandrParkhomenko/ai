1
00:00:04,253 --> 00:00:05,670
CHRISTOPHER POTTS:
Hello everyone.

2
00:00:05,670 --> 00:00:07,795
Welcome to part 6 in our
series on distributed word

3
00:00:07,795 --> 00:00:08,660
representations.

4
00:00:08,660 --> 00:00:10,370
This can be considered
an optional part

5
00:00:10,370 --> 00:00:13,460
but it's on the irresistibly
cool idea of retrofitting

6
00:00:13,460 --> 00:00:16,100
vectors to knowledge graphs.

7
00:00:16,100 --> 00:00:17,480
Here are the central goals.

8
00:00:17,480 --> 00:00:20,360
On the one hand, as we've seen,
distributional representations

9
00:00:20,360 --> 00:00:23,030
are powerful and
also easy to obtain,

10
00:00:23,030 --> 00:00:26,300
but they tend to reflect only
relatively primitive semantic

11
00:00:26,300 --> 00:00:29,420
notions like similarity, or
synonymy, or connotation,

12
00:00:29,420 --> 00:00:32,659
or relatedness, so that
might feel limiting.

13
00:00:32,659 --> 00:00:34,880
On the other hand,
structured resources

14
00:00:34,880 --> 00:00:37,520
like knowledge graphs, while
sparse and kind of hard

15
00:00:37,520 --> 00:00:41,000
to obtain, support
really rich learning

16
00:00:41,000 --> 00:00:44,180
of very diverse
semantic distinctions.

17
00:00:44,180 --> 00:00:45,990
So the question
naturally arises,

18
00:00:45,990 --> 00:00:48,590
can we have the best
aspects of both of these?

19
00:00:48,590 --> 00:00:51,800
And the inspiring answer
given by retrofitting is yes.

20
00:00:51,800 --> 00:00:53,263
We can combine them.

21
00:00:53,263 --> 00:00:54,680
The original method
for doing this

22
00:00:54,680 --> 00:00:57,860
is due to this lovely
paper Faruqui et al., 2015,

23
00:00:57,860 --> 00:01:01,040
which I'm going to be giving a
brief summary of in the screen

24
00:01:01,040 --> 00:01:02,720
cast.

25
00:01:02,720 --> 00:01:04,430
So here is the
retrofitting model.

26
00:01:04,430 --> 00:01:07,130
It consists of two sums
and they constitute

27
00:01:07,130 --> 00:01:09,480
kind of opposing forces.

28
00:01:09,480 --> 00:01:12,950
Imagine that we have an existing
embedding space, like GloVe,

29
00:01:12,950 --> 00:01:16,490
or some embedding space that you
built yourself-- that's q-hat.

30
00:01:16,490 --> 00:01:19,400
And we're learning
these qi's and qj's.

31
00:01:19,400 --> 00:01:21,080
The term on the
left is basically

32
00:01:21,080 --> 00:01:24,230
saying, remain faithful
to those original vectors

33
00:01:24,230 --> 00:01:26,030
as you learn these
new vectors qi.

34
00:01:26,030 --> 00:01:29,420
Try not to be too dissimilar
from where you started.

35
00:01:29,420 --> 00:01:31,610
That pressure is balanced
against the pressure

36
00:01:31,610 --> 00:01:35,120
on the right, which is
saying, make representations

37
00:01:35,120 --> 00:01:39,230
that look more like the
neighbors for the current node

38
00:01:39,230 --> 00:01:42,380
in the knowledge
graph, which is defined

39
00:01:42,380 --> 00:01:46,050
by this set of
relations E. So two

40
00:01:46,050 --> 00:01:47,550
opposing pressures,
on the one hand,

41
00:01:47,550 --> 00:01:49,370
we're saying to be
faithful to the original,

42
00:01:49,370 --> 00:01:51,912
on the other hand, we're saying
look more like your neighbors

43
00:01:51,912 --> 00:01:53,090
in the knowledge graph.

44
00:01:53,090 --> 00:01:55,640
If we set alpha
to 1 and beta to 1

45
00:01:55,640 --> 00:01:58,670
over the out-degree for the
node that we're targeting,

46
00:01:58,670 --> 00:02:02,250
then we have basically
balanced these two pressures.

47
00:02:02,250 --> 00:02:04,100
If we set alpha
really large, we'll

48
00:02:04,100 --> 00:02:06,800
mostly want to stay faithful
to the original vectors.

49
00:02:06,800 --> 00:02:09,858
If we set beta
comparatively very large,

50
00:02:09,858 --> 00:02:11,900
then we'll mostly want to
look like the neighbors

51
00:02:11,900 --> 00:02:13,430
in the knowledge
graph, and we won't

52
00:02:13,430 --> 00:02:16,070
remain so tethered to the
original embedding space

53
00:02:16,070 --> 00:02:17,600
that we started with.

54
00:02:17,600 --> 00:02:19,640
This illustration
kind of nicely depicts

55
00:02:19,640 --> 00:02:20,960
what happens in the model--

56
00:02:20,960 --> 00:02:23,732
the gray vectors of the
original embedding space.

57
00:02:23,732 --> 00:02:25,190
We have these
knowledge graphs that

58
00:02:25,190 --> 00:02:27,080
connect the associated nodes.

59
00:02:27,080 --> 00:02:30,560
And because they're connected
in the retrofitting space which

60
00:02:30,560 --> 00:02:33,130
is given in white, these nodes
are kind of pulled together

61
00:02:33,130 --> 00:02:35,953
and look more similar.

62
00:02:35,953 --> 00:02:37,870
There's a bunch of code
for doing retrofitting

63
00:02:37,870 --> 00:02:39,495
in the course
repository, and I'll just

64
00:02:39,495 --> 00:02:41,890
show you a few quick
illustrations using that code.

65
00:02:41,890 --> 00:02:44,680
Let's start with a simple case,
we have a very simple knowledge

66
00:02:44,680 --> 00:02:48,520
graph where node 0 is
connected to node 1 and node 0

67
00:02:48,520 --> 00:02:52,047
is connected to node
2, just directionally.

68
00:02:52,047 --> 00:02:54,130
What happens when we run
the retrofitting model is

69
00:02:54,130 --> 00:02:58,150
that 0 is called equally
close to 1 and to 2, kind

70
00:02:58,150 --> 00:03:00,190
of equidistant between
them and closer to both

71
00:03:00,190 --> 00:03:03,670
than it was in the
original embedded space.

72
00:03:03,670 --> 00:03:05,785
Here's a situation
in which every node

73
00:03:05,785 --> 00:03:08,410
is connected to every other node
that's represented on the left

74
00:03:08,410 --> 00:03:08,910
here.

75
00:03:08,910 --> 00:03:10,690
That's where we
start, and as a result

76
00:03:10,690 --> 00:03:13,390
of running the retrofitting
model with alpha and beta set

77
00:03:13,390 --> 00:03:15,250
in their default
parameters, what happens

78
00:03:15,250 --> 00:03:16,780
is that triangle
just gets smaller

79
00:03:16,780 --> 00:03:20,350
in kind of fully symmetric
way as the nodes become

80
00:03:20,350 --> 00:03:24,198
more similar to each other
because of the graph structure.

81
00:03:24,198 --> 00:03:25,740
Here's a kind of
degenerate solution.

82
00:03:25,740 --> 00:03:28,170
If I set alpha to 0,
I have no pressure

83
00:03:28,170 --> 00:03:30,300
to be faithful to
the original vectors.

84
00:03:30,300 --> 00:03:32,490
All I care about is
looking like my neighbors

85
00:03:32,490 --> 00:03:33,693
from the term on the right.

86
00:03:33,693 --> 00:03:35,610
And as a result, all
these vectors shrink down

87
00:03:35,610 --> 00:03:37,620
to be the same point
after the models run

88
00:03:37,620 --> 00:03:39,210
for a few iterations.

89
00:03:39,210 --> 00:03:41,100
If instead I had
done the opposite,

90
00:03:41,100 --> 00:03:43,890
I've made alpha really
large comparative to beta,

91
00:03:43,890 --> 00:03:45,600
then basically
nothing would have

92
00:03:45,600 --> 00:03:47,308
happened in the learning
of the triangle.

93
00:03:47,308 --> 00:03:50,380
It would remain
its original size.

94
00:03:50,380 --> 00:03:52,120
It's worth considering
some extension.

95
00:03:52,120 --> 00:03:54,820
So I think the fundamental
limitation of this model

96
00:03:54,820 --> 00:03:58,030
is that it is kind of assuming,
right there in its objective,

97
00:03:58,030 --> 00:03:59,950
that to have an
edge between nodes

98
00:03:59,950 --> 00:04:01,450
is to say that they are similar.

99
00:04:01,450 --> 00:04:02,950
But of course, the
whole point might

100
00:04:02,950 --> 00:04:05,110
be that your knowledge
graph has very rich edge

101
00:04:05,110 --> 00:04:07,630
relations corresponding to
different linguistic notions

102
00:04:07,630 --> 00:04:09,770
like antonymy.

103
00:04:09,770 --> 00:04:12,610
And we certainly wouldn't want
to treat synonymy and antonymy

104
00:04:12,610 --> 00:04:15,580
as the same relation and just
assume that it meant similarity

105
00:04:15,580 --> 00:04:17,125
in our model.

106
00:04:17,125 --> 00:04:18,458
So there are various extensions.

107
00:04:18,459 --> 00:04:20,740
I think the most general
extension that I've seen

108
00:04:20,740 --> 00:04:22,480
is from a paper
that I was involved

109
00:04:22,480 --> 00:04:25,300
with led by Ben
Lengerich, which is called

110
00:04:25,300 --> 00:04:27,040
functional retrofitting,
which allows you

111
00:04:27,040 --> 00:04:30,280
to very flexibly learn
different retrofitting

112
00:04:30,280 --> 00:04:33,093
modes for different
edge semantics.

113
00:04:33,093 --> 00:04:34,510
And once you start
down that road,

114
00:04:34,510 --> 00:04:36,843
you have a really natural
connection with the literature

115
00:04:36,843 --> 00:04:38,380
on graph embedding,
that is learning

116
00:04:38,380 --> 00:04:42,220
distributional representations
for nodes and knowledge graphs.

117
00:04:42,220 --> 00:04:44,260
And this paper led
by Will Hamilton

118
00:04:44,260 --> 00:04:46,990
is an outstanding overview
of methods in that space.

119
00:04:46,990 --> 00:04:49,990
And then you have this nice
synergy between NLP methods

120
00:04:49,990 --> 00:04:51,520
and methods that
are more associated

121
00:04:51,520 --> 00:04:54,160
with work on knowledge
graphs and social networks

122
00:04:54,160 --> 00:04:56,252
and so forth.

123
00:04:56,252 --> 00:04:58,460
And finally, here are some
code snippets just showing

124
00:04:58,460 --> 00:05:00,710
some simple
illustrations of the sort

125
00:05:00,710 --> 00:05:02,642
that I showed you earlier
in the screen cast.

126
00:05:02,642 --> 00:05:04,100
And I would just
mention at the end

127
00:05:04,100 --> 00:05:07,850
here, if you would like to apply
these methods to WordNet, which

128
00:05:07,850 --> 00:05:10,380
could be a powerful ingredient
for the first assignment

129
00:05:10,380 --> 00:05:13,160
and bake off, I would encourage
you to check out this notebook

130
00:05:13,160 --> 00:05:16,670
bsm_03_retrofitting,
because it walks through all

131
00:05:16,670 --> 00:05:19,120
the steps for doing that.

