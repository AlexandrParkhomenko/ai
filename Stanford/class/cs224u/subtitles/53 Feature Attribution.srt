1
00:00:00,000 --> 00:00:04,698


2
00:00:04,698 --> 00:00:06,240
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:06,240 --> 00:00:08,940
This is part 5 in our series
on analysis methods in NLP.

4
00:00:08,940 --> 00:00:11,440
We're going to be talking about
feature attribution methods.

5
00:00:11,440 --> 00:00:13,870
This is fundamentally
a powerful tool kit

6
00:00:13,870 --> 00:00:16,600
for helping you understand
how the features in your model

7
00:00:16,600 --> 00:00:19,940
contribute to its
output predictions.

8
00:00:19,940 --> 00:00:22,010
Our fundamental question
here is, kind of,

9
00:00:22,010 --> 00:00:24,710
why does your model make the
predictions that it makes?

10
00:00:24,710 --> 00:00:27,140
There are many motivations
for asking this question,

11
00:00:27,140 --> 00:00:28,378
here are just a few.

12
00:00:28,378 --> 00:00:30,170
To start, you might
just want to understand

13
00:00:30,170 --> 00:00:32,180
whether your model is
systematic with regard

14
00:00:32,180 --> 00:00:34,100
to some specific
linguistic phenomenon.

15
00:00:34,100 --> 00:00:37,173
Has it actually captured
that phenomenon?

16
00:00:37,173 --> 00:00:38,840
You might also want
to know whether it's

17
00:00:38,840 --> 00:00:42,260
robust to minor
perturbations in its input.

18
00:00:42,260 --> 00:00:45,290
You might use these techniques
to diagnose unwanted biases

19
00:00:45,290 --> 00:00:45,890
in your model.

20
00:00:45,890 --> 00:00:48,920
And relatedly, you might
use them to find weaknesses

21
00:00:48,920 --> 00:00:50,570
in your model that
an adversary could

22
00:00:50,570 --> 00:00:55,010
exploit to lead your model to
do really problematic things.

23
00:00:55,010 --> 00:00:57,140
Fundamentally, I think that
this is a tool kit that

24
00:00:57,140 --> 00:00:59,870
will help you write
really excellent analysis

25
00:00:59,870 --> 00:01:01,312
sections for your paper.

26
00:01:01,312 --> 00:01:03,020
To that end, I'm going
to try to show you

27
00:01:03,020 --> 00:01:05,060
a bunch of code that
will help you get hands

28
00:01:05,060 --> 00:01:06,860
on with these techniques.

29
00:01:06,860 --> 00:01:09,200
I'll do it at a kind of high
level in the screen cast.

30
00:01:09,200 --> 00:01:11,360
And I've just contributed
this new notebook,

31
00:01:11,360 --> 00:01:14,870
feature attribution, to
the course code repository.

32
00:01:14,870 --> 00:01:17,090
And that should be
flexible and adaptable,

33
00:01:17,090 --> 00:01:18,560
and help you take
these techniques

34
00:01:18,560 --> 00:01:21,140
and apply them to
whatever models and ideas

35
00:01:21,140 --> 00:01:24,240
you're exploring
for your projects.

36
00:01:24,240 --> 00:01:27,345
The star of our show, really the
only reason that I can do this,

37
00:01:27,345 --> 00:01:30,270
is this amazing
Captum.ai library.

38
00:01:30,270 --> 00:01:34,050
It implements a wide range of
feature attribution techniques.

39
00:01:34,050 --> 00:01:37,140
We're going to talk extensively
about the integrated gradients

40
00:01:37,140 --> 00:01:39,660
method and use the
gradient-based method

41
00:01:39,660 --> 00:01:42,360
as a kind of simple
baseline for that method.

42
00:01:42,360 --> 00:01:45,210
But as you can see here,
Captum implements a wide range

43
00:01:45,210 --> 00:01:47,160
of different
algorithms, some very

44
00:01:47,160 --> 00:01:50,820
particular to specific model
designs and others completely

45
00:01:50,820 --> 00:01:53,070
agnostic about what kind
of model you're exploring.

46
00:01:53,070 --> 00:01:54,810
So it's a very
exciting tool kit.

47
00:01:54,810 --> 00:01:57,320


48
00:01:57,320 --> 00:02:00,080
The Sundararajan
et al 2017 paper

49
00:02:00,080 --> 00:02:02,210
introduced the integrated
gradients method.

50
00:02:02,210 --> 00:02:04,910
It's also a lovely contribution
because it gives us

51
00:02:04,910 --> 00:02:09,038
a kind of framework for thinking
about feature attribution

52
00:02:09,038 --> 00:02:10,399
methods in general.

53
00:02:10,400 --> 00:02:12,760
And as part of that,
they offer two axioms

54
00:02:12,760 --> 00:02:15,110
that I'm going to use to
guide this discussion.

55
00:02:15,110 --> 00:02:18,280
The first, and the more
important one, is sensitivity.

56
00:02:18,280 --> 00:02:22,330
If two inputs x and x prime
differ only at dimension i

57
00:02:22,330 --> 00:02:24,610
and lead to different
predictions,

58
00:02:24,610 --> 00:02:26,980
then the feature associated
with that dimension

59
00:02:26,980 --> 00:02:29,770
must have non-zero attribution.

60
00:02:29,770 --> 00:02:31,270
And with my simple
example here, you

61
00:02:31,270 --> 00:02:33,250
can get a sense
for why sensitivity

62
00:02:33,250 --> 00:02:35,170
is such a fundamental axiom.

63
00:02:35,170 --> 00:02:39,670
If for some model m and three
dimensional input 1, 0, 1,

64
00:02:39,670 --> 00:02:41,810
we get a prediction of positive.

65
00:02:41,810 --> 00:02:44,530
And if for that same
model the input 1, 1, 1,

66
00:02:44,530 --> 00:02:46,480
leads to the
prediction negative,

67
00:02:46,480 --> 00:02:49,510
then we really ought to expect
that the feature associated

68
00:02:49,510 --> 00:02:53,020
with the second position must
have non-zero attribution.

69
00:02:53,020 --> 00:02:56,200
Because it must be decisive
in leading the model

70
00:02:56,200 --> 00:02:59,650
to make these two
different predictions.

71
00:02:59,650 --> 00:03:01,150
The second axiom
is going to be less

72
00:03:01,150 --> 00:03:03,442
important to our discussion,
but it's nonetheless worth

73
00:03:03,442 --> 00:03:04,370
having in mind.

74
00:03:04,370 --> 00:03:06,340
It is implementation invariance.

75
00:03:06,340 --> 00:03:09,730
If two models m and m prime
have identical input/output

76
00:03:09,730 --> 00:03:12,730
behavior, then the
attributions for m and m prime

77
00:03:12,730 --> 00:03:13,880
are identical.

78
00:03:13,880 --> 00:03:16,660
This is really just saying
that the attributions we give

79
00:03:16,660 --> 00:03:20,530
should be separate from any
incidental differences in model

80
00:03:20,530 --> 00:03:23,980
implementation that don't
affect the input/output behavior

81
00:03:23,980 --> 00:03:26,750
of that model.

82
00:03:26,750 --> 00:03:28,640
To start our
discussion let's begin

83
00:03:28,640 --> 00:03:31,460
with this simple baseline,
which is simply multiplying

84
00:03:31,460 --> 00:03:33,480
the gradients by the inputs.

85
00:03:33,480 --> 00:03:35,330
This is implemented
in Captum as input

86
00:03:35,330 --> 00:03:38,090
by gradient that I'm
showing with respect

87
00:03:38,090 --> 00:03:42,380
to some particular feature
i, given model M and input x.

88
00:03:42,380 --> 00:03:45,560
And we simply get the
gradients for that feature

89
00:03:45,560 --> 00:03:49,050
and then multiply it by the
actual value of that feature.

90
00:03:49,050 --> 00:03:50,150
It's as simple as that.

91
00:03:50,150 --> 00:03:51,620
Here are two implementations.

92
00:03:51,620 --> 00:03:54,410
The first one in
cell 2 does this kind

93
00:03:54,410 --> 00:03:56,600
of using raw PyTorch
just to show you

94
00:03:56,600 --> 00:04:00,110
how we can use PyTorch's
autograd functionality

95
00:04:00,110 --> 00:04:01,880
to implement this method.

96
00:04:01,880 --> 00:04:04,190
And the second implementation
is from Captum.

97
00:04:04,190 --> 00:04:05,690
And it's probably more flexible.

98
00:04:05,690 --> 00:04:10,530
And it uses this input
by gradient class.

99
00:04:10,530 --> 00:04:12,180
To give you a full
illustration here,

100
00:04:12,180 --> 00:04:14,790
I've just set up a simple
synthetic classification

101
00:04:14,790 --> 00:04:16,950
problem using scikit tools.

102
00:04:16,950 --> 00:04:19,800
My model will be a
TorchShallowNeuralClassifier

103
00:04:19,800 --> 00:04:21,839
which I fit on that data.

104
00:04:21,839 --> 00:04:25,170
And then in cells 9 and 10, I
use those two implementations

105
00:04:25,170 --> 00:04:26,470
of this method.

106
00:04:26,470 --> 00:04:28,170
And you can see in
11 and 12, that they

107
00:04:28,170 --> 00:04:30,420
give identical outputs.

108
00:04:30,420 --> 00:04:31,860
Another thing worth
noting here is

109
00:04:31,860 --> 00:04:33,990
that I have used the
method by taking gradients

110
00:04:33,990 --> 00:04:37,200
with respect to the actual
labels in our data set.

111
00:04:37,200 --> 00:04:39,150
You can often get
a different picture

112
00:04:39,150 --> 00:04:40,620
if you take gradients
with respect

113
00:04:40,620 --> 00:04:42,637
to the predictions
of that model.

114
00:04:42,637 --> 00:04:44,220
And that might give
you a better sense

115
00:04:44,220 --> 00:04:48,390
for why the model is making
the predictions that it makes.

116
00:04:48,390 --> 00:04:50,190
In this case, since
the model is very good,

117
00:04:50,190 --> 00:04:53,990
the attributions are
only slightly different.

118
00:04:53,990 --> 00:04:55,730
That's our kind of baseline.

119
00:04:55,730 --> 00:04:58,490
I want to show you now that
the input by gradients method

120
00:04:58,490 --> 00:05:00,300
fails the sensitivity test.

121
00:05:00,300 --> 00:05:04,160
And this is an example from
the Sundararajan et al paper.

122
00:05:04,160 --> 00:05:06,090
They give this
simple model M here,

123
00:05:06,090 --> 00:05:11,330
which is effectively just ReLU
applied to 1 minus the input.

124
00:05:11,330 --> 00:05:14,745
And then you take 1 minus
that ReLU calculation there.

125
00:05:14,745 --> 00:05:15,620
And that's the model.

126
00:05:15,620 --> 00:05:18,680
It's got one dimensional
inputs and outputs.

127
00:05:18,680 --> 00:05:22,850
If you calculate for input
0, you get an outcome of 0.

128
00:05:22,850 --> 00:05:26,990
And if you give the model input
2, you get an output of 1.

129
00:05:26,990 --> 00:05:29,600
Since we have differing
output predictions,

130
00:05:29,600 --> 00:05:31,550
sensitivity tells
us that we have

131
00:05:31,550 --> 00:05:35,090
to have differing
attributions for these two

132
00:05:35,090 --> 00:05:38,450
cases here, these two
one dimensional inputs.

133
00:05:38,450 --> 00:05:40,580
But unfortunately, when
you calculate through

134
00:05:40,580 --> 00:05:45,140
with this method you get 0
attribution in both cases.

135
00:05:45,140 --> 00:05:47,270
That's a failure of
sensitivity and points

136
00:05:47,270 --> 00:05:51,260
to a weakness of this method.

137
00:05:51,260 --> 00:05:53,030
Let's move now to
integrated gradients.

138
00:05:53,030 --> 00:05:55,430
And let me start by giving
you the intuition for how

139
00:05:55,430 --> 00:05:56,900
this method is going to work.

140
00:05:56,900 --> 00:05:59,540
Imagine we have a simple two
dimensional feature space,

141
00:05:59,540 --> 00:06:01,580
feature x1 and x2.

142
00:06:01,580 --> 00:06:05,160
So here's the actual
point represented here.

143
00:06:05,160 --> 00:06:07,130
The idea behind
integrated gradients

144
00:06:07,130 --> 00:06:09,260
is that we're going to
compare that with respect

145
00:06:09,260 --> 00:06:10,610
to some baseline.

146
00:06:10,610 --> 00:06:14,233
That typical baseline for us
will be the all 0s vector.

147
00:06:14,233 --> 00:06:16,400
And then to do the comparison
what we'll actually do

148
00:06:16,400 --> 00:06:19,850
is interpolate a bunch of
points between that baseline

149
00:06:19,850 --> 00:06:23,060
and our actual input, take
gradients with respect

150
00:06:23,060 --> 00:06:26,630
to each one of them, and average
all of those gradient results.

151
00:06:26,630 --> 00:06:30,720
And that will give us some
measure of future importance.

152
00:06:30,720 --> 00:06:33,290
Here's the calculation of
the method in full detail.

153
00:06:33,290 --> 00:06:36,170
I've taken this presentation
from this really excellent

154
00:06:36,170 --> 00:06:39,980
tutorial from TensorFlow
integrated gradients.

155
00:06:39,980 --> 00:06:42,950
It does all these annotations
that I find quite helpful.

156
00:06:42,950 --> 00:06:44,990
Here's fundamentally
how this works.

157
00:06:44,990 --> 00:06:46,760
The core thing is in purple.

158
00:06:46,760 --> 00:06:48,410
We're going to
interpolate a bunch

159
00:06:48,410 --> 00:06:50,540
of different inputs
between that baseline

160
00:06:50,540 --> 00:06:53,120
of all 0s and our actual input.

161
00:06:53,120 --> 00:06:54,650
That's what's happening here.

162
00:06:54,650 --> 00:06:56,630
And we'll take the
gradients with respect

163
00:06:56,630 --> 00:06:58,190
to each one of
those with respect

164
00:06:58,190 --> 00:07:00,210
to each one of the features.

165
00:07:00,210 --> 00:07:02,600
And we're going to sum
those up and average them.

166
00:07:02,600 --> 00:07:05,450
And that gives us the
core calculation here.

167
00:07:05,450 --> 00:07:09,230
And then in 5, we just kind of
scale that resulting average

168
00:07:09,230 --> 00:07:11,630
with respect to the
original input to put it

169
00:07:11,630 --> 00:07:13,500
back on the same scale.

170
00:07:13,500 --> 00:07:15,800
And as I showed here,
integrated gradients

171
00:07:15,800 --> 00:07:17,750
obeys the sensitivity axiom.

172
00:07:17,750 --> 00:07:19,610
Let's go back to
that original example

173
00:07:19,610 --> 00:07:23,150
of that simple ReLU-based
model presented here.

174
00:07:23,150 --> 00:07:25,430
I showed you that the
input by gradients method

175
00:07:25,430 --> 00:07:28,010
failed sensitivity
for this model.

176
00:07:28,010 --> 00:07:29,750
Integrated gradients
of course is

177
00:07:29,750 --> 00:07:31,550
sensitive in the relevant sense.

178
00:07:31,550 --> 00:07:32,990
And you can kind
of see why that's

179
00:07:32,990 --> 00:07:35,540
happening, because our
core calculation now

180
00:07:35,540 --> 00:07:37,610
is not with respect
to a single input,

181
00:07:37,610 --> 00:07:40,220
in the case of the input
2, but rather with respect

182
00:07:40,220 --> 00:07:43,460
to all of those interpolated
feature representations.

183
00:07:43,460 --> 00:07:45,440
Although some of those
interpolated feature

184
00:07:45,440 --> 00:07:49,430
representations give a gradient
of 0 not all of them do.

185
00:07:49,430 --> 00:07:51,200
And the result in
effect is that you'll

186
00:07:51,200 --> 00:07:54,230
get a feature attribution
of approximately 1

187
00:07:54,230 --> 00:07:56,720
for this case of an input 2.

188
00:07:56,720 --> 00:07:59,210
The desired result
showing sensitivity,

189
00:07:59,210 --> 00:08:01,760
because of course the
input of 0 in this case

190
00:08:01,760 --> 00:08:05,457
would give an attribution of 0.

191
00:08:05,457 --> 00:08:07,790
Now let me walk you through
a few examples that show you

192
00:08:07,790 --> 00:08:11,690
how you can use Captum to get
hands on with the integrated

193
00:08:11,690 --> 00:08:12,590
gradients method.

194
00:08:12,590 --> 00:08:15,100
And I'm going to do that
for two classes of model.

195
00:08:15,100 --> 00:08:17,990
The first one is just a
simple feed-forward network.

196
00:08:17,990 --> 00:08:20,630
And what I'm doing is
reconnecting with the Stanford

197
00:08:20,630 --> 00:08:23,660
Sentiment Treebank that we
used during our sentiment unit.

198
00:08:23,660 --> 00:08:27,380
So on this slide, I've just
set up an SST experiment

199
00:08:27,380 --> 00:08:32,789
using sst.experiment
from that SST module.

200
00:08:32,789 --> 00:08:34,375
My feature
representations are going

201
00:08:34,375 --> 00:08:35,750
to be essentially
a bag of words.

202
00:08:35,750 --> 00:08:38,207
And I've filtered off stop
words to make this a little more

203
00:08:38,207 --> 00:08:39,379
interpretable.

204
00:08:39,380 --> 00:08:43,130
And our classifier is a
TorchShallowNeuralClassifier.

205
00:08:43,130 --> 00:08:45,470
I run the experiment
and a lot of information

206
00:08:45,470 --> 00:08:47,180
about that experiment,
you'll recall,

207
00:08:47,180 --> 00:08:51,450
is stored in this
variable, experiment.

208
00:08:51,450 --> 00:08:55,800
Here I extract the model
from that experiment report.

209
00:08:55,800 --> 00:08:57,660
And here, we get a
bunch of other metadata

210
00:08:57,660 --> 00:09:00,630
that we're going to use to run
the IntegratedGradients method.

211
00:09:00,630 --> 00:09:03,660
The feature representations
of our test examples,

212
00:09:03,660 --> 00:09:06,870
the actual labels, and
the predictive labels,

213
00:09:06,870 --> 00:09:08,340
along with the feature names.

214
00:09:08,340 --> 00:09:11,160
And the one thing to note here
is that for the sake of Captum,

215
00:09:11,160 --> 00:09:13,890
we need to turn the
string class names

216
00:09:13,890 --> 00:09:15,770
into their
corresponding indices.

217
00:09:15,770 --> 00:09:18,210
And that's what's
happening in cell 9 here.

218
00:09:18,210 --> 00:09:20,130
Then we set up the
integrated gradients

219
00:09:20,130 --> 00:09:23,040
using the forward
method for our model.

220
00:09:23,040 --> 00:09:26,400
And we set up the baseline,
which is that all 0s vector.

221
00:09:26,400 --> 00:09:28,810
And then finally use
the attribute method.

222
00:09:28,810 --> 00:09:31,410
And here I'm taking
attributions with respect

223
00:09:31,410 --> 00:09:34,570
to the predictions of the model.

224
00:09:34,570 --> 00:09:36,580
I think this can be a
powerful device for doing

225
00:09:36,580 --> 00:09:38,020
some simple error analysis.

226
00:09:38,020 --> 00:09:40,300
And that's what I've set
up on this slide here.

227
00:09:40,300 --> 00:09:42,610
I've offered two
functions, error analysis

228
00:09:42,610 --> 00:09:44,590
and create attribution
lookup that

229
00:09:44,590 --> 00:09:47,230
will help you understand
how features in this model

230
00:09:47,230 --> 00:09:49,580
are relating to its
output predictions.

231
00:09:49,580 --> 00:09:51,520
You can see in cell
14 here, I'm looking

232
00:09:51,520 --> 00:09:53,680
for cases where the
actual label is neutral

233
00:09:53,680 --> 00:09:55,540
and the model
predicted positive.

234
00:09:55,540 --> 00:09:56,980
We can find those attributions.

235
00:09:56,980 --> 00:09:59,410
And this is actually an
informative picture here.

236
00:09:59,410 --> 00:10:03,010
It looks like the model
has overfit to features

237
00:10:03,010 --> 00:10:04,750
like the period and the comma.

238
00:10:04,750 --> 00:10:07,420
This ought to be indicative
of the neutral category.

239
00:10:07,420 --> 00:10:09,310
But here it's using
them in ways that

240
00:10:09,310 --> 00:10:11,030
lead to a positive prediction.

241
00:10:11,030 --> 00:10:13,600
So that's something that
we might want to address.

242
00:10:13,600 --> 00:10:15,880
And we can go one level
further if we choose and look

243
00:10:15,880 --> 00:10:17,660
at individual examples.

244
00:10:17,660 --> 00:10:20,050
So here I have pulled out
an individual example.

245
00:10:20,050 --> 00:10:21,910
"No one goes
unindicted here, which

246
00:10:21,910 --> 00:10:23,470
is probably for the best."

247
00:10:23,470 --> 00:10:26,380
This is a case where the
correct label is neutral.

248
00:10:26,380 --> 00:10:28,640
And our model
predicted positive.

249
00:10:28,640 --> 00:10:30,070
And I think the
attributions again

250
00:10:30,070 --> 00:10:32,590
help us understand
why because by far

251
00:10:32,590 --> 00:10:36,350
the feature with the highest
attribution is this "best" one.

252
00:10:36,350 --> 00:10:38,650
And this is revealing that
the model just does not

253
00:10:38,650 --> 00:10:41,770
understand the context in
which the word "best" is

254
00:10:41,770 --> 00:10:43,510
used in this example.

255
00:10:43,510 --> 00:10:45,790
That might point to a
fundamental weakness

256
00:10:45,790 --> 00:10:47,125
of the bag of words approach.

257
00:10:47,125 --> 00:10:49,690


258
00:10:49,690 --> 00:10:51,340
For my second
example let's connect

259
00:10:51,340 --> 00:10:53,763
with transformer models, since
I assume that a lot of you

260
00:10:53,763 --> 00:10:55,180
will be working
with these models.

261
00:10:55,180 --> 00:10:57,460
And these present
exciting new opportunities

262
00:10:57,460 --> 00:10:58,870
for feature attribution.

263
00:10:58,870 --> 00:11:02,440
Because in these models, we
have so many representations

264
00:11:02,440 --> 00:11:05,320
that we could think about
doing attributions for.

265
00:11:05,320 --> 00:11:08,560
Here's a kind of general picture
of a BERT-like model, where

266
00:11:08,560 --> 00:11:10,300
I have the outputs up here.

267
00:11:10,300 --> 00:11:13,360
You have many layers of
transformer block outputs.

268
00:11:13,360 --> 00:11:16,390
Those are given in purple and
probably an embedding layer

269
00:11:16,390 --> 00:11:17,290
in green.

270
00:11:17,290 --> 00:11:20,410
And that embedding layer
might be itself composed

271
00:11:20,410 --> 00:11:23,680
of like a word-embedding layer
and a positional embedding

272
00:11:23,680 --> 00:11:25,490
layer, and maybe others.

273
00:11:25,490 --> 00:11:28,210
All of these layers
are potential targets

274
00:11:28,210 --> 00:11:29,920
for integrated gradients.

275
00:11:29,920 --> 00:11:33,310
And Captum again makes
that relatively easy.

276
00:11:33,310 --> 00:11:35,800
So to start this off, I just
downloaded from HuggingFace

277
00:11:35,800 --> 00:11:39,250
a RoBERTa-based
Twitter sentiment model

278
00:11:39,250 --> 00:11:40,840
that seemed really interesting.

279
00:11:40,840 --> 00:11:42,910
And I wrote a
predict_one_proba method

280
00:11:42,910 --> 00:11:47,730
that will help us with the error
analysis that we want to do.

281
00:11:47,730 --> 00:11:49,710
This next step here
does the encodings

282
00:11:49,710 --> 00:11:54,150
of both the actual example,
using the model's tokenizer

283
00:11:54,150 --> 00:11:56,100
as well as the
baseline of all 0s

284
00:11:56,100 --> 00:11:58,290
that we'll use for comparisons.

285
00:11:58,290 --> 00:12:02,270
In cell 7, I've just designed
a small custom forward method

286
00:12:02,270 --> 00:12:04,380
to help Captum out,
because this model

287
00:12:04,380 --> 00:12:09,130
has slightly different output
structure than is expected.

288
00:12:09,130 --> 00:12:12,180
Here in cell 8, we set up the
layer that we want to target.

289
00:12:12,180 --> 00:12:14,560
And as you can see I'm
targeting the embedding layer.

290
00:12:14,560 --> 00:12:17,110
But many other layers
could be targeted.

291
00:12:17,110 --> 00:12:18,710
Captum makes that easy.

292
00:12:18,710 --> 00:12:21,430
For our example, we use,
"This is illuminating!"

293
00:12:21,430 --> 00:12:24,740
which I'll take to have
true class positive.

294
00:12:24,740 --> 00:12:27,820
We do our encodings in cell
11 of both the actual example

295
00:12:27,820 --> 00:12:28,720
and the baseline.

296
00:12:28,720 --> 00:12:30,700
And then that's the
basis for our attribution

297
00:12:30,700 --> 00:12:32,990
of this single example.

298
00:12:32,990 --> 00:12:36,680
Now for BERT, because
we have high dimensional

299
00:12:36,680 --> 00:12:38,840
representations for
each one of the tokens

300
00:12:38,840 --> 00:12:40,970
that we're looking
at, we need to perform

301
00:12:40,970 --> 00:12:42,590
another layer of
compression that we

302
00:12:42,590 --> 00:12:45,120
didn't have to for the
feed-forward example.

303
00:12:45,120 --> 00:12:47,150
As you can see here,
the attributions

304
00:12:47,150 --> 00:12:51,420
have for one example
dimensionality 6 by 768.

305
00:12:51,420 --> 00:12:54,110
This is one vector
per word token.

306
00:12:54,110 --> 00:12:57,320
To summarize those at the level
of individual word tokens,

307
00:12:57,320 --> 00:12:59,570
we'll just sum them
up and then z-score

308
00:12:59,570 --> 00:13:02,570
normalize them to kind of put
them on a consistent scale.

309
00:13:02,570 --> 00:13:05,060
So that will reduce
the attributions down

310
00:13:05,060 --> 00:13:08,660
to one per sub-word token.

311
00:13:08,660 --> 00:13:12,060
And that feeds into our final
kind of cumulative analysis.

312
00:13:12,060 --> 00:13:14,450
So we'll do the
probabilistic predictions,

313
00:13:14,450 --> 00:13:18,200
look at the actual class,
convert the input to something

314
00:13:18,200 --> 00:13:21,530
that Captum can digest, and
then use this visualization data

315
00:13:21,530 --> 00:13:24,230
recorder method to
bring this all together

316
00:13:24,230 --> 00:13:26,585
into a nice tabular
visualization.

317
00:13:26,585 --> 00:13:27,960
And that's what's
happening here.

318
00:13:27,960 --> 00:13:30,620
You can see, for example,
we have the True Label,

319
00:13:30,620 --> 00:13:33,650
the Predicted Label with
the associated probability.

320
00:13:33,650 --> 00:13:37,140
And then the really interesting
part, per word token,

321
00:13:37,140 --> 00:13:39,080
we have a summary
of its attributions.

322
00:13:39,080 --> 00:13:40,910
And you can see that
green is associated

323
00:13:40,910 --> 00:13:44,510
with positive, white with
neutral, and red with negative.

324
00:13:44,510 --> 00:13:46,610
And this is giving us
a reassuring picture

325
00:13:46,610 --> 00:13:49,100
about the systematicity
of these predictions.

326
00:13:49,100 --> 00:13:50,780
It's a positive prediction.

327
00:13:50,780 --> 00:13:53,450
And most of that is the result
of the word "illuminating"

328
00:13:53,450 --> 00:13:55,940
and the exclamation mark.

329
00:13:55,940 --> 00:13:57,680
And that kind of
feeds into a nice kind

330
00:13:57,680 --> 00:14:01,250
of error
analysis/challenge analysis

331
00:14:01,250 --> 00:14:04,190
that you can do with models
like this using Captum.

332
00:14:04,190 --> 00:14:06,470
For this slide here, I've
posed a little challenge

333
00:14:06,470 --> 00:14:10,760
or adversarial test to see how
deeply my model understands

334
00:14:10,760 --> 00:14:12,920
sentences like, "They
said it would be great

335
00:14:12,920 --> 00:14:14,385
and they were right."

336
00:14:14,385 --> 00:14:16,760
You can see it makes the
correct prediction in that case.

337
00:14:16,760 --> 00:14:19,070
And when I change it to,
"They said it would be great

338
00:14:19,070 --> 00:14:21,350
and they were wrong.",
it predicts negative.

339
00:14:21,350 --> 00:14:23,990
That's reassuring, and so
are the future attributions.

340
00:14:23,990 --> 00:14:26,930
It seems to be keying
into exactly the pieces

341
00:14:26,930 --> 00:14:29,510
of information that I would
hope and even doing it

342
00:14:29,510 --> 00:14:31,950
in a context sensitive way.

343
00:14:31,950 --> 00:14:33,390
For the next two
examples, I just

344
00:14:33,390 --> 00:14:35,340
change up the syntax
to see whether it's

345
00:14:35,340 --> 00:14:38,640
kind of overfit to the position
of these words in the string.

346
00:14:38,640 --> 00:14:40,110
And it again looks robust.

347
00:14:40,110 --> 00:14:42,270
"They were right to say
that it would be great."

348
00:14:42,270 --> 00:14:43,470
Prediction of positive.

349
00:14:43,470 --> 00:14:45,470
"They were wrong to say
that it would be great."

350
00:14:45,470 --> 00:14:46,620
Prediction of negative.

351
00:14:46,620 --> 00:14:48,060
Very reassuring.

352
00:14:48,060 --> 00:14:50,070
As is the second
to last example,

353
00:14:50,070 --> 00:14:52,650
"They said it would be stellar
and they were correct."

354
00:14:52,650 --> 00:14:55,530
The only disappointing thing
in this challenge problem

355
00:14:55,530 --> 00:14:58,230
is for this final example
it predicts neutral

356
00:14:58,230 --> 00:15:00,120
for, "They said it
would be stellar

357
00:15:00,120 --> 00:15:01,410
and they were incorrect."

358
00:15:01,410 --> 00:15:03,960
And the attributions are
also a little bit worrisome

359
00:15:03,960 --> 00:15:06,360
about the extent to
which the model has truly

360
00:15:06,360 --> 00:15:09,042
understood this example.

361
00:15:09,042 --> 00:15:11,250
Maybe we can think about
how to address that problem.

362
00:15:11,250 --> 00:15:13,530
But the fundamental
takeaway for now

363
00:15:13,530 --> 00:15:17,070
is simply that you can see how
you can use feature attribution

364
00:15:17,070 --> 00:15:20,130
together with challenge
examples to kind of home

365
00:15:20,130 --> 00:15:24,480
in on exactly how systematic
a model's predictions are

366
00:15:24,480 --> 00:15:27,380
for an interesting
class of cases.

367
00:15:27,380 --> 00:15:32,000


