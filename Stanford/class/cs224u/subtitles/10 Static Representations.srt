1
00:00:00,000 --> 00:00:04,042


2
00:00:04,042 --> 00:00:05,500
CHRISTOPHER POTTS:
Hello, everyone,

3
00:00:05,500 --> 00:00:07,450
welcome to our final
screencast in our unit

4
00:00:07,450 --> 00:00:09,250
on distributed word
representations.

5
00:00:09,250 --> 00:00:12,070
Our topic is going to be
deriving static representations

6
00:00:12,070 --> 00:00:13,600
from contextual models.

7
00:00:13,600 --> 00:00:15,940
That might sound awfully
specific, but as you'll see,

8
00:00:15,940 --> 00:00:17,950
I think this could be
really empowering for you

9
00:00:17,950 --> 00:00:20,950
as you work on your original
system for your assignment

10
00:00:20,950 --> 00:00:23,940
and the associated bake-off.

11
00:00:23,940 --> 00:00:24,700
So let's dive in.

12
00:00:24,700 --> 00:00:26,370
A question on your
minds might be,

13
00:00:26,370 --> 00:00:29,970
how can I use BERT or related
models like RoBERTa or XLNet

14
00:00:29,970 --> 00:00:32,369
or ELECTRA in the
context of deriving

15
00:00:32,369 --> 00:00:34,450
good static
representations of words?

16
00:00:34,450 --> 00:00:36,210
You probably have heard
about these models

17
00:00:36,210 --> 00:00:38,520
and heard that they lift
all boats and the question

18
00:00:38,520 --> 00:00:42,075
is, how can you take
advantage of those benefits?

19
00:00:42,075 --> 00:00:43,200
But there's a tension here.

20
00:00:43,200 --> 00:00:45,660
We've been developing
static representations,

21
00:00:45,660 --> 00:00:48,390
but these models like
BERT are designed

22
00:00:48,390 --> 00:00:51,242
to deliver contextual
representations of words.

23
00:00:51,242 --> 00:00:53,200
And I'll return to what
that means in a second,

24
00:00:53,200 --> 00:00:57,100
but that is the central tension
between static and contextual.

25
00:00:57,100 --> 00:00:58,920
So the question is,
are there good methods

26
00:00:58,920 --> 00:01:01,080
for deriving static
representations

27
00:01:01,080 --> 00:01:04,590
from the contextual ones
that these models offer?

28
00:01:04,590 --> 00:01:07,350
And the answer from
Bommasani et al is yes.

29
00:01:07,350 --> 00:01:09,072
They are effective
methods for doing this

30
00:01:09,072 --> 00:01:10,530
and it's those
methods that will be

31
00:01:10,530 --> 00:01:12,328
the focus of this screencast.

32
00:01:12,328 --> 00:01:14,620
I really want to do two things
though for this lecture.

33
00:01:14,620 --> 00:01:17,400
I would like to get
hands on a little bit

34
00:01:17,400 --> 00:01:20,370
with a high-level overview
of models like BERT.

35
00:01:20,370 --> 00:01:22,980
We're going to look later in
the quarter in much more detail

36
00:01:22,980 --> 00:01:24,260
at how these models work.

37
00:01:24,260 --> 00:01:26,010
So for now, we're just
going to treat them

38
00:01:26,010 --> 00:01:27,300
as kind of black box.

39
00:01:27,300 --> 00:01:29,760
It's just like you might look
up a GloVe representation

40
00:01:29,760 --> 00:01:32,870
of a word and just get back
that representation and use it.

41
00:01:32,870 --> 00:01:35,520
So too here, we can think
of these models as devices

42
00:01:35,520 --> 00:01:37,950
for feeding in sequences
and getting back

43
00:01:37,950 --> 00:01:40,650
lots and lots of representations
that we might use.

44
00:01:40,650 --> 00:01:42,300
And later in the
quarter, we'll come

45
00:01:42,300 --> 00:01:44,040
to a deeper understanding
of precisely

46
00:01:44,040 --> 00:01:47,273
where those
representations come from.

47
00:01:47,273 --> 00:01:48,690
And in addition,
of course, I want

48
00:01:48,690 --> 00:01:51,510
to give you an overview of these
exciting methods from Bommasani

49
00:01:51,510 --> 00:01:54,390
et al in the hopes that
they are useful to you

50
00:01:54,390 --> 00:01:58,140
in developing your
original system.

51
00:01:58,140 --> 00:02:00,470
So let's start with
the structure of BERT.

52
00:02:00,470 --> 00:02:02,305
BERT processes
sequences, here I've

53
00:02:02,305 --> 00:02:05,900
got a sequence, the class
token, the day broke, SEP, class

54
00:02:05,900 --> 00:02:08,600
and separate designated tokens,
the class token typically

55
00:02:08,600 --> 00:02:11,150
starts the sequence and
then SEP ends the sequence.

56
00:02:11,150 --> 00:02:13,370
It can be also used
internally in sequences

57
00:02:13,370 --> 00:02:15,260
to mark boundaries
within the sequence

58
00:02:15,260 --> 00:02:16,940
that you're processing.

59
00:02:16,940 --> 00:02:18,440
But the fundamental
thing is that we

60
00:02:18,440 --> 00:02:21,020
have the short sentence,
"the day broke."

61
00:02:21,020 --> 00:02:24,650
BERT processes those into an
embedding layer and then a lot

62
00:02:24,650 --> 00:02:25,955
of additional layers.

63
00:02:25,955 --> 00:02:30,530
And here I depicted 4, but it
could be 12 or even 24 layers.

64
00:02:30,530 --> 00:02:34,430
What we're seeing here, the
rectangles represent vectors.

65
00:02:34,430 --> 00:02:38,270
They are the outputs of
each layer in the network.

66
00:02:38,270 --> 00:02:42,200
A lot of computation goes into
computing those output vector

67
00:02:42,200 --> 00:02:43,940
representations at each layer.

68
00:02:43,940 --> 00:02:46,160
We're going to set that
computation aside for now

69
00:02:46,160 --> 00:02:49,340
so that we can just think
of this as a grid of vector

70
00:02:49,340 --> 00:02:51,250
representations.

71
00:02:51,250 --> 00:02:54,140
Here is the crucial thing
that makes BERT contextual.

72
00:02:54,140 --> 00:02:56,400
For different sequences
that we process,

73
00:02:56,400 --> 00:02:59,090
we will get very
different representations.

74
00:02:59,090 --> 00:03:02,660
In fact, individual
tokens occurring

75
00:03:02,660 --> 00:03:05,600
in different sequences will get
very different representations.

76
00:03:05,600 --> 00:03:07,340
I've tried to signal
that with the colors

77
00:03:07,340 --> 00:03:10,970
here so like the two sequences
both contain the word "the"

78
00:03:10,970 --> 00:03:12,137
and the word "broke."

79
00:03:12,137 --> 00:03:14,720
But in virtue of the fact that
they have different surrounding

80
00:03:14,720 --> 00:03:17,180
material and different
positions in the sequence,

81
00:03:17,180 --> 00:03:19,070
almost all of the
representations

82
00:03:19,070 --> 00:03:21,000
will be different.

83
00:03:21,000 --> 00:03:23,630
The class and SEP tokens
might have the same embedding,

84
00:03:23,630 --> 00:03:25,970
but through all of these
layers because of the way

85
00:03:25,970 --> 00:03:28,220
all these tokens are going
to interact with each other

86
00:03:28,220 --> 00:03:29,900
when we derive the
representations,

87
00:03:29,900 --> 00:03:31,230
everything will be different.

88
00:03:31,230 --> 00:03:33,200
We do not get a static
representation out

89
00:03:33,200 --> 00:03:34,220
of these models.

90
00:03:34,220 --> 00:03:36,770
And I've specified that
even in the embedding

91
00:03:36,770 --> 00:03:39,200
layer, if the positions
of the words vary,

92
00:03:39,200 --> 00:03:41,900
one and the same token will
get different representations.

93
00:03:41,900 --> 00:03:45,080
The reason for that is that
this embedding layer is actually

94
00:03:45,080 --> 00:03:47,040
hiding two components.

95
00:03:47,040 --> 00:03:49,730
We do at the very
center of this model

96
00:03:49,730 --> 00:03:52,340
have a fixed static embedding,
where we can look up

97
00:03:52,340 --> 00:03:54,290
individual word sequences.

98
00:03:54,290 --> 00:03:56,690
But for this thing that I've
called the embedding layer,

99
00:03:56,690 --> 00:03:58,760
that static
representation is combined

100
00:03:58,760 --> 00:04:00,530
with a separate
positional encoding

101
00:04:00,530 --> 00:04:02,810
from a separate
embedding space, and that

102
00:04:02,810 --> 00:04:05,090
delivers what I've called
the embedding layer here.

103
00:04:05,090 --> 00:04:07,610
And that means that
even at this first layer

104
00:04:07,610 --> 00:04:09,590
because, for
example, "the" occurs

105
00:04:09,590 --> 00:04:11,580
in different points
in the sequence,

106
00:04:11,580 --> 00:04:13,520
it will get different
representations even

107
00:04:13,520 --> 00:04:15,150
in the embedding space.

108
00:04:15,150 --> 00:04:17,779
And from there, of course, as
we travel through these layers,

109
00:04:17,779 --> 00:04:19,760
we expect even more
things to change

110
00:04:19,760 --> 00:04:23,490
about the representations.

111
00:04:23,490 --> 00:04:25,920
A second important
preliminary is

112
00:04:25,920 --> 00:04:28,860
to give some attention to
how BERT and models like it

113
00:04:28,860 --> 00:04:30,677
tokenize sequences.

114
00:04:30,677 --> 00:04:32,760
And here I'm giving you a
bit of code in the hopes

115
00:04:32,760 --> 00:04:34,800
that you can get hands-on
and get a feel for how

116
00:04:34,800 --> 00:04:36,350
these tokenizers behave.

117
00:04:36,350 --> 00:04:38,700
I'm taking advantage of
the Hugging Face library.

118
00:04:38,700 --> 00:04:41,460
I have loaded a BERT
tokenizer and I load that

119
00:04:41,460 --> 00:04:43,538
from a pre-trained model.

120
00:04:43,538 --> 00:04:46,080
In cell 3, you can see that I've
called the tokenize function

121
00:04:46,080 --> 00:04:46,770
on the sentence.

122
00:04:46,770 --> 00:04:50,070
This isn't too surprising and
the result is a pretty normal

123
00:04:50,070 --> 00:04:51,760
looking sequence of tokens.

124
00:04:51,760 --> 00:04:54,100
You see some punctuation
has been separated off,

125
00:04:54,100 --> 00:04:56,220
but you also see a lot of words.

126
00:04:56,220 --> 00:04:57,720
When you get down
to cell 4, though,

127
00:04:57,720 --> 00:05:00,760
for the sequence "encode me"
this is a bit surprising.

128
00:05:00,760 --> 00:05:02,370
The word encode in
the input that's

129
00:05:02,370 --> 00:05:06,180
been broken apart into
two subword tokens, "en"

130
00:05:06,180 --> 00:05:08,955
and then "code" with these
boundary markers on it.

131
00:05:08,955 --> 00:05:12,690
BERT has broken that apart
into two subword sequences.

132
00:05:12,690 --> 00:05:16,620
And if I feed in a sequence
that has a really unfamiliar set

133
00:05:16,620 --> 00:05:20,370
of tokens in it, it will
do a lot of breaking apart

134
00:05:20,370 --> 00:05:23,010
of that sequence, as you can
see in cell 5 for the input

135
00:05:23,010 --> 00:05:26,700
"snuffleupagus" where a lot
of these pieces have come out.

136
00:05:26,700 --> 00:05:28,620
This is the essential
piece for why

137
00:05:28,620 --> 00:05:31,470
BERT is able to have such a
small vocabulary, only about

138
00:05:31,470 --> 00:05:34,890
30,000 words, compare that
with the 400,000 words

139
00:05:34,890 --> 00:05:36,360
that are in the GloVe space.

140
00:05:36,360 --> 00:05:37,920
The reason it can
get away with that

141
00:05:37,920 --> 00:05:39,690
is that it does a
lot of breaking apart

142
00:05:39,690 --> 00:05:42,390
of words into subword tokens.

143
00:05:42,390 --> 00:05:44,550
And of course, because
the model is contextual,

144
00:05:44,550 --> 00:05:47,790
we have an expectation that,
for example, when it encounters

145
00:05:47,790 --> 00:05:51,570
code here in the context of
"en" at some conceptual level,

146
00:05:51,570 --> 00:05:54,510
the model will recognize that it
has processed the word "encode"

147
00:05:54,510 --> 00:05:59,120
even though there was two
tokens underlining it.

148
00:05:59,120 --> 00:06:01,670
Let's flesh this out a bit by
looking at the full interface

149
00:06:01,670 --> 00:06:03,270
for dealing with these
models and, again,

150
00:06:03,270 --> 00:06:04,645
taking advantage
of Hugging Face.

151
00:06:04,645 --> 00:06:07,850
I'm going to load a BERT
model and a BERT tokenizer.

152
00:06:07,850 --> 00:06:10,820
It's important that they use the
same pre-trained weights, which

153
00:06:10,820 --> 00:06:13,358
Hugging Face will download
for you from the web.

154
00:06:13,358 --> 00:06:14,900
And so those are
tied in and I set up

155
00:06:14,900 --> 00:06:16,130
the tokenizer and the model.

156
00:06:16,130 --> 00:06:18,335
If I call tokenizer.encode
on a sequence,

157
00:06:18,335 --> 00:06:21,110
it will give me back
a list of indices.

158
00:06:21,110 --> 00:06:23,000
And those indices will
be used as a lookup

159
00:06:23,000 --> 00:06:26,090
to start the process of
computing this entire sequence.

160
00:06:26,090 --> 00:06:27,980
In cell 6, I actually
use the model

161
00:06:27,980 --> 00:06:30,530
to derive that grid
of representations.

162
00:06:30,530 --> 00:06:32,210
Hugging Face is
giving us an object

163
00:06:32,210 --> 00:06:33,500
that has a lot of attributes.

164
00:06:33,500 --> 00:06:37,220
If I call output_hidden_states
equals true when I use

165
00:06:37,220 --> 00:06:40,520
the model here, but I can call
.hidden_states and get that

166
00:06:40,520 --> 00:06:43,730
full grid of representations
that I showed you before.

167
00:06:43,730 --> 00:06:45,470
This is a sequence
with 13 layers.

168
00:06:45,470 --> 00:06:49,790
That's 1 embedding layer plus
12 of the additional layers.

169
00:06:49,790 --> 00:06:51,860
And if I key into one
of the first layer,

170
00:06:51,860 --> 00:06:53,150
that will be the embedding.

171
00:06:53,150 --> 00:06:56,970
You can see that its
shape is 1 by 5 by 768.

172
00:06:56,970 --> 00:06:59,060
This is the batch
of one example.

173
00:06:59,060 --> 00:07:00,770
It has five tokens.

174
00:07:00,770 --> 00:07:04,280
The three that we can see here
plus the class and SEP tokens.

175
00:07:04,280 --> 00:07:06,755
And each one of those tokens
in the embedding layer

176
00:07:06,755 --> 00:07:10,837
is represented by a
vector of dimension 768.

177
00:07:10,837 --> 00:07:12,920
And that remains consistent
through all the layers

178
00:07:12,920 --> 00:07:13,540
in the model.

179
00:07:13,540 --> 00:07:15,320
So I went to the
final output states,

180
00:07:15,320 --> 00:07:18,260
I again just index into
.hidden_states here.

181
00:07:18,260 --> 00:07:20,120
The shape is the
same and that will be

182
00:07:20,120 --> 00:07:22,963
consistent for all the layers.

183
00:07:22,963 --> 00:07:24,130
Those are the preliminaries.

184
00:07:24,130 --> 00:07:25,838
And let's think about
how we could derive

185
00:07:25,838 --> 00:07:27,430
some static representations.

186
00:07:27,430 --> 00:07:30,280
The first approach that
Bommasani et al considered

187
00:07:30,280 --> 00:07:32,770
is what they call the
decontextualized approach

188
00:07:32,770 --> 00:07:35,140
and this is like the
simplest thing possible.

189
00:07:35,140 --> 00:07:38,290
We are just going to process
individual words as though they

190
00:07:38,290 --> 00:07:41,660
were sequences and see if we
can make any sense of them.

191
00:07:41,660 --> 00:07:44,380
So we would start by feeding
in a word like "kitten"

192
00:07:44,380 --> 00:07:46,420
and we would allow the
model to break it apart

193
00:07:46,420 --> 00:07:48,250
into its subword pieces.

194
00:07:48,250 --> 00:07:50,290
And then we simply process
that with the model,

195
00:07:50,290 --> 00:07:53,110
we get a full grid
of representations.

196
00:07:53,110 --> 00:07:55,870
Now because we potentially
have subword tokens here,

197
00:07:55,870 --> 00:07:57,680
we need some pooling function.

198
00:07:57,680 --> 00:07:59,770
So what we can do is
just pool using something

199
00:07:59,770 --> 00:08:05,350
like mean to get a fixed static
representation of dimension 768

200
00:08:05,350 --> 00:08:07,233
for this individual word.

201
00:08:07,233 --> 00:08:09,400
And of course, we don't
have to use the final layer,

202
00:08:09,400 --> 00:08:11,080
we can use lower down layers.

203
00:08:11,080 --> 00:08:13,330
And we don't have to use
mean as the pooling function.

204
00:08:13,330 --> 00:08:15,760
You could consider
something like max or min

205
00:08:15,760 --> 00:08:18,010
or even last, which
would just disregard

206
00:08:18,010 --> 00:08:20,830
all of the representations
except for the one

207
00:08:20,830 --> 00:08:25,690
corresponding to the
final subword token.

208
00:08:25,690 --> 00:08:27,530
This is really simple.

209
00:08:27,530 --> 00:08:29,560
It's potentially
unnatural, though.

210
00:08:29,560 --> 00:08:30,850
BERT is a contextual model.

211
00:08:30,850 --> 00:08:32,558
It was trained on
full sequences.

212
00:08:32,558 --> 00:08:35,139
And especially if we leave
off the class and SEP tokens,

213
00:08:35,140 --> 00:08:38,440
we might be feeding in
sequences that BERT has really

214
00:08:38,440 --> 00:08:39,840
never seen before.

215
00:08:39,840 --> 00:08:41,380
And so it might be
unknown how it's

216
00:08:41,380 --> 00:08:43,900
going to behave with
these unusual inputs.

217
00:08:43,900 --> 00:08:46,120
Nonetheless, though, we
could repeat this process

218
00:08:46,120 --> 00:08:48,010
for all the words
in our vocabulary

219
00:08:48,010 --> 00:08:50,050
and derive a static
embedding space

220
00:08:50,050 --> 00:08:52,240
and maybe it has some promise.

221
00:08:52,240 --> 00:08:55,450
However, to address this
potential unnaturalness

222
00:08:55,450 --> 00:08:58,720
and potentially take more
advantage of the virtues

223
00:08:58,720 --> 00:09:00,820
that BERT and
related models have,

224
00:09:00,820 --> 00:09:04,400
Bommasani et al consider
also the aggregated approach.

225
00:09:04,400 --> 00:09:07,420
So in this approach, you
process lots of corpus examples

226
00:09:07,420 --> 00:09:09,130
that contain your target word.

227
00:09:09,130 --> 00:09:11,800
You've got that sort
of glimpse of a corpus.

228
00:09:11,800 --> 00:09:13,810
Our target word is
kitten, of course,

229
00:09:13,810 --> 00:09:16,480
we allow it to be broken
apart into subword tokens.

230
00:09:16,480 --> 00:09:18,490
The full sequences
in these examples

231
00:09:18,490 --> 00:09:21,152
would also be broken
apart into subword tokens.

232
00:09:21,152 --> 00:09:23,110
But the important thing
is that our target word

233
00:09:23,110 --> 00:09:24,580
might have subword tokens.

234
00:09:24,580 --> 00:09:27,760
We pool those as we did before
for the decontextualized

235
00:09:27,760 --> 00:09:29,770
approach and we're
also going to pool

236
00:09:29,770 --> 00:09:32,080
across all of the
different context examples

237
00:09:32,080 --> 00:09:33,970
that we processed.

238
00:09:33,970 --> 00:09:37,210
And the result of that should
be a bunch of natural inputs

239
00:09:37,210 --> 00:09:38,180
to the model.

240
00:09:38,180 --> 00:09:40,630
But in the end, we derive
a static representation

241
00:09:40,630 --> 00:09:43,720
that is some kind of average
across all of the examples

242
00:09:43,720 --> 00:09:45,220
that we processed.

243
00:09:45,220 --> 00:09:46,490
This seems very natural.

244
00:09:46,490 --> 00:09:49,000
It's taking advantage
of what BERT is best at.

245
00:09:49,000 --> 00:09:51,640
I will warn you, though, that
this is very computationally

246
00:09:51,640 --> 00:09:52,150
demanding.

247
00:09:52,150 --> 00:09:54,340
We're going to want to
process lots of examples,

248
00:09:54,340 --> 00:09:56,290
and BERT requires
lots of resources

249
00:09:56,290 --> 00:10:00,370
because it develops really large
representations as we've seen,

250
00:10:00,370 --> 00:10:02,300
but it might be worth it.

251
00:10:02,300 --> 00:10:04,670
Now, Bommasani et al
offer lots of results

252
00:10:04,670 --> 00:10:06,340
that help us understand
these approaches

253
00:10:06,340 --> 00:10:08,038
and how they perform.

254
00:10:08,038 --> 00:10:10,330
Let me give you a glimpse of
them as a kind of summary.

255
00:10:10,330 --> 00:10:13,840
So what we've got here is
results for the SimVerb 3,500

256
00:10:13,840 --> 00:10:15,978
dataset, a word
similarity data set

257
00:10:15,978 --> 00:10:17,770
that's very similar to
the ones that you'll

258
00:10:17,770 --> 00:10:20,440
be working with on the
homework and bake-off.

259
00:10:20,440 --> 00:10:22,780
Our metric is Spearman
correlation and higher

260
00:10:22,780 --> 00:10:23,440
is better.

261
00:10:23,440 --> 00:10:25,000
That's along the y-axis.

262
00:10:25,000 --> 00:10:27,580
And along the x-axis, I
have the layer in the model

263
00:10:27,580 --> 00:10:28,842
that we're keying into.

264
00:10:28,842 --> 00:10:30,550
And then, of course,
what we should watch

265
00:10:30,550 --> 00:10:33,160
is that we have two
pooling functions f and g.

266
00:10:33,160 --> 00:10:36,400
f is subword pooling
and g is context pooling

267
00:10:36,400 --> 00:10:37,840
for models that
have it, and it's

268
00:10:37,840 --> 00:10:41,380
decont for the
decontextualized approach.

269
00:10:41,380 --> 00:10:44,050
Now we have a very clear
result across these results,

270
00:10:44,050 --> 00:10:46,180
and I think across all
the results in the paper.

271
00:10:46,180 --> 00:10:48,370
Lower layers are better.

272
00:10:48,370 --> 00:10:51,340
Lower layers are giving us good
high fidelity representations

273
00:10:51,340 --> 00:10:52,300
of individual words.

274
00:10:52,300 --> 00:10:54,040
As we travel higher
in the model we

275
00:10:54,040 --> 00:10:58,630
seem to lose a lot of that
word level discrimination.

276
00:10:58,630 --> 00:11:01,270
In addition, your
best choice is to do

277
00:11:01,270 --> 00:11:04,840
mean pooling for the
context and subword pooling

278
00:11:04,840 --> 00:11:06,070
seems to matter less, right?

279
00:11:06,070 --> 00:11:08,770
All of these lines here
are all for the context

280
00:11:08,770 --> 00:11:12,640
pooling model with mean as
your context pooling function.

281
00:11:12,640 --> 00:11:14,980
The very best choice,
though, I think consistently

282
00:11:14,980 --> 00:11:18,677
is mean for both of these
pooling functions here.

283
00:11:18,677 --> 00:11:20,260
You can see that in
this result, and I

284
00:11:20,260 --> 00:11:23,020
think that's consistent across
all the results in the paper.

285
00:11:23,020 --> 00:11:26,830
But the overall takeaway
here is that, as expected,

286
00:11:26,830 --> 00:11:28,570
the aggregated
approach is better

287
00:11:28,570 --> 00:11:30,770
than the decontextualized
approach.

288
00:11:30,770 --> 00:11:33,820
However, if you don't have the
computational budget for that,

289
00:11:33,820 --> 00:11:36,520
then mean pooling and the
decontextualized approach

290
00:11:36,520 --> 00:11:38,260
looks really competitive.

291
00:11:38,260 --> 00:11:40,030
That's not so
evident in this spot,

292
00:11:40,030 --> 00:11:42,460
but if you look across all
the results in the paper,

293
00:11:42,460 --> 00:11:44,450
I think that's a
pretty clear finding.

294
00:11:44,450 --> 00:11:46,010
So that would be a good choice.

295
00:11:46,010 --> 00:11:48,580
And one thing is clear,
that simple approach

296
00:11:48,580 --> 00:11:50,950
is better than some
kinds of context

297
00:11:50,950 --> 00:11:53,800
pooling where you choose the
wrong context pooling function

298
00:11:53,800 --> 00:11:55,180
like min or max.

299
00:11:55,180 --> 00:11:58,240
Despite all of the effort that
went into this set of results

300
00:11:58,240 --> 00:12:00,250
and also these,
they're all kind of

301
00:12:00,250 --> 00:12:03,790
down here entangled with the
decontextualized approach.

302
00:12:03,790 --> 00:12:07,180
But mean as the pooling
function there is really

303
00:12:07,180 --> 00:12:10,950
an outstanding choice, as you
can see from these results.

304
00:12:10,950 --> 00:12:15,000


