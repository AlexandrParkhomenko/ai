1
00:00:00,000 --> 00:00:04,790


2
00:00:04,790 --> 00:00:06,540
CHRISTOPHER POTTS:
Welcome back, everyone.

3
00:00:06,540 --> 00:00:08,540
This is part 6 in our
series on Contextual Word

4
00:00:08,540 --> 00:00:09,380
Representations.

5
00:00:09,380 --> 00:00:11,630
We're going to be talking
about practical fine tuning.

6
00:00:11,630 --> 00:00:14,270
It's time to get hands-on
with these parameters we've

7
00:00:14,270 --> 00:00:15,570
been talking about.

8
00:00:15,570 --> 00:00:17,360
So here's the guiding idea.

9
00:00:17,360 --> 00:00:20,120
Your existing architecture, say,
for the current original system

10
00:00:20,120 --> 00:00:22,250
and bake off
probably can benefit

11
00:00:22,250 --> 00:00:24,110
from contextual representation.

12
00:00:24,110 --> 00:00:26,130
We've seen that in
many, many contexts

13
00:00:26,130 --> 00:00:27,980
and I know you
know these things.

14
00:00:27,980 --> 00:00:29,810
The notebook fine
tuning shows you

15
00:00:29,810 --> 00:00:32,520
how to bring in transformer
representations in two ways.

16
00:00:32,520 --> 00:00:35,150
First, with simple
featurization, and then

17
00:00:35,150 --> 00:00:36,320
with full-on fine tuning.

18
00:00:36,320 --> 00:00:39,740
And I'm going to talk about both
of those in this screencast.

19
00:00:39,740 --> 00:00:42,530
The heart of this idea is
that by extending existing

20
00:00:42,530 --> 00:00:46,130
PyTorch modules from the
course code distribution,

21
00:00:46,130 --> 00:00:48,980
you can very easily create
customized fine tuning models

22
00:00:48,980 --> 00:00:50,645
with just a few lines of code.

23
00:00:50,645 --> 00:00:52,520
And that should be really
empowering in terms

24
00:00:52,520 --> 00:00:54,500
of exploring lots
of different designs

25
00:00:54,500 --> 00:00:57,583
and seeing how best to use these
parameters for your problem.

26
00:00:57,583 --> 00:00:59,000
I just want to
mention that really

27
00:00:59,000 --> 00:01:02,492
and truly, this is only possible
because of the amazing work

28
00:01:02,492 --> 00:01:03,950
that the Hugging
Face team has done

29
00:01:03,950 --> 00:01:07,900
to make these parameters
accessible to all of us.

30
00:01:07,900 --> 00:01:09,700
So let's start with
simple featurization.

31
00:01:09,700 --> 00:01:11,860
And I actually want to
rewind to our discussion

32
00:01:11,860 --> 00:01:15,010
of recurrent neural networks
and think about how we represent

33
00:01:15,010 --> 00:01:16,870
examples for those models.

34
00:01:16,870 --> 00:01:20,290
In the standard mode, we have
as our examples lists of tokens

35
00:01:20,290 --> 00:01:21,400
here.

36
00:01:21,400 --> 00:01:23,590
We convert those into
lists of indices,

37
00:01:23,590 --> 00:01:25,570
and those indices
help us look up

38
00:01:25,570 --> 00:01:28,930
vector representations of those
words in some fixed embedding

39
00:01:28,930 --> 00:01:29,560
space.

40
00:01:29,560 --> 00:01:32,800
And the result of that is that
each example is represented

41
00:01:32,800 --> 00:01:34,320
by a list of vectors.

42
00:01:34,320 --> 00:01:36,100
That's important
to keep in mind.

43
00:01:36,100 --> 00:01:38,140
We tend to think of
the model as taking

44
00:01:38,140 --> 00:01:41,882
as its inputs lists of tokens
and having an embedding.

45
00:01:41,882 --> 00:01:43,840
But from the point of
view of the model itself,

46
00:01:43,840 --> 00:01:47,350
it really wants to process
as inputs lists of vectors.

47
00:01:47,350 --> 00:01:50,080
And that's the empowering
idea because if we use it

48
00:01:50,080 --> 00:01:53,050
in fixed embedding, of course,
then these two occurrences of A

49
00:01:53,050 --> 00:01:54,970
will be the same
vector, and these two

50
00:01:54,970 --> 00:01:58,515
occurrences of B across examples
will be the same vector.

51
00:01:58,515 --> 00:01:59,890
But the model
doesn't really care

52
00:01:59,890 --> 00:02:01,210
that there's a same vector.

53
00:02:01,210 --> 00:02:03,490
We could, if we wanted
to, convert directly

54
00:02:03,490 --> 00:02:07,120
from token sequences into
lists of vectors using

55
00:02:07,120 --> 00:02:08,870
a device like a BERT model.

56
00:02:08,870 --> 00:02:11,290
And that would allow that
A in the first position

57
00:02:11,290 --> 00:02:13,030
and A in the third
could correspond

58
00:02:13,030 --> 00:02:16,120
to different vectors, or B
across these two examples

59
00:02:16,120 --> 00:02:17,740
might correspond to
different vectors.

60
00:02:17,740 --> 00:02:19,960
That would be the
contextual representation

61
00:02:19,960 --> 00:02:21,685
part of these models.

62
00:02:21,685 --> 00:02:23,560
And again, from the
point of view of the RNN,

63
00:02:23,560 --> 00:02:25,090
we can feed these indirectly.

64
00:02:25,090 --> 00:02:26,320
That's straight forward.

65
00:02:26,320 --> 00:02:28,120
This is a complete
recipe for doing

66
00:02:28,120 --> 00:02:31,450
that using the SST
code and the PyTorch

67
00:02:31,450 --> 00:02:33,500
modules from the course
code distribution.

68
00:02:33,500 --> 00:02:36,430
So you can see that beyond
the setup stuff which we've

69
00:02:36,430 --> 00:02:39,580
done a few times, the
feature function is just

70
00:02:39,580 --> 00:02:42,820
going to use BERT functionality
to look up the example's

71
00:02:42,820 --> 00:02:45,700
indices and then convert them
into vector representations.

72
00:02:45,700 --> 00:02:47,440
And here, as a
summary, we're going

73
00:02:47,440 --> 00:02:50,740
to use the representation
above the class token.

74
00:02:50,740 --> 00:02:53,440
But lots of things are
possible at that point.

75
00:02:53,440 --> 00:02:55,330
And then when we have
our model wrapper here,

76
00:02:55,330 --> 00:02:57,058
we set up a Torch
RNN Classifier.

77
00:02:57,058 --> 00:02:58,600
And there're just
two things of note.

78
00:02:58,600 --> 00:03:01,180
First, we say use
embedding equals false,

79
00:03:01,180 --> 00:03:03,220
because we're going to
feed vectors in directly.

80
00:03:03,220 --> 00:03:05,290
There's no embedding
involved here.

81
00:03:05,290 --> 00:03:07,210
And we also don't need
to have a vocabulary.

82
00:03:07,210 --> 00:03:08,770
You could specify
one, but it's not

83
00:03:08,770 --> 00:03:10,960
involved because
fundamentally, again,

84
00:03:10,960 --> 00:03:14,200
the model deals
directly with vectors.

85
00:03:14,200 --> 00:03:16,090
And then at SST
experiment, you, again,

86
00:03:16,090 --> 00:03:17,770
say vectorized equals false.

87
00:03:17,770 --> 00:03:20,560
And that is a complete
recipe for bringing

88
00:03:20,560 --> 00:03:24,240
in BERT representations
with the standard RNN.

89
00:03:24,240 --> 00:03:26,170
This isn't quite
fine tuning though,

90
00:03:26,170 --> 00:03:28,360
so let's think about how
we might get added benefits

91
00:03:28,360 --> 00:03:31,000
from actually updating those
BERT parameters as opposed

92
00:03:31,000 --> 00:03:33,760
to just using them as
frozen representations

93
00:03:33,760 --> 00:03:36,600
inputs to another model.

94
00:03:36,600 --> 00:03:38,250
What I'd encourage
you to do is think

95
00:03:38,250 --> 00:03:40,650
about subclassing the
PyTorch modules that

96
00:03:40,650 --> 00:03:42,750
are included in our
course code distribution.

97
00:03:42,750 --> 00:03:46,380
Because then, you will be able
to write code, just oriented

98
00:03:46,380 --> 00:03:48,420
toward your model
architecture, and a lot

99
00:03:48,420 --> 00:03:50,880
of the details of optimization
and data processing

100
00:03:50,880 --> 00:03:52,590
will be handled for you.

101
00:03:52,590 --> 00:03:54,550
This is, I hope, a
powerful example of that.

102
00:03:54,550 --> 00:03:57,720
It comes from the tutorial
PyTorch models notebook.

103
00:03:57,720 --> 00:04:00,570
It's a Torch Softmax
Classifier, and the only thing

104
00:04:00,570 --> 00:04:03,990
we have to do is rewrite this
build_graph function to specify

105
00:04:03,990 --> 00:04:06,180
one single dense layer.

106
00:04:06,180 --> 00:04:08,700
We are using as our base
class the Torch Shallow Neural

107
00:04:08,700 --> 00:04:10,710
Classifier which
handles everything else

108
00:04:10,710 --> 00:04:13,870
about setting up this
model and optimizing it.

109
00:04:13,870 --> 00:04:15,730
If we wanted to go in
the other direction

110
00:04:15,730 --> 00:04:17,260
and instead fit a
really deep model,

111
00:04:17,260 --> 00:04:20,529
we could, again, begin from
Torch Shallow Neural Classifier

112
00:04:20,529 --> 00:04:22,750
and rewrite the build_graph
function so that it just

113
00:04:22,750 --> 00:04:24,250
has more layers, essentially.

114
00:04:24,250 --> 00:04:26,380
And then what's happening
in this init method

115
00:04:26,380 --> 00:04:28,150
is we're just giving
the user access

116
00:04:28,150 --> 00:04:30,280
to the various hyperparameters
that they could

117
00:04:30,280 --> 00:04:33,200
choose to set up this model.

118
00:04:33,200 --> 00:04:35,260
Finally, here's a
more involved example.

119
00:04:35,260 --> 00:04:38,710
This one, we start with a
PyTorch NN module, kind of all

120
00:04:38,710 --> 00:04:40,120
the way down at the base here.

121
00:04:40,120 --> 00:04:42,560
This is a Torch Linear
Regression model.

122
00:04:42,560 --> 00:04:44,080
We set up the weight
parameters here

123
00:04:44,080 --> 00:04:45,860
and then we have this
single forward pass,

124
00:04:45,860 --> 00:04:47,290
which corresponds
to the structure

125
00:04:47,290 --> 00:04:49,850
of a simple linear regression.

126
00:04:49,850 --> 00:04:52,085
Now, for the actual
interface, we

127
00:04:52,085 --> 00:04:53,710
need to do a little
bit more work here.

128
00:04:53,710 --> 00:04:56,170
So we set up the
loss so that it's

129
00:04:56,170 --> 00:04:58,190
appropriate for our
regression model.

130
00:04:58,190 --> 00:05:01,270
So most of the classifiers we've
been looking at up until now.

131
00:05:01,270 --> 00:05:03,340
build_graph just
uses the NN module

132
00:05:03,340 --> 00:05:05,025
that I showed you a second ago.

133
00:05:05,025 --> 00:05:07,150
We need to do a little bit
of work in build_dataset

134
00:05:07,150 --> 00:05:09,940
and rewrite that so we
process linear regression

135
00:05:09,940 --> 00:05:11,463
data correctly.

136
00:05:11,463 --> 00:05:13,630
And then we do need to
rewrite the predict and score

137
00:05:13,630 --> 00:05:16,240
functions to be kind of good
citizens of the code base

138
00:05:16,240 --> 00:05:19,120
and allow for hyperparameter
optimization and cross

139
00:05:19,120 --> 00:05:20,122
validation and so forth.

140
00:05:20,122 --> 00:05:21,580
But that's, again,
straightforward,

141
00:05:21,580 --> 00:05:24,070
and fundamentally for predict
we're actually making use

142
00:05:24,070 --> 00:05:27,580
of the base class's _predict
method for the heavy lifting

143
00:05:27,580 --> 00:05:28,140
there.

144
00:05:28,140 --> 00:05:30,640
And then score, of course, is
just moving us out of the mode

145
00:05:30,640 --> 00:05:33,850
of evaluating classifiers and
into the mode of evaluating

146
00:05:33,850 --> 00:05:35,107
regression models.

147
00:05:35,107 --> 00:05:36,190
That's all you need to do.

148
00:05:36,190 --> 00:05:38,020
And again, conspicuously
absent from this

149
00:05:38,020 --> 00:05:40,450
is most of the aspects
of data processing

150
00:05:40,450 --> 00:05:42,550
and all of the details
of optimization.

151
00:05:42,550 --> 00:05:45,400
The base class Torch
model base here

152
00:05:45,400 --> 00:05:47,590
has a very full
featured fit method

153
00:05:47,590 --> 00:05:49,330
that you can use to
optimize these models

154
00:05:49,330 --> 00:05:52,280
and do hyperparameter
exploration.

155
00:05:52,280 --> 00:05:54,710
And that brings us to the
star of the show, which

156
00:05:54,710 --> 00:05:57,590
would be BERT fine tuning
with Hugging Face parameters.

157
00:05:57,590 --> 00:06:00,860
Here we'll start with
a PyTorch nn.module.

158
00:06:00,860 --> 00:06:04,040
We load in a BERT module
as we've done before,

159
00:06:04,040 --> 00:06:07,228
and make sure to set it to
train so that it can be updated.

160
00:06:07,228 --> 00:06:09,020
And then the new
parameters here are really

161
00:06:09,020 --> 00:06:11,132
just this classifier
layer, a dense layer

162
00:06:11,132 --> 00:06:13,340
that's going to be oriented
toward the classification

163
00:06:13,340 --> 00:06:16,490
structure that we want
our model to have.

164
00:06:16,490 --> 00:06:18,530
The forward method
calls the forward method

165
00:06:18,530 --> 00:06:20,990
of the BERT model, and you get
a bunch of representations.

166
00:06:20,990 --> 00:06:22,830
There are a lot of options here.

167
00:06:22,830 --> 00:06:25,460
What I've decided to do is just
use the Hugging Face Pooler

168
00:06:25,460 --> 00:06:27,470
Output, which is some
parameters on top

169
00:06:27,470 --> 00:06:31,490
of the class token as the
input to the classifier.

170
00:06:31,490 --> 00:06:34,890
When we optimize this model,
with luck in a productive way,

171
00:06:34,890 --> 00:06:37,820
not only will these classifier
parameters be updated, but also

172
00:06:37,820 --> 00:06:39,470
all the parameters
of this BERT model

173
00:06:39,470 --> 00:06:42,170
that you loaded
in in train mode.

174
00:06:42,170 --> 00:06:44,460
The interface is a
little bit involved here.

175
00:06:44,460 --> 00:06:46,280
So what we do is
provide the user

176
00:06:46,280 --> 00:06:49,558
with some flexibility
about what choices to make.

177
00:06:49,558 --> 00:06:51,350
build_graph, again just
loads in the module

178
00:06:51,350 --> 00:06:53,240
that I showed you
just a second ago.

179
00:06:53,240 --> 00:06:55,800
And then build_dataset
is a bit involved.

180
00:06:55,800 --> 00:06:59,750
But what we do fundamentally is
use the BERT tokenizer to batch

181
00:06:59,750 --> 00:07:01,130
encode our data.

182
00:07:01,130 --> 00:07:03,620
And then we do a little bit
of processing on the output

183
00:07:03,620 --> 00:07:06,440
labels to make sure PyTorch
can make sense of them.

184
00:07:06,440 --> 00:07:07,160
That's really it.

185
00:07:07,160 --> 00:07:09,243
In the heart of this, it's
just that we're, again,

186
00:07:09,243 --> 00:07:11,750
using Hugging Face
functionality to represent

187
00:07:11,750 --> 00:07:13,790
our data to the BERT model.

188
00:07:13,790 --> 00:07:16,400
And then this is the
really interesting part.

189
00:07:16,400 --> 00:07:18,530
Calling the forward
method and then fitting

190
00:07:18,530 --> 00:07:21,690
the classifier on top is
pretty much all you need to do.

191
00:07:21,690 --> 00:07:23,840
And of course, that opens
up a world of options.

192
00:07:23,840 --> 00:07:25,430
Reps here has lots
of other things

193
00:07:25,430 --> 00:07:28,993
that you could use as the
input to this classifier layer.

194
00:07:28,993 --> 00:07:30,410
And many of them
actually might be

195
00:07:30,410 --> 00:07:34,300
more productive than the simple
approach that I've taken here.

196
00:07:34,300 --> 00:07:38,198


