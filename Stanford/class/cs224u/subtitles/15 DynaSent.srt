1
00:00:00,000 --> 00:00:04,672


2
00:00:04,672 --> 00:00:05,880
CHRIS POTTS: Hello, everyone.

3
00:00:05,880 --> 00:00:08,180
Welcome to part 4 of our
series on supervised sentiment

4
00:00:08,180 --> 00:00:08,750
analysis.

5
00:00:08,750 --> 00:00:11,360
This is the second screencast
in the series that is focused

6
00:00:11,360 --> 00:00:13,700
on a dataset for sentiment.

7
00:00:13,700 --> 00:00:15,530
And that dataset is DynaSent.

8
00:00:15,530 --> 00:00:17,630
This video could be
considered an optional element

9
00:00:17,630 --> 00:00:18,650
in the series.

10
00:00:18,650 --> 00:00:20,540
I'm offering it for
two reasons really.

11
00:00:20,540 --> 00:00:23,540
First, this is a new data
set that I helped produce.

12
00:00:23,540 --> 00:00:25,520
And I would love it if
people worked on it.

13
00:00:25,520 --> 00:00:28,650
It would be great to see some
new models, new insights.

14
00:00:28,650 --> 00:00:30,650
All of that would help
push this project forward

15
00:00:30,650 --> 00:00:32,060
in interesting ways.

16
00:00:32,060 --> 00:00:33,560
The second reason
is more practical.

17
00:00:33,560 --> 00:00:35,750
I think that this data
set could be useful to you

18
00:00:35,750 --> 00:00:38,750
as you work on the assignment
and the associated bake-off.

19
00:00:38,750 --> 00:00:40,400
You could use the
data set itself

20
00:00:40,400 --> 00:00:42,530
for supplementary training data.

21
00:00:42,530 --> 00:00:44,512
You could use it to
evaluate your system.

22
00:00:44,512 --> 00:00:46,220
And as you'll see,
there are a few points

23
00:00:46,220 --> 00:00:49,370
of conceptual connection
between this data set

24
00:00:49,370 --> 00:00:53,450
and the brand new dev and test
sets of restaurant sentences

25
00:00:53,450 --> 00:00:57,552
that are part of the
bake-off this year.

26
00:00:57,552 --> 00:00:58,260
So let's dive in.

27
00:00:58,260 --> 00:00:59,343
Here's a project overview.

28
00:00:59,343 --> 00:01:02,310
First, all the data, code, and
models are available on GitHub

29
00:01:02,310 --> 00:01:04,239
at this link.

30
00:01:04,239 --> 00:01:08,540
This dataset itself consists
of about 122,000 sentences.

31
00:01:08,540 --> 00:01:10,160
They are across two rounds.

32
00:01:10,160 --> 00:01:12,440
And I'm going to cover
what each round means.

33
00:01:12,440 --> 00:01:15,560
And each of the sentences has
five gold labels in addition

34
00:01:15,560 --> 00:01:17,797
to an inferred majority
label where there is one.

35
00:01:17,797 --> 00:01:19,130
And I'll return to that as well.

36
00:01:19,130 --> 00:01:21,980
I think that's an interesting
aspect to this kind of data

37
00:01:21,980 --> 00:01:23,390
collection.

38
00:01:23,390 --> 00:01:25,202
The associated paper
is Potts et al.

39
00:01:25,202 --> 00:01:26,660
2020, which I
encourage you to read

40
00:01:26,660 --> 00:01:29,690
if you want to learn even more
about this dataset and how,

41
00:01:29,690 --> 00:01:33,080
in particular, it relates to
the Stanford Sentiment Treebank,

42
00:01:33,080 --> 00:01:35,870
our other core dataset.

43
00:01:35,870 --> 00:01:37,820
And another ingredient
here, as you'll

44
00:01:37,820 --> 00:01:40,850
see when we get to round
two, is that this is partly

45
00:01:40,850 --> 00:01:44,960
an effort in model in the loop
adversarial data set creation.

46
00:01:44,960 --> 00:01:47,810
For round two, crowd
workers interacted

47
00:01:47,810 --> 00:01:49,880
with the model
attempting to fool it

48
00:01:49,880 --> 00:01:52,400
and, thereby, creating sentences
that are really difficult

49
00:01:52,400 --> 00:01:54,608
and are going to challenge
our models in what we hope

50
00:01:54,608 --> 00:01:57,330
are exciting and
productive ways.

51
00:01:57,330 --> 00:01:58,938
So here's a complete
project overview.

52
00:01:58,938 --> 00:02:00,230
Let me walk through it quickly.

53
00:02:00,230 --> 00:02:02,120
And then we'll dive
into the details.

54
00:02:02,120 --> 00:02:04,460
We begin with what we've
called Model 0, which

55
00:02:04,460 --> 00:02:07,460
is a RoBERTa model that's
fine-tuned on a bunch of very

56
00:02:07,460 --> 00:02:10,788
large, sentiment
benchmark data sets.

57
00:02:10,788 --> 00:02:12,920
The primary utility
of Model 0 is

58
00:02:12,920 --> 00:02:16,070
that we're going to use it as
a device to find challenging,

59
00:02:16,070 --> 00:02:20,390
naturally occurring sentences
out in a large corpus.

60
00:02:20,390 --> 00:02:23,870
And then we human validate those
to get actual labels for them.

61
00:02:23,870 --> 00:02:25,580
The result of that
process is what

62
00:02:25,580 --> 00:02:29,870
we hope is a really challenging
round 1 dataset of naturally

63
00:02:29,870 --> 00:02:33,350
occurring sentences that are
hard for a very good sentiment

64
00:02:33,350 --> 00:02:35,780
model like Model 0.

65
00:02:35,780 --> 00:02:38,750
On that basis, we
then train a Model 1,

66
00:02:38,750 --> 00:02:42,620
which is similar to Model 0 but
now extended with that round 1

67
00:02:42,620 --> 00:02:43,390
training data.

68
00:02:43,390 --> 00:02:45,860
So we hope that, in
bringing in that new data

69
00:02:45,860 --> 00:02:48,500
and combining it with
the sentiment benchmarks,

70
00:02:48,500 --> 00:02:50,900
we get an even stronger model.

71
00:02:50,900 --> 00:02:53,390
That is the model that
crowd workers interacted

72
00:02:53,390 --> 00:02:55,430
with on the Dynabench
platform to try

73
00:02:55,430 --> 00:02:57,980
to create examples
that are adversarial

74
00:02:57,980 --> 00:02:59,330
with respect to Model 1.

75
00:02:59,330 --> 00:03:01,260
So they ought to be
really difficult.

76
00:03:01,260 --> 00:03:03,770
We feed those through exactly
the same human validation

77
00:03:03,770 --> 00:03:04,340
pipeline.

78
00:03:04,340 --> 00:03:07,312
And that gives us our
second round of data.

79
00:03:07,312 --> 00:03:09,020
So two rounds of data
that can be thought

80
00:03:09,020 --> 00:03:11,360
of as separate problems
are merged together

81
00:03:11,360 --> 00:03:12,750
into a larger data set.

82
00:03:12,750 --> 00:03:14,540
I think we're kind
of still deciding

83
00:03:14,540 --> 00:03:19,200
how best to conceptualize
these various data aspects.

84
00:03:19,200 --> 00:03:21,390
So let's look at round 1
in a little more detail.

85
00:03:21,390 --> 00:03:23,270
This is where we
begin with Model 0,

86
00:03:23,270 --> 00:03:27,770
and try to harvest interesting
naturally occurring sentences.

87
00:03:27,770 --> 00:03:30,920
We sort of run Model 0 as
a RoBERTa-based classifier.

88
00:03:30,920 --> 00:03:34,470
And its training data are
from customer reviews,

89
00:03:34,470 --> 00:03:37,730
which is small, the IMDB
dataset, which I linked to

90
00:03:37,730 --> 00:03:40,460
in an earlier
screencast, SST-3, which

91
00:03:40,460 --> 00:03:42,710
you saw in the previous
screencast, and then these two

92
00:03:42,710 --> 00:03:47,330
very large external benchmarks
of product and service reviews

93
00:03:47,330 --> 00:03:48,650
from Yelp and Amazon.

94
00:03:48,650 --> 00:03:51,170
You can see that
they're very big indeed.

95
00:03:51,170 --> 00:03:54,285
And the performance of
Model 0 on the datasets,

96
00:03:54,285 --> 00:03:55,910
these are our three
external data sets.

97
00:03:55,910 --> 00:03:56,618
It's pretty good.

98
00:03:56,618 --> 00:04:00,170
They range from the
low 70s, for SST-3,

99
00:04:00,170 --> 00:04:02,670
to the high 70s for
Yelp and Amazon.

100
00:04:02,670 --> 00:04:03,800
So this is a solid model.

101
00:04:03,800 --> 00:04:05,750
And I will say,
impressionistically,

102
00:04:05,750 --> 00:04:08,030
if you download Model 0
and play around with it,

103
00:04:08,030 --> 00:04:12,500
you will find that it is a very
good sentiment model indeed.

104
00:04:12,500 --> 00:04:15,110
So we used Model 0 to
harvest what we hope

105
00:04:15,110 --> 00:04:16,339
are challenging sentences.

106
00:04:16,339 --> 00:04:18,740
And for this, we used
the Yelp academic dataset

107
00:04:18,740 --> 00:04:21,908
which is a very large collection
of about 8 million reviews.

108
00:04:21,908 --> 00:04:23,450
And our heuristic
is that we're going

109
00:04:23,450 --> 00:04:26,810
to favor in our sampling
process harvesting sentences

110
00:04:26,810 --> 00:04:30,350
where the review was one
star, so it's very low,

111
00:04:30,350 --> 00:04:33,620
and Model 0 predicted
positive for a given sentence

112
00:04:33,620 --> 00:04:36,140
and, conversely, where
the review is five stars,

113
00:04:36,140 --> 00:04:38,420
and Model 0 predicted negative.

114
00:04:38,420 --> 00:04:41,570
We are hoping that that at least
creates a bias for sentences

115
00:04:41,570 --> 00:04:43,220
that are very
challenging for Model 0,

116
00:04:43,220 --> 00:04:45,260
where it's actually
making a wrong prediction.

117
00:04:45,260 --> 00:04:47,135
We're not going to depend
on that assumption.

118
00:04:47,135 --> 00:04:48,740
Because we'll have
a validation step.

119
00:04:48,740 --> 00:04:51,770
But we're hoping that this
is as kind of as adversarial

120
00:04:51,770 --> 00:04:55,980
as we can be without actually
having labels to begin with.

121
00:04:55,980 --> 00:04:58,240
This is a picture of the
validation interface.

122
00:04:58,240 --> 00:05:01,290
You can see that there were some
examples given and a little bit

123
00:05:01,290 --> 00:05:03,360
of training about how
to use the labels.

124
00:05:03,360 --> 00:05:05,520
And then, fundamentally,
what crowd workers did is

125
00:05:05,520 --> 00:05:06,937
they were prompted
for a sentence,

126
00:05:06,937 --> 00:05:09,960
and they made one of four
choices, positive, negative,

127
00:05:09,960 --> 00:05:13,080
no sentiment, which is our
notion of neutral, and mixed

128
00:05:13,080 --> 00:05:15,540
sentiment, which is
indicating a sentence that

129
00:05:15,540 --> 00:05:18,360
has a balance of positive and
negative sentiments expressed

130
00:05:18,360 --> 00:05:18,945
in it.

131
00:05:18,945 --> 00:05:21,070
I think that's an important
category to single out.

132
00:05:21,070 --> 00:05:23,340
We're not going to try
to model those sentences.

133
00:05:23,340 --> 00:05:24,900
But we certainly
want crowd workers

134
00:05:24,900 --> 00:05:30,430
to register that kind of mixing
of emotions where it appears.

135
00:05:30,430 --> 00:05:31,900
So here's the resulting dataset.

136
00:05:31,900 --> 00:05:35,710
And because we got five gold
labels for every sentence,

137
00:05:35,710 --> 00:05:38,080
there are two perspectives
that you can take.

138
00:05:38,080 --> 00:05:40,083
The first one I've called
distributional train.

139
00:05:40,083 --> 00:05:41,500
And this is where,
essentially, we

140
00:05:41,500 --> 00:05:44,350
take each one of the
examples and reproduce it

141
00:05:44,350 --> 00:05:47,440
five times for each of
the labels that it got.

142
00:05:47,440 --> 00:05:51,100
So if an individual sentence
got three positive labels, two

143
00:05:51,100 --> 00:05:54,520
negative, then we would have
five examples, three labeled

144
00:05:54,520 --> 00:05:56,140
positive and three
labeled negative,

145
00:05:56,140 --> 00:05:59,500
with the actual text of the
example repeated five times.

146
00:05:59,500 --> 00:06:01,870
What that is doing
is essentially

147
00:06:01,870 --> 00:06:04,750
simulating having a
distribution over the labels.

148
00:06:04,750 --> 00:06:06,850
And for many
classifier models, that

149
00:06:06,850 --> 00:06:08,440
is literally the
same as training

150
00:06:08,440 --> 00:06:11,920
on a distribution of the labels
as given by our crowd workers.

151
00:06:11,920 --> 00:06:15,658
I think this is an exciting
way to bring in uncertainty

152
00:06:15,658 --> 00:06:17,200
and capture the fact
that there might

153
00:06:17,200 --> 00:06:19,810
be kind of inherent disagreement
among the crowd workers

154
00:06:19,810 --> 00:06:22,510
that we want our model
to at least grapple with.

155
00:06:22,510 --> 00:06:25,270
And in the paper,
as we discuss, this

156
00:06:25,270 --> 00:06:28,480
gives better models than
training on just the majority

157
00:06:28,480 --> 00:06:29,380
labels.

158
00:06:29,380 --> 00:06:31,340
But you can take a
more traditional view.

159
00:06:31,340 --> 00:06:33,340
So majority label here
means that at least three

160
00:06:33,340 --> 00:06:36,250
of the five workers
chose that label.

161
00:06:36,250 --> 00:06:40,180
That gives you 94,000 or
95,000 sentences for training.

162
00:06:40,180 --> 00:06:43,360
And then these dev and test
sets have 3,600 samples each.

163
00:06:43,360 --> 00:06:45,970
And presumably, we would
predict just the majority label

164
00:06:45,970 --> 00:06:47,050
for them.

165
00:06:47,050 --> 00:06:50,590
What's more open is how
we train these systems.

166
00:06:50,590 --> 00:06:53,680
And in the end, what we found
is that 47% of these examples

167
00:06:53,680 --> 00:06:56,350
are adversarial with
respect to Model 0.

168
00:06:56,350 --> 00:06:58,195
And as you'll see,
the dev and test set

169
00:06:58,195 --> 00:07:02,104
are designed so that Model 0
performs at chance on them.

170
00:07:02,104 --> 00:07:04,410
Yeah, that's the Model
0 versus the human.

171
00:07:04,410 --> 00:07:06,250
So here's a summary
of the performance.

172
00:07:06,250 --> 00:07:08,550
I showed you these
categories before.

173
00:07:08,550 --> 00:07:11,270
And I'm just signaling that
we have, by design, ensured

174
00:07:11,270 --> 00:07:15,070
that Model 0 performs
at chance on round zero.

175
00:07:15,070 --> 00:07:17,140
We could compare that
to our human baseline.

176
00:07:17,140 --> 00:07:20,440
For this, we kind of
synthesized five annotators

177
00:07:20,440 --> 00:07:22,690
and did pairwise
F1 scoring for them

178
00:07:22,690 --> 00:07:24,400
to get an estimate
of human performance

179
00:07:24,400 --> 00:07:27,730
that is on the same scale
as what we got from Model 0

180
00:07:27,730 --> 00:07:28,450
up here.

181
00:07:28,450 --> 00:07:32,140
And we put that estimate of
88% for the dev and test sets.

182
00:07:32,140 --> 00:07:34,150
I think that's a good
conservative number.

183
00:07:34,150 --> 00:07:36,550
I think if you got close to
it, that would be a signal

184
00:07:36,550 --> 00:07:38,680
that we had kind of
saturated this round.

185
00:07:38,680 --> 00:07:41,422
And we'd like to think about
additional dataset creation.

186
00:07:41,422 --> 00:07:42,880
I do want to signal,
though, that I

187
00:07:42,880 --> 00:07:45,760
think this is a conservative
estimate of how humans do.

188
00:07:45,760 --> 00:07:47,860
And one indicator of
that is that, actually,

189
00:07:47,860 --> 00:07:52,120
614 of the roughly 1,200
people who worked on this task

190
00:07:52,120 --> 00:07:55,990
for validation never disagreed
with the majority label, which

191
00:07:55,990 --> 00:07:58,630
sort of starts to suggest
that there are humans who

192
00:07:58,630 --> 00:08:00,910
are performing
perfectly at this task,

193
00:08:00,910 --> 00:08:03,320
putting the set at
pretty low bound.

194
00:08:03,320 --> 00:08:04,840
And here are some
example sentences.

195
00:08:04,840 --> 00:08:07,510
These are fully randomly
sampled with the only bias

196
00:08:07,510 --> 00:08:09,850
being that I set a length
restriction, so that the slide

197
00:08:09,850 --> 00:08:10,910
would be manageable.

198
00:08:10,910 --> 00:08:12,670
These are the same
examples that appear

199
00:08:12,670 --> 00:08:14,740
in the paper, where
we needed to fit them

200
00:08:14,740 --> 00:08:16,390
all into a pretty small table.

201
00:08:16,390 --> 00:08:17,890
I think this is
illuminating though.

202
00:08:17,890 --> 00:08:19,473
So it's showing all
the different ways

203
00:08:19,473 --> 00:08:21,490
that Model 0 could get
confused with respect

204
00:08:21,490 --> 00:08:23,188
to the majority response.

205
00:08:23,188 --> 00:08:24,730
And I would like to
highlight for you

206
00:08:24,730 --> 00:08:28,390
that there is a real discrepancy
here on the neutral category.

207
00:08:28,390 --> 00:08:31,030
What we find is that,
because Model 0 was trained

208
00:08:31,030 --> 00:08:34,720
on large external benchmarks,
its notion of neutral

209
00:08:34,720 --> 00:08:36,580
actually mixes
together things that

210
00:08:36,580 --> 00:08:38,740
are mixed sentiment
and things that

211
00:08:38,740 --> 00:08:41,558
are highly uncertain about the
sentiment that is expressed,

212
00:08:41,558 --> 00:08:42,509
for whatever reason.

213
00:08:42,510 --> 00:08:45,220
So you get a lot of borderline
cases and a lot of cases

214
00:08:45,220 --> 00:08:47,290
where humans are
kind of inherently

215
00:08:47,290 --> 00:08:50,590
having a hard time agreeing
about what the fixed sentiment

216
00:08:50,590 --> 00:08:52,440
label would be.

217
00:08:52,440 --> 00:08:54,190
I think that DynaSent
is doing a better

218
00:08:54,190 --> 00:08:56,380
job of capturing some
notion of neutral

219
00:08:56,380 --> 00:08:57,880
in these labels over here.

220
00:08:57,880 --> 00:09:00,970
And we should be a little wary
of treating three-star reviews

221
00:09:00,970 --> 00:09:06,330
and things like that as a
true proxy for neutrality.

222
00:09:06,330 --> 00:09:09,580
This is a good point to signal
that the validation and test

223
00:09:09,580 --> 00:09:13,330
sets for the bake off of
the restaurant sentences

224
00:09:13,330 --> 00:09:16,540
were validated in the
same way as DynaSent.

225
00:09:16,540 --> 00:09:19,930
So those sentences will have
the same kind of neutrality

226
00:09:19,930 --> 00:09:22,750
that DynaSent has,
which could be opposed

227
00:09:22,750 --> 00:09:25,240
to the sense of neutrality
that you get from the Stanford

228
00:09:25,240 --> 00:09:27,970
Sentiment Treebank, which
was, of course, underlying we

229
00:09:27,970 --> 00:09:30,040
kind of gathered in
this setting of having

230
00:09:30,040 --> 00:09:34,540
a fixed five-star rating scale.

231
00:09:34,540 --> 00:09:35,440
So that's round 1.

232
00:09:35,440 --> 00:09:37,240
That's all naturally
occurring sentences.

233
00:09:37,240 --> 00:09:38,560
Let's turn to round 2.

234
00:09:38,560 --> 00:09:41,140
So recall that we benefit
from round 1 at this point

235
00:09:41,140 --> 00:09:44,230
by training a brand new model
on all those external datasets

236
00:09:44,230 --> 00:09:46,270
plus the round 1 dataset.

237
00:09:46,270 --> 00:09:49,450
And then we have workers
on Dynabench interact

238
00:09:49,450 --> 00:09:51,430
with this model
to try to fool it.

239
00:09:51,430 --> 00:09:53,320
And we validate the
resulting sentences

240
00:09:53,320 --> 00:09:55,150
to get our round 2 data set.

241
00:09:55,150 --> 00:09:57,582
So Model 1 is, again, a
RoBERTa-based classifier.

242
00:09:57,582 --> 00:09:59,290
What we've done for
our training here is,

243
00:09:59,290 --> 00:10:02,560
more or less, carry over what
we did for the first round.

244
00:10:02,560 --> 00:10:05,990
Except, we have upsampled the
SST to give it more weight.

245
00:10:05,990 --> 00:10:07,870
And we have
dramatically upsampled

246
00:10:07,870 --> 00:10:10,930
the distributional labels
from our round 1 dataset,

247
00:10:10,930 --> 00:10:13,840
effectively, trying to
give it equal weight as all

248
00:10:13,840 --> 00:10:16,690
of these other datasets combined
in the training procedure.

249
00:10:16,690 --> 00:10:19,810
So we're trying to get a
model that, as a priority,

250
00:10:19,810 --> 00:10:23,700
does really well on
our round one dataset.

251
00:10:23,700 --> 00:10:26,940
Here is a look at the
performance of this model.

252
00:10:26,940 --> 00:10:28,560
And first, I would
just note that it's

253
00:10:28,560 --> 00:10:29,800
doing well on round 1.

254
00:10:29,800 --> 00:10:33,330
We're at about 81%, which
is a little below humans

255
00:10:33,330 --> 00:10:35,580
but certainly much better
than the chance performance,

256
00:10:35,580 --> 00:10:38,220
by design, that we
set up for Model 0.

257
00:10:38,220 --> 00:10:39,720
I do want to signal,
though, that we

258
00:10:39,720 --> 00:10:41,400
have a kind of
drop in performance

259
00:10:41,400 --> 00:10:42,930
for a few of these categories.

260
00:10:42,930 --> 00:10:45,150
You can see that especially
for Yelp and Amazon,

261
00:10:45,150 --> 00:10:48,930
where Model 0 was at about,
for example, 80 here.

262
00:10:48,930 --> 00:10:50,940
Model 1 dropped down to 73.

263
00:10:50,940 --> 00:10:52,920
And it's a similar
picture for dev.

264
00:10:52,920 --> 00:10:55,500
And, more or less, that's
repeated for Amazon with a drop

265
00:10:55,500 --> 00:11:00,820
from about 76 to 73 and
77 to 73, similarly.

266
00:11:00,820 --> 00:11:02,790
So we have a trade
off in performance

267
00:11:02,790 --> 00:11:04,680
that I believe
traces to the fact

268
00:11:04,680 --> 00:11:07,650
that we are performing some
changes to the underlying

269
00:11:07,650 --> 00:11:09,478
semantics of the labels.

270
00:11:09,478 --> 00:11:11,020
But that's something
to keep in mind.

271
00:11:11,020 --> 00:11:12,812
And you can see that
there's a tension here

272
00:11:12,812 --> 00:11:16,440
as we try to do well at our
dataset versus continuing

273
00:11:16,440 --> 00:11:21,240
to do well on these fixed
external benchmarks.

274
00:11:21,240 --> 00:11:22,573
Here is the Dynabench interface.

275
00:11:22,573 --> 00:11:24,698
And there's one thing that
I want to note about it.

276
00:11:24,698 --> 00:11:25,920
This is the stock interface.

277
00:11:25,920 --> 00:11:28,200
But we've actually
concentrated on a condition

278
00:11:28,200 --> 00:11:30,930
that we call the prompt
condition, where workers,

279
00:11:30,930 --> 00:11:33,990
instead of having to just write
a sentence as a blank slate,

280
00:11:33,990 --> 00:11:36,870
sit down to an empty buffer
and try to fool the model,

281
00:11:36,870 --> 00:11:38,970
they were given an
inspirational prompt, which

282
00:11:38,970 --> 00:11:42,330
was an attested sentence from
the Yelp academic data set,

283
00:11:42,330 --> 00:11:45,510
and invited to modify that
sentence if they chose in order

284
00:11:45,510 --> 00:11:49,140
to achieve their goal of fooling
the model in a particular way.

285
00:11:49,140 --> 00:11:51,015
And this proved to be
vastly more productive.

286
00:11:51,015 --> 00:11:54,060
It led to more diverse
and realistic sentences.

287
00:11:54,060 --> 00:11:56,610
I think we've essentially
freed the crowd workers

288
00:11:56,610 --> 00:12:00,000
from the creative burden of
having each time to come up

289
00:12:00,000 --> 00:12:01,500
with a completely new sentence.

290
00:12:01,500 --> 00:12:03,750
And we're hoping that
this procedure leads

291
00:12:03,750 --> 00:12:06,810
to fewer artifacts,
more diversity, and more

292
00:12:06,810 --> 00:12:12,610
realism for this adversarial
dataset collection procedure.

293
00:12:12,610 --> 00:12:15,970
Our validation pipeline was
exactly the same as round 1.

294
00:12:15,970 --> 00:12:17,570
And here is the
resulting data set.

295
00:12:17,570 --> 00:12:19,362
It's a little bit
smaller because this kind

296
00:12:19,362 --> 00:12:21,460
of adversarial dataset
collection is hard.

297
00:12:21,460 --> 00:12:23,845
And you can see how
good Model 1 is.

298
00:12:23,845 --> 00:12:25,720
It was actually pretty
hard for crowd workers

299
00:12:25,720 --> 00:12:26,800
to fool this model.

300
00:12:26,800 --> 00:12:30,280
They did so only
about 19% of the time.

301
00:12:30,280 --> 00:12:32,260
Here's the dataset for
distributional training.

302
00:12:32,260 --> 00:12:34,540
You have about 93,000 sentences.

303
00:12:34,540 --> 00:12:36,670
And if you go for the
majority-label training,

304
00:12:36,670 --> 00:12:38,440
you have about 19,000.

305
00:12:38,440 --> 00:12:40,318
And the dev and test
sets are smaller.

306
00:12:40,318 --> 00:12:41,860
But again, the reason
they're smaller

307
00:12:41,860 --> 00:12:45,340
is that they are designed to
set Model 1 as having chance

308
00:12:45,340 --> 00:12:47,615
performance on this data set.

309
00:12:47,615 --> 00:12:49,240
And so that's what
I'll flesh out here.

310
00:12:49,240 --> 00:12:51,630
You can see that this
model chance performance,

311
00:12:51,630 --> 00:12:54,420
I showed you before that it's
doing pretty well on round one.

312
00:12:54,420 --> 00:12:57,900
And we had that kind of tension
with the external benchmarks.

313
00:12:57,900 --> 00:13:00,720
In terms of human
performance, we're at about 90

314
00:13:00,720 --> 00:13:03,210
using that procedure
of synthesized,

315
00:13:03,210 --> 00:13:05,495
kind of averaged F1 values.

316
00:13:05,495 --> 00:13:07,620
And I would just note,
again, that that's certainly

317
00:13:07,620 --> 00:13:08,400
conservative.

318
00:13:08,400 --> 00:13:10,740
In that, almost
half of the workers

319
00:13:10,740 --> 00:13:12,930
never disagreed with
the majority label.

320
00:13:12,930 --> 00:13:16,350
So it is certainly within the
capacity of individual humans

321
00:13:16,350 --> 00:13:19,500
to perform essentially
perfectly on this data set.

322
00:13:19,500 --> 00:13:22,230
But 90 is, nonetheless,
a good signpost for us

323
00:13:22,230 --> 00:13:24,450
as we think about hill
climbing and launching

324
00:13:24,450 --> 00:13:26,670
subsequent rounds of DynaSent.

325
00:13:26,670 --> 00:13:28,080
And here are some
short examples.

326
00:13:28,080 --> 00:13:29,580
And I think they
make the same point

327
00:13:29,580 --> 00:13:32,160
that our neutral category is
more aligned with the semantics

328
00:13:32,160 --> 00:13:34,470
of what we mean when we
identify neutral sentences

329
00:13:34,470 --> 00:13:38,610
and less heterogeneous than you
get from naturally occurring,

330
00:13:38,610 --> 00:13:41,100
neutral sentences derived
from star rating metadata

331
00:13:41,100 --> 00:13:41,950
and so forth.

332
00:13:41,950 --> 00:13:43,470
So I'm hopeful
that this is a kind

333
00:13:43,470 --> 00:13:46,710
of positive step toward
getting true ternary sentiment.

334
00:13:46,710 --> 00:13:49,260
But we should be aware
that this label shift has

335
00:13:49,260 --> 00:13:51,140
happened in these data sets.

336
00:13:51,140 --> 00:13:53,640
And the final thing I want to
say is just to reiterate that,

337
00:13:53,640 --> 00:13:55,950
if people do exciting
work with this dataset

338
00:13:55,950 --> 00:13:59,430
and start to make real progress
on the existing rounds, that

339
00:13:59,430 --> 00:14:01,650
would be our cue to
launch new rounds.

340
00:14:01,650 --> 00:14:03,930
The Dyna in DynaSent
is that we would

341
00:14:03,930 --> 00:14:06,300
like to have an evolving
benchmark, not one that's

342
00:14:06,300 --> 00:14:08,700
static but rather
responsive to progress

343
00:14:08,700 --> 00:14:11,100
that's made in the field
and the evolving needs

344
00:14:11,100 --> 00:14:13,710
of people who are trying to
develop practical sentiment

345
00:14:13,710 --> 00:14:15,210
analysis systems.

346
00:14:15,210 --> 00:14:18,000
So do let us know what
kind of progress you make

347
00:14:18,000 --> 00:14:19,910
and what you discover.

348
00:14:19,910 --> 00:14:24,000


