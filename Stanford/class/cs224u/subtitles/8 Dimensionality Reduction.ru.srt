1
00:00:05,120 --> 00:00:06,879
привет всем, с возвращением, это

2
00:00:06,879 --> 00:00:08,639
пятая часть нашей серии о распределенных представлениях слов,

3
00:00:08,639 --> 00:00:09,920
мы собираемся

4
00:00:09,920 --> 00:00:11,759
поговорить о методах уменьшения размерности

5
00:00:11,759 --> 00:00:13,759
, которые мы видели в предыдущем

6
00:00:13,759 --> 00:00:15,280
скринкасте, о том, что повторное взвешивание является

7
00:00:15,280 --> 00:00:17,520
мощным инструментом для поиска скрытой

8
00:00:17,520 --> 00:00:20,480
семантической информации в счетных матрицах, которые

9
00:00:20,480 --> 00:00:22,000
мы  Я собираюсь продвинуться еще дальше.

10
00:00:22,000 --> 00:00:23,760
Перспектива методов уменьшения размерности

11
00:00:23,760 --> 00:00:25,680
заключается в том, что они могут фиксировать

12
00:00:25,680 --> 00:00:27,519
понятия совместного возникновения более высокого порядка,

13
00:00:27,519 --> 00:00:29,920
соответствующие еще более глубоким видам

14
00:00:29,920 --> 00:00:33,119
семантической связи.

15
00:00:33,360 --> 00:00:34,640
Существует широкий мир этих

16
00:00:34,640 --> 00:00:36,320
методов уменьшения размерности.

17
00:00:36,320 --> 00:00:37,920
мы собираемся сосредоточиться

18
00:00:37,920 --> 00:00:39,840
на интересных представителях

19
00:00:39,840 --> 00:00:42,160
гораздо большего пространства, мы рассмотрим латентный

20
00:00:42,160 --> 00:00:44,079
семантический анализ, который является классическим

21
00:00:44,079 --> 00:00:46,320
линейным методом, затем мы поговорим об автоматических

22
00:00:46,320 --> 00:00:48,480
кодировщиках, новом мощном режиме глубокого обучения

23
00:00:48,480 --> 00:00:50,800
для обучения уменьшению размерных

24
00:00:50,800 --> 00:00:53,440
представлений и, наконец,

25
00:00:53,440 --> 00:00:55,520
GloVe — это простой, но очень мощный

26
00:00:55,520 --> 00:00:57,440
метод, который, как вы увидите, имеет

27
00:00:57,440 --> 00:00:59,199
глубокую связь с  o точечная взаимная

28
00:00:59,199 --> 00:01:01,199
информация, и затем я собираюсь

29
00:01:01,199 --> 00:01:02,800
закончить, кратко рассказав о

30
00:01:02,800 --> 00:01:04,559
визуализации, которая является еще одним видом

31
00:01:04,559 --> 00:01:06,479
техники уменьшения размерности, которую

32
00:01:06,479 --> 00:01:10,000
мы могли бы использовать для самых разных целей,

33
00:01:10,000 --> 00:01:11,520
поэтому давайте начнем со скрытого семантического

34
00:01:11,520 --> 00:01:13,680
анализа. классический метод, которому посвящена статья.

35
00:01:13,680 --> 00:01:16,159
дорогой вестер вообще 1990 это

36
00:01:16,159 --> 00:01:18,400
классическая статья которая действительно произвела фурор

37
00:01:18,400 --> 00:01:20,320
эээ это одна из самых лса сейчас одна

38
00:01:20,320 --> 00:01:21,840
из старейших наиболее широко используемых

39
00:01:21,840 --> 00:01:23,439
технологий уменьшения размерности не

40
00:01:23,439 --> 00:01:25,680
только в научных исследованиях но и в

41
00:01:25,680 --> 00:01:26,960
промышленности я думаю это было действительно

42
00:01:26,960 --> 00:01:29,040
поучительно  для людей во время

43
00:01:29,040 --> 00:01:32,000
появления статьи, чтобы увидеть, насколько

44
00:01:32,000 --> 00:01:34,000
мощной может быть эта техника,

45
00:01:34,000 --> 00:01:36,000
особенно в контексте

46
00:01:36,000 --> 00:01:38,240
поиска информации,

47
00:01:38,240 --> 00:01:40,159
метод также известен как

48
00:01:40,159 --> 00:01:42,240
разложение усеченного сингулярного значения,

49
00:01:42,240 --> 00:01:44,320
и я объясню, почему это в

50
00:01:44,320 --> 00:01:45,600
секунде окончательный  вещь, которую я хочу сказать на этом

51
00:01:45,600 --> 00:01:47,759
высоком уровне, это то, что lsa остается

52
00:01:47,759 --> 00:01:50,159
очень мощной базовой линией, особенно когда она является

53
00:01:50,159 --> 00:01:52,000
частью конвейера других  r методы повторного взвешивания,

54
00:01:52,000 --> 00:01:54,320
поэтому он, вероятно, должен быть в вашей

55
00:01:54,320 --> 00:01:56,159
таблице результатов, и его часто очень

56
00:01:56,159 --> 00:01:59,119
трудно превзойти

57
00:01:59,200 --> 00:02:00,799
сейчас, я думаю, что мы не можем за

58
00:02:00,799 --> 00:02:02,719
отведенное нам время охватить все

59
00:02:02,719 --> 00:02:04,240
технические детали, связанные со скрытым

60
00:02:04,240 --> 00:02:06,079
семантическим анализом. по моему опыту, это

61
00:02:06,079 --> 00:02:07,759
было бы  своего рода кульминация

62
00:02:07,759 --> 00:02:10,080
полного курса линейной алгебры, но я действительно

63
00:02:10,080 --> 00:02:11,440
думаю, что могу передать направляющую

64
00:02:11,440 --> 00:02:13,040
интуицию, и это поможет вам

65
00:02:13,040 --> 00:02:15,200
ответственно использовать метод,

66
00:02:15,200 --> 00:02:16,400
так что давайте представим, что у нас есть эта

67
00:02:16,400 --> 00:02:18,160
простая двухмерная модель векторного пространства, которую

68
00:02:18,160 --> 00:02:20,879
я получил  четыре точки abc и

69
00:02:20,879 --> 00:02:22,319
d, расположенные в этом двумерном

70
00:02:22,319 --> 00:02:23,280
пространстве,

71
00:02:23,280 --> 00:02:24,959
я думаю, мы все знакомы с подгонкой

72
00:02:24,959 --> 00:02:27,280
линейных моделей, которые фиксируют самый большой

73
00:02:27,280 --> 00:02:29,280
источник вариаций в данных,

74
00:02:29,280 --> 00:02:30,959
вот эта оранжевая линия.

75
00:02:30,959 --> 00:02:32,400


76
00:02:32,400 --> 00:02:34,800
можно думать об этой

77
00:02:34,800 --> 00:02:36,879
модели линейной регрессии как об

78
00:02:36,879 --> 00:02:38,480
уменьшении размерности, поскольку она

79
00:02:38,480 --> 00:02:41,599
побуждает нас проецировать точки, такие как b

80
00:02:41,599 --> 00:02:44,160
и c, вниз на эту линию

81
00:02:44,160 --> 00:02:45,840
и проецировать их вниз на эту линию.

82
00:02:45,840 --> 00:02:48,239
линии по существу в абстрагировании

83
00:02:48,239 --> 00:02:49,920
от их точки вариации вдоль

84
00:02:49,920 --> 00:02:52,160
оси y мы можем видеть смысл, в котором

85
00:02:52,160 --> 00:02:53,920
они абстрактно похожи они

86
00:02:53,920 --> 00:02:55,840
близки друг к другу в этом уменьшенном

87
00:02:55,840 --> 00:02:57,599
пространстве измерений

88
00:02:57,599 --> 00:02:59,120
теперь с помощью линейной модели мы

89
00:02:59,120 --> 00:03:00,879
захватили источник наибольшей вариации этого

90
00:03:00,879 --> 00:03:02,720
маленького  набор данных в многомерном

91
00:03:02,720 --> 00:03:04,959
пространстве, мы могли бы продолжить подгонку

92
00:03:04,959 --> 00:03:07,120
линий к другим источникам

93
00:03:07,120 --> 00:03:08,959
вариаций данных, другим осям вариаций, так что

94
00:03:08,959 --> 00:03:10,640
вот здесь синяя линия, которая

95
00:03:10,640 --> 00:03:12,720
захватывает следующее измерение, и мы могли бы снова

96
00:03:12,720 --> 00:03:14,640
спроецировать точки, такие как a и c, вниз на

97
00:03:14,640 --> 00:03:16,159
эту линию и  это отразило бы

98
00:03:16,159 --> 00:03:18,560
абстрактный смысл, в котором a и c, хотя и

99
00:03:18,560 --> 00:03:20,800
очень разбросаны по измерению x

100
00:03:20,800 --> 00:03:22,800
, очень близки друг к другу по измерению y,

101
00:03:22,800 --> 00:03:24,959
и, конечно, если бы у нас было больше

102
00:03:24,959 --> 00:03:27,280
измерений в этой модели векторного пространства, мы

103
00:03:27,280 --> 00:03:29,360
могли бы продолжать выполнять эти сокращения и

104
00:03:29,360 --> 00:03:31,920
уменьшения размерности  захватывать все

105
00:03:31,920 --> 00:03:34,480
более абстрактные понятия сходства

106
00:03:34,480 --> 00:03:36,879
по этим различным осям, и это,

107
00:03:36,879 --> 00:03:38,879
по сути, то, что Isa собирается сделать для

108
00:03:38,879 --> 00:03:42,959
нас в o  для действительно больших

109
00:03:42,959 --> 00:03:44,799
матриц фундаментальный метод, как я уже сказал, -

110
00:03:44,799 --> 00:03:46,720
это разложение по сингулярным числам, это

111
00:03:46,720 --> 00:03:48,640
теорема из линейной алгебры, которая говорит, что

112
00:03:48,640 --> 00:03:52,400
любую матрицу размерности m на n можно

113
00:03:52,400 --> 00:03:54,239
разложить на произведение трех

114
00:03:54,239 --> 00:03:56,480
матриц ts и d

115
00:03:56,480 --> 00:03:57,360
с

116
00:03:57,360 --> 00:03:59,280
размерами, указанными здесь, более конкретно

117
00:03:59,280 --> 00:04:00,560
Например,

118
00:04:00,560 --> 00:04:02,239
начните с этой матрицы размерности

119
00:04:02,239 --> 00:04:04,319
три на четыре, мы узнали матрицу терминов,

120
00:04:04,319 --> 00:04:07,519
которая заполнена нормированными по длине

121
00:04:07,519 --> 00:04:09,200
ортогональными векторами,

122
00:04:09,200 --> 00:04:12,080
у нас есть эта матрица сингулярных значений

123
00:04:12,080 --> 00:04:13,760
вдоль диагонали, они организованы

124
00:04:13,760 --> 00:04:15,760
от наибольшего к наименьшему, что

125
00:04:15,760 --> 00:04:17,759
соответствует наибольшему и наименьшему источнику

126
00:04:17,759 --> 00:04:19,519
вариации в  данные,

127
00:04:19,519 --> 00:04:21,199
а затем у нас есть матрица документа или столбца,

128
00:04:21,199 --> 00:04:23,360
которая также

129
00:04:23,360 --> 00:04:26,240
нормализована по длине и ортогональна в своем пространстве,

130
00:04:26,240 --> 00:04:27,680
и теорема здесь заключается в том, что мы можем

131
00:04:27,680 --> 00:04:30,560
восстановить a из этих трех матриц,

132
00:04:30,560 --> 00:04:32,160
конечно, мы не хотим точно

133
00:04:32,160 --> 00:04:33,680
реконструировать a, что, вероятно,  не

134
00:04:33,680 --> 00:04:35,440
принесет нам многого,

135
00:04:35,440 --> 00:04:37,840
но что мы можем сделать, так это использовать это для изучения

136
00:04:37,840 --> 00:04:40,160
уменьшенных размерных представлений а

137
00:04:40,160 --> 00:04:42,800
, будучи  избирательно в отношении того, какие термины и

138
00:04:42,800 --> 00:04:45,120
измерения единичных значений мы включаем

139
00:04:45,120 --> 00:04:46,400
в модель,

140
00:04:46,400 --> 00:04:47,759
позвольте мне показать вам пример того,

141
00:04:47,759 --> 00:04:49,440
как это происходит, и сначала позвольте мне

142
00:04:49,440 --> 00:04:50,639
немного мотивировать это

143
00:04:50,639 --> 00:04:53,199
идеализированным лингвистическим случаем, поэтому я получил

144
00:04:53,199 --> 00:04:56,560
здесь пословную матрицу документа  его

145
00:04:56,560 --> 00:04:58,960
словарный запас корявый, злой, ужасный, хромой

146
00:04:58,960 --> 00:05:00,560
и ужасный, и тщеславие моего

147
00:05:00,560 --> 00:05:02,880
примера состоит в том, что и корявый, и злой

148
00:05:02,880 --> 00:05:04,639
являются положительными терминами, поэтому они имеют тенденцию

149
00:05:04,639 --> 00:05:06,720
сочетаться с удивительным, а не вместе

150
00:05:06,720 --> 00:05:08,880
с хромым и ужасным, однако корявый

151
00:05:08,880 --> 00:05:10,400
и злой никогда не встречаются в языке.  В том же

152
00:05:10,400 --> 00:05:13,039
документе идея состоит в том, что gnarly — это

153
00:05:13,039 --> 00:05:15,039
сленговый положительный термин, связанный с

154
00:05:15,039 --> 00:05:17,039
западным побережьем Соединенных Штатов, а

155
00:05:17,039 --> 00:05:18,560
wicked — сленговый

156
00:05:18,560 --> 00:05:20,320
термин, связанный с восточным

157
00:05:20,320 --> 00:05:22,560
побережьем Соединенных Штатов, в силу того

158
00:05:22,560 --> 00:05:25,280
идеализированного диалектного разделения, которое они никогда не встречаются

159
00:05:25,280 --> 00:05:27,360
в одном и том же документе.  но, тем не менее, у

160
00:05:27,360 --> 00:05:28,880
них есть похожие соседи в этом

161
00:05:28,880 --> 00:05:30,880
векторном пространстве, и это своего рода

162
00:05:30,880 --> 00:05:32,639
абстрактное понятие совместной встречаемости, которое мы

163
00:05:32,639 --> 00:05:34,880
хотим зафиксировать,

164
00:05:34,880 --> 00:05:37,520
если просто используем нашу стандартную диспозицию.

165
00:05:37,520 --> 00:05:39,360
меры сопротивления, методы повторного взвешивания и

166
00:05:39,360 --> 00:05:41,280
т. д., мы не будем улавливать это более

167
00:05:41,280 --> 00:05:43,120
абстрактное понятие одновременности здесь,

168
00:05:43,120 --> 00:05:45,520
расстояния в этом необработанном векторном пространстве,

169
00:05:45,520 --> 00:05:47,440
корявый, ужасный, ужасный и злой,

170
00:05:47,440 --> 00:05:49,600
злой дальше от грубого, даже

171
00:05:49,600 --> 00:05:51,120
чем ужасный, поэтому у нас есть

172
00:05:51,120 --> 00:05:53,280
путаница в настроениях и на самом деле просто не

173
00:05:53,280 --> 00:05:55,680
тот результат, к

174
00:05:55,680 --> 00:05:57,600


175
00:05:57,600 --> 00:05:59,600


176
00:05:59,600 --> 00:06:01,840


177
00:06:01,840 --> 00:06:03,120
которому мы стремились, поэтому мы выполняем разложение по сингулярным значениям на эти три матрицы, а затем усеченная часть заключается в том, что мы собираемся рассмотреть

178
00:06:03,120 --> 00:06:04,880
только первые два измерения

179
00:06:04,880 --> 00:06:07,759
матрицы терминов, соответствующие этим двум

180
00:06:07,759 --> 00:06:10,240
сингулярным значениям  фиксируя два верхних

181
00:06:10,240 --> 00:06:12,960
источника вариаций в данных,

182
00:06:12,960 --> 00:06:15,039
поэтому мы перемножаем их вместе, и мы получаем

183
00:06:15,039 --> 00:06:16,960
эту уменьшенную размерную матрицу

184
00:06:16,960 --> 00:06:17,759
здесь,

185
00:06:17,759 --> 00:06:20,560
две по размеру словаря, и если

186
00:06:20,560 --> 00:06:22,720
мы делаем измерения расстояния в этом пространстве,

187
00:06:22,720 --> 00:06:25,120
как мы надеялись

188
00:06:25,120 --> 00:06:26,560
, теперь корявые и злые  соседи метод

189
00:06:26,560 --> 00:06:29,199
уловил это более абстрактное понятие

190
00:06:29,199 --> 00:06:31,360
наличия тех же соседей, что и другое

191
00:06:31,360 --> 00:06:34,360
слово

192
00:06:34,720 --> 00:06:36,160
в предыдущей лекции, я призываю вас

193
00:06:36,160 --> 00:06:38,080
чтобы подумать о том, что вы делаете с

194
00:06:38,080 --> 00:06:39,840
матрицей, когда выполняете какую-то

195
00:06:39,840 --> 00:06:41,759
схему повторного взвешивания, давайте расширим это до

196
00:06:41,759 --> 00:06:43,120
этих методов уменьшения размерности

197
00:06:43,120 --> 00:06:44,800
, так что вот картина того, что

198
00:06:44,800 --> 00:06:47,360
делает lsa, начиная с исходного

199
00:06:47,360 --> 00:06:49,280
распределения счетчиков здесь,

200
00:06:49,280 --> 00:06:51,360
если я просто запускаю lsa  на этом

201
00:06:51,360 --> 00:06:54,479
распределении количества камней я получаю то, что также выглядит

202
00:06:54,479 --> 00:06:56,880
как очень сложное распределение значений,

203
00:06:56,880 --> 00:06:59,680
значения продаж очень разбросаны, и

204
00:06:59,680 --> 00:07:01,840
у них есть большая масса, сосредоточенная

205
00:07:01,840 --> 00:07:04,080
вокруг нуля, здесь, что соответствует

206
00:07:04,080 --> 00:07:05,919
пику, и необработанные значения здесь также близки к

207
00:07:05,919 --> 00:07:07,840
нулю, поэтому  это не похоже на

208
00:07:07,840 --> 00:07:09,840
то, что мы сделали очень много с точки

209
00:07:09,840 --> 00:07:11,440
зрения укрощения неуправляемого асимметричного

210
00:07:11,440 --> 00:07:13,360
распределения, с которого мы начали,

211
00:07:13,360 --> 00:07:15,280
однако, если вместо этого мы возьмем необработанные

212
00:07:15,280 --> 00:07:17,199
подсчеты и сначала пропустим их через pmi,

213
00:07:17,199 --> 00:07:19,280
который, как мы видели ранее, дает нам это

214
00:07:19,280 --> 00:07:21,120
хорошее распределение  значения сильно

215
00:07:21,120 --> 00:07:23,440
ограничены по оси x,

216
00:07:23,440 --> 00:07:25,599
а затем мы запускаем lsa, мы сохраняем много

217
00:07:25,599 --> 00:07:27,360
этих хороших свойств, значения

218
00:07:27,360 --> 00:07:29,360
несколько более разбросаны, но все еще

219
00:07:29,360 --> 00:07:31,360
хорошо распределены, это выглядит так

220
00:07:31,360 --> 00:07:33,440
гораздо более удачный ввод в последующие

221
00:07:33,440 --> 00:07:35,759
аналитические методы, чем в верхней версии

222
00:07:35,759 --> 00:07:37,199
здесь, и я думаю, что это начинает

223
00:07:37,199 --> 00:07:40,080
показывать, что может быть мощным для конвейерных

224
00:07:40,080 --> 00:07:41,599


225
00:07:41,599 --> 00:07:44,080
методов повторного взвешивания и уменьшения размерности

226
00:07:44,080 --> 00:07:45,440
еще одно примечание, которое я хотел бы сделать, как

227
00:07:45,440 --> 00:07:47,360
вы выбираете размерность для lsa  у него

228
00:07:47,360 --> 00:07:49,599
есть эта переменная k, соответствующая

229
00:07:49,599 --> 00:07:51,840
количеству измерений, которые вы сохраняете,

230
00:07:51,840 --> 00:07:53,840
если вы читаете литературу по lsa, они

231
00:07:53,840 --> 00:07:55,360
часто представляют себе то, что я

232
00:07:55,360 --> 00:07:57,360
назвал сценарием мечты, где вы

233
00:07:57,360 --> 00:07:59,120
рисуете сингулярные значения,

234
00:07:59,120 --> 00:08:00,960
и вы видите, что многие из них  очень

235
00:08:00,960 --> 00:08:03,280
большой, а затем происходит внезапный спад,

236
00:08:03,280 --> 00:08:05,199
и если вы видите это, то очевидно,

237
00:08:05,199 --> 00:08:06,479
что вы должны выбрать точку

238
00:08:06,479 --> 00:08:08,560
внезапного спада в качестве вашего k, поэтому здесь вы

239
00:08:08,560 --> 00:08:10,400
должны выбрать k равно 20, и вы будете

240
00:08:10,400 --> 00:08:12,000
уверены, что захватили  почти

241
00:08:12,000 --> 00:08:13,680
все вариации ваших данных в

242
00:08:13,680 --> 00:08:15,199
уменьшенном пространстве измерений, которые вы

243
00:08:15,199 --> 00:08:16,720
создавали,

244
00:08:16,720 --> 00:08:18,160
к сожалению,

245
00:08:18,160 --> 00:08:20,080
для тех типов матриц и задач,

246
00:08:20,080 --> 00:08:21,919
которые мы рассматриваем, я действительно никогда не

247
00:08:21,919 --> 00:08:23,759
вижу сценария мечты, что я  смотрите,

248
00:08:23,759 --> 00:08:25,360
выглядит гораздо больше похоже на это, где у вас

249
00:08:25,360 --> 00:08:27,520
есть своего рода внезапный спад в начале, а

250
00:08:27,520 --> 00:08:29,599
затем длительный спад и, возможно, внезапный

251
00:08:29,599 --> 00:08:31,840
спад в конце, и в основном

252
00:08:31,840 --> 00:08:33,760
совершенно неясно, где в пространстве вы

253
00:08:33,760 --> 00:08:35,200
должны выбрать k,

254
00:08:35,200 --> 00:08:37,760
и в результате получается k  часто выбирается

255
00:08:37,760 --> 00:08:40,479
эмпирически в качестве гиперпараметра,

256
00:08:40,479 --> 00:08:42,240
настроенного на любую проблему, которую вы на

257
00:08:42,240 --> 00:08:44,000
самом деле пытаетесь решить,

258
00:08:44,000 --> 00:08:45,839
если при выполнении этой работы вы действительно видите

259
00:08:45,839 --> 00:08:47,600
сценарий мечты, пожалуйста, напишите мне,

260
00:08:47,600 --> 00:08:49,600
было бы очень интересно увидеть, что

261
00:08:49,600 --> 00:08:51,920
произойдет

262
00:08:51,920 --> 00:08:54,399
lsa - это всего лишь один  из большого семейства

263
00:08:54,399 --> 00:08:57,040
методов матричной декомпозиции, вот

264
00:08:57,040 --> 00:08:59,200
список некоторых из них, и многие из

265
00:08:59,200 --> 00:09:01,120
них реализованы в scikit-learn в его

266
00:09:01,120 --> 00:09:03,200
библиотеке декомпозиции, и я бы

267
00:09:03,200 --> 00:09:04,959
посоветовал вам попробовать их и просто

268
00:09:04,959 --> 00:09:06,560
посмотреть, как они работают с задачами, которые

269
00:09:06,560 --> 00:09:09,279
вы пытаетесь решить,

270
00:09:09,279 --> 00:09:11,200
и, наконец, вот немного кода

271
00:09:11,200 --> 00:09:12,800
vsm.lsa,

272
00:09:12,800 --> 00:09:15,920
так что с k, установленным на 100, я возвращаю

273
00:09:15,920 --> 00:09:17,279
уменьшенную размерную версию этой

274
00:09:17,279 --> 00:09:19,279
матрицы, сохраняя, конечно, тот же словарь,

275
00:09:19,279 --> 00:09:21,440
но теперь только со 100 столбцами

276
00:09:21,440 --> 00:09:23,839
размеры

277
00:09:24,800 --> 00:09:26,399
давайте перейдем к автоматическим кодировщикам, это

278
00:09:26,399 --> 00:09:28,320
будет пунктом контрактов с lsa, потому

279
00:09:28,320 --> 00:09:30,880
что это гораздо более мощный метод,

280
00:09:30,880 --> 00:09:32,640
поэтому вот обзор автокодировщики —

281
00:09:32,640 --> 00:09:34,240
это гибкий класс архитектур глубокого обучения

282
00:09:34,240 --> 00:09:35,920
для изучения уменьшенных

283
00:09:35,920 --> 00:09:38,160
размерных представлений, если вы

284
00:09:38,160 --> 00:09:40,000
хотите узнать больше о  эта классная

285
00:09:40,000 --> 00:09:41,360
модель, я бы посоветовал вам прочитать

286
00:09:41,360 --> 00:09:43,440
главу 14 книги хорошего парня о

287
00:09:43,440 --> 00:09:45,360
глубоком обучении, в ней много

288
00:09:45,360 --> 00:09:47,360
деталей и множество вариаций на эту

289
00:09:47,360 --> 00:09:48,959
тему,

290
00:09:48,959 --> 00:09:51,680
вот базовая модель автоэнкодера,

291
00:09:51,680 --> 00:09:54,480
входные данные будут, скажем, векторами из

292
00:09:54,480 --> 00:09:56,640
нашего  строки в наших матрицах, так что это

293
00:09:56,640 --> 00:09:58,080
могут быть подсчеты или что-то, что

294
00:09:58,080 --> 00:10:00,240
вы сделали с подсчетами,

295
00:10:00,240 --> 00:10:02,320
которые подаются через скрытый уровень

296
00:10:02,320 --> 00:10:04,399
представления, а затем цель этой

297
00:10:04,399 --> 00:10:06,959
модели - попытаться буквально

298
00:10:06,959 --> 00:10:09,360
восстановить ввод

299
00:10:09,360 --> 00:10:11,680
сейчас, когда  может быть тривиальным, если бы h имел ту

300
00:10:11,680 --> 00:10:13,760
же размерность, что и x, но вся

301
00:10:13,760 --> 00:10:15,760
идея здесь в том, что вы собираетесь

302
00:10:15,760 --> 00:10:18,000
подавать входные данные через очень узкий канал, а

303
00:10:18,000 --> 00:10:20,560
затем пытаться реконструировать

304
00:10:20,560 --> 00:10:21,760
Учитывая, что вы вводите входные данные через

305
00:10:21,760 --> 00:10:23,839
потенциально очень узкую трубу,

306
00:10:23,839 --> 00:10:25,680
маловероятно, что вы сможете полностью

307
00:10:25,680 --> 00:10:28,160
реконструировать входные данные, но идея состоит в

308
00:10:28,160 --> 00:10:29,920
том, что модель научится

309
00:10:29,920 --> 00:10:32,160
реконструировать важные источники вариаций при

310
00:10:32,160 --> 00:10:35,040
выполнении этого автоматического  шаг кодирования,

311
00:10:35,040 --> 00:10:37,680
а затем, когда мы используем эти модели для

312
00:10:37,680 --> 00:10:39,279
обучения представлению в режиме, в котором

313
00:10:39,279 --> 00:10:41,120
мы были для этой единицы,

314
00:10:41,120 --> 00:10:42,959
представление, которое мы выбираем, является этой

315
00:10:42,959 --> 00:10:45,360
скрытой единицей здесь, мы обычно не заботимся

316
00:10:45,360 --> 00:10:47,040
о том, что было реконструировано на

317
00:10:47,040 --> 00:10:49,519
выходе, а скорее только  о скрытом

318
00:10:49,519 --> 00:10:51,200
уменьшенном многомерном представлении,

319
00:10:51,200 --> 00:10:52,959
которое модель изучила, на этом слайде есть

320
00:10:52,959 --> 00:10:54,399
куча других аннотаций, и

321
00:10:54,399 --> 00:10:56,399
причина, по которой я их включил, заключается в том, что

322
00:10:56,399 --> 00:10:58,640
репозиторий курса включает эталонную

323
00:10:58,640 --> 00:11:00,399
реализацию автоэнкодера и

324
00:11:00,399 --> 00:11:01,760
все другие модели глубокого обучения, которые мы рассматриваем

325
00:11:01,760 --> 00:11:04,240
в чистом виде.  numpy, и поэтому, если вы

326
00:11:04,240 --> 00:11:05,839
хотите понять все технические

327
00:11:05,839 --> 00:11:07,680
детали того, как была построена

328
00:11:07,680 --> 00:11:09,920
и оптимизирована модель, вы можете использовать это как

329
00:11:09,920 --> 00:11:11,760
ki  nd шпаргалки, чтобы понять,

330
00:11:11,760 --> 00:11:13,120
как работает код,

331
00:11:13,120 --> 00:11:14,640
я думаю, что основная идея, которую вы

332
00:11:14,640 --> 00:11:16,640
хотите иметь, заключается в том, что модель просто

333
00:11:16,640 --> 00:11:18,480
пытается восстановить свои входные данные.

334
00:11:18,480 --> 00:11:20,000
Сигнал ошибки, который мы получаем, представляет собой

335
00:11:20,000 --> 00:11:21,839
разницу между реконструированным и

336
00:11:21,839 --> 00:11:24,160
фактическим вводом и этим сигналом ошибки.  это

337
00:11:24,160 --> 00:11:26,079
то, что мы используем для обновления

338
00:11:26,079 --> 00:11:29,040
параметров модели. И

339
00:11:29,040 --> 00:11:30,959
последнее, что я хотел бы упомянуть здесь, это то, что

340
00:11:30,959 --> 00:11:32,640
это может быть очень сложно для этой

341
00:11:32,640 --> 00:11:34,240
модели, если вы вводите сюда необработанные

342
00:11:34,240 --> 00:11:35,839
векторы счета, они имеют очень высокую

343
00:11:35,839 --> 00:11:37,600
размерность, и их распределение

344
00:11:37,600 --> 00:11:40,000
сильно искажено, как мы  Я видел, поэтому может быть

345
00:11:40,000 --> 00:11:41,519
очень продуктивно немного

346
00:11:41,519 --> 00:11:43,120
перевзвесить и, возможно, даже

347
00:11:43,120 --> 00:11:45,920
уменьшить размерность с помощью lsa, прежде чем

348
00:11:45,920 --> 00:11:48,640
вы начнете вводить входные данные в эту модель,

349
00:11:48,640 --> 00:11:50,240
конечно, это все еще может иметь смысл,

350
00:11:50,240 --> 00:11:51,600
даже если вы сделали lsa в качестве

351
00:11:51,600 --> 00:11:53,600
предварительного  -этап обработки для изучения скрытого

352
00:11:53,600 --> 00:11:55,440
многомерного представления, потому что эта

353
00:11:55,440 --> 00:11:58,000
модель предположительно способна изучать

354
00:11:58,000 --> 00:12:01,040
даже более абстрактные понятия, чем lsa,

355
00:12:01,040 --> 00:12:03,279
в силу своей нелинейности на этом

356
00:12:03,279 --> 00:12:05,920
h  idden,

357
00:12:05,920 --> 00:12:07,279
и вот немного кода, просто показывающего,

358
00:12:07,279 --> 00:12:09,680
как это работает, используя как эталонную

359
00:12:09,680 --> 00:12:11,279
реализацию, которую я упомянул, так и

360
00:12:11,279 --> 00:12:13,440
более быстрый и гибкий автоматический кодировщик факела,

361
00:12:13,440 --> 00:12:15,760
который также неправильно включен

362
00:12:15,760 --> 00:12:17,760
в репозиторий курса, я думаю, что

363
00:12:17,760 --> 00:12:19,519
единственная вещь интерфейса, которую следует упомянуть здесь  заключается в

364
00:12:19,519 --> 00:12:21,920
том, что эти модели имеют метод подгонки, как и

365
00:12:21,920 --> 00:12:23,279
все другие модели машинного обучения

366
00:12:23,279 --> 00:12:25,200
для этого, но те методы подгонки

367
00:12:25,200 --> 00:12:26,880
возвращают это скрытое трехмерное

368
00:12:26,880 --> 00:12:28,639
представление, которое является целью нашего

369
00:12:28,639 --> 00:12:30,399
обучения в этом контексте, что немного

370
00:12:30,399 --> 00:12:32,399
нестандартно, но это предполагаемое

371
00:12:32,399 --> 00:12:33,680
приложение

372
00:12:33,680 --> 00:12:37,200
для этого вида

373
00:12:37,279 --> 00:12:38,880
представления еще одна вещь, о которой я хотел бы упомянуть, так что

374
00:12:38,880 --> 00:12:40,720
давайте посмотрим, насколько хорошо работает автоматический кодировщик,

375
00:12:40,720 --> 00:12:43,440
это необработанные расстояния

376
00:12:43,440 --> 00:12:45,600
в матрице giga5 для финансов, это

377
00:12:45,600 --> 00:12:48,000
матрица количества, это выглядит не очень хорошо,

378
00:12:48,000 --> 00:12:50,000
если мы запустим автокодировщик непосредственно

379
00:12:50,000 --> 00:12:51,680
на счетчике  матрица выглядит немного

380
00:12:51,680 --> 00:12:53,760
лучше, но все же не идеально,

381
00:12:53,760 --> 00:12:55,200
если мы думаем об этом как о части

382
00:12:55,200 --> 00:12:57,120
конвейера, где мы впервые выполнили положительное

383
00:12:57,120 --> 00:12:59,279
точечное му.  tual, а затем

384
00:12:59,279 --> 00:13:01,360
lsa в измерении 100, а затем выполняется

385
00:13:01,360 --> 00:13:03,440
этап автоматического кодирования, он начинает выглядеть

386
00:13:03,440 --> 00:13:04,880
как действительно хорошее и интересное

387
00:13:04,880 --> 00:13:06,240
семантическое пространство, и я думаю, что это

388
00:13:06,240 --> 00:13:07,839
указывает на возможности включения

389
00:13:07,839 --> 00:13:10,320
автоматического кодировщика в более крупный конвейер

390
00:13:10,320 --> 00:13:12,800
предварительной обработки на  считать матрицы

391
00:13:12,800 --> 00:13:14,720
хорошо, давайте обратимся к GloVe или глобальным

392
00:13:14,720 --> 00:13:16,720
векторам для последней основной единицы для

393
00:13:16,720 --> 00:13:18,320
этого скринкаста

394
00:13:18,320 --> 00:13:19,839
вот краткий обзор GloVe была

395
00:13:19,839 --> 00:13:21,920
представлена гм Джеффри Пеннингтоном

396
00:13:21,920 --> 00:13:23,279
Ричардом Сочером и Крисом в Стэнфордской

397
00:13:23,279 --> 00:13:25,680
команде в 2014 году.

398
00:13:25,680 --> 00:13:27,600
грубо говоря, руководящая идея здесь

399
00:13:27,600 --> 00:13:29,920
заключается в том, что мы хотим  чтобы узнать векторы для

400
00:13:29,920 --> 00:13:32,079
слов, так что скалярное произведение этих

401
00:13:32,079 --> 00:13:34,160
векторов пропорционально логарифмической

402
00:13:34,160 --> 00:13:35,920
вероятности совместного появления для этих

403
00:13:35,920 --> 00:13:38,079
слов, и я подробнее расскажу об этом через

404
00:13:38,079 --> 00:13:39,920
секунду

405
00:13:39,920 --> 00:13:42,240
для выполнения вычислительной работы, мы можем полагаться

406
00:13:42,240 --> 00:13:44,320
на реализацию torchglove.pi,

407
00:13:44,320 --> 00:13:46,399
которая  находится в репозитории курса, я упомяну,

408
00:13:46,399 --> 00:13:47,600
что есть также эталонная

409
00:13:47,600 --> 00:13:50,160
реализация в bsn.pi, она очень медленная,

410
00:13:50,160 --> 00:13:52,000
но как бы прозрачно

411
00:13:52,000 --> 00:13:54,160
реализует кор  Алгоритм GloVe, поэтому его может быть

412
00:13:54,160 --> 00:13:56,160
интересно проверить, а затем, если

413
00:13:56,160 --> 00:13:57,760
вы выполняете практическую работу с действительно

414
00:13:57,760 --> 00:13:59,199
большими корпоративными и действительно большими

415
00:13:59,199 --> 00:14:00,720
словарями, я бы посоветовал вам

416
00:14:00,720 --> 00:14:03,199
использовать ограничение cm команды GloVe, это

417
00:14:03,199 --> 00:14:04,959
выдающийся программный артефакт, который

418
00:14:04,959 --> 00:14:06,720
позволит вам многому научиться.  хороших

419
00:14:06,720 --> 00:14:08,560
представлений быстро,

420
00:14:08,560 --> 00:14:09,839
и это подводит меня к моему последнему

421
00:14:09,839 --> 00:14:11,199
замечанию, я просто хочу упомянуть, что

422
00:14:11,199 --> 00:14:13,680
команда GloVe была одной из первых команд в

423
00:14:13,680 --> 00:14:16,320
nlp, которые выпустили не только данные и код,

424
00:14:16,320 --> 00:14:19,120
но предварительно обученные параметры модели,

425
00:14:19,120 --> 00:14:20,800
все делают это в эти дни, но это  было

426
00:14:20,800 --> 00:14:22,480
редкостью в то время, и я думаю, что эта команда

427
00:14:22,480 --> 00:14:24,079
действительно

428
00:14:24,079 --> 00:14:26,160
дальновидна и видит ценность

429
00:14:26,160 --> 00:14:27,920
высвобождения этих централизованных ресурсов,

430
00:14:27,920 --> 00:14:30,000
и много действительно интересной работы

431
00:14:30,000 --> 00:14:31,040
было выполнено

432
00:14:31,040 --> 00:14:34,959
с векторами GloVe в качестве основы,

433
00:14:35,040 --> 00:14:36,000
хорошо, так что давайте подумаем о

434
00:14:36,000 --> 00:14:37,680
технических аспектах этого  модели,

435
00:14:37,680 --> 00:14:39,120
это цель GloVe, и вы

436
00:14:39,120 --> 00:14:40,079
увидите, как

437
00:14:40,079 --> 00:14:41,760
точечная взаимная информация

438
00:14:41,760 --> 00:14:43,120
вкрадывается в эту картину

439
00:14:43,120 --> 00:14:44,399
интересным образом,

440
00:14:44,399 --> 00:14:46,079
так что это  это уравнение шесть из статьи,

441
00:14:46,079 --> 00:14:47,839
это своего рода идеализированная цель для

442
00:14:47,839 --> 00:14:50,160
модели GloVe, и она говорит то, что я сказал

443
00:14:50,160 --> 00:14:52,560
до того, как у нас есть вектор-строка и вектор-столбец

444
00:14:52,560 --> 00:14:53,680


445
00:14:53,680 --> 00:14:56,320
i wi и wk, мы собираемся получить их

446
00:14:56,320 --> 00:14:58,880
скалярное произведение, и цель состоит в том, чтобы  научитесь,

447
00:14:58,880 --> 00:15:00,880
чтобы этот скалярный продукт был

448
00:15:00,880 --> 00:15:02,320
пропорционален логарифму вероятности

449
00:15:02,320 --> 00:15:04,880
совместного появления слова i и слова k,

450
00:15:04,880 --> 00:15:06,639
где вероятность совместного

451
00:15:06,639 --> 00:15:08,720
появления определяется так, как мы определили ее

452
00:15:08,720 --> 00:15:10,320
раньше, когда мы говорили о нормализации строк,

453
00:15:10,320 --> 00:15:12,000
это просто  сделано в пространстве журнала,

454
00:15:12,000 --> 00:15:14,560
это счетчик совпадений,

455
00:15:14,560 --> 00:15:16,560
это сумма всех счетчиков в

456
00:15:16,560 --> 00:15:18,720
этой строке, и в основном в пространстве журнала

457
00:15:18,720 --> 00:15:20,560
мы просто делим это значение на это

458
00:15:20,560 --> 00:15:21,680
значение,

459
00:15:21,680 --> 00:15:23,760
так что имейте это в виду, теперь причина, по которой у них

460
00:15:23,760 --> 00:15:25,600
есть только строка  представлено то, что

461
00:15:25,600 --> 00:15:27,279
в статье они предполагают, что строки

462
00:15:27,279 --> 00:15:29,199
и столбцы в базовой

463
00:15:29,199 --> 00:15:31,360
матрице счета идентичны, и поэтому нам не

464
00:15:31,360 --> 00:15:33,279
нужно включать и то, и другое, однако, если бы мы

465
00:15:33,279 --> 00:15:35,440
допускали, что строка и контекст могут быть

466
00:15:35,440 --> 00:15:37,120
разными, мы бы просто разработали

467
00:15:37,120 --> 00:15:39,279
уравнение  n 6 чтобы иметь немного другой

468
00:15:39,279 --> 00:15:40,880
правильный знаменатель, мы бы получили

469
00:15:40,880 --> 00:15:43,600
произведение суммы строк и суммы столбцов

470
00:15:43,600 --> 00:15:45,279
, взяли бы логарифм этого и

471
00:15:45,279 --> 00:15:46,880
вычли бы это,

472
00:15:46,880 --> 00:15:48,399
и это было бы нашей целью для

473
00:15:48,399 --> 00:15:50,480
изучения этих точечных произведений здесь,

474
00:15:50,480 --> 00:15:52,639
но ага, это  где pmi прокрадывается,

475
00:15:52,639 --> 00:15:55,440
потому что это просто цель pmi

476
00:15:55,440 --> 00:15:57,519
прямо там, где мы заявили, что как

477
00:15:57,519 --> 00:16:00,000
логарифм вероятности совместного появления, деленный

478
00:16:00,000 --> 00:16:01,600
на произведение вероятностей строки и столбца

479
00:16:01,600 --> 00:16:03,600
здесь, они только что указали

480
00:16:03,600 --> 00:16:05,519
именно это вычисление в пространстве журнала

481
00:16:05,519 --> 00:16:07,519
и  они численно эквивалентны тем,

482
00:16:07,519 --> 00:16:10,320
что эквивалентность логарифма x относительно

483
00:16:10,320 --> 00:16:13,199
y равна логарифму x минус логарифм y,

484
00:16:13,199 --> 00:16:14,959
так что это глубокая связь, которую я

485
00:16:14,959 --> 00:16:17,279
подчеркивал между GloVe и pmi, и я

486
00:16:17,279 --> 00:16:18,639
думаю, что это действительно интересно, потому что

487
00:16:18,639 --> 00:16:20,399
это показывает, что в основном мы

488
00:16:20,399 --> 00:16:23,120
проверяем очень похожую гипотезу, используя

489
00:16:23,120 --> 00:16:25,279
очень похожие понятия контекста строки и столбца,

490
00:16:25,279 --> 00:16:27,040


491
00:16:27,040 --> 00:16:28,639
теперь команда GloVe не просто останавливается на

492
00:16:28,639 --> 00:16:29,519
достигнутом,

493
00:16:29,519 --> 00:16:30,959
цель GloVe на самом деле

494
00:16:30,959 --> 00:16:32,720
намного интереснее, чем  разработка

495
00:16:32,720 --> 00:16:35,440
этого ядра, этой основной идеи PMI, но

496
00:16:35,440 --> 00:16:37,120
стоит иметь в виду PMI, потому что она

497
00:16:37,120 --> 00:16:40,079
присутствует на протяжении всей презентации

498
00:16:40,079 --> 00:16:41,519
в документе, в котором они заявляют, что это своего

499
00:16:41,519 --> 00:16:43,199
рода идеализированная цель, в которой мы

500
00:16:43,199 --> 00:16:45,040
собираемся иметь точечный продукт, как я сказал ранее,

501
00:16:45,040 --> 00:16:46,959
и два смещения  термины, и цель будет состоять

502
00:16:46,959 --> 00:16:48,880
в том, чтобы сделать это эквивалентным журналу

503
00:16:48,880 --> 00:16:50,639
подсчета совпадений,

504
00:16:50,639 --> 00:16:52,800
который имеет некоторые нежелательные свойства

505
00:16:52,800 --> 00:16:54,079
с точки зрения машинного

506
00:16:54,079 --> 00:16:56,399
обучения, поэтому в конце они предлагают

507
00:16:56,399 --> 00:16:58,560
взвешенную версию этой цели, как

508
00:16:58,560 --> 00:16:59,920
вы можете видеть, у нас все еще есть  скалярное

509
00:16:59,920 --> 00:17:01,839
произведение строки и

510
00:17:01,839 --> 00:17:03,920
векторов-столбцов и два члена смещения, которые мы

511
00:17:03,920 --> 00:17:05,280
собираемся вычесть из журнала

512
00:17:05,280 --> 00:17:06,880
количества совпадений

513
00:17:06,880 --> 00:17:08,400
и возьмем его квадрат, и это

514
00:17:08,400 --> 00:17:10,160
будет взвешено f

515
00:17:10,160 --> 00:17:11,760
числа совпадений,

516
00:17:11,760 --> 00:17:13,439
где  f - это функция, которую вы можете

517
00:17:13,439 --> 00:17:14,799
определить вручную, и в статье они делают то,

518
00:17:14,799 --> 00:17:16,720
что это два параметра x

519
00:17:16,720 --> 00:17:19,039
max и alpha

520
00:17:19,039 --> 00:17:21,280
для любого количества, превышающего x max, мы

521
00:17:21,280 --> 00:17:22,959
собираемся установить его равным единице, чтобы как бы

522
00:17:22,959 --> 00:17:25,599
сгладить все  действительно я  arge подсчитывает

523
00:17:25,599 --> 00:17:27,520
все, что меньше x max, мы

524
00:17:27,520 --> 00:17:30,080
возьмем как пропорцию x max с некоторым

525
00:17:30,080 --> 00:17:32,640
экспоненциальным масштабированием, которое определяется

526
00:17:32,640 --> 00:17:35,039
альфой, это функция там, и

527
00:17:35,039 --> 00:17:37,600
обычно альфа устанавливается на 0,75, а x max

528
00:17:37,600 --> 00:17:40,240
200, но я призываю вас

529
00:17:40,240 --> 00:17:42,480
критически относиться к обоим  эти варианты и то,

530
00:17:42,480 --> 00:17:44,480
как они соотносятся с вашими данными, я вернусь

531
00:17:44,480 --> 00:17:46,799
к этому через секунду,

532
00:17:46,799 --> 00:17:48,400
поэтому у GloVe действительно есть эти три гиперпараметра:

533
00:17:48,400 --> 00:17:50,799
изученная размерность,

534
00:17:50,799 --> 00:17:53,360
размерность представлений

535
00:17:53,360 --> 00:17:54,799
x max, которая будет иметь этот

536
00:17:54,799 --> 00:17:56,960
эффект выравнивания, и альфа, которая

537
00:17:56,960 --> 00:17:58,720
будет  правильно масштабируйте значения,

538
00:17:58,720 --> 00:18:00,080
вот пример того, как они

539
00:18:00,080 --> 00:18:01,280
взаимодействуют

540
00:18:01,280 --> 00:18:03,039
x max и alpha, если я начну с этого

541
00:18:03,039 --> 00:18:06,640
вектора 199 75 10 и 1,

542
00:18:06,640 --> 00:18:09,600
функция f, как мы указали

543
00:18:09,600 --> 00:18:12,799
, сгладит это до 1 99 81

544
00:18:12,799 --> 00:18:14,720
18 и 0,03

545
00:18:14,720 --> 00:18:16,160
вы  следует просто знать, что такого

546
00:18:16,160 --> 00:18:19,120
рода уплощение происходит,

547
00:18:19,120 --> 00:18:21,039
так что GloVe обучение, поэтому

548
00:18:21,039 --> 00:18:22,960
интересно подумать аналитически

549
00:18:22,960 --> 00:18:24,640
о том, как любовь умудряется изучать

550
00:18:24,640 --> 00:18:26,640
интересные репрезентации и  одна

551
00:18:26,640 --> 00:18:28,080
вещь, которая может быть у вас на уме, это

552
00:18:28,080 --> 00:18:30,080
вопрос, может ли он на самом деле изучить

553
00:18:30,080 --> 00:18:31,840
понятия совместного возникновения более высокого порядка, который

554
00:18:31,840 --> 00:18:33,440
был основным преимуществом этой

555
00:18:33,440 --> 00:18:35,679
лекции, я привел этот пример с участием

556
00:18:35,679 --> 00:18:38,400
gnarly wicked с lsa, сможет ли GloVe

557
00:18:38,400 --> 00:18:39,919
сделать это правильно  мы могли бы просто

558
00:18:39,919 --> 00:18:42,320
поставить это как вопрос,

559
00:18:42,320 --> 00:18:44,160
поэтому давайте начнем с этого и посмотрим, что произойдет,

560
00:18:44,160 --> 00:18:46,400
посмотрим, как это работает расчеты потерь

561
00:18:46,400 --> 00:18:48,000
для GloVe, это своего рода упрощенная

562
00:18:48,000 --> 00:18:50,240
версия производной модели,

563
00:18:50,240 --> 00:18:51,120


564
00:18:51,120 --> 00:18:52,480
и мы собираемся показать, как GloVe

565
00:18:52,480 --> 00:18:55,120
удается  тянуть грубые и злые к

566
00:18:55,120 --> 00:18:57,360
удивительным в этом маленьком маленьком идеализированном

567
00:18:57,360 --> 00:18:59,200
пространстве, которое я использовал до того, как я собираюсь

568
00:18:59,200 --> 00:19:00,880
опустить термины предвзятости для простоты,

569
00:19:00,880 --> 00:19:02,640
но мы могли бы ввести их,

570
00:19:02,640 --> 00:19:04,240
и вот как это будет

571
00:19:04,240 --> 00:19:06,400
происходить, что я только что сделал  потому что этот

572
00:19:06,400 --> 00:19:08,640
идеализированный пример начинается в GloVe

573
00:19:08,640 --> 00:19:10,640
пространстве, где злые и грубые настолько далеки

574
00:19:10,640 --> 00:19:12,160
друг от друга, насколько я мог бы их сделать настолько

575
00:19:12,160 --> 00:19:14,000
разными, насколько я мог бы их сделать, у

576
00:19:14,000 --> 00:19:16,000
меня есть классный и ужасный, а

577
00:19:16,000 --> 00:19:17,760
классный как бы близок к  корявый

578
00:19:17,760 --> 00:19:19,760
уже то, что вы увидите, это то, что после

579
00:19:19,760 --> 00:19:22,559
всего лишь одной итерации модели

580
00:19:22,559 --> 00:19:25,039
произошло то, что злой и корявый

581
00:19:25,039 --> 00:19:27,039
были потянуты к ужасному, и это как

582
00:19:27,039 --> 00:19:28,720
раз тот эффект, который мы хотели

583
00:19:28,720 --> 00:19:30,559
, это тот смысл, в котором GloVe может

584
00:19:30,559 --> 00:19:32,320
уловить этот более высокий порядок  понятия

585
00:19:32,320 --> 00:19:33,840
сосуществования

586
00:19:33,840 --> 00:19:35,919
чуть более подробно, вы, возможно,

587
00:19:35,919 --> 00:19:37,280
захотите изучить это самостоятельно, но

588
00:19:37,280 --> 00:19:39,200
общий обзор того, как именно

589
00:19:39,200 --> 00:19:41,600
происходит это обучение, происходит следующим образом: мы

590
00:19:41,600 --> 00:19:43,679
начинаем с этих подсчетов здесь, и

591
00:19:43,679 --> 00:19:45,840
решающим предположением, которое я делаю, является

592
00:19:45,840 --> 00:19:48,240
злой и грубый никогда не встречаются вместе,

593
00:19:48,240 --> 00:19:50,320
но они часто встречаются с удивительным.

594
00:19:50,320 --> 00:19:51,280
Удивительный будет своего рода

595
00:19:51,280 --> 00:19:53,840
гравитационным притяжением, из-за которого грубый и

596
00:19:53,840 --> 00:19:56,080
злой выглядят одинаково,

597
00:19:56,080 --> 00:19:57,520
имейте в виду, что из-за этой

598
00:19:57,520 --> 00:19:59,760
функции f в целом с GloVe мы

599
00:19:59,760 --> 00:20:01,200
имеем дело не с  необработанные подсчеты, а

600
00:20:01,200 --> 00:20:02,960
скорее с повторно взвешенной матрицей, и

601
00:20:02,960 --> 00:20:04,960
это сохраняет то свойство, что они

602
00:20:04,960 --> 00:20:06,720
никогда не встречались одновременно, дает разные

603
00:20:06,720 --> 00:20:08,240
масштабированные значения для остальной части

604
00:20:08,240 --> 00:20:10,159
совместного появления или псевдосовместного появления.  вероятности появления в

605
00:20:10,159 --> 00:20:12,400


606
00:20:12,400 --> 00:20:13,600
порядке, а затем вот так вот, что

607
00:20:13,600 --> 00:20:15,039
мы собираемся отслеживать, это корявый

608
00:20:15,039 --> 00:20:17,200
вектор в нуле, и вы можете видеть, что я сделал

609
00:20:17,200 --> 00:20:18,720
их настолько далеко друг от друга, насколько мог, они как

610
00:20:18,720 --> 00:20:20,480
бы противопоставлены друг другу

611
00:20:20,480 --> 00:20:21,679
мы  мы увидим, как они тянутся

612
00:20:21,679 --> 00:20:24,640
к удивительным в контексте фактора,

613
00:20:24,640 --> 00:20:26,400
так что это расчет потерь, я

614
00:20:26,400 --> 00:20:28,640
только что вставил сюда все значения, и

615
00:20:28,640 --> 00:20:30,640
вы можете видеть, что мы получаем этот начальный

616
00:20:30,640 --> 00:20:32,000
набор потерь,

617
00:20:32,000 --> 00:20:34,080
который после одной итерации мы обновляем

618
00:20:34,080 --> 00:20:36,400
матрицы весов и  мы выполняем еще один

619
00:20:36,400 --> 00:20:38,080
раунд обучения, и вы можете видеть, что

620
00:20:38,080 --> 00:20:40,159
обе эти модели значения здесь

621
00:20:40,159 --> 00:20:42,000
становятся больше, что соответствует тому,

622
00:20:42,000 --> 00:20:43,919
что они подтягиваются все ближе и ближе к

623
00:20:43,919 --> 00:20:46,000
потрясающему, и вы можете видеть, что это графически

624
00:20:46,000 --> 00:20:48,000
происходит здесь, на этих графиках

625
00:20:48,000 --> 00:20:50,320
слева и как я  сделайте больше итераций

626
00:20:50,320 --> 00:20:52,240
модели GloVe, этот эффект только

627
00:20:52,240 --> 00:20:54,000
усилится, соответствуя тому, что

628
00:20:54,000 --> 00:20:56,159
злые и грубые будут тянуться к

629
00:20:56,159 --> 00:20:58,400
ужасным и прочь от ужасных в

630
00:20:58,400 --> 00:21:00,720
результате этих основных подсчетов, и

631
00:21:00,720 --> 00:21:02,720
я беру  является хорошим доказательством того, что GloVe,

632
00:21:02,720 --> 00:21:04,640
как и другие методы, которые мы обсуждали

633
00:21:04,640 --> 00:21:06,559
, способна улавливать те

634
00:21:06,559 --> 00:21:08,080
понятия совместного возникновения более высокого порядка, которые

635
00:21:08,080 --> 00:21:10,080
мы так заинтересованы в продвижении с помощью

636
00:21:10,080 --> 00:21:12,799
этих методов,

637
00:21:13,600 --> 00:21:15,360
давайте замкнем петлю, также у нас есть те

638
00:21:15,360 --> 00:21:17,200
центральные вопросы, что такое GloVe  работая с

639
00:21:17,200 --> 00:21:19,200
нашими базовыми пространствами с помощью GloVe из

640
00:21:19,200 --> 00:21:21,120
-за дизайна матрицы, мы должны

641
00:21:21,120 --> 00:21:23,039
начать с

642
00:21:23,039 --> 00:21:25,679
слова за словом матрицы совпадений

643
00:21:25,679 --> 00:21:26,720
подсчетов,

644
00:21:26,720 --> 00:21:28,480
поэтому мы начинаем с этих необработанных значений подсчета

645
00:21:28,480 --> 00:21:30,240
и GloVe.

646
00:21:30,240 --> 00:21:32,080
к этим уменьшенным

647
00:21:32,080 --> 00:21:34,720
представлениям размеров и, мальчик,

648
00:21:34,720 --> 00:21:36,480
по критериям, которые мы установили, делает ли GloVe

649
00:21:36,480 --> 00:21:39,039
выдающуюся работу, это результат

650
00:21:39,039 --> 00:21:41,600
запуска GloVe в измерении 50,

651
00:21:41,600 --> 00:21:43,120
и вы можете видеть, что значения

652
00:21:43,120 --> 00:21:44,960
очень хорошо масштабируются между отрицательными

653
00:21:44,960 --> 00:21:47,120
двумя и двумя и хорошо нормально

654
00:21:47,120 --> 00:21:49,919
распределено, это выдающийся вклад

655
00:21:49,919 --> 00:21:52,480
в современные модели машинного обучения, и я

656
00:21:52,480 --> 00:21:54,720
думаю, что это, вероятно, нетривиальный

657
00:21:54,720 --> 00:21:56,840
аспект того, почему GloVe были настолько

658
00:21:56,840 --> 00:21:59,360
успешными в качестве своего рода предварительного обучения.

659
00:21:59,360 --> 00:22:01,360
d для множества последующих

660
00:22:01,360 --> 00:22:04,400
архитектур машинного обучения,

661
00:22:04,640 --> 00:22:06,400
а затем небольшой фрагмент кода,

662
00:22:06,400 --> 00:22:07,919
просто показывающий вам, как вы можете работать с

663
00:22:07,919 --> 00:22:10,320
этими интерфейсами, используя нашу кодовую базу.

664
00:22:10,320 --> 00:22:11,840


665
00:22:11,840 --> 00:22:13,600
определили

666
00:22:13,600 --> 00:22:16,240
процент ненулевых значений этой функции

667
00:22:16,240 --> 00:22:17,200
выше,

668
00:22:17,200 --> 00:22:19,360
и вы можете установить здесь x max и просто

669
00:22:19,360 --> 00:22:21,840
ввести матрицу и изучить, какой

670
00:22:21,840 --> 00:22:23,760
процент значений в этой матрице

671
00:22:23,760 --> 00:22:25,919
будет сглажен до 1 в

672
00:22:25,919 --> 00:22:28,159
результате x max, который вы  выбрали, и это

673
00:22:28,159 --> 00:22:30,640
значение действительно зависит от дизайна

674
00:22:30,640 --> 00:22:33,440
матрицы, если я подаю yelp 5,

675
00:22:33,440 --> 00:22:35,919
только около 5 значений становятся

676
00:22:35,919 --> 00:22:38,320
сглаженными, но если я подаю yelp 20,

677
00:22:38,320 --> 00:22:40,159
который намного плотнее и имеет гораздо более высокие

678
00:22:40,159 --> 00:22:41,039


679
00:22:41,039 --> 00:22:43,360
значения 20 значений

680
00:22:43,360 --> 00:22:45,520
сглаживаются до того, что вы знаете, если это число становится

681
00:22:45,520 --> 00:22:47,200
слишком большим, матрица может стать

682
00:22:47,200 --> 00:22:48,960
полностью однородной, и поэтому

683
00:22:48,960 --> 00:22:50,799
мы должны действительно знать,

684
00:22:50,799 --> 00:22:53,039
как настройка x max влияет

685
00:22:53,039 --> 00:22:54,640
на тип обучения, которое мы могли бы даже

686
00:22:54,640 --> 00:22:57,120
выполнять с помощью wi  GloVe, и может

687
00:22:57,120 --> 00:22:58,720
оказаться, что это даже

688
00:22:58,720 --> 00:23:00,960
важнее, чем количество итераций

689
00:23:00,960 --> 00:23:02,320
или размерность

690
00:23:02,320 --> 00:23:04,559
представлений, которые вы изучаете

691
00:23:04,559 --> 00:23:05,840
после того, как сделали этот выбор,

692
00:23:05,840 --> 00:23:07,360
хотя интерфейс довольно ясен, а

693
00:23:07,360 --> 00:23:09,840
метод подгонки, как и в случае с автоэнкодером,

694
00:23:09,840 --> 00:23:11,520
возвращает  матрица изученных

695
00:23:11,520 --> 00:23:13,280
репрезентаций, которые мы хотим использовать

696
00:23:13,280 --> 00:23:14,960
для текущих целей,

697
00:23:14,960 --> 00:23:17,360
и, наконец, я включил метод оценки,

698
00:23:17,360 --> 00:23:19,600
и метод оценки буквально

699
00:23:19,600 --> 00:23:21,360
просто проверяет,

700
00:23:21,360 --> 00:23:23,440
насколько хорошо изученные вами векторы

701
00:23:23,440 --> 00:23:25,120
соответствуют цели GloVe -

702
00:23:25,120 --> 00:23:28,000
иметь  точечные произведения должны быть

703
00:23:28,000 --> 00:23:29,600
пропорциональны логарифму вероятностей совпадений,

704
00:23:29,600 --> 00:23:31,120
и вы можете получить оценку

705
00:23:31,120 --> 00:23:32,960
за то, что у нас здесь все хорошо

706
00:23:32,960 --> 00:23:37,559
, скажем, для большой эмпирической матрицы,

707
00:23:38,000 --> 00:23:39,840
последний раздел, давайте просто скажем немного о

708
00:23:39,840 --> 00:23:41,360
визуализации, и это наше

709
00:23:41,360 --> 00:23:43,120
уменьшение размерности  метод в

710
00:23:43,120 --> 00:23:44,720
том смысле, что весь смысл в том, чтобы

711
00:23:44,720 --> 00:23:46,400
попытаться сгладить пространство очень высокого измерения,

712
00:23:46,400 --> 00:23:48,400
возможно, в два или три

713
00:23:48,400 --> 00:23:49,600
измерения, которые

714
00:23:49,600 --> 00:23:51,520
вы должны распознать.  Я понимаю, что

715
00:23:51,520 --> 00:23:53,520
это неизбежно повлечет за собой множество

716
00:23:53,520 --> 00:23:55,840
компромиссов, просто невозможно зафиксировать все

717
00:23:55,840 --> 00:23:57,679
источники вариаций в вашей базовой

718
00:23:57,679 --> 00:24:00,400
матрице всего в нескольких измерениях, но,

719
00:24:00,400 --> 00:24:02,720
тем не менее, это может быть продуктивно.

720
00:24:02,720 --> 00:24:04,720


721
00:24:04,720 --> 00:24:07,039
на

722
00:24:07,039 --> 00:24:09,039
качественном исследовании с использованием чего-то

723
00:24:09,039 --> 00:24:11,600
вроде соседей bsm, чтобы понять на

724
00:24:11,600 --> 00:24:14,000
низком уровне, что кодирует ваша матрица, а

725
00:24:14,000 --> 00:24:15,679
затем визуализация высокого уровня может

726
00:24:15,679 --> 00:24:18,640
быть своего рода аналогом того, что

727
00:24:18,640 --> 00:24:20,480
существует множество методов визуализации,

728
00:24:20,480 --> 00:24:22,000
и многие из них реализованы в

729
00:24:22,000 --> 00:24:24,000
пакете scikit коллектора

730
00:24:24,000 --> 00:24:26,320
поэтому я призываю вас использовать их,

731
00:24:26,320 --> 00:24:28,159
я собираюсь показать вам некоторые результаты из

732
00:24:28,159 --> 00:24:30,240
этого колена, которое означает встраивание

733
00:24:30,240 --> 00:24:31,360
распределенного

734
00:24:31,360 --> 00:24:33,600
стохастического соседа,

735
00:24:33,600 --> 00:24:35,200
там есть много руководств пользователя, которые

736
00:24:35,200 --> 00:24:36,799
вы можете изучить для получения более подробной информации, позвольте мне

737
00:24:36,799 --> 00:24:38,799
просто дать вам высокую оценку  уровень, который должен

738
00:24:38,799 --> 00:24:41,600
работать на нашей матрице giga20, я думаю, что

739
00:24:41,600 --> 00:24:43,520
это типичный довольно хороший

740
00:24:43,520 --> 00:24:45,279
результат, поэтому то, что мы видим здесь, это

741
00:24:45,279 --> 00:24:48,000
несколько карманов.  высокой плотности, это

742
00:24:48,000 --> 00:24:50,559
области локальной согласованности в глобальном масштабе, мы

743
00:24:50,559 --> 00:24:52,320
должны быть осторожны, чтобы не переоценить

744
00:24:52,320 --> 00:24:54,799
всю эту диаграмму, потому что, когда вы

745
00:24:54,799 --> 00:24:56,400
повторно запускаете модель с другими случайными начальными значениями,

746
00:24:56,400 --> 00:24:57,840
вы увидите, что она как бы

747
00:24:57,840 --> 00:24:59,600
переориентируется в разных частях

748
00:24:59,600 --> 00:25:01,440
соответственно близко к другим другим

749
00:25:01,440 --> 00:25:04,000
части, но на что вы можете рассчитывать довольно

750
00:25:04,000 --> 00:25:06,080
надежно, так это на то, что эти локальные

751
00:25:06,080 --> 00:25:09,120
очаги когерентности соответствуют

752
00:25:09,120 --> 00:25:11,440
частям когерентности пространства, которое вы определили,

753
00:25:11,440 --> 00:25:13,360
и если вы увеличите их, вы сможете как

754
00:25:13,360 --> 00:25:15,520
бы оценить, что обнаружила модель,

755
00:25:15,520 --> 00:25:17,600
поэтому для этого  giga21 например, я думаю, что

756
00:25:17,600 --> 00:25:19,440
мы видим заметные кластеры, соответствующие

757
00:25:19,440 --> 00:25:22,640
таким вещам, как приготовление пищи и конфликты, если

758
00:25:22,640 --> 00:25:24,799
мы снова сделаем то же самое для нашей матрицы визга,

759
00:25:24,799 --> 00:25:27,120
это выглядит довольно хорошо с точки зрения

760
00:25:27,120 --> 00:25:29,120
наличия некоторой подструктуры, которую мы могли бы

761
00:25:29,120 --> 00:25:31,200
проанализировать, и если мы увеличим масштаб, мы увидим

762
00:25:31,200 --> 00:25:33,360
кластеры  как положительные термины и

763
00:25:33,360 --> 00:25:35,440
отрицательные термины, соответствующие

764
00:25:35,440 --> 00:25:37,679
оценочной установке этих обзоров визга,

765
00:25:37,679 --> 00:25:39,200
так что все это очень обнадеживает и

766
00:25:39,200 --> 00:25:41,279
предполагает, что лежащий в основе  пробелы

767
00:25:41,279 --> 00:25:42,960
имеют действительно интересную структуру, которая

768
00:25:42,960 --> 00:25:47,120
может быть полезна для последующего анализа,

769
00:25:47,120 --> 00:25:48,799
и вот несколько фрагментов кода, у нас есть

770
00:25:48,799 --> 00:25:51,760
эта простая оболочка вокруг реализации scikit

771
00:25:51,760 --> 00:25:53,360
disney, которая позволит

772
00:25:53,360 --> 00:25:55,760
вам гибко работать с этим материалом,

773
00:25:55,760 --> 00:25:56,880
используя

774
00:25:56,880 --> 00:25:59,120
матрицы подсчета из нашего модуля, и я

775
00:25:59,120 --> 00:26:00,799
просто упомянем здесь, что это довольно

776
00:26:00,799 --> 00:26:02,400
легко, если вы хотите закодировать

777
00:26:02,400 --> 00:26:04,320
слова в своем словаре цветом, скажем, в соответствии

778
00:26:04,320 --> 00:26:06,240
с словарем настроений или каким-либо другим

779
00:26:06,240 --> 00:26:08,400
видом словаря, который может помочь

780
00:26:08,400 --> 00:26:10,400
вам точно определить, какую структуру

781
00:26:10,400 --> 00:26:12,400
ваша модель смогла раскрыть с помощью

782
00:26:12,400 --> 00:26:14,240
уважение к этим лежащим в основе ярлыкам, и

783
00:26:14,240 --> 00:26:17,720
это может быть полезно

