1
00:00:00,000 --> 00:00:04,288


2
00:00:04,288 --> 00:00:05,830
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,830 --> 00:00:08,100
This is part two in our
series on contextual word

4
00:00:08,100 --> 00:00:08,928
representations.

5
00:00:08,928 --> 00:00:11,470
We're going to be talking about
the transformer architecture,

6
00:00:11,470 --> 00:00:13,980
which is the central piece
for all the models we'll

7
00:00:13,980 --> 00:00:15,930
be exploring in this unit.

8
00:00:15,930 --> 00:00:17,700
Let's dive into the
model structure,

9
00:00:17,700 --> 00:00:19,830
we'll work through this
using a simple example.

10
00:00:19,830 --> 00:00:22,440
At the bottom here I've got
the input sequence, "the Rock

11
00:00:22,440 --> 00:00:24,830
rules" and I've indicated
in red that we're

12
00:00:24,830 --> 00:00:26,580
going to be keeping
track of the positions

13
00:00:26,580 --> 00:00:29,508
of each one of those
tokens in the sequence.

14
00:00:29,508 --> 00:00:31,050
But the first step
is a familiar one.

15
00:00:31,050 --> 00:00:33,480
We're going to look at both
the words and the positions

16
00:00:33,480 --> 00:00:35,700
in separate embedding spaces.

17
00:00:35,700 --> 00:00:37,500
Those are fixed embedding
spaces that we'll

18
00:00:37,500 --> 00:00:40,320
learn as part of learning all
the parameters in this model.

19
00:00:40,320 --> 00:00:42,480
I've given the word
embeddings in light

20
00:00:42,480 --> 00:00:45,900
gray and the positional
embeddings in dark gray.

21
00:00:45,900 --> 00:00:48,120
To form what we think of
as the actual embedding

22
00:00:48,120 --> 00:00:50,550
for this model, we do
an elementwise addition

23
00:00:50,550 --> 00:00:53,310
of the word embedding with
the positional embedding.

24
00:00:53,310 --> 00:00:54,990
And that gives us
the representations

25
00:00:54,990 --> 00:00:56,880
that are in green here,
and you can see that

26
00:00:56,880 --> 00:00:58,338
on the right side
of the slide, I'm

27
00:00:58,338 --> 00:01:01,260
going to be keeping track of all
of the calculations with regard

28
00:01:01,260 --> 00:01:03,630
to this C column here
and they're completely

29
00:01:03,630 --> 00:01:05,790
parallel for columns A and B.

30
00:01:05,790 --> 00:01:09,540
So to form C input, we do
elementwise addition of X34,

31
00:01:09,540 --> 00:01:11,310
the embedding for
the word "rules"

32
00:01:11,310 --> 00:01:16,390
and P3 which is the embedding
for position 3 in this model.

33
00:01:16,390 --> 00:01:19,270
The next layer is really the
hallmark of this architecture

34
00:01:19,270 --> 00:01:20,800
and what gives the
paper its title,

35
00:01:20,800 --> 00:01:22,520
Attention is All You Need.

36
00:01:22,520 --> 00:01:24,880
We're going to form a
bunch of dense dot product

37
00:01:24,880 --> 00:01:27,850
connections between all
of these representations.

38
00:01:27,850 --> 00:01:29,380
So you can think
of those as forming

39
00:01:29,380 --> 00:01:31,240
these connections
that look like there's

40
00:01:31,240 --> 00:01:32,980
a dense thicket of them.

41
00:01:32,980 --> 00:01:35,230
On the right here, I've
given the core calculation

42
00:01:35,230 --> 00:01:37,870
and it should be familiar
from part one in this unit.

43
00:01:37,870 --> 00:01:40,360
It's exactly the calculation
I've presented there

44
00:01:40,360 --> 00:01:43,930
with just two small changes,
but fundamentally if our target

45
00:01:43,930 --> 00:01:45,640
vector is the C
input here, we're

46
00:01:45,640 --> 00:01:49,480
attending to inputs A
and B, and we do that

47
00:01:49,480 --> 00:01:51,250
by forming the
dot products here.

48
00:01:51,250 --> 00:01:53,890
And the one twist from before
is that instead of just taking

49
00:01:53,890 --> 00:01:56,020
those dot products,
we'll normalize them

50
00:01:56,020 --> 00:01:59,800
by the square root of the
dimensionality of the model dk.

51
00:01:59,800 --> 00:02:02,320
dk is an important value
here because of the way

52
00:02:02,320 --> 00:02:05,020
we combine representations
in the transformer.

53
00:02:05,020 --> 00:02:07,690
All of the outputs and
all the layers we look at

54
00:02:07,690 --> 00:02:10,728
have to have the same
dimensionality as given by dk.

55
00:02:10,728 --> 00:02:12,520
And so what we're doing
here is essentially

56
00:02:12,520 --> 00:02:14,478
scaling these dot products
to kind of keep them

57
00:02:14,478 --> 00:02:17,300
within a sensible range.

58
00:02:17,300 --> 00:02:20,040
That gives us a score
vector off the tilde,

59
00:02:20,040 --> 00:02:21,423
we softmax normalize that.

60
00:02:21,423 --> 00:02:23,090
And then the other
twist is that instead

61
00:02:23,090 --> 00:02:26,060
of using mean here as we did
before, we use summation.

62
00:02:26,060 --> 00:02:28,790
But the actual vector is the
one we calculated before, we're

63
00:02:28,790 --> 00:02:32,090
going to take weighted
versions of A input and B input

64
00:02:32,090 --> 00:02:35,690
according to this vector of
weights that we created here.

65
00:02:35,690 --> 00:02:38,150
That gives us the
representation C attention

66
00:02:38,150 --> 00:02:40,250
as given in orange
here, and we do

67
00:02:40,250 --> 00:02:43,690
that of course for all the
other positions in the model.

68
00:02:43,690 --> 00:02:45,500
The next step is
kind of interesting,

69
00:02:45,500 --> 00:02:47,800
we're creating what's called
a residual connection.

70
00:02:47,800 --> 00:02:50,650
So to get CA layer
here in yellow,

71
00:02:50,650 --> 00:02:54,580
we add up C input and this
attention representation

72
00:02:54,580 --> 00:02:56,830
that we just created
and apply dropout

73
00:02:56,830 --> 00:02:58,640
as a regularization step there.

74
00:02:58,640 --> 00:03:00,877
And that gives us CA layer,
the interesting thing

75
00:03:00,877 --> 00:03:02,710
there of course is this
residual connection.

76
00:03:02,710 --> 00:03:05,830
Instead of simply feeding
forward C attention,

77
00:03:05,830 --> 00:03:07,540
we feed forward
actually a version of it

78
00:03:07,540 --> 00:03:13,170
that's combined with our initial
positionally encoded embedding.

79
00:03:13,170 --> 00:03:16,042
If we follow that with a step
of layer normalization, which

80
00:03:16,042 --> 00:03:17,500
should help with
optimization, it's

81
00:03:17,500 --> 00:03:19,030
going to kind of
scale the weights

82
00:03:19,030 --> 00:03:21,220
in these representations.

83
00:03:21,220 --> 00:03:22,670
The next step is
more meaningful.

84
00:03:22,670 --> 00:03:25,180
This is a series of
two dense layers,

85
00:03:25,180 --> 00:03:27,340
so we'll take Ca
norm here and feed it

86
00:03:27,340 --> 00:03:30,370
through this dense layer
with a non-linearity followed

87
00:03:30,370 --> 00:03:33,430
by another linear
layer to give us Cfx

88
00:03:33,430 --> 00:03:35,553
that's given in dark blue here.

89
00:03:35,553 --> 00:03:36,970
And that's followed
by another one

90
00:03:36,970 --> 00:03:38,920
of these interesting
residual connections,

91
00:03:38,920 --> 00:03:41,350
so we'll apply dropout to Cff.

92
00:03:41,350 --> 00:03:42,970
And then add that
in with Ca norm,

93
00:03:42,970 --> 00:03:44,720
as given down here
at the bottom,

94
00:03:44,720 --> 00:03:47,790
and that gives us the second
yellow representation.

95
00:03:47,790 --> 00:03:51,100
And we follow that by one more
step of layer normalization

96
00:03:51,100 --> 00:03:54,160
and that gives us the output
for this block of transformer

97
00:03:54,160 --> 00:03:56,305
representations.

98
00:03:56,305 --> 00:03:57,930
And you can imagine
of course as you'll

99
00:03:57,930 --> 00:04:00,330
see that we can stack up
these transformer blocks,

100
00:04:00,330 --> 00:04:02,790
and the way that we
do that is essentially

101
00:04:02,790 --> 00:04:05,790
by taking these dark green
representations at the top

102
00:04:05,790 --> 00:04:08,280
here and using them as inputs
and all the calculations

103
00:04:08,280 --> 00:04:08,920
are the same.

104
00:04:08,920 --> 00:04:11,220
So you might imagine that
we could continue here

105
00:04:11,220 --> 00:04:13,200
by just doing a dense
series of attention

106
00:04:13,200 --> 00:04:15,030
connections across
these and then

107
00:04:15,030 --> 00:04:17,740
continuing on with the
calculations I just presented.

108
00:04:17,740 --> 00:04:20,535
And in that way, we could
stack up transformer blocks.

109
00:04:20,535 --> 00:04:23,075
And I'll return
to that later on.

110
00:04:23,075 --> 00:04:24,450
There are a few
other things that

111
00:04:24,450 --> 00:04:25,620
are worth pointing
out that are kind

112
00:04:25,620 --> 00:04:27,230
of noteworthy about this model.

113
00:04:27,230 --> 00:04:30,512
It looks like a complicated
series of calculations,

114
00:04:30,512 --> 00:04:32,220
but I would say that
fundamentally what's

115
00:04:32,220 --> 00:04:35,610
happening here is we're doing
positional encoding to get

116
00:04:35,610 --> 00:04:38,280
embeddings, so that they
are position-sensitive

117
00:04:38,280 --> 00:04:40,560
representations of words.

118
00:04:40,560 --> 00:04:42,630
We follow that within
an attention layer

119
00:04:42,630 --> 00:04:44,970
which creates that dense
thicket of connections

120
00:04:44,970 --> 00:04:48,330
between all of the words
as positionally encoded.

121
00:04:48,330 --> 00:04:50,490
Then we have these
optimization things woven in,

122
00:04:50,490 --> 00:04:53,220
but fundamentally we're
following that attention step

123
00:04:53,220 --> 00:04:55,560
with two series of
feed-forward layer steps

124
00:04:55,560 --> 00:04:59,260
here, followed by the same
process of dropout and layer

125
00:04:59,260 --> 00:05:00,450
normalization.

126
00:05:00,450 --> 00:05:03,410
So if you kind of elided
the yellow and the purple,

127
00:05:03,410 --> 00:05:05,160
you would see that
what we're really doing

128
00:05:05,160 --> 00:05:07,237
is attention followed
by feed-forward.

129
00:05:07,237 --> 00:05:08,820
And then as we stack
these things that

130
00:05:08,820 --> 00:05:10,880
would be attention
feed-forward, attention

131
00:05:10,880 --> 00:05:12,750
feed-forward as we climbed up.

132
00:05:12,750 --> 00:05:15,060
And interwoven into
there are some things

133
00:05:15,060 --> 00:05:17,920
that I would say help
with optimization.

134
00:05:17,920 --> 00:05:20,410
Another noteworthy
thing about this model

135
00:05:20,410 --> 00:05:22,330
is that the only
sense in which we

136
00:05:22,330 --> 00:05:25,210
are keeping track of the
linear order of the sequence

137
00:05:25,210 --> 00:05:27,100
is in those
positional embeddings,

138
00:05:27,100 --> 00:05:30,640
if not for them the column order
would be completely irrelevant.

139
00:05:30,640 --> 00:05:32,140
Because of course,
we've created all

140
00:05:32,140 --> 00:05:35,090
of these symmetric connections
at the attention layer.

141
00:05:35,090 --> 00:05:36,670
And there are no
other connections

142
00:05:36,670 --> 00:05:37,970
across these columns.

143
00:05:37,970 --> 00:05:40,900
So the only sense in
which column order, that

144
00:05:40,900 --> 00:05:44,020
is word order matters here,
is via those positional

145
00:05:44,020 --> 00:05:44,997
embeddings.

146
00:05:44,997 --> 00:05:47,800


147
00:05:47,800 --> 00:05:50,700
Here's a more detailed look
at the attention calculations

148
00:05:50,700 --> 00:05:51,332
themselves.

149
00:05:51,332 --> 00:05:53,040
I just want to bring
up how this actually

150
00:05:53,040 --> 00:05:54,610
works at a mechanical level.

151
00:05:54,610 --> 00:05:56,520
So this is the calculation
as I presented it

152
00:05:56,520 --> 00:06:00,000
on the previous slide and
in part one of this unit.

153
00:06:00,000 --> 00:06:02,070
In the paper, and
now commonly it's

154
00:06:02,070 --> 00:06:03,750
presented in this matrix format.

155
00:06:03,750 --> 00:06:06,120
And if you're like
me, it's not obvious

156
00:06:06,120 --> 00:06:08,870
right away that these are
equivalent calculations.

157
00:06:08,870 --> 00:06:11,010
So what I've done for
these next two slides

158
00:06:11,010 --> 00:06:14,190
is to show you via
worked out examples

159
00:06:14,190 --> 00:06:16,740
how those calculations
work and how they arrive

160
00:06:16,740 --> 00:06:18,570
at exactly the same values.

161
00:06:18,570 --> 00:06:20,580
I'm not gonna spend too
much time on this here,

162
00:06:20,580 --> 00:06:22,560
this is really just
here for you if you

163
00:06:22,560 --> 00:06:24,810
would like to work through
the calculations in detail,

164
00:06:24,810 --> 00:06:27,480
which I strongly encourage
because this is really

165
00:06:27,480 --> 00:06:30,092
the fundamental
step in this model.

166
00:06:30,092 --> 00:06:31,550
And here's all the
details that you

167
00:06:31,550 --> 00:06:35,030
would need to get hands
on with these ideas.

168
00:06:35,030 --> 00:06:37,100
Now so far, I've
presented attention

169
00:06:37,100 --> 00:06:38,720
in a kind of simplified way.

170
00:06:38,720 --> 00:06:41,450
A hallmark of attention
is in the transformer

171
00:06:41,450 --> 00:06:43,880
is that it is typically
multi-headed attention.

172
00:06:43,880 --> 00:06:46,970
So let me unpack that idea
a little bit concretely.

173
00:06:46,970 --> 00:06:50,120
We'll start with our
input sequence from before

174
00:06:50,120 --> 00:06:52,940
and we'll be looking at these
green representations here.

175
00:06:52,940 --> 00:06:55,370
And the idea behind the
multi-headed attention

176
00:06:55,370 --> 00:06:56,840
mechanisms that
we're gonna inject

177
00:06:56,840 --> 00:07:00,200
a bunch of learned parameters
into this process to encourage

178
00:07:00,200 --> 00:07:02,420
diversity as part of
the learning process,

179
00:07:02,420 --> 00:07:05,540
and are really diverse and
interesting representations.

180
00:07:05,540 --> 00:07:07,200
So here's how that works.

181
00:07:07,200 --> 00:07:09,410
We're gonna form three
representations here using

182
00:07:09,410 --> 00:07:11,630
that same dot product
mechanism as before.

183
00:07:11,630 --> 00:07:15,282
And fundamentally it's the
same calculation except now

184
00:07:15,282 --> 00:07:17,490
we're gonna have a bunch of
learned weight parameters

185
00:07:17,490 --> 00:07:19,225
that's given in orange here.

186
00:07:19,225 --> 00:07:20,850
And those will help
us with two things.

187
00:07:20,850 --> 00:07:23,210
First injecting diversity
into this process,

188
00:07:23,210 --> 00:07:25,220
and also smushing
the dimensionality

189
00:07:25,220 --> 00:07:28,010
of the representations down
to one third of the size

190
00:07:28,010 --> 00:07:31,190
that we're targeting SDK for
our model dimensionality.

191
00:07:31,190 --> 00:07:33,590
And you'll see why that
happens in a second.

192
00:07:33,590 --> 00:07:35,180
But fundamentally
what we're doing

193
00:07:35,180 --> 00:07:37,910
is exactly the calculation
we did before but now

194
00:07:37,910 --> 00:07:40,038
with these learned
parameters injected into it.

195
00:07:40,038 --> 00:07:42,080
So if you squint, you can
see that this is really

196
00:07:42,080 --> 00:07:45,590
the dot product of c input
with a input as before.

197
00:07:45,590 --> 00:07:48,260
But now, it's transformed
by these learned parameters

198
00:07:48,260 --> 00:07:49,520
that are given in orange.

199
00:07:49,520 --> 00:07:52,752
And that repeats for
these other calculations.

200
00:07:52,752 --> 00:07:54,460
So we're going to do
that for position a.

201
00:07:54,460 --> 00:07:56,590
And we do it also
for position b.

202
00:07:56,590 --> 00:07:58,690
And it's the same
calculation, but now it's

203
00:07:58,690 --> 00:08:02,090
new parameters for
the second position.

204
00:08:02,090 --> 00:08:05,080
And then for the third head,
exactly the same calculation

205
00:08:05,080 --> 00:08:07,550
but new learned parameters
up at the top here.

206
00:08:07,550 --> 00:08:09,670
So this is
three-headed attention.

207
00:08:09,670 --> 00:08:12,700
And then we actually form
the representations that

208
00:08:12,700 --> 00:08:15,580
proceed with the rest of the
calculation of the transformer

209
00:08:15,580 --> 00:08:20,020
architectures presented before
is by concatenating the three

210
00:08:20,020 --> 00:08:22,610
representations we created
for each one of these units.

211
00:08:22,610 --> 00:08:26,200
So the A column is the
first representation

212
00:08:26,200 --> 00:08:27,760
in each one of these heads.

213
00:08:27,760 --> 00:08:31,060
The B column is the second
representation in each head

214
00:08:31,060 --> 00:08:33,010
and similarly for
the C column it's

215
00:08:33,010 --> 00:08:35,230
the third representation
in each one of these heads.

216
00:08:35,230 --> 00:08:37,150
And that's why each
one of these needs

217
00:08:37,150 --> 00:08:39,039
to add one-third
the dimensionality

218
00:08:39,039 --> 00:08:42,400
of our full model so that we
can concatenate them and then

219
00:08:42,400 --> 00:08:45,610
feed those into the
subsequent calculations.

220
00:08:45,610 --> 00:08:48,070
The idea here, of course, is
that injecting all of these

221
00:08:48,070 --> 00:08:51,520
learned parameters into all
of these different heads we're

222
00:08:51,520 --> 00:08:54,520
providing the model a chance
to learn lots of diverse ways

223
00:08:54,520 --> 00:08:58,592
of relating the words
in the sequence.

224
00:08:58,592 --> 00:09:00,800
The final point is one I've
already mentioned before,

225
00:09:00,800 --> 00:09:03,217
which is that typically we
don't have just one transformer

226
00:09:03,217 --> 00:09:05,470
block but rather a
whole stack of them,

227
00:09:05,470 --> 00:09:07,480
we can repeat them N times.

228
00:09:07,480 --> 00:09:10,810
For models you're working with
you might have 12 or 24 or even

229
00:09:10,810 --> 00:09:13,990
more blocks in the
transformer architecture.

230
00:09:13,990 --> 00:09:15,940
And the way we do that
as I said is simply

231
00:09:15,940 --> 00:09:19,060
by taking the dark green
representations of the output

232
00:09:19,060 --> 00:09:22,090
layer here and using them as
inputs to subsequent blocks

233
00:09:22,090 --> 00:09:24,190
so they get attended
to, and we proceed

234
00:09:24,190 --> 00:09:26,110
with the subsequent
regularization

235
00:09:26,110 --> 00:09:28,900
and feed-forward
steps just as before.

236
00:09:28,900 --> 00:09:31,480
And when you work with these
models in Hugging Face,

237
00:09:31,480 --> 00:09:34,210
if you ask for all of the hidden
states, what you're getting

238
00:09:34,210 --> 00:09:37,570
is a grid of representations
corresponding to these output

239
00:09:37,570 --> 00:09:39,970
blocks in green here.

240
00:09:39,970 --> 00:09:41,530
And of course,
just as a reminder

241
00:09:41,530 --> 00:09:43,690
I'm not indicating it
here but there is actually

242
00:09:43,690 --> 00:09:46,180
multi-headed attention of
each one of these blocks

243
00:09:46,180 --> 00:09:47,500
through each one of the layers.

244
00:09:47,500 --> 00:09:50,540
So there are a lot of learned
parameters in this model,

245
00:09:50,540 --> 00:09:55,525
especially if you have
12 or 24 attention heads.

246
00:09:55,525 --> 00:09:58,110
At this point, I'm hoping
that you can now fruitfully

247
00:09:58,110 --> 00:10:00,870
return to the original
Vaswani, et al paper

248
00:10:00,870 --> 00:10:03,360
and look at their model
diagram and get more out of it.

249
00:10:03,360 --> 00:10:05,640
For me, it's kind of
hyper compressed but now

250
00:10:05,640 --> 00:10:07,770
that we've done a deep
dive into all the pieces,

251
00:10:07,770 --> 00:10:10,530
I think this serves as a kind
of useful shorthand for how

252
00:10:10,530 --> 00:10:11,920
all the pieces fit together.

253
00:10:11,920 --> 00:10:13,500
So let's just do that quickly.

254
00:10:13,500 --> 00:10:16,560
As before we have positional
encodings and input word

255
00:10:16,560 --> 00:10:18,450
embeddings, and
those get added up

256
00:10:18,450 --> 00:10:22,090
to give us the intuitive notion
of an embedding in this model.

257
00:10:22,090 --> 00:10:24,630
That's followed by the
attention layer as we discussed,

258
00:10:24,630 --> 00:10:27,930
and it has a residual
connection here into that layer

259
00:10:27,930 --> 00:10:29,610
normalization part.

260
00:10:29,610 --> 00:10:31,620
That's fed into the
feed-forward blocks

261
00:10:31,620 --> 00:10:34,590
and that's followed by that
same process of kind of dropout

262
00:10:34,590 --> 00:10:37,050
attention layer normalization.

263
00:10:37,050 --> 00:10:39,330
And this is essentially
saying each one

264
00:10:39,330 --> 00:10:42,270
of these is repeated for
every step in the encoder

265
00:10:42,270 --> 00:10:46,720
process, every one of the
columns that we looked at.

266
00:10:46,720 --> 00:10:48,970
And if because
then in the paper,

267
00:10:48,970 --> 00:10:51,010
they're working with an
encoder/decoder model.

268
00:10:51,010 --> 00:10:54,250
Each decoder state self attends
with all of the fellow decoder

269
00:10:54,250 --> 00:10:56,960
states and with all of the
encoder states, so right?

270
00:10:56,960 --> 00:10:59,620
Imagine this double sequence,
we have a dense area

271
00:10:59,620 --> 00:11:02,890
to potentially connect some
connections across both parts

272
00:11:02,890 --> 00:11:05,310
of the representation.

273
00:11:05,310 --> 00:11:07,440
On the right side when
we're doing decoding,

274
00:11:07,440 --> 00:11:11,400
again, we repeat this block
for every decoder state.

275
00:11:11,400 --> 00:11:14,850
And if every state there
has an output as for machine

276
00:11:14,850 --> 00:11:17,370
translation or some kind
of generation process,

277
00:11:17,370 --> 00:11:19,140
then we'll have something
like this output

278
00:11:19,140 --> 00:11:21,750
stack at every one of
those output states.

279
00:11:21,750 --> 00:11:23,430
If by contrast we're
doing something

280
00:11:23,430 --> 00:11:26,600
like NLI or sentiment
classification problem,

281
00:11:26,600 --> 00:11:28,380
maybe just one of
those states will have

282
00:11:28,380 --> 00:11:30,963
one of these outputs on them.

283
00:11:30,963 --> 00:11:33,130
And then the attention gets
a little bit complicated

284
00:11:33,130 --> 00:11:36,070
if you are doing decoding for
a model like natural language

285
00:11:36,070 --> 00:11:38,860
generation or machine
translation, in the decoder,

286
00:11:38,860 --> 00:11:41,830
you can't attend into the future
as you're doing generation.

287
00:11:41,830 --> 00:11:43,690
So there's a
masking process that

288
00:11:43,690 --> 00:11:46,900
limits self-attention to the
preceding words in the sequence

289
00:11:46,900 --> 00:11:48,720
that you're creating.

290
00:11:48,720 --> 00:11:52,000


