1
00:00:00,000 --> 00:00:04,460


2
00:00:04,460 --> 00:00:05,960
CHRISTOPHER POTTS:
Welcome, everyone

3
00:00:05,960 --> 00:00:08,900
to part 3 in our series on
analysis methods in NLP.

4
00:00:08,900 --> 00:00:11,510
We're going to be talking about
adversarial training as well

5
00:00:11,510 --> 00:00:12,590
as testing of systems.

6
00:00:12,590 --> 00:00:15,682
This is the second of the
behavioral evaluation methods

7
00:00:15,682 --> 00:00:16,640
that we're considering.

8
00:00:16,640 --> 00:00:19,580
We've previously talked
about adversarial testing.

9
00:00:19,580 --> 00:00:21,560
Adversarial training
and testing, of course,

10
00:00:21,560 --> 00:00:23,810
implies that we have much
larger datasets, that this

11
00:00:23,810 --> 00:00:25,820
is more difficult to do.

12
00:00:25,820 --> 00:00:28,490
But for selected tasks
where we have such datasets,

13
00:00:28,490 --> 00:00:30,050
this can be very
exciting and push

14
00:00:30,050 --> 00:00:32,270
you to address all sorts
of interesting cutting edge

15
00:00:32,270 --> 00:00:34,080
questions.

16
00:00:34,080 --> 00:00:35,190
I'll start with SWAG.

17
00:00:35,190 --> 00:00:38,070
This is an early entry into the
space of adversarial training

18
00:00:38,070 --> 00:00:38,730
sets.

19
00:00:38,730 --> 00:00:43,590
SWAG stands for Situations
With Adversarial Generations.

20
00:00:43,590 --> 00:00:46,470
There's actually two data sets,
SWAG and the colorfully named

21
00:00:46,470 --> 00:00:47,295
HellaSWAG.

22
00:00:47,295 --> 00:00:49,170
And you'll see why there
are two in a second.

23
00:00:49,170 --> 00:00:52,380
This is fundamentally, again,
another interesting story

24
00:00:52,380 --> 00:00:56,180
of very rapid
progress in our field.

25
00:00:56,180 --> 00:00:57,860
Here's how SWAG examples work.

26
00:00:57,860 --> 00:01:01,970
We're given as a system input
a context like, "he is throwing

27
00:01:01,970 --> 00:01:04,879
darts at a target" and
another system input, which

28
00:01:04,879 --> 00:01:08,090
is the start of a sentence,
here it's, "another man."

29
00:01:08,090 --> 00:01:10,340
And the task of the system
is to figure out what

30
00:01:10,340 --> 00:01:11,730
the continuation should be.

31
00:01:11,730 --> 00:01:14,030
So the actual continuation
that we predict might be,

32
00:01:14,030 --> 00:01:16,580
"throws a dart at
the target board."

33
00:01:16,580 --> 00:01:19,050
And this is fundamentally
a classification task,

34
00:01:19,050 --> 00:01:21,320
the system is given
some distractors like,

35
00:01:21,320 --> 00:01:23,600
"comes running in and shoots
an arrow at a target,"

36
00:01:23,600 --> 00:01:25,160
or "is shown on
the side of men,"

37
00:01:25,160 --> 00:01:26,810
or "throws darts at a disk."

38
00:01:26,810 --> 00:01:28,610
And the system is
tasked with figuring out

39
00:01:28,610 --> 00:01:31,250
which of the options is
the actual continuation

40
00:01:31,250 --> 00:01:34,280
for the sentence,
given the context.

41
00:01:34,280 --> 00:01:37,550
The data sources for this are
ActivityNet and the Large Scale

42
00:01:37,550 --> 00:01:38,840
Movie Description Challenge.

43
00:01:38,840 --> 00:01:40,298
I think the idea
here is that we're

44
00:01:40,298 --> 00:01:42,530
going to key in to all
sorts of interesting notions

45
00:01:42,530 --> 00:01:45,510
of common sense reasoning.

46
00:01:45,510 --> 00:01:47,960
Now here's where the adversarial
part of this comes in,

47
00:01:47,960 --> 00:01:50,930
we're going to do adversarial
filtering for SWAG.

48
00:01:50,930 --> 00:01:52,790
For each of the
examples in our corpus,

49
00:01:52,790 --> 00:01:55,792
and there are over
100,000 examples in SWAG,

50
00:01:55,792 --> 00:01:58,250
we're going to be given the
system input like, "the mixture

51
00:01:58,250 --> 00:01:59,220
creams the butter.

52
00:01:59,220 --> 00:02:01,370
Sugar."

53
00:02:01,370 --> 00:02:04,300
And then we'll have a generator
model- in the case of SWAG,

54
00:02:04,300 --> 00:02:08,120
this was an LSTM produce some
distractors for the target.

55
00:02:08,120 --> 00:02:11,200
So let's suppose that the actual
target continuation is added,

56
00:02:11,200 --> 00:02:15,020
we'd have a model produce "is
sweet" and "is in many foods."

57
00:02:15,020 --> 00:02:17,350
And then we have
the filtering model.

58
00:02:17,350 --> 00:02:19,540
If it guesses correctly
for "is added,"

59
00:02:19,540 --> 00:02:22,180
then we're going to drop
out this entire example

60
00:02:22,180 --> 00:02:24,520
and we'll create some
new distractors like

61
00:02:24,520 --> 00:02:26,980
"is sprinkled on top"
or "is in many foods."

62
00:02:26,980 --> 00:02:29,515
And in this case, if the
model guesses incorrectly

63
00:02:29,515 --> 00:02:32,290
like suppose it
chooses b in this case,

64
00:02:32,290 --> 00:02:35,350
then we'll keep this example
because relative to the current

65
00:02:35,350 --> 00:02:38,140
models for the thing
we're using to generate

66
00:02:38,140 --> 00:02:41,050
these distractors and the thing
that we're using to filter,

67
00:02:41,050 --> 00:02:43,083
this is a challenging example.

68
00:02:43,083 --> 00:02:44,500
And the idea is
that we can repeat

69
00:02:44,500 --> 00:02:47,032
this for a bunch of iterations,
continually retraining

70
00:02:47,032 --> 00:02:48,490
the filtering model
so that it gets

71
00:02:48,490 --> 00:02:51,670
better and better and therefore,
ending up with a dataset that

72
00:02:51,670 --> 00:02:54,820
is really, really difficult
in terms of the current models

73
00:02:54,820 --> 00:02:57,740
that we had available to us.

74
00:02:57,740 --> 00:02:59,470
Here's a picture
of test accuracy.

75
00:02:59,470 --> 00:03:00,610
This is interesting here.

76
00:03:00,610 --> 00:03:03,430
They actually did an
ensemble of filtering models

77
00:03:03,430 --> 00:03:05,800
to try to key into
different notions that

78
00:03:05,800 --> 00:03:09,340
might be indicating which
is the correct continuation.

79
00:03:09,340 --> 00:03:11,980
So they start by using just
a multi-layer perceptron

80
00:03:11,980 --> 00:03:13,720
for efficiency,
and then they bring

81
00:03:13,720 --> 00:03:15,040
in all of these ensembles.

82
00:03:15,040 --> 00:03:17,050
And you can see
the test accuracy

83
00:03:17,050 --> 00:03:20,080
as we do this iterative
filtering very quickly goes

84
00:03:20,080 --> 00:03:24,465
down so that by iteration
140 we're at 10% accuracy.

85
00:03:24,465 --> 00:03:25,840
So that's the
sense in which this

86
00:03:25,840 --> 00:03:28,510
is a very difficult dataset
because given the generator

87
00:03:28,510 --> 00:03:31,780
model and the filtering model
that we have available to us,

88
00:03:31,780 --> 00:03:34,600
we have a dataset that is
very difficult in terms

89
00:03:34,600 --> 00:03:36,805
of a classification task.

90
00:03:36,805 --> 00:03:38,680
So that looks really
exciting and challenging

91
00:03:38,680 --> 00:03:41,140
and I think the authors
expected this dataset

92
00:03:41,140 --> 00:03:42,920
to last for a very long time.

93
00:03:42,920 --> 00:03:46,540
However, the BERT paper,
the original BERT paper,

94
00:03:46,540 --> 00:03:49,675
did evaluations on SWAG and
essentially solved the problem.

95
00:03:49,675 --> 00:03:55,330
BERT Large got 86.6 and 86.3%
on the dev and test sets

96
00:03:55,330 --> 00:03:58,660
for SWAG respectively, a
very unexpected result given

97
00:03:58,660 --> 00:04:02,680
that I just showed you that
the SWAG authors got about 10%

98
00:04:02,680 --> 00:04:04,390
with their current models.

99
00:04:04,390 --> 00:04:08,800
And even closely related models
to BERT like this ESIM model

100
00:04:08,800 --> 00:04:11,225
here, were really pretty
low in their performance.

101
00:04:11,225 --> 00:04:12,850
So BERT looked like
a real breakthrough

102
00:04:12,850 --> 00:04:15,550
and you can see that it's
in some sense superhuman

103
00:04:15,550 --> 00:04:17,950
relative to the SWAG estimates.

104
00:04:17,950 --> 00:04:18,880
So wow.

105
00:04:18,880 --> 00:04:21,490
So, of course, we know what the
response should be given that

106
00:04:21,490 --> 00:04:24,490
we're talking essentially about
model-in-the-loop adversarial

107
00:04:24,490 --> 00:04:25,930
dataset creation.

108
00:04:25,930 --> 00:04:27,760
That leads us to HellaSWAG.

109
00:04:27,760 --> 00:04:29,890
They made some
changes to the dataset

110
00:04:29,890 --> 00:04:31,450
that they use for
HellaSWAG, but I

111
00:04:31,450 --> 00:04:33,490
would say the
fundamental thing is

112
00:04:33,490 --> 00:04:37,210
that we do the same adversarial
filtering with the generator,

113
00:04:37,210 --> 00:04:40,030
except now we have much
more powerful filtering

114
00:04:40,030 --> 00:04:42,460
and generator models,
thanks to developments

115
00:04:42,460 --> 00:04:44,290
related to transformers.

116
00:04:44,290 --> 00:04:46,720
So for HellaSWAG, we again
have human performance

117
00:04:46,720 --> 00:04:47,530
that's really good.

118
00:04:47,530 --> 00:04:49,540
This is very
reassuring because we

119
00:04:49,540 --> 00:04:52,630
are using much more
powerful models at step 4.

120
00:04:52,630 --> 00:04:55,840
As you can expect, BERT
is no longer easily

121
00:04:55,840 --> 00:04:57,830
able to solve this problem.

122
00:04:57,830 --> 00:05:00,400
Here's a further summary of
the results with BERT Large

123
00:05:00,400 --> 00:05:03,160
before I remember that it's
essentially solved SWAG.

124
00:05:03,160 --> 00:05:07,000
Now it's down around 50%
which shows that it still gets

125
00:05:07,000 --> 00:05:10,660
traction but is nothing like
the superhuman performance

126
00:05:10,660 --> 00:05:12,910
that we saw for SWAG.

127
00:05:12,910 --> 00:05:13,410
OK.

128
00:05:13,410 --> 00:05:15,000
Now let's move into a
slightly different mode,

129
00:05:15,000 --> 00:05:17,125
and this is going to be a
kind of human-in-the-loop

130
00:05:17,125 --> 00:05:19,590
adversarial dataset
creation method.

131
00:05:19,590 --> 00:05:22,770
The first entry in this space
was the adversarial NLI data

132
00:05:22,770 --> 00:05:26,490
set, I think this is a really
visionary and exciting paper.

133
00:05:26,490 --> 00:05:31,050
Adversarial NLI is a direct
response to the previous things

134
00:05:31,050 --> 00:05:34,230
that we've seen with the
SNLI and multi-NLI datasets

135
00:05:34,230 --> 00:05:37,620
where models seem to do
well on those benchmarks

136
00:05:37,620 --> 00:05:40,710
but are easily susceptible
to simple adversaries.

137
00:05:40,710 --> 00:05:42,240
With adversarial
NLI, we're going

138
00:05:42,240 --> 00:05:45,390
to hopefully push
systems to be much more

139
00:05:45,390 --> 00:05:47,430
robust to those
adversaries and explore

140
00:05:47,430 --> 00:05:49,620
a much wider range of
the space of things

141
00:05:49,620 --> 00:05:53,020
you might see under the heading
of natural language inference.

142
00:05:53,020 --> 00:05:54,425
So here's how it worked.

143
00:05:54,425 --> 00:05:56,925
There's a human in the loop,
an annotator, and the annotator

144
00:05:56,925 --> 00:06:00,120
is presented with a premise
sentence and a condition

145
00:06:00,120 --> 00:06:02,370
that they need to be in,
which is just an NLI label-

146
00:06:02,370 --> 00:06:05,350
entailment,
contradiction, or neutral.

147
00:06:05,350 --> 00:06:07,810
The annotator writes a
hypothesis to go along with

148
00:06:07,810 --> 00:06:10,720
the premise and the condition
and then a state-of-the-art

149
00:06:10,720 --> 00:06:13,330
model comes in and makes a
prediction about the premise

150
00:06:13,330 --> 00:06:15,190
hypothesis pair.

151
00:06:15,190 --> 00:06:17,980
If the model's prediction
matches the condition, that

152
00:06:17,980 --> 00:06:19,720
is, if the model
was correct, then

153
00:06:19,720 --> 00:06:22,720
the annotator needs to return
to step 2 and try again

154
00:06:22,720 --> 00:06:24,400
with a new hypothesis.

155
00:06:24,400 --> 00:06:26,350
And we could continue
in that loop.

156
00:06:26,350 --> 00:06:29,020
If the model was fooled,
the premise-hypothesis pair

157
00:06:29,020 --> 00:06:31,990
is independently validated by
other annotators, of course.

158
00:06:31,990 --> 00:06:35,380
So what we get out of this
is we hope a dataset that

159
00:06:35,380 --> 00:06:38,690
is intuitive for humans,
because of the check in step 5,

160
00:06:38,690 --> 00:06:42,010
but assuming we continue to
loop around through 2, 3 and 4,

161
00:06:42,010 --> 00:06:44,410
an example that is really
difficult for whatever

162
00:06:44,410 --> 00:06:46,000
model is in the loop.

163
00:06:46,000 --> 00:06:49,150
And the expectation is that as
we put better and better models

164
00:06:49,150 --> 00:06:52,150
in the loop here, we're going to
get even more challenging data

165
00:06:52,150 --> 00:06:55,560
sets as an outcome.

166
00:06:55,560 --> 00:06:58,040
NLI examples tend to be
impressively complex.

167
00:06:58,040 --> 00:07:01,130
You can see that this example
has a very long premise.

168
00:07:01,130 --> 00:07:03,080
The hypothesis was
relatively shorter.

169
00:07:03,080 --> 00:07:05,780
And an intriguing aspect
of adverserial NLI

170
00:07:05,780 --> 00:07:07,790
is that annotators
also constructed

171
00:07:07,790 --> 00:07:09,950
a reason or a rationale
for their label

172
00:07:09,950 --> 00:07:12,650
holding between the
premise-hypothesis pair.

173
00:07:12,650 --> 00:07:14,762
To date, as far as I know,
relatively little use

174
00:07:14,762 --> 00:07:16,220
has been made of
these texts, but I

175
00:07:16,220 --> 00:07:19,453
think they could bring in other
aspects of natural language

176
00:07:19,453 --> 00:07:20,870
inference reasoning
and that could

177
00:07:20,870 --> 00:07:24,230
be an exciting new direction.

178
00:07:24,230 --> 00:07:27,590
Adversarial NLI is a
difficult dataset indeed.

179
00:07:27,590 --> 00:07:29,210
We have a similar
sort of leaderboard

180
00:07:29,210 --> 00:07:32,420
that we've seen throughout
this adversarial regime where

181
00:07:32,420 --> 00:07:35,240
across different rounds
of NLI- there are three,

182
00:07:35,240 --> 00:07:37,400
or cumulatively
for the data set,

183
00:07:37,400 --> 00:07:41,210
even really excellent models
that do really well on SNLI

184
00:07:41,210 --> 00:07:45,380
and multi-NLI are posting
really low numbers for all

185
00:07:45,380 --> 00:07:48,080
of these variants of the
dataset and that shows you

186
00:07:48,080 --> 00:07:50,690
that this is truly
a difficult problem.

187
00:07:50,690 --> 00:07:52,850
And as far as I know,
not much progress

188
00:07:52,850 --> 00:07:55,670
has been made since this
dataset was released

189
00:07:55,670 --> 00:07:57,600
on boosting these numbers.

190
00:07:57,600 --> 00:08:00,977
So it stands as an
interesting challenge.

191
00:08:00,977 --> 00:08:03,310
Stepping back here, I'd just
like to say that I think we

192
00:08:03,310 --> 00:08:06,550
find in this paper a real vision
for future development and that

193
00:08:06,550 --> 00:08:10,300
you see this also in the SWAG
and HellaSWAG papers as those

194
00:08:10,300 --> 00:08:13,840
authors say this adversarial
dataset creation is "a path

195
00:08:13,840 --> 00:08:16,630
for NLP progress going
forward: toward benchmarks that

196
00:08:16,630 --> 00:08:19,600
adversariallly co-evolve with
evolving state-of-the-art

197
00:08:19,600 --> 00:08:20,100
models."

198
00:08:20,100 --> 00:08:22,690
Right, with SWAG and
HellaSWAG, we saw this.

199
00:08:22,690 --> 00:08:25,600
SWAG got solved but
the response was clear,

200
00:08:25,600 --> 00:08:27,280
bring the best
model in and use it

201
00:08:27,280 --> 00:08:29,440
to create the successor
dataset set that

202
00:08:29,440 --> 00:08:31,390
stands as a real challenge.

203
00:08:31,390 --> 00:08:34,840
You have the similar picture
from the adversarial NLI paper.

204
00:08:34,840 --> 00:08:38,770
This process of having iterative
rounds with humans in the loop

205
00:08:38,770 --> 00:08:41,799
yields a moving post-dynamic
target for natural language

206
00:08:41,799 --> 00:08:45,040
understanding systems, rather
than the static benchmarks

207
00:08:45,040 --> 00:08:46,360
that eventually saturate.

208
00:08:46,360 --> 00:08:49,330
And we've seen repeatedly
that our benchmarks saturate

209
00:08:49,330 --> 00:08:52,540
very quickly these days, so we
need this kind of moving post

210
00:08:52,540 --> 00:08:56,920
to make sure we continue to
make meaningful progress.

211
00:08:56,920 --> 00:08:59,260
The Nie et al project
gave rise, I believe,

212
00:08:59,260 --> 00:09:02,680
to this Dynabench platform, an
open source platform for model

213
00:09:02,680 --> 00:09:05,680
and human-in-the-loop creation.

214
00:09:05,680 --> 00:09:08,500
As of this writing, there are
four datasets available that

215
00:09:08,500 --> 00:09:11,710
have been created on
Dynabench, an NLI data

216
00:09:11,710 --> 00:09:14,460
set which is a
successor to ANLI,

217
00:09:14,460 --> 00:09:18,910
a question-answering dataset,
a sentiment dataset, and a hate

218
00:09:18,910 --> 00:09:20,210
speech dataset.

219
00:09:20,210 --> 00:09:22,510
So if you're working on
problems of this form

220
00:09:22,510 --> 00:09:25,030
or you have a model that would
fit into this mold for one

221
00:09:25,030 --> 00:09:26,920
of these tasks,
I would encourage

222
00:09:26,920 --> 00:09:30,460
you to explore some training of
the systems on these datasets

223
00:09:30,460 --> 00:09:32,950
to see whether you're
making progress

224
00:09:32,950 --> 00:09:35,410
or whether they stand as
true adversaries for whatever

225
00:09:35,410 --> 00:09:38,020
innovative thing you're doing.

226
00:09:38,020 --> 00:09:40,590
Finally, I want to close with
a really important question

227
00:09:40,590 --> 00:09:43,230
for this area that
kind of remains open,

228
00:09:43,230 --> 00:09:45,720
can adversarial training
improve systems?

229
00:09:45,720 --> 00:09:48,360
There is a course of
concern that as we construct

230
00:09:48,360 --> 00:09:51,060
ever harder data sets,
we're pushing systems

231
00:09:51,060 --> 00:09:54,840
into stranger parts of the
linguistic and conceptual space

232
00:09:54,840 --> 00:09:58,060
which could actually degrade
their real world performance.

233
00:09:58,060 --> 00:10:00,060
We have to keep an eye on that.

234
00:10:00,060 --> 00:10:02,220
And the evidence so
far, I think is pointing

235
00:10:02,220 --> 00:10:04,050
to yes as an answer
to this question

236
00:10:04,050 --> 00:10:05,490
but the evidence is a bit mixed.

237
00:10:05,490 --> 00:10:08,820
So I've mentioned that in
the SQuAD adversarial paper

238
00:10:08,820 --> 00:10:11,820
from Jia and Liang, training
on adversarial examples

239
00:10:11,820 --> 00:10:14,880
makes them more robust
to of those examples

240
00:10:14,880 --> 00:10:19,950
but not to simple variants, so
it's hardly very much progress.

241
00:10:19,950 --> 00:10:22,590
In this paper, they found that
adversarial training provided

242
00:10:22,590 --> 00:10:25,620
no additional robustness
benefit in the experiments

243
00:10:25,620 --> 00:10:28,240
using the testset, despite the
fact that the model achieved

244
00:10:28,240 --> 00:10:32,100
near 100% accuracy classifying
adversarial examples included

245
00:10:32,100 --> 00:10:33,180
in the train set.

246
00:10:33,180 --> 00:10:35,430
So that's a more
worrisome picture.

247
00:10:35,430 --> 00:10:36,690
But this is more hopeful.

248
00:10:36,690 --> 00:10:38,890
Fine-tuning with a few
adversarial examples

249
00:10:38,890 --> 00:10:42,210
improved systems in some
cases, especially where

250
00:10:42,210 --> 00:10:43,980
you bring in inoculation.

251
00:10:43,980 --> 00:10:46,020
And this is hopefully
yet again, adversarially

252
00:10:46,020 --> 00:10:49,080
generated paraphrases
improve model robustness

253
00:10:49,080 --> 00:10:50,640
to syntactic variation.

254
00:10:50,640 --> 00:10:53,160
That's really the dream
there- that as a result

255
00:10:53,160 --> 00:10:55,230
of doing this new
kind of training,

256
00:10:55,230 --> 00:10:57,900
we get systems that
are truly more robust.

257
00:10:57,900 --> 00:11:00,630
But I think we might need more
evidence on this picture, which

258
00:11:00,630 --> 00:11:02,790
means more datasets
of this form,

259
00:11:02,790 --> 00:11:06,210
and more and interesting use
of the available resources

260
00:11:06,210 --> 00:11:08,850
and I would just love to see
what the emerging picture is

261
00:11:08,850 --> 00:11:11,300
over the next year or two.

262
00:11:11,300 --> 00:11:15,350


