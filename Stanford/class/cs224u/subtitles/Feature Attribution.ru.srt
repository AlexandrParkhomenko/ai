1
00:00:05,359 --> 00:00:06,960
приветствую всех, это пятая часть

2
00:00:06,960 --> 00:00:09,040
нашей серии статей о методах анализа в nlp,

3
00:00:09,040 --> 00:00:10,400
мы собираемся поговорить о

4
00:00:10,400 --> 00:00:11,679
методах атрибуции признаков, это, по

5
00:00:11,679 --> 00:00:14,000
сути, мощный набор инструментов, который

6
00:00:14,000 --> 00:00:15,759
поможет вам понять, как

7
00:00:15,759 --> 00:00:17,840
признаки вашей модели влияют на ее выходные

8
00:00:17,840 --> 00:00:19,840
прогнозы,

9
00:00:19,840 --> 00:00:22,080
наш основной вопрос здесь  это вроде того,

10
00:00:22,080 --> 00:00:23,760
почему ваша модель делает предсказания,

11
00:00:23,760 --> 00:00:25,920
которые она делает, есть много мотивов

12
00:00:25,920 --> 00:00:28,480
для того, чтобы задать этот вопрос, вот лишь несколько

13
00:00:28,480 --> 00:00:29,679
для начала, вы можете просто захотеть

14
00:00:29,679 --> 00:00:30,800
понять, является ли ваша модель

15
00:00:30,800 --> 00:00:32,880
систематической в отношении какого-либо конкретного

16
00:00:32,880 --> 00:00:34,800
языкового явления, действительно ли она

17
00:00:34,800 --> 00:00:37,440
уловила это  явления,

18
00:00:37,440 --> 00:00:38,719
вы также можете узнать, устойчиво ли оно

19
00:00:38,719 --> 00:00:40,960
к незначительным возмущениям на

20
00:00:40,960 --> 00:00:42,160
входе.

21
00:00:42,160 --> 00:00:43,600
Вы можете использовать эти методы для

22
00:00:43,600 --> 00:00:45,840
диагностики нежелательных смещений в вашей модели

23
00:00:45,840 --> 00:00:48,160
и, соответственно, вы можете использовать их для поиска

24
00:00:48,160 --> 00:00:49,760
слабых мест в вашей модели, которые

25
00:00:49,760 --> 00:00:51,840
злоумышленник может использовать, чтобы привести вашу

26
00:00:51,840 --> 00:00:54,960
модель к  делать действительно проблематичные вещи

27
00:00:54,960 --> 00:00:56,399
принципиально, я думаю, что это

28
00:00:56,399 --> 00:00:58,640
набор инструментов, который поможет вам  Вы пишете действительно

29
00:00:58,640 --> 00:01:00,640
отличные аналитические разделы для своей

30
00:01:00,640 --> 00:01:02,559
статьи, с этой целью я попытаюсь

31
00:01:02,559 --> 00:01:04,239
показать вам кучу кода, который поможет

32
00:01:04,239 --> 00:01:06,720
вам освоить эти методы,

33
00:01:06,720 --> 00:01:08,320
я сделаю это на высоком уровне

34
00:01:08,320 --> 00:01:10,080
в скринкасте.  и я только что добавил

35
00:01:10,080 --> 00:01:12,479
эту новую функцию записной книжки

36
00:01:12,479 --> 00:01:15,119
в репозиторий кода курса, и она

37
00:01:15,119 --> 00:01:17,200
должна быть гибкой и адаптируемой и

38
00:01:17,200 --> 00:01:18,960
помочь вам использовать эти методы и применять

39
00:01:18,960 --> 00:01:21,200
их к любым моделям и идеям, которые вы

40
00:01:21,200 --> 00:01:24,159
исследуете для своих

41
00:01:24,159 --> 00:01:26,000
проектов звезда нашего шоу действительно  единственная

42
00:01:26,000 --> 00:01:27,680
причина, по которой я могу это сделать, — это

43
00:01:27,680 --> 00:01:30,880
потрясающая библиотека Captain.ai, в ней

44
00:01:30,880 --> 00:01:32,960
реализован широкий спектр методов атрибуции функций

45
00:01:32,960 --> 00:01:34,799
, мы собираемся подробно поговорить

46
00:01:34,799 --> 00:01:36,560
о

47
00:01:36,560 --> 00:01:38,880
методе интегрированных градиентов и использовать метод на

48
00:01:38,880 --> 00:01:40,560
основе градиента в качестве простой

49
00:01:40,560 --> 00:01:42,880
основы для этого.  метод, но, как вы можете

50
00:01:42,880 --> 00:01:45,119
видеть здесь, captive реализует широкий

51
00:01:45,119 --> 00:01:47,040
спектр различных алгоритмов, некоторые из которых очень

52
00:01:47,040 --> 00:01:49,759
специфичны для конкретных моделей моделей, а

53
00:01:49,759 --> 00:01:51,680
другие совершенно

54
00:01:51,680 --> 00:01:53,360
не зависят от того, какую модель вы исследуете.  Итак, это

55
00:01:53,360 --> 00:01:56,720
очень интересный инструментарий. В

56
00:01:57,200 --> 00:02:00,079
статье Сундара Раджана в 2017 году

57
00:02:00,079 --> 00:02:01,680
представлен метод интегрированных градиентов.

58
00:02:01,680 --> 00:02:03,840
Это также прекрасный вклад,

59
00:02:03,840 --> 00:02:06,399
потому что он дает нам своего рода

60
00:02:06,399 --> 00:02:08,318
основу для размышлений о

61
00:02:08,318 --> 00:02:10,639
методах атрибуции признаков в целом, и как

62
00:02:10,639 --> 00:02:12,720
часть этого они предлагают две аксиомы, которые

63
00:02:12,720 --> 00:02:13,920
я собираюсь использовать для руководства в этом

64
00:02:13,920 --> 00:02:15,760
обсуждении первый и более

65
00:02:15,760 --> 00:02:18,160
важный из них - чувствительность,

66
00:02:18,160 --> 00:02:20,800
если два входных параметра x и x prime отличаются только

67
00:02:20,800 --> 00:02:23,280
в измерении i и приводят к разным

68
00:02:23,280 --> 00:02:26,000
прогнозам, тогда функция, связанная

69
00:02:26,000 --> 00:02:28,160
с этим измерением, должна иметь ненулевую

70
00:02:28,160 --> 00:02:29,680
атрибуцию

71
00:02:29,680 --> 00:02:31,280
и  с моим простым примером здесь вы

72
00:02:31,280 --> 00:02:33,519
можете понять, почему чувствительность является

73
00:02:33,519 --> 00:02:36,800
такой фундаментальной аксиомой, если для некоторой модели m

74
00:02:36,800 --> 00:02:39,519
и трехмерного ввода один ноль один

75
00:02:39,519 --> 00:02:41,920
мы получаем прогноз положительного, и если

76
00:02:41,920 --> 00:02:44,160
для той же модели вход один

77
00:02:44,160 --> 00:02:46,400
один приводит к  предсказание отрицательное,

78
00:02:46,400 --> 00:02:48,319
то мы действительно должны ожидать, что

79
00:02:48,319 --> 00:02:50,080
признак, связанный со второй

80
00:02:50,080 --> 00:02:52,959
позицией, должен иметь ненулевую атрибуцию,

81
00:02:52,959 --> 00:02:55,599
потому что он должен быть решающим

82
00:02:55,599 --> 00:02:57,280
Приводя модель делать эти два разных

83
00:02:57,280 --> 00:02:59,599
прогноза,

84
00:02:59,599 --> 00:03:01,040
вторая аксиома будет менее

85
00:03:01,040 --> 00:03:02,480
важной для нашего обсуждения, но,

86
00:03:02,480 --> 00:03:04,640
тем не менее, стоит иметь в виду, что это

87
00:03:04,640 --> 00:03:06,640
реализация и дисперсия, если две

88
00:03:06,640 --> 00:03:08,959
модели m и m prime имеют одинаковое

89
00:03:08,959 --> 00:03:10,959
входное-выходное поведение, тогда

90
00:03:10,959 --> 00:03:12,800
атрибуты для  m и m prime

91
00:03:12,800 --> 00:03:14,800
идентичны, это на самом деле просто говорит о том,

92
00:03:14,800 --> 00:03:17,120
что атрибуты, которые мы даем, должны быть

93
00:03:17,120 --> 00:03:19,920
отделены от любых случайных различий

94
00:03:19,920 --> 00:03:21,760
в реализации модели, которые не

95
00:03:21,760 --> 00:03:24,239
влияют на поведение этой модели на входе и выходе.

96
00:03:24,239 --> 00:03:26,560


97
00:03:26,640 --> 00:03:28,720
Чтобы начать наше обсуждение, давайте начнем с

98
00:03:28,720 --> 00:03:30,640
этой простой базовой линии, которая  просто

99
00:03:30,640 --> 00:03:33,280
умножая градиенты на входные данные,

100
00:03:33,280 --> 00:03:34,879
это реализовано и сохраняет их в качестве

101
00:03:34,879 --> 00:03:37,599
входных данных по градиенту, теперь я показываю в

102
00:03:37,599 --> 00:03:40,159
отношении некоторой конкретной функции, которую я

103
00:03:40,159 --> 00:03:42,959
дал модели m и вводил x, тогда мы просто

104
00:03:42,959 --> 00:03:45,519
получаем градиенты для этой функции,

105
00:03:45,519 --> 00:03:47,599
а затем умножаем их на  фактическое

106
00:03:47,599 --> 00:03:50,000
значение этой функции так же просто, как

107
00:03:50,000 --> 00:03:51,920
вот две реализации,

108
00:03:51,920 --> 00:03:54,400
первая в ячейке 2 делает это для вида

109
00:03:54,400 --> 00:03:56,640
используя необработанный pi torch, просто чтобы показать вам, как

110
00:03:56,640 --> 00:03:59,040
мы можем использовать функцию автоградации pi torch

111
00:03:59,040 --> 00:04:01,760
для реализации этого метода,

112
00:04:01,760 --> 00:04:03,599
а вторая реализация от

113
00:04:03,599 --> 00:04:05,599
captum, и она, вероятно, более гибкая,

114
00:04:05,599 --> 00:04:10,319
и она использует этот ввод с помощью класса градиента,

115
00:04:10,319 --> 00:04:12,080
чтобы дать вам полную иллюстрацию здесь

116
00:04:12,080 --> 00:04:13,920
i'  Я только что создал простую

117
00:04:13,920 --> 00:04:15,920
задачу синтетической классификации с помощью инструментов scikit.

118
00:04:15,920 --> 00:04:16,880


119
00:04:16,880 --> 00:04:18,959
Моя модель будет факельным неглубоким нейронным

120
00:04:18,959 --> 00:04:21,759
классификатором, который я подгоню к этим данным,

121
00:04:21,759 --> 00:04:23,759
а затем в ячейках 9 и 10 я использую эти

122
00:04:23,759 --> 00:04:26,400
две реализации этого метода, и

123
00:04:26,400 --> 00:04:28,320
вы можете видеть в 11 и 12  то, что они дают

124
00:04:28,320 --> 00:04:30,320
идентичные результаты,

125
00:04:30,320 --> 00:04:31,919
еще одна вещь, на которую стоит обратить внимание, это то, что

126
00:04:31,919 --> 00:04:33,440
я использовал метод, взяв

127
00:04:33,440 --> 00:04:35,199
градиенты по отношению к фактическим

128
00:04:35,199 --> 00:04:37,040
меткам в нашем наборе данных,

129
00:04:37,040 --> 00:04:39,120
вы часто можете получить другую картину, если

130
00:04:39,120 --> 00:04:40,800
вы берете градиенты по отношению к

131
00:04:40,800 --> 00:04:43,040
предсказаниям этой модели и  это может

132
00:04:43,040 --> 00:04:44,800
дать вам лучшее представление о том, почему

133
00:04:44,800 --> 00:04:46,960
модель делает прогнозы, которые она

134
00:04:46,960 --> 00:04:48,320
делает

135
00:04:48,320 --> 00:04:49,840
в этом случае, поскольку модель очень

136
00:04:49,840 --> 00:04:51,600
хороша, атрибуции лишь немного различаются.

137
00:04:51,600 --> 00:04:53,919
В общем

138
00:04:53,919 --> 00:04:56,000
, это наш базовый уровень, который я хочу

139
00:04:56,000 --> 00:04:58,000
показать вам сейчас, что метод ввода с помощью градиентов

140
00:04:58,000 --> 00:05:00,240
не проходит тест на чувствительность, и

141
00:05:00,240 --> 00:05:02,479
это пример из статьи о Золушке,

142
00:05:02,479 --> 00:05:04,960
если они дают здесь эту простую

143
00:05:04,960 --> 00:05:08,160
модель, которая фактически просто

144
00:05:08,160 --> 00:05:11,360
относится к одному  минус вход, а

145
00:05:11,360 --> 00:05:13,520
затем вы берете один минус этот

146
00:05:13,520 --> 00:05:15,680
расчет rel, это модель, у нее

147
00:05:15,680 --> 00:05:18,560
есть одномерные входы и выходы,

148
00:05:18,560 --> 00:05:20,800
если вы вычисляете для входа ноль, вы

149
00:05:20,800 --> 00:05:22,720
получаете нулевой результат,

150
00:05:22,720 --> 00:05:24,720
и если вы даете вход модели 2, вы

151
00:05:24,720 --> 00:05:26,960
получаете выход 1.

152
00:05:26,960 --> 00:05:28,400
поскольку у нас есть разные выходные

153
00:05:28,400 --> 00:05:31,280
прогнозы, чувствительность говорит нам, что у

154
00:05:31,280 --> 00:05:33,840
нас должны быть разные атрибуции

155
00:05:33,840 --> 00:05:36,720
для этих двух случаев, вот эти два

156
00:05:36,720 --> 00:05:39,360
одномерных входа, но, к сожалению,

157
00:05:39,360 --> 00:05:40,880
когда вы вычисляете с помощью этого

158
00:05:40,880 --> 00:05:43,759
метода, вы получаете нулевую атрибуцию в обоих

159
00:05:43,759 --> 00:05:45,039
случаях

160
00:05:45,039 --> 00:05:46,880
, это отказ чувствительности и

161
00:05:46,880 --> 00:05:50,720
очков  к слабости этого метода,

162
00:05:51,120 --> 00:05:52,880
давайте теперь перейдем к интегрированным градиентам,

163
00:05:52,880 --> 00:05:54,320
и позвольте мне начать с того, что я дам вам

164
00:05:54,320 --> 00:05:56,000
интуицию о том, как этот метод будет

165
00:05:56,000 --> 00:05:57,840
работать.  Представьте, что у нас есть простая

166
00:05:57,840 --> 00:06:00,319
функция двумерного пространства признаков x1

167
00:06:00,319 --> 00:06:03,039
и x2, поэтому здесь представлена фактическая

168
00:06:03,039 --> 00:06:04,960
точка.

169
00:06:04,960 --> 00:06:07,120
Идея интегрированных градиентов заключается в

170
00:06:07,120 --> 00:06:08,800
том, что мы собираемся сравнить это

171
00:06:08,800 --> 00:06:11,120
относительно некоторой базовой линии, типичной

172
00:06:11,120 --> 00:06:13,039
базовой линией для нас будут все нули.

173
00:06:13,039 --> 00:06:14,319
вектор,

174
00:06:14,319 --> 00:06:15,840
а затем провести сравнение, что мы на

175
00:06:15,840 --> 00:06:17,840
самом деле будем делать, это интерполировать набор

176
00:06:17,840 --> 00:06:20,160
точек между этой базовой линией и нашими

177
00:06:20,160 --> 00:06:21,520
фактическими входными

178
00:06:21,520 --> 00:06:23,520


179
00:06:23,520 --> 00:06:25,600
данными, взять градиенты по отношению к каждому из них и усреднить все эти

180
00:06:25,600 --> 00:06:27,600
результаты градиента, и это даст нам

181
00:06:27,600 --> 00:06:30,639
меру  Важность функции

182
00:06:30,639 --> 00:06:32,400
здесь расчет метода во

183
00:06:32,400 --> 00:06:34,560
всех подробностях. Я взял эту презентацию

184
00:06:34,560 --> 00:06:37,280
из этого действительно отличного учебника по

185
00:06:37,280 --> 00:06:39,680
интегрированным градиентам tensorflow,

186
00:06:39,680 --> 00:06:41,440
и он делает все эти аннотации, которые я

187
00:06:41,440 --> 00:06:43,759
считаю весьма полезными, вот в основном,

188
00:06:43,759 --> 00:06:45,919
как это работает, основная вещь выделена

189
00:06:45,919 --> 00:06:48,000
фиолетовым цветом мы'  мы собираемся интерполировать

190
00:06:48,000 --> 00:06:49,919
кучу разных входных данных между этой

191
00:06:49,919 --> 00:06:52,160
базовой линией всех нулей и нашим фактическим

192
00:06:52,160 --> 00:06:54,720
вводом, вот что происходит здесь, и

193
00:06:54,720 --> 00:06:56,720
w  e возьмем градиенты по отношению к

194
00:06:56,720 --> 00:06:58,400
каждому из них по отношению к

195
00:06:58,400 --> 00:07:00,000
каждой из функций,

196
00:07:00,000 --> 00:07:01,440
и мы собираемся суммировать их и

197
00:07:01,440 --> 00:07:03,919
усреднять, и это дает нам основной

198
00:07:03,919 --> 00:07:05,440
расчет здесь,

199
00:07:05,440 --> 00:07:07,680
а затем в пяти мы просто как бы масштабируем

200
00:07:07,680 --> 00:07:09,840
что результирующее среднее значение по отношению

201
00:07:09,840 --> 00:07:12,160
к исходному вводу, чтобы вернуть его в ту

202
00:07:12,160 --> 00:07:14,800
же шкалу, и, как я показываю здесь, интегрированные

203
00:07:14,800 --> 00:07:17,599
градиенты подчиняются аксиоме чувствительности,

204
00:07:17,599 --> 00:07:19,440
давайте вернемся к этому исходному

205
00:07:19,440 --> 00:07:21,840
примеру простой модели, основанной на значениях,

206
00:07:21,840 --> 00:07:24,000
представленной здесь, я показал вам, что

207
00:07:24,000 --> 00:07:25,840
ввод  Чувствительность метода градиентов для этой модели не удалась.

208
00:07:25,840 --> 00:07:28,000


209
00:07:28,000 --> 00:07:29,680
Интегрированные градиенты, конечно,

210
00:07:29,680 --> 00:07:31,680
чувствительны в соответствующем смысле, и вы

211
00:07:31,680 --> 00:07:33,360
можете понять, почему это происходит,

212
00:07:33,360 --> 00:07:35,759
потому что наш основной расчет теперь выполняется не

213
00:07:35,759 --> 00:07:37,680
по отношению к одному входу в

214
00:07:37,680 --> 00:07:39,759
случае входа 2, а скорее по

215
00:07:39,759 --> 00:07:41,599
отношению к  ко всем этим интерполированным

216
00:07:41,599 --> 00:07:44,160
представлениям объектов, хотя некоторые из

217
00:07:44,160 --> 00:07:45,360
этих интерполированных представлений объектов

218
00:07:45,360 --> 00:07:47,919
дают градиент, равный нулю,

219
00:07:47,919 --> 00:07:50,160
не все из них делают это, и

220
00:07:50,160 --> 00:07:52,080
фактический результат  что вы получите

221
00:07:52,080 --> 00:07:54,319
атрибуцию функции примерно один для

222
00:07:54,319 --> 00:07:56,639
этого случая ввода

223
00:07:56,639 --> 00:07:59,120
два желаемый результат, показывающий чувствительность,

224
00:07:59,120 --> 00:08:00,879
потому что, конечно, ввод нуля в

225
00:08:00,879 --> 00:08:02,800
этом случае даст атрибуцию

226
00:08:02,800 --> 00:08:05,199
ноль

227
00:08:05,440 --> 00:08:06,720
теперь позвольте мне провести вас через несколько

228
00:08:06,720 --> 00:08:08,639
примеров, которые показывают  как вы можете использовать

229
00:08:08,639 --> 00:08:11,039
captum, чтобы освоить

230
00:08:11,039 --> 00:08:12,720
метод интегрированных градиентов, я собираюсь

231
00:08:12,720 --> 00:08:15,039
сделать это для двух классов моделей,

232
00:08:15,039 --> 00:08:17,120
первый из которых представляет собой просто простую

233
00:08:17,120 --> 00:08:18,800
сеть с прямой связью, и то, что я делаю, это

234
00:08:18,800 --> 00:08:20,960
воссоединение со Стэнфордским настроением

235
00:08:20,960 --> 00:08:22,400
банк дерева, который мы используем во время нашего

236
00:08:22,400 --> 00:08:24,800
блока настроений, поэтому на этом слайде я

237
00:08:24,800 --> 00:08:27,680
только что настроил эксперимент с ssd, используя

238
00:08:27,680 --> 00:08:29,360
sst.experiment

239
00:08:29,360 --> 00:08:32,719
из этого модуля sst.

240
00:08:32,719 --> 00:08:34,320
Мои представления функций будут по

241
00:08:34,320 --> 00:08:35,839
существу набором слов, и я

242
00:08:35,839 --> 00:08:37,599
отфильтровал стандартные слова.  чтобы сделать это

243
00:08:37,599 --> 00:08:39,279
немного более интерпретируемым,

244
00:08:39,279 --> 00:08:41,279
и наш классификатор представляет собой поверхностный нейронный классификатор факела,

245
00:08:41,279 --> 00:08:43,039


246
00:08:43,039 --> 00:08:44,800
я запускаю эксперимент, и много

247
00:08:44,800 --> 00:08:46,640
информации об этом эксперименте, которую вы

248
00:08:46,640 --> 00:08:48,399
помните, хранится в этой переменной

249
00:08:48,399 --> 00:08:51,040
эксперимент

250
00:08:51,360 --> 00:08:54,240
ее  т. е. извлеките модель из этого

251
00:08:54,240 --> 00:08:55,760
отчета об эксперименте,

252
00:08:55,760 --> 00:08:57,040
и здесь мы получим кучу других

253
00:08:57,040 --> 00:08:58,560
метаданных, которые мы собираемся использовать для запуска

254
00:08:58,560 --> 00:09:00,880
метода интегрированных градиентов.

255
00:09:00,880 --> 00:09:02,560
Представления функций наших тестовых

256
00:09:02,560 --> 00:09:05,360
примеров. Фактические метки

257
00:09:05,360 --> 00:09:07,360
и прогнозируемые метки вместе с

258
00:09:07,360 --> 00:09:09,120
именами функций.  и здесь следует отметить,

259
00:09:09,120 --> 00:09:11,200
что ради капитана нам

260
00:09:11,200 --> 00:09:14,080
нужно превратить имена строковых классов в

261
00:09:14,080 --> 00:09:15,839
их соответствующие индексы, и это то,

262
00:09:15,839 --> 00:09:18,080
что происходит здесь в строке ячеек,

263
00:09:18,080 --> 00:09:20,080
затем мы настраиваем интегрированные градиенты,

264
00:09:20,080 --> 00:09:22,959
используя прямой метод для нашей модели,

265
00:09:22,959 --> 00:09:24,800
и мы  установить базовую линию, которая представляет собой

266
00:09:24,800 --> 00:09:27,279
вектор всех нулей, а затем, наконец, использовать

267
00:09:27,279 --> 00:09:29,920
метод атрибутов, и здесь я беру

268
00:09:29,920 --> 00:09:31,839
атрибуции в отношении

269
00:09:31,839 --> 00:09:34,480
прогнозов модели,

270
00:09:34,480 --> 00:09:36,080
я думаю, что это может быть мощным устройством

271
00:09:36,080 --> 00:09:38,000
для выполнения простого анализа ошибок, и это

272
00:09:38,000 --> 00:09:39,600
то, что я  Я настроил на слайде

273
00:09:39,600 --> 00:09:41,920
здесь, я предложил две функции

274
00:09:41,920 --> 00:09:44,240
анализа ошибок и создание поиска по атрибуции

275
00:09:44,240 --> 00:09:45,839
, которые помогут вам понять, как

276
00:09:45,839 --> 00:09:47,920
функции в этой модели r  в восторге от

277
00:09:47,920 --> 00:09:50,240
его выходных прогнозов, которые вы можете увидеть в

278
00:09:50,240 --> 00:09:52,080
ячейке 14 здесь, я ищу случаи,

279
00:09:52,080 --> 00:09:53,839
когда фактическая метка нейтральна, а

280
00:09:53,839 --> 00:09:56,000
модель предсказала положительный результат, мы можем найти

281
00:09:56,000 --> 00:09:57,519
эти атрибуты, и это на самом

282
00:09:57,519 --> 00:10:00,000
деле информативное изображение здесь, похоже,

283
00:10:00,000 --> 00:10:03,040
что модель имеет функции переобучения  такие

284
00:10:03,040 --> 00:10:05,120
как точка и запятая, они

285
00:10:05,120 --> 00:10:07,279
должны указывать на нейтральную категорию,

286
00:10:07,279 --> 00:10:09,200
но здесь они используются таким образом, чтобы

287
00:10:09,200 --> 00:10:11,200
привести к положительному прогнозу, так что это

288
00:10:11,200 --> 00:10:13,440
то, к чему мы, возможно, захотим обратиться,

289
00:10:13,440 --> 00:10:15,200
и мы можем пойти на один уровень дальше, если мы

290
00:10:15,200 --> 00:10:17,440
выберем и посмотрим на  отдельные примеры,

291
00:10:17,440 --> 00:10:19,120
поэтому здесь я привел отдельный

292
00:10:19,120 --> 00:10:21,600
пример, здесь никто не остается безнаказанным,

293
00:10:21,600 --> 00:10:23,760
что, вероятно, к лучшему, это

294
00:10:23,760 --> 00:10:26,240
случай, когда правильный ярлык нейтрален,

295
00:10:26,240 --> 00:10:28,640
а наша модель предсказывает положительный результат, и я

296
00:10:28,640 --> 00:10:30,320
думаю, что атрибуции снова помогают нам

297
00:10:30,320 --> 00:10:32,640
понять, почему, потому что  пока

298
00:10:32,640 --> 00:10:34,399
функция с наивысшей атрибуцией

299
00:10:34,399 --> 00:10:36,240
является лучшей,

300
00:10:36,240 --> 00:10:37,839
и это показывает, что модель

301
00:10:37,839 --> 00:10:40,560
просто не понимает контекст, в

302
00:10:40,560 --> 00:10:42,399
котором слово «лучший» используется  sed в этом

303
00:10:42,399 --> 00:10:44,320
примере, что может указывать на

304
00:10:44,320 --> 00:10:46,800
фундаментальную слабость подхода «мешок слов».

305
00:10:46,800 --> 00:10:49,200


306
00:10:49,519 --> 00:10:51,519
Для моего второго примера давайте обратимся к

307
00:10:51,519 --> 00:10:53,279
моделям-трансформерам, поскольку я предполагаю, что

308
00:10:53,279 --> 00:10:54,720
многие из вас будут работать с этими

309
00:10:54,720 --> 00:10:56,480
моделями, и они открывают захватывающие новые

310
00:10:56,480 --> 00:10:58,800
возможности для атрибуции функций,

311
00:10:58,800 --> 00:11:01,120
потому что  в этих моделях у нас так много

312
00:11:01,120 --> 00:11:03,120
представлений, что мы могли бы подумать

313
00:11:03,120 --> 00:11:05,519
о том, чтобы сделать атрибуции, потому что вот своего

314
00:11:05,519 --> 00:11:07,600
рода общая картина вертикальной

315
00:11:07,600 --> 00:11:10,160
модели, где у меня есть выходы здесь у

316
00:11:10,160 --> 00:11:12,240
вас есть много слоев выходов блоков трансформаторов,

317
00:11:12,240 --> 00:11:14,720
которые выделены фиолетовым цветом,

318
00:11:14,720 --> 00:11:16,480
тогда, вероятно,  слой встраивания, выделенный

319
00:11:16,480 --> 00:11:19,200
зеленым, и этот слой встраивания может

320
00:11:19,200 --> 00:11:21,760
сам состоять из слоя встраивания слов,

321
00:11:21,760 --> 00:11:24,000
слоя позиционного встраивания

322
00:11:24,000 --> 00:11:25,519
и, возможно, других,

323
00:11:25,519 --> 00:11:27,519
все эти слои являются потенциальными

324
00:11:27,519 --> 00:11:30,320
целями для интегрированных градиентов, и

325
00:11:30,320 --> 00:11:33,200
их повторное закрытие делает это относительно легко,

326
00:11:33,200 --> 00:11:35,120
поэтому начать с этого  Я только что скачал

327
00:11:35,120 --> 00:11:36,959
с

328
00:11:36,959 --> 00:11:39,120
Hugging Face модель настроений в Твиттере, основанную на Роберте,

329
00:11:39,120 --> 00:11:41,040
которая показалась мне действительно интересной и  я

330
00:11:41,040 --> 00:11:42,880
написал метод прогнозирования одного прабха,

331
00:11:42,880 --> 00:11:45,120
который поможет нам с анализом ошибок,

332
00:11:45,120 --> 00:11:47,600
который мы хотим сделать на

333
00:11:47,600 --> 00:11:49,600
следующем шаге, здесь

334
00:11:49,600 --> 00:11:52,639
кодирует как фактический пример с использованием

335
00:11:52,639 --> 00:11:55,279
токенизатора моделей, так и базовую линию

336
00:11:55,279 --> 00:11:56,720
всех нулей, которые мы будем использовать для

337
00:11:56,720 --> 00:11:58,240
сравнения

338
00:11:58,240 --> 00:12:00,880
в седьмой ячейке. Я только что разработал небольшой

339
00:12:00,880 --> 00:12:02,959
пользовательский прямой метод, чтобы помочь капитану

340
00:12:02,959 --> 00:12:04,880
, потому что эта модель имеет немного

341
00:12:04,880 --> 00:12:06,800
другую структуру вывода, чем ожидается

342
00:12:06,800 --> 00:12:09,040


343
00:12:09,040 --> 00:12:11,200
здесь, в ячейке 8 мы настраиваем слой, который

344
00:12:11,200 --> 00:12:12,800
мы хотим настроить, как вы можете видеть, я

345
00:12:12,800 --> 00:12:15,360
нацеливание на слой внедрения, но многие

346
00:12:15,360 --> 00:12:17,120
другие слои могут быть нацелены на капитана,

347
00:12:17,120 --> 00:12:18,480
это

348
00:12:18,480 --> 00:12:20,079
упрощает наш пример, который мы используем, это

349
00:12:20,079 --> 00:12:22,399
освещает, что я возьму, чтобы иметь

350
00:12:22,399 --> 00:12:24,480
истинный положительный класс,

351
00:12:24,480 --> 00:12:26,720
мы делаем наши кодировки в ячейке 11

352
00:12:26,720 --> 00:12:28,720
как фактического примера, так и базового уровня, а

353
00:12:28,720 --> 00:12:30,000
затем  это основа для нашей

354
00:12:30,000 --> 00:12:32,959
атрибуции этого единственного примера

355
00:12:32,959 --> 00:12:36,079
теперь для сгоревшего, потому что у нас есть

356
00:12:36,079 --> 00:12:38,079
многомерные представления для каждого

357
00:12:38,079 --> 00:12:40,000
из токенов, которые мы рассматриваем, нам

358
00:12:40,000 --> 00:12:41,680
нужно выполнить еще один уровень

359
00:12:41,680 --> 00:12:43,440
c  сжатие, которое нам не нужно

360
00:12:43,440 --> 00:12:45,600
для примера прямой связи, как вы можете видеть

361
00:12:45,600 --> 00:12:47,839
здесь, атрибуции имеют для одного

362
00:12:47,839 --> 00:12:48,959
примера

363
00:12:48,959 --> 00:12:52,000
размерность 6 на 768, это один

364
00:12:52,000 --> 00:12:54,000
вектор на токен слова,

365
00:12:54,000 --> 00:12:55,760
чтобы суммировать те, что на уровне

366
00:12:55,760 --> 00:12:57,839
отдельных токенов слова, мы просто суммируем

367
00:12:57,839 --> 00:13:00,160
их, а затем z-оценка нормализует их,

368
00:13:00,160 --> 00:13:01,760
чтобы как бы поместить их в согласованную

369
00:13:01,760 --> 00:13:03,519
шкалу, чтобы уменьшить

370
00:13:03,519 --> 00:13:06,639
атрибуции до одной на

371
00:13:06,639 --> 00:13:08,560
токен

372
00:13:08,560 --> 00:13:10,560
подслова, и это используется в нашем окончательном

373
00:13:10,560 --> 00:13:12,399
кумулятивном анализе, поэтому мы будем делать

374
00:13:12,399 --> 00:13:14,320
вероятностные прогнозы, которые

375
00:13:14,320 --> 00:13:16,639
мы  Получим фактический класс,

376
00:13:16,639 --> 00:13:18,240
преобразующий входные данные во что-то, что

377
00:13:18,240 --> 00:13:20,240
капитан может переварить, а затем используйте этот

378
00:13:20,240 --> 00:13:22,880
метод записи данных визуализации, чтобы

379
00:13:22,880 --> 00:13:24,880
объединить все это в красивую

380
00:13:24,880 --> 00:13:27,200
табличную визуализацию, и это то, что

381
00:13:27,200 --> 00:13:28,639
происходит здесь, вы можете видеть для нашего

382
00:13:28,639 --> 00:13:30,639
примера, у нас есть истинная метка,

383
00:13:30,639 --> 00:13:32,399
предсказанная  метка со связанной

384
00:13:32,399 --> 00:13:33,600
вероятностью,

385
00:13:33,600 --> 00:13:35,760
а затем действительно интересная часть

386
00:13:35,760 --> 00:13:38,079
токена слова, у нас есть сводка его

387
00:13:38,079 --> 00:13:40,079
атрибутов, и вы можете видеть, что зеленый

388
00:13:40,079 --> 00:13:42,480
цвет связан с позицией.  белый с

389
00:13:42,480 --> 00:13:44,720
нейтральным и красный с отрицательным, и

390
00:13:44,720 --> 00:13:46,720
это дает нам обнадеживающую

391
00:13:46,720 --> 00:13:49,040
картину систематичности этих предсказаний

392
00:13:49,040 --> 00:13:51,360
, это положительное предсказание, и большая часть из

393
00:13:51,360 --> 00:13:52,639
них является результатом слова «

394
00:13:52,639 --> 00:13:55,839
освещение» и восклицательного знака,

395
00:13:55,839 --> 00:13:57,519
и тому подобное питает хороший вид

396
00:13:57,519 --> 00:14:00,480
анализ ошибок косая черта анализ вызовов,

397
00:14:00,480 --> 00:14:02,399
которые вы можете сделать для таких моделей, как

398
00:14:02,399 --> 00:14:04,959
эта, используя captum для этого слайда, здесь

399
00:14:04,959 --> 00:14:06,480
я поставил небольшую задачу или

400
00:14:06,480 --> 00:14:09,120
состязательный тест, чтобы увидеть, насколько глубоко моя

401
00:14:09,120 --> 00:14:11,839
модель понимает предложения, как они

402
00:14:11,839 --> 00:14:13,279
сказали, что это было бы здорово, и они были

403
00:14:13,279 --> 00:14:14,320
правы

404
00:14:14,320 --> 00:14:15,600
вы  могу видеть, что

405
00:14:15,600 --> 00:14:16,959
в этом случае он делает правильный прогноз, и когда я

406
00:14:16,959 --> 00:14:18,959
меняю его на они сказали, что это было бы здорово,

407
00:14:18,959 --> 00:14:21,199
и они были неправы, он предсказывает отрицательное,

408
00:14:21,199 --> 00:14:23,120
что обнадеживает, и поэтому атрибуции функций,

409
00:14:23,120 --> 00:14:25,760
кажется, вводят

410
00:14:25,760 --> 00:14:27,680
именно те фрагменты информации, которые я

411
00:14:27,680 --> 00:14:29,680
Я надеюсь и даже делаю это

412
00:14:29,680 --> 00:14:31,839
контекстно-зависимым способом

413
00:14:31,839 --> 00:14:33,680
для следующих двух примеров. Я

414
00:14:33,680 --> 00:14:35,360
просто изменю синтаксис, чтобы увидеть, не подходит ли он

415
00:14:35,360 --> 00:14:37,279
для p  позиция этих

416
00:14:37,279 --> 00:14:39,440
слов в строке, и это снова выглядит

417
00:14:39,440 --> 00:14:41,040
надежным, они были правы, говоря, что это

418
00:14:41,040 --> 00:14:42,160
было бы отличным

419
00:14:42,160 --> 00:14:43,920
предсказанием положительного, они были неправы

420
00:14:43,920 --> 00:14:45,519
, говоря, что это было бы отличным

421
00:14:45,519 --> 00:14:48,639
предсказанием отрицательного, очень обнадеживающим, как

422
00:14:48,639 --> 00:14:50,480
предпоследний пример, в котором они это сказали.

423
00:14:50,480 --> 00:14:52,560
была бы звездной, и они были правы,

424
00:14:52,560 --> 00:14:54,399
единственная разочаровывающая вещь в этой

425
00:14:54,399 --> 00:14:56,639
задаче - это то, что для этого последнего

426
00:14:56,639 --> 00:14:58,959
примера она предсказывает нейтральную, потому что они

427
00:14:58,959 --> 00:15:00,399
сказали, что она будет звездной, и они были

428
00:15:00,399 --> 00:15:02,800
неверны, и атрибуции

429
00:15:02,800 --> 00:15:04,639
также немного беспокоят степень,

430
00:15:04,639 --> 00:15:06,800
в которой модель  действительно понял

431
00:15:06,800 --> 00:15:08,560
этот пример,

432
00:15:08,560 --> 00:15:10,000
может быть, мы можем подумать о том, как

433
00:15:10,000 --> 00:15:12,079
решить эту проблему, но основной

434
00:15:12,079 --> 00:15:14,480
вывод на данный момент заключается в том, что вы просто можете

435
00:15:14,480 --> 00:15:16,959
увидеть, как вы можете использовать атрибуцию функций

436
00:15:16,959 --> 00:15:19,519
вместе с примерами задач, чтобы как

437
00:15:19,519 --> 00:15:22,560
бы отточить то, насколько именно систематична

438
00:15:22,560 --> 00:15:24,800
модельная модель  прогнозы для

439
00:15:24,800 --> 00:15:28,760
интересного класса случаев

