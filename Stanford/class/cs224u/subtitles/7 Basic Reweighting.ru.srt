1
00:00:05,679 --> 00:00:08,719
приветствую всех, это четвертая часть

2
00:00:07,359 --> 00:00:09,839
нашей серии о распределенных представлениях слов

3
00:00:09,839 --> 00:00:13,199
поговорить об основных схемах повторного взвешивания, по

4
00:00:12,000 --> 00:00:14,718
сути, я чувствую, что мы слишком долго были

5
00:00:18,160 --> 00:00:20,800
вот некоторые цели высокого уровня, которые у нас

6
00:00:19,439 --> 00:00:22,480
есть для переписывания, мы хотели бы в

7
00:00:24,399 --> 00:00:28,239
заслуживают доверия и необычны, при этом

8
00:00:26,480 --> 00:00:30,080
соответственно, уменьшая акцент на

9
00:00:28,239 --> 00:00:32,000
вещах, которые являются обыденными или причудливыми, или

10
00:00:30,079 --> 00:00:33,519
отражают ошибки или особенности в

11
00:00:33,520 --> 00:00:37,600
сейчас, конечно, в отсутствие определенной целевой

12
00:00:36,159 --> 00:00:39,679
функции в смысле машинного обучения

13
00:00:39,679 --> 00:00:43,600
у нас есть некоторые количественные крючки, я

14
00:00:41,439 --> 00:00:45,439
думаю, у нас есть эта направляющая интуиция

15
00:00:43,600 --> 00:00:47,920
которую мы хотели бы отойти от необработанных

16
00:00:45,439 --> 00:00:49,920
подсчетов, потому что частота  сам по себе, как

17
00:00:47,920 --> 00:00:51,600
правило, является плохим заменителем той

18
00:00:49,920 --> 00:00:52,960
семантической информации, которую мы надеемся

19
00:00:52,960 --> 00:00:56,000
чтобы мы могли запросить каждый из

20
00:00:54,320 --> 00:00:57,759
повторных запросов.  Схемы борьбы, которые мы рассматриваем в

21
00:00:56,000 --> 00:00:59,840
первую очередь, как они соотносятся с

22
00:00:57,759 --> 00:01:01,920
базовыми необработанными значениями счетчика

23
00:00:59,840 --> 00:01:03,600
если схема просто перемасштабирует

24
00:01:01,920 --> 00:01:05,439
базовые значения счетчиков, с другой стороны, вероятно, не

25
00:01:05,438 --> 00:01:08,798
дает нам совсем другое распределение

26
00:01:07,359 --> 00:01:10,799
тогда, по крайней мере, мы знаем, что мы  готовим

27
00:01:08,799 --> 00:01:12,799
с огнем, когда дело доходит до отказа

28
00:01:12,799 --> 00:01:15,920
есть связанный с этим вопрос, который я хотел

29
00:01:14,000 --> 00:01:17,680
бы, чтобы мы имели в виду, каково

30
00:01:15,920 --> 00:01:19,680
общее распределение значений, которое

31
00:01:17,680 --> 00:01:21,920
обеспечивает схема повторного взвешивания.

32
00:01:25,040 --> 00:01:29,118
методов и методов машинного обучения, поэтому мы можем надеяться

33
00:01:27,280 --> 00:01:30,239
что повторное взвешивание в дополнение к захвату

34
00:01:30,239 --> 00:01:33,759
вещей и уменьшению акцента на мирских вещах

35
00:01:33,759 --> 00:01:36,799
распределение значений, которое  было более

36
00:01:35,280 --> 00:01:38,478
податливым для этих нижестоящих

37
00:01:40,000 --> 00:01:44,239
цель, которую мы хотели бы не делать при

38
00:01:41,680 --> 00:01:46,720
выборе функций на основе подсчетов.  или внешние

39
00:01:44,239 --> 00:01:48,640
ресурсы, такие как словари стандартных слов, я

40
00:01:48,640 --> 00:01:52,960
части словаря априори, потому что, насколько

41
00:01:51,280 --> 00:01:54,879
я знаю, что-то, что является скучным стоп-

42
00:01:52,959 --> 00:01:56,879
словом для одного жанра, на самом деле является

43
00:01:54,879 --> 00:01:58,718
важным содержательным словом для другого, мы

44
00:01:56,879 --> 00:02:01,359
хотели бы, чтобы метод  вроде как принять

45
00:02:02,078 --> 00:02:05,599
поэтому давайте начнем с самой простой

46
00:02:03,840 --> 00:02:07,840
схемы, и это схема, которая будет

47
00:02:05,599 --> 00:02:09,519
обращать внимание только на контекст строки

48
00:02:09,520 --> 00:02:13,760
самом деле это повторение из лекции о

49
00:02:11,199 --> 00:02:15,759
сравнении векторов l2 нормирование, мы

50
00:02:13,759 --> 00:02:17,919
вычислили  l2 длина как фиксированная

51
00:02:15,759 --> 00:02:19,759
величина для каждого вектора-строки, а

52
00:02:19,759 --> 00:02:23,919
вектора-строки просто берет каждое значение в

53
00:02:21,759 --> 00:02:27,120
исходном векторе и делит его на эту

54
00:02:23,919 --> 00:02:28,399
фиксированную величину. Длина l2

55
00:02:27,120 --> 00:02:29,680
есть родственное и, возможно, более

56
00:02:28,400 --> 00:02:31,599
знакомое понятие, которое я  называемое

57
00:02:29,680 --> 00:02:33,840
распределением вероятностей, где мы следуем

58
00:02:31,598 --> 00:02:36,159
той же логике, мы просто заменяем эту

59
00:02:33,840 --> 00:02:37,680
нормализующую константу длины l2

60
00:02:37,680 --> 00:02:41,680
векторах, но  снова мы делаем это

61
00:02:39,120 --> 00:02:43,840
поэлементное деление на эту фиксированную

62
00:02:41,680 --> 00:02:46,319
величину, чтобы нормализовать вектор в

63
00:02:46,318 --> 00:02:49,598
Я думаю, что оба эти метода могут быть

64
00:02:47,680 --> 00:02:51,360
мощными, но их позор заключается в том, что

65
00:02:49,598 --> 00:02:54,000
они обращают внимание только на

66
00:02:51,360 --> 00:02:56,560
контекст строки для данной ячейки ij  мы

67
00:02:54,000 --> 00:02:58,400
смотрим только через строку i мы не

68
00:02:56,560 --> 00:03:00,640
учитываем контекст, который может исходить

69
00:02:58,400 --> 00:03:02,640
из столбца j, поэтому давайте начнем

70
00:03:02,639 --> 00:03:06,238
вот своего рода звезда нашего шоу в

71
00:03:04,560 --> 00:03:07,680
тихом смысле это первая схема, которую

72
00:03:06,239 --> 00:03:09,519
мы будем  посмотрите на это обращает внимание

73
00:03:07,680 --> 00:03:12,000
как на контекст строки, так и на столбец, это

74
00:03:09,519 --> 00:03:13,439
наблюдается выше ожидаемого, давайте просто пройдемся

75
00:03:12,000 --> 00:03:15,280
по этим обозначениям, здесь у нас есть

76
00:03:13,439 --> 00:03:17,280
сумма строки, я думаю, что это интуитивно понятно

77
00:03:17,280 --> 00:03:21,598
сумма всех значений в столбце, а

78
00:03:19,759 --> 00:03:23,280
затем сумма для некоторого  матрица x - это просто

79
00:03:21,598 --> 00:03:24,479
сумма всех значений ячеек в этой

80
00:03:24,479 --> 00:03:27,679
которые являются исходными материалами для

81
00:03:25,840 --> 00:03:30,479
вычисления того, что называется ожидаемым

82
00:03:27,680 --> 00:03:34,000
значением, ожидаемое значение, заданное матрицей

83
00:03:30,479 --> 00:03:35,359
x для ячейки ij, - это время суммы строк  s

84
00:03:35,360 --> 00:03:39,519
виде числителя, деленная на сумму

85
00:03:42,158 --> 00:03:46,959
дает нам число, которое мы ожидали бы, если

86
00:03:44,878 --> 00:03:48,719
бы строка и столбец были независимыми

87
00:03:46,959 --> 00:03:50,239
друг от друга в статистическом смысле, и

88
00:03:48,719 --> 00:03:51,759
это  В том смысле, в котором это

89
00:03:51,759 --> 00:03:55,679
наблюдаемое значение выше ожидаемого, просто

90
00:03:53,759 --> 00:03:58,959
сравните наблюдаемое значение в

91
00:03:55,680 --> 00:04:00,480
числителе с этим ожидаемым значением

92
00:03:58,959 --> 00:04:02,560
поэтому, немного подробнее, вот как

93
00:04:00,479 --> 00:04:04,238
работают вычисления. У нас есть эта

94
00:04:02,560 --> 00:04:07,039
крошечная матрица счета, давайте посмотрим на

95
00:04:04,239 --> 00:04:08,959
ячейку xa.  у него есть счет 34. это

96
00:04:08,959 --> 00:04:14,560
числителе, знаменатель - это произведение

97
00:04:11,919 --> 00:04:17,199
суммы строк и суммы столбцов 45 на

98
00:04:14,560 --> 00:04:20,000
81, деленное на сумму всех значений

99
00:04:17,199 --> 00:04:22,400
в этой матрице, которая равна 99. мы повторяем

100
00:04:20,000 --> 00:04:24,399
что  расчет для всех других ячеек

101
00:04:22,399 --> 00:04:26,319
внесение соответствующих корректировок, и

102
00:04:24,399 --> 00:04:28,879
это дает нам полностью перевзвешенную

103
00:04:30,478 --> 00:04:34,399
расчетом, давайте подумаем, почему мы

104
00:04:32,160 --> 00:04:37,040
можем захотеть это сделать, поэтому я  у нас есть

105
00:04:34,399 --> 00:04:39,279
очень идеализированная матрица небольшого количества

106
00:04:37,040 --> 00:04:41,600
и тщеславие этого примера состоит в том, что держать

107
00:04:39,279 --> 00:04:43,439
вкладки в английском языке - это идиома, и в

108
00:04:41,600 --> 00:04:45,120
противном случае само по себе слово вкладки не

109
00:04:43,439 --> 00:04:48,079
появляется со многими другими словами, оно как

110
00:04:45,120 --> 00:04:50,319
бы ограничено этим идиоматическим контекстом

111
00:04:48,079 --> 00:04:52,639
поэтому мы получаем  действительно большое количество для 

112
00:04:50,319 --> 00:04:54,719
вкладок и относительно низкое количество для

113
00:04:52,639 --> 00:04:57,519
вкладок с удовольствием, потому что вкладки на

114
00:04:54,720 --> 00:04:59,360
самом деле не ассоциируются со словом «наслаждаться»

115
00:04:57,519 --> 00:05:01,680
справа здесь, у меня есть ожидаемый

116
00:04:59,360 --> 00:05:03,759
расчет, и он выходит так же, как

117
00:05:01,680 --> 00:05:06,639
мы надеемся, ожидаемое количество  для

118
00:05:03,759 --> 00:05:08,240
keep tabs составляет всего 12,48

119
00:05:06,639 --> 00:05:10,639
сравните это с наблюдаемым числом

120
00:05:08,240 --> 00:05:13,038
20. keep tabs представлено избыточно по

121
00:05:10,639 --> 00:05:14,639
сравнению с нашими ожиданиями в

122
00:05:13,038 --> 00:05:16,399
силу того факта, что предположение о независимости

123
00:05:16,399 --> 00:05:20,478
расчет, здесь просто не выполняется из

124
00:05:20,478 --> 00:05:25,599
аналогичного  ожидаемое количество

125
00:05:22,879 --> 00:05:27,600
вкладок удовольствия составляет 8,5, что снова намного больше, чем 

126
00:05:25,600 --> 00:05:30,160
наблюдение, потому что они как бы не связаны

127
00:05:30,160 --> 00:05:33,840
из-за ограниченного диссоциирования.  распределение

128
00:05:33,839 --> 00:05:36,959
и это приводит нас к действительной

129
00:05:35,279 --> 00:05:38,879
звезде нашего показанного на самом деле звезды

130
00:05:38,879 --> 00:05:44,800
это точка y, это взаимная информация, или pmi

131
00:05:42,000 --> 00:05:46,639
pmi просто наблюдается сверх ожидаемого в

132
00:05:44,800 --> 00:05:49,520
пространстве журнала, где мы оговариваем, что

133
00:05:46,639 --> 00:05:52,800
логарифм нуля равен нулю немного более подробно

134
00:05:49,519 --> 00:05:54,639
для матрицы x с заданной ячейкой ij

135
00:05:54,639 --> 00:05:58,160
логарифм наблюдаемого количества по сравнению с ожидаемым

136
00:05:58,160 --> 00:06:01,680
многие люди считают более интуитивным

137
00:05:59,918 --> 00:06:03,038
думать об этом в вероятностных терминах

138
00:06:03,038 --> 00:06:07,199
справа, это эквивалентно численно, но

139
00:06:05,279 --> 00:06:09,758
для такого рода расчетов мы сначала

140
00:06:07,199 --> 00:06:11,680
формируем совместную таблицу вероятностей

141
00:06:09,759 --> 00:06:13,439
, просто разделив все значения ячеек

142
00:06:11,680 --> 00:06:14,478
на общее количество значений во всех

143
00:06:16,000 --> 00:06:20,319
таблицу вероятностей и  затем вероятность строки

144
00:06:18,240 --> 00:06:21,759
и вероятность столбца просто суммируются

145
00:06:21,759 --> 00:06:25,600
соответственно, и снова мы их умножаем

146
00:06:24,240 --> 00:06:27,038
и это приятно, потому что тогда вы

147
00:06:25,600 --> 00:06:28,560
можете видеть, что мы действительно тестируем

148
00:06:27,038 --> 00:06:30,399
предположение о независимости, это как если бы

149
00:06:28,560 --> 00:06:31,600
мы сказали, что можем умножить эти

150
00:06:31,600 --> 00:06:35,840
независимы, если распределение действительно

151
00:06:34,000 --> 00:06:37,759
независимо, то будет наблюдаться осеннее совпадение

152
00:06:37,759 --> 00:06:40,960
это то, что эти матрицы будут

153
00:06:40,959 --> 00:06:43,918
давайте посмотрим на пример, и есть одна

154
00:06:42,319 --> 00:06:45,840
вещь  что я хочу отслеживать, пока мы работаем

155
00:06:45,839 --> 00:06:50,000
ячейка здесь, одинокая маленькая, так что это

156
00:06:48,319 --> 00:06:51,919
матрица счета, у меня есть это как

157
00:06:50,000 --> 00:06:53,199
матрица слова за документом, это очень

158
00:06:51,918 --> 00:06:55,120
гибкий метод, и мы применим его  для

159
00:06:55,120 --> 00:06:59,759
дизайнов здесь я формирую совместную таблицу вероятностей

160
00:06:57,199 --> 00:07:01,280
и у меня есть здесь сумма столбца

161
00:06:59,759 --> 00:07:03,038
и сумма строки, соответствующая

162
00:07:01,279 --> 00:07:04,559
вероятности столбца и строки, это

163
00:07:04,560 --> 00:07:09,519
для матрицы pmi, которая получена

164
00:07:06,800 --> 00:07:10,960
здесь, применяя это  вычисление всех

165
00:07:10,959 --> 00:07:14,399
обратите внимание, что произошло с этим

166
00:07:12,800 --> 00:07:16,000
одиноким здесь, потому что он находится в очень

167
00:07:16,000 --> 00:07:21,439
редком столбце, у него самый большой pm  я

168
00:07:19,038 --> 00:07:22,879
значение в результирующей матрице сейчас, что

169
00:07:21,439 --> 00:07:25,199
может быть хорошо, потому что это может быть

170
00:07:22,879 --> 00:07:28,000
очень важным событием, и в этом случае мы

171
00:07:25,199 --> 00:07:30,400
хотим усилить его, с другой стороны

172
00:07:28,000 --> 00:07:32,399
nlp, что это такое, это может быть

173
00:07:30,399 --> 00:07:34,159
просто ошибка в данных или что-то в этом роде, а

174
00:07:32,399 --> 00:07:36,079
затем это  преувеличенное значение здесь может

175
00:07:37,598 --> 00:07:41,598
при работе с этим методом стоит помнить

176
00:07:49,120 --> 00:07:52,399
хотел бы думать об этом как о представлении по

177
00:07:50,319 --> 00:07:53,919
умолчанию, которое мы используем для pmi, по

178
00:07:53,918 --> 00:07:57,519
pmi на самом деле не определено, где

179
00:07:55,759 --> 00:07:59,199
счетчик равен 0, потому что нам нужно взять

180
00:07:59,199 --> 00:08:04,560
поэтому мы должны были оговорить, что журнал

181
00:08:00,720 --> 00:08:04,560
ноль был равен нулю для этого расчета

182
00:08:04,639 --> 00:08:08,079
однако это, возможно, не является последовательным, если

183
00:08:06,879 --> 00:08:09,919
вы думаете о том, что представляет собой базовая

184
00:08:09,918 --> 00:08:14,318
pmi, так это то, что значения, превышающие ожидаемые

185
00:08:14,319 --> 00:08:18,560
меньше, чем ожидаемые значения, получают

186
00:08:16,079 --> 00:08:20,399
меньший pmi, что хорошо, но когда мы

187
00:08:18,560 --> 00:08:22,079
сталкиваемся с нулем, мы помещаем его прямо

188
00:08:20,399 --> 00:08:23,839
в середину, и это просто странно

189
00:08:22,079 --> 00:08:25,839
потому что ноль не свидетельствует о

190
00:08:23,839 --> 00:08:27,359
чем-то большем или меньшем, он не

191
00:08:25,839 --> 00:08:29,038
заслуживает того, чтобы быть в середине.  в середине этого, если

192
00:08:27,360 --> 00:08:30,800
что-нибудь, мы просто не знаем, что делать

193
00:08:30,800 --> 00:08:35,360
так что это, возможно, бессвязно

194
00:08:33,278 --> 00:08:36,320
и стандартный ответ на это состоит в том, чтобы

195
00:08:36,320 --> 00:08:40,479
превратить все отрицательные значения в

196
00:08:38,320 --> 00:08:42,800
нули, и это положительный pmi, который

197
00:08:40,479 --> 00:08:44,560
определен здесь, так что  мы просто отсекаем

198
00:08:42,799 --> 00:08:45,759
все отрицательные значения, сопоставляя их с

199
00:08:45,759 --> 00:08:49,439
и это, по крайней мере, восстанавливает

200
00:08:47,278 --> 00:08:51,278
общую согласованность утверждений, где

201
00:08:49,440 --> 00:08:53,360
все, что мы делаем, — это отражение того факта

202
00:08:51,278 --> 00:08:56,159
что большее, чем ожидалось, число имеет

203
00:08:53,360 --> 00:08:58,080
большие положительные PMI, а остальные помещаются

204
00:08:58,080 --> 00:09:01,120
давайте кратко рассмотрим несколько других

205
00:08:59,600 --> 00:09:02,959
схем повторного взвешивания, начиная с

206
00:09:05,919 --> 00:09:09,599
крыса  ing схема, и мне она нравится

207
00:09:08,000 --> 00:09:11,759
потому что она, очевидно, отражает многие из

208
00:09:09,600 --> 00:09:14,879
тех же интуитивных предположений, которые определяют PMI

209
00:09:11,759 --> 00:09:17,120
и наблюдаемые в ожидаемых расчетах.

210
00:09:14,879 --> 00:09:19,120
tf idf совершенно отличается, поэтому это

211
00:09:17,120 --> 00:09:21,360
обычно выполняется для матриц слово за документом

212
00:09:19,120 --> 00:09:23,839
в контексте поиска информации с

213
00:09:21,360 --> 00:09:25,120
учетом некоторого корпуса документов.

214
00:09:23,839 --> 00:09:27,279
d мы собираемся сказать, что термин

215
00:09:27,278 --> 00:09:30,879
- это значение, деленное на сумму

216
00:09:29,200 --> 00:09:32,560
всех значений в столбце, что дает нам своего

217
00:09:32,559 --> 00:09:36,559
учетом документа, в котором мы находимся

218
00:09:36,559 --> 00:09:40,159
- это журнал этой величины, здесь

219
00:09:38,480 --> 00:09:42,000
это количество корпусов, количество

220
00:09:40,159 --> 00:09:44,319
документов в нашем корпусе, то есть дорога

221
00:09:44,320 --> 00:09:48,480
количество документов, содержащих

222
00:09:46,000 --> 00:09:51,120
целевое слово, и снова мы сопоставляем логарифм нуля

223
00:09:48,480 --> 00:09:52,560
с нулем tf  idf - это произведение

224
00:09:52,559 --> 00:09:56,879
я думаю, что это может быть выдающимся

225
00:09:54,399 --> 00:09:58,799
методом для очень больших разреженных матриц

226
00:09:56,879 --> 00:10:01,039
таких как текстовый документ, и

227
00:09:58,799 --> 00:10:03,679
наоборот, он обычно не очень хорошо

228
00:10:01,039 --> 00:10:05,439
себя ведет для очень ден.  матрицы se, такие как

229
00:10:03,679 --> 00:10:07,599
пословные, которые мы используем

230
00:10:07,600 --> 00:10:12,000
которой это значение idf, очень маловероятно

231
00:10:10,559 --> 00:10:14,319
что у вас будет слово, которое появляется

232
00:10:14,320 --> 00:10:19,360
однако в контексте очень плотного

233
00:10:19,360 --> 00:10:23,839
некоторые слова могут встречаться вместе с каждым

234
00:10:21,919 --> 00:10:26,240
другим словом, и в этом случае вы получаете значение idf

235
00:10:23,839 --> 00:10:27,680
равное нулю, что, вероятно, не является

236
00:10:26,240 --> 00:10:30,399
предполагаемым результатом для чего-то, что является

237
00:10:27,679 --> 00:10:32,399
высокочастотным, но, тем не менее, может быть

238
00:10:33,679 --> 00:10:37,120
так что я бы, вероятно, держался подальше от tf idf

239
00:10:35,839 --> 00:10:39,120
если вы не работаете с разреженной

240
00:10:40,639 --> 00:10:43,919
что мы обсуждали, вы могли бы исследовать

241
00:10:42,159 --> 00:10:46,480
используя, например, матрицы попарных расстояний

242
00:10:46,480 --> 00:10:50,720
расстояние между каждой парой  слов

243
00:10:48,240 --> 00:10:53,600
по строкам и сформировать на этой основе матрицу

244
00:10:50,720 --> 00:10:55,278
действительно отличающуюся по своему подходу

245
00:10:53,600 --> 00:10:58,240
и, вероятно, по своим результатам, но это

246
00:10:58,240 --> 00:11:01,120
давайте вернемся к нашему центу

247
00:11:01,120 --> 00:11:05,919
переписываний мы хотим спросить, как это

248
00:11:03,200 --> 00:11:08,000
соотносится с необработанными значениями счетчика и какое

249
00:11:05,919 --> 00:11:09,519
общее распределение значений оно

250
00:11:08,000 --> 00:11:11,360
обеспечивает, поэтому давайте немного

251
00:11:11,360 --> 00:11:15,360
матрицей giga5, которая  вы можете загрузить как

252
00:11:13,519 --> 00:11:17,360
часть материалов курса, которые являются гигавардными

253
00:11:15,360 --> 00:11:19,039
с окном из пяти и масштабированием на единицу

254
00:11:19,039 --> 00:11:22,958
n здесь слева у меня есть необработанные

255
00:11:22,958 --> 00:11:27,599
оси x и количество вещей, которые

256
00:11:24,639 --> 00:11:29,919
имеют это значение  вдоль оси Y, и вы

257
00:11:27,600 --> 00:11:31,839
можете видеть, что необработанные подсчеты - это очень

258
00:11:29,919 --> 00:11:34,078
сложное распределение, во-первых, это

259
00:11:35,360 --> 00:11:40,320
поэтому, начиная с нуля, у большинства вещей

260
00:11:38,078 --> 00:11:42,399
есть количества, близкие к нулю

261
00:11:40,320 --> 00:11:44,640
тогда у вас есть это очень  длинный тонкий

262
00:11:42,399 --> 00:11:46,480
хвост вещей с очень высокой частотой

263
00:11:44,639 --> 00:11:48,319
этот вид сильно асимметричного распределения

264
00:11:46,480 --> 00:11:50,560
сложен для многих методов машинного обучения

265
00:11:50,559 --> 00:11:54,319
сторону нуля и очень низких значений, так

266
00:11:52,720 --> 00:11:56,639
и с точки зрения диапазона t  эти

267
00:11:54,320 --> 00:11:58,480
значения по оси x, поэтому мы хотели бы

268
00:11:56,639 --> 00:11:59,519
отойти от них, это одна из мотивирующих

269
00:11:59,519 --> 00:12:02,720
когда мы смотрим на нормирование l2 и

270
00:12:00,879 --> 00:12:04,879
распределения вероятностей, они делают

271
00:12:04,879 --> 00:12:10,399
значения ячеек между 0 и 1 или

272
00:12:10,399 --> 00:12:14,000
но они по-прежнему имеют сильный перекос в

273
00:12:12,240 --> 00:12:15,919
сторону вещей, которые очень малы в

274
00:12:14,000 --> 00:12:17,440
своих скорректированных значениях, и их связанные

275
00:12:17,440 --> 00:12:22,320
наблюдаемые выше ожидаемых, являются более экстремальными

276
00:12:19,278 --> 00:12:24,559
в том, как это есть f idf, поэтому

277
00:12:22,320 --> 00:12:27,440
снова наблюдаемые сверх ожидаемые значения варьируются от

278
00:12:24,559 --> 00:12:28,879
довольно высоких до почти почти  50 000

279
00:12:27,440 --> 00:12:31,120
что несколько лучше, чем необработанные

280
00:12:28,879 --> 00:12:32,879
подсчеты, но все еще очень велико с

281
00:12:31,120 --> 00:12:35,759
точки зрения его разброса, и у нас все еще есть

282
00:12:32,879 --> 00:12:37,838
этот сильный перекос в сторону нуля. tf-idf

283
00:12:35,759 --> 00:12:39,360
решает проблему диапазона здесь

284
00:12:37,839 --> 00:12:41,519
потому что он сильно ограничен

285
00:12:39,360 --> 00:12:43,839
набором значений, но все еще  имеет очень сильную

286
00:12:41,519 --> 00:12:45,679
асимметрию, очень похожую на необработанное распределение 

287
00:12:45,679 --> 00:12:49,439
с этой точки зрения это выглядит как pmi

288
00:12:47,600 --> 00:12:52,240
и положительный pmi действительно является шагом

289
00:12:49,440 --> 00:12:53,839
вперед, прежде всего для pmi этого

290
00:12:52,240 --> 00:12:56,159
распределения.  значений ячеек имеет

291
00:12:53,839 --> 00:12:57,920
хороший тип нормального распределения, а

292
00:12:56,159 --> 00:13:00,240
сами значения довольно ограничены

293
00:12:57,919 --> 00:13:03,120
, например, от отрицательных 10 до 10.

294
00:13:00,240 --> 00:13:04,799
а затем для положительных pmi мы просто

295
00:13:03,120 --> 00:13:06,879
блокируем все отрицательные значения и заставляем их

296
00:13:04,799 --> 00:13:09,278
сопоставляться с нулем, поэтому он более смещен к

297
00:13:06,879 --> 00:13:10,720
нулю, но  не настолько искажены, как все

298
00:13:09,278 --> 00:13:11,519
эти другие методы, которые мы рассматриваем

299
00:13:11,519 --> 00:13:16,159
так что это выглядит так, что pmi и ppmi являются

300
00:13:14,320 --> 00:13:18,800
хорошим выбором здесь только с точки

301
00:13:18,799 --> 00:13:22,559
необработанных подсчетов и предоставления вычитаемого

302
00:13:22,559 --> 00:13:26,719
вот еще одна точка зрения, где мы

303
00:13:24,078 --> 00:13:28,958
напрямую  сравните в этих матрицах

304
00:13:26,720 --> 00:13:31,120
количество совпадений в логарифмической шкале, чтобы его

305
00:13:28,958 --> 00:13:33,039
можно было просмотреть с результатом новое

306
00:13:31,120 --> 00:13:35,759
взвешенное значение ячейки, которое мы

307
00:13:33,039 --> 00:13:38,000
ищем здесь, предположительно, является общим

308
00:13:38,000 --> 00:13:41,360
я думаю, что мы находим, что нормирование l2 и

309
00:13:39,600 --> 00:13:43,519
вероятности довольно хороши на  эта

310
00:13:41,360 --> 00:13:45,600
оценка у них как бы имеет низкую

311
00:13:43,519 --> 00:13:47,440
корреляцию, и они хорошо используют

312
00:13:45,600 --> 00:13:48,879
большую часть шкалы, на которой они

313
00:13:48,879 --> 00:13:52,159
наблюдаемое по сравнению с ожидаемым имеет низкую

314
00:13:50,480 --> 00:13:54,720
корреляцию с остроумием  h подсчет ячеек, который

315
00:13:52,159 --> 00:13:56,399
изначально выглядит хорошо, но у него есть

316
00:13:54,720 --> 00:13:58,320
проблема, заключающаяся в том, что значения ячеек как-то

317
00:13:58,320 --> 00:14:02,000
значение корреляции может даже не иметь

318
00:14:00,000 --> 00:14:03,839
особого значения, учитывая, что у нас

319
00:14:02,000 --> 00:14:06,399
есть несколько выбросов, а затем множество

320
00:14:03,839 --> 00:14:08,399
вещей, которые близки  равным нулю, а tf

321
00:14:06,399 --> 00:14:10,159
idf откровенно похож на низкую корреляцию, но

322
00:14:08,399 --> 00:14:12,399
возможно, не так надежен с точки зрения

323
00:14:10,159 --> 00:14:13,838
этого значения корреляции, в принципе

324
00:14:12,399 --> 00:14:16,399
опять же, это похоже на сложные

325
00:14:13,839 --> 00:14:18,800
распределения значений для работы, с

326
00:14:16,399 --> 00:14:19,600
которыми снова pmi и положительный pmi выглядят действительно

327
00:14:19,600 --> 00:14:23,839
относительно низкая корреляция, поэтому мы

328
00:14:21,679 --> 00:14:26,000
кое-что сделали  значимы, и оба

329
00:14:26,000 --> 00:14:30,320
значительную часть общего пространства, в

330
00:14:28,639 --> 00:14:32,879
котором они работают, у нас есть много

331
00:14:30,320 --> 00:14:34,879
различных комбинаций значений ячеек

332
00:14:32,879 --> 00:14:36,399
и лежащие в основе совпадения подсчитывают

333
00:14:34,879 --> 00:14:38,639
что-то вроде корреляции, но это

334
00:14:36,399 --> 00:14:40,399
может быть хорошо, но мы не  заперты в

335
00:14:38,639 --> 00:14:41,759
этой корреляции, поэтому мы сделали что-то

336
00:14:41,759 --> 00:14:45,838
чтобы подвести итог, давайте сделаем некоторые отношения

337
00:14:43,839 --> 00:14:47,519
и обобщения просто так  Я напоминаю

338
00:14:45,839 --> 00:14:49,360
здесь, поэтому тема, проходящая через почти

339
00:14:47,519 --> 00:14:51,198
все эти схемы, заключается в том, что мы хотим

340
00:14:49,360 --> 00:14:53,278
переписать значение ячейки относительно

341
00:14:51,198 --> 00:14:55,198
значений, которые мы ожидаем, учитывая строку и

342
00:14:53,278 --> 00:14:58,159
столбец, и мы хотели бы использовать

343
00:14:58,159 --> 00:15:02,319
величина  из подсчетов может быть

344
00:15:00,159 --> 00:15:05,360
важно, просто подумайте о том, что

345
00:15:02,320 --> 00:15:07,519
110 как часть доказательства и тысяча

346
00:15:05,360 --> 00:15:09,600
десять тысяч как часть доказательства могут

347
00:15:07,519 --> 00:15:11,839
быть очень разными ситуациями с точки

348
00:15:09,600 --> 00:15:13,759
зрения доказательств, которые вы собрали

349
00:15:11,839 --> 00:15:16,000
создание распределений вероятностей и

350
00:15:17,839 --> 00:15:21,040
то, на чем вы хотите остановиться на

351
00:15:23,679 --> 00:15:27,439
значения счетчиков, которые крошечны по сравнению

352
00:15:25,440 --> 00:15:29,279
с их строками и содержимым, тогда они находятся

353
00:15:27,440 --> 00:15:30,880
в столбцах, которые могут быть хорошими, потому

354
00:15:29,278 --> 00:15:32,399
что это может быть то, что вы хотите сделать

355
00:15:30,879 --> 00:15:34,879
найти  вещи, которые действительно важны и

356
00:15:32,399 --> 00:15:36,879
необычны, к сожалению, с языковыми данными

357
00:15:34,879 --> 00:15:39,278
мы должны следить за тем, чтобы они могли быть

358
00:15:39,278 --> 00:15:43,600
и, наконец, tfidf строго наказывает

359
00:15:41,600 --> 00:15:45,600
слова, которые  не появляется во многих документах, он

360
00:15:43,600 --> 00:15:47,199
ведет себя странно для плотных матриц, которые

361
00:15:47,198 --> 00:15:50,559
матрицы, с которыми мы работаем, поэтому вы

362
00:15:49,039 --> 00:15:52,480
можете с осторожностью использовать эту

363
00:15:54,799 --> 00:15:58,399
наконец, некоторые фрагменты кода, которые я  просто

364
00:15:56,240 --> 00:16:00,159
демонстрируя, что наш модуль vsm в

365
00:15:58,399 --> 00:16:02,559
репозитории курса позволяет очень легко

366
00:16:00,159 --> 00:16:03,919
выполнять эти схемы повторного взвешивания, многие

367
00:16:02,559 --> 00:16:05,758
из тех, о которых мы говорили, и

368
00:16:07,440 --> 00:16:11,839
метода сравнения векторов, который вы, возможно, помните  что

369
00:16:09,519 --> 00:16:14,078
я посмотрел на соседей плохого в этой

370
00:16:11,839 --> 00:16:15,600
матрице yelp 5, и это действительно не выглядело

371
00:16:14,078 --> 00:16:17,439
хорошо, это не выглядит особенно

372
00:16:17,440 --> 00:16:22,560
когда я беру эти базовые подсчеты

373
00:16:19,759 --> 00:16:24,000
и просто корректирую их с помощью положительного PMI, я

374
00:16:22,559 --> 00:16:26,078
начинаю видеть что-то, что выглядит довольно

375
00:16:24,000 --> 00:16:27,519
семантически связны, и я думаю, что мы

376
00:16:26,078 --> 00:16:29,278
начинаем видеть перспективу этих

377
00:16:27,519 --> 00:16:30,799
методов, и это действительно только

378
00:16:29,278 --> 00:16:32,480
начало с точки зрения появления

379
00:16:30,799 --> 00:16:37,399
семантически связной и интересной

380
00:16:32,480 --> 00:16:37,399
информации.  из этих основных подсчетов

