1
00:00:04,799 --> 00:00:06,319
привет всем, добро пожаловать на наш последний

2
00:00:06,319 --> 00:00:08,160
скринкаст в нашем подразделении по распределенным

3
00:00:08,160 --> 00:00:10,000
представлениям слов, нашей темой

4
00:00:10,000 --> 00:00:12,000
будет получение статических представлений

5
00:00:12,000 --> 00:00:14,320
из контекстуальных моделей, что может показаться

6
00:00:14,320 --> 00:00:15,920
ужасно специфичным, но, как вы увидите, я

7
00:00:15,920 --> 00:00:17,520
думаю, что это может действительно расширить

8
00:00:17,520 --> 00:00:19,119
ваши возможности, когда вы работаете над  ваша исходная

9
00:00:19,119 --> 00:00:21,039
система для задания и

10
00:00:21,039 --> 00:00:23,760
связанного с ней выпекания,

11
00:00:23,760 --> 00:00:25,359
поэтому давайте углубимся в вопрос, который у вас на

12
00:00:25,359 --> 00:00:27,680
уме, может быть, как я могу использовать bert или

13
00:00:27,680 --> 00:00:29,920
связанные модели, такие как roberta или excel net

14
00:00:29,920 --> 00:00:32,238
или electro, в контексте получения

15
00:00:32,238 --> 00:00:34,559
хороших статических представлений слов, которые у вас,

16
00:00:34,559 --> 00:00:36,079
вероятно, есть  слышал об этих моделях

17
00:00:36,079 --> 00:00:37,920
и слышал, что они поднимают все лодки,

18
00:00:37,920 --> 00:00:39,280
и вопрос в том, как вы можете

19
00:00:39,280 --> 00:00:42,000
воспользоваться этими преимуществами,

20
00:00:42,000 --> 00:00:43,440
но здесь есть противоречие, мы

21
00:00:43,440 --> 00:00:45,760
разрабатываем статические представления, но

22
00:00:45,760 --> 00:00:47,760
эти модели, такие как bert,

23
00:00:47,760 --> 00:00:49,600
предназначены для предоставления контекстных

24
00:00:49,600 --> 00:00:51,760
представлений слов  и я вернусь

25
00:00:51,760 --> 00:00:53,360
к тому, что это значит, через секунду, но

26
00:00:53,360 --> 00:00:55,600
это центральное противоречие между статическим

27
00:00:55,600 --> 00:00:57,920
и контекстуальным, поэтому  вопрос в том,

28
00:00:57,920 --> 00:00:59,840
есть ли хорошие методы для получения статических

29
00:00:59,840 --> 00:01:02,239
представлений из контекстных,

30
00:01:02,239 --> 00:01:04,559
которые предлагают эти модели,

31
00:01:04,559 --> 00:01:06,640
и ответ от bamasan, но все же

32
00:01:06,640 --> 00:01:08,640
да, есть эффективные методы для

33
00:01:08,640 --> 00:01:10,159
этого, и именно эти методы

34
00:01:10,159 --> 00:01:12,400
будут в центре внимания этого скринкаста, я

35
00:01:12,400 --> 00:01:13,920
действительно хочу  сделать две вещи, хотя для

36
00:01:13,920 --> 00:01:15,840
этой лекции я хотел бы немного ознакомиться

37
00:01:15,840 --> 00:01:18,240
с высокоуровневым

38
00:01:18,240 --> 00:01:20,560
обзором моделей, таких как bert, мы собираемся

39
00:01:20,560 --> 00:01:22,240
позже в этом квартале

40
00:01:22,240 --> 00:01:24,479
более подробно рассмотреть, как эти модели работают, так

41
00:01:24,479 --> 00:01:25,920
что сейчас мы  мы просто собираемся рассматривать их

42
00:01:25,920 --> 00:01:27,840
как своего рода черные ящики, точно так же, как вы

43
00:01:27,840 --> 00:01:29,840
можете найти представление слова в перчатке

44
00:01:29,840 --> 00:01:31,520
, просто получить это

45
00:01:31,520 --> 00:01:33,600
представление и использовать его, так что здесь мы также

46
00:01:33,600 --> 00:01:35,680
можем думать об этих моделях как об устройствах для

47
00:01:35,680 --> 00:01:37,840
ввода последовательностей и возврата.

48
00:01:37,840 --> 00:01:39,680
много-много репрезентаций, которые мы

49
00:01:39,680 --> 00:01:42,000
могли бы использовать, и позже в этом квартале мы

50
00:01:42,000 --> 00:01:43,360
придем к более глубокому пониманию того,

51
00:01:43,360 --> 00:01:45,520
откуда именно берутся эти репрезентации,

52
00:01:45,520 --> 00:01:47,360


53
00:01:47,360 --> 00:01:48,799
и в дополнение, конечно, я хочу дать

54
00:01:48,799 --> 00:01:50,240
вам обзор этих захватывающих

55
00:01:50,240 --> 00:01:52,320
методов от bomasani вообще в

56
00:01:52,320 --> 00:01:54,479
надежде, что они будут полезны для вас при

57
00:01:54,479 --> 00:01:57,840
разработке вашей исходной системы,

58
00:01:57,840 --> 00:01:59,439
поэтому давайте начнем со структуры

59
00:01:59,439 --> 00:02:00,399


60
00:02:00,399 --> 00:02:02,479
последовательностей процессов Берта Берта.

61
00:02:02,479 --> 00:02:04,719


62
00:02:04,719 --> 00:02:07,280
класс и отдельные назначенные токены,

63
00:02:07,280 --> 00:02:09,038
токен класса обычно запускает

64
00:02:09,038 --> 00:02:11,038
последовательность, а затем устанавливает конец последовательности,

65
00:02:11,038 --> 00:02:12,720
а также может использоваться внутри

66
00:02:12,720 --> 00:02:14,720
последовательностей для обозначения границ внутри

67
00:02:14,720 --> 00:02:16,720
последовательности, которую вы обрабатываете,

68
00:02:16,720 --> 00:02:18,319
но основная вещь заключается в том, что у нас

69
00:02:18,319 --> 00:02:20,080
есть это короткое предложение  день

70
00:02:20,080 --> 00:02:20,959
сломался,

71
00:02:20,959 --> 00:02:23,040
vert обрабатывает их в слой внедрения,

72
00:02:23,040 --> 00:02:25,440
а затем много дополнительных

73
00:02:25,440 --> 00:02:27,360
слоев, которые вы знаете, здесь я изобразил 4, но

74
00:02:27,360 --> 00:02:30,400
это может быть 12 или даже 24 слоя,

75
00:02:30,400 --> 00:02:32,239
что мы видим здесь, vec

76
00:02:32,239 --> 00:02:34,720
прямоугольники представляют векторы, они

77
00:02:34,720 --> 00:02:38,239
являются выходами  на каждом слое в

78
00:02:38,239 --> 00:02:41,040
сети много вычислений уходит на вычисление

79
00:02:41,040 --> 00:02:43,120
этих выходных векторных представлений на

80
00:02:43,120 --> 00:02:44,640
каждом уровне, мы собираемся отложить эти

81
00:02:44,640 --> 00:02:46,640
вычисления на данный момент, поэтому  что мы можем

82
00:02:46,640 --> 00:02:49,280
просто думать об этом как о сетке векторных

83
00:02:49,280 --> 00:02:51,280
представлений,

84
00:02:51,280 --> 00:02:52,959
вот ключевая вещь, которая делает vert

85
00:02:52,959 --> 00:02:55,440
контекстуальным для различных последовательностей, которые

86
00:02:55,440 --> 00:02:57,599
мы обрабатываем, мы получим очень разные

87
00:02:57,599 --> 00:03:00,239
представления, на самом деле

88
00:03:00,239 --> 00:03:02,159
отдельные токены,

89
00:03:02,159 --> 00:03:03,840
встречающиеся в разных последовательностях, мы

90
00:03:03,840 --> 00:03:05,680
получим очень разные представления i'  Мы

91
00:03:05,680 --> 00:03:07,280
пытались показать, что с помощью цветов

92
00:03:07,280 --> 00:03:09,760
здесь, таких как t, две последовательности

93
00:03:09,760 --> 00:03:12,159
содержат слово the и слово break,

94
00:03:12,159 --> 00:03:13,680
но в силу того факта, что они имеют

95
00:03:13,680 --> 00:03:15,280
разный окружающий материал и

96
00:03:15,280 --> 00:03:17,200
разные положения в последовательности,

97
00:03:17,200 --> 00:03:19,120
почти все представления

98
00:03:19,120 --> 00:03:20,879
будут разными.  гм,

99
00:03:20,879 --> 00:03:22,560
токены класса и набора могут иметь

100
00:03:22,560 --> 00:03:24,959
одинаковое вложение, но через все эти

101
00:03:24,959 --> 00:03:26,319
слои из-за того, как все эти

102
00:03:26,319 --> 00:03:27,840
токены будут взаимодействовать друг с

103
00:03:27,840 --> 00:03:29,920
другом, когда мы получаем представления,

104
00:03:29,920 --> 00:03:31,599
все будет по-другому, мы не

105
00:03:31,599 --> 00:03:33,360
получаем статическое представление из этих

106
00:03:33,360 --> 00:03:36,000
моделей, и я указал, что даже на

107
00:03:36,000 --> 00:03:38,159
уровне встраивания, если

108
00:03:38,159 --> 00:03:40,159
позиции слов меняются на единицу в  один и тот же токен

109
00:03:40,159 --> 00:03:42,000
будет иметь разные представления,

110
00:03:42,000 --> 00:03:44,159
причина этого в том, что этот

111
00:03:44,159 --> 00:03:46,879
слой встраивания фактически скрывает два компонента, которые

112
00:03:46,879 --> 00:03:49,200
мы делаем в самом центре этой

113
00:03:49,200 --> 00:03:51,360
модели, имеем фиксированное статическое вложение,

114
00:03:51,360 --> 00:03:53,040
где мы можем искать отдельные

115
00:03:53,040 --> 00:03:54,159
последовательности слов,

116
00:03:54,159 --> 00:03:55,599
но для этого я'  мы назвали

117
00:03:55,599 --> 00:03:57,280
слоем внедрения, статическое

118
00:03:57,280 --> 00:03:58,959
представление которого комбинируется с

119
00:03:58,959 --> 00:04:00,720
отдельным позиционным кодированием из

120
00:04:00,720 --> 00:04:02,720
отдельного пространства внедрения, и это

121
00:04:02,720 --> 00:04:04,480
обеспечивает то, что я назвал здесь слоем внедрения,

122
00:04:04,480 --> 00:04:06,319
и это означает, что даже на

123
00:04:06,319 --> 00:04:09,120
этом первом уровне, поскольку, например,

124
00:04:09,120 --> 00:04:10,560
происходит в разных точках в

125
00:04:10,560 --> 00:04:12,239
последовательность, которую он будет получать разные

126
00:04:12,239 --> 00:04:14,000
представления даже в пространстве встраивания,

127
00:04:14,000 --> 00:04:15,120


128
00:04:15,120 --> 00:04:16,720
и оттуда, конечно, по мере того, как мы путешествуем

129
00:04:16,720 --> 00:04:18,798
по этим слоям, мы ожидаем, что еще больше

130
00:04:18,798 --> 00:04:20,000
вещей изменится в

131
00:04:20,000 --> 00:04:22,880
представлениях.

132
00:04:23,360 --> 00:04:26,240
Второй важный предварительный этап -

133
00:04:26,240 --> 00:04:28,000
уделить некоторое внимание тому, как bert и

134
00:04:28,000 --> 00:04:30,720
подобные ему модели токенизируют  последовательности,

135
00:04:30,720 --> 00:04:32,320
и здесь я дал вам немного кода

136
00:04:32,320 --> 00:04:33,840
в надежде, что вы сможете попрактиковаться и

137
00:04:33,840 --> 00:04:35,520
почувствовать, как  или как ведут себя эти токенизаторы.

138
00:04:35,520 --> 00:04:37,440
Я использую

139
00:04:37,440 --> 00:04:39,360
библиотеку обнимающих лиц. Я загрузил

140
00:04:39,360 --> 00:04:41,919
токенизатор отрыжки, и я загружаю его из

141
00:04:41,919 --> 00:04:43,680
предварительно обученной модели

142
00:04:43,680 --> 00:04:45,040
в ячейке 3. Вы можете видеть, что я

143
00:04:45,040 --> 00:04:46,639
вызвал токенизированную функцию в предложении,

144
00:04:46,639 --> 00:04:48,720
это не так.  Это не слишком удивительно, и в результате

145
00:04:48,720 --> 00:04:50,800
получается довольно нормальная последовательность

146
00:04:50,800 --> 00:04:53,120
токенов, вы видите, что некоторые знаки препинания были

147
00:04:53,120 --> 00:04:54,960
отделены, но вы также видите много

148
00:04:54,960 --> 00:04:57,600
слов, когда переходите к ячейке 4, хотя

149
00:04:57,600 --> 00:05:00,000
для последовательности в кодеме это немного

150
00:05:00,000 --> 00:05:02,080
удивительно.  Кодированное слово во входных данных

151
00:05:02,080 --> 00:05:04,320
было разбито на два токена подслова

152
00:05:04,320 --> 00:05:05,360


153
00:05:05,360 --> 00:05:07,759
en, а затем код с этими граничными

154
00:05:07,759 --> 00:05:10,400
маркерами на нем bert разбил это

155
00:05:10,400 --> 00:05:13,520
на две последовательности подслов, и если я

156
00:05:13,520 --> 00:05:15,600
ввожу последовательность, которая имеет действительно

157
00:05:15,600 --> 00:05:17,600
незнакомый набор

158
00:05:17,600 --> 00:05:19,520
токенов в  это сильно

159
00:05:19,520 --> 00:05:21,280
сломает часть этой последовательности, как вы

160
00:05:21,280 --> 00:05:22,880
можете видеть в пятой ячейке для ввода

161
00:05:22,880 --> 00:05:25,199
snuffleupagus, где появилось много этих

162
00:05:25,199 --> 00:05:27,120
частей, это

163
00:05:27,120 --> 00:05:29,280
важная часть, почему у

164
00:05:29,280 --> 00:05:31,360
bert такой маленький словарный запас.  только около

165
00:05:31,360 --> 00:05:33,759
30 000 слов по сравнению с 400

166
00:05:33,759 --> 00:05:36,240
000 слов, которые находятся в перчаточном пространстве,

167
00:05:36,240 --> 00:05:37,919
причина, по которой это может сойти с рук, заключается в

168
00:05:37,919 --> 00:05:39,680
том, что он много разбивает

169
00:05:39,680 --> 00:05:42,320
слова на токены подслов

170
00:05:42,320 --> 00:05:43,759
и, конечно, потому что модель

171
00:05:43,759 --> 00:05:46,320
контекстуальна, мы  ожидать, что,

172
00:05:46,320 --> 00:05:48,400
например, когда он столкнется с кодом здесь

173
00:05:48,400 --> 00:05:51,199
в контексте en на каком-то концептуальном

174
00:05:51,199 --> 00:05:52,960
уровне, мать распознает, что его

175
00:05:52,960 --> 00:05:54,960
процесс кодирует слово, даже если в основе этого

176
00:05:54,960 --> 00:05:58,639
было два токена,

177
00:05:59,039 --> 00:06:00,639
давайте немного конкретизируем это, взглянув

178
00:06:00,639 --> 00:06:02,080
на полный интерфейс для  Имея дело с

179
00:06:02,080 --> 00:06:03,759
этими моделями, я снова пользуюсь

180
00:06:03,759 --> 00:06:05,759
преимуществами обнимающего лица. Я собираюсь загрузить

181
00:06:05,759 --> 00:06:08,000
модель отрыжки и токенизатор отрыжки.

182
00:06:08,000 --> 00:06:09,520
Важно, чтобы они использовали те же

183
00:06:09,520 --> 00:06:11,280
предварительно обученные веса, которые обнимающее

184
00:06:11,280 --> 00:06:13,440
лицо загрузит для вас из Интернета

185
00:06:13,440 --> 00:06:14,880
,  связаны, и я настроил

186
00:06:14,880 --> 00:06:16,840
токенизатор и модель, если я вызову

187
00:06:16,840 --> 00:06:19,199
tokenizer.encode для последовательности, это

188
00:06:19,199 --> 00:06:21,440
вернет мне список индексов, и эти

189
00:06:21,440 --> 00:06:23,039
индексы будут использоваться в качестве поиска, чтобы

190
00:06:23,039 --> 00:06:24,720
начать процесс компиляции  Используя

191
00:06:24,720 --> 00:06:27,440
всю эту последовательность в ячейке 6, я фактически

192
00:06:27,440 --> 00:06:29,199
использую модель для получения этой сетки

193
00:06:29,199 --> 00:06:31,600
представлений, обнимающих лица, что дает нам

194
00:06:31,600 --> 00:06:33,440
объект, который имеет множество атрибутов,

195
00:06:33,440 --> 00:06:34,800
если я вызываю

196
00:06:34,800 --> 00:06:36,880
выходные скрытые состояния, равные истине, когда я

197
00:06:36,880 --> 00:06:39,199
использую модель здесь, тогда я могу назвать точку

198
00:06:39,199 --> 00:06:41,280
скрытой  состояний и получить ту полную сетку

199
00:06:41,280 --> 00:06:43,440
представлений, которую я показывал вам ранее,

200
00:06:43,440 --> 00:06:45,360
так что это последовательность из 13 слоев,

201
00:06:45,360 --> 00:06:47,840
которая представляет собой один слой внедрения плюс

202
00:06:47,840 --> 00:06:49,680
12 дополнительных слоев,

203
00:06:49,680 --> 00:06:51,440
и если я наберу один из таких, как первый

204
00:06:51,440 --> 00:06:53,360
слой, который будет встраиванием, которое вы можете

205
00:06:53,360 --> 00:06:56,880
увидеть  что его форма один на пять на 768,

206
00:06:56,880 --> 00:06:59,520
это пакет из одного примера, он имеет

207
00:06:59,520 --> 00:07:01,759
пять токенов, три из которых мы видим

208
00:07:01,759 --> 00:07:04,479
здесь, плюс токены класса и набора, и

209
00:07:04,479 --> 00:07:06,080
каждый из этих токенов в

210
00:07:06,080 --> 00:07:08,000
слое внедрения представлен

211
00:07:08,000 --> 00:07:10,800
вектором размера  768,

212
00:07:10,800 --> 00:07:12,319
и это остается согласованным во

213
00:07:12,319 --> 00:07:13,919
всех слоях модели, поэтому, если я перейду

214
00:07:13,919 --> 00:07:15,759
к конечным выходным состояниям, я снова просто

215
00:07:15,759 --> 00:07:18,319
проиндексирую точечные скрытые состояния, здесь

216
00:07:18,319 --> 00:07:20,000
форма одинакова, и она будет

217
00:07:20,000 --> 00:07:22,880
постоянной для всех слоев.

218
00:07:22,880 --> 00:07:24,400
слои — это предварительные этапы, и давайте

219
00:07:24,400 --> 00:07:25,840
подумаем о том, как мы могли бы получить некоторые

220
00:07:25,840 --> 00:07:27,919
статические представления. Первый

221
00:07:27,919 --> 00:07:30,160
подход, который придумали асани и все считают,

222
00:07:30,160 --> 00:07:32,080
— это то, что они

223
00:07:32,080 --> 00:07:34,000
называют деконтекстуализированным подходом.

224
00:07:34,000 --> 00:07:36,000


225
00:07:36,000 --> 00:07:38,160
они

226
00:07:38,160 --> 00:07:40,400
были последовательностями, и посмотрим, может ли Берк понять

227
00:07:40,400 --> 00:07:42,400
их смысл, поэтому мы начали бы с

228
00:07:42,400 --> 00:07:44,479
подачи слова, такого как котенок, и

229
00:07:44,479 --> 00:07:46,319
позволили бы модели разбить

230
00:07:46,319 --> 00:07:48,560
его на части подслов, а затем мы

231
00:07:48,560 --> 00:07:50,319
просто обработали бы это с моделью, которую мы

232
00:07:50,319 --> 00:07:53,120
получить полную сетку представлений

233
00:07:53,120 --> 00:07:55,120
сейчас, потому что у нас потенциально есть

234
00:07:55,120 --> 00:07:56,720
токены подслов, нам нужна некоторая охлаждающая

235
00:07:56,720 --> 00:07:58,639
функция, поэтому мы могли бы просто

236
00:07:58,639 --> 00:08:00,960
объединить, используя что-то вроде среднего, чтобы получить

237
00:08:00,960 --> 00:08:03,759
фиксированное статическое представление измерения

238
00:08:03,759 --> 00:08:05,280
768

239
00:08:05,280 --> 00:08:07,280
для этого отдельного слова,

240
00:08:07,280 --> 00:08:08,639
и, конечно, мы  не нужно использовать

241
00:08:08,639 --> 00:08:10,319
последний слой, мы могли бы использовать нижние

242
00:08:10,319 --> 00:08:12,319
слои, и нам не нужно использовать среднее значение в

243
00:08:12,319 --> 00:08:14,000
качестве функции заливки, вы можете рассмотреть

244
00:08:14,000 --> 00:08:16,560
что-то вроде max o  r min или даже last,

245
00:08:16,560 --> 00:08:18,319
который просто проигнорировал бы все

246
00:08:18,319 --> 00:08:20,400
векторные представления, кроме

247
00:08:20,400 --> 00:08:23,280
одного, соответствующего

248
00:08:23,280 --> 00:08:25,520
токену последнего подслова,

249
00:08:25,520 --> 00:08:27,520
это действительно просто, это

250
00:08:27,520 --> 00:08:29,759
потенциально неестественно, хотя птица - это

251
00:08:29,759 --> 00:08:31,840
контекстуальная модель, она была обучена на полных

252
00:08:31,840 --> 00:08:33,679
последовательностях, и особенно если мы оставим

253
00:08:33,679 --> 00:08:35,679
токенов class и sep, которые мы могли бы

254
00:08:35,679 --> 00:08:38,000
вводить в последовательности, которые bert

255
00:08:38,000 --> 00:08:40,240
действительно никогда раньше не видел, и, таким образом, может

256
00:08:40,240 --> 00:08:42,080
быть неизвестно, как он будет вести себя с

257
00:08:42,080 --> 00:08:44,640
этими необычными входными данными, тем не менее, хотя

258
00:08:44,640 --> 00:08:46,399
мы могли бы повторить этот процесс для всех

259
00:08:46,399 --> 00:08:48,720
слов в нашем словаре и вывести

260
00:08:48,720 --> 00:08:50,720
статическое пространство встраивания, и, возможно, у него есть

261
00:08:50,720 --> 00:08:52,160
некоторые перспективы,

262
00:08:52,160 --> 00:08:54,480
однако, чтобы устранить эту потенциальную

263
00:08:54,480 --> 00:08:56,720
неестественность и потенциально получить больше

264
00:08:56,720 --> 00:08:59,120
преимуществ от достоинств, которыми

265
00:08:59,120 --> 00:09:01,680
обладают модели burt и родственные модели, вообще

266
00:09:01,680 --> 00:09:04,480
учитывать также агрегированный подход, поэтому

267
00:09:04,480 --> 00:09:06,399
в этом подходе вы обрабатываете множество

268
00:09:06,399 --> 00:09:08,399
примеров корпуса, которые  содержать ваше целевое

269
00:09:08,399 --> 00:09:10,480
слово здесь у меня есть своего рода

270
00:09:10,480 --> 00:09:13,440
проблеск корпуса, наше целевое слово,

271
00:09:13,440 --> 00:09:15,120
конечно же, котенок  мы разрешаем разбивать его

272
00:09:15,120 --> 00:09:17,600
на токены подслов, полные последовательности

273
00:09:17,600 --> 00:09:19,279
в этих примерах также будут разбиты

274
00:09:19,279 --> 00:09:21,279
на токены подслов,

275
00:09:21,279 --> 00:09:22,399
но важно то, что наше

276
00:09:22,399 --> 00:09:24,480
целевое слово может иметь токены подслов, которые

277
00:09:24,480 --> 00:09:26,959
мы объединяем, как мы это делали раньше для

278
00:09:26,959 --> 00:09:29,120
контекстуализированный подход, и мы также

279
00:09:29,120 --> 00:09:30,640
собираемся объединить все

280
00:09:30,640 --> 00:09:32,720
различные контекстные примеры, которые мы

281
00:09:32,720 --> 00:09:33,920
обработали,

282
00:09:33,920 --> 00:09:36,160
и результатом этого должно быть

283
00:09:36,160 --> 00:09:38,240
множество естественных входных данных для модели, но

284
00:09:38,240 --> 00:09:39,680
в конце мы получаем статическое

285
00:09:39,680 --> 00:09:41,519
представление, которое является своего рода

286
00:09:41,519 --> 00:09:43,680
среднее значение по всем примерам, которые

287
00:09:43,680 --> 00:09:45,040
мы обработали,

288
00:09:45,040 --> 00:09:46,720
это кажется очень естественным, он использует

289
00:09:46,720 --> 00:09:49,040
преимущество того, в чем какая птица лучше всего, я

290
00:09:49,040 --> 00:09:50,720
должен предупредить вас, что это требует больших

291
00:09:50,720 --> 00:09:52,320
вычислительных ресурсов, мы

292
00:09:52,320 --> 00:09:54,480
собираемся обработать много примеров,

293
00:09:54,480 --> 00:09:56,480
ресурсы, потому что

294
00:09:56,480 --> 00:09:58,880
он развивает действительно большие представления,

295
00:09:58,880 --> 00:10:00,240
как мы видели,

296
00:10:00,240 --> 00:10:02,160
но, возможно, это того стоит

297
00:10:02,160 --> 00:10:04,079
сейчас, bumasonia предложит множество

298
00:10:04,079 --> 00:10:05,760
результатов, которые помогут нам понять эти

299
00:10:05,760 --> 00:10:08,079
подходы и то, как они работают.  rform, позвольте

300
00:10:08,079 --> 00:10:09,600
мне дать вам представление о них в качестве своего

301
00:10:09,600 --> 00:10:11,519
рода резюме, так что у нас есть

302
00:10:11,519 --> 00:10:14,800
результаты для набора данных simverb 3500,

303
00:10:14,800 --> 00:10:16,560
набора данных сходства слов, который очень

304
00:10:16,560 --> 00:10:17,839
похож на те, с которыми вы будете

305
00:10:17,839 --> 00:10:19,600
работать на  Домашнее задание и выпечка,

306
00:10:19,600 --> 00:10:20,320


307
00:10:20,320 --> 00:10:22,320
наша метрика - это мгновенная корреляция, и чем

308
00:10:22,320 --> 00:10:24,880
выше, тем лучше, это вдоль оси Y

309
00:10:24,880 --> 00:10:27,040
и вдоль оси X. У меня есть слой

310
00:10:27,040 --> 00:10:29,120
в модели, который мы вводим,

311
00:10:29,120 --> 00:10:30,399
и тогда, конечно, мы должны следить за тем

312
00:10:30,399 --> 00:10:32,399
, что у нас есть  две функции объединения f

313
00:10:32,399 --> 00:10:35,440
и gf — это объединение подслов, а g — это

314
00:10:35,440 --> 00:10:37,360
объединение контекста для моделей, в которых оно есть,

315
00:10:37,360 --> 00:10:38,760
и оно декомпозировано для

316
00:10:38,760 --> 00:10:41,279
деконтекстуализированного подхода,

317
00:10:41,279 --> 00:10:43,279
теперь у нас есть очень четкий результат по

318
00:10:43,279 --> 00:10:44,880
этим результатам, и я думаю, что по всем

319
00:10:44,880 --> 00:10:47,200
результатам в нижних слоях бумаги

320
00:10:47,200 --> 00:10:48,320
лучше

321
00:10:48,320 --> 00:10:49,920
нижние слои дают нам хорошее

322
00:10:49,920 --> 00:10:51,839
представление отдельных

323
00:10:51,839 --> 00:10:53,760
слов с высокой точностью, поскольку мы продвигаемся выше в модели,

324
00:10:53,760 --> 00:10:55,839
мы, кажется, теряем большую часть этого различения на уровне слов,

325
00:10:55,839 --> 00:10:58,560


326
00:10:58,560 --> 00:11:01,200
кроме того, ваш лучший выбор - сделать

327
00:11:01,200 --> 00:11:03,600
среднее объединение для контекста

328
00:11:03,600 --> 00:11:05,760
a  и объединение поддеревьев, кажется, имеет меньшее значение,

329
00:11:05,760 --> 00:11:07,760
все эти строки здесь все

330
00:11:07,760 --> 00:11:10,160
для модели объединения контекстов со средним значением в

331
00:11:10,160 --> 00:11:12,560
качестве вашей функции объединения контекстов.

332
00:11:12,560 --> 00:11:14,240


333
00:11:14,240 --> 00:11:16,640


334
00:11:16,640 --> 00:11:18,320


335
00:11:18,320 --> 00:11:20,160
этот результат, и я

336
00:11:20,160 --> 00:11:21,600
думаю, что он согласуется со всеми

337
00:11:21,600 --> 00:11:23,680
результатами в статье, но

338
00:11:23,680 --> 00:11:24,480


339
00:11:24,480 --> 00:11:26,160
общий вывод здесь заключается в том, что, как и

340
00:11:26,160 --> 00:11:28,160
ожидалось, агрегированный подход

341
00:11:28,160 --> 00:11:29,680
лучше, чем деконтекстуализированный

342
00:11:29,680 --> 00:11:30,640
подход,

343
00:11:30,640 --> 00:11:31,920
однако, если у вас нет

344
00:11:31,920 --> 00:11:34,480
вычислительного бюджета для этого, тогда означает

345
00:11:34,480 --> 00:11:36,560
объединение в деконтекстуализированном  подход

346
00:11:36,560 --> 00:11:38,880
выглядит действительно конкурентоспособным, что не так

347
00:11:38,880 --> 00:11:40,480
очевидно на этом графике, но если вы посмотрите

348
00:11:40,480 --> 00:11:42,480
на все результаты в статье, я

349
00:11:42,480 --> 00:11:44,320
думаю, что это довольно четкий вывод, так

350
00:11:44,320 --> 00:11:46,240
что это был бы хороший выбор, и

351
00:11:46,240 --> 00:11:48,880
ясно одно, что простой подход

352
00:11:48,880 --> 00:11:50,880
лучше, чем некоторые виды  пул контекста,

353
00:11:50,880 --> 00:11:52,399
когда вы выбираете неправильную

354
00:11:52,399 --> 00:11:55,040
функцию пула контекста, такую как min или max, не 

355
00:11:55,040 --> 00:11:57,040
мотря на все усилия, которые были затрачены на эт 

356
00:11:57,040 --> 00:11:59,279
т набор r  результаты, а также эти,

357
00:11:59,279 --> 00:12:01,360
они все здесь запутались

358
00:12:01,360 --> 00:12:03,760
с деконтекстуализированным подходом,

359
00:12:03,760 --> 00:12:04,480
но

360
00:12:04,480 --> 00:12:06,800
означают, что в качестве функции объединения есть

361
00:12:06,800 --> 00:12:08,639
действительно выдающийся выбор, как вы можете

362
00:12:08,639 --> 00:12:11,839
видеть из этих результатов.

