1
00:00:04,719 --> 00:00:08,480
привет всем, добро пожаловать на самый первый

2
00:00:06,639 --> 00:00:09,919
скринкаст самого первого раздела нашего

3
00:00:09,919 --> 00:00:14,000
распределенных представлениях слов или

4
00:00:15,359 --> 00:00:18,239
расскажу о некоторых целях высокого уровня, которые у нас есть.

5
00:00:19,600 --> 00:00:23,199
гипотезы не только для этого модуля, но

6
00:00:21,439 --> 00:00:26,000
и гипотезы, которые будут с нами на

7
00:00:26,800 --> 00:00:30,640
то, что я изобразил на слайде здесь, является

8
00:00:28,559 --> 00:00:32,879
нашей отправной точкой как концептуально, так и

9
00:00:32,880 --> 00:00:37,120
фрагмент  очень большая матрица совпадения слово за словом

10
00:00:37,119 --> 00:00:40,640
здесь у вас большой словарный запас

11
00:00:38,640 --> 00:00:42,558
слов, первые несколько являются смайликами, по

12
00:00:40,640 --> 00:00:44,719
крайней мере, слова, похожие на объекты

13
00:00:42,558 --> 00:00:46,238
точно такой же словарь повторяется

14
00:00:48,000 --> 00:00:52,159
количество раз  что каждое слово строки появилось

15
00:00:49,679 --> 00:00:53,679
с каждым словом столбца в очень большом

16
00:00:53,679 --> 00:00:56,960
я думаю, что большая идея, к которой вы хотите

17
00:00:55,439 --> 00:00:58,640
начать привыкать, заключается в том, что

18
00:00:56,960 --> 00:01:00,320
может быть среднее значение  скрыто в таких

19
00:01:00,320 --> 00:01:03,679
, для простых смертных не очевидно, что мы

20
00:01:02,159 --> 00:01:05,759
могли бы извлечь что-либо о значении

21
00:01:03,679 --> 00:01:07,438
из такого абстрактного пространства, но мы

22
00:01:05,760 --> 00:01:09,359
будем видеть снова и снова, что на

23
00:01:07,438 --> 00:01:13,559
самом деле это очень мощная основа для

24
00:01:14,400 --> 00:01:19,200
чтобы начать  развивая интуицию, давайте проведем

25
00:01:16,799 --> 00:01:22,000
небольшой мысленный эксперимент, поэтому представьте, что

26
00:01:19,200 --> 00:01:23,840
я даю вам небольшой словарь слов

27
00:01:22,000 --> 00:01:25,599
каждое из которых помечено как отрицательное

28
00:01:23,840 --> 00:01:26,799
или положительное в смысле анализа настроений

29
00:01:26,799 --> 00:01:30,159
который может быть полезным, но

30
00:01:28,640 --> 00:01:32,960
я назвал его безнадежным.  сценарий обучения

31
00:01:30,159 --> 00:01:35,359
потому что, если я дам вам четыре новых

32
00:01:32,959 --> 00:01:37,919
анонимных слова, чтобы делать прогнозы на

33
00:01:37,920 --> 00:01:42,000
совершенно бесполезно для прогнозирования, на самом деле у

34
00:01:42,000 --> 00:01:46,078
о том, что эти анонимные слова должны

35
00:01:46,078 --> 00:01:50,239
отличие от ситуации  в котором

36
00:01:48,239 --> 00:01:51,280
я даю вам этот лексикон ярлыков, но вдобавок

37
00:01:51,280 --> 00:01:55,759
я даю вам количество раз, которое каждое

38
00:01:53,040 --> 00:01:58,240
слово лексикона встречается в некотором большом

39
00:01:55,759 --> 00:01:59,680
тексте  корпус с двумя словами превосходный

40
00:01:59,680 --> 00:02:04,320
я думаю, с этой информацией с этими

41
00:02:01,759 --> 00:02:05,920
столбцами из пословной матрицы вы

42
00:02:04,319 --> 00:02:08,478
можете видеть, что у вас есть большая

43
00:02:05,920 --> 00:02:11,120
предсказательная сила, на самом деле очень простой

44
00:02:08,479 --> 00:02:12,879
классификатор или даже правило принятия решений

45
00:02:11,120 --> 00:02:15,360
сможет работать очень хорошо  при предсказании

46
00:02:12,878 --> 00:02:17,359
этих ярлыков, если слово со встречается

47
00:02:15,360 --> 00:02:18,400
чаще с ужасным, чем с превосходным, назовите

48
00:02:18,400 --> 00:02:22,800
если слово со встречается с превосходным

49
00:02:20,239 --> 00:02:24,878
чаще, чем с ужасным, назовите его положительным

50
00:02:22,800 --> 00:02:27,120
, это хорошая прогностическая модель, и теперь

51
00:02:24,878 --> 00:02:29,120
если я дам вам четыре новых анонимных слова

52
00:02:27,120 --> 00:02:30,800
и в в в  Кроме того, вам разрешено

53
00:02:30,800 --> 00:02:34,879
о их совпадении в отношении отличного и

54
00:02:32,800 --> 00:02:36,239
ужасного, тогда ваше же правило

55
00:02:34,878 --> 00:02:38,479
сможет делать действительно хорошие прогнозы

56
00:02:36,239 --> 00:02:40,080
об этих новых анонимных словах

57
00:02:40,080 --> 00:02:44,080
к очень многообещающему  сценарий обучения, и

58
00:02:42,239 --> 00:02:46,719
это всего лишь проблеск того, как мы могли бы

59
00:02:44,080 --> 00:02:48,879
извлечь скрытую информацию о значении

60
00:02:46,719 --> 00:02:50,719
из этих шаблонов совместного появления, а

61
00:02:48,878 --> 00:02:52,159
теперь просто воспроизвести ее вперед и  Я думаю, что

62
00:02:50,719 --> 00:02:53,439
модели векторного пространства, которые мы будем

63
00:02:52,159 --> 00:02:55,280
строить, будут иметь не только два

64
00:02:57,360 --> 00:03:03,200
сказать, сколько информации мы найдем

65
00:02:59,199 --> 00:03:04,719
скрытой в таком многомерном пространстве

66
00:03:03,199 --> 00:03:06,238
так что это подводит меня к этим высоко-

67
00:03:04,719 --> 00:03:08,000
цели уровня здесь сначала мы хотим начать

68
00:03:06,239 --> 00:03:10,800
думать о том, как эти векторы могут

69
00:03:08,000 --> 00:03:12,639
кодировать значения лингвистических единиц

70
00:03:10,800 --> 00:03:14,640
привыкнуть к той идее, которую я только что

71
00:03:14,639 --> 00:03:18,559
это основополагающие концепции, которые

72
00:03:16,800 --> 00:03:20,879
мы будем обсуждать не только для нашего

73
00:03:18,560 --> 00:03:22,479
модуля по моделям векторного пространства  которые

74
00:03:20,878 --> 00:03:24,000
также называются вложениями в современном

75
00:03:24,000 --> 00:03:27,680
основополагающие концепции для всех

76
00:03:25,840 --> 00:03:29,920
более сложных моделей глубокого обучения

77
00:03:27,680 --> 00:03:30,959
которые мы будем обсуждать позже в этом

78
00:03:30,959 --> 00:03:35,280
и, конечно, я действительно надеюсь, что

79
00:03:32,560 --> 00:03:37,039
этот материал будет полезен для  вы

80
00:03:35,280 --> 00:03:38,959
выполняете задания, которые вы выполняете

81
00:03:38,959 --> 00:03:43,519
проектной работы, которую вы выполняете во второй

82
00:03:45,759 --> 00:03:49,439
с литературой я был бы небрежен в

83
00:03:47,519 --> 00:03:50,959
лекции, подобной этой, если бы я не процитировал первое

84
00:03:50,959 --> 00:03:56,319
компании, которую оно составляет

85
00:03:54,318 --> 00:03:58,238
это проблеск номиналистической позиции, которая

86
00:03:56,318 --> 00:04:00,238
впервые заняла о том, как проводить лингвистический

87
00:03:58,239 --> 00:04:02,959
анализ.  на самом деле он говорит, что мы

88
00:04:00,239 --> 00:04:04,400
должны доверять информации о распределении

89
00:04:04,400 --> 00:04:08,239
Зелич Харрис, лингвист, работавший

90
00:04:06,318 --> 00:04:10,798
примерно в то же время, имеет еще более

91
00:04:08,239 --> 00:04:12,799
четкое изложение этой гипотезы, Харрис сказал, что

92
00:04:10,799 --> 00:04:14,959
утверждения о распределении могут охватывать

93
00:04:14,959 --> 00:04:18,399
требуя поддержки со стороны других типов

94
00:04:18,399 --> 00:04:22,478
daily cares на самом деле только достоверная

95
00:04:20,639 --> 00:04:24,560
информация об использовании, я думаю, что нам не нужно быть

96
00:04:22,478 --> 00:04:26,399
столь экстремальными в нашей позиции, но мы

97
00:04:24,560 --> 00:04:28,879
безусловно, можем согласиться с Харрисом, думая

98
00:04:26,399 --> 00:04:30,560
, что в этих дистрибутивных заявлениях может быть много скрытого 

99
00:04:30,560 --> 00:04:33,759
который находится в шаблонах совместного появления.

100
00:04:33,759 --> 00:04:37,439
мы могли бы также процитировать wittgenstein

101
00:04:35,839 --> 00:04:38,799
смысл слова это его использование в

102
00:04:42,319 --> 00:04:45,360
соприкосновения для него с Фертом и Харрисом, я не

103
00:04:45,360 --> 00:04:48,800
но, наконец, вот своего рода прямая

104
00:04:48,800 --> 00:04:52,319
операционализация нашей гипотезы высокого уровня, это

105
00:04:50,720 --> 00:04:54,400
из одного из рекомендуемых чтений

106
00:04:52,319 --> 00:04:56,639
адвокатом и пентелом, и они говорят  если

107
00:04:54,399 --> 00:04:58,239
единицы текста имеют схожие векторы в

108
00:04:56,639 --> 00:05:00,560
матрице частотности текста, такой как

109
00:04:58,240 --> 00:05:02,879
матрица совпадения, которую я показывал вам ранее

110
00:05:00,560 --> 00:05:04,639
то они, как правило, имеют схожие значения

111
00:05:02,879 --> 00:05:06,079
если мы верим в эту гипотезу, то мы как

112
00:05:04,639 --> 00:05:08,160
бы имеем лицензию на построение этих

113
00:05:06,079 --> 00:05:10,240
матриц совпадения, а затем  делать

114
00:05:08,160 --> 00:05:12,320
выводы, по крайней мере, о схожести

115
00:05:10,240 --> 00:05:15,199
значения на основе тех объектов, которые

116
00:05:15,680 --> 00:05:20,160
чтобы закончить здесь под заголовком

117
00:05:17,360 --> 00:05:21,439
великая сила очень много вариантов дизайна

118
00:05:20,160 --> 00:05:22,800
я думаю, что одна из трудностей

119
00:05:21,439 --> 00:05:24,879
в работе в этом пространстве заключается в том, что

120
00:05:24,879 --> 00:05:28,399
первый выбор, который вам нужно сделать

121
00:05:35,439 --> 00:05:38,719
разные способы, которыми вы можете построить

122
00:05:37,120 --> 00:05:40,639
свои строки и свои столбцы в одной из

123
00:05:40,639 --> 00:05:44,560
действительно фундаментально, вы будете фиксировать очень

124
00:05:42,639 --> 00:05:47,439
разные факты распределения в зависимости

125
00:05:44,560 --> 00:05:48,879
от того, какой дизайн матрицы вы выберете

126
00:05:48,879 --> 00:05:52,000
выбор, который вам нужно сделать, потому что при

127
00:05:50,399 --> 00:05:54,159
построении этой матрицы вы будете делать

128
00:05:52,000 --> 00:05:56,079
много вариантов того, как токенизировать

129
00:05:56,079 --> 00:06:00,159
помечать ли части речи тегами для дальнейшего

130
00:05:57,839 --> 00:06:01,918
различия, анализируя выбор функций

131
00:06:00,160 --> 00:06:03,280
и т. д. и т. д.  вы также должны

132
00:06:01,918 --> 00:06:05,359
решить, как вы собираетесь группировать свои

133
00:06:03,279 --> 00:06:07,519
тексты, будет ли ваше представление о совместном появлении

134
00:06:07,519 --> 00:06:12,560
документе, или, возможно, документах, сгруппированных по

135
00:06:10,160 --> 00:06:14,080
дате, автору или контексту дискурса

136
00:06:12,560 --> 00:06:16,160
все эти вещи дадут вам очень много

137
00:06:14,079 --> 00:06:18,240
различные представления о том, что значит

138
00:06:16,160 --> 00:06:20,560
сосуществовать, и это будет учитываться при разработке вашей

139
00:06:20,560 --> 00:06:23,918
после того, как вы сделали все те трудные

140
00:06:22,240 --> 00:06:25,918
выборы, которые вы теперь, вероятно

141
00:06:23,918 --> 00:06:28,318
захотите сделать.  возьмите свою матрицу счета и, как

142
00:06:25,918 --> 00:06:30,399
мы скажем, измените ее вес, то есть

143
00:06:28,319 --> 00:06:33,039
отрегулируйте значения, растягивая и изгибая

144
00:06:30,399 --> 00:06:34,959
пространство, чтобы найти больше скрытой

145
00:06:33,038 --> 00:06:36,719
информации о значении, мы собираемся

146
00:06:34,959 --> 00:06:39,038
поговорить о множестве методов для этого

147
00:06:36,720 --> 00:06:40,960
, а затем  вы, кроме того, можете

148
00:06:39,038 --> 00:06:42,800
захотеть сделать какое-то уменьшение размерности

149
00:06:40,959 --> 00:06:44,879
что является шагом, который вы могли бы предпринять

150
00:06:42,800 --> 00:06:47,038
чтобы уловить понятия совместного возникновения еще более высокого 

151
00:06:47,038 --> 00:06:51,279
простых совпадений, которые вы видите

152
00:06:48,720 --> 00:06:53,520
очевидными в исходной матрице, это

153
00:06:51,279 --> 00:06:54,879
мощный шаг есть  много вариантов, которые

154
00:06:54,879 --> 00:06:58,399
а затем, наконец, каким будет ваше представление о

155
00:06:58,399 --> 00:07:01,839
применим это как метод сравнения векторов

156
00:07:01,839 --> 00:07:06,000
расстояние, косинусное расстояние, жаккардовое

157
00:07:06,000 --> 00:07:09,439
зависимости от предыдущего выбора, который

158
00:07:09,439 --> 00:07:13,439
метода сравнения векторов может иметь реальное

159
00:07:11,279 --> 00:07:15,598
влияние на то, что вы считаете похожим и

160
00:07:13,439 --> 00:07:17,598
отличающимся в вашем векторном пространстве

161
00:07:15,598 --> 00:07:19,360
так что это своего рода головокружительный набор

162
00:07:17,598 --> 00:07:22,959
вариантов, которые вы можете выбрать.  хотя, возможно, придется 

163
00:07:19,360 --> 00:07:25,759
есть проблеск надежды, поэтому

164
00:07:22,959 --> 00:07:28,000
такие модели, как GloVe и vec2word, 

165
00:07:25,759 --> 00:07:30,319
предлагают комплексные решения, по крайней мере

166
00:07:28,000 --> 00:07:31,360
для этапов взвешивания и уменьшения конструкции

167
00:07:31,360 --> 00:07:34,720
поэтому они скажут вам, например, если вы

168
00:07:32,879 --> 00:07:36,639
используете GloVe, что она должна быть  слово за

169
00:07:34,720 --> 00:07:39,039
словом, а затем GloVe будет одновременно

170
00:07:36,639 --> 00:07:40,639
выполнять эти два шага и, кроме того

171
00:07:39,038 --> 00:07:42,240
для этих методов, поскольку они, как правило

172
00:07:40,639 --> 00:07:44,400
предоставляют векторы, которые довольно хорошо

173
00:07:42,240 --> 00:07:46,960
масштабируются с точки зрения их индивидуальных

174
00:07:44,399 --> 00:07:49,439
значений, выбор векторного сравнения

175
00:07:46,959 --> 00:07:51,038
может не иметь большого значения, поэтому такие модели, как

176
00:07:49,439 --> 00:07:53,598
GloVe и workvec  являются настоящим шагом

177
00:07:51,038 --> 00:07:54,878
вперед с точки зрения укрощения этого пространства

178
00:07:54,879 --> 00:07:59,439
и мы можем добавить, что более поздние

179
00:07:56,720 --> 00:08:01,360
модели контекстного встраивания диктуют еще

180
00:08:01,360 --> 00:08:06,080
возможно, вплоть до того, как вы токенизируете, и поэтому

181
00:08:03,598 --> 00:08:08,560
их можно рассматривать как еще более

182
00:08:06,079 --> 00:08:10,800
унифицированные решения для большого количества

183
00:08:08,560 --> 00:08:13,360
вариантов дизайна, которые у вас есть здесь, так

184
00:08:10,800 --> 00:08:16,000
что это своего рода концептуально настоящий

185
00:08:16,000 --> 00:08:19,120
базовые модели построены  Полученные из

186
00:08:17,839 --> 00:08:21,359
простых вещей, которые у меня есть в этих

187
00:08:21,360 --> 00:08:25,520
более продвинутыми моделями, но

188
00:08:25,519 --> 00:08:31,719
вам, вероятно, придется обсудить

189
00:08:27,519 --> 00:08:31,719
, чтобы обнаружить эмпирически.

