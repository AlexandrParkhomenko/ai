1
00:00:04,720 --> 00:00:06,480
приветствую всех, это пятая часть

2
00:00:06,480 --> 00:00:08,160
нашей серии статей о методах и метриках, мы

3
00:00:08,160 --> 00:00:09,760
собираемся поговорить об основных

4
00:00:09,760 --> 00:00:12,480
выбранных темах оценки моделей в

5
00:00:12,480 --> 00:00:14,719
нашей области,

6
00:00:14,719 --> 00:00:16,880
вот наш обзор, я хотел бы начать с

7
00:00:16,880 --> 00:00:19,119
обсуждения базовых показателей и их роли

8
00:00:19,119 --> 00:00:21,279
в экспериментальных сравнениях, а

9
00:00:21,279 --> 00:00:22,720
затем  мы обсудим

10
00:00:22,720 --> 00:00:24,480
оптимизацию гиперпараметров как процесса, так и

11
00:00:24,480 --> 00:00:27,680
мотивации, а также компромиссы, на которые

12
00:00:27,680 --> 00:00:29,359
вам, возможно, придется пойти из-за

13
00:00:29,359 --> 00:00:31,519
ограничений ресурсов и других ограничений,

14
00:00:31,519 --> 00:00:33,040
мы кратко коснемся сравнения классификаторов,

15
00:00:33,040 --> 00:00:34,960
которое является темой, которую мы рассмотрели

16
00:00:34,960 --> 00:00:37,200
в модуле анализа настроений

17
00:00:37,200 --> 00:00:38,800
и  затем мы закончим двумя темами

18
00:00:38,800 --> 00:00:40,640
, которые действительно актуальны для

19
00:00:40,640 --> 00:00:42,320
моделей глубокого обучения, а именно

20
00:00:42,320 --> 00:00:44,399
оценкой моделей без конвергенции

21
00:00:44,399 --> 00:00:45,920
и ролью инициализации случайных параметров

22
00:00:45,920 --> 00:00:48,559
в формировании экспериментальных

23
00:00:48,559 --> 00:00:51,039
результатов,

24
00:00:51,199 --> 00:00:53,920
поэтому давайте начнем с исходных данных.

25
00:00:53,920 --> 00:00:56,000
Основное понимание здесь заключается в том, что в

26
00:00:56,000 --> 00:00:58,079
нашей области оценочные числа могут  никогда

27
00:00:58,079 --> 00:01:00,239
не понять должным образом по отдельности,

28
00:01:00,239 --> 00:01:02,960
давайте рассмотрим два крайних случая, предположим, что

29
00:01:02,960 --> 00:01:05,438
ваша система  получает 0,95 f1, тогда вы можете

30
00:01:05,438 --> 00:01:07,040
почувствовать, что можете объявить о победе в

31
00:01:07,040 --> 00:01:08,080
этот момент,

32
00:01:08,080 --> 00:01:10,560
но для людей,

33
00:01:10,560 --> 00:01:12,960
которые потребляют ваши результаты, будет естественно спросить,

34
00:01:12,960 --> 00:01:15,200
слишком ли проста задача, действительно ли это

35
00:01:15,200 --> 00:01:17,600
достижение, которое вы получили 0,95 или

36
00:01:17,600 --> 00:01:19,680
даже более простые системы

37
00:01:19,680 --> 00:01:21,680
достигли чего-то подобного

38
00:01:21,680 --> 00:01:23,200
на другом конце спектра предположим, что

39
00:01:23,200 --> 00:01:26,240
ваша система получает 0,6 f1 вы можете подумать,

40
00:01:26,240 --> 00:01:27,520
что это означает, что вы не получили

41
00:01:27,520 --> 00:01:29,360
поддержки, но сначала мы должны задать два вопроса,

42
00:01:29,360 --> 00:01:31,759
что люди получают в качестве своего рода

43
00:01:31,759 --> 00:01:34,079
верхней границы, а также что  получить случайный

44
00:01:34,079 --> 00:01:36,479
классификатор, и если ваш 0,6 действительно

45
00:01:36,479 --> 00:01:38,720
отличается от случайного классификатора, а

46
00:01:38,720 --> 00:01:40,880
человеческая производительность довольно низкая, мы

47
00:01:40,880 --> 00:01:43,200
можем увидеть, что эта точка шесть f1

48
00:01:43,200 --> 00:01:45,840
является реальным достижением,

49
00:01:45,840 --> 00:01:47,439
которое как бы показывает вам, что басовые партии

50
00:01:47,439 --> 00:01:49,360
просто необходимы для сильных экспериментов

51
00:01:49,360 --> 00:01:51,759
в  нашей области, поэтому определение басовых партий для

52
00:01:51,759 --> 00:01:53,439
вас не должно быть второстепенным, а

53
00:01:53,439 --> 00:01:55,280
скорее центральным в том, как вы определяете свою

54
00:01:55,280 --> 00:01:57,600
общую гипотезу.

55
00:01:57,600 --> 00:01:59,200
Базовые линии действительно важны для

56
00:01:59,200 --> 00:02:01,360
построения убедительного аргумента и  d они могут

57
00:02:01,360 --> 00:02:03,759
быть использованы для устранения конкретных

58
00:02:03,759 --> 00:02:05,680
аспектов проблемы, которую вы решаете, и

59
00:02:05,680 --> 00:02:08,318
конкретных достоинств предлагаемой вами системы,

60
00:02:08,318 --> 00:02:10,000
что на самом деле сводится к тому, что

61
00:02:10,000 --> 00:02:11,599
с самого начала вы могли бы сказать,

62
00:02:11,599 --> 00:02:14,400
например, вот базовая модель, вот

63
00:02:14,400 --> 00:02:16,959
моя предложенная модификация  это и то,

64
00:02:16,959 --> 00:02:18,879
как мы проверяем вашу гипотезу, заключается в

65
00:02:18,879 --> 00:02:20,800
сравнении производительности этих двух

66
00:02:20,800 --> 00:02:23,040
систем в этом контексте, вы можете видеть,

67
00:02:23,040 --> 00:02:25,200
что базовый уровень играет решающую роль

68
00:02:25,200 --> 00:02:27,120
в количественном определении степени, в которой ваша

69
00:02:27,120 --> 00:02:29,840
гипотеза верна, и, следовательно, тщательная

70
00:02:29,840 --> 00:02:32,080
модель для сравнений на этом уровне

71
00:02:32,080 --> 00:02:34,400
будут иметь решающее значение для

72
00:02:34,400 --> 00:02:38,000
успешной реализации гипотезы,

73
00:02:38,000 --> 00:02:39,760
когда вы сомневаетесь, вы можете включить случайные

74
00:02:39,760 --> 00:02:42,000
исходные данные в свою таблицу результатов, их

75
00:02:42,000 --> 00:02:43,840
очень легко настроить, и они могут пролить свет на

76
00:02:43,840 --> 00:02:45,360
то, на что это похоже, если мы просто делаем

77
00:02:45,360 --> 00:02:47,599
случайные прогнозы  я показываю

78
00:02:47,599 --> 00:02:49,280
вам, что scikit-learn вы

79
00:02:49,280 --> 00:02:50,800
рассмотрели в этом вопросе, у них есть два

80
00:02:50,800 --> 00:02:53,200
класса, фиктивный классификатор и фиктивный

81
00:02:53,200 --> 00:02:55,440
регрессор, каждый с широким диапазоном

82
00:02:55,440 --> 00:02:57,120
различных способов, которыми они могут делать случайные

83
00:02:57,120 --> 00:02:59,200
предположения на основе данных,

84
00:02:59,200 --> 00:03:00,720
и я бы посоветовал вам использовать эти

85
00:03:00,720 --> 00:03:02,560
классы, потому что это облегчит

86
00:03:02,560 --> 00:03:04,959
вам вписывание случайных базовых показателей в

87
00:03:04,959 --> 00:03:07,280
ваш общий экспериментальный конвейер, что

88
00:03:07,280 --> 00:03:08,720
уменьшит объем кода, который у вас

89
00:03:08,720 --> 00:03:10,560
есть.  написать и, возможно, избежать ошибок,

90
00:03:10,560 --> 00:03:12,480
которые могут возникнуть в результате самостоятельной реализации этих

91
00:03:12,480 --> 00:03:14,560
базовых планов, поэтому настоятельно

92
00:03:14,560 --> 00:03:16,400
рекомендуется

93
00:03:16,400 --> 00:03:17,440
и как бы на другом конце

94
00:03:17,440 --> 00:03:19,280
спектра вы можете подумать о своей задаче

95
00:03:19,280 --> 00:03:21,040
, существуют ли конкретные

96
00:03:21,040 --> 00:03:23,360
базовые планы задач, которые вы должны учитывать,

97
00:03:23,360 --> 00:03:24,720
потому что они могут раскрыть что-то

98
00:03:24,720 --> 00:03:26,720
о  набор данных, или проблема, или то,

99
00:03:26,720 --> 00:03:28,560
как люди моделируют

100
00:03:28,560 --> 00:03:31,120
проблему, мы видели пример этого раньше,

101
00:03:31,120 --> 00:03:32,480
в контексте вывода на естественном языке

102
00:03:32,480 --> 00:03:35,120
мы видели, что только

103
00:03:35,120 --> 00:03:37,840
базовые линии гипотезы имели тенденцию делать прогнозы,

104
00:03:37,840 --> 00:03:41,360
которые были такими же хорошими, как 0,65 до 0,70 f1,

105
00:03:41,360 --> 00:03:43,519
что  значительно лучше, чем

106
00:03:43,519 --> 00:03:45,360
базовый случайный шанс, который был

107
00:03:45,360 --> 00:03:47,519
бы около 0,33,

108
00:03:47,519 --> 00:03:49,680
и это показывает нам, что  en мы

109
00:03:49,680 --> 00:03:51,280
измеряем производительность, мы действительно должны

110
00:03:51,280 --> 00:03:53,040
думать о выигрышах сверх этой

111
00:03:53,040 --> 00:03:55,599
гипотезы, только базовые сравнения

112
00:03:55,599 --> 00:03:57,040
со случайным случаем будут

113
00:03:57,040 --> 00:03:59,040
значительно преувеличивать степень, в которой мы

114
00:03:59,040 --> 00:04:01,760
добились значимого прогресса в этих

115
00:04:01,760 --> 00:04:03,920
наборах данных.

116
00:04:03,920 --> 00:04:06,080
история задачи закрытия истории

117
00:04:06,080 --> 00:04:07,920
несколько похожа здесь  задача состоит в том, чтобы

118
00:04:07,920 --> 00:04:09,599
различать связный и

119
00:04:09,599 --> 00:04:11,920
бессвязный конец истории,

120
00:04:11,920 --> 00:04:13,599
и люди заметили, что системы, которые

121
00:04:13,599 --> 00:04:15,599
рассматривали только варианты концовки, были в

122
00:04:15,599 --> 00:04:17,199
состоянии работать очень хорошо.

123
00:04:17,199 --> 00:04:19,839


124
00:04:19,839 --> 00:04:22,560


125
00:04:22,560 --> 00:04:24,240
это

126
00:04:24,240 --> 00:04:26,160
решение о классификации

127
00:04:26,160 --> 00:04:27,759
снова, вы можете подумать, что это показывает,

128
00:04:27,759 --> 00:04:29,360
что существует фундаментальная проблема

129
00:04:29,360 --> 00:04:31,120
с набором данных, и это может быть правдой, но

130
00:04:31,120 --> 00:04:33,440
другая точка зрения просто заключается в том, что когда

131
00:04:33,440 --> 00:04:35,440
мы делаем сравнения и думаем о производительности модели,

132
00:04:35,440 --> 00:04:37,520
это должно

133
00:04:37,520 --> 00:04:42,120
быть базовым, а не случайным  угадывая

134
00:04:43,199 --> 00:04:45,120
хорошо, давайте поговорим об оптимизации гиперпараметров,

135
00:04:45,120 --> 00:04:47,199
мы обсуждали это в нашем

136
00:04:47,199 --> 00:04:49,120
модуль по анализу настроений, и мы рассмотрим

137
00:04:49,120 --> 00:04:50,960
некоторые обоснования, позвольте мне

138
00:04:50,960 --> 00:04:53,600
быстро повторить полный случай, поскольку эта

139
00:04:53,600 --> 00:04:55,680
первая оптимизация гиперпараметров может

140
00:04:55,680 --> 00:04:57,440
иметь решающее значение для получения наилучшей

141
00:04:57,440 --> 00:04:59,199
версии вашей модели, что

142
00:04:59,199 --> 00:05:01,360
может быть вашей основной целью,

143
00:05:01,360 --> 00:05:03,199
вероятно, для любой современной модели, которая

144
00:05:03,199 --> 00:05:04,880
вы смотрите на то, что это широкий

145
00:05:04,880 --> 00:05:07,039
диапазон гиперпараметров, и мы можем

146
00:05:07,039 --> 00:05:08,720
знать, что разные их настройки

147
00:05:08,720 --> 00:05:10,960
приводят к очень разным результатам, поэтому

148
00:05:10,960 --> 00:05:12,479
в ваших интересах выполнить

149
00:05:12,479 --> 00:05:14,240
поиск гиперпараметров, чтобы представить вашу модель

150
00:05:14,240 --> 00:05:16,240
в самом лучшем свете,

151
00:05:16,240 --> 00:05:18,080
мы также  подробно рассказал о том, что

152
00:05:18,080 --> 00:05:20,720
это решающий шаг в проведении объективных

153
00:05:20,720 --> 00:05:23,280
сравнений между моделями. Очень

154
00:05:23,280 --> 00:05:24,560
важно, чтобы при проведении

155
00:05:24,560 --> 00:05:27,280
сравнения вы не представляли одну модель в

156
00:05:27,280 --> 00:05:29,199
лучшем свете с ее лучшими

157
00:05:29,199 --> 00:05:31,280
настройками гиперпараметров, а все остальные модели

158
00:05:31,280 --> 00:05:33,199
были случайными.  выбраны или даже

159
00:05:33,199 --> 00:05:35,600
плохо выбраны настройки гиперпараметров,

160
00:05:35,600 --> 00:05:37,039
потому что это привело бы к несправедливым

161
00:05:37,039 --> 00:05:39,440
сравнениям и преувеличению различий

162
00:05:39,440 --> 00:05:41,840
между  модели, что мы хотим сделать, это

163
00:05:41,840 --> 00:05:43,919
сравнить все модели с их наилучшими

164
00:05:43,919 --> 00:05:45,919
возможными настройками гиперпараметров, и

165
00:05:45,919 --> 00:05:48,160
это подразумевает выполнение обширного поиска, чтобы

166
00:05:48,160 --> 00:05:50,080
найти эти настройки,

167
00:05:50,080 --> 00:05:51,600
и третья мотивация, которую вы можете иметь,

168
00:05:51,600 --> 00:05:53,360
- это просто понять стабильность

169
00:05:53,360 --> 00:05:55,440
вашей архитектуры, которую мы могли бы захотеть знать

170
00:05:55,440 --> 00:05:57,440
для  некоторое большое пространство гиперпараметров,

171
00:05:57,440 --> 00:05:59,199
какие из них действительно важны для конечной

172
00:05:59,199 --> 00:06:01,360
производительности, возможно, какие из них приводят к

173
00:06:01,360 --> 00:06:03,600
действительно вырожденным решениям, и какое

174
00:06:03,600 --> 00:06:05,840
пространство гиперпараметров в целом работает

175
00:06:05,840 --> 00:06:07,600
лучше всего, так что у нас есть

176
00:06:07,600 --> 00:06:09,280
не просто один набор параметров,

177
00:06:09,280 --> 00:06:11,520
которые хорошо работают, но, возможно, реальные идеи

178
00:06:11,520 --> 00:06:13,919
в общих настройках моделей

179
00:06:13,919 --> 00:06:15,919
, которые действительно хороши,

180
00:06:15,919 --> 00:06:17,520
есть еще одно правило, которое я должен

181
00:06:17,520 --> 00:06:18,880
повторить здесь,

182
00:06:18,880 --> 00:06:21,120
вся настройка гиперпараметров должна выполняться

183
00:06:21,120 --> 00:06:23,759
только для данных обучения и разработки,

184
00:06:23,759 --> 00:06:26,080
в нашей области грех делать какую-либо

185
00:06:26,080 --> 00:06:28,560
настройку гиперпериметра на  тестовый набор,

186
00:06:28,560 --> 00:06:30,400
вся эта настройка должна происходить за

187
00:06:30,400 --> 00:06:32,720
пределами тестового набора, а затем, как обычно, вы получаете

188
00:06:32,720 --> 00:06:34,560
один прогон тестового набора с выбранным вами

189
00:06:34,560 --> 00:06:36,560
p  параметры, и это число, которое

190
00:06:36,560 --> 00:06:38,880
вы сообщаете как производительность на тестовых

191
00:06:38,880 --> 00:06:39,840
данных

192
00:06:39,840 --> 00:06:42,000
, это единственный способ, которым мы действительно

193
00:06:42,000 --> 00:06:44,240
можем посмотреть, как эти системы ведут себя

194
00:06:44,240 --> 00:06:46,240
на совершенно невидимых данных, так что это

195
00:06:46,240 --> 00:06:47,840
действительно важно для

196
00:06:47,840 --> 00:06:51,520
понимания прогресса в нашей области

197
00:06:51,680 --> 00:06:54,319
теперь оптимизация гиперпараметров  как вы

198
00:06:54,319 --> 00:06:56,880
можете себе представить, это может быть очень дорого, давайте

199
00:06:56,880 --> 00:06:58,319
рассмотрим это, а затем поговорим о некоторых

200
00:06:58,319 --> 00:07:00,960
компромиссах. Идеал для

201
00:07:00,960 --> 00:07:02,560
оптимизации гиперпараметров заключается в том, что вы

202
00:07:02,560 --> 00:07:04,720
определяете большой набор значений для своей

203
00:07:04,720 --> 00:07:05,919
модели,

204
00:07:05,919 --> 00:07:07,199
вы создаете список всех

205
00:07:07,199 --> 00:07:08,880
комбинаций этих значений, это

206
00:07:08,880 --> 00:07:11,039
будет  перекрестное произведение всех

207
00:07:11,039 --> 00:07:13,680
значений функций, которые вы определили,

208
00:07:13,680 --> 00:07:15,440
затем для каждой из настроек вы должны

209
00:07:15,440 --> 00:07:17,199
перекрестно проверить ее на доступных

210
00:07:17,199 --> 00:07:18,639
обучающих данных,

211
00:07:18,639 --> 00:07:20,319
а затем выбрать настройки, которые

212
00:07:20,319 --> 00:07:22,960
показали наилучшие результаты на третьем шаге, тренируются на

213
00:07:22,960 --> 00:07:24,639
всех обучающих данных, используемых с использованием  эти

214
00:07:24,639 --> 00:07:26,800
настройки, а затем, наконец,

215
00:07:26,800 --> 00:07:29,120
оцените тестовые данные, которые здесь идеальны, и

216
00:07:29,120 --> 00:07:30,319
давайте просто подумаем о том, как это на

217
00:07:30,319 --> 00:07:32,319
самом деле будет работать su  Предположим для нашего

218
00:07:32,319 --> 00:07:34,080
примера, что у нас есть один гиперпараметр

219
00:07:34,080 --> 00:07:36,479
с пятью значениями,

220
00:07:36,479 --> 00:07:38,240
и у нас есть второй гиперпараметр с

221
00:07:38,240 --> 00:07:40,560
десятью значениями, тогда перекрестное произведение

222
00:07:40,560 --> 00:07:42,319
приведет к тому, что у нас будет 50 общих

223
00:07:42,319 --> 00:07:44,800
настроек для этих гиперпараметров.

224
00:07:44,800 --> 00:07:46,639
Предположим, мы добавим третий гиперпараметр

225
00:07:46,639 --> 00:07:48,639
с двумя значениями.  теперь количество

226
00:07:48,639 --> 00:07:50,720
настроек, которые у нас есть, подскочило до

227
00:07:50,720 --> 00:07:52,560
100.

228
00:07:52,560 --> 00:07:53,840
если мы хотим выполнить пятикратную

229
00:07:53,840 --> 00:07:55,919
перекрестную проверку для выбора этих оптимальных

230
00:07:55,919 --> 00:07:57,840
параметров, то мы говорим о

231
00:07:57,840 --> 00:08:01,199
проведении 500 различных экспериментов,

232
00:08:01,199 --> 00:08:03,039
что, вероятно, прекрасно, если вы имеете

233
00:08:03,039 --> 00:08:05,039
дело с  небольшая линейная модель с

234
00:08:05,039 --> 00:08:07,440
некоторыми функциями, созданными вручную, но если вы

235
00:08:07,440 --> 00:08:09,599
устанавливаете большую модель на основе трансформатора,

236
00:08:09,599 --> 00:08:11,360
где каждый эксперимент занимает до

237
00:08:11,360 --> 00:08:12,400
одного дня,

238
00:08:12,400 --> 00:08:14,080
это будет непомерно

239
00:08:14,080 --> 00:08:16,080
дорого с точки зрения времени или вычислительных

240
00:08:16,080 --> 00:08:18,319
ресурсов, и это заставит нас

241
00:08:18,319 --> 00:08:20,240
сделать некоторые  компромиссы это

242
00:08:20,240 --> 00:08:23,039
суть здесь приведенная выше картина что

243
00:08:23,039 --> 00:08:26,080
идеал несостоятелен как свод законов для

244
00:08:26,080 --> 00:08:28,400
нашего научного сообщества если мы

245
00:08:28,400 --> 00:08:30,879
его примем то сложный режим  Специалисты, обученные работе с большими

246
00:08:30,879 --> 00:08:33,200
наборами данных, в конечном итоге окажутся в немилости, и

247
00:08:33,200 --> 00:08:35,279
только очень богатые смогут

248
00:08:35,279 --> 00:08:37,360
участвовать, и просто чтобы дать вам

249
00:08:37,360 --> 00:08:39,200
представление о том, насколько дорого это может

250
00:08:39,200 --> 00:08:40,880
обойтись, вот цитата из этой прекрасной

251
00:08:40,880 --> 00:08:43,120
статьи о машинном обучении НЛП для

252
00:08:43,120 --> 00:08:45,200
здравоохранения в их  В дополнительных

253
00:08:45,200 --> 00:08:47,600
материалах они сообщают, что производительность

254
00:08:47,600 --> 00:08:49,600
на всех вышеперечисленных нейронных сетях была

255
00:08:49,600 --> 00:08:52,399
настроена автоматически с помощью google

256
00:08:52,399 --> 00:08:56,880
vizier с общим объемом более 200 000 часов GPU

257
00:08:56,880 --> 00:08:58,560
для меня, как для частного лица, что может

258
00:08:58,560 --> 00:09:01,040
легко стоить полмиллиона долларов

259
00:09:01,040 --> 00:09:03,839
только для процесса гиперпараметра.

260
00:09:03,839 --> 00:09:05,839
оптимизация, и вот что я имею в виду,

261
00:09:05,839 --> 00:09:07,440
говоря, что это принципиально

262
00:09:07,440 --> 00:09:09,120
неприемлемо для нас,

263
00:09:09,120 --> 00:09:11,040
поэтому что мы должны делать в ответ, нам

264
00:09:11,040 --> 00:09:13,839
нужен прагматичный ответ здесь, вот несколько

265
00:09:13,839 --> 00:09:15,519
шагов, которые вы предпринимаете, чтобы облегчить

266
00:09:15,519 --> 00:09:17,360
проблему в том, что я рассматриваю как своего

267
00:09:17,360 --> 00:09:20,080
рода убывающую привлекательность  поэтому,

268
00:09:20,080 --> 00:09:22,080
начав с лучшего варианта, вы можете

269
00:09:22,080 --> 00:09:24,560
сделать случайную выборку и, возможно, управляемую

270
00:09:24,560 --> 00:09:27,040
выборку, чтобы исследовать большое пространство

271
00:09:27,040 --> 00:09:28,640
гиперпараметров на  фиксированный

272
00:09:28,640 --> 00:09:31,839
вычислительный бюджет,

273
00:09:31,920 --> 00:09:33,760
вы можете выполнять поиск, основываясь всего на нескольких

274
00:09:33,760 --> 00:09:35,519
эпохах обучения, вместо того, чтобы

275
00:09:35,519 --> 00:09:37,760
позволять вашей модели работать в течение многих

276
00:09:37,760 --> 00:09:40,080
эпох, что может занять целый день, вы

277
00:09:40,080 --> 00:09:41,920
можете выбрать гиперпараметры на основе

278
00:09:41,920 --> 00:09:45,519
одной или двух эпох, исходя из предположения, что

279
00:09:45,519 --> 00:09:47,120
настройки, которые  хороши в начале,

280
00:09:47,120 --> 00:09:49,120
останутся хорошими настройки, которые плохи в

281
00:09:49,120 --> 00:09:51,200
начале, останутся плохими, это эвристическое

282
00:09:51,200 --> 00:09:53,360
предположение, но кажется разумным, вы

283
00:09:53,360 --> 00:09:54,959
могли бы подкрепить его некоторыми

284
00:09:54,959 --> 00:09:56,880
кривыми обучения и т. д., и это

285
00:09:56,880 --> 00:09:58,959
могло бы значительно сократить количество, которое у

286
00:09:58,959 --> 00:10:02,800
вас есть  потратить в этом процессе поиска,

287
00:10:02,800 --> 00:10:04,640
вы также можете искать на основе

288
00:10:04,640 --> 00:10:06,079
подмножеств данных, это будет другой

289
00:10:06,079 --> 00:10:08,800
вид компромисса, однако, поскольку многие

290
00:10:08,800 --> 00:10:10,640
гиперпараметры зависят от

291
00:10:10,640 --> 00:10:13,760
размера набора данных, я думаю, что условия регуляризации

292
00:10:13,760 --> 00:10:16,160
могут быть более рискованными, чем версия

293
00:10:16,160 --> 00:10:17,839
в два  там, где мы просто тренируемся в течение

294
00:10:17,839 --> 00:10:20,399
нескольких эпох,

295
00:10:20,399 --> 00:10:22,880
вы также можете выполнить эвристический поиск,

296
00:10:22,880 --> 00:10:24,720
возможно, определив, какие гиперпараметры

297
00:10:24,720 --> 00:10:27,040
имеют меньшее значение, а затем установите  вручную

298
00:10:27,040 --> 00:10:28,560
на основе этого эвристического поиска, а затем

299
00:10:28,560 --> 00:10:30,240
вы можете просто описать этот процесс

300
00:10:30,240 --> 00:10:32,320
в статье, которую вы знаете из нескольких

301
00:10:32,320 --> 00:10:34,720
наблюдений, вы сделали некоторые предположения о

302
00:10:34,720 --> 00:10:36,320
параметрах, которые вы могли бы исправить, и

303
00:10:36,320 --> 00:10:38,320
поэтому исследовали меньшее

304
00:10:38,320 --> 00:10:39,680
подмножество пространства, которое вам могло бы понравиться.  чтобы

305
00:10:39,680 --> 00:10:40,640
исследовать

306
00:10:40,640 --> 00:10:42,079
снова, я думаю, что если вы сделаете это, и

307
00:10:42,079 --> 00:10:44,079
вам будет ясно об этом, читатели будут

308
00:10:44,079 --> 00:10:45,920
восприимчивы, потому что мы знаем о

309
00:10:45,920 --> 00:10:47,680
затратах,

310
00:10:47,680 --> 00:10:49,200
вы также можете найти оптимальные гиперпараметры с

311
00:10:49,200 --> 00:10:51,200
помощью одного разделения ваших

312
00:10:51,200 --> 00:10:52,880
данных и использовать их для всех последующих

313
00:10:52,880 --> 00:10:54,079
разделений

314
00:10:54,079 --> 00:10:55,760
это было бы оправдано, если бы разделения

315
00:10:55,760 --> 00:10:57,120
были очень похожими, а производительность вашей модели

316
00:10:57,120 --> 00:10:59,279
очень стабильной, и это

317
00:10:59,279 --> 00:11:01,200
уменьшило бы всю ту перекрестную проверку, которая привела к тому,

318
00:11:01,200 --> 00:11:03,200
что количество экспериментов, которые

319
00:11:03,200 --> 00:11:05,360
нам пришлось провести, резко увеличилось,

320
00:11:05,360 --> 00:11:07,360


321
00:11:07,360 --> 00:11:09,040
и, наконец, вы могли бы принять другие

322
00:11:09,040 --> 00:11:10,079
варианты

323
00:11:10,079 --> 00:11:12,000
теперь скептик будет жаловаться, что эти

324
00:11:12,000 --> 00:11:14,000
результаты не переводятся в новые наборы данных,

325
00:11:14,000 --> 00:11:16,240
но это может быть единственный вариант,

326
00:11:16,240 --> 00:11:18,160
который вы просто наблюдаете, например, что

327
00:11:18,160 --> 00:11:20,399
для некоторых очень больших  модели оригинальные

328
00:11:20,399 --> 00:11:22,959
авторы используют настройки xy и z, и вы

329
00:11:22,959 --> 00:11:25,200
можете просто принять их, даже зная,

330
00:11:25,200 --> 00:11:27,920
что ваш набор данных или ваш тест могут

331
00:11:27,920 --> 00:11:30,480
потребовать других оптимальных настроек,

332
00:11:30,480 --> 00:11:32,480
это не самое лучшее, но если это

333
00:11:32,480 --> 00:11:34,480
единственное, что вы можете себе позволить,

334
00:11:34,480 --> 00:11:38,320
это, безусловно, разумно  случай, чтобы сделать,

335
00:11:38,880 --> 00:11:40,640
наконец, некоторые инструменты для

336
00:11:40,640 --> 00:11:42,800
поиска гиперпараметров, как обычно, у scikit-learn есть

337
00:11:42,800 --> 00:11:44,959
куча отличных инструментов для этого поиска по сетке

338
00:11:44,959 --> 00:11:47,040
рандомизированный поиск и половинный

339
00:11:47,040 --> 00:11:48,000
поиск по

340
00:11:48,000 --> 00:11:49,839
сетке поиск по сетке будет самым дорогим

341
00:11:49,839 --> 00:11:51,519
рандомизированный поиск наименее дорогой,

342
00:11:51,519 --> 00:11:53,040
и половинный поиск по сетке поможет вам

343
00:11:53,040 --> 00:11:55,519
своего рода стратегическая навигация по

344
00:11:55,519 --> 00:11:56,399


345
00:11:56,399 --> 00:11:58,240
пространству гиперпараметров, и если вы хотите

346
00:11:58,240 --> 00:12:00,160
пойти еще дальше в этом направлении,

347
00:12:00,160 --> 00:12:02,079
пакет оптимизации

348
00:12:02,079 --> 00:12:04,399
scikit предлагает набор инструментов для

349
00:12:04,399 --> 00:12:05,200


350
00:12:05,200 --> 00:12:07,120


351
00:12:07,120 --> 00:12:08,880
исследования пространства гиперпараметров на основе модели, основанного на производительности,

352
00:12:08,880 --> 00:12:10,480
и это может быть

353
00:12:10,480 --> 00:12:13,360
действительно очень эффективно

354
00:12:13,440 --> 00:12:14,800
хорошо, давайте кратко поговорим о

355
00:12:14,800 --> 00:12:17,360
сравнении классификаторов, мы уже рассматривали эту тему

356
00:12:17,360 --> 00:12:18,800
ранее, но я просто

357
00:12:18,800 --> 00:12:21,120
кратко напомню  ap сценарий

358
00:12:21,120 --> 00:12:22,959
таков: предположим, вы оценили две модели классификатора,

359
00:12:22,959 --> 00:12:24,959
их производительность, вероятно,

360
00:12:24,959 --> 00:12:27,360
в некоторой степени отличается в числовом выражении,

361
00:12:27,360 --> 00:12:28,959
что можно сделать, чтобы установить,

362
00:12:28,959 --> 00:12:30,800
различаются ли эти модели в каком-то

363
00:12:30,800 --> 00:12:32,240
значимом смысле,

364
00:12:32,240 --> 00:12:34,399
как мы обсуждали, я думаю, руководство

365
00:12:34,399 --> 00:12:36,000
из литературы таково:  во-первых, мы могли бы

366
00:12:36,000 --> 00:12:37,920
охватить практические различия, если бы вы просто

367
00:12:37,920 --> 00:12:40,800
заметили, что одна модель делает на 10 000 более

368
00:12:40,800 --> 00:12:42,240
важных прогнозов, чем

369
00:12:42,240 --> 00:12:44,399
другая, тогда этого может быть достаточно, чтобы

370
00:12:44,399 --> 00:12:47,200
доказать, что это лучшая модель

371
00:12:47,200 --> 00:12:49,200
для более узких различий.

372
00:12:49,200 --> 00:12:50,720


373
00:12:50,720 --> 00:12:53,519
использовать доверительные интервалы при повторных

374
00:12:53,519 --> 00:12:55,760
прогонах — это ранжированный критерий Уилкоксона со знаком Уилкоксона,

375
00:12:55,760 --> 00:12:58,000
чтобы получить единую сводную статистику о

376
00:12:58,000 --> 00:13:00,240
том, действительно ли различные прогоны

377
00:13:00,240 --> 00:13:02,000
различаются по своим средствам и

378
00:13:02,000 --> 00:13:03,680
вариантам.

379
00:13:03,680 --> 00:13:06,240


380
00:13:06,240 --> 00:13:08,639


381
00:13:08,639 --> 00:13:10,399
Уилкоксона и доверительные

382
00:13:10,399 --> 00:13:12,880
интервалы потребуют от вас проведения от 10 до

383
00:13:12,880 --> 00:13:15,279
20 различных экспериментов,

384
00:13:15,279 --> 00:13:17,200
Это будет непомерно дорого, и в

385
00:13:17,200 --> 00:13:19,519
таких ситуациях вы можете вернуться к

386
00:13:19,519 --> 00:13:21,279
тесту Макнемара, потому что это

387
00:13:21,279 --> 00:13:23,600
дешевле и, возможно, лучше, чем

388
00:13:23,600 --> 00:13:26,320
ничего, особенно в сценариях, где

389
00:13:26,320 --> 00:13:27,920
трудно сказать, есть ли

390
00:13:27,920 --> 00:13:29,440
практические различия между

391
00:13:29,440 --> 00:13:31,920
системами.

392
00:13:32,720 --> 00:13:34,720
Наконец, давайте поговорим о двух темах, которые

393
00:13:34,720 --> 00:13:36,720
кажутся особенно актуальными.  в

394
00:13:36,720 --> 00:13:38,399
контексте крупномасштабных моделей глубокого обучения,

395
00:13:38,399 --> 00:13:40,639
и первое — это оценка моделей без

396
00:13:40,639 --> 00:13:41,920
конвергенции.

397
00:13:41,920 --> 00:13:44,000
При работе с линейными моделями

398
00:13:44,000 --> 00:13:47,040
проблемы конвергенции возникают редко,

399
00:13:47,040 --> 00:13:49,440
потому что кажется, что модели быстро сходятся

400
00:13:49,440 --> 00:13:51,279
на основе любого установленного вами порога,

401
00:13:51,279 --> 00:13:53,600
а конвергенция подразумевает своего рода максимальную

402
00:13:53,600 --> 00:13:56,639
производительность.  в широком диапазоне случаев

403
00:13:56,639 --> 00:13:58,800
с нейронными сетями, однако,

404
00:13:58,800 --> 00:14:00,639
вопросы сходимости действительно занимают центральное место,

405
00:14:00,639 --> 00:14:02,880
модели редко сходятся даже на основе

406
00:14:02,880 --> 00:14:05,760
либеральных порогов, которые вы можете установить,

407
00:14:05,760 --> 00:14:07,600
они сходятся с разной скоростью между

408
00:14:07,600 --> 00:14:09,680
запусками, поэтому их трудно предсказать, и их

409
00:14:09,680 --> 00:14:12,240
производительность на тестовых данных ужасно

410
00:14:12,240 --> 00:14:15,120
в значительной степени часто в значительной степени зависит от  видеть

411
00:14:15,120 --> 00:14:17,519
различия правильно иногда модель

412
00:14:17,519 --> 00:14:20,240
с низкой конечной ошибкой оказывается отличной,

413
00:14:20,240 --> 00:14:22,240
а иногда иногда

414
00:14:22,240 --> 00:14:23,760
оказывается хуже, чем модель, которая закончилась с более

415
00:14:23,760 --> 00:14:25,680
высокой ошибкой, кто действительно знает, что

416
00:14:25,680 --> 00:14:26,720
происходит

417
00:14:26,720 --> 00:14:28,720
наш единственный выход в этих ситуациях

418
00:14:28,720 --> 00:14:30,720
- просто просто  проводите эксперименты и

419
00:14:30,720 --> 00:14:33,760
наблюдайте, что работает лучше всего,

420
00:14:33,760 --> 00:14:36,160
поэтому я думаю, что очень естественным и простым в

421
00:14:36,160 --> 00:14:37,839
реализации ответом на это, который доказывает свою

422
00:14:37,839 --> 00:14:39,600
высокую эффективность, является то, что я называю

423
00:14:39,600 --> 00:14:42,560
здесь инкрементным тестированием набора разработчиков. Это

424
00:14:42,560 --> 00:14:45,120
просто идея, что по мере обучения

425
00:14:45,120 --> 00:14:47,360
мы будем регулярно  собирать информацию

426
00:14:47,360 --> 00:14:49,440
о производительности на каком-то

427
00:14:49,440 --> 00:14:52,399
наборе разработчиков в рамках процесса обучения,

428
00:14:52,399 --> 00:14:54,720
например, на каждой сотой итерации вы

429
00:14:54,720 --> 00:14:56,800
можете делать прогнозы для этого набора разработчиков

430
00:14:56,800 --> 00:14:58,800
и сохранять эти прогнозы для какой-

431
00:14:58,800 --> 00:15:00,959
либо оценки.

432
00:15:00,959 --> 00:15:03,040


433
00:15:03,040 --> 00:15:04,959
параметр остановки,

434
00:15:04,959 --> 00:15:06,880
который позволит вам проводить эксперименты

435
00:15:06,880 --> 00:15:09,519
таким образом и удерживать то, что

436
00:15:09,519 --> 00:15:11,920
казалось лучшим с точки зрения производительности модели, а

437
00:15:11,920 --> 00:15:13,519
затем сообщать  это

438
00:15:13,519 --> 00:15:15,360
основано на критериях остановки, которые вы

439
00:15:15,360 --> 00:15:17,839
установили, и с помощью эвристической блокировки,

440
00:15:17,839 --> 00:15:19,839
которая даст вам лучшую модель за

441
00:15:19,839 --> 00:15:21,600
наименьшее количество эпох.

442
00:15:21,600 --> 00:15:23,440
Параметр ранней остановки имеет

443
00:15:23,440 --> 00:15:25,120
множество других настроек, которые вы можете

444
00:15:25,120 --> 00:15:27,600
использовать, чтобы точно контролировать, как он ведет себя,

445
00:15:27,600 --> 00:15:29,920
что может быть важно  для конкретных

446
00:15:29,920 --> 00:15:33,600
структур модели или наборов данных

447
00:15:33,839 --> 00:15:34,959
вот небольшая

448
00:15:34,959 --> 00:15:36,720
мотивация для ранней остановки, вы

449
00:15:36,720 --> 00:15:38,800
можете подумать, почему бы просто не позволить моей модели

450
00:15:38,800 --> 00:15:41,279
работать до конвергенции, если я, возможно, смогу

451
00:15:41,279 --> 00:15:44,079
в контексте этих больших и

452
00:15:44,079 --> 00:15:46,320
очень сложных процессов оптимизации,

453
00:15:46,320 --> 00:15:48,399
которые могут привести вас действительно далеко в заблуждение,

454
00:15:48,399 --> 00:15:50,720
верно  Итак, вот изображение модели глубокого

455
00:15:50,720 --> 00:15:52,399
обучения, и вы можете видеть, что ее ошибка

456
00:15:52,399 --> 00:15:54,240
очень быстро снижается на протяжении многих

457
00:15:54,240 --> 00:15:56,079
итераций, и похоже, что вы, возможно,

458
00:15:56,079 --> 00:15:58,720
захотите повторить даже до 80 эпох

459
00:15:58,720 --> 00:15:59,680
обучения,

460
00:15:59,680 --> 00:16:01,440
однако, если вы посмотрите на производительность

461
00:16:01,440 --> 00:16:03,440
этого задержанного разработчика  Вы видите, что эта

462
00:16:03,440 --> 00:16:05,440
модель на самом деле очень быстро достигла своего

463
00:16:05,440 --> 00:16:07,920
пика производительности, а затем все

464
00:16:07,920 --> 00:16:09,920
оставшееся обучение было просто обучением.

465
00:16:09,920 --> 00:16:13,120
э., тратить время или

466
00:16:13,120 --> 00:16:15,040
снижать производительность, которую вы видели в

467
00:16:15,040 --> 00:16:16,959
начале процесса, и именно поэтому,

468
00:16:16,959 --> 00:16:19,040
поскольку это наша настоящая цель здесь, вы

469
00:16:19,040 --> 00:16:21,120
можете захотеть выполнить какое-то

470
00:16:21,120 --> 00:16:25,839
тестирование на этапе разработки с ранней

471
00:16:26,000 --> 00:16:27,519
остановкой последней вещи, которую я хотел бы  сказать здесь,

472
00:16:27,519 --> 00:16:29,279
что все это может привести нас к тому, чтобы

473
00:16:29,279 --> 00:16:30,720
выйти из режима предположения, что мы

474
00:16:30,720 --> 00:16:32,720
всегда должны сообщать одно число, чтобы

475
00:16:32,720 --> 00:16:34,880
обобщить наши модели мы имеем дело с

476
00:16:34,880 --> 00:16:37,360
очень мощными моделями в пределе, который они

477
00:16:37,360 --> 00:16:39,360
могут изучить очень сложно

478
00:16:39,360 --> 00:16:41,279
вещи, и мы могли бы захотеть задать

479
00:16:41,279 --> 00:16:43,360
разные вопросы, например, как быстро

480
00:16:43,360 --> 00:16:45,120
они могут учиться и насколько эффективно и насколько

481
00:16:45,120 --> 00:16:47,600
надежно, и это может означать, что то, что

482
00:16:47,600 --> 00:16:49,600
мы действительно хотим сделать, это не сообщать

483
00:16:49,600 --> 00:16:52,320
сводные таблицы статистики, а скорее

484
00:16:52,320 --> 00:16:54,160
полные кривые обучения с доверительными

485
00:16:54,160 --> 00:16:56,320
интервалами это  это изображение из статьи, с

486
00:16:56,320 --> 00:16:58,079
которой я был связан, но я думаю,

487
00:16:58,079 --> 00:17:00,480
что будет полезно увидеть разбивку по категориям

488
00:17:00,480 --> 00:17:02,720
того, как работает модель,

489
00:17:02,720 --> 00:17:04,880
в дополнение к общему среднему,

490
00:17:04,880 --> 00:17:06,880
потому что вы можете видеть  что, хотя эта красная

491
00:17:06,880 --> 00:17:08,959
модель, возможно, намного лучше, чем

492
00:17:08,959 --> 00:17:11,520
желтая и серая в целом,

493
00:17:11,520 --> 00:17:13,119
ее сложно отличить в

494
00:17:13,119 --> 00:17:15,599
целом от этой синей модели, но для

495
00:17:15,599 --> 00:17:17,679
различных подкатегорий вы видите

496
00:17:17,679 --> 00:17:19,839
некоторые различия, тогда как для других

497
00:17:19,839 --> 00:17:20,880
вы видите, что они хорошие  из

498
00:17:20,880 --> 00:17:23,039
неразличимых это очень богатая

499
00:17:23,039 --> 00:17:25,439
картина, вы также можете видеть, что на ранних этапах

500
00:17:25,439 --> 00:17:26,959
для некоторых из этих категорий некоторые из

501
00:17:26,959 --> 00:17:28,559
этих моделей действительно дифференцированы,

502
00:17:28,559 --> 00:17:30,640
они обучаются более эффективно,

503
00:17:30,640 --> 00:17:32,400
тогда как к тому времени, когда вы исчерпали

504
00:17:32,400 --> 00:17:34,720
100 000 эпох, многие

505
00:17:34,720 --> 00:17:36,720
различия моделей исчезли.

506
00:17:36,720 --> 00:17:38,559
это своего рода богатая картина, которая

507
00:17:38,559 --> 00:17:40,080
уже дает нам представление о том, как

508
00:17:40,080 --> 00:17:42,400
разные ценности и разные цели, которые у нас

509
00:17:42,400 --> 00:17:44,880
есть, могут определять

510
00:17:44,880 --> 00:17:47,280
разные варианты выбора модели и

511
00:17:47,280 --> 00:17:49,440
способов оптимизации, и мне бы очень

512
00:17:49,440 --> 00:17:51,360
понравилось, если бы наша область попала в поле зрения.

513
00:17:51,360 --> 00:17:54,080
привычка сообщать эту очень полную картину, а

514
00:17:54,080 --> 00:17:55,840
не сводить все к

515
00:17:55,840 --> 00:17:58,320
одному числу,

516
00:17:58,320 --> 00:18:00,240
последняя тема - роль

517
00:18:00,240 --> 00:18:02,480
инициализации случайного параметра  n это своего рода

518
00:18:02,480 --> 00:18:04,400
еще один гиперпараметр, который находится

519
00:18:04,400 --> 00:18:05,840
в фоновом режиме, о котором гораздо

520
00:18:05,840 --> 00:18:07,919
сложнее думать, что в

521
00:18:07,919 --> 00:18:09,280
большинстве моделей глубокого обучения их

522
00:18:09,280 --> 00:18:12,240
параметры инициализируются случайным образом, или многие

523
00:18:12,240 --> 00:18:13,679
из этих параметров инициализируются

524
00:18:13,679 --> 00:18:14,880
случайным образом,

525
00:18:14,880 --> 00:18:16,720
это явно имеет значение для этих

526
00:18:16,720 --> 00:18:18,960
невыпуклых задач оптимизации, которые

527
00:18:18,960 --> 00:18:21,440
мы  позирует, но даже простые модели

528
00:18:21,440 --> 00:18:23,039
также могут быть затронуты, если вы имеете дело с

529
00:18:23,039 --> 00:18:25,039
очень маленькими наборами данных с очень большими

530
00:18:25,039 --> 00:18:27,440
пространствами признаков

531
00:18:27,440 --> 00:18:29,600
в этой классической статье здесь эти авторы

532
00:18:29,600 --> 00:18:30,880
просто отмечают, что разные

533
00:18:30,880 --> 00:18:32,559
инициализации для моделей нейронной последовательности

534
00:18:32,559 --> 00:18:34,240
, которые выполняли

535
00:18:34,240 --> 00:18:35,440
распознавание именованных объектов,

536
00:18:35,440 --> 00:18:37,760
привели к  статистически значимо

537
00:18:37,760 --> 00:18:39,600
отличающиеся результаты, полученные в одной и той

538
00:18:39,600 --> 00:18:42,080
же модели с другим случайным

539
00:18:42,080 --> 00:18:43,600
начальным числом, выполнялись способами, которые выглядели

540
00:18:43,600 --> 00:18:45,600
значительно по-разному в этих наборах данных,

541
00:18:45,600 --> 00:18:48,160
и ряд недавних систем

542
00:18:48,160 --> 00:18:49,320
фактически оказались

543
00:18:49,320 --> 00:18:51,280
неотличимыми с точки зрения их исходной

544
00:18:51,280 --> 00:18:52,400
производительности

545
00:18:52,400 --> 00:18:54,400
после того, как этот источник  вариация была

546
00:18:54,400 --> 00:18:56,480
учтена вот только мощная

547
00:18:56,480 --> 00:19:00,000
пример того, насколько случайное начальное число может

548
00:19:00,000 --> 00:19:02,000
повлиять на окончательную производительность в

549
00:19:02,000 --> 00:19:04,080
контексте моделей, подобных этой,

550
00:19:04,080 --> 00:19:05,520
на другом конце

551
00:19:05,520 --> 00:19:07,280
спектра вы можете увидеть катастрофический

552
00:19:07,280 --> 00:19:09,600
сбой в результате неудачной

553
00:19:09,600 --> 00:19:12,160
инициализации, некоторые настройки хороши,

554
00:19:12,160 --> 00:19:14,720
а некоторые могут быть жалкими сбоями, которые

555
00:19:14,720 --> 00:19:16,320
мы не делаем.  мы действительно не знаем заранее, что

556
00:19:16,320 --> 00:19:17,600
будет чем,

557
00:19:17,600 --> 00:19:19,039
и это означает, что мы просто должны быть

558
00:19:19,039 --> 00:19:20,320
очень внимательны к тому, как мы

559
00:19:20,320 --> 00:19:22,320
инициализируем эти системы в широком

560
00:19:22,320 --> 00:19:24,880
диапазоне настроек, и вы заметите это

561
00:19:24,880 --> 00:19:27,200
в блокноте методов оценки, который

562
00:19:27,200 --> 00:19:29,120
я  распространяемый в качестве дополнения к этой

563
00:19:29,120 --> 00:19:31,360
лекции, я подгоняю простую сеть прямой

564
00:19:31,360 --> 00:19:33,360
связи, очень маленькую, к классической

565
00:19:33,360 --> 00:19:35,440
проблеме xor, которая является одной из оригинальных

566
00:19:35,440 --> 00:19:37,760
мотивирующих проблем для использования

567
00:19:37,760 --> 00:19:40,080
моделей глубокого обучения вообще, и что вы видите, так

568
00:19:40,080 --> 00:19:41,760
это то, что она успешно работает примерно в восьми из

569
00:19:41,760 --> 00:19:44,000
десять раз, когда единственное, что

570
00:19:44,000 --> 00:19:46,720
мы меняем в этих моделях, —

571
00:19:46,720 --> 00:19:48,559
это то, как они случайным образом инициализируются,

572
00:19:48,559 --> 00:19:50,640
и это снова просто показывает вам, что это

573
00:19:50,640 --> 00:19:52,720
может сильно повлиять на

574
00:19:52,720 --> 00:19:54,799
окончательную производительность.  значение для наших систем, и,

575
00:19:54,799 --> 00:19:56,400
вероятно, нам нужно

576
00:19:56,400 --> 00:19:58,799
думать об этом как о еще одном гиперпараметре,

577
00:19:58,799 --> 00:20:01,200
который нам нужно настроить и

578
00:20:01,200 --> 00:20:05,960
оптимизировать вместе со всем остальным, что

579
00:20:08,240 --> 00:20:10,320
вам нужно.

