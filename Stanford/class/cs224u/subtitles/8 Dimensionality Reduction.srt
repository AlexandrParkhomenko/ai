1
00:00:00,000 --> 00:00:04,352


2
00:00:04,352 --> 00:00:05,810
CHRISTOPHER POTTS:
Hello, everyone.

3
00:00:05,810 --> 00:00:06,420
Welcome back.

4
00:00:06,420 --> 00:00:08,660
This is part 5 in our
series on distributed word

5
00:00:08,660 --> 00:00:09,387
representations.

6
00:00:09,387 --> 00:00:11,720
We're going to be talking
about dimensionality reduction

7
00:00:11,720 --> 00:00:12,650
techniques.

8
00:00:12,650 --> 00:00:14,330
We saw in the
previous screencast

9
00:00:14,330 --> 00:00:16,160
that reweighting
is a powerful tool

10
00:00:16,160 --> 00:00:20,557
for finding latent semantic
information in count matrices.

11
00:00:20,557 --> 00:00:22,140
We're going to push
that even further.

12
00:00:22,140 --> 00:00:24,590
The promise of dimensionality
reduction techniques

13
00:00:24,590 --> 00:00:26,480
is that they can
capture higher order

14
00:00:26,480 --> 00:00:28,220
notions of co-occurrence
corresponding

15
00:00:28,220 --> 00:00:33,500
to even deeper sorts of
semantic relatedness.

16
00:00:33,500 --> 00:00:35,750
There's a wide world of these
dimensionality reduction

17
00:00:35,750 --> 00:00:36,260
techniques.

18
00:00:36,260 --> 00:00:37,677
I've chosen three
that we're going

19
00:00:37,677 --> 00:00:40,160
to focus on as interesting
representatives of a much

20
00:00:40,160 --> 00:00:41,570
larger space.

21
00:00:41,570 --> 00:00:43,490
We'll look at latent
semantic analysis, which

22
00:00:43,490 --> 00:00:45,410
is a classic linear method.

23
00:00:45,410 --> 00:00:47,450
Then we'll talk about
autoencoders and newer,

24
00:00:47,450 --> 00:00:50,870
powerful deep learning mode for
learning reduced dimensional

25
00:00:50,870 --> 00:00:52,250
representations.

26
00:00:52,250 --> 00:00:55,580
And then finally, GloVe, which
is a simple yet very powerful

27
00:00:55,580 --> 00:00:58,040
method that, as you'll
see, has a deep connection

28
00:00:58,040 --> 00:01:00,350
to pointwise mutual information.

29
00:01:00,350 --> 00:01:02,690
And then I'm going to
close by talking briefly

30
00:01:02,690 --> 00:01:04,610
about visualization,
which is another kind

31
00:01:04,610 --> 00:01:06,380
of dimensionality
reduction technique

32
00:01:06,380 --> 00:01:10,172
that we might use for
very different purposes.

33
00:01:10,172 --> 00:01:11,630
So let's begin with
Latent Semantic

34
00:01:11,630 --> 00:01:13,370
Analysis, a classic method.

35
00:01:13,370 --> 00:01:15,950
The paper is due to
Deerwester et al, 1990.

36
00:01:15,950 --> 00:01:18,770
That's a classic paper
that really made a splash.

37
00:01:18,770 --> 00:01:19,670
It's one of the--

38
00:01:19,670 --> 00:01:22,550
LSA is now one of the oldest,
most widely used dimensionality

39
00:01:22,550 --> 00:01:25,700
reduction techniques, not only
in scientific research but also

40
00:01:25,700 --> 00:01:26,310
an industry.

41
00:01:26,310 --> 00:01:28,190
I think it was really
eye opening for people

42
00:01:28,190 --> 00:01:31,010
at the time of the
paper's appearance

43
00:01:31,010 --> 00:01:34,070
to see just how powerful
this technique could be,

44
00:01:34,070 --> 00:01:38,360
especially in contexts
involving information retrieval.

45
00:01:38,360 --> 00:01:41,630
The method is also known
as Truncated Singular Value

46
00:01:41,630 --> 00:01:42,350
Decomposition.

47
00:01:42,350 --> 00:01:44,250
And I'll explain why
that is in a second.

48
00:01:44,250 --> 00:01:46,250
The final thing I want
to say at this high level

49
00:01:46,250 --> 00:01:49,070
is just that LSA remains
a very powerful baseline,

50
00:01:49,070 --> 00:01:52,080
especially when part of a
pipeline of other reweighting

51
00:01:52,080 --> 00:01:52,970
methods.

52
00:01:52,970 --> 00:01:55,220
So it should probably be
in your results table.

53
00:01:55,220 --> 00:01:59,310
And it's often very
difficult to beat.

54
00:01:59,310 --> 00:02:01,800
Now I think we can't, in
the time allotted to us,

55
00:02:01,800 --> 00:02:04,140
cover all of the technical
details surrounding

56
00:02:04,140 --> 00:02:05,250
latent semantic analysis.

57
00:02:05,250 --> 00:02:06,750
In my experience,
this would be kind

58
00:02:06,750 --> 00:02:09,750
of the culmination of a full
course in linear algebra.

59
00:02:09,750 --> 00:02:12,240
But I do think I can convey
the guiding intuitions.

60
00:02:12,240 --> 00:02:15,205
And that will help you with
responsible use of the method.

61
00:02:15,205 --> 00:02:17,580
So let's imagine that we have
this simple two-dimensional

62
00:02:17,580 --> 00:02:18,720
vector space model.

63
00:02:18,720 --> 00:02:21,090
I've got four points,
A, B, C, and D

64
00:02:21,090 --> 00:02:23,400
arrayed out in this
two-dimensional space.

65
00:02:23,400 --> 00:02:26,160
I think we're all familiar with
fitting linear models, which

66
00:02:26,160 --> 00:02:29,160
capture the largest source
of variation in the data.

67
00:02:29,160 --> 00:02:30,688
That's this orange line here.

68
00:02:30,688 --> 00:02:32,730
And the perspective I
would encourage you to take

69
00:02:32,730 --> 00:02:36,180
is that we can think of
that linear regression model

70
00:02:36,180 --> 00:02:39,210
as performing dimensionality
reduction in that it encourages

71
00:02:39,210 --> 00:02:44,250
us to project points like B
and C down onto that line.

72
00:02:44,250 --> 00:02:46,290
And then projecting them
down onto that line,

73
00:02:46,290 --> 00:02:49,590
essentially in abstracting away
from their point of variation

74
00:02:49,590 --> 00:02:52,260
along the y-axis, we can
see the sense in which

75
00:02:52,260 --> 00:02:53,780
they are abstractly similar.

76
00:02:53,780 --> 00:02:57,720
They're close together in this
reduced dimensional space.

77
00:02:57,720 --> 00:02:59,220
Now with a linear
model, we captured

78
00:02:59,220 --> 00:03:01,920
the source of greatest variation
in this little data set.

79
00:03:01,920 --> 00:03:05,580
In the high dimensional space,
we could continue fitting lines

80
00:03:05,580 --> 00:03:07,590
to other sources of
variation in the data,

81
00:03:07,590 --> 00:03:08,990
other axes of variation.

82
00:03:08,990 --> 00:03:12,090
So here's a blue line here that
captures the next dimension.

83
00:03:12,090 --> 00:03:14,250
And we could, again,
project points like A and C

84
00:03:14,250 --> 00:03:15,210
down onto that line.

85
00:03:15,210 --> 00:03:17,070
And that would capture
the abstract sense

86
00:03:17,070 --> 00:03:19,500
in which A and C,
although very spread out

87
00:03:19,500 --> 00:03:22,230
along the x dimension,
are very close together

88
00:03:22,230 --> 00:03:23,790
along the y dimension.

89
00:03:23,790 --> 00:03:26,580
And of course, if we had more
dimensions in this vector space

90
00:03:26,580 --> 00:03:28,680
model, we could
continue to perform

91
00:03:28,680 --> 00:03:30,930
these cuts and
dimensionality reductions,

92
00:03:30,930 --> 00:03:34,590
capturing ever more abstract
notions of similarity

93
00:03:34,590 --> 00:03:36,270
along these different axes.

94
00:03:36,270 --> 00:03:38,220
And that is, in
essence, what LSA

95
00:03:38,220 --> 00:03:43,110
is going to do for us in
our really large matrices.

96
00:03:43,110 --> 00:03:44,830
The fundamental
method, as I said,

97
00:03:44,830 --> 00:03:46,630
is singular value decomposition.

98
00:03:46,630 --> 00:03:48,270
This is a theorem
from linear algebra

99
00:03:48,270 --> 00:03:51,900
that says any matrix
of dimension n by n

100
00:03:51,900 --> 00:03:55,320
can be decomposed into the
product of three matrices, T,

101
00:03:55,320 --> 00:03:58,420
S, and D, with the
dimensions given.

102
00:03:58,420 --> 00:04:00,630
Here's a more concrete example.

103
00:04:00,630 --> 00:04:03,120
Start with this matrix
of dimension 3 by 4.

104
00:04:03,120 --> 00:04:07,620
We learn the term matrix, which
is full of length normalized

105
00:04:07,620 --> 00:04:09,300
orthogonal vectors.

106
00:04:09,300 --> 00:04:13,150
We have this matrix of singular
values along the diagonal.

107
00:04:13,150 --> 00:04:15,840
They are organized from largest
to smallest, corresponding

108
00:04:15,840 --> 00:04:19,560
to the greatest to least source
of variation in the data.

109
00:04:19,560 --> 00:04:22,019
And then we have the document
or columnized matrix,

110
00:04:22,019 --> 00:04:26,400
which is also length normalized
and orthogonal in its space.

111
00:04:26,400 --> 00:04:28,590
And the theorem here is
that we can reconstruct A

112
00:04:28,590 --> 00:04:30,630
from these three matrices.

113
00:04:30,630 --> 00:04:32,840
Of course, we don't want
to precisely reconstruct

114
00:04:32,840 --> 00:04:35,520
A. That probably wouldn't
accomplish very much for us.

115
00:04:35,520 --> 00:04:37,890
But what we can do
is use this to learn

116
00:04:37,890 --> 00:04:40,350
reduced dimensional
representations of A

117
00:04:40,350 --> 00:04:42,750
by being selective
about which term

118
00:04:42,750 --> 00:04:46,518
and singular value dimensions
we include in the model.

119
00:04:46,518 --> 00:04:48,810
Let me walk you through an
example of how that happens.

120
00:04:48,810 --> 00:04:50,643
And first, let me
motivate this a little bit

121
00:04:50,643 --> 00:04:52,840
with an idealized
linguistic case.

122
00:04:52,840 --> 00:04:56,460
So I've got up here a
word by document matrix.

123
00:04:56,460 --> 00:04:59,008
Its vocabulary is gnarly,
wicked, awesome, lame,

124
00:04:59,008 --> 00:04:59,550
and terrible.

125
00:04:59,550 --> 00:05:02,910
And the conceit of my example
is that both gnarly and wicked

126
00:05:02,910 --> 00:05:03,990
are positive terms.

127
00:05:03,990 --> 00:05:06,320
So they tend to co-occur
with awesome and not

128
00:05:06,320 --> 00:05:08,350
co-occur with lame and terrible.

129
00:05:08,350 --> 00:05:11,140
However, gnarly and wicked never
occur in the same doc here.

130
00:05:11,140 --> 00:05:14,880
The idea is that gnarly is a
slang positive term associated

131
00:05:14,880 --> 00:05:16,890
with the West Coast
of the United States.

132
00:05:16,890 --> 00:05:20,040
And wicked is a slang term
associated with the East

133
00:05:20,040 --> 00:05:21,510
Coast of the United States.

134
00:05:21,510 --> 00:05:24,420
In virtue of that
idealized dialect split,

135
00:05:24,420 --> 00:05:26,670
they never occur in
the same document.

136
00:05:26,670 --> 00:05:28,740
But nonetheless, they
have similar neighbors

137
00:05:28,740 --> 00:05:30,000
in this vector space.

138
00:05:30,000 --> 00:05:32,490
And that's the kind of abstract
notion of co-occurrence

139
00:05:32,490 --> 00:05:34,940
that we want to capture.

140
00:05:34,940 --> 00:05:37,610
If we simply use our
standard distance

141
00:05:37,610 --> 00:05:39,950
measures and reweighting
techniques and so forth,

142
00:05:39,950 --> 00:05:42,110
we will not capture that
more abstract notion

143
00:05:42,110 --> 00:05:42,920
of co-occurrence.

144
00:05:42,920 --> 00:05:45,560
Here, distances in
this raw vector space,

145
00:05:45,560 --> 00:05:48,020
gnarly, awesome, terrible,
and wicked, wicked

146
00:05:48,020 --> 00:05:50,570
is farther away from gnarly
even than terrible is.

147
00:05:50,570 --> 00:05:53,150
So we've got a sentiment
confusion and really just

148
00:05:53,150 --> 00:05:55,860
not the result we
were shooting for.

149
00:05:55,860 --> 00:05:59,240
So we perform singular
value decomposition

150
00:05:59,240 --> 00:06:00,890
into these three matrices.

151
00:06:00,890 --> 00:06:02,600
And then the truncated
part is that we're

152
00:06:02,600 --> 00:06:05,270
going to consider just the
first two dimensions of the term

153
00:06:05,270 --> 00:06:09,110
matrix corresponding to these
two singular values capturing

154
00:06:09,110 --> 00:06:13,230
the top two sources of
variation in the data.

155
00:06:13,230 --> 00:06:14,990
So we multiply those
together and we

156
00:06:14,990 --> 00:06:18,140
get this reduced dimensional
matrix down here, 2

157
00:06:18,140 --> 00:06:20,370
by the size of the vocabulary.

158
00:06:20,370 --> 00:06:22,820
And if we do distance
measures in that space,

159
00:06:22,820 --> 00:06:26,030
just as we were hoping, gnarly
and wicked are now neighbors.

160
00:06:26,030 --> 00:06:29,060
The method has captured
that more abstract notion

161
00:06:29,060 --> 00:06:31,760
of having the same
neighbors as the other word.

162
00:06:31,760 --> 00:06:34,658


163
00:06:34,658 --> 00:06:36,200
In the previous
lecture, I encouraged

164
00:06:36,200 --> 00:06:38,748
you to think about what
you're doing to a matrix

165
00:06:38,748 --> 00:06:40,790
when you perform some kind
of reweighting scheme.

166
00:06:40,790 --> 00:06:43,190
Let's extend that to these
dimensionality reduction

167
00:06:43,190 --> 00:06:43,780
techniques.

168
00:06:43,780 --> 00:06:47,480
So here's a picture of what LSA
does starting with a raw count

169
00:06:47,480 --> 00:06:49,340
distribution over here.

170
00:06:49,340 --> 00:06:52,610
If I just run LSA on that
raw count distribution,

171
00:06:52,610 --> 00:06:56,000
I get what looks also like a
very difficult distribution

172
00:06:56,000 --> 00:06:56,960
of values.

173
00:06:56,960 --> 00:06:59,390
The cell values are
very spread out.

174
00:06:59,390 --> 00:07:03,230
And they have a lot of the
mass centered around 0 here,

175
00:07:03,230 --> 00:07:05,720
corresponding to the peak
in the raw counts over here

176
00:07:05,720 --> 00:07:07,028
near 0 as well.

177
00:07:07,028 --> 00:07:09,320
So that doesn't look like
we've done very much in terms

178
00:07:09,320 --> 00:07:12,200
of taming the kind of
untractable skewed distribution

179
00:07:12,200 --> 00:07:13,430
we started with.

180
00:07:13,430 --> 00:07:16,280
However, if instead we take
the raw counts and first feed

181
00:07:16,280 --> 00:07:17,960
them through PMI,
which as we saw

182
00:07:17,960 --> 00:07:21,230
before gives us this nice
distribution of values, highly

183
00:07:21,230 --> 00:07:24,770
constrained along the
x-axis, and then we run LSA,

184
00:07:24,770 --> 00:07:26,750
we retain a lot of
those good properties.

185
00:07:26,750 --> 00:07:28,940
The values are somewhat
more spread out

186
00:07:28,940 --> 00:07:30,830
but still nicely distributed.

187
00:07:30,830 --> 00:07:32,660
This looks like a
much happier input

188
00:07:32,660 --> 00:07:35,978
to downstream analytic methods
than the top version here.

189
00:07:35,978 --> 00:07:37,520
And I think this is
beginning to show

190
00:07:37,520 --> 00:07:40,850
that it can be powerful
to pipeline reweighting

191
00:07:40,850 --> 00:07:43,745
and dimensionality
reduction techniques.

192
00:07:43,745 --> 00:07:45,620
Another note I would
want to make, how do you

193
00:07:45,620 --> 00:07:47,480
choose the dimensionality
for, let's say, it

194
00:07:47,480 --> 00:07:49,940
has this variable k
corresponding to the number

195
00:07:49,940 --> 00:07:51,890
of dimensions that you keep.

196
00:07:51,890 --> 00:07:53,733
If you read the
literature on LSA,

197
00:07:53,733 --> 00:07:56,150
they often imagine this kind
of what I've called the dream

198
00:07:56,150 --> 00:07:59,150
scenario where you plot
the singular values

199
00:07:59,150 --> 00:08:01,430
and you see that a lot
of them are very large.

200
00:08:01,430 --> 00:08:03,350
And then there's
a sudden drop off.

201
00:08:03,350 --> 00:08:04,845
And if you do see
this, then it's

202
00:08:04,845 --> 00:08:07,220
obvious that you should pick
the point of the sudden drop

203
00:08:07,220 --> 00:08:08,330
off as your k.

204
00:08:08,330 --> 00:08:09,998
So here, you would pick k as 20.

205
00:08:09,998 --> 00:08:12,290
And you'd be confident that
you had captured almost all

206
00:08:12,290 --> 00:08:15,020
the variation in your data in
the reduced dimensional space

207
00:08:15,020 --> 00:08:16,700
you were creating.

208
00:08:16,700 --> 00:08:20,150
Unfortunately, for the kinds
of matrices and problems

209
00:08:20,150 --> 00:08:22,805
that we're looking at, I really
never see the dream scenario.

210
00:08:22,805 --> 00:08:24,590
What I see looks
something much more

211
00:08:24,590 --> 00:08:26,930
like this, where you have
kind of a sudden drop

212
00:08:26,930 --> 00:08:29,960
off early and then a long
decline and maybe a sudden drop

213
00:08:29,960 --> 00:08:31,040
off at the end.

214
00:08:31,040 --> 00:08:33,770
And it's basically totally
unclear where in the space

215
00:08:33,770 --> 00:08:35,240
you should pick k.

216
00:08:35,240 --> 00:08:37,130
And the result is
that k is often

217
00:08:37,130 --> 00:08:40,817
chosen kind of empirically
as a hyperparameter to

218
00:08:40,817 --> 00:08:42,650
and against whatever
problem you're actually

219
00:08:42,650 --> 00:08:44,070
trying to solve.

220
00:08:44,070 --> 00:08:46,700
If, in doing this work, you
do see the dream scenario,

221
00:08:46,700 --> 00:08:47,720
please do write to me.

222
00:08:47,720 --> 00:08:52,010
It would be very exciting
to see that happen.

223
00:08:52,010 --> 00:08:55,640
LSA is just one of a large
family of matrix decomposition

224
00:08:55,640 --> 00:08:56,880
methods.

225
00:08:56,880 --> 00:08:58,460
Here's a list of a few of them.

226
00:08:58,460 --> 00:09:00,920
And a lot of them are
implemented in scikit-learn

227
00:09:00,920 --> 00:09:02,960
in its decomposition library.

228
00:09:02,960 --> 00:09:05,240
And I would encourage you
to try them out and just see

229
00:09:05,240 --> 00:09:09,400
how they perform on problems
that you're trying to solve.

230
00:09:09,400 --> 00:09:11,230
And finally, here's
a little bit of code.

231
00:09:11,230 --> 00:09:15,760
vsm.lsa with k set
to 100 gives me back

232
00:09:15,760 --> 00:09:17,980
a reduced dimensional
version of that matrix,

233
00:09:17,980 --> 00:09:19,930
keeping the same
vocabulary, of course,

234
00:09:19,930 --> 00:09:22,240
but now with only 100
column dimensions.

235
00:09:22,240 --> 00:09:24,890


236
00:09:24,890 --> 00:09:26,180
Let's move to autoencoders.

237
00:09:26,180 --> 00:09:28,280
This will be a point
of contast with LSA.

238
00:09:28,280 --> 00:09:31,042
Because this is a much
more powerful method.

239
00:09:31,042 --> 00:09:32,000
So here's the overview.

240
00:09:32,000 --> 00:09:33,740
Autoencoders are
a flexible class

241
00:09:33,740 --> 00:09:36,020
of deep learning architectures
for learning reduced

242
00:09:36,020 --> 00:09:37,880
dimensional representations.

243
00:09:37,880 --> 00:09:40,490
If you want to hear much more
about this class of models,

244
00:09:40,490 --> 00:09:43,310
I would encourage you to read
chapter 14 of the Goodfellow et

245
00:09:43,310 --> 00:09:44,830
al book, Deep Learning.

246
00:09:44,830 --> 00:09:46,460
It has a lot of
details and a lot

247
00:09:46,460 --> 00:09:49,070
of variations on this theme.

248
00:09:49,070 --> 00:09:51,500
Here is the basic
autoencoder model.

249
00:09:51,500 --> 00:09:54,350
The input would be
the, say, that vectors

250
00:09:54,350 --> 00:09:56,463
from the rows in our matrices.

251
00:09:56,463 --> 00:09:58,130
So this could be the
counts or something

252
00:09:58,130 --> 00:10:00,370
that you've done to the counts.

253
00:10:00,370 --> 00:10:03,310
Those are fed through a hidden
layer of representation.

254
00:10:03,310 --> 00:10:06,040
And then the goal
of this model is

255
00:10:06,040 --> 00:10:09,340
to try to literally
reconstruct the input.

256
00:10:09,340 --> 00:10:11,620
Now, that might be
trivial if h had

257
00:10:11,620 --> 00:10:13,270
the same dimensionality as x.

258
00:10:13,270 --> 00:10:14,950
But the whole idea
here is that you're

259
00:10:14,950 --> 00:10:17,920
going to feed the input
through a very narrow pipe

260
00:10:17,920 --> 00:10:20,650
and then try to
reconstruct the input.

261
00:10:20,650 --> 00:10:22,990
Given that you're feeding it
through a potentially very

262
00:10:22,990 --> 00:10:24,880
narrow pipe, it's
unlikely that you'll

263
00:10:24,880 --> 00:10:27,310
be able to fully
reconstruct the inputs.

264
00:10:27,310 --> 00:10:29,020
But the idea is
that the model will

265
00:10:29,020 --> 00:10:31,990
learn to reconstruct the
important sources of variation

266
00:10:31,990 --> 00:10:35,180
in performing this
autoencoding step.

267
00:10:35,180 --> 00:10:38,530
And then when we use these
models for representation

268
00:10:38,530 --> 00:10:41,120
learning in the mode that
we've been in for this unit,

269
00:10:41,120 --> 00:10:42,550
the representation
that we choose

270
00:10:42,550 --> 00:10:44,230
is this hidden unit here.

271
00:10:44,230 --> 00:10:46,000
We typically don't
care about what

272
00:10:46,000 --> 00:10:48,280
was reconstructed on
the output, but rather

273
00:10:48,280 --> 00:10:51,190
only about the hidden reduced
dimensional representation

274
00:10:51,190 --> 00:10:52,270
that the model learned.

275
00:10:52,270 --> 00:10:54,470
This slide has a bunch of
other annotations on it.

276
00:10:54,470 --> 00:10:57,760
And the reason I included them
is that the course repository

277
00:10:57,760 --> 00:10:59,237
includes a reference
implementation

278
00:10:59,237 --> 00:11:01,570
of an autoencoder and all the
other deep learning models

279
00:11:01,570 --> 00:11:03,532
that we cover in pure NumPy.

280
00:11:03,532 --> 00:11:04,990
And so if you want
it to understand

281
00:11:04,990 --> 00:11:07,780
all of the technical details
of how the model is constructed

282
00:11:07,780 --> 00:11:10,870
and optimized, you could use
this as a kind of cheat sheet

283
00:11:10,870 --> 00:11:13,200
to understand how
the code works.

284
00:11:13,200 --> 00:11:15,700
I think the fundamental idea
that you want to have is simply

285
00:11:15,700 --> 00:11:18,430
that the model is trying
to reconstruct its inputs.

286
00:11:18,430 --> 00:11:20,500
The error signal that
we get is the difference

287
00:11:20,500 --> 00:11:23,300
between the reconstructed
and actual input.

288
00:11:23,300 --> 00:11:25,360
And that error signal
is what we use to update

289
00:11:25,360 --> 00:11:29,072
the parameters of the model.

290
00:11:29,072 --> 00:11:30,530
Final thing I would
mention here is

291
00:11:30,530 --> 00:11:33,020
that it could be very
difficult for this model

292
00:11:33,020 --> 00:11:35,180
if you feed in the raw
current vectors down here.

293
00:11:35,180 --> 00:11:36,740
They have very high
dimensionality.

294
00:11:36,740 --> 00:11:39,440
And their distribution is
highly skewed as we've seen.

295
00:11:39,440 --> 00:11:42,200
So it can be very productive to
do a little bit of reweighting

296
00:11:42,200 --> 00:11:45,470
and maybe even dimensionality
reduction with LSA

297
00:11:45,470 --> 00:11:48,620
before you start feeding
inputs into this model.

298
00:11:48,620 --> 00:11:50,360
Of course, it could
still be meaningful,

299
00:11:50,360 --> 00:11:53,030
even if you've done LSA
as a pre-processing step,

300
00:11:53,030 --> 00:11:55,010
to learn a hidden
dimensional representation

301
00:11:55,010 --> 00:11:58,730
because this model is presumably
capable of learning even more

302
00:11:58,730 --> 00:12:03,050
abstract notions than LSA is
in virtue of its non-linearity

303
00:12:03,050 --> 00:12:06,050
at this hidden layer.

304
00:12:06,050 --> 00:12:08,600
And here's a bit of code
just showing how this works,

305
00:12:08,600 --> 00:12:11,030
using both the reference
implementation that I mentioned

306
00:12:11,030 --> 00:12:12,830
as well as a faster
and more flexible

307
00:12:12,830 --> 00:12:16,370
Torch autoencoder which is
also included in the course

308
00:12:16,370 --> 00:12:17,480
repository.

309
00:12:17,480 --> 00:12:19,490
I think the only interface
thing to mention here

310
00:12:19,490 --> 00:12:21,740
is that these models
have a fit method,

311
00:12:21,740 --> 00:12:24,050
like all the other machine
learning models for this.

312
00:12:24,050 --> 00:12:26,450
But the fit method
returns that hidden

313
00:12:26,450 --> 00:12:28,400
dimensional
representation, the target

314
00:12:28,400 --> 00:12:30,000
for our learning
in this context,

315
00:12:30,000 --> 00:12:31,520
which is a bit non-standard.

316
00:12:31,520 --> 00:12:33,680
But it's the
intended application

317
00:12:33,680 --> 00:12:37,352
for this kind of
representation learning.

318
00:12:37,352 --> 00:12:38,810
The other thing I
would mention is,

319
00:12:38,810 --> 00:12:41,890
so let's see how well the
autoencoder is performing.

320
00:12:41,890 --> 00:12:45,370
This is the raw distances in
the giga5 matrix for finance.

321
00:12:45,370 --> 00:12:46,480
This is the count matrix.

322
00:12:46,480 --> 00:12:48,070
It doesn't look great.

323
00:12:48,070 --> 00:12:50,950
If we run the autoencoder
directly on the count matrix,

324
00:12:50,950 --> 00:12:52,120
it looks a little better.

325
00:12:52,120 --> 00:12:53,860
But it's still not excellent.

326
00:12:53,860 --> 00:12:55,780
If we think of this
as part of a pipeline

327
00:12:55,780 --> 00:12:59,120
where we've first done positive
pointwise mutual information,

328
00:12:59,120 --> 00:13:01,180
and then LSA at
dimension 100, and then

329
00:13:01,180 --> 00:13:03,280
dub the autoencoding
step, it starts

330
00:13:03,280 --> 00:13:05,870
to look like a really good and
interesting semantic space.

331
00:13:05,870 --> 00:13:08,120
And I think that's pointing
out the power of including

332
00:13:08,120 --> 00:13:11,230
the autoencoder in a larger
pipeline of preprocessing

333
00:13:11,230 --> 00:13:13,330
on the count matrices.

334
00:13:13,330 --> 00:13:15,250
Let's turn to GloVe
for Global Vectors

335
00:13:15,250 --> 00:13:18,480
for the final major unit
for this screencast.

336
00:13:18,480 --> 00:13:19,480
Here's a brief overview.

337
00:13:19,480 --> 00:13:22,780
GloVe was introduced by Jeffrey
Pennington, Richard Socher,

338
00:13:22,780 --> 00:13:25,660
and Chris Manning, a
Stanford team, in 2014.

339
00:13:25,660 --> 00:13:27,670
Roughly speaking,
the guiding idea here

340
00:13:27,670 --> 00:13:30,400
is that we want to
learn vectors for words

341
00:13:30,400 --> 00:13:32,680
such that the dot
product of those vectors

342
00:13:32,680 --> 00:13:35,650
is proportional to the log
probability of co-occurrence

343
00:13:35,650 --> 00:13:36,730
for those words.

344
00:13:36,730 --> 00:13:39,930
And I'll elaborate
on that in a second.

345
00:13:39,930 --> 00:13:42,000
For doing computational
work, we can

346
00:13:42,000 --> 00:13:44,430
rely on the implementation
torch.glove.py

347
00:13:44,430 --> 00:13:45,920
which is in the course repo.

348
00:13:45,920 --> 00:13:47,670
I'll mention that
there's also a reference

349
00:13:47,670 --> 00:13:49,440
implementation in bsm.py.

350
00:13:49,440 --> 00:13:52,140
It's very slow but it kind
of transparently implements

351
00:13:52,140 --> 00:13:53,760
the core GloVe
algorithm so it could

352
00:13:53,760 --> 00:13:55,765
be interesting to inspect.

353
00:13:55,765 --> 00:13:57,390
And then if you're
doing practical work

354
00:13:57,390 --> 00:14:00,060
with really large corpora and
really large vocabularies,

355
00:14:00,060 --> 00:14:02,970
I would encourage you to use the
GloVe team's C implementation.

356
00:14:02,970 --> 00:14:04,800
It's an outstanding
software artifact

357
00:14:04,800 --> 00:14:07,650
that will allow you to learn
lots of good representations

358
00:14:07,650 --> 00:14:08,367
quickly.

359
00:14:08,367 --> 00:14:10,200
And that kind of brings
me to my last point.

360
00:14:10,200 --> 00:14:12,480
I just want to mention
that the GloVe team was

361
00:14:12,480 --> 00:14:15,810
among the first teams in
NLP to release not just data

362
00:14:15,810 --> 00:14:19,170
and code but pre-trained
model parameters.

363
00:14:19,170 --> 00:14:20,613
Everyone does that these days.

364
00:14:20,613 --> 00:14:21,780
But it was rare at the time.

365
00:14:21,780 --> 00:14:24,150
And I think this team
was kind of really

366
00:14:24,150 --> 00:14:26,730
forward thinking in seeing
the value of releasing

367
00:14:26,730 --> 00:14:27,990
these centralized resources.

368
00:14:27,990 --> 00:14:30,000
And a lot of really
interesting work

369
00:14:30,000 --> 00:14:34,762
happened with GloVe
vectors as a foundation.

370
00:14:34,762 --> 00:14:36,970
All right, so let's think
about the technical aspects

371
00:14:36,970 --> 00:14:37,553
of this model.

372
00:14:37,553 --> 00:14:38,770
This is the GloVe objective.

373
00:14:38,770 --> 00:14:41,560
And you're going to see,
pointwise, mutual information

374
00:14:41,560 --> 00:14:44,668
kind of creep into this
picture in an interesting way.

375
00:14:44,668 --> 00:14:46,210
So this is equation
6 from the paper.

376
00:14:46,210 --> 00:14:48,940
It's kind of an idealized
objective for the GloVe model.

377
00:14:48,940 --> 00:14:50,680
And it says what I said before.

378
00:14:50,680 --> 00:14:55,708
We have a row vector and a
column vector, wi and wk.

379
00:14:55,708 --> 00:14:57,250
We're going to get
their dot product.

380
00:14:57,250 --> 00:15:00,310
And the goal is to learn
to have that dot product be

381
00:15:00,310 --> 00:15:03,160
proportional to the log of the
probability of co-occurrence

382
00:15:03,160 --> 00:15:04,960
of word i and word k.

383
00:15:04,960 --> 00:15:06,700
Where the probability
of co-occurence

384
00:15:06,700 --> 00:15:09,340
is defined in the way that
we defined it before when

385
00:15:09,340 --> 00:15:11,230
we were talking about
row normalization.

386
00:15:11,230 --> 00:15:12,790
It's just done in log space.

387
00:15:12,790 --> 00:15:14,680
This is the co-occurrence count.

388
00:15:14,680 --> 00:15:17,590
This is the sum of all
the counts along that row.

389
00:15:17,590 --> 00:15:18,970
And basically in
log space, we're

390
00:15:18,970 --> 00:15:21,920
just dividing this
value by this value.

391
00:15:21,920 --> 00:15:23,210
So keep that in mind.

392
00:15:23,210 --> 00:15:25,288
Now the reason they have
only the row represented

393
00:15:25,288 --> 00:15:26,830
is that in the paper
they're assuming

394
00:15:26,830 --> 00:15:29,770
that the rows and columns in
the underlying count matrix

395
00:15:29,770 --> 00:15:30,868
are identical.

396
00:15:30,868 --> 00:15:32,410
And so we don't need
to include both.

397
00:15:32,410 --> 00:15:35,410
However, if we did allow that
the row and context could

398
00:15:35,410 --> 00:15:37,210
be different, we
would just elaborate

399
00:15:37,210 --> 00:15:40,300
equation 6 to have a slightly
different denominator, right?

400
00:15:40,300 --> 00:15:43,930
We would have the product of
the row sum and the column sum,

401
00:15:43,930 --> 00:15:46,925
and take the log of that
and subtract that out.

402
00:15:46,925 --> 00:15:49,300
And that would be kind of our
goal for learning these dot

403
00:15:49,300 --> 00:15:50,620
products here.

404
00:15:50,620 --> 00:15:52,690
But aha, this is
where PMI sneaks in.

405
00:15:52,690 --> 00:15:55,780
Because that simply is
the PMI objective, right?

406
00:15:55,780 --> 00:15:58,630
Where we stated that is
the log of the probability

407
00:15:58,630 --> 00:16:01,210
of co-occurrence divided
by the product of the row

408
00:16:01,210 --> 00:16:03,160
and the column
probabilities, here they've

409
00:16:03,160 --> 00:16:05,740
just stated exactly that
calculation in log space.

410
00:16:05,740 --> 00:16:08,350
And these are numerically
equivalent by the equivalence

411
00:16:08,350 --> 00:16:11,200
of log of x over y
being the same as log

412
00:16:11,200 --> 00:16:13,312
of x minus the log of y.

413
00:16:13,312 --> 00:16:15,520
So that's the deep connection
that I was highlighting

414
00:16:15,520 --> 00:16:17,038
between GloVe and PMI.

415
00:16:17,038 --> 00:16:18,580
And I think that's
really interesting

416
00:16:18,580 --> 00:16:20,530
because it shows that
fundamentally we're

417
00:16:20,530 --> 00:16:23,170
testing a very similar
hypothesis using

418
00:16:23,170 --> 00:16:27,070
very similar notions of
row and column context.

419
00:16:27,070 --> 00:16:29,542
Now the GloVe team
doesn't just stop there.

420
00:16:29,542 --> 00:16:31,750
The GloVe objective is
actually much more interesting

421
00:16:31,750 --> 00:16:35,260
as an elaboration of
that core PMI idea.

422
00:16:35,260 --> 00:16:36,880
But it's worth
having PMI in mind

423
00:16:36,880 --> 00:16:40,090
because it's there
throughout this presentation.

424
00:16:40,090 --> 00:16:43,000
In the paper, they state this
is a kind of idealized objective

425
00:16:43,000 --> 00:16:45,500
where we're going to have the
dot product, as I said before,

426
00:16:45,500 --> 00:16:46,360
and two bias terms.

427
00:16:46,360 --> 00:16:48,850
And the goal will be to make
that equivalent to the log

428
00:16:48,850 --> 00:16:50,590
of the co-occurrence count.

429
00:16:50,590 --> 00:16:52,738
That has some
undesirable properties

430
00:16:52,738 --> 00:16:54,530
from the point of view
of machine learning.

431
00:16:54,530 --> 00:16:57,310
So they propose, in the
end, a weighted version

432
00:16:57,310 --> 00:16:58,630
of that objective.

433
00:16:58,630 --> 00:17:01,180
You can see we still have
the product of the row

434
00:17:01,180 --> 00:17:03,820
and the column vectors
and two biased terms.

435
00:17:03,820 --> 00:17:06,910
We're going to subtract out the
log of the co-occurance count

436
00:17:06,910 --> 00:17:08,140
and take the square of that.

437
00:17:08,140 --> 00:17:10,075
And that's going
to be weighted by f

438
00:17:10,075 --> 00:17:13,420
of the co-occurrence count,
where f is a function that you

439
00:17:13,420 --> 00:17:14,227
can define by hand.

440
00:17:14,227 --> 00:17:16,810
And what they do in the paper
is that it was two parameters, x

441
00:17:16,810 --> 00:17:19,180
max and alpha.

442
00:17:19,180 --> 00:17:21,250
For any count that
is above x max,

443
00:17:21,250 --> 00:17:24,520
we're going to set it to 1, kind
of flatten out all the really

444
00:17:24,520 --> 00:17:25,598
large counts.

445
00:17:25,598 --> 00:17:27,489
Everything that's
below x max, we

446
00:17:27,490 --> 00:17:29,380
will take as a
proportion of x max

447
00:17:29,380 --> 00:17:33,400
with some exponential scaling
as specified by alpha.

448
00:17:33,400 --> 00:17:35,120
That's the function there.

449
00:17:35,120 --> 00:17:38,500
And typically, alpha is set
to 0.75 and x max to 100.

450
00:17:38,500 --> 00:17:40,870
But I encourage you to
be critical in thinking

451
00:17:40,870 --> 00:17:43,930
about both those choices and
how they relate to your data.

452
00:17:43,930 --> 00:17:46,920
I'll return to that in a second.

453
00:17:46,920 --> 00:17:48,940
So GloVe really has these
three hyperparameters,

454
00:17:48,940 --> 00:17:54,160
the dimensionality of the
learned representations, x max

455
00:17:54,160 --> 00:17:56,110
which is going to have
this flattening effect,

456
00:17:56,110 --> 00:17:58,790
and alpha which is going
to scale the values, right?

457
00:17:58,790 --> 00:18:01,360
And so here's an example of
how those are interacting,

458
00:18:01,360 --> 00:18:02,320
x max and alpha.

459
00:18:02,320 --> 00:18:05,480
If I start with
this vector, 100 99,

460
00:18:05,480 --> 00:18:09,520
75, 10, and 1, the function
f, as we specified it,

461
00:18:09,520 --> 00:18:14,860
is going to flatten that out
into 1, 99, 81, 18, and 0.3.

462
00:18:14,860 --> 00:18:16,930
You should just be aware
that kind of flattening

463
00:18:16,930 --> 00:18:19,260
is happening.

464
00:18:19,260 --> 00:18:22,220
So GloVe learning, so it's kind
of interesting to think about

465
00:18:22,220 --> 00:18:24,750
it analytically about how
GloVe manages to learn

466
00:18:24,750 --> 00:18:26,185
interesting representations.

467
00:18:26,185 --> 00:18:28,560
And one thing that might be
on your mind is the question,

468
00:18:28,560 --> 00:18:31,690
can it actually learn higher
order notions of co-occurrence?

469
00:18:31,690 --> 00:18:34,260
That's been the major selling
point of this lecture.

470
00:18:34,260 --> 00:18:37,620
I gave that example involving
"gnarly" and "wicked" with LSA.

471
00:18:37,620 --> 00:18:39,670
Is GloVe going to be
able to do that, right?

472
00:18:39,670 --> 00:18:42,490
We could just pose
that as a question.

473
00:18:42,490 --> 00:18:45,540
So let's start that and see what
happens, see how this works.

474
00:18:45,540 --> 00:18:47,160
The loss calculations
for GloVe, this

475
00:18:47,160 --> 00:18:51,150
is a kind of simplified version
of the derivative of the model.

476
00:18:51,150 --> 00:18:53,070
And we're going to
show how GloVe manages

477
00:18:53,070 --> 00:18:55,830
to pull "gnarly" and
"wicked" toward off center,

478
00:18:55,830 --> 00:18:58,912
that little idealized
space that I used before.

479
00:18:58,912 --> 00:19:01,120
I'm going to leave out the
bias terms for simplicity.

480
00:19:01,120 --> 00:19:02,760
But we could bring those in.

481
00:19:02,760 --> 00:19:05,070
And so here's how this
is going to proceed.

482
00:19:05,070 --> 00:19:07,410
What I've done, just from
this idealized example,

483
00:19:07,410 --> 00:19:10,740
is begin in a GloVe space where
wicked and gnarly are as far

484
00:19:10,740 --> 00:19:12,540
apart as I could make
them, so as different

485
00:19:12,540 --> 00:19:14,190
as I could possibly make them.

486
00:19:14,190 --> 00:19:15,900
But I've got awesome
and terrible.

487
00:19:15,900 --> 00:19:18,720
And awesome is kind of
close to gnarly already.

488
00:19:18,720 --> 00:19:21,090
What you'll see is that
after just one iteration

489
00:19:21,090 --> 00:19:24,960
of the model, what has happened
is that wicked and gnarly

490
00:19:24,960 --> 00:19:26,687
have been pulled toward awesome.

491
00:19:26,687 --> 00:19:28,770
And that's just the kind
of effect that we wanted.

492
00:19:28,770 --> 00:19:31,920
That's the sense in which GloVe
can capture these higher order

493
00:19:31,920 --> 00:19:33,940
notions of co-occurrence.

494
00:19:33,940 --> 00:19:35,728
Just in a little
more detail, you

495
00:19:35,728 --> 00:19:37,270
might want to study
this on your own.

496
00:19:37,270 --> 00:19:39,630
But the high level overview
of exactly how that learning

497
00:19:39,630 --> 00:19:41,560
happens proceeds as follows.

498
00:19:41,560 --> 00:19:43,500
We start from these
counts up here.

499
00:19:43,500 --> 00:19:45,150
And the crucial
assumption I'm making

500
00:19:45,150 --> 00:19:48,390
is that wicked and
gnarly never co-occur.

501
00:19:48,390 --> 00:19:50,340
But they occur a
lot with awesome.

502
00:19:50,340 --> 00:19:53,490
And awesome will be kind of the
gravitational pull that makes

503
00:19:53,490 --> 00:19:56,220
gnarly and wicked look similar.

504
00:19:56,220 --> 00:19:58,753
Keep in mind that because
of that function f, by

505
00:19:58,753 --> 00:20:01,170
and large with GloVe we're
dealing not with the raw counts

506
00:20:01,170 --> 00:20:03,030
but rather with the
reweighted matrix.

507
00:20:03,030 --> 00:20:05,980
And that preserves this property
that may never co-occur.

508
00:20:05,980 --> 00:20:07,710
It gives differently
scaled values

509
00:20:07,710 --> 00:20:09,690
for the rest of the
co-occurrence or pseudo

510
00:20:09,690 --> 00:20:12,487
co-occurrence probabilities.

511
00:20:12,487 --> 00:20:14,320
Right, and here's what
we're going to track.

512
00:20:14,320 --> 00:20:16,408
This is the "gnarly"
vector in 0.

513
00:20:16,408 --> 00:20:18,700
And you can see I've made
them as far apart as I could.

514
00:20:18,700 --> 00:20:20,530
They're kind of
opposed to each other.

515
00:20:20,530 --> 00:20:22,988
But we're going to see how they
get pulled toward "awesome"

516
00:20:22,988 --> 00:20:24,880
in the context vector.

517
00:20:24,880 --> 00:20:26,340
So this is that
loss calculation.

518
00:20:26,340 --> 00:20:28,620
I have just plugged in
all the values here.

519
00:20:28,620 --> 00:20:32,100
And you can see that we get
this initial set of losses.

520
00:20:32,100 --> 00:20:35,460
That's after one iteration, and
we update the weight matrices.

521
00:20:35,460 --> 00:20:37,290
And we perform one
more round of learning.

522
00:20:37,290 --> 00:20:39,480
And you can see that
both of these models,

523
00:20:39,480 --> 00:20:42,330
the values here are getting
larger corresponding to getting

524
00:20:42,330 --> 00:20:44,640
pulled closer and
closer toward awesome.

525
00:20:44,640 --> 00:20:47,280
And you can see that
graphically happening over here

526
00:20:47,280 --> 00:20:48,790
in these plots on the left.

527
00:20:48,790 --> 00:20:51,510
And as I do more iterations
of the GloVe model,

528
00:20:51,510 --> 00:20:53,940
this effect is just going
to strengthen corresponding

529
00:20:53,940 --> 00:20:56,760
to wicked and gnarly getting
pulled toward awesome

530
00:20:56,760 --> 00:20:59,670
and away from terrible as a
result of these underlying

531
00:20:59,670 --> 00:21:00,600
counts.

532
00:21:00,600 --> 00:21:02,820
And I take this as good
evidence that GloVe,

533
00:21:02,820 --> 00:21:04,650
like the other methods
we've discussed,

534
00:21:04,650 --> 00:21:06,960
is capable of capturing
those higher order

535
00:21:06,960 --> 00:21:09,948
notions of co-occurrence that
we're so interested in pursuing

536
00:21:09,948 --> 00:21:10,740
with these methods.

537
00:21:10,740 --> 00:21:13,790


538
00:21:13,790 --> 00:21:14,990
Let's close the loop also.

539
00:21:14,990 --> 00:21:16,380
We have those central questions.

540
00:21:16,380 --> 00:21:18,380
What is GloVe doing to
our underlying spaces?

541
00:21:18,380 --> 00:21:20,600
With GloVe, because of
the design of the matrix,

542
00:21:20,600 --> 00:21:25,610
we have to begin from word by
word co-occurrence matrices

543
00:21:25,610 --> 00:21:26,900
accounts.

544
00:21:26,900 --> 00:21:28,580
So we begin with these
raw count values.

545
00:21:28,580 --> 00:21:30,210
And GloVe is one-stop shopping.

546
00:21:30,210 --> 00:21:31,585
It's going to take
us all the way

547
00:21:31,585 --> 00:21:33,920
to these reduced
dimensional representations.

548
00:21:33,920 --> 00:21:36,020
And boy, by the
criteria we've set up,

549
00:21:36,020 --> 00:21:37,710
does GloVe do an
outstanding job.

550
00:21:37,710 --> 00:21:41,720
This is the result of running
GloVe at dimension 50.

551
00:21:41,720 --> 00:21:44,360
And you can see that the values
are extremely well scaled

552
00:21:44,360 --> 00:21:47,840
between negative 2 and 2 and
nicely, normally distributed.

553
00:21:47,840 --> 00:21:52,260
This is an outstanding input to
modern machine learning models.

554
00:21:52,260 --> 00:21:56,180
And I think this is probably a
non-trivial aspect of why GloVe

555
00:21:56,180 --> 00:22:00,020
has been so successful as
a kind of pre-trained basis

556
00:22:00,020 --> 00:22:04,800
for a lot of subsequent
machine learning architectures.

557
00:22:04,800 --> 00:22:06,450
And then here's a
little bit of code,

558
00:22:06,450 --> 00:22:08,880
just showing you how you can
work with these interfaces

559
00:22:08,880 --> 00:22:10,283
using our code base.

560
00:22:10,283 --> 00:22:11,700
The one thing I
wanted to call out

561
00:22:11,700 --> 00:22:13,075
is that I'm trying
to be careful.

562
00:22:13,075 --> 00:22:15,270
I have defined this
function percentage

563
00:22:15,270 --> 00:22:17,280
non-zero values above.

564
00:22:17,280 --> 00:22:19,500
And you can set x max
here and just feed

565
00:22:19,500 --> 00:22:21,840
in a matrix and
study what percentage

566
00:22:21,840 --> 00:22:23,760
of the values in
that matrix are going

567
00:22:23,760 --> 00:22:26,700
to get flattened out to 1
as a result of the x max

568
00:22:26,700 --> 00:22:27,870
that you've chosen.

569
00:22:27,870 --> 00:22:31,330
And this value really varies
by the design of the matrix.

570
00:22:31,330 --> 00:22:35,670
If I feed in yelp5, only
about 5% of the values

571
00:22:35,670 --> 00:22:36,760
are getting flattened out.

572
00:22:36,760 --> 00:22:39,285
But if I feed in yelp20,
which is much denser

573
00:22:39,285 --> 00:22:42,750
and has much higher
counts, 20% of the values

574
00:22:42,750 --> 00:22:44,820
are getting flattened out to 1.

575
00:22:44,820 --> 00:22:46,740
If this number gets
too high, the matrix

576
00:22:46,740 --> 00:22:48,510
might become
completely homogeneous.

577
00:22:48,510 --> 00:22:52,530
And so we should really be aware
of how the setting of x max

578
00:22:52,530 --> 00:22:54,750
is affecting the kind of
learning that we could even

579
00:22:54,750 --> 00:22:56,490
be performing with GloVe.

580
00:22:56,490 --> 00:22:59,340
And it might turn out that
this is even more important

581
00:22:59,340 --> 00:23:02,220
than the number of iterations
or the dimensionality

582
00:23:02,220 --> 00:23:04,272
of the representations
that you learn.

583
00:23:04,272 --> 00:23:06,480
Once you've made those
choices, though, the interface

584
00:23:06,480 --> 00:23:07,200
is pretty clear.

585
00:23:07,200 --> 00:23:09,870
And the fit method, as
with the autoencoder,

586
00:23:09,870 --> 00:23:12,570
returns the matrix of
learned representations

587
00:23:12,570 --> 00:23:15,000
that we want to use for
the current purposes.

588
00:23:15,000 --> 00:23:18,150
And then finally, I've
included a score method.

589
00:23:18,150 --> 00:23:19,950
And the score method
is literally just

590
00:23:19,950 --> 00:23:23,490
testing to see how well the
vectors that you've learned

591
00:23:23,490 --> 00:23:25,620
correspond to the GloVe
objective of having the dot

592
00:23:25,620 --> 00:23:28,830
products be
proportional to the log

593
00:23:28,830 --> 00:23:30,420
of the co-occurrence
probabilities.

594
00:23:30,420 --> 00:23:31,878
Since you can get
a score for that,

595
00:23:31,878 --> 00:23:33,510
we're doing pretty
well here, let's say

596
00:23:33,510 --> 00:23:34,935
for a large empirical matrix.

597
00:23:34,935 --> 00:23:38,080


598
00:23:38,080 --> 00:23:40,840
Final section, let's just say
a bit about visualization.

599
00:23:40,840 --> 00:23:43,060
And this is a dimensionality
reduction technique

600
00:23:43,060 --> 00:23:44,440
in the sense that
the whole point

601
00:23:44,440 --> 00:23:47,140
is to try to flatten out a
very high dimensional space

602
00:23:47,140 --> 00:23:49,690
into possibly two
or three dimensions.

603
00:23:49,690 --> 00:23:51,940
You have to recognize
that inevitably this will

604
00:23:51,940 --> 00:23:53,590
involve a lot of compromises.

605
00:23:53,590 --> 00:23:57,070
It's just impossible to capture
all the sources of variation

606
00:23:57,070 --> 00:24:00,260
in your underlying matrix
in just a few dimensions.

607
00:24:00,260 --> 00:24:02,710
But nonetheless, this
can be productive.

608
00:24:02,710 --> 00:24:05,740
I think it's especially
valuable if you pair it

609
00:24:05,740 --> 00:24:08,860
with some kind of hands-on
qualitative exploration using

610
00:24:08,860 --> 00:24:12,430
something like BSN neighbors
to understand at a low level

611
00:24:12,430 --> 00:24:14,020
what your matrix encodes.

612
00:24:14,020 --> 00:24:15,670
And then the high
level visualizations

613
00:24:15,670 --> 00:24:18,795
can be a kind of
counterpart to that.

614
00:24:18,795 --> 00:24:21,170
There are many visualization
techniques and a lot of them

615
00:24:21,170 --> 00:24:24,890
are implemented in the
scikit manifold package.

616
00:24:24,890 --> 00:24:26,520
So I encourage you to use them.

617
00:24:26,520 --> 00:24:29,630
I'm going to show you some
results from t-SNE which

618
00:24:29,630 --> 00:24:33,557
stands for t-distributed
stochastic neighbor embedding.

619
00:24:33,557 --> 00:24:35,390
There are lots of user
guides there that you

620
00:24:35,390 --> 00:24:36,680
can study for more details.

621
00:24:36,680 --> 00:24:38,330
Let me just give
you the high level.

622
00:24:38,330 --> 00:24:41,480
This is t-SNE run on
our giga20 matrix.

623
00:24:41,480 --> 00:24:44,430
I think this is typical of
pretty good output from t-SNE.

624
00:24:44,430 --> 00:24:47,660
So what we're seeing here is
some pockets of high density.

625
00:24:47,660 --> 00:24:49,820
Those are areas of
local coherence.

626
00:24:49,820 --> 00:24:52,400
Globally, we should be
careful not to over interpret

627
00:24:52,400 --> 00:24:54,890
this entire diagram
because as you rerun

628
00:24:54,890 --> 00:24:56,480
the model with
different random seeds,

629
00:24:56,480 --> 00:24:59,630
you'll see that it gets kind of
reoriented in different parts

630
00:24:59,630 --> 00:25:02,460
or correspondingly close
to different other parts.

631
00:25:02,460 --> 00:25:04,760
But what you can count
on pretty reliably

632
00:25:04,760 --> 00:25:07,610
is that these local
pockets of coherence

633
00:25:07,610 --> 00:25:11,640
correspond to coherence parts of
the space that you've defined.

634
00:25:11,640 --> 00:25:14,060
And if you zoom in on
them, you can assess

635
00:25:14,060 --> 00:25:15,590
what the model has uncovered.

636
00:25:15,590 --> 00:25:17,570
So for this giga21,
for example, I

637
00:25:17,570 --> 00:25:19,940
think we see prominent clusters
corresponding to things

638
00:25:19,940 --> 00:25:22,550
like cooking and conflict.

639
00:25:22,550 --> 00:25:24,950
If we do the same thing
for our Yelp matrix,

640
00:25:24,950 --> 00:25:27,110
again, this looks
pretty good in terms

641
00:25:27,110 --> 00:25:29,870
of having some substructure
that we can analyze.

642
00:25:29,870 --> 00:25:33,350
And if we zoom in, we do see
clusters like positive terms

643
00:25:33,350 --> 00:25:36,620
and negative terms corresponding
to the evaluative setting

644
00:25:36,620 --> 00:25:37,877
of these Yelp reviews.

645
00:25:37,877 --> 00:25:39,710
So this is all very
encouraging and suggests

646
00:25:39,710 --> 00:25:42,380
that the underlying spaces
have some really interesting

647
00:25:42,380 --> 00:25:47,257
structure that might be useful
for subsequent analysis.

648
00:25:47,257 --> 00:25:48,590
And here are some code snippets.

649
00:25:48,590 --> 00:25:50,840
We have this simple
wrapper around

650
00:25:50,840 --> 00:25:53,120
the scikit t-SNE
implementation that

651
00:25:53,120 --> 00:25:56,270
will allow you to flexibly
work with this stuff using

652
00:25:56,270 --> 00:25:58,910
the account matrices
from our unit.

653
00:25:58,910 --> 00:26:01,190
And I'm just mentioning
here that it's pretty easy

654
00:26:01,190 --> 00:26:03,740
if you want it to color code
the words in your vocabulary,

655
00:26:03,740 --> 00:26:06,560
say, according to a sentiment
lexicon or some other kind

656
00:26:06,560 --> 00:26:07,610
of lexicon.

657
00:26:07,610 --> 00:26:10,040
That could be a way for
you to reveal exactly what

658
00:26:10,040 --> 00:26:12,950
structure your model has been
able to uncover with respect

659
00:26:12,950 --> 00:26:14,300
to those underlying labels.

660
00:26:14,300 --> 00:26:16,360
And that can be useful.

661
00:26:16,360 --> 00:26:20,246


