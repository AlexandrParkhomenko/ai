1
00:00:00,000 --> 00:00:04,058


2
00:00:04,058 --> 00:00:05,600
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,600 --> 00:00:08,330
This is part two in our series
on analysis methods in NLP.

4
00:00:08,330 --> 00:00:10,690
We're going to be talking
about adversarial testing.

5
00:00:10,690 --> 00:00:14,170
This is an exciting mode because
as you'll see with a few dozen

6
00:00:14,170 --> 00:00:16,270
carefully created
examples, you can

7
00:00:16,270 --> 00:00:18,520
learn something really
interesting about the systems

8
00:00:18,520 --> 00:00:20,440
that you're developing.

9
00:00:20,440 --> 00:00:23,860
To start, let's remind ourselves
of how evaluation standardly

10
00:00:23,860 --> 00:00:25,210
work in our field.

11
00:00:25,210 --> 00:00:28,750
At step one you create a dataset
from some single homogeneous

12
00:00:28,750 --> 00:00:29,380
process.

13
00:00:29,380 --> 00:00:31,422
It could be that you've
scraped data from the web

14
00:00:31,422 --> 00:00:34,603
or crowd-sourced a dataset,
or labelled examples yourself,

15
00:00:34,603 --> 00:00:36,520
but the important thing
is that we, typically,

16
00:00:36,520 --> 00:00:39,250
do this as one single process.

17
00:00:39,250 --> 00:00:41,950
And then in step two
we divide that dataset

18
00:00:41,950 --> 00:00:44,290
into disjoint train
and test sets,

19
00:00:44,290 --> 00:00:46,570
and we set the test set aside.

20
00:00:46,570 --> 00:00:49,180
And you do all your
development on the train set

21
00:00:49,180 --> 00:00:51,400
and, only after all
development is complete,

22
00:00:51,400 --> 00:00:54,340
you do an evaluation of
your system usually based

23
00:00:54,340 --> 00:00:57,820
on accuracy or some similar
metric on that held out test

24
00:00:57,820 --> 00:00:58,868
set.

25
00:00:58,868 --> 00:01:00,910
And then finally, and this
is the important part,

26
00:01:00,910 --> 00:01:04,090
you report the results of
that test set evaluation

27
00:01:04,090 --> 00:01:07,890
as providing an estimate of the
system's capacity to generalize

28
00:01:07,890 --> 00:01:09,610
to new experiences.

29
00:01:09,610 --> 00:01:11,950
I hope you can hear in that
that we're being awfully

30
00:01:11,950 --> 00:01:13,540
generous to our systems.

31
00:01:13,540 --> 00:01:17,830
At step one we create a single
data set from a single process.

32
00:01:17,830 --> 00:01:21,700
We hold out the test set, and we
use that test set as our device

33
00:01:21,700 --> 00:01:23,440
for measuring how
well the system is

34
00:01:23,440 --> 00:01:26,240
going to do if deployed
out in the real world.

35
00:01:26,240 --> 00:01:28,660
But, of course, we know
that the real world is not

36
00:01:28,660 --> 00:01:31,390
created from some single
homogeneous process.

37
00:01:31,390 --> 00:01:34,570
We know in our heart of hearts
that if we deploy the system,

38
00:01:34,570 --> 00:01:37,510
it will encounter
examples that are entirely

39
00:01:37,510 --> 00:01:40,420
unlike those that it saw
in training and assessment

40
00:01:40,420 --> 00:01:42,820
in the standard mode,
and that might worry us,

41
00:01:42,820 --> 00:01:45,400
that we're overstating the
capacity of our systems

42
00:01:45,400 --> 00:01:48,790
to actually deal with the
complexity of the world.

43
00:01:48,790 --> 00:01:51,100
And adversarial
evaluations are one way

44
00:01:51,100 --> 00:01:54,020
that we can begin to
close this gap here.

45
00:01:54,020 --> 00:01:57,280
So in adversarial evaluations
we create a dataset

46
00:01:57,280 --> 00:02:00,190
by whatever means you
like, at step one,

47
00:02:00,190 --> 00:02:02,830
and you develop and assess
the system using that dataset

48
00:02:02,830 --> 00:02:04,670
according to whatever
protocols you choose.

49
00:02:04,670 --> 00:02:08,139
It could be the standard
evaluation mode, if you like.

50
00:02:08,139 --> 00:02:09,070
Here's the new part.

51
00:02:09,070 --> 00:02:12,880
At step three you develop
a new test set of examples

52
00:02:12,880 --> 00:02:15,940
that you suspect, or
know, will be challenging

53
00:02:15,940 --> 00:02:18,160
given your system and
the original data set

54
00:02:18,160 --> 00:02:20,150
that it was trained on.

55
00:02:20,150 --> 00:02:22,490
And then, as usual, after
all your system development

56
00:02:22,490 --> 00:02:24,740
is complete you evaluate
the system, usually based

57
00:02:24,740 --> 00:02:29,030
on accuracy, again, on
that new test dataset.

58
00:02:29,030 --> 00:02:30,950
And that's the result
that you report

59
00:02:30,950 --> 00:02:33,440
as your system's
capacity to generalize

60
00:02:33,440 --> 00:02:35,570
to new experiences,
at least of the sort

61
00:02:35,570 --> 00:02:38,810
that you carved out in
your adversarial test set.

62
00:02:38,810 --> 00:02:40,790
And the idea here
is that in having

63
00:02:40,790 --> 00:02:44,930
a distinction between the data
that we develop our system on

64
00:02:44,930 --> 00:02:47,510
and the challenge
problems that we pose,

65
00:02:47,510 --> 00:02:50,600
we'll get a better estimate
of how our systems perform

66
00:02:50,600 --> 00:02:54,500
on examples that are presumably
important examples that it's

67
00:02:54,500 --> 00:02:59,310
likely to encounter when it's
deployed in the real world.

68
00:02:59,310 --> 00:03:01,080
A brief historical
note, I think it's

69
00:03:01,080 --> 00:03:03,510
fair to say that the idea
of adversarial testing

70
00:03:03,510 --> 00:03:05,790
traces all the way back
to the Turing test,

71
00:03:05,790 --> 00:03:08,910
which is introduced by Turing
in this classic paper, Computing

72
00:03:08,910 --> 00:03:10,780
Machinery and intelligence.

73
00:03:10,780 --> 00:03:13,710
The Turing test has an
inherently adversarial flavor

74
00:03:13,710 --> 00:03:15,870
to it because, of
course, a computer

75
00:03:15,870 --> 00:03:18,120
is trying to trick a
human into thinking

76
00:03:18,120 --> 00:03:20,730
that the computer is human.

77
00:03:20,730 --> 00:03:22,943
We also hear echoes
of adversarial testing

78
00:03:22,943 --> 00:03:24,360
in this classic
article slash book

79
00:03:24,360 --> 00:03:28,020
from Terry Winograd called
Understanding Natural Language.

80
00:03:28,020 --> 00:03:29,940
There are discussions
in that book

81
00:03:29,940 --> 00:03:32,970
of the idea of constructing
examples that we know

82
00:03:32,970 --> 00:03:36,300
will stress test our systems
by probing to see whether they

83
00:03:36,300 --> 00:03:38,220
have knowledge of
what the world is like

84
00:03:38,220 --> 00:03:40,650
and the true
complexity of language.

85
00:03:40,650 --> 00:03:43,470
And then I think Hector Levesque
really elevated those Winograd

86
00:03:43,470 --> 00:03:46,740
ideas into a full-fledged
testing mode in his paper,

87
00:03:46,740 --> 00:03:49,090
On Our Best Behavior.

88
00:03:49,090 --> 00:03:51,060
Let's look briefly at
those Winograd sentences,

89
00:03:51,060 --> 00:03:53,393
or Winograd schema, because
they're kind of interesting.

90
00:03:53,393 --> 00:03:55,860
The idea is that they will
key into whether a system has

91
00:03:55,860 --> 00:03:58,887
deep knowledge of the
world and of language.

92
00:03:58,887 --> 00:04:00,720
So we start with an
example like, the trophy

93
00:04:00,720 --> 00:04:02,430
doesn't fit into
the brown suitcase

94
00:04:02,430 --> 00:04:03,670
because it's too small.

95
00:04:03,670 --> 00:04:05,010
What's too small?

96
00:04:05,010 --> 00:04:07,470
And the human answer
is, the suitcase.

97
00:04:07,470 --> 00:04:09,430
There's a minimally
contrasting example,

98
00:04:09,430 --> 00:04:11,550
the trophy doesn't fit
into the brown suitcase

99
00:04:11,550 --> 00:04:12,720
because it's too large.

100
00:04:12,720 --> 00:04:13,920
What's too large?

101
00:04:13,920 --> 00:04:15,450
Here we answer the trophy.

102
00:04:15,450 --> 00:04:17,279
And presumably, we
do this because we

103
00:04:17,279 --> 00:04:19,140
can do a kind of
mental simulation

104
00:04:19,140 --> 00:04:22,290
involving suitcases and
trophies and figure out

105
00:04:22,290 --> 00:04:25,750
how to answer these
questions on that basis.

106
00:04:25,750 --> 00:04:27,640
This next pair of
examples is similar,

107
00:04:27,640 --> 00:04:31,030
but it keys more into
normative social roles.

108
00:04:31,030 --> 00:04:32,860
The council refused
the demonstrators

109
00:04:32,860 --> 00:04:34,750
a permit because
they feared violence.

110
00:04:34,750 --> 00:04:36,040
Who feared violence?

111
00:04:36,040 --> 00:04:38,710
And our standard answer, drawing
on standard social roles,

112
00:04:38,710 --> 00:04:40,300
is the council.

113
00:04:40,300 --> 00:04:42,910
Again, we have a minimally
contrasting example.

114
00:04:42,910 --> 00:04:44,710
The council refused
the demonstrators

115
00:04:44,710 --> 00:04:46,870
a permit because they
advocated violence.

116
00:04:46,870 --> 00:04:48,310
Who advocated violence?

117
00:04:48,310 --> 00:04:51,040
And here, again drawing
on default social roles,

118
00:04:51,040 --> 00:04:53,680
we're inclined to answer,
the demonstrators.

119
00:04:53,680 --> 00:04:56,530
The intuition, as I
said, is that to resolve

120
00:04:56,530 --> 00:04:59,050
these questions, given
how minimally different

121
00:04:59,050 --> 00:05:01,450
these examples are, you need
to have a deep understanding

122
00:05:01,450 --> 00:05:04,060
of the questions and
the context and also

123
00:05:04,060 --> 00:05:07,743
a deep understanding of
what our world is like.

124
00:05:07,743 --> 00:05:09,910
And then, as I said, Levesque
kind of continues this

125
00:05:09,910 --> 00:05:11,550
and begins to systematize it.

126
00:05:11,550 --> 00:05:14,830
So he says, we should pose
questions to our systems like,

127
00:05:14,830 --> 00:05:17,470
could a crocodile
run a steeplechase?

128
00:05:17,470 --> 00:05:18,620
And the intuition is clear.

129
00:05:18,620 --> 00:05:21,220
The question can be answered
by thinking it through.

130
00:05:21,220 --> 00:05:23,050
A crocodile has short legs.

131
00:05:23,050 --> 00:05:24,520
The hedges in a
steeplechase would

132
00:05:24,520 --> 00:05:26,950
be too tall for the
crocodile to jump over.

133
00:05:26,950 --> 00:05:27,610
So no.

134
00:05:27,610 --> 00:05:29,680
A crocodile cannot
run a steeplechase.

135
00:05:29,680 --> 00:05:32,170
Again, a mental
simulation that leads us

136
00:05:32,170 --> 00:05:35,080
to an answer to this
surprising question.

137
00:05:35,080 --> 00:05:37,510
And the idea is that
questions like this

138
00:05:37,510 --> 00:05:39,880
will not be susceptible
to cheap tricks.

139
00:05:39,880 --> 00:05:41,890
As Levesque says,
can we find questions

140
00:05:41,890 --> 00:05:43,540
where cheap tricks
like this will not

141
00:05:43,540 --> 00:05:46,120
be sufficient to produce
the desired behavior?

142
00:05:46,120 --> 00:05:48,040
This, unfortunately,
has no easy answer.

143
00:05:48,040 --> 00:05:50,380
The best we can do,
perhaps, is to come up

144
00:05:50,380 --> 00:05:52,210
with a suite of multiple
choice questions

145
00:05:52,210 --> 00:05:55,180
carefully, and then study the
sorts of computer programs

146
00:05:55,180 --> 00:05:56,950
that might be able
to answer them.

147
00:05:56,950 --> 00:05:59,110
I think you can hear in
that what we now call

148
00:05:59,110 --> 00:06:02,610
the adversarial testing mode.

149
00:06:02,610 --> 00:06:05,220
Now, I'm encouraging you
to pose adversarial tests

150
00:06:05,220 --> 00:06:06,003
to your systems.

151
00:06:06,003 --> 00:06:07,920
And as I said, you can
do that by constructing

152
00:06:07,920 --> 00:06:10,410
just a few novel
examples, but we should

153
00:06:10,410 --> 00:06:13,070
be aware of what we're doing.

154
00:06:13,070 --> 00:06:16,100
Primary question, what can
adversarial testing tell us,

155
00:06:16,100 --> 00:06:18,170
and what can't it tell
us about the systems

156
00:06:18,170 --> 00:06:19,460
that we're developing?

157
00:06:19,460 --> 00:06:20,930
Here are just a
few considerations

158
00:06:20,930 --> 00:06:23,400
that should guide your
work in this area.

159
00:06:23,400 --> 00:06:26,060
First, you don't need
to be too adversarial.

160
00:06:26,060 --> 00:06:29,300
It could just be that you're
posing a challenge problem

161
00:06:29,300 --> 00:06:31,220
to assess whether
your system has

162
00:06:31,220 --> 00:06:34,130
an understanding of a
particular set of phenomena.

163
00:06:34,130 --> 00:06:37,310
Has my system learned anything
about numerical terms?

164
00:06:37,310 --> 00:06:40,040
Does my system understand
how negation works?

165
00:06:40,040 --> 00:06:42,638
Does my system work with
a new style or genre?

166
00:06:42,638 --> 00:06:44,180
These are challenge
problems that you

167
00:06:44,180 --> 00:06:45,650
could pose
open-mindedly, and you

168
00:06:45,650 --> 00:06:49,690
might find that your system
is surprisingly good at them.

169
00:06:49,690 --> 00:06:52,570
Second consideration, we should
be thoughtful about the metrics

170
00:06:52,570 --> 00:06:53,410
we use.

171
00:06:53,410 --> 00:06:56,380
As I signaled to you
before, the limitations

172
00:06:56,380 --> 00:06:59,620
of accuracy-based metrics,
like F1 and so forth,

173
00:06:59,620 --> 00:07:03,100
are generally left unaddressed
by the adversarial paradigm.

174
00:07:03,100 --> 00:07:05,170
And that's because we
want a minimal contrast

175
00:07:05,170 --> 00:07:07,300
with our standard
evaluation modes.

176
00:07:07,300 --> 00:07:09,710
But I think you can hear
in the mission statements,

177
00:07:09,710 --> 00:07:12,400
especially from Levesque,
that we might at some point

178
00:07:12,400 --> 00:07:15,310
want to break free of
that very restrictive mode

179
00:07:15,310 --> 00:07:19,090
and pose more open-ended complex
evaluations for our systems

180
00:07:19,090 --> 00:07:22,390
that would involve requiring
them to offer their evidence

181
00:07:22,390 --> 00:07:25,240
and interact with us to resolve
uncertainty about what they're

182
00:07:25,240 --> 00:07:26,320
supposed to be doing.

183
00:07:26,320 --> 00:07:30,220
All of that is left aside in
the standard adversarial mode,

184
00:07:30,220 --> 00:07:33,460
and we should be aware
that it's a limitation.

185
00:07:33,460 --> 00:07:36,400
This next question is really
fundamentally important.

186
00:07:36,400 --> 00:07:40,090
In adversarial testing, if
you see a model failure,

187
00:07:40,090 --> 00:07:43,570
is it actually a failure of
the model or of the dataset

188
00:07:43,570 --> 00:07:45,550
that the model was trained on?

189
00:07:45,550 --> 00:07:47,050
Liu, et al. posed this nicely.

190
00:07:47,050 --> 00:07:48,790
What should we
conclude when a system

191
00:07:48,790 --> 00:07:51,130
fails on a challenge dataset?

192
00:07:51,130 --> 00:07:53,950
In some cases, the challenge
might exploit blind spots

193
00:07:53,950 --> 00:07:55,630
in the design of the
original dataset.

194
00:07:55,630 --> 00:07:57,670
That would be a
dataset weakness.

195
00:07:57,670 --> 00:08:00,880
In others, the challenge might
expose an inherent inability

196
00:08:00,880 --> 00:08:04,480
of a particular model family to
handle certain natural language

197
00:08:04,480 --> 00:08:05,080
phenomenon.

198
00:08:05,080 --> 00:08:07,300
That would be a model weakness.

199
00:08:07,300 --> 00:08:09,550
Now, these are interestingly
different from the point

200
00:08:09,550 --> 00:08:12,010
of view of development
in our field.

201
00:08:12,010 --> 00:08:15,250
We're apt to hope, I think,
that we find model weaknesses

202
00:08:15,250 --> 00:08:17,710
because those are really
fundamental discoveries,

203
00:08:17,710 --> 00:08:21,490
but we should be aware that it
might be that the system could

204
00:08:21,490 --> 00:08:23,410
have done well on
our adversarial test

205
00:08:23,410 --> 00:08:25,990
if it had just been trained
on the right kind of examples.

206
00:08:25,990 --> 00:08:27,910
And that would be
a dataset weakness.

207
00:08:27,910 --> 00:08:30,250
Data set weaknesses are
presumably relatively

208
00:08:30,250 --> 00:08:31,520
easy for us to address.

209
00:08:31,520 --> 00:08:33,549
We can just supplement
the training data

210
00:08:33,549 --> 00:08:35,080
with examples of
the relevant kind,

211
00:08:35,080 --> 00:08:38,692
whereas model
weaknesses are forcing

212
00:08:38,692 --> 00:08:40,150
us to confront
something that might

213
00:08:40,150 --> 00:08:43,330
be an inherent limitation of
the set of approaches that we're

214
00:08:43,330 --> 00:08:47,600
taking, a much more
fundamental insight.

215
00:08:47,600 --> 00:08:49,190
Atticus Geiger, et
al., in this paper

216
00:08:49,190 --> 00:08:51,800
offers a similar
insight in the context

217
00:08:51,800 --> 00:08:53,930
of being fair to our models.

218
00:08:53,930 --> 00:08:56,810
For any evaluation method, we
should ask whether it's fair.

219
00:08:56,810 --> 00:08:59,750
Has the model been shown
data sufficient to support

220
00:08:59,750 --> 00:09:02,240
the kind of generalization
we're asking of it?

221
00:09:02,240 --> 00:09:04,400
Unless we can say yes
with complete certainty,

222
00:09:04,400 --> 00:09:07,460
we can't be sure whether
a failed evaluation traces

223
00:09:07,460 --> 00:09:10,310
to a model limitation
or a data limitation

224
00:09:10,310 --> 00:09:12,800
that no model could overcome.

225
00:09:12,800 --> 00:09:15,950
And I'm emphasizing this
because it's surprisingly easy

226
00:09:15,950 --> 00:09:17,900
to fall into the
trap of thinking

227
00:09:17,900 --> 00:09:21,230
you have imposed an unambiguous
learning target when,

228
00:09:21,230 --> 00:09:22,610
in fact, you have not.

229
00:09:22,610 --> 00:09:24,530
Just think about the
simple example here.

230
00:09:24,530 --> 00:09:28,160
Human to human, suppose I
begin the sequence 3, 3, 5, 4,

231
00:09:28,160 --> 00:09:31,950
and I say to you, what
comes next in the sequence?

232
00:09:31,950 --> 00:09:34,460
Now, I might have
in mind the number 7

233
00:09:34,460 --> 00:09:36,080
But the evidence
that I have offered

234
00:09:36,080 --> 00:09:39,570
you wildly underdetermines
how to continue the sequence.

235
00:09:39,570 --> 00:09:41,570
And so I think it's fair
to say that no learning

236
00:09:41,570 --> 00:09:43,430
agent, without a
lot of ambiguity,

237
00:09:43,430 --> 00:09:47,030
could figure out what my
intended continuation is.

238
00:09:47,030 --> 00:09:49,370
And sometimes our
adversarial tests

239
00:09:49,370 --> 00:09:52,580
have this quality that the
available data and experiences

240
00:09:52,580 --> 00:09:54,500
of these systems
just don't fully

241
00:09:54,500 --> 00:09:57,560
disambiguate what our
intended learning targets are.

242
00:09:57,560 --> 00:09:58,910
So we should be aware of that.

243
00:09:58,910 --> 00:10:02,240
Those are dataset failings,
rather than model failings.

244
00:10:02,240 --> 00:10:04,490
Now, I can offer you
a constructive set

245
00:10:04,490 --> 00:10:06,830
of techniques to figure
out whether you're

246
00:10:06,830 --> 00:10:09,380
dealing with a dataset
weakness or a model weakness,

247
00:10:09,380 --> 00:10:11,630
and that falls under the
heading of inoculation

248
00:10:11,630 --> 00:10:13,890
by fine tuning from
this wonderful paper,

249
00:10:13,890 --> 00:10:16,530
Liu, et al., that I
quoted from before.

250
00:10:16,530 --> 00:10:17,960
So Liu, et al.,
just to remind us

251
00:10:17,960 --> 00:10:20,840
that in the standard
challenge evaluation mode

252
00:10:20,840 --> 00:10:23,630
we train our system on
some original dataset

253
00:10:23,630 --> 00:10:26,420
and then we test it on
both the original test

254
00:10:26,420 --> 00:10:28,640
set and our challenge test set.

255
00:10:28,640 --> 00:10:30,927
And our expectation is
that we'll see outcomes

256
00:10:30,927 --> 00:10:32,510
like this where the
system does really

257
00:10:32,510 --> 00:10:35,210
well on the original data,
the original test set,

258
00:10:35,210 --> 00:10:38,350
and really poorly on
the challenge test set.

259
00:10:38,350 --> 00:10:40,370
But when we see this
outcome we should ask,

260
00:10:40,370 --> 00:10:41,690
why this is happening?

261
00:10:41,690 --> 00:10:45,380
In particular, is it a model
weakness or a dataset weakness?

262
00:10:45,380 --> 00:10:48,020
And their proposed method,
this inoculation method,

263
00:10:48,020 --> 00:10:49,650
works as follows.

264
00:10:49,650 --> 00:10:51,320
We're going to
fine-tune our system

265
00:10:51,320 --> 00:10:53,990
on a few of our
challenge examples,

266
00:10:53,990 --> 00:10:58,520
and then retest on both the
original test set and the held

267
00:10:58,520 --> 00:11:00,830
out parts of the
challenge test set.

268
00:11:00,830 --> 00:11:03,560
When we do this, there are
three classes of outcome

269
00:11:03,560 --> 00:11:04,820
that you might see.

270
00:11:04,820 --> 00:11:07,490
The first would point
to a dataset weakness.

271
00:11:07,490 --> 00:11:11,390
If via this little bit of fine
tuning on the challenge dataset

272
00:11:11,390 --> 00:11:14,750
we can get good performance on
the original and the challenge

273
00:11:14,750 --> 00:11:18,620
data set, that shows us that in
this original evaluation mode

274
00:11:18,620 --> 00:11:20,570
the system just
didn't see enough

275
00:11:20,570 --> 00:11:23,030
of the relevant
kinds of examples

276
00:11:23,030 --> 00:11:26,390
from your adversarial tests to
have any hope of succeeding,

277
00:11:26,390 --> 00:11:29,210
but a modest amount of
training on those examples

278
00:11:29,210 --> 00:11:31,070
leads it to do fine.

279
00:11:31,070 --> 00:11:32,780
That's a dataset weakness.

280
00:11:32,780 --> 00:11:34,370
A model weakness
is what we might

281
00:11:34,370 --> 00:11:36,860
have in the back of our minds
for our adversarial testing,

282
00:11:36,860 --> 00:11:38,235
and this would be
the case where,

283
00:11:38,235 --> 00:11:40,400
even though we have
fine-tuned our system on some

284
00:11:40,400 --> 00:11:43,940
of these challenge examples, its
performance remains really low,

285
00:11:43,940 --> 00:11:46,520
even though the system can
maintain good performance

286
00:11:46,520 --> 00:11:47,680
on the original dataset.

287
00:11:47,680 --> 00:11:50,180
And this is just like there is
something special about these

288
00:11:50,180 --> 00:11:53,682
new examples and the model
simply cannot get traction.

289
00:11:53,682 --> 00:11:56,140
And there's a third outcome
that might be really worrisome,

290
00:11:56,140 --> 00:11:58,490
and they would trace this
to kind of like annotation

291
00:11:58,490 --> 00:12:01,350
artifacts or label shift
or something like that.

292
00:12:01,350 --> 00:12:04,310
And that's where, in doing this
fine-tuning on some challenge

293
00:12:04,310 --> 00:12:06,260
examples, we see
degraded performance

294
00:12:06,260 --> 00:12:09,740
on both the original dataset
and the challenge dataset.

295
00:12:09,740 --> 00:12:12,440
And that would show that there
is something fundamentally

296
00:12:12,440 --> 00:12:16,940
confusing about these
adversarial testing examples

297
00:12:16,940 --> 00:12:19,303
that are causing a lot of
problems for the system

298
00:12:19,303 --> 00:12:20,720
that we've developed,
because even

299
00:12:20,720 --> 00:12:25,010
a modest amount of fine tuning
causes kind of consequences

300
00:12:25,010 --> 00:12:26,750
to ripple through
the system that

301
00:12:26,750 --> 00:12:31,093
are impacting even performance
on the original dataset.

302
00:12:31,093 --> 00:12:32,760
All right to close
out this screen cast,

303
00:12:32,760 --> 00:12:36,150
let me offer you two examples
of interesting adversarial tests

304
00:12:36,150 --> 00:12:39,790
in our field beginning with the
SQUaD question answering data

305
00:12:39,790 --> 00:12:40,290
set.

306
00:12:40,290 --> 00:12:42,750
I showed you this
leaderboard from SQUaD 2.0

307
00:12:42,750 --> 00:12:45,310
at the start of the quarter,
and the funny thing, of course,

308
00:12:45,310 --> 00:12:47,490
is that you have to go
all the way to place 13

309
00:12:47,490 --> 00:12:50,010
on the leaderboard
to find a system that

310
00:12:50,010 --> 00:12:52,960
is worse than our estimate
of human performance.

311
00:12:52,960 --> 00:12:55,470
So we have superhuman
performance on SQUaD,

312
00:12:55,470 --> 00:12:56,820
but what does that really mean?

313
00:12:56,820 --> 00:13:00,120
SQUaD was also the site of one
of the first really systematic

314
00:13:00,120 --> 00:13:02,530
adversarial testing
efforts in our field.

315
00:13:02,530 --> 00:13:05,160
This is from Jia and Liang 2017.

316
00:13:05,160 --> 00:13:06,460
What they did is quite simple.

317
00:13:06,460 --> 00:13:08,100
We begin with SQUaD
examples, where

318
00:13:08,100 --> 00:13:11,040
we have passages and
questions as inputs,

319
00:13:11,040 --> 00:13:14,430
and the system task is
to answer the question,

320
00:13:14,430 --> 00:13:16,050
and we can count
on the answer being

321
00:13:16,050 --> 00:13:18,210
a literal substring
in the passage

322
00:13:18,210 --> 00:13:20,460
that the system was given.

323
00:13:20,460 --> 00:13:22,560
The adversarial thing
that Jia and Liang did

324
00:13:22,560 --> 00:13:24,930
was simply to append
misleading sentences

325
00:13:24,930 --> 00:13:26,940
to the ends of those passages.

326
00:13:26,940 --> 00:13:29,490
And what they found is that
systems were systematically

327
00:13:29,490 --> 00:13:31,290
misled by those final sentences.

328
00:13:31,290 --> 00:13:33,360
Whereas humans could
easily ignore them,

329
00:13:33,360 --> 00:13:35,940
systems were now
inclined to answer

330
00:13:35,940 --> 00:13:39,920
drawing on information from
those misleading new sentences.

331
00:13:39,920 --> 00:13:41,670
And this is kind of
an interesting dynamic

332
00:13:41,670 --> 00:13:44,850
because you might think, well,
we'll just train our system now

333
00:13:44,850 --> 00:13:47,940
on passages that have these
augmented misleading sentences,

334
00:13:47,940 --> 00:13:50,700
and then surely our systems
will be more robust.

335
00:13:50,700 --> 00:13:52,140
That might be true
in some sense,

336
00:13:52,140 --> 00:13:53,557
but, of course,
we could then just

337
00:13:53,557 --> 00:13:55,890
append sentences to the
start of the passages,

338
00:13:55,890 --> 00:13:58,230
and Jia and Liang found
that systems were now

339
00:13:58,230 --> 00:14:01,650
confused by the appended
initial sentences,

340
00:14:01,650 --> 00:14:04,470
and they started to give wrong
answers in that mode, as well.

341
00:14:04,470 --> 00:14:06,120
And you could kind
of go back and forth

342
00:14:06,120 --> 00:14:10,260
in this adversarial mode showing
that systems were worrisomely

343
00:14:10,260 --> 00:14:14,325
easy to trick based on
these simple appending

344
00:14:14,325 --> 00:14:16,800
of misleading sentences.

345
00:14:16,800 --> 00:14:21,090
Now, that's very interesting
about adversarial evaluation

346
00:14:21,090 --> 00:14:21,810
mode.

347
00:14:21,810 --> 00:14:24,540
What I think is more
interesting about the outcomes

348
00:14:24,540 --> 00:14:26,880
is that they begin to
show us just how different

349
00:14:26,880 --> 00:14:28,830
adversarial testing can be.

350
00:14:28,830 --> 00:14:31,650
So here's the SQUaD
leaderboard, the original,

351
00:14:31,650 --> 00:14:34,470
at the time of the paper,
as well as the results

352
00:14:34,470 --> 00:14:35,610
of this adversarial test.

353
00:14:35,610 --> 00:14:37,860
And you can see, first of
all, that system performance

354
00:14:37,860 --> 00:14:38,920
has really plummeted.

355
00:14:38,920 --> 00:14:42,150
So this turns out to be highly
adversarial to these systems,

356
00:14:42,150 --> 00:14:43,500
for whatever reason.

357
00:14:43,500 --> 00:14:46,500
I think it's more interesting
to note that the system ranking

358
00:14:46,500 --> 00:14:48,180
has really been mixed up.

359
00:14:48,180 --> 00:14:52,090
So it's not like the systems
uniformly drop in performance.

360
00:14:52,090 --> 00:14:54,070
Now, as we move from
the original rank

361
00:14:54,070 --> 00:14:57,210
to the adversarial rank, we
have the first place system

362
00:14:57,210 --> 00:14:58,590
is now in place five.

363
00:14:58,590 --> 00:15:01,800
The second has dropped
all the way to place 10,

364
00:15:01,800 --> 00:15:04,380
but the seventh place system
is now in first place.

365
00:15:04,380 --> 00:15:06,960
It looks kind of chaotic.

366
00:15:06,960 --> 00:15:09,750
Here's a scatterplot where
we have the original system

367
00:15:09,750 --> 00:15:12,840
performance along the x-axis
and the adversarial system

368
00:15:12,840 --> 00:15:14,610
performance along the y-axis.

369
00:15:14,610 --> 00:15:16,360
And you can see that
it's kind of chaotic.

370
00:15:16,360 --> 00:15:18,280
There's no way that
one predicts the other.

371
00:15:18,280 --> 00:15:20,820
So something very
interesting has happened,

372
00:15:20,820 --> 00:15:23,010
and that's noteworthy
because it looks

373
00:15:23,010 --> 00:15:25,920
like that's meaningfully
different from what we do when

374
00:15:25,920 --> 00:15:27,540
we do standard evaluations.

375
00:15:27,540 --> 00:15:30,510
I don't have direct
evidence of this from SQUaD,

376
00:15:30,510 --> 00:15:34,140
but here's a case where people
took two classic image datasets

377
00:15:34,140 --> 00:15:36,600
and simply created new
test sets according

378
00:15:36,600 --> 00:15:40,320
to the same protocols that were
used for the original dataset

379
00:15:40,320 --> 00:15:41,640
and test sets.

380
00:15:41,640 --> 00:15:44,520
And what you find is a
very strong correlation.

381
00:15:44,520 --> 00:15:46,380
Even though the
examples are new,

382
00:15:46,380 --> 00:15:49,200
because it's the same
protocol, system performance

383
00:15:49,200 --> 00:15:52,050
is highly predictive, in the
sense that the original test

384
00:15:52,050 --> 00:15:55,320
accuracy is perfectly
correlated with accuracy

385
00:15:55,320 --> 00:15:56,730
on these new testsets.

386
00:15:56,730 --> 00:15:58,920
Very different from
this adversarial mode

387
00:15:58,920 --> 00:16:01,500
where something much
more chaotic happened.

388
00:16:01,500 --> 00:16:04,710
So adversarial testing is
meaningfully different it seems

389
00:16:04,710 --> 00:16:08,270
from standard evaluations.

390
00:16:08,270 --> 00:16:09,510
Let's move to NLI now.

391
00:16:09,510 --> 00:16:11,180
This will tell us
two different lessons

392
00:16:11,180 --> 00:16:13,700
about how adversarial
testing can be informative.

393
00:16:13,700 --> 00:16:15,200
So we saw at the
start of the course

394
00:16:15,200 --> 00:16:16,970
that we now have
superhuman performance

395
00:16:16,970 --> 00:16:18,860
on the SNLI data set.

396
00:16:18,860 --> 00:16:20,870
That's certainly noteworthy.

397
00:16:20,870 --> 00:16:23,060
And we're reaching
superhuman performance

398
00:16:23,060 --> 00:16:24,485
on the MultiNLI testset.

399
00:16:24,485 --> 00:16:26,360
We're sure to be there,
if we're not already,

400
00:16:26,360 --> 00:16:28,880
at the time of this screencast.

401
00:16:28,880 --> 00:16:31,640
But we've also seen that
systems that perform really

402
00:16:31,640 --> 00:16:35,570
well on these datasets are often
susceptible to adversaries.

403
00:16:35,570 --> 00:16:37,160
In the first screen
cast I showed you

404
00:16:37,160 --> 00:16:40,010
these examples from Glockner,
et al.'s, Breaking NLI

405
00:16:40,010 --> 00:16:42,830
paper, where they make
simple modifications

406
00:16:42,830 --> 00:16:46,010
to these hypotheses and
find that systems do not

407
00:16:46,010 --> 00:16:49,430
behave systematically with
respect to human intuitions

408
00:16:49,430 --> 00:16:51,260
about the modified examples.

409
00:16:51,260 --> 00:16:53,780
And that's the worrisome
part, when they quantify that,

410
00:16:53,780 --> 00:16:55,820
of course, and that you
can see that the best

411
00:16:55,820 --> 00:16:59,750
systems at the time up here
were doing pretty well on SNLI,

412
00:16:59,750 --> 00:17:02,510
and their performance plummeted
on these new test sets,

413
00:17:02,510 --> 00:17:04,520
showing that this was,
for whatever reason,

414
00:17:04,520 --> 00:17:08,000
truly adversarial when it
comes to those systems.

415
00:17:08,000 --> 00:17:10,670
But I also presented this
as a story of progress,

416
00:17:10,670 --> 00:17:13,430
right, because I showed you
that in simply downloading

417
00:17:13,430 --> 00:17:16,310
RoBERTA-multi-NLI,
that is, RoBERTA fine

418
00:17:16,310 --> 00:17:18,800
tuned on the MultiNLI
data set, you now

419
00:17:18,800 --> 00:17:20,810
have a device
where, with no work,

420
00:17:20,810 --> 00:17:23,598
you can essentially solve
this adversarial test.

421
00:17:23,598 --> 00:17:25,999
I think that's really striking,
and it points to the fact

422
00:17:26,000 --> 00:17:28,790
that RoBERTA, unlike
those earlier models,

423
00:17:28,790 --> 00:17:31,400
might truly have
systematic understanding

424
00:17:31,400 --> 00:17:33,830
of the relevant kinds
of lexical relationships

425
00:17:33,830 --> 00:17:37,610
that you need to solve
this adversarial test set.

426
00:17:37,610 --> 00:17:39,860
So that's an exciting outcome.

427
00:17:39,860 --> 00:17:41,750
Here's a second outcome
that you might see,

428
00:17:41,750 --> 00:17:45,800
and this is from the Naik,et al.
paper that does a wide battery

429
00:17:45,800 --> 00:17:49,970
of different adversarial
tests on multi-NLI data.

430
00:17:49,970 --> 00:17:52,310
They did a bunch of
things like antonyms--

431
00:17:52,310 --> 00:17:54,260
"I love the Cinderella
story" contradicts

432
00:17:54,260 --> 00:17:56,390
"I hate the Cinderella
story" just drawing

433
00:17:56,390 --> 00:17:57,770
on lexical knowledge.

434
00:17:57,770 --> 00:17:59,840
They asked about
numerical reasoning

435
00:17:59,840 --> 00:18:01,653
across these two premises.

436
00:18:01,653 --> 00:18:04,070
Word overlap, and this is a
little bit different, in that,

437
00:18:04,070 --> 00:18:06,260
you're doing something like
just inserting material

438
00:18:06,260 --> 00:18:08,470
that you might think is
going to be distracting

439
00:18:08,470 --> 00:18:10,855
in the mode of the
SQUaD adversary,

440
00:18:10,855 --> 00:18:12,230
and seeing what
effects that has,

441
00:18:12,230 --> 00:18:13,760
and the same thing for negation.

442
00:18:13,760 --> 00:18:16,490
Adding on to the end
information that's

443
00:18:16,490 --> 00:18:18,110
going to be misleading
for a system

444
00:18:18,110 --> 00:18:20,660
and also includes a lot
of negation elements.

445
00:18:20,660 --> 00:18:23,240
There are a few other modes
that I didn't have space for.

446
00:18:23,240 --> 00:18:26,450
It's a very rich paper with
a very fine-grained breakdown

447
00:18:26,450 --> 00:18:31,320
of how systems do on these
different adversarial problems.

448
00:18:31,320 --> 00:18:34,190
Here's a picture of the
dataset and here's a breakdown,

449
00:18:34,190 --> 00:18:36,080
and I think the
overall takeaway is

450
00:18:36,080 --> 00:18:38,030
that the numbers
across the board

451
00:18:38,030 --> 00:18:41,148
are very low on
these adversaries.

452
00:18:41,148 --> 00:18:43,190
And so that's interesting,
and it looks like even

453
00:18:43,190 --> 00:18:45,680
top performing
systems from multi-NLI

454
00:18:45,680 --> 00:18:47,480
are stumbling with
these problems

455
00:18:47,480 --> 00:18:50,120
that we surely want our
systems to be able to solve,

456
00:18:50,120 --> 00:18:53,940
if we're going to call them
true common sense reasoners.

457
00:18:53,940 --> 00:18:55,550
However, this was
actually the basis

458
00:18:55,550 --> 00:18:57,950
for a number of the
experiments in that Inoculation

459
00:18:57,950 --> 00:19:01,490
By Fine Tuning paper that
I quoted from before.

460
00:19:01,490 --> 00:19:04,460
Here's a kind of rough picture
of the performance results

461
00:19:04,460 --> 00:19:06,950
that they report on
different subsets

462
00:19:06,950 --> 00:19:09,860
of that adversarial
test on MultiNLI.

463
00:19:09,860 --> 00:19:12,620
And you can see that it shows
all the different outcomes

464
00:19:12,620 --> 00:19:14,960
that we discussed under
the heading of Inoculation

465
00:19:14,960 --> 00:19:16,280
By Fine Tuning.

466
00:19:16,280 --> 00:19:19,830
To simplify things, just
focus in on the green lines.

467
00:19:19,830 --> 00:19:22,820
The ones with dots are the
original system performance,

468
00:19:22,820 --> 00:19:26,460
and the ones with crosses
are on the challenge dataset.

469
00:19:26,460 --> 00:19:29,870
So this first column here is
identifying dataset weaknesses.

470
00:19:29,870 --> 00:19:32,150
What you're seeing is
that, as we fine-tune

471
00:19:32,150 --> 00:19:34,610
on more examples from
the challenge dataset

472
00:19:34,610 --> 00:19:38,450
going along the x-axis
here, we very quickly

473
00:19:38,450 --> 00:19:41,190
get a system that's actually
good at this challenge problem,

474
00:19:41,190 --> 00:19:41,690
right.

475
00:19:41,690 --> 00:19:44,150
After just about 50
examples, the system

476
00:19:44,150 --> 00:19:48,020
is, basically, learning to solve
the word overlap and negation

477
00:19:48,020 --> 00:19:48,590
problem.

478
00:19:48,590 --> 00:19:52,410
So the negation takes a little
bit longer at 400 examples.

479
00:19:52,410 --> 00:19:55,250
So that's a case where the
original systems were failing,

480
00:19:55,250 --> 00:19:57,980
not because of any intrinsic
property of the models being

481
00:19:57,980 --> 00:19:59,780
used, but rather
because the data

482
00:19:59,780 --> 00:20:01,700
clearly just didn't
have enough information

483
00:20:01,700 --> 00:20:04,520
to resolve these
learning targets.

484
00:20:04,520 --> 00:20:07,045
We also see outcome two,
which is a model weakness.

485
00:20:07,045 --> 00:20:09,170
Again, follow the green
lines, and you can see here

486
00:20:09,170 --> 00:20:12,050
that, for spelling errors
and length mismatch,

487
00:20:12,050 --> 00:20:15,650
no amount of fine tuning
on challenge examples

488
00:20:15,650 --> 00:20:18,050
helps these systems get
traction on these problems.

489
00:20:18,050 --> 00:20:19,457
That's these flat lines here.

490
00:20:19,457 --> 00:20:22,040
And that's showing that there's
something fundamentally wrong,

491
00:20:22,040 --> 00:20:23,210
possibly, with these models.

492
00:20:23,210 --> 00:20:26,960
They are just unable to solve
these two challenge problems.

493
00:20:26,960 --> 00:20:29,150
And then we also see
the third artifact.

494
00:20:29,150 --> 00:20:30,830
For numerical
reasoning, you'll notice

495
00:20:30,830 --> 00:20:33,572
that system performance is
kind of really chaotic here,

496
00:20:33,572 --> 00:20:35,030
and that's suggesting
that there is

497
00:20:35,030 --> 00:20:37,100
something importantly
and problematically

498
00:20:37,100 --> 00:20:40,910
different about these challenge
examples because fine tuning

499
00:20:40,910 --> 00:20:43,610
on them causes the
system to become really

500
00:20:43,610 --> 00:20:46,730
chaotic in its predictions, and
we get degraded performance,

501
00:20:46,730 --> 00:20:49,370
not only on the
challenge set, but also

502
00:20:49,370 --> 00:20:50,720
on the original dataset.

503
00:20:50,720 --> 00:20:53,820
showing that we've done
something quite disruptive.

504
00:20:53,820 --> 00:20:55,820
So this is really interesting
that in this case,

505
00:20:55,820 --> 00:20:57,950
from this battery of
adversarial tests,

506
00:20:57,950 --> 00:21:00,290
we see all these different
outcomes pointing us

507
00:21:00,290 --> 00:21:02,120
to all sorts of
different lessons

508
00:21:02,120 --> 00:21:05,300
about what action we should
take to make these systems more

509
00:21:05,300 --> 00:21:06,850
robust.

510
00:21:06,850 --> 00:21:12,000


