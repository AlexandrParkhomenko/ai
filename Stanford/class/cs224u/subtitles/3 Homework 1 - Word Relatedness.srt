1
00:00:05,040 --> 00:00:08,559
hello everyone this screencast is going

2
00:00:06,480 --> 00:00:10,320
to be a brief playthrough of homework

3
00:00:08,558 --> 00:00:11,919
one on word relatedness i hope to give

4
00:00:10,320 --> 00:00:14,000
you a sense for the problem that you're

5
00:00:11,919 --> 00:00:15,359
tackling and also our expectations

6
00:00:14,000 --> 00:00:18,079
around the homework questions and the

7
00:00:18,079 --> 00:00:20,959
the overview is just explaining the

8
00:00:19,519 --> 00:00:22,320
character of this problem which is

9
00:00:20,960 --> 00:00:24,960
essentially that we're going to give you

10
00:00:22,320 --> 00:00:26,560
a development set of word pairs with

11
00:00:26,559 --> 00:00:30,079
relatedness the scores were produced by

12
00:00:28,320 --> 00:00:31,839
humans and we've just scaled them into

13
00:00:31,839 --> 00:00:36,159
where a larger score means more related

14
00:00:33,759 --> 00:00:37,759
in this human sense and your task is

15
00:00:36,159 --> 00:00:39,199
essentially to develop a system that

16
00:00:37,759 --> 00:00:41,439
will predict scores that are highly

17
00:00:39,200 --> 00:00:43,120
correlated with those human scores

18
00:00:41,439 --> 00:00:45,039
according to the spearman correlation

19
00:00:43,119 --> 00:00:47,599
coefficient which is the traditional

20
00:00:47,600 --> 00:00:51,280
so this is just some setup stuff for the

21
00:00:49,439 --> 00:00:52,718
environment and then we introduce the

22
00:00:51,280 --> 00:00:54,960
development set itself which is a

23
00:00:52,719 --> 00:00:57,039
panda's data frame it's loaded in from

24
00:00:57,039 --> 00:01:00,719
and it looks like this it's got a bunch

25
00:00:58,640 --> 00:01:02,320
of word pairs each with scores as i said

26
00:01:00,719 --> 00:01:04,719
before these are the human provided

27
00:01:02,320 --> 00:01:06,400
scores this is a development data set in

28
00:01:04,719 --> 00:01:08,158
the sense that you can make whatever use

29
00:01:06,400 --> 00:01:10,478
you want of it you can train systems you

30
00:01:08,159 --> 00:01:12,479
can explore your results and so forth

31
00:01:10,478 --> 00:01:14,239
because as you'll see for the actual

32
00:01:12,478 --> 00:01:16,560
bake off we have a fresh test set that

33
00:01:16,560 --> 00:01:19,759
there are about 5 000 words in this

34
00:01:19,759 --> 00:01:24,239
um you can train on any subset and you

35
00:01:22,000 --> 00:01:26,079
can expand the data set to other things

36
00:01:24,239 --> 00:01:28,000
if you want to include them as well it's

37
00:01:26,079 --> 00:01:29,840
really up to you to decide what you want

38
00:01:28,000 --> 00:01:31,920
to do because this is all about making

39
00:01:29,840 --> 00:01:33,280
predictions on that brand new test set

40
00:01:33,280 --> 00:01:37,359
and i will just say that the test set

41
00:01:34,719 --> 00:01:38,478
has 1500 word pairs with scores of the

42
00:01:38,478 --> 00:01:42,399
and in terms of the overlap i'll also

43
00:01:40,159 --> 00:01:44,719
tell you no word pair in this

44
00:01:42,399 --> 00:01:46,320
development set is in the test set so

45
00:01:44,719 --> 00:01:48,319
it's disjoint at the level of these

46
00:01:46,319 --> 00:01:50,078
pairs but some of the individual words

47
00:01:48,319 --> 00:01:53,438
are repeated in the test sets you do

48
00:01:53,438 --> 00:01:57,039
in this code here we load the full

49
00:01:55,118 --> 00:01:58,560
vocabulary for this thing which is you

50
00:01:57,040 --> 00:01:59,600
know all the words appearing in all the

51
00:01:59,599 --> 00:02:03,359
the vocabulary for the bake off test is

52
00:02:01,759 --> 00:02:06,000
different it's partially overlapping

53
00:02:03,359 --> 00:02:08,080
with the above as i said now if you

54
00:02:06,000 --> 00:02:09,840
wanted to make sure ahead of time that

55
00:02:08,080 --> 00:02:11,680
your system has a representation for

56
00:02:09,840 --> 00:02:13,759
every word in both the dev and the test

57
00:02:11,680 --> 00:02:15,920
sets then you can check against the

58
00:02:13,759 --> 00:02:18,159
vocabularies in any of the vector space

59
00:02:15,919 --> 00:02:19,119
models that we've distributed with this

60
00:02:19,120 --> 00:02:23,200
so for example if you ran this code you

61
00:02:20,878 --> 00:02:24,959
get the full task vocabulary and if you

62
00:02:23,199 --> 00:02:26,799
have a representation for every word in

63
00:02:24,959 --> 00:02:30,400
there then you're good shape in good

64
00:02:26,800 --> 00:02:30,400
shape when it comes to the test set

65
00:02:31,280 --> 00:02:34,719
it's also useful to look at the score

66
00:02:32,878 --> 00:02:36,000
distribution this will give you a sense

67
00:02:34,719 --> 00:02:37,439
for what kind of space you're making

68
00:02:36,000 --> 00:02:39,360
predictions into and i'll give you the

69
00:02:37,439 --> 00:02:43,759
hint that the test distribution looks an

70
00:02:39,360 --> 00:02:45,280
awful lot like this dev set distribution

71
00:02:43,759 --> 00:02:47,598
it's also worth being aware that there

72
00:02:45,280 --> 00:02:49,919
are some repeated pairs in the training

73
00:02:47,598 --> 00:02:51,439
set some words that have different

74
00:02:49,919 --> 00:02:53,359
scores associated with them and they're

75
00:02:51,439 --> 00:02:54,719
repeated therefore what i've done here

76
00:02:53,360 --> 00:02:56,879
is just provide you with some code that

77
00:02:54,719 --> 00:02:59,199
will allow you to rank pairs of words by

78
00:02:56,878 --> 00:03:01,598
the variance in their scores and so you

79
00:02:59,199 --> 00:03:03,679
could decide for yourself what you want

80
00:03:01,598 --> 00:03:05,919
to do about these minor inconsistencies

81
00:03:03,680 --> 00:03:08,400
you could filter the data set or keep

82
00:03:05,919 --> 00:03:10,079
all of these examples in it's entirely

83
00:03:08,400 --> 00:03:11,680
up to you i will just say that the test

84
00:03:10,080 --> 00:03:15,680
set does not force you to confront this

85
00:03:11,680 --> 00:03:15,680
issue it has no repeated pairs in it

86
00:03:15,759 --> 00:03:18,878
all right and then we come to the

87
00:03:17,039 --> 00:03:20,400
evaluation topic here so there's a

88
00:03:18,878 --> 00:03:22,318
central function you'll be using a lot

89
00:03:20,400 --> 00:03:24,480
in the homework and the bake off word

90
00:03:24,479 --> 00:03:28,079
and so there are some instructions about

91
00:03:26,318 --> 00:03:29,679
how the interface works let me just give

92
00:03:29,680 --> 00:03:35,120
in this cell i'm loading in one of our

93
00:03:31,360 --> 00:03:36,959
count matrices it's the giga5 matrix

94
00:03:35,120 --> 00:03:39,439
and i'm going to evaluate that directly

95
00:03:36,959 --> 00:03:41,759
so you can see in the next cell that

96
00:03:39,439 --> 00:03:44,479
word relatedness evaluation takes in our

97
00:03:41,759 --> 00:03:46,399
development data for our test data

98
00:03:44,479 --> 00:03:48,079
and whatever vector space model you've

99
00:03:46,400 --> 00:03:51,200
developed as its two arguments and it

100
00:03:48,080 --> 00:03:52,879
returns a new version of this input here

101
00:03:51,199 --> 00:03:55,759
with a column for the predictions you

102
00:03:52,878 --> 00:03:57,280
made as well as this value here which is

103
00:03:57,280 --> 00:04:01,280
coefficient that's our primary metric

104
00:04:02,239 --> 00:04:06,158
is the score that i achieved not so good

105
00:04:04,479 --> 00:04:07,919
i'm sure you'll be able to do better and

106
00:04:06,158 --> 00:04:09,759
here's a look at the new account data

107
00:04:07,919 --> 00:04:13,280
frame with that new predict column of

108
00:04:13,280 --> 00:04:16,879
and this is just another baseline here a

109
00:04:15,039 --> 00:04:18,879
truly random system that just predicts a

110
00:04:16,879 --> 00:04:20,798
random score and that's uh even worse

111
00:04:18,879 --> 00:04:22,319
than the simple count baseline again

112
00:04:20,798 --> 00:04:24,560
you'll be able to do much better without

113
00:04:24,560 --> 00:04:27,600
error analysis i've provided you with

114
00:04:26,160 --> 00:04:29,199
some functions that will allow you to

115
00:04:27,600 --> 00:04:31,520
look at what your system is doing in

116
00:04:29,199 --> 00:04:33,439
terms of the best predictions in terms

117
00:04:31,519 --> 00:04:35,039
of comparing against the human scores

118
00:04:33,439 --> 00:04:36,399
and the worst predictions and i am

119
00:04:35,040 --> 00:04:38,000
imagining that this might help you

120
00:04:36,399 --> 00:04:39,439
figure out where you're doing well and

121
00:04:38,000 --> 00:04:42,478
where you're doing poorly and then you

122
00:04:42,478 --> 00:04:46,478
that brings us to the homework questions

123
00:04:44,000 --> 00:04:48,959
so what we're trying to do here is help

124
00:04:46,478 --> 00:04:51,199
you establish some baseline systems get

125
00:04:48,959 --> 00:04:52,959
used to the code and also think in new

126
00:04:51,199 --> 00:04:54,240
and creative ways about the underlying

127
00:04:54,240 --> 00:04:58,720
our first one is positive point wise

128
00:04:56,560 --> 00:05:00,160
mutual information as a baseline as

129
00:04:58,720 --> 00:05:02,560
you've seen in the materials for this

130
00:05:00,160 --> 00:05:04,639
unit point wise mutual information is a

131
00:05:02,560 --> 00:05:06,959
very strong baseline for lots of

132
00:05:04,639 --> 00:05:09,120
different applications and it also

133
00:05:06,959 --> 00:05:10,799
embodies a kind of core insight that we

134
00:05:09,120 --> 00:05:12,079
see running through a lot of the methods

135
00:05:12,079 --> 00:05:14,959
so it's a natural and pretty strong

136
00:05:13,519 --> 00:05:18,799
baseline and what we're asking you to do

137
00:05:14,959 --> 00:05:20,478
here is simply establish that baseline

138
00:05:18,800 --> 00:05:21,280
here and throughout all of the work for

139
00:05:21,279 --> 00:05:24,638
course we're going to ask you to

140
00:05:22,720 --> 00:05:26,320
implement things and in general we will

141
00:05:24,639 --> 00:05:27,600
provide you with test functions that

142
00:05:26,319 --> 00:05:29,360
will help you make sure you have

143
00:05:27,600 --> 00:05:32,320
iterated towards the solution that we're

144
00:05:29,360 --> 00:05:34,160
looking for so you can feel rest assured

145
00:05:32,319 --> 00:05:35,360
that if you have meaningfully passed

146
00:05:35,360 --> 00:05:38,400
then you'll do well in terms of the

147
00:05:36,720 --> 00:05:41,600
overall evaluation and your code is

148
00:05:41,918 --> 00:05:45,680
next question is similar so again now

149
00:05:43,759 --> 00:05:47,600
we're exploring latent semantic analysis

150
00:05:45,680 --> 00:05:49,280
and in particular we're asking you to

151
00:05:47,600 --> 00:05:51,280
build up some code that will allow you

152
00:05:49,279 --> 00:05:53,599
to test different dimensionalities for a

153
00:05:51,279 --> 00:05:55,918
given vector space input and try to get

154
00:05:55,918 --> 00:05:58,959
so again you have to implement a

155
00:05:57,360 --> 00:06:00,080
function and then there's a test that

156
00:05:58,959 --> 00:06:01,758
will help you make sure that you've

157
00:06:00,079 --> 00:06:03,519
implemented the correct function in case

158
00:06:01,759 --> 00:06:06,479
there's any uncertainty in the

159
00:06:06,478 --> 00:06:10,318
next question as i mentioned in the

160
00:06:08,160 --> 00:06:12,720
lectures t-test free weighting is a very

161
00:06:10,319 --> 00:06:14,319
powerful re-weighting scheme it has some

162
00:06:12,720 --> 00:06:16,400
affinities with point-wise mutual

163
00:06:14,319 --> 00:06:18,080
information but it is different

164
00:06:16,399 --> 00:06:20,000
uh and this question is just asking you

165
00:06:18,079 --> 00:06:21,439
to implement that rewriting function

166
00:06:20,000 --> 00:06:24,800
we've given the instructions here you

167
00:06:21,439 --> 00:06:26,639
might also look in vsm.pi the module at

168
00:06:24,800 --> 00:06:28,478
the implementation of pointwise mutual

169
00:06:26,639 --> 00:06:30,160
information because you could adopt some

170
00:06:30,160 --> 00:06:33,919
i want to emphasize that you don't need

171
00:06:31,680 --> 00:06:35,600
the fastest possible implementation any

172
00:06:33,918 --> 00:06:38,639
working implementation will get full

173
00:06:35,600 --> 00:06:40,319
credit but the code in bsm.pi is really

174
00:06:38,639 --> 00:06:41,840
nicely optimized in terms of its

175
00:06:40,319 --> 00:06:42,800
implementation so you might want to push

176
00:06:42,800 --> 00:06:46,720
to do something similarly efficient

177
00:06:46,720 --> 00:06:52,000
as long as your function t test here

178
00:06:48,560 --> 00:06:53,360
passes this test you're in good shape

179
00:06:52,000 --> 00:06:54,560
and you don't need to evaluate this

180
00:06:53,360 --> 00:06:56,240
function we're just asking you to

181
00:06:54,560 --> 00:06:57,199
implement it but we're assuming since

182
00:06:57,199 --> 00:07:00,478
re-weighting scheme that you'll be

183
00:06:58,800 --> 00:07:02,560
curious about how it performs in the

184
00:07:00,478 --> 00:07:04,399
context context of the system you're

185
00:07:04,399 --> 00:07:07,918
all right for the final two questions

186
00:07:05,918 --> 00:07:09,918
we're asking you to think further afield

187
00:07:07,918 --> 00:07:12,159
pooled bert representations is drawing

188
00:07:09,918 --> 00:07:14,478
on the material in this notebook here

189
00:07:12,160 --> 00:07:17,280
which is just an exploration of the

190
00:07:14,478 --> 00:07:19,360
ideas from bamasani in all 2020 on how

191
00:07:17,279 --> 00:07:20,959
to derive static representations from

192
00:07:20,959 --> 00:07:24,638
and so what we've got here is some

193
00:07:22,560 --> 00:07:26,319
starter code for you and a kind of

194
00:07:24,639 --> 00:07:28,879
skeleton for implementing your own

195
00:07:28,879 --> 00:07:32,240
again we're hoping that this is a

196
00:07:30,240 --> 00:07:33,038
foundation for further exploration for

197
00:07:33,038 --> 00:07:36,800
we've got the implementation to do here

198
00:07:35,279 --> 00:07:38,239
and then a test that you can pass to

199
00:07:36,800 --> 00:07:42,000
make sure that you've implemented things

200
00:07:38,240 --> 00:07:43,519
according to the design specification

201
00:07:42,000 --> 00:07:45,439
the final question is also really

202
00:07:43,519 --> 00:07:47,839
exploratory it's called learn distance

203
00:07:45,439 --> 00:07:49,598
functions the idea here is that much of

204
00:07:47,839 --> 00:07:51,439
the code in this notebook pushes you to

205
00:07:49,598 --> 00:07:54,000
think about distance in terms of things

206
00:07:51,439 --> 00:07:55,120
like cosine distance or euclidean

207
00:07:55,120 --> 00:07:59,280
but we should have in mind that the only

208
00:07:56,639 --> 00:08:01,199
formal requirement is that you have some

209
00:07:59,279 --> 00:08:03,279
function that will map a pair of vectors

210
00:08:03,279 --> 00:08:06,159
as soon as you see things from that

211
00:08:04,639 --> 00:08:08,240
perspective you realize that a whole

212
00:08:06,160 --> 00:08:09,680
world of options opens up to you and

213
00:08:08,240 --> 00:08:11,759
what this question is asking you to do

214
00:08:09,680 --> 00:08:13,759
is train a k nearest neighbors model on

215
00:08:11,759 --> 00:08:16,080
the development data that will learn to

216
00:08:13,759 --> 00:08:18,479
predict scores and then you can use that

217
00:08:16,079 --> 00:08:19,918
in place of cosine or euclidean we've

218
00:08:18,478 --> 00:08:21,918
walked you through how to implement that

219
00:08:19,918 --> 00:08:23,758
there's a bunch of guidance here and a

220
00:08:21,918 --> 00:08:25,198
few tests for the sub components if you

221
00:08:25,199 --> 00:08:28,160
again if the test pass you should be in

222
00:08:26,720 --> 00:08:29,919
good shape we're not asking you to

223
00:08:29,918 --> 00:08:33,038
but we're hoping that this is a

224
00:08:31,120 --> 00:08:36,479
foundation for for exploring what could

225
00:08:33,038 --> 00:08:38,879
be a quite productive avenue of

226
00:08:38,879 --> 00:08:42,320
and then finally the original system

227
00:08:40,639 --> 00:08:44,480
this is worth three points this is a big

228
00:08:42,320 --> 00:08:46,320
deal here you can piece together any

229
00:08:44,480 --> 00:08:48,320
part of what you've done previously all

230
00:08:46,320 --> 00:08:50,399
that stuff is fair game you can think in

231
00:08:48,320 --> 00:08:52,000
entirely original and new ways you can

232
00:08:50,399 --> 00:08:53,519
do something simple you can do something

233
00:08:53,519 --> 00:08:56,480
what we'd like you to do here is not

234
00:08:54,799 --> 00:08:58,639
only provide the implementation in the

235
00:08:56,480 --> 00:09:00,399
scope of this conditional so that it

236
00:08:58,639 --> 00:09:02,000
doesn't cause the autograder to fail if

237
00:09:00,399 --> 00:09:03,839
you have special requirements and so

238
00:09:02,000 --> 00:09:05,839
forth but we're also looking for a

239
00:09:05,839 --> 00:09:10,000
uh and a report on what your highest

240
00:09:10,000 --> 00:09:13,519
the idea here is that at the end of the

241
00:09:11,600 --> 00:09:16,080
bake off the teaching team will create a

242
00:09:13,519 --> 00:09:18,080
report that kind of analyzes across all

243
00:09:16,080 --> 00:09:20,160
the different submissions and reflects

244
00:09:18,080 --> 00:09:22,560
back to you all what worked and what

245
00:09:20,159 --> 00:09:24,159
didn't and as part of that effort

246
00:09:24,159 --> 00:09:29,278
development scores can really help us

247
00:09:25,839 --> 00:09:30,880
understand um how things played out

248
00:09:29,278 --> 00:09:32,320
and that brings us to the bake off so

249
00:09:30,879 --> 00:09:34,240
for the bake off what you really need to

250
00:09:32,320 --> 00:09:35,600
do is just run this function create bake

251
00:09:36,958 --> 00:09:41,599
your vector space model here it's my

252
00:09:39,120 --> 00:09:43,039
simple one count df that i loaded before

253
00:09:41,600 --> 00:09:44,800
and as a reminder that this is an

254
00:09:43,039 --> 00:09:47,278
important piece you almost also need to

255
00:09:47,278 --> 00:09:50,958
so the idea is that here my bake off

256
00:09:49,120 --> 00:09:53,278
would be the simple submission where i'm

257
00:09:50,958 --> 00:09:55,599
just doing account data frame and

258
00:09:53,278 --> 00:09:59,039
euclidean as my distance and when i run

259
00:09:55,600 --> 00:10:01,360
this function it creates a file cs224u

260
00:09:59,039 --> 00:10:03,199
word relatedness fakeoff entry and

261
00:10:01,360 --> 00:10:04,800
you'll just upload that to gradescope

262
00:10:03,200 --> 00:10:07,040
we'll give some instructions about that

263
00:10:04,799 --> 00:10:08,719
later on and that will be evaluated by

