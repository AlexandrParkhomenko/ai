1
00:00:04,720 --> 00:00:06,480
приветствую всех, это седьмая часть

2
00:00:06,480 --> 00:00:08,080
нашей серии статей о контролируемом анализе настроений.

3
00:00:08,080 --> 00:00:10,240
Основное внимание в этом скринкасте

4
00:00:10,240 --> 00:00:12,400
уделяется представлению признаков данных. На

5
00:00:12,400 --> 00:00:14,400
самом деле я хотел бы сделать две вещи: сначала

6
00:00:14,400 --> 00:00:16,400
изучить некоторые идеи для эффективного

7
00:00:16,400 --> 00:00:18,080
представления признаков в контексте

8
00:00:18,080 --> 00:00:20,640
анализа настроений и  во-вторых, рассмотрим некоторые

9
00:00:20,640 --> 00:00:22,480
из основных технических концепций,

10
00:00:22,480 --> 00:00:24,640
связанных с представлением функций, которые

11
00:00:24,640 --> 00:00:26,560
вам стоит иметь в виду, когда вы пишете новые

12
00:00:26,560 --> 00:00:28,840
функции функций и оптимизируете

13
00:00:28,840 --> 00:00:31,599
модели.

14
00:00:31,599 --> 00:00:34,079


15
00:00:34,079 --> 00:00:35,920


16
00:00:35,920 --> 00:00:38,160
мы просто сосредоточились на

17
00:00:38,160 --> 00:00:39,920
функциональных функциях униграмм, которые также называются

18
00:00:39,920 --> 00:00:42,719
моделью мешка слов, и мы можем легко

19
00:00:42,719 --> 00:00:45,039
обобщить эту идею на биграммы,

20
00:00:45,039 --> 00:00:47,039
триграммы и т. д.

21
00:00:47,039 --> 00:00:48,559
Все эти схемы будут сильно

22
00:00:48,559 --> 00:00:50,239
зависеть от токенизатора, который вы

23
00:00:50,239 --> 00:00:51,920
выбрали из-за  Конечно, в конце

24
00:00:51,920 --> 00:00:54,160
каждого примера, который мы представляем, мы просто

25
00:00:54,160 --> 00:00:55,920
размечаем этот пример, а затем

26
00:00:55,920 --> 00:00:59,680
подсчитываем токены в этом примере.

27
00:00:59,680 --> 00:01:01,199
Конечно, это можно комбинировать с

28
00:01:01,199 --> 00:01:03,440
этапами предварительной обработки во второй части этой

29
00:01:03,440 --> 00:01:05,760
серии статей.

30
00:01:05,760 --> 00:01:08,400


31
00:01:08,400 --> 00:01:11,040


32
00:01:11,040 --> 00:01:13,680


33
00:01:13,680 --> 00:01:16,159
указать, что, например,

34
00:01:16,159 --> 00:01:19,200
добро является положительным в обычном контексте, но

35
00:01:19,200 --> 00:01:21,360
может стать отрицательным, когда оно находится в

36
00:01:21,360 --> 00:01:24,560
области отрицания, например, нет или никогда,

37
00:01:24,560 --> 00:01:26,159
мы обработаем это как шаг предварительной обработки,

38
00:01:26,159 --> 00:01:27,680
и это просто создаст больше

39
00:01:27,680 --> 00:01:30,720
униграмм, которые наш токенизатор превратит

40
00:01:30,720 --> 00:01:33,439
в токены, а затем будут учитываться

41
00:01:33,439 --> 00:01:37,200
этими схемами представления функций.

42
00:01:37,200 --> 00:01:39,439
Отличительной чертой этих подходов к функциям

43
00:01:39,439 --> 00:01:41,439
является то, что они создают очень большие и очень

44
00:01:41,439 --> 00:01:43,360
разреженные представления функций. У вас

45
00:01:43,360 --> 00:01:45,200
будет столбец в вашем представлении функций

46
00:01:45,200 --> 00:01:47,360
для каждого слова,

47
00:01:47,360 --> 00:01:49,200
которое появляется где-либо в ваших обучающих

48
00:01:49,200 --> 00:01:50,479
данных.

49
00:01:50,479 --> 00:01:51,840
и еще одна важная вещь, которую следует

50
00:01:51,840 --> 00:01:53,600
помнить об этом подходе, заключается в том, что в

51
00:01:53,600 --> 00:01:55,680
целом они не смогут напрямую моделировать

52
00:01:55,680 --> 00:01:57,920
отношения между функциями.  если

53
00:01:57,920 --> 00:01:59,439
вы не приложите особых усилий для

54
00:01:59,439 --> 00:02:02,479
эффективного взаимодействия этих функций,

55
00:02:02,479 --> 00:02:03,920
все, что вы будете делать, — это изучать их

56
00:02:03,920 --> 00:02:05,600
распределение по отношению к

57
00:02:05,600 --> 00:02:07,520
имеющимся у вас меткам классов, и очень

58
00:02:07,520 --> 00:02:10,000
маловероятно, что вы каким-либо глубоким

59
00:02:10,000 --> 00:02:12,879
образом восстановите род лежащей в основе синонимии

60
00:02:12,879 --> 00:02:15,360
слов.  например, диван и диван,

61
00:02:15,360 --> 00:02:17,040
и это недостаток, который мы, возможно,

62
00:02:17,040 --> 00:02:19,360
захотим устранить по мере того, как мы переходим к

63
00:02:19,360 --> 00:02:21,680
распределенным представлениям примеров

64
00:02:21,680 --> 00:02:24,400
и глубокому обучению,

65
00:02:24,879 --> 00:02:27,360
поэтому для первой технической концепции я

66
00:02:27,360 --> 00:02:29,599
хотел бы просто различать функции

67
00:02:29,599 --> 00:02:31,680
функций и функции и сделать это

68
00:02:31,680 --> 00:02:33,440
я  я только что получил полностью проработанный

69
00:02:33,440 --> 00:02:35,760
пример с использованием инструментов из scikit-learn, который,

70
00:02:35,760 --> 00:02:37,680
я думаю, сделает важность

71
00:02:37,680 --> 00:02:39,200
этого различия действительно ясным и

72
00:02:39,200 --> 00:02:41,760
конкретным, поэтому в ячейке 1 я только что загрузил

73
00:02:41,760 --> 00:02:44,800
кучу библиотек, в ячейку 2 я  получил мою

74
00:02:44,800 --> 00:02:47,200
стандартную функцию функции ленивых юниграмм,

75
00:02:47,200 --> 00:02:49,440
которая принимает строковый

76
00:02:49,440 --> 00:02:51,519
текст вниз по обложке, а затем просто

77
00:02:51,519 --> 00:02:53,440
разбивает на пробел, а затем

78
00:02:53,440 --> 00:02:55,360
счетчик здесь просто поворачивается  в

79
00:02:55,360 --> 00:02:57,440
словаре учетных записей, отображающем каждый токен

80
00:02:57,440 --> 00:02:59,840
на количество раз, которое он появляется в этом

81
00:02:59,840 --> 00:03:02,879
примере, в соответствии с нашим токенизатором,

82
00:03:02,879 --> 00:03:04,879
который сейчас будет в порядке в ячейке 3, у меня

83
00:03:04,879 --> 00:03:07,280
есть крошечный корпус, в котором есть только два

84
00:03:07,280 --> 00:03:09,360
слова a и b

85
00:03:09,360 --> 00:03:11,440
в ячейке 4, которую я создаю  список

86
00:03:11,440 --> 00:03:14,000
словарей, вызвав unigram шпионить за

87
00:03:14,000 --> 00:03:16,640
каждым текстом в моем корпусе здесь,

88
00:03:16,640 --> 00:03:18,560
чтобы получить список подсчета

89
00:03:18,560 --> 00:03:20,319
словарей

90
00:03:20,319 --> 00:03:22,720
в пяти я использую векторизатор dict, как

91
00:03:22,720 --> 00:03:24,319
описано в предыдущем скринкасте, и

92
00:03:24,319 --> 00:03:25,840
что это собирается делать, так это когда я вызываю

93
00:03:25,840 --> 00:03:28,400
подгонку  Преобразуйте мой список словарей функций,

94
00:03:28,400 --> 00:03:30,159
он превратит его в

95
00:03:30,159 --> 00:03:32,239
матрицу, которая является входом, который все

96
00:03:32,239 --> 00:03:34,400
эти модели машинного обучения scikit

97
00:03:34,400 --> 00:03:36,879
ожидают для своих обучающих данных, и в

98
00:03:36,879 --> 00:03:38,560
ячейке 7 я только что дал вам то, что, я надеюсь,

99
00:03:38,560 --> 00:03:40,480
является довольно интуитивным представлением  эта

100
00:03:40,480 --> 00:03:43,040
матрица дизайна по

101
00:03:43,040 --> 00:03:45,760
сути представляет собой просто массив numpy, но если мы используем pandas, мы можем

102
00:03:45,760 --> 00:03:48,159
видеть, что столбцы здесь

103
00:03:48,159 --> 00:03:50,239
соответствуют именам каждой из функций,

104
00:03:50,239 --> 00:03:52,000
потому что у нас есть только два типа слов в

105
00:03:52,000 --> 00:03:54,640
нашем корпусе есть два столбца a и b

106
00:03:54,640 --> 00:03:56,560
и каждая из строк соответствует

107
00:03:56,560 --> 00:03:58,400
примеру из нашего корпуса, и поэтому вы можете

108
00:03:58,400 --> 00:04:00,080
видеть, что наш первый пример был

109
00:04:00,080 --> 00:04:02,560
сведен к представлению, которое имеет

110
00:04:02,560 --> 00:04:04,480
три в своем первом измерении и ноль

111
00:04:04,480 --> 00:04:06,239
во втором, что соответствует тому факту,

112
00:04:06,239 --> 00:04:08,879
что он имеет три а и ни одного

113
00:04:08,879 --> 00:04:11,599
пример b два aab представлены как

114
00:04:11,599 --> 00:04:13,360
два в первом столбце и один во

115
00:04:13,360 --> 00:04:14,560
втором столбце

116
00:04:14,560 --> 00:04:16,560
и так далее, так что это первое

117
00:04:16,560 --> 00:04:18,000
различие, у нас есть эта

118
00:04:18,000 --> 00:04:20,478
функция здесь, которая похожа на фабрику

119
00:04:20,478 --> 00:04:22,240
и зависит от данных, которые поступают

120
00:04:22,240 --> 00:04:24,320
для нашего корпуса  мы собираемся получить очень

121
00:04:24,320 --> 00:04:26,800
разные функции, которые соответствуют

122
00:04:26,800 --> 00:04:29,040
каждому из столбцов в этой матрице представления функций,

123
00:04:29,040 --> 00:04:31,919


124
00:04:31,919 --> 00:04:33,280
давайте немного продолжим и

125
00:04:33,280 --> 00:04:34,880
подумаем о том, как это на самом деле взаимодействует

126
00:04:34,880 --> 00:04:37,520
с процессом оптимизации, поэтому в ячейке

127
00:04:37,520 --> 00:04:39,520
7 я просто повторил это  предыдущая

128
00:04:39,520 --> 00:04:41,440
матрица для ссылки

129
00:04:41,440 --> 00:04:43,680
в ячейке 8. У меня есть метки классов для

130
00:04:43,680 --> 00:04:45,199
наших четырех примеров, и вы можете видеть, что

131
00:04:45,199 --> 00:04:49,120
есть три разных класса c1, c2 и c3.

132
00:04:49,120 --> 00:04:50,880
Я настроил модель логистической регрессии.

133
00:04:50,880 --> 00:04:52,400
a  хотя это не особенно важно,

134
00:04:52,400 --> 00:04:54,400
это просто полезная иллюстрация,

135
00:04:54,400 --> 00:04:57,040
и я называю соответствие моей паре xy, которая является моим

136
00:04:57,040 --> 00:04:59,919
представлением функций и моими метками,

137
00:04:59,919 --> 00:05:02,240
и это процесс оптимизации как

138
00:05:02,240 --> 00:05:04,000
часть этого, и для соглашения для

139
00:05:04,000 --> 00:05:06,080
scikit процесс оптимизации создает

140
00:05:06,080 --> 00:05:08,720
этот новый атрибут co-  f подчеркивание, и

141
00:05:08,720 --> 00:05:11,280
эти новые классы атрибутов подчеркивают

142
00:05:11,280 --> 00:05:13,919
эти co co-f здесь это веса,

143
00:05:13,919 --> 00:05:15,039
которые мы узнали как часть

144
00:05:15,039 --> 00:05:17,039
процесса оптимизации, и, конечно,

145
00:05:17,039 --> 00:05:19,120
классы соответствуют классам, которые

146
00:05:19,120 --> 00:05:22,080
выведены из метки y, которую мы ввели,

147
00:05:22,080 --> 00:05:23,680
и здесь я просто  снова используя фрейм данных панды,

148
00:05:23,680 --> 00:05:25,039
чтобы попытаться сделать это

149
00:05:25,039 --> 00:05:27,039
интуитивно понятным, это действительно просто массив numpy,

150
00:05:27,039 --> 00:05:29,520
этот co-f объект здесь, но вы можете видеть,

151
00:05:29,520 --> 00:05:32,560
что результирующая матрица имеет строку для

152
00:05:32,560 --> 00:05:35,280
каждого из наших классов и столбец для

153
00:05:35,280 --> 00:05:37,199
каждой из наших функций

154
00:05:37,199 --> 00:05:39,039
и это полезное напоминание о том, что

155
00:05:39,039 --> 00:05:41,039
на самом деле процесс оптимизации для моделей, подобных

156
00:05:41,039 --> 00:05:42,720
этой, -

157
00:05:42,720 --> 00:05:45,280
это изучение веса, который связывает

158
00:05:45,280 --> 00:05:47,759
пары имен функций класса

159
00:05:47,759 --> 00:05:49,520
с весом, поэтому  дело не только в

160
00:05:49,520 --> 00:05:50,960
том, что мы изучаем индивидуальные веса для

161
00:05:50,960 --> 00:05:52,800
признаков, но скорее в том, что мы изучаем их по

162
00:05:52,800 --> 00:05:54,880
отношению к каждому из классов, и

163
00:05:54,880 --> 00:05:57,199
это является отличительной чертой оптимизации для

164
00:05:57,199 --> 00:05:59,840
моделей с несколькими классами, таких как эта,

165
00:05:59,840 --> 00:06:01,520
а затем в ячейке 12 я только что показал вам,

166
00:06:01,520 --> 00:06:03,680
что вы  на самом деле можно использовать co-f и

167
00:06:03,680 --> 00:06:05,520
этот другой термин смещения, подчеркивание перехвата между баллами,

168
00:06:05,520 --> 00:06:07,199


169
00:06:07,199 --> 00:06:09,199
чтобы воссоздать прогнозы модели,

170
00:06:09,199 --> 00:06:11,919
все, что вы делаете, это умножение примеров

171
00:06:11,919 --> 00:06:14,080
на эти коэффициенты и добавление

172
00:06:14,080 --> 00:06:16,639
члена смещения, и эта матрица здесь

173
00:06:16,639 --> 00:06:18,960
идентична тому, что вы  войдите внутрь, если вы

174
00:06:18,960 --> 00:06:21,280
просто напрямую вызовете прогнозирование prabha для

175
00:06:21,280 --> 00:06:25,919
предсказания вероятностей на ваших примерах,

176
00:06:28,080 --> 00:06:29,759
давайте вернемся к тому, что мы пытаемся

177
00:06:29,759 --> 00:06:31,840
сделать, чтобы создать хорошие модели, имея

178
00:06:31,840 --> 00:06:33,680
в виду эти идеи, поэтому давайте просто рассмотрим несколько

179
00:06:33,680 --> 00:06:35,680
других идей для функций функций, созданных вручную,

180
00:06:35,680 --> 00:06:36,800
которые  я думаю, что это может быть

181
00:06:36,800 --> 00:06:38,240
эффективно для настроения, поэтому, конечно,

182
00:06:38,240 --> 00:06:40,080
у нас будут функции, производные от лексики, которые я

183
00:06:40,080 --> 00:06:41,840
ранее показывал вам, куча разных

184
00:06:41,840 --> 00:06:44,160
лексиконов, и их можно использовать для группировки

185
00:06:44,160 --> 00:06:45,680
наших униграмм, чтобы мы могли  если бы эти

186
00:06:45,680 --> 00:06:47,759
функции функций работали в сочетании

187
00:06:47,759 --> 00:06:49,759
с моделью мешка слов или мешка инграмм,

188
00:06:49,759 --> 00:06:52,000
или мы могли бы использовать их для замены

189
00:06:52,000 --> 00:06:54,479
этой модели и разработки более разреженного пространства представления признаков,

190
00:06:54,479 --> 00:06:56,639


191
00:06:56,639 --> 00:06:58,240
мы также могли бы сделать маркировку отрицания,

192
00:06:58,240 --> 00:07:00,160
о которой я упоминал ранее, и мы могли бы

193
00:07:00,160 --> 00:07:02,160
обобщить эту идею  так много вещей в

194
00:07:02,160 --> 00:07:04,240
языке приобретают объем таким образом, что это

195
00:07:04,240 --> 00:07:06,240
влияет на семантику слов, которые находятся

196
00:07:06,240 --> 00:07:08,319
в их объеме, поэтому еще одним классическим

197
00:07:08,319 --> 00:07:10,479
примером, помимо отрицания, являются эти

198
00:07:10,479 --> 00:07:13,039
модальные наречия, такие как вполне возможно или

199
00:07:13,039 --> 00:07:15,120
полностью, у нас может быть идея, что

200
00:07:15,120 --> 00:07:17,039
они модулируют степень до  который

201
00:07:17,039 --> 00:07:19,360
говорящий считает шедевром или

202
00:07:19,360 --> 00:07:21,840
удивительным в данном случае, и

203
00:07:21,840 --> 00:07:23,840
отслеживание этой семантической ассоциации с помощью

204
00:07:23,840 --> 00:07:26,479
какой-либо простой маркировки подчеркивания может быть

205
00:07:26,479 --> 00:07:28,639
полезно для того, чтобы дать нашей модели возможность

206
00:07:28,639 --> 00:07:30,479
увидеть, что эти униграммы различаются в

207
00:07:30,479 --> 00:07:33,199
зависимости от их окружения,

208
00:07:33,199 --> 00:07:34,960
мы также могли бы иметь  функции, основанные на длине,

209
00:07:34,960 --> 00:07:36,319
и это просто полезное напоминание о том,

210
00:07:36,319 --> 00:07:37,759
что не все они должны быть

211
00:07:37,759 --> 00:07:39,919
подсчетными функциями, которые мы могли бы  иметь действительно ценные

212
00:07:39,919 --> 00:07:41,919
функции различных видов, и они могут

213
00:07:41,919 --> 00:07:43,599
сигнализировать о чем-то важном в

214
00:07:43,599 --> 00:07:46,319
ярлыке класса, например, я думаю, что

215
00:07:46,319 --> 00:07:48,560
нейтральные обзоры трехзвездочные обзоры, как

216
00:07:48,560 --> 00:07:50,479
правило, длиннее, чем один из пятизвездочных

217
00:07:50,479 --> 00:07:53,599
обзоров, так что можно было бы добавить это,

218
00:07:53,599 --> 00:07:55,199
и мы можем расширить эту идею  функций с плавающей запятой

219
00:07:55,199 --> 00:07:56,960
немного больше, мне

220
00:07:56,960 --> 00:07:58,960
нравится идея несостоявшихся ожиданий, которые

221
00:07:58,960 --> 00:08:01,360
вы можете отслеживать как соотношение

222
00:08:01,360 --> 00:08:04,240
положительных и отрицательных слов в предложении.

223
00:08:04,240 --> 00:08:06,800
Идея заключается в том, что очень часто, если

224
00:08:06,800 --> 00:08:09,520
это соотношение преувеличено, оно рассказывает

225
00:08:09,520 --> 00:08:11,120
вам противоположную историю.  что вы можете

226
00:08:11,120 --> 00:08:13,360
ожидать от общего настроения,

227
00:08:13,360 --> 00:08:15,919
многие положительные слова, сложенные вместе,

228
00:08:15,919 --> 00:08:17,680
могут на самом деле подготовить вас к

229
00:08:17,680 --> 00:08:20,639
отрицательной оценке и наоборот,

230
00:08:20,639 --> 00:08:22,160
но важная вещь в этой

231
00:08:22,160 --> 00:08:24,000
функции заключается в том, что она не будет решать за

232
00:08:24,000 --> 00:08:25,919
вас, что означают эти соотношения, вы

233
00:08:25,919 --> 00:08:27,680
просто  надеюсь, что это был полезный сигнал,

234
00:08:27,680 --> 00:08:29,520
который ваша модель может уловить как

235
00:08:29,520 --> 00:08:31,520
часть оптимизации, чтобы выяснить, как

236
00:08:31,520 --> 00:08:34,240
использовать информацию,

237
00:08:34,240 --> 00:08:35,839
а затем  наконец, мы могли бы использовать

238
00:08:35,839 --> 00:08:37,360
различные виды специальных функциональных

239
00:08:37,360 --> 00:08:39,519
функций, чтобы попытаться уловить тот факт,

240
00:08:39,519 --> 00:08:41,519
что многие виды использования языка

241
00:08:41,519 --> 00:08:43,279
не являются буквальными и могут сигнализировать о

242
00:08:43,279 --> 00:08:45,040
прямо противоположном тому, что они, кажется

243
00:08:45,040 --> 00:08:47,600
, делают на поверхности, например, не

244
00:08:47,600 --> 00:08:50,000
совсем шедевр, вероятно,  довольно

245
00:08:50,000 --> 00:08:51,440
негативный обзор

246
00:08:51,440 --> 00:08:53,519
, который длился около 50 часов, не говорит

247
00:08:53,519 --> 00:08:55,279
о том, что на самом деле он длился 50 часов, а

248
00:08:55,279 --> 00:08:57,680
скорее с преувеличением, указывающим на то, что он

249
00:08:57,680 --> 00:09:00,160
был слишком длинным или что-то в этом роде,

250
00:09:00,160 --> 00:09:01,760
и лучший фильм в истории

251
00:09:01,760 --> 00:09:03,680
Вселенной может быть

252
00:09:03,680 --> 00:09:05,839
громким одобрением  но с таким же успехом это может

253
00:09:05,839 --> 00:09:08,320
быть и сарказмом.

254
00:09:08,320 --> 00:09:09,680
Уловить такого рода тонкие

255
00:09:09,680 --> 00:09:11,200
различия, конечно, намного

256
00:09:11,200 --> 00:09:13,120
сложнее, но функции дескриптора,

257
00:09:13,120 --> 00:09:14,880
которые вы пишете, можно

258
00:09:14,880 --> 00:09:16,480
попытаться уловить, и если они окажут

259
00:09:16,480 --> 00:09:18,560
положительный эффект, то,

260
00:09:18,560 --> 00:09:20,720
возможно, вы  добились некоторого реального прогресса,

261
00:09:20,720 --> 00:09:22,160
и это хорошая точка перехода к

262
00:09:22,160 --> 00:09:23,920
этой теме оценки отдельных

263
00:09:23,920 --> 00:09:25,920
функций функций, поскольку вы можете видеть, что

264
00:09:25,920 --> 00:09:27,760
философия этого режима работы заключается в том, что

265
00:09:27,760 --> 00:09:29,920
вы  напишите много функций функций и

266
00:09:29,920 --> 00:09:32,160
посмотрите, насколько хорошо они могут

267
00:09:32,160 --> 00:09:34,240
улучшить вашу модель в целом,

268
00:09:34,240 --> 00:09:36,240
вы можете получить очень большую модель

269
00:09:36,240 --> 00:09:38,000
со многими коррелированными функциями, и это

270
00:09:38,000 --> 00:09:39,200
может привести к тому, что вы захотите сделать некоторый

271
00:09:39,200 --> 00:09:41,279
выбор функций, чтобы отсеять те,

272
00:09:41,279 --> 00:09:43,360
которые  не вносят положительного

273
00:09:43,360 --> 00:09:44,560


274
00:09:44,560 --> 00:09:46,959
вклада, теперь у scikit-learn есть целая библиотека для

275
00:09:46,959 --> 00:09:48,480
выполнения этого, называемого выбором функций, и

276
00:09:48,480 --> 00:09:50,560
он предлагает множество функций, которые

277
00:09:50,560 --> 00:09:52,480
позволят вам оценить, сколько информации

278
00:09:52,480 --> 00:09:54,640
содержат ваши функции функций по отношению

279
00:09:54,640 --> 00:09:56,560
к меткам для вашей задачи классификации,

280
00:09:56,560 --> 00:09:58,480
так что это  является очень мощным, и я

281
00:09:58,480 --> 00:10:00,560
призываю вас использовать их, но вы должны

282
00:10:00,560 --> 00:10:02,800
быть

283
00:10:02,800 --> 00:10:04,399
немного осторожны, оценивая функции признаков по

284
00:10:04,399 --> 00:10:06,880
отдельности, потому что

285
00:10:06,880 --> 00:10:09,040
корреляции между этими функциями

286
00:10:09,040 --> 00:10:11,360
сделают оценки очень трудными для

287
00:10:11,360 --> 00:10:12,480
интерпретации.

288
00:10:12,480 --> 00:10:14,320
Проблема здесь в том, что ваша модель

289
00:10:14,320 --> 00:10:16,000
целостно думает о том, как  все

290
00:10:16,000 --> 00:10:17,519
эти функции относятся к метке вашего класса

291
00:10:17,519 --> 00:10:19,760
и выяснению того, как оптимизировать

292
00:10:19,760 --> 00:10:21,839
веса на этой основе, в то время как

293
00:10:21,839 --> 00:10:23,519
многие из них

294
00:10:23,519 --> 00:10:25,519
просто смотрят на отдельные функции и то, как

295
00:10:25,519 --> 00:10:26,959
они соотносятся с метками классов, поэтому

296
00:10:26,959 --> 00:10:28,720
вы теряете весь этот корреляционный

297
00:10:28,720 --> 00:10:29,839
контекст,

298
00:10:29,839 --> 00:10:31,279
и чтобы сделать это немного конкретным, я

299
00:10:31,279 --> 00:10:32,880
только что приготовил пример здесь,

300
00:10:32,880 --> 00:10:35,200
идеализированный, который показывает, как вы могли бы

301
00:10:35,200 --> 00:10:38,079
быть введенным в заблуждение, поэтому у меня есть три функции x1 x2

302
00:10:38,079 --> 00:10:40,160
и x3 и простая

303
00:10:40,160 --> 00:10:42,720
проблема бинарной классификации, и я использую

304
00:10:42,720 --> 00:10:44,880
критерий хи-квадрат от выбора функций,

305
00:10:44,880 --> 00:10:46,720
чтобы оценить, насколько важна каждая

306
00:10:46,720 --> 00:10:48,880
из этих функций по отношению к

307
00:10:48,880 --> 00:10:50,880
этой проблеме классификации

308
00:10:50,880 --> 00:10:52,959
и что я нашел  заключается в том, что интуитивно

309
00:10:52,959 --> 00:10:55,680
кажется, что x1 и x2 действительно мощные

310
00:10:55,680 --> 00:10:56,880
функции,

311
00:10:56,880 --> 00:10:58,640
и это может натолкнуть меня на мысль, что

312
00:10:58,640 --> 00:11:00,880
я отброшу третью функцию и включу

313
00:11:00,880 --> 00:11:03,360
только одну и две в мою модель.

314
00:11:03,360 --> 00:11:06,079


315
00:11:06,079 --> 00:11:08,079
Обнаружение заключается в том, что

316
00:11:08,079 --> 00:11:10,800
на самом деле простая линейная модель

317
00:11:10,800 --> 00:11:13,440
лучше всего работает только с функцией x1, а фактическое

318
00:11:13,440 --> 00:11:16,640
включение x2 вредит модели, несмотря на

319
00:11:16,640 --> 00:11:18,880
то, что она имеет это положительное

320
00:11:18,880 --> 00:11:20,880
значение важности функции.  так что на самом деле

321
00:11:20,880 --> 00:11:22,720
мы должны использовать только эту единственную

322
00:11:22,720 --> 00:11:24,800
функцию, но эти методы не могут сказать нам об

323
00:11:24,800 --> 00:11:27,440
этом, и даже положительное

324
00:11:27,440 --> 00:11:29,920
значение выбора функции может на самом деле

325
00:11:29,920 --> 00:11:31,519
быть чем-то, что противоречит тому, что

326
00:11:31,519 --> 00:11:33,200
мы пытаемся сделать с нашей моделью, как

327
00:11:33,200 --> 00:11:35,200
это  Пример показывает

328
00:11:35,200 --> 00:11:38,000
, что в идеале вы должны рассмотреть

329
00:11:38,000 --> 00:11:39,920
более целостные методы оценки, которые

330
00:11:39,920 --> 00:11:42,160
также предлагает scikit.

331
00:11:42,160 --> 00:11:44,079


332
00:11:44,079 --> 00:11:46,720


333
00:11:46,720 --> 00:11:49,279


334
00:11:49,279 --> 00:11:51,200


335
00:11:51,200 --> 00:11:53,120
дороже,

336
00:11:53,120 --> 00:11:54,639
потому что вы оптимизируете много-много

337
00:11:54,639 --> 00:11:56,880
моделей, поэтому для

338
00:11:56,880 --> 00:11:58,320
некоторых классов моделей, которые вы

339
00:11:58,320 --> 00:12:00,639
изучаете, это может быть непомерно высоким, но если вы можете это сделать, это

340
00:12:00,639 --> 00:12:02,160
будет более надежно,

341
00:12:02,160 --> 00:12:04,480
однако, если это невозможно, может

342
00:12:04,480 --> 00:12:06,720
быть продуктивно сделать некоторые функции

343
00:12:06,720 --> 00:12:09,360
выбор с использованием более простых методов, вы

344
00:12:09,360 --> 00:12:11,360
должны просто знать, что вы можете

345
00:12:11,360 --> 00:12:13,839
делать что-то, что не является оптимальным

346
00:12:13,839 --> 00:12:15,680
для реальной проблемы оптимизации, которую

347
00:12:15,680 --> 00:12:18,320
вы поставили.  sed

348
00:12:18,399 --> 00:12:19,760
хорошо в заключительном разделе этого

349
00:12:19,760 --> 00:12:21,680
скринкаста - это своего рода переход

350
00:12:21,680 --> 00:12:23,279
в мир глубокого обучения. Я назвал

351
00:12:23,279 --> 00:12:25,600
это распределенными представлениями как

352
00:12:25,600 --> 00:12:27,440
функции, это совсем другой

353
00:12:27,440 --> 00:12:30,079
способ думать о представлении примеров,

354
00:12:30,079 --> 00:12:32,320
что мы делаем в этом случае, это берем наш

355
00:12:32,320 --> 00:12:34,959
токен  поток, как и раньше, но вместо того, чтобы

356
00:12:34,959 --> 00:12:36,399
писать множество функций функций, созданных вручную,

357
00:12:36,399 --> 00:12:38,320
мы просто ищем каждый из

358
00:12:38,320 --> 00:12:40,480
этих токенов в каком-то встраивании, которое у нас

359
00:12:40,480 --> 00:12:42,000
есть, например, это может быть

360
00:12:42,000 --> 00:12:43,760
вложение, которое вы создали в первом

361
00:12:43,760 --> 00:12:45,279
разделе этого курса,

362
00:12:45,279 --> 00:12:47,440
или это может быть  быть вложением перчаток или

363
00:12:47,440 --> 00:12:49,120
статическим вложением, которое вы получили из

364
00:12:49,120 --> 00:12:51,600
представлений vert и т. д. и т. д.

365
00:12:51,600 --> 00:12:53,279
важно то, что каждый

366
00:12:53,279 --> 00:12:56,000
токен теперь представлен вектором,

367
00:12:56,000 --> 00:12:57,600
и это может быть мощной идеей,

368
00:12:57,600 --> 00:12:59,440
потому что при представлении каждого из этих

369
00:12:59,440 --> 00:13:02,000
слов в виде векторов мы  теперь

370
00:13:02,000 --> 00:13:04,480
фиксируют отношения между

371
00:13:04,480 --> 00:13:06,639
этими токенами, у нас теперь может быть надежда

372
00:13:06,639 --> 00:13:09,600
увидеть, что диван и кушетка на самом деле

373
00:13:09,600 --> 00:13:11,839
похожи в целом, а не только

374
00:13:11,839 --> 00:13:13,760
в отношении  ct к меткам классов, которые у нас

375
00:13:13,760 --> 00:13:15,120
есть,

376
00:13:15,120 --> 00:13:16,560
так что идея, почему это может быть

377
00:13:16,560 --> 00:13:18,079
мощным, поэтому мы берем все эти векторы

378
00:13:18,079 --> 00:13:19,839
и просматриваем их, однако для всех этих

379
00:13:19,839 --> 00:13:21,519
моделей классификатора нам нужно представление с фиксированным

380
00:13:21,519 --> 00:13:23,440
размером для подачи

381
00:13:23,440 --> 00:13:25,600
в фактическую единицу классификатора, поэтому мы

382
00:13:25,600 --> 00:13:27,040
придется

383
00:13:27,040 --> 00:13:28,639
каким-то образом комбинировать эти векторы, и самое простое, что вы

384
00:13:28,639 --> 00:13:30,560
можете сделать, это объединить их с помощью какой-либо

385
00:13:30,560 --> 00:13:33,360
функции, такой как сумма или среднее значение,

386
00:13:33,360 --> 00:13:34,639
поэтому я бы взял все эти вещи,

387
00:13:34,639 --> 00:13:36,240
например, и взял их среднее значение, и это

388
00:13:36,240 --> 00:13:37,920
дало бы мне еще одно фиксированное размерное

389
00:13:37,920 --> 00:13:40,240
представление  независимо от того, сколько

390
00:13:40,240 --> 00:13:42,320
токенов в каждом из примеров,

391
00:13:42,320 --> 00:13:44,959
и этот средний вектор будет

392
00:13:44,959 --> 00:13:47,120
вводом для классификатора,

393
00:13:47,120 --> 00:13:48,720
поэтому, если каждый из этих векторов имеет

394
00:13:48,720 --> 00:13:51,199
размерность 300, то то же самое

395
00:13:51,199 --> 00:13:53,040
будет и представление признаков всего моего

396
00:13:53,040 --> 00:13:56,079
примера, и теперь у меня есть  классификатор

397
00:13:56,079 --> 00:13:57,760
, который обрабатывает представления функций

398
00:13:57,760 --> 00:14:00,800
, у которых есть 300 столбцов в

399
00:14:00,800 --> 00:14:02,639
каждом измерении в базовом

400
00:14:02,639 --> 00:14:04,560
пространстве встраивания, теперь соответствует

401
00:14:04,560 --> 00:14:05,600
функции,

402
00:14:05,600 --> 00:14:07,199
и это основа  для оптимизации,

403
00:14:07,199 --> 00:14:08,880
и я бы сказал, что в этом классе моделей открывается поразительная вещь

404
00:14:08,880 --> 00:14:10,959
: несмотря на то, что

405
00:14:10,959 --> 00:14:13,199
они очень компактны, вы знаете, что 300

406
00:14:13,199 --> 00:14:15,360
измерений против 20 000, которые вы могли бы

407
00:14:15,360 --> 00:14:17,920
иметь в модели мешка слов, они

408
00:14:17,920 --> 00:14:20,480
оказываются очень мощными,

409
00:14:20,480 --> 00:14:22,399
и это последнее  слайд здесь просто показывает вам,

410
00:14:22,399 --> 00:14:24,399
как реализовать те, которые используют инструменты и

411
00:14:24,399 --> 00:14:26,560
другие утилиты для нашего курса,

412
00:14:26,560 --> 00:14:28,000
поэтому я собираюсь использовать перчатку, и я

413
00:14:28,000 --> 00:14:30,000
собираюсь использовать 300-мерное пространство перчатки,

414
00:14:30,000 --> 00:14:31,440
которое включено в ваше распределение данных

415
00:14:31,440 --> 00:14:32,880


416
00:14:32,880 --> 00:14:34,399
в четвертом и пятом здесь мы  просто напишите

417
00:14:34,399 --> 00:14:35,760
простые функции функций, и их

418
00:14:35,760 --> 00:14:37,120
отличительной чертой является то, что они

419
00:14:37,120 --> 00:14:39,600
просто ищут слова во встраивании,

420
00:14:39,600 --> 00:14:41,519
а затем комбинируют их с помощью любой

421
00:14:41,519 --> 00:14:43,600
функции, указанной пользователем, поэтому

422
00:14:43,600 --> 00:14:45,760
вывод этого является прямым векторным

423
00:14:45,760 --> 00:14:48,720
представлением каждого примера

424
00:14:48,720 --> 00:14:50,480
в ячейке шесть, которую мы настроили  логистическая

425
00:14:50,480 --> 00:14:52,399
регрессия, как и раньше, конечно, это может

426
00:14:52,399 --> 00:14:54,160
быть гораздо более причудливая модель, но логистическая

427
00:14:54,160 --> 00:14:55,680
регрессия подойдет,

428
00:14:55,680 --> 00:14:57,920
и тогда мы используем эксперимент с твердотельным накопителем почти

429
00:14:57,920 --> 00:15:00,160
точно так же, как и раньше, одно изменение, которое мы

430
00:15:00,160 --> 00:15:02,320
должны помнить.  make при работе в

431
00:15:02,320 --> 00:15:04,480
этом режиме заключается в том, чтобы установить флаг vectorize

432
00:15:04,480 --> 00:15:05,920
равным false,

433
00:15:05,920 --> 00:15:08,800
у нас уже есть каждый пример, представленный

434
00:15:08,800 --> 00:15:10,959
в виде вектора, поэтому нам не нужно пропускать его

435
00:15:10,959 --> 00:15:12,639
через весь этот процесс использования

436
00:15:12,639 --> 00:15:14,720
векторизатора dict для превращения

437
00:15:14,720 --> 00:15:17,120
словарей count в векторы

438
00:15:17,120 --> 00:15:19,040
и, как я уже сказал  прежде чем они

439
00:15:19,040 --> 00:15:20,480
окажутся неплохими моделями, несмотря на их

440
00:15:20,480 --> 00:15:22,560
компактность, и последнее, что я скажу,

441
00:15:22,560 --> 00:15:25,600
это то, что эта модель является хорошим переходом

442
00:15:25,600 --> 00:15:27,600
к рекуррентным нейронным сетям, которые

443
00:15:27,600 --> 00:15:29,440
мы будем изучать в финальном скринкасте для

444
00:15:29,440 --> 00:15:31,519
этого модуля, который по существу обобщил

445
00:15:31,519 --> 00:15:34,240
эту идею.  изучив интересную

446
00:15:34,240 --> 00:15:36,320
комбинированную функцию для всех векторов

447
00:15:36,320 --> 00:15:40,440
для каждого из отдельных токенов, которые

448
00:15:42,800 --> 00:15:44,880
вы

