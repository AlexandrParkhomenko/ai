1
00:00:00,000 --> 00:00:05,050


2
00:00:05,050 --> 00:00:07,450
CHRIS POTTS: Welcome, everyone,
to our first screencast

3
00:00:07,450 --> 00:00:09,400
on contextual word
representations.

4
00:00:09,400 --> 00:00:12,280
My goal here is to give you
an overview for this unit

5
00:00:12,280 --> 00:00:15,850
and also give you a sense
for the conceptual landscape.

6
00:00:15,850 --> 00:00:17,740
Let's start with the
associated materials.

7
00:00:17,740 --> 00:00:20,170
You might think that the name
of the game for this unit

8
00:00:20,170 --> 00:00:23,020
is to get you to the point
where you can work productively

9
00:00:23,020 --> 00:00:24,910
with this notebook
called finetuning,

10
00:00:24,910 --> 00:00:28,240
which shows you how to fine-tune
contextual word representations

11
00:00:28,240 --> 00:00:29,620
for classification problems.

12
00:00:29,620 --> 00:00:32,110
I think that could be a
very powerful mode for you

13
00:00:32,110 --> 00:00:35,910
as you work on the current
assignment and bakeoff.

14
00:00:35,910 --> 00:00:38,670
For background and intuitions,
I highly recommend this paper

15
00:00:38,670 --> 00:00:40,880
by Noah Smith.

16
00:00:40,880 --> 00:00:42,830
The beating heart of
this unit is really

17
00:00:42,830 --> 00:00:44,510
the Transformer
architecture, which

18
00:00:44,510 --> 00:00:47,510
was introduced by Vaswani,
et al., 2017 in a paper

19
00:00:47,510 --> 00:00:49,550
called "Attention
is All You Need."

20
00:00:49,550 --> 00:00:51,380
It's a highly readable paper.

21
00:00:51,380 --> 00:00:53,240
But I recommend that
if you want to read it,

22
00:00:53,240 --> 00:00:56,330
you instead read Sasha Rush's
outstanding contribution,

23
00:00:56,330 --> 00:00:58,280
"The Annotated Transformer."

24
00:00:58,280 --> 00:01:01,610
What this does is literally
reproduce the text of Vaswani,

25
00:01:01,610 --> 00:01:06,020
et al., 2017 with
PyTorch code woven in,

26
00:01:06,020 --> 00:01:08,300
culminating in a
complete implementation

27
00:01:08,300 --> 00:01:10,490
of the Transformer as
applied to problems

28
00:01:10,490 --> 00:01:12,005
in machine translation.

29
00:01:12,005 --> 00:01:13,880
This is a wonderful
contribution in the sense

30
00:01:13,880 --> 00:01:16,088
that to the extent that
there are points of unclarity

31
00:01:16,088 --> 00:01:18,110
or uncertainty in
the original text,

32
00:01:18,110 --> 00:01:21,080
they are fully resolved
by Sasha's code.

33
00:01:21,080 --> 00:01:23,330
And of course, this can give
you a really good example

34
00:01:23,330 --> 00:01:25,730
of how to do efficient and
effective implementation

35
00:01:25,730 --> 00:01:30,095
of model architectures
like this using PyTorch.

36
00:01:30,095 --> 00:01:31,720
In practical terms,
we're going to make

37
00:01:31,720 --> 00:01:33,940
extensive use of the
Hugging Face Transformers

38
00:01:33,940 --> 00:01:36,370
library, which has
really opened up access

39
00:01:36,370 --> 00:01:39,610
to a wide range of pretrained
transformer models.

40
00:01:39,610 --> 00:01:40,480
It's very exciting.

41
00:01:40,480 --> 00:01:43,360
And it's enabled
lots of new things.

42
00:01:43,360 --> 00:01:45,730
For us, the central
architecture will be BERT.

43
00:01:45,730 --> 00:01:47,570
We'll have a separate
screencast on that.

44
00:01:47,570 --> 00:01:50,028
And we're also going to have
a screencast on RoBERTa, which

45
00:01:50,028 --> 00:01:51,430
is Robustly Optimized BERT.

46
00:01:51,430 --> 00:01:53,680
I think it's an interesting
perspective in the sense

47
00:01:53,680 --> 00:01:57,160
that they explored more deeply
some of the open questions

48
00:01:57,160 --> 00:01:58,930
from the original BERT paper.

49
00:01:58,930 --> 00:02:02,080
And they also released very
powerful pretrained parameters

50
00:02:02,080 --> 00:02:03,850
that you could, again,
use in the context

51
00:02:03,850 --> 00:02:06,260
of your own fine-tuning.

52
00:02:06,260 --> 00:02:08,193
And then for a slightly
different perspective

53
00:02:08,193 --> 00:02:09,610
on these transformers,
we're going

54
00:02:09,610 --> 00:02:11,470
to look at the ELECTRA
architecture, which

55
00:02:11,470 --> 00:02:13,210
came from Kevin
Clark and colleagues

56
00:02:13,210 --> 00:02:14,830
at Stanford and Google.

57
00:02:14,830 --> 00:02:16,600
I really like this
as a new perspective.

58
00:02:16,600 --> 00:02:18,490
There are, of course,
many different modes

59
00:02:18,490 --> 00:02:20,200
of using the transformer
at this point.

60
00:02:20,200 --> 00:02:22,942
I'm going to mention a few
at the end of the screencast.

61
00:02:22,942 --> 00:02:24,400
And just for the
sake of time, I've

62
00:02:24,400 --> 00:02:26,440
decided to focus on ELECTRA.

63
00:02:26,440 --> 00:02:30,460
And you can explore the
others in your own research.

64
00:02:30,460 --> 00:02:31,840
Let's begin with
some intuitions.

65
00:02:31,840 --> 00:02:34,180
And I'd like to begin with a
linguistic intuition, which

66
00:02:34,180 --> 00:02:36,400
has to do with word
representations

67
00:02:36,400 --> 00:02:38,530
and how they should be
shaped by the context.

68
00:02:38,530 --> 00:02:41,200
Let's focus on the
English verb break.

69
00:02:41,200 --> 00:02:43,030
We have a simple
example, the vase broke,

70
00:02:43,030 --> 00:02:45,650
which means it
shattered to pieces.

71
00:02:45,650 --> 00:02:48,700
Here's a superficially
similar sentence, dawn broke.

72
00:02:48,700 --> 00:02:52,690
Now the sense of break is
something more like begin.

73
00:02:52,690 --> 00:02:53,710
The news broke.

74
00:02:53,710 --> 00:02:55,660
Again, a simple
intransitive sentence.

75
00:02:55,660 --> 00:02:57,940
But now the verb
break means something

76
00:02:57,940 --> 00:03:01,770
more like publish, or up
here, or become known.

77
00:03:01,770 --> 00:03:03,180
Sandy broke the world record.

78
00:03:03,180 --> 00:03:05,250
This is a transitive
use of the verb break.

79
00:03:05,250 --> 00:03:08,760
And it means, surpass
the previous level.

80
00:03:08,760 --> 00:03:10,770
Sandy broke the law is
another transitive use.

81
00:03:10,770 --> 00:03:14,070
But now it means,
Sandy transgressed.

82
00:03:14,070 --> 00:03:17,010
The burglar broke into the
house is a physical act

83
00:03:17,010 --> 00:03:18,870
of transgression on its face.

84
00:03:18,870 --> 00:03:21,390
The newscaster broke
into the movie broadcast

85
00:03:21,390 --> 00:03:24,610
is a sense that it's
more like interrupt.

86
00:03:24,610 --> 00:03:26,770
And we have idioms
like break even, which

87
00:03:26,770 --> 00:03:29,380
means we neither
gained nor lost money.

88
00:03:29,380 --> 00:03:31,810
And this is just a
few of the many ways

89
00:03:31,810 --> 00:03:33,940
that the verb break
can be used in English.

90
00:03:33,940 --> 00:03:35,590
How many senses
are at work here?

91
00:03:35,590 --> 00:03:36,513
It's very hard to say.

92
00:03:36,513 --> 00:03:37,180
It could be one.

93
00:03:37,180 --> 00:03:37,847
It could be two.

94
00:03:37,847 --> 00:03:39,230
It could be ten.

95
00:03:39,230 --> 00:03:40,960
It's very hard to
delimit word senses.

96
00:03:40,960 --> 00:03:43,300
But it is very
clear from this data

97
00:03:43,300 --> 00:03:45,460
that our sense
for the verb break

98
00:03:45,460 --> 00:03:49,000
is being shaped by the
immediate linguistic context.

99
00:03:49,000 --> 00:03:51,280
Here are a few
additional examples,

100
00:03:51,280 --> 00:03:54,700
things like flat tire, flat
beer, flat note, flat surface.

101
00:03:54,700 --> 00:03:58,000
It's clear that there is
a conceptual core running

102
00:03:58,000 --> 00:03:59,980
through all of these uses.

103
00:03:59,980 --> 00:04:02,260
But it's also true
that a flat tire

104
00:04:02,260 --> 00:04:04,120
is a very different
sense for flat

105
00:04:04,120 --> 00:04:07,360
than we get from flat
note or flat surface.

106
00:04:07,360 --> 00:04:09,880
I have something similar for
throw a party, throw a fight,

107
00:04:09,880 --> 00:04:11,590
throw a ball, throw a fit.

108
00:04:11,590 --> 00:04:13,270
We have a mixture of
what you might call

109
00:04:13,270 --> 00:04:14,860
literal and metaphorical here.

110
00:04:14,860 --> 00:04:18,050
But again, a kind of common
core that we're drawing on.

111
00:04:18,050 --> 00:04:21,310
But the bottom line is that the
sense for throw, in this case,

112
00:04:21,310 --> 00:04:24,550
is very different depending on
what kind of linguistic context

113
00:04:24,550 --> 00:04:26,360
it's in.

114
00:04:26,360 --> 00:04:29,300
And we can extend this to
things that seem to turn more

115
00:04:29,300 --> 00:04:30,360
on world knowledge.

116
00:04:30,360 --> 00:04:32,990
So if you have something
like, a crane caught a fish,

117
00:04:32,990 --> 00:04:35,930
we have a sense that the
crane here is a bird.

118
00:04:35,930 --> 00:04:38,300
Whereas, if we have a crane
picked up the steel beam,

119
00:04:38,300 --> 00:04:41,115
we have a sense that it's
a piece of equipment.

120
00:04:41,115 --> 00:04:42,740
This seems like
something that's guided

121
00:04:42,740 --> 00:04:46,010
by our understanding of birds,
and equipment, and fish,

122
00:04:46,010 --> 00:04:47,090
and beams.

123
00:04:47,090 --> 00:04:49,580
And when we have relatively
unbiased sentences like,

124
00:04:49,580 --> 00:04:51,470
I saw a crane,
we're kind of left

125
00:04:51,470 --> 00:04:53,450
guessing about which
object is involved,

126
00:04:53,450 --> 00:04:55,468
the bird or the machine.

127
00:04:55,468 --> 00:04:57,260
And we can extend this
past world knowledge

128
00:04:57,260 --> 00:04:59,760
into things that are more
like discourse understanding.

129
00:04:59,760 --> 00:05:02,240
So if you have a sentence
like, "are there typos?

130
00:05:02,240 --> 00:05:03,690
I didn't see any."

131
00:05:03,690 --> 00:05:05,720
The sense of "any"
here, we have a feeling

132
00:05:05,720 --> 00:05:08,590
that something is elided but
probably localized on "any."

133
00:05:08,590 --> 00:05:12,380
And "any" here means any typos
as a result of the preceding

134
00:05:12,380 --> 00:05:14,580
linguistic context.

135
00:05:14,580 --> 00:05:16,040
"Are there any
bookstores downtown?

136
00:05:16,040 --> 00:05:16,790
I didn't see any."

137
00:05:16,790 --> 00:05:18,140
Same second sentence.

138
00:05:18,140 --> 00:05:20,240
But now the sense
of "any" is probably

139
00:05:20,240 --> 00:05:22,100
going to be something
more like bookstores,

140
00:05:22,100 --> 00:05:26,120
as a result of the discourse
context that it appears in.

141
00:05:26,120 --> 00:05:28,370
So all of this is
just showing how much

142
00:05:28,370 --> 00:05:31,850
individual linguistic units
can be shaped by context.

143
00:05:31,850 --> 00:05:33,210
Linguists know this deeply.

144
00:05:33,210 --> 00:05:36,170
This is a primary thing that
linguists try to get a grip on.

145
00:05:36,170 --> 00:05:38,360
And I think it's a wonderful
point of connection

146
00:05:38,360 --> 00:05:40,610
between what linguists
do and the way we're

147
00:05:40,610 --> 00:05:44,390
representing examples in
NLP using contextual models.

148
00:05:44,390 --> 00:05:46,040
This is a very
exciting development

149
00:05:46,040 --> 00:05:48,380
for me as a linguist,
as well as an NLPer.

150
00:05:48,380 --> 00:05:51,850


151
00:05:51,850 --> 00:05:54,820
Here's another set of intuitions
that's related more to things

152
00:05:54,820 --> 00:05:58,390
like model architecture and what
you might call inductive biases

153
00:05:58,390 --> 00:06:00,195
for different model designs.

154
00:06:00,195 --> 00:06:01,570
Let's start off
here in the left.

155
00:06:01,570 --> 00:06:03,610
This is a high-bias
model in the sense

156
00:06:03,610 --> 00:06:06,760
that it makes a lot of a
priori decisions about how

157
00:06:06,760 --> 00:06:08,470
we will represent our examples.

158
00:06:08,470 --> 00:06:11,230
The idea is that we have
three tokens, which we look up

159
00:06:11,230 --> 00:06:13,360
in a fixed embedding space.

160
00:06:13,360 --> 00:06:15,610
And then we have decided to
summarize those embeddings

161
00:06:15,610 --> 00:06:18,220
by simply summing them together
to get a representation

162
00:06:18,220 --> 00:06:19,870
for the entire example.

163
00:06:19,870 --> 00:06:22,870
Very few of these components are
learned as part of our problem.

164
00:06:22,870 --> 00:06:25,990
We've made most of the
decisions ahead of time.

165
00:06:25,990 --> 00:06:28,540
As we've seen, as we move to
a recurrent neural network,

166
00:06:28,540 --> 00:06:30,880
we relax some of
those assumptions.

167
00:06:30,880 --> 00:06:33,130
We're still going to look
upwards in a fixed embedding

168
00:06:33,130 --> 00:06:33,630
space.

169
00:06:33,630 --> 00:06:36,010
But now instead of deciding
that we know the proper way

170
00:06:36,010 --> 00:06:37,870
to combine them
is with summation,

171
00:06:37,870 --> 00:06:39,400
we're going to
learn from our data

172
00:06:39,400 --> 00:06:42,555
a very complicated function
for combining them.

173
00:06:42,555 --> 00:06:43,930
And that will
presumably allow us

174
00:06:43,930 --> 00:06:47,890
to be more responsive, that is,
less biased about what the data

175
00:06:47,890 --> 00:06:50,020
are likely to look like.

176
00:06:50,020 --> 00:06:51,820
The tree structured
architecture down here

177
00:06:51,820 --> 00:06:53,950
is an interesting
mixture of these ideas.

178
00:06:53,950 --> 00:06:55,660
It's like the recurrent
neural network.

179
00:06:55,660 --> 00:06:59,200
Instead of assuming that I can
process the data left-to-right,

180
00:06:59,200 --> 00:07:01,720
the data get processed
into constituents,

181
00:07:01,720 --> 00:07:04,150
like "the Rock" is a
constituent, as excluded

182
00:07:04,150 --> 00:07:06,520
from "rules" over here.

183
00:07:06,520 --> 00:07:08,740
Now this is probably
going to be very powerful

184
00:07:08,740 --> 00:07:11,290
if we're correct that the
language data are structured

185
00:07:11,290 --> 00:07:13,150
according to these constituents.

186
00:07:13,150 --> 00:07:16,113
Because it will give us a
boost in terms of learning.

187
00:07:16,113 --> 00:07:18,280
It could be counterproductive,
though, to the extent

188
00:07:18,280 --> 00:07:20,140
that constituent
structure is wrong.

189
00:07:20,140 --> 00:07:22,270
And I think that's
showing that biases

190
00:07:22,270 --> 00:07:24,940
that we impose at the
level of our architectures

191
00:07:24,940 --> 00:07:27,580
can be helpful, as well
as a hindrance, depending

192
00:07:27,580 --> 00:07:30,130
on how they align with
the data-driven problem

193
00:07:30,130 --> 00:07:32,570
that we're trying to solve.

194
00:07:32,570 --> 00:07:35,570
In the bottom right here, I
have the least biased model

195
00:07:35,570 --> 00:07:38,647
in all these sentences, of
all the ones depicted here.

196
00:07:38,647 --> 00:07:40,730
I've got a recurrent neural
network like this one,

197
00:07:40,730 --> 00:07:42,520
except now I'm assuming
that information

198
00:07:42,520 --> 00:07:44,540
can flow bidirectionally,
so no longer

199
00:07:44,540 --> 00:07:46,550
a presumption of left-to-right.

200
00:07:46,550 --> 00:07:49,560
And in addition, I've added
these attention mechanisms,

201
00:07:49,560 --> 00:07:51,537
which we'll talk a lot
about in this unit.

202
00:07:51,537 --> 00:07:53,120
But essentially,
think of them as ways

203
00:07:53,120 --> 00:07:56,630
of creating special connections
between all of the hidden

204
00:07:56,630 --> 00:07:57,470
units.

205
00:07:57,470 --> 00:08:00,620
And the idea here is that we
would let the data tell us

206
00:08:00,620 --> 00:08:02,600
how to weight all of
these various connections

207
00:08:02,600 --> 00:08:05,280
and, in turn,
represent our examples.

208
00:08:05,280 --> 00:08:07,880
We're making very few
decisions ahead of time

209
00:08:07,880 --> 00:08:09,740
about what connections
could be made,

210
00:08:09,740 --> 00:08:12,530
and instead of just listening
to the data and the learning

211
00:08:12,530 --> 00:08:13,700
process.

212
00:08:13,700 --> 00:08:16,038
And we are, at this
point, on the road

213
00:08:16,038 --> 00:08:17,580
toward the Transformer,
which is kind

214
00:08:17,580 --> 00:08:21,150
of an extreme case of connecting
everything with everything else

215
00:08:21,150 --> 00:08:23,660
and then allowing the data
to tell us how to weight all

216
00:08:23,660 --> 00:08:26,925
of those various connections.

217
00:08:26,925 --> 00:08:29,300
And that does bring us to this
notion of attention, which

218
00:08:29,300 --> 00:08:30,530
we've not discussed before.

219
00:08:30,530 --> 00:08:33,500
But I think I can introduce the
concepts and a bit of the math.

220
00:08:33,500 --> 00:08:35,940
And then we'll see them
again throughout this unit.

221
00:08:35,940 --> 00:08:37,940
So let's start with the
simple sentiment example

222
00:08:37,940 --> 00:08:40,315
and imagine we're dealing with
a recurrent neural network

223
00:08:40,315 --> 00:08:41,390
classifier.

224
00:08:41,390 --> 00:08:43,100
Our example is
really not so good.

225
00:08:43,100 --> 00:08:45,620
And we're going to fit the
classifier, traditionally,

226
00:08:45,620 --> 00:08:48,680
on top of this final
hidden state over here.

227
00:08:48,680 --> 00:08:51,240
But we might worry
about doing that.

228
00:08:51,240 --> 00:08:53,570
That by the time we've
gotten to this final state,

229
00:08:53,570 --> 00:08:55,520
the contribution of
these earlier words,

230
00:08:55,520 --> 00:08:57,500
which are clearly
important linguistically,

231
00:08:57,500 --> 00:09:00,740
might be sort of forgotten
or overly diffuse.

232
00:09:00,740 --> 00:09:03,260
So attention mechanisms
would be a way for us

233
00:09:03,260 --> 00:09:06,290
to bring that information
back in and infuse

234
00:09:06,290 --> 00:09:08,660
this representation,
hc, with some

235
00:09:08,660 --> 00:09:12,248
of those previous
important connections.

236
00:09:12,248 --> 00:09:13,290
So here's how we do that.

237
00:09:13,290 --> 00:09:15,540
We're going to first have
some attention scores, which

238
00:09:15,540 --> 00:09:18,110
are simply dot products of
our target vector with all

239
00:09:18,110 --> 00:09:19,822
the preceding hidden states.

240
00:09:19,822 --> 00:09:22,280
So that gives us a vector of
scores which are traditionally

241
00:09:22,280 --> 00:09:24,440
softmax normalized.

242
00:09:24,440 --> 00:09:26,150
And then what we do
is create a context

243
00:09:26,150 --> 00:09:29,900
vector by weighting each of
the previous hidden states

244
00:09:29,900 --> 00:09:32,450
by its attention
weight, and then taking

245
00:09:32,450 --> 00:09:36,082
the average of those to give
us the context vector K.

246
00:09:36,082 --> 00:09:38,540
And then we're going to have
this special layer here, which

247
00:09:38,540 --> 00:09:42,230
concatenates K with our
previous final hidden state,

248
00:09:42,230 --> 00:09:45,140
and feeds that through this
layer of learned parameters

249
00:09:45,140 --> 00:09:49,100
and a nonlinearity to give us
this new hidden representation,

250
00:09:49,100 --> 00:09:50,540
h tilde.

251
00:09:50,540 --> 00:09:52,700
And it is h tilde
that is finally

252
00:09:52,700 --> 00:09:55,160
the input to our
softmax classifier.

253
00:09:55,160 --> 00:09:58,130
Whereas before, we would
have simply directly input hc

254
00:09:58,130 --> 00:10:01,640
up here, we now input
this more refined version

255
00:10:01,640 --> 00:10:03,950
that is drawing on all of
these attention connections

256
00:10:03,950 --> 00:10:06,230
that we created with
these mechanisms.

257
00:10:06,230 --> 00:10:08,360
And again, as you'll
see, the transformer

258
00:10:08,360 --> 00:10:11,450
does this all over the place
with all of its representations

259
00:10:11,450 --> 00:10:15,570
at various points
in its computations.

260
00:10:15,570 --> 00:10:17,212
Here's another guiding
idea that really

261
00:10:17,212 --> 00:10:18,420
shapes how these models work.

262
00:10:18,420 --> 00:10:20,025
I've called this word pieces.

263
00:10:20,025 --> 00:10:21,150
And we've seen this before.

264
00:10:21,150 --> 00:10:24,420
These models typically do
not tokenize data in the way

265
00:10:24,420 --> 00:10:25,680
that we might expect.

266
00:10:25,680 --> 00:10:27,930
I've loaded in a BERT tokenizer.

267
00:10:27,930 --> 00:10:29,430
And you can see
that for a sentence,

268
00:10:29,430 --> 00:10:31,847
like "this isn't too surprising"
the result is some pretty

269
00:10:31,847 --> 00:10:34,180
familiar tokens, by and large.

270
00:10:34,180 --> 00:10:36,180
But when I feed in
something like "encode me"

271
00:10:36,180 --> 00:10:38,340
the intuitive word
"encode" is split apart

272
00:10:38,340 --> 00:10:39,990
into two-word pieces.

273
00:10:39,990 --> 00:10:42,180
And clearly, we're
implicitly assuming

274
00:10:42,180 --> 00:10:44,430
that the model, because
it's contextual,

275
00:10:44,430 --> 00:10:47,610
can figure out that these pieces
are in some conceptual sense

276
00:10:47,610 --> 00:10:48,900
one word.

277
00:10:48,900 --> 00:10:51,930
And you might extend that up to
idioms like "out of this world"

278
00:10:51,930 --> 00:10:54,750
where we treat them as a
bunch of distinct tokens.

279
00:10:54,750 --> 00:10:56,430
But we might hope
the model can learn

280
00:10:56,430 --> 00:10:59,910
that there's an idiomatic
unity to that phrase.

281
00:10:59,910 --> 00:11:01,410
And this also has
the side advantage

282
00:11:01,410 --> 00:11:03,900
that for unknown tokens
like "Snuffleupagus"

283
00:11:03,900 --> 00:11:06,180
it can break them apart
into familiar pieces.

284
00:11:06,180 --> 00:11:08,190
And we at least have
a hope of getting

285
00:11:08,190 --> 00:11:12,127
a sensible representation for
that out-of-vocabulary item.

286
00:11:12,127 --> 00:11:14,460
The result of all this is
that these models can get away

287
00:11:14,460 --> 00:11:16,590
with having very
small vocabularies,

288
00:11:16,590 --> 00:11:20,370
precisely because we are
relying on them implicitly

289
00:11:20,370 --> 00:11:24,290
to be truly contextual.

290
00:11:24,290 --> 00:11:26,348
Here's another inspiring
idea that we've not

291
00:11:26,348 --> 00:11:27,140
encountered before.

292
00:11:27,140 --> 00:11:29,570
This is called
positional encoding.

293
00:11:29,570 --> 00:11:32,480
And it's another way in which
we can capture sensitivity

294
00:11:32,480 --> 00:11:34,590
of words to their contexts.

295
00:11:34,590 --> 00:11:36,530
So as you'll see, when
you go all the way down

296
00:11:36,530 --> 00:11:38,270
inside the transformer
architecture,

297
00:11:38,270 --> 00:11:40,790
you do have a traditional
static embedding

298
00:11:40,790 --> 00:11:43,800
of the sort we discussed in
the first unit for this course.

299
00:11:43,800 --> 00:11:46,310
Those are in light gray
here, fixed representations

300
00:11:46,310 --> 00:11:47,540
for the words.

301
00:11:47,540 --> 00:11:50,420
However, in the context
of a model like BERT,

302
00:11:50,420 --> 00:11:54,230
what we traditionally think of
as its embedding representation

303
00:11:54,230 --> 00:11:57,590
is actually a combination
of that fixed embedding

304
00:11:57,590 --> 00:11:59,330
and a separate
embedding space called

305
00:11:59,330 --> 00:12:03,080
the positional embedding, where
we have learned representations

306
00:12:03,080 --> 00:12:05,870
for each position in a sequence.

307
00:12:05,870 --> 00:12:08,540
This has the intriguing property
that one and the same word,

308
00:12:08,540 --> 00:12:11,030
like the, will have
a different embedding

309
00:12:11,030 --> 00:12:13,370
in the sense of the green
representation here,

310
00:12:13,370 --> 00:12:15,930
depending on where it
appears in your sequence.

311
00:12:15,930 --> 00:12:18,500
So right from the
get-go, we have a notion

312
00:12:18,500 --> 00:12:20,960
of context sensitivity
even before we've

313
00:12:20,960 --> 00:12:25,720
started to connect things in
all sorts of interesting ways.

314
00:12:25,720 --> 00:12:27,940
Now let's move to some
current issues and efforts,

315
00:12:27,940 --> 00:12:29,982
some high-level things
that you might think about

316
00:12:29,982 --> 00:12:31,570
as you work through this unit.

317
00:12:31,570 --> 00:12:33,970
This is a really nice
graph from the ELECTRA

318
00:12:33,970 --> 00:12:35,950
paper from Clark, et al.

319
00:12:35,950 --> 00:12:38,770
Along the x-axis here, we have
floating point operations,

320
00:12:38,770 --> 00:12:41,290
which you could think of
as a kind of basic measure

321
00:12:41,290 --> 00:12:44,860
of compute resources needed to
create these representations.

322
00:12:44,860 --> 00:12:47,020
And along the y-axis,
we have GLUE score.

323
00:12:47,020 --> 00:12:50,110
So that's like a
standard NLU benchmark.

324
00:12:50,110 --> 00:12:51,730
And the point of
this plot here is

325
00:12:51,730 --> 00:12:53,970
that we're reaching kind
of diminishing returns.

326
00:12:53,970 --> 00:12:57,160
So we had rapid increases
from GloVE, GPT,

327
00:12:57,160 --> 00:12:59,920
and up through to BERT,
where we're really

328
00:12:59,920 --> 00:13:02,732
doing much better on
these GLUE scores.

329
00:13:02,732 --> 00:13:04,690
We're increasing the
floating point operations.

330
00:13:04,690 --> 00:13:06,400
But it seems to be
commensurate with how

331
00:13:06,400 --> 00:13:08,110
we're doing on the benchmark.

332
00:13:08,110 --> 00:13:11,230
But now with these larger
models like XLNet and RoBERTa,

333
00:13:11,230 --> 00:13:13,000
it's arguably the
case that we're

334
00:13:13,000 --> 00:13:15,010
reaching diminishing returns.

335
00:13:15,010 --> 00:13:18,700
RoBERTa involves more than
3,000 times the floating point

336
00:13:18,700 --> 00:13:21,010
operations of GloVe.

337
00:13:21,010 --> 00:13:24,940
But it's not that much
better along this y-axis

338
00:13:24,940 --> 00:13:27,340
than some of its simpler
variants like BERT-Base.

339
00:13:27,340 --> 00:13:29,257
And so this is something
we should think about

340
00:13:29,257 --> 00:13:31,960
when we think about the
costs in terms of money,

341
00:13:31,960 --> 00:13:34,120
and in the environments,
and energy,

342
00:13:34,120 --> 00:13:38,340
and so forth when we think about
developing these large models.

343
00:13:38,340 --> 00:13:40,030
And here's a really
extreme case.

344
00:13:40,030 --> 00:13:42,400
Who knows how long we
can train these things

345
00:13:42,400 --> 00:13:45,020
or how much benefit
we'll get when we do so.

346
00:13:45,020 --> 00:13:47,080
But at a certain point,
we're likely to incur

347
00:13:47,080 --> 00:13:49,570
costs that are
larger than any gains

348
00:13:49,570 --> 00:13:52,780
that we can justify on the
problems we're trying to solve.

349
00:13:52,780 --> 00:13:54,850
And that goes and leads
us to this lovely paper,

350
00:13:54,850 --> 00:13:57,310
which talks about the
environmental footprint

351
00:13:57,310 --> 00:13:58,990
of training these
really big models.

352
00:13:58,990 --> 00:14:01,720
And it just shows that by
training a big transformer

353
00:14:01,720 --> 00:14:05,560
from scratch, it really incurs
a large environmental cost.

354
00:14:05,560 --> 00:14:07,060
That's certainly
something we should

355
00:14:07,060 --> 00:14:09,250
have in mind as we think
about using these models.

356
00:14:09,250 --> 00:14:11,360
For me, it's a complicated
question though.

357
00:14:11,360 --> 00:14:14,710
Because it's offset by the fact
that, by and large, all of us

358
00:14:14,710 --> 00:14:17,860
aren't training these from
scratch, but rather, benefiting

359
00:14:17,860 --> 00:14:21,580
from publicly available
pretrained representations.

360
00:14:21,580 --> 00:14:24,130
So while the pretraining
for that one version

361
00:14:24,130 --> 00:14:26,320
had a large
environmental cost, it

362
00:14:26,320 --> 00:14:29,020
feels like it's kind of offset
by the fact that a lot of us

363
00:14:29,020 --> 00:14:30,080
are benefiting from it.

364
00:14:30,080 --> 00:14:32,350
And it might be that
in aggregate, this

365
00:14:32,350 --> 00:14:35,590
is less environmentally
costly than the old days

366
00:14:35,590 --> 00:14:38,080
when all of us always
trained all of our models

367
00:14:38,080 --> 00:14:39,620
literally from scratch.

368
00:14:39,620 --> 00:14:41,710
I just don't know how to
do the calculations here.

369
00:14:41,710 --> 00:14:45,190
But I do know that increased
access has been empowering

370
00:14:45,190 --> 00:14:47,170
and is likely offsetting
some of the costs.

371
00:14:47,170 --> 00:14:50,018
And a lot of that is due to the
contributions of the Hugging

372
00:14:50,018 --> 00:14:50,560
Face library.

373
00:14:50,560 --> 00:14:53,028


374
00:14:53,028 --> 00:14:55,070
There are a lot of efforts
along these same lines

375
00:14:55,070 --> 00:14:57,260
to make BERT smaller
by compressing it

376
00:14:57,260 --> 00:15:00,590
literally fewer dimensions than
other kinds of simplifications

377
00:15:00,590 --> 00:15:02,930
of the training process,
and BERT distillation,

378
00:15:02,930 --> 00:15:04,080
and so forth.

379
00:15:04,080 --> 00:15:06,230
Here are two outstanding
contributions,

380
00:15:06,230 --> 00:15:09,440
kind of compendiums of lots of
different ideas in this space.

381
00:15:09,440 --> 00:15:11,840
And I also highly
recommend this lovely paper

382
00:15:11,840 --> 00:15:14,210
called "A Primer in
BERTology," which

383
00:15:14,210 --> 00:15:16,580
explores a lot of
different aspects of what

384
00:15:16,580 --> 00:15:20,510
we know about BERT and how
it works, various variations

385
00:15:20,510 --> 00:15:22,187
people have tried,
various things

386
00:15:22,187 --> 00:15:24,020
that people have done
to probe these models,

387
00:15:24,020 --> 00:15:25,670
and understand their
learning dynamics,

388
00:15:25,670 --> 00:15:26,780
and so forth and so on.

389
00:15:26,780 --> 00:15:29,750
It's a very rich
contribution, certainly can

390
00:15:29,750 --> 00:15:34,330
be a resource for you as you
think about these models.

391
00:15:34,330 --> 00:15:38,152
And just because we don't
have time to cover them all,

392
00:15:38,152 --> 00:15:40,360
there are a bunch of
interesting Transformer variants

393
00:15:40,360 --> 00:15:42,717
that we will not be able
to discuss in detail.

394
00:15:42,717 --> 00:15:44,050
I thought I'd mention them here.

395
00:15:44,050 --> 00:15:47,200
SBERT is an attempt to develop
sentence-level representations

396
00:15:47,200 --> 00:15:50,620
from BERT that are particularly
good at finding which sentences

397
00:15:50,620 --> 00:15:52,810
are similar to which
other sentences according

398
00:15:52,810 --> 00:15:54,100
to cosine similarity.

399
00:15:54,100 --> 00:15:55,900
And I think that could
be a powerful mode

400
00:15:55,900 --> 00:15:58,120
of thinking about
these representations

401
00:15:58,120 --> 00:15:59,920
and also of practical
utility if you

402
00:15:59,920 --> 00:16:03,940
need to find which sentences
are similar to which others.

403
00:16:03,940 --> 00:16:07,330
You have probably heard of
GPT, the Generative Pre-trained

404
00:16:07,330 --> 00:16:10,130
Transformer, in
various of its forms.

405
00:16:10,130 --> 00:16:12,652
You can get GPT-2
from Hugging Space--

406
00:16:12,652 --> 00:16:13,360
and Hugging Face.

407
00:16:13,360 --> 00:16:15,280
And you have unfettered
access to it.

408
00:16:15,280 --> 00:16:17,320
And of course, there's
more restrictive access

409
00:16:17,320 --> 00:16:19,360
at this point to GPT-3.

410
00:16:19,360 --> 00:16:21,130
These are conditional
language models,

411
00:16:21,130 --> 00:16:22,600
so quite different from BERT.

412
00:16:22,600 --> 00:16:24,070
And they might be
better than BERT

413
00:16:24,070 --> 00:16:26,830
for things like truly
conditional language

414
00:16:26,830 --> 00:16:28,770
generation.

415
00:16:28,770 --> 00:16:31,710
XLNet is an attempt to
bring in much more context

416
00:16:31,710 --> 00:16:33,270
into these models.

417
00:16:33,270 --> 00:16:35,480
It stands for Xtra
Long Transformer.

418
00:16:35,480 --> 00:16:37,350
So if you need to
process long sequences,

419
00:16:37,350 --> 00:16:38,670
this might be a good choice.

420
00:16:38,670 --> 00:16:40,170
And this is also
an attempt to bring

421
00:16:40,170 --> 00:16:43,110
in some of the benefits of
conditional language models

422
00:16:43,110 --> 00:16:47,072
into a mode that is more
bidirectional, the way BERT is.

423
00:16:47,072 --> 00:16:50,660
T5 is another conditional
language mode, as is BART.

424
00:16:50,660 --> 00:16:52,550
These models might be
better choices for you

425
00:16:52,550 --> 00:16:54,300
if you need to actually
generate language.

426
00:16:54,300 --> 00:16:56,240
What I think the
standard wisdom is

427
00:16:56,240 --> 00:16:58,280
that models like
BERT and RoBERTa

428
00:16:58,280 --> 00:17:01,060
are better if you simply
need good representations

429
00:17:01,060 --> 00:17:03,320
for fine-tuning on a
classification problem,

430
00:17:03,320 --> 00:17:05,270
for example.

431
00:17:05,270 --> 00:17:07,190
More models will
appear every day.

432
00:17:07,190 --> 00:17:09,079
And I think it's worth
trying to stay up

433
00:17:09,079 --> 00:17:11,690
to speed on the various
developments in this space.

434
00:17:11,690 --> 00:17:15,940
Because this is probably just
the tip of the iceberg here.

435
00:17:15,940 --> 00:17:20,000


