1
00:00:00,000 --> 00:00:05,860


2
00:00:05,860 --> 00:00:07,110
OMAR KHATTAB: Hello, everyone.

3
00:00:07,110 --> 00:00:11,330
Welcome to part 2 of our
series on NLU and IR.

4
00:00:11,330 --> 00:00:14,690
The screen cast will be a
crash course in classical IR,

5
00:00:14,690 --> 00:00:17,980
as well as evaluation methods
and information retrieval.

6
00:00:17,980 --> 00:00:20,620


7
00:00:20,620 --> 00:00:23,560
Let us first define the
simplest form of our task,

8
00:00:23,560 --> 00:00:26,170
namely ranked retrieval.

9
00:00:26,170 --> 00:00:29,320
We will be given a large
collection of text documents.

10
00:00:29,320 --> 00:00:32,530
This could be all of the
passages in Wikipedia,

11
00:00:32,530 --> 00:00:35,050
perhaps a crawl of
parts of the web

12
00:00:35,050 --> 00:00:37,480
or maybe all of the
documentation of Hugging Face

13
00:00:37,480 --> 00:00:40,870
or other software libraries.

14
00:00:40,870 --> 00:00:44,620
This corpus will be provided to
us offline, that is before we

15
00:00:44,620 --> 00:00:46,640
interact with any users.

16
00:00:46,640 --> 00:00:49,900
And we will be able to spend a
one time effort at organizing

17
00:00:49,900 --> 00:00:51,940
or otherwise
understanding the content

18
00:00:51,940 --> 00:00:57,250
of these documents in the corpus
before we start searching.

19
00:00:57,250 --> 00:00:59,830
Online though, we
will receive a query

20
00:00:59,830 --> 00:01:03,160
from the users, which
could be a natural language

21
00:01:03,160 --> 00:01:06,370
question written in
English, for example.

22
00:01:06,370 --> 00:01:08,770
The goal of our ranked
retrieval system

23
00:01:08,770 --> 00:01:11,920
will be to output a Top-K
list of documents, sorted

24
00:01:11,920 --> 00:01:14,410
in decreasing order of
relevance to the information

25
00:01:14,410 --> 00:01:16,930
need that the user
expressed in the query.

26
00:01:16,930 --> 00:01:19,720


27
00:01:19,720 --> 00:01:24,440
So this might be the top
10 or the top 100 results.

28
00:01:24,440 --> 00:01:29,510
So how do we conduct this
task of ranked retrieval?

29
00:01:29,510 --> 00:01:31,740
As it turns out, we've
already looked at the way

30
00:01:31,740 --> 00:01:35,750
for doing this before, when
discussing matrix designs.

31
00:01:35,750 --> 00:01:39,200
In particular, we know that
we can build Term-Document

32
00:01:39,200 --> 00:01:40,760
Occurrence Matrices.

33
00:01:40,760 --> 00:01:43,670
And in such a matrix
like the one shown,

34
00:01:43,670 --> 00:01:47,390
each term-document pair
has a corresponding cell

35
00:01:47,390 --> 00:01:50,030
in which the matrix will
store the number of times

36
00:01:50,030 --> 00:01:52,715
that the term appears in
the document, in our corpus.

37
00:01:52,715 --> 00:01:55,380


38
00:01:55,380 --> 00:01:57,420
Of course, we will
probably want to apply

39
00:01:57,420 --> 00:01:59,760
some sort of reweighting
here, because we don't want

40
00:01:59,760 --> 00:02:01,680
to work with these row counts.

41
00:02:01,680 --> 00:02:03,720
But once we've done
that, we can already

42
00:02:03,720 --> 00:02:07,890
answer queries that contain
just a single term pretty well.

43
00:02:07,890 --> 00:02:10,169
And to do that, we would
basically just return

44
00:02:10,169 --> 00:02:12,180
the K-documents with
the largest weight

45
00:02:12,180 --> 00:02:17,477
after normalization or other
processes for the single term.

46
00:02:17,477 --> 00:02:19,560
And again, as it turns
out, this is precisely what

47
00:02:19,560 --> 00:02:23,130
is done in classical IR if
we have just a single query--

48
00:02:23,130 --> 00:02:26,340
just a single term in our query.

49
00:02:26,340 --> 00:02:29,160
When we have multiple
terms in the same query,

50
00:02:29,160 --> 00:02:32,560
classical IR tends to
treat them independently.

51
00:02:32,560 --> 00:02:34,590
So we would basically
add the weights up

52
00:02:34,590 --> 00:02:38,470
across all of the terms
in the query per document.

53
00:02:38,470 --> 00:02:40,800
And then, that's the
score for the document.

54
00:02:40,800 --> 00:02:42,390
This is precisely
the computation

55
00:02:42,390 --> 00:02:46,380
that is shown here, where we
compute the relevance score

56
00:02:46,380 --> 00:02:48,480
between a query and a document.

57
00:02:48,480 --> 00:02:50,820
We would go over all
the terms in the query

58
00:02:50,820 --> 00:02:54,750
and simply add the corresponding
document-term weights

59
00:02:54,750 --> 00:02:57,960
for all of these terms
for that document.

60
00:02:57,960 --> 00:03:00,210
This gives us a score
for the document

61
00:03:00,210 --> 00:03:02,220
and we can then
return the K-documents

62
00:03:02,220 --> 00:03:05,040
with the largest total scores.

63
00:03:05,040 --> 00:03:08,290
Interestingly, this reduces
much of classical IR,

64
00:03:08,290 --> 00:03:10,110
of course, not all
of it, to thinking

65
00:03:10,110 --> 00:03:15,500
about how do we best weigh
each term-document pair, which

66
00:03:15,500 --> 00:03:18,110
has an undeniable
similarity to our first task

67
00:03:18,110 --> 00:03:20,780
this quarter in Homework 1.

68
00:03:20,780 --> 00:03:22,370
Except of course,
that here in IR,

69
00:03:22,370 --> 00:03:25,430
we look at the term to document
relevance, and not word

70
00:03:25,430 --> 00:03:26,870
to word relatedness.

71
00:03:26,870 --> 00:03:30,630


72
00:03:30,630 --> 00:03:33,240
So thinking about
term-document weighting,

73
00:03:33,240 --> 00:03:36,270
here are some intuitions
that might be useful,

74
00:03:36,270 --> 00:03:38,670
as we think about what makes
a strong term weighting

75
00:03:38,670 --> 00:03:40,586
model in IR.

76
00:03:40,586 --> 00:03:44,090
Of course, later, in the next
screencast in particular,

77
00:03:44,090 --> 00:03:49,540
we'll be looking at neural
models that go beyond this.

78
00:03:49,540 --> 00:03:53,350
But for now, perhaps the two
most prominent intuitions

79
00:03:53,350 --> 00:03:55,420
for term-document
weighting are connected

80
00:03:55,420 --> 00:03:59,890
to our first unit's discussion
of frequency and normalization.

81
00:03:59,890 --> 00:04:03,760
In particular, if a
term t occurs frequently

82
00:04:03,760 --> 00:04:05,190
in document d.

83
00:04:05,190 --> 00:04:06,820
The document is
more likely to be

84
00:04:06,820 --> 00:04:09,460
relevant for queries
that include the term t

85
00:04:09,460 --> 00:04:13,030
or so is one of our intuitions.

86
00:04:13,030 --> 00:04:18,130
And in terms of normalization,
if that term t is quite rare,

87
00:04:18,130 --> 00:04:21,040
so if it occurs in only
a few documents overall,

88
00:04:21,040 --> 00:04:25,060
we take that as a stronger
signal that document d

89
00:04:25,060 --> 00:04:28,090
is even more likely to be
relevant for queries including

90
00:04:28,090 --> 00:04:30,570
t.

91
00:04:30,570 --> 00:04:32,940
Lastly, if document
d is rather short,

92
00:04:32,940 --> 00:04:35,130
we take that as also
yet another signal

93
00:04:35,130 --> 00:04:37,890
that might increase our
confidence that the term

94
00:04:37,890 --> 00:04:43,130
t was included in that rather
short document for a reason.

95
00:04:43,130 --> 00:04:45,470
Taking a step back and
thinking more broadly,

96
00:04:45,470 --> 00:04:47,990
we're still functioning
under the same statement

97
00:04:47,990 --> 00:04:49,310
from the first unit.

98
00:04:49,310 --> 00:04:52,940
Our goal is ultimately to
amplify the important signals,

99
00:04:52,940 --> 00:04:56,480
trustworthy and the
unusual and to deemphasize

100
00:04:56,480 --> 00:04:57,770
the mundane and the quirky.

101
00:04:57,770 --> 00:05:01,190


102
00:05:01,190 --> 00:05:04,620
There are so many different
term-weighting functions in IR,

103
00:05:04,620 --> 00:05:07,880
but most of them are
directly inspired by TF-IDF

104
00:05:07,880 --> 00:05:11,390
and take a very similar
computational form.

105
00:05:11,390 --> 00:05:14,690
For TF-IDF, this is a
slightly different version

106
00:05:14,690 --> 00:05:16,490
to the one used in Unit 1.

107
00:05:16,490 --> 00:05:18,342
What I have here is
slightly different.

108
00:05:18,342 --> 00:05:20,300
This is the more popular
version in the context

109
00:05:20,300 --> 00:05:21,530
of IR applications.

110
00:05:21,530 --> 00:05:23,540
But TF-IDF is
overloaded frequently,

111
00:05:23,540 --> 00:05:27,300
and you will see
multiple implementations,

112
00:05:27,300 --> 00:05:29,230
if you go look for them.

113
00:05:29,230 --> 00:05:32,040
So we'll define N to be
the size of the collection.

114
00:05:32,040 --> 00:05:36,270
And DF or document
frequency of a term,

115
00:05:36,270 --> 00:05:41,290
DF of term to be the number
of documents that contain

116
00:05:41,290 --> 00:05:43,830
that term in the collection.

117
00:05:43,830 --> 00:05:48,180
Then TF or term frequency
of a term-document pair

118
00:05:48,180 --> 00:05:50,640
will be defined as the
logarithm of the frequency

119
00:05:50,640 --> 00:05:55,290
of this term in this
document, with 1 just

120
00:05:55,290 --> 00:05:58,550
for mathematical reasons.

121
00:05:58,550 --> 00:06:01,010
IDF or inverse
document frequency

122
00:06:01,010 --> 00:06:04,370
is defined as the logarithm
of N divided by the document

123
00:06:04,370 --> 00:06:05,330
frequency of the term.

124
00:06:05,330 --> 00:06:07,940


125
00:06:07,940 --> 00:06:10,430
TF-IDF is then nothing
but the product

126
00:06:10,430 --> 00:06:14,810
of these two values for each
query term summed up at the end

127
00:06:14,810 --> 00:06:17,810
to assign a single overall
score to each document

128
00:06:17,810 --> 00:06:19,610
by summing up across
all query terms,

129
00:06:19,610 --> 00:06:21,260
as we've discussed before.

130
00:06:21,260 --> 00:06:23,180
Of course, higher
scores are better

131
00:06:23,180 --> 00:06:25,040
and the Top-K
scoring documents are

132
00:06:25,040 --> 00:06:26,900
those that we would
return to the searcher

133
00:06:26,900 --> 00:06:30,430
if we were to use TF-IDF.

134
00:06:30,430 --> 00:06:35,710
Notice how both TF and IDF grow
sub-linearly, in particular,

135
00:06:35,710 --> 00:06:41,230
logarithmically with frequency
and 1 over DF, respectively.

136
00:06:41,230 --> 00:06:45,640


137
00:06:45,640 --> 00:06:48,220
A much stronger term
weighting model in practice

138
00:06:48,220 --> 00:06:51,590
is BM25 or best match number 25.

139
00:06:51,590 --> 00:06:53,620
And as you might imagine,
it took many attempts

140
00:06:53,620 --> 00:06:55,831
until BM25 was developed.

141
00:06:55,831 --> 00:06:58,480


142
00:06:58,480 --> 00:07:03,730
For our purposes, unlike
TF-IDF, term frequency in BM25

143
00:07:03,730 --> 00:07:07,390
saturates towards the
constant value for each term,

144
00:07:07,390 --> 00:07:09,730
and also, it penalizes
longer documents

145
00:07:09,730 --> 00:07:12,370
when counting frequencies,
since a longer

146
00:07:12,370 --> 00:07:17,360
document will naturally contain
more occurrences of its terms.

147
00:07:17,360 --> 00:07:19,790
These are the main
differences, and it really

148
00:07:19,790 --> 00:07:26,230
helps BM25 in practice be a much
stronger term weighting model.

149
00:07:26,230 --> 00:07:30,340
Now that we've decided the
behavior of these weighting

150
00:07:30,340 --> 00:07:32,800
functions or at least
a couple of them,

151
00:07:32,800 --> 00:07:36,280
how would we actually implement
this as an actual system

152
00:07:36,280 --> 00:07:39,660
that we could use for search?

153
00:07:39,660 --> 00:07:42,590
So let's think about this,
whereas the raw collection,

154
00:07:42,590 --> 00:07:46,850
the actual text, supports fast
access from documents to terms,

155
00:07:46,850 --> 00:07:51,350
so basically, [INAUDIBLE] gives
us the terms of each document.

156
00:07:51,350 --> 00:07:54,680
The term-document matrix
that we've studied so far,

157
00:07:54,680 --> 00:07:57,720
allows fast access from
a term to the documents.

158
00:07:57,720 --> 00:07:59,570
So it's a bit of
the reverse process.

159
00:07:59,570 --> 00:08:04,280
Unfortunately, the term-document
matrix is way too sparse

160
00:08:04,280 --> 00:08:07,040
and contains too many
zeros to be useful,

161
00:08:07,040 --> 00:08:09,560
since the average
term does not occur

162
00:08:09,560 --> 00:08:11,420
in the vast majority
of documents,

163
00:08:11,420 --> 00:08:13,570
if you think about it.

164
00:08:13,570 --> 00:08:15,730
For the inverted index,
that's where it comes in.

165
00:08:15,730 --> 00:08:18,550
This is a data structure
that solves this problem.

166
00:08:18,550 --> 00:08:21,820
It's essentially just a sparse
representation of our matrix

167
00:08:21,820 --> 00:08:25,330
here, which maps each unique
term in the collection.

168
00:08:25,330 --> 00:08:28,120
So each unique term
in our vocabulary

169
00:08:28,120 --> 00:08:31,300
is what we call a posting list.

170
00:08:31,300 --> 00:08:34,659
The posting list
of a term t simply

171
00:08:34,659 --> 00:08:37,900
enumerates all of the actual
occurrences of the term t

172
00:08:37,900 --> 00:08:41,710
in the documents, recording
both the ID of each document

173
00:08:41,710 --> 00:08:43,450
in which the term t appears.

174
00:08:43,450 --> 00:08:50,980
And also, its frequency
in each of these documents

175
00:08:50,980 --> 00:08:53,710
So beyond term weighting
models, IR, of course,

176
00:08:53,710 --> 00:08:56,250
contains lots of models
for other things.

177
00:08:56,250 --> 00:08:59,680
So they're models for expanding
queries and documents.

178
00:08:59,680 --> 00:09:02,050
This basically entails
adding new terms

179
00:09:02,050 --> 00:09:04,450
to queries or to
documents, or to both,

180
00:09:04,450 --> 00:09:06,580
to help with the
vocabulary mismatch problem

181
00:09:06,580 --> 00:09:09,700
that we discussed in the first
screencast of the series,

182
00:09:09,700 --> 00:09:12,580
basically, when queries and
documents use different terms

183
00:09:12,580 --> 00:09:14,870
to express the same thing.

184
00:09:14,870 --> 00:09:17,960
There's also plenty of work
on term dependence and phrase

185
00:09:17,960 --> 00:09:18,460
search.

186
00:09:18,460 --> 00:09:21,280
Notice that so far, we've
assumed the terms in each query

187
00:09:21,280 --> 00:09:23,200
and in each document
are independent,

188
00:09:23,200 --> 00:09:27,100
and we function in a
bag-of-words fashion.

189
00:09:27,100 --> 00:09:29,620
But work on term dependence
and phrase search

190
00:09:29,620 --> 00:09:32,290
relaxes these assumptions
that each query

191
00:09:32,290 --> 00:09:34,390
is a bag of independent terms.

192
00:09:34,390 --> 00:09:35,950
Lastly, there is
also lots of work

193
00:09:35,950 --> 00:09:39,160
on learning to rank
with various features,

194
00:09:39,160 --> 00:09:41,380
like how to estimate
relevance when documents have

195
00:09:41,380 --> 00:09:45,040
multiple fields, like maybe a
title, a body, some headings,

196
00:09:45,040 --> 00:09:47,200
a footer and also,
anchor text, which

197
00:09:47,200 --> 00:09:50,270
is a very strong signal when
you have it, like in web search.

198
00:09:50,270 --> 00:09:52,720
So this is basically
the text from links

199
00:09:52,720 --> 00:09:54,700
in other pages to your page.

200
00:09:54,700 --> 00:09:56,890
The text in those links
or around those links

201
00:09:56,890 --> 00:10:04,040
tends to be very useful
as a relevant signal.

202
00:10:04,040 --> 00:10:06,860
And of course, also things like
PageRank with link analysis

203
00:10:06,860 --> 00:10:09,530
and lots of other
features for IR,

204
00:10:09,530 --> 00:10:12,120
like recency and other stuff.

205
00:10:12,120 --> 00:10:15,300
But I think it's worth
mentioning that until recently,

206
00:10:15,300 --> 00:10:18,270
if you just had a collection
that you want to search

207
00:10:18,270 --> 00:10:21,030
and you didn't want
to do a lot of tuning,

208
00:10:21,030 --> 00:10:24,900
BM25 was a very strong
baseline on the best

209
00:10:24,900 --> 00:10:27,540
that you could do ad hoc,
so without lots of tuning

210
00:10:27,540 --> 00:10:31,440
and without lots of
training data, et cetera.

211
00:10:31,440 --> 00:10:33,360
And this only
changed a year or two

212
00:10:33,360 --> 00:10:36,390
ago with the advent of
BERT-based ranking, which we'll

213
00:10:36,390 --> 00:10:38,670
discuss in detail in
the next screencast

214
00:10:38,670 --> 00:10:39,810
of this set of the series.

215
00:10:39,810 --> 00:10:45,960


216
00:10:45,960 --> 00:10:50,130
OK, so we just
built an IR system.

217
00:10:50,130 --> 00:10:52,230
How do we evaluate our work?

218
00:10:52,230 --> 00:10:54,150
What is success like?

219
00:10:54,150 --> 00:10:56,100
Well, a search system,
as you can imagine,

220
00:10:56,100 --> 00:10:59,070
must be both efficient
and effective.

221
00:10:59,070 --> 00:11:01,710
If we had infinite time
and infinite resources,

222
00:11:01,710 --> 00:11:04,830
we would just hire experts to
look through all the documents

223
00:11:04,830 --> 00:11:06,880
one by one to
conduct the search,

224
00:11:06,880 --> 00:11:11,950
but clearly, we don't
have that sort of ability.

225
00:11:11,950 --> 00:11:14,290
So efficiency in
IR is paramount,

226
00:11:14,290 --> 00:11:16,510
after all, we want
our retrieval models

227
00:11:16,510 --> 00:11:19,300
to work with subsecond
latencies for collections that

228
00:11:19,300 --> 00:11:21,340
may have hundreds of
millions of documents,

229
00:11:21,340 --> 00:11:24,130
if not even larger than that.

230
00:11:24,130 --> 00:11:26,740
The most common measure
of efficiency in IR

231
00:11:26,740 --> 00:11:29,620
is latency, which is simply
the time it takes to run

232
00:11:29,620 --> 00:11:31,240
one query through the system.

233
00:11:31,240 --> 00:11:34,000
Say, on average or
perhaps, at the tail,

234
00:11:34,000 --> 00:11:37,460
like the 95th
percentile, for example.

235
00:11:37,460 --> 00:11:39,520
But you can also
measure throughput

236
00:11:39,520 --> 00:11:41,410
in queries per second.

237
00:11:41,410 --> 00:11:44,350
Space, how much, maybe
the inverted index

238
00:11:44,350 --> 00:11:48,790
takes on disk versus say
a term-document matrix.

239
00:11:48,790 --> 00:11:51,172
How well do you scale to
different collection sizes

240
00:11:51,172 --> 00:11:52,630
in terms of the
number of documents

241
00:11:52,630 --> 00:11:54,940
or the size of the documents?

242
00:11:54,940 --> 00:11:57,250
And how do you perform on
the different query loads?

243
00:11:57,250 --> 00:12:00,880
Many queries, few queries,
short queries, long queries.

244
00:12:00,880 --> 00:12:03,520
And lastly, of course, what
sort of hardware do you require?

245
00:12:03,520 --> 00:12:04,930
Is it just one CPU core?

246
00:12:04,930 --> 00:12:05,800
Many cores?

247
00:12:05,800 --> 00:12:07,840
A bunch of GPUs?

248
00:12:07,840 --> 00:12:09,850
But latency tends to
be kind of, once you've

249
00:12:09,850 --> 00:12:13,810
determined the other ones, it's
the go-to metric in most cases.

250
00:12:13,810 --> 00:12:16,470


251
00:12:16,470 --> 00:12:18,890
More central to our discussion
today, and we'll focus on

252
00:12:18,890 --> 00:12:21,200
this for the rest
of the screencast

253
00:12:21,200 --> 00:12:23,960
is IR effectiveness
or the quality,

254
00:12:23,960 --> 00:12:27,620
basically of an IR system.

255
00:12:27,620 --> 00:12:31,510
And here we ask, do our
top-k rankings for a query

256
00:12:31,510 --> 00:12:35,100
satisfy the users'
information need?

257
00:12:35,100 --> 00:12:37,950
Answering this question
tends to be harder

258
00:12:37,950 --> 00:12:40,530
than evaluation for typical
machine learning tasks,

259
00:12:40,530 --> 00:12:43,350
like classification
or regression

260
00:12:43,350 --> 00:12:47,130
because we're not really just
taking an item and assigning it

261
00:12:47,130 --> 00:12:48,180
a class.

262
00:12:48,180 --> 00:12:51,630
We're trying to rank all
of the items in our corpus

263
00:12:51,630 --> 00:12:54,790
with respect to a query.

264
00:12:54,790 --> 00:12:56,980
In practice, if you
have lots of users,

265
00:12:56,980 --> 00:13:00,580
you could run online
experiments where you basically

266
00:13:00,580 --> 00:13:03,760
give different versions of
your system to different users

267
00:13:03,760 --> 00:13:08,770
and compare some metrics of
satisfaction or conversion,

268
00:13:08,770 --> 00:13:12,340
basically in terms of
purchases or otherwise.

269
00:13:12,340 --> 00:13:14,530
But for research
purposes, we're typically

270
00:13:14,530 --> 00:13:17,530
interested in reusable
test collections.

271
00:13:17,530 --> 00:13:20,860
That's collections that allow
us to evaluate IR models offline

272
00:13:20,860 --> 00:13:22,810
and then, compare them
against each other.

273
00:13:22,810 --> 00:13:25,700


274
00:13:25,700 --> 00:13:28,787
Building a test collection
entails three things.

275
00:13:28,787 --> 00:13:30,370
First, we need to
decide on a document

276
00:13:30,370 --> 00:13:34,150
collection for our corpus,
a set of test queries,

277
00:13:34,150 --> 00:13:38,020
and we need to find or get or
produce relevant assessments

278
00:13:38,020 --> 00:13:39,820
for each query.

279
00:13:39,820 --> 00:13:41,590
If resources
permit, a collection

280
00:13:41,590 --> 00:13:45,520
could also include the
train dev split of queries,

281
00:13:45,520 --> 00:13:47,470
but given the high
annotation cost,

282
00:13:47,470 --> 00:13:50,980
it's actually not uncommon
in IR to find or create

283
00:13:50,980 --> 00:13:53,600
only a test set.

284
00:13:53,600 --> 00:13:55,310
The key component
of a test collection

285
00:13:55,310 --> 00:13:57,200
is the relevance assessments.

286
00:13:57,200 --> 00:13:59,300
These are basically
human annotated labels

287
00:13:59,300 --> 00:14:02,340
for each query that
enumerate for us,

288
00:14:02,340 --> 00:14:07,040
whether specific documents are
relevant or not to that query.

289
00:14:07,040 --> 00:14:10,460
These query document
assessments can either be binary

290
00:14:10,460 --> 00:14:14,970
or they could take on a more
fine grained graded nature.

291
00:14:14,970 --> 00:14:17,240
An example of that is
grading a query document pair

292
00:14:17,240 --> 00:14:21,590
as -1, 0, 1 or 2,
with meanings of hey,

293
00:14:21,590 --> 00:14:23,720
this is a junk document, -1.

294
00:14:23,720 --> 00:14:26,330
You should not retrieve it
for any query or this document

295
00:14:26,330 --> 00:14:29,120
is irrelevant, but
it might be useful

296
00:14:29,120 --> 00:14:30,835
for other queries
or this document

297
00:14:30,835 --> 00:14:32,210
is quite relevant
for this query,

298
00:14:32,210 --> 00:14:33,830
but it's not a perfect match.

299
00:14:33,830 --> 00:14:36,920
Or here is a really,
really good match

300
00:14:36,920 --> 00:14:39,470
for our query, which would
be a score of 2 or 3,

301
00:14:39,470 --> 00:14:42,740
depending on the
grades that you're

302
00:14:42,740 --> 00:14:47,260
using for the assessments.

303
00:14:47,260 --> 00:14:49,330
As you might imagine,
because we work

304
00:14:49,330 --> 00:14:52,130
with potentially many
millions of documents,

305
00:14:52,130 --> 00:14:55,390
it's usually infeasible to
judge every single document

306
00:14:55,390 --> 00:14:57,860
for every single query.

307
00:14:57,860 --> 00:15:00,670
So instead, we're often
forced to make the assumption

308
00:15:00,670 --> 00:15:03,880
that unjudged documents
are not relevant

309
00:15:03,880 --> 00:15:08,080
or at least to ignore them
in some metrics of IR.

310
00:15:08,080 --> 00:15:11,260
Though for most purposes, they
are treated as not relevant.

311
00:15:11,260 --> 00:15:14,260
Some test collections take
this further and only label

312
00:15:14,260 --> 00:15:17,230
one or two key documents
per query as relevant

313
00:15:17,230 --> 00:15:20,512
and assume everything
else is not relevant.

314
00:15:20,512 --> 00:15:21,970
So this tends to
be useful when you

315
00:15:21,970 --> 00:15:23,840
work with particular data sets.

316
00:15:23,840 --> 00:15:26,251
And you want to keep it in
mind as you do evaluation.

317
00:15:26,251 --> 00:15:30,020


318
00:15:30,020 --> 00:15:32,790
So many of the test
collections out there in IR

319
00:15:32,790 --> 00:15:36,530
are annotated by TREC or the
Text Retrieval Conference,

320
00:15:36,530 --> 00:15:40,580
which includes annual tracks
for competing and comparing

321
00:15:40,580 --> 00:15:42,320
IR systems.

322
00:15:42,320 --> 00:15:45,230
For instance, the
2021 TREC Conference

323
00:15:45,230 --> 00:15:47,000
has tracks for
search in the context

324
00:15:47,000 --> 00:15:50,600
of conversational assistance,
health misinformation,

325
00:15:50,600 --> 00:15:54,260
fair ranking, and has a
very popular deep learning

326
00:15:54,260 --> 00:15:57,610
track, as well, which we'll
discuss in more detail.

327
00:15:57,610 --> 00:16:00,730
Each TREC campaign
emphasizes careful evaluation

328
00:16:00,730 --> 00:16:02,690
with a very small
set of queries.

329
00:16:02,690 --> 00:16:06,970
So just 50 queries is a
very typical size, actually.

330
00:16:06,970 --> 00:16:09,820
But TREC extensively judges
many, many documents,

331
00:16:09,820 --> 00:16:12,770
possibly hundreds of documents
or even more for each query

332
00:16:12,770 --> 00:16:13,270
here.

333
00:16:13,270 --> 00:16:15,840


334
00:16:15,840 --> 00:16:17,340
So you can imagine
an alternative,

335
00:16:17,340 --> 00:16:20,220
which we'll look at next where
you have lots of queries.

336
00:16:20,220 --> 00:16:21,890
But you only judge
a very small number

337
00:16:21,890 --> 00:16:26,750
of key documents for those
queries with the intention

338
00:16:26,750 --> 00:16:28,430
that the performance
that you get

339
00:16:28,430 --> 00:16:31,730
will average out over a
large enough set of queries.

340
00:16:31,730 --> 00:16:34,580
And this is exactly what
happens in MS MARCO Ranking

341
00:16:34,580 --> 00:16:41,480
Tasks, which is a collection
of really popular IR

342
00:16:41,480 --> 00:16:44,150
benchmarks by Microsoft.

343
00:16:44,150 --> 00:16:47,000
And this MARCO contains more
than half a million Bing search

344
00:16:47,000 --> 00:16:47,660
queries.

345
00:16:47,660 --> 00:16:51,500
And this is the largest
public IR benchmark.

346
00:16:51,500 --> 00:16:56,030
Each query here is assessed with
one or two relevant documents,

347
00:16:56,030 --> 00:16:59,700
and we assume everything
else is not relevant,

348
00:16:59,700 --> 00:17:02,600
and having this
sparse annotation

349
00:17:02,600 --> 00:17:05,780
is often not a problem at all
for training because we have

350
00:17:05,780 --> 00:17:08,030
so many training instances.

351
00:17:08,030 --> 00:17:11,930
And so, MS MARCO provides a
tremendous resource for us

352
00:17:11,930 --> 00:17:14,730
when it comes to building
and training IR models,

353
00:17:14,730 --> 00:17:17,368
especially in the neural domain.

354
00:17:17,368 --> 00:17:19,338
It also turns out that
sparse labels are not

355
00:17:19,339 --> 00:17:21,290
too bad for evaluation, either.

356
00:17:21,290 --> 00:17:24,410
Especially because of the
size of the test queries,

357
00:17:24,410 --> 00:17:27,050
we can use many thousands
of test queries, an average

358
00:17:27,050 --> 00:17:30,140
of results across all of them
to get a pretty reliable signal

359
00:17:30,140 --> 00:17:33,075
about how different
systems compare.

360
00:17:33,075 --> 00:17:34,950
There are multiple test
collections out there

361
00:17:34,950 --> 00:17:36,840
on top of MS MARCO.

362
00:17:36,840 --> 00:17:38,970
And so, there's the
original passage ranking

363
00:17:38,970 --> 00:17:41,730
task and newer
document ranking task

364
00:17:41,730 --> 00:17:44,220
where the documents
are much longer,

365
00:17:44,220 --> 00:17:46,080
but there's fewer of them.

366
00:17:46,080 --> 00:17:48,570
And then, there is also a
track, the deep learning track,

367
00:17:48,570 --> 00:17:50,040
which we've mentioned
before, which

368
00:17:50,040 --> 00:17:53,110
is happening every
year since 2019

369
00:17:53,110 --> 00:17:55,920
and which uses the MS
MARCO data, especially

370
00:17:55,920 --> 00:17:57,780
for training, mostly.

371
00:17:57,780 --> 00:18:01,470
But has far fewer queries
for testing with lots

372
00:18:01,470 --> 00:18:06,210
more labels for evaluation,
a lot more extensive

373
00:18:06,210 --> 00:18:08,710
assessments and
judgments for evaluation,

374
00:18:08,710 --> 00:18:12,790
so these are much denser labels.

375
00:18:12,790 --> 00:18:15,850
There are also plenty of other
rather domain specific IR

376
00:18:15,850 --> 00:18:21,070
benchmarks, many of which
are collected in this table

377
00:18:21,070 --> 00:18:22,660
by Nandan et al.

378
00:18:22,660 --> 00:18:26,160
in a very recent preprint.

379
00:18:26,160 --> 00:18:28,410
As you can see, these
benchmarks vary greatly

380
00:18:28,410 --> 00:18:30,810
in terms of the
training size, if there

381
00:18:30,810 --> 00:18:32,720
is any training at all.

382
00:18:32,720 --> 00:18:35,010
The test set size, the
average query length,

383
00:18:35,010 --> 00:18:39,470
the average document length,
and many other factors.

384
00:18:39,470 --> 00:18:44,210
BEIR, or benchmarking for IR
is a recent effort by Nandan

385
00:18:44,210 --> 00:18:47,360
et al. here to use all
of these different data

386
00:18:47,360 --> 00:18:51,740
sets for zero shot or out of
domain testing of IR models.

387
00:18:51,740 --> 00:18:55,430
Specifically, in BEIR, we
take already trained IR models

388
00:18:55,430 --> 00:18:58,790
that do not have access to any
validation or training data

389
00:18:58,790 --> 00:19:02,840
on these downstream IR tasks
and test them out of the box

390
00:19:02,840 --> 00:19:05,780
to observe their out of
domain retrieval quality,

391
00:19:05,780 --> 00:19:09,545
so that is without training
on these new domains.

392
00:19:09,545 --> 00:19:16,230


393
00:19:16,230 --> 00:19:18,870
OK, so we now have
a test collection

394
00:19:18,870 --> 00:19:22,300
with queries, documents,
and assessments.

395
00:19:22,300 --> 00:19:25,970
How do we compare IR
systems in this collection?

396
00:19:25,970 --> 00:19:29,240
First, we will ask each IR
system to produce its Top-K

397
00:19:29,240 --> 00:19:31,760
ranking, say its top 10 results.

398
00:19:31,760 --> 00:19:35,480
And we'll use an IR metric to
compare all of these systems

399
00:19:35,480 --> 00:19:37,580
at that cutoff K.

400
00:19:37,580 --> 00:19:39,800
The choice of IR
metric and the cutoff K

401
00:19:39,800 --> 00:19:41,990
will depend entirely
on the task,

402
00:19:41,990 --> 00:19:46,390
so I will briefly motivate each
metric as we go through them.

403
00:19:46,390 --> 00:19:48,190
All of the metrics
we will go through

404
00:19:48,190 --> 00:19:50,830
are simply averaged
across all queries.

405
00:19:50,830 --> 00:19:52,983
And so, to keep
things simple, I will

406
00:19:52,983 --> 00:19:54,400
show the computation
of the metric

407
00:19:54,400 --> 00:19:56,500
for just one query
in each case, but you

408
00:19:56,500 --> 00:20:01,530
want to keep in mind that this
is averaged across queries.

409
00:20:01,530 --> 00:20:05,130
Let us start with two of
the simplest IR metrics,

410
00:20:05,130 --> 00:20:07,380
which are Success and MRR.

411
00:20:07,380 --> 00:20:10,890
For a given query, let
rank be the position

412
00:20:10,890 --> 00:20:13,320
of the first relevant
document that we can see

413
00:20:13,320 --> 00:20:16,060
in the Top-K list of results.

414
00:20:16,060 --> 00:20:18,820
Success@K will
just be 1, if there

415
00:20:18,820 --> 00:20:23,110
is a relevant result in the
Top-K list, and 0 otherwise.

416
00:20:23,110 --> 00:20:25,240
This is a very simple
metric, as you can see,

417
00:20:25,240 --> 00:20:27,190
that can be useful
in cases where

418
00:20:27,190 --> 00:20:28,840
we assume that the
user just needs

419
00:20:28,840 --> 00:20:31,720
one relevant result
anywhere in the Top-K.

420
00:20:31,720 --> 00:20:33,490
And in particular,
it can be useful

421
00:20:33,490 --> 00:20:36,910
if our retrieval is spread
to a downstream model that

422
00:20:36,910 --> 00:20:39,770
looks at the Top-K results and
then does something with them.

423
00:20:39,770 --> 00:20:42,187
So you can read all of them,
and it would read all of them

424
00:20:42,187 --> 00:20:46,430
anyway, so we're just interested
in buying the irrelevance here.

425
00:20:46,430 --> 00:20:51,470
Mean reciprocal rank or MRR
also assumes that these only

426
00:20:51,470 --> 00:20:54,640
needs one relevant
query in the Top-K,

427
00:20:54,640 --> 00:20:57,277
but it assumes that
the user does care

428
00:20:57,277 --> 00:20:59,110
about the position of
that relevant document

429
00:20:59,110 --> 00:21:00,760
in the ranking.

430
00:21:00,760 --> 00:21:03,950
So a relevant document at the
second position for example,

431
00:21:03,950 --> 00:21:06,760
is only given half of the
weight of a relevant document

432
00:21:06,760 --> 00:21:07,615
in the top position.

433
00:21:07,615 --> 00:21:16,650


434
00:21:16,650 --> 00:21:19,760
You're probably already familiar
with precision and recall,

435
00:21:19,760 --> 00:21:22,580
but let's define them here
in the context of Top-K

436
00:21:22,580 --> 00:21:24,310
ranked retrieval.

437
00:21:24,310 --> 00:21:28,540
For a given query, let Ret
of K be the Top-K retrieved

438
00:21:28,540 --> 00:21:31,570
documents, that set of
Top-K retrieved documents.

439
00:21:31,570 --> 00:21:33,790
And let Rel be the
set of all documents

440
00:21:33,790 --> 00:21:39,140
that we judged as relevance
as part of our assessments.

441
00:21:39,140 --> 00:21:41,180
In this case,
Precision@K is just

442
00:21:41,180 --> 00:21:42,770
a fraction of the
retrieved items

443
00:21:42,770 --> 00:21:44,750
that are actually relevant.

444
00:21:44,750 --> 00:21:46,640
And recall that
K is the fraction

445
00:21:46,640 --> 00:21:49,550
of all the relevant items
that are actually retrieved.

446
00:21:49,550 --> 00:21:53,870


447
00:21:53,870 --> 00:21:57,710
A pretty popular metric is also
a MAP or mean average precision

448
00:21:57,710 --> 00:22:00,590
or just average precision
for one query, which

449
00:22:00,590 --> 00:22:03,140
essentially brings together
notions from both precision

450
00:22:03,140 --> 00:22:04,430
and recall.

451
00:22:04,430 --> 00:22:06,950
To compute average
precision for one query,

452
00:22:06,950 --> 00:22:11,150
we will add up the Precision@i
for every position i from 1

453
00:22:11,150 --> 00:22:14,900
through K, where the i-th
document is relevant.

454
00:22:14,900 --> 00:22:16,730
We will divide
this whole quantity

455
00:22:16,730 --> 00:22:19,460
by the total number of documents
that we're judged as relevant

456
00:22:19,460 --> 00:22:20,120
for this query.

457
00:22:20,120 --> 00:22:23,150


458
00:22:23,150 --> 00:22:26,090
All of the metrics that we've
considered so far only interact

459
00:22:26,090 --> 00:22:28,070
with binary relevance,
that is they

460
00:22:28,070 --> 00:22:30,560
just care whether each
document that is retrieved

461
00:22:30,560 --> 00:22:33,260
is considered relevant
or not relevant.

462
00:22:33,260 --> 00:22:36,200
DCG or discounted
cumulative gain

463
00:22:36,200 --> 00:22:37,640
works with graded relevance.

464
00:22:37,640 --> 00:22:41,520
So for instance, 0, 1, 2, and 3.

465
00:22:41,520 --> 00:22:44,100
For each position in the
ranking from 1 through K,

466
00:22:44,100 --> 00:22:46,080
we will divide the
graded relevance

467
00:22:46,080 --> 00:22:49,140
of the retrieved
document at that position

468
00:22:49,140 --> 00:22:51,960
by the logarithm of the
position, which essentially

469
00:22:51,960 --> 00:22:54,330
discounts the value
of a relevant document

470
00:22:54,330 --> 00:22:58,420
if it appears late
in the ranking.

471
00:22:58,420 --> 00:23:00,760
Unlike the other
metrics, the maximum DCG

472
00:23:00,760 --> 00:23:03,610
is often not equal to 1.

473
00:23:03,610 --> 00:23:07,810
So we can also compute
normalized DCG or NDCG

474
00:23:07,810 --> 00:23:11,590
by dividing for each
query by the ideal DCG.

475
00:23:11,590 --> 00:23:15,130
This is obtained basically, if
all of the relevant documents

476
00:23:15,130 --> 00:23:17,350
are at the top of
our Top-K ranking,

477
00:23:17,350 --> 00:23:20,890
and they are sorted by
decreasing relevance,

478
00:23:20,890 --> 00:23:24,730
so all of the 2's before all of
the 1's before all of the 0's

479
00:23:24,730 --> 00:23:28,580
in this case that
are not relevant.

480
00:23:28,580 --> 00:23:31,480
All right, having discussed
classical IR and evaluation

481
00:23:31,480 --> 00:23:34,240
in this screencast, we
will focus on Neural IR

482
00:23:34,240 --> 00:23:35,710
and in particular,
state of the art

483
00:23:35,710 --> 00:23:39,250
IR models that use what
we've learned so far in NLU

484
00:23:39,250 --> 00:23:41,970
in the next screencast.

485
00:23:41,970 --> 00:23:47,000


