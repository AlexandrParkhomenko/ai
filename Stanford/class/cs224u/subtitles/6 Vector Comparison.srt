1
00:00:05,040 --> 00:00:08,000
welcome back everyone this is part three

2
00:00:06,559 --> 00:00:09,279
in our series on distributed word

3
00:00:09,279 --> 00:00:13,839
talking about vector comparison methods

4
00:00:12,240 --> 00:00:15,599
to try to make this discussion pretty

5
00:00:13,839 --> 00:00:18,079
intuitive i'm going to ground things in

6
00:00:15,599 --> 00:00:20,320
this running example on the left i have

7
00:00:18,079 --> 00:00:22,159
a very small vector space model we have

8
00:00:22,160 --> 00:00:25,920
and you can imagine that we've measured

9
00:00:23,679 --> 00:00:28,079
two dimensions dx and dy you could think

10
00:00:25,920 --> 00:00:29,359
of them as documents if you wanted

11
00:00:28,079 --> 00:00:31,839
there are two perspectives that you

12
00:00:29,359 --> 00:00:34,079
might take on this vector space model

13
00:00:31,839 --> 00:00:35,200
the first is just at the level of raw

14
00:00:35,200 --> 00:00:38,879
b and c seem to be united they are

15
00:00:36,960 --> 00:00:39,920
frequent in both the x and the y

16
00:00:39,920 --> 00:00:44,879
whereas a is comparatively infrequent

17
00:00:44,878 --> 00:00:49,039
that's the first perspective the second

18
00:00:46,878 --> 00:00:50,960
perspective though is more subtle you

19
00:00:49,039 --> 00:00:54,000
might just observe that if we kind of

20
00:00:50,960 --> 00:00:55,920
correct for the overall frequency

21
00:00:54,000 --> 00:00:57,679
of the individual words then it's

22
00:00:55,920 --> 00:00:59,760
actually a and b that are united because

23
00:00:57,679 --> 00:01:01,840
they both have a bias in some sense for

24
00:01:01,840 --> 00:01:06,079
whereas by comparison c has a bias for

25
00:01:04,478 --> 00:01:07,920
the x dimension again thinking

26
00:01:07,920 --> 00:01:11,760
both of those are perspectives that we

27
00:01:09,519 --> 00:01:13,920
might want to capture and different

28
00:01:11,760 --> 00:01:16,478
notions of distance will key into one or

29
00:01:16,478 --> 00:01:20,560
one more preliminary uh i think it's

30
00:01:18,478 --> 00:01:22,400
very intuitive to depict these vector

31
00:01:20,560 --> 00:01:24,000
spaces and in only two dimensions that's

32
00:01:22,400 --> 00:01:26,719
pretty easy you can imagine that this is

33
00:01:24,000 --> 00:01:28,640
the dx dimension along the x-axis and

34
00:01:26,719 --> 00:01:30,719
this is the d-y dimension along the

35
00:01:28,640 --> 00:01:33,118
y-axis and then i have placed these

36
00:01:30,719 --> 00:01:35,039
individual points in that plane and then

37
00:01:33,118 --> 00:01:37,280
you can see graphically that b and c are

38
00:01:35,040 --> 00:01:39,360
pretty close together and a is kind of

39
00:01:37,280 --> 00:01:42,239
lonely down here in the corner the

40
00:01:42,799 --> 00:01:46,240
let's start with euclidean distance very

41
00:01:44,719 --> 00:01:48,239
common notion of distance in these

42
00:01:48,239 --> 00:01:52,239
we can measure the euclidean distance

43
00:01:50,239 --> 00:01:55,118
between vectors u and v if they share

44
00:01:52,239 --> 00:01:56,239
the same dimension n by just calculating

45
00:01:56,239 --> 00:02:00,718
of the squared element y differences

46
00:01:58,718 --> 00:02:02,798
absolute differences and then taking the

47
00:02:02,799 --> 00:02:06,240
that's the math here let's look at that

48
00:02:04,478 --> 00:02:07,679
in terms of this space so here we have

49
00:02:07,680 --> 00:02:11,760
depicted graphically a b and c and

50
00:02:10,159 --> 00:02:14,079
euclidean distance is measuring the

51
00:02:11,759 --> 00:02:15,679
length of these lines i've annotated

52
00:02:14,080 --> 00:02:17,680
with the full calculations but the

53
00:02:15,680 --> 00:02:19,040
intuition is just that we are measuring

54
00:02:17,680 --> 00:02:21,439
the length of these lines the most

55
00:02:19,039 --> 00:02:23,598
direct path between these points in our

56
00:02:23,598 --> 00:02:26,878
and you can see that euclidean distance

57
00:02:26,878 --> 00:02:31,199
perspective that we took on the vector

58
00:02:28,560 --> 00:02:36,840
space which unites the frequent items b

59
00:02:31,199 --> 00:02:36,839
and c as against the infrequent one a

60
00:02:37,120 --> 00:02:40,159
as a stepping stone toward cosine

61
00:02:38,878 --> 00:02:41,679
distance which will behave quite

62
00:02:40,159 --> 00:02:43,039
differently let's talk about length

63
00:02:43,039 --> 00:02:47,598
given a vector u of dimension n the l2

64
00:02:47,598 --> 00:02:52,719
is the sum of the squared values in that

65
00:02:50,318 --> 00:02:55,199
matrix and then we take the square root

66
00:02:52,719 --> 00:02:57,199
that's our normalization quantity there

67
00:02:55,199 --> 00:02:59,759
and then the actual normalization of

68
00:02:57,199 --> 00:03:02,399
that original vector u involves taking

69
00:02:59,759 --> 00:03:05,759
each one of its elements and dividing it

70
00:03:02,400 --> 00:03:07,439
by that fixed quantity the l2 ranks

71
00:03:05,759 --> 00:03:09,199
let's look look at what happens to our

72
00:03:07,439 --> 00:03:11,840
little illustrative example on the left

73
00:03:09,199 --> 00:03:13,359
here i have the original count matrix

74
00:03:11,840 --> 00:03:15,759
and in this column here i've given the

75
00:03:15,759 --> 00:03:19,518
and then when we take that quantity and

76
00:03:17,598 --> 00:03:21,839
divide each one of the values in that

77
00:03:21,840 --> 00:03:25,280
you can see that we've done something

78
00:03:23,120 --> 00:03:27,759
significant to the space so they're all

79
00:03:25,280 --> 00:03:30,080
kind of united on the same scale here

80
00:03:27,759 --> 00:03:32,000
and a and b are now close together

81
00:03:30,080 --> 00:03:34,239
whereas b and c are comparatively far

82
00:03:32,000 --> 00:03:36,158
apart so that is capturing the second

83
00:03:34,239 --> 00:03:38,319
perspective that we took on the matrix

84
00:03:36,158 --> 00:03:39,840
where a and b have something in common

85
00:03:39,840 --> 00:03:43,439
and that has come entirely from the

86
00:03:41,280 --> 00:03:45,759
normalization step and if we measured

87
00:03:43,439 --> 00:03:47,359
euclidean distance in this space just

88
00:03:45,759 --> 00:03:49,359
the length of the lines between these

89
00:03:47,360 --> 00:03:51,519
points we would again be capturing that

90
00:03:49,360 --> 00:03:54,480
a and b are alike and b and c are

91
00:03:54,560 --> 00:03:59,360
cosine kind of does that all in one step

92
00:03:57,199 --> 00:04:01,518
so the cosine distance or approximately

93
00:03:59,360 --> 00:04:03,280
the distance as you'll see between two

94
00:04:01,519 --> 00:04:04,640
vectors u and v of dimension share

95
00:04:04,639 --> 00:04:08,639
this calculation has two parts this is

96
00:04:06,719 --> 00:04:10,959
the similarity calculation cosine

97
00:04:08,639 --> 00:04:13,598
similarity and it is the dot product of

98
00:04:10,959 --> 00:04:15,360
the two vectors divided by the product

99
00:04:15,360 --> 00:04:18,720
and then to get something like the

100
00:04:16,639 --> 00:04:20,959
distance we just take one and subtract

101
00:04:20,959 --> 00:04:24,959
again let's ground this in our example

102
00:04:22,800 --> 00:04:26,800
here we have the original count vector

103
00:04:26,800 --> 00:04:32,240
and what we do with cosine distance is

104
00:04:29,120 --> 00:04:34,240
essentially measure the angles between

105
00:04:32,240 --> 00:04:35,680
these lines that i've drawn from this

106
00:04:35,680 --> 00:04:40,400
and so you can see that cosine distance

107
00:04:38,079 --> 00:04:42,719
is capturing the fact that a and b are

108
00:04:40,399 --> 00:04:44,959
close together as measured by this angle

109
00:04:42,720 --> 00:04:45,919
whereas b and c are comparatively far

110
00:04:45,918 --> 00:04:50,399
so again with cosine we're abstracting

111
00:04:48,240 --> 00:04:52,639
away from frequency information

112
00:04:50,399 --> 00:04:54,959
and keying into that abstract notion of

113
00:04:52,639 --> 00:04:57,840
similarity that connects a and b as

114
00:04:58,639 --> 00:05:03,038
another perspective that you could take

115
00:05:00,240 --> 00:05:04,800
is just observe that if we first

116
00:05:06,399 --> 00:05:11,839
via via the l2 norm and then apply the

117
00:05:09,120 --> 00:05:13,519
cosine calculation we change the space

118
00:05:11,839 --> 00:05:15,758
as i showed you before so they're all up

119
00:05:13,519 --> 00:05:18,079
here kind of on the unit sphere

120
00:05:15,759 --> 00:05:20,000
and notice that the actual values that

121
00:05:20,000 --> 00:05:24,319
whether or not we did that l2 norming

122
00:05:22,000 --> 00:05:26,720
step and that is because cosine is

123
00:05:24,319 --> 00:05:29,120
building the effects of l2 norming

124
00:05:26,720 --> 00:05:32,000
directly into this normalization here in

125
00:05:33,519 --> 00:05:36,639
there are a few other methods that we

126
00:05:35,038 --> 00:05:38,159
could think about or classes of methods

127
00:05:36,639 --> 00:05:39,600
i think we don't need to get distracted

128
00:05:38,160 --> 00:05:40,960
by the details but i thought i would

129
00:05:39,600 --> 00:05:42,879
mention them in case they come up and

130
00:05:40,959 --> 00:05:44,319
you're reading our research the first

131
00:05:42,879 --> 00:05:45,918
class are what are what i've called

132
00:05:44,319 --> 00:05:48,560
matching based methods they're all kind

133
00:05:45,918 --> 00:05:50,560
of based in this matching coefficient

134
00:05:48,560 --> 00:05:51,680
and then jacquard dice and overlap are

135
00:05:50,560 --> 00:05:52,399
terms that you might see in the

136
00:05:52,399 --> 00:05:55,839
these are often defined only for binary

137
00:05:54,399 --> 00:05:58,079
vectors but here i've given their

138
00:05:55,839 --> 00:06:00,318
generalizations to the real valued

139
00:05:58,079 --> 00:06:01,680
vectors that we're talking about

140
00:06:00,319 --> 00:06:03,360
and the other class of methods that you

141
00:06:01,680 --> 00:06:05,120
might see come up are probabilistic

142
00:06:03,360 --> 00:06:07,439
methods which tend to be grounded in

143
00:06:07,439 --> 00:06:11,519
kl divergence is essentially a way of

144
00:06:09,519 --> 00:06:13,918
measuring the distance between two

145
00:06:15,038 --> 00:06:20,079
to be more precise from a reference

146
00:06:17,279 --> 00:06:21,918
distribution p to some other probability

147
00:06:21,918 --> 00:06:27,038
um and it has symmetric notions

148
00:06:24,399 --> 00:06:28,799
symmetric kl and jensen shannon distance

149
00:06:27,038 --> 00:06:31,279
which is another symmetric notion that's

150
00:06:28,800 --> 00:06:33,600
based in kl divergence again these are

151
00:06:31,279 --> 00:06:35,279
probably appropriate measures to choose

152
00:06:33,600 --> 00:06:37,840
if the quantities that you're thinking

153
00:06:35,279 --> 00:06:41,038
of are appropriately thought of as

154
00:06:41,759 --> 00:06:45,199
now i've alluded to the fact that the

155
00:06:43,360 --> 00:06:47,360
cosine distance measure that i gave you

156
00:06:45,199 --> 00:06:49,598
before is not quite what's called a

157
00:06:47,360 --> 00:06:50,879
proper distance metric let me expand on

158
00:06:50,879 --> 00:06:55,360
to qualify as a proper distance metric a

159
00:06:53,680 --> 00:06:57,360
vector comparison method has to have

160
00:06:55,360 --> 00:06:59,439
three properties it needs to be

161
00:06:57,360 --> 00:07:02,400
symmetric that is it needs to give the

162
00:06:59,439 --> 00:07:05,598
same value for x y as it does to yx

163
00:07:02,399 --> 00:07:06,478
kl divergence actually fails that first

164
00:07:06,478 --> 00:07:09,758
it needs to assign zero to identical

165
00:07:09,759 --> 00:07:13,840
and crucially it needs to satisfy what's

166
00:07:11,839 --> 00:07:17,198
called the triangle inequality which

167
00:07:13,839 --> 00:07:19,598
says that the distance between x and z

168
00:07:17,199 --> 00:07:23,598
is less than or equal to the distance

169
00:07:19,598 --> 00:07:25,439
between x and y and then y to z

170
00:07:23,598 --> 00:07:28,079
cosine distance as i showed it to you

171
00:07:25,439 --> 00:07:29,839
before fails to satisfy the triangle

172
00:07:28,079 --> 00:07:32,000
inequality and this is just a simple

173
00:07:29,839 --> 00:07:34,159
example that makes an intuitive it just

174
00:07:32,000 --> 00:07:36,399
happens that this distance here is

175
00:07:34,160 --> 00:07:37,199
actually greater than these two values

176
00:07:37,199 --> 00:07:40,960
which is a failure of the statement of

177
00:07:40,959 --> 00:07:44,878
now this is relatively easily corrected

178
00:07:43,360 --> 00:07:47,038
but this is also kind of a useful

179
00:07:44,879 --> 00:07:48,400
framework of all the different choices

180
00:07:48,399 --> 00:07:52,560
of all the options for vector comparison

181
00:07:50,800 --> 00:07:55,360
suppose we decided to favor the ones

182
00:07:52,560 --> 00:07:56,959
that counted as true distance metrics

183
00:07:55,360 --> 00:07:59,280
then that would at least push us to

184
00:07:59,279 --> 00:08:03,519
jacquard for binary vectors only and

185
00:08:01,918 --> 00:08:05,839
jensen shannon distance if we were

186
00:08:03,519 --> 00:08:07,758
talking about probabilistic spaces and

187
00:08:05,839 --> 00:08:09,918
we would further amend the definition of

188
00:08:07,759 --> 00:08:11,360
cosine distance to the more careful one

189
00:08:11,360 --> 00:08:15,120
which which satisfies the triangle

190
00:08:13,519 --> 00:08:16,719
inequality as well as the other two

191
00:08:16,720 --> 00:08:20,319
and by this kind of way of dividing the

192
00:08:20,319 --> 00:08:25,840
matching jaccard dice overlap

193
00:08:23,519 --> 00:08:27,839
um kale divergence and symmetric tail

194
00:08:25,839 --> 00:08:29,839
divergence as ones that failed to be

195
00:08:27,839 --> 00:08:31,598
proper distance metrics and so that

196
00:08:29,839 --> 00:08:33,838
might be a useful framework for thinking

197
00:08:33,839 --> 00:08:38,159
one other point in relation to this this

198
00:08:36,240 --> 00:08:40,320
is obviously a more involved calculation

199
00:08:38,158 --> 00:08:42,158
than the one that i gave you before and

200
00:08:40,320 --> 00:08:44,720
in truth it is probably not worth the

201
00:08:42,158 --> 00:08:46,559
effort here's an example of just a bunch

202
00:08:44,720 --> 00:08:48,399
of vectors that i sampled from one of

203
00:08:46,559 --> 00:08:50,399
our vector space models and i've

204
00:08:48,399 --> 00:08:52,879
compared the improper cosine distance

205
00:08:50,399 --> 00:08:55,278
that i showed you before on the x-axis

206
00:08:52,879 --> 00:08:56,799
with the proper cosine distance measure

207
00:08:56,799 --> 00:09:00,719
and the correlation between the two is

208
00:08:58,879 --> 00:09:02,879
almost perfect so there is essentially

209
00:09:00,720 --> 00:09:05,200
no difference between these two

210
00:09:02,879 --> 00:09:06,720
different ways of measuring cosine

211
00:09:05,200 --> 00:09:08,720
and i think that they are probably

212
00:09:06,720 --> 00:09:10,560
essentially identical up to ranking

213
00:09:08,720 --> 00:09:12,639
which is often the quantity that we care

214
00:09:10,559 --> 00:09:14,239
about when we're doing these comparisons

215
00:09:12,639 --> 00:09:16,159
so probably stick with a simpler and

216
00:09:14,240 --> 00:09:18,560
less involved calculation would be my

217
00:09:18,799 --> 00:09:22,879
let's close with some generalizations

218
00:09:20,399 --> 00:09:24,958
and relationships first euclidean as

219
00:09:22,879 --> 00:09:27,278
well as jaccard and dice with raw count

220
00:09:24,958 --> 00:09:29,039
vectors will tend to favor raw frequency

221
00:09:27,278 --> 00:09:31,679
over other distributional patterns like

222
00:09:29,039 --> 00:09:34,719
that more abstract one that i showed you

223
00:09:34,720 --> 00:09:38,959
euclidean with l2 norm vectors is

224
00:09:36,958 --> 00:09:41,278
equivalent to cosine when it comes to

225
00:09:38,958 --> 00:09:43,278
ranking which is just to say that if you

226
00:09:41,278 --> 00:09:45,200
want to use euclidean and you first l2

227
00:09:43,278 --> 00:09:46,639
norm your vectors you're probably just

228
00:09:45,200 --> 00:09:49,759
doing something that might as well just

229
00:09:49,759 --> 00:09:53,519
and dice are equivalent with regard to

230
00:09:51,278 --> 00:09:54,958
ranking that's something to keep in mind

231
00:09:53,519 --> 00:09:56,080
uh and then this is maybe a more

232
00:09:54,958 --> 00:09:58,639
fundamental point that you'll see

233
00:09:56,080 --> 00:10:00,560
recurring throughout this unit both l2

234
00:09:58,639 --> 00:10:02,000
norming and also a related calculation

235
00:10:00,559 --> 00:10:04,319
which would just create probability

236
00:10:02,000 --> 00:10:06,879
distributions out of the rows they can

237
00:10:04,320 --> 00:10:09,200
be useful steps as we've seen but they

238
00:10:06,879 --> 00:10:10,879
can obscure differences in the amount or

239
00:10:09,200 --> 00:10:12,720
strength of evidence that you have which

240
00:10:10,879 --> 00:10:14,958
can in turn have an effect on the

241
00:10:12,720 --> 00:10:17,839
reliability of for example cosine

242
00:10:14,958 --> 00:10:19,278
normuclidian or kale divergence right

243
00:10:17,839 --> 00:10:20,800
these shortcomings might be addressed

244
00:10:19,278 --> 00:10:22,958
through waiting schemes though but

245
00:10:20,799 --> 00:10:25,278
here's the bottom line there is valuable

246
00:10:25,278 --> 00:10:28,879
if we abstract away from it some other

247
00:10:27,278 --> 00:10:30,799
information might come to the surface

248
00:10:28,879 --> 00:10:32,879
but we also might lose that important

249
00:10:30,799 --> 00:10:34,479
frequency information in distorting the

250
00:10:32,879 --> 00:10:36,399
space in that way and it can be

251
00:10:34,480 --> 00:10:38,879
difficult to balance these competing

252
00:10:39,120 --> 00:10:43,440
finally i just close with some code

253
00:10:40,958 --> 00:10:45,919
snippets our course repository has lots

254
00:10:43,440 --> 00:10:48,000
of handy utilities for doing these

255
00:10:45,919 --> 00:10:50,399
distance calculations and also length

256
00:10:48,000 --> 00:10:52,879
norming your vectors and so forth and it

257
00:10:50,399 --> 00:10:55,519
also has this function called neighbors

258
00:10:52,879 --> 00:10:58,320
in the vsm module it allows you to pick

259
00:10:55,519 --> 00:11:00,480
a target word and supply a vector space

260
00:10:58,320 --> 00:11:02,879
model and then it will give you

261
00:11:00,480 --> 00:11:04,480
a full ranking of the entire vocabulary

262
00:11:02,879 --> 00:11:06,639
in that vector space with respect to

263
00:11:04,480 --> 00:11:08,959
your target word starting with the ones

264
00:11:06,639 --> 00:11:10,559
that are closest so here are the results

265
00:11:10,559 --> 00:11:15,599
cosine distance in cell 12 and jakarta

266
00:11:12,799 --> 00:11:16,958
distance and swell in cell 13 and i

267
00:11:15,600 --> 00:11:18,320
would just like to say that these

268
00:11:16,958 --> 00:11:20,719
neighbors don't look especially

269
00:11:18,320 --> 00:11:22,800
intuitive to me it does not look like

270
00:11:22,799 --> 00:11:27,199
really interesting semantic information

271
00:11:25,360 --> 00:11:29,440
but don't worry we're going to correct

272
00:11:27,200 --> 00:11:31,759
this we're going to start to massage and

273
00:11:29,440 --> 00:11:33,600
stretch and bend our vector space models

274
00:11:31,759 --> 00:11:35,360
and we'll see we will see much better

275
00:11:33,600 --> 00:11:36,800
results for these neighbor functions and

