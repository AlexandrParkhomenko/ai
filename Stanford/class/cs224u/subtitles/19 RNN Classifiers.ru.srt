1
00:00:04,640 --> 00:00:06,240
приветствую всех, это восьмая часть

2
00:00:06,240 --> 00:00:07,919
нашей серии по анализу настроений под наблюдением,

3
00:00:07,919 --> 00:00:09,679
последний скринкаст в

4
00:00:09,679 --> 00:00:11,360
серии, в которой мы будем говорить о

5
00:00:11,360 --> 00:00:13,280
рекуррентных нейронных сетях или классификаторах rnn,

6
00:00:13,280 --> 00:00:15,360
я полагаю, что это официально

7
00:00:15,360 --> 00:00:16,960
наш первый шаг в мир глубокого

8
00:00:16,960 --> 00:00:19,520
обучения для настроений.  анализ

9
00:00:19,520 --> 00:00:21,039
Этот слайд дает обзор

10
00:00:21,039 --> 00:00:22,960
модели, и давайте рассмотрим ее более

11
00:00:22,960 --> 00:00:25,119
подробно, поэтому у нас есть один пример

12
00:00:25,119 --> 00:00:27,279
с тремя токенами, правила рока, эти

13
00:00:27,279 --> 00:00:29,199
модели подготовлены для последовательностей переменной длины

14
00:00:29,199 --> 00:00:31,119
, но этот пример

15
00:00:31,119 --> 00:00:33,600
должен иметь длину три

16
00:00:33,600 --> 00:00:35,040
и первый шаг, чтобы запустить эту модель,

17
00:00:35,040 --> 00:00:36,719
является знакомым: мы собираемся

18
00:00:36,719 --> 00:00:38,480
искать каждый из этих токенов в том,

19
00:00:38,480 --> 00:00:41,200
что предположительно является фиксированным пространством для встраивания,

20
00:00:41,200 --> 00:00:42,719
поэтому для каждого токена мы получим

21
00:00:42,719 --> 00:00:45,039
векторное представление,

22
00:00:45,039 --> 00:00:46,559
следующий шаг заключается в том, что  у нас есть некоторые

23
00:00:46,559 --> 00:00:49,680
изученные параметры весовая матрица w xh,

24
00:00:49,680 --> 00:00:51,120
а нижний индекс указывает, что мы

25
00:00:51,120 --> 00:00:53,520
переходим от входных данных x к скрытому

26
00:00:53,520 --> 00:00:56,000
слою h, так что это первое преобразование,

27
00:00:56,000 --> 00:00:57,840
а затем  Матрица весов используется на

28
00:00:57,840 --> 00:01:00,239
каждом из этих временных шагов.

29
00:01:00,239 --> 00:01:01,920
Существует вторая матрица весов обучения,

30
00:01:01,920 --> 00:01:04,239
которую я назвал whhh, чтобы указать, что

31
00:01:04,239 --> 00:01:06,080
мы сейчас путешествуем через скрытый

32
00:01:06,080 --> 00:01:07,280
слой,

33
00:01:07,280 --> 00:01:09,439
поэтому мы начинаем с некоторого начального состояния h0,

34
00:01:09,439 --> 00:01:11,360
которое может быть полностью нулевым.  или случайно

35
00:01:11,360 --> 00:01:13,200
инициализированный вектор, или вектор, поступающий

36
00:01:13,200 --> 00:01:15,920
из какого-либо другого компонента в модели,

37
00:01:15,920 --> 00:01:17,840
и это представление комбинируется

38
00:01:17,840 --> 00:01:19,600
с представлением, которое мы получаем, идя

39
00:01:19,600 --> 00:01:21,600
вертикально вверх из вложения,

40
00:01:21,600 --> 00:01:24,000
обычно каким-то аддитивным образом, чтобы создать это

41
00:01:24,000 --> 00:01:26,560
скрытое состояние здесь h1 и те

42
00:01:26,560 --> 00:01:29,200
параметры wh  используется снова на каждом

43
00:01:29,200 --> 00:01:31,759
из этих временных шагов, так что у нас есть две

44
00:01:31,759 --> 00:01:33,759
изученные матрицы весов как часть

45
00:01:33,759 --> 00:01:35,840
основной структуры этой модели: та,

46
00:01:35,840 --> 00:01:37,360
которая ведет нас от встраивания в

47
00:01:37,360 --> 00:01:39,439
скрытый слой, и та, которая перемещает нас

48
00:01:39,439 --> 00:01:41,680
через скрытый слой, и снова те

49
00:01:41,680 --> 00:01:43,360
обычно комбинируются некоторым аддитивным

50
00:01:43,360 --> 00:01:45,360
образом для создания этих внутренних скрытых

51
00:01:45,360 --> 00:01:46,960
представлений,

52
00:01:46,960 --> 00:01:48,479
теперь мы можем делать с этими внутренними скрытыми представлениями все, что захотим.

53
00:01:48,479 --> 00:01:50,799


54
00:01:50,799 --> 00:01:53,040
Иногда, когда мы используем rnns в качестве классификаторов, мы делаем

55
00:01:53,040 --> 00:01:55,119
то, что, возможно, является самой простой

56
00:01:55,119 --> 00:01:58,399
вещью: берем окончательное представление

57
00:01:58,399 --> 00:02:00,640
и используем его в качестве входных данных для стандартного

58
00:02:00,640 --> 00:02:02,960
классификатора softmax, поэтому с точки

59
00:02:02,960 --> 00:02:06,079
зрения h3, идущего к y здесь, у нас просто

60
00:02:06,079 --> 00:02:08,080
есть вес обучения  матрица для классификатора

61
00:02:08,080 --> 00:02:10,318
может быть также термином смещения, но с этого

62
00:02:10,318 --> 00:02:11,680
момента это действительно просто

63
00:02:11,680 --> 00:02:13,120
классификатор того типа, который мы

64
00:02:13,120 --> 00:02:16,080
изучали до этого момента в модуле,

65
00:02:16,080 --> 00:02:17,840
конечно, мы могли бы разработать эту

66
00:02:17,840 --> 00:02:19,360
модель всеми возможными способами, которыми она могла бы работать

67
00:02:19,360 --> 00:02:21,680
двунаправленно мы могли бы более полно

68
00:02:21,680 --> 00:02:23,040
использовать различные скрытые

69
00:02:23,040 --> 00:02:25,120
представления здесь, но в простейшем

70
00:02:25,120 --> 00:02:27,360
режиме наши классификаторы rnn будут просто

71
00:02:27,360 --> 00:02:29,599
получать скрытые представления на каждом

72
00:02:29,599 --> 00:02:31,440
временном шаге и использовать последнее в качестве

73
00:02:31,440 --> 00:02:33,680
входных данных для классификатора. Пара вещей, которые я

74
00:02:33,680 --> 00:02:35,040
хотел бы сказать об этом сначала  если

75
00:02:35,040 --> 00:02:37,360
вам нужен дополнительный уровень детализации того, как

76
00:02:37,360 --> 00:02:38,560
эти модели структурированы и

77
00:02:38,560 --> 00:02:40,239
оптимизированы, я бы посоветовал вам взглянуть

78
00:02:40,239 --> 00:02:42,720
на эту чистую эталонную

79
00:02:42,720 --> 00:02:45,200
реализацию классификатора rnn t

80
00:02:45,200 --> 00:02:46,640
это включено в наш дистрибутив кода курса,

81
00:02:46,640 --> 00:02:48,160
я думаю, что это отличный

82
00:02:48,160 --> 00:02:51,200
способ почувствовать рекурсивный процесс

83
00:02:51,200 --> 00:02:51,920


84
00:02:51,920 --> 00:02:53,519
вычисления через полные

85
00:02:53,519 --> 00:02:55,280
последовательности, а затем

86
00:02:55,280 --> 00:02:57,280
обратное распространение сигналов об ошибках для обновления

87
00:02:57,280 --> 00:02:58,959
матрицы весов,

88
00:02:58,959 --> 00:03:01,280
но сейчас я думаю, что просто понимаю

89
00:03:01,280 --> 00:03:03,840
что основной структуры этой модели

90
00:03:03,840 --> 00:03:06,400
достаточно,

91
00:03:06,400 --> 00:03:07,760
я просто хочу напомнить вам из

92
00:03:07,760 --> 00:03:09,440
предыдущего скринкаста, что мы очень

93
00:03:09,440 --> 00:03:11,440
близки к идее распределенных

94
00:03:11,440 --> 00:03:13,040
представлений функций, которую я

95
00:03:13,040 --> 00:03:14,560
представил перед тем, как

96
00:03:14,560 --> 00:03:16,560
вспомнить, что для этого режима мы

97
00:03:16,560 --> 00:03:18,560
ищем каждый токен.  в пространстве вложения

98
00:03:18,560 --> 00:03:20,480
так же, как мы делаем для rnn,

99
00:03:20,480 --> 00:03:22,640
но вместо того, чтобы изучать какую-то сложную

100
00:03:22,640 --> 00:03:24,400
комбинированную функцию с кучей

101
00:03:24,400 --> 00:03:26,799
изученных параметров, мы просто объединяем

102
00:03:26,799 --> 00:03:28,799
их через сумму или среднее, и это

103
00:03:28,799 --> 00:03:31,040
основа, которая является входом для классификатора

104
00:03:31,040 --> 00:03:33,360
здесь, rnn можно считать

105
00:03:33,360 --> 00:03:35,120
разработка этого, потому что вместо того, чтобы

106
00:03:35,120 --> 00:03:36,720
предполагать, что эти векторы здесь будут

107
00:03:36,720 --> 00:03:38,640
объединены каким-то простым способом, таким как летнее

108
00:03:38,640 --> 00:03:42,080
среднее, у нас теперь есть реальные  y огромная способность

109
00:03:42,080 --> 00:03:43,920
выучить гораздо более сложный способ

110
00:03:43,920 --> 00:03:45,840
их комбинирования, который является оптимальным по

111
00:03:45,840 --> 00:03:47,599
отношению к классификатору, который мы

112
00:03:47,599 --> 00:03:48,959
пытаемся подогнать,

113
00:03:48,959 --> 00:03:50,879
но в основе своей это очень похожие

114
00:03:50,879 --> 00:03:52,799
идеи, и если случалось, что

115
00:03:52,799 --> 00:03:54,480
какое-то или среднее значение на этой картинке было

116
00:03:54,480 --> 00:03:56,319
именно  правильная функция для изучения

117
00:03:56,319 --> 00:03:58,239
ваших данных, тогда у rnn, безусловно,

118
00:03:58,239 --> 00:04:00,319
будет возможность сделать это, мы просто

119
00:04:00,319 --> 00:04:02,000
склонны отдавать предпочтение rnn, потому что он,

120
00:04:02,000 --> 00:04:04,080
конечно, может изучить гораздо более широкий спектр

121
00:04:04,080 --> 00:04:06,239
сложных пользовательских функций, которые относятся

122
00:04:06,239 --> 00:04:07,840
к проблеме, которую вы

123
00:04:07,840 --> 00:04:10,239
поставили

124
00:04:10,799 --> 00:04:12,799
до сих пор мы работали в

125
00:04:12,799 --> 00:04:15,360
режиме, который я назвал стандартной подготовкой набора данных rnn, давайте остановимся

126
00:04:15,360 --> 00:04:17,040


127
00:04:17,040 --> 00:04:18,560
на этом немного подробнее, предположим, что

128
00:04:18,560 --> 00:04:20,720
у нас есть два примера, содержащие

129
00:04:20,720 --> 00:04:24,400
токены aba и bc, это два наших необработанных

130
00:04:24,400 --> 00:04:25,600
входа.

131
00:04:25,600 --> 00:04:27,360
Первым шагом в стандартном режиме

132
00:04:27,360 --> 00:04:29,840
является просмотр каждого из них в каком-

133
00:04:29,840 --> 00:04:31,759
то списке индексов,

134
00:04:31,759 --> 00:04:34,000
а затем эти индексы вводятся в

135
00:04:34,000 --> 00:04:36,400
пространство вложения, и они, наконец, дают

136
00:04:36,400 --> 00:04:38,320
нам вектор, представляющий  Ионы каждого

137
00:04:38,320 --> 00:04:41,199
примера, так что на самом деле

138
00:04:41,199 --> 00:04:43,919
входные данные rnn представляют собой список

139
00:04:43,919 --> 00:04:45,360
векторов, просто мы обычно

140
00:04:45,360 --> 00:04:47,280
получали эти векторы, просматривая

141
00:04:47,280 --> 00:04:49,759
их в фиксированном пространстве вложений, и поэтому,

142
00:04:49,759 --> 00:04:51,840
например, поскольку a встречается дважды в этом

143
00:04:51,840 --> 00:04:54,320
первом примере, это  буквально повторяется

144
00:04:54,320 --> 00:04:57,360
как первый и третий векторы здесь,

145
00:04:57,360 --> 00:04:59,360
теперь, я думаю, вы можете видеть скрытую на этом

146
00:04:59,360 --> 00:05:01,120
рисунке возможность того, что мы могли бы

147
00:05:01,120 --> 00:05:03,360
отказаться от пространства вложения и вместо этого

148
00:05:03,360 --> 00:05:05,919
просто напрямую ввести списки векторов, и

149
00:05:05,919 --> 00:05:07,759
это один из способов, который мы рассмотрим

150
00:05:07,759 --> 00:05:09,680
позже в  четверть использования

151
00:05:09,680 --> 00:05:11,600
контекстных моделей, таких как bert, мы

152
00:05:11,600 --> 00:05:14,080
просто искали бы целые потоки токенов,

153
00:05:14,080 --> 00:05:16,560
получали списки векторов и использовали их в

154
00:05:16,560 --> 00:05:19,440
качестве фиксированных входных данных для модели, такой как rnn,

155
00:05:19,440 --> 00:05:21,199
и это первый шаг к

156
00:05:21,199 --> 00:05:24,000
точной настройке моделей, таких как bert, для решения подобных задач.

157
00:05:24,000 --> 00:05:26,479
мы представили в этом разделе,

158
00:05:26,479 --> 00:05:28,960
так что имейте в виду эту идею, когда мы поговорим

159
00:05:28,960 --> 00:05:32,880
о стратегиях тонкой настройки,

160
00:05:32,880 --> 00:05:34,800
теперь еще одно практическое замечание, которое я

161
00:05:34,800 --> 00:05:37,280
показал вам до сих пор, это то, что вы бы назвали

162
00:05:37,280 --> 00:05:39,600
простой ванильным rnn

163
00:05:39,600 --> 00:05:42,720
l  Сети долговременной краткосрочной памяти stms

164
00:05:42,720 --> 00:05:44,400
являются гораздо более мощными моделями и

165
00:05:44,400 --> 00:05:45,840
по умолчанию будут использовать их, когда мы будем проводить

166
00:05:45,840 --> 00:05:48,080
эксперименты. Основная проблема заключается в

167
00:05:48,080 --> 00:05:50,400
том, что плоские rnns имеют тенденцию плохо работать

168
00:05:50,400 --> 00:05:52,800
с очень длинными последовательностями, вы получаете этот

169
00:05:52,800 --> 00:05:54,800
сигнал ошибки от классификатора там

170
00:05:54,800 --> 00:05:57,039
в конце.  токен, и теперь

171
00:05:57,039 --> 00:05:58,960
информация должна пройти весь обратный путь

172
00:05:58,960 --> 00:06:00,560
через сеть, это может быть очень

173
00:06:00,560 --> 00:06:03,120
длинная последовательность, и в результате

174
00:06:03,120 --> 00:06:04,880
информация, поступающая из этого

175
00:06:04,880 --> 00:06:08,080
сигнала ошибки, часто теряется или искажается.

176
00:06:08,080 --> 00:06:10,319
Теперь ячейки lstm являются важным ответом

177
00:06:10,319 --> 00:06:12,240
на эту проблему.  представить

178
00:06:12,240 --> 00:06:14,240
механизмы, которые контролируют поток

179
00:06:14,240 --> 00:06:16,319
информации и помогут вам избежать

180
00:06:16,319 --> 00:06:18,319
проблем с оптимизацией, которые возникают для

181
00:06:18,319 --> 00:06:20,880
обычных rnns, теперь я не собираюсь

182
00:06:20,880 --> 00:06:22,800
тратить время на подробный обзор этого механизма,

183
00:06:22,800 --> 00:06:25,039
я бы вместо этого рекомендовал

184
00:06:25,039 --> 00:06:27,039
эти два отличных поста в блоге, у них есть

185
00:06:27,039 --> 00:06:29,280
отличные  диаграммы и действительно подробные

186
00:06:29,280 --> 00:06:31,680
обсуждения, они могут гораздо лучше,

187
00:06:31,680 --> 00:06:33,520
чем я, передать

188
00:06:33,520 --> 00:06:36,400
интуицию визуально, а также с помощью математики

189
00:06:36,400 --> 00:06:38,479
и  я думаю, вы могли бы выбрать один или оба

190
00:06:38,479 --> 00:06:40,560
и очень быстро получить глубокое

191
00:06:40,560 --> 00:06:42,800
понимание того, как именно функционируют ячейки lstm.

192
00:06:42,800 --> 00:06:45,680


193
00:06:45,680 --> 00:06:47,039
Последнее, что здесь есть, это просто фрагмент кода,

194
00:06:47,039 --> 00:06:49,039
чтобы показать вам, как легко

195
00:06:49,039 --> 00:06:51,440
использовать наш репозиторий кода курса для соответствия

196
00:06:51,440 --> 00:06:53,440
моделям, подобным этой.  в контексте

197
00:06:53,440 --> 00:06:55,199
анализа настроений вы можете снова

198
00:06:55,199 --> 00:06:57,840
использовать эту библиотеку sst, и то, что я

199
00:06:57,840 --> 00:06:59,599
сделал здесь, представляет собой своего рода сложную

200
00:06:59,599 --> 00:07:01,360
версию, показывающую вам кучу различных

201
00:07:01,360 --> 00:07:03,520
функций, поэтому

202
00:07:03,520 --> 00:07:05,039
в ячейке 2 вы можете видеть, что у меня

203
00:07:05,039 --> 00:07:07,039
будет  указатель на GloVe, и я собираюсь

204
00:07:07,039 --> 00:07:09,280
создать поиск GloVe,

205
00:07:09,280 --> 00:07:11,280
используя 50-мерные векторы, просто чтобы

206
00:07:11,280 --> 00:07:12,880
не усложнять

207
00:07:12,880 --> 00:07:15,199
задачу функция признаков для этой модели

208
00:07:15,199 --> 00:07:17,599
не является той, которая возвращает словари подсчета,

209
00:07:17,599 --> 00:07:19,360
это важно для структуры

210
00:07:19,360 --> 00:07:21,360
модели, которую мы собираемся  использовать то, что вы вводите

211
00:07:21,360 --> 00:07:23,520
необработанные последовательности токенов, поэтому все, что мы

212
00:07:23,520 --> 00:07:25,680
здесь делаем, это обводить последовательность,

213
00:07:25,680 --> 00:07:27,360
а затем разбивать на пробелы,

214
00:07:27,360 --> 00:07:28,639
конечно, вы могли бы сделать что-то более

215
00:07:28,639 --> 00:07:30,319
сложное,

216
00:07:30,319 --> 00:07:31,440
хотя идея заключается в том, что вы хотите

217
00:07:31,440 --> 00:07:34,000
выровнять остроумие  h словарь GloVe, наша

218
00:07:34,000 --> 00:07:36,160
оболочка модели делает несколько вещей:

219
00:07:36,160 --> 00:07:38,319
создает словарь, загружает его и

220
00:07:38,319 --> 00:07:40,800
встраивает, используя это пространство GloVe, которое

221
00:07:40,800 --> 00:07:43,440
будет начальным вложением для нашей модели,

222
00:07:43,440 --> 00:07:44,800
и если вы пропустите этот шаг, у вас

223
00:07:44,800 --> 00:07:46,560
будет случайно инициализированное вложение

224
00:07:46,560 --> 00:07:48,720
пространство, что тоже может быть хорошо, но, по-

225
00:07:48,720 --> 00:07:51,520
видимому, GloVe даст нам шаг вперед,

226
00:07:51,520 --> 00:07:53,440
а затем мы настроим классификатор torch rnn,

227
00:07:53,440 --> 00:07:55,039
и то, что я сделал здесь, это

228
00:07:55,039 --> 00:07:56,800
выставило множество различных аргументов ключевого слова

229
00:07:56,800 --> 00:07:58,879
не все из них есть

230
00:07:58,879 --> 00:08:00,560
много  ручки, с которыми вы можете возиться, как это

231
00:08:00,560 --> 00:08:02,720
типично для моделей глубокого обучения,

232
00:08:02,720 --> 00:08:04,080
возможно, я бы назвал то, что

233
00:08:04,080 --> 00:08:05,680
мы используем то фиксированное вложение, которое

234
00:08:05,680 --> 00:08:07,919
мы получили от GloVe, и я установил раннее

235
00:08:07,919 --> 00:08:09,919
прекращение равным true, что может помочь

236
00:08:09,919 --> 00:08:12,319
вам эффективно оптимизировать эти модели в

237
00:08:12,319 --> 00:08:13,759
противном случае  вам нужно будет выяснить,

238
00:08:13,759 --> 00:08:15,440
сколько итераций вы на самом деле хотите, чтобы он

239
00:08:15,440 --> 00:08:17,759
выполнялся, и вы можете запускать его

240
00:08:17,759 --> 00:08:19,840
слишком долго или намного меньше времени, чем

241
00:08:19,840 --> 00:08:22,080
необходимо для получения оптимальной модели.

242
00:08:22,080 --> 00:08:23,919
Есть несколько других

243
00:08:23,919 --> 00:08:26,400
параметров, которые могут помочь

244
00:08:26,400 --> 00:08:28,160
вам эффективно оптимизировать эти модели

245
00:08:28,160 --> 00:08:30,080


246
00:08:30,080 --> 00:08:31,520
в конце, хотя, настроив все,

247
00:08:31,520 --> 00:08:33,919
что вы называете подходящими, как обычно, и верните

248
00:08:33,919 --> 00:08:36,080
модель поезда, и в этом контексте вы

249
00:08:36,080 --> 00:08:38,320
можете просто использовать sst эксперимент с этими

250
00:08:38,320 --> 00:08:40,240
предыдущих компонентов для проведения

251
00:08:40,240 --> 00:08:42,640
экспериментов с rnns так же, как вы это делали

252
00:08:42,640 --> 00:08:44,800
для более простых линейных моделей, как и в предыдущих

253
00:08:44,800 --> 00:08:47,519
скринкастах, одно изменение, которое

254
00:08:47,519 --> 00:08:49,120
будет знакомо по предыдущему

255
00:08:49,120 --> 00:08:50,720
скринкасту, заключается в том, что вам нужно установить

256
00:08:50,720 --> 00:08:53,120
vectorize равным false, и это

257
00:08:53,120 --> 00:08:55,279
важно, потому что мы снова  собираемся

258
00:08:55,279 --> 00:08:57,360
позволить модели обрабатывать эти примеры, мы

259
00:08:57,360 --> 00:08:58,800
не хотим передавать все через

260
00:08:58,800 --> 00:09:00,959
какой-то векторизатор члена, который предназначен

261
00:09:00,959 --> 00:09:02,399
исключительно для функций, созданных вручную,

262
00:09:02,399 --> 00:09:04,720
и разреженных линейных моделей здесь,

263
00:09:04,720 --> 00:09:06,560
в стране глубокого обучения, векторизация

264
00:09:06,560 --> 00:09:08,480
равна false, и мы будем использовать

265
00:09:08,480 --> 00:09:10,320
компоненты  модели для представления

266
00:09:10,320 --> 00:09:15,240
каждого примера, как я обсуждал ранее

