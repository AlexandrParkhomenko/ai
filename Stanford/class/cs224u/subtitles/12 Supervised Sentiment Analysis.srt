1
00:00:05,800 --> 00:00:06,800
Hello, everyone.

2
00:00:06,800 --> 00:00:09,190
This video kicks off our
series of screencasts

3
00:00:09,190 --> 00:00:11,060
on supervised
sentiment analysis.

4
00:00:11,060 --> 00:00:13,770
I just want to provide you
with an overview of the problem

5
00:00:13,770 --> 00:00:15,770
and of the kind of work
we'll be doing, and also

6
00:00:15,770 --> 00:00:19,670
a rationale for why
we'll be doing it.

7
00:00:19,670 --> 00:00:22,510
So here's an overview
of the entire unit.

8
00:00:22,510 --> 00:00:24,640
I want to, in this
screencast, motivate for you

9
00:00:24,640 --> 00:00:26,380
the idea that
sentiment analysis is

10
00:00:26,380 --> 00:00:28,930
a deep problem and an
important problem for NLU,

11
00:00:28,930 --> 00:00:32,698
not only scientifically but also
for industrial applications.

12
00:00:32,698 --> 00:00:34,240
In the next screencast,
I'll give you

13
00:00:34,240 --> 00:00:38,092
some practical general tips
for doing sentiment analysis.

14
00:00:38,092 --> 00:00:40,300
Following that, we'll have
two short screencasts that

15
00:00:40,300 --> 00:00:43,810
introduce our core data sets,
the Stanford Sentiment Treebank

16
00:00:43,810 --> 00:00:46,900
and a new data set
called DynaSent.

17
00:00:46,900 --> 00:00:48,720
After that, I'll
introduce the code base

18
00:00:48,720 --> 00:00:51,220
that will be working with on
the assignment in the bake-off.

19
00:00:51,220 --> 00:00:54,640
That's sst.py, which is
included in the course code

20
00:00:54,640 --> 00:00:55,910
distribution.

21
00:00:55,910 --> 00:00:58,270
And I'm going to use
that code to illustrate

22
00:00:58,270 --> 00:01:00,760
some important methodological
issues surrounding

23
00:01:00,760 --> 00:01:04,090
supervised analysis in general,
which would be hyperparameter

24
00:01:04,090 --> 00:01:06,880
tuning and comparison
of different classifiers

25
00:01:06,880 --> 00:01:09,970
to see whether they are
different in some significant

26
00:01:09,970 --> 00:01:11,890
statistical sense.

27
00:01:11,890 --> 00:01:14,350
Then we'll talk about
feature representation,

28
00:01:14,350 --> 00:01:18,520
both for large sparse linear
models with handbuilt features,

29
00:01:18,520 --> 00:01:20,830
and also with more
deep learning oriented

30
00:01:20,830 --> 00:01:22,810
distributional representations.

31
00:01:22,810 --> 00:01:25,480
And that will be a nice segue
into the final unit, which

32
00:01:25,480 --> 00:01:29,230
is on using recurrent neural
networks as classifiers

33
00:01:29,230 --> 00:01:31,540
for supervised
sentiment analysis.

34
00:01:31,540 --> 00:01:34,450
My hope is that this unit
can provide a refresher

35
00:01:34,450 --> 00:01:37,090
on core concepts in
supervised learning,

36
00:01:37,090 --> 00:01:39,940
introduce you to the problem
of sentiment analysis, which

37
00:01:39,940 --> 00:01:42,700
I think is, as I said, a central
problem for natural language

38
00:01:42,700 --> 00:01:45,670
understanding, and also
set you on your way

39
00:01:45,670 --> 00:01:49,090
toward doing the
assignment and the bake-off

40
00:01:49,090 --> 00:01:52,750
and possibly building
projects in this space.

41
00:01:52,750 --> 00:01:54,740
For the associated
materials, as I said,

42
00:01:54,740 --> 00:01:58,090
we've got a bunch of code.
sst.py is the core module.

43
00:01:58,090 --> 00:02:00,160
And then we have a
notebook introducing

44
00:02:00,160 --> 00:02:03,370
the Stanford Sentiment
Treebank as a dataset.

45
00:02:03,370 --> 00:02:05,080
We have a second
notebook that's on what

46
00:02:05,080 --> 00:02:08,530
I've called handbuilt features
and mostly linear models,

47
00:02:08,530 --> 00:02:11,800
and then a third notebook that's
on using neural networks, which

48
00:02:11,800 --> 00:02:15,160
more or less pushes you to using
distributional representations

49
00:02:15,160 --> 00:02:18,100
instead of handbuilt features,
although, as you'll see,

50
00:02:18,100 --> 00:02:22,540
the notebooks explore various
combinations of these ideas.

51
00:02:22,540 --> 00:02:25,360
The homework in the bake-off
is in the notebook homework

52
00:02:25,360 --> 00:02:26,483
sentiment.

53
00:02:26,483 --> 00:02:28,150
And I'm going to
introduce that probably

54
00:02:28,150 --> 00:02:30,220
in a separate screencast.

55
00:02:30,220 --> 00:02:32,230
The core readings
are the two papers

56
00:02:32,230 --> 00:02:34,660
that are oriented around
our data sets, the Standard

57
00:02:34,660 --> 00:02:36,970
Sentiment Treebank and DynaSent.

58
00:02:36,970 --> 00:02:38,440
And as supplementary
readings, you

59
00:02:38,440 --> 00:02:40,780
might enjoy this compendium
from Pang and Lee.

60
00:02:40,780 --> 00:02:43,090
It's a kind of overview of
the whole field of sentiment

61
00:02:43,090 --> 00:02:43,780
analysis.

62
00:02:43,780 --> 00:02:46,330
And it poses challenges
and questions that

63
00:02:46,330 --> 00:02:48,190
are still relevant to this day.

64
00:02:48,190 --> 00:02:51,310
And then Goldberg 2015
is an excellent overview

65
00:02:51,310 --> 00:02:55,570
of using neural networks
in NLP, very generally.

66
00:02:55,570 --> 00:02:57,880
But with lots of helpful
notation and so forth that

67
00:02:57,880 --> 00:02:58,743
we're aligned with.

68
00:02:58,743 --> 00:03:00,160
And that might
help you get a feel

69
00:03:00,160 --> 00:03:01,960
for the landscape
of modeling choices

70
00:03:01,960 --> 00:03:03,670
that you might
make in this space

71
00:03:03,670 --> 00:03:07,188
and in subsequent
units for this course.

72
00:03:07,188 --> 00:03:08,730
So I want to start
by just motivating

73
00:03:08,730 --> 00:03:11,530
the idea that sentiment analysis
is an interesting problem.

74
00:03:11,530 --> 00:03:13,930
Because you often hear people
say things like sentiment

75
00:03:13,930 --> 00:03:17,770
analysis is solved or it's
overly simplistic or just too

76
00:03:17,770 --> 00:03:18,480
easy.

77
00:03:18,480 --> 00:03:20,230
And I think none of
those things are true.

78
00:03:20,230 --> 00:03:23,200
And to motivate that, I just
want to do a little data driven

79
00:03:23,200 --> 00:03:24,230
exercise with you.

80
00:03:24,230 --> 00:03:26,960
So for these examples,
you should ask yourself,

81
00:03:26,960 --> 00:03:30,352
which of these sentences
expresses sentiment at all?

82
00:03:30,352 --> 00:03:32,560
And for the ones that you
think do express sentiment,

83
00:03:32,560 --> 00:03:33,760
what is that sentiment?

84
00:03:33,760 --> 00:03:36,100
Is it positive or
negative or maybe neutral

85
00:03:36,100 --> 00:03:37,330
or something else?

86
00:03:37,330 --> 00:03:39,580
So you might think those are
straightforward questions

87
00:03:39,580 --> 00:03:41,680
but this is going to get
difficult really fast.

88
00:03:41,680 --> 00:03:43,150
Consider the first example.

89
00:03:43,150 --> 00:03:45,340
There was an earthquake
in California.

90
00:03:45,340 --> 00:03:48,790
This is probably going to
sound like bad news to you.

91
00:03:48,790 --> 00:03:50,680
And many sentiment
analysis systems

92
00:03:50,680 --> 00:03:52,990
will assign this
negative sentiment.

93
00:03:52,990 --> 00:03:55,720
But we should ask
ourselves, is this actually

94
00:03:55,720 --> 00:03:57,730
a sentiment-laden sentence?

95
00:03:57,730 --> 00:04:00,640
It is on the face of it
merely stating a fact.

96
00:04:00,640 --> 00:04:04,000
And we might hold that for
sentiment to be expressed,

97
00:04:04,000 --> 00:04:07,000
we need some kind of subjective
evaluative perspective

98
00:04:07,000 --> 00:04:09,820
to be included in here, like
it was bad that there was

99
00:04:09,820 --> 00:04:11,350
an earthquake in California.

100
00:04:11,350 --> 00:04:13,750
And absent the "it
was bad" clause,

101
00:04:13,750 --> 00:04:15,970
this might just be a neutral
statement of something

102
00:04:15,970 --> 00:04:17,212
that had happened.

103
00:04:17,212 --> 00:04:19,420
But the important point here
is that unless we settle

104
00:04:19,420 --> 00:04:21,790
these questions, we'll have
continued indeterminacy

105
00:04:21,790 --> 00:04:24,390
about what we're actually doing.

106
00:04:24,390 --> 00:04:26,820
The team failed to
complete the challenge.

107
00:04:26,820 --> 00:04:28,470
Is that positive or negative?

108
00:04:28,470 --> 00:04:31,860
We might agree that it's more
than just a statement of fact,

109
00:04:31,860 --> 00:04:35,067
although it's a borderline
case even for that question.

110
00:04:35,067 --> 00:04:36,900
But if we did decide
it was sentiment-laden,

111
00:04:36,900 --> 00:04:39,390
we would need to figure out
the perspective of the speaker.

112
00:04:39,390 --> 00:04:41,490
Is the speaker
advocating for this team

113
00:04:41,490 --> 00:04:44,280
or advocating for a
different team, right?

114
00:04:44,280 --> 00:04:44,940
We win.

115
00:04:44,940 --> 00:04:45,630
We lose.

116
00:04:45,630 --> 00:04:49,008
It's really going to depend on
how the speaker is involved.

117
00:04:49,008 --> 00:04:50,550
And that, of course,
is going to have

118
00:04:50,550 --> 00:04:53,430
to become part of our definition
of what we're doing when

119
00:04:53,430 --> 00:04:55,980
we assign sentiment labels.

120
00:04:55,980 --> 00:04:57,210
They said it would be great.

121
00:04:57,210 --> 00:05:00,180
On the face of it, this
expresses no speaker

122
00:05:00,180 --> 00:05:01,200
perspective at all.

123
00:05:01,200 --> 00:05:03,480
This is merely reporting
what somebody else said.

124
00:05:03,480 --> 00:05:06,210
And we need to decide for
those obviously different

125
00:05:06,210 --> 00:05:08,040
perspectives what
we're going to do

126
00:05:08,040 --> 00:05:09,510
in terms of sentiment analysis.

127
00:05:09,510 --> 00:05:11,160
Because after all,
this could continue.

128
00:05:11,160 --> 00:05:13,410
They said it would be great
and they were right, which

129
00:05:13,410 --> 00:05:14,660
is straightforwardly positive.

130
00:05:14,660 --> 00:05:17,130
But it could also continue,
they said it would be great

131
00:05:17,130 --> 00:05:18,100
and they were wrong.

132
00:05:18,100 --> 00:05:21,600
And I think that reveals that
sentence 3 is not so obviously

133
00:05:21,600 --> 00:05:24,150
encoding a particular
speaker perspective,

134
00:05:24,150 --> 00:05:27,270
whereas these clauses are what
really tell the story for us

135
00:05:27,270 --> 00:05:29,760
as sentiment analysts.

136
00:05:29,760 --> 00:05:31,860
And then we get into
things that you might call

137
00:05:31,860 --> 00:05:34,020
non-literal use of language.

138
00:05:34,020 --> 00:05:37,330
The party fat cats are sipping
their expensive imported wines.

139
00:05:37,330 --> 00:05:39,990
This has a lot of
positive language in it.

140
00:05:39,990 --> 00:05:41,910
Maybe only fat cats
is the thing that

141
00:05:41,910 --> 00:05:44,010
sounds like a direct smear.

142
00:05:44,010 --> 00:05:46,560
But I think we could agree
that overall, this is probably

143
00:05:46,560 --> 00:05:48,987
negative in its valence.

144
00:05:48,987 --> 00:05:50,820
And that will be a
challenge for our systems

145
00:05:50,820 --> 00:05:52,530
and also a challenge
for us in just

146
00:05:52,530 --> 00:05:54,840
characterizing precisely
what was done here

147
00:05:54,840 --> 00:05:56,490
in terms of sentiment.

148
00:05:56,490 --> 00:05:58,600
Here's a similar example,
oh, you're terrible.

149
00:05:58,600 --> 00:06:00,360
This might be a criticism.

150
00:06:00,360 --> 00:06:02,778
And it might therefore be
straightforwardly negative.

151
00:06:02,778 --> 00:06:04,320
On the other hand,
it could be a kind

152
00:06:04,320 --> 00:06:06,480
of teasing form
of social bonding

153
00:06:06,480 --> 00:06:09,450
that overall has a positive
effect on the discourse.

154
00:06:09,450 --> 00:06:12,808
How are we going to resolve
that kind of context dependence?

155
00:06:12,808 --> 00:06:13,600
Here's another one.

156
00:06:13,600 --> 00:06:14,760
"Here's to ya, ya bastard."

157
00:06:14,760 --> 00:06:16,718
It's got some negative
language, even something

158
00:06:16,718 --> 00:06:18,000
that's kind of like a swear.

159
00:06:18,000 --> 00:06:22,900
But this could be a friendly,
jocular phrase of some kind.

160
00:06:22,900 --> 00:06:25,710
And we'll have to sort out
whether it's friendly and fun

161
00:06:25,710 --> 00:06:28,380
because of its negativity,
or whether this

162
00:06:28,380 --> 00:06:32,173
is straightforwardly
just a positive sentence.

163
00:06:32,173 --> 00:06:33,590
And then here's a
case that's just

164
00:06:33,590 --> 00:06:35,820
going to be a challenge
for our systems.

165
00:06:35,820 --> 00:06:37,400
This is out of the movie 2001.

166
00:06:37,400 --> 00:06:38,960
This is from an actual review.

167
00:06:38,960 --> 00:06:41,180
"Many consider the
masterpiece bewildering,

168
00:06:41,180 --> 00:06:42,800
boring, slow moving,
or annoying."

169
00:06:42,800 --> 00:06:45,110
There is lot of
negative language there.

170
00:06:45,110 --> 00:06:46,610
In fact, there's
very little that's

171
00:06:46,610 --> 00:06:48,380
positive except "masterpiece."

172
00:06:48,380 --> 00:06:50,990
But I think we can all
anticipate that overall this

173
00:06:50,990 --> 00:06:54,982
is probably going to be a
positive review of that movie.

174
00:06:54,982 --> 00:06:56,690
So that just shows
you that even if we're

175
00:06:56,690 --> 00:06:59,060
clear about what we're
doing in terms of sentiment,

176
00:06:59,060 --> 00:07:02,160
the linguistic challenge
here is significant.

177
00:07:02,160 --> 00:07:03,950
And we could also
extend that to sentiment

178
00:07:03,950 --> 00:07:06,890
like long suffering fans,
bittersweet memories,

179
00:07:06,890 --> 00:07:08,785
hilariously
embarrassing moments.

180
00:07:08,785 --> 00:07:10,910
These are things that are
going to blend positivity

181
00:07:10,910 --> 00:07:13,970
and negativity and all sorts
of other emotional dimensions

182
00:07:13,970 --> 00:07:17,330
in ways that just make sentiment
analysis very difficult to do

183
00:07:17,330 --> 00:07:19,040
reliably.

184
00:07:19,040 --> 00:07:21,290
And that's a nice segue
into this topic of sentiment

185
00:07:21,290 --> 00:07:22,340
analysis in industry.

186
00:07:22,340 --> 00:07:24,380
Because of course,
sentiment analysis

187
00:07:24,380 --> 00:07:26,750
is one of the first tasks
that was really transformed

188
00:07:26,750 --> 00:07:28,470
by data-driven approaches.

189
00:07:28,470 --> 00:07:30,860
And it was the first
task to really make

190
00:07:30,860 --> 00:07:33,033
an impact in industry.

191
00:07:33,033 --> 00:07:34,700
There are lots of
startups and companies

192
00:07:34,700 --> 00:07:36,830
that offer sentiment
analysis tools.

193
00:07:36,830 --> 00:07:38,510
And it has obvious
import for things

194
00:07:38,510 --> 00:07:41,185
like marketing, and customer
experience, and so forth.

195
00:07:41,185 --> 00:07:42,560
And I would say
the first thing I

196
00:07:42,560 --> 00:07:45,350
would say is that to this day,
the sentiment from industry,

197
00:07:45,350 --> 00:07:48,410
so to speak, is that
sentiment analysis tools still

198
00:07:48,410 --> 00:07:49,190
fall short.

199
00:07:49,190 --> 00:07:51,290
This is from an
article from 2013.

200
00:07:51,290 --> 00:07:53,330
And the gist of
it is, anyone who

201
00:07:53,330 --> 00:07:55,370
says they're getting
better than 70% today

202
00:07:55,370 --> 00:07:57,260
is lying, generally speaking.

203
00:07:57,260 --> 00:07:59,660
From whatever notion
of 70% we have here,

204
00:07:59,660 --> 00:08:02,540
I think we can agree that that's
too low and that we as a field

205
00:08:02,540 --> 00:08:04,950
ought to be offering
tools that are better.

206
00:08:04,950 --> 00:08:07,070
This is another kind
of equivocal headline.

207
00:08:07,070 --> 00:08:11,330
Emotion AI technology has great
promise when used responsibly.

208
00:08:11,330 --> 00:08:14,890
Affective computing knows
how you feel, sort of.

209
00:08:14,890 --> 00:08:17,810
The sort of is kind of like
the equivalent of 70% here.

210
00:08:17,810 --> 00:08:20,450
I think it shows that there's a
lot of work to be done if we're

211
00:08:20,450 --> 00:08:21,908
going to have the
kind of impact we

212
00:08:21,908 --> 00:08:25,063
want to have in the
technological sphere.

213
00:08:25,063 --> 00:08:26,480
And then there's
another dimension

214
00:08:26,480 --> 00:08:28,400
to this which we're not going
to really get to capture

215
00:08:28,400 --> 00:08:30,110
but is worth planting
in your minds

216
00:08:30,110 --> 00:08:32,280
because this could
become projects, right?

217
00:08:32,280 --> 00:08:34,610
We're going to do
classification of sentiment

218
00:08:34,610 --> 00:08:37,048
into positive,
negative, and neutral.

219
00:08:37,048 --> 00:08:39,590
And that's often the starting
point for these industry tools.

220
00:08:39,590 --> 00:08:41,330
Many business leaders
think they want

221
00:08:41,330 --> 00:08:44,540
these pie charts to point
out like 30% negative,

222
00:08:44,540 --> 00:08:45,530
70% positive.

223
00:08:45,530 --> 00:08:48,620
And then in Q2, the
negativity is slightly up.

224
00:08:48,620 --> 00:08:51,080
And that's surely a leading
indicator of something.

225
00:08:51,080 --> 00:08:53,030
It looks like negativity
is on the rise.

226
00:08:53,030 --> 00:08:55,430
But the issue is,
what do you do?

227
00:08:55,430 --> 00:08:57,590
How does this help
with decision making?

228
00:08:57,590 --> 00:09:00,560
Merely classifying these texts
and showing change over time

229
00:09:00,560 --> 00:09:03,380
is not enough for any business
leader to take action.

230
00:09:03,380 --> 00:09:07,400
But we need to know why this is
happening, what the underlying

231
00:09:07,400 --> 00:09:10,010
factors are, basically what
are the customers saying

232
00:09:10,010 --> 00:09:11,870
beyond these gross
classifications

233
00:09:11,870 --> 00:09:14,840
into positive,
negative, and neutral.

234
00:09:14,840 --> 00:09:16,280
And we should be
pushing ourselves

235
00:09:16,280 --> 00:09:21,400
to design tools that can offer
that next layer of insight.

236
00:09:21,400 --> 00:09:23,650
Affective computing, this
is a kind of transition

237
00:09:23,650 --> 00:09:24,910
into the wider world here.

238
00:09:24,910 --> 00:09:26,980
We're going to focus on
just sentiment analysis.

239
00:09:26,980 --> 00:09:29,200
But you could think
about emotional analysis

240
00:09:29,200 --> 00:09:32,710
and all other kinds of kind of
context dependent expression

241
00:09:32,710 --> 00:09:35,890
in language, put that under the
heading of affective computing.

242
00:09:35,890 --> 00:09:38,950
This is a diagram from a paper I
did a few years ago with Moritz

243
00:09:38,950 --> 00:09:40,210
Sudhof.

244
00:09:40,210 --> 00:09:43,990
It's a diagram of emotions
and other kinds of moods

245
00:09:43,990 --> 00:09:45,220
that people feel.

246
00:09:45,220 --> 00:09:47,140
The arcs give you a transition.

247
00:09:47,140 --> 00:09:49,090
So they show that people
tend to transition

248
00:09:49,090 --> 00:09:52,577
systematically from one
emotional state to another.

249
00:09:52,577 --> 00:09:54,160
So what we're seeing
here is basically

250
00:09:54,160 --> 00:09:56,620
just that this is a very
high dimensional space.

251
00:09:56,620 --> 00:09:58,540
It's not just positive,
negative, neutral.

252
00:09:58,540 --> 00:10:01,090
We have a wide
range of feelings,

253
00:10:01,090 --> 00:10:04,010
and moods and emotions and
states that we go into.

254
00:10:04,010 --> 00:10:06,280
And there's a lot of
structure to how we experience

255
00:10:06,280 --> 00:10:08,110
those moods in our lives.

256
00:10:08,110 --> 00:10:09,790
And it would be
great to break out

257
00:10:09,790 --> 00:10:12,520
of the simple,
positive, negative mode

258
00:10:12,520 --> 00:10:15,522
and tackle all of
these dimensions.

259
00:10:15,522 --> 00:10:17,730
And in that spirit, here
what I've done on this slide

260
00:10:17,730 --> 00:10:20,460
is just list out a whole
bunch of other tasks

261
00:10:20,460 --> 00:10:22,907
that you might consider
adjacent to sentiment analysis.

262
00:10:22,907 --> 00:10:24,990
But they are meaningfully
different from sentiment

263
00:10:24,990 --> 00:10:27,570
analysis, things
like subjectivity,

264
00:10:27,570 --> 00:10:31,620
bias, stance taking, hate
speech, microaggressions,

265
00:10:31,620 --> 00:10:34,860
condescension, sarcasm,
deception and betrayal,

266
00:10:34,860 --> 00:10:37,290
online trolls,
polarization, politeness,

267
00:10:37,290 --> 00:10:38,890
and linguistic alignment.

268
00:10:38,890 --> 00:10:41,790
These are all deeply social
things that are influenced by

269
00:10:41,790 --> 00:10:43,560
and shape our language.

270
00:10:43,560 --> 00:10:46,260
And I've selected these
papers in particular

271
00:10:46,260 --> 00:10:50,880
because all of them have really
nice either crisp statements

272
00:10:50,880 --> 00:10:54,390
of the problem and/or really
great public data sets

273
00:10:54,390 --> 00:10:56,850
that you could use for
experiments in this wide world.

274
00:10:56,850 --> 00:10:58,350
I think that's a
very exciting space

275
00:10:58,350 --> 00:11:00,570
to explore as a
kind of next step

276
00:11:00,570 --> 00:11:02,913
from what we're
doing in this unit.

277
00:11:02,913 --> 00:11:04,080
But back down to Earth here.

278
00:11:04,080 --> 00:11:05,920
Our primary data
sets, as I said,

279
00:11:05,920 --> 00:11:08,370
are going to be the ternary
formulation of the Stanford

280
00:11:08,370 --> 00:11:11,580
Sentiment Treebank which is just
positive, negative, neutral.

281
00:11:11,580 --> 00:11:15,090
And also the DynaSent data set,
which has that same ternary

282
00:11:15,090 --> 00:11:17,430
formulation.

283
00:11:17,430 --> 00:11:19,410
The SST is movie reviews.

284
00:11:19,410 --> 00:11:22,300
DynaSent is mostly reviews
of products and services,

285
00:11:22,300 --> 00:11:24,030
I think heavily biased
toward restaurants

286
00:11:24,030 --> 00:11:27,937
because the underlying
data is from Yelp.

287
00:11:27,937 --> 00:11:29,520
And then for the
bake-off, we're going

288
00:11:29,520 --> 00:11:31,410
to have a new dev/test split.

289
00:11:31,410 --> 00:11:33,930
We'll use SST-3 as
well as this new one

290
00:11:33,930 --> 00:11:36,740
of a corpus of sentences
from restaurant reviews.

291
00:11:36,740 --> 00:11:39,980
So you can see that DynaSent
might be an asset here.

292
00:11:39,980 --> 00:11:42,090
They all have this
ternary formulation.

293
00:11:42,090 --> 00:11:44,420
And I'm hoping that the
combination of these data sets

294
00:11:44,420 --> 00:11:46,170
gives us a really
interesting perspective,

295
00:11:46,170 --> 00:11:48,360
not only on sentiment
analysis but also on kind

296
00:11:48,360 --> 00:11:50,730
of how we design systems
that effectively transfer

297
00:11:50,730 --> 00:11:53,550
across domains and maybe
learn simultaneously

298
00:11:53,550 --> 00:11:55,640
in multiple domains.

