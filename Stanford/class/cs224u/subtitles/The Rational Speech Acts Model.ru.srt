1
00:00:04,880 --> 00:00:06,720
привет всем, добро пожаловать в пятую часть

2
00:00:06,720 --> 00:00:08,080
нашей серии статей об обоснованном понимании языка,

3
00:00:08,080 --> 00:00:09,360
мы будем говорить

4
00:00:09,360 --> 00:00:12,160
о модели рациональной речи x или rsa,

5
00:00:12,160 --> 00:00:13,519
это захватывающая модель,

6
00:00:13,519 --> 00:00:15,200
разработанная исследователями из

7
00:00:15,200 --> 00:00:17,119
Стэнфорда Майком Фрэнком и Ноем Гудманом, и это шанс

8
00:00:17,119 --> 00:00:19,119
для нас.  Чтобы связать идеи из когнитивной

9
00:00:19,119 --> 00:00:21,520
психологии и лингвистики с

10
00:00:21,520 --> 00:00:24,560
крупномасштабными проблемами машинного обучения,

11
00:00:24,560 --> 00:00:25,599
то, что я собираюсь сделать для этого

12
00:00:25,599 --> 00:00:27,599
скринкаста, — это как бы намекнуть на концепции высокого

13
00:00:27,599 --> 00:00:29,760
уровня и структуру базовой модели, чтобы перейти

14
00:00:29,760 --> 00:00:32,079
к

15
00:00:32,079 --> 00:00:33,600
следующему скринкасту.  который покажет

16
00:00:33,600 --> 00:00:35,440
вам, как включить части этой

17
00:00:35,440 --> 00:00:37,600
модели в стандартные модели машинного обучения.

18
00:00:37,600 --> 00:00:38,640


19
00:00:38,640 --> 00:00:40,719


20
00:00:40,719 --> 00:00:43,200


21
00:00:43,200 --> 00:00:45,360


22
00:00:45,360 --> 00:00:46,640


23
00:00:46,640 --> 00:00:48,399
ресурсы здесь, так что этот первый документ

24
00:00:48,399 --> 00:00:50,079
Goodman and Frank от разработчиков

25
00:00:50,079 --> 00:00:52,719
rsa - хороший обзор, который показывает не

26
00:00:52,719 --> 00:00:54,480
только все технические детали модели

27
00:00:54,480 --> 00:00:56,960
с реальной точностью.  r, но также связывает

28
00:00:56,960 --> 00:01:00,800
идеи с теорией принятия решений, теорией игр,

29
00:01:00,800 --> 00:01:02,559
когнитивной психологией и байесовской

30
00:01:02,559 --> 00:01:05,360
когнитивной наукой, а также лингвистикой

31
00:01:05,360 --> 00:01:06,560
оттуда, вы можете посмотреть этот

32
00:01:06,560 --> 00:01:08,400
технический скринкаст, который я сделал, это

33
00:01:08,400 --> 00:01:10,000
на YouTube, и вот соответствующие

34
00:01:10,000 --> 00:01:11,439
слайды для этого, если вы хотите следовать

35
00:01:11,439 --> 00:01:13,840
и  оттуда у меня есть

36
00:01:13,840 --> 00:01:16,000
эталонная реализация Python базовой модели rsa,

37
00:01:16,000 --> 00:01:17,840
и это был бы отличный способ

38
00:01:17,840 --> 00:01:19,680
познакомиться с моделью и начать

39
00:01:19,680 --> 00:01:21,759
думать о том, как вы могли бы включить ее

40
00:01:21,759 --> 00:01:25,680
в свой собственный проект или исходную систему

41
00:01:25,680 --> 00:01:27,040
без дальнейших церемоний, хотя давайте  Погрузитесь

42
00:01:27,040 --> 00:01:28,320
в модель, и я собираюсь начать

43
00:01:28,320 --> 00:01:29,680
с того, что я назвал прагматичными

44
00:01:29,680 --> 00:01:31,600
слушателями, и мы также можем, как вы увидите

45
00:01:31,600 --> 00:01:34,079
позже, рассмотреть перспективу говорящего,

46
00:01:34,079 --> 00:01:35,840
поэтому модель начинается с того, что называется

47
00:01:35,840 --> 00:01:38,159
буквальным слушателем, это вероятностный

48
00:01:38,159 --> 00:01:40,400
агент, а вы  может видеть, что он

49
00:01:40,400 --> 00:01:42,240
обусловливает сообщение, то есть слышит или

50
00:01:42,240 --> 00:01:44,320
наблюдает за сообщением, и делает предположение

51
00:01:44,320 --> 00:01:45,920
о состоянии мира на этой

52
00:01:45,920 --> 00:01:47,040
основе,

53
00:01:47,040 --> 00:01:48,880
и способ, которым он это делает, b  y рассуждая по

54
00:01:48,880 --> 00:01:50,960
существу полностью об

55
00:01:50,960 --> 00:01:52,799
условиях истинности языка здесь у меня есть

56
00:01:52,799 --> 00:01:54,479
эти двойные скобки, указывающие, что у

57
00:01:54,479 --> 00:01:57,280
нас есть семантический словарь, отображающий слова

58
00:01:57,280 --> 00:01:59,600
и фразы в их истинностные значения,

59
00:01:59,600 --> 00:02:01,680
этот агент также принимает во внимание предшествующее,

60
00:02:01,680 --> 00:02:03,360
но это единственный способ, которым

61
00:02:03,360 --> 00:02:05,280
это прагматично, в противном случае это своего рода

62
00:02:05,280 --> 00:02:08,959
фундаментально семантический агент,

63
00:02:08,959 --> 00:02:10,399
из которого мы создаем прагматических

64
00:02:10,399 --> 00:02:11,680
говорящих

65
00:02:11,680 --> 00:02:13,840
говорящих в этой модели, наблюдаем

66
00:02:13,840 --> 00:02:14,879
состояния мира, вещи, о которых они хотят

67
00:02:14,879 --> 00:02:16,400
сообщить, а затем они выбирают

68
00:02:16,400 --> 00:02:18,400
сообщения на этой основе,

69
00:02:18,400 --> 00:02:19,840
и основное, что нужно здесь наблюдать, это

70
00:02:19,840 --> 00:02:22,239
то, что  прагматичный говорящий рассуждает не

71
00:02:22,239 --> 00:02:24,400
о семантике языка, как

72
00:02:24,400 --> 00:02:26,160
это делает буквальный слушатель, а скорее

73
00:02:26,160 --> 00:02:28,080
о буквальном слушателе, который рассуждает

74
00:02:28,080 --> 00:02:30,480
о семантике языка, и

75
00:02:30,480 --> 00:02:32,160
для этого прагматичного говорящего здесь он делает

76
00:02:32,160 --> 00:02:34,080
это, принимая во внимание стоимость сообщений,

77
00:02:34,080 --> 00:02:35,040


78
00:02:35,040 --> 00:02:36,720
и он также имеет эту температуру

79
00:02:36,720 --> 00:02:38,160
параметр альфа, который поможет нам

80
00:02:38,160 --> 00:02:40,400
контролировать, насколько агрессивно он рассуждает

81
00:02:40,400 --> 00:02:42,160
об этом нижнем агенте.  Буквальный

82
00:02:42,160 --> 00:02:43,440
слушатель,

83
00:02:43,440 --> 00:02:44,879
кроме этого, вы, вероятно, можете видеть,

84
00:02:44,879 --> 00:02:46,959
что эта модель является своего рода

85
00:02:46,959 --> 00:02:49,680
правилом мягкого максимального решения, где мы

86
00:02:49,680 --> 00:02:53,360
объединяем буквальный слушатель со стоимостью сообщения,

87
00:02:53,360 --> 00:02:55,200
а затем, наконец, у нас есть прагматический

88
00:02:55,200 --> 00:02:56,879
слушатель, который имеет по существу ту же

89
00:02:56,879 --> 00:02:58,640
форму, что и литерал.  слушатель, он наблюдает

90
00:02:58,640 --> 00:03:00,239
за сообщением и делает предположения о

91
00:03:00,239 --> 00:03:02,480
состоянии мира на этой основе,

92
00:03:02,480 --> 00:03:04,239
и он имеет ту же общую форму, что и

93
00:03:04,239 --> 00:03:05,840
буквальный слушатель, за исключением того, что он рассуждает

94
00:03:05,840 --> 00:03:07,360
не об условиях истинности, а

95
00:03:07,360 --> 00:03:09,920
скорее о прагматичном говорящем,

96
00:03:09,920 --> 00:03:11,280
который рассуждает о буквальном

97
00:03:11,280 --> 00:03:13,200
слушателе, который  наконец, рассуждения

98
00:03:13,200 --> 00:03:15,360
о семантической грамматике, поэтому вы можете видеть, что

99
00:03:15,360 --> 00:03:16,959


100
00:03:16,959 --> 00:03:18,319
в этой модели есть своего рода рекурсивное движение вперед и назад, вы можете думать об

101
00:03:18,319 --> 00:03:21,040
этом как о рассуждениях о других разумах, и

102
00:03:21,040 --> 00:03:24,000
именно в этой рекурсии мы получаем

103
00:03:24,000 --> 00:03:26,159
использование прагматического языка.

104
00:03:26,159 --> 00:03:27,840
основные

105
00:03:27,840 --> 00:03:29,440
компоненты модели немного буквальный

106
00:03:29,440 --> 00:03:31,360
слушатель рассуждает о лексике,

107
00:03:31,360 --> 00:03:33,280
а априор

108
00:03:33,280 --> 00:03:34,959
преувеличивает прагматичный говорящий рассуждает о

109
00:03:34,959 --> 00:03:36,879
буквальный слушатель принимает

110
00:03:36,879 --> 00:03:39,200
во внимание стоимость сообщения и, наконец, прагматичный

111
00:03:39,200 --> 00:03:40,879
слушатель рассуждает о том, что прагматичный

112
00:03:40,879 --> 00:03:43,120
говорящий принимает во внимание предшествующее состояние,

113
00:03:43,120 --> 00:03:44,959
и тогда вы можете хорошо видеть эту

114
00:03:44,959 --> 00:03:46,480
точку косвенности вплоть до

115
00:03:46,480 --> 00:03:49,120
семантического лексикона, и, как я сказал, это в

116
00:03:49,120 --> 00:03:51,840
той рекурсии, которую мы получаем  интересное

117
00:03:51,840 --> 00:03:53,920
прагматичное использование языка позвольте мне показать вам,

118
00:03:53,920 --> 00:03:55,599
как это происходит, на небольшом

119
00:03:55,599 --> 00:03:58,239
примере здесь, поэтому вдоль строк в этом у меня

120
00:03:58,239 --> 00:04:00,480
есть сообщения, которые мы представляем очень

121
00:04:00,480 --> 00:04:02,000
простой язык, на котором есть только

122
00:04:02,000 --> 00:04:03,519
три сообщения, которые вы можете воспринимать как

123
00:04:03,519 --> 00:04:05,599
стенограмму  например, у человека, которого я

124
00:04:05,599 --> 00:04:07,599
имею в виду, есть борода,

125
00:04:07,599 --> 00:04:09,519
у человека, которого я имею в виду, есть очки

126
00:04:09,519 --> 00:04:11,599
и так далее, и у нас есть только три

127
00:04:11,599 --> 00:04:13,200
упоминания, и я скажу вам, что это

128
00:04:13,200 --> 00:04:14,400
Дэвид Льюис,

129
00:04:14,400 --> 00:04:16,079
один из создателей сигнализации.

130
00:04:16,079 --> 00:04:18,079
системы, которая является важным

131
00:04:18,079 --> 00:04:19,759
предшественником rsa,

132
00:04:19,759 --> 00:04:21,199
это философ и лингвист

133
00:04:21,199 --> 00:04:23,919
Пол Грейс, который проделал основополагающую работу в области

134
00:04:23,919 --> 00:04:26,160
прагматики, а это Клод Шеннон,

135
00:04:26,160 --> 00:04:27,840
который, конечно же, является разработчиком  лопер

136
00:04:27,840 --> 00:04:29,520
теории информации,

137
00:04:29,520 --> 00:04:30,960
и в этой таблице здесь у нас есть

138
00:04:30,960 --> 00:04:32,880
семантическая грамматика условия

139
00:04:32,880 --> 00:04:34,479
истинности языка, вы можете видеть, что у Льюиса есть

140
00:04:34,479 --> 00:04:36,160
эта замечательная борода,

141
00:04:36,160 --> 00:04:38,320
но ни у Грайса, ни у Шеннона нет

142
00:04:38,320 --> 00:04:39,360
бороды.

143
00:04:39,360 --> 00:04:41,759


144
00:04:41,759 --> 00:04:45,440
и Шеннон,

145
00:04:45,440 --> 00:04:47,680
буквальный слушатель, предполагающий, что у нас есть

146
00:04:47,680 --> 00:04:50,800
плоские монахи, просто ряд нормализует эти

147
00:04:50,800 --> 00:04:52,960
условия истинности, поэтому мы переходим от всех

148
00:04:52,960 --> 00:04:55,280
этих к равномерному распределению, и вы можете

149
00:04:55,280 --> 00:04:57,600
видеть, что уже борода недвусмысленна

150
00:04:57,600 --> 00:04:59,440
для этого слушателя, но очки и галстук

151
00:04:59,440 --> 00:05:00,479
представляют то, что выглядит как

152
00:05:00,479 --> 00:05:02,880
непреодолимая двусмысленность  на слуховых

153
00:05:02,880 --> 00:05:04,880
очках слушатель скорости просто должен

154
00:05:04,880 --> 00:05:06,720
угадать, был ли референт

155
00:05:06,720 --> 00:05:10,800
Льюисом или Грайсом, и то же самое для галстука,

156
00:05:10,800 --> 00:05:12,479
когда мы переходим к прагматичному говорящему, мы

157
00:05:12,479 --> 00:05:14,240
уже видим, что система начинает

158
00:05:14,240 --> 00:05:16,560
становиться более эффективной, поэтому мы рассматриваем

159
00:05:16,560 --> 00:05:18,639
перспективу говорящего по рядам  теперь

160
00:05:18,639 --> 00:05:20,320
и мы, поскольку мы собираемся предположить

161
00:05:20,320 --> 00:05:22,800
нулевую стоимость сообщений, мы снова можем просто

162
00:05:22,800 --> 00:05:25,039
нормализовать строку в этом случае из предыдущего

163
00:05:25,039 --> 00:05:27,360
матрица транспонировала его,

164
00:05:27,360 --> 00:05:29,440
и теперь вы можете видеть, что при попытке

165
00:05:29,440 --> 00:05:31,039
сообщить о Льюисе говорящий

166
00:05:31,039 --> 00:05:32,479
должен просто выбрать бороду, для этого существует

167
00:05:32,479 --> 00:05:34,639
подавляющее предубеждение,

168
00:05:34,639 --> 00:05:36,880
а здесь, внизу, наблюдая за Шенноном или

169
00:05:36,880 --> 00:05:38,240
желая поговорить о Шеннон,

170
00:05:38,240 --> 00:05:39,919
говорящий должен сказать, что это

171
00:05:39,919 --> 00:05:42,080
совершенно однозначно, но мы  все еще

172
00:05:42,080 --> 00:05:44,160
есть проблема, если мы хотим сослаться на грайс,

173
00:05:44,160 --> 00:05:46,080
у нас нет предубеждений относительно того,

174
00:05:46,080 --> 00:05:48,160
следует ли нам выбирать очки или тодд, но

175
00:05:48,160 --> 00:05:49,919
у нас уже есть более эффективная система,

176
00:05:49,919 --> 00:05:52,400
чем у нас была для буквального слушателя,

177
00:05:52,400 --> 00:05:53,759
и затем, наконец, когда мы переходим к

178
00:05:53,759 --> 00:05:55,759
прагматическому слушателю  у нас есть то, о чем вы

179
00:05:55,759 --> 00:05:57,039
могли бы думать как о полностью

180
00:05:57,039 --> 00:05:59,520
разделяющей лингвистической системе,

181
00:05:59,520 --> 00:06:02,160
э-э, услышав бороду, сделайте вывод о льюисе, о

182
00:06:02,160 --> 00:06:04,479
слуховых очках, ваш лучший выбор - Грайс,

183
00:06:04,479 --> 00:06:06,160
а услышав галстук, ваш лучший выбор -

184
00:06:06,160 --> 00:06:08,240
Шеннон, и таким образом вы можете видеть, что

185
00:06:08,240 --> 00:06:10,000
мы начали с системы, которая  выглядело

186
00:06:10,000 --> 00:06:12,560
безнадежно двусмысленным, и теперь в

187
00:06:12,560 --> 00:06:14,800
рассуждениях туда-сюда мы пришли

188
00:06:14,800 --> 00:06:16,639
к системе, которая вероятностно

189
00:06:16,639 --> 00:06:18,400
совершенно однозначна и что  в том

190
00:06:18,400 --> 00:06:20,479
смысле, в котором мы можем прагматично

191
00:06:20,479 --> 00:06:21,759
использовать

192
00:06:21,759 --> 00:06:23,680
язык и в результате этого рассуждения получить более эффективные языки.

193
00:06:23,680 --> 00:06:26,880


194
00:06:26,880 --> 00:06:28,880
Теперь для проблем генерации естественного языка

195
00:06:28,880 --> 00:06:30,560
часто бывает полезно взглянуть на

196
00:06:30,560 --> 00:06:32,240
говорящего с точки зрения, как мы обсуждали

197
00:06:32,240 --> 00:06:34,080
ранее, и я просто хочу указать  сообщаю

198
00:06:34,080 --> 00:06:35,280
вам, что

199
00:06:35,280 --> 00:06:37,440
эту модель легко сформулировать, начиная с

200
00:06:37,440 --> 00:06:38,960
говорящего, мы бы сделали это здесь,

201
00:06:38,960 --> 00:06:40,880
внизу, это имеет ту же форму, что и

202
00:06:40,880 --> 00:06:42,319
предыдущие выступающие,

203
00:06:42,319 --> 00:06:44,240
мы собираемся вычесть стоимость сообщений,

204
00:06:44,240 --> 00:06:46,160
и у нас есть это правило принятия решений softmax в

205
00:06:46,160 --> 00:06:48,080
целом, но  теперь говорящий,

206
00:06:48,080 --> 00:06:49,680
конечно, будет рассуждать непосредственно об

207
00:06:49,680 --> 00:06:51,840
условиях истинности языка,

208
00:06:51,840 --> 00:06:53,520
тогда у нас есть наш прагматичный слушатель

209
00:06:53,520 --> 00:06:55,680
, для этой точки зрения есть только один,

210
00:06:55,680 --> 00:06:57,120
и похоже, что только эти другие

211
00:06:57,120 --> 00:06:59,120
слушатели приняли доводы не об

212
00:06:59,120 --> 00:07:01,039
условиях истинности, а скорее об этом

213
00:07:01,039 --> 00:07:02,720
буквально говорящий

214
00:07:02,720 --> 00:07:04,240
и затем  наконец, для нашего прагматичного

215
00:07:04,240 --> 00:07:06,000
спикера, на котором вы могли бы

216
00:07:06,000 --> 00:07:08,560
сосредоточиться для задач генерации, он имеет ту

217
00:07:08,560 --> 00:07:10,960
же форму, что и раньше, кроме  Но теперь мы

218
00:07:10,960 --> 00:07:12,880
рассуждаем о прагматичном слушателе,

219
00:07:12,880 --> 00:07:14,240
который рассуждает о буквальном

220
00:07:14,240 --> 00:07:16,000
говорящем, так что у нас тот же вид

221
00:07:16,000 --> 00:07:18,560
косвенности,

222
00:07:18,560 --> 00:07:19,759
и еще раз здесь есть своего рода

223
00:07:19,759 --> 00:07:21,199
сокращенный способ думать о

224
00:07:21,199 --> 00:07:23,360
точке зрения говорящего, так что буквальный

225
00:07:23,360 --> 00:07:24,880
говорящий рассуждает о вычитании лексики.

226
00:07:24,880 --> 00:07:27,039
стоит, прагматичный

227
00:07:27,039 --> 00:07:28,479
слушатель рассуждает об этом буквальном

228
00:07:28,479 --> 00:07:30,560
говорящем и предшествующем состоянии, а затем,

229
00:07:30,560 --> 00:07:32,160
наконец, прагматичный говорящий рассуждает

230
00:07:32,160 --> 00:07:34,080
о прагматическом слушателе,

231
00:07:34,080 --> 00:07:36,160
принимая во внимание стоимость сообщения, и

232
00:07:36,160 --> 00:07:38,000
снова вы видите эту рекурсию вниз

233
00:07:38,000 --> 00:07:40,400
в лексикон,

234
00:07:40,400 --> 00:07:41,840
теперь я дал вам представление о том, почему это

235
00:07:41,840 --> 00:07:43,680
модель может быть мощной, но давайте

236
00:07:43,680 --> 00:07:45,199
закончим с некоторыми ограничениями, которые мы могли бы

237
00:07:45,199 --> 00:07:47,520
решить в контексте выполнения современного

238
00:07:47,520 --> 00:07:49,440
nlp и машинного обучения,

239
00:07:49,440 --> 00:07:51,280
поэтому сначала нам пришлось вручную указать этот

240
00:07:51,280 --> 00:07:53,680
лексикон в когнитивной психологии и

241
00:07:53,680 --> 00:07:55,280
лингвистике, это часто хорошо, мы

242
00:07:55,280 --> 00:07:57,120
собираемся запустить контролируемый  поэкспериментировать и

243
00:07:57,120 --> 00:07:59,360
указать лексикон на самом деле не

244
00:07:59,360 --> 00:08:01,039
препятствие, но если мы хотим работать в

245
00:08:01,039 --> 00:08:03,280
открытых доменах с la  rge corpora это,

246
00:08:03,280 --> 00:08:05,840
вероятно, нарушение условий сделки.

247
00:08:05,840 --> 00:08:08,000
Возникает связанная проблема, если вы

248
00:08:08,000 --> 00:08:09,680
более внимательно посмотрите на то, как говорящие

249
00:08:09,680 --> 00:08:11,440
агенты сформулированы в их

250
00:08:11,440 --> 00:08:13,199
знаменателе, у них есть это неявное

251
00:08:13,199 --> 00:08:15,680
суммирование по всем возможным сообщениям,

252
00:08:15,680 --> 00:08:18,000
где мы делаем это вычисление здесь, но

253
00:08:18,000 --> 00:08:20,240
в контексте естественного  язык,

254
00:08:20,240 --> 00:08:22,080
что значит суммировать по всем сообщениям,

255
00:08:22,080 --> 00:08:24,400
которые могут быть бесконечным набором,

256
00:08:24,400 --> 00:08:26,240
и даже если он конечен, потому что мы делаем

257
00:08:26,240 --> 00:08:27,840
некоторые приближения, он все равно

258
00:08:27,840 --> 00:08:29,919
будет настолько большим, что сделать этот расчет

259
00:08:29,919 --> 00:08:32,159
трудновыполнимым, поэтому для вычислительных

260
00:08:32,159 --> 00:08:34,240
приложений нам придется учитывать

261
00:08:34,240 --> 00:08:36,799
этот потенциал  недостаток,

262
00:08:36,799 --> 00:08:39,120
это также то, что вы можете считать

263
00:08:39,120 --> 00:08:40,799
моделью с очень высоким смещением, у нас

264
00:08:40,799 --> 00:08:42,719
относительно мало шансов извлечь уроки из

265
00:08:42,719 --> 00:08:45,120
данных, которые она жестко запрограммировала в конкретном

266
00:08:45,120 --> 00:08:47,760
механизме рассуждений, поскольку она негибка в

267
00:08:47,760 --> 00:08:50,959
отношении того, как этот механизм применяется,

268
00:08:50,959 --> 00:08:53,120
соответственно, мы можем столкнуться с

269
00:08:53,120 --> 00:08:54,880
такими вещами, как  трудно быть

270
00:08:54,880 --> 00:08:56,880
оратором, а ораторы, даже

271
00:08:56,880 --> 00:08:59,279
прагматичные, не всегда совершенно рациональны

272
00:08:59,279 --> 00:09:01,040
в том смысле, в каком  модель может изобразить

273
00:09:01,040 --> 00:09:03,040
их такими, и мы можем захотеть зафиксировать это,

274
00:09:03,040 --> 00:09:05,120
хотя бы для того, чтобы хорошо справляться с фактическими

275
00:09:05,120 --> 00:09:06,399
данными об использовании

276
00:09:06,399 --> 00:09:08,320
и, соответственно, даже отбросить в сторону

277
00:09:08,320 --> 00:09:10,240
давление на говорящих, чтобы они были рациональными, у

278
00:09:10,240 --> 00:09:11,760
них просто могут быть предпочтения в отношении

279
00:09:11,760 --> 00:09:13,360
определенных слов и другие вещи,

280
00:09:13,360 --> 00:09:15,440
которые модель  просто даже не

281
00:09:15,440 --> 00:09:17,440
пытаясь захватить, и мы могли бы надеяться, что в

282
00:09:17,440 --> 00:09:18,959
контексте крупномасштабной

283
00:09:18,959 --> 00:09:20,320
модели машинного обучения у нас будут

284
00:09:20,320 --> 00:09:22,800
механизмы для их ввода,

285
00:09:22,800 --> 00:09:24,880
и, наконец, это просто не масштабируется, вы

286
00:09:24,880 --> 00:09:26,320
можете видеть, что в первых двух

287
00:09:26,320 --> 00:09:28,240
пунктах списка есть много  В других смыслах,

288
00:09:28,240 --> 00:09:30,720
в которых rsa в том виде, в каком я его представил, просто

289
00:09:30,720 --> 00:09:32,880
не масштабируется до тех больших амбициозных

290
00:09:32,880 --> 00:09:34,399
задач, которые мы пытаемся решить в

291
00:09:34,399 --> 00:09:35,760
этом классе,

292
00:09:35,760 --> 00:09:37,440
следующий скринкаст попытается

293
00:09:37,440 --> 00:09:39,440
устранить все эти ограничения,

294
00:09:39,440 --> 00:09:41,760
включив rsa в  крупномасштабные

295
00:09:41,760 --> 00:09:45,080
модели машинного обучения

