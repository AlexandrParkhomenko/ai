1
00:00:05,120 --> 00:00:07,440
приветствую всех на нашем первом скринкасте

2
00:00:07,440 --> 00:00:09,519
по контекстным репрезентациям слов. Моя

3
00:00:09,519 --> 00:00:11,280
цель здесь — дать вам обзор

4
00:00:11,280 --> 00:00:13,599
этого модуля, а также дать вам представление

5
00:00:13,599 --> 00:00:15,759
о концептуальном ландшафте.

6
00:00:15,759 --> 00:00:17,039


7
00:00:17,039 --> 00:00:18,800


8
00:00:18,800 --> 00:00:20,720


9
00:00:20,720 --> 00:00:21,840
чтобы вы могли продуктивно работать

10
00:00:21,840 --> 00:00:24,080
с этой записной книжкой под названием «

11
00:00:24,080 --> 00:00:26,000
тонкая настройка», которая показывает вам, как

12
00:00:26,000 --> 00:00:27,359
точно настроить контекстуальные

13
00:00:27,359 --> 00:00:29,119
представления слов для задач классификации,

14
00:00:29,119 --> 00:00:30,960
я думаю, что это может быть очень

15
00:00:30,960 --> 00:00:32,880
мощным режимом для вас, когда вы работаете над

16
00:00:32,880 --> 00:00:35,840
текущим заданием  и выпекать

17
00:00:35,840 --> 00:00:37,680
для фона и интуиции, я настоятельно

18
00:00:37,680 --> 00:00:40,800
рекомендую эту статью Ноя Смита,

19
00:00:40,800 --> 00:00:42,719
бьющееся сердце этого устройства на самом

20
00:00:42,719 --> 00:00:44,559
деле является архитектурой трансформатора, которая была

21
00:00:44,559 --> 00:00:47,120
представлена Весвани в течение всего 2017 года в

22
00:00:47,120 --> 00:00:49,440
статье под названием «Внимание, это все, что вам нужно

23
00:00:49,440 --> 00:00:51,680
, это хорошо читаемая статья, но я

24
00:00:51,680 --> 00:00:53,120
порекомендуйте, чтобы, если вы хотите прочитать это,

25
00:00:53,120 --> 00:00:54,879
вы вместо этого прочитали выдающийся вклад Саши Раш,

26
00:00:54,879 --> 00:00:56,960
аннотированный

27
00:00:56,960 --> 00:00:59,600
трансформатор, что это делает  буквально

28
00:00:59,600 --> 00:01:01,840
воспроизводит текст veswani вообще

29
00:01:01,840 --> 00:01:03,280
2017 года

30
00:01:03,280 --> 00:01:06,000
с переплетенным кодом pi torch,

31
00:01:06,000 --> 00:01:08,159
завершающимся полной

32
00:01:08,159 --> 00:01:09,920
реализацией преобразователя применительно к

33
00:01:09,920 --> 00:01:12,159
проблемам машинного перевода,

34
00:01:12,159 --> 00:01:13,760
это замечательный вклад в том смысле,

35
00:01:13,760 --> 00:01:15,200
что в той степени, в которой есть

36
00:01:15,200 --> 00:01:16,880
точки неясности или  неопределенность в

37
00:01:16,880 --> 00:01:19,520
исходном тексте, они полностью разрешаются

38
00:01:19,520 --> 00:01:21,119
кодом Саши,

39
00:01:21,119 --> 00:01:22,560
и, конечно, это может дать вам действительно

40
00:01:22,560 --> 00:01:24,400
хороший пример того, как эффективно и

41
00:01:24,400 --> 00:01:26,080
действенно реализовать такие архитектуры моделей,

42
00:01:26,080 --> 00:01:30,159
используя pi torch

43
00:01:30,159 --> 00:01:31,600
в практических условиях, которые мы собираемся

44
00:01:31,600 --> 00:01:33,200
широко использовать  из библиотеки трансформеров с обнимающим лицом,

45
00:01:33,200 --> 00:01:34,880
которая действительно

46
00:01:34,880 --> 00:01:37,600
открыла доступ к широкому спектру

47
00:01:37,600 --> 00:01:39,920
предварительно обученных моделей трансформеров, это очень

48
00:01:39,920 --> 00:01:41,520
интересно и дало нам много новых

49
00:01:41,520 --> 00:01:43,280
вещей

50
00:01:43,280 --> 00:01:45,280
, центральная архитектура будет

51
00:01:45,280 --> 00:01:46,960
сожжена, у нас будет отдельный скринкаст

52
00:01:46,960 --> 00:01:48,240
об этом и  у нас также будет

53
00:01:48,240 --> 00:01:50,479
скринкаст о Роберте, который сильно

54
00:01:50,479 --> 00:01:52,000
оптимизирован, я думаю, это

55
00:01:52,000 --> 00:01:53,520
интересная перспектива в  ощущение,

56
00:01:53,520 --> 00:01:56,000
что они более глубоко изучили

57
00:01:56,000 --> 00:01:57,759
некоторые открытые вопросы из оригинальной

58
00:01:57,759 --> 00:02:00,399
статьи bert, а также выпустили очень

59
00:02:00,399 --> 00:02:02,240
мощные предварительно обученные параметры, которые вы

60
00:02:02,240 --> 00:02:04,000
могли бы снова использовать в контексте вашей

61
00:02:04,000 --> 00:02:06,079
собственной тонкой настройки,

62
00:02:06,079 --> 00:02:07,600
а затем для немного другого

63
00:02:07,600 --> 00:02:09,360
взгляда на эти преобразователи.  мы

64
00:02:09,360 --> 00:02:10,560
собираемся рассмотреть

65
00:02:10,560 --> 00:02:12,560
архитектуру электры, разработанную Кевином Кларком

66
00:02:12,560 --> 00:02:14,959
и его коллегами из Стэнфорда и Google. Мне

67
00:02:14,959 --> 00:02:16,480
очень нравится это как новая перспектива.

68
00:02:16,480 --> 00:02:18,400
Конечно, существует множество различных

69
00:02:18,400 --> 00:02:20,080
способов использования трансформатора.

70
00:02:20,080 --> 00:02:22,080
несколько в

71
00:02:22,080 --> 00:02:23,840
конце скринкаста, и просто ради экономии

72
00:02:23,840 --> 00:02:26,319
времени я решил сосредоточиться на Электре,

73
00:02:26,319 --> 00:02:28,000
а вы можете изучить других в своем

74
00:02:28,000 --> 00:02:30,319
собственном исследовании.

75
00:02:30,319 --> 00:02:31,920
Давайте начнем с некоторых интуиций, и я

76
00:02:31,920 --> 00:02:33,360
хотел бы начать с лингвистической

77
00:02:33,360 --> 00:02:35,040
интуиции, которая  что делать со словесными

78
00:02:35,040 --> 00:02:37,200
представлениями и как они должны

79
00:02:37,200 --> 00:02:39,360
формироваться в зависимости от контекста, давайте сосредоточимся на

80
00:02:39,360 --> 00:02:41,040
английском глаголе break,

81
00:02:41,040 --> 00:02:42,959
у нас есть простой пример: ваза разбилась,

82
00:02:42,959 --> 00:02:45,440
что означает, что она разбита на куски  s

83
00:02:45,440 --> 00:02:47,760
вот внешне похожее предложение

84
00:02:47,760 --> 00:02:49,920
рассвет сломался теперь смысл перерыва

85
00:02:49,920 --> 00:02:52,560
больше похож на

86
00:02:52,560 --> 00:02:54,560
начало новости снова сломалось простое

87
00:02:54,560 --> 00:02:56,959
непереходное предложение, но теперь глагол

88
00:02:56,959 --> 00:02:58,879
перерыв означает что-то больше похожее на опубликовать

89
00:02:58,879 --> 00:03:01,760
или появиться или стать известным

90
00:03:01,760 --> 00:03:03,519
песчаный побил мировой рекорд это

91
00:03:03,519 --> 00:03:05,360
переходное  использование глагола «ломать», и это

92
00:03:05,360 --> 00:03:08,720
означает «превзойти предыдущий уровень».

93
00:03:08,720 --> 00:03:10,319


94
00:03:10,319 --> 00:03:12,000


95
00:03:12,000 --> 00:03:13,920


96
00:03:13,920 --> 00:03:16,080


97
00:03:16,080 --> 00:03:18,720


98
00:03:18,720 --> 00:03:20,640


99
00:03:20,640 --> 00:03:22,640
это смысл, который больше похож на

100
00:03:22,640 --> 00:03:24,480
прерывание,

101
00:03:24,480 --> 00:03:26,640
и у нас есть такие идиомы, как безубыточность, что

102
00:03:26,640 --> 00:03:29,280
означает, что мы не получаем и не теряем деньги,

103
00:03:29,280 --> 00:03:31,680
и это лишь некоторые из многих

104
00:03:31,680 --> 00:03:33,200
способов использования глагола break в

105
00:03:33,200 --> 00:03:35,519
английском языке, сколько смыслов здесь задействовано

106
00:03:35,519 --> 00:03:37,120
очень трудно сказать, может быть один

107
00:03:37,120 --> 00:03:39,040
может быть два может быть десять

108
00:03:39,040 --> 00:03:40,879


109
00:03:40,879 --> 00:03:43,440


110
00:03:43,440 --> 00:03:46,000


111
00:03:46,000 --> 00:03:47,760
формируется непосредственным лингвистическим

112
00:03:47,760 --> 00:03:48,879
контекстом,

113
00:03:48,879 --> 00:03:51,040
вот несколько дополнительных примеров, у нас

114
00:03:51,040 --> 00:03:53,280
есть такие вещи, как спущенная шина, спущенное пиво,

115
00:03:53,280 --> 00:03:55,280
плоская нота, плоская поверхность.

116
00:03:55,280 --> 00:03:57,920


117
00:03:57,920 --> 00:04:00,799


118
00:04:00,799 --> 00:04:02,799
совсем

119
00:04:02,799 --> 00:04:04,560
другой смысл слова «плоский», чем мы получаем

120
00:04:04,560 --> 00:04:07,280
от «плоского нота» или «плоская поверхность»

121
00:04:07,280 --> 00:04:09,120
есть что-то похожее на «устроить вечеринку,

122
00:04:09,120 --> 00:04:11,439
устроить драку, бросить мяч, бросить, припадок»

123
00:04:11,439 --> 00:04:13,200
у нас есть смесь того, что вы могли бы назвать

124
00:04:13,200 --> 00:04:15,200
здесь буквальным и метафорическим, но опять же

125
00:04:15,200 --> 00:04:17,199
своего рода общее ядро  на что мы опираемся

126
00:04:17,199 --> 00:04:19,680
, но суть в том, что смысл

127
00:04:19,680 --> 00:04:22,240
слова бросок в этом случае сильно различается в

128
00:04:22,240 --> 00:04:23,840
зависимости от того, в каком лингвистическом

129
00:04:23,840 --> 00:04:26,240
контексте он находится,

130
00:04:26,240 --> 00:04:28,400
и мы можем распространить это на вещи, которые,

131
00:04:28,400 --> 00:04:30,400
кажется, больше обращаются к познанию мира, поэтому,

132
00:04:30,400 --> 00:04:32,160
если вы  есть что-то вроде крана,

133
00:04:32,160 --> 00:04:33,919
поймавшего рыбу, у нас есть ощущение, что

134
00:04:33,919 --> 00:04:35,759
кран здесь птица,

135
00:04:35,759 --> 00:04:37,440
тогда как если у нас есть кран, поднимающий

136
00:04:37,440 --> 00:04:39,199
стальную балку, у нас есть ощущение, что это

137
00:04:39,199 --> 00:04:40,800
часть оборудования,

138
00:04:40,800 --> 00:04:42,240
это похоже на  что-то, что

139
00:04:42,240 --> 00:04:44,720
руководствуется нашим пониманием птиц и

140
00:04:44,720 --> 00:04:47,440
оборудования, рыбы и лучей, и когда у нас

141
00:04:47,440 --> 00:04:49,520
есть относительно беспристрастные предложения, такие как

142
00:04:49,520 --> 00:04:51,360
я видел журавль, мы как бы

143
00:04:51,360 --> 00:04:53,360
гадаем о том, какой объект

144
00:04:53,360 --> 00:04:55,360
задействован птица или машина,

145
00:04:55,360 --> 00:04:56,880
и мы можем расширить этот прошлый мир

146
00:04:56,880 --> 00:04:58,320
знание в вещи, которые больше похожи на

147
00:04:58,320 --> 00:05:00,160
понимание дискурса, поэтому, если у вас есть

148
00:05:00,160 --> 00:05:02,560
предложение типа «есть ли опечатки, я не

149
00:05:02,560 --> 00:05:03,680
видел

150
00:05:03,680 --> 00:05:05,600
никакого смысла здесь», у нас есть ощущение

151
00:05:05,600 --> 00:05:07,360
, что что-то остановлено, но, вероятно,

152
00:05:07,360 --> 00:05:10,080
локализовано на любом, и любой здесь означает любые

153
00:05:10,080 --> 00:05:12,320
опечатки как  в результате предыдущего

154
00:05:12,320 --> 00:05:14,479
лингвистического контекста

155
00:05:14,479 --> 00:05:16,080
есть ли какие-нибудь книжные магазины в центре города я

156
00:05:16,080 --> 00:05:18,160
не видел ни одного такого же второго предложения, но

157
00:05:18,160 --> 00:05:20,320
теперь смысл любого, вероятно,

158
00:05:20,320 --> 00:05:22,160
будет больше похож на книжные магазины

159
00:05:22,160 --> 00:05:24,240
в результате дискурсивного контекста, в котором

160
00:05:24,240 --> 00:05:26,000
оно появляется,

161
00:05:26,000 --> 00:05:28,320
так что все  это просто показывает, насколько

162
00:05:28,320 --> 00:05:30,160
отдельные языковые единицы могут быть

163
00:05:30,160 --> 00:05:32,639
сформированы контекстными лингвистами, это

164
00:05:32,639 --> 00:05:34,320
глубоко известно, это первичная вещь, которую

165
00:05:34,320 --> 00:05:36,320
лингвисты пытаются уловить, и я

166
00:05:36,320 --> 00:05:37,680
думаю  k это замечательная точка

167
00:05:37,680 --> 00:05:40,080
связи между тем, что делают лингвисты, и

168
00:05:40,080 --> 00:05:41,840
тем, как мы представляем примеры в

169
00:05:41,840 --> 00:05:44,720
НЛП, используя контекстуальные модели, это

170
00:05:44,720 --> 00:05:46,560
очень захватывающее развитие для меня как для

171
00:05:46,560 --> 00:05:50,240
лингвиста, так и для НЛП

172
00:05:51,759 --> 00:05:53,440
вот еще один набор интуиций, который

173
00:05:53,440 --> 00:05:55,360
больше связан с вещами  как

174
00:05:55,360 --> 00:05:57,039
архитектура модели и то, что вы могли бы назвать

175
00:05:57,039 --> 00:05:59,120
индуктивными смещениями для различных моделей

176
00:05:59,120 --> 00:06:00,240
моделей,

177
00:06:00,240 --> 00:06:01,759
давайте начнем здесь слева, это

178
00:06:01,759 --> 00:06:03,680
модель с высоким смещением в том смысле, что она

179
00:06:03,680 --> 00:06:06,479
принимает множество априорных решений о

180
00:06:06,479 --> 00:06:08,479
том, как мы будем представлять наши примеры,

181
00:06:08,479 --> 00:06:10,560
идея заключается в том, что  у нас есть три токена, которые

182
00:06:10,560 --> 00:06:13,199
мы ищем в фиксированном пространстве вложений,

183
00:06:13,199 --> 00:06:14,880
а затем мы решили суммировать

184
00:06:14,880 --> 00:06:16,560
эти вложения, просто суммируя их

185
00:06:16,560 --> 00:06:18,400
вместе, чтобы получить представление для

186
00:06:18,400 --> 00:06:19,840
всего примера,

187
00:06:19,840 --> 00:06:21,680
очень немногие из этих компонентов изучены

188
00:06:21,680 --> 00:06:23,440
как часть нашей проблемы, которую мы  приняли

189
00:06:23,440 --> 00:06:25,919
большинство решений заранее,

190
00:06:25,919 --> 00:06:27,759
как мы видели, когда мы переходим к рекуррентной

191
00:06:27,759 --> 00:06:29,840
нейронной сети, мы ослабляем некоторые из этих

192
00:06:29,840 --> 00:06:31,759
предположений, мы все еще собираемся искать

193
00:06:31,759 --> 00:06:33,840
слова  в фиксированном пространстве встраивания, но теперь

194
00:06:33,840 --> 00:06:35,440
вместо того, чтобы решить, что мы знаем

195
00:06:35,440 --> 00:06:36,800
правильный способ их объединения с

196
00:06:36,800 --> 00:06:39,039
суммированием, мы изучим из наших

197
00:06:39,039 --> 00:06:41,360
данных очень сложную функцию для

198
00:06:41,360 --> 00:06:43,440
их объединения, и это, по-видимому,

199
00:06:43,440 --> 00:06:45,600
позволит нам быть более отзывчивыми, чем

200
00:06:45,600 --> 00:06:47,919
менее  предвзято относится к тому, как данные

201
00:06:47,919 --> 00:06:49,919
могут выглядеть,

202
00:06:49,919 --> 00:06:51,520
как древовидная архитектура

203
00:06:51,520 --> 00:06:53,120
здесь представляет собой интересную смесь этих

204
00:06:53,120 --> 00:06:55,120
идей, это похоже на рекуррентную нейронную

205
00:06:55,120 --> 00:06:56,880
сеть, вместо того, чтобы предполагать,

206
00:06:56,880 --> 00:06:58,479
что я могу обрабатывать данные слева

207
00:06:58,479 --> 00:07:00,400
направо, данные обрабатываются в

208
00:07:00,400 --> 00:07:02,560
составные части, такие как  камень

209
00:07:02,560 --> 00:07:05,520
как составляющая исключен из правил

210
00:07:05,520 --> 00:07:06,400
здесь и

211
00:07:06,400 --> 00:07:08,000
сейчас это, вероятно, будет очень

212
00:07:08,000 --> 00:07:09,759
мощным, если мы правы в том, что

213
00:07:09,759 --> 00:07:11,599
языковые данные структурированы в соответствии

214
00:07:11,599 --> 00:07:13,520
с этими составляющими, потому что это

215
00:07:13,520 --> 00:07:16,080
даст нам толчок в плане обучения, это

216
00:07:16,080 --> 00:07:17,840
может быть контрпродуктивно  хотя в той

217
00:07:17,840 --> 00:07:19,599
степени, в которой эта составляющая структура

218
00:07:19,599 --> 00:07:21,599
неверна, и я думаю, что это показывает, что

219
00:07:21,599 --> 00:07:23,759
предубеждения, которые мы навязываем на уровне

220
00:07:23,759 --> 00:07:26,000
уровня  наши архитектуры могут быть как полезными,

221
00:07:26,000 --> 00:07:27,759
так и помехой в зависимости от того, как

222
00:07:27,759 --> 00:07:30,000
они согласуются с проблемой, управляемой данными,

223
00:07:30,000 --> 00:07:32,479
которую мы пытаемся решить

224
00:07:32,479 --> 00:07:34,319
в правом нижнем углу здесь. У меня есть

225
00:07:34,319 --> 00:07:36,560
наименее предвзятая модель во всех этих смыслах

226
00:07:36,560 --> 00:07:39,039
из всех изображенных здесь.  у меня есть

227
00:07:39,039 --> 00:07:40,639
рекуррентная нейронная сеть, подобная этой,

228
00:07:40,639 --> 00:07:42,319
за исключением того, что теперь я предполагаю, что информация

229
00:07:42,319 --> 00:07:44,479
может передаваться в двух направлениях, поэтому больше не

230
00:07:44,479 --> 00:07:46,479
предполагается, что слева направо,

231
00:07:46,479 --> 00:07:48,160
и, кроме того, я добавил эти

232
00:07:48,160 --> 00:07:50,000
механизмы внимания, о которых мы будем

233
00:07:50,000 --> 00:07:52,000
много говорить  в этой единице, но, по сути,

234
00:07:52,000 --> 00:07:54,319
думайте о них как о способах создания

235
00:07:54,319 --> 00:07:56,240
особых связей между всеми

236
00:07:56,240 --> 00:07:58,560
скрытыми единицами, и идея здесь состоит в том, что

237
00:07:58,560 --> 00:08:00,800
мы позволим данным сказать нам, как

238
00:08:00,800 --> 00:08:02,479
взвесить все эти различные связи

239
00:08:02,479 --> 00:08:05,280
и, в свою очередь, представить наши примеры, которые мы

240
00:08:05,280 --> 00:08:07,840
принимая очень мало решений заранее

241
00:08:07,840 --> 00:08:09,360
о том, какие соединения могут быть

242
00:08:09,360 --> 00:08:11,120
установлены, и вместо этого просто слушая

243
00:08:11,120 --> 00:08:13,680
данные и процесс обучения

244
00:08:13,680 --> 00:08:16,080
, мы находимся на этом этапе на пути

245
00:08:16,080 --> 00:08:17,759
к преобразователю, который является своего

246
00:08:17,759 --> 00:08:19,759
рода  f крайний случай соединения всего

247
00:08:19,759 --> 00:08:21,680
со всем остальным, а затем

248
00:08:21,680 --> 00:08:23,599
предоставление данных сказать нам, как взвесить все

249
00:08:23,599 --> 00:08:26,960
эти различные связи,

250
00:08:26,960 --> 00:08:28,560
и это приводит нас к этому понятию

251
00:08:28,560 --> 00:08:30,000
внимания, которое мы не обсуждали

252
00:08:30,000 --> 00:08:31,759
раньше, но я думаю, что могу представить

253
00:08:31,759 --> 00:08:33,679
концепции  немного математики, а затем

254
00:08:33,679 --> 00:08:35,039
мы снова увидим их в этом разделе,

255
00:08:35,039 --> 00:08:36,958
поэтому давайте начнем с

256
00:08:36,958 --> 00:08:38,399
простого примера тональности и представим, что мы

257
00:08:38,399 --> 00:08:40,159
имеем дело с рекуррентным классификатором нейронной сети,

258
00:08:40,159 --> 00:08:42,719
наш пример действительно не так

259
00:08:42,719 --> 00:08:43,679
хорош, и мы собираемся  помещать

260
00:08:43,679 --> 00:08:46,320
классификатор традиционно поверх этого

261
00:08:46,320 --> 00:08:48,560
конечного скрытого состояния здесь,

262
00:08:48,560 --> 00:08:51,519
но мы можем беспокоиться об этом, что

263
00:08:51,519 --> 00:08:53,120
к тому времени, когда мы доберемся до этого конечного

264
00:08:53,120 --> 00:08:54,959
состояния, вклад этих более ранних

265
00:08:54,959 --> 00:08:56,399
слов, которые явно важны с

266
00:08:56,399 --> 00:08:58,160
лингвистической точки зрения, может быть как бы

267
00:08:58,160 --> 00:09:00,640
забыт или чрезмерно  рассредоточиться,

268
00:09:00,640 --> 00:09:02,880
чтобы механизмы внимания были

269
00:09:02,880 --> 00:09:05,360
для нас способом вернуть эту информацию

270
00:09:05,360 --> 00:09:08,320
и наполнить эту репрезентацию

271
00:09:08,320 --> 00:09:10,000
некоторыми из этих предыдущих

272
00:09:10,000 --> 00:09:12,080
важных связей,

273
00:09:12,080 --> 00:09:13,360
чтобы ее  д., как мы это делаем.

274
00:09:13,360 --> 00:09:15,360
Сначала мы собираемся получить некоторые оценки напряженности, которые являются

275
00:09:15,360 --> 00:09:17,600
просто скалярными произведениями нашего целевого вектора

276
00:09:17,600 --> 00:09:20,000
со всеми предшествующими скрытыми состояниями, так

277
00:09:20,000 --> 00:09:21,440
что это дает нам вектор оценок,

278
00:09:21,440 --> 00:09:24,320
которые традиционно нормализованы по мягкому максимуму,

279
00:09:24,320 --> 00:09:26,080
а затем то, что мы делаем, это  создайте вектор контекста

280
00:09:26,080 --> 00:09:27,519
,

281
00:09:27,519 --> 00:09:29,200
взвесив каждое из предыдущих скрытых

282
00:09:29,200 --> 00:09:31,680
состояний по его весу внимания,

283
00:09:31,680 --> 00:09:33,360
а затем взяв среднее из них, чтобы

284
00:09:33,360 --> 00:09:36,240
получить вектор контекста k,

285
00:09:36,240 --> 00:09:37,120
а затем у нас будет этот

286
00:09:37,120 --> 00:09:39,600
специальный слой, который объединяет k

287
00:09:39,600 --> 00:09:42,160
с нашим предыдущим окончательным скрытым  состояние

288
00:09:42,160 --> 00:09:44,080
и подает это через этот слой

289
00:09:44,080 --> 00:09:46,720
параметров обучения и нелинейности, чтобы

290
00:09:46,720 --> 00:09:49,279
дать нам это новое скрытое представление h

291
00:09:49,279 --> 00:09:50,480
тильда,

292
00:09:50,480 --> 00:09:52,800
и это h тильда, которая, наконец, является

293
00:09:52,800 --> 00:09:55,440
входом для нашего классификатора softmax, тогда как

294
00:09:55,440 --> 00:09:57,120
раньше мы просто прямо вводили

295
00:09:57,120 --> 00:10:00,480
hc здесь мы  теперь введите эту более

296
00:10:00,480 --> 00:10:02,560
совершенную версию, которая привлекает

297
00:10:02,560 --> 00:10:04,160
все эти связи внимания, которые мы

298
00:10:04,160 --> 00:10:06,160
создали с помощью этих механизмов,

299
00:10:06,160 --> 00:10:08,320
и снова, как вы увидите, трансформатор

300
00:10:08,320 --> 00:10:10,320
делает все это  над местом со всеми

301
00:10:10,320 --> 00:10:12,720
его представлениями в различных точках

302
00:10:12,720 --> 00:10:15,440
его вычислений

303
00:10:15,440 --> 00:10:17,120
вот еще одна направляющая идея, которая

304
00:10:17,120 --> 00:10:18,640
действительно формирует то, как работают

305
00:10:18,640 --> 00:10:20,640


306
00:10:20,640 --> 00:10:22,560
эти модели.

307
00:10:22,560 --> 00:10:24,800
мы могли бы

308
00:10:24,800 --> 00:10:27,839
ожидать, что я загрузил токенизатор burt,

309
00:10:27,839 --> 00:10:29,279
и вы можете видеть, что для такого

310
00:10:29,279 --> 00:10:31,279
предложения не так уж удивительно, что результатом являются

311
00:10:31,279 --> 00:10:34,000
довольно знакомые токены в целом,

312
00:10:34,000 --> 00:10:35,760
но когда я ввожу что-то вроде encode

313
00:10:35,760 --> 00:10:37,839
me, интуитивно понятное слово encode  разделены

314
00:10:37,839 --> 00:10:40,480
на две части слова, и ясно,

315
00:10:40,480 --> 00:10:42,160
что мы как бы неявно предполагаем,

316
00:10:42,160 --> 00:10:44,640
что модель, поскольку она контекстуальна, может

317
00:10:44,640 --> 00:10:46,480
понять, что эти части в некотором

318
00:10:46,480 --> 00:10:49,279
концептуальном смысле являются одним словом, и вы можете

319
00:10:49,279 --> 00:10:51,279
расширить это до идиом, таких как «из

320
00:10:51,279 --> 00:10:52,880
этого мира», где мы рассматриваем  их как

321
00:10:52,880 --> 00:10:55,120
набор отдельных токенов, но мы можем

322
00:10:55,120 --> 00:10:56,800
надеяться, что модель сможет понять, что

323
00:10:56,800 --> 00:11:00,160
в этой фразе есть идиоматическое единство, и это

324
00:11:00,160 --> 00:11:01,600
также имеет дополнительное преимущество, заключающееся в том, что для

325
00:11:01,600 --> 00:11:04,160
неизвестных токенов, таких как snuffleupa  Да, это может

326
00:11:04,160 --> 00:11:06,079
разбить их на знакомые части,

327
00:11:06,079 --> 00:11:08,480
и у нас, по крайней мере, есть надежда получить

328
00:11:08,480 --> 00:11:10,560
осмысленное представление для этого

329
00:11:10,560 --> 00:11:12,240
словарного элемента,

330
00:11:12,240 --> 00:11:13,600
результатом всего этого является то, что этим

331
00:11:13,600 --> 00:11:15,360
моделям может сойти с рук наличие очень

332
00:11:15,360 --> 00:11:18,320
маленького словаря именно потому, что

333
00:11:18,320 --> 00:11:20,720
мы полагаемся на  их неявно, чтобы они были

334
00:11:20,720 --> 00:11:23,920
действительно контекстуальными,

335
00:11:24,160 --> 00:11:26,000
вот еще одна вдохновляющая идея, с которой мы

336
00:11:26,000 --> 00:11:27,600
не сталкивались раньше, это называется

337
00:11:27,600 --> 00:11:29,440
позиционным кодированием,

338
00:11:29,440 --> 00:11:30,880
и это еще один способ, которым мы можем

339
00:11:30,880 --> 00:11:33,200
зафиксировать чувствительность слов к их

340
00:11:33,200 --> 00:11:34,399
контексту,

341
00:11:34,399 --> 00:11:36,160
так что вы увидите, когда пройдете весь путь

342
00:11:36,160 --> 00:11:38,160
вниз  внутри архитектуры трансформатора у

343
00:11:38,160 --> 00:11:40,240
вас есть традиционное статическое

344
00:11:40,240 --> 00:11:41,920
вложение того типа, которое мы обсуждали

345
00:11:41,920 --> 00:11:44,000
в первом разделе этого курса, они

346
00:11:44,000 --> 00:11:46,160
выделены светло-серым цветом, здесь фиксированные представления

347
00:11:46,160 --> 00:11:47,440
для слов,

348
00:11:47,440 --> 00:11:49,600
однако в контексте такой модели, как

349
00:11:49,600 --> 00:11:52,320
vert, то, что мы традиционно считаем

350
00:11:52,320 --> 00:11:54,639
ее  представление вложения на самом деле

351
00:11:54,639 --> 00:11:57,519
является комбинацией этого фиксированного вложения

352
00:11:57,519 --> 00:11:59,200
и отдельного пространства вложения,

353
00:11:59,200 --> 00:12:01,040
называемого позиционным вложением,

354
00:12:01,040 --> 00:12:02,959
где  мы изучили представления

355
00:12:02,959 --> 00:12:05,760
для каждой позиции в последовательности, у

356
00:12:05,760 --> 00:12:07,519
этого есть интригующее свойство, что

357
00:12:07,519 --> 00:12:09,680
одно и то же слово, такое как , будет

358
00:12:09,680 --> 00:12:11,600
иметь различное вложение в этом

359
00:12:11,600 --> 00:12:13,839
смысле зеленого представления здесь в зависимости

360
00:12:13,839 --> 00:12:16,079
от того, где оно появляется в вашей последовательности, так что

361
00:12:16,079 --> 00:12:18,399
прямо с самого начала  у нас есть

362
00:12:18,399 --> 00:12:20,880
понятие контекстной чувствительности еще до того, как мы

363
00:12:20,880 --> 00:12:22,560
начали связывать вещи

364
00:12:22,560 --> 00:12:25,600
всевозможными интересными способами,

365
00:12:25,600 --> 00:12:27,279
теперь давайте перейдем к некоторым текущим проблемам

366
00:12:27,279 --> 00:12:28,800
и усилиям, некоторым высокоуровневым вещам, о которых

367
00:12:28,800 --> 00:12:30,079
вы можете подумать, работая

368
00:12:30,079 --> 00:12:31,440
над этим блоком,

369
00:12:31,440 --> 00:12:33,440
это  действительно хороший график из

370
00:12:33,440 --> 00:12:35,839
электра-бумаги от Кларка вообще

371
00:12:35,839 --> 00:12:37,760
вдоль оси X здесь у нас есть

372
00:12:37,760 --> 00:12:39,279
операции с плавающей запятой, которые вы могли бы рассматривать

373
00:12:39,279 --> 00:12:41,839
как своего рода базовую меру вычислительных

374
00:12:41,839 --> 00:12:43,519
ресурсов, необходимых для создания этих

375
00:12:43,519 --> 00:12:46,079
представлений, а по оси Y у нас

376
00:12:46,079 --> 00:12:47,519
есть клей  оценка, так что это похоже на

377
00:12:47,519 --> 00:12:50,079
стандартный тест nlu,

378
00:12:50,079 --> 00:12:51,680
и смысл этого графика здесь в том, что

379
00:12:51,680 --> 00:12:53,200
мы достигаем своего рода убывающей

380
00:12:53,200 --> 00:12:55,920
отдачи, поэтому у нас был быстрый рост от

381
00:12:55,920 --> 00:12:58,959
перчаток  d до bert,

382
00:12:58,959 --> 00:13:00,800
где мы действительно намного лучше справляемся с

383
00:13:00,800 --> 00:13:02,399
этими клеевыми показателями,

384
00:13:02,399 --> 00:13:04,000
мы увеличиваем количество операций с плавающей запятой,

385
00:13:04,000 --> 00:13:05,279
но, похоже, это

386
00:13:05,279 --> 00:13:07,040
соизмеримо с тем, как мы работаем в

387
00:13:07,040 --> 00:13:09,360
тесте, но теперь с этими более крупными

388
00:13:09,360 --> 00:13:11,519
моделями, такими как excel net  и Роберта,

389
00:13:11,519 --> 00:13:13,440
возможно, дело в том, что мы достигаем

390
00:13:13,440 --> 00:13:16,000
убывающей отдачи. Роберта включает в себя

391
00:13:16,000 --> 00:13:18,639
более чем в 3000 раз больше операций с плавающей запятой, чем

392
00:13:18,639 --> 00:13:21,040
перчатка,

393
00:13:21,040 --> 00:13:23,839
но это не намного лучше по этой

394
00:13:23,839 --> 00:13:26,399
оси Y, чем некоторые из его более простых вариантов,

395
00:13:26,399 --> 00:13:28,000
таких как bert base, и поэтому это  что-

396
00:13:28,000 --> 00:13:29,200
то, о чем мы должны думать, когда мы думаем

397
00:13:29,200 --> 00:13:32,160
о затратах с точки зрения денег,

398
00:13:32,160 --> 00:13:35,040
окружающей среды, энергии и так далее,

399
00:13:35,040 --> 00:13:36,560
когда мы думаем о разработке этих

400
00:13:36,560 --> 00:13:38,240
больших моделей,

401
00:13:38,240 --> 00:13:40,160
и вот действительно крайний случай, кто

402
00:13:40,160 --> 00:13:42,320
знает, как долго мы можем обучать эти вещи

403
00:13:42,320 --> 00:13:44,399
или сколько  выгоду, которую мы получим, когда поступим

404
00:13:44,399 --> 00:13:46,399
так, но в определенный момент мы, вероятно

405
00:13:46,399 --> 00:13:49,120
, понесем затраты, которые превысят любые

406
00:13:49,120 --> 00:13:50,720
выгоды, которые мы можем оправдать в отношении

407
00:13:50,720 --> 00:13:52,720
проблем, которые мы пытаемся решить,

408
00:13:52,720 --> 00:13:54,079
и такого рода  приводит нас к этой

409
00:13:54,079 --> 00:13:55,600
прекрасной статье, в которой говорится об

410
00:13:55,600 --> 00:13:57,760
экологическом воздействии обучения

411
00:13:57,760 --> 00:13:59,279
этих действительно больших моделей, и это просто

412
00:13:59,279 --> 00:14:01,040
показывает, что, как и обучение большого

413
00:14:01,040 --> 00:14:03,279
трансформатора с нуля, действительно влечет за собой

414
00:14:03,279 --> 00:14:05,519
большие экологические затраты

415
00:14:05,519 --> 00:14:06,880
, и это то, что мы, безусловно, должны

416
00:14:06,880 --> 00:14:08,480
иметь в виду, когда мы думаем об использовании

417
00:14:08,480 --> 00:14:10,639
этих  моделей для меня это сложный

418
00:14:10,639 --> 00:14:12,240
вопрос, потому что это компенсируется

419
00:14:12,240 --> 00:14:14,079
тем фактом, что в целом

420
00:14:14,079 --> 00:14:15,519
все мы не обучаем их с

421
00:14:15,519 --> 00:14:18,160
нуля, а скорее пользуемся

422
00:14:18,160 --> 00:14:19,920
общедоступными предварительно обученными

423
00:14:19,920 --> 00:14:21,519
представлениями,

424
00:14:21,519 --> 00:14:23,519
поэтому, хотя предварительное обучение для этой

425
00:14:23,519 --> 00:14:26,000
версии имело  большие экологические затраты,

426
00:14:26,000 --> 00:14:27,839
кажется, что это как бы компенсируется тем

427
00:14:27,839 --> 00:14:29,600
фактом, что многие из нас получают

428
00:14:29,600 --> 00:14:31,040
от этого выгоду, и может быть, что в

429
00:14:31,040 --> 00:14:33,760
совокупности это менее затратно для окружающей среды,

430
00:14:33,760 --> 00:14:36,320
чем в старые времена, когда все мы

431
00:14:36,320 --> 00:14:38,000
всегда обучали все наши модели

432
00:14:38,000 --> 00:14:40,160
буквально с  на самом деле я просто не знаю,

433
00:14:40,160 --> 00:14:42,079
как здесь производить расчеты, но я

434
00:14:42,079 --> 00:14:44,480
знаю, что расширенный доступ

435
00:14:44,480 --> 00:14:46,399
расширяет возможности и, вероятно, компенсирует

436
00:14:46,399 --> 00:14:48,560
часть затрат, и во многом это связано

437
00:14:48,560 --> 00:14:50,240
с вкладом библиотеки обнимающих лиц

438
00:14:50,240 --> 00:14:52,720


439
00:14:53,040 --> 00:14:54,399
, в том же направлении предпринимается много усилий,

440
00:14:54,399 --> 00:14:56,240
чтобы сделать bert меньше,

441
00:14:56,240 --> 00:14:58,000
сжимая его буквально меньше

442
00:14:58,000 --> 00:14:59,680
размеров и другие виды

443
00:14:59,680 --> 00:15:01,760
упрощения тренировочного процесса

444
00:15:01,760 --> 00:15:04,000
и отрыжки.  дистилляция и т. д.

445
00:15:04,000 --> 00:15:05,920
вот два выдающихся вклада, своего

446
00:15:05,920 --> 00:15:07,680
рода сборники множества различных

447
00:15:07,680 --> 00:15:09,360
идей в этой области,

448
00:15:09,360 --> 00:15:11,360
и я также настоятельно рекомендую эту прекрасную

449
00:15:11,360 --> 00:15:13,839
статью под названием виртуология прайм-рана, в

450
00:15:13,839 --> 00:15:15,600
которой исследуется множество различных

451
00:15:15,600 --> 00:15:17,600
аспектов того, что мы знаем о bert и о

452
00:15:17,600 --> 00:15:18,800
том, как он работает.

453
00:15:18,800 --> 00:15:21,360
различные варианты, люди пробовали

454
00:15:21,360 --> 00:15:22,959
различные вещи, которые люди делали, чтобы

455
00:15:22,959 --> 00:15:24,720
исследовать эти модели и понять их

456
00:15:24,720 --> 00:15:26,639
динамику обучения и т. д. и т. д.

457
00:15:26,639 --> 00:15:29,440
это очень ценный вклад, безусловно,

458
00:15:29,440 --> 00:15:31,120
может быть ресурсом для вас, когда вы думаете

459
00:15:31,120 --> 00:15:34,000
об этих моделях,

460
00:15:34,320 --> 00:15:35,120
и

461
00:15:35,120 --> 00:15:37,120
просто потому, что мы этого не делаем.  у вас нет времени охватить

462
00:15:37,120 --> 00:15:38,800
их все, есть куча

463
00:15:38,800 --> 00:15:40,480
интересных вариантов трансформеров, которые

464
00:15:40,480 --> 00:15:42,800
мы не сможем обсуждать в деталях.  или

465
00:15:42,800 --> 00:15:44,639
я подумал, что упомяну их здесь. esper —

466
00:15:44,639 --> 00:15:46,320
это попытка разработать

467
00:15:46,320 --> 00:15:47,839
репрезентации на уровне предложений от bert, которые

468
00:15:47,839 --> 00:15:49,920
особенно хороши для определения того, какие

469
00:15:49,920 --> 00:15:51,759
предложения похожи на какие другие

470
00:15:51,759 --> 00:15:54,320
предложения в соответствии с косинусным сходством,

471
00:15:54,320 --> 00:15:55,920
я думаю, что это может быть мощным способом

472
00:15:55,920 --> 00:15:58,240
мышления об этих  представления, а

473
00:15:58,240 --> 00:16:00,160
также практическая полезность, если вам нужно

474
00:16:00,160 --> 00:16:00,959
найти,

475
00:16:00,959 --> 00:16:02,639
какие предложения похожи на какие

476
00:16:02,639 --> 00:16:03,839
другие,

477
00:16:03,839 --> 00:16:06,160
вы, вероятно, слышали о gpt,

478
00:16:06,160 --> 00:16:08,480
генеративном предварительно обученном преобразователе в

479
00:16:08,480 --> 00:16:11,360
различных его формах, вы можете получить gpt2

480
00:16:11,360 --> 00:16:13,360
из обнимающего пространства и обнимающего лица, и у

481
00:16:13,360 --> 00:16:15,440
вас есть неограниченный доступ  к нему, и,

482
00:16:15,440 --> 00:16:17,199
конечно, на данный момент существует более ограничительный доступ

483
00:16:17,199 --> 00:16:19,600
к gpt3. Это

484
00:16:19,600 --> 00:16:21,360
условные языковые модели, которые сильно

485
00:16:21,360 --> 00:16:23,199
отличаются от burt, и они могут быть

486
00:16:23,199 --> 00:16:25,759
лучше, чем сжигание, для таких вещей, как действительно

487
00:16:25,759 --> 00:16:28,720
условная генерация языка.

488
00:16:28,720 --> 00:16:30,720
xlnet - это попытка внести гораздо

489
00:16:30,720 --> 00:16:33,120
больше контекста в эти  моделей

490
00:16:33,120 --> 00:16:35,519
это означает сверхдлинный преобразователь, поэтому,

491
00:16:35,519 --> 00:16:37,199
если вам нужно обрабатывать длинные последовательности,

492
00:16:37,199 --> 00:16:39,040
это может быть  хороший выбор, и это

493
00:16:39,040 --> 00:16:40,639
также попытка привнести некоторые

494
00:16:40,639 --> 00:16:43,040
преимущества условных языковых моделей

495
00:16:43,040 --> 00:16:45,040
в режим, который является более двунаправленным,

496
00:16:45,040 --> 00:16:47,279
так как burt is

497
00:16:47,279 --> 00:16:49,440
t5 — еще один условный языковой режим,

498
00:16:49,440 --> 00:16:51,759
как и bart, эти модели могут быть лучшим

499
00:16:51,759 --> 00:16:53,360
выбором для вас, если  вам нужно на самом деле

500
00:16:53,360 --> 00:16:55,279
сгенерировать язык, где я думаю, что

501
00:16:55,279 --> 00:16:57,120
стандартная мудрость заключается в том, что такие модели, как

502
00:16:57,120 --> 00:16:59,360
Берт и Роберта, лучше, если вам просто

503
00:16:59,360 --> 00:17:01,519
нужны хорошие представления для точной

504
00:17:01,519 --> 00:17:03,440
настройки проблемы классификации, например

505
00:17:03,440 --> 00:17:05,199


506
00:17:05,199 --> 00:17:07,520
, каждый день будет появляться больше моделей, и я

507
00:17:07,520 --> 00:17:09,199
думаю, что стоит попытаться остаться  быть в

508
00:17:09,199 --> 00:17:11,039
курсе различных событий в

509
00:17:11,039 --> 00:17:13,199
этой области, потому что это, вероятно,

510
00:17:13,199 --> 00:17:17,319
только верхушка айсберга здесь

511
00:17:19,679 --> 00:17:21,760
вы

