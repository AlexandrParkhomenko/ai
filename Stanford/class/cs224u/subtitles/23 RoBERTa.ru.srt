1
00:00:05,120 --> 00:00:06,799
приветствую всех, это четвертая часть

2
00:00:06,799 --> 00:00:08,320
нашей серии о контекстных представлениях слов,

3
00:00:08,320 --> 00:00:09,679
мы собираемся

4
00:00:09,679 --> 00:00:11,920
говорить о надежно оптимизированном подходе bert, также

5
00:00:11,920 --> 00:00:15,120
известном как roberta,

6
00:00:15,120 --> 00:00:16,480
так что помните, что я закончил сожженный

7
00:00:16,480 --> 00:00:18,160
скринкаст, перечислив некоторые известные

8
00:00:18,160 --> 00:00:20,320
ограничения модели burp, большинство из

9
00:00:20,320 --> 00:00:22,000
которых были  идентифицированные

10
00:00:22,000 --> 00:00:23,920
самими авторами оригинального bert,

11
00:00:23,920 --> 00:00:26,000
и первое место в списке было просто о том, что,

12
00:00:26,000 --> 00:00:27,840
хотя исходная статья о burp

13
00:00:27,840 --> 00:00:30,240
хорошо справляется с исследованием абляции их

14
00:00:30,240 --> 00:00:31,840
системы и различных

15
00:00:31,840 --> 00:00:34,320
вариантов оптимизации, здесь есть очень большой

16
00:00:34,320 --> 00:00:36,320
ландшафт идей, и большая часть его осталась

17
00:00:36,320 --> 00:00:38,879
неисследованной в  в исходной статье, по

18
00:00:38,879 --> 00:00:40,800
сути, то, что сделала команда roberta, — это

19
00:00:40,800 --> 00:00:42,960
более широко изучить эту область, которая

20
00:00:42,960 --> 00:00:45,039
является надежно оптимизированной частью

21
00:00:45,039 --> 00:00:47,039
roberta,

22
00:00:47,039 --> 00:00:48,399
поэтому то, что я сделал для этого слайда, —

23
00:00:48,399 --> 00:00:50,079
перечислил то, что я считаю центральными

24
00:00:50,079 --> 00:00:52,160
различиями между bert и roberta и

25
00:00:52,160 --> 00:00:53,760
я дополню это некоторыми доказательствами

26
00:00:53,760 --> 00:00:55,920
из статьи Роберты во втором,

27
00:00:55,920 --> 00:00:57,360
сначала давайте рассмотрим основные

28
00:00:57,360 --> 00:00:59,280
различия, начавшиеся  Разбираясь с этим вопросом

29
00:00:59,280 --> 00:01:02,000
о статической и динамической маскировке,

30
00:01:02,000 --> 00:01:04,159
поэтому для исходной бумаги для отрыжки

31
00:01:04,159 --> 00:01:06,560
они создали четыре копии своего набора данных,

32
00:01:06,560 --> 00:01:07,280


33
00:01:07,280 --> 00:01:09,119
каждая с разной маскировкой, а затем

34
00:01:09,119 --> 00:01:11,439
эти четыре копии неоднократно использовались в

35
00:01:11,439 --> 00:01:13,680
течение эпох обучения,

36
00:01:13,680 --> 00:01:15,360
команда Роберты интуитивно догадалась, что

37
00:01:15,360 --> 00:01:16,640
это  было бы полезно внести некоторое

38
00:01:16,640 --> 00:01:19,040
разнообразие в этот процесс обучения, поэтому

39
00:01:19,040 --> 00:01:21,119
они перешли к другому крайнему динамическому

40
00:01:21,119 --> 00:01:23,280
маскированию, каждый отдельный пример, когда он

41
00:01:23,280 --> 00:01:25,200
представлен модели, маскируется

42
00:01:25,200 --> 00:01:27,200
потенциально другим способом с помощью некоторой

43
00:01:27,200 --> 00:01:29,840
случайной функции,

44
00:01:29,840 --> 00:01:31,200
существуют также различия в том, как

45
00:01:31,200 --> 00:01:33,360
примеры представляются для  модель, таким образом,

46
00:01:33,360 --> 00:01:35,439
представляла два соединенных сегмента документа,

47
00:01:35,439 --> 00:01:37,680
что было крайне важно для ее следующей

48
00:01:37,680 --> 00:01:39,759
задачи прогнозирования предложения, тогда как для

49
00:01:39,759 --> 00:01:41,280
roberta мы просто собираемся иметь

50
00:01:41,280 --> 00:01:43,520
последовательности предложений, которые представляют собой пары, которые

51
00:01:43,520 --> 00:01:46,799
могут даже пересекать границы документа, в

52
00:01:46,799 --> 00:01:49,040
то время как у bert это было в качестве одной из его

53
00:01:49,040 --> 00:01:51,040
центральных частей.

54
00:01:51,040 --> 00:01:52,799
задача

55
00:01:52,799 --> 00:01:54,960
предсказания предложения Роберта просто опускает это как часть

56
00:01:54,960 --> 00:01:56,880
цели здесь, что просто  Улучшает

57
00:01:56,880 --> 00:01:58,960
представление примеров, а также

58
00:01:58,960 --> 00:02:01,439
упрощает задачу моделирования. Теперь

59
00:02:01,439 --> 00:02:03,759
Роберта просто использует цель моделирования массового языка.

60
00:02:03,759 --> 00:02:06,719


61
00:02:07,040 --> 00:02:08,720
Кроме того, внесены изменения в

62
00:02:08,720 --> 00:02:10,800
размер обучающих пакетов, поэтому для

63
00:02:10,800 --> 00:02:13,920
Берта размер пакета составлял 256 примеров.

64
00:02:13,920 --> 00:02:16,800
до 2

65
00:02:16,800 --> 00:02:18,160
000. Существуют различия, когда дело доходит до

66
00:02:18,160 --> 00:02:20,160
токенизации, поэтому, как мы видели, Берт использует

67
00:02:20,160 --> 00:02:21,840
этот очень интересный

68
00:02:21,840 --> 00:02:23,840
подход к токенизации фрагментов слов, который смешивает некоторые

69
00:02:23,840 --> 00:02:26,879
фрагменты слов с некоторыми целыми словами.

70
00:02:26,879 --> 00:02:28,720
Роберта упростил это до

71
00:02:28,720 --> 00:02:31,360
кодирования пар байтов на уровне символов,

72
00:02:31,360 --> 00:02:33,360
которое  думаю, что интуитивно приводит к гораздо большему количеству слов,

73
00:02:33,360 --> 00:02:34,840


74
00:02:34,840 --> 00:02:36,640


75
00:02:36,640 --> 00:02:38,400
есть также различия в том, как

76
00:02:38,400 --> 00:02:40,640
данные обучались модели, поэтому берт

77
00:02:40,640 --> 00:02:42,640
обучался на существенном корпусе

78
00:02:42,640 --> 00:02:44,800
корпус книг плюс английская википедия - это

79
00:02:44,800 --> 00:02:47,360
много данных, действительно, роберта снова провернула

80
00:02:47,360 --> 00:02:49,120
это, даже дальше они обучались на

81
00:02:49,120 --> 00:02:52,319
корпус книг, корпус новостей cc, корпус open

82
00:02:52,319 --> 00:02:54,879
webtex и корпус историй,

83
00:02:54,879 --> 00:02:56,879
существенное увеличение объема

84
00:02:56,879 --> 00:02:59,519
торгов.  данных

85
00:02:59,519 --> 00:03:00,959
также есть различия в

86
00:03:00,959 --> 00:03:02,800
количестве шагов обучения, и здесь есть тонкость,

87
00:03:02,800 --> 00:03:04,800
поэтому для сгоревшей модели она изначально была

88
00:03:04,800 --> 00:03:07,360
обучена на 1 миллионе шагов,

89
00:03:07,360 --> 00:03:10,080
модель Роберты была обучена на 500 000

90
00:03:10,080 --> 00:03:12,560
шагов, что звучит как меньшее количество шагов, но в

91
00:03:12,560 --> 00:03:14,800
целом это значительно больше

92
00:03:14,800 --> 00:03:16,480
обучения  в силу того факта, что

93
00:03:16,480 --> 00:03:17,519


94
00:03:17,519 --> 00:03:19,680
размеры тренировочных пакетов для roberta намного больше,

95
00:03:19,680 --> 00:03:23,120
чем для bert,

96
00:03:23,120 --> 00:03:24,799
и, наконец, у авторов оригинального bert

97
00:03:24,799 --> 00:03:26,560
была интуиция, которая была бы полезна для

98
00:03:26,560 --> 00:03:28,720
того, чтобы процесс оптимизации

99
00:03:28,720 --> 00:03:31,280
сначала тренировался только на коротких последовательностях,

100
00:03:31,280 --> 00:03:33,200
команда roberta  отказались от этой идеи, и

101
00:03:33,200 --> 00:03:34,720
они тренируются на полноразмерных последовательностях на

102
00:03:34,720 --> 00:03:36,000
протяжении всего жизненного цикла

103
00:03:36,000 --> 00:03:37,760
оптимизации.

104
00:03:37,760 --> 00:03:39,280
Есть некоторые дополнительные различия,

105
00:03:39,280 --> 00:03:41,040
связанные с оптимизатором и представлением данных.

106
00:03:41,040 --> 00:03:42,720
Я собираюсь отложить их в

107
00:03:42,720 --> 00:03:44,720
сторону, если вам нужны подробности.

108
00:03:44,720 --> 00:03:47,760
документ,

109
00:03:47,760 --> 00:03:48,879
так что давайте рассмотрим немного

110
00:03:48,879 --> 00:03:50,879
доказательств этих различных вариантов,

111
00:03:50,879 --> 00:03:52,480
начиная с вопроса о динамическом

112
00:03:52,480 --> 00:03:54,400
и статическом  маскирование,

113
00:03:54,400 --> 00:03:55,840
так что это основное доказательство того, что они

114
00:03:55,840 --> 00:03:58,840
используют три тестовых отряда mnli и sst2,

115
00:03:58,840 --> 00:04:01,519
и вы можете видеть, что более или менее

116
00:04:01,519 --> 00:04:03,439
по всем направлениям динамическая маскировка

117
00:04:03,439 --> 00:04:06,799
лучше не намного, но динамическая маскировка

118
00:04:06,799 --> 00:04:09,280
также имеет смысл для этой интуиции, которую

119
00:04:09,280 --> 00:04:10,560
вы знаете, птица  данные

120
00:04:10,560 --> 00:04:12,480
неэффективны, мы можем маскировать только небольшое

121
00:04:12,480 --> 00:04:14,720
количество токенов, и кажется, что было

122
00:04:14,720 --> 00:04:16,399
бы полезно внести в них много

123
00:04:16,399 --> 00:04:18,160
разнообразия, чтобы множество

124
00:04:18,160 --> 00:04:20,238
разных токенов маскировалось

125
00:04:20,238 --> 00:04:22,000
по мере прохождения процесса обучения, но

126
00:04:22,000 --> 00:04:23,280
выбор  здесь, конечно, поддерживается

127
00:04:23,280 --> 00:04:24,560
численно, я думаю, что

128
00:04:24,560 --> 00:04:27,440


129
00:04:27,520 --> 00:04:29,120
этот боковой слайд в этой таблице

130
00:04:29,120 --> 00:04:31,040
резюмирует выбор того, как

131
00:04:31,040 --> 00:04:33,040
представлять примеры в модели, и

132
00:04:33,040 --> 00:04:34,960
это также немного тонко, поэтому

133
00:04:34,960 --> 00:04:37,680
численно подход doc-предложений

134
00:04:37,680 --> 00:04:39,280
был лучшим, и это был подход, в котором

135
00:04:39,280 --> 00:04:41,120
они  только два они взяли непрерывные

136
00:04:41,120 --> 00:04:43,280
предложения из документов, но

137
00:04:43,280 --> 00:04:45,040
обработали границу документа как своего рода

138
00:04:45,040 --> 00:04:47,520
жесткую границу, которая численно лучше в

139
00:04:47,520 --> 00:04:49,440
соответствии с результатами теста.  lts, но на

140
00:04:49,440 --> 00:04:51,199
самом деле они решили использовать

141
00:04:51,199 --> 00:04:53,120
подход полных предложений, и

142
00:04:53,120 --> 00:04:55,600
причина этого в том, что не соблюдаются

143
00:04:55,600 --> 00:04:58,000
границы документа, легче создавать

144
00:04:58,000 --> 00:05:00,479
множество пакетов точно такого же размера,

145
00:05:00,479 --> 00:05:02,160
что приводит к всевозможным выгодам, когда

146
00:05:02,160 --> 00:05:04,160
вы думаете об оптимизации большого  модель

147
00:05:04,160 --> 00:05:06,400
подобна этой, поэтому в основном они решили, что

148
00:05:06,400 --> 00:05:09,520
эти преимущества компенсируют немного более низкую

149
00:05:09,520 --> 00:05:11,440
производительность полных предложений по

150
00:05:11,440 --> 00:05:13,520
сравнению с предложениями doc, и поэтому

151
00:05:13,520 --> 00:05:17,199
это стало их основным подходом.

152
00:05:17,280 --> 00:05:19,440


153
00:05:19,440 --> 00:05:21,360


154
00:05:21,360 --> 00:05:23,759
256, который был

155
00:05:23,759 --> 00:05:26,240
оригинальным bert 2k и 8k

156
00:05:26,240 --> 00:05:28,000
и 2k выглядит как сладкое пятно в

157
00:05:28,000 --> 00:05:31,120
соответствии с mli sst2 и такого рода

158
00:05:31,120 --> 00:05:33,199
значением псевдо недоумения, которое вы получаете

159
00:05:33,199 --> 00:05:35,039
из двунаправленных моделей, таких как bert и

160
00:05:35,039 --> 00:05:36,160
roberta,

161
00:05:36,160 --> 00:05:37,919
так что это явный аргумент, и затем,

162
00:05:37,919 --> 00:05:39,280
наконец, когда мы  подойди к тому

163
00:05:39,280 --> 00:05:41,680
количеству тренировок, которое мы делаем,

164
00:05:41,680 --> 00:05:44,240
урок здесь, по-видимому, чем больше, тем лучше

165
00:05:44,240 --> 00:05:45,680
в верхней части этой таблицы, здесь у нас есть

166
00:05:45,680 --> 00:05:47,360
некоторые сравнения с  модель roberta, указывающая на 500 000,

167
00:05:47,360 --> 00:05:50,320
является лучшей, и

168
00:05:50,320 --> 00:05:51,759
я просто напомню вам, что в

169
00:05:51,759 --> 00:05:54,240
целом это значительно больше обучения, чем

170
00:05:54,240 --> 00:05:56,080
было сделано за один миллион шагов с bert,

171
00:05:56,080 --> 00:05:58,240
в силу того факта, что наши

172
00:05:58,240 --> 00:06:02,400
размеры пакетов для roberta намного больше

173
00:06:02,479 --> 00:06:03,919
при закрытии i.  просто хочу сказать, что

174
00:06:03,919 --> 00:06:06,720
Роберта тоже изучила только небольшую

175
00:06:06,720 --> 00:06:08,720
часть потенциальных вариантов дизайна, которые мы

176
00:06:08,720 --> 00:06:11,520
могли бы сделать в этом большом ландшафте,

177
00:06:11,520 --> 00:06:12,960
если вы хотите услышать еще больше

178
00:06:12,960 --> 00:06:14,560
о том, что мы знаем и что мы думаем, что

179
00:06:14,560 --> 00:06:16,800
знаем о таких моделях, как Берт и Роберта

180
00:06:16,800 --> 00:06:18,720
я  настоятельно рекомендую эту статью, называемую

181
00:06:18,720 --> 00:06:21,280
учебником по виртуологии, в которой содержится много

182
00:06:21,280 --> 00:06:24,160
дополнительной мудрости, идей и идей

183
00:06:24,160 --> 00:06:27,720
об этих моделях.

