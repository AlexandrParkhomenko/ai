1
00:00:05,040 --> 00:00:08,000
приветствую всех, это третья часть

2
00:00:06,559 --> 00:00:09,279
нашей серии о распределенных представлениях слов

3
00:00:09,279 --> 00:00:13,839
поговорить о методах сравнения векторов

4
00:00:12,240 --> 00:00:15,599
чтобы попытаться сделать это обсуждение довольно

5
00:00:13,839 --> 00:00:18,079
интуитивным. Я собираюсь обосновать вещи в

6
00:00:18,079 --> 00:00:22,159
очень маленькая модель векторного пространства, у нас есть

7
00:00:22,160 --> 00:00:25,920
и вы можете себе представить, что мы измерили

8
00:00:23,679 --> 00:00:28,079
два измерения dx и dy, вы могли бы думать

9
00:00:25,920 --> 00:00:29,359
о них как о документах, если бы вы хотели

10
00:00:28,079 --> 00:00:31,839
есть две точки зрения, которые вы

11
00:00:29,359 --> 00:00:34,079
могли бы использовать для этой модели векторного пространства.

12
00:00:31,839 --> 00:00:35,200
во-первых, просто на уровне исходной

13
00:00:35,200 --> 00:00:38,879
b и c кажутся объединенными, они

14
00:00:36,960 --> 00:00:39,920
часто встречаются как в измерениях x, так и в y

15
00:00:39,920 --> 00:00:44,879
тогда как a сравнительно нечасто встречается в

16
00:00:44,878 --> 00:00:49,039
, это первая точка зрения вторая

17
00:00:46,878 --> 00:00:50,960
точка зрения, хотя и более тонкая, вы

18
00:00:49,039 --> 00:00:54,000
можете просто  обратите внимание, что если мы как бы

19
00:00:55,920 --> 00:00:59,760
самом деле это a и b, которые объединены, потому что

20
00:00:57,679 --> 00:01:01,840
они оба имеют смещение в некотором смысле f

21
00:01:01,840 --> 00:01:06,079
то время как для сравнения c имеет смещение

22
00:01:07,920 --> 00:01:11,760
обе эти точки зрения, которые мы

23
00:01:09,519 --> 00:01:13,920
могли бы захотеть зафиксировать, и разные

24
00:01:11,760 --> 00:01:16,478
понятия расстояния будут ключом к тому

25
00:01:16,478 --> 00:01:20,560
еще один предварительный, я думаю, это

26
00:01:18,478 --> 00:01:22,400
очень интуитивно понятно изображать эти векторные

27
00:01:20,560 --> 00:01:24,000
пространства, и только в двух измерениях, что

28
00:01:22,400 --> 00:01:26,719
довольно легко, вы можете представить, что

29
00:01:30,719 --> 00:01:35,039
отдельные точки в это  плоскость, и затем

30
00:01:33,118 --> 00:01:37,280
вы можете графически увидеть, что b и c

31
00:01:35,040 --> 00:01:39,360
довольно близко друг к другу, а a немного

32
00:01:42,799 --> 00:01:46,240
давайте начнем с евклидова расстояния очень

33
00:01:44,719 --> 00:01:48,239
распространенное понятие расстояния в этих

34
00:01:46,239 --> 00:01:50,239
пространствах и довольно интуитивно

35
00:01:48,239 --> 00:01:52,239
мы можем измерить евклидово расстояние  расстояние

36
00:01:50,239 --> 00:01:55,118
между векторами u и v, если они

37
00:01:52,239 --> 00:01:56,239
имеют одинаковую размерность n, просто вычислив

38
00:01:56,239 --> 00:02:00,718
сумму квадратов элементов y разностей

39
00:01:58,718 --> 00:02:02,798
абсолютных разностей, а затем  возьмем

40
00:02:02,799 --> 00:02:06,240
, это математика, давайте посмотрим на это

41
00:02:04,478 --> 00:02:07,679
с точки зрения этого пространства, поэтому здесь у нас есть

42
00:02:07,680 --> 00:02:11,760
изображенное графически ab и c, а

43
00:02:14,080 --> 00:02:17,680
аннотировал полными вычислениями, но

44
00:02:17,680 --> 00:02:21,439
измеряем длину этих линий, самый

45
00:02:19,039 --> 00:02:23,598
прямой путь между этими точками в нашем

46
00:02:23,598 --> 00:02:26,878
и вы можете видеть, что евклидово

47
00:02:26,878 --> 00:02:31,199
перспективу, которую мы взяли на векторное

48
00:02:28,560 --> 00:02:36,840
пространство, которое объединяет часто встречающиеся элементы b

49
00:02:38,878 --> 00:02:41,679
расстоянию, которое будет вести себя совершенно по-

50
00:02:40,159 --> 00:02:43,039
другому, давайте поговорим о нормализации длины с

51
00:02:43,039 --> 00:02:47,598
учетом вектора u размерности n, l2

52
00:02:47,598 --> 00:02:52,719
представляет собой сумму квадратов значений в этой

53
00:02:50,318 --> 00:02:55,199
матрице, и тогда мы  возьмите квадратный корень

54
00:02:52,719 --> 00:02:57,199
который является нашей величиной нормализации

55
00:02:55,199 --> 00:02:59,759
а затем фактическая нормализация

56
00:02:57,199 --> 00:03:02,399
этого исходного вектора u включает в себя взятие

57
00:02:59,759 --> 00:03:05,759
каждого из его e  элементы и разделив его

58
00:03:02,400 --> 00:03:07,439
на это фиксированное количество рангов l2

59
00:03:05,759 --> 00:03:09,199
давайте посмотрим, что происходит с нашим

60
00:03:07,439 --> 00:03:11,840
небольшим иллюстративным примером слева

61
00:03:09,199 --> 00:03:13,359
здесь у меня есть исходная матрица счета

62
00:03:13,360 --> 00:03:17,599
задал длину l2 как количество

63
00:03:15,759 --> 00:03:19,518
а затем, когда  мы берем это количество и

64
00:03:17,598 --> 00:03:21,839
делим каждое из значений в этом

65
00:03:19,519 --> 00:03:23,120
векторе, чтобы получить его норму l2

66
00:03:21,840 --> 00:03:25,280
вы можете видеть, что мы сделали что-то

67
00:03:23,120 --> 00:03:27,759
важное с пространством, поэтому они все как

68
00:03:25,280 --> 00:03:30,080
бы объединены в одном масштабе здесь

69
00:03:27,759 --> 00:03:32,000
а a и b равны  теперь близко друг к другу

70
00:03:30,080 --> 00:03:34,239
тогда как b и c сравнительно далеко

71
00:03:32,000 --> 00:03:36,158
друг от друга, так что это отражает вторую

72
00:03:34,239 --> 00:03:38,319
перспективу, которую мы взяли на матрицу

73
00:03:41,280 --> 00:03:45,759
шага нормализации, и если мы измерили

74
00:03:43,439 --> 00:03:47,359
евклидово расстояние  в этом пространстве

75
00:03:59,360 --> 00:04:03,280
приблизительное расстояние, как вы увидите, между двумя

76
00:04:01,519 --> 00:04:04,640
векторами u и v размерности общего

77
00:04:04,639 --> 00:04:08,639
этот расчет состоит из двух частей

78
00:04:06,719 --> 00:04:10,959
это расчет подобия косинусного

79
00:04:10,959 --> 00:04:15,360
произведение двух векторов, деленное на

80
00:04:15,360 --> 00:04:18,720
а затем, чтобы получить что-то вроде

81
00:04:16,639 --> 00:04:20,959
расстояния, мы просто берем единицу и снова

82
00:04:20,959 --> 00:04:24,959
Давайте заземлим это в нашем примере.

83
00:04:22,800 --> 00:04:26,800
Здесь у нас есть исходная модель векторного пространства счета

84
00:04:26,800 --> 00:04:32,240
и то, что мы делаем с косинусным расстоянием, по

85
00:04:32,240 --> 00:04:35,680
этими линиями, которые я  я нарисовал из этой

86
00:04:35,680 --> 00:04:40,400
и поэтому вы можете видеть, что косинусное

87
00:04:38,079 --> 00:04:42,719
расстояние отражает тот факт, что a и b

88
00:04:40,399 --> 00:04:44,959
близки друг к другу, измеренные этим углом

89
00:04:42,720 --> 00:04:45,919
тогда как b и c сравнительно далеко

90
00:04:45,918 --> 00:04:50,399
поэтому снова с помощью косинуса мы абстрагируемся

91
00:04:50,399 --> 00:04:54,959
и вводить это абстрактное понятие

92
00:04:52,639 --> 00:04:57,840
подобия, которое связывает a и b

93
00:04:58,639 --> 00:05:03,038
другой точкой зрения, которую вы могли бы принять

94
00:05:00,240 --> 00:05:04,800
это просто заметить, что  если мы сначала

95
00:05:06,399 --> 00:05:11,839
через норму l2, а затем применим

96
00:05:09,120 --> 00:05:13,519
вычисление косинуса, мы изменим пространство

97
00:05:11,839 --> 00:05:15,758
как я показал вам ранее, так что все они находятся

98
00:05:13,519 --> 00:05:18,079
здесь как бы на единичной сфере

99
00:05:15,759 --> 00:05:20,000
и обратите внимание, что фактические значения, которые

100
00:05:18,079 --> 00:05:22,000
мы получаем, равны  то же самое

101
00:05:20,000 --> 00:05:24,319
независимо от того, сделали ли мы этот шаг нормирования l2 или 

102
00:05:24,319 --> 00:05:29,120
встраивает эффекты нормирования l2

103
00:05:26,720 --> 00:05:32,000
непосредственно в эту нормализацию здесь

104
00:05:33,519 --> 00:05:36,639
есть несколько других методов, о которых мы

105
00:05:35,038 --> 00:05:38,159
могли бы подумать, или классы методов, которые

106
00:05:36,639 --> 00:05:39,600
я думаю, мы не делаем  не нужно отвлекаться

107
00:05:39,600 --> 00:05:42,879
упомяну их на случай, если они появятся, и

108
00:05:40,959 --> 00:05:44,319
вы читаете наше исследование, первый

109
00:05:44,319 --> 00:05:48,560
методами, основанными на сопоставлении, они все

110
00:05:45,918 --> 00:05:50,560
основаны на  этот коэффициент соответствия

111
00:05:48,560 --> 00:05:51,680
а затем жаккардовые кости и перекрытие — это

112
00:05:50,560 --> 00:05:52,399
термины, которые вы можете встретить в

113
00:05:52,399 --> 00:05:55,839
Они часто определяются только для двоичных

114
00:05:55,839 --> 00:06:00,318
обобщения для векторов с действительными значениями

115
00:05:58,079 --> 00:06:01,680
которые мы рассматриваем.  Обсуждение

116
00:06:00,319 --> 00:06:03,360
и другой класс методов, которые вы

117
00:06:01,680 --> 00:06:05,120
можете встретить, являются вероятностными

118
00:06:03,360 --> 00:06:07,439
методами, которые, как правило, основаны на

119
00:06:07,439 --> 00:06:11,519
kl. Расхождение kl по сути является способом

120
00:06:09,519 --> 00:06:13,918
измерения расстояния между двумя

121
00:06:15,038 --> 00:06:20,079
, если быть более точным, от эталона.

122
00:06:17,279 --> 00:06:21,918
распределение p к некоторому другому

123
00:06:21,918 --> 00:06:27,038
um, и оно имеет симметричные понятия

124
00:06:24,399 --> 00:06:28,799
симметричного kl и расстояния Дженсена-Шеннона

125
00:06:27,038 --> 00:06:31,279
которое является еще одним симметричным понятием

126
00:06:28,800 --> 00:06:33,600
основанным на расхождении kl, опять же, это

127
00:06:31,279 --> 00:06:35,279
вероятно, подходящие меры для выбора

128
00:06:35,279 --> 00:06:41,038
думаете, правильно продуманы  как

129
00:06:41,759 --> 00:06:45,199
теперь я сослался на тот факт, что

130
00:06:43,360 --> 00:06:47,360
мера косинусного расстояния, которую я дал вам

131
00:06:45,199 --> 00:06:49,598
ранее, не совсем то, что называется

132
00:06:47,360 --> 00:06:50,879
правильной метрикой расстояния, позвольте мне

133
00:06:50,879 --> 00:06:55,360
чтобы квалифицировать как правильную метрику расстояния

134
00:06:53,680 --> 00:06:57,360
метод сравнения векторов  должен иметь

135
00:06:57,360 --> 00:07:02,400
симметричным, то есть он должен давать

136
00:06:59,439 --> 00:07:05,598
одинаковое значение  для xy, как и для yx

137
00:07:02,399 --> 00:07:06,478
kl, расхождение на самом деле не соответствует первому

138
00:07:06,478 --> 00:07:09,758
которое необходимо присвоить нулю идентичным

139
00:07:09,759 --> 00:07:13,840
и, что особенно важно, оно должно удовлетворять так

140
00:07:11,839 --> 00:07:17,198
называемому неравенству треугольника, которое

141
00:07:13,839 --> 00:07:19,598
говорит, что расстояние между x и

142
00:07:19,598 --> 00:07:25,439
между x и y, а затем косинусным расстоянием y и z

143
00:07:25,439 --> 00:07:29,839
ранее, не удовлетворяет неравенству треугольника

144
00:07:29,839 --> 00:07:34,159
пример, который делает интуитивно понятным, просто

145
00:07:32,000 --> 00:07:36,399
случается, что это расстояние здесь на

146
00:07:34,160 --> 00:07:37,199
самом деле больше, чем эти два значения

147
00:07:37,199 --> 00:07:40,960
что является ошибкой утверждения

148
00:07:40,959 --> 00:07:44,878
теперь это относительно легко исправить

149
00:07:43,360 --> 00:07:47,038
но это также своего рода полезная

150
00:07:44,879 --> 00:07:48,400
основа для всех различных вариантов выбора

151
00:07:48,399 --> 00:07:52,560
из всех вариантов сравнения векторов.

152
00:07:50,800 --> 00:07:55,360
предположим, что мы решили отдать предпочтение тем

153
00:07:52,560 --> 00:07:56,959
которые  считаются метриками истинного расстояния

154
00:07:55,360 --> 00:07:59,280
то это, по крайней мере, подтолкнет нас к тому, чтобы

155
00:07:56,959 --> 00:08:01,918
отдать предпочтение жаккардовому евклидову расстоянию только

156
00:08:01,918 --> 00:08:05,839
расстоянию Дженсена-Шеннона, если мы  мы

157
00:08:03,519 --> 00:08:07,758
говорили о вероятностных пространствах, и

158
00:08:05,839 --> 00:08:09,918
мы хотели бы дополнительно изменить определение

159
00:08:07,759 --> 00:08:11,360
косинусного расстояния на более точное

160
00:08:11,360 --> 00:08:15,120
которое удовлетворяет неравенству треугольника

161
00:08:20,319 --> 00:08:25,840
совпадающие кости жаккарда, перекрывающиеся, т. е.

162
00:08:23,519 --> 00:08:27,839
дивергенцию кале, и симметричную

163
00:08:25,839 --> 00:08:29,839
дивергенцию хвоста как те, которые не могут быть

164
00:08:27,839 --> 00:08:31,598
надлежащими метриками расстояния, и поэтому это

165
00:08:29,839 --> 00:08:33,838
может быть полезной основой для размышлений

166
00:08:31,598 --> 00:08:36,240
о выборе в этом пространстве. еще

167
00:08:33,839 --> 00:08:38,159
один момент в связи с этим, это

168
00:08:36,240 --> 00:08:40,320
, очевидно, более  более сложный расчет

169
00:08:38,158 --> 00:08:42,158
чем тот, который я дал вам раньше, и

170
00:08:40,320 --> 00:08:44,720
по правде говоря, это, вероятно, не стоит

171
00:08:44,720 --> 00:08:48,399
набора векторов, которые я выбрал из одной из

172
00:08:46,559 --> 00:08:50,399
наших моделей векторного пространства, и я

173
00:08:48,399 --> 00:08:52,879
сравнил неправильное косинусное расстояние

174
00:08:50,399 --> 00:08:55,278
которое я  показал вам раньше на оси x

175
00:08:52,879 --> 00:08:56,799
с правильной мерой косинусного расстояния

176
00:08:55,278 --> 00:08:58,879
которую я только что показал вам

177
00:08:58,879 --> 00:09:02,879
почти идеальна, поэтому  по сути

178
00:09:00,720 --> 00:09:05,200
нет никакой разницы между этими двумя

179
00:09:02,879 --> 00:09:06,720
разными способами измерения косинуса

180
00:09:05,200 --> 00:09:08,720
и я думаю, что они, вероятно, по

181
00:09:06,720 --> 00:09:10,560
существу идентичны до ранжирования

182
00:09:08,720 --> 00:09:12,639
которое часто является величиной, которая нас волнует

183
00:09:12,639 --> 00:09:16,159
поэтому, вероятно, придерживайтесь более простого и

184
00:09:14,240 --> 00:09:18,560
менее сложного  Моим советом был бы расчет

185
00:09:18,799 --> 00:09:22,879
давайте закончим некоторыми обобщениями

186
00:09:20,399 --> 00:09:24,958
и отношениями. Первый евклидов

187
00:09:22,879 --> 00:09:27,278
а также жаккард и кости с необработанными

188
00:09:24,958 --> 00:09:29,039
векторами счета будут иметь тенденцию отдавать предпочтение 

189
00:09:27,278 --> 00:09:31,679
по сравнению с другими шаблонами распределения, такими

190
00:09:29,039 --> 00:09:34,719
как более абстрактный, который я показал вам

191
00:09:31,679 --> 00:09:36,958
в нашем иллюстративном примере

192
00:09:34,720 --> 00:09:38,959
евклидова с l2  нормальные векторы

193
00:09:36,958 --> 00:09:41,278
эквивалентны косинусу, когда дело доходит до

194
00:09:38,958 --> 00:09:43,278
ранжирования, что означает, что если вы

195
00:09:41,278 --> 00:09:45,200
хотите использовать евклидово и сначала l2

196
00:09:43,278 --> 00:09:46,639
нормируете свои векторы, вы, вероятно, просто

197
00:09:45,200 --> 00:09:49,759
делаете что-то, что может быть просто

198
00:09:49,759 --> 00:09:53,519
а кости эквивалентны  что касается

199
00:09:51,278 --> 00:09:54,958
ранжирования, об этом нужно помнить

200
00:09:54,958 --> 00:09:58,639
фундаментальное  обратите внимание, что вы увидите

201
00:09:56,080 --> 00:10:00,560
повторяющиеся в этом разделе как нормирование l2, так

202
00:10:02,000 --> 00:10:06,879
распределения вероятностей из строк, они могут

203
00:10:04,320 --> 00:10:09,200
быть полезными шагами, как мы видели, но они

204
00:10:06,879 --> 00:10:10,879
могут скрыть различия в количестве или

205
00:10:09,200 --> 00:10:12,720
силе доказательств.  что у вас есть, что

206
00:10:10,879 --> 00:10:14,958
в свою очередь, может повлиять на

207
00:10:12,720 --> 00:10:17,839
надежность, например, косинусного

208
00:10:14,958 --> 00:10:19,278
нормуклидиана или дивергенции капусты

209
00:10:17,839 --> 00:10:20,800
эти недостатки могут быть устранены с

210
00:10:20,799 --> 00:10:25,278
вот суть в том, что есть ценная

211
00:10:22,958 --> 00:10:27,278
информация в необработанной частоте

212
00:10:25,278 --> 00:10:28,879
если мы абстрагируемся от нее  другая

213
00:10:27,278 --> 00:10:30,799
информация может выйти на поверхность

214
00:10:28,879 --> 00:10:32,879
но мы также можем потерять эту важную

215
00:10:32,879 --> 00:10:36,399
пространство таким образом, и может быть

216
00:10:34,480 --> 00:10:38,879
трудно сбалансировать эти конкурирующие

217
00:10:48,000 --> 00:10:52,879
нормирование длины ваших векторов и т. д., и это

218
00:10:50,399 --> 00:10:55,519
также  имеет эту функцию, называемую соседями

219
00:10:52,879 --> 00:10:58,320
в модуле vsm, она позволяет вам

220
00:10:55,519 --> 00:11:00,480
выбрать целевое слово и предоставить модель векторного 

221
00:11:00,480 --> 00:11:04,480
вам полный рейтинг всего словаря

222
00:11:02,879 --> 00:11:06,639
в этом векторном пространстве по отношению к

223
00:11:04,480 --> 00:11:08,959
вашему целевому слову, начиная с тех

224
00:11:06,639 --> 00:11:10,559
которые являются ближайшими, так что вот результаты

225
00:11:10,559 --> 00:11:15,599
косинусного расстояния в ячейке 12 и

226
00:11:12,799 --> 00:11:16,958
расстояния Джакарты и набухания в ячейке 13, и я

227
00:11:15,600 --> 00:11:18,320
просто хотел бы сказать, что эти

228
00:11:18,320 --> 00:11:22,800
интуитивными для меня, это не похоже, что

229
00:11:22,799 --> 00:11:27,199
действительно интересную семантическую информацию

230
00:11:29,440 --> 00:11:33,600
растягивать и сгибать наши модели векторного пространства

231
00:11:31,759 --> 00:11:35,360
и мы увидим, что мы увидим гораздо лучшие

232
00:11:33,600 --> 00:11:36,800
результаты для этих функций соседей и

233
00:11:36,799 --> 00:11:40,759
когда мы пройдемся по этому материалу

