1
00:00:05,120 --> 00:00:06,399
приветствую всех, это первый

2
00:00:06,399 --> 00:00:08,160
скринкаст в нашей серии о

3
00:00:08,160 --> 00:00:09,679
методах анализа в nlp, это один из моих

4
00:00:09,679 --> 00:00:11,200
любимых разделов в курсе, потому что

5
00:00:11,200 --> 00:00:12,960
он напрямую ориентирован на то,

6
00:00:12,960 --> 00:00:15,759
чтобы помочь вам сделать еще лучше окончательные проекты,

7
00:00:15,759 --> 00:00:17,279
теперь мы можем многое обсудить

8
00:00:17,279 --> 00:00:19,359
в рубрике методов анализа.  в NLP

9
00:00:19,359 --> 00:00:21,199
я выбрал четыре вещи,

10
00:00:21,199 --> 00:00:22,880
первые две из которых относятся к

11
00:00:22,880 --> 00:00:24,880
поведенческим оценкам, мы поговорим о

12
00:00:24,880 --> 00:00:26,560
состязательном тестировании, которое является очень

13
00:00:26,560 --> 00:00:28,720
гибким способом для вас выявить, что ваша

14
00:00:28,720 --> 00:00:31,599
система может иметь некоторые слабости или

15
00:00:31,599 --> 00:00:33,120
не может уловить какое-то лингвистическое

16
00:00:33,120 --> 00:00:35,920
явление в  очень систематически,

17
00:00:35,920 --> 00:00:37,760
и затем на этом этапе у нас также есть

18
00:00:37,760 --> 00:00:39,840
возможность выполнить ряд задач для

19
00:00:39,840 --> 00:00:42,000
обучения и тестирования противников, это

20
00:00:42,000 --> 00:00:44,160
будут большие наборы данных,

21
00:00:44,160 --> 00:00:46,640
полные примеров, которые, как мы знаем, сложны

22
00:00:46,640 --> 00:00:48,399
для современной архитектуры, поэтому для

23
00:00:48,399 --> 00:00:50,640
любой архитектуры вы  мы изучаем

24
00:00:50,640 --> 00:00:52,239
это, это будет шанс действительно провести стресс-

25
00:00:52,239 --> 00:00:54,559
тестирование этой архитектуры,

26
00:00:54,559 --> 00:00:55,760
а затем мы собираемся выйти за рамки

27
00:00:55,760 --> 00:00:57,440
поведенческих оценок.  o поговорим о

28
00:00:57,440 --> 00:00:59,359
том, что я назвал методами структурной оценки,

29
00:00:59,359 --> 00:01:00,960
и они включают в себя зондирование и

30
00:01:00,960 --> 00:01:02,719
атрибуцию признаков.

31
00:01:02,719 --> 00:01:04,080
Это методы, которые вы можете использовать,

32
00:01:04,080 --> 00:01:06,080
чтобы заглянуть внутрь вашей системы и

33
00:01:06,080 --> 00:01:07,840
понять, на что похожи ее скрытые

34
00:01:07,840 --> 00:01:09,920
представления и как эти

35
00:01:09,920 --> 00:01:11,680
представления влияют на

36
00:01:11,680 --> 00:01:14,560
прогнозы модели.

37
00:01:14,560 --> 00:01:16,720
мотивов для этого много,

38
00:01:16,720 --> 00:01:18,320
здесь всего несколько высокоуровневых, которые как

39
00:01:18,320 --> 00:01:20,000
бы ориентированы на проекты, во-

40
00:01:20,000 --> 00:01:22,159
первых, мы можем захотеть

41
00:01:22,159 --> 00:01:23,600
найти ограничения системы, которую вы

42
00:01:23,600 --> 00:01:25,040
разрабатываете, все наши системы имеют

43
00:01:25,040 --> 00:01:27,439
ограничения и найти их  всегда

44
00:01:27,439 --> 00:01:29,600
полезно с научной точки зрения,

45
00:01:29,600 --> 00:01:31,439
мы могли бы также захотеть

46
00:01:31,439 --> 00:01:33,200
лучше понять поведение вашей системы, каковы

47
00:01:33,200 --> 00:01:35,200
ее внутренние представления и

48
00:01:35,200 --> 00:01:36,799
как они влияют на ее окончательные

49
00:01:36,799 --> 00:01:38,560
прогнозы и ее общее поведение,

50
00:01:38,560 --> 00:01:40,960
что также просто невероятно полезно,

51
00:01:40,960 --> 00:01:42,640
и обе эти вещи могут способствовать

52
00:01:42,640 --> 00:01:44,640
простому достижению большего  надежные системы в

53
00:01:44,640 --> 00:01:46,560
той степени, в которой мы можем найти слабые места

54
00:01:46,560 --> 00:01:47,920
и понять

55
00:01:47,920 --> 00:01:50,159
поведение  возможно, мы можем предпринять

56
00:01:50,159 --> 00:01:53,520
шаги для создания еще более надежных систем,

57
00:01:53,520 --> 00:01:55,200
и, как я уже сказал, все это ориентировано

58
00:01:55,200 --> 00:01:56,560
на ваши окончательные проекты,

59
00:01:56,560 --> 00:01:58,320
методы, которые мы обсуждаем, являются

60
00:01:58,320 --> 00:02:00,399
мощными и простыми способами улучшить

61
00:02:00,399 --> 00:02:02,960
раздел анализа статьи.

62
00:02:02,960 --> 00:02:04,560
разделы анализа важны, но это  может быть

63
00:02:04,560 --> 00:02:06,240
трудно написать их, они кажутся очень

64
00:02:06,240 --> 00:02:08,318
открытыми, и часто очень неструктурированные

65
00:02:08,318 --> 00:02:10,639
люди говорят в общих чертах об

66
00:02:10,639 --> 00:02:12,480
анализе ошибок и т. д., но может

67
00:02:12,480 --> 00:02:14,239
быть трудно точно определить, что было

68
00:02:14,239 --> 00:02:16,319
бы продуктивно, я думаю, методы, о которых

69
00:02:16,319 --> 00:02:18,560
мы говорим  здесь очень

70
00:02:18,560 --> 00:02:20,319
широко применимы

71
00:02:20,319 --> 00:02:21,680
и могут привести к действительно продуктивным и

72
00:02:21,680 --> 00:02:24,000
богатым разделам

73
00:02:24,000 --> 00:02:25,680
анализа. Начнем с состязательного тестирования.

74
00:02:25,680 --> 00:02:27,120


75
00:02:27,120 --> 00:02:28,800


76
00:02:28,800 --> 00:02:30,879


77
00:02:30,879 --> 00:02:33,920
то, что

78
00:02:33,920 --> 00:02:35,440
они сделали, на самом деле лишь слегка

79
00:02:35,440 --> 00:02:36,879
состязательно, это просто своего рода

80
00:02:36,879 --> 00:02:38,800
вызов, и это обнажает некоторую нехватку

81
00:02:38,800 --> 00:02:41,760
систематичности в некоторых моделях nli, так что

82
00:02:41,760 --> 00:02:43,519
вот »  что они сделали они начали с

83
00:02:43,519 --> 00:02:45,680
снили примеров как маленькая девочка стоит на

84
00:02:45,680 --> 00:02:48,000
коленях в грязи плачет влечет за собой

85
00:02:48,000 --> 00:02:50,400
маленькая девочка очень грустная и они просто

86
00:02:50,400 --> 00:02:52,480
используют лексические ресурсы чтобы изменить

87
00:02:52,480 --> 00:02:54,480
гипотезу одним словом так что теперь она

88
00:02:54,480 --> 00:02:57,200
читается маленькая девочка очень несчастна

89
00:02:57,200 --> 00:02:59,519
мы ожидали бы, что система, которая действительно

90
00:02:59,519 --> 00:03:01,120
понимает рассуждения, связанные с

91
00:03:01,120 --> 00:03:03,360
этими примерами, продолжит предсказывать

92
00:03:03,360 --> 00:03:05,680
последствия во втором случае, потому что эти

93
00:03:05,680 --> 00:03:07,760
примеры примерно синонимичны, но

94
00:03:07,760 --> 00:03:09,519
они обнаружили, что системы часто

95
00:03:09,519 --> 00:03:11,440
начинают предсказывать противоречие, возможно,

96
00:03:11,440 --> 00:03:14,000
из-за отрицания, которое здесь происходит

97
00:03:14,000 --> 00:03:16,239
второй пример аналогичен, мы начинаем

98
00:03:16,239 --> 00:03:18,480
с примера sli: пожилая

99
00:03:18,480 --> 00:03:19,920
пара сидит возле ресторана,

100
00:03:19,920 --> 00:03:22,560
наслаждаясь вином, влечет за собой пару, пьющую

101
00:03:22,560 --> 00:03:24,799
вино, и здесь они просто заменили вино на

102
00:03:24,799 --> 00:03:27,360
шампанское. Мы ожидаем, что

103
00:03:27,360 --> 00:03:29,120
система, которая знала об этих лексических

104
00:03:29,120 --> 00:03:31,200
единицах и их  отношения

105
00:03:31,200 --> 00:03:33,280
в этом случае перевернутся к предсказанию нейтральных,

106
00:03:33,280 --> 00:03:35,200
но, как вы можете себе представить, системы

107
00:03:35,200 --> 00:03:37,280
продолжают предсказывать влечет за собой, потому что  у них

108
00:03:37,280 --> 00:03:39,840
есть лишь очень смутное представление о

109
00:03:39,840 --> 00:03:42,080
том, как вино и шампанское связаны

110
00:03:42,080 --> 00:03:44,400
друг с другом,

111
00:03:44,400 --> 00:03:46,159
вот таблица результатов, и помните,

112
00:03:46,159 --> 00:03:48,480
что это статья 2018 года, и они в основном

113
00:03:48,480 --> 00:03:50,480
тестируют здесь модели, которые мы можем

114
00:03:50,480 --> 00:03:52,879
рассматривать как предшественники трансформаторов,

115
00:03:52,879 --> 00:03:54,879
которые мы  были так сосредоточены, и

116
00:03:54,879 --> 00:03:56,720
картина очень ясна, эти модели

117
00:03:56,720 --> 00:03:59,120
хорошо справляются с тестовым набором snli от середины до высокого уровня

118
00:03:59,120 --> 00:04:01,519
80, но их производительность резко падает на

119
00:04:01,519 --> 00:04:03,760
этом новом состязательном тестовом наборе.

120
00:04:03,760 --> 00:04:05,599
Здесь есть два исключения:

121
00:04:05,599 --> 00:04:07,280
базовый уровень wordnet и архитектура kim,

122
00:04:07,280 --> 00:04:09,040
но это  важно отметить,

123
00:04:09,040 --> 00:04:11,840
что эти модели фактически имели

124
00:04:11,840 --> 00:04:13,439
прямой доступ в случае wordnet и

125
00:04:13,439 --> 00:04:15,360
косвенно в случае kim к

126
00:04:15,360 --> 00:04:17,600
лексическому ресурсу, который использовался для

127
00:04:17,600 --> 00:04:19,279
создания состязательного теста,

128
00:04:19,279 --> 00:04:20,959
и поэтому они не видят здесь такого большого

129
00:04:20,959 --> 00:04:23,600
падения производительности, но даже все же

130
00:04:23,600 --> 00:04:25,440
все эти цифры довольно скромные на

131
00:04:25,440 --> 00:04:26,800
данный момент,

132
00:04:26,800 --> 00:04:28,000
и я сказал вам, что это была

133
00:04:28,000 --> 00:04:29,919
интересная история, вот интересный

134
00:04:29,919 --> 00:04:32,720
поворот на этом этапе в 2021 году, который вы можете

135
00:04:32,720 --> 00:04:35,360
просто скачать.  ad roberta mnli это

136
00:04:35,360 --> 00:04:37,520
параметры roberta, точно настроенные на

137
00:04:37,520 --> 00:04:39,600
наборе данных с несколькими nli,

138
00:04:39,600 --> 00:04:41,520
и запустите этот состязательный тест, и

139
00:04:41,520 --> 00:04:43,120
вы обнаружите, что эта модель

140
00:04:43,120 --> 00:04:45,759
поразительно хорошо работает с ломаным

141
00:04:45,759 --> 00:04:47,759
набором данных nli, я бы сосредоточился на этих двух

142
00:04:47,759 --> 00:04:49,600
оценках f1 здесь для  два класса, где у

143
00:04:49,600 --> 00:04:51,680
нас много противоречий поддержки и

144
00:04:51,680 --> 00:04:54,080
следствий, числа выше 90,

145
00:04:54,080 --> 00:04:55,919
как и точность здесь, которая напрямую

146
00:04:55,919 --> 00:04:58,080
сопоставима с числами, о

147
00:04:58,080 --> 00:05:00,000
которых сообщил Глокнер. Удивительное

148
00:05:00,000 --> 00:05:02,000
достижение, напомним, что оригинальные

149
00:05:02,000 --> 00:05:04,240
примеры из гм состязательного теста

150
00:05:04,240 --> 00:05:07,199
взяты из snli  это мульти-нли, он

151
00:05:07,199 --> 00:05:09,120
не был разработан специально для решения этого

152
00:05:09,120 --> 00:05:11,039
состязательного теста, и, тем не менее,

153
00:05:11,039 --> 00:05:13,199
похоже, что Роберта систематически

154
00:05:13,199 --> 00:05:15,520
знает лексические отношения,

155
00:05:15,520 --> 00:05:18,240
задействованные и необходимые для решения этого

156
00:05:18,240 --> 00:05:20,880
состязательного теста, так что, возможно, реальный прогресс Маркова,

157
00:05:20,880 --> 00:05:23,280


158
00:05:23,280 --> 00:05:25,199
как я сказал, вы можете также для избранных

159
00:05:25,199 --> 00:05:26,639
задачи переходят в режим проведения

160
00:05:26,639 --> 00:05:29,280
состязательного обучения и тестирования, хм, вот

161
00:05:29,280 --> 00:05:30,960
случаи, которые я знаю, когда набор

162
00:05:30,960 --> 00:05:33,120
данных велик.  тьфу, чтобы поддержать обучение и

163
00:05:33,120 --> 00:05:35,440
тестирование на примерах, которые были созданы с

164
00:05:35,440 --> 00:05:38,000
помощью некоторых состязательных динамических

165
00:05:38,000 --> 00:05:39,680
рассуждений на основе здравого смысла,

166
00:05:39,680 --> 00:05:41,840
вопроса о выводе на естественном языке, отвечающего на чувства

167
00:05:41,840 --> 00:05:44,000
и ненавистнические высказывания, и, как я уже сказал, это

168
00:05:44,000 --> 00:05:46,160
действительно захватывающая возможность увидеть,

169
00:05:46,160 --> 00:05:48,639
насколько надежна ваша система, когда она подвергается

170
00:05:48,639 --> 00:05:50,639
воздействию примеров, которые  мы знаем, что это сложно

171
00:05:50,639 --> 00:05:52,800
для современных архитектур, потому что

172
00:05:52,800 --> 00:05:56,000
именно так были разработаны эти наборы данных.

173
00:05:56,000 --> 00:05:57,680
Теперь давайте перейдем к более поведенческому

174
00:05:57,680 --> 00:05:59,280
режиму, мы начнем с проверки

175
00:05:59,280 --> 00:06:01,120
внутренних представлений.

176
00:06:01,120 --> 00:06:02,880


177
00:06:02,880 --> 00:06:05,360


178
00:06:05,360 --> 00:06:08,000
чтобы

179
00:06:08,000 --> 00:06:10,000
показать, что эти скрытые представления

180
00:06:10,000 --> 00:06:12,400
латентно кодируют, это из классической

181
00:06:12,400 --> 00:06:15,199
статьи Яна Тенни в 2019 году,

182
00:06:15,199 --> 00:06:17,600
и то, что у нас есть по оси X, — это

183
00:06:17,600 --> 00:06:19,199
слой burt, начинающийся со

184
00:06:19,199 --> 00:06:21,360
слоя встраивания и идущий к 24, он сожжен

185
00:06:21,360 --> 00:06:23,680
большой, так что есть  24 слоя, и

186
00:06:23,680 --> 00:06:25,360
картина довольно поразительна, так как вы начинаете

187
00:06:25,360 --> 00:06:27,360
сверху и двигаетесь вниз, вы можете

188
00:06:27,360 --> 00:06:29,440
видите, что по мере того, как мы переходим от более синтаксических

189
00:06:29,440 --> 00:06:31,840
вещей к более дискурсивному семантическому

190
00:06:31,840 --> 00:06:34,479
содержанию, такому как совместная ссылка и

191
00:06:34,479 --> 00:06:37,039
извлечение отношений, вы обнаруживаете, что

192
00:06:37,039 --> 00:06:39,199
более высокие уровни модели отрыжки

193
00:06:39,199 --> 00:06:40,960
кодируют эту информацию латентно

194
00:06:40,960 --> 00:06:43,280
, что эти результаты исследования показывают

195
00:06:43,280 --> 00:06:46,000
на этой картине довольно поразительно.  посмотрите,

196
00:06:46,000 --> 00:06:48,319
что процесс предварительной подготовки в этом

197
00:06:48,319 --> 00:06:51,919
случае с Бертом латентно узнает

198
00:06:51,919 --> 00:06:55,120
о структурах языка,

199
00:06:55,120 --> 00:06:56,319
а затем мы, наконец, поговорим об

200
00:06:56,319 --> 00:06:58,400
атрибуции признаков, которая является еще одним шагом

201
00:06:58,400 --> 00:07:00,319
вперед в этом более интроспективном режиме,

202
00:07:00,319 --> 00:07:02,400
потому что здесь, как вы увидите, я думаю  мы

203
00:07:02,400 --> 00:07:05,199
можем получить действительно глубокую картину того, как

204
00:07:05,199 --> 00:07:07,759
отдельные функции и

205
00:07:07,759 --> 00:07:10,000
представления напрямую связаны с прогнозами модели.

206
00:07:10,000 --> 00:07:12,080
Что я сделал здесь, так это использовал

207
00:07:12,080 --> 00:07:13,840
модель интегрированных градиентов, которая

208
00:07:13,840 --> 00:07:16,080
является моделью, на которой мы сосредоточимся. я запустил ее

209
00:07:16,080 --> 00:07:17,759
на модели настроений и  Вы можете видеть

210
00:07:17,759 --> 00:07:19,520
здесь, что у нас есть истинная метка,

211
00:07:19,520 --> 00:07:21,520
предсказанная метка с вероятностью, а

212
00:07:21,520 --> 00:07:23,680
затем у нас есть важность уровня слова,

213
00:07:23,680 --> 00:07:25,599
измеренная интегрированными градиентами, где

214
00:07:25,599 --> 00:07:28,160
синий  означает, что это смещение в сторону положительных

215
00:07:28,160 --> 00:07:30,240
прогнозов, а красный цвет означает, что это смещение в

216
00:07:30,240 --> 00:07:32,160
сторону отрицательных прогнозов,

217
00:07:32,160 --> 00:07:33,759
и я выбрал пример, который, я

218
00:07:33,759 --> 00:07:35,440
думаю, является своего рода стресс-тестом модели,

219
00:07:35,440 --> 00:07:37,120
он немного состязательный, потому

220
00:07:37,120 --> 00:07:40,319
что все эти примеры включают в себя среднее

221
00:07:40,319 --> 00:07:42,720
в смысле хорошего  как в случае со средним яблочным

222
00:07:42,720 --> 00:07:45,199
пирогом, означающим вкусный или хороший,

223
00:07:45,199 --> 00:07:46,720
и вы можете видеть, что в целом

224
00:07:46,720 --> 00:07:48,000
прогнозы этой модели довольно

225
00:07:48,000 --> 00:07:49,919
систематичны, она в основном предсказывает

226
00:07:49,919 --> 00:07:52,240
положительные варианты, как они продаются,

227
00:07:52,240 --> 00:07:54,800
они делают, что он делает, хотя этот последний,

228
00:07:54,800 --> 00:07:56,800
который он продает, может нас немного беспокоить  бит,

229
00:07:56,800 --> 00:07:58,479
потому что он стал отрицательным,

230
00:07:58,479 --> 00:08:00,560
несмотря на то, что изменения в примере были

231
00:08:00,560 --> 00:08:02,879
действительно случайными,

232
00:08:02,879 --> 00:08:04,479
и это может указывать на способ,

233
00:08:04,479 --> 00:08:06,560
которым модель имеет или не имеет знания

234
00:08:06,560 --> 00:08:08,560
о том, как

235
00:08:08,560 --> 00:08:10,879
следует предсказывать отдельные компоненты этих примеров.

236
00:08:10,879 --> 00:08:12,639
окончательные прогнозы,

237
00:08:12,639 --> 00:08:14,319
которые делает модель, я думаю, что это

238
00:08:14,319 --> 00:08:16,160
замечательная возможность получить представление о

239
00:08:16,160 --> 00:08:17,840
том, насколько устойчива модель на самом деле

240
00:08:17,840 --> 00:08:19,919
к вариациям, подобным  е тот, что

241
00:08:19,919 --> 00:08:23,240
ты видишь здесь

242
00:08:26,240 --> 00:08:28,319
ты

