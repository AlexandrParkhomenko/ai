1
00:00:04,850 --> 00:00:07,500
Um, today what we're gonna be doing is we are gonna be

2
00:00:07,500 --> 00:00:10,335
starting to talk about fast reinforcement learning.

3
00:00:10,335 --> 00:00:13,380
Um, so in terms of where we are in the class,

4
00:00:13,380 --> 00:00:15,060
we've just finished up policy search.

5
00:00:15,060 --> 00:00:16,850
You guys are working on policy gradient,

6
00:00:16,850 --> 00:00:18,720
uh, right now for your homework.

7
00:00:18,720 --> 00:00:22,710
And then, that'll be the last homework and then the rest of the time will be on projects.

8
00:00:22,710 --> 00:00:24,330
[NOISE] Excuse me.

9
00:00:24,330 --> 00:00:26,370
Um, and then, uh,

10
00:00:26,370 --> 00:00:28,950
right now we're gonna start to talk about fast reinforcement learning,

11
00:00:28,950 --> 00:00:31,500
which is something that we haven't talked about so much.

12
00:00:31,500 --> 00:00:36,200
So, so the things we've discussed a lot so far in the term are things like optimization,

13
00:00:36,200 --> 00:00:38,270
generalization, and delayed consequences.

14
00:00:38,270 --> 00:00:41,495
So, how we do planning and Markov decision processes?

15
00:00:41,495 --> 00:00:45,740
How do we scale up to really large state spaces using like deep neural networks?

16
00:00:45,740 --> 00:00:48,845
Um, and how do we do this optimization?

17
00:00:48,845 --> 00:00:51,875
And I think that that works really well for,

18
00:00:51,875 --> 00:00:56,300
um, uh, a lot of cases where we have good simulators or where data is pretty cheap.

19
00:00:56,300 --> 00:00:58,130
Um, but a lot of the work that I do in

20
00:00:58,130 --> 00:01:01,360
my own lab thinks about how do we teach computers to help us,

21
00:01:01,360 --> 00:01:03,830
uh, which naturally involves reinforcement learning because we're

22
00:01:03,830 --> 00:01:06,965
teaching computers about how to make decisions that would help us.

23
00:01:06,965 --> 00:01:08,750
But I think there are a lot of other applications

24
00:01:08,750 --> 00:01:10,520
where we'd really like computers to help us.

25
00:01:10,520 --> 00:01:12,155
So, things like, uh,

26
00:01:12,155 --> 00:01:22,420
education or healthcare or consumer marketing.

27
00:01:25,210 --> 00:01:29,060
And in each of these cases we can think of them as being reinforcement learning

28
00:01:29,060 --> 00:01:33,320
problems because we'd have some sort of agent like our computer, uh,

29
00:01:33,320 --> 00:01:36,110
that is making decisions as it interacts with

30
00:01:36,110 --> 00:01:41,000
a person and it's trying to optimize some reward, like,

31
00:01:41,000 --> 00:01:42,680
it's trying to help someone learn something or it's

32
00:01:42,680 --> 00:01:45,095
trying to treat a patient or it's trying to,

33
00:01:45,095 --> 00:01:49,220
uh, increase revenue for a company by having consumers click on ads.

34
00:01:49,220 --> 00:01:51,140
And in all of those cases,

35
00:01:51,140 --> 00:01:53,320
the place where data comes from is people.

36
00:01:53,320 --> 00:01:56,705
Um, and so, there's at least two big challenges with that.

37
00:01:56,705 --> 00:01:58,790
The first is that, uh, you know, people are finite.

38
00:01:58,790 --> 00:02:01,760
There's not [NOISE] an infinite number of people, um, and also,

39
00:02:01,760 --> 00:02:04,265
that it's expensive and costly to,

40
00:02:04,265 --> 00:02:06,980
um, try to gather data when you interact with people.

41
00:02:06,980 --> 00:02:10,854
And so, it raises the concern about sample efficiency.

42
00:02:10,854 --> 00:02:13,090
So, in general, of course,

43
00:02:13,090 --> 00:02:15,410
we would love to have reinforcement learning algorithms that are both

44
00:02:15,410 --> 00:02:17,750
computationally efficient and sample efficient.

45
00:02:17,750 --> 00:02:21,440
But, uh, most of the techniques we've been looking at so far particularly these sort of

46
00:02:21,440 --> 00:02:24,170
Q learning type techniques were really sort

47
00:02:24,170 --> 00:02:27,990
of inspired by this need for computational efficiency.

48
00:02:29,240 --> 00:02:33,830
Um, so if we think back to when we were just doing planning at

49
00:02:33,830 --> 00:02:38,270
the beginning when we talked about doing dynamic programming versus doing Q learning,

50
00:02:38,270 --> 00:02:40,070
um, in dynamic programming,

51
00:02:40,070 --> 00:02:45,250
we had to do a sum over all next states and in TD learning we sampled that.

52
00:02:45,250 --> 00:02:49,655
So, in TD learning we sort of had this constant cost per update

53
00:02:49,655 --> 00:02:54,540
versus for dynamic programming where we had this S squared times A and cost.

54
00:02:54,540 --> 00:02:56,720
Um, so it was much more expensive to do things like

55
00:02:56,720 --> 00:02:59,270
dynamic programming than it was to do TD learning,

56
00:02:59,270 --> 00:03:01,970
um, on a, on a per step basis.

57
00:03:01,970 --> 00:03:04,070
And so, a lot of the techniques that have been developed in

58
00:03:04,070 --> 00:03:05,510
reinforcement learning have really been thinking

59
00:03:05,510 --> 00:03:07,300
about this computational efficiency issue.

60
00:03:07,300 --> 00:03:10,750
Um, and there are a lot of times where computational efficiency is important.

61
00:03:10,750 --> 00:03:13,040
Like, if you wanted to plan from scratch and you were sort of driving

62
00:03:13,040 --> 00:03:18,350
a car at 60 miles per hour,

63
00:03:19,200 --> 00:03:21,670
then if it takes you-.

64
00:03:21,670 --> 00:03:24,620
Uh, so if you're driving a car at 60 miles per

65
00:03:24,620 --> 00:03:28,230
hour and it takes your computer one second to make a decision about like,

66
00:03:28,230 --> 00:03:31,055
you know, how to turn the wheel or something like that, um,

67
00:03:31,055 --> 00:03:34,745
then during that one second you've traveled, you know, many feet.

68
00:03:34,745 --> 00:03:36,710
So, in a lot of cases,

69
00:03:36,710 --> 00:03:38,465
you really do have real-time constraints,

70
00:03:38,465 --> 00:03:40,520
uh, on the computation you can do.

71
00:03:40,520 --> 00:03:42,605
Uh, and in many situations like for,

72
00:03:42,605 --> 00:03:44,255
you know, in the cases of, uh,

73
00:03:44,255 --> 00:03:47,360
some robotics and particularly when we have simulators

74
00:03:47,360 --> 00:03:51,530
[NOISE] we really want

75
00:03:51,530 --> 00:03:54,890
computational efficiency because we need to be able to do these things very quickly.

76
00:03:54,890 --> 00:03:57,140
Um, we can sort of use our simulators but we need

77
00:03:57,140 --> 00:03:59,975
our simulators to be fast so our agent can learn.

78
00:03:59,975 --> 00:04:03,260
Um, in contrast to those sort of examples are things where sample efficiency

79
00:04:03,260 --> 00:04:06,470
is super expens- important and maybe computation is less important.

80
00:04:06,470 --> 00:04:15,280
So, whenever experience is costly or hard to gather.

81
00:04:17,390 --> 00:04:20,524
And so, this is particularly things that involve people.

82
00:04:20,524 --> 00:04:24,595
Um, uh, so we think about students

83
00:04:24,595 --> 00:04:33,370
or patients or customers,

84
00:04:33,620 --> 00:04:37,500
like the way that our agent will learn about the world is making decisions,

85
00:04:37,500 --> 00:04:39,215
um, and that data affects real people.

86
00:04:39,215 --> 00:04:42,550
So, it might be very reasonable for us to take, you know,

87
00:04:42,550 --> 00:04:47,500
several days of computation if we could figure out a better way to treat cancer, um,

88
00:04:47,500 --> 00:04:50,500
because we don't wanna randomly experiment on people and we wanna

89
00:04:50,500 --> 00:04:53,935
use the data as well as we can to be really really sample efficient,

90
00:04:53,935 --> 00:04:57,025
um, versus in the case of like Atari it's, uh,

91
00:04:57,025 --> 00:05:01,210
we wanna be really computationally efficient because we can do many, many simulations.

92
00:05:01,210 --> 00:05:02,620
It's fine. No one's getting hurt.

93
00:05:02,620 --> 00:05:05,245
But, um, but we need to eventually learn to make,

94
00:05:05,245 --> 00:05:07,760
you know, to derive a good game.

95
00:05:09,320 --> 00:05:12,525
So, one natural question might be,

96
00:05:12,525 --> 00:05:14,885
okay, so maybe now we care about sample efficiency.

97
00:05:14,885 --> 00:05:17,990
Um, and before we cared perhaps more about computational efficiency but

98
00:05:17,990 --> 00:05:21,575
maybe the algorithms we've already discussed are already sample efficient.

99
00:05:21,575 --> 00:05:25,860
So, does anybody remember like on the order of magnitude or like, you know,

100
00:05:25,860 --> 00:05:28,230
somewhere in the rough ballpark how many steps it

101
00:05:28,230 --> 00:05:32,050
took DQN to learn a good policy for Pong?

102
00:05:32,090 --> 00:05:35,970
Maybe, there's multiple answers maybe someone could say.

103
00:05:35,970 --> 00:05:44,640
Yeah. [NOISE] I think it varies somewhere partly between 2 to 10,

104
00:05:44,640 --> 00:05:46,365
is my guess. 2 to 10 million.

105
00:05:46,365 --> 00:05:48,715
So, um, that's a lot,

106
00:05:48,715 --> 00:05:51,030
that's a lot of data [LAUGHTER] to learn to play Pong.

107
00:05:51,030 --> 00:05:53,990
So, I would argue that the techniques we've seen so far, um,

108
00:05:53,990 --> 00:05:55,940
are not gonna address this issue and it's

109
00:05:55,940 --> 00:05:58,010
not gonna be reasonable for us to need, you know,

110
00:05:58,010 --> 00:06:01,940
somewhere between 2 to 10 million customers before we figure out a good way to target

111
00:06:01,940 --> 00:06:05,900
ads or 2 to 10 million patients before we figure out the right decisions to do.

112
00:06:05,900 --> 00:06:09,290
So, the techniques we've seen so far are not gonna be, uh,

113
00:06:09,290 --> 00:06:12,950
they're formally not sample efficient and they're also empirically not sample efficient.

114
00:06:12,950 --> 00:06:16,525
Um, so we're gonna need new types of techniques than what we've seen so far.

115
00:06:16,525 --> 00:06:19,010
So, of course, when we start to think about this,

116
00:06:19,010 --> 00:06:20,360
we think about the general issue of.

117
00:06:20,360 --> 00:06:22,280
you know, what does it mean for an algorithm to be good?

118
00:06:22,280 --> 00:06:24,410
We've talked about computational efficiency

119
00:06:24,410 --> 00:06:26,615
and I've mentioned this thing called sample efficiency.

120
00:06:26,615 --> 00:06:29,840
But in general, I think one thing we care a lot about is, you know,

121
00:06:29,840 --> 00:06:32,540
how good is our reinforcement learning algorithm and we're going to start

122
00:06:32,540 --> 00:06:35,855
to try to quantify that in terms of sample efficiency.

123
00:06:35,855 --> 00:06:37,250
Um, but, of course,

124
00:06:37,250 --> 00:06:40,970
you could have an algorithm that's really sample efficient in the sense that

125
00:06:40,970 --> 00:06:45,170
maybe it only uses the first 10 data points and then it never updates its policy.

126
00:06:45,170 --> 00:06:49,240
So, it doesn't need very much data to find a policy but the policy is bad.

127
00:06:49,240 --> 00:06:51,500
So, when we talk about sample efficiency,

128
00:06:51,500 --> 00:06:54,290
we're gonna want both but we don't need very much data

129
00:06:54,290 --> 00:06:57,575
and we don't need very much data in order to make good decisions.

130
00:06:57,575 --> 00:06:59,720
So, we still wanna get good performance we

131
00:06:59,720 --> 00:07:02,510
just don't need- wanna need very much experience to get there.

132
00:07:02,510 --> 00:07:07,265
Um, so when we talk about what it means for an algorithm to be good,

133
00:07:07,265 --> 00:07:11,300
you know, one a, one possibility is we can talk about whether or not it converges at all.

134
00:07:11,300 --> 00:07:12,950
Um, that just means

135
00:07:12,950 --> 00:07:17,440
whether or not the value function or the policy is stable at some point,

136
00:07:17,440 --> 00:07:21,745
like asymptotically because the number of time steps goes to infinity.

137
00:07:21,745 --> 00:07:24,680
And we talked about how sometimes with, uh,

138
00:07:24,680 --> 00:07:27,515
value function approximation we don't even have that,

139
00:07:27,515 --> 00:07:29,480
like, ah, things can oscillate.

140
00:07:29,480 --> 00:07:33,680
Um, then another thing that's stronger than that that you might wanna us to say, well,

141
00:07:33,680 --> 00:07:35,780
asymptotically as t goes to infinity,

142
00:07:35,780 --> 00:07:38,855
um, will we converge to the optimal policy?

143
00:07:38,855 --> 00:07:43,160
And we talked about some algorithms that would do that under some different assumptions.

144
00:07:43,160 --> 00:07:46,370
Um, but what we haven't talked about very much is sort of,

145
00:07:46,370 --> 00:07:48,450
you know, well, how quickly do we get there?

146
00:07:48,450 --> 00:07:50,940
Asymptotically is a very long time.

147
00:07:50,940 --> 00:07:54,010
Um, and so we might wanna be able to say,

148
00:07:54,010 --> 00:07:58,280
if we have two algorithms and one of them gets the optimal policy here,

149
00:07:58,280 --> 00:08:03,905
like if this is performance with this time and another algorithm goes like this,

150
00:08:03,905 --> 00:08:07,550
intuitively, algorithm two is better than algorithm

151
00:08:07,550 --> 00:08:13,025
one even though they both get to the optimal policy, eventually.

152
00:08:13,025 --> 00:08:17,030
So, we'd like to be able to sort of account for either,

153
00:08:17,030 --> 00:08:20,240
we can think about things like how many mistakes our algorithm makes

154
00:08:20,240 --> 00:08:24,665
or its relative performance over time compared to the optimal.

155
00:08:24,665 --> 00:08:27,080
And so, we'll start to talk today about

156
00:08:27,080 --> 00:08:30,810
some other forms of measures for how good an algorithm is.

157
00:08:31,130 --> 00:08:35,360
So, in this lecture and the next couple of lectures, we're gonna, um,

158
00:08:35,360 --> 00:08:37,669
do several different things trying to talk about sort of

159
00:08:37,669 --> 00:08:40,009
how good are these reinforcement learning algorithms and think

160
00:08:40,010 --> 00:08:45,230
about algorithms that could be much better in terms of their guarantees for performance.

161
00:08:45,230 --> 00:08:49,035
Um, we're gonna start off and talk about tabular settings.

162
00:08:49,035 --> 00:08:52,250
But today we're only gonna talk about simple bandits.

163
00:08:52,250 --> 00:08:53,805
But generally, for the next,

164
00:08:53,805 --> 00:08:55,150
um, today and next lecture,

165
00:08:55,150 --> 00:08:57,740
we'll talk about tabular settings and then, um,

166
00:08:57,740 --> 00:09:02,390
hopefully also get to some about function approximation plus sample efficiency.

167
00:09:02,390 --> 00:09:06,235
But we'll start to talk about sort of settings frameworks and approaches.

168
00:09:06,235 --> 00:09:09,395
So, the settings that we're going to be covering like today and next time,

169
00:09:09,395 --> 00:09:11,270
it's gonna be bandits which, uh,

170
00:09:11,270 --> 00:09:12,935
a number of you- who, who,

171
00:09:12,935 --> 00:09:14,855
who here is doing the default project?

172
00:09:14,855 --> 00:09:16,550
Okay. So, a number of you are,

173
00:09:16,550 --> 00:09:18,905
are already starting to think about this in terms of the project.

174
00:09:18,905 --> 00:09:20,795
So, we'll introduce bandits today, um,

175
00:09:20,795 --> 00:09:23,330
and then we'll also talk about this for MDPs.

176
00:09:23,330 --> 00:09:26,059
And then, we'll also introduce frameworks,

177
00:09:26,059 --> 00:09:28,850
and these are evaluation criteria for formally

178
00:09:28,850 --> 00:09:31,955
assessing the quality of a reinforcement learning algorithm.

179
00:09:31,955 --> 00:09:36,200
So, they're way- there's sort of a tool you could use to evaluate

180
00:09:36,200 --> 00:09:38,660
many different algorithms and algorithms will either satisfy

181
00:09:38,660 --> 00:09:41,250
this framework or not, or have different properties,

182
00:09:41,250 --> 00:09:43,370
um, under these different frameworks.

183
00:09:43,370 --> 00:09:45,200
And then, we'll also start to talk about

184
00:09:45,200 --> 00:09:47,840
approaches, which are classes of algorithms for achieving

185
00:09:47,840 --> 00:09:50,270
these different evaluation criteria for

186
00:09:50,270 --> 00:09:54,440
these different frameworks in different settings, either for the MDP setting or,

187
00:09:54,440 --> 00:09:55,925
or for the bandit setting.

188
00:09:55,925 --> 00:09:59,105
And what we'll shortly see is that there's a couple of main ideas of

189
00:09:59,105 --> 00:10:02,930
styles or approaches of algorithms which turned out to both have, um,

190
00:10:02,930 --> 00:10:06,590
be applicable both to bandit settings and MDP settings, um,

191
00:10:06,590 --> 00:10:08,525
and function approximation actually

192
00:10:08,525 --> 00:10:11,295
and also that have some really nice formal properties.

193
00:10:11,295 --> 00:10:13,610
There's sort of a couple of big conceptual ideas

194
00:10:13,610 --> 00:10:16,830
about how we might do fast reinforcement learning.

195
00:10:17,650 --> 00:10:20,990
Okay. So, the, the plan for today will be

196
00:10:20,990 --> 00:10:23,435
that we'll first start with an introduction to multi-armed bandits.

197
00:10:23,435 --> 00:10:27,230
Then we'll talk about the definition of regret on a mathematical formal sense.

198
00:10:27,230 --> 00:10:30,230
Um, and then we'll talk about optimism under uncertainty.

199
00:10:30,230 --> 00:10:32,915
And then, as we can, we'll talk about Bayesian regret,

200
00:10:32,915 --> 00:10:36,320
um, and probability matching and Thompson sampling.

201
00:10:36,320 --> 00:10:40,390
I'm curious, who here has ever seen this sort of material before?

202
00:10:40,390 --> 00:10:43,395
Okay. A couple of people, most people not.

203
00:10:43,395 --> 00:10:45,630
I wouldn't- is it covered in AI?

204
00:10:45,630 --> 00:10:51,545
I don't think they would cover it. Oh, good. Okay.

205
00:10:51,545 --> 00:10:54,145
All right. So, for some of you,

206
00:10:54,145 --> 00:10:56,395
uh, this will be a review, for most of you, it will be new.

207
00:10:56,395 --> 00:10:57,925
So, for multi-armed bandits,

208
00:10:57,925 --> 00:11:00,670
we can think of them as a subset of Reinforcement Learning.

209
00:11:00,670 --> 00:11:03,910
So, it's generally considered a set of arms,

210
00:11:03,910 --> 00:11:06,175
the- there's a set of m arms,

211
00:11:06,175 --> 00:11:09,745
which were, uh, the equivalent to what we used to call actions.

212
00:11:09,745 --> 00:11:11,140
So, in Reinforcement Learning,

213
00:11:11,140 --> 00:11:12,955
in our set of actions, um,

214
00:11:12,955 --> 00:11:15,190
we're thinking about like, there being m different actions.

215
00:11:15,190 --> 00:11:16,690
Now we're often gonna call those arms.

216
00:11:16,690 --> 00:11:19,120
[NOISE] And then, for each of those arms,

217
00:11:19,120 --> 00:11:22,855
they have a distribution of rewards you could get.

218
00:11:22,855 --> 00:11:27,550
So, we haven't talked a lot about sort of having uncertainty over our rewards.

219
00:11:27,550 --> 00:11:29,935
We mostly just talked about the expected reward.

220
00:11:29,935 --> 00:11:31,720
Um, and for multi-armed bandits,

221
00:11:31,720 --> 00:11:34,570
today we're gonna explicitly think about the fact that, um,

222
00:11:34,570 --> 00:11:37,420
rewards might be sampled from a stochastic distribution.

223
00:11:37,420 --> 00:11:42,820
[NOISE] So, there's some distribution that we don't know which is conditioned on the arm.

224
00:11:42,820 --> 00:11:43,990
So, conditioned on the arm,

225
00:11:43,990 --> 00:11:45,400
you're gonna get different rewards.

226
00:11:45,400 --> 00:11:50,290
So, for example, it could be that for arm 1,

227
00:11:50,290 --> 00:11:52,240
your distribution looks something like this.

228
00:11:52,240 --> 00:11:56,230
This is the probability of that reward and this is rewards.

229
00:11:56,230 --> 00:12:00,290
Um, and then for arm 2,

230
00:12:02,040 --> 00:12:04,150
it looks something like this.

231
00:12:04,150 --> 00:12:07,525
[NOISE] So, in this particular example, um,

232
00:12:07,525 --> 00:12:12,010
the average reward for arm 1 would be higher than the average reward for arm 2.

233
00:12:12,010 --> 00:12:14,425
And then they would have different variances.

234
00:12:14,425 --> 00:12:16,495
But it doesn't have to be Gaussian.

235
00:12:16,495 --> 00:12:18,550
You could have lots of different distributions.

236
00:12:18,550 --> 00:12:22,180
Um, essentially, what we're trying to capture here is that whenever you,

237
00:12:22,180 --> 00:12:24,265
uh, um, take a particular action,

238
00:12:24,265 --> 00:12:26,635
which we also refer to as pulling an arm, um,

239
00:12:26,635 --> 00:12:29,080
for the multi-armed bandit, um,

240
00:12:29,080 --> 00:12:32,710
then the reward you get might vary even if you pull the same arm twice.

241
00:12:32,710 --> 00:12:36,265
So, in this case, you can imagine that if you pull the arm once,

242
00:12:36,265 --> 00:12:38,320
um, arm 1 once, you might get a reward here

243
00:12:38,320 --> 00:12:41,030
and maybe the second time you get a reward there.

244
00:12:41,220 --> 00:12:44,560
So, the idea in this case is it's similar to

245
00:12:44,560 --> 00:12:47,395
MDPs except for now there's no transition function.

246
00:12:47,395 --> 00:12:49,990
So, there's no state or equivalently,

247
00:12:49,990 --> 00:12:52,075
you can think of it as there's only a single state.

248
00:12:52,075 --> 00:12:54,055
Um, and so when you take an arm,

249
00:12:54,055 --> 00:12:55,345
um, you stay in the same state.

250
00:12:55,345 --> 00:12:59,050
There's always these m actions available to you, and on each step,

251
00:12:59,050 --> 00:13:02,140
you get to pick what action to take and then you observe some reward that is

252
00:13:02,140 --> 00:13:06,515
sampled from the unknown probability distribution associated with that arm.

253
00:13:06,515 --> 00:13:09,600
And just like in Reinforcement Learning, um,

254
00:13:09,600 --> 00:13:11,820
we don't know what those reward distributions are in

255
00:13:11,820 --> 00:13:16,235
advance and our goal is to maximize the cumulative reward.

256
00:13:16,235 --> 00:13:19,600
So, if someone told you what these distributions were in advance,

257
00:13:19,600 --> 00:13:21,175
you would know exactly which arm to pull,

258
00:13:21,175 --> 00:13:22,810
whichever one has the highest expected,

259
00:13:22,810 --> 00:13:24,040
uh, as, expected mean.

260
00:13:24,040 --> 00:13:31,405
[NOISE] So, we're gonna try to use pretty similar notation to what we had for,

261
00:13:31,405 --> 00:13:32,875
um, the reinforcement learning case.

262
00:13:32,875 --> 00:13:36,085
Um, but if you notice things that are confusing, just let me know.

263
00:13:36,085 --> 00:13:40,990
Um, so we're gonna define the action value as the mean reward for a particular action.

264
00:13:40,990 --> 00:13:44,320
So, that's Qa, this is unknown,

265
00:13:44,320 --> 00:13:46,150
agent doesn't know this in advance.

266
00:13:46,150 --> 00:13:53,450
The optimal value V star is gonna be equal to Q of the best action.

267
00:13:53,580 --> 00:13:59,065
And then, the regret is gonna be the opportunity loss for one step.

268
00:13:59,065 --> 00:14:02,860
So, what that means is that if instead of taking, if,

269
00:14:02,860 --> 00:14:08,695
if you could have taken Q of a star and instead you took Q of at.

270
00:14:08,695 --> 00:14:11,870
So, this is the actual arm you selected.

271
00:14:12,150 --> 00:14:18,970
How much in expectation did you lose by taking the sub-optimal arm?

272
00:14:18,970 --> 00:14:21,070
And this is how we're gonna mathematically define

273
00:14:21,070 --> 00:14:23,500
regret in the case of Reinforcement Learning.

274
00:14:23,500 --> 00:14:27,595
So, if you selected the optimal arm, your regret,

275
00:14:27,595 --> 00:14:30,100
your expected regret will be 0 for that time step,

276
00:14:30,100 --> 00:14:31,435
um, but for any other arm,

277
00:14:31,435 --> 00:14:33,430
there will be some loss.

278
00:14:33,430 --> 00:14:36,085
And the total regret is just, um,

279
00:14:36,085 --> 00:14:38,845
the total opportunity loss if you sum over

280
00:14:38,845 --> 00:14:42,235
all the time steps that the agent acts and compare,

281
00:14:42,235 --> 00:14:44,425
um, the actions it took and the, um,

282
00:14:44,425 --> 00:14:46,510
the expected reward of each of those actions

283
00:14:46,510 --> 00:14:49,120
to the expected reward of the optimal action.

284
00:14:49,120 --> 00:14:50,830
So, just to be clear here,

285
00:14:50,830 --> 00:14:52,000
this is not known,

286
00:14:52,000 --> 00:14:54,190
this is not known to the agent,

287
00:14:54,190 --> 00:14:56,350
and this is unknown to the agent.

288
00:14:56,350 --> 00:15:03,130
[NOISE] Just to check for understanding for a second,

289
00:15:03,130 --> 00:15:09,655
why is this, so why is this second thing unknown to the agent? Yeah.

290
00:15:09,655 --> 00:15:12,190
Because you don't know the probability distribution of Q.

291
00:15:12,190 --> 00:15:13,900
Right. So, [NOISE], correct.

292
00:15:13,900 --> 00:15:15,850
So, you don't know, er,

293
00:15:15,850 --> 00:15:17,380
what the distribution is,

294
00:15:17,380 --> 00:15:18,820
so you don't know what Q is.

295
00:15:18,820 --> 00:15:21,325
You get to observe, um, a sample from that.

296
00:15:21,325 --> 00:15:25,135
So, you get to observe R. You get to get an R which was sampled

297
00:15:25,135 --> 00:15:29,470
from the probability distribution of rewards given the action that was taken.

298
00:15:29,470 --> 00:15:33,070
[NOISE] But you don't get to observe either, uh,

299
00:15:33,070 --> 00:15:35,440
the true expected value of the optimal arm

300
00:15:35,440 --> 00:15:40,070
nor the true expected value of the arm that you selected.

301
00:15:40,380 --> 00:15:46,705
So, this isn't something we can normally evaluate unless we're in a simulated domain, okay?

302
00:15:46,705 --> 00:15:48,850
But we're gonna talk about ways that we can bound this and,

303
00:15:48,850 --> 00:15:52,370
and think about algorithms or try to minimize the regret.

304
00:15:55,400 --> 00:15:58,680
So, if we think about ways to sort of quantify it,

305
00:15:58,680 --> 00:16:01,410
another way to think about it alternatively is that,

306
00:16:01,410 --> 00:16:04,920
um, think about the number of times that you take a particular action.

307
00:16:04,920 --> 00:16:06,890
We can call that Nt of a.

308
00:16:06,890 --> 00:16:09,880
So, that's like the number of times we select action 1,

309
00:16:09,880 --> 00:16:11,575
action 2, et cetera.

310
00:16:11,575 --> 00:16:16,570
And then, we can define a gap which is the difference between [NOISE] the optimal

311
00:16:16,570 --> 00:16:21,465
arm's value and the value of the arm that we selected, and that's the gap.

312
00:16:21,465 --> 00:16:27,285
So, that's how much we lost by picking a different arm than the optimal arm.

313
00:16:27,285 --> 00:16:30,275
So, this gap is equal to,

314
00:16:30,275 --> 00:16:35,590
gap for a star is equal to 0, if you don't lose anything by taking the optimal arm,

315
00:16:35,590 --> 00:16:38,365
and for all other arms, it's gonna be positive.

316
00:16:38,365 --> 00:16:42,400
So, another way to think about the regret which is equivalent is to say,

317
00:16:42,400 --> 00:16:44,770
this is equivalent to thinking about what are the expected number of

318
00:16:44,770 --> 00:16:48,400
times you select each of the arms times their gap.

319
00:16:48,400 --> 00:16:52,670
So, how much you would lose by selecting that arm compared to the optimal arm.

320
00:16:52,740 --> 00:16:57,310
And what we would like is that sort of an algorithm, um,

321
00:16:57,310 --> 00:17:02,305
it should be able to adjust how many times you pull arms which have large gaps.

322
00:17:02,305 --> 00:17:05,130
So, if there's a really, really bad arm, uh,

323
00:17:05,130 --> 00:17:07,780
like if there's really bad action which has a very low reward,

324
00:17:07,780 --> 00:17:09,640
you would like not to pull that as much,

325
00:17:09,640 --> 00:17:12,730
to take that action as much as the arms that are close to optimal.

326
00:17:12,730 --> 00:17:19,615
[NOISE] So, one approach that we've seen before is greedy.

327
00:17:19,615 --> 00:17:21,535
Um, in the case of, ah,

328
00:17:21,535 --> 00:17:23,980
bandits, the greedy algorithm is very simple.

329
00:17:23,980 --> 00:17:26,665
Um, we just average the,

330
00:17:26,665 --> 00:17:28,315
the rewards we've seen for an arm.

331
00:17:28,315 --> 00:17:31,750
So, we just look at every single time that we took that arm,

332
00:17:31,750 --> 00:17:35,680
we look at the reward we got for each those timestamps, and then we average it.

333
00:17:35,680 --> 00:17:37,540
And that just gives us, um,

334
00:17:37,540 --> 00:17:39,850
an estimate of the, of Q hat.

335
00:17:39,850 --> 00:17:43,945
And then what greedy does is it selects the action with the highest value,

336
00:17:43,945 --> 00:17:46,435
um, and takes that arm forever.

337
00:17:46,435 --> 00:17:52,870
So, it's probably clear in this case that because the rewards are sampled from

338
00:17:52,870 --> 00:17:58,780
a stochastic distribution that if you are unlucky and get samples that are,

339
00:17:58,780 --> 00:18:04,420
uh, misrepresentative, then you could lock into the wrong action and stay there forever.

340
00:18:04,420 --> 00:18:07,330
So, if we think of that little example I gave before,

341
00:18:07,330 --> 00:18:10,480
and I'll work out, uh, a bigger example shortly, so in this case,

342
00:18:10,480 --> 00:18:11,740
imagine this is reward,

343
00:18:11,740 --> 00:18:17,275
this is probability, and this was a2.

344
00:18:17,275 --> 00:18:20,080
Okay. So, let's imagine that the first,

345
00:18:20,080 --> 00:18:22,510
um, and I'll make this 1 and this 0.

346
00:18:22,510 --> 00:18:25,030
So, if you sample from a1, in this case,

347
00:18:25,030 --> 00:18:29,440
you could imagine there's some non-zero probability that the first sample you get is say,

348
00:18:29,440 --> 00:18:36,940
0.2 for a1, and the first sample you get for a2 with non-zero probability might be 0.5.

349
00:18:36,940 --> 00:18:40,870
So, the true mean of a2 is lower than the mean of a1.

350
00:18:40,870 --> 00:18:43,240
But if you sampled each of these once, um,

351
00:18:43,240 --> 00:18:45,535
then if you're greedy with respect to that,

352
00:18:45,535 --> 00:18:47,290
then you will take the wrong action forever.

353
00:18:47,290 --> 00:18:50,815
[NOISE] Yeah.

354
00:18:50,815 --> 00:18:54,190
Wrong action forever, is the idea that

355
00:18:54,190 --> 00:18:56,800
our policy is gonna be influencing what times we'll get in

356
00:18:56,800 --> 00:18:59,500
the future or is the idea that there are

357
00:18:59,500 --> 00:19:02,695
some set of samples independent of this greedy policy to begin?

358
00:19:02,695 --> 00:19:05,170
Because it seems otherwise, if there is non-zero reward,

359
00:19:05,170 --> 00:19:07,180
you just take that one forever.

360
00:19:07,180 --> 00:19:10,150
Uh, great question. So, um, is it, yeah,

361
00:19:10,150 --> 00:19:12,040
so what [inaudible] said is, um, you know,

362
00:19:12,040 --> 00:19:14,320
is there an additional thing that we're doing kind of before this?

363
00:19:14,320 --> 00:19:16,135
Normally, for a lot of these algorithms, um,

364
00:19:16,135 --> 00:19:18,610
we're gonna assume that all of them operate by

365
00:19:18,610 --> 00:19:22,030
selecting each arm once at least if you have a finite set of arms.

366
00:19:22,030 --> 00:19:25,840
Um, and equivalently, you [NOISE] can say if you don't have any data,

367
00:19:25,840 --> 00:19:28,000
you treat everything equivalently or, um,

368
00:19:28,000 --> 00:19:30,160
but essentially most of these ones say,

369
00:19:30,160 --> 00:19:31,795
until you have data for all the arms,

370
00:19:31,795 --> 00:19:33,595
we are gonna do round robin,

371
00:19:33,595 --> 00:19:34,720
you're gonna sample everything once.

372
00:19:34,720 --> 00:19:37,630
And after you do that, either you can be greedy or we can do something else.

373
00:19:37,630 --> 00:19:41,950
So, there has to be a pre-initialization space. It's a good question.

374
00:19:41,950 --> 00:19:45,460
So, and we're also gonna assume for right now that we split ties,

375
00:19:45,460 --> 00:19:46,960
um, with equal probability.

376
00:19:46,960 --> 00:19:49,960
So, if there are two arms that have the same pro- probability, um,

377
00:19:49,960 --> 00:19:52,360
and they both have the max actio- max value,

378
00:19:52,360 --> 00:19:56,035
then you would split your time between those until the value is changed.

379
00:19:56,035 --> 00:19:59,770
So, this is an example where if we first sampled a1 once [NOISE],

380
00:19:59,770 --> 00:20:01,225
then sampled a2 once.

381
00:20:01,225 --> 00:20:03,430
Um, and because there's a non-zero probability

382
00:20:03,430 --> 00:20:05,260
that those samples would make it look that,

383
00:20:05,260 --> 00:20:08,080
um, action a1 has a lower mean than action a2,

384
00:20:08,080 --> 00:20:10,940
then you could lock into the wrong action forever.

385
00:20:12,270 --> 00:20:14,470
Now, an e-greedy algorithm,

386
00:20:14,470 --> 00:20:16,450
which we've seen before in class, um, uh,

387
00:20:16,450 --> 00:20:20,290
it does something very similar except for with probability 1 - epsilon, we select,

388
00:20:20,290 --> 00:20:22,030
um, the greedy action,

389
00:20:22,030 --> 00:20:23,350
and otherwise with epsilon,

390
00:20:23,350 --> 00:20:27,610
we split our probability across all the other actions or all the other arms.

391
00:20:27,610 --> 00:20:31,915
[NOISE] So, in these cases,

392
00:20:31,915 --> 00:20:34,470
um, we have some more robustness.

393
00:20:34,470 --> 00:20:35,580
So, in this case, you know,

394
00:20:35,580 --> 00:20:38,460
we would continue to sample, um, the other action,

395
00:20:38,460 --> 00:20:42,060
but we're always gonna make a sub-optimal decision at least epsilon percent of

396
00:20:42,060 --> 00:20:46,390
the time, well, approximately.

397
00:20:46,390 --> 00:20:49,600
It's a little bit less than that because if you do it totally randomly not a,

398
00:20:49,600 --> 00:20:52,940
um, uh, but it's order epsilon.

399
00:20:54,900 --> 00:20:58,600
I mean, er, it's slightly less than that because, um, er,

400
00:20:58,600 --> 00:21:03,520
if you uniformly split across all your arms with one over the number of arms probability,

401
00:21:03,520 --> 00:21:04,990
we'll be selecting the optimal action.

402
00:21:04,990 --> 00:21:10,830
[NOISE]

403
00:21:10,830 --> 00:21:12,990
Okay. So let's see these in practice for a

404
00:21:12,990 --> 00:21:15,180
second before we talk more about better algorithms.

405
00:21:15,180 --> 00:21:17,610
Um, so let's imagine we're trying to figure out how to

406
00:21:17,610 --> 00:21:20,355
treat broken toes and this is not a real medical example.

407
00:21:20,355 --> 00:21:24,135
Um, but let's imagine there's three different surger- three different options.

408
00:21:24,135 --> 00:21:27,690
Um, one is surgery. One is buddy taping the broken toe with another toe,

409
00:21:27,690 --> 00:21:29,565
which is what the Internet might tell you to do.

410
00:21:29,565 --> 00:21:32,295
Um, and the third is to do nothing.

411
00:21:32,295 --> 00:21:34,920
And the outcome measure is gonna be

412
00:21:34,920 --> 00:21:37,920
a binary variable of whether or not your toe is healed um,

413
00:21:37,920 --> 00:21:41,530
after six weeks as assessed by an x-ray.

414
00:21:43,000 --> 00:21:47,750
Okay. So let's imagine that we model this as a multi-armed bandit with three arms,

415
00:21:47,750 --> 00:21:50,735
where each arm corresponds to um,

416
00:21:50,735 --> 00:21:52,925
well, I'll ask you in a second what it corresponds to.

417
00:21:52,925 --> 00:21:55,375
And, and there's an unknown parameter here.

418
00:21:55,375 --> 00:22:00,030
So each arm, there's an unknown parameter which is the reward outcome.

419
00:22:00,030 --> 00:22:02,055
So let's just take just you know,

420
00:22:02,055 --> 00:22:04,170
one or two minutes just to say what does uh, um,

421
00:22:04,170 --> 00:22:06,000
a pull of an arm correspond to in

422
00:22:06,000 --> 00:22:08,460
this case and why is it reasonable to model it as a bandit,

423
00:22:08,460 --> 00:22:10,230
instead of an MDP?

424
00:22:10,230 --> 00:22:39,330
[NOISE]

425
00:22:39,330 --> 00:22:40,140
Yeah.

426
00:22:40,140 --> 00:22:44,250
I'm , and in terms of why we model it as a bandit,

427
00:22:44,250 --> 00:22:47,355
one reason is that MDPs usually,

428
00:22:47,355 --> 00:22:49,800
think of an agent walking through the world and each,

429
00:22:49,800 --> 00:22:53,100
the state, or the world has many different states, and we analyze those.

430
00:22:53,100 --> 00:22:54,810
Here we have just one state,

431
00:22:54,810 --> 00:22:57,210
a toe is broken and various actions are considered.

432
00:22:57,210 --> 00:22:59,235
Right. So  great.

433
00:22:59,235 --> 00:23:00,870
So here we just have one there.

434
00:23:00,870 --> 00:23:02,520
And, and so what is, what is the,

435
00:23:02,520 --> 00:23:05,265
what does it mean to pull an arm in this case or take an action?

436
00:23:05,265 --> 00:23:06,810
Which does that correspond to in the real world?

437
00:23:06,810 --> 00:23:12,390
[NOISE] That would be

438
00:23:12,390 --> 00:23:16,425
a new patient coming in and then making a decision about the care for that patient.

439
00:23:16,425 --> 00:23:17,880
Great. So that's like um, uh,

440
00:23:17,880 --> 00:23:22,920
a patient coming in and then us deciding to either do surgery on them or giving them um,

441
00:23:22,920 --> 00:23:25,740
er, er, in this case a um,

442
00:23:25,740 --> 00:23:29,235
like a er, er, doing one of the three options for treatment.

443
00:23:29,235 --> 00:23:31,950
Um, and so in this case too,

444
00:23:31,950 --> 00:23:34,095
the, each pool is a different patient.

445
00:23:34,095 --> 00:23:36,960
So how we treat patient one isn't gonna generally affect how we treat

446
00:23:36,960 --> 00:23:40,170
patient two in terms of whether they healed or not,

447
00:23:40,170 --> 00:23:41,610
or whether that particularly your you know,

448
00:23:41,610 --> 00:23:45,450
surgery worked for them, doesn't affect the next patient coming in.

449
00:23:45,450 --> 00:23:48,105
So all the patients are IID.

450
00:23:48,105 --> 00:23:50,490
Um, and what we wanna figure out to do,

451
00:23:50,490 --> 00:23:53,530
is which of these treatments on average is most effective.

452
00:23:55,970 --> 00:23:59,430
Okay. So let's think about these in a particular set- setting.

453
00:23:59,430 --> 00:24:01,755
So um, uh, a

454
00:24:01,755 --> 00:24:03,120
par- particular set of values.

455
00:24:03,120 --> 00:24:06,390
So let's imagine that they're all Bernoulli reward var- variables

456
00:24:06,390 --> 00:24:10,260
because either the toe is gonna be be healed or it's not gonna be healed after six weeks.

457
00:24:10,260 --> 00:24:13,680
Um, it turns out in this particular fake example, surgery is best.

458
00:24:13,680 --> 00:24:16,260
So if you do surgery with 95% probability,

459
00:24:16,260 --> 00:24:18,090
it will be healed after six weeks.

460
00:24:18,090 --> 00:24:19,905
Buddy taping is 90%,

461
00:24:19,905 --> 00:24:22,420
and doing nothing is 0.1.

462
00:24:23,270 --> 00:24:27,255
So what would happen if we did something like a greedy algorithm? Oh, yeah.

463
00:24:27,255 --> 00:24:31,500
Sorry, is it possible to incorporate other factors into like pulling the arms?

464
00:24:31,500 --> 00:24:35,940
For example, surgeries like [inaudible] like cost effective versus buddy taping,

465
00:24:35,940 --> 00:24:37,215
are there ways to incorporate that?

466
00:24:37,215 --> 00:24:40,170
Yeah. It's a great question. So question's about like uh,

467
00:24:40,170 --> 00:24:41,790
you know, could we you know,

468
00:24:41,790 --> 00:24:43,230
surgery is a lot more invasive.

469
00:24:43,230 --> 00:24:44,850
And there might be other side effects, etc.

470
00:24:44,850 --> 00:24:47,505
There's a couple different ways you could imagine putting that information in.

471
00:24:47,505 --> 00:24:49,650
One is, you could just change the reward outcome.

472
00:24:49,650 --> 00:24:51,120
So you could say um,

473
00:24:51,120 --> 00:24:53,610
maybe it's more effective but it's also really costly and I've

474
00:24:53,610 --> 00:24:56,565
gotta have some way to combine outcomes with cost.

475
00:24:56,565 --> 00:25:00,210
Um, another thing that one might be interested in doing in

476
00:25:00,210 --> 00:25:03,660
these sorts of cases is that you might have a distribution of outcome.

477
00:25:03,660 --> 00:25:06,210
So in this case, all of them have the same um, distribution.

478
00:25:06,210 --> 00:25:09,075
They're all Bernoulli. But in some cases um,

479
00:25:09,075 --> 00:25:11,520
your reward outcomes will not be,

480
00:25:11,520 --> 00:25:13,050
will be complicated functions, right?

481
00:25:13,050 --> 00:25:14,625
Like, so it might be that um,

482
00:25:14,625 --> 00:25:15,990
maybe for some people,

483
00:25:15,990 --> 00:25:20,250
surgery is really awful and for most people, it's really good.

484
00:25:20,250 --> 00:25:23,130
But it's really bad for some people because they have you know,

485
00:25:23,130 --> 00:25:26,775
some really bad side effects and so its mean is still better.

486
00:25:26,775 --> 00:25:29,670
Um, but there is like, this really bad risk tail of like,

487
00:25:29,670 --> 00:25:33,045
maybe people you know, react badly to anesthesia or something like that.

488
00:25:33,045 --> 00:25:36,210
So in those cases, you might wanna not focus on expected outcomes.

489
00:25:36,210 --> 00:25:38,010
You might wanna look at risk sensitivity.

490
00:25:38,010 --> 00:25:40,695
And in fact, one of the things that we're doing in my group is looking at

491
00:25:40,695 --> 00:25:43,155
safe reinforcement learning including safe bandits um,

492
00:25:43,155 --> 00:25:47,940
and thinking about how you could optimize for risk-sensitive criteria.

493
00:25:47,940 --> 00:25:50,100
Another thing that we're not gonna talk about today which you

494
00:25:50,100 --> 00:25:52,080
also might wanna do in this case is that,

495
00:25:52,080 --> 00:25:53,985
patients are not all identical.

496
00:25:53,985 --> 00:25:57,240
Um, and you might wanna incorporate some sort of contextual features about

497
00:25:57,240 --> 00:26:01,260
the patient to try to decide which surgery to do or no surgery versus buddy taping.

498
00:26:01,260 --> 00:26:03,255
Ah, and hopefully, well,

499
00:26:03,255 --> 00:26:04,680
those of you who are doing the default project,

500
00:26:04,680 --> 00:26:06,645
we'll think about this definitely.

501
00:26:06,645 --> 00:26:08,535
And we'll probably get to this in a couple lectures.

502
00:26:08,535 --> 00:26:09,870
In general, we often have sort of,

503
00:26:09,870 --> 00:26:13,720
a rich contextual state which will also [NOISE] affect the outcomes.

504
00:26:13,880 --> 00:26:17,385
Okay, so in this case, let's imagine that we have

505
00:26:17,385 --> 00:26:19,920
these three potential interventions

506
00:26:19,920 --> 00:26:22,155
that we can do and we're running the greedy algorithm.

507
00:26:22,155 --> 00:26:25,620
So um, as ah,  brought up before,

508
00:26:25,620 --> 00:26:27,690
we're gonna sample each of these arms once,

509
00:26:27,690 --> 00:26:29,820
and now we're gonna get an empirical average.

510
00:26:29,820 --> 00:26:34,365
So let's say that we sample action A1 and we get a plus one.

511
00:26:34,365 --> 00:26:40,830
And so now our empirical average of what the expected reward is for action A1 is 1.

512
00:26:40,830 --> 00:26:44,445
And then we do A2, and we also get 1 so that's our new average.

513
00:26:44,445 --> 00:26:47,430
And then we do A3, we get a 0.

514
00:26:47,430 --> 00:26:50,175
And so at this point,

515
00:26:50,175 --> 00:26:54,270
what is the probability of greedy selecting each arm assuming

516
00:26:54,270 --> 00:27:00,520
that the ties are split randomly? Yeah.

517
00:27:02,150 --> 00:27:08,175
Two plus two, then epsilon pair at one, plus or minus a little.

518
00:27:08,175 --> 00:27:08,955
And, and your name?

519
00:27:08,955 --> 00:27:09,540
.

520
00:27:09,540 --> 00:27:11,205
, yeah. So what  said is um,

521
00:27:11,205 --> 00:27:12,780
exactly correct for the e-greedy case.

522
00:27:12,780 --> 00:27:14,715
So you're jumping ahead a little bit but that's totally right.

523
00:27:14,715 --> 00:27:16,560
Um, ah, in this case for greedy,

524
00:27:16,560 --> 00:27:18,195
it'll just be 50-50.

525
00:27:18,195 --> 00:27:20,610
Yeah. You're already moving to e-greedy. So yes.

526
00:27:20,610 --> 00:27:24,060
So the probability of A1 is gonna equal the probability of A2,

527
00:27:24,060 --> 00:27:26,400
just gonna equal to a half.

528
00:27:26,400 --> 00:27:29,595
So let's imagine that we did this for a few time steps.

529
00:27:29,595 --> 00:27:33,435
So, so we can think about what the regret is that we incur along all of this way.

530
00:27:33,435 --> 00:27:34,725
So at the beginning um,

531
00:27:34,725 --> 00:27:35,790
we have this sort of,

532
00:27:35,790 --> 00:27:40,830
an initialization period where we're gonna select each action

533
00:27:40,830 --> 00:27:43,350
once and we're always comparing this to what

534
00:27:43,350 --> 00:27:46,350
was the reward we could have gotten under the optimal action.

535
00:27:46,350 --> 00:27:49,560
So the regret here is gonna be exactly equal,

536
00:27:49,560 --> 00:27:51,640
so this is optimal.

537
00:27:52,700 --> 00:27:59,850
And remember, the regret is gonna be Q of A star minus Q of the action you took.

538
00:27:59,850 --> 00:28:01,680
So in the first case,

539
00:28:01,680 --> 00:28:06,165
it's gonna be zero because we took the optimal action.

540
00:28:06,165 --> 00:28:11,925
And the second case, it's gonna be 0.95 - 0.9 which is 0.05.

541
00:28:11,925 --> 00:28:16,000
And then the third case, it's gonna be 0.95 - 0.1.

542
00:28:17,810 --> 00:28:20,160
And then in the third case, it's gonna be

543
00:28:20,160 --> 00:28:24,195
zero or fourth case it's gonna be 0 and then it's gonna be 0.05 again.

544
00:28:24,195 --> 00:28:26,970
Now, in this situation um,

545
00:28:26,970 --> 00:28:28,935
will we ever select a greedy?

546
00:28:28,935 --> 00:28:35,070
Will we ever select A3 again given the values we've seen so far?

547
00:28:35,070 --> 00:28:37,845
No. Yeah, I see people say you know,

548
00:28:37,845 --> 00:28:39,000
no. So why not?

549
00:28:39,000 --> 00:28:49,830
[NOISE]

550
00:28:49,830 --> 00:28:50,730
Is it possible,

551
00:28:50,730 --> 00:28:53,010
what's our current estimate of um,

552
00:28:53,010 --> 00:28:55,300
the reward for A3?

553
00:28:55,820 --> 00:28:58,590
. Yeah. So I guess I didn't show,

554
00:28:58,590 --> 00:29:00,735
put those here but the, this was the actual.

555
00:29:00,735 --> 00:29:02,070
These were the rewards we got.

556
00:29:02,070 --> 00:29:04,470
So it's 1, 1, 0.

557
00:29:04,470 --> 00:29:07,425
So our current estimate for A3 is 0.

558
00:29:07,425 --> 00:29:10,560
We know our rewards are bounded between 0 and 1.

559
00:29:10,560 --> 00:29:15,150
None of our estimates can ever collapse below 0.

560
00:29:15,150 --> 00:29:19,320
Um, and we already have a positive 1 for these other two actions,

561
00:29:19,320 --> 00:29:23,085
which means that their averages can never go to 0.

562
00:29:23,085 --> 00:29:26,230
So we're never gonna take A3 again.

563
00:29:27,530 --> 00:29:29,790
Now, in this case,

564
00:29:29,790 --> 00:29:32,490
that's not actually that bad like [LAUGHTER] here, here,

565
00:29:32,490 --> 00:29:33,960
that's not actually a problem because A3 is

566
00:29:33,960 --> 00:29:36,705
a bad arm and it's got a much lower expected reward.

567
00:29:36,705 --> 00:29:39,075
Um, in other cases er,

568
00:29:39,075 --> 00:29:41,805
it could be that A3, we just got unlucky.

569
00:29:41,805 --> 00:29:43,800
Um, and in that case,

570
00:29:43,800 --> 00:29:45,105
it could mean that we never should,

571
00:29:45,105 --> 00:29:47,910
never take the optimal action. Yeah.

572
00:29:47,910 --> 00:29:53,610
I thought we used B star like in terms of the reward for an action.

573
00:29:53,610 --> 00:29:55,965
Yes. And that's the same as this. Good question.

574
00:29:55,965 --> 00:29:58,200
So this is the same as B star.

575
00:29:58,200 --> 00:30:03,400
Yeah. And I'll go back and forth between notation but yeah, definitely just ask me.

576
00:30:03,500 --> 00:30:07,425
So in this case, we're never gonna select A3 again and um,

577
00:30:07,425 --> 00:30:09,090
now notice that in the greedy case,

578
00:30:09,090 --> 00:30:10,890
there are cases where um,

579
00:30:10,890 --> 00:30:13,980
if I had used slightly different values here um,

580
00:30:13,980 --> 00:30:16,045
that you might have selected A3 again later.

581
00:30:16,045 --> 00:30:18,215
Because if it's the case that um,

582
00:30:18,215 --> 00:30:21,740
like if you didn't have Bernoulli rewards but you had Gaussians um,

583
00:30:21,740 --> 00:30:24,860
it could be that the rewards for other arms dropped

584
00:30:24,860 --> 00:30:28,525
below another arm later and then you start to switch.

585
00:30:28,525 --> 00:30:31,950
So you don't necessarily always stick with which other arm looked best at the beginning.

586
00:30:31,950 --> 00:30:34,290
But in this particular case with these outcomes

587
00:30:34,290 --> 00:30:36,390
then you're not gonna select A3 ever again.

588
00:30:36,390 --> 00:30:40,845
[NOISE] All right, now let's do um, e-Greedy.

589
00:30:40,845 --> 00:30:42,270
So in this case,

590
00:30:42,270 --> 00:30:45,345
we're gonna assume we got exactly the same outcomes for the first few.

591
00:30:45,345 --> 00:30:47,730
Um, and then what  said is that we're gonna um,

592
00:30:47,730 --> 00:30:52,620
have ah, one-half minus epsilon um, over 2.

593
00:30:52,620 --> 00:30:55,440
So we're gonna split ties randomly again.

594
00:30:55,440 --> 00:30:57,915
So with probability epsilon,

595
00:30:57,915 --> 00:31:00,270
we're gonna take um,

596
00:31:00,270 --> 00:31:01,770
with epsilon over 3.

597
00:31:01,770 --> 00:31:06,250
We're gonna take A1 or A2 or A3.

598
00:31:07,250 --> 00:31:10,590
And with 1 - epsilon over 2,

599
00:31:10,590 --> 00:31:12,390
we're gonna take A1 [NOISE] or A2.

600
00:31:12,390 --> 00:31:19,090
[NOISE] Interesting, okay.

601
00:31:21,560 --> 00:31:25,050
Okay. So in this case, it's gonna look almost identical except we're

602
00:31:25,050 --> 00:31:28,635
still gonna have some probability of taking A3 in this case.

603
00:31:28,635 --> 00:31:31,470
And we can do a similar computation here.

604
00:31:31,470 --> 00:31:32,760
In this case um,

605
00:31:32,760 --> 00:31:35,805
we've assumed that all of these are exactly the same.

606
00:31:35,805 --> 00:31:37,470
So e- e- e-Greedy in this case,

607
00:31:37,470 --> 00:31:38,790
will select A3 again.

608
00:31:38,790 --> 00:31:42,855
Yes. Um, if epsilon is fixed,

609
00:31:42,855 --> 00:31:52,350
not updating, yeah. [NOISE]

610
00:31:52,350 --> 00:31:54,210
Um, if Epsilon is fixed,

611
00:31:54,210 --> 00:31:57,190
how many times is it gonna select a_3?

612
00:32:04,040 --> 00:32:07,665
Main question's whether it's finite or infinite.

613
00:32:07,665 --> 00:32:12,045
Maybe talk to your neighbor for a second and decide whether if Epsilon is fixed,

614
00:32:12,045 --> 00:32:16,260
whether it'll be- a_3 will be selected a finite or infinite number of times,

615
00:32:16,260 --> 00:32:18,000
and what that means in terms of the regret?

616
00:32:18,000 --> 00:32:47,580
[NOISE]

617
00:32:47,580 --> 00:32:49,095
Okay. I'm gonna have everybody vote.

618
00:32:49,095 --> 00:32:52,815
So if you think it's gonna be selected an infinite number of times, raise your hand.

619
00:32:52,815 --> 00:32:55,425
Great. So what's that mean in terms of regret,

620
00:32:55,425 --> 00:32:57,645
is there- it gonna be good or bad?

621
00:32:57,645 --> 00:32:59,250
It's gonna be bad.

622
00:32:59,250 --> 00:33:01,200
Great. Bad, bad regret.

623
00:33:01,200 --> 00:33:04,200
I mean, in general, regret's gonna be unbounded unfortunately in these cases.

624
00:33:04,200 --> 00:33:07,710
[LAUGHTER] Um, so we're always gonna unfortunately have infinite regret but, um,

625
00:33:07,710 --> 00:33:09,510
but the rate at which it grows can be

626
00:33:09,510 --> 00:33:11,910
[LAUGHTER] can be much smaller depending on the algorithm you do.

627
00:33:11,910 --> 00:33:15,255
[NOISE] So, um, in particular and,

628
00:33:15,255 --> 00:33:18,150
uh, yeah, so we can also think about it in this case.

629
00:33:18,150 --> 00:33:21,030
So if you have a large gap which we do for a_3

630
00:33:21,030 --> 00:33:24,975
here and we're gonna be selecting that arm an infinite number of times,

631
00:33:24,975 --> 00:33:27,090
then e-greedy is also gonna have,

632
00:33:27,090 --> 00:33:29,070
um, a large regret.

633
00:33:29,070 --> 00:33:32,145
So I like this plot, this plot comes from David Silver's slides.

634
00:33:32,145 --> 00:33:34,485
Um, uh, so if you explore forever,

635
00:33:34,485 --> 00:33:36,345
like, if you just do random, um,

636
00:33:36,345 --> 00:33:38,280
which we didn't discuss, but you could also do,

637
00:33:38,280 --> 00:33:40,560
then you're gonna have linear total regret, um,

638
00:33:40,560 --> 00:33:43,050
which means that with the number of time steps,

639
00:33:43,050 --> 00:33:46,080
t, you're- you're gonna scale linearly with t. Essentially,

640
00:33:46,080 --> 00:33:47,850
your regret is growing unboundedly and it's growing

641
00:33:47,850 --> 00:33:50,940
linearly which is equi- essentially proportio- I mean,

642
00:33:50,940 --> 00:33:53,160
it's gonna generally have a constant in front of it but

643
00:33:53,160 --> 00:33:56,865
um, it's gonna be a constant times the worst you could do at every time step.

644
00:33:56,865 --> 00:34:00,495
Because if you always select the worst arm at every time step,

645
00:34:00,495 --> 00:34:02,515
your regret will also grow linearly.

646
00:34:02,515 --> 00:34:04,205
So it's pretty bad.

647
00:34:04,205 --> 00:34:05,510
Um, if you explore never,

648
00:34:05,510 --> 00:34:07,235
if you do greedy, uh,

649
00:34:07,235 --> 00:34:09,230
then it also can be linear,

650
00:34:09,230 --> 00:34:12,025
and if you do e-greedy, it's also linear.

651
00:34:12,025 --> 00:34:14,909
So essentially, it means that all of these algorithms

652
00:34:14,909 --> 00:34:17,099
that we've been using so far can have really,

653
00:34:17,100 --> 00:34:18,465
really bad performance, um,

654
00:34:18,465 --> 00:34:20,054
certainly in bad cases,

655
00:34:20,054 --> 00:34:21,899
and so the critical question is whether or not

656
00:34:21,900 --> 00:34:23,699
it's better tha- it's possible to do better than that.

657
00:34:23,699 --> 00:34:27,239
So can we have sub- what's often called sublinear regret. So we want to have regret.

658
00:34:27,239 --> 00:34:29,159
If an algorithm's gonna be considered good in

659
00:34:29,159 --> 00:34:31,499
terms of its performance and its sample efficiency,

660
00:34:31,500 --> 00:34:33,764
we're gonna want its regret to grow sublinearly.

661
00:34:33,764 --> 00:34:39,209
Um, when we think about this,

662
00:34:39,210 --> 00:34:41,040
we're generally gonna think about, um,

663
00:34:41,040 --> 00:34:43,110
whether the bounds that we create,

664
00:34:43,110 --> 00:34:46,514
in the performance bounds are gonna be problem independent or problem dependent.

665
00:34:46,514 --> 00:34:48,209
So most of the bounds,

666
00:34:48,210 --> 00:34:49,650
i- it depends a little bit.

667
00:34:49,650 --> 00:34:53,864
For MDPs, most of the bounds that we can get are gonna be problem independent.

668
00:34:53,864 --> 00:34:56,639
For bandits, there's a lot of problem dependent bounds.

669
00:34:56,639 --> 00:35:00,045
Um, problem dependent bounds in the case of bandits mean that, um,

670
00:35:00,045 --> 00:35:04,800
the amount of regret that we get is gonna be a function of those gaps,

671
00:35:04,800 --> 00:35:07,500
and that should be sort of intuitive.

672
00:35:07,500 --> 00:35:10,080
So if you have- let's imagine that we just have two arms, like,

673
00:35:10,080 --> 00:35:13,710
a_1 and a_2 and the mean,

674
00:35:13,710 --> 00:35:16,480
um, the expected reward.

675
00:35:18,800 --> 00:35:24,420
So if this is, the expected reward is 1 and this is an expected reward of 0.001,

676
00:35:24,420 --> 00:35:27,660
intuitively it should be easier to figure out that arm one is better than arm

677
00:35:27,660 --> 00:35:32,110
two that if this is like 0.53 and this is 0.525.

678
00:35:32,930 --> 00:35:35,115
Because in one case,

679
00:35:35,115 --> 00:35:38,700
really hard to tell the difference between the mean of the two arms and the other case,

680
00:35:38,700 --> 00:35:40,620
the means are really, really far apart.

681
00:35:40,620 --> 00:35:44,760
So somewhat intuitively if the gap is really large,

682
00:35:44,760 --> 00:35:47,040
it should be easier for us to learn,

683
00:35:47,040 --> 00:35:49,620
and if it's really small, it should be harder. Yeah.

684
00:35:49,620 --> 00:35:51,090
Um, so if the, uh- [NOISE]

685
00:35:51,090 --> 00:35:51,660
is that [OVERLAPPING].

686
00:35:51,660 --> 00:35:53,715
[NOISE] Uh, so, uh,

687
00:35:53,715 --> 00:35:57,270
optimal reward is deterministic, for some actions.

688
00:35:57,270 --> 00:35:58,980
I mean, we have zero regret.

689
00:35:58,980 --> 00:36:00,540
[NOISE]

690
00:36:00,540 --> 00:36:01,560
Good question. Um, uh,

691
00:36:01,560 --> 00:36:03,480
the question is if the optimal are- if the,

692
00:36:03,480 --> 00:36:05,250
uh, optimal reward is,

693
00:36:05,250 --> 00:36:07,545
uh, are you just saying optimal reward?

694
00:36:07,545 --> 00:36:12,555
Yeah. The optimal reward is deterministic till we have zero regret, if you know it.

695
00:36:12,555 --> 00:36:16,500
So if you know that all the rewards of the arms are [NOISE] deterministic,

696
00:36:16,500 --> 00:36:17,550
then you just need to pull them once.

697
00:36:17,550 --> 00:36:19,485
Then you can make a good decision and then you're done.

698
00:36:19,485 --> 00:36:21,240
In general, these algorithms aren't going to know

699
00:36:21,240 --> 00:36:22,980
that information if you don't- even if it

700
00:36:22,980 --> 00:36:26,400
was deterministic you're still gonna have these other forms of bounds.

701
00:36:26,400 --> 00:36:31,260
So what about the greedy case then? What if it's deterministic?

702
00:36:31,260 --> 00:36:34,485
If it's determinant- if in a greedy [NOISE] case- it's a good question.

703
00:36:34,485 --> 00:36:37,125
In a greedy case, if your real rewards are deterministic,

704
00:36:37,125 --> 00:36:39,510
um, then you don't need- then you would have pulled all the arms once,

705
00:36:39,510 --> 00:36:40,620
and you will make no mistakes.

706
00:36:40,620 --> 00:36:43,680
You'll make, uh, you'll have zero regret basically from that point onwards.

707
00:36:43,680 --> 00:36:46,440
So we'd consider that as just like you'd have some initial constant regret,

708
00:36:46,440 --> 00:36:50,385
and then afterward it would be independent of t. Did you have a question?

709
00:36:50,385 --> 00:36:51,320
Yeah. [NOISE] Um-

710
00:36:51,320 --> 00:36:52,320
Remind me your name.

711
00:36:52,320 --> 00:36:56,505
[NOISE] Is it also a function of the variance of each arm-

712
00:36:56,505 --> 00:36:57,150
Oh, good question.

713
00:36:57,150 --> 00:37:00,570
- [OVERLAPPING] because in that case like you take upon [inaudible]

714
00:37:00,570 --> 00:37:02,715
Yeah. Great question. So we're not gonna ta- uh,

715
00:37:02,715 --> 00:37:05,400
question is whether it also depends on the variance in addition to the mean.

716
00:37:05,400 --> 00:37:07,830
Um, yes, we're not gonna talk about that,

717
00:37:07,830 --> 00:37:09,870
um, uh, but in addition to problem dependent bounds,

718
00:37:09,870 --> 00:37:12,060
you can certainly think about parametric, like,

719
00:37:12,060 --> 00:37:14,085
if you have some parametric knowledge

720
00:37:14,085 --> 00:37:16,740
on the distribution of the rewards, then you can exploit it.

721
00:37:16,740 --> 00:37:19,275
Um, so if you know it's a Gaussian or other things like that.

722
00:37:19,275 --> 00:37:22,230
Um, I think in general if you know them,

723
00:37:22,230 --> 00:37:24,120
or like if you have information about the moments,

724
00:37:24,120 --> 00:37:25,395
then you should be able to exploit it.

725
00:37:25,395 --> 00:37:26,955
Most of the information that I've seen is,

726
00:37:26,955 --> 00:37:28,695
um, looking at the mean and the variance.

727
00:37:28,695 --> 00:37:31,170
We very frequently throughout a lot of

728
00:37:31,170 --> 00:37:33,555
this stuff is are gonna assume that our rewards are bounded.

729
00:37:33,555 --> 00:37:36,915
Um, that's gonna be important for most of the proofs that we do,

730
00:37:36,915 --> 00:37:39,945
even without making any other parametric assumptions.

731
00:37:39,945 --> 00:37:43,215
Okay. But then the other version of this is problem independent,

732
00:37:43,215 --> 00:37:45,630
which just says [NOISE] regardless of the,

733
00:37:45,630 --> 00:37:47,745
of the domain you're in, regardless of the gaps,

734
00:37:47,745 --> 00:37:50,250
can we also show that, um, uh,

735
00:37:50,250 --> 00:37:52,380
regardless of any structure of the problem,

736
00:37:52,380 --> 00:37:54,885
can we still ensure that regret grows up linearly,

737
00:37:54,885 --> 00:37:57,760
and that's what we're gonna mostly focus on today.

738
00:37:58,190 --> 00:38:00,360
So I think lower bounds,

739
00:38:00,360 --> 00:38:03,915
lower theoretical bounds are- are helpful to try to understand how hard a problem is.

740
00:38:03,915 --> 00:38:08,310
So I said, is it possible for something to be sublinear, um,

741
00:38:08,310 --> 00:38:11,370
and there's been previous work to look at sort of,

742
00:38:11,370 --> 00:38:14,265
well, how much does the regret have to grow?

743
00:38:14,265 --> 00:38:16,350
So, um, in this case,

744
00:38:16,350 --> 00:38:18,300
this is regret we're mostly getting,

745
00:38:18,300 --> 00:38:20,190
um, to, mu's, to write out regret.

746
00:38:20,190 --> 00:38:24,045
But in this case they- they prove that in terms of the gaps, um,

747
00:38:24,045 --> 00:38:27,600
and the similarity in the dis- distributions in terms of the KL divergence,

748
00:38:27,600 --> 00:38:33,780
the KL divergence, that you can show a lower bound on how much regret grows.

749
00:38:33,780 --> 00:38:36,690
[NOISE] So this is where there's a sort of

750
00:38:36,690 --> 00:38:40,050
unfortunate [NOISE] aspect of regret growing unboundedly comes up.

751
00:38:40,050 --> 00:38:43,230
If you don't make any other assumptions on the distributions of your rewards,

752
00:38:43,230 --> 00:38:45,765
um, uh, in general,

753
00:38:45,765 --> 00:38:47,760
the regret of your, um,

754
00:38:47,760 --> 00:38:49,439
your regret will grow unboundedly,

755
00:38:49,439 --> 00:38:52,380
and it will ha- do so in terms of these gaps,

756
00:38:52,380 --> 00:38:54,255
and the KL divergence.

757
00:38:54,255 --> 00:38:57,240
But it's still sublinear. So that's nice.

758
00:38:57,240 --> 00:39:02,140
So it's growing logarithmically with t here. T is the time steps.

759
00:39:02,780 --> 00:39:05,610
It's encouraging that, like, our lower bound

760
00:39:05,610 --> 00:39:07,950
suggests that there's room for traction, right?

761
00:39:07,950 --> 00:39:09,540
That we can definitely, um,

762
00:39:09,540 --> 00:39:12,570
at least there's no formal result that says it has to be linear,

763
00:39:12,570 --> 00:39:15,330
it says no, we should be able to grow much slower.

764
00:39:15,330 --> 00:39:18,090
So how could we do- why- how would we maybe do this?

765
00:39:18,090 --> 00:39:22,095
So this is now, um, we've talked about a particular framework which is regret,

766
00:39:22,095 --> 00:39:23,700
we talked about a setting, which is bandits,

767
00:39:23,700 --> 00:39:25,740
and now we're gonna talk about an approach which is,

768
00:39:25,740 --> 00:39:27,990
um, optimism in the face of uncertainty.

769
00:39:27,990 --> 00:39:32,595
And the idea is simply to choose actions which might have high value.

770
00:39:32,595 --> 00:39:34,965
Why should we do this? Um.

771
00:39:34,965 --> 00:39:39,030
I have a question on the- in the previous slide, um,

772
00:39:39,030 --> 00:39:42,540
so is that true at every t, or only in the one

773
00:39:42,540 --> 00:39:46,780
of because as it's really isn't that just saying that it's greater than infinity?

774
00:39:47,450 --> 00:39:50,445
All right. Um, that's a good question.

775
00:39:50,445 --> 00:39:52,950
Question is about whether i- it holds.

776
00:39:52,950 --> 00:39:54,420
I think this holds on every time step,

777
00:39:54,420 --> 00:39:55,920
I'd have to check the exact, like,

778
00:39:55,920 --> 00:39:58,245
way that they wrote this, um, uh,

779
00:39:58,245 --> 00:40:00,090
there's also constants, um,

780
00:40:00,090 --> 00:40:03,810
but I think this should hold on a per time step basis.

781
00:40:03,810 --> 00:40:06,690
We're really saying that as time is going along,

782
00:40:06,690 --> 00:40:07,890
it should be true [OVERLAPPING]

783
00:40:07,890 --> 00:40:10,080
Yeah. The limit as t goes large,

784
00:40:10,080 --> 00:40:12,210
this is where I'd have to look back at the original paper, um,

785
00:40:12,210 --> 00:40:15,569
there's probably additional constant terms which are transitory,

786
00:40:15,569 --> 00:40:19,500
um, and so this is probably the dominant term as t goes large.

787
00:40:19,500 --> 00:40:22,890
But in a lot of the regret bounds particularly in our MDP cases,

788
00:40:22,890 --> 00:40:25,560
we often have transitory terms that are independent of the time step,

789
00:40:25,560 --> 00:40:26,970
but are still large early on.

790
00:40:26,970 --> 00:40:29,535
That's my guess. But great question.

791
00:40:29,535 --> 00:40:32,955
Okay. So optimism in the face of uncertainty,

792
00:40:32,955 --> 00:40:36,765
um, says that we should choose actions that might have a high value.

793
00:40:36,765 --> 00:40:39,210
Why? Well, there's two possible outcomes.

794
00:40:39,210 --> 00:40:41,310
If we pick something that we think might be good,

795
00:40:41,310 --> 00:40:44,140
um, one is it's good.

796
00:40:47,390 --> 00:40:49,815
So if it is good,

797
00:40:49,815 --> 00:40:52,365
or I'll be more precise in this case.

798
00:40:52,365 --> 00:40:59,650
So let's say we select, so, a_1 and 1 is a_1 has high reward.

799
00:41:02,210 --> 00:41:05,370
So that's good. If we, um,

800
00:41:05,370 --> 00:41:07,770
if we took an action and it actually does- and we- because we thought it

801
00:41:07,770 --> 00:41:10,140
might have high reward and actually does have high reward,

802
00:41:10,140 --> 00:41:12,165
then we're gonna have small regret. So that's good.

803
00:41:12,165 --> 00:41:13,635
Um, what's the other outcome?

804
00:41:13,635 --> 00:41:15,750
a_1 does not have high reward.

805
00:41:15,750 --> 00:41:18,630
Has lower has, um,

806
00:41:18,630 --> 00:41:19,755
when we sample this,

807
00:41:19,755 --> 00:41:23,370
we get our r for a_1 with lower reward.

808
00:41:23,370 --> 00:41:32,850
[NOISE]

809
00:41:32,850 --> 00:41:35,640
Well, if we get something with low reward, we learn something.

810
00:41:35,640 --> 00:41:37,770
So, we're like, hey, you know, we tried that restaurant again,

811
00:41:37,770 --> 00:41:38,880
we thought it was great the first time.

812
00:41:38,880 --> 00:41:40,515
The second time it was horrible.

813
00:41:40,515 --> 00:41:43,080
So, now we've learned that that restaurant is not as good and we

814
00:41:43,080 --> 00:41:46,155
update our estimate of how good that restaurant is.

815
00:41:46,155 --> 00:41:50,910
And that means we don't think that a- that action has as high a value as it did before.

816
00:41:50,910 --> 00:41:53,730
So, essentially, either the world really is great,

817
00:41:53,730 --> 00:41:55,875
and in which case that's great, we're going to have low regret.

818
00:41:55,875 --> 00:41:58,440
Or the world is not that great and then we learn something.

819
00:41:58,440 --> 00:42:00,730
So, this gave us information.

820
00:42:03,230 --> 00:42:05,850
And so this b,

821
00:42:05,850 --> 00:42:08,490
acting optimistically gives us

822
00:42:08,490 --> 00:42:11,310
both information about the reward or allows us to achieve high reward.

823
00:42:11,310 --> 00:42:13,350
And so it turns out to have been a really nice principle.

824
00:42:13,350 --> 00:42:19,830
It's been around since at least Leslie Kaelbling in 1993.

825
00:42:19,830 --> 00:42:23,828
Introduced this idea of sort of interval estimation and then there started

826
00:42:23,828 --> 00:42:28,390
to be a lot of analysis of these types of optimism under uncertainty techniques.

827
00:42:29,720 --> 00:42:32,610
So, how can we do this more formally, or like,

828
00:42:32,610 --> 00:42:34,440
where would we get to be more precise

829
00:42:34,440 --> 00:42:36,690
about what it means for an action to have a high-value?

830
00:42:36,690 --> 00:42:40,485
Let's imagine that we estimate an upper confidence bound for each action value,

831
00:42:40,485 --> 00:42:42,750
such that the real value of

832
00:42:42,750 --> 00:42:49,630
that action is less than or equal to the upper confidence bound with high probability.

833
00:42:50,660 --> 00:42:54,030
And those upper confidence bounds in general are going to depend

834
00:42:54,030 --> 00:42:57,195
on how many times we've selected that particular action.

835
00:42:57,195 --> 00:43:01,575
Because we would like it to be such that if we've selected that action a lot,

836
00:43:01,575 --> 00:43:05,295
that upper confidence bound should be pre- pretty close to Q of A.

837
00:43:05,295 --> 00:43:07,365
And if we haven't selected it very much,

838
00:43:07,365 --> 00:43:09,880
maybe we're really optimistic.

839
00:43:10,700 --> 00:43:14,580
And then we can divide an upper confidence bound bandit algorithm

840
00:43:14,580 --> 00:43:18,165
by just selecting whichever action has the highest upper confidence bound.

841
00:43:18,165 --> 00:43:21,555
So, for every single action we maintain an upper confidence bound,

842
00:43:21,555 --> 00:43:23,625
and then we just select whichever one has the max,

843
00:43:23,625 --> 00:43:28,590
and then we update the upper confidence bound for that action after we take it.

844
00:43:28,590 --> 00:43:35,670
So, a UCB algorithm would for t = 1 dot, dot, dot,

845
00:43:35,670 --> 00:43:41,020
we would first have an initialization phase where we pull each arm once,

846
00:43:41,510 --> 00:43:52,650
each arm once, and then we compute UT of at for all A.

847
00:43:52,650 --> 00:43:54,975
And then for T = t dot, dot, dot,

848
00:43:54,975 --> 00:43:59,590
we would select at equaling this arg max,

849
00:44:02,810 --> 00:44:05,940
and then we would get a reward that is sampled from

850
00:44:05,940 --> 00:44:09,970
the true reward distribution for that arm.

851
00:44:12,950 --> 00:44:18,330
And then we would update Ut of

852
00:44:18,330 --> 00:44:26,050
at and all other arms.

853
00:44:28,940 --> 00:44:33,210
Turns out that often we have to update not just the action of the arm that we took,

854
00:44:33,210 --> 00:44:35,895
but the action of all the other arms we took too.

855
00:44:35,895 --> 00:44:38,205
And this basically comes into,

856
00:44:38,205 --> 00:44:39,420
you, you don't have to do that,

857
00:44:39,420 --> 00:44:42,180
but in terms of the theory we often have to do that in order to

858
00:44:42,180 --> 00:44:45,180
account for high probability bounds.

859
00:44:45,180 --> 00:44:47,160
And we'll see more about that in just a second.

860
00:44:47,160 --> 00:44:49,260
So, every time you get a reward,

861
00:44:49,260 --> 00:44:51,540
you update the upper confidence bounds of all your arms,

862
00:44:51,540 --> 00:44:55,810
and then you select the next action and you repeat this just over and over again.

863
00:44:56,450 --> 00:45:00,165
Okay. So, how are we going to define these U of T?

864
00:45:00,165 --> 00:45:04,545
So, we're going to use Hoeffding's inequality, so a refresher.

865
00:45:04,545 --> 00:45:10,395
In Hoeffding's inequality, we can apply this to a set of iid random variables.

866
00:45:10,395 --> 00:45:14,440
We're going to assume right now that all of them are bounded between 0 and 1,

867
00:45:14,450 --> 00:45:19,155
and we're going to define our sample mean just to be a,

868
00:45:19,155 --> 00:45:21,990
the average over all of those variables.

869
00:45:21,990 --> 00:45:27,880
And then what Hoeffding says is that the probability of that expected mean is,

870
00:45:28,220 --> 00:45:30,795
like, this is the true expected mean.

871
00:45:30,795 --> 00:45:32,460
So, this is the true mean.

872
00:45:32,460 --> 00:45:35,380
This is our empirical mean.

873
00:45:36,500 --> 00:45:40,590
And this is, you can think of this as like some epsilon.

874
00:45:40,590 --> 00:45:42,760
This is just some constant.

875
00:45:47,270 --> 00:45:51,150
Is the probability that your true mean is greater

876
00:45:51,150 --> 00:45:54,765
than the empirical mean plus some constant U,

877
00:45:54,765 --> 00:45:59,940
is less than or equal to expo- the exponential minus 2nu squared.

878
00:45:59,940 --> 00:46:02,770
So, this is the number of samples we have.

879
00:46:04,700 --> 00:46:12,390
Okay. So, we can also invert this to say,

880
00:46:12,390 --> 00:46:16,194
if you want this to be true with a certain probability,

881
00:46:16,194 --> 00:46:24,265
you can pick a mu so that Xn plus mu is gonna be at least as large as the real mean.

882
00:46:24,265 --> 00:46:26,520
So, let's say what we wanna do is,

883
00:46:26,520 --> 00:46:28,020
we want that this to,

884
00:46:28,020 --> 00:46:30,435
we want that the empirical mean plus mu,

885
00:46:30,435 --> 00:46:32,910
the probability that that's less than the real mean,

886
00:46:32,910 --> 00:46:36,910
to equal to delta over T squared.

887
00:46:36,910 --> 00:46:41,255
And we'll see why we might want to choose that particular probability shortly,

888
00:46:41,255 --> 00:46:44,165
but let's imagine for a second that's what we want our probability to be,

889
00:46:44,165 --> 00:46:46,670
since then we can solve for what mu has to be.

890
00:46:46,670 --> 00:46:53,880
Okay, So, that's exponential -2nu squared has to equal to delta over t squared.

891
00:46:57,310 --> 00:47:03,830
And then what we do is we just solve for what mu is. Thanks for letting me know.

892
00:47:03,830 --> 00:47:07,920
Okay. So, mu in this case is going to be equal to

893
00:47:07,920 --> 00:47:15,730
the square root 1 over 2n log of t squared over delta.

894
00:47:23,480 --> 00:47:26,940
So, I just solve for that equation. What does that tell us?

895
00:47:26,940 --> 00:47:28,935
That says that if we do,

896
00:47:28,935 --> 00:47:31,305
if we define our ut of,

897
00:47:31,305 --> 00:47:34,485
or in this case we define,

898
00:47:34,485 --> 00:47:36,945
I'll keep with the same notation as for Hoeffding.

899
00:47:36,945 --> 00:47:41,295
So, if we have Xn plus mu,

900
00:47:41,295 --> 00:47:43,290
with that particular choice of mu,

901
00:47:43,290 --> 00:47:48,420
that's generally going to be greater than or equal to the true expected value of X,

902
00:47:48,420 --> 00:47:55,270
with probability greater than or equal to 1 - delta over t squared.

903
00:47:58,880 --> 00:48:03,465
So, this, Hoeffding's inequality gives us a way to define an upper bound.

904
00:48:03,465 --> 00:48:05,700
Because instead of these being Xs right now,

905
00:48:05,700 --> 00:48:08,760
you can imagine those are just pulled from our arm and those are all rewards.

906
00:48:08,760 --> 00:48:12,540
And so this says, if you take your empirical average of your words so far,

907
00:48:12,540 --> 00:48:14,955
and you add in this upper bound,

908
00:48:14,955 --> 00:48:21,030
which depends on the number of times we pulled that arm and t. So,

909
00:48:21,030 --> 00:48:24,975
t here notices the total number of time steps we pulled any arms,

910
00:48:24,975 --> 00:48:27,990
and n is the number of times we pulled that arm.

911
00:48:27,990 --> 00:48:29,805
So, they're not the same thing.

912
00:48:29,805 --> 00:48:32,160
So, we're [NOISE] inside of this competence bound,

913
00:48:32,160 --> 00:48:34,230
we have a competence bound that is decreasing at

914
00:48:34,230 --> 00:48:36,855
a rate of how many times we pull this particular arm,

915
00:48:36,855 --> 00:48:39,660
and then we have a log term which is increasing at

916
00:48:39,660 --> 00:48:43,005
the rate of the number of times we pulled any arm.

917
00:48:43,005 --> 00:48:46,110
And that is the reason why after each time step,

918
00:48:46,110 --> 00:48:50,310
we have to up- um, update the upper confidence bounds of all the arms.

919
00:48:50,310 --> 00:48:54,510
So, you kind of have these two competing rates that are going on.

920
00:48:54,510 --> 00:48:56,070
As you pull an arm more,

921
00:48:56,070 --> 00:48:59,565
you're gonna get a better estimate of its reward, so it's shrinking.

922
00:48:59,565 --> 00:49:03,285
But then you also have this slower growing term, this log,

923
00:49:03,285 --> 00:49:08,760
which is increasing with the number of time steps. All right.

924
00:49:08,760 --> 00:49:11,400
So, this is one way we could define our upper confidence bounds.

925
00:49:11,400 --> 00:49:14,580
So, we could use this in the case of our rewards.

926
00:49:14,580 --> 00:49:21,465
So, we might wanna say that ut of at is equal to our empirical average of that arm,

927
00:49:21,465 --> 00:49:27,465
+ 1 over 2, the number of times we pulled that arm,

928
00:49:27,465 --> 00:49:32,230
times log of t squared divided by delta.

929
00:49:36,990 --> 00:49:40,780
So this is how, one way for us to define our upper confidence bounds.

930
00:49:40,780 --> 00:49:47,005
[NOISE] All right.

931
00:49:47,005 --> 00:49:48,880
So now the next question is.

932
00:49:48,880 --> 00:49:50,080
Okay. So we've done that,

933
00:49:50,080 --> 00:49:52,420
how is that gonna help us in terms of showing that maybe

934
00:49:52,420 --> 00:49:56,350
the regret of something which is optimistic is actually sub-linear?

935
00:49:56,350 --> 00:50:00,580
Okay. So what we're gonna do now is,

936
00:50:00,580 --> 00:50:02,890
um, I'll do a quick poll.

937
00:50:02,890 --> 00:50:05,575
Do you guys want me to write it on here or do you guys want me to do it on the board?

938
00:50:05,575 --> 00:50:08,710
So raise your hand if you want it on the board. All right.

939
00:50:08,710 --> 00:50:11,200
Raise your hand if you want it on here. Okay. We'll do

940
00:50:11,200 --> 00:50:14,050
this next part on the board [LAUGHTER] that was easy. Was there a question in the back?

941
00:50:14,050 --> 00:50:18,610
Yeah. I have question isn't this derivation about t because it seems like.

942
00:50:18,610 --> 00:50:18,970
[OVERLAPPING] About what?

943
00:50:18,970 --> 00:50:20,830
You first introduced it, about t?

944
00:50:20,830 --> 00:50:21,190
Yes.

945
00:50:21,190 --> 00:50:23,770
It seems you first introduced it there was basically a constant the way you moved

946
00:50:23,770 --> 00:50:25,480
around but then later you're saying that

947
00:50:25,480 --> 00:50:27,310
that's actually the time that we we're updating every time step.

948
00:50:27,310 --> 00:50:29,815
So how are we able to do that?

949
00:50:29,815 --> 00:50:31,000
Yeah. It's a good question.

950
00:50:31,000 --> 00:50:32,245
So, um, you're right.

951
00:50:32,245 --> 00:50:34,135
And I'm being slightly imprecise about this.

952
00:50:34,135 --> 00:50:36,310
If you know what the time horizon is that you're gonna

953
00:50:36,310 --> 00:50:40,855
be acting on a bandit, you could set t to be the maximum.

954
00:50:40,855 --> 00:50:43,675
So you, if you know that you're gonna act for t time steps,

955
00:50:43,675 --> 00:50:45,730
you can plug that in and then

956
00:50:45,730 --> 00:50:49,685
your confidence bounds are, then that log term is fixed basically.

957
00:50:49,685 --> 00:50:53,155
Um, in online settings if you don't know that,

958
00:50:53,155 --> 00:50:56,290
you can also constantly be updating it with a time step.

959
00:50:56,290 --> 00:50:58,735
It's a good question. Yeah.

960
00:50:58,735 --> 00:51:00,880
How is delta decided?

961
00:51:00,880 --> 00:51:01,630
How is what? Pardon.

962
00:51:01,630 --> 00:51:03,535
How is delta, is like what is delta?

963
00:51:03,535 --> 00:51:07,540
Okay. Good question. Um, so, question is what is delta.

964
00:51:07,540 --> 00:51:09,655
What we're gonna, I did not tell you, uh,

965
00:51:09,655 --> 00:51:11,815
in this ca- well, in this case it's telling us,

966
00:51:11,815 --> 00:51:14,050
um, it's specifying what is the probability

967
00:51:14,050 --> 00:51:16,555
this is holding like what this inequality is holding.

968
00:51:16,555 --> 00:51:20,320
Later we're gonna pro- provide a regret bound that is high probability.

969
00:51:20,320 --> 00:51:23,530
So we're gonna say we're gonna have a regret bound which is

970
00:51:23,530 --> 00:51:26,920
something that like with probability is 1 minus a function of delta,

971
00:51:26,920 --> 00:51:29,635
um, your regret will be sub-linear.

972
00:51:29,635 --> 00:51:31,390
So that's how.

973
00:51:31,390 --> 00:51:35,815
You can get expected regret bounds too and the UCB paper which, um,

974
00:51:35,815 --> 00:51:40,030
one of the original UCB papers provides an expected bound but I thought it was a little,

975
00:51:40,030 --> 00:51:42,160
the, this bound was a little bit easier to do in class.

976
00:51:42,160 --> 00:51:45,710
So I thought I would do the high probability bound. Yeah.

977
00:51:45,710 --> 00:51:47,895
So before we were talking about regret,

978
00:51:47,895 --> 00:51:50,430
I didn't exactly understand how you use

979
00:51:50,430 --> 00:51:53,700
regret to update your estimate of the action value.

980
00:51:53,700 --> 00:51:56,010
Oh, good question. Um, so question is,

981
00:51:56,010 --> 00:51:57,990
do we use or how would we use

982
00:51:57,990 --> 00:52:01,140
the regret bound, the regret to update our estimate action, we don't.

983
00:52:01,140 --> 00:52:03,390
Regret is just a tool to analyze our algorithm.

984
00:52:03,390 --> 00:52:05,790
Great clarification. So regret is a way for

985
00:52:05,790 --> 00:52:08,160
us to analyze whether or not an algorithm is gonna be good or bad

986
00:52:08,160 --> 00:52:12,895
in terms of how fast the regret gro- grows but it's not used in the algorithm itself.

987
00:52:12,895 --> 00:52:14,695
The algorithm doesn't compute regret.

988
00:52:14,695 --> 00:52:17,510
And it's not used in terms of the updating.

989
00:52:18,390 --> 00:52:21,985
Excuse me. Okay. So actually I'll leave this one up here.

990
00:52:21,985 --> 00:52:23,950
So you guys can continue to see that.

991
00:52:23,950 --> 00:52:25,990
All right. So let's do our proof.

992
00:52:25,990 --> 00:52:28,870
So what we're gonna wanna do now is we're gonna wanna try to

993
00:52:28,870 --> 00:52:31,210
prove something about, um,

994
00:52:31,210 --> 00:52:36,610
the regret but before and how quickly it grows for the upper confidence bound algorithm.

995
00:52:36,610 --> 00:52:38,260
But before I prove that,

996
00:52:38,260 --> 00:52:41,530
I'm gonna try to argue to you that, um,

997
00:52:41,530 --> 00:52:47,230
we're gonna look at, sort of, the probability of failure of these regret bounds,

998
00:52:47,230 --> 00:52:48,730
of these confidence bounds.

999
00:52:48,730 --> 00:52:51,520
So what I said here is that [NOISE] we're gonna define

1000
00:52:51,520 --> 00:52:54,250
these upper confidence bounds like this in

1001
00:52:54,250 --> 00:52:56,920
terms of the empirical mean for that arm so far plus

1002
00:52:56,920 --> 00:53:00,145
this term that depends on the number of times we pulled that arm.

1003
00:53:00,145 --> 00:53:02,620
And what I wanna convince you now is,

1004
00:53:02,620 --> 00:53:04,210
what is the probability,

1005
00:53:04,210 --> 00:53:15,230
what is the probability that on one step, that on some step,

1006
00:53:16,530 --> 00:53:20,900
step the confidence bounds will fail to hold.

1007
00:53:34,200 --> 00:53:37,255
Why is this bad?

1008
00:53:37,255 --> 00:53:41,410
Okay. So we wanna bound the probability that on

1009
00:53:41,410 --> 00:53:45,775
any step, excuse me, as we're running our algorithm that our confidence bounds fail to hold.

1010
00:53:45,775 --> 00:53:48,805
Why? Because if they all hold,

1011
00:53:48,805 --> 00:53:50,845
we can guarantee we're gonna be making some,

1012
00:53:50,845 --> 00:53:53,170
um, we're gonna have some nice properties.

1013
00:53:53,170 --> 00:53:58,545
Okay. So note, if all the confidence bounds hold,

1014
00:53:58,545 --> 00:54:02,790
like, on every step then we can ensure the following.

1015
00:54:02,790 --> 00:54:11,220
So if all confidence hold, bounds hold then Ut of at,

1016
00:54:11,220 --> 00:54:14,139
this is the actual arm we selected,

1017
00:54:15,740 --> 00:54:20,440
is gonna be greater than q of a star.

1018
00:54:20,690 --> 00:54:25,630
The real value. The real value of the optimal arm.

1019
00:54:26,150 --> 00:54:32,290
Okay. So why is this true?

1020
00:54:34,550 --> 00:54:39,610
There's two cases here, either

1021
00:54:39,610 --> 00:54:45,730
at is equal to a star,  or at is not equal to a star.

1022
00:54:45,730 --> 00:54:50,680
So let's just take a second and maybe talk to your neighbor [NOISE] to say,

1023
00:54:50,680 --> 00:54:54,210
if it's the case that our confidence bounds hold which means

1024
00:54:54,210 --> 00:54:57,710
that really is the case that we have ut with,

1025
00:54:57,710 --> 00:55:00,915
um, that this confidence bound is,

1026
00:55:00,915 --> 00:55:05,580
um, gonna be greater than or equal to the mean for that arm.

1027
00:55:05,580 --> 00:55:07,780
So if these are true confidence bounds,

1028
00:55:07,780 --> 00:55:08,920
this equation is holding,

1029
00:55:08,920 --> 00:55:10,030
we're not getting a failure.

1030
00:55:10,030 --> 00:55:14,560
So we know that Q t which is this is gonna be greater than, um,

1031
00:55:14,560 --> 00:55:17,035
the real expected value for that arm.

1032
00:55:17,035 --> 00:55:20,350
Okay. So if that's true,

1033
00:55:20,350 --> 00:55:24,050
then this is gonna hold at every time step.

1034
00:55:27,450 --> 00:55:30,670
So maybe, let's take a neighbor or if it's not clear what I'm

1035
00:55:30,670 --> 00:55:33,850
asking or how to think about that, feel free to raise your hand too.

1036
00:55:33,850 --> 00:55:38,800
So there's two cases either the arm that we selected is a star

1037
00:55:38,800 --> 00:55:44,350
or the arm we selected is not a star and in both cases, this is gonna hold,

1038
00:55:44,350 --> 00:55:47,540
if the confidence bounds are correct.

1039
00:55:51,060 --> 00:55:54,490
So maybe let's take a second to to think about this or feel free

1040
00:55:54,490 --> 00:55:56,965
to raise your hand if it's not clear how to,

1041
00:55:56,965 --> 00:55:59,120
how to get started on that.

1042
00:56:00,720 --> 00:56:03,430
I wanna be just be clear here so.

1043
00:56:03,430 --> 00:56:42,100
[NOISE]

1044
00:56:42,100 --> 00:56:46,465
I just wanted to note at the top there that if the confidence bounds hold,

1045
00:56:46,465 --> 00:56:50,305
then that upper confidence bounds which is equal to that is going to be greater than

1046
00:56:50,305 --> 00:56:54,920
the real expected value for that arm. Yeah.

1047
00:56:54,920 --> 00:56:58,165
On the confidence bound on your other equation, you have written over there?

1048
00:56:58,165 --> 00:56:58,660
Yeah.

1049
00:56:58,660 --> 00:57:00,970
So you're saying that the optimal,

1050
00:57:00,970 --> 00:57:03,250
like your optimal Q value,

1051
00:57:03,250 --> 00:57:06,055
should be less than all the confidence bounds for any action?

1052
00:57:06,055 --> 00:57:07,240
No, good question.

1053
00:57:07,240 --> 00:57:09,475
So just need to clarify what it is.

1054
00:57:09,475 --> 00:57:12,100
This is saying for the arm that you selected,

1055
00:57:12,100 --> 00:57:15,730
the upper confidence bound of the arm you selected is- has

1056
00:57:15,730 --> 00:57:19,600
a value that- that- that upper confidence bound that you use to choose the arm,

1057
00:57:19,600 --> 00:57:20,860
whichever arm you selected,

1058
00:57:20,860 --> 00:57:28,615
the upper confidence bound of that arm is higher than the true value of the optimal arm.

1059
00:57:28,615 --> 00:57:31,600
That's what this equation is saying.

1060
00:57:31,600 --> 00:57:35,875
Saying that if the confidence bounds hold on all time steps,

1061
00:57:35,875 --> 00:57:37,660
which they might not, but let's say that they do

1062
00:57:37,660 --> 00:57:39,310
because these are only high probability bounds,

1063
00:57:39,310 --> 00:57:41,260
but if they hold on all time steps,

1064
00:57:41,260 --> 00:57:43,405
then whatever arm you selected,

1065
00:57:43,405 --> 00:57:48,235
its upper confidence bound is higher than the value of the true arm.

1066
00:57:48,235 --> 00:57:50,530
The real value of the true arm.

1067
00:57:50,530 --> 00:57:54,280
Okay, and I just wanted

1068
00:57:54,280 --> 00:57:57,295
to be clear what I mean for the confidence bound to hold, I put this up there.

1069
00:57:57,295 --> 00:58:01,600
So that means that the upper confidence bound of an arm holding means that

1070
00:58:01,600 --> 00:58:03,340
that upper confidence bound which is defined in

1071
00:58:03,340 --> 00:58:07,190
that way is greater than the true value of that arm.

1072
00:58:11,430 --> 00:58:13,990
So let's work- work through this a little bit.

1073
00:58:13,990 --> 00:58:16,330
So let's say there's two cases.

1074
00:58:16,330 --> 00:58:21,650
So if A T is equal to A star,

1075
00:58:22,050 --> 00:58:25,255
then that's what this is saying is that saying is,

1076
00:58:25,255 --> 00:58:31,240
is Ut of a star greater than Q of a star?

1077
00:58:31,240 --> 00:58:34,825
Does that hold if the confidence bounds hold?

1078
00:58:34,825 --> 00:58:37,660
Yes. By definition. So if you look up there.

1079
00:58:37,660 --> 00:58:42,100
So the upper confidence bounds if it holds for an action for

1080
00:58:42,100 --> 00:58:44,320
that- the upper confidence bound for an action has to

1081
00:58:44,320 --> 00:58:46,930
be bigger than the mean for that action.

1082
00:58:46,930 --> 00:58:49,375
If that upper confidence bounds hold.

1083
00:58:49,375 --> 00:58:51,850
Okay. So this ho- this works.

1084
00:58:51,850 --> 00:58:54,655
So if we really selected the optimal action,

1085
00:58:54,655 --> 00:58:56,500
we've defined our upper confidence bounds,

1086
00:58:56,500 --> 00:59:00,760
so they really are better than the mean of that arm, and so this holds.

1087
00:59:00,760 --> 00:59:05,080
The other case is that at is not equal to a star.

1088
00:59:05,080 --> 00:59:06,640
So what does that mean?

1089
00:59:06,640 --> 00:59:13,610
That means that Ut of at is greater than Ut of a star.

1090
00:59:14,250 --> 00:59:17,035
Because otherwise, we would have selected a star.

1091
00:59:17,035 --> 00:59:21,970
It means that some other arm had a higher upper confidence bound than the optimal action,

1092
00:59:21,970 --> 00:59:26,170
and we know that this is greater than Q of a star.

1093
00:59:26,170 --> 00:59:35,620
Okay? So if

1094
00:59:35,620 --> 00:59:37,150
the confidence bound holds,

1095
00:59:37,150 --> 00:59:42,160
we know at every single time step the upper confidence bound of the arm we selected is

1096
00:59:42,160 --> 00:59:49,330
better than the true mean of the optimal arm. Yes.

1097
00:59:49,330 --> 00:59:52,194
Is that true in the epsilon greedy case as well?

1098
00:59:52,194 --> 00:59:55,135
Is it true in epsilon greedy case as well?

1099
00:59:55,135 --> 00:59:59,455
I know I- I don't follow your question yet.

1100
00:59:59,455 --> 01:00:06,505
Like, you're selecting this arm using some strategy, right?

1101
01:00:06,505 --> 01:00:07,000
Yeah.

1102
01:00:07,000 --> 01:00:11,170
And it gets some maximizing action, right?

1103
01:00:11,170 --> 01:00:14,590
No. Ut, so this only holds,

1104
01:00:14,590 --> 01:00:20,050
this part only holds because we're picking the arg max, you're not going to be able to see this,

1105
01:00:20,050 --> 01:00:25,285
but the arg max A of Ut of at. So that first inequality,

1106
01:00:25,285 --> 01:00:28,015
only- well, there might be other algorithms that t holds for too,

1107
01:00:28,015 --> 01:00:30,550
but it holds in particular for the upper confidence bound.

1108
01:00:30,550 --> 01:00:37,075
Great- great question. Okay. So this says if we could get it,

1109
01:00:37,075 --> 01:00:38,500
and we will see shortly why that matters,

1110
01:00:38,500 --> 01:00:39,940
but kind of intuitively.

1111
01:00:39,940 --> 01:00:43,195
This says if the confidence bounds hold,

1112
01:00:43,195 --> 01:00:45,490
then we know that separate confidence bound of the arm we

1113
01:00:45,490 --> 01:00:48,415
select is going to be better than the optimal arm.

1114
01:00:48,415 --> 01:00:51,970
And the reason we're going to want that is later when we're doing the regret bounds,

1115
01:00:51,970 --> 01:00:54,700
we do not want to deal with properties we don't

1116
01:00:54,700 --> 01:00:58,060
observe namely the value of the optimal arm.

1117
01:00:58,060 --> 01:01:00,595
Because we don't know what Q of a star is.

1118
01:01:00,595 --> 01:01:03,640
We can't com- we don't know which arm it is.

1119
01:01:03,640 --> 01:01:05,350
So when we look at regret bound right now,

1120
01:01:05,350 --> 01:01:07,990
regret bounds are in terms of Q of a star.

1121
01:01:07,990 --> 01:01:09,490
We don't know what that quantity is.

1122
01:01:09,490 --> 01:01:11,230
So we're going to need to figure out some way to get rid of

1123
01:01:11,230 --> 01:01:14,965
that quantity and we're going to end up using these upper bounds,

1124
01:01:14,965 --> 01:01:17,290
but we're going to need the fact that the upper bound of the arm we

1125
01:01:17,290 --> 01:01:20,695
select is better than the Q star, Q of a star.

1126
01:01:20,695 --> 01:01:25,225
Okay. So- so this is saying that that's true if our upper confidence bounds hold,

1127
01:01:25,225 --> 01:01:28,465
what is the probability that that occurs?

1128
01:01:28,465 --> 01:01:33,070
Okay. So what that means is,

1129
01:01:33,070 --> 01:01:36,550
if we want to say that on all time steps,

1130
01:01:36,550 --> 01:01:37,915
this is a union bound,

1131
01:01:37,915 --> 01:01:40,040
this is our union over events.

1132
01:01:40,040 --> 01:01:45,570
The union over all the events for ut = 1 to t of the probability that

1133
01:01:45,570 --> 01:01:52,450
Q of a star minus the upper confidence bound of the action that we took.

1134
01:01:53,480 --> 01:02:00,280
We want this, oops, not that way.

1135
01:02:00,360 --> 01:02:02,590
We want the probability of this,

1136
01:02:02,590 --> 01:02:04,645
which is essentially the probability of failure.

1137
01:02:04,645 --> 01:02:08,890
So this is, what's the pro- this is if all confidence bounds hold things are good,

1138
01:02:08,890 --> 01:02:11,740
this says, what is the probability that that failed?

1139
01:02:11,740 --> 01:02:14,620
That the arm that you took it actually is not better.

1140
01:02:14,620 --> 01:02:19,840
The upper confidence bound is not better than the- the real mean of the optimal arm?

1141
01:02:19,840 --> 01:02:22,660
So this is the failure case, where the confidence,

1142
01:02:22,660 --> 01:02:25,195
but um, we're gonna say that if we look at that,

1143
01:02:25,195 --> 01:02:26,995
we- we don't want this thing to happen.

1144
01:02:26,995 --> 01:02:28,885
We can upper bound that

1145
01:02:28,885 --> 01:02:33,640
by making sure our upper confidence bounds hold at all time steps.

1146
01:02:33,640 --> 01:02:38,875
Okay. So these are the arms.

1147
01:02:38,875 --> 01:02:41,200
So what I said up there is that if all of

1148
01:02:41,200 --> 01:02:44,395
our confidence bounds hold on all time steps we can ensure that.

1149
01:02:44,395 --> 01:02:46,330
So we're now going to write that down in terms of what?

1150
01:02:46,330 --> 01:02:50,215
The probability that the upper confidence bounds do hold on all time steps.

1151
01:02:50,215 --> 01:02:59,755
And so we're gonna do probability that Q of at,

1152
01:02:59,755 --> 01:03:04,960
that's Q hat of at is greater than U.

1153
01:03:04,960 --> 01:03:08,050
Okay. This is the upper confidence bound we defined over there.

1154
01:03:08,050 --> 01:03:13,015
This is just saying that the upper confidence bound holds for each arm on each time step.

1155
01:03:13,015 --> 01:03:17,890
Well, by definition, over there we said we- we picked an upper confidence bound

1156
01:03:17,890 --> 01:03:22,940
to make sure this held with the least probability of delta over t squared.

1157
01:03:28,050 --> 01:03:31,150
Because that's how we defined our upper confidence bound.

1158
01:03:31,150 --> 01:03:34,960
We picked a big enough thing to add onto our empirical means so that we

1159
01:03:34,960 --> 01:03:39,790
could ensure that the upper confidence bound really was larger than our mean.

1160
01:03:39,790 --> 01:03:43,420
So now we have a union over all time steps,

1161
01:03:43,420 --> 01:03:45,310
a union over all arms,

1162
01:03:45,310 --> 01:03:47,920
delta divided by t squared.

1163
01:03:47,920 --> 01:03:53,440
And note that if you sum over t = 1 to

1164
01:03:53,440 --> 01:04:00,385
infinity of t to the -t, that's equal to pi squared over 6 which is less than 2.

1165
01:04:00,385 --> 01:04:07,100
So when you do the sum you get 2m delta.

1166
01:04:07,200 --> 01:04:10,615
So what this says is that the probability-

1167
01:04:10,615 --> 01:04:14,860
that your upper confidence bounds hold over all time steps.

1168
01:04:14,860 --> 01:04:16,435
So this is the negative,

1169
01:04:16,435 --> 01:04:21,680
this is that they- that they don't hold is at least 1 - 2m delta.

1170
01:04:23,010 --> 01:04:26,020
So all our- our- what we're gonna end up

1171
01:04:26,020 --> 01:04:28,420
doing is we're gonna have a high probability regret bound that says,

1172
01:04:28,420 --> 01:04:30,805
with probability at least 1 - 2m delta,

1173
01:04:30,805 --> 01:04:33,730
we're gonna get a smaller regret. Yeah.

1174
01:04:33,730 --> 01:04:36,580
So what about the infinite horizon case?

1175
01:04:36,580 --> 01:04:38,800
Great question. Yes. This is all for an infinite horizon case.

1176
01:04:38,800 --> 01:04:40,750
We're gonna look at the- we're gonna find

1177
01:04:40,750 --> 01:04:43,970
our regret in terms of t, the number of time steps.

1178
01:04:45,480 --> 01:04:50,090
Okay, all right. So why is this useful?

1179
01:04:51,270 --> 01:04:54,925
Do you guys want me to leave this up or we can move that now?

1180
01:04:54,925 --> 01:04:56,905
Everyone's written it down, who wants it?

1181
01:04:56,905 --> 01:05:01,690
Okay. So let's- can this go up?

1182
01:05:01,690 --> 01:05:11,815
Let me see or not.

1183
01:05:11,815 --> 01:05:14,125
Okay. So why is this useful?

1184
01:05:14,125 --> 01:05:17,050
Okay. We're now gonna define our regret.

1185
01:05:17,050 --> 01:05:20,170
So, this part of the board just says that we've

1186
01:05:20,170 --> 01:05:23,065
made it so these upper confidence bounds holds with high probability.

1187
01:05:23,065 --> 01:05:26,660
Now we're gonna try to show what our regret is gonna be, oh good.

1188
01:05:30,810 --> 01:05:36,100
Okay. Thank you. All right.

1189
01:05:36,100 --> 01:05:37,270
So, what's our regret?

1190
01:05:37,270 --> 01:05:45,610
Regret of our UCB algorithm after T time steps is just equal to the sum over

1191
01:05:45,610 --> 01:05:54,250
all those time steps t = 1 to t of Q of a star - Q of at. Okay?

1192
01:05:54,250 --> 01:05:55,930
Remember, we don't know either of these things.

1193
01:05:55,930 --> 01:05:58,120
We don't know what the real mean is of any arm

1194
01:05:58,120 --> 01:06:00,610
we pick and we don't know what the real mean is of the optimal arm.

1195
01:06:00,610 --> 01:06:02,905
So we need to turn this into things that we know.

1196
01:06:02,905 --> 01:06:04,300
These are unknown.

1197
01:06:04,300 --> 01:06:11,290
Okay? So what we're gonna do is one of our favorite tricks in reinforcement learning,

1198
01:06:11,290 --> 01:06:13,420
which is we add and subtract the same thing.

1199
01:06:13,420 --> 01:06:22,140
So we do sum over t equal- -t =1  to t of Ut, this upper bound,

1200
01:06:22,140 --> 01:06:28,200
at - Q of at + Q of

1201
01:06:28,200 --> 01:06:36,745
A star - Ut at. I just added and subtracted the same thing.

1202
01:06:36,745 --> 01:06:41,215
I picked the upper confidence bound of the arm that we selected at each time point.

1203
01:06:41,215 --> 01:06:46,090
Okay? So then the important thing is that what

1204
01:06:46,090 --> 01:06:50,920
we showed over here is that if all of our confidence bounds hold,

1205
01:06:50,920 --> 01:06:56,695
then the upper confidence bound of the arm we selected is larger than Q of a star.

1206
01:06:56,695 --> 01:06:58,960
That's what we showed over there.

1207
01:06:58,960 --> 01:07:02,980
So that means that this has to be less than or equal to 0.

1208
01:07:02,980 --> 01:07:06,625
Because the upper confidence bound of whatever arm we selected,

1209
01:07:06,625 --> 01:07:07,660
we proved over there,

1210
01:07:07,660 --> 01:07:10,555
is gonna be higher than the real mean of the optimal arm.

1211
01:07:10,555 --> 01:07:14,470
So this second part of this equation is less than or equal to 0,

1212
01:07:14,470 --> 01:07:18,430
which means we can upper bound our regret as follows.

1213
01:07:18,430 --> 01:07:21,680
So now we can drop that second term.

1214
01:07:27,690 --> 01:07:29,890
So that's nice, right?

1215
01:07:29,890 --> 01:07:32,810
Because now we don't have any a stars anymore.

1216
01:07:32,810 --> 01:07:36,450
We only are looking at the actions that we actually took at each time

1217
01:07:36,450 --> 01:07:38,040
step and we're comparing

1218
01:07:38,040 --> 01:07:42,340
the upper confidence bound of the- at that time step versus the real mean.

1219
01:07:42,680 --> 01:07:48,880
But remember the way we've defined our upper confidence bound and put it over here,

1220
01:07:48,880 --> 01:07:56,065
the way we defined our upper confidence bound Ut of at is exactly equal to, um,

1221
01:07:56,065 --> 01:08:04,900
the empirical mean, at, plus this square root 1 over 2

1222
01:08:04,900 --> 01:08:14,455
and at log t squared over delta. Okay?

1223
01:08:14,455 --> 01:08:21,310
And we said here that this was going to be the difference from

1224
01:08:21,310 --> 01:08:31,120
Hoeffding between Q of at - Q hat of at.

1225
01:08:31,120 --> 01:08:34,375
So the probability that this,

1226
01:08:34,375 --> 01:08:36,160
remember I called this U,

1227
01:08:36,160 --> 01:08:41,870
the probability that this was greater than U was small.

1228
01:08:42,390 --> 01:08:45,970
So now we're assuming that all of our confidence bounds hold,

1229
01:08:45,970 --> 01:08:48,490
which means that we know that the difference between

1230
01:08:48,490 --> 01:08:53,830
the real empirical mean and the true mean of this arm is bounded by U. Yeah.

1231
01:08:53,830 --> 01:09:01,029
Going back, for the bottom panel, sorry, it's a little hard to see, two questions. First of all,

1232
01:09:01,029 --> 01:09:05,409
where does this second, um, so you have a union over i = 1, the number of arms.

1233
01:09:05,410 --> 01:09:07,060
I don't see where that index actually factors in.

1234
01:09:07,060 --> 01:09:13,015
And then also if you could just go over the third line with the delta over t squared, and summation, how we derived that.

1235
01:09:13,015 --> 01:09:14,590
Sure? Yeah, so, um,

1236
01:09:14,590 --> 01:09:18,055
so what we did there is we said that if,

1237
01:09:18,055 --> 01:09:21,399
um, are you asking about the second line to the third line?

1238
01:09:21,399 --> 01:09:23,979
Yes. So what we did that in this case is,

1239
01:09:23,979 --> 01:09:25,479
um, we said for each,

1240
01:09:25,479 --> 01:09:27,474
we wanna make sure that on each of the time steps,

1241
01:09:27,475 --> 01:09:29,710
all of the upper confidence bounds hold.

1242
01:09:29,710 --> 01:09:33,100
Um, and so that's where we get an additional sum,

1243
01:09:33,100 --> 01:09:35,470
um, over here over all the arms.

1244
01:09:35,470 --> 01:09:37,750
So this is conservative, um,

1245
01:09:37,750 --> 01:09:40,420
trying to make sure we don't know- you could imagine

1246
01:09:40,420 --> 01:09:43,359
just doing this over the arm that's selected,

1247
01:09:43,359 --> 01:09:45,789
and but we don't know which arm is selected.

1248
01:09:45,790 --> 01:09:47,620
We want to be able to show, um,

1249
01:09:47,620 --> 01:09:49,345
this is going to be a looser upper,

1250
01:09:49,345 --> 01:09:50,859
upper bound saying this is sufficient.

1251
01:09:50,859 --> 01:09:54,610
So, um, we're saying that if you want to make sure that Q of

1252
01:09:54,610 --> 01:09:58,555
a star is greater than the upper bound of the arm that is selected,

1253
01:09:58,555 --> 01:10:00,730
it is sufficient to ensure that

1254
01:10:00,730 --> 01:10:03,685
your upper confidence bounds are valid at all time points from this,

1255
01:10:03,685 --> 01:10:06,055
from this, um, reasoning up here.

1256
01:10:06,055 --> 01:10:08,050
And so this is the probability that

1257
01:10:08,050 --> 01:10:10,915
your upper confidence bounds are correct on all times points,

1258
01:10:10,915 --> 01:10:12,070
for every single time point,

1259
01:10:12,070 --> 01:10:15,220
for every single arm, your upper confidence bounds have to hold.

1260
01:10:15,220 --> 01:10:18,100
And then, what we get in this case is,

1261
01:10:18,100 --> 01:10:21,130
we said that the probability on a particular time-step,

1262
01:10:21,130 --> 01:10:24,445
the upper confidence bounds holds is delta over t squared.

1263
01:10:24,445 --> 01:10:29,890
That's how we defined that U term [NOISE] so that according to Hoeffding's inequality,

1264
01:10:29,890 --> 01:10:33,460
it would hold with the least probability delta over t squared.

1265
01:10:33,460 --> 01:10:36,775
And then, so this is that,

1266
01:10:36,775 --> 01:10:40,270
and then I just made a side note that this is not something,

1267
01:10:40,270 --> 01:10:43,150
some of you guys might have seen this but I certainly wouldn't expected people to,

1268
01:10:43,150 --> 01:10:45,700
that just it turns out that in terms of the limit,

1269
01:10:45,700 --> 01:10:48,640
um, if you sum over t = 1 to infinity of t to the -2

1270
01:10:48,640 --> 01:10:52,540
that's bounded by pi squared over 6 which is less than 2.

1271
01:10:52,540 --> 01:10:56,320
Um, fun fact. Um [LAUGHTER] so,

1272
01:10:56,320 --> 01:10:59,455
so you can plug that in, right, because then you just get a 2 here,

1273
01:10:59,455 --> 01:11:02,110
and then you just get a sum over all arms which is m,

1274
01:11:02,110 --> 01:11:03,910
um, and you have a delta.

1275
01:11:03,910 --> 01:11:09,950
So this just allows us to take that infinite sum.

1276
01:11:10,410 --> 01:11:12,910
So notice also that,

1277
01:11:12,910 --> 01:11:16,690
um this goes to  question before this holds for the infinite horizon

1278
01:11:16,690 --> 01:11:18,580
because when we did this summing we're basically

1279
01:11:18,580 --> 01:11:21,800
making sure that our confidence bounds hold forever.

1280
01:11:23,340 --> 01:11:25,720
So we're, okay.

1281
01:11:25,720 --> 01:11:28,840
Great. So we said here now that we're

1282
01:11:28,840 --> 01:11:32,305
doing all of this part under the assumption that our confidence bounds hold.

1283
01:11:32,305 --> 01:11:36,835
Our confidence bounds hold mean that the difference between our expected mean

1284
01:11:36,835 --> 01:11:41,965
and the true mean for that same arm is bounded by mu,

1285
01:11:41,965 --> 01:11:45,535
bounded by U with high probability where this is the definition of U.

1286
01:11:45,535 --> 01:11:48,250
So that's what our Hoeffding inequality allowed us to state.

1287
01:11:48,250 --> 01:11:50,320
So take that quantity now,

1288
01:11:50,320 --> 01:11:52,345
that's our U and plug it in here.

1289
01:11:52,345 --> 01:11:56,710
So this is looking exactly the difference between our upper confidence bound and Q.

1290
01:11:56,710 --> 01:12:04,435
So this is exactly equal to sum over t = 1 to t of U. Um,

1291
01:12:04,435 --> 01:12:06,895
it's a little confusing in terms of notation.

1292
01:12:06,895 --> 01:12:10,225
Um, so I'll just plug in the exact expression right there.

1293
01:12:10,225 --> 01:12:14,650
1 over 2 and t of a,

1294
01:12:14,650 --> 01:12:20,600
t log t squared over delta.

1295
01:12:21,000 --> 01:12:25,090
Okay, so we just plugged in that,  the difference between

1296
01:12:25,090 --> 01:12:29,350
the empirical mean and the true mean is bounded by this quantity U.

1297
01:12:29,350 --> 01:12:32,665
Okay. All right.

1298
01:12:32,665 --> 01:12:34,300
So then in this case,

1299
01:12:34,300 --> 01:12:38,695
what we can do is we can split this up into the different arms that we pulled, okay?

1300
01:12:38,695 --> 01:12:42,250
So this is sum over all timesteps.

1301
01:12:42,250 --> 01:12:43,945
We can pull out,

1302
01:12:43,945 --> 01:12:45,910
note that this is, um,

1303
01:12:45,910 --> 01:12:48,550
if we upper bound this by big T,

1304
01:12:48,550 --> 01:12:56,155
this is equal to less than or equal to square root log big T squared over delta,

1305
01:12:56,155 --> 01:12:59,170
and then we're gonna get a sum over

1306
01:12:59,170 --> 01:13:02,875
all time steps and we're gonna split this up according to which arms we selected, okay?

1307
01:13:02,875 --> 01:13:07,600
So for, this is the same as if we look at for each of the arms,

1308
01:13:07,600 --> 01:13:10,075
how many times did we pull it?

1309
01:13:10,075 --> 01:13:15,640
So sum over n equals 1 to n, t, i,

1310
01:13:15,640 --> 01:13:21,640
square root of 1 over n. So we just divided this up,

1311
01:13:21,640 --> 01:13:22,660
like for each of the arms,

1312
01:13:22,660 --> 01:13:24,865
we selected them some amount of times.

1313
01:13:24,865 --> 01:13:26,590
That's here, i is,

1314
01:13:26,590 --> 01:13:28,480
i is indexing our arms.

1315
01:13:28,480 --> 01:13:33,340
So nt, i is the total number of times we selected arm i.

1316
01:13:33,340 --> 01:13:35,960
And then we sum that up, okay.

1317
01:13:36,690 --> 01:13:44,740
And then, if we note

1318
01:13:44,740 --> 01:13:53,140
the fact that if you sum from n = 1 to t square root 1 over n,

1319
01:13:53,140 --> 01:14:02,245
that is less than or equal to 2 square root t. You use an integral argument for that,

1320
01:14:02,245 --> 01:14:04,540
I'm happy to talk about it offline. Yes, in the back.

1321
01:14:04,540 --> 01:14:14,290
What happened to the 1 over 2 [inaudible].

1322
01:14:14,290 --> 01:14:15,820
Thank you, we have a 2,

1323
01:14:15,820 --> 01:14:17,990
we can put a 2 here.

1324
01:14:18,090 --> 01:14:24,670
Thanks. I'll be a little loose with constants but definitely catch me on them.

1325
01:14:24,670 --> 01:14:26,890
Because most of these bounds will all

1326
01:14:26,890 --> 01:14:29,200
end up being about whether it's sublinear or linear.

1327
01:14:29,200 --> 01:14:31,165
It's good to be precise.

1328
01:14:31,165 --> 01:14:34,060
Okay. So we have this quantity here,

1329
01:14:34,060 --> 01:14:36,640
um, when is this quantity maximized?

1330
01:14:36,640 --> 01:14:41,635
This quantity is going to be maximized if we pulled all arms an equal number of times.

1331
01:14:41,635 --> 01:14:47,935
Why? Because, um, 1 over n is decreasing with n. And so the,

1332
01:14:47,935 --> 01:14:50,200
the largest these can be is if you split,

1333
01:14:50,200 --> 01:14:51,340
if you split, um,

1334
01:14:51,340 --> 01:14:53,350
your pools across all arms equally.

1335
01:14:53,350 --> 01:14:56,005
So if we go back up to here,

1336
01:14:56,005 --> 01:14:58,375
and I call this a.

1337
01:14:58,375 --> 01:15:08,005
So a is gonna be less than or equal to, excuse me,

1338
01:15:08,005 --> 01:15:18,850
square root, 2 log t squared over delta times sum over i equals 1 to m,

1339
01:15:18,850 --> 01:15:23,710
sum over n equals 1 to t divided by

1340
01:15:23,710 --> 01:15:29,890
m. So this is as if we split all of our pools equally across all the arms.

1341
01:15:29,890 --> 01:15:35,665
1 over square root n, okay, and then we can use this expression.

1342
01:15:35,665 --> 01:15:42,370
Okay? So this is less than or equal to 1 over 2, we're almost there.

1343
01:15:42,370 --> 01:15:49,150
Um, t squared over delta and then we're gonna get sum

1344
01:15:49,150 --> 01:15:56,570
over i equals 1 to m of 2 square root t over m. Okay.

1345
01:15:56,610 --> 01:16:00,220
And then, when you sum this over m,

1346
01:16:00,220 --> 01:16:03,970
you get less than or equal to square root 1 over

1347
01:16:03,970 --> 01:16:11,050
2 log t squared over delta that brings in m into there.

1348
01:16:11,050 --> 01:16:15,730
When we look at another two in here times T,

1349
01:16:15,730 --> 01:16:20,570
m. And now we're done.

1350
01:16:21,630 --> 01:16:26,695
So what has this shown? This is shown that if we use upper confidence bounds,

1351
01:16:26,695 --> 01:16:32,335
that the rate at which our regret grows is sublinear square root times a log.

1352
01:16:32,335 --> 01:16:35,380
So t here is the number of timesteps.

1353
01:16:35,380 --> 01:16:40,750
So timesteps, so as if we are,

1354
01:16:40,750 --> 01:16:43,075
if we use upper confidence bounds, um,

1355
01:16:43,075 --> 01:16:44,919
in order to make our decisions,

1356
01:16:44,919 --> 01:16:48,050
then the regret grows much slower.

1357
01:16:48,480 --> 01:16:51,010
This is a problem independent bound.

1358
01:16:51,010 --> 01:16:52,525
It doesn't depend on the gaps.

1359
01:16:52,525 --> 01:16:56,125
There's, there's much nicer ones and tighter ones that depend on the gaps.

1360
01:16:56,125 --> 01:16:58,930
But this indicates why optimism is

1361
01:16:58,930 --> 01:17:01,600
fundamentally a sound thing to do in the case of bandits,

1362
01:17:01,600 --> 01:17:03,355
is that it allows us to grow much,

1363
01:17:03,355 --> 01:17:05,350
it allows us to have much better performance in terms of

1364
01:17:05,350 --> 01:17:07,620
regret than it does for the e-greedy case. Yeah.

1365
01:17:07,620 --> 01:17:11,180
Can you just display one more time the last one on the top board, um,

1366
01:17:11,180 --> 01:17:16,885
how you went from summation over t1 to big T and you just pulled out the t squared big T,

1367
01:17:16,885 --> 01:17:19,240
what happens to t = 1 to big T - 1?

1368
01:17:19,240 --> 01:17:22,315
Great question. Yeah, so this log term here t equals,

1369
01:17:22,315 --> 01:17:24,970
um, is ranging from t = 1 to t,

1370
01:17:24,970 --> 01:17:28,165
this log term is maximized when t is big T.

1371
01:17:28,165 --> 01:17:33,380
So we're upper bounding that log term and then it becomes a constant and we can pull it out.

1372
01:17:36,510 --> 01:17:39,640
Okay, so the cool thing here is that this is sublinear.

1373
01:17:39,640 --> 01:17:41,980
That's, that's really the main point.

1374
01:17:41,980 --> 01:17:48,520
Um, well, I go through an example and we'll go through,

1375
01:17:48,520 --> 01:17:49,825
um, more of this next time.

1376
01:17:49,825 --> 01:17:51,595
Um, I, I go,

1377
01:17:51,595 --> 01:17:55,360
we next go through an example for the toy domain for the broken toes of like what do

1378
01:17:55,360 --> 01:17:57,520
these upper confidence bounds look like in that case

1379
01:17:57,520 --> 01:18:00,055
and then what will the algorithm do in these scenarios?

1380
01:18:00,055 --> 01:18:02,845
Um, so that's what we'll look at next.

1381
01:18:02,845 --> 01:18:04,675
And then, after that we'll look at,

1382
01:18:04,675 --> 01:18:09,250
so this is one class of techniques which is this optimism under uncertainty approach,

1383
01:18:09,250 --> 01:18:13,420
which is saying that we're going to look at what the value is based on a combination of

1384
01:18:13,420 --> 01:18:17,965
the empirical rewards we've seen so far plus an upper confidence bound over them,

1385
01:18:17,965 --> 01:18:19,780
um, and use that to make decisions.

1386
01:18:19,780 --> 01:18:21,580
[NOISE] And then the next thing we'll see too

1387
01:18:21,580 --> 01:18:23,650
is where we are Bayesian about the world and we

1388
01:18:23,650 --> 01:18:28,060
instead maintain a prior and we update our prior and use that to figure out how to act.

1389
01:18:28,060 --> 01:18:29,830
So we'll go through this next week,

1390
01:18:29,830 --> 01:18:32,390
um, and I'll see you then.

