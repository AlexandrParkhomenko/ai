1
00:00:04,610 --> 00:00:07,290
So, homework two is out now.

2
00:00:07,290 --> 00:00:10,170
I recognize that there's a really broad spectrum of

3
00:00:10,170 --> 00:00:13,095
background in terms of whether people have seen deep learning,

4
00:00:13,095 --> 00:00:14,235
or not before, or,

5
00:00:14,235 --> 00:00:16,560
or taken a class, [NOISE] or used it extensively.

6
00:00:16,560 --> 00:00:19,050
[NOISE] Um, just a quick humble, who,

7
00:00:19,050 --> 00:00:23,640
which of you have used TensorFlow or, um, PyTorch before?

8
00:00:23,640 --> 00:00:25,470
Okay. A number of you, but not everybody.

9
00:00:25,470 --> 00:00:27,670
So, what we're gonna be doing this weekend sessions is,

10
00:00:27,670 --> 00:00:30,750
we gonna be having some more background on deep learning.

11
00:00:30,750 --> 00:00:32,490
You're not expected to become, or,

12
00:00:32,490 --> 00:00:35,595
or to be a deep learning expert to be in this class, but we,

13
00:00:35,595 --> 00:00:38,760
you only need to have some basic skills in order to do homework two,

14
00:00:38,760 --> 00:00:42,110
um, be able to use function approximation with a deep neural network.

15
00:00:42,110 --> 00:00:45,480
So, I encourage you to go to session this week if you don't have a background on that.

16
00:00:45,480 --> 00:00:49,100
We're gonna today cover a little bit on deep learning but very,

17
00:00:49,100 --> 00:00:50,585
very, very small amount, um,

18
00:00:50,585 --> 00:00:52,820
and focus more on deep reinforcement learning.

19
00:00:52,820 --> 00:00:57,275
[NOISE] Uh, but the sessions will be a good chance to catch up on that material.

20
00:00:57,275 --> 00:00:59,385
Um, we're also gonna be reaching,

21
00:00:59,385 --> 00:01:01,650
uh, releasing by the end of tomorrow.

22
00:01:01,650 --> 00:01:04,440
Um, what the default projects will be, uh, for this class.

23
00:01:04,440 --> 00:01:06,680
Er, and you guys will get to pick whether or

24
00:01:06,680 --> 00:01:09,080
not you wanna do your own construction project or,

25
00:01:09,080 --> 00:01:10,900
uh, the default project.

26
00:01:10,900 --> 00:01:14,250
Um, and those proposals will be due,

27
00:01:14,250 --> 00:01:16,700
um, very soon, er, in a little over a week.

28
00:01:16,700 --> 00:01:20,705
Are there any other questions that people have right now? Yeah.

29
00:01:20,705 --> 00:01:24,475
The assignments, [inaudible] are they limited to TensorFlow?

30
00:01:24,475 --> 00:01:26,500
asked the question if,

31
00:01:26,500 --> 00:01:28,595
if the assignment is limited to TensorFlow.

32
00:01:28,595 --> 00:01:30,665
I'm, I'm pretty sure that everything relies that,

33
00:01:30,665 --> 00:01:32,555
er, you're using TensorFlow. So, yeah.

34
00:01:32,555 --> 00:01:35,500
Just a-feel free to reach out on Piazza and double-check that,

35
00:01:35,500 --> 00:01:38,150
but I'm pretty sure any Oliver auto-graders is just set up for TensorFlow,

36
00:01:38,150 --> 00:01:39,680
for, so for this one even if you use PyTorch,

37
00:01:39,680 --> 00:01:41,335
some way please use TensorFlow.

38
00:01:41,335 --> 00:01:45,410
Um, I'll believe you guys also should have access to the Azure credit.

39
00:01:45,410 --> 00:01:47,570
Um, If you have any questions about getting setup

40
00:01:47,570 --> 00:01:50,150
without feel free to use the Piazza, uh, Piazza channel.

41
00:01:50,150 --> 00:01:53,690
We also released a tutorial for how to just sort of set up your machine last week.

42
00:01:53,690 --> 00:01:55,610
So, if you're having any questions with that,

43
00:01:55,610 --> 00:01:57,095
that's a great place to get started.

44
00:01:57,095 --> 00:01:58,550
Um, you could look at the tutorial,

45
00:01:58,550 --> 00:02:01,400
you can look at the video, or you can reach out to us on Piazza.

46
00:02:01,400 --> 00:02:06,360
Any other questions? All right.

47
00:02:06,360 --> 00:02:07,520
So, we're gonna go ahead and get started.

48
00:02:07,520 --> 00:02:09,590
Um, uh, what we're gonna be covering today

49
00:02:09,590 --> 00:02:12,040
is sort of a very brief overview about Deep Learning,

50
00:02:12,040 --> 00:02:13,960
um, as well as Deep Q Learning.

51
00:02:13,960 --> 00:02:18,470
Um So, in terms of where we are in the class, we've been,

52
00:02:18,470 --> 00:02:21,140
we have been discussing how to learn to make decisions in

53
00:02:21,140 --> 00:02:24,815
the world when we don't know the dynamics model of the Reward Model in advance.

54
00:02:24,815 --> 00:02:28,250
Um, and last week, we were, we were discussing value function approximation,

55
00:02:28,250 --> 00:02:30,470
particularly linear value function approximation.

56
00:02:30,470 --> 00:02:32,450
And today we're gonna start to talk about other forms of

57
00:02:32,450 --> 00:02:34,610
value function approximation in particular,

58
00:02:34,610 --> 00:02:37,810
um, uh, using deep neural networks.

59
00:02:37,810 --> 00:02:40,560
So, the- why do we wanna do this at all?

60
00:02:40,560 --> 00:02:43,400
Well, the reasons we wanted to start thinking about, uh, er,

61
00:02:43,400 --> 00:02:46,340
using function approximators is that if we wanna be able to use

62
00:02:46,340 --> 00:02:49,565
reinforcement learning to tackle really complex carry problems.

63
00:02:49,565 --> 00:02:52,850
Um, we need to be able to deal with the fact that often we're gonna have

64
00:02:52,850 --> 00:02:56,295
very high dimensional input signals or observations.

65
00:02:56,295 --> 00:02:59,090
Um, so, we wanna be able to deal with sort of pixel input,

66
00:02:59,090 --> 00:03:02,330
like images, or we wanna be able to deal with really complex, um,

67
00:03:02,330 --> 00:03:04,220
information about customers, or patients,

68
00:03:04,220 --> 00:03:08,765
or students, um, where we might have enormous state and our actions spaces.

69
00:03:08,765 --> 00:03:13,070
I'll note today that we're mostly not gonna talk so much about enormous action spaces,

70
00:03:13,070 --> 00:03:16,010
but we are gonna think a lot about really large state spaces.

71
00:03:16,010 --> 00:03:18,620
And so, when we started talking about those,

72
00:03:18,620 --> 00:03:21,455
I was arguing that we either need representations of models.

73
00:03:21,455 --> 00:03:23,179
Those mean that's sort of the dynamics,

74
00:03:23,179 --> 00:03:24,290
or the reward models.

75
00:03:24,290 --> 00:03:30,660
T, T or R or a state-action values Q,

76
00:03:30,660 --> 00:03:33,030
or V, or our policies,

77
00:03:33,030 --> 00:03:36,080
um, that can generalize across states and our actions.

78
00:03:36,080 --> 00:03:40,115
With the idea being that we may in fact never encountered the exact same state again.

79
00:03:40,115 --> 00:03:42,785
You might never see the exact same image of the world again,

80
00:03:42,785 --> 00:03:45,740
um, but we wanna be able to generalize from our past experience.

81
00:03:45,740 --> 00:03:47,060
And so, we thought about it,

82
00:03:47,060 --> 00:03:49,910
instead of having a table to represent our value functions, um,

83
00:03:49,910 --> 00:03:54,170
we were gonna use this generic function approximator where we have a W now,

84
00:03:54,170 --> 00:03:55,370
which are some parameters.

85
00:03:55,370 --> 00:04:00,330
[NOISE] And when we thought about doing this,

86
00:04:00,330 --> 00:04:02,120
we said what we're gonna focus on is we're gonna focus on

87
00:04:02,120 --> 00:04:04,910
function approximations that are differentiable.

88
00:04:04,910 --> 00:04:06,350
Um, and the nice thing about

89
00:04:06,350 --> 00:04:09,500
differentiable rep-representations is that we can use our data,

90
00:04:09,500 --> 00:04:11,464
and we can estimate our parameters,

91
00:04:11,464 --> 00:04:14,555
and then we can take use gradient descent to try to fit our function,

92
00:04:14,555 --> 00:04:15,950
to try to write that into write,

93
00:04:15,950 --> 00:04:18,620
or represent our q function or a value function.

94
00:04:18,620 --> 00:04:21,589
So, I mentioned last time that most the time we're

95
00:04:21,589 --> 00:04:24,410
gonna think about trying to quantify the fit of our function,

96
00:04:24,410 --> 00:04:26,180
compared to the true value as,

97
00:04:26,180 --> 00:04:27,575
um, a mean squared error.

98
00:04:27,575 --> 00:04:29,480
So, we can define our loss j,

99
00:04:29,480 --> 00:04:34,655
and we can use gradient descent on that to try to find the parameters w that optimize.

100
00:04:34,655 --> 00:04:38,840
And just as a reminder stochastic gradient descent was useful because when we could

101
00:04:38,840 --> 00:04:42,995
just slowly update our parameters as we get more information.

102
00:04:42,995 --> 00:04:45,140
And that information now could be [NOISE] in the form of

103
00:04:45,140 --> 00:04:47,210
episodes or it [NOISE] could be individual tuples.

104
00:04:47,210 --> 00:04:48,905
[NOISE] When I say a tuple,

105
00:04:48,905 --> 00:04:55,375
I generally mean a state-action reward next state tuple.

106
00:04:55,375 --> 00:04:57,680
Um, and the nice thing is that

107
00:04:57,680 --> 00:05:02,790
the expected stochastic gradient descent is the same as the full gradient update.

108
00:05:03,670 --> 00:05:06,620
So, just to remind ourselves,

109
00:05:06,620 --> 00:05:09,155
last time we were talking about linear value function approximations.

110
00:05:09,155 --> 00:05:11,300
Um, and that meant that what we're gonna do is,

111
00:05:11,300 --> 00:05:14,330
we're gonna have a whole bunch of features to describe our world, um,

112
00:05:14,330 --> 00:05:15,740
as so, you know,

113
00:05:15,740 --> 00:05:17,990
these features we will input our state,

114
00:05:17,990 --> 00:05:21,470
state as the real state of the world and we would output our features.

115
00:05:21,470 --> 00:05:26,875
And so, this could be things like a laser range finder for our robot,

116
00:05:26,875 --> 00:05:32,300
which told us how far away the walls were in all 180 deg- degree directions.

117
00:05:32,300 --> 00:05:36,005
We talked about the fact that that was an aliased version of the world because,

118
00:05:36,005 --> 00:05:38,690
um, multiple hallways might, might look identical.

119
00:05:38,690 --> 00:05:42,155
So, our value function now is a dot product between those features,

120
00:05:42,155 --> 00:05:45,265
that we've got out about the world, um, with the weights.

121
00:05:45,265 --> 00:05:47,875
Our objective function is again the mean squared error.

122
00:05:47,875 --> 00:05:49,940
And then we could do this same weight update.

123
00:05:49,940 --> 00:05:53,920
And the key hard thing was that we don't know what this is.

124
00:05:53,920 --> 00:05:58,285
So, this is the true value of a policy.

125
00:05:58,285 --> 00:06:00,910
And the problem is we don't know what the true value of a policy is,

126
00:06:00,910 --> 00:06:03,400
otherwise we wouldn't have to be doing all of this learning.

127
00:06:03,400 --> 00:06:06,755
Um, and so we needed to have different ways to approximate it.

128
00:06:06,755 --> 00:06:12,010
And so, the two ways we talked about last time was inspired by a work on Monte Carlo,

129
00:06:12,010 --> 00:06:18,490
or on TD learning is we could either plug-in the return from the full episode.

130
00:06:18,490 --> 00:06:21,710
This is the sum of her words.

131
00:06:24,140 --> 00:06:27,790
Or we could put in a bootstrapped return.

132
00:06:27,790 --> 00:06:30,500
So, now we're doing bootstrapping.

133
00:06:33,170 --> 00:06:35,805
Where we look at the reward,

134
00:06:35,805 --> 00:06:38,425
the next state, and the value of our next state.

135
00:06:38,425 --> 00:06:41,875
And in this case we're using a linear value function approximators for everything,

136
00:06:41,875 --> 00:06:45,250
which gave us a really simple form of what the derivative is,

137
00:06:45,250 --> 00:06:47,470
of this function with respect to W. Basically it's

138
00:06:47,470 --> 00:06:53,485
just our features times essentially this prediction error.

139
00:06:53,485 --> 00:06:57,560
So, people sometimes call this is the prediction error.

140
00:06:57,770 --> 00:07:01,260
Cause it's the difference between the value,

141
00:07:01,260 --> 00:07:04,270
or right now we're using GT as the true value.

142
00:07:04,270 --> 00:07:07,480
Of course in reality it's just a sample of the value but, well,

143
00:07:07,480 --> 00:07:09,195
it's the difference between, um,

144
00:07:09,195 --> 00:07:11,960
the true value and our estimated value.

145
00:07:11,960 --> 00:07:13,960
I'm gonna shrink that difference.

146
00:07:13,960 --> 00:07:16,350
So, in this case I've written, um,

147
00:07:16,350 --> 00:07:18,980
these equations of all for linear value function approximation,

148
00:07:18,980 --> 00:07:22,280
but there are some limitations to use the linear value function approximation,

149
00:07:22,280 --> 00:07:25,530
even though this has been probably the most well-studied.

150
00:07:26,120 --> 00:07:30,360
So, if you have the right set of features,

151
00:07:30,360 --> 00:07:32,150
and historically there was a lot of work on

152
00:07:32,150 --> 00:07:34,190
figuring out what those rights set of features are.

153
00:07:34,190 --> 00:07:35,755
They often worked really well.

154
00:07:35,755 --> 00:07:37,425
And in fact when we get into,

155
00:07:37,425 --> 00:07:38,865
I think I mentioned briefly before.

156
00:07:38,865 --> 00:07:41,780
When we start to talk about deep neural networks you can think of,

157
00:07:41,780 --> 00:07:44,900
a deep neural network is just a really complicated way to get out features,

158
00:07:44,900 --> 00:07:48,275
plus the last layer being a linear combination of those features.

159
00:07:48,275 --> 00:07:51,770
For most of the time when we're talking about deep RL with,

160
00:07:51,770 --> 00:07:54,320
um, a deep neural networks represent the Q function.

161
00:07:54,320 --> 00:07:56,300
That's the type of representation will be looking at.

162
00:07:56,300 --> 00:07:58,205
So, linear value function is often

163
00:07:58,205 --> 00:08:00,410
really works very well if you're the right set of features,

164
00:08:00,410 --> 00:08:03,305
but is this challenge of what is the right set of features.

165
00:08:03,305 --> 00:08:06,560
Um, and there are all sorts of implications about whether or not

166
00:08:06,560 --> 00:08:09,500
we're even gonna be able to write down the true p- um,

167
00:08:09,500 --> 00:08:11,650
value function using our set of features,

168
00:08:11,650 --> 00:08:14,845
and how easy is it for us to converge to that.

169
00:08:14,845 --> 00:08:17,570
So, one alternative that we didn't talk so much

170
00:08:17,570 --> 00:08:19,670
about last time is to use sort of a really,

171
00:08:19,670 --> 00:08:21,905
really rich function approximator class.

172
00:08:21,905 --> 00:08:23,660
Um, where we don't have to,

173
00:08:23,660 --> 00:08:25,670
have to have a direct representation of the features.

174
00:08:25,670 --> 00:08:29,550
Er, and some of those are Kernel based approaches.

175
00:08:29,550 --> 00:08:31,260
Um, has anybody seen like,

176
00:08:31,260 --> 00:08:33,000
ah, Kernel based approaches before?

177
00:08:33,000 --> 00:08:36,000
Or like k-nearest neighbor type approaches?

178
00:08:36,000 --> 00:08:38,440
If you take a machine learning you've heard of k-nearest neighbors,

179
00:08:38,440 --> 00:08:40,575
those are sort of these non-parametric approaches,

180
00:08:40,575 --> 00:08:44,450
where your representation size tends to grow with the number of data points.

181
00:08:44,450 --> 00:08:47,420
Um, and then they can be really nice and they have

182
00:08:47,420 --> 00:08:50,705
some actually really nice convergence properties for reinforcement learning.

183
00:08:50,705 --> 00:08:52,275
The problem is, um,

184
00:08:52,275 --> 00:08:55,600
that the number of data points you need tends to scale with the dimension.

185
00:08:55,600 --> 00:09:01,590
So, um, if you have let's say those 180, um, features,

186
00:09:01,590 --> 00:09:05,240
um, the number of points you need to tile that 180 degrees space,

187
00:09:05,240 --> 00:09:08,350
generally scales exponentially with the dimension.

188
00:09:08,350 --> 00:09:13,070
So, that's not so appealing both in terms of computational complexity,

189
00:09:13,070 --> 00:09:15,580
memory requirements, and sample complexity.

190
00:09:15,580 --> 00:09:16,910
So, these actually have

191
00:09:16,910 --> 00:09:21,440
a lot stronger convergence results compared to linear value function approximators.

192
00:09:21,440 --> 00:09:25,160
Um, but they haven't far been used for in a very widespread way.

193
00:09:25,160 --> 00:09:28,875
Yeah. Um, and everyone just [inaudible] name first please to stop me.

194
00:09:28,875 --> 00:09:29,505
Yes.

195
00:09:29,505 --> 00:09:30,740
Yeah. [LAUGHTER]

196
00:09:30,740 --> 00:09:33,955
So, can you repeat again why the exponential behavior happening?

197
00:09:33,955 --> 00:09:36,970
Yeah, student's question was why does the exponential behavior happen and

198
00:09:36,970 --> 00:09:40,165
a lot of these sort of kernel based approximators or non-parametric.

199
00:09:40,165 --> 00:09:43,540
The intuition is that if you

200
00:09:43,540 --> 00:09:47,350
want to have sort of an accurate representation of your value function,

201
00:09:47,350 --> 00:09:49,540
um, and you're representing it by say,

202
00:09:49,540 --> 00:09:51,190
uh, local points around it.

203
00:09:51,190 --> 00:09:53,470
For example, like, with the k-nearest neighbor approach.

204
00:09:53,470 --> 00:09:56,650
then the number of points you need to have everything be close like in

205
00:09:56,650 --> 00:10:00,460
an epsilon ball scales with the- the dimensionality.

206
00:10:00,460 --> 00:10:02,470
So, basically you're just gridding the space.

207
00:10:02,470 --> 00:10:07,630
So, if you think of -- if

208
00:10:07,630 --> 00:10:12,370
you think you have sort of- if you want to have any point on this line, be close,

209
00:10:12,370 --> 00:10:16,330
then you could put a point here and a point here in order to have

210
00:10:16,330 --> 00:10:20,500
everything be sort of epsilon close for all points on that line to have,

211
00:10:20,500 --> 00:10:22,945
uh, a neighbor that's within epsilon distance.

212
00:10:22,945 --> 00:10:24,520
If you want to have it in a square,

213
00:10:24,520 --> 00:10:26,770
you're gonna need four points so that

214
00:10:26,770 --> 00:10:29,680
everything can be somewhat close to one of the points.

215
00:10:29,680 --> 00:10:31,540
Generally, the number of points you need this going

216
00:10:31,540 --> 00:10:33,220
to scale exponentially with the dimension.

217
00:10:33,220 --> 00:10:37,870
[NOISE] But they are really nice, um, uh,

218
00:10:37,870 --> 00:10:41,350
because they can be guaranteed to be averagers which we talked about really

219
00:10:41,350 --> 00:10:45,100
briefly last time that views a linear value function approximator.

220
00:10:45,100 --> 00:10:47,230
Um, when you do a bellman backup,

221
00:10:47,230 --> 00:10:49,900
it's not necessarily a contraction operator anymore which

222
00:10:49,900 --> 00:10:52,960
is why you can sometimes blow up as you do more and more backups.

223
00:10:52,960 --> 00:10:56,230
A really cool thing about averagers is sort of by their name.

224
00:10:56,230 --> 00:10:58,329
Um, when you use this type of approximation,

225
00:10:58,329 --> 00:11:01,255
you don't- they're guaranteed to be- to be a non-expansion,

226
00:11:01,255 --> 00:11:02,800
which means that when you combine them with

227
00:11:02,800 --> 00:11:06,355
a bellman backup it's guaranteed it'd still be a contraction which is really cool.

228
00:11:06,355 --> 00:11:08,380
So, that means these sort of approximators are

229
00:11:08,380 --> 00:11:11,620
guaranteed to converge compared to a lot of other ones.

230
00:11:11,620 --> 00:11:13,900
All right, but they're not gonna scale very well

231
00:11:13,900 --> 00:11:15,535
and in practice you don't tend to see them,

232
00:11:15,535 --> 00:11:18,190
though there's some really cool work by my colleague, Finale Doshi-Velez,

233
00:11:18,190 --> 00:11:20,905
over at Harvard who's thinking about using these for things like, um,

234
00:11:20,905 --> 00:11:24,535
health care applications and how do you sort of generalized from related patients.

235
00:11:24,535 --> 00:11:27,745
So, they can be useful but they generally don't scale so well.

236
00:11:27,745 --> 00:11:31,885
So, what we're gonna talk about today is thinking about deep neural networks which also

237
00:11:31,885 --> 00:11:36,475
have very flexible representations but we hope we're gonna scale a lot better.

238
00:11:36,475 --> 00:11:38,200
Um, now, in general we're going to have

239
00:11:38,200 --> 00:11:41,095
almost no theoretical guarantees for the rest of the day,

240
00:11:41,095 --> 00:11:45,235
um, and- but in practice they often work really really well.

241
00:11:45,235 --> 00:11:47,320
So, they become an incredibly useful tool in

242
00:11:47,320 --> 00:11:51,010
reinforcement learning and everywhere else really in terms of machine learning.

243
00:11:51,010 --> 00:11:52,990
So, what do we mean by deep neural networks?

244
00:11:52,990 --> 00:11:55,180
Well, a number of you guys are experts but, um,

245
00:11:55,180 --> 00:11:58,510
what it generally means in this case is we're just gonna think of com- making

246
00:11:58,510 --> 00:12:02,365
a function approximator which is a composition of a number of functions.

247
00:12:02,365 --> 00:12:06,160
So, we're gonna have our input x and I'm gonna feed it into

248
00:12:06,160 --> 00:12:11,320
some function which is gonna take in some weights.

249
00:12:11,320 --> 00:12:13,810
So, in general, all of these things can be vectors.

250
00:12:13,810 --> 00:12:16,030
So, you're gonna take in some weights and

251
00:12:16,030 --> 00:12:18,550
combine them with your x and then you're going to push

252
00:12:18,550 --> 00:12:20,770
them into some function and then you're gonna

253
00:12:20,770 --> 00:12:23,890
output something which is probably gonna be also a vector.

254
00:12:23,890 --> 00:12:27,010
Then you're gonna push that into another function,

255
00:12:27,010 --> 00:12:30,040
and throw in some more weights.

256
00:12:30,040 --> 00:12:33,110
I'm gonna do that a whole bunch of times,

257
00:12:33,450 --> 00:12:36,820
and then at the very end of that you can output

258
00:12:36,820 --> 00:12:39,850
some y which you could think of as being like our Q.

259
00:12:39,850 --> 00:12:43,285
Then, we can output that to some loss function j.

260
00:12:43,285 --> 00:12:44,770
So, what does that mean here?

261
00:12:44,770 --> 00:12:50,110
It means that y is equal to hn of hn

262
00:12:50,110 --> 00:12:55,660
minus one dot dot dot dot dot h1

263
00:12:55,660 --> 00:12:59,080
of x. I haven't

264
00:12:59,080 --> 00:13:02,695
written all the weights that are going in there but there's a whole bunch of weights too,

265
00:13:02,695 --> 00:13:04,840
and then this is sort of loss function like

266
00:13:04,840 --> 00:13:11,770
before and this you can think of as like our Q for example.

267
00:13:11,770 --> 00:13:14,665
But these are- happen a lot in unsupervised learning

268
00:13:14,665 --> 00:13:17,500
like predicting whether or not something is a cat or not or,

269
00:13:17,500 --> 00:13:18,820
you know, an image, uh,

270
00:13:18,820 --> 00:13:22,150
of a particular object, um, or for regression.

271
00:13:22,150 --> 00:13:23,650
So, why do we want to do this?

272
00:13:23,650 --> 00:13:27,610
Well, first of all it should be clear that as you compose lots of functions, um,

273
00:13:27,610 --> 00:13:29,620
you could represent really complicated functions

274
00:13:29,620 --> 00:13:33,370
by adding and subtracting and taking polynomials and all sorts

275
00:13:33,370 --> 00:13:36,070
of things you could do by just composing functions together that this could be

276
00:13:36,070 --> 00:13:39,325
a really powerful space of functions you could represent.

277
00:13:39,325 --> 00:13:41,320
But the nice reason to write it down like this

278
00:13:41,320 --> 00:13:43,240
is that you can do the chain rule to try to

279
00:13:43,240 --> 00:13:47,200
do stochastic gradient descent. So, how does this work?

280
00:13:47,200 --> 00:13:51,550
Well, that means that we can write down that dj.

281
00:13:51,550 --> 00:13:53,125
So, we really want, you know,

282
00:13:53,125 --> 00:13:55,735
dj with respect to all these different parameters.

283
00:13:55,735 --> 00:14:01,170
So, what we can write down here is we can write down dj of hn and

284
00:14:01,170 --> 00:14:06,565
dhn of dwn and we can do- do this kind of everywhere.

285
00:14:06,565 --> 00:14:08,890
So, dj of h2,

286
00:14:08,890 --> 00:14:14,665
dh of h2, and dh2 of dw2.

287
00:14:14,665 --> 00:14:19,510
So, you can use the chain rule to propagate all of this the- the gradient of, um,

288
00:14:19,510 --> 00:14:21,760
your loss function with respect to w,

289
00:14:21,760 --> 00:14:26,860
all the way back down all of these different compositions by writing out the chain rule.

290
00:14:26,860 --> 00:14:29,620
Um, so that's nice because it means that you can

291
00:14:29,620 --> 00:14:32,515
take our output signal and then propagate that back,

292
00:14:32,515 --> 00:14:35,335
um, in terms of updating all of your weights.

293
00:14:35,335 --> 00:14:37,225
Now, I'm gonna date myself.

294
00:14:37,225 --> 00:14:40,345
So, when I first learned about deep neural networks, you have to do this by hand.

295
00:14:40,345 --> 00:14:42,730
Um, and, uh, so as you might imagine,

296
00:14:42,730 --> 00:14:46,030
this was a less popular assignment and,

297
00:14:46,030 --> 00:14:48,400
uh, it's called backpropagation.

298
00:14:48,400 --> 00:14:50,875
So, you can derive this by hand,

299
00:14:50,875 --> 00:14:53,980
um, and I'll talk in a second about what these type of functions are, you know,

300
00:14:53,980 --> 00:14:56,680
you need differentiable functions for h. But I think one of

301
00:14:56,680 --> 00:14:59,530
the major major innovations that's happened over there, you know, roughly what?

302
00:14:59,530 --> 00:15:03,220
Like last 5 to 8 years is that there's auto differentiation.

303
00:15:03,220 --> 00:15:04,900
So, that now, um,

304
00:15:04,900 --> 00:15:06,940
you don't have to derive all of these, uh,

305
00:15:06,940 --> 00:15:08,410
gradients by hand instead,

306
00:15:08,410 --> 00:15:10,765
you can just write down your network parameter.

307
00:15:10,765 --> 00:15:13,960
Um, and then your network of para- which

308
00:15:13,960 --> 00:15:16,840
includes a bunch of parameters and then you have software like,

309
00:15:16,840 --> 00:15:19,930
um, TensorFlow to do all of the differentiation for you.

310
00:15:19,930 --> 00:15:23,080
So, I think these sort of tools have made it much much

311
00:15:23,080 --> 00:15:26,050
more practical for people- lots and lots of people to use,

312
00:15:26,050 --> 00:15:28,930
um, deep neural networks because you don't- you can have

313
00:15:28,930 --> 00:15:31,000
very very complicated networks so very very large number of

314
00:15:31,000 --> 00:15:34,460
layers and there's no sort of hand writing down of what the gradients are.

315
00:15:35,610 --> 00:15:38,200
So, what are these h functions?

316
00:15:38,200 --> 00:15:39,520
Generally, they combine, um,

317
00:15:39,520 --> 00:15:41,755
both linear and nonlinear transformations.

318
00:15:41,755 --> 00:15:44,020
Um, basically they just- they have to be differentiable.

319
00:15:44,020 --> 00:15:47,290
So, you know, this- this h need to be

320
00:15:47,290 --> 00:15:52,070
differentiable if we're gonna use gradient descent to fit them.

321
00:15:52,980 --> 00:16:00,040
So, the common choices are either linear so you can think of hn is equal to whn minus

322
00:16:00,040 --> 00:16:07,225
one or non-linear where we can think of hn is equal to some function hn minus one.

323
00:16:07,225 --> 00:16:11,870
If it's nonlinear, we often call this an activation function.

324
00:16:14,370 --> 00:16:18,940
Due to time, I'm not gonna talk in class much about

325
00:16:18,940 --> 00:16:21,220
the connections with neural networks which is

326
00:16:21,220 --> 00:16:23,500
what inside of our brain which was what's inspiring,

327
00:16:23,500 --> 00:16:25,180
these sort of artificial neural networks.

328
00:16:25,180 --> 00:16:27,265
Um, but inside of the brain,

329
00:16:27,265 --> 00:16:28,540
people think of there is being the sort of

330
00:16:28,540 --> 00:16:32,350
non-linear activation functions where if the signal passes a certain threshold then,

331
00:16:32,350 --> 00:16:34,225
for example, the neuron would fire.

332
00:16:34,225 --> 00:16:37,750
So, these sort of non-linear activation functions can be things like

333
00:16:37,750 --> 00:16:42,490
sigmoid functions or ReLU.

334
00:16:42,490 --> 00:16:47,050
ReLU's particularly popular right but- um.

335
00:16:47,050 --> 00:16:49,015
So- so, you can choose different combinations, uh,

336
00:16:49,015 --> 00:16:51,100
of linear functions or non-linear functions,

337
00:16:51,100 --> 00:16:53,950
um, and as usual we need a loss function at the end.

338
00:16:53,950 --> 00:16:55,600
Typically, we use mean squared error.

339
00:16:55,600 --> 00:16:59,155
You could also use log likelihood but we need something that- that we can differentiate

340
00:16:59,155 --> 00:17:03,130
how close we are achieving that target in order to update our weights.

341
00:17:03,130 --> 00:17:06,550
[NOISE] Yeah? Name first.

342
00:17:06,550 --> 00:17:11,859
So, this ReLU function is not differentiable, right?

343
00:17:11,859 --> 00:17:13,869
It is differentiable, like,

344
00:17:13,869 --> 00:17:17,019
you can- you- you can- you can take it to- the- the- differentiable and it's

345
00:17:17,020 --> 00:17:18,579
ended up being a lot more popular than

346
00:17:18,579 --> 00:17:20,439
sigmoid recently, though I feel like it [OVERLAPPING].

347
00:17:20,440 --> 00:17:21,550
It's not differentiable at one point?

348
00:17:21,550 --> 00:17:22,390
Yes.

349
00:17:22,390 --> 00:17:26,065
But I don't see how gradient [inaudible] is gonna work on the part where it's flat.

350
00:17:26,065 --> 00:17:28,615
Well, if it's flat, it's zero.

351
00:17:28,615 --> 00:17:33,440
So, that ends up just- your gradient is just zero. [OVERLAPPING] Yeah.

352
00:17:33,480 --> 00:17:36,595
The question is about how for- for ReLU,

353
00:17:36,595 --> 00:17:38,095
there's a lot of it where it's flat.

354
00:17:38,095 --> 00:17:41,425
Um, and so if your gradient is zero then your gradients can vanish there.

355
00:17:41,425 --> 00:17:43,555
Um, in- in- in general actually,

356
00:17:43,555 --> 00:17:45,520
we're not gonna talk about this at all in class but, uh,

357
00:17:45,520 --> 00:17:49,615
um, there's certainly a problem is you start having very deep neural networks.

358
00:17:49,615 --> 00:17:53,545
Um, but because of some of these functions you can sometimes end up sort of having,

359
00:17:53,545 --> 00:17:57,370
um, almost no signal going back to the- the earlier layers.

360
00:17:57,370 --> 00:17:59,860
But I- I'm not gonna talk about any of that.

361
00:17:59,860 --> 00:18:01,840
We'll talk- we'll talk some about that in sessions.

362
00:18:01,840 --> 00:18:03,460
Um, they're good to be aware of, um,

363
00:18:03,460 --> 00:18:05,155
and we're also happy to give other pointers.

364
00:18:05,155 --> 00:18:07,570
But yeah, if it's flat, it's okay, you can still just have,

365
00:18:07,570 --> 00:18:12,565
uh, a zero derivative.

366
00:18:12,565 --> 00:18:16,390
Okay. All right.

367
00:18:16,390 --> 00:18:18,070
So, why do we want to do this?

368
00:18:18,070 --> 00:18:21,670
Well, it's nice if we can use this sort of like much more complicated representation.

369
00:18:21,670 --> 00:18:23,935
Um, another thing is that, um,

370
00:18:23,935 --> 00:18:26,020
if you have at least one hidden layer,

371
00:18:26,020 --> 00:18:29,065
um, if you have a sufficient number of nodes.

372
00:18:29,065 --> 00:18:31,750
Um, nodes you can think of as a- if you're not familiar with this is

373
00:18:31,750 --> 00:18:34,525
basically just sort of a sufficiently complicated,

374
00:18:34,525 --> 00:18:36,100
uh, set of, uh,

375
00:18:36,100 --> 00:18:38,410
combination of features, um, and functions.

376
00:18:38,410 --> 00:18:41,530
Um, this is a universal function approximators which means that you

377
00:18:41,530 --> 00:18:44,730
can represent any function with the deep neural network. So, that's really nice.

378
00:18:44,730 --> 00:18:46,860
We're not gonna have any capacity problems if we use

379
00:18:46,860 --> 00:18:49,595
a sufficiently expressive function approximators.

380
00:18:49,595 --> 00:18:52,335
Um, and that's important because if you

381
00:18:52,335 --> 00:18:55,320
think about what we're doing with linear value function approximators,

382
00:18:55,320 --> 00:18:58,560
it was clearly the case sometimes that you might have too limited features and you

383
00:18:58,560 --> 00:19:02,100
just wouldn't be able to express the true value function for some states.

384
00:19:02,100 --> 00:19:05,410
What the universal function approximator, um,

385
00:19:05,410 --> 00:19:09,490
property is stating is that that will not occur for,

386
00:19:09,490 --> 00:19:13,480
um, uh, deep neural network if it is, uh, sufficiently rich.

387
00:19:13,480 --> 00:19:15,130
All right. Now, of course,

388
00:19:15,130 --> 00:19:17,470
you can always think of doing a linear value function approximator

389
00:19:17,470 --> 00:19:20,275
with very very rich features and then that becomes equivalent.

390
00:19:20,275 --> 00:19:21,730
So, given that, you know,

391
00:19:21,730 --> 00:19:22,885
what's another benefit, um,

392
00:19:22,885 --> 00:19:27,070
another benefit is that potentially you can use exponentially less nodes or

393
00:19:27,070 --> 00:19:31,570
parameters compared to using a shallow net which means not as many of those compositions,

394
00:19:31,570 --> 00:19:35,110
um, to represent the same function and that's pretty elegant and,

395
00:19:35,110 --> 00:19:38,395
uh, I'm happy to talk about that offline or- or we can talk about on Piazza.

396
00:19:38,395 --> 00:19:39,880
Then the final thing is that you can learn

397
00:19:39,880 --> 00:19:42,860
the parameters using stochastic gradient descent.

398
00:19:44,040 --> 00:19:46,765
All right. So, that's pretty much that, you know,

399
00:19:46,765 --> 00:19:48,865
deep neural networks in like five seconds.

400
00:19:48,865 --> 00:19:52,600
Um, we're now gonna talk a little bit about convolutional neural networks.

401
00:19:52,600 --> 00:19:55,060
Um, and again this is all gonna be

402
00:19:55,060 --> 00:19:58,300
a pretty light introduction because you're not gonna need to know the details

403
00:19:58,300 --> 00:20:00,940
in order to do the homework except for mostly the fact

404
00:20:00,940 --> 00:20:03,880
of understanding that these are sort of very,

405
00:20:03,880 --> 00:20:06,770
um, expressive function approximators.

406
00:20:06,870 --> 00:20:10,510
So, why do we care about convolutional neural networks?

407
00:20:10,510 --> 00:20:13,000
Um, well, they're used very extensively in computer vision, um,

408
00:20:13,000 --> 00:20:15,580
and if we're interested in having robots

409
00:20:15,580 --> 00:20:17,965
and other sorts of agents that can interact in the real world,

410
00:20:17,965 --> 00:20:21,040
one of our primary sensory modalities is vision, um,

411
00:20:21,040 --> 00:20:23,620
and it's very likely that we're going to want to be able to use similar sorts of

412
00:20:23,620 --> 00:20:27,290
input on our- our robots in our artificial agents.

413
00:20:27,420 --> 00:20:31,060
So, if you think about this,

414
00:20:31,060 --> 00:20:32,320
um, think about there being an image,

415
00:20:32,320 --> 00:20:33,595
in this case, of Einstein.

416
00:20:33,595 --> 00:20:37,540
Um, and there's a whole bunch of different pixels on Einstein,

417
00:20:37,540 --> 00:20:38,995
um, of this picture of Einstein.

418
00:20:38,995 --> 00:20:40,705
Let's say it's 1,000 by 1,000.

419
00:20:40,705 --> 00:20:43,735
So, it's 1,000 by 1,000, you know, x and y.

420
00:20:43,735 --> 00:20:49,420
So, we have 10 to the 6 pixels.

421
00:20:49,420 --> 00:20:53,980
So, this standard often called feedforward deep neural network.

422
00:20:53,980 --> 00:20:58,555
Um, you would have all of those pixels, um,

423
00:20:58,555 --> 00:21:02,290
and then they would be going as input to another layer and, um,

424
00:21:02,290 --> 00:21:05,350
you might want to have a bunch of different nodes that are taking input from

425
00:21:05,350 --> 00:21:08,890
all of those and so you can get a huge number of weights.

426
00:21:08,890 --> 00:21:14,470
So, you might have 10 to the 6 weights per- the st- the- often,

427
00:21:14,470 --> 00:21:17,140
we think about sort of- I know I haven't given you enough details about this,

428
00:21:17,140 --> 00:21:19,450
but often we think of there as being sort of

429
00:21:19,450 --> 00:21:23,080
this deep neural network where we have many functions in parallel.

430
00:21:23,080 --> 00:21:28,945
So, it's not just like a single line but we might have x going into, uh, h1,

431
00:21:28,945 --> 00:21:32,200
h2, h3, h4 then all of those would

432
00:21:32,200 --> 00:21:35,650
then be going in some complicated way to some other functions.

433
00:21:35,650 --> 00:21:39,850
So, you can have lots of sort of functions being computed in parallel.

434
00:21:39,850 --> 00:21:42,820
So, you can imagine your image goes and you've got

435
00:21:42,820 --> 00:21:45,100
one function that computes some aspect of the image and

436
00:21:45,100 --> 00:21:47,530
another function that compute some other aspect of the image and then you're

437
00:21:47,530 --> 00:21:50,485
gonna combin- combine those in all sorts of complicated ways.

438
00:21:50,485 --> 00:21:51,790
So, what this is saying is,

439
00:21:51,790 --> 00:21:54,130
well for that very first one there's maybe gonna be, you know,

440
00:21:54,130 --> 00:21:56,995
a whole bunch of n different functions were computing of the image.

441
00:21:56,995 --> 00:21:59,290
There'd be 10 to the 6 parameters here.

442
00:21:59,290 --> 00:22:03,275
So, if we have these weight times x,

443
00:22:03,275 --> 00:22:08,695
then that would be 10 to the 6 parameters to take in all of that x. That's a lot.

444
00:22:08,695 --> 00:22:12,580
Um, and then if we want to do this for doing different types of weights all in parallel,

445
00:22:12,580 --> 00:22:14,515
then that's gonna be a very very large number of

446
00:22:14,515 --> 00:22:18,580
parameters and we do have a lot of data off it now but that's

447
00:22:18,580 --> 00:22:21,350
still an enormous number of parameters to- to

448
00:22:21,350 --> 00:22:23,050
represent and it also sort of misses some of

449
00:22:23,050 --> 00:22:25,000
the point of what we often think about with vision.

450
00:22:25,000 --> 00:22:30,460
So, if we think about doing this many times and having lots of hidden units,

451
00:22:30,460 --> 00:22:33,265
we can get a really an enormous number of parameters.

452
00:22:33,265 --> 00:22:37,030
Um, so to avoid sort of

453
00:22:37,030 --> 00:22:39,040
this space-time complexity and the fact that

454
00:22:39,040 --> 00:22:41,260
we're sort of ignoring the structure of images,

455
00:22:41,260 --> 00:22:44,680
convolutional neural networks try to have a particular form of

456
00:22:44,680 --> 00:22:49,030
deep neural network that tries to think about the properties of images.

457
00:22:49,030 --> 00:22:53,140
So, in particular, images often have structure, um,

458
00:22:53,140 --> 00:22:55,660
in the way that our- our brain promises images also has

459
00:22:55,660 --> 00:22:59,930
structure and this sort of distinctive features in space and frequency.

460
00:22:59,970 --> 00:23:02,920
So, when you have a convolutional neural network,

461
00:23:02,920 --> 00:23:05,635
we think of there being particular types of operators.

462
00:23:05,635 --> 00:23:10,540
Having so operators again here are like our functions, h1 and hn,

463
00:23:10,540 --> 00:23:13,840
which I said before could either be linear or nonlinear and then

464
00:23:13,840 --> 00:23:17,770
convolutional neural network learn a particular structures for those, um, uh,

465
00:23:17,770 --> 00:23:20,020
for those functions to try to sort of

466
00:23:20,020 --> 00:23:22,270
think about the properties that we might want to be extracting from

467
00:23:22,270 --> 00:23:25,375
images and kind of the key aspects here is that

468
00:23:25,375 --> 00:23:29,290
we're gonna do a lot of weight sharing to do parameter reduction.

469
00:23:29,290 --> 00:23:31,345
So, instead of saying,

470
00:23:31,345 --> 00:23:36,565
"I'm going to have totally different parameters each taking in all of the pixels."

471
00:23:36,565 --> 00:23:40,690
I'm gonna end up having sort of local parameters that are identical and

472
00:23:40,690 --> 00:23:42,610
then I apply them to different parts of the image

473
00:23:42,610 --> 00:23:45,800
to try to extract, for example, features.

474
00:23:46,140 --> 00:23:50,230
Because ultimately, the point of doing this is gonna be trying to extracting

475
00:23:50,230 --> 00:23:51,790
features that we think are gonna be useful for

476
00:23:51,790 --> 00:23:53,470
either predicting things like whether or not,

477
00:23:53,470 --> 00:23:56,020
you know, a face isn't an image or that are gonna

478
00:23:56,020 --> 00:23:59,410
help us in terms of understanding what the Q function should be.

479
00:23:59,410 --> 00:24:03,070
So, the key idea- one of the key ideas is to say that we're gonna have

480
00:24:03,070 --> 00:24:07,465
a filter or a receptive field which is that we're gonna have some hidden unit.

481
00:24:07,465 --> 00:24:11,005
Um, so it's gonna be a function that's applied to some previous input.

482
00:24:11,005 --> 00:24:14,545
At the beginning, that's just gonna be a subset of our image and instead of,

483
00:24:14,545 --> 00:24:16,030
um, taking in the whole image,

484
00:24:16,030 --> 00:24:17,230
we're just going to take in part.

485
00:24:17,230 --> 00:24:19,390
So, we're just gonna take in a patch.

486
00:24:19,390 --> 00:24:23,650
So, we're gonna take the upper corner and we're gonna take the middle.

487
00:24:23,650 --> 00:24:26,290
So, it's like we're just gonna try to compute

488
00:24:26,290 --> 00:24:29,990
some properties of a particular patch of the image.

489
00:24:30,510 --> 00:24:33,580
So, then we can imagine taking,

490
00:24:33,580 --> 00:24:36,085
it's often called a filter, that little, um,

491
00:24:36,085 --> 00:24:38,410
those set of weights that we're applying to that patch and we

492
00:24:38,410 --> 00:24:41,035
could do that all over the image, um,

493
00:24:41,035 --> 00:24:45,100
and we often called the- there's a stride which means sort of how much you move,

494
00:24:45,100 --> 00:24:47,590
um, that little patch at each time point.

495
00:24:47,590 --> 00:24:50,860
There's also this thing called zero-padding which is how many zeros to add on

496
00:24:50,860 --> 00:24:53,860
each input layer and this determines sort of help,

497
00:24:53,860 --> 00:24:56,860
um, helps determine what your output is.

498
00:24:56,860 --> 00:25:00,040
So in this case, if you have an input of 28 by 28 and you have

499
00:25:00,040 --> 00:25:03,910
a little five-by-five patch that you're going to slide over the entire image,

500
00:25:03,910 --> 00:25:06,010
then you're gonna end up with a 24 by

501
00:25:06,010 --> 00:25:08,710
24 layer next because

502
00:25:08,710 --> 00:25:11,560
basically you just take this and then you move it over a little bit.

503
00:25:11,560 --> 00:25:18,385
You move it over, and each of those times you're gonna take those 25.

504
00:25:18,385 --> 00:25:22,210
So, this is five-by-five so you're gonna have 25 input x's and

505
00:25:22,210 --> 00:25:26,860
you're gonna dot-product them with some weights and that's gonna give you an output.

506
00:25:26,860 --> 00:25:31,640
So, here in this case that means we're gonna need 25 weights.

507
00:25:31,920 --> 00:25:35,890
Okay. So, one thing is instead of having our full x input,

508
00:25:35,890 --> 00:25:39,250
we're just gonna take in- we're gonna direct different parts of the x

509
00:25:39,250 --> 00:25:42,745
input to different neurons which you can think of just different functions.

510
00:25:42,745 --> 00:25:45,745
Um, but the other nice idea here is that

511
00:25:45,745 --> 00:25:49,330
we're going to have the same weights for everything.

512
00:25:49,330 --> 00:25:52,840
So, when we took those weights we're going to have sort of,

513
00:25:52,840 --> 00:25:54,460
um, you can think of them as trying to extract

514
00:25:54,460 --> 00:25:56,425
a feature from that sub patch of the image.

515
00:25:56,425 --> 00:25:59,320
For example, whether or not there's an edge.

516
00:25:59,320 --> 00:26:02,440
So, you can imagine I'm trying to detect whether or not there's

517
00:26:02,440 --> 00:26:05,215
something that looks like a horizontal edge in that part of the image

518
00:26:05,215 --> 00:26:08,470
and I try to- and that is determined by the weights I'm specifying and

519
00:26:08,470 --> 00:26:12,010
I just move that over my entire image to see whether or not it's present.

520
00:26:12,010 --> 00:26:13,960
So, now the weights are identical,

521
00:26:13,960 --> 00:26:15,985
and you're just moving them over the entire image.

522
00:26:15,985 --> 00:26:17,500
So, instead of having,

523
00:26:17,500 --> 00:26:19,270
um, you know, 10 to the 6 weights,

524
00:26:19,270 --> 00:26:23,815
I might only having 25 weights and I'm applying those to the same- uh,

525
00:26:23,815 --> 00:26:26,570
just applying them to lots of different parts of the image.

526
00:26:27,080 --> 00:26:30,340
Okay. So, this is sort of what that would look like.

527
00:26:30,340 --> 00:26:31,390
You sort of have this input,

528
00:26:31,390 --> 00:26:33,865
you go to the hi- um, the hidden layer and, yeah,

529
00:26:33,865 --> 00:26:35,890
you're sort of do- also down-sampling the image.

530
00:26:35,890 --> 00:26:40,180
Um.

531
00:26:40,180 --> 00:26:41,860
Why would you want to do this?

532
00:26:41,860 --> 00:26:43,300
Well, we think that often, the brain is doing this.

533
00:26:43,300 --> 00:26:45,040
It's trying to pick up different sort of features.

534
00:26:45,040 --> 00:26:47,980
In fact, a lot of computer vision before deep learning was,

535
00:26:47,980 --> 00:26:50,500
um, trying to construct these special sorts of features,

536
00:26:50,500 --> 00:26:53,200
things like sift features or other features that

537
00:26:53,200 --> 00:26:56,185
they really think captures sort of important properties of the image,

538
00:26:56,185 --> 00:26:59,665
but they're also may be invariant to things like translation.

539
00:26:59,665 --> 00:27:01,420
Because we also think that, you know,

540
00:27:01,420 --> 00:27:03,730
whether I'm looking at, um, the world like this,

541
00:27:03,730 --> 00:27:05,335
or I move my head slightly, um,

542
00:27:05,335 --> 00:27:08,200
that the features that I see are often gonna be identical,

543
00:27:08,200 --> 00:27:10,240
whether I moved to the left or right a little bit.

544
00:27:10,240 --> 00:27:12,640
There are particular salient aspects of the world that are gonna

545
00:27:12,640 --> 00:27:15,340
be relevant for detecting whether or not there's a face,

546
00:27:15,340 --> 00:27:17,590
and relevant for deciding my value function.

547
00:27:17,590 --> 00:27:20,020
So, we want to sort of extract features that we think are gonna

548
00:27:20,020 --> 00:27:23,660
represent this sort of translation in variance.

549
00:27:25,170 --> 00:27:29,170
This means also that rather than just computing, uh, you- you can do this.

550
00:27:29,170 --> 00:27:31,675
You'll use the same weights all the way across the feature, er,

551
00:27:31,675 --> 00:27:36,115
all across the image and then you can do this for multiple different types of features.

552
00:27:36,115 --> 00:27:39,175
So there's a really nice, um,

553
00:27:39,175 --> 00:27:42,160
discussion of this that goes into more depth from 231-n,

554
00:27:42,160 --> 00:27:43,750
which some of you guys might've taken.

555
00:27:43,750 --> 00:27:46,270
Um, and there's a nice animation where they show, okay,

556
00:27:46,270 --> 00:27:47,620
imagine you have your input,

557
00:27:47,620 --> 00:27:49,125
you can think of this as an image,

558
00:27:49,125 --> 00:27:52,425
and then you could apply these different filters on top of it,

559
00:27:52,425 --> 00:27:54,480
which you can think of as trying to detect different features,

560
00:27:54,480 --> 00:27:56,010
and then you move them around your image,

561
00:27:56,010 --> 00:27:58,665
and see whether or not that feature is present anywhere.

562
00:27:58,665 --> 00:28:01,260
So you can do that with multiple different fype- types of filters.

563
00:28:01,260 --> 00:28:03,795
You could think of this as trying to look for whether something's like that

564
00:28:03,795 --> 00:28:06,450
or something's horizontal or vertical,

565
00:28:06,450 --> 00:28:08,160
different types of edges, um,

566
00:28:08,160 --> 00:28:12,895
and, uh, these give you different features essentially that are been extracted.

567
00:28:12,895 --> 00:28:15,970
Um, the other really important thing in CNNs,

568
00:28:15,970 --> 00:28:17,605
is what are known as pooling layers.

569
00:28:17,605 --> 00:28:20,575
They are often used as a way to sort of down-sample the image.

570
00:28:20,575 --> 00:28:22,690
So you can do things like max pooling to

571
00:28:22,690 --> 00:28:25,795
detect whether or not a particular feature is present, um,

572
00:28:25,795 --> 00:28:29,170
or take averages or other ways to kind of just down,

573
00:28:29,170 --> 00:28:30,415
ah, and compress the,

574
00:28:30,415 --> 00:28:31,915
the information that you got it in.

575
00:28:31,915 --> 00:28:35,020
So, just remember in this case and in many cases,

576
00:28:35,020 --> 00:28:36,925
we're gonna start with a really high dimensional input,

577
00:28:36,925 --> 00:28:40,045
like x might be an image and output,

578
00:28:40,045 --> 00:28:43,405
a scalar, like, um, you know, the Q value.

579
00:28:43,405 --> 00:28:46,060
So we're somehow gonna have to go for really high dimensional input and

580
00:28:46,060 --> 00:28:49,075
kind of average and slow down until we can get to,

581
00:28:49,075 --> 00:28:51,500
um, a low dimensional output.

582
00:28:51,750 --> 00:28:55,315
So, the final layer is typically fully connected.

583
00:28:55,315 --> 00:28:58,420
So we can again think about all of these previous processes

584
00:28:58,420 --> 00:29:02,320
as essentially computing some new feature representation,

585
00:29:02,320 --> 00:29:05,065
so essentially from here to here.

586
00:29:05,065 --> 00:29:08,245
We're kind of computing this new feature representation of the image,

587
00:29:08,245 --> 00:29:09,685
and at the very end,

588
00:29:09,685 --> 00:29:11,605
we can take some fully connected layer,

589
00:29:11,605 --> 00:29:14,155
where it's like doing linear regression,

590
00:29:14,155 --> 00:29:17,905
and use that to output predictions or scalars.

591
00:29:17,905 --> 00:29:20,410
Again, I know either for some of you, guys,

592
00:29:20,410 --> 00:29:22,600
this is sort of a quick shallow refresher.

593
00:29:22,600 --> 00:29:25,270
For others of you, this is clearly not in, ah, this is, ah,

594
00:29:25,270 --> 00:29:27,625
would be a whirlwind introduction, um,

595
00:29:27,625 --> 00:29:30,400
but we won't be requiring you to know a lot of these details.

596
00:29:30,400 --> 00:29:31,540
And again, just go to a session,

597
00:29:31,540 --> 00:29:34,850
if you have some questions and feel free to reach out to us.

598
00:29:35,190 --> 00:29:38,590
Okay. So these type of representations,

599
00:29:38,590 --> 00:29:40,810
both Deep Neural Networks and Convolutional Neural Networks,

600
00:29:40,810 --> 00:29:43,840
are both used extensively in deep reinforcement learning.

601
00:29:43,840 --> 00:29:47,800
So it was around in 2014, um,

602
00:29:47,800 --> 00:29:52,150
where I- the workshop where David Silver started talking about,

603
00:29:52,150 --> 00:29:55,225
um, how we could use these type of approximations for Atari.

604
00:29:55,225 --> 00:29:57,490
So why was the surprising? I just sort of wandering back.

605
00:29:57,490 --> 00:30:02,770
So in around 1994, personally in 1994,

606
00:30:02,770 --> 00:30:08,680
um, we had TD backgammon which used Deep Neural Networks.

607
00:30:08,680 --> 00:30:10,225
Well, they used neural networks.

608
00:30:10,225 --> 00:30:11,800
I think there was someone that deep,

609
00:30:11,800 --> 00:30:16,120
and I think out like a world-class backgammon player out of that.

610
00:30:16,120 --> 00:30:17,635
So, that was pretty early on.

611
00:30:17,635 --> 00:30:20,860
And then we had the results that were kind of happening around like

612
00:30:20,860 --> 00:30:25,930
1995 to maybe like 1998, which said that,

613
00:30:25,930 --> 00:30:33,680
"Function approximation plus offline off policy control,

614
00:30:33,960 --> 00:30:40,100
plus bootstrapping can be bad,

615
00:30:40,530 --> 00:30:43,540
can fail to converge."

616
00:30:43,540 --> 00:30:46,720
So, we talked about this a little last time.

617
00:30:46,720 --> 00:30:49,150
That in general, as soon as we start doing

618
00:30:49,150 --> 00:30:52,645
this function approximation even with the linear function approximator,

619
00:30:52,645 --> 00:30:55,299
um, that when you're combining off policy control, bootstrapping,

620
00:30:55,299 --> 00:30:57,655
which means we're doing like TD learning or Q learning,

621
00:30:57,655 --> 00:31:00,100
um, and, uh, in functional approximator,

622
00:31:00,100 --> 00:31:02,320
then you can start to have this, uh,

623
00:31:02,320 --> 00:31:07,090
challenging triad, um, which often means that we're not guaranteed to converge.

624
00:31:07,090 --> 00:31:08,590
And even if we're guaranteed to converge,

625
00:31:08,590 --> 00:31:10,420
the solution may not be a good one.

626
00:31:10,420 --> 00:31:13,690
So sort of there was this early encouraging success and then there were

627
00:31:13,690 --> 00:31:15,640
these results in sort of the middle of

628
00:31:15,640 --> 00:31:17,650
the nineties that we're trying to better understand this,

629
00:31:17,650 --> 00:31:19,750
that indicated that things could be very bad,

630
00:31:19,750 --> 00:31:22,000
and the risk was some of the- In addition to the theoretical results,

631
00:31:22,000 --> 00:31:24,400
there were sort of these simple test cases,

632
00:31:24,400 --> 00:31:27,820
that, you know, these simple cases that went wrong.

633
00:31:27,820 --> 00:31:30,805
So, it wasn't just sort of in principle this could happen,

634
00:31:30,805 --> 00:31:35,420
uh, but there were cases which failed.

635
00:31:35,490 --> 00:31:38,305
And so I think for a long time after that, the,

636
00:31:38,305 --> 00:31:41,440
the community was sort of backed away from Deep Neural Networks for a while.

637
00:31:41,440 --> 00:31:44,140
People were quite cautious about using them because they were clearly,

638
00:31:44,140 --> 00:31:47,545
even simple cases where things started to go really badly with function approximation.

639
00:31:47,545 --> 00:31:50,620
And theoretically, people could prove that it could go badly,

640
00:31:50,620 --> 00:31:53,560
and so there's less attention to it for quite a while.

641
00:31:53,560 --> 00:31:57,190
And then, um, there was the rise of Deep Neural Networks in sort of, you know,

642
00:31:57,190 --> 00:32:02,925
the- the mid 2000s, like, to now.

643
00:32:02,925 --> 00:32:07,615
So, uh, Deep Neural Networks became huge,

644
00:32:07,615 --> 00:32:10,300
and there was called a huge success in them for things like vision,

645
00:32:10,300 --> 00:32:11,950
in other areas, there was a whole bunch of data,

646
00:32:11,950 --> 00:32:13,285
there's a whole bunch of compute.

647
00:32:13,285 --> 00:32:15,235
They we're getting really extraordinary results.

648
00:32:15,235 --> 00:32:17,140
And so, then, perhaps it was natural that, like,

649
00:32:17,140 --> 00:32:19,639
around in like 2014,

650
00:32:20,970 --> 00:32:27,505
DeepMind, DeepMind combined them and had some really amazing successes with Atari.

651
00:32:27,505 --> 00:32:29,620
And so I think it sort of really changed the story of

652
00:32:29,620 --> 00:32:31,540
how people are perceiving using, um,

653
00:32:31,540 --> 00:32:34,120
this sort of complicated function approximation, meters,

654
00:32:34,120 --> 00:32:36,070
and RL, and that yes,

655
00:32:36,070 --> 00:32:37,570
it can fail to converge.

656
00:32:37,570 --> 00:32:39,130
Yes, things can go really badly,

657
00:32:39,130 --> 00:32:41,305
and they do go really badly sometimes in practice,

658
00:32:41,305 --> 00:32:45,235
but it is also possible of them that despite that- you know,

659
00:32:45,235 --> 00:32:48,070
the fact that we don't always fully understand why they always work,

660
00:32:48,070 --> 00:32:49,420
um, that often in practice,

661
00:32:49,420 --> 00:32:51,310
we can still get pretty good policies out.

662
00:32:51,310 --> 00:32:52,960
Now, we often don't know if they're optimal.

663
00:32:52,960 --> 00:32:55,945
Often, we know they're not optimal because we know that people can play better,

664
00:32:55,945 --> 00:32:58,090
but that doesn't mean that they might not be pretty good,

665
00:32:58,090 --> 00:33:00,400
and so we sort of saw this resurgence of interest

666
00:33:00,400 --> 00:33:02,710
in- into deep reinforcement learning. Yeah.

667
00:33:02,710 --> 00:33:04,660
[NOISE]

668
00:33:04,660 --> 00:33:09,265
Um, I guess is there anything from your perspective that the,

669
00:33:09,265 --> 00:33:13,540
the deep learning has solved the problems that they had come up with in the mid '90s?

670
00:33:13,540 --> 00:33:16,390
Or, is it just that kind of through increases

671
00:33:16,390 --> 00:33:20,305
in computational power and the ability to gather a lot of data,

672
00:33:20,305 --> 00:33:21,460
that when it failed,

673
00:33:21,460 --> 00:33:22,570
it kinda doesn't matter,

674
00:33:22,570 --> 00:33:23,710
and we can try some different,

675
00:33:23,710 --> 00:33:25,720
like we- you know,

676
00:33:25,720 --> 00:33:29,110
try it again and kinda put it together and just keep trying until it works?

677
00:33:29,110 --> 00:33:30,880
I guess my question is, did we actually overcome

678
00:33:30,880 --> 00:33:32,920
any of the problems that arose in the late '90s,

679
00:33:32,920 --> 00:33:35,290
or is it just that we're just kinda powered through?

680
00:33:35,290 --> 00:33:37,000
The question is, you know, how we sort of

681
00:33:37,000 --> 00:33:39,160
fundamentally resolve some of the issues of the late my '90s,

682
00:33:39,160 --> 00:33:41,665
or, um, we kind of brute forcing it.

683
00:33:41,665 --> 00:33:44,650
Um, I think that some of the issues that were coming

684
00:33:44,650 --> 00:33:47,500
up in 1995 to 1998 in terms of convergence,

685
00:33:47,500 --> 00:33:49,810
there are some algorithms now that are more

686
00:33:49,810 --> 00:33:52,540
true stochastic gradient algorithms that are covered in chapter 11,

687
00:33:52,540 --> 00:33:56,530
um, so that I- a- a- are guaranteed to converge.

688
00:33:56,530 --> 00:33:59,470
They may not be guaranteed to converge to the optimal policy,

689
00:33:59,470 --> 00:34:01,450
um, so there's still lot of- there's still a ton of work,

690
00:34:01,450 --> 00:34:03,790
I think to be done to understand function approximator and off

691
00:34:03,790 --> 00:34:06,895
policy control and bootstrapping.

692
00:34:06,895 --> 00:34:09,460
I think there's also a couple algorithmic ideas

693
00:34:09,460 --> 00:34:11,469
that we're gonna see later in this lecture,

694
00:34:11,469 --> 00:34:15,444
that help the performance kind of avoid some of those convergence problems.

695
00:34:15,445 --> 00:34:19,495
So I think people knew about this when they started going into 2013, 2014.

696
00:34:19,495 --> 00:34:20,965
And so they tried to think about, "Well,

697
00:34:20,965 --> 00:34:22,570
when might this issue happen,

698
00:34:22,570 --> 00:34:24,219
and how could we avoid some of that stuff?"

699
00:34:24,219 --> 00:34:25,284
Like, what's causing that?

700
00:34:25,284 --> 00:34:26,619
And so at least algorithmically,

701
00:34:26,620 --> 00:34:27,850
can we try to make things,

702
00:34:27,850 --> 00:34:29,514
that people often talk about stability,

703
00:34:29,514 --> 00:34:30,939
so can we try to make sure that

704
00:34:30,940 --> 00:34:33,820
the Deep Neural Network doesn't seem to start having weights that are

705
00:34:33,820 --> 00:34:35,560
going off towards infinity and at least

706
00:34:35,560 --> 00:34:38,110
empirically have sort of more stable performance. Yes,

707
00:34:38,110 --> 00:34:39,730
[inaudible] for me.

708
00:34:39,730 --> 00:34:42,145
So with the Atari case specifically,

709
00:34:42,145 --> 00:34:44,050
did you- did you avoid that problem?

710
00:34:44,050 --> 00:34:47,500
Well, sort of, that if you tried it by having on policy control? I just don't know.

711
00:34:47,500 --> 00:34:50,245
[inaudible] there wasn't the case that in fact,

712
00:34:50,245 --> 00:34:51,610
that in your Deep Neural experiment,

713
00:34:51,610 --> 00:34:59,530
they updated the performance to match the Apple policies [inaudible].

714
00:34:59,530 --> 00:35:01,585
The question is whether or not in this sort of,

715
00:35:01,585 --> 00:35:03,040
um, ah, if I understood correctly,

716
00:35:03,040 --> 00:35:04,480
in the Atari case like, you know,

717
00:35:04,480 --> 00:35:06,520
where they changed things to be more on policy or,

718
00:35:06,520 --> 00:35:08,785
or which we know can be much more stable.

719
00:35:08,785 --> 00:35:11,005
Um, ah, they are doing deep learning in,

720
00:35:11,005 --> 00:35:12,415
in this case, Deep-Q Learning.

721
00:35:12,415 --> 00:35:13,990
And so it can still be very unstable,

722
00:35:13,990 --> 00:35:15,790
but they're gonna do something about how they do

723
00:35:15,790 --> 00:35:17,830
with the frequency of updates to the networks,

724
00:35:17,830 --> 00:35:19,165
to try to make it more stable.

725
00:35:19,165 --> 00:35:21,490
Um, and it's a great question for me.

726
00:35:21,490 --> 00:35:22,900
We'll see how it works here.

727
00:35:22,900 --> 00:35:25,330
Anyone else? Okay, cool.

728
00:35:25,330 --> 00:35:29,440
So, um, we'll- we'll see an example for breakout shortly, um, of what they did.

729
00:35:29,440 --> 00:35:31,450
Um, so again right now,

730
00:35:31,450 --> 00:35:34,690
we're gonna be talking about using Deep Neural Networks to represent the value function.

731
00:35:34,690 --> 00:35:36,880
Um, we'll talk about using Deep Neural Networks to

732
00:35:36,880 --> 00:35:40,045
represent the policy pretty shortly, next week.

733
00:35:40,045 --> 00:35:43,060
So what are we going to do? We're gonna, again, have our weights.

734
00:35:43,060 --> 00:35:44,575
We're gonna have our same approximators.

735
00:35:44,575 --> 00:35:47,390
Now, we're gonna be using Deep Neural Networks.

736
00:35:48,360 --> 00:35:52,165
And in this case, again, just, uh,

737
00:35:52,165 --> 00:35:54,475
to be clear we're gonna be using a Q function,

738
00:35:54,475 --> 00:35:57,025
um, because we're gonna wanna be able to be doing control.

739
00:35:57,025 --> 00:36:01,120
So, we're gonna be doing control, in this case.

740
00:36:01,120 --> 00:36:03,730
Um, and so, we're gonna need to be learning the,

741
00:36:03,730 --> 00:36:05,050
the values of the actions.

742
00:36:05,050 --> 00:36:06,970
Just to be clear here, an Atari, it generally,

743
00:36:06,970 --> 00:36:09,055
doesn't have a really high dimensional action space.

744
00:36:09,055 --> 00:36:13,060
It's discrete. It's normally somewhere between like four to 18, depends on the game.

745
00:36:13,060 --> 00:36:14,770
Um, so, it's fairly low dimensional,

746
00:36:14,770 --> 00:36:17,890
fairly, um, uh, it's discreet and fairly small.

747
00:36:17,890 --> 00:36:21,610
So, though the state space is enormous because it's pixels it's images,

748
00:36:21,610 --> 00:36:24,865
um, uh, the, the action space is pretty small.

749
00:36:24,865 --> 00:36:28,870
Okay. So, just as a reminder, for Q learning,

750
00:36:28,870 --> 00:36:33,265
what we saw is Q learning looks like this for our weights.

751
00:36:33,265 --> 00:36:35,920
We have to have the derivative of our function.

752
00:36:35,920 --> 00:36:38,620
This is not necessarily gonna be linear anymore, um,

753
00:36:38,620 --> 00:36:40,240
but the way we updated our weights,

754
00:36:40,240 --> 00:36:42,055
was we did this, um,

755
00:36:42,055 --> 00:36:44,665
TD backup, where we have this target.

756
00:36:44,665 --> 00:36:48,130
Um, but we're now gonna be taking a max over a,

757
00:36:48,130 --> 00:36:52,285
over our next state and our action and our weights.

758
00:36:52,285 --> 00:36:54,280
Now, notice in this equation,

759
00:36:54,280 --> 00:36:57,400
all the W's you see are identical on the right-hand side.

760
00:36:57,400 --> 00:37:02,650
So, we're using the same weights to represent our current value and we're using

761
00:37:02,650 --> 00:37:04,870
our same weights to plug in and get an estimate of

762
00:37:04,870 --> 00:37:07,825
our future value as well as in our derivative.

763
00:37:07,825 --> 00:37:09,685
And whether we're gonna see is, is uh,

764
00:37:09,685 --> 00:37:11,840
an alternative to that.

765
00:37:11,910 --> 00:37:14,770
Okay. So, their idea was,

766
00:37:14,770 --> 00:37:17,890
we'd really like to be able to use this type of function approximator.

767
00:37:17,890 --> 00:37:20,470
These deep function approximators to do Atari.

768
00:37:20,470 --> 00:37:22,045
They picked Atari in part.

769
00:37:22,045 --> 00:37:24,610
Well, I think at least Dennis and I think, Dennis and David,

770
00:37:24,610 --> 00:37:27,130
both had sort of a joint startup on,

771
00:37:27,130 --> 00:37:29,860
um, video games, a long time ago.

772
00:37:29,860 --> 00:37:32,410
I think it was before David went back to grad school if I remember correctly.

773
00:37:32,410 --> 00:37:34,900
Um, so, they're both interest in this it's clearly,

774
00:37:34,900 --> 00:37:37,720
uh, games are often hard for people to learn.

775
00:37:37,720 --> 00:37:39,580
I'm so, it's a nice sort of, uh,

776
00:37:39,580 --> 00:37:41,875
demonstration of intellect and they thought well,

777
00:37:41,875 --> 00:37:43,465
we can get access to this and,

778
00:37:43,465 --> 00:37:45,025
and there was a paper published.

779
00:37:45,025 --> 00:37:47,860
I'm forgetting when, maybe 2011, 2013,

780
00:37:47,860 --> 00:37:52,630
talking about Atari games and emulators as being sort of interesting challenge for RL.

781
00:37:52,630 --> 00:37:54,700
So, what happens in this case, well,

782
00:37:54,700 --> 00:37:57,070
the state is just gonna be the full image.

783
00:37:57,070 --> 00:37:59,260
The action is gonna be the equivalent of

784
00:37:59,260 --> 00:38:00,940
actions of what you could normally do in the game.

785
00:38:00,940 --> 00:38:03,580
This is normally somewhere between four to 18,

786
00:38:03,580 --> 00:38:06,070
approximately four to 18 actions.

787
00:38:06,070 --> 00:38:08,740
Um, and the reward can be, well,

788
00:38:08,740 --> 00:38:12,295
really whatever you want but you can use the score or some other aspect to,

789
00:38:12,295 --> 00:38:14,485
um, uh, as proxy reward.

790
00:38:14,485 --> 00:38:16,760
Generally we're gonna think about score.

791
00:38:17,220 --> 00:38:19,615
So, what's gonna happen?

792
00:38:19,615 --> 00:38:22,480
Well, they're gonna use a particular input state.

793
00:38:22,480 --> 00:38:24,625
We've talked before about whether or not,

794
00:38:24,625 --> 00:38:27,160
um, a representation is Markov.

795
00:38:27,160 --> 00:38:31,000
In these games, you typically need to have velocity.

796
00:38:31,000 --> 00:38:33,220
So, because you need velocity,

797
00:38:33,220 --> 00:38:35,455
you need more than just the current image.

798
00:38:35,455 --> 00:38:37,555
So what they chose to do is,

799
00:38:37,555 --> 00:38:40,525
you'd need to use four previous frames.

800
00:38:40,525 --> 00:38:44,455
So this at least allows you to catch for a velocity and position,

801
00:38:44,455 --> 00:38:47,395
observe the balls and things like that.

802
00:38:47,395 --> 00:38:50,360
It's not always sufficient.

803
00:38:50,400 --> 00:38:53,995
Can anybody think of an example where maybe an Atari game,

804
00:38:53,995 --> 00:38:55,465
I don't know how many people played Atari.

805
00:38:55,465 --> 00:38:59,935
Um, uh, that might not be sufficient for

806
00:38:59,935 --> 00:39:02,140
the last four images still might not be sufficient

807
00:39:02,140 --> 00:39:04,750
or the type of game where it might not be sufficient.

808
00:39:04,750 --> 00:39:09,190
Yeah.

809
00:39:09,190 --> 00:39:17,260
[inaudible].

810
00:39:17,260 --> 00:39:19,390
Microbes exactly right. So like things like Montezuma's Revenge,

811
00:39:19,390 --> 00:39:23,410
things we often have to get like a key and then you have to grab that key and then, uh,

812
00:39:23,410 --> 00:39:25,330
maybe it's visible on screen, it maybe sad, um,

813
00:39:25,330 --> 00:39:27,955
and, er, maybe it stored in inventory somewhere.

814
00:39:27,955 --> 00:39:31,210
So you have to sort of remember that you have it in order

815
00:39:31,210 --> 00:39:32,860
to make the right decision much later or there might be

816
00:39:32,860 --> 00:39:34,690
some information you've seen early on.

817
00:39:34,690 --> 00:39:37,240
So there are a lot of games and a lot of tasks where, um,

818
00:39:37,240 --> 00:39:40,435
the, even the last four frames will not give you the information you need.

819
00:39:40,435 --> 00:39:42,220
But it's not about approximation,

820
00:39:42,220 --> 00:39:44,290
it's much easier than representing the entire history.

821
00:39:44,290 --> 00:39:48,400
So they started with that. Um, er, so, here in this case,

822
00:39:48,400 --> 00:39:50,230
there's 80 joystick button positions,

823
00:39:50,230 --> 00:39:53,230
um, may or may not need to use all of them in particular game.

824
00:39:53,230 --> 00:39:55,735
And the reward can be the change in score.

825
00:39:55,735 --> 00:40:00,340
Now notice that that can be very helpful or may not be it, depends on the game.

826
00:40:00,340 --> 00:40:01,960
So in some games it takes you a really,

827
00:40:01,960 --> 00:40:04,690
really long time to get to anywhere where your score can possibly change.

828
00:40:04,690 --> 00:40:06,130
Um, so in that case,

829
00:40:06,130 --> 00:40:07,825
you might have a really sparse reward.

830
00:40:07,825 --> 00:40:09,580
In other cases, you're gonna win a reward a lot.

831
00:40:09,580 --> 00:40:12,055
And so that's gonna be much easier to learn what to do.

832
00:40:12,055 --> 00:40:15,160
What are the important things that they, um, did in their paper,

833
00:40:15,160 --> 00:40:17,545
this is a nature paper from 2015,

834
00:40:17,545 --> 00:40:22,615
is they use the same architecture and hyperparameters across all games.

835
00:40:22,615 --> 00:40:25,030
Now just to be clear, they're gonna then learn

836
00:40:25,030 --> 00:40:28,360
different Q functions and different policies for each game.

837
00:40:28,360 --> 00:40:31,990
But their point was that they didn't have to use totally different architectures,

838
00:40:31,990 --> 00:40:33,790
do totally different hyperparameter tuning for

839
00:40:33,790 --> 00:40:36,295
every single game separately in order to get it to work.

840
00:40:36,295 --> 00:40:39,085
It really was the sort of general, um,

841
00:40:39,085 --> 00:40:41,800
architecture and setup was sufficient for them to be able

842
00:40:41,800 --> 00:40:44,860
to learn to make good decisions for all of the games.

843
00:40:44,860 --> 00:40:46,690
I think it was another nice contribution

844
00:40:46,690 --> 00:40:48,625
to this paper is to say well we're going to try to get

845
00:40:48,625 --> 00:40:51,580
sort of a general algorithm and setup that is gonna go much

846
00:40:51,580 --> 00:40:55,135
beyond the sort of normal three examples that we see in reinforcement learning papers.

847
00:40:55,135 --> 00:40:57,775
But just try and get to try to do well in all 50 games.

848
00:40:57,775 --> 00:41:02,920
Again, each agent is gonna learn from scratch in each of the 50 games, um,

849
00:41:02,920 --> 00:41:04,870
because it's gonna do so with the same basic parameters,

850
00:41:04,870 --> 00:41:10,915
same hyperparameters and same neural network so same function approximators act.

851
00:41:10,915 --> 00:41:13,540
And the nice thing is that, I think this is actually required by nature.

852
00:41:13,540 --> 00:41:15,070
They, they released the source code as well.

853
00:41:15,070 --> 00:41:19,435
So you can play around with this. So how did they do it?

854
00:41:19,435 --> 00:41:21,490
Well, they're gonna do value function approximators.

855
00:41:21,490 --> 00:41:23,365
So they're, they're representing the Q function.

856
00:41:23,365 --> 00:41:27,295
They're going to minimize the mean squared lost by stochastic gradient descent.

857
00:41:27,295 --> 00:41:31,810
Uh, but we know that this can diverge with value function approximators.

858
00:41:31,810 --> 00:41:34,015
And what are the two of the problems for this?

859
00:41:34,015 --> 00:41:35,425
Well one is that, uh,

860
00:41:35,425 --> 00:41:43,255
there is this or the correlation between samples which means that if you have s,

861
00:41:43,255 --> 00:41:45,715
a, r, s prime,

862
00:41:45,715 --> 00:41:49,510
a prime, r prime, double prime.

863
00:41:49,510 --> 00:41:55,180
If you think about what the value function or the return is

864
00:41:55,180 --> 00:42:01,450
for us and the value function and the return for S prime, are they independent?

865
00:42:01,450 --> 00:42:05,320
No, right. In fact, like you expect them to be highly correlated with their part,

866
00:42:05,320 --> 00:42:07,720
you know, I mean, it depends on the probability of S prime.

867
00:42:07,720 --> 00:42:09,250
If this is a deterministic system,

868
00:42:09,250 --> 00:42:11,725
the only difference between them will be R. So,

869
00:42:11,725 --> 00:42:12,880
so these are highly correlated,

870
00:42:12,880 --> 00:42:15,010
this is not IID samples when we're doing updates,

871
00:42:15,010 --> 00:42:16,345
there's a lot of correlations.

872
00:42:16,345 --> 00:42:19,990
Um, and also this issue with non-stationary targets. What does that mean?

873
00:42:19,990 --> 00:42:21,460
It means that when you're trying to do

874
00:42:21,460 --> 00:42:24,535
your supervised learning and train your value function predictor,

875
00:42:24,535 --> 00:42:26,710
um, it's not like you always have

876
00:42:26,710 --> 00:42:29,740
the same v pi oracle that's telling you what the true value is.

877
00:42:29,740 --> 00:42:33,550
That's changing over time because you are doing Q-learning to try to estimate what

878
00:42:33,550 --> 00:42:37,405
that is and your policies changing and so it's huge amounts of non-stationarity.

879
00:42:37,405 --> 00:42:40,030
So you don't have a stationary target when you're even just trying to fit

880
00:42:40,030 --> 00:42:43,135
your function because it could be constantly changing at each step.

881
00:42:43,135 --> 00:42:45,490
Um, so you change your po-, you change your weights,

882
00:42:45,490 --> 00:42:48,010
then you change your policy and then now you're gonna change your weights again.

883
00:42:48,010 --> 00:42:50,230
And so, perhaps it's not surprising that things

884
00:42:50,230 --> 00:42:52,825
might be very hard in terms of convergence.

885
00:42:52,825 --> 00:42:55,720
So the way that sort of, uh, DQN,

886
00:42:55,720 --> 00:43:00,505
deep Q-learning addresses these is by experienced replay and fixed Q-targets.

887
00:43:00,505 --> 00:43:03,850
Experienced replay, prime number if you guys have heard about this,

888
00:43:03,850 --> 00:43:07,525
if you learned about DQN before is we're just gonna stroll data.

889
00:43:07,525 --> 00:43:09,790
We've talked about a little bit before of how like

890
00:43:09,790 --> 00:43:11,905
TD learning in their standard approach,

891
00:43:11,905 --> 00:43:13,315
just uses a data point.

892
00:43:13,315 --> 00:43:18,220
Now what I mean by a data point here is really one of these sar, S prime tuples.

893
00:43:18,220 --> 00:43:20,470
In the simplest way of TD Learning or Q-learning,

894
00:43:20,470 --> 00:43:22,210
you use that once and you throw it away.

895
00:43:22,210 --> 00:43:24,115
That's great for data storage,

896
00:43:24,115 --> 00:43:25,975
um, it's not so good for performance.

897
00:43:25,975 --> 00:43:28,900
So the idea is that we're just gonna store this.

898
00:43:28,900 --> 00:43:31,570
Uh, we're gonna keep around some finite buffer of

899
00:43:31,570 --> 00:43:36,955
prior experience and we're gonna re-basically redo Q-learning updates.

900
00:43:36,955 --> 00:43:40,540
Just remember a Q-learning update here would be looking like this.

901
00:43:40,540 --> 00:43:41,710
We would update our weights,

902
00:43:41,710 --> 00:43:44,815
that's considered one update to take a tuple and update the weight.

903
00:43:44,815 --> 00:43:47,860
It's like one stochastic gradient descent update.

904
00:43:47,860 --> 00:43:51,745
And so you can just sample from your experience, um, ah, replay,

905
00:43:51,745 --> 00:43:54,280
your replay buffer and compute the target value given

906
00:43:54,280 --> 00:43:58,465
your current Q function and then you do stochastic gradient descent.

907
00:43:58,465 --> 00:44:03,820
Now notice here because your Q function will be changing over time.

908
00:44:03,820 --> 00:44:07,375
Each time you do your update of the same tuple,

909
00:44:07,375 --> 00:44:09,490
you might have a different target value

910
00:44:09,490 --> 00:44:12,710
because your Q function has changed for that point.

911
00:44:14,430 --> 00:44:17,500
So this is nice because basically it means that you reuse

912
00:44:17,500 --> 00:44:19,690
your data instead of just using each data point once,

913
00:44:19,690 --> 00:44:21,715
you can reuse it and that can be helpful.

914
00:44:21,715 --> 00:44:24,290
And what we'll look at that more in a minute.

915
00:44:26,700 --> 00:44:29,650
So, even though we're treating the target as a scalar,

916
00:44:29,650 --> 00:44:31,360
the weights will get updated the next round which means

917
00:44:31,360 --> 00:44:33,625
our target value changes and so, um,

918
00:44:33,625 --> 00:44:36,940
you can sort of propagate this information and essentially the main idea,

919
00:44:36,940 --> 00:44:39,160
is just that we're gonna use data more than once,

920
00:44:39,160 --> 00:44:40,525
um, and that's often very helpful.

921
00:44:40,525 --> 00:44:42,550
Yes, and, um, name first.

922
00:44:42,550 --> 00:44:49,990
Um, my question is is this equivalent to keeping more frames in our,

923
00:44:49,990 --> 00:44:54,430
uh, representation or is this, uh [inaudible]

924
00:44:54,430 --> 00:44:56,410
It's a great question which is is this equivalent to

925
00:44:56,410 --> 00:44:58,690
keeping more frames in our representation? It's not.

926
00:44:58,690 --> 00:45:00,040
Though that's a really interesting question.

927
00:45:00,040 --> 00:45:01,960
Um, more frames would be like keeping,

928
00:45:01,960 --> 00:45:03,865
uh, a more complicated state representation.

929
00:45:03,865 --> 00:45:05,625
But you can still just use

930
00:45:05,625 --> 00:45:09,060
a state action or word next state tuple once and throw that data way.

931
00:45:09,060 --> 00:45:13,305
This is like saying that periodically I- like let's say I went s1 a1

932
00:45:13,305 --> 00:45:18,090
r1 s2 and then I keep going on and now I'm at like s3 a3 r3 s4.

933
00:45:18,090 --> 00:45:19,260
So, that's really where I am in the world,

934
00:45:19,260 --> 00:45:20,610
I'm now in state four.

935
00:45:20,610 --> 00:45:23,095
It's like I suddenly pretend, oh wait,

936
00:45:23,095 --> 00:45:24,910
I'm gonna pretend that I'm back in s1,

937
00:45:24,910 --> 00:45:26,530
took a1, got r1,

938
00:45:26,530 --> 00:45:29,035
and went to s2 and I'm gonna update my weights again,

939
00:45:29,035 --> 00:45:32,530
and the reason that that update will be different than before is

940
00:45:32,530 --> 00:45:36,775
because I've now updated using my second update and my third update.

941
00:45:36,775 --> 00:45:37,870
So, my Q function,

942
00:45:37,870 --> 00:45:39,535
in general, will be different than before.

943
00:45:39,535 --> 00:45:41,830
So, it'll cause a different weight update.

944
00:45:41,830 --> 00:45:44,185
So, even though it's the same data point as before,

945
00:45:44,185 --> 00:45:46,075
it's gonna cause a different weight update.

946
00:45:46,075 --> 00:45:49,645
In general, one thing we talked about a long time ago is that if you, um, uh,

947
00:45:49,645 --> 00:45:51,430
do TD learning to converge it,

948
00:45:51,430 --> 00:45:54,970
which means that you go over your data mu- like, um, an infinite amount of time.

949
00:45:54,970 --> 00:45:57,250
Um, at least in the tabular case,

950
00:45:57,250 --> 00:46:00,355
that is equivalent to if you learned an MDP model.

951
00:46:00,355 --> 00:46:01,930
You learned the transition dynamics in

952
00:46:01,930 --> 00:46:04,900
the reward model and you just did MDP planning with that.

953
00:46:04,900 --> 00:46:07,090
That's what TD learning converges to,

954
00:46:07,090 --> 00:46:10,585
is that if you repeatedly go through your data in infinite amount of time,

955
00:46:10,585 --> 00:46:13,165
eventually it will converge to as if you'll learn to model,

956
00:46:13,165 --> 00:46:14,500
a dynamics model, a word model,

957
00:46:14,500 --> 00:46:16,945
and then the planning for that which is pretty cool.

958
00:46:16,945 --> 00:46:20,200
So, this is getting us closer to that.

959
00:46:20,200 --> 00:46:23,620
But we don't wanna do that all the time because there's

960
00:46:23,620 --> 00:46:27,040
a computation trade-off and particularly here because we're in games.

961
00:46:27,040 --> 00:46:30,850
Um, there's a direct trade-off between computation and getting more experience.

962
00:46:30,850 --> 00:46:32,470
It's actually a really interesting trade-off

963
00:46:32,470 --> 00:46:34,030
because in these cases it's sort of like should you

964
00:46:34,030 --> 00:46:37,570
think more and plan more and use your old data or should you just gather more experience?

965
00:46:37,570 --> 00:46:39,550
Um, but we can talk more about that later.

966
00:46:39,550 --> 00:46:40,795
Yeah, question and name first please.

967
00:46:40,795 --> 00:46:46,255
And so, the experienced replay buffer has like a fixed size.

968
00:46:46,255 --> 00:46:49,525
Just for, like, clarification of understanding,

969
00:46:49,525 --> 00:46:50,800
are those samples, like,

970
00:46:50,800 --> 00:46:54,010
replaced by new samples after fixed amount of time?

971
00:46:54,010 --> 00:46:58,435
Or is there, like, a specific way to choose what samples to store in the buffer?

972
00:46:58,435 --> 00:47:00,130
That's a great question which is, okay,

973
00:47:00,130 --> 00:47:02,095
this is presumably gonna be a fixed size buffer.

974
00:47:02,095 --> 00:47:05,350
Um, and if it's a fixed size buffer, how do you pick what's in it?

975
00:47:05,350 --> 00:47:07,720
Um, is it the most recent and- and how do you get things,

976
00:47:07,720 --> 00:47:09,610
uh, how do you remove items from it.

977
00:47:09,610 --> 00:47:10,975
It's a really interesting question.

978
00:47:10,975 --> 00:47:12,160
Different people do different things.

979
00:47:12,160 --> 00:47:15,040
Normally, it's often the most recent buffer, um,

980
00:47:15,040 --> 00:47:17,320
can be for example the last one million samples,

981
00:47:17,320 --> 00:47:20,740
which gives you a highlight of how many samples we are gonna be talking about.

982
00:47:20,740 --> 00:47:23,470
But you can make different choices and there's

983
00:47:23,470 --> 00:47:26,125
interesting questions of what thing that you should kick out.

984
00:47:26,125 --> 00:47:30,430
Um, it also depends if your problem is really non-stationary or not,

985
00:47:30,430 --> 00:47:31,720
and I want to mean there's, like,

986
00:47:31,720 --> 00:47:32,965
the real world is non-stationary,

987
00:47:32,965 --> 00:47:36,220
like your customer base is changing. Yeah?

988
00:47:36,220 --> 00:47:38,634
Uh, I'm trying to strike the right balance between

989
00:47:38,634 --> 00:47:41,890
continuing experience like new data points versus re-flagging it.

990
00:47:41,890 --> 00:47:46,165
Can we use something similar to like exploitation versus exploration.

991
00:47:46,165 --> 00:47:49,690
Um, essentially like with random probability just decide to re-flag [inaudible].

992
00:47:49,690 --> 00:47:52,090
The question is about how would we choose between,

993
00:47:52,090 --> 00:47:53,740
like, what, um, you know,

994
00:47:53,740 --> 00:47:56,020
getting new data and how much to replay et cetera, um,

995
00:47:56,020 --> 00:47:58,750
and could we do that sort of as an exploration-exploitation trade-off.

996
00:47:58,750 --> 00:48:00,520
I think this is generally understudied but

997
00:48:00,520 --> 00:48:02,230
there's lots of different heuristics people use.

998
00:48:02,230 --> 00:48:05,500
Often people have some of- sort of a fixed ratio of how much

999
00:48:05,500 --> 00:48:09,085
they're updating based on the experience replay versus getting,

1000
00:48:09,085 --> 00:48:11,890
um, putting new samples into there.

1001
00:48:11,890 --> 00:48:14,365
So, generally right now is really heuristic trade-off.

1002
00:48:14,365 --> 00:48:16,630
Could certainly imagine trying to optimally figure this

1003
00:48:16,630 --> 00:48:18,775
out but that also requires computation.

1004
00:48:18,775 --> 00:48:20,620
Um, this gets us into the really interesting question

1005
00:48:20,620 --> 00:48:22,795
of metacomputation and metacognition.

1006
00:48:22,795 --> 00:48:25,090
Um, but if, you know, your agent thinking about how to prioritize

1007
00:48:25,090 --> 00:48:27,475
its own computation which is a super cool problem.

1008
00:48:27,475 --> 00:48:28,990
Which is what we solve all the time.

1009
00:48:28,990 --> 00:48:31,780
Okay. So, um, the second thing that

1010
00:48:31,780 --> 00:48:35,365
DQN does is it first have- it first keeps route this old data.

1011
00:48:35,365 --> 00:48:38,755
The second thing that it does is it has fixed Q targets.

1012
00:48:38,755 --> 00:48:41,740
So, what does that mean? Um, so to improve stability,

1013
00:48:41,740 --> 00:48:44,800
and what we mean by stability here is that we don't want our weights to explode and go to

1014
00:48:44,800 --> 00:48:48,090
infinity which we saw could happen in linear value function.

1015
00:48:48,090 --> 00:48:50,670
Um, we're gonna fix the target weights that are used in

1016
00:48:50,670 --> 00:48:53,430
the target calculation for multiple updates.

1017
00:48:53,430 --> 00:48:56,490
So, remember here what I mean by the target calculation here is

1018
00:48:56,490 --> 00:49:00,740
that reward plus Gamma V of S prime.

1019
00:49:00,740 --> 00:49:05,965
So, this itself is a function of w and we're gonna

1020
00:49:05,965 --> 00:49:11,370
fix the w we use in that value of S prime for several rounds.

1021
00:49:11,370 --> 00:49:14,490
So, instead of always update- taking whatever the most recent one is,

1022
00:49:14,490 --> 00:49:19,020
we're just gonna fix it for awhile and that's basically like making this more stable.

1023
00:49:19,020 --> 00:49:22,830
Because this, in general,

1024
00:49:22,830 --> 00:49:28,540
is an approximation of the oracle of V star.

1025
00:49:28,800 --> 00:49:32,935
So, you'd really like an oracle to just give this to you every time you reach, you know,

1026
00:49:32,935 --> 00:49:36,280
an S prime or you take an action [inaudible] and go to S prime,

1027
00:49:36,280 --> 00:49:38,305
you'd like an oracle to give you what the true value is.

1028
00:49:38,305 --> 00:49:39,865
You don't have that, um,

1029
00:49:39,865 --> 00:49:43,105
and it could change on every single step because you could be updating the weights.

1030
00:49:43,105 --> 00:49:45,235
What this is saying is, don't do that,

1031
00:49:45,235 --> 00:49:50,005
keep the weights fixed that used to compute VS prime for a little while,

1032
00:49:50,005 --> 00:49:51,265
maybe for 10 steps,

1033
00:49:51,265 --> 00:49:53,275
maybe for a 100 steps, um,

1034
00:49:53,275 --> 00:49:55,135
and that just makes the target,

1035
00:49:55,135 --> 00:49:57,100
the sort of the thing that you're trying to minimize

1036
00:49:57,100 --> 00:49:59,755
your loss with respect to, more stable.

1037
00:49:59,755 --> 00:50:02,725
So, we're gonna have, um, we still have

1038
00:50:02,725 --> 00:50:04,390
our single network but we're just gonna maintain

1039
00:50:04,390 --> 00:50:06,565
two different sets of weights for that network.

1040
00:50:06,565 --> 00:50:09,850
Um, one is gonna be this weight minus.

1041
00:50:09,850 --> 00:50:12,295
I'll call it minus because, um,

1042
00:50:12,295 --> 00:50:15,295
well there might be other conventions but in particular it's the older set of weights,

1043
00:50:15,295 --> 00:50:16,690
the ones we're not updating right now.

1044
00:50:16,690 --> 00:50:19,840
Those are the ones that we're using them as target calculation.

1045
00:50:19,840 --> 00:50:23,230
So, those are the ones we're gonna use when we want to figure out the value of S

1046
00:50:23,230 --> 00:50:27,890
prime and then we have some other W which is what we're using to update.

1047
00:50:28,230 --> 00:50:31,960
So, when we compute our target value we, again,

1048
00:50:31,960 --> 00:50:37,090
can sample and experience tuple from the dataset from our experience replay buffer,

1049
00:50:37,090 --> 00:50:40,330
compute the target value using our w minus,

1050
00:50:40,330 --> 00:50:44,260
and then we use stochastic gradient descent to update the network weights.

1051
00:50:44,260 --> 00:50:50,815
So, this is used with minus this is used with the current one. Yeah?

1052
00:50:50,815 --> 00:50:54,580
So, uh, I guess two questions like intuitively,

1053
00:50:54,580 --> 00:50:56,590
why does this is help and, like,

1054
00:50:56,590 --> 00:50:59,260
why does it make it more stable and, like, secondly, like,

1055
00:50:59,260 --> 00:51:02,650
are there any other benefits on the stability from doing this?

1056
00:51:02,650 --> 00:51:04,195
These are two questions, one is,

1057
00:51:04,195 --> 00:51:05,350
intuitively, why does this help?

1058
00:51:05,350 --> 00:51:07,150
Um, which is a great question and second of all,

1059
00:51:07,150 --> 00:51:09,265
beyond the stability, is there any other benefits?

1060
00:51:09,265 --> 00:51:11,605
So, intuitively, why does this help,

1061
00:51:11,605 --> 00:51:13,330
um, in terms of stability?

1062
00:51:13,330 --> 00:51:15,490
In terms of stability, it helps because you're

1063
00:51:15,490 --> 00:51:18,205
basically reducing the noise in your target.

1064
00:51:18,205 --> 00:51:20,530
If you think back to Monte Carlo, um,

1065
00:51:20,530 --> 00:51:26,410
there instead of using this target like this bootstrap target where we're using GT.

1066
00:51:26,410 --> 00:51:29,650
So, in Monte Carlo, we used GT and I told you the nice thing

1067
00:51:29,650 --> 00:51:32,950
about that was that it was an unbiased estimator of V pie.

1068
00:51:32,950 --> 00:51:35,035
But the downside was that it was high variance

1069
00:51:35,035 --> 00:51:37,675
because you're summing up the rewards till the end of the episode.

1070
00:51:37,675 --> 00:51:41,260
Um, and so if things are high-variance when you're trying to regress on them,

1071
00:51:41,260 --> 00:51:42,535
it's gonna be more noisy,

1072
00:51:42,535 --> 00:51:44,395
um, and you could [inaudible] gradients.

1073
00:51:44,395 --> 00:51:47,545
Imagine that we do something- take the extreme of this, if we want to be,

1074
00:51:47,545 --> 00:51:51,460
um, for stability, you could always make your target equal to a constant.

1075
00:51:51,460 --> 00:51:54,040
You could always make it equal to zero for example,

1076
00:51:54,040 --> 00:51:56,245
and if you kept your target fixed forever,

1077
00:51:56,245 --> 00:51:58,990
you would learn the weights that- that minimize the error to

1078
00:51:58,990 --> 00:52:02,680
a constant function and that would then be stable because you always

1079
00:52:02,680 --> 00:52:05,830
have the same target value that you're always trying to predict and

1080
00:52:05,830 --> 00:52:07,060
eventually you'd learn that you should just set

1081
00:52:07,060 --> 00:52:09,400
your w equal to zero and- and that would be fine.

1082
00:52:09,400 --> 00:52:13,615
So, this is just reducing the noise and the target that we're trying to sort of,

1083
00:52:13,615 --> 00:52:15,790
um, if you think of this as a supervised learning problem,

1084
00:52:15,790 --> 00:52:17,710
we have an input x and output y.

1085
00:52:17,710 --> 00:52:20,260
The challenge in RL is that our y is changing,

1086
00:52:20,260 --> 00:52:24,655
if you make it that you're- so your y is not changing, it's much easier to fit.

1087
00:52:24,655 --> 00:52:26,890
Um, unless I convince whether or not there's, uh,

1088
00:52:26,890 --> 00:52:28,450
any benefit beyond stability.

1089
00:52:28,450 --> 00:52:30,295
I think mostly not, um,

1090
00:52:30,295 --> 00:52:34,660
I- the- this is also sort of reducing how quickly you propagate

1091
00:52:34,660 --> 00:52:36,070
information because you're using

1092
00:52:36,070 --> 00:52:39,910
an- a stale set of weights to represent the value of a state.

1093
00:52:39,910 --> 00:52:42,910
So, you might misestimate the value of a state because you haven't updated it

1094
00:52:42,910 --> 00:52:45,790
with holding permit- with new information. Yeah?

1095
00:52:45,790 --> 00:52:51,460
Uh, assuming we want to do [inaudible] approximator.

1096
00:52:51,460 --> 00:52:54,760
Is there something that's specific to the deep neural networks?.

1097
00:52:54,760 --> 00:52:55,960
That's a great questions which is,

1098
00:52:55,960 --> 00:52:57,820
is this specific to deep neural networks or can we use

1099
00:52:57,820 --> 00:52:59,905
this with linear value function approximate or any value par-,

1100
00:52:59,905 --> 00:53:01,945
you can use those to any value function approximators.

1101
00:53:01,945 --> 00:53:03,520
Yeah, this is not specific.

1102
00:53:03,520 --> 00:53:04,840
This is really just about stability and

1103
00:53:04,840 --> 00:53:06,850
that's- that's true for the experience replay too.

1104
00:53:06,850 --> 00:53:10,675
Experience replay is just kinda propagate information more- more effectively and,

1105
00:53:10,675 --> 00:53:12,385
um, this is just gonna make it more stable.

1106
00:53:12,385 --> 00:53:15,700
Uh, so these aren't sort of unique using deep neural network.

1107
00:53:15,700 --> 00:53:17,890
I think they were just more worried about the stability with

1108
00:53:17,890 --> 00:53:21,310
these really complicated function approximators. Yeah, in the red.

1109
00:53:21,310 --> 00:53:23,570
Do you every update the-

1110
00:53:23,570 --> 00:53:26,340
Minus at all, or is that [inaudible].

1111
00:53:26,340 --> 00:53:28,140
Great question. So, the- Di- Dell?

1112
00:53:28,140 --> 00:53:29,160
Dian.

1113
00:53:29,160 --> 00:53:30,975
Dian. Dian's question is whether or not,

1114
00:53:30,975 --> 00:53:32,565
um, we ever update w minus, yes we do.

1115
00:53:32,565 --> 00:53:34,995
We pu- can periodically update w minus as well.

1116
00:53:34,995 --> 00:53:37,425
So, in a fixed schedule,

1117
00:53:37,425 --> 00:53:39,000
say every 50 or, you know,

1118
00:53:39,000 --> 00:53:41,925
every n episodes or every n steps,

1119
00:53:41,925 --> 00:53:50,810
you, um, update sort of like every- and you would set w minus dw. Yeah?

1120
00:53:50,810 --> 00:53:52,485
I was thinking, like,

1121
00:53:52,485 --> 00:53:55,650
given that we know that this is work for

1122
00:53:55,650 --> 00:54:00,720
gradient descent and you're not using the same kind of structure as gradient descent,

1123
00:54:00,720 --> 00:54:02,460
you're using to create, you know,

1124
00:54:02,460 --> 00:54:06,960
different option subtracting the [inaudible] value of- of the function.

1125
00:54:06,960 --> 00:54:08,550
How is this supposed to, like,

1126
00:54:08,550 --> 00:54:10,080
not grade- grade- gradient descent,

1127
00:54:10,080 --> 00:54:12,810
like, all those assumptions from [inaudible]

1128
00:54:12,810 --> 00:54:16,620
Your question is, okay this- does this really work in terms of gradient descent?

1129
00:54:16,620 --> 00:54:17,850
This is not- I mean,

1130
00:54:17,850 --> 00:54:19,290
the- it's a great question,

1131
00:54:19,290 --> 00:54:22,710
and these sort of Q learning are not true gradient descent methods.

1132
00:54:22,710 --> 00:54:24,540
They're- they're are approximations to such,

1133
00:54:24,540 --> 00:54:26,775
they often do shockingly well given that.

1134
00:54:26,775 --> 00:54:28,380
Some of the more recent ones which,

1135
00:54:28,380 --> 00:54:31,320
um, uh, Chapter 11 has a nice discussion of this,

1136
00:54:31,320 --> 00:54:32,550
sort of the GTD's or

1137
00:54:32,550 --> 00:54:36,210
gradient temporal difference learning are more true gradient descent algorithms.

1138
00:54:36,210 --> 00:54:37,800
These are really just approximations and it's,

1139
00:54:37,800 --> 00:54:39,000
uh, this, um, uh,

1140
00:54:39,000 --> 00:54:42,525
as to the point, this has no guarantees of convergence still.

1141
00:54:42,525 --> 00:54:45,645
This is hopefully gonna help but we have no guarantees. Yeah?

1142
00:54:45,645 --> 00:54:47,820
Uh, George, uh, so in practice,

1143
00:54:47,820 --> 00:54:51,330
do people have some cyclical pattern and how can they refresh

1144
00:54:51,330 --> 00:54:55,260
the- the gradient that's used to compute, uh, the gradients?

1145
00:54:55,260 --> 00:54:57,315
Yeah, his question is, um, you know,

1146
00:54:57,315 --> 00:55:01,110
in practice are there some sort of cyclical pattern of how often you update w minus.

1147
00:55:01,110 --> 00:55:04,110
Yes, yeah there's often particular patterns or- or hyper-

1148
00:55:04,110 --> 00:55:07,335
it's a hyperparameter choice of how quickly and how frequently you update this.

1149
00:55:07,335 --> 00:55:12,105
Um, and it will trade-off between propagating information fester,

1150
00:55:12,105 --> 00:55:14,280
um, and possibly being less stable.

1151
00:55:14,280 --> 00:55:16,170
So. If you make, um, you know,

1152
00:55:16,170 --> 00:55:19,455
if n here is one that you're back to standard TD learning.

1153
00:55:19,455 --> 00:55:22,305
If n is infinity, that means you've never updated it.

1154
00:55:22,305 --> 00:55:25,260
Um, so, there's a- there's a smooth continuum there. William?

1155
00:55:25,260 --> 00:55:27,585
Uh, we notice, like, for w,

1156
00:55:27,585 --> 00:55:30,840
there are better initializations than just like zero,

1157
00:55:30,840 --> 00:55:32,940
uh, something, like, if you take into account,

1158
00:55:32,940 --> 00:55:34,230
I guess like the mean and variance.

1159
00:55:34,230 --> 00:55:37,050
Uh, would you initialize w minus just two

1160
00:55:37,050 --> 00:55:41,205
w or is there like an even better initialization for w minus?

1161
00:55:41,205 --> 00:55:43,140
Yeah, his questions is about, you know,

1162
00:55:43,140 --> 00:55:44,460
the- the impact of how we,

1163
00:55:44,460 --> 00:55:46,740
um, uh, initialize w ca- can matter.

1164
00:55:46,740 --> 00:55:49,890
Um, uh, and is the- how do we initialize w minus.

1165
00:55:49,890 --> 00:55:53,625
Typically, we initialize w minus to be exactly the same as w at the beginning.

1166
00:55:53,625 --> 00:55:56,295
Um, the choice of it will also affect, uh,

1167
00:55:56,295 --> 00:55:59,870
certainly the early performance. Those are great questions.

1168
00:55:59,870 --> 00:56:02,600
Let me keep going because I wanna make sure we get to some of the extensions as well.

1169
00:56:02,600 --> 00:56:05,195
Um, so just to summarize how DQN works.

1170
00:56:05,195 --> 00:56:09,695
Um, the main two innovations that data- it uses experienced replay and fixed Q targets.

1171
00:56:09,695 --> 00:56:12,590
It stores the transition in this sort of replay buffer,

1172
00:56:12,590 --> 00:56:13,880
a replay memory, um,

1173
00:56:13,880 --> 00:56:17,040
use sample random mini-batches from D. So,

1174
00:56:17,040 --> 00:56:19,125
normally sample in mini-batch instead of a single one.

1175
00:56:19,125 --> 00:56:22,440
So, maybe a sample 1- or whatever other parameter.

1176
00:56:22,440 --> 00:56:25,200
You do your gradient descent given those.

1177
00:56:25,200 --> 00:56:28,020
Um, you compute Q learning using these old targets and you

1178
00:56:28,020 --> 00:56:31,230
optimize the mean squared error between the Q network and Q learning targets,

1179
00:56:31,230 --> 00:56:32,640
use stochastic gradient descent,

1180
00:56:32,640 --> 00:56:34,380
and something I did not mention on here is that we're

1181
00:56:34,380 --> 00:56:37,425
typically doing E-greedy exploration.

1182
00:56:37,425 --> 00:56:41,700
So, you need some schedule here too for how to do E-greedy.

1183
00:56:41,700 --> 00:56:43,620
So, they were not doing, um,

1184
00:56:43,620 --> 00:56:46,860
sophisticated exploration in their original paper.

1185
00:56:46,860 --> 00:56:48,720
So, this is what it looks like.

1186
00:56:48,720 --> 00:56:51,030
You sort of go in and you do multiple different convolutions.

1187
00:56:51,030 --> 00:56:52,935
They have the images, um,

1188
00:56:52,935 --> 00:56:57,820
and they do some fully connected layers and then the output a Q value for each action.

1189
00:56:58,730 --> 00:57:01,155
Let me just bring it up.

1190
00:57:01,155 --> 00:57:03,585
Um, for those of you who haven't seen it before.

1191
00:57:03,585 --> 00:57:06,345
So, the nice thing is what they- so,

1192
00:57:06,345 --> 00:57:08,700
you're about to see breakout which is, um,

1193
00:57:08,700 --> 00:57:11,100
an Atari game and what they do is they show you

1194
00:57:11,100 --> 00:57:13,650
sort of the performance of what the agent is doing.

1195
00:57:13,650 --> 00:57:16,380
So, remember the agent's just learning from pixels here how to do this.

1196
00:57:16,380 --> 00:57:19,560
So, it was pretty extraordinary when they showed this in about 2014.

1197
00:57:19,560 --> 00:57:22,920
Um, and the beginning of its learning sort of this policy.

1198
00:57:22,920 --> 00:57:25,665
You can see it's not making- doing the right thing very much, um,

1199
00:57:25,665 --> 00:57:28,890
and that over time as it gets more episodes it starting

1200
00:57:28,890 --> 00:57:32,685
to learn to make better decisions about how to do it.

1201
00:57:32,685 --> 00:57:38,130
Um, and one of the interesting things about it is that as you'd hope,

1202
00:57:38,130 --> 00:57:39,630
as it gets more and more data,

1203
00:57:39,630 --> 00:57:41,910
it learns to make better decisions.

1204
00:57:41,910 --> 00:57:45,305
But one of the things people like about this a lot is that,

1205
00:57:45,305 --> 00:57:48,215
uh, you can learn to exploit,

1206
00:57:48,215 --> 00:57:49,580
um, the reward function.

1207
00:57:49,580 --> 00:57:53,120
Uh, so in this case,

1208
00:57:53,120 --> 00:57:56,915
um, it figures out that if you really just want me to maximize the expected reward,

1209
00:57:56,915 --> 00:57:58,940
what the best thing for me to do is to just kind of

1210
00:57:58,940 --> 00:58:00,950
get a hole through there and then as soon as I

1211
00:58:00,950 --> 00:58:05,360
can start to just bounce around the top [inaudible].

1212
00:58:05,360 --> 00:58:09,750
Um, and so this is one of the things where, you know,

1213
00:58:09,750 --> 00:58:12,060
if you ask the agent to maximize the reward,

1214
00:58:12,060 --> 00:58:14,460
it'll- it'll learn the right way to maximize the reward given enough data.

1215
00:58:14,460 --> 00:58:17,940
Um, and so this is really cool that sort of it could discover things that maybe

1216
00:58:17,940 --> 00:58:19,350
are strategies that people take

1217
00:58:19,350 --> 00:58:22,480
a little while to learn when they're first learning the game as well.

1218
00:58:26,980 --> 00:58:29,030
So, when they did this,

1219
00:58:29,030 --> 00:58:30,185
they then showed, um,

1220
00:58:30,185 --> 00:58:32,450
some pretty amazing performance on a lot of different games.

1221
00:58:32,450 --> 00:58:34,745
Many games they could do as well as humans.

1222
00:58:34,745 --> 00:58:36,870
Now, to be precise here- oh yeah

1223
00:58:36,870 --> 00:58:38,730
I'm sorry.

1224
00:58:38,730 --> 00:58:40,200
Uh, I'm just wondering why, um,

1225
00:58:40,200 --> 00:58:42,990
it's playing, like, why was it [inaudible] around a lot, like,

1226
00:58:42,990 --> 00:58:45,300
it wasn't sure of its movements like it moved

1227
00:58:45,300 --> 00:58:48,165
around places often, like [OVERLAPPING]. Yeah.

1228
00:58:48,165 --> 00:58:49,680
Yeah, so, um, you might see, uh,

1229
00:58:49,680 --> 00:58:52,830
I think she is talking- she is referring to the fact that the paddle was moving a lot.

1230
00:58:52,830 --> 00:58:54,840
As the agent is trying to learn, like,

1231
00:58:54,840 --> 00:58:56,040
when we see that, you sort of go,

1232
00:58:56,040 --> 00:58:57,135
"Why would he jerk a lot."

1233
00:58:57,135 --> 00:58:59,985
From the agent's perspective, particularly if there's a cost to moving,

1234
00:58:59,985 --> 00:59:02,220
then it may just be kind of babbling, uh,

1235
00:59:02,220 --> 00:59:04,860
and doing exploration just to see what works and- and it-

1236
00:59:04,860 --> 00:59:07,890
from our perspective that's clearly sort of an inexperienced player to do that.

1237
00:59:07,890 --> 00:59:09,390
That would be a strange thing but from

1238
00:59:09,390 --> 00:59:11,475
the agent's perspective, that's completely reasonable.

1239
00:59:11,475 --> 00:59:13,860
Um, and it does not give him positive or negative reward from that.

1240
00:59:13,860 --> 00:59:15,720
So, it can't distinguish between, you know,

1241
00:59:15,720 --> 00:59:17,895
stay in stationary versus going left or right.

1242
00:59:17,895 --> 00:59:21,390
If you put it in a cost for movement that could help. Yeah?

1243
00:59:21,390 --> 00:59:25,290
This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer?

1244
00:59:25,290 --> 00:59:27,150
Puling layer? There might be one in there.

1245
00:59:27,150 --> 00:59:30,675
I- the- the- I don't remember the complete arc- network architecture, um.

1246
00:59:30,675 --> 00:59:32,490
The question is whether or not there's a pulling layer in there.

1247
00:59:32,490 --> 00:59:34,140
I think there prob- there might be inside.

1248
00:59:34,140 --> 00:59:36,390
There has- they have to be going from images all the way up.

1249
00:59:36,390 --> 00:59:39,375
But they have the complete architecture. It's a good question.

1250
00:59:39,375 --> 00:59:42,540
So, the next thing that you can see here is that, um,

1251
00:59:42,540 --> 00:59:46,140
they got sort of human level performance on a number of different Atari games.

1252
00:59:46,140 --> 00:59:47,460
There's about 50 games up here.

1253
00:59:47,460 --> 00:59:48,960
Um, just to be clear here.

1254
00:59:48,960 --> 00:59:51,945
When they say human level performance that means asymptotically.

1255
00:59:51,945 --> 00:59:54,555
So, after they have trained their agent, this-, uh,

1256
00:59:54,555 --> 00:59:57,090
they're not talking about how long it took them or

1257
00:59:57,090 --> 01:00:00,180
their agent to learn and as you guys will find out for homework two,

1258
01:00:00,180 --> 01:00:01,725
it can be a lot of experience.

1259
01:00:01,725 --> 01:00:05,475
Um, uh, a lot of time to learn how to make- do a good performance.

1260
01:00:05,475 --> 01:00:07,230
But nevertheless, there are a lot of cases where that might

1261
01:00:07,230 --> 01:00:09,165
be reasonable in terms of games.

1262
01:00:09,165 --> 01:00:11,070
So, they did very well on some domains.

1263
01:00:11,070 --> 01:00:12,735
Some domains, they did very poorly.

1264
01:00:12,735 --> 01:00:15,870
Um, there's been a lot of interest in these sort of games on the bottom end of

1265
01:00:15,870 --> 01:00:19,200
the tail which often known as those hard exploration games.

1266
01:00:19,200 --> 01:00:23,175
We'll probably talk- uh, we'll talk a lot more about exploration later on in the course.

1267
01:00:23,175 --> 01:00:25,410
So, what was critical?

1268
01:00:25,410 --> 01:00:27,840
So, I- I like the, uh, there's a lot of really lovely things about

1269
01:00:27,840 --> 01:00:30,780
this paper and one of the really nice things is that they did a nice ablation study,

1270
01:00:30,780 --> 01:00:32,730
um, uh, for us to sort of understand what were

1271
01:00:32,730 --> 01:00:35,925
the important features and if you look at these numbers.

1272
01:00:35,925 --> 01:00:39,870
Um, I think that it's clear that the really important feature is replay.

1273
01:00:39,870 --> 01:00:42,735
So, this is their performance using a linear network,

1274
01:00:42,735 --> 01:00:45,270
deeply network seemed to not help so much.

1275
01:00:45,270 --> 01:00:47,445
Using that fixed Q. Um,

1276
01:00:47,445 --> 01:00:49,515
fixed Q here means you seem like a fixed target.

1277
01:00:49,515 --> 01:00:51,585
Okay, that gets you a little bit three from ten.

1278
01:00:51,585 --> 01:00:54,045
You do replay and suddenly you're at 241.

1279
01:00:54,045 --> 01:00:56,520
Okay, so throwing away each data point what-

1280
01:00:56,520 --> 01:00:59,070
after you use it once is not a very good thing to do.

1281
01:00:59,070 --> 01:01:00,555
You want to reuse that data.

1282
01:01:00,555 --> 01:01:02,880
Um, and then if you combine replay and

1283
01:01:02,880 --> 01:01:06,330
fixed Q you do get an improvement over that but, uh,

1284
01:01:06,330 --> 01:01:09,075
it's really that you get this huge increase,

1285
01:01:09,075 --> 01:01:13,020
um, uh, at least in break out in some of the other games by doing replay.

1286
01:01:13,020 --> 01:01:14,730
Now, in some other ones, um,

1287
01:01:14,730 --> 01:01:16,770
you start to get a significant improvement as soon as you

1288
01:01:16,770 --> 01:01:19,035
use a more complicated function approximators.

1289
01:01:19,035 --> 01:01:21,840
But in general, replay is hugely important

1290
01:01:21,840 --> 01:01:25,155
and it just gives us a much better way to use the data. Yeah?

1291
01:01:25,155 --> 01:01:28,830
Um, you know, because here in this table it seems like you'd want to use replay and

1292
01:01:28,830 --> 01:01:32,820
fixed Q with the linear model and that it might be a mistake to be using,

1293
01:01:32,820 --> 01:01:34,650
uh, a deep model here.

1294
01:01:34,650 --> 01:01:37,785
Do you agree with that table reference table or?

1295
01:01:37,785 --> 01:01:39,510
So, the question is like, "Well, maybe we could use,

1296
01:01:39,510 --> 01:01:41,220
like, linear-" also I guess I should be clear.

1297
01:01:41,220 --> 01:01:45,630
So, this is all- everything on the- the next four were all deep.

1298
01:01:45,630 --> 01:01:48,480
So, they don't have here linear plus replay.

1299
01:01:48,480 --> 01:01:51,150
But you could certainly imagine trying linear plus

1300
01:01:51,150 --> 01:01:53,340
replay and it seems like you might do very well here,

1301
01:01:53,340 --> 01:01:55,110
it might depend on which features you're using.

1302
01:01:55,110 --> 01:01:57,090
There's some cool work, um, uh,

1303
01:01:57,090 --> 01:01:58,470
over the last few years looking also at,

1304
01:01:58,470 --> 01:02:00,270
uh, whether you can combine these two.

1305
01:02:00,270 --> 01:02:02,100
So, we've done some work, um,

1306
01:02:02,100 --> 01:02:03,525
using a Bayesian last layer,

1307
01:02:03,525 --> 01:02:06,180
using like Bayesian linear regression which is useful for uncertainty.

1308
01:02:06,180 --> 01:02:10,440
Other people have just done linear regression where the idea is you- you sort of, um, uh,

1309
01:02:10,440 --> 01:02:13,785
deep neural network up to a certain point and then you do,

1310
01:02:13,785 --> 01:02:16,110
um, kind of direct linear regression

1311
01:02:16,110 --> 01:02:18,150
to fit exactly what the weights are at the final layer.

1312
01:02:18,150 --> 01:02:20,235
So, that can be much more efficient,

1313
01:02:20,235 --> 01:02:23,380
um, but you still have a complicated representation.

1314
01:02:25,130 --> 01:02:27,480
All right. So, since then,

1315
01:02:27,480 --> 01:02:30,375
there's been a huge number amount of interest in this area.

1316
01:02:30,375 --> 01:02:31,860
Um, ah, so, again,

1317
01:02:31,860 --> 01:02:33,180
dating myself reinforcement learning,

1318
01:02:33,180 --> 01:02:35,280
we used to go and give a talk about reinforcement learning,

1319
01:02:35,280 --> 01:02:36,660
and like 40 people would show up,

1320
01:02:36,660 --> 01:02:38,100
but most of them you knew, and,

1321
01:02:38,100 --> 01:02:39,780
um, and then, uh,

1322
01:02:39,780 --> 01:02:40,950
and then it started really changing.

1323
01:02:40,950 --> 01:02:44,100
I think I was maybe in 2016, when, um, er,

1324
01:02:44,100 --> 01:02:46,770
ICML, I was in New York and like suddenly

1325
01:02:46,770 --> 01:02:49,350
there were 400 people in the room for reinforcement learning talks.

1326
01:02:49,350 --> 01:02:53,580
Um, and then, this year at NLP's which is one of the major machine-learning conferences,

1327
01:02:53,580 --> 01:02:55,050
it's sold out in like eight minutes.

1328
01:02:55,050 --> 01:02:57,240
Um, so, there were 8,000 people there,

1329
01:02:57,240 --> 01:02:59,490
and there was a huge amount of interest in deep learning,

1330
01:02:59,490 --> 01:03:00,540
and for the deep learning workshop,

1331
01:03:00,540 --> 01:03:03,330
you sort of have a 2,000 person auditorium.

1332
01:03:03,330 --> 01:03:06,150
So, there's been a huge amount of excitement based on this work,

1333
01:03:06,150 --> 01:03:07,800
which I think really a huge credit to- to

1334
01:03:07,800 --> 01:03:10,425
deep mind and to the work that David Silver and others have been doing,

1335
01:03:10,425 --> 01:03:12,885
um, to sort of show that this was possible.

1336
01:03:12,885 --> 01:03:14,850
Uh, some of the immediate improvements that we're

1337
01:03:14,850 --> 01:03:17,070
going to go through really quickly here is,

1338
01:03:17,070 --> 01:03:20,745
um, Doubled DQN, prioritize replay, and dueling DQN.

1339
01:03:20,745 --> 01:03:22,260
Um, there's been way,

1340
01:03:22,260 --> 01:03:23,490
way, way more papers in that,

1341
01:03:23,490 --> 01:03:28,530
but these are some of the early really big improvements on top of DQN.

1342
01:03:28,530 --> 01:03:32,910
So, double DQN is kind of like double Q learning,

1343
01:03:32,910 --> 01:03:36,390
which we covered very briefly at the end of a couple of classes ago.

1344
01:03:36,390 --> 01:03:40,965
The thing that we discussed there was this sort of maximization bias, is that,

1345
01:03:40,965 --> 01:03:46,260
um, the max of estimated state action values can be a biased estimator of the true max.

1346
01:03:46,260 --> 01:03:49,905
So, we talked really briefly about double Q learning.

1347
01:03:49,905 --> 01:03:52,230
Um, so, a double Q learning,

1348
01:03:52,230 --> 01:03:56,115
the idea was that we are going to maintain two different Q networks.

1349
01:03:56,115 --> 01:03:58,770
Uh, we can select our action using,

1350
01:03:58,770 --> 01:04:02,295
like an E Greedy Policy where we average between those Q networks,

1351
01:04:02,295 --> 01:04:05,850
and then we'll observe a reward in a state and we basically

1352
01:04:05,850 --> 01:04:09,405
use one of the Qs as the target for the other.

1353
01:04:09,405 --> 01:04:11,175
So, if, you know,

1354
01:04:11,175 --> 01:04:13,170
with a 50 percent probability,

1355
01:04:13,170 --> 01:04:15,640
we're going to update one network,

1356
01:04:15,890 --> 01:04:22,300
and we're going to do that by using picking the action from the other network.

1357
01:04:22,490 --> 01:04:28,455
This is to try to separate how we pick our action

1358
01:04:28,455 --> 01:04:30,570
versus our estimate of the value of

1359
01:04:30,570 --> 01:04:34,905
that action to deal with this sort of maximization bias issue.

1360
01:04:34,905 --> 01:04:37,365
Then with 50 percent other probability,

1361
01:04:37,365 --> 01:04:43,470
we update Q2, and we pick the next action from the other network.

1362
01:04:45,980 --> 01:04:48,585
So, this is a pretty small change,

1363
01:04:48,585 --> 01:04:50,609
it means you have to- you have to maintain

1364
01:04:50,609 --> 01:04:53,295
two different networks or two different sets of weights,

1365
01:04:53,295 --> 01:04:56,145
um, and it can be pretty helpful.

1366
01:04:56,145 --> 01:04:59,554
So, um, if you extend this idea to DQN,

1367
01:04:59,554 --> 01:05:01,415
you have sort of our current Q network,

1368
01:05:01,415 --> 01:05:05,535
w select actions, and this older one to evaluate actions.

1369
01:05:05,535 --> 01:05:10,050
So, you can put this in there to do action selection,

1370
01:05:10,050 --> 01:05:12,180
and then you can evaluate the value of it with your

1371
01:05:12,180 --> 01:05:15,795
other networks- other, other network weights.

1372
01:05:15,795 --> 01:05:17,820
So, it's a fairly small change,

1373
01:05:17,820 --> 01:05:23,190
it's very similar to what we were doing already for the target network, network weights.

1374
01:05:23,190 --> 01:05:26,100
It turns out that it gives you a huge benefit in many,

1375
01:05:26,100 --> 01:05:28,050
many cases for the Atari games.

1376
01:05:28,050 --> 01:05:32,835
So, uh, this is something that's generally very useful to do, um,

1377
01:05:32,835 --> 01:05:36,105
and gives you sort- of sort of immediate significant boost in performance,

1378
01:05:36,105 --> 01:05:39,780
sort of, you know, for the equivalent of like a small amount of coding.

1379
01:05:39,780 --> 01:05:42,930
That's one idea, and that's sort of a direct lift up from sort of,

1380
01:05:42,930 --> 01:05:44,385
you know, double Q learning.

1381
01:05:44,385 --> 01:05:46,920
The second thing is prioritized replay.

1382
01:05:46,920 --> 01:05:50,235
So, let's go back to the Mars Rover example.

1383
01:05:50,235 --> 01:05:54,330
Um, er, so, in Mars Rover we had this really small domain,

1384
01:05:54,330 --> 01:05:57,975
we are talking about tabular setting through just seven states, um,

1385
01:05:57,975 --> 01:06:00,540
and we're talking about a policy that just

1386
01:06:00,540 --> 01:06:03,450
always took action a1 which turned out to mostly go left.

1387
01:06:03,450 --> 01:06:04,965
So, we had this trajectory,

1388
01:06:04,965 --> 01:06:07,455
we started off in state s3,

1389
01:06:07,455 --> 01:06:09,300
we took action a1,

1390
01:06:09,300 --> 01:06:10,920
we got rewarded zero,

1391
01:06:10,920 --> 01:06:15,810
we went to s2, we stayed in s2 for one round when we did a1,

1392
01:06:15,810 --> 01:06:17,715
and then eventually went to s1,

1393
01:06:17,715 --> 01:06:19,170
and then we terminated.

1394
01:06:19,170 --> 01:06:24,749
So, it was this. And the first visit Monte Carlo estimate

1395
01:06:24,749 --> 01:06:28,185
of v for every state was 1110000,

1396
01:06:28,185 --> 01:06:33,480
and the TD estimate with alpha equal one was this.

1397
01:06:33,480 --> 01:06:36,750
That was when we talked about the fact that TD only uses

1398
01:06:36,750 --> 01:06:39,795
each data point once and it didn't propagate the information back.

1399
01:06:39,795 --> 01:06:44,445
So, the only update for TD learning was when we reached state s1,

1400
01:06:44,445 --> 01:06:47,370
we took action one, we got a reward of one, and then we terminated.

1401
01:06:47,370 --> 01:06:50,205
So, we only updated the value of state one.

1402
01:06:50,205 --> 01:06:53,520
So, now let's imagine that you get to do-

1403
01:06:53,520 --> 01:06:56,760
now let's think about what your- like your replay back up would be in this case.

1404
01:06:56,760 --> 01:06:59,190
You'd have something like this, you'd have s3, a1,

1405
01:06:59,190 --> 01:07:02,100
0, s2, s2, a1,

1406
01:07:02,100 --> 01:07:05,280
0 s2, s2, a1, 0,

1407
01:07:05,280 --> 01:07:09,525
s1, s1, a1, 1, terminate.

1408
01:07:09,525 --> 01:07:12,700
That's what your replay back up would look like.

1409
01:07:12,830 --> 01:07:16,890
So, let's say you get to choose two replay backups to do.

1410
01:07:16,890 --> 01:07:19,275
So, you have four possible replay backups,

1411
01:07:19,275 --> 01:07:21,750
you can pick the same one twice, if you want to,

1412
01:07:21,750 --> 01:07:24,390
and I'm going to ask you to pick to replay backups to

1413
01:07:24,390 --> 01:07:28,045
do to improve the value function, in some way.

1414
01:07:28,045 --> 01:07:31,290
Um, and I'd like you to think for a

1415
01:07:31,290 --> 01:07:34,185
second or talk to your neighbor about which of the two you should pick,

1416
01:07:34,185 --> 01:07:38,520
and why, and which order you do them in as well,

1417
01:07:38,520 --> 01:07:40,125
and whether it makes any difference.

1418
01:07:40,125 --> 01:07:42,120
Maybe it doesn't matter if you can just pick any of these,

1419
01:07:42,120 --> 01:07:44,340
you're going to get the same value function no matter what you do.

1420
01:07:44,340 --> 01:07:47,820
So, are there two- two updates that are particularly good,

1421
01:07:47,820 --> 01:07:49,860
and if so, why and what order would you do them in?

1422
01:07:49,860 --> 01:08:58,170
[NOISE]

1423
01:08:58,170 --> 01:09:00,240
Hopefully you had a chance to think about that for a second.

1424
01:09:00,240 --> 01:09:02,835
First of all, does it matter?

1425
01:09:02,835 --> 01:09:06,060
So, I'm going to first ask you guys, uh, the question.

1426
01:09:06,060 --> 01:09:08,415
Vote if you think it matters which ones you pick,

1427
01:09:08,415 --> 01:09:11,819
in terms of the value function you get out. That's right.

1428
01:09:11,819 --> 01:09:15,119
So, it absolutely matters which two you pick in terms of the resulting value function,

1429
01:09:15,120 --> 01:09:17,790
you will not get the same value function no matter which two you pick.

1430
01:09:17,790 --> 01:09:21,000
Um, uh, now as for another voting.

1431
01:09:21,000 --> 01:09:22,470
So, I will ask for which one we should do first?

1432
01:09:22,470 --> 01:09:24,450
Should we update- should we do four first?

1433
01:09:24,450 --> 01:09:26,880
Four is the last one on our replay buffer.

1434
01:09:26,880 --> 01:09:28,545
Should we do three first?

1435
01:09:28,545 --> 01:09:33,135
Should we do two first? Okay. All right.

1436
01:09:33,135 --> 01:09:38,205
So, 3s have it, does somebody want to explain why? Yeah.

1437
01:09:38,205 --> 01:09:39,395
I think.

1438
01:09:39,395 --> 01:09:41,680
You've got to back-propagate from

1439
01:09:41,680 --> 01:09:44,560
the information you're already [NOISE] have on step one to step two.

1440
01:09:44,560 --> 01:09:45,850


1441
01:09:45,850 --> 01:09:46,225
Right.

1442
01:09:46,225 --> 01:09:47,755
Yeah. So what the student said is right.

1443
01:09:47,755 --> 01:09:49,194
So, if you pick, um,

1444
01:09:49,194 --> 01:09:51,039
backup three, so what's backup three?

1445
01:09:51,040 --> 01:09:54,580
It is, S2, A1, 0, S1.

1446
01:09:54,580 --> 01:09:56,830
So if you do the backup, that's, zero,

1447
01:09:56,830 --> 01:09:59,035
plus gamma V of S prime,

1448
01:09:59,035 --> 01:10:01,015
S1. And this is one.

1449
01:10:01,015 --> 01:10:03,595
So that means now you're gonna get to backup

1450
01:10:03,595 --> 01:10:06,355
and so now your V of S2 is gonna be equal to one.

1451
01:10:06,355 --> 01:10:09,200
So you get to backup that information.

1452
01:10:10,050 --> 01:10:14,410
Yeah. So I, I wasn't extremely specific on what like,

1453
01:10:14,410 --> 01:10:16,195
the right thing to do here is, that,

1454
01:10:16,195 --> 01:10:18,820
that the- um, that my main thing is that

1455
01:10:18,820 --> 01:10:21,460
I wanted to emphasize that it makes the big difference and that,

1456
01:10:21,460 --> 01:10:22,780
and that, um, it's gonna matter in terms of order.

1457
01:10:22,780 --> 01:10:23,890
What's the next one we should do?

1458
01:10:23,890 --> 01:10:27,370
Should we do, raise your hand if we should do, three again.

1459
01:10:27,370 --> 01:10:30,010
Raise your hand if we should do two.

1460
01:10:30,010 --> 01:10:32,530
Raise your hand if we should do one.

1461
01:10:32,530 --> 01:10:38,770
Yeah. The ones have it. I- someone want to explain why? Yeah, in the back.

1462
01:10:38,770 --> 01:10:45,820
And that's the same as the last time I [inaudible]

1463
01:10:45,820 --> 01:10:47,320
That's right. Yes. So, um,

1464
01:10:47,320 --> 01:10:49,840
if you wanted to get all the way to the Monte Carlo estimate.

1465
01:10:49,840 --> 01:10:51,010
What you would wanna do here,

1466
01:10:51,010 --> 01:10:53,470
is you'd wanna do S3, a1,

1467
01:10:53,470 --> 01:10:57,745
0, S2 which would allow your V of S3 to be updated to one.

1468
01:10:57,745 --> 01:11:02,185
And at this point your value function will be exactly the same as the Monte Carlo.

1469
01:11:02,185 --> 01:11:04,285
So it definitely matters.

1470
01:11:04,285 --> 01:11:06,040
It matters the order in which you did, do it.

1471
01:11:06,040 --> 01:11:07,840
If you had done S3, a1, 0,

1472
01:11:07,840 --> 01:11:10,180
S2, your S3 wouldn't have changed.

1473
01:11:10,180 --> 01:11:12,280
Um, so ordering can make a big difference.

1474
01:11:12,280 --> 01:11:14,800
Uh, so not only do we wanna think about like, what, um,

1475
01:11:14,800 --> 01:11:17,035
was being brought up before but I think

1476
01:11:17,035 --> 01:11:19,720
to say like what should we be putting in our replay buffer,

1477
01:11:19,720 --> 01:11:21,490
not only do we wanna think about what- should be in

1478
01:11:21,490 --> 01:11:22,930
a replay buffer but also what order do

1479
01:11:22,930 --> 01:11:26,095
we sampled them can make a big difference in terms of convergence rates.

1480
01:11:26,095 --> 01:11:28,705
Um, uh, and in particular,

1481
01:11:28,705 --> 01:11:30,130
there's some really cool work from a couple of years

1482
01:11:30,130 --> 01:11:31,930
ago looking at this formally of like how,

1483
01:11:31,930 --> 01:11:33,280
at what the ordering, matters.

1484
01:11:33,280 --> 01:11:36,190
Um, so, there is this paper back in

1485
01:11:36,190 --> 01:11:39,610
2016 that tried to look at what the optimal order would be.

1486
01:11:39,610 --> 01:11:43,210
So imagine that you had an oracle that could, um, exactly compute.

1487
01:11:43,210 --> 01:11:45,430
Now, this is gonna be computationally intractable,

1488
01:11:45,430 --> 01:11:46,735
we're not gonna be able to do this in general,

1489
01:11:46,735 --> 01:11:48,910
but imagine that the oracle could go through and pick

1490
01:11:48,910 --> 01:11:51,295
and figure out exactly what the right order is.

1491
01:11:51,295 --> 01:11:53,530
Um, then what they found out in this case is that,

1492
01:11:53,530 --> 01:11:55,705
for this or a small chain like example,

1493
01:11:55,705 --> 01:11:58,300
um, you'd get this exponential improvement in convergence,

1494
01:11:58,300 --> 01:12:00,430
which is pretty awesome. So what does that mean?

1495
01:12:00,430 --> 01:12:01,990
The number, of, um,

1496
01:12:01,990 --> 01:12:05,470
updates you need to do until your value function converges to the right thing.

1497
01:12:05,470 --> 01:12:07,120
It can be exponentially smaller,

1498
01:12:07,120 --> 01:12:09,310
if you update carefully and you,

1499
01:12:09,310 --> 01:12:12,040
you could have an oracle tells you exactly what tuple the sample.

1500
01:12:12,040 --> 01:12:15,065
Which is super cool. Um, so you can be much much better.

1501
01:12:15,065 --> 01:12:16,800
But you can't do that. You're not gonna spend all this.

1502
01:12:16,800 --> 01:12:18,720
It- it's very computationally,

1503
01:12:18,720 --> 01:12:21,975
expensive or impossible in some cases to figure out exactly what that uh,

1504
01:12:21,975 --> 01:12:24,090
that oracle ordering should be.

1505
01:12:24,090 --> 01:12:26,320
Um, but it does illustrate that we,

1506
01:12:26,320 --> 01:12:30,475
we might wanna be careful about the order that we do it and- so, their,

1507
01:12:30,475 --> 01:12:32,695
intuition, for this, was,

1508
01:12:32,695 --> 01:12:38,860
let's try to prioritize a tuple for replay according to its DQN error.

1509
01:12:38,860 --> 01:12:41,635
So, the DQN error,

1510
01:12:41,635 --> 01:12:43,930
um, in this case is just our TD Learning error.

1511
01:12:43,930 --> 01:12:46,780
So it's gonna be the difference between, our current.

1512
01:12:46,780 --> 01:12:48,565
This is basically our prediction error.

1513
01:12:48,565 --> 01:12:52,090
So this is our prediction error [NOISE],

1514
01:12:52,090 --> 01:12:53,170
Almost our prediction error,

1515
01:12:53,170 --> 01:12:54,715
I'll just call it TD,

1516
01:12:54,715 --> 01:12:56,695
because it's not quite because we were doing the max.

1517
01:12:56,695 --> 01:13:00,050
So this is like, sort of our predicted,

1518
01:13:00,750 --> 01:13:05,590
TD error minus our current.

1519
01:13:05,590 --> 01:13:09,340
Let us say, if you have a really really big error,

1520
01:13:09,340 --> 01:13:11,650
that we're gonna prioritize, updating that more.

1521
01:13:11,650 --> 01:13:14,320
And you update this quiet quantity at every update,

1522
01:13:14,320 --> 01:13:18,505
you set it for new tuples to be zero and one method- they have two different methods for,

1523
01:13:18,505 --> 01:13:20,725
for trying to do this sort of prioritization.

1524
01:13:20,725 --> 01:13:25,570
That one method basically takes these, um, priorities,

1525
01:13:25,570 --> 01:13:28,060
raises them to some power alpha, um,

1526
01:13:28,060 --> 01:13:31,915
and then normalizes And then that's the probability,

1527
01:13:31,915 --> 01:13:33,490
of selecting that tuple.

1528
01:13:33,490 --> 01:13:36,040
So you prioritize more things that are weights. Yeah.

1529
01:13:36,040 --> 01:13:37,300
Doesn't freezing.

1530
01:13:37,300 --> 01:13:38,110
Name first, please.

1531
01:13:38,110 --> 01:13:41,065
Oh, Sorry. Doesn't freezing in the old ways

1532
01:13:41,065 --> 01:13:44,170
were a counter to propagating back the information there?

1533
01:13:44,170 --> 01:13:46,720
It's like, you first the old ways and uh,

1534
01:13:46,720 --> 01:13:48,340
example we're just going through,

1535
01:13:48,340 --> 01:13:50,980
after you like propagated the one back once,

1536
01:13:50,980 --> 01:13:54,745
you wouldn't be able to do anymore because your value's totally zero

1537
01:13:54,745 --> 01:13:56,320
It's a great point, which is,

1538
01:13:56,320 --> 01:13:57,580
if you are fixing,

1539
01:13:57,580 --> 01:14:00,385
um, uh, your w minus, then,

1540
01:14:00,385 --> 01:14:02,980
if you were looking at our case that we had before,

1541
01:14:02,980 --> 01:14:05,275
then you wouldn't be able to continue propagating that back,

1542
01:14:05,275 --> 01:14:07,420
because you wouldn't update yet, yet, that's exactly right.

1543
01:14:07,420 --> 01:14:09,175
So there's gonna be this tension between,

1544
01:14:09,175 --> 01:14:12,280
when you fix things versus her propagating information back.

1545
01:14:12,280 --> 01:14:17,230
Um, I, and, it's a tention that one has to sort of figure out,

1546
01:14:17,230 --> 01:14:18,910
there's not necessarily principled ways for,

1547
01:14:18,910 --> 01:14:20,470
exactly what the right schedule is to do that,

1548
01:14:20,470 --> 01:14:22,465
but it's a hyperparameter to do.

1549
01:14:22,465 --> 01:14:24,760
So why does it, what does ordering matter,

1550
01:14:24,760 --> 01:14:25,840
that if you're fixing,

1551
01:14:25,840 --> 01:14:26,890
and so you are not changing,

1552
01:14:26,890 --> 01:14:31,690
uh, like, then it wouldn't matter what order we sampled those previous ones, right?

1553
01:14:31,690 --> 01:14:34,630
Uh, okay. So basically, ordering matter at all, in that case.

1554
01:14:34,630 --> 01:14:38,140
It still matters because we're still gonna be doing replay, o- over,

1555
01:14:38,140 --> 01:14:39,700
uh, the weights will be changing during

1556
01:14:39,700 --> 01:14:42,040
the time period of which will be replayed over that buffer.

1557
01:14:42,040 --> 01:14:43,330
So that buffer could be like,

1558
01:14:43,330 --> 01:14:46,990
million and you might re-update your weights like every 50 steps or something like that.

1559
01:14:46,990 --> 01:14:49,465
So there's still gonna be a whole bunch of data points in,

1560
01:14:49,465 --> 01:14:50,950
uh, in your replay buffer,

1561
01:14:50,950 --> 01:14:52,240
that it's useful to think about,

1562
01:14:52,240 --> 01:14:53,755
now that your weights have changed,

1563
01:14:53,755 --> 01:14:57,380
what ordering do you wanna go through those? It's a great question.

1564
01:14:57,930 --> 01:15:00,070
Okay. So what method is this?

1565
01:15:00,070 --> 01:15:03,625
Lemme just, just to clarify, if we set, um,

1566
01:15:03,625 --> 01:15:04,840
alpha equal to zero,

1567
01:15:04,840 --> 01:15:08,330
what's the rule for selecting among the existing tuples?

1568
01:15:09,300 --> 01:15:11,935
So out- so Pi is our, uh,

1569
01:15:11,935 --> 01:15:14,185
sort of basically our DQN error.

1570
01:15:14,185 --> 01:15:19,180
If we set Alpha equal to zero, you know, it's right.

1571
01:15:19,180 --> 01:15:23,350
Yeah. So, so this sort of trades off between uniform,

1572
01:15:23,350 --> 01:15:27,310
no prioritization to completely picking the one that,

1573
01:15:27,310 --> 01:15:29,050
um, like if alpha's infinity then that's

1574
01:15:29,050 --> 01:15:31,060
gonna be picking the one with the highest DQN error.

1575
01:15:31,060 --> 01:15:34,105
So it's a trade-off, it's a stochastic. All right.

1576
01:15:34,105 --> 01:15:36,895
So, um, then, they combine this with,

1577
01:15:36,895 --> 01:15:38,320
sort of- the reason why I'm picking these three

1578
01:15:38,320 --> 01:15:40,405
[inaudible] they are sort of layer on top of each other.

1579
01:15:40,405 --> 01:15:42,640
So prioritise replay versus, um,

1580
01:15:42,640 --> 01:15:44,620
I think this is prioritise replay plus D, um,

1581
01:15:44,620 --> 01:15:47,020
double DQN versus just double DQN.

1582
01:15:47,020 --> 01:15:49,420
Most of the time, um, this is zero would be,

1583
01:15:49,420 --> 01:15:51,820
they're both the same underneath means flat,

1584
01:15:51,820 --> 01:15:54,415
uh, vanilla DQ, double DQN is better.

1585
01:15:54,415 --> 01:15:56,605
Above means that prioritize replay is better.

1586
01:15:56,605 --> 01:15:59,215
Most, the time prioritize replay is better

1587
01:15:59,215 --> 01:16:01,240
and there's some hyper parameters here to

1588
01:16:01,240 --> 01:16:02,920
play with but most of the time it's, it's useful.

1589
01:16:02,920 --> 01:16:04,390
And it's certainly useful to think about,

1590
01:16:04,390 --> 01:16:06,460
you know, we're order might matter.

1591
01:16:06,460 --> 01:16:09,250
All right. We don't have very much time left so I'm just gonna do,

1592
01:16:09,250 --> 01:16:10,960
short through this just so you're aware of it.

1593
01:16:10,960 --> 01:16:14,930
Um, one of the best papers from ICML 2016 was dueling.

1594
01:16:15,420 --> 01:16:18,175
Um, the idea is that,

1595
01:16:18,175 --> 01:16:21,580
if you want to, make decisions in the world,

1596
01:16:21,580 --> 01:16:23,695
they're working some states that are better or worse,

1597
01:16:23,695 --> 01:16:25,660
um, and they're just gonna have higher value or lower value,

1598
01:16:25,660 --> 01:16:27,340
but that- what you really wanna be able to do is,

1599
01:16:27,340 --> 01:16:30,265
figure out what the right action is to do, in a particular state.

1600
01:16:30,265 --> 01:16:32,935
Um, and so that- what you want us to have understand is,

1601
01:16:32,935 --> 01:16:34,855
this, this advantage function.

1602
01:16:34,855 --> 01:16:37,060
You wanna know, how much better or worse taking

1603
01:16:37,060 --> 01:16:40,780
a particular action is versus following the current policy.

1604
01:16:40,780 --> 01:16:44,350
That really like I don't care about estimating the value of a state,

1605
01:16:44,350 --> 01:16:48,265
i care about being able to understand which of the actions has the better value.

1606
01:16:48,265 --> 01:16:51,380
So I'm looking at this advantage function.

1607
01:16:51,390 --> 01:16:55,390
So, what they do to do this is that in contrast to DQN,

1608
01:16:55,390 --> 01:16:57,820
where you output all of the Q's.They're gonna

1609
01:16:57,820 --> 01:17:00,580
separate and they're gonna first estimate the value of a state,

1610
01:17:00,580 --> 01:17:02,740
and they're gonna estimate this advantage function,

1611
01:17:02,740 --> 01:17:07,615
which is Q of s. One minus V of s,

1612
01:17:07,615 --> 01:17:12,730
Q of s, a2 minus V of s. [inaudible] just gonna separate it.

1613
01:17:12,730 --> 01:17:16,780
It's an architectural choice and learning a recombine these for the Q.

1614
01:17:16,780 --> 01:17:20,950
And I get this is gonna help us refocus on the signal that we care about which is,

1615
01:17:20,950 --> 01:17:25,100
um, you know,after accurately estimate which action is better or worse.

1616
01:17:26,100 --> 01:17:29,725
Um, there's intruding questions about whether or not this is identifiable,

1617
01:17:29,725 --> 01:17:31,480
I don't have enough time to go into these today.

1618
01:17:31,480 --> 01:17:32,770
It is not identifiable.

1619
01:17:32,770 --> 01:17:34,975
I'm happy to talk about all of that off light, um,

1620
01:17:34,975 --> 01:17:38,560
uh, the, the reason this is, uh, important is,

1621
01:17:38,560 --> 01:17:42,610
they just forces one to make some sort of default assumptions about,

1622
01:17:42,610 --> 01:17:45,770
um, specifying the appendage functions.

1623
01:17:45,960 --> 01:17:48,895
Empirically, it's often super helpful.

1624
01:17:48,895 --> 01:17:52,570
So, again compared to double DQN with prioritize replay,

1625
01:17:52,570 --> 01:17:55,810
which we just saw, which was already better than w- double DQN,

1626
01:17:55,810 --> 01:17:57,355
which is also better than DQN.

1627
01:17:57,355 --> 01:18:00,670
Um, this again gives you another performance gain, substantial one.

1628
01:18:00,670 --> 01:18:02,935
So basically these are sort of threes,

1629
01:18:02,935 --> 01:18:07,240
three different ones that came up within the- for two years after DQN that started

1630
01:18:07,240 --> 01:18:12,115
making some really big big performance gains compared to destroy completely vanilla DQN.

1631
01:18:12,115 --> 01:18:15,820
For homework two, you're gonna be implementing DQ and not the other ones,

1632
01:18:15,820 --> 01:18:17,620
they welcome to implement some of the other ones.

1633
01:18:17,620 --> 01:18:19,630
They just good to be aware of- those are some and sort of

1634
01:18:19,630 --> 01:18:23,950
the major initial improvements to giving it substantially better performance on ATARI.

1635
01:18:23,950 --> 01:18:26,815
Um, I'll leave this up. We're almost out of time.

1636
01:18:26,815 --> 01:18:28,990
Uh, feel free to look at the last couple slides of this

1637
01:18:28,990 --> 01:18:31,240
for some practical [NOISE] tips that came from John Schulman,

1638
01:18:31,240 --> 01:18:33,340
um John Schulman was a PhD student at Berkeley,

1639
01:18:33,340 --> 01:18:35,510
that is now of the heads of open AI.

1640
01:18:35,510 --> 01:18:39,135
Um, I- just one thing that I will make sure to highlight,

1641
01:18:39,135 --> 01:18:41,550
it could be super tempting to try to start,

1642
01:18:41,550 --> 01:18:44,790
by like implementing Q learning directly on the ATARI.

1643
01:18:44,790 --> 01:18:46,890
Highly encourage you to first go through,

1644
01:18:46,890 --> 01:18:48,615
sort of the order of the assignment and like,

1645
01:18:48,615 --> 01:18:49,680
do the linear case,

1646
01:18:49,680 --> 01:18:51,615
make sure your Q learning is totally working,

1647
01:18:51,615 --> 01:18:53,505
um, before you deploy on ATARI.

1648
01:18:53,505 --> 01:18:55,380
Even with the smaller games,

1649
01:18:55,380 --> 01:18:56,865
like Pong which we're working on,

1650
01:18:56,865 --> 01:18:59,410
um, it is enormously time consuming.

1651
01:18:59,410 --> 01:19:01,810
Um,and so in terms of just understanding and deep again,

1652
01:19:01,810 --> 01:19:04,930
it's way better to make sure that you know your Q Learning method is working,

1653
01:19:04,930 --> 01:19:07,630
before you wait, 12 hours to see whether or not,

1654
01:19:07,630 --> 01:19:09,490
oh it didn't learn anything on Pogge.

1655
01:19:09,490 --> 01:19:11,440
So, that, that- there's a reason for why we,

1656
01:19:11,440 --> 01:19:13,435
sort of build up, the way we do in the assignment.

1657
01:19:13,435 --> 01:19:17,575
Um, another practical, to a few other practical tips, feel free to,

1658
01:19:17,575 --> 01:19:19,135
to look at those, um,

1659
01:19:19,135 --> 01:19:22,120
and then we were on Thursday.

1660
01:19:22,120 --> 01:19:23,600
Thanks.

