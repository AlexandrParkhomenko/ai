1
00:00:04,070 --> 00:00:06,825
Hi everybody, um, I'm Emma Brunskill.

2
00:00:06,825 --> 00:00:12,030
Um, I'm an assistant professor in Computer Science and welcome to CS234, um,

3
00:00:12,030 --> 00:00:15,450
which is a reinforcement learning class, um,

4
00:00:15,450 --> 00:00:18,270
which was designed to be sort of an entry-level masters or

5
00:00:18,270 --> 00:00:21,570
PhD student in an introduction to reinforcement learning.

6
00:00:21,570 --> 00:00:24,420
So, what we're gonna do today is I'm gonna start with

7
00:00:24,420 --> 00:00:27,690
just a really brief overview of what is reinforcement learning.

8
00:00:27,690 --> 00:00:28,980
Um, and then we're gonna go through

9
00:00:28,980 --> 00:00:31,560
course logistics and when I go through course logistics,

10
00:00:31,560 --> 00:00:34,755
I'll also pause and ask for any questions about logistics.

11
00:00:34,755 --> 00:00:37,220
Um, the website is now live and so that's

12
00:00:37,220 --> 00:00:40,135
also the best source of information about the class.

13
00:00:40,135 --> 00:00:42,755
That and Piazza will be the best source of information.

14
00:00:42,755 --> 00:00:44,930
Um, uh, so I'll stop there when we get to

15
00:00:44,930 --> 00:00:47,240
that part to ask if there's anything that I don't go over that

16
00:00:47,240 --> 00:00:49,730
you have questions about and if you have questions about

17
00:00:49,730 --> 00:00:53,525
the wait-list or any particular things relating to your own circumstance,

18
00:00:53,525 --> 00:00:55,625
feel free to come up to me at the end.

19
00:00:55,625 --> 00:00:58,640
Um, and then the third part of the class is gonna be where we start

20
00:00:58,640 --> 00:01:01,400
to get into the technical content that we're thinking about,

21
00:01:01,400 --> 00:01:04,795
uh, an introduction to sequential decision making under uncertainty.

22
00:01:04,795 --> 00:01:08,150
Um, just so I have a sense before we get started,

23
00:01:08,150 --> 00:01:10,955
who here has taken a machine learning class?

24
00:01:10,955 --> 00:01:13,745
All right. Who here has taken AI?

25
00:01:13,745 --> 00:01:16,380
Okay. So, a little bit less but most people. All right.

26
00:01:16,380 --> 00:01:20,750
Great. So, probably everybody here has seen a little bit about reinforcement learning.

27
00:01:20,750 --> 00:01:23,435
Um, varies a little bit depending on where you've been at.

28
00:01:23,435 --> 00:01:25,760
We will be covering stuff starting from

29
00:01:25,760 --> 00:01:28,670
the beginning as if you don't know any reinforcement learning, um,

30
00:01:28,670 --> 00:01:31,490
but then we'll rapidly be getting to other content, um,

31
00:01:31,490 --> 00:01:35,910
that's beyond anything that's covered in at least other Stanford related classes.

32
00:01:36,920 --> 00:01:41,740
So, reinforcement learning is concerned with this really foundational issue of

33
00:01:41,740 --> 00:01:46,970
how can an intelligent agent learn to make a good sequence of decisions?

34
00:01:46,970 --> 00:01:51,100
Um, and that's sort of a single sentence that summarizes what reinforcement learning is.

35
00:01:51,100 --> 00:01:53,320
Do we know what we'll be covering during this class?

36
00:01:53,320 --> 00:01:57,095
But it actually encodes a lot of really important ideas.

37
00:01:57,095 --> 00:02:02,645
Um, so the first thing is that we're really concerned now with sequences of decisions.

38
00:02:02,645 --> 00:02:05,980
So, in contrast to a lot of what is covered in,

39
00:02:05,980 --> 00:02:08,590
uh, machine learning, we're gonna be thinking about agents,

40
00:02:08,590 --> 00:02:10,870
intelligent agents or an intelligent agent in

41
00:02:10,870 --> 00:02:14,170
general that might or might not be human or biological.

42
00:02:14,170 --> 00:02:19,115
Um, and how it can make not just one decision but a whole sequence of decisions.

43
00:02:19,115 --> 00:02:21,970
We're gonna be concerned with goodness.

44
00:02:21,970 --> 00:02:24,070
In other words, we're gonna be interested in- the,

45
00:02:24,070 --> 00:02:26,875
the second thing is how do we learn to make good decisions,

46
00:02:26,875 --> 00:02:30,670
um, and what we mean by good here is some notion of optimality.

47
00:02:30,670 --> 00:02:34,580
We have some utility measure over the decisions that are being made.

48
00:02:34,580 --> 00:02:40,020
Um, and the final critical aspect of reinforcement learning is the learning, but, um,

49
00:02:40,020 --> 00:02:43,685
that the agent doesn't know in advance how its decisions are gonna affect

50
00:02:43,685 --> 00:02:48,400
the world or what decisions might necessarily be associated with good outcomes,

51
00:02:48,400 --> 00:02:52,880
and instead it has to acquire that information through experience.

52
00:02:52,880 --> 00:02:55,440
So, when we think about this.

53
00:02:55,440 --> 00:02:58,090
This is really something that we do all the time.

54
00:02:58,090 --> 00:02:59,785
We've done it since we were babies.

55
00:02:59,785 --> 00:03:01,240
We try to figure out,

56
00:03:01,240 --> 00:03:02,390
how do you, um,

57
00:03:02,390 --> 00:03:05,300
sort of achieve high reward in the world and there's a lot of

58
00:03:05,300 --> 00:03:08,645
really exciting work that's going on in neuroscience and psychology,

59
00:03:08,645 --> 00:03:10,880
um, that's trying to think about this same fundamental issue

60
00:03:10,880 --> 00:03:13,550
from the perspective of human intelligent agents.

61
00:03:13,550 --> 00:03:17,705
And so I think that if we wanna be able to solve AI,

62
00:03:17,705 --> 00:03:19,120
um, or make significant progress,

63
00:03:19,120 --> 00:03:21,875
we have to be able to make significant progress

64
00:03:21,875 --> 00:03:25,950
in allowing us to create agents that do reinforcement learning.

65
00:03:26,000 --> 00:03:28,020
So, where does this come up?

66
00:03:28,020 --> 00:03:31,410
There's this, um, nice example from Yael Niv who's,

67
00:03:31,410 --> 00:03:36,200
uh, an amazing sort of psychologist and neuroscience researcher over at Princeton.

68
00:03:36,200 --> 00:03:40,070
Um, where she gives us an example of this sort of primitive creature,

69
00:03:40,070 --> 00:03:43,760
um, which evolves as following during its lifetime.

70
00:03:43,760 --> 00:03:45,260
So, when it's a baby,

71
00:03:45,260 --> 00:03:50,315
it has a primitive brain and one eye and it swims around and it attaches to a rock.

72
00:03:50,315 --> 00:03:54,260
And then when it's an adult, it digests its brain and it sits there.

73
00:03:54,260 --> 00:03:58,850
And so maybe this is some indication that the point of intelligence or

74
00:03:58,850 --> 00:04:03,595
the point of having a brain in at least in part is helping to guide decisions,

75
00:04:03,595 --> 00:04:06,560
and so that once all the decisions and the agent's life

76
00:04:06,560 --> 00:04:10,015
has been completed maybe we no longer need a brain.

77
00:04:10,015 --> 00:04:12,100
So, I think this is, you know,

78
00:04:12,100 --> 00:04:14,660
this is one example of a biological creature but I think it's

79
00:04:14,660 --> 00:04:17,360
a useful reminder to think about why would an agent

80
00:04:17,360 --> 00:04:19,820
need to be intelligent and is it somehow

81
00:04:19,820 --> 00:04:23,790
fundamentally related to the fact that it has to make decisions?

82
00:04:24,080 --> 00:04:26,150
Now of course, um,

83
00:04:26,150 --> 00:04:29,400
there's been a sort of really a paradigm shift in reinforcement learning.

84
00:04:29,400 --> 00:04:31,965
Um, around 2015, um,

85
00:04:31,965 --> 00:04:35,650
in the Neurex Conference which is one of the main machine learning conferences,

86
00:04:35,650 --> 00:04:39,760
David Silver came and went to a workshop and presented

87
00:04:39,760 --> 00:04:44,500
these incredible results of using reinforcement learning to directly control Atari games.

88
00:04:44,500 --> 00:04:47,710
Now, these are important whether you like video games or not.

89
00:04:47,710 --> 00:04:50,530
Um, video games are a really interesting example of

90
00:04:50,530 --> 00:04:54,385
sort of complex tasks that take human players a while often to learn.

91
00:04:54,385 --> 00:04:56,080
We don't know how to do them in advance.

92
00:04:56,080 --> 00:04:58,360
It takes us at least a little bit of experience.

93
00:04:58,360 --> 00:05:01,540
And what the really incredible thing about this example was, this is, uh,

94
00:05:01,540 --> 00:05:06,450
Breakout, is that the agent learns to play directly from pixel input.

95
00:05:06,450 --> 00:05:08,600
So, from the agent's perspective,

96
00:05:08,600 --> 00:05:10,580
they're just seeing sort of these colored pixels

97
00:05:10,580 --> 00:05:12,650
coming in and it's having to learn what's

98
00:05:12,650 --> 00:05:15,050
the right decisions to make in order to learn to play

99
00:05:15,050 --> 00:05:17,930
the game well and in fact even better than people.

100
00:05:17,930 --> 00:05:20,990
So, this was really incredible that this was possible.

101
00:05:20,990 --> 00:05:23,330
Um, when I first started doing reinforcement learning,

102
00:05:23,330 --> 00:05:27,465
a lot of the work was really focused on very artificial toy problems.

103
00:05:27,465 --> 00:05:29,840
Um, a lot of the foundations were there but these sort of

104
00:05:29,840 --> 00:05:32,660
larger scale applications we're really lacking.

105
00:05:32,660 --> 00:05:34,370
And so I think in the last five years,

106
00:05:34,370 --> 00:05:37,460
we've seen really a huge improvement, um,

107
00:05:37,460 --> 00:05:39,950
in the types of techniques that are going on in

108
00:05:39,950 --> 00:05:43,580
reinforcement learning and in the scale of the problems that can be tackled.

109
00:05:43,580 --> 00:05:47,250
Now, it's not just in video game, um, playing.

110
00:05:47,250 --> 00:05:49,240
Uh, it's also in things like robotics, um,

111
00:05:49,240 --> 00:05:51,935
and particularly some of my colleagues up at University of,

112
00:05:51,935 --> 00:05:54,935
um, California Berkeley, um, uh,

113
00:05:54,935 --> 00:05:57,770
had been doing some really incredible work on robotics

114
00:05:57,770 --> 00:06:01,080
and using reinforcement learning in these types of scenarios,

115
00:06:01,080 --> 00:06:03,650
um, to try to have the agents do grasping,

116
00:06:03,650 --> 00:06:06,510
fold clothes, things like that.

117
00:06:06,730 --> 00:06:10,555
Now, those are some of examples if you guys have, um,

118
00:06:10,555 --> 00:06:12,380
looked at reinforcement learning before,

119
00:06:12,380 --> 00:06:13,655
are probably the ones you've heard about.

120
00:06:13,655 --> 00:06:16,685
You probably heard about things like video games or robotics.

121
00:06:16,685 --> 00:06:20,420
Um, but one of the things that I think is really exciting is that, uh,

122
00:06:20,420 --> 00:06:23,390
reinforcement learning is actually applicable to a huge number of domains,

123
00:06:23,390 --> 00:06:26,665
um, which is both an opportunity and a responsibility.

124
00:06:26,665 --> 00:06:28,315
So, in particular, um,

125
00:06:28,315 --> 00:06:32,630
I direct the AI for human Impact Lab here at Stanford and one of the things that we're

126
00:06:32,630 --> 00:06:34,070
really interested in is how do we use

127
00:06:34,070 --> 00:06:37,175
artificial intelligence to help amplify human potential?

128
00:06:37,175 --> 00:06:40,960
So, one way you could imagine doing that is through something like educational games.

129
00:06:40,960 --> 00:06:43,480
Where the goal is to figure out, um,

130
00:06:43,480 --> 00:06:49,380
how to quickly and effectively teach people how to learn material such as fractions.

131
00:06:50,120 --> 00:06:53,555
Another really important application area is health care.

132
00:06:53,555 --> 00:06:57,155
Um, this is sort of a cutout, um,

133
00:06:57,155 --> 00:06:59,930
of looking at seizures that some work that's been done by

134
00:06:59,930 --> 00:07:03,335
Joel Pineau up at McGill University

135
00:07:03,335 --> 00:07:06,410
and I think there's also a lot of excitement right now thinking

136
00:07:06,410 --> 00:07:10,055
about how can we use AI in a particular reinforcement learning,

137
00:07:10,055 --> 00:07:13,550
um, to do things like to interact with things like

138
00:07:13,550 --> 00:07:18,330
electronic medical records systems and use them to inform patient treatment.

139
00:07:19,330 --> 00:07:22,640
There's also a lot of recent excitement and thinking about how we

140
00:07:22,640 --> 00:07:25,009
can use reinforcement learning and lots of other applications

141
00:07:25,009 --> 00:07:27,260
kind of as an optimization technique for

142
00:07:27,260 --> 00:07:29,900
when it's really hard to solve optimization problems.

143
00:07:29,900 --> 00:07:31,340
And so this is arising in things like

144
00:07:31,340 --> 00:07:35,700
natural language processing in vision and a number of other areas.

145
00:07:35,990 --> 00:07:40,940
So, I think if we have to think about what are the key aspects of reinforcement learning,

146
00:07:40,940 --> 00:07:43,100
they probably boil down to the following four,

147
00:07:43,100 --> 00:07:44,900
and these are things that are gonna distinguish it

148
00:07:44,900 --> 00:07:47,125
from other aspects of AI and machine learning.

149
00:07:47,125 --> 00:07:49,760
So, reinforcement learning from my sentence

150
00:07:49,760 --> 00:07:52,985
about that we're learning to make good decisions under uncertainty,

151
00:07:52,985 --> 00:07:55,460
fundamentally involves optimization, delayed

152
00:07:55,460 --> 00:07:59,190
consequences, exploration and generalization.

153
00:07:59,270 --> 00:08:04,325
So, optimization naturally comes up because we're interested in good decisions.

154
00:08:04,325 --> 00:08:07,790
There's some notion of relative different types of decisions that we can make,

155
00:08:07,790 --> 00:08:12,120
um, and we want to be able to get decisions that are good.

156
00:08:12,530 --> 00:08:15,980
The second situation is delayed consequences.

157
00:08:15,980 --> 00:08:19,090
So, this is the challenge that the decisions that are made now.

158
00:08:19,090 --> 00:08:22,820
You might not realize whether or not they're a good decision until much later.

159
00:08:22,820 --> 00:08:27,140
So, you eat the chocolate Sunday now and you don't realize until an hour later that

160
00:08:27,140 --> 00:08:31,730
that was a bad idea to eat all two courts of ice cream or, um,

161
00:08:31,730 --> 00:08:35,100
you in the case of things like video games like Montezuma's Revenge,

162
00:08:35,100 --> 00:08:38,750
you have to pick up a key and then much later you realize that's helpful

163
00:08:38,750 --> 00:08:41,000
or you study really hard now and

164
00:08:41,000 --> 00:08:44,135
Friday night and then three weeks you do well on the midterm.

165
00:08:44,135 --> 00:08:47,270
So, one of the challenges to doing this is that

166
00:08:47,270 --> 00:08:50,000
because you don't necessarily receive immediate outcome feedback,

167
00:08:50,000 --> 00:08:51,620
it can be hard to do what is known as

168
00:08:51,620 --> 00:08:54,860
the credit assignment problem which is how do you figure

169
00:08:54,860 --> 00:08:57,440
out the causal relationship between the decisions

170
00:08:57,440 --> 00:09:00,825
you made in the past and the outcomes in the future?

171
00:09:00,825 --> 00:09:05,580
And that's a really different problem than we tend to see in most of machine learning.

172
00:09:06,650 --> 00:09:09,680
So, one of the things that comes up when we start to

173
00:09:09,680 --> 00:09:12,250
think about this is how do we do exploration?

174
00:09:12,250 --> 00:09:14,930
So, the agent is fundamentally trying to figure

175
00:09:14,930 --> 00:09:17,540
out how the world works through experience in much of

176
00:09:17,540 --> 00:09:20,210
reinforcement learning and so we think about the agent as

177
00:09:20,210 --> 00:09:23,180
really kinda being the scientist of trying things out in the world

178
00:09:23,180 --> 00:09:26,870
like having an agent that tries to ride a bicycle and then learning about how

179
00:09:26,870 --> 00:09:31,265
physics and riding a balanced bike works by falling.

180
00:09:31,265 --> 00:09:34,475
And one of the really big challenges here is that data is

181
00:09:34,475 --> 00:09:37,170
censored and what we mean by

182
00:09:37,170 --> 00:09:41,275
censoring in this case is that you only get to learn about what you try to do.

183
00:09:41,275 --> 00:09:45,895
So, all of you guys are here at Stanford clearly that was the optimal choice.

184
00:09:45,895 --> 00:09:47,935
Um, but you don't actually get to

185
00:09:47,935 --> 00:09:49,750
figure out what it would have been like if you'd went to

186
00:09:49,750 --> 00:09:53,665
MIT it's possible that would've been a good choice as well,

187
00:09:53,665 --> 00:09:54,910
but you can't- you,

188
00:09:54,910 --> 00:09:57,850
can't experience that because you only get to live one life and so you

189
00:09:57,850 --> 00:10:01,960
only get to see that particular choice you made at this particular time.

190
00:10:03,410 --> 00:10:07,185
So, one question you might wonder about is, um,

191
00:10:07,185 --> 00:10:08,835
you know, policy, what we're gonna,

192
00:10:08,835 --> 00:10:10,200
we're gonna talk a lot about policies.

193
00:10:10,200 --> 00:10:15,210
Policies, decision policies is gonna be some mapping from experiences to a decision.

194
00:10:15,210 --> 00:10:16,620
And you might answer why,

195
00:10:16,620 --> 00:10:18,820
we, this needs to be learned.

196
00:10:19,160 --> 00:10:22,440
So, if we think about something like Deep Mind,

197
00:10:22,440 --> 00:10:24,360
um, Atari playing game.

198
00:10:24,360 --> 00:10:25,620
What it was learning from here,

199
00:10:25,620 --> 00:10:27,105
is it was learning from pixels.

200
00:10:27,105 --> 00:10:31,530
So, it was essentially learning from the space of images what to do next.

201
00:10:31,530 --> 00:10:34,080
And if you wanted to write that down as a program,

202
00:10:34,080 --> 00:10:35,400
a series of if then statements,

203
00:10:35,400 --> 00:10:36,660
it would be absolutely enormous.

204
00:10:36,660 --> 00:10:38,160
This is not tractable.

205
00:10:38,160 --> 00:10:41,520
So, this is why we need some form of generalization and

206
00:10:41,520 --> 00:10:44,655
why it may be much better for us to learn from data directly,

207
00:10:44,655 --> 00:10:47,820
as well as to have some high level representation of the task.

208
00:10:47,820 --> 00:10:50,220
So, that even if we then run into

209
00:10:50,220 --> 00:10:53,220
a particular configuration of pixels we've never seen before,

210
00:10:53,220 --> 00:10:55,720
our agent can still know what to do.

211
00:10:56,900 --> 00:11:01,470
So, these are sort of the four things that really make up reinforcement learning,

212
00:11:01,470 --> 00:11:03,630
at least online reinforcement learning and why are they

213
00:11:03,630 --> 00:11:06,880
different than some other types of AI and machine learning.

214
00:11:06,890 --> 00:11:11,880
So, another thing that comes up a lot in artificial intelligence is planning.

215
00:11:11,880 --> 00:11:14,685
So, for example, the Go game,

216
00:11:14,685 --> 00:11:17,220
um, is, can be part of as a planning problem.

217
00:11:17,220 --> 00:11:18,840
So, what does planning involve?

218
00:11:18,840 --> 00:11:23,295
Involves optimization, often generalization and delayed consequences.

219
00:11:23,295 --> 00:11:27,000
You might take a move and go early and it might not be immediately obvious if that was

220
00:11:27,000 --> 00:11:31,695
a good move until many steps later but it doesn't involve exploration.

221
00:11:31,695 --> 00:11:36,360
The idea and planning is that you're given a model of how the world works.

222
00:11:36,360 --> 00:11:37,950
So, your given the rules of the game,

223
00:11:37,950 --> 00:11:40,140
for example, and you know what the reward is.

224
00:11:40,140 --> 00:11:44,400
Um, and the hard part is computing what you should do given the model of the world.

225
00:11:44,400 --> 00:11:47,380
So, it doesn't require exploration.

226
00:11:47,750 --> 00:11:51,435
And supervised machine learning versus reinforcement learning.

227
00:11:51,435 --> 00:11:55,875
It often involves optimization and generalization but frequently it doesn't invo-,

228
00:11:55,875 --> 00:11:59,490
involve either exploration or delayed consequences.

229
00:11:59,490 --> 00:12:01,980
So, it doesn't tend to involve exploration because

230
00:12:01,980 --> 00:12:05,250
typically in supervised learning you're given a data set.

231
00:12:05,250 --> 00:12:10,050
So, your agent isn't collecting its experience or data about the world instead it's given

232
00:12:10,050 --> 00:12:15,345
experience and that it has to use that to say in for whether an image is a face or not.

233
00:12:15,345 --> 00:12:17,970
Similarly, um, it's typically making

234
00:12:17,970 --> 00:12:21,195
essentially one decision like whether this image is a face or not

235
00:12:21,195 --> 00:12:23,550
instead of having to think about making decisions

236
00:12:23,550 --> 00:12:27,670
now and then only learning whether or not those were the right decisions later.

237
00:12:29,750 --> 00:12:32,400
Unsupervised machine learning awful,

238
00:12:32,400 --> 00:12:36,450
also involves optimization and generalization but generally does not involve

239
00:12:36,450 --> 00:12:41,415
exploration or delayed consequences and typically you have no labels about the world.

240
00:12:41,415 --> 00:12:43,020
So, in supervised learning,

241
00:12:43,020 --> 00:12:47,265
you often get the exact label for the world like this image really is,

242
00:12:47,265 --> 00:12:49,350
has a, contains a face or not.

243
00:12:49,350 --> 00:12:53,160
Um, in unsupervised learning you normally get no labels about the world

244
00:12:53,160 --> 00:12:57,435
and an RL you typically get something kind of halfway in between those which you get a,

245
00:12:57,435 --> 00:13:00,060
a utility of the label you put.

246
00:13:00,060 --> 00:13:02,580
So, for example, you might decide that there's

247
00:13:02,580 --> 00:13:04,560
a face in here and it might say, ''Okay, yeah,

248
00:13:04,560 --> 00:13:06,375
we'll give you partial credit for that,''

249
00:13:06,375 --> 00:13:09,075
because maybe there's something that looks sort of like a face.

250
00:13:09,075 --> 00:13:13,980
But you don't get the true label of the world or maybe you decide to go to Stanford,

251
00:13:13,980 --> 00:13:15,465
um, and then you don't know.

252
00:13:15,465 --> 00:13:17,520
And you're like okay that was a really great experience but I

253
00:13:17,520 --> 00:13:20,919
don't know if it was, ''the right experience.''

254
00:13:22,760 --> 00:13:26,070
Imitation learning which is something that we'll probably touch on

255
00:13:26,070 --> 00:13:28,695
briefly in this class and is becoming very important,

256
00:13:28,695 --> 00:13:32,340
um, is similar, um, but a little bit different.

257
00:13:32,340 --> 00:13:35,070
So, in, uh, it involves optimization, generalization,

258
00:13:35,070 --> 00:13:37,080
and often delayed consequences but

259
00:13:37,080 --> 00:13:40,200
the idea is that we're going to be learning from experience of others.

260
00:13:40,200 --> 00:13:42,660
So, instead of our intelligent agent getting to ex-,

261
00:13:42,660 --> 00:13:46,230
take experiences, um, from the world and make its own decisions,

262
00:13:46,230 --> 00:13:49,560
it might watch another intelligent agent which might be a person,

263
00:13:49,560 --> 00:13:52,020
make decisions, observe outcomes and then

264
00:13:52,020 --> 00:13:55,170
use that experience to figure out how it wants to act.

265
00:13:55,170 --> 00:13:58,680
There'll be a lot of benefits to doing this but it's a little bit

266
00:13:58,680 --> 00:14:02,920
different because it doesn't have to directly think about the exploration problem.

267
00:14:03,560 --> 00:14:07,080
Imitation learning and I just want to spend a little bit more time on

268
00:14:07,080 --> 00:14:09,915
that one because it's become increasingly important.

269
00:14:09,915 --> 00:14:11,340
So, to my knowledge,

270
00:14:11,340 --> 00:14:14,970
it was first really sort of popularized by Andrew Ng,

271
00:14:14,970 --> 00:14:17,460
um, who's a former professor here, um,

272
00:14:17,460 --> 00:14:20,010
through some of his helicopter stuff

273
00:14:20,010 --> 00:14:22,935
where he was looking at expert flights together with Pieter Abbeel,

274
00:14:22,935 --> 00:14:24,960
whose a professor over at Berkeley, um,

275
00:14:24,960 --> 00:14:27,854
to see how you could imitate very quickly,

276
00:14:27,854 --> 00:14:30,960
um, experts flying toy helicopters.

277
00:14:30,960 --> 00:14:32,790
And that was one of sort of the first kind of

278
00:14:32,790 --> 00:14:35,985
major application successes of invitation learning.

279
00:14:35,985 --> 00:14:38,325
It can be very effective.

280
00:14:38,325 --> 00:14:42,150
There can be some challenges to it because essentially,

281
00:14:42,150 --> 00:14:44,370
if you get to observe one trajectory,

282
00:14:44,370 --> 00:14:47,940
let's imagine it's a circle of a helicopter flying and

283
00:14:47,940 --> 00:14:52,230
your agent learns something that isn't exactly the same as what the expert was doing,

284
00:14:52,230 --> 00:14:55,395
that you can essentially start to go off that path and ven-,

285
00:14:55,395 --> 00:14:59,010
venture into territory where you really don't know what the right thing is to do.

286
00:14:59,010 --> 00:15:03,420
So, there's been a lot of extensive work on imitation learning that's sort of combining

287
00:15:03,420 --> 00:15:08,110
between imitation learning and reinforcement learning that ends up being very promising.

288
00:15:08,840 --> 00:15:13,110
So, in terms of how we think about trying to do reinforcement learning,

289
00:15:13,110 --> 00:15:15,600
we can build on a lot of these different types of techniques.

290
00:15:15,600 --> 00:15:18,570
Um, and then also think about some of the challenges that are unique to

291
00:15:18,570 --> 00:15:22,230
reinforcement learning which involves all four of these challenges.

292
00:15:22,230 --> 00:15:25,140
And so these RL agents really need to explore

293
00:15:25,140 --> 00:15:29,200
the world and then use that exploration to guide their future decisions.

294
00:15:29,960 --> 00:15:33,375
So, we'll talk more about this throughout the course.

295
00:15:33,375 --> 00:15:37,635
Um, a really important question that comes up is where do these rewards come from,

296
00:15:37,635 --> 00:15:40,290
where is this information that the agents are using to

297
00:15:40,290 --> 00:15:43,035
try to guide whether or not their decisions are good,

298
00:15:43,035 --> 00:15:46,050
um, and who is providing those and what happens if they're wrong?

299
00:15:46,050 --> 00:15:47,745
And we'll talk a lot more about that.

300
00:15:47,745 --> 00:15:49,680
Um, we won't talk very much about

301
00:15:49,680 --> 00:15:53,385
multi agent reinforcement learning systems but that's also a really important case,

302
00:15:53,385 --> 00:15:58,860
as well as thinking about game theoretic aspects, right.

303
00:15:58,860 --> 00:16:01,860
So, that's just a really short overview about some of the aspects of

304
00:16:01,860 --> 00:16:03,630
reinforcement learning and why it's different

305
00:16:03,630 --> 00:16:05,610
than some of the other classes that you might have taken.

306
00:16:05,610 --> 00:16:09,420
Um, and now we're gonna go briefly through course logistics and then start sort of

307
00:16:09,420 --> 00:16:13,185
more of the content and I'll pause after course logistics to answer any questions.

308
00:16:13,185 --> 00:16:14,910
In terms of prerequisites, um,

309
00:16:14,910 --> 00:16:18,660
we expect that everybody here has either taken an AI class or

310
00:16:18,660 --> 00:16:20,340
a machine-learning class either here at

311
00:16:20,340 --> 00:16:23,460
Stanford or the equivalent to another institution.

312
00:16:23,460 --> 00:16:26,460
And if you're not sure whether or not you have the right background for the class,

313
00:16:26,460 --> 00:16:29,790
feel free to reach out to us on Piazza and we will respond.

314
00:16:29,790 --> 00:16:32,910
Um, if you've done extensive work in sort of related stuff,

315
00:16:32,910 --> 00:16:34,530
it will probably be sufficient.

316
00:16:34,530 --> 00:16:38,385
In general, we expect that you have basic Python proficiency,

317
00:16:38,385 --> 00:16:40,229
um, and that you're familiar with probability,

318
00:16:40,229 --> 00:16:42,300
statistics, and multi-variable calculus.

319
00:16:42,300 --> 00:16:45,855
Um, things like gradient descent,

320
00:16:45,855 --> 00:16:49,800
loss derivatives, um, those should all be very familiar to you.

321
00:16:49,800 --> 00:16:54,285
Um, and I expect that most people have probably heard of MDPs,

322
00:16:54,285 --> 00:16:57,910
um, before, but it's not totally critical.

323
00:16:59,360 --> 00:17:02,340
So, this is a long list [LAUGHTER] but

324
00:17:02,340 --> 00:17:04,589
I'll go through it slowly because I think it's pretty important.

325
00:17:04,589 --> 00:17:07,154
So, this is what are the goals for the class,

326
00:17:07,155 --> 00:17:08,550
what are the learning objectives?

327
00:17:08,550 --> 00:17:10,275
So, these are the things that we expect

328
00:17:10,275 --> 00:17:12,690
that you guys should be able to do by the time you

329
00:17:12,690 --> 00:17:14,670
finish this class and that it's our role to help

330
00:17:14,670 --> 00:17:17,760
you be able to understand how to do these things.

331
00:17:17,760 --> 00:17:22,079
So, the first thing is that it's important to be able to define the key features of

332
00:17:22,079 --> 00:17:26,399
reinforcement learning that distinguish it from other types of AI and machine learning,

333
00:17:26,400 --> 00:17:28,695
um, frames of problems.

334
00:17:28,695 --> 00:17:32,040
So, that's what I was doing a little bit of so far in this class to figure out,

335
00:17:32,040 --> 00:17:33,630
how does this distinguish this.

336
00:17:33,630 --> 00:17:37,365
How does RL distinguish itself from other types of pro-, problems.

337
00:17:37,365 --> 00:17:39,375
So, related to that,

338
00:17:39,375 --> 00:17:40,830
um, for most of you,

339
00:17:40,830 --> 00:17:42,915
you'll probably not end up being academics,

340
00:17:42,915 --> 00:17:45,225
um, and most of you will go into industry.

341
00:17:45,225 --> 00:17:48,060
And so, one of the big challenges when you do that is that when you're faced with

342
00:17:48,060 --> 00:17:52,770
a particular problem from your boss or when you're giving a problem to one of your, um,

343
00:17:52,770 --> 00:17:55,470
supervisees is for them to think about whether or

344
00:17:55,470 --> 00:17:58,500
not it should be framed as a reinforcement learning problem,

345
00:17:58,500 --> 00:18:01,230
um, and what things are applicable to it.

346
00:18:01,230 --> 00:18:04,110
So, I think it's very important that by the end of this class,

347
00:18:04,110 --> 00:18:06,810
that you have a sense of if you're given a real-world problem like

348
00:18:06,810 --> 00:18:11,265
web advertising or patient treatment or robotics problem, um,

349
00:18:11,265 --> 00:18:14,610
that you have a sense whether or not it is useful to formulate it as

350
00:18:14,610 --> 00:18:16,920
a reinforcement learning problem and how to write

351
00:18:16,920 --> 00:18:20,115
it down in that framework and what algorithms are relevant.

352
00:18:20,115 --> 00:18:23,250
Um, during the class, uh, we'll also be

353
00:18:23,250 --> 00:18:26,100
introducing you to a number of reinforcement learning algorithms,

354
00:18:26,100 --> 00:18:28,530
um, and you will have the chance to implement those in code

355
00:18:28,530 --> 00:18:32,040
including deep reinforcement learning cla-, uh, problems.

356
00:18:32,040 --> 00:18:35,220
Another really important aspect is if you're trying

357
00:18:35,220 --> 00:18:37,710
to decide what tools to use for a particular,

358
00:18:37,710 --> 00:18:40,350
say robotics problem or health care problem, um,

359
00:18:40,350 --> 00:18:44,100
is to understand which of the algorithms is likely to be beneficial one and why.

360
00:18:44,100 --> 00:18:47,160
And so, in addition to things like empirical performance,

361
00:18:47,160 --> 00:18:48,900
I think it's really important to understand,

362
00:18:48,900 --> 00:18:51,615
generally, how do we evaluate algorithms.

363
00:18:51,615 --> 00:18:57,645
Um, and can we use things like theoretical tools like regret sample complexity, um,

364
00:18:57,645 --> 00:19:00,000
as well as things like computational complexity to

365
00:19:00,000 --> 00:19:04,425
decide which algorithms are suitable for particular tasks.

366
00:19:04,425 --> 00:19:06,180
And then the final thing is

367
00:19:06,180 --> 00:19:08,280
that one really important aspect of

368
00:19:08,280 --> 00:19:11,400
reinforcement learning is exploration versus exploitation.

369
00:19:11,400 --> 00:19:15,600
This issue that arises when the agents have to figure out what decisions

370
00:19:15,600 --> 00:19:17,610
they wanna make and what they're gonna learn

371
00:19:17,610 --> 00:19:19,995
about the environment by making those decisions.

372
00:19:19,995 --> 00:19:21,540
And so, by the end of the class,

373
00:19:21,540 --> 00:19:24,840
you should also be able to compare different types of techniques for doing

374
00:19:24,840 --> 00:19:29,475
exploration versus exploitation and what are the strengths and limitations of these.

375
00:19:29,475 --> 00:19:34,390
Does anyone have any questions about what these learning objectives are.

376
00:19:39,260 --> 00:19:46,625
Okay. So, we'll have three main assignments for the class,

377
00:19:46,625 --> 00:19:48,245
um, will also have a midterm.

378
00:19:48,245 --> 00:19:51,335
Um, we'll have a quiz at the end of the class,

379
00:19:51,335 --> 00:19:53,965
um, as well as a final project.

380
00:19:53,965 --> 00:19:56,520
The quiz is a little bit unusual.

381
00:19:56,520 --> 00:19:59,745
Um, so, I just want to spend a little bit of time talking about it right now.

382
00:19:59,745 --> 00:20:04,455
The quiz is done on both individually and in groups.

383
00:20:04,455 --> 00:20:08,800
Um, the reason that we do this is because we want

384
00:20:08,800 --> 00:20:10,750
a low stakes way to sort of have

385
00:20:10,750 --> 00:20:14,620
people practice with the material that they learn in the second half of the course.

386
00:20:14,620 --> 00:20:17,860
Um, in a way that's sort of fun engaging and really tries to get

387
00:20:17,860 --> 00:20:21,130
you to think about it and also learn from your peers.

388
00:20:21,130 --> 00:20:25,190
Um, and so, we did it last year and I think a number of people who are

389
00:20:25,190 --> 00:20:29,365
a little bit nervous about how it would go before and then ended up really enjoying it.

390
00:20:29,365 --> 00:20:32,660
So, the way that the quiz works is it's a multiple choice quiz.

391
00:20:32,660 --> 00:20:34,370
At the beginning and everybody does it by

392
00:20:34,370 --> 00:20:37,730
themselves and then after everybody has submitted their answers,

393
00:20:37,730 --> 00:20:41,900
then we do it again in groups that are pre-assigned by us.

394
00:20:41,900 --> 00:20:44,990
And the goal is that you have to get everyone to decide on what

395
00:20:44,990 --> 00:20:48,980
the right answer is before you scratch off and see what the correct answer is.

396
00:20:48,980 --> 00:20:51,500
And then we grade it according to, um,

397
00:20:51,500 --> 00:20:53,090
whether you scratched off the right answer,

398
00:20:53,090 --> 00:20:54,695
correctly first or not.

399
00:20:54,695 --> 00:20:58,010
You can't do worse than your individual grade.

400
00:20:58,010 --> 00:21:01,205
So, doing it in a group can only help you.

401
00:21:01,205 --> 00:21:03,440
Um, and for SCPD students,

402
00:21:03,440 --> 00:21:04,670
they don't do it in groups.

403
00:21:04,670 --> 00:21:08,080
So, they just write down justifications for their answers.

404
00:21:08,080 --> 00:21:14,040
Again, um, it's a pretty lightweight way to do assessment, um,

405
00:21:14,040 --> 00:21:17,990
the goal is that you sort of have to be able to articulate why you believe that

406
00:21:17,990 --> 00:21:22,420
answers are the way they are and discuss them in small groups and they use that informa-,

407
00:21:22,420 --> 00:21:25,675
um, use that to figure out what the correct answer is.

408
00:21:25,675 --> 00:21:28,640
Um, the final project is paired pretty

409
00:21:28,640 --> 00:21:31,610
similar to other projects that you guys have done in other classes.

410
00:21:31,610 --> 00:21:32,990
Um, it's an open-ended project.

411
00:21:32,990 --> 00:21:35,315
It's a chance to, uh, reason about, um, and,

412
00:21:35,315 --> 00:21:38,120
and think about reinforcement learning, uh, stuff in more depth.

413
00:21:38,120 --> 00:21:41,360
We will also be offering a default project that will be announced

414
00:21:41,360 --> 00:21:44,885
over the next couple of weeks before the first milestone is due.

415
00:21:44,885 --> 00:21:48,020
If you choose to do the default project, your breakdown,

416
00:21:48,020 --> 00:21:51,050
because you will not need to do a proposal or milestone,

417
00:21:51,050 --> 00:21:55,410
will be based on the project presentation in your assignment, uh, write up.

418
00:21:57,900 --> 00:22:00,505
Since we believe that, um,

419
00:22:00,505 --> 00:22:02,590
you guys are all of each other's best resource,

420
00:22:02,590 --> 00:22:04,435
um, we use Piazza, um,

421
00:22:04,435 --> 00:22:07,240
that should be used for pretty much all class communication

422
00:22:07,240 --> 00:22:08,380
unless it's something that's sort of

423
00:22:08,380 --> 00:22:10,630
a private or sensitive manner in which case of

424
00:22:10,630 --> 00:22:13,825
course please feel free to reach out to the course staff directly,

425
00:22:13,825 --> 00:22:16,570
ah, and for things like lectures and

426
00:22:16,570 --> 00:22:20,600
homework and project questions pretty much all of that should go through Piazza.

427
00:22:21,090 --> 00:22:23,800
For late day policy,

428
00:22:23,800 --> 00:22:25,480
we have six late days, ah,

429
00:22:25,480 --> 00:22:28,780
for details you can see the webpage and for

430
00:22:28,780 --> 00:22:33,440
collaboration please see the webpage for some of the details about that.

431
00:22:33,720 --> 00:22:36,420
So before we go on to the next part,

432
00:22:36,420 --> 00:22:39,065
do I have any questions about logistics for the class?

433
00:22:39,065 --> 00:22:40,660
Okay, let's get started.

434
00:22:40,660 --> 00:22:42,550
Um, so, we're not going to do

435
00:22:42,550 --> 00:22:45,505
an introduction to sequential decision-making under uncertainty,

436
00:22:45,505 --> 00:22:48,235
a number of you guys who have seen some of this content before,

437
00:22:48,235 --> 00:22:50,455
um, we will be going into this in prime,

438
00:22:50,455 --> 00:22:53,290
more depth than you've seen for some of

439
00:22:53,290 --> 00:22:57,025
this stuff including some theory not theory today but in other lectures,

440
00:22:57,025 --> 00:22:59,140
and then we'll also be moving on to content that will

441
00:22:59,140 --> 00:23:02,425
be new to all of you later in the class.

442
00:23:02,425 --> 00:23:05,545
So, sequential decision-making under uncertainty.

443
00:23:05,545 --> 00:23:08,740
Um, the fundamental that we- thing that we think about in

444
00:23:08,740 --> 00:23:12,429
these settings is sort of an interactive closed-loop process,

445
00:23:12,429 --> 00:23:14,080
where we have some agent,

446
00:23:14,080 --> 00:23:17,440
an intelligent agent hopefully that is taking actions that are

447
00:23:17,440 --> 00:23:21,865
affecting the state of the world and then it's giving back an observation and a reward.

448
00:23:21,865 --> 00:23:28,915
The key goal is that the agent is trying to maximize the total expected future reward.

449
00:23:28,915 --> 00:23:36,370
Now, this expected aspect, um,

450
00:23:36,370 --> 00:23:39,070
is going to be important because sometimes the world itself will be

451
00:23:39,070 --> 00:23:44,110
stochastic and so the agent is going to be maximizing things in expectation,

452
00:23:44,110 --> 00:23:47,080
this may not always be the right criteria, um,

453
00:23:47,080 --> 00:23:50,275
this has been what has been focused on for the majority of reinforcement learning

454
00:23:50,275 --> 00:23:53,740
but there's now some interest in thinking about distribution honorable,

455
00:23:53,740 --> 00:23:55,960
RL and some other aspects.

456
00:23:55,960 --> 00:23:59,680
One of the key challenges here is that it can require balancing between

457
00:23:59,680 --> 00:24:02,980
immediate and long-term rewards and that it might

458
00:24:02,980 --> 00:24:07,045
require strategic behavior in order to achieve those high rewards,

459
00:24:07,045 --> 00:24:09,490
indicating that you might have to sacrifice

460
00:24:09,490 --> 00:24:15,085
initial higher rewards in order to achieve a better awards over the long-term.

461
00:24:15,085 --> 00:24:17,080
So as an example,

462
00:24:17,080 --> 00:24:19,870
something like web advertising might be that you have

463
00:24:19,870 --> 00:24:22,180
an agent that is running the website and it

464
00:24:22,180 --> 00:24:24,685
has to choose which web ad to give to a customer,

465
00:24:24,685 --> 00:24:28,975
the customer gives you back an observation such as how long they spent on the web page,

466
00:24:28,975 --> 00:24:33,115
and also you get some information about whether or not they click on an ad,

467
00:24:33,115 --> 00:24:36,775
and the goal is to say how people click on ads the most.

468
00:24:36,775 --> 00:24:41,575
So you have to pick which ad to show people so that they're going to click on ads.

469
00:24:41,575 --> 00:24:45,445
Another example is a robot that's unloading a dishwasher,

470
00:24:45,445 --> 00:24:50,830
so in this case the action space of the agent might be joint movements.

471
00:24:50,830 --> 00:24:54,550
The information that agent might get backwards are camera image of

472
00:24:54,550 --> 00:24:59,380
the kitchen and it might get a plus one reward if there are no dishes on the counter.

473
00:24:59,380 --> 00:25:01,870
So in this case it would generally be a delayed reward,

474
00:25:01,870 --> 00:25:04,600
for a long time there're going to be dishes on the counter, er,

475
00:25:04,600 --> 00:25:08,050
unless it can just sweep all of them off and have them crash onto the floor,

476
00:25:08,050 --> 00:25:12,505
which may or may not be the intended goal of the person who's writing the system.

477
00:25:12,505 --> 00:25:15,220
Um, and so, it may have to make a sequence of

478
00:25:15,220 --> 00:25:18,500
decisions where it can't get any reward for a long time.

479
00:25:18,690 --> 00:25:22,900
Another example is something like blood pressure control, um,

480
00:25:22,900 --> 00:25:26,830
where the actions might be things like prescribed exercise or

481
00:25:26,830 --> 00:25:29,020
prescribed medication and we get

482
00:25:29,020 --> 00:25:32,650
an observation back of what is the blood pressure of the individual.

483
00:25:32,650 --> 00:25:35,530
Um, then the reward might be plus one if it's in the- if

484
00:25:35,530 --> 00:25:38,800
the blood pressures in a healthy range maybe

485
00:25:38,800 --> 00:25:41,320
a small negative reward if medication is

486
00:25:41,320 --> 00:25:44,560
prescribed due to side effects and maybe zero reward otherwise.

487
00:25:44,560 --> 00:25:50,380
[NOISE] So, let's think about another case,

488
00:25:50,380 --> 00:25:53,830
like some of the cases that I think about in my lab like having an artificial tutor.

489
00:25:53,830 --> 00:25:57,470
So now what you could have is you could have a teaching agent,

490
00:25:58,620 --> 00:26:01,780
and what it gets to do is pick an activity,

491
00:26:01,780 --> 00:26:03,835
so pick a teaching activity.

492
00:26:03,835 --> 00:26:08,380
Let's say it only has two different types of teaching activities to give, um,

493
00:26:08,380 --> 00:26:11,050
it's going to either give an addition activity or

494
00:26:11,050 --> 00:26:14,630
a subtraction activity and it gives this to a student.

495
00:26:15,120 --> 00:26:22,250
Then the student either gets the problem right, right or wrong.

496
00:26:23,670 --> 00:26:28,465
And let's say the student initially does not no addition or subtraction.

497
00:26:28,465 --> 00:26:31,000
So, it's a kindergartner that student doesn't know anything about

498
00:26:31,000 --> 00:26:34,210
math and we're trying to figure out how to teach the student math,

499
00:26:34,210 --> 00:26:37,930
and that the reward structure for the teaching agent is they get a plus

500
00:26:37,930 --> 00:26:39,820
one every time a student gets something

501
00:26:39,820 --> 00:26:43,135
right and they get a minus one if the student gets it wrong.

502
00:26:43,135 --> 00:26:46,150
So, I'd like you to just take a minute turn to somebody

503
00:26:46,150 --> 00:26:49,540
nearby and describe what you think an agent that's trying to learn,

504
00:26:49,540 --> 00:26:53,440
to maximize its expected rewards would do in this type of case,

505
00:26:53,440 --> 00:26:54,850
what type of problems it would give to

506
00:26:54,850 --> 00:26:57,880
the student and whether or not that is doing the right thing.

507
00:26:57,880 --> 00:27:41,680
[NOISE].

508
00:27:41,680 --> 00:27:46,135
Let me just- let me just clarify here,

509
00:27:46,135 --> 00:27:49,930
and let me just clarify here [NOISE].

510
00:27:49,930 --> 00:27:52,450
Let me just clarify here is that let's assume that for

511
00:27:52,450 --> 00:27:56,980
most students addition is easier than subtraction, so that,

512
00:27:56,980 --> 00:27:58,180
like what it says here that

513
00:27:58,180 --> 00:28:01,480
the problem even though the student doesn't know either of these things

514
00:28:01,480 --> 00:28:03,235
that the skill of learning addition is

515
00:28:03,235 --> 00:28:06,340
simpler for a new student to learn than subtraction.

516
00:28:06,340 --> 00:28:09,655
So what would, what might happen under those cases?

517
00:28:09,655 --> 00:28:13,330
Is there maybe we want to, raise their hand and tell me what they and somebody

518
00:28:13,330 --> 00:28:17,110
nearby them was thinking might happen for an agent in this scenario?

519
00:28:17,110 --> 00:28:18,850
[NOISE].

520
00:28:18,850 --> 00:28:22,270
The agent would give them really easy addition problems, that's correct.

521
00:28:22,270 --> 00:28:24,085
That's exactly actually what happened.

522
00:28:24,085 --> 00:28:30,080
There's a nice paper from approximately 2,000 with Bev Wolf,

523
00:28:30,330 --> 00:28:33,070
which is one of the earliest ones but I know

524
00:28:33,070 --> 00:28:34,840
where they're using reinforcement learning to

525
00:28:34,840 --> 00:28:39,130
create an intelligent tutoring system and the reward was for the agent to,

526
00:28:39,130 --> 00:28:42,025
to give problems to the student in order to get them correct.

527
00:28:42,025 --> 00:28:45,550
Because, you know, if the students getting things correct them they've learned them.

528
00:28:45,550 --> 00:28:48,430
But the problem here is with that reward specification

529
00:28:48,430 --> 00:28:51,700
what the agent learns to do is to give really easy problems,

530
00:28:51,700 --> 00:28:55,360
and then maybe the student doesn't know how to do those initially but

531
00:28:55,360 --> 00:28:59,095
then they quickly learn how and then there's no incentive to give hard problems.

532
00:28:59,095 --> 00:29:04,330
So this is just sort of a small example of what is known as reward hacking,

533
00:29:04,330 --> 00:29:06,760
[LAUGHTER] which is that your agent is gonna

534
00:29:06,760 --> 00:29:09,280
learn to do exactly what it is that you tell him to

535
00:29:09,280 --> 00:29:14,380
do in terms of the rewards function that you specify and yet in reinforcement learning,

536
00:29:14,380 --> 00:29:16,450
often we spend very little of our time

537
00:29:16,450 --> 00:29:19,285
thinking very carefully about what that reward function is.

538
00:29:19,285 --> 00:29:21,070
So, whenever you get out and test for

539
00:29:21,070 --> 00:29:23,605
the real world this is the really really critical part.

540
00:29:23,605 --> 00:29:27,340
But normally, it is the designer that gets to pick what the reward function is,

541
00:29:27,340 --> 00:29:32,350
the agent is not having intrinsic internal reward and so depending on how you specify it,

542
00:29:32,350 --> 00:29:35,245
the agent will learn to do different things. Yeah, was there question in the back?

543
00:29:35,245 --> 00:29:38,035
In this case, it seems like the student will also be

544
00:29:38,035 --> 00:29:41,620
RL agent and that like in real life the student,

545
00:29:41,620 --> 00:29:44,750
so what we asked for her questions

546
00:29:45,120 --> 00:29:49,555
so techniques to approach or is it okay that we ignore that part?

547
00:29:49,555 --> 00:29:51,520
So, the question was to say well, you know,

548
00:29:51,520 --> 00:29:53,110
we also think that people are probably

549
00:29:53,110 --> 00:29:55,885
reinforcement learning agents as well and that's exactly correct,

550
00:29:55,885 --> 00:29:57,490
and maybe they would start to say, "Hey,

551
00:29:57,490 --> 00:29:58,900
I need to get harder questions,

552
00:29:58,900 --> 00:30:00,925
or be interactive in this process."

553
00:30:00,925 --> 00:30:03,880
For most of this class we're going to ignore the fact that the world

554
00:30:03,880 --> 00:30:06,970
that we interact with itself might also be an RL agent,

555
00:30:06,970 --> 00:30:09,220
in reality it's really critical, um,

556
00:30:09,220 --> 00:30:13,675
sometimes this is often considered in an adversarial way like for game theory,

557
00:30:13,675 --> 00:30:16,000
I think one of the most exciting things to me is when we

558
00:30:16,000 --> 00:30:18,040
think about it in a cooperative way?

559
00:30:18,040 --> 00:30:23,240
Um, so, who here has heard about the sub-discipline of machine teaching?

560
00:30:23,310 --> 00:30:25,525
Nobody yet, so, er,

561
00:30:25,525 --> 00:30:28,870
it's a really interesting new area that's been around for maybe 5-10 years,

562
00:30:28,870 --> 00:30:30,835
some a little bit beyond that.

563
00:30:30,835 --> 00:30:32,200
One of the ideas there is,

564
00:30:32,200 --> 00:30:34,570
what happens if you have two intelligent agents that are

565
00:30:34,570 --> 00:30:37,960
interacting with each other where they know that each other's trying to help them?

566
00:30:37,960 --> 00:30:41,650
Er, so there's a really nice classic example

567
00:30:41,650 --> 00:30:45,700
from sorry for those of you that aren't so familiar with machine learning but,

568
00:30:45,700 --> 00:30:49,870
imagine that you're trying to learn a classifier to decide where

569
00:30:49,870 --> 00:30:54,100
along this line things are either positive or negative.

570
00:30:54,100 --> 00:30:57,760
So in general you're going to need some amount of samples,

571
00:30:57,760 --> 00:30:59,785
samples if you, uh,

572
00:30:59,785 --> 00:31:01,450
wear that sort of the number of points on

573
00:31:01,450 --> 00:31:04,820
the line where you have to get positive or negative labels.

574
00:31:04,820 --> 00:31:07,545
Um, if you're in an active learning setting,

575
00:31:07,545 --> 00:31:10,550
generally I think you can reduce that to roughly log n

576
00:31:10,550 --> 00:31:15,250
by being strategic about asking people to label particularly points in a line,

577
00:31:15,250 --> 00:31:17,980
one of the really cool things for machine teaching is that,

578
00:31:17,980 --> 00:31:22,270
if I know you are trying to teach me where to divide this line,

579
00:31:22,270 --> 00:31:27,550
you'll only need one point or at most two points essentially constant, right?

580
00:31:27,550 --> 00:31:29,590
Because, if I'm trying to teach you,

581
00:31:29,590 --> 00:31:32,395
there's no way I'm just going to randomly label things.

582
00:31:32,395 --> 00:31:34,510
I'm just gonna label you a single plus and

583
00:31:34,510 --> 00:31:37,540
a minus and that's gonna tell you exactly where the line goes.

584
00:31:37,540 --> 00:31:40,060
So that's one of the reasons why if

585
00:31:40,060 --> 00:31:43,420
the agent knows that the other agent is trying to teach them something,

586
00:31:43,420 --> 00:31:47,515
it can actually be enormously more efficient than what we normally think of for learning.

587
00:31:47,515 --> 00:31:51,380
And so, I think there's a lot of potential for machine teaching to be really effective.

588
00:31:51,380 --> 00:31:53,985
But all that said, we're going to ignore most of that for the course,

589
00:31:53,985 --> 00:31:57,570
if it's something you want to explore in your project, you're very welcome to.

590
00:31:57,570 --> 00:32:00,280
There's a lot of connections with reinforcement learning.

591
00:32:01,980 --> 00:32:05,200
Okay. So, if we think about this process in general um,

592
00:32:05,200 --> 00:32:08,650
if we think of sort of a sequential decision making process, we have this agent.

593
00:32:08,650 --> 00:32:11,875
We're going to think about almost always about there being discreet timer.

594
00:32:11,875 --> 00:32:14,020
So, agent is gonna make a decision,

595
00:32:14,020 --> 00:32:15,955
it's gonna affect the world in some way,

596
00:32:15,955 --> 00:32:19,795
it's gonna see the world, it's gonna give some new observation and a reward.

597
00:32:19,795 --> 00:32:23,990
The agent receives those and uses it to make another decision.

598
00:32:24,810 --> 00:32:28,120
So, in this case when we think about a history,

599
00:32:28,120 --> 00:32:32,050
what we mean by history is simply the sequence of previous actions that the agent took,

600
00:32:32,050 --> 00:32:34,885
and the observations and rewards it received.

601
00:32:34,885 --> 00:32:39,070
Then the second thing that's really important is to define a state-space.

602
00:32:39,070 --> 00:32:41,290
Again, often when this was first discussed,

603
00:32:41,290 --> 00:32:44,290
this is sort of thought about is some immutable thing.

604
00:32:44,290 --> 00:32:46,090
But whenever you're in a real application,

605
00:32:46,090 --> 00:32:47,785
this is exactly what you have to define,

606
00:32:47,785 --> 00:32:50,905
is how to write down the representation of the world.

607
00:32:50,905 --> 00:32:53,140
Um, what we're going to assume in

608
00:32:53,140 --> 00:32:55,765
this class is that the state is a function of the history.

609
00:32:55,765 --> 00:32:58,300
So, there might be other aspects of- there might be

610
00:32:58,300 --> 00:33:00,220
other sensory information that the agent would like

611
00:33:00,220 --> 00:33:02,275
to have access to in order to make its decision.

612
00:33:02,275 --> 00:33:05,890
But it's going to be constrained to the observations is received so far,

613
00:33:05,890 --> 00:33:09,170
the actions is taken, and the rewards is observed.

614
00:33:09,660 --> 00:33:14,785
Now, there's also gonna be some real-world state. So, that's the real world.

615
00:33:14,785 --> 00:33:17,170
The agent doesn't necessarily have access to the real world.

616
00:33:17,170 --> 00:33:20,470
They may have access only to a small subset of the real world.

617
00:33:20,470 --> 00:33:22,450
So, for example as a human,

618
00:33:22,450 --> 00:33:24,670
right now, I have eyes that allow me to look forward.

619
00:33:24,670 --> 00:33:26,635
You know, roughly 180 degrees.

620
00:33:26,635 --> 00:33:28,615
Um, but I can't see behind my head.

621
00:33:28,615 --> 00:33:31,420
But behind my head is still part of the world state.

622
00:33:31,420 --> 00:33:33,865
So, the world state is the real world,

623
00:33:33,865 --> 00:33:39,110
and then the agent has its own state space it uses to try to make decisions.

624
00:33:39,810 --> 00:33:45,860
So, in general, we're gonna assume that it has some function of the history.

625
00:33:46,530 --> 00:33:49,870
Now, one assumption that we're gonna use a lot in this class

626
00:33:49,870 --> 00:33:52,345
which you guys have probably seen before is the Markov assumption.

627
00:33:52,345 --> 00:33:55,330
The Markov assumption simply says that we're going

628
00:33:55,330 --> 00:33:58,720
to assume that the state used by the agent uh,

629
00:33:58,720 --> 00:34:01,210
is a sufficient statistic of the history,

630
00:34:01,210 --> 00:34:03,340
and that in order to predict the future,

631
00:34:03,340 --> 00:34:06,385
you only need to know the current state of the environment.

632
00:34:06,385 --> 00:34:08,965
So, it's simply basically indicates that

633
00:34:08,965 --> 00:34:11,725
the future is independent of the past given the present,

634
00:34:11,725 --> 00:34:14,590
if in the present, you have the right aggregate statistic.

635
00:34:14,590 --> 00:34:19,975
[NOISE] So, as a couple of examples of this,

636
00:34:19,975 --> 00:34:22,165
yeah? Question name and-.

637
00:34:22,165 --> 00:34:24,040
Would you just explain maybe with

638
00:34:24,040 --> 00:34:26,710
an example the difference again between the state and the history?

639
00:34:26,710 --> 00:34:28,554
Like I'm having trouble to differentiate.

640
00:34:28,554 --> 00:34:30,819
Yeah. So, the state, um, uh,

641
00:34:30,820 --> 00:34:37,000
if we think about something like uh, um, a robot.

642
00:34:37,000 --> 00:34:42,505
Um, so let's say you have a robot that is walking down a long corridor.

643
00:34:42,505 --> 00:34:45,500
Okay. Let's say there's two long corridors.

644
00:34:46,560 --> 00:34:49,120
Okay. So, your robot starts here.

645
00:34:49,120 --> 00:34:50,665
This is where your robot starts,

646
00:34:50,665 --> 00:34:52,255
and it tries to go right,

647
00:34:52,255 --> 00:34:55,960
right, and then it goes down, down, down.

648
00:34:55,960 --> 00:35:01,660
Okay. Let's say its sensors are just that it can observe whether in front of it um,

649
00:35:01,660 --> 00:35:05,725
uh, um, whether there is a wall on any of its sides.

650
00:35:05,725 --> 00:35:10,030
So, it can- the observation space of the robot is simply is there

651
00:35:10,030 --> 00:35:15,475
a wall on any side-on each of its four sides?

652
00:35:15,475 --> 00:35:17,680
I'm sorry, it's probably a little bit small on the back.

653
00:35:17,680 --> 00:35:19,330
But the agent basically has, you know,

654
00:35:19,330 --> 00:35:23,530
some sort of local amount via laser range finder or something like that.

655
00:35:23,530 --> 00:35:26,560
So, it knows whether or not there's a wall immediately around it,

656
00:35:26,560 --> 00:35:29,680
that has been immediately around it square, and nothing else.

657
00:35:29,680 --> 00:35:35,050
So, in this case, what the agent would see is that initially the wall looks like this,

658
00:35:35,050 --> 00:35:38,290
and then like this, and then like this, and then like this.

659
00:35:38,290 --> 00:35:40,540
The history would include all of this.

660
00:35:40,540 --> 00:35:43,945
But it's local state is just this.

661
00:35:43,945 --> 00:35:47,275
So, local state could just be the current observation.

662
00:35:47,275 --> 00:35:49,600
That starts to be important when you're going down

663
00:35:49,600 --> 00:35:52,315
here because there are many places that looked like that.

664
00:35:52,315 --> 00:35:54,220
So, if you keep track of the whole history,

665
00:35:54,220 --> 00:35:55,810
the agent can figure out where it is.

666
00:35:55,810 --> 00:35:58,180
But if it only keeps track of where it is locally,

667
00:35:58,180 --> 00:36:01,250
then a lot of partial aliasing can occur.

668
00:36:01,410 --> 00:36:05,830
So, I put up a couple of examples here.

669
00:36:05,830 --> 00:36:07,600
So, in something like hypertension control,

670
00:36:07,600 --> 00:36:10,390
you can imagine the state is just the current blood pressure,

671
00:36:10,390 --> 00:36:14,245
um and your action is whether to take medication or not.

672
00:36:14,245 --> 00:36:17,140
So, current blood pressure meaning like you know,

673
00:36:17,140 --> 00:36:20,035
every second for example what is your blood pressure?

674
00:36:20,035 --> 00:36:24,350
So, do you think this sort of system is Markov?

675
00:36:24,930 --> 00:36:28,465
I see some people shaking their heads. Almost definitely not.

676
00:36:28,465 --> 00:36:32,080
Almost definitely there are other features that have to do with, you know,

677
00:36:32,080 --> 00:36:34,015
maybe whether or not you're exercising,

678
00:36:34,015 --> 00:36:35,785
whether or not you just ate a meal,

679
00:36:35,785 --> 00:36:37,270
whether it's hot outside.

680
00:36:37,270 --> 00:36:39,190
What the- if you just got an the airplane.

681
00:36:39,190 --> 00:36:42,925
All these other features probably affect whether or not

682
00:36:42,925 --> 00:36:44,890
your next blood pressure is going to be high or

683
00:36:44,890 --> 00:36:47,980
low and particularly in response to some medication.

684
00:36:47,980 --> 00:36:51,310
Um, similarly in something like website shopping, um,

685
00:36:51,310 --> 00:36:52,960
you can imagine the state is just sort of

686
00:36:52,960 --> 00:36:54,805
what is the product you're looking at right now?

687
00:36:54,805 --> 00:36:56,754
So, like I open up A- Amazon,

688
00:36:56,754 --> 00:36:59,409
I'm looking at some um, you know, computer,

689
00:36:59,409 --> 00:37:02,035
and um that's up on my webpage right now,

690
00:37:02,035 --> 00:37:04,390
and the action is what other products to recommend.

691
00:37:04,390 --> 00:37:06,305
Do you think that system is Markov?

692
00:37:06,305 --> 00:37:07,770
Systems is not Markov?

693
00:37:07,770 --> 00:37:10,065
Do you mean the system generally?

694
00:37:10,065 --> 00:37:13,275
But if the assumption is Markov and if it doesn't fit?

695
00:37:13,275 --> 00:37:16,620
Question is whether or not the system generally is Markov and

696
00:37:16,620 --> 00:37:20,430
the assumption just doesn't fit or make- just some more details. I'll think about this.

697
00:37:20,430 --> 00:37:21,720
What I mean here is that

698
00:37:21,720 --> 00:37:25,245
this particular choice re-representing the system is that Markov.

699
00:37:25,245 --> 00:37:28,200
Um, and so, there's the real-world going on,

700
00:37:28,200 --> 00:37:31,050
and then there's sort of the model of the world that the agent can use.

701
00:37:31,050 --> 00:37:35,120
What I'm arguing here is that these particular models of the world are not Markov.

702
00:37:35,120 --> 00:37:37,315
There might be other models of the world that are.

703
00:37:37,315 --> 00:37:38,680
Um, but if we choose

704
00:37:38,680 --> 00:37:42,880
this particular observation say just the current blood pressure as our state,

705
00:37:42,880 --> 00:37:45,385
that is probably not really a Markov state.

706
00:37:45,385 --> 00:37:49,165
Now it doesn't mean that we can't use algorithms that treat it as if it is.

707
00:37:49,165 --> 00:37:50,860
It is just that we should be aware that we might be

708
00:37:50,860 --> 00:37:53,200
violating some of those assumptions. Yeah?

709
00:37:53,200 --> 00:37:55,840
Um, I'm wondering so if you include um,

710
00:37:55,840 --> 00:37:57,610
enough history into a state,

711
00:37:57,610 --> 00:37:59,155
can you make them part of the Markov?

712
00:37:59,155 --> 00:38:00,700
Okay. It's a great question.

713
00:38:00,700 --> 00:38:03,685
So, why is it so popular?

714
00:38:03,685 --> 00:38:06,580
Can you know-can you always make something Markov?

715
00:38:06,580 --> 00:38:08,620
Um, generally yes.

716
00:38:08,620 --> 00:38:11,305
If you include all the- the history,

717
00:38:11,305 --> 00:38:13,360
then you can always make the system Markov.

718
00:38:13,360 --> 00:38:17,890
Um, in practice often you can get away with just using

719
00:38:17,890 --> 00:38:19,450
the most recent observation or

720
00:38:19,450 --> 00:38:23,545
maybe the last four observations as a reasonably sufficient statistic.

721
00:38:23,545 --> 00:38:25,180
It depends a lot on the domain.

722
00:38:25,180 --> 00:38:26,920
There's certainly domain, maybe like

723
00:38:26,920 --> 00:38:30,325
the navigation world I put up there where it's really important to model.

724
00:38:30,325 --> 00:38:33,430
Either use the whole history as the- as the state um,

725
00:38:33,430 --> 00:38:36,310
or think about the partial observability um,

726
00:38:36,310 --> 00:38:38,260
and other cases where you know,

727
00:38:38,260 --> 00:38:43,240
maybe the current- the most recent observation is completely sufficient.

728
00:38:43,240 --> 00:38:46,660
Now, one of the challenges here is you might not want to use

729
00:38:46,660 --> 00:38:50,290
the whole history because that's a lot of information.

730
00:38:50,290 --> 00:38:52,705
[LAUGHTER] and you have to keep track of it over time.

731
00:38:52,705 --> 00:38:55,735
And so, it's much nicer to have sort of a sufficient statistic.

732
00:38:55,735 --> 00:38:58,150
Um, of course, some of these things are changing

733
00:38:58,150 --> 00:39:00,610
a little bit with LSTMs and other things like that.

734
00:39:00,610 --> 00:39:04,630
So, um, some of our prior assumptions about how

735
00:39:04,630 --> 00:39:06,699
things scale with the size of the state-space

736
00:39:06,699 --> 00:39:08,860
are changing a little bit right now with deep learning.

737
00:39:08,860 --> 00:39:10,960
Um, but historically certainly,

738
00:39:10,960 --> 00:39:14,425
there's been advantages to having a- a smaller state-space.

739
00:39:14,425 --> 00:39:16,390
And um, again historically,

740
00:39:16,390 --> 00:39:19,269
there's been a lot of implications for things like computational complexity,

741
00:39:19,269 --> 00:39:20,380
the data required, and

742
00:39:20,380 --> 00:39:24,010
the resulting performance depending on the size of the state space.

743
00:39:24,010 --> 00:39:27,220
So, just to give some intuition for why that might be, um,

744
00:39:27,220 --> 00:39:31,300
if you made your state everything that's ever happened to you in your life,

745
00:39:31,300 --> 00:39:34,660
um, that would give you a really, really rich representation.

746
00:39:34,660 --> 00:39:36,790
You'd only have one data point for every state.

747
00:39:36,790 --> 00:39:39,205
There would be no repeating.

748
00:39:39,205 --> 00:39:41,740
So, it's really hard to learn because um,

749
00:39:41,740 --> 00:39:43,690
they're- all states are different.

750
00:39:43,690 --> 00:39:46,450
Um, and in general if we wanna learn how to do something,

751
00:39:46,450 --> 00:39:49,780
we're gonna either need some form a generalization or some form of

752
00:39:49,780 --> 00:39:53,170
clustering or aggregation so that we can compare experiences,

753
00:39:53,170 --> 00:39:57,200
so that we can learn from prior similar experience in order to what to do.

754
00:39:58,200 --> 00:40:02,605
So, if we think about assuming that your observation is your state,

755
00:40:02,605 --> 00:40:04,855
so the most recent observations that the agent gets,

756
00:40:04,855 --> 00:40:06,400
we're gonna treat that as the state.

757
00:40:06,400 --> 00:40:10,765
Then we- the agent is modelling the world is that Markov decision process.

758
00:40:10,765 --> 00:40:13,165
So, it is thinking of taking an action,

759
00:40:13,165 --> 00:40:14,680
getting observation and reward,

760
00:40:14,680 --> 00:40:16,015
and it's setting the state,

761
00:40:16,015 --> 00:40:20,485
the world state- that the environment state it's using to be the observation.

762
00:40:20,485 --> 00:40:25,255
If the world- if it is treating the world as partially observable um,

763
00:40:25,255 --> 00:40:28,750
then it says the agent state is not the same, um,

764
00:40:28,750 --> 00:40:32,620
and it sort of uses things like the history or beliefs about the world state

765
00:40:32,620 --> 00:40:37,210
to aggregate the sequence of previous actions taken and observations received,

766
00:40:37,210 --> 00:40:40,280
um, and uses that to make its decisions.

767
00:40:41,250 --> 00:40:44,380
For example, in something like poker,

768
00:40:44,380 --> 00:40:46,705
um, you get to see your own cards.

769
00:40:46,705 --> 00:40:50,320
Other people have cards that are clearly affecting the course of the game.

770
00:40:50,320 --> 00:40:52,330
Um, but you don't actually know what those are.

771
00:40:52,330 --> 00:40:55,000
You can see which cards are are discarded

772
00:40:55,000 --> 00:40:58,045
And so that's somewhere where it's naturally partially observable.

773
00:40:58,045 --> 00:41:02,200
And so you can maintain a belief state over what the other cards or at the other players.

774
00:41:02,200 --> 00:41:04,870
And you can use that information or to make your decisions.

775
00:41:04,870 --> 00:41:07,600
And similarly often in health care there's a whole bunch of

776
00:41:07,600 --> 00:41:09,940
really complicated physiological processes that are going

777
00:41:09,940 --> 00:41:12,430
on but you could monitor parts of them for things

778
00:41:12,430 --> 00:41:15,205
like you know blood pressure or temperature et cetera.

779
00:41:15,205 --> 00:41:18,290
Uh, and then use that in order to make decisions.

780
00:41:18,660 --> 00:41:22,540
So, in terms of types of sequential decision making processes,

781
00:41:22,540 --> 00:41:25,045
um, one of them is Bandits.

782
00:41:25,045 --> 00:41:26,965
We'll talk more about this later the term.

783
00:41:26,965 --> 00:41:31,210
Um, Bandits is sort of a really simple version of a markup decision process in

784
00:41:31,210 --> 00:41:33,640
the sense that the ideas that the actions

785
00:41:33,640 --> 00:41:37,280
that are taken have no influence over the next observation.

786
00:41:37,560 --> 00:41:40,360
So, when might this be reasonable?

787
00:41:40,360 --> 00:41:42,970
So, let's imagine that you have a series of customers coming to

788
00:41:42,970 --> 00:41:46,360
your website and you show each of them an ad.

789
00:41:46,360 --> 00:41:49,030
So, and then they either click on it or not and

790
00:41:49,030 --> 00:41:51,505
then you get another customer login into your website.

791
00:41:51,505 --> 00:41:55,270
So, in this case the ad that you show to customer one,

792
00:41:55,270 --> 00:41:59,635
generally doesn't affect who cuts- which customer two comes along.

793
00:41:59,635 --> 00:42:03,130
Now it could maybe in really complicated ways maybe customer one

794
00:42:03,130 --> 00:42:06,625
goes to Facebook and says I really really loved this ad, you should go watch it.

795
00:42:06,625 --> 00:42:09,520
Um, but most of the time whatever ad you showed a customer

796
00:42:09,520 --> 00:42:12,535
one does not at all affect who next logs into your website.

797
00:42:12,535 --> 00:42:16,315
And so the decisions you make only affect the local, um,

798
00:42:16,315 --> 00:42:21,070
the first customer and then the customer two is totally independent.

799
00:42:21,070 --> 00:42:24,235
Bandits have been really really important,

800
00:42:24,235 --> 00:42:26,020
um, for at least 50 years.

801
00:42:26,020 --> 00:42:28,480
Um, people thought about them for things like clinical trials,

802
00:42:28,480 --> 00:42:30,295
how to allocate people to clinical trials.

803
00:42:30,295 --> 00:42:34,150
You will think of them for websites and a whole bunch of other applications.

804
00:42:34,150 --> 00:42:39,535
MDPs and POMDPs say no wait the actions that you take can affect the state of the world,

805
00:42:39,535 --> 00:42:41,725
they affect often the next observation you get,

806
00:42:41,725 --> 00:42:43,270
um, as well as the reward.

807
00:42:43,270 --> 00:42:45,775
And you have to think about this closed loop system

808
00:42:45,775 --> 00:42:49,075
of the actions that you're taking changing the state of the world.

809
00:42:49,075 --> 00:42:52,870
So, the product that I recommend to my customer might affect what

810
00:42:52,870 --> 00:42:56,740
the customer's opinion is on the next time-step. In fact, you hope it will.

811
00:42:56,740 --> 00:42:59,320
Um, and so in these cases we think about,

812
00:42:59,320 --> 00:43:02,480
um, the actions actually affecting the state of the world.

813
00:43:03,320 --> 00:43:07,170
So, another important question is how the world changes?

814
00:43:07,170 --> 00:43:10,335
Um, one idea is that it changes deterministically.

815
00:43:10,335 --> 00:43:13,470
So, when you take an action in a particular state,

816
00:43:13,470 --> 00:43:17,810
you go to a different state but the state you go to it's deterministic. There's only one.

817
00:43:17,810 --> 00:43:20,620
And this is often a pretty common assumption in a lot of

818
00:43:20,620 --> 00:43:23,860
robotics and controls. All right.

819
00:43:23,860 --> 00:43:27,160
Remember, um, Toms Lozano-Prez who's a professor over

820
00:43:27,160 --> 00:43:30,520
at MIT ones suggesting to me that if you flip a coin,

821
00:43:30,520 --> 00:43:32,320
it's actually deterministic process.

822
00:43:32,320 --> 00:43:33,940
We're just modeling it as stochastic.

823
00:43:33,940 --> 00:43:35,560
We don't have good enough models.

824
00:43:35,560 --> 00:43:39,955
Um, so, there are many processes that if you could sort of write down,

825
00:43:39,955 --> 00:43:44,350
um, a sufficient perfect model of the world it would actually look deterministic.

826
00:43:44,350 --> 00:43:47,950
Um, but in many cases even though it maybe hard to write down those models.

827
00:43:47,950 --> 00:43:50,095
And so we're going to approximate them as stochastic.

828
00:43:50,095 --> 00:43:53,905
And the idea is that then when we take an action there are many possible outcomes.

829
00:43:53,905 --> 00:43:57,535
So, you couldn't show an ad to someone and they may or may not click on it.

830
00:43:57,535 --> 00:44:02,200
And we may just want to represent that with a stochastic, stochastic model.

831
00:44:02,200 --> 00:44:04,750
So, think about a particular example.

832
00:44:04,750 --> 00:44:07,570
So, if we think about something like Mars Rover, um,

833
00:44:07,570 --> 00:44:12,160
ah when we deploy rovers or robots on really far-off,

834
00:44:12,160 --> 00:44:15,805
um, planets, it's hard to do communication back and forth.

835
00:44:15,805 --> 00:44:19,030
So, be nice to be able to make these sort of robots more autonomous.

836
00:44:19,030 --> 00:44:21,940
Let's imagine that we have a very simple Mars rover that's,

837
00:44:21,940 --> 00:44:25,240
um, thinking about a seven state system.

838
00:44:25,240 --> 00:44:26,785
So, it's just landed.

839
00:44:26,785 --> 00:44:29,140
Um, it's got a particular location and it can

840
00:44:29,140 --> 00:44:31,600
either try to go left or try to go the right.

841
00:44:31,600 --> 00:44:34,390
I write down try left or try right meaning that that's

842
00:44:34,390 --> 00:44:37,120
what it's going to try to do but maybe you'll succeed or fail.

843
00:44:37,120 --> 00:44:41,215
Let's imagine that there's different sorts of scientific information to be discovered,

844
00:44:41,215 --> 00:44:43,840
and so over in S1 there's a little bit of

845
00:44:43,840 --> 00:44:46,060
useful scientific information but actually over at

846
00:44:46,060 --> 00:44:49,375
S7 there's an incredibly rich place where there might be water.

847
00:44:49,375 --> 00:44:52,430
And then there's zero in all other states.

848
00:44:53,130 --> 00:44:56,110
So, we'll go through that is a little bit of an example.

849
00:44:56,110 --> 00:44:59,785
As I start to talk about different common components of an oral agent.

850
00:44:59,785 --> 00:45:03,700
So, one often common component is a model.

851
00:45:03,700 --> 00:45:07,915
So, a model is simply going to be a representation of the agent has

852
00:45:07,915 --> 00:45:12,010
for what happens in the world as it takes its actions and what rewards it might get.

853
00:45:12,010 --> 00:45:14,920
So, in the case of the markup decision process it's simply

854
00:45:14,920 --> 00:45:18,220
a model that says if I start in this state and I take this action A,

855
00:45:18,220 --> 00:45:21,505
what is the distribution over next states I might reach

856
00:45:21,505 --> 00:45:26,170
and it also is going to have a reward model that predicts the expected reward of taking,

857
00:45:26,170 --> 00:45:28,810
um, an action in a certain state.

858
00:45:28,810 --> 00:45:31,165
So, in this case, ah,

859
00:45:31,165 --> 00:45:33,790
let's imagine that the reward of the agent is that

860
00:45:33,790 --> 00:45:36,520
it thinks that there's zero reward everywhere.

861
00:45:36,520 --> 00:45:41,200
Um, and let's imagine that it thinks its motor control is very bad.

862
00:45:41,200 --> 00:45:44,950
And so it estimates that whenever it tries to move with 50% probability it

863
00:45:44,950 --> 00:45:49,390
stays in the same place and 50% probability it actually moves.

864
00:45:49,390 --> 00:45:51,760
Now the model can be wrong.

865
00:45:51,760 --> 00:45:57,640
So, if you remember what I put up here the actual reward is

866
00:45:57,640 --> 00:46:00,670
that in state S1 you get plus one and in state

867
00:46:00,670 --> 00:46:04,780
S7 you get S you get 10 and everything else you get zero.

868
00:46:04,780 --> 00:46:09,055
And the reward I just wrote down here is that it's zero everywhere.

869
00:46:09,055 --> 00:46:12,700
So, this is a totally reasonable reward model the agent could have.

870
00:46:12,700 --> 00:46:14,455
It just happens to be wrong.

871
00:46:14,455 --> 00:46:17,620
And in many cases the model will be wrong, um,

872
00:46:17,620 --> 00:46:21,770
but often can still be used by the agent in useful ways.

873
00:46:22,050 --> 00:46:27,745
So, the next important component that is always needed by an oral agent is a policy.

874
00:46:27,745 --> 00:46:32,935
Um, and a policy or decision policy is simply how we make decisions.

875
00:46:32,935 --> 00:46:35,890
Now, because we're thinking about Markov decision processes here,

876
00:46:35,890 --> 00:46:39,055
we're going to think about them as being mappings from states to actions.

877
00:46:39,055 --> 00:46:43,240
And a deterministic policy simply means there's one action prostate.

878
00:46:43,240 --> 00:46:47,665
And the stochastic means you can have a distribution over actions you might take.

879
00:46:47,665 --> 00:46:50,230
So, maybe every time you drive to the airport,

880
00:46:50,230 --> 00:46:52,030
you flip a coin to decide whether you're going to take

881
00:46:52,030 --> 00:46:54,890
the back roads or whether you're going to take the highway.

882
00:46:54,930 --> 00:47:00,640
So, as a quick check imagine that in every single state we do the action try right

883
00:47:00,640 --> 00:47:06,445
is this the deterministic policy or stochastic policy? Deterministic great.

884
00:47:06,445 --> 00:47:09,640
We'll talk more about why deterministic policies are

885
00:47:09,640 --> 00:47:12,655
useful and when stochastic policies are useful shortly.

886
00:47:12,655 --> 00:47:14,875
Now, the value function, um,

887
00:47:14,875 --> 00:47:17,680
is the expected discounted sum of future rewards under

888
00:47:17,680 --> 00:47:20,710
a particular policy. So, it's a waiting.

889
00:47:20,710 --> 00:47:24,730
It's saying how much reward do I think I'm going to get both now and in the future

890
00:47:24,730 --> 00:47:29,740
weighted by how much I care about immediate versus long-term rewards.

891
00:47:29,740 --> 00:47:35,450
The discount factor gamma is going to be between zero and one.

892
00:47:37,170 --> 00:47:40,120
And so the value function that allows us to say

893
00:47:40,120 --> 00:47:43,130
sort of how good or bad different states are.

894
00:47:44,400 --> 00:47:49,390
So, again in the case of the Mars rovers let's imagine that our discount factor is zero.

895
00:47:49,390 --> 00:47:51,865
Our policy is to try to go right.

896
00:47:51,865 --> 00:47:55,405
And in this case say this is our value function.

897
00:47:55,405 --> 00:47:58,270
It says that the value of being in state one is plus one everything

898
00:47:58,270 --> 00:48:01,795
else is zero and the value of being in S7 is 10.

899
00:48:01,795 --> 00:48:04,600
Again, this might or might not be the correct value function.

900
00:48:04,600 --> 00:48:06,595
Depends also on the true dynamics model,

901
00:48:06,595 --> 00:48:10,105
but this is a value function that the agent could have for this policy.

902
00:48:10,105 --> 00:48:13,000
Simply tells us what is the expected discounted sum of

903
00:48:13,000 --> 00:48:16,060
rewards you'd get if you follow this policy starting in

904
00:48:16,060 --> 00:48:18,160
this state where you weigh

905
00:48:18,160 --> 00:48:23,330
each reward by gamma to the number of time steps at which you reach it.

906
00:48:24,690 --> 00:48:27,040
So, when we think about, yeah.

907
00:48:27,040 --> 00:48:32,380
So, if we wanted to extend the discount fac- factor to this example, um,

908
00:48:32,380 --> 00:48:34,570
would there be, like, ah,

909
00:48:34,570 --> 00:48:39,670
an increasing value or decreasing value to reward depending on how far it went?

910
00:48:39,670 --> 00:48:41,050
Yes. Question was if,

911
00:48:41,050 --> 00:48:42,910
if the Gamma was not 0 here.

912
00:48:42,910 --> 00:48:45,490
Um, so gamma is being 0 here

913
00:48:45,490 --> 00:48:48,325
indicates that essentially we just care about immediate rewards,

914
00:48:48,325 --> 00:48:49,570
whether or not we'd start to,

915
00:48:49,570 --> 00:48:50,650
sort of, if I understood correctly,

916
00:48:50,650 --> 00:48:54,700
you start to see like rewards slew into other states, and the answer is yes.

917
00:48:54,700 --> 00:48:56,440
So, we'll see more of that next time,

918
00:48:56,440 --> 00:48:58,405
but if the discount factor is non-zero,

919
00:48:58,405 --> 00:49:01,690
then it basically says you care about not just the immediate reward you get,

920
00:49:01,690 --> 00:49:02,875
[NOISE] you're not just myopic,

921
00:49:02,875 --> 00:49:05,750
you care about their reward you're gonna get in the future.

922
00:49:08,190 --> 00:49:12,115
So, in terms of common types of reinforcement learning agents,

923
00:49:12,115 --> 00:49:13,420
um, some of them are model-based,

924
00:49:13,420 --> 00:49:17,680
which means they maintain in their representation a direct model of how the world works,

925
00:49:17,680 --> 00:49:20,200
like a transition model and a reward model.

926
00:49:20,200 --> 00:49:23,365
Um, and they may or may not have a policy or a value function.

927
00:49:23,365 --> 00:49:25,150
They always have to compute a policy.

928
00:49:25,150 --> 00:49:26,350
They have to figure out what to do.

929
00:49:26,350 --> 00:49:28,630
But they may or may not have an explicit representation

930
00:49:28,630 --> 00:49:30,940
for what they would do in any state.

931
00:49:30,940 --> 00:49:34,660
Um, model free approaches have an explicit value function and

932
00:49:34,660 --> 00:49:38,625
a policy function and no model. Yeah.

933
00:49:38,625 --> 00:49:40,650
Going back with [NOISE] the- the earlier slide,

934
00:49:40,650 --> 00:49:43,830
I'm confusing when the value function is

935
00:49:43,830 --> 00:49:47,670
evaluated ice with the- with well the setting yes.

936
00:49:47,670 --> 00:49:50,915
So, why is it not [NOISE] S_6 that has value

937
00:49:50,915 --> 00:49:55,360
of 10 because if you try right at S_6 you get to S_7.

938
00:49:55,360 --> 00:49:58,870
You were saying well how do I- when do we think of the rewards happening.

939
00:49:58,870 --> 00:50:01,225
Um, we'll talk more about that next time.

940
00:50:01,225 --> 00:50:05,230
When really, uh, there's many different ways people think of where the rewards happening.

941
00:50:05,230 --> 00:50:08,965
Some people think of it as the reward happening for the current state you're in.

942
00:50:08,965 --> 00:50:12,490
Some people think of it as it's the reward you're in [NOISE] and the action you take.

943
00:50:12,490 --> 00:50:17,050
And some people- some- another common definition is r- SAS prime,

944
00:50:17,050 --> 00:50:20,185
meaning that you don't see what reward you get until you transition.

945
00:50:20,185 --> 00:50:22,420
And this particular definition that I'm using

946
00:50:22,420 --> 00:50:25,930
here we're assuming that rewards happened in one year in that state.

947
00:50:25,930 --> 00:50:28,195
All of them are, um,

948
00:50:28,195 --> 00:50:30,280
basically isomorphic, um, but we'll

949
00:50:30,280 --> 00:50:32,290
try to be careful about which one we're using [NOISE].

950
00:50:32,290 --> 00:50:36,910
The most common one we'll use in the class is s,a which says that when you're in a state,

951
00:50:36,910 --> 00:50:38,185
and you choose a particular action,

952
00:50:38,185 --> 00:50:41,990
then you get a reward, and then you transition to your next state.

953
00:50:42,600 --> 00:50:49,210
Great question. Okay. So, when we think about reinforcement learning agents,

954
00:50:49,210 --> 00:50:52,810
and whether or not they're maintaining these models and these values and these policies,

955
00:50:52,810 --> 00:50:54,595
um, we get a lot of intersection.

956
00:50:54,595 --> 00:50:57,520
So, I really like this figure from David Silver, um,

957
00:50:57,520 --> 00:50:59,185
I- where he thinks about, sort of,

958
00:50:59,185 --> 00:51:03,115
RL algorithms or agents mostly falling into these three different classes.

959
00:51:03,115 --> 00:51:06,865
They even have a model or explicit policy or explicit value function.

960
00:51:06,865 --> 00:51:08,740
And then there's a whole bunch of algorithms that are,

961
00:51:08,740 --> 00:51:10,540
sort of, in the intersection of these.

962
00:51:10,540 --> 00:51:13,705
So, things like actor critic often have an explicit.

963
00:51:13,705 --> 00:51:15,100
And what do I mean by explicit?

964
00:51:15,100 --> 00:51:17,590
I mean like often they have a way so that if you give it

965
00:51:17,590 --> 00:51:20,440
a state you could tell- I could tell you what the value is,

966
00:51:20,440 --> 00:51:22,660
if I give you a state you could tell me immediately what

967
00:51:22,660 --> 00:51:25,555
the policy is, without additional computation.

968
00:51:25,555 --> 00:51:29,335
So, actor-critic combines value functions and policies.

969
00:51:29,335 --> 00:51:31,570
Um, there's a lot of algorithms that are also

970
00:51:31,570 --> 00:51:33,550
in the intersection of all of these different ones.

971
00:51:33,550 --> 00:51:36,385
And often in practice it's just very hopeful to maintain.

972
00:51:36,385 --> 00:51:39,520
Many of these and they have different strengths and weaknesses.

973
00:51:39,520 --> 00:51:43,330
For those of you that are interested in the theoretical aspects of learning theory,

974
00:51:43,330 --> 00:51:45,520
there's some really cool recent work, um,

975
00:51:45,520 --> 00:51:49,180
that explicitly looks at what is the formal foundational differences

976
00:51:49,180 --> 00:51:52,720
between model-based and model-free RL that just came out of MSR,

977
00:51:52,720 --> 00:51:53,770
Microsoft Research [NOISE] ,

978
00:51:53,770 --> 00:51:56,080
um, in New York, which indicates that there may be

979
00:51:56,080 --> 00:51:59,020
a fundamental gap between model-based and model-free methods,

980
00:51:59,020 --> 00:52:02,230
um, which on the deep learning side has been very unclear.

981
00:52:02,230 --> 00:52:04,885
So, feel free to come ask me about that.

982
00:52:04,885 --> 00:52:08,020
So, what are the challenges in learning to make good decisions,

983
00:52:08,020 --> 00:52:09,280
um, in this, sort of, framework?

984
00:52:09,280 --> 00:52:12,220
Um, one, is this issue of planning that we talked about a little bit before,

985
00:52:12,220 --> 00:52:14,740
which is even once I've got a model of how the world works,

986
00:52:14,740 --> 00:52:17,695
I have to use it to figure out what decisions I should make,

987
00:52:17,695 --> 00:52:21,010
in a way that I think it's going to allow me to achieve high reward.

988
00:52:21,010 --> 00:52:24,910
Um, [NOISE] and in this case if you're given

989
00:52:24,910 --> 00:52:29,350
a model you couldn't do this planning without any interaction in the real world.

990
00:52:29,350 --> 00:52:30,820
So, if someone says,

991
00:52:30,820 --> 00:52:31,960
here's your transition model,

992
00:52:31,960 --> 00:52:34,810
and here's your reward model, you can go off and do a bunch of computations,

993
00:52:34,810 --> 00:52:36,280
on your computer or by paper,

994
00:52:36,280 --> 00:52:38,335
and decide what the optimal action is to do,

995
00:52:38,335 --> 00:52:41,005
and then go back to the real world and take that action.

996
00:52:41,005 --> 00:52:45,610
It doesn't require any additional experience to compute that.

997
00:52:46,800 --> 00:52:48,940
But in reinforcement learning,

998
00:52:48,940 --> 00:52:52,240
we have this at other additional issue that we might want to think about not

999
00:52:52,240 --> 00:52:56,290
just what I think is the best thing for me to do given the information I have so far,

1000
00:52:56,290 --> 00:52:58,600
but what is the way I should act so that I can get

1001
00:52:58,600 --> 00:53:02,800
the information I need in order to make good decisions in the future.

1002
00:53:02,800 --> 00:53:06,370
So, [NOISE] it's, like, you know, you go to a brand new restaurant, and, ah,

1003
00:53:06,370 --> 00:53:08,035
let's say- let's say you move to a new town,

1004
00:53:08,035 --> 00:53:09,610
you go to- there's only one restaurant,

1005
00:53:09,610 --> 00:53:12,265
you go there the first day, and they have five different dishes.

1006
00:53:12,265 --> 00:53:13,690
You're gonna be there for a long time,

1007
00:53:13,690 --> 00:53:15,595
and you wanna optimizing at the best dish.

1008
00:53:15,595 --> 00:53:17,965
And so maybe the first day you try dish one,

1009
00:53:17,965 --> 00:53:19,990
and the second day you tr- try dish two,

1010
00:53:19,990 --> 00:53:21,460
and then the third day three,

1011
00:53:21,460 --> 00:53:23,620
and then et cetera so that you can try everything,

1012
00:53:23,620 --> 00:53:26,320
and then use that to figure out which one is best so that over

1013
00:53:26,320 --> 00:53:29,785
the long term you pick something that is really delicious.

1014
00:53:29,785 --> 00:53:33,850
So, in this case the agent has to think explicitly about what decision it should take

1015
00:53:33,850 --> 00:53:38,450
so it can get the information it needs so that in the future it can make good decisions.

1016
00:53:40,110 --> 00:53:42,685
So, in the case of planning,

1017
00:53:42,685 --> 00:53:44,665
and the fact that this is already a hard problem,

1018
00:53:44,665 --> 00:53:46,900
um, you think about something like solitaire,

1019
00:53:46,900 --> 00:53:49,915
um, you could already know the rules of the game,

1020
00:53:49,915 --> 00:53:54,220
this is also true for things like go or chess or many other scenarios.

1021
00:53:54,220 --> 00:53:57,070
Um, and you could know if you take an action

1022
00:53:57,070 --> 00:53:59,545
what would be the probability distribution of the next [NOISE] state,

1023
00:53:59,545 --> 00:54:02,560
and you can use this to compute a potential score.

1024
00:54:02,560 --> 00:54:06,490
And so using things like tree search or dynamic programming,

1025
00:54:06,490 --> 00:54:08,200
and we'll talk a lot more [NOISE] about these, um,

1026
00:54:08,200 --> 00:54:11,260
ah, particularly the dynamic programming aspect you can use

1027
00:54:11,260 --> 00:54:15,440
that to decide given a model of the world what is the right decision to make.

1028
00:54:15,450 --> 00:54:18,565
But sol- the reinforcement learning itself

1029
00:54:18,565 --> 00:54:21,025
is a little bit more like solitary without a rule book.

1030
00:54:21,025 --> 00:54:23,965
We're here just playing things and you're observing what is happening,

1031
00:54:23,965 --> 00:54:26,050
and you're trying to get larger reward.

1032
00:54:26,050 --> 00:54:28,750
And you might use your experience to explicitly compute

1033
00:54:28,750 --> 00:54:31,270
a model and then plan in that model,

1034
00:54:31,270 --> 00:54:35,720
or you might not and you might directly compute a policy or a value function.

1035
00:54:36,840 --> 00:54:41,845
Now, I just wanna reemphasize here this issue of exploration and exploitation.

1036
00:54:41,845 --> 00:54:44,620
So, in the case of the Mars rover it's only going to

1037
00:54:44,620 --> 00:54:47,725
learn about how the world works for the actions it tries.

1038
00:54:47,725 --> 00:54:52,285
So, in state S2 if it tries to go left it can see what happens there.

1039
00:54:52,285 --> 00:54:55,465
And then from there it can decide the right next action.

1040
00:54:55,465 --> 00:54:59,530
Now, this is obvious but it can lead to a dilemma because it has to

1041
00:54:59,530 --> 00:55:03,385
be able to balance between things that seem like they might be good,

1042
00:55:03,385 --> 00:55:04,885
based on your prior experience,

1043
00:55:04,885 --> 00:55:06,715
and things that might be good in the future,

1044
00:55:06,715 --> 00:55:09,740
um, that perhaps you've got unlucky before.

1045
00:55:11,100 --> 00:55:17,605
So, in exploration we're often interested in trying things that we've never tried before,

1046
00:55:17,605 --> 00:55:20,095
or trying things that so far might have looked bad,

1047
00:55:20,095 --> 00:55:21,970
but we think in the future might be good.

1048
00:55:21,970 --> 00:55:24,220
But an exploitation we're trying things that are

1049
00:55:24,220 --> 00:55:27,770
expected to be good given the past experience.

1050
00:55:28,960 --> 00:55:32,435
So, here's three examples of this.

1051
00:55:32,435 --> 00:55:34,210
In the case of movies, um,

1052
00:55:34,210 --> 00:55:36,660
exploitation is like watching your favorite movie,

1053
00:55:36,660 --> 00:55:38,650
and exploration is watching a new movie,

1054
00:55:38,650 --> 00:55:40,665
that might be good or it might be awful.

1055
00:55:40,665 --> 00:55:45,340
Advertising is showing the ad that sealed the most highest click-through rate so far.

1056
00:55:45,340 --> 00:55:47,870
Um, exploration is showing a different ad.

1057
00:55:47,870 --> 00:55:54,060
And driving exploitation is trying the fastest route given your prior experience and

1058
00:55:54,060 --> 00:56:16,880
exploration is driving a different route.

1059
00:56:16,880 --> 00:56:19,615
[inaudible].

1060
00:56:19,615 --> 00:56:21,255
Great question, which is,

1061
00:56:21,255 --> 00:56:23,040
what's the imagine for that example that I gave?

1062
00:56:23,040 --> 00:56:25,130
I am that you're only going to be in town for five days.

1063
00:56:25,130 --> 00:56:27,410
Um, and with the policy that you would

1064
00:56:27,410 --> 00:56:30,325
compute in that case if you're in a finite horizon setting,

1065
00:56:30,325 --> 00:56:32,690
be the same or different as one where you know you're going

1066
00:56:32,690 --> 00:56:35,075
to live in this for all of infinite time.

1067
00:56:35,075 --> 00:56:37,105
Um, we'll talk a little bit more about this next,

1068
00:56:37,105 --> 00:56:39,035
ah, next time but very different.

1069
00:56:39,035 --> 00:56:41,140
Um, and in particular, um,

1070
00:56:41,140 --> 00:56:43,270
the normally the policy if you only have

1071
00:56:43,270 --> 00:56:46,750
a finite horizon is non-stationary which means that,

1072
00:56:46,750 --> 00:56:50,510
um, the decision you will make depends on the time step as well as the state.

1073
00:56:50,510 --> 00:56:53,805
In the infinite horizon case the assumption is that, um,

1074
00:56:53,805 --> 00:56:57,015
the optimal policy and the mark off setting is stationary,

1075
00:56:57,015 --> 00:56:59,770
which means that if you're in the same state whether you're there on time

1076
00:56:59,770 --> 00:57:03,110
step three or time step 3,000 you will always do the same thing.

1077
00:57:03,110 --> 00:57:07,415
Um, but in the finite horizon case that's not true.

1078
00:57:07,415 --> 00:57:09,760
And as a critical example of that.

1079
00:57:09,760 --> 00:57:11,180
So, why do we explore?

1080
00:57:11,180 --> 00:57:14,570
We explore in order to learn information that we can use in the future.

1081
00:57:14,570 --> 00:57:17,590
So, if you're in a finite horizon setting and it's the last day is

1082
00:57:17,590 --> 00:57:20,865
your last day in Hollywood and you know you're trying to decide what to do, um,

1083
00:57:20,865 --> 00:57:23,660
you're not going to explore because there's no benefit from

1084
00:57:23,660 --> 00:57:26,440
exploration for future because you're not making any more decisions,

1085
00:57:26,440 --> 00:57:28,830
so in that case you will always exploit,

1086
00:57:28,830 --> 00:57:30,690
its always optimal to exploit.

1087
00:57:30,690 --> 00:57:33,460
So, in the finite horizon case, um,

1088
00:57:33,460 --> 00:57:35,970
the decisions you make have to depend on the value of

1089
00:57:35,970 --> 00:57:39,830
the information you gain to change your decisions and the remaining horizon.

1090
00:57:39,830 --> 00:57:43,680
And there's this often comes up in real cases. Yeah.

1091
00:57:43,680 --> 00:57:48,270
How much, um, how much more complicated

1092
00:57:48,270 --> 00:57:53,125
is if there's a finite horizon but you don't know where is this?

1093
00:57:53,125 --> 00:57:58,925
Uh, is just something I remember from game theory this tends to be very complicated.

1094
00:57:58,925 --> 00:58:01,015
How this [inaudible]?

1095
00:58:01,015 --> 00:58:04,310
Question is what about what I would call it indefinite horizon problems where there

1096
00:58:04,310 --> 00:58:07,580
is a finite horizon but you don't know what it is that can get very tricky.

1097
00:58:07,580 --> 00:58:11,920
One way to model it is as an infinite horizon problem with termination states.

1098
00:58:11,920 --> 00:58:14,030
So, there are some states which are essentially stink

1099
00:58:14,030 --> 00:58:16,360
states once you get there the process ends.

1100
00:58:16,360 --> 00:58:18,100
There's often happens in games, um,

1101
00:58:18,100 --> 00:58:20,575
you don't know when the game will end but it's going to be finite.

1102
00:58:20,575 --> 00:58:23,670
Um, and that answer that's one way to put it into the formalism,

1103
00:58:23,670 --> 00:58:25,460
um, but it is tricky.

1104
00:58:25,460 --> 00:58:27,150
In those cases we tend to model it has

1105
00:58:27,150 --> 00:58:30,640
infinite horizon and look at the probability of reaching different termination states.

1106
00:58:30,640 --> 00:58:33,770
[inaudible] you miss exploitation,

1107
00:58:33,770 --> 00:58:37,680
exploration essentially subproblems, I guess particulary for driving.

1108
00:58:37,680 --> 00:58:40,490
It seems like it would be better to kind of

1109
00:58:40,490 --> 00:58:43,470
exploit has you know are really good and maybe

1110
00:58:43,470 --> 00:58:50,195
explore on some [inaudible] don't know her as good rather than trying like completely brand new route.

1111
00:58:50,195 --> 00:58:53,530
In about how this mix happens of exploration,

1112
00:58:53,530 --> 00:58:56,090
exploitation and maybe in the cases of cars, maybe you would,

1113
00:58:56,090 --> 00:58:57,770
um, sort of, er,

1114
00:58:57,770 --> 00:58:59,380
not try things totally randomly.

1115
00:58:59,380 --> 00:59:01,310
You might need some evidence that they might be good,

1116
00:59:01,310 --> 00:59:03,265
um, it's a great question,

1117
00:59:03,265 --> 00:59:07,375
um, there's generally it is better to intermix exploration exploitation.

1118
00:59:07,375 --> 00:59:12,010
In some cases it is optimal to do all your exploration early or at least equivalent, um,

1119
00:59:12,010 --> 00:59:14,880
and then it came from all of that information for later,

1120
00:59:14,880 --> 00:59:17,020
but it depends on the decision process.

1121
00:59:17,020 --> 00:59:18,875
Um, and we'll spend a significant chunk of

1122
00:59:18,875 --> 00:59:21,640
the course after the midterm thinking about exploration,

1123
00:59:21,640 --> 00:59:25,245
exploitation, it's definitely a really critical part of reinforcement learning,

1124
00:59:25,245 --> 00:59:27,590
um, particularly in high stakes domains.

1125
00:59:27,590 --> 00:59:29,160
What do I mean by high-stakes domains?

1126
00:59:29,160 --> 00:59:30,495
I mean domains that affect people.

1127
00:59:30,495 --> 00:59:33,880
So, whether it's customers or patients or students, um,

1128
00:59:33,880 --> 00:59:37,265
that's where the decisions we make actually affect real people and so we want

1129
00:59:37,265 --> 00:59:41,255
to try to learn as quickly as possible and make good decisions as quick as we can.

1130
00:59:41,255 --> 00:59:43,920
Any other questions about this?

1131
00:59:43,920 --> 00:59:47,820
If you're in- you're in sort of state that you haven't seen before,

1132
00:59:47,820 --> 00:59:52,240
do you have any other better option and just take a random action to get out of there?

1133
00:59:52,240 --> 00:59:56,270
Or you can use your previous experience even though you're not never been there before?

1134
00:59:56,270 --> 00:59:57,310
The question is great.

1135
00:59:57,310 --> 01:00:00,080
It's the same if you're in a new state you've never been in before, what do you do?

1136
01:00:00,080 --> 01:00:01,520
Can you do anything better than random?

1137
01:00:01,520 --> 01:00:03,530
Or can you somehow use your prior experience?

1138
01:00:03,530 --> 01:00:05,410
Um, one of the really great things about

1139
01:00:05,410 --> 01:00:07,605
doing generalization means that we're going to use

1140
01:00:07,605 --> 01:00:10,040
state features either learned by deep learning or

1141
01:00:10,040 --> 01:00:12,605
some other representation to try to share information.

1142
01:00:12,605 --> 01:00:16,210
So, that even though [NOISE] the state might not be one you've ever exactly visited

1143
01:00:16,210 --> 01:00:18,090
before you can share prior information to

1144
01:00:18,090 --> 01:00:20,805
try to inform what might be a good action to do.

1145
01:00:20,805 --> 01:00:23,995
Of course if you share in the wrong direction,

1146
01:00:23,995 --> 01:00:25,720
um, you can make the wrong decision.

1147
01:00:25,720 --> 01:00:27,810
So, if you overshoot-overgeneralize you could

1148
01:00:27,810 --> 01:00:29,990
overfit your prior experience and in fact that there's

1149
01:00:29,990 --> 01:00:35,130
a better action to do in the new scenario. Any questions for this?

1150
01:00:35,970 --> 01:00:41,110
Okay. So, one of the things we're going to be talking about over

1151
01:00:41,110 --> 01:00:43,690
the next few lectures is this trend two

1152
01:00:43,690 --> 01:00:47,120
really fundamental problems which is evaluation and control.

1153
01:00:47,120 --> 01:00:51,095
So, evaluation is the problem as saying if someone gives you a policy,

1154
01:00:51,095 --> 01:00:53,800
if they're like hey this is what you should do or this is what your agent should do,

1155
01:00:53,800 --> 01:00:58,355
this is how your robot should act in the world to evaluate how good it is.

1156
01:00:58,355 --> 01:01:01,690
So, we want to be able to figure out you know your manager says Oh

1157
01:01:01,690 --> 01:01:04,670
I think this is the right way we should show ads to customers,

1158
01:01:04,670 --> 01:01:06,125
um, can you tell me how good it is?

1159
01:01:06,125 --> 01:01:08,090
What's the quick [inaudible]?

1160
01:01:08,090 --> 01:01:11,180
Um, so one really important question is evaluation,

1161
01:01:11,180 --> 01:01:13,610
um, and you know you might not have a model of the world.

1162
01:01:13,610 --> 01:01:15,690
So, you might have to go out and gather data to try to

1163
01:01:15,690 --> 01:01:18,060
evaluate this policy be useful to know how good it is,

1164
01:01:18,060 --> 01:01:20,665
you're not trying to make a new policy

1165
01:01:20,665 --> 01:01:23,835
with not yet you're just trying to see how good this current one is.

1166
01:01:23,835 --> 01:01:27,200
And then the control problem is optimization.

1167
01:01:27,200 --> 01:01:29,890
It's saying let's try to find a really good policy.

1168
01:01:29,890 --> 01:01:31,560
This typically involves as

1169
01:01:31,560 --> 01:01:37,440
a sub-component evaluation because often we're going to need to know what does best mean?

1170
01:01:37,440 --> 01:01:38,840
Best means a really good policy.

1171
01:01:38,840 --> 01:01:40,185
How do we know how good the policies?

1172
01:01:40,185 --> 01:01:41,845
We need to do evaluation.

1173
01:01:41,845 --> 01:01:45,020
Now one of the really cool aspects of reinforcement learning, um,

1174
01:01:45,020 --> 01:01:49,215
is that often we can do this evaluation off policy.

1175
01:01:49,215 --> 01:01:52,510
Which means we can use data gathered from other policies to

1176
01:01:52,510 --> 01:01:56,045
evaluate the counterfactual of what different policies might do.

1177
01:01:56,045 --> 01:01:58,180
This is really helpful with because it means we

1178
01:01:58,180 --> 01:02:01,040
don't have to try out all policies exhaustively.

1179
01:02:02,090 --> 01:02:06,140
So, um, in terms of what these questions look like,

1180
01:02:06,140 --> 01:02:08,680
if we go back to our Mars Rover example for

1181
01:02:08,680 --> 01:02:12,275
policy evaluation it would be if someone says your policy is this,

1182
01:02:12,275 --> 01:02:15,965
in all of your states the action you should take as try right.

1183
01:02:15,965 --> 01:02:18,855
This is the discount factor I care about, um,

1184
01:02:18,855 --> 01:02:23,550
please compute for me or evaluate for me what is the value of this policy?

1185
01:02:24,260 --> 01:02:26,475
In the control case,

1186
01:02:26,475 --> 01:02:28,630
they would say I don't know what the policy should be.

1187
01:02:28,630 --> 01:02:31,305
I just want you to give me whatever ever policy has

1188
01:02:31,305 --> 01:02:34,490
the highest expected discounted sum of rewards,

1189
01:02:34,490 --> 01:02:38,915
um, and there's actually sort of a key question here is.

1190
01:02:38,915 --> 01:02:41,905
Okay. Expected discounted sum of rewards from what?

1191
01:02:41,905 --> 01:02:44,930
So, they might care about a particular starting state,

1192
01:02:44,930 --> 01:02:49,340
they might say I want you to figure out the best policy assuming I'm starting from S4.

1193
01:02:49,340 --> 01:02:53,255
They might say I want you to compute the best policy from all starting states,

1194
01:02:53,255 --> 01:02:56,360
um, or sort of some average.

1195
01:02:59,120 --> 01:03:02,460
So, in terms of the rest of the course where we get- yeah.

1196
01:03:02,460 --> 01:03:04,810
I was just wondering if it's possible to learned

1197
01:03:04,810 --> 01:03:09,140
the optimal policy and the reward function simultaneously?

1198
01:03:09,140 --> 01:03:13,820
Through example if I has some belief of what the reward review that

1199
01:03:13,820 --> 01:03:16,210
included or for some sort of action there

1200
01:03:16,210 --> 01:03:18,680
will be a state and that turned out to be wrong,

1201
01:03:18,680 --> 01:03:21,105
ah, we have to start over and trained to find

1202
01:03:21,105 --> 01:03:23,990
optimal policy or could I use what I've learned so far.

1203
01:03:23,990 --> 01:03:29,745
In addition assumption organization of data with a belief of the rewards that [inaudible]?

1204
01:03:29,745 --> 01:03:34,500
Fake question, which is. Okay. Let's say I have a policy to start with I'm evaluating it,

1205
01:03:34,500 --> 01:03:36,870
um, and I don't know what the reward function is and I don't know what

1206
01:03:36,870 --> 01:03:39,710
the optimal policy is and it turns out this [inaudible] isn't very good.

1207
01:03:39,710 --> 01:03:41,690
Do I need to sort of restart or can I use

1208
01:03:41,690 --> 01:03:45,100
that prior experience to sort of inform what's the next policy I try?

1209
01:03:45,100 --> 01:03:47,800
Ah, perhaps a whole suite of different policies?

1210
01:03:47,800 --> 01:03:50,590
In general you can use the prior experience in order to

1211
01:03:50,590 --> 01:03:53,780
inform what the next policy is that you try our next suite of policies.

1212
01:03:53,780 --> 01:03:56,810
Um, there's a little bit of a caveat there which is,

1213
01:03:56,810 --> 01:03:59,625
uh, you need to have some stochasticity in the actions you take.

1214
01:03:59,625 --> 01:04:03,120
So, if you only take the same you know one action in a state,

1215
01:04:03,120 --> 01:04:05,270
you can't really learn about any other,

1216
01:04:05,270 --> 01:04:06,905
um, actions you would take.

1217
01:04:06,905 --> 01:04:10,910
So, you need to assume some sort of generalization or some sort of stochasticity in

1218
01:04:10,910 --> 01:04:16,565
your policy in order for that information to be useful to try to evaluate other policies.

1219
01:04:16,565 --> 01:04:18,300
This is a really important issue.

1220
01:04:18,300 --> 01:04:20,910
This is the issue of sort of counterfactual reasoning and how do we

1221
01:04:20,910 --> 01:04:24,165
use our old data to figure out how we should act in the future,

1222
01:04:24,165 --> 01:04:27,290
um, if the old policies may not be the optimal ones.

1223
01:04:27,290 --> 01:04:28,590
So, in general we can, um,

1224
01:04:28,590 --> 01:04:31,535
and we'll talk a lot about that it's a really important issue.

1225
01:04:31,535 --> 01:04:34,620
So, we're first going to start off talking about sort of Markov

1226
01:04:34,620 --> 01:04:36,810
decision processes and planning and

1227
01:04:36,810 --> 01:04:41,325
talking about how do we sort of do this evaluation both whom we know how the world works,

1228
01:04:41,325 --> 01:04:45,160
meaning that we are given a transition model and reward model and when we're not,

1229
01:04:45,160 --> 01:04:46,530
then we're also going to talk about

1230
01:04:46,530 --> 01:04:49,810
model-free policy evaluation and then model-free control.

1231
01:04:49,810 --> 01:04:51,880
We're going to then spend some time on

1232
01:04:51,880 --> 01:04:54,250
deep-deep reinforcement learning and

1233
01:04:54,250 --> 01:04:56,525
reinforcement learning in general with function approximation,

1234
01:04:56,525 --> 01:04:59,125
which is a hugely growing area right now.

1235
01:04:59,125 --> 01:05:01,130
Um, I thought about making a plot of

1236
01:05:01,130 --> 01:05:04,260
how many papers are going on in this area right now it's pretty incredible.

1237
01:05:04,260 --> 01:05:06,860
Um, and then we're going to talk a lot about

1238
01:05:06,860 --> 01:05:09,620
policy search which I think in practice particularly in robotics

1239
01:05:09,620 --> 01:05:12,310
is one of the most influential methods right now and we're

1240
01:05:12,310 --> 01:05:15,440
going to spend quite a lot of time on exploration as well as have,

1241
01:05:15,440 --> 01:05:18,150
um, a few advanced topics.

1242
01:05:18,290 --> 01:05:21,060
So, just to summarize what we've done

1243
01:05:21,060 --> 01:05:23,290
today is talk a little bit about reinforcement learning,

1244
01:05:23,290 --> 01:05:26,010
how it differs compared to other aspects of AI machine learning.

1245
01:05:26,010 --> 01:05:28,090
We went through course logistics started to talk

1246
01:05:28,090 --> 01:05:30,405
about sequential decision making under uncertainty.

1247
01:05:30,405 --> 01:05:32,595
Just as a quick note for next time, um,

1248
01:05:32,595 --> 01:05:35,955
we will try to post the lecture slides, um,

1249
01:05:35,955 --> 01:05:40,400
two days in advance or by the end of you know the evening of two days in advance,

1250
01:05:40,400 --> 01:05:43,660
so that you can print them out if you want to, um, in class.

1251
01:05:43,660 --> 01:05:46,480
And I'll see you guys on Wednesday.

