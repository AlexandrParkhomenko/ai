1
00:00:05,240 --> 00:00:07,570
So before we get started, I'm just gonna say a

2
00:00:07,570 --> 00:00:10,000
brief note about the logistics for the midterm.

3
00:00:10,000 --> 00:00:11,770
We're gonna be split across two rooms,

4
00:00:11,770 --> 00:00:15,295
the room you're in, it depends on your normal Stanford ID,

5
00:00:15,295 --> 00:00:16,630
whatever the first letter is.

6
00:00:16,630 --> 00:00:19,960
We'll send an email out about this to confirm it.

7
00:00:19,960 --> 00:00:23,435
But we're gonna be in either Gates B1 or Cubberley Auditorium,

8
00:00:23,435 --> 00:00:27,865
and it depends on the first- the first letter of your Stanford ID.

9
00:00:27,865 --> 00:00:30,805
In addition, you're allowed to have one page of notes,

10
00:00:30,805 --> 00:00:33,790
typed or written is fine, one sided.

11
00:00:33,790 --> 00:00:37,100
Anybody have any other questions about the midterm?

12
00:00:38,300 --> 00:00:42,505
Okay, well, reach out to us on Piazza if you have any questions about the midterm.

13
00:00:42,505 --> 00:00:44,470
Um, what we'll do today is we're gonna

14
00:00:44,470 --> 00:00:48,530
split- we're gonna go finish up the rest of policy gradient.

15
00:00:48,530 --> 00:00:51,655
So in terms of where we are in the class, right now.

16
00:00:51,655 --> 00:00:54,260
We are almost done with policy search.

17
00:00:54,260 --> 00:00:55,745
We're gonna have the midterm on Wednesday,

18
00:00:55,745 --> 00:01:00,110
Monday is a holiday and then we'll also be releasing the last homework this week,

19
00:01:00,110 --> 00:01:01,805
which will be over policy search.

20
00:01:01,805 --> 00:01:04,040
And so we're gonna have policy search,

21
00:01:04,040 --> 00:01:05,840
and then we're gonna have the project.

22
00:01:05,840 --> 00:01:09,035
That's the remaining sort of main assignments for the term.

23
00:01:09,035 --> 00:01:11,720
And then we're gonna be getting into fast exploration and sort of

24
00:01:11,720 --> 00:01:14,930
fast reinforcement learning after we come back from the midterm.

25
00:01:14,930 --> 00:01:17,390
So I wanted to make sure to get through policy search

26
00:01:17,390 --> 00:01:20,030
today because you're gonna have the assignment released later this week.

27
00:01:20,030 --> 00:01:23,180
So we'll spend hopefully around like 20 to 25 minutes on policy search,

28
00:01:23,180 --> 00:01:25,400
and then we're gonna do a brief review before we get

29
00:01:25,400 --> 00:01:28,300
into the midterm- before- about the midterm material.

30
00:01:28,300 --> 00:01:30,850
Does anybody have any questions?

31
00:01:30,850 --> 00:01:33,920
Oh, and just a friendly reminder to please say

32
00:01:33,920 --> 00:01:36,370
your name whenever you ask a question because it helps me remember.

33
00:01:36,370 --> 00:01:40,530
It also helps everybody else learn your names as well. All right.

34
00:01:40,530 --> 00:01:42,920
So where we were is that for the last couple of lectures,

35
00:01:42,920 --> 00:01:45,980
we've been starting to talk about policy-based reinforcement learning,

36
00:01:45,980 --> 00:01:48,080
where we're specifically trying to find

37
00:01:48,080 --> 00:01:52,250
a parameterized policy to learn how to make good decisions in the environment.

38
00:01:52,250 --> 00:01:53,900
And so just like what we saw with

39
00:01:53,900 --> 00:01:56,720
value function approximation and what you're doing with Atari,

40
00:01:56,720 --> 00:02:01,010
for our policy parameterization we're gonna assume there's some vector of parameters.

41
00:02:01,010 --> 00:02:06,610
We can represent policies by things like softmax or by a deep neural network.

42
00:02:06,610 --> 00:02:09,770
And then we're gonna wanna be able to take gradients of these types of

43
00:02:09,770 --> 00:02:14,070
policies in order to learn a policy that has a high value.

44
00:02:14,260 --> 00:02:18,880
So we introduced sort of the vanilla policy gradient algorithm,

45
00:02:18,880 --> 00:02:20,620
where the idea is that you start off,

46
00:02:20,620 --> 00:02:22,850
your initialize your policy in some way.

47
00:02:22,850 --> 00:02:25,279
And you also have some baseline,

48
00:02:25,279 --> 00:02:29,300
and then across different iterations you run out your current policy.

49
00:02:29,300 --> 00:02:31,160
And the goal is we're, by running these

50
00:02:31,160 --> 00:02:33,560
out that we're gonna be able to estimate the gradient.

51
00:02:33,560 --> 00:02:40,680
So we're gonna be doing this part to estimate the gradient of our policy at the current, so

52
00:02:40,680 --> 00:02:46,280
we wanna get sort of dV d theta with respect to our current policy.

53
00:02:46,280 --> 00:02:50,550
So what we talked about is that you would run out trajectories from your current policy.

54
00:02:50,550 --> 00:02:52,925
So you'd use your policy to execute in the environment.

55
00:02:52,925 --> 00:02:55,310
You would get state action rewards, next state,

56
00:02:55,310 --> 00:02:57,305
next section, next rewards.

57
00:02:57,305 --> 00:03:01,430
And then you would look at returns and advantage estimates,

58
00:03:01,430 --> 00:03:03,725
which compared your returns to a baseline,

59
00:03:03,725 --> 00:03:05,600
could refit your baseline,

60
00:03:05,600 --> 00:03:08,105
and then you could update your policy.

61
00:03:08,105 --> 00:03:12,260
And so this was sort of the most vanilla policy gradient algorithm we talked about.

62
00:03:12,260 --> 00:03:15,650
And then we started saying that there's a number of different choices um,

63
00:03:15,650 --> 00:03:17,960
that we're making in this algorithm and almost all sort of

64
00:03:17,960 --> 00:03:20,885
policy gradient based algorithms are gonna form uh,

65
00:03:20,885 --> 00:03:22,540
follow this type of formula.

66
00:03:22,540 --> 00:03:26,480
So, in particular, we are making a decision that's

67
00:03:26,480 --> 00:03:30,755
sort of estimating some sort of return or targets.

68
00:03:30,755 --> 00:03:32,540
Often we're picking a baseline,

69
00:03:32,540 --> 00:03:35,480
and then we had to make some decision about after we

70
00:03:35,480 --> 00:03:39,095
compute the gradient how far along the gradient do we go?

71
00:03:39,095 --> 00:03:47,310
So this is sort of helping us to determine how far do we move on our gradient?

72
00:03:55,820 --> 00:03:58,075
Okay, so for the first part,

73
00:03:58,075 --> 00:04:01,360
we talked about how do we estimate sort of the value of where we are right now,

74
00:04:01,360 --> 00:04:05,105
that we're gonna be using to try to estimate our gradient.

75
00:04:05,105 --> 00:04:07,970
And we talked about the fact that the most vanilla thing that we could

76
00:04:07,970 --> 00:04:10,895
do is just roll out the policy and look at the returns,

77
00:04:10,895 --> 00:04:15,390
that this was really similar to what we'd seen in Monte Carlo estimates.

78
00:04:15,470 --> 00:04:17,600
So we could do that,

79
00:04:17,600 --> 00:04:19,880
we could get sort of an estimate of the value function by just

80
00:04:19,880 --> 00:04:22,550
rolling out the policy for one episode,

81
00:04:22,550 --> 00:04:24,170
but that was just like what we saw in

82
00:04:24,170 --> 00:04:28,460
Monte Carlo, an unbiased estimator but high variance.

83
00:04:28,460 --> 00:04:33,320
And so we talked about how we could play all the sorts of- we use all the same tools

84
00:04:33,320 --> 00:04:38,035
as what we've been doing in the past to try to balance between bias and variance.

85
00:04:38,035 --> 00:04:40,460
So in particular, we talked about how we could

86
00:04:40,460 --> 00:04:43,775
introduce bias using bootstrapping and function approximation.

87
00:04:43,775 --> 00:04:46,160
Just like what we saw in TD MC and just

88
00:04:46,160 --> 00:04:49,145
like what we talked about with value function approximation.

89
00:04:49,145 --> 00:04:52,760
So we repeatedly see these same sorts of ideas of the fact that we're trying to

90
00:04:52,760 --> 00:04:56,599
understand what the value is of a particular policy.

91
00:04:56,599 --> 00:04:59,300
And when we're estimating that value,

92
00:04:59,300 --> 00:05:02,960
then we can trade off between getting sort of unbiased estimators of how

93
00:05:02,960 --> 00:05:06,900
good decisions are versus biased estimators um,

94
00:05:06,900 --> 00:05:08,870
that might allow us to propagate information more

95
00:05:08,870 --> 00:05:12,515
quickly and allow us to learn to make better decisions faster.

96
00:05:12,515 --> 00:05:17,030
We also talked about the fact that they're actor-critic methods that both maintain

97
00:05:17,030 --> 00:05:19,250
an explicit parameterized representation of

98
00:05:19,250 --> 00:05:23,525
the policy and a parameterized representation of the value function.

99
00:05:23,525 --> 00:05:27,605
But the thing that we really started getting into last time is to say, "Well,

100
00:05:27,605 --> 00:05:30,710
both you know there are all these sort of existing techniques that we know

101
00:05:30,710 --> 00:05:34,145
of to try to estimate these targets and estimate the value function."

102
00:05:34,145 --> 00:05:39,785
But then there's this additional question of how far do we move along the gradient?

103
00:05:39,785 --> 00:05:43,910
So once we estimate the gradient of the policy,

104
00:05:43,910 --> 00:05:46,340
we need to figure out how far along that gradient do we

105
00:05:46,340 --> 00:05:49,160
go in terms of computing a new policy.

106
00:05:49,160 --> 00:05:51,770
And the reason we argued this was particularly important

107
00:05:51,770 --> 00:05:54,530
in reinforcement learning versus supervised learning,

108
00:05:54,530 --> 00:05:56,375
is that whatever step we take,

109
00:05:56,375 --> 00:05:58,220
whatever new policy we look at,

110
00:05:58,220 --> 00:06:00,550
is gonna determine the data we get next.

111
00:06:00,550 --> 00:06:03,500
And so it was a particularly important for us to think about how

112
00:06:03,500 --> 00:06:07,415
far along do we wanna go on our gradient to get a new policy.

113
00:06:07,415 --> 00:06:11,570
And one desirable property we were talking about is,

114
00:06:11,570 --> 00:06:14,080
how do we ensure monotonic improvement?

115
00:06:14,080 --> 00:06:16,130
So what we would like here is we'd really like

116
00:06:16,130 --> 00:06:25,265
monotonic improvement, that's our goal.

117
00:06:25,265 --> 00:06:28,430
And we talked about wanting monotonic improvement,

118
00:06:28,430 --> 00:06:32,615
which was not guaranteed in DQN or a lot of other algorithms.

119
00:06:32,615 --> 00:06:34,699
Because in a lot of high stakes domains,

120
00:06:34,699 --> 00:06:37,935
like finance, customers, patients,

121
00:06:37,935 --> 00:06:41,020
you might really wanna ensure that the new policy you're deploying is

122
00:06:41,020 --> 00:06:45,220
expected to be better at least in expectation before you deploy it.

123
00:06:45,220 --> 00:06:47,710
So we talked about wanting this,

124
00:06:47,710 --> 00:06:49,580
but there's this big problem that we don't have

125
00:06:49,580 --> 00:06:51,920
data from the new policies that we're considering.

126
00:06:51,920 --> 00:06:54,995
And we don't wanna try out all the possible next policies

127
00:06:54,995 --> 00:06:56,465
because some of them might be bad.

128
00:06:56,465 --> 00:06:59,810
And so we wanted to try to use our existing data to figure out how do we

129
00:06:59,810 --> 00:07:04,260
take a step and determine a new policy that we think is gonna be good.

130
00:07:04,970 --> 00:07:10,280
So, in particular, our main goal for policy gradients is to try

131
00:07:10,280 --> 00:07:15,050
to find a set of policy parameters that maximize our value function.

132
00:07:15,050 --> 00:07:18,080
And the challenge is that we currently have access to

133
00:07:18,080 --> 00:07:20,825
data that was gathered with our current policy,

134
00:07:20,825 --> 00:07:23,640
which we're gonna call pi old.

135
00:07:26,310 --> 00:07:29,905
That's parameterized by a set of thetas,

136
00:07:29,905 --> 00:07:32,860
that we can also denote by theta old.

137
00:07:32,860 --> 00:07:34,945
And- and throughout policy,

138
00:07:34,945 --> 00:07:37,060
the policy gradient lectures that I've been through,

139
00:07:37,060 --> 00:07:40,465
going back and forth between talking about policies and talking about thetas.

140
00:07:40,465 --> 00:07:42,760
But it's good just to remember that there's, sort of,

141
00:07:42,760 --> 00:07:45,145
this direct mapping between pi and theta.

142
00:07:45,145 --> 00:07:47,770
You know there's- for each- for a policy,

143
00:07:47,770 --> 00:07:50,680
it's exactly defined by a set of parameters.

144
00:07:50,680 --> 00:07:56,860
[NOISE] So whether we're talking about policies,

145
00:07:56,860 --> 00:07:58,570
or we're talking about parameters,

146
00:07:58,570 --> 00:08:01,120
those are referring to exactly the same thing.

147
00:08:01,120 --> 00:08:05,380
Um, so the challenge is- is that we have data from our current policy,

148
00:08:05,380 --> 00:08:07,420
um, which has some set of parameters,

149
00:08:07,420 --> 00:08:09,970
and we want to predict the value of a different policy.

150
00:08:09,970 --> 00:08:13,070
And so this is a challenge with off policy learning.

151
00:08:14,700 --> 00:08:18,490
So what we tal- were talking about last time, um,

152
00:08:18,490 --> 00:08:22,945
is how do we express the value of a policy in terms of stuff that we do know?

153
00:08:22,945 --> 00:08:25,060
Um, and we talked about how we could write it down in

154
00:08:25,060 --> 00:08:27,865
terms of an advantage over the current policy.

155
00:08:27,865 --> 00:08:31,570
So if we think about a value being parameterized by

156
00:08:31,570 --> 00:08:35,950
a new set- a new policy with a new set of theta tilde parameters,

157
00:08:35,950 --> 00:08:40,090
it's equal to the value of another policy parameterized

158
00:08:40,090 --> 00:08:44,320
by a set of Theta plus the expected advantage.

159
00:08:44,320 --> 00:08:49,930
So we can write that as the distribution of states that we would expect to get to under

160
00:08:49,930 --> 00:08:53,830
the new policy times the advantage we

161
00:08:53,830 --> 00:08:58,790
would get under the old policy if we were to follow the new policy.

162
00:09:00,930 --> 00:09:04,540
And the reason- what- what we're trying to do in this case is just to- to

163
00:09:04,540 --> 00:09:07,255
keep us thinking about what the main goal is here.

164
00:09:07,255 --> 00:09:09,190
Is what we're trying to do is figure out a way to do

165
00:09:09,190 --> 00:09:12,505
policy gradient where we're guaranteed to have monotonic improvement,

166
00:09:12,505 --> 00:09:16,480
where our new policy is gonna be guaranteed to be better than our old policy.

167
00:09:16,480 --> 00:09:19,960
But we wanna do this without actually trying out our new policy.

168
00:09:19,960 --> 00:09:23,080
So we are trying to re-express what the value is of

169
00:09:23,080 --> 00:09:26,395
a new policy in terms of quantities we have access to.

170
00:09:26,395 --> 00:09:27,880
So what do we have access to?

171
00:09:27,880 --> 00:09:31,195
We have access to existing samples from the current policy,

172
00:09:31,195 --> 00:09:34,780
and we wanna use those and the returns we've observed in order to

173
00:09:34,780 --> 00:09:38,935
estimate the value of a new policy before we deploy it.

174
00:09:38,935 --> 00:09:41,365
So that's kinda where we're trying to get to.

175
00:09:41,365 --> 00:09:43,600
And we noticed here that maybe, you know,

176
00:09:43,600 --> 00:09:46,840
we can have access to an explicit form of a new policy,

177
00:09:46,840 --> 00:09:50,650
that's like whatever new parameters we're considering putting into our neural network.

178
00:09:50,650 --> 00:09:54,624
Um, and we could imagine estimating the advantage function,

179
00:09:54,624 --> 00:09:58,675
but we don't know the state distribution under the new policy,

180
00:09:58,675 --> 00:10:02,060
because that would require us actually to run it.

181
00:10:03,270 --> 00:10:08,210
So what we talked about is- let's just define a new objective function,

182
00:10:08,210 --> 00:10:11,320
um, which is just a different objective function.

183
00:10:11,320 --> 00:10:12,490
Might be good, might be bad.

184
00:10:12,490 --> 00:10:14,800
I'm going to argue to it's a good- it's good but this right

185
00:10:14,800 --> 00:10:17,440
now is just a quantity that we can optimize.

186
00:10:17,440 --> 00:10:20,215
So the quantity that we can optimize here,

187
00:10:20,215 --> 00:10:23,035
we're gonna call that sort of this new objective function,

188
00:10:23,035 --> 00:10:26,410
and it is going to be a function of the previous value.

189
00:10:26,410 --> 00:10:33,040
So here, remember this is always equal to [NOISE] direct mapping between thetas and pis.

190
00:10:33,040 --> 00:10:35,035
So we're going to just say,

191
00:10:35,035 --> 00:10:37,480
it looks like the objective function we just talked about,

192
00:10:37,480 --> 00:10:39,775
which really was the value of the new policy,

193
00:10:39,775 --> 00:10:43,060
but we don't know what that stationary weighted distribution is,

194
00:10:43,060 --> 00:10:45,025
of states under the new policy.

195
00:10:45,025 --> 00:10:50,660
So we're just gonna substitute in the stationary distribution under the current policy.

196
00:10:50,760 --> 00:10:54,925
Now in general, this is not gonna be- so

197
00:10:54,925 --> 00:11:00,505
it's not going to be equal to your new policy distribution.

198
00:11:00,505 --> 00:11:02,650
The only time you're gonna get

199
00:11:02,650 --> 00:11:07,885
the same state distribution under two policies is generally if they're identical.

200
00:11:07,885 --> 00:11:12,490
Occasionally, you can get the same state distribution under two different policies,

201
00:11:12,490 --> 00:11:14,230
but then that means they have the same value.

202
00:11:14,230 --> 00:11:16,300
So in general, we're going to expect that this is going to be

203
00:11:16,300 --> 00:11:18,550
different but we're going to ignore that for now.

204
00:11:18,550 --> 00:11:20,200
We're just going to say this is an objective function,

205
00:11:20,200 --> 00:11:22,255
this is something we can optimize.

206
00:11:22,255 --> 00:11:26,320
And a nice thing about this is that we have samples from the current policy.

207
00:11:26,320 --> 00:11:31,790
So we can imagine just using those samples to estimate this expectation.

208
00:11:31,860 --> 00:11:35,590
The thing that I also want us to note here is that,

209
00:11:35,590 --> 00:11:37,360
this is just this new objective function called

210
00:11:37,360 --> 00:11:41,515
L. If you evaluate the objective function L,

211
00:11:41,515 --> 00:11:43,420
um, at the current policy,

212
00:11:43,420 --> 00:11:48,270
so if you plug in your old policy into your objective function,

213
00:11:48,270 --> 00:11:51,600
it's exactly equal to the value of your current policy.

214
00:11:51,600 --> 00:11:53,940
So this second term becomes 0,

215
00:11:53,940 --> 00:12:02,210
[NOISE] because the advantage of the existing policy over the existing policy is 0.

216
00:12:02,210 --> 00:12:07,230
So this objective function is exactly equal to the value of the old policy,

217
00:12:07,230 --> 00:12:09,255
if you evaluated the old policy,

218
00:12:09,255 --> 00:12:13,775
and another case is for new policies it's gonna be something different. Yes.

219
00:12:13,775 --> 00:12:16,990
How's this similar to importance sampling?

220
00:12:16,990 --> 00:12:18,610
That's a great question.

221
00:12:18,610 --> 00:12:21,070
You asked, "How is this similar to importance sampling?"

222
00:12:21,070 --> 00:12:23,540
Um, if we were gonna do importance- Well,

223
00:12:23,540 --> 00:12:25,185
it's different in a number of ways.

224
00:12:25,185 --> 00:12:27,270
Um, in importance sampling,

225
00:12:27,270 --> 00:12:29,130
what we tend to do is we re-weight,

226
00:12:29,130 --> 00:12:34,035
um, uh, the distribution that we want by the distribution that we have.

227
00:12:34,035 --> 00:12:36,180
Um, in this case we're looking,

228
00:12:36,180 --> 00:12:38,415
and we normally do that on a per state level.

229
00:12:38,415 --> 00:12:43,095
In this case we are looking at the stationary distribution over states.

230
00:12:43,095 --> 00:12:46,575
It's actually a really cool paper that just came out in NeurIPS,

231
00:12:46,575 --> 00:12:48,480
like a month ago, um,

232
00:12:48,480 --> 00:12:54,655
[NOISE] 2018, with Lihong Li and some other colleagues.

233
00:12:54,655 --> 00:12:56,680
Um, they looked at,

234
00:12:56,680 --> 00:12:57,880
how would you re-weight

235
00:12:57,880 --> 00:13:02,665
stationary distributions to try to get off policy estimates of the value function?

236
00:13:02,665 --> 00:13:05,530
Um, and so to try to directly re-weight what,

237
00:13:05,530 --> 00:13:08,455
like, mu pi would be versus mu pi tilde.

238
00:13:08,455 --> 00:13:10,600
So we're not doing that here,

239
00:13:10,600 --> 00:13:12,820
there's some really nice ideas in that that could help

240
00:13:12,820 --> 00:13:15,115
really reduce the variance in long horizon problems.

241
00:13:15,115 --> 00:13:16,900
Um, in this case, we're just substituting,

242
00:13:16,900 --> 00:13:18,055
so we're ignoring the difference.

243
00:13:18,055 --> 00:13:19,270
We're not doing importance sampling,

244
00:13:19,270 --> 00:13:24,295
we're just pretending that the distribution of states that we get to is exactly the same.

245
00:13:24,295 --> 00:13:28,060
It's not, but we're gonna show that this is going to end up being a useful

246
00:13:28,060 --> 00:13:32,300
lower bound to what we wanna- what we actually want to optimize.

247
00:13:33,810 --> 00:13:36,805
Okay. So you might say,

248
00:13:36,805 --> 00:13:40,450
if you take this objective function which might be good, might not be good.

249
00:13:40,450 --> 00:13:42,280
If we optimize with respect to it,

250
00:13:42,280 --> 00:13:44,080
do we have any guarantees on whether

251
00:13:44,080 --> 00:13:46,570
the new value function that we get if we optimize with

252
00:13:46,570 --> 00:13:50,890
respect to this wrong objective function is better than the old value function?

253
00:13:50,890 --> 00:13:52,315
Because remember that's where we're trying to go to.

254
00:13:52,315 --> 00:13:54,130
We don't really care what we're optimizing,

255
00:13:54,130 --> 00:13:56,680
what we care about is that the resulting value function we

256
00:13:56,680 --> 00:13:59,290
get out is actually better than the old value function.

257
00:13:59,290 --> 00:14:02,470
[NOISE] So last time I said that if you have

258
00:14:02,470 --> 00:14:06,685
a mixture policy which blend between your current policy and a new policy,

259
00:14:06,685 --> 00:14:10,930
so let's say you have a pi old and you have some other policy,

260
00:14:10,930 --> 00:14:12,760
I haven't said how you get it,

261
00:14:12,760 --> 00:14:14,530
but just say you have some other policy,

262
00:14:14,530 --> 00:14:16,120
and that defines your new policy.

263
00:14:16,120 --> 00:14:17,875
So with probability 1 - alpha,

264
00:14:17,875 --> 00:14:19,600
you take the same action as you used to.

265
00:14:19,600 --> 00:14:21,865
With probability alpha, you take a new action.

266
00:14:21,865 --> 00:14:27,535
In this case you can guarantee a lower bound on the value of the new policy.

267
00:14:27,535 --> 00:14:30,490
So the value of the new policy is greater than or

268
00:14:30,490 --> 00:14:33,400
equal to this objective function we have here,

269
00:14:33,400 --> 00:14:37,010
minus this particular quantity.

270
00:14:37,320 --> 00:14:43,600
So that says that if you optimize with respect to this weird L objective function,

271
00:14:43,600 --> 00:14:46,960
you can actually get bounds on how good your new policy is.

272
00:14:46,960 --> 00:14:48,685
So that seems promising,

273
00:14:48,685 --> 00:14:51,880
but in general we're not going to want to just consider mixture policies.

274
00:14:51,880 --> 00:14:56,440
Okay. So what this theorem says is that,

275
00:14:56,440 --> 00:14:58,750
for any stochastic policy,

276
00:14:58,750 --> 00:15:00,655
not just this weird mixture,

277
00:15:00,655 --> 00:15:05,880
you can get a bound on the performance by using this slightly strange objective function.

278
00:15:05,880 --> 00:15:07,630
So in particular, um,

279
00:15:07,630 --> 00:15:10,685
define the distance of total variation as follows.

280
00:15:10,685 --> 00:15:14,000
So DTV between two policies,

281
00:15:14,000 --> 00:15:17,615
so I'm using a dot there to denote that there's,

282
00:15:17,615 --> 00:15:20,930
um, there's a number of different actions.

283
00:15:20,930 --> 00:15:25,610
The- the policies there are denoting a probability distribution over actions.

284
00:15:25,610 --> 00:15:28,310
This is equal to the max over all a,

285
00:15:28,310 --> 00:15:35,970
the distance between the probability that each of the two policies put on that action.

286
00:15:36,900 --> 00:15:40,240
So it's giving us sort of a maximum difference,

287
00:15:40,240 --> 00:15:44,380
in what's the probability of an action under one policy versus the other policy.

288
00:15:44,380 --> 00:15:46,720
And then we can do- [NOISE] Bless you.

289
00:15:46,720 --> 00:15:52,720
-we can do D max of total variation by taking the max of that quantity over all states.

290
00:15:52,720 --> 00:15:54,265
So it's essentially saying,

291
00:15:54,265 --> 00:15:56,530
over all states was the biggest difference that

292
00:15:56,530 --> 00:15:59,350
the two policies give over a particular action.

293
00:15:59,350 --> 00:16:01,615
So where do they most differ?

294
00:16:01,615 --> 00:16:03,760
And then what this theorem says is that,

295
00:16:03,760 --> 00:16:05,439
if you have that quantity,

296
00:16:05,439 --> 00:16:07,675
in general we're not gonna be able to evaluate that.

297
00:16:07,675 --> 00:16:08,920
But what that's saying is that,

298
00:16:08,920 --> 00:16:11,095
if you know what that quantity is,

299
00:16:11,095 --> 00:16:12,235
then you can define that.

300
00:16:12,235 --> 00:16:15,460
If you use this objective function L,

301
00:16:15,460 --> 00:16:18,855
that the new value of your policy is

302
00:16:18,855 --> 00:16:22,270
at least the objective function you compute minus this quantity,

303
00:16:22,270 --> 00:16:25,240
it's a function of the distance of total variation,

304
00:16:25,240 --> 00:16:27,840
the max distance and total variation.

305
00:16:27,840 --> 00:16:30,800
So this gives us some confidence that if we were to

306
00:16:30,800 --> 00:16:33,680
optimize with respect to the objective function L,

307
00:16:33,680 --> 00:16:36,900
then we can get a bound on the value function.

308
00:16:37,060 --> 00:16:40,160
Now this distance- this max or

309
00:16:40,160 --> 00:16:43,085
the total variation distance isn't particularly easy to work with.

310
00:16:43,085 --> 00:16:48,559
So we can use the fact that the square of it is upper bounded by the KL divergence,

311
00:16:48,559 --> 00:16:51,860
and then get a new bound which is a little bit easier to work with.

312
00:16:51,860 --> 00:16:55,790
That looks at the KL divergence between the two policies,

313
00:16:55,790 --> 00:16:58,500
and we again get the similar bound.

314
00:16:59,670 --> 00:17:01,930
Okay. So why is this useful?

315
00:17:01,930 --> 00:17:04,490
So what I've told you right now is that we have this new objective function.

316
00:17:04,490 --> 00:17:06,335
If we use this new objective function,

317
00:17:06,335 --> 00:17:11,030
we could- in principle get this lower bound on the performance of the new policy.

318
00:17:11,030 --> 00:17:15,895
So how do we use this to ensure that we wanna get monotonic improvement?

319
00:17:15,895 --> 00:17:19,790
So the goal is monotonic improvement.

320
00:17:19,790 --> 00:17:27,355
We want to have the V_ Pi i + 1 is greater than or equal to V_ Pi i. That is our goal.

321
00:17:27,355 --> 00:17:28,454
So i is iterations,

322
00:17:28,454 --> 00:17:31,040
we want that the new policy that we deploy is actually better than

323
00:17:31,040 --> 00:17:33,770
the policy we had before. So how are we gonna do this?

324
00:17:33,770 --> 00:17:35,270
So what we're gonna say is,

325
00:17:35,270 --> 00:17:37,355
first we have this objective function here,

326
00:17:37,355 --> 00:17:39,020
this lower bound objective function,

327
00:17:39,020 --> 00:17:44,705
[NOISE] and what we're going to define is that Mi of pi i

328
00:17:44,705 --> 00:17:51,560
is equal to L of pi i pi.

329
00:17:51,560 --> 00:17:57,590
So I'm just copying the equation from the previous, um, previous slide,

330
00:17:57,590 --> 00:18:09,570
- 4 epsilon gamma divided by 1 - gamma squared DKL_ max of pi i.

331
00:18:09,900 --> 00:18:12,110
Okay. So this is the lower bound.

332
00:18:12,110 --> 00:18:14,375
That's what we just defined on the previous slide.

333
00:18:14,375 --> 00:18:18,180
Okay? So what we said here is that,

334
00:18:18,180 --> 00:18:23,340
the value of our new policy- so this is equal to this M function I've defined.

335
00:18:23,340 --> 00:18:28,480
So we've said that the new value is gonna be at least as good as this lower bound.

336
00:18:28,480 --> 00:18:38,140
So we're gonna say V_i + 1 is gonna be equal to Mi of pi i + 1,

337
00:18:38,140 --> 00:18:42,790
which is equal to L pi i + 1.

338
00:18:42,790 --> 00:18:46,250
I'm just writing out what the definition is here.

339
00:18:49,880 --> 00:18:53,280
And again, what we're trying to do here is get to the point where we're

340
00:18:53,280 --> 00:18:57,705
confident that we can get something that's better than our old value function.

341
00:18:57,705 --> 00:19:00,780
Now, the thing that I want to now look at is, well,

342
00:19:00,780 --> 00:19:05,700
what is- if we were to evaluate the lower bound at the current policy what would that be?

343
00:19:05,700 --> 00:19:08,805
So let's look at Mi of pi_i.

344
00:19:08,805 --> 00:19:12,450
So that's going to be equal to L pi_i of pi_i.

345
00:19:12,450 --> 00:19:14,835
I'm just plugging it into this equation up there,

346
00:19:14,835 --> 00:19:24,240
- 4 epsilon gamma - gamma squared DKL max of pi_i, pi_i.

347
00:19:24,240 --> 00:19:28,530
Okay, so why is this nice?

348
00:19:28,530 --> 00:19:29,580
Well, this is nice because

349
00:19:29,580 --> 00:19:33,540
the KL diver- divergence between two identical policies is 0.

350
00:19:33,540 --> 00:19:35,415
So these are exactly the same.

351
00:19:35,415 --> 00:19:38,100
This is equal to 0. So now,

352
00:19:38,100 --> 00:19:41,670
this is just equal to L pi_i of pi_i.

353
00:19:41,670 --> 00:19:44,640
But what I told you before is that if we go back

354
00:19:44,640 --> 00:19:48,375
a few slides to what the definition is of pi_i,

355
00:19:48,375 --> 00:19:51,090
L of pi_i is that if you evaluate it at

356
00:19:51,090 --> 00:19:59,325
the current policy it's just equal to the value of that policy, okay?

357
00:19:59,325 --> 00:20:03,210
So if we evaluate this objective function at the current policy,

358
00:20:03,210 --> 00:20:06,015
it's just the same as the value of the current policy.

359
00:20:06,015 --> 00:20:11,320
So now, if we go back here this is just equal to V of pi_i.

360
00:20:11,690 --> 00:20:13,785
Okay. So what does this say?

361
00:20:13,785 --> 00:20:18,045
This says, that if I wanna look at how my I- my-

362
00:20:18,045 --> 00:20:23,264
the value of my i + 1 policy looks compared to the value of my old policy,

363
00:20:23,264 --> 00:20:28,690
we know that's greater than or equal to Mi of pi_i + 1.

364
00:20:28,910 --> 00:20:33,765
So because we said that the V that we knew from this theorem,

365
00:20:33,765 --> 00:20:35,520
that the new value of the policy is greater

366
00:20:35,520 --> 00:20:37,380
than or equal to this lower bound we computed.

367
00:20:37,380 --> 00:20:43,270
So it's greater than or equal to Mi of pi i + 1 - Mi of pi_i.

368
00:20:44,840 --> 00:20:46,950
So what does this say?

369
00:20:46,950 --> 00:20:48,165
This says that if

370
00:20:48,165 --> 00:20:52,020
your new value function has a better lower bound than your old value function,

371
00:20:52,020 --> 00:20:54,120
you have monotonic improvement.

372
00:20:54,120 --> 00:20:59,680
So if this is greater than 0, then monotonic improvement,

373
00:21:01,430 --> 00:21:06,750
which means that if you optimize with respect to this lower bound and

374
00:21:06,750 --> 00:21:08,910
you can evaluate that quantity and your new

375
00:21:08,910 --> 00:21:11,595
lower bound is higher than your old lower bound,

376
00:21:11,595 --> 00:21:14,725
then your value has to be better.

377
00:21:14,725 --> 00:21:18,485
So we can guarantee monotonic improvement. Yes.

378
00:21:18,485 --> 00:21:21,845
So just to clarify. So for these value comparisons,

379
00:21:21,845 --> 00:21:24,455
are we implicitly considering as an infinity norm,

380
00:21:24,455 --> 00:21:26,485
in terms of saying one is better?

381
00:21:26,485 --> 00:21:28,950
Yes, generally. Yeah, I think, I mean,

382
00:21:28,950 --> 00:21:30,780
they probably go through with L squared 2.

383
00:21:30,780 --> 00:21:32,535
But yeah, yeah.

384
00:21:32,535 --> 00:21:35,010
Um, question is whether or not we're always defining

385
00:21:35,010 --> 00:21:37,560
this with respect to L infinity norm almost always.

386
00:21:37,560 --> 00:21:40,290
Um, ah, there certainly is some analysis particularly when we

387
00:21:40,290 --> 00:21:43,215
get into a function approximation which looks at an L2 norm.

388
00:21:43,215 --> 00:21:46,530
Um, but most of this is all with respect to an L infinity norm,

389
00:21:46,530 --> 00:21:48,990
which means that when we're looking at this,

390
00:21:48,990 --> 00:21:50,550
for example, we're looking at, um,

391
00:21:50,550 --> 00:21:52,965
ensuring that for all states, um,

392
00:21:52,965 --> 00:21:59,325
the value of those states is at least as good as the previous value of the states. Yes.

393
00:21:59,325 --> 00:22:02,670
So the- the claim was if our lower bound improves,

394
00:22:02,670 --> 00:22:06,690
then it must be the case that what is the lower bound must also be improving, right?

395
00:22:06,690 --> 00:22:07,125
Yeah.

396
00:22:07,125 --> 00:22:09,150
Uh, so there's never a case where, for example,

397
00:22:09,150 --> 00:22:12,000
your lower bound might improve even though the actual value of the policy

398
00:22:12,000 --> 00:22:15,375
evaluated decreases, it seems like.

399
00:22:15,375 --> 00:22:18,765
That's right. So what this- this is- what this

400
00:22:18,765 --> 00:22:22,230
is asking us if you know this lower bound and what that relates to the actual value.

401
00:22:22,230 --> 00:22:24,780
What this is stating is that if you improve

402
00:22:24,780 --> 00:22:28,320
your two low- like if you have a lower bound of your existing policy and you get a new

403
00:22:28,320 --> 00:22:31,320
lower bound with for some new policy and that new lower bound is

404
00:22:31,320 --> 00:22:35,710
higher than your lower bound of the other one, that you're guaranteed to be improving.

405
00:22:35,750 --> 00:22:39,900
So this is what guarantee- so this is assuming you can solve this.

406
00:22:39,900 --> 00:22:41,550
But if you, um,

407
00:22:41,550 --> 00:22:46,845
if you can get this lower- if you optimize with respect to this lower-bound quantity,

408
00:22:46,845 --> 00:22:51,645
um, because when you plug in the lower bound for the current policy under that,

409
00:22:51,645 --> 00:22:53,460
that's exactly equal to th- the value of

410
00:22:53,460 --> 00:22:56,220
that policy then you are guaranteed to be improving.

411
00:22:56,220 --> 00:22:59,775
Because you're basically saying, here's my lower- here's my existing value.

412
00:22:59,775 --> 00:23:02,880
I have something whose lower bound is better than my existing value.

413
00:23:02,880 --> 00:23:07,545
And so I know my new thing has to be better. Okay. Yeah.

414
00:23:07,545 --> 00:23:12,350
How do you like, um, you get the epsilon term back because it seems

415
00:23:12,350 --> 00:23:16,925
like the epsilon changes depending on your pi and it's also like a global property.

416
00:23:16,925 --> 00:23:19,115
Absolutely. So, um, now this discussion is a great one.

417
00:23:19,115 --> 00:23:20,820
I- few might ask us about this.

418
00:23:20,820 --> 00:23:22,950
Um, ah, so note this,

419
00:23:22,950 --> 00:23:24,975
that your lower bound is in terms of epsilon.

420
00:23:24,975 --> 00:23:28,785
Epsilon is a max over all states and actions of your advantage.

421
00:23:28,785 --> 00:23:32,190
Um, in principle, you could

422
00:23:32,190 --> 00:23:35,550
evaluate this [LAUGHTER] particularly if you're at a discrete state and action space.

423
00:23:35,550 --> 00:23:38,910
I- in practice, that's something that you would not wanna do.

424
00:23:38,910 --> 00:23:41,715
Um, this, I view this part as sort of

425
00:23:41,715 --> 00:23:44,460
saying this is formally if you could evaluate this lower bound.

426
00:23:44,460 --> 00:23:46,650
Um, and what we're gonna do now is talk about, um,

427
00:23:46,650 --> 00:23:49,605
a more practical algorithm which tries to take,

428
00:23:49,605 --> 00:23:54,030
um, this guarantee of conservative policy improvement and actually make it practical,

429
00:23:54,030 --> 00:23:57,405
in terms of quantities that are a little bit easier to compute.

430
00:23:57,405 --> 00:23:59,595
Because that's right. Yeah, in general,

431
00:23:59,595 --> 00:24:02,190
it- it would be very hard to evaluate this this epsi- uh, this epsilon.

432
00:24:02,190 --> 00:24:04,065
Now you could take upper or lower bounds on it.

433
00:24:04,065 --> 00:24:07,245
Um, but you won't generally know what this epsilon is.

434
00:24:07,245 --> 00:24:09,360
And note that this, uh, as co- just pointing out,

435
00:24:09,360 --> 00:24:11,775
this epsilon is dependent on the policy.

436
00:24:11,775 --> 00:24:16,800
Um, so- okay. All right. But this is pretty cool.

437
00:24:16,800 --> 00:24:18,720
So it means that you can do this guaranteed improvement.

438
00:24:18,720 --> 00:24:21,630
This is a form of mineral- minimization maximization.

439
00:24:21,630 --> 00:24:24,480
Um, and it's this nice idea of sort of saying you can have this new

440
00:24:24,480 --> 00:24:27,780
lower bound that's guaranteed to be better than the value of your current policy.

441
00:24:27,780 --> 00:24:31,750
So you can get this sort of conservative monotonic improving policy.

442
00:24:32,090 --> 00:24:35,310
All right. So I

443
00:24:35,310 --> 00:24:37,920
just- I wanna make sure we have enough time to go through some of the midterm review.

444
00:24:37,920 --> 00:24:41,280
But I wanna briefly talk about how we would make this practical.

445
00:24:41,280 --> 00:24:43,410
Particularly because, um, trust

446
00:24:43,410 --> 00:24:47,985
region policy optimization is an extremely popular policy gradient algorithm.

447
00:24:47,985 --> 00:24:50,130
So I think it's useful for you guys just to be aware of.

448
00:24:50,130 --> 00:24:53,190
Um, you- some of you might use it in some of your projects.

449
00:24:53,190 --> 00:24:56,850
Um, we won't cover it in uh- won't be a mandatory part of the homework,

450
00:24:56,850 --> 00:24:58,140
um, or on the midterm.

451
00:24:58,140 --> 00:25:01,080
But I think it's just a useful idea to be familiar with.

452
00:25:01,080 --> 00:25:05,700
So again, if we look at sort of what this objective function was that we just discussed.

453
00:25:05,700 --> 00:25:09,390
We said we had this L function and then we turned it into a lower bound

454
00:25:09,390 --> 00:25:13,470
by subtracting off this constant that might be hard for us to compute.

455
00:25:13,470 --> 00:25:16,290
And so what we do in this case is we take this constant

456
00:25:16,290 --> 00:25:19,710
here and we turned it into a hyperparameter.

457
00:25:19,710 --> 00:25:23,460
So you could turn it into a constant C. Um,

458
00:25:23,460 --> 00:25:26,745
but the problem wi- always is that even if you could compute this,

459
00:25:26,745 --> 00:25:28,170
um, often we don't know what this is.

460
00:25:28,170 --> 00:25:31,200
But even if you could compute it or compute a bound on it, um,

461
00:25:31,200 --> 00:25:33,135
generally, if we use this,

462
00:25:33,135 --> 00:25:35,790
we would take very small step sizes.

463
00:25:35,790 --> 00:25:39,495
So intuitively, this is because, um, you know,

464
00:25:39,495 --> 00:25:42,375
it's often very hard to extrapolate far away,

465
00:25:42,375 --> 00:25:44,250
um, from your current policy.

466
00:25:44,250 --> 00:25:46,530
And so this would say if you wanna be really sure that

467
00:25:46,530 --> 00:25:48,900
your new value is better than your old value,

468
00:25:48,900 --> 00:25:51,195
then just take a very small step size.

469
00:25:51,195 --> 00:25:54,450
And intuitively, it's because if you change your policy very,

470
00:25:54,450 --> 00:25:57,270
very small amounts at least under some smoothness guarantees,

471
00:25:57,270 --> 00:26:00,165
um, the value of your policy can't change that much.

472
00:26:00,165 --> 00:26:02,490
You know, it should also be intuitive that, like,

473
00:26:02,490 --> 00:26:03,780
your gradient is often

474
00:26:03,780 --> 00:26:07,320
a pretty good estimate very close to your current value of your function.

475
00:26:07,320 --> 00:26:10,020
But we also need to quickly try to get

476
00:26:10,020 --> 00:26:12,390
to a good policy so this is not generally practical.

477
00:26:12,390 --> 00:26:15,960
Um, and so the idea of sort of TRPO,

478
00:26:15,960 --> 00:26:19,080
one of the main ideas is to think of it being kind of a trusted region.

479
00:26:19,080 --> 00:26:22,575
Um, and- and use this to constrain our step sizes.

480
00:26:22,575 --> 00:26:25,995
So again, if we go back to this sort of the gene- generic, um, ah,

481
00:26:25,995 --> 00:26:27,990
template for policy gradient algorithms,

482
00:26:27,990 --> 00:26:31,455
we have to make this choice of how far out to step in our gradient.

483
00:26:31,455 --> 00:26:34,635
Um, and the idea is we're going to sort of define a constraint.

484
00:26:34,635 --> 00:26:37,170
So we're going to have our objective function here.

485
00:26:37,170 --> 00:26:40,110
And instead of explicitly subtracting off our lower bound,

486
00:26:40,110 --> 00:26:42,345
we're just going to say you could move.

487
00:26:42,345 --> 00:26:44,235
You can change your gradient but not too far.

488
00:26:44,235 --> 00:26:46,620
We're gonna put, uh, a constraint on how far

489
00:26:46,620 --> 00:26:50,025
the KL divergence can be as a way to just sort of say

490
00:26:50,025 --> 00:26:52,770
you're kind of having this region of which in your- in

491
00:26:52,770 --> 00:26:56,595
your parameter space that allows you to know how far you can change your policy.

492
00:26:56,595 --> 00:26:59,640
Okay. Yeah. All right.

493
00:26:59,640 --> 00:27:03,870
So I'm just going to talk very briefly about how this is instantiated.

494
00:27:03,870 --> 00:27:08,100
Um, so the main idea is that if we look at, um,

495
00:27:08,100 --> 00:27:10,020
what these objective functions are,

496
00:27:10,020 --> 00:27:13,095
um, this may or may not be easy for us to evaluate.

497
00:27:13,095 --> 00:27:15,090
So if we look back at what,

498
00:27:15,090 --> 00:27:17,730
um, our theta is, um, even here, right,

499
00:27:17,730 --> 00:27:22,065
we have sort of our discounted visitation weights under the current policy,

500
00:27:22,065 --> 00:27:24,080
but we don't have direct access to that.

501
00:27:24,080 --> 00:27:27,410
We have only access to samples from rolling out our current policy.

502
00:27:27,410 --> 00:27:31,700
So the first idea is that instead of taking an explicit sum over the state space,

503
00:27:31,700 --> 00:27:33,485
where that state space might be, you know,

504
00:27:33,485 --> 00:27:36,775
continuous and in-infinite, we're just going to look at

505
00:27:36,775 --> 00:27:41,385
the states that were actually sampled by our current old policy and re-weight them.

506
00:27:41,385 --> 00:27:44,820
So that's the first depa- the first substitution we do.

507
00:27:44,820 --> 00:27:46,830
Yeah. What we're trying to do right now is say we have

508
00:27:46,830 --> 00:27:49,860
this objective function and we wanna make it so that this can be part of

509
00:27:49,860 --> 00:27:52,890
an algorithm where we can compute all the quantities we need to in order

510
00:27:52,890 --> 00:27:56,760
to take a step size where we think the new policy is gonna be better.

511
00:27:56,760 --> 00:28:00,435
Um, the second thing we do and this relates to,

512
00:28:00,435 --> 00:28:02,385
um, question about importance sampling.

513
00:28:02,385 --> 00:28:06,390
Um, I- is we have this second quantity in here,

514
00:28:06,390 --> 00:28:11,040
where this is the probability of an action under our new policy.

515
00:28:11,040 --> 00:28:14,370
Um, we do have access to that, in the sense that,

516
00:28:14,370 --> 00:28:16,545
if someone gives us a state we can tell, um,

517
00:28:16,545 --> 00:28:19,770
we can say exactly what our probability would be under all the actions.

518
00:28:19,770 --> 00:28:24,490
But again, this often can be a continuous set.

519
00:28:27,210 --> 00:28:31,780
And so instead of doing sort of this continuous set, we are just going to

520
00:28:31,780 --> 00:28:35,590
say we're gonna use importance sampling and we can take samples.

521
00:28:35,590 --> 00:28:38,140
This is typically goi- going to be from pi old.

522
00:28:38,140 --> 00:28:41,890
So we look at what times we have taken an action given our current policy and we

523
00:28:41,890 --> 00:28:43,870
re-weight them according to the probability

524
00:28:43,870 --> 00:28:46,420
we would have taken those actions that drive the new policy.

525
00:28:46,420 --> 00:28:51,320
So it allows us to approximate that expectation using data that we have.

526
00:28:52,320 --> 00:28:56,860
And then the third substitution is switching the advantage back to the Q function,

527
00:28:56,860 --> 00:29:00,400
and it's just important to note that all of these three substitutions don't

528
00:29:00,400 --> 00:29:04,375
change the solution to the object- to the optimization problem.

529
00:29:04,375 --> 00:29:07,045
These are all sort of taking at, uh,

530
00:29:07,045 --> 00:29:14,125
these different substitutions or different ways to evaluate these quantities, okay?

531
00:29:14,125 --> 00:29:16,315
So we end up with the following: um, uh,

532
00:29:16,315 --> 00:29:19,255
we have this objective function that we are optimizing.

533
00:29:19,255 --> 00:29:21,640
This is after we've done the substitutions I just mentioned,

534
00:29:21,640 --> 00:29:24,700
and we have this constraint on how far away we can be.

535
00:29:24,700 --> 00:29:28,375
Um, and empirically, they generally just sample, um,

536
00:29:28,375 --> 00:29:32,950
this sort of alternative sampling distribution Q is just your existing old policy.

537
00:29:32,950 --> 00:29:35,575
So there's a bunch of other stuff in the paper.

538
00:29:35,575 --> 00:29:36,700
It's a really nice paper.

539
00:29:36,700 --> 00:29:38,290
Um, a lot of really interesting ideas.

540
00:29:38,290 --> 00:29:40,405
Uh, I will just,

541
00:29:40,405 --> 00:29:44,050
I will skip through sort of exactly how they do some of the additional details.

542
00:29:44,050 --> 00:29:45,655
There's some nice complexity there.

543
00:29:45,655 --> 00:29:48,010
Um, but I will just say briefly the main thing

544
00:29:48,010 --> 00:29:50,515
they're doing here is they're sort of running a policy.

545
00:29:50,515 --> 00:29:51,880
They're computing this gradient.

546
00:29:51,880 --> 00:29:53,305
They have to, um,

547
00:29:53,305 --> 00:29:55,330
consider these constraints, um,

548
00:29:55,330 --> 00:29:58,420
and they do this sort of line search with a KL constraint.

549
00:29:58,420 --> 00:30:01,960
And perhaps the most important thing is just to be aware of this and just sort of

550
00:30:01,960 --> 00:30:06,085
understand kind of them being inspired by this conservative policy improvement,

551
00:30:06,085 --> 00:30:09,055
and then trying to make that more practical and fast.

552
00:30:09,055 --> 00:30:11,380
Um, they've applied it

553
00:30:11,380 --> 00:30:12,565
to a lot of different problems.

554
00:30:12,565 --> 00:30:15,445
Um, there's some really nice stuff on locomotion controllers,

555
00:30:15,445 --> 00:30:19,210
cases where you have continuous action spaces, continuous state spaces.

556
00:30:19,210 --> 00:30:22,495
These are cases where policy gradient is often very helpful,

557
00:30:22,495 --> 00:30:24,985
uh, and they have some very nice results.

558
00:30:24,985 --> 00:30:27,400
Um, I won't step through this here.

559
00:30:27,400 --> 00:30:32,005
Um, the main thing to know is that empirically this is a really good tool to know about.

560
00:30:32,005 --> 00:30:34,210
Often, if you're doing policy gradient-style approaches,

561
00:30:34,210 --> 00:30:36,775
TRPO can be a very useful thing to build on,

562
00:30:36,775 --> 00:30:38,575
um, and it's been incredibly influential.

563
00:30:38,575 --> 00:30:41,305
This was came out in ICML in 2015.

564
00:30:41,305 --> 00:30:43,450
There's hundreds of citations to it already.

565
00:30:43,450 --> 00:30:47,810
So this has sort of become one of the main benchmarks for policy gradient.

566
00:30:48,240 --> 00:30:50,890
Okay. So if we go back just to kinda what

567
00:30:50,890 --> 00:30:54,235
the, to summarize what the policy gradient algorithm template is,

568
00:30:54,235 --> 00:30:55,990
whether you're looking at the existing algorithms

569
00:30:55,990 --> 00:30:57,700
or whether you're trying to define your own,

570
00:30:57,700 --> 00:31:00,130
generally, they look like something like the following.

571
00:31:00,130 --> 00:31:02,710
For each iteration, you run your policy out and you

572
00:31:02,710 --> 00:31:05,575
gather trajectories of data by running that policy.

573
00:31:05,575 --> 00:31:08,980
You compute some target that might be just the rewards,

574
00:31:08,980 --> 00:31:10,585
that might be a Q function.

575
00:31:10,585 --> 00:31:14,155
We can trade off between bias and variance in that, um,

576
00:31:14,155 --> 00:31:16,600
and then we use that to estimate the policy gradient,

577
00:31:16,600 --> 00:31:18,940
and then we may want to smartly take a step along

578
00:31:18,940 --> 00:31:22,820
that gradient to try to ensure monotonic improvement.

579
00:31:23,220 --> 00:31:28,330
Um, the things to be aware of and some of the things you're going to have practice

580
00:31:28,330 --> 00:31:30,490
on soon i- is that you should be very familiar with

581
00:31:30,490 --> 00:31:32,995
these sort of vanilla approaches and REINFORCE,

582
00:31:32,995 --> 00:31:35,680
um, and this general template and sort of understand how some of

583
00:31:35,680 --> 00:31:39,250
the different algorithms we're talking about might instantiate these different things.

584
00:31:39,250 --> 00:31:41,830
Um, you don't have to derive and remember

585
00:31:41,830 --> 00:31:45,655
all the formula that I just went through quickly for TRPO,um,

586
00:31:45,655 --> 00:31:49,105
and you will have the opportunity to practice these more in homework 3,

587
00:31:49,105 --> 00:31:52,420
but we'll only cover these lightly in terms of the midterm.

588
00:31:52,420 --> 00:31:56,155
All right. So is somebody may have any questions about this before we go into

589
00:31:56,155 --> 00:31:59,710
sort of a short overview of the stuff we have done so far for- before the mid-term?

590
00:31:59,710 --> 00:32:01,540
[inaudible]

591
00:32:01,540 --> 00:32:05,570
Okay. All right. Let's switch over.

592
00:32:07,110 --> 00:32:13,570
Okay. So what this is going be is sort of a very short recap of what we have,

593
00:32:13,570 --> 00:32:14,995
uh, done so far.

594
00:32:14,995 --> 00:32:17,245
And in terms of why this is useful, um,

595
00:32:17,245 --> 00:32:19,300
there's certainly a lot of good evidence from learning

596
00:32:19,300 --> 00:32:22,360
sciences that space repetition of ideas is really helpful,

597
00:32:22,360 --> 00:32:23,785
as is forced recall,

598
00:32:23,785 --> 00:32:26,245
which is one of the other benefits of doing exams.

599
00:32:26,245 --> 00:32:28,135
Um, uh, so, uh,

600
00:32:28,135 --> 00:32:29,710
that's what we are going to do today is just sort of do

601
00:32:29,710 --> 00:32:32,320
a quick recap of a lot of the different main ideas.

602
00:32:32,320 --> 00:32:36,355
So again, uh, reinforcement learning generally involves optimization,

603
00:32:36,355 --> 00:32:39,385
delayed consequences, generalization, and exploration.

604
00:32:39,385 --> 00:32:41,860
We haven't really talked about that yet.

605
00:32:41,860 --> 00:32:44,620
Um, so that's not really going to be on the midterm.

606
00:32:44,620 --> 00:32:47,830
Um, we are going to start talking a lot more about that post the mid-term.

607
00:32:47,830 --> 00:32:49,375
It's an incredibly important topic,

608
00:32:49,375 --> 00:32:53,470
one I think is super fascinating and one of the main reasons why RL is interesting.

609
00:32:53,470 --> 00:32:55,270
Um, but these other things are really important

610
00:32:55,270 --> 00:32:58,160
too and we spent some time on those so far.

611
00:32:58,320 --> 00:33:02,590
So in terms of thinking about the mid-term and indeed thinking about the class,

612
00:33:02,590 --> 00:33:03,880
um, on the very first day,

613
00:33:03,880 --> 00:33:07,255
I put up this sort of blizzard of learning objectives, um,

614
00:33:07,255 --> 00:33:09,610
and I just want to highlight, uh,

615
00:33:09,610 --> 00:33:12,280
a few of these which are the things that I mentioned

616
00:33:12,280 --> 00:33:15,100
were going to explicitly evaluated in the exam,

617
00:33:15,100 --> 00:33:16,555
which is that, um,

618
00:33:16,555 --> 00:33:18,685
by the end of the class including on the exam, um,

619
00:33:18,685 --> 00:33:22,090
you should be very familiar with sort of what are the key features of reinforcement ment-

620
00:33:22,090 --> 00:33:23,380
learning that make it different than

621
00:33:23,380 --> 00:33:25,990
other machine learning problems, and other AI problems.

622
00:33:25,990 --> 00:33:28,540
So we spent some time on that on the first day,

623
00:33:28,540 --> 00:33:30,685
um, and I've sort of tried to talk about it throughout.

624
00:33:30,685 --> 00:33:34,930
But the fact that the agent is collecting its own data and that the data,

625
00:33:34,930 --> 00:33:38,935
um, it gathers influences the policies it can learn.

626
00:33:38,935 --> 00:33:41,185
So we sort of have this censored data issue.

627
00:33:41,185 --> 00:33:43,840
The agent can't know about other lives that didn't li- didn't

628
00:33:43,840 --> 00:33:48,310
live, it makes a very big difference compared to supervised learning.

629
00:33:48,310 --> 00:33:53,485
Um, a second really important thing is that if you are given an application problem, um,

630
00:33:53,485 --> 00:33:56,260
it's important to try to know why or why not to

631
00:33:56,260 --> 00:33:59,290
formula- formulate it as a reinforcement learning problem.

632
00:33:59,290 --> 00:34:01,390
Um, and if so, how you would.

633
00:34:01,390 --> 00:34:03,475
Generally, there's not a single answer to this.

634
00:34:03,475 --> 00:34:07,210
So it's good to think of like what is one or more way to define the state-space,

635
00:34:07,210 --> 00:34:08,920
the action space, the dynamics,

636
00:34:08,920 --> 00:34:11,005
and the reward model, um,

637
00:34:11,005 --> 00:34:15,100
and what algorithm you would suggest from class to try to tackle it.

638
00:34:15,100 --> 00:34:17,425
This is in general sort of, uh,

639
00:34:17,425 --> 00:34:19,239
something that you'll probably run into much more

640
00:34:19,239 --> 00:34:21,579
than like looking at any particular algorithm,

641
00:34:21,580 --> 00:34:23,440
um, particularly in industry.

642
00:34:23,440 --> 00:34:25,719
And then a third thing that I think is really important is to

643
00:34:25,719 --> 00:34:28,614
understand how we decide whether or not an RL algorithm is good.

644
00:34:28,614 --> 00:34:31,179
And so what is the criteria for performance and

645
00:34:31,179 --> 00:34:33,759
evaluation we can use to sort of evaluate,

646
00:34:33,760 --> 00:34:35,500
um, what are the benefits, strengths and

647
00:34:35,500 --> 00:34:38,469
weaknesses of different algorithms and how they compare.

648
00:34:38,469 --> 00:34:40,629
So this could be things like bias and variance.

649
00:34:40,630 --> 00:34:42,835
It also could be computational complexity,

650
00:34:42,835 --> 00:34:46,190
um, or sample efficiency or other aspects.

651
00:34:47,130 --> 00:34:52,705
So what we have covered so far is planning where we know how the world works, um,

652
00:34:52,705 --> 00:34:57,143
policy evaluation, um, model free learning how to make good decisions,

653
00:34:57,143 --> 00:35:01,660
value function approximation, and then imitation learning and policy search.

654
00:35:01,660 --> 00:35:05,650
And we've have also talked about the fact that for reinforcement learning in general,

655
00:35:05,650 --> 00:35:10,150
you can think of either trying to find a value function of policy or a model,

656
00:35:10,150 --> 00:35:12,790
and that model is sufficient to generate

657
00:35:12,790 --> 00:35:15,850
a value function which is sufficient to generate a policy,

658
00:35:15,850 --> 00:35:18,280
um, but they are not all necessary that you

659
00:35:18,280 --> 00:35:21,590
don't have to have a model in order to get a policy.

660
00:35:23,340 --> 00:35:27,100
So, um, I will go through

661
00:35:27,100 --> 00:35:30,340
this part pretty fast and so I think a lot of you guys have also seen some of this stuff,

662
00:35:30,340 --> 00:35:32,200
um, in previous classes.

663
00:35:32,200 --> 00:35:34,240
So we're- almost everything we have been talking about so

664
00:35:34,240 --> 00:35:36,520
far assumes the world is a Markov decision process.

665
00:35:36,520 --> 00:35:39,880
But I have mentioned that often the world is not a Markov decision process.

666
00:35:39,880 --> 00:35:41,980
Um, and in the MDP case,

667
00:35:41,980 --> 00:35:44,410
we assume that the, the state is sufficient.

668
00:35:44,410 --> 00:35:47,275
Um, a sufficient statistic of all the prior history.

669
00:35:47,275 --> 00:35:50,650
So we don't have to keep track of the full set of states and

670
00:35:50,650 --> 00:35:53,965
observations and actions rewards from the whole time period,

671
00:35:53,965 --> 00:35:55,869
but we can just look at the current observation

672
00:35:55,869 --> 00:35:58,580
in order to make good decisions in the world.

673
00:35:59,370 --> 00:36:02,170
Um, in terms of this,

674
00:36:02,170 --> 00:36:05,335
i- it's very useful to know what the Markov property is, why it's important,

675
00:36:05,335 --> 00:36:06,640
why it might be violated,

676
00:36:06,640 --> 00:36:08,650
what are things like models, value,

677
00:36:08,650 --> 00:36:10,690
functions, and queues, um,

678
00:36:10,690 --> 00:36:13,015
and what is planning, and what is the difference.

679
00:36:13,015 --> 00:36:16,750
So in planning, we assume that you are given a model of how the world works,

680
00:36:16,750 --> 00:36:17,770
you know the dynamics model,

681
00:36:17,770 --> 00:36:20,920
you know the reward model, it still can be really hard to figure out how to act.

682
00:36:20,920 --> 00:36:24,370
This is like knowing the game of Go, and it's still really,

683
00:36:24,370 --> 00:36:27,580
really computationally intensive and tricky to try to figure out what's

684
00:36:27,580 --> 00:36:30,040
the optimal decision to take in Go even though you

685
00:36:30,040 --> 00:36:33,340
know all the dynamics and all of the rewards.

686
00:36:33,340 --> 00:36:36,400
In learning, we don't know the dynamics and rewards and we still

687
00:36:36,400 --> 00:36:38,995
have to gather data in order to learn a good policy,

688
00:36:38,995 --> 00:36:42,745
which has a high value, a high discounted expected sum of rewards.

689
00:36:42,745 --> 00:36:45,880
We talked about the Bellman backup operator, which is a contraction.

690
00:36:45,880 --> 00:36:48,430
If your discount factor is less than 1, um,

691
00:36:48,430 --> 00:36:50,320
which means that with repeated applications you are

692
00:36:50,320 --> 00:36:53,480
guaranteed to converge to a single fixed point.

693
00:36:53,880 --> 00:36:57,460
We talked about value versus policy iteration.

694
00:36:57,460 --> 00:37:00,415
In value iteration on the iteration k,

695
00:37:00,415 --> 00:37:05,230
you are always computing the optimal value is if you only get to make k decisions,

696
00:37:05,230 --> 00:37:10,810
um, and then you use that to back up aga- and get the k + 1 policy.

697
00:37:10,810 --> 00:37:13,990
In policy iteration, you always have a policy

698
00:37:13,990 --> 00:37:17,290
and the value of that policy if you were to act using it forever.

699
00:37:17,290 --> 00:37:22,510
Um, but it might not be a very good policy and then you update this.

700
00:37:22,510 --> 00:37:23,980
And as we have seen,

701
00:37:23,980 --> 00:37:25,390
it's closely related to sort of

702
00:37:25,390 --> 00:37:27,670
policy gradient-style algorithms where

703
00:37:27,670 --> 00:37:31,010
you sort of try to estimate the gradient of a policy.

704
00:37:31,380 --> 00:37:34,360
So in policy iteration generally,

705
00:37:34,360 --> 00:37:36,730
and similar to what we have been seeing in policy gradient approaches,

706
00:37:36,730 --> 00:37:38,950
we intermix evaluation and improvement.

707
00:37:38,950 --> 00:37:40,930
So we compute the value of a policy and then we

708
00:37:40,930 --> 00:37:43,210
use that in order to take a step and improve it.

709
00:37:43,210 --> 00:37:47,260
Um, if we are in the case of being model-free, um,

710
00:37:47,260 --> 00:37:48,490
and not having extra model,

711
00:37:48,490 --> 00:37:54,440
we often want to compute Q-values instead so that we can directly improve the policy.

712
00:37:55,740 --> 00:37:58,405
So let's just take a quick second.

713
00:37:58,405 --> 00:38:01,360
Um, so these are check your understandings, they're good things to go back through.

714
00:38:01,360 --> 00:38:03,635
These are all sort of like, you know, um,

715
00:38:03,635 --> 00:38:07,225
sm- small conceptual questions of the type that we might ask you on the exam.

716
00:38:07,225 --> 00:38:08,530
So let's just take a minute, um,

717
00:38:08,530 --> 00:38:11,995
to check our understanding and think about for a finite state and action MDP,

718
00:38:11,995 --> 00:38:13,870
the lookup table representation,

719
00:38:13,870 --> 00:38:17,035
which means that we just have a table entry for each state and action.

720
00:38:17,035 --> 00:38:19,900
Um, gamma less than 1, uh,

721
00:38:19,900 --> 00:38:22,450
does the initial setting of the value function impact

722
00:38:22,450 --> 00:38:24,505
the final computed values? Why or why not?

723
00:38:24,505 --> 00:38:28,045
Does value iteration and policy iteration always yield the same solution?

724
00:38:28,045 --> 00:38:31,075
And, um, is the number of iterations needed for

725
00:38:31,075 --> 00:38:34,990
poli- policy iteration in a finite state and action MDP bounded?

726
00:38:34,990 --> 00:38:40,180
And if, so how many? Let's just take a minute and think about those.

727
00:38:40,180 --> 00:38:41,770
Feel free to talk to somebody next to you.

728
00:38:41,770 --> 00:39:34,920
[BACKGROUND].

729
00:39:34,920 --> 00:39:36,700
All right. We're-

730
00:39:41,370 --> 00:39:45,880
We're gonna vote. So um, I'm gonna ask,

731
00:39:45,880 --> 00:39:50,950
who thinks that the initial setting of value- of the value function does not matter?

732
00:39:50,950 --> 00:39:53,140
Great. Yes, it does not matter,

733
00:39:53,140 --> 00:39:55,075
and it doesn't matter, so no.

734
00:39:55,075 --> 00:39:56,680
Doesn't matter and why not?

735
00:39:56,680 --> 00:40:00,530
Because there's only a single fixed point.

736
00:40:00,840 --> 00:40:06,110
Because it's like in- uh, the Bellman operator's a contraction operator. Just single.

737
00:40:06,390 --> 00:40:09,580
And how about- who thinks the value

738
00:40:09,580 --> 00:40:12,310
iteration and policy iteration always yield the same solution?

739
00:40:12,310 --> 00:40:17,680
Yes? No. Who thinks what- uh, why no.

740
00:40:17,680 --> 00:40:19,540
Give me an example where they might not.

741
00:40:19,540 --> 00:40:22,680
[NOISE] Yeah.

742
00:40:22,680 --> 00:40:24,160
Verbal [inaudible].

743
00:40:24,160 --> 00:40:25,840
Exactly, yeah. So that it is correct.

744
00:40:25,840 --> 00:40:27,925
You- you're gonna get the same, uh, value function.

745
00:40:27,925 --> 00:40:29,485
So it depends which way you're trying to answer this.

746
00:40:29,485 --> 00:40:31,585
They're gonna have the same value,

747
00:40:31,585 --> 00:40:34,490
they could have different policies.

748
00:40:35,280 --> 00:40:37,810
And that's possible if there's

749
00:40:37,810 --> 00:40:40,720
more than one policy that has the same optimal [NOISE] value.

750
00:40:40,720 --> 00:40:43,045
That can come up because often there's, um,

751
00:40:43,045 --> 00:40:46,225
multiple policies where you're splitting ties.

752
00:40:46,225 --> 00:40:51,220
Um, I- who thinks that the number of iterations needed for policy iteration is bounded?

753
00:40:51,220 --> 00:40:53,215
[NOISE] That's right.

754
00:40:53,215 --> 00:40:55,465
Um, anyone wanna tell me how many it is?

755
00:40:55,465 --> 00:40:56,350
A S.

756
00:40:56,350 --> 00:41:03,445
[NOISE] That's the- that's the total number of policies,

757
00:41:03,445 --> 00:41:07,600
um, and policy iteration in tabular MDPs.

758
00:41:07,600 --> 00:41:10,255
With like pol- or policy improvement in tabular MDPs,

759
00:41:10,255 --> 00:41:12,340
you're guaranteed to be monotonically improving.

760
00:41:12,340 --> 00:41:16,330
So you can at most go through every policy once and then you're done.

761
00:41:16,330 --> 00:41:18,550
So it sort of relates to what we were just talking about.

762
00:41:18,550 --> 00:41:19,795
In that case, um,

763
00:41:19,795 --> 00:41:22,600
you definitely get guaranteed policy improvement,

764
00:41:22,600 --> 00:41:25,795
because every- there's no function approximation there, there's no errors,

765
00:41:25,795 --> 00:41:27,415
you exactly know what the current value is,

766
00:41:27,415 --> 00:41:30,290
then you can take a monotonic improvement step.

767
00:41:30,480 --> 00:41:33,220
All right. So now we're gonna talk briefly of

768
00:41:33,220 --> 00:41:35,620
a refresher on model-free policy evaluation.

769
00:41:35,620 --> 00:41:37,600
Um, so this is

770
00:41:37,600 --> 00:41:40,990
model-free policy evaluation was this sort of passive reinforcement learning,

771
00:41:40,990 --> 00:41:45,730
um, where we're just trying to understand how good an existing policy is.

772
00:41:45,730 --> 00:41:47,695
Ideally, with not too much data.

773
00:41:47,695 --> 00:41:50,650
Um, and so we want to either

774
00:41:50,650 --> 00:41:54,890
directly estimate the Q function or the value function of a policy.

775
00:41:55,050 --> 00:41:59,545
And so we talked mostly in this case about episodic domains.

776
00:41:59,545 --> 00:42:01,360
When I say episodic domains, I mean,

777
00:42:01,360 --> 00:42:03,955
that we are gonna act in the world for a fixed number of steps,

778
00:42:03,955 --> 00:42:07,375
or where we are in a setting where we know we have terminal states,

779
00:42:07,375 --> 00:42:09,130
so we know the episodes will end.

780
00:42:09,130 --> 00:42:11,395
With probability 1, they have to end.

781
00:42:11,395 --> 00:42:16,690
Um, and then at that point you reset to a start state with some fixed distribution.

782
00:42:16,690 --> 00:42:19,600
[NOISE] And in Monte Carlo approaches,

783
00:42:19,600 --> 00:42:23,350
we directly average the episodic rewards. It's pretty simple.

784
00:42:23,350 --> 00:42:24,460
We take our existing policy,

785
00:42:24,460 --> 00:42:28,015
we run it out for H steps or until the end of, um, the episode.

786
00:42:28,015 --> 00:42:31,270
We reset, we repeat that a whole bunch of times, then we just average.

787
00:42:31,270 --> 00:42:33,760
Um, but in TD learning or Q learning,

788
00:42:33,760 --> 00:42:36,025
we use a target to bootstrap.

789
00:42:36,025 --> 00:42:38,680
And I know you guys have seen this a number of times,

790
00:42:38,680 --> 00:42:40,390
but just as a refresher, um,

791
00:42:40,390 --> 00:42:43,285
and I like these diagrams to start thinking about the distinctions.

792
00:42:43,285 --> 00:42:45,910
So when we've talked about dynamic programming here,

793
00:42:45,910 --> 00:42:48,820
we've thought about the case where we know the transition model,

794
00:42:48,820 --> 00:42:50,410
we know the reward model.

795
00:42:50,410 --> 00:42:54,085
So when we think about what the value is of a policy,

796
00:42:54,085 --> 00:42:58,000
it's exactly equal to the expected distribution of states and

797
00:42:58,000 --> 00:43:02,380
actions we would encounter by following this policy of the reward

798
00:43:02,380 --> 00:43:06,040
we would get plus gamma times the value of the next state.

799
00:43:06,040 --> 00:43:11,660
So note that when we think about this expectation here there's really an s prime.

800
00:43:17,790 --> 00:43:22,465
So that expectation is thinking about all the next states that we might get to.

801
00:43:22,465 --> 00:43:25,120
And so in dynamic programming,

802
00:43:25,120 --> 00:43:27,625
we just explicitly think about that sum.

803
00:43:27,625 --> 00:43:31,030
That sum over all the next states we much reach- might reach,

804
00:43:31,030 --> 00:43:32,620
and the value of each of those states.

805
00:43:32,620 --> 00:43:34,030
So if we had started in a state,

806
00:43:34,030 --> 00:43:35,545
we take an action,

807
00:43:35,545 --> 00:43:37,780
we get to some next new states.

808
00:43:37,780 --> 00:43:41,875
In general, we could repeat this process all the way out till we reach,

809
00:43:41,875 --> 00:43:44,560
you know, the horizon H or terminal states.

810
00:43:44,560 --> 00:43:47,530
Um, and this- what we think of here

811
00:43:47,530 --> 00:43:50,830
is taking an expectation over the next states that we would reach.

812
00:43:50,830 --> 00:43:54,790
And what we do in dynamic programming is instead bootstrap.

813
00:43:54,790 --> 00:43:58,490
So what we mean by bootstrap here,

814
00:43:59,340 --> 00:44:02,470
is that instead of building this whole tree,

815
00:44:02,470 --> 00:44:05,020
we keep [NOISE] track of what the value is of all the states,

816
00:44:05,020 --> 00:44:09,190
and we use that to take an explicit expectation over the next states we'd reach,

817
00:44:09,190 --> 00:44:11,665
and average over the value of those next states.

818
00:44:11,665 --> 00:44:13,435
And note that in this case,

819
00:44:13,435 --> 00:44:15,580
we're assuming we know the model.

820
00:44:15,580 --> 00:44:18,325
Now, there are ways to extend this where we don't know the model,

821
00:44:18,325 --> 00:44:19,885
but we haven't talked very much about those,

822
00:44:19,885 --> 00:44:21,820
this term so, um.

823
00:44:21,820 --> 00:44:24,595
But when I say dynamic programming here,

824
00:44:24,595 --> 00:44:29,030
I mean, that unless we otherwise specified that we know the models of the world.

825
00:44:29,130 --> 00:44:34,180
So this is a case where we're bootstrapping because we are- our update is using V,

826
00:44:34,180 --> 00:44:36,670
for V uses an estimate.

827
00:44:36,670 --> 00:44:40,060
Okay? 'Cause those values are not going to be perfect estimates

828
00:44:40,060 --> 00:44:42,625
of the true expected discounted reward for those next states,

829
00:44:42,625 --> 00:44:44,050
because we're still computing them.

830
00:44:44,050 --> 00:44:47,875
So then we looked at Monte Carlo policy evaluation,

831
00:44:47,875 --> 00:44:51,370
and it looks pretty similar in many ways except for what we're doing

832
00:44:51,370 --> 00:44:54,715
is we're running a trajectory all the way out to the horizon,

833
00:44:54,715 --> 00:44:56,860
we're adding up all the rewards,

834
00:44:56,860 --> 00:44:59,200
and that is our target.

835
00:44:59,200 --> 00:45:01,630
And when we say that policy, um,

836
00:45:01,630 --> 00:45:03,715
evaluation with Monte Carlo is sampling,

837
00:45:03,715 --> 00:45:05,665
it means we're sampling the return.

838
00:45:05,665 --> 00:45:08,110
What is the expectation that we're approximating?

839
00:45:08,110 --> 00:45:10,240
We're expectation- uh, we're

840
00:45:10,240 --> 00:45:12,880
approximating an expectation over that probability of s prime.

841
00:45:12,880 --> 00:45:16,540
[NOISE] So we only got a single s prime,

842
00:45:16,540 --> 00:45:19,885
instead of getting an expectation over all the next ones.

843
00:45:19,885 --> 00:45:22,689
And the problem with that is that we said it was high-variance,

844
00:45:22,689 --> 00:45:24,760
even though it's unbiased.

845
00:45:24,760 --> 00:45:28,840
And then we talked about combining these ideas with temporal difference methods,

846
00:45:28,840 --> 00:45:32,020
um, where we're both gonna bootstrap and sample.

847
00:45:32,020 --> 00:45:36,580
So we're sampling because [NOISE] we are only looking at a single next state,

848
00:45:36,580 --> 00:45:38,770
[NOISE] and we're bootstrapping [NOISE]

849
00:45:38,770 --> 00:45:45,625
because we're plugging in our estimate of V. So we're sampling a single s,

850
00:45:45,625 --> 00:45:47,950
t+1, and we're bootstrapping because we're not

851
00:45:47,950 --> 00:45:49,840
rolling all the way out like we did with Monte Carlo,

852
00:45:49,840 --> 00:45:53,420
which is plugging in our current estimate of that value function.

853
00:45:56,130 --> 00:45:59,065
So let's do another quick understanding of,

854
00:45:59,065 --> 00:46:01,420
um, for each of these cases, um,

855
00:46:01,420 --> 00:46:03,730
it's good to know whether- uh,

856
00:46:03,730 --> 00:46:06,520
whether it applies to dynamic programming, um,

857
00:46:06,520 --> 00:46:08,185
which requires you to know the models,

858
00:46:08,185 --> 00:46:10,810
um, Monte Carlo or TD learning.

859
00:46:10,810 --> 00:46:14,005
So is it usable when we don't know the models of the current domain?

860
00:46:14,005 --> 00:46:17,350
Does it handle continuin- continuing non episodic domains?

861
00:46:17,350 --> 00:46:19,645
Does it handle non-Markovian domains?

862
00:46:19,645 --> 00:46:23,725
Um, let me be clear by that- clear what I mean by that.

863
00:46:23,725 --> 00:46:25,915
You can always apply any algorithm to anything.

864
00:46:25,915 --> 00:46:27,160
It just may give you garbage out.

865
00:46:27,160 --> 00:46:28,630
And so my question is, um,

866
00:46:28,630 --> 00:46:31,795
when you- when I say handling non-Markovian domains,

867
00:46:31,795 --> 00:46:33,970
is it guaranteed to do something good or does

868
00:46:33,970 --> 00:46:36,415
it fumbling-fundamentally make a Markov assumption?

869
00:46:36,415 --> 00:46:40,465
Um, does it converge to the true value of the policy and the limit of updates?

870
00:46:40,465 --> 00:46:43,790
Right now, we're thinking about tabular case.

871
00:46:44,010 --> 00:46:48,085
So the value function is exactly representable and

872
00:46:48,085 --> 00:46:52,250
is it giving us an unbiased estimate of the value along the way?

873
00:46:53,160 --> 00:46:55,645
The estimates still might be consistent,

874
00:46:55,645 --> 00:46:59,020
which means that eventually with an updater they converge to the right thing,

875
00:46:59,020 --> 00:47:01,330
but they could give you biased estimates along the way.

876
00:47:01,330 --> 00:47:03,805
So again, let's just spend like a minute or two and, um,

877
00:47:03,805 --> 00:47:06,220
they are- this- there's just binary answers to each of these.

878
00:47:06,220 --> 00:47:08,350
So yes or no for each of them.

879
00:47:08,350 --> 00:47:10,270
And feel free to talk to somebody next to you.

880
00:47:10,270 --> 00:48:05,275
[BACKGROUND]

881
00:48:05,275 --> 00:48:07,450
Converge to different quality [OVERLAPPING].

882
00:48:07,450 --> 00:48:13,030
But you could start with [BACKGROUND].

883
00:48:13,030 --> 00:48:20,920
Yeah, it's a great question. Policy iteration [inaudible] good question.

884
00:48:20,920 --> 00:48:31,030
[BACKGROUND]

885
00:48:31,030 --> 00:48:31,240
All right.

886
00:48:31,240 --> 00:48:32,665
I'm gonna ask people to vote again.

887
00:48:32,665 --> 00:48:37,595
Okay, so, um, I'll just ask you to raise your hands if the answer is, yes.

888
00:48:37,595 --> 00:48:39,930
So, um, is DP,

889
00:48:39,930 --> 00:48:43,725
is dynamic programming usable when there are no models of the current domain?

890
00:48:43,725 --> 00:48:47,565
No. Is Monte Carlo usable?

891
00:48:47,565 --> 00:48:50,195
Yes. Is TD usable?

892
00:48:50,195 --> 00:48:57,775
Great. Okay, um, does DP handle continuing non-episodic domains? Raise your hand if, yes.

893
00:48:57,775 --> 00:49:00,250
Correct. Yep. So, you can use dynamic programming.

894
00:49:00,250 --> 00:49:03,280
You can use Bellman operators and contractions even if the,

895
00:49:03,280 --> 00:49:04,960
you know, for infinite horizon domains.

896
00:49:04,960 --> 00:49:07,120
You generally want your gamma function to be less than one,

897
00:49:07,120 --> 00:49:09,355
so your values don't explode.

898
00:49:09,355 --> 00:49:10,870
Um, uh, but you can do it.

899
00:49:10,870 --> 00:49:13,765
It's fine. What about Monte Carlo estimates?

900
00:49:13,765 --> 00:49:18,100
No. Monte Carlo only updates when you get to the end of an episode.

901
00:49:18,100 --> 00:49:24,694
TD estimates? Yes. Great. Um, does DP handle non-Markovian domains?

902
00:49:24,694 --> 00:49:25,000
No.

903
00:49:25,000 --> 00:49:26,935
No. Does Monte Carlo?

904
00:49:26,935 --> 00:49:27,640
Yes.

905
00:49:27,640 --> 00:49:29,065
Yes. TD?

906
00:49:29,065 --> 00:49:32,590
No. Again, you can run all of these things wherever you want,

907
00:49:32,590 --> 00:49:39,085
but- Converges to the true value of the policy and the limit of updates for DP?

908
00:49:39,085 --> 00:49:40,120
Yes.

909
00:49:40,120 --> 00:49:41,002
Yes, Monte Carlo?

910
00:49:41,002 --> 00:49:41,545
Yes.

911
00:49:41,545 --> 00:49:43,090
Yes. TD?

912
00:49:43,090 --> 00:49:43,660
Yes.

913
00:49:43,660 --> 00:49:46,450
Yes. Unbiased estimate of the value,

914
00:49:46,450 --> 00:49:48,220
DP it's kind of not applicable,

915
00:49:48,220 --> 00:49:49,750
because we're not really using data.

916
00:49:49,750 --> 00:49:50,785
It's sort of a little bit different.

917
00:49:50,785 --> 00:49:54,085
Um, Monte Carlo was an unbiased estimate of the value.

918
00:49:54,085 --> 00:49:54,700
Yes.

919
00:49:54,700 --> 00:49:55,660
Yes, TD?

920
00:49:55,660 --> 00:49:56,695
No.

921
00:49:56,695 --> 00:49:59,365
Great. Okay. So, um,

922
00:49:59,365 --> 00:50:01,180
and if we're asking you about this in the exam

923
00:50:01,180 --> 00:50:03,175
we'd be sure to clarify whether we're talking about

924
00:50:03,175 --> 00:50:04,720
the tabular setting or

925
00:50:04,720 --> 00:50:08,050
the function approximation setting where everything can be very different. Yes?

926
00:50:08,050 --> 00:50:12,115
Can you explain exactly why TD doesn't work for non-Markovian?

927
00:50:12,115 --> 00:50:15,240
Yeah. So, yeah that's a good one.

928
00:50:15,240 --> 00:50:17,100
So why does TD not work for Markovian?

929
00:50:17,100 --> 00:50:20,250
Um, it's because it's fundamentally making a Markov assumption,

930
00:50:20,250 --> 00:50:21,960
um, about the domain.

931
00:50:21,960 --> 00:50:23,820
And the reason that comes up is here.

932
00:50:23,820 --> 00:50:26,670
So the way it is writing down the value function,

933
00:50:26,670 --> 00:50:29,740
is it saying that the expected discounted sum

934
00:50:29,740 --> 00:50:31,630
of rewards from the current state is exactly

935
00:50:31,630 --> 00:50:34,090
equal to the immediate reward plus

936
00:50:34,090 --> 00:50:37,180
the discounted future sum of rewards for each of the next states,

937
00:50:37,180 --> 00:50:40,570
where that's encapsulated only by St + 1.

938
00:50:40,570 --> 00:50:43,255
So that is where you're making the Markovian assumption,

939
00:50:43,255 --> 00:50:44,740
because your aliases- if,

940
00:50:44,740 --> 00:50:46,735
um, if you have an observation space which was

941
00:50:46,735 --> 00:50:48,985
aliased that would ignore the whole history.

942
00:50:48,985 --> 00:50:52,000
Whereas Monte Carlo is summing up all the rewards from

943
00:50:52,000 --> 00:50:56,080
that current state onwards. Good question.

944
00:50:56,080 --> 00:51:02,410
[inaudible] has that assumed Markovian process?

945
00:51:02,410 --> 00:51:06,610
Great question, and remind me your name.

946
00:51:06,610 --> 00:51:09,760
Saying, um, we talked almost everything we've been talking about is TD

947
00:51:09,760 --> 00:51:13,585
0 where we just have this reward plus gamma times the value function,

948
00:51:13,585 --> 00:51:15,805
but we also talked briefly about N step.

949
00:51:15,805 --> 00:51:21,100
Um, where you sort of would do r1 + r2 + gamma times r2 et cetera.

950
00:51:21,100 --> 00:51:23,230
So for the n-step you'd have something like this.

951
00:51:23,230 --> 00:51:28,135
You'd have rt + gamma rt + 1 + gamma squared

952
00:51:28,135 --> 00:51:33,910
rt + 2 + gamma cubed V of st + 3.

953
00:51:33,910 --> 00:51:36,175
So that would be like an n-step.

954
00:51:36,175 --> 00:51:41,185
Um, and that is essentially making different notions of Markovian assumptions.

955
00:51:41,185 --> 00:51:43,330
Because you can have continuums you can either have

956
00:51:43,330 --> 00:51:46,795
completely non-Markovian domains or we can have things like n-step Markov domains.

957
00:51:46,795 --> 00:51:48,430
Which essentially means that if you're making it-

958
00:51:48,430 --> 00:51:54,390
keeping track of a certain amount of history.

959
00:51:54,390 --> 00:52:00,970
[NOISE] So um, just to sort of give

960
00:52:00,970 --> 00:52:04,180
an example that similar to some of the ones that we've seen before.

961
00:52:04,180 --> 00:52:06,670
We can think of something like a random walk process.

962
00:52:06,670 --> 00:52:11,020
So imagine that we have a domain where we have three states and two terminal states.

963
00:52:11,020 --> 00:52:13,780
So we always start in state B and

964
00:52:13,780 --> 00:52:17,275
then with probability of 50% we go left or right.

965
00:52:17,275 --> 00:52:23,815
Um, and if you reached either the black nodes then the process terminates.

966
00:52:23,815 --> 00:52:29,185
Um, and when you get there either you get + 1 on this one or you get 0.

967
00:52:29,185 --> 00:52:32,755
And it's a random walk with equal probability,

968
00:52:32,755 --> 00:52:36,740
um, until you get to a terminal state and then the process ends.

969
00:52:36,930 --> 00:52:39,130
And so in this case,

970
00:52:39,130 --> 00:52:41,890
we could try to compute like what is the true value of the state.

971
00:52:41,890 --> 00:52:46,435
Um, so the true value of a state in this case, um,

972
00:52:46,435 --> 00:52:49,525
would involve us thinking about what is the, uh,

973
00:52:49,525 --> 00:52:53,665
distribution of states that you would visit under this random walk pro- process.

974
00:52:53,665 --> 00:52:55,660
So for example, um,

975
00:52:55,660 --> 00:52:59,560
if we think about what the value is of- I'll do this here.

976
00:52:59,560 --> 00:53:05,890
So if you think about what the value is of state C,

977
00:53:05,890 --> 00:53:08,860
that's always gonna be equal to the immediate reward plus

978
00:53:08,860 --> 00:53:12,820
gamma times the sum over the next states,

979
00:53:13,400 --> 00:53:15,915
value of S prime.

980
00:53:15,915 --> 00:53:19,110
Well, let's call this one like I know,

981
00:53:19,110 --> 00:53:21,690
SD and this one S0.

982
00:53:21,690 --> 00:53:26,790
So SD's value, is always gonna be equal to + 1.

983
00:53:26,790 --> 00:53:29,640
So V of SD is equal to + 1,

984
00:53:29,640 --> 00:53:32,280
um, because you get that reward and then it terminates.

985
00:53:32,280 --> 00:53:34,425
So this would say with, um,

986
00:53:34,425 --> 00:53:39,645
gamma times half probability you would go to the value of SB,

987
00:53:39,645 --> 00:53:43,720
SB plus half you get 1.

988
00:53:45,030 --> 00:53:49,195
And eventually if you look at this distribution it's gonna be,

989
00:53:49,195 --> 00:53:52,525
um, so you could do this process for each of the different states.

990
00:53:52,525 --> 00:53:54,940
And what you would find when you do this is that you get

991
00:53:54,940 --> 00:53:57,370
through this random walk terminating on the right side

992
00:53:57,370 --> 00:53:59,904
or the left side in terms of the probability distribution

993
00:53:59,904 --> 00:54:02,990
and you could compute the values for this.

994
00:54:03,390 --> 00:54:07,210
Um, in an exam, we would probably make this a little bit easier,

995
00:54:07,210 --> 00:54:10,225
but it's good to be able to sort of look at this example and work through it,

996
00:54:10,225 --> 00:54:13,765
um, and see what this part would be in terms of the value function.

997
00:54:13,765 --> 00:54:18,520
Um, then the next question is let's imagine that we have a particular trajectory,

998
00:54:18,520 --> 00:54:21,610
we want to compare what would happen under different algorithms.

999
00:54:21,610 --> 00:54:24,370
So let's imagine what we have is we have, um,

1000
00:54:24,370 --> 00:54:26,740
a trajectory where you go BC,

1001
00:54:26,740 --> 00:54:30,925
BC terminal + 1.

1002
00:54:30,925 --> 00:54:35,410
So that's our episode.

1003
00:54:35,410 --> 00:54:42,040
So what is the first visit Monte Carlo estimate of B?

1004
00:54:42,040 --> 00:54:45,580
One.

1005
00:54:45,580 --> 00:54:47,365
One. That's right.

1006
00:54:47,365 --> 00:54:50,800
So, so V of B is equal to 1. Why is that?

1007
00:54:50,800 --> 00:54:54,250
Because what we do in Monte Carlo is we add up, versus at Monte Carlo,

1008
00:54:54,250 --> 00:54:56,980
we look at the first time we visited the state and we add

1009
00:54:56,980 --> 00:54:59,980
up all the rewards we get from that state till the end of the episode.

1010
00:54:59,980 --> 00:55:02,635
In this case, that reward is just 1.

1011
00:55:02,635 --> 00:55:06,350
So, the estimate of this would be 1.

1012
00:55:06,360 --> 00:55:08,770
The only other, you know,

1013
00:55:08,770 --> 00:55:10,390
thing that we might want to know about there is

1014
00:55:10,390 --> 00:55:12,130
if you're doing sort of this sliding average

1015
00:55:12,130 --> 00:55:15,715
like an alpha estimate to update the Monte Carlo estimate,

1016
00:55:15,715 --> 00:55:18,640
you'd want to know what the initial values were and what alpha was.

1017
00:55:18,640 --> 00:55:23,500
But let's imagine that here you just look at exactly taking that return.

1018
00:55:23,500 --> 00:55:28,970
So this is equal to the return starting at B going to the end of the episode.

1019
00:55:29,220 --> 00:55:32,740
So then the next question is, um,

1020
00:55:32,740 --> 00:55:35,620
what are the TD learning updates given the data in this order?

1021
00:55:35,620 --> 00:55:38,215
C terminal + 1 BC0,

1022
00:55:38,215 --> 00:55:41,780
CB0 with a learning rate of A.

1023
00:55:42,300 --> 00:55:45,175
Maybe just take like a minute or two and,

1024
00:55:45,175 --> 00:55:49,285
um, do one or two of these updates.

1025
00:55:49,285 --> 00:55:51,880
And then think about what would happen if we

1026
00:55:51,880 --> 00:55:55,510
reverse the order of the data with the same learning rate.

1027
00:55:55,510 --> 00:55:58,120
So, this relates to a point we've talked about a couple of

1028
00:55:58,120 --> 00:56:00,190
times about whether or not the order of

1029
00:56:00,190 --> 00:56:05,110
updates we do given some set of data matters in terms of the values we compute.

1030
00:56:05,110 --> 00:56:08,320
So I guess I would go at this in the following way.

1031
00:56:08,320 --> 00:56:10,690
I would first commit yourself either way whether or

1032
00:56:10,690 --> 00:56:14,095
not the order matters in terms of the values we're gonna compute,

1033
00:56:14,095 --> 00:56:17,570
um, and then try to compute one or two of them.

1034
00:56:18,990 --> 00:56:25,870
So let's just spend like a minute or two to decide whether or not the order matters here

1035
00:56:25,870 --> 00:56:32,950
[NOISE] in terms of the resulting values and then we can also compute.

1036
00:56:32,950 --> 00:57:45,250
[NOISE]

1037
00:57:45,250 --> 00:57:48,160
I know I'm not giving you guys enough time to do all the computations here,

1038
00:57:48,160 --> 00:57:51,160
but this is mostly to just sort of do that forced recall aspect to try to

1039
00:57:51,160 --> 00:57:54,700
remember exactly what the formulas are and then remember whether or not this matters.

1040
00:57:54,700 --> 00:57:56,005
So I'm just gonna ask you to vote.

1041
00:57:56,005 --> 00:57:59,560
Um, who here thinks that the order matters in terms of some of the values we compute?

1042
00:57:59,560 --> 00:58:02,410
[NOISE] It's right. No, it won't always.

1043
00:58:02,410 --> 00:58:04,840
Sometimes do- you can do things in different orders um,

1044
00:58:04,840 --> 00:58:07,900
the fact that we've emphasized a lot might lead you to believe that it always matters,

1045
00:58:07,900 --> 00:58:10,210
but it doesn't always matter. But in this case it does.

1046
00:58:10,210 --> 00:58:12,490
So um, in this case,

1047
00:58:12,490 --> 00:58:15,175
if we look at what the value is in the first-order,

1048
00:58:15,175 --> 00:58:19,570
what we would do is we'd say V of C = 0 + alpha 1 - 0.

1049
00:58:19,570 --> 00:58:22,270
So the new reward we've observed.

1050
00:58:22,270 --> 00:58:26,380
That would be alpha, then when we're computing the value of B,

1051
00:58:26,380 --> 00:58:30,895
we could use the new B of C we just computed because when we have this update,

1052
00:58:30,895 --> 00:58:35,545
now we've already got a non-zero estimate for V of C. Um,

1053
00:58:35,545 --> 00:58:36,820
note to be precise,

1054
00:58:36,820 --> 00:58:40,165
I should have told you here exactly how we're initializing all the values.

1055
00:58:40,165 --> 00:58:42,370
So in this case, we've implicitly assumed that

1056
00:58:42,370 --> 00:58:48,460
the initial values are 0 which matters a lot.

1057
00:58:48,460 --> 00:58:51,490
[NOISE] We'll talk some more, um,

1058
00:58:51,490 --> 00:58:53,499
in a week or two about

1059
00:58:53,499 --> 00:58:57,940
smarter exploration and the fact that being optimistic often really is very helpful.

1060
00:58:57,940 --> 00:58:59,890
One challenge can be in deep neural networks is

1061
00:58:59,890 --> 00:59:02,065
how to set things so that they're optimistic.

1062
00:59:02,065 --> 00:59:03,835
Um, but in this case,

1063
00:59:03,835 --> 00:59:05,395
so we're assuming that everything is 0,

1064
00:59:05,395 --> 00:59:06,850
so V of B will be alpha squared,

1065
00:59:06,850 --> 00:59:09,220
V of C will be the following expression.

1066
00:59:09,220 --> 00:59:13,060
Um, these are basically me just applying TD learning to these cases.

1067
00:59:13,060 --> 00:59:15,640
[inaudible]

1068
00:59:15,640 --> 00:59:21,220
They're in the second line, yeah.

1069
00:59:21,220 --> 00:59:27,625
Good catch. [NOISE] In the, where?

1070
00:59:27,625 --> 00:59:29,155
Should be gamma squared.

1071
00:59:29,155 --> 00:59:33,535
Gamma squared, yeah, yes,

1072
00:59:33,535 --> 00:59:34,990
yeah, that final expression.

1073
00:59:34,990 --> 00:59:40,240
Thanks. Um, so which appears in the third line.

1074
00:59:40,240 --> 00:59:44,080
[LAUGHTER] If we do it in the reverse order,

1075
00:59:44,080 --> 00:59:48,880
V of C will be 0 for our first update because C goes to B,

1076
00:59:48,880 --> 00:59:50,830
B, B of B is 0.

1077
00:59:50,830 --> 00:59:53,125
Then when we update BC0,

1078
00:59:53,125 --> 00:59:54,745
the value of C is still 0,

1079
00:59:54,745 --> 00:59:57,895
and we only update V of C in the final one.

1080
00:59:57,895 --> 01:00:00,100
So this just points out that order matters.

1081
01:00:00,100 --> 01:00:04,370
This comes up also when we're doing function approximation and episodic replay.

1082
01:00:05,880 --> 01:00:09,190
Just in general, when we think about policy evaluation algorithms.

1083
01:00:09,190 --> 01:00:11,245
It's good to be aware of the bias-variance tradeoff,

1084
01:00:11,245 --> 01:00:13,435
data efficiency, and computational efficiency.

1085
01:00:13,435 --> 01:00:16,435
TD learning tends to be pretty good on computational efficiency,

1086
01:00:16,435 --> 01:00:19,420
um data efficiency-wise, it depends a little bit.

1087
01:00:19,420 --> 01:00:23,455
Sometimes if you do experience replay with TD, it gets better.

1088
01:00:23,455 --> 01:00:29,515
So um, it's useful to think about there's often a lot of variants of these algorithms.

1089
01:00:29,515 --> 01:00:32,500
And so just being precise in whatever your, whatever your state is.

1090
01:00:32,500 --> 01:00:35,110
And if you're just assuming the vanilla version, we're using or if you're like,

1091
01:00:35,110 --> 01:00:37,240
well if you do this additional experience replay,

1092
01:00:37,240 --> 01:00:39,260
this is how it can change.

1093
01:00:39,780 --> 01:00:45,025
Okay. Now let's think about how we can do model free learning to make good decisions.

1094
01:00:45,025 --> 01:00:47,305
Um, we've talked a lot about Q learning.

1095
01:00:47,305 --> 01:00:52,480
Q learning is a bootstrapping technique that assumes Markovian, a Markovian world.

1096
01:00:52,480 --> 01:00:56,755
Where we say that the value- the Q value is gonna be um

1097
01:00:56,755 --> 01:01:01,735
approximated by the reward plus gamma times max over a prime with the next Q function.

1098
01:01:01,735 --> 01:01:03,820
And we can use that as sort of our target,

1099
01:01:03,820 --> 01:01:05,440
and then we do the slow slewing.

1100
01:01:05,440 --> 01:01:08,410
We sort of have this updated learning rate, and our learning rate,

1101
01:01:08,410 --> 01:01:15,535
[NOISE] where we're slewing between the one sample we just saw versus,

1102
01:01:15,535 --> 01:01:17,185
um, our previous estimate.

1103
01:01:17,185 --> 01:01:19,975
And we slowly slew this towards, um,

1104
01:01:19,975 --> 01:01:25,460
we generally decrease alpha over time to try to converge Q to a single value.

1105
01:01:26,850 --> 01:01:31,060
Uh, we talked about some conditions under which for Q learning to converge.

1106
01:01:31,060 --> 01:01:33,070
Again this is all under sort of well,

1107
01:01:33,070 --> 01:01:34,900
this is both under reachability assumptions and

1108
01:01:34,900 --> 01:01:37,180
also we're right now we're talking about the tabular setting.

1109
01:01:37,180 --> 01:01:40,960
[NOISE] So there's no function approximation going on.

1110
01:01:40,960 --> 01:01:42,580
[NOISE] So if you act randomly,

1111
01:01:42,580 --> 01:01:46,540
Q-learning will converge to Q star under mild reachability assumptions,

1112
01:01:46,540 --> 01:01:48,580
um, which means that, you know,

1113
01:01:48,580 --> 01:01:50,380
you can't have a helicopter which if you crash it,

1114
01:01:50,380 --> 01:01:52,270
the world is over and you can't get any more samples.

1115
01:01:52,270 --> 01:01:54,670
So you have to be able to sort of repeatedly visit all the states,

1116
01:01:54,670 --> 01:01:58,900
an infinite number of times and try all of the actions an infinite number of times.

1117
01:01:58,900 --> 01:02:02,290
Um, [NOISE] and it has this interesting property that, um,

1118
01:02:02,290 --> 01:02:04,720
when you are doing Q-learning you can use data gathered by

1119
01:02:04,720 --> 01:02:07,465
one policy to estimate the value of another policy.

1120
01:02:07,465 --> 01:02:10,555
So this is where we're trying to estimate the optimal Q function,

1121
01:02:10,555 --> 01:02:12,775
but we can use for example a random data,

1122
01:02:12,775 --> 01:02:16,315
random samples, [NOISE] or random policy to try to estimate that.

1123
01:02:16,315 --> 01:02:19,210
And the reason for that is because we're doing this max.

1124
01:02:19,210 --> 01:02:22,585
We're always looking at what's the best thing we could do next.

1125
01:02:22,585 --> 01:02:25,210
So that's a pretty cool property.

1126
01:02:25,210 --> 01:02:29,650
Um, so then if we sort of think about in this case,

1127
01:02:29,650 --> 01:02:32,110
um, there's some different things,

1128
01:02:32,110 --> 01:02:34,315
we'll go through these I guess just briefly.

1129
01:02:34,315 --> 01:02:39,355
Um, if you have a Q-learning policy, um,

1130
01:02:39,355 --> 01:02:45,205
which has e-greedy, e-greedy here is with probability 1 - epsilon.

1131
01:02:45,205 --> 01:02:48,970
You take the action which is expected to be best under your current Q function.

1132
01:02:48,970 --> 01:02:52,105
Um, and with probability epsilon, you would act randomly.

1133
01:02:52,105 --> 01:02:54,310
So if you were in a lookup table,

1134
01:02:54,310 --> 01:02:59,215
this is guaranteed to converge to the optimal policy and the limit of infinite data.

1135
01:02:59,215 --> 01:03:02,260
So this is yes, with mild reachability.

1136
01:03:02,260 --> 01:03:13,270
[NOISE] Um, for this second one can we use Monte Carlo estimation

1137
01:03:13,270 --> 01:03:16,070
and MDPs with large state spaces?

1138
01:03:16,620 --> 01:03:18,640
Let's vote if yes.

1139
01:03:18,640 --> 01:03:23,020
[NOISE] Whatever, I'll take a second,

1140
01:03:23,020 --> 01:03:24,760
and just talk to your neighbor, and we'll vote again.

1141
01:03:24,760 --> 01:03:28,270
[NOISE] I'm not saying that those people are wrong,

1142
01:03:28,270 --> 01:03:29,710
I'm just saying that since most people didn't vote,

1143
01:03:29,710 --> 01:03:33,160
I'm assuming that most people would benefit from just thinking about it for a sec.

1144
01:03:33,160 --> 01:03:35,180
[NOISE]

1145
01:04:19,180 --> 01:04:22,345
All right.

1146
01:04:22,345 --> 01:04:25,600
Let's vote again. Um, vote if you think

1147
01:04:25,600 --> 01:04:29,875
Monte Carlo estimation can be used in MDPs with large state spaces.

1148
01:04:29,875 --> 01:04:32,470
Yes. Yeah. So it's not, um,

1149
01:04:32,470 --> 01:04:35,080
it's not- you're not restricted to whether it's a large state space or not,

1150
01:04:35,080 --> 01:04:37,345
you can use Monte Carlo estimation there, um,

1151
01:04:37,345 --> 01:04:41,320
I, that, that's really, I guess it can.

1152
01:04:41,320 --> 01:04:46,390
So no Monte Carlo can be used.

1153
01:04:46,390 --> 01:04:52,510
That number can be point positive depending [inaudible]

1154
01:04:52,510 --> 01:04:53,650
Yes. It's a great question.

1155
01:04:53,650 --> 01:04:56,665
So, um, uh, Monte Carlo,

1156
01:04:56,665 --> 01:04:59,470
if you- the number of data points per state could be very low.

1157
01:04:59,470 --> 01:05:01,435
If you have a single start state, that's not too bad.

1158
01:05:01,435 --> 01:05:03,460
If you have a distribution of starts space- states,

1159
01:05:03,460 --> 01:05:05,080
that can be trickier, or we're gonna want to move

1160
01:05:05,080 --> 01:05:07,030
into the value function approximation setting.

1161
01:05:07,030 --> 01:05:08,590
But there's nothing a priori,

1162
01:05:08,590 --> 01:05:09,910
which means you can't apply it.

1163
01:05:09,910 --> 01:05:11,920
If you can put in there it might be really bad.

1164
01:05:11,920 --> 01:05:16,220
[LAUGHTER] Um, we might need to start doing, ah, function approximation.

1165
01:05:16,220 --> 01:05:19,365
The last thing I put on there- this is something that's,

1166
01:05:19,365 --> 01:05:22,275
um, we haven't discussed a lot yet,

1167
01:05:22,275 --> 01:05:24,900
um, but I think it's an interesting start for one to sort of start

1168
01:05:24,900 --> 01:05:28,475
connecting these between the dynamic programming aspects we've talked about.

1169
01:05:28,475 --> 01:05:31,720
Um, a model-based reinforcement learning is not

1170
01:05:31,720 --> 01:05:35,185
necessarily always more data efficient than model-free,

1171
01:05:35,185 --> 01:05:36,820
though we- we talked mostly about model-free.

1172
01:05:36,820 --> 01:05:39,550
Um, so we haven't discussed this too much, but, well,

1173
01:05:39,550 --> 01:05:43,450
it's a good thing to be thinking about particularly as we start getting into exploration.

1174
01:05:43,450 --> 01:05:45,730
Um, and I mentioned briefly before that there's

1175
01:05:45,730 --> 01:05:49,345
a nice new paper by Wen Sun and some colleagues at MSR,

1176
01:05:49,345 --> 01:05:51,550
Microsoft Research New York City,

1177
01:05:51,550 --> 01:05:54,325
that is showing that in some cases, um,

1178
01:05:54,325 --> 01:05:57,505
model-based is strictly better than model-free.

1179
01:05:57,505 --> 01:06:00,850
And the intuition there is that you can compactly represent the model,

1180
01:06:00,850 --> 01:06:03,250
but you can't compactly represent the value function.

1181
01:06:03,250 --> 01:06:05,605
So you don't need a lot of parameters to learn the model,

1182
01:06:05,605 --> 01:06:06,745
and then you can plan with it,

1183
01:06:06,745 --> 01:06:08,560
but if you try to directly learn the value function,

1184
01:06:08,560 --> 01:06:12,475
you need a lot more. All right.

1185
01:06:12,475 --> 01:06:15,865
So as we're sort of starting to move into an even just in that discussion,

1186
01:06:15,865 --> 01:06:19,225
a lot of our recent focus has been in value function approximation.

1187
01:06:19,225 --> 01:06:21,055
I'm including in homework 2.

1188
01:06:21,055 --> 01:06:26,740
So um, we talked about if you were looking at Monte Carlo methods versus TD learning,

1189
01:06:26,740 --> 01:06:31,045
um, what sort of convergence guarantees do we have in the on policy case?

1190
01:06:31,045 --> 01:06:36,250
So this is, um, important to emphasize.

1191
01:06:36,250 --> 01:06:39,745
So we're looking at evaluating the value of a single policy,

1192
01:06:39,745 --> 01:06:43,900
and we talked about how we can think about the on policy stationary distribution.

1193
01:06:43,900 --> 01:06:45,939
That when we define a single policy,

1194
01:06:45,939 --> 01:06:47,200
then, uh, [NOISE] and we run it,

1195
01:06:47,200 --> 01:06:50,650
that's like we're inducing a Markov reward process or a Markov chain,

1196
01:06:50,650 --> 01:06:52,840
and we think- can think about the stationary distribution

1197
01:06:52,840 --> 01:06:55,165
of states that we would visit under that policy.

1198
01:06:55,165 --> 01:06:58,280
Um, and we talked about convergence properties.

1199
01:06:58,280 --> 01:07:02,475
And in particular, we said that what Monte Carlo does, um,

1200
01:07:02,475 --> 01:07:05,010
no matter what sort of function approximator you are using,

1201
01:07:05,010 --> 01:07:07,950
is it tries to minimize, um, the mean squared error.

1202
01:07:07,950 --> 01:07:11,860
So this style of techniques we've talked about with Monte Carlo, um,

1203
01:07:11,860 --> 01:07:15,580
is that it simply tries to minimize the mean squared error of your data.

1204
01:07:15,580 --> 01:07:20,560
[NOISE] And so we can think about this

1205
01:07:20,560 --> 01:07:22,870
for linear value function approximators shouldn't- this also

1206
01:07:22,870 --> 01:07:25,600
holds for other value function approximation- mators,

1207
01:07:25,600 --> 01:07:26,950
it's gonna minimize the error.

1208
01:07:26,950 --> 01:07:32,935
[NOISE] In the case of linear value function approximation with TD learner- learning,

1209
01:07:32,935 --> 01:07:36,625
it is gonna converge to a constant factor of the best mean squared error.

1210
01:07:36,625 --> 01:07:38,530
And what does that mean in this case?

1211
01:07:38,530 --> 01:07:45,835
Um, here, what we have is- we might have a gap,

1212
01:07:45,835 --> 01:07:49,255
so particularly if you have some like linear value function approximators, um,

1213
01:07:49,255 --> 01:07:53,305
you just might not be able to write to exactly represent the value of all the states,

1214
01:07:53,305 --> 01:07:58,840
use the- the chosen like a parametric family that you have.

1215
01:07:58,840 --> 01:08:02,695
And so there might fundamentally just be a gap between, um,

1216
01:08:02,695 --> 01:08:06,550
the value function that's representable with the space that you have,

1217
01:08:06,550 --> 01:08:08,185
and the true value function.

1218
01:08:08,185 --> 01:08:10,120
I often like to think about like this.

1219
01:08:10,120 --> 01:08:12,565
There's a nice picture and set no partner about this two.

1220
01:08:12,565 --> 01:08:13,720
This is sort of this, ah,

1221
01:08:13,720 --> 01:08:15,745
showing with your set of W,

1222
01:08:15,745 --> 01:08:17,845
what are the value functions you can represent,

1223
01:08:17,845 --> 01:08:21,160
and it might be that your variable value function lives up here.

1224
01:08:21,160 --> 01:08:23,740
You just can't with- for example,

1225
01:08:23,740 --> 01:08:26,979
with a line be able to represent all of the true value functions.

1226
01:08:26,979 --> 01:08:30,009
Um, if you think about this in two dimen- in, um, another dimension,

1227
01:08:30,010 --> 01:08:33,220
you can imagine for a state maybe your real value function looks something like this,

1228
01:08:33,220 --> 01:08:35,350
but you are using a line approximator,

1229
01:08:35,350 --> 01:08:39,470
so you just can't represent that exactly a- a straight line.

1230
01:08:39,660 --> 01:08:45,040
So Monte Carlo converges to the best mean squared error possible,

1231
01:08:45,040 --> 01:08:47,095
giving your value function approximator space,

1232
01:08:47,095 --> 01:08:51,310
and TD learner converges to that times this additional factor.

1233
01:08:51,310 --> 01:08:56,890
[NOISE] Oop, see.

1234
01:08:56,890 --> 01:09:00,654
Well, I think that's not going to like that.

1235
01:09:00,654 --> 01:09:03,519
Okay. Um, so note that there's this.

1236
01:09:03,520 --> 01:09:15,160
[NOISE]

1237
01:09:15,160 --> 01:09:17,170
Okay. Well, now I'm just gonna

1238
01:09:17,170 --> 01:09:27,320
make that not do that anymore, all right.

1239
01:09:37,859 --> 01:09:41,544
We talked about the fact that when you're doing off policy learning,

1240
01:09:41,545 --> 01:09:43,765
Q-learning with function approximation can diverge,

1241
01:09:43,765 --> 01:09:47,330
which means that it doesn't even converge with infinite amounts of data.

1242
01:09:48,000 --> 01:09:52,255
This is even separate than what it might converge to if it converges.

1243
01:09:52,255 --> 01:09:54,730
It just says that the actual- your parameters just may never stop

1244
01:09:54,730 --> 01:09:58,480
changing if you're doing sort of gradient updates. Yeah.

1245
01:09:58,480 --> 01:10:02,620
Can we initialize the function approximator [NOISE] in those parameters in

1246
01:10:02,620 --> 01:10:07,480
such a way that [inaudible] push convergence and not guarantee it's not like?

1247
01:10:07,480 --> 01:10:09,280
Well, we have conditions on whether or not, um,

1248
01:10:09,280 --> 01:10:12,010
the initialization of the parameters helps determine whether or not,

1249
01:10:12,010 --> 01:10:13,885
for example, you might converge or diverge.

1250
01:10:13,885 --> 01:10:15,310
Um, it's an interesting question,

1251
01:10:15,310 --> 01:10:16,990
I don't think there's work that I know

1252
01:10:16,990 --> 01:10:19,060
that formally tries to characterize this, like, you know,

1253
01:10:19,060 --> 01:10:23,020
are there places where you could formally do this, um, so that,

1254
01:10:23,020 --> 01:10:25,660
ah, in terms of your gradients,

1255
01:10:25,660 --> 01:10:27,505
for example, they wouldn't start to explode?

1256
01:10:27,505 --> 01:10:29,920
There might be, I suspect it depends a lot on the problem.

1257
01:10:29,920 --> 01:10:31,300
And I also suspect that there might be

1258
01:10:31,300 --> 01:10:33,715
pathological examples that you can construct where it's hard to do.

1259
01:10:33,715 --> 01:10:35,350
But certainly worth a try.

1260
01:10:35,350 --> 01:10:37,300
You can also ob- ob- observe whether or not

1261
01:10:37,300 --> 01:10:40,280
your parameter estimates are continuing to change.

1262
01:10:41,220 --> 01:10:43,405
Um, we talked quite a lot,

1263
01:10:43,405 --> 01:10:46,045
you guys had a lot of practice with deep learning and model-free Q-learning,

1264
01:10:46,045 --> 01:10:48,970
um, where we looked at having this Q-learning target and the Q network,

1265
01:10:48,970 --> 01:10:50,800
and we're doing stochastic gradient descent.

1266
01:10:50,800 --> 01:10:53,800
Um, we're using a deep neural network to approximate Q.

1267
01:10:53,800 --> 01:10:57,220
[NOISE] Um, we talked about the- some of the challenges with

1268
01:10:57,220 --> 01:11:00,775
this divergence might be that we have these correlated local updates.

1269
01:11:00,775 --> 01:11:03,040
The value of a state is often very closely related

1270
01:11:03,040 --> 01:11:05,620
to the value of its next successor state.

1271
01:11:05,620 --> 01:11:10,465
Um, and that also by changing these targets frequently,

1272
01:11:10,465 --> 01:11:13,495
um, then that might cause, ah, instability.

1273
01:11:13,495 --> 01:11:15,580
So that's a lot of the recent progress over

1274
01:11:15,580 --> 01:11:17,830
roughly the last five years has been in sort of

1275
01:11:17,830 --> 01:11:19,990
ways to modify this equation in order to make

1276
01:11:19,990 --> 01:11:23,035
it more stable when you're doing gradient descent.

1277
01:11:23,035 --> 01:11:25,285
Um, and in DQN,

1278
01:11:25,285 --> 01:11:28,270
it's sort of both introduced that we should do experience replay,

1279
01:11:28,270 --> 01:11:29,920
so don't use each data point once,

1280
01:11:29,920 --> 01:11:32,350
um, and also fix the target for a while.

1281
01:11:32,350 --> 01:11:33,955
So you're sort of saying, "I'm going to use this."

1282
01:11:33,955 --> 01:11:37,225
A fixed value function approximator my next state for a while,

1283
01:11:37,225 --> 01:11:39,980
and then we can minimize our mean squared error.

1284
01:11:40,380 --> 01:11:44,365
We talked about the fact that experience replay is particularly hugely helpful,

1285
01:11:44,365 --> 01:11:46,315
um, and the targets is also quite.

1286
01:11:46,315 --> 01:11:51,115
Ah, and there aren't good guarantees yet on convergence,

1287
01:11:51,115 --> 01:11:54,520
though there's a lot of interesting work that's being done in this space.

1288
01:11:54,520 --> 01:11:56,020
People are very interested in trying to understand

1289
01:11:56,020 --> 01:11:58,495
the formal properties of these type of networks.

1290
01:11:58,495 --> 01:12:00,700
Um, we also talked about double Q,

1291
01:12:00,700 --> 01:12:04,240
dueling, um, and, uh, like prioritized replay,

1292
01:12:04,240 --> 01:12:06,760
um, as things that we could look at to try to

1293
01:12:06,760 --> 01:12:11,300
improve how quickly our Q functions converge to something reasonable.

1294
01:12:12,300 --> 01:12:15,850
So I think this is the last one for today.

1295
01:12:15,850 --> 01:12:17,680
Um, ah so quick question.

1296
01:12:17,680 --> 01:12:21,415
Um, in finite state spaces with features that can represent the true value function.

1297
01:12:21,415 --> 01:12:24,520
Does TD learning with value function approximation always find

1298
01:12:24,520 --> 01:12:28,700
the true value function of the policy given sufficient data?

1299
01:12:30,150 --> 01:12:33,115
So this is for TD learning.

1300
01:12:33,115 --> 01:12:36,850
So we're- we're essentially doing policy evaluation right now.

1301
01:12:36,850 --> 01:12:43,820
So in this case, are we guaranteed to find the true value function given sufficient data?

1302
01:12:46,320 --> 01:12:51,475
Maybe take one, chat with a neighbor for one minute and then I'll ask people to vote.

1303
01:12:51,475 --> 01:12:53,965
Should we assume this is on on policy?

1304
01:12:53,965 --> 01:12:56,210
We're gonna assume this is on policy,

1305
01:12:57,780 --> 01:13:01,810
or at least with sufficient amounts of data from the on policy distribution.

1306
01:13:01,810 --> 01:13:27,790
[BACKGROUND]

1307
01:13:27,790 --> 01:13:32,545
Who wants to vote yes that we do find the true value function approximator?

1308
01:13:32,545 --> 01:13:35,470
That's right. Okay, so how could we have checked this?

1309
01:13:35,470 --> 01:13:38,980
So if we go back to what I was saying over here.

1310
01:13:38,980 --> 01:13:41,680
What I said is that we are going to converge to

1311
01:13:41,680 --> 01:13:44,020
a constant factor of the best mean squared error.

1312
01:13:44,020 --> 01:13:45,940
This mean squared error is always 0,

1313
01:13:45,940 --> 01:13:51,230
if you can exactly represent the value in the current space.

1314
01:13:51,240 --> 01:13:56,815
So that additional sort of constant factor is just a constant factor times 0.

1315
01:13:56,815 --> 01:14:00,890
So in this case, yes.

1316
01:14:01,890 --> 01:14:04,360
So I- because I said here that

1317
01:14:04,360 --> 01:14:08,490
it- with features that can represent the true value function.

1318
01:14:08,490 --> 01:14:10,860
So we've said that it is perfectly

1319
01:14:10,860 --> 01:14:13,290
possible to represent the value function of this policy in

1320
01:14:13,290 --> 01:14:17,970
the features that are given to you and so it will be possible to achieve that. Yeah.

1321
01:14:17,970 --> 01:14:21,235
Is it true for a non-linear parameterization?

1322
01:14:21,235 --> 01:14:25,390
For TD learning? Yeah. So for policy evaluation, um, uh,

1323
01:14:25,390 --> 01:14:29,170
if you have a nonlinear- if you have features- like if you have

1324
01:14:29,170 --> 01:14:31,630
a general representation that allows you to exactly

1325
01:14:31,630 --> 01:14:34,150
represent the value function and you're doing on policy learning.

1326
01:14:34,150 --> 01:14:35,755
So you're doing TD learning, um,

1327
01:14:35,755 --> 01:14:37,780
you will be able to get zero error,

1328
01:14:37,780 --> 01:14:40,495
with infinite, you know, sufficient data etc.

1329
01:14:40,495 --> 01:14:42,385
Finite data, this is all of it. Yeah.

1330
01:14:42,385 --> 01:14:43,810
One part that I-

1331
01:14:43,810 --> 01:14:45,035
Remind me your name, please.

1332
01:14:45,035 --> 01:14:48,450
I, I got a little confused given that we,

1333
01:14:48,450 --> 01:14:51,045
we might have all of the features that we want,

1334
01:14:51,045 --> 01:14:52,770
but we might not have

1335
01:14:52,770 --> 01:14:56,820
any representative value function approximation

1336
01:14:56,820 --> 01:14:59,310
that would actually be able to generate the Qs.

1337
01:14:59,310 --> 01:15:03,205
Like the- a- are those two things like identical?

1338
01:15:03,205 --> 01:15:07,600
Like, I guess the way I was thinking about this was we might have all the features,

1339
01:15:07,600 --> 01:15:09,910
but we might not find a space of functions that

1340
01:15:09,910 --> 01:15:13,225
actually would be able to represent the value function.

1341
01:15:13,225 --> 01:15:15,430
Okay. So I think the question is say, okay.

1342
01:15:15,430 --> 01:15:16,780
Well, what if we had a lot of features,

1343
01:15:16,780 --> 01:15:19,359
but like- does that actually give us a parameterization

1344
01:15:19,359 --> 01:15:22,180
of the value function that can represent the true value function?

1345
01:15:22,180 --> 01:15:25,960
When I say here sort of features and representation I mean that we

1346
01:15:25,960 --> 01:15:29,740
have picked a function class that can exactly represent the value function,

1347
01:15:29,740 --> 01:15:32,425
if we have an algorithm to try to fit it well enough.

1348
01:15:32,425 --> 01:15:35,230
So, um what I'm assuming- what I'm saying in this case is

1349
01:15:35,230 --> 01:15:37,780
that if your value function- if someone could- if

1350
01:15:37,780 --> 01:15:42,970
an oracle could give you those features- the- the- the parameter vector that would make,

1351
01:15:42,970 --> 01:15:44,665
um, uh, that zero,

1352
01:15:44,665 --> 01:15:46,180
that TD learning can find it.

1353
01:15:46,180 --> 01:15:51,205
So this essentially like [inaudible] because we can generate the table, keep it.

1354
01:15:51,205 --> 01:15:52,765
It doesn't have to be tabular.

1355
01:15:52,765 --> 01:15:56,315
So to go over- this does not to have to only hold for tabular cases.

1356
01:15:56,315 --> 01:16:00,840
It's that if like- so if we look at, um, something here.

1357
01:16:00,840 --> 01:16:02,340
Let's imagine this is your state space.

1358
01:16:02,340 --> 01:16:03,735
This is your value function.

1359
01:16:03,735 --> 01:16:05,745
So if someone gives you,

1360
01:16:05,745 --> 01:16:07,950
uh, a line, or a quadratic,

1361
01:16:07,950 --> 01:16:11,595
or a deep neural network with enough parameters to exactly represent that line,

1362
01:16:11,595 --> 01:16:13,590
what this statement is saying is that

1363
01:16:13,590 --> 01:16:17,255
TD learning can find- can fit those parameters exactly.

1364
01:16:17,255 --> 01:16:20,305
This is not true when we start to go into Q learning.

1365
01:16:20,305 --> 01:16:22,225
So in some cases, you can have

1366
01:16:22,225 --> 01:16:25,255
a representation that could optimally represent the value function,

1367
01:16:25,255 --> 01:16:26,680
but you can't find it.

1368
01:16:26,680 --> 01:16:28,855
Like Q learning will not identify it.

1369
01:16:28,855 --> 01:16:30,790
So that's sort of the difference that we're trying to make here,

1370
01:16:30,790 --> 01:16:34,135
is that in TD learning, if that exists and you're on policy, you can find it.

1371
01:16:34,135 --> 01:16:35,635
Q learning, you may not be able to.

1372
01:16:35,635 --> 01:16:37,585
Yeah. There's a question in the back. Your name first, please.

1373
01:16:37,585 --> 01:16:43,465
So can I just clarify this is to whether the value approximation is linear or nonlinear?

1374
01:16:43,465 --> 01:16:46,840
Yes. Yes. So what's we're trying- this is true,

1375
01:16:46,840 --> 01:16:52,795
um, for generic representations.

1376
01:16:52,795 --> 01:16:55,990
If your representation, whether it's linear,

1377
01:16:55,990 --> 01:16:57,895
or tabular or, um,

1378
01:16:57,895 --> 01:17:00,085
tabular generally always assume it's- it's exact.

1379
01:17:00,085 --> 01:17:01,690
So linear or otherwise,

1380
01:17:01,690 --> 01:17:05,695
then- then this is- this is true. Yes.

1381
01:17:05,695 --> 01:17:09,445
Uh, does this have anything to do with if

1382
01:17:09,445 --> 01:17:14,830
our value function approximator is a contracting operator or not? So.

1383
01:17:14,830 --> 01:17:17,890
Yeah. It's a great question. You asked whether or not this has to do with whether or

1384
01:17:17,890 --> 01:17:20,785
not our value function approximator is a contraction.

1385
01:17:20,785 --> 01:17:24,070
You can think of when we're doing this sort of TD learning, that we have two steps.

1386
01:17:24,070 --> 01:17:26,155
We're kinda doing our approximated Bellman.

1387
01:17:26,155 --> 01:17:27,550
And our Bellman operator,

1388
01:17:27,550 --> 01:17:29,575
if we could do it exactly we know is a contraction,

1389
01:17:29,575 --> 01:17:31,885
then we have to do this additional part of fitting a function.

1390
01:17:31,885 --> 01:17:34,405
And if you can exactly fit your function, um,

1391
01:17:34,405 --> 01:17:37,660
then you're not going to introduce additional error during that part.

1392
01:17:37,660 --> 01:17:42,595
Um, and that's- that's one of- that's one of the benefits in here.

1393
01:17:42,595 --> 01:17:44,440
That cannot- that can start to be divert-

1394
01:17:44,440 --> 01:17:45,910
So in this case again, it's all on

1395
01:17:45,910 --> 01:17:48,205
policies so it's much closer to the supervised learning setting.

1396
01:17:48,205 --> 01:17:52,960
When you start to be off policy this gets more complicated. Question or?

1397
01:17:52,960 --> 01:17:55,390
No. Okay. All right.

1398
01:17:55,390 --> 01:17:57,940
So let's just go really briefly through imitation learning,

1399
01:17:57,940 --> 01:17:59,350
um, and policy search.

1400
01:17:59,350 --> 01:18:01,210
This will be kind of at the same lighter level that,

1401
01:18:01,210 --> 01:18:03,655
um, you'd be expected to know it for the exam.

1402
01:18:03,655 --> 01:18:05,680
You haven't had the chance to practice either of these,

1403
01:18:05,680 --> 01:18:07,295
um, except where from lecture.

1404
01:18:07,295 --> 01:18:09,855
So imitation learning was the idea that

1405
01:18:09,855 --> 01:18:12,975
the specification of reward functions can be really complicated.

1406
01:18:12,975 --> 01:18:16,965
What if we could just have people demonstrate procedures and then learn from them?

1407
01:18:16,965 --> 01:18:19,725
Behavior cloning is where we're doing supervised learning.

1408
01:18:19,725 --> 01:18:23,530
So we're trying to learn a mapping of actions to states,

1409
01:18:23,530 --> 01:18:26,530
and we're treating this as a supervised learning problem.

1410
01:18:26,530 --> 01:18:29,199
So we just look at for an expert,

1411
01:18:29,199 --> 01:18:31,555
pairs of states and action,

1412
01:18:31,555 --> 01:18:35,260
and you can try to fit your favorite machine learning supervised,

1413
01:18:35,260 --> 01:18:38,140
like classification algorithm to- to predict that.

1414
01:18:38,140 --> 01:18:41,950
And the thing that can go wrong in this case is that your state

1415
01:18:41,950 --> 01:18:45,895
distribution that you induce under your approximate policy,

1416
01:18:45,895 --> 01:18:47,320
that's trying to mimic the expert,

1417
01:18:47,320 --> 01:18:49,000
can be different than,

1418
01:18:49,000 --> 01:18:52,465
um, the- the distribution of states you'd reach on to the expert policy.

1419
01:18:52,465 --> 01:18:56,350
Which means that you can end up with these sort of different state distributions,

1420
01:18:56,350 --> 01:18:59,410
and you don't know what the right thing is

1421
01:18:59,410 --> 01:19:02,710
to do under these new states because you don't have any data about that.

1422
01:19:02,710 --> 01:19:05,620
So things can go pretty badly in some of those cases.

1423
01:19:05,620 --> 01:19:07,300
We talked about imitation learning,

1424
01:19:07,300 --> 01:19:11,785
where the idea is that we have again trajectories of- of demonstrations.

1425
01:19:11,785 --> 01:19:18,295
And now the goal is to directly learn rewards.

1426
01:19:18,295 --> 01:19:20,770
A good thing to rethink about here is

1427
01:19:20,770 --> 01:19:25,255
how many reward functions are compatible with an expert's demonstration.

1428
01:19:25,255 --> 01:19:26,920
We talked about this before.

1429
01:19:26,920 --> 01:19:28,915
If it's not clear, feel free to reach out to me

1430
01:19:28,915 --> 01:19:32,005
either at the end of class or, um, on Piazza.

1431
01:19:32,005 --> 01:19:34,000
And we talked about policy search.

1432
01:19:34,000 --> 01:19:36,220
So just really briefly.

1433
01:19:36,220 --> 01:19:37,960
These are the types of levels of,

1434
01:19:37,960 --> 01:19:40,255
um, questions I would expect you to be familiar with.

1435
01:19:40,255 --> 01:19:43,285
So why do we want to do stochastic parametrized policies?

1436
01:19:43,285 --> 01:19:45,535
Can be a nice way to put it in domain knowledge.

1437
01:19:45,535 --> 01:19:48,190
It can help us with non-Markovian structure.

1438
01:19:48,190 --> 01:19:49,960
We talked about aliasing,

1439
01:19:49,960 --> 01:19:51,700
and we talked about game theory settings,

1440
01:19:51,700 --> 01:19:54,415
where deterministic policies would do badly,

1441
01:19:54,415 --> 01:19:56,485
but stochastic ones would do well.

1442
01:19:56,485 --> 01:20:00,310
Um, policy gradient methods are not the only form of policy search.

1443
01:20:00,310 --> 01:20:04,645
We talked about exoskeleton optimization by my colleague Steve Collins,

1444
01:20:04,645 --> 01:20:06,865
and the fact that, um, that worked pretty well.

1445
01:20:06,865 --> 01:20:09,010
But generally, we're going to talk mostly about gradients.

1446
01:20:09,010 --> 01:20:12,070
Um, the likelihood ratio policy gradient method does

1447
01:20:12,070 --> 01:20:14,935
not need us to have the dynamics model,

1448
01:20:14,935 --> 01:20:17,800
which is really important because when we don't have it.

1449
01:20:17,800 --> 01:20:20,725
And then two ideas to reduce the variance of

1450
01:20:20,725 --> 01:20:25,550
a policy gradient estimator is to use the temporal structure.

1451
01:20:26,670 --> 01:20:31,150
And here, it involves the fact that the reward you get at a timestep now can't be

1452
01:20:31,150 --> 01:20:35,605
influenced by your future decisions because of the structure of time.

1453
01:20:35,605 --> 01:20:39,260
And then, um- and secondly baselines.

1454
01:20:39,750 --> 01:20:41,830
So that's kind of the level,

1455
01:20:41,830 --> 01:20:43,390
the sort of- the stuff we talked about in class,

1456
01:20:43,390 --> 01:20:45,610
but not deep procedural knowledge.

1457
01:20:45,610 --> 01:20:47,830
So just to summarize.

1458
01:20:47,830 --> 01:20:50,200
Um, recommendations would be to go through lecture notes,

1459
01:20:50,200 --> 01:20:52,015
look at things, like check your understanding.

1460
01:20:52,015 --> 01:20:53,680
If you want to look at existing, uh,

1461
01:20:53,680 --> 01:20:55,750
additional examples going through session,

1462
01:20:55,750 --> 01:20:57,385
um, notes can be useful.

1463
01:20:57,385 --> 01:20:59,380
Um, the practice midterms particularly

1464
01:20:59,380 --> 01:21:02,335
last year will be more similar to the one from two years ago.

1465
01:21:02,335 --> 01:21:05,215
Um, if you see some topic that we haven't covered in this class,

1466
01:21:05,215 --> 01:21:07,240
it's not going to be covered on the midterm, um,

1467
01:21:07,240 --> 01:21:09,580
but feel free to reach out to us if you have any questions,

1468
01:21:09,580 --> 01:21:11,830
and you can bring a one-sided one page of notes that's

1469
01:21:11,830 --> 01:21:15,920
handwritten or typed. Okay. Good luck.

