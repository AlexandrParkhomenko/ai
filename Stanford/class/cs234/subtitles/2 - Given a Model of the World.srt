1
00:00:04,220 --> 00:00:08,070
All right. So, last time we were starting to talk about the sort

2
00:00:08,070 --> 00:00:11,085
of the general overview of what reinforcement learning involves,

3
00:00:11,085 --> 00:00:15,300
um, and, we introduced the notion [NOISE] of a model,

4
00:00:15,300 --> 00:00:16,830
a value, and a policy.

5
00:00:16,830 --> 00:00:20,910
[NOISE] Um, so it's good to just refresh your brain right now,

6
00:00:20,910 --> 00:00:22,650
about what, what those three things are.

7
00:00:22,650 --> 00:00:26,130
Can anybody remember off the top of their head what a value, a model,

8
00:00:26,130 --> 00:00:28,710
or a policy was in the context of reinforcement learning?

9
00:00:28,710 --> 00:00:30,210
[NOISE]

10
00:00:30,210 --> 00:00:33,239
Um, so policy is a set of actions,

11
00:00:33,239 --> 00:00:36,360
that, uh, the agent should take [NOISE] in a work. [NOISE]

12
00:00:36,360 --> 00:00:39,240
Exactly right. So, the definition of a policy is a mapping from

13
00:00:39,240 --> 00:00:42,930
the state you're in to what is the action, um, to take.

14
00:00:42,930 --> 00:00:45,320
And it might be a good policy or a bad policy.

15
00:00:45,320 --> 00:00:46,580
And the way we evaluate that,

16
00:00:46,580 --> 00:00:49,610
is in terms of its [NOISE] expected discounted sum of rewards.

17
00:00:49,610 --> 00:00:52,230
Does anybody remember what a model was? Yeah?

18
00:00:52,230 --> 00:00:53,640
A model is like, uh,

19
00:00:53,640 --> 00:00:58,020
representation of the world and how that changes in response to agent's accident. [NOISE]

20
00:00:58,020 --> 00:01:02,000
Yeah. So right, so normally we think of a model of incorporating either reward model,

21
00:01:02,000 --> 00:01:04,610
or a decision, uh, or, or dynamics model,

22
00:01:04,610 --> 00:01:08,660
[NOISE] which specifies in response to the current state and, uh,

23
00:01:08,660 --> 00:01:10,715
an action how the world might change,

24
00:01:10,715 --> 00:01:13,130
could be a stochastic model or a deterministic model.

25
00:01:13,130 --> 00:01:15,410
[NOISE] Um, and the reward model specifies,

26
00:01:15,410 --> 00:01:17,390
what is the expected reward, um,

27
00:01:17,390 --> 00:01:20,150
that the agent receives from taking a state in a particular action.

28
00:01:20,150 --> 00:01:23,690
[NOISE] So what we're gonna talk about today is, um,

29
00:01:23,690 --> 00:01:26,150
thinking about, if you know a model of the world,

30
00:01:26,150 --> 00:01:28,100
so, you know, um,

31
00:01:28,100 --> 00:01:30,710
what happens if you take an action in a particular state,

32
00:01:30,710 --> 00:01:34,190
or what the distribution of next states might be if you [NOISE] take an action,

33
00:01:34,190 --> 00:01:36,565
[NOISE] um, how we should make decisions.

34
00:01:36,565 --> 00:01:38,865
So, how do we do the planning problem?

35
00:01:38,865 --> 00:01:40,610
So, we're not gonna talk about learning today.

36
00:01:40,610 --> 00:01:44,030
We're just gonna talk about the problem of figuring out what is the right thing to do,

37
00:01:44,030 --> 00:01:47,195
[NOISE] when your actions may have delayed consequences,

38
00:01:47,195 --> 00:01:49,550
which means that you may have to sacrifice

39
00:01:49,550 --> 00:01:52,280
immediate reward in order to maximize long-term reward.

40
00:01:52,280 --> 00:01:57,060
[NOISE] So as we just stated, um,

41
00:01:57,060 --> 00:02:01,465
the model generally we're gonna think about is statistical or mathematical models,

42
00:02:01,465 --> 00:02:03,530
of the dynamics and the reward function.

43
00:02:03,530 --> 00:02:06,950
Um, a policy is a function that maps the students each,

44
00:02:06,950 --> 00:02:09,320
uh, uh these agents states to actions,

45
00:02:09,320 --> 00:02:13,190
and the value function as the expected discounted sum of rewards, um,

46
00:02:13,190 --> 00:02:15,110
from being in a state, um,

47
00:02:15,110 --> 00:02:17,960
and/or an action, [NOISE] and then following a particular policy.

48
00:02:17,960 --> 00:02:21,830
[NOISE] So what we're gonna do today is,

49
00:02:21,830 --> 00:02:24,410
sort of, um, build up for Markov Processes,

50
00:02:24,410 --> 00:02:26,900
um, up to Markov Decision Processes.

51
00:02:26,900 --> 00:02:28,640
And this build, I think,

52
00:02:28,640 --> 00:02:30,530
is sort of a nice one because it sort of allows one to

53
00:02:30,530 --> 00:02:32,870
think about what happens in the cases where you

54
00:02:32,870 --> 00:02:37,160
might not have control over the world but the world might still be evolving in some way.

55
00:02:37,160 --> 00:02:40,070
[NOISE] Um, and think about what the reward might be in those, sort of,

56
00:02:40,070 --> 00:02:43,820
processes, for an agent that is sort of passively experiencing the world.

57
00:02:43,820 --> 00:02:48,290
Um, and then we can start to think about the control problem of how the agent should be

58
00:02:48,290 --> 00:02:52,850
choosing to act in the world in order to maximize its expected discounted sum of rewards.

59
00:02:52,850 --> 00:02:58,310
[NOISE] So, what we're gonna focus about on today and, er,

60
00:02:58,310 --> 00:03:01,560
and most of the rest of the classes is this Markov Decision Process,

61
00:03:01,560 --> 00:03:04,870
um, where we think about an agent interacting with the world.

62
00:03:04,870 --> 00:03:06,600
So the agent gets to take actions,

63
00:03:06,600 --> 00:03:08,360
typically denoted by a,

64
00:03:08,360 --> 00:03:11,150
[NOISE] those affect the state of the world in some way, um,

65
00:03:11,150 --> 00:03:14,240
and then the agent receives back a state and a reward.

66
00:03:14,240 --> 00:03:16,760
So last time we talked about the fact that this could in

67
00:03:16,760 --> 00:03:20,035
fact be an observation, instead of a state.

68
00:03:20,035 --> 00:03:23,265
But then, when we think about the world being Markov,

69
00:03:23,265 --> 00:03:24,615
we're going to [NOISE] think of an agent,

70
00:03:24,615 --> 00:03:27,030
just focusing on the current state, um,

71
00:03:27,030 --> 00:03:29,640
so the most recent observation, like, you know,

72
00:03:29,640 --> 00:03:32,130
whether or not the robots laser range finders saying,

73
00:03:32,130 --> 00:03:34,105
that there are walls, to the left or right of it,

74
00:03:34,105 --> 00:03:37,280
as opposed to thinking of the full sequence of prior history

75
00:03:37,280 --> 00:03:40,820
of the sequences of actions taken and the observations received.

76
00:03:40,820 --> 00:03:43,670
[NOISE] Um, as we talked about last time

77
00:03:43,670 --> 00:03:46,970
but you can always incorporate [NOISE] the full history to make something Markov,

78
00:03:46,970 --> 00:03:48,260
um, [NOISE] but most of the time today,

79
00:03:48,260 --> 00:03:50,435
we'll be thinking about, sort of, immediate sensors.

80
00:03:50,435 --> 00:03:52,130
If it's not clear, feel free to reach out.

81
00:03:52,130 --> 00:03:56,865
[NOISE] So, what did the Markov Process mean?

82
00:03:56,865 --> 00:03:59,120
The Markov process is to say that

83
00:03:59,120 --> 00:04:01,685
the state that the agent is using to make their decisions,

84
00:04:01,685 --> 00:04:03,710
is the sufficient [NOISE] statistic of the history.

85
00:04:03,710 --> 00:04:08,480
[NOISE] Which means that in order to predict the future distribution of states,

86
00:04:08,480 --> 00:04:09,635
on the next time step.

87
00:04:09,635 --> 00:04:11,630
Here we're using t to denote time step.

88
00:04:11,630 --> 00:04:17,880
[NOISE] That given our current state s_t,

89
00:04:17,880 --> 00:04:19,740
and the action that is taken a_t,

90
00:04:19,740 --> 00:04:21,690
[NOISE] this is again the action,

91
00:04:21,690 --> 00:04:24,965
[NOISE] um, that this is equivalent to,

92
00:04:24,965 --> 00:04:27,020
if we'd actually remember the entire history,

93
00:04:27,020 --> 00:04:29,660
where the history recall was gonna be the sequence

94
00:04:29,660 --> 00:04:33,350
of all the previous actions and rewards.

95
00:04:33,350 --> 00:04:37,490
And next states that we have seen up until the current time point.

96
00:04:37,490 --> 00:04:41,180
[NOISE] And so essentially, it allows us to say that,

97
00:04:41,180 --> 00:04:43,460
the future is independent of the past given

98
00:04:43,460 --> 00:04:46,500
some current aggregate statistic about the present.

99
00:04:46,500 --> 00:04:52,790
[NOISE] So when we think about a Markov Process or a Markov Chain,

100
00:04:52,790 --> 00:04:54,815
we don't think of there being any control yet.

101
00:04:54,815 --> 00:04:56,015
There's no actions.

102
00:04:56,015 --> 00:04:57,230
Um, but the idea is that,

103
00:04:57,230 --> 00:05:00,020
you might have a stochastic process that's evolving over time.

104
00:05:00,020 --> 00:05:03,530
[NOISE] Um, so whether or not I invest in the stock market,

105
00:05:03,530 --> 00:05:05,330
the stock market is changing over time.

106
00:05:05,330 --> 00:05:07,460
And you could think of that as a Markov Process,

107
00:05:07,460 --> 00:05:09,320
[NOISE] um, so I could just, sort of be,

108
00:05:09,320 --> 00:05:12,790
passively observing how the stock market for a particular,

109
00:05:12,790 --> 00:05:14,600
th- the stock value for a particular stock,

110
00:05:14,600 --> 00:05:15,650
is changing over time.

111
00:05:15,650 --> 00:05:19,730
[NOISE] Um, and a Markov Chain is,

112
00:05:19,730 --> 00:05:22,460
is sort of just the sequence of random states,

113
00:05:22,460 --> 00:05:26,020
where the transition dynamics satisfies this Markov property.

114
00:05:26,020 --> 00:05:29,390
So formally, the definition of a Markov Process is that,

115
00:05:29,390 --> 00:05:32,495
you have, um, a finite or potentially infinite set of states.

116
00:05:32,495 --> 00:05:34,430
And you have a dynamics model which

117
00:05:34,430 --> 00:05:37,400
specifies the probability for the next state given the previous state.

118
00:05:37,400 --> 00:05:38,930
[NOISE] There's no rewards,

119
00:05:38,930 --> 00:05:40,220
there's no actions yet.

120
00:05:40,220 --> 00:05:42,310
Um, and if you have a finite set of states,

121
00:05:42,310 --> 00:05:44,150
you can just write this down as a matrix.

122
00:05:44,150 --> 00:05:46,010
Just a transition matrix that says,

123
00:05:46,010 --> 00:05:47,345
you're starting at some state.

124
00:05:47,345 --> 00:05:50,540
What's the probability distribution over next states that you could reach?

125
00:05:50,540 --> 00:05:57,020
[NOISE] So if we go back to the Mars Rover example that we talked about last time.

126
00:05:57,020 --> 00:05:58,880
[NOISE] Um, In this little Mars Rover example,

127
00:05:58,880 --> 00:06:00,770
we thought of a Mars Rover landing on Mars

128
00:06:00,770 --> 00:06:03,095
and there might be different sorts of landing sites,

129
00:06:03,095 --> 00:06:06,900
um, so maybe our Mars Rovers starts off here.

130
00:06:07,250 --> 00:06:11,130
And then, it can go to the left or right, um, er,

131
00:06:11,130 --> 00:06:14,900
under different actions or we could just think of those actions as being a_1 or a_2,

132
00:06:14,900 --> 00:06:16,520
where it's trying to act in the world.

133
00:06:16,520 --> 00:06:19,635
[NOISE] Um, and in this case,

134
00:06:19,635 --> 00:06:21,460
uh, the transition dynamics, it doesn't,

135
00:06:21,460 --> 00:06:22,880
we don't actually have actions yet,

136
00:06:22,880 --> 00:06:24,365
and we just think of it as, sort of,

137
00:06:24,365 --> 00:06:25,750
maybe it already has some way,

138
00:06:25,750 --> 00:06:27,860
it's moving in the world, the motors are just working.

139
00:06:27,860 --> 00:06:29,900
[NOISE] And so in this case,

140
00:06:29,900 --> 00:06:31,945
the transition dynamics looks like this,

141
00:06:31,945 --> 00:06:33,885
which says that, for example,

142
00:06:33,885 --> 00:06:35,570
the way you could read this, is you could say, well,

143
00:06:35,570 --> 00:06:39,065
the probability that I start in a particular state s_1, um,

144
00:06:39,065 --> 00:06:43,520
and then, I can transition to the next state on the next time step is 0.4.

145
00:06:43,520 --> 00:06:48,090
[NOISE] There is a 0.6 chance that I stay in the same state on the next time step. Yeah?

146
00:06:48,090 --> 00:06:52,150
[NOISE] Um, which dimension represents the start state?

147
00:06:52,150 --> 00:06:54,600
Um, so, this is a great question.

148
00:06:54,600 --> 00:06:56,810
Which dimension, which, which state is the start state?

149
00:06:56,810 --> 00:06:58,625
[NOISE] I'm not specifying that here.

150
00:06:58,625 --> 00:07:01,610
Um, uh, In general when we think about Markov chains,

151
00:07:01,610 --> 00:07:04,355
we think about looking at their steady-state distribution.

152
00:07:04,355 --> 00:07:05,960
So they're stationary distribution will [NOISE]

153
00:07:05,960 --> 00:07:08,000
converge to some distribution over states,

154
00:07:08,000 --> 00:07:09,860
[NOISE] that is independent of the start state,

155
00:07:09,860 --> 00:07:11,095
if you run it for long enough.

156
00:07:11,095 --> 00:07:12,360
Oh, sorry, I meant to ask,

157
00:07:12,360 --> 00:07:13,410
like, on that matrix,

158
00:07:13,410 --> 00:07:16,950
which dimension represents the initial state of-

159
00:07:16,950 --> 00:07:18,375
Oh, you mean, like, where you are now right now?

160
00:07:18,375 --> 00:07:20,340
Yeah. So in this particular case,

161
00:07:20,340 --> 00:07:22,590
you could have it as, um,

162
00:07:22,590 --> 00:07:24,300
the transition of saying,

163
00:07:24,300 --> 00:07:26,250
if you start in state,

164
00:07:26,250 --> 00:07:28,650
[NOISE] uh, let me make sure that I get it right.

165
00:07:28,650 --> 00:07:31,095
In this case, [NOISE] answer there, there,

166
00:07:31,095 --> 00:07:34,920
so if you start in state here,

167
00:07:34,920 --> 00:07:38,135
um, so this is yours initial start at a state s_1

168
00:07:38,135 --> 00:07:43,525
and then you take the dot product of that with,

169
00:07:43,525 --> 00:07:46,665
I may have mo- let me see if I get it right in terms of mixing it up.

170
00:07:46,665 --> 00:07:48,090
It's either on one side or the other side,

171
00:07:48,090 --> 00:07:49,680
and then, I may have transitioned it.

172
00:07:49,680 --> 00:07:53,855
Um, I think you'll have to do it for the [NOISE] other side here.

173
00:07:53,855 --> 00:07:55,160
Yep, it'll be flipped.

174
00:07:55,160 --> 00:07:56,915
So, you would have your initial state.

175
00:07:56,915 --> 00:07:58,815
So 1, 0, 0,

176
00:07:58,815 --> 00:08:00,750
0, 0, 0, 1, 2, 3, 4,

177
00:08:00,750 --> 00:08:02,985
5, 6, and then times P,

178
00:08:02,985 --> 00:08:04,695
and that would give you your next state

179
00:08:04,695 --> 00:08:07,470
distribution s'. Yeah? [NOISE]

180
00:08:07,470 --> 00:08:10,980
Um, um, so what are the probabilities computed of,

181
00:08:10,980 --> 00:08:13,310
like the rewards, I guess, the probability,

182
00:08:13,310 --> 00:08:16,215
based on the reward of going from state 1 to 2 [NOISE] or?

183
00:08:16,215 --> 00:08:17,580
Great question, so was, you know,

184
00:08:17,580 --> 00:08:19,200
one of this transition probabilities looking

185
00:08:19,200 --> 00:08:20,940
at [NOISE] this relate to their word, in this case,

186
00:08:20,940 --> 00:08:22,200
we're just thinking of Markov Chains,

187
00:08:22,200 --> 00:08:24,330
so there's no reward yet, and there's no actions.

188
00:08:24,330 --> 00:08:27,330
[NOISE] Um, and this is just specifying that there's some state of the,

189
00:08:27,330 --> 00:08:28,605
uh, of the process.

190
00:08:28,605 --> 00:08:30,840
So it's as if you're,

191
00:08:30,840 --> 00:08:32,690
let's say your agent, um,

192
00:08:32,690 --> 00:08:34,640
had some configuration of its motors.

193
00:08:34,640 --> 00:08:35,809
[NOISE] You don't know what that is,

194
00:08:35,809 --> 00:08:38,344
that was set down on Mars, and then it just starts moving about.

195
00:08:38,345 --> 00:08:39,980
And what this would say is,

196
00:08:39,980 --> 00:08:41,990
this is the transition probabilities of if

197
00:08:41,990 --> 00:08:44,495
that agent starts in state, I can write it this way.

198
00:08:44,495 --> 00:08:46,460
So, if it starts in state,

199
00:08:46,460 --> 00:08:53,060
[NOISE] s_1, then the probability that it stays in state s_1 is 0.6.

200
00:08:53,060 --> 00:08:55,910
So, the probability that you're starting in this particular state here,

201
00:08:55,910 --> 00:08:57,980
[NOISE] on the next time step that you're still there,

202
00:08:57,980 --> 00:09:01,940
is 0.6 because of whatever configuration of the motors were for that robot.

203
00:09:01,940 --> 00:09:04,445
[inaudible] world works.

204
00:09:04,445 --> 00:09:06,965
This is specifying that, this is how, yeah,

205
00:09:06,965 --> 00:09:09,230
this is how the world works. So that's a great question.

206
00:09:09,230 --> 00:09:11,375
So we're assuming right now, this is, um, the,

207
00:09:11,375 --> 00:09:14,385
this Markov process is a state of the world that you were,

208
00:09:14,385 --> 00:09:15,705
there is some the,

209
00:09:15,705 --> 00:09:18,450
the environment you're in is just described as a Markov Process,

210
00:09:18,450 --> 00:09:20,870
and this describes the dynamics of that process.

211
00:09:20,870 --> 00:09:22,700
We're not talking about how you would estimate those.

212
00:09:22,700 --> 00:09:24,865
This is really as if, this is how that world works.

213
00:09:24,865 --> 00:09:26,240
This is, like, this is the,

214
00:09:26,240 --> 00:09:28,700
this is the world of the fake little Mars Rover.

215
00:09:28,700 --> 00:09:31,645
[NOISE] We have any questions about that? Yeah?

216
00:09:31,645 --> 00:09:34,410
Uh, [NOISE] the serum one cloud action needs to be

217
00:09:34,410 --> 00:09:38,220
transposed [NOISE] when you multiplied by P and all [NOISE] we can see is [inaudible]

218
00:09:38,220 --> 00:09:42,900
Yes. Yeah. [inaudible] [NOISE] Let me just write down and correct vector notation.

219
00:09:45,720 --> 00:09:47,890
Would be like this.

220
00:09:47,890 --> 00:09:51,595
One, one two, three, four, five, six.

221
00:09:51,595 --> 00:09:56,605
That would be, that would be a sample starting state you could be in for example.

222
00:09:56,605 --> 00:09:59,720
So, this could be your initial state.

223
00:09:59,760 --> 00:10:05,420
Initial state and that would mean that your agent is initially in state S one.

224
00:10:06,240 --> 00:10:11,845
Okay. And then if you want to know where it might be on the next state,

225
00:10:11,845 --> 00:10:14,980
you would multiply that by the transition model P

226
00:10:14,980 --> 00:10:20,110
depending on the notation and whether you take the transpose of this transition model,

227
00:10:20,110 --> 00:10:22,015
it will be on the left or the right.

228
00:10:22,015 --> 00:10:25,150
It should always be obvious from context but if it's not clear,

229
00:10:25,150 --> 00:10:27,415
feel free to ask us. And so what would that say?

230
00:10:27,415 --> 00:10:28,990
That would say if you took the, uh,

231
00:10:28,990 --> 00:10:31,720
the matrix multiplication of this vector which just

232
00:10:31,720 --> 00:10:34,180
says you're starting in state s_1, what would that look like?

233
00:10:34,180 --> 00:10:43,059
Afterwards it would say that you are in state s_1 still with probability 0.6,

234
00:10:43,059 --> 00:10:46,370
you're in state s_2 with probably to 0.4.

235
00:10:48,570 --> 00:10:50,740
And this would be your new (state distribution).

236
00:10:50,740 --> 00:10:52,900
And I think that should be transposed.

237
00:10:52,900 --> 00:10:55,690
But it's just a one it which specify

238
00:10:55,690 --> 00:10:58,730
the distribution over next states that you would be in.

239
00:10:59,520 --> 00:11:02,750
You may have any questions about that?

240
00:11:03,810 --> 00:11:05,830
Okay. All right.

241
00:11:05,830 --> 00:11:08,380
So, this is just specifying that the transition model

242
00:11:08,380 --> 00:11:11,590
over how the world works over time and it's just I,

243
00:11:11,590 --> 00:11:14,095
I've written it in matrix notation there to be compact.

244
00:11:14,095 --> 00:11:15,760
But if it's easier to think about it,

245
00:11:15,760 --> 00:11:17,440
it's fine to just think about it in terms of

246
00:11:17,440 --> 00:11:20,350
these probability of next states given the previous state.

247
00:11:20,350 --> 00:11:22,420
And so you can just enumerate those,

248
00:11:22,420 --> 00:11:25,090
you can write it in a matrix form if the,

249
00:11:25,090 --> 00:11:27,580
if the number of states happens to be finite.

250
00:11:27,580 --> 00:11:31,000
So, what would this look like if you wanted to think of what might happen to

251
00:11:31,000 --> 00:11:33,985
the agent over time in this case or what the process might look like,

252
00:11:33,985 --> 00:11:35,695
you could just sample episodes.

253
00:11:35,695 --> 00:11:38,470
So, let's say that your initial starting state is S four,

254
00:11:38,470 --> 00:11:40,000
and then you could say, well,

255
00:11:40,000 --> 00:11:41,440
I can write that as a one-hot vector.

256
00:11:41,440 --> 00:11:43,405
I multiply it by my probability.

257
00:11:43,405 --> 00:11:46,270
And that gives me some probability distribution over the next states that

258
00:11:46,270 --> 00:11:49,195
I might be in and the world will sample one of those.

259
00:11:49,195 --> 00:11:52,075
So, your agent can't be in multiple states at the same time.

260
00:11:52,075 --> 00:11:55,555
So, for example, if we were looking at state s_1,

261
00:11:55,555 --> 00:12:00,055
it has a 0.6 chance to abstain in s_1 or 0.4 chance of transitioning.

262
00:12:00,055 --> 00:12:04,420
So, the world will sample one of those two outcomes for you and it might be state s_1.

263
00:12:04,420 --> 00:12:07,675
So in this case, we have similar dynamics from s_4.

264
00:12:07,675 --> 00:12:12,055
From s_4, it has a probability of 0.4 going to state s_3.

265
00:12:12,055 --> 00:12:14,575
Probability of 0.4 going to state s_4

266
00:12:14,575 --> 00:12:17,320
or a probability of 0.2 of staying in the same place.

267
00:12:17,320 --> 00:12:22,105
So, if we were going to sample an episode of what might happen to the agent over time,

268
00:12:22,105 --> 00:12:25,330
you can start with s_4 then maybe it will transition to s_5.

269
00:12:25,330 --> 00:12:26,740
Maybe they'll go to s_6,

270
00:12:26,740 --> 00:12:28,420
s_7, s_7, s_7.

271
00:12:28,420 --> 00:12:33,010
So, you're just sampling from this transition matrix to generate a particular trajectory.

272
00:12:33,010 --> 00:12:36,415
So, it's like the world you know what the dynamics,

273
00:12:36,415 --> 00:12:40,315
the dynamics is of the world and then nature is gonna pick one of those outcomes.

274
00:12:40,315 --> 00:12:43,870
It's like sampling from sort of a probability distribution.

275
00:12:43,870 --> 00:12:46,930
Anyone having questions about that?

276
00:12:46,930 --> 00:12:50,755
Okay. So, that just gives you a particular episode.

277
00:12:50,755 --> 00:12:53,170
And we're going to be interested in episodes because later we're gonna be

278
00:12:53,170 --> 00:12:55,450
thinking about rewards over those episodes and

279
00:12:55,450 --> 00:13:00,205
how do we compare the rewards we might achieve over those episodes but for right now,

280
00:13:00,205 --> 00:13:01,930
this is just a process.

281
00:13:01,930 --> 00:13:05,020
This is just giving you a sequence of states.

282
00:13:05,020 --> 00:13:07,780
So, next we're gonna add in rewards.

283
00:13:07,780 --> 00:13:09,505
So, that was just a Markov chain.

284
00:13:09,505 --> 00:13:12,310
And so now what is a Markov reward process?

285
00:13:12,310 --> 00:13:15,520
Again, we don't have actions yet just like before.

286
00:13:15,520 --> 00:13:18,115
But now we also have a reward function.

287
00:13:18,115 --> 00:13:21,085
So, we still have a dynamics model like before.

288
00:13:21,085 --> 00:13:22,930
And now we have a reward function that says,

289
00:13:22,930 --> 00:13:24,370
if you're in a particular state,

290
00:13:24,370 --> 00:13:28,030
what is the expected reward you get from being in that state?

291
00:13:28,030 --> 00:13:32,260
We can also now have a discount factor which allows us to trade off

292
00:13:32,260 --> 00:13:34,540
between or allows us to think about how

293
00:13:34,540 --> 00:13:37,360
much we weight immediate rewards versus feature rewards.

294
00:13:37,360 --> 00:13:38,950
So, again just like before,

295
00:13:38,950 --> 00:13:41,440
if we have a finite number of states in

296
00:13:41,440 --> 00:13:44,020
this case R can be represented in matrix notation which

297
00:13:44,020 --> 00:13:50,090
is just a vector because it's just the expected reward we get for being in each state.

298
00:13:50,280 --> 00:13:53,860
So, if we look at the Mars Rover MRP,

299
00:13:53,860 --> 00:14:00,085
then we could say that the reward for being an s_1 is equal to 1.

300
00:14:00,085 --> 00:14:02,830
The reward for being an s_7 is equal to

301
00:14:02,830 --> 00:14:07,020
10 and everything else that reward is zero. Yeah.

302
00:14:07,020 --> 00:14:10,575
Are the words always just tied to the state you're in?

303
00:14:10,575 --> 00:14:13,545
I think last time you talked about it also having an option.

304
00:14:13,545 --> 00:14:15,605
So, why are we not consider that here?

305
00:14:15,605 --> 00:14:17,950
Great question. I'm saying that I mentioned last time that

306
00:14:17,950 --> 00:14:20,950
rewards for the Markov Decision Process can either be a function of the state,

307
00:14:20,950 --> 00:14:22,720
the state in action, or state action next state.

308
00:14:22,720 --> 00:14:26,255
Right now we're still in Markov Reward Processes so there's no action.

309
00:14:26,255 --> 00:14:28,185
So, in this case,

310
00:14:28,185 --> 00:14:30,450
the ways you could define rewards would either

311
00:14:30,450 --> 00:14:33,790
be over the immediate state or state and next state.

312
00:14:36,360 --> 00:14:39,400
So, once we start to think about there being rewards,

313
00:14:39,400 --> 00:14:43,480
we can start to think about there being returns and expected returns.

314
00:14:43,480 --> 00:14:46,045
So, first of all let's define what a horizon is.

315
00:14:46,045 --> 00:14:49,210
A horizon is just the number of time steps in an episode.

316
00:14:49,210 --> 00:14:54,055
So, it's sort of like how long the agent is acting for or how long it,

317
00:14:54,055 --> 00:14:58,015
how long this process is going on for it and it could be infinite.

318
00:14:58,015 --> 00:14:59,800
So, if it's not infinite,

319
00:14:59,800 --> 00:15:02,140
then we call it a finite Markov Decision Process.

320
00:15:02,140 --> 00:15:03,820
We talked about those briefly last time.

321
00:15:03,820 --> 00:15:07,105
Um, but it often we think about the case where,

322
00:15:07,105 --> 00:15:10,840
um, an agent might be acting forever or this process might be going on forever.

323
00:15:10,840 --> 00:15:12,490
There's no termination of it.

324
00:15:12,490 --> 00:15:14,140
The stock market is up today.

325
00:15:14,140 --> 00:15:16,795
It'll be up tomorrow. We expect it to be up for a long time.

326
00:15:16,795 --> 00:15:22,525
We're not necessarily tried to think about evaluating it over a short time period.

327
00:15:22,525 --> 00:15:24,535
One might wanna think about evaluating it over

328
00:15:24,535 --> 00:15:27,805
a very long time period. So, we've done this.

329
00:15:27,805 --> 00:15:31,585
The definition of a return is just the discounted sum of rewards you get

330
00:15:31,585 --> 00:15:36,040
from the current time step to a horizon and that horizon could be infinite.

331
00:15:36,040 --> 00:15:38,410
So, a return just says,

332
00:15:38,410 --> 00:15:40,510
if I start off in time step T,

333
00:15:40,510 --> 00:15:44,380
what is the immediate reward I get and then I transition maybe to

334
00:15:44,380 --> 00:15:49,570
a new state and then I weigh that return reward by Gamma.

335
00:15:49,570 --> 00:15:55,250
And then I transitioned again and I weigh that one by Gamma squared, et cetera.

336
00:15:55,260 --> 00:16:00,475
And then the definition of a value function is just the expected return.

337
00:16:00,475 --> 00:16:03,250
If the process is deterministic,

338
00:16:03,250 --> 00:16:05,260
these two things will be identical.

339
00:16:05,260 --> 00:16:10,210
But in general if the process is stochastic, they will be different.

340
00:16:10,210 --> 00:16:14,635
So, what I mean by deterministic is that if you always go to the same next state,

341
00:16:14,635 --> 00:16:16,870
no matter which if you start at a state

342
00:16:16,870 --> 00:16:18,760
if there's only a single next state you can go to,

343
00:16:18,760 --> 00:16:23,005
uh, then the expectation is equivalent to a single return.

344
00:16:23,005 --> 00:16:24,910
But in the general case, we are gonna be interested in

345
00:16:24,910 --> 00:16:27,310
these stochastic decision processes which

346
00:16:27,310 --> 00:16:30,505
means averages will be different than particularly runs.

347
00:16:30,505 --> 00:16:32,575
So, for an example of that well,

348
00:16:32,575 --> 00:16:35,650
let me first just talk about discount factor and then I'll give an example.

349
00:16:35,650 --> 00:16:37,795
Discount factors are a little bit tricky.

350
00:16:37,795 --> 00:16:42,235
They're both sort of somewhat motivated and somewhat used for mathematical convenience.

351
00:16:42,235 --> 00:16:46,090
So, we'll see later one of the benefits of mathematic, uh,

352
00:16:46,090 --> 00:16:48,580
benefits of discount factors mathematically is that we can

353
00:16:48,580 --> 00:16:51,100
be sure that the value function sort of expected

354
00:16:51,100 --> 00:16:56,995
discounted sum of returns is bounded as long as here reward function is bounded.

355
00:16:56,995 --> 00:17:00,775
Uh, people empirically often act as if there is a discount factor.

356
00:17:00,775 --> 00:17:05,005
We weigh future rewards lower than,

357
00:17:05,005 --> 00:17:06,609
than immediate rewards typically.

358
00:17:06,609 --> 00:17:08,379
Businesses often do the same.

359
00:17:08,380 --> 00:17:10,119
If Gamma is equal to 0,

360
00:17:10,119 --> 00:17:12,309
you only care about immediate reward.

361
00:17:12,310 --> 00:17:14,515
So, you're the agent is acting myopically.

362
00:17:14,515 --> 00:17:18,115
It's not thinking about the future of what could happen later on.

363
00:17:18,115 --> 00:17:19,869
And if Gamma is equal to one,

364
00:17:19,869 --> 00:17:21,819
then that means that your future rewards are

365
00:17:21,819 --> 00:17:25,194
exactly as beneficial to you as the immediate rewards.

366
00:17:25,194 --> 00:17:27,159
Now, one thing just to note,

367
00:17:27,160 --> 00:17:31,015
if you're only using discount factors for mathematical convenience, um,

368
00:17:31,015 --> 00:17:34,780
if your horizon is always guaranteed to be finite,

369
00:17:34,780 --> 00:17:37,390
it's fine to use gamma equal to one in terms

370
00:17:37,390 --> 00:17:40,850
of from a perspective mathematical convenience.

371
00:17:41,700 --> 00:17:45,025
Someone having any questions about discount factors? Yeah.

372
00:17:45,025 --> 00:17:49,510
My question is, does the discount factor of Gamma always have to progress

373
00:17:49,510 --> 00:17:54,145
in a geometric fashion or like is there a reason why we do that?

374
00:17:54,145 --> 00:17:56,170
It's a great question. You know,

375
00:17:56,170 --> 00:17:59,620
the- what we're defining here is that using a Gamma

376
00:17:59,620 --> 00:18:03,940
that progresses through this exponential geometric fashion is that necessary.

377
00:18:03,940 --> 00:18:08,930
It's one nice choice that ends up having very nice mathematical properties.

378
00:18:08,940 --> 00:18:12,910
There, one could try using other participant is certainly

379
00:18:12,910 --> 00:18:14,470
the most common one and we'll see

380
00:18:14,470 --> 00:18:16,540
later why it has some really nice mathematical properties.

381
00:18:16,540 --> 00:18:21,935
Any other questions? Okay.

382
00:18:21,935 --> 00:18:24,360
So, what would be some examples of this?

383
00:18:24,360 --> 00:18:29,205
Um, if we go back to our Mars Rover here and we now have this definition of reward,

384
00:18:29,205 --> 00:18:31,290
um, what would be a sample return?

385
00:18:31,290 --> 00:18:35,700
So, let's imagine that we start off in state s_4 and then we transitioned to s_5,

386
00:18:35,700 --> 00:18:38,955
s_6, s_7 and we only have four-step returns.

387
00:18:38,955 --> 00:18:41,790
So, what that means here is that our, um,

388
00:18:41,790 --> 00:18:48,015
our process only continues for four time steps and then it maybe it resets.

389
00:18:48,015 --> 00:18:50,640
So, why might something like that be reasonable?

390
00:18:50,640 --> 00:18:53,715
Well, particularly when we start to get into decision-making, um you know,

391
00:18:53,715 --> 00:18:58,695
maybe customers interact with the website for on average two or three times steps.

392
00:18:58,695 --> 00:19:03,030
Um, there's often a bounded number of time you know bounded

393
00:19:03,030 --> 00:19:07,590
length of course in many many cases that the horizon is naturally bounded.

394
00:19:07,590 --> 00:19:11,520
So, in this case you know what might happen in this scenario we start off in s_4.

395
00:19:11,520 --> 00:19:15,345
s_4, s_5, s_6 all have zero rewards by definition.

396
00:19:15,345 --> 00:19:19,095
Um, and then on time-step s_7 we get a reward of 10.

397
00:19:19,095 --> 00:19:24,600
But that has to be weighed down by the discount factor which here is 1/2.

398
00:19:24,600 --> 00:19:27,460
So, it's 1/2 to the power of 3.

399
00:19:28,280 --> 00:19:32,340
And so the sample return for this particular episode is just 1.25.

400
00:19:32,340 --> 00:19:37,140
[NOISE] And of course we could define this for any particular, um,

401
00:19:37,140 --> 00:19:40,710
episode and these episodes generally might go through different states even

402
00:19:40,710 --> 00:19:42,285
if they're starting in the same initial state

403
00:19:42,285 --> 00:19:44,580
because we have a stochastic transition model.

404
00:19:44,580 --> 00:19:48,090
So, in this case maybe the agent just stays in s_4,

405
00:19:48,090 --> 00:19:50,895
s_4, s_5, s_4 and it doesn't get any reward.

406
00:19:50,895 --> 00:19:52,335
And in other cases,

407
00:19:52,335 --> 00:19:55,660
um, it might go all the way to the left.

408
00:19:55,880 --> 00:20:00,765
So, if we then think about what the expected value function would be,

409
00:20:00,765 --> 00:20:04,620
it would involve averaging over a lot of these.

410
00:20:04,620 --> 00:20:07,980
And as we average over all of these, um,

411
00:20:07,980 --> 00:20:12,640
then we can start to get different rewards for different time steps.

412
00:20:17,810 --> 00:20:20,685
So, how would we compute this?

413
00:20:20,685 --> 00:20:22,800
Um, now one thing you could do which is sort of

414
00:20:22,800 --> 00:20:24,915
motivated by what I would just showing before,

415
00:20:24,915 --> 00:20:27,645
is that you could estimate it by simulation.

416
00:20:27,645 --> 00:20:31,320
So, you could, um,

417
00:20:31,320 --> 00:20:35,700
just take for say an initial starting state distribution, um,

418
00:20:35,700 --> 00:20:37,650
which could be just a single starting state or

419
00:20:37,650 --> 00:20:40,815
many starting states and you could just roll out your process.

420
00:20:40,815 --> 00:20:42,570
So, right now we're assuming that we have

421
00:20:42,570 --> 00:20:46,575
a transition model transition matrix and a reward model.

422
00:20:46,575 --> 00:20:48,600
Um, and you could just roll this out just like what

423
00:20:48,600 --> 00:20:51,030
we're showing on the previous couple of time-steps.

424
00:20:51,030 --> 00:20:55,575
And you could just do this many many many times. And then average.

425
00:20:55,575 --> 00:20:58,230
And that would asymptotically

426
00:20:58,230 --> 00:21:01,560
converge to what the value function is cause the value function is just,

427
00:21:01,560 --> 00:21:04,695
um, the expected return.

428
00:21:04,695 --> 00:21:07,575
So, one thing you could do with simulation, um,

429
00:21:07,575 --> 00:21:09,570
and there are mathematical bounds you can

430
00:21:09,570 --> 00:21:11,820
use to say how many simulations would you need to

431
00:21:11,820 --> 00:21:17,680
do in order for your empirical average to be close to the true expected value.

432
00:21:17,780 --> 00:21:22,020
The accuracy roughly goes down on the order of one

433
00:21:22,020 --> 00:21:25,665
over square root of N where N is the number of roll-outs you've done.

434
00:21:25,665 --> 00:21:27,150
So, it just tells you that, you know,

435
00:21:27,150 --> 00:21:32,070
if you want to figure out what the value is of your Markov Reward Process,

436
00:21:32,070 --> 00:21:35,460
um, you could just do simulations and that would give you an estimate of the value.

437
00:21:35,460 --> 00:21:37,080
The nice thing about doing this,

438
00:21:37,080 --> 00:21:40,185
is this requires no assumption of the Markov structure.

439
00:21:40,185 --> 00:21:43,755
Not actually using the fact that it's a Markov Reward Process at all.

440
00:21:43,755 --> 00:21:48,855
It's just a way to estimate sums of returns- sums up rewards.

441
00:21:48,855 --> 00:21:52,425
So, that's both nice in the sense that, um,

442
00:21:52,425 --> 00:21:55,500
if you're using this in a process that you had estimated from

443
00:21:55,500 --> 00:21:58,980
some data or you're making the assumption that things are, er, um,

444
00:21:58,980 --> 00:22:00,990
you know this is the dynamics model but that's also

445
00:22:00,990 --> 00:22:03,570
estimated from data and it might be wrong, um,

446
00:22:03,570 --> 00:22:05,655
then this can give you sort of, um,

447
00:22:05,655 --> 00:22:08,280
if you can really roll out in the world then you can get these sort

448
00:22:08,280 --> 00:22:11,445
of nice estimates of really how the process is working.

449
00:22:11,445 --> 00:22:15,299
But it doesn't leverage anything about the fact that if the world really is Markov,

450
00:22:15,299 --> 00:22:18,630
um, there's additional structure we could do in order to get better estimates.

451
00:22:18,630 --> 00:22:20,580
So, what do I mean by better estimates here?

452
00:22:20,580 --> 00:22:22,470
I mean if we want to, um,

453
00:22:22,470 --> 00:22:26,100
get sort of better meaning sort of computationally cheaper,

454
00:22:26,100 --> 00:22:30,075
um, ways of estimating what the value is a process.

455
00:22:30,075 --> 00:22:32,895
So, what the Markov structure allows us to do,

456
00:22:32,895 --> 00:22:34,860
with the fact that the present that

457
00:22:34,860 --> 00:22:37,230
the future is independent of the past given the present,

458
00:22:37,230 --> 00:22:39,870
is it allows us to decompose the value function.

459
00:22:39,870 --> 00:22:43,785
So, the value function of a mark forward process is simply

460
00:22:43,785 --> 00:22:45,690
the immediate reward the agent gets from

461
00:22:45,690 --> 00:22:48,300
the current state it's in plus the discounted sum of

462
00:22:48,300 --> 00:22:50,910
future rewards weighed by

463
00:22:50,910 --> 00:22:56,405
the discount factor times

464
00:22:56,405 --> 00:22:59,540
the- and where we express that discounted sum of future words is we

465
00:22:59,540 --> 00:23:02,890
can just express it with V, V(s').

466
00:23:02,890 --> 00:23:05,970
So, we sort of say well whatever state you're in right now,

467
00:23:05,970 --> 00:23:07,860
you're going to get your immediate word and then you're going to

468
00:23:07,860 --> 00:23:09,990
transition to some state s'.

469
00:23:09,990 --> 00:23:12,420
Um, and then you're going to get the value of

470
00:23:12,420 --> 00:23:17,320
whatever state s' you ended up in discounted by our discount factor.

471
00:23:18,890 --> 00:23:25,380
So, if we're in a finite state MRP we can express this using matrix notation.

472
00:23:25,380 --> 00:23:28,950
So, we can say that the value function which is a vector is equal to

473
00:23:28,950 --> 00:23:32,160
the reward plus gamma times the transition model times

474
00:23:32,160 --> 00:23:40,590
V. Again note that in this case because of the way we're defining the transition model,

475
00:23:40,590 --> 00:23:42,390
um, then the value functions here

476
00:23:42,390 --> 00:23:44,490
the transition model is defined as the next [NOISE] state given

477
00:23:44,490 --> 00:23:50,370
the previous state and multiplying that by the value function there.

478
00:23:50,370 --> 00:23:53,850
So, in this case we can express it just using a matrix notation.

479
00:23:53,850 --> 00:23:56,250
Um, and the nice thing is that once we've done that

480
00:23:56,250 --> 00:23:58,815
we can just analytically solve for the value function.

481
00:23:58,815 --> 00:24:00,435
So, remember all of this is known.

482
00:24:00,435 --> 00:24:04,905
So, this is known. And this is known.

483
00:24:04,905 --> 00:24:09,690
And what we're trying to do is to compute what V(S) is.

484
00:24:09,690 --> 00:24:13,455
So, what we can do in this case is we just move this over to the other side.

485
00:24:13,455 --> 00:24:18,330
So, you can do V minus gamma PV is equal to R or we can

486
00:24:18,330 --> 00:24:23,805
say the identity matrix minus the discount factor times P. These are all matrices.

487
00:24:23,805 --> 00:24:31,440
So, this is the identity matrix times

488
00:24:31,440 --> 00:24:39,375
V is equal to R which means V is just equal to the inverse of this matrix times R.

489
00:24:39,375 --> 00:24:44,340
Um. So, if one of the transitions can be back to itself,

490
00:24:44,340 --> 00:24:50,385
um wouldn't it be become a circular to try to express V(s) in terms of V(s)?

491
00:24:50,385 --> 00:24:54,900
Um, the question was was if it's possible to have self-loops?

492
00:24:54,900 --> 00:24:59,040
Um, could it be that this is sort of circulator defined [NOISE] in this case.

493
00:24:59,040 --> 00:25:04,380
Um, I in this case because we're thinking about processes that are infinite horizon,

494
00:25:04,380 --> 00:25:06,570
the value function is stationary, um,

495
00:25:06,570 --> 00:25:09,615
and it's fine if you have include self loops.

496
00:25:09,615 --> 00:25:12,240
So, it's fine if some of the states that you

497
00:25:12,240 --> 00:25:14,745
might transition back to the same state there's no problem.

498
00:25:14,745 --> 00:25:17,085
You do need that this matrix is well-defined.

499
00:25:17,085 --> 00:25:19,665
That you can take that you can take the inverse of it.

500
00:25:19,665 --> 00:25:22,660
Um, but for most processes that is.

501
00:25:25,940 --> 00:25:29,910
Um, so, if we wanna solve this directly, um,

502
00:25:29,910 --> 00:25:31,845
this is nice it's analytic, um,

503
00:25:31,845 --> 00:25:34,425
but it requires taking a matrix inverse.

504
00:25:34,425 --> 00:25:40,050
And if you have N states so let's say you have N states there's generally

505
00:25:40,050 --> 00:25:42,570
on the order of somewhere between N squared and N cubed

506
00:25:42,570 --> 00:25:45,945
depending on which matrix inversion you're using. Yeah.

507
00:25:45,945 --> 00:25:48,375
Is it ever actually possible for, uh,

508
00:25:48,375 --> 00:25:51,930
that matrix not to have an inverse or does like the property

509
00:25:51,930 --> 00:25:55,770
that like column sum to one or something make it not possible?

510
00:25:55,770 --> 00:26:00,075
Question was is it ever possible for this not to have an inverse?

511
00:26:00,075 --> 00:26:03,030
Um, it's a it's a good question.

512
00:26:03,030 --> 00:26:06,225
Um, I think it's basically never possible for this not to have an inverse.

513
00:26:06,225 --> 00:26:10,965
I'm trying to think whether or not that can be violated in some cases.

514
00:26:10,965 --> 00:26:15,540
Um, if yeah sorry go ahead.

515
00:26:15,540 --> 00:26:18,300
Okay. [NOISE] Yeah.

516
00:26:18,300 --> 00:26:19,635
So, I think there's a couple,

517
00:26:19,635 --> 00:26:25,425
um, if there's a- if this ends up being the zero matrix,

518
00:26:25,425 --> 00:26:27,540
um depending on how things are defined.

519
00:26:27,540 --> 00:26:32,535
Um, but I'll double-check then send a note on a Piazza. Yeah.

520
00:26:32,535 --> 00:26:43,560
Well, actually I think the biggest side about the transition matrix [inaudible]

521
00:26:43,560 --> 00:26:46,260
Let me just double check so I don't say anything that's

522
00:26:46,260 --> 00:26:50,380
incorrect and then I'll just send a note on- on Piazza. It's a good question.

523
00:26:52,310 --> 00:26:55,785
So, that's the analytic way for computing this.

524
00:26:55,785 --> 00:26:58,605
The other way is to use dynamic programming.

525
00:26:58,605 --> 00:27:00,945
So, in this case,

526
00:27:00,945 --> 00:27:03,630
it's an iterative algorithm instead of a one shot.

527
00:27:03,630 --> 00:27:08,070
So, the idea in this scenario is that you initialize the value function to be

528
00:27:08,070 --> 00:27:12,780
zero everywhere and in fact you can initialize it to anything and it doesn't matter.

529
00:27:12,780 --> 00:27:15,209
If you're doing this until convergence.

530
00:27:15,209 --> 00:27:18,420
And so then what we're gonna do is we're going to

531
00:27:18,420 --> 00:27:20,370
do what's going to be close to

532
00:27:20,370 --> 00:27:23,085
something we're going to see later which is a bellman backup.

533
00:27:23,085 --> 00:27:27,150
So, the idea in this case is because of the Markov property,

534
00:27:27,150 --> 00:27:29,880
we've said that the value of a state is exactly equal to

535
00:27:29,880 --> 00:27:33,240
the immediate reward we get plus the discounted sum of future rewards.

536
00:27:33,240 --> 00:27:35,985
And in this case,

537
00:27:35,985 --> 00:27:41,040
we can simply use that to derive an iterative equation where we use the previous value of

538
00:27:41,040 --> 00:27:43,665
the state in order to bootstrap

539
00:27:43,665 --> 00:27:47,085
and compute the next value of the state and we do that for all states.

540
00:27:47,085 --> 00:27:51,030
And the computational complexity of this is a little bit lower because it's only

541
00:27:51,030 --> 00:27:53,700
|S| squared because you're doing this for each

542
00:27:53,700 --> 00:27:57,360
of the states and then you're summing over all the possible next states.

543
00:27:57,360 --> 00:28:00,060
When I say we do this total convergence

544
00:28:00,060 --> 00:28:02,760
generally what we do in this case is we define a norm.

545
00:28:02,760 --> 00:28:04,695
So, generally we would do something like this,

546
00:28:04,695 --> 00:28:08,740
V_k minus V_k-1.

547
00:28:09,020 --> 00:28:13,720
I need to do this until it's lower than some epsilon.

548
00:28:16,640 --> 00:28:20,460
So, the advantage of this is that each of the iteration updates are

549
00:28:20,460 --> 00:28:27,225
cheaper and they'd also will be some benefits later when we start to think about actions.

550
00:28:27,225 --> 00:28:29,970
The other thing does not apply as easily when we

551
00:28:29,970 --> 00:28:32,535
start to have actions but we'll see also where it can be relevant.

552
00:28:32,535 --> 00:28:35,040
So, here are two different ways to try to compute the value of

553
00:28:35,040 --> 00:28:39,254
Markov Reward Process or three really one is simulation,

554
00:28:39,254 --> 00:28:40,575
the second is analytically.

555
00:28:40,575 --> 00:28:42,990
The analytic one requires us a step

556
00:28:42,990 --> 00:28:46,530
a finite set of states and the third one is dynamic programming.

557
00:28:46,530 --> 00:28:50,700
We're also right now defining only all of these for when the state space is finite,

558
00:28:50,700 --> 00:28:54,520
but we'll talk about when the state space is infinite later on.

559
00:28:55,520 --> 00:28:59,310
So, now we can finally get onto Markov Decision Processes.

560
00:28:59,310 --> 00:29:01,530
Markov Decision Processes are the same as

561
00:29:01,530 --> 00:29:04,680
the Markov Reward Process except for now we have actions.

562
00:29:04,680 --> 00:29:06,960
So, we still have the dynamics model but now

563
00:29:06,960 --> 00:29:08,970
we have a dynamics model that is specified for

564
00:29:08,970 --> 00:29:13,080
each action separately and we also have a reward function.

565
00:29:13,080 --> 00:29:16,110
And as was asked before by Camilla I think,

566
00:29:16,110 --> 00:29:19,290
the reward can either be a function of the immediate state,

567
00:29:19,290 --> 00:29:22,950
the state and action to the state action and next state for most of the rest

568
00:29:22,950 --> 00:29:27,375
of today we'll be using that it's the function of both the state and action.

569
00:29:27,375 --> 00:29:30,570
So, the agent is in a state they take an action,

570
00:29:30,570 --> 00:29:31,845
they get immediate reward,

571
00:29:31,845 --> 00:29:34,080
and then they transition to the next state.

572
00:29:34,080 --> 00:29:37,290
So, if you think about serve an observation you'd see something like

573
00:29:37,290 --> 00:29:42,100
this s, a, r, and then transition to state s'.

574
00:29:43,220 --> 00:29:46,290
And so a Markov Decision Process is typically

575
00:29:46,290 --> 00:29:48,765
described as a tuple which is just the set of states,

576
00:29:48,765 --> 00:29:52,695
actions, rewards, dynamics, model, and discount factor.

577
00:29:52,695 --> 00:29:55,500
Because of the way you've defined that dynamic model,

578
00:29:55,500 --> 00:29:59,490
is the case that if you take a specific action that is

579
00:29:59,490 --> 00:30:01,530
intended for you to move to your state s',

580
00:30:01,530 --> 00:30:04,020
you won't fully successful move to that state?

581
00:30:04,020 --> 00:30:08,010
Like I guess I'm curious about why there's a- why there is a probability at all?

582
00:30:08,010 --> 00:30:10,260
Like if you're deep in a state in K action,

583
00:30:10,260 --> 00:30:13,035
why is it deterministic what the next state is?

584
00:30:13,035 --> 00:30:16,725
Question is same like well why is this- why are there stochastic processes I think.

585
00:30:16,725 --> 00:30:21,420
Um, there are a lot of cases where we don't have perfect models of the environment.

586
00:30:21,420 --> 00:30:24,180
May be if we had better models then things would be deterministic.

587
00:30:24,180 --> 00:30:28,320
And so, we're going to approximate our uncertainty over those models with stochasticity.

588
00:30:28,320 --> 00:30:30,690
So, maybe you have a robot that's a little bit faulty and so

589
00:30:30,690 --> 00:30:33,300
sometimes it gets stuck on carpet and then sometimes it goes forward.

590
00:30:33,300 --> 00:30:36,360
And we can write that down as a stochastic transition matrix

591
00:30:36,360 --> 00:30:40,410
where sometimes it stays in the same place and sometimes it advances to the next state.

592
00:30:40,410 --> 00:30:43,170
Or maybe you're on sand or things like that.

593
00:30:43,170 --> 00:30:47,085
Maybe when you're trying to drive to SFO sometimes you hit traffic, sometimes you don't.

594
00:30:47,085 --> 00:30:50,100
You can imagine putting a lot more variables into your state-space to

595
00:30:50,100 --> 00:30:53,085
try to make that a deterministic outcome or you could just say,

596
00:30:53,085 --> 00:30:54,780
"Hey sometimes when I try to go to work, you know,

597
00:30:54,780 --> 00:30:57,810
like I hit these number of red lights and so I'm late and other times,

598
00:30:57,810 --> 00:31:00,820
you know, I don't hit those red lights and so I'm fine."

599
00:31:03,820 --> 00:31:06,965
So, if we think about our Mars Rover MDP.

600
00:31:06,965 --> 00:31:09,920
Now, let's just define there being two actions A1 and A2.

601
00:31:09,920 --> 00:31:12,650
You can think about these things as the agent trying to move

602
00:31:12,650 --> 00:31:15,440
left or right but it's also perhaps

603
00:31:15,440 --> 00:31:18,210
easier just to think about in general them as sort of

604
00:31:18,210 --> 00:31:21,165
these deterministic actions for this particular example.

605
00:31:21,165 --> 00:31:24,030
So, we can write down what the transition matrix would be in each of

606
00:31:24,030 --> 00:31:26,400
these two cases that shows us

607
00:31:26,400 --> 00:31:29,880
exactly where the next state would be given the previous action.

608
00:31:29,880 --> 00:31:33,060
So, what's happening in this case is if the agent tries to do

609
00:31:33,060 --> 00:31:36,450
a_1 in state s_1 then it stays in that state.

610
00:31:36,450 --> 00:31:39,765
Otherwise, it will generally move to the next state over.

611
00:31:39,765 --> 00:31:42,660
If it's trying to do action a_1 and for action

612
00:31:42,660 --> 00:31:47,440
a_2 it'll move to the right unless it hits s_7 and then it'll stay there.

613
00:31:49,760 --> 00:31:52,935
So, like we said at the beginning of class,

614
00:31:52,935 --> 00:31:57,585
a Markov Decision Process policy specifies what action to take in each state.

615
00:31:57,585 --> 00:32:01,725
And the policies themselves can be deterministic or stochastic,

616
00:32:01,725 --> 00:32:05,400
meaning that you could either have a distribution over in the next action you might

617
00:32:05,400 --> 00:32:09,060
take given the state you're in or you could have a deterministic mapping.

618
00:32:09,060 --> 00:32:11,295
It says whenever I'm in this state I always,

619
00:32:11,295 --> 00:32:13,750
you know, do action a_1.

620
00:32:13,970 --> 00:32:18,000
Now- and a lot of this class we'll be thinking about

621
00:32:18,000 --> 00:32:20,670
deterministic policies but later on when we get into

622
00:32:20,670 --> 00:32:24,580
policy search we'll talk a lot more about stochastic policies.

623
00:32:25,040 --> 00:32:28,305
So, if you have an MDP plus a policy

624
00:32:28,305 --> 00:32:32,114
then that immediately specifies a Markov Reward Process.

625
00:32:32,114 --> 00:32:36,240
Because once you have specified the policy then you can think of that as

626
00:32:36,240 --> 00:32:40,020
inducing a Markov Reward Process because you're only

627
00:32:40,020 --> 00:32:43,290
ever taking you've specified your distribution over actions for

628
00:32:43,290 --> 00:32:48,090
your state and so then you can think of sort of what is the reward,

629
00:32:48,090 --> 00:32:53,970
the expected reward you get under that policy for any state and similarly you can define

630
00:32:53,970 --> 00:32:58,350
your transition model for Markov Reward Process by

631
00:32:58,350 --> 00:33:00,240
averaging across your transition models

632
00:33:00,240 --> 00:33:03,790
according to the weight at which you would take those different actions.

633
00:33:04,040 --> 00:33:07,950
So, the reason why it's useful to think about these connections between

634
00:33:07,950 --> 00:33:11,790
Markov Decision Processes and Markov Reward Processes is it implies that if

635
00:33:11,790 --> 00:33:15,300
you have a fixed policy you could just use all the techniques that

636
00:33:15,300 --> 00:33:18,810
we just described for Markov Reward Processes mainly simulation,

637
00:33:18,810 --> 00:33:21,780
analytic, analytic solution or dynamic

638
00:33:21,780 --> 00:33:26,020
programming in order to compute what is the value of a policy.

639
00:33:28,280 --> 00:33:35,700
So, if we go back to the iterative algorithm then it's exactly the same as before,

640
00:33:35,700 --> 00:33:38,400
exactly the same as the Markov Reward Process except

641
00:33:38,400 --> 00:33:41,310
for now we're indexing our reward by the policy.

642
00:33:41,310 --> 00:33:45,060
So, in order to learn what is the value of a particular policy we

643
00:33:45,060 --> 00:33:49,410
instantiate the reward function by always picking the action that the policy would take.

644
00:33:49,410 --> 00:33:51,990
So, in this case, I'm doing it for simplicity for

645
00:33:51,990 --> 00:33:58,170
deterministic policy and then

646
00:33:58,170 --> 00:34:01,380
similarly just indexing which transition model

647
00:34:01,380 --> 00:34:05,380
to look up based on the action that we would take in that state.

648
00:34:06,650 --> 00:34:11,385
And this is also known as a bellman backup for a particular policy.

649
00:34:11,385 --> 00:34:14,520
So, it allows us to state what is the value of the state under

650
00:34:14,520 --> 00:34:17,790
this policy well it's just the immediate reward I would get by

651
00:34:17,790 --> 00:34:19,949
following the policy in the current state plus

652
00:34:19,949 --> 00:34:24,433
the expected discounted sum of rewards I get by following this policy.

653
00:34:24,434 --> 00:34:29,295
And then for whatever state I end up by next continuing to follow this policy.

654
00:34:29,295 --> 00:34:32,880
So that's what the V^pi_k-1 specifies.

655
00:34:32,880 --> 00:34:36,150
What would happen if the expected discounted sum of rewards we get by

656
00:34:36,150 --> 00:34:40,420
continuing to follow policy from whatever state we just transitioned to.

657
00:34:44,000 --> 00:34:47,625
So, if we go to

658
00:34:47,625 --> 00:34:50,699
the Markov- the Markov chain

659
00:34:50,699 --> 00:34:54,194
or the Ma- now the Markov Decision Process for the Mars Rover,

660
00:34:54,195 --> 00:34:57,825
then let's look at the case now where we have these two actions.

661
00:34:57,825 --> 00:35:02,460
The reward function is still that you either have for any action if you're in state

662
00:35:02,460 --> 00:35:07,230
one you get plus one and in any state any action for state s_7 you get plus 10.

663
00:35:07,230 --> 00:35:08,775
Everything else is zero.

664
00:35:08,775 --> 00:35:15,465
So, imagine your policy is always to do action a_1 and your discount factor is zero.

665
00:35:15,465 --> 00:35:18,510
So, in this case,

666
00:35:18,510 --> 00:35:21,820
what is the value of the policy

667
00:35:24,820 --> 00:35:31,080
and this is just to remind you of what like the iterative way of computing it would be.

668
00:35:42,720 --> 00:35:45,040
Yeah in the back.

669
00:35:45,040 --> 00:35:48,340
Um, and I think that will be zero for everything

670
00:35:48,340 --> 00:35:51,250
except s_1 and s_7 where it's +1 and +10.

671
00:35:51,250 --> 00:35:53,680
That's exactly right. So this is a little bit of a trick question

672
00:35:53,680 --> 00:35:56,395
because I didn't show you again what the transition model is.

673
00:35:56,395 --> 00:35:57,760
Said is exactly correct.

674
00:35:57,760 --> 00:36:00,790
The- it doesn't matter what the transition model is here,

675
00:36:00,790 --> 00:36:03,550
um, because gamma is equal to zero.

676
00:36:03,550 --> 00:36:05,965
So that means that all of this goes away,

677
00:36:05,965 --> 00:36:08,395
um, and so you just have the immediate reward.

678
00:36:08,395 --> 00:36:12,670
So if your discount factor is zero then you just care about immediate reward.

679
00:36:12,670 --> 00:36:15,294
And so the immediate reward for this policy

680
00:36:15,294 --> 00:36:19,660
because the reward for all actions and state one is always +1.

681
00:36:19,660 --> 00:36:22,660
And the reward for all actions and all other states is zero except

682
00:36:22,660 --> 00:36:26,545
for in state s_7 where it's always 10 no matter which action you take.

683
00:36:26,545 --> 00:36:30,500
So this is just equal to one.

684
00:36:33,450 --> 00:36:36,370
That's the value function address.

685
00:36:36,370 --> 00:36:39,010
Okay. So let's, um, look at another one.

686
00:36:39,010 --> 00:36:42,010
So now we've got exactly the same process.

687
00:36:42,010 --> 00:36:47,710
Um, I've written down a particular choice of the dynamics model for ah, state s_6.

688
00:36:47,710 --> 00:36:49,060
So let's imagine that when you're in

689
00:36:49,060 --> 00:36:51,730
state s_6 which is almost all the way to the right, um,

690
00:36:51,730 --> 00:36:54,880
you have a 50% probability of staying there under action A1

691
00:36:54,880 --> 00:36:58,150
or a 50% probability of going to state s_7.

692
00:36:58,150 --> 00:36:59,725
That's what this top line says.

693
00:36:59,725 --> 00:37:01,960
And then there's a whole bunch of other dynamics models that we're

694
00:37:01,960 --> 00:37:04,525
not going to need to worry about to do this computation.

695
00:37:04,525 --> 00:37:08,110
And then the reward is still +1 for state s_1,

696
00:37:08,110 --> 00:37:09,640
+10 in state s_7,

697
00:37:09,640 --> 00:37:11,485
zero for all the states in the middle.

698
00:37:11,485 --> 00:37:13,555
And then let's imagine that, um,

699
00:37:13,555 --> 00:37:18,295
we're still trying to evaluate the policy where you're always taking action a_1.

700
00:37:18,295 --> 00:37:24,025
Um, and we've just said that V_k is equal to 1,0,0,0,10,

701
00:37:24,025 --> 00:37:29,350
um, and now what we wanna do is do one more backup essentially.

702
00:37:29,350 --> 00:37:36,025
So we want to move from V_k=1 and now compute V_k=2.

703
00:37:36,025 --> 00:37:40,810
So how [NOISE] about everybody take a second and figure [NOISE] out what would be

704
00:37:40,810 --> 00:37:46,760
the value under this particular policy, okay, for s_6.

705
00:37:47,640 --> 00:37:50,575
So you can use this equation, um,

706
00:37:50,575 --> 00:37:53,125
to figure out given that I know what

707
00:37:53,125 --> 00:37:58,290
my previous value function is because I've specified it there it's 1,0,0,0,10.

708
00:37:58,290 --> 00:38:02,380
Um, and now I'm going to be doing one backup,

709
00:38:02,380 --> 00:38:04,465
and I'm only asking you to do it for one state,

710
00:38:04,465 --> 00:38:05,755
you could do it for others if you want.

711
00:38:05,755 --> 00:38:12,290
Um, what would be the new value of s_6 if you use this equation to compute it?

712
00:38:14,310 --> 00:38:18,730
And it just requires plugging in what is the value of the reward.

713
00:38:18,730 --> 00:38:25,525
The value is and- and the particular numbers for the dynamics and the old value function.

714
00:38:25,525 --> 00:38:28,630
And the reason that I bring this up as an example is to show sort of

715
00:38:28,630 --> 00:38:32,095
essentially how could have information flows as you do this computation.

716
00:38:32,095 --> 00:38:33,940
So you start off in the very initial.

717
00:38:33,940 --> 00:38:35,335
Let me just go over here first.

718
00:38:35,335 --> 00:38:37,360
So when you start off, you're going to initialize

719
00:38:37,360 --> 00:38:39,445
the value function to be zero everywhere.

720
00:38:39,445 --> 00:38:41,980
The first backup you do basically initializes

721
00:38:41,980 --> 00:38:44,920
the value function to be the immediate reward everywhere.

722
00:38:44,920 --> 00:38:46,870
And then after that you're going to continue to

723
00:38:46,870 --> 00:38:48,790
do these backups and essentially you're trying to

724
00:38:48,790 --> 00:38:51,160
compute its expected discounted sum of

725
00:38:51,160 --> 00:38:54,355
future rewards for each of the states under this policy.

726
00:38:54,355 --> 00:38:57,040
So if you think about looking at this,

727
00:38:57,040 --> 00:39:00,040
that's with information of the fact that state s_7 is good,

728
00:39:00,040 --> 00:39:03,910
is going to kinda flow backwards to the other states because they're saying "Okay well,

729
00:39:03,910 --> 00:39:08,050
I've been in state s_4 I don't have any reward right now but at a couple of timesteps

730
00:39:08,050 --> 00:39:12,430
under this process I might because I might reach that really great +10 state."

731
00:39:12,430 --> 00:39:16,435
So as we do these iterations of policy evaluation,

732
00:39:16,435 --> 00:39:21,400
we start to propagate the information about future rewards back to earlier states.

733
00:39:21,400 --> 00:39:25,855
And so what I'm asking you to do here is to just do that for one, one more step.

734
00:39:25,855 --> 00:39:27,520
Just say for state s_6,

735
00:39:27,520 --> 00:39:28,960
what would its new value be?

736
00:39:28,960 --> 00:39:30,520
Its previous value was zero.

737
00:39:30,520 --> 00:39:32,860
Now we're going to do one backup and what's this new value.

738
00:39:32,860 --> 00:39:34,180
So what if you just uh,

739
00:39:34,180 --> 00:39:36,985
let's ask a question then we can all take a second to uh.

740
00:39:36,985 --> 00:39:43,180
I'm just wondering, er, if repeating the same process to find the value function.

741
00:39:43,180 --> 00:39:46,780
I guess if you don't necessarily know the value function of s,

742
00:39:46,780 --> 00:39:49,105
you could just like reversibly follow it down.

743
00:39:49,105 --> 00:39:51,280
Question was can you- if you don't know what

744
00:39:51,280 --> 00:39:53,290
the value function is. I guess I'm not totally sure.

745
00:39:53,290 --> 00:39:54,970
This is a way to compute the value,

746
00:39:54,970 --> 00:39:57,745
wait your question is asking because this is a way to compute the value function.

747
00:39:57,745 --> 00:39:59,875
So what we've done here is we've said,

748
00:39:59,875 --> 00:40:02,500
we've initialized the value function to be zero everywhere.

749
00:40:02,500 --> 00:40:04,390
That is not the real value function,

750
00:40:04,390 --> 00:40:06,190
that just sort of an initialization.

751
00:40:06,190 --> 00:40:08,875
And what this process is allowing us to do is we keep

752
00:40:08,875 --> 00:40:12,950
updating the values of every single state until they stop changing.

753
00:40:12,950 --> 00:40:16,710
And then that gives us the expected discounted sum of rewards.

754
00:40:16,710 --> 00:40:20,685
Now you might ask, okay well they- are they ever guaranteed to stop changing?

755
00:40:20,685 --> 00:40:21,915
And we'll get to that part later.

756
00:40:21,915 --> 00:40:24,120
We'll get to the fact that this whole process is guaranteed

757
00:40:24,120 --> 00:40:26,985
to be a contraction so it's not going to go on forever.

758
00:40:26,985 --> 00:40:30,230
So the distance between the value functions is going to be shrinking.

759
00:40:30,230 --> 00:40:33,070
And that's one of the benefits of the discount factor.

760
00:40:33,070 --> 00:40:36,220
So if people don't have any more immediate questions,

761
00:40:36,220 --> 00:40:38,050
I suggest we all take a minute and then just compare with

762
00:40:38,050 --> 00:40:41,080
your neighbor of what number you get when you do this computation.

763
00:40:41,080 --> 00:40:43,810
Just to quickly check that the Bellman equation make sense.

764
00:40:43,810 --> 00:40:46,420
[NOISE] All right. So, um,

765
00:40:46,420 --> 00:40:47,830
wherever you got to, um,

766
00:40:47,830 --> 00:40:49,660
hope we got a chance to sort of compare check

767
00:40:49,660 --> 00:40:52,240
any understanding with anybody else that was next to you.

768
00:40:52,240 --> 00:40:54,205
Um, before we go on I just want to, um,

769
00:40:54,205 --> 00:40:56,830
answer a question that was asked before about whether or

770
00:40:56,830 --> 00:40:59,710
not the analytics solution is always possible,

771
00:40:59,710 --> 00:41:02,870
um, to invert. Let's go back to that.

772
00:41:03,210 --> 00:41:05,815
So in this case, um,

773
00:41:05,815 --> 00:41:08,065
because p is a stochastic matrix,

774
00:41:08,065 --> 00:41:11,350
its eigenvalues are always going to be less than or equal to one.

775
00:41:11,350 --> 00:41:13,465
If your discount factor is less than one,

776
00:41:13,465 --> 00:41:19,600
then I which is the identity matrix minus gamma times P is always going to be invertible.

777
00:41:19,600 --> 00:41:21,460
That's the answer to that question.

778
00:41:21,460 --> 00:41:26,620
So this matrix is always invertible as long as gamma is less than one. All right.

779
00:41:26,620 --> 00:41:28,735
So let's go back to this one, um,

780
00:41:28,735 --> 00:41:32,725
which we're going to require any way for some of the other important properties we want.

781
00:41:32,725 --> 00:41:35,410
So in this case what is that?

782
00:41:35,410 --> 00:41:37,180
So the immediate reward of this is

783
00:41:37,180 --> 00:41:41,890
zero plus gamma times [NOISE] 0.5 probability that we stay in

784
00:41:41,890 --> 00:41:49,735
that state times the previous V of s_6 plus 0.5 probability that we go to V of s_7.

785
00:41:49,735 --> 00:41:58,790
And this is going to be equal to zero plus 0.5 times zero plus 0.5 times 10.

786
00:42:02,760 --> 00:42:05,560
So that's just an example of, um,

787
00:42:05,560 --> 00:42:07,960
how you would compute one Bellman backup.

788
00:42:07,960 --> 00:42:11,305
And that's back to my original question which is you seem to be using

789
00:42:11,305 --> 00:42:15,970
V_k without the superscript pi to evaluate it.

790
00:42:15,970 --> 00:42:17,320
Oh, sorry this should, yes.

791
00:42:17,320 --> 00:42:18,565
This should have been pi.

792
00:42:18,565 --> 00:42:22,630
That's just a typo. And that's that was correct in there.

793
00:42:22,630 --> 00:42:26,440
Question was just whether or not that was supposed to be pi up there.

794
00:42:26,440 --> 00:42:28,660
Yes it was, thanks for catching.

795
00:42:28,660 --> 00:42:33,490
All right, so now we can start to talk about Markov Decision Process control.

796
00:42:33,490 --> 00:42:38,260
Now just to note there. So I led us through or we just went through policy evaluation

797
00:42:38,260 --> 00:42:40,360
in an iterative way you could have also

798
00:42:40,360 --> 00:42:44,000
done it analytically or you could have done it with simulation.

799
00:42:44,040 --> 00:42:48,430
But as a particularly nice analogy now that we're going to start to think about control.

800
00:42:48,430 --> 00:42:50,275
So again what do I mean by control?

801
00:42:50,275 --> 00:42:52,390
Control here is going to be the fact that ultimately

802
00:42:52,390 --> 00:42:54,670
we don't care about just evaluating policies,

803
00:42:54,670 --> 00:42:57,670
typically we want our agent actually be learning policies.

804
00:42:57,670 --> 00:43:02,230
And so in this case we're not going to talk about learning policies,

805
00:43:02,230 --> 00:43:05,365
we're just going to be talking about computing optimal policies.

806
00:43:05,365 --> 00:43:11,030
So the important thing is that there exists a unique optimal value function.

807
00:43:11,580 --> 00:43:15,490
So- um, and the optimal policy for

808
00:43:15,490 --> 00:43:20,200
an MDP and an infinite horizon finite state MDP is deterministic.

809
00:43:20,200 --> 00:43:22,750
So that's one really good reason why it's

810
00:43:22,750 --> 00:43:25,765
sufficient for us to just focus on deterministic policies,

811
00:43:25,765 --> 00:43:27,820
with a finite state MDPs,

812
00:43:27,820 --> 00:43:30,475
um, in infinite horizons.

813
00:43:30,475 --> 00:43:33,070
Okay. So how do we compute it?

814
00:43:33,070 --> 00:43:36,355
Well first before we do this let's think about how many policies there might be.

815
00:43:36,355 --> 00:43:38,080
So there are seven discrete states.

816
00:43:38,080 --> 00:43:40,375
In this case it's the locations that the robot.

817
00:43:40,375 --> 00:43:41,650
There are two actions.

818
00:43:41,650 --> 00:43:42,730
I won't call them left and right,

819
00:43:42,730 --> 00:43:45,040
I'm just going to call them a_1 and a_2.

820
00:43:45,040 --> 00:43:48,100
Because left and right kind of implies that you will definitely achieve that.

821
00:43:48,100 --> 00:43:51,010
We can also just think of these as generally being stochastic scenarios.

822
00:43:51,010 --> 00:43:53,600
So let's just call them a_1 and a_2.

823
00:43:53,880 --> 00:43:57,910
Then the question is how many deterministic policies are

824
00:43:57,910 --> 00:44:00,940
there and is the optimal policy for MDP always unique?

825
00:44:00,940 --> 00:44:02,980
So kind of right we just take like

826
00:44:02,980 --> 00:44:05,995
one minute or say one or two minutes feel free to talk to a neighbor

827
00:44:05,995 --> 00:44:08,230
about how [NOISE] many deterministic policies there are for

828
00:44:08,230 --> 00:44:11,455
this particular case and then if that's- um,

829
00:44:11,455 --> 00:44:14,770
once you've answered that it's fine to think about in general if you

830
00:44:14,770 --> 00:44:17,785
have |S| states and |A| actions,

831
00:44:17,785 --> 00:44:19,975
and this is the cardinality of those sets.

832
00:44:19,975 --> 00:44:23,335
How many possible deterministic policies are there?

833
00:44:23,335 --> 00:44:26,470
Um, and then the second question which is whether or not these are always unique.

834
00:44:26,470 --> 00:44:33,610
[NOISE] Can anyone I'd

835
00:44:33,610 --> 00:44:37,030
take a guess at how many deterministic policies that are in this case?

836
00:44:37,030 --> 00:44:43,930
[NOISE].

837
00:44:43,930 --> 00:44:48,145
It's a mapping from states to actions so it's gonna be 2 to the 7th.

838
00:44:48,145 --> 00:44:50,425
That's exactly right. That is it's a mapping.

839
00:44:50,425 --> 00:44:53,185
Er, if we remember back to our definition of what a policy is,

840
00:44:53,185 --> 00:44:56,800
a mapping is going to be a map from states to actions.

841
00:44:56,800 --> 00:44:59,770
So what that means in this case is that there are

842
00:44:59,770 --> 00:45:04,300
two choices for every state and there are seven states.

843
00:45:04,300 --> 00:45:11,710
And more generally that the [NOISE] number of policies is |A| to the |S|. So we can be large,

844
00:45:11,710 --> 00:45:15,205
its exponential and the state-space but it's finite.

845
00:45:15,205 --> 00:45:18,250
So it's bounded. Um, any

846
00:45:18,250 --> 00:45:20,650
one want to take a guess of whether or not the optimal policy is always unique?

847
00:45:20,650 --> 00:45:22,990
I told you the value function is unique.

848
00:45:22,990 --> 00:45:25,045
Is the policy unique?

849
00:45:25,045 --> 00:45:29,680
Yeah.

850
00:45:29,680 --> 00:45:32,185
I think there might be cases where it's not.

851
00:45:32,185 --> 00:45:34,960
Exactly right, um. It's not always unique.

852
00:45:34,960 --> 00:45:38,530
The value function is unique but if there may be cases where you get ties.

853
00:45:38,530 --> 00:45:40,870
And so there might be that there are two actions that,

854
00:45:40,870 --> 00:45:43,030
um, are or two policies that have the same value.

855
00:45:43,030 --> 00:45:45,830
So no. Depends on the process.

856
00:45:45,830 --> 00:45:47,850
You mean like unique optimal value function?

857
00:45:47,850 --> 00:45:49,470
Ah, yes.

858
00:45:49,470 --> 00:45:53,850
So the question is can I explain what I mean by there's a unique optimal value function.

859
00:45:53,850 --> 00:45:56,960
I mean that the optimal value of the state.

860
00:45:56,960 --> 00:46:01,495
So the expected discounted sum of returns, um,

861
00:46:01,495 --> 00:46:06,145
there is- there may be more than one optimal policy but there

862
00:46:06,145 --> 00:46:11,645
exists at least one optimal policy which leads to the maximum value for that state.

863
00:46:11,645 --> 00:46:16,705
Um, and there's a single value of that.

864
00:46:16,705 --> 00:46:18,700
We'll talk about- probably a little bit clearer

865
00:46:18,700 --> 00:46:21,010
to when we talk about contraction properties later.

866
00:46:21,010 --> 00:46:25,060
Um, that there's- so for each state it's just a scalar value.

867
00:46:25,060 --> 00:46:28,510
It says exactly what is the expected discounted sum of returns and this

868
00:46:28,510 --> 00:46:32,350
is the maximum expected discounted sum of returns under the optimal policy.

869
00:46:32,350 --> 00:46:35,980
Yeah.

870
00:46:35,980 --> 00:46:39,755
And on the [inaudible] policies in our-

871
00:46:39,755 --> 00:46:43,565
When we first define policies I thought I was describing the- um,

872
00:46:43,565 --> 00:46:46,415
the entire hash table with sort of

873
00:46:46,415 --> 00:46:50,410
one action per state rather than saying all possible combinations.

874
00:46:50,410 --> 00:46:53,710
It's a little surprised that is 2 to the 7th rather than being

875
00:46:53,710 --> 00:46:57,950
just the number of states with each one of the maps because of action.

876
00:46:57,950 --> 00:47:00,520
For me to sort of better clarify, you know,

877
00:47:00,520 --> 00:47:03,460
what this- what this how many policies there are and whether

878
00:47:03,460 --> 00:47:05,100
maybe- there maybe it looked like it

879
00:47:05,100 --> 00:47:07,055
was going to be linear and it's actually exponential.

880
00:47:07,055 --> 00:47:09,995
Um, the way that we're defining a decision policy here,

881
00:47:09,995 --> 00:47:13,890
um, a deterministic decision policy is a mapping from a state to an action.

882
00:47:13,890 --> 00:47:17,150
And so that means for each state we get to choose an action and so

883
00:47:17,150 --> 00:47:20,445
just as an illustration of why this ends up being exponential.

884
00:47:20,445 --> 00:47:22,940
Um, so, in this case let's imagine instead of having

885
00:47:22,940 --> 00:47:25,865
seven states we just have six or two states.

886
00:47:25,865 --> 00:47:27,430
Now we have s_1 and s_2.

887
00:47:27,430 --> 00:47:30,230
[NOISE] So, you could either have action a_1-a_1,

888
00:47:30,230 --> 00:47:31,680
you could have action a_1-a_2,

889
00:47:31,680 --> 00:47:35,540
you could have action a_2-a_1 or action a_2-a_2.

890
00:47:36,040 --> 00:47:40,100
And you have to and all of those are distinct policies.

891
00:47:40,100 --> 00:47:45,490
So, that's why the space ends up being exponential. Sure.

892
00:47:45,490 --> 00:47:50,975
When you have like A to the power S. I'm assuming that A refers to

893
00:47:50,975 --> 00:47:53,600
legal actions per state assuming like

894
00:47:53,600 --> 00:47:56,345
you could have different actions depending on the state.

895
00:47:56,345 --> 00:47:58,160
The question is whether or not you might be able to have

896
00:47:58,160 --> 00:48:01,150
different constraints on the action space for state, absolutely.

897
00:48:01,150 --> 00:48:03,220
So, in this case, today for simplicity,

898
00:48:03,220 --> 00:48:06,225
we're going to assume that all actions are applicable in all states.

899
00:48:06,225 --> 00:48:08,025
Um, in reality that's often not true.

900
00:48:08,025 --> 00:48:10,320
Um, in many real-world cases,

901
00:48:10,320 --> 00:48:13,590
um, some of the actions might be specific to the state.

902
00:48:13,590 --> 00:48:17,160
Ah, for totally, there's a huge space of medical interventions.

903
00:48:17,160 --> 00:48:18,890
Um, er for many of them,

904
00:48:18,890 --> 00:48:22,370
they might not be at all even reasonable to ever consider,

905
00:48:22,370 --> 00:48:24,390
um, for certain states are applicable.

906
00:48:24,390 --> 00:48:26,620
Um, so, in general,

907
00:48:26,620 --> 00:48:28,860
you can have different actions sub-spaces per

908
00:48:28,860 --> 00:48:32,275
state and then you would take the product over the actions,

909
00:48:32,275 --> 00:48:35,125
the cardinality of the action set that is relevant for each of the states.

910
00:48:35,125 --> 00:48:37,980
But for right now, I think it's simple as just to think of it as there's

911
00:48:37,980 --> 00:48:41,690
one uniform action space and then they can be applied in any state.

912
00:48:43,480 --> 00:48:47,045
Okay. So, um, the optimal policy for an MDP and a

913
00:48:47,045 --> 00:48:50,305
finite horizon problem where the agent acts forever.

914
00:48:50,305 --> 00:48:51,945
Um, it's deterministic.

915
00:48:51,945 --> 00:48:55,415
It's stationary which means it doesn't depend on the time-step.

916
00:48:55,415 --> 00:48:57,980
We started talking about that a little bit last time.

917
00:48:57,980 --> 00:49:03,160
Um, so, it means that if I'm in this state- if I'm in state s_7,

918
00:49:03,160 --> 00:49:05,230
there is an optimal policy for being in

919
00:49:05,230 --> 00:49:08,790
state s_7 whether I encountered that at time-step one,

920
00:49:08,790 --> 00:49:12,820
time-step 37, time-step 242 stationary.

921
00:49:12,820 --> 00:49:16,210
Um, er one of the intuitions for this is that if you get to act

922
00:49:16,210 --> 00:49:17,710
forever there's always like

923
00:49:17,710 --> 00:49:20,925
an infinite number of future time steps no matter when you're at.

924
00:49:20,925 --> 00:49:25,190
So, if you would always do action a_1 from state s_7 now,

925
00:49:25,190 --> 00:49:29,110
um then if you encounter it again in 50 time-steps you still have

926
00:49:29,110 --> 00:49:31,310
an infinite amount of time to go from there and so you'd still

927
00:49:31,310 --> 00:49:34,485
take the same action if that was the optimal thing to do.

928
00:49:34,485 --> 00:49:38,820
As we were just discussing, it's not the optimal policy is not necessarily unique,

929
00:49:38,820 --> 00:49:44,790
um because you might have ah more than one policy with the same value function.

930
00:49:45,810 --> 00:49:48,160
So, how would we compute this?

931
00:49:48,160 --> 00:49:51,710
One option is policy search uh and we'll talk a lot more about this in

932
00:49:51,710 --> 00:49:53,110
a few weeks when we're talking about

933
00:49:53,110 --> 00:49:56,450
function approximation and having really really large state spaces.

934
00:49:56,450 --> 00:49:58,480
Um, but even in tabular cases,

935
00:49:58,480 --> 00:50:00,405
er we can just think of searching.

936
00:50:00,405 --> 00:50:04,410
So, the number of deterministic policies we just discussed is A to the S,

937
00:50:04,410 --> 00:50:09,235
um and policy iteration is a technique that is generally better than enumeration.

938
00:50:09,235 --> 00:50:11,655
So, what do I mean by enumeration in this context?

939
00:50:11,655 --> 00:50:14,175
I mean there's a finite number of policies.

940
00:50:14,175 --> 00:50:18,680
You could just evaluate each of them separately and then pick the max.

941
00:50:18,680 --> 00:50:21,160
So, if you have a lot of compute,

942
00:50:21,160 --> 00:50:23,720
you might just want to and this might be better if you really care

943
00:50:23,720 --> 00:50:26,635
about wall clock and you have many many many processors.

944
00:50:26,635 --> 00:50:28,070
You could just do this exhaustively.

945
00:50:28,070 --> 00:50:29,920
You could just try all of your policies,

946
00:50:29,920 --> 00:50:32,780
evaluate all of them either analytically or iteratively or

947
00:50:32,780 --> 00:50:37,090
whatever scheme you want to use and then take the max over all of them.

948
00:50:37,090 --> 00:50:40,185
But if you don't have kind of infinite compute,

949
00:50:40,185 --> 00:50:43,570
it's generally more computationally efficient if you have to do

950
00:50:43,570 --> 00:50:47,690
this serially to do policy iteration and so we'll talk about what that is.

951
00:50:47,690 --> 00:50:51,095
So, in policy iteration what we do is we basically

952
00:50:51,095 --> 00:50:54,320
keep track of a guess of what the optimal policy might be.

953
00:50:54,320 --> 00:50:58,120
We evaluate its value and then we try to improve it.

954
00:50:58,120 --> 00:51:00,070
If we can't improve it any more,

955
00:51:00,070 --> 00:51:02,585
um then we can- then we can halt.

956
00:51:02,585 --> 00:51:06,965
So, the idea is that we start by initializing randomly.

957
00:51:06,965 --> 00:51:12,410
Here now you can think of the subscript is indexing which policy we're at.

958
00:51:12,410 --> 00:51:15,640
So, initially we start off with some random policy and

959
00:51:15,640 --> 00:51:17,570
then π_i is always going to index

960
00:51:17,570 --> 00:51:20,860
sort of our current guess of what the optimal policy might be.

961
00:51:20,860 --> 00:51:25,220
So, what we do is we initialize our policy randomly and while it's not changing

962
00:51:25,220 --> 00:51:29,730
and we'll talk about whether or not it can change or go back to the same one in a second,

963
00:51:29,730 --> 00:51:32,535
we do value function policy.

964
00:51:32,535 --> 00:51:36,010
We evaluate the policy using the same sorts of techniques we just

965
00:51:36,010 --> 00:51:37,550
discussed because it's a fixed policy

966
00:51:37,550 --> 00:51:41,410
which means we are now basically in a Markov Reward Process.

967
00:51:41,410 --> 00:51:43,205
And then we do policy improvement.

968
00:51:43,205 --> 00:51:45,290
So, the really the new thing compared to what we were

969
00:51:45,290 --> 00:51:48,170
doing before now is policy improvement.

970
00:51:49,290 --> 00:51:53,110
So, in order to define how we could improve a policy,

971
00:51:53,110 --> 00:51:56,270
we're going to define something new which is the state action value.

972
00:51:56,270 --> 00:51:58,880
So, before we were just talking about state values,

973
00:51:58,880 --> 00:52:05,280
state values are denoted by V. We're talking about

974
00:52:05,280 --> 00:52:08,300
like V^pi(s) which says if you start in state s and you

975
00:52:08,300 --> 00:52:12,220
follow policy pi what is the expected discounted sum of rewards.

976
00:52:12,220 --> 00:52:15,655
A state action value says well,

977
00:52:15,655 --> 00:52:19,095
I'm going to follow this policy pi but not right away.

978
00:52:19,095 --> 00:52:21,465
I'm going to first take an action a,

979
00:52:21,465 --> 00:52:24,930
which might be different than what my policy is telling me to

980
00:52:24,930 --> 00:52:29,460
do and then later on the next time-step I'm going to follow policy pi.

981
00:52:29,460 --> 00:52:33,810
So, it just says I'm going to get my immediate reward from taking this action a

982
00:52:33,810 --> 00:52:37,935
that I'm choosing and then I'm going to transition to a new state.

983
00:52:37,935 --> 00:52:40,530
Again, that depends on my current state and the action I just

984
00:52:40,530 --> 00:52:44,350
took and from then on I'm going to take policy pi.

985
00:52:45,840 --> 00:52:49,655
So, that defines the Q function

986
00:52:49,655 --> 00:52:54,115
and what policy improvement does is it says okay you've got a policy,

987
00:52:54,115 --> 00:52:58,875
you just did policy evaluation and you got a value of it.

988
00:52:58,875 --> 00:53:03,150
So, policy evaluation just allowed you to compute what was the value of

989
00:53:03,150 --> 00:53:08,220
that policy [NOISE] and now I want to see if I can improve it.

990
00:53:08,220 --> 00:53:11,020
Now, remember right now we're in the case where we know

991
00:53:11,020 --> 00:53:13,755
the dynamics model and we know the reward model.

992
00:53:13,755 --> 00:53:16,385
So, what we can do then is we can do this with

993
00:53:16,385 --> 00:53:19,640
Q computation where we say okay well I've got

994
00:53:19,640 --> 00:53:23,230
that previous value function by policy and now

995
00:53:23,230 --> 00:53:26,820
I compute Q^pi which says if I take a different action,

996
00:53:26,820 --> 00:53:34,470
it could be the same and we do this for all A and for all S. So,

997
00:53:34,470 --> 00:53:38,245
for all A and all S we compute this and then we're

998
00:53:38,245 --> 00:53:43,260
going to compute a new policy and this is the improvement step which maximizes this Q.

999
00:53:43,300 --> 00:53:48,490
So, we just do this computation and then we take the max.

1000
00:53:48,490 --> 00:53:58,980
Now, by definition this has to be greater than or equal to Q^πi(s, pi_i(a)),

1001
00:53:59,860 --> 00:54:05,810
right, because either a is equal to pi_i(a),

1002
00:54:05,810 --> 00:54:14,190
sorry pi_i(s). So,

1003
00:54:14,190 --> 00:54:20,680
either you the arg max is going to be the same as that

1004
00:54:20,680 --> 00:54:24,600
your previous policy π_i or it's going to be different and the only time you're going

1005
00:54:24,600 --> 00:54:29,600
to pick it differently as if the Q function of that alternative action is better.

1006
00:54:29,710 --> 00:54:41,357
So, by definition this Q^π that max over A of Q^π_i(s,a),

1007
00:54:41,357 --> 00:54:47,940
has to be greater than or equal to Q^π_i(s, π_i(s)). Question at the back.

1008
00:54:47,940 --> 00:54:50,095
Is this going to be susceptible?

1009
00:54:50,095 --> 00:54:53,730
Is this going to be like finding a local maximum goal then

1010
00:54:53,730 --> 00:54:58,295
its kind of gets stuck there and [inaudible] for actions.

1011
00:54:58,295 --> 00:55:03,305
Okay. So, this is going to allow us to maybe do some local monotonic improvement maybe,

1012
00:55:03,305 --> 00:55:05,945
um but are we going to be susceptible to gain stuck.

1013
00:55:05,945 --> 00:55:08,140
Um, in fact, ah for any of you that have played

1014
00:55:08,140 --> 00:55:10,220
around with reinforcement learning and and policy gradient and

1015
00:55:10,220 --> 00:55:13,230
stuff that is exactly one of the problems that can happen when we start doing

1016
00:55:13,230 --> 00:55:16,680
gradient based approaches nicely in this case this does not occur.

1017
00:55:16,680 --> 00:55:23,080
So, we're guaranteed to converge to the global optima and we'll see why for a second.

1018
00:55:23,080 --> 00:55:26,130
Okay. All right. So this is how it works.

1019
00:55:26,130 --> 00:55:29,640
You do this policy evaluation and then you compute the Q function and then

1020
00:55:29,640 --> 00:55:34,360
you compute the new policy that takes an arg max of the Q function.

1021
00:55:34,360 --> 00:55:37,010
So, that's how policy improvement works.

1022
00:55:37,010 --> 00:55:39,100
The next critical question is Iris was bringing up

1023
00:55:39,100 --> 00:55:42,270
is okay why do we do this and is this a good idea.

1024
00:55:43,590 --> 00:55:46,195
So, when we look at this,

1025
00:55:46,195 --> 00:55:50,200
um let's look through this stuff a little bit more.

1026
00:55:51,560 --> 00:55:54,730
What we're going to get is we're going to get,

1027
00:55:54,730 --> 00:55:56,710
um this sort of interesting type of

1028
00:55:56,710 --> 00:55:59,720
policy improvements step and it's kind of involving a few different things.

1029
00:55:59,720 --> 00:56:01,985
So, I just want to highlight the subtlety of it.

1030
00:56:01,985 --> 00:56:08,660
So, what is happening here is that we compute this Q function and then we've got this.

1031
00:56:08,660 --> 00:56:19,005
We've got max over A of Q^π_i(s,a) has to be greater than equal to R(s, π(a)).

1032
00:56:19,005 --> 00:56:24,640
The previous policy that we were using before.

1033
00:56:24,640 --> 00:56:29,580
[NOISE].

1034
00:56:36,810 --> 00:56:39,490
So, what I've done there is I've said, okay,

1035
00:56:39,490 --> 00:56:42,400
the max action over the Q has to be

1036
00:56:42,400 --> 00:56:46,675
at least as good as following your old policy by definition,

1037
00:56:46,675 --> 00:56:48,670
because otherwise you could always pick the same policy as

1038
00:56:48,670 --> 00:56:51,160
before or else you're gonna pick a better action.

1039
00:56:51,160 --> 00:56:54,130
And this reward function here is

1040
00:56:54,130 --> 00:56:58,040
just exactly the definition of the value of your old policy.

1041
00:56:58,830 --> 00:57:02,110
So, that means that you're- the max over your Q function

1042
00:57:02,110 --> 00:57:05,350
has to be at least as good as the old value you had.

1043
00:57:05,350 --> 00:57:08,350
So, that's encouraging. But here's the weird part.

1044
00:57:08,350 --> 00:57:10,345
So, when we do this,

1045
00:57:10,345 --> 00:57:14,300
if we instead take arg max we're gonna get our new policy.

1046
00:57:22,350 --> 00:57:25,075
So, what is this doing? It's saying,

1047
00:57:25,075 --> 00:57:26,665
I'm computing this new Q function.

1048
00:57:26,665 --> 00:57:28,195
What does this Q function represent?

1049
00:57:28,195 --> 00:57:33,325
It represents, if I take an action and then I follow my old policy from then onwards.

1050
00:57:33,325 --> 00:57:39,535
And then I'm picking whatever action is maximizing that quantity for each state.

1051
00:57:39,535 --> 00:57:43,010
Okay. So, I'm gonna do this process for each state.

1052
00:57:44,550 --> 00:57:48,475
But then- so that's going to just define a new policy, right?

1053
00:57:48,475 --> 00:57:50,575
Like I thought that might be the same or it could be a,

1054
00:57:50,575 --> 00:57:52,780
a different policy than the one you've had before.

1055
00:57:52,780 --> 00:57:54,145
Here's the weird thing.

1056
00:57:54,145 --> 00:57:57,445
So, this is saying that if you were to follow

1057
00:57:57,445 --> 00:58:01,675
that arg max A and then follow your old policy from then onwards,

1058
00:58:01,675 --> 00:58:05,185
you will be guaranteed to be doing better than you were before.

1059
00:58:05,185 --> 00:58:10,120
But the strange thing is that we're not gonna follow the old policy from then onwards.

1060
00:58:10,120 --> 00:58:13,075
We are going to follow this new policy for all time.

1061
00:58:13,075 --> 00:58:16,180
So, remember what we're doing is we're completely changing

1062
00:58:16,180 --> 00:58:20,425
our policy and then we're going to evaluate that new policy for all time steps,

1063
00:58:20,425 --> 00:58:24,325
not just for the first time step and then follow the old policy from then on.

1064
00:58:24,325 --> 00:58:28,750
So, it should be at least a little unclear that this is a good thing to do [LAUGHTER].

1065
00:58:28,750 --> 00:58:29,905
Should be like, okay, so you're,

1066
00:58:29,905 --> 00:58:31,270
you're saying that if I were to take

1067
00:58:31,270 --> 00:58:33,535
this one different action and then follow my old policy,

1068
00:58:33,535 --> 00:58:36,250
then I know that my value would be better than before.

1069
00:58:36,250 --> 00:58:41,185
But what you really want is that this new policy is just better overall.

1070
00:58:41,185 --> 00:58:44,230
And so the cool thing is that you can show that by doing

1071
00:58:44,230 --> 00:58:49,670
this policy improvement it is monotonically better than the old policy.

1072
00:58:50,430 --> 00:58:53,770
So, this is just saying this on a words, we're saying,

1073
00:58:53,770 --> 00:58:56,320
you know, if we took the new policy for one action,

1074
00:58:56,320 --> 00:58:58,600
then follow pi_i forever then we're guaranteed to be

1075
00:58:58,600 --> 00:59:01,525
at least as good as we were before in terms of our value function,

1076
00:59:01,525 --> 00:59:05,480
but our new proposed policy is just to always follow this new policy.

1077
00:59:05,520 --> 00:59:09,130
Okay. So, why did we get a monotonic improvement in

1078
00:59:09,130 --> 00:59:13,540
the policy value by doing this say in the policy value?

1079
00:59:13,540 --> 00:59:16,795
So, what- first of all what do I mean by a monotonic improvement?

1080
00:59:16,795 --> 00:59:20,395
Um, what I mean is that the value, uh,

1081
00:59:20,395 --> 00:59:22,780
something that is monotonic if, um,

1082
00:59:22,780 --> 00:59:27,310
the new policy is greater than equal to the old policy for all states.

1083
00:59:27,310 --> 00:59:29,935
So, it has to either have the same value or be better.

1084
00:59:29,935 --> 00:59:34,840
And my proposition is that the new policy is greater than or equal to

1085
00:59:34,840 --> 00:59:41,150
the old policy in all states with strict inequality if the old policy was suboptimal.

1086
00:59:44,220 --> 00:59:50,365
So, why does this work? So, it works for the following reasons.

1087
00:59:50,365 --> 00:59:53,710
Let's go ahead and just like walk through the proof briefly.

1088
00:59:53,710 --> 00:59:58,420
Okay. So, this is- what we've said here is that,

1089
00:59:58,420 --> 01:00:00,595
um, V^pi_i(s),

1090
01:00:00,595 --> 01:00:02,290
that's our old value of our policy.

1091
01:00:02,290 --> 01:00:05,845
So, this is like our old policy value.

1092
01:00:05,845 --> 01:00:11,510
Has to be less than or equal to max a of Q^pi_i(s, a).

1093
01:00:12,210 --> 01:00:21,050
And this is just by definition. Uh, let me write it like this.

1094
01:00:27,240 --> 01:00:34,870
Is equal to R(s, pi_i+1(s)). Because remember the way that we

1095
01:00:34,870 --> 01:00:38,110
define pi_i+1(s) is just equal

1096
01:00:38,110 --> 01:00:43,040
to the policy that match- maximizes the Q^pi_i.

1097
01:00:43,800 --> 01:00:47,450
Okay. So, this is gonna be by definition.

1098
01:00:47,460 --> 01:00:51,170
So, I've gotten rid of the max there.

1099
01:01:04,200 --> 01:01:12,190
Okay. So, this is going to be less than or equal to R the same thing at

1100
01:01:12,190 --> 01:01:28,940
the beginning times max over a of our Q^pi_i.

1101
01:01:32,100 --> 01:01:35,410
Again by definition, because we've said

1102
01:01:35,410 --> 01:01:37,840
that the first thing there that we know that the pie i of

1103
01:01:37,840 --> 01:01:43,855
s prime would also be less than or equal to max over a of Q^pi_i(s', a').

1104
01:01:43,855 --> 01:01:46,670
Okay. So, we just made that substitution.

1105
01:01:47,400 --> 01:01:54,775
And then we can re-expand this part using r reward.

1106
01:01:54,775 --> 01:02:01,060
So, this is gonna be the max over a' R(s',a') plus dot-dot-dot,

1107
01:02:01,060 --> 01:02:04,360
basically making that substitution from that line into there.

1108
01:02:04,360 --> 01:02:09,415
So, I'm nesting it. I'm re-expanding what the definition is of Q^pi.

1109
01:02:09,415 --> 01:02:11,755
And if you keep doing this forever,

1110
01:02:11,755 --> 01:02:15,040
essentially we just keep pushing in as if we get to continue to

1111
01:02:15,040 --> 01:02:18,745
take pi_i+1 on all future time steps.

1112
01:02:18,745 --> 01:02:22,975
And what- the key thing to notice here is that this is a greater than or equal to.

1113
01:02:22,975 --> 01:02:28,850
So, if you nest this in completely what you get is that this is the value pi_i+1.

1114
01:02:30,900 --> 01:02:33,430
So, there's kind of two key tricks in here.

1115
01:02:33,430 --> 01:02:35,140
The, the first thing is to say,

1116
01:02:35,140 --> 01:02:40,405
notice that the V^pi_i is always lower- is the lower bound to max a over Q^pi.

1117
01:02:40,405 --> 01:02:45,100
And then to re-express this using the definition of pi_i+1.

1118
01:02:45,100 --> 01:02:50,470
And then to re-upper bound that V by Q^pi and just keep re-expanding it.

1119
01:02:50,470 --> 01:02:54,685
And so you can do this out and then that allows you to

1120
01:02:54,685 --> 01:03:00,235
redefine to- when you substituted it in for all actions using pi_i+1,

1121
01:03:00,235 --> 01:03:03,580
then you've now defined what the value is of pi_i+1.

1122
01:03:03,580 --> 01:03:12,980
So, this is what it allows us to know that the new pi_i+1 value is by definition at least as good as the previous value function.

1123
01:03:14,640 --> 01:03:20,515
So, I'll just put that in there [inaudible]. All right.

1124
01:03:20,515 --> 01:03:22,330
So, the next questions that might come up is so we

1125
01:03:22,330 --> 01:03:24,460
know we're gonna get this monotonic improvement,

1126
01:03:24,460 --> 01:03:28,540
um, so the questions would be if the policy doesn't change, can it ever change again?

1127
01:03:28,540 --> 01:03:31,810
And is there a maximum number of iterations of policy iteration?

1128
01:03:31,810 --> 01:03:33,340
So, what do I mean by iterations?

1129
01:03:33,340 --> 01:03:35,200
Here iterations is i.

1130
01:03:35,200 --> 01:03:38,690
It's a kind of how many policies could we step through?

1131
01:03:38,820 --> 01:03:43,000
So, why don't we take like a minute and just think about this maybe talk to somebody

1132
01:03:43,000 --> 01:03:44,485
around you that you haven't met before

1133
01:03:44,485 --> 01:03:47,300
and just see what they think of these two questions.

1134
01:03:52,690 --> 01:03:56,095
So policy is monotonically improving

1135
01:03:56,095 --> 01:03:59,800
and is there a maximum number of iterations as we've read before?

1136
01:03:59,800 --> 01:04:06,540
[NOISE] Just in the interest

1137
01:04:06,540 --> 01:04:09,460
of time for today- just in the interest of

1138
01:04:09,460 --> 01:04:12,870
time for today because I want us to try to get through value iteration as well,

1139
01:04:12,870 --> 01:04:16,770
um, why doesn't- does somebody wanna give me, um,

1140
01:04:16,770 --> 01:04:21,335
a guess of whether or not the policy can ever- if the policy stops changing,

1141
01:04:21,335 --> 01:04:23,225
whether it can ever change again?

1142
01:04:23,225 --> 01:04:26,420
So, what I mean by that is that if the policy at pi,

1143
01:04:26,420 --> 01:04:27,910
so the question here was to say,

1144
01:04:27,910 --> 01:04:32,355
if pi of i+1 is equal to pi i for all states,

1145
01:04:32,355 --> 01:04:33,710
could it ever change again?

1146
01:04:33,710 --> 01:04:36,150
Somebody wanna share a guess of whether or not that is true.

1147
01:04:36,150 --> 01:04:39,020
Once it has stopped changing it can never change again.

1148
01:04:39,020 --> 01:04:42,460
So, no. And the second question is, um,

1149
01:04:42,460 --> 01:04:46,670
is there a maximum number of policy iterations? Yeah.

1150
01:04:46,670 --> 01:04:49,340
There's no- you can't have more iterations than there are policies.

1151
01:04:49,340 --> 01:04:53,380
That's right. There- We know that there is at most a to the s policies.

1152
01:04:53,380 --> 01:04:56,020
You cannot repeat a policy ever,

1153
01:04:56,020 --> 01:04:58,875
um, because of this monotonic improvement.

1154
01:04:58,875 --> 01:05:03,080
And so, there- there's a maximum number of iterations.

1155
01:05:03,080 --> 01:05:09,260
Okay? Great. And this just- um,

1156
01:05:09,260 --> 01:05:11,710
I'll skip through this now just so we can go through a bit of value iteration,

1157
01:05:11,710 --> 01:05:13,695
but this just steps through to show a little bit

1158
01:05:13,695 --> 01:05:16,395
more of how once your policy stopped changing,

1159
01:05:16,395 --> 01:05:18,860
essentially your Q^pi will be identical.

1160
01:05:18,860 --> 01:05:20,240
And so you can't- uh,

1161
01:05:20,240 --> 01:05:23,050
there's no policy improvements to be, yeah, to change.

1162
01:05:23,050 --> 01:05:26,160
After it's sort of converged, you're gonna stay there forever.

1163
01:05:26,240 --> 01:05:29,040
Okay, so policy iteration computes,

1164
01:05:29,040 --> 01:05:31,305
um, the optimal value in a policy in one way.

1165
01:05:31,305 --> 01:05:34,860
The idea in policy iteration is you always have a policy,

1166
01:05:34,860 --> 01:05:39,545
um, that is- that you know the value of it for the infinite horizon.

1167
01:05:39,545 --> 01:05:42,355
And then you incrementally try to improve it.

1168
01:05:42,355 --> 01:05:44,825
Value iteration is an alternative approach.

1169
01:05:44,825 --> 01:05:48,130
Value iteration in itself says we're gonna think of computing

1170
01:05:48,130 --> 01:05:51,450
the optimal value if you get to act for a finite number of steps.

1171
01:05:51,450 --> 01:05:55,070
The beginning just one step and then two steps and then three steps et cetera.

1172
01:05:55,070 --> 01:05:58,725
Um, and you just keep iterating to longer and longer.

1173
01:05:58,725 --> 01:06:01,210
So that's different, right? Because policy says you

1174
01:06:01,210 --> 01:06:03,460
always have a policy and you know what its value is.

1175
01:06:03,460 --> 01:06:04,880
It just might not be very good.

1176
01:06:04,880 --> 01:06:08,755
Value iteration says you always know what the optimal value in policy is,

1177
01:06:08,755 --> 01:06:11,490
but only if you're gonna get to act for say k time steps.

1178
01:06:11,490 --> 01:06:14,240
So they're just- they're computing different things,

1179
01:06:14,240 --> 01:06:18,105
um, and they both will converge to the same thing eventually.

1180
01:06:18,105 --> 01:06:20,810
So when we start to talk about value iteration,

1181
01:06:20,810 --> 01:06:22,365
it's useful to think about Bellman.

1182
01:06:22,365 --> 01:06:24,990
Um, so the Bellman equation and

1183
01:06:24,990 --> 01:06:28,070
Bellman backup operators are things that are often talked about in,

1184
01:06:28,070 --> 01:06:31,095
um, Markov Decision Processes and reinforcement learning.

1185
01:06:31,095 --> 01:06:34,700
So this constraint here that we've seen before,

1186
01:06:34,700 --> 01:06:37,010
which says that the value of a policy is

1187
01:06:37,010 --> 01:06:40,215
its immediate reward plus its discounted sum of future rewards,

1188
01:06:40,215 --> 01:06:42,655
um, is known as the Bellman equation.

1189
01:06:42,655 --> 01:06:45,215
The constraint for a Markov process, er,

1190
01:06:45,215 --> 01:06:48,220
Markov Decision Process say that it as to satisfy that.

1191
01:06:48,220 --> 01:06:50,200
And we can alternatively,

1192
01:06:50,200 --> 01:06:51,695
like what we were just seeing before,

1193
01:06:51,695 --> 01:06:53,395
think of this is as, um,

1194
01:06:53,395 --> 01:06:55,115
as a backup operator,

1195
01:06:55,115 --> 01:06:58,150
which means that we can apply it to

1196
01:06:58,150 --> 01:07:02,715
an old value function and transform it to a new value function.

1197
01:07:02,715 --> 01:07:06,275
So just like what we were doing in some of the, um, ah,

1198
01:07:06,275 --> 01:07:08,180
evaluation of a policy,

1199
01:07:08,180 --> 01:07:10,365
we can also just sort of do these operators.

1200
01:07:10,365 --> 01:07:12,550
In this case, the difference compared to what we've seen with

1201
01:07:12,550 --> 01:07:15,240
evaluation before is we're taking a max there.

1202
01:07:15,240 --> 01:07:18,500
We're taking this max a over th-

1203
01:07:18,500 --> 01:07:22,095
the best immediate already credit plus the discounted sum of future rewards.

1204
01:07:22,095 --> 01:07:26,020
So sometimes we'll use the notation of BV to mean a Bellman operator,

1205
01:07:26,020 --> 01:07:27,860
which means you take your old V and then you'd

1206
01:07:27,860 --> 01:07:31,060
plug it into here and you do this operation.

1207
01:07:31,870 --> 01:07:34,265
So how does value iteration work?

1208
01:07:34,265 --> 01:07:36,285
The algorithm can be summarized as follows.

1209
01:07:36,285 --> 01:07:40,535
You start off, you can initialize your value function to zero for all states.

1210
01:07:40,535 --> 01:07:44,100
And then you loop until you converge, um,

1211
01:07:44,100 --> 01:07:46,030
or if you're doing a finite horizon,

1212
01:07:46,030 --> 01:07:47,660
which we might not have time to get to today, but,

1213
01:07:47,660 --> 01:07:49,865
um, I- then you'd go to that horizon.

1214
01:07:49,865 --> 01:07:51,685
And basically, for each state,

1215
01:07:51,685 --> 01:07:53,740
you do this Bellman backup operator.

1216
01:07:53,740 --> 01:07:59,250
So you'd say, my value at k plus one time steps for that state is if I get to pick

1217
01:07:59,250 --> 01:08:02,275
the best immediate action plus the discounted sum of

1218
01:08:02,275 --> 01:08:07,580
future rewards using that old value function I had from the previous time step.

1219
01:08:07,800 --> 01:08:12,640
And that Vk said what is the optimal thing my optimal value for

1220
01:08:12,640 --> 01:08:17,859
that state s prime given that I got to act for k more time steps.

1221
01:08:17,859 --> 01:08:22,954
So that's why initializing it to zero is a good thing to do because in this case,

1222
01:08:22,955 --> 01:08:25,350
or a certainly reasonable thing to do if you want the result to

1223
01:08:25,350 --> 01:08:28,250
be the optimal as if you had that many time steps to go.

1224
01:08:28,250 --> 01:08:31,935
If you have no more time steps to act, your value is zero.

1225
01:08:31,935 --> 01:08:35,120
The first backup you do will basically say what is

1226
01:08:35,120 --> 01:08:38,460
the optimal immediate action you should take if you only get to take one action.

1227
01:08:38,460 --> 01:08:41,580
And then after that you start backing up,

1228
01:08:41,580 --> 01:08:43,090
um, and continuing to say well,

1229
01:08:43,090 --> 01:08:44,500
what if I got to act for two time steps?

1230
01:08:44,500 --> 01:08:46,145
What if I got to act for three time steps?

1231
01:08:46,145 --> 01:08:49,950
What's the best sequence of decisions you could do in each of those cases?

1232
01:08:50,620 --> 01:08:53,130
Um, again just in terms of

1233
01:08:53,130 --> 01:08:57,085
Bellman operations if we think back to sort of what policy iteration is doing,

1234
01:08:57,085 --> 01:09:01,880
you can instantiate this Bellman operator by fixing what the policy is.

1235
01:09:01,880 --> 01:09:05,640
And so, if you see sort of a B with, um, ah,

1236
01:09:05,640 --> 01:09:07,010
pi on top and saying, well,

1237
01:09:07,010 --> 01:09:09,055
instead of taking that max over actions,

1238
01:09:09,055 --> 01:09:12,255
you're specifying what is the action you get to take.

1239
01:09:12,255 --> 01:09:15,640
So policy evaluation you can think of as basically just

1240
01:09:15,640 --> 01:09:18,340
computing a fixed point of repeatedly applying

1241
01:09:18,340 --> 01:09:24,800
this Bellman backup until V stops converging and stops changing.

1242
01:09:25,880 --> 01:09:29,930
So, um, in terms of policy iteration,

1243
01:09:29,930 --> 01:09:32,149
this is very similar to what we saw before you can think of it

1244
01:09:32,149 --> 01:09:34,719
in these Bellman operators and doing this argmax.

1245
01:09:34,720 --> 01:09:38,505
Wanna see if we can get to a little bit on sort of the contraction operator.

1246
01:09:38,505 --> 01:09:41,399
So this is what, um, value iteration does.

1247
01:09:41,399 --> 01:09:44,364
It's a very similar policy iteration and evaluation.

1248
01:09:44,365 --> 01:09:48,345
Um, let me talk a little bit about the contraction aspect.

1249
01:09:48,345 --> 01:09:50,880
So, for any operator, um,

1250
01:09:50,880 --> 01:09:54,805
let's let O be an operator and x denote a norm of x.

1251
01:09:54,805 --> 01:09:57,810
So x could be a vector like a value function and then we could look at

1252
01:09:57,810 --> 01:10:01,575
like an L2 norm or an L1 norm or L infinity norm.

1253
01:10:01,575 --> 01:10:04,660
So, if you wanna- if an operator is

1254
01:10:04,660 --> 01:10:07,500
a contraction it means that if you apply it to two different things,

1255
01:10:07,500 --> 01:10:09,785
you can think of these as value functions, um,

1256
01:10:09,785 --> 01:10:13,430
then the distance between them shrinks after,

1257
01:10:13,430 --> 01:10:15,640
um, or at least is no bigger after you

1258
01:10:15,640 --> 01:10:18,730
apply the operator compared to their distance before.

1259
01:10:18,730 --> 01:10:22,000
So just to, um- actually,

1260
01:10:22,000 --> 01:10:23,520
I'll, I'll save examples for later.

1261
01:10:23,520 --> 01:10:25,240
Feel free to come up to me after class if you wanna see

1262
01:10:25,240 --> 01:10:27,375
an example of this, um, or I can do it on Piazza.

1263
01:10:27,375 --> 01:10:31,130
But this is the formal definition of what it means to be a contraction.

1264
01:10:31,130 --> 01:10:32,410
Is that the distance between,

1265
01:10:32,410 --> 01:10:35,260
in this case we're gonna think about it as two vectors, um,

1266
01:10:35,260 --> 01:10:39,120
doesn't get bigger and can shrink after you apply this operator.

1267
01:10:39,840 --> 01:10:43,570
So, the key question of whether or not value iteration will

1268
01:10:43,570 --> 01:10:47,820
converge is because the Bellman backup is a contraction operator.

1269
01:10:47,820 --> 01:10:52,115
And it's a contraction operator as long as gamma is less than one.

1270
01:10:52,115 --> 01:10:55,930
Which means that if you do- if let's say have two different Bell- er,

1271
01:10:55,930 --> 01:10:59,480
two different value functions and then you did the Bellman backup on both of them.

1272
01:10:59,480 --> 01:11:02,100
Then the distance between them would shrink.

1273
01:11:02,430 --> 01:11:05,110
So how do we prove this?

1274
01:11:05,110 --> 01:11:09,640
Um, we prove it- for interest of time I'll show you the proof.

1275
01:11:09,640 --> 01:11:10,970
Again, I'm happy to go through it,

1276
01:11:10,970 --> 01:11:13,455
um, I- or we can go through it in office hours et cetera.

1277
01:11:13,455 --> 01:11:15,975
Let me just show it kind of briefly.

1278
01:11:15,975 --> 01:11:20,620
So the idea to, to prove that the Bellman backup is a contraction operator,

1279
01:11:20,620 --> 01:11:24,220
is we consider there being two different value functions, k and j.

1280
01:11:24,220 --> 01:11:27,725
They don't have to be- This has- doesn't have to be anything to do with value iteration.

1281
01:11:27,725 --> 01:11:29,430
These are just two different value functions.

1282
01:11:29,430 --> 01:11:33,715
One could be, you know, 1,3,7,2 and the other one could be 5,6,9,8.

1283
01:11:33,715 --> 01:11:37,570
Okay. So we just have two different vectors of value functions and then we

1284
01:11:37,570 --> 01:11:41,740
re-express what they are after we apply the Bellman backup operator.

1285
01:11:41,740 --> 01:11:43,360
So there's that max a,

1286
01:11:43,360 --> 01:11:45,220
the immediate reward plus the discounted sum of

1287
01:11:45,220 --> 01:11:48,740
future rewards where we've plugged in our two different value functions.

1288
01:11:48,740 --> 01:11:50,900
And then what we say there is, well,

1289
01:11:50,900 --> 01:11:54,590
if you get to pick that max a separately for those two,

1290
01:11:54,590 --> 01:11:58,070
the distance between those is lower

1291
01:11:58,070 --> 01:11:59,820
bounded than if you kind of try to

1292
01:11:59,820 --> 01:12:03,735
maximize that difference there by putting that max a in.

1293
01:12:03,735 --> 01:12:06,130
And then you can cancel the rewards.

1294
01:12:06,130 --> 01:12:09,045
So that's what happens in the third line.

1295
01:12:09,045 --> 01:12:12,390
And then the next thing we can do is we can bound and say

1296
01:12:12,390 --> 01:12:15,020
the difference between these two value functions is diff- is,

1297
01:12:15,020 --> 01:12:18,230
um, bounded by the maximum of the distance between those two.

1298
01:12:18,230 --> 01:12:22,230
So you can pick the places at which those value functions most differ.

1299
01:12:22,230 --> 01:12:24,180
And then you can move it out of the sum.

1300
01:12:24,180 --> 01:12:28,090
And now you're summing over a probability distribution that has to sum to one.

1301
01:12:28,090 --> 01:12:31,640
And that gives you this. And so that means that

1302
01:12:31,640 --> 01:12:37,710
the Bellman backup as long as this is less than one has to be a contraction operator.

1303
01:12:37,710 --> 01:12:39,930
The distance between the two value functions can't be

1304
01:12:39,930 --> 01:12:43,960
larger after you apply the Bellman operator than it was before.

1305
01:12:45,700 --> 01:12:49,655
So, I think a good exercise to do, um,

1306
01:12:49,655 --> 01:12:52,735
is to then say given that it's a contraction operator,

1307
01:12:52,735 --> 01:12:55,555
um, that means it has to converge to a fixed point.

1308
01:12:55,555 --> 01:12:57,530
There has to be a unique solution.

1309
01:12:57,530 --> 01:13:00,800
So if you apply the Bellman operator repeatedly you- there is

1310
01:13:00,800 --> 01:13:03,630
a single fixed point that you will go to which is a single,

1311
01:13:03,630 --> 01:13:06,940
um, vector value fun- uh, values.

1312
01:13:06,940 --> 01:13:10,540
It's also good to think about whether the initialization and values impacts

1313
01:13:10,540 --> 01:13:14,980
anything if you only care about the result after it's converged.

1314
01:13:15,810 --> 01:13:19,110
All right. So, um, I think we can halt there.

1315
01:13:19,110 --> 01:13:20,160
Class is basically over.

1316
01:13:20,160 --> 01:13:22,835
There's a little bit more in the slides to talk about, um,

1317
01:13:22,835 --> 01:13:24,820
the finite horizon case, um,

1318
01:13:24,820 --> 01:13:28,950
and feel free to reach out to us on Piazza with any questions. Thanks. [NOISE]

