1
00:00:04,280 --> 00:00:06,810
All right. We're gonna go ahead and get started,

2
00:00:06,810 --> 00:00:08,595
and in consistent with this, uh,

3
00:00:08,595 --> 00:00:10,800
theme of this section, we're gonna be optimistic under

4
00:00:10,800 --> 00:00:13,470
uncertainty and hope that this will work but we will see.

5
00:00:13,470 --> 00:00:16,230
Um, before we get into the content, um,

6
00:00:16,230 --> 00:00:19,620
I wanted to ask if anybody has any questions about logistics or,

7
00:00:19,620 --> 00:00:22,035
um, any other aspects of the general course. Yeah.

8
00:00:22,035 --> 00:00:26,040
I just wanna double check with people who are doing the default project are okay,

9
00:00:26,040 --> 00:00:27,960
either not submitting or just submitting

10
00:00:27,960 --> 00:00:31,360
another thing for the milestone instead of doing the default.

11
00:00:32,090 --> 00:00:36,240
Yes. Yeah, so, the question was asking about, um, to repeat.

12
00:00:36,240 --> 00:00:38,300
The question was asking about whether or not people who were doing

13
00:00:38,300 --> 00:00:41,480
the default project if they need to do anything in particular for the milestone.

14
00:00:41,480 --> 00:00:43,475
Um, no, you don't.

15
00:00:43,475 --> 00:00:48,920
Any other questions? Okay. So, just as a reminder,

16
00:00:48,920 --> 00:00:51,290
where we are sort of in the class right now is, um,

17
00:00:51,290 --> 00:00:55,085
last time we started talking about bandits and regret and we'll do,

18
00:00:55,085 --> 00:00:56,540
um, a brief, uh,

19
00:00:56,540 --> 00:00:59,885
recap of that today and we're gonna continue to talk about fast learning.

20
00:00:59,885 --> 00:01:04,160
Um, and we're gonna go from Bayesian bandits towards Markov decision processes today,

21
00:01:04,160 --> 00:01:07,280
and then on Wednesday we're gonna talk some more about fast learning and then we're

22
00:01:07,280 --> 00:01:11,520
gonna try to talk about sort of fast learning and exploration.

23
00:01:16,400 --> 00:01:18,690
And just to remind us all about,

24
00:01:18,690 --> 00:01:20,370
like, why we're doing this, um,

25
00:01:20,370 --> 00:01:24,320
the idea was that if we wanna move reinforcement learning into real-world applications,

26
00:01:24,320 --> 00:01:27,860
we need to think about carefully taking the data that we have, um,

27
00:01:27,860 --> 00:01:30,950
and how do we gather it and how do we best use it so that we don't need to

28
00:01:30,950 --> 00:01:35,280
collect a lot of data in order for our agents to learn to make good decisions.

29
00:01:35,280 --> 00:01:36,950
One of my original interests in

30
00:01:36,950 --> 00:01:39,350
this whole topic was to think sort of formally about what does

31
00:01:39,350 --> 00:01:43,160
it mean for an agent to learn to make good decisions and what are the sort of,

32
00:01:43,160 --> 00:01:45,860
information theoretic limits of how much information

33
00:01:45,860 --> 00:01:50,250
would an agent need in order to be able to make a provably optimal decision.

34
00:01:50,630 --> 00:01:54,560
So, we've been thinking about sort of a couple of different main things here.

35
00:01:54,560 --> 00:01:56,030
We're, we're talking about a couple of settings.

36
00:01:56,030 --> 00:01:57,620
Last time we talked about bandits,

37
00:01:57,620 --> 00:02:00,545
today we'll also talk about bandits and Markov decision processes.

38
00:02:00,545 --> 00:02:02,270
We're talking about frameworks,

39
00:02:02,270 --> 00:02:05,375
which are ways for us to formally assess how good an algorithm is.

40
00:02:05,375 --> 00:02:08,690
Um, you know, these could be the framework of empirical success.

41
00:02:08,690 --> 00:02:10,039
We, we talked last time about

42
00:02:10,039 --> 00:02:12,920
the mathematical framework of regret and we'll talk today about

43
00:02:12,920 --> 00:02:14,840
some other frameworks for evaluating in

44
00:02:14,840 --> 00:02:17,420
general how good a reinforcement learning algorithm is.

45
00:02:17,420 --> 00:02:20,240
And then, we're also talking about styles of approaches that

46
00:02:20,240 --> 00:02:23,390
tend to allow us to achieve these different frameworks.

47
00:02:23,390 --> 00:02:28,360
Um, and last time what we started to do is to talk about optimism under uncertainty.

48
00:02:28,360 --> 00:02:31,965
So, just a quick recap from bandits.

49
00:02:31,965 --> 00:02:36,290
Um, so, bandit was basically a simplified version of a

50
00:02:36,290 --> 00:02:40,660
Markov decision process where in the most simple setting there's no state,

51
00:02:40,660 --> 00:02:42,880
there's a set of actions, um,

52
00:02:42,880 --> 00:02:45,110
and now we're going to think specifically about the fact that

53
00:02:45,110 --> 00:02:48,670
the reward is through some stochastic distribution.

54
00:02:48,670 --> 00:02:52,070
Um, there's some unknown probability distribution over rewards.

55
00:02:52,070 --> 00:02:55,505
Um, at each timestep you get to select an action,

56
00:02:55,505 --> 00:02:57,200
um, and see, you know, a reward.

57
00:02:57,200 --> 00:02:59,540
And then your goal is to select actions in

58
00:02:59,540 --> 00:03:02,255
a way that is gonna optimize your rewards over time.

59
00:03:02,255 --> 00:03:05,945
And the reason this was different than a supervised learning problem is that you

60
00:03:05,945 --> 00:03:09,625
only get to observe the reward for the action that you sample.

61
00:03:09,625 --> 00:03:11,870
So, it's what's known as censored data.

62
00:03:11,870 --> 00:03:14,990
Um, you don't get to see what would have happened if you'd went to Harvard.

63
00:03:14,990 --> 00:03:17,420
So, um, we get to see censored data and

64
00:03:17,420 --> 00:03:19,640
we have to use that censored data to make good decisions.

65
00:03:19,640 --> 00:03:25,410
Um, and what we discussed here talking about regret, what we mean that in a,

66
00:03:25,410 --> 00:03:29,015
in a formal mathematical sense is that we were comparing, um,

67
00:03:29,015 --> 00:03:32,495
the expected reward from the action we took,

68
00:03:32,495 --> 00:03:36,340
um, to the expected reward of the optimal action.

69
00:03:36,340 --> 00:03:39,710
Now, notice that all of these things are stochastic.

70
00:03:39,710 --> 00:03:43,430
Um, so, it doesn't have to have a particular parametric distribution

71
00:03:43,430 --> 00:03:48,210
but imagine that we're thinking about, um, Gaussians.

72
00:03:48,590 --> 00:03:51,270
Let's say, we had two Gaussians.

73
00:03:51,270 --> 00:03:55,420
So, this is action 2 and this is action 1.

74
00:03:55,520 --> 00:03:58,830
Okay. So, here's the mean of action 1.

75
00:03:58,830 --> 00:04:03,540
So, Q of a1 is greater than Q of a2.

76
00:04:03,540 --> 00:04:06,755
So, action 1 has the better expected reward.

77
00:04:06,755 --> 00:04:08,960
But notice that on any particular trial,

78
00:04:08,960 --> 00:04:10,490
you might sometimes get a,

79
00:04:10,490 --> 00:04:11,600
um, uh, you could,

80
00:04:11,600 --> 00:04:14,120
ima- imagine getting a result where, um,

81
00:04:14,120 --> 00:04:15,950
the actual reward you get from

82
00:04:15,950 --> 00:04:19,480
a sub-optimal arm is better than the expected reward of the optimal arm.

83
00:04:19,480 --> 00:04:21,120
I just wanna highlight that.

84
00:04:21,120 --> 00:04:24,995
So, imagine that you've sampled from action A2 and you got here.

85
00:04:24,995 --> 00:04:27,900
It's a particular sample.

86
00:04:30,220 --> 00:04:33,625
So, you could have a particular sample

87
00:04:33,625 --> 00:04:36,460
be better than the expected reward of the optimal arm.

88
00:04:36,460 --> 00:04:38,470
But because we're imagining doing this many,

89
00:04:38,470 --> 00:04:40,825
many times we're again just looking at expectations.

90
00:04:40,825 --> 00:04:42,385
So, we're saying, "On average,

91
00:04:42,385 --> 00:04:43,780
which is the best arm,

92
00:04:43,780 --> 00:04:48,410
and on average how much do we lose from selecting a sub-optimal arm?"

93
00:04:48,490 --> 00:04:52,450
And the goal was to minimize our total regret

94
00:04:52,450 --> 00:04:56,810
which is equivalent to maximizing our cumulative reward over time.

95
00:04:57,320 --> 00:05:02,615
So, we then introduced this idea of optimism under, of, under uncertainty.

96
00:05:02,615 --> 00:05:06,090
Um, and the idea was to, um, uh, uh,

97
00:05:06,090 --> 00:05:11,690
estimate an upper confidence bound on the potential expected reward of each of the arms.

98
00:05:11,690 --> 00:05:17,325
So, this was to say we wanna be able to say for each of the arms,

99
00:05:17,325 --> 00:05:20,090
what do we think is an upper bound of their expected value,

100
00:05:20,090 --> 00:05:21,995
and then when we act,

101
00:05:21,995 --> 00:05:25,870
we're gonna pick whichever arm has the highest upper confidence bound.

102
00:05:25,870 --> 00:05:29,070
And this was gonna lead to one of two outcomes.

103
00:05:29,070 --> 00:05:32,630
So, this could, um, two things could happen.

104
00:05:35,190 --> 00:05:43,470
So, either, either at is equal to A star,

105
00:05:43,470 --> 00:05:46,990
and in that case, what's our regret?

106
00:05:47,210 --> 00:05:49,640
0. So, if we select

107
00:05:49,640 --> 00:05:52,690
the optimal action and we have our regret at 0. So, that's good.

108
00:05:52,690 --> 00:05:59,800
And we have our regret at 0 like at per timestep or it's not.

109
00:06:02,420 --> 00:06:05,210
And if it's not on average,

110
00:06:05,210 --> 00:06:07,060
what happens to that upper confidence bound?

111
00:06:07,060 --> 00:06:10,600
Yeah.

112
00:06:10,600 --> 00:06:11,375
Yes.

113
00:06:11,375 --> 00:06:13,005
Yeah, [NOISE] that answer is correct.

114
00:06:13,005 --> 00:06:14,625
So, we lower it.

115
00:06:14,625 --> 00:06:19,230
So, if we get, if we select an arm which is, um, not optimal,

116
00:06:19,230 --> 00:06:22,210
then it means its real mean is lower, um, uh,

117
00:06:22,210 --> 00:06:24,650
than the upper confidence bound we're averaging,

118
00:06:24,650 --> 00:06:26,420
um, at least with high probability.

119
00:06:26,420 --> 00:06:31,560
And so, then in general our UT of AT will decrease.

120
00:06:32,780 --> 00:06:35,840
So, we're gonna gain information about what

121
00:06:35,840 --> 00:06:39,080
the real mean is of that arm and we'll reduce it.

122
00:06:39,080 --> 00:06:40,460
And if we reduce it enough,

123
00:06:40,460 --> 00:06:45,060
over time we should find that the optimal arm's upper confidence bound is higher.

124
00:06:45,410 --> 00:06:49,190
So, I'll ask you to play about what might happen if we do lower bounds.

125
00:06:49,190 --> 00:06:52,100
Um, but that's one of the reasons why upper bounds is really good, and,

126
00:06:52,100 --> 00:06:55,070
and I mentioned that these ideas have really been around for a long time,

127
00:06:55,070 --> 00:06:58,635
um, at least around like 20, almos- 25-30 years.

128
00:06:58,635 --> 00:07:01,485
So, I think the first one was 1993,

129
00:07:01,485 --> 00:07:05,070
Kaelbling, by Leslie Kaelbling. She's at MIT.

130
00:07:05,070 --> 00:07:09,440
Um, don't remember if it was her PhD thesis or if it was just after that.

131
00:07:09,440 --> 00:07:13,800
Um, uh, but she talked about this idea of interval estimation of estimating,

132
00:07:13,800 --> 00:07:15,545
um, the potential rewards.

133
00:07:15,545 --> 00:07:18,350
She didn't do formal proofs of this being a good idea,

134
00:07:18,350 --> 00:07:20,000
but she did it for Markov decision processes,

135
00:07:20,000 --> 00:07:21,470
and they found that it was, uh,

136
00:07:21,470 --> 00:07:24,030
a very good idea in terms of empirical performance,

137
00:07:24,030 --> 00:07:26,420
and then a lot of people went around and did

138
00:07:26,420 --> 00:07:29,465
the theoretical analysis and showed that provably this is a good thing.

139
00:07:29,465 --> 00:07:31,820
And I think it's interesting often about which area ends up

140
00:07:31,820 --> 00:07:35,585
being more advanced whether it's the empirical side or the theoretical side.

141
00:07:35,585 --> 00:07:37,580
Okay. So, optimism under uncertainty in

142
00:07:37,580 --> 00:07:41,300
the bandit case just involves keeping track of the rewards we've seen and we showed,

143
00:07:41,300 --> 00:07:42,500
saw that we could use things like

144
00:07:42,500 --> 00:07:47,940
Hoeffding inequality to compute these upper confidence bounds.

145
00:07:49,660 --> 00:07:53,915
Because remember each of our samples from the arms is iid,

146
00:07:53,915 --> 00:07:56,690
because they're all coming from the same parametric distribution,

147
00:07:56,690 --> 00:07:59,560
which is unknown and we're given these samples.

148
00:07:59,560 --> 00:08:03,400
So, what we found last time is that, uh,

149
00:08:03,400 --> 00:08:06,500
if we use the upper confidence bound algorithm I did a proof on the board

150
00:08:06,500 --> 00:08:10,570
to show that with high probability we had logarithmic regret.

151
00:08:10,570 --> 00:08:13,235
Why was this important? Because we looked at

152
00:08:13,235 --> 00:08:16,205
greedy algorithms and showed that they could have linear regret.

153
00:08:16,205 --> 00:08:17,960
And what is it linear in?

154
00:08:17,960 --> 00:08:21,200
So, um, this is the number of timesteps,

155
00:08:21,200 --> 00:08:24,150
number of timesteps we act.

156
00:08:25,920 --> 00:08:28,720
So why is linear regret bad?

157
00:08:28,720 --> 00:08:29,890
Well, if it's linear,

158
00:08:29,890 --> 00:08:32,260
it means that essentially you could be making the worst decision on

159
00:08:32,260 --> 00:08:34,735
every single time point, and that's pretty bad.

160
00:08:34,735 --> 00:08:37,419
So we'd like to have things that are going slower which means

161
00:08:37,419 --> 00:08:40,689
that our algorithm is essentially learning to make good decisions.

162
00:08:40,690 --> 00:08:42,580
Now, notice that in this case,

163
00:08:42,580 --> 00:08:44,500
this is a little bit different, um,

164
00:08:44,500 --> 00:08:46,825
statement of our result than what we saw before.

165
00:08:46,825 --> 00:08:49,855
Um, it's related, this involves the gaps.

166
00:08:49,855 --> 00:08:58,555
This is the gaps, delta a is equal to Q of a star, - q of a.

167
00:08:58,555 --> 00:09:00,655
It's how much worse,

168
00:09:00,655 --> 00:09:03,835
it is to take a particular action than to take the optimal action.

169
00:09:03,835 --> 00:09:10,855
Um, and this is related to a bit different,

170
00:09:10,855 --> 00:09:14,540
to the bound from last time.

171
00:09:15,570 --> 00:09:20,200
Last time, we proved a bound that was independent of the gaps,

172
00:09:20,200 --> 00:09:22,735
so it didn't matter what the gaps are in the problem.

173
00:09:22,735 --> 00:09:25,555
Um, this is the bound that depends on the gaps.

174
00:09:25,555 --> 00:09:28,075
Now, of course, you don't know what the gaps are in practice.

175
00:09:28,075 --> 00:09:29,110
If you didn't know the gaps,

176
00:09:29,110 --> 00:09:31,075
then you would know the optimal arm to prove, um,

177
00:09:31,075 --> 00:09:35,790
but the nice thing about this is that i- it's always important to know whether or

178
00:09:35,790 --> 00:09:40,470
not this sort of knowledge appears in the analysis or in the algorithm.

179
00:09:40,470 --> 00:09:43,020
This is saying, you don't need to know what the gaps are,

180
00:09:43,020 --> 00:09:44,560
but if you use this algorithm,

181
00:09:44,560 --> 00:09:49,345
[NOISE] how your, how your regret grows depends on a property of the domain.

182
00:09:49,345 --> 00:09:51,445
You don't have to know the property of that domain,

183
00:09:51,445 --> 00:09:53,605
but that's what your regret will depend on.

184
00:09:53,605 --> 00:09:57,295
And so this is saying that for upper confidence bounds,

185
00:09:57,295 --> 00:10:00,340
if you have different sizes of gaps,

186
00:10:00,340 --> 00:10:01,825
you're gonna get different regrets.

187
00:10:01,825 --> 00:10:05,695
Um, your algorithm is just going to proceed by following upper confidence bounds,

188
00:10:05,695 --> 00:10:07,345
it doesn't need to know about these gaps,

189
00:10:07,345 --> 00:10:10,480
you're gonna get better or worse performance depending on what the gaps are.

190
00:10:10,480 --> 00:10:12,550
And in general, we'd really like these.

191
00:10:12,550 --> 00:10:15,250
We'd like our algorithms to be able to adapt to the problem so

192
00:10:15,250 --> 00:10:19,100
that thi- this is known as a problem-dependent bound,

193
00:10:19,440 --> 00:10:23,050
right here, and you would like those.

194
00:10:23,050 --> 00:10:25,090
You'd like to have algorithms that are agnostic,

195
00:10:25,090 --> 00:10:26,545
that are sort of saying,

196
00:10:26,545 --> 00:10:29,470
provide problem-dependent balance, that they work better if the problem

197
00:10:29,470 --> 00:10:32,590
is easier. All right.

198
00:10:32,590 --> 00:10:34,345
So let's go to our toy example,

199
00:10:34,345 --> 00:10:37,285
which we were talking about as a way to sort of see how these different algorithms work,

200
00:10:37,285 --> 00:10:39,700
and we're looking at fake, um, ah,

201
00:10:39,700 --> 00:10:41,980
ways to treat broken toes, um,

202
00:10:41,980 --> 00:10:43,300
where we are looking at surgery,

203
00:10:43,300 --> 00:10:44,875
buddy taping, or doing nothing,

204
00:10:44,875 --> 00:10:47,170
and we're imagining that we had a,

205
00:10:47,170 --> 00:10:50,620
a Bernoulli variable that determined whether or not these treatments worked.

206
00:10:50,620 --> 00:10:54,175
So surgery on in expectation was the most expect- uh,

207
00:10:54,175 --> 00:10:57,040
most effective thing, with 0.95 success,

208
00:10:57,040 --> 00:10:58,990
ah, buddy taping was 0.9,

209
00:10:58,990 --> 00:11:00,640
and doing nothing was 0.1.

210
00:11:00,640 --> 00:11:03,775
So how would something like upper confidence bound algorithms work?

211
00:11:03,775 --> 00:11:05,320
Well, in the beginning we don't have any data,

212
00:11:05,320 --> 00:11:07,720
so let's sample all the arms once.

213
00:11:07,720 --> 00:11:11,440
That's going to involve sampling from a Bernoulli distribution.

214
00:11:11,440 --> 00:11:12,790
Um, so in this case,

215
00:11:12,790 --> 00:11:14,605
let's imagine that we've got,

216
00:11:14,605 --> 00:11:17,185
um, a1, a2, and a3 here.

217
00:11:17,185 --> 00:11:18,730
And note that this is,

218
00:11:18,730 --> 00:11:21,580
[NOISE], yeah, all right,

219
00:11:21,580 --> 00:11:27,085
this is our, their empirical estimate where we just average,

220
00:11:27,085 --> 00:11:31,060
over all the, all the rewards we've seen from a particular arm.

221
00:11:31,060 --> 00:11:35,590
Okay? So we pulled each arm once,

222
00:11:35,590 --> 00:11:38,695
which is the same as taking each action once, we got 1-1-0.

223
00:11:38,695 --> 00:11:41,680
And now what we have to do is compute the upper confidence bounds

224
00:11:41,680 --> 00:11:45,520
for each of those arms before we know what to do next.

225
00:11:45,520 --> 00:11:48,265
Okay. So in this case,

226
00:11:48,265 --> 00:11:52,360
we're gonna define our upper confidence bounds by being the empirical average,

227
00:11:52,360 --> 00:11:55,555
plus the square root of 2 log t,

228
00:11:55,555 --> 00:11:58,090
t is the number of times we pulled arms,

229
00:11:58,090 --> 00:12:01,780
and N t of a is the number of times we've sampled a particular arm.

230
00:12:01,780 --> 00:12:05,000
So this is total arm pulls.

231
00:12:07,290 --> 00:12:11,630
This is a particular arm.

232
00:12:13,200 --> 00:12:16,030
Okay. So what does that gonna be in this case?

233
00:12:16,030 --> 00:12:17,410
Let's just define it for each of them.

234
00:12:17,410 --> 00:12:21,715
So UCB of a1 is gonna be equal to 1,

235
00:12:21,715 --> 00:12:24,280
because that's what we've got so far,

236
00:12:24,280 --> 00:12:27,970
plus square root 2 log of 3,

237
00:12:27,970 --> 00:12:32,440
because we pulled three arms so far, divided by 1.

238
00:12:32,440 --> 00:12:37,045
UCB of a2 is gonna be the same,

239
00:12:37,045 --> 00:12:39,835
because we've also pulled that arm once,

240
00:12:39,835 --> 00:12:41,845
and it got the same outcome.

241
00:12:41,845 --> 00:12:45,115
And then UCB of a3 is gonna be different,

242
00:12:45,115 --> 00:12:49,135
because its current reward is- or current expected value is 0.

243
00:12:49,135 --> 00:12:55,660
So it's just gonna be equal to 2 log 3 divided by 1.

244
00:12:55,820 --> 00:12:58,710
That's how we could instantiate each of the bounds,

245
00:12:58,710 --> 00:13:01,080
and now we've defined the upper confidence bounds for each of the,

246
00:13:01,080 --> 00:13:02,840
ah, um, each of the arms.

247
00:13:02,840 --> 00:13:04,390
So in this case,

248
00:13:04,390 --> 00:13:07,405
after we've done that, we're going to pull one of these arms.

249
00:13:07,405 --> 00:13:09,535
Um, let's say that we break ties randomly,

250
00:13:09,535 --> 00:13:12,685
so the upper confidence bound of a1 and a2 is identical.

251
00:13:12,685 --> 00:13:15,325
So with 50% probability, we select one.

252
00:13:15,325 --> 00:13:18,500
With 50% probability we can select the other.

253
00:13:18,500 --> 00:13:23,590
Okay. Um, and let's just compare that for a second.

254
00:13:23,590 --> 00:13:27,925
So, um, if we're using UCB, I said that,

255
00:13:27,925 --> 00:13:32,060
so I'll just redefine this here so people can remember,

256
00:13:32,060 --> 00:13:35,945
is equal to UCB of a2.

257
00:13:35,945 --> 00:13:39,850
This is- UCB stands for upper confidence bound is equal to 1,

258
00:13:39,850 --> 00:13:44,545
plus square root 2 log 3 divided by 1.

259
00:13:44,545 --> 00:13:52,465
And UCB of a3 is equal to square root 2 log 3 divided by 1.

260
00:13:52,465 --> 00:13:55,495
Okay. So why don't we just take a second, um,

261
00:13:55,495 --> 00:13:57,970
and define what would be the probability of selecting

262
00:13:57,970 --> 00:14:01,145
each arm if you're using e-greedy with epsilon = 0.1.

263
00:14:01,145 --> 00:14:03,890
And what about if you're using UCB?

264
00:14:04,530 --> 00:14:07,300
As always, feel free to talk to anybody nearby.

265
00:14:07,300 --> 00:14:46,780
[NOISE]

266
00:14:46,780 --> 00:14:49,780
All right, so let's vote [NOISE].

267
00:14:49,780 --> 00:14:52,810
I'm gonna ask you to vote if, um,

268
00:14:52,810 --> 00:14:54,800
if two arms have, um,

269
00:14:54,800 --> 00:14:57,850
non-zero probability or three arms have non-zero probability.

270
00:14:57,850 --> 00:15:00,580
So if you're using UCB [NOISE],

271
00:15:00,580 --> 00:15:03,910
do two arms have a non-zero probability?

272
00:15:03,910 --> 00:15:06,895
Do three arms have a non-zero probability?

273
00:15:06,895 --> 00:15:09,400
Somebody who know- who thought with UCB,

274
00:15:09,400 --> 00:15:13,550
you only have two arms with non-zero probability want to explain why? Yeah.

275
00:15:14,400 --> 00:15:20,770
Because since you're picking the maximum action you're only going to pick a1 or a2.

276
00:15:20,770 --> 00:15:22,480
That's right, yeah. So, um,

277
00:15:22,480 --> 00:15:24,790
if we're picking the maximum action here, um,

278
00:15:24,790 --> 00:15:26,710
we're only gonna pick, um,

279
00:15:26,710 --> 00:15:27,820
action a1 or a2,

280
00:15:27,820 --> 00:15:29,950
we have zero probability of my third arm.

281
00:15:29,950 --> 00:15:33,145
Okay. Let's do a quick vote for, um, e-greedy.

282
00:15:33,145 --> 00:15:36,280
For e-greedy, do we have non-zero probability on two arms?

283
00:15:36,280 --> 00:15:38,860
On three arms? That's right.

284
00:15:38,860 --> 00:15:41,140
What's the probability of selecting a3?

285
00:15:41,140 --> 00:15:44,560
[NOISE]. Yeah, [inaudible]

286
00:15:44,560 --> 00:15:51,400
[NOISE] Um, 0.1 [NOISE].

287
00:15:51,400 --> 00:15:53,020
Anyone else wanna add? Yeah?

288
00:15:53,020 --> 00:15:55,075
It's 0.03.

289
00:15:55,075 --> 00:15:57,820
Yes. Or 0323, yes, exactly.

290
00:15:57,820 --> 00:15:59,830
So in the 0.1 in this case, just,

291
00:15:59,830 --> 00:16:02,380
um, remember we normally define that by uniformly splitting.

292
00:16:02,380 --> 00:16:06,355
Yeah. So we're gonna just have 0.1 divided by the number of arms.

293
00:16:06,355 --> 00:16:10,480
Okay. So here, um, wh- why do I bring this up?

294
00:16:10,480 --> 00:16:13,480
So I bring this up to indicate that while UCB is still, um,

295
00:16:13,480 --> 00:16:15,880
splitting its attention among all the arms that look good,

296
00:16:15,880 --> 00:16:18,925
it's not putting any weight right now on the arms that it doesn't think could be good,

297
00:16:18,925 --> 00:16:20,815
which in this case is arm 3.

298
00:16:20,815 --> 00:16:23,005
Whereas an e-greedy, um,

299
00:16:23,005 --> 00:16:26,845
approach is gonna be uniform probability across anything it doesn't think it's best.

300
00:16:26,845 --> 00:16:29,800
So this is one of the insights for why these arguments might be better,

301
00:16:29,800 --> 00:16:33,430
is they're being more strategic in how they're weighing, um,

302
00:16:33,430 --> 00:16:34,645
what arms to pull, um,

303
00:16:34,645 --> 00:16:37,495
compared to what a- what an epsilon greedy,

304
00:16:37,495 --> 00:16:39,590
which is just doing uniform.

305
00:16:39,750 --> 00:16:42,250
Okay. So let's look at, um,

306
00:16:42,250 --> 00:16:45,010
sort of, uh, what the regret would be in this case.

307
00:16:45,010 --> 00:16:47,890
So, um, the actions we pulled is a1,

308
00:16:47,890 --> 00:16:50,005
a2, a3, a1, a2.

309
00:16:50,005 --> 00:16:52,675
Um, so why would,

310
00:16:52,675 --> 00:16:54,345
why would we pull a2 again,

311
00:16:54,345 --> 00:16:55,670
let's just go through that briefly.

312
00:16:55,670 --> 00:16:57,320
So let's say, um,

313
00:16:57,320 --> 00:16:58,475
so first, let's say we're, we're,

314
00:16:58,475 --> 00:16:59,675
we're gonna pull a1.

315
00:16:59,675 --> 00:17:01,350
Let's imagine that's which one we picked.

316
00:17:01,350 --> 00:17:02,860
Okay, so we pulled a1.

317
00:17:02,860 --> 00:17:04,609
So let me just go through one more step of what

318
00:17:04,609 --> 00:17:06,784
the upper confidence bounds would be in this case.

319
00:17:06,785 --> 00:17:10,280
So let's say we pull a1, okay.

320
00:17:10,280 --> 00:17:13,130
So now, we need to redefine the upper confidence bounds,

321
00:17:13,130 --> 00:17:15,775
we actually need to redefine them for all the arms,

322
00:17:15,775 --> 00:17:20,089
because if the denominator of that upper confidence bound, it depends on t,

323
00:17:20,089 --> 00:17:22,234
which is the total number of pulls

324
00:17:22,234 --> 00:17:25,009
or the total number of pulls so far, if we're using that form.

325
00:17:25,010 --> 00:17:27,305
So now, we're gonna have that UCB.

326
00:17:27,305 --> 00:17:29,545
Let's say we pulled action a1,

327
00:17:29,545 --> 00:17:32,000
and let's say I got a 1.

328
00:17:32,000 --> 00:17:37,724
Okay? So UCB of a1 is gonna have the same mean which is 1,

329
00:17:37,724 --> 00:17:40,250
plus square root 2 log 4,

330
00:17:40,250 --> 00:17:42,020
because we've now pulled things four times,

331
00:17:42,020 --> 00:17:43,745
but now, we pulled this arm twice.

332
00:17:43,745 --> 00:17:45,925
So it's- we're gonna divide this by 2.

333
00:17:45,925 --> 00:17:51,435
UCB of a2 is gonna have the same mean as before because we didn't pull it.

334
00:17:51,435 --> 00:17:55,040
And then it's gonna also have the 2 log 4,

335
00:17:55,040 --> 00:17:57,405
but we've only pulled it once,

336
00:17:57,405 --> 00:18:03,130
and UCB of a3 still has a mean of 0,

337
00:18:03,130 --> 00:18:08,630
and it's also gonna have 2 log 4 divided by 1 [NOISE].

338
00:18:08,630 --> 00:18:09,850
Okay. So in this case,

339
00:18:09,850 --> 00:18:11,890
what we're gonna find is that we sort of are gonna get

340
00:18:11,890 --> 00:18:16,400
this trading off, we still have the same empirical mean for a1 and a2.

341
00:18:16,400 --> 00:18:19,145
But now, we haven't pulled a2 as much as a1,

342
00:18:19,145 --> 00:18:21,470
so we're gonna flip, and we're gonna pick a2 now.

343
00:18:21,470 --> 00:18:25,365
So that's why if we imagine that we got,

344
00:18:25,365 --> 00:18:28,400
well actually, as a quick check your understanding, um,

345
00:18:28,400 --> 00:18:31,955
ah, this result would happen whether we picked 1,

346
00:18:31,955 --> 00:18:33,620
whether we got a 1 or 0,

347
00:18:33,620 --> 00:18:36,070
for a1 when we last pulled it.

348
00:18:36,070 --> 00:18:38,200
Um, so it's a good thing to check, is all I'm saying.

349
00:18:38,200 --> 00:18:40,040
But even if we got a 1 for a1,

350
00:18:40,040 --> 00:18:42,665
we'd still select a2 on the next round,

351
00:18:42,665 --> 00:18:45,965
because the upper confidence bound of it would drop,

352
00:18:45,965 --> 00:18:48,200
even if its mean was the same.

353
00:18:48,200 --> 00:18:50,560
So if we, if we look at this then,

354
00:18:50,560 --> 00:18:52,010
so we're gonna compare it to what would be

355
00:18:52,010 --> 00:18:54,785
the optimal action if we take him out the whole time, which is a1.

356
00:18:54,785 --> 00:18:55,970
So what is our regret?

357
00:18:55,970 --> 00:18:58,245
Our regret is gonna be 0,

358
00:18:58,245 --> 00:19:00,505
and then here it's gonna be 0.05,

359
00:19:00,505 --> 00:19:03,445
which is just equal to 0.95 - 0.9.

360
00:19:03,445 --> 00:19:09,415
Here it's gonna be 0.85, because it's 0.95 - 0.1.

361
00:19:09,415 --> 00:19:11,080
Here it's gonna be 0,

362
00:19:11,080 --> 00:19:14,365
and here it is going to be 0.05 again.

363
00:19:14,365 --> 00:19:17,080
So that's how we'd sum up regret, the regret.

364
00:19:17,080 --> 00:19:18,340
Of course, we don't actually know this,

365
00:19:18,340 --> 00:19:20,450
and we can know this if we're doing this in the simulated world,

366
00:19:20,450 --> 00:19:22,910
but we can't knew- know this in reality because, again,

367
00:19:22,910 --> 00:19:24,455
if we actually knew it in reality,

368
00:19:24,455 --> 00:19:26,105
then we wouldn't have to be doing this.

369
00:19:26,105 --> 00:19:30,890
You would know the optimal arm. May I have any other questions

370
00:19:30,890 --> 00:19:36,405
about how UCB is working? Okay.

371
00:19:36,405 --> 00:19:39,910
Now, just I guess, one other quick note here which is, you know,

372
00:19:39,910 --> 00:19:43,450
these upper confidence bounds are high probability bounds, so they can fail.

373
00:19:43,450 --> 00:19:47,410
That is possible that sometimes the upper confidence bound is lower than,

374
00:19:47,410 --> 00:19:48,970
um, the true mean.

375
00:19:48,970 --> 00:19:50,800
And so that's why when we did the proof last

376
00:19:50,800 --> 00:19:52,880
time we had to talk about sort of these different,

377
00:19:52,880 --> 00:19:55,980
what happens if the upper confidence bounds hold?

378
00:19:55,980 --> 00:20:00,130
Um, uh, and there we did sort of a high probability bound.

379
00:20:01,460 --> 00:20:07,580
Okay, so an alternative would be to always select the arm with the highest lower bound.

380
00:20:07,580 --> 00:20:10,960
So what we're doing right now is just selecting the arm with the highest upper bound,

381
00:20:10,960 --> 00:20:13,585
but you could select the arm with the best lower bound.

382
00:20:13,585 --> 00:20:18,070
Um, so what might that look like, let's imagine that we have two arms,

383
00:20:18,070 --> 00:20:22,730
a1 and a2, and this is our estimate of the Q of a.

384
00:20:28,350 --> 00:20:32,095
Okay. So let's imagine that these are uncertainty bounds.

385
00:20:32,095 --> 00:20:35,170
So in this case a1 has a higher upper bound,

386
00:20:35,170 --> 00:20:37,720
but a2 has a higher lower bound.

387
00:20:37,720 --> 00:20:40,990
So why don't we take a minute or two and think about,

388
00:20:40,990 --> 00:20:44,150
why this could lead to linear regret?

389
00:20:46,320 --> 00:20:50,680
I think two-arm case is the easiest one to think about for this.

390
00:20:50,680 --> 00:20:55,130
Feel free to talk to anybody around you, if you wanna brainstorm.

391
00:21:01,710 --> 00:21:06,160
And this is actually an important reason why it's good to be optimistic,

392
00:21:06,160 --> 00:21:08,710
at least in reinforcement learning.

393
00:21:08,710 --> 00:21:43,930
[NOISE]

394
00:21:43,930 --> 00:21:44,440
What do you guys think?

395
00:21:44,440 --> 00:21:57,070
Does it lead to linear regret?

396
00:21:57,070 --> 00:21:57,730
[NOISE] Right, so we're going to get,

397
00:21:57,730 --> 00:21:58,900
sort of like, confirmation bill,

398
00:21:58,900 --> 00:22:01,240
so like, a2 is giving out smaller and smaller [inaudible].

399
00:22:01,240 --> 00:22:05,515
Okay, so I talked to at least one person in the audience that gave the right answer.

400
00:22:05,515 --> 00:22:08,785
Which is, um, in this case if you select a2,

401
00:22:08,785 --> 00:22:12,670
you're gonna continue to get towards the mean.

402
00:22:12,670 --> 00:22:16,615
The real mean is above the lower bound of a1.

403
00:22:16,615 --> 00:22:18,880
And so you're gonna,

404
00:22:18,880 --> 00:22:19,990
kind of, get confirmation bias,

405
00:22:19,990 --> 00:22:22,060
like, a2 is gonna continue to look good,

406
00:22:22,060 --> 00:22:23,755
and you're never gonna select a1.

407
00:22:23,755 --> 00:22:26,620
So we're never gonna get information that allows us to disprove our,

408
00:22:26,620 --> 00:22:30,295
our hypothesis, and we're never gonna learn what the true optimal mean is.

409
00:22:30,295 --> 00:22:32,810
So that's why we can get linear regret.

410
00:22:34,350 --> 00:22:37,525
So one thing I just wanna highlight is that, um,

411
00:22:37,525 --> 00:22:40,510
upper confidence bounds are one nice way to do optimism, er,

412
00:22:40,510 --> 00:22:43,270
and they can change over time in terms of upper bound,

413
00:22:43,270 --> 00:22:45,220
but the simpler thing you might imagine doing is just to

414
00:22:45,220 --> 00:22:47,875
initialize things really to a high value.

415
00:22:47,875 --> 00:22:49,690
Um, so pretend, for example,

416
00:22:49,690 --> 00:22:51,160
that you already observed one pool of each of

417
00:22:51,160 --> 00:22:52,945
the arms and that it was really, really good.

418
00:22:52,945 --> 00:22:56,305
So just initialize all your values at like a million or something like that.

419
00:22:56,305 --> 00:22:58,870
Um, and then you just kind of average in

420
00:22:58,870 --> 00:23:03,385
that weird fake pool when you're doing your empirical average.

421
00:23:03,385 --> 00:23:06,550
So you can imagine you pretend you pull each of your arms once you've got a million,

422
00:23:06,550 --> 00:23:08,230
you've got a trillion, and

423
00:23:08,230 --> 00:23:12,340
then after that you just average in all your empirical rewards.

424
00:23:12,340 --> 00:23:15,430
So this actually can work fairly well in a lot of cases,

425
00:23:15,430 --> 00:23:19,420
the challenge is to figure out how optimistic you need to be for that fake pool.

426
00:23:19,420 --> 00:23:23,140
So just in terms of sort of comparing that approach to other approaches,

427
00:23:23,140 --> 00:23:25,615
recall that greedy gives you linear total regret.

428
00:23:25,615 --> 00:23:28,180
Constant e-greedy can also give you a linear total regret.

429
00:23:28,180 --> 00:23:30,685
If you decay e-greedy,

430
00:23:30,685 --> 00:23:33,535
you can get actually sub-linear regret,

431
00:23:33,535 --> 00:23:36,490
um, if you use the right schedule for decay in epsilon,

432
00:23:36,490 --> 00:23:39,715
but that generally requires knowledge of the gaps which are unknown.

433
00:23:39,715 --> 00:23:44,500
So this is sort of uh,

434
00:23:44,500 --> 00:23:46,015
it's generally impossible to actually achieve.

435
00:23:46,015 --> 00:23:47,380
But if, in principle,

436
00:23:47,380 --> 00:23:49,150
you know, if you sort of, you know,

437
00:23:49,150 --> 00:23:50,515
had an oracle, you could,

438
00:23:50,515 --> 00:23:52,645
uh, you could figure out how to decay things.

439
00:23:52,645 --> 00:23:55,060
If you're too optimistic in your initialization,

440
00:23:55,060 --> 00:23:58,210
um, if you initialize the value sufficiently optimistically,

441
00:23:58,210 --> 00:24:01,060
then you can achieve sub-linear regret, but again,

442
00:24:01,060 --> 00:24:04,645
it can be pretty subtle to figure out exactly how optimistic those need to be.

443
00:24:04,645 --> 00:24:09,220
I, and I'll talk later about an example where in that Markov decision process case they

444
00:24:09,220 --> 00:24:11,380
have to be much more optimistic than you might think they

445
00:24:11,380 --> 00:24:14,660
would need to be in order for this to work well.

446
00:24:15,660 --> 00:24:21,490
All right, so we're gonna now start to talk about Bayesian bandits and Bayesian regret.

447
00:24:21,490 --> 00:24:26,050
And so far we've sort of made very little assumptions about the reward distribution.

448
00:24:26,050 --> 00:24:28,585
We've assumed that the rewards might be bounded,

449
00:24:28,585 --> 00:24:33,745
so we typically assume that the rewards are gonna lie in sort of 0,

450
00:24:33,745 --> 00:24:38,935
0 to 1, or 0 to some, you know,

451
00:24:38,935 --> 00:24:40,390
0 to R max or something like that,

452
00:24:40,390 --> 00:24:44,125
that we have bounded rewards that you can't have infinite rewards as nice as it would be.

453
00:24:44,125 --> 00:24:45,880
Um, [NOISE] but we haven't been making

454
00:24:45,880 --> 00:24:49,180
strong parametric assumptions over the distribution of rewards.

455
00:24:49,180 --> 00:24:51,130
[NOISE] Um, Hoeffding doesn't, uh,

456
00:24:51,130 --> 00:24:54,070
Hoeffding requires the rewards to be bounded but it doesn't assume,

457
00:24:54,070 --> 00:24:57,265
for example, that they're Gaussian or Bernoulli or things like that.

458
00:24:57,265 --> 00:25:01,090
So an alternative approach is to assume that we actually do have information

459
00:25:01,090 --> 00:25:04,660
on the parametric distribution of the rewards, and it'll exploit that.

460
00:25:04,660 --> 00:25:08,125
Um, so we're gonna talk about Bayesian bandits now,

461
00:25:08,125 --> 00:25:12,505
where we sort of explicitly compute a posterior over the rewards given the history.

462
00:25:12,505 --> 00:25:14,080
So given the previous actions,

463
00:25:14,080 --> 00:25:17,710
we, the arms we've pulled and the rewards we've observed.

464
00:25:17,710 --> 00:25:22,910
Uh, uh, and given that posterior we can use that posterior to guide exploration.

465
00:25:23,550 --> 00:25:27,805
And of course, if your prior knowledge is accurate, that might help you.

466
00:25:27,805 --> 00:25:32,110
Um, there's sort of somewhat of a debate between Frequentist and Bayesian,

467
00:25:32,110 --> 00:25:33,895
um, views of the world.

468
00:25:33,895 --> 00:25:36,085
We're not kind of get really too much into that in this class,

469
00:25:36,085 --> 00:25:38,950
but the idea is that it's also gonna be a nice way to put in prior knowledge.

470
00:25:38,950 --> 00:25:41,380
If you have prior knowledge about the particular reward structure of

471
00:25:41,380 --> 00:25:45,850
your environment you can put those in and it can help in terms of exploration.

472
00:25:45,850 --> 00:25:49,390
Okay, so in the Bayesian view now we're

473
00:25:49,390 --> 00:25:52,195
just gonna do sort of a quick review of Bayesian inference.

474
00:25:52,195 --> 00:25:54,940
Um, a number of you guys it's probably gonna be a- a refresher for,

475
00:25:54,940 --> 00:25:56,290
for some people it might be new.

476
00:25:56,290 --> 00:25:59,635
We're gonna assume that we have a prior over the unknown parameters.

477
00:25:59,635 --> 00:26:03,280
So in our case the unknown parameters are gonna be

478
00:26:03,280 --> 00:26:09,010
the parameters that determine the reward probability distribution for each of the arms.

479
00:26:09,010 --> 00:26:12,610
And the idea is that given sort of observations or data about

480
00:26:12,610 --> 00:26:16,135
that parameter like observing rewards when you pull an arm,

481
00:26:16,135 --> 00:26:20,395
we're gonna update our uncertainty over the unknown parameters using Bayes' rule.

482
00:26:20,395 --> 00:26:24,070
So let's look at that as a specific example.

483
00:26:24,070 --> 00:26:27,220
So for example imagine that the reward of an arm i is

484
00:26:27,220 --> 00:26:31,960
a probability distribution that depends on some unknown parameter phi i.

485
00:26:31,960 --> 00:26:34,610
So note that this is unknown.

486
00:26:36,420 --> 00:26:39,880
And we're gonna have some initial prior over phi i,

487
00:26:39,880 --> 00:26:41,815
which is the probability of phi i.

488
00:26:41,815 --> 00:26:43,840
So this was before we pulled that arm at all.

489
00:26:43,840 --> 00:26:46,390
This is sort of our uncertainty over that parameter.

490
00:26:46,390 --> 00:26:50,965
[NOISE] And then we pull arm i and we observe a particular reward, ri1.

491
00:26:50,965 --> 00:26:55,660
And then we can use this to update our estimate of the distribution over

492
00:26:55,660 --> 00:27:00,400
the parameters that determine our reward probability distribution for this arm.

493
00:27:00,400 --> 00:27:03,200
So, we do that using Bayes' rule.

494
00:27:04,110 --> 00:27:08,320
And we say that the posterior probability of our parameters

495
00:27:08,320 --> 00:27:11,650
phi i given that we've observed that reward is equal to

496
00:27:11,650 --> 00:27:19,940
our prior probability over those times provide data evidence or likelihood,

497
00:27:21,390 --> 00:27:24,580
divided by the probability of

498
00:27:24,580 --> 00:27:28,070
observing that reward regardless of what your parameters were.

499
00:27:30,990 --> 00:27:33,550
And so this is Bayes' rule, um,

500
00:27:33,550 --> 00:27:39,190
and the challenge or the important thing here is how do we compute all of those things.

501
00:27:39,190 --> 00:27:42,565
So, that tells us how to update our posterior over the parameters,

502
00:27:42,565 --> 00:27:44,380
and the question is how do we do this?

503
00:27:44,380 --> 00:27:47,365
[NOISE] Okay.

504
00:27:47,365 --> 00:27:52,575
So, in general, doing this sort of updating can be very tricky to do.

505
00:27:52,575 --> 00:27:54,525
Because if you don't have any structure on

506
00:27:54,525 --> 00:27:58,050
the sort of parametric form of the prior and the data likelihood.

507
00:27:58,050 --> 00:28:00,225
So, this again is the prior,

508
00:28:00,225 --> 00:28:03,430
and this is the data likelihood.

509
00:28:06,870 --> 00:28:10,615
If you have no structure on this, um,

510
00:28:10,615 --> 00:28:15,325
one of them is a deep neural network and another one of them is some random other, um,

511
00:28:15,325 --> 00:28:18,280
parametric distribution, then it may be impossible to have

512
00:28:18,280 --> 00:28:22,210
a closed-form representation for what the posterior is.

513
00:28:22,210 --> 00:28:24,580
So in general, this can be really hard.

514
00:28:24,580 --> 00:28:27,190
Um, [NOISE] but it turns out that there's particular forms of the

515
00:28:27,190 --> 00:28:29,830
prior and the data likelihood that mean that we can do this analytically.

516
00:28:29,830 --> 00:28:34,540
[NOISE] So, who here is familiar with conjugates?

517
00:28:34,540 --> 00:28:37,195
Okay, some people but not everybody.

518
00:28:37,195 --> 00:28:39,295
So, these are really cool conjugates.

519
00:28:39,295 --> 00:28:42,850
Um, exponential families, for example, are conjugate distributions.

520
00:28:42,850 --> 00:28:44,890
Um, [NOISE] the idea is that if

521
00:28:44,890 --> 00:28:48,565
the parametric representation of the prior and the posterior is the same,

522
00:28:48,565 --> 00:28:51,055
we call the prior and the model conjugate.

523
00:28:51,055 --> 00:28:52,990
So, what would that mean so, for example,

524
00:28:52,990 --> 00:28:55,210
what if this is like a Gaussian?

525
00:28:55,210 --> 00:28:59,660
If this is a Gaussian and this is a Gaussian,

526
00:29:00,540 --> 00:29:04,795
then we would say that this and this are conjugate.

527
00:29:04,795 --> 00:29:07,970
Whatever thing we're using for the data likelihood.

528
00:29:08,310 --> 00:29:10,720
It essentially means that we can do

529
00:29:10,720 --> 00:29:16,345
this posterior update- updating analytically or in closed form, which is really nice.

530
00:29:16,345 --> 00:29:20,560
So, we call it, this means that we sort of keep things in

531
00:29:20,560 --> 00:29:24,400
the same parametric family as we're getting more evidence about these hidden parameters.

532
00:29:24,400 --> 00:29:26,800
[NOISE] I'll give an example of this in a second.

533
00:29:26,800 --> 00:29:31,990
Um, but there are a number of different parametric families which have conjugate priors.

534
00:29:31,990 --> 00:29:33,295
Which means that, um,

535
00:29:33,295 --> 00:29:37,165
if you have an initial uncertainty over the parameter in that distribution,

536
00:29:37,165 --> 00:29:38,890
then if you observe some data you can

537
00:29:38,890 --> 00:29:41,785
update it and you are still in the same parametric family.

538
00:29:41,785 --> 00:29:44,830
So, they're super elegant, and come up in statistics a lot.

539
00:29:44,830 --> 00:29:46,600
Um, [NOISE] all right. So, here's

540
00:29:46,600 --> 00:29:49,360
a particular example that's relevant to us which is Bernoulli's.

541
00:29:49,360 --> 00:29:51,550
Um, so let's think about a bandit problem

542
00:29:51,550 --> 00:29:54,145
where the reward of an arm is just a binary outcome.

543
00:29:54,145 --> 00:29:59,350
Um, and that this is sampled from a Bernoulli with parameter theta. So this comes up a lot.

544
00:29:59,350 --> 00:30:01,675
This is things like advertisement click-through rates,

545
00:30:01,675 --> 00:30:04,705
patient treatment succeeds or fails et cetera.

546
00:30:04,705 --> 00:30:07,600
So, many, many cases we- when we pull an arm,

547
00:30:07,600 --> 00:30:10,960
or when we take an action we're gonna get a binary reward, either 0 or 1.

548
00:30:10,960 --> 00:30:13,645
[NOISE] So it turns out that, um,

549
00:30:13,645 --> 00:30:19,370
the beta distribution, beta alpha beta is conjugate for the Bernoulli distribution.

550
00:30:19,590 --> 00:30:27,625
So that means we can write down our prior over the Bernoulli parameter,

551
00:30:27,625 --> 00:30:30,610
um, given alpha and beta as follows.

552
00:30:30,610 --> 00:30:33,340
It's theta to the alpha - 1,

553
00:30:33,340 --> 00:30:35,410
1 - theta to the beta - 1,

554
00:30:35,410 --> 00:30:39,170
times a ratio of the gammas.

555
00:30:39,270 --> 00:30:43,675
Gammas are related to the factorial, factorial distribution.

556
00:30:43,675 --> 00:30:46,435
So, all of this can be computed analytically.

557
00:30:46,435 --> 00:30:50,140
And one nice way I like to think about this is that we can think of alpha and beta

558
00:30:50,140 --> 00:30:54,130
as essentially being the result of prior pulls of the arm.

559
00:30:54,130 --> 00:30:58,510
So, we can use them also to encode sort of prior information about this.

560
00:30:58,510 --> 00:31:01,270
And I'll show you shor- shortly an example of how these get updated.

561
00:31:01,270 --> 00:31:07,045
[NOISE] But what happens is that if you assume that the prior over Theta is a Beta.

562
00:31:07,045 --> 00:31:10,700
Um, so, if it looks like this, this is the prior.

563
00:31:10,950 --> 00:31:14,170
Then if you observe our reward that's in 0, 1,

564
00:31:14,170 --> 00:31:17,540
because that's what our reward, distribute rewards are whenever we sample one,

565
00:31:17,540 --> 00:31:20,370
then the updated posterior over theta is a really nice form.

566
00:31:20,370 --> 00:31:22,950
It's just the same beta distribution with

567
00:31:22,950 --> 00:31:26,625
either 1 added to the alpha or 1 added to the beta.

568
00:31:26,625 --> 00:31:37,120
So, essentially if you observe r = 1 then you get beta of alpha + 1 and beta.

569
00:31:37,120 --> 00:31:42,910
If you observe r = 0 you instead add it to the Beta term.

570
00:31:42,910 --> 00:31:48,070
So, you can think of alpha as being all the number of times that you saw a reward of 1,

571
00:31:48,070 --> 00:31:51,155
and beta as all the number of times you saw a reward of 0.

572
00:31:51,155 --> 00:31:55,660
Can you explaining how the fact that theta was Bernoulli factored into

573
00:31:55,660 --> 00:31:58,150
this description and why this isn't just a description

574
00:31:58,150 --> 00:32:00,670
of a beta distribution, the main equation?

575
00:32:00,670 --> 00:32:05,185
[NOISE] Like why- why it is important to first say that theta is Bernoulli?

576
00:32:05,185 --> 00:32:07,240
I'm bringing up that, uh, great question.

577
00:32:07,240 --> 00:32:12,595
So, the reason I bring this up is that beta is a conjugate prior for the Bernoulli.

578
00:32:12,595 --> 00:32:15,340
So, the idea is that in this case if we're thinking

579
00:32:15,340 --> 00:32:19,165
about an arm which has binary outcomes,

580
00:32:19,165 --> 00:32:21,370
then we can think of the average of

581
00:32:21,370 --> 00:32:23,785
that arm as being represented by a Bernoulli parameter.

582
00:32:23,785 --> 00:32:26,335
So, let's say like 0.7%, you know,

583
00:32:26,335 --> 00:32:28,420
on average 0.7 times,

584
00:32:28,420 --> 00:32:30,145
we get a reward of 1,

585
00:32:30,145 --> 00:32:32,545
and 0.3 we get a reward of 0.

586
00:32:32,545 --> 00:32:37,225
So, [NOISE], um, the mean of that arm is 0.7.

587
00:32:37,225 --> 00:32:39,730
Okay. So, we're thinking about an arm

588
00:32:39,730 --> 00:32:42,895
which has a Bernoulli parameter that describes its mean.

589
00:32:42,895 --> 00:32:50,860
So, we're thinking of like you know an arm with mean equal to theta.

590
00:32:50,860 --> 00:32:54,250
So, that's what the mean is for a Bernoulli distribution.

591
00:32:54,250 --> 00:32:58,060
Um, and what I'm saying is that we want to now be Bayesian about that,

592
00:32:58,060 --> 00:33:00,100
and we want to think about what is the probability of

593
00:33:00,100 --> 00:33:04,375
that parameter given the data we've seen so far?

594
00:33:04,375 --> 00:33:09,369
And if we wanna be able to update our estimate over theta,

595
00:33:09,369 --> 00:33:11,590
not over rewards, over theta,

596
00:33:11,590 --> 00:33:17,019
then we're gonna write down our distribution over what theta's might be possible,

597
00:33:17,019 --> 00:33:19,315
um, [NOISE] as a beta distribution.

598
00:33:19,315 --> 00:33:21,775
And we're going to update that as we see evidence.

599
00:33:21,775 --> 00:33:25,600
And I'll show you shortly like what these betas look like, so we can think of so,

600
00:33:25,600 --> 00:33:30,010
what is the probability distribution over thetas as we get more evidence.

601
00:33:30,010 --> 00:33:34,225
So, for example, you might imagine if you see a reward which is 1, 1, 1, 1, 1, 1, 1,

602
00:33:34,225 --> 00:33:36,610
um, then your beta distribution is going to

603
00:33:36,610 --> 00:33:39,865
indicate that a theta which is really high is more likely.

604
00:33:39,865 --> 00:33:45,895
If you get 0, 0, 0, 0, 0, your beta is gonna have a different shifted posterior,

605
00:33:45,895 --> 00:33:50,450
which is gonna say probably your theta's really low, close to 0.

606
00:33:51,090 --> 00:33:54,700
Cool. So, we'll, we'll see an example of this in just a second.

607
00:33:54,700 --> 00:33:57,160
Um, so, the nice thing in this case is that for

608
00:33:57,160 --> 00:34:00,595
Bernoulli's which is a really common distribution that we often want to think about,

609
00:34:00,595 --> 00:34:02,215
we can write down, um,

610
00:34:02,215 --> 00:34:07,030
a prior over that parameter and we can update it analytically just using the counts.

611
00:34:07,030 --> 00:34:08,770
So, we just keep track of how many times we've seen

612
00:34:08,770 --> 00:34:10,735
a 1 or how many times we have a 0,

613
00:34:10,735 --> 00:34:13,190
and we use that to update our posterior.

614
00:34:15,090 --> 00:34:19,764
Okay so how do we evaluate performance now we're in this Bayesian setting?

615
00:34:19,764 --> 00:34:21,759
So in the frequentist regret,

616
00:34:21,760 --> 00:34:24,594
we didn't think about having distributions over parameters.

617
00:34:24,594 --> 00:34:26,874
We just thought of there being some parameter like,

618
00:34:26,875 --> 00:34:28,435
you know what's the mean of that arm.

619
00:34:28,435 --> 00:34:32,935
Um, and then we defined our regret with respect to the best arm.

620
00:34:32,935 --> 00:34:36,429
Bayesian regret assumes there's this prior over parameters.

621
00:34:36,429 --> 00:34:38,124
And so Bayesian regret says,

622
00:34:38,125 --> 00:34:40,420
what is my expected regret by thinking

623
00:34:40,420 --> 00:34:43,195
about what are the possible parameters given my prior.

624
00:34:43,195 --> 00:34:48,219
Um, and then looking at the expected performance if I got a particular theta.

625
00:34:48,219 --> 00:34:50,649
So it's a little bit different way of looking at the world.

626
00:34:50,650 --> 00:34:54,370
Um, again, we're not gonna really get into the philosophical aspects of this.

627
00:34:54,370 --> 00:34:56,844
But Ba- Bayesian regret is saying like well,

628
00:34:56,844 --> 00:34:59,905
we're not sure, you know what the distributions are of these arms.

629
00:34:59,905 --> 00:35:02,380
Um, and there'll be different worlds in which they'll take on

630
00:35:02,380 --> 00:35:04,780
different values and how well do you do in those different worlds

631
00:35:04,780 --> 00:35:09,430
on average. All right.

632
00:35:09,430 --> 00:35:13,450
So how do we try to make good decisions for Bayesian bandits?

633
00:35:13,450 --> 00:35:16,105
So one thing you might imagine is let's say we have

634
00:35:16,105 --> 00:35:19,435
a parametric distribution over the rewards for each of the arms.

635
00:35:19,435 --> 00:35:21,460
Um, we, we could have,

636
00:35:21,460 --> 00:35:23,875
we could certainly have that in the Bayesian case.

637
00:35:23,875 --> 00:35:29,725
The idea of probability matching which I think has been around since around 1929.

638
00:35:29,725 --> 00:35:32,230
Its been around a long time, like almost 100 years.

639
00:35:32,230 --> 00:35:34,660
Um, ah, is that we wanna

640
00:35:34,660 --> 00:35:38,930
select an action a according to the probability that it's optimal.

641
00:35:39,030 --> 00:35:42,130
So it seems quite intuitively appealing like we

642
00:35:42,130 --> 00:35:44,455
want to select arms that might be optimal more.

643
00:35:44,455 --> 00:35:48,640
Um, we want to select arms that probably aren't likely to be optimal less.

644
00:35:48,640 --> 00:35:52,510
And it is optimistic in the face of uncertainty because [NOISE] in

645
00:35:52,510 --> 00:35:56,950
general uncertain actions have a higher probability of being the best.

646
00:35:56,950 --> 00:36:01,090
So uncertain actions mean we don't know very much about what their rewards are.

647
00:36:01,090 --> 00:36:04,780
Um, the problem is this sounds really nice,

648
00:36:04,780 --> 00:36:07,270
we'd like to sort of select arms according to the probability that they're

649
00:36:07,270 --> 00:36:11,035
optimal but it's completely unclear how to compute that.

650
00:36:11,035 --> 00:36:16,600
So this expression here is saying we wanna sample an arm given a history.

651
00:36:16,600 --> 00:36:22,735
So the history here, here is prior pulls and reward outcomes.

652
00:36:22,735 --> 00:36:25,120
So this is the history of the arms we

653
00:36:25,120 --> 00:36:28,330
pulled and whether we've got what sort of rewards we've gotten.

654
00:36:28,330 --> 00:36:31,930
And then we wanna pull an arm according to the probability that

655
00:36:31,930 --> 00:36:36,190
that arm is better than all the other arms given that history.

656
00:36:36,190 --> 00:36:38,890
So that's quite intellectually, um,

657
00:36:38,890 --> 00:36:43,195
appealing but it's not at all clear how we would compute that quantity.

658
00:36:43,195 --> 00:36:46,270
And so it's sort of somewhat magical that, um,

659
00:36:46,270 --> 00:36:50,905
a very simple approach turns out to implement probability matching.

660
00:36:50,905 --> 00:36:53,560
And the idea is called Thompson sampling,

661
00:36:53,560 --> 00:36:54,640
and again this came out, you know,

662
00:36:54,640 --> 00:36:56,335
roughly in the 1920s.

663
00:36:56,335 --> 00:36:58,870
And one of the really interesting aspects of that is it sort

664
00:36:58,870 --> 00:37:02,050
of disappeared for a long time in terms of bandits.

665
00:37:02,050 --> 00:37:04,360
Certainly in the AI community and CS community.

666
00:37:04,360 --> 00:37:06,595
And then around eight years ago,

667
00:37:06,595 --> 00:37:07,705
eight to nine years ago,

668
00:37:07,705 --> 00:37:11,830
people sort of got re-interested in understanding these, um,

669
00:37:11,830 --> 00:37:15,070
in part due to a paper that [NOISE] a colleague of mine published which you'll see

670
00:37:15,070 --> 00:37:19,210
results have shortly which indicated that empirically it can be really good.

671
00:37:19,210 --> 00:37:22,255
Okay. So how does Thompson sampling work?

672
00:37:22,255 --> 00:37:25,195
We're gonna initialize a prior over each of the arms.

673
00:37:25,195 --> 00:37:28,015
Often we'd like this to be conjugate, doesn't have to be.

674
00:37:28,015 --> 00:37:29,395
It's nice if it's conjugate.

675
00:37:29,395 --> 00:37:32,335
But we gonna have a probability over each of the arms.

676
00:37:32,335 --> 00:37:36,730
Um, now remember that this is

677
00:37:36,730 --> 00:37:41,275
sort of a probability over the parameters determining the distribution.

678
00:37:41,275 --> 00:37:45,535
So, um, it could be if we have Bernoulli arms,

679
00:37:45,535 --> 00:37:49,370
it could be the probability of theta.

680
00:37:49,530 --> 00:37:54,370
I for i equals 1 to the number of arms.

681
00:37:54,370 --> 00:37:59,410
So, for example, this could be a beta distribution of 1, 1.

682
00:37:59,410 --> 00:38:03,850
We could say the probability that

683
00:38:03,850 --> 00:38:09,940
my ith arm has a Bernoulli parameter of theta is equal to,

684
00:38:09,940 --> 00:38:12,325
um, uh, sampling from a beta 1, 1.

685
00:38:12,325 --> 00:38:15,175
Okay. So we're gonna pick a particular parametric family

686
00:38:15,175 --> 00:38:18,250
to represent our prior distribution over the,

687
00:38:18,250 --> 00:38:21,130
um, reward distributions for each of the arms.

688
00:38:21,130 --> 00:38:24,295
And then what we do is for each round,

689
00:38:24,295 --> 00:38:28,180
we first sample a rewards distribution from that posterior.

690
00:38:28,180 --> 00:38:31,165
Again we'll go through a concrete example of this in a second.

691
00:38:31,165 --> 00:38:34,360
But this is like picking a particular theta.

692
00:38:34,360 --> 00:38:37,060
So it's like saying I'm assuming that my mean for

693
00:38:37,060 --> 00:38:40,585
this arm e- or my Bernoulli parameter for this arm is 0.7,

694
00:38:40,585 --> 00:38:43,180
picking a particular value for that parameter.

695
00:38:43,180 --> 00:38:45,250
And then once you have that you can compute

696
00:38:45,250 --> 00:38:49,765
the action value function by just taking the mean of whatever that is.

697
00:38:49,765 --> 00:38:53,095
So notice that if theta,

698
00:38:53,095 --> 00:38:55,660
let's say we sample for arm 1,

699
00:38:55,660 --> 00:39:00,100
let's say we sample 0.9.

700
00:39:00,100 --> 00:39:03,820
We just happen to sample that arm 1 has,

701
00:39:03,820 --> 00:39:05,785
um, uh, a theta parameter 0.9.

702
00:39:05,785 --> 00:39:10,840
Well, the Q for arm 1 is then

703
00:39:10,840 --> 00:39:16,450
gonna be the expected value of a Bernoulli parameter theta one which is just 0.9.

704
00:39:16,450 --> 00:39:22,975
Because the expected value for a Bernoulli variable is just p,

705
00:39:22,975 --> 00:39:24,895
just the, just the theta.

706
00:39:24,895 --> 00:39:28,225
So we're gonna compute the action value function for each of these.

707
00:39:28,225 --> 00:39:30,160
Um, in the case of a Gaussian,

708
00:39:30,160 --> 00:39:31,990
it would just be its mean, for example.

709
00:39:31,990 --> 00:39:36,280
So you just compute what the mean expected reward is for each of the arms under

710
00:39:36,280 --> 00:39:39,220
the particular reward distribution we sampled and then you take

711
00:39:39,220 --> 00:39:43,175
whichever action looks best for those sampled parameters.

712
00:39:43,175 --> 00:39:46,960
So that's this. And then we take

713
00:39:46,960 --> 00:39:51,970
that action and we observe a reward and then we update our posterior using Bayes' Law.

714
00:39:51,970 --> 00:39:55,780
So we're just gonna take our priors over the parameters.

715
00:39:55,780 --> 00:39:58,105
We're gonna sample a particular set of parameters.

716
00:39:58,105 --> 00:39:59,740
These are probably totally wrong.

717
00:39:59,740 --> 00:40:01,915
This is just us making up what the actual,

718
00:40:01,915 --> 00:40:04,045
you know, parameters are for each of the arms.

719
00:40:04,045 --> 00:40:06,280
Then we act as if that world is optimal,

720
00:40:06,280 --> 00:40:08,780
we get some data, and we repeat.

721
00:40:10,890 --> 00:40:14,095
So it's a, it's a fairly simple thing to do.

722
00:40:14,095 --> 00:40:16,540
Um, we of course have to see how we can do this sampling.

723
00:40:16,540 --> 00:40:19,375
Um, and I'll show you an example with Bernoulli's in a second.

724
00:40:19,375 --> 00:40:23,230
Um, but nowhere here are we trying to explicitly

725
00:40:23,230 --> 00:40:28,015
compute like what is the posterior probability that this arm is optimal.

726
00:40:28,015 --> 00:40:29,935
We're just sampling some and then we're going to

727
00:40:29,935 --> 00:40:33,290
be sort of greedy with respect to those samples.

728
00:40:33,510 --> 00:40:37,270
Okay. So Thompson sampling

729
00:40:37,270 --> 00:40:40,360
turns out to implement probability matching which is super cool.

730
00:40:40,360 --> 00:40:42,280
Um, and for some of the intuition of this,

731
00:40:42,280 --> 00:40:44,905
so this is what probability matching is.

732
00:40:44,905 --> 00:40:49,480
Probability matching is that we wanna select an action given the,

733
00:40:49,480 --> 00:40:53,320
um, history according to the probability that it's optimal.

734
00:40:53,320 --> 00:40:56,140
According to the probability that arm really is the best arm.

735
00:40:56,140 --> 00:40:59,620
And we can think of that as being equivalent to the expected value

736
00:40:59,620 --> 00:41:02,770
of picking a reward given the H and that,

737
00:41:02,770 --> 00:41:07,420
ah, the arm is equal to the arg max of QA given that history.

738
00:41:07,420 --> 00:41:10,640
And that's what Thompson sampling is computing.

739
00:41:12,060 --> 00:41:15,760
Okay, so let's see how this actually looks for the broken toe example,

740
00:41:15,760 --> 00:41:17,515
because I think that'll make it a lot more concrete.

741
00:41:17,515 --> 00:41:19,030
So again, in this case,

742
00:41:19,030 --> 00:41:20,710
remember that we have, um,

743
00:41:20,710 --> 00:41:26,050
three different arms and they each are looking at the success or failure of,

744
00:41:26,050 --> 00:41:29,110
um, doing this treatment to try to make people's broken toes better.

745
00:41:29,110 --> 00:41:32,965
Um, and surgery is a Bernoulli parameter with 0.95.

746
00:41:32,965 --> 00:41:35,110
Taping is, uh, is,

747
00:41:35,110 --> 00:41:37,945
uh, 0.9 and nothing is 0.1.

748
00:41:37,945 --> 00:41:41,365
So if we wanted to then run Thompson sampling in this environment,

749
00:41:41,365 --> 00:41:44,065
remember it doesn't know what those actual parameters are.

750
00:41:44,065 --> 00:41:45,910
We're gonna choose a beta 1, 1

751
00:41:45,910 --> 00:41:48,235
prior over the parameters.

752
00:41:48,235 --> 00:41:54,320
So that means that we're gonna say the probability of theta 1 is equal to a beta.

753
00:41:55,000 --> 00:41:58,565
Okay. So what does a beta 1, 1 look like?

754
00:41:58,565 --> 00:42:00,740
It looks like a uniform distribution.

755
00:42:00,740 --> 00:42:05,405
So this is 0 to 1. This is theta.

756
00:42:05,405 --> 00:42:07,920
This is probability of theta.

757
00:42:07,930 --> 00:42:12,799
So what this says is that if someone gives you a beta 1, 1 distribution,

758
00:42:12,799 --> 00:42:16,265
it says you're going to select a Bernoulli parameter from that.

759
00:42:16,265 --> 00:42:17,960
I have no idea what its value is.

760
00:42:17,960 --> 00:42:20,510
Could be 0, it could be 1, it could be 0.5.

761
00:42:20,510 --> 00:42:23,000
It- it's sort of an uninformative prior.

762
00:42:23,000 --> 00:42:26,795
Okay. So it says that initially I have no idea what,

763
00:42:26,795 --> 00:42:29,915
um, these theta parameters might be for each of the arms.

764
00:42:29,915 --> 00:42:33,860
Um, so I'm just gonna pretend it's- I'm gonna start off and assume it's flat.

765
00:42:33,860 --> 00:42:35,420
I have no information.

766
00:42:35,420 --> 00:42:39,035
Okay. So this is what,

767
00:42:39,035 --> 00:42:41,150
um, a beta 1, 1 looks like.

768
00:42:41,150 --> 00:42:43,850
But what's gonna happen in Thompson sampling?

769
00:42:43,850 --> 00:42:47,510
So this is our distribution over thetas.

770
00:42:47,510 --> 00:42:49,370
And what Thompson sampling is gonna do is it's gonna

771
00:42:49,370 --> 00:42:52,280
sample a parameter from that distribution.

772
00:42:52,280 --> 00:42:56,270
So in this case it's just a uniform distribution between 0 and 1.

773
00:42:56,270 --> 00:43:00,630
So we're just gonna select some value between 0 and 1.

774
00:43:01,690 --> 00:43:05,315
So in this case imagine what we got is,

775
00:43:05,315 --> 00:43:12,470
I'll just leave this up for a second so people can see 0.3, 0.5. and 0.6.

776
00:43:12,470 --> 00:43:15,020
There's no reason that arm three would be

777
00:43:15,020 --> 00:43:18,080
higher or lower than arm one or arm two or arm three.

778
00:43:18,080 --> 00:43:19,640
In reality arm one is best,

779
00:43:19,640 --> 00:43:21,845
but we have no information about that so far.

780
00:43:21,845 --> 00:43:23,660
We have no rewards so far.

781
00:43:23,660 --> 00:43:25,535
All we've done is we've just said,

782
00:43:25,535 --> 00:43:27,920
I have a uniform distribution over what

783
00:43:27,920 --> 00:43:30,995
my theta parameter might be, I'm gonna sample from it.

784
00:43:30,995 --> 00:43:35,150
And so in this case, it's like sampling and you've got this value once,

785
00:43:35,150 --> 00:43:36,995
you got this value once,

786
00:43:36,995 --> 00:43:39,240
and you got this value once.

787
00:43:40,330 --> 00:43:42,950
And we just sampled from that distribute-

788
00:43:42,950 --> 00:43:45,950
that uniform distribution and these are the parameters we pegged.

789
00:43:45,950 --> 00:43:47,810
And now we're going to pretend that's real.

790
00:43:47,810 --> 00:43:52,865
So we're gonna say I'm gonna pretend that my theta for surgery is 0.3.

791
00:43:52,865 --> 00:43:57,600
My theta for taping is 0.5 and my theta for nothing is 0.6.

792
00:43:58,420 --> 00:44:01,835
So if that was the real world we lived in,

793
00:44:01,835 --> 00:44:04,230
what arm would we select?

794
00:44:05,350 --> 00:44:06,770
Third arm.

795
00:44:06,770 --> 00:44:09,650
Third arm. Exactly. So in this world,

796
00:44:09,650 --> 00:44:13,490
the third arm really is best because that's a theta of 0.6.

797
00:44:13,490 --> 00:44:18,080
So we're going to select theta as 0.6. Yeah.

798
00:44:18,240 --> 00:44:26,080
Using uninformative priors because I could have many values of beta 2, 2 or 5, 5.

799
00:44:26,080 --> 00:44:32,180
So is that a significant advantage of using this for Thompson or something?

800
00:44:32,180 --> 00:44:35,960
Yeah. It makes a really good point.

801
00:44:35,960 --> 00:44:38,630
She said we currently use an uninformative prior,

802
00:44:38,630 --> 00:44:41,660
is it better or worse to do like that compared to using an uninformative prior.

803
00:44:41,660 --> 00:44:44,765
So you could have had a beta of like 3, 4 et cetera.

804
00:44:44,765 --> 00:44:49,220
Um, a beta of 3, 4 or anything that's not 1, 1 is gonna give you,

805
00:44:49,220 --> 00:44:51,350
um, is gonna bias your distribution,

806
00:44:51,350 --> 00:44:54,380
it's gonna change the shape of how you sample things.

807
00:44:54,380 --> 00:44:57,590
If you have actually good information that can be really useful,

808
00:44:57,590 --> 00:45:00,095
um, because it's essentially like having fake pools.

809
00:45:00,095 --> 00:45:03,710
Um, and it can- it can guide sort of your initial samples.

810
00:45:03,710 --> 00:45:06,365
The downside is that if that isn't correct,

811
00:45:06,365 --> 00:45:08,015
you can be misled for a while.

812
00:45:08,015 --> 00:45:10,820
So we often talk about like how robust are we

813
00:45:10,820 --> 00:45:13,460
to misspecified priors or to wrong priors.

814
00:45:13,460 --> 00:45:16,175
Um, and so using the uninformative prior means that,

815
00:45:16,175 --> 00:45:18,470
ah, you're not getting a lot of benefit

816
00:45:18,470 --> 00:45:21,810
from prior knowledge but you're also not gonna get a disadvantage.

817
00:45:22,900 --> 00:45:27,155
Okay. So in this case we're gonna select the arm- arm three

818
00:45:27,155 --> 00:45:31,100
because that's just the arm that has the best expected mean,

819
00:45:31,100 --> 00:45:33,020
um, under the samples that we did.

820
00:45:33,020 --> 00:45:35,030
Okay, but arm three is actually not very good.

821
00:45:35,030 --> 00:45:39,020
And we know that because arm three actually only has a reward of 0.1.

822
00:45:39,020 --> 00:45:41,390
And so when we sample it and we get the patient's outcome,

823
00:45:41,390 --> 00:45:43,895
we're gonna get a 0 in this particular case.

824
00:45:43,895 --> 00:45:47,465
Because the real arm three is 0.1.

825
00:45:47,465 --> 00:45:50,270
So if we sample from a Bernoulli with 0.1,

826
00:45:50,270 --> 00:45:52,310
most of the time we're going to get a 0.

827
00:45:52,310 --> 00:45:57,650
So now we have to do is we have to update our posterior over arm three.

828
00:45:57,650 --> 00:46:01,880
Okay, we have to update what sort of what are our probability of

829
00:46:01,880 --> 00:46:06,750
theta of arm three is, given that the reward was equal to 0.

830
00:46:08,230 --> 00:46:11,030
Okay. [NOISE] So what we talked about is that the beta

831
00:46:11,030 --> 00:46:13,370
is a conjugate prior for the Bernoulli.

832
00:46:13,370 --> 00:46:15,095
And if we observe a 1,

833
00:46:15,095 --> 00:46:16,430
we're going to update the first parameter,

834
00:46:16,430 --> 00:46:18,275
also we're gonna update the second parameter,

835
00:46:18,275 --> 00:46:19,910
so we just saw a 0.

836
00:46:19,910 --> 00:46:22,535
So our new beta is 1, 2.

837
00:46:22,535 --> 00:46:25,280
Because we just saw a 0 and so we update.

838
00:46:25,280 --> 00:46:26,600
So this is our new parameter.

839
00:46:26,600 --> 00:46:34,715
That's our new posterior over arm- the arm three and it looks like this.

840
00:46:34,715 --> 00:46:37,385
Okay. So this is still theta,

841
00:46:37,385 --> 00:46:41,615
always has to be between 0 and 1 because this is a Bernoulli parameter.

842
00:46:41,615 --> 00:46:45,095
And this is what the probability looks like now.

843
00:46:45,095 --> 00:46:47,464
So notice it shifted,

844
00:46:47,464 --> 00:46:49,010
so it used to be flat,

845
00:46:49,010 --> 00:46:50,270
and now it says well no,

846
00:46:50,270 --> 00:46:53,285
I just observed that we've got a reward of 0.

847
00:46:53,285 --> 00:46:56,810
So now I have a higher probability that theta is small.

848
00:46:56,810 --> 00:46:58,670
So if I was going to sample from this,

849
00:46:58,670 --> 00:47:03,860
it is more likely I would get a lower value compared to a higher value unlike before.

850
00:47:03,860 --> 00:47:07,310
Okay. So this is our new posterior.

851
00:47:07,310 --> 00:47:10,130
And what does the posterior look like for the other arm?

852
00:47:10,130 --> 00:47:11,780
So this is for, um,

853
00:47:11,780 --> 00:47:13,820
this is for theta three.

854
00:47:13,820 --> 00:47:16,980
And for the other two,

855
00:47:17,530 --> 00:47:21,320
they still look uniform because they're still a beta 1, 1.

856
00:47:21,320 --> 00:47:24,060
So for beta 1, 1.

857
00:47:25,810 --> 00:47:28,115
This is for the other arms,

858
00:47:28,115 --> 00:47:29,600
theta 1, theta 2,

859
00:47:29,600 --> 00:47:31,445
and this is probability of theta.

860
00:47:31,445 --> 00:47:34,730
The other two are still uniform because we- we haven't pulled them yet.

861
00:47:34,730 --> 00:47:35,780
We don't have any outcomes.

862
00:47:35,780 --> 00:47:37,745
So they still look like uniform distributions.

863
00:47:37,745 --> 00:47:41,375
But the probability over theta 3 looks skewed towards 0.

864
00:47:41,375 --> 00:47:43,070
So now in Thompson sampling,

865
00:47:43,070 --> 00:47:46,625
we again are just going to sample a value from each of these different ones.

866
00:47:46,625 --> 00:47:48,845
Each of those three distributions.

867
00:47:48,845 --> 00:47:52,860
And now imagine we get 0.7, 0.5 and 0.3, yeah?

868
00:47:54,070 --> 00:47:57,965
Turn back a slide, should that say p of Q a3, not Q of a1 because didn't you say a3-

869
00:47:57,965 --> 00:48:05,550
Thank you. Yeah, hold on, there's a couple of errors there, yeah.

870
00:48:09,370 --> 00:48:13,200
Thanks for catching that. Any other questions?

871
00:48:13,510 --> 00:48:17,285
Okay. So we updated our posterior over arm three.

872
00:48:17,285 --> 00:48:19,580
Our posterior over arm one and arm

873
00:48:19,580 --> 00:48:22,655
two is the same as the prior because we didn't- we didn't pull them.

874
00:48:22,655 --> 00:48:26,930
Okay. So now we're gonna sample from those three distributions.

875
00:48:26,930 --> 00:48:30,380
What Thompson sampling would say is now given our posterior over all of the arms,

876
00:48:30,380 --> 00:48:33,125
let's select an actual parameter for each of the arm.

877
00:48:33,125 --> 00:48:36,560
And this time we're going to get 0.7, 0.5 and 0.3.

878
00:48:36,560 --> 00:48:38,750
So which arm where are we going to select this time?

879
00:48:38,750 --> 00:48:42,470
Arm one.

880
00:48:42,470 --> 00:48:47,525
Arm one. Right? So now the max is gonna be arm one.

881
00:48:47,525 --> 00:48:51,890
Okay. So now we're gonna

882
00:48:51,890 --> 00:48:56,915
have a posterior that looks like beta 2,1 because we update our pe- beta.

883
00:48:56,915 --> 00:49:01,820
And remember we can just think of this as being the number of R equals

884
00:49:01,820 --> 00:49:05,630
1s + 1 because we started with a beta

885
00:49:05,630 --> 00:49:10,820
1, 1 and this is the number of r = 0s plus 1.

886
00:49:10,820 --> 00:49:14,870
So now our new posterior for this one makes it look like this.

887
00:49:14,870 --> 00:49:16,640
So this is theta 1,

888
00:49:16,640 --> 00:49:20,330
this is 1, 0 probability of theta 1.

889
00:49:20,330 --> 00:49:22,730
Okay. So now as we would expect,

890
00:49:22,730 --> 00:49:25,160
we saw that arm- arm one had a good outcome

891
00:49:25,160 --> 00:49:27,590
and so now our probability that that Bernoulli parameter

892
00:49:27,590 --> 00:49:33,020
is higher than 0.5 is going up because we saw some positive results.

893
00:49:33,020 --> 00:49:37,820
Okay. So now we have a- so what does our new distributions look like?

894
00:49:37,820 --> 00:49:40,670
We have a beta 2, 1,

895
00:49:40,670 --> 00:49:44,000
we have a beta 1, 1 because we haven't selected arm two yet,

896
00:49:44,000 --> 00:49:47,460
and then we have a beta 1, 2.

897
00:49:47,710 --> 00:49:50,330
All right. So what's going to happen next,

898
00:49:50,330 --> 00:49:53,960
um, let me again are gonna sample a Bernoulli parameter.

899
00:49:53,960 --> 00:49:58,020
So let's imagine that we got 0.71, 0.65 and 0.1.

900
00:49:58,750 --> 00:50:02,375
And so that means we're again gonna select arm one.

901
00:50:02,375 --> 00:50:05,240
And we again observe a one,

902
00:50:05,240 --> 00:50:06,890
surgery is pretty effective.

903
00:50:06,890 --> 00:50:09,440
And now our posterior is 3, 1.

904
00:50:09,440 --> 00:50:12,200
So now this again is 0 to 1.

905
00:50:12,200 --> 00:50:15,395
This is our probability of theta 1.

906
00:50:15,395 --> 00:50:18,485
Okay. So now it's looking even more peaked.

907
00:50:18,485 --> 00:50:23,690
So what's your guess of what's the next arm we're likely to sample?

908
00:50:23,690 --> 00:50:29,060
So remember the three distributions that we have right now is for arm two,

909
00:50:29,060 --> 00:50:33,905
looks like this, for arm three, looks like this.

910
00:50:33,905 --> 00:50:37,290
So this is the probability of that arm.

911
00:50:39,940 --> 00:50:45,680
Since theta a2, theta a3, 0, 1, 0, 1.

912
00:50:45,680 --> 00:50:48,665
So who thinks that, um,

913
00:50:48,665 --> 00:50:52,130
theta 1 is again gonna be sampled and look better than everything else?

914
00:50:52,130 --> 00:50:55,700
That's right because it's going to have- has a posterior over

915
00:50:55,700 --> 00:51:00,305
its Bernoulli parameter that is getting closer and more and more steep towards 1.

916
00:51:00,305 --> 00:51:03,710
Theta 2 we still never- we've still never taken action a2,

917
00:51:03,710 --> 00:51:05,615
but it just has a uniform probability.

918
00:51:05,615 --> 00:51:08,540
So it's very unlikely that we're gonna sample a value

919
00:51:08,540 --> 00:51:11,870
for it that is better than the value we sampled for arm one.

920
00:51:11,870 --> 00:51:13,610
So again in this case,

921
00:51:13,610 --> 00:51:15,005
we can imagine sampling again,

922
00:51:15,005 --> 00:51:17,525
we get 0.75, 0.45, 0.4.

923
00:51:17,525 --> 00:51:23,160
We select action a1 again and now we have a beta 4,1 and it's looking even more sharp up.

924
00:51:24,300 --> 00:51:28,330
So notice this is quite different than what UCB was doing.

925
00:51:28,330 --> 00:51:33,040
UCB was splitting its time between a1 and 2 at the beginning because,

926
00:51:33,040 --> 00:51:36,205
um, they were both reliant on their empirical means,

927
00:51:36,205 --> 00:51:38,560
um, but then a2 had been taken less times.

928
00:51:38,560 --> 00:51:41,845
In this case, we still haven't taken action a2 yet.

929
00:51:41,845 --> 00:51:44,710
And it may be hard for us to pull it for a while.

930
00:51:44,710 --> 00:51:48,820
Now, that's not actually bad in this case because theta 1 is actually the best arm.

931
00:51:48,820 --> 00:51:51,700
But there can be sor- some trade-offs. Yes.

932
00:51:51,700 --> 00:51:56,350
Is this the only way we can update the Beta distributions?

933
00:51:56,350 --> 00:51:59,290
Uh, is, is there a rule that we should increment it

934
00:51:59,290 --> 00:52:02,605
it by one or [NOISE] of course we have different kinds of rewards.

935
00:52:02,605 --> 00:52:04,345
Here rewards are 0 and 1, right?

936
00:52:04,345 --> 00:52:08,560
So do you have some kinds of rewards probably of beta, beta distribution.

937
00:52:08,560 --> 00:52:10,360
Great question. Category is like, okay,

938
00:52:10,360 --> 00:52:12,625
so here we've got, um, binary rewards.

939
00:52:12,625 --> 00:52:15,145
How would we do this if the things were not binary?

940
00:52:15,145 --> 00:52:17,305
In that case we wouldn't use a beta in a Bernoulli.

941
00:52:17,305 --> 00:52:18,880
So if you didn't have, uh,

942
00:52:18,880 --> 00:52:22,120
for binary rewards, Bernoulli's a really nice choice and betas conjugate.

943
00:52:22,120 --> 00:52:24,925
If you have real-valued rewards you might use a Gaussian.

944
00:52:24,925 --> 00:52:26,680
Um, and then you'd have a, a,

945
00:52:26,680 --> 00:52:29,695
a sort of Gaussian prior depending on whether you know your theta or not.

946
00:52:29,695 --> 00:52:33,580
And in general, uh, like for multinomials you can use Dirichlet distributions.

947
00:52:33,580 --> 00:52:35,920
Depends on what your reward distribution looks

948
00:52:35,920 --> 00:52:38,650
like and then you wanna find a conjugate prior for that distribution.

949
00:52:38,650 --> 00:52:41,860
So there's a lot of different families of parametric distributions

950
00:52:41,860 --> 00:52:45,130
for which you can do this sort of updating. Yeah.

951
00:52:45,130 --> 00:52:47,155
Then the other things we're talking about.

952
00:52:47,155 --> 00:52:48,490
Remind me your name.

953
00:52:48,490 --> 00:52:50,560
About being optimistic.

954
00:52:50,560 --> 00:52:50,950
Yes.

955
00:52:50,950 --> 00:52:54,085
And here is like a uniform distribution for initialization.

956
00:52:54,085 --> 00:52:57,010
Is it better to use something more optimistic?

957
00:52:57,010 --> 00:52:59,800
That is a great question. His question was, uh,

958
00:52:59,800 --> 00:53:02,380
so we talked before about the benefits of optimism.

959
00:53:02,380 --> 00:53:04,270
Here we just used a uniform prior.

960
00:53:04,270 --> 00:53:07,825
Um, and wouldn't be better to use one that is optimistic.

961
00:53:07,825 --> 00:53:09,880
It depends, um, I,

962
00:53:09,880 --> 00:53:13,225
the empirically what, so what is this doing?

963
00:53:13,225 --> 00:53:15,220
I- I'm just gonna, let me hold on that question for a second.

964
00:53:15,220 --> 00:53:16,975
So we can look at sort of what these look like.

965
00:53:16,975 --> 00:53:19,510
So if we did optimism,

966
00:53:19,510 --> 00:53:21,820
we sampled all the actions first and then we sort of

967
00:53:21,820 --> 00:53:24,490
got this interleaving of a1 or, and a2.

968
00:53:24,490 --> 00:53:28,330
In Thompson sampling, we took a3 and we took a1,

969
00:53:28,330 --> 00:53:32,260
a1, a1, a1, and a1, a1, a1, a1.

970
00:53:32,260 --> 00:53:34,060
a1 is optimal in this case.

971
00:53:34,060 --> 00:53:37,420
So by using a uniform prior here, essentially, um,

972
00:53:37,420 --> 00:53:41,005
as soon as you see something that looks pretty good like better than 0.5,

973
00:53:41,005 --> 00:53:43,600
um, you're gonna tend to often sample it a lot more.

974
00:53:43,600 --> 00:53:46,715
Um, so you're sort of exploiting faster to some extent.

975
00:53:46,715 --> 00:53:50,245
Um, you can put priors in there.

976
00:53:50,245 --> 00:53:51,280
The question is often like,

977
00:53:51,280 --> 00:53:53,950
how much to put that in there and if it actually helps.

978
00:53:53,950 --> 00:53:56,020
So one of the cool things with Thompson sampling is it

979
00:53:56,020 --> 00:53:58,180
turns out in terms of sort of the Bayesian regret bounds,

980
00:53:58,180 --> 00:54:00,850
[NOISE] they're as good as the, as the upper confidence bounds,

981
00:54:00,850 --> 00:54:03,985
but empirically often exploring faster is helpful.

982
00:54:03,985 --> 00:54:06,190
So you could put optimism in there,

983
00:54:06,190 --> 00:54:09,100
but it might actually hurt performance because it's gonna force you to take,

984
00:54:09,100 --> 00:54:11,605
like in this case, r1 actually is optimal.

985
00:54:11,605 --> 00:54:15,790
Um, now, you could have imagined maybe we were just lucky there and instead we got an a2.

986
00:54:15,790 --> 00:54:19,180
In that case, you'd want something to help you eventually take a1.

987
00:54:19,180 --> 00:54:24,325
Now note, we, we will still take a2 likely at some point

988
00:54:24,325 --> 00:54:26,920
because there still will be a probability under that uniform

989
00:54:26,920 --> 00:54:30,160
prior that you'll sample like 0.999 and then you'll take a2.

990
00:54:30,160 --> 00:54:36,430
So you can still be sure to start taking other actions even using these uniform priors.

991
00:54:36,430 --> 00:54:38,905
But it's a really good question about sort of,

992
00:54:38,905 --> 00:54:41,530
you know, weird, what information to put in there. Yeah.

993
00:54:41,530 --> 00:54:42,440
Um.

994
00:54:42,440 --> 00:54:48,550
Remind me of your name.

995
00:54:48,550 --> 00:54:52,540
[inaudible].it's stuck, it becomes very hard for a different action to catch up.

996
00:54:52,540 --> 00:54:59,260
So the, uh, so in that sense that chance that is suck is more that important? And then [inaudible].

997
00:54:59,260 --> 00:55:00,850
That is a really good one which is, okay,

998
00:55:00,850 --> 00:55:02,680
so maybe if one arm is really good, then, um,

999
00:55:02,680 --> 00:55:04,915
it becomes really hard for the other arms to, to catch up.

1000
00:55:04,915 --> 00:55:06,100
So in this case,

1001
00:55:06,100 --> 00:55:08,095
that's true because theta 1,

1002
00:55:08,095 --> 00:55:09,895
um, really does have 0.95.

1003
00:55:09,895 --> 00:55:11,800
Let's imagine a slightly [NOISE] different case where this was

1004
00:55:11,800 --> 00:55:13,975
like 0.7 or something like that.

1005
00:55:13,975 --> 00:55:16,495
Then in that case over time it would be likely that

1006
00:55:16,495 --> 00:55:18,520
your Beta distribution is going to

1007
00:55:18,520 --> 00:55:21,550
converge to around the real distribution of the parameter.

1008
00:55:21,550 --> 00:55:25,360
So if you keep sampling theta 1 forever, uh, eventually,

1009
00:55:25,360 --> 00:55:29,695
[NOISE] you know, it's gonna sort of collapse towards what the true value is.

1010
00:55:29,695 --> 00:55:33,070
Um, and so if the true value isn't very close to one,

1011
00:55:33,070 --> 00:55:34,840
there will be some probability you'll sample.

1012
00:55:34,840 --> 00:55:38,320
Like so imagine this versus a uniform.

1013
00:55:38,320 --> 00:55:40,060
That's not very close to 1.

1014
00:55:40,060 --> 00:55:43,150
At some point, there is a non-zero probability that you'd sample something that's higher.

1015
00:55:43,150 --> 00:55:46,720
[NOISE] So the beginning matters,

1016
00:55:46,720 --> 00:55:49,255
um, but you can outweigh it over time.

1017
00:55:49,255 --> 00:55:53,000
Just like what we can with the empirical distributions.

1018
00:55:53,460 --> 00:55:56,200
Okay. So if we look at this and we sort of look

1019
00:55:56,200 --> 00:55:58,120
at the incurred frequentist regret [NOISE]

1020
00:55:58,120 --> 00:55:59,920
which is not the same as Bayesian regret because in

1021
00:55:59,920 --> 00:56:02,380
that case we'd have to average over the parameters.

1022
00:56:02,380 --> 00:56:05,425
Um, in this case Thompson sampling is doing, uh, a lot better.

1023
00:56:05,425 --> 00:56:07,810
So [NOISE] in this case, here,

1024
00:56:07,810 --> 00:56:11,410
this would be 0.85 and that'll be 0, 0, 0, 0.

1025
00:56:11,410 --> 00:56:13,718
[NOISE] Okay. So in this case,

1026
00:56:13,718 --> 00:56:15,490
Thompson sampling would be doing a much better job.

1027
00:56:15,490 --> 00:56:21,205
[NOISE] Now, um, Thompson sampling,

1028
00:56:21,205 --> 00:56:24,190
uh, actually does achieve the Lai and Robbins lower bound

1029
00:56:24,190 --> 00:56:27,715
for the performance of a- an algorithm.

1030
00:56:27,715 --> 00:56:31,330
So, um, in terms of its lower bound is similar.

1031
00:56:31,330 --> 00:56:34,135
So that's one indication this might be a good algorithm.

1032
00:56:34,135 --> 00:56:36,715
But we have, there's a lot of bounds for optimism.

1033
00:56:36,715 --> 00:56:38,725
In general, um, the,

1034
00:56:38,725 --> 00:56:41,500
the bounds for optimism are better than the bounds for Thompson sampling.

1035
00:56:41,500 --> 00:56:43,975
A lot of the Thompson sampling, uh,

1036
00:56:43,975 --> 00:56:47,650
bounds end up converting to sort of upper confidence bounds.

1037
00:56:47,650 --> 00:56:48,940
A little bit like what  was asking.

1038
00:56:48,940 --> 00:56:50,765
So if we, um,

1039
00:56:50,765 --> 00:56:55,065
if you want to make Thompson sampling have frequentist-like bounds, um,

1040
00:56:55,065 --> 00:56:58,050
often we end up sort of making our comfort, our sort

1041
00:56:58,050 --> 00:57:01,925
of being more optimistic in terms of Thompson sampling.

1042
00:57:01,925 --> 00:57:06,655
Okay, to put those. Um, but empirically Thompson sampling is often great. Yeah.

1043
00:57:06,655 --> 00:57:07,960
Can you mix them?

1044
00:57:07,960 --> 00:57:10,870
Can you mix Thompson sampling and upper confidence bounds?

1045
00:57:10,870 --> 00:57:13,840
Uh, maybe start with some form of upper confidence bound and

1046
00:57:13,840 --> 00:57:16,930
use that information to update like your priors on the Thompson sampling?

1047
00:57:16,930 --> 00:57:20,560
[NOISE] Um, I, I shouldn't just say,

1048
00:57:20,560 --> 00:57:21,820
can you, could you mix them?

1049
00:57:21,820 --> 00:57:23,050
Y- You probably could.

1050
00:57:23,050 --> 00:57:25,315
I, you could probably do it. I don't know.

1051
00:57:25,315 --> 00:57:26,965
So I guess to me one of the,

1052
00:57:26,965 --> 00:57:29,740
so maybe you could start with upper confidence bounds and then use Thompson sampling.

1053
00:57:29,740 --> 00:57:32,230
Um, to me one of the big benefits of Thompson sampling

1054
00:57:32,230 --> 00:57:35,620
empirically is that it is less optimistic than upper confidence bounds.

1055
00:57:35,620 --> 00:57:38,365
Upper confidence bounds tend to be too optimistic for too long.

1056
00:57:38,365 --> 00:57:41,050
And it's like, "Oh, you know, I ran into the door 30,000 times,

1057
00:57:41,050 --> 00:57:42,985
but maybe that 30,001th time I won't."

1058
00:57:42,985 --> 00:57:45,190
You know, like for your robot or something like that.

1059
00:57:45,190 --> 00:57:49,225
So, um, it often tends to sort of think about the extreme events.

1060
00:57:49,225 --> 00:57:53,110
Whereas, if you know that say the world is really Gaussian like the probability that

1061
00:57:53,110 --> 00:57:55,180
your robot is going to run into a wall again is still really

1062
00:57:55,180 --> 00:57:57,580
high even if it's only run into the wall 10 times.

1063
00:57:57,580 --> 00:57:59,305
So maybe you should pick a different path.

1064
00:57:59,305 --> 00:58:05,200
So I think often you probably would want to start with Thompson sampling,

1065
00:58:05,200 --> 00:58:07,615
but you would like to be robust to your prior.

1066
00:58:07,615 --> 00:58:11,425
And so some work that tries to combine these ideas is known as PAC-Bayesian,

1067
00:58:11,425 --> 00:58:12,985
where you try to get,

1068
00:58:12,985 --> 00:58:14,470
I'll define what PAC is in a second.

1069
00:58:14,470 --> 00:58:17,140
But, um, you'll try to get bounds that are kinda frequentist-like,

1070
00:58:17,140 --> 00:58:18,850
but also get the best of both worlds.

1071
00:58:18,850 --> 00:58:22,030
So you'd like to be like Bayesian if your prior is really good and, um,

1072
00:58:22,030 --> 00:58:26,515
and PAC if you like sort of frequentist, if it turns out that your prior's wrong.

1073
00:58:26,515 --> 00:58:30,580
Okay. So, um, one of the papers that I

1074
00:58:30,580 --> 00:58:34,105
think sort of changed a lot of people's minds about Bayesian and, uh,

1075
00:58:34,105 --> 00:58:35,710
bandits and also Thompson sampling being

1076
00:58:35,710 --> 00:58:38,935
a good idea was this paper by my colleague Lihong Li and also,

1077
00:58:38,935 --> 00:58:40,570
uh, Chapelle where they looked at

1078
00:58:40,570 --> 00:58:44,065
contextual bandits and we will hopefully get to this for a little bit on Wednesday.

1079
00:58:44,065 --> 00:58:48,955
But the idea in contextual bandits is that you have a state and an action.

1080
00:58:48,955 --> 00:58:51,580
So it's a little bit different than the bandits we've seen so far.

1081
00:58:51,580 --> 00:58:55,180
Unlike in MDPs your action does not affect the next state.

1082
00:58:55,180 --> 00:58:57,280
So for those of you doing the default project,

1083
00:58:57,280 --> 00:58:59,530
you're seeing examples of this where how you treat

1084
00:58:59,530 --> 00:59:02,530
the current patient doesn't impact the next patient that comes along,

1085
00:59:02,530 --> 00:59:06,760
but the patient characteristics can affect which arm is best.

1086
00:59:06,760 --> 00:59:11,080
So, so contextual bandits is a very popular and powerful framework.

1087
00:59:11,080 --> 00:59:15,880
Um, and so in this case they were looking at news article recommendations and they

1088
00:59:15,880 --> 00:59:18,160
were [NOISE] finding Thompson sampling did much better

1089
00:59:18,160 --> 00:59:20,995
than upper confidence bounds in a number of other algorithms.

1090
00:59:20,995 --> 00:59:22,960
It also can be more robust,

1091
00:59:22,960 --> 00:59:25,120
uh, when your outcomes are delayed.

1092
00:59:25,120 --> 00:59:27,190
So this happens a lot often in real cases.

1093
00:59:27,190 --> 00:59:29,080
You can imagine here you treat a patient,

1094
00:59:29,080 --> 00:59:33,655
you're not gonna find out whether or not that toe procedure helps for another six weeks.

1095
00:59:33,655 --> 00:59:36,730
But in the meantime other people come in whose toes needs to be treated.

1096
00:59:36,730 --> 00:59:38,470
Um, and if you use upper confidence bound

1097
00:59:38,470 --> 00:59:40,705
algorithms [NOISE] they tend to be deterministic. Bless you.

1098
00:59:40,705 --> 00:59:42,580
And, um, and so you just keep treating

1099
00:59:42,580 --> 00:59:45,355
everybody with the same thing until you get the outcome from the first,

1100
00:59:45,355 --> 00:59:47,755
whereas Thompson sampling is stochastic.

1101
00:59:47,755 --> 00:59:50,740
So you'll be sort of trying out a lot of things.

1102
00:59:50,740 --> 00:59:54,835
That's another good reason in practice why Thompson sampling can be helpful.

1103
00:59:54,835 --> 00:59:57,820
Okay. So I'm not gonna go through the proof today,

1104
00:59:57,820 --> 00:59:59,755
but I'll put some pointers so that, um, the,

1105
00:59:59,755 --> 01:00:03,160
the nice thing is that if you look at the Bayesian regret of Thompson sampling,

1106
01:00:03,160 --> 01:00:07,285
uh, it's going to have a similar result to what upper confidence bounds has.

1107
01:00:07,285 --> 01:00:09,680
So it essentially has,

1108
01:00:09,720 --> 01:00:16,175
has the same regar- regrets bounds as UCB, essentially.

1109
01:00:16,175 --> 01:00:18,520
I'm being slightly hand-wavey for that,

1110
01:00:18,520 --> 01:00:19,870
there's some important subtle details,

1111
01:00:19,870 --> 01:00:22,240
but roughly you can show that these also have

1112
01:00:22,240 --> 01:00:24,970
good Bayesian regret bounds if your prior's correct.

1113
01:00:24,970 --> 01:00:27,610
Um, and so that's sort of again a nice sanity check,

1114
01:00:27,610 --> 01:00:30,650
that you kind of get this logarithmic regret growth.

1115
01:00:32,100 --> 01:00:35,530
All right. Another framework that I just mentioned sort

1116
01:00:35,530 --> 01:00:39,025
of in passing just now is probably approximately correct.

1117
01:00:39,025 --> 01:00:43,555
So, these theoretical regret bounds specify how your regret grows over time.

1118
01:00:43,555 --> 01:00:46,090
Um, and one thing that's hard to know is whether you're

1119
01:00:46,090 --> 01:00:48,925
making a lot of small mistakes or a few big mistakes.

1120
01:00:48,925 --> 01:00:50,545
Your regret bounds are cumulative,

1121
01:00:50,545 --> 01:00:53,005
so it doesn't allow you to distinguish between those two.

1122
01:00:53,005 --> 01:00:55,150
So, you can imagine in the case of patient treatments,

1123
01:00:55,150 --> 01:00:56,245
this could be pretty important.

1124
01:00:56,245 --> 01:00:58,600
Like are you giving everybody a headache, um,

1125
01:00:58,600 --> 01:01:00,295
or are a few patients really,

1126
01:01:00,295 --> 01:01:02,560
you know, having really really bad side effects.

1127
01:01:02,560 --> 01:01:06,130
So, so regret- cumulative regret,

1128
01:01:06,130 --> 01:01:08,020
um, doesn't distinguish between those two,

1129
01:01:08,020 --> 01:01:10,150
because if a couple of people have really bad side effects,

1130
01:01:10,150 --> 01:01:14,140
that's the same as a lot of people having headaches when you average over those.

1131
01:01:14,140 --> 01:01:17,620
Um, and so one idea is to say well,

1132
01:01:17,620 --> 01:01:21,895
maybe we just wanna kinda bound the number of non-small errors.

1133
01:01:21,895 --> 01:01:23,530
So, we wanna bound the number of people that

1134
01:01:23,530 --> 01:01:26,380
experience really bad side effects, for example.

1135
01:01:26,380 --> 01:01:30,970
[NOISE] So, Probably Approximately Correct comes up in supervised learning.

1136
01:01:30,970 --> 01:01:32,605
In the context of decision-making,

1137
01:01:32,605 --> 01:01:34,390
we often define it as follows,

1138
01:01:34,390 --> 01:01:38,080
a Probably Approximately Correct algorithm or a PAC state that

1139
01:01:38,080 --> 01:01:42,205
the algorithm will choose an action who is- which is epsilon close to optimal,

1140
01:01:42,205 --> 01:01:44,710
with probabilities 1 - delta,

1141
01:01:44,710 --> 01:01:47,875
on all but a polynomial number of steps.

1142
01:01:47,875 --> 01:01:51,940
So, the probability part comes from here,

1143
01:01:51,940 --> 01:01:54,100
so it's not guaranteeing that you will do this,

1144
01:01:54,100 --> 01:01:58,150
but with high confidence or probably it will do this.

1145
01:01:58,150 --> 01:02:03,475
It's approximately correct because we're only guaranteeing epsilon-optimality.

1146
01:02:03,475 --> 01:02:09,130
And the important aspect is it's- only it does- makes these sort of, um,

1147
01:02:09,130 --> 01:02:12,490
makes mistakes that might be bigger than epsilon, so the number of, you know,

1148
01:02:12,490 --> 01:02:13,990
patients we might treat that have really,

1149
01:02:13,990 --> 01:02:18,370
really bad side effects is gonna be no more than a polynomial function.

1150
01:02:18,370 --> 01:02:23,650
Where the polynomial function is a function of the parameters of your domain.

1151
01:02:23,650 --> 01:02:28,850
So, things like the number of actions you have, epsilon and delta.

1152
01:02:29,130 --> 01:02:32,320
And you should be able to compute this in advance too.

1153
01:02:32,320 --> 01:02:35,485
So, you should be able to compute how many mistakes you might make.

1154
01:02:35,485 --> 01:02:40,135
Um, and one of the cool things is that you g- a lot of the PAC algorithms,

1155
01:02:40,135 --> 01:02:44,275
um, algorithms that are PAC are based on optimism or Thompson sampling.

1156
01:02:44,275 --> 01:02:47,875
Now, PAC for bandits is a much less common,

1157
01:02:47,875 --> 01:02:50,575
uh, approach than when we go to MDPs.

1158
01:02:50,575 --> 01:02:53,665
In bandits, most of the time we look at regret.

1159
01:02:53,665 --> 01:02:56,830
But for when we look at Markov Decision Processes,

1160
01:02:56,830 --> 01:02:59,350
PAC is more popular, and,

1161
01:02:59,350 --> 01:03:02,035
and we'll see one of the reasons for that probably later,

1162
01:03:02,035 --> 01:03:05,140
or feel free to ask me about it if we don't get to it today.

1163
01:03:05,140 --> 01:03:09,810
Okay. So, what would PAC look like in our little example we had here before?

1164
01:03:09,810 --> 01:03:11,880
So, let's use O to denote optimism,

1165
01:03:11,880 --> 01:03:13,860
TS to denote Thompson sampling,

1166
01:03:13,860 --> 01:03:15,990
and within epsilon, um,

1167
01:03:15,990 --> 01:03:20,345
means that the action that we select is within epsilon of the optimal action.

1168
01:03:20,345 --> 01:03:23,815
So, its value is epsilon close to the optimal action.

1169
01:03:23,815 --> 01:03:26,260
So, I've written down the regret in this case.

1170
01:03:26,260 --> 01:03:30,925
Um, here what we'd have is that the- for, um,

1171
01:03:30,925 --> 01:03:35,725
optimism, the first action that we pull is a1, so, um,

1172
01:03:35,725 --> 01:03:37,300
it's within epsilon, yes,

1173
01:03:37,300 --> 01:03:41,154
because we're close to the optimal action, a2,

1174
01:03:41,154 --> 01:03:44,050
um, has a mean of 0.9,

1175
01:03:44,050 --> 01:03:48,520
so that's within 0.05 of 0.95, so this is yes.

1176
01:03:48,520 --> 01:03:50,830
Action a3 is 0.1,

1177
01:03:50,830 --> 01:03:53,110
so it's not within epsilon of the optimal action,

1178
01:03:53,110 --> 01:03:56,870
so this is no, and so forth.

1179
01:03:57,150 --> 01:04:00,520
So, this essentially allows the algorithm to be taking

1180
01:04:00,520 --> 01:04:04,150
either a1 or action a2 under this definition of epsilon.

1181
01:04:04,150 --> 01:04:07,030
Because I don't care whether or not you're taking action a1 or a2,

1182
01:04:07,030 --> 01:04:08,455
both of them are really pretty good;

1183
01:04:08,455 --> 01:04:12,415
both of them are within 0.05 of each other, I mean, that's fine.

1184
01:04:12,415 --> 01:04:17,575
Um, but you- we don't want you to take action 3 very much because it's much worse.

1185
01:04:17,575 --> 01:04:19,975
Um, and then in this case,

1186
01:04:19,975 --> 01:04:21,775
uh, this one would say,

1187
01:04:21,775 --> 01:04:24,100
this is not within epsilon because the first action we

1188
01:04:24,100 --> 01:04:27,760
take is bad but then all the rest are good.

1189
01:04:27,760 --> 01:04:30,820
And what a PAC approach would do would they'd be counting

1190
01:04:30,820 --> 01:04:34,010
all these- counting the, the mistakes.

1191
01:04:41,790 --> 01:04:45,399
Okay. So, we just talked about for bandits,

1192
01:04:45,399 --> 01:04:48,040
um, different sorts of frameworks and criterias.

1193
01:04:48,040 --> 01:04:49,120
We talked about regret,

1194
01:04:49,120 --> 01:04:51,370
Bayesian regret and PAC, um,

1195
01:04:51,370 --> 01:04:53,530
[NOISE] and we talked about two styles of approaches,

1196
01:04:53,530 --> 01:04:56,230
either optimism or Thompson sampling.

1197
01:04:56,230 --> 01:04:58,900
And what we can see now is that Markov decision processes

1198
01:04:58,900 --> 01:05:02,200
have many of the same sorts of ideas being applicable,

1199
01:05:02,200 --> 01:05:05,180
but it also does get a lot more challenging.

1200
01:05:05,250 --> 01:05:08,740
So, in particular what we're gonna talk about right now

1201
01:05:08,740 --> 01:05:12,430
is we're gonna talk about tabular MDPs.

1202
01:05:12,770 --> 01:05:17,520
And it turns out that even from with tabular MDPs that things are a lot more subtle.

1203
01:05:17,520 --> 01:05:20,260
Um, so, how does this work?

1204
01:05:20,260 --> 01:05:23,650
The, the regret- the Bayesian regret in PAC is all gonna be applicable,

1205
01:05:23,650 --> 01:05:27,110
so is optimism, and so is probability matching.

1206
01:05:27,570 --> 01:05:32,000
So, let's start with thinking about optimism under uncertainty.

1207
01:05:32,040 --> 01:05:36,040
First, let's think about just doing optimistic initialization.

1208
01:05:36,040 --> 01:05:40,645
So, in this case, imagine that we just initialize all of our queue state actions,

1209
01:05:40,645 --> 01:05:42,610
um, to some value.

1210
01:05:42,610 --> 01:05:47,155
So, let's imagine that we initialize them to rmax divided by 1 - gamma,

1211
01:05:47,155 --> 01:05:52,190
where rmax is the highest reward you could see in any state-action pair.

1212
01:05:52,740 --> 01:05:57,680
Let's just take one minute, why is that value guaranteed to be optimistic?

1213
01:06:09,600 --> 01:06:14,030
Anybody wanna answer why that's guaranteed to be optimistic?

1214
01:06:17,070 --> 01:06:18,280
Right.

1215
01:06:18,280 --> 01:06:19,045
Yeah.

1216
01:06:19,045 --> 01:06:20,965
It's higher than like,

1217
01:06:20,965 --> 01:06:24,280
the possible, um, total value,

1218
01:06:24,280 --> 01:06:25,660
because like we've shown a couple of times

1219
01:06:25,660 --> 01:06:28,975
that rmax one line of scandal would be the highest value,

1220
01:06:28,975 --> 01:06:30,250
but it goes on a bit [inaudible].

1221
01:06:30,250 --> 01:06:31,810
That's right. Yeah, so what  said is correct.

1222
01:06:31,810 --> 01:06:34,855
Um, we've shown that for a discounted Markov decision process

1223
01:06:34,855 --> 01:06:38,305
that the highest value you could get is rmax divided by 1 - gamma.

1224
01:06:38,305 --> 01:06:40,285
At best all of your states have that,

1225
01:06:40,285 --> 01:06:41,560
or else some of them might not,

1226
01:06:41,560 --> 01:06:44,305
so this is guaranteed to be an optimistic value.

1227
01:06:44,305 --> 01:06:46,360
So, you could start off and if you've, uh,

1228
01:06:46,360 --> 01:06:50,725
initialized all of your state action values to be rmax divided by 1 - gamma.

1229
01:06:50,725 --> 01:06:53,095
And then you can do Monte-Carlo,

1230
01:06:53,095 --> 01:06:54,910
you can do Q-learning, you can do Sarsa.

1231
01:06:54,910 --> 01:06:58,420
Um, and you could incrementally update using that.

1232
01:06:58,420 --> 01:07:00,595
And this can be very helpful,

1233
01:07:00,595 --> 01:07:03,700
it can sort of encourage systematic exploration of states and actions,

1234
01:07:03,700 --> 01:07:06,910
because essentially you're pretending that everything in the world is really awesome,

1235
01:07:06,910 --> 01:07:09,470
um, until proven otherwise.

1236
01:07:10,650 --> 01:07:13,060
So, on the downside,

1237
01:07:13,060 --> 01:07:15,880
unfortunately if you do this in general there's no guarantees on performance,

1238
01:07:15,880 --> 01:07:19,000
um, even though it's often empirically better.

1239
01:07:19,000 --> 01:07:22,165
So, even though this really is,

1240
01:07:22,165 --> 01:07:23,560
um, you- you know,

1241
01:07:23,560 --> 01:07:25,495
an upper bound, this is optimistic.

1242
01:07:25,495 --> 01:07:31,135
Um, a key issue is how quickly you're updating from those optimistic values.

1243
01:07:31,135 --> 01:07:33,655
So, as an early result in this case,

1244
01:07:33,655 --> 01:07:36,760
Even-Dar and Mansour in 2002 proved that,

1245
01:07:36,760 --> 01:07:41,690
if you run Q-learning with learning rates- this should say alpha-i.

1246
01:07:41,970 --> 01:07:46,690
So if you, uh, run Q-learning with particular alpha rates, um,

1247
01:07:46,690 --> 01:07:49,030
alpha-i on each time step i,

1248
01:07:49,030 --> 01:07:51,835
and you initialize the value of a state,

1249
01:07:51,835 --> 01:07:54,010
so this is the very beginning, um,

1250
01:07:54,010 --> 01:07:58,705
to be rmax divide by 1 - gamma times the product of those learning rates,

1251
01:07:58,705 --> 01:08:01,780
and t is the number of samples you need to learn optimal Q,

1252
01:08:01,780 --> 01:08:05,980
then greedy-only Q-learning is PAC with that initialization.

1253
01:08:05,980 --> 01:08:10,600
So, I just wanna highlight something here which is this part.

1254
01:08:10,600 --> 01:08:12,910
So, notice this is way, way,

1255
01:08:12,910 --> 01:08:16,255
way larger than just rmax over 1 - gamma,

1256
01:08:16,255 --> 01:08:19,270
because this is a product of all your learning rates.

1257
01:08:19,270 --> 01:08:23,800
Okay, so, this could be really enormous,

1258
01:08:23,800 --> 01:08:25,675
like you'd imagine that, um,

1259
01:08:25,675 --> 01:08:31,270
imagine that alpha = 0.1 for all time steps,

1260
01:08:31,270 --> 01:08:35,814
then what you have here is you have 1 over 0.1 to the t,

1261
01:08:35,814 --> 01:08:41,829
which is approximately- it would just equal to 10 to the t [NOISE].

1262
01:08:41,830 --> 01:08:46,630
So, this is like exponential in the number of time steps you're gonna make decisions.

1263
01:08:46,630 --> 01:08:48,895
It's incredibly optimistic.

1264
01:08:48,895 --> 01:08:51,205
Um, it turns out this is sufficient to be PAC,

1265
01:08:51,205 --> 01:08:52,600
but it's also not very good.

1266
01:08:52,600 --> 01:08:57,370
Um, uh, it's, it's very, very extremely large.

1267
01:08:57,370 --> 01:09:00,609
Okay? Um, now, there's been some really cool work

1268
01:09:00,609 --> 01:09:03,744
by Chi Jin and some others over at Berkeley that showed that,

1269
01:09:03,745 --> 01:09:07,645
um, if you use a less optimistic initialization,

1270
01:09:07,645 --> 01:09:10,375
um, that's strongly related to upper confidence bounds,

1271
01:09:10,375 --> 01:09:13,555
um, and you were careful about your learning rates,

1272
01:09:13,555 --> 01:09:14,950
so you have to change your learning rates,

1273
01:09:14,950 --> 01:09:16,359
but if you're careful about your learning rates,

1274
01:09:16,359 --> 01:09:20,184
they proved that, um, model-free Q-learning could also be PAC.

1275
01:09:20,185 --> 01:09:22,930
And this was a pretty big deal recently because almost

1276
01:09:22,930 --> 01:09:25,854
all of the work that's been going on has been in the model-based setting.

1277
01:09:25,854 --> 01:09:27,939
So this just came out in NeurIPS, uh,

1278
01:09:27,939 --> 01:09:29,379
about two months ago, um,

1279
01:09:29,380 --> 01:09:30,490
and so they- oh sorry, not PAC.

1280
01:09:30,490 --> 01:09:32,080
They, they showed the regret bounds.

1281
01:09:32,080 --> 01:09:34,825
Um, they're not optimal regret bounds, but they're good.

1282
01:09:34,825 --> 01:09:37,810
So, um, they're, they're not tight yet but, uh,

1283
01:09:37,810 --> 01:09:40,029
it shows that model-free algorithms can do pretty well.

1284
01:09:40,029 --> 01:09:43,059
[NOISE]

1285
01:09:43,060 --> 01:09:46,210
Okay. So what about model-based approaches?

1286
01:09:46,210 --> 01:09:47,950
And the model-based approaches for

1287
01:09:47,950 --> 01:09:51,100
MDPs are the ones where we really have the best bounds right now.

1288
01:09:51,100 --> 01:09:54,610
So there's a couple of main ideas or a couple different procedures we could go with.

1289
01:09:54,610 --> 01:09:56,350
One is that, you can be really,

1290
01:09:56,350 --> 01:09:58,299
really optimistic in all your estimates,

1291
01:09:58,299 --> 01:10:02,080
until you're confident that your empirical estimates of

1292
01:10:02,080 --> 01:10:07,880
your dynamics and reward model are close to the true dynamics in reward model parameters.

1293
01:10:08,370 --> 01:10:12,055
So these sort of algorithms proceed as if they say,

1294
01:10:12,055 --> 01:10:14,800
the reward for all state action pairs is amazing,

1295
01:10:14,800 --> 01:10:16,930
it's rmax divided by 1 - gamma.

1296
01:10:16,930 --> 01:10:19,150
And I'm gonna continue to pretend that's true,

1297
01:10:19,150 --> 01:10:20,920
until I think I have enough data for

1298
01:10:20,920 --> 01:10:24,655
that state action pair that I think that if I did a MLE,

1299
01:10:24,655 --> 01:10:26,830
maximum likelihood estimate of those parameters,

1300
01:10:26,830 --> 01:10:29,480
they will be close to the true parameters.

1301
01:10:29,760 --> 01:10:33,910
So you could say, I'm just going to be incredibly optimistic until I've got enough data.

1302
01:10:33,910 --> 01:10:35,800
And then when I've got enough data then I, um,

1303
01:10:35,800 --> 01:10:39,250
think I can get a good empirical estimate that is close to the true estimate,

1304
01:10:39,250 --> 01:10:40,675
and then I'll use those instead.

1305
01:10:40,675 --> 01:10:42,730
So it's almost kinda like a switching point.

1306
01:10:42,730 --> 01:10:44,860
You sort of keep, um, you pretend everything's really,

1307
01:10:44,860 --> 01:10:46,525
really great until you get enough data,

1308
01:10:46,525 --> 01:10:49,120
and then you switch over to the empirical estimate.

1309
01:10:49,120 --> 01:10:51,835
So these were some of the earliest ones, um,

1310
01:10:51,835 --> 01:10:54,490
that showed that MDPs could be pa- oh,

1311
01:10:54,490 --> 01:10:56,335
algorithms for MDPs could be PAC.

1312
01:10:56,335 --> 01:10:57,595
This is from 2002.

1313
01:10:57,595 --> 01:11:01,630
Uh, but they're also empirically not normally

1314
01:11:01,630 --> 01:11:03,070
so good because, um,

1315
01:11:03,070 --> 01:11:04,855
you're pretending things are really, really awesome,

1316
01:11:04,855 --> 01:11:06,460
even though you might have quite a lot of evidence

1317
01:11:06,460 --> 01:11:08,905
for that state-action pair that it's not awesome.

1318
01:11:08,905 --> 01:11:11,770
So another approach is to be optimistic given

1319
01:11:11,770 --> 01:11:14,350
the information you have. So what do I mean by that?

1320
01:11:14,350 --> 01:11:16,600
I mean that as your agent walks around and gathers

1321
01:11:16,600 --> 01:11:20,215
observations of the actions and rewards it gets,

1322
01:11:20,215 --> 01:11:22,360
it uses that to try to estimate,

1323
01:11:22,360 --> 01:11:25,555
um, how good the world could be given that data.

1324
01:11:25,555 --> 01:11:30,805
And so one approach to this is to compute confidence sets on dynamics and rewards models.

1325
01:11:30,805 --> 01:11:32,860
So we already saw this for bandits,

1326
01:11:32,860 --> 01:11:34,540
where we computed upper and lower confidence,

1327
01:11:34,540 --> 01:11:37,735
or we could compute upper and lower confidence bounds for the rewards.

1328
01:11:37,735 --> 01:11:42,040
Turns out we can also compute confidence sets over the dynamics model.

1329
01:11:42,040 --> 01:11:47,440
Or we could just add reward bonuses that depend on the experience or data.

1330
01:11:47,440 --> 01:11:49,675
And I'm gonna talk, um,

1331
01:11:49,675 --> 01:11:51,385
at least a little bit today before we finish,

1332
01:11:51,385 --> 01:11:53,660
about the second thing.

1333
01:11:54,020 --> 01:11:57,900
And the reason I'm gonna talk about this particular approach is because when we

1334
01:11:57,900 --> 01:12:01,520
start to think about doing this in the function approximation setting,

1335
01:12:01,520 --> 01:12:06,685
if the way that your dynamics model is represented is by a deep neural network,

1336
01:12:06,685 --> 01:12:08,305
um, then writing down, ah,

1337
01:12:08,305 --> 01:12:11,125
uncertainties over that can be really tricky.

1338
01:12:11,125 --> 01:12:12,790
And also a lot of the progress in

1339
01:12:12,790 --> 01:12:16,030
deep neural networks for RL focus on model-free approaches.

1340
01:12:16,030 --> 01:12:18,070
And if we have reward bonuses,

1341
01:12:18,070 --> 01:12:20,980
then we can easily extend that to the model-free case.

1342
01:12:20,980 --> 01:12:23,755
Um, and empirically these ones generally do

1343
01:12:23,755 --> 01:12:27,410
pretty much as well as if we use explicit confidence sets.

1344
01:12:27,690 --> 01:12:31,825
So I'm just gonna explain how the model-based confidence,

1345
01:12:31,825 --> 01:12:35,065
model-based interval estimation with exploration bonus works.

1346
01:12:35,065 --> 01:12:43,750
Um, so it's gonna assume that we're given an epsilon delta and some constant m. Okay.

1347
01:12:43,750 --> 01:12:48,220
And then what we're gonna do is we're gonna initialize some counts.

1348
01:12:48,220 --> 01:12:51,160
[NOISE] So this is just

1349
01:12:51,160 --> 01:12:54,535
gonna keep track of the number of times we've seen a state-action pair.

1350
01:12:54,535 --> 01:12:57,250
So we're gonna do this for all s and for all a.

1351
01:12:57,250 --> 01:12:59,860
We're also gonna keep track of the number of times we've

1352
01:12:59,860 --> 01:13:03,085
seen an actio- state-action, next state pair.

1353
01:13:03,085 --> 01:13:08,275
0 for all s, for all a, for all s prime.

1354
01:13:08,275 --> 01:13:11,080
And we're also going to keep track of the total sum of

1355
01:13:11,080 --> 01:13:13,795
rewards we've gotten from any state and action pair.

1356
01:13:13,795 --> 01:13:18,730
So we're gonna say rc of s, a = 0 for

1357
01:13:18,730 --> 01:13:27,640
all s. So essentially we are gonna keep track of the times that we've been in any state,

1358
01:13:27,640 --> 01:13:30,340
taking any action and went to any next state,

1359
01:13:30,340 --> 01:13:34,160
and what the sum of rewards are for when we've done that.

1360
01:13:35,490 --> 01:13:40,190
And then we're going to define a beta parameter.

1361
01:13:41,640 --> 01:13:44,720
Okay, I'm going to double-check, I get the-

1362
01:13:49,590 --> 01:14:01,870
Yeah.

1363
01:14:01,870 --> 01:14:04,240
Okay. All right. So beta is gonna be

1364
01:14:04,240 --> 01:14:08,270
a parameter that we're gonna use to define our reward bonuses.

1365
01:14:08,310 --> 01:14:11,035
Okay, it's 1 over 1 - gamma,

1366
01:14:11,035 --> 01:14:12,610
2 log the number of states,

1367
01:14:12,610 --> 01:14:17,170
number of actions, 2 times m, m is an input parameter divided by delta.

1368
01:14:17,170 --> 01:14:25,960
Okay. And then- yeah,

1369
01:14:25,960 --> 01:14:27,145
I think that's all I need here.

1370
01:14:27,145 --> 01:14:29,125
Now I'm gonna say t = 0.

1371
01:14:29,125 --> 01:14:31,880
We're going to initialize our state.

1372
01:14:38,250 --> 01:14:49,970
And to start, we can just say Qt of s, a = 1 divided by 1 - gamma.

1373
01:14:51,440 --> 01:14:56,460
And this assumes that all of our rewards are bounded between 0 and 1.

1374
01:14:56,460 --> 01:14:58,400
So they're bounded rewards.

1375
01:14:58,400 --> 01:15:01,540
Okay, so we start off when we initialize all our accounts to 0,

1376
01:15:01,540 --> 01:15:03,670
we said we haven't observe- observed any rewards yet,

1377
01:15:03,670 --> 01:15:05,410
and we pretend that the world is awesome,

1378
01:15:05,410 --> 01:15:09,910
and that our Q value is the highest it could possibly be in every state-action pair.

1379
01:15:09,910 --> 01:15:13,180
Um, here r-max = 1.

1380
01:15:13,180 --> 01:15:15,190
So r-max is going to be equal to 1

1381
01:15:15,190 --> 01:15:18,217
because our rewards are bounded between 0 and 1.

1382
01:15:18,217 --> 01:15:22,390
So what we do then is we take an action in the current state,

1383
01:15:22,390 --> 01:15:28,465
given our- let's do tildes given our Q function.

1384
01:15:28,465 --> 01:15:30,700
So getting, which is going to break ties randomly.

1385
01:15:30,700 --> 01:15:34,150
And then we're going to observe the reward and observe the next state.

1386
01:15:34,150 --> 01:15:35,950
And then we just update our counts.

1387
01:15:35,950 --> 01:15:39,970
So we update our counts for that particular state action pair.

1388
01:15:39,970 --> 01:15:44,350
We update our counts for s, a, s prime, s, a,

1389
01:15:44,350 --> 01:15:47,740
s prime, for the number of

1390
01:15:47,740 --> 01:15:51,565
times we've been in that state taking that action and went to that particular next state.

1391
01:15:51,565 --> 01:15:55,495
And then we update our rewards for that state-action pair.

1392
01:15:55,495 --> 01:16:00,740
It is equal to the previous rewards for that state action pair plus r_t.

1393
01:16:01,710 --> 01:16:05,260
And then what we're gonna do is we're gonna use, um,

1394
01:16:05,260 --> 01:16:07,390
those empirical counts to define

1395
01:16:07,390 --> 01:16:10,960
an empirical transition model and empirical reward model.

1396
01:16:10,960 --> 01:16:15,970
So our reward model is going to just be the MLE reward model,

1397
01:16:15,970 --> 01:16:21,085
which is just gonna be rc for s, a divide-

1398
01:16:21,085 --> 01:16:28,220
times- divided by the number of times we've been in that state-action pair.

1399
01:16:28,350 --> 01:16:33,260
That's just the average reward for that state-action pair.

1400
01:16:34,860 --> 01:16:45,670
And then our transition model is also just going to be the number of times a,

1401
01:16:45,670 --> 01:16:51,700
s prime divided by the number of times you've been in that state-action pair.

1402
01:16:51,700 --> 01:16:56,545
We're just gonna define our empirical transition model and our empirical reward model.

1403
01:16:56,545 --> 01:17:01,510
And it doesn't matter how we initialize things that we haven't seen at all.

1404
01:17:01,510 --> 01:17:04,705
But you can treat them as uniform.

1405
01:17:04,705 --> 01:17:07,850
Okay. So we're gonna do this for all s, a.

1406
01:17:08,040 --> 01:17:11,800
And then we're gonna compute some new Q functions.

1407
01:17:11,800 --> 01:17:17,660
Okay. And we're gonna compute some new Q functions where we do this.

1408
01:17:20,040 --> 01:17:25,240
Where we take our empirical models and we also add in

1409
01:17:25,240 --> 01:17:27,190
a reward bonus term that depends on

1410
01:17:27,190 --> 01:17:31,210
beta and the number of times we've tried that state action pair.

1411
01:17:31,210 --> 01:17:33,520
And we can do value iteration.

1412
01:17:33,520 --> 01:17:36,550
That's what I'm doing here. But you could solve it however you'd like.

1413
01:17:36,550 --> 01:17:40,300
But the main idea here is that we're gonna use our empirical estimates

1414
01:17:40,300 --> 01:17:44,170
of the reward model and the transition model by just averaging our counts,

1415
01:17:44,170 --> 01:17:46,945
or averaging the rewards we've gotten for that state-action pair.

1416
01:17:46,945 --> 01:17:50,510
And then we're gonna add in this as a reward bonus.

1417
01:17:54,510 --> 01:17:57,910
And note at the beginning of this reward bonus can be,

1418
01:17:57,910 --> 01:17:59,320
like, it can be infinity,

1419
01:17:59,320 --> 01:18:01,390
so you can- because if we have no counts for that,

1420
01:18:01,390 --> 01:18:03,625
so then we can just initialize for,

1421
01:18:03,625 --> 01:18:05,185
for any Q s,a.

1422
01:18:05,185 --> 01:18:12,820
So for all s, a such that nsa of s,a = 0.

1423
01:18:12,820 --> 01:18:15,710
You can just set this to be Q-max.

1424
01:18:17,130 --> 01:18:22,510
So to deal with if you haven't sampled that state-action pair yet.

1425
01:18:22,510 --> 01:18:25,180
So that means anything for which you haven't

1426
01:18:25,180 --> 01:18:27,940
sampled it yet is gonna look maximally awesome.

1427
01:18:27,940 --> 01:18:34,455
And anything else is going to be a combination of its empirical average parameters,

1428
01:18:34,455 --> 01:18:36,090
plus a reward bonus.

1429
01:18:36,090 --> 01:18:40,530
And that reward bonus is gonna get smaller as we have more data.

1430
01:18:40,530 --> 01:18:43,380
So I'll put this on here where it will be neater.

1431
01:18:43,380 --> 01:18:47,500
Um, so this is the reward bonus.

1432
01:18:50,400 --> 01:18:54,565
And what you can see here is that over time that's going to shrink.

1433
01:18:54,565 --> 01:18:57,820
Over time, um, you're going to get closer and closer to

1434
01:18:57,820 --> 01:19:01,405
using the empirical estimates for a particular state action pair.

1435
01:19:01,405 --> 01:19:03,460
But for state action pairs you haven't tried very much,

1436
01:19:03,460 --> 01:19:06,140
there's going to be a large reward bonus.

1437
01:19:08,670 --> 01:19:11,905
So the- the cool thing about this is that,

1438
01:19:11,905 --> 01:19:13,735
um, we can think about whether it's PAC.

1439
01:19:13,735 --> 01:19:14,965
So I'll just take one more minute,

1440
01:19:14,965 --> 01:19:17,935
which is in an RL case, ah,

1441
01:19:17,935 --> 01:19:20,380
an algorithm is PAC if on all but N time steps,

1442
01:19:20,380 --> 01:19:22,945
the action selected is epsilon-close to the optimal action,

1443
01:19:22,945 --> 01:19:25,960
where N is a polynomial function of these things.

1444
01:19:25,960 --> 01:19:28,140
The number of states,

1445
01:19:28,140 --> 01:19:29,280
number of actions, gamma,

1446
01:19:29,280 --> 01:19:32,475
epsilon, and delta, this is not true for all algorithms.

1447
01:19:32,475 --> 01:19:34,095
Greedy is not PAC.

1448
01:19:34,095 --> 01:19:36,180
Greedy can be exponential.

1449
01:19:36,180 --> 01:19:41,640
Um, we might talk about that on Wednesday. So no.

1450
01:19:41,640 --> 01:19:47,010
And the nice thing is that the MBIE-EB algorithm I just showed you is PAC.

1451
01:19:47,010 --> 01:19:49,125
So what does it PAC in?

1452
01:19:49,125 --> 01:19:50,835
It means that on all,

1453
01:19:50,835 --> 01:19:52,485
but this number of time-steps,

1454
01:19:52,485 --> 01:19:55,300
well I'll just circle it.

1455
01:19:57,120 --> 01:20:00,520
So this is sort of a large ugly expression,

1456
01:20:00,520 --> 01:20:03,325
but it is polynomial in the number of states and actions.

1457
01:20:03,325 --> 01:20:06,685
It's also a function of the discount factor and the epsilon.

1458
01:20:06,685 --> 01:20:08,500
In general, if you want to be closer to optimal,

1459
01:20:08,500 --> 01:20:11,065
it's gonna take you more data to ensure that you're close to optimal.

1460
01:20:11,065 --> 01:20:13,840
So it's inversely dependent on epsilon,

1461
01:20:13,840 --> 01:20:16,510
ah, it's polynomially dependent on,

1462
01:20:16,510 --> 01:20:18,160
ah, state and actions.

1463
01:20:18,160 --> 01:20:20,050
And this says on all but,

1464
01:20:20,050 --> 01:20:21,745
ah, this many time steps,

1465
01:20:21,745 --> 01:20:24,190
your algorithm is gonna be taking actions that are close to

1466
01:20:24,190 --> 01:20:26,770
optimal. So this is pretty cool.

1467
01:20:26,770 --> 01:20:30,325
It says like by just using these average estimates plus tacking on a bonus term,

1468
01:20:30,325 --> 01:20:32,950
and then computing the Q functions, um,

1469
01:20:32,950 --> 01:20:38,455
then you can actually act really well on all time-steps except for a polynomial number.

1470
01:20:38,455 --> 01:20:40,975
Okay. And then, um,

1471
01:20:40,975 --> 01:20:43,570
I put in here this are theoretical of that,

1472
01:20:43,570 --> 01:20:46,960
and I'll just say briefly that on some sort of hard to construct,

1473
01:20:46,960 --> 01:20:48,730
sort of simple toy domains.

1474
01:20:48,730 --> 01:20:50,770
These type of algorithms do much better even

1475
01:20:50,770 --> 01:20:53,440
than some other ones that are provably efficient.

1476
01:20:53,440 --> 01:20:55,960
And they can do much, much better than things like greedy.

1477
01:20:55,960 --> 01:20:58,450
So algorithms like MBIE-EB,

1478
01:20:58,450 --> 01:21:02,275
MBIE is a related one that uses confidence sets, um,

1479
01:21:02,275 --> 01:21:05,275
it can do much, much better than this is sort of be,

1480
01:21:05,275 --> 01:21:10,790
be optimistic until confident.

1481
01:21:12,420 --> 01:21:16,225
And these ones are generally much better than greedy.

1482
01:21:16,225 --> 01:21:19,570
So these types of optimistic algorithms can empirically be much better,

1483
01:21:19,570 --> 01:21:21,100
as well being provably better.

1484
01:21:21,100 --> 01:21:22,900
And on Wednesday we'll start to talk about how to

1485
01:21:22,900 --> 01:21:25,910
combine them with generalization. Thanks.

