1
00:00:05,200 --> 00:00:07,680
привет всем, я начну хорошо, так

2
00:00:07,680 --> 00:00:10,320
что теперь я вернулся на вторую неделю

3
00:00:11,719 --> 00:00:13,280
cs224n

4
00:00:13,280 --> 00:00:15,759
ммм по обработке естественного языка с

5
00:00:15,759 --> 00:00:17,520
глубоким обучением

6
00:00:17,520 --> 00:00:19,600
хорошо, так что на сегодняшней лекции

7
00:00:19,600 --> 00:00:21,680
мы будем смотреть на

8
00:00:21,680 --> 00:00:22,880
всю

9
00:00:22,880 --> 00:00:26,000
математику  детали обучения нейронной сети,

10
00:00:26,000 --> 00:00:28,320
прежде всего, глядя на то,

11
00:00:28,320 --> 00:00:31,359
как мы можем вручную разработать градиенты

12
00:00:31,359 --> 00:00:33,920
для обучения нейронных сетей, а затем

13
00:00:33,920 --> 00:00:35,440
посмотреть, как это делается более

14
00:00:35,440 --> 00:00:37,200
алгоритмически, что известно как

15
00:00:37,200 --> 00:00:40,239
алгоритм обратного распространения

16
00:00:40,239 --> 00:00:41,040
и,

17
00:00:41,040 --> 00:00:44,160
соответственно, для вас, ребята, ну, я

18
00:00:44,160 --> 00:00:46,000
надеюсь  вы вспомнили, что знаете, что

19
00:00:46,000 --> 00:00:48,320
минуту назад была дата первого задания,

20
00:00:48,320 --> 00:00:50,960
и все его сдали, если

21
00:00:50,960 --> 00:00:53,600
по какой-то случайности вы его не сдали,

22
00:00:53,600 --> 00:00:54,480
эм,

23
00:00:54,480 --> 00:00:56,000
действительно, нужно сдать его как

24
00:00:56,000 --> 00:00:57,760
можно скорее, лучше всего, чтобы сохранить эти поздние

25
00:00:57,760 --> 00:01:00,320
дни для трудностей  задания, так что

26
00:01:00,320 --> 00:01:02,160
я имею в виду, что на самом деле я забыл упомянуть, что мы

27
00:01:02,160 --> 00:01:05,119
действительно внесли одно изменение в этом

28
00:01:05,119 --> 00:01:06,880
году, чтобы было немного легче, когда

29
00:01:06,880 --> 00:01:09,280
иногда люди присоединяются к классу с

30
00:01:09,280 --> 00:01:11,760
опозданием на неделю, если вы хотите, чтобы в этом году

31
00:01:11,760 --> 00:01:14,479
при выставлении оценок эм  первое задание можно

32
00:01:14,479 --> 00:01:16,880
обесценить, и мы просто воспользуемся вашими другими

33
00:01:16,880 --> 00:01:19,200
четырьмя заданиями, но если вы

34
00:01:19,200 --> 00:01:21,920
уже прошли курс для этих 98 процентов

35
00:01:21,920 --> 00:01:24,320
людей хорошо, так как

36
00:01:24,320 --> 00:01:26,799
первое задание снова является самым простым заданием, глупо не

37
00:01:26,799 --> 00:01:28,640
выполнять его и не получить его  как часть вашей

38
00:01:28,640 --> 00:01:30,000
оценки

39
00:01:30,000 --> 00:01:31,600
хорошо, поэтому,

40
00:01:31,600 --> 00:01:34,240
начиная с сегодняшнего дня, мы выпускаем задание

41
00:01:34,240 --> 00:01:36,960
два, а задание два - это все о том,

42
00:01:36,960 --> 00:01:39,119
чтобы убедиться, что вы действительно понимаете

43
00:01:39,119 --> 00:01:41,360
математику нейронных сетей, а затем

44
00:01:41,360 --> 00:01:45,360
программное обеспечение, которое мы используем для выполнения этой математики,

45
00:01:45,360 --> 00:01:48,079
так что это будет

46
00:01:48,079 --> 00:01:50,399
для некоторых это была трудная неделя, поэтому для некоторых людей, которые

47
00:01:50,399 --> 00:01:53,119
хорошо разбираются в математике и

48
00:01:53,119 --> 00:01:56,159
знаниях, они будут чувствовать, что это то, что они

49
00:01:56,159 --> 00:01:58,640
хорошо знают.

50
00:02:02,079 --> 00:02:04,079
Неделя - это самая большая

51
00:02:04,079 --> 00:02:06,240
проблема курса,

52
00:02:06,240 --> 00:02:08,800
мы действительно хотим, чтобы люди

53
00:02:08,800 --> 00:02:10,878
действительно понимали, что происходит в

54
00:02:10,878 --> 00:02:13,360
процессе обучения вашей сети, а не

55
00:02:13,360 --> 00:02:16,160
рассматривали это как некую глубокую магию,

56
00:02:16,160 --> 00:02:18,080
и я надеюсь, что некоторые из материалов, которые мы

57
00:02:18,080 --> 00:02:20,959
даем сегодня, и что они  Прочтение и

58
00:02:20,959 --> 00:02:23,120
использование в задании действительно даст

59
00:02:23,120 --> 00:02:23,840
вам

60
00:02:23,840 --> 00:02:25,760
больше понимания того, что делают эти нейронные

61
00:02:25,760 --> 00:02:28,959
сети, и что это просто

62
00:02:28,959 --> 00:02:31,200
математика, которая применяется в систематическом

63
00:02:31,200 --> 00:02:34,319
крупном масштабе, которая вырабатывает ответы,

64
00:02:34,319 --> 00:02:35,760
и что это будет ценно и

65
00:02:35,760 --> 00:02:37,760
дает  у вас есть более глубокое понимание того, что

66
00:02:37,760 --> 00:02:40,879
происходит, но если этот материал кажется очень

67
00:02:40,879 --> 00:02:44,560
страшным и сложным,

68
00:02:44,560 --> 00:02:46,400
вы можете укрыться в том факте,

69
00:02:46,400 --> 00:02:48,640
что в конце туннеля есть быстрый свет,

70
00:02:48,640 --> 00:02:51,120
поскольку это действительно единственная

71
00:02:51,120 --> 00:02:53,680
лекция, в которой

72
00:02:53,680 --> 00:02:56,400
подробно рассматриваются математические детали  нейронных сетей после

73
00:02:56,400 --> 00:02:58,720
этого мы как бы выскочим на

74
00:02:58,720 --> 00:03:00,560
более высокий уровень,

75
00:03:00,560 --> 00:03:03,440
и, по большому счету, после этой недели мы

76
00:03:03,440 --> 00:03:05,840
будем использовать программное обеспечение, чтобы делать за нас большую

77
00:03:05,840 --> 00:03:09,040
часть сложной математики,

78
00:03:09,040 --> 00:03:11,280
но, тем не менее, я надеюсь, что это

79
00:03:11,280 --> 00:03:13,280
ценный, я быстро все пройду

80
00:03:13,280 --> 00:03:15,519
сегодня, но

81
00:03:15,519 --> 00:03:17,120
если это не то, что вы знаете

82
00:03:17,120 --> 00:03:19,760
задом наперед, я действительно рекомендую вам,

83
00:03:19,760 --> 00:03:22,879
чтобы вы знали, проработайте это и получите помощь

84
00:03:22,879 --> 00:03:25,599
по мере необходимости, так что приходите в наш

85
00:03:25,599 --> 00:03:27,760
офис часы есть  также несколько

86
00:03:27,760 --> 00:03:30,720
частей учебного материала, приведенного в

87
00:03:30,720 --> 00:03:32,959
программе, так что есть обе лекции,

88
00:03:32,959 --> 00:03:36,560
есть некоторые материалы из cs231

89
00:03:36,560 --> 00:03:39,120
ммм в списке чтений.

90
00:03:46,000 --> 00:03:47,840
одна моя любимая

91
00:03:47,840 --> 00:03:50,640
презентация там довольно близко

92
00:03:50,640 --> 00:03:53,280
следует за презентацией в этой

93
00:03:53,280 --> 00:03:55,920
лекции прохождения матричного исчисления, так что вы

94
00:03:55,920 --> 00:03:57,599
лично знаете, что я бы рекомендовал начать

95
00:03:57,599 --> 00:03:59,120
с этого, но есть четыре

96
00:03:59,120 --> 00:04:01,120
разных, из которых вы можете выбрать, если

97
00:04:01,120 --> 00:04:05,040
один из них покажется вам более полезным

98
00:04:05,120 --> 00:04:07,840
две другие вещи о том, что нас ждет

99
00:04:07,840 --> 00:04:10,879
в четверговой лекции,

100
00:04:10,879 --> 00:04:12,799
мы вносим большие изменения, и четвергная

101
00:04:12,799 --> 00:04:14,879
лекция, вероятно, самая лингвистическая

102
00:04:14,879 --> 00:04:16,959
лекция всего класса, в которой мы подробно изучаем

103
00:04:18,560 --> 00:04:20,639
грамматику зависимостей и анализ зависимостей, некоторые

104
00:04:20,639 --> 00:04:22,639
люди находят это тоже трудным, но  по

105
00:04:22,639 --> 00:04:25,120
крайней мере, это будет сложно по-другому,

106
00:04:25,120 --> 00:04:26,720
а затем еще одна действительно хорошая

107
00:04:26,720 --> 00:04:28,560
возможность - в

108
00:04:28,560 --> 00:04:30,960
эту пятницу у нас есть второй наставник

109
00:04:30,960 --> 00:04:33,280
в 10 утра, это введение в

110
00:04:33,280 --> 00:04:35,120
пирог-факел, который представляет собой структуру глубокого обучения,

111
00:04:35,120 --> 00:04:37,759
которую мы будем использовать для

112
00:04:37,759 --> 00:04:39,759
остальной части класса после того, как мы

113
00:04:39,759 --> 00:04:41,440
пройдем эти первые два задания,

114
00:04:41,440 --> 00:04:44,400
где вы, ммм, делаете что-то самостоятельно, так

115
00:04:44,400 --> 00:04:46,320
что это  отличный шанс познакомиться с

116
00:04:46,320 --> 00:04:48,000
pytorch, они будут действительно полезны в

117
00:04:48,000 --> 00:04:50,800
дальнейшем в классе,

118
00:04:50,800 --> 00:04:52,000
ладно,

119
00:04:52,000 --> 00:04:55,600
сегодняшний материал действительно посвящен

120
00:04:55,600 --> 00:04:59,040
своего рода математике нейронных сетей, но

121
00:04:59,040 --> 00:05:01,199
просто для того, чтобы представить настройку, в

122
00:05:01,199 --> 00:05:03,440
которой мы можем работать с этим i '  m

123
00:05:03,440 --> 00:05:07,199
собираюсь представить простую задачу nlp и

124
00:05:07,199 --> 00:05:09,120
простую форму классификатора, которую мы можем

125
00:05:09,120 --> 00:05:11,600
использовать для нее, поэтому задача распознавания именованных сущностей

126
00:05:11,600 --> 00:05:15,120
является очень распространенной базовой задачей nlp,

127
00:05:15,120 --> 00:05:17,440
и цель этого состоит в том, что вы

128
00:05:17,440 --> 00:05:20,080
просматриваете фрагменты текста, а

129
00:05:20,080 --> 00:05:22,160
вы  Мы хотим обозначить

130
00:05:22,160 --> 00:05:24,720
слова, которые принадлежат к категориям сущностей,

131
00:05:24,720 --> 00:05:27,759
например, люди, местонахождение,

132
00:05:27,759 --> 00:05:31,919
продукты, даты, время и т. д., поэтому за этот

133
00:05:31,919 --> 00:05:34,560
фрагмент текста прошлой ночью

134
00:05:34,560 --> 00:05:37,120
Пэрис Хилтон в платье с блестками

135
00:05:37,120 --> 00:05:39,039
была арестована Сэмюэл Куинн был арестован в отеле Hilton в

136
00:05:39,039 --> 00:05:42,720
Париже.  в апреле 1989 года некоторые слова

137
00:05:42,720 --> 00:05:45,360
помечаются как именованные сущности, как

138
00:05:45,360 --> 00:05:47,840
показано на рисунке. Эти два предложения на

139
00:05:47,840 --> 00:05:49,360
самом деле не принадлежат друг к другу в одной

140
00:05:49,360 --> 00:05:52,880
статье, но я выбрал эти два предложения,

141
00:05:52,880 --> 00:05:55,440
чтобы проиллюстрировать основную мысль,

142
00:05:55,440 --> 00:05:57,840
что вы не можете просто выполнить эту задачу  с

143
00:05:57,840 --> 00:06:00,400
помощью словаря да, словарь

144
00:06:00,400 --> 00:06:03,120
полезен, чтобы знать, что Париж может

145
00:06:03,120 --> 00:06:06,240
быть местом, но Париж также может быть

146
00:06:06,240 --> 00:06:08,639
именем человека, поэтому вам нужно использовать контекст,

147
00:06:08,639 --> 00:06:10,000
чтобы

148
00:06:10,000 --> 00:06:13,600
правильно распознавать именованные объекты,

149
00:06:14,319 --> 00:06:16,720
хорошо, хорошо, как мы можем сделать это

150
00:06:16,720 --> 00:06:18,800
с нейронной сетью

151
00:06:18,800 --> 00:06:21,280
там  гораздо более продвинутые способы

152
00:06:21,280 --> 00:06:24,240
сделать это, но простой, но уже

153
00:06:24,240 --> 00:06:27,360
довольно хороший способ сделать

154
00:06:27,360 --> 00:06:30,240
распознавание именованных сущностей в простой

155
00:06:30,240 --> 00:06:32,479
нейронной сети - это хорошо сказать,

156
00:06:32,479 --> 00:06:35,680
что мы собираемся сделать, это использовать

157
00:06:35,680 --> 00:06:38,080
слова векторы, о которых мы узнали,

158
00:06:38,080 --> 00:06:40,960
и  мы собираемся создать контекстное

159
00:06:40,960 --> 00:06:43,680
окно векторов слов,

160
00:06:43,680 --> 00:06:46,160
а затем мы собираемся пропустить их

161
00:06:46,160 --> 00:06:48,400
через слой нейронной сети,

162
00:06:48,400 --> 00:06:50,319
а затем пропустить его через

163
00:06:50,319 --> 00:06:54,240
классификатор softmax того типа, который,

164
00:06:54,240 --> 00:06:55,599
извините, я сказал, что не так  d, затем мы

165
00:06:55,599 --> 00:06:57,520
собираемся пропустить его через логистический

166
00:06:57,520 --> 00:06:59,919
классификатор того типа, который мы видели

167
00:06:59,919 --> 00:07:02,479
при рассмотрении отрицательной выборки,

168
00:07:02,479 --> 00:07:04,639
которая говорит о конкретном типе объекта,

169
00:07:04,639 --> 00:07:07,440
таком как

170
00:07:09,759 --> 00:07:12,319
местоположение, является ли это местоположение с высокой вероятностью или это местоположение с невысокой вероятностью

171
00:07:12,319 --> 00:07:15,120
так что для предложения вроде

172
00:07:15,120 --> 00:07:17,840
музеи в париже удивительно видеть,

173
00:07:17,840 --> 00:07:20,000
что мы собираемся сделать, это для каждого слова

174
00:07:20,000 --> 00:07:22,080
сказать, что мы делаем слово париж, мы

175
00:07:22,080 --> 00:07:24,560
собираемся сформировать окно вокруг него, скажем

176
00:07:24,560 --> 00:07:27,280
окно плюс или минус два слова  и поэтому для

177
00:07:27,280 --> 00:07:30,240
этих пяти слов мы собираемся получить

178
00:07:30,240 --> 00:07:33,360
векторы слов из того типа

179
00:07:33,360 --> 00:07:35,360
векторов слов, которые мы

180
00:07:35,360 --> 00:07:38,160
выучили, и мы собираемся сделать длинный

181
00:07:38,160 --> 00:07:40,400
вектор из конкатенации этих

182
00:07:40,400 --> 00:07:42,240
пяти векторов слов, чтобы

183
00:07:42,240 --> 00:07:44,240
интересующее слово находится в середине, а затем мы

184
00:07:44,240 --> 00:07:46,160
собираемся передать

185
00:07:46,160 --> 00:07:47,199
этот

186
00:07:47,199 --> 00:07:50,160
вектор классификатору, который в

187
00:07:50,160 --> 00:07:53,280
конце будет иметь вероятность того, что

188
00:07:53,280 --> 00:07:55,919
слово является местоположением, и тогда у нас может

189
00:07:55,919 --> 00:07:57,759
быть другой классификатор, который говорит

190
00:07:57,759 --> 00:07:59,840
вероятность слова  быть

191
00:07:59,840 --> 00:08:01,919
именем человека и так  как только мы это сделаем,

192
00:08:01,919 --> 00:08:03,120
мы запустим его в следующей

193
00:08:03,120 --> 00:08:05,520
позиции, поэтому мы скажем хорошо, это слово

194
00:08:05,520 --> 00:08:09,039
ra location, и мы скормим окно из

195
00:08:09,039 --> 00:08:11,599
пяти слов, поскольку тогда в Париже тоже удивительно,

196
00:08:11,599 --> 00:08:13,759
и пропустили его через  тот же тип

197
00:08:13,759 --> 00:08:15,840
классификатора,

198
00:08:15,840 --> 00:08:17,840
и поэтому это классификатор, который мы будем

199
00:08:17,840 --> 00:08:21,919
использовать, поэтому его вводом будет это

200
00:08:21,919 --> 00:08:24,479
окно слов, поэтому, если у нас есть d-мерные

201
00:08:24,479 --> 00:08:27,520
векторы слов, это будет 5d вектор,

202
00:08:27,520 --> 00:08:29,759
а затем мы собираемся пропустить его через

203
00:08:29,759 --> 00:08:32,479
слой  нейронная сеть, поэтому

204
00:08:32,479 --> 00:08:36,000
слой нейронной сети будет

205
00:08:36,000 --> 00:08:39,039
умножать этот вектор на матрицу,

206
00:08:39,039 --> 00:08:41,279
добавлять вектор смещения,

207
00:08:41,279 --> 00:08:42,559
а затем передавать это через

208
00:08:42,559 --> 00:08:45,560
нелинейность,

209
00:08:45,839 --> 00:08:47,760
такую как

210
00:08:47,760 --> 00:08:49,839
мягкое преобразование максимального значения, которое мы видели

211
00:08:49,839 --> 00:08:50,800
раньше,

212
00:08:50,800 --> 00:08:53,839
и это даст нам  скрытый вектор,

213
00:08:53,839 --> 00:08:55,279
который может иметь меньшую

214
00:08:55,279 --> 00:08:58,240
размерность, такой как этот здесь,

215
00:08:58,240 --> 00:09:01,600
а затем с этим скрытым

216
00:09:01,600 --> 00:09:04,720
вектором мы собираемся взять его скалярное

217
00:09:04,720 --> 00:09:08,080
произведение с дополнительным

218
00:09:08,080 --> 00:09:11,519
вектором вот здесь u, поэтому мы берем u скалярное

219
00:09:11,519 --> 00:09:15,040
произведение h, и поэтому  когда мы это делаем, мы

220
00:09:15,040 --> 00:09:17,519
получаем одно число, и это

221
00:09:17,519 --> 00:09:19,839
число  может быть любым действительным числом,

222
00:09:19,839 --> 00:09:22,399
и, наконец, мы собираемся провести

223
00:09:22,399 --> 00:09:24,560
это число

224
00:09:24,560 --> 00:09:28,480
через логистическое преобразование того же типа,

225
00:09:28,480 --> 00:09:31,279
которое мы видели при отрицательной выборке,

226
00:09:31,279 --> 00:09:33,839
логистическое преобразование примет любое

227
00:09:33,839 --> 00:09:36,560
действительное число и преобразует его

228
00:09:36,560 --> 00:09:39,600
в вероятность того, что это  word - это

229
00:09:39,600 --> 00:09:43,120
местоположение, поэтому его выход - это прогнозируемая

230
00:09:43,120 --> 00:09:45,600
вероятность того, что слово принадлежит

231
00:09:45,600 --> 00:09:48,240
определенному классу, и поэтому это может быть

232
00:09:48,240 --> 00:09:50,880
наш классификатор местоположения, который может

233
00:09:50,880 --> 00:09:53,760
классифицировать каждое слово в окне относительно

234
00:09:53,760 --> 00:09:56,080
того, какова вероятность того, что это

235
00:09:56,080 --> 00:09:57,760
слово местоположения,

236
00:09:57,760 --> 00:10:00,640
и поэтому этот маленький  нейронная сеть

237
00:10:00,640 --> 00:10:01,440
-

238
00:10:01,440 --> 00:10:03,040
это нейронная сеть, которую я

239
00:10:03,040 --> 00:10:05,040
собираюсь использовать сегодня при выполнении некоторых математических операций,

240
00:10:06,320 --> 00:10:07,920
но на самом деле я собираюсь сделать это еще

241
00:10:07,920 --> 00:10:11,440
проще для себя, я собираюсь отбросить

242
00:10:12,959 --> 00:10:15,200
логистическую функцию вверху, и я '  Я

243
00:10:15,200 --> 00:10:16,640
действительно просто собираюсь проработать

244
00:10:16,640 --> 00:10:19,200
математику нижних трех четвертей

245
00:10:19,200 --> 00:10:22,560
этого, если вы посмотрите на раздаточный материал Кевина Кларка,

246
00:10:22,560 --> 00:10:24,640
который я только что упомянул, он

247
00:10:24,640 --> 00:10:27,120
включает, когда он работает над ним, также

248
00:10:27,120 --> 00:10:29,440
работая через логистическую

249
00:10:29,440 --> 00:10:32,240
функцию  Мы также видели работу

250
00:10:32,240 --> 00:10:34,720
через softnext в первой лекции, когда

251
00:10:34,720 --> 00:10:36,560
я работал над некоторыми из словесных

252
00:10:36,560 --> 00:10:38,399
моделей сегодня,

253
00:10:38,399 --> 00:10:39,279
хорошо,

254
00:10:40,079 --> 00:10:42,959
так что общий вопрос, на который мы хотим

255
00:10:42,959 --> 00:10:45,680
ответить, таков: вот наше

256
00:10:45,680 --> 00:10:48,240
уравнение стохастического градиентного спуска,

257
00:10:48,240 --> 00:10:52,560
которое у нас есть.  параметры

258
00:10:52,560 --> 00:10:53,680
нашей модели,

259
00:10:53,680 --> 00:10:58,320
и мы хотим обновить их на основе

260
00:10:58,320 --> 00:11:01,120
наших текущих потерь, которые находятся на j

261
00:11:01,120 --> 00:11:02,800
тета, поэтому

262
00:11:02,800 --> 00:11:06,320
для получения наших потерь um здесь

263
00:11:06,320 --> 00:11:08,399
истинный ответ относительно того, является ли слово

264
00:11:08,399 --> 00:11:11,120
местоположением или нет, будет либо вы знаете

265
00:11:11,120 --> 00:11:13,760
его, если  это местоположение или ноль, если

266
00:11:13,760 --> 00:11:16,160
это не наш логистический классификатор,

267
00:11:16,160 --> 00:11:19,680
вернет какое-то число, например um 0,9, и мы будем

268
00:11:19,680 --> 00:11:21,360
использовать расстояние, отличное от того, что оно

269
00:11:21,360 --> 00:11:24,079
должно было быть возведено в квадрат, как нашу потерю

270
00:11:24,079 --> 00:11:27,120
um, чтобы мы вычислили убыток, а затем мы '

271
00:11:27,120 --> 00:11:30,160
удалите небольшое расстояние в отрицательной

272
00:11:30,160 --> 00:11:33,279
части градиента, которое будет заключаться в

273
00:11:33,279 --> 00:11:35,519
изменении наших оценок параметров

274
00:11:35,519 --> 00:11:38,320
таким образом, чтобы они уменьшали потери,

275
00:11:38,320 --> 00:11:41,200
и поэтому это уже записывается в

276
00:11:41,200 --> 00:11:43,760
терминах всего вектора параметров,

277
00:11:43,760 --> 00:11:46,480
который обновляется как  ew

278
00:11:46,480 --> 00:11:48,880
вектор параметров, но вы также можете

279
00:11:48,880 --> 00:11:51,440
подумать о том, что для каждого отдельного

280
00:11:51,440 --> 00:11:53,680
параметра theta

281
00:11:53,680 --> 00:11:55,760
j мы вычисляем частную

282
00:11:55,760 --> 00:11:58,320
производную потерь по

283
00:11:58,320 --> 00:11:59,200
этому

284
00:11:59,200 --> 00:12:01,920
параметру, а затем мы немного движемся

285
00:12:01,920 --> 00:12:06,000
в отрицательном направлении этого  гм,

286
00:12:06,000 --> 00:12:08,959
это даст нам новое значение для

287
00:12:08,959 --> 00:12:11,760
параметра theta j,

288
00:12:11,760 --> 00:12:14,320
и мы собираемся обновить все

289
00:12:14,320 --> 00:12:16,959
параметры нашей модели, когда узнаем, что

290
00:12:16,959 --> 00:12:19,839
я имею в виду, в частности, в отличие от того, что

291
00:12:19,839 --> 00:12:24,160
обычно происходит в статистике, мы также

292
00:12:25,200 --> 00:12:26,959
обновляем не

293
00:12:26,959 --> 00:12:29,200
только сортировку  параметров нашей модели, которые

294
00:12:29,200 --> 00:12:31,279
являются своего рода весами в классификаторе,

295
00:12:31,279 --> 00:12:33,360
но мы также будем обновлять наше представление данных,

296
00:12:33,360 --> 00:12:36,160
поэтому мы также будем изменять

297
00:12:36,160 --> 00:12:38,720
наши векторы слов по мере того, как мы хорошо учимся,

298
00:12:38,720 --> 00:12:41,120
чтобы строить нейронные сети, то есть

299
00:12:41,120 --> 00:12:44,480
обучать нейронные сети на основе данных, которые мы

300
00:12:44,480 --> 00:12:47,839
необходимо иметь возможность вычислять этот

301
00:12:47,839 --> 00:12:50,320
градиент параметров, чтобы

302
00:12:50,320 --> 00:12:52,959
затем мы могли итеративно обновлять

303
00:12:52,959 --> 00:12:55,519
веса модели и эффективно обучать

304
00:12:55,519 --> 00:12:58,959
модель с хорошими весами, то есть с

305
00:12:58,959 --> 00:13:00,639
высокой точностью,

306
00:13:00,639 --> 00:13:04,880
и как мы можем  делай это хорошо

307
00:13:04,880 --> 00:13:07,200
то, о чем я собираюсь поговорить сегодня, в

308
00:13:07,200 --> 00:13:09,920
первую очередь, о том, как это можно сделать

309
00:13:09,920 --> 00:13:11,519
вручную,

310
00:13:11,519 --> 00:13:13,839
и поэтому для того, чтобы делать это вручную, это в

311
00:13:13,839 --> 00:13:18,720
основном обзор матричного исчисления,

312
00:13:18,720 --> 00:13:20,639
и это займет довольно много

313
00:13:20,639 --> 00:13:21,760
лекцию,

314
00:13:21,760 --> 00:13:23,360
а затем,

315
00:13:23,360 --> 00:13:26,000
после того, как мы поговорили об этом некоторое

316
00:13:26,000 --> 00:13:28,399
время, я затем переключу передачи и

317
00:13:28,399 --> 00:13:30,959
представлю алгоритм обратного распространения,

318
00:13:30,959 --> 00:13:33,680
который является центральной технологией

319
00:13:33,680 --> 00:13:37,360
для нейронных сетей, и эта технология,

320
00:13:37,360 --> 00:13:38,959
по сути,

321
00:13:38,959 --> 00:13:41,360
является эффективным применением вычислений

322
00:13:41,360 --> 00:13:43,680
в больших масштабах, поскольку мы  скоро мы поговорим

323
00:13:43,680 --> 00:13:45,120
о том,

324
00:13:45,120 --> 00:13:48,639
что для вычисления градиентов вручную

325
00:13:48,639 --> 00:13:52,560
мы занимаемся матричным исчислением, поэтому

326
00:13:52,560 --> 00:13:55,839
мы работаем с векторами и матрицами

327
00:13:55,839 --> 00:13:59,680
и разрабатываем градиенты, и это может

328
00:13:59,680 --> 00:14:04,079
показаться довольно пугающим

329
00:14:04,079 --> 00:14:06,959
и хорошо в той степени, в которой вы »  он

330
00:14:06,959 --> 00:14:10,560
напуган и не знает, что происходит.

331
00:14:10,560 --> 00:14:13,800
Один из вариантов -

332
00:14:13,800 --> 00:14:17,279
разработать не векторизованный градиент, просто

333
00:14:17,279 --> 00:14:20,320
выяснив, что такое частная производная для

334
00:14:20,320 --> 00:14:23,040
одного параметра за раз, и я показал

335
00:14:23,040 --> 00:14:25,279
небольшой пример этого в первой

336
00:14:25,279 --> 00:14:26,399
лекции

337
00:14:26,399 --> 00:14:27,839
бу  t

338
00:14:27,839 --> 00:14:31,440
гораздо быстрее и

339
00:14:31,440 --> 00:14:33,519
полезнее иметь возможность работать

340
00:14:33,519 --> 00:14:34,399
с

341
00:14:34,399 --> 00:14:37,199
векторизованными градиентами,

342
00:14:37,199 --> 00:14:39,839
и в некотором смысле, если вы не очень

343
00:14:39,839 --> 00:14:42,560
уверены, это своего рода

344
00:14:42,560 --> 00:14:45,600
прыжок веры, но на самом деле это тот случай, когда

345
00:14:45,600 --> 00:14:48,240
исчисление с несколькими переменными - это просто  например,

346
00:14:48,240 --> 00:14:50,720
исчисление с одной переменной, за исключением того, что вы

347
00:14:50,720 --> 00:14:53,360
используете векторы и матрицы, поэтому, если

348
00:14:53,360 --> 00:14:56,240
вы помните некоторые основы

349
00:14:56,240 --> 00:14:58,560
исчисления с одной переменной, вы действительно должны

350
00:14:58,560 --> 00:15:00,800
уметь делать это и заставлять его работать

351
00:15:00,800 --> 00:15:02,480
с

352
00:15:02,480 --> 00:15:03,920
множеством других

353
00:15:03,920 --> 00:15:06,800
источников, я уже упоминал примечания, которые

354
00:15:06,800 --> 00:15:08,399
вы также можете  посмотрите на

355
00:15:08,399 --> 00:15:11,440
учебник по математике 51 или в котором также

356
00:15:11,440 --> 00:15:14,000
довольно много материала по этому

357
00:15:15,920 --> 00:15:19,040
поводу, я знаю, что у некоторых из вас плохие воспоминания о 51-й математике. Хорошо, давайте пройдемся через

358
00:15:19,040 --> 00:15:21,440
это и посмотрим, как это работает,

359
00:15:21,440 --> 00:15:23,519
начиная с самого начала, так что

360
00:15:23,519 --> 00:15:26,320
начало исчисления - вы знаете, что у нас есть

361
00:15:26,320 --> 00:15:29,360
функция с одним входом и одним выходом,

362
00:15:29,360 --> 00:15:32,399
f x равно x в кубе, и поэтому ее

363
00:15:32,399 --> 00:15:35,120
градиент - это ее наклон вправо, так что это

364
00:15:35,120 --> 00:15:38,800
ее производная, поэтому ее производная равна

365
00:15:38,800 --> 00:15:41,199
три 3x в квадрате, и способ мыслить

366
00:15:41,199 --> 00:15:44,720
о том, насколько изменится вывод,

367
00:15:44,720 --> 00:15:47,360
если мы изменим ввод

368
00:15:47,360 --> 00:15:49,680
немного правильно, поэтому то, что мы хотим сделать в

369
00:15:49,680 --> 00:15:52,480
наших моделях нейронных сетей, - это изменить то, что

370
00:15:52,480 --> 00:15:56,000
они выводят, чтобы они

371
00:15:56,000 --> 00:15:58,240
лучше предсказывали правильные ответы, когда

372
00:15:58,240 --> 00:16:00,720
мы  мы проводим контролируемое обучение, и поэтому

373
00:16:00,720 --> 00:16:02,880
мы хотим знать, если мы будем

374
00:16:02,880 --> 00:16:05,040
изменять различные параметры модели,

375
00:16:05,040 --> 00:16:06,560
насколько сильно это повлияет

376
00:16:06,560 --> 00:16:08,560
на результат, потому что тогда мы можем выбрать,

377
00:16:08,560 --> 00:16:10,959
как правильно их изменять, чтобы

378
00:16:10,959 --> 00:16:13,839
сдвинуть вещи вниз вправо  Итак, вы знаете, когда

379
00:16:13,839 --> 00:16:16,240
мы говорим, что производная здесь равна

380
00:16:16,240 --> 00:16:18,560
3x в квадрате,

381
00:16:18,560 --> 00:16:22,000
мы говорим, что если вы находитесь в x,

382
00:16:22,000 --> 00:16:25,519
равном единице, если вы немного поиграете с вводом,

383
00:16:25,519 --> 00:16:28,160
вывод изменится в три

384
00:16:28,160 --> 00:16:30,880
раза больше в три раза  в квадрате,

385
00:16:30,880 --> 00:16:33,360
и это так, если я скажу, какое значение

386
00:16:33,360 --> 00:16:36,399
при 1,01, это примерно 1,03,

387
00:16:36,399 --> 00:16:38,160
оно изменилось в три раза больше, и

388
00:16:38,160 --> 00:16:39,839
это его наклон,

389
00:16:39,839 --> 00:16:43,279
но при x, равном 4 мкм,

390
00:16:43,279 --> 00:16:45,040
производная равна

391
00:16:45,040 --> 00:16:49,120
16 умножить на 348, поэтому, если мы немного поиграем с вводом,

392
00:16:49,120 --> 00:16:51,759
оно изменится  В 48 раз больше, и

393
00:16:51,759 --> 00:16:54,240
это примерно  то, что происходит, 4,01 в кубе

394
00:16:54,240 --> 00:16:55,480
- это

395
00:16:55,480 --> 00:16:58,320
64,48, теперь, конечно, вы знаете, что это

396
00:16:58,320 --> 00:17:00,639
просто своего рода, я показываю это для небольшой

397
00:17:00,639 --> 00:17:02,880
скрипки, но вы знаете, что это

398
00:17:02,880 --> 00:17:05,839
приближение к реальной истине,

399
00:17:05,839 --> 00:17:09,280
хорошо, тогда мы как бы переходим

400
00:17:09,280 --> 00:17:11,359
к более сложным случаям  которые больше

401
00:17:11,359 --> 00:17:13,039
отражают то, что мы делаем с нейронными

402
00:17:13,039 --> 00:17:16,240
сетями, поэтому, если у нас есть функция

403
00:17:16,240 --> 00:17:19,520
с одним выходом и n входами, тогда у

404
00:17:19,520 --> 00:17:22,000
нас есть градиент, поэтому градиент - это

405
00:17:22,000 --> 00:17:24,559
вектор частных производных

406
00:17:24,559 --> 00:17:26,880
по каждому входу, поэтому у нас есть n

407
00:17:26,880 --> 00:17:29,440
входов  от x1 до xn, и мы

408
00:17:29,440 --> 00:17:31,520
вычисляем частную производную f

409
00:17:31,520 --> 00:17:33,840
относительно x1, частную производную f

410
00:17:33,840 --> 00:17:37,520
относительно ожидаемой x2 и т. д., а затем мы

411
00:17:37,520 --> 00:17:41,520
получаем вектор частных производных,

412
00:17:41,520 --> 00:17:43,919
где каждый элемент этого вектора

413
00:17:43,919 --> 00:17:46,559
похож на простую производную

414
00:17:46,559 --> 00:17:49,360
относительно одной переменной,

415
00:17:49,360 --> 00:17:52,160
хорошо, так что с этого момента мы просто продолжаем

416
00:17:52,160 --> 00:17:54,960
наращивать то, что мы делаем с нейронными

417
00:17:54,960 --> 00:17:58,000
сетями, так что обычно, когда у нас есть

418
00:17:58,000 --> 00:17:59,840
что-то вроде слоя в нейронной

419
00:17:59,840 --> 00:18:03,280
сети, у нас будет функция внутри

420
00:18:03,280 --> 00:18:05,919
входов, они будут похожи на наши  слово ве  ctors,

421
00:18:05,919 --> 00:18:06,960
затем мы

422
00:18:06,960 --> 00:18:10,480
выполняем что-то вроде умножения на матрицу,

423
00:18:10,480 --> 00:18:13,039
и тогда у нас будет m выходов, так что теперь у нас

424
00:18:13,039 --> 00:18:15,919
есть функция, которая принимает n входов

425
00:18:15,919 --> 00:18:18,880
и производит m выходов,

426
00:18:18,880 --> 00:18:21,840
поэтому в этот момент мы

427
00:18:21,840 --> 00:18:24,880
вычисляем для градиента то, что

428
00:18:24,880 --> 00:18:28,880
называется  матрица якобиана, поэтому для m входов

429
00:18:28,880 --> 00:18:32,000
и n выходов якобиан представляет собой матрицу m на n

430
00:18:32,000 --> 00:18:35,039
частей каждой

431
00:18:35,039 --> 00:18:38,960
комбинации частных производных, поэтому функция um i

432
00:18:38,960 --> 00:18:41,760
f разделяется на эти

433
00:18:41,760 --> 00:18:45,360
различные подфункции с f1 по m

434
00:18:45,360 --> 00:18:48,640
fm, которые генерируют каждый из m выходов

435
00:18:48,640 --> 00:18:50,720
и  Итак, мы берем частную

436
00:18:50,720 --> 00:18:53,919
производную от f1 по x1

437
00:18:53,919 --> 00:18:55,760
через частную производную от

438
00:18:55,760 --> 00:18:57,679
f1 по xn, а

439
00:18:57,679 --> 00:18:59,600
затем идем вниз, как вы знаете, мы делаем это

440
00:18:59,600 --> 00:19:01,840
до частной производной от fm

441
00:19:01,840 --> 00:19:04,559
по x1 и т. д., поэтому у нас есть все

442
00:19:04,559 --> 00:19:07,919
возможные  частная производная выходной

443
00:19:07,919 --> 00:19:09,919
переменной по одной из

444
00:19:09,919 --> 00:19:12,880
входных переменных в

445
00:19:13,200 --> 00:19:14,720
порядке,

446
00:19:14,720 --> 00:19:18,000
поэтому в простом исчислении,

447
00:19:18,000 --> 00:19:20,640
когда у вас есть композиция из

448
00:19:20,640 --> 00:19:23,760
функций одной переменной, так что если у вас

449
00:19:23,760 --> 00:19:25,039
um

450
00:19:25,039 --> 00:19:27,760
y равно x в квадрате, а затем z равно

451
00:19:27,760 --> 00:19:29,200
th  ree y

452
00:19:29,200 --> 00:19:31,919
um, тогда z -

453
00:19:31,919 --> 00:19:35,039
это композиция из двух функций,

454
00:19:35,039 --> 00:19:37,360
а вы составляете две функции,

455
00:19:37,360 --> 00:19:40,000
чтобы получить z как функцию от x, тогда вы можете

456
00:19:40,000 --> 00:19:42,320
вычислить производную z

457
00:19:42,320 --> 00:19:44,880
по x, и то, как вы это делаете,

458
00:19:44,880 --> 00:19:47,280
с помощью  цепное правило, и поэтому в цепном

459
00:19:47,280 --> 00:19:50,240
правиле вы умножаете производные, так что

460
00:19:50,240 --> 00:19:54,640
dz dx равно dz dy, умноженное на dydx,

461
00:19:54,640 --> 00:19:55,400
поэтому

462
00:19:55,400 --> 00:19:57,360
dzy

463
00:19:57,360 --> 00:19:58,960
равно 3,

464
00:19:58,960 --> 00:20:04,640
а dydx равно 2x, поэтому мы получаем 3 раза 2x, так

465
00:20:04,640 --> 00:20:06,880
что общая um

466
00:20:07,600 --> 00:20:11,440
производная здесь равна 6x, и

467
00:20:11,440 --> 00:20:13,600
поскольку если мы умножим это вместе  мы

468
00:20:13,600 --> 00:20:16,640
действительно говорим, что z равно 3x в квадрате

469
00:20:16,640 --> 00:20:19,039
um, вы должны тривиально снова увидеть,

470
00:20:19,039 --> 00:20:20,799
aha

471
00:20:20,799 --> 00:20:24,320
его производная равна 6x, так что это работает нормально,

472
00:20:24,320 --> 00:20:27,120
так что, как только мы перейдем к векторам,

473
00:20:27,120 --> 00:20:30,799
матрицам и якобианам, это на самом деле

474
00:20:30,799 --> 00:20:33,440
та же игра, поэтому, когда мы  работая с ними,

475
00:20:33,440 --> 00:20:36,159
мы можем составлять функции

476
00:20:36,159 --> 00:20:38,159
и вычислять их производные, просто

477
00:20:38,159 --> 00:20:41,520
умножая якобианы, поэтому, если у нас

478
00:20:41,520 --> 00:20:44,400
есть вход x, а затем пропустить его

479
00:20:45,360 --> 00:20:47,360
через простейшую форму слоя нейронной сети

480
00:20:47,360 --> 00:20:49,600
и сказать, что z

481
00:20:49,600 --> 00:20:52,880
равно wx плюс b, поэтому мы умножаем это

482
00:20:52,880 --> 00:20:55,600
вектор x по m  atrix w, а затем добавляем

483
00:20:55,600 --> 00:20:57,039
вектор смещения b,

484
00:20:57,039 --> 00:20:58,640
а затем обычно мы

485
00:20:58,640 --> 00:21:00,799
пропускаем вещи через нелинейность

486
00:21:00,799 --> 00:21:03,840
f, чтобы f могла быть сигмоидной функцией, мы

487
00:21:03,840 --> 00:21:06,480
затем говорим, что h равно f от z, так что это

488
00:21:06,480 --> 00:21:10,320
композиция двух функций в  термы

489
00:21:10,320 --> 00:21:13,039
векторов и матриц um,

490
00:21:13,039 --> 00:21:15,280
поэтому мы можем использовать якобианы, и мы можем сказать,

491
00:21:15,280 --> 00:21:18,640
что частичное число h относительно x

492
00:21:18,640 --> 00:21:22,159
будет произведением um

493
00:21:22,159 --> 00:21:24,080
частичного числа h относительно z и

494
00:21:24,080 --> 00:21:27,440
частичного нуля относительно x,

495
00:21:27,440 --> 00:21:30,799
и это  все действительно работает, поэтому давайте

496
00:21:30,799 --> 00:21:32,799
начнем с

497
00:21:32,799 --> 00:21:35,520
некоторых примеров того, как эти вещи работают

498
00:21:35,520 --> 00:21:37,919
немного более конкретно,

499
00:21:37,919 --> 00:21:38,799
сначала

500
00:21:38,799 --> 00:21:42,240
только с конкретными якобианами, а затем

501
00:21:42,240 --> 00:21:45,679
объединяем их вместе, поэтому один случай, который мы

502
00:21:45,679 --> 00:21:47,559
рассмотрим, - это

503
00:21:47,559 --> 00:21:50,240
нелинейности, которые мы пропускаем через вектор,

504
00:21:50,240 --> 00:21:52,080
так что это что-то вроде

505
00:21:52,080 --> 00:21:55,039
пропускаем вектор через сигмовидную

506
00:21:55,039 --> 00:21:56,559
функцию f

507
00:21:56,559 --> 00:21:58,720
um, и поэтому, если

508
00:21:58,720 --> 00:22:01,039
у нас есть промежуточный вектор z, и мы

509
00:22:01,039 --> 00:22:03,520
превращаем его в вектор h, передавая его через

510
00:22:03,520 --> 00:22:07,200
нас логистической функцией, мы можем сказать,

511
00:22:07,200 --> 00:22:09,320
что такое

512
00:22:09,320 --> 00:22:12,080
dhdz um

513
00:22:12,080 --> 00:22:13,200
хорошо

514
00:22:13,200 --> 00:22:14,480
для этого

515
00:22:14,480 --> 00:22:15,360
um

516
00:22:15,360 --> 00:22:19,440
формально это функция, которая имеет  п

517
00:22:19,440 --> 00:22:22,240
в  помещает и n выходов, поэтому в

518
00:22:22,240 --> 00:22:23,360
конце дня

519
00:22:23,360 --> 00:22:27,600
мы вычисляем якобиан n на n, и

520
00:22:28,400 --> 00:22:31,039
это означает, что элементы

521
00:22:31,039 --> 00:22:35,039
этого якобиана n на n будут

522
00:22:35,039 --> 00:22:37,520
брать частную производную каждого

523
00:22:37,520 --> 00:22:42,480
вывода по каждому входу

524
00:22:42,559 --> 00:22:44,559
и  хорошо, что это будет в

525
00:22:44,559 --> 00:22:45,760
этом случае

526
00:22:45,760 --> 00:22:49,039
хорошо в этом случае, потому что мы фактически

527
00:22:49,039 --> 00:22:52,720
просто вычисляем

528
00:22:52,720 --> 00:22:55,600
поэлементное преобразование, такое как логистическое

529
00:22:55,600 --> 00:22:58,880
преобразование каждого элемента zi,

530
00:22:58,880 --> 00:23:01,200
как второе уравнение здесь,

531
00:23:01,200 --> 00:23:02,480
если

532
00:23:02,480 --> 00:23:04,960
i равно j, у нас есть что

533
00:23:04,960 --> 00:23:08,400
вычислить  тогда как если i не равно j

534
00:23:08,400 --> 00:23:10,080
um, то

535
00:23:10,080 --> 00:23:12,960
просто ввод не влияет на вывод,

536
00:23:12,960 --> 00:23:15,760
поэтому производная от нуля, поэтому, если i

537
00:23:15,760 --> 00:23:17,760
не равно j, мы получим

538
00:23:17,760 --> 00:23:21,120
ноль, и если i действительно равно j, что тогда

539
00:23:21,120 --> 00:23:23,120
мы  собираемся получить обычную

540
00:23:24,400 --> 00:23:27,200
производную логистической функции с одной переменной,

541
00:23:27,200 --> 00:23:30,960
которую, если я правильно помню,

542
00:23:30,960 --> 00:23:32,400
вас попросили

543
00:23:32,400 --> 00:23:33,600
вычислить

544
00:23:33,600 --> 00:23:35,200
сейчас, я не могу вспомнить, что ей назначен один

545
00:23:35,200 --> 00:23:36,720
или два назначения, но один из двух

546
00:23:36,720 --> 00:23:39,760
просит вас вычислить его, поэтому наш

547
00:23:39,760 --> 00:23:42,320
якобиан в этом случае выглядит как  s

548
00:23:42,320 --> 00:23:46,480
у нас есть диагональная матрица, где

549
00:23:46,480 --> 00:23:48,000
um -

550
00:23:48,000 --> 00:23:50,640
производные каждого элемента по

551
00:23:50,640 --> 00:23:54,400
диагонали, а все остальное - ноль,

552
00:23:54,400 --> 00:23:56,720
хорошо, поэтому давайте посмотрим на пару других

553
00:23:56,720 --> 00:24:00,400
якобианов.

554
00:24:03,600 --> 00:24:05,760
слой, и мы

555
00:24:05,760 --> 00:24:08,799
просим um о градиенте

556
00:24:08,799 --> 00:24:10,799
относительно x,

557
00:24:10,799 --> 00:24:14,000
тогда то, что мы собираемся сделать,

558
00:24:14,000 --> 00:24:15,360
это то, что на самом деле это будет

559
00:24:15,360 --> 00:24:18,480
матрица w, так

560
00:24:18,480 --> 00:24:20,320
что именно здесь, я надеюсь, вы можете сделать,

561
00:24:20,320 --> 00:24:21,840
это

562
00:24:21,840 --> 00:24:24,799
посмотреть примечания на  домой и поработайте

563
00:24:24,799 --> 00:24:25,040
через

564
00:24:25,040 --> 00:24:26,880
[Music]

565
00:24:26,880 --> 00:24:29,440
именно это и убедитесь, что это на

566
00:24:29,440 --> 00:24:32,640
самом деле правильный ответ, но это

567
00:24:32,640 --> 00:24:35,919
тот способ, которым, если вы просто верите

568
00:24:35,919 --> 00:24:38,240
и думаете, что это то же самое, что и

569
00:24:38,240 --> 00:24:40,480
исчисление с одной переменной,

570
00:24:40,480 --> 00:24:43,360
за исключением того, что теперь у меня есть векторы и матрицы,

571
00:24:43,360 --> 00:24:44,960
ответ, который вы получите, на самом деле то, что вы

572
00:24:44,960 --> 00:24:46,880
ожидали получить, потому что это точно так же,

573
00:24:46,880 --> 00:24:48,720
как

574
00:24:48,720 --> 00:24:51,760
um производная от ax плюс b

575
00:24:51,760 --> 00:24:54,159
по x, где это

576
00:24:54,159 --> 00:24:56,159
очень похоже, если мы берем частную

577
00:24:56,159 --> 00:24:59,120
производную по b

578
00:24:59,120 --> 00:25:02,480
от wx плюс b, мы получаем тождество

579
00:25:02,480 --> 00:25:04,799
матрица

580
00:25:04,799 --> 00:25:07,919
хорошо тогда еще один  ээ, якобиан, о котором мы

581
00:25:07,919 --> 00:25:10,799
упоминали um в первой лекции,

582
00:25:10,799 --> 00:25:13,200
работая с word to vac, - это если у

583
00:25:13,200 --> 00:25:18,000
вас есть скалярное произведение двух

584
00:25:18,799 --> 00:25:20,080
векторов um

585
00:25:20,080 --> 00:25:23,520
i, это число, которое вы

586
00:25:23,520 --> 00:25:26,480
получаете из этого, так

587
00:25:26,480 --> 00:25:28,640
что частная

588
00:25:28,640 --> 00:25:31,279
производная uth по отношению к  to u

589
00:25:31,279 --> 00:25:33,440
- это h транспонирование,

590
00:25:33,440 --> 00:25:35,520
и здесь есть мелкий

591
00:25:35,520 --> 00:25:38,640
шрифт, к которому я вернусь

592
00:25:38,640 --> 00:25:39,600
через

593
00:25:39,600 --> 00:25:40,799
минуту,

594
00:25:40,799 --> 00:25:43,200
так что это правильный якобиан,

595
00:25:43,200 --> 00:25:46,720
потому что в этом случае у нас

596
00:25:46,720 --> 00:25:48,320
есть размер

597
00:25:49,360 --> 00:25:52,720
h входов, и у нас есть один выход

598
00:25:52,720 --> 00:25:54,720
и  Итак, мы хотим иметь

599
00:25:54,720 --> 00:25:56,640
вектор-строку

600
00:25:56,640 --> 00:25:57,440
ммм,

601
00:25:57,440 --> 00:25:59,279
но есть немного больше, чтобы сказать по

602
00:25:59,279 --> 00:26:01,520
этому поводу, к которому я вернусь примерно через 20

603
00:26:01,520 --> 00:26:04,080
слайдов, но это правильный

604
00:26:04,080 --> 00:26:06,000
якобианец,

605
00:26:06,000 --> 00:26:07,440
хорошо, поэтому,

606
00:26:07,440 --> 00:26:09,520
если вы не знакомы с

607
00:26:09,520 --> 00:26:12,000
такими якобианами, сделайте  пожалуйста,

608
00:26:12,000 --> 00:26:14,320
просмотрите некоторые из

609
00:26:14,320 --> 00:26:16,960
доступных примечаний и попробуйте

610
00:26:16,960 --> 00:26:18,960
вычислить их более подробно по

611
00:26:18,960 --> 00:26:21,120
элементам и убедитесь, что они

612
00:26:21,120 --> 00:26:23,520
действительно правы, но я собираюсь принять

613
00:26:23,520 --> 00:26:26,559
их сейчас и показать вам, что происходит, когда

614
00:26:26,559 --> 00:26:29,520
мы на самом деле затем разрабатываем градиенты для

615
00:26:29,520 --> 00:26:33,279
по крайней мере, небольшая небольшая нейронная сеть,

616
00:26:35,520 --> 00:26:37,600
хорошо,

617
00:26:37,600 --> 00:26:42,400
так что вот большая часть этой нейронной сети, я

618
00:26:42,400 --> 00:26:45,520
имею в виду, когда я прокомментировал,

619
00:26:45,520 --> 00:26:48,400
что вы действительно знаете, что мы будем

620
00:26:48,400 --> 00:26:51,039
вычислять частную производную потерь

621
00:26:51,039 --> 00:26:53,600
j по этим переменным, но

622
00:26:53,600 --> 00:26:56,320
для примера i  я делаю здесь, я просто

623
00:26:56,320 --> 00:26:58,080
заблокировал это, чтобы сделать его немного

624
00:26:58,080 --> 00:26:59,679
проще и удобнее для

625
00:26:59,679 --> 00:27:02,400
лекции, и поэтому мы собираемся

626
00:27:02,400 --> 00:27:05,039
просто вычислить частную производную оценки

627
00:27:05,039 --> 00:27:07,760
s, которая является действительным числом по отношению

628
00:27:07,760 --> 00:27:10,799
к  различные параметры этой модели,

629
00:27:10,799 --> 00:27:13,200
где параметрами этой модели

630
00:27:13,200 --> 00:27:16,480
будут w, b

631
00:27:16,480 --> 00:27:18,320
и

632
00:27:18,320 --> 00:27:19,360
u,

633
00:27:19,360 --> 00:27:20,399
а

634
00:27:20,399 --> 00:27:23,440
также входные данные, потому что мы можем обновлять

635
00:27:23,440 --> 00:27:25,679
весовые векторы

636
00:27:26,399 --> 00:27:28,480
векторов слов различных слов

637
00:27:28,480 --> 00:27:29,760
на основе

638
00:27:29,760 --> 00:27:32,080
их настройки, чтобы лучше предсказать um

639
00:27:32,080 --> 00:27:34,960
результаты классификации, которые мы желаем,

640
00:27:34,960 --> 00:27:37,039
поэтому давайте начнем с довольно простого,

641
00:27:37,039 --> 00:27:40,799
где мы хотим обновить вектор смещения

642
00:27:40,799 --> 00:27:42,080
b,

643
00:27:42,080 --> 00:27:45,760
чтобы наша система лучше классифицировала,

644
00:27:45,760 --> 00:27:47,520
чтобы иметь возможность делать то, что мы хотим

645
00:27:47,520 --> 00:27:50,640
решить, - это частные производные от  s

646
00:27:50,640 --> 00:27:53,039
с  относительно b,

647
00:27:53,039 --> 00:27:55,039
поэтому мы знаем, как поместить это в наше

648
00:27:56,720 --> 00:27:57,840
обновление стохастического градиента

649
00:27:57,840 --> 00:28:00,240
для параметров b,

650
00:28:00,240 --> 00:28:02,720
хорошо, так как нам делать эти

651
00:28:02,720 --> 00:28:06,320
вещи, так что первым шагом мы хотим

652
00:28:06,320 --> 00:28:08,799
разбить вещи

653
00:28:08,799 --> 00:28:11,600
на разные функции минимальной

654
00:28:11,600 --> 00:28:14,720
сложности, которые составляют  вместе, поэтому, в

655
00:28:14,720 --> 00:28:17,120
частности, этот слой нейронной сети h

656
00:28:17,120 --> 00:28:19,840
равен f из wx плюс b, он все еще

657
00:28:19,840 --> 00:28:22,480
немного сложен, поэтому давайте разложим

658
00:28:22,480 --> 00:28:26,240
этот еще один шаг, чтобы у нас был

659
00:28:26,240 --> 00:28:27,679
вход x,

660
00:28:27,679 --> 00:28:29,200
мы затем

661
00:28:30,399 --> 00:28:33,679
вычисляем линейное преобразование z, равное wx

662
00:28:33,679 --> 00:28:35,600
плюс b,

663
00:28:35,600 --> 00:28:37,200
а затем

664
00:28:37,200 --> 00:28:39,279
um мы  пройти

665
00:28:39,279 --> 00:28:42,960
через вид поэлементной нелинейности

666
00:28:42,960 --> 00:28:44,880
h равняется f от z,

667
00:28:44,880 --> 00:28:49,279
а затем мы делаем скалярное произведение с u,

668
00:28:49,279 --> 00:28:52,320
и вы знаете, что это полезно для решения

669
00:28:52,320 --> 00:28:53,840
этих вещей,

670
00:28:53,840 --> 00:28:57,360
вы знаете, разбить на части, как это,

671
00:28:57,360 --> 00:28:58,720
прямо, какие у вас разные

672
00:28:58,720 --> 00:29:00,880
переменные  и чтобы знать

673
00:29:00,880 --> 00:29:03,679
размерность каждой из этих

674
00:29:03,679 --> 00:29:05,760
переменных, стоит просто

675
00:29:05,760 --> 00:29:07,600
записать размерность каждой

676
00:29:07,600 --> 00:29:09,520
переменной и убедиться, что

677
00:29:09,520 --> 00:29:11,679
ответы, которые вы вычисляете, являются

678
00:29:11,679 --> 00:29:13,919
правильными.  t,

679
00:29:13,919 --> 00:29:16,799
поэтому на данный момент мы можем видеть,

680
00:29:16,799 --> 00:29:18,960
что вычисление s

681
00:29:18,960 --> 00:29:21,600
является произведением

682
00:29:21,600 --> 00:29:22,799
трех

683
00:29:22,799 --> 00:29:25,200
извините, это композиция трех

684
00:29:25,200 --> 00:29:29,600
функций вокруг x, поэтому для

685
00:29:29,600 --> 00:29:33,600
вычисления частичных значений s относительно b

686
00:29:33,600 --> 00:29:36,640
um это композиция трех

687
00:29:36,640 --> 00:29:39,760
показанных функций  слева и,

688
00:29:39,760 --> 00:29:41,039
следовательно,

689
00:29:41,039 --> 00:29:43,919
градиент s по отношению к b,

690
00:29:43,919 --> 00:29:46,799
мы собираемся взять um

691
00:29:46,799 --> 00:29:48,559
произведение

692
00:29:48,559 --> 00:29:49,600
этих

693
00:29:49,600 --> 00:29:53,600
трех частных производных,

694
00:29:53,840 --> 00:29:57,600
хорошо, так как же мы это делаем,

695
00:29:57,600 --> 00:30:00,640
чтобы вы знали, что у нас s равно

696
00:30:00,640 --> 00:30:04,000
uth, поэтому  это своего рода верхняя um

697
00:30:04,000 --> 00:30:06,640
соответствующая частная производная частная

698
00:30:06,640 --> 00:30:08,880
производная h относительно z

699
00:30:08,880 --> 00:30:11,919
частная производная z относительно um

700
00:30:11,919 --> 00:30:14,720
к b, которая является первой, над которой мы работаем

701
00:30:16,000 --> 00:30:19,120
нормально, поэтому мы хотим решить эту проблему, и если

702
00:30:19,120 --> 00:30:21,679
мы '  Нам повезло, что мы помним те якобианы, которые

703
00:30:21,679 --> 00:30:25,200
я показал ранее о якобиане

704
00:30:25,200 --> 00:30:28,080
для векторного скалярного произведения,

705
00:30:28,080 --> 00:30:29,679
якобиане

706
00:30:29,679 --> 00:30:33,360
для нелинейности и якобиане

707
00:30:33,360 --> 00:30:36,399
для простого линейного преобразования, и

708
00:30:36,399 --> 00:30:39,279
поэтому мы можем использовать их так

709
00:30:40,159 --> 00:30:42,480
для частичных чисел s

710
00:30:42,480 --> 00:30:45,039
с res  pct to h

711
00:30:45,039 --> 00:30:47,679
хорошо, что будет использовать

712
00:30:49,440 --> 00:30:52,880
первую часть h по отношению к z, хорошо,

713
00:30:52,880 --> 00:30:55,279
так что это нелинейность, и это

714
00:30:57,039 --> 00:30:59,679
будет матрица, которая является диагональной матрицей

715
00:30:59,679 --> 00:31:02,799
с поэлементной производной f,

716
00:31:02,799 --> 00:31:06,799
простым числом z и  0 в другом месте, а

717
00:31:06,799 --> 00:31:09,519
затем для wx плюс b,

718
00:31:09,519 --> 00:31:11,600
когда мы берем частичные по

719
00:31:11,600 --> 00:31:13,039
отношению к b,

720
00:31:13,039 --> 00:31:15,600
это просто единичная матрица,

721
00:31:15,600 --> 00:31:17,840
поэтому мы можем немного упростить это,

722
00:31:17,840 --> 00:31:20,559
единичная матрица исчезает,

723
00:31:20,559 --> 00:31:21,600
и

724
00:31:21,600 --> 00:31:23,919
поскольку

725
00:31:24,320 --> 00:31:25,919
ut

726
00:31:25,919 --> 00:31:28,960
- вектор, а это диагональная

727
00:31:28,960 --> 00:31:32,799
матрица, мы  можно переписать это как ut

728
00:31:32,799 --> 00:31:36,159
hadamard, продукт f prime of zi, думаю,

729
00:31:36,159 --> 00:31:39,200
это первый раз, когда я использовал этот

730
00:31:39,200 --> 00:31:41,360
маленький кружок для продукта hadamard, но

731
00:31:41,360 --> 00:31:43,519
это то, что вы

732
00:31:43,519 --> 00:31:46,240
увидите довольно часто в своей сетевой работе,

733
00:31:46,240 --> 00:31:49,440
так как он часто используется, поэтому, когда

734
00:31:49,440 --> 00:31:52,880
у нас есть два вектора

735
00:31:53,200 --> 00:31:56,399
ut, и этот вектор здесь иногда вы

736
00:31:56,399 --> 00:31:57,840
хотите сделать

737
00:31:57,840 --> 00:32:00,080
поэлементное произведение, поэтому выходом

738
00:32:00,080 --> 00:32:02,000
этого будет вектор, в котором вы

739
00:32:02,000 --> 00:32:04,000
взяли первый элемент каждого и

740
00:32:04,000 --> 00:32:05,519
затем умножили второй элемент каждого и

741
00:32:05,519 --> 00:32:08,559
умножили t  край и т. д. вниз, и поэтому

742
00:32:08,559 --> 00:32:10,559
это называется произведением Хадамара, и

743
00:32:10,559 --> 00:32:14,240
это то, что мы вычисляем,

744
00:32:14,240 --> 00:32:16,480
чтобы вычислить вектор,

745
00:32:16,480 --> 00:32:18,960
который является градиентом

746
00:32:18,960 --> 00:32:22,240
s относительно b,

747
00:32:22,799 --> 00:32:24,880
хорошо,

748
00:32:24,880 --> 00:32:28,240
это хорошо, поэтому теперь у нас есть градиент

749
00:32:28,240 --> 00:32:30,559
s относительно b, и мы  мы могли бы использовать

750
00:32:30,559 --> 00:32:33,440
это в нашем стохастическом градиенте, но мы

751
00:32:33,440 --> 00:32:35,039
не останавливаемся на этом,

752
00:32:35,039 --> 00:32:39,440
мы также хотим разработать градиент

753
00:32:39,440 --> 00:32:42,399
по отношению к другим нашим параметрам,

754
00:32:42,399 --> 00:32:45,279
поэтому мы можем продолжить

755
00:32:45,279 --> 00:32:48,159
и разработать градиент s по

756
00:32:48,159 --> 00:32:50,880
отношению к тому, что

757
00:32:50,880 --> 00:32:54,880
мы  может использовать цепное правило так же, как

758
00:32:54,880 --> 00:32:57,760
мы делали это раньше, так что у нас есть тот

759
00:32:57,760 --> 00:33:00,880
же продукт функций, и

760
00:33:00,880 --> 00:33:03,200
все будет таким же, кроме меня, который

761
00:33:03,200 --> 00:33:04,880
теперь берет

762
00:33:04,880 --> 00:33:07,440
производные по w,

763
00:33:07,440 --> 00:33:08,880
а не b

764
00:33:08,880 --> 00:33:12,799
um, так что теперь он будет  быть

765
00:33:12,799 --> 00:33:15,120
um частью s по отношению к hh по

766
00:33:15,120 --> 00:33:19,120
отношению к z и z по отношению к w,

767
00:33:19,120 --> 00:33:22,960
и здесь важно отметить,

768
00:33:22,960 --> 00:33:25,600
и это приводит к тому, что мы делаем с

769
00:33:25,600 --> 00:33:28,960
алгоритмом обратного распространения, подождите

770
00:33:28,960 --> 00:33:29,919
минуту,

771
00:33:29,919 --> 00:33:32,880
это очень похоже на то, что мы '  я

772
00:33:32,880 --> 00:33:35,679
уже сделал это, когда  мы

773
00:33:35,679 --> 00:33:38,399
разрабатываем градиенты s по отношению к b,

774
00:33:38,399 --> 00:33:41,360
первые два члена были точно такими

775
00:33:41,360 --> 00:33:45,360
же, только последний из них отличается,

776
00:33:45,360 --> 00:33:46,320
поэтому,

777
00:33:46,320 --> 00:33:50,720
чтобы иметь возможность построить um или обучить

778
00:33:50,720 --> 00:33:52,960
нейронные сети эффективно,

779
00:33:52,960 --> 00:33:55,440
это то, что происходит постоянно, и

780
00:33:55,440 --> 00:33:58,320
это  абсолютно важно, чтобы мы использовали

781
00:33:58,320 --> 00:34:01,120
алгоритм, который избегает повторных

782
00:34:01,120 --> 00:34:02,720
вычислений,

783
00:34:02,720 --> 00:34:03,679
и

784
00:34:03,679 --> 00:34:06,480
поэтому идея, которую мы собираемся разработать, заключается в том, что когда

785
00:34:06,480 --> 00:34:09,199
у нас есть стек уравнений, в котором есть что-

786
00:34:09,199 --> 00:34:11,359
то, что выше

787
00:34:11,359 --> 00:34:13,599
um, где мы вычисляем z,

788
00:34:13,599 --> 00:34:15,520
и мы собираемся быть чем-то вроде этого  будет каждый раз одним и

789
00:34:15,520 --> 00:34:17,839
тем же, и мы хотим

790
00:34:17,839 --> 00:34:20,800
вычислить что-то из этого, что мы можем

791
00:34:20,800 --> 00:34:24,239
затем как бы направить вниз при

792
00:34:24,239 --> 00:34:28,800
разработке градиентов относительно wx или

793
00:34:28,800 --> 00:34:30,000
b,

794
00:34:30,000 --> 00:34:31,679
и поэтому

795
00:34:31,679 --> 00:34:35,359
мы делаем это, определяя delta, которая является

796
00:34:35,359 --> 00:34:38,879
delta - это составные части  находятся

797
00:34:40,320 --> 00:34:43,440
над линейным преобразованием, и это

798
00:34:43,440 --> 00:34:45,679
называется локальным сигналом ошибки, это то,

799
00:34:45,679 --> 00:34:48,399
что передается сверху

800
00:34:48,399 --> 00:34:50,560
в линейное преобразование, и мы

801
00:34:50,560 --> 00:34:53,280
уже

802
00:34:53,280 --> 00:34:55,760
вычислили градиент этого на предыдущих

803
00:34:55,760 --> 00:34:59,520
слайдах, поэтому конечный результат  l форма

804
00:34:59,520 --> 00:35:03,359
частичного s относительно b будет

805
00:35:03,359 --> 00:35:05,440
um

806
00:35:05,440 --> 00:35:06,560
дельта,

807
00:35:06,560 --> 00:35:10,400
умноженная на оставшуюся часть, и мы

808
00:35:10,400 --> 00:35:12,560
видели, что вы знаете, что для

809
00:35:12,560 --> 00:35:14,880
um partial of s относительно b

810
00:35:14,880 --> 00:35:17,520
um частичное z относительно b является

811
00:35:17,520 --> 00:35:19,760
просто тождеством  итак, конечным результатом была

812
00:35:19,760 --> 00:35:22,400
дельта, но на этот раз

813
00:35:22,400 --> 00:35:24,800
нам нужно будет вычислить частичную z

814
00:35:24,800 --> 00:35:27,760
относительно w и умножить ее на

815
00:35:27,760 --> 00:35:31,119
дельту, так что это та часть, которую мы

816
00:35:31,119 --> 00:35:32,880
еще не сделали,

817
00:35:34,960 --> 00:35:37,200
а это  где все становится

818
00:35:37,200 --> 00:35:42,640
в некотором смысле немного сложнее,

819
00:35:42,640 --> 00:35:44,560
и поэтому есть кое-что, что

820
00:35:44,560 --> 00:35:47,680
важно объяснить, чтобы

821
00:35:47,680 --> 00:35:49,359
вы знали, что у нас должно быть для

822
00:35:49,359 --> 00:35:51,760
якобиана

823
00:35:51,760 --> 00:35:54,400
um dsdw

824
00:35:54,400 --> 00:35:55,440
хорошо,

825
00:35:56,800 --> 00:35:59,200
что функция,

826
00:35:59,200 --> 00:36:02,480
которая имеет один выход, выход - это просто

827
00:36:02,480 --> 00:36:05,680
оценка действительного числа и  тогда он имеет n на

828
00:36:05,680 --> 00:36:07,839
m входов, так

829
00:36:07,839 --> 00:36:09,839
что якобиан -

830
00:36:09,839 --> 00:36:12,480
это матрица размером 1 на n

831
00:36:12,480 --> 00:36:15,200
на m с

832
00:36:15,200 --> 00:36:18,160
очень длинным вектором-строкой,

833
00:36:18,160 --> 00:36:19,359
но

834
00:36:19,359 --> 00:36:22,720
это правильная математика, но оказывается,

835
00:36:22,720 --> 00:36:25,280
что это плохо для наших нейронных

836
00:36:25,280 --> 00:36:26,560
сетей,

837
00:36:26,560 --> 00:36:28,240
потому что помните, что мы хотим делать с

838
00:36:28,240 --> 00:36:30,160
наши нейронные сети делают

839
00:36:30,160 --> 00:36:32,480
стохастический градиент де  запах, и мы

840
00:36:32,480 --> 00:36:36,240
хотим сказать, что тэта-ню равно тэта-старому

841
00:36:37,680 --> 00:36:40,400
минус небольшой множитель, умноженный

842
00:36:40,400 --> 00:36:42,720
на градиент, и на

843
00:36:42,720 --> 00:36:44,000
самом

844
00:36:44,000 --> 00:36:47,280
деле матрица w

845
00:36:48,480 --> 00:36:52,880
представляет собой матрицу n на m,

846
00:36:52,960 --> 00:36:54,800
и поэтому мы не могли бы фактически выполнить

847
00:36:54,800 --> 00:36:57,440
вычитание, если этот градиент, который мы

848
00:36:57,440 --> 00:37:01,040
вычисляем, представляет собой просто огромную строку  вектор, мы бы

849
00:37:01,040 --> 00:37:04,079
хотели, чтобы он имел ту же форму,

850
00:37:04,079 --> 00:37:06,320
что и матрица w

851
00:37:06,320 --> 00:37:08,880
в нейронной сети, когда мы это делаем,

852
00:37:08,880 --> 00:37:11,760
мы отходим от чистой математики в этом

853
00:37:11,760 --> 00:37:14,400
месте и используем то, что мы называем соглашением о форме,

854
00:37:15,680 --> 00:37:19,200
так что мы собираемся сказать  это гм, и

855
00:37:19,200 --> 00:37:20,960
вы должны использовать это для ответов

856
00:37:20,960 --> 00:37:23,200
в задании, что форма

857
00:37:23,200 --> 00:37:26,160
градиента, которую мы всегда будем делать,

858
00:37:26,160 --> 00:37:28,640
будет формой параметров,

859
00:37:28,640 --> 00:37:30,160
и поэтому,

860
00:37:31,440 --> 00:37:35,040
мм, SDW мы также собираемся представить

861
00:37:35,040 --> 00:37:38,640
как  матрица n на m, как и w,

862
00:37:38,640 --> 00:37:41,359
и мы собираемся изменить форму

863
00:37:41,359 --> 00:37:43,920
якобиана, чтобы поместить его в эту

864
00:37:43,920 --> 00:37:46,800
форму матрицы,

865
00:37:46,800 --> 00:37:49,920
хорошо, поэтому, если мы хотим поместить его в эту

866
00:37:49,920 --> 00:37:52,320
форму матрицы,

867
00:37:52,320 --> 00:37:54,240
что

868
00:37:54,240 --> 00:37:57,680
мы будем делать, что мы собираемся получить

869
00:37:57,680 --> 00:37:59,920
для SDW

870
00:37:59,920 --> 00:38:01,760
хорошо

871
00:38:01,760 --> 00:38:05,680
мы знаем, что это будет включать

872
00:38:05,680 --> 00:38:09,680
дельту нашего локального сигнала ошибки

873
00:38:10,240 --> 00:38:12,480
и

874
00:38:13,599 --> 00:38:16,240
тогда мы должны что-то придумать для

875
00:38:16,240 --> 00:38:18,560
dz dw

876
00:38:18,560 --> 00:38:23,119
um хорошо, так как c равно wx плюс b,

877
00:38:23,119 --> 00:38:26,240
вы как бы ожидаете, что ответ

878
00:38:26,240 --> 00:38:27,839
должен быть x

879
00:38:27,839 --> 00:38:28,640
um,

880
00:38:28,640 --> 00:38:32,359
и это правильно, поэтому ответ um на

881
00:38:32,359 --> 00:38:33,920
dsdw

882
00:38:33,920 --> 00:38:36,640
будет um будет

883
00:38:36,640 --> 00:38:40,560
дельта-транспонировать времена x транспонировать  и

884
00:38:40,560 --> 00:38:42,480
поэтому форма, которую мы получаем для этой

885
00:38:42,480 --> 00:38:45,520
производной, будет

886
00:38:45,520 --> 00:38:49,119
произведением локального сигнала ошибки,

887
00:38:49,119 --> 00:38:52,400
полученного сверху, по

888
00:38:52,400 --> 00:38:55,440
сравнению с тем, что мы вычисляем на локальном

889
00:38:55,440 --> 00:38:58,240
входе x,

890
00:38:58,240 --> 00:39:00,000
так что пока не должно быть очевидно, почему

891
00:39:00,000 --> 00:39:02,720
это правда.  так что позвольте мне

892
00:39:02,720 --> 00:39:06,079
более подробно остановиться на том, почему это правда,

893
00:39:06,079 --> 00:39:08,400
поэтому, когда мы хотим

894
00:39:10,880 --> 00:39:14,320
вычислить um dsdw правильно, это своего рода дельта-времена dz

895
00:39:14,320 --> 00:39:18,400
dw, где um то, что вычисляется для z,

896
00:39:18,400 --> 00:39:20,400
равно wx плюс b,

897
00:39:20,400 --> 00:39:23,440
поэтому давайте на мгновение рассмотрим, что

898
00:39:23,440 --> 00:39:26,160
производная относится к

899
00:39:26,160 --> 00:39:29,200
одному весу wij,

900
00:39:29,200 --> 00:39:34,160
поэтому wij может быть w 2 3, что показано здесь в

901
00:39:34,160 --> 00:39:36,560
моей маленькой нейронной сети,

902
00:39:36,560 --> 00:39:39,200
и поэтому первое, что нужно заметить,

903
00:39:39,200 --> 00:39:41,520
это то, что wij

904
00:39:41,520 --> 00:39:44,560
вносит вклад только в zi,

905
00:39:44,560 --> 00:39:45,280
поэтому

906
00:39:45,280 --> 00:39:48,880
он переходит в

907
00:39:49,280 --> 00:39:53,200
z2, который затем вычисляет h2, и это  не имеет

908
00:39:53,200 --> 00:39:57,040
никакого эффекта  n h1

909
00:39:57,040 --> 00:40:02,560
хорошо, поэтому, когда мы будем работать с um dzi dw

910
00:40:02,560 --> 00:40:03,760
ij,

911
00:40:03,760 --> 00:40:05,680
это будет

912
00:40:07,599 --> 00:40:09,920
dwix такого рода строка,

913
00:40:09,920 --> 00:40:13,280
эта строка матрицы плюс bi,

914
00:40:13,280 --> 00:40:16,480
что означает, что

915
00:40:16,480 --> 00:40:20,079
у нас есть своего рода сумма wik, умноженная на

916
00:40:20,079 --> 00:40:21,280
xk,

917
00:40:21,280 --> 00:40:24,160
а затем  для этой суммы это похоже на

918
00:40:24,160 --> 00:40:27,119
исчисление одной переменной: когда мы

919
00:40:27,119 --> 00:40:29,359
берем производную этого по wij для

920
00:40:30,720 --> 00:40:33,680
каждого члена, и эта сумма будет равна

921
00:40:33,680 --> 00:40:35,200
нулю,

922
00:40:35,200 --> 00:40:36,960
производная будет равна нулю,

923
00:40:36,960 --> 00:40:40,000
за исключением той, которая включает wij,

924
00:40:40,000 --> 00:40:42,480
а затем  производная от этого

925
00:40:42,480 --> 00:40:43,440
похожа на

926
00:40:43,440 --> 00:40:46,960
ax по отношению к a, это будет x,

927
00:40:46,960 --> 00:40:51,440
поэтому вы получите xj в качестве ответа,

928
00:40:51,440 --> 00:40:55,040
и поэтому конечный результат этого заключается в том, что

929
00:40:55,040 --> 00:40:57,760
когда мы работаем над тем, что мы хотим,

930
00:40:57,760 --> 00:41:01,440
ответ состоит в том, что мы  собираюсь

931
00:41:01,440 --> 00:41:02,240
получить,

932
00:41:02,240 --> 00:41:04,560
что эти

933
00:41:04,560 --> 00:41:08,560
столбцы, где x1 - это все, что осталось, x2

934
00:41:08,560 --> 00:41:11,440
- это все, что осталось через xm, это все,

935
00:41:11,440 --> 00:41:15,119
что осталось, а затем это умножается

936
00:41:15,119 --> 00:41:18,319
на векторы локального сигнала ошибки

937
00:41:18,319 --> 00:41:22,000
сверху, и мы хотим

938
00:41:22,000 --> 00:41:24,400
вычислить эту матрицу внешнего продукта, где  мы

939
00:41:24,400 --> 00:41:26,560
получаем разные

940
00:41:26,560 --> 00:41:28,319
комбинации дельты

941
00:41:28,319 --> 00:41:32,560
и x, и поэтому мы можем получить матрицу n на m,

942
00:41:32,560 --> 00:41:35,440
которую мы хотели бы иметь в соответствии с нашим

943
00:41:35,440 --> 00:41:38,079
соглашением о форме, взяв

944
00:41:38,079 --> 00:41:42,079
дельта-транспонирование, которое равно n на 1, умноженное на транспонирование x,

945
00:41:42,079 --> 00:41:44,720
которое затем равно 1 на m, а затем

946
00:41:44,720 --> 00:41:48,720
мы получаем эту внешнюю матрицу произведения  гм так

947
00:41:48,720 --> 00:41:51,359
похоже, что это своего рода хакерский аргумент,

948
00:41:51,359 --> 00:41:53,520
который я привел, это, безусловно, способ

949
00:41:53,520 --> 00:41:55,520
делать то, что размеры работают,

950
00:41:55,520 --> 00:41:58,640
и это вроде как имеет смысл ммм,

951
00:41:58,640 --> 00:42:01,040
есть более подробный анализ этого,

952
00:42:01,040 --> 00:42:04,880
который появляется в лекционных заметках, и я

953
00:42:04,880 --> 00:42:07,040
поощряю  вы также можете взглянуть

954
00:42:07,040 --> 00:42:09,760
на более матовую версию этого,

955
00:42:09,760 --> 00:42:12,160
вот немного больше информации

956
00:42:12,160 --> 00:42:16,079
о соглашении о формах, так что

957
00:42:16,079 --> 00:42:19,680
прежде всего еще один

958
00:42:20,560 --> 00:42:22,800
пример этого, поэтому, когда вы

959
00:42:23,599 --> 00:42:26,319
разрабатываете sdb

960
00:42:26,319 --> 00:42:28,560
,

961
00:42:28,880 --> 00:42:32,560
который выходит как  это якобиан -

962
00:42:32,560 --> 00:42:36,720
вектор-строка um, но аналогично вы знаете, что в

963
00:42:36,720 --> 00:42:40,000
соответствии с соглашением о формах мы хотим, чтобы

964
00:42:40,000 --> 00:42:43,599
наш градиент имел ту же форму, что и b,

965
00:42:43,599 --> 00:42:46,160
а b - вектор-столбец,

966
00:42:46,160 --> 00:42:47,599
так что это вроде опять же они

967
00:42:47,599 --> 00:42:49,839
разные формы, и вам нужно

968
00:42:49,839 --> 00:42:52,240
переставить один, чтобы получить  другой

969
00:42:52,240 --> 00:42:54,480
и такой эффективный  У нас есть

970
00:42:54,480 --> 00:42:58,000
разногласия между якобианской формой,

971
00:42:58,000 --> 00:43:01,040
поэтому якобианова форма имеет смысл, поскольку

972
00:43:01,040 --> 00:43:03,119
вы знаете исчисление и математику, потому что, если

973
00:43:03,119 --> 00:43:07,119
вы хотите, чтобы это было так, как я утверждал,

974
00:43:07,119 --> 00:43:09,520
матричное исчисление похоже на

975
00:43:09,520 --> 00:43:11,440
исчисление с одной переменной, за исключением использования

976
00:43:11,440 --> 00:43:13,599
векторов и матриц, вы можете  просто

977
00:43:13,599 --> 00:43:16,160
умножьте части вместе, что

978
00:43:16,160 --> 00:43:19,520
работает только в том случае, если вы используете якобианы,

979
00:43:19,520 --> 00:43:23,200
но, с другой стороны, если вы хотите выполнить

980
00:43:23,200 --> 00:43:26,079
стохастический градиентный спуск

981
00:43:26,079 --> 00:43:26,880
и

982
00:43:26,880 --> 00:43:29,520
иметь возможность как бы вычесть

983
00:43:29,520 --> 00:43:32,560
часть градиента, который работает только в том

984
00:43:32,560 --> 00:43:35,520
случае, если у вас такая же форма  матрица

985
00:43:35,520 --> 00:43:37,040
для

986
00:43:37,040 --> 00:43:39,280
градиента, как вы это делаете для исходной

987
00:43:39,280 --> 00:43:41,119
матрицы,

988
00:43:41,119 --> 00:43:42,160
и поэтому

989
00:43:42,160 --> 00:43:44,240
это немного сбивает с толку, но это

990
00:43:44,240 --> 00:43:47,200
просто реальность, есть обе эти um

991
00:43:47,200 --> 00:43:50,400
две вещи, поэтому форма Якоби

992
00:43:50,400 --> 00:43:54,880
полезна при выполнении um исчисления,

993
00:43:54,880 --> 00:43:58,640
но для ответов в задании мы

994
00:43:58,640 --> 00:44:01,520
хотите, чтобы ответы um были представлены

995
00:44:01,520 --> 00:44:05,680
с использованием соглашения о форме,

996
00:44:05,680 --> 00:44:08,720
чтобы градиент отображался в той же форме,

997
00:44:10,000 --> 00:44:12,400
что и параметры, и поэтому вы

998
00:44:12,400 --> 00:44:15,440
сможете выбрать правильную форму для doi  ng

999
00:44:15,440 --> 00:44:17,920
обновление градиента путем простого вычитания

1000
00:44:17,920 --> 00:44:21,520
небольшого количества градиента,

1001
00:44:21,520 --> 00:44:24,560
поэтому для работы с вещами

1002
00:44:24,560 --> 00:44:28,240
в основном есть два варианта,

1003
00:44:28,240 --> 00:44:31,599
один выбор - проработать всю

1004
00:44:31,599 --> 00:44:35,359
математику с использованием якобианов, а затем прямо

1005
00:44:35,359 --> 00:44:38,560
в конце изменить форму, следуя

1006
00:44:38,560 --> 00:44:40,960
соглашению о форме, чтобы дать  ответ,

1007
00:44:40,960 --> 00:44:45,599
вот что я сделал, когда разработал dsdb,

1008
00:44:45,599 --> 00:44:46,720
мы

1009
00:44:46,720 --> 00:44:48,560
проработали его,

1010
00:44:48,560 --> 00:44:49,520
используя

1011
00:44:49,520 --> 00:44:52,560
якобианы, мы получили ответ,

1012
00:44:52,560 --> 00:44:54,960
но он оказался вектором-строкой, и

1013
00:44:54,960 --> 00:44:57,440
так хорошо, мы должны транспонировать его

1014
00:44:57,440 --> 00:44:59,680
в конце, чтобы придать ему правильную форму

1015
00:44:59,680 --> 00:45:01,760
для соглашения о форме

1016
00:45:02,560 --> 00:45:04,880
um альтернатива -

1017
00:45:04,880 --> 00:45:10,160
um всегда следовать соглашению формы

1018
00:45:10,160 --> 00:45:13,040
um, и это то, что я делал, когда

1019
00:45:13,040 --> 00:45:17,200
тогда работал над dsdw, я не полностью

1020
00:45:17,200 --> 00:45:20,480
использовал jacobians, я сказал, хорошо, когда мы

1021
00:45:20,480 --> 00:45:23,040
разработаем то,

1022
00:45:23,359 --> 00:45:26,800
что было dz dw давайте  решить, какой

1023
00:45:26,800 --> 00:45:29,200
формы мы хотим, чтобы он был и чем

1024
00:45:29,200 --> 00:45:31,280
заполнить ячейки,

1025
00:45:31,280 --> 00:45:33,760
и если вы вроде как

1026
00:45:33,760 --> 00:45:36,240
пытаетесь сделать это немедленно с

1027
00:45:36,240 --> 00:45:38,079
соглашением о форме,

1028
00:45:38,079 --> 00:45:40,240
это немного более

1029
00:45:40,240 --> 00:45:41,280
хакерским

1030
00:45:41,280 --> 00:45:43,599
способом, так как вы знаете, что должны смотреть

1031
00:45:43,599 --> 00:45:45,599
в размерах для того, что вы хотите, и

1032
00:45:45,599 --> 00:45:48,079
выясните, когда транспонировать или

1033
00:45:48,079 --> 00:45:50,560
изменить форму матрицы, чтобы она имела правильную

1034
00:45:50,560 --> 00:45:53,839
форму, но

1035
00:45:53,839 --> 00:45:55,760
я привел неформальные рассуждения о

1036
00:45:55,760 --> 00:45:58,400
том, что вы делаете и что работает, и вы

1037
00:45:58,400 --> 00:46:00,400
знаете один способ,

1038
00:46:00,400 --> 00:46:02,400
и есть вид  подсказок, которые вы можете

1039
00:46:02,400 --> 00:46:04,480
использовать правильно, что вы знаете, что ваш

1040
00:46:04,480 --> 00:46:07,200
градиент всегда должен быть той же формы,

1041
00:46:07,200 --> 00:46:10,319
что и ваши параметры, и вы знаете, что

1042
00:46:10,319 --> 00:46:12,400
приходящее сообщение об ошибке

1043
00:46:12,400 --> 00:46:14,960
всегда будет иметь ту же размерность,

1044
00:46:14,960 --> 00:46:17,440
что и этот скрытый слой, и вы всегда можете как

1045
00:46:17,440 --> 00:46:19,440
бы с этим справиться  следуя соглашению о форме,

1046
00:46:23,119 --> 00:46:24,560
хорошо,

1047
00:46:25,760 --> 00:46:27,119
так что

1048
00:46:27,119 --> 00:46:28,480
,

1049
00:46:28,480 --> 00:46:29,599
эй,

1050
00:46:29,599 --> 00:46:31,680
это все

1051
00:46:31,680 --> 00:46:34,560
матричное исчисление,

1052
00:46:34,560 --> 00:46:37,680
поэтому после паузы для дыхания

1053
00:46:37,680 --> 00:46:40,160
на секунду

1054
00:46:41,040 --> 00:46:42,560
остальная часть

1055
00:46:42,560 --> 00:46:45,200
лекции будет в порядке,

1056
00:46:45,200 --> 00:46:48,560
давайте посмотрим, как наше программное обеспечение

1057
00:46:48,560 --> 00:46:51,359
обучает нейронные сети, используя то, что

1058
00:46:51,359 --> 00:46:54,160
называется обратным распространением

1059
00:46:54,160 --> 00:46:57,839
алгоритм обратного распространения

1060
00:47:02,720 --> 00:47:04,400
гм,

1061
00:47:04,400 --> 00:47:05,440
так

1062
00:47:05,440 --> 00:47:08,319
что краткий ответ:

1063
00:47:08,319 --> 00:47:09,359
вы знаете, что в

1064
00:47:09,359 --> 00:47:11,520
основном мы уже сделали это, остальная

1065
00:47:11,520 --> 00:47:14,800
часть лекции проста, так что вы знаете, по

1066
00:47:14,800 --> 00:47:16,480
сути,

1067
00:47:16,480 --> 00:47:17,920
я только что показал вам, что за бак

1068
00:47:17,920 --> 00:47:20,240
алгоритм распространения k не работает,

1069
00:47:20,240 --> 00:47:21,680
поэтому

1070
00:47:21,680 --> 00:47:24,400
алгоритм обратного распространения

1071
00:47:25,240 --> 00:47:26,960
разумно

1072
00:47:26,960 --> 00:47:28,079
берет

1073
00:47:28,079 --> 00:47:30,400
и

1074
00:47:30,800 --> 00:47:32,440
распространяет

1075
00:47:32,440 --> 00:47:34,079
производные

1076
00:47:34,079 --> 00:47:38,720
с использованием правила цепочки матриц,

1077
00:47:38,720 --> 00:47:40,640
остальная часть алгоритма обратного

1078
00:47:40,640 --> 00:47:42,720
распространения говорит:

1079
00:47:42,720 --> 00:47:45,920
`` Хорошо, когда у нас есть эти нейронные сети,

1080
00:47:45,920 --> 00:47:50,319
у нас есть много общей структуры и

1081
00:47:50,319 --> 00:47:54,559
общих производных, так что что  мы

1082
00:47:54,559 --> 00:47:58,079
хотим максимально эффективно повторно использовать

1083
00:47:59,839 --> 00:48:03,119
производные более высоких уровней, когда мы вычисляем

1084
00:48:03,119 --> 00:48:06,000
производные для более низких уровней, чтобы

1085
00:48:06,000 --> 00:48:08,400
минимизировать вычисления, и я уже

1086
00:48:08,400 --> 00:48:10,640
указывал на это в первой половине, но

1087
00:48:10,640 --> 00:48:14,800
мы хотим систематически использовать это

1088
00:48:14,800 --> 00:48:17,119
и, таким образом, то, как мы это делаем

1089
00:48:17,119 --> 00:48:20,559
в наших вычислительных системах они

1090
00:48:20,559 --> 00:48:23,359
строят графы вычислений,

1091
00:48:23,359 --> 00:48:27,200
так что это, возможно, немного похоже на

1092
00:48:27,200 --> 00:48:29,359
то, что вы видели в классе компилятора, если

1093
00:48:29,359 --> 00:48:33,200
вы сделали то, что вы делаете правильно,

1094
00:48:33,200 --> 00:48:35,440
я называю это здесь графом вычислений, но

1095
00:48:35,440 --> 00:48:37,200
на самом деле это дерево правильно  Итак, вы

1096
00:48:37,200 --> 00:48:41,280
создаете здесь это дерево вычислений

1097
00:48:41,280 --> 00:48:44,079
в этом случае, но в более общем случае

1098
00:48:44,079 --> 00:48:46,079
это какой-то ориентированный граф

1099
00:48:46,079 --> 00:48:48,559
вычислений

1100
00:48:48,559 --> 00:48:50,960
wh  ich имеет

1101
00:48:50,960 --> 00:48:54,720
исходные узлы, которые являются входами, либо

1102
00:48:54,720 --> 00:48:58,640
входами, такими как x, либо входными параметрами, такими как w

1103
00:48:58,640 --> 00:48:59,520
и b,

1104
00:48:59,520 --> 00:49:03,680
и его внутренние узлы являются операциями,

1105
00:49:03,680 --> 00:49:05,839
и поэтому, как только мы построили

1106
00:49:05,839 --> 00:49:08,720
граф, и этот граф

1107
00:49:08,720 --> 00:49:11,040
точно соответствует примеру, который я делал раньше,

1108
00:49:11,040 --> 00:49:12,800
это был наш  маленькая нейронная сеть, которая находится в

1109
00:49:12,800 --> 00:49:14,400
правом верхнем углу, и вот

1110
00:49:14,400 --> 00:49:16,960
соответствующий граф вычислений для

1111
00:49:16,960 --> 00:49:19,920
вычисления wx плюс b,

1112
00:49:19,920 --> 00:49:21,200
пропустите его

1113
00:49:21,200 --> 00:49:24,240
через сигмовидную нелинейность. f

1114
00:49:24,240 --> 00:49:26,559
умножьте результирующий скалярный

1115
00:49:26,559 --> 00:49:29,280
продукт результирующего вектора на u, и получите нашу

1116
00:49:29,280 --> 00:49:31,440
результативную оценку s

1117
00:49:31,440 --> 00:49:32,720
um

1118
00:49:32,720 --> 00:49:34,559
ok, так

1119
00:49:34,559 --> 00:49:37,599
что мы  Чтобы вычислить это, мы передаем

1120
00:49:37,599 --> 00:49:39,520
по краям результаты

1121
00:49:39,520 --> 00:49:43,200
операций, так что это wx, затем z, затем h,

1122
00:49:43,200 --> 00:49:45,359
а затем наш вывод - s,

1123
00:49:45,359 --> 00:49:47,760
и поэтому первое, что мы хотим

1124
00:49:47,760 --> 00:49:49,520
сделать для вычислений с помощью нейронных

1125
00:49:49,520 --> 00:49:52,400
сетей, - это иметь возможность  вычислить для

1126
00:49:52,400 --> 00:49:55,920
разных входов, что такое выход, и

1127
00:49:55,920 --> 00:49:58,400
поэтому это называется прямым

1128
00:49:58,400 --> 00:50:01,359
распространением, и поэтому мы просто

1129
00:50:01,359 --> 00:50:02,480
запускаем

1130
00:50:02,480 --> 00:50:03,680
это

1131
00:50:03,680 --> 00:50:05,040
выражение

1132
00:50:05,040 --> 00:50:06,480
так же, как

1133
00:50:06,480 --> 00:50:09,599
вы обычно делаете это в компиляторе, чтобы

1134
00:50:09,599 --> 00:50:12,000
вычислить значение s, и это

1135
00:50:12,000 --> 00:50:14,559
фаза прямого распространения, но

1136
00:50:14,559 --> 00:50:17,280
важным дополнительным элементом нейронных

1137
00:50:17,280 --> 00:50:21,200
сетей является то, что мы также хотим иметь

1138
00:50:21,200 --> 00:50:24,400
возможность отправлять обратно градиенты, которые

1139
00:50:24,400 --> 00:50:26,880
сообщают нам, как обновлять

1140
00:50:26,880 --> 00:50:31,040
параметры модели, и поэтому именно эта способность

1141
00:50:31,040 --> 00:50:33,440
отправлять градиенты обратно дает нам

1142
00:50:33,440 --> 00:50:36,720
способность этих моделей учиться, как только

1143
00:50:36,720 --> 00:50:39,520
у нас будет функция потерь в конце, мы

1144
00:50:39,520 --> 00:50:41,119
сможем решить, как изменить

1145
00:50:41,119 --> 00:50:43,839
параметры модели, чтобы они

1146
00:50:43,839 --> 00:50:45,359
более точно

1147
00:50:45,359 --> 00:50:48,720
производили желаемый результат, они

1148
00:50:48,720 --> 00:50:52,400
минимизируют потери, и поэтому он выполняет эту

1149
00:50:52,400 --> 00:50:55,200
часть, которая затем  называется обратным

1150
00:50:55,200 --> 00:50:56,559
распространением,

1151
00:50:56,559 --> 00:51:00,319
поэтому мы затем, как только мы передаем

1152
00:51:00,319 --> 00:51:02,960
значение с нашими текущими параметрами,

1153
00:51:02,960 --> 00:51:04,880
мы затем

1154
00:51:04,880 --> 00:51:07,520
идем назад, меняя

1155
00:51:07,520 --> 00:51:11,280
направление стрелок, и передаем градиенты

1156
00:51:11,280 --> 00:51:14,880
вниз к различным параметрам, таким как b

1157
00:51:14,880 --> 00:51:19,040
и w и u, которые мы можем использовать для изменения,

1158
00:51:19,040 --> 00:51:21,359
используя  стохастический градиент, чтобы передать,

1159
00:51:21,359 --> 00:51:23,599
какое значение b соответствует значению w,

1160
00:51:23,599 --> 00:51:27,680
поэтому мы начинаем с dsds, который является

1161
00:51:27,680 --> 00:51:28,960
всего одним,

1162
00:51:28,960 --> 00:51:30,319
а затем

1163
00:51:30,319 --> 00:51:33,599
запускаем наше обратное распространение, и мы  петь

1164
00:51:33,599 --> 00:51:36,960
что-то вроде

1165
00:51:36,960 --> 00:51:40,000
композиции якобиана, поэтому у нас есть sdh здесь

1166
00:51:40,000 --> 00:51:43,359
и sdz, и мы постепенно

1167
00:51:43,359 --> 00:51:46,000
возвращаем эти градиенты,

1168
00:51:46,000 --> 00:51:48,960
поэтому нам просто нужно решить, как

1169
00:51:48,960 --> 00:51:51,839
эффективно и чисто сделать это в

1170
00:51:51,839 --> 00:51:54,720
вычислительной системе, и поэтому давайте

1171
00:51:54,720 --> 00:51:57,280
поработаем  через несколько из этих

1172
00:51:57,280 --> 00:52:00,880
случаев, так что общая ситуация такова, что

1173
00:52:00,880 --> 00:52:02,960
у нас

1174
00:52:02,960 --> 00:52:06,400
есть конкретный узел, поэтому узел - это место, где происходит

1175
00:52:06,400 --> 00:52:08,400
какая-то

1176
00:52:08,400 --> 00:52:11,599
операция, такая как умножение

1177
00:52:11,599 --> 00:52:14,400
или нелинейность, и поэтому в

1178
00:52:14,400 --> 00:52:17,599
простейшем случае у нас есть один

1179
00:52:17,599 --> 00:52:20,160
выход и один  input, поэтому мы сделаем это в

1180
00:52:20,160 --> 00:52:23,599
первую очередь так, как будто h равно f of z,

1181
00:52:23,599 --> 00:52:24,400
поэтому

1182
00:52:24,400 --> 00:52:25,920
у нас

1183
00:52:25,920 --> 00:52:28,800
есть восходящий градиент

1184
00:52:28,800 --> 00:52:29,720
um

1185
00:52:29,720 --> 00:52:33,200
dsdh, и что мы хотим сделать,

1186
00:52:33,200 --> 00:52:37,240
это вычислить градиент вниз по потоку

1187
00:52:37,240 --> 00:52:38,960
dsdz,

1188
00:52:38,960 --> 00:52:41,280
и способ, которым мы собираемся это сделать,

1189
00:52:41,280 --> 00:52:45,119
скажите хорошо для этой функции f

1190
00:52:45,119 --> 00:52:46,480
это функция, у

1191
00:52:46,480 --> 00:52:49,359
нее есть производная градиента, поэтому

1192
00:52:49,359 --> 00:52:52,079
мы хотим разработать этот локальный

1193
00:52:52,079 --> 00:52:55,680
градиент dhdz,

1194
00:52:55,680 --> 00:52:57,119
а затем

1195
00:52:57,119 --> 00:52:59,599
это дает нам все, что нам нужно для

1196
00:52:59,599 --> 00:53:02,640
разработки sdz, потому что

1197
00:53:02,640 --> 00:53:04,240
именно мы собираемся  использовать ча  в

1198
00:53:04,240 --> 00:53:07,359
правиле мы собираемся сказать, что dsdz

1199
00:53:07,359 --> 00:53:10,720
равняется произведению sdh на hdz,

1200
00:53:10,720 --> 00:53:14,640
где снова используются якобианы,

1201
00:53:14,640 --> 00:53:17,119
хорошо, поэтому общий принцип, который мы

1202
00:53:17,119 --> 00:53:19,839
собираемся использовать, заключается в том, что градиент вниз по потоку

1203
00:53:19,839 --> 00:53:22,240
равен градиенту вверх по потоку, умноженному на

1204
00:53:22,240 --> 00:53:23,920
локальный градиент,

1205
00:53:23,920 --> 00:53:26,720
хорошо  иногда это становится немного

1206
00:53:26,720 --> 00:53:29,440
сложнее, поэтому у нас может быть несколько

1207
00:53:29,440 --> 00:53:32,240
входов для функции, так что

1208
00:53:32,240 --> 00:53:35,520
это матричный вектор умножения, поэтому z равно

1209
00:53:35,520 --> 00:53:39,680
wx, хорошо, когда есть несколько входов,

1210
00:53:39,680 --> 00:53:43,680
у нас все еще есть восходящий градиент

1211
00:53:43,680 --> 00:53:45,359
dsdz,

1212
00:53:45,359 --> 00:53:48,559
но что мы собираемся сделать, это  разработать

1213
00:53:48,559 --> 00:53:51,359
локальный градиент по отношению к каждому

1214
00:53:51,359 --> 00:53:54,839
входу, чтобы у нас были dz dw и

1215
00:53:54,839 --> 00:53:58,000
dzdx, и поэтому в этот момент это

1216
00:53:58,000 --> 00:53:59,680
точно то же самое

1217
00:53:59,680 --> 00:54:02,000
для каждого его фрагмента, мы собираемся

1218
00:54:02,000 --> 00:54:05,200
разработать нисходящие градиенты sdw

1219
00:54:05,200 --> 00:54:08,400
и sdx, используя  цепное правило по

1220
00:54:08,400 --> 00:54:10,480
отношению к

1221
00:54:10,480 --> 00:54:14,079
конкретному локальному градиенту,

1222
00:54:14,079 --> 00:54:16,800
так что давайте рассмотрим пример

1223
00:54:16,800 --> 00:54:19,680
этого, я имею в виду, что это своего рода глупый

1224
00:54:19,680 --> 00:54:22,240
пример, это не совсем пример, который

1225
00:54:22,240 --> 00:54:24,880
выглядит как типичная нейронная сеть, но это своего

1226
00:54:24,880 --> 00:54:26,880
рода простой пример  mple, где мы можем

1227
00:54:26,880 --> 00:54:29,200
показать некоторые из компонентов того, что мы

1228
00:54:29,200 --> 00:54:32,000
делаем, поэтому мы собираемся

1229
00:54:32,000 --> 00:54:33,599
вычислить

1230
00:54:33,599 --> 00:54:35,920
f для xyz,

1231
00:54:35,920 --> 00:54:38,960
которое вычисляется как x плюс y,

1232
00:54:38,960 --> 00:54:41,839
умноженное на max y и z

1233
00:54:41,839 --> 00:54:44,559
um, и мы получили вас  знать конкретные

1234
00:54:44,559 --> 00:54:47,200
значения, которые мы начинаем с x

1235
00:54:47,200 --> 00:54:49,920
равно одному, y равно двум, а z равно

1236
00:54:49,920 --> 00:54:53,040
нулю, так что это текущие значения

1237
00:54:53,040 --> 00:54:54,480
наших параметров,

1238
00:54:54,480 --> 00:54:56,559
и поэтому мы можем сказать, хорошо, что

1239
00:54:56,559 --> 00:54:58,640
мы хотим

1240
00:54:58,640 --> 00:55:01,040
построить дерево выражений для этого, вот

1241
00:55:01,040 --> 00:55:03,040
наше дерево выражений, которое

1242
00:55:03,040 --> 00:55:04,559
мы  мы берем x

1243
00:55:04,559 --> 00:55:07,599
плюс y, мы берем максимум y и z,

1244
00:55:07,599 --> 00:55:10,000
а затем умножаем их,

1245
00:55:10,000 --> 00:55:13,200
и поэтому наша фаза прямого распространения

1246
00:55:13,200 --> 00:55:15,760
- это просто запустить это, поэтому мы берем

1247
00:55:15,760 --> 00:55:19,440
значения наших параметров и просто

1248
00:55:19,440 --> 00:55:21,440
начинаем вычислять с ними  Итак, у нас

1249
00:55:21,440 --> 00:55:25,040
есть один, два, два, ноль, и мы складываем их

1250
00:55:25,040 --> 00:55:28,319
как три, максимум - два, мы их умножаем,

1251
00:55:28,319 --> 00:55:31,520
и это дает нам шесть,

1252
00:55:32,079 --> 00:55:35,359
хорошо, так что в этот момент

1253
00:55:35,359 --> 00:55:39,280
мы хотим пойти и решить,

1254
00:55:39,280 --> 00:55:40,319
как

1255
00:55:40,319 --> 00:55:43,760
это сделать для обратного распространения  и

1256
00:55:43,760 --> 00:55:46,799
как работают эти шаги обратного распространения,

1257
00:55:46,799 --> 00:55:50,000
и поэтому первая часть этого - сор  t определения того,

1258
00:55:50,000 --> 00:55:52,079
какими будут наши локальные градиенты,

1259
00:55:55,920 --> 00:56:00,240
так что это здесь, а это x и y,

1260
00:56:00,240 --> 00:56:03,680
так что dadx, поскольку a равно x плюс y, будет просто единицей,

1261
00:56:03,680 --> 00:56:07,440
а папа также будет одним

1262
00:56:09,440 --> 00:56:10,559
um,

1263
00:56:10,559 --> 00:56:11,520
тогда

1264
00:56:11,520 --> 00:56:14,319
um для

1265
00:56:14,640 --> 00:56:17,359
b равняется max yz

1266
00:56:17,359 --> 00:56:19,839
um, так что это этот максимальный узел, поэтому локальные

1267
00:56:19,839 --> 00:56:23,280
градиенты для него um будут

1268
00:56:23,280 --> 00:56:26,319
зависеть от yb, больше ли y, чем

1269
00:56:26,319 --> 00:56:28,799
z, поэтому

1270
00:56:28,799 --> 00:56:31,440
dbdy будет равным

1271
00:56:31,440 --> 00:56:34,720
единице тогда и только тогда, когда y больше, чем  z,

1272
00:56:34,720 --> 00:56:36,799
который находится в нашей конкретной точке здесь,

1273
00:56:36,799 --> 00:56:40,319
так что это единица, и db

1274
00:56:40,319 --> 00:56:43,520
dz будет единицей, только если

1275
00:56:43,520 --> 00:56:45,760
z больше, чем y, поэтому для наших

1276
00:56:45,760 --> 00:56:48,880
конкретных значений

1277
00:56:48,880 --> 00:56:50,640
здесь единица будет равна

1278
00:56:50,640 --> 00:56:52,160
нулю

1279
00:56:52,160 --> 00:56:55,280
um, а затем, наконец, здесь мы

1280
00:56:55,280 --> 00:56:58,480
вычисляем  произведение f равно ab

1281
00:56:58,480 --> 00:57:00,240
um, так

1282
00:57:00,240 --> 00:57:03,520
что для этого um

1283
00:57:03,520 --> 00:57:05,280
мы собираемся

1284
00:57:06,400 --> 00:57:07,200
подождать,

1285
00:57:07,200 --> 00:57:10,079
извините, что скользит по идеально хорошо, поэтому

1286
00:57:10,079 --> 00:57:13,040
для произведения um производная

1287
00:57:13,040 --> 00:57:15,599
f по a равна b, которая равна

1288
00:57:15,599 --> 00:57:18,160
двум, а производная f

1289
00:57:18,160 --> 00:57:21,440
по  b равно трем, что дает нам

1290
00:57:21,440 --> 00:57:23,839
все локальные градиенты

1291
00:57:23,839 --> 00:57:26,720
в каждом узле, и тогда  Чтобы запустить

1292
00:57:26,720 --> 00:57:28,240
обратное распространение,

1293
00:57:28,240 --> 00:57:32,079
мы начинаем с dfdf, который является всего лишь одним,

1294
00:57:32,079 --> 00:57:34,720
а затем мы

1295
00:57:34,720 --> 00:57:36,319
собираемся вычислить,

1296
00:57:36,319 --> 00:57:39,520
что нисходящий поток равен времени восходящего потока,

1297
00:57:39,520 --> 00:57:40,720
локально

1298
00:57:40,720 --> 00:57:43,200
хорошо, поэтому локально,

1299
00:57:43,200 --> 00:57:46,240
поэтому, когда у вас есть такой продукт, эм,

1300
00:57:46,240 --> 00:57:49,520
обратите внимание, что вид градиентов меняется,

1301
00:57:49,520 --> 00:57:52,000
поэтому мы  взять

1302
00:57:52,000 --> 00:57:55,359
время восходящего потока локальное, что составляет 2

1303
00:57:55,359 --> 00:57:57,680
ой,

1304
00:57:59,440 --> 00:58:00,880
так что

1305
00:58:00,880 --> 00:58:03,440
нисходящее время равно 2

1306
00:58:03,440 --> 00:58:06,480
на этой стороне

1307
00:58:06,839 --> 00:58:09,440
dfdb равно трем,

1308
00:58:09,440 --> 00:58:11,440
поэтому мы берем время восходящего потока локально,

1309
00:58:11,440 --> 00:58:13,200
что дает нам три

1310
00:58:13,200 --> 00:58:16,799
мкм, и, таким образом, это возвращает нас, распространяет

1311
00:58:16,799 --> 00:58:20,480
значения на плюсовые и максимальные узлы, и

1312
00:58:20,480 --> 00:58:22,480
так  затем мы продолжаем

1313
00:58:22,480 --> 00:58:25,760
так, чтобы для максимального узла

1314
00:58:25,760 --> 00:58:30,319
um локальный градиент dbdy был равен единице, поэтому мы собираемся взять верхний поток, равный

1315
00:58:30,319 --> 00:58:33,359
трем, поэтому

1316
00:58:33,359 --> 00:58:36,559
мы склонны брать три раза по единице,

1317
00:58:36,559 --> 00:58:39,280
и это дает нам три

1318
00:58:39,280 --> 00:58:42,640
dbdz, равных нулю, из-за того, что

1319
00:58:42,640 --> 00:58:45,200
значение z равно  а не максимальное um, поэтому мы

1320
00:58:45,200 --> 00:58:47,760
берем три раза ноль и говорим, что

1321
00:58:47,760 --> 00:58:50,960
градиент равен нулю, поэтому, наконец,

1322
00:58:50,960 --> 00:58:55,359
выполняем плюс node um, локальные градиенты для

1323
00:58:55,359 --> 00:58:58,319
обоих x и y есть один, поэтому мы просто

1324
00:58:58,319 --> 00:59:00,960
получаем два раза один в обоих случаях и

1325
00:59:00,960 --> 00:59:03,119
мы говорим, что гр  у нас

1326
00:59:03,119 --> 00:59:04,400
есть двое,

1327
00:59:04,400 --> 00:59:07,839
и поэтому опять же, в конце дня,

1328
00:59:07,839 --> 00:59:11,280
интерпретация здесь такова, что

1329
00:59:11,280 --> 00:59:14,240
это дает информацию о том, если

1330
00:59:14,240 --> 00:59:18,400
мы будем двигать значениями xy и z,

1331
00:59:18,400 --> 00:59:20,400
насколько это повлияет на

1332
00:59:20,400 --> 00:59:24,559
результат, что  наклон градиента

1333
00:59:24,559 --> 00:59:27,280
по отношению к переменной, поэтому

1334
00:59:27,280 --> 00:59:30,240
мы видели, что, поскольку

1335
00:59:32,160 --> 00:59:35,119
z не является максимальным значением y и z,

1336
00:59:35,119 --> 00:59:37,599
если я изменю значение za немного, как

1337
00:59:37,599 --> 00:59:42,400
если бы я сделал z 0,1 или минус 0,1, это не имеет

1338
00:59:42,400 --> 00:59:44,640
значения при  все для того, что я вычисляю в

1339
00:59:44,640 --> 00:59:46,559
качестве вывода, поэтому градиент

1340
00:59:46,559 --> 00:59:49,520
там равен нулю,

1341
00:59:49,520 --> 00:59:52,079
если я

1342
00:59:52,079 --> 00:59:55,440
немного изменю значение x,

1343
00:59:55,440 --> 00:59:59,280
тогда это будет иметь эффект, и

1344
00:59:59,280 --> 01:00:03,040
это повлияет на вывод в

1345
01:00:03,040 --> 01:00:08,079
два раза больше, чем количество, которое я меняю,

1346
01:00:08,559 --> 01:00:10,880
упс

1347
01:00:12,400 --> 01:00:16,880
правильно  так, и это потому, что dfdz

1348
01:00:16,880 --> 01:00:18,480
равняется двум

1349
01:00:18,480 --> 01:00:19,280
um,

1350
01:00:19,280 --> 01:00:20,079
так

1351
01:00:20,079 --> 01:00:23,280
интересно,

1352
01:00:23,280 --> 01:00:26,240
так что я имею в виду, что мы можем в основном это решить,

1353
01:00:26,240 --> 01:00:28,400
так что если мы представим, что um

1354
01:00:28,400 --> 01:00:32,480
делает что-то вроде x 2,1 хорошо, то мы бы

1355
01:00:32,480 --> 01:00:35,940
вычислили максимум для

1356
01:00:35,940 --> 01:00:37,839
[Music]

1357
01:00:37,839 --> 01:00:41,520
ой, извините, если  мы делаем x 1.1,

1358
01:00:41,520 --> 01:00:44,240
тогда мы получаем максимум здесь 2,

1359
01:00:44,240 --> 01:00:48,319
и мы получаем 1.1 плюс  2 равно 3,1, поэтому мы получаем

1360
01:00:48,319 --> 01:00:50,720
3,1 умножить на

1361
01:00:50,720 --> 01:00:54,240
2, так что это будет примерно 6,2,

1362
01:00:54,240 --> 01:00:56,400
поэтому изменение x на

1363
01:00:56,400 --> 01:01:00,000
0,1 добавило 0,2 к значению f

1364
01:01:00,000 --> 01:01:03,839
um, наоборот, для значения y мы обнаружим,

1365
01:01:03,839 --> 01:01:07,680
что df dy равно 5, так

1366
01:01:07,680 --> 01:01:09,599
что мы делаем, когда мы  У меня есть две

1367
01:01:09,599 --> 01:01:12,160
вещи, которые я

1368
01:01:12,160 --> 01:01:14,079
сейчас повторю, это мы суммируем

1369
01:01:14,079 --> 01:01:16,400
градиенты, так что снова три плюс два равняются

1370
01:01:16,400 --> 01:01:19,200
пяти, и эмпирически это то, что происходит,

1371
01:01:19,200 --> 01:01:22,000
поэтому, если мы рассмотрим изменение значения

1372
01:01:22,000 --> 01:01:25,040
ya little, скажем, мы  сделайте для

1373
01:01:25,040 --> 01:01:28,799
него значение 2,1, тогда прогноз таков, что

1374
01:01:28,799 --> 01:01:29,839
они будут иметь в

1375
01:01:29,839 --> 01:01:32,319
пять раз больший эффект на

1376
01:01:32,319 --> 01:01:35,040
выходное значение, которое мы вычисляем, и хорошо, что

1377
01:01:35,040 --> 01:01:37,040
мы вычисляем, поэтому мы

1378
01:01:37,040 --> 01:01:41,839
вычисляем 1 плюс 2,1, так что это 3,1, и мы

1379
01:01:41,839 --> 01:01:44,559
вычисляем максимум um

1380
01:01:44,559 --> 01:01:48,400
2,1 и  0 как 2,1, поэтому мы возьмем

1381
01:01:48,400 --> 01:01:52,640
произведение 2,1 и 3,1, и я рассчитаю

1382
01:01:52,640 --> 01:01:54,240
это заранее, так как я не могу выполнить

1383
01:01:54,240 --> 01:01:56,480
эту арифметику в уме, а

1384
01:01:56,480 --> 01:01:59,200
произведение этих двух составляет 6,51,

1385
01:01:59,200 --> 01:02:02,079
поэтому оно увеличилось примерно на

1386
01:02:02,079 --> 01:02:05,280
0,5, поэтому мы  Я умножил мою неуклюжесть на

1387
01:02:05,280 --> 01:02:07,280
0,1 в

1388
01:02:07,280 --> 01:02:09,520
пять раз, чтобы определить

1389
01:02:09,520 --> 01:02:12,000
величину эффекта от выходного сигнала.

1390
01:02:12,000 --> 01:02:12,880
Хорошо,

1391
01:02:12,880 --> 01:02:14,640
так что для этого материала

1392
01:02:14,640 --> 01:02:16,160
вы знаете, прежде чем

1393
01:02:16,160 --> 01:02:17,839
я сделал случай,

1394
01:02:19,119 --> 01:02:22,799
когда у нас был один упс, один зашел, один

1395
01:02:24,000 --> 01:02:25,920
здесь, и

1396
01:02:25,920 --> 01:02:31,200
несколько концов, и один здесь,

1397
01:02:31,200 --> 01:02:33,680
случай, с которым я на самом деле не имел дело,

1398
01:02:35,200 --> 01:02:38,079
это случай, когда у вас есть  несколько

1399
01:02:38,079 --> 01:02:41,280
внешних ветвей, но это затем появилось

1400
01:02:41,280 --> 01:02:43,520
при вычислении y,

1401
01:02:43,520 --> 01:02:45,920
поэтому, когда у вас есть несколько внешних

1402
01:02:45,920 --> 01:02:48,640
ветвей, вы делаете

1403
01:02:48,640 --> 01:02:50,319
суммирование,

1404
01:02:50,319 --> 01:02:55,119
так что, когда вы хотите отработать

1405
01:02:56,240 --> 01:02:58,799
dfdy,

1406
01:02:58,799 --> 01:03:01,280
у вас есть локальный

1407
01:03:01,280 --> 01:03:03,680
градиент, у вас есть два  восходящие

1408
01:03:03,680 --> 01:03:05,839
градиенты,

1409
01:03:05,839 --> 01:03:08,799
и вы прорабатываете их по отношению

1410
01:03:08,799 --> 01:03:10,400
к каждому из них,

1411
01:03:10,400 --> 01:03:12,480
как в цепном правиле, а затем

1412
01:03:12,480 --> 01:03:15,280
суммируете их вместе, чтобы определить

1413
01:03:15,280 --> 01:03:18,480
влияние в конце

1414
01:03:20,559 --> 01:03:23,280
справа, поэтому мы также увидели некоторые

1415
01:03:23,280 --> 01:03:26,079
интуиции других узлов, которые это полезно

1416
01:03:26,079 --> 01:03:28,240
чтобы um делал это

1417
01:03:28,240 --> 01:03:31,359
так, когда у вас есть дополнительный

1418
01:03:31,359 --> 01:03:34,319
um, который распределяет восходящий

1419
01:03:34,319 --> 01:03:39,280
градиент для каждой из вещей под ним,

1420
01:03:39,280 --> 01:03:41,520
когда у вас есть max, это похоже на узел маршрутизации,

1421
01:03:41,520 --> 01:03:44,079
поэтому, когда у вас есть max, у вас есть

1422
01:03:44,079 --> 01:03:47,119
восходящий градиент, и он переходит к одному

1423
01:03:47,119 --> 01:03:49,359
из  ветви под ним и th  Остальные из

1424
01:03:49,359 --> 01:03:52,720
них не получают градиента

1425
01:03:52,960 --> 01:03:56,799
um, когда у вас затем есть умножение,

1426
01:03:56,799 --> 01:03:59,680
это имеет эффект переключения

1427
01:03:59,680 --> 01:04:02,480
градиента, поэтому, если вы берете

1428
01:04:02,480 --> 01:04:04,160
три на два

1429
01:04:04,160 --> 01:04:07,599
um, градиент на двух сторонах равен трех,

1430
01:04:07,599 --> 01:04:10,640
а на трех сторонах - два, и если вы

1431
01:04:10,640 --> 01:04:13,119
подумайте о том, какой эффект

1432
01:04:13,119 --> 01:04:15,119
вы получите, когда будете делать такое

1433
01:04:15,119 --> 01:04:17,440
покачивание, которое имеет смысл

1434
01:04:17,440 --> 01:04:19,359
правильно, потому что если вы умножаете

1435
01:04:19,359 --> 01:04:22,079
другое число на три, то

1436
01:04:22,079 --> 01:04:24,079
любое изменение здесь будет

1437
01:04:24,079 --> 01:04:28,799
умножено на 3, и наоборот.

1438
01:04:31,599 --> 01:04:34,400
так что это тот вид графа вычислений,

1439
01:04:34,400 --> 01:04:36,640
который мы хотим использовать

1440
01:04:36,640 --> 01:04:38,720
для вычисления

1441
01:04:38,720 --> 01:04:40,960
производных в автоматическом

1442
01:04:40,960 --> 01:04:43,680
вычислительном режиме um, который является

1443
01:04:43,680 --> 01:04:47,119
основой алгоритма обратного распространения,

1444
01:04:47,119 --> 01:04:49,760
но в этот момент вы знаете, что это

1445
01:04:49,760 --> 01:04:51,839
то, что мы делаем, но есть  Тем не менее, вы

1446
01:04:51,839 --> 01:04:54,400
знаете одну ошибку, которую мы можем сделать,

1447
01:04:54,400 --> 01:04:56,640
было бы неправильно для нас сказать,

1448
01:04:56,640 --> 01:04:58,640
хорошо, в первую очередь, мы хотим

1449
01:04:58,640 --> 01:05:00,559
проработать SDB,

1450
01:05:00,559 --> 01:05:04,640
так что посмотрим, мы можем начать здесь, мы можем

1451
01:05:04,640 --> 01:05:07,920
распространять наши ошибки в восходящем потоке, разрабатывать

1452
01:05:07,920 --> 01:05:09,440
локальные градиенты в

1453
01:05:09,440 --> 01:05:11,760
восходящем направлении  локальный градиент ошибки и держитесь

1454
01:05:11,760 --> 01:05:13,599
полностью вниз

1455
01:05:13,599 --> 01:05:15,920
и

1456
01:05:16,880 --> 01:05:20,799
загрузите dsdb здесь, хорошо, теперь мы хотим сделать

1457
01:05:20,799 --> 01:05:22,160
это

1458
01:05:22,160 --> 01:05:25,440
для dsdw, давайте просто запустим его

1459
01:05:25,440 --> 01:05:27,920
снова, потому что если бы мы это сделали, мы бы

1460
01:05:27,920 --> 01:05:28,960
выполняли

1461
01:05:28,960 --> 01:05:31,599
повторные вычисления, как я показал в

1462
01:05:31,599 --> 01:05:34,160
первой половине  что этот

1463
01:05:34,160 --> 01:05:37,119
член один и тот же оба раза, этот член

1464
01:05:37,119 --> 01:05:39,599
один и тот же, оба раза этот член один и тот же

1465
01:05:39,599 --> 01:05:42,480
оба раза, что различаются только биты в конце,

1466
01:05:43,520 --> 01:05:45,680
поэтому мы

1467
01:05:45,680 --> 01:05:48,960
хотим избежать дублирования вычислений и

1468
01:05:48,960 --> 01:05:52,960
вычислить все градиенты,

1469
01:05:52,960 --> 01:05:56,240
которые мы  нам понадобится um последовательно,

1470
01:05:56,240 --> 01:05:59,200
чтобы мы выполняли их только один раз, и поэтому это

1471
01:05:59,200 --> 01:06:02,000
было аналогично, когда я ввел эту

1472
01:06:02,000 --> 01:06:04,640
дельта-переменную, когда мы вычисляли

1473
01:06:04,640 --> 01:06:07,920
градиенты вручную, поэтому, начиная с

1474
01:06:10,480 --> 01:06:11,359
d

1475
01:06:11,359 --> 01:06:15,839
um, мы начинаем здесь с dsds, это тот, который

1476
01:06:15,839 --> 01:06:20,400
мы затем хотим один раз  вычислить

1477
01:06:20,400 --> 01:06:23,359
градиент зеленого цвета здесь один раз

1478
01:06:23,359 --> 01:06:25,680
вычислить градиент зеленого цвета здесь,

1479
01:06:25,680 --> 01:06:27,920
это обычная работа,

1480
01:06:27,920 --> 01:06:29,760
затем мы

1481
01:06:29,760 --> 01:06:32,480
возьмем локальный градиент um

1482
01:06:32,480 --> 01:06:34,079
для

1483
01:06:34,079 --> 01:06:35,599
dz db

1484
01:06:35,599 --> 01:06:37,839
и умножим его на градиент восходящего потока,

1485
01:06:37,839 --> 01:06:40,559
чтобы вычислить dsdb,

1486
01:06:40,559 --> 01:06:42,400
а затем мы собираемся t  o взять тот же

1487
01:06:42,400 --> 01:06:46,799
восходящий градиент, а затем

1488
01:06:46,799 --> 01:06:50,000
разработать локальный градиент здесь

1489
01:06:50,000 --> 01:06:50,880
um,

1490
01:06:50,880 --> 01:06:53,359
а затем как бы распространить его вниз, чтобы

1491
01:06:53,359 --> 01:06:55,760
получить sdw,

1492
01:06:55,760 --> 01:06:58,880
поэтому конечный результат заключается в том, что мы хотим вроде как

1493
01:06:58,880 --> 01:07:00,400
систематически

1494
01:07:00,400 --> 01:07:01,520
работать, чтобы

1495
01:07:01,520 --> 01:07:04,319
продвигать вычисления вперед по графику

1496
01:07:04,319 --> 01:07:06,640
и вычисления в обратном направлении

1497
01:07:06,640 --> 01:07:09,440
обратное распространение по графику

1498
01:07:09,440 --> 01:07:11,680
в обратном направлении таким образом, чтобы мы действовали

1499
01:07:11,680 --> 01:07:13,039
эффективно,

1500
01:07:13,039 --> 01:07:16,400
так что это общая форма

1501
01:07:16,400 --> 01:07:21,760
алгоритма, который работает для произвольного

1502
01:07:21,760 --> 01:07:23,680
графа вычислений,

1503
01:07:23,680 --> 01:07:26,720
поэтому в конце дня у нас есть

1504
01:07:26,720 --> 01:07:29,680
один скалярный выход

1505
01:07:29,680 --> 01:07:30,720
z,

1506
01:07:30,720 --> 01:07:35,119
а затем мы имеем  входные данные и параметры,

1507
01:07:35,119 --> 01:07:36,160
которые

1508
01:07:36,160 --> 01:07:37,200
вычисляют

1509
01:07:37,200 --> 01:07:38,160
z,

1510
01:07:38,160 --> 01:07:40,799
и поэтому, как только у нас есть этот граф вычислений,

1511
01:07:40,799 --> 01:07:44,160
и я добавил сюда эту забавную

1512
01:07:44,160 --> 01:07:46,559
дополнительную стрелку, чтобы сделать его более

1513
01:07:46,559 --> 01:07:49,039
общим графом вычислений, мы

1514
01:07:49,039 --> 01:07:51,119
всегда можем сказать,

1515
01:07:51,119 --> 01:07:53,920
что мы можем разработать начальную точку,

1516
01:07:53,920 --> 01:07:55,440
что не зависит

1517
01:07:55,440 --> 01:07:57,839
ни на что, поэтому в этом случае оба этих

1518
01:07:57,839 --> 01:08:00,640
двух нижних узла не зависят

1519
01:08:00,640 --> 01:08:04,000
ни от чего другого, поэтому мы можем начать с них,

1520
01:08:04,000 --> 01:08:06,160
и мы можем начать вычислять вперед, мы

1521
01:08:06,160 --> 01:08:07,359
можем вычислить

1522
01:08:07,359 --> 01:08:10,000
значения для всех этих s  орт второй

1523
01:08:10,000 --> 01:08:12,400
строки от нижних узлов, а затем мы

1524
01:08:12,400 --> 01:08:13,680
можем вычислить

1525
01:08:15,119 --> 01:08:18,080
третью линзу вверх, чтобы

1526
01:08:18,080 --> 01:08:20,479
мы могли иметь топологическую сортировку

1527
01:08:20,479 --> 01:08:23,120
узлов на основе зависимостей в этом

1528
01:08:23,120 --> 01:08:24,479
ориентированном графе,

1529
01:08:24,479 --> 01:08:27,439
и мы можем вычислить значение каждого

1530
01:08:27,439 --> 01:08:30,399
узла с учетом некоторого  подмножество его

1531
01:08:30,399 --> 01:08:33,679
предшественников, от которых оно зависит, и

1532
01:08:33,679 --> 01:08:35,600
поэтому оно называется фазой прямого

1533
01:08:35,600 --> 01:08:38,399
распространения и дает нам

1534
01:08:38,399 --> 01:08:41,759
вычисление скалярного выхода z

1535
01:08:41,759 --> 01:08:44,158
с использованием наших текущих параметров и наших

1536
01:08:44,158 --> 01:08:46,319
текущих входов,

1537
01:08:46,319 --> 01:08:49,040
а затем, после этого, мы запускаем обратное

1538
01:08:49,040 --> 01:08:52,319
распространение, так что для возврата  распространение мы

1539
01:08:52,319 --> 01:08:55,040
инициализируем выходной градиент

1540
01:08:55,040 --> 01:08:56,479
dz dz

1541
01:08:56,479 --> 01:09:00,719
как единицу, а затем мы посещаем узлы в

1542
01:09:00,719 --> 01:09:04,799
порядке, обратном топологической сортировке,

1543
01:09:04,799 --> 01:09:07,759
и вычисляем градиенты вниз,

1544
01:09:07,759 --> 01:09:11,120
и поэтому наш рецепт заключается в том, что для каждого узла,

1545
01:09:11,120 --> 01:09:14,560
когда мы идем вниз, мы собираемся

1546
01:09:14,560 --> 01:09:17,600
вычислить градиент  узла в

1547
01:09:17,600 --> 01:09:21,120
отношении его успехов и того,

1548
01:09:21,120 --> 01:09:22,880
что он использует,

1549
01:09:22,880 --> 01:09:26,479
и того, как мы вычисляем этот градиент,

1550
01:09:26,479 --> 01:09:29,040
использует это цепное правило, которое мы рассмотрели,

1551
01:09:29,040 --> 01:09:31,359
так что это своего рода обобщенная

1552
01:09:31,359 --> 01:09:34,080
форма o  f правило цепочки, где у нас есть

1553
01:09:34,080 --> 01:09:36,399
несколько выходов, и поэтому мы суммируем

1554
01:09:36,399 --> 01:09:38,479
по различным выходам, а затем для

1555
01:09:38,479 --> 01:09:41,279
каждого выхода мы вычисляем

1556
01:09:41,279 --> 01:09:43,759
произведение восходящего градиента и локального

1557
01:09:43,759 --> 01:09:45,040
градиента

1558
01:09:45,040 --> 01:09:47,759
по отношению к этому узлу, и поэтому мы направляемся

1559
01:09:47,759 --> 01:09:51,040
вниз, и мы  продолжаем вниз

1560
01:09:51,040 --> 01:09:53,679
в обратном топологическом порядке сортировки,

1561
01:09:53,679 --> 01:09:56,480
и мы прорабатываем градиент по

1562
01:09:56,480 --> 01:09:59,360
отношению к каждой переменной

1563
01:09:59,360 --> 01:10:01,520
в этом графике,

1564
01:10:01,520 --> 01:10:05,760
и поэтому мы надеемся, что это выглядит

1565
01:10:05,760 --> 01:10:07,280
интуитивно,

1566
01:10:07,280 --> 01:10:09,600
глядя на эту картинку,

1567
01:10:09,600 --> 01:10:11,840
что,

1568
01:10:11,920 --> 01:10:14,719
если вы думаете об этом так, большая дубовая

1569
01:10:14,719 --> 01:10:17,679
сложность прямого  распространение и

1570
01:10:17,679 --> 01:10:20,560
обратное распространение одинаковы,

1571
01:10:20,560 --> 01:10:24,000
в обоих случаях вы выполняете линейный проход

1572
01:10:24,000 --> 01:10:26,320
через все эти узлы и

1573
01:10:26,320 --> 01:10:30,159
вычисляете значения с учетом предшественников,

1574
01:10:30,159 --> 01:10:32,880
а затем значения с учетом успехов, я имею в виду, что

1575
01:10:32,880 --> 01:10:35,679
вам нужно сделать немного больше работы,

1576
01:10:35,679 --> 01:10:38,480
чтобы разработать  градиенты вроде

1577
01:10:38,480 --> 01:10:40,719
как показано этим правилом цепочки, но это такая

1578
01:10:40,719 --> 01:10:42,880
же большая сложность,

1579
01:10:42,880 --> 01:10:44,640
поэтому, если вы каким-то образом реализуете что-то

1580
01:10:44,640 --> 01:10:46,719
для себя, а не полагаетесь на

1581
01:10:46,719 --> 01:10:49,120
программное обеспечение, и вы  вычисление

1582
01:10:49,120 --> 01:10:51,040
градиентов разного порядка

1583
01:10:51,040 --> 01:10:54,159
сложности прямого распространения

1584
01:10:54,159 --> 01:10:56,239
означает, что вы делаете что-то неправильно,

1585
01:10:56,239 --> 01:10:57,920
вы выполняете повторяющуюся работу, которую

1586
01:10:57,920 --> 01:10:59,520
вам не нужно делать

1587
01:10:59,520 --> 01:11:02,480
нормально, поэтому этот алгоритм работает

1588
01:11:02,480 --> 01:11:05,520
для полностью произвольного

1589
01:11:05,520 --> 01:11:08,880
графа вычислений любой ориентированный ациклический

1590
01:11:08,880 --> 01:11:09,840
граф

1591
01:11:09,840 --> 01:11:12,400
вы можете применить этот алгоритм

1592
01:11:12,400 --> 01:11:16,000
в целом.Мы обнаруживаем, что мы создаем

1593
01:11:16,000 --> 01:11:18,000
нейронные сети с регулярной

1594
01:11:18,000 --> 01:11:20,320
структурой слоев, поэтому у нас есть такие вещи, как

1595
01:11:20,320 --> 01:11:22,400
вектор входных данных, а затем он

1596
01:11:22,400 --> 01:11:25,199
умножается на матрицу, которая преобразуется

1597
01:11:25,199 --> 01:11:27,600
в другой вектор, который

1598
01:11:27,600 --> 01:11:29,920
может быть умножен на другую матрицу  или

1599
01:11:29,920 --> 01:11:31,920
суммировать с другой матрицей, или что-то в этом роде,

1600
01:11:31,920 --> 01:11:34,080
поэтому, как только мы используем такую

1601
01:11:34,080 --> 01:11:36,800
структуру регулярных слоев, мы можем затем

1602
01:11:36,800 --> 01:11:39,679
распараллелить вычисления, разработав

1603
01:11:41,760 --> 01:11:44,320
градиенты в терминах

1604
01:11:44,320 --> 01:11:45,920
якобианов

1605
01:11:45,920 --> 01:11:48,560
векторов и матриц, и делать что-то

1606
01:11:48,560 --> 01:11:51,360
параллельно гораздо более эффективно,

1607
01:11:51,360 --> 01:11:52,400
хорошо,

1608
01:11:52,400 --> 01:11:55,600
так что делаем  тогда это называется

1609
01:11:55,600 --> 01:11:57,840
автоматическим дифференцированием

1610
01:11:57,840 --> 01:12:00,640
и, по сути,

1611
01:12:00,640 --> 01:12:04,159
если вы знаете граф вычислений,

1612
01:12:04,159 --> 01:12:06,239
 могла бы ваша вычислительная

1613
01:12:06,239 --> 01:12:08,480
умная компьютерная система вычислить

1614
01:12:08,480 --> 01:12:10,400
,

1615
01:12:11,760 --> 01:12:14,880
каковы производные всего, а

1616
01:12:14,880 --> 01:12:16,719
затем

1617
01:12:16,719 --> 01:12:20,080
применить обратное распространение, чтобы выяснить, как

1618
01:12:20,080 --> 01:12:22,480
обновить параметры и учиться,

1619
01:12:22,480 --> 01:12:24,719
и на самом деле есть что-то вроде

1620
01:12:24,719 --> 01:12:26,640
интересного,

1621
01:12:26,640 --> 01:12:27,920
вроде того,

1622
01:12:27,920 --> 01:12:30,560
как история  пошел назад,

1623
01:12:31,520 --> 01:12:33,120
что я просто

1624
01:12:33,120 --> 01:12:35,040
отмечу,

1625
01:12:35,040 --> 01:12:36,960
поэтому некоторые из вас могут быть

1626
01:12:36,960 --> 01:12:40,960
знакомы с

1627
01:12:40,960 --> 01:12:43,600
пакетами символьных вычислений, так что это такие вещи,

1628
01:12:43,600 --> 01:12:47,040
как математика, так что математика, вы можете

1629
01:12:47,040 --> 01:12:51,040
дать ему символическую форму вычисления,

1630
01:12:51,040 --> 01:12:53,600
а затем он может вычислить производные для

1631
01:12:53,600 --> 01:12:54,400
вы,

1632
01:12:54,400 --> 01:12:56,640
поэтому должно быть так, что если вы

1633
01:12:56,640 --> 01:12:58,640
дадите полную символьную форму

1634
01:12:58,640 --> 01:13:01,280
графа вычислений um,

1635
01:13:01,280 --> 01:13:03,360
тогда он должен иметь возможность вычислить

1636
01:13:03,360 --> 01:13:05,840
все производные для вас, и

1637
01:13:05,840 --> 01:13:07,679
вам никогда не придется разрабатывать

1638
01:13:07,679 --> 01:13:09,600
производную вручную,

1639
01:13:09,600 --> 01:13:12,159
и это было фактически попыткой  в

1640
01:13:12,159 --> 01:13:14,880
известной библиотеке глубокого обучения под названием

1641
01:13:14,880 --> 01:13:17,440
fianno, которая вышла из группы Джошуа

1642
01:13:18,480 --> 01:13:20,800
Бендио в университете Монреаля,

1643
01:13:20,800 --> 01:13:23,280
что у нее был компилятор, который делал такого

1644
01:13:23,280 --> 01:13:25,679
рода символические  манипуляция

1645
01:13:25,679 --> 01:13:26,480
ммм,

1646
01:13:26,480 --> 01:13:27,280
но

1647
01:13:27,280 --> 01:13:30,800
вы знаете, что это вроде как оказалось

1648
01:13:33,440 --> 01:13:36,080
слишком сложным путем, по которому идти, я полагаю, он на

1649
01:13:36,080 --> 01:13:37,600
самом деле может вернуться снова в

1650
01:13:37,600 --> 01:13:39,760
будущем, и поэтому

1651
01:13:39,760 --> 01:13:40,800
для

1652
01:13:40,800 --> 01:13:42,880
современных фреймворков глубокого обучения, которые

1653
01:13:42,880 --> 01:13:46,480
включают как тензорный поток, так и пи-факел,

1654
01:13:46,480 --> 01:13:47,920
они делают

1655
01:13:47,920 --> 01:13:49,360
90 процентов

1656
01:13:49,360 --> 01:13:52,800
Это вычисление автоматического

1657
01:13:52,800 --> 01:13:55,280
дифференцирования для вас,

1658
01:13:55,280 --> 01:13:57,360
но на самом деле они не

1659
01:13:57,360 --> 01:14:00,480
вычисляют производные символически, поэтому для каждого

1660
01:14:00,480 --> 01:14:04,560
конкретного узла или уровня вашей

1661
01:14:04,560 --> 01:14:06,159
системы глубокого обучения

1662
01:14:06,159 --> 01:14:10,080
кто-то, будь то вы или человек, который

1663
01:14:10,080 --> 01:14:11,760
написал

1664
01:14:11,760 --> 01:14:12,800
этот

1665
01:14:12,800 --> 01:14:15,679
слой, вручную написал локальные

1666
01:14:15,679 --> 01:14:18,239
производные, но затем все  с

1667
01:14:18,239 --> 01:14:21,600
этого момента, вроде принятия

1668
01:14:21,600 --> 01:14:24,320
цепного правила объединения

1669
01:14:24,320 --> 01:14:27,199
восходящих градиентов с локальными градиентами

1670
01:14:27,199 --> 01:14:29,280
для разработки нисходящих градиентов, которые

1671
01:14:29,280 --> 01:14:31,840
затем все выполняются автоматически для

1672
01:14:31,840 --> 01:14:34,239
обратного распространения на графе вычислений,

1673
01:14:35,840 --> 01:14:38,560
и это означает, что это означает

1674
01:14:38,560 --> 01:14:41,040
для всей нейронной сети  у вас есть

1675
01:14:41,040 --> 01:14:43,600
граф вычислений, и он будет

1676
01:14:43,600 --> 01:14:45,280
иметь прямой

1677
01:14:45,280 --> 01:14:47,600
проход и обратный проход,

1678
01:14:47,600 --> 01:14:50,239
и поэтому для прямого  ard pass вы

1679
01:14:50,239 --> 01:14:53,280
топологически сортируете узлы на основе

1680
01:14:53,280 --> 01:14:55,920
их зависимостей в графе вычислений,

1681
01:14:55,920 --> 01:14:59,199
а затем для каждого

1682
01:14:59,199 --> 01:15:01,679
узла, который вы выполняете,

1683
01:15:01,679 --> 01:15:04,560
прямое вычисление на этом узле, а

1684
01:15:04,560 --> 01:15:06,800
затем для обратного распространения вы

1685
01:15:06,800 --> 01:15:08,960
меняете топологическую сортировку

1686
01:15:08,960 --> 01:15:11,520
графа, а затем для  каждый узел в

1687
01:15:11,520 --> 01:15:13,840
графе вы запускаете обратное

1688
01:15:13,840 --> 01:15:16,159
распространение, которое является небольшим

1689
01:15:16,159 --> 01:15:19,199
фоном для правила цепочки на этом узле, а

1690
01:15:19,199 --> 01:15:21,280
затем результатом этого является то, что у вас

1691
01:15:21,280 --> 01:15:23,760
есть градиенты

1692
01:15:23,760 --> 01:15:24,480
для

1693
01:15:24,480 --> 01:15:28,000
ваших входных данных и параметров,

1694
01:15:28,000 --> 01:15:29,840
и

1695
01:15:29,840 --> 01:15:32,719
это общее программное обеспечение, которое запускает это

1696
01:15:32,719 --> 01:15:35,760
для вас и поэтому то, что вы

1697
01:15:35,760 --> 01:15:38,400
хотите сделать, это действительно

1698
01:15:38,400 --> 01:15:40,320
иметь материал

1699
01:15:40,320 --> 01:15:43,040
для определенных узлов или слоев в

1700
01:15:43,040 --> 01:15:46,000
графе, поэтому, если у меня есть вентиль умножения,

1701
01:15:46,000 --> 01:15:48,480
он будет иметь прямой

1702
01:15:48,480 --> 01:15:50,840
алгоритм, который просто вычисляет, что

1703
01:15:50,840 --> 01:15:54,640
результат x умножен на y с точки зрения  два

1704
01:15:54,640 --> 01:15:56,880
входа, а затем я собираюсь

1705
01:15:56,880 --> 01:16:00,159
вычислить, чтобы сказать ему также, как

1706
01:16:00,159 --> 01:16:02,880
вычислить локальную производную, поэтому я хочу сказать,

1707
01:16:02,880 --> 01:16:05,920
что такое локальная производная,

1708
01:16:05,920 --> 01:16:06,719
поэтому

1709
01:16:06,719 --> 01:16:10,880
dl dx и ldy в терминах

1710
01:16:10,880 --> 01:16:13,760
восходящий градиент dldz,

1711
01:16:13,760 --> 01:16:14,880
и поэтому

1712
01:16:14,880 --> 01:16:17,840
я затем вручную выясню, как это

1713
01:16:17,840 --> 01:16:20,400
вычислить, и обычно мне нужно

1714
01:16:20,400 --> 01:16:21,360
сделать, чтобы

1715
01:16:22,400 --> 01:16:26,159
я предполагал, что сначала выполняется прямой проход,

1716
01:16:26,159 --> 01:16:29,520
и я собираюсь вставить в некоторые

1717
01:16:29,520 --> 01:16:32,880
локальные переменные для своего класса значения,

1718
01:16:32,880 --> 01:16:34,320
которые были  используется в прямом

1719
01:16:34,320 --> 01:16:36,880
вычислении, так что, а также вычисление z

1720
01:16:36,880 --> 01:16:39,120
равно x умножить на y,

1721
01:16:39,120 --> 01:16:41,440
я собираюсь как бы вспомнить, какие x и

1722
01:16:41,440 --> 01:16:42,960
y были,

1723
01:16:42,960 --> 01:16:46,000
чтобы затем, когда меня попросят

1724
01:16:46,000 --> 01:16:49,040
вычислить обратный проход, я тогда собираюсь

1725
01:16:49,040 --> 01:16:52,719
реализовать здесь  гм, что мы видели ранее,

1726
01:16:53,679 --> 01:16:56,320
что когда наступает xy, вы собираетесь как

1727
01:16:56,320 --> 01:16:59,679
бы поменять местами y и x um, чтобы

1728
01:17:00,640 --> 01:17:03,199
выработать локальные градиенты, а затем я

1729
01:17:03,199 --> 01:17:05,280
собираюсь умножить их на восходящий

1730
01:17:05,280 --> 01:17:07,920
градиент, и я собираюсь  чтобы вернуться,

1731
01:17:07,920 --> 01:17:09,760
я только что написал это здесь как своего рода

1732
01:17:09,760 --> 01:17:12,320
небольшой список, но на самом деле это

1733
01:17:12,320 --> 01:17:16,960
будет набор градиентов,

1734
01:17:16,960 --> 01:17:18,480
хорошо,

1735
01:17:18,480 --> 01:17:21,920
так что это 98 из того, что я хотел

1736
01:17:21,920 --> 01:17:25,679
осветить сегодня, просто пару быстрых

1737
01:17:26,800 --> 01:17:29,199
комментариев  осталось, так

1738
01:17:29,199 --> 01:17:32,640
что это можно и нужно автоматизировать,

1739
01:17:32,640 --> 01:17:34,320
иногда вы хотите просто  проверьте

1740
01:17:34,320 --> 01:17:35,520
,

1741
01:17:35,520 --> 01:17:38,239
вычисляете ли вы правильные градиенты, и поэтому

1742
01:17:38,239 --> 01:17:40,640
стандартный способ проверки того, что вы

1743
01:17:40,640 --> 01:17:43,120
вычисляете правильные градиенты, - это

1744
01:17:43,120 --> 01:17:46,080
вручную вычислить градиент,

1745
01:17:46,080 --> 01:17:48,800
выполнив числовой расчет градиента,

1746
01:17:48,800 --> 01:17:49,840
и

1747
01:17:49,840 --> 01:17:52,640
поэтому вы можете сделать это, чтобы вы могли работать  выяснить,

1748
01:17:52,640 --> 01:17:55,280
какой

1749
01:17:55,280 --> 01:17:59,120
должна быть производная x от f по x,

1750
01:17:59,120 --> 01:18:01,679
выбрав какое-то небольшое число,

1751
01:18:01,679 --> 01:18:03,679
например 10, к минусу 4,

1752
01:18:03,679 --> 01:18:06,880
добавив его к x, вычтя его из x, а

1753
01:18:06,880 --> 01:18:08,400
затем так, чтобы разница между этими

1754
01:18:08,400 --> 01:18:12,080
числами составила 2h, разделив его на 2h

1755
01:18:12,080 --> 01:18:14,159
и вы просто вычисляете подъем

1756
01:18:14,159 --> 01:18:16,560
по пробегу, который является наклоном этой

1757
01:18:16,560 --> 01:18:19,440
точки по отношению к x, и это

1758
01:18:19,440 --> 01:18:22,560
приближение градиента f по

1759
01:18:22,560 --> 01:18:24,560
отношению к x

1760
01:18:24,560 --> 01:18:28,560
при этом значении x, так что это настолько просто, что

1761
01:18:28,560 --> 01:18:30,560
вы не можете  сделать ошибку при реализации

1762
01:18:30,560 --> 01:18:32,719
этого, и поэтому вы можете использовать это,

1763
01:18:32,719 --> 01:18:35,120
чтобы проверить, где у вас

1764
01:18:35,120 --> 01:18:38,640
правильные значения градиента или нет,

1765
01:18:38,640 --> 01:18:40,320
это не то, что вы хотели бы

1766
01:18:40,320 --> 01:18:42,800
использовать много, потому что это не только

1767
01:18:42,800 --> 01:18:45,440
приблизительное, но и очень медленное

1768
01:18:45,440 --> 01:18:47,760
потому что  чтобы решить эту проблему, вам нужно

1769
01:18:47,760 --> 01:18:50,239
выполнить прямое вычисление для каждого

1770
01:18:50,239 --> 01:18:52,480
параметра модели, поэтому, если у вас есть

1771
01:18:52,480 --> 01:18:54,480
модель с миллионом параметров, вы

1772
01:18:54,480 --> 01:18:56,640
теперь выполняете в миллион раз больше работы,

1773
01:18:56,640 --> 01:18:58,880
чтобы запустить обратную опору,

1774
01:18:58,880 --> 01:19:00,960
чем если бы вы сделали, если бы вы '  на самом деле используется

1775
01:19:00,960 --> 01:19:03,280
исчисление, поэтому исчисление - это хорошая вещь, которую нужно

1776
01:19:03,280 --> 01:19:06,000
знать, но может быть действительно полезно

1777
01:19:06,000 --> 01:19:09,360
проверить, что правильные значения

1778
01:19:09,360 --> 01:19:11,520
вычисляются в старые времена, когда мы вручную

1779
01:19:11,520 --> 01:19:13,040
писали все,

1780
01:19:13,040 --> 01:19:15,199
это был своего рода ключевой модульный тест, который

1781
01:19:15,199 --> 01:19:17,199
люди использовали повсюду в

1782
01:19:17,199 --> 01:19:18,880
наши дни  большую часть времени вы

1783
01:19:18,880 --> 01:19:21,520
повторно используете слои, встроенные в пирог-

1784
01:19:21,520 --> 01:19:23,040
факел или какую-либо другую структуру глубокого обучения,

1785
01:19:23,040 --> 01:19:25,360
поэтому в нем гораздо меньше

1786
01:19:25,360 --> 01:19:27,199
необходимости, но иногда вы реализуете

1787
01:19:27,199 --> 01:19:29,360
свой собственный слой и действительно хотите

1788
01:19:29,360 --> 01:19:30,640
проверить, что все реализовано

1789
01:19:30,640 --> 01:19:32,159
правильно,

1790
01:19:32,159 --> 01:19:34,080
есть штраф  Обратите внимание на то, как это

1791
01:19:34,080 --> 01:19:37,280
написано, если бы вы видели

1792
01:19:37,280 --> 01:19:40,080
это на уроках математического анализа в старшей школе,

1793
01:19:40,080 --> 01:19:43,280
вы бы увидели, как увеличивается число f,

1794
01:19:43,280 --> 01:19:50,239
равное x, плюс h минус f, x, деленное на h,

1795
01:19:50,239 --> 01:19:52,560
оказывается, что выполнение этой двусторонней

1796
01:19:52,560 --> 01:19:55,840
оценки  Это намного

1797
01:19:55,840 --> 01:19:58,400
точнее, чем односторонняя оценка,

1798
01:19:58,400 --> 01:20:00,640
и поэтому вам действительно очень рекомендуется

1799
01:20:00,640 --> 01:20:03,280
использовать это приближение,

1800
01:20:03,280 --> 01:20:05,840
хорошо, так что в этот момент мы

1801
01:20:05,840 --> 01:20:08,679
освоили основную технологию нейронных сетей.

1802
01:20:11,920 --> 01:20:14,800
цепное правило

1803
01:20:14,800 --> 01:20:16,880
вдоль графа вычислений с

1804
01:20:16,880 --> 01:20:19,440
таким ключевым шагом, что градиент вниз по потоку

1805
01:20:19,440 --> 01:20:22,480
равен восходящему абстрактному градиенту восходящего

1806
01:20:22,480 --> 01:20:25,440
потока, умноженному на локальный градиент,

1807
01:20:25,440 --> 01:20:26,719
и поэтому

1808
01:20:26,719 --> 01:20:28,719
для вычислений с помощью нейронных сетей мы

1809
01:20:28,719 --> 01:20:31,440
выполняем прямой проход, чтобы вычислить значения с

1810
01:20:31,440 --> 01:20:33,600
текущими параметрами, затем запускаем обратное

1811
01:20:33,600 --> 01:20:35,120
распространение

1812
01:20:35,120 --> 01:20:37,520
и работаем  Из

1813
01:20:37,520 --> 01:20:40,800
градиента потерь и в настоящее время вычисленных потерь

1814
01:20:40,800 --> 01:20:44,560
относительно этих параметров

1815
01:20:44,960 --> 01:20:46,880
теперь в некоторой степени

1816
01:20:47,679 --> 01:20:49,199
вы знаете, что с современными фреймворками глубокого обучения

1817
01:20:49,199 --> 01:20:50,880
вам на самом деле не нужно

1818
01:20:50,880 --> 01:20:52,960
знать, как делать что-либо из этого правильно, это

1819
01:20:52,960 --> 01:20:54,719
похоже на то, что вы делаете  не нужно

1820
01:20:54,719 --> 01:20:58,320
знать, как реализовать компилятор ac, вы

1821
01:20:58,320 --> 01:21:02,239
можете просто написать код c и сказать gcc, и

1822
01:21:02,239 --> 01:21:05,040
он скомпилирует его, и он запустит

1823
01:21:05,040 --> 01:21:08,480
нужный вам материал  гм, и это

1824
01:21:08,480 --> 01:21:10,239
та функциональность, которую вы получаете от

1825
01:21:10,239 --> 01:21:13,120
фреймворка pytorch, так что приходите к

1826
01:21:13,120 --> 01:21:15,840
руководству по пирогу в эту пятницу и

1827
01:21:15,840 --> 01:21:18,159
получите представление о том, насколько легко

1828
01:21:18,159 --> 01:21:20,880
писать новые сети с помощью фреймворка,

1829
01:21:20,880 --> 01:21:23,920
такого как pytorch или tensorflow, и вы знаете,

1830
01:21:23,920 --> 01:21:26,400
что это так  легко, поэтому вы знаете, что

1831
01:21:26,400 --> 01:21:28,400
старшеклассники по всей

1832
01:21:28,400 --> 01:21:31,280
стране сейчас выполняют свои научные проекты,

1833
01:21:31,280 --> 01:21:33,120
тренируя системы глубокого обучения, потому что

1834
01:21:33,120 --> 01:21:35,120
вам на самом деле не нужно

1835
01:21:35,120 --> 01:21:37,760
много разбираться в нескольких

1836
01:21:37,760 --> 01:21:39,840
уровнях нейронной сети вместе и настраивать их

1837
01:21:39,840 --> 01:21:41,120
вычисление

1838
01:21:41,120 --> 01:21:42,480
на некоторых данных,

1839
01:21:42,480 --> 01:21:44,400
но вы знаете, что мы  надеюсь в этом классе, что

1840
01:21:44,400 --> 01:21:47,199
вы на самом деле также изучаете, как эти

1841
01:21:47,199 --> 01:21:49,120
вещи реализованы,

1842
01:21:49,920 --> 01:21:52,080
так что у вас есть более глубокое понимание,

1843
01:21:52,080 --> 01:21:54,639
чем это, и вы знаете, что

1844
01:21:54,639 --> 01:21:56,320
иногда вам нужно иметь более глубокое

1845
01:21:56,320 --> 01:21:58,800
понимание, поэтому обратное распространение

1846
01:21:58,800 --> 01:22:01,600
не всегда работает тщательно и идеально

1847
01:22:01,600 --> 01:22:03,440
и поэтому понимание того, что он на самом деле

1848
01:22:03,440 --> 01:22:06,560
делает, может иметь решающее значение для отладки,

1849
01:22:06,560 --> 01:22:08,239
и поэтому мы на самом деле увидим пример

1850
01:22:08,239 --> 01:22:10,960
этого довольно скоро, когда начнем lo  столкнувшись

1851
01:22:10,960 --> 01:22:12,719
с повторяющимися моделями и некоторыми

1852
01:22:12,719 --> 01:22:14,639
проблемами, которые у них есть, что

1853
01:22:14,639 --> 01:22:17,040
потребует от нас более глубокого размышления

1854
01:22:17,040 --> 01:22:18,719
о том, что происходит в наших вычислениях градиента,

1855
01:22:20,560 --> 01:22:22,000
хорошо,

1856
01:22:22,000 --> 01:22:25,560
на сегодня все

