1
00:00:05,180 --> 00:00:07,860
All right. We're going to go ahead and get started.

2
00:00:07,860 --> 00:00:10,625
Um, uh, before I get into the technical stuff,

3
00:00:10,625 --> 00:00:12,085
we'll do a little bit of logistics.

4
00:00:12,085 --> 00:00:15,790
Um, so, we are starting these things called sessions.

5
00:00:15,790 --> 00:00:17,530
Um, we announced them on Piazza.

6
00:00:17,530 --> 00:00:18,760
If you didn't, uh,

7
00:00:18,760 --> 00:00:20,005
if you're not getting our Piazza stuff,

8
00:00:20,005 --> 00:00:23,035
definitely make sure you've signed up for Piazza, or send us a note.

9
00:00:23,035 --> 00:00:26,640
Um, the sessions are designed to go into the material a little bit deeper,

10
00:00:26,640 --> 00:00:28,750
also to discuss something about the homework.

11
00:00:28,750 --> 00:00:32,340
Um, these are structured session as opposed to an office hour,

12
00:00:32,340 --> 00:00:34,770
where you can ask one-on-one questions about the homework .

13
00:00:34,770 --> 00:00:38,110
the sessions are designed to go a little bit deeper into the material,

14
00:00:38,110 --> 00:00:41,290
and they were prompted both based on some colleagues of mine,

15
00:00:41,290 --> 00:00:44,265
feedback of how much students have liked them in their other classes,

16
00:00:44,265 --> 00:00:48,580
as well as some request from last year for opportunities to go deeper into the material.

17
00:00:48,580 --> 00:00:51,025
So, we've announced these on Piazza.

18
00:00:51,025 --> 00:00:53,330
The idea is that you will sign up for a session.

19
00:00:53,330 --> 00:00:55,505
They are optional, you don't have to do them.

20
00:00:55,505 --> 00:00:57,740
We will be giving one percent extra credit for

21
00:00:57,740 --> 00:01:00,230
attending them if you attend a sufficient number of them.

22
00:01:00,230 --> 00:01:02,960
Um, the details for that has also been announced,

23
00:01:02,960 --> 00:01:05,069
um, and I, I think that's true.

24
00:01:05,069 --> 00:01:06,375
If I've got that right, just email me.

25
00:01:06,375 --> 00:01:08,355
I think that's been announced also on Piazza.

26
00:01:08,355 --> 00:01:10,000
Um, so, if you go to Piazza,

27
00:01:10,000 --> 00:01:11,990
there's a number of different sessions you can sign up for.

28
00:01:11,990 --> 00:01:15,035
The point of signing up for them is to make sure that we have room capacity,

29
00:01:15,035 --> 00:01:18,740
but I'm pretty sure we'll be able to accommodate almost any session you want to go to.

30
00:01:18,740 --> 00:01:21,230
The last session will be done via Zoom,

31
00:01:21,230 --> 00:01:23,930
and it's particularly targeted at SCPD students,

32
00:01:23,930 --> 00:01:25,495
but anyone is welcome to do it.

33
00:01:25,495 --> 00:01:27,680
Um, the way that we'll be keeping

34
00:01:27,680 --> 00:01:30,050
track of whether or not people are going to sessions or not is,

35
00:01:30,050 --> 00:01:31,550
we will have a code that's, uh,

36
00:01:31,550 --> 00:01:33,740
mentioned inside of the,

37
00:01:33,740 --> 00:01:35,190
the material, and so, then,

38
00:01:35,190 --> 00:01:41,390
you will just write in that code to indicate your attendance. Um, we'll record the last session so that if, for some reason,

39
00:01:41,390 --> 00:01:43,400
your schedule is such that you can't attend

40
00:01:43,400 --> 00:01:45,440
any of these but you want to participate in session,

41
00:01:45,440 --> 00:01:47,900
you can go through the material later and then

42
00:01:47,900 --> 00:01:50,825
record that you attended it by using that code.

43
00:01:50,825 --> 00:01:53,360
And we'll be relying on the Stanford Honor Code,

44
00:01:53,360 --> 00:01:56,590
that only people that are doing this will upload the codes.

45
00:01:56,590 --> 00:02:00,225
Somebody had any question about sessions and what those involve?

46
00:02:00,225 --> 00:02:03,105
Again, they're optional, they're way to go deeper into the material.

47
00:02:03,105 --> 00:02:06,660
Um, some other people have really liked these sort of things.

48
00:02:06,660 --> 00:02:08,430
You can see what you think, it's an experiment.

49
00:02:08,430 --> 00:02:12,810
All right.

50
00:02:12,810 --> 00:02:15,080
Any questions about anything else outside of sessions?

51
00:02:15,080 --> 00:02:16,470
So, homework's been released,

52
00:02:16,470 --> 00:02:18,855
office hour is happening as usual this week.

53
00:02:18,855 --> 00:02:23,425
Feel free to come talk to us or use Piazza for any questions that you have.

54
00:02:23,425 --> 00:02:25,850
All right. We're gonna go ahead and get started now then.

55
00:02:25,850 --> 00:02:28,910
Um, as usual, I really appreciate

56
00:02:28,910 --> 00:02:32,180
it if you use your name whenever you're asking a [NOISE] question or making a comment.

57
00:02:32,180 --> 00:02:34,490
So, today, we're gonna finally start to get into

58
00:02:34,490 --> 00:02:37,160
making decisions where we don't have a model of the world,

59
00:02:37,160 --> 00:02:40,640
and in particular, we are going to be focusing on model-free control.

60
00:02:40,640 --> 00:02:43,460
[NOISE] So, the things that we're going be

61
00:02:43,460 --> 00:02:46,400
covering today is really focusing on how can an agent start

62
00:02:46,400 --> 00:02:49,010
to be making good decisions when it doesn't know how the world

63
00:02:49,010 --> 00:02:52,460
works and it's not going to be explicitly constructing a model?

64
00:02:52,460 --> 00:02:55,790
Um, and remember, a model, in this case,

65
00:02:55,790 --> 00:02:59,900
is going to be a reward and/or a dynamics model of the environment.

66
00:02:59,900 --> 00:03:03,740
So, today, we're gonna be looking at methods that do not involve constructing a verbal,

67
00:03:03,740 --> 00:03:05,765
um, a dynamics or a reward model,

68
00:03:05,765 --> 00:03:08,060
but it's just going to be directly learning from experience.

69
00:03:08,060 --> 00:03:14,720
[NOISE] So, um, [NOISE] before- we were mostly talking last time about,

70
00:03:14,720 --> 00:03:16,170
well, maybe we don't know how the world works,

71
00:03:16,170 --> 00:03:18,440
we don't have these explicit dynamics and reward models,

72
00:03:18,440 --> 00:03:21,800
but we're going to be trying to evaluate a policy that was provided to us.

73
00:03:21,800 --> 00:03:23,480
And now, we're going to be thinking about

74
00:03:23,480 --> 00:03:26,150
the real problems that often comes up in reinforcement learning, which is,

75
00:03:26,150 --> 00:03:29,360
how should an agent make decisions when they don't know how the world works,

76
00:03:29,360 --> 00:03:34,470
and they still want to maximize [NOISE] their expected discounted sum of rewards.

77
00:03:35,090 --> 00:03:39,440
So, when we think about sort of how good is the policy,

78
00:03:39,440 --> 00:03:41,990
as soon as we have information about how good a policy is,

79
00:03:41,990 --> 00:03:45,290
then we can start to think about how do we learn a good policy instead.

80
00:03:45,290 --> 00:03:48,150
And, in fact, when we started off at the very start of the class,

81
00:03:48,150 --> 00:03:51,275
we'd talked about how you would learn to make good decisions or how

82
00:03:51,275 --> 00:03:54,650
you would compute good decisions if you were given a model of the world.

83
00:03:54,650 --> 00:03:57,405
And so, that's what we're gonna be going back to now.

84
00:03:57,405 --> 00:03:59,760
So, in particular, now,

85
00:03:59,760 --> 00:04:04,475
we can think of starting to get at this issue of this optimization and exploration.

86
00:04:04,475 --> 00:04:06,815
We're still not going to get into generalization yet.

87
00:04:06,815 --> 00:04:09,850
Um, this will be happening soon.

88
00:04:09,850 --> 00:04:13,809
Um, we've already seen this a little bit it came up with planning,

89
00:04:13,809 --> 00:04:17,440
but now,

90
00:04:17,440 --> 00:04:20,980
we're going to start to think about how do we explore and how do we do optimization.

91
00:04:20,980 --> 00:04:27,015
So, when we think about- well,

92
00:04:27,015 --> 00:04:29,100
I, I think I'm just gonna go through more of these as we start to,

93
00:04:29,100 --> 00:04:31,080
to go into this area.

94
00:04:31,080 --> 00:04:32,190
So, um, again,

95
00:04:32,190 --> 00:04:34,600
we're going to be thinking about how do we identify a policy

96
00:04:34,600 --> 00:04:37,345
that has a high expected discounted sum of rewards.

97
00:04:37,345 --> 00:04:40,060
There's going to be delayed consequences, which means is,

98
00:04:40,060 --> 00:04:42,360
our agent takes actions that may not see the result

99
00:04:42,360 --> 00:04:45,040
of whether or not those actions were good or bad for a while,

100
00:04:45,040 --> 00:04:48,620
and we're going to start to think about this exploration aspect.

101
00:04:49,200 --> 00:04:53,160
Okay. So, let's start with, um,  you know,

102
00:04:53,160 --> 00:04:56,340
where these types of problems come up and where people model things when

103
00:04:56,340 --> 00:04:59,930
we're thinking about Markov decision processes and maybe not building a model.

104
00:04:59,930 --> 00:05:03,590
So, I think probably one of the first really big examples of success for

105
00:05:03,590 --> 00:05:05,690
doing reinforcement learning and doing it in

106
00:05:05,690 --> 00:05:08,105
this sort of model free way was for Backgammon,

107
00:05:08,105 --> 00:05:10,190
which was roughly 1994.

108
00:05:10,190 --> 00:05:12,775
They trained an agent to play Backgammon,

109
00:05:12,775 --> 00:05:14,140
the board game, um,

110
00:05:14,140 --> 00:05:16,660
I- actually using the- a neural network.

111
00:05:16,660 --> 00:05:20,380
Um, neural networks went sort of out of fashion for probably around 10, 15 years,

112
00:05:20,380 --> 00:05:22,410
and then came back, but in the early '90s,

113
00:05:22,410 --> 00:05:24,105
people were using neural networks.

114
00:05:24,105 --> 00:05:29,010
Um, and, uh, Gerald Tesauro used it for Backgammon and got some very nice results,

115
00:05:29,010 --> 00:05:32,135
and that was sort of one of the first demonstrations of reinforcement learning

116
00:05:32,135 --> 00:05:35,540
in kind of a larger setting that you could solve these sort of complicated games.

117
00:05:35,540 --> 00:05:39,605
Um, many other g- problems can also be modeled in MDP,

118
00:05:39,605 --> 00:05:44,795
whether games or robots or customer ad selection or invasive species management.

119
00:05:44,795 --> 00:05:46,130
Um, and in many of these cases,

120
00:05:46,130 --> 00:05:48,050
we don't know the models in, i- in advance.

121
00:05:48,050 --> 00:05:51,350
So, what we're going to be thinking about today is, um,

122
00:05:51,350 --> 00:05:54,365
situations in particular, mostly here,

123
00:05:54,365 --> 00:05:56,090
where we think about the model being unknown,

124
00:05:56,090 --> 00:05:57,785
but if we can sample it.

125
00:05:57,785 --> 00:06:01,280
But there are occasionally cases too [NOISE] where you do know the model,

126
00:06:01,280 --> 00:06:03,685
but it's [NOISE] really, really expensive.

127
00:06:03,685 --> 00:06:08,120
So, for something like computational sustainability or climate modeling,

128
00:06:08,120 --> 00:06:10,700
you might be able to write down a good model of the world, but it's really,

129
00:06:10,700 --> 00:06:13,850
really expensive to run because actually simulating the climate is really,

130
00:06:13,850 --> 00:06:15,650
really compete- really, really hard,

131
00:06:15,650 --> 00:06:17,330
and even then, your model will probably be about.

132
00:06:17,330 --> 00:06:20,570
But I just- I raise this second point in

133
00:06:20,570 --> 00:06:22,430
the sense that when we mostly think about sort of

134
00:06:22,430 --> 00:06:24,320
learning from the world, we think of a robot,

135
00:06:24,320 --> 00:06:26,225
like, running around in the world, and,

136
00:06:26,225 --> 00:06:28,565
and that being an expensive thing to do because

137
00:06:28,565 --> 00:06:31,415
robots are taking real time to, to do this.

138
00:06:31,415 --> 00:06:33,200
But you can also think about

139
00:06:33,200 --> 00:06:36,030
agents that are learning to sort of interact with a simulator,

140
00:06:36,030 --> 00:06:37,260
where that's also really costly.

141
00:06:37,260 --> 00:06:40,890
All right.

142
00:06:40,890 --> 00:06:43,190
So, what we're going to be thinking about mostly

143
00:06:43,190 --> 00:06:45,815
today is what is known as on-policy learning,

144
00:06:45,815 --> 00:06:48,590
where we get direct experience about the world, [NOISE] and then,

145
00:06:48,590 --> 00:06:53,690
we try to use it to estimate and evaluate a policy from that experience.

146
00:06:53,690 --> 00:06:57,430
But we're also going to stock- start to talk more about off-policy learning,

147
00:06:57,430 --> 00:07:00,210
where we get data about the world and we use it

148
00:07:00,210 --> 00:07:03,640
to [NOISE] estimate alternative ways of doing things.

149
00:07:03,640 --> 00:07:07,430
So, we can kind of combine experience from trying out different things,

150
00:07:07,430 --> 00:07:10,545
to try to learn about something we didn't do by itself.

151
00:07:10,545 --> 00:07:13,020
And, um, this three thing's really important. So, I'm- all right.

152
00:07:13,020 --> 00:07:14,040
The second thing is really important,

153
00:07:14,040 --> 00:07:16,380
so I'm just gonna talk about it briefly up here.

154
00:07:16,380 --> 00:07:18,290
So, imagine you have a case where,

155
00:07:18,290 --> 00:07:20,600
say, there's only a single state for now,

156
00:07:20,600 --> 00:07:22,880
but it's like, you have a state S1, you do A1,

157
00:07:22,880 --> 00:07:25,245
you stay at S1, then you do A1.

158
00:07:25,245 --> 00:07:27,704
Or you're, you're in S1, you do A2,

159
00:07:27,704 --> 00:07:29,460
so you're in S1, and then, you do A2.

160
00:07:29,460 --> 00:07:32,510
[NOISE] So, you'd like to be able to kind of

161
00:07:32,510 --> 00:07:36,410
combine between these experiences so you could learn about doing this.

162
00:07:36,410 --> 00:07:40,445
[NOISE] even though you've never done that in the world.

163
00:07:40,445 --> 00:07:42,710
You've never experienced that full trajectory,

164
00:07:42,710 --> 00:07:46,580
but you'd like to be able to sort of extrapolate from that, that prior experience.

165
00:07:46,580 --> 00:07:51,390
So, [NOISE] this sort of policy would be an off pol- [NOISE] uh,

166
00:07:51,390 --> 00:07:56,990
an off-policy learning because it's different than the previous policies we've tried.

167
00:07:56,990 --> 00:08:00,125
We'll go into that more when we think about Q-learning.

168
00:08:00,125 --> 00:08:03,535
All right. So, let's start with generalized policy iteration.

169
00:08:03,535 --> 00:08:05,910
Okay. So, if we go back to policy iteration,

170
00:08:05,910 --> 00:08:07,920
we talked about that a couple lectures ago,

171
00:08:07,920 --> 00:08:11,935
and policy iteration, we originally saw it when we knew the model of the world.

172
00:08:11,935 --> 00:08:15,200
So, it's a way for us to compute what was the right thing to do

173
00:08:15,200 --> 00:08:17,600
given- right thing meaning the policy that

174
00:08:17,600 --> 00:08:20,240
maximizes our expected discounted sum of rewards.

175
00:08:20,240 --> 00:08:22,830
So, how do we do this when we know how the world works?

176
00:08:22,830 --> 00:08:24,875
We're given our dynamics and reward model.

177
00:08:24,875 --> 00:08:28,695
In that case, we initialize some policy, probably randomly.

178
00:08:28,695 --> 00:08:31,260
So, initializing, again, would mean that we'd said,

179
00:08:31,260 --> 00:08:34,544
pi of S equal to some A for all S,

180
00:08:34,544 --> 00:08:37,579
and this is generally probably going to be chosen at random.

181
00:08:37,580 --> 00:08:40,880
[NOISE] Um, and then,

182
00:08:40,880 --> 00:08:43,190
we did this policy evaluation procedure,

183
00:08:43,190 --> 00:08:46,610
where we first computed the current value of the policy,

184
00:08:46,610 --> 00:08:49,115
and then, we updated the policy.

185
00:08:49,115 --> 00:08:53,240
So, we took whatever we had, and then,

186
00:08:53,240 --> 00:08:54,590
we did this sort of one more thing,

187
00:08:54,590 --> 00:08:57,715
which you can think of as kinda doing one more Bellman backup,

188
00:08:57,715 --> 00:08:59,160
where we said, okay,

189
00:08:59,160 --> 00:09:00,480
we're taking that V pi,

190
00:09:00,480 --> 00:09:02,110
we're plugging it in over to here,

191
00:09:02,110 --> 00:09:06,275
we're using the fact that we know the dynamics model and we know the reward model,

192
00:09:06,275 --> 00:09:10,100
and we're computing this one-step updated pi prime.

193
00:09:10,100 --> 00:09:13,040
And we talked about the fact that when we do this,

194
00:09:13,040 --> 00:09:15,380
we actually get monotonic policy improvement,

195
00:09:15,380 --> 00:09:24,970
[NOISE] which is sometimes referred to as a policy improvement theorem.

196
00:09:24,970 --> 00:09:28,960
So, this procedure, when we're doing it with sort of,

197
00:09:28,960 --> 00:09:30,290
um, in this dy, uh,

198
00:09:30,290 --> 00:09:33,145
in this case where we knew the dynamics model and we knew the reward model,

199
00:09:33,145 --> 00:09:35,795
um, would guarantee to always give us

200
00:09:35,795 --> 00:09:39,020
a policy that was at least as good as the previous policy or better.

201
00:09:39,020 --> 00:09:43,690
Um, and, eventually, it was guaranteed to converge at least in

202
00:09:43,690 --> 00:09:45,670
the case where we have finite states and

203
00:09:45,670 --> 00:09:48,580
actions because there's only a finite number of policies.

204
00:09:48,580 --> 00:09:50,365
So, in this case, there were only A,

205
00:09:50,365 --> 00:09:52,150
to the S possible policies.

206
00:09:52,150 --> 00:09:53,950
So, we only need to do this whole procedure,

207
00:09:53,950 --> 00:09:55,505
at most A to the S times.

208
00:09:55,505 --> 00:09:58,705
Each time, we're either picking a better policy, or we stayed in the same.

209
00:09:58,705 --> 00:10:01,855
And once you find the same policy, you're, you're done.

210
00:10:01,855 --> 00:10:04,705
So, now, we want to do all of this, um,

211
00:10:04,705 --> 00:10:08,150
but we don't have access to the dynamics or reward model.

212
00:10:08,150 --> 00:10:11,900
So, does anybody have any ideas of how we might be able to do

213
00:10:11,900 --> 00:10:19,970
the same thing now that we don't know the dynamics or reward?

214
00:10:19,970 --> 00:10:24,010
We can maintain another- [NOISE] a matrix transition probabilities,

215
00:10:24,010 --> 00:10:26,995
uh, that you calculate [NOISE] it as you experience the world.

216
00:10:26,995 --> 00:10:28,770
Yeah. The better suggestion is, well,

217
00:10:28,770 --> 00:10:31,110
what if we try to, uh, if I interpret correctly,

218
00:10:31,110 --> 00:10:32,640
wha- what if we try to basically estimate

219
00:10:32,640 --> 00:10:35,410
the dynamics model and a reward model from the world, and then,

220
00:10:35,410 --> 00:10:37,770
we could use this to- you could still compute your,

221
00:10:37,770 --> 00:10:41,465
your value function maybe using some of the methods we saw last time, um, and then,

222
00:10:41,465 --> 00:10:43,790
you could do this update as policy improvement using

223
00:10:43,790 --> 00:10:46,385
your estimated dynamics and reward model of the world.

224
00:10:46,385 --> 00:10:48,830
That's a completely reasonable thing to do, um,

225
00:10:48,830 --> 00:10:52,230
and may have any other idea of what we could do. Yeah, name and-

226
00:10:52,230 --> 00:10:54,830
Uh, I think instead of having to,

227
00:10:54,830 --> 00:10:57,765
uh, uh, a compute a model,

228
00:10:57,765 --> 00:11:01,580
can we do away with model and directly try to estimate what is

229
00:11:01,580 --> 00:11:10,750
the value of a particular state or state-action pair? Doing away with models

230
00:11:10,750 --> 00:11:15,420
So estimate the value of a particular state.

231
00:11:15,420 --> 00:11:16,185
state and action?

232
00:11:16,185 --> 00:11:16,605
Yes.

233
00:11:16,605 --> 00:11:18,060
Yes and with actually?

234
00:11:18,060 --> 00:11:18,780
Yes [OVERLAPPING].

235
00:11:18,780 --> 00:11:21,180
What she actually said is exactly the path that we're going to look

236
00:11:21,180 --> 00:11:24,120
about today which is we're going to focus on model-free control.

237
00:11:24,120 --> 00:11:26,610
So we're not going to directly estimate a model today.

238
00:11:26,610 --> 00:11:28,860
I'm actually personally very partial to models that

239
00:11:28,860 --> 00:11:31,020
can be a very simple efficient but for today we're not gonna

240
00:11:31,020 --> 00:11:32,970
look at that and we're gonna do exactly what

241
00:11:32,970 --> 00:11:35,010
was just proposed which is we're gonna compute

242
00:11:35,010 --> 00:11:37,720
a Q and if we compute

243
00:11:37,720 --> 00:11:42,235
a Q function which just to remember Q is always a function of state and action.

244
00:11:42,235 --> 00:11:45,820
We're gonna estimate the Q function directly and after we have

245
00:11:45,820 --> 00:11:49,510
that then we can do policy improvement directly using that Q function.

246
00:11:49,510 --> 00:11:52,060
So how would we do that?

247
00:11:52,060 --> 00:11:56,770
So this is Monte Carlo for on policy Q evaluation and it's gonna look

248
00:11:56,770 --> 00:12:01,150
very similar to Monte Carlo for on policy value evaluation.

249
00:12:01,150 --> 00:12:03,730
Um but we have to make a couple modifications.

250
00:12:03,730 --> 00:12:06,280
So before, if we were doing this for V,

251
00:12:06,280 --> 00:12:08,260
I'm just gonna write it to kind of contrast.

252
00:12:08,260 --> 00:12:11,515
So for V we just had a count of the number of states.

253
00:12:11,515 --> 00:12:14,065
Now we have a count of the number of state action pairs.

254
00:12:14,065 --> 00:12:20,200
Um before we could just keep track of G here um which can

255
00:12:20,200 --> 00:12:23,410
be the sum of previous rewards we've

256
00:12:23,410 --> 00:12:27,145
seen across all episodes for G of S. Now we're gonna do that for S,

257
00:12:27,145 --> 00:12:32,200
A and then now we're gonna end up having a value function.

258
00:12:32,200 --> 00:12:34,210
We're gonna have a Q pi. So essentially

259
00:12:34,210 --> 00:12:36,550
almost everywhere where we'd just had S before now we have S,

260
00:12:36,550 --> 00:12:39,595
A and then it's gonna look very similar.

261
00:12:39,595 --> 00:12:41,725
So we're gonna assume that we're still provided a policy,

262
00:12:41,725 --> 00:12:45,505
we can sample an episode and then we compute

263
00:12:45,505 --> 00:12:50,365
GIT for every single time step and that remember now is gonna,

264
00:12:50,365 --> 00:12:53,230
I mean it was before but we're gonna to think about the fact that it was

265
00:12:53,230 --> 00:12:55,930
also associated with a particular state and a particular action for

266
00:12:55,930 --> 00:13:00,850
that time step and then for every state action pair instead of just state visited in the

267
00:13:00,850 --> 00:13:03,790
episode either for every first time we saw

268
00:13:03,790 --> 00:13:07,150
that state action pair or every time we saw that state action pair,

269
00:13:07,150 --> 00:13:11,380
we can always, just like before we can either have first visit or every visit.

270
00:13:11,380 --> 00:13:14,650
There we're just gonna update our counts update our Sum

271
00:13:14,650 --> 00:13:18,220
of Total Rewards and then estimate our Q function.

272
00:13:18,220 --> 00:13:21,250
It's basically exactly the same as before

273
00:13:21,250 --> 00:13:24,890
except that now we're doing everything over state action pairs.

274
00:13:27,210 --> 00:13:32,410
Now once we have that now policy improvement is even simpler than before.

275
00:13:32,410 --> 00:13:34,480
So we're given this estimate of

276
00:13:34,480 --> 00:13:39,350
the Q function and now we can just directly take an arg max over it.

277
00:13:39,350 --> 00:13:45,010
So we define our new policy to simply be arg max of the previous one.

278
00:13:46,620 --> 00:13:50,740
Alright so did anybody see any problems with

279
00:13:50,740 --> 00:13:54,325
doing this so far for the type of policies we've been thinking about in the class?

280
00:13:54,325 --> 00:13:56,920
So far we've been thinking about mostly policies but are all

281
00:13:56,920 --> 00:14:00,010
deterministic which means that per,

282
00:14:00,010 --> 00:14:03,190
there are mapping from states to actions and we've been thinking

283
00:14:03,190 --> 00:14:06,970
about cases where this is um a deterministic mapping.

284
00:14:06,970 --> 00:14:09,715
So we always pick a particular action for a particular state.

285
00:14:09,715 --> 00:14:12,700
Yeah in the back and name first please. Oh what's your name?

286
00:14:12,700 --> 00:14:13,210


287
00:14:13,210 --> 00:14:16,300


288
00:14:16,300 --> 00:14:18,610
Oh yeah but what's the problem the problem is we're never

289
00:14:18,610 --> 00:14:21,475
exploring which is correct but what's the problem with not exploring?

290
00:14:21,475 --> 00:14:24,610
We only sample one path over and over again and we never

291
00:14:24,610 --> 00:14:27,700
actually learned anything about the rest of the world that we don't see.

292
00:14:27,700 --> 00:14:29,935
So we don't know whether there's a better policy.

293
00:14:29,935 --> 00:14:33,940
So what he is saying that maybe we're only going to sample one path.

294
00:14:33,940 --> 00:14:35,830
I think what he means more than that is so you can

295
00:14:35,830 --> 00:14:38,080
still sample different paths because your state space

296
00:14:38,080 --> 00:14:42,415
can be stochastic but you are only gonna ever try one action from one state.

297
00:14:42,415 --> 00:14:45,055
So you're never gonna learn about what it would be like if you took

298
00:14:45,055 --> 00:14:48,070
A2 instead of A1 in that state which means that

299
00:14:48,070 --> 00:14:54,580
when you're doing this for any particular state you'll only see one corresponding action.

300
00:14:54,580 --> 00:14:58,210
So the time whenever you see state S1 the only

301
00:14:58,210 --> 00:15:02,200
time the only action you'll see will be A1 or whatever your policy says to do there.

302
00:15:02,200 --> 00:15:04,240
Which means that you're not gonna have any information

303
00:15:04,240 --> 00:15:06,430
about doing anything else there which means

304
00:15:06,430 --> 00:15:09,310
your policy improvements is gonna be pretty boring because you're

305
00:15:09,310 --> 00:15:12,775
not gonna get any other information about things you should be doing instead.

306
00:15:12,775 --> 00:15:15,700
So we're going to have to do some form of exploration essentially now,

307
00:15:15,700 --> 00:15:17,290
we are gonna have to start to have some sort of

308
00:15:17,290 --> 00:15:20,650
stochasticity in our policy or there needs to be

309
00:15:20,650 --> 00:15:22,720
changing over time but we can actually try

310
00:15:22,720 --> 00:15:26,440
different things even from the same state and know what to do.Yes name first.

311
00:15:26,440 --> 00:15:30,145
My name is . Do we know the whole action space beforehand?

312
00:15:30,145 --> 00:15:33,760
Great question.  question is do we know the whole action space beforehand?

313
00:15:33,760 --> 00:15:38,230
Yeah we're gonna assume that we do for at least all this lecture and in general yeah.

314
00:15:38,230 --> 00:15:43,330
Since you've made the action initially have high values so

315
00:15:43,330 --> 00:15:45,370
then after it's computed is probably computed took it

316
00:15:45,370 --> 00:15:48,580
low so the next time you see it you would try to other actions?

317
00:15:48,580 --> 00:15:50,530
has made a very nice suggestion

318
00:15:50,530 --> 00:15:54,645
so that relates to how we're initializing the Qs.

319
00:15:54,645 --> 00:15:58,030
So one thing you can do which is what he just

320
00:15:58,030 --> 00:16:01,000
suggested is you can initialize your Q function really

321
00:16:01,000 --> 00:16:03,250
high everywhere to basically do

322
00:16:03,250 --> 00:16:08,050
what's known as optimistic initialization um and that actually can be a really useful strategy for

323
00:16:08,050 --> 00:16:12,880
exploration and if you initialize it in a particular way then you can

324
00:16:12,880 --> 00:16:15,010
have a provable guarantee on

325
00:16:15,010 --> 00:16:18,565
how much data you're going to need in order to converge to the optimal policy.

326
00:16:18,565 --> 00:16:20,890
So optimistic initialization is often

327
00:16:20,890 --> 00:16:23,350
a really good thing to do to be a little bit careful of

328
00:16:23,350 --> 00:16:25,330
how you initialize things like what

329
00:16:25,330 --> 00:16:27,880
those value should be but generally empirically it's really good.

330
00:16:27,880 --> 00:16:29,815
And formally it can be very good to.

331
00:16:29,815 --> 00:16:32,800
We're not gonna talk about Optimistic initialization today but

332
00:16:32,800 --> 00:16:35,620
we will later in the class or talk about optimization.

333
00:16:35,620 --> 00:16:40,960
So doesn't really on the Markov assumption to be able to estimate Q right.

334
00:16:40,960 --> 00:16:41,275
Yes.

335
00:16:41,275 --> 00:16:44,980
But my question is whenever we're defining the policy,

336
00:16:44,980 --> 00:16:46,915
we only define it in terms of the state,

337
00:16:46,915 --> 00:16:50,710
and if the reward that you get from the state depends on officially you have,

338
00:16:50,710 --> 00:16:56,230
then that brings the Markov assumption in like even sorry,

339
00:16:56,230 --> 00:16:58,600
even though the reward is not Markov

340
00:16:58,600 --> 00:17:03,625
then your policy will act you we're defining a policy as if it were.

341
00:17:03,625 --> 00:17:08,650
Yeah so your real world may or may not be

342
00:17:08,650 --> 00:17:13,540
Markov all the policies we're talking about right now is assuming a world as Markov.

343
00:17:13,540 --> 00:17:16,869
The policies are only mappings from current state to action.

344
00:17:16,869 --> 00:17:19,269
They are not a function of history.

345
00:17:19,270 --> 00:17:23,500
So those may or may not work well because your real-world may or may not be

346
00:17:23,500 --> 00:17:28,119
an MDP and if it's not then you're considering essentially restricted policy class.

347
00:17:28,119 --> 00:17:32,185
Considering only mappings from the immediate state to the action and if you,

348
00:17:32,185 --> 00:17:34,840
what you should do really depends on the whole history then you might not

349
00:17:34,840 --> 00:17:37,960
make good decisions. Good point.

350
00:17:37,960 --> 00:17:40,750
Okay so this is sort of how

351
00:17:40,750 --> 00:17:43,330
the basic way you would extend Monte Carlo to be able to start to

352
00:17:43,330 --> 00:17:45,100
estimate Q and once you have that you could do

353
00:17:45,100 --> 00:17:47,710
policy improvement but now it's clear that we need to do

354
00:17:47,710 --> 00:17:51,010
something in terms of how we should get- gather experience so we

355
00:17:51,010 --> 00:17:55,090
can actually improve when we tried to do this policy improvement um.

356
00:17:55,090 --> 00:17:59,005
Because now we don't know how the real dynamics of the world work.

357
00:17:59,005 --> 00:18:04,300
So we need to do some with some sort of interleaving of policy evaluation and

358
00:18:04,300 --> 00:18:10,420
improvement and we also need to think about how we're doing this exploration aspect.

359
00:18:10,420 --> 00:18:18,520
So in general it might seem a little bit subtle.

360
00:18:18,520 --> 00:18:21,340
So we've already got one nice suggestion from  like maybe you could

361
00:18:21,340 --> 00:18:24,535
initialize everything optimistically and maybe that would help you explore.

362
00:18:24,535 --> 00:18:27,580
It does, but in

363
00:18:27,580 --> 00:18:29,170
general it might seem like it's a little bit

364
00:18:29,170 --> 00:18:30,910
hard of how are we going to get this good estimate

365
00:18:30,910 --> 00:18:35,320
of Q pi because what Q pi does is it says um if you wanted

366
00:18:35,320 --> 00:18:40,120
a really good estimate of Q pi of S,A for all S and all A it would

367
00:18:40,120 --> 00:18:43,600
say you kinda need to get to every different state take every possible action

368
00:18:43,600 --> 00:18:47,080
and then follow pi from then onwards and so how do I make sure

369
00:18:47,080 --> 00:18:49,390
that I visit all of those things

370
00:18:49,390 --> 00:18:53,200
and what we're gonna talk about today is a very simple strategy to make

371
00:18:53,200 --> 00:18:56,470
sure that you visit things which works

372
00:18:56,470 --> 00:19:00,740
generally under some mild conditions about the underlying process.

373
00:19:01,740 --> 00:19:05,500
So the really simple idea is to balance

374
00:19:05,500 --> 00:19:09,235
exploration and exploitation by being random some of the time.

375
00:19:09,235 --> 00:19:11,830
So let's imagine that there's a finite number of actions

376
00:19:11,830 --> 00:19:14,320
we're gonna call that cardinality A, um,

377
00:19:14,320 --> 00:19:19,615
here then e-greedy policy with respect to a state action value is as follows.

378
00:19:19,615 --> 00:19:25,389
With probability one minus Epsilon,

379
00:19:25,389 --> 00:19:28,270
you're going to take the best action according to

380
00:19:28,270 --> 00:19:33,760
your current state action value function and

381
00:19:33,760 --> 00:19:36,505
else then you're gonna take

382
00:19:36,505 --> 00:19:45,755
an action A with probability Epsilon divided by A.

383
00:19:45,755 --> 00:19:49,210
So with probability one minus Epsilon you take what you

384
00:19:49,210 --> 00:19:52,210
currently think is best according to your group or your estimate of the

385
00:19:52,210 --> 00:19:58,790
Q function and with probability Epsilon you select one of the other actions.

386
00:20:00,290 --> 00:20:06,160
So it's a pretty simple strategy and the nice thing is that, it's still sufficient.

387
00:20:06,470 --> 00:20:09,015
But before we do that why don't we just do

388
00:20:09,015 --> 00:20:11,505
a brief example to make sure that we're on the same page.

389
00:20:11,505 --> 00:20:13,980
So let's think about how we would do Monte Carlo for

390
00:20:13,980 --> 00:20:17,535
on policy Q evaluation for our little Mars rover.

391
00:20:17,535 --> 00:20:21,390
So now our Mars Rover is gonna have two things that can

392
00:20:21,390 --> 00:20:24,870
do instead of we're gonna be reasoning more about that.

393
00:20:24,870 --> 00:20:27,480
So I've written down the reward function here.

394
00:20:27,480 --> 00:20:30,130
I'm saying that if you take action A1 you get

395
00:20:30,130 --> 00:20:33,520
the same rewards we've been talking about before which is 1,

396
00:20:33,520 --> 00:20:35,170
0, 0, 0, 0, 0 plus 10.

397
00:20:35,170 --> 00:20:37,540
And now I'm changing it I'm saying well you're

398
00:20:37,540 --> 00:20:40,510
action does -- your rewards do depend on your state and

399
00:20:40,510 --> 00:20:43,330
the action you take and so the action for

400
00:20:43,330 --> 00:20:47,005
A2 is now going to be 0 everywhere and then you get a plus five at the end

401
00:20:47,005 --> 00:20:50,500
and gamma is one and let's assume that

402
00:20:50,500 --> 00:20:53,800
our current greedy policy is you take

403
00:20:53,800 --> 00:20:57,965
action A1 everywhere and that we're using an Epsilon of 0.5.

404
00:20:57,965 --> 00:21:01,080
And we sample a trajectory from an e-greedy policy.

405
00:21:01,080 --> 00:21:04,410
And again what an e-greedy policy means here is I set Epsilon

406
00:21:04,410 --> 00:21:07,800
equal to 0.5 which means that half the time we're

407
00:21:07,800 --> 00:21:10,245
gonna take our current greedy policy of

408
00:21:10,245 --> 00:21:15,140
action A1 and the other half the time we're going to either take A1 or A2.

409
00:21:15,140 --> 00:21:18,880
So what that would yield as an example would be a trajectory such

410
00:21:18,880 --> 00:21:22,855
as state three action A1 0, state two.

411
00:21:22,855 --> 00:21:25,900
And now this is a case where we're sampling randomly.

412
00:21:25,900 --> 00:21:27,985
So we flipped a coin.

413
00:21:27,985 --> 00:21:29,905
We said oh this time I'm gonna be random.

414
00:21:29,905 --> 00:21:31,750
Then I have to flip a coin again to see whether I'm

415
00:21:31,750 --> 00:21:34,030
taking an action A1 or A2 and I took A2 there.

416
00:21:34,030 --> 00:21:38,020
I got a reward of 0 and then the rest of trajectory as

417
00:21:38,020 --> 00:21:42,490
follows and my question to you and feel free to talk to a neighbor of

418
00:21:42,490 --> 00:21:46,600
course is what is now the Q estimate for all states for

419
00:21:46,600 --> 00:21:53,000
both action A1 and action A2 at the end of this trajectory using Monte Carlo estimates?

420
00:21:53,000 --> 00:23:17,128
So we're doing first visit in this case.

421
00:23:17,128 --> 00:23:17,228
Yeah.

422
00:23:17,228 --> 00:23:22,543
Uh, [NOISE] I have a question about the action we choose on the Epsilon table.

423
00:23:22,543 --> 00:23:22,643
Yeah.

424
00:23:22,643 --> 00:23:25,280
Uh, is it important

425
00:23:25,280 --> 00:23:30,530
when- what would the actions on policies ,

426
00:23:30,530 --> 00:23:37,820
or should we pass in that action

427
00:23:37,820 --> 00:23:40,010
question is whether or not when you hit,

428
00:23:40,010 --> 00:23:41,690
uh, now do something random,

429
00:23:41,690 --> 00:23:43,610
whether you should include the action that you'll

430
00:23:43,610 --> 00:23:45,650
be taking normally if you're being greedy.

431
00:23:45,650 --> 00:23:46,850
Um, you could.

432
00:23:46,850 --> 00:23:49,470
In some ways, that's like just picking a different Epsilon.

433
00:23:49,720 --> 00:23:52,880
Yeah. I hear less talking than normal,

434
00:23:52,880 --> 00:23:56,400
so that they may have any clarification questions about this or,

435
00:23:57,640 --> 00:24:01,070
or [NOISE] or are there questions? Good. [LAUGHTER] Sorry.

436
00:24:01,070 --> 00:24:03,290
I have an idea.

437
00:24:03,290 --> 00:24:05,570
Okay. Yeah. So, um, uh,

438
00:24:05,570 --> 00:24:08,750
if everybody's ready to ask yourselves.So , what, what did you guys think?

439
00:24:08,750 --> 00:24:14,540
Uh, so, you will have in this case S3,

440
00:24:14,540 --> 00:24:17,540
well ,

441
00:24:17,540 --> 00:24:18,890
so everything that you did not,

442
00:24:18,890 --> 00:24:21,560
every state of action pair you did not see will remain zero.

443
00:24:21,560 --> 00:24:21,935
Yeah.

444
00:24:21,935 --> 00:24:25,580
And at a particular, uh Q of S3, A1 will be zero,

445
00:24:25,580 --> 00:24:28,085
cause you saw that once and reduce that to zero,

446
00:24:28,085 --> 00:24:29,630
Q of S2h will also be zero,

447
00:24:29,630 --> 00:24:32,405
Q of S3A1 will be zero, and then,

448
00:24:32,405 --> 00:24:35,300
the only one that will be non-zero will be Q of S1, A1,

449
00:24:35,300 --> 00:24:37,250
which in this case will be 1.

450
00:24:37,250 --> 00:24:39,425
Because you saw it once,

451
00:24:39,425 --> 00:24:42,530
and the reward that you got when you saw it was 1.

452
00:24:42,530 --> 00:24:44,090
That's one answer.

453
00:24:44,090 --> 00:24:45,320
Anybody with a different answer?

454
00:24:45,320 --> 00:24:52,730
so,

455
00:24:52,730 --> 00:24:56,495
uh, all of the state action pairs that we've seen will be one,

456
00:24:56,495 --> 00:24:59,270
and all other state action pairs will be zero.

457
00:24:59,270 --> 00:25:03,200
That's another answer. So what,

458
00:25:03,200 --> 00:25:05,210
uh,  was say would be right for the TD case.

459
00:25:05,210 --> 00:25:05,905
Okay.

460
00:25:05,905 --> 00:25:08,210
What you were saying would have been right for last week,

461
00:25:08,210 --> 00:25:10,010
uh, or if yesterday, or, or Monday.

462
00:25:10,010 --> 00:25:12,300
Any else who may have a third answer?

463
00:25:15,640 --> 00:25:19,100
Could you repeat what the second- the second choice was?

464
00:25:19,100 --> 00:25:23,000
The first choice is that we only update, um, S1, A1.

465
00:25:23,000 --> 00:25:26,555
The second choice is that everything that we saw will now be 1,

466
00:25:26,555 --> 00:25:27,950
and maybe I misunderstood over there.

467
00:25:27,950 --> 00:25:29,705
So we're gonna have two different um,

468
00:25:29,705 --> 00:25:30,935
we have two vectors now.

469
00:25:30,935 --> 00:25:33,635
So we have Q of A1,

470
00:25:33,635 --> 00:25:35,465
and we have Q of A2,

471
00:25:35,465 --> 00:25:37,610
and they're not gonna look identical.

472
00:25:37,610 --> 00:25:40,745
So, sometimes we take action A1,

473
00:25:40,745 --> 00:25:42,485
and sometimes we take action A2,

474
00:25:42,485 --> 00:25:48,155
and we can only update what we saw the returns for the action we took.

475
00:25:48,155 --> 00:25:52,890
So what actions do we take for S3?

476
00:25:53,530 --> 00:25:54,785
A1.

477
00:25:54,785 --> 00:25:57,200
Just A1, right? So that means for those ones,

478
00:25:57,200 --> 00:25:58,805
for S3 it's gonna be 1,

479
00:25:58,805 --> 00:26:01,265
and that for Q of S3, A1,

480
00:26:01,265 --> 00:26:03,410
so I'll fill in all the ones that are zero,

481
00:26:03,410 --> 00:26:05,165
one, two, three, four.

482
00:26:05,165 --> 00:26:08,520
Um, do we ever take A2 in S3?

483
00:26:08,560 --> 00:26:11,090
No. So that also has to be zero,

484
00:26:11,090 --> 00:26:12,410
cause we didn't ever start there,

485
00:26:12,410 --> 00:26:14,300
take action A2, and get a return.

486
00:26:14,300 --> 00:26:16,340
Uh, what about for,

487
00:26:16,340 --> 00:26:18,630
what action do we take from S2?

488
00:26:18,730 --> 00:26:23,820
Right. So for that one, we get a 1.

489
00:26:27,370 --> 00:26:31,160
So we basically, uh, distributing your experience.

490
00:26:31,160 --> 00:26:33,125
So now if you were going to take a max over those,

491
00:26:33,125 --> 00:26:35,960
then you would get the same thing that we saw last time for Monte Carlo,

492
00:26:35,960 --> 00:26:39,440
which would be 11100000 to the end,

493
00:26:39,440 --> 00:26:42,575
um, but here we- we're subdividing our samples.

494
00:26:42,575 --> 00:26:47,675
So, you only get to get an experience for the action that you actually took in the state.

495
00:26:47,675 --> 00:26:49,760
And because we're in the Monte Carlo case,

496
00:26:49,760 --> 00:26:50,990
we'll see the TD case or,

497
00:26:50,990 --> 00:26:52,250
Q-learning we'll call it later,

498
00:26:52,250 --> 00:26:56,825
um, then we get to add up all the rewards to the end of the episode.

499
00:26:56,825 --> 00:26:59,420
So G here is gonna be the sum of all these steps,

500
00:26:59,420 --> 00:27:01,610
and I didn't speci- oh, I did.

501
00:27:01,610 --> 00:27:04,160
Good. And we're keeping Gamma equal to 1 here just

502
00:27:04,160 --> 00:27:06,740
to make all the math. Just adding. Yeah?

503
00:27:06,740 --> 00:27:08,420
Should we just [OVERLAPPING].

504
00:27:08,420 --> 00:27:09,320
Sorry.

505
00:27:09,320 --> 00:27:21,000
Can just be one half for Q S1A1 or in Q S3A1?

506
00:27:21,000 --> 00:27:22,970
Uh, is talking about whether or not if we did every visit,

507
00:27:22,970 --> 00:27:24,860
if anything would change here. [NOISE] Excuse me.

508
00:27:24,860 --> 00:27:27,440
It would not change in this case, because, um,

509
00:27:27,440 --> 00:27:29,780
both times when you visited S3,

510
00:27:29,780 --> 00:27:31,985
the sum of rewards to the end of the episode was 1.

511
00:27:31,985 --> 00:27:33,440
So you'd have two counts of 1,

512
00:27:33,440 --> 00:27:35,460
and then we divide by 2.

513
00:27:35,500 --> 00:27:38,150
It da- it can actually be different,

514
00:27:38,150 --> 00:27:40,160
but it's mostly different if you got like a different sum of

515
00:27:40,160 --> 00:27:42,665
rewards from then to the end of the episode. Yes?

516
00:27:42,665 --> 00:27:43,850
So is [OVERLAPPING].

517
00:27:43,850 --> 00:27:44,600
Remind me [OVERLAPPING].

518
00:27:44,600 --> 00:27:44,900


519
00:27:44,900 --> 00:27:45,090


520
00:27:45,090 --> 00:27:49,730
Yeah. Isn't that?

521
00:27:49,730 --> 00:27:50,510
Maybe I misunderstood.

522
00:27:50,510 --> 00:27:50,670
Yeah.

523
00:27:50,670 --> 00:27:53,750
So, I thought we were supposed to say that everything was, and I missed that.

524
00:27:53,750 --> 00:27:56,285
Did, did you say that that was different for the two actions?

525
00:27:56,285 --> 00:28:01,360
That was one for in the projectory, um, zero.

526
00:28:01,360 --> 00:28:03,950
I understand. Sorry about that.

527
00:28:05,430 --> 00:28:07,700
Okay.

528
00:28:08,670 --> 00:28:13,015
Okay. So now we're gonna show formally that this does the right thing.

529
00:28:13,015 --> 00:28:16,390
So, um, we're gonna show provably

530
00:28:16,390 --> 00:28:20,975
that like what we did before when we were doing policy improvement,

531
00:28:20,975 --> 00:28:23,360
we're showing that if you pick a policy,

532
00:28:23,360 --> 00:28:26,570
um, pi i, that was, uh,

533
00:28:26,570 --> 00:28:30,560
generated by being greedy with respect to your Q function,

534
00:28:30,560 --> 00:28:33,305
then that was guaranteed to yield monotonic improvement,

535
00:28:33,305 --> 00:28:35,270
and the same thing is gonna be true here too,

536
00:28:35,270 --> 00:28:37,070
when you do e-greedy.

537
00:28:37,070 --> 00:28:38,690
Um, so if you use sort of er,

538
00:28:38,690 --> 00:28:42,830
an e-greedy policy, then you can gather data such that, uh,

539
00:28:42,830 --> 00:28:45,290
the new policy- the new value you get,

540
00:28:45,290 --> 00:28:47,420
if you're optimistic with respect to that- oh, sorry,

541
00:28:47,420 --> 00:28:48,740
if you're greedy with respect to that,

542
00:28:48,740 --> 00:28:51,125
that means you're gonna get any better policy.

543
00:28:51,125 --> 00:28:53,300
Okay. So let's say that, um,

544
00:28:53,300 --> 00:28:55,070
we have an e-greedy policy,

545
00:28:55,070 --> 00:29:00,965
Pi i, and then we're gonna call an e-greedy policy with respect to Q Pi i,

546
00:29:00,965 --> 00:29:02,960
which is gonna be Pi i plus 1,

547
00:29:02,960 --> 00:29:05,960
so we had a greedy- e-greedy policy Pi i that was

548
00:29:05,960 --> 00:29:09,380
doing some amount of exploration and some amount of greediness in the past,

549
00:29:09,380 --> 00:29:11,165
we use that to gather data,

550
00:29:11,165 --> 00:29:14,570
we then evaluated that policy and we got this Q Pi i,

551
00:29:14,570 --> 00:29:16,820
and now we're gonna extract a new policy.

552
00:29:16,820 --> 00:29:18,335
We're going to do policy improvements.

553
00:29:18,335 --> 00:29:20,675
I'm gonna show that that's a monotonic improvement.

554
00:29:20,675 --> 00:29:24,260
Okay? Does anyone have any questions about the, what we are showing?

555
00:29:24,260 --> 00:29:27,320
Okay. So, what does this mean?

556
00:29:27,320 --> 00:29:32,180
So right now what we're gonna be trying to show is that this, this Q function,

557
00:29:32,180 --> 00:29:34,655
the Pi of s Pi i plus 1,

558
00:29:34,655 --> 00:29:38,045
so, is gonna be better than our previous value.

559
00:29:38,045 --> 00:29:43,490
At least as good or better than our previous value of our old policy Pi i.

560
00:29:43,490 --> 00:29:46,745
So the way we define this is now, um,

561
00:29:46,745 --> 00:29:51,350
the Q function here is going to be a sum over,

562
00:29:51,350 --> 00:29:53,225
our policy is stochastic.

563
00:29:53,225 --> 00:29:55,250
So it's Pi i plus 1,

564
00:29:55,250 --> 00:29:58,535
of the probability we take an action in a certain state,

565
00:29:58,535 --> 00:30:01,920
times Q Pi i of SA,

566
00:30:03,340 --> 00:30:05,375
and then we're gonna expand that out,

567
00:30:05,375 --> 00:30:07,775
and we're going to redefine it in terms of what it,

568
00:30:07,775 --> 00:30:09,800
what it means to be an e-greedy policy.

569
00:30:09,800 --> 00:30:15,110
So with, remember in a e-greedy policy we either take something randomly,

570
00:30:15,110 --> 00:30:16,520
and that's with probability S1,

571
00:30:16,520 --> 00:30:19,460
and we split our probability mass across all the actions.

572
00:30:19,460 --> 00:30:21,530
So that's how we get this equation.

573
00:30:21,530 --> 00:30:25,970
So this says, this is the- this is the random part.

574
00:30:25,970 --> 00:30:30,214
So with probability, with probability epsilon,

575
00:30:30,214 --> 00:30:32,390
we take one action,

576
00:30:32,390 --> 00:30:36,050
one of the actions, and then we would follow that from then always.

577
00:30:36,050 --> 00:30:38,195
So that's just Q Pi i of SA,

578
00:30:38,195 --> 00:30:42,380
and then with probability 1 minus Epsilon, we're greedy.

579
00:30:42,380 --> 00:30:48,570
And we follow the best action according to our current Q Pi i.

580
00:30:48,640 --> 00:30:53,525
So, now what we're gonna do is we're going to rewrite that.

581
00:30:53,525 --> 00:30:56,510
The first term isn't gonna change and I'm gonna expand the second.

582
00:30:56,510 --> 00:31:15,350
[NOISE]

583
00:31:15,350 --> 00:31:16,250
I haven't done anything here.

584
00:31:16,250 --> 00:31:19,220
I just multiplied the last term by 1,

585
00:31:19,220 --> 00:31:23,240
but I expressed the 1 as 1 over Epsilon divided by 1 over Epsilon,

586
00:31:23,240 --> 00:31:25,040
and now I'm gonna re-express that part.

587
00:31:25,040 --> 00:31:27,950
So, and I'm gonna rewrite the first term,

588
00:31:27,950 --> 00:31:30,650
plus 1 minus Epsilon,

589
00:31:30,650 --> 00:31:39,150
max over a, and what I'm gonna rewrite this as-

590
00:31:50,670 --> 00:31:55,915
It's gonna use the fact that whenever we define our e-greedy policy,

591
00:31:55,915 --> 00:31:58,540
if you sum over all actions in a certain state,

592
00:31:58,540 --> 00:32:01,480
those are all probabilities of us taking those actions in that state,

593
00:32:01,480 --> 00:32:03,370
so it has to sum to one.

594
00:32:03,370 --> 00:32:06,440
So I just first divide it,

595
00:32:06,440 --> 00:32:07,595
I just multiply by one,

596
00:32:07,595 --> 00:32:10,775
and we're expressing as 1 minus Epsilon divide by 1 minus Epsilon,

597
00:32:10,775 --> 00:32:12,305
then I re-expressed the 1,

598
00:32:12,305 --> 00:32:14,000
because it has to equal to 1,

599
00:32:14,000 --> 00:32:16,475
cause we have to take some action in a particular state.

600
00:32:16,475 --> 00:32:17,930
A policy always has to,

601
00:32:17,930 --> 00:32:21,215
the probability of us taking any action state has to be equal to 1,

602
00:32:21,215 --> 00:32:26,330
and then I'm gonna do the that expression because we're,

603
00:32:26,330 --> 00:32:28,220
here is where we'll take the best action.

604
00:32:28,220 --> 00:32:30,680
So by definition, the best action has to

605
00:32:30,680 --> 00:32:33,920
be at least as good as taking any of the other actions.

606
00:32:33,920 --> 00:32:36,215
So we're gonna do the following;

607
00:32:36,215 --> 00:32:39,860
we're gonna push that Q inside.

608
00:32:39,860 --> 00:32:57,200
[NOISE]

609
00:32:57,200 --> 00:32:59,360
So that has to be smaller than what we saw before,

610
00:32:59,360 --> 00:33:01,670
because basically we just push the Q inside,

611
00:33:01,670 --> 00:33:03,695
and we're no longer taking a max.

612
00:33:03,695 --> 00:33:07,430
And the Q values- all the Q values at best have to be equal to the max,

613
00:33:07,430 --> 00:33:09,620
and in other cases they'll generally be worse.

614
00:33:09,620 --> 00:33:11,390
Okay? But then once we have that,

615
00:33:11,390 --> 00:33:14,000
we can cancel that 1 over Epsilon minus 1 over Epsilon,

616
00:33:14,000 --> 00:33:15,170
and what do we have?

617
00:33:15,170 --> 00:33:17,900
We have two different terms here that look very similar.

618
00:33:17,900 --> 00:33:29,150
We have one.

619
00:33:29,150 --> 00:33:32,940
Let's see. We need one was taking that apart.

620
00:33:35,020 --> 00:33:37,535
And we'll keep this up.

621
00:33:37,535 --> 00:33:41,665
Yeah. There is an Epsilon over a right there.

622
00:33:41,665 --> 00:33:44,650
Okay. So now I'm going to pull that out.

623
00:33:44,650 --> 00:34:22,710
[NOISE]

624
00:34:22,710 --> 00:34:24,389
If I split those terms up,

625
00:34:24,389 --> 00:34:26,953
the first term and the third term are identical,

626
00:34:26,954 --> 00:34:32,864
and one is subtracted and one is added. Make sure that's clear.

627
00:34:32,864 --> 00:34:35,369
So, this just ends up becoming the middle term.

628
00:34:35,370 --> 00:34:45,239
[NOISE]

629
00:34:45,239 --> 00:34:45,630
And that was

630
00:34:45,630 --> 00:34:46,949
just the previous value.

631
00:34:46,949 --> 00:34:49,139
Yeah?

632
00:34:49,139 --> 00:34:50,834
first line, where we changed it to,

633
00:34:50,835 --> 00:34:52,500
instead of the sum, over all,

634
00:34:52,500 --> 00:34:55,110
A of Pi I a given s minus Epsilon to, [NOISE]

635
00:34:55,110 --> 00:34:57,855
minus Epsilon over the cardinality of A in this case?

636
00:34:57,855 --> 00:34:58,440
Yes.

637
00:34:58,440 --> 00:35:00,120
Um, is that s- that,

638
00:35:00,120 --> 00:35:03,280
is that [NOISE] still one minus Epsilon,

639
00:35:03,280 --> 00:35:05,320
[NOISE] I mean, that, that looks all-

640
00:35:05,320 --> 00:35:06,680
That's all . Got it.

641
00:35:06,680 --> 00:35:09,040
Does that answer your question?

642
00:35:09,040 --> 00:35:09,945
I think so.

643
00:35:09,945 --> 00:35:14,490
Yeah. So, the, what we did from the one minus Epsilon, to the next one.

644
00:35:14,490 --> 00:35:18,900
[NOISE] So, we had a one minus Epsilon divided by one mi- minus Epsilon,

645
00:35:18,900 --> 00:35:21,900
[NOISE] and, I re-express that as the sum over A [NOISE] ,

646
00:35:21,900 --> 00:35:26,550
Pi I of A given S minus Epsilon divided by sum over A, and then,

647
00:35:26,550 --> 00:35:28,665
if you sum over A that's second term,

648
00:35:28,665 --> 00:35:30,860
just this Epsilon, and the first term is one.

649
00:35:30,860 --> 00:35:32,460
Okay? Yes.

650
00:35:32,460 --> 00:35:33,520
Isn't that [OVERLAPPING]

651
00:35:33,520 --> 00:35:37,470
Can you remind me your name?

652
00:35:37,470 --> 00:35:39,590
[OVERLAPPING] the Pi, the, what?

653
00:35:39,590 --> 00:35:40,120
Name?

654
00:35:40,120 --> 00:35:43,300
Oh. Pi I is the Pi,

655
00:35:43,300 --> 00:35:45,950
like Pi I plus one,

656
00:35:45,950 --> 00:35:50,200
negative Pi, and then Pi I.

657
00:35:50,200 --> 00:35:52,220
Pi I plus one. [NOISE]

658
00:35:52,220 --> 00:35:53,370
Which line are you thinking about?

659
00:35:53,370 --> 00:35:54,950
[NOISE]

660
00:35:54,950 --> 00:35:55,050
.

661
00:35:55,050 --> 00:35:55,850
Which- [OVERLAPPING]

662
00:35:55,850 --> 00:35:59,550
The means, I'm sorry. The second line .

663
00:35:59,550 --> 00:36:00,210
Yeah.

664
00:36:00,210 --> 00:36:02,070
You wrote E Pi I plus [NOISE] one,

665
00:36:02,070 --> 00:36:07,680
negative five [NOISE], five Pi times five.

666
00:36:07,680 --> 00:36:10,140
I'm just not understanding your question

667
00:36:10,140 --> 00:36:12,090
with- so you're on that second line is that right or-?

668
00:36:12,090 --> 00:36:16,270


669
00:36:16,270 --> 00:36:17,220
In the  .

670
00:36:17,220 --> 00:36:19,830
Okay. Pi plus one is- sorry. What is the question?

671
00:36:19,830 --> 00:36:28,680
[NOISE] .

672
00:36:28,680 --> 00:36:30,210
Yeah. The?

673
00:36:30,210 --> 00:36:31,480
Yeah.

674
00:36:32,240 --> 00:36:38,160
Yeah. [NOISE]

675
00:36:38,160 --> 00:36:38,622
[NOISE] .

676
00:36:38,622 --> 00:36:39,675
Yeah, so good question.

677
00:36:39,675 --> 00:36:43,350
The greater than or equal happen because we push that Q Pi we had a max over

678
00:36:43,350 --> 00:36:47,400
A Q Pi I of s, a we pushed it inside of the sum.

679
00:36:47,400 --> 00:36:50,685
And so that sum now no longer includes a max.

680
00:36:50,685 --> 00:36:57,030
And so, now that the max is always greater than or equal to any of the other elements.

681
00:36:57,030 --> 00:36:58,545
So, that's where you got the,

682
00:36:58,545 --> 00:37:01,416
greater than or equal to. Yeah?

683
00:37:01,416 --> 00:37:03,320
So, I just wondering if you could explain like

684
00:37:03,320 --> 00:37:12,570
intuitively  you go random or optimal actions and then you end up with monotonic improvement.

685
00:37:12,570 --> 00:37:17,925
[NOISE] Yeah. Can we get some intuition this is  the algebraic derivation.

686
00:37:17,925 --> 00:37:20,220
And I think intuitively the idea is that by doing

687
00:37:20,220 --> 00:37:25,080
some e greedy exploration and you're gonna get evidence about some other state action pairs.

688
00:37:25,080 --> 00:37:27,990
Um, and then you can use this to estimate your Q function and that

689
00:37:27,990 --> 00:37:31,500
when you do that then that's also gonna give you uh,

690
00:37:31,500 --> 00:37:34,620
then that can improve your policy and you can have evidence that there is something

691
00:37:34,620 --> 00:37:37,605
better you could do then the current one, the current thing you're doing.

692
00:37:37,605 --> 00:37:42,405
If you don't do any exploration your Q function wouldn't change from before.

693
00:37:42,405 --> 00:37:45,120
But now because you're doing exploration then you can learn about

694
00:37:45,120 --> 00:37:53,699
other stuff and then if it's better you'll see that in your Q function.  function like your exploration

695
00:37:53,699 --> 00:37:58,530
is not as good then you just take the old one?

696
00:37:58,530 --> 00:38:00,510
Yeah. Um, yeah.

697
00:38:00,510 --> 00:38:02,400
So, this is now um,

698
00:38:02,400 --> 00:38:03,690
this is saying that you'll get

699
00:38:03,690 --> 00:38:07,230
this monotonic improvement if you're computing this exactly.

700
00:38:07,230 --> 00:38:11,295
So that's an important part.

701
00:38:11,295 --> 00:38:14,670
So this show- so what this shows here is

702
00:38:14,670 --> 00:38:17,340
that if you get a Q function and it looks like there's

703
00:38:17,340 --> 00:38:20,340
some improvement from some other actions that you're not taking right now you're

704
00:38:20,340 --> 00:38:23,760
gonna shift your policy over towards focusing on those actions.

705
00:38:23,760 --> 00:38:25,500
This is assuming right now in terms of

706
00:38:25,500 --> 00:38:28,950
the monotonic improvement that Q Pi I's have been computed exactly.

707
00:38:28,950 --> 00:38:32,730
So that's what we thought when we were doing planning where we knew what

708
00:38:32,730 --> 00:38:34,740
the dynamics model was in the Reward Model and we're

709
00:38:34,740 --> 00:38:37,830
using that [NOISE] to compute a value function.

710
00:38:37,830 --> 00:38:40,260
Um, so if we we're doing in that case we have

711
00:38:40,260 --> 00:38:42,450
the guaranteed monotonic improvement because we had

712
00:38:42,450 --> 00:38:48,780
the exact value of Pi and similarly here if we have the exact value of Q Pi I,

713
00:38:48,780 --> 00:38:51,840
then when you do this improvement then you're guaranteed to

714
00:38:51,840 --> 00:38:54,870
be monotonically improving if you didn't,

715
00:38:54,870 --> 00:38:58,630
like if you have just an approximation of Q Pi I,

716
00:38:58,630 --> 00:39:00,470
then it may not be monotonic.

717
00:39:00,470 --> 00:39:03,650
like let's say you tried another action once in that state.

718
00:39:03,650 --> 00:39:06,850
You may have a bad estimate of how good things are from that point.

719
00:39:06,850 --> 00:39:09,135
So, this, this is an important aspect.

720
00:39:09,135 --> 00:39:11,430
And this is going to be really important when we start to think about

721
00:39:11,430 --> 00:39:15,525
function approximation because we almost never will have computed Q Pi I, exactly.

722
00:39:15,525 --> 00:39:16,890
But if you do like that say,

723
00:39:16,890 --> 00:39:18,720
you can just iterate through this a ton of times

724
00:39:18,720 --> 00:39:21,360
like you're learning that's still a tabular environment.

725
00:39:21,360 --> 00:39:24,060
You've converged you know your Q Pi I, is perfect.

726
00:39:24,060 --> 00:39:25,200
Then when you- then do

727
00:39:25,200 --> 00:39:29,580
policy improvement you can get a benefit even you can improve- though

728
00:39:29,580 --> 00:39:32,040
there's going to be this interesting question of how often do you improve

729
00:39:32,040 --> 00:39:35,295
your policy   versus how much time do you spend evaluating your current policy  .

730
00:39:35,295 --> 00:39:36,570
Yeah?

731
00:39:36,570 --> 00:39:36,975
Uh, yes.

732
00:39:36,975 --> 00:39:37,230
Yeah.

733
00:39:37,230 --> 00:39:44,340
So does this mean that it definitely converges  to like an optimal Q that was Q function?

734
00:39:44,340 --> 00:39:46,200
The overall- Perfect.

735
00:39:46,200 --> 00:39:48,000
Yeah, we'll talk about that   question is great too.

736
00:39:48,000 --> 00:39:51,030
So, this  is just saying like one step monotonic improvement what,

737
00:39:51,030 --> 00:39:53,970
what's gonna happen in terms of total convergence we'll talk about that in just a second.

738
00:39:53,970 --> 00:39:55,560
Yes? Remind me name, please.

739
00:39:55,560 --> 00:39:56,062


740
00:39:56,062 --> 00:39:57,109


741
00:39:57,109 --> 00:40:07,943
[NOISE]  question and answers When I think of V Pi, I think of it as being a function of a state? But action given a state?

742
00:40:07,943 --> 00:40:11,125
Uh. So,  to sort of re- refresh all over my

743
00:40:11,125 --> 00:40:13,890
what is a Pi and how we define the function.

744
00:40:13,890 --> 00:40:15,330
Now we're thinking of it as a mapping from states

745
00:40:15,330 --> 00:40:17,385
to actions but it can be a stochastic function.

746
00:40:17,385 --> 00:40:20,860
So, it can be a probability distribution over actions.

747
00:40:20,900 --> 00:40:24,690
So, I can select action A1 with 50 percent probability.

748
00:40:24,690 --> 00:40:26,640
or action A2 with 50 percent probability.

749
00:40:26,640 --> 00:40:30,711
For example from the things- [OVERLAPPING] ? .

750
00:40:30,711 --> 00:40:31,320
Okay.

751
00:40:31,320 --> 00:40:34,665
I mean depends how you want to implement it like that concerned to be a bit.

752
00:40:34,665 --> 00:40:37,860
Essentially I think of it as you're in a state and then you have

753
00:40:37,860 --> 00:40:40,110
some probability distribution over actions you have to

754
00:40:40,110 --> 00:40:42,690
sample from that to decide what action you take.

755
00:40:42,690 --> 00:40:42,930


756
00:40:42,930 --> 00:40:50,880
The policy the- so what we're doing here when we

757
00:40:50,880 --> 00:40:54,570
expanded this as we said what is the policy for an action given a state

758
00:40:54,570 --> 00:40:59,490
we said with one minus Epsilon probability we will be taking this max action.

759
00:40:59,490 --> 00:41:05,280
So that one. And with Epsilon probability we would be taking one of the actions.

760
00:41:05,280 --> 00:41:07,680
And so, then we summed over each of the actions we could take.

761
00:41:07,680 --> 00:41:10,440
So, what we did there is we split this sum up

762
00:41:10,440 --> 00:41:13,200
into the probability of taking one action and what'll be

763
00:41:13,200 --> 00:41:15,690
the Q function of that action and the probability of

764
00:41:15,690 --> 00:41:18,630
taking each of the other actions and what would be the value of those.

765
00:41:18,630 --> 00:41:21,015
So, it's like our expected value.

766
00:41:21,015 --> 00:41:33,685
Yeah at the back.  Yeah . So when we talked about the Bellman operator, we said that if you got to the same value function - [NOISE]

767
00:41:33,685 --> 00:41:35,785
You can stop iterating.

768
00:41:35,785 --> 00:41:41,455
Here, would you have to have tried every ah,

769
00:41:41,455 --> 00:41:45,370
action to know that you are done?

770
00:41:45,370 --> 00:41:51,190
That's a great question before  in policy improvement if you got to the same policy you you,

771
00:41:51,190 --> 00:41:53,155
you are done you don't have to do any more improvement.

772
00:41:53,155 --> 00:41:56,785
The question is, in this case is it true or there's some other additional conditions?

773
00:41:56,785 --> 00:41:58,450
Um, this is very related to question too.

774
00:41:58,450 --> 00:42:00,430
So why don't we go onto the next part that is saying, you know,

775
00:42:00,430 --> 00:42:04,405
under what conditions are these going to converge and converge to optimal?

776
00:42:04,405 --> 00:42:06,280
Um,  do- do you have question before?

777
00:42:06,280 --> 00:42:10,630
Yeah, in this  this also say that the only time we get strict equalities is when Epsilon is 1. So you just act purely randomly?

778
00:42:10,630 --> 00:42:17,365
Uh, the question is whether or not there's,

779
00:42:17,365 --> 00:42:19,600
um,so if if policy is random,

780
00:42:19,600 --> 00:42:21,835
would you get such a quality here?

781
00:42:21,835 --> 00:42:24,880
Um, yeah, you should get.

782
00:42:24,880 --> 00:42:28,270
I mean if you can get such a quality whenever you've converged to,

783
00:42:28,270 --> 00:42:31,360
like if your Q function is converged your policy is optimal.

784
00:42:31,360 --> 00:42:35,560
Are you guaranteed such a quality against what interest are?

785
00:42:35,560 --> 00:42:38,020
No, I don't think so. Because if you're acting totally randomly in fact

786
00:42:38,020 --> 00:42:41,815
that's normally often how you start off and then you want to improve from there.

787
00:42:41,815 --> 00:42:44,980
Could you review I mean if you're, if you're if you're worried

788
00:42:44,980 --> 00:42:47,305
if it's uniform some things are going to look better than others.

789
00:42:47,305 --> 00:42:48,400
So even if acting randomly,

790
00:42:48,400 --> 00:42:50,050
some actions are going to have higher rewards than

791
00:42:50,050 --> 00:42:52,090
others and that can be reflected in your Q function.

792
00:42:52,090 --> 00:43:02,440
Any other questions before we get on to convergence in the back  .

793
00:43:02,440 --> 00:43:04,885
One outside this should um,

794
00:43:04,885 --> 00:43:08,440
yeah . Yeah, another question.

795
00:43:08,440 --> 00:43:11,560
Um, um, do you exclude um, argmax when you explore?

796
00:43:11,560 --> 00:43:12,160
Do we what?

797
00:43:12,160 --> 00:43:13,960
Do we exclude the argmax action?

798
00:43:13,960 --> 00:43:17,410
Like, you know by exploration, um, and e greedy part.

799
00:43:17,410 --> 00:43:18,205
And what is your name ?

800
00:43:18,205 --> 00:43:19,090


801
00:43:19,090 --> 00:43:19,825
Pardon.

802
00:43:19,825 --> 00:43:20,665


803
00:43:20,665 --> 00:43:24,325
Um, I- no you don't exclude it, don't exclude.

804
00:43:24,325 --> 00:43:29,455
You don't exclude the argmax action when you explore. You pick all of them.

805
00:43:29,455 --> 00:43:33,010
Um, if you wanted to that would be equivalent to sort of defining. You could do that.

806
00:43:33,010 --> 00:43:35,500
But in the simplest version, including in this proof here,

807
00:43:35,500 --> 00:43:36,970
we assume that when you're um,

808
00:43:36,970 --> 00:43:39,850
acting randomly you just sample from any of the states.

809
00:43:39,850 --> 00:43:42,490
It's often easier from implementation, too.

810
00:43:42,490 --> 00:43:44,170
Okay, great questions.

811
00:43:44,170 --> 00:43:45,370
Let's, um,

812
00:43:45,370 --> 00:43:47,335
write that up here as well.

813
00:43:47,335 --> 00:43:52,645
Er, okay, so this other really great question that's coming up from several people here.

814
00:43:52,645 --> 00:43:54,130
Um, er, I have, okay,

815
00:43:54,130 --> 00:43:55,915
what does this mean over time?

816
00:43:55,915 --> 00:43:59,965
Um, I have call it monotonic improvement and what guarantees do we have?

817
00:43:59,965 --> 00:44:02,620
So, the guarantees that we have is, um,

818
00:44:02,620 --> 00:44:06,115
if you assemble all state action pairs an infinite number of times,

819
00:44:06,115 --> 00:44:09,905
and your behavior policy converges to the greedy policy.

820
00:44:09,905 --> 00:44:11,580
So what do I mean by that?

821
00:44:11,580 --> 00:44:14,730
Um, so the behavior policy here is sort of what policy you- you're

822
00:44:14,730 --> 00:44:19,500
using versus what policy is greedy with respect to your current Q.

823
00:44:19,500 --> 00:44:25,230
So, if you have the case that as the limit as I goes to infinity,

824
00:44:25,230 --> 00:44:29,010
have Pi a given s

825
00:44:29,010 --> 00:44:37,840
goes to argmax Q,

826
00:44:37,840 --> 00:44:40,160
s, a with probability one.

827
00:44:40,440 --> 00:44:44,560
Which means that in the limit you converge to

828
00:44:44,560 --> 00:44:48,950
always taking the greedy action with respect to your Q function  .

829
00:44:51,120 --> 00:44:55,840
Then, um, then you are greedy in the limit of infinity exploration.

830
00:44:55,840 --> 00:44:57,655
That's called GLIE often.

831
00:44:57,655 --> 00:44:59,545
So that means you visit all these state action pairs

832
00:44:59,545 --> 00:45:01,720
an infinite number of times but you are also

833
00:45:01,720 --> 00:45:06,685
converging [NOISE] in the limit to be greedy with respect to your Q function.

834
00:45:06,685 --> 00:45:11,590
Um, and there's different ways to do this.The simple way to do it is to sort of,

835
00:45:11,590 --> 00:45:16,780
decay your um, your Epsilon or your or your E greedy Policy over time.

836
00:45:16,780 --> 00:45:21,985
Um, so you can reduce your Epsilon towards zero at a rate of like one over I,

837
00:45:21,985 --> 00:45:25,675
for example, that's sufficient. It's not necessarily

838
00:45:25,675 --> 00:45:28,195
This is, this is separate than what you wanna do empirically.

839
00:45:28,195 --> 00:45:31,360
This is just to sort of show under these conditions.

840
00:45:31,360 --> 00:45:36,310
Then, um, then we're generally going to be able to show that we are going to converge to

841
00:45:36,310 --> 00:45:42,670
the optimal policy and optimal value for Monte Carlo and TD methods.

842
00:45:42,670 --> 00:45:45,610
So, generally when think will talk about this again as we talk about

843
00:45:45,610 --> 00:45:48,475
some of the other algorithms generally when you're GLIE, um,

844
00:45:48,475 --> 00:45:53,140
and you have some conditions over how you're learning the Q functions,

845
00:45:53,140 --> 00:45:57,145
um, then you will be guaranteed to converge to optimal policy.

846
00:45:57,145 --> 00:45:58,590
Yeah.

847
00:45:58,590 --> 00:46:00,820
Um, do you realize  like,

848
00:46:00,820 --> 00:46:03,700
like we've seen Epsilon and.

849
00:46:03,700 --> 00:46:07,165
Yeah. So question is is

850
00:46:07,165 --> 00:46:09,160
this the only way to guarantee it, um,

851
00:46:09,160 --> 00:46:11,575
there's sort of interesting different things that are happening here.

852
00:46:11,575 --> 00:46:13,870
Um, you could be guaranteed that you're converging to

853
00:46:13,870 --> 00:46:19,345
the optimal Q function without converging to the optimal policy.

854
00:46:19,345 --> 00:46:22,555
So, you could keep Epsilon really high, um,

855
00:46:22,555 --> 00:46:24,700
and you could get a lot of information you will be

856
00:46:24,700 --> 00:46:27,055
learning about what the optimal Q function is,

857
00:46:27,055 --> 00:46:29,170
but you might not be following that policy.

858
00:46:29,170 --> 00:46:32,270
And we'll talk more about that in a, in a minute.

859
00:46:32,910 --> 00:46:37,090
All right, so let's talk a little bit more about Monte Carlo Control.

860
00:46:37,090 --> 00:46:39,190
In that given this precursor.

861
00:46:39,190 --> 00:46:42,155
So, if we wanted to do Monte Carlo Online Control,

862
00:46:42,155 --> 00:46:45,120
instead of just this evaluation we talked about before,

863
00:46:45,120 --> 00:46:48,915
we can kind of combine these ideas of   learning the Q function and doing this er,

864
00:46:48,915 --> 00:46:50,655
improvement at the same time.

865
00:46:50,655 --> 00:46:53,130
So we can initialize our Q functions and

866
00:46:53,130 --> 00:46:55,890
our counts in the same way we were talking about before.

867
00:46:55,890 --> 00:47:01,055
Um, and then what we could do is we can construct an E greedy policy.

868
00:47:01,055 --> 00:47:04,060
So E greedy policy in this case is always going to be

869
00:47:04,060 --> 00:47:06,985
that with probability one minus Epsilon.

870
00:47:06,985 --> 00:47:16,300
We pick the argmax with respect to Q with probability Epsilon we select an action and,

871
00:47:16,300 --> 00:47:25,015
let me write it this way: probability Epsilon over a we select action a.

872
00:47:25,015 --> 00:47:28,225
So we're just mixing up between this random um,

873
00:47:28,225 --> 00:47:29,680
or being greedy. Yeah.

874
00:47:29,680 --> 00:47:31,060
If I heard that so,

875
00:47:31,060 --> 00:47:34,840
actually like  the optimal actions in this case you are selecting with

876
00:47:34,840 --> 00:47:39,265
probability one minus Epsilon plus Epsilon over the cardinality of A right?

877
00:47:39,265 --> 00:47:40,240
Yeah.

878
00:47:40,240 --> 00:47:41,140
Okay.

879
00:47:41,140 --> 00:47:42,700
Several people would ask about this.

880
00:47:42,700 --> 00:47:44,710
So essentially, you're being greedy  with

881
00:47:44,710 --> 00:47:47,875
probability one minus Epsilon plus Epsilon over a.

882
00:47:47,875 --> 00:47:52,960
And then the remaining part of your probability is going to be an exploration.

883
00:47:52,960 --> 00:47:57,805
Because when you're being random you could also select what's currently the best action.

884
00:47:57,805 --> 00:48:00,880
So, um, it looks pretty similar to what we saw before.

885
00:48:00,880 --> 00:48:02,590
We're going to sample an episode,

886
00:48:02,590 --> 00:48:07,270
after we finished the episode then in this case I'm defining as first visit,

887
00:48:07,270 --> 00:48:08,830
no, you could make this every visit.

888
00:48:08,830 --> 00:48:12,440
I could do every visit.

889
00:48:13,650 --> 00:48:18,820
The same, um, benefits and restrictions apply here.

890
00:48:18,820 --> 00:48:22,135
So what we had before in the sense that you could either be getting

891
00:48:22,135 --> 00:48:24,160
a slightly more biased estimator if you're doing

892
00:48:24,160 --> 00:48:26,740
every visit but generally going to be able to use more data.

893
00:48:26,740 --> 00:48:28,555
It's going to be sort of less noisy.

894
00:48:28,555 --> 00:48:31,795
Um, so in this case what we're doing is we're

895
00:48:31,795 --> 00:48:35,710
just maintaining counts over state action pairs and we're updating our Q function.

896
00:48:35,710 --> 00:48:41,770
And then after we finished that episode then we can update, um,

897
00:48:41,770 --> 00:48:43,990
our k and our Epsilon,

898
00:48:43,990 --> 00:48:46,615
in this case we're just using Epsilon equal to one over k,

899
00:48:46,615 --> 00:48:50,245
and then we redefine our new E greedy policy with respect to Q,

900
00:48:50,245 --> 00:48:52,270
and then we get another episode.

901
00:48:52,270 --> 00:48:56,300
So that's just sort of Monte Carlo Online Control.

902
00:48:58,620 --> 00:49:02,965
So why don't we go back to that Mars Rover example?

903
00:49:02,965 --> 00:49:09,130
So, in the Mars Rover example what we had is for this is what our 2 Q functions look like.

904
00:49:09,130 --> 00:49:12,370
So, at this point what would

905
00:49:12,370 --> 00:49:15,505
be just spend a minute and say what would be our new policy, um,

906
00:49:15,505 --> 00:49:21,010
if we're at the end of this episode and- and its fine just write down tie if there,

907
00:49:21,010 --> 00:49:24,280
if there are two Q functions that have exactly the same, um,

908
00:49:24,280 --> 00:49:28,120
value for the, for the same state for two different actions and it's just a tie.

909
00:49:28,120 --> 00:49:29,830
Then you can choose how to break, the break the tie.

910
00:49:29,830 --> 00:49:34,280
Um, and then also write down what the new E greedy policy is.

911
00:49:37,410 --> 00:49:40,730
I'll just take a minute to do that.

912
00:51:04,260 --> 00:51:06,820
Okay, what's our greedy policy?

913
00:51:06,820 --> 00:51:10,405
What is the greedy policy for S1?

914
00:51:10,405 --> 00:51:11,815
A1.

915
00:51:11,815 --> 00:51:13,480
What is the greedy policy for S2?

916
00:51:13,480 --> 00:51:14,680
Two.

917
00:51:14,680 --> 00:51:16,390
And then what's our greedy policy for S3?

918
00:51:16,390 --> 00:51:17,130
One.

919
00:51:17,130 --> 00:51:19,465
And then what is it for everything else?

920
00:51:19,465 --> 00:51:25,390
Tie. Okay. And depending on your implementation you could

921
00:51:25,390 --> 00:51:28,000
either always be you could either sort of define

922
00:51:28,000 --> 00:51:29,890
your greedy policy or you would just like

923
00:51:29,890 --> 00:51:32,140
break ties randomly and keep track of that.

924
00:51:32,140 --> 00:51:33,985
Could constantly be breaking ties randomly.

925
00:51:33,985 --> 00:51:35,890
That would probably be better empirically like,

926
00:51:35,890 --> 00:51:38,815
instead of predefining one greedy policy,

927
00:51:38,815 --> 00:51:40,660
you can probably just always be Q,

928
00:51:40,660 --> 00:51:42,760
er, querying what argmax is of Q.

929
00:51:42,760 --> 00:51:45,760
And if you're getting ties just break them randomly to get more exploration.

930
00:51:45,760 --> 00:51:48,610
Um, so then if we then define

931
00:51:48,610 --> 00:51:52,750
an E greedy policy where K is three and our Epsilon is one over k,

932
00:51:52,750 --> 00:51:54,640
with what probability do we follow?

933
00:51:54,640 --> 00:52:01,580
Random. So k is three,

934
00:52:01,710 --> 00:52:04,970
Epsilon is equal to one over three.

935
00:52:04,970 --> 00:52:08,310
So that would mean that with one-third probability,

936
00:52:08,310 --> 00:52:10,500
we select something random and with two-thirds probability,

937
00:52:10,500 --> 00:52:13,120
we select the pi greedy policy.

938
00:52:13,430 --> 00:52:17,680
And then that would be the update for that particular episode.

939
00:52:24,270 --> 00:52:27,100
So, if you do this,

940
00:52:27,100 --> 00:52:31,090
if you do- if you have greedy in the limit of infinite exploration Monte Carlo,

941
00:52:31,090 --> 00:52:33,700
then you're gonna converge to the optimal state-action value.

942
00:52:33,700 --> 00:52:43,795
[NOISE] So, now, we're gonna start to talk about TD methods.

943
00:52:43,795 --> 00:52:47,155
So, similar to what we were seeing, um,

944
00:52:47,155 --> 00:52:48,880
for Q, uh, Monte Carlo,

945
00:52:48,880 --> 00:52:51,835
there is gonna be sort of this simple analogy that moves us over to TD.

946
00:52:51,835 --> 00:52:54,130
So, remember, for TD what we had before is,

947
00:52:54,130 --> 00:52:58,540
we have our V pi of S. It was equal to our previous V pi of

948
00:52:58,540 --> 00:53:04,960
S plus one minus Alpha times- oops,

949
00:53:04,960 --> 00:53:14,170
let me rewrite that- plus Alpha times R plus Gamma V pi of

950
00:53:14,170 --> 00:53:21,490
S prime minus V pi of S. [NOISE] And this was where we were

951
00:53:21,490 --> 00:53:29,080
sampling an expectation [NOISE] because we're only getting one sample of S prime,

952
00:53:29,080 --> 00:53:34,960
and we were bootstrapping because we're using our previous estimate of V pi.

953
00:53:34,960 --> 00:53:38,320
So, that was kinda the kwo- two key aspects

954
00:53:38,320 --> 00:53:41,215
of TD learning that we're both bootstrapping and sampling.

955
00:53:41,215 --> 00:53:44,005
In Monte Carlo, we were sampling, but not bootstrapping.

956
00:53:44,005 --> 00:53:47,320
Um, and one of the nice aspects of TD learning is that then we could

957
00:53:47,320 --> 00:53:51,190
update it after every tuple instead of waiting till the end of the episode.

958
00:53:51,190 --> 00:53:53,710
So, just as, like, what we do with Monte Carlo,

959
00:53:53,710 --> 00:53:55,855
we're kinda replacing all of our Vs with Qs,

960
00:53:55,855 --> 00:53:57,490
we're gonna do exactly the same thing here.

961
00:53:57,490 --> 00:54:00,520
[NOISE] So, now, we're gonna think about

962
00:54:00,520 --> 00:54:03,640
this sort of what's often known as temporal difference methods for control.

963
00:54:03,640 --> 00:54:08,650
[NOISE] So, what we're gonna do now is,

964
00:54:08,650 --> 00:54:12,580
we can do- we can estimate the Q pi function using temporal difference updating with,

965
00:54:12,580 --> 00:54:13,780
like, a e-greedy policy,

966
00:54:13,780 --> 00:54:16,525
um, and then, we could do Monte Carlo improvement

967
00:54:16,525 --> 00:54:19,390
by setting pi to an e-greedy version of Q pi.

968
00:54:19,390 --> 00:54:22,000
That would be one thing we can do.

969
00:54:22,000 --> 00:54:27,760
[NOISE] There's an algorithm called SARSA,

970
00:54:27,760 --> 00:54:33,010
which stands for state-action-reward- next state-next action, so SARSA.

971
00:54:33,010 --> 00:54:34,585
Um, how does SARSA work?

972
00:54:34,585 --> 00:54:38,185
So, what we do is, we initialize our e-greedy policy randomly.

973
00:54:38,185 --> 00:54:41,035
For example, uh, we take an action,

974
00:54:41,035 --> 00:54:43,900
we observe reward and next state, and then,

975
00:54:43,900 --> 00:54:45,535
we take another action,

976
00:54:45,535 --> 00:54:48,190
and we observe another reward and next state,

977
00:54:48,190 --> 00:54:53,125
and then, we update our Q as follows: We say our previous va- um,

978
00:54:53,125 --> 00:54:54,580
our value of Q for [NOISE] ST,

979
00:54:54,580 --> 00:54:59,260
AT is gonna be whatever our previous value was.

980
00:54:59,260 --> 00:55:02,710
Actually, I'm gonna be careful with this.

981
00:55:02,710 --> 00:55:04,540
We're not going to index them with pi

982
00:55:04,540 --> 00:55:07,420
anymore because we sort of have this running estimate,

983
00:55:07,420 --> 00:55:09,400
and our policy is gonna be changing, too.

984
00:55:09,400 --> 00:55:14,110
So, the, the Q function that we get here is now not just for one policy,

985
00:55:14,110 --> 00:55:16,105
but we're going to be averaging it over different samples,

986
00:55:16,105 --> 00:55:18,520
and we can be changing how we're acting over time.

987
00:55:18,520 --> 00:55:20,500
So, it's ST, AT,

988
00:55:20,500 --> 00:55:23,575
it's gonna be equal to Q of ST,

989
00:55:23,575 --> 00:55:29,755
AT plus one minus Alpha RT plus

990
00:55:29,755 --> 00:55:34,570
Gamma Q of ST plus

991
00:55:34,570 --> 00:55:41,270
one AT plus one minus Q of ST, AT.

992
00:55:42,900 --> 00:55:46,045
The important thing about this equation is

993
00:55:46,045 --> 00:55:50,155
that I am plugging in the actual action that was taken next.

994
00:55:50,155 --> 00:55:52,960
So, you see- you're in a state, you do an action,

995
00:55:52,960 --> 00:55:54,625
you get a reward, you go to a next state,

996
00:55:54,625 --> 00:55:56,365
and then, you do another action.

997
00:55:56,365 --> 00:55:59,635
And so, once you know what the next action is that you've done,

998
00:55:59,635 --> 00:56:01,000
then you can do this update in

999
00:56:01,000 --> 00:56:05,860
SARSA as you're actually plugging in the action that was taken.

1000
00:56:05,860 --> 00:56:09,280
And then, once you have that, you can do policy improvement in the normal way.

1001
00:56:09,280 --> 00:56:13,420
So, you can have ST is equal

1002
00:56:13,420 --> 00:56:31,030
to arg, max Q,

1003
00:56:31,030 --> 00:56:33,190
[NOISE] so, like, the E-greedy wrapper for that.

1004
00:56:33,190 --> 00:56:36,340
Now- so, this is a little bit different than Monte Carlo for two reasons.

1005
00:56:36,340 --> 00:56:37,945
Um, it's sort of,

1006
00:56:37,945 --> 00:56:40,870
uh, we're doing these tuple updates,

1007
00:56:40,870 --> 00:56:42,850
we see the state, action, reward, next state,

1008
00:56:42,850 --> 00:56:44,800
next action tuples, and then,

1009
00:56:44,800 --> 00:56:46,960
once we do those, we can update our Q function.

1010
00:56:46,960 --> 00:56:48,820
Um, we can do those along the way,

1011
00:56:48,820 --> 00:56:50,350
we don't have to [NOISE] wait till the end of the episode,

1012
00:56:50,350 --> 00:56:51,910
and similarly, we don't have to wait till the end of

1013
00:56:51,910 --> 00:56:54,190
the episode to change how we're acting in the world.

1014
00:56:54,190 --> 00:56:55,780
So, like, in the, um,

1015
00:56:55,780 --> 00:56:57,190
trajectory that we saw before,

1016
00:56:57,190 --> 00:56:59,440
we saw some states multiple times.

1017
00:56:59,440 --> 00:57:01,720
In this case, we could actually be changing our policy

1018
00:57:01,720 --> 00:57:04,465
for how we act in those states during the same episode.

1019
00:57:04,465 --> 00:57:06,820
So, if your episodes are really long, this can be really helpful.

1020
00:57:06,820 --> 00:57:14,425
[NOISE] So, in general,

1021
00:57:14,425 --> 00:57:16,765
um, I think it's often extremely helpful to,

1022
00:57:16,765 --> 00:57:21,970
um, update the policy a lot. Yeah  Is there

1023
00:57:21,970 --> 00:57:34,430
a reason [NOISE] ?

1024
00:57:34,430 --> 00:57:36,420
Oh, yeah. So, they're both the same,

1025
00:57:36,420 --> 00:57:39,675
it's just either you could write it where you put the V in the next part or not.

1026
00:57:39,675 --> 00:57:42,880
So, you can either have it as one minus Alpha times

1027
00:57:42,880 --> 00:57:46,225
your old value plus reward plus Gamma of your next thing,

1028
00:57:46,225 --> 00:57:50,380
or you could have it as V plus Alpha times, or that,

1029
00:57:50,380 --> 00:57:57,340
that should be still an Alpha here plus reward  minus V. So that's either.

1030
00:57:57,340 --> 00:57:59,440
They- they're the same. If you know that I've made a typo,

1031
00:57:59,440 --> 00:58:00,460
just let me know. Yeah.

1032
00:58:00,460 --> 00:58:05,530
Uh, and is there a reason  we use, like,

1033
00:58:05,530 --> 00:58:07,930
the next state action  pair that we choose, uh,

1034
00:58:07,930 --> 00:58:12,070
uh, A plus one rather than the max state action?

1035
00:58:12,070 --> 00:58:14,080
question's about why do we use

1036
00:58:14,080 --> 00:58:16,180
the next state action pair you choose instead of the max.

1037
00:58:16,180 --> 00:58:17,425
Q-learning is going to be the max,

1038
00:58:17,425 --> 00:58:18,775
we'll see that in about a slide.

1039
00:58:18,775 --> 00:58:21,879
Um, SARSA's basically updating on policy,

1040
00:58:21,879 --> 00:58:25,480
um, that can have- generally,

1041
00:58:25,480 --> 00:58:26,620
you want to do Q-learning,

1042
00:58:26,620 --> 00:58:28,015
which is going to be doing the max.

1043
00:58:28,015 --> 00:58:29,380
Sometimes, there's some benefits,

1044
00:58:29,380 --> 00:58:31,660
particularly in cases where, um,

1045
00:58:31,660 --> 00:58:34,584
[NOISE] you could have lot of negative outcomes,

1046
00:58:34,584 --> 00:58:39,490
that the optimism of being max can end up sort of causing your agent to make a lot of

1047
00:58:39,490 --> 00:58:42,250
bad decisions early on because it's really optimistic about what it's-

1048
00:58:42,250 --> 00:58:45,475
what it's- could do instead of what it's actually doing.

1049
00:58:45,475 --> 00:58:49,360
Um, there's a nice cliff walk example inside Sutton and Barto

1050
00:58:49,360 --> 00:58:53,410
where they show that SARSA actually is doing better in sort of early,

1051
00:58:53,410 --> 00:58:55,975
early stages, early samples compared to Q-learning,

1052
00:58:55,975 --> 00:58:58,780
because SARSA is realistic about what happens

1053
00:58:58,780 --> 00:59:01,780
if you take certain actions next to- as opposed to optimistic.

1054
00:59:01,780 --> 00:59:03,985
And if you're doing a lot of randomness, um,

1055
00:59:03,985 --> 00:59:07,210
that means that SARSA can be more realistic in the early stages.

1056
00:59:07,210 --> 00:59:09,640
But empirically, generally, you want to do Q-learning,

1057
00:59:09,640 --> 00:59:15,070
and both will convert to the same thing. Yeah.

1058
00:59:15,070 --> 00:59:18,370
Um, so, [NOISE]  ,

1059
00:59:18,370 --> 00:59:24,170
should be, um, Q  ST one A be plus one be ST plus one.

1060
00:59:24,300 --> 00:59:32,590
Yeah. Thank you. Yes

1061
00:59:32,590 --> 00:59:35,410
This might be  question but you're talking about

1062
00:59:35,410 --> 00:59:38,410
how its getting the information from the future action,

1063
00:59:38,410 --> 00:59:40,240
but you have to have already done that action.

1064
00:59:40,240 --> 00:59:43,060
So, why is it called, um, er,

1065
00:59:43,060 --> 00:59:47,335
state action or -or next state actions,

1066
00:59:47,335 --> 00:59:51,220
when it's really the past one that you're updating from what I'm understanding.

1067
00:59:51,220 --> 00:59:54,430
Because you- you're doing this one and you're using the information you learn,

1068
00:59:54,430 --> 00:59:55,855
that take the one in the back.

1069
00:59:55,855 --> 00:59:58,360
So, why is it- why are we talking about it like

1070
00:59:58,360 --> 01:00:01,655
it's a future action? What's the purpose of that?

1071
01:00:01,655 --> 01:00:07,480
Um, all right. I- I don't think isn't about the particular terms used to define SARSA ,

1072
01:00:07,480 --> 01:00:08,935
I don't think it- I mean, I guess,

1073
01:00:08,935 --> 01:00:11,740
it's really just that you have to wait till you get,

1074
01:00:11,740 --> 01:00:14,335
um,f that- that last A is important.

1075
01:00:14,335 --> 01:00:16,660
So that instead of saying that- but before we

1076
01:00:16,660 --> 01:00:19,120
thought with TD learning if you were in a state action reward

1077
01:00:19,120 --> 01:00:21,940
next state and then you could update your Q-function

1078
01:00:21,940 --> 01:00:23,320
now we're just saying you have to wait till you've

1079
01:00:23,320 --> 01:00:25,195
actually decided what to do in that next state.

1080
01:00:25,195 --> 01:00:26,165
Okay.

1081
01:00:26,165 --> 01:00:30,745
Because that's how you're choosing how to do update your Q-function here,

1082
01:00:30,745 --> 01:00:33,830
and that's what you're plugging in for your target.

1083
01:00:35,700 --> 01:00:38,440
So, in terms of the convergent properties,

1084
01:00:38,440 --> 01:00:41,170
um, it requires a couple of different things,

1085
01:00:41,170 --> 01:00:44,715
uh, so, if we are,

1086
01:00:44,715 --> 01:00:46,380
um, we need sort of two things.

1087
01:00:46,380 --> 01:00:48,960
We're gonna need the fact that we're- we're updating

1088
01:00:48,960 --> 01:00:52,260
our Q-function and it's gonna be updating incrementally,

1089
01:00:52,260 --> 01:00:54,540
and so, like what we talked about before,

1090
01:00:54,540 --> 01:00:56,630
we're gonna need some conditions over the Alphas.

1091
01:00:56,630 --> 01:00:58,930
Um, if alpha is equal to 1, uh,

1092
01:00:58,930 --> 01:01:00,910
generally your Q-function is not gonna converge,

1093
01:01:00,910 --> 01:01:03,535
because it means you're not remembering anything about the past.

1094
01:01:03,535 --> 01:01:05,260
Um, if alpha is 0,

1095
01:01:05,260 --> 01:01:06,745
then you're not updating anymore.

1096
01:01:06,745 --> 01:01:09,385
So, generally, you need something in terms of the step sizes,

1097
01:01:09,385 --> 01:01:13,000
which allows you to sort of slowly be incrementing but still be converging.

1098
01:01:13,000 --> 01:01:16,720
So these are one sufficient set of conditions, um,

1099
01:01:16,720 --> 01:01:19,270
so if you have stuff like Alpha T,

1100
01:01:19,270 --> 01:01:21,610
is equal to 1 over T. Now

1101
01:01:21,610 --> 01:01:25,330
empirically often you're going to want to pick very different forms of learning rates.

1102
01:01:25,330 --> 01:01:27,010
So Alpha T, is often referred to as,

1103
01:01:27,010 --> 01:01:30,110
like, the learning rate parameter,

1104
01:01:30,900 --> 01:01:43,899
and empirically you are often not gonna wanna use this [NOISE],

1105
01:01:43,899 --> 01:01:45,040
generally not gonna use this.

1106
01:01:45,040 --> 01:01:47,575
This is gonna, um, uh,

1107
01:01:47,575 --> 01:01:50,590
you- you're often gonna wanna use different things empirically,

1108
01:01:50,590 --> 01:01:52,870
you could end up using sometimes small constants,

1109
01:01:52,870 --> 01:01:54,190
or slowly decaying constants.

1110
01:01:54,190 --> 01:01:55,870
Often that depends on the domain,

1111
01:01:55,870 --> 01:01:59,635
but this is from a theoretical side what is sufficient to ensure convergence.

1112
01:01:59,635 --> 01:02:02,290
And then the other aspect is that the way that your,

1113
01:02:02,290 --> 01:02:06,025
uh, that your policy itself has to satisfy the condition of GLIE,

1114
01:02:06,025 --> 01:02:07,930
which means that you are, sort of,

1115
01:02:07,930 --> 01:02:10,810
slowly getting more greedy over the time but you're doing so in a way that

1116
01:02:10,810 --> 01:02:14,455
you're still sampling all state action pairs an infinite number of times.

1117
01:02:14,455 --> 01:02:17,905
Now- now, just note for a second that that's not always possible,

1118
01:02:17,905 --> 01:02:19,885
like, so if you have a domain, um,

1119
01:02:19,885 --> 01:02:25,510
where, uh, things are not reachable after a point it's not argotic,

1120
01:02:25,510 --> 01:02:27,970
you can't get back to certain states after you get there.

1121
01:02:27,970 --> 01:02:30,790
Let's say you're flying a helicopter and you break the helicopter.

1122
01:02:30,790 --> 01:02:32,440
So you can't get back up there, um,

1123
01:02:32,440 --> 01:02:34,180
then you're not gonna be able to satisfy GLIE

1124
01:02:34,180 --> 01:02:36,160
because at some point you broke your helicopter and

1125
01:02:36,160 --> 01:02:37,960
then like you have no idea what it would have been like

1126
01:02:37,960 --> 01:02:40,180
if you continued to fly your helicopter in the air.

1127
01:02:40,180 --> 01:02:45,310
So, there can be some domains for which it is very hard to satisfy GLIE, um,

1128
01:02:45,310 --> 01:02:47,260
but we generally are going to ignore those even

1129
01:02:47,260 --> 01:02:49,180
though there are some really interesting work on,

1130
01:02:49,180 --> 01:02:51,295
so, how do we deal with those cases as well.

1131
01:02:51,295 --> 01:02:54,805
In those cases, somebody might assume that it's more of an episodic problem,

1132
01:02:54,805 --> 01:02:57,580
so maybe you have like a 100 helicopters and so when you

1133
01:02:57,580 --> 01:03:01,120
crash one that's considered a termination condition and then you get out your next one.

1134
01:03:01,120 --> 01:03:05,545
Um, so you may or may not be able to be greedy in the limit of information,

1135
01:03:05,545 --> 01:03:08,140
in the limit of infinite exploration there but you can,

1136
01:03:08,140 --> 01:03:10,135
sort of, have a bounded amounts of exploration.

1137
01:03:10,135 --> 01:03:12,130
And we're going to talk a lot more later about, sort of,

1138
01:03:12,130 --> 01:03:15,760
how to do this exploration in a much more smart manner and in a way that can give us

1139
01:03:15,760 --> 01:03:20,540
finite example guarantees on how much data we need to learn a good policy.

1140
01:03:23,700 --> 01:03:27,100
So, this is just what I said before which is,

1141
01:03:27,100 --> 01:03:29,830
you know, we generally are not gonna use the step type.

1142
01:03:29,830 --> 01:03:35,020
where you have

1143
01:03:35,020 --> 01:03:40,300
Q  plus alpha because 1 is alpha, times the-the .

1144
01:03:40,300 --> 01:03:42,700
Yeah. Okay, yeah. So this is the- this is for SARSA.

1145
01:03:42,700 --> 01:03:45,205
So this is the condition for SARSA assuming that-

1146
01:03:45,205 --> 01:03:49,690
that particular update of how we're updating our Q-functions.

1147
01:03:49,690 --> 01:03:52,180
Okay? So, yeah.

1148
01:03:52,180 --> 01:03:57,130
Uh, so in the Monte Carlo case,

1149
01:03:57,130 --> 01:04:00,280
we have sufficient  condition on- with the pie

1150
01:04:00,280 --> 01:04:03,700
that has been GLIE with the Epsilon going down to 1 over t.

1151
01:04:03,700 --> 01:04:05,770
Do we have anything similar in general?

1152
01:04:05,770 --> 01:04:08,380
Um, great question. Uh, question is about if,

1153
01:04:08,380 --> 01:04:11,170
for the Monte Carlo, do we have a sufficient,  uh, a similar condition.

1154
01:04:11,170 --> 01:04:12,595
If you're just, um,

1155
01:04:12,595 --> 01:04:15,350
if you're doing first visit that alone is sufficient.

1156
01:04:15,350 --> 01:04:18,825
Because you're getting an unbiased estimator that's converging

1157
01:04:18,825 --> 01:04:21,030
for all of your returns with only a

1158
01:04:21,030 --> 01:04:23,280
few of all the state action pairs in infinite number of times.

1159
01:04:23,280 --> 01:04:26,520
If you're doing it in this incremental fashion, um,

1160
01:04:26,520 --> 01:04:30,910
then if you're, if you're- if you're playing around with how about Alpha is,

1161
01:04:30,910 --> 01:04:33,290
then you need to have similar conditions to make sure it guarantees.

1162
01:04:33,290 --> 01:04:36,535
What I mean is, uh, how do you,

1163
01:04:36,535 --> 01:04:39,595
like, how do you know that condition will impulse?

1164
01:04:39,595 --> 01:04:41,185
How do you know that things are GLIE?

1165
01:04:41,185 --> 01:04:43,930
Yes. Like in Monte Carlo we did have condition rate.

1166
01:04:43,930 --> 01:04:46,465
It was that Epsilon decay as long as it-

1167
01:04:46,465 --> 01:04:48,010
Oh, so, um, great question.

1168
01:04:48,010 --> 01:04:49,885
so is like, how do you make sure something's GLIE.

1169
01:04:49,885 --> 01:04:53,440
Um, one sufficient condition is that Epsilon is 1 over- uh,

1170
01:04:53,440 --> 01:04:55,315
it's over T or one over I.

1171
01:04:55,315 --> 01:04:57,250
And do you know like, uh,

1172
01:04:57,250 --> 01:05:02,290
with that  that work like, yeah.

1173
01:05:02,290 --> 01:05:05,620
Oh, if you need to like know if there are sufficient conditions like this?

1174
01:05:05,620 --> 01:05:08,800
Yeah, like, it will be GLIE if and only

1175
01:05:08,800 --> 01:05:13,360
if- if Epsilon put to 0 but there's some diverges or something like that?

1176
01:05:13,360 --> 01:05:15,940
Yeah I think it's quite similar as  sequence,

1177
01:05:15,940 --> 01:05:17,290
similar like you're, sort of,

1178
01:05:17,290 --> 01:05:20,500
essentially you're- you're ensuring that you're doing infinite number of updates,

1179
01:05:20,500 --> 01:05:22,030
infinite amount of grid its like

1180
01:05:22,030 --> 01:05:25,345
random exploration but still its going down fast enough to converge in.

1181
01:05:25,345 --> 01:05:28,190
I think it's probably exactly the same but converges.

1182
01:05:30,030 --> 01:05:32,620
Okay. So then when we get into Q-Learning which

1183
01:05:32,620 --> 01:05:34,360
is related to the question which was asked, okay,

1184
01:05:34,360 --> 01:05:36,160
why are we just picking in that particular action

1185
01:05:36,160 --> 01:05:38,365
next why don't we just pick the max. Um, yeah.

1186
01:05:38,365 --> 01:05:39,625
We could just pick the max instead.

1187
01:05:39,625 --> 01:05:42,790
So SARSA is picking this particular action next,

1188
01:05:42,790 --> 01:05:47,470
Q-learning is picking the max action next. Yeah.

1189
01:05:47,470 --> 01:05:50,830
as   you said what  does it take to do better

1190
01:05:50,830 --> 01:05:54,430
early on because its not too often that statement that  later.

1191
01:05:54,430 --> 01:05:57,520
Um, is there any way that we could mix SARSA and Q-learning,

1192
01:05:57,520 --> 01:05:58,600
you certainly could, um,

1193
01:05:58,600 --> 01:06:02,365
but then that also means that maybe I wasn't being clear enough with the earlier part.

1194
01:06:02,365 --> 01:06:05,500
So SARSA can do better in some domains early

1195
01:06:05,500 --> 01:06:08,845
on particularly if there's a lot of really negative rewards because it's being realistic,

1196
01:06:08,845 --> 01:06:10,360
um, another case is Q learning.

1197
01:06:10,360 --> 01:06:11,815
Will it be better even early on?

1198
01:06:11,815 --> 01:06:15,790
Because you're being more optimistic and as we talked about a little bit before,

1199
01:06:15,790 --> 01:06:18,550
often optimism is really helpful for exploration.

1200
01:06:18,550 --> 01:06:22,150
The cliff walk example in Sutton and Barto is a case where

1201
01:06:22,150 --> 01:06:25,420
some actions lead the agent to like fall off a cliff and so

1202
01:06:25,420 --> 01:06:27,190
some actions are really bad and so there

1203
01:06:27,190 --> 01:06:29,140
being optimistic early on means that you're gonna take

1204
01:06:29,140 --> 01:06:33,205
a lot of really bad decisions and suffer a lot of negative rewards for a while.

1205
01:06:33,205 --> 01:06:36,250
Many other domains are not like that so depends on a lot.

1206
01:06:36,250 --> 01:06:38,420
And yes you could certainly mix them.

1207
01:06:39,810 --> 01:06:45,385
Alright. So I guess in terms of Q-learning one thing that's interesting here is,

1208
01:06:45,385 --> 01:06:50,365
uh, so we can again sort of think about how are we're improving this and we're gonna,

1209
01:06:50,365 --> 01:06:54,790
sort of, be e greedy with respect to the current estimate of optimal Q,

1210
01:06:54,790 --> 01:06:59,200
and- and really this is quite similar to what we we're doing in

1211
01:06:59,200 --> 01:07:03,370
SARSA except for now when we update this Q we're really just gonna be doing this MAX.

1212
01:07:03,370 --> 01:07:09,620
So Q of ST, AT is gonna be equal to the previous value,

1213
01:07:10,260 --> 01:07:15,595
plus alpha or plus max over A.

1214
01:07:15,595 --> 01:07:19,810
So, now also note that you can update this a little bit earlier, so,

1215
01:07:19,810 --> 01:07:26,230
you don't have to wait until the next action is taken.

1216
01:07:26,230 --> 01:07:28,885
So, you only need to observe this part.

1217
01:07:28,885 --> 01:07:31,450
You don't need to actually see the next action that's taken and then

1218
01:07:31,450 --> 01:07:34,210
you can perform policy improvement,

1219
01:07:34,210 --> 01:07:35,515
and in general, in this case,

1220
01:07:35,515 --> 01:07:37,450
you're only gonna- you only need

1221
01:07:37,450 --> 01:07:40,075
to update the policy for the state that you were just in.

1222
01:07:40,075 --> 01:07:41,950
So you can do pi,

1223
01:07:41,950 --> 01:07:46,570
you can update pi b for ST for the action you just took.

1224
01:07:46,570 --> 01:07:50,420
You don't need to- particularly, in large state space, that can be helpful.

1225
01:07:51,690 --> 01:07:54,550
So we actually ended up talking about this a little bit

1226
01:07:54,550 --> 01:07:57,850
already about whether or not how you initialize Q matters.

1227
01:07:57,850 --> 01:08:00,070
It doesn't asymptotically, I mean,

1228
01:08:00,070 --> 01:08:03,070
if you have a case where your Q function is gonna converge to the right thing,

1229
01:08:03,070 --> 01:08:05,320
it will still converge to the right thing no matter how you

1230
01:08:05,320 --> 01:08:08,770
initialize it as long as it satisfies these other conditions,

1231
01:08:08,770 --> 01:08:11,770
but it certainly matters a lot empirically and so even though

1232
01:08:11,770 --> 01:08:15,280
often we think of just initializing it randomly or initialize it with 0,

1233
01:08:15,280 --> 01:08:18,055
initializing it optimistically is often really helpful.

1234
01:08:18,055 --> 01:08:24,250
So we'll talk more about that when we talk about exploration . Yeah

1235
01:08:24,250 --> 01:08:27,024
On the previous slide  line six,

1236
01:08:27,024 --> 01:08:31,999
either max or a argmax?. Thank you.

1237
01:08:32,000 --> 01:08:38,380
[NOISE]

1238
01:08:38,380 --> 01:08:41,189
So now, um, if we do Q-learning.

1239
01:08:41,189 --> 01:08:42,749
Um, Let's see.

1240
01:08:42,750 --> 01:08:45,300
I think wha- I'm gonna leave this as just an exercise you can do later,

1241
01:08:45,300 --> 01:08:47,930
but you could just do the exact same exercise for Q-learning,

1242
01:08:47,930 --> 01:08:49,810
um, and see how these updates propagate.

1243
01:08:49,810 --> 01:08:57,600
Um, so just like Monte Carlo versus Q- Monte Carlo versus TD for policy evaluation,

1244
01:08:57,600 --> 01:08:59,830
there's some of the same issues with Q-learning.

1245
01:08:59,830 --> 01:09:05,069
Q-learning is only gonna update your Q function for the state you are just in.

1246
01:09:05,069 --> 01:09:07,909
So, even if it turns out later in the same episode,

1247
01:09:07,910 --> 01:09:08,990
you get a really high reward.

1248
01:09:08,990 --> 01:09:10,380
You're not gonna backpropagate

1249
01:09:10,380 --> 01:09:14,645
that information at the end of the episode in the way that you would with Monte Carlo.

1250
01:09:14,645 --> 01:09:17,529
So Q-learning updates can be much slower often,

1251
01:09:17,529 --> 01:09:19,749
um, than Monte Carlo.

1252
01:09:19,750 --> 01:09:23,010
Just like enter that has implications for how quickly you

1253
01:09:23,010 --> 01:09:27,015
can learn to make better decisions [NOISE].

1254
01:09:27,015 --> 01:09:29,640
So, the conditions that are sufficient to

1255
01:09:29,640 --> 01:09:31,990
ensure that Q-learning with the -greedy converges,

1256
01:09:31,990 --> 01:09:33,990
it's basically the same as SARSA.

1257
01:09:33,990 --> 01:09:36,300
We need to make sure that things are, um,

1258
01:09:36,300 --> 01:09:43,115
that are GLIE, and,

1259
01:09:43,115 --> 01:09:46,165
I see, and slightly revise this.

1260
01:09:46,165 --> 01:09:49,055
So, if you just wanna make sure that you converge,

1261
01:09:49,055 --> 01:09:52,560
that needed to be the all SA infinitely often.

1262
01:09:52,560 --> 01:09:55,100
I need to have these conditions on the Alpha.

1263
01:09:55,100 --> 01:10:04,720
So if you look at the same conditions,

1264
01:10:04,720 --> 01:10:06,580
in order for the Q functions to converge,

1265
01:10:06,580 --> 01:10:08,255
you need to have these conditions on how you're

1266
01:10:08,255 --> 01:10:10,565
updating your li- like what you're learning rates are.

1267
01:10:10,565 --> 01:10:13,415
Ah, and that you visit all state action pairs infinitely often.

1268
01:10:13,415 --> 01:10:17,500
But that just- that's sufficient to allow you to converge to the optimal Q values.

1269
01:10:17,500 --> 01:10:20,030
And then if you want to actually make sure that the policy

1270
01:10:20,030 --> 01:10:23,035
you're following is really the optimal policy,

1271
01:10:23,035 --> 01:10:24,360
then you need to be GLIE.

1272
01:10:24,360 --> 01:10:27,970
You also need the policy you chose to be more and more greedy.

1273
01:10:27,970 --> 01:10:34,195
All right, let me just briefly into the maximization bias before we finish.

1274
01:10:34,195 --> 01:10:36,620
The maximization bias is an interesting question.

1275
01:10:36,620 --> 01:10:38,620
Ah, so why are we going to talk about this?

1276
01:10:38,620 --> 01:10:40,910
Well okay, let's go back to this one.

1277
01:10:40,910 --> 01:10:42,980
So in Q learning, what are we doing?

1278
01:10:42,980 --> 01:10:44,110
In Q learning, we're computing

1279
01:10:44,110 --> 01:10:46,640
the Q function and then we're being e-greedy with respect to it.

1280
01:10:46,640 --> 01:10:48,790
Now, we're going to need some more data and we're re-updating

1281
01:10:48,790 --> 01:10:51,405
our Q function and we're being greedy with respect to it.

1282
01:10:51,405 --> 01:10:53,550
And so we're e-greedy with respect to it.

1283
01:10:53,550 --> 01:10:55,140
And so, we're always sort of doing

1284
01:10:55,140 --> 01:10:57,470
this dance between updating stuff, getting more evidence,

1285
01:10:57,470 --> 01:11:01,750
but then trying to kind of exploit that knowledge up to some random exploration.

1286
01:11:01,750 --> 01:11:06,215
And the maximization bias points out that maybe there can be some problems with this.

1287
01:11:06,215 --> 01:11:08,550
Okay. So, let's just consider a particular example.

1288
01:11:08,550 --> 01:11:12,155
Imagine there is a single state MDP which means there's only one state.

1289
01:11:12,155 --> 01:11:16,765
Um, but there are two actions and both of them actually have 0 mean random rewards.

1290
01:11:16,765 --> 01:11:20,005
So now, you can think of these as being like, Gaussians.

1291
01:11:20,005 --> 01:11:22,100
Right now, we're mostly talking about it when the reward is

1292
01:11:22,100 --> 01:11:24,270
actually deterministic but it doesn't have to be.

1293
01:11:24,270 --> 01:11:26,605
It could be stochastic reward.

1294
01:11:26,605 --> 01:11:30,400
But in this case, where you would imagine that whether you take action a1 or action a2,

1295
01:11:30,400 --> 01:11:32,285
your expected value is zero,

1296
01:11:32,285 --> 01:11:36,400
but the value you get on any particular episode- any particular step might not be zero.

1297
01:11:36,400 --> 01:11:40,490
Might be one or minus one or things like that.

1298
01:11:40,490 --> 01:11:43,000
The average is still zero but on any particular step,

1299
01:11:43,000 --> 01:11:45,680
you could have something different, okay?

1300
01:11:45,680 --> 01:11:48,065
But the expected value is zero um,

1301
01:11:48,065 --> 01:11:50,720
and so the Q value for both sa1 and

1302
01:11:50,720 --> 01:11:53,350
the Q value for sa2 is zero which is the same as the value.

1303
01:11:53,350 --> 01:11:57,375
And these are all the optimal Q and S values.

1304
01:11:57,375 --> 01:12:00,000
So let's imagine that there are some prior samples.

1305
01:12:00,000 --> 01:12:01,565
You've tried action a1 a bunch of times,

1306
01:12:01,565 --> 01:12:03,480
you've tried action a2 a bunch of times,

1307
01:12:03,480 --> 01:12:05,965
and you compute an empirical estimate of this.

1308
01:12:05,965 --> 01:12:07,350
And here again where um,

1309
01:12:07,350 --> 01:12:09,605
there's just a single state.

1310
01:12:09,605 --> 01:12:11,975
Um, and we can just average over these.

1311
01:12:11,975 --> 01:12:15,140
Let's imagine that it's super simple that we have um,

1312
01:12:15,140 --> 01:12:17,090
gamma is equal to zeros.

1313
01:12:17,090 --> 01:12:19,710
So, we're really just estimating over the immediate reward.

1314
01:12:19,710 --> 01:12:21,640
Okay, so there's no future rewards.

1315
01:12:21,640 --> 01:12:24,595
We're just saying all the times that we've tried this action before.

1316
01:12:24,595 --> 01:12:26,895
What were all the rewards we get when we average?

1317
01:12:26,895 --> 01:12:29,065
And now what we wanna do is we wanna take

1318
01:12:29,065 --> 01:12:32,180
our empirical estimates of the Q function for a1 and a2,

1319
01:12:32,180 --> 01:12:34,460
and we want to figure out what the greedy policy is.

1320
01:12:34,460 --> 01:12:38,280
And the problem is that it can be biased.

1321
01:12:38,650 --> 01:12:44,505
So even though each of these unbiased estimators of k- of Q are themselves,

1322
01:12:44,505 --> 01:12:47,165
even- even though the two estimates the ah,

1323
01:12:47,165 --> 01:12:51,800
actions are unbiased, when you take a max over it, it can be biased.

1324
01:12:51,800 --> 01:12:59,190
Let's just write out what that is. So our V Pi hat is equal to the expected value of

1325
01:12:59,190 --> 01:13:06,325
max over Q a1, Q a2.

1326
01:13:06,325 --> 01:13:13,290
So I'm going to

1327
01:13:13,290 --> 01:13:15,930
be taking the expected value of max of these two things because

1328
01:13:15,930 --> 01:13:18,640
that's how I defined my policy.

1329
01:13:18,640 --> 01:13:21,875
My policy says pick whichever of these two empirically looks best.

1330
01:13:21,875 --> 01:13:24,020
But we know that from Jensens,

1331
01:13:24,020 --> 01:13:27,220
this is greater than equal to if you switch the max and the expectation [NOISE].

1332
01:13:27,220 --> 01:13:42,200
And this is just equal to max of zero, zero.

1333
01:13:43,350 --> 01:13:46,410
So the important part is this,

1334
01:13:49,070 --> 01:13:54,540
and this is equal to the true V Pi.

1335
01:13:55,540 --> 01:14:00,005
So that means that whatever we compute um,

1336
01:14:00,005 --> 01:14:02,195
can be a biased estimator of the true V Pi.

1337
01:14:02,195 --> 01:14:04,465
So why did this happen?

1338
01:14:04,465 --> 01:14:05,800
Well if you get ah, you know,

1339
01:14:05,800 --> 01:14:08,930
if you only have a finite number of samples um,

1340
01:14:08,930 --> 01:14:12,250
I- if I have tried action a1 a finite number of times,

1341
01:14:12,250 --> 01:14:14,360
it might be on that finite number of times it happens to

1342
01:14:14,360 --> 01:14:16,705
look slightly positive like it's like,

1343
01:14:16,705 --> 01:14:18,470
a 0.1 instead of zero.

1344
01:14:18,470 --> 01:14:20,445
And then when I take my policy,

1345
01:14:20,445 --> 01:14:21,930
I'm going to maximize over those.

1346
01:14:21,930 --> 01:14:24,000
So I'm going to immediately exploit whichever one happens

1347
01:14:24,000 --> 01:14:26,985
to look better even if with statistical chance.

1348
01:14:26,985 --> 01:14:31,460
So that's why you can get this maximization bias.

1349
01:14:32,590 --> 01:14:37,010
And the same thing can happen in terms of MDPs.

1350
01:14:37,010 --> 01:14:40,595
So ah, this generally can happen.

1351
01:14:40,595 --> 01:14:43,765
You can also look at some nice examples from this paper by

1352
01:14:43,765 --> 01:14:46,155
Johns- Johnson Tsitsiklis and

1353
01:14:46,155 --> 01:14:49,680
Shie Mannor where they show how this can also happen in Markov decision processes.

1354
01:14:49,680 --> 01:14:52,940
Where essentially if you ah,

1355
01:14:52,940 --> 01:14:55,780
if your estimates for these Q functions ah,

1356
01:14:55,780 --> 01:14:58,330
then you're going to be sort of biased to whatever has happened to look good in

1357
01:14:58,330 --> 01:15:01,795
your data and so you can have a maximization bias.

1358
01:15:01,795 --> 01:15:04,665
So one thing that was proposed to try to

1359
01:15:04,665 --> 01:15:07,965
handle- deal with this case is called double Q learning.

1360
01:15:07,965 --> 01:15:10,725
And so the idea is instead of ah,

1361
01:15:10,725 --> 01:15:12,770
having one Q function,

1362
01:15:12,770 --> 01:15:15,365
we are going to have two different Q functions.

1363
01:15:15,365 --> 01:15:19,110
And we're going to create two independent unbiased estimators of Q,

1364
01:15:19,110 --> 01:15:20,880
and you're going to use one of them for

1365
01:15:20,880 --> 01:15:25,295
your decision-making and the other to try to estimate the value.

1366
01:15:25,295 --> 01:15:29,280
And that's gonna allow us to have an unbiased estimator.

1367
01:15:30,520 --> 01:15:35,190
And the reason that you might want to do this is because ah,

1368
01:15:35,190 --> 01:15:38,670
then it can sort of help- help with this issue that

1369
01:15:38,670 --> 01:15:41,900
you can end up being overly bias towards things that have happened to look good.

1370
01:15:41,900 --> 01:15:45,000
Yes, now you're separating like between the samples that you're ga-

1371
01:15:45,000 --> 01:15:48,935
that you're getting to estimate how good an action is versus ah,

1372
01:15:48,935 --> 01:15:52,055
the way you're trying to estimate your policy.

1373
01:15:52,055 --> 01:15:55,540
So I'm just going to be a little brief with this because of time.

1374
01:15:55,540 --> 01:15:57,720
Q learning basically- double Q learning basically

1375
01:15:57,720 --> 01:16:00,265
means that we're going to have these two different Q functions.

1376
01:16:00,265 --> 01:16:02,900
Um, and then with 50% probability,

1377
01:16:02,900 --> 01:16:05,850
we're going to update one, at 50% probability, we're going to update the other.

1378
01:16:05,850 --> 01:16:10,570
So, this was- and in this case,

1379
01:16:11,130 --> 01:16:13,895
I'm going to skip out all others um,

1380
01:16:13,895 --> 01:16:16,085
the final slides I want to show you the difference.

1381
01:16:16,085 --> 01:16:19,770
Um, the difference here can be significant sometimes.

1382
01:16:19,770 --> 01:16:20,890
So, in this case,

1383
01:16:20,890 --> 01:16:24,010
this is sort of looking at the percent of time that we're taking

1384
01:16:24,010 --> 01:16:27,260
bad actions in this domain where you can have,

1385
01:16:27,260 --> 01:16:29,575
in this case, you have a scenario where

1386
01:16:29,575 --> 01:16:32,290
it's actually the wrong thing to do but it's stochastic.

1387
01:16:32,290 --> 01:16:33,875
And so with a small amount of data,

1388
01:16:33,875 --> 01:16:37,150
it can end up looking better compared to another option where

1389
01:16:37,150 --> 01:16:41,760
the reward is deterministic and actually better but has no stochasticity,

1390
01:16:41,760 --> 01:16:45,800
and then Q learning can suffer quite a lot from this maximization bias.

1391
01:16:45,800 --> 01:16:48,520
Um, if you're using the same Q function to essentially

1392
01:16:48,520 --> 01:16:51,665
immediately define your policy as you are um,

1393
01:16:51,665 --> 01:16:54,190
for estimating the value of that policy,

1394
01:16:54,190 --> 01:16:58,140
whereas double Q learning does a lot better in this case.

1395
01:16:58,800 --> 01:17:03,665
So it's something to consider in terms of when you're implementing these things

1396
01:17:03,665 --> 01:17:05,505
and it's pretty small overhead

1397
01:17:05,505 --> 01:17:08,760
too because you can just maintain two different Q functions.

1398
01:17:08,770 --> 01:17:12,570
Right. I know that was a little bit fast but make sure to put details on there,

1399
01:17:12,570 --> 01:17:16,960
um, when I- we upload the additional slides today um.

1400
01:17:16,960 --> 01:17:20,050
The main things that you should know from today is to be able to understand how you do

1401
01:17:20,050 --> 01:17:24,000
this Monte Carlo on policy controls and same for SARSA and Q-learning.

1402
01:17:24,000 --> 01:17:27,205
It's useful to understand how quickly they update, um,

1403
01:17:27,205 --> 01:17:29,560
both in terms of whether you have to wait to the end of

1404
01:17:29,560 --> 01:17:32,395
the episode and then how quickly information propagates back.

1405
01:17:32,395 --> 01:17:35,260
And also to understand how to define the conditions on

1406
01:17:35,260 --> 01:17:39,400
the algorithms converging to the optimal Q function. Thanks.

