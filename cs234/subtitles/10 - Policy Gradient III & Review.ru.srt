1
00:00:05,600 --> 00:00:07,400
поэтому, прежде чем мы начнем, я просто хочу

2
00:00:07,400 --> 00:00:09,260
сделать краткое примечание о логистике

3
00:00:09,260 --> 00:00:10,850
на промежуточный экзамен, мы будем разделены на

4
00:00:10,850 --> 00:00:13,250
две комнаты, комната, в которой вы находитесь, зависит

5
00:00:13,250 --> 00:00:13,820
от вашего

6
00:00:13,820 --> 00:00:15,889
обычного Стэмфордского удостоверения.

7
00:00:15,889 --> 00:00:18,710
отправьте электронное письмо об

8
00:00:18,710 --> 00:00:20,900
этом, чтобы подтвердить это, но мы собираемся быть

9
00:00:20,900 --> 00:00:23,180
либо в воротах b1, либо в аудитории Cubberley,

10
00:00:23,180 --> 00:00:25,369
и это зависит от первых нескольких

11
00:00:25,369 --> 00:00:28,669
первых букв вашего Стэнфордского удостоверения личности,

12
00:00:28,669 --> 00:00:30,140
кроме того, вам разрешено иметь одну

13
00:00:30,140 --> 00:00:32,809
страницу заметок, напечатанных или написанных как  хорошо, с одной

14
00:00:32,809 --> 00:00:35,420
стороны, у нас есть другие вопросы по

15
00:00:35,420 --> 00:00:40,309
поводу промежуточного экзамена, хорошо, мы свяжемся с нами

16
00:00:40,309 --> 00:00:41,570
на Пьяцца, если у вас есть какие-либо вопросы по

17
00:00:41,570 --> 00:00:44,089
поводу промежуточного экзамена, что мы будем делать сегодня,

18
00:00:44,089 --> 00:00:46,789
мы разделимся, мы собираемся

19
00:00:46,789 --> 00:00:48,770
закончить остальное  градиента политики, так что с

20
00:00:48,770 --> 00:00:50,480
точки зрения того, где мы находимся в классе прямо

21
00:00:50,480 --> 00:00:53,899
сейчас, мы почти закончили поиск политики,

22
00:00:53,899 --> 00:00:55,039
у нас будет промежуточный экзамен в

23
00:00:55,039 --> 00:00:57,020
среду, понедельник - выходной, а затем

24
00:00:57,020 --> 00:00:58,730
мы также выпустим последнее

25
00:00:58,730 --> 00:01:00,770
домашнее задание на этой неделе, которое будет  быть над

26
00:01:00,770 --> 00:01:03,199
поиском политики, и поэтому у нас будет

27
00:01:03,199 --> 00:01:04,729
поиск политики, а затем у нас

28
00:01:04,729 --> 00:01:06,770
будет проект, который представляет собой оставшиеся

29
00:01:06,770 --> 00:01:09,350
основные задания на семестр, а затем

30
00:01:09,350 --> 00:01:10,369
мы собираемся перейти к быстрому

31
00:01:10,369 --> 00:01:11,990
исследованию и своего рода быстрому

32
00:01:11,990 --> 00:01:13,640
обучению с подкреплением после того, как мы

33
00:01:13,640 --> 00:01:15,830
вернемся с середины семестра, поэтому я хотел

34
00:01:15,830 --> 00:01:17,240
не забудьте выполнить поиск политики

35
00:01:17,240 --> 00:01:18,590
сегодня, потому что вы получите

36
00:01:18,590 --> 00:01:20,150
выпуск задания позже на этой неделе, поэтому

37
00:01:20,150 --> 00:01:21,680
мы надеемся потратить около

38
00:01:21,680 --> 00:01:23,390
20 25 минут на поиск политики, а затем мы

39
00:01:23,390 --> 00:01:25,369
сделаем краткий обзор, прежде чем мы перейдем

40
00:01:25,369 --> 00:01:27,259
к промежуточному сроку о  промежуточный

41
00:01:27,259 --> 00:01:30,880
материал, у кого-то есть вопросы, о,

42
00:01:30,880 --> 00:01:33,229
и просто дружеское напоминание, пожалуйста,

43
00:01:33,229 --> 00:01:34,880
называйте свое имя всякий раз, когда вы задаете

44
00:01:34,880 --> 00:01:36,500
вопрос, потому что это помогает мне помнить, а

45
00:01:36,500 --> 00:01:37,700
также помогает всем остальным узнать ваши

46
00:01:37,700 --> 00:01:41,150
имена, хорошо, так что где мы были

47
00:01:41,150 --> 00:01:42,619
, так это на последних двух лекциях

48
00:01:42,619 --> 00:01:44,240
мы начали говорить об

49
00:01:44,240 --> 00:01:46,520
обучении с подкреплением на основе политики, где мы

50
00:01:46,520 --> 00:01:48,110
специально пытаемся найти

51
00:01:48,110 --> 00:01:50,630
параметризованную политику, чтобы научиться принимать

52
00:01:50,630 --> 00:01:52,759
правильные решения в окружающей среде.  и так

53
00:01:52,759 --> 00:01:54,170
же, как мы видели с

54
00:01:54,170 --> 00:01:55,579
аппроксимацией функции ценности и тем, что вы

55
00:01:55,579 --> 00:01:57,560
делаете с atari для параметризации нашей политики,

56
00:01:57,560 --> 00:01:59,060
мы собираемся предположить,

57
00:01:59,060 --> 00:02:01,399
что есть некоторые векторные параметры, которые мы можем

58
00:02:01,399 --> 00:02:03,200
представлять для политик с помощью таких вещей, как

59
00:02:03,200 --> 00:02:06,460
softmax или глубокая нейронная сеть, а

60
00:02:06,460 --> 00:02:08,479
затем мы  вы хотите иметь возможность использовать

61
00:02:08,479 --> 00:02:10,489
градиенты этих типов политик

62
00:02:10,489 --> 00:02:12,620
, чтобы изучить политику, которая имеет большое

63
00:02:12,620 --> 00:02:16,549
значение, поэтому мы представили своего рода

64
00:02:16,549 --> 00:02:19,549
алгоритм градиента ванильной политики,

65
00:02:19,549 --> 00:02:20,690
где идея состоит в том, что вы начинаете

66
00:02:20,690 --> 00:02:23,060
инициализировать свою политику в некотором  образом, и у

67
00:02:23,060 --> 00:02:25,790
вас также есть некоторая базовая линия, а затем

68
00:02:25,790 --> 00:02:27,830
на разных итерациях вы запускаете

69
00:02:27,830 --> 00:02:30,440
свою текущую политику, и цель состоит в том, чтобы работать

70
00:02:30,440 --> 00:02:31,910
, выполняя эти выходы, чтобы мы

71
00:02:31,910 --> 00:02:33,710
могли оценить градиент, поэтому

72
00:02:33,710 --> 00:02:35,000
мы собираемся сделать эту часть, чтобы

73
00:02:35,000 --> 00:02:40,190
оценить градиент  нашей политики

74
00:02:40,190 --> 00:02:43,130
в настоящее время мы хотим получить своего рода тету DB D

75
00:02:43,130 --> 00:02:45,800
по отношению к нашей текущей политике,

76
00:02:45,800 --> 00:02:47,750
поэтому мы говорили о том, что вы

77
00:02:47,750 --> 00:02:49,880
исчерпали бы траектории из вашей

78
00:02:49,880 --> 00:02:51,650
текущей политики, поэтому используйте свой  политику для

79
00:02:51,650 --> 00:02:53,450
выполнения в среде, вы получите

80
00:02:53,450 --> 00:02:55,460
вознаграждение за действие в следующем состоянии,

81
00:02:55,460 --> 00:02:58,580
а также награды X и X, а затем вы посмотрите

82
00:02:58,580 --> 00:03:01,550
на прибыль и оценки преимуществ, которые

83
00:03:01,550 --> 00:03:04,220
сравните вашу прибыль с базовым уровнем, хорошо

84
00:03:04,220 --> 00:03:06,530
перенастройте свой базовый уровень, а затем вы можете

85
00:03:06,530 --> 00:03:09,260
обновить свою политику и так далее.  это был своего

86
00:03:09,260 --> 00:03:10,910
рода самый ванильный алгоритм градиента политики

87
00:03:10,910 --> 00:03:12,860
, о котором мы говорили, а затем мы

88
00:03:12,860 --> 00:03:14,030
начали говорить, что есть несколько

89
00:03:14,030 --> 00:03:16,340
различных вариантов, которые мы делаем в

90
00:03:16,340 --> 00:03:17,960
этом алгоритме, и почти все виды

91
00:03:17,960 --> 00:03:19,430
алгоритмов, основанных на градиенте политики, будут

92
00:03:19,430 --> 00:03:22,160
формироваться по формуле этого типа.

93
00:03:22,160 --> 00:03:25,790
поэтому, в частности, мы принимаем

94
00:03:25,790 --> 00:03:27,890
решение о своего рода оценке какой-

95
00:03:27,890 --> 00:03:31,670
то прибыли или целей, часто мы

96
00:03:31,670 --> 00:03:33,230
выбираем базовую линию, а затем нам нужно было

97
00:03:33,230 --> 00:03:35,450
принять какое-то решение о том, после того, как мы

98
00:03:35,450 --> 00:03:37,520
вычислим градиент, как далеко по

99
00:03:37,520 --> 00:03:40,400
градиенту мы пройдем, так что это вид

100
00:03:40,400 --> 00:03:45,380
помочь нам определить, как далеко мы

101
00:03:45,380 --> 00:03:57,530
продвинемся по нашему градиенту, поэтому в первой

102
00:03:57,530 --> 00:03:59,180
части мы говорили о том, как мы оцениваем

103
00:03:59,180 --> 00:04:01,130
своего рода значение того, где мы правы.

104
00:04:01,130 --> 00:04:03,410
теперь, когда мы собираемся использовать, чтобы

105
00:04:03,410 --> 00:04:05,780
попытаться оценить наш градиент, и мы говорили

106
00:04:05,780 --> 00:04:07,250
о том факте, что самое банальное

107
00:04:07,250 --> 00:04:08,870
, что мы могли бы сделать, это просто

108
00:04:08,870 --> 00:04:11,060
развернуть политику и посмотреть на результаты, и

109
00:04:11,060 --> 00:04:12,410
что это было действительно похоже на то, что

110
00:04:12,410 --> 00:04:17,060
мы  мы видели в оценках Монте-Карло, поэтому мы

111
00:04:17,060 --> 00:04:18,260
могли сделать, чтобы мы могли получить своего рода

112
00:04:18,260 --> 00:04:19,880
оценку функции ценности, просто

113
00:04:19,880 --> 00:04:22,100
развернув политику для одного эпизода,

114
00:04:22,100 --> 00:04:24,080
но это было точно так же, как то, что мы видели в

115
00:04:24,080 --> 00:04:27,080
Монте-Карло: беспристрастная оценка, но

116
00:04:27,080 --> 00:04:30,380
высокая дисперсия и  так что мы говорили о том, как

117
00:04:30,380 --> 00:04:32,330
мы могли бы играть во все виды, мы

118
00:04:32,330 --> 00:04:33,860
также используем те же инструменты, что и

119
00:04:33,860 --> 00:04:36,260
в прошлом, чтобы попытаться сбалансировать

120
00:04:36,260 --> 00:04:37,870
между предвзятостью и дисперсией,

121
00:04:37,870 --> 00:04:40,270
поэтому, в частности, мы говорили о том, как мы

122
00:04:40,270 --> 00:04:42,040
можем ввести предвзятость, используя начальную загрузку

123
00:04:42,040 --> 00:04:44,410
и  аппроксимация функции точно так же, как

124
00:04:44,410 --> 00:04:46,270
то, что мы видели в TDM, и точно так же, как

125
00:04:46,270 --> 00:04:47,710
то, о чем мы говорили с аппроксимацией функции ценности,

126
00:04:47,710 --> 00:04:50,410
поэтому мы могли неоднократно видеть

127
00:04:50,410 --> 00:04:52,150
те же самые идеи в том факте,

128
00:04:52,150 --> 00:04:54,100
что мы пытаемся понять, каково

129
00:04:54,100 --> 00:04:57,370
значение p  конкретной политики, и когда

130
00:04:57,370 --> 00:04:59,620
мы оцениваем это значение, мы можем найти

131
00:04:59,620 --> 00:05:01,090
компромисс между получением своего рода

132
00:05:01,090 --> 00:05:03,130
беспристрастных оценок того, насколько хорошими

133
00:05:03,130 --> 00:05:06,000
являются решения, и предвзятыми оценками,

134
00:05:06,000 --> 00:05:08,080
которые могут позволить нам быстрее распространять

135
00:05:08,080 --> 00:05:10,000
информацию и позволять нам

136
00:05:10,000 --> 00:05:13,300
учиться принимать лучшие решения быстрее, чем мы.

137
00:05:13,300 --> 00:05:14,320
также говорил о том факте, что

138
00:05:14,320 --> 00:05:15,750
существуют методы критики акторов, которые

139
00:05:15,750 --> 00:05:18,370
поддерживают явное параметризованное

140
00:05:18,370 --> 00:05:20,440
представление политики и

141
00:05:20,440 --> 00:05:22,150
параметризованное представление функции ценности,

142
00:05:22,150 --> 00:05:25,120
но

143
00:05:25,120 --> 00:05:26,919
в последний раз мы действительно начали вникать в то, что

144
00:05:26,919 --> 00:05:28,750
хорошо говорят оба, вы знаете, есть все  такого

145
00:05:28,750 --> 00:05:30,610
рода существующие методы, о которых мы знаем,

146
00:05:30,610 --> 00:05:32,470
чтобы попытаться оценить эти цели и

147
00:05:32,470 --> 00:05:34,780
оценить функцию ценности, но тогда

148
00:05:34,780 --> 00:05:36,760
возникает дополнительный вопрос о том, как

149
00:05:36,760 --> 00:05:40,930
далеко мы продвинемся по градиенту, поэтому, как

150
00:05:40,930 --> 00:05:43,000
только мы оценим градиент

151
00:05:43,000 --> 00:05:45,220
политики, нам нужно выяснить, как  далеко

152
00:05:45,220 --> 00:05:47,620
ли мы продвинулись по этому градиенту с точки зрения

153
00:05:47,620 --> 00:05:50,080
расчета новой политики, и причина, по которой мы

154
00:05:50,080 --> 00:05:51,700
утверждали, что это особенно важно

155
00:05:51,700 --> 00:05:53,500
Отличие обучения с подкреплением от обучения с

156
00:05:53,500 --> 00:05:55,180
учителем заключается в том, что какой бы

157
00:05:55,180 --> 00:05:57,700
шаг мы ни предприняли, какую бы новую политику мы ни рассматривали

158
00:05:57,700 --> 00:05:59,770
, она будет определять данные, которые мы получим

159
00:05:59,770 --> 00:06:02,020
следующими, и поэтому для нас было особенно

160
00:06:02,020 --> 00:06:03,460
важно подумать о том, как далеко

161
00:06:03,460 --> 00:06:05,289
мы хотим продвинуться в нашей стратегии.  градиент,

162
00:06:05,289 --> 00:06:09,490
чтобы получить новую политику, и одно желаемое

163
00:06:09,490 --> 00:06:11,770
свойство, о котором мы говорили, - это

164
00:06:11,770 --> 00:06:14,710
то, как мы можем обеспечить монотонное улучшение, поэтому

165
00:06:14,710 --> 00:06:16,030
мы хотели бы здесь, чтобы мы действительно хотели

166
00:06:16,030 --> 00:06:24,340
монотонного улучшения, это наша цель,

167
00:06:24,340 --> 00:06:27,639
и мы говорили о монотонном

168
00:06:27,639 --> 00:06:29,979
улучшении, которое не было гарантировано и

169
00:06:29,979 --> 00:06:33,010
dqn или множество других алгоритмов, потому что

170
00:06:33,010 --> 00:06:34,840
во многих доменах с высокими ставками, таких как

171
00:06:34,840 --> 00:06:38,229
финансовые клиенты, пациенты, вы

172
00:06:38,229 --> 00:06:39,550
действительно можете хотеть убедиться, что новая

173
00:06:39,550 --> 00:06:41,650
политика или развертывание, как ожидается, будут

174
00:06:41,650 --> 00:06:43,870
лучше, по крайней мере, ожидание, прежде чем

175
00:06:43,870 --> 00:06:47,140
вы ее развернете, поэтому мы говорили о желании

176
00:06:47,140 --> 00:06:49,030
этого  но есть большая проблема в том, что

177
00:06:49,030 --> 00:06:50,650
у нас нет данных о новых политиках,

178
00:06:50,650 --> 00:06:51,730
которые мы рассматриваем,

179
00:06:51,730 --> 00:06:53,260
и мы не хотим опробовать все

180
00:06:53,260 --> 00:06:55,750
возможные следующие политики, потому что  некоторые из

181
00:06:55,750 --> 00:06:57,700
них могут быть плохими, и поэтому мы хотим

182
00:06:57,700 --> 00:06:59,260
попытаться использовать наши существующие данные, чтобы выяснить,

183
00:06:59,260 --> 00:07:01,540
как нам сделать шаг и определить

184
00:07:01,540 --> 00:07:02,950
новую политику, которая, по нашему мнению, будет

185
00:07:02,950 --> 00:07:07,840
хорошей, поэтому, в частности, наша главная цель для

186
00:07:07,840 --> 00:07:11,200
градиентов политики  чтобы попытаться найти набор

187
00:07:11,200 --> 00:07:13,810
параметров политики, которые максимизируют нашу

188
00:07:13,810 --> 00:07:16,450
функцию ценности, и проблема заключается в том,

189
00:07:16,450 --> 00:07:18,580
что в настоящее время у нас есть доступ к данным, которые

190
00:07:18,580 --> 00:07:20,650
были собраны с помощью нашей текущей политики,

191
00:07:20,650 --> 00:07:24,390
которую мы собираемся назвать пи старая

192
00:07:26,700 --> 00:07:29,500
парадигма пытается по активу, если они делают

193
00:07:29,500 --> 00:07:33,700
это, мы  также может обозначать старые данные и

194
00:07:33,700 --> 00:07:35,890
во всех лекциях о градиенте политики.

195
00:07:35,890 --> 00:07:37,390
Я как бы ходил туда

196
00:07:37,390 --> 00:07:39,040
-сюда между разговорами о политиках

197
00:07:39,040 --> 00:07:41,020
и разговорами о бета-версиях, но хорошо

198
00:07:41,020 --> 00:07:42,700
просто помнить, что

199
00:07:42,700 --> 00:07:44,850
между PI и тета есть своего рода прямое сопоставление, которое, как

200
00:07:44,850 --> 00:07:47,380
вы знаете, существует.  четыре для каждой политики,

201
00:07:47,380 --> 00:07:50,050
она точно определяется набором

202
00:07:50,050 --> 00:07:55,360
параметров, поэтому, говорим ли

203
00:07:55,360 --> 00:07:57,580
мы о политиках или о

204
00:07:57,580 --> 00:07:59,470
параметрах, которые относятся к

205
00:07:59,470 --> 00:08:03,190
одному и тому же, поэтому проблемы

206
00:08:03,190 --> 00:08:04,630
i  что у нас есть данные из нашей текущей

207
00:08:04,630 --> 00:08:06,970
политики, которая имеет некоторый набор параметров,

208
00:08:06,970 --> 00:08:09,160
и мы хотим предсказать значение

209
00:08:09,160 --> 00:08:10,540
другой политики, и поэтому это

210
00:08:10,540 --> 00:08:15,880
проблема с обучением вне политики, поэтому в

211
00:08:15,880 --> 00:08:18,280
прошлый раз мы говорили о том,

212
00:08:18,280 --> 00:08:20,860
как мы выражаем  ценность политики

213
00:08:20,860 --> 00:08:23,680
с точки зрения вещей, которые мы знаем, и мы

214
00:08:23,680 --> 00:08:24,850
говорили о том, как мы могли бы записать ее

215
00:08:24,850 --> 00:08:26,470
с точки зрения преимущества перед

216
00:08:26,470 --> 00:08:29,650
текущей политикой, поэтому, если мы думаем о

217
00:08:29,650 --> 00:08:32,380
значении, параметризуемом новым набором

218
00:08:32,380 --> 00:08:35,020
новой политики с  новый набор параметров тета-тильды,

219
00:08:35,020 --> 00:08:37,590
он равен значению

220
00:08:37,590 --> 00:08:40,419
другой политики, параметризованной набором

221
00:08:40,419 --> 00:08:45,370
тета плюс ожидаемое преимущество, поэтому мы

222
00:08:45,370 --> 00:08:47,830
можем записать это как распределение

223
00:08:47,830 --> 00:08:49,510
состояний, которые мы ожидаем получить

224
00:08:49,510 --> 00:08:53,560
при новой политике, умноженное на преимущество, которое

225
00:08:53,560 --> 00:08:56,380
мы  попасть под старую политику, если

226
00:08:56,380 --> 00:08:59,730
мы будем следовать новой политике,

227
00:09:01,890 --> 00:09:03,450
и причина того, что мы пытаемся

228
00:09:03,450 --> 00:09:04,890
сделать в этом случае, просто чтобы заставить нас

229
00:09:04,890 --> 00:09:06,750
думать о том, какова главная цель

230
00:09:06,750 --> 00:09:08,190
здесь, это то, что мы пытаемся сделать, это

231
00:09:08,190 --> 00:09:10,019
найти способ  сделать градиент политики,

232
00:09:10,019 --> 00:09:11,700
когда мы гарантированно получим монотонное

233
00:09:11,700 --> 00:09:13,860
улучшение, или наша новая политика

234
00:09:13,860 --> 00:09:15,480
будет гарантированно лучше, чем наша

235
00:09:15,480 --> 00:09:17,430
старая политика, но мы хотим сделать это,

236
00:09:17,430 --> 00:09:19,019
фактически не испытывая нашу новую

237
00:09:19,019 --> 00:09:22,200
политику, поэтому мы пытаемся повторно выразить то, что

238
00:09:22,200 --> 00:09:24,450
значение новой политики с точки зрения

239
00:09:24,450 --> 00:09:27,240
количества, к которому у нас есть доступ, так что у

240
00:09:27,240 --> 00:09:28,500
нас есть доступ, у нас есть доступ к

241
00:09:28,500 --> 00:09:30,800
существующим образцам из текущей политики,

242
00:09:30,800 --> 00:09:33,510
и мы хотим использовать их и результаты, которые

243
00:09:33,510 --> 00:09:35,880
мы наблюдали, чтобы оценить

244
00:09:35,880 --> 00:09:38,220
значение новой политики, прежде чем мы ее развернем,

245
00:09:38,220 --> 00:09:40,380
это то, к чему мы пытаемся

246
00:09:40,380 --> 00:09:43,170
добраться, и мы заметили здесь, что, возможно,

247
00:09:43,170 --> 00:09:45,000
вы знаете, что у нас может быть доступ к

248
00:09:45,000 --> 00:09:47,040
явной форме новой политики,

249
00:09:47,040 --> 00:09:48,510
похожей на любые новые параметры, которые мы

250
00:09:48,510 --> 00:09:49,920
рассматривали для включения в нашу  нейронной

251
00:09:49,920 --> 00:09:52,950
сети, и мы могли бы представить себе оценку

252
00:09:52,950 --> 00:09:55,829
функции преимущества, но мы не знаем

253
00:09:55,829 --> 00:09:57,690
распределения состояний в соответствии с новой

254
00:09:57,690 --> 00:10:00,120
политикой, потому что это потребовало бы от нас ее

255
00:10:00,120 --> 00:10:04,920
фактического запуска, поэтому мы говорили

256
00:10:04,920 --> 00:10:06,630
о том, что давайте просто определим новую

257
00:10:06,630 --> 00:10:10,260
целевая функция, которая является просто

258
00:10:10,260 --> 00:10:11,640
другой целевой функцией, может быть

259
00:10:11,640 --> 00:10:13,170
хорошей может быть плохой, я тоже буду спорить

260
00:10:13,170 --> 00:10:14,910
, это хорошо, это хорошо, но сейчас

261
00:10:14,910 --> 00:10:16,910
это просто количество, которое мы можем оптимизировать,

262
00:10:16,910 --> 00:10:19,380
поэтому количество, которое мы можем оптимизировать

263
00:10:19,380 --> 00:10:21,480
здесь, мы собираемся  вызовите такую

264
00:10:21,480 --> 00:10:24,480
ую целевую функцию, и она бу 

265
00:10:24,480 --> 00:10:26,550
ет функцией предыдущего значения, по 

266
00:10:26,550 --> 00:10:29,420
тому помните, что это всегда равно пр 

267
00:10:29,420 --> 00:10:32,810
мому отображению между Thetas и PI, по 

268
00:10:32,810 --> 00:10:35,399
тому мы просто скажем, что это похоже на 

269
00:10:35,399 --> 00:10:37,110
елевую функцию, о которой мы только что го 

270
00:10:37,110 --> 00:10:38,760
орили.  что на самом деле было значением

271
00:10:38,760 --> 00:10:40,860
новой политики, но мы не знаем, что представляет собой это

272
00:10:40,860 --> 00:10:43,019
стационарное взвешенное распределение

273
00:10:43,019 --> 00:10:45,480
состояний в соответствии с новой политикой, поэтому мы

274
00:10:45,480 --> 00:10:47,940
просто заменим стационарное

275
00:10:47,940 --> 00:10:50,870
распределение в соответствии с текущей политикой

276
00:10:50,870 --> 00:10:53,310
сейчас, в общем, это не собирается  быть

277
00:10:53,310 --> 00:10:58,410
таким образом, это не будет равно вашему

278
00:10:58,410 --> 00:11:01,470
новому распределению политик, единственный раз,

279
00:11:01,470 --> 00:11:03,779
когда вы собираетесь получить одно и то же распределение состояний в

280
00:11:03,779 --> 00:11:05,730
рамках двух политик, как

281
00:11:05,730 --> 00:11:08,060
правило, если они идентичны,

282
00:11:08,060 --> 00:11:10,260
иногда вы можете получить t  одно и то же

283
00:11:10,260 --> 00:11:11,819
распределение состояния при двух разных

284
00:11:11,819 --> 00:11:13,290
политиках, но тогда это означает, что они

285
00:11:13,290 --> 00:11:15,300
имеют одинаковое значение, поэтому в целом

286
00:11:15,300 --> 00:11:15,690
мы ожидаем,

287
00:11:15,690 --> 00:11:17,670
но они будут разными, но

288
00:11:17,670 --> 00:11:18,660
мы будем игнорировать это сейчас,

289
00:11:18,660 --> 00:11:19,650
мы просто скажем, что это  целевая

290
00:11:19,650 --> 00:11:21,060
функция это то, что мы можем

291
00:11:21,060 --> 00:11:23,730
оптимизировать, и хорошая вещь в этом

292
00:11:23,730 --> 00:11:25,770
заключается в том, что у нас есть образцы из текущей

293
00:11:25,770 --> 00:11:28,620
политики, поэтому мы можем представить, что просто используем

294
00:11:28,620 --> 00:11:29,970
эти образцы для оценки этого

295
00:11:29,970 --> 00:11:33,990
ожидания. Я также хочу, чтобы

296
00:11:33,990 --> 00:11:36,090
мы отметили здесь, что это  просто

297
00:11:36,090 --> 00:11:37,830
эта новая целевая функция, называемая L, если

298
00:11:37,830 --> 00:11:40,560
вы оценили целевую функцию

299
00:11:40,560 --> 00:11:44,910
L при текущей политике, поэтому, если вы

300
00:11:44,910 --> 00:11:47,670
подключите свою старую политику к своей целевой

301
00:11:47,670 --> 00:11:49,470
функции, она точно равна

302
00:11:49,470 --> 00:11:52,890
значению вашей текущей политики, поэтому второй

303
00:11:52,890 --> 00:11:58,050
член становится равным нулю, потому что преимущество

304
00:11:58,050 --> 00:12:00,330
существующей политики над существующей

305
00:12:00,330 --> 00:12:04,080
политикой равна нулю, поэтому эта целевая

306
00:12:04,080 --> 00:12:05,670
функция точно равна

307
00:12:05,670 --> 00:12:08,250
значению старой политики, если вы оцениваете ее

308
00:12:08,250 --> 00:12:11,100
старой политикой, а в других случаях для  новая

309
00:12:11,100 --> 00:12:12,240
политика может быть чем-то другим,

310
00:12:12,240 --> 00:12:16,380
угадайте, как это похоже на важную

311
00:12:16,380 --> 00:12:19,590
точку зрения, которую вас спрашивают, как это

312
00:12:19,590 --> 00:12:21,750
похоже на важную выборку, если мы

313
00:12:21,750 --> 00:12:24,120
собираемся преуспеть в важном, это

314
00:12:24,120 --> 00:12:26,250
отличается во многих отношениях, я

315
00:12:26,250 --> 00:12:28,110
важная выборка, к чему мы склонны  Делаем ли

316
00:12:28,110 --> 00:12:31,770
мы взвешивание распределения, которое мы

317
00:12:31,770 --> 00:12:34,830
хотим, по распределению, которое мы имеем в

318
00:12:34,830 --> 00:12:36,480
данном случае, которое мы ищем, и мы обычно

319
00:12:36,480 --> 00:12:39,120
делаем это на уровне штата. В этом

320
00:12:39,120 --> 00:12:41,190
случае мы смотрим на стационарное

321
00:12:41,190 --> 00:12:43,470
распределение по штатам.

322
00:12:43,470 --> 00:12:45,090
крутая статья, которая только что

323
00:12:45,090 --> 00:12:51,620
вышла на нервах, как месяц назад 2018

324
00:12:51,620 --> 00:12:53,820
остроумно Хонг Ли и некоторые другие

325
00:12:53,820 --> 00:12:57,180
коллеги, которые рассмотрели, как бы вы

326
00:12:57,180 --> 00:12:59,550
перевзвесили стационарные распределения, чтобы

327
00:12:59,550 --> 00:13:01,440
попытаться получить политические оценки

328
00:13:01,440 --> 00:13:04,740
функции ценности и, таким образом, попытаться напрямую

329
00:13:04,740 --> 00:13:06,750
перевзвесить то, что нравится Мью  PI будет

330
00:13:06,750 --> 00:13:09,900
сравниваться с тильдой Mew PI, поэтому мы не делаем

331
00:13:09,900 --> 00:13:11,820
этого здесь, есть несколько действительно хороших идей

332
00:13:11,820 --> 00:13:13,290
, которые могут действительно помочь

333
00:13:13,290 --> 00:13:14,970
уменьшить дисперсию и проблемы с длинным горизонтом

334
00:13:14,970 --> 00:13:17,040
в этом случае мы  мы просто заменяем, поэтому

335
00:13:17,040 --> 00:13:18,360
мы игнорируем разницу мы не

336
00:13:18,360 --> 00:13:19,680
делаем важную выборку мы просто

337
00:13:19,680 --> 00:13:21,720
притворяемся, что распределение

338
00:13:21,720 --> 00:13:23,370
состояний, которое мы получаем, точно такое

339
00:13:23,370 --> 00:13:26,730
же это не так, но мы собираемся показать, что

340
00:13:26,730 --> 00:13:27,870
это закончится  быть полезным

341
00:13:27,870 --> 00:13:29,100
ниже о

342
00:13:29,100 --> 00:13:30,780
том, что мы хотим, что мы на самом деле хотим

343
00:13:30,780 --> 00:13:35,160
оптимизировать, хорошо,

344
00:13:35,160 --> 00:13:37,830
поэтому вы можете сказать, что если вы возьмете эту

345
00:13:37,830 --> 00:13:39,510
целевую функцию, которая может быть хорошей,

346
00:13:39,510 --> 00:13:41,550
может быть плохой, если мы оптимизируем

347
00:13:41,550 --> 00:13:43,350
относительно нее, есть ли у нас какие-либо гарантии

348
00:13:43,350 --> 00:13:45,450
того, будет ли  новая функция ценности, которую

349
00:13:45,450 --> 00:13:47,010
мы получаем, если мы оптимизируем по отношению к

350
00:13:47,010 --> 00:13:49,290
этой неправильной целевой функции, лучше,

351
00:13:49,290 --> 00:13:51,060
чем старая функция ценности, потому что

352
00:13:51,060 --> 00:13:52,080
помните, что это то, к

353
00:13:52,080 --> 00:13:53,340
чему мы стремимся - нас действительно не волнует, что мы

354
00:13:53,340 --> 00:13:55,170
оптимизируем, что нас волнует

355
00:13:55,170 --> 00:13:56,970
что результирующая функция значения, которую мы получаем,

356
00:13:56,970 --> 00:13:58,890
на самом деле лучше, чем старая функция значения,

357
00:13:58,890 --> 00:14:02,310
поэтому в прошлый раз я сказал, что если у вас

358
00:14:02,310 --> 00:14:03,990
есть смешанная политика, которая

359
00:14:03,990 --> 00:14:05,760
смешивает вашу текущую политику и новую

360
00:14:05,760 --> 00:14:08,460
политику, скажем, у вас есть PI старый

361
00:14:08,460 --> 00:14:10,170
и йо  у вас есть какая-то другая политика,

362
00:14:10,170 --> 00:14:13,170
я не сказал, как вы ее получаете, но просто

363
00:14:13,170 --> 00:14:14,970
скажите, что у вас есть какая-то другая политика, и она

364
00:14:14,970 --> 00:14:16,440
определяет вашу новую политику, поэтому с

365
00:14:16,440 --> 00:14:18,330
вероятностью 1 минус альфа вы предпринимаете то

366
00:14:18,330 --> 00:14:19,680
же действие, что и раньше, с

367
00:14:19,680 --> 00:14:21,180
вероятностью альфа вы принимаете новую  действие,

368
00:14:21,180 --> 00:14:24,060
в этом случае вы можете гарантировать нижнюю

369
00:14:24,060 --> 00:14:28,410
границу значения новой политики,

370
00:14:28,410 --> 00:14:30,210
чтобы значение новой политики было

371
00:14:30,210 --> 00:14:31,740
больше или равно этой целевой функции, которую

372
00:14:31,740 --> 00:14:37,460
мы имеем здесь - это конкретное количество,

373
00:14:37,460 --> 00:14:40,290
так что это говорит о том, что если вы оптимизируете по

374
00:14:40,290 --> 00:14:42,900
отношению к  эта странная целевая функция L,

375
00:14:42,900 --> 00:14:44,880
вы на самом деле не можете получить границы

376
00:14:44,880 --> 00:14:47,850
того, насколько хороша ваша новая политика, так что это

377
00:14:47,850 --> 00:14:48,600
кажется многообещающим,

378
00:14:48,600 --> 00:14:49,860
но в целом мы не

379
00:14:49,860 --> 00:14:52,520
собираемся просто рассматривать смешанные политики,

380
00:14:52,520 --> 00:14:56,250
так что эта теорема говорит, что для

381
00:14:56,250 --> 00:14:59,490
любой стохастической политики  не только эта

382
00:14:59,490 --> 00:15:02,010
странная смесь, вы можете получить ограничение на

383
00:15:02,010 --> 00:15:04,050
производительность, используя эту немного

384
00:15:04,050 --> 00:15:06,720
странную целевую функцию, поэтому, в

385
00:15:06,720 --> 00:15:09,060
частности, определите расстояние общей

386
00:15:09,060 --> 00:15:12,330
вариации следующим образом, поэтому D TV между двумя

387
00:15:12,330 --> 00:15:16,230
политиками, так что я нас  Ставим точку там, чтобы

388
00:15:16,230 --> 00:15:19,200
обозначить, что существует ряд

389
00:15:19,200 --> 00:15:22,890
различных действий, которые политики

390
00:15:22,890 --> 00:15:24,780
обозначают распределение вероятности по

391
00:15:24,780 --> 00:15:27,210
действиям это равно максимальному из

392
00:15:27,210 --> 00:15:32,190
всех a расстояние между

393
00:15:32,190 --> 00:15:33,480
вероятностью того, что каждая из двух

394
00:15:33,480 --> 00:15:38,160
политик наложила на это действие, так что это

395
00:15:38,160 --> 00:15:38,680
давая

396
00:15:38,680 --> 00:15:40,660
нам максимальную разницу в

397
00:15:40,660 --> 00:15:42,220
вероятности действия в соответствии с одной

398
00:15:42,220 --> 00:15:45,370
политикой по сравнению с другой политикой, и тогда

399
00:15:45,370 --> 00:15:48,190
мы можем сделать, давайте посмотрим, что мы можем сделать D max

400
00:15:48,190 --> 00:15:50,710
общей вариации, взяв максимум

401
00:15:50,710 --> 00:15:53,230
этой величины по всем состояниям, так что это, по

402
00:15:53,230 --> 00:15:55,149
сути, говорит о  все указывает, в

403
00:15:55,149 --> 00:15:56,649
чем самая большая разница, которую

404
00:15:56,649 --> 00:15:58,360
две политики дают для конкретного

405
00:15:58,360 --> 00:16:02,050
действия, так в чем же они больше всего различаются, и

406
00:16:02,050 --> 00:16:03,850
тогда эта теорема говорит, что если у

407
00:16:03,850 --> 00:16:06,310
вас есть это количество в целом, мы

408
00:16:06,310 --> 00:16:07,870
не сможем это оценить, но

409
00:16:07,870 --> 00:16:09,850
что это говорит  заключается в том, что если вы знаете,

410
00:16:09,850 --> 00:16:11,529
что это за количество, то вы можете

411
00:16:11,529 --> 00:16:14,170
определить, что если вы используете эту целевую

412
00:16:14,170 --> 00:16:17,500
функцию L, то новое значение вашей

413
00:16:17,500 --> 00:16:19,870
политики будет по крайней мере целевой

414
00:16:19,870 --> 00:16:21,790
функцией  n вы вычисляете минус это количество

415
00:16:21,790 --> 00:16:24,490
, это функция расстояния полной

416
00:16:24,490 --> 00:16:26,560
вариации максимальное расстояние полной

417
00:16:26,560 --> 00:16:29,410
вариации, так что это дает нам некоторую

418
00:16:29,410 --> 00:16:31,209
уверенность в том, что если бы мы оптимизировали

419
00:16:31,209 --> 00:16:32,950
по отношению к целевой функции L,

420
00:16:32,950 --> 00:16:35,410
то мы могли бы получить оценку функции значения

421
00:16:35,410 --> 00:16:40,149
теперь это расстояние, это максимальное

422
00:16:40,149 --> 00:16:41,290
или общее расстояние вариации, не

423
00:16:41,290 --> 00:16:44,770
особенно легко работать, поэтому мы можем

424
00:16:44,770 --> 00:16:46,510
использовать тот факт, что его квадрат

425
00:16:46,510 --> 00:16:48,279
ограничен сверху дивергенцией KL, а

426
00:16:48,279 --> 00:16:50,470
затем получить новую границу, с которой

427
00:16:50,470 --> 00:16:52,900
немного легче работать.  при этом смотрит

428
00:16:52,900 --> 00:16:55,060
на расхождение KL между двумя

429
00:16:55,060 --> 00:16:57,160
политиками, и мы снова получаем эту аналогичную

430
00:16:57,160 --> 00:17:02,200
границу, хорошо, так почему это полезно, так что

431
00:17:02,200 --> 00:17:03,310
я сказал вам прямо сейчас, что у нас есть

432
00:17:03,310 --> 00:17:04,900
эта новая целевая функция, если мы используем

433
00:17:04,900 --> 00:17:07,119
эту новую целевую функцию, мы могли бы  в

434
00:17:07,119 --> 00:17:09,309
принципе получить эту нижнюю границу

435
00:17:09,309 --> 00:17:11,650
производительности новой политики, так как

436
00:17:11,650 --> 00:17:13,240
мы можем использовать это, чтобы гарантировать, что мы хотим

437
00:17:13,240 --> 00:17:18,400
получить монотонное улучшение, поэтому цель -

438
00:17:18,400 --> 00:17:20,410
монотонное улучшение, мы хотим

439
00:17:20,410 --> 00:17:23,650
иметь V of Pi I плюс  1 больше или

440
00:17:23,650 --> 00:17:27,819
равно V из PI I в качестве нашей цели, поэтому у меня есть

441
00:17:27,819 --> 00:17:29,350
итерации, которые мы хотим, чтобы новая политика,

442
00:17:29,350 --> 00:17:30,970
которую мы развертываем, была на самом деле лучше,

443
00:17:30,970 --> 00:17:33,250
чем политика, которая у нас была раньше, поэтому как мы

444
00:17:33,250 --> 00:17:35,050
собираемся это сделать, и что мы  хочу сказать,

445
00:17:35,050 --> 00:17:36,910
что сначала у нас есть вот эта целевая функция,

446
00:17:36,910 --> 00:17:40,230
эта нижняя граница целевой функции,

447
00:17:40,230 --> 00:17:42,910
и мы собираемся определить, что

448
00:17:42,910 --> 00:17:49,809
mi числа Pi I равно L числа Pi I

449
00:17:49,809 --> 00:17:54,129
hi, поэтому я просто копирую уравнение

450
00:17:54,129 --> 00:17:59,259
с предыдущего предыдущего слайда.  - для

451
00:17:59,259 --> 00:18:02,139
эпсилон гаммы разделить на 1 минус гамма в

452
00:18:02,139 --> 00:18:11,559
квадрате DK l max числа pi I, так что это

453
00:18:11,559 --> 00:18:12,999
нижняя граница, которую мы только что определили

454
00:18:12,999 --> 00:18:17,229
на предыдущем слайде, поэтому здесь мы сказали,

455
00:18:17,229 --> 00:18:19,809
что значение нашей новой политики,

456
00:18:19,809 --> 00:18:22,389
так что это равно этому  Функция M, которую я

457
00:18:22,389 --> 00:18:25,059
определил, поэтому мы сказали, что новое

458
00:18:25,059 --> 00:18:27,249
значение будет по крайней мере таким же хорошим, как эта

459
00:18:27,249 --> 00:18:30,820
нижняя граница, поэтому мы скажем, что V of I

460
00:18:30,820 --> 00:18:36,399
плюс 1 будет равно M I of

461
00:18:36,399 --> 00:18:41,279
Pi I плюс 1  что равно L PI.

462
00:18:41,279 --> 00:18:44,229
Я просто записываю

463
00:18:44,229 --> 00:18:51,460
определение здесь и снова, что мы

464
00:18:51,460 --> 00:18:52,960
пытаемся здесь сделать.  мы подошли к моменту,

465
00:18:52,960 --> 00:18:54,669
когда мы уверены, что можем получить

466
00:18:54,669 --> 00:18:55,929
что-то лучше, чем наша старая

467
00:18:55,929 --> 00:18:59,649
функция значения, теперь я

468
00:18:59,649 --> 00:19:02,139
хочу рассмотреть то, что будет, если

469
00:19:02,139 --> 00:19:03,700
мы оценим нижнюю границу при

470
00:19:03,700 --> 00:19:06,129
текущей политике, что будет  это так,

471
00:19:06,129 --> 00:19:09,729
давайте посмотрим на mi от PI I,

472
00:19:09,729 --> 00:19:12,729
чтобы оно было равно L PI I от Pi I. Я просто

473
00:19:12,729 --> 00:19:14,460
подставляю его в это уравнение наверху

474
00:19:14,460 --> 00:19:19,229
- для эпсилон гамма минус гамма в квадрате

475
00:19:19,229 --> 00:19:28,330
DK L max от hi I pi так почему  это

476
00:19:28,330 --> 00:19:29,889
хорошо, это хорошо, потому что

477
00:19:29,889 --> 00:19:31,629
расхождение KL между двумя идентичными

478
00:19:31,629 --> 00:19:34,629
политиками равно 0, потому что они

479
00:19:34,629 --> 00:19:38,169
точно такие же, это равно 0, так что теперь

480
00:19:38,169 --> 00:19:42,639
это просто равно L PI I для Pi I, но

481
00:19:42,639 --> 00:19:44,559
я сказал вам раньше, что если  мы возвращаемся

482
00:19:44,559 --> 00:19:46,479
на несколько слайдов назад к

483
00:19:46,479 --> 00:19:50,109
определению Pi I L of Pi I заключается в том, что если вы

484
00:19:50,109 --> 00:19:53,969
оцениваете его при текущей политике, он

485
00:19:56,179 --> 00:19:58,539
просто равен значению этой политики,

486
00:19:58,539 --> 00:20:01,340
поэтому, если мы оцениваем эту целевую

487
00:20:01,340 --> 00:20:03,470
функцию при текущей политике, это

488
00:20:03,470 --> 00:20:04,759
то же самое  как значение текущей

489
00:20:04,759 --> 00:20:08,690
политики, так что теперь, если мы вернемся сюда, это

490
00:20:08,690 --> 00:20:13,190
просто равнозначно тому, чтобы быть PI Я хорошо, так что

491
00:20:13,190 --> 00:20:15,169
это говорит, это говорит, что если я

492
00:20:15,169 --> 00:20:18,529
хочу посмотреть, как мое значение

493
00:20:18,529 --> 00:20:21,529
моей политики I Plus One выглядит по сравнению

494
00:20:21,529 --> 00:20:23,779
со значением моей старой политики, мы знаем,

495
00:20:23,779 --> 00:20:26,299
что это больше, чем или  равно M I от

496
00:20:26,299 --> 00:20:31,580
Pi I плюс один, так как мы сказали,

497
00:20:31,580 --> 00:20:33,980
что через мы знали из этой теоремы,

498
00:20:33,980 --> 00:20:35,419
что новое значение политики

499
00:20:35,419 --> 00:20:36,799
больше или равно этой нижней границе, которую мы

500
00:20:36,799 --> 00:20:39,049
вычислили, поэтому оно больше, чем равно M

501
00:20:39,049 --> 00:20:46,159
I pious плюс 1  минус M I из PI I, так что

502
00:20:46,159 --> 00:20:48,740
это говорит о том, что если ваша новая

503
00:20:48,740 --> 00:20:50,659
функция значения имеет лучшую нижнюю границу,

504
00:20:50,659 --> 00:20:52,309
чем ваша старая функция значения, у вас есть

505
00:20:52,309 --> 00:20:55,399
монотонное улучшение, поэтому, если оно

506
00:20:55,399 --> 00:20:58,100
больше нуля, тогда монотонное

507
00:20:58,100 --> 00:21:03,649
улучшение, что означает, что если вы

508
00:21:03,649 --> 00:21:05,480
оптимизируете с уважением  к этой нижней

509
00:21:05,480 --> 00:21:08,179
границе, и вы можете оценить это количество,

510
00:21:08,179 --> 00:21:10,009
и ваша новая нижняя граница выше, чем

511
00:21:10,009 --> 00:21:13,669
ваша старая нижняя граница, тогда ваше значение

512
00:21:13,669 --> 00:21:15,679
должно быть лучше, чтобы мы могли гарантировать

513
00:21:15,679 --> 00:21:19,639
монотонное улучшение да, так что просто чтобы

514
00:21:19,639 --> 00:21:21,649
уточнить, так что для этих сравнений

515
00:21:21,649 --> 00:21:23,330
значений мы неявно  y рассматривая как

516
00:21:23,330 --> 00:21:25,549
норму бесконечности с точки зрения того, чтобы сказать, что один

517
00:21:25,549 --> 00:21:28,850
лучше да, в целом, да, я думаю, я имею в виду, что

518
00:21:28,850 --> 00:21:30,080
они, вероятно, тоже проходят через l в квадрате,

519
00:21:30,080 --> 00:21:34,129
но да, вопрос в том

520
00:21:34,129 --> 00:21:35,629
, всегда ли мы определяем это

521
00:21:35,629 --> 00:21:38,539
относительно нормы бесконечности L почти всегда I

522
00:21:38,539 --> 00:21:40,070
конечно, есть некоторый анализ, особенно

523
00:21:40,070 --> 00:21:41,149
когда мы входим в аппроксимацию функции,

524
00:21:41,149 --> 00:21:44,330
которая рассматривает норму l2, но большая часть всего

525
00:21:44,330 --> 00:21:45,590
этого относится к норме бесконечности L,

526
00:21:45,590 --> 00:21:48,080
что означает, что когда

527
00:21:48,080 --> 00:21:49,700
мы смотрим на это, например, мы

528
00:21:49,700 --> 00:21:52,119
смотрим на обеспечение того, чтобы  для всех

529
00:21:52,119 --> 00:21:55,100
состояний значение этих состояний, по крайней мере, так же

530
00:21:55,100 --> 00:21:56,330
хорошо, как и предыдущее значение

531
00:21:56,330 --> 00:22:01,490
состояния «да», поэтому утверждение заключалось в том, что если наша

532
00:22:01,490 --> 00:22:03,169
нижняя граница улучшится, то должно быть

533
00:22:03,169 --> 00:22:04,909
так, что то, что является нижней границей,

534
00:22:04,909 --> 00:22:07,789
давайте также улучшим правильно так

535
00:22:07,789 --> 00:22:08,960
никогда не бывает случая, когда, например,

536
00:22:08,960 --> 00:22:09,860
вы снижаете мое

537
00:22:09,860 --> 00:22:11,510
доказано, даже если фактическая

538
00:22:11,510 --> 00:22:14,840
стоимость оцениваемой политики уменьшается, кажется,

539
00:22:14,840 --> 00:22:18,500
что это правильно, поэтому, когда это то, о чем

540
00:22:18,500 --> 00:22:19,970
спрашивается, так что знаете ли вы

541
00:22:19,970 --> 00:22:21,380
это?  r и что это относится

542
00:22:21,380 --> 00:22:23,120
к фактическому значению, что это за стадион, так

543
00:22:23,120 --> 00:22:25,610
это то, что если вы улучшите свой tular, например,

544
00:22:25,610 --> 00:22:26,750
если у вас есть нижняя граница вашей

545
00:22:26,750 --> 00:22:28,490
существующей политики, и вы получите новую нижнюю

546
00:22:28,490 --> 00:22:30,470
границу с помощью какой-то новой политики, и

547
00:22:30,470 --> 00:22:32,390
эта новая нижняя граница  выше, чем ваша

548
00:22:32,390 --> 00:22:33,559
нижняя граница, до той, которую вы

549
00:22:33,559 --> 00:22:37,100
гарантированно улучшите, так что это

550
00:22:37,100 --> 00:22:38,960
то, что гарантирует, так что это предполагает, что вы

551
00:22:38,960 --> 00:22:42,919
можете решить эту проблему, но если бы вы могли

552
00:22:42,919 --> 00:22:44,840
получить это ниже, если вы оптимизируете по

553
00:22:44,840 --> 00:22:47,260
отношению к этому количеству нижней границы,

554
00:22:47,260 --> 00:22:49,700
потому что, когда вы  подставьте нижнюю границу

555
00:22:49,700 --> 00:22:51,919
для текущей политики под тем, что

556
00:22:51,919 --> 00:22:53,540
точно равно значению этой

557
00:22:53,540 --> 00:22:55,190
политики, тогда вы гарантированно

558
00:22:55,190 --> 00:22:57,320
улучшитесь, потому что вы в основном

559
00:22:57,320 --> 00:22:58,760
говорите, что вот моя нижняя вот моя

560
00:22:58,760 --> 00:23:00,710
существующая ценность У меня есть что-то, чья

561
00:23:00,710 --> 00:23:02,210
нижняя граница лучше, чем  моя существующая

562
00:23:02,210 --> 00:23:04,190
ценность, и поэтому я знаю, что моя новая вещь должна

563
00:23:04,190 --> 00:23:18,770
быть лучше изменена в зависимости от великого

564
00:23:18,770 --> 00:23:22,490
, я мог бы спросить нас об этом, поэтому обратите внимание,

565
00:23:22,490 --> 00:23:24,260
что ваша нижняя граница выражена в

566
00:23:24,260 --> 00:23:26,150
терминах эпсилон эпсилон - это максимум  Вернее, все

567
00:23:26,150 --> 00:23:31,070
состояния и действия, приносящие вам преимущество, в

568
00:23:31,070 --> 00:23:33,010
принципе, вы могли бы оценить это,

569
00:23:33,010 --> 00:23:34,940
если бы у вас было дискретное состояние в

570
00:23:34,940 --> 00:23:37,010
пространстве действия, на практике это

571
00:23:37,010 --> 00:23:38,679
то, чего вы бы не хотели делать,

572
00:23:38,679 --> 00:23:39,950


573
00:23:39,950 --> 00:23:42,620
я считаю, что эта часть как бы говорит, что

574
00:23:42,620 --> 00:23:44,059
это формально, если бы вы могли  оцените этот

575
00:23:44,059 --> 00:23:45,440
laravel, и что мы собираемся сделать сейчас, так это

576
00:23:45,440 --> 00:23:47,690
поговорить о более практичном алгоритме,

577
00:23:47,690 --> 00:23:51,470
который пытается взять эту гарантию

578
00:23:51,470 --> 00:23:52,910
консервативного улучшения политики и

579
00:23:52,910 --> 00:23:54,559
фактически сделать его практичным с точки зрения

580
00:23:54,559 --> 00:23:55,880
величин, которые немного

581
00:23:55,880 --> 00:23:59,120
легче вычислить, потому что это правильно да в

582
00:23:59,120 --> 00:24:00,530
в общем, было бы очень сложно

583
00:24:00,530 --> 00:24:02,419
оценить это как этот эпсилон, который вы

584
00:24:02,419 --> 00:24:04,179
могли бы взять для него верхние или нижние границы,

585
00:24:04,179 --> 00:24:06,500
но вы обычно не будете знать, что это за

586
00:24:06,500 --> 00:24:08,809
эпсилон, и обратите внимание, что это просто

587
00:24:08,809 --> 00:24:10,400
указание на этот эпсилон зависит

588
00:24:10,400 --> 00:24:16,340
от политики, поэтому все  правильно, но это

589
00:24:16,340 --> 00:24:17,600
довольно круто, так что это означает, что вы можете сделать

590
00:24:17,600 --> 00:24:18,980
это гарантированное улучшение, это

591
00:24:18,980 --> 00:24:20,360
форма максимизации минеральной минерализации

592
00:24:20,360 --> 00:24:23,110
, это хорошая идея, которую

593
00:24:23,110 --> 00:24:24,429
они говорят вам  может быть, эта новая

594
00:24:24,429 --> 00:24:26,260
нижняя граница гарантированно будет лучше,

595
00:24:26,260 --> 00:24:28,000
чем значение вашей текущей политики, поэтому

596
00:24:28,000 --> 00:24:29,049
вы можете получить этот вид консервативной

597
00:24:29,049 --> 00:24:34,960
монотонной улучшающей политики, все в порядке, поэтому

598
00:24:34,960 --> 00:24:36,250
я просто хочу убедиться, что у нас есть

599
00:24:36,250 --> 00:24:37,269
достаточно времени, чтобы пройти некоторые из

600
00:24:37,269 --> 00:24:38,710
среднесрочных  обзор, но я хочу кратко

601
00:24:38,710 --> 00:24:40,570
рассказать о том, как мы могли бы сделать это

602
00:24:40,570 --> 00:24:43,360
практичным, в частности потому, что

603
00:24:43,360 --> 00:24:45,250
оптимизация политик доверенных регионов является

604
00:24:45,250 --> 00:24:47,440
чрезвычайно популярным алгоритмом градиента политик

605
00:24:47,440 --> 00:24:48,909
, поэтому я думаю, что вам,

606
00:24:48,909 --> 00:24:51,490
ребята, будет полезно просто знать, что некоторые из вас

607
00:24:51,490 --> 00:24:53,740
могут использовать его в некоторых из  ваши проекты, мы

608
00:24:53,740 --> 00:24:55,539
не будем освещать это, и я не буду

609
00:24:55,539 --> 00:24:57,760
обязательной частью домашнего задания или на промежуточном этапе,

610
00:24:57,760 --> 00:24:59,110
но я думаю, что это просто полезная

611
00:24:59,110 --> 00:25:02,529
идея, чтобы еще раз ознакомиться с ней, если мы

612
00:25:02,529 --> 00:25:04,299
посмотрим, что это была за целевая

613
00:25:04,299 --> 00:25:05,799
функция.  мы только что обсудили, мы

614
00:25:05,799 --> 00:25:07,899
сказали, что у нас есть эта функция ell, а затем

615
00:25:07,899 --> 00:25:09,490
мы превратили ее в нижнюю границу,

616
00:25:09,490 --> 00:25:12,159
вычитая эту константу, которую нам может

617
00:25:12,159 --> 00:25:14,260
быть трудно вычислить, и поэтому

618
00:25:14,260 --> 00:25:16,059
в этом случае мы берем эту константу

619
00:25:16,059 --> 00:25:18,909
он  re, и мы превращаем его в гиперпараметр,

620
00:25:18,909 --> 00:25:21,909
чтобы вы могли превратить его в

621
00:25:21,909 --> 00:25:24,789
константу C, но проблема всегда

622
00:25:24,789 --> 00:25:26,409
в том, что даже если бы вы могли вычислить это

623
00:25:26,409 --> 00:25:28,240
часто, мы не знаем, что это такое, но

624
00:25:28,240 --> 00:25:29,710
даже если бы вы могли это вычислить или

625
00:25:29,710 --> 00:25:33,279
привязать к компьютеру  как правило, если мы используем это, мы

626
00:25:33,279 --> 00:25:36,610
будем делать очень маленькие шаги, поэтому

627
00:25:36,610 --> 00:25:38,880
интуитивно понятно, что это потому, что вы знаете,

628
00:25:38,880 --> 00:25:41,740
что часто очень сложно экстраполировать

629
00:25:41,740 --> 00:25:44,740
далеко от вашей текущей политики, и поэтому

630
00:25:44,740 --> 00:25:46,120
это будет означать, что вы хотите быть действительно

631
00:25:46,120 --> 00:25:48,070
уверены, что ваше новое значение  лучше, чем

632
00:25:48,070 --> 00:25:49,600
ваше старое значение, тогда просто сделайте очень

633
00:25:49,600 --> 00:25:52,779
маленький размер шага и интуитивно понятно,

634
00:25:52,779 --> 00:25:54,399
потому что, если вы измените свою политику на очень

635
00:25:54,399 --> 00:25:55,960
очень небольшие суммы, по крайней мере, с некоторой

636
00:25:55,960 --> 00:25:58,059
плавностью, гарантируется, что стоимость вашей

637
00:25:58,059 --> 00:26:01,269
политики не может измениться так сильно, она

638
00:26:01,269 --> 00:26:02,740
также должна быть интуитивно понятной, например  ваш

639
00:26:02,740 --> 00:26:04,419
градиент часто является довольно хорошей оценкой,

640
00:26:04,419 --> 00:26:06,190
очень близкой к вашему текущему значению вашей

641
00:26:06,190 --> 00:26:09,789
функции, но нам также нужно быстро

642
00:26:09,789 --> 00:26:11,260
попытаться добраться до хорошей политики, так что это

643
00:26:11,260 --> 00:26:14,230
обычно непрактично, и поэтому идея

644
00:26:14,230 --> 00:26:16,779
вроде trp oh  одна из основных идей состоит в том,

645
00:26:16,779 --> 00:26:17,740
чтобы думать о том, что существует своего рода

646
00:26:17,740 --> 00:26:21,519
доверенная область, и использовать ее для ограничения

647
00:26:21,519 --> 00:26:23,769
размеров наших шагов, поэтому снова, если мы вернемся к

648
00:26:23,769 --> 00:26:27,340
сортировке общего шаблона для

649
00:26:27,340 --> 00:26:28,659
алгоритмов градиента политики, мы должны сделать этот

650
00:26:28,659 --> 00:26:30,580
выбор того, насколько далеко  чтобы вступить в наш

651
00:26:30,580 --> 00:26:32,860
градиент, и идея состоит в том, что мы собираемся

652
00:26:32,860 --> 00:26:35,529
определить ограничение, поэтому у нас будет

653
00:26:35,529 --> 00:26:36,169
наша целевая

654
00:26:36,169 --> 00:26:38,179
функция здесь, и вместо явного

655
00:26:38,179 --> 00:26:40,429
вычитания из нашей нижней границы мы

656
00:26:40,429 --> 00:26:42,649
просто скажем, что вы можете двигаться, вы можете

657
00:26:42,649 --> 00:26:44,809
изменить свой градиент  но не слишком далеко, я

658
00:26:44,809 --> 00:26:46,970
собираюсь наложить ограничение на то, насколько далеко

659
00:26:46,970 --> 00:26:49,669
может быть расхождение KL, чтобы как

660
00:26:49,669 --> 00:26:51,559
бы сказать, что у вас есть эта область

661
00:26:51,559 --> 00:26:53,419
чего, и вы находитесь в своем пространстве параметров,

662
00:26:53,419 --> 00:26:54,950
которое позволяет вам знать, как  далеко вы можете

663
00:26:54,950 --> 00:27:00,710
изменить свою политику, хорошо, поэтому я просто

664
00:27:00,710 --> 00:27:02,480
очень кратко расскажу о том, как это

665
00:27:02,480 --> 00:27:06,440
реализуется, поэтому основная идея заключается в том, что

666
00:27:06,440 --> 00:27:08,869
если мы посмотрим, что представляют собой эти целевые

667
00:27:08,869 --> 00:27:11,419
функции, это может быть легко или не так

668
00:27:11,419 --> 00:27:13,820
легко для нас оценить, поэтому, если мы

669
00:27:13,820 --> 00:27:17,869
оглянуться на то, что теты даже здесь, прямо мы

670
00:27:17,869 --> 00:27:20,299
у нас есть своего рода дисконтированные веса посещений в

671
00:27:20,299 --> 00:27:22,460
соответствии с текущей политикой, у

672
00:27:22,460 --> 00:27:24,289
нас нет прямого доступа к этому, у нас есть

673
00:27:24,289 --> 00:27:26,330
доступ только к образцам из действительно не

674
00:27:26,330 --> 00:27:28,759
текущей политики, поэтому первая идея заключается в том, что

675
00:27:28,759 --> 00:27:30,739
вместо того, чтобы брать явную сумму по

676
00:27:30,739 --> 00:27:32,389
пространству состояний, где это  пространство состояний

677
00:27:32,389 --> 00:27:34,249
может быть, как вы знаете, непрерывным и

678
00:27:34,249 --> 00:27:37,009
бесконечным, мы просто посмотрим на

679
00:27:37,009 --> 00:27:38,570
состояния, которые были фактически выбраны нашей

680
00:27:38,570 --> 00:27:42,409
текущей старой политикой, и перевзвесили их, так

681
00:27:42,409 --> 00:27:43,940
что это первые две части, первая

682
00:27:43,940 --> 00:27:45,830
замена, которую мы делаем, что мы пытаемся

683
00:27:45,830 --> 00:27:46,879
сделать правильно  Теперь скажем, что у нас есть эта

684
00:27:46,879 --> 00:27:48,679
целевая функция, и мы хотим сделать

685
00:27:48,679 --> 00:27:49,940
ее так, чтобы она могла быть частью

686
00:27:49,940 --> 00:27:51,379
алгоритма, но мы могли вычислить все

687
00:27:51,379 --> 00:27:53,690
количества, которые нам нужны, чтобы выбрать

688
00:27:53,690 --> 00:27:55,220
размер шага, при котором, как мы думаем, будет новая политика.

689
00:27:55,220 --> 00:27:58,940
лучше второе, что

690
00:27:58,940 --> 00:28:00,950
мы делаем, и это относится к

691
00:28:00,950 --> 00:28:04,129
вопросу Эндрюса о выборке важности. У нас

692
00:28:04,129 --> 00:28:06,950
есть эта вторая величина здесь, где

693
00:28:06,950 --> 00:28:08,779
это вероятность действия в

694
00:28:08,779 --> 00:28:13,100
соответствии с нашей новой политикой, к которой у нас есть

695
00:28:13,100 --> 00:28:14,749
доступ в  в том смысле, что если кто-то

696
00:28:14,749 --> 00:28:17,119
дает нам состояние, мы можем сказать, что мы можем

697
00:28:17,119 --> 00:28:18,440
точно сказать, какова будет наша вероятность

698
00:28:18,440 --> 00:28:21,679
при всех действиях, но опять же, это

699
00:28:21,679 --> 00:28:28,369
часто может быть непрерывным набором, и поэтому

700
00:28:28,369 --> 00:28:31,070
вместо того, чтобы делать что-то вроде этого непрерывного

701
00:28:31,070 --> 00:28:32,119
, мы просто собираемся  скажем, мы

702
00:28:32,119 --> 00:28:34,129
собираемся использовать выборку по важности, и мы можем

703
00:28:34,129 --> 00:28:36,739
взять образцы, которые, как правило,

704
00:28:36,739 --> 00:28:39,230
будут из старого PI, поэтому мы смотрим,

705
00:28:39,230 --> 00:28:40,580
когда мы предприняли действия, учитывая нашу

706
00:28:40,580 --> 00:28:42,559
текущую политику, взвешиваем их в

707
00:28:42,559 --> 00:28:44,149
соответствии с вероятностью, которую мы бы

708
00:28:44,149 --> 00:28:45,529
имели  предприняли эти действия в соответствии с нашей новой

709
00:28:45,529 --> 00:28:48,109
политикой, так что это позволяет нам аппроксимировать

710
00:28:48,109 --> 00:28:49,639
это ожидание, используя

711
00:28:49,639 --> 00:28:53,839
то, что у нас есть, а затем третья

712
00:28:53,839 --> 00:28:55,429
замена переключает преимущество

713
00:28:55,429 --> 00:28:57,619
обратно на функцию q, и просто

714
00:28:57,619 --> 00:28:58,940
важно отметить, что все эти

715
00:28:58,940 --> 00:29:00,829
три замены не меняют

716
00:29:00,829 --> 00:29:02,359
решение цели

717
00:29:02,359 --> 00:29:05,749
задачи оптимизации, все они

718
00:29:05,749 --> 00:29:08,389
берут эти разные подстановки,

719
00:29:08,389 --> 00:29:10,849
это разные способы оценки этих

720
00:29:10,849 --> 00:29:15,200
величин, поэтому мы получаем

721
00:29:15,200 --> 00:29:17,359
следующее: хм, у нас есть  ve эта целевая

722
00:29:17,359 --> 00:29:19,489
функция, которая оптимизировала это,

723
00:29:19,489 --> 00:29:20,809
после того, как мы сделали замены, о которых я

724
00:29:20,809 --> 00:29:22,219
только что упомянул, и у нас есть это

725
00:29:22,219 --> 00:29:25,659
ограничение на то, насколько далеко мы можем быть, и

726
00:29:25,659 --> 00:29:27,769
эмпирически они обычно просто выбирают

727
00:29:27,769 --> 00:29:29,509
своего рода альтернативное

728
00:29:29,509 --> 00:29:31,700
распределение выборки q это просто ваша существующая старая

729
00:29:31,700 --> 00:29:34,849
политика  так что в статье есть куча других вещей

730
00:29:34,849 --> 00:29:36,799
, это действительно хорошая статья,

731
00:29:36,799 --> 00:29:39,529
много действительно интересных идей,

732
00:29:39,529 --> 00:29:41,749
я просто пропущу,

733
00:29:41,749 --> 00:29:43,519
как именно они делают некоторые дополнительные

734
00:29:43,519 --> 00:29:45,289
детали, там есть некоторая сложность,

735
00:29:45,289 --> 00:29:47,779
но я  Скажу кратко, главное,

736
00:29:47,779 --> 00:29:48,829
что они здесь делают, это то, что они

737
00:29:48,829 --> 00:29:51,200
запускают политику, они вычисляют этот

738
00:29:51,200 --> 00:29:54,079
градиент, они должны учитывать эти

739
00:29:54,079 --> 00:29:56,329
ограничения, и они выполняют поиск линии подачи

740
00:29:56,329 --> 00:29:58,389
с ограничением Kail, и,

741
00:29:58,389 --> 00:30:00,499
возможно, самое главное, это просто

742
00:30:00,499 --> 00:30:01,940
знать об этом и как

743
00:30:01,940 --> 00:30:03,859
бы понимать, что они были

744
00:30:03,859 --> 00:30:05,989
вдохновлены этим консервативным улучшением политики,

745
00:30:05,989 --> 00:30:07,159
а затем пытались сделать это более

746
00:30:07,159 --> 00:30:11,389
практичным и быстрым, они применили его

747
00:30:11,389 --> 00:30:13,129
к ло  Среди различных проблем есть

748
00:30:13,129 --> 00:30:14,599
несколько действительно хороших вещей о контроллерах передвижения.

749
00:30:14,599 --> 00:30:16,249
Случаи, когда у вас есть

750
00:30:16,249 --> 00:30:18,079
непрерывные пространства действий. Непрерывные

751
00:30:18,079 --> 00:30:20,179
пространства состояний. Это случаи, когда

752
00:30:20,179 --> 00:30:22,129
градиент политик часто очень полезен.

753
00:30:22,129 --> 00:30:25,269


754
00:30:25,269 --> 00:30:28,969


755
00:30:28,969 --> 00:30:30,409
знаю, что эмпирически

756
00:30:30,409 --> 00:30:31,639
это действительно хороший инструмент, о котором часто нужно знать,

757
00:30:31,639 --> 00:30:33,409
если вы используете

758
00:30:33,409 --> 00:30:35,299
подходы в стиле градиента политики. ERP может быть очень

759
00:30:35,299 --> 00:30:37,549
полезной вещью для построения, и

760
00:30:37,549 --> 00:30:39,379
это оказало невероятное влияние.

761
00:30:39,379 --> 00:30:42,200


762
00:30:42,200 --> 00:30:43,429
уже упоминаний о нем,

763
00:30:43,429 --> 00:30:45,109
так что это своего рода стало одним из

764
00:30:45,109 --> 00:30:49,459
основных ориентиров для градиента политики, хорошо,

765
00:30:49,459 --> 00:30:50,839
поэтому, если мы вернемся только к тому,

766
00:30:50,839 --> 00:30:52,369
что, чтобы обобщить, что представляет собой

767
00:30:52,369 --> 00:30:54,529
шаблон алгоритма градиента политики,

768
00:30:54,529 --> 00:30:55,489
смотрите ли вы на существующие

769
00:30:55,489 --> 00:30:56,659
алгоритмы или  пытаетесь ли вы

770
00:30:56,659 --> 00:30:58,639
определить свои собственные, как правило, они выглядят

771
00:30:58,639 --> 00:31:00,679
примерно так: для каждой

772
00:31:00,679 --> 00:31:02,659
итерации вы запускаете свою политику и

773
00:31:02,659 --> 00:31:03,200
собираете true

774
00:31:03,200 --> 00:31:05,179
победы над данными, запустив эту политику,

775
00:31:05,179 --> 00:31:07,940
вы вычисляете некую цель, которая может быть

776
00:31:07,940 --> 00:31:09,740
просто вознаграждением, которая может быть ключевой

777
00:31:09,740 --> 00:31:12,380
функцией, мы можем найти компромисс между предвзятостью

778
00:31:12,380 --> 00:31:14,600
и дисперсией, и затем мы используем

779
00:31:14,600 --> 00:31:16,429
это для оценки градиента политики, а

780
00:31:16,429 --> 00:31:18,620
затем мы можем захотеть разумно  сделайте шаг

781
00:31:18,620 --> 00:31:20,539
по этому градиенту, чтобы попытаться обеспечить

782
00:31:20,539 --> 00:31:26,570
монотонное улучшение вещей, о которых нужно

783
00:31:26,570 --> 00:31:27,649
знать, и некоторые из вещей, в которых вы

784
00:31:27,649 --> 00:31:29,630
скоро попрактикуетесь, заключаются в том, что вы

785
00:31:29,630 --> 00:31:30,740
должны быть хорошо знакомы с такими

786
00:31:30,740 --> 00:31:33,470
ванильными подходами и укреплять и

787
00:31:33,470 --> 00:31:35,269
этот общий шаблон  понять,

788
00:31:35,269 --> 00:31:36,110
как некоторые из различных алгоритмов, о которых

789
00:31:36,110 --> 00:31:37,850
мы говорим, могут создавать экземпляры

790
00:31:37,850 --> 00:31:40,970
этих разных вещей, которые вам не нужно

791
00:31:40,970 --> 00:31:42,769
выводить, и запомнить всю формулу, которую

792
00:31:42,769 --> 00:31:44,870
я только что быстро прошел сорок RPO,

793
00:31:44,870 --> 00:31:47,240
и у вас будет возможность

794
00:31:47,240 --> 00:31:49,250
попрактиковаться в этом больше  в домашнем задании 3, но

795
00:31:49,250 --> 00:31:50,870
мы лишь слегка коснемся этого с точки

796
00:31:50,870 --> 00:31:54,289
зрения промежуточного семестра, хорошо, у Сьюзен могут быть

797
00:31:54,289 --> 00:31:55,460
какие-либо вопросы по этому поводу, прежде чем мы

798
00:31:55,460 --> 00:31:57,380
перейдем к краткому обзору

799
00:31:57,380 --> 00:31:59,360
то, что мы сделали до сих пор перед

800
00:31:59,360 --> 00:32:06,159
промежуточным семестром, хорошо, давайте переключимся,

801
00:32:07,299 --> 00:32:09,919
хорошо, так что это будет своего

802
00:32:09,919 --> 00:32:14,269
рода очень краткий обзор того, что мы сделали до

803
00:32:14,269 --> 00:32:16,610
сих пор, и с точки зрения того, почему это полезно,

804
00:32:16,610 --> 00:32:18,380
есть, конечно,  много веских доказательств

805
00:32:18,380 --> 00:32:20,360
из научных наук о том, что интервальное

806
00:32:20,360 --> 00:32:22,279
повторение идей действительно полезно, так

807
00:32:22,279 --> 00:32:24,440
как принудительное запоминание, что является одним из

808
00:32:24,440 --> 00:32:27,440
других преимуществ сдачи экзаменов, и

809
00:32:27,440 --> 00:32:29,210
вот что мы собираемся сделать сегодня,

810
00:32:29,210 --> 00:32:30,980
это просто быстро резюмировать многое  из

811
00:32:30,980 --> 00:32:33,429
различных основных идей, так что опять же,

812
00:32:33,429 --> 00:32:35,059
обучение с подкреплением обычно

813
00:32:35,059 --> 00:32:36,679
включает в себя оптимизацию отсроченных

814
00:32:36,679 --> 00:32:38,360
последствий, обобщение и

815
00:32:38,360 --> 00:32:40,789
исследование, мы еще не говорили

816
00:32:40,789 --> 00:32:43,669
об этом, так что это не

817
00:32:43,669 --> 00:32:45,320
будет в среднесрочной перспективе, мы

818
00:32:45,320 --> 00:32:47,059
начнем гораздо больше говорить об этом посте

819
00:32:47,059 --> 00:32:48,950
в среднесрочной перспективе, это  невероятно важная

820
00:32:48,950 --> 00:32:50,990
тема, я думаю, очень увлекательная

821
00:32:50,990 --> 00:32:52,730
и одна из основных причин, почему RL

822
00:32:52,730 --> 00:32:54,799
интересен, но эти другие вещи

823
00:32:54,799 --> 00:32:56,090
также действительно важны, и мы потратили некоторое

824
00:32:56,090 --> 00:32:59,960
время на них до сих пор в te

825
00:32:59,960 --> 00:33:01,340
думая о промежуточном экзамене и действительно

826
00:33:01,340 --> 00:33:03,409
думая о классе в самый

827
00:33:03,409 --> 00:33:05,330
первый день, я поставил своего

828
00:33:05,330 --> 00:33:08,510
рода цели обучения Blizzard, и я просто

829
00:33:08,510 --> 00:33:10,610
хочу выделить некоторые из них

830
00:33:10,610 --> 00:33:12,409
, которые я упомянул, мы

831
00:33:12,409 --> 00:33:14,360
собираемся  явно оценивается на

832
00:33:14,360 --> 00:33:16,560
экзамене, который заключается в том, что

833
00:33:16,560 --> 00:33:18,060
к концу занятия, включая

834
00:33:18,060 --> 00:33:19,710
экзамен, вы должны быть хорошо знакомы с

835
00:33:19,710 --> 00:33:21,080
тем, каковы ключевые особенности

836
00:33:21,080 --> 00:33:22,830
обучения с подкреплением, которые

837
00:33:22,830 --> 00:33:24,060
отличают его от других проблем машинного обучения

838
00:33:24,060 --> 00:33:26,430
и других проблем ИИ, поэтому мы

839
00:33:26,430 --> 00:33:28,320
потратили  некоторое время в первый день,

840
00:33:28,320 --> 00:33:30,090
и я как бы пытался говорить об этом

841
00:33:30,090 --> 00:33:31,950
повсюду, но тот факт, что агент

842
00:33:31,950 --> 00:33:34,230
собирает свои собственные данные и что

843
00:33:34,230 --> 00:33:37,830
данные, которые он собирает, влияют на политики, которые

844
00:33:37,830 --> 00:33:39,720
он может изучить, поэтому у нас есть этот

845
00:33:39,720 --> 00:33:42,060
датчик  проблема с данными, которую агент не может знать

846
00:33:42,060 --> 00:33:43,860
о других жизнях, которых не было Ли не

847
00:33:43,860 --> 00:33:46,290
жил, имеет очень большое значение

848
00:33:46,290 --> 00:33:49,620
по сравнению с контролируемым обучением. Вторая

849
00:33:49,620 --> 00:33:51,150
действительно важная вещь заключается в том, что если

850
00:33:51,150 --> 00:33:53,820
вам дается приложение  проблема,

851
00:33:53,820 --> 00:33:55,740
важно попытаться узнать, почему я или почему бы

852
00:33:55,740 --> 00:33:57,690
не сформулировать ее формально как

853
00:33:57,690 --> 00:34:00,270
проблему обучения с подкреплением, и если да, то

854
00:34:00,270 --> 00:34:02,370
как бы вы вообще поступили,

855
00:34:02,370 --> 00:34:03,900
на этот вопрос нет единого ответа, поэтому хорошо

856
00:34:03,900 --> 00:34:05,940
подумать о том, что является одним или несколькими способами

857
00:34:05,940 --> 00:34:07,890
определить  пространство состояний действия основываются

858
00:34:07,890 --> 00:34:11,070
на динамике и модели вознаграждения, а также на

859
00:34:11,070 --> 00:34:12,750
том, какой алгоритм вы могли бы предложить из

860
00:34:12,750 --> 00:34:16,260
класса, чтобы попытаться решить эту проблему.

861
00:34:16,260 --> 00:34:18,000


862
00:34:18,000 --> 00:34:19,409


863
00:34:19,409 --> 00:34:21,469


864
00:34:21,469 --> 00:34:24,030
промышленности, а затем

865
00:34:24,030 --> 00:34:25,110
третья вещь, которая, как мне кажется, действительно

866
00:34:25,110 --> 00:34:26,730
важна, — это понять, как мы решаем

867
00:34:26,730 --> 00:34:28,260
, хорош ли алгоритм RL,

868
00:34:28,260 --> 00:34:30,300
и, таким образом, каковы критерии

869
00:34:30,300 --> 00:34:32,550
оценки производительности, которые мы можем использовать для

870
00:34:32,550 --> 00:34:34,800
оценки преимуществ,

871
00:34:34,800 --> 00:34:36,210
сильных и слабых сторон.  различных

872
00:34:36,210 --> 00:34:39,210
алгоритмов и как они сравниваются, так что это

873
00:34:39,210 --> 00:34:40,290
могут быть такие вещи, как смещение и дисперсия,

874
00:34:40,290 --> 00:34:41,909
это также может быть вычислительная

875
00:34:41,909 --> 00:34:44,699
сложность или эффективность выборки или другие

876
00:34:44,699 --> 00:34:49,668
аспекты, так что  До сих пор мы рассмотрели

877
00:34:49,668 --> 00:34:51,719
планирование, где мы знаем, как работает мир,

878
00:34:51,719 --> 00:34:55,380
модель оценки политики, бесплатное

879
00:34:55,380 --> 00:34:56,540
обучение, как принимать правильные решения,

880
00:34:56,540 --> 00:34:58,920
аппроксимацию функции ценности, а затем

881
00:34:58,920 --> 00:35:02,430
имитацию обучения, поиск политики, и

882
00:35:02,430 --> 00:35:03,990
мы также говорили о том, что

883
00:35:03,990 --> 00:35:05,640
для обучения с подкреплением в  В общем,

884
00:35:05,640 --> 00:35:07,350
вы можете думать либо о попытке найти

885
00:35:07,350 --> 00:35:09,830
функцию ценности политики, либо о модели, и

886
00:35:09,830 --> 00:35:12,780
что модели достаточно, чтобы сгенерировать

887
00:35:12,780 --> 00:35:14,430
функцию ценности, достаточную для

888
00:35:14,430 --> 00:35:17,100
создания политики, но они не все

889
00:35:17,100 --> 00:35:19,290
необходимы, вам не нужно иметь  модель

890
00:35:19,290 --> 00:35:26,150
, чтобы получить политику, поэтому

891
00:35:26,150 --> 00:35:27,830
я пройдусь по этой части довольно быстро,

892
00:35:27,830 --> 00:35:28,940
так как я думаю, что многие из вас, ребята,

893
00:35:28,940 --> 00:35:31,520
также видели кое-что из этого на предыдущих

894
00:35:31,520 --> 00:35:33,560
занятиях, так что почти все, о чем

895
00:35:33,560 --> 00:35:34,850
мы говорили до сих пор, предполагает

896
00:35:34,850 --> 00:35:36,770
мир - это марковский процесс принятия решений, о котором

897
00:35:36,770 --> 00:35:38,090
я упоминал, что часто мир

898
00:35:38,090 --> 00:35:41,240
не является марковским процессом принятия решений, и в

899
00:35:41,240 --> 00:35:43,490
случае MDP мы предполагаем, что состояние является

900
00:35:43,490 --> 00:35:45,830
достаточным. Я являюсь достаточной статистикой

901
00:35:45,830 --> 00:35:48,050
всей предшествующей истории, поэтому  нам не

902
00:35:48,050 --> 00:35:50,420
нужно отслеживать полный набор состояний

903
00:35:50,420 --> 00:35:52,190
и наблюдений и вознаграждений за действия

904
00:35:52,190 --> 00:35:54,410
за весь период времени, но мы можем

905
00:35:54,410 --> 00:35:55,970
просто смотреть на текущее наблюдение

906
00:35:55,970 --> 00:35:57,290
, чтобы принимать правильные решения в

907
00:35:57,290 --> 00:36:03,020
мире, с точки зрения этого очень

908
00:36:03,020 --> 00:36:04,520
полезно  знать, что такое марковское свойство, почему

909
00:36:04,520 --> 00:36:06,320
оно важно, почему оно может быть нарушено,

910
00:36:06,320 --> 00:36:08,630
что такое такие вещи, как модели,

911
00:36:08,630 --> 00:36:11,060
функции ценности и очереди, и что такое

912
00:36:11,060 --> 00:36:13,610
планирование и в чем разница, какое

913
00:36:13,610 --> 00:36:15,680
планирование мы предполагаем, что вам дана

914
00:36:15,680 --> 00:36:16,970
модель того, как устроен мир, вы знаете

915
00:36:16,970 --> 00:36:18,290
модель динамики, которую вы знаете, модель вознаграждения,

916
00:36:18,290 --> 00:36:20,000
все еще может быть очень трудно

917
00:36:20,000 --> 00:36:21,890
понять, как действовать, это похоже на

918
00:36:21,890 --> 00:36:24,290
знание игры в го, она по-прежнему требует

919
00:36:24,290 --> 00:36:25,850
очень больших вычислительных ресурсов и

920
00:36:25,850 --> 00:36:27,620
сложна, чтобы попытаться выяснить, какое

921
00:36:27,620 --> 00:36:29,660
оптимальное решение принять и даже пойти

922
00:36:29,660 --> 00:36:32,120
хотя вы знаете всю динамику и

923
00:36:32,120 --> 00:36:34,910
все преимущества обучения, мы не

924
00:36:34,910 --> 00:36:36,350
знаем динамику и преимущества, и нам все

925
00:36:36,350 --> 00:36:38,180
еще нужно собирать данные, чтобы изучить

926
00:36:38,180 --> 00:36:40,040
хорошую политику, которая имеет высокую ценность и высокую ценность.

927
00:36:40,040 --> 00:36:41,840
со скидкой ожидаются некоторые из

928
00:36:41,840 --> 00:36:44,060
вознаграждений, о которых мы говорили о резервном операторе bellman,

929
00:36:44,060 --> 00:36:45,830
что является сокращением,

930
00:36:45,830 --> 00:36:47,530
если ваш коэффициент скидки меньше 1,

931
00:36:47,530 --> 00:36:50,150
что означает, что те повторные приложения,

932
00:36:50,150 --> 00:36:51,470
которые вы гарантированно сойдетесь в

933
00:36:51,470 --> 00:36:55,670
одну фиксированную точку, мы говорили о ценности по

934
00:36:55,670 --> 00:36:57,950
сравнению с итерацией политики  в

935
00:36:57,950 --> 00:37:00,800
итерации значения на итерации K вы

936
00:37:00,800 --> 00:37:02,600
всегда вычисляете оптимальное значение, как если бы

937
00:37:02,600 --> 00:37:06,290
вы принимали только K решений, а

938
00:37:06,290 --> 00:37:08,960
затем использовали это для резервного копирования и получения

939
00:37:08,960 --> 00:37:09,770
K плюс 1

940
00:37:09,770 --> 00:37:13,100
итерации политики политики, у вас всегда

941
00:37:13,100 --> 00:37:15,290
есть политика и значение этого  политику, если

942
00:37:15,290 --> 00:37:18,560
вы будете действовать, используя ее навсегда, но это

943
00:37:18,560 --> 00:37:21,470
может быть не очень хорошая политика, а затем

944
00:37:21,470 --> 00:37:24,080
вы обновляете ее, и, как мы видели, она

945
00:37:24,080 --> 00:37:25,700
тесно связана с своего рода

946
00:37:25,700 --> 00:37:27,890
алгоритмами стиля градиента политики, где мы как

947
00:37:27,890 --> 00:37:29,510
бы пытаемся оценить градиент

948
00:37:29,510 --> 00:37:34,190
политики, поэтому в итерации политики в целом

949
00:37:34,190 --> 00:37:35,780
и аналогично тому, что мы видели в

950
00:37:35,780 --> 00:37:37,280
подходах к оценке политики, мы смешиваем

951
00:37:37,280 --> 00:37:38,420
оценку,

952
00:37:38,420 --> 00:37:40,369
но поэтому мы можем быть ценностью политики,

953
00:37:40,369 --> 00:37:41,900
а затем мы используем это в заказе.  r сделать

954
00:37:41,900 --> 00:37:44,930
шаг и улучшить его, если мы в

955
00:37:44,930 --> 00:37:47,720
случае отсутствия модели и не

956
00:37:47,720 --> 00:37:49,130
имеем доступа к модели, мы часто хотим

957
00:37:49,130 --> 00:37:51,859
вместо этого вычислить значения Q, чтобы мы могли

958
00:37:51,859 --> 00:37:57,170
напрямую улучшить политику, поэтому

959
00:37:57,170 --> 00:37:59,240
давайте просто уделим немного времени, чтобы эти

960
00:37:59,240 --> 00:38:00,470
проверить свое понимание или хорошие вещи,

961
00:38:00,470 --> 00:38:01,940
чтобы вернуться к ним, все вроде

962
00:38:01,940 --> 00:38:05,000
как вы знаете некоторые небольшие концептуальные

963
00:38:05,000 --> 00:38:06,410
вопросы типа, которые мы могли бы задать

964
00:38:06,410 --> 00:38:07,880
вам на экзамене, так что давайте уделим

965
00:38:07,880 --> 00:38:09,380
минуту и проверим наше понимание и п

966
00:38:09,380 --> 00:38:10,910
думаем над  конечное состояние

967
00:38:10,910 --> 00:38:12,769
действия MDP представление таблицы поиска,

968
00:38:12,769 --> 00:38:14,690
что означает, что у нас

969
00:38:14,690 --> 00:38:16,190
есть просто запись в таблице для каждого состояния в

970
00:38:16,190 --> 00:38:20,240
действии гамма меньше 1 влияет ли

971
00:38:20,240 --> 00:38:21,400
начальное изучение функции значения

972
00:38:21,400 --> 00:38:23,779
на окончательные вычисленные значения почему или

973
00:38:23,779 --> 00:38:26,029
почему не всегда итерация значения и итерация политики

974
00:38:26,029 --> 00:38:27,680
дают одно и то же решение,

975
00:38:27,680 --> 00:38:30,980
и это количество итераций, необходимых

976
00:38:30,980 --> 00:38:33,470
для итерации политики пали в конечном

977
00:38:33,470 --> 00:38:35,420
состоянии, действие ограничено MDP, и если да,

978
00:38:35,420 --> 00:38:38,690
то сколько, позвольте мне просто потратить минуту и

979
00:38:38,690 --> 00:38:41,119
умать об этом.  ose, не стесняйтесь говорить с

980
00:38:41,119 --> 00:38:43,690
кем-то рядом с вами,

981
00:39:41,079 --> 00:39:45,169
хорошо, мы будем голосовать, так что

982
00:39:45,169 --> 00:39:46,939
я спрошу, кто думает, что первоначальная

983
00:39:46,939 --> 00:39:48,679
установка значения функции значения

984
00:39:48,679 --> 00:39:52,789
не имеет большого значения да, это не имеет

985
00:39:52,789 --> 00:39:54,759
значения, и это не имеет значения  неважно, так что нет

986
00:39:54,759 --> 00:39:57,549
не имеет значения и почему бы и нет, потому что

987
00:39:57,549 --> 00:40:00,939
есть только одна фиксированная точка,

988
00:40:00,939 --> 00:40:03,349
потому что это похоже на то, что оператор посыльного

989
00:40:03,349 --> 00:40:08,989
- это сокращение, хорошо, как насчет того, кто

990
00:40:08,989 --> 00:40:10,429
думает, что итерация значения и итерация политики

991
00:40:10,429 --> 00:40:12,140
всегда дают одно и то же решение

992
00:40:12,140 --> 00:40:12,939
да

993
00:40:12,939 --> 00:40:25,729
нет точно да так что  правильно,

994
00:40:25,729 --> 00:40:27,859
вы получите одну и ту же функцию значения,

995
00:40:27,859 --> 00:40:28,939
так что это зависит от того, как вы пытаетесь

996
00:40:28,939 --> 00:40:30,409
ответить на это, они будут иметь одинаковое

997
00:40:30,409 --> 00:40:33,289
значение, у них могут быть разные политики,

998
00:40:33,289 --> 00:40:38,209
и это возможно, если существует более

999
00:40:38,209 --> 00:40:39,979
одной политики, которая имеет один и тот же оптимальный

1000
00:40:39,979 --> 00:40:42,319
значение, которое может возникнуть, потому что часто

1001
00:40:42,319 --> 00:40:44,719
существует несколько политик, в которых вы

1002
00:40:44,719 --> 00:40:49,009
разделяете связи, кто думает, что

1003
00:40:49,009 --> 00:40:50,359
количество итераций, необходимых для

1004
00:40:50,359 --> 00:40:52,929
продолжительности политики, ограничено, это правильно,

1005
00:40:52,929 --> 00:40:58,689
все говорят мне, сколько

1006
00:41:00,519 --> 00:41:04,039
это  общее количество политик в

1007
00:41:04,039 --> 00:41:07,759
итерации политики в табличных MDP с

1008
00:41:07,759 --> 00:41:09,919
аналогичным улучшением политики в табличных mdps,

1009
00:41:09,919 --> 00:41:11,749
которые вы гарантированно будете монотонно

1010
00:41:11,749 --> 00:41:13,130
улучшать, поэтому вы не сможете пройти

1011
00:41:13,130 --> 00:41:15,349
через каждую политику один раз, а затем

1012
00:41:15,349 --> 00:41:17,719
закончите, поэтому это как бы связано с

1013
00:41:17,719 --> 00:41:18,679
тем, что мы  мы только что говорили, в этом

1014
00:41:18,679 --> 00:41:21,049
случае вы определенно получите гарантированное

1015
00:41:21,049 --> 00:41:23,599
улучшение политики, потому что каждый раз, когда

1016
00:41:23,599 --> 00:41:25,099
нет аппроксимации функции,

1017
00:41:25,099 --> 00:41:26,569
нет ошибок, вы точно знаете, каково

1018
00:41:26,569 --> 00:41:27,949
текущее значение, и тогда вы можете сделать

1019
00:41:27,949 --> 00:41:31,849
шаг монотонного улучшения, так что

1020
00:41:31,849 --> 00:41:33,049
теперь мы собираемся поговорить  краткое

1021
00:41:33,049 --> 00:41:34,789
напоминание об оценке политики без моделей,

1022
00:41:34,789 --> 00:41:38,389
так что это оценка политики без моделей

1023
00:41:38,389 --> 00:41:39,949
была своего рода пассивным

1024
00:41:39,949 --> 00:41:40,880
обучением с подкреплением,

1025
00:41:40,880 --> 00:41:43,669
когда мы просто пытаемся понять,

1026
00:41:43,669 --> 00:41:46,369
насколько хороша существующая политика в идеале

1027
00:41:46,369 --> 00:41:48,369
с не слишком большим количеством данных,

1028
00:41:48,369 --> 00:41:50,950
и поэтому мы хотим  либо напрямую

1029
00:41:50,950 --> 00:41:52,630
оценить функцию сигнала, либо функцию ценности

1030
00:41:52,630 --> 00:41:57,099
политики, и поэтому

1031
00:41:57,099 --> 00:41:58,779
в этом случае мы говорили в основном об эпизодических

1032
00:41:58,779 --> 00:41:59,410
доменах,

1033
00:41:59,410 --> 00:42:01,450
когда я говорю об эпизодическом домене.  ains Я имею в виду, что

1034
00:42:01,450 --> 00:42:02,859
мы будем действовать в мире в течение фиксированного

1035
00:42:02,859 --> 00:42:05,380
количества шагов или где мы находимся в

1036
00:42:05,380 --> 00:42:06,700
обстановке, где мы знаем, что у нас есть терминальные состояния,

1037
00:42:06,700 --> 00:42:08,769
поэтому мы знаем, что эпизоды закончатся

1038
00:42:08,769 --> 00:42:12,279
с вероятностью один, они должны закончиться, а

1039
00:42:12,279 --> 00:42:14,319
затем в  в этот момент вы сбрасываетесь в

1040
00:42:14,319 --> 00:42:16,660
начальное состояние с некоторым фиксированным распределением,

1041
00:42:16,660 --> 00:42:19,690
и в подходах Монте-Карло мы

1042
00:42:19,690 --> 00:42:22,059
напрямую усредняем эпизодические награды,

1043
00:42:22,059 --> 00:42:24,039
это довольно просто, мы берем нашу существующую

1044
00:42:24,039 --> 00:42:25,900
политику, мы запускаем ее на H шагов или

1045
00:42:25,900 --> 00:42:28,960
до конца эпизода, мы сбрасываем, мы

1046
00:42:28,960 --> 00:42:30,130
повторяем это много раз, и

1047
00:42:30,130 --> 00:42:33,249
мы просто усредняем, но в телевизионном обучении или

1048
00:42:33,249 --> 00:42:34,749
ключевом обучении мы используем цель для

1049
00:42:34,749 --> 00:42:37,749
начальной загрузки, и я знаю, что вы, ребята, видели

1050
00:42:37,749 --> 00:42:39,130
это несколько раз, но просто для

1051
00:42:39,130 --> 00:42:41,619
освежения знаний, и мне нравятся эти диаграммы, просто

1052
00:42:41,619 --> 00:42:44,349
чтобы подумать  различия, поэтому мы

1053
00:42:44,349 --> 00:42:45,849
говорили о динамическом программировании здесь,

1054
00:42:45,849 --> 00:42:47,140
мы подумали о случае, когда мы

1055
00:42:47,140 --> 00:42:49,299
знаем модель перехода, мы знаем

1056
00:42:49,299 --> 00:42:52,630
модель вознаграждения, поэтому, когда мы думаем о том,

1057
00:42:52,630 --> 00:42:54,670
какова ценность политики, она точно

1058
00:42:54,670 --> 00:42:57,039
равна ожидаемому распределению

1059
00:42:57,039 --> 00:43:00,039
состояний и действий, с которыми мы столкнемся,

1060
00:43:00,039 --> 00:43:02,499
следуя этой политике вознаграждения, которое

1061
00:43:02,499 --> 00:43:04,869
мы получим, плюс гамма, умноженная на

1062
00:43:04,869 --> 00:43:06,999
значение следующего состояния, поэтому обратите внимание, что, когда мы

1063
00:43:06,999 --> 00:43:08,469
думаем об этом ожидании здесь,

1064
00:43:08,469 --> 00:43:18,999
на самом деле есть простое число S, так что

1065
00:43:18,999 --> 00:43:20,589
ожидание думает обо всех

1066
00:43:20,589 --> 00:43:23,200
следующих  состояния, в которые мы можем попасть, и поэтому

1067
00:43:23,200 --> 00:43:25,420
в динамическом программировании мы просто

1068
00:43:25,420 --> 00:43:28,059
явно думаем об этой сумме, которую может достичь сумма

1069
00:43:28,059 --> 00:43:30,130
по всем следующим состояниям, которых мы много

1070
00:43:30,130 --> 00:43:31,869
достигаем, и о значении каждого из

1071
00:43:31,869 --> 00:43:33,670
этих состояний, поэтому, если мы начали в

1072
00:43:33,670 --> 00:43:36,460
этом состоянии, мы предпринимаем действие  мы добираемся до

1073
00:43:36,460 --> 00:43:38,950
некоторых следующих новых состояний, в общем, мы могли бы

1074
00:43:38,950 --> 00:43:41,229
повторять этот процесс до тех пор, пока

1075
00:43:41,229 --> 00:43:43,269
не достигнем горизонта H или

1076
00:43:43,269 --> 00:43:47,079
конечных состояний, и здесь мы

1077
00:43:47,079 --> 00:43:48,339
думаем о том, чтобы принять ожидание

1078
00:43:48,339 --> 00:43:50,309
относительно следующих состояний, которых мы достигнем,

1079
00:43:50,309 --> 00:43:53,109
и что  мы делаем в динамическом программировании

1080
00:43:53,109 --> 00:43:56,200
вместо начальной загрузки, поэтому здесь мы подразумеваем под

1081
00:43:56,200 --> 00:43:59,279
начальной загрузкой

1082
00:44:00,220 --> 00:44:01,839
то, что вместо построения всего этого

1083
00:44:01,839 --> 00:44:03,819
дерева мы отслеживаем, каково

1084
00:44:03,819 --> 00:44:06,190
значение всех состояний, и мы используем это для

1085
00:44:06,190 --> 00:44:08,050
t  сделать явное ожидание по

1086
00:44:08,050 --> 00:44:10,119
следующим состояниям, мы достигнем среднего

1087
00:44:10,119 --> 00:44:12,310
значения этих следующих состояний и обратите внимание, что

1088
00:44:12,310 --> 00:44:14,349
в этом случае мы предполагаем, что знаем

1089
00:44:14,349 --> 00:44:17,319
модель, теперь есть способы расширить это,

1090
00:44:17,319 --> 00:44:18,520
когда мы не знаем модель, но мы

1091
00:44:18,520 --> 00:44:19,599
я не очень много говорил о них в

1092
00:44:19,599 --> 00:44:23,619
этом термине, поэтому, но мы хорошо относимся к динамическому

1093
00:44:23,619 --> 00:44:26,020
программированию здесь, я имею в виду, что, если мы не

1094
00:44:26,020 --> 00:44:27,280
укажем иное, что мы знаем

1095
00:44:27,280 --> 00:44:31,060
модели мира, так что это тот случай,

1096
00:44:31,060 --> 00:44:32,530
когда мы загружаемся, потому что мы

1097
00:44:32,530 --> 00:44:36,690
обновляем  использование v4v использовать в качестве оценки,

1098
00:44:36,690 --> 00:44:39,099
хорошо, потому что эти значения не

1099
00:44:39,099 --> 00:44:40,420
будут идеальными оценками, чем истинное

1100
00:44:40,420 --> 00:44:41,950
ожидаемое вознаграждение со скидкой для

1101
00:44:41,950 --> 00:44:43,180
следующих состояний, потому что мы все еще

1102
00:44:43,180 --> 00:44:46,180
их вычисляем, затем мы посмотрели на

1103
00:44:46,180 --> 00:44:48,460
оценку политики Монте-Карло, и она

1104
00:44:48,460 --> 00:44:50,650
во многих отношениях выглядит очень похожей  кроме

1105
00:44:50,650 --> 00:44:52,660
того, что мы делаем, мы пишем

1106
00:44:52,660 --> 00:44:54,190
траекторию до самого

1107
00:44:54,190 --> 00:44:55,930
горизонта, мы суммируем все вознаграждения,

1108
00:44:55,930 --> 00:45:00,250
и это наша цель, и когда мы говорим,

1109
00:45:00,250 --> 00:45:02,920
что оценка политики с помощью Монте-Карло

1110
00:45:02,920 --> 00:45:04,780
является выборкой, это означает  мы пробуем

1111
00:45:04,780 --> 00:45:07,119
Ричарда, каково ожидание, к которому

1112
00:45:07,119 --> 00:45:09,750
мы приближаемся, мы ожидаем

1113
00:45:09,750 --> 00:45:11,770
примерно ожидание этой

1114
00:45:11,770 --> 00:45:15,280
вероятности простого числа s, поэтому мы получили только

1115
00:45:15,280 --> 00:45:17,500
одно простое число s вместо того, чтобы получить

1116
00:45:17,500 --> 00:45:20,589
ожидание всех следующих,

1117
00:45:20,589 --> 00:45:21,819
и проблема с  то есть мы сказали, что это

1118
00:45:21,819 --> 00:45:23,200
была высокая дисперсия, хотя она была

1119
00:45:23,200 --> 00:45:26,380
беспристрастной, а затем мы говорили об

1120
00:45:26,380 --> 00:45:27,910
объединении этих идей с

1121
00:45:27,910 --> 00:45:30,579
методами временной разницы, где мы

1122
00:45:30,579 --> 00:45:32,349
собираемся выполнить начальную загрузку и выборку, поэтому мы делаем

1123
00:45:32,349 --> 00:45:35,380
выборку, потому что мы смотрим только

1124
00:45:35,380 --> 00:45:37,839
на один следующий  состояние, и мы выполняем

1125
00:45:37,839 --> 00:45:41,650
начальную загрузку, потому что мы подставляем

1126
00:45:41,650 --> 00:45:44,380
нашу оценку V, поэтому мы выбираем

1127
00:45:44,380 --> 00:45:46,839
одно значение s T плюс 1, и мы

1128
00:45:46,839 --> 00:45:48,250


1129
00:45:48,250 --> 00:45:49,510
выполняем начальную загрузку, потому что мы не выкатываемся полностью, как в случае с Монте-

1130
00:45:49,510 --> 00:45:51,130
Карло, мы просто  подключив нашу текущую

1131
00:45:51,130 --> 00:45:57,369
оценку этой функции значения, поэтому

1132
00:45:57,369 --> 00:45:59,890
давайте еще раз быстро разберемся, для

1133
00:45:59,890 --> 00:46:01,930
каждого из этих случаев полезно знать

1134
00:46:01,930 --> 00:46:04,630
, применимо ли оно к

1135
00:46:04,630 --> 00:46:07,119
динамическому программированию, которое требует от

1136
00:46:07,119 --> 00:46:10,060
вас знания t  он моделирует Монте-Карло или TD-

1137
00:46:10,060 --> 00:46:12,490
обучение, так что его можно использовать, когда мы не

1138
00:46:12,490 --> 00:46:13,650
знаем модели тока

1139
00:46:13,650 --> 00:46:16,020
, он обрабатывает продолжающиеся

1140
00:46:16,020 --> 00:46:18,119
неэпизодические домены, обрабатывает ли он

1141
00:46:18,119 --> 00:46:21,960
немарковские домены, позвольте мне

1142
00:46:21,960 --> 00:46:23,970
прояснить, что я имею в виду под этим  вы

1143
00:46:23,970 --> 00:46:25,680
всегда можете применить любой алгоритм к чему угодно,

1144
00:46:25,680 --> 00:46:27,359
он просто может дать вам мусор, и поэтому

1145
00:46:27,359 --> 00:46:30,180
мой вопрос заключается в том, когда вы, когда я говорю об

1146
00:46:30,180 --> 00:46:32,039
обработке немарковских областей,

1147
00:46:32,039 --> 00:46:33,900
гарантированно ли он делает что-то хорошее, или

1148
00:46:33,900 --> 00:46:35,579
он, вероятно, фундаментально делает марковское

1149
00:46:35,579 --> 00:46:38,250
предположение, сходится ли он к  истинное

1150
00:46:38,250 --> 00:46:39,720
значение политики и предел

1151
00:46:39,720 --> 00:46:41,640
обновлений прямо сейчас мы думаем о

1152
00:46:41,640 --> 00:46:46,349
табличном случае, поэтому функция значения

1153
00:46:46,349 --> 00:46:48,569
точно представима, и дает ли она

1154
00:46:48,569 --> 00:46:50,039
нам непредвзятую оценку значения

1155
00:46:50,039 --> 00:46:54,960
по пути более быстрая середина все еще может быть

1156
00:46:54,960 --> 00:46:56,940
последовательной, что означает, что в конечном итоге

1157
00:46:56,940 --> 00:46:58,559
с достаточным количеством данных они сходятся к

1158
00:46:58,559 --> 00:46:59,700
правильному результату, но они могут дать вам

1159
00:46:59,700 --> 00:47:01,770
предвзятые оценки по пути, поэтому снова

1160
00:47:01,770 --> 00:47:03,029
давайте просто потратим минуту или две,

1161
00:47:03,029 --> 00:47:04,829
и реклама в том, что они просто б

1162
00:47:04,829 --> 00:47:07,260
да или нет

1163
00:47:07,260 --> 00:47:09,450
для каждого из них, не стесняйтесь говорить с

1164
00:47:09,450 --> 00:47:12,079
кем-нибудь следующим

1165
00:48:12,410 --> 00:48:16,790
, это отличный вопрос, качественная итерация,

1166
00:48:30,080 --> 00:48:32,340
хорошо, и попросите людей проголосовать еще раз,

1167
00:48:32,340 --> 00:48:35,550
хорошо, поэтому я просто попрошу вас поднять

1168
00:48:35,550 --> 00:48:37,470
руки, если ответ  Да,

1169
00:48:37,470 --> 00:48:41,220
значит, можно ли использовать динамическое программирование DP,

1170
00:48:41,220 --> 00:48:42,810
когда нет моделей текущего

1171
00:48:42,810 --> 00:48:48,900
домена? Нет, можно использовать по методу Монте-Карло. Да,

1172
00:48:48,900 --> 00:48:53,100
можно использовать TD.

1173
00:48:53,100 --> 00:48:57,030


1174
00:48:57,030 --> 00:48:58,200


1175
00:48:58,200 --> 00:48:59,760


1176
00:48:59,760 --> 00:49:01,800
вы можете использовать операторы Биллмана и

1177
00:49:01,800 --> 00:49:03,780
сокращения, даже если вы знаете для

1178
00:49:03,780 --> 00:49:05,490
областей с бесконечным горизонтом, вы обычно

1179
00:49:05,490 --> 00:49:06,810
хотите, чтобы ваша гамма-функция была меньше единицы,

1180
00:49:06,810 --> 00:49:10,350
чтобы ваши значения не взрывались, но вы

1181
00:49:10,350 --> 00:49:11,670
можете сделать это,

1182
00:49:11,670 --> 00:49:14,430
что насчет оценок Монте-Карло нет

1183
00:49:14,430 --> 00:49:16,650
права Монте-Карло только обновляет  когда вы

1184
00:49:16,650 --> 00:49:18,660
дойдете до конца эпизода, телевизионные

1185
00:49:18,660 --> 00:49:20,670
оценки да,

1186
00:49:20,670 --> 00:49:23,250
отлично, DP обрабатывает немарковские

1187
00:49:23,250 --> 00:49:30,080
домены, нет, Монте-Карло снова догадывается, что телевидение

1188
00:49:30,080 --> 00:49:32,040
вы можете запускать все эти вещи,

1189
00:49:32,040 --> 00:49:35,730
где хотите, но да, сходится

1190
00:49:35,730 --> 00:49:37,140
к истинному значению политики в

1191
00:49:37,140 --> 00:49:41,120
пределе обновлений для DP да Монте-Карло

1192
00:49:41,120 --> 00:49:44,420
да TD да

1193
00:49:44,420 --> 00:49:46,920
непредвзятая оценка значения DP это как

1194
00:49:46,920 --> 00:49:48,720
бы неприменимо, потому что мы на

1195
00:49:48,720 --> 00:49:50,280
самом деле не используем данные это как бы

1196
00:49:50,280 --> 00:49:52,320
немного другое Монте-Карло это

1197
00:49:52,320 --> 00:49:56,360
непредвзятая оценка значения да TD нет

1198
00:49:56,360 --> 00:50:00,600
отлично так что если мы спрашиваем вас об

1199
00:50:00,600 --> 00:50:01,950
этом на экзамене мы обязательно уточним

1200
00:50:01,950 --> 00:50:03,750
говорим ли мы о табличной

1201
00:50:03,750 --> 00:50:05,340
настройке или о настройке приближения функции

1202
00:50:05,340 --> 00:50:07,020
где все может быть очень

1203
00:50:07,020 --> 00:50:07,380
разным

1204
00:50:07,380 --> 00:50:10,780
да, белый чай не работает,

1205
00:50:10,780 --> 00:50:15,490
да, да, есть хороший, так

1206
00:50:15,490 --> 00:50:17,170
почему он не работает для марковского эм,

1207
00:50:17,170 --> 00:50:19,120
это потому, что он в основном делает

1208
00:50:19,120 --> 00:50:22,060
марковское предположение о домене,

1209
00:50:22,060 --> 00:50:24,220
и причина, по которой возникает, здесь, так

1210
00:50:24,220 --> 00:50:26,140
как он пишет  вниз по функции ценности

1211
00:50:26,140 --> 00:50:29,050
, это говорит о том,

1212
00:50:29,050 --> 00:50:30,430
что ожидаемая сумма вознаграждений от

1213
00:50:30,430 --> 00:50:32,230
текущего состояния в точности равна

1214
00:50:32,230 --> 00:50:34,720
немедленному вознаграждению плюс дисконтированная

1215
00:50:34,720 --> 00:50:36,310
будущая сумма вознаграждений для каждого из

1216
00:50:36,310 --> 00:50:37,960
следующих состояний, где это инкапсулировано

1217
00:50:37,960 --> 00:50:41,740
только s T плюс 1, так что именно здесь

1218
00:50:41,740 --> 00:50:43,150
вы делаете марковское предположение,

1219
00:50:43,150 --> 00:50:45,640
потому что ваш псевдоним видит, есть ли у вас

1220
00:50:45,640 --> 00:50:47,680
пространство наблюдения, которое было бы псевдонимом,

1221
00:50:47,680 --> 00:50:49,330
который игнорирует всю историю, тогда как

1222
00:50:49,330 --> 00:50:51,250
Монте-Карло суммирует все

1223
00:50:51,250 --> 00:50:55,350
награды от этого текущего  состояние вперед

1224
00:51:01,740 --> 00:51:05,440
отличный вопрос эмммм юнинг

1225
00:51:05,440 --> 00:51:08,410
очень хорошо, так что говоря, что мы говорили почти

1226
00:51:08,410 --> 00:51:09,490
обо всем, о чем мы говорили, это

1227
00:51:09,490 --> 00:51:11,740
td0, где у нас просто есть это вознаграждение плюс

1228
00:51:11,740 --> 00:51:13,869
гамма, время от времени функция значения, но мы

1229
00:51:13,869 --> 00:51:16,599
также кратко обсудили n шаг, где

1230
00:51:16,599 --> 00:51:19,030
вы вроде как будете делать  r1 плюс r2 плюс

1231
00:51:19,030 --> 00:51:22,030
гамма, умноженная на r2 и т. д., поэтому для шага N у

1232
00:51:22,030 --> 00:51:23,349
вас будет что-то вроде этого:

1233
00:51:23,349 --> 00:51:27,430
RT плюс гамма RT плюс 1 плюс гамма в

1234
00:51:27,430 --> 00:51:31,329
квадрате RT плюс 2 плюс гамма в кубе V из

1235
00:51:31,329 --> 00:51:35,319
s T плюс 3, так что это будет похоже на  шаг N,

1236
00:51:35,319 --> 00:51:38,260
и это, по сути, делает

1237
00:51:38,260 --> 00:51:39,790
разные представления о марковских

1238
00:51:39,790 --> 00:51:42,160
предположениях, потому что у вас может быть

1239
00:51:42,160 --> 00:51:43,270
континуум, вы можете иметь либо

1240
00:51:43,270 --> 00:51:44,859
полностью немарковские области, либо у нас

1241
00:51:44,859 --> 00:51:46,089
могут быть такие вещи, как n шагов марковских

1242
00:51:46,089 --> 00:51:47,890
областей, что по существу означает  если

1243
00:51:47,890 --> 00:51:49,060
вы отслеживаете

1244
00:51:49,060 --> 00:52:00,550
определенный период истории, просто для

1245
00:52:00,550 --> 00:52:02,890
того, чтобы привести пример, похожий на

1246
00:52:02,890 --> 00:52:03,970
некоторые из тех, что мы видели раньше,

1247
00:52:03,970 --> 00:52:04,990
мы могли бы подумать о чем-то вроде

1248
00:52:04,990 --> 00:52:07,720
процесса случайного блуждания, так что представьте, что мы

1249
00:52:07,720 --> 00:52:09,369
есть домен, в котором у нас есть три состояния

1250
00:52:09,369 --> 00:52:11,920
и два терминальных состояния, поэтому мы всегда

1251
00:52:11,920 --> 00:52:14,829
начинаем с состояния B, а затем с

1252
00:52:14,829 --> 00:52:18,359
вероятностью 50% идем влево или вправо, и

1253
00:52:18,359 --> 00:52:21,490
если вы достигли любого из черных узлов,

1254
00:52:21,490 --> 00:52:25,420
то процесс завершается, и когда вы

1255
00:52:25,420 --> 00:52:27,160
туда доберетесь, либо вы  получите плюс один к

1256
00:52:27,160 --> 00:52:29,859
этому или вы получите ноль, и это

1257
00:52:29,859 --> 00:52:34,060
случайное блуждание с равной вероятностью, пока

1258
00:52:34,060 --> 00:52:35,050
вы не дойдете до конечного состояния, а затем

1259
00:52:35,050 --> 00:52:39,220
процесс завершится, и поэтому в этом случае мы

1260
00:52:39,220 --> 00:52:40,420
могли бы попытаться вычислить, например, каково

1261
00:52:40,420 --> 00:52:44,349
истинное значение состояния, поэтому  истинное

1262
00:52:44,349 --> 00:52:47,079
значение состояния в этом случае заставило бы нас

1263
00:52:47,079 --> 00:52:50,319
подумать о том, каково

1264
00:52:50,319 --> 00:52:51,910
распределение состояний, которые вы бы посетили в

1265
00:52:51,910 --> 00:52:54,720
процессе этого случайного блуждания, поэтому, например,

1266
00:52:54,720 --> 00:52:58,540
если мы подумаем о том, каково значение,

1267
00:52:58,540 --> 00:53:00,520
я сделаю это, так что если вы думаете  о

1268
00:53:00,520 --> 00:53:06,640
чем v  значение имеет состояние C, которое всегда

1269
00:53:06,640 --> 00:53:08,200
будет равно непосредственному слову

1270
00:53:08,200 --> 00:53:11,230
плюс гамма, умноженному на сумму следующего

1271
00:53:11,230 --> 00:53:13,500


1272
00:53:13,780 --> 00:53:17,110
значения состояний, лучшее простое число, давайте назовем это

1273
00:53:17,110 --> 00:53:22,240
, как я знаю, SD в этом случае равно нулю,

1274
00:53:22,240 --> 00:53:24,940
поэтому значение DS всегда будет равно  быть

1275
00:53:24,940 --> 00:53:28,840
охлажденным плюс один, так что V SD

1276
00:53:28,840 --> 00:53:30,760
равно плюс один, потому что вы получаете это вознаграждение,

1277
00:53:30,760 --> 00:53:33,700
а затем оно прекращается, так что это будет означать, что

1278
00:53:33,700 --> 00:53:36,790
с гаммой, умноженной на половину вероятности, вы

1279
00:53:36,790 --> 00:53:40,540
перейдете к значению SB, это DB

1280
00:53:40,540 --> 00:53:46,840
плюс половина, вы получите один и, в конце концов, если

1281
00:53:46,840 --> 00:53:48,400
вы посмотрите на это распределение, которое

1282
00:53:48,400 --> 00:53:50,770
будет C, вы можете выполнить этот процесс

1283
00:53:50,770 --> 00:53:52,930
для каждого из различных состояний, что

1284
00:53:52,930 --> 00:53:54,430
вы обнаружите, когда сделаете это, так это то, что

1285
00:53:54,430 --> 00:53:55,840
вы проходите через это случайное блуждание,

1286
00:53:55,840 --> 00:53:57,460
заканчивающееся с правой или

1287
00:53:57,460 --> 00:53:58,870
левой стороны с точки зрения  распределение вероятностей,

1288
00:53:58,870 --> 00:54:01,000
и вы могли бы вычислить

1289
00:54:01,000 --> 00:54:05,380
значения для этого на экзамене, мы,

1290
00:54:05,380 --> 00:54:07,000
вероятно, сделали бы это немного проще,

1291
00:54:07,000 --> 00:54:08,590
но хорошо иметь возможность посмотреть

1292
00:54:08,590 --> 00:54:10,870
на этот пример, проработать его и

1293
00:54:10,870 --> 00:54:12,820
увидеть, какой будет эта часть с точки

1294
00:54:12,820 --> 00:54:15,760
зрения  value, тогда следующий

1295
00:54:15,760 --> 00:54:17,440
вопрос: давайте представим, что у нас есть

1296
00:54:17,440 --> 00:54:19,060
конкретная траектория, которую мы хотим сравнить,

1297
00:54:19,060 --> 00:54:20,800
что произойдет при использовании разных

1298
00:54:20,800 --> 00:54:22,870
алгоритмов, поэтому давайте представим, что у

1299
00:54:22,870 --> 00:54:28,170
нас есть траектория, по которой мы идем b CB

1300
00:54:28,170 --> 00:54:35,490
c-терминал плюс 1, так что это наш эпизод

1301
00:54:35,490 --> 00:54:40,390
Итак, что такое Монте-Карло при первом посещении?

1302
00:54:40,390 --> 00:54:48,730
Оценка B, что правильно, так что V

1303
00:54:48,730 --> 00:54:50,920
of B равно 1, почему это происходит потому,

1304
00:54:50,920 --> 00:54:53,140
что мы делаем в Монте-Карло то, что мы

1305
00:54:53,140 --> 00:54:54,490
суммируем кошельки в Монте-Карло, калории, которые мы смотрим

1306
00:54:54,490 --> 00:54:56,500
при первом посещении.  состояние, и мы

1307
00:54:56,500 --> 00:54:58,570
суммируем все награды, которые мы получаем от этого

1308
00:54:58,570 --> 00:55:00,070
состояния до конца эпизода, в

1309
00:55:00,070 --> 00:55:04,090
этом случае, который вознаграждает только 1, поэтому

1310
00:55:04,090 --> 00:55:07,630
оценка этого будет 1 единственное

1311
00:55:07,630 --> 00:55:09,190
другое исключение, о котором вы знаете, что мы

1312
00:55:09,190 --> 00:55:10,480
могли бы знать о нем  если

1313
00:55:10,480 --> 00:55:11,590
вы выполняете своего рода скользящее

1314
00:55:11,590 --> 00:55:14,320
среднее, например, оценку альфа, чтобы

1315
00:55:14,320 --> 00:55:16,300
обновить оценку Монте-Карло, вы хотели бы

1316
00:55:16,300 --> 00:55:17,800
знать, каковы были начальные значения и

1317
00:55:17,800 --> 00:55:19,300
какая была альфа, но давайте представим, что

1318
00:55:19,300 --> 00:55:22,810
здесь вы просто смотрите на то, чтобы точно получить

1319
00:55:22,810 --> 00:55:25,330
этот доход, поэтому  это  равен

1320
00:55:25,330 --> 00:55:27,050
возврату, начиная с B

1321
00:55:27,050 --> 00:55:30,650
и заканчивая концом эпизода, поэтому

1322
00:55:30,650 --> 00:55:33,620
следующий вопрос заключается в том, каковы

1323
00:55:33,620 --> 00:55:35,240
обновления обучения TD с учетом данных в этом

1324
00:55:35,240 --> 00:55:39,650
порядке c-терминал + 1 vc 0 CB 0 со

1325
00:55:39,650 --> 00:55:44,000
скоростью обучения, возможно, просто

1326
00:55:44,000 --> 00:55:47,900
потратьте минуту или две и выполните одно или два

1327
00:55:47,900 --> 00:55:51,200
таких обновления, а затем подумайте о том,

1328
00:55:51,200 --> 00:55:52,730
что произойдет, если мы изменим

1329
00:55:52,730 --> 00:55:56,360
порядок данных на обратный с той же скоростью обучения, так что

1330
00:55:56,360 --> 00:55:57,590
это действительно то, о чем мы

1331
00:55:57,590 --> 00:55:59,030
говорили пару раз  независимо

1332
00:55:59,030 --> 00:56:01,670
от того, имеет ли значение порядок обновлений, которые мы делаем, учитывая

1333
00:56:01,670 --> 00:56:03,740
некоторый набор данных, с точки зрения

1334
00:56:03,740 --> 00:56:06,890
значений, которые мы вычисляем, поэтому я думаю, что я бы пошел

1335
00:56:06,890 --> 00:56:08,510
по этому пути следующим образом: я бы

1336
00:56:08,510 --> 00:56:10,370
сначала убедил себя в любом случае, имеет

1337
00:56:10,370 --> 00:56:12,020
ли порядок значение с

1338
00:56:12,020 --> 00:56:13,070
точки зрения  значения, которые мы собираемся

1339
00:56:13,070 --> 00:56:16,160
вычислить, а затем попробуем вычислить одно или

1340
00:56:16,160 --> 00:56:21,020
два из них, поэтому давайте просто потратим

1341
00:56:21,020 --> 00:56:24,350
минуту или две, чтобы решить, имеет ли

1342
00:56:24,350 --> 00:56:28,340
значение порядок здесь с точки зрения

1343
00:56:28,340 --> 00:56:32,210
результирующих значений, а затем мы также можем

1344
00:56:32,210 --> 00:56:34,690
вычислить,

1345
00:57:44,310 --> 00:57:46,360
и я  знаю, что я не даю вам, ребята,

1346
00:57:46,360 --> 00:57:47,440
достаточно ti  мне делать все вычисления

1347
00:57:47,440 --> 00:57:49,450
здесь, но это в основном

1348
00:57:49,450 --> 00:57:51,160
просто вынужденный аспект вспоминания, пытаясь

1349
00:57:51,160 --> 00:57:52,510
вспомнить, что именно представляют собой формулы,

1350
00:57:52,510 --> 00:57:54,490
и вспомнить, имеет ли это значение,

1351
00:57:54,490 --> 00:57:56,440
поэтому я просто попрошу вас проголосовать за то, кто

1352
00:57:56,440 --> 00:57:57,970
здесь думает, что  порядок имеет значение с

1353
00:57:57,970 --> 00:57:59,700
точки зрения некоторых значений, которые мы вычисляем

1354
00:57:59,700 --> 00:58:02,020
, это правильно нет, это не всегда будет

1355
00:58:02,020 --> 00:58:03,820
иногда делать, вы можете делать что-то в

1356
00:58:03,820 --> 00:58:05,440
другом порядке, поскольку мы

1357
00:58:05,440 --> 00:58:06,730
подчеркивали, что многое может привести вас к

1358
00:58:06,730 --> 00:58:08,050
мысли, что это всегда имеет значение, это

1359
00:58:08,050 --> 00:58:09,640
не так.  не всегда имеет значение, но в данном случае

1360
00:58:09,640 --> 00:58:13,030
это имеет значение. В данном случае, если мы посмотрим,

1361
00:58:13,030 --> 00:58:15,010
какое значение имеет значение в первом порядке,

1362
00:58:15,010 --> 00:58:16,930
мы бы сказали, что B of C

1363
00:58:16,930 --> 00:58:20,110
равно 0 плюс альфа 1 минус 0, поэтому

1364
00:58:20,110 --> 00:58:22,960
новый  Награда Уиверс заметила, что это

1365
00:58:22,960 --> 00:58:25,660
будет альфа, тогда, когда мы вычисляем

1366
00:58:25,660 --> 00:58:28,120
значение b, мы могли бы использовать новое b для c, которое

1367
00:58:28,120 --> 00:58:30,040
мы только что вычислили, потому что, когда у нас есть

1368
00:58:30,040 --> 00:58:32,200
это обновление сейчас, мы уже получили

1369
00:58:32,200 --> 00:58:36,340
ненулевую оценку для B is C note to be

1370
00:58:36,340 --> 00:58:37,750
точно я должен был сказать вам здесь

1371
00:58:37,750 --> 00:58:39,400
, как именно мы инициализируем все

1372
00:58:39,400 --> 00:58:41,740
значения, так что в этом случае мы неявно

1373
00:58:41,740 --> 00:58:46,650
предположили, что начальные значения равны 0,

1374
00:58:46,650 --> 00:58:50,800
это очень важно, мы еще поговорим

1375
00:58:50,800 --> 00:58:54,190
через неделю или две о более разумном

1376
00:58:54,190 --> 00:58:55,600
исследовании и о том, что

1377
00:58:55,600 --> 00:58:57,370
оптимизм часто действительно очень полезен,

1378
00:58:57,370 --> 00:58:59,230
одна проблема может быть  в глубоких нейронных

1379
00:58:59,230 --> 00:59:00,940
сетях, как настроить вещи так, чтобы

1380
00:59:00,940 --> 00:59:04,000
они были оптимистичными, но в этом случае

1381
00:59:04,000 --> 00:59:05,200
мы предполагаем, что все равно нулю,

1382
00:59:05,200 --> 00:59:07,300
поэтому через B будет альфа-квадрат B of C

1383
00:59:07,300 --> 00:59:09,790
будет следующим выражением, это

1384
00:59:09,790 --> 00:59:12,070
в основном я просто применяю TV

1385
00:59:12,070 --> 00:59:16,480
изучая эти случаи

1386
00:59:16,480 --> 00:59:25,960
во второй строке, да, хороший улов в

1387
00:59:25,960 --> 00:59:34,060
том, что мы уходим от кальмара, да, да,

1388
00:59:34,060 --> 00:59:37,260
это последнее выражение. Спасибо,

1389
00:59:37,380 --> 00:59:41,950
поэтому, которое снова появляется в третьей строке, если

1390
00:59:41,950 --> 00:59:45,040
мы делаем это в обратном порядке.

1391
00:59:45,040 --> 00:59:46,770
update,

1392
00:59:46,770 --> 00:59:50,290
потому что C переходит в B B при V из B равно нулю,

1393
00:59:50,290 --> 00:59:53,589
тогда, когда мы обновляем BC до нуля, значение

1394
00:59:53,589 --> 00:59:55,869
C по-прежнему равно нулю, и мы только обновляем V из

1395
00:59:55,869 --> 00:59:59,290
C окончательным способом, так что это просто

1396
00:59:59,290 --> 01:00:00,550
указывает на то, что порядок имеет значение, это возникает

1397
01:00:00,550 --> 01:00:01,869
также, когда мы  выполняешь функцию

1398
01:00:01,869 --> 01:00:07,030
аппроксимации  и эпизодическое воспроизведение просто

1399
01:00:07,030 --> 01:00:08,260
в целом мы думаем об

1400
01:00:08,260 --> 01:00:09,670
алгоритмах оценки политики полезно

1401
01:00:09,670 --> 01:00:11,650
знать компромисс между смещением и дисперсией

1402
01:00:11,650 --> 01:00:13,150
эффективность данных и вычислительная эффективность

1403
01:00:13,150 --> 01:00:14,890
обучение TD, как правило, довольно хорошо

1404
01:00:14,890 --> 01:00:17,859
влияет на вычислительную эффективность эффективность данных,

1405
01:00:17,859 --> 01:00:19,810
иногда это немного зависит от того,

1406
01:00:19,810 --> 01:00:22,450
если вы это сделаете  опыт воспроизведения с TD

1407
01:00:22,450 --> 01:00:26,410
становится лучше, поэтому полезно подумать

1408
01:00:26,410 --> 01:00:28,510
о том, что часто существует множество вариантов

1409
01:00:28,510 --> 01:00:30,190
этих алгоритмов, поэтому просто будьте

1410
01:00:30,190 --> 01:00:32,050
точны и независимо от того, что

1411
01:00:32,050 --> 01:00:33,670
вы заявляете, если вы просто предполагаете

1412
01:00:33,670 --> 01:00:34,990
ванильную версию, которую мы используем, или если вы

1413
01:00:34,990 --> 01:00:36,280
хорошо, если вы

1414
01:00:36,280 --> 01:00:37,839
воспроизведете этот дополнительный опыт, вот как это может

1415
01:00:37,839 --> 01:00:41,109
измениться, хорошо, а

1416
01:00:41,109 --> 01:00:42,220
теперь давайте подумаем о том, как мы можем использовать

1417
01:00:42,220 --> 01:00:43,930
модель свободного обучения, чтобы принимать правильные

1418
01:00:43,930 --> 01:00:46,750
решения, мы много говорили о Q-

1419
01:00:46,750 --> 01:00:48,430
обучении Q-обучение - это метод начальной загрузки

1420
01:00:48,430 --> 01:00:50,980
, который предполагает марковскую  В

1421
01:00:50,980 --> 01:00:54,339
марковском мире мы говорим, что

1422
01:00:54,339 --> 01:00:57,280
значение Q будет

1423
01:00:57,280 --> 01:00:59,530
аппроксимировано вознаграждением плюс гамма, умноженное на максимальное число

1424
01:00:59,530 --> 01:01:02,260
по простому числу со следующим Q f  и мы

1425
01:01:02,260 --> 01:01:03,849
можем использовать это как своего рода цель, а

1426
01:01:03,849 --> 01:01:05,740
затем мы делаем медленное замедление, у нас как

1427
01:01:05,740 --> 01:01:07,540
бы есть эта обновленная скорость обучения, это

1428
01:01:07,540 --> 01:01:12,430
не скорость обучения, когда мы замедляемся

1429
01:01:12,430 --> 01:01:14,380
между одной выборкой, которую мы только что видели, по

1430
01:01:14,380 --> 01:01:17,650
сравнению с нашей предыдущей оценкой, когда  мы

1431
01:01:17,650 --> 01:01:20,500
медленно повернули это к тому, что мы обычно

1432
01:01:20,500 --> 01:01:22,480
уменьшаем альфу с течением времени, чтобы попытаться приблизить

1433
01:01:22,480 --> 01:01:26,609
Q к одному значению,

1434
01:01:27,920 --> 01:01:29,330
мы говорили о некоторых условиях, при

1435
01:01:29,330 --> 01:01:31,280
которых q-обучение снова сходится

1436
01:01:31,280 --> 01:01:33,290


1437
01:01:33,290 --> 01:01:34,880


1438
01:01:34,880 --> 01:01:36,140
также или прямо сейчас мы говорим

1439
01:01:36,140 --> 01:01:39,560
о табличной настройке, поэтому

1440
01:01:39,560 --> 01:01:41,720
аппроксимация функции не происходит, поэтому, если

1441
01:01:41,720 --> 01:01:43,160
вы действуете случайным образом, q-обучение будет

1442
01:01:43,160 --> 01:01:45,140
сходиться к звезде Q при

1443
01:01:45,140 --> 01:01:48,500
предположениях о слабой досягаемости, что означает, что вы

1444
01:01:48,500 --> 01:01:49,700
знаете, что у вас не может быть вертолета, который

1445
01:01:49,700 --> 01:01:51,050
если вы сломаете его, мир закончится, и

1446
01:01:51,050 --> 01:01:52,460
вы не сможете получить больше образцов, поэтому вы

1447
01:01:52,460 --> 01:01:53,690
должны иметь возможность многократно

1448
01:01:53,690 --> 01:01:55,280
посещать все состояния определенное

1449
01:01:55,280 --> 01:01:57,200
количество раз и пробовать все действия

1450
01:01:57,200 --> 01:02:00,980
бесконечное количество раз.  f раз, и у него есть

1451
01:02:00,980 --> 01:02:02,570
интересное свойство: когда

1452
01:02:02,570 --> 01:02:04,040
вы изучаете Q, вы можете использовать данные,

1453
01:02:04,040 --> 01:02:06,050
собранные одной политикой, для оценки

1454
01:02:06,050 --> 01:02:08,180
значения другой политики, поэтому мы

1455
01:02:08,180 --> 01:02:09,860
пытаемся оценить оптимальную функцию Q,

1456
01:02:09,860 --> 01:02:11,660
но мы можем использовать, например

1457
01:02:11,660 --> 01:02:14,420
случайные данные случайные выборки случайной

1458
01:02:14,420 --> 01:02:16,820
политики, чтобы попытаться оценить это, и

1459
01:02:16,820 --> 01:02:18,200
причина этого в том, что мы делаем

1460
01:02:18,200 --> 01:02:20,390
этот максимум, мы всегда смотрим,

1461
01:02:20,390 --> 01:02:23,360
что лучшее, что мы могли бы сделать дальше, так

1462
01:02:23,360 --> 01:02:27,920
что это довольно крутое свойство, поэтому, если

1463
01:02:27,920 --> 01:02:29,770
мы сортируем  в этом случае

1464
01:02:29,770 --> 01:02:32,330
есть несколько разных вещей, которые мы рассмотрим,

1465
01:02:32,330 --> 01:02:36,290
я думаю, вкратце, если у

1466
01:02:36,290 --> 01:02:40,160
вас есть политика обучения Q, в которой есть

1467
01:02:40,160 --> 01:02:42,790
жадность, будьте жадными, здесь с

1468
01:02:42,790 --> 01:02:45,560
вероятностью один минус эпсилон вы

1469
01:02:45,560 --> 01:02:47,300
предпримете действие, которое, как ожидается,  быть лучшим в

1470
01:02:47,300 --> 01:02:49,760
соответствии с вашей текущей функцией Q, и с

1471
01:02:49,760 --> 01:02:52,670
вероятностью эпсилон вы действуете случайным образом, поэтому,

1472
01:02:52,670 --> 01:02:55,040
если вы находитесь в таблице поиска, это

1473
01:02:55,040 --> 01:02:56,900
гарантированно сходится к оптимальной

1474
01:02:56,900 --> 01:02:59,960
политике в пределе бесконечных данных, так

1475
01:02:59,960 --> 01:03:10,430
что это да, с небольшой досягаемостью для

1476
01:03:10,430 --> 01:03:12,350
это второй, мы можем использовать оценку Монте-Карло

1477
01:03:12,350 --> 01:03:14,540
в mdps с большими пространствами состояний,

1478
01:03:14,540 --> 01:03:20,230
давайте проголосуем за

1479
01:03:21,260 --> 01:03:23,220
то, что все возьмут секунду, затем просто

1480
01:03:23,220 --> 01:03:24,420
поговорите со своим соседом, тогда мы снова проголосуем

1481
01:03:24,420 --> 01:03:27,960
Я не говорю, что эти люди

1482
01:03:27,960 --> 01:03:28,920
не правы, я просто  говоря, что, поскольку

1483
01:03:28,920 --> 01:03:30,120
большинство людей не голосовало, я предполагаю,

1484
01:03:30,120 --> 01:03:32,160
что большинству людей было бы полезно просто

1485
01:03:32,160 --> 01:04:22,650
подумать на секунду, хорошо, давайте

1486
01:04:22,650 --> 01:04:25,890
проголосуем еще раз, я голосую, если вы думаете, что оценку Монте-

1487
01:04:25,890 --> 01:04:28,290
Карло можно использовать в mdps

1488
01:04:28,290 --> 01:04:31,920
с большими пространствами состояний да да так  это

1489
01:04:31,920 --> 01:04:33,570
не это не вы не ограничены тем, является

1490
01:04:33,570 --> 01:04:35,010
ли это большим пространством состояний или нет,

1491
01:04:35,010 --> 01:04:36,720
вы можете использовать много оценок цвета там

1492
01:04:36,720 --> 01:04:43,730
я, что это так, я думаю, что это можно, поэтому

1493
01:04:43,730 --> 01:04:47,870
нельзя использовать Монте-Карло

1494
01:04:51,710 --> 01:04:52,980
да

1495
01:04:52,980 --> 01:04:57,090
отличный вопрос, так что я Монте-Карло, если вы

1496
01:04:57,090 --> 01:04:58,410
там  число точек данных на

1497
01:04:58,410 --> 01:04:59,820
состояние может быть очень низким, если у вас есть

1498
01:04:59,820 --> 01:05:01,470
одно начальное состояние, что не так уж плохо, если у

1499
01:05:01,470 --> 01:05:03,300
вас есть распределение начальных состояний,

1500
01:05:03,300 --> 01:05:04,920
может быть сложнее, или мы собираемся

1501
01:05:04,920 --> 01:05:05,880
перейти к настройке приближения функции значения,

1502
01:05:05,880 --> 01:05:07,590
но есть

1503
01:05:07,590 --> 01:05:09,180
север  ing a priori, что означает, что вы не можете

1504
01:05:09,180 --> 01:05:11,100
применить его так, как вы вставили, это может

1505
01:05:11,100 --> 01:05:13,860
быть очень плохо, вам может понадобиться начать

1506
01:05:13,860 --> 01:05:17,310
делать аппроксимацию функции последнее,

1507
01:05:17,310 --> 01:05:18,750
что я добавил, это то,

1508
01:05:18,750 --> 01:05:22,260
что мы еще не обсуждали, я

1509
01:05:22,260 --> 01:05:23,610
думаю  интересно начать

1510
01:05:23,610 --> 01:05:25,110
с того, чтобы начать связывать

1511
01:05:25,110 --> 01:05:26,430
их между аспектами динамического программирования, о которых

1512
01:05:26,430 --> 01:05:29,970
мы говорили,

1513
01:05:29,970 --> 01:05:31,530
обучение с подкреплением на основе моделей не

1514
01:05:31,530 --> 01:05:33,930
обязательно всегда более эффективно с точки зрения данных,

1515
01:05:33,930 --> 01:05:34,250
чем

1516
01:05:34,250 --> 01:05:36,140
все бесплатное, хотя мы говорили в основном о

1517
01:05:36,140 --> 01:05:38,420
бесплатном модели, и поэтому мы  Я не обсуждал

1518
01:05:38,420 --> 01:05:40,040
это слишком много, но

1519
01:05:40,040 --> 01:05:41,030
об этом стоит подумать, особенно когда мы

1520
01:05:41,030 --> 01:05:44,450
начинаем исследовать, и я

1521
01:05:44,450 --> 01:05:45,770
кратко упомянул ранее, что есть

1522
01:05:45,770 --> 01:05:47,870
хорошая новая статья, написанная Саном и некоторыми

1523
01:05:47,870 --> 01:05:50,810
коллегами из MSR Microsoft Research New

1524
01:05:50,810 --> 01:05:52,850
York City, которая показывает  что в некоторых

1525
01:05:52,850 --> 01:05:56,120
случаях модель на основе строго лучше,

1526
01:05:56,120 --> 01:05:58,760
чем модель без модели, и интуитивно понятно

1527
01:05:58,760 --> 01:06:00,440
, что вы не можете компактно

1528
01:06:00,440 --> 01:06:01,850
представить модель, но вы не можете компактно

1529
01:06:01,850 --> 01:06:03,590
представить ее.  функции значения, поэтому вам

1530
01:06:03,590 --> 01:06:05,090
не нужно много параметров для

1531
01:06:05,090 --> 01:06:06,590
изучения модели, и затем вы можете планировать с ней,

1532
01:06:06,590 --> 01:06:07,940
но если вы попытаетесь напрямую изучить

1533
01:06:07,940 --> 01:06:12,200
функцию значения, вам нужно гораздо больше,

1534
01:06:12,200 --> 01:06:13,910
так что, поскольку мы как бы начинаем

1535
01:06:13,910 --> 01:06:15,350
двигаться дальше, и даже только в этом

1536
01:06:15,350 --> 01:06:17,420
обсуждении в последнее время мы много

1537
01:06:17,420 --> 01:06:19,460
внимания уделяли аппроксимации функции ценности, которую я

1538
01:06:19,460 --> 01:06:23,210
включаю в домашнюю работу 2, поэтому мы говорили

1539
01:06:23,210 --> 01:06:25,190
о том, рассматривали ли вы методы Монте-Карло по

1540
01:06:25,190 --> 01:06:27,590
сравнению с TD, узнавая, какие

1541
01:06:27,590 --> 01:06:29,540
гарантии сходимости у нас есть

1542
01:06:29,540 --> 01:06:35,060
в случае политики, поэтому это важно

1543
01:06:35,060 --> 01:06:37,820
подчеркнуть, поэтому мы рассматриваем

1544
01:06:37,820 --> 01:06:40,160
оценку ценности отдельной политики, и мы

1545
01:06:40,160 --> 01:06:41,330
говорили о том, как мы можем думать

1546
01:06:41,330 --> 01:06:43,460
о стационарном распределении политики

1547
01:06:43,460 --> 01:06:46,130
, когда мы определяем единую политику, а затем

1548
01:06:46,130 --> 01:06:48,170
запускаем ее.  это похоже на то, что мы

1549
01:06:48,170 --> 01:06:50,060
вызываем марковский процесс вознаграждения или цепь Маркова,

1550
01:06:50,060 --> 01:06:51,800
и мы думаем, что можем подумать о

1551
01:06:51,800 --> 01:06:53,360
стационарном распределении состояний, которые

1552
01:06:53,360 --> 01:06:56,390
мы посетили бы при этой политике, и мы

1553
01:06:56,390 --> 01:06:58,760
говорили о свойствах сходимости и,

1554
01:06:58,760 --> 01:07:00,230
в частности, мы  сказал, что метод Монте-

1555
01:07:00,230 --> 01:07:03,590
Карло, независимо от того, какой тип

1556
01:07:03,590 --> 01:07:05,240
аппроксиматора функции вы используете, заключается в том, что он

1557
01:07:05,240 --> 01:07:07,600
пытается минимизировать среднеквадратичную ошибку,

1558
01:07:07,600 --> 01:07:10,040
поэтому стиль методов, о которых мы

1559
01:07:10,040 --> 01:07:13,100
говорили с Монте-Карло, заключается в том, что он просто

1560
01:07:13,100 --> 01:07:14,780
пытается минимизировать среднеквадратичную ошибку

1561
01:07:14,780 --> 01:07:20,210
ваших данных, и поэтому мы могли бы подумать об

1562
01:07:20,210 --> 01:07:21,440
этом для аппроксимации линейной функции ценности,

1563
01:07:21,440 --> 01:07:23,780
это также справедливо для другой

1564
01:07:23,780 --> 01:07:25,040
аппроксимации функции

1565
01:07:25,040 --> 01:07:28,370
ценности, которая будет минимизирована там

1566
01:07:28,370 --> 01:07:30,620
в случае аппроксимации линейной функции ценности,

1567
01:07:30,620 --> 01:07:32,840
когда Т. Д. Лернер узнает,

1568
01:07:32,840 --> 01:07:34,640
что я могу сходиться к постоянному вектору

1569
01:07:34,640 --> 01:07:37,250
из наилучшей среднеквадратичной ошибки теперь, что

1570
01:07:37,250 --> 01:07:40,550
это означает в данном случае, что у нас есть, так это то, что у

1571
01:07:40,550 --> 01:07:46,040
нас может быть разрыв, поэтому,

1572
01:07:46,040 --> 01:07:46,910
особенно если у вас есть что-то вроде

1573
01:07:46,910 --> 01:07:48,040
функции линейного значения R

1574
01:07:48,040 --> 01:07:50,260
mater, вы просто не сможете

1575
01:07:50,260 --> 01:07:52,450
написать, чтобы точно представить значение  из

1576
01:07:52,450 --> 01:07:56,290
всех состояний используйте выбранное

1577
01:07:56,290 --> 01:07:59,320
семейство параметрических параметров, которое у вас есть, и поэтому

1578
01:07:59,320 --> 01:08:01,000
в принципе может быть просто разрыв

1579
01:08:01,000 --> 01:08:04,090
между функцией значения, которая

1580
01:08:04,090 --> 01:08:05,980
представляет  в состоянии с пространством, которое у вас

1581
01:08:05,980 --> 01:08:08,860
есть, и функцией истинного значения, о которой я

1582
01:08:08,860 --> 01:08:10,390
хотел бы подумать, вот так, есть

1583
01:08:10,390 --> 01:08:11,980
хорошая картинка внутри, частично знаю об

1584
01:08:11,980 --> 01:08:14,350
этом, это своего рода демонстрация с

1585
01:08:14,350 --> 01:08:16,390
вашим набором w, какие функции значения

1586
01:08:16,390 --> 01:08:18,490
вы можете представить и  может

1587
01:08:18,490 --> 01:08:19,839
случиться так, что ваша функция переменного значения

1588
01:08:19,839 --> 01:08:23,439
живет здесь, вы просто не можете,

1589
01:08:23,439 --> 01:08:24,760
например, с помощью линии, чтобы представить

1590
01:08:24,760 --> 01:08:27,609
все функции истинного значения, если вы

1591
01:08:27,609 --> 01:08:29,529
думаете об этом в DeMint, и другое

1592
01:08:29,529 --> 01:08:30,880
измерение, которое вы можете представить для состояния,

1593
01:08:30,880 --> 01:08:32,290
может быть  ваша функция реального значения выглядит

1594
01:08:32,290 --> 01:08:34,149
примерно так, но вы использовали

1595
01:08:34,149 --> 01:08:35,170
линейный аппроксиматор,

1596
01:08:35,170 --> 01:08:37,299
поэтому вы просто не можете точно представить, что

1597
01:08:37,299 --> 01:08:43,029
да, прямая линия, поэтому Монте-Карло

1598
01:08:43,029 --> 01:08:44,380
сходится к наилучшей возможной среднеквадратической ошибке,

1599
01:08:44,380 --> 01:08:46,060
учитывая вашу функцию значений

1600
01:08:46,060 --> 01:08:48,100
паркового пространства и TD Lerner

1601
01:08:48,100 --> 01:08:50,410
сходится к тому времени его дополнительный

1602
01:08:50,410 --> 01:09:00,430
фактор я думаю, что это не совсем так

1603
01:09:00,430 --> 01:09:05,520
хорошо, так что обратите внимание, что есть это

1604
01:09:13,499 --> 01:09:17,639
хорошо нет, я просто собираюсь сделать это не

1605
01:09:17,639 --> 01:09:28,009
иметь дела никуда действовать

1606
01:09:37,979 --> 01:09:40,029
мы говорили о е  действовать так, что когда

1607
01:09:40,029 --> 01:09:41,319
вы проводите обучение вне политики,

1608
01:09:41,319 --> 01:09:42,639
q-обучение с аппроксимацией функций

1609
01:09:42,639 --> 01:09:44,618
может расходиться, что означает, что оно даже не

1610
01:09:44,618 --> 01:09:46,000
сходится с бесконечными объемами

1611
01:09:46,000 --> 01:09:50,529
данных, это даже отдельно от того, к чему оно

1612
01:09:50,529 --> 01:09:52,389
могло бы сходиться, если оно сходится, это

1613
01:09:52,389 --> 01:09:53,618
просто говорит, что фактическое  ваши

1614
01:09:53,618 --> 01:09:55,139
параметры просто никогда не перестанут меняться,

1615
01:09:55,139 --> 01:09:59,489
если вы выполняете своего рода оценочные обновления

1616
01:10:07,560 --> 01:10:09,550
ограничений,

1617
01:10:09,550 --> 01:10:11,199
помогает ли инициализация параметров

1618
01:10:11,199 --> 01:10:12,580
определить, например, вы

1619
01:10:12,580 --> 01:10:14,650
можете сходиться или расходиться, это

1620
01:10:14,650 --> 01:10:15,699
интересный вопрос, я не думаю, что

1621
01:10:15,699 --> 01:10:17,619
есть работа, которую я  знаете, если он формально

1622
01:10:17,619 --> 01:10:18,940
пытается охарактеризовать это, как вы знаете

1623
01:10:18,940 --> 01:10:20,380
, есть ли места, где вы могли бы

1624
01:10:20,380 --> 01:10:25,090
формально сделать это, чтобы, например, с точки зрения

1625
01:10:25,090 --> 01:10:26,199
ваших ингредиентов, они

1626
01:10:26,199 --> 01:10:28,060
не начали бы взрываться, может быть,

1627
01:10:28,060 --> 01:10:29,380
я подозреваю, что это во многом зависит от

1628
01:10:29,380 --> 01:10:31,030
проблемы, и я  также подозреваю, что

1629
01:10:31,030 --> 01:10:32,469
могут быть патологические примеры, которые вы можете

1630
01:10:32,469 --> 01:10:34,000
построить там, где это трудно сделать, но,

1631
01:10:34,000 --> 01:10:35,830
безусловно, стоит попробовать, вы также можете

1632
01:10:35,830 --> 01:10:38,139
наблюдать, соответствует ли ваш параметр

1633
01:10:38,139 --> 01:10:42,790
timates продолжают меняться, мы

1634
01:10:42,790 --> 01:10:43,900
довольно много говорили о вас, ребята, у вас было много

1635
01:10:43,900 --> 01:10:45,219
практики с глубоким обучением, моделью

1636
01:10:45,219 --> 01:10:47,050
причудливого обучения, гм, где мы посмотрели на то,

1637
01:10:47,050 --> 01:10:48,520
что эта цель обучения подсказки находится в сети Q,

1638
01:10:48,520 --> 01:10:49,630
и мы делаем стохастический

1639
01:10:49,630 --> 01:10:52,030
градиентный спуск, мы'  при использовании глубокой

1640
01:10:52,030 --> 01:10:55,690
нейронной сети для аппроксимации Q мы

1641
01:10:55,690 --> 01:10:56,679
говорили о том, что некоторые проблемы

1642
01:10:56,679 --> 01:10:58,420
с расхождением могут заключаться в

1643
01:10:58,420 --> 01:10:59,949
том, что у нас есть такие коррелированные

1644
01:10:59,949 --> 01:11:01,840
локальные обновления, значение состояния

1645
01:11:01,840 --> 01:11:03,550
часто очень тесно связано со

1646
01:11:03,550 --> 01:11:07,090
значением его следующего преемника  состояние, а

1647
01:11:07,090 --> 01:11:09,119
также частое изменение этих целей

1648
01:11:09,119 --> 01:11:11,670
может вызвать

1649
01:11:11,670 --> 01:11:14,590
нестабильность, так что

1650
01:11:14,590 --> 01:11:16,060
недавний прогресс примерно за последние

1651
01:11:16,060 --> 01:11:18,250
пять лет был своего рода способами

1652
01:11:18,250 --> 01:11:20,080
изменить это уравнение, чтобы сделать его

1653
01:11:20,080 --> 01:11:21,550
более стабильным, когда вы  повторно делаем градиентный

1654
01:11:21,550 --> 01:11:26,320
спуск, и в dqn как бы

1655
01:11:26,320 --> 01:11:27,880
вводится, что мы должны испытать

1656
01:11:27,880 --> 01:11:29,619
повтор, поэтому не используйте каждую точку данных один раз,

1657
01:11:29,619 --> 01:11:32,530
а также фиксируйте цель на некоторое время, так

1658
01:11:32,530 --> 01:11:33,760
что вы как бы говорите  g Я собираюсь

1659
01:11:33,760 --> 01:11:35,560
некоторое время использовать этот аппроксиматор функции с фиксированным значением для

1660
01:11:35,560 --> 01:11:37,630
моего следующего состояния, а затем мы

1661
01:11:37,630 --> 01:11:41,440
сможем минимизировать нашу среднеквадратичную ошибку, мы

1662
01:11:41,440 --> 01:11:42,520
говорили о том факте, что опытный

1663
01:11:42,520 --> 01:11:43,929
повтор особенно чрезвычайно полезен,

1664
01:11:43,929 --> 01:11:46,780
а цели также тихие,

1665
01:11:46,780 --> 01:11:49,930
и их нет.  Пока еще нет хороших гарантий

1666
01:11:49,930 --> 01:11:52,300
конвергенции, хотя в этой области проделано много

1667
01:11:52,300 --> 01:11:53,710
интересной работы.

1668
01:11:53,710 --> 01:11:55,330


1669
01:11:55,330 --> 01:11:56,380


1670
01:11:56,380 --> 01:11:59,230


1671
01:11:59,230 --> 01:12:02,100


1672
01:12:02,100 --> 01:12:05,380


1673
01:12:05,380 --> 01:12:07,660
мы могли бы попытаться улучшить

1674
01:12:07,660 --> 01:12:09,550
скорость сходимости наших ключевых функций к

1675
01:12:09,550 --> 01:12:14,860
чему-то разумному, поэтому я думаю, что

1676
01:12:14,860 --> 01:12:17,500
это последний вопрос на сегодня, так что быстрый вопрос

1677
01:12:17,500 --> 01:12:19,300
Я имею в виду конечные пространства состояний с функциями,

1678
01:12:19,300 --> 01:12:20,530
которые могут представлять функцию истинного значения.

1679
01:12:20,530 --> 01:12:22,900
Обучает ли TD обучение с помощью функции значения.

1680
01:12:22,900 --> 01:12:24,610
аппроксимация всегда находит

1681
01:12:24,610 --> 01:12:26,800
функцию истинного значения политики при наличии

1682
01:12:26,800 --> 01:12:31,960
достаточного количества данных, так что это для обучения TD,

1683
01:12:31,960 --> 01:12:34,840
поэтому мы, по сути, делаем

1684
01:12:34,840 --> 01:12:37,780
poli  cy оценка прямо сейчас, так что в этом

1685
01:12:37,780 --> 01:12:40,840
случае мы гарантированно найдем функцию истинного

1686
01:12:40,840 --> 01:12:44,700
значения, учитывая достаточное количество данных,

1687
01:12:46,530 --> 01:12:49,390
может быть, поболтаем с соседом в

1688
01:12:49,390 --> 01:12:50,320
течение одной минуты, а затем я попрошу

1689
01:12:50,320 --> 01:12:59,110
людей проголосовать за политику или, по крайней мере,

1690
01:12:59,110 --> 01:13:00,730
с достаточным количеством данных  из

1691
01:13:00,730 --> 01:13:03,780
распределения по политике

1692
01:13:10,220 --> 01:13:13,339
[Музыка

1693
01:13:27,019 --> 01:13:29,880
] хорошо, кто хочет голосовать, да, мы

1694
01:13:29,880 --> 01:13:31,380
действительно находим аппроксиматор функции истинного значения,

1695
01:13:31,380 --> 01:13:34,829
хорошо, хорошо, так как мы

1696
01:13:34,829 --> 01:13:36,300
могли это проверить, так что, если мы вернемся к

1697
01:13:36,300 --> 01:13:37,380
тому, что я

1698
01:13:37,380 --> 01:13:40,829
говорил здесь, я сказал, что  мы

1699
01:13:40,829 --> 01:13:42,300
собираемся сходиться к постоянному

1700
01:13:42,300 --> 01:13:44,579
коэффициенту наилучшей среднеквадратичной ошибки. эта

1701
01:13:44,579 --> 01:13:46,320
среднеквадратическая ошибка всегда равна нулю, если вы можете

1702
01:13:46,320 --> 01:13:49,409
точно представить значение в

1703
01:13:49,409 --> 01:13:54,360
текущем пространстве, так что дополнительный вид

1704
01:13:54,360 --> 01:13:55,769
постоянного коэффициента представляет собой просто постоянный

1705
01:13:55,769 --> 01:14:01,579
коэффициент, умноженный на ноль, поэтому в  в этом случае да,

1706
01:14:01,999 --> 01:14:05,579
так что я, потому что я сказал здесь, что он с

1707
01:14:05,579 --> 01:14:07,079
функциями, которые могут представлять функцию истинного

1708
01:14:07,079 --> 01:14:10,230
значения, поэтому мы сказали, что

1709
01:14:10,230 --> 01:14:11,789
вполне возможно представить

1710
01:14:11,789 --> 01:14:13,380
функцию значения этой политики в

1711
01:14:13,380 --> 01:14:15,329
функциях, которые я  d, данное вам, и поэтому

1712
01:14:15,329 --> 01:14:19,139
можно будет достичь этого да, так что верно

1713
01:14:19,139 --> 01:14:21,960
для нелинейной амортизации для обучения tt,

1714
01:14:21,960 --> 01:14:23,099
да,

1715
01:14:23,099 --> 01:14:26,449
так что для оценки политики, если у вас есть

1716
01:14:26,449 --> 01:14:28,769
нелинейная, если у вас есть функции, например, если у

1717
01:14:28,769 --> 01:14:30,719
вас есть общее представление, которое

1718
01:14:30,719 --> 01:14:31,980
позволяет вам точно  представляют

1719
01:14:31,980 --> 01:14:33,090
значение функции, и вы

1720
01:14:33,090 --> 01:14:35,039
изучаете политику, поэтому вы изучаете TD,

1721
01:14:35,039 --> 01:14:38,340
вы сможете получить 0 с

1722
01:14:38,340 --> 01:14:40,110
бесконечным, вы знаете достаточно данных

1723
01:14:40,110 --> 01:14:43,789
и так далее, конечные данные, это все да,

1724
01:14:45,370 --> 01:14:47,620
эй, я немного запутался, учитывая, что

1725
01:14:47,620 --> 01:14:50,530
они  могут иметь все функции, которые мы

1726
01:14:50,530 --> 01:14:53,730
хотим, но у нас может не быть никакого

1727
01:14:53,730 --> 01:14:55,810


1728
01:14:55,810 --> 01:14:57,700
приближения репрезентативной функции ценности, которое действительно

1729
01:14:57,700 --> 01:15:00,730
могло бы создать симпатичную картинку, делают

1730
01:15:00,730 --> 01:15:04,870
две вещи, как одинаковые дамы, я думаю,

1731
01:15:04,870 --> 01:15:06,310
что то, как я думал об этом, могло

1732
01:15:06,310 --> 01:15:07,780
бы иметь все  функции, но мы

1733
01:15:07,780 --> 01:15:09,160
можем не найти пространства функций,

1734
01:15:09,160 --> 01:15:11,500
которые на самом деле были бы в состоянии представить

1735
01:15:11,500 --> 01:15:14,020
функцию значения, хорошо, поэтому я думаю, что

1736
01:15:14,020 --> 01:15:15,970
вопросы говорят: хорошо, хорошо, что, если бы у нас было

1737
01:15:15,970 --> 01:15:17,680
много функций, но я  Например, это

1738
01:15:17,680 --> 01:15:19,390
действительно дает нам параметризацию

1739
01:15:19,390 --> 01:15:20,800
функции значения, которая может

1740
01:15:20,800 --> 01:15:23,170
представлять вопрос истинного значения.

1741
01:15:23,170 --> 01:15:25,240


1742
01:15:25,240 --> 01:15:26,830


1743
01:15:26,830 --> 01:15:28,870


1744
01:15:28,870 --> 01:15:30,610
алгоритм,

1745
01:15:30,610 --> 01:15:33,760
чтобы попытаться хорошо его подогнать, поэтому я

1746
01:15:33,760 --> 01:15:34,870
предполагаю, что я говорю в этом случае

1747
01:15:34,870 --> 01:15:36,610
, что если ваша функция значения, если бы

1748
01:15:36,610 --> 01:15:38,500
кто-то мог, если бы Oracle мог дать

1749
01:15:38,500 --> 01:15:41,740
вам эти функции, вектор параметров

1750
01:15:41,740 --> 01:15:45,280
, который сделал бы этот ноль, что  TD-

1751
01:15:45,280 --> 01:15:48,000
обучение может найти его,

1752
01:15:48,000 --> 01:15:50,950
потому что мы можем генерировать табличный кубит, который

1753
01:15:50,950 --> 01:15:53,560
вам не обязательно должен быть табличным, поэтому для перехода на

1754
01:15:53,560 --> 01:15:55,360
самом деле не нужно удерживать только

1755
01:15:55,360 --> 01:15:57,700
табличные случаи, если так, если мы

1756
01:15:57,700 --> 01:16:01,210
посмотрим на что-то здесь, давайте представим, что

1757
01:16:01,210 --> 01:16:02,650
это ваш  пространство состояний — это ваша

1758
01:16:02,650 --> 01:16:06,010
функция ценности, поэтому, если кто-то даст вам

1759
01:16:06,010 --> 01:16:08,410
прямую, квадратичную или глубокую нейронную

1760
01:16:08,410 --> 01:16:10,120
сеть с достаточным количеством параметров, чтобы

1761
01:16:10,120 --> 01:16:12,070
точно представить эту линию, это

1762
01:16:12,070 --> 01:16:14,530
утверждение говорит о том, что телевизионное обучение

1763
01:16:14,530 --> 01:16:16,000
может найти

1764
01:16:16,000 --> 01:16:19,090
именно эти параметры не соответствуют действительности, когда мы начинаем

1765
01:16:19,090 --> 01:16:21,280
изучать подсказки, поэтому в некоторых случаях у

1766
01:16:21,280 --> 01:16:23,500
вас может быть представление, которое могло бы

1767
01:16:23,500 --> 01:16:25,060
оптимально представлять функцию значения,

1768
01:16:25,060 --> 01:16:27,370
но вы не можете найти его, как q-обучение

1769
01:16:27,370 --> 01:16:29,470
не идентифицирует его, так что вроде

1770
01:16:29,470 --> 01:16:30,550
разница, которую мы пытаемся сделать

1771
01:16:30,550 --> 01:16:31,990
здесь, заключается в том, что обучение по телевизору, если оно

1772
01:16:31,990 --> 01:16:33,880
существует в вашей политике, вы можете найти его,

1773
01:16:33,880 --> 01:16:35,770
узнав, что вы не сможете узнать

1774
01:16:35,770 --> 01:16:36,910
других, вопрос в задней части именного кошелька,

1775
01:16:36,910 --> 01:16:39,000
пожалуйста,

1776
01:16:43,399 --> 01:16:45,449
да, так что профессиональное

1777
01:16:45,449 --> 01:16:51,229
это верно для общих

1778
01:16:51,229 --> 01:16:54,539
представлений, если ваше представление,

1779
01:16:54,539 --> 01:16:58,349
будь то линейное табличное или актуальное,

1780
01:16:58,349 --> 01:16:59,339
вы обычно всегда предполагаете, что оно

1781
01:16:59,339 --> 01:17:02,280
точное, линейное или иное, тогда

1782
01:17:02,280 --> 01:17:05,810
это верно да,

1783
01:17:05,810 --> 01:17:09,120
имеет ли это какое-либо отношение к тому, если

1784
01:17:09,120 --> 01:17:11,600
наша близость функции значения

1785
01:17:11,600 --> 01:17:15,570
сжимающий оператор или нет, так что да, это хорошо,

1786
01:17:15,570 --> 01:17:17,070
да,

1787
01:17:17,070 --> 01:17:18,630
связано ли это с тем, является ли наш

1788
01:17:18,630 --> 01:17:20,640
аппроксиматор функции ценности сокращением, о котором

1789
01:17:20,640 --> 01:17:22,050
вы можете думать, когда мы это делаем

1790
01:17:22,050 --> 01:17:23,760
что-то вроде TD, узнающего, что у нас есть два

1791
01:17:23,760 --> 01:17:24,690
шага, мы как бы делаем наш

1792
01:17:24,690 --> 01:17:27,090
приблизительный посыльный и наш оператор посыльного,

1793
01:17:27,090 --> 01:17:28,590
если бы мы могли сделать это точно, мы

1794
01:17:28,590 --> 01:17:29,880
знаем это как сокращение, а затем мы должны

1795
01:17:29,880 --> 01:17:31,140
сделать эту дополнительную часть подгонки

1796
01:17:31,140 --> 01:17:33,330
функции, и если вы можете  точно соответствует вашей

1797
01:17:33,330 --> 01:17:36,030
функции, тогда вы не собираетесь вводить

1798
01:17:36,030 --> 01:17:40,800
дополнительную ошибку в этой части, так

1799
01:17:40,800 --> 01:17:42,090
что это одно из

1800
01:17:42,090 --> 01:17:43,920
преимуществ здесь, которое не могло бы начать

1801
01:17:43,920 --> 01:17:45,300
отличаться, поэтому в этом случае снова

1802
01:17:45,300 --> 01:17:47,190
все зависит от политики, так что это намного ближе

1803
01:17:47,190 --> 01:17:48,450
настройка обучения под наблюдением, когда вы

1804
01:17:48,450 --> 01:17:50,640
начинаете отклоняться от правил, все становится

1805
01:17:50,640 --> 01:17:52,910
сложнее,

1806
01:17:54,170 --> 01:17:56,280
так что давайте очень

1807
01:17:56,280 --> 01:17:58,260
кратко пройдемся по имитационному обучению и

1808
01:17:58,260 --> 01:18:00,030
поиску правил, это будет вроде

1809
01:18:00,030 --> 01:18:02,040
того же более позднего уровня, на котором вы

1810
01:18:02,040 --> 01:18:03,690
должны знать это для экзамена.  у

1811
01:18:03,690 --> 01:18:04,980
вас не было возможности попрактиковаться ни

1812
01:18:04,980 --> 01:18:07,680
в одном из них, кроме как на лекции, поэтому

1813
01:18:07,680 --> 01:18:10,140
имитационное обучение заключалось в том, что

1814
01:18:10,140 --> 01:18:12,060
спецификация функций вознаграждения может быть

1815
01:18:12,060 --> 01:18:13,500
очень сложной, что, если бы мы могли

1816
01:18:13,500 --> 01:18:15,989
просто  Пять людей демонстрируют процедуры, а

1817
01:18:15,989 --> 01:18:18,150
затем учатся на них. Кодирование поведения — это то место,

1818
01:18:18,150 --> 01:18:19,920
где мы

1819
01:18:19,920 --> 01:18:21,680
проводим обучение с учителем, поэтому мы пытаемся изучить сопоставление

1820
01:18:21,680 --> 01:18:24,120
действий с состояниями, и мы рассматриваем

1821
01:18:24,120 --> 01:18:27,390
это как проблему обучения с учителем, поэтому

1822
01:18:27,390 --> 01:18:29,940
мы просто смотрим на  экспертные пары

1823
01:18:29,940 --> 01:18:32,790
состояний и действий, и вы можете попытаться подогнать

1824
01:18:32,790 --> 01:18:34,110
свой любимый

1825
01:18:34,110 --> 01:18:37,140
алгоритм классификации с контролируемым обучением в Мичигане, чтобы

1826
01:18:37,140 --> 01:18:39,180
предсказать это, и что может пойти

1827
01:18:39,180 --> 01:18:41,820
не так в этом случае, так это то, что ваше распределение состояний,

1828
01:18:41,820 --> 01:18:44,640
которое вы вызываете в соответствии с вашей

1829
01:18:44,640 --> 01:18:46,590
приблизительной политикой, пытается

1830
01:18:46,590 --> 01:18:49,500
имитировать экспертное  может отличаться от

1831
01:18:49,500 --> 01:18:51,300
распределения состояний, которых вы достигли бы

1832
01:18:51,300 --> 01:18:53,580
при экспертной политике, а это означает, что вы

1833
01:18:53,580 --> 01:18:54,780
можете получить такие разные

1834
01:18:54,780 --> 01:18:58,500
распределения состояний, и вы не знаете,

1835
01:18:58,500 --> 01:19:00,300
что правильно делать в

1836
01:19:00,300 --> 01:19:01,650
этих новых состояниях, потому что вы'  У меня есть какие-либо

1837
01:19:01,650 --> 01:19:03,780
данные об этом, так что

1838
01:19:03,780 --> 01:19:06,210
в некоторых из тех случаев, когда мы говорили

1839
01:19:06,210 --> 01:19:07,920
об имитационном обучении, все может пойти очень плохо, когда идея

1840
01:19:07,920 --> 01:19:10,489
состоит в том, что у нас снова есть траектории

1841
01:19:10,489 --> 01:19:12,780
демонстрации.  nstrations и теперь цель состоит в том, чтобы

1842
01:19:12,780 --> 01:19:17,580
напрямую изучить вознаграждения.

1843
01:19:17,580 --> 01:19:20,640
Хорошая вещь, чтобы переосмыслить вот это,

1844
01:19:20,640 --> 01:19:22,500
сколько функций вознаграждения совместимы

1845
01:19:22,500 --> 01:19:26,160
с демонстрацией эксперта, мы говорили

1846
01:19:26,160 --> 01:19:27,810
об этом раньше, если это не ясно,

1847
01:19:27,810 --> 01:19:29,340
не стесняйтесь обращаться ко мне в

1848
01:19:29,340 --> 01:19:32,850
конце урока  или на Piazza, а затем мы

1849
01:19:32,850 --> 01:19:35,400
говорили о поиске политик, так что

1850
01:19:35,400 --> 01:19:37,110
очень кратко это типы

1851
01:19:37,110 --> 01:19:39,390
уровней вопросов, с которыми я ожидаю, что

1852
01:19:39,390 --> 01:19:41,190
вы знакомы, так почему мы хотим использовать

1853
01:19:41,190 --> 01:19:43,440
стохастическую параметризацию политик, это

1854
01:19:43,440 --> 01:19:44,850
может быть хорошим способом поместить в предметную область

1855
01:19:44,850 --> 01:19:46,590
знания, которые могут помочь нам с

1856
01:19:46,590 --> 01:19:49,020
немарковской структурой, мы говорили о

1857
01:19:49,020 --> 01:19:51,150
алиасинге, и мы говорили о настройках теории игр

1858
01:19:51,150 --> 01:19:53,460
, где детерминированные политики

1859
01:19:53,460 --> 01:19:55,740
будут работать плохо, а стохастические —

1860
01:19:55,740 --> 01:19:58,260
хорошо. Политика создания оружия —

1861
01:19:58,260 --> 01:20:00,810
не единственная форма поиска политики, мы говорили

1862
01:20:00,810 --> 01:20:03,180
об оптимизации экзоскелета моим

1863
01:20:03,180 --> 01:20:05,370
коллега Steve culling и тот факт,

1864
01:20:05,370 --> 01:20:06,960
что это сработало довольно хорошо, но в

1865
01:20:06,960 --> 01:20:07,950
целом мы собирались говорить в основном

1866
01:20:07,950 --> 01:20:10,680
о градиентах, полигоне отношения правдоподобия.

1867
01:20:10,680 --> 01:20:13,380
Метод cy grading не требует от

1868
01:20:13,380 --> 01:20:15,120
нас наличия динамической модели, которая

1869
01:20:15,120 --> 01:20:17,010
действительно важна, потому что, когда у нас ее нет

1870
01:20:17,010 --> 01:20:19,680
, а затем две идеи по уменьшению

1871
01:20:19,680 --> 01:20:21,930
дисперсии оценки градиента политики

1872
01:20:21,930 --> 01:20:26,960
- это использовать временную структуру, и

1873
01:20:26,960 --> 01:20:28,920
здесь это связано с тем, что

1874
01:20:28,920 --> 01:20:30,960
вознаграждение, которое вы получаете на временном шаге сейчас, не

1875
01:20:30,960 --> 01:20:32,840
может зависеть от ваших будущих решений

1876
01:20:32,840 --> 01:20:35,640
из-за структуры времени, а

1877
01:20:35,640 --> 01:20:41,040
затем и во-вторых базовых показателей, так

1878
01:20:41,040 --> 01:20:42,240
что это своего рода уровень, о

1879
01:20:42,240 --> 01:20:43,680
котором мы говорили в классе, но не

1880
01:20:43,680 --> 01:20:47,040
глубокие процедурные знания  Таким образом, просто

1881
01:20:47,040 --> 01:20:49,440
подытоживая рекомендации, можно было бы

1882
01:20:49,440 --> 01:20:50,610
просмотреть конспекты лекций, посмотреть на такие вещи,

1883
01:20:50,610 --> 01:20:52,290
как проверить свое понимание, если вы

1884
01:20:52,290 --> 01:20:54,000
хотите посмотреть на существующие дополнительные

1885
01:20:54,000 --> 01:20:56,580
примеры, прохождение конспектов сеансов может

1886
01:20:56,580 --> 01:20:59,010
быть полезным, практика в середине семестра,

1887
01:20:59,010 --> 01:21:00,300
особенно в прошлом году, будет больше

1888
01:21:00,300 --> 01:21:02,490
похожа на тот, что был в прошлом году.  два года назад,

1889
01:21:02,490 --> 01:21:04,260
если вы увидите какую-то тему, которую мы не

1890
01:21:04,260 --> 01:21:05,730
рассмотрели на этом занятии, она не

1891
01:21:05,730 --> 01:21:07,620
будет затронута в середине семестра, но не стесняйтесь

1892
01:21:07,620 --> 01:21:08,940
обращаться к нам, если у вас есть  ve любые

1893
01:21:08,940 --> 01:21:10,770
вопросы, и вы можете принести одностороннюю

1894
01:21:10,770 --> 01:21:12,450
одну страницу заметок, написанных от руки или

1895
01:21:12,450 --> 01:21:16,599
напечатанных хорошо, удачи

1896
01:21:16,599 --> 01:21:18,659
вам

