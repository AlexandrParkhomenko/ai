1
00:00:05,089 --> 00:00:07,049
хорошо, мы продолжим и начнем,

2
00:00:07,049 --> 00:00:09,750
прежде чем я перейду к

3
00:00:09,750 --> 00:00:11,160
техническим вопросам, мы немного

4
00:00:11,160 --> 00:00:14,490
займемся логистикой, поэтому мы начинаем эти

5
00:00:14,490 --> 00:00:16,350
вещи, называемые сессиями, и мы объявили о

6
00:00:16,350 --> 00:00:18,750
них на Пьяцца, если вы этого не сделали, если вы'  Если вы

7
00:00:18,750 --> 00:00:19,980
не получаете ваши вещи на Piazza,

8
00:00:19,980 --> 00:00:22,080
обязательно зарегистрируйтесь на Piazza или отправьте

9
00:00:22,080 --> 00:00:24,930
нам сообщение. Сессии предназначены для

10
00:00:24,930 --> 00:00:26,490
более глубокого изучения материала, а

11
00:00:26,490 --> 00:00:27,900
также для обсуждения

12
00:00:27,900 --> 00:00:30,810
домашних заданий. Это структурированная сессия, а

13
00:00:30,810 --> 00:00:32,520
не офис.  час, когда вы можете

14
00:00:32,520 --> 00:00:34,079
задавать вопросы один на один о

15
00:00:34,079 --> 00:00:36,000
домашнем задании. Занятия предназначены

16
00:00:36,000 --> 00:00:37,410
для того, чтобы немного углубить

17
00:00:37,410 --> 00:00:39,090
материал, и они были

18
00:00:39,090 --> 00:00:40,950
основаны на отзывах некоторых моих коллег

19
00:00:40,950 --> 00:00:42,660
о том, насколько ученикам они понравились.

20
00:00:42,660 --> 00:00:44,370
другие классы, а

21
00:00:44,370 --> 00:00:46,050
также некоторые просьбы прошлого года о

22
00:00:46,050 --> 00:00:47,399
возможности углубиться в

23
00:00:47,399 --> 00:00:50,100
материал, поэтому мы объявили об этом на

24
00:00:50,100 --> 00:00:52,410
площади. Идея состоит в том, что вы зарегистрируетесь

25
00:00:52,410 --> 00:00:54,210
на сеансе, они необязательны, вам

26
00:00:54,210 --> 00:00:55,980
не нужно их делать, мы  будет давать

27
00:00:55,980 --> 00:00:58,379
1% дополнительного кредита за посещение их, если

28
00:00:58,379 --> 00:00:59,790
вы посещаете достаточное количество из них,

29
00:00:59,790 --> 00:01:01,800
подробности об этом также были

30
00:01:01,800 --> 00:01:05,069
объявлены, и я думаю, что - если у меня

31
00:01:05,069 --> 00:01:06,420
есть право на электронное письмо мне, я думаю,

32
00:01:06,420 --> 00:01:08,790
что это было объявлено также на площади  так что,

33
00:01:08,790 --> 00:01:10,380
если вы идете на площадь,

34
00:01:10,380 --> 00:01:11,549
вы можете записаться на несколько разных сеансов, так

35
00:01:11,549 --> 00:01:13,229
как смысл подписки на них заключается в

36
00:01:13,229 --> 00:01:14,970
том, чтобы убедиться, что у нас есть вместимость, но

37
00:01:14,970 --> 00:01:15,900
я почти уверен, что мы сможем

38
00:01:15,900 --> 00:01:17,820
разместить практически любой сеанс, который вы  хотите

39
00:01:17,820 --> 00:01:20,100
пойти на последнюю сессию, будет сделано с

40
00:01:20,100 --> 00:01:22,260
помощью нашего масштабирования и особенно ориентировано

41
00:01:22,260 --> 00:01:24,420
на студентов SCPD, но любой может

42
00:01:24,420 --> 00:01:27,479
сделать это таким образом, что мы будем

43
00:01:27,479 --> 00:01:28,860
отслеживать, собираются ли люди

44
00:01:28,860 --> 00:01:30,270
на сессии или нет, у нас будет

45
00:01:30,270 --> 00:01:33,140
код, который упоминается внутри

46
00:01:33,140 --> 00:01:35,640
материала, и поэтому вы просто

47
00:01:35,640 --> 00:01:37,850
напишете этот код, чтобы указать свое присутствие,

48
00:01:37,850 --> 00:01:40,470
мы запишем последнюю сессию, чтобы,

49
00:01:40,470 --> 00:01:41,880
если по какой-то причине ваше расписание таково,

50
00:01:41,880 --> 00:01:43,950
что вы не можете посетить ни одно из них, которые вы

51
00:01:43,950 --> 00:01:45,450
хотите  участвовать в  вы

52
00:01:45,450 --> 00:01:47,369
можете просмотреть материал позже, а затем

53
00:01:47,369 --> 00:01:49,799
записать, что вы посетили его, используя

54
00:01:49,799 --> 00:01:52,170
этот код, и мы будем полагаться на

55
00:01:52,170 --> 00:01:54,000
кодекс чести Стэнфорда, что только люди

56
00:01:54,000 --> 00:01:55,740
, которые делают это, будут загружать

57
00:01:55,740 --> 00:01:57,750
коды, у которых могут возникнуть вопросы о

58
00:01:57,750 --> 00:01:59,340
сеансах и что  хочу, чтобы те, кто

59
00:01:59,340 --> 00:02:01,259
участвует снова, они не являются обязательными, есть

60
00:02:01,259 --> 00:02:04,079
способ углубиться в материал, некоторым

61
00:02:04,079 --> 00:02:05,430
другим людям действительно понравились такие

62
00:02:05,430 --> 00:02:07,290
вещи, вы можете увидеть, что вы

63
00:02:07,290 --> 00:02:10,189
думаете, это эксперимент,

64
00:02:12,060 --> 00:02:13,590
хорошо, любые вопросы о чем

65
00:02:13,590 --> 00:02:15,600
-либо еще вне сессий, поэтому домашние задания

66
00:02:15,600 --> 00:02:17,280
были выпущены  на

67
00:02:17,280 --> 00:02:19,860
этой неделе в нерабочее время, как обычно, не стесняйтесь

68
00:02:19,860 --> 00:02:21,540
обращаться к нам или использовать Piazza для любых

69
00:02:21,540 --> 00:02:24,180
вопросов, которые у вас есть, все в порядке, мы

70
00:02:24,180 --> 00:02:25,410
собираемся продолжить и начать сейчас, тогда,

71
00:02:25,410 --> 00:02:29,040
как обычно, я очень ценю это, если вы

72
00:02:29,040 --> 00:02:30,480
используете свое имя всякий раз, когда вы  задаем

73
00:02:30,480 --> 00:02:32,670
вопрос или делаем комментарий, так что сегодня

74
00:02:32,670 --> 00:02:34,080
мы, наконец, начнем

75
00:02:34,080 --> 00:02:36,030
принимать решения там, где у нас нет

76
00:02:36,030 --> 00:02:37,560
модели мира, и, в частности,

77
00:02:37,560 --> 00:02:39,750
мы будем  сосредоточившись на свободном управлении моделями,

78
00:02:39,750 --> 00:02:43,080
поэтому вещи, которые мы

79
00:02:43,080 --> 00:02:44,910
собираемся осветить сегодня, на самом деле сосредоточены на

80
00:02:44,910 --> 00:02:46,890
том, как агент может начать принимать правильные

81
00:02:46,890 --> 00:02:48,510
решения, когда он не знает, как

82
00:02:48,510 --> 00:02:50,280
устроен мир, и он не собирается

83
00:02:50,280 --> 00:02:53,330
явно строить модель.  и

84
00:02:53,330 --> 00:02:55,860
помните, что моделью в этом случае

85
00:02:55,860 --> 00:02:58,680
будет вознаграждение и/или модель

86
00:02:58,680 --> 00:03:00,420
динамики окружающей среды, поэтому сегодня мы

87
00:03:00,420 --> 00:03:01,709
рассмотрим методы, которые не

88
00:03:01,709 --> 00:03:04,739
предполагают построения динамики или

89
00:03:04,739 --> 00:03:06,209
модели вознаграждения, а просто будут

90
00:03:06,209 --> 00:03:11,690
непосредственно обучаться.  по опыту, так что

91
00:03:11,690 --> 00:03:14,100
прежде чем мы в прошлый раз в основном говорили

92
00:03:14,100 --> 00:03:15,420
о том, что, может быть, мы не знаем, как

93
00:03:15,420 --> 00:03:16,829


94
00:03:16,829 --> 00:03:18,450


95
00:03:18,450 --> 00:03:20,220
устроен мир, у нас нет этих явных моделей динамики и вознаграждения, но мы попытаемся оценить политику,

96
00:03:20,220 --> 00:03:22,260
которая была нам предоставлена, и  теперь мы

97
00:03:22,260 --> 00:03:24,000
собираемся подумать о реальной проблеме,

98
00:03:24,000 --> 00:03:25,410
которая часто возникает при

99
00:03:25,410 --> 00:03:26,760
обучении с подкреплением, а именно о том, как агент должен

100
00:03:26,760 --> 00:03:28,320
принимать решения, когда он не знает,

101
00:03:28,320 --> 00:03:30,480
как устроен мир, и он все еще хочет

102
00:03:30,480 --> 00:03:32,640
максимизировать ожидаемую дискотеку.  неограниченная

103
00:03:32,640 --> 00:03:38,160
сумма вознаграждений, поэтому, когда мы думаем

104
00:03:38,160 --> 00:03:39,750
о том, насколько хороша политика, как только у нас

105
00:03:39,750 --> 00:03:41,489
есть информация о том, насколько хороша

106
00:03:41,489 --> 00:03:43,290
политика, мы можем начать думать о том, как

107
00:03:43,290 --> 00:03:45,660
мы можем изучить хорошую политику вместо этого и

108
00:03:45,660 --> 00:03:47,220
фактически когда мы начали  в самом

109
00:03:47,220 --> 00:03:49,079
начале урока мы говорили о том, как

110
00:03:49,079 --> 00:03:50,220
бы вы научились принимать правильные решения

111
00:03:50,220 --> 00:03:52,470
или как бы вы вычисляли правильные решения,

112
00:03:52,470 --> 00:03:54,180
если бы вам дали модель мира,

113
00:03:54,180 --> 00:03:55,530
и именно к этому

114
00:03:55,530 --> 00:03:59,430
мы сейчас вернемся, так что в  в частности, теперь

115
00:03:59,430 --> 00:04:01,140
мы можем подумать о том, чтобы начать заниматься

116
00:04:01,140 --> 00:04:02,989
этим вопросом оптимизации и

117
00:04:02,989 --> 00:04:05,250
исследования, мы все еще не собираемся

118
00:04:05,250 --> 00:04:08,190
вдаваться в обобщения, но это произойдет в

119
00:04:08,190 --> 00:04:11,459
ближайшее время, мы уже видели это

120
00:04:11,459 --> 00:04:12,900
немного, это даже придумали

121
00:04:12,900 --> 00:04:17,789
планирование, но сейчас  мы собираемся начать

122
00:04:17,789 --> 00:04:19,769
думать о том, как мы исследуем и как

123
00:04:19,769 --> 00:04:22,729
мы проводим оптимизацию,

124
00:04:23,860 --> 00:04:27,290
поэтому, когда мы подумаем о том, что хорошо, я думаю, что

125
00:04:27,290 --> 00:04:28,400
я просто пройдусь по этим вопросам, когда

126
00:04:28,400 --> 00:04:31,880
мы начнем углубляться в эту область, так что снова

127
00:04:31,880 --> 00:04:33,170
мы собираемся  думать о том, как мы

128
00:04:33,170 --> 00:04:35,120
идентифицируем поли  cy, который имеет высокое

129
00:04:35,120 --> 00:04:36,890
ожидаемое количество пыли в некоторых

130
00:04:36,890 --> 00:04:38,420
наградах, будут отсроченные последствия,

131
00:04:38,420 --> 00:04:40,970
что означает, что, поскольку наш агент предпринимает действия,

132
00:04:40,970 --> 00:04:43,010
которые могут не видеть результата, были

133
00:04:43,010 --> 00:04:44,210
ли эти действия хорошими или плохими

134
00:04:44,210 --> 00:04:45,770
какое-то время, и мы начнем

135
00:04:45,770 --> 00:04:50,090
подумайте об этом аспекте исследования, хорошо,

136
00:04:50,090 --> 00:04:53,390
так что давайте начнем с того, что вы знаете, мы

137
00:04:53,390 --> 00:04:54,740
начинаем, когда возникают проблемы, и когда

138
00:04:54,740 --> 00:04:56,660
люди моделируют вещи, когда мы думаем

139
00:04:56,660 --> 00:04:58,670
о марковских процессах принятия решений и,

140
00:04:58,670 --> 00:05:00,530
возможно, не строим модель, так что я думаю, что,

141
00:05:00,530 --> 00:05:01,970
вероятно, один из первых действительно  большие

142
00:05:01,970 --> 00:05:03,770
примеры успеха

143
00:05:03,770 --> 00:05:05,420
обучения с подкреплением и его выполнения в

144
00:05:05,420 --> 00:05:07,490
этой модели автострады были для игры в нарды

145
00:05:07,490 --> 00:05:11,120
, примерно в 1994 году они обучили

146
00:05:11,120 --> 00:05:13,130
агента играть в нарды в настольную игру.

147
00:05:13,130 --> 00:05:16,970
Я на самом деле использовал нейронные сети,

148
00:05:16,970 --> 00:05:18,860
когда нейронные сети вышли из моды,

149
00:05:18,860 --> 00:05:20,420
вероятно, около 10  -15 лет, а затем

150
00:05:20,420 --> 00:05:22,640
вернулись в начале 90-х, люди

151
00:05:22,640 --> 00:05:26,240
использовали нейронные сети, и Джерри Саро

152
00:05:26,240 --> 00:05:28,100
использовал их для игры в нарды и получил очень

153
00:05:28,100 --> 00:05:29,540
хорошие результаты, и это было своего рода  одной

154
00:05:29,540 --> 00:05:30,770
из первых демонстраций

155
00:05:30,770 --> 00:05:32,420
обучения с подкреплением в виде более

156
00:05:32,420 --> 00:05:33,950
крупной среды, в которой вы могли бы

157
00:05:33,950 --> 00:05:36,770
решать такие сложные игры, многие

158
00:05:36,770 --> 00:05:38,570
другие проблемы с получением также могут быть смоделированы

159
00:05:38,570 --> 00:05:41,210
как погодные игры MDP или роботы, или

160
00:05:41,210 --> 00:05:43,160
выбор рекламы для клиентов, или

161
00:05:43,160 --> 00:05:45,590
управление инвазивными видами, и во многих  из этих

162
00:05:45,590 --> 00:05:47,450
случаев мы не знаем модели в нем

163
00:05:47,450 --> 00:05:49,610
заранее, поэтому сегодня мы будем думать

164
00:05:49,610 --> 00:05:53,240
о ситуациях, в частности, в

165
00:05:53,240 --> 00:05:54,950
основном здесь, когда мы думаем о том, что

166
00:05:54,950 --> 00:05:56,630
модель неизвестна, но если мы можем ее попробовать

167
00:05:56,630 --> 00:05:59,690
, но иногда бывают случаи, чтобы

168
00:05:59,690 --> 00:06:01,400
где вы знаете модель, но это

169
00:06:01,400 --> 00:06:04,550
действительно очень дорого, поэтому для чего-то

170
00:06:04,550 --> 00:06:06,620
вроде вычислительной устойчивости или

171
00:06:06,620 --> 00:06:08,570
моделирования климата вы можете

172
00:06:08,570 --> 00:06:10,070
написать хорошую модель мира, но

173
00:06:10,070 --> 00:06:11,330
это действительно очень дорого,

174
00:06:11,330 --> 00:06:13,220
потому что на самом деле моделирование

175
00:06:13,220 --> 00:06:14,600
климата действительно очень сложно

176
00:06:14,600 --> 00:06:16,460
очень сложно, и даже тогда ваша модель

177
00:06:16,460 --> 00:06:18,680
, вероятно, будет невозможна, но я только что

178
00:06:18,680 --> 00:06:20,660
затронул этот второй пункт в том смысле,

179
00:06:20,660 --> 00:06:22,250
что, когда мы больше всего  вы думаете о том, чтобы

180
00:06:22,250 --> 00:06:23,720
учиться у мира, мы думаем о

181
00:06:23,720 --> 00:06:25,310
роботе, который бегает по миру

182
00:06:25,310 --> 00:06:27,320
, и это дорогое

183
00:06:27,320 --> 00:06:29,870
занятие, потому что роботы делают это в режиме реального

184
00:06:29,870 --> 00:06:32,860
времени, но вы также можете подумать об

185
00:06:32,860 --> 00:06:34,790
агентах, которые учатся сортировать.

186
00:06:34,790 --> 00:06:36,140
взаимодействия с симулятором, где с этим

187
00:06:36,140 --> 00:06:38,830
также

188
00:06:39,390 --> 00:06:41,440
все в порядке, поэтому сегодня мы будем

189
00:06:41,440 --> 00:06:43,780
думать в основном

190
00:06:43,780 --> 00:06:46,240
о так называемом политическом обучении, когда мы получаем

191
00:06:46,240 --> 00:06:48,160
непосредственный опыт о мире, а

192
00:06:48,160 --> 00:06:50,770
затем пытаемся использовать его для оценки

193
00:06:50,770 --> 00:06:53,020
оценочная политика на основе этого опыта,

194
00:06:53,020 --> 00:06:55,270
но мы также собираемся больше говорить

195
00:06:55,270 --> 00:06:58,000
об обучении вне политики, когда мы

196
00:06:58,000 --> 00:06:59,920
получаем данные о мире и используем их

197
00:06:59,920 --> 00:07:02,080
для активации альтернативных способов ведения

198
00:07:02,080 --> 00:07:04,960
дел, чтобы мы могли как бы объединить

199
00:07:04,960 --> 00:07:06,520
опыт, полученный от пробы разных

200
00:07:06,520 --> 00:07:08,530
вещей.  попытаться узнать о чем-

201
00:07:08,530 --> 00:07:11,290
то, что мы не сделали сами, и я

202
00:07:11,290 --> 00:07:12,910
очень важен, поэтому я или

203
00:07:12,910 --> 00:07:13,990
второе очень важно, поэтому

204
00:07:13,990 --> 00:07:16,000
я просто кратко расскажу об этом здесь,

205
00:07:16,000 --> 00:07:18,910
так что представьте  у вас есть случай, когда, скажем,

206
00:07:18,910 --> 00:07:20,530
сейчас есть только одно состояние, но,

207
00:07:20,530 --> 00:07:22,780
как и в случае с состоянием s1, вы делаете 1, вы

208
00:07:22,780 --> 00:07:25,690
остаетесь в s1, затем вы делаете 1 или вы

209
00:07:25,690 --> 00:07:28,630
s1, вы делаете а, чтобы остаться s-1, а затем  вы

210
00:07:28,630 --> 00:07:32,230
делаете 2, поэтому вы хотели бы иметь возможность

211
00:07:32,230 --> 00:07:34,240
комбинировать эти опыты, чтобы

212
00:07:34,240 --> 00:07:38,410
вы могли научиться делать это, даже

213
00:07:38,410 --> 00:07:39,610
если вы никогда не делали этого в

214
00:07:39,610 --> 00:07:41,980
мире вы никогда не испытывали эту полную

215
00:07:41,980 --> 00:07:43,330
траекторию, но вы хотели бы  чтобы иметь возможность как

216
00:07:43,330 --> 00:07:45,100
бы экстраполировать этот предыдущий

217
00:07:45,100 --> 00:07:49,300
опыт, чтобы такая политика

218
00:07:49,300 --> 00:07:52,090
была отключенной, за которой следует обучение вне политики,

219
00:07:52,090 --> 00:07:53,530
потому что она отличается от

220
00:07:53,530 --> 00:07:57,070
предыдущих политик, которые мы пробовали, мы углубимся

221
00:07:57,070 --> 00:07:58,570
в это больше, когда будем думать о Q

222
00:07:58,570 --> 00:08:01,630
хорошо учимся, так что давайте начнем с

223
00:08:01,630 --> 00:08:04,390
обобщенной итерации политики, ладно, если

224
00:08:04,390 --> 00:08:06,070
мы вернемся к итерации политики, мы говорили

225
00:08:06,070 --> 00:08:07,930
об этом пару лекций назад,

226
00:08:07,930 --> 00:08:09,700
итерация политики, которую мы изначально видели,

227
00:08:09,700 --> 00:08:12,250
когда знали модель мира, так что

228
00:08:12,250 --> 00:08:13,690
это был способ для нас вычислить  что

229
00:08:13,690 --> 00:08:16,120
было правильно сделать, учитывая правильное

230
00:08:16,120 --> 00:08:18,220
значение  политика, которая максимизирует наше

231
00:08:18,220 --> 00:08:20,710
ожидаемое дисконтированное летнее вознаграждение, так

232
00:08:20,710 --> 00:08:22,090
как мы это делаем, когда мы знаем, как

233
00:08:22,090 --> 00:08:23,830
устроен мир, нам дана наша динамика и

234
00:08:23,830 --> 00:08:26,230
модель вознаграждения, в этом случае мы инициализируем

235
00:08:26,230 --> 00:08:29,140
некоторую политику, вероятно, случайным образом, поэтому

236
00:08:29,140 --> 00:08:30,760
повторная инициализация будет означать, что мы бы

237
00:08:30,760 --> 00:08:33,870
установить PI для s равным некоторому a для всех s,

238
00:08:33,870 --> 00:08:36,159
и это, как правило, вероятно, не

239
00:08:36,159 --> 00:08:41,020
выбрано случайным образом, а затем мы выполнили эту

240
00:08:41,020 --> 00:08:43,929
процедуру оценки политики, в которой мы

241
00:08:43,929 --> 00:08:45,670
сначала вычислили текущее значение

242
00:08:45,670 --> 00:08:50,400
политики, а затем обновили политику, которую

243
00:08:50,569 --> 00:08:53,339
мы взяли все, что мы  было, а затем мы

244
00:08:53,339 --> 00:08:54,689
сделали еще одну вещь, о которой вы

245
00:08:54,689 --> 00:08:56,040
не могли подумать, как о еще одном

246
00:08:56,040 --> 00:08:58,800
посыльном, где мы сказали, хорошо,

247
00:08:58,800 --> 00:09:01,050
мы берем этот V PI, мы подключаем

248
00:09:01,050 --> 00:09:02,879
его сюда, мы  используя тот факт,

249
00:09:02,879 --> 00:09:04,800
что мы знаем модель динамики, и мы

250
00:09:04,800 --> 00:09:06,629
знаем модель вознаграждения, и мы

251
00:09:06,629 --> 00:09:08,879
вычисляем этот одношаговый обновленный PI Prime,

252
00:09:08,879 --> 00:09:12,209
и мы говорили о том факте, что, когда

253
00:09:12,209 --> 00:09:14,100
мы делаем это, мы фактически получаем монотонное

254
00:09:14,100 --> 00:09:22,620
улучшение политики, которое иногда

255
00:09:22,620 --> 00:09:24,449
называют  теорема об улучшении политики,

256
00:09:24,449 --> 00:09:26,910
поэтому эта процедура, когда мы

257
00:09:26,910 --> 00:09:30,209
делаем это с этим парнем в этом случае,

258
00:09:30,209 --> 00:09:31,769
когда мы знали модель динамики, и мы

259
00:09:31,769 --> 00:09:35,160
знали, что модель вознаграждения

260
00:09:35,160 --> 00:09:36,420
всегда давала нам политику, которая была

261
00:09:36,420 --> 00:09:38,100
бы, по крайней мере, так же хороша, как предыдущая политика или

262
00:09:38,100 --> 00:09:41,670
лучше, и  в конечном итоге было

263
00:09:41,670 --> 00:09:44,160
гарантировано сходимость, по крайней мере, в случае, когда

264
00:09:44,160 --> 00:09:45,839
у нас есть конечные состояния и действия,

265
00:09:45,839 --> 00:09:47,370
потому что было только конечное

266
00:09:47,370 --> 00:09:49,470
количество политик, поэтому в этом случае было

267
00:09:49,470 --> 00:09:52,589
только одно из возможных политик, поэтому нам

268
00:09:52,589 --> 00:09:53,850
не нужно делать все это целиком  процедура не

269
00:09:53,850 --> 00:09:56,040
более чем S раз, потому что каждый раз, когда

270
00:09:56,040 --> 00:09:57,509
мы либо выбираем лучшую политику, либо

271
00:09:57,509 --> 00:09:59,399
остаемся прежней, и как только вы найдете ту

272
00:09:59,399 --> 00:10:02,579
же политику, что и ваша, вы закончили, поэтому теперь мы

273
00:10:02,579 --> 00:10:05,160
хотим сделать все это, но мы не  иметь

274
00:10:05,160 --> 00:10:07,970
доступ к динамике или модели вознаграждения,

275
00:10:07,970 --> 00:10:10,860
поэтому есть ли у кого-нибудь идеи о том, как мы

276
00:10:10,860 --> 00:10:12,720
могли бы сделать то же самое сейчас

277
00:10:12,720 --> 00:10:14,550
, когда мы не знаем

278
00:10:14,550 --> 00:10:23,550
вероятности перехода матрицы динамики или вознаграждения,

279
00:10:23,550 --> 00:10:28,350
которые вы рассчитываете, поскольку ваше предложение

280
00:10:28,350 --> 00:10:30,600
хорошо, что, если мы  т  попробуйте я, если я правильно интерпретирую,

281
00:10:30,600 --> 00:10:31,889
что, если мы попытаемся в основном

282
00:10:31,889 --> 00:10:33,779
оценить модель динамики в модели вознаграждения

283
00:10:33,779 --> 00:10:35,519
из мира, а затем мы могли бы

284
00:10:35,519 --> 00:10:37,379
использовать это, чтобы все еще могли вычислить вашу

285
00:10:37,379 --> 00:10:39,269
функцию ценности, возможно, используя некоторые из

286
00:10:39,269 --> 00:10:41,370
методов, которые мы видели в прошлый раз, а затем вы

287
00:10:41,370 --> 00:10:42,779
мог бы сделать это, обновить это улучшение политики,

288
00:10:42,779 --> 00:10:44,129
используя вашу предполагаемую

289
00:10:44,129 --> 00:10:45,870
динамику и модель вознаграждения мира

290
00:10:45,870 --> 00:10:47,699
, это вполне разумно,

291
00:10:47,699 --> 00:10:49,860
и у вас могут быть какие-либо другие идеи о том, что

292
00:10:49,860 --> 00:10:50,490
мы могли бы сделать,

293
00:10:50,490 --> 00:10:57,029
да, по имени Адам, на самом деле в компьютерную

294
00:10:57,029 --> 00:10:59,730
модель. могу ли я покончить с Мори и

295
00:10:59,730 --> 00:11:02,040
готов  попытайтесь оценить ценность

296
00:11:02,040 --> 00:11:06,990
конкретного пространства, см. этот конкурс,

297
00:11:06,990 --> 00:11:09,630
так что, не отказываясь от модели,

298
00:11:09,630 --> 00:11:16,800
мы получаем оценочное состояние в действии, да, это

299
00:11:16,800 --> 00:11:19,290
было на самом деле да, так что на самом деле было сказано,

300
00:11:19,290 --> 00:11:21,000
это именно тот путь, который мы собираемся

301
00:11:21,000 --> 00:11:22,740
рассмотреть сегодня, а именно мы  я собираюсь сосредоточиться

302
00:11:22,740 --> 00:11:24,630
на свободном управлении моделями, поэтому мы не

303
00:11:24,630 --> 00:11:26,730
собираемся напрямую оценивать модель сегодня, я

304
00:11:26,730 --> 00:11:28,199
лично очень неравнодушен к

305
00:11:28,199 --> 00:11:29,760
моделям, они могут быть очень эффективными с точки зрения выборки,

306
00:11:29,760 --> 00:11:31,110
но на сегодня мы не  я не собираюсь смотреть на

307
00:11:31,110 --> 00:11:32,910
это, и мы будем делать именно то, что было

308
00:11:32,910 --> 00:11:34,440
только что предложено, то есть мы собираемся

309
00:11:34,440 --> 00:11:37,769
вычислить куб, и если мы вычислим функцию q,

310
00:11:37,769 --> 00:11:39,690
нужно просто помнить, что q

311
00:11:39,690 --> 00:11:41,750
всегда является функцией состояния в действии, которое

312
00:11:41,750 --> 00:11:43,829
мы собираемся оценить  функция q

313
00:11:43,829 --> 00:11:46,019
напрямую, и после того, как мы получим это, мы

314
00:11:46,019 --> 00:11:47,940
можем улучшить политику напрямую, используя

315
00:11:47,940 --> 00:11:51,769
эту функцию q, так как бы мы это сделали,

316
00:11:51,769 --> 00:11:54,990
так что это Монте-Карло для оценки политики Q,

317
00:11:54,990 --> 00:11:56,639
и он будет очень

318
00:11:56,639 --> 00:11:59,220
похож на Монте-Карло для

319
00:11:59,220 --> 00:12:02,220
оценки ценности политики  но мы должны сделать

320
00:12:02,220 --> 00:12:05,100
пару модификаций, поэтому раньше, если мы

321
00:12:05,100 --> 00:12:06,449
делали это для V, я просто собираюсь

322
00:12:06,449 --> 00:12:09,000
написать это для контраста, поэтому для V у нас

323
00:12:09,000 --> 00:12:11,040
просто был подсчет количества состояний,

324
00:12:11,040 --> 00:12:12,839
теперь у нас есть подсчет количества состояний.

325
00:12:12,839 --> 00:12:16,079
пары действий состояния, прежде чем мы могли просто

326
00:12:16,079 --> 00:12:20,519
отслеживать G здесь, что может быть

327
00:12:20,519 --> 00:12:23,370
суммой предыдущих вознаграждений, которые мы видели

328
00:12:23,370 --> 00:12:25,829
во всех эпизодах для G of s, теперь мы

329
00:12:25,829 --> 00:12:30,360
собираемся сделать это для s запятая, а затем теперь

330
00:12:30,360 --> 00:12:31,620
мы собираемся вместо этого  функции ценности

331
00:12:31,620 --> 00:12:33,510
у нас будет Q PI, поэтому

332
00:12:33,510 --> 00:12:34,800
по сути, почти везде, где мы

333
00:12:34,800 --> 00:12:37,100
только что имели s, теперь у нас есть s запятая 8,

334
00:12:37,100 --> 00:12:39,540
и тогда это будет выглядеть очень похоже, поэтому

335
00:12:39,540 --> 00:12:40,500
мы будем считать, что у нас все

336
00:12:40,500 --> 00:12:42,569
еще есть политика, которую мы можем выбрать из

337
00:12:42,569 --> 00:12:46,949
эпизода, а затем мы вычисляем GI T для

338
00:12:46,949 --> 00:12:49,199
каждого отдельного  временной шаг, и это помните

339
00:12:49,199 --> 00:12:51,480
сейчас, я имею в виду, что это было раньше, но

340
00:12:51,480 --> 00:12:52,800
мы будем думать о том факте, что

341
00:12:52,800 --> 00:12:54,389
мы также связаны с определенным

342
00:12:54,389 --> 00:12:55,860
состоянием и конкретным действием для этого

343
00:12:55,860 --> 00:12:58,319
временного шага, а затем для каждой

344
00:12:58,319 --> 00:13:00,480
пары действий состояния, а не просто  посещение состояния

345
00:13:00,480 --> 00:13:03,029
в эпизоде либо каждый ра 

346
00:13:03,029 --> 00:13:04,769
, когда мы впервые видели эту пару действий состояния, либо ка 

347
00:13:04,769 --> 00:13:06,149
дый раз, когда вы видели эту пару действий состояния, мы 

348
00:13:06,149 --> 00:13:08,699
сегда можем, как прежде, мы мо 

349
00:13:08,699 --> 00:13:10,110
ем либо иметь первое посещение, либо каждое по 

350
00:13:10,110 --> 00:13:12,750
ещение там, мы просто собираемся обновить на 

351
00:13:12,750 --> 00:13:13,620
и по 

352
00:13:13,620 --> 00:13:16,170
счеты  обновите нашу сумму общих вознаграждений, а затем

353
00:13:16,170 --> 00:13:18,930
оцените нашу ключевую функцию, которая в

354
00:13:18,930 --> 00:13:20,850
основном точно такая же, как и раньше,

355
00:13:20,850 --> 00:13:22,290
за исключением того, что теперь мы делаем все

356
00:13:22,290 --> 00:13:28,470
по парам действий состояния, когда у нас есть

357
00:13:28,470 --> 00:13:30,779
это, теперь улучшение политики стало еще

358
00:13:30,779 --> 00:13:33,390
проще.  чем раньше, поэтому нам дали эту

359
00:13:33,390 --> 00:13:35,910
оценку функции cue, и

360
00:13:35,910 --> 00:13:37,800
теперь мы можем просто напрямую взять

361
00:13:37,800 --> 00:13:38,820
аргумент над ней,

362
00:13:38,820 --> 00:13:42,000
поэтому мы определяем нашу новую политику просто как

363
00:13:42,000 --> 00:13:48,660
Arg max предыдущей, все в порядке, так

364
00:13:48,660 --> 00:13:50,700
что кто-нибудь видит какие-либо проблемы с  Делая

365
00:13:50,700 --> 00:13:52,529
это до сих пор для типов политик, о которых

366
00:13:52,529 --> 00:13:53,880
мы думали в этом классе,

367
00:13:53,880 --> 00:13:55,170
до сих пор мы думали в

368
00:13:55,170 --> 00:13:56,670
основном о политиках, которые все являются

369
00:13:56,670 --> 00:14:00,540
детерминированными, что означает, что для их

370
00:14:00,540 --> 00:14:02,430
отображения от состояний к действиям, и

371
00:14:02,430 --> 00:14:04,290
мы думали о  случаи, когда

372
00:14:04,290 --> 00:14:07,320
это детерминистическое отображение, поэтому мы всегда

373
00:14:07,320 --> 00:14:08,730
выбираем конкретное действие для

374
00:14:08,730 --> 00:14:10,320
определенного состояния, да, в конце и

375
00:14:10,320 --> 00:14:16,980
имя лично я не был, в чем

376
00:14:16,980 --> 00:14:19,050
проблема, которую мы никогда не исследуем, что

377
00:14:19,050 --> 00:14:20,370
правильно, но в чем проблема с тем, чтобы не

378
00:14:20,370 --> 00:14:32,940
исследовать, что, возможно  мы только

379
00:14:32,940 --> 00:14:34,589
пробуем один путь, я думаю, что он имеет в виду

380
00:14:34,589 --> 00:14:35,850
больше, чем это, так что вы все еще можете

381
00:14:35,850 --> 00:14:37,290
пробовать разные пути, потому что ваше

382
00:14:37,290 --> 00:14:40,350
пространство состояний может быть стохастическим, но вы

383
00:14:40,350 --> 00:14:41,700
когда-либо будете пытаться только одно действие из

384
00:14:41,700 --> 00:14:43,170
одного состояния, так что вы  никогда не узнать

385
00:14:43,170 --> 00:14:44,790
о том, что было бы, если бы вы

386
00:14:44,790 --> 00:14:47,370
взяли - вместо 1 в этом состоянии, что

387
00:14:47,370 --> 00:14:49,950
означает, что когда вы делаете это для

388
00:14:49,950 --> 00:14:52,310
любого конкретного состояния, вы увидите только одно

389
00:14:52,310 --> 00:14:56,040
соответствующее действие, поэтому время,

390
00:14:56,040 --> 00:14:58,200
когда вы видите состояние  s1 единственный раз,

391
00:14:58,200 --> 00:14:59,940
когда единственное действие, которое вы увидите, будет 1

392
00:14:59,940 --> 00:15:01,709
или что-то другое, что говорит ваша политика,

393
00:15:01,709 --> 00:15:03,270
что означает, что у вас не будет

394
00:15:03,270 --> 00:15:05,339
никакой информации о том, чтобы делать что

395
00:15:05,339 --> 00:15:06,720
-либо еще там, что означает, что улучшение вашей политики

396
00:15:06,720 --> 00:15:08,160
будет довольно скучным,

397
00:15:08,160 --> 00:15:09,900
потому что  вы не получите никакой другой

398
00:15:09,900 --> 00:15:11,370
информации о вещах, которые вы должны

399
00:15:11,370 --> 00:15:13,800
делать вместо этого, поэтому нам придется провести

400
00:15:13,800 --> 00:15:15,480
некоторую форму исследования, по сути, сейчас,

401
00:15:15,480 --> 00:15:16,770
нам нужно начать иметь какой-

402
00:15:16,770 --> 00:15:19,410
то стохастический подход в нашей политике или

403
00:15:19,410 --> 00:15:21,330
там, где это необходимо.  со временем меняться,

404
00:15:21,330 --> 00:15:22,709
чтобы мы могли пробовать разные

405
00:15:22,709 --> 00:15:24,360
вещи даже из одного и того же состояния и

406
00:15:24,360 --> 00:15:27,199
узнавать, что делать да участники

407
00:15:27,199 --> 00:15:31,290
получают доступ к пространству заранее отличные вопросы

408
00:15:31,290 --> 00:15:32,430
первый вопрос: знаем ли мы все

409
00:15:32,430 --> 00:15:34,079
пространство действия заранее да мы

410
00:15:34,079 --> 00:15:36,329
Предположим, что мы делаем, по крайней мере, для всей этой

411
00:15:36,329 --> 00:15:41,939
лекции, и в целом изначально у

412
00:15:41,939 --> 00:15:43,589
нас высокие значения, поэтому НАСА

413
00:15:43,589 --> 00:15:44,910
вычислило, когда оно, вероятно, вычислило

414
00:15:44,910 --> 00:15:46,620
что-то низкое, поэтому в следующий раз, когда вы увидите

415
00:15:46,620 --> 00:15:49,350
это, вы попробуете другое действие, я только что

416
00:15:49,350 --> 00:15:51,809
сделал очень хороший  предложение, которое

417
00:15:51,809 --> 00:15:53,430
относится к тому, как мы инициализируем

418
00:15:53,430 --> 00:15:56,550
Q, поэтому одна вещь, которую вы можете сделать, это

419
00:15:56,550 --> 00:15:59,249
то, что только что было предложено, это то, что вы не можете

420
00:15:59,249 --> 00:16:01,050
инициализировать свою функцию Q действительно высоко

421
00:16:01,050 --> 00:16:03,240
везде, вы можете в основном делать то, что

422
00:16:03,240 --> 00:16:05,160
известно как оптимистическая инициализация, и

423
00:16:05,160 --> 00:16:06,689
это на самом деле может быть  действительно полезная

424
00:16:06,689 --> 00:16:10,410
стратегия для исследования, и если вы видели, как

425
00:16:10,410 --> 00:16:12,389
инициализировать ее определенным образом, то

426
00:16:12,389 --> 00:16:14,939
вы можете получить доказуемые гарантии того,

427
00:16:14,939 --> 00:16:16,559
сколько данных вам понадобится

428
00:16:16,559 --> 00:16:18,930
для достижения оптимальной политики, поэтому

429
00:16:18,930 --> 00:16:20,790
оптимистичная инициализация часто является

430
00:16:20,790 --> 00:16:22,410
действительно хорошей вещью для  сделать, это может быть

431
00:16:22,410 --> 00:16:23,999
немного осторожным с тем, как вы инициализируете

432
00:16:23,999 --> 00:16:25,889
такие вещи, как эти значения,

433
00:16:25,889 --> 00:16:27,449
но в целом эмпирически это действительно

434
00:16:27,449 --> 00:16:28,949
хорошо, и формально это может быть очень хорошо,

435
00:16:28,949 --> 00:16:30,809
мы тоже  Мы не собираемся сегодня говорить об

436
00:16:30,809 --> 00:16:32,730
оптимистичной инициализации, но

437
00:16:32,730 --> 00:16:34,529
мы поговорим позже в классе или поговорим об

438
00:16:34,529 --> 00:16:43,230
оптимизации, но мой вопрос заключается в том,

439
00:16:43,230 --> 00:16:45,029
что мы определяем политику, которую нам

440
00:16:45,029 --> 00:16:46,319
нужно найти с точки зрения состояния,

441
00:16:46,319 --> 00:16:48,509
и если вознаграждение, которое вы получаете от

442
00:16:48,509 --> 00:16:51,089
состояние зависит от вашей истории,

443
00:16:51,089 --> 00:16:54,149
которая нарушает марковское предположение, и,

444
00:16:54,149 --> 00:16:57,240
как бы даже жаль, даже если

445
00:16:57,240 --> 00:17:01,110
награда не марковская, что ваша политика активна,

446
00:17:01,110 --> 00:17:02,610
мы определяем политику, как если бы она

447
00:17:02,610 --> 00:17:06,240
работала да, прямо сейчас это важный

448
00:17:06,240 --> 00:17:08,159
момент, поэтому ваш реальный  мир может быть марковским, а может и не

449
00:17:08,159 --> 00:17:11,250
быть, все политики, о которых мы

450
00:17:11,250 --> 00:17:12,869
сейчас говорим, предполагают, что мир является

451
00:17:12,869 --> 00:17:15,029
марковским.

452
00:17:15,029 --> 00:17:17,220


453
00:17:17,220 --> 00:17:20,520


454
00:17:20,520 --> 00:17:22,349
реальный

455
00:17:22,349 --> 00:17:24,179
мир может быть или не быть MDP, и если

456
00:17:24,179 --> 00:17:26,039
это не так, то вы рассматриваете по

457
00:17:26,039 --> 00:17:27,648
существу ограниченный класс политики,

458
00:17:27,648 --> 00:17:29,640
рассматривая только сопоставления из

459
00:17:29,640 --> 00:17:31,740
непосредственного состояния в действие, и если

460
00:17:31,740 --> 00:17:33,210
вы действительно должны делать  зависит от

461
00:17:33,210 --> 00:17:34,679
всей истории, тогда вы можете не принимать

462
00:17:34,679 --> 00:17:38,920
хороших решений, хорошо,

463
00:17:38,920 --> 00:17:41,260
так что это своего рода основной

464
00:17:41,260 --> 00:17:42,820
способ расширения Монте-Карло, чтобы иметь возможность

465
00:17:42,820 --> 00:17:44,230
начать оценивать очереди, и как только они у вас

466
00:17:44,230 --> 00:17:45,280
есть, вы можете улучшить политику,

467
00:17:45,280 --> 00:17:47,110
но теперь ясно  что нам

468
00:17:47,110 --> 00:17:49,060
нужно что-то сделать с точки зрения того, как мы

469
00:17:49,060 --> 00:17:50,950
должны получить опыт, чтобы мы могли

470
00:17:50,950 --> 00:17:52,390
действительно улучшить, когда мы пытаемся сделать это

471
00:17:52,390 --> 00:17:56,290
улучшение политики, потому что в искусстве сейчас мы

472
00:17:56,290 --> 00:17:57,760
не знаем, как работает реальная динамика

473
00:17:57,760 --> 00:18:02,200
мира, поэтому нам нужно делать  своего

474
00:18:02,200 --> 00:18:03,940
рода чередование оценки

475
00:18:03,940 --> 00:18:07,210
и улучшения политики, и нам также нужно

476
00:18:07,210 --> 00:18:09,280
подумать о том, как мы делаем этот

477
00:18:09,280 --> 00:18:16,510
аспект исследования, так что в целом это

478
00:18:16,510 --> 00:18:17,830
может показаться немного тонким,

479
00:18:17,830 --> 00:18:19,450
поэтому мы уже слышали одно хорошее

480
00:18:19,450 --> 00:18:21,160
предложение, например, вы могли бы  не

481
00:18:21,160 --> 00:18:22,990
инициализируйте все оптимистично, и,

482
00:18:22,990 --> 00:18:24,430
возможно, это поможет вам изучить

483
00:18:24,430 --> 00:18:28,240
это, но в целом может показаться, что

484
00:18:28,240 --> 00:18:29,650
это немного сложно, как мы

485
00:18:29,650 --> 00:18:31,270
собираемся получить эту хорошую оценку Q pi,

486
00:18:31,270 --> 00:18:34,900
потому что Q PI делает это  говорит, что если вы

487
00:18:34,900 --> 00:18:36,550
хотите действительно хорошую оценку Q PI

488
00:18:36,550 --> 00:18:40,120
Si для всех s и всех a, это будет означать, что

489
00:18:40,120 --> 00:18:41,590
вам нужно добраться до каждого другого

490
00:18:41,590 --> 00:18:43,510
состояния, предпринять все возможные действия, а

491
00:18:43,510 --> 00:18:46,090
затем следовать PI с этого момента, и так

492
00:18:46,090 --> 00:18:47,680
как я могу убедиться  то, что я посещаю все

493
00:18:47,680 --> 00:18:50,770
эти вещи, и мы собираемся поговорить

494
00:18:50,770 --> 00:18:52,090
о них сегодня, является своего рода очень простой

495
00:18:52,090 --> 00:18:54,520
стратегией, чтобы убедиться, что вы посещаете

496
00:18:54,520 --> 00:18:57,190
вещи, которые обычно работают в некоторых

497
00:18:57,190 --> 00:18:58,690
мягких условиях относительно основного

498
00:18:58,690 --> 00:19:04,840
процесса, поэтому действительно простая идея состоит в том, чтобы

499
00:19:04,840 --> 00:19:07,000
сбалансировать исследование  и эксплуатация,

500
00:19:07,000 --> 00:19:09,670
будучи случайной некоторое время, поэтому давайте

501
00:19:09,670 --> 00:19:11,080
представим, что существует конечное число

502
00:19:11,080 --> 00:19:12,370
действий, которые мы собираемся назвать

503
00:19:12,370 --> 00:19:15,070
кардинальностью, а здесь, тогда политика Egret II

504
00:19:15,070 --> 00:19:16,780
в отношении

505
00:19:16,780 --> 00:19:23,410
значения действия состояния выглядит следующим образом с вероятностью один

506
00:19:23,410 --> 00:19:24,880
минус эпсилон

507
00:19:24,880 --> 00:19:26,970
вы'  Если вы собираетесь предпринять наилучшее действие в

508
00:19:26,970 --> 00:19:28,840
соответствии с вашей текущей политикой,

509
00:19:28,840 --> 00:19:31,030
это не функция значения действия состояния,

510
00:19:31,030 --> 00:19:36,430
а в противном случае вы собираетесь предпринять

511
00:19:36,430 --> 00:19:43,240
действие с вероятностью эпсилон,

512
00:19:43,240 --> 00:19:46,350
деленной на

513
00:19:46,700 --> 00:19:48,499
с вероятностью 1 минус Ep  silon

514
00:19:48,499 --> 00:19:49,879
вы берете то, что в данный момент считаете лучшим в

515
00:19:49,879 --> 00:19:51,919
соответствии с вашей группой или вашей

516
00:19:51,919 --> 00:19:53,840
оценкой функции Q, и с вероятностью

517
00:19:53,840 --> 00:19:56,869
эпсилон вы выбираете одно из других

518
00:19:56,869 --> 00:20:02,450
действий, так что это довольно простая стратегия,

519
00:20:02,450 --> 00:20:04,249
и приятно то, что ее все еще

520
00:20:04,249 --> 00:20:08,090
достаточно, но прежде чем мы это сделаем

521
00:20:08,090 --> 00:20:09,859
почему бы нам просто не сделать краткий пример, чтобы

522
00:20:09,859 --> 00:20:11,149
убедиться, что мы находимся на одной странице, так что

523
00:20:11,149 --> 00:20:13,129
давайте подумаем о том, как мы будем проводить Монте-

524
00:20:13,129 --> 00:20:15,859
Карло для оценки политики Q для нашего

525
00:20:15,859 --> 00:20:19,489
маленького марсохода, так что теперь наш марсоход

526
00:20:19,489 --> 00:20:21,350
собирается  есть две вещи, которые можно сделать

527
00:20:21,350 --> 00:20:23,929
вместо , и мы собираемся больше рассуждать

528
00:20:23,929 --> 00:20:26,389
об этом, поэтому я записал

529
00:20:26,389 --> 00:20:27,919
здесь функцию вознаграждения, говоря, что если

530
00:20:27,919 --> 00:20:30,230
вы предпримете действие a1, вы получите те же

531
00:20:30,230 --> 00:20:31,580
вознаграждения, о которых мы говорили ранее

532
00:20:31,580 --> 00:20:35,869
, а именно  1 0 0 0 плюс 10, и теперь я

533
00:20:35,869 --> 00:20:37,340
меняю это, я говорю, хорошо, что вы на

534
00:20:37,340 --> 00:20:39,470
самом деле и зависит ли ваша награда

535
00:20:39,470 --> 00:20:41,090
от вашего состояния и действия, которое вы предпринимаете,

536
00:20:41,090 --> 00:20:44,119
и поэтому действие для a2 теперь будет

537
00:20:44,119 --> 00:20:45,649
вы получите ноль везде  тогда вы получите

538
00:20:45,649 --> 00:20:49,549
плюс пять в конце и гамма  равно 1, теперь

539
00:20:49,549 --> 00:20:51,049
давайте предположим, что наша текущая жадная

540
00:20:51,049 --> 00:20:54,859
политика заключается в том, что вы выполняете действие a1 везде,

541
00:20:54,859 --> 00:20:57,230
и что мы используем эпсилон 0,5,

542
00:20:57,230 --> 00:20:59,749
и мы выбираем траекторию из

543
00:20:59,749 --> 00:21:02,090
жадной политики e, и снова что

544
00:21:02,090 --> 00:21:03,980
здесь означает политика аннуитета: я установил эпсилон

545
00:21:03,980 --> 00:21:06,980
равным 0,5, что означает, что в половине

546
00:21:06,980 --> 00:21:09,019
случаев мы будем использовать нашу текущую жадную

547
00:21:09,019 --> 00:21:12,109
политику действий a1, а в другой

548
00:21:12,109 --> 00:21:13,909
половине времени мы будем использовать либо a1, либо

549
00:21:13,909 --> 00:21:16,909
a2, так что в качестве

550
00:21:16,909 --> 00:21:18,830
примера это приведет к такой траектории, как

551
00:21:18,830 --> 00:21:23,239
состояние 3 действие a1 0 состояние 2 и теперь

552
00:21:23,239 --> 00:21:25,480
это случай, когда мы делаем выборку случайным образом,

553
00:21:25,480 --> 00:21:28,489
поэтому мы подбрасываем монету, мы сказали, о, на этот

554
00:21:28,489 --> 00:21:30,109
раз я буду случайным, тогда мне нужно

555
00:21:30,109 --> 00:21:31,549
снова подбросить монету, чтобы увидеть,

556
00:21:31,549 --> 00:21:33,289
предприняв действие a1 или a2, и я взял ци,

557
00:21:33,289 --> 00:21:36,409
я получил награду в 0, и это, а

558
00:21:36,409 --> 00:21:38,619
затем остальная часть траектории следующим образом,

559
00:21:38,619 --> 00:21:41,239
и мой вопрос к вам и людям, с которыми вы

560
00:21:41,239 --> 00:21:43,519
разговаривали с соседом, конечно, это то,

561
00:21:43,519 --> 00:21:46,429
что сейчас является оценкой Q для всех  состояния для

562
00:21:46,429 --> 00:21:49,609
обоих действий a1 и действия a2 в

563
00:21:49,609 --> 00:21:51,739
конце этой траектории, используя Монте  Карло

564
00:21:51,739 --> 00:21:55,460
оценивает, поэтому мы делаем первый визит в

565
00:21:55,460 --> 00:21:57,789
этом случае,

566
00:23:15,850 --> 00:23:20,749
да, вопрос о действиях, которые мы

567
00:23:20,749 --> 00:23:25,190
выбираем, фаза эпсилон важна для

568
00:23:25,190 --> 00:23:29,360
женщин, действие на политической фазе

569
00:23:29,360 --> 00:23:31,610
исследования, или мы должны исключить это

570
00:23:31,610 --> 00:23:33,830
действие из наших действий, что нам

571
00:23:33,830 --> 00:23:36,460
нравится, просто увеличьте руки  изучение

572
00:23:36,460 --> 00:23:39,320
вопроса политики заключается в том, следует ли, когда

573
00:23:39,320 --> 00:23:41,360
вы нажмете сейчас, сделать что-то случайное

574
00:23:41,360 --> 00:23:43,159
, следует ли вам исключить действие,

575
00:23:43,159 --> 00:23:44,330
которое вы обычно предпринимаете, если

576
00:23:44,330 --> 00:23:46,909
вы жадны, вы могли бы в некотором

577
00:23:46,909 --> 00:23:47,929
роде это как просто шестерня другой

578
00:23:47,929 --> 00:23:52,669
эпсилон Я слышу меньше разговоров, чем обычно

579
00:23:52,669 --> 00:23:54,080
так что у него могут быть какие-либо уточняющие

580
00:23:54,080 --> 00:24:05,480
вопросы по этому поводу или что-то в этом роде, если

581
00:24:05,480 --> 00:24:07,999
все готовы ответить, так что

582
00:24:07,999 --> 00:24:12,919
вы, ребята, думаете, так что у вас нет в этом

583
00:24:12,919 --> 00:24:15,830
случае s3, ну, вы инициализируете,

584
00:24:15,830 --> 00:24:17,419
все здесь для начала прямо сейчас, так

585
00:24:17,419 --> 00:24:19,100
что все, что вы сделали  не каждое действие состояния,

586
00:24:19,100 --> 00:24:20,899
и вы не видели, останется

587
00:24:20,899 --> 00:24:24,200
нулем, да, и тогда конкретный куб, как

588
00:24:24,200 --> 00:24:25,940
3 1, будет 0, потому что вы видели, что

589
00:24:25,940 --> 00:24:28,549
когда-то в новой стороне было 0 Q s до

590
00:24:28,549 --> 00:24:31,639
8  также быть 0 P of S 3 a 1 будет 0,

591
00:24:31,639 --> 00:24:33,049
и тогда единственное, что будет

592
00:24:33,049 --> 00:24:35,480
ненулевым, будет Q of s 1 a 1, что в

593
00:24:35,480 --> 00:24:37,789
этом случае будет 1, потому что вы решаете

594
00:24:37,789 --> 00:24:41,299
один раз, и награда, которую вы получили на вашей стороне,

595
00:24:41,299 --> 00:24:44,360
была  тот, который является одним ответом, может иметь

596
00:24:44,360 --> 00:24:46,809
другой ответ,

597
00:24:48,070 --> 00:24:53,509
поэтому мы должны прийти к логическому значению, поэтому все

598
00:24:53,509 --> 00:24:55,039
заявления о действиях состояния, которые они

599
00:24:55,039 --> 00:24:57,559
видели, будут равны 1, а все остальные

600
00:24:57,559 --> 00:24:59,960
пары действий состояния будут равны 0 и другой

601
00:24:59,960 --> 00:25:02,010
ответ,

602
00:25:02,010 --> 00:25:04,350
так что вы говорите  было бы правильно

603
00:25:04,350 --> 00:25:06,929
для случая TD то, что вы сказали,

604
00:25:06,929 --> 00:25:08,730
было бы правильно для прошлой недели или

605
00:25:08,730 --> 00:25:10,230
вчера, все в порядке в понедельник, я думаю, у него

606
00:25:10,230 --> 00:25:16,830
может быть третий ответ, не могли бы вы повторить,

607
00:25:16,830 --> 00:25:19,950
что то же самое первый вариант, что

608
00:25:19,950 --> 00:25:23,520
мы обновляем только s 1 a 1  второй вариант

609
00:25:23,520 --> 00:25:25,740
заключается в том, что все, что мы видели, теперь

610
00:25:25,740 --> 00:25:27,600
будет одним, и, возможно, я неправильно понял,

611
00:25:27,600 --> 00:25:28,799
поэтому у нас будет два

612
00:25:28,799 --> 00:25:31,309
разных вектора, теперь у нас есть два вектора, поэтому у

613
00:25:31,309 --> 00:25:35,130
нас есть Q, равный 1, и Q, равный 2, и

614
00:25:35,130 --> 00:25:36,990
они'  Мы не будем выглядеть одинаково,

615
00:25:36,990 --> 00:25:40,620
поэтому иногда мы действуем 1, а

616
00:25:40,620 --> 00:25:42,840
иногда действуем 2 и  мы можем

617
00:25:42,840 --> 00:25:46,110
только обновить то, что мы видели, результаты

618
00:25:46,110 --> 00:25:49,679
действий, которые мы предприняли, поэтому какие действия

619
00:25:49,679 --> 00:25:55,980
мы предприняли для s3, просто 1, так что это

620
00:25:55,980 --> 00:25:57,990
означает, что для тех, что для s3, это

621
00:25:57,990 --> 00:26:01,500
будет 1 меньше для Q из s 3 a 1, так что я '

622
00:26:01,500 --> 00:26:06,000
заполним все те, которые равны 0, мы

623
00:26:06,000 --> 00:26:10,020
когда-либо брали 2 и s 3, нет, так что это

624
00:26:10,020 --> 00:26:11,880
также должно быть 0, потому что мы никогда не начинали

625
00:26:11,880 --> 00:26:14,570
там, действуем 2 и получаем ответ

626
00:26:14,570 --> 00:26:16,890
что насчет того, какое действие мы предпринимаем

627
00:26:16,890 --> 00:26:24,410
из s2 правильно, так что для этого мы получаем 1,

628
00:26:27,290 --> 00:26:30,210
поэтому мы в основном распределяем ваш

629
00:26:30,210 --> 00:26:31,740
опыт, так что теперь, если вы собираетесь

630
00:26:31,740 --> 00:26:33,540
взять максимум над ними, вы получите

631
00:26:33,540 --> 00:26:35,250
то же самое, что мы видели в прошлый раз для

632
00:26:35,250 --> 00:26:38,040
Монте-Карло, что будет 1 1  1 0 0 0 0

633
00:26:38,040 --> 00:26:41,610
0 до конца, но здесь мы разделяем

634
00:26:41,610 --> 00:26:44,010
наши образцы, чтобы вы могли получить

635
00:26:44,010 --> 00:26:45,450
опыт только за действие, которое вы

636
00:26:45,450 --> 00:26:48,570
фактически совершили в штате, и, поскольку

637
00:26:48,570 --> 00:26:49,710
мы находимся в деле марширующего карло, мы

638
00:26:49,710 --> 00:26:51,570
увидим  Телевизионный кейс или q-обучение, мы назовем

639
00:26:51,570 --> 00:26:54,720
это позже, тогда мы суммируем все

640
00:26:54,720 --> 00:26:57,090
награды до конца эпизода, так что G

641
00:26:57,090 --> 00:26:58,530
здесь будет суммой ов.  все

642
00:26:58,530 --> 00:27:01,290
эти шаги, и я не уточнил, были

643
00:27:01,290 --> 00:27:03,450
хорошими, и мы сохраняем гамму равной

644
00:27:03,450 --> 00:27:05,730
1 просто для того, чтобы вся математика, которую мы просто

645
00:27:05,730 --> 00:27:08,059


646
00:27:10,290 --> 00:27:21,990
добавили, касалась того, делаем ли

647
00:27:21,990 --> 00:27:23,400
мы каждое посещение, если что-то здесь изменится,

648
00:27:23,400 --> 00:27:25,740
извините меня  в этом случае это не изменится,

649
00:27:25,740 --> 00:27:28,470
потому что оба раза, когда вы

650
00:27:28,470 --> 00:27:30,780
посещали s3, летние награды до

651
00:27:30,780 --> 00:27:32,640
конца эпизода были одним, поэтому у вас было бы два

652
00:27:32,640 --> 00:27:35,390
счета за один, а затем вы делите на два,

653
00:27:35,390 --> 00:27:38,070
это действительно может быть по-другому,

654
00:27:38,070 --> 00:27:39,360
но в основном это  отличается, если вы

655
00:27:39,360 --> 00:27:40,740
где-то получили разницу Награды с того момента и

656
00:27:40,740 --> 00:27:49,800
до конца эпизода да, может быть, я

657
00:27:49,800 --> 00:27:51,360
неправильно понял, поэтому я подумал, что вы

658
00:27:51,360 --> 00:27:53,010
должны были сказать, что все было, и

659
00:27:53,010 --> 00:27:54,600
я пропустил это вы сказали, что это было по-

660
00:27:54,600 --> 00:27:58,920
разному для двух действий, которые мы

661
00:27:58,920 --> 00:28:02,370
видели  в дереве проекта r0a да, извините

662
00:28:02,370 --> 00:28:10,590
за это, хорошо, хорошо, так что теперь мы

663
00:28:10,590 --> 00:28:12,240
собираемся официально показать, что это

664
00:28:12,240 --> 00:28:15,180
делает правильно, поэтому мы собираемся показать

665
00:28:15,180 --> 00:28:18,300
доказуемо, как то, что мы делали раньше,

666
00:28:18,300 --> 00:28:20,910
когда мы занимались улучшением политики,

667
00:28:20,910 --> 00:28:23,420
мы показывали, что  если ты  Я выбрал политику,

668
00:28:23,420 --> 00:28:28,290
которая была сгенерирована жадностью

669
00:28:28,290 --> 00:28:30,600
по отношению к вашей сигнальной функции, и

670
00:28:30,600 --> 00:28:32,340
которая гарантированно давала монотонное

671
00:28:32,340 --> 00:28:34,020
улучшение, и то же самое

672
00:28:34,020 --> 00:28:36,120
будет верно и здесь, когда вы едите

673
00:28:36,120 --> 00:28:39,150
жадно, поэтому, если вы используете что-то вроде жадности

674
00:28:39,150 --> 00:28:41,400
политики, то вы можете собрать данные таким образом

675
00:28:41,400 --> 00:28:44,670
, чтобы новая политика получила новое значение,

676
00:28:44,670 --> 00:28:46,740
если вы оптимистичны в отношении

677
00:28:46,740 --> 00:28:47,880
этого или извините, если вы жадны в

678
00:28:47,880 --> 00:28:49,530
отношении этого, тогда вы получите

679
00:28:49,530 --> 00:28:52,520
новую лучшую политику, хорошо, так что давайте  скажем,

680
00:28:52,520 --> 00:28:56,700
у нас есть политика e Grady PI I, а затем

681
00:28:56,700 --> 00:28:59,040
мы собираемся вызвать политику Egret по

682
00:28:59,040 --> 00:29:01,830
отношению к Q PI I, которая будет PI

683
00:29:01,830 --> 00:29:04,290
плюс один, поэтому у нас была жадная жадная

684
00:29:04,290 --> 00:29:06,420
политика pi i, которая выполняла некоторое

685
00:29:06,420 --> 00:29:07,680
количество исследований и  некоторое количество

686
00:29:07,680 --> 00:29:10,200
жадности в прошлом мы используем это для

687
00:29:10,200 --> 00:29:12,180
сбора данных мы затем оценили эту

688
00:29:12,180 --> 00:29:15,330
политику мы получили этот Q PI I и теперь мы

689
00:29:15,330 --> 00:29:16,830
собираемся извлечь новую политику мы собираемся

690
00:29:16,830 --> 00:29:18,600
улучшить политику и мы собираемся

691
00:29:18,600 --> 00:29:20,130
показать что это  моноатомное улучшение

692
00:29:20,130 --> 00:29:23,010
хорошо так много вопросов о  то, что

693
00:29:23,010 --> 00:29:24,090
мы показываем,

694
00:29:24,090 --> 00:29:28,050
хорошо, так что это значит, так что прямо сейчас

695
00:29:28,050 --> 00:29:29,340
мы попытаемся показать,

696
00:29:29,340 --> 00:29:33,540
что это функция Q от PI of s PI

697
00:29:33,540 --> 00:29:36,570
I плюс 1, так что это будет лучше, чем

698
00:29:36,570 --> 00:29:39,030
наше предыдущее значение, по крайней мере, так же хорошо  или

699
00:29:39,030 --> 00:29:40,860
лучше, чем наше предыдущее значение нашей

700
00:29:40,860 --> 00:29:44,940
старой политики PI I, поэтому то, как мы определили

701
00:29:44,940 --> 00:29:48,960
это, теперь функция Q здесь

702
00:29:48,960 --> 00:29:51,780
будет суммой по нашей политике

703
00:29:51,780 --> 00:29:55,830
стохастика, поэтому это PI I плюс 1

704
00:29:55,830 --> 00:29:57,180
вероятности, что мы предпримем действие в

705
00:29:57,180 --> 00:30:03,350
определенное время состояния Q PI I Si, а

706
00:30:03,350 --> 00:30:05,280
затем мы собираемся расширить это и

707
00:30:05,280 --> 00:30:07,290
переопределить с точки зрения того

708
00:30:07,290 --> 00:30:12,600
, что значит быть ингредиентом

709
00:30:12,600 --> 00:30:14,310
и жадной политикой, мы либо берем что-то

710
00:30:14,310 --> 00:30:15,690
случайным образом, и это, вероятно, с

711
00:30:15,690 --> 00:30:17,670
эпсилон  и мы разделяем нашу вероятностную

712
00:30:17,670 --> 00:30:19,710
массу на все действия, так что вот

713
00:30:19,710 --> 00:30:22,470
как мы получаем это уравнение, так что это говорит,

714
00:30:22,470 --> 00:30:26,790
что это случайная часть, поэтому

715
00:30:26,790 --> 00:30:28,530
с вероятностью с вероятностью

716
00:30:28,530 --> 00:30:32,700
эпсилон мы выполняем одно действие одно из

717
00:30:32,700 --> 00:30:35,220
действий, а затем мы будем следовать этому

718
00:30:35,220 --> 00:30:36,750
из этого  знания, так что это просто Q PI

719
00:30:36,750 --> 00:30:40,020
I из  Si, а затем с вероятностью один

720
00:30:40,020 --> 00:30:44,610
минус эпсилон, мы жадны, мы следуем

721
00:30:44,610 --> 00:30:46,980
лучшему действию в соответствии с нашим текущим q

722
00:30:46,980 --> 00:30:51,720
pi i, поэтому теперь

723
00:30:51,720 --> 00:30:53,880
мы собираемся переписать, что первый

724
00:30:53,880 --> 00:30:55,230
член не изменится.  и я

725
00:30:55,230 --> 00:30:58,220
собираюсь расширить второй.

726
00:31:05,540 --> 00:31:16,190
Я ничего здесь не делал, я просто

727
00:31:16,190 --> 00:31:20,000
умножил последний член на единицу, но я

728
00:31:20,000 --> 00:31:21,770
выразил 1 как 1 по эпсилону,

729
00:31:21,770 --> 00:31:23,360
деленному на 1 по эпсилону, и теперь я

730
00:31:23,360 --> 00:31:26,270
собираюсь повторно выразить эту часть, так что  Я не

731
00:31:26,270 --> 00:31:29,120
собираюсь переписывать первое слагаемое плюс 1

732
00:31:29,120 --> 00:31:36,890
минус эпсилон макс за некоторое время, когда я

733
00:31:36,890 --> 00:31:39,580
собираюсь переписать это, поскольку

734
00:31:50,620 --> 00:31:53,990
он будет использовать тот факт, что всякий раз, когда мы

735
00:31:53,990 --> 00:31:56,270
определяем нашу политику справедливости, если вы суммируете

736
00:31:56,270 --> 00:31:58,910
все действия в определенном состоянии, эти

737
00:31:58,910 --> 00:32:00,380
все вероятности того, что мы предпримем эти

738
00:32:00,380 --> 00:32:01,940
действия в этом состоянии, поэтому они должны в сумме

739
00:32:01,940 --> 00:32:06,530
равняться единице, поэтому я просто сначала купил его, я просто

740
00:32:06,530 --> 00:32:08,540
умножил на один в / - Выразите мне, что

741
00:32:08,540 --> 00:32:09,890
это 1 минус эпсилон, деленное на 1 минус

742
00:32:09,890 --> 00:32:12,440
эпсилон, затем я переоценил  1, потому что

743
00:32:12,440 --> 00:32:14,540
это должно быть равно 1, потому что мы

744
00:32:14,540 --> 00:32:15,830
должны предпринять какое-то действие в конкретном

745
00:32:15,830 --> 00:32:17,990
Sta  т. е. наша политика всегда предполагает, что

746
00:32:17,990 --> 00:32:19,280
вероятность того, что мы предпримем какие-либо действия

747
00:32:19,280 --> 00:32:22,340
, должна быть равна 1, и тогда я

748
00:32:22,340 --> 00:32:25,730
сделаю это выражение, потому что

749
00:32:25,730 --> 00:32:27,200
мы находимся здесь, где мы предпримем

750
00:32:27,200 --> 00:32:29,840
наилучшие действия, поэтому по определению наилучшее

751
00:32:29,840 --> 00:32:31,370
действие имеет  чтобы быть по крайней мере таким же хорошим, как

752
00:32:31,370 --> 00:32:34,460
выполнение любых других действий, поэтому мы

753
00:32:34,460 --> 00:32:37,280
собираемся сделать следующее: мы втолкнем

754
00:32:37,280 --> 00:32:58,070
этот Q внутрь, так что он должен быть меньше,

755
00:32:58,070 --> 00:32:59,450
чем то, что мы видели раньше, потому что в

756
00:32:59,450 --> 00:33:01,400
основном мы просто вталкиваем Q внутрь, и

757
00:33:01,400 --> 00:33:04,160
мы  мы больше не берем это максимальное значение, а

758
00:33:04,160 --> 00:33:06,380
значения Q все значения Q, которые лучше всего

759
00:33:06,380 --> 00:33:07,760
должны быть равны максимальному, в других случаях

760
00:33:07,760 --> 00:33:10,550
они, как правило, будут хуже, но затем, как

761
00:33:10,550 --> 00:33:12,200
только мы получим это, мы можем отменить это значение

762
00:33:12,200 --> 00:33:14,420
больше эпсилон минус 1 больше  Эпсилон, что

763
00:33:14,420 --> 00:33:16,010
у нас есть, у нас есть два разных термина,

764
00:33:16,010 --> 00:33:20,200
которые выглядят очень похожими, у нас есть,

765
00:33:28,180 --> 00:33:33,700
давайте посмотрим, встретимся с одной ошибкой в этой части, да,

766
00:33:36,670 --> 00:33:40,640
прямо здесь есть эпсилон, та 

767
00:33:40,640 --> 00:33:46,210
что теперь я собираюсь это вытащить, по 

768
00:34:22,129 --> 00:34:24,899
тому я разделил эти термины на части.  первый член

769
00:34:24,899 --> 00:34:27,360
и третий член идентичны, и один

770
00:34:27,360 --> 00:34:31,800
вычитается  один добавляется, конечно, это

771
00:34:31,800 --> 00:34:34,080
ясно, и поэтому это просто

772
00:34:34,080 --> 00:34:45,690
становится средним термином, и это было просто

773
00:34:45,690 --> 00:34:50,280
предыдущее значение, да, мы изменили его на

774
00:34:50,280 --> 00:34:53,100
вместо суммы по всем a числа Pi I

775
00:34:53,100 --> 00:34:55,320
заданное s минус эпсилон 2 минус эпсилон

776
00:34:55,320 --> 00:34:59,900
по мощности K  в этом случае да

777
00:35:09,320 --> 00:35:12,720
да, то, что мы сделали от 1 минус

778
00:35:12,720 --> 00:35:16,859
эпсилон до следующего слайда, поэтому у нас был 1

779
00:35:16,859 --> 00:35:18,390
минус эпсилон, разделенный на 1 минус эпсилон,

780
00:35:18,390 --> 00:35:22,590
и я понял сумму по PI I

781
00:35:22,590 --> 00:35:25,109
данного s минус эпсилон, деленную на сумму

782
00:35:25,109 --> 00:35:27,420
по  а, а затем, если вы просуммируете этот

783
00:35:27,420 --> 00:35:29,760
второй член так же, как эпсилон, а

784
00:35:29,760 --> 00:35:36,240
первый член равен 1, да, вы можете зарифмовать свое

785
00:35:36,240 --> 00:35:38,330


786
00:35:39,710 --> 00:35:52,200
имя своего рода борьбой, хорошо, пока, я плюс 1, какой

787
00:35:52,200 --> 00:36:09,089
вкладыш вы думаете, да я все еще не

788
00:36:09,089 --> 00:36:10,560
говорю  ваш вопрос, который, так что вы находитесь

789
00:36:10,560 --> 00:36:15,359
во второй строке, таков, что правильно или

790
00:36:15,359 --> 00:36:17,700
первое предложение в первом нормальном PI плюс

791
00:36:17,700 --> 00:36:39,720
один да, да, больше, чем

792
00:36:39,720 --> 00:36:41,010
гранулированное и равное, потому что мы

793
00:36:41,010 --> 00:36:43,829
толкаем этот Q PI, у нас был максимум над Q PI

794
00:36:43,829 --> 00:36:46,829
I  мы поместили его внутрь Солнца,

795
00:36:46,829 --> 00:36:49,290
и теперь некоторые из них больше не включают

796
00:36:49,290 --> 00:36:54,300
в себя максимальное количество  d так что теперь максимальное значение всегда

797
00:36:54,300 --> 00:36:55,589
больше или равно любому из

798
00:36:55,589 --> 00:36:57,990
других элементов, так что вот где вы

799
00:36:57,990 --> 00:37:07,710
получили большее или равное да для

800
00:37:07,710 --> 00:37:09,900
некоторых случайных, поэтому возможных действий, и тогда

801
00:37:09,900 --> 00:37:13,650
вы получите больше метоники, да, так что

802
00:37:13,650 --> 00:37:15,030
мы можем получить  некоторая интуиция, это что-то

803
00:37:15,030 --> 00:37:18,300
вроде алгебраического вывода, я думаю, что

804
00:37:18,300 --> 00:37:19,980
интуитивно идея состоит в том, что, выполняя

805
00:37:19,980 --> 00:37:22,440
какое-то жадное исследование, вы

806
00:37:22,440 --> 00:37:23,970
получите свидетельство о некоторых других

807
00:37:23,970 --> 00:37:26,190
парах действий состояния, а затем вы можете использовать их

808
00:37:26,190 --> 00:37:27,810
для оценки вашей Q-функции и что,

809
00:37:27,810 --> 00:37:29,910
когда вы  сделайте это, тогда это также

810
00:37:29,910 --> 00:37:32,339
даст вам то, что вы можете

811
00:37:32,339 --> 00:37:33,599
улучшить свою политику, потому что тогда у вас есть

812
00:37:33,599 --> 00:37:34,890
доказательства того, что есть что-то лучшее, что

813
00:37:34,890 --> 00:37:36,390
вы могли бы сделать, чем то,

814
00:37:36,390 --> 00:37:37,980
что вы делаете в настоящее время, если вы не

815
00:37:37,980 --> 00:37:40,260
проводите никаких исследований, которые вы

816
00:37:40,260 --> 00:37:42,450


817
00:37:42,450 --> 00:37:44,339


818
00:37:44,339 --> 00:37:45,660


819
00:37:45,660 --> 00:37:47,190
Если вы считаете, что ваша Q-функция не изменится по сравнению с предыдущей, но теперь, поскольку вы занимаетесь исследованием, вы можете узнать о других вещах, а затем, если это лучше, вы увидите это в своей Q-

820
00:37:47,190 --> 00:37:49,310
функции,

821
00:37:49,809 --> 00:37:51,640
поэтому, когда вы думаете, что это делает биток

822
00:37:51,640 --> 00:37:54,009
фу  Скажите, что ваше исследование не

823
00:37:54,009 --> 00:37:56,579
так хорошо, тогда вы просто берете старые

824
00:37:56,579 --> 00:37:59,140
значения,

825
00:37:59,140 --> 00:38:02,979
да, так что теперь это говорит о том, что

826
00:38:02,979 --> 00:38:04,900
вы получите это монотонное улучшение, если

827
00:38:04,900 --> 00:38:09,009
будете вычислять это точно, так что

828
00:38:09,009 --> 00:38:13,690
это важная часть, поэтому это шоу так  что

829
00:38:13,690 --> 00:38:15,459
это показывает здесь, так это то, что если вы получаете функцию подсказки

830
00:38:15,459 --> 00:38:17,170
и похоже, что есть

831
00:38:17,170 --> 00:38:18,880
некоторое улучшение по сравнению с некоторыми другими действиями,

832
00:38:18,880 --> 00:38:20,140
которые вы не предпринимаете прямо сейчас, вы

833
00:38:20,140 --> 00:38:21,940
собираетесь сместить свою политику в сторону

834
00:38:21,940 --> 00:38:24,190
сосредоточения на тех действиях, которые она

835
00:38:24,190 --> 00:38:25,390
предполагает прямо сейчас.  с точки зрения

836
00:38:25,390 --> 00:38:27,039
монотонного улучшения, то, что ключевые PI

837
00:38:27,039 --> 00:38:29,200
были вычислены точно, так что это

838
00:38:29,200 --> 00:38:31,749
то, что мы видели, когда мы занимались планированием,

839
00:38:31,749 --> 00:38:33,309
когда мы знали, что модель динамики

840
00:38:33,309 --> 00:38:34,569
была в модели вознаграждения, и мы

841
00:38:34,569 --> 00:38:37,650
использовали это для вычисления функции ценности,

842
00:38:37,650 --> 00:38:40,059
поэтому, если бы мы  делая в этом случае, мы

843
00:38:40,059 --> 00:38:41,680
имели гарантированное монотонное улучшение,

844
00:38:41,680 --> 00:38:45,039
потому что у нас было точное значение pi, и

845
00:38:45,039 --> 00:38:46,959
аналогично здесь, если у нас есть точное

846
00:38:46,959 --> 00:38:49,779
значение q pi, тогда, когда вы делаете это

847
00:38:49,779 --> 00:38:51,759
улучшение, вам гарантировано t  o быть

848
00:38:51,759 --> 00:38:54,369
монотонно улучшающимся, если вам не

849
00:38:54,369 --> 00:38:56,680
понравилось, если у вас есть только приблизительное значение

850
00:38:56,680 --> 00:39:00,130
q pi, тогда оно может быть немонотонным,

851
00:39:00,130 --> 00:39:01,839
например, скажем, вы попробовали другое действие

852
00:39:01,839 --> 00:39:04,420
один раз в этом состоянии, у вас может быть плохая

853
00:39:04,420 --> 00:39:06,009
оценка того, насколько хорошо обстоят дела с

854
00:39:06,009 --> 00:39:08,229
этой точки  так что это важный

855
00:39:08,229 --> 00:39:10,059
аспект, и это будет действительно

856
00:39:10,059 --> 00:39:11,079
важно, когда мы начнем думать об

857
00:39:11,079 --> 00:39:12,849
аппроксимации функции, потому что мы почти

858
00:39:12,849 --> 00:39:15,039
никогда не вычислим q pi I точно,

859
00:39:15,039 --> 00:39:16,900
но если вы это сделаете, скажем, вы можете

860
00:39:16,900 --> 00:39:18,190
просто перебрать это тонну  раз,

861
00:39:18,190 --> 00:39:19,900
когда вы изучаете, что это все еще

862
00:39:19,900 --> 00:39:22,150
табличная среда, вы сошлись, вы

863
00:39:22,150 --> 00:39:24,519
знаете, что ваш q pi идеален, тогда, когда вы

864
00:39:24,519 --> 00:39:27,819
затем делаете улучшение политики, вы можете получить

865
00:39:27,819 --> 00:39:29,829
выгоду, которую вы можете, и поэтому будет

866
00:39:29,829 --> 00:39:31,180
этот интересный вопрос о том, как часто

867
00:39:31,180 --> 00:39:32,890
вы совершенствуетесь  ваша политика по сравнению с тем,

868
00:39:32,890 --> 00:39:34,359
сколько времени вы тратите на оценку вашей

869
00:39:34,359 --> 00:39:39,190
текущей политики да, может быть, это хорошо,

870
00:39:39,190 --> 00:39:41,709
определенно сходится к оптимальной

871
00:39:41,709 --> 00:39:45,219
общественной функции Q, что в целом

872
00:39:45,219 --> 00:39:46,809
идеально, да, мы поговорим об этом

873
00:39:46,809 --> 00:39:48,729
вопрос отличный, так что это просто говорит о

874
00:39:48,729 --> 00:39:50,709
монотонном улучшении на один шаг,

875
00:39:50,709 --> 00:39:52,209
что произойдет с точки зрения общей

876
00:39:52,209 --> 00:39:53,199
конверсии, поэтому мы поговорим об этом

877
00:39:53,199 --> 00:39:56,410
всего через секунду да, напомните мне имя, пожалуйста,

878
00:39:56,410 --> 00:39:58,420
медленно, очень простой вопрос, на который нужно ответить -

879
00:39:58,420 --> 00:40:01,390
запомните прямо сейчас, но  что я думаю о

880
00:40:01,390 --> 00:40:03,490
своем, я думаю, что это функция

881
00:40:03,490 --> 00:40:05,890
состояния, и он читает это как функцию, но

882
00:40:05,890 --> 00:40:09,700
действие, заданное состоянием, просто для того, чтобы как бы

883
00:40:09,700 --> 00:40:11,860
обновить весь мой родитель, например, что

884
00:40:11,860 --> 00:40:13,540
наверху и как мы определяем функцию сейчас

885
00:40:13,540 --> 00:40:14,770
мы  думать об этом как о сопоставлении

886
00:40:14,770 --> 00:40:16,030
состояний с действиями, но это может быть

887
00:40:16,030 --> 00:40:17,950
стохастическая функция, поэтому это может быть

888
00:40:17,950 --> 00:40:21,820
распределение вероятностей по действиям, поэтому

889
00:40:21,820 --> 00:40:23,890
я могу выбрать действие a1 с вероятностью 50%

890
00:40:23,890 --> 00:40:25,600
или действие 80 с вероятностью 50%,

891
00:40:25,600 --> 00:40:29,160
например, из того же

892
00:40:29,160 --> 00:40:31,900
состояния I  среднее значение зависит от того, как вы хотите

893
00:40:31,900 --> 00:40:33,850
реализовать это, как это может начаться

894
00:40:33,850 --> 00:40:35,890
в нем, но, по сути, я думаю об этом, как о том, что

895
00:40:35,890 --> 00:40:37,840
вы находитесь в состоянии, а затем у вас есть некоторое

896
00:40:37,840 --> 00:40:39,580
распределение вероятностей по действиям, которые

897
00:40:39,580 --> 00:40:41,320
вы должны выбрать из этого, чтобы решить,

898
00:40:41,320 --> 00:40:49,210
какое действие  ion вы принимаете политику, так

899
00:40:49,210 --> 00:40:51,130
что мы делаем здесь, когда мы расширили

900
00:40:51,130 --> 00:40:52,870
это, как мы сказали, какова политика

901
00:40:52,870 --> 00:40:55,900
для действия, учитывая состояние, которое мы сказали с

902
00:40:55,900 --> 00:40:57,460
вероятностью 1 минус эпсилон, мы бы

903
00:40:57,460 --> 00:41:00,870
предприняли это максимальное действие, чтобы одно и

904
00:41:00,870 --> 00:41:03,490
с эпсилон  вероятность того, что мы предпримем

905
00:41:03,490 --> 00:41:05,620
одно из действий, и поэтому мы

906
00:41:05,620 --> 00:41:07,000
просуммировали каждое из действий, которые мы могли бы

907
00:41:07,000 --> 00:41:09,430
предпринять, поэтому мы разделили

908
00:41:09,430 --> 00:41:11,740
эту сумму на вероятность

909
00:41:11,740 --> 00:41:13,510
совершения одного действия, и она будет Q-

910
00:41:13,510 --> 00:41:15,010
функцией этого действия и

911
00:41:15,010 --> 00:41:16,210
вероятность выполнения каждого из других

912
00:41:16,210 --> 00:41:17,920
действий и какова будет их ценность,

913
00:41:17,920 --> 00:41:20,880
так что это похоже на наше ожидаемое значение,

914
00:41:20,880 --> 00:41:24,240
да, в конце

915
00:41:24,430 --> 00:41:26,200
это похоже на то, когда мы говорили о

916
00:41:26,200 --> 00:41:29,470
сбросе оператора посыльного, если вы, если вы

917
00:41:29,470 --> 00:41:32,589
получили то же самое значение, но вы  получил,

918
00:41:32,589 --> 00:41:34,509
прежде чем вы знаете, что вы можете прекратить

919
00:41:34,509 --> 00:41:38,490
итерацию здесь, вам нужно было бы

920
00:41:38,490 --> 00:41:44,079
попробовать каждое действие, чтобы знать, что вы

921
00:41:44,079 --> 00:41:48,579
сделали, что было раньше в улучшении политики,

922
00:41:48,579 --> 00:41:49,930
если бы вы добрались до той же

923
00:41:49,930 --> 00:41:51,910
политики, вы знали, что вы сделали, вы не сделали

924
00:41:51,910 --> 00:41:53,049
должен сделать любой  больше улучшений,

925
00:41:53,049 --> 00:41:55,210
вопрос в том, в этом случае это правда или

926
00:41:55,210 --> 00:41:56,470
есть какие-то другие дополнительные условия,

927
00:41:56,470 --> 00:41:58,509
это тоже очень связанный вопрос, так почему

928
00:41:58,509 --> 00:41:59,859
бы нам не перейти к следующей части, говоря, что

929
00:41:59,859 --> 00:42:01,720
вы знаете, при каких условиях они

930
00:42:01,720 --> 00:42:05,289
сходятся и сходятся оптимально, не так

931
00:42:05,289 --> 00:42:08,079
ли?  у вас есть вопрос, прежде чем также сказать,

932
00:42:08,079 --> 00:42:09,549
что единственный вид получения строгого

933
00:42:09,549 --> 00:42:11,710
равенства - это когда эпсилон вам нужен, поэтому

934
00:42:11,710 --> 00:42:14,700
мы просто действуем чисто случайно,

935
00:42:14,700 --> 00:42:17,499
особенно в том, есть ли так,

936
00:42:17,499 --> 00:42:20,289
если политика случайна, вы получите

937
00:42:20,289 --> 00:42:23,950
здесь строгое равенство да, вы должны получить

938
00:42:23,950 --> 00:42:25,839
Я имею в виду, что вы можете получить строгое равенство

939
00:42:25,839 --> 00:42:28,539
всякий раз, когда вы сходитесь, например, если

940
00:42:28,539 --> 00:42:29,980
ваша Q-функция сходится, а ваша

941
00:42:29,980 --> 00:42:36,759
политика оптимальна, поэтому, если вы на

942
00:42:36,759 --> 00:42:38,230
самом деле оказываете совершенно случайное влияние, это

943
00:42:38,230 --> 00:42:39,519
обычно часто то, как вы начинаете, а

944
00:42:39,519 --> 00:42:41,670
затем вы хотите улучшить оттуда

945
00:42:41,670 --> 00:42:44,289
потому что вы здесь, я имею в виду, если вы, если

946
00:42:44,289 --> 00:42:46,059
ваша награда неоднородна, некоторые вещи будут

947
00:42:46,059 --> 00:42:47,529
выглядеть лучше, чем другие, поэтому, даже если

948
00:42:47,529 --> 00:42:48,700
вы на самом деле случайны, некоторые действия

949
00:42:48,700 --> 00:42:49,869
будут иметь более высокую оценку.  варды, чем

950
00:42:49,869 --> 00:42:51,160
другие, и это будет отражено в

951
00:42:51,160 --> 00:42:54,519
вашей Q-функции любые другие, прежде чем мы

952
00:42:54,519 --> 00:43:02,799
перейдем к конвергенции сзади, что я был

953
00:43:02,799 --> 00:43:09,220
прав, это должно быть да, я делаю, я

954
00:43:09,220 --> 00:43:10,630
облажался, искусство делает, когда вы

955
00:43:10,630 --> 00:43:12,970
исследуете делать это, выполняйте искусство

956
00:43:12,970 --> 00:43:15,279
дальше  действие, как да, как

957
00:43:15,279 --> 00:43:17,470
партнер по исследованию и сложная часть, я не знаю,

958
00:43:17,470 --> 00:43:19,619
что

959
00:43:20,530 --> 00:43:23,860
я знаю, что вы не демонстрируете, не исключайте,

960
00:43:23,860 --> 00:43:26,650
вы не исключаете действие аргументов,

961
00:43:26,650 --> 00:43:29,980
когда вы исследуете, вы выбираете их все, если

962
00:43:29,980 --> 00:43:30,970
хотите, это было бы

963
00:43:30,970 --> 00:43:32,680
эквивалентно своего рода определению, вы могли бы сделать это,

964
00:43:32,680 --> 00:43:34,780
но в самой простой версии, включая

965
00:43:34,780 --> 00:43:36,100
это доказательство здесь, мы предполагаем, что когда

966
00:43:36,100 --> 00:43:38,320
вы действуете случайным образом, вы просто выбираете

967
00:43:38,320 --> 00:43:40,630
из любого из состояний, часто проще

968
00:43:40,630 --> 00:43:43,480
от реализации до ладно, отличные

969
00:43:43,480 --> 00:43:48,210
вопросы, давайте это поднимем.  здесь тоже

970
00:43:48,210 --> 00:43:50,710
хорошо, так что это еще один действительно отличный вопрос,

971
00:43:50,710 --> 00:43:51,850
который возникает у нескольких людей

972
00:43:51,850 --> 00:43:54,730
здесь, я хорошо, хорошо, что это значит с

973
00:43:54,730 --> 00:43:57,040
течением времени, у меня есть это монотонное

974
00:43:57,040 --> 00:43:58,930
улучшение, и какие у нас

975
00:43:58,930 --> 00:44:01,560
есть гарантии, поэтому гарантия такова.  у нас есть,

976
00:44:01,560 --> 00:44:04,330
если вы выбираете все пары действий состояния

977
00:44:04,330 --> 00:44:06,910
бесконечное количество раз, и ваша

978
00:44:06,910 --> 00:44:08,530
политика поведения сходится к жадной

979
00:44:08,530 --> 00:44:11,980
политике, так что я имею в виду, так что

980
00:44:11,980 --> 00:44:13,450
политика поведения здесь — это своего рода

981
00:44:13,450 --> 00:44:16,780
политика, которую вы используете, а не какая

982
00:44:16,780 --> 00:44:18,430
политика  жадный по отношению к вашему текущему

983
00:44:18,430 --> 00:44:21,730
сигналу, хорошо, поэтому, если у вас есть случай, который

984
00:44:21,730 --> 00:44:27,580
является пределом, когда я приближаюсь к бесконечности PI,

985
00:44:27,580 --> 00:44:38,050
заданное s переходит к Arg max Q хорошо с

986
00:44:38,050 --> 00:44:42,340
вероятностью 1, что означает, что в

987
00:44:42,340 --> 00:44:45,550
пределе вы сходитесь к тому, чтобы всегда брать

988
00:44:45,550 --> 00:44:47,260
жадный  действие по отношению к вашей Q-

989
00:44:47,260 --> 00:44:53,980
функции, затем ваша жадность в

990
00:44:53,980 --> 00:44:56,140
пределе бесконечного исследования, которое

991
00:44:56,140 --> 00:44:58,090
часто называется ликованием, так что это означает, что

992
00:44:58,090 --> 00:44:59,470
пара действий посещения всех в состоянии происходит

993
00:44:59,470 --> 00:45:01,240
бесконечное количество раз, но вы

994
00:45:01,240 --> 00:45:02,980
также сходится, это просто дает мне

995
00:45:02,980 --> 00:45:05,440
предел и  быть жадным по отношению

996
00:45:05,440 --> 00:45:09,370
к вашей Q-функции, и есть разные

997
00:45:09,370 --> 00:45:10,870
способы сделать это. Простой способ сделать это -

998
00:45:10,870 --> 00:45:13,930
как бы уменьшить ваш эпсилон или

999
00:45:13,930 --> 00:45:17,230
вашу жадную политику с течением времени, чтобы

1000
00:45:17,230 --> 00:45:19,180
вы могли уменьшить свой эпсилон по направлению к ze  ro

1001
00:45:19,180 --> 00:45:22,090
это скорость примерно на 1 выше I, например,

1002
00:45:22,090 --> 00:45:24,930
этого достаточно, это честно,

1003
00:45:24,930 --> 00:45:27,040
они говорят, что это отдельно от того, что вы

1004
00:45:27,040 --> 00:45:28,570
хотите сделать эмпирически, это просто для

1005
00:45:28,570 --> 00:45:32,430
того, чтобы показать в этих условиях,

1006
00:45:32,430 --> 00:45:34,370
тогда мы, как правило, сможем

1007
00:45:34,370 --> 00:45:36,230
показать, что  мы собираемся прийти к

1008
00:45:36,230 --> 00:45:39,670
оптимальной политике и оптимальному значению для

1009
00:45:39,670 --> 00:45:43,370
методов Монте-Карло и ТВ, поэтому, как правило

1010
00:45:43,370 --> 00:45:44,300
, не думаю, что мы будем говорить об этом

1011
00:45:44,300 --> 00:45:45,740
снова, поскольку мы обычно говорим о некоторых других

1012
00:45:45,740 --> 00:45:47,110
алгоритмах, когда вы Glee

1013
00:45:47,110 --> 00:45:50,180
и у вас есть некоторые  условия того, как

1014
00:45:50,180 --> 00:45:53,900
вы изучаете функции подсказки, тогда

1015
00:45:53,900 --> 00:45:55,910
вы гарантированно совпадете.

1016
00:45:55,910 --> 00:46:07,100
Социальная политика да да, так что версия

1017
00:46:07,100 --> 00:46:08,870
это единственный способ гарантировать, но, гм,

1018
00:46:08,870 --> 00:46:10,100
они вроде интересных разных

1019
00:46:10,100 --> 00:46:12,050
вещей, которые происходят здесь АБ, вы

1020
00:46:12,050 --> 00:46:13,010
можете быть гарантированы  что вы

1021
00:46:13,010 --> 00:46:15,140
сходитесь к оптимальной функции метки,

1022
00:46:15,140 --> 00:46:18,940
не сходясь к оптимальной политике,

1023
00:46:18,940 --> 00:46:21,590
чтобы вы могли поддерживать эпсилон действительно высоким,

1024
00:46:21,590 --> 00:46:23,960
и вы могли бы получить много информации, которую

1025
00:46:23,960 --> 00:46:25,820
вы узнали бы о том, что оптимальная

1026
00:46:25,820 --> 00:46:27,590
функция метки  но вы можете не

1027
00:46:27,590 --> 00:46:29,720
следовать этой политике, и мы

1028
00:46:29,720 --> 00:46:33,860
поговорим об этом подробнее через минуту, хорошо,

1029
00:46:33,860 --> 00:46:35,300
так что давайте поговорим немного больше о

1030
00:46:35,300 --> 00:46:37,790
контроле Монте-Карло, и они как бы

1031
00:46:37,790 --> 00:46:39,950
дали этот предшественник, поэтому, если мы хотим

1032
00:46:39,950 --> 00:46:42,590
сделать Монте-Карло онлайн  контроль вместо

1033
00:46:42,590 --> 00:46:44,270
только этой оценки, о которой мы говорили,

1034
00:46:44,270 --> 00:46:46,340
прежде чем мы сможем как бы объединить эти

1035
00:46:46,340 --> 00:46:48,020
идеи изучения функции метки и

1036
00:46:48,020 --> 00:46:50,150
одновременного выполнения этого улучшения,

1037
00:46:50,150 --> 00:46:52,550
чтобы мы могли инициализировать наши функции метки

1038
00:46:52,550 --> 00:46:54,470
и наши подсчеты так же, как мы

1039
00:46:54,470 --> 00:46:57,530
говорили ранее и  то, что

1040
00:46:57,530 --> 00:46:59,810
мы могли бы сделать, это построить жадную

1041
00:46:59,810 --> 00:47:02,900
политику, поэтому жадная политика в этом

1042
00:47:02,900 --> 00:47:04,760
случае всегда будет заключаться в том, что с

1043
00:47:04,760 --> 00:47:07,310
вероятностью 1 минус эпсилон мы выбираем

1044
00:47:07,310 --> 00:47:11,030
Arg max по отношению к Q, а когда

1045
00:47:11,030 --> 00:47:15,610
вероятность эпсилон мы выбираем действие

1046
00:47:15,730 --> 00:47:19,390
здесь, мы  напишите это так:

1047
00:47:20,420 --> 00:47:24,490
bility epsilon над a мы выбираем действие a,

1048
00:47:24,490 --> 00:47:26,930
поэтому всегда просто смешиваем это

1049
00:47:26,930 --> 00:47:31,279
случайное или жадное, так что на самом деле

1050
00:47:31,279 --> 00:47:33,890
как оптимальное действие в этом случае вы

1051
00:47:33,890 --> 00:47:35,480
выбираете вероятность 1 минус

1052
00:47:35,480 --> 00:47:37,910
эпсилон плюс эпсилон над корнелом

1053
00:47:37,910 --> 00:47:41,059
каждый день, хорошо, да,

1054
00:47:41,059 --> 00:47:42,650
несколько человек спрашивали об этом, так что, по

1055
00:47:42,650 --> 00:47:44,510
сути, вы жадничаете с

1056
00:47:44,510 --> 00:47:46,130
вероятностью 1 минус эпсилон плюс эпсилон

1057
00:47:46,130 --> 00:47:50,420
над а, а затем оставшаяся часть

1058
00:47:50,420 --> 00:47:51,740
вашей вероятности идет на случайное

1059
00:47:51,740 --> 00:47:54,109
исследование, потому что  когда вы

1060
00:47:54,109 --> 00:47:55,880
случайны, вы также можете выбрать, какое действие на

1061
00:47:55,880 --> 00:47:59,299
данный момент является лучшим, так что это выглядит

1062
00:47:59,299 --> 00:48:00,440
очень похоже на то, что мы видели до

1063
00:48:00,440 --> 00:48:03,140
того, как мы собираемся попробовать эпизод после того, как мы

1064
00:48:03,140 --> 00:48:05,569
закончили эпизод, тогда в этом случае

1065
00:48:05,569 --> 00:48:07,400
я определяю его первую заметку о посещении  вы

1066
00:48:07,400 --> 00:48:14,569
можете сделать так, чтобы при каждом посещении

1067
00:48:14,569 --> 00:48:18,170
применялись те же преимущества и

1068
00:48:18,170 --> 00:48:20,089
ограничения, что и раньше, в том смысле,

1069
00:48:20,089 --> 00:48:21,950
что вы могли бы либо дать

1070
00:48:21,950 --> 00:48:23,720
немного более предвзятую оценку, если вы

1071
00:48:23,720 --> 00:48:25,309
делаете каждое посещение, но в целом вы

1072
00:48:25,309 --> 00:48:26,839
сможете использовать  больше данных,

1073
00:48:26,839 --> 00:48:30,079
это будет менее шумно, поэтому в

1074
00:48:30,079 --> 00:48:31,940
этом случае мы просто

1075
00:48:31,940 --> 00:48:33,380
поддерживаем подсчеты по парам действий состояния,

1076
00:48:33,380 --> 00:48:34,849
и мы обновляем нашу Q-функцию,

1077
00:48:34,849 --> 00:48:38,059
а затем после того, как мы закончим  В этом

1078
00:48:38,059 --> 00:48:43,519
эпизоде мы можем обновить наш K и наш эпсилон, в э

1079
00:48:43,519 --> 00:48:45,170
ом случае мы просто используем эпсилон, ра 

1080
00:48:45,170 --> 00:48:47,539
ный 1 по сравнению с K, а затем мы переопределяем на 

1081
00:48:47,539 --> 00:48:49,460
у новую жадную политику e по отношению к Q, а

1082
00:48:49,460 --> 00:48:52,940
затем мы получаем еще один эпизод, так чт 

1083
00:48:52,940 --> 00:48:54,589
это просто своего рода  Монте-Карло на линии

1084
00:48:54,589 --> 00:49:01,099
управления, так почему бы нам не вернуться к этому

1085
00:49:01,099 --> 00:49:03,890
примеру с марсоходом,

1086
00:49:03,890 --> 00:49:06,950
так что в примере с марсоходом у нас было для этого то,

1087
00:49:06,950 --> 00:49:11,150
как выглядят наши две Q-функции, так что на

1088
00:49:11,150 --> 00:49:12,920
этом этапе давайте просто потратим

1089
00:49:12,920 --> 00:49:14,119
минуту  и скажите, какова будет наша новая

1090
00:49:14,119 --> 00:49:16,430
политика, если мы подошли к концу этого

1091
00:49:16,430 --> 00:49:19,460
эпизода, и все в порядке, просто

1092
00:49:19,460 --> 00:49:21,619
запишите ty, если есть, если есть две ключевые

1093
00:49:21,619 --> 00:49:23,410
функции, которые имеют точно такое же

1094
00:49:23,410 --> 00:49:26,420
значение для одного и того же состояния для двух

1095
00:49:26,420 --> 00:49:27,710
разных действий.  и есть только ничья,

1096
00:49:27,710 --> 00:49:29,000
и вы можете выбрать, как

1097
00:49:29,000 --> 00:49:31,579
разбить ничью, а затем также записать,

1098
00:49:31,579 --> 00:49:35,019
какова новая политика цапли,

1099
00:49:37,290 --> 00:49:41,460
я просто займу минуту, чтобы сделать это

1100
00:51:04,250 --> 00:51:06,660
хорошо, какова наша жадная политика,

1101
00:51:06,660 --> 00:51:11,460
какова наша жадная политика для s1 a  одна

1102
00:51:11,460 --> 00:51:14,820
какая у нас жадная политика для s2 а потом

1103
00:51:14,820 --> 00:51:16,140
что о  ваша жадная политика для нас троих,

1104
00:51:16,140 --> 00:51:18,720
а потом что за визит для всего

1105
00:51:18,720 --> 00:51:24,090
остального, тайский, хорошо, и в зависимости от вашей

1106
00:51:24,090 --> 00:51:26,340
реализации вы можете либо всегда

1107
00:51:26,340 --> 00:51:27,750
быть там, чтобы найти

1108
00:51:27,750 --> 00:51:29,550
свою жадную политику, либо вы просто

1109
00:51:29,550 --> 00:51:31,560
хотели бы случайно разорвать связи и

1110
00:51:31,560 --> 00:51:33,000
отслеживать это  может постоянно разрывать

1111
00:51:33,000 --> 00:51:34,680
связи случайным образом, что, вероятно, было бы лучше

1112
00:51:34,680 --> 00:51:37,170
эмпирически, например, вместо того, чтобы предварительно определять

1113
00:51:37,170 --> 00:51:39,359
одну жадную политику, вы, вероятно,

1114
00:51:39,359 --> 00:51:42,089
всегда можете просто мило спрашивать, какова максимальная дуга

1115
00:51:42,089 --> 00:51:43,380
Q, и если вы получаете связи, просто

1116
00:51:43,380 --> 00:51:44,940
разорвите их случайным образом, чтобы получить больше

1117
00:51:44,940 --> 00:51:48,150
исследований, поэтому  затем, если мы затем

1118
00:51:48,150 --> 00:51:51,390
определим e жадную политику, где K равно 3, а наш

1119
00:51:51,390 --> 00:51:52,920
эпсилон является одним из наших K, с какой

1120
00:51:52,920 --> 00:51:59,910
вероятностью мы будем следовать случайному, поэтому K равно

1121
00:51:59,910 --> 00:52:04,700
3, эпсилон равен 1 там, да,

1122
00:52:04,700 --> 00:52:07,440
так что это будет означать, что с

1123
00:52:07,440 --> 00:52:09,089
вероятностью 1/3  мы выбираем что-то случайное с

1124
00:52:09,089 --> 00:52:11,250
вероятностью 2/3 мы выбираем

1125
00:52:11,250 --> 00:52:14,849
политику жадности PI, а затем это будет

1126
00:52:14,849 --> 00:52:18,170
обновление для этого конкретного эпизода,

1127
00:52:24,130 --> 00:52:28,460
поэтому, если вы сделаете это, если вы сделаете, если у вас есть

1128
00:52:28,460 --> 00:52:29,540
жадность в пределе информации  начните

1129
00:52:29,540 --> 00:52:30,980
исследование Монте-Карло,

1130
00:52:30,980 --> 00:52:32,570
тогда вы сойдетесь в оптимальном

1131
00:52:32,570 --> 00:52:41,750
значении действия состояния, так что теперь мы

1132
00:52:41,750 --> 00:52:44,530
начнем говорить о телевизионных методах, настолько

1133
00:52:44,530 --> 00:52:48,440
похожих на то, что мы видели для Монте-

1134
00:52:48,440 --> 00:52:49,370
Карло, будет своего рода

1135
00:52:49,370 --> 00:52:51,380
простая аналогия, которая движет нами.  переходим к GD,

1136
00:52:51,380 --> 00:52:53,840
так что помните для ТВ то, что у нас было раньше, это

1137
00:52:53,840 --> 00:52:56,810
то, что у нас был бы наш V PI для s, он был равен

1138
00:52:56,810 --> 00:53:01,340
нашему предыдущему be PI для s плюс 1 минус

1139
00:53:01,340 --> 00:53:03,860
альфа, умноженное на

1140
00:53:03,860 --> 00:53:14,120
oops альфа, умноженное на R плюс гамма V PI для s,

1141
00:53:14,120 --> 00:53:20,900
простое минус V PI  из s, и это было то место, где

1142
00:53:20,900 --> 00:53:27,020
мы сэмплировали ожидание, потому что

1143
00:53:27,020 --> 00:53:28,700
мы получили только один образец s Prime,

1144
00:53:28,700 --> 00:53:30,950
и мы загрузились, потому что

1145
00:53:30,950 --> 00:53:33,440
мы использовали нашу предыдущую оценку V

1146
00:53:33,440 --> 00:53:37,610
PI, так что это было своего рода двумя ключевыми

1147
00:53:37,610 --> 00:53:39,380
аспектами обучения TD.  заключается в том, что мы

1148
00:53:39,380 --> 00:53:41,180
выполняли и загрузку, и выборку, и

1149
00:53:41,180 --> 00:53:42,770
методом Монте-Карло мы выполняли выборку, но не

1150
00:53:42,770 --> 00:53:43,840
загрузку,

1151
00:53:43,840 --> 00:53:46,190
и одним из приятных аспектов обучения TD

1152
00:53:46,190 --> 00:53:47,450
является то, что тогда мы могли обновлять его

1153
00:53:47,450 --> 00:53:49,280
после каждого кортежа, а не ждать

1154
00:53:49,280 --> 00:53:52,310
до конца эпизода, так же,

1155
00:53:52,310 --> 00:53:53,510
как и то, что  мы делали с Мо  nte Carlo

1156
00:53:53,510 --> 00:53:54,800
мы как бы гонялись за нашими

1157
00:53:54,800 --> 00:53:56,570
B с Q, мы собираемся сделать то

1158
00:53:56,570 --> 00:54:00,080
же самое здесь, так что теперь мы собираемся подумать

1159
00:54:00,080 --> 00:54:01,340
об этом или о том, что часто называют

1160
00:54:01,340 --> 00:54:05,200
методами временной разницы для контроля,

1161
00:54:05,860 --> 00:54:08,990
так что мы собираемся делать  теперь мы можем сделать,

1162
00:54:08,990 --> 00:54:10,580
мы можем оценить функцию PI очереди,

1163
00:54:10,580 --> 00:54:12,350
используя обновление временной разницы с помощью,

1164
00:54:12,350 --> 00:54:15,050
например, e-жадной политики, и мы можем выполнить

1165
00:54:15,050 --> 00:54:17,090
Монте-Карло или улучшить, установив PI

1166
00:54:17,090 --> 00:54:20,630
на e-жадную версию Q PI, это

1167
00:54:20,630 --> 00:54:26,180
была бы одна вещь, которую мы можем сделать, есть

1168
00:54:26,180 --> 00:54:28,160
алгоритм под названием sarsa, который обозначает

1169
00:54:28,160 --> 00:54:30,530
вознаграждение за действие в состоянии, следующее состояние, следующее

1170
00:54:30,530 --> 00:54:34,550
действие, sarsa как начинает нашу работу, так

1171
00:54:34,550 --> 00:54:36,830
что мы делаем, мы инициализируем нашу жадную

1172
00:54:36,830 --> 00:54:37,530
политику.

1173
00:54:37,530 --> 00:54:41,070


1174
00:54:41,070 --> 00:54:43,740


1175
00:54:43,740 --> 00:54:46,200
другое действие, и мы наблюдаем

1176
00:54:46,200 --> 00:54:48,330
еще одно следующее состояние с вознаграждением, а затем мы

1177
00:54:48,330 --> 00:54:51,330
обновляем нашу реплику следующим образом, мы говорим, что наш

1178
00:54:51,330 --> 00:54:55,160
предыдущий vet наше значение Q для sta T

1179
00:54:55,160 --> 00:54:57,420
это будет таким, каким было наше предыдущее

1180
00:54:57,420 --> 00:55:01,410
значение, обычно я буду осторожен

1181
00:55:01,410 --> 00:55:03,930
с этим, мы не  грамм  мы больше не будем индексировать их

1182
00:55:03,930 --> 00:55:04,950
с помощью PI,

1183
00:55:04,950 --> 00:55:06,600
потому что у нас есть эта текущая

1184
00:55:06,600 --> 00:55:08,250
оценка, и наша политика тоже будет

1185
00:55:08,250 --> 00:55:10,620
меняться, поэтому функция Q, которую

1186
00:55:10,620 --> 00:55:13,350
мы получаем здесь, теперь не только для одной

1187
00:55:13,350 --> 00:55:14,850
политики, но мы собираемся усреднять

1188
00:55:14,850 --> 00:55:16,320
по разным  образцы, и мы можем

1189
00:55:16,320 --> 00:55:18,720
изменить то, как мы действуем с течением времени, так

1190
00:55:18,720 --> 00:55:22,320
что это будет равно Q от

1191
00:55:22,320 --> 00:55:29,210
s T at плюс 1 минус альфа R T плюс

1192
00:55:29,210 --> 00:55:38,610
гамма Q от s T плюс 180 плюс 1 минус Q

1193
00:55:38,610 --> 00:55:44,730
от s T 80 важно  Особенность этого

1194
00:55:44,730 --> 00:55:47,820
уравнения в том, что я подставляю

1195
00:55:47,820 --> 00:55:50,610
реальное действие, которое было предпринято следующим, чтобы вы

1196
00:55:50,610 --> 00:55:52,740
видели, что находитесь в состоянии, вы выполняете действие

1197
00:55:52,740 --> 00:55:54,360
, получаете вознаграждение, переходите в следующее состояние,

1198
00:55:54,360 --> 00:55:56,850
а затем выполняете другое действие, и так, как

1199
00:55:56,850 --> 00:55:58,590
только вы  знаете, какое следующее

1200
00:55:58,590 --> 00:56:00,270
действие вы сделали, тогда вы можете сделать это

1201
00:56:00,270 --> 00:56:03,600
обновление и sersa в порядке, так что вы

1202
00:56:03,600 --> 00:56:04,830
фактически подключите его к действию, которое было

1203
00:56:04,830 --> 00:56:06,930
предпринято, а затем, как только вы это сделаете,

1204
00:56:06,930 --> 00:56:08,580
вы можете улучшить политику обычным

1205
00:56:08,580 --> 00:56:15,470
способом, чтобы вы  может иметь s T равно

1206
00:56:30,060 --> 00:56:33,460
как жадная оболочка e для этого сейчас,

1207
00:56:33,460 --> 00:56:34,690
так что это l  Это немного отличается от

1208
00:56:34,690 --> 00:56:37,060
Монте-Карло по двум причинам, и это

1209
00:56:37,060 --> 00:56:40,569
вроде как мы делаем эти пару обновлений,

1210
00:56:40,569 --> 00:56:42,640
мы видим состояние, действие, награда, следующая дата,

1211
00:56:42,640 --> 00:56:44,980
следующие кортежи действий, а затем, когда мы это делаем,

1212
00:56:44,980 --> 00:56:47,680
мы можем обновить нашу функцию подсказки, но

1213
00:56:47,680 --> 00:56:48,880
мы можем делать это вдоль  таким образом, нам не

1214
00:56:48,880 --> 00:56:49,900
нужно ждать в конце эпизода,

1215
00:56:49,900 --> 00:56:51,490
и точно так же нам не нужно ждать

1216
00:56:51,490 --> 00:56:52,720
до конца эпизода, чтобы изменить то, как

1217
00:56:52,720 --> 00:56:55,089
мы действуем в мире, так как в

1218
00:56:55,089 --> 00:56:57,339
траектории, которую мы видели до того, как мы увидели

1219
00:56:57,339 --> 00:56:59,650
некоторые состояния или несколько раз в этом

1220
00:56:59,650 --> 00:57:01,030
случае мы могли бы фактически изменить нашу

1221
00:57:01,030 --> 00:57:02,680
политику в отношении того, как мы действуем в этих состояниях

1222
00:57:02,680 --> 00:57:04,869
во время одного и того же эпизода, поэтому, если ваши

1223
00:57:04,869 --> 00:57:06,099
эпизоды очень длинные, это может быть

1224
00:57:06,099 --> 00:57:14,800
очень полезно, поэтому в целом я думаю,

1225
00:57:14,800 --> 00:57:17,170
что часто очень полезно

1226
00:57:17,170 --> 00:57:37,150
обновлять  политика много да, либо вы можете

1227
00:57:37,150 --> 00:57:38,800
написать это там, где вы поместите V в следующей

1228
00:57:38,800 --> 00:57:40,720
части, либо нет, так что вы можете либо иметь его как

1229
00:57:40,720 --> 00:57:43,300
один минус альфа, умноженный на ваше старое значение

1230
00:57:43,300 --> 00:57:45,609
плюс вознаграждение плюс гамма вашей следующей

1231
00:57:45,609 --> 00:57:48,130
вещи, либо вы могли бы иметь это как V плюс

1232
00:57:48,130 --> 00:57:50,560
альфа раз или так  at должно быть

1233
00:57:50,560 --> 00:57:53,579
все еще альфой здесь плюс вознаграждение за

1234
00:57:53,579 --> 00:57:57,300
минус V, так что это либо то, что я сам,

1235
00:57:57,300 --> 00:57:59,500
если вы заметили, что я сделал опечатку, просто дайте

1236
00:57:59,500 --> 00:58:13,990
мне знать, да, о том, почему мы используем

1237
00:58:13,990 --> 00:58:15,369
следующую пару действий состояния, которую вы выбираете, и

1238
00:58:15,369 --> 00:58:16,810
сказал, если макс  q-обучение

1239
00:58:16,810 --> 00:58:18,780
будет максимальным, мы увидим, что примерно на слайде

1240
00:58:18,780 --> 00:58:22,650
источники в основном обновляются в соответствии с политикой,

1241
00:58:22,650 --> 00:58:25,810
которая может иметь, как правило, вы хотите выполнять

1242
00:58:25,810 --> 00:58:27,190
q-обучение, которое будет

1243
00:58:27,190 --> 00:58:28,660
делать максимум, иногда есть некоторые

1244
00:58:28,660 --> 00:58:31,650
преимущества, особенно в случаях, когда  у вас

1245
00:58:31,650 --> 00:58:33,760
может быть много негативных последствий,

1246
00:58:33,760 --> 00:58:37,060
что оптимизм быть максимальным может в конечном

1247
00:58:37,060 --> 00:58:37,390
итоге

1248
00:58:37,390 --> 00:58:39,430
заставить вашего агента принимать много плохих

1249
00:58:39,430 --> 00:58:40,780
решений на раннем этапе, потому что он действительно

1250
00:58:40,780 --> 00:58:42,280
оптимистичен в отношении того, что он

1251
00:58:42,280 --> 00:58:44,590
может сделать, а не того, что он на самом

1252
00:58:44,590 --> 00:58:46,810
деле делает.  ходячий

1253
00:58:46,810 --> 00:58:49,360
пример внутри Sutton umberto, где

1254
00:58:49,360 --> 00:58:50,860
они показывают, что sarsa на самом деле работает

1255
00:58:50,860 --> 00:58:53,980
лучше на ранних стадиях

1256
00:58:53,980 --> 00:58:55,750
ранних образцов по сравнению с обучением в очереди,

1257
00:58:55,750 --> 00:58:58,120
потому что sarsa реалистично относится к тому, что

1258
00:58:58,120 --> 00:59:00,130
произойдет, если вы предпримете определенные действия.  затем

1259
00:59:00,130 --> 00:59:02,110
это пост-оптимистический подход, и если вы

1260
00:59:02,110 --> 00:59:04,210
делаете много случайностей, это означает,

1261
00:59:04,210 --> 00:59:05,650
что старты могут быть более реалистичными

1262
00:59:05,650 --> 00:59:07,720
на ранних стадиях, но эмпирически,

1263
00:59:07,720 --> 00:59:09,160
как правило, вы хотите провести ключевое обучение,

1264
00:59:09,160 --> 00:59:13,170
и оба сойдутся в одном и том же,

1265
00:59:31,440 --> 00:59:34,420
это может быть номенклатура  вопрос, но

1266
00:59:34,420 --> 00:59:35,770
вы говорите о том, как он

1267
00:59:35,770 --> 00:59:37,990
получает информацию из будущего действия,

1268
00:59:37,990 --> 00:59:39,490
но вы должны уже выполнить это

1269
00:59:39,490 --> 00:59:44,530
действие, так почему же оно называется действием состояния,

1270
00:59:44,530 --> 00:59:47,770
вознаграждением за действия следующего состояния, когда на

1271
00:59:47,770 --> 00:59:49,240
самом деле вы обновляете прошлое

1272
00:59:49,240 --> 00:59:51,580
, из чего  Я понимаю, потому

1273
00:59:51,580 --> 00:59:53,170
что вы делаете то, что он знал,

1274
00:59:53,170 --> 00:59:54,760
используя информацию, которую вы узнали, чтобы обновить ту, что

1275
00:59:54,760 --> 00:59:57,250
в конце, так почему же мы

1276
00:59:57,250 --> 00:59:58,600
говорили об этом, как будто это будущее

1277
00:59:58,600 --> 01:00:04,540
? цель этого -

1278
01:00:04,540 --> 01:00:06,820
о  конкретные термины, используемые для определения набора звезд,

1279
01:00:06,820 --> 01:00:09,040
я не думаю, что это я имею в виду, я думаю, на

1280
01:00:09,040 --> 01:00:10,690
самом деле вам просто нужно подождать, пока

1281
01:00:10,690 --> 01:00:14,350
вы не поймете, что последнее «а» важно, так

1282
01:00:14,350 --> 01:00:16,390
что вместо того, чтобы говорить, что до того, что

1283
01:00:16,390 --> 01:00:17,770
мы видели по телевизору, научитесь  облегчить вы были в

1284
01:00:17,770 --> 01:00:20,320
состоянии, вознаградить за действие в следующем состоянии, а затем

1285
01:00:20,320 --> 01:00:21,910
вы могли бы обновить свою функцию подсказки, теперь

1286
01:00:21,910 --> 01:00:22,960
мы просто говорим, что вам нужно подождать, пока

1287
01:00:22,960 --> 01:00:24,340
вы на самом деле не решите, что делать в

1288
01:00:24,340 --> 01:00:27,190
этом следующем состоянии, потому что именно так

1289
01:00:27,190 --> 01:00:29,260
вы выбираете, как действовать.  обновите свою

1290
01:00:29,260 --> 01:00:31,540
ключевую функцию здесь, это то, что вы

1291
01:00:31,540 --> 01:00:37,030
подключаете для своей цели, поэтому с точки

1292
01:00:37,030 --> 01:00:39,460
зрения конвергентных свойств

1293
01:00:39,460 --> 01:00:43,390
требуется несколько разных вещей, поэтому, если

1294
01:00:43,390 --> 01:00:46,300
нам нужны две вещи, нам

1295
01:00:46,300 --> 01:00:48,340
понадобится тот факт, что мы  о том, как мы

1296
01:00:48,340 --> 01:00:50,040
обновляем нашу функцию метки, и

1297
01:00:50,040 --> 01:00:52,470
она будет обновляться постепенно, и

1298
01:00:52,470 --> 01:00:54,150
так, как мы говорили, прежде чем

1299
01:00:54,150 --> 01:00:55,530
мы собираемся выполнить некоторые условия по сравнению

1300
01:00:55,530 --> 01:00:58,160
с Альфой, если альфа равна единице,

1301
01:00:58,160 --> 01:01:00,240
как правило, ваши Q-функции не будут

1302
01:01:00,240 --> 01:01:01,500
сходиться, потому что это означает  вы ничего не

1303
01:01:01,500 --> 01:01:03,990
помните о прошлом, если

1304
01:01:03,990 --> 01:01:06,000
альфа-ноль, то вы больше не обновляетесь

1305
01:01:06,000 --> 01:01:07,800
, поэтому, как правило, вам нужно что-то

1306
01:01:07,800 --> 01:01:09,360
с точки зрения этих размеров шага, что

1307
01:01:09,360 --> 01:01:10,890
позволяет вам постепенно

1308
01:01:10,890 --> 01:01:13,080
увеличиваться, но все же сходится, так

1309
01:01:13,080 --> 01:01:14,730
что это  один достаточный набор

1310
01:01:14,730 --> 01:01:17,640
условий, поэтому, если у вас есть такие вещи, как

1311
01:01:17,640 --> 01:01:21,080
альфа T, равна единице больше T, теперь

1312
01:01:21,080 --> 01:01:23,130
эмпирически часто вам нужно

1313
01:01:23,130 --> 01:01:24,540
выбрать очень разные формы весов обучения,

1314
01:01:24,540 --> 01:01:26,580
поэтому альфа T часто упоминается

1315
01:01:26,580 --> 01:01:31,650
как параметр скорости обучения и

1316
01:01:31,650 --> 01:01:33,300
эмпирически  вы часто не

1317
01:01:33,300 --> 01:01:44,490
захотите использовать это, как правило, не собираетесь использовать

1318
01:01:44,490 --> 01:01:48,210
это, это будет да, здесь вы

1319
01:01:48,210 --> 01:01:49,560
часто захотите использовать разные вещи

1320
01:01:49,560 --> 01:01:51,390
эмпирически, вы можете в конечном итоге использовать

1321
01:01:51,390 --> 01:01:53,040
иногда небольшие константы или медленно

1322
01:01:53,040 --> 01:01:54,750
затухающие константы что-то, что зависит

1323
01:01:54,750 --> 01:01:56,640
от  домен, но это с

1324
01:01:56,640 --> 01:01:58,140
теоретической точки зрения, что достаточно для

1325
01:01:58,140 --> 01:02:00,330
обеспечения конвергенции, а затем другой

1326
01:02:00,330 --> 01:02:02,940
аспект заключается в том, что

1327
01:02:02,940 --> 01:02:04,530
ваша политика сама по себе должна удовлетворять

1328
01:02:04,530 --> 01:02:07,110
условию Glee, что означает, что вы

1329
01:02:07,110 --> 01:02:08,790
как бы постепенно становитесь более жадными

1330
01:02:08,790 --> 01:02:10,410
по отношению к  время, но вы делаете это таким

1331
01:02:10,410 --> 01:02:11,760
образом, что вы все еще отбираете всю

1332
01:02:11,760 --> 01:02:13,110
пару действий состояния бесконечное количество

1333
01:02:13,110 --> 01:02:16,080
раз, теперь я просто знаю на секунду

1334
01:02:16,080 --> 01:02:18,060
, что это не всегда возможно  вот так,

1335
01:02:18,060 --> 01:02:22,710
если у вас есть домен, в котором вещи

1336
01:02:22,710 --> 01:02:24,930
недоступны после точки, это не так или

1337
01:02:24,930 --> 01:02:26,490
гхатак, вы не можете вернуться к определенным

1338
01:02:26,490 --> 01:02:28,080
состояниям после того, как вы туда доберетесь, скажем,

1339
01:02:28,080 --> 01:02:29,610
вы летите на вертолете, и вы

1340
01:02:29,610 --> 01:02:31,590
разбиваете вертолет, чтобы вы могли  не вернешься

1341
01:02:31,590 --> 01:02:33,030
туда, и тогда ты не

1342
01:02:33,030 --> 01:02:34,620
сможешь удовлетворить ликование, потому что в какой-то момент

1343
01:02:34,620 --> 01:02:36,330
ты сломал свой вертолет, а потом, как будто

1344
01:02:36,330 --> 01:02:37,470
ты понятия не имеешь, что было бы,

1345
01:02:37,470 --> 01:02:38,790
если бы ты продолжал летать на своем

1346
01:02:38,790 --> 01:02:40,920
вертолете в воздухе, так что  могут быть

1347
01:02:40,920 --> 01:02:42,810
некоторые области, в которых очень

1348
01:02:42,810 --> 01:02:46,110
трудно удовлетворить Кли, но мы, как правило,

1349
01:02:46,110 --> 01:02:47,970
собираемся игнорировать их, даже несмотря на

1350
01:02:47,970 --> 01:02:49,410
то, что есть действительно интересная работа о том, как

1351
01:02:49,410 --> 01:02:51,390
мы справляемся с такими случаями, и в

1352
01:02:51,390 --> 01:02:52,860
этих случаях вы можете предположить

1353
01:02:52,860 --> 01:02:54,540
, что это более  эпизодической проблемы,

1354
01:02:54,540 --> 01:02:55,740
так что, может быть, у вас есть около сотни

1355
01:02:55,740 --> 01:02:57,840
вертолетов, и поэтому, когда вы разбиваете один из них

1356
01:02:57,840 --> 01:02:59,070
, это считается условием прекращения,

1357
01:02:59,070 --> 01:03:00,270
а затем вы получаете следующий

1358
01:03:00,270 --> 01:03:03,900
, поэтому вы можете или не можете быть в состоянии

1359
01:03:03,900 --> 01:03:05,549
читать в пределе информации в

1360
01:03:05,549 --> 01:03:07,319
t  Ограничение бесконечного исследования

1361
01:03:07,319 --> 01:03:08,789
заключается в том, что вы можете как бы иметь сбалансированное

1362
01:03:08,789 --> 01:03:10,410
количество исследований, и мы

1363
01:03:10,410 --> 01:03:12,000
поговорим намного позже о том, как

1364
01:03:12,000 --> 01:03:13,529
сделать это исследование гораздо более

1365
01:03:13,529 --> 01:03:15,510
разумным и таким образом, чтобы получить

1366
01:03:15,510 --> 01:03:17,760
конечную выборку.  гарантии того, сколько de

1367
01:03:17,760 --> 01:03:24,599
нам нужно, чтобы изучить хорошую политику, так что

1368
01:03:24,599 --> 01:03:26,339
это просто говорит о том, что я сказал там раньше

1369
01:03:26,339 --> 01:03:28,349
, а именно, вы знаете, что мы обычно не

1370
01:03:28,349 --> 01:03:34,170
собираемся использовать размер шага, где у вас

1371
01:03:34,170 --> 01:03:37,650
есть Q of s th t плюс alpha стоимость один

1372
01:03:37,650 --> 01:03:43,200
минус альфа, умноженное на то, что это

1373
01:03:43,200 --> 01:03:45,329
условие наоборот, предполагая, что это

1374
01:03:45,329 --> 01:03:48,059
конкретное обновление того, как мы обновляем

1375
01:03:48,059 --> 01:03:56,279
наши ключевые функции, да, так что в случае Монте-

1376
01:03:56,279 --> 01:03:58,260
Карло у нас было достаточное условие,

1377
01:03:58,260 --> 01:04:00,900
при котором высота эсата вонючий писк,

1378
01:04:00,900 --> 01:04:02,329
как у Эпсилон,

1379
01:04:02,329 --> 01:04:04,770
хорошо, у нас есть что-нибудь  в

1380
01:04:04,770 --> 01:04:07,799
общем-то похоже, отличный вопрос, миссия об

1381
01:04:07,799 --> 01:04:09,260
этом для Монте-Карло, есть ли у нас

1382
01:04:09,260 --> 01:04:12,480
аналогичные условия, если бы вы были просто, если

1383
01:04:12,480 --> 01:04:14,309
бы вы совершали первое посещение, одного этого

1384
01:04:14,309 --> 01:04:16,349
достаточно, потому что вы получаете

1385
01:04:16,349 --> 01:04:19,170
беспристрастную оценку, которая выпукла  rging для

1386
01:04:19,170 --> 01:04:21,059
всех ваших возвратов, так что, пока вы находитесь

1387
01:04:21,059 --> 01:04:22,380
во всех парах действий состояния, бесконечное

1388
01:04:22,380 --> 01:04:22,980
количество раз,

1389
01:04:22,980 --> 01:04:24,660
если вы делаете это постепенно,

1390
01:04:24,660 --> 01:04:28,859
то если вы, если вы, если

1391
01:04:28,859 --> 01:04:30,210
вы играете с тем, как это

1392
01:04:30,210 --> 01:04:31,740
альфа, тогда вам нужно иметь аналогичные

1393
01:04:31,740 --> 01:04:37,049
условия, чтобы убедиться, что я гарантирую, откуда

1394
01:04:37,049 --> 01:04:39,930
вы знаете, что выполняется условие 1, откуда

1395
01:04:39,930 --> 01:04:41,220
вы знаете, что все чисто, да,

1396
01:04:41,220 --> 01:04:42,809
как Монте-Карло, у нас не было

1397
01:04:42,809 --> 01:04:44,849
скорости условия, которая заключалась в том, что

1398
01:04:44,849 --> 01:04:48,270
распад эпсилон - это один  другой вопрос, так что,

1399
01:04:48,270 --> 01:04:49,260
как вы убедитесь, что что-то

1400
01:04:49,260 --> 01:04:51,630
Glee одно достаточное условие состоит в том, что

1401
01:04:51,630 --> 01:04:54,299
эпсилон на 1 больше, чем на 1, или

1402
01:04:54,299 --> 01:05:03,359
что-то еще, что я работаю, как да, о, вы имеете в виду,

1403
01:05:03,359 --> 01:05:04,619
что с точки зрения достаточных условий,

1404
01:05:04,619 --> 01:05:06,650
как это, да, действительно, в

1405
01:05:06,650 --> 01:05:09,750
это поверят  если и только если этот

1406
01:05:09,750 --> 01:05:11,339
эпсилон обнулится, но есть несколько

1407
01:05:11,339 --> 01:05:13,410
мертвых тараканов или что-то в этом роде, да,

1408
01:05:13,410 --> 01:05:14,480
я думаю, что это бесценно,

1409
01:05:14,480 --> 01:05:16,880
и мать Робина последовательность, подобная вашей

1410
01:05:16,880 --> 01:05:18,680
репе, по сути, вы очень гарантируете

1411
01:05:18,680 --> 01:05:19,790
нам, что вы делаете бесконечное количество

1412
01:05:19,790 --> 01:05:21,800
обновлений, если это так  а  гора зелени или как

1413
01:05:21,800 --> 01:05:23,630
случайное исследование, это все еще идет

1414
01:05:23,630 --> 01:05:25,280
вниз достаточно быстро, что в Вирджинии, я

1415
01:05:25,280 --> 01:05:26,540
думаю, что это, вероятно, именно то, что нужно, но

1416
01:05:26,540 --> 01:05:31,760
я могу коснуться, хорошо, так что тогда, когда мы приступаем к

1417
01:05:31,760 --> 01:05:33,110
ключевому обучению, которое связано с

1418
01:05:33,110 --> 01:05:34,550
вопросом, который мы только что задали, хорошо, почему мы

1419
01:05:34,550 --> 01:05:36,350
затем выбираем это конкретное действие,

1420
01:05:36,350 --> 01:05:38,450
почему бы нам просто не выбрать максимальное, да, мы могли бы

1421
01:05:38,450 --> 01:05:40,670
просто выбрать максимальное вместо этого, поэтому sarsa выбирает

1422
01:05:40,670 --> 01:05:42,260
это конкретное действие, следующее

1423
01:05:42,260 --> 01:05:43,340
q-обучение

1424
01:05:43,340 --> 01:05:52,040
выбирает следующий раздел карты, это не

1425
01:05:52,040 --> 01:05:53,810
так оптимистично, если бы вы были лучше

1426
01:05:53,810 --> 01:05:56,750
взвешены, поэтому  что мы могли бы смешивать набор

1427
01:05:56,750 --> 01:05:58,580
и q-обучение, вы, конечно, могли бы, но

1428
01:05:58,580 --> 01:06:00,140
затем это также заставило меня понять, что, возможно, я

1429
01:06:00,140 --> 01:06:01,520
был недостаточно ясен для

1430
01:06:01,520 --> 01:06:04,130
более ранней части, см. Эшли может добиться большего успеха в

1431
01:06:04,130 --> 01:06:06,170
некоторых областях на раннем этапе, особенно если

1432
01:06:06,170 --> 01:06:07,550
есть много действительно негативных  вознаграждения,

1433
01:06:07,550 --> 01:06:08,690
потому что это реалистично

1434
01:06:08,690 --> 01:06:10,550
в других случаях q-обучение будет

1435
01:06:10,550 --> 01:06:12,140
лучше даже на раннем этапе, потому что вы могли

1436
01:06:12,140 --> 01:06:14,330
бы быть более оптимистичными, и, как мы

1437
01:06:14,330 --> 01:06:15,890
говорили немного раньше, часто

1438
01:06:15,890 --> 01:06:17,120
оптимизм действительно помогает  Полезный для

1439
01:06:17,120 --> 01:06:20,180
исследования пример работы на скале в

1440
01:06:20,180 --> 01:06:22,220
Саттоне в Бартоу — это случай, когда некоторые

1441
01:06:22,220 --> 01:06:24,440
действия приводят агента к тому, что он падает

1442
01:06:24,440 --> 01:06:26,090
со скалы, и поэтому некоторые действия действительно

1443
01:06:26,090 --> 01:06:27,590
плохи, и поэтому они оптимистичны

1444
01:06:27,590 --> 01:06:29,000
на ранней стадии, что означает, что вы собираетесь принять

1445
01:06:29,000 --> 01:06:30,650
много действительно плохих решений и какое-то время страдать от

1446
01:06:30,650 --> 01:06:33,410
множества негативных наград во многих

1447
01:06:33,410 --> 01:06:35,090
других областях, они не такие, поэтому

1448
01:06:35,090 --> 01:06:36,680
зависит от того, я думаю, вы, безусловно, могли бы

1449
01:06:36,680 --> 01:06:42,410
смешать их все правильно, так что я думаю, с точки

1450
01:06:42,410 --> 01:06:43,700
зрения изучения Q одна

1451
01:06:43,700 --> 01:06:47,990
интересная вещь здесь  так что мы можем снова

1452
01:06:47,990 --> 01:06:49,400
подумать о том, как мы это улучшаем,

1453
01:06:49,400 --> 01:06:51,830
и мы собираемся быть жадными по

1454
01:06:51,830 --> 01:06:53,210
отношению к текущей оценке при

1455
01:06:53,210 --> 01:06:57,530
оптимальном Q, и на самом деле это очень

1456
01:06:57,530 --> 01:06:59,420
похоже на то, что мы делали в сарсе,

1457
01:06:59,420 --> 01:07:01,220
за исключением того момента, когда  мы обновляем этот Q,

1458
01:07:01,220 --> 01:07:02,810
мы действительно просто собираемся сделать это

1459
01:07:02,810 --> 01:07:07,070
максимальное, поэтому Q s t-80 будет

1460
01:07:07,070 --> 01:07:13,100
равно предыдущему значению плюс буквенное слово

1461
01:07:13,100 --> 01:07:16,850
плюс максимальное значение, так что теперь также обратите внимание, что

1462
01:07:16,850 --> 01:07:19,160
вы можете немного обновить это  раньше,

1463
01:07:19,160 --> 01:07:21,110
потому что тебе не нужно ждать тебя  пока

1464
01:07:21,110 --> 01:07:26,510
не будет предпринято следующее действие,

1465
01:07:26,510 --> 01:07:28,580
поэтому вам нужно только наблюдать за этой частью,

1466
01:07:28,580 --> 01:07:29,960
вам не нужно фактически видеть следующий

1467
01:07:29,960 --> 01:07:31,690
выполненный раздел, а затем вы можете

1468
01:07:31,690 --> 01:07:34,400
выполнить улучшение политики, и в

1469
01:07:34,400 --> 01:07:36,440
целом в этом случае вы только в этом,

1470
01:07:36,440 --> 01:07:38,150
вам нужно только  обновите политику

1471
01:07:38,150 --> 01:07:40,910
для состояния, в котором вы только что находились, чтобы вы

1472
01:07:40,910 --> 01:07:45,020
могли выполнить PI вы можете обновить PI b4 st

1473
01:07:45,020 --> 01:07:47,180
поместите действие, которое вы только что предприняли, вам не нужны

1474
01:07:47,180 --> 01:07:48,710
особенно большие пространства состояний, которые могут

1475
01:07:48,710 --> 01:07:53,240
быть полезны, поэтому мы фактически закончили

1476
01:07:53,240 --> 01:07:54,320
разговор об этом  уже немного

1477
01:07:54,320 --> 01:07:56,690
о том, имеет ли значение то, как вы инициализируете

1478
01:07:56,690 --> 01:07:59,480
Q, это не имеет значения асимптотически, я

1479
01:07:59,480 --> 01:08:01,160
имею в виду, что если у вас есть случай, когда ваша

1480
01:08:01,160 --> 01:08:02,510
функция Q будет сходиться к

1481
01:08:02,510 --> 01:08:03,770
правильной вещи, она все равно будет сходиться к

1482
01:08:03,770 --> 01:08:04,730
правильной вещи, независимо от того, как вы ее

1483
01:08:04,730 --> 01:08:07,280
инициализировали  Я согласен, если он удовлетворяет

1484
01:08:07,280 --> 01:08:09,260
этим другим условиям, но он, безусловно,

1485
01:08:09,260 --> 01:08:11,390
имеет большое значение с эмпирической точки зрения, и поэтому,

1486
01:08:11,390 --> 01:08:12,770
хотя часто мы думаем о

1487
01:08:12,770 --> 01:08:14,600
простом случайном общении или

1488
01:08:14,600 --> 01:08:16,279
инициализации его с помощью нуля, оптимистичная

1489
01:08:16,279 --> 01:08:18,799
инициализация часто действительно полезна, поэтому мы

1490
01:08:18,799 --> 01:08:20,330
подробнее об этом, когда мы говорим об

1491
01:08:20,330 --> 01:08:27,830
исследовании в шестой строке, да, максимум, это

1492
01:08:27,830 --> 01:08:40,790
жесткий максимум, так что теперь я думаю, если мы сделаем Q, изучая,

1493
01:08:40,790 --> 01:08:43,310
хм, давайте посмотрим, я думаю, что я

1494
01:08:43,310 --> 01:08:44,689
оставлю это как просто упражнение, которое вы можете

1495
01:08:44,689 --> 01:08:46,100
сделать там, но вы  можно просто выполнить то

1496
01:08:46,100 --> 01:08:48,410
же самое упражнение для Q-обучения и посмотреть, как

1497
01:08:48,410 --> 01:08:51,950
эти обновления распространяются, так же, как

1498
01:08:51,950 --> 01:08:54,859
Монте-Карло против милого Монте-Карло

1499
01:08:54,859 --> 01:08:57,770
против TD для оценки политики, есть

1500
01:08:57,770 --> 01:08:59,450
некоторые из тех же проблем с Q-обучением.

1501
01:08:59,450 --> 01:09:01,850
Q-обучение - это всего лишь обновление вашей Q-

1502
01:09:01,850 --> 01:09:04,460
функции для  заявите, что вы только что были,

1503
01:09:04,460 --> 01:09:06,979
поэтому, даже если позже в том же эпизоде выяснится, что вы 

1504
01:09:06,979 --> 01:09:08,270
олучите действительно высокую на 

1505
01:09:08,270 --> 01:09:10,220
раду, вы не собираетесь распространять эт 

1506
01:09:10,220 --> 01:09:12,229
информацию в конце эп 

1507
01:09:12,229 --> 01:09:13,609
зода, и так, как вы сделали бы с Мо 

1508
01:09:13,609 --> 01:09:15,890
те-Карло, так что Q изучение обновлений  часто может быть

1509
01:09:15,890 --> 01:09:20,180
намного медленнее, чем метод Монте-Карло,

1510
01:09:20,180 --> 01:09:21,620
и это влияет на то,

1511
01:09:21,620 --> 01:09:23,660
как быстро вы можете научиться принимать лучшие

1512
01:09:23,660 --> 01:09:28,640
решения, поэтому условия,

1513
01:09:28,640 --> 01:09:30,529
достаточные для обеспечения того, чтобы Q-обучение

1514
01:09:30,529 --> 01:09:32,960
с жадными сходится, в

1515
01:09:32,960 --> 01:09:34,640
основном такие же, как и  SAR, поэтому нам нужно убедиться,

1516
01:09:34,640 --> 01:09:38,799
что все так уродливо,

1517
01:09:40,170 --> 01:09:45,630
и на самом деле, и немного пересмотрел это,

1518
01:09:45,630 --> 01:09:47,890
поэтому, если вы просто хотите убедиться, что

1519
01:09:47,890 --> 01:09:49,899
вы сходитесь, вам нужно бесконечно часто посещать все

1520
01:09:49,899 --> 01:09:54,040
sa. Мне нужно, чтобы эти

1521
01:09:54,040 --> 01:10:03,550
условия были на альфе, поэтому, если вы  посмотрите

1522
01:10:03,550 --> 01:10:05,260
на те же условия, чтобы

1523
01:10:05,260 --> 01:10:06,910
функции Q сходились, вам нужны

1524
01:10:06,910 --> 01:10:08,500
эти условия о том, как вы обновляете

1525
01:10:08,500 --> 01:10:09,940
свой внешний вид, какова ваша скорость обучения

1526
01:10:09,940 --> 01:10:12,070
, и что вы посещаете все пары действий состояния

1527
01:10:12,070 --> 01:10:13,900
бесконечно часто, но это

1528
01:10:13,900 --> 01:10:15,640
просто не  этого достаточно, чтобы позволить вам

1529
01:10:15,640 --> 01:10:18,010
сойтись с оптимальными значениями Q, а затем,

1530
01:10:18,010 --> 01:10:19,450
если вы действительно хотите убедиться, что

1531
01:10:19,450 --> 01:10:21,910
политика, которой вы следуете, действительно является

1532
01:10:21,910 --> 01:10:24,040
оптимальной политикой, тогда вам нужно бигли,

1533
01:10:24,040 --> 01:10:25,720
потому что вам также нужна политика, которую вы выбрали,

1534
01:10:25,720 --> 01:10:30,670
чтобы быть все более и более жадной  хорошо, позвольте

1535
01:10:30,670 --> 01:10:31,600
мне вкратце остановиться на

1536
01:10:31,600 --> 01:10:34,420
предвзятости максимизации, прежде чем мы закончим,

1537
01:10:34,420 --> 01:10:35,860
предвзятость максимизации - интересный

1538
01:10:35,860 --> 01:10:38,080
вопрос, так почему мы собираемся говорить

1539
01:10:38,080 --> 01:10:39,970
об этом, хорошо, давайте вернемся к

1540
01:10:39,970 --> 01:10:42,520
этому, так что в Q узнаем, что  мы

1541
01:10:42,520 --> 01:10:44,260
делаем Q обучение, мы вычисляем Q-

1542
01:10:44,260 --> 01:10:45,820
функцию, а затем мы жадны

1543
01:10:45,820 --> 01:10:47,320
по отношению к ней, теперь нам понадобятся

1544
01:10:47,320 --> 01:10:48,940
еще некоторые данные, и мы читаем нашу Q-

1545
01:10:48,940 --> 01:10:50,470
функцию, и мы жадны по

1546
01:10:50,470 --> 01:10:52,600
отношению к ней.  это, и поэтому мы жадны

1547
01:10:52,600 --> 01:10:54,400
по отношению к этому, и поэтому мы всегда как

1548
01:10:54,400 --> 01:10:55,630
бы делаем этот танец между

1549
01:10:55,630 --> 01:10:57,460
обновлением материала, получением большего количества доказательств, но

1550
01:10:57,460 --> 01:10:58,630
затем пытаемся использовать это

1551
01:10:58,630 --> 01:11:01,180
знание для какого-то случайного исследования,

1552
01:11:01,180 --> 01:11:03,940
и уклон максимизации указывает,

1553
01:11:03,940 --> 01:11:05,320
что  может быть, с этим могут быть некоторые проблемы,

1554
01:11:05,320 --> 01:11:07,300
хорошо, поэтому давайте просто рассмотрим

1555
01:11:07,300 --> 01:11:09,100
конкретный пример, представьте, что есть

1556
01:11:09,100 --> 01:11:10,780
MDP с одним состоянием, что означает, что есть

1557
01:11:10,780 --> 01:11:13,420
только одно состояние, но есть два действия,

1558
01:11:13,420 --> 01:11:15,430
и оба они на самом деле имеют 0 средних

1559
01:11:15,430 --> 01:11:17,770
случайных вознаграждений, так что теперь вы можете думать об

1560
01:11:17,770 --> 01:11:20,110
этом  как гауссовы прямо сейчас

1561
01:11:20,110 --> 01:11:21,550
мы в основном говорим об этом, когда их

1562
01:11:21,550 --> 01:11:23,350
вознаграждение на самом деле детерминировано, но это

1563
01:11:23,350 --> 01:11:25,180
не обязательно, это может быть

1564
01:11:25,180 --> 01:11:27,489
стохастическое вознаграждение, но в этом случае мы

1565
01:11:27,489 --> 01:11:28,750
собираемся представить, что если вы примете действие

1566
01:11:28,750 --> 01:11:31,030
n a1 или действие a2 ваше ожидаемое

1567
01:11:31,030 --> 01:11:33,489
значение равно 0, но значение, полученное в любом

1568
01:11:33,489 --> 01:11:35,380
конкретном эпизоде, любой конкретный шаг

1569
01:11:35,380 --> 01:11:38,650
может быть не 0, может быть, вы знаете, 1 или

1570
01:11:38,650 --> 01:11:40,270
минус 1 или что-

1571
01:11:40,270 --> 01:11:41,920
то в этом роде, среднее значение по-прежнему равно 0, но на любом

1572
01:11:41,920 --> 01:11:44,170
конкретном шаге вы могли бы иметь  что-то

1573
01:11:44,170 --> 01:11:46,989
другое, но ожидаемое значение равно 0

1574
01:11:46,989 --> 01:11:50,680
, поэтому значение q для s a1 и

1575
01:11:50,680 --> 01:11:52,730
значение q для s a2 равно 0, что является одним и

1576
01:11:52,730 --> 01:11:54,650
тем же значением.

1577
01:11:54,650 --> 01:11:58,489


1578
01:11:58,489 --> 01:11:59,989
образцы вы

1579
01:11:59,989 --> 01:12:01,550
пробовали действие a1 множество раз вы

1580
01:12:01,550 --> 01:12:03,560
пробовали действие a2 множество раз и вы

1581
01:12:03,560 --> 01:12:05,420
вычисляете эмпирическую оценку этого

1582
01:12:05,420 --> 01:12:08,390
и здесь снова, где есть только

1583
01:12:08,390 --> 01:12:11,120
одно состояние, и мы можем просто

1584
01:12:11,120 --> 01:12:12,980
усреднить их, поэтому давайте представим, что это

1585
01:12:12,980 --> 01:12:16,130
очень просто  что у нас есть гамма

1586
01:12:16,130 --> 01:12:17,450
равна нулю, поэтому мы действительно просто

1587
01:12:17,450 --> 01:12:19,220
оцениваем по немедленному вознаграждению,

1588
01:12:19,220 --> 01:12:21,620
хорошо, поэтому нет никаких будущих вознаграждений, мы

1589
01:12:21,620 --> 01:12:23,239
просто говорим все время, когда мы

1590
01:12:23,239 --> 01:12:24,980
пробовали это действие до того, что мы

1591
01:12:24,980 --> 01:12:27,530
все роиды, которые мы  получить и мы в среднем и сейчас

1592
01:12:27,530 --> 01:12:28,850
что мы хотим сделать, так это взять

1593
01:12:28,850 --> 01:12:30,620
наши эмпирические оценки функции Q

1594
01:12:30,620 --> 01:12:33,050
для a1 и a2, и мы хотим

1595
01:12:33,050 --> 01:12:34,870
выяснить, что такое жадная политика,

1596
01:12:34,870 --> 01:12:39,590
и проблема в том, что она может быть предвзятой, поэтому

1597
01:12:39,590 --> 01:12:41,780
даже если каждая из этих несмещенных

1598
01:12:41,780 --> 01:12:45,320
оценок имеет Q  или сами по себе, даже

1599
01:12:45,320 --> 01:12:47,630
несмотря на то, что две оценки действий

1600
01:12:47,630 --> 01:12:50,239
смещены, когда вы берете максимальное значение над ним,

1601
01:12:50,239 --> 01:12:52,880
оно может быть смещенным, поэтому давайте просто напишем,

1602
01:12:52,880 --> 01:12:57,230
что это такое, так что RB I равно

1603
01:12:57,230 --> 01:13:13,040
ожидаемому значению максимального значения Q, поэтому я

1604
01:13:13,040 --> 01:13:14,780
собираюсь  тщательные спектры по максимальному количеству

1605
01:13:14,780 --> 01:13:16,250
этих двух вещей, потому что именно так я

1606
01:13:16,250 --> 01:13:19,520
определил свою политику, моя политика говорит, что вы выбираете то,

1607
01:13:19,520 --> 01:13:21,050
что эмпирически выглядит

1608
01:13:21,050 --> 01:13:24,020
лучше, но мы знаем, что из Дженсена

1609
01:13:24,020 --> 01:13:25,760
это больше, чем равно, если вы

1610
01:13:25,760 --> 01:13:37,670
переключаете максимум, и ожидание, что это просто

1611
01:13:37,670 --> 01:13:44,870
равно  максимальному значению 0, поэтому важная часть

1612
01:13:44,870 --> 01:13:52,430
заключается в том, что это так, и это равно

1613
01:13:52,430 --> 01:13:57,620
истинному PI, так что это означает, что все, что мы

1614
01:13:57,620 --> 01:14:01,430
вычисляем, может быть смещенной оценкой

1615
01:14:01,430 --> 01:14:04,610
истинного PI, так почему это происходит хорошо,

1616
01:14:04,610 --> 01:14:06,290
если вы получаете вы знаете, если  у тебя только

1617
01:14:06,290 --> 01:14:06,739
ф

1618
01:14:06,739 --> 01:14:11,119
число образцов I, если я пробовал действие a1

1619
01:14:11,119 --> 01:14:12,559
конечное число раз, оно могло бы быть в

1620
01:14:12,559 --> 01:14:14,090
это конечное число раз, оно

1621
01:14:14,090 --> 01:14:15,409
выглядело слегка положительным,

1622
01:14:15,409 --> 01:14:17,630
как будто это была нулевая точка 1 вместо 0,

1623
01:14:17,630 --> 01:14:20,630
а затем, когда я принимаю свою политику, я собираюсь

1624
01:14:20,630 --> 01:14:22,190
максимизируйте их, поэтому я собираюсь

1625
01:14:22,190 --> 01:14:23,630
немедленно использовать тот

1626
01:14:23,630 --> 01:14:25,280
, который выглядит лучше, даже если это был

1627
01:14:25,280 --> 01:14:28,940
просто школьный шанс, поэтому вы

1628
01:14:28,940 --> 01:14:33,619
можете получить это смещение максимизации, и то

1629
01:14:33,619 --> 01:14:36,739
же самое может произойти с точки зрения MDP,

1630
01:14:36,739 --> 01:14:40,670
так что это обычно может случиться с вами

1631
01:14:40,670 --> 01:14:42,559
также можете посмотреть на некоторые хорошие примеры из

1632
01:14:42,559 --> 01:14:45,019
этой статьи Джона, так что Джон говорит, что это

1633
01:14:45,019 --> 01:14:47,239
николас и шимон, или где они показывают,

1634
01:14:47,239 --> 01:14:48,829
как это также может происходить в марковских

1635
01:14:48,829 --> 01:14:51,289
процессах принятия решений, где, по сути,

1636
01:14:51,289 --> 01:14:54,559
если вы оцениваете эти ключевые

1637
01:14:54,559 --> 01:14:56,480
функции, то вы собираетесь  быть своего

1638
01:14:56,480 --> 01:14:57,860
рода предвзятым к тому, что произошло, чтобы

1639
01:14:57,860 --> 01:15:00,019
посмотреть в ваших данных, и поэтому вы можете

1640
01:15:00,019 --> 01:15:03,110
иметь предвзятость максимизации, поэтому одна вещь, которая

1641
01:15:03,110 --> 01:15:05,539
была предложена, чтобы попытаться справиться с

1642
01:15:05,539 --> 01:15:07,130
этим случаем, называется двойным q-обучением,

1643
01:15:07,130 --> 01:15:11,480
и поэтому идея  вместо

1644
01:15:11,480 --> 01:15:13,760
одной ключевой функции или двух разных

1645
01:15:13,760 --> 01:15:16,429
контрольных функций, и мы собираемся создать

1646
01:15:16,429 --> 01:15:18,920
два независимых несмещенных оценщика Q,

1647
01:15:18,920 --> 01:15:20,630
и вы будете использовать один из них

1648
01:15:20,630 --> 01:15:23,150
для принятия решений, а другой —

1649
01:15:23,150 --> 01:15:26,239
для оценки значения  и это

1650
01:15:26,239 --> 01:15:27,590
позволит нам иметь беспристрастную

1651
01:15:27,590 --> 01:15:32,809
оценку, и причина, по которой вы можете

1652
01:15:32,809 --> 01:15:35,900
захотеть сделать это, заключается в том, что тогда

1653
01:15:35,900 --> 01:15:38,539


1654
01:15:38,539 --> 01:15:40,340
вы можете подумать о помощи в этом вопросе, что вы можете в конечном итоге быть чрезмерно предвзятым

1655
01:15:40,340 --> 01:15:41,449
к вещам, которые

1656
01:15:41,449 --> 01:15:42,980
выглядели хорошо, но теперь вы  настолько разделены,

1657
01:15:42,980 --> 01:15:44,630
например, между примерами, что вы

1658
01:15:44,630 --> 01:15:46,880
хороши, что вы можете оценить, насколько

1659
01:15:46,880 --> 01:15:49,730
хорошо действие по сравнению с тем, как вы

1660
01:15:49,730 --> 01:15:53,090
пытаетесь оценить свою политику, поэтому я

1661
01:15:53,090 --> 01:15:54,440
просто буду немного краток с этим

1662
01:15:54,440 --> 01:15:56,449
из-за  время q-обучения в основном

1663
01:15:56,449 --> 01:15:57,800
двойное обучение Q в основном означает, что

1664
01:15:57,800 --> 01:15:59,179
у нас будут эти две разные функции Q,

1665
01:15:59,179 --> 01:16:02,480
а затем с вероятностью 50%

1666
01:16:02,480 --> 01:16:04,340
мы будем обновлять 1 и с вероятностью 50%

1667
01:16:04,340 --> 01:16:06,260
мы будем обновлять другую, так что этот

1668
01:16:06,260 --> 01:16:12,650
лес и в этом  случае я г  onna skip

1669
01:16:12,650 --> 01:16:14,809
Я добавлю это на последних слайдах, я

1670
01:16:14,809 --> 01:16:17,119
хочу показать вам разницу, поэтому

1671
01:16:17,119 --> 01:16:19,489
разница здесь может быть значительной,

1672
01:16:19,489 --> 01:16:21,500
поэтому в этом случае это как бы

1673
01:16:21,500 --> 01:16:23,630
просмотр процента времени, когда мы предпринимаем

1674
01:16:23,630 --> 01:16:26,630
плохие действия в этой области, где  у вас

1675
01:16:26,630 --> 01:16:28,160
не могло быть в этом случае, у вас есть

1676
01:16:28,160 --> 01:16:30,320
сценарий, в котором на самом

1677
01:16:30,320 --> 01:16:32,390
деле неправильно поступать, но он стохастический, и поэтому

1678
01:16:32,390 --> 01:16:34,160
с небольшим объемом данных он может в конечном

1679
01:16:34,160 --> 01:16:35,960
итоге выглядеть лучше по сравнению с другим

1680
01:16:35,960 --> 01:16:38,719
вариантом, где вознаграждение является детерминированным

1681
01:16:38,719 --> 01:16:40,760
и на самом деле лучше, но  не имеет

1682
01:16:40,760 --> 01:16:43,070
стохастичности, и тогда q-обучение может

1683
01:16:43,070 --> 01:16:44,180
довольно сильно пострадать от этой

1684
01:16:44,180 --> 01:16:46,969
предвзятости к максимизации, если вы используете ту

1685
01:16:46,969 --> 01:16:48,380
же функцию q, по существу, для

1686
01:16:48,380 --> 01:16:50,150
немедленного определения своей политики,

1687
01:16:50,150 --> 01:16:53,300
что и для оценки ценности этой

1688
01:16:53,300 --> 01:16:56,150
политики, тогда как двойное Q-обучение работает

1689
01:16:56,150 --> 01:16:59,989
намного лучше  в этом случае, так что это

1690
01:16:59,989 --> 01:17:02,270
то, что нужно учитывать с точки зрения

1691
01:17:02,270 --> 01:17:03,710
реализации этих вещей, и

1692
01:17:03,710 --> 01:17:05,870
это тоже довольно небольшие накладные расходы, потому что

1693
01:17:05,870 --> 01:17:07,190
вы можете просто поддерживать две разные функции q,

1694
01:17:07,190 --> 01:17:10,550
и если это было

1695
01:17:10,550 --> 01:17:11,600
немного быстрее, я позабочусь о том, чтобы добавить

1696
01:17:11,600 --> 01:17:14,690
туда детали I, когда я буду загружать

1697
01:17:14,690 --> 01:17:17,540
дополнительные слайды сегодня, главное,

1698
01:17:17,540 --> 01:17:18,830
что вы должны знать о них сегодня, - это иметь

1699
01:17:18,830 --> 01:17:20,360
возможность понять, как вы делаете этот Монте-

1700
01:17:20,360 --> 01:17:22,580
Карло по контролю политики, и то же самое для

1701
01:17:22,580 --> 01:17:24,620
Изучение sarsa и Q полезно для

1702
01:17:24,620 --> 01:17:27,680
понимания того, как быстро они обновляются

1703
01:17:27,680 --> 01:17:29,150
с точки зрения того, нужно ли вам

1704
01:17:29,150 --> 01:17:30,680
ждать конца эпизода, и насколько быстро

1705
01:17:30,680 --> 01:17:32,719
информация распространяется обратно, а затем

1706
01:17:32,719 --> 01:17:34,370
также для понимания того, как определить

1707
01:17:34,370 --> 01:17:36,260
условия для алгоритмов, сходящихся

1708
01:17:36,260 --> 01:17:37,489
к оптимальным.  q функция

1709
01:17:37,489 --> 01:17:40,300
спасибо

