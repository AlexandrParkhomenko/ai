1
00:00:04,010 --> 00:00:06,750
Portion, and then a group portion.

2
00:00:06,750 --> 00:00:09,255
You'll be assigned groups in advance, um,

3
00:00:09,255 --> 00:00:10,755
and there'll be numbers on

4
00:00:10,755 --> 00:00:13,710
the chairs for the room that you're in so you'll know where to go sit.

5
00:00:13,710 --> 00:00:15,570
And the way that it will work is you'll first

6
00:00:15,570 --> 00:00:17,400
do the individual part, you'll turn that in,

7
00:00:17,400 --> 00:00:19,710
and then you'll receive another exam for the group part,

8
00:00:19,710 --> 00:00:22,320
and then go in and discuss your answers,

9
00:00:22,320 --> 00:00:24,405
agree on them, and then scratch that off

10
00:00:24,405 --> 00:00:26,610
to see if you got it right, and then when you're done,

11
00:00:26,610 --> 00:00:27,990
you'll hand that in as your group one.

12
00:00:27,990 --> 00:00:31,530
[NOISE] And, yeah.

13
00:00:31,530 --> 00:00:33,930
Uh, just, logist- logistical question.

14
00:00:33,930 --> 00:00:37,230
So we're splitting into the two groups again without being asked later?

15
00:00:37,230 --> 00:00:37,376
Yes.

16
00:00:37,376 --> 00:00:37,710
Okay.

17
00:00:37,710 --> 00:00:40,950
Great question. [inaudible] asked is whether or not we're splitting into two rooms,

18
00:00:40,950 --> 00:00:42,225
yes, we're again gonna do that,

19
00:00:42,225 --> 00:00:44,055
um, and that will be your assignment.

20
00:00:44,055 --> 00:00:46,220
Um, I think it's likely to be the same as last time,

21
00:00:46,220 --> 00:00:47,405
but I will confirm that.

22
00:00:47,405 --> 00:00:48,740
We might make it slightly different because

23
00:00:48,740 --> 00:00:52,715
some SC- SCPD students will be joining us for this one that didn't before and vice-versa.

24
00:00:52,715 --> 00:00:53,990
So it's possible, particularly,

25
00:00:53,990 --> 00:00:55,070
if you are right on the borderline,

26
00:00:55,070 --> 00:00:57,330
we'll, um, you'll be in a different room this time.

27
00:00:57,330 --> 00:01:00,690
So we'll announce that. Um, [NOISE] just as a reminder, right now,

28
00:01:00,690 --> 00:01:02,510
we're basically done with all of the assignments so this is

29
00:01:02,510 --> 00:01:04,640
a great chance to be focusing on the projects.

30
00:01:04,640 --> 00:01:08,815
Um, you should have been getting feedback about all of those along with, with, um,

31
00:01:08,815 --> 00:01:13,620
with a little bit of information back about your project and the person who graded it,

32
00:01:13,620 --> 00:01:14,700
will have signed that.

33
00:01:14,700 --> 00:01:18,450
So that's a great TA to go ask questions of and their office hours are on Piazza.

34
00:01:18,450 --> 00:01:21,740
Um, but you're welcome to go to any of the office hours to ask about project questions.

35
00:01:21,740 --> 00:01:26,925
[NOISE] And that poster session will be [NOISE] at the time,

36
00:01:26,925 --> 00:01:28,455
the original time announced.

37
00:01:28,455 --> 00:01:30,975
It is sub-optimal but what can you do?

38
00:01:30,975 --> 00:01:33,710
We're gonna meet in the morning on, uh, the last day of finals.

39
00:01:33,710 --> 00:01:36,590
[NOISE] So that was the one that we are assigned.

40
00:01:36,590 --> 00:01:39,755
Anybody else have any other questions about this?

41
00:01:39,755 --> 00:01:41,720
Who here went to Chelsea's talk on Monday?

42
00:01:41,720 --> 00:01:44,175
[NOISE] Okay.

43
00:01:44,175 --> 00:01:46,260
She does, uh, for those of you who did- didn't get to see it,

44
00:01:46,260 --> 00:01:47,460
Chelsea's talk will be online,

45
00:01:47,460 --> 00:01:49,595
it's a really nice talk about meta-reinforcement learning.

46
00:01:49,595 --> 00:01:51,920
It will be covered on the quiz next week,

47
00:01:51,920 --> 00:01:53,465
but pretty light, um,

48
00:01:53,465 --> 00:01:55,535
because you haven't had much exposure to that idea.

49
00:01:55,535 --> 00:01:57,530
So again, just as a recap for the quiz.

50
00:01:57,530 --> 00:02:00,155
The quiz will cover everything, um, in the course.

51
00:02:00,155 --> 00:02:03,660
But things that you didn't have a chance to actually think about, um,

52
00:02:03,660 --> 00:02:06,430
uh, because you didn't get practice on it with an assignment,

53
00:02:06,430 --> 00:02:08,240
uh, we will test more lightly.

54
00:02:08,240 --> 00:02:10,055
It also will be multiple choice.

55
00:02:10,055 --> 00:02:12,200
I highly recommend that you take the quiz from

56
00:02:12,200 --> 00:02:14,570
last year ignoring any topics that we haven't covered,

57
00:02:14,570 --> 00:02:18,050
[NOISE] um, and do that without looking at the answers.

58
00:02:18,050 --> 00:02:20,780
Um, one of the robust finding for

59
00:02:20,780 --> 00:02:22,160
educational research is that

60
00:02:22,160 --> 00:02:24,710
forced recall is incredibly effective at helping people learn.

61
00:02:24,710 --> 00:02:27,770
Forced recall is a nice way for testing [LAUGHTER].

62
00:02:27,770 --> 00:02:29,690
So, um, this is, of course,

63
00:02:29,690 --> 00:02:32,550
in the case where you're not getting assessed by it so you just can use that to,

64
00:02:32,550 --> 00:02:34,955
to check whether or not you understand these things and

65
00:02:34,955 --> 00:02:38,970
look at anythi- re-look up anything [NOISE] that might- you might have questions about.

66
00:02:38,990 --> 00:02:44,010
Any other questions? All right.

67
00:02:44,010 --> 00:02:45,675
So let's go ahead and get started.

68
00:02:45,675 --> 00:02:48,360
Um, we're gonna talk today about batch reinforcement learning,

69
00:02:48,360 --> 00:02:50,610
and in particular, safe batch reinforcement learning,

70
00:02:50,610 --> 00:02:52,115
I'll define that what that is.

71
00:02:52,115 --> 00:02:55,620
Um, this is a topic that I think is extremely important,

72
00:02:55,620 --> 00:02:57,170
we do a lot of work on it in my own lab.

73
00:02:57,170 --> 00:03:01,430
And most of the topics I'm gonna focus on today will be work that came out of,

74
00:03:01,430 --> 00:03:03,350
uh, work that was done by my postdoc,

75
00:03:03,350 --> 00:03:05,500
Phil Thomas, who's now a professor at UMass Amherst,

76
00:03:05,500 --> 00:03:07,715
but the work he did before he worked with me, um,

77
00:03:07,715 --> 00:03:11,280
some of our joint work, and I'll also highlight some other related work.

78
00:03:13,420 --> 00:03:15,860
So let's think about a simple case.

79
00:03:15,860 --> 00:03:18,545
Um, let's think about doing a scientific experiment, um,

80
00:03:18,545 --> 00:03:21,320
where you have a group of people who we're- we're gonna call the A group,

81
00:03:21,320 --> 00:03:25,025
and they get to first do this particular type of fractions problem.

82
00:03:25,025 --> 00:03:28,400
This is a fractions problem for one of our tutoring systems, um, uh,

83
00:03:28,400 --> 00:03:30,380
where people are having to add fractions

84
00:03:30,380 --> 00:03:32,960
together and then reduce the sum to the lowest terms.

85
00:03:32,960 --> 00:03:36,025
And then they have to do something where they do cross multiplication,

86
00:03:36,025 --> 00:03:39,060
um, and then after that, they get an exam.

87
00:03:39,060 --> 00:03:42,370
[NOISE] And, again, an average score of a 95.

88
00:03:42,370 --> 00:03:47,495
And then we have the B group that does the same activities but in the opposite order,

89
00:03:47,495 --> 00:03:50,575
and then they get an average score of 92.

90
00:03:50,575 --> 00:03:54,000
And the question is, for a new student, what should we do?

91
00:03:54,000 --> 00:03:57,020
[NOISE] And what would I- what would- what

92
00:03:57,020 --> 00:03:58,550
additional information might you want to

93
00:03:58,550 --> 00:04:01,790
know in order to be able to answer this question?

94
00:04:01,790 --> 00:04:04,220
So feel free to shout that out.

95
00:04:04,220 --> 00:04:07,550
So what would you need to do for like now, a new student comes along?

96
00:04:07,550 --> 00:04:16,730
[NOISE] Now,

97
00:04:16,730 --> 00:04:21,170
let's imagine our objective is to get a high score on this exam,

98
00:04:21,170 --> 00:04:23,480
for that student to get a high score on this exam.

99
00:04:23,480 --> 00:04:27,350
So what- which sequence of activities would you give to that new student in order

100
00:04:27,350 --> 00:04:30,919
to maximize their probability that they get a good score on this exam,

101
00:04:30,919 --> 00:04:33,395
indicating hopefully that they've learned the material?

102
00:04:33,395 --> 00:04:36,630
So, yeah.

103
00:04:36,630 --> 00:04:38,850
Do you know how big A and B are?

104
00:04:38,850 --> 00:04:40,845
So one great question. [inaudible] to say,

105
00:04:40,845 --> 00:04:42,390
um, how big is this group?

106
00:04:42,390 --> 00:04:48,525
Um, so how large is an A and B?

107
00:04:48,525 --> 00:04:50,450
And you might want to know this for a number of reasons.

108
00:04:50,450 --> 00:04:53,030
For example, if the number of people in group A is 1,

109
00:04:53,030 --> 00:04:54,935
and the number of people in group B is 2,

110
00:04:54,935 --> 00:04:57,425
maybe these are just sort of statistical noise

111
00:04:57,425 --> 00:04:59,990
between these di- distinctions. What else might you want to know?

112
00:04:59,990 --> 00:05:02,895
[NOISE] Um, the variance.

113
00:05:02,895 --> 00:05:05,300
Yes. So I suggest that you might want to know the variance.

114
00:05:05,300 --> 00:05:07,835
That's another thing you might want to know. This is just the mean.

115
00:05:07,835 --> 00:05:09,890
So what is the variance?

116
00:05:10,650 --> 00:05:18,735
What other pieces of information might you want to know? Yeah.

117
00:05:18,735 --> 00:05:22,070
Probably the difference between the median and the mean.

118
00:05:22,070 --> 00:05:24,950
Yeah. So other sort of forms of statistics about,

119
00:05:24,950 --> 00:05:26,660
sort of, you know, the distribution.

120
00:05:26,660 --> 00:05:29,110
I'm- I'm thinking about something also more,

121
00:05:29,110 --> 00:05:30,935
um, in a different direction. Yeah.

122
00:05:30,935 --> 00:05:32,570
Um, the people in the group.

123
00:05:32,570 --> 00:05:35,210
So is that bigger group A is all high school students,

124
00:05:35,210 --> 00:05:37,895
group B is all like lower grade or something like that.

125
00:05:37,895 --> 00:05:42,490
Exactly. So maybe the- maybe group A is kindergartners and maybe group B is,

126
00:05:42,490 --> 00:05:45,360
um, you know, high schoolers and [LAUGHTER],

127
00:05:45,360 --> 00:05:47,380
we're all regressing [LAUGHTER]. So yeah.

128
00:05:47,380 --> 00:05:48,680
So- so who's in these groups?

129
00:05:48,680 --> 00:05:53,240
[NOISE] And in addition to that,

130
00:05:53,240 --> 00:05:55,540
often, you'd want to know who was the new student.

131
00:05:55,540 --> 00:05:58,230
So is the new student then a kindergartner or a high schooler?

132
00:05:58,230 --> 00:06:01,130
Okay. So there's really a lot of additional information that you'd

133
00:06:01,130 --> 00:06:03,740
want to know in order to be able to answer this question precisely,

134
00:06:03,740 --> 00:06:06,380
uh, and it involves a lot of different challenges.

135
00:06:06,380 --> 00:06:09,455
And one of the challenges here is that, um,

136
00:06:09,455 --> 00:06:13,690
if group B is different than group A,

137
00:06:13,690 --> 00:06:17,195
then we have this fundamental issue of, um, censored data.

138
00:06:17,195 --> 00:06:20,810
You don't get to know what would have happened in group A if they had had

139
00:06:20,810 --> 00:06:22,700
the same seq- or in the group B if they'd had

140
00:06:22,700 --> 00:06:25,535
the same sequence of interventions as in group A.

141
00:06:25,535 --> 00:06:28,280
So this is sort of the fundamental challenge that you'll never know

142
00:06:28,280 --> 00:06:30,590
what it would be like if you were at Harvard right now, um,

143
00:06:30,590 --> 00:06:32,435
but, uh, but there's this, uh,

144
00:06:32,435 --> 00:06:35,300
you only get to observe the outcome for the action that's taken.

145
00:06:35,300 --> 00:06:37,150
We've seen that a lot in reinforcement learning,

146
00:06:37,150 --> 00:06:39,980
and that's true in this case too where we have old data, um,

147
00:06:39,980 --> 00:06:41,270
about sequences of decisions,

148
00:06:41,270 --> 00:06:43,915
and so it requires this kind of actual reasoning.

149
00:06:43,915 --> 00:06:47,255
Another thing that it involves is, um, generalization.

150
00:06:47,255 --> 00:06:51,785
So here's a simple example where you can think of it basically as being just two actions.

151
00:06:51,785 --> 00:06:53,960
Each of the two different prob- each of

152
00:06:53,960 --> 00:06:56,645
the two different problems and you can think of this as the reward.

153
00:06:56,645 --> 00:06:59,450
The delayed reward may be of a reward of 0,

154
00:06:59,450 --> 00:07:03,000
reward of 0, and then a reward of the test score.

155
00:07:04,000 --> 00:07:06,560
And so here there's only two actions,

156
00:07:06,560 --> 00:07:08,440
and who knows how big the state space is,

157
00:07:08,440 --> 00:07:10,520
it depends how we'd want to model students.

158
00:07:10,520 --> 00:07:13,190
Um, [NOISE] but we don't want to think about

159
00:07:13,190 --> 00:07:16,100
combinatorially all the different orders of actions.

160
00:07:16,100 --> 00:07:18,680
Um, and even if we're writing down a decision policy,

161
00:07:18,680 --> 00:07:21,215
that might start to be very large very quickly.

162
00:07:21,215 --> 00:07:23,780
And so we're gonna need to be able to do some form of generalization,

163
00:07:23,780 --> 00:07:25,490
either cross states, or actions,

164
00:07:25,490 --> 00:07:27,590
or both so that we don't have to run

165
00:07:27,590 --> 00:07:29,210
a combinatorial number of experiments to

166
00:07:29,210 --> 00:07:31,100
figure out what's most effective for student learning.

167
00:07:31,100 --> 00:07:35,090
[NOISE] So we're gonna talk about

168
00:07:35,090 --> 00:07:36,410
this problem today in the context of batch

169
00:07:36,410 --> 00:07:39,515
reinforcement learning which we can also think of as being offline.

170
00:07:39,515 --> 00:07:44,670
So this has to be offline batch RL,

171
00:07:44,670 --> 00:07:47,800
and this is frequently gonna generally be off policy.

172
00:07:47,800 --> 00:07:50,550
Now, we've seen a lot of off-policy learning before,

173
00:07:50,550 --> 00:07:52,615
Q-learning uses an off-policy algorithm.

174
00:07:52,615 --> 00:07:55,370
Um, but I want to distinguish here that what we're gonna be mostly

175
00:07:55,370 --> 00:07:58,745
focusing on today is the case where someone has already collected the data.

176
00:07:58,745 --> 00:08:00,950
So we already have a prior set of data, um,

177
00:08:00,950 --> 00:08:04,620
and then we're gonna want to use it to make better decisions going forward.

178
00:08:04,880 --> 00:08:07,910
Now, this problem comes up not just, um,

179
00:08:07,910 --> 00:08:10,925
in the case that I just mentioned, but in a huge number of different domains.

180
00:08:10,925 --> 00:08:14,120
You could argue that areas like econom- economics, um,

181
00:08:14,120 --> 00:08:17,780
and statistics, and epidemiology are constantly asking these sort of questions.

182
00:08:17,780 --> 00:08:20,600
Um, it comes up in things like maintenance, you know, um,

183
00:08:20,600 --> 00:08:21,860
what sort of order of, um,

184
00:08:21,860 --> 00:08:24,210
actions do you want to do to make sure that your machines,

185
00:08:24,210 --> 00:08:25,650
your cars run for longest.

186
00:08:25,650 --> 00:08:27,520
Um, it comes up in health care,

187
00:08:27,520 --> 00:08:29,690
like what sort of sequence of activity should we give to

188
00:08:29,690 --> 00:08:32,299
patients in order to maximize their outcomes,

189
00:08:32,299 --> 00:08:34,324
their quality-adjusted life years.

190
00:08:34,325 --> 00:08:38,150
And in many of these cases, it's gonna be state-dependent because what's gonna work

191
00:08:38,150 --> 00:08:41,630
best for patient A is gonna be different than what works best [NOISE] for patient B.

192
00:08:41,630 --> 00:08:44,720
[NOISE] Now, one of the big challenges here too is

193
00:08:44,720 --> 00:08:48,055
that when we think about a lot of the cases where we have this old data,

194
00:08:48,055 --> 00:08:50,120
it's gonna be high-stake scenarios,

195
00:08:50,120 --> 00:08:53,480
which means that whether it's because we have really expensive, you know,

196
00:08:53,480 --> 00:08:56,675
nuclear power equipment which we don't want to go wrong, um,

197
00:08:56,675 --> 00:08:58,070
or we're treating people,

198
00:08:58,070 --> 00:08:59,345
um, for, you know,

199
00:08:59,345 --> 00:09:01,700
really significant diseases, then we wanna

200
00:09:01,700 --> 00:09:04,670
make sure that we make good decisions in the future.

201
00:09:04,670 --> 00:09:07,810
So we might may or may not have a lot of data, um,

202
00:09:07,810 --> 00:09:10,070
but the data that we have is precious and we

203
00:09:10,070 --> 00:09:13,470
wanna make as good decisions as possible from that.

204
00:09:13,900 --> 00:09:16,940
So that means we need to have some form of

205
00:09:16,940 --> 00:09:20,255
confidence in how well it's gonna work going forward.

206
00:09:20,255 --> 00:09:23,210
So we would really like to have some sort of upper and

207
00:09:23,210 --> 00:09:25,580
lower bounds on its performance before we deploy it.

208
00:09:25,580 --> 00:09:26,660
Um, and in general,

209
00:09:26,660 --> 00:09:29,120
we just want good methods to try to estimate, um,

210
00:09:29,120 --> 00:09:30,440
if we do this counterfactual reasoning,

211
00:09:30,440 --> 00:09:33,050
if we think about how well people, or, you know,

212
00:09:33,050 --> 00:09:36,450
how much more healthy people might be if we were to treat them in a different way,

213
00:09:36,450 --> 00:09:41,850
um, how confident can we be before we convince a doctor to go actually deploy this.

214
00:09:42,370 --> 00:09:46,880
So what I'm gonna talk about today is thinking about sort of

215
00:09:46,880 --> 00:09:51,235
this general question of how can we do batch safe reinforcement learning.

216
00:09:51,235 --> 00:09:53,485
Safety can mean a lot of different things.

217
00:09:53,485 --> 00:09:55,665
Um, when I'm talking about safety today,

218
00:09:55,665 --> 00:09:58,370
I'm mostly gonna be thinking about this in terms of this confidence.

219
00:09:58,370 --> 00:10:00,050
This ability to say, um,

220
00:10:00,050 --> 00:10:01,475
before we deploy something,

221
00:10:01,475 --> 00:10:03,110
how good do we know it is.

222
00:10:03,110 --> 00:10:05,360
Um, now, there's different forms of safety.

223
00:10:05,360 --> 00:10:06,950
There's things like safe exploration,

224
00:10:06,950 --> 00:10:10,550
making sure you don't make mistakes online, there's risk sensitivity,

225
00:10:10,550 --> 00:10:12,205
thinking about the fact that,

226
00:10:12,205 --> 00:10:14,640
um, each of us is only gonna experience one outcome,

227
00:10:14,640 --> 00:10:16,035
not the expectation, um,

228
00:10:16,035 --> 00:10:19,325
so we may want to think about the full distribution instead of averaging.

229
00:10:19,325 --> 00:10:22,190
But what we're gonna talk about today is mostly still thinking about

230
00:10:22,190 --> 00:10:26,870
expected outcomes but thinking about being confident in the expected outcomes.

231
00:10:26,870 --> 00:10:29,705
And so, in general,

232
00:10:29,705 --> 00:10:32,689
we would like to really be able to say with high confidence

233
00:10:32,689 --> 00:10:36,145
this new decision policy we're gonna deploy for patients,

234
00:10:36,145 --> 00:10:37,840
or for nuclear power plants,

235
00:10:37,840 --> 00:10:41,195
or for other sorts of high-stakes scenarios, we think it's better.

236
00:10:41,195 --> 00:10:45,440
We think it is better than what we're currently doing. And why might you want this?

237
00:10:45,440 --> 00:10:46,910
You might want to, sort of, to guarantee kind of

238
00:10:46,910 --> 00:10:49,920
monotonic improvement particularly in these high-stakes domains,

239
00:10:49,920 --> 00:10:51,830
which is something that we've seen earlier this quarter.

240
00:10:51,830 --> 00:10:52,940
[NOISE] Okay.

241
00:10:52,940 --> 00:10:55,205
[NOISE]

242
00:10:55,205 --> 00:10:58,750
So let's talk just briefly about some of the notation, some of this will be familiar.

243
00:10:58,750 --> 00:11:00,640
I just wanna make sure that, we're all on board with that,

244
00:11:00,640 --> 00:11:02,830
and then I'll talk about sort of some of the different steps

245
00:11:02,830 --> 00:11:05,200
we might think about to try to create batch,

246
00:11:05,200 --> 00:11:08,560
um, safe reinforcement learning algorithms.

247
00:11:08,560 --> 00:11:12,190
So whereas usually gonna use pi to denote a policy,

248
00:11:12,190 --> 00:11:16,165
um, we're gonna use T or H to denote a trajectory.

249
00:11:16,165 --> 00:11:20,860
I'll often use big D to denote the data that we have access to.

250
00:11:20,860 --> 00:11:23,575
This is like [NOISE] electronic medical records systems,

251
00:11:23,575 --> 00:11:26,650
um, or you know data about power-plants, etc.

252
00:11:26,650 --> 00:11:28,600
And for most of this,

253
00:11:28,600 --> 00:11:32,450
we're gonna assume that we know what the behavior policy is.

254
00:11:33,000 --> 00:11:38,035
So we know what was the mapping of states to actions,

255
00:11:38,035 --> 00:11:41,950
or it could have been histories to actions that was used to gather the data.

256
00:11:41,950 --> 00:11:45,250
[NOISE] Can anybody give me an example where that might not be reasonable,

257
00:11:45,250 --> 00:11:47,470
where we might not know the behavior policy?

258
00:11:47,470 --> 00:11:54,760
[NOISE] [inaudible] actions, and we don't know the questions.

259
00:11:54,760 --> 00:11:55,360
Exactly.

260
00:11:55,360 --> 00:11:57,925
In many, many cases where the data is generated from humans,

261
00:11:57,925 --> 00:11:59,995
um, we will not know what pi_b is.

262
00:11:59,995 --> 00:12:02,725
So when we look at medical health data,

263
00:12:02,725 --> 00:12:04,435
we typically don't know what pi_b is.

264
00:12:04,435 --> 00:12:07,060
So, you know, if this is generated by doctors,

265
00:12:07,060 --> 00:12:13,450
[NOISE] we typically don't know what pi_b will be.

266
00:12:13,450 --> 00:12:18,040
There's obviously guidance, but we don't typically have access to, um,

267
00:12:18,040 --> 00:12:20,530
the exact policy people used and, and if we did,

268
00:12:20,530 --> 00:12:22,135
they probably wouldn't have phrased it as like,

269
00:12:22,135 --> 00:12:23,440
you know, a stochastic process.

270
00:12:23,440 --> 00:12:26,980
[NOISE] But like when someone comes into their office with probability 0.5,

271
00:12:26,980 --> 00:12:28,945
they're gonna treat them with this person 0.5.

272
00:12:28,945 --> 00:12:30,820
They probably think of it, in deterministic terms and they

273
00:12:30,820 --> 00:12:33,445
probably wouldn't think of it in terms of these if-then rules.

274
00:12:33,445 --> 00:12:35,605
So there are many cases where [NOISE] we don't have that.

275
00:12:35,605 --> 00:12:37,390
Um, we've done some work on that recently,

276
00:12:37,390 --> 00:12:38,590
other people have as well.

277
00:12:38,590 --> 00:12:40,300
I might talk about that briefly at the end.

278
00:12:40,300 --> 00:12:42,895
But for most of today, we're gonna assume that we have access to this.

279
00:12:42,895 --> 00:12:44,050
So can someone give me an example,

280
00:12:44,050 --> 00:12:46,000
where it is reasonable to assume that we have pi_b?

281
00:12:46,000 --> 00:12:56,740
[NOISE] Sure.

282
00:12:56,740 --> 00:12:59,590
[NOISE] [inaudible] is based on a [inaudible] set of guidelines or something?

283
00:12:59,590 --> 00:13:01,150
Yeah. So in some cases, you know,

284
00:13:01,150 --> 00:13:05,470
[NOISE] [inaudible] like sometimes you have like an algorithm to make a decision or,

285
00:13:05,470 --> 00:13:07,090
you know, a clear set of guidelines.

286
00:13:07,090 --> 00:13:09,160
Were you gonna say something similar or different?

287
00:13:09,160 --> 00:13:13,630
A little different. Um, so if you have a power plant with maintenance records,

288
00:13:13,630 --> 00:13:15,820
ah, [NOISE] like an established maintenance

289
00:13:15,820 --> 00:13:19,855
plan that has records [NOISE] match the plan then you basically have pi_b.

290
00:13:19,855 --> 00:13:22,495
That's right. Yeah. So in those real cases where you have these fixed protocols.

291
00:13:22,495 --> 00:13:24,280
Another example, I often think about is,

292
00:13:24,280 --> 00:13:27,745
you know when your decisions were made by reinforcement learning [LAUGHTER] agents,

293
00:13:27,745 --> 00:13:30,655
or, or they're made by supervised learning agents.

294
00:13:30,655 --> 00:13:32,905
Um, if you go to a lot of different,

295
00:13:32,905 --> 00:13:35,395
ah, like how, you know, Google serves ads.

296
00:13:35,395 --> 00:13:37,720
We know exactly how it serves ads, it has to all be logged.

297
00:13:37,720 --> 00:13:40,000
And there's a, there's an algorithm that is making that decision.

298
00:13:40,000 --> 00:13:41,320
So in many cases,

299
00:13:41,320 --> 00:13:46,060
[NOISE] we have access to the code that is being used to generate algorithms automa- ah,

300
00:13:46,060 --> 00:13:47,785
generate decisions [NOISE] automatically.

301
00:13:47,785 --> 00:13:49,360
In that case, we can just look it up,

302
00:13:49,360 --> 00:13:51,430
as long as we've saved it.

303
00:13:51,430 --> 00:13:55,645
So, and our objective is usual is to think about how do we get good,

304
00:13:55,645 --> 00:13:59,720
good policies out, and good policies with good values.

305
00:14:00,990 --> 00:14:06,835
Um, when we think about trying to do safe batch reinforcement learning in a setting,

306
00:14:06,835 --> 00:14:07,930
we're gonna be thinking about,

307
00:14:07,930 --> 00:14:09,385
how do we take that old data?

308
00:14:09,385 --> 00:14:13,855
So we're gonna take our data's input and push it through some black box,

309
00:14:13,855 --> 00:14:17,140
and get out a policy that we think is good.

310
00:14:17,140 --> 00:14:19,750
So we're gonna have sort of some algorithm or transformation that

311
00:14:19,750 --> 00:14:21,730
is instead of interacting with the real-world,

312
00:14:21,730 --> 00:14:23,800
and getting to make [NOISE] decisions and choose its data,

313
00:14:23,800 --> 00:14:25,420
it's just taking this fixed data,

314
00:14:25,420 --> 00:14:26,920
and it'll put a policy,

315
00:14:26,920 --> 00:14:29,635
and one thing that we would like is that,

316
00:14:29,635 --> 00:14:31,870
if we feed data into our algorithm,

317
00:14:31,870 --> 00:14:34,225
and our algorithm could be stochastic,

318
00:14:34,225 --> 00:14:37,030
then the value of the policy it outputs.

319
00:14:37,030 --> 00:14:40,375
So we can think A(D). So that's,

320
00:14:40,375 --> 00:14:44,755
you know, this is being our algorithm A, it's gonna output some policy.

321
00:14:44,755 --> 00:14:48,805
That might be a deterministic function, that might be a stochastic function.

322
00:14:48,805 --> 00:14:51,490
Whatever policy it outputs, we want it to be good.

323
00:14:51,490 --> 00:14:54,460
Ideally, at least as good as the behavior policy [NOISE].

324
00:14:54,460 --> 00:14:56,140
So that's what this first equation says,

325
00:14:56,140 --> 00:14:58,990
is it says the value of whatever policy is output,

326
00:14:58,990 --> 00:15:01,765
by our algorithm when we feed in some data set.

327
00:15:01,765 --> 00:15:03,625
We would [NOISE] like it to be as good,

328
00:15:03,625 --> 00:15:07,630
as what [NOISE] the behavior policy that generated that dataset,

329
00:15:07,630 --> 00:15:10,195
um, whatever that value was.

330
00:15:10,195 --> 00:15:19,540
So this is sort of [NOISE] the value of the policy used to generate the data.

331
00:15:22,860 --> 00:15:25,060
Now, we don't normally,

332
00:15:25,060 --> 00:15:28,720
we're not normally given via pi_b, um, directly.

333
00:15:28,720 --> 00:15:31,180
But can anybody give me an example of how we might learn that?

334
00:15:31,180 --> 00:15:33,970
[NOISE] Given a dataset,

335
00:15:33,970 --> 00:15:35,830
which was generated on policy,

336
00:15:35,830 --> 00:15:38,410
from that policy generated, um,

337
00:15:38,410 --> 00:15:42,430
using that policy. Yeah.

338
00:15:42,430 --> 00:15:48,310
Just do like dynamic programming or whatever on that small [inaudible] approximately.

339
00:15:48,310 --> 00:15:50,560
Yeah. So I think, like one thing is that,

340
00:15:50,560 --> 00:15:51,820
you know, you could use that data.

341
00:15:51,820 --> 00:15:53,860
Um, I don't know if you could do dynamic programming,

342
00:15:53,860 --> 00:15:56,380
because you don't necessarily have access to the transition and reward models.

343
00:15:56,380 --> 00:15:58,720
But you could do something like Monte Carlo estimation,

344
00:15:58,720 --> 00:16:00,565
could average the reward, um,

345
00:16:00,565 --> 00:16:03,880
let's assume in the dataset that you can see states, actions and rewards.

346
00:16:03,880 --> 00:16:05,395
Um, so you could certainly just average it,

347
00:16:05,395 --> 00:16:07,270
you know, average over all the trajectories.

348
00:16:07,270 --> 00:16:08,964
So we can get an estimate,

349
00:16:08,964 --> 00:16:12,940
we can estimate p pi_b,

350
00:16:12,940 --> 00:16:14,950
by looking at, let's imagine,

351
00:16:14,950 --> 00:16:16,825
it's an epic- episodic problem.

352
00:16:16,825 --> 00:16:18,820
So you can look at sum over all,

353
00:16:18,820 --> 00:16:22,150
equals 1 to number of trajectories,

354
00:16:22,150 --> 00:16:26,215
of the return for that trajectory. This is a return.

355
00:16:26,215 --> 00:16:29,725
Which is essentially just doing Monte Carlo estimation,

356
00:16:29,725 --> 00:16:32,200
because you know that that data was generated on policy,

357
00:16:32,200 --> 00:16:33,700
[NOISE] and so you can just average.

358
00:16:33,700 --> 00:16:37,270
[NOISE] So that'll give us a way to estimate the pi_b but then we

359
00:16:37,270 --> 00:16:41,050
need some way to estimate V of our algorithm,

360
00:16:41,050 --> 00:16:42,685
outputting D. Um,

361
00:16:42,685 --> 00:16:44,980
and that means we're gonna have to do something off policy,

362
00:16:44,980 --> 00:16:49,180
because in general we're gonna be wanting to find policies that are better than pi_b,

363
00:16:49,180 --> 00:16:53,125
which means that they would have had to be making some different decisions.

364
00:16:53,125 --> 00:16:55,780
And I'll just highlight here that you know,

365
00:16:55,780 --> 00:16:58,990
sometimes you might not just want to be better than the existing behavior policy,

366
00:16:58,990 --> 00:17:01,330
but you might need to be substantially better.

367
00:17:01,330 --> 00:17:04,060
Um, often, if we're thinking about real production systems,

368
00:17:04,060 --> 00:17:05,740
it costs time, and money,

369
00:17:05,740 --> 00:17:08,125
and effort, whenever we want to change protocols.

370
00:17:08,125 --> 00:17:10,630
If you want to get doctors to change the way they're making decisions,

371
00:17:10,630 --> 00:17:13,135
we want to change things in a power plant, there's often overhead.

372
00:17:13,135 --> 00:17:15,010
So often, you might not just need to be

373
00:17:15,010 --> 00:17:16,960
better than the existing sort of state of the art,

374
00:17:16,960 --> 00:17:18,760
you need to be significantly better.

375
00:17:18,760 --> 00:17:23,710
So the same types of ideas we're talking about for relative to sort of the current um,

376
00:17:23,710 --> 00:17:25,869
performance, you can always add a delta to

377
00:17:25,869 --> 00:17:29,120
that because they have to be at least this much better.

378
00:17:29,550 --> 00:17:31,780
And so again, just to sort of summarize,

379
00:17:31,780 --> 00:17:32,860
what does this equation says?

380
00:17:32,860 --> 00:17:35,200
It says, I want to have situations,

381
00:17:35,200 --> 00:17:37,285
where the policy that's output,

382
00:17:37,285 --> 00:17:38,830
when I plug in my dataset,

383
00:17:38,830 --> 00:17:43,090
I want that to be better than my existing policy with high probability.

384
00:17:43,090 --> 00:17:45,130
So delta here, you know,

385
00:17:45,130 --> 00:17:48,320
is gonna be something between 0 and 1.

386
00:17:50,820 --> 00:17:54,010
So now, let's talk about how we might do this.

387
00:17:54,010 --> 00:17:56,920
Um, we're going to start with off-policy policy evaluation.

388
00:17:56,920 --> 00:18:03,010
[NOISE] So the idea in this case,

389
00:18:03,010 --> 00:18:06,205
well, okay, first of all, go through all three of these really briefly.

390
00:18:06,205 --> 00:18:08,695
Um, and then we'll, we'll go step through them more slowly.

391
00:18:08,695 --> 00:18:10,930
So the three things in terms of stuff we

392
00:18:10,930 --> 00:18:13,495
might want to do safe batch reinforcement learning,

393
00:18:13,495 --> 00:18:16,945
and there's tons of variants for each of these depending on the setting we're looking at,

394
00:18:16,945 --> 00:18:21,190
is we need to re do- have to do this off-policy batch policy evaluation.

395
00:18:21,190 --> 00:18:24,535
Which is we need to be able to take our old data,

396
00:18:24,535 --> 00:18:28,015
and then use it to estimate how good an alternative policy would do.

397
00:18:28,015 --> 00:18:31,300
We might want to get confidence bounds over how good that is.

398
00:18:31,300 --> 00:18:34,390
So this could just allow us to get some estimate of V

399
00:18:34,390 --> 00:18:37,940
A(D), or V pi_e.

400
00:18:38,910 --> 00:18:42,490
Pi_e is often used to denote an evaluation policy,

401
00:18:42,490 --> 00:18:44,110
a policy we wanna evaluate.

402
00:18:44,110 --> 00:18:47,005
So the first thing is just doing off-policy policy evaluation.

403
00:18:47,005 --> 00:18:50,350
The second thing is saying how would we know how good that estimate is,

404
00:18:50,350 --> 00:18:53,230
so this is an estimate could be really good, could be bad,

405
00:18:53,230 --> 00:18:57,830
so you might want to have some uncertainty, over this estimate.

406
00:18:58,260 --> 00:19:01,840
So that we can quantify how good or bad it is.

407
00:19:01,840 --> 00:19:04,990
And then finally, we might want to be able to actually,

408
00:19:04,990 --> 00:19:08,170
take like, you know, an argmax over possible policies.

409
00:19:08,170 --> 00:19:11,919
So you might want to be able to do something like argmax,

410
00:19:11,919 --> 00:19:20,030
[NOISE] pi V pi_e, with some confidence bounds.

411
00:19:22,320 --> 00:19:25,450
So in general, you're not gonna just want to be able

412
00:19:25,450 --> 00:19:27,730
to evaluate how good alternative policies will be,

413
00:19:27,730 --> 00:19:30,385
you're gonna wanna figure out a good policy to deploy in the future,

414
00:19:30,385 --> 00:19:31,615
which is gonna require us to do

415
00:19:31,615 --> 00:19:34,885
optimization because we don't normally know what that good policy is yet.

416
00:19:34,885 --> 00:19:38,780
So typically, we're gonna end up evaluating a number of different policies.

417
00:19:40,860 --> 00:19:46,060
So we can think of it as sort of the first part is we're gonna take our historical data,

418
00:19:46,060 --> 00:19:47,455
take a proposed policy,

419
00:19:47,455 --> 00:19:49,900
plug it into some algorithm that we haven't talked about yet,

420
00:19:49,900 --> 00:19:53,810
and get out an estimate of the value of that policy,

421
00:19:54,150 --> 00:19:58,670
and we're gonna talk about how to do important sampling [NOISE] with that.

422
00:19:58,890 --> 00:20:02,635
And then after that, we're gonna go into the high-confidence,

423
00:20:02,635 --> 00:20:05,770
um, policy evaluation and safe policy improvement.

424
00:20:05,770 --> 00:20:07,450
To get confidence bounds,

425
00:20:07,450 --> 00:20:08,650
we're gonna look at Hoeffding.

426
00:20:08,650 --> 00:20:11,110
We've seen Hoeffding before, um,

427
00:20:11,110 --> 00:20:14,695
as something that we looked at when we were starting to talk about exploration.

428
00:20:14,695 --> 00:20:20,060
So when we look at high-confidence and then think back- think back to exploration.

429
00:20:22,770 --> 00:20:25,930
So in exploration, we're often trying to quantify how

430
00:20:25,930 --> 00:20:28,210
uncertain we were about the value of a policy or

431
00:20:28,210 --> 00:20:30,670
its models in order to be optimistic

432
00:20:30,670 --> 00:20:33,670
with respect to how good it could be and use that to drive exploration.

433
00:20:33,670 --> 00:20:35,200
But we also could have computed

434
00:20:35,200 --> 00:20:38,170
confidence bounds that are lower bounds on how good things could be,

435
00:20:38,170 --> 00:20:40,180
and that's gonna be useful here when we try to

436
00:20:40,180 --> 00:20:43,310
figure out how good policies are before we deploy them.

437
00:20:44,130 --> 00:20:46,405
And we'll do Hoeffding inequality for that,

438
00:20:46,405 --> 00:20:49,870
and then finally we're gonna be able to wanna do things like safe policy improvement,

439
00:20:49,870 --> 00:20:52,000
which is can you answer the question of saying,

440
00:20:52,000 --> 00:20:53,830
if someone gives you some data and they say,

441
00:20:53,830 --> 00:20:55,630
"Hey, can you give me a better policy?"

442
00:20:55,630 --> 00:20:59,170
Can one have an algorithm that either gives a better policy that it is

443
00:20:59,170 --> 00:21:02,575
actually better like when you go and deploy it with high probability?

444
00:21:02,575 --> 00:21:05,635
Or can the algorithm also know its limitations and say,

445
00:21:05,635 --> 00:21:11,065
"Nope, there's no- there's no way for me to give you a policy that's better."

446
00:21:11,065 --> 00:21:13,360
So I think it's also really nice to have

447
00:21:13,360 --> 00:21:15,550
algorithms that are aware of their own limitations.

448
00:21:15,550 --> 00:21:18,130
We're doing quite a bit of work on that in my lab right now,

449
00:21:18,130 --> 00:21:20,200
um, so that when people who are using these,

450
00:21:20,200 --> 00:21:22,585
particularly for human in the loop systems, um,

451
00:21:22,585 --> 00:21:25,750
that they can understand if the algorithm is giving out garbage or not.

452
00:21:25,750 --> 00:21:29,320
And so in this case, the idea is that sometimes if you have very little data,

453
00:21:29,320 --> 00:21:32,530
you can't do improvement in an, uh, confident way.

454
00:21:32,530 --> 00:21:34,300
So for that example I showed you before,

455
00:21:34,300 --> 00:21:36,625
we had like two different ways of teaching students,

456
00:21:36,625 --> 00:21:38,170
and someone, you know,

457
00:21:38,170 --> 00:21:41,020
made the good point of saying, how many people are in each of these.

458
00:21:41,020 --> 00:21:43,810
If only one person has tried either of these and someone says,

459
00:21:43,810 --> 00:21:45,820
"Can you definitely tell me what's going to be better

460
00:21:45,820 --> 00:21:47,950
for students in the future?" You should say, "No."

461
00:21:47,950 --> 00:21:49,795
[LAUGHTER] Because there is only one data point,

462
00:21:49,795 --> 00:21:52,165
like there's no way we would have enough data from,

463
00:21:52,165 --> 00:21:54,190
you know, one data point in each group to be able to

464
00:21:54,190 --> 00:21:57,130
confidently say in the future how we should teach students.

465
00:21:57,130 --> 00:22:02,020
So I think that the safe policy improvement needs us to be able to both say

466
00:22:02,020 --> 00:22:07,520
when we can be confident about deploying better policies in the future or when we can't.

467
00:22:08,430 --> 00:22:14,080
So we're gonna look at sort of a- a pipeline for how to answer that sort of question.

468
00:22:14,080 --> 00:22:16,285
All right. So let's first go back,

469
00:22:16,285 --> 00:22:19,880
go and think about off-policy policy evaluation.

470
00:22:20,070 --> 00:22:23,530
So the aim of this,

471
00:22:23,530 --> 00:22:28,450
um, is to get a- an off-policy estimate that is unbiased.

472
00:22:28,450 --> 00:22:29,875
So we want to get sort of a really,

473
00:22:29,875 --> 00:22:31,120
you know, a good estimate of how,

474
00:22:31,120 --> 00:22:34,960
um, how good an alternative decision policy would be.

475
00:22:34,960 --> 00:22:38,275
So we have data right now,

476
00:22:38,275 --> 00:22:40,000
which is sampled, um,

477
00:22:40,000 --> 00:22:43,720
under some policy, let's call it a behavior policy Pi 2.

478
00:22:43,720 --> 00:22:45,400
So we have like this dataset.

479
00:22:45,400 --> 00:22:51,415
This is D, which is giving us through these samples of these trajectories,

480
00:22:51,415 --> 00:22:55,885
and then we want to use them to evaluate how good an alternative policy would be.

481
00:22:55,885 --> 00:22:58,915
And while we could do this for something like Q-learning,

482
00:22:58,915 --> 00:23:01,030
we want to do it with a different method that's gonna allow

483
00:23:01,030 --> 00:23:03,535
us to get better confidence intervals,

484
00:23:03,535 --> 00:23:07,240
um, and where it's going to be an unbiased estimator.

485
00:23:07,240 --> 00:23:10,675
So in Q-learning, if we think back to sort of what Q-learning was doing,

486
00:23:10,675 --> 00:23:12,370
you know, Q-learning is off-policy.

487
00:23:12,370 --> 00:23:13,900
We could do this with Q-learning.

488
00:23:13,900 --> 00:23:16,165
Q-learning is off-policy,

489
00:23:16,165 --> 00:23:20,540
and it- it samples and it bootstraps.

490
00:23:24,180 --> 00:23:26,950
And because it samples and bootstraps,

491
00:23:26,950 --> 00:23:32,530
it can be biased, okay?

492
00:23:32,530 --> 00:23:35,470
And so we're wanna do a different thing right now,

493
00:23:35,470 --> 00:23:37,630
which still allows us to be off-policy,

494
00:23:37,630 --> 00:23:39,430
but in a way that we're not biased,

495
00:23:39,430 --> 00:23:42,340
that our estimator might not systematically be, um,

496
00:23:42,340 --> 00:23:45,625
above or below, particularly because right now we're always gonna have finite data.

497
00:23:45,625 --> 00:23:47,560
We're never gonna be in the asymptotic regime

498
00:23:47,560 --> 00:23:49,345
where we have tons and tons and tons of data,

499
00:23:49,345 --> 00:23:51,685
um, and we can sort of assume this went away.

500
00:23:51,685 --> 00:23:54,010
So again let's think about the return.

501
00:23:54,010 --> 00:23:55,270
G_t is the return.

502
00:23:55,270 --> 00:23:59,260
It's just how much reward we got under a policy, you know,

503
00:23:59,260 --> 00:24:03,910
either over a finite number of steps like for one episode or across all time.

504
00:24:03,910 --> 00:24:06,370
And our policy is just or the value of

505
00:24:06,370 --> 00:24:09,515
a policy again is just the expected discounted reward of that.

506
00:24:09,515 --> 00:24:14,235
So the nice thing is that if Pi 2 is stochastic,

507
00:24:14,235 --> 00:24:17,310
the data that you're using- that you're gathering from your behavior policy,

508
00:24:17,310 --> 00:24:19,850
then you can use it to do off-policy evaluation.

509
00:24:19,850 --> 00:24:22,810
This would have been essential for doing Q-learning too.

510
00:24:22,810 --> 00:24:25,030
And one of the nice things is that because we're

511
00:24:25,030 --> 00:24:27,400
kind of following this Monte Carlo type frame work,

512
00:24:27,400 --> 00:24:31,940
you don't need a model and you don't need to be Markov.

513
00:24:31,940 --> 00:24:36,240
That's really nice because we're gonna end up getting an estimator that is,

514
00:24:36,240 --> 00:24:41,080
um, unbiased and it does not rely on the Markov assumption holding.

515
00:24:41,080 --> 00:24:42,640
And in many cases,

516
00:24:42,640 --> 00:24:44,170
the Markov assumption is not gonna hold,

517
00:24:44,170 --> 00:24:47,620
particularly when we start to think about patient data or other cases

518
00:24:47,620 --> 00:24:51,280
where we just have a set of features that happen to have been recorded in our dataset,

519
00:24:51,280 --> 00:24:54,220
and who knows whether or not that system is Markov or not.

520
00:24:54,220 --> 00:24:56,980
We've certainly seen in some of our projects that it is not.

521
00:24:56,980 --> 00:24:58,360
And that in some of those cases,

522
00:24:58,360 --> 00:25:00,760
if you make the assumption that the world is Markov,

523
00:25:00,760 --> 00:25:03,220
you have really bad estimates of how good,

524
00:25:03,220 --> 00:25:06,340
you know, an alternative way of teaching students might do.

525
00:25:06,340 --> 00:25:09,520
Okay. So why is this a hard problem?

526
00:25:09,520 --> 00:25:13,030
Well, um, it's because we have a distribution mismatch, okay?

527
00:25:13,030 --> 00:25:15,730
So if we look at, um,

528
00:25:15,730 --> 00:25:18,909
imagine we just had a two-state process,

529
00:25:18,909 --> 00:25:20,830
where we thought about, you know,

530
00:25:20,830 --> 00:25:23,200
kind of this is S and this

531
00:25:23,200 --> 00:25:28,510
is- or like we can say this is the probability of your next state S prime,

532
00:25:28,510 --> 00:25:30,040
and I've sort of made it smooth.

533
00:25:30,040 --> 00:25:31,645
We can think of a Gaussian here,

534
00:25:31,645 --> 00:25:34,510
and this is under pi behavior.

535
00:25:34,510 --> 00:25:40,075
Under pi evaluation, it might look different, okay, versus this.

536
00:25:40,075 --> 00:25:43,360
In general, the distribution of returns you're gonna get,

537
00:25:43,360 --> 00:25:46,180
the sequence of the state-action, reward,

538
00:25:46,180 --> 00:25:47,740
next state, next action,

539
00:25:47,740 --> 00:25:52,525
next reward, so this sort of trajectory.

540
00:25:52,525 --> 00:25:56,920
The distribution of trajectory is gonna be different under different policies.

541
00:25:56,920 --> 00:26:01,480
So the distribution of

542
00:26:01,480 --> 00:26:08,720
tau here is not gonna look like the distribution of tau here.

543
00:26:09,000 --> 00:26:12,265
If it looks identical,

544
00:26:12,265 --> 00:26:14,920
what does that say about the value of the two policies?

545
00:26:14,920 --> 00:26:17,410
[inaudible].

546
00:26:17,410 --> 00:26:17,800
Sorry what?

547
00:26:17,800 --> 00:26:19,360
[inaudible].

548
00:26:19,360 --> 00:26:20,665
Exactly. So what you said is correct.

549
00:26:20,665 --> 00:26:23,290
If, um, I- if the distribution of

550
00:26:23,290 --> 00:26:26,320
states and actions that you get under both of these policies are identical,

551
00:26:26,320 --> 00:26:27,460
then the value is identical.

552
00:26:27,460 --> 00:26:29,590
And we saw this idea also in imitation learning when we

553
00:26:29,590 --> 00:26:32,305
are gonna be doing sort of state action,

554
00:26:32,305 --> 00:26:35,200
uh, or- or state feature matching.

555
00:26:35,200 --> 00:26:37,870
Now in this case, we're talking about not just states and actions,

556
00:26:37,870 --> 00:26:39,760
we're talking about full distributions or

557
00:26:39,760 --> 00:26:41,980
full trajectories because we're not making the Markov assumption,

558
00:26:41,980 --> 00:26:43,450
but the idea is exactly the same.

559
00:26:43,450 --> 00:26:47,110
The only way we define the- the value is basically

560
00:26:47,110 --> 00:26:51,040
the probability of a trajectory and the value of that or,

561
00:26:51,040 --> 00:26:55,070
you know, the sum of rewards in that trajectory.

562
00:26:55,470 --> 00:26:58,600
So if a distribution is identical,

563
00:26:58,600 --> 00:27:00,445
then the value is identical.

564
00:27:00,445 --> 00:27:01,960
We don't care if the policies are different

565
00:27:01,960 --> 00:27:03,640
because we already know how to estimate the value.

566
00:27:03,640 --> 00:27:07,300
[NOISE] So the key problem here is that they're gonna look different,

567
00:27:07,300 --> 00:27:11,395
which means that you would have went- done different things under different policy.

568
00:27:11,395 --> 00:27:13,705
So it's like, you know, right now,

569
00:27:13,705 --> 00:27:15,940
maybe you go and visit this part of the state space a lot,

570
00:27:15,940 --> 00:27:21,985
[NOISE] excuse me, [NOISE] and this part infrequently.

571
00:27:21,985 --> 00:27:23,830
And now you're gonna have an alternative policy,

572
00:27:23,830 --> 00:27:27,740
which only goes here infrequently and goes over there a lot.

573
00:27:28,650 --> 00:27:33,280
[NOISE] Excuse me. But thinking about it in this way gives us an idea about how

574
00:27:33,280 --> 00:27:38,180
we can look at our existing data over there and make it look more like that.

575
00:27:38,310 --> 00:27:41,260
Does anybody have any idea of how we could do that?

576
00:27:41,260 --> 00:27:44,080
So if someone gives you a bunch of trajectories, um,

577
00:27:44,080 --> 00:27:50,245
how might you maybe change them so they look like the distribution you care about? Yeah?

578
00:27:50,245 --> 00:27:51,295
Importance sampling.

579
00:27:51,295 --> 00:27:54,280
Right. So we can do importance sampling here, okay?

580
00:27:54,280 --> 00:28:00,340
So let's just review and refresh importance sampling.

581
00:28:00,340 --> 00:28:02,965
So the idea is that for any distribution,

582
00:28:02,965 --> 00:28:06,625
um, we can reweigh them to get an unbiased sample, okay?

583
00:28:06,625 --> 00:28:10,975
So let's imagine that we have data generated from, um,

584
00:28:10,975 --> 00:28:13,990
or we want data generated from some distribution q,

585
00:28:13,990 --> 00:28:17,110
we wanna estimate f(x), okay?

586
00:28:17,110 --> 00:28:22,615
So we'd have- wanna get f(x) under the probability distribution q(x).

587
00:28:22,615 --> 00:28:25,765
So we can multiply and divide by the same thing,

588
00:28:25,765 --> 00:28:28,735
let's incorporate another distribution.

589
00:28:28,735 --> 00:28:37,975
It's just a different distribution over x times q(x), f(x), dx, okay?

590
00:28:37,975 --> 00:28:42,100
So we can just rewrite this as being equal to integral

591
00:28:42,100 --> 00:28:46,390
over x probability of x times this quantity, which is q(x),

592
00:28:46,390 --> 00:28:51,020
divided by p(x) times f(x),

593
00:28:51,630 --> 00:28:55,675
and let's imagine that we actually have data from q(x).

594
00:28:55,675 --> 00:28:59,575
So we want data from q(x) but we have data from p(x).

595
00:28:59,575 --> 00:29:04,000
So we can approximate this by 1 over n, sum over i,

596
00:29:04,000 --> 00:29:10,000
q(xi) divided by p(xi) f(xi),

597
00:29:10,000 --> 00:29:15,920
where xi is sampled from p(x).

598
00:29:18,660 --> 00:29:21,790
I remember when I first learned about this a number of years ago and I thought it was

599
00:29:21,790 --> 00:29:25,000
a really lovely insight just to say we're just gonna reweight our data.

600
00:29:25,000 --> 00:29:27,445
And so we're gonna focus on, um,

601
00:29:27,445 --> 00:29:29,890
the data points that come from, you know,

602
00:29:29,890 --> 00:29:33,145
that- that are ones that we would sample under the distribution we care about.

603
00:29:33,145 --> 00:29:37,045
We're just gonna reweigh them so they look like they're having the same probability,

604
00:29:37,045 --> 00:29:39,415
um, that they would under- under q(x).

605
00:29:39,415 --> 00:29:42,715
In our case, under our desired policy.

606
00:29:42,715 --> 00:29:47,140
Okay. So importance sampling works for any distribution mismatch.

607
00:29:47,140 --> 00:29:49,780
If you have data from one distribution you wish you had it from another,

608
00:29:49,780 --> 00:29:51,730
um, those can come up in things like physics.

609
00:29:51,730 --> 00:29:54,805
Often you have really rare events like Higgs bosons.

610
00:29:54,805 --> 00:29:56,650
And in those cases, you might, um,

611
00:29:56,650 --> 00:29:58,960
there are different scenarios where you could reweigh things,

612
00:29:58,960 --> 00:30:02,425
um, so you can get an estimate of the true- the true expectation.

613
00:30:02,425 --> 00:30:06,235
Okay. This is for just generic distributions.

614
00:30:06,235 --> 00:30:09,655
Let's remind ourselves how this works for- for the reinforcement learning setting.

615
00:30:09,655 --> 00:30:11,620
So again, we're gonna have our episodes.

616
00:30:11,620 --> 00:30:13,690
We can call them h, we can call them tau,

617
00:30:13,690 --> 00:30:17,270
which is a sequence of states, actions, and rewards.

618
00:30:18,210 --> 00:30:22,060
And then if we wanna do importance sampling or let me just write this out.

619
00:30:22,060 --> 00:30:23,725
So, um, in this case,

620
00:30:23,725 --> 00:30:30,550
we're gonna wanna get something like p of hj under our desired policy, okay?

621
00:30:30,550 --> 00:30:32,380
So what does that gonna be?

622
00:30:32,380 --> 00:30:35,485
That's gonna be our probability of our initial state.

623
00:30:35,485 --> 00:30:39,580
Let's assume that's identical no matter what policy that you're in,

624
00:30:39,580 --> 00:30:45,670
and then we're gonna have the probability of taking a particular action,

625
00:30:45,670 --> 00:30:47,860
given we're in that state,

626
00:30:47,860 --> 00:30:49,570
times the probability that we go to

627
00:30:49,570 --> 00:30:56,600
a next state and the probability of the reward we saw.

628
00:30:58,470 --> 00:31:06,970
And we can sum that over j = 1 up to n - 1 or lj - 1.

629
00:31:06,970 --> 00:31:11,900
So we just repeatedly look at what was the probability we pick the action,

630
00:31:11,910 --> 00:31:18,650
the transition model, and the reward model.

631
00:31:21,120 --> 00:31:23,785
So that's how what we have for,

632
00:31:23,785 --> 00:31:25,870
um, the probability of a history.

633
00:31:25,870 --> 00:31:28,330
And then if we wanna do this for importance sampling,

634
00:31:28,330 --> 00:31:30,325
so what we want is we wanna have, um,

635
00:31:30,325 --> 00:31:32,920
the probability of this history, um,

636
00:31:32,920 --> 00:31:36,295
we need to be able to compute this ratio of q(x) divided by p(x),

637
00:31:36,295 --> 00:31:40,330
which for us is gonna be the probability of a history j under

638
00:31:40,330 --> 00:31:42,190
or the evaluation policy divided by

639
00:31:42,190 --> 00:31:45,800
the probability of a history j under the behavior policy.

640
00:31:45,990 --> 00:31:49,780
So- and we wanna do this and we're hoping that everything is gonna

641
00:31:49,780 --> 00:31:53,500
cancel because we don't have access to the dynamics model or the reward model.

642
00:31:53,500 --> 00:31:59,230
So unfortunately, just like what we saw in some of the policy gradient work, it will.

643
00:31:59,230 --> 00:32:03,920
So if we have probability of hj divided by pi b,

644
00:32:04,080 --> 00:32:07,780
we're gonna have again the initial state distribution,

645
00:32:07,780 --> 00:32:10,970
which will be the same in both cases,

646
00:32:11,520 --> 00:32:15,024
and then we have this ratio of probabilities,

647
00:32:15,024 --> 00:32:19,270
the probability of aj divided by sj.

648
00:32:19,270 --> 00:32:21,685
And this is under pi e,

649
00:32:21,685 --> 00:32:24,955
probability of aj divided, um,

650
00:32:24,955 --> 00:32:27,730
given sj for pi b,

651
00:32:27,730 --> 00:32:37,910
and then the transition model and reward model.

652
00:32:42,870 --> 00:32:51,470
Okay. And so this is nice because this cancels and this cancels.

653
00:32:51,470 --> 00:32:54,850
Because the dynamics of the world is what determines

654
00:32:54,850 --> 00:32:57,745
the next state and the dynamics of the world is what determines the reward.

655
00:32:57,745 --> 00:33:00,445
And notice here just to make this not incredibly long,

656
00:33:00,445 --> 00:33:02,740
I- here, I've made a Markov assumption.

657
00:33:02,740 --> 00:33:04,420
So this is the Markov version.

658
00:33:04,420 --> 00:33:06,820
[NOISE] But you could do,

659
00:33:06,820 --> 00:33:09,740
um, you could condition on the full history.

660
00:33:10,470 --> 00:33:16,495
So this trick does not require us- does not require the system to be Markov.

661
00:33:16,495 --> 00:33:18,970
Because no matter whether your- your dynamics

662
00:33:18,970 --> 00:33:21,175
depend on the full history or the immediate last step,

663
00:33:21,175 --> 00:33:24,565
they're gonna be the same in the behavior policy and in the dynamics.

664
00:33:24,565 --> 00:33:26,080
And in the evaluation policies,

665
00:33:26,080 --> 00:33:28,285
you can cancel these and same for the reward model.

666
00:33:28,285 --> 00:33:30,175
So this- this, um,

667
00:33:30,175 --> 00:33:33,325
insight does not require a Markovian assumption.

668
00:33:33,325 --> 00:33:37,585
And what that means is that we just end up getting this ratio of, um,

669
00:33:37,585 --> 00:33:43,930
the way we would pick actions under the evaluation policy divided by the ratio

670
00:33:43,930 --> 00:33:50,500
of the way we would pick actions under the behavior policy. Yeah?

671
00:33:50,500 --> 00:33:55,675
Assuming that, uh, same trajectory is generated by two different policies.

672
00:33:55,675 --> 00:33:57,265
Great question. Um, yes,

673
00:33:57,265 --> 00:33:59,740
we're assuming the same trajectory was generated by two different policies.

674
00:33:59,740 --> 00:34:01,180
Yes. So we're saying for this trajectory,

675
00:34:01,180 --> 00:34:03,100
what's the probability you would have seen this under

676
00:34:03,100 --> 00:34:06,355
the behavior policy versus the evaluation policy?

677
00:34:06,355 --> 00:34:10,795
And so if it was more likely under the evaluation policy,

678
00:34:10,795 --> 00:34:14,050
we wanna upweight whatever reward we got for that trajectory.

679
00:34:14,050 --> 00:34:18,175
And if it's less likely under that evaluation policy, we wanna downweight it.

680
00:34:18,175 --> 00:34:20,830
So the intuition is that we have a bunch of, um,

681
00:34:20,830 --> 00:34:23,290
uh, trajectories and their sum of rewards.

682
00:34:23,290 --> 00:34:25,630
So we kind of have these h,

683
00:34:25,630 --> 00:34:28,820
you know, h_1, G_1 pairs.

684
00:34:29,250 --> 00:34:35,180
So we have these sort of trajectory, sum of rewards.

685
00:34:35,460 --> 00:34:38,440
And if we had the same behavior policy as the

686
00:34:38,440 --> 00:34:42,639
evaluation policy in order to know how good that evaluation policy is,

687
00:34:42,639 --> 00:34:45,909
we just average all of those G's but they are different.

688
00:34:45,909 --> 00:34:48,564
And so what we wanna do is we wanna say for

689
00:34:48,565 --> 00:34:52,675
h's that are more likely under an evaluation policy, upweight those.

690
00:34:52,675 --> 00:34:56,605
For h's that are less likely under a evaluation policy,

691
00:34:56,605 --> 00:34:58,810
downweight those so that you get, um,

692
00:34:58,810 --> 00:35:04,130
a true expectation when you do those G evaluation, G weightings.

693
00:35:05,850 --> 00:35:09,460
Does that makes sense? Does anybody have any questions about that part?

694
00:35:09,460 --> 00:35:12,850
So this is just so far telling us how we re-weight our data.

695
00:35:12,850 --> 00:35:16,630
It's allowing us to get a distribution that looks more like the distribution of

696
00:35:16,630 --> 00:35:21,380
trajectories we would get under our evaluation policy. Yeah?

697
00:35:24,930 --> 00:35:28,150
In the final, ah, the final row,

698
00:35:28,150 --> 00:35:31,375
we have denominator, ah, the behavior policy.

699
00:35:31,375 --> 00:35:33,940
We get that as empirically?

700
00:35:33,940 --> 00:35:37,300
Good question. You know, where does the behavior policy come from,

701
00:35:37,300 --> 00:35:41,455
basically like- like what is these probabilities? Is that the question?

702
00:35:41,455 --> 00:35:43,300
So I think it depends, So in the case,

703
00:35:43,300 --> 00:35:45,730
um- if you're- if it's a machine learning algorithm,

704
00:35:45,730 --> 00:35:47,260
you still generate your data, you just know it.

705
00:35:47,260 --> 00:35:48,910
Like you would just look up in your algorithm and

706
00:35:48,910 --> 00:35:50,770
see what the probability distribution is.

707
00:35:50,770 --> 00:35:54,640
And for today, we're going to assume that this is known and it's perfect.

708
00:35:54,640 --> 00:35:58,495
Um, that is obviously not true when we get into people data.

709
00:35:58,495 --> 00:35:59,770
In those cases, um,

710
00:35:59,770 --> 00:36:01,225
there's a couple of different things you can do.

711
00:36:01,225 --> 00:36:05,180
One is that you can build estimators that are robust to this being wrong.

712
00:36:05,180 --> 00:36:09,045
So you can use other ways to try to kind of be doubly robust if that estimator is wrong.

713
00:36:09,045 --> 00:36:11,160
In others you can take the empirical distribution.

714
00:36:11,160 --> 00:36:12,690
And actually there's some cool work recently,

715
00:36:12,690 --> 00:36:14,940
I think was from Peter Stone's lab at UT Austin,

716
00:36:14,940 --> 00:36:17,310
showing that sometimes you- it's better to

717
00:36:17,310 --> 00:36:20,675
use the empirical estimate then even if you know the true estimate.

718
00:36:20,675 --> 00:36:24,620
Like in terms of the resulting estimator, which is kinda cool.

719
00:36:26,400 --> 00:36:31,855
Okay, so this is just writing that out in LaTex instead of me hand-writing it,

720
00:36:31,855 --> 00:36:33,910
um, and so this just writes out,

721
00:36:33,910 --> 00:36:36,775
um, [NOISE] the- the probability of a history.

722
00:36:36,775 --> 00:36:40,000
And now we can see that equation that I put here.

723
00:36:40,000 --> 00:36:41,890
So, um, you know,

724
00:36:41,890 --> 00:36:44,845
the value of the policy that you wanna evaluate, um,

725
00:36:44,845 --> 00:36:48,520
is gonna be this ratio of history's times the return of that history.

726
00:36:48,520 --> 00:36:52,090
And what we said here is,

727
00:36:52,090 --> 00:36:54,730
and this is, you know, one over n,

728
00:36:54,730 --> 00:36:59,770
is that this is simply the probability of taking each of

729
00:36:59,770 --> 00:37:05,575
those actions or I'll write it just in terms of the Pi notation.

730
00:37:05,575 --> 00:37:12,790
So this is Pi e of aj given sji,

731
00:37:12,790 --> 00:37:22,970
i equals one to the- the length of your trajectory divide by Pi b aji.

732
00:37:25,680 --> 00:37:29,620
All times the return of that particular history.

733
00:37:29,620 --> 00:37:35,920
[NOISE] So the beautiful thing is you can- this is an unbiased estimator.

734
00:37:35,920 --> 00:37:40,435
This really does give you a good estimate of the value under a few assumptions,

735
00:37:40,435 --> 00:37:42,865
um, which I'll ask you guys about in a second.

736
00:37:42,865 --> 00:37:48,160
And, um, you don't need to know the dynamics, so no dynamics.

737
00:37:48,890 --> 00:37:52,120
No reward.

738
00:37:52,340 --> 00:37:55,510
No need to be Markov.

739
00:37:57,270 --> 00:38:01,720
Can anybody, um, [NOISE] tell me case where maybe this doesn't work.

740
00:38:01,720 --> 00:38:04,795
So, I was just seeing that if you use this for running,

741
00:38:04,795 --> 00:38:07,960
then starting from your initial policy to your final policy,

742
00:38:07,960 --> 00:38:09,100
they couldn't be that different, right?

743
00:38:09,100 --> 00:38:10,870
Because otherwise then you- the

744
00:38:10,870 --> 00:38:13,585
samples [BACKGROUND] from the previous strikers won't be useful.

745
00:38:13,585 --> 00:38:16,195
That's a good question. That's exactly what I was asking about is, you know,

746
00:38:16,195 --> 00:38:19,210
how different can Pi e and Pi b [BACKGROUND] and allow us to do this.

747
00:38:19,210 --> 00:38:21,580
So can- that was exactly what I was about to ask you guys about.

748
00:38:21,580 --> 00:38:23,050
So, can anyone give me, uh,

749
00:38:23,050 --> 00:38:26,530
what they think might be a condition for this estimator to be valid.

750
00:38:26,530 --> 00:38:30,670
Like, where might be some cases where you would expect this might do

751
00:38:30,670 --> 00:38:35,740
badly in terms of differences between the evaluation policy and the behavior policy?

752
00:38:35,740 --> 00:38:45,520
And it has to do with the probability of taking actions in a certain state.

753
00:38:45,520 --> 00:38:48,940
If either of those probabilities are too small,

754
00:38:48,940 --> 00:38:51,790
you are gonna have things blow up in bad ways.

755
00:38:51,790 --> 00:38:53,425
Yeah, so in which that,

756
00:38:53,425 --> 00:38:54,760
either of these probabilities are too small.

757
00:38:54,760 --> 00:38:56,590
This might be bad. Which of these ways is worse?

758
00:38:56,590 --> 00:38:59,410
If they- uh, it's not [inaudible].

759
00:38:59,410 --> 00:39:02,680
Right. So pi b is really small or at, you know, 0 [LAUGHTER].

760
00:39:02,680 --> 00:39:06,565
Um, this could be really bad now.

761
00:39:06,565 --> 00:39:10,660
Um, pi b can't ever be 0 and us observe something.

762
00:39:10,660 --> 00:39:13,690
So that's good, because we're getting data from pi b.

763
00:39:13,690 --> 00:39:15,910
And so we have never observed a trajectory under which

764
00:39:15,910 --> 00:39:19,465
pi b is 0 but it could be really, really small.

765
00:39:19,465 --> 00:39:23,380
So it could be, you know, you see something and it's incredibly unlikely there,

766
00:39:23,380 --> 00:39:26,035
but your behavior policy would have generated that.

767
00:39:26,035 --> 00:39:29,590
Um, and what if it is 0?

768
00:39:29,590 --> 00:39:32,425
So it might be 0 for some actions.

769
00:39:32,425 --> 00:39:36,820
What would that do if it's 0 in places where pi e is not 0 ?

770
00:39:36,820 --> 00:39:44,035
So what happens if pi b of a some particular a is equal to 0, uh,

771
00:39:44,035 --> 00:39:50,845
but pi e of a greater than 0 is,

772
00:39:50,845 --> 00:39:52,480
you know, greater than 0, might be 1.

773
00:39:52,480 --> 00:39:54,310
[NOISE] What might be bad there?

774
00:39:54,310 --> 00:39:59,130
Like do you think this is a- let's raise hands for a second.

775
00:39:59,130 --> 00:40:01,710
If that happened, do you think we are hu- let's raise your hands, either

776
00:40:01,710 --> 00:40:05,765
yes if you think this is a good estimator or no if you think, oh, that's rough.

777
00:40:05,765 --> 00:40:09,970
So if the behavior policy is 0 probability of taking an action,

778
00:40:09,970 --> 00:40:13,495
but the evaluation policy has a positive probability of taking an action.

779
00:40:13,495 --> 00:40:15,685
Raise your hand if you think, um,

780
00:40:15,685 --> 00:40:19,645
this estimator could be really bad. That's right.

781
00:40:19,645 --> 00:40:23,380
Yes. So, you know, if there are cases where you're just never trying actions,

782
00:40:23,380 --> 00:40:24,940
like you never saw actions in your data that

783
00:40:24,940 --> 00:40:28,240
your new evaluation policy would take, you can't use this.

784
00:40:28,240 --> 00:40:29,920
So we often call this as coverage.

785
00:40:29,920 --> 00:40:32,720
So coverage or support.

786
00:40:32,720 --> 00:40:36,780
So we often make a few basic assumptions in order for this to be valid.

787
00:40:36,780 --> 00:40:39,630
So our coverage or support normally means that pi b of

788
00:40:39,630 --> 00:40:44,520
a given s has to be greater than 0 for all a and s,

789
00:40:44,520 --> 00:40:48,925
such that pi e of a given s is greater than 0.

790
00:40:48,925 --> 00:40:50,800
So you kind of have to have support over.

791
00:40:50,800 --> 00:40:52,630
It doesn't have to be non-zero everywhere.

792
00:40:52,630 --> 00:40:54,715
But for anything that you might want to evaluate,

793
00:40:54,715 --> 00:40:56,950
for anywhere if you're really going to generate data from

794
00:40:56,950 --> 00:40:59,800
your evaluation policy and it might take an action,

795
00:40:59,800 --> 00:41:02,710
you need to be able to get to that state and you need to be able

796
00:41:02,710 --> 00:41:05,635
to take that action with some non-zero probability. Yeah.

797
00:41:05,635 --> 00:41:09,055
So terminology questions. So, we're calling pi b

798
00:41:09,055 --> 00:41:12,835
is the same as pi 2 in the other part, right?

799
00:41:12,835 --> 00:41:13,405
That's right.

800
00:41:13,405 --> 00:41:15,760
Okay. Originally I thought you said that the evaluation policy was

801
00:41:15,760 --> 00:41:18,295
the one that you observed the data from. That's incorrect?

802
00:41:18,295 --> 00:41:20,335
Thanks for making me clarify that.

803
00:41:20,335 --> 00:41:23,680
Um, the- the- the behavior policy is always one you observe data from,

804
00:41:23,680 --> 00:41:26,095
evaluation is the way you wanna look at.

805
00:41:26,095 --> 00:41:30,895
And, I apologize, notation often here is a little bit [NOISE] snaggly because I think, um,

806
00:41:30,895 --> 00:41:35,680
people sometimes call the evaluation policy the target policy or evaluation policy or,

807
00:41:35,680 --> 00:41:37,150
you know, one or two, um,

808
00:41:37,150 --> 00:41:39,040
and most of the time,

809
00:41:39,040 --> 00:41:43,840
the policy used to gather the data is called the behavior policy. Yeah.

810
00:41:43,840 --> 00:41:49,915
[inaudible] you just like not include it in the product.

811
00:41:49,915 --> 00:41:52,405
Question is whether or not what if they're both 0,

812
00:41:52,405 --> 00:41:53,560
um, is that a problem?

813
00:41:53,560 --> 00:41:55,570
Would you ever get data from that?

814
00:41:55,570 --> 00:41:58,015
So, yes. So if they're both 0, it's okay.

815
00:41:58,015 --> 00:42:00,880
So you don't actually have to- it does not require you to

816
00:42:00,880 --> 00:42:05,740
take- to have a non-zero probability of taking every action in every state.

817
00:42:05,740 --> 00:42:07,870
Um, so it can be okay.

818
00:42:07,870 --> 00:42:12,460
So pi b of a given s can't equal 0,

819
00:42:12,460 --> 00:42:16,330
if pi e of a given s is equal to 0.

820
00:42:16,330 --> 00:42:18,820
It's fine for that to be the case.

821
00:42:18,820 --> 00:42:21,010
You just can't have any case where you would never

822
00:42:21,010 --> 00:42:23,200
have either reached the state or generated

823
00:42:23,200 --> 00:42:26,035
that action for things that you could have potentially done

824
00:42:26,035 --> 00:42:29,320
under your, um, evaluation policy.

825
00:42:29,320 --> 00:42:32,395
And that doesn't sound too strict.

826
00:42:32,395 --> 00:42:34,330
Um, but in practice, that can be a big deal.

827
00:42:34,330 --> 00:42:36,760
So if you think about like Montezuma's Revenge,

828
00:42:36,760 --> 00:42:38,845
excuse me, or different forms of Atari,

829
00:42:38,845 --> 00:42:40,960
like under a random behavior policy,

830
00:42:40,960 --> 00:42:43,105
you're never gonna get to see a lot of states,

831
00:42:43,105 --> 00:42:45,250
and you're never gonna take actions in those states.

832
00:42:45,250 --> 00:42:47,380
Like you're just- uh, it's incredibly unlikely,

833
00:42:47,380 --> 00:42:49,240
unless you have an enormous amount of data.

834
00:42:49,240 --> 00:42:51,160
So in practice, you can think of sort of

835
00:42:51,160 --> 00:42:54,115
the behavior policy you have is kinda of defining like,

836
00:42:54,115 --> 00:42:58,720
um, like a ball [NOISE] under which you can evaluate other potential policies.

837
00:42:58,720 --> 00:43:02,335
So if you have like- it's not actually a sphere,

838
00:43:02,335 --> 00:43:03,760
but like, you know, if it's, um,

839
00:43:03,760 --> 00:43:05,560
if you have a behavior policy here,

840
00:43:05,560 --> 00:43:09,580
you can think of kind of having some distance metric under which you can still get

841
00:43:09,580 --> 00:43:15,025
good estimates of pi e. So you kind of have a radius,

842
00:43:15,025 --> 00:43:18,610
and it depends on these are sort of essentially the- the policies for

843
00:43:18,610 --> 00:43:23,060
which you have support over and anything else you can't evaluate.

844
00:43:24,210 --> 00:43:26,605
Okay, all right.

845
00:43:26,605 --> 00:43:28,750
So just to summarize there, um, you know,

846
00:43:28,750 --> 00:43:31,930
importance sampling is this beautiful idea that works for lots of statistics,

847
00:43:31,930 --> 00:43:33,130
including for reinforcement learning.

848
00:43:33,130 --> 00:43:35,620
I think- The first introduction to this,

849
00:43:35,620 --> 00:43:40,390
um- I think it was first used for RL I think in,

850
00:43:40,390 --> 00:43:43,990
um, Doina Precup's paper, Precup 2000?

851
00:43:43,990 --> 00:43:46,765
Around then. It's been around for a lot longer, but, um,

852
00:43:46,765 --> 00:43:47,800
but I think for reinforcement learning,

853
00:43:47,800 --> 00:43:49,630
that was their first introduction of using this.

854
00:43:49,630 --> 00:43:53,770
And of course these ideas also come up in policy gradients type- type methods.

855
00:43:53,770 --> 00:43:56,080
Um, and the great thing is thi- is,

856
00:43:56,080 --> 00:43:58,810
is this unbiased and consistent estimator,

857
00:43:58,810 --> 00:44:00,850
just as a refresher consistent means that

858
00:44:00,850 --> 00:44:03,820
asymptotically it really will give you the right estimate.

859
00:44:03,820 --> 00:44:07,465
So, you know, as n goes to infinity,

860
00:44:07,465 --> 00:44:16,050
the r estimated v pi e goes to e pi e. Just kind of a nice sanity check.

861
00:44:16,050 --> 00:44:18,840
As you get more and more data, you will get the right estimator.

862
00:44:18,840 --> 00:44:21,435
Um, and just to check here, this is, you know,

863
00:44:21,435 --> 00:44:24,855
under- under a few assumptions.

864
00:44:24,855 --> 00:44:26,730
You have to have support.

865
00:44:26,730 --> 00:44:32,665
[BACKGROUND]

866
00:44:32,665 --> 00:44:34,645
All right. Now, um,

867
00:44:34,645 --> 00:44:36,520
in our particular case,

868
00:44:36,520 --> 00:44:40,675
I- we can leverage a few aspects of the fact that this is a temporal process.

869
00:44:40,675 --> 00:44:43,945
So again, like what we saw for policy gradient type work,

870
00:44:43,945 --> 00:44:48,235
um, we'll call policy gradient methods.

871
00:44:48,235 --> 00:44:49,795
We can leverage the fact that,

872
00:44:49,795 --> 00:44:53,095
um, the future can't affect past rewards.

873
00:44:53,095 --> 00:44:56,380
So when we think about generating these importance ratios,

874
00:44:56,380 --> 00:45:00,235
we only have to for a particular time step t,

875
00:45:00,235 --> 00:45:05,710
um, instead of multiplying it by- so I guess just to back up for there.

876
00:45:05,710 --> 00:45:08,980
Remember that Gt is defined to be equal to, [NOISE] you know,

877
00:45:08,980 --> 00:45:13,990
the rewards, [NOISE] like the sum of rewards.

878
00:45:13,990 --> 00:45:17,500
So when we think about this equation for importance sampling,

879
00:45:17,500 --> 00:45:19,540
um, let me just go back to here.

880
00:45:19,540 --> 00:45:22,525
So this could be expanded into, you know,

881
00:45:22,525 --> 00:45:28,450
r_1 + gamma times r_2 + gamma squared times r_3, dot, dot, dot.

882
00:45:28,450 --> 00:45:30,370
And right now in that equation,

883
00:45:30,370 --> 00:45:35,469
that's like multiplying your full product of importance ratios,

884
00:45:35,469 --> 00:45:37,970
times e to the rewards.

885
00:45:37,970 --> 00:45:40,290
Um, so we're not doing any,

886
00:45:40,290 --> 00:45:45,135
it's the same ratio of probability of action given by probability of action,

887
00:45:45,135 --> 00:45:47,775
multiplied by each of these different reward terms.

888
00:45:47,775 --> 00:45:50,310
But since, you know,

889
00:45:50,310 --> 00:45:54,885
r_3 can't be affected by any actions that are in r_4 longer.

890
00:45:54,885 --> 00:45:57,210
In some ways you're just- this isn't wrong.

891
00:45:57,210 --> 00:45:59,750
It's just that you're introducing additional variance.

892
00:45:59,750 --> 00:46:03,385
So similar to what we saw, um,

893
00:46:03,385 --> 00:46:05,920
in policy gradient stuff,

894
00:46:05,920 --> 00:46:09,145
we don't actually- we only have to multiply by that product of ratios,

895
00:46:09,145 --> 00:46:11,815
up to the time point at which we got that reward.

896
00:46:11,815 --> 00:46:15,445
So this allows us to get to per-decision importance sampling.

897
00:46:15,445 --> 00:46:20,635
So this is only up to- only [NOISE] up to

898
00:46:20,635 --> 00:46:27,790
point got reward [NOISE] because the- the future can't inputs past rewards.

899
00:46:27,790 --> 00:46:30,160
And again, this is independent of it being Markov or not.

900
00:46:30,160 --> 00:46:35,300
So this is just the fact that it's a temporal process and we can't go back in time.

901
00:46:37,950 --> 00:46:42,400
All right. So another thing just in general is that, um,

902
00:46:42,400 --> 00:46:44,575
in importance sampling, um,

903
00:46:44,575 --> 00:46:46,165
we're just sort of these weights,

904
00:46:46,165 --> 00:46:47,860
these weights to these products.

905
00:46:47,860 --> 00:46:49,660
You know, products of like,

906
00:46:49,660 --> 00:46:53,930
um, picking an action under different policies.

907
00:46:55,110 --> 00:46:58,735
So we often call these weight terms

908
00:46:58,735 --> 00:47:00,160
nicely and confusingly with

909
00:47:00,160 --> 00:47:02,260
all the weights we talked about with function approximation.

910
00:47:02,260 --> 00:47:04,630
Um, and weighted importance sampling compared to

911
00:47:04,630 --> 00:47:08,210
importance sampling just renormalizes by the sum of weights.

912
00:47:08,820 --> 00:47:12,640
The reason you might wanna do this is that as we were talking about before,

913
00:47:12,640 --> 00:47:15,190
if your pi_b is really tiny.

914
00:47:15,190 --> 00:47:18,800
So let's say this, this might be super tiny,

915
00:47:19,200 --> 00:47:23,500
super small [NOISE] for some trajectories,

916
00:47:23,500 --> 00:47:27,535
then that can mean that your importance weight for those trajectories is enormous.

917
00:47:27,535 --> 00:47:29,515
In fact, we have,

918
00:47:29,515 --> 00:47:30,790
um, a proof that, you know,

919
00:47:30,790 --> 00:47:32,650
that generally the size of your importance weights

920
00:47:32,650 --> 00:47:34,870
can grow exponentially with the horizon.

921
00:47:34,870 --> 00:47:40,375
Um, uh, and so these importance weights can be incredibly large in some cases.

922
00:47:40,375 --> 00:47:44,080
And so what weighted importance sampling does is it just renormalizes.

923
00:47:44,080 --> 00:47:45,940
So effectively, you're making it

924
00:47:45,940 --> 00:47:48,895
so all your importance weights are somewhere between 0 and 1.

925
00:47:48,895 --> 00:47:53,210
Then you're using that when you're reweighting your distribution.

926
00:47:54,030 --> 00:47:56,620
So when you do this,

927
00:47:56,620 --> 00:47:58,360
this is something that's very common, um,

928
00:47:58,360 --> 00:48:00,880
to do again this pre- predates the reinforcement learning side,

929
00:48:00,880 --> 00:48:02,710
but has also been used in reinforcement learning.

930
00:48:02,710 --> 00:48:04,765
Um, this is, uh,

931
00:48:04,765 --> 00:48:09,640
this is biased, um, still consistent.

932
00:48:09,640 --> 00:48:12,640
[NOISE] So that means asymptotically it's

933
00:48:12,640 --> 00:48:15,700
going to get to the right thing and lower variance.

934
00:48:15,700 --> 00:48:19,510
[NOISE] So we're essentially going to play,

935
00:48:19,510 --> 00:48:23,050
um, the bias variance trade-off that we've often seen.

936
00:48:23,050 --> 00:48:25,390
We can make Q learning versus Monte Carlo estimates.

937
00:48:25,390 --> 00:48:28,030
Monte Carlo were unbiased but very high-variance.

938
00:48:28,030 --> 00:48:30,040
Q Learning bootstrap so it's biased but

939
00:48:30,040 --> 00:48:32,080
often much better because you're so much lower variance.

940
00:48:32,080 --> 00:48:35,590
Weighted importance sampling is much lower variance empirically much,

941
00:48:35,590 --> 00:48:36,865
much better, most of the time,

942
00:48:36,865 --> 00:48:39,970
particularly for small amounts of data. Yeah.

943
00:48:39,970 --> 00:48:42,820
Yeah. I was wondering if you could comment on, um,

944
00:48:42,820 --> 00:48:46,555
you know, before you're saying that we were intentionally designing something to be unbiased.

945
00:48:46,555 --> 00:48:50,320
So we're going to ignore certain techniques and now we're reintroducing bias at the end?

946
00:48:50,320 --> 00:48:51,060
Right.

947
00:48:51,060 --> 00:48:54,240
I guess, what is the intuition behind when it's okay,

948
00:48:54,240 --> 00:48:57,365
I guess, to introduce bias and like when and why?

949
00:48:57,365 --> 00:49:00,580
It's great one. Okay. So you just made a big deal before about saying let's go for

950
00:49:00,580 --> 00:49:03,580
unbiased estimators and now you're telling us that we're going to go back to bias in,

951
00:49:03,580 --> 00:49:06,910
[LAUGHTER] that- that case, you know, how do we make decisions about when this is okay?

952
00:49:06,910 --> 00:49:08,890
Um, I think it totally depends on the domain.

953
00:49:08,890 --> 00:49:12,895
Uh, and it als- I think one challenge and issue that comes up is,

954
00:49:12,895 --> 00:49:14,410
if things are unbiased,

955
00:49:14,410 --> 00:49:16,960
it's often easier to have confidence intervals around them.

956
00:49:16,960 --> 00:49:18,730
We know better how to do that, um,

957
00:49:18,730 --> 00:49:21,295
when it's biased, it's often hard to quantify.

958
00:49:21,295 --> 00:49:24,820
Um, I think, uh, I'll talk briefly a little bit later about times

959
00:49:24,820 --> 00:49:27,925
where we really just want to directly optimize for bias plus variance.

960
00:49:27,925 --> 00:49:29,410
Like we want to look at accuracy,

961
00:49:29,410 --> 00:49:32,140
mean squared error, just the sum of bias and variance.

962
00:49:32,140 --> 00:49:34,000
And so then I think you can- it provides you a way to

963
00:49:34,000 --> 00:49:36,310
directly trade-off between those two because you're like,

964
00:49:36,310 --> 00:49:38,380
I know I want to minimize the mean squared error.

965
00:49:38,380 --> 00:49:40,420
And that's the sum of these two, that gives me a way like,

966
00:49:40,420 --> 00:49:42,205
a principled way, how to trade those off.

967
00:49:42,205 --> 00:49:46,220
I think another thing I often like just for sort of a sanity  check is that,

968
00:49:46,220 --> 00:49:48,480
if it's biased but still consistent,

969
00:49:48,480 --> 00:49:50,430
um, that's sort of a nice sanity check.

970
00:49:50,430 --> 00:49:52,590
It's like okay maybe there's a small amount of bias early on,

971
00:49:52,590 --> 00:49:53,820
but eventually if I get a lot of data,

972
00:49:53,820 --> 00:49:55,395
it's really given me the right answer.

973
00:49:55,395 --> 00:49:58,500
And some of the function approximation stuff we solve for Q learning.

974
00:49:58,500 --> 00:50:00,760
That's not true, like we just, all bets are off,

975
00:50:00,760 --> 00:50:02,875
who knows what's happening asymptotically.

976
00:50:02,875 --> 00:50:05,530
But again, it depends on the- depends on the day.

977
00:50:05,530 --> 00:50:08,425
It's a great question. It's a big challenge in that area.

978
00:50:08,425 --> 00:50:11,455
Okay. So as I was just saying in this case,

979
00:50:11,455 --> 00:50:15,025
um, you know, weighted importance sampling, it's strongly consistent.

980
00:50:15,025 --> 00:50:16,930
You're going to converge, um,

981
00:50:16,930 --> 00:50:19,555
if you have a finite horizon or, um,

982
00:50:19,555 --> 00:50:22,225
one behavior policy or bounded rewards, and, uh,

983
00:50:22,225 --> 00:50:25,150
Phil and I, so that if you'd look at Thomas and Brunskill.

984
00:50:25,150 --> 00:50:28,225
[NOISE] We think about this for

985
00:50:28,225 --> 00:50:34,490
the RL case in our ICML 2016 paper. It's one reference for this.

986
00:50:34,920 --> 00:50:38,920
Okay. So we're going to have these estimates.

987
00:50:38,920 --> 00:50:40,780
Um, they may be,

988
00:50:40,780 --> 00:50:42,340
if we're using weighted importance sampling,

989
00:50:42,340 --> 00:50:43,930
there might be a little bit bias, but lower variance.

990
00:50:43,930 --> 00:50:47,875
Otherwise, there might be high variance but low bias, or zero bias.

991
00:50:47,875 --> 00:50:49,600
What's something else we could do?

992
00:50:49,600 --> 00:50:52,420
So, uh, let's briefly talk about control variance

993
00:50:52,420 --> 00:50:55,240
and be thinking in your head again at back to policy gradient,

994
00:50:55,240 --> 00:50:56,755
and thinking about baselines.

995
00:50:56,755 --> 00:50:59,200
So just from a statistics perspective,

996
00:50:59,200 --> 00:51:00,940
um, you know, if we have an X,

997
00:51:00,940 --> 00:51:02,920
we have the mean of that variable, um,

998
00:51:02,920 --> 00:51:04,870
if it's unbiased, it means that our estimate of

999
00:51:04,870 --> 00:51:08,350
the mean is matching the true expectation,

1000
00:51:08,350 --> 00:51:10,825
and then we have our variance.

1001
00:51:10,825 --> 00:51:15,475
So let's imagine that we just kinda shift these estimates.

1002
00:51:15,475 --> 00:51:20,950
Okay. So we're just going to subtract a random variable Y,

1003
00:51:20,950 --> 00:51:24,610
and then in your head you can think about this as like a Q function, something like that.

1004
00:51:24,610 --> 00:51:28,280
So, you know, Y might be a Q function,

1005
00:51:29,250 --> 00:51:38,485
and expected value of Y might be a V. Do you think about Q being,

1006
00:51:38,485 --> 00:51:41,110
um, as an a, an expected value of

1007
00:51:41,110 --> 00:51:44,905
Y being an average over all the actions you could take in that state.

1008
00:51:44,905 --> 00:51:48,250
So then, i- if you redefine your mu so,

1009
00:51:48,250 --> 00:51:50,530
um, X is gonna be like, let's,

1010
00:51:50,530 --> 00:51:54,250
we're gonna be going back towards trying to get our estimate of V_pi_e.

1011
00:51:54,250 --> 00:51:59,020
[NOISE] Then if you subtract off something else and add on this expectation,

1012
00:51:59,020 --> 00:52:00,730
you still get an unbiased estimator.

1013
00:52:00,730 --> 00:52:03,370
[NOISE] So we can just write that out here.

1014
00:52:03,370 --> 00:52:07,285
So X - Y + expected value of Y,

1015
00:52:07,285 --> 00:52:10,480
just equal to expected value of X,

1016
00:52:10,480 --> 00:52:13,075
minus expected value of Y,

1017
00:52:13,075 --> 00:52:15,890
plus expected value of Y.

1018
00:52:19,080 --> 00:52:22,180
So you can do this. I mean, you can do this in statistics.

1019
00:52:22,180 --> 00:52:26,020
You can subtract a variable and add on its expectation and on average,

1020
00:52:26,020 --> 00:52:28,855
that does not change the mean of your original X.

1021
00:52:28,855 --> 00:52:31,060
But you might ask, "Why would I do that?"

1022
00:52:31,060 --> 00:52:33,580
[LAUGHTER] So you can do this for anything like

1023
00:52:33,580 --> 00:52:36,250
any- these are where X and Y are random variables.

1024
00:52:36,250 --> 00:52:39,085
So these are general, just any random variables,

1025
00:52:39,085 --> 00:52:42,340
where Y is conc- called a control variant.

1026
00:52:42,340 --> 00:52:45,100
And this may be useful if it

1027
00:52:45,100 --> 00:52:47,560
allows you to reduce variance. And that means that. You know.

1028
00:52:47,560 --> 00:52:49,405
Y has to have something to do with X.

1029
00:52:49,405 --> 00:52:51,310
Okay. If you just subtract off something random,

1030
00:52:51,310 --> 00:52:52,645
this is probably not going to be helpful.

1031
00:52:52,645 --> 00:52:56,410
But if you subtract off something that allows you to have some insight on X,

1032
00:52:56,410 --> 00:52:59,635
in our case we're gonna be interested in things that allows us to help predict the value,

1033
00:52:59,635 --> 00:53:01,885
um, then we might get a lower variance.

1034
00:53:01,885 --> 00:53:06,520
So we can look this by looking at what is the variance of this weird quantity mu hat,

1035
00:53:06,520 --> 00:53:10,630
where we had this X - Y + the expected value of Y.

1036
00:53:10,630 --> 00:53:13,135
[NOISE] We can write it down as follows,

1037
00:53:13,135 --> 00:53:15,475
X - Y + the expected value of Y.

1038
00:53:15,475 --> 00:53:18,310
Now, the variance of the expected value of Y,

1039
00:53:18,310 --> 00:53:22,240
there's no more variability in that so we can just say that's the variance of X - Y.

1040
00:53:22,240 --> 00:53:27,865
[NOISE] So what that means is that we're gonna get something which is variance of X,

1041
00:53:27,865 --> 00:53:31,375
variance of Y, and the covariance of the two.

1042
00:53:31,375 --> 00:53:35,860
So if it is the case that the covariance of X and Y, meaning that the, you know,

1043
00:53:35,860 --> 00:53:38,125
there is relationship between these two variables,

1044
00:53:38,125 --> 00:53:40,525
one of them is giving us information about the other.

1045
00:53:40,525 --> 00:53:42,775
If that is bigger than the variance of Y,

1046
00:53:42,775 --> 00:53:44,185
then you're going to have a win.

1047
00:53:44,185 --> 00:53:46,825
Then you're gonna have that the resulting estimator.

1048
00:53:46,825 --> 00:53:48,295
So if this is true,

1049
00:53:48,295 --> 00:53:52,825
if true- if true,

1050
00:53:52,825 --> 00:54:00,230
variance of mu hat is gonna be [NOISE] less than variance of X.

1051
00:54:01,020 --> 00:54:04,540
So this is nice because it means that we

1052
00:54:04,540 --> 00:54:07,405
didn't change the mean and we reduced the variance,

1053
00:54:07,405 --> 00:54:10,480
which in some ways, kind of seems like a free lunch but it's not really

1054
00:54:10,480 --> 00:54:11,755
a free lunch because we're using

1055
00:54:11,755 --> 00:54:14,350
information that is actually telling us something about X.

1056
00:54:14,350 --> 00:54:18,100
And this is very similar to us using a baseline term and policy gradient.

1057
00:54:18,100 --> 00:54:20,875
Where instead of just relying on the Monte Carlo returns,

1058
00:54:20,875 --> 00:54:24,190
we could also subtract off a baseline like the value function.

1059
00:54:24,190 --> 00:54:30,625
[NOISE] So you can do this in importance sampling as well.

1060
00:54:30,625 --> 00:54:36,100
Um, so where X is the importance sampling estimator and Y is some control variate.

1061
00:54:36,100 --> 00:54:39,490
Typically, you know, this can either be a Q function which you

1062
00:54:39,490 --> 00:54:42,985
build from an approximate model of the Markov decision process.

1063
00:54:42,985 --> 00:54:45,475
It can be a Q function from Q learning.

1064
00:54:45,475 --> 00:54:52,120
Um, this gives you some way to get- we can think of this as Q, you know,

1065
00:54:52,120 --> 00:55:03,190
some- some estimate [NOISE] of state-action [NOISE] value, okay.

1066
00:55:04,800 --> 00:55:09,895
And doing this is called a doubly robust estimator.

1067
00:55:09,895 --> 00:55:12,610
Doubly robust estimator is again, um,

1068
00:55:12,610 --> 00:55:14,380
where in statistics for a long time, um,

1069
00:55:14,380 --> 00:55:17,920
in around 2011, were brought over to the,

1070
00:55:17,920 --> 00:55:21,710
I remember were brought to the multi-armed bandit community,

1071
00:55:21,870 --> 00:55:29,060
like the machine-learning multi-armed bandit community with Dudik et al.

1072
00:55:29,220 --> 00:55:31,750
And why do they call it doubly robust?

1073
00:55:31,750 --> 00:55:32,905
Well, the idea is that, um,

1074
00:55:32,905 --> 00:55:37,240
if you use information both from like your normal importance sampling, um,

1075
00:55:37,240 --> 00:55:41,080
plus some control variate like a Q value estimate,

1076
00:55:41,080 --> 00:55:44,725
um, you can be robust too if either of those are wrong.

1077
00:55:44,725 --> 00:55:47,410
So either if you have a poor model or you have

1078
00:55:47,410 --> 00:55:49,990
a bad estimate if your control variate is bad.

1079
00:55:49,990 --> 00:55:55,450
[NOISE] So you know, why would this be important?

1080
00:55:55,450 --> 00:55:57,160
Well, if we go back to sort of questions,

1081
00:55:57,160 --> 00:55:58,705
some other people's questions, um,

1082
00:55:58,705 --> 00:56:01,450
an alternative is just to like do Q learning on your data, right?

1083
00:56:01,450 --> 00:56:03,100
But Q learning might be biased or it might be

1084
00:56:03,100 --> 00:56:05,140
a horrible estimate and who knows if it's good?

1085
00:56:05,140 --> 00:56:06,655
Um, but it might be good.

1086
00:56:06,655 --> 00:56:08,950
So if it's great- good you'd like to be able to say,

1087
00:56:08,950 --> 00:56:10,360
"We've got a good estimate."

1088
00:56:10,360 --> 00:56:11,710
Or if it's bad,

1089
00:56:11,710 --> 00:56:15,295
you'd like to say that it's with your importance sampling can compensate for that,

1090
00:56:15,295 --> 00:56:18,625
and this says that either if you have a bad model,

1091
00:56:18,625 --> 00:56:21,460
um, or you have error in your estimates of pi B.

1092
00:56:21,460 --> 00:56:24,670
So this is like those cases where we've got data from positions,

1093
00:56:24,670 --> 00:56:26,815
so we don't really know what the behavior policy is.

1094
00:56:26,815 --> 00:56:30,430
So if you have inaccuracies there, then you, um,

1095
00:56:30,430 --> 00:56:33,280
also would like to if it turns out your control variate is accurate,

1096
00:56:33,280 --> 00:56:34,690
then you could also do well.

1097
00:56:34,690 --> 00:56:37,885
Now, if- in some cases both of these are bad and in that case,

1098
00:56:37,885 --> 00:56:39,565
kind of all bets are still off.

1099
00:56:39,565 --> 00:56:42,520
But it gives you more robustness about different parts of what you're

1100
00:56:42,520 --> 00:56:46,060
you're trying to estimate how good your evaluation policy is.

1101
00:56:46,060 --> 00:56:51,400
Okay, and Bill and I discussed sort of different ways

1102
00:56:51,400 --> 00:56:52,480
to do this as well as doing it in

1103
00:56:52,480 --> 00:56:56,740
a weighted way, so incorporating weighted importance sampling.

1104
00:56:56,740 --> 00:56:59,140
So what does this allow you to do?

1105
00:56:59,140 --> 00:57:01,210
Okay, I'll- I'll briefly show the equation.

1106
00:57:01,210 --> 00:57:05,315
Then essentially, the idea here is these are like the importance sampling weights.

1107
00:57:05,315 --> 00:57:11,190
This is the raw returns and then we can add and subtract.

1108
00:57:11,190 --> 00:57:17,010
This was Y and this is the expected value of Y and these can be computed

1109
00:57:17,010 --> 00:57:19,540
by computing like Q learning or

1110
00:57:19,540 --> 00:57:23,080
doing like an empirical model and doing dynamic programming on it.

1111
00:57:23,080 --> 00:57:27,370
You could get these sort of estimates of Q of pi E in lots of different ways,

1112
00:57:27,370 --> 00:57:29,440
um, and they might be good or they might be bad.

1113
00:57:29,440 --> 00:57:30,880
But you can plug them in and often,

1114
00:57:30,880 --> 00:57:33,565
they're gonna end up helping you in terms of variance.

1115
00:57:33,565 --> 00:57:35,680
So let's see empirically what this does.

1116
00:57:35,680 --> 00:57:39,610
[NOISE] So this is a really small Gridworld.

1117
00:57:39,610 --> 00:57:41,710
Think something on the order of like,

1118
00:57:41,710 --> 00:57:45,310
you know, maybe five-by-five, four-by-four.

1119
00:57:45,310 --> 00:57:48,895
This is a really small world and we're using it to try to illustrate and understand,

1120
00:57:48,895 --> 00:57:51,580
um, the benefits of these different types of techniques.

1121
00:57:51,580 --> 00:57:53,140
So what's on the x-axis?

1122
00:57:53,140 --> 00:57:54,235
This is the amount of data.

1123
00:57:54,235 --> 00:57:56,900
So this is the size of the dataset.

1124
00:57:57,060 --> 00:58:00,625
What's on the y-axis? This is mean squared error.

1125
00:58:00,625 --> 00:58:04,030
This is the difference between our estimate of

1126
00:58:04,030 --> 00:58:06,940
the evaluation policy and the true evaluation policy,

1127
00:58:06,940 --> 00:58:08,335
and you want this to be small.

1128
00:58:08,335 --> 00:58:12,640
So smaller, better and this is a log scale.

1129
00:58:12,640 --> 00:58:16,330
[NOISE] So what do we see here?

1130
00:58:16,330 --> 00:58:19,615
So one thing you could do is you could build an approximate model with your data.

1131
00:58:19,615 --> 00:58:21,580
That model might be wrong like maybe,

1132
00:58:21,580 --> 00:58:23,440
you're making a Markov assumption and it's wrong.

1133
00:58:23,440 --> 00:58:24,790
Or maybe, um, you know,

1134
00:58:24,790 --> 00:58:26,530
there's other parts where you just can't estimate well.

1135
00:58:26,530 --> 00:58:28,090
So this is the model-based.

1136
00:58:28,090 --> 00:58:30,175
So this is just we use a model,

1137
00:58:30,175 --> 00:58:35,260
and we compute V pi e for that model.

1138
00:58:35,260 --> 00:58:36,925
So we take our data,

1139
00:58:36,925 --> 00:58:39,325
we build an MDP model, then we, um,

1140
00:58:39,325 --> 00:58:41,950
use that to- then that's like a simulator and then we can

1141
00:58:41,950 --> 00:58:44,935
just compute V_pi e. So you can see here it's flat.

1142
00:58:44,935 --> 00:58:47,500
Um, in this case, um, I'd have to remember here.

1143
00:58:47,500 --> 00:58:49,450
I think we are using a different dataset.

1144
00:58:49,450 --> 00:58:51,520
Either I- I would have to double check whether or not

1145
00:58:51,520 --> 00:58:53,620
just after we have that number of episodes,

1146
00:58:53,620 --> 00:58:55,840
the model just doesn't change with further data.

1147
00:58:55,840 --> 00:58:57,355
Like the model just isn't great.

1148
00:58:57,355 --> 00:59:00,175
Maybe we're not using all the features that we should be, um,

1149
00:59:00,175 --> 00:59:01,420
or the world isn't really Markov,

1150
00:59:01,420 --> 00:59:03,505
and so you kind of have this fixed bias.

1151
00:59:03,505 --> 00:59:05,545
Your model can be asymptotically wrong.

1152
00:59:05,545 --> 00:59:07,779
The estimates you get from it can be asymptotically

1153
00:59:07,779 --> 00:59:10,060
wrong if your model is not a good fit to the environment.

1154
00:59:10,060 --> 00:59:16,070
[NOISE] The- the second thing we can do is we can do importance sampling.

1155
00:59:16,380 --> 00:59:19,210
So importance sampling is unbiased.

1156
00:59:19,210 --> 00:59:21,700
It's going down as we get more data as we would hope.

1157
00:59:21,700 --> 00:59:23,920
Eventually, it should collapse to 0.

1158
00:59:23,920 --> 00:59:26,710
Um, but we'd like to do better than that.

1159
00:59:26,710 --> 00:59:29,935
So now this is per decision importance sampling.

1160
00:59:29,935 --> 00:59:32,230
You can see you get a benefit from leveraging the fact

1161
00:59:32,230 --> 00:59:35,155
that rewards can't be influenced by future decisions.

1162
00:59:35,155 --> 00:59:36,685
That reduces the variance,

1163
00:59:36,685 --> 00:59:40,520
kind of gives you this nice kind of automatic shift down.

1164
00:59:40,920 --> 00:59:43,420
Um, if you do doubly robust,

1165
00:59:43,420 --> 00:59:44,680
you get a significant bump.

1166
00:59:44,680 --> 00:59:46,240
So what's doubly robust doing again?

1167
00:59:46,240 --> 00:59:50,240
It's combining our approximate model plus IS.

1168
00:59:50,430 --> 00:59:52,930
So you can see again, here we're getting,

1169
00:59:52,930 --> 00:59:54,760
ah, a significant bump.

1170
00:59:54,760 --> 00:59:57,880
Now, I talk about this mostly in terms of mean squared error

1171
00:59:57,880 --> 01:00:01,045
but I think it's really important to think about what mean squared error means.

1172
01:00:01,045 --> 01:00:03,085
Um, so mean squared error here is, you know,

1173
01:00:03,085 --> 01:00:08,740
how accurately are we estimating this V_pi E minus the true one?

1174
01:00:08,740 --> 01:00:10,870
But we can alternatively think about this in

1175
01:00:10,870 --> 01:00:13,315
terms of how much data we need to get a good estimate.

1176
01:00:13,315 --> 01:00:19,060
So look at this. This is- you want a mean squared error of 1.

1177
01:00:19,060 --> 01:00:20,995
Maybe that's sufficient. Maybe that's not.

1178
01:00:20,995 --> 01:00:27,100
That means that under a per decision important sampling estimator,

1179
01:00:27,100 --> 01:00:30,460
you would need 2,000 episodes and with doubly robust,

1180
01:00:30,460 --> 01:00:32,275
you'd need less than 200.

1181
01:00:32,275 --> 01:00:36,805
So that's awesome because it means that we need like an order of magnitude less data,

1182
01:00:36,805 --> 01:00:38,680
in order to get good estimates,

1183
01:00:38,680 --> 01:00:41,575
and a lot of real world applications we just don't have enough data.

1184
01:00:41,575 --> 01:00:43,390
You know, there might not be a lot of patients with

1185
01:00:43,390 --> 01:00:45,655
a particular condition and

1186
01:00:45,655 --> 01:00:48,040
you'd really like to still be able to make good decisions for them,

1187
01:00:48,040 --> 01:00:52,045
and so you need estimators that need much less data to give you good answers.

1188
01:00:52,045 --> 01:00:53,785
And so that's why this is important.

1189
01:00:53,785 --> 01:00:56,440
Okay? Right.

1190
01:00:56,440 --> 01:00:58,450
And then in these cases,

1191
01:00:58,450 --> 01:01:00,040
we can see also that if you do weighted

1192
01:01:00,040 --> 01:01:03,010
importance sampling [NOISE] and weighted throughput decision,

1193
01:01:03,010 --> 01:01:04,510
that also ends up helping a lot.

1194
01:01:04,510 --> 01:01:10,315
[NOISE] Here, is if you do weighted doubly robust,

1195
01:01:10,315 --> 01:01:13,450
again, sort of answering that question of when should you do,

1196
01:01:13,450 --> 01:01:15,670
you know, how do you trade off between this bias and variance?

1197
01:01:15,670 --> 01:01:18,745
Um, here, we can see that if we do some form of weighted,

1198
01:01:18,745 --> 01:01:22,030
uh, doubly robust which is one of the things we introduced in our ICML paper.

1199
01:01:22,030 --> 01:01:23,800
You again get a really big gain.

1200
01:01:23,800 --> 01:01:26,725
So we had this to there,

1201
01:01:26,725 --> 01:01:28,630
and this to there, right?

1202
01:01:28,630 --> 01:01:32,620
So now you are needing like five episodes to get to the same.

1203
01:01:32,620 --> 01:01:35,890
So again, some of these improvements can be- this is of course Gridworld, right?

1204
01:01:35,890 --> 01:01:37,870
Like this- you- you need- one needs to look at

1205
01:01:37,870 --> 01:01:40,105
this also for the particular domain one's interested in.

1206
01:01:40,105 --> 01:01:42,970
But it indicates there are just some of these cases by being, um,

1207
01:01:42,970 --> 01:01:45,100
a little bit better in terms of these estimators,

1208
01:01:45,100 --> 01:01:48,490
you can get substantial gains in terms of how accurate your estimators are.

1209
01:01:48,490 --> 01:01:51,625
[NOISE] All right,

1210
01:01:51,625 --> 01:01:55,375
again to continue on this slide of like, how do you balance between variance, um, and bias.

1211
01:01:55,375 --> 01:01:57,490
One thing that Phil and I thought about is, okay,

1212
01:01:57,490 --> 01:02:00,070
well, you know, you might want to have, um,

1213
01:02:00,070 --> 01:02:01,165
low bias and variance, ah,

1214
01:02:01,165 --> 01:02:02,815
and- and how do you do this trade-off,

1215
01:02:02,815 --> 01:02:05,230
let's just think about optimizing for it directly.

1216
01:02:05,230 --> 01:02:13,735
So our magic estimator just tries to directly- directly minimize mean squared error.

1217
01:02:13,735 --> 01:02:18,235
Okay. So- so again let's say mean squared error is gonna be,

1218
01:02:18,235 --> 01:02:21,290
you know, it's a function of bias and variance.

1219
01:02:22,260 --> 01:02:25,090
So if you knew what bias and variance was,

1220
01:02:25,090 --> 01:02:27,790
you could hopefully just optimize this directly.

1221
01:02:27,790 --> 01:02:29,830
Do we know what bias is?

1222
01:02:29,830 --> 01:02:33,520
So bias again just to remember is bias is the difference

1223
01:02:33,520 --> 01:02:39,410
between this minus this. That's bias.

1224
01:02:40,830 --> 01:02:46,165
Do we know that? No, unfortunately, if we knew that,

1225
01:02:46,165 --> 01:02:48,940
if we knew the bias, then we wouldn't have to do anything else because we would

1226
01:02:48,940 --> 01:02:51,865
know exactly what the real value was.

1227
01:02:51,865 --> 01:02:54,130
So a big challenge when we're trying to do this work is

1228
01:02:54,130 --> 01:02:56,530
how do you get a good estimate of bias?

1229
01:02:56,530 --> 01:02:58,990
Or like an under or overestimated bias?

1230
01:02:58,990 --> 01:03:02,050
And just briefly, the idea that we had there is to say,

1231
01:03:02,050 --> 01:03:03,835
if you can get confidence intervals,

1232
01:03:03,835 --> 01:03:06,010
which you can using importance sampling.

1233
01:03:06,010 --> 01:03:07,600
So let's say, I have these, you know,

1234
01:03:07,600 --> 01:03:10,390
this is my estimate from importance sampling.

1235
01:03:10,390 --> 01:03:12,355
And I have some uncertainty over it.

1236
01:03:12,355 --> 01:03:13,720
So this gives me some estimate,

1237
01:03:13,720 --> 01:03:16,630
[NOISE] with some upper and lower bounds.

1238
01:03:16,630 --> 01:03:19,090
So you know, I'm like, you can do, ah,

1239
01:03:19,090 --> 01:03:21,895
let's say the value is 5 plus or minus 2.

1240
01:03:21,895 --> 01:03:25,585
Okay. And then let's say I have a model, um,

1241
01:03:25,585 --> 01:03:28,180
I have a model that I built from this data and then I

1242
01:03:28,180 --> 01:03:31,225
used it to evaluate and I got another estimate up here.

1243
01:03:31,225 --> 01:03:33,190
So I have a V_pi,

1244
01:03:33,190 --> 01:03:35,470
and this is using a model.

1245
01:03:35,470 --> 01:03:38,290
Okay, so let's say this is 8.

1246
01:03:38,290 --> 01:03:43,190
So what is the minimum bias my model has to have?

1247
01:03:44,610 --> 01:03:48,490
How could you use these confidence intervals to get like a lower

1248
01:03:48,490 --> 01:03:52,075
bound on how- how bad my- assume these confidence intervals are correct.

1249
01:03:52,075 --> 01:03:54,580
Now that these are real confidence intervals so we know

1250
01:03:54,580 --> 01:03:57,145
the real value has to be between 3 and 7.

1251
01:03:57,145 --> 01:04:01,370
What's the- what's the minimum bias that my model would have?

1252
01:04:01,980 --> 01:04:05,860
One right, because it's this difference, okay.

1253
01:04:05,860 --> 01:04:08,350
So this gives you a lower bound on the bias.

1254
01:04:08,350 --> 01:04:10,180
So how far off you are from

1255
01:04:10,180 --> 01:04:12,685
these confidence intervals that gives you a lower bound on the bias?

1256
01:04:12,685 --> 01:04:15,535
It's optimistic, your bias- your model might be way more biased.

1257
01:04:15,535 --> 01:04:18,940
Um, uh, but it gives you a way to quantify what that bias is,

1258
01:04:18,940 --> 01:04:20,800
and that's what we use in this approach.

1259
01:04:20,800 --> 01:04:24,490
So we combine our importance sampling estimators and think about how variable they are.

1260
01:04:24,490 --> 01:04:26,695
We have to get an estimate of their variance, um,

1261
01:04:26,695 --> 01:04:28,180
as well as the bias on the model,

1262
01:04:28,180 --> 01:04:31,645
and that allows us to figure out how to trade off between these.

1263
01:04:31,645 --> 01:04:34,225
And again, you get, um,

1264
01:04:34,225 --> 01:04:36,370
you get a really substantial gain often.

1265
01:04:36,370 --> 01:04:40,000
This is still Gridworld but, um,

1266
01:04:40,000 --> 01:04:47,275
you're gonna get again [NOISE] roughly an order by magnitude difference in some domains.

1267
01:04:47,275 --> 01:04:50,690
You're gonna need an order of magnitude less data.

1268
01:04:52,680 --> 01:04:55,660
And in this case, I've just zoomed in so you don't even

1269
01:04:55,660 --> 01:04:57,910
see some of these other methods because they're so much higher up.

1270
01:04:57,910 --> 01:05:01,540
[NOISE] Okay, so, you know,

1271
01:05:01,540 --> 01:05:03,490
that's one thing that we could do to try to get sort of

1272
01:05:03,490 --> 01:05:06,010
good off-policy policy evaluation estimates.

1273
01:05:06,010 --> 01:05:08,050
Um, I haven't talked to you too much so far about like

1274
01:05:08,050 --> 01:05:10,075
how are we gonna get these confidence bounds over those.

1275
01:05:10,075 --> 01:05:12,220
But I've mentioned sort of a number of different ways that we

1276
01:05:12,220 --> 01:05:14,470
could try to get just an estimate of, you know,

1277
01:05:14,470 --> 01:05:16,795
V_pi E. So we want to get some estimate of

1278
01:05:16,795 --> 01:05:20,890
this new alternative policy that we might wanna unleash on the wild.

1279
01:05:20,890 --> 01:05:24,790
Um, I'll- I'm gonna skip this part, I'll just say briefly,

1280
01:05:24,790 --> 01:05:27,055
you know, there are some subtleties here with whether or not,

1281
01:05:27,055 --> 01:05:28,480
um, ah, you know,

1282
01:05:28,480 --> 01:05:31,210
what's the support of your behavior policy, um,

1283
01:05:31,210 --> 01:05:33,460
and how we do some different weighting and can we

1284
01:05:33,460 --> 01:05:35,920
improve over this sort of weighted importance sampling?

1285
01:05:35,920 --> 01:05:38,695
Um, [NOISE]. The answer is, is yes.

1286
01:05:38,695 --> 01:05:40,585
You can do some slightly different weightings,

1287
01:05:40,585 --> 01:05:42,265
um, and I'll- I'll defer that.

1288
01:05:42,265 --> 01:05:44,490
And then also, another really important question,

1289
01:05:44,490 --> 01:05:46,095
really important in practice is that,

1290
01:05:46,095 --> 01:05:48,785
um, your distributions are often non-stationary.

1291
01:05:48,785 --> 01:05:50,920
You know, imagine that like you're looking at

1292
01:05:50,920 --> 01:05:53,545
patient data and during that time period like

1293
01:05:53,545 --> 01:05:55,735
some new food pyramid came out from

1294
01:05:55,735 --> 01:05:58,195
the Food and Drug Administration and so everyone changed how they're eating.

1295
01:05:58,195 --> 01:05:59,470
Um, so now that, you know,

1296
01:05:59,470 --> 01:06:02,470
the dynamics of your patients are gonna be really different than before.

1297
01:06:02,470 --> 01:06:04,420
So you'd like to be able to identify whether or not you

1298
01:06:04,420 --> 01:06:06,640
have sort of non-stationarity in your data-set.

1299
01:06:06,640 --> 01:06:09,340
Like if the dynamics model of the world is changing.

1300
01:06:09,340 --> 01:06:11,680
So we have some other ideas about how to handle that.

1301
01:06:11,680 --> 01:06:14,260
[NOISE] Okay, but now let's go to and say,

1302
01:06:14,260 --> 01:06:16,840
let's assume we've done this off-policy policy evaluation.

1303
01:06:16,840 --> 01:06:18,620
We've gotten out some estimate, um,

1304
01:06:18,620 --> 01:06:21,850
of these, how good our alternative policy would be,

1305
01:06:21,850 --> 01:06:24,850
and we want to go beyond that and we want to get some confidence over it.

1306
01:06:24,850 --> 01:06:28,300
So again remember we're trying to move to a world where we can say, you know,

1307
01:06:28,300 --> 01:06:32,620
the probability that the policy output by

1308
01:06:32,620 --> 01:06:39,820
our algorithm being better than our previous policy, [NOISE] is high.

1309
01:06:39,820 --> 01:06:42,880
So with high probability we're gonna give you a policy that's better,

1310
01:06:42,880 --> 01:06:45,970
which means not only do we need to have an estimate on how good that policy is but

1311
01:06:45,970 --> 01:06:49,450
how much better it is or not than your behavior policy.

1312
01:06:49,450 --> 01:06:51,355
Okay. How would we do that?

1313
01:06:51,355 --> 01:06:54,895
So let's first consider using importance sampling plots Hoeffding's inequality.

1314
01:06:54,895 --> 01:06:58,609
Again let's think back to what we are doing with exploration,

1315
01:06:59,460 --> 01:07:03,145
to do high confidence off-policy policy evaluation.

1316
01:07:03,145 --> 01:07:04,315
So just as a refresher,

1317
01:07:04,315 --> 01:07:06,340
mountain car is the simple control task,

1318
01:07:06,340 --> 01:07:08,800
[NOISE] where, you know,

1319
01:07:08,800 --> 01:07:13,130
you have your agent trying to reach here, we get high reward.

1320
01:07:13,890 --> 01:07:16,810
And we're gonna have data gathered from

1321
01:07:16,810 --> 01:07:19,990
some behavior policy and we want to try to learn a better policy from it,

1322
01:07:19,990 --> 01:07:22,735
and be able to get competence bounds over its performance.

1323
01:07:22,735 --> 01:07:26,050
Okay, so remember that in Hoeffding's inequality,

1324
01:07:26,050 --> 01:07:30,145
it's a way to look at how different your- your mean is from,

1325
01:07:30,145 --> 01:07:31,840
um, your average so far.

1326
01:07:31,840 --> 01:07:35,590
So how different can we- how can your- how different can your empirical mean

1327
01:07:35,590 --> 01:07:39,580
be [NOISE] minus your true mean?

1328
01:07:39,580 --> 01:07:45,625
[NOISE] And it gives you a bound on that in terms of the amount of data you have.

1329
01:07:45,625 --> 01:07:49,030
Okay, so it's a function of the amount of data you have.

1330
01:07:49,030 --> 01:07:53,785
And it depends on the range of your variables, okay.

1331
01:07:53,785 --> 01:07:55,540
So thought about about this for arms,

1332
01:07:55,540 --> 01:07:57,700
which might have rewards of between 0 and 1,

1333
01:07:57,700 --> 01:07:59,050
and then b would be 1.

1334
01:07:59,050 --> 01:08:01,615
Okay, so we can use Hoeffding's inequality also,

1335
01:08:01,615 --> 01:08:03,025
we talked about it briefly for,

1336
01:08:03,025 --> 01:08:06,145
um, uh, [NOISE] you can use a model-based reinforcement learning.

1337
01:08:06,145 --> 01:08:09,265
Let's think about using that in the context of this off-policy evaluation.

1338
01:08:09,265 --> 01:08:13,000
So we can also use it using our old data to try to estimate, um,

1339
01:08:13,000 --> 01:08:17,470
what the- what our upper or lower bounds might be on the value of the evaluation policy.

1340
01:08:17,470 --> 01:08:22,045
[NOISE] So let's imagine that we use 100,000 trajectories.

1341
01:08:22,045 --> 01:08:25,359
And the evaluation policy's true performance is 0.19.

1342
01:08:25,359 --> 01:08:27,879
And if we use Hoeffding's inequality,

1343
01:08:27,880 --> 01:08:33,740
we're very confident that the new policy has at least -5 million.

1344
01:08:34,890 --> 01:08:40,600
Okay. And we know that the real reward is somewhere between 0 and 1.

1345
01:08:40,600 --> 01:08:44,800
But Hoeffding's inequality gives us this bound of -5 million,

1346
01:08:44,800 --> 01:08:46,300
and that's true, right?

1347
01:08:46,300 --> 01:08:47,380
Like [LAUGHTER] you know,

1348
01:08:47,380 --> 01:08:52,250
is like 0.19 is greater than -5 million.

1349
01:08:52,890 --> 01:08:55,435
But it's not particularly informative.

1350
01:08:55,435 --> 01:08:58,090
Um, like we know that the real returns

1351
01:08:58,090 --> 01:09:00,850
for the- for this domain is somewhere between 0 and 1.

1352
01:09:00,850 --> 01:09:03,745
And if we use Hoeffding's inequality there,

1353
01:09:03,745 --> 01:09:06,625
we're getting something that we'd call is vacuous.

1354
01:09:06,625 --> 01:09:10,375
Okay, so you're getting a bound that is true but entirely uninformative.

1355
01:09:10,375 --> 01:09:12,310
Because it is incredibly negative, right?

1356
01:09:12,310 --> 01:09:13,510
Like we know that this is

1357
01:09:13,510 --> 01:09:20,080
the true- true value is- for all policy is between 0 and 1.

1358
01:09:20,080 --> 01:09:22,885
[NOISE] Okay.

1359
01:09:22,885 --> 01:09:25,135
So why did this happen?

1360
01:09:25,135 --> 01:09:27,910
Um, [NOISE] let's look at importance sampling.

1361
01:09:27,910 --> 01:09:31,975
Importance sampling says we're gonna take this product of weights.

1362
01:09:31,975 --> 01:09:35,005
Okay, and as we've talked about before,

1363
01:09:35,005 --> 01:09:37,550
this might be pretty small.

1364
01:09:38,399 --> 01:09:42,129
So let's imagine this is like, you know, 0.1.

1365
01:09:42,130 --> 01:09:49,210
So then you have 10 to the L. Like if you take a series of actions,

1366
01:09:49,210 --> 01:09:52,060
let's say for every single action of that trajectory it was pretty unlikely

1367
01:09:52,060 --> 01:09:55,090
[NOISE] which you often need in domains like mountain car,

1368
01:09:55,090 --> 01:09:56,890
because in mountain car you have to take

1369
01:09:56,890 --> 01:10:00,985
a pretty specific sequence of actions in order to finally see some reward at the end,

1370
01:10:00,985 --> 01:10:03,100
and under a policy [NOISE] that is not optimal,

1371
01:10:03,100 --> 01:10:05,575
it might be pretty unlikely to see those series of actions.

1372
01:10:05,575 --> 01:10:08,305
So let's say, you know, most of your data you never get up the hill,

1373
01:10:08,305 --> 01:10:11,620
in like one or two of your data points you actually get up to the top of the hill,

1374
01:10:11,620 --> 01:10:13,615
and those were very rare trajectories.

1375
01:10:13,615 --> 01:10:14,770
Which means your, um,

1376
01:10:14,770 --> 01:10:17,215
importance sampling weights are gonna be extremely high.

1377
01:10:17,215 --> 01:10:19,165
It's gonna be this, you know,

1378
01:10:19,165 --> 01:10:25,735
1 divided by 0.1 up to the L. [NOISE] That's just enormous you know.

1379
01:10:25,735 --> 01:10:29,575
Um, so these can start to be incredibly large.

1380
01:10:29,575 --> 01:10:35,455
And Hoeffding's Inequality depends on this.

1381
01:10:35,455 --> 01:10:38,365
The range of the potential returns you have.

1382
01:10:38,365 --> 01:10:40,330
What are the range of our potential returns?

1383
01:10:40,330 --> 01:10:43,990
The range of our potential returns are g times or

1384
01:10:43,990 --> 01:10:48,870
product of i equals 1 to t of our importance weights.

1385
01:10:48,870 --> 01:10:53,140
[NOISE]

1386
01:10:53,140 --> 01:10:56,095
So b is equal to max over this.

1387
01:10:56,095 --> 01:10:58,600
It says, "In the maximum case,

1388
01:10:58,600 --> 01:11:00,070
what could it look like, your return is?"

1389
01:11:00,070 --> 01:11:01,630
[NOISE].

1390
01:11:01,630 --> 01:11:04,120
And so it's gonna depend most on what your real range is.

1391
01:11:04,120 --> 01:11:06,820
Our real [NOISE] b is going to be between 0 and 1,

1392
01:11:06,820 --> 01:11:09,235
and this product of importance sampling weights,

1393
01:11:09,235 --> 01:11:10,315
and that's where the problem is.

1394
01:11:10,315 --> 01:11:13,795
The product of importance sampling weights can be enormous.

1395
01:11:13,795 --> 01:11:14,745
Okay.

1396
01:11:14,745 --> 01:11:17,695
Because you have really unlikely sequence of actions,

1397
01:11:17,695 --> 01:11:21,940
and then you get this blow up. All right.

1398
01:11:21,940 --> 01:11:23,395
So if we look at that here,

1399
01:11:23,395 --> 01:11:26,245
we can get this distribution, um, and some of the,

1400
01:11:26,245 --> 01:11:28,120
some of these are incredibly large,

1401
01:11:28,120 --> 01:11:31,225
and that means that our Hoeffding Inequality ended up being,

1402
01:11:31,225 --> 01:11:34,630
because Hoeffding again is you subtract b,

1403
01:11:34,630 --> 01:11:39,100
basically -b times square root of 1 over n. So if b,

1404
01:11:39,100 --> 01:11:40,300
let's say- let's say,

1405
01:11:40,300 --> 01:11:42,430
your trajectory lengths are something like 200,

1406
01:11:42,430 --> 01:11:45,040
which is pretty, somewhat reasonable for Monte Carlo,

1407
01:11:45,040 --> 01:11:46,495
I'm sorry, for mountain car.

1408
01:11:46,495 --> 01:11:53,110
Then you'd have something like 1 over 0.1 to the 200 times 1,

1409
01:11:53,110 --> 01:11:56,320
times the square root of 1 over n, roughly right?

1410
01:11:56,320 --> 01:11:58,930
And so [NOISE] you'd have just this crazy,

1411
01:11:58,930 --> 01:12:01,840
crazy large term and you're subtracting this.

1412
01:12:01,840 --> 01:12:04,960
So that would mean that your bounds are vacuous,

1413
01:12:04,960 --> 01:12:08,330
basically I have no idea how good this evaluation policy would be.

1414
01:12:08,540 --> 01:12:12,670
So does anybody have any questions about that, about why that issue occurs?

1415
01:12:12,670 --> 01:12:16,930
Okay. So the insight that Phil had in some of his previous work is,

1416
01:12:16,930 --> 01:12:20,140
just get rid of those, just cut it down.

1417
01:12:20,140 --> 01:12:23,409
Um, so if you remove this tail,

1418
01:12:23,409 --> 01:12:26,395
what does that do to your expected value?

1419
01:12:26,395 --> 01:12:28,780
It just decreases it.

1420
01:12:28,780 --> 01:12:31,270
So if you ignore those like really,

1421
01:12:31,270 --> 01:12:33,640
really crazy high returns [NOISE] you're

1422
01:12:33,640 --> 01:12:36,460
not gonna get an estimate anymore that's as good,

1423
01:12:36,460 --> 01:12:38,395
but it's just gonna get smaller.

1424
01:12:38,395 --> 01:12:40,450
You're not gonna overestimate it.

1425
01:12:40,450 --> 01:12:42,460
So again, if we're thinking about say policy improvement,

1426
01:12:42,460 --> 01:12:44,200
we're concerned about deploying policies

1427
01:12:44,200 --> 01:12:46,435
that are worse than we think they are in practice.

1428
01:12:46,435 --> 01:12:48,565
What this is gonna do, is say, we're gonna

1429
01:12:48,565 --> 01:12:50,755
underestimate how good our behavior policy is,

1430
01:12:50,755 --> 01:12:52,990
or our evaluation policy is.

1431
01:12:52,990 --> 01:12:56,185
And so if we underestimate it, that's okay because that's safe,

1432
01:12:56,185 --> 01:12:58,390
like if we don't deploy things that actually would have been good,

1433
01:12:58,390 --> 01:13:00,145
maybe there's a lost opportunity cost,

1434
01:13:00,145 --> 01:13:02,350
but it's not gonna be bad for the world,

1435
01:13:02,350 --> 01:13:05,050
um, so that's what the insight was,

1436
01:13:05,050 --> 01:13:07,300
for here is that you can like remove this upper tail.

1437
01:13:07,300 --> 01:13:10,750
And so you don't need to read the proof, ah, um,

1438
01:13:10,750 --> 01:13:14,950
but the idea is that you can basically define a new confidence interval,

1439
01:13:14,950 --> 01:13:18,730
that is conservative [NOISE].

1440
01:13:18,730 --> 01:13:21,395
And you can think about how you choose that conservatism,

1441
01:13:21,395 --> 01:13:23,925
depending also on the amount of data you have.

1442
01:13:23,925 --> 01:13:26,580
So here's the beautiful take-home, um,

1443
01:13:26,580 --> 01:13:28,890
so let's say that you kind of use 20% of your data

1444
01:13:28,890 --> 01:13:31,290
to figure out exactly how to tune this confidence interval,

1445
01:13:31,290 --> 01:13:34,160
so this is sort of sets your confidence interval.

1446
01:13:34,160 --> 01:13:37,195
And then your next part to compute your lower bound,

1447
01:13:37,195 --> 01:13:39,430
so for mountain car with the same amount of data,

1448
01:13:39,430 --> 01:13:41,635
you've got 100k trajectories.

1449
01:13:41,635 --> 01:13:44,320
So this is the new estimator,

1450
01:13:44,320 --> 01:13:46,015
and you get the, um,

1451
01:13:46,015 --> 01:13:48,159
the mu, the estimator of the V pi,

1452
01:13:48,159 --> 01:13:55,435
so this is a lower bound on V pi e. [NOISE] It says it's gonna be at least 0.145,

1453
01:13:55,435 --> 01:13:58,300
and the true value is 0.19.

1454
01:13:58,300 --> 01:14:01,930
And this is compared to all these other forms of concentration inequalities,

1455
01:14:01,930 --> 01:14:04,570
which were all, except for Anderson,

1456
01:14:04,570 --> 01:14:06,100
really, really bad [NOISE].

1457
01:14:06,100 --> 01:14:07,720
So things like Chernoff-Hoeffding and these other

1458
01:14:07,720 --> 01:14:09,340
ones you don't have to be familiar with all of these.

1459
01:14:09,340 --> 01:14:11,710
But basically, it just says if you'd used other approaches,

1460
01:14:11,710 --> 01:14:15,040
to try to get this lower bound [NOISE] they would have been entirely vacuous,

1461
01:14:15,040 --> 01:14:16,690
whereas this one says, "Okay.

1462
01:14:16,690 --> 01:14:17,950
We're not sure exactly how good it is.

1463
01:14:17,950 --> 01:14:20,380
It's gonna be at least 0.145,

1464
01:14:20,380 --> 01:14:22,465
and the real value is 0.19."

1465
01:14:22,465 --> 01:14:24,610
It's not perfect, but you know,

1466
01:14:24,610 --> 01:14:26,920
if- if your behavior policy was 0.05,

1467
01:14:26,920 --> 01:14:32,350
that would be good enough to say you should use a new thing [NOISE].

1468
01:14:32,350 --> 01:14:34,615
So, um, they use this idea for digital marketing,

1469
01:14:34,615 --> 01:14:36,310
this is some work that Phil had done in

1470
01:14:36,310 --> 01:14:38,380
collaboration with some colleagues over at Adobe.

1471
01:14:38,380 --> 01:14:41,230
Um, and the nice thing about this is you can say,

1472
01:14:41,230 --> 01:14:42,790
you know, if I want it, I'll figure it out,

1473
01:14:42,790 --> 01:14:45,280
if I [NOISE] am gonna deploy something, and get, um,

1474
01:14:45,280 --> 01:14:46,750
more effective digital marketing,

1475
01:14:46,750 --> 01:14:48,700
and I have access to our previous data.

1476
01:14:48,700 --> 01:14:51,625
[NOISE] Can I say with what confidence,

1477
01:14:51,625 --> 01:14:53,350
I can deploy something that's gonna, you know,

1478
01:14:53,350 --> 01:14:55,510
generate higher clicks, get more revenue.

1479
01:14:55,510 --> 01:14:58,780
[NOISE] And these confidence bounds turn out to be tight

1480
01:14:58,780 --> 01:15:00,190
enough that you can actually know that

1481
01:15:00,190 --> 01:15:02,260
the new policy is gonna be better, which is pretty cool.

1482
01:15:02,260 --> 01:15:07,585
[NOISE] [inaudible] Yeah. You can also,

1483
01:15:07,585 --> 01:15:09,970
so this is one form of trying to get those confidence bounds,

1484
01:15:09,970 --> 01:15:13,630
turns out you can also use t-tests and empirically that's often very good.

1485
01:15:13,630 --> 01:15:16,675
Um, and some of you guys might have seen some of these in,

1486
01:15:16,675 --> 01:15:18,370
ah, some of your statistics class.

1487
01:15:18,370 --> 01:15:20,740
I'll just really briefly take, because we're almost out of time,

1488
01:15:20,740 --> 01:15:22,135
that you can combine these ideas,

1489
01:15:22,135 --> 01:15:24,460
and then think about trying to get these lower bounds,

1490
01:15:24,460 --> 01:15:26,860
um, here, and combine it with optimization.

1491
01:15:26,860 --> 01:15:29,395
So you can think about doing this for a number of different policies,

1492
01:15:29,395 --> 01:15:31,270
trying to compute lower bounds on all of them.

1493
01:15:31,270 --> 01:15:33,490
And then using that information to try to

1494
01:15:33,490 --> 01:15:36,715
decide which one to deploy in the future, in a way that is safe.

1495
01:15:36,715 --> 01:15:38,350
Okay. So you can sort of say,

1496
01:15:38,350 --> 01:15:40,660
"I'm gonna use some of my data to optimize,

1497
01:15:40,660 --> 01:15:42,910
some of my data to, um, ah,

1498
01:15:42,910 --> 01:15:44,845
to try to evaluate the resulting one,

1499
01:15:44,845 --> 01:15:47,560
and make sure that it's got a good confidence bound before I deploy it.

1500
01:15:47,560 --> 01:15:52,300
[NOISE] And again, you can do this in digital marketing,

1501
01:15:52,300 --> 01:15:54,805
some of the other work that Phil Thomas and I have looked at is,

1502
01:15:54,805 --> 01:15:56,860
using a diabetes simulator,

1503
01:15:56,860 --> 01:15:59,365
and looking at whether or not we can infer

1504
01:15:59,365 --> 01:16:02,589
higher-performing policies for things like [NOISE] insulin regulation,

1505
01:16:02,589 --> 01:16:07,375
um, ah, using similar ideas in something that in a way that you could be,

1506
01:16:07,375 --> 01:16:10,700
um, with high-confidence better before you deploy it.

1507
01:16:11,550 --> 01:16:15,310
I'm gonna skip briefly through this.

1508
01:16:15,310 --> 01:16:17,230
This- this is a really big ba- ah,

1509
01:16:17,230 --> 01:16:18,865
like this is an increasingly, ah,

1510
01:16:18,865 --> 01:16:21,295
big area of work in the community.

1511
01:16:21,295 --> 01:16:23,230
Um, I think a lot of people are thinking about,

1512
01:16:23,230 --> 01:16:25,630
it is counterfactual reasoning issue because we have

1513
01:16:25,630 --> 01:16:27,940
more and more data from electronic medical records systems,

1514
01:16:27,940 --> 01:16:29,965
that we'd like to use to improve patient health.

1515
01:16:29,965 --> 01:16:31,390
We have data, um, you know,

1516
01:16:31,390 --> 01:16:33,460
on- on online platforms etc.

1517
01:16:33,460 --> 01:16:34,930
There's a lot of additional challenges,

1518
01:16:34,930 --> 01:16:37,015
things like how do you deal with long horizons?

1519
01:16:37,015 --> 01:16:39,910
Um, [NOISE] the fact that importance sampling can be unfair,

1520
01:16:39,910 --> 01:16:42,115
ah, [NOISE] what do I mean by that?

1521
01:16:42,115 --> 01:16:44,425
I mean that, essentially,

1522
01:16:44,425 --> 01:16:46,495
different policies when you evaluate them,

1523
01:16:46,495 --> 01:16:48,610
might have different amounts of variance depending on

1524
01:16:48,610 --> 01:16:50,905
how well they match to your behavior policy.

1525
01:16:50,905 --> 01:16:54,355
Because of that, it may be hard to decide which of those you should deploy.

1526
01:16:54,355 --> 01:16:58,150
Um, we have various work thinking about when the behavior policy is unknown.

1527
01:16:58,150 --> 01:17:00,445
Where we combine these ideas with deep neural networks,

1528
01:17:00,445 --> 01:17:02,875
um, and we're also thinking about transfer learning.

1529
01:17:02,875 --> 01:17:04,960
So I know Chelsea talked about meta-learning on Monday.

1530
01:17:04,960 --> 01:17:07,315
Um, one of the interesting ideas here is,

1531
01:17:07,315 --> 01:17:09,190
you're building these forms of models.

1532
01:17:09,190 --> 01:17:13,615
Can you kinda use the same ideas of fine tuning in the reinforcement learning case?

1533
01:17:13,615 --> 01:17:17,170
So can you think about building models for our policy evaluation

1534
01:17:17,170 --> 01:17:20,785
that leveraged as data that does not match the policy you care about?

1535
01:17:20,785 --> 01:17:22,630
In order to get generally better models,

1536
01:17:22,630 --> 01:17:23,770
you know, things like health care,

1537
01:17:23,770 --> 01:17:24,940
I think that can be pretty helpful.

1538
01:17:24,940 --> 01:17:27,550
[NOISE] But there's a lot of additional work on this,

1539
01:17:27,550 --> 01:17:31,075
and there's a number of other groups that are thinking about these types of ideas.

1540
01:17:31,075 --> 01:17:35,800
Um, and also on campus if you're interested in these ideas in general.

1541
01:17:35,800 --> 01:17:37,630
There's also a number of great colleagues that are

1542
01:17:37,630 --> 01:17:39,520
thinking about this from other perspectives.

1543
01:17:39,520 --> 01:17:41,694
People coming at it from the perspective of economics,

1544
01:17:41,694 --> 01:17:43,450
or statistics, or epidemiology.

1545
01:17:43,450 --> 01:17:46,735
And it's been really fun to try to get to collaborate with these people as well.

1546
01:17:46,735 --> 01:17:50,110
So just to pop-up a level briefly, um,

1547
01:17:50,110 --> 01:17:53,620
the goal in these cases is to think about if you have some set of data,

1548
01:17:53,620 --> 01:18:00,070
we're gonna go from data to a good new policy [NOISE].

1549
01:18:00,070 --> 01:18:01,900
Okay. And you want it to be good in a way that you

1550
01:18:01,900 --> 01:18:04,690
know something about the quality of it before you deploy it.

1551
01:18:04,690 --> 01:18:08,710
And so that's really what's sort of safe off-policy policy evaluation and selection,

1552
01:18:08,710 --> 01:18:10,375
or optimization is about.

1553
01:18:10,375 --> 01:18:13,645
And so in terms of the things that you should understand from here,

1554
01:18:13,645 --> 01:18:16,660
you should be able to define and apply importance sampling [NOISE] ,

1555
01:18:16,660 --> 01:18:18,700
know some limitations of importance sampling.

1556
01:18:18,700 --> 01:18:21,310
[NOISE] List a couple of- of alternatives.

1557
01:18:21,310 --> 01:18:24,850
Know, you- why you might want to be able to do this sort of safe reinforcement learning,

1558
01:18:24,850 --> 01:18:26,920
like what sort of applications this might be important in.

1559
01:18:26,920 --> 01:18:32,900
[NOISE] And sort of the- define what type of guarantees we're getting in these scenarios.

1560
01:18:33,030 --> 01:18:35,380
So that's it, and then next week,

1561
01:18:35,380 --> 01:18:36,550
we'll have the quiz on Monday,

1562
01:18:36,550 --> 01:18:37,690
and then on Wednesday we'll talk

1563
01:18:37,690 --> 01:18:40,460
about Monte Carlo tree search. Thanks.

