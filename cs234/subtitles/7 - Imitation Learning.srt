1
00:00:05,360 --> 00:00:07,470
All right. So, homework two,

2
00:00:07,470 --> 00:00:09,090
you guys are probably starting to work on,

3
00:00:09,090 --> 00:00:10,860
and we're having sessions this week that

4
00:00:10,860 --> 00:00:14,295
are good for if you don't have background in deep learning,

5
00:00:14,295 --> 00:00:17,974
and feel free to reach out on Piazza. Oh, yeah,

6
00:00:17,974 --> 00:00:19,440
I just have a question about the project.

7
00:00:19,440 --> 00:00:21,300
I just want to make sure,

8
00:00:21,300 --> 00:00:23,010
it seemed currently with the note on Piazza that like, I-50 was the default suggested one.

9
00:00:23,010 --> 00:00:27,090
Can we also do something outside of that?

10
00:00:27,090 --> 00:00:29,160
Oh, yeah, no,   question is a great one.

11
00:00:29,160 --> 00:00:31,740
Yeah, there's a, the post on Piazza,

12
00:00:31,740 --> 00:00:34,050
you're always welcome to design your own project.

13
00:00:34,050 --> 00:00:35,295
That's always completely fine,

14
00:00:35,295 --> 00:00:38,385
and a number of you have come talk to me about those, or talked to other TAs.

15
00:00:38,385 --> 00:00:40,160
These are an additional option.

16
00:00:40,160 --> 00:00:42,710
So, if people are interested in looking at either

17
00:00:42,710 --> 00:00:44,730
the default project which we released yesterday,

18
00:00:44,730 --> 00:00:46,830
which has to do with bandits and warfarin,

19
00:00:46,830 --> 00:00:53,120
or if you want to look at some of the suggestions from senior PhD students or postdocs,

20
00:00:53,120 --> 00:00:55,085
those are great opportunities.

21
00:00:55,085 --> 00:00:58,100
Particularly, I think if you haven't ever done reinforcement learning before,

22
00:00:58,100 --> 00:00:59,970
it's often I wouldn't expect at three weeks in

23
00:00:59,970 --> 00:01:02,390
that you'd be able to define a state of the art project.

24
00:01:02,390 --> 00:01:05,630
So, if you're interested in learning more about RL research,

25
00:01:05,630 --> 00:01:07,730
then it can be a really great opportunity to look at some of

26
00:01:07,730 --> 00:01:10,000
those suggested projects then reach out to people.

27
00:01:10,000 --> 00:01:12,005
All right.

28
00:01:12,005 --> 00:01:15,620
The other thing that I just wanted to do a friendly reminder

29
00:01:15,620 --> 00:01:19,310
about is we explicitly post FAQs for each of the homeworks.

30
00:01:19,310 --> 00:01:21,980
Um, and as some of the TAs are mentioning that some of

31
00:01:21,980 --> 00:01:23,560
the students coming into office hours right now

32
00:01:23,560 --> 00:01:25,280
might not have had a chance to look at those.

33
00:01:25,280 --> 00:01:29,330
So, if you ever have a question when you're going over the homework,

34
00:01:29,330 --> 00:01:31,100
the first thing to do is to go to Piazza and

35
00:01:31,100 --> 00:01:32,930
particularly to look at those pinned notes at the

36
00:01:32,930 --> 00:01:37,150
very top which have very common FAQs about the assignment.

37
00:01:37,150 --> 00:01:41,090
So, make sure to read those before you go to office hours,

38
00:01:41,090 --> 00:01:43,610
and then, of course, feel free to come to office hours as well.

39
00:01:43,610 --> 00:01:46,055
But those are a really good resource to look at.

40
00:01:46,055 --> 00:01:51,030
Any other questions? All right,

41
00:01:51,030 --> 00:01:53,315
so just in terms of where we are in the course right now,

42
00:01:53,315 --> 00:01:55,745
we went through DQN on Monday.

43
00:01:55,745 --> 00:01:58,925
We're gonna talk today some a bit more about,

44
00:01:58,925 --> 00:02:02,270
we can wrap up some of the stuff that I had to rush through at the end of Monday in terms

45
00:02:02,270 --> 00:02:05,450
of deep Q-learning and some of the recent extensions.

46
00:02:05,450 --> 00:02:07,760
Then we're gonna talk some about imitation learning and

47
00:02:07,760 --> 00:02:12,690
large state spaces before next week starting to talk about policy gradient methods.

48
00:02:14,210 --> 00:02:19,550
So just to, we'll start off with sort of a refresher from what DQN was doing,

49
00:02:19,550 --> 00:02:22,790
DQN was this idea of combining between

50
00:02:22,790 --> 00:02:26,945
Q-learning and using deep neural networks as function approximators.

51
00:02:26,945 --> 00:02:32,185
And the two key sort of algorithmic changes compared to prior work was that,

52
00:02:32,185 --> 00:02:38,030
they used experience replay and fixed Q targets.

53
00:02:38,030 --> 00:02:42,620
And by fixed Q targets there that was meaning that when we used our r,

54
00:02:42,620 --> 00:02:47,270
rt plus gamma, max over a,

55
00:02:47,270 --> 00:02:52,540
Q of sta, st plus one, right.

56
00:02:52,540 --> 00:02:59,060
That the weights that were used for that Q representation were fixed for a while.

57
00:02:59,060 --> 00:03:05,165
So maybe we'd update those every 100 steps or every 50 episodes or some interval.

58
00:03:05,165 --> 00:03:09,470
And, uh, so this provided a more stable target for

59
00:03:09,470 --> 00:03:12,770
supervised learning because the supervised learning part

60
00:03:12,770 --> 00:03:14,810
again is that we were had this combination of,

61
00:03:14,810 --> 00:03:17,180
of we want to have weights and we want to

62
00:03:17,180 --> 00:03:20,540
minimize this error versus our current estimate,

63
00:03:20,540 --> 00:03:23,880
sort of minimizing the TD error.

64
00:03:24,550 --> 00:03:32,045
So the way that this preceded is that we'd restore transition in a replay memory buffer.

65
00:03:32,045 --> 00:03:37,640
We do mini batches, where we would sample a bunch of state extra word and next state tuples and

66
00:03:37,640 --> 00:03:40,295
then do these backups where we're sort of

67
00:03:40,295 --> 00:03:43,295
updating our Q function and refitting our Q function.

68
00:03:43,295 --> 00:03:47,270
Um, and like a lot of the linear value function methods we saw before,

69
00:03:47,270 --> 00:03:49,325
it uses stochastic gradient descent.

70
00:03:49,325 --> 00:03:52,430
And the really cool thing about this is that they did it on 50 games.

71
00:03:52,430 --> 00:03:54,560
They used the same architecture for those 50 games and

72
00:03:54,560 --> 00:03:57,920
the same hyper parameters and they got human level performance.

73
00:03:57,920 --> 00:03:59,915
So we've talked quite a lot about that before.

74
00:03:59,915 --> 00:04:01,955
And then we sort of briefly talked about

75
00:04:01,955 --> 00:04:06,590
three sort of major extensions to that in the immediate following years.

76
00:04:06,590 --> 00:04:07,910
And again, there's been a lot of

77
00:04:07,910 --> 00:04:11,570
extensions and a lot of work in deep reinforcement learning right now.

78
00:04:11,570 --> 00:04:14,495
The three of them were as follows.

79
00:04:14,495 --> 00:04:16,865
The first was Double DQN.

80
00:04:16,865 --> 00:04:19,579
And we talked before we got it,

81
00:04:19,579 --> 00:04:24,205
the function approximation talking about the issue with maximization bias,

82
00:04:24,205 --> 00:04:27,875
that when you're using the same representation

83
00:04:27,875 --> 00:04:31,130
to pick an action and estimate the value of that action,

84
00:04:31,130 --> 00:04:33,350
you can get into a maximization bias problem.

85
00:04:33,350 --> 00:04:35,270
And the way that that's avoided in

86
00:04:35,270 --> 00:04:37,010
Double DQN and I wanted to go

87
00:04:37,010 --> 00:04:39,020
over this again because I had a couple questions after class.

88
00:04:39,020 --> 00:04:40,880
We didn't have much time to discuss it.

89
00:04:40,880 --> 00:04:44,630
Is what happens is we have a current queue network which is

90
00:04:44,630 --> 00:04:49,180
parameterized by a set of weights and that is what is used to select actions.

91
00:04:49,180 --> 00:04:51,220
Just to be clear here,

92
00:04:51,220 --> 00:04:53,795
often we're doing some sort of E-greedy method.

93
00:04:53,795 --> 00:04:57,590
So we'd used the current Q-network weights to decide on the best action.

94
00:04:57,590 --> 00:05:00,470
And we would pick that with one minus epsilon probability.

95
00:05:00,470 --> 00:05:04,520
And then there's an older Q-network that is used to evaluate those actions.

96
00:05:04,520 --> 00:05:08,075
So if we look at how we're gonna be changing our weights,

97
00:05:08,075 --> 00:05:14,480
we're gonna be having an action evaluation using these other weights,

98
00:05:14,480 --> 00:05:20,870
w minus and then action selection using W. So when you look at this,

99
00:05:20,870 --> 00:05:24,995
that might start to look pretty similar to what DQN was doing

100
00:05:24,995 --> 00:05:29,240
because DQN was saying we're gonna use a fixed set of weights for,

101
00:05:29,240 --> 00:05:30,440
for these target updates.

102
00:05:30,440 --> 00:05:34,295
So what DQN was doing was this,

103
00:05:34,295 --> 00:05:37,625
r plus Gamma Q.

104
00:05:37,625 --> 00:05:46,905
I'll write the max in, max a,

105
00:05:46,905 --> 00:05:53,355
Q of s prime, a, w minus,

106
00:05:53,355 --> 00:06:00,600
minus the current s. So in the normal DQN,

107
00:06:00,600 --> 00:06:03,525
they were also using a w minus.

108
00:06:03,525 --> 00:06:07,380
But here in a Double DQN,

109
00:06:07,380 --> 00:06:08,750
it can be a little bit different.

110
00:06:08,750 --> 00:06:10,760
And the reason it's a little bit different than what we just

111
00:06:10,760 --> 00:06:13,430
saw is that you can maintain two sets of

112
00:06:13,430 --> 00:06:18,320
weights at all times and you can flip between them on every step or every batch.

113
00:06:18,320 --> 00:06:20,480
So when DQN was introduced,

114
00:06:20,480 --> 00:06:23,510
it was more of an idea of you fix your weights.

115
00:06:23,510 --> 00:06:27,410
Let's say, from time step t to time step t plus 100,

116
00:06:27,410 --> 00:06:31,235
use the same weights that whole time period for your target.

117
00:06:31,235 --> 00:06:33,410
In Double DQN, you don't necessarily have to do that.

118
00:06:33,410 --> 00:06:36,260
You can flip back and forth between these which is what we'd seen

119
00:06:36,260 --> 00:06:39,870
with a double Q-learning that on,

120
00:06:39,870 --> 00:06:44,465
you know, on step one you can use weights one to act and weights two to evaluate.

121
00:06:44,465 --> 00:06:47,420
On step two, you could do weights two to evaluate and weights one to act.

122
00:06:47,420 --> 00:06:50,045
So it means that you can propagate information faster.

123
00:06:50,045 --> 00:06:54,530
So instead of waiting 50 episodes or 100 episodes to update,

124
00:06:54,530 --> 00:06:57,410
um, the weights that you're using for your target,

125
00:06:57,410 --> 00:06:59,640
so again this is your target,

126
00:07:00,500 --> 00:07:03,425
you can flip back and forth between them

127
00:07:03,425 --> 00:07:05,675
which allows you to update both networks a lot have,

128
00:07:05,675 --> 00:07:07,715
update both set of net- network weights.

129
00:07:07,715 --> 00:07:09,455
The networks are identical. Yeah.

130
00:07:09,455 --> 00:07:12,230
Um, in general, when you're evaluating

131
00:07:12,230 --> 00:07:15,365
these kinds of different approaches to improve these techniques,

132
00:07:15,365 --> 00:07:18,260
is there, is there a trade off between

133
00:07:18,260 --> 00:07:20,990
how fast information propagates and then how unstable it is?

134
00:07:20,990 --> 00:07:25,970
So we might find that if the system we're trying to learn on is itself,

135
00:07:25,970 --> 00:07:27,530
relatively well-behaved and stable,

136
00:07:27,530 --> 00:07:30,800
we want to pick something that has faster information propagation but if it's

137
00:07:30,800 --> 00:07:34,775
highly noisy or unstable that we need to do something that's more conservative.

138
00:07:34,775 --> 00:07:37,130
uh, makes a good question which is, you know,

139
00:07:37,130 --> 00:07:40,940
is there generally a trade-off in terms of these methods between sort of

140
00:07:40,940 --> 00:07:42,890
characterizing the stability of the system and

141
00:07:42,890 --> 00:07:45,425
then how fast you can propagate information back?

142
00:07:45,425 --> 00:07:47,915
Unfortunately, I feel like it's not very well characterized.

143
00:07:47,915 --> 00:07:50,240
So I feel like most of the time,

144
00:07:50,240 --> 00:07:52,540
these are heuristics and people evaluate them,

145
00:07:52,540 --> 00:07:54,180
they evaluate them with a lot of different benchmarks

146
00:07:54,180 --> 00:07:56,090
and that's sort of the way we get generalization.

147
00:07:56,090 --> 00:07:58,310
But I don't think that there's a good characterization

148
00:07:58,310 --> 00:08:02,255
systematically of how to characterize the stability of the system,

149
00:08:02,255 --> 00:08:04,010
with these deep neural networks,

150
00:08:04,010 --> 00:08:05,540
particularly in the context of RL.

151
00:08:05,540 --> 00:08:07,670
So there's a lot of great opportunities for

152
00:08:07,670 --> 00:08:10,630
theoretical analysis here too or just sort of more formal understanding.

153
00:08:10,630 --> 00:08:12,530
Right now, I think we're at the level of saying this either just

154
00:08:12,530 --> 00:08:14,600
seems to consistently work a bunch across

155
00:08:14,600 --> 00:08:19,535
Atari games and maybe MuJoCo or it doesn't try to characterize the, the successes.

156
00:08:19,535 --> 00:08:22,010
Yeah. Is it

157
00:08:22,010 --> 00:08:26,510
Yes. I was wondering if we kind of get a bit more about the switching then.

158
00:08:26,510 --> 00:08:30,835
You're representing like why, how.

159
00:08:30,835 --> 00:08:33,610
Yeah. So, question is about, you know,

160
00:08:33,610 --> 00:08:36,070
how can we switch between these w and w minus,

161
00:08:36,070 --> 00:08:37,570
and how would, you know,

162
00:08:37,570 --> 00:08:39,280
um, why and how would you do this?

163
00:08:39,280 --> 00:08:41,905
So, in the DQN setting,

164
00:08:41,905 --> 00:08:44,515
um, you could set w minus.

165
00:08:44,515 --> 00:08:48,280
So at the beginning, w minus is equal to w on time step zero.

166
00:08:48,280 --> 00:08:53,395
And then, in DQN you would keep w minus to be the same maybe for the next 50 episodes,

167
00:08:53,395 --> 00:08:56,170
but you'd be updating w. And then 50 episodes in,

168
00:08:56,170 --> 00:08:57,895
you would update w minus.

169
00:08:57,895 --> 00:09:02,395
The downside about that which we talked a little bit about before is that

170
00:09:02,395 --> 00:09:07,525
you're not using the information you're getting to update this estimate.

171
00:09:07,525 --> 00:09:10,690
Okay. Because you're using that old stale set of w's.

172
00:09:10,690 --> 00:09:13,660
So essentially, you're just not using the information you've got over

173
00:09:13,660 --> 00:09:15,490
those 50 episodes to update what would

174
00:09:15,490 --> 00:09:18,775
happen if you were caught in S prime and then took action a.

175
00:09:18,775 --> 00:09:23,725
So, an alternative would be to flip between,

176
00:09:23,725 --> 00:09:26,830
let's say, instead of thinking of this as w and w minus, then you can think of it that way.

177
00:09:26,830 --> 00:09:29,710
You can just think of maintaining two different sets of weights.

178
00:09:29,710 --> 00:09:32,485
And imagine, um, I'll say,

179
00:09:32,485 --> 00:09:34,180
this is t time equals one,

180
00:09:34,180 --> 00:09:36,475
time equals two, time equals three.

181
00:09:36,475 --> 00:09:40,480
So, imagine that we're just picking between what are the weights that we used

182
00:09:40,480 --> 00:09:43,915
to select an action and the weights that we use to evaluate the action.

183
00:09:43,915 --> 00:09:45,100
So, on the first time step,

184
00:09:45,100 --> 00:09:49,015
you could use this to evaluate and this to the- to select the action,

185
00:09:49,015 --> 00:09:52,690
and then you could flip it back and forth.

186
00:09:52,690 --> 00:09:55,045
So that essentially means that,

187
00:09:55,045 --> 00:09:57,835
both sets of weights are getting updated very frequently.

188
00:09:57,835 --> 00:10:01,405
So, instead of updating only one of the- one of them every 50 episodes,

189
00:10:01,405 --> 00:10:04,705
you're- you're continuing to propagate that information back quickly.

190
00:10:04,705 --> 00:10:09,190
And there's of course tons of chart choices here about how frequently do you update,

191
00:10:09,190 --> 00:10:11,200
you know, when do you switch back and forth between these.

192
00:10:11,200 --> 00:10:15,070
Um, you can think of all of those as hyper-parameters you can imagine tuning.

193
00:10:15,070 --> 00:10:17,800
But this is instead of keeping that- keeping

194
00:10:17,800 --> 00:10:21,055
this target fixed for 50 steps, um, or, you know,

195
00:10:21,055 --> 00:10:22,839
n steps these are all parameters,

196
00:10:22,839 --> 00:10:24,820
you could flip back and forth between them which is what

197
00:10:24,820 --> 00:10:27,655
double Q-learning did before. Yeah, .

198
00:10:27,655 --> 00:10:31,990
Like in the normal DQN settings when we're using a target weight, uh,

199
00:10:31,990 --> 00:10:34,750
wouldn't that target weight like for action selection or

200
00:10:34,750 --> 00:10:38,335
for like evaluation still be another queuing network.

201
00:10:38,335 --> 00:10:40,990
So, how is that was different from double DQN

202
00:10:40,990 --> 00:10:43,915
except for the fact that you're searching double-Q more often?

203
00:10:43,915 --> 00:10:45,970
It is. Or more of the- what question is,

204
00:10:45,970 --> 00:10:48,415
how different is this from the previous year? It's almost identical.

205
00:10:48,415 --> 00:10:51,505
So, I think the- the main difference here is that you could switch,

206
00:10:51,505 --> 00:10:55,165
uh, as long as you're maintaining some set of weights for your target.

207
00:10:55,165 --> 00:10:57,145
This is saying you could sort of switch.

208
00:10:57,145 --> 00:10:58,720
Now you really just have- you've the same network,

209
00:10:58,720 --> 00:11:00,880
two sets of weights that you have to maintain in memory.

210
00:11:00,880 --> 00:11:02,080
And what this is saying is,

211
00:11:02,080 --> 00:11:04,840
you can switch back and forth with those very frequently,

212
00:11:04,840 --> 00:11:08,590
um, and help avoid the maximization bias during that time.

213
00:11:08,590 --> 00:11:11,185
It doesn't always work, it frequently helps.

214
00:11:11,185 --> 00:11:13,195
There is still the issue with stability, um,

215
00:11:13,195 --> 00:11:16,760
but it can be better and it avoids the maximization bias.

216
00:11:17,010 --> 00:11:20,275
We also talked about prioritized experience replay.

217
00:11:20,275 --> 00:11:21,970
Um, we went through a small sort of

218
00:11:21,970 --> 00:11:25,090
tabular example where we looked at the impact of doing backups.

219
00:11:25,090 --> 00:11:29,380
So, if we have this experience replay buffer of SAR S-prime tuples,

220
00:11:29,380 --> 00:11:33,760
which one should we use to do our backups and how do we propagate that information back?

221
00:11:33,760 --> 00:11:36,820
Um, and the- in this algorithm,

222
00:11:36,820 --> 00:11:38,350
they say the- or in this paper,

223
00:11:38,350 --> 00:11:40,060
they talked about the fact, um,

224
00:11:40,060 --> 00:11:41,080
if you can do this optimally.

225
00:11:41,080 --> 00:11:44,365
In some cases you might get an exponential speedup and convergence,

226
00:11:44,365 --> 00:11:46,840
uh, but it's hard to do that, it's computationally intensive.

227
00:11:46,840 --> 00:11:50,170
So, what they proposed here is to prioritize something

228
00:11:50,170 --> 00:11:53,710
based on the size of sort of the DQN error.

229
00:11:53,710 --> 00:11:56,470
The difference between the current estimate of it

230
00:11:56,470 --> 00:12:01,330
and your sort of target estimate that you're looking at.

231
00:12:01,330 --> 00:12:05,035
And so, we talked about how you could use that as a priority, um,

232
00:12:05,035 --> 00:12:06,490
and it could be a stochastic priority,

233
00:12:06,490 --> 00:12:09,220
uh, to try to select items.

234
00:12:09,220 --> 00:12:12,040
And we also talked about the fact that if you set Alpha equal to zero,

235
00:12:12,040 --> 00:12:13,900
this becomes uniform and so then,

236
00:12:13,900 --> 00:12:17,260
there's no particular prioritization over your tuples.

237
00:12:17,260 --> 00:12:21,205
Another thing that we had almost no time to talk about was dueling.

238
00:12:21,205 --> 00:12:23,380
So, dueling was a Best Paper, um, from,

239
00:12:23,380 --> 00:12:27,160
uh, two- 2016, um, in ICML.

240
00:12:27,160 --> 00:12:29,605
Um, let me just give a little bit of

241
00:12:29,605 --> 00:12:31,960
a refresher on this because we went through it very, very fast.

242
00:12:31,960 --> 00:12:34,345
So, the- the intuition here

243
00:12:34,345 --> 00:12:37,150
is that the features that you might need to write down the value of

244
00:12:37,150 --> 00:12:39,490
a state might be different than those need to

245
00:12:39,490 --> 00:12:42,970
specify the relative benefit of different actions in that state.

246
00:12:42,970 --> 00:12:47,860
And you want to understand the relative benefit of actions in order to decide what,

247
00:12:47,860 --> 00:12:49,525
what your policy should be.

248
00:12:49,525 --> 00:12:53,770
So, um, looking at things like game score,

249
00:12:53,770 --> 00:12:55,465
it's obviously very relevant to the value.

250
00:12:55,465 --> 00:12:57,370
Um, that it might be- you might want

251
00:12:57,370 --> 00:13:00,445
other features try to decide what actions to do right now in a game.

252
00:13:00,445 --> 00:13:02,560
And so, the advantage function that came up,

253
00:13:02,560 --> 00:13:05,230
uh, that was designed by Baird a long time ago.

254
00:13:05,230 --> 00:13:07,840
And this is the same Baird that had that counter example to

255
00:13:07,840 --> 00:13:10,765
show why value function approximation can be bad.

256
00:13:10,765 --> 00:13:14,200
Um, so, uh, Baird's work before it said,

257
00:13:14,200 --> 00:13:16,750
well, look you can decompose, um,

258
00:13:16,750 --> 00:13:20,920
if you think of your Q function which is representing the value of a policy starting in

259
00:13:20,920 --> 00:13:25,735
state and taking a particular action versus the value of just that state.

260
00:13:25,735 --> 00:13:28,825
So, this is sort of implicitly Q pi,

261
00:13:28,825 --> 00:13:33,280
S pi of S. So,

262
00:13:33,280 --> 00:13:35,350
like what is the difference between- difference between taking

263
00:13:35,350 --> 00:13:38,590
this particular action versus just following your policy from the current state?

264
00:13:38,590 --> 00:13:40,764
And he called this the advantage.

265
00:13:40,764 --> 00:13:44,990
What's the advantage of that action for that state?

266
00:13:47,660 --> 00:13:50,490
So, in dueling DQN,

267
00:13:50,490 --> 00:13:54,825
instead of having one network that just predicts Q functions,

268
00:13:54,825 --> 00:13:57,720
they use an architecture that separates into predicting

269
00:13:57,720 --> 00:14:00,705
values and predicting these advantage functions and then

270
00:14:00,705 --> 00:14:03,780
adds them back together with the idea being that you might get

271
00:14:03,780 --> 00:14:07,945
different sort of features here and here.

272
00:14:07,945 --> 00:14:11,260
So, you have to decouple for a little bit to make sure that you're capturing

273
00:14:11,260 --> 00:14:13,030
the features that are relevant to capturing

274
00:14:13,030 --> 00:14:15,860
the salient things you want to look at for Q's.

275
00:14:17,490 --> 00:14:21,685
Now, one thing that I mentioned very briefly last time is that,

276
00:14:21,685 --> 00:14:25,675
um, is the- is the advantage function identifiable?

277
00:14:25,675 --> 00:14:27,625
And what do I mean by that in this case?

278
00:14:27,625 --> 00:14:32,080
I mean that if you have a Q function which is what ultimately we're going to use,

279
00:14:32,080 --> 00:14:36,025
um, can we decompose it into a unique a pi and v pi.

280
00:14:36,025 --> 00:14:38,590
So, here ultimately we want a cube.

281
00:14:38,590 --> 00:14:41,080
And the question is, if we then in our

282
00:14:41,080 --> 00:14:43,840
architecture decomposing this into a value and an advantage,

283
00:14:43,840 --> 00:14:45,760
is there a unique way to do that?

284
00:14:45,760 --> 00:14:48,610
Is there? Um, but there isn't.

285
00:14:48,610 --> 00:14:57,655
So, if you- if you add a constant to both Q and V,

286
00:14:57,655 --> 00:15:03,070
um, then you can get the same advantage function.

287
00:15:03,070 --> 00:15:05,785
So, there's not a unique, you can always shift your um,

288
00:15:05,785 --> 00:15:09,715
shift your awards by a constant and that's not going to change your policy,

289
00:15:09,715 --> 00:15:11,290
it will change your value function.

290
00:15:11,290 --> 00:15:14,110
Um, I- so, there's lots of

291
00:15:14,110 --> 00:15:17,425
different ways to decompose your advantage function and your values,

292
00:15:17,425 --> 00:15:21,490
it's not a unique decomposition.

293
00:15:21,490 --> 00:15:24,370
So, the way that they defined it there is to say, well,

294
00:15:24,370 --> 00:15:31,255
let's force the advantage for state and action to be zero if A is the action taken.

295
00:15:31,255 --> 00:15:35,470
So, here they compare

296
00:15:35,470 --> 00:15:39,895
it to the action that's taken if you're using sort of say, a greedy approach.

297
00:15:39,895 --> 00:15:43,645
Um, and this is really just a way to-

298
00:15:43,645 --> 00:15:48,385
all of this we can think of it in some ways as an analogy to supervised learning.

299
00:15:48,385 --> 00:15:51,340
And so, we want to have a stable target and we want to be able to learn

300
00:15:51,340 --> 00:15:52,660
these advantage functions and

301
00:15:52,660 --> 00:15:56,110
these value functions if we have lots and lots of data about them.

302
00:15:56,110 --> 00:15:57,280
And so, this is sort of choosing

303
00:15:57,280 --> 00:16:00,880
a particular fixed point for how to define the advantage function.

304
00:16:00,880 --> 00:16:02,140
And then, they also said, well,

305
00:16:02,140 --> 00:16:04,720
empirically you could just use the mean too.

306
00:16:04,720 --> 00:16:05,770
So, you could just average over

307
00:16:05,770 --> 00:16:10,370
your advantage functions, it's more of just a heuristic approach.

308
00:16:10,620 --> 00:16:13,285
And what they find, again, so,

309
00:16:13,285 --> 00:16:15,520
we sort of we're layering up these additional techniques.

310
00:16:15,520 --> 00:16:18,670
We started with DQN, then we thought about adding, um,

311
00:16:18,670 --> 00:16:22,345
double-Q learning to DQN and then we thought about adding prioritized replay.

312
00:16:22,345 --> 00:16:24,025
And then this is dueling.

313
00:16:24,025 --> 00:16:26,710
And what they find is  dueling versus double DQN with

314
00:16:26,710 --> 00:16:29,650
prioritized replay is a lot better most of the time.

315
00:16:29,650 --> 00:16:32,529
Now, let me see if I can find Montezuma's.

316
00:16:32,529 --> 00:16:38,335
Yep. So, for Montezuma's this new method is basically no better.

317
00:16:38,335 --> 00:16:42,040
Like none of these methods are really tackling hard exploration problems.

318
00:16:42,040 --> 00:16:44,950
But they are doing better ways of sort of propagating information

319
00:16:44,950 --> 00:16:47,830
in the network and trying to change the way we're training the network.

320
00:16:47,830 --> 00:16:49,660
Yeah, questions about that, and name first please.

321
00:16:49,660 --> 00:16:51,850
Can you speak a little bit louder,

322
00:16:51,850 --> 00:16:53,650
I'm unable to hear you well.

323
00:16:53,650 --> 00:16:56,150
Okay. I'll try to speak a little bit louder.

324
00:16:56,280 --> 00:17:00,170
Can- can people in the back over there hear me or is it just him?

325
00:17:00,170 --> 00:17:02,470
Okay. Good. All right.

326
00:17:02,470 --> 00:17:05,650
So, these were three of the methods that ended up making a big difference.

327
00:17:05,650 --> 00:17:07,825
We talked very briefly about practical tips.

328
00:17:07,825 --> 00:17:09,970
Um, I won't go in these too much.

329
00:17:09,970 --> 00:17:12,640
The main thing is just that we try to actively encourage you to

330
00:17:12,640 --> 00:17:16,060
build up your acuity representation first before you try on Atari.

331
00:17:16,060 --> 00:17:18,880
Um, you can try different forms of losses.

332
00:17:18,880 --> 00:17:22,825
Uh, learning rate is important, but in this case,

333
00:17:22,825 --> 00:17:24,040
in our assignment we're going to be using

334
00:17:24,040 --> 00:17:27,605
the Adam optimizer which means you don't have to worry too much about it.

335
00:17:27,605 --> 00:17:31,770
There's a issue of sort of trying different exploration schemes,

336
00:17:31,770 --> 00:17:34,185
is something that we're going to talk about later in this class.

337
00:17:34,185 --> 00:17:38,620
So, for right now we're still thinking about just simple E-greedy approaches.

338
00:17:38,810 --> 00:17:41,275
Um, a nice paper that came out,

339
00:17:41,275 --> 00:17:44,605
I think it was start of 2018, um,

340
00:17:44,605 --> 00:17:47,260
was Rainbow which was a paper that basically

341
00:17:47,260 --> 00:17:49,945
just tried to combine a whole bunch of these recent methods,

342
00:17:49,945 --> 00:17:53,110
um, to see really how big of an improvement do you get.

343
00:17:53,110 --> 00:17:57,130
Uh, now again, note in this case and we'll come back to this in just a couple of slides.

344
00:17:57,130 --> 00:18:00,310
This is a lot of data for a lot of experience in the world,

345
00:18:00,310 --> 00:18:02,725
200 million frames of experience.

346
00:18:02,725 --> 00:18:05,260
But they developed an algorithm called Rainbow that

347
00:18:05,260 --> 00:18:07,540
combines a lot of the things we've just been talking about,

348
00:18:07,540 --> 00:18:10,795
double DQN, prioritized, and dueling,

349
00:18:10,795 --> 00:18:13,195
as well as some other recent advances.

350
00:18:13,195 --> 00:18:18,085
Um, noisy is one that also tries to do some different forms of exploration.

351
00:18:18,085 --> 00:18:21,130
And so, they found that kind of by adding these improvements together,

352
00:18:21,130 --> 00:18:23,720
then you could get a significant improvement.

353
00:18:23,720 --> 00:18:27,180
I think this is a useful insight because often it's not clear whether or

354
00:18:27,180 --> 00:18:29,865
not these different gains are additive or if they're just, um,

355
00:18:29,865 --> 00:18:32,455
sort of, um, you're,

356
00:18:32,455 --> 00:18:35,515
you're- they're kind of doing the same thing but maybe in a slightly different way.

357
00:18:35,515 --> 00:18:38,110
And so, it's nice to see that in some of these cases these different sort

358
00:18:38,110 --> 00:18:41,035
of ideas are additive in terms of the resulting performance gain.

359
00:18:41,035 --> 00:18:45,250
Um, these aren't sort of- this is still a very large amount of data.

360
00:18:45,250 --> 00:18:48,580
[NOISE]

361
00:18:48,580 --> 00:18:52,570
Okay. So just to summarize which we're wrapping up where we are with model-free,

362
00:18:52,570 --> 00:18:54,670
ah, deep neural networks for RL right now.

363
00:18:54,670 --> 00:18:57,955
Uh, they're very expressive function approximators.

364
00:18:57,955 --> 00:19:01,000
Uh, you should be able to understand how you represent the Q function,

365
00:19:01,000 --> 00:19:04,615
and you could do some Monte Carlo-based methods or TD style methods.

366
00:19:04,615 --> 00:19:06,760
Um, and- and at this point,

367
00:19:06,760 --> 00:19:10,240
it's sort of good to make sure you understand how you would do that with tabular methods,

368
00:19:10,240 --> 00:19:11,980
with linear value function methods,

369
00:19:11,980 --> 00:19:13,540
and with deep neural networks.

370
00:19:13,540 --> 00:19:15,280
So it's sort of, algorithmically,

371
00:19:15,280 --> 00:19:18,850
it looks very similar across all of those but then you- in some cases,

372
00:19:18,850 --> 00:19:22,855
you have to do this step of doing function approximation and in other cases you don't.

373
00:19:22,855 --> 00:19:25,690
Um, and then it'd be good to just make sure you can sort of list

374
00:19:25,690 --> 00:19:28,690
a few extensions that help beyond DQN um,

375
00:19:28,690 --> 00:19:32,275
and why they do. All right.

376
00:19:32,275 --> 00:19:34,840
So now let's go back to our, um, sort of,

377
00:19:34,840 --> 00:19:38,425
high level, uh, view of what we want from the reinforcement learning algorithms,

378
00:19:38,425 --> 00:19:42,490
these are algorithms that are sort of doing optimization, handling generalization,

379
00:19:42,490 --> 00:19:46,900
ah, doing exploration and doing it at all statistically and computationally efficiently.

380
00:19:46,900 --> 00:19:49,120
And we've just been spending quite a lot of time on looking at

381
00:19:49,120 --> 00:19:51,520
generalization as well as optimization.

382
00:19:51,520 --> 00:19:55,310
Um, but we haven't talked very much about efficiency.

383
00:19:56,030 --> 00:19:59,895
So one of the challenges is- is, that, um,

384
00:19:59,895 --> 00:20:02,520
if you want to define efficiency formally like in terms of

385
00:20:02,520 --> 00:20:05,805
how much data an agent lead, needs to learn to make a good decision.

386
00:20:05,805 --> 00:20:08,055
Um, there are hardness results, ah,

387
00:20:08,055 --> 00:20:11,010
that- that are known so our lab has developed some,

388
00:20:11,010 --> 00:20:12,930
uh, lower bounds, other people have too.

389
00:20:12,930 --> 00:20:14,700
Um, uh, I think we now have

390
00:20:14,700 --> 00:20:19,370
basically tight upper and lower bounds for the tabular MDP case, um,

391
00:20:19,370 --> 00:20:21,940
which indicate that there's some really pathological MDPs

392
00:20:21,940 --> 00:20:24,250
out there for which we just need a lot of data,

393
00:20:24,250 --> 00:20:26,200
lot of data, though, you know, would not scale very

394
00:20:26,200 --> 00:20:28,300
well as you start to go up to really huge domains.

395
00:20:28,300 --> 00:20:30,460
So some of these problems are really hard to do.

396
00:20:30,460 --> 00:20:32,665
Formerly, you would just need a lot of exploration.

397
00:20:32,665 --> 00:20:36,130
You can do something much better than E greedy but we'll talk about that soon.

398
00:20:36,130 --> 00:20:38,050
But even when we do those much better things than

399
00:20:38,050 --> 00:20:40,720
E Greedy we can prove that it's still really hard to learn in those,

400
00:20:40,720 --> 00:20:42,490
we still might need a lot of data.

401
00:20:42,490 --> 00:20:45,220
So an alternative is to say well there's lots of

402
00:20:45,220 --> 00:20:48,775
other supervision that we could have in the world to try to learn how to do things.

403
00:20:48,775 --> 00:20:50,800
Um, and so how can we use

404
00:20:50,800 --> 00:20:54,355
that additional information in order to sort of speed reinforcement learning.

405
00:20:54,355 --> 00:20:57,850
And so what we're going to do, is talk about some- about imitation learning today.

406
00:20:57,850 --> 00:20:59,050
And then we're going to start talking about

407
00:20:59,050 --> 00:21:01,820
policy search and policy gradient methods next week.

408
00:21:01,820 --> 00:21:06,450
And those, you can also think of as another different way to impose structure,

409
00:21:06,450 --> 00:21:08,670
um, because in policy gradient methods,

410
00:21:08,670 --> 00:21:11,580
you always have to define your policy class.

411
00:21:11,580 --> 00:21:14,200
Sometimes that can be a really rich policy class,

412
00:21:14,200 --> 00:21:16,540
and so maybe that's not too much of a limitation,

413
00:21:16,540 --> 00:21:17,680
um, but other times,

414
00:21:17,680 --> 00:21:22,430
you're encoding domain knowledge by the class that you- that you represent.

415
00:21:24,390 --> 00:21:27,940
Okay. And in particular, we're going to be thinking about imitation learning and

416
00:21:27,940 --> 00:21:31,630
large state spaces which is exactly the place where you might hope to benefit from,

417
00:21:31,630 --> 00:21:34,490
um, additional help or supervision.

418
00:21:34,890 --> 00:21:39,370
So if we think about something like Montezuma's Revenge, um,

419
00:21:39,370 --> 00:21:44,935
there's some nice work on looking at sort of how far did DQN get in this case.

420
00:21:44,935 --> 00:21:47,350
So Montezuma's Revenge, for those of you who haven't played,

421
00:21:47,350 --> 00:21:48,775
it is sort of a, um,

422
00:21:48,775 --> 00:21:50,650
a long- very long horizon,

423
00:21:50,650 --> 00:21:54,520
uh, game in which you're sort of trying to navigate through this world,

424
00:21:54,520 --> 00:21:56,800
and like pick up keys and make decisions.

425
00:21:56,800 --> 00:21:59,380
Um, and it involves a lot of different rooms.

426
00:21:59,380 --> 00:22:02,215
So you can see here what the outline of the,

427
00:22:02,215 --> 00:22:04,120
all the white squares are basically rooms.

428
00:22:04,120 --> 00:22:06,760
And on the left-hand side, um,

429
00:22:06,760 --> 00:22:10,225
sort of a DQN that was trained for 50 million frames,

430
00:22:10,225 --> 00:22:13,295
um, only gets through the first two rooms.

431
00:22:13,295 --> 00:22:14,935
Like, it's just doing very badly.

432
00:22:14,935 --> 00:22:17,080
It's not making very much progress.

433
00:22:17,080 --> 00:22:19,420
Um, whereas on the right-hand side,

434
00:22:19,420 --> 00:22:22,480
we see something which is explicitly trying to explore.

435
00:22:22,480 --> 00:22:26,275
Um, and it uses some of the techniques that we'll be talking more about later.

436
00:22:26,275 --> 00:22:28,765
But notice that it still doesn't get all the way through the game.

437
00:22:28,765 --> 00:22:31,000
Um, and so I think this sort

438
00:22:31,000 --> 00:22:33,565
of illustrates the fact that some of these games are really hard.

439
00:22:33,565 --> 00:22:37,315
Um, there has been some really nice additional progress since, um, ah,

440
00:22:37,315 --> 00:22:42,280
both, ah, with me and Percy Liang's lab we now can basically solve Montezuma's.

441
00:22:42,280 --> 00:22:46,645
Um, and there's also been some really nice work from Uber AI lab on solving Montezuma's.

442
00:22:46,645 --> 00:22:49,570
But a lot of the places that people originally got traction on

443
00:22:49,570 --> 00:22:53,275
this was by starting to use imitation learning and demonstrations.

444
00:22:53,275 --> 00:22:56,290
Um, so in particular,

445
00:22:56,290 --> 00:22:59,620
if we think about cases where RL might work well, RL works,

446
00:22:59,620 --> 00:23:03,220
you know, pretty well when it's easy or certainly we've seen a lot of success.

447
00:23:03,220 --> 00:23:06,310
So far when data is cheap and parallelization is easy,

448
00:23:06,310 --> 00:23:08,650
and it can be much harder to use the methods that

449
00:23:08,650 --> 00:23:11,094
we've talked about so far when data is expensive,

450
00:23:11,094 --> 00:23:14,185
um, and when maybe failure is not tolerable.

451
00:23:14,185 --> 00:23:17,320
So , if you tried to use the methods that we just described

452
00:23:17,320 --> 00:23:20,680
to learn to fly like a remote control helicopter.

453
00:23:20,680 --> 00:23:25,120
Typically require a lot of helicopters [LAUGHTER] because it would be very expensive.

454
00:23:25,120 --> 00:23:27,565
And so there's many cases where this type of,

455
00:23:27,565 --> 00:23:30,950
um, performance is just not gonna be practical.

456
00:23:31,320 --> 00:23:36,010
So one of the benefits is that if you can give the agent a lot of rewards,

457
00:23:36,010 --> 00:23:37,735
you can shape behavior pretty quickly.

458
00:23:37,735 --> 00:23:42,280
Um, so one of the challenges in Montezuma's Revenge is that reward is very sparse that,

459
00:23:42,280 --> 00:23:44,380
you know, the agent has to try lots of different things before

460
00:23:44,380 --> 00:23:46,840
it gets any signal of whether it's doing the right thing.

461
00:23:46,840 --> 00:23:49,525
Um, and where do these rewards come from?

462
00:23:49,525 --> 00:23:51,865
I think it's generally it's actually a really deep question.

463
00:23:51,865 --> 00:23:53,380
Um, but for right now,

464
00:23:53,380 --> 00:23:57,100
let's think about sort of just even the challenge of specifying rewards.

465
00:23:57,100 --> 00:23:58,930
Um, so if you manually design them,

466
00:23:58,930 --> 00:24:01,765
that might be pretty brittle depending on the task.

467
00:24:01,765 --> 00:24:05,395
Um, and an alternative is just to demonstrate.

468
00:24:05,395 --> 00:24:08,170
So if you had to write down the reward function for driving a car,

469
00:24:08,170 --> 00:24:09,520
it's quite com- complicated,

470
00:24:09,520 --> 00:24:10,960
like you don't want to hit roads,

471
00:24:10,960 --> 00:24:12,850
here or hit the ro- hit, um, people.

472
00:24:12,850 --> 00:24:14,590
You don't wanna, um, drive off the road.

473
00:24:14,590 --> 00:24:16,060
You want to get to your destination.

474
00:24:16,060 --> 00:24:18,580
And so it's a  very complicated reward function to write down.

475
00:24:18,580 --> 00:24:21,010
But it's pretty easy for most of us to just drive to

476
00:24:21,010 --> 00:24:24,790
a destination and show an example of maybe an optimal behavior.

477
00:24:24,790 --> 00:24:29,245
So that's sort of the idea behind learning from demonstrations.

478
00:24:29,245 --> 00:24:33,070
Um, there's been lots and lots of work on this but since people

479
00:24:33,070 --> 00:24:37,015
started thinking about learning from demonstrations or imitation learning.

480
00:24:37,015 --> 00:24:40,660
I would argue probably this was started roughly 20 years ago,

481
00:24:40,660 --> 00:24:43,240
around 1999, 2000 was a paper

482
00:24:43,240 --> 00:24:46,255
which started to think about learning rewards from demonstration.

483
00:24:46,255 --> 00:24:48,865
Um, but then there's been lots of applications to it since.

484
00:24:48,865 --> 00:24:51,250
So thinking about it for things like highway driving,

485
00:24:51,250 --> 00:24:54,010
um, or navigation, or parking lot navigation.

486
00:24:54,010 --> 00:24:56,590
There's a lot of these cases particularly in driving right now,

487
00:24:56,590 --> 00:24:59,845
but, ah, where people have been thinking and- and robotics too.

488
00:24:59,845 --> 00:25:01,765
To think about how do you do, um,

489
00:25:01,765 --> 00:25:04,180
demonstrations of like how to pick up a cup or things

490
00:25:04,180 --> 00:25:07,195
like that to try to teach robots how to do those tasks.

491
00:25:07,195 --> 00:25:10,825
Um, there's also some really interesting questions too about like,

492
00:25:10,825 --> 00:25:12,385
you know, how do you, uh,

493
00:25:12,385 --> 00:25:14,440
do things like path planning or goal inference,

494
00:25:14,440 --> 00:25:17,080
and again these sorts of cases where it's quite complicated

495
00:25:17,080 --> 00:25:20,065
to write down a reward function directly or it might be brittle.

496
00:25:20,065 --> 00:25:22,885
And the problem with brittle reward functions is that

497
00:25:22,885 --> 00:25:27,560
your agent will optimize to that and it may not be the behavior that you wanted.

498
00:25:27,570 --> 00:25:31,015
So- so the setting from learning from demonstrations,

499
00:25:31,015 --> 00:25:33,160
and- and today I'm going to be somewhat informal

500
00:25:33,160 --> 00:25:35,530
about whether I call things learning from demonstrations,

501
00:25:35,530 --> 00:25:38,095
um, there's also inverse RL.

502
00:25:38,095 --> 00:25:41,300
And there's also imitation learning.

503
00:25:45,450 --> 00:25:50,290
And there are sort of differences but a lot of these things are somewhat interchangeable.

504
00:25:50,290 --> 00:25:53,860
Most of this is about the idea of saying that you're- you have some demonstration data,

505
00:25:53,860 --> 00:25:55,240
and then you're going to use it, uh,

506
00:25:55,240 --> 00:25:59,620
in order to help boot- either bootstrap or completely learn a new policy.

507
00:25:59,620 --> 00:26:03,280
So the idea is that you might get an expert and maybe they're a perfect expert or

508
00:26:03,280 --> 00:26:07,135
maybe they're a pretty good expert to provide some demonstration trajectories of,

509
00:26:07,135 --> 00:26:10,360
um, taking actions, um, in states.

510
00:26:10,360 --> 00:26:12,565
And in many cases,

511
00:26:12,565 --> 00:26:15,040
it'll be easier for people to do this but it's useful to

512
00:26:15,040 --> 00:26:18,055
think about when it's easier to specify one or the other,

513
00:26:18,055 --> 00:26:22,000
and what situations are- are common for each.

514
00:26:22,000 --> 00:26:24,130
So what's the problem setup?

515
00:26:24,130 --> 00:26:27,490
The problem setup is that we have this state space and action space.

516
00:26:27,490 --> 00:26:30,850
Um, some transition model that is typically unknown,

517
00:26:30,850 --> 00:26:34,270
no reward function and instead sort of a set of teachers

518
00:26:34,270 --> 00:26:39,055
demonstrations from some particular we assume for now optimal policy.

519
00:26:39,055 --> 00:26:42,295
Um, and the behavior cloning we're gonna say,

520
00:26:42,295 --> 00:26:44,980
how do we directly learn the teacher's policy?

521
00:26:44,980 --> 00:26:47,350
So how do we match, how do we get

522
00:26:47,350 --> 00:26:52,960
sort of an approximation of pi star directly from those demonstrations?

523
00:26:52,960 --> 00:26:55,000
Inverse RL is typically about saying,

524
00:26:55,000 --> 00:26:57,070
how can we recover the reward function?

525
00:26:57,070 --> 00:26:58,660
Once we have the reward function,

526
00:26:58,660 --> 00:27:00,730
then we can use it to compute a policy.

527
00:27:00,730 --> 00:27:06,445
Uh, and that often- that last step often is combined with the apprenticeship learning.

528
00:27:06,445 --> 00:27:08,110
So that we're both trying to get

529
00:27:08,110 --> 00:27:11,245
that reward function and then actually generate a good policy with that.

530
00:27:11,245 --> 00:27:15,085
In some cases, you might just want the reward function, um,

531
00:27:15,085 --> 00:27:19,210
can anybody think of a case where you might be interested in just the reward function,

532
00:27:19,210 --> 00:27:20,830
maybe you don't want to recover the policy,

533
00:27:20,830 --> 00:27:28,000
but you're just curious about what the reward function is of another agent. Okay.

534
00:27:28,000 --> 00:27:35,250
If you're trying to understand, say, [inaudible]

535
00:27:35,250 --> 00:27:40,200
Yeah, how the environment [inaudible].

536
00:27:40,200 --> 00:27:41,970
Yeah,  I think is a great example.

537
00:27:41,970 --> 00:27:45,000
So, in a lot of, um, uh, science, you know,

538
00:27:45,000 --> 00:27:46,650
biology et cetera you often want to understand

539
00:27:46,650 --> 00:27:48,930
the behavior of organisms or animals or things like that.

540
00:27:48,930 --> 00:27:51,420
And so, if you can just look at their behavior,

541
00:27:51,420 --> 00:27:55,665
you could say track monkeys or things like that and then use that to back solve,

542
00:27:55,665 --> 00:27:57,780
like, what is their reward function.

543
00:27:57,780 --> 00:27:59,745
What are the- the goals or preferences.

544
00:27:59,745 --> 00:28:02,160
Um, I think there's a number of cases where that's useful,

545
00:28:02,160 --> 00:28:03,360
and maybe down the line, you know,

546
00:28:03,360 --> 00:28:05,085
maybe there's some optimization that'll happen

547
00:28:05,085 --> 00:28:07,380
but- but generally often there it's just about understanding,

548
00:28:07,380 --> 00:28:08,790
like, what is the goal structure or what is

549
00:28:08,790 --> 00:28:11,610
the preference structure of- of the organism or individual.

550
00:28:11,610 --> 00:28:14,325
That could happen with people too that you might want to understand, like,

551
00:28:14,325 --> 00:28:17,625
the choices people are making in terms of nav- you know, um, uh,

552
00:28:17,625 --> 00:28:21,675
commuting or in terms of buying preferences or things like that.

553
00:28:21,675 --> 00:28:24,450
Maybe later you want to optimize for that but also you're just curious

554
00:28:24,450 --> 00:28:28,290
about how- how do people's behavior reveal the,

555
00:28:28,290 --> 00:28:30,720
sort of, um, an underlying reward structure,

556
00:28:30,720 --> 00:28:32,745
underlying preference model. Yeah,

557
00:28:32,745 --> 00:28:36,225
Imitation learning just using the teacher's demonstration set,

558
00:28:36,225 --> 00:28:38,010
like, an upper bound, I guess,

559
00:28:38,010 --> 00:28:41,415
or, uh, have there been cases such that, yes,

560
00:28:41,415 --> 00:28:45,420
that, like, agent learns to perform better than the actor did.

561
00:28:45,420 --> 00:28:47,070
Like we find a new path side.

562
00:28:47,070 --> 00:28:49,815
Yeah, asked a nice question of, like,

563
00:28:49,815 --> 00:28:51,960
is the expert's behavior an

564
00:28:51,960 --> 00:28:55,095
upper bound or are there cases also where the agent can go beyond this?

565
00:28:55,095 --> 00:28:56,940
We're not gonna talk too much about this today,

566
00:28:56,940 --> 00:29:00,480
but there's a lot of work right now on combining imitation learning with RL.

567
00:29:00,480 --> 00:29:06,525
So, um, uh, there's a lot of work on say like inverse RL plus RL.

568
00:29:06,525 --> 00:29:10,050
Where for example, you might use this to s- bootstrap the system,

569
00:29:10,050 --> 00:29:12,870
um, and then your agent would continue to learn on top of this.

570
00:29:12,870 --> 00:29:15,150
There's also some nice work from Pieter Abbeel's Group, um,

571
00:29:15,150 --> 00:29:19,560
where they looked at assuming that the expert was providing like a noisy demonstration of

572
00:29:19,560 --> 00:29:21,720
an optimal path and then the goal is to

573
00:29:21,720 --> 00:29:24,165
learn the optimal path not the noisy demonstration of it.

574
00:29:24,165 --> 00:29:26,610
So, often you do want to go beyond the expert.

575
00:29:26,610 --> 00:29:28,275
There's limitations to that,

576
00:29:28,275 --> 00:29:30,030
and we'll talk about that in a second actually.

577
00:29:30,030 --> 00:29:32,550
What- what are some of the limitations that you might have if

578
00:29:32,550 --> 00:29:36,250
you don't get to continue to gather data in the new environment.

579
00:29:38,330 --> 00:29:42,555
Okay, so let's start with behavioral cloning which is probably the simplest one,

580
00:29:42,555 --> 00:29:45,540
um, because essentially in behavioral cloning we're

581
00:29:45,540 --> 00:29:48,480
just gonna treat this as a standard supervised learning problem.

582
00:29:48,480 --> 00:29:51,330
So, we're going to fix a policy class which means, sort of,

583
00:29:51,330 --> 00:29:53,055
some way to represent, um,

584
00:29:53,055 --> 00:29:55,465
our mapping from states to actions.

585
00:29:55,465 --> 00:29:57,110
And this could be a deep neural network.

586
00:29:57,110 --> 00:29:58,490
It could be a decision tree,

587
00:29:58,490 --> 00:30:00,200
could be lots of different things.

588
00:30:00,200 --> 00:30:03,985
Um, and then we're just going to estimate a policy from the training examples.

589
00:30:03,985 --> 00:30:06,390
So, we're just gonna say, we saw all these times.

590
00:30:06,390 --> 00:30:08,655
We saw a state and an action from our expert

591
00:30:08,655 --> 00:30:11,385
and that's just our input output for our supervised learning model.

592
00:30:11,385 --> 00:30:14,490
And we're just gonna learn a mapping from states to actions.

593
00:30:14,490 --> 00:30:17,715
And early on, so this has been around, uh,

594
00:30:17,715 --> 00:30:19,320
really for quite a long time and I,

595
00:30:19,320 --> 00:30:21,495
uh, should have said more like 30 years.

596
00:30:21,495 --> 00:30:24,390
Um, there were some nice examples of doing this.

597
00:30:24,390 --> 00:30:27,660
So, ALVINN, um, was a very early, uh,

598
00:30:27,660 --> 00:30:31,800
paper, uh, and system about thinking about, uh, driving on the road.

599
00:30:31,800 --> 00:30:35,100
See, it was a neural network and it was trained, uh,

600
00:30:35,100 --> 00:30:40,930
at least in part using behavioral cloning or supervised learning to imitate trajectories.

601
00:30:41,900 --> 00:30:46,620
Um, okay, so let's think about why this might go wrong.

602
00:30:46,620 --> 00:30:48,240
Um, and to first,

603
00:30:48,240 --> 00:30:50,400
let's think about what happens in supervised learning.

604
00:30:50,400 --> 00:30:52,560
So, in supervised learning, um,

605
00:30:52,560 --> 00:30:54,330
we're gonna assume iid pairs s,

606
00:30:54,330 --> 00:30:56,820
a and we're gonna ig- ignore the temporal structure.

607
00:30:56,820 --> 00:31:00,000
So, we're gonna- if we're just doing supervised learning in general,

608
00:31:00,000 --> 00:31:02,640
we just imagine that we have these state-action pairs and then

609
00:31:02,640 --> 00:31:05,580
maybe we learn some classifier or,

610
00:31:05,580 --> 00:31:07,755
um, yeah, let's just say a classifier,

611
00:31:07,755 --> 00:31:10,260
to classify what action, you know, we should do.

612
00:31:10,260 --> 00:31:12,345
And it might have some sort of errors.

613
00:31:12,345 --> 00:31:13,560
It might have errors of,

614
00:31:13,560 --> 00:31:16,080
uh, well, you know, with probability epsilon.

615
00:31:16,080 --> 00:31:19,020
And so if we were thinking about doing this over the course

616
00:31:19,020 --> 00:31:21,870
of T time steps than we might have, you know, sort of,

617
00:31:21,870 --> 00:31:27,555
an expected total number of errors of epsilon times T. So,

618
00:31:27,555 --> 00:31:30,060
let's just take a second and think about what goes wrong when we're doing

619
00:31:30,060 --> 00:31:32,550
this in the supervised learning or in the- in,

620
00:31:32,550 --> 00:31:33,660
uh, in the RL context.

621
00:31:33,660 --> 00:31:35,640
So, by the RL context, I mean,

622
00:31:35,640 --> 00:31:39,030
the fact that the decisions that we make influence the next state.

623
00:31:39,030 --> 00:31:42,450
So, let's just take, like, one minute maybe talk to your neighbor and say, like,

624
00:31:42,450 --> 00:31:44,100
what do you think could be the problem with

625
00:31:44,100 --> 00:31:46,755
behavioral cloning in these sorts of scenarios.

626
00:31:46,755 --> 00:31:49,410
And if, uh, that's a simple thing to

627
00:31:49,410 --> 00:31:52,185
think about then maybe think about how you would address it.

628
00:31:52,185 --> 00:31:54,600
So, what you might do in that case if there is

629
00:31:54,600 --> 00:31:56,940
problems that happen when we try to apply standard

630
00:31:56,940 --> 00:32:02,730
supervised learning to this case where it's really underlying, uh, an MDP.

631
00:32:02,730 --> 00:33:01,500
[OVERLAPPING].

632
00:33:01,500 --> 00:33:04,200
All right. So, first of all let's just make a guess.

633
00:33:04,200 --> 00:33:06,525
I'm going to ask you guys whether you think the, um,

634
00:33:06,525 --> 00:33:08,700
the total expected errors if we're

635
00:33:08,700 --> 00:33:11,295
doing this in- in where the underlying world is an MDP,

636
00:33:11,295 --> 00:33:14,190
is gonna be greater than or less than the number of errors that

637
00:33:14,190 --> 00:33:17,310
we'd expect according to a supervised learning approach.

638
00:33:17,310 --> 00:33:21,880
Um, so who thinks that we're going to have greater expected total errors?

639
00:33:22,040 --> 00:33:25,270
Okay. Who think we're gonna have less?

640
00:33:26,210 --> 00:33:29,160
Who- many people must be confused.

641
00:33:29,160 --> 00:33:31,500
[LAUGHTER] Okay.

642
00:33:31,500 --> 00:33:34,200
So, how about somebody who thinks the answer is that we're gonna have greater.

643
00:33:34,200 --> 00:33:36,330
Maybe somebody who thinks that's the case could

644
00:33:36,330 --> 00:33:38,715
say why they think we might have more errors

645
00:33:38,715 --> 00:33:41,010
if the real world is MDP and we've tried to do

646
00:33:41,010 --> 00:33:43,545
the supervised learning technique. Yeah, and name first, please.

647
00:33:43,545 --> 00:33:46,740
My idea is that kind of, like,

648
00:33:46,740 --> 00:33:51,900
uh, as a human you're planning a more long-term horizon or, like,

649
00:33:51,900 --> 00:33:55,260
you're doing one step and then you know how that action is gonna then give you, like,

650
00:33:55,260 --> 00:33:58,500
another sequence, but since we've

651
00:33:58,500 --> 00:34:02,220
just had taken a state and action and then, like, predicting.

652
00:34:02,220 --> 00:34:04,860
Right from there we can't plan that long-term sequence,

653
00:34:04,860 --> 00:34:07,695
so it's gonna, like, compound our errors as we go.

654
00:34:07,695 --> 00:34:11,610
That's right. So, when says that, correct, we will compound those errors,

655
00:34:11,610 --> 00:34:16,050
and one of the- the challenging aspects of this is that the errors can compound a lot.

656
00:34:16,050 --> 00:34:19,290
Um, and this is because the distribution of states that

657
00:34:19,290 --> 00:34:22,770
you get can- depends on the actions that you take.

658
00:34:22,770 --> 00:34:25,050
So, if you think about this like a navigation case, like,

659
00:34:25,050 --> 00:34:27,270
if I was supposed to go out the right-hand door,

660
00:34:27,270 --> 00:34:31,620
um, and I watched go out this door and I saw that he went right.

661
00:34:31,620 --> 00:34:33,000
And I- and I, you know,

662
00:34:33,000 --> 00:34:34,590
tried to learn a supervised learning,

663
00:34:34,590 --> 00:34:36,405
uh, classifier for what I should do here.

664
00:34:36,405 --> 00:34:38,909
But my supervised learner was a little bit broken,

665
00:34:38,909 --> 00:34:42,119
and so instead of going right here, I actually went left.

666
00:34:42,120 --> 00:34:44,610
Well, now I'm in part of the room which never went

667
00:34:44,610 --> 00:34:46,605
to because he was going over there to go to the door.

668
00:34:46,605 --> 00:34:49,230
And so, like, now I have no idea what to do here.

669
00:34:49,230 --> 00:34:53,370
All right, like, now I'm in the state distribution,

670
00:34:53,370 --> 00:34:55,199
it's something that I haven't seen before,

671
00:34:55,199 --> 00:34:57,014
it's very likely that I'm going to make an error.

672
00:34:57,014 --> 00:35:02,190
In fact, my probability of error now may not be- my probability of error here is

673
00:35:02,190 --> 00:35:04,590
under-assuming the fact that the data that you get in the future

674
00:35:04,590 --> 00:35:07,740
is the same distribution as the data you got in the past.

675
00:35:07,740 --> 00:35:11,025
Our supervised learning guarantee is generally safe when we have them.

676
00:35:11,025 --> 00:35:16,905
Um, uh, that I- if- if your data is- comes from an iid distribution,

677
00:35:16,905 --> 00:35:19,515
then in the future this is what your test error will be.

678
00:35:19,515 --> 00:35:20,895
The problem is, is that,

679
00:35:20,895 --> 00:35:23,475
in reinforcement learning or markup decision processes,

680
00:35:23,475 --> 00:35:26,790
your actions depe- determine what is the data you're gonna see.

681
00:35:26,790 --> 00:35:29,775
So, the fact that instead of following the right action here,

682
00:35:29,775 --> 00:35:32,070
um, I went over here.

683
00:35:32,070 --> 00:35:35,025
And now I have no data and my data distribution is different,

684
00:35:35,025 --> 00:35:36,600
so now there's no guarantees from

685
00:35:36,600 --> 00:35:39,660
my supervised learning algorithm because my data is different.

686
00:35:39,660 --> 00:35:42,585
It's never been trained on anything like that so we can't generalize.

687
00:35:42,585 --> 00:35:44,325
So that's exactly the problem.

688
00:35:44,325 --> 00:35:47,040
Um, and this was noted by, like,

689
00:35:47,040 --> 00:35:50,430
Drew Bagnell's group from CMU in 2011 of arguing that,

690
00:35:50,430 --> 00:35:53,730
you know, this is a really big problem for what's called behavioral cloning.

691
00:35:53,730 --> 00:35:56,865
So, even though there had been some nice empirical demonstrations of it,

692
00:35:56,865 --> 00:35:59,280
uh, he and his former students,

693
00:35:59,280 --> 00:36:01,695
Stephane Ross indicated why this might fundamentally be

694
00:36:01,695 --> 00:36:04,620
a very big problem an- and sort of ill- uh,

695
00:36:04,620 --> 00:36:07,395
demonstrated some things that people had sometimes seen empirically.

696
00:36:07,395 --> 00:36:09,465
The idea is that as soon as you deviate,

697
00:36:09,465 --> 00:36:11,370
so this is the time where you make your mistake,

698
00:36:11,370 --> 00:36:14,565
then essentially the whole rest of the trajectory might all be errors.

699
00:36:14,565 --> 00:36:17,010
You might make T more- T more errors.

700
00:36:17,010 --> 00:36:21,930
Um, and so that means that the total number of errors that you make is not expected,

701
00:36:21,930 --> 00:36:26,970
uh, uh epsilon times T but it's epsilon times T squared, it's much worse.

702
00:36:26,970 --> 00:36:32,010
And it's really due to these compounding errors leading to- you to

703
00:36:32,010 --> 00:36:34,170
a place where your distribution of states

704
00:36:34,170 --> 00:36:36,570
is very different than what you have data about.

705
00:36:36,570 --> 00:36:38,430
And this issue will come up again, again,

706
00:36:38,430 --> 00:36:43,080
this sort of equiv- this thinking about what is the distribution of states that you get,

707
00:36:43,080 --> 00:36:44,970
um, under, you know, the policy you're

708
00:36:44,970 --> 00:36:46,950
following versus the policy that you want to follow.

709
00:36:46,950 --> 00:36:48,000
That issue comes up again, and

710
00:36:48,000 --> 00:36:50,190
again in- in reinforcement learning.

711
00:36:50,190 --> 00:36:55,310
So, um, it really is just a foundational issue that, you know,

712
00:36:55,310 --> 00:36:59,070
what is the data distribution you're gonna get under the policy that you've

713
00:36:59,070 --> 00:37:03,345
learned versus the true policy and looking at this mismatch.

714
00:37:03,345 --> 00:37:07,605
And so once you go off the racecourse you're not gonna have any data about that.

715
00:37:07,605 --> 00:37:09,540
So, one of the ideas, uh,

716
00:37:09,540 --> 00:37:14,175
that Drew Bagnell and his students came up with to think about this was to say,

717
00:37:14,175 --> 00:37:16,650
well, what if we could get more data?

718
00:37:16,650 --> 00:37:19,140
So, what if when I, you know,

719
00:37:19,140 --> 00:37:21,495
my- I- I only have a little amount of data to start with.

720
00:37:21,495 --> 00:37:24,645
I've learned my per- my supervised learning policy to say,

721
00:37:24,645 --> 00:37:27,060
you know, what shall I do in each state, and sometimes I make a mistake.

722
00:37:27,060 --> 00:37:29,880
So, sometimes, you know, I- I go out that way,

723
00:37:29,880 --> 00:37:32,205
my race car drives off the racecourse.

724
00:37:32,205 --> 00:37:35,910
What if I could know what to do in that, in that state?

725
00:37:35,910 --> 00:37:38,205
So, I reached a state that I don't have any data about,

726
00:37:38,205 --> 00:37:40,695
what if I could ask my expert, hey, what should I do now.

727
00:37:40,695 --> 00:37:42,315
So, like, I go over here and I'm like,

728
00:37:42,315 --> 00:37:44,550
oh my god I don't know, you know, what to do now.

729
00:37:44,550 --> 00:37:46,935
And then you ask your expert,  and they're like, oh just turn right.

730
00:37:46,935 --> 00:37:49,755
It's fine, you can still get out of the right door, it's okay.

731
00:37:49,755 --> 00:37:53,190
Um, so if you could ask your expert for labels,

732
00:37:53,190 --> 00:37:56,790
well, now you're getting labels about states that you are encountering.

733
00:37:56,790 --> 00:37:59,910
And as long as all the- as long as you have data that covers

734
00:37:59,910 --> 00:38:03,315
all the states that you're gonna potentially experience,

735
00:38:03,315 --> 00:38:05,775
then your- then your supervised learning should do pretty well.

736
00:38:05,775 --> 00:38:08,460
But the bigger issue tends to come up with the fact that you are

737
00:38:08,460 --> 00:38:12,135
encountering states that you don't have any coverage of in your training data.

738
00:38:12,135 --> 00:38:14,250
So, the idea of DAGGER which is

739
00:38:14,250 --> 00:38:18,240
a data set aggregation is essentially you just keep growing your data set.

740
00:38:18,240 --> 00:38:21,495
So, what happens in this case is you start off, you don't have any data.

741
00:38:21,495 --> 00:38:23,055
You initialize your policy.

742
00:38:23,055 --> 00:38:24,390
You follow your policy.

743
00:38:24,390 --> 00:38:27,495
So, in this case we're gonna assume that you have an expert,

744
00:38:27,495 --> 00:38:30,940
um, some expert policy.

745
00:38:32,070 --> 00:38:36,430
So, that might be, you know, an expert is taking some steps and then

746
00:38:36,430 --> 00:38:40,360
also your other policy you can- you project a trajectory.

747
00:38:40,360 --> 00:38:42,610
So, sometimes you're following one policy sentence you're at.

748
00:38:42,610 --> 00:38:47,575
And, ah, and then what you get is you get this state.

749
00:38:47,575 --> 00:38:50,470
You get to go ask your expert for every single state,

750
00:38:50,470 --> 00:38:52,640
what would you have done here?

751
00:38:53,820 --> 00:38:57,250
So for every state that you encountered in that trajectory.

752
00:38:57,250 --> 00:38:59,050
And then, you add all of

753
00:38:59,050 --> 00:39:04,525
those tuples to your dataset and you train your supervised learning policy on that.

754
00:39:04,525 --> 00:39:09,430
So, everything inside of your dataset that you're using to train your policy on,

755
00:39:09,430 --> 00:39:14,800
is with an expert label and you're slowly growing the size of that dataset.

756
00:39:14,800 --> 00:39:17,935
So, the idea is as you're getting more and more experts, ah,

757
00:39:17,935 --> 00:39:22,540
more and more labels of expert actions across the the trajectories you've actually seen.

758
00:39:22,540 --> 00:39:24,310
And there's nice formal properties for this.

759
00:39:24,310 --> 00:39:28,615
So you can be guaranteed that you will converge to a good policy, um, ah,

760
00:39:28,615 --> 00:39:30,595
by following this, um,

761
00:39:30,595 --> 00:39:33,865
under the induced state distribution. Yeah, .

762
00:39:33,865 --> 00:39:37,945
Just to confirm, this is assuming we have pi star over all space,

763
00:39:37,945 --> 00:39:40,600
like when we're doing the case where we don't have the expert to go to.

764
00:39:40,600 --> 00:39:42,190
Yeah, no this is a great question, so

765
00:39:42,190 --> 00:39:43,780
when I was looking at this just now, I would have to double-check.

766
00:39:43,780 --> 00:39:46,795
I think this is assuming your expert can give you the action there.

767
00:39:46,795 --> 00:39:50,245
It doesn't assume that you have explicit access to pi star.

768
00:39:50,245 --> 00:39:53,155
Because if you had explicit access to pi star,  you wouldn't need to learn anything.

769
00:39:53,155 --> 00:39:56,890
So, I think in this case it's like tossing a coin

770
00:39:56,890 --> 00:39:58,600
about whether the expert just directly gives you

771
00:39:58,600 --> 00:40:01,750
the action in that case or whether you follow the other policy.

772
00:40:01,750 --> 00:40:05,155
Which because you have to have the expert around all the time anyway,

773
00:40:05,155 --> 00:40:06,580
because they're always going to have to eventually

774
00:40:06,580 --> 00:40:08,095
tell you what they would have done there.

775
00:40:08,095 --> 00:40:11,965
Get- that's how you, excuse me, that dataset.

776
00:40:11,965 --> 00:40:15,040
So, that's- there was one.

777
00:40:15,040 --> 00:40:16,900
I'm and, um,

778
00:40:16,900 --> 00:40:20,140
I'm curious about how is this done efficiently?

779
00:40:20,140 --> 00:40:24,760
I can imagine that for some situations having a person set

780
00:40:24,760 --> 00:40:31,120
the command line while your, your GPU trains and inputs actions won't be efficient.

781
00:40:31,120 --> 00:40:36,895
How do people generally do this in such a way that it doesn't require manual intervention?

782
00:40:36,895 --> 00:40:40,075
Yeah, 's question is a great one which is, um, you know,

783
00:40:40,075 --> 00:40:43,000
well, this requires you have an expert either around for

784
00:40:43,000 --> 00:40:46,480
every single step or like at the end of the trajectory that can go back and label everything.

785
00:40:46,480 --> 00:40:47,815
And that's incredibly expensive.

786
00:40:47,815 --> 00:40:49,060
And if you're doing this for, you know,

787
00:40:49,060 --> 00:40:51,565
millions and millions of time steps that's completely intractable.

788
00:40:51,565 --> 00:40:53,540
I think that's why, um, this,

789
00:40:53,540 --> 00:40:58,965
this line of research has been less influential in certain ways than the,

790
00:40:58,965 --> 00:41:01,560
some of the other techniques that we're going to see next in terms

791
00:41:01,560 --> 00:41:04,635
of how you do sort of inverse RL.

792
00:41:04,635 --> 00:41:06,630
So, what this is really assuming is

793
00:41:06,630 --> 00:41:09,490
that you have this human in the loop that's really in the loop,

794
00:41:09,490 --> 00:41:11,500
um, ah, as opposed to just asking them to

795
00:41:11,500 --> 00:41:14,335
provide demonstrations once and then your algorithm goes off.

796
00:41:14,335 --> 00:41:18,625
And I think that practically in most cases it's much more realistic to say,

797
00:41:18,625 --> 00:41:20,920
you know, drive the car around the block 10 times,

798
00:41:20,920 --> 00:41:23,170
but then you can leave and then we'll do all of our RL

799
00:41:23,170 --> 00:41:25,900
versus saying I need you to be in the car or,

800
00:41:25,900 --> 00:41:27,820
you know, like, label all of the trajectories

801
00:41:27,820 --> 00:41:29,935
that the car is doing and keep telling you whether it's right or wrong.

802
00:41:29,935 --> 00:41:32,380
I think this is just very label-intensive. It's very expensive.

803
00:41:32,380 --> 00:41:34,000
So I think that, um,

804
00:41:34,000 --> 00:41:35,350
in some limited cases,

805
00:41:35,350 --> 00:41:38,680
like, if your action rate is very slow,

806
00:41:38,680 --> 00:41:41,110
like, if your action rate is, you know,

807
00:41:41,110 --> 00:41:43,750
making decisions in the military or you know, others that are at

808
00:41:43,750 --> 00:41:46,450
a very high level, with very sparse decisions, this can be very reasonable.

809
00:41:46,450 --> 00:41:47,845
Because you could basically throw, you know,

810
00:41:47,845 --> 00:41:51,700
infinite compute at it before, between each decision-making.

811
00:41:51,700 --> 00:41:56,140
If you're doing this for sort of real-time hertz-level decisions,

812
00:41:56,140 --> 00:42:00,400
I think that's very hard. Okay, yeah, .

813
00:42:00,400 --> 00:42:04,060
Will this be compatible with like an expert taking over the system.

814
00:42:04,060 --> 00:42:08,380
Right, like, somebody sitting behind the wheel letting an agent drive and then,

815
00:42:08,380 --> 00:42:12,760
uh, like, recognizing that there's an emergency situation coming up and taking the wheel.

816
00:42:12,760 --> 00:42:15,115
Yeah so, like, um, so what said,

817
00:42:15,115 --> 00:42:17,905
is this compatible with sort of a- an expert taking over?

818
00:42:17,905 --> 00:42:19,465
Yes. I mean I think.

819
00:42:19,465 --> 00:42:21,430
And that might be an easier way to get labels.

820
00:42:21,430 --> 00:42:23,200
So you might say if you have an expert there,

821
00:42:23,200 --> 00:42:25,990
every action that's taken that's the same as the action the expert would take.

822
00:42:25,990 --> 00:42:27,910
Maybe they don't intervene. Otherwise, they only provide

823
00:42:27,910 --> 00:42:30,610
labels or interventions when it would differ.

824
00:42:30,610 --> 00:42:33,385
But it still requires like an expert to be monitoring,

825
00:42:33,385 --> 00:42:40,130
which is often still mentally challenging essentially. You know, it's still high-cost, okay.

826
00:42:40,920 --> 00:42:43,435
All right. So that- that is a nice.

827
00:42:43,435 --> 00:42:46,869
I mean there- it's very nice to see sort of the formal characterization

828
00:42:46,869 --> 00:42:50,500
of why behavioral cloning can be bad and what is the reason for this.

829
00:42:50,500 --> 00:42:55,510
Um, and I think that DAGGER is can be very useful in certain circumstances,

830
00:42:55,510 --> 00:42:57,700
but there's a lot of cases where just practically it's

831
00:42:57,700 --> 00:43:00,145
much easier to get a- example demonstrations,

832
00:43:00,145 --> 00:43:03,175
um, and then assume that there's no longer a human in the loop.

833
00:43:03,175 --> 00:43:04,600
All right.

834
00:43:04,600 --> 00:43:08,230
So inverse RL is more of one of the- the second categories.

835
00:43:08,230 --> 00:43:10,540
So, what does- what happens in inverse RL?

836
00:43:10,540 --> 00:43:14,365
Well, first let's just think about you know, feature-based reward function.

837
00:43:14,365 --> 00:43:18,145
So, well okay, we'll get to there in a second.

838
00:43:18,145 --> 00:43:19,930
So again, we're thinking about this case where we

839
00:43:19,930 --> 00:43:22,495
have some transition model that we might not observe.

840
00:43:22,495 --> 00:43:24,430
Um, a- or maybe we're doing.

841
00:43:24,430 --> 00:43:26,005
A lot of the techniques here,

842
00:43:26,005 --> 00:43:29,110
to start with, they didn't assume that you knew the transition model.

843
00:43:29,110 --> 00:43:31,780
[NOISE] That's pretty strong for a lot of real-world domains,

844
00:43:31,780 --> 00:43:33,550
but in some cases that's reasonable.

845
00:43:33,550 --> 00:43:35,170
So, for right now, we're going to assume that

846
00:43:35,170 --> 00:43:37,675
the only thing that we don't know is the reward function.

847
00:43:37,675 --> 00:43:40,795
There's some extensions to when you don't know the transition model too.

848
00:43:40,795 --> 00:43:44,950
Okay. So, then we have again our set of demonstrations and the goal

849
00:43:44,950 --> 00:43:49,600
now is not to directly learn a policy but just to infer the reward function.

850
00:43:49,600 --> 00:43:54,955
So, if I don't tell you anything about the optimality of the teacher's policy,

851
00:43:54,955 --> 00:43:57,475
what can we infer about the reward function?

852
00:43:57,475 --> 00:44:01,910
Like let's just say, like its not an expert, it's just demonstrations.

853
00:44:02,220 --> 00:44:05,740
If you get a demonstration of a state,

854
00:44:05,740 --> 00:44:07,690
action state et cetera.

855
00:44:07,690 --> 00:44:10,240
Can you infer anything about R?

856
00:44:10,240 --> 00:44:12,310
If you don't know anything about the optimality?

857
00:44:12,310 --> 00:44:23,980
[NOISE]

858
00:44:23,980 --> 00:44:24,805
Like, I mean,

859
00:44:24,805 --> 00:44:26,290
would it be the same as samples,

860
00:44:26,290 --> 00:44:30,130
like as we get more demonstrations, R will approach like, R star, I guess.

861
00:44:30,130 --> 00:44:33,565
This is assuming no assumptions about optimality.

862
00:44:33,565 --> 00:44:38,080
So, if I don't tell you anything about the optimality of the policy you're seeing,

863
00:44:38,080 --> 00:44:42,250
is there any or any information you can gather in that case?

864
00:44:42,250 --> 00:44:45,940
I'd be able to say that the choice that the teacher made under

865
00:44:45,940 --> 00:44:48,850
their policy just wrapped hot air under

866
00:44:48,850 --> 00:44:52,255
the reward each other function that the alternatives.

867
00:44:52,255 --> 00:44:56,665
So is saying, for that particular person you could say something about,

868
00:44:56,665 --> 00:44:59,365
um, for their reward function,

869
00:44:59,365 --> 00:45:00,880
assuming that they're a rational agent,

870
00:45:00,880 --> 00:45:03,055
that that- that was, um, higher in their own function.

871
00:45:03,055 --> 00:45:07,585
That's true. But if, if you wanted it to be about the general word function, um,

872
00:45:07,585 --> 00:45:11,140
this would tell you maybe I'm understating it so that it seems a little bit

873
00:45:11,140 --> 00:45:14,725
more subtle than I mean it to be.

874
00:45:14,725 --> 00:45:16,060
It doesn't tell you anything, right.

875
00:45:16,060 --> 00:45:18,340
Like if you- if I- if you see me like

876
00:45:18,340 --> 00:45:21,400
wandering around and like if you see an agent flailing around,

877
00:45:21,400 --> 00:45:23,770
right, and and you know nothing about whether it's making

878
00:45:23,770 --> 00:45:26,995
good decisions or not with respect to the true reward function.

879
00:45:26,995 --> 00:45:28,795
Demonstrations don't tell you anything,

880
00:45:28,795 --> 00:45:31,120
like they don't give you any information about the reward function

881
00:45:31,120 --> 00:45:33,670
unless you know something about, um.

882
00:45:33,670 --> 00:45:36,070
Unless you're trying to make an assumption that the agent is

883
00:45:36,070 --> 00:45:39,595
acting rationally with respect to the true reward function.

884
00:45:39,595 --> 00:45:43,120
Or maybe you get some information about their internal one like what was saying.

885
00:45:43,120 --> 00:45:48,100
But in general, if we don't make any assumptions about agent behavior and we don't, um,

886
00:45:48,100 --> 00:45:50,050
and we don't assume that they're doing

887
00:45:50,050 --> 00:45:52,330
anything optimal with respect to the global reward function,

888
00:45:52,330 --> 00:45:54,430
there's no information you can get.

889
00:45:54,430 --> 00:45:56,530
Now, the more challenging one is the next.

890
00:45:56,530 --> 00:45:59,980
So, let's assume that the teacher's policy is optimal with respect to

891
00:45:59,980 --> 00:46:02,110
the true global reward function which

892
00:46:02,110 --> 00:46:04,675
the agent is also maybe going to want to optimize in the future.

893
00:46:04,675 --> 00:46:08,995
So, you get expert driver perfo- driver, um, driving around.

894
00:46:08,995 --> 00:46:13,045
Um, this is like,

895
00:46:13,045 --> 00:46:18,520
think for a second about what you can infer about the reward function in this case.

896
00:46:18,520 --> 00:46:20,560
Um, and whether in particular there's

897
00:46:20,560 --> 00:46:25,105
more than one reward function that could explain their behavior.

898
00:46:25,105 --> 00:46:26,980
So think about whether it's unique.

899
00:46:26,980 --> 00:46:29,155
So let's say, imagine data has no issue.

900
00:46:29,155 --> 00:46:31,750
I give you 10 trillion examples of-

901
00:46:31,750 --> 00:46:36,355
of the agent following the optimal policy. 10 trillion examples.

902
00:46:36,355 --> 00:46:39,370
Um, and you want to see if

903
00:46:39,370 --> 00:46:42,730
you could learn what the reward function is and the question is, um,

904
00:46:42,730 --> 00:46:49,375
is there a single reward function that is consistent with their data or are there many?

905
00:46:49,375 --> 00:46:52,840
Maybe take a second to talk to somebody around you, um,

906
00:46:52,840 --> 00:46:55,075
just to see whether there's,

907
00:46:55,075 --> 00:46:57,070
the question really is there one reward function?

908
00:46:57,070 --> 00:46:59,005
Is there a unique reward function?

909
00:46:59,005 --> 00:47:03,370
If you have infinite data so it's not a data sparsity issue, um, versus money.

910
00:47:03,370 --> 00:48:01,210
[OVERLAPPING]

911
00:48:01,210 --> 00:48:04,135
All right, I'm going to do a quick poll.

912
00:48:04,135 --> 00:48:05,770
Okay, we're going to- I'm going to poll you guys

913
00:48:05,770 --> 00:48:07,300
then I'm going to ask whether you think there

914
00:48:07,300 --> 00:48:10,855
is one reward function or if there's more than one reward function.

915
00:48:10,855 --> 00:48:13,465
So, who thinks there's a single reward function?

916
00:48:13,465 --> 00:48:16,555
Infinite data, single reward function that's consistent.

917
00:48:16,555 --> 00:48:18,970
Who thinks there's more than one reward function?

918
00:48:18,970 --> 00:48:22,585
Okay. Could someone give me a reward function that is

919
00:48:22,585 --> 00:48:27,110
always consistent with any optimal policy?

920
00:48:30,640 --> 00:48:38,770
I guess that's what we need to do is in the first step just get the older apart that

921
00:48:38,770 --> 00:48:45,310
the feature is going to have as a policy and then just be random after that.

922
00:48:45,310 --> 00:48:47,470
is saying maybe on the first step,

923
00:48:47,470 --> 00:48:49,450
that you could maybe give most of the reward that, like,

924
00:48:49,450 --> 00:48:52,030
the agent would experience and then be random after that.

925
00:48:52,030 --> 00:48:54,040
So, and that might depend on the state.

926
00:48:54,040 --> 00:48:56,935
I guess I was thinking of, if- can anybody tell me like,

927
00:48:56,935 --> 00:49:00,820
um, a number which would allow or, you know,

928
00:49:00,820 --> 00:49:04,585
specification of a reward function like a constant,

929
00:49:04,585 --> 00:49:10,779
like a choice of a constant which would make any policy optimal. Yeah,

930
00:49:10,779 --> 00:49:14,140
. You just give a reward of zero for every action.

931
00:49:14,140 --> 00:49:16,090
Sorry, yes. So, what says is exactly correct.

932
00:49:16,090 --> 00:49:17,950
If you give a reward of zero, um,

933
00:49:17,950 --> 00:49:20,530
any policy is optimal in the respect that you'd never get any reward.

934
00:49:20,530 --> 00:49:22,255
Anywhere, it's a sad life unfortunately.

935
00:49:22,255 --> 00:49:23,965
And, um, uh, in this case,

936
00:49:23,965 --> 00:49:25,255
all policies are optimal.

937
00:49:25,255 --> 00:49:29,830
Right. So, uh, so if you just observe trajectories,

938
00:49:29,830 --> 00:49:35,965
then one reward function for which that policy is optimal at zero but it's not unique.

939
00:49:35,965 --> 00:49:40,440
So, um, this issue was observed,

940
00:49:40,440 --> 00:49:47,120
I think it was by Andrew Ng and Stuart Russell back in 2000.

941
00:49:47,120 --> 00:49:52,300
Right there is a paper talking about inverse RL where they noted this issue.

942
00:49:52,300 --> 00:49:54,340
The problem is that this is, uh,

943
00:49:54,340 --> 00:49:59,275
not unique without further assumptions, there are many reward functions that are consistent.

944
00:49:59,275 --> 00:50:03,310
Um, so, we have

945
00:50:03,310 --> 00:50:06,040
to- we're gonna have to think about

946
00:50:06,040 --> 00:50:09,475
how do we break ties and how do we impose additional structure. Yeah in the back.

947
00:50:09,475 --> 00:50:11,920
If you have a consistent reward function,

948
00:50:11,920 --> 00:50:15,220
for instance, if you add a constant and what are the rewards also?

949
00:50:15,220 --> 00:50:19,375
They're loss, oh, remind me of your name.

950
00:50:19,375 --> 00:50:19,945
.

951
00:50:19,945 --> 00:50:21,535
. So, yeah what said is,

952
00:50:21,535 --> 00:50:23,710
there's, there's many many reward functions.

953
00:50:23,710 --> 00:50:26,410
So, if you have, um, a constant, er,

954
00:50:26,410 --> 00:50:28,015
everything has the same, uh,

955
00:50:28,015 --> 00:50:31,855
any constant also be identical.

956
00:50:31,855 --> 00:50:35,650
So, um, there's generally many different,

957
00:50:35,650 --> 00:50:38,245
um, reward functions that would all give you.

958
00:50:38,245 --> 00:50:42,275
There are many different reward functions for which any policy is optimal.

959
00:50:42,275 --> 00:50:44,400
Instead, that would mean that if you're trying to

960
00:50:44,400 --> 00:50:46,650
infer what function given some data there are

961
00:50:46,650 --> 00:50:48,690
many reward functions that you can write down so that

962
00:50:48,690 --> 00:50:51,390
that data would be optimal with respect to the reward function.

963
00:50:51,390 --> 00:50:53,730
[NOISE] And, and that second part is really what we're trying to

964
00:50:53,730 --> 00:50:56,295
get as we're trying to sort of, uh,

965
00:50:56,295 --> 00:51:02,470
infer what reward function would make this data look like it's,

966
00:51:02,470 --> 00:51:04,870
um, coming from an optimal policy,

967
00:51:04,870 --> 00:51:07,940
if we assume that the expert is optimal.

968
00:51:08,310 --> 00:51:11,425
So, let's think about also how we do this in,

969
00:51:11,425 --> 00:51:12,805
um, enlarged state spaces.

970
00:51:12,805 --> 00:51:16,675
So, we're gonna think about linear value function approximators, um, because, again,

971
00:51:16,675 --> 00:51:19,300
often the places where we particularly need to be sample efficient is

972
00:51:19,300 --> 00:51:22,300
when our state space is huge and we're not gonna be able to explore it efficiently.

973
00:51:22,300 --> 00:51:24,520
So, let's think about linear value function approximator.

974
00:51:24,520 --> 00:51:29,395
And we're gonna think of this reward also as being linear over the features.

975
00:51:29,395 --> 00:51:32,380
So, our reward function might be

976
00:51:32,380 --> 00:51:36,600
some weights times some featurized representation of our state space.

977
00:51:36,600 --> 00:51:41,725
And the goal is to compute a good set of weights given our demonstration.

978
00:51:41,725 --> 00:51:43,495
I already said that in general,

979
00:51:43,495 --> 00:51:45,220
this is not unique but, um,

980
00:51:45,220 --> 00:51:47,275
we're gonna try to figure out ways to do this in,

981
00:51:47,275 --> 00:51:49,345
in different, um, methods.

982
00:51:49,345 --> 00:51:54,790
So, the value function for a policy Pi can be expressed as the follows- following,

983
00:51:54,790 --> 00:51:56,335
and you just write it down as,

984
00:51:56,335 --> 00:51:59,845
uh, the expected value of the discounted sum of rewards,

985
00:51:59,845 --> 00:52:04,370
and this is the states that you would reach under that policy.

986
00:52:04,410 --> 00:52:07,060
Under the distribution of states that you

987
00:52:07,060 --> 00:52:09,595
get to under this policy, these are their words.

988
00:52:09,595 --> 00:52:12,835
So, now what we're gonna do is re-express this.

989
00:52:12,835 --> 00:52:16,210
So, what we're doing now is that this is gonna

990
00:52:16,210 --> 00:52:23,560
be- we're assuming a linear representation of our reward function.

991
00:52:23,560 --> 00:52:28,430
So, we can write or we can re-express it like this.

992
00:52:28,650 --> 00:52:31,915
So, we can write it down in terms of

993
00:52:31,915 --> 00:52:36,160
the features of the state we reach at each time step times the weight.

994
00:52:36,160 --> 00:52:40,960
And then because the weight vector is constant for everything, you can just move it out.

995
00:52:40,960 --> 00:52:44,770
And then you get this interesting expression which is you basically just have

996
00:52:44,770 --> 00:52:51,210
this discounted sum of the state features that you encounter. And we're gonna call that Mu.

997
00:52:51,210 --> 00:52:54,390
So we talked about this very briefly earlier, but, um,

998
00:52:54,390 --> 00:52:56,100
now we're talking about Mu as being sort of

999
00:52:56,100 --> 00:52:59,645
the discounted weighted frequency of state features under our policy.

1000
00:52:59,645 --> 00:53:01,810
How much time do you spend, um,

1001
00:53:01,810 --> 00:53:04,285
uh, in different features, um,

1002
00:53:04,285 --> 00:53:06,190
or you know that basically how much time you spend in

1003
00:53:06,190 --> 00:53:08,440
different states, sort of a featurized version of that,

1004
00:53:08,440 --> 00:53:11,020
um, discounted by kind of when you reach those because

1005
00:53:11,020 --> 00:53:14,065
some states you might reach really far in the future versus now.

1006
00:53:14,065 --> 00:53:18,520
So, it's related to the sort of stationary distributions we were talking about before,

1007
00:53:18,520 --> 00:53:20,780
but now we're using discounting.

1008
00:53:22,170 --> 00:53:24,820
So, why is this good?

1009
00:53:24,820 --> 00:53:27,820
Okay. So what, er, we're gonna say now is

1010
00:53:27,820 --> 00:53:31,150
that instead of thinking directly about reward functions,

1011
00:53:31,150 --> 00:53:35,050
um, then we can start to think about distributions of states.

1012
00:53:35,050 --> 00:53:38,410
Um, and think about sort of the probability of,

1013
00:53:38,410 --> 00:53:40,675
of reaching different distributions of states,

1014
00:53:40,675 --> 00:53:42,790
different state distributions, um,

1015
00:53:42,790 --> 00:53:47,155
as representing different, uh, different policies essentially.

1016
00:53:47,155 --> 00:53:49,855
Different policies, um, for a particular reward function,

1017
00:53:49,855 --> 00:53:53,125
um, would reach different distributions of states.

1018
00:53:53,125 --> 00:53:57,160
So, we can think about using this formulation for apprenticeship learning.

1019
00:53:57,160 --> 00:53:59,575
So, in this case,

1020
00:53:59,575 --> 00:54:02,695
we have this nice setting for apprenticeship learning.

1021
00:54:02,695 --> 00:54:06,250
Right now, um, we're using the linear value function approximation we call

1022
00:54:06,250 --> 00:54:08,005
it apprenticeship learning because we're learning like

1023
00:54:08,005 --> 00:54:10,030
the agent is being an apprenticeship from,

1024
00:54:10,030 --> 00:54:12,700
uh, from the, from the demonstrator.

1025
00:54:12,700 --> 00:54:16,720
So, now we have this discounted weighted frequency of the feature states.

1026
00:54:16,720 --> 00:54:20,440
So, we're always sort of moving into the feature space of states now.

1027
00:54:20,440 --> 00:54:23,110
Um, and then we wanna note the following.

1028
00:54:23,110 --> 00:54:30,025
So, if we define what is the value function for Pi star,

1029
00:54:30,025 --> 00:54:34,240
that's just equal to the expected discounted sum of rewards we reach.

1030
00:54:34,240 --> 00:54:39,295
And by definition, that is better than the value for any other policy.

1031
00:54:39,295 --> 00:54:41,920
At least as good because either Pi is the same as

1032
00:54:41,920 --> 00:54:44,365
Pi is the same as the optimal policy or it's different,

1033
00:54:44,365 --> 00:54:48,685
and this is just equal to the same thing, um,

1034
00:54:48,685 --> 00:54:50,995
same reward function which we don't know,

1035
00:54:50,995 --> 00:54:53,335
but under a different distribution of states.

1036
00:54:53,335 --> 00:54:57,980
It's under the distribution of states you'd get to if you follow this alternative policy.

1037
00:54:57,990 --> 00:55:03,280
And so, if we think that the expert's demonstrations are from the optimal policy,

1038
00:55:03,280 --> 00:55:04,810
in order to identify W,

1039
00:55:04,810 --> 00:55:07,765
it's sufficient to find the W star such that,

1040
00:55:07,765 --> 00:55:10,240
if you dot-product that with

1041
00:55:10,240 --> 00:55:14,230
the distribution of states you got to under the optimal policy.

1042
00:55:14,230 --> 00:55:15,745
Remember, this is what we know,

1043
00:55:15,745 --> 00:55:18,685
these are- we can get this from our demonstrations.

1044
00:55:18,685 --> 00:55:23,890
This has to look better than the distri- than

1045
00:55:23,890 --> 00:55:26,380
the same weight vector times the distribution

1046
00:55:26,380 --> 00:55:30,020
of states you get to under any other policy.

1047
00:55:31,470 --> 00:55:33,790
Are there questions about that?

1048
00:55:33,790 --> 00:55:38,110
So, it's by making this observation that the value of the optimal policy is directly

1049
00:55:38,110 --> 00:55:42,520
related to the distribution of states you get under it times this weight vector.

1050
00:55:42,520 --> 00:55:47,065
And that value has to be higher than the value of, uh,

1051
00:55:47,065 --> 00:55:50,140
any other policy which is using that same weight vector and this gives you

1052
00:55:50,140 --> 00:55:54,010
a different distribution of states. Yes, .

1053
00:55:54,010 --> 00:55:56,860
[inaudible]

1054
00:55:56,860 --> 00:56:11,050
U in this case is sort of like the stationary distribution of

1055
00:56:11,050 --> 00:56:14,080
the proportions of its refugees state gave the policy.

1056
00:56:14,080 --> 00:56:15,160
[NOISE]

1057
00:56:15,160 --> 00:56:17,410
Yeah. Especially in serv- in terms of conceptualizing Mu.

1058
00:56:17,410 --> 00:56:19,570
What's the sort of a good way to think about it is it should we think

1059
00:56:19,570 --> 00:56:21,880
of it as like the stationary distribution of states?

1060
00:56:21,880 --> 00:56:23,500
Yeah. I think it's reasonable as everything that is

1061
00:56:23,500 --> 00:56:25,090
essentially the stationary distribution of

1062
00:56:25,090 --> 00:56:29,150
states weighted with this discount factor on top of it.

1063
00:56:29,820 --> 00:56:35,300
So, it's very similar to the stationary distributions we saw before.

1064
00:56:40,670 --> 00:56:43,020
All right. So, essentially it's the same,

1065
00:56:43,020 --> 00:56:44,730
we want to find a reward function so that

1066
00:56:44,730 --> 00:56:46,830
the expert policy and the distribution of states

1067
00:56:46,830 --> 00:56:51,015
you reach under it looks better when you compute the value function compared to,

1068
00:56:51,015 --> 00:56:55,170
um, that same weight vector under any other distribution of states.

1069
00:56:55,170 --> 00:57:00,390
Um, and so if we can find a policy so

1070
00:57:00,390 --> 00:57:05,805
that its distribution of states matches the distribution of states of our expert,

1071
00:57:05,805 --> 00:57:07,725
then we're gonna do pretty well.

1072
00:57:07,725 --> 00:57:10,860
So, what this says here is that if we have

1073
00:57:10,860 --> 00:57:14,250
a policy so the dis-discounted distribution of states that we

1074
00:57:14,250 --> 00:57:16,770
reach under it is close to

1075
00:57:16,770 --> 00:57:21,430
the distribution of states that you got from your demonstration, since it's the expert.

1076
00:57:21,950 --> 00:57:30,010
So, if that's small then your value function error will also be small.

1077
00:57:31,520 --> 00:57:37,140
So, for, for any w if you can- if you can basically match features,

1078
00:57:37,140 --> 00:57:41,670
match feature- expected features or matched distributions of states, then you're good.

1079
00:57:41,670 --> 00:57:44,190
Then, then you found, um, er,

1080
00:57:44,190 --> 00:57:49,240
found a value that's going to have a very similar value to the true value, okay.

1081
00:57:50,110 --> 00:57:54,020
So, this actually means for any w_t here.

1082
00:57:54,020 --> 00:57:56,960
So, it means no matter what the true reward function is,

1083
00:57:56,960 --> 00:57:59,645
if you can find a policy so that your, uh,

1084
00:57:59,645 --> 00:58:03,180
state features match, then no matter what

1085
00:58:03,180 --> 00:58:05,160
the true reward function is you know that you're going to be

1086
00:58:05,160 --> 00:58:07,845
close to the true value. Yeah, .

1087
00:58:07,845 --> 00:58:12,300
So, this, uh, this w that we will- we will

1088
00:58:12,300 --> 00:58:16,905
be finding would be used to calculate I guess, an expectation.

1089
00:58:16,905 --> 00:58:18,735
What your, I guess,

1090
00:58:18,735 --> 00:58:21,915
values or state. Right?

1091
00:58:21,915 --> 00:58:26,475
Yes. You could- once you have a w you combine that with your mu's to compute,

1092
00:58:26,475 --> 00:58:29,715
like, a value of a state or you can sum over it.

1093
00:58:29,715 --> 00:58:31,725
How do you, uh, I guess,

1094
00:58:31,725 --> 00:58:33,090
that translates directly to,

1095
00:58:33,090 --> 00:58:37,170
being able to use that to make decisions when they're out of state?

1096
00:58:37,170 --> 00:58:40,050
So, I think, I think question is about saying, like, okay,

1097
00:58:40,050 --> 00:58:41,640
so if we're getting these w's or,

1098
00:58:41,640 --> 00:58:42,690
sort of, what are we solving for?

1099
00:58:42,690 --> 00:58:43,800
Are we solving for the policy,

1100
00:58:43,800 --> 00:58:45,390
are we solving for w et cetera.

1101
00:58:45,390 --> 00:58:46,695
In this case, I think, uh,

1102
00:58:46,695 --> 00:58:48,675
a reasonable way to think about it is,

1103
00:58:48,675 --> 00:58:51,255
um, solving for w if it's solving for pi.

1104
00:58:51,255 --> 00:58:55,035
So, what this is saying is that let's say you're optimizing over pi.

1105
00:58:55,035 --> 00:58:57,105
If you found a pi,

1106
00:58:57,105 --> 00:59:00,315
so right now we know the transition model which is not always true,

1107
00:59:00,315 --> 00:59:02,055
but if you know the transition model,

1108
00:59:02,055 --> 00:59:06,975
for a given pi you can compute mu because it's just following,

1109
00:59:06,975 --> 00:59:09,105
like, you could do Monte Carlo roll outs for example.

1110
00:59:09,105 --> 00:59:12,360
So if you- someone's given you a pi and they tell you the transition model,

1111
00:59:12,360 --> 00:59:15,060
you can roll that out and you can estimate mu of pi.

1112
00:59:15,060 --> 00:59:16,830
Then it's saying that if you do that,

1113
00:59:16,830 --> 00:59:18,555
so let's say I have some policy,

1114
00:59:18,555 --> 00:59:20,160
I roll this out a bunch of times,

1115
00:59:20,160 --> 00:59:22,590
I estimate my mu and I check whether that seems to

1116
00:59:22,590 --> 00:59:25,185
be close to my mu of my demonstration policy.

1117
00:59:25,185 --> 00:59:28,020
If that's small, this is saying no matter what

1118
00:59:28,020 --> 00:59:32,085
the real reward function is you're gonna have the same value as,

1119
00:59:32,085 --> 00:59:33,615
like, uh, what you're,

1120
00:59:33,615 --> 00:59:34,920
like, like, you've matched,

1121
00:59:34,920 --> 00:59:38,010
um, uh, the value that you would get under the expert policy.

1122
00:59:38,010 --> 00:59:40,320
So, you're good. You can just use this policy to act.

1123
00:59:40,320 --> 00:59:45,315
[NOISE] Yeah.

1124
00:59:45,315 --> 00:59:50,760
. And I'm looking at the constraint on the [inaudible] for w,

1125
00:59:50,760 --> 00:59:54,315
and I, I don't quite see where that comes from.

1126
00:59:54,315 --> 00:59:57,990
I'm curious since I'm missing it here or we haven't gone over it.

1127
00:59:57,990 --> 01:00:02,025
's question is about why we have this constraint over w. Um,

1128
01:00:02,025 --> 01:00:05,430
I- my- I would have to double-check the details to

1129
01:00:05,430 --> 01:00:08,850
be careful about this but I'm pretty sure it's there, so that,

1130
01:00:08,850 --> 01:00:12,120
um, as we do these backups when we do this approximation,

1131
01:00:12,120 --> 01:00:16,365
that, um, your errors are all bounded [NOISE] so that things don't explode.

1132
01:00:16,365 --> 01:00:22,770
Um, and that when you do this proof that- I think I'd have to double-check it,

1133
01:00:22,770 --> 01:00:26,280
but uh, but I think you basically use Holder's inequality and then you use the fact

1134
01:00:26,280 --> 01:00:30,540
that the w is bounded to ensure that your ultimate value is bounded.

1135
01:00:30,540 --> 01:00:37,590
So, you can check that. [NOISE] In general,

1136
01:00:37,590 --> 01:00:40,680
you want your, your reward function to be bounded, um,

1137
01:00:40,680 --> 01:00:45,045
particularly the- in the- even with just counting, like, it's useful.

1138
01:00:45,045 --> 01:00:46,350
You always need to make sure that

1139
01:00:46,350 --> 01:00:49,350
your Bellman operator's like a contraction to have a hope of- I mean,

1140
01:00:49,350 --> 01:00:52,125
we've already talked about the fact that with linear value function approximated,

1141
01:00:52,125 --> 01:00:54,690
you don't always converge, um, uh,

1142
01:00:54,690 --> 01:01:00,630
but if your rewards are unbounded it gets worse. Yeah, .

1143
01:01:00,630 --> 01:01:04,290
Um, I'm trying to fit this into uh,

1144
01:01:04,290 --> 01:01:05,820
other things I'm familiar with, is this basically, like,

1145
01:01:05,820 --> 01:01:09,120
a maximum likelihood way of looking at the policy, right?

1146
01:01:09,120 --> 01:01:13,920
Like, if we flipped a coin 100 times and we got 99 heads and 1 tails,

1147
01:01:13,920 --> 01:01:15,510
it's possible that came from,

1148
01:01:15,510 --> 01:01:16,755
uh, a fair coin.

1149
01:01:16,755 --> 01:01:18,495
You know that uh,

1150
01:01:18,495 --> 01:01:20,550
we can't discount that but it's unlikely, right?

1151
01:01:20,550 --> 01:01:25,950
So, if we observe some expert agent doing the same thing 100 times

1152
01:01:25,950 --> 01:01:27,960
that could come from a reward function that's zero

1153
01:01:27,960 --> 01:01:31,530
everywhere but not as likely as some other reward function? Does that-

1154
01:01:31,530 --> 01:01:33,000
Oh, great question. So is asking,

1155
01:01:33,000 --> 01:01:34,380
like, so is this, sort of,

1156
01:01:34,380 --> 01:01:37,890
giving us some way to deal with the fact that the reward could be zero.

1157
01:01:37,890 --> 01:01:40,845
That we have this, sort of, unidentifiability problem.

1158
01:01:40,845 --> 01:01:43,440
This does not handle that unfortunately.

1159
01:01:43,440 --> 01:01:47,700
So, um, er, this is still not guaranteeing that we couldn't,

1160
01:01:47,700 --> 01:01:50,100
uh, learn a weight if it's zero everywhere,

1161
01:01:50,100 --> 01:01:52,125
but what this is saying here is that, um,

1162
01:01:52,125 --> 01:01:55,545
instead of thinking about trying to learn the reward function directly,

1163
01:01:55,545 --> 01:01:58,815
if you match expected state features,

1164
01:01:58,815 --> 01:02:00,810
um, then that's another way to

1165
01:02:00,810 --> 01:02:03,705
guarantee that your policy is basically doing the same thing as the expert.

1166
01:02:03,705 --> 01:02:06,750
So, if you have a policy that basically it looks like- that

1167
01:02:06,750 --> 01:02:10,185
visits the same states in exactly the same frequency as what the expert does,

1168
01:02:10,185 --> 01:02:11,790
then you've matched their policy.

1169
01:02:11,790 --> 01:02:14,835
And you still don't necessarily know that the w you've got is

1170
01:02:14,835 --> 01:02:17,850
accurate or is a good estimate of the reward function but maybe you

1171
01:02:17,850 --> 01:02:20,760
don't need it because if you really just care about being able to

1172
01:02:20,760 --> 01:02:24,105
match the expert's policy then you've matched it.

1173
01:02:24,105 --> 01:02:25,830
Because if you're- if you visit

1174
01:02:25,830 --> 01:02:28,875
all the states with exactly the same frequency as what the expert does,

1175
01:02:28,875 --> 01:02:30,915
you have identical policies.

1176
01:02:30,915 --> 01:02:32,805
So, it's, sort of, giving up on it.

1177
01:02:32,805 --> 01:02:35,550
It's saying, well, we still don't know what the real reward function is but it

1178
01:02:35,550 --> 01:02:38,760
doesn't matter because we've uncovered the expert policy. .

1179
01:02:38,760 --> 01:02:46,335
Um, is there, like, a, um, nonlinear analog to this that might be more effective?

1180
01:02:46,335 --> 01:02:48,540
Great question, yes. So, there's been a lot of

1181
01:02:48,540 --> 01:02:50,400
work also on doing this with deep neural networks.

1182
01:02:50,400 --> 01:02:54,280
I'll give a couple of pointers later to sort of, uh, other approaches.

1183
01:02:54,380 --> 01:03:00,540
Okay. So, this sort of observation led to an algorithm for learning the policy which is,

1184
01:03:00,540 --> 01:03:04,455
um, uh, you try to find some sort of reward function.

1185
01:03:04,455 --> 01:03:07,590
Like, that means, you know, a w and choice of w,

1186
01:03:07,590 --> 01:03:12,165
um, such that the teacher looks better than everything else.

1187
01:03:12,165 --> 01:03:15,045
Looks better than all the other controllers you've got before.

1188
01:03:15,045 --> 01:03:16,995
So, it makes it look like,

1189
01:03:16,995 --> 01:03:18,960
sort of, the, um,

1190
01:03:18,960 --> 01:03:26,130
this w for this state distribution looks better than w for all other distributions.

1191
01:03:26,130 --> 01:03:30,120
[NOISE] And then you find the optimal control policy for

1192
01:03:30,120 --> 01:03:35,820
the current w which can allow you to then get new,

1193
01:03:35,820 --> 01:03:39,630
uh, mu's because you have your transition model here.

1194
01:03:39,630 --> 01:03:42,480
And you repeat this until,

1195
01:03:42,480 --> 01:03:45,310
sort of, the gap is sufficiently small.

1196
01:03:46,340 --> 01:03:49,605
Now, this is not perfect.

1197
01:03:49,605 --> 01:03:53,610
Um, if, if your expert policy is sub-optimal,

1198
01:03:53,610 --> 01:03:55,785
it's a little tricky how to combine these.

1199
01:03:55,785 --> 01:03:59,130
Um, I don't want to dwell too much on this particular algorithm,

1200
01:03:59,130 --> 01:04:01,530
it is not something most people use anymore.

1201
01:04:01,530 --> 01:04:03,450
Um, people would use, uh,

1202
01:04:03,450 --> 01:04:05,445
more deep learning approaches,

1203
01:04:05,445 --> 01:04:07,155
but I think that the,

1204
01:04:07,155 --> 01:04:09,525
the key things to understand from this is,

1205
01:04:09,525 --> 01:04:11,760
sort of, this aspect of, kind of,

1206
01:04:11,760 --> 01:04:16,920
if you match state features that that's sufficient to say that the policies are identical.

1207
01:04:16,920 --> 01:04:20,190
It's actually bigger than. Yeah, and [OVERLAPPING] remind me your name first, please.

1208
01:04:20,190 --> 01:04:20,520
.

1209
01:04:20,520 --> 01:04:20,970
.

1210
01:04:20,970 --> 01:04:24,030
[NOISE] Is there any significance in using norm one

1211
01:04:24,030 --> 01:04:27,585
versus the other norm, like, norm two?

1212
01:04:27,585 --> 01:04:31,560
Uh, goo- great question. question is why do we use norm one in equation seven.

1213
01:04:31,560 --> 01:04:33,225
That is actually important.

1214
01:04:33,225 --> 01:04:35,310
Um, it's not necessarily the only choice,

1215
01:04:35,310 --> 01:04:39,850
but here this is saying you have to match on all states.

1216
01:04:39,860 --> 01:04:42,270
That's what the norm one is saying here.

1217
01:04:42,270 --> 01:04:46,110
So, you can think of mu pi as- are really being of s. I'm not showing

1218
01:04:46,110 --> 01:04:50,880
that explicit dependency here but it is of s. And so what norm one is saying is that,

1219
01:04:50,880 --> 01:04:53,610
um, when you sum up all of those errors it has to be one.

1220
01:04:53,610 --> 01:04:56,220
So, you're really, you're evaluating the error over all of this.

1221
01:04:56,220 --> 01:04:58,380
You could choose other things to change the analysis.

1222
01:04:58,380 --> 01:05:00,420
Um, uh, norm one in an infinity norm

1223
01:05:00,420 --> 01:05:04,110
norm tend be particularly easy to reason about when you start to do,

1224
01:05:04,110 --> 01:05:08,590
um, uh, when you're trying to bound the error in the value function.

1225
01:05:13,010 --> 01:05:19,950
Okay. So, um, there's still this ambiguity that we've talked about,

1226
01:05:19,950 --> 01:05:22,785
so there's the sort of infinite number of different reward functions,

1227
01:05:22,785 --> 01:05:25,770
the algorithm that we just talked about doesn't solve that issue.

1228
01:05:25,770 --> 01:05:30,015
Um, and so there's been a lot of work on,

1229
01:05:30,015 --> 01:05:33,165
on, on imitation, uh, learning and inverse reinforcement learning.

1230
01:05:33,165 --> 01:05:36,060
And two of the key papers are as follows.

1231
01:05:36,060 --> 01:05:39,345
The first one is called Maximum Entropy Inverse RL.

1232
01:05:39,345 --> 01:05:44,400
And the idea here was to say we wa- I don't wanna pick something, uh,

1233
01:05:44,400 --> 01:05:47,280
which has the maximum uncertainty, uh,

1234
01:05:47,280 --> 01:05:52,690
given that still respects the constraints of the data that we have from our expert.

1235
01:05:52,690 --> 01:05:55,320
So saying, we're really not sure what the reward function is,

1236
01:05:55,320 --> 01:05:57,030
we may not really be sure what the policy is,

1237
01:05:57,030 --> 01:06:00,285
but let's try to pick distributions that have maximum entropy,

1238
01:06:00,285 --> 01:06:02,100
sort of make Least Commitment, um,

1239
01:06:02,100 --> 01:06:03,539
sort of the opposite of overfitting,

1240
01:06:03,539 --> 01:06:06,690
you kinda want to like underfit as much as possible.

1241
01:06:06,690 --> 01:06:13,365
Um, and only makes sure that you match in these expected, uh, state frequencies.

1242
01:06:13,365 --> 01:06:16,470
So, both of these,

1243
01:06:16,470 --> 01:06:18,750
the- both of these methods and a lot of the methods think

1244
01:06:18,750 --> 01:06:21,780
very carefully about the expected state frequencies you get,

1245
01:06:21,780 --> 01:06:25,035
um, er- and comparing the da- the data you get versus,

1246
01:06:25,035 --> 01:06:27,765
um, the data you have from the demonstrated- demonstrator.

1247
01:06:27,765 --> 01:06:32,040
Um, these type of methods can be extended to where the transition model is a node.

1248
01:06:32,040 --> 01:06:41,460
[NOISE] Often that requires access to a simulator.

1249
01:06:41,460 --> 01:06:44,040
Often it means, so you can imagine for the thing we had before,

1250
01:06:44,040 --> 01:06:46,350
if you didn't have access to the transition model

1251
01:06:46,350 --> 01:06:48,780
but did- you did have access to it actually in the real world,

1252
01:06:48,780 --> 01:06:50,400
you could just try out new policies,

1253
01:06:50,400 --> 01:06:53,565
see where your distribution of states that are like and how that matches your,

1254
01:06:53,565 --> 01:06:56,700
um, expert demonstration and in fact that's often what's done.

1255
01:06:56,700 --> 01:06:59,985
So, maximum entropy inverse RL has been hugely influential.

1256
01:06:59,985 --> 01:07:01,140
And, and, and then the second one,

1257
01:07:01,140 --> 01:07:03,060
and this is also a note is from, uh,

1258
01:07:03,060 --> 01:07:04,200
also from Drew Bagnell's group,

1259
01:07:04,200 --> 01:07:05,940
who was the same person that came up with DAGGER,

1260
01:07:05,940 --> 01:07:07,890
so that group's been thinking a lot about

1261
01:07:07,890 --> 01:07:10,695
and did a lot of nice contributions to inverse RL.

1262
01:07:10,695 --> 01:07:15,195
And then, in terms of extending this to sort of much,

1263
01:07:15,195 --> 01:07:17,849
um, broader function approximators,

1264
01:07:17,849 --> 01:07:25,320
um, er, Stefano Ermon who's here at Stanford extended this to using deep neural networks,

1265
01:07:25,320 --> 01:07:28,755
um, and again, is doing sort of this feature matching.

1266
01:07:28,755 --> 01:07:30,450
So, the idea in this case,

1267
01:07:30,450 --> 01:07:32,370
both of these methods compared to the sort of

1268
01:07:32,370 --> 01:07:35,745
the DAGGER work or assuming that you have a fixed set of trajectories at the beginning,

1269
01:07:35,745 --> 01:07:37,635
and then you're going to do more things for the future.

1270
01:07:37,635 --> 01:07:40,065
And in particular, um, the general, uh,

1271
01:07:40,065 --> 01:07:43,890
generative adversarial inverse imitation learning, um,

1272
01:07:43,890 --> 01:07:45,450
has these initial trajectories,

1273
01:07:45,450 --> 01:07:48,195
and then it's gonna allow the agent to go out and gather more data.

1274
01:07:48,195 --> 01:07:49,845
So, they can gather more data,

1275
01:07:49,845 --> 01:07:50,955
it can compute the,

1276
01:07:50,955 --> 01:07:52,710
the state frequencies, um,

1277
01:07:52,710 --> 01:07:57,810
it can also use sort of a discriminator to compare- one of the challenges is,

1278
01:07:57,810 --> 01:07:59,100
you know, writing down,

1279
01:07:59,100 --> 01:08:01,320
um, the form of Mu,

1280
01:08:01,320 --> 01:08:05,685
can be hard when you have a really really high dimensional state space.

1281
01:08:05,685 --> 01:08:08,835
So, writing down, you know, a distribution over images is hard.

1282
01:08:08,835 --> 01:08:11,850
Um, so, what they do in this case,

1283
01:08:11,850 --> 01:08:15,240
they're mostly focusing on MuJoCo-style tasks like robotic-style tasks,

1284
01:08:15,240 --> 01:08:16,620
where you'd have a lot of different joints,

1285
01:08:16,620 --> 01:08:17,760
but it's still hard to write down,

1286
01:08:17,760 --> 01:08:19,410
you know, nice distributions over that.

1287
01:08:19,410 --> 01:08:21,660
So, what they focus on in this case is thinking about things

1288
01:08:21,660 --> 01:08:23,910
like a discriminator that could tell between

1289
01:08:23,910 --> 01:08:25,950
your expert demonstrations and the demonstration-

1290
01:08:25,950 --> 01:08:28,979
and the trajectories that are being generated by an agent.

1291
01:08:28,979 --> 01:08:32,609
And so, if you can tell the difference between those, then you're not matching.

1292
01:08:32,609 --> 01:08:34,574
That's it, that's a nice insight,

1293
01:08:34,575 --> 01:08:36,779
is to say that we could use these sort of, uh,

1294
01:08:36,779 --> 01:08:39,899
discriminators, uh, again, is, you know,

1295
01:08:39,899 --> 01:08:43,859
the discriminator function, to try to figure out how do we quantify what it means

1296
01:08:43,859 --> 01:08:48,239
to have the same state distribution in really high state- high dimensional state spaces.

1297
01:08:48,240 --> 01:08:51,135
So, um, uh, this is known as Gail.

1298
01:08:51,135 --> 01:08:55,950
And there's been a lot of extensions to Gail as well. Yeah, .

1299
01:08:55,950 --> 01:08:58,529
Uh, earlier, we said that there could

1300
01:08:58,529 --> 01:09:01,289
be real practical benefits to learning the reward function,

1301
01:09:01,290 --> 01:09:03,540
in certain situations, um,

1302
01:09:03,540 --> 01:09:06,090
but it seems like a takeaway to those that we can't actually do that,

1303
01:09:06,090 --> 01:09:07,590
is that the correct takeaway here?

1304
01:09:07,590 --> 01:09:10,260
Um, er, yeah. So, was saying earlier, well, earlier as

1305
01:09:10,260 --> 01:09:13,200
you were arguing that maybe there are times where we really want the reward function,

1306
01:09:13,200 --> 01:09:16,229
um, but maybe you're telling us that we can't really do that, um,

1307
01:09:16,229 --> 01:09:18,329
I think in this- so we've mostly

1308
01:09:18,330 --> 01:09:20,745
been talking about like frequentist-style statistics, er,

1309
01:09:20,745 --> 01:09:22,965
when we're talking about statistical methods here,

1310
01:09:22,965 --> 01:09:25,830
from that perspective, it's often very hard to uncover the word function.

1311
01:09:25,830 --> 01:09:28,500
One thing that's often people do when they want to say,

1312
01:09:28,500 --> 01:09:30,840
understand animal behavior or things like that,

1313
01:09:30,840 --> 01:09:33,180
so you have a prior, you can do it another way to do this,

1314
01:09:33,180 --> 01:09:35,430
is you have Bayesian prior reward functions,

1315
01:09:35,430 --> 01:09:37,140
and then you do Bayesian updating,

1316
01:09:37,140 --> 01:09:38,550
so that given the data that you see,

1317
01:09:38,550 --> 01:09:42,390
you try to refine your posterior over the possible reward functions.

1318
01:09:42,390 --> 01:09:45,630
So, it avoids like then you can just not

1319
01:09:45,630 --> 01:09:49,140
have your prior cover that reward is a zero everywhere, for example.

1320
01:09:49,140 --> 01:09:52,050
Um, er, so, if you have a structured prior,

1321
01:09:52,050 --> 01:09:54,570
that can be one way to still use information to try

1322
01:09:54,570 --> 01:09:57,510
to reduce your uncertainty over people's or agent's or,

1323
01:09:57,510 --> 01:10:00,000
uh, animal's reward functions, yeah,

1324
01:10:00,000 --> 01:10:05,610
[inaudible].

1325
01:10:05,610 --> 01:10:06,225
Yeah.

1326
01:10:06,225 --> 01:10:18,570
Um, it's a great,  says you know,

1327
01:10:18,570 --> 01:10:20,940
what are, what are realistic priors for word functions?

1328
01:10:20,940 --> 01:10:22,740
Um, uh, it's a great question.

1329
01:10:22,740 --> 01:10:24,855
I think mostly it depends on the domain.

1330
01:10:24,855 --> 01:10:27,060
Um, that people do use, uh,

1331
01:10:27,060 --> 01:10:29,295
I think we'll talk a little bit about it for the exploration aspects,

1332
01:10:29,295 --> 01:10:32,910
people do use priors over reward functions for exploration as well,

1333
01:10:32,910 --> 01:10:35,595
um, things like Thompson sampling require you to do that.

1334
01:10:35,595 --> 01:10:38,505
If you want it to be as close to frequentist as possible, um,

1335
01:10:38,505 --> 01:10:40,140
often people do, uh,

1336
01:10:40,140 --> 01:10:43,290
Dirichlet distributions, over multinomials or things like that, um,

1337
01:10:43,290 --> 01:10:45,030
or Gaussians, and, um, uh,

1338
01:10:45,030 --> 01:10:46,860
and so you'd use conjugate, um,

1339
01:10:46,860 --> 01:10:48,630
uh, uh, exponential families,

1340
01:10:48,630 --> 01:10:51,300
so everything's conjugate, but those aren't necessarily realistic.

1341
01:10:51,300 --> 01:10:53,025
I think in, in real domains, um,

1342
01:10:53,025 --> 01:10:55,920
the benefit probably of using these sort of priors would

1343
01:10:55,920 --> 01:10:58,860
be to really encode domain knowledge about, you know,

1344
01:10:58,860 --> 01:11:00,690
uh, whether people are very sensitive,

1345
01:11:00,690 --> 01:11:02,160
what sort of rewards you expect,

1346
01:11:02,160 --> 01:11:05,385
uh, to be reasonable in these cases.

1347
01:11:05,385 --> 01:11:08,760
Yeah, I mean, I think I- to go back to point too,

1348
01:11:08,760 --> 01:11:11,400
I think a lot of it does depend on what you want out.

1349
01:11:11,400 --> 01:11:12,750
So if you really want to just understand

1350
01:11:12,750 --> 01:11:14,295
the reward function and the preference function,

1351
01:11:14,295 --> 01:11:16,200
then we need to maybe do something Bayesian or we'd

1352
01:11:16,200 --> 01:11:18,330
need to try to have a method that's gonna help us like cover it.

1353
01:11:18,330 --> 01:11:21,120
I think what a lot of other methods ended up saying is, well,

1354
01:11:21,120 --> 01:11:22,470
maybe we care about the reward function,

1355
01:11:22,470 --> 01:11:24,675
but mostly we just care about getting high performance.

1356
01:11:24,675 --> 01:11:28,050
So, if we can uncover a policy that's matching an expert policy, we're fine.

1357
01:11:28,050 --> 01:11:30,900
Behavior cloning wasn't a good way to do that because errors compound,

1358
01:11:30,900 --> 01:11:33,300
but now there are these other ways that can do that better,

1359
01:11:33,300 --> 01:11:35,700
and so we're fine with that part.

1360
01:11:35,700 --> 01:11:38,100
And again, I just wanna emphasize like Sergei Lamin

1361
01:11:38,100 --> 01:11:40,695
and others have done work which really combines,

1362
01:11:40,695 --> 01:11:42,255
like you can take Gail and,

1363
01:11:42,255 --> 01:11:45,330
and then go beyond that in terms of, er, exploration,

1364
01:11:45,330 --> 01:11:48,045
so you can end up with a policy that's better than your demonstrator,

1365
01:11:48,045 --> 01:11:49,620
which I think is good, because often,

1366
01:11:49,620 --> 01:11:52,410
like if your demonstrator comes from YouTube, um, uh,

1367
01:11:52,410 --> 01:11:55,815
which is nice, so that's a freely available place to get demonstrations.

1368
01:11:55,815 --> 01:11:57,300
Um, you don't actually know the quality,

1369
01:11:57,300 --> 01:11:58,950
so often you might want to use that to sort of

1370
01:11:58,950 --> 01:12:02,350
bootstrap learning but not necessarily be limited by it.

1371
01:12:02,810 --> 01:12:05,115
All right, so just to summarize,

1372
01:12:05,115 --> 01:12:06,240
um, you know, in practice,

1373
01:12:06,240 --> 01:12:08,580
there's been an enormous amount of work on imitation learning,

1374
01:12:08,580 --> 01:12:09,930
particularly in robotics, uh,

1375
01:12:09,930 --> 01:12:11,250
but in lots of domains.

1376
01:12:11,250 --> 01:12:13,650
Um, and I think that, you know,

1377
01:12:13,650 --> 01:12:16,215
if you're gonna leave class today and go out into industry,

1378
01:12:16,215 --> 01:12:18,015
um, that imitation learning, uh,

1379
01:12:18,015 --> 01:12:20,250
can be very useful practically, uh,

1380
01:12:20,250 --> 01:12:25,395
because often it's easier to get demonstrations and it can really bootstrap learning,

1381
01:12:25,395 --> 01:12:27,915
um, uh, complicated Atari games, et cetera.

1382
01:12:27,915 --> 01:12:30,900
Um, but there's still a lot of challenges that remain, uh,

1383
01:12:30,900 --> 01:12:33,240
particularly, in a lot of the domains

1384
01:12:33,240 --> 01:12:35,895
that I think about we don't know the optimal policy.

1385
01:12:35,895 --> 01:12:37,650
Um, so, I think about, er,

1386
01:12:37,650 --> 01:12:40,575
healthcare or like customers or,

1387
01:12:40,575 --> 01:12:42,870
um, education like intelligent tutoring systems,

1388
01:12:42,870 --> 01:12:44,550
and all of those one of the big challenges is that you don't

1389
01:12:44,550 --> 01:12:46,410
know the optimal policy and you're maybe

1390
01:12:46,410 --> 01:12:48,090
doing all this because you think you could do

1391
01:12:48,090 --> 01:12:50,505
something better than what's in the existing data.

1392
01:12:50,505 --> 01:12:52,665
Um, so, that's, that's a big challenge.

1393
01:12:52,665 --> 01:12:55,755
Um, and how do you combine sort of inverse RL?

1394
01:12:55,755 --> 01:12:58,380
Ah, and maybe online RL in a, in a safe way.

1395
01:12:58,380 --> 01:13:01,485
So, one of the motivations I said for imitation learning was,

1396
01:13:01,485 --> 01:13:03,240
oh well if you want to be safe, um,

1397
01:13:03,240 --> 01:13:06,390
but then if your, if your- the only safe things right now don't do very well,

1398
01:13:06,390 --> 01:13:10,210
then you have to figure out how to do safe exploration in the future.

1399
01:13:11,120 --> 01:13:13,785
All right. I think that's everything for today,

1400
01:13:13,785 --> 01:13:15,540
and I'll see you guys next week where we're gonna start to talk

1401
01:13:15,540 --> 01:13:23,630
about policy search [NOISE].

