1
00:00:04,670 --> 00:00:07,545
All right. We're gonna go ahead and get started.

2
00:00:07,545 --> 00:00:10,935
Um, homework two, it should be well underway.

3
00:00:10,935 --> 00:00:12,120
If you have any questions feel,

4
00:00:12,120 --> 00:00:14,085
feel free to reach out to us.

5
00:00:14,085 --> 00:00:16,830
Um, [NOISE] project proposals,

6
00:00:16,830 --> 00:00:17,910
if you have questions about that,

7
00:00:17,910 --> 00:00:22,020
feel free to come to our office hours or to reach out, um, via Piazza.

8
00:00:22,020 --> 00:00:25,390
Somebody have any other questions I can answer right now?

9
00:00:25,580 --> 00:00:30,420
All right. So today,

10
00:00:30,420 --> 00:00:32,205
we're gonna start- this is a little bit loud.

11
00:00:32,205 --> 00:00:36,315
Um, today we're gonna start talking about policy gradient methods.

12
00:00:36,315 --> 00:00:38,390
Um, policy gradient methods are probably the

13
00:00:38,390 --> 00:00:40,730
most well used in reinforcement learning right now.

14
00:00:40,730 --> 00:00:44,650
Um, so, I think they're an incredibly useful thing to be familiar with.

15
00:00:44,650 --> 00:00:47,850
Um, [NOISE] whenever we talk about reinforcement learning,

16
00:00:47,850 --> 00:00:50,630
we keep coming back to these main properties that we'd like

17
00:00:50,630 --> 00:00:54,020
about agents that learn to make decisions about them, you know,

18
00:00:54,020 --> 00:00:57,755
to do this sort of optimization, handling delayed consequences,

19
00:00:57,755 --> 00:01:01,820
doing exploration, um, [NOISE] and do it all through statistically,

20
00:01:01,820 --> 00:01:05,040
and efficiently, in really high dimensional spaces.

21
00:01:05,560 --> 00:01:09,935
Um, and what we were sort of talking about last time in terms of imitation learning was

22
00:01:09,935 --> 00:01:11,600
sort of a different way to kind of provide

23
00:01:11,600 --> 00:01:14,450
additional structure or additional support for our agents,

24
00:01:14,450 --> 00:01:17,225
um, so that they could try to learn how to do things faster.

25
00:01:17,225 --> 00:01:20,350
Um, and imitation learning was one way to provide

26
00:01:20,350 --> 00:01:23,790
structural support by leveraging demonstrations from people.

27
00:01:23,790 --> 00:01:26,195
And we've seen other ways to sort of, um,

28
00:01:26,195 --> 00:01:30,110
encode structure or human prior knowledge,

29
00:01:30,110 --> 00:01:32,750
um, when we started talking about function approximation.

30
00:01:32,750 --> 00:01:36,960
So, when we think about how we define q,

31
00:01:36,960 --> 00:01:39,090
like when we define q as s, a,

32
00:01:39,090 --> 00:01:43,020
and w, where this was a set of parameters.

33
00:01:43,270 --> 00:01:47,180
We were implicitly making a choice about sort of

34
00:01:47,180 --> 00:01:51,080
imposing some structure in terms of how we are going to represent our value function,

35
00:01:51,080 --> 00:01:55,010
and that choice might be fairly strong like assuming it was linear.

36
00:01:55,010 --> 00:01:57,599
So, this is sort of a quite a strong assumption,

37
00:01:57,599 --> 00:02:02,220
or it might be a very weak assumption like using a deep neural net.

38
00:02:03,650 --> 00:02:09,440
And so, when we specify sort of these function approximations, and representations,

39
00:02:09,440 --> 00:02:12,020
we're sort of implicitly making, uh, uh,

40
00:02:12,020 --> 00:02:15,390
choices about how much structure and how much domain knowledge we want to put in,

41
00:02:15,390 --> 00:02:18,150
um, in order for our agents to learn.

42
00:02:18,650 --> 00:02:22,670
So, what we're gonna start to talk about today and we're gonna talk about this week is

43
00:02:22,670 --> 00:02:24,770
policy search which is another place

44
00:02:24,770 --> 00:02:27,500
where it can be very natural to put in domain knowledge.

45
00:02:27,500 --> 00:02:30,440
I mean, we'll see that in in some robotics examples today,

46
00:02:30,440 --> 00:02:34,320
and it can be also a very efficient way to try to learn.

47
00:02:36,560 --> 00:02:38,765
So, as I was saying,

48
00:02:38,765 --> 00:02:40,540
before we sort of we're approximating where we're

49
00:02:40,540 --> 00:02:42,355
doing model-free reinforcement learning,

50
00:02:42,355 --> 00:02:45,490
and when we started to try to scale up to really large state spaces.

51
00:02:45,490 --> 00:02:48,610
Um, I've been having several different people ask me about

52
00:02:48,610 --> 00:02:52,000
really large action spaces which is a really important topic.

53
00:02:52,000 --> 00:02:54,300
We're not gonna talk too much about that in this quarter,

54
00:02:54,300 --> 00:02:55,720
but we will talk a little bit about when

55
00:02:55,720 --> 00:02:58,525
your action space is continuous but low-dimensional.

56
00:02:58,525 --> 00:03:00,550
But we have started to talk a lot about when

57
00:03:00,550 --> 00:03:02,690
the state space is really high-dimensional and,

58
00:03:02,690 --> 00:03:04,045
and, and really large.

59
00:03:04,045 --> 00:03:07,240
And so, we talked about approximating things, um, uh,

60
00:03:07,240 --> 00:03:09,170
with some sort of parameterization,

61
00:03:09,170 --> 00:03:11,325
like whether it will be parameters Theta or we often,

62
00:03:11,325 --> 00:03:19,060
or we often use w, but some sort of parameterization of the function.

63
00:03:19,060 --> 00:03:22,565
So, we used our value function, um,

64
00:03:22,565 --> 00:03:27,300
to define expected discounted sum of rewards from a particular state or state action,

65
00:03:27,300 --> 00:03:29,530
and then we could extract a policy from

66
00:03:29,530 --> 00:03:33,055
that value function or at least from a state action value function.

67
00:03:33,055 --> 00:03:37,420
And instead, what we're gonna do today is just directly parameterize the policy.

68
00:03:37,420 --> 00:03:40,650
So, when we talked about tabular policies,

69
00:03:40,650 --> 00:03:44,580
our policy was just a mapping from states to actions.

70
00:03:44,580 --> 00:03:46,695
And in the tabular setting,

71
00:03:46,695 --> 00:03:48,570
we could just look- do that as a lookup table.

72
00:03:48,570 --> 00:03:52,375
For every single state, we could write down what action we would take.

73
00:03:52,375 --> 00:03:54,310
And what we're going to do now is to say, "Well,

74
00:03:54,310 --> 00:03:58,750
it's not gonna be feasible to write down our table of our policy,

75
00:03:58,750 --> 00:04:00,590
so instead what we're going to do is parameterize it,

76
00:04:00,590 --> 00:04:03,440
and we're gonna use a set of weights or Thetas."

77
00:04:03,440 --> 00:04:05,090
Today, we're mostly gonna use Thetas,

78
00:04:05,090 --> 00:04:07,940
but this could equally well think of this as weights.

79
00:04:07,940 --> 00:04:11,230
Um, just some way to parameterize our policy.

80
00:04:11,230 --> 00:04:14,930
We'll talk more about particular forms of parameterization.

81
00:04:14,930 --> 00:04:19,394
Um, but just like what we saw for state action value functions, um,

82
00:04:19,394 --> 00:04:22,280
this is gonna have a big implication because this is effectively

83
00:04:22,280 --> 00:04:25,135
defining the space that you can learn over.

84
00:04:25,135 --> 00:04:27,510
So, it's sort of, um, it's determining the,

85
00:04:27,510 --> 00:04:30,805
the class of policies you could possibly learn.

86
00:04:30,805 --> 00:04:34,780
Um, [NOISE] and we're again gonna sort of focus on model-free reinforcement learning,

87
00:04:34,780 --> 00:04:37,250
meaning that we're not gonna assume that we have access to

88
00:04:37,250 --> 00:04:41,520
an a priori model of the dynamics or reward of the world.

89
00:04:42,290 --> 00:04:46,260
So, we had thrown some of these diagrams up at the start of the quarter,

90
00:04:46,260 --> 00:04:47,640
I just want to go back to it.

91
00:04:47,640 --> 00:04:49,965
Um, we've been talking about sort of value,

92
00:04:49,965 --> 00:04:51,840
we- we haven't talked so much about models,

93
00:04:51,840 --> 00:04:53,580
the models are also super important.

94
00:04:53,580 --> 00:04:56,110
Um, but we've been talking a lot about sort of value function,

95
00:04:56,110 --> 00:04:58,050
based approaches which is this,

96
00:04:58,050 --> 00:05:00,560
and now we're gonna talk about policy,

97
00:05:00,560 --> 00:05:03,005
um, direct policy search methods.

98
00:05:03,005 --> 00:05:04,490
And as you might expect,

99
00:05:04,490 --> 00:05:07,000
there's a lot of work which tries to combine between the two of them,

100
00:05:07,000 --> 00:05:09,130
and these are often called actor-critic methods.

101
00:05:09,130 --> 00:05:13,395
Um, where you try to explicitly maintain a parameterized policy,

102
00:05:13,395 --> 00:05:17,620
and explicitly maintain a parameterized critic or value function.

103
00:05:17,620 --> 00:05:21,730
So, this is the policy, and this is a Q.

104
00:05:23,120 --> 00:05:27,565
Okay, so, we're gonna start today and we're gonna be talking about policy-based methods.

105
00:05:27,565 --> 00:05:29,970
So, why would you wanna do this?

106
00:05:29,970 --> 00:05:31,585
Um, [NOISE] well, uh,

107
00:05:31,585 --> 00:05:33,590
it actually goes back a little bit to also what we were

108
00:05:33,590 --> 00:05:35,645
talking about last week with imitation learning.

109
00:05:35,645 --> 00:05:37,970
For imitation learning, we talked about the fact that

110
00:05:37,970 --> 00:05:40,680
sometimes it's hard for humans to write down a reward function,

111
00:05:40,680 --> 00:05:44,465
and so it might be easier for them just to demonstrate what the policy looks like.

112
00:05:44,465 --> 00:05:46,100
Similarly, in some cases,

113
00:05:46,100 --> 00:05:49,580
maybe it's easier to write down sort of a parametrization of, um,

114
00:05:49,580 --> 00:05:51,680
the space of policies than it is to write down

115
00:05:51,680 --> 00:05:55,700
a parameterization of the space of state action value functions.

116
00:05:55,700 --> 00:05:58,790
Um, in addition, they're often

117
00:05:58,790 --> 00:06:02,495
much more effective in high-dimensional or continuous action spaces,

118
00:06:02,495 --> 00:06:07,450
and they allow us to learn stochastic policies which we haven't talked very much about so far,

119
00:06:07,450 --> 00:06:09,410
but I'm gonna give you some illustrations about where

120
00:06:09,410 --> 00:06:12,420
we definitely want stochastic policies.

121
00:06:12,640 --> 00:06:16,220
Um, [NOISE] and they sometimes have better convergent policy- convergence,

122
00:06:16,220 --> 00:06:19,430
uh, properties um, that can be a little bit debated,

123
00:06:19,430 --> 00:06:22,250
it depends exactly what- whether we're comparing that to model-free

124
00:06:22,250 --> 00:06:25,400
or model-based approaches and how much computation we're doing.

125
00:06:25,400 --> 00:06:30,850
Um so, this can be a little bit of a function of computation to computation can matter.

126
00:06:32,480 --> 00:06:36,340
One of the really big disadvantages is that they are

127
00:06:36,340 --> 00:06:40,370
typically only gonna converge to a local optimum.

128
00:06:41,660 --> 00:06:46,520
So, where you're going to converge to something that is hopefully a pretty good policy,

129
00:06:46,520 --> 00:06:49,600
but we're not generally guaranteed to converge to the global optima.

130
00:06:49,600 --> 00:06:51,790
[NOISE]. Now, there are some techniques that are

131
00:06:51,790 --> 00:06:54,820
guaranteed to converge to a local- to the global optima,

132
00:06:54,820 --> 00:06:56,620
and I'll try to highlight some of those today,

133
00:06:56,620 --> 00:06:59,260
but generally, almost all of the methods that you see in like

134
00:06:59,260 --> 00:07:02,140
deep reinforcement learning that are policy gradient based,

135
00:07:02,140 --> 00:07:05,110
um, only converge to a local optima.

136
00:07:05,110 --> 00:07:08,710
Um, and then the other challenge is that typically we're

137
00:07:08,710 --> 00:07:12,360
gonna do this by trying to evaluate a policy and then estimate its gradient,

138
00:07:12,360 --> 00:07:15,185
and often that can be somewhat sample inefficient.

139
00:07:15,185 --> 00:07:18,275
So, there might be quite a lot of data to estimate,

140
00:07:18,275 --> 00:07:22,770
um, what that gradient is when we're taking a gradient-based approach.

141
00:07:23,990 --> 00:07:27,965
So, why might we want sort of a stochastic policy?

142
00:07:27,965 --> 00:07:30,410
So, in what I mentioned before, um,

143
00:07:30,410 --> 00:07:31,835
in the tabular setting,

144
00:07:31,835 --> 00:07:33,215
so let me just go back to here.

145
00:07:33,215 --> 00:07:36,020
So, in a- now,

146
00:07:36,020 --> 00:07:39,830
why do we want this? Do we want this?

147
00:07:39,830 --> 00:07:43,850
If you think back to the very first lectures, um,

148
00:07:43,850 --> 00:07:48,640
what I said is that if we have a tabular MDP,

149
00:07:48,640 --> 00:07:55,780
there exist a Pi which is deterministic and optimal.

150
00:08:01,430 --> 00:08:04,725
So, in the tabular MDP setting,

151
00:08:04,725 --> 00:08:07,020
we do not need, um, er,

152
00:08:07,020 --> 00:08:11,600
deter- we do not need stochastic policies because there always exists

153
00:08:11,600 --> 00:08:17,365
a policy that is deterministic that has the same value as the optimal policy does.

154
00:08:17,365 --> 00:08:21,590
So, this is not needed in the tabular Markov Decision Process case,

155
00:08:21,590 --> 00:08:23,510
but we don't always- we're not always

156
00:08:23,510 --> 00:08:26,000
acting in the tabular Markov Decision Process case.

157
00:08:26,000 --> 00:08:27,680
So, as an example, um,

158
00:08:27,680 --> 00:08:30,700
[NOISE] who here is familiar with rock-paper-scissors ?

159
00:08:30,700 --> 00:08:33,015
Okay. Most people. Um, er,

160
00:08:33,015 --> 00:08:35,315
possibly if you're not, you might have played it by another name.

161
00:08:35,315 --> 00:08:37,205
So, in rock-paper-scissors, uh,

162
00:08:37,205 --> 00:08:39,020
it's a two player game, um,

163
00:08:39,020 --> 00:08:40,909
[NOISE] everyone can either pick,

164
00:08:40,909 --> 00:08:43,834
uh, paper or scissors or rock.

165
00:08:43,835 --> 00:08:45,370
And you have to pick one of those,

166
00:08:45,370 --> 00:08:46,705
and scissors beats paper,

167
00:08:46,705 --> 00:08:49,360
paper- rock beats scissors, and paper beats rock.

168
00:08:49,360 --> 00:08:51,520
Um, and in this case,

169
00:08:51,520 --> 00:08:54,110
if you had a deterministic policy,

170
00:08:54,110 --> 00:08:55,770
you could lose a lot,

171
00:08:55,770 --> 00:08:58,070
you could easily be exploited by the other agent.

172
00:08:58,070 --> 00:09:02,620
Um, but a uniform random policy is basically optimal.

173
00:09:02,620 --> 00:09:04,350
What do I mean by optimality?

174
00:09:04,350 --> 00:09:05,545
In this case, I mean, that you,

175
00:09:05,545 --> 00:09:07,440
you could say a plus one if you win,

176
00:09:07,440 --> 00:09:10,350
and let say zero or minus one if you lose.

177
00:09:10,350 --> 00:09:13,950
We're not gonna talk too much about multi-agent cases,

178
00:09:13,950 --> 00:09:17,300
um, er, in this class, but it's a super interesting area of research.

179
00:09:17,300 --> 00:09:19,030
Um, and in this case,

180
00:09:19,030 --> 00:09:21,860
um, you know, the environment is not agnostic.

181
00:09:21,860 --> 00:09:24,310
Um, the environment can react to, uh,

182
00:09:24,310 --> 00:09:26,930
the policies that we're doing and could be adversarial,

183
00:09:26,930 --> 00:09:28,430
and so we want a policy that,

184
00:09:28,430 --> 00:09:31,560
um, is robust to an adversary.

185
00:09:33,150 --> 00:09:36,745
So, a second case, um, is Aliased Gridword.

186
00:09:36,745 --> 00:09:39,550
So, um, so in this case,

187
00:09:39,550 --> 00:09:40,810
so why, you know,

188
00:09:40,810 --> 00:09:42,490
why is being stochastic important here?

189
00:09:42,490 --> 00:09:44,800
Well, because we're not really in a stochastic setting,

190
00:09:44,800 --> 00:09:46,360
we are in an adversarial setting,

191
00:09:46,360 --> 00:09:49,045
and we have another agent that is playing with us and they can be

192
00:09:49,045 --> 00:09:52,405
non-stationary and changing their policy in response to ours.

193
00:09:52,405 --> 00:09:54,100
Um, so it's not, uh,

194
00:09:54,100 --> 00:09:56,620
the environment doesn't pick the next- doesn't

195
00:09:56,620 --> 00:09:58,885
pick rock-paper-scissors regardless of our actions,

196
00:09:58,885 --> 00:10:01,150
um, in the past it can respond to those.

197
00:10:01,150 --> 00:10:05,815
[NOISE] Um, so it's sort of got this non-stationarity or adversarial nature.

198
00:10:05,815 --> 00:10:08,635
Um, another case is where it's not Markov,

199
00:10:08,635 --> 00:10:10,570
so it's really partially observable,

200
00:10:10,570 --> 00:10:11,994
and you have aliasing,

201
00:10:11,994 --> 00:10:16,210
which means that we can't distinguish between multiple states in terms of our sensors.

202
00:10:16,210 --> 00:10:19,720
So, we saw this before a little bit when we talked about robots that, you know,

203
00:10:19,720 --> 00:10:24,310
could have laser range finders and sort of tell where they were in a hallway.

204
00:10:24,310 --> 00:10:27,760
By how far away each of the,

205
00:10:27,760 --> 00:10:30,205
um, the, the first point of,

206
00:10:30,205 --> 00:10:34,510
um, uh, uh, obstacle was for all of their 180 degrees,

207
00:10:34,510 --> 00:10:36,775
and so that will look the same in lots of different hallways.

208
00:10:36,775 --> 00:10:38,650
Um, so this is a simple example of that.

209
00:10:38,650 --> 00:10:40,750
So, in an Aliased Gridword, um,

210
00:10:40,750 --> 00:10:44,050
let's say that the agent because of their sensors cannot distinguish

211
00:10:44,050 --> 00:10:47,590
the gray states and they have features of a particular form.

212
00:10:47,590 --> 00:10:51,850
Um, they have a feature for whether or not there's a wall to the North,

213
00:10:51,850 --> 00:10:53,860
um, er, or East,

214
00:10:53,860 --> 00:10:55,075
or South, or West.

215
00:10:55,075 --> 00:10:57,565
So, it can basically, like, if it's here it can tell, like,

216
00:10:57,565 --> 00:11:01,090
"Oh I have walls to either side of me and not in front or behind me."

217
00:11:01,090 --> 00:11:03,985
Um, but that could be the same over in that in the,

218
00:11:03,985 --> 00:11:06,940
in the other [NOISE], um, grey state.

219
00:11:06,940 --> 00:11:09,100
So, if we did

220
00:11:09,100 --> 00:11:11,410
a value-based reinforcement learning approach

221
00:11:11,410 --> 00:11:13,885
using some sort of approximate value function,

222
00:11:13,885 --> 00:11:16,015
um, it would take these features which

223
00:11:16,015 --> 00:11:18,655
are a combination of what action am I going to take?

224
00:11:18,655 --> 00:11:20,770
And whether there are walls around me or not.

225
00:11:20,770 --> 00:11:24,940
Um, or we could have a policy-based approach which also, um,

226
00:11:24,940 --> 00:11:26,950
takes some of these features but then just directly

227
00:11:26,950 --> 00:11:29,515
tries to make decisions about what action to take,

228
00:11:29,515 --> 00:11:32,170
and those actions can be stochastic.

229
00:11:32,170 --> 00:11:36,175
So, in this case, the agent is trying to figure out how to navigate in this world.

230
00:11:36,175 --> 00:11:37,645
It really wants to get to here.

231
00:11:37,645 --> 00:11:40,315
This is where there's a large reward. So, this is good.

232
00:11:40,315 --> 00:11:43,405
It wants to avoid the skull and crossbones,

233
00:11:43,405 --> 00:11:46,075
and those will be negative reward.

234
00:11:46,075 --> 00:11:48,970
So, because of the aliasing,

235
00:11:48,970 --> 00:11:54,130
the agent can't distinguish whether or not it's here or here.

236
00:11:54,130 --> 00:11:57,625
Um, and so it has to do the same thing in both states.

237
00:11:57,625 --> 00:12:01,180
And so either it has to go left or it has to go right,

238
00:12:01,180 --> 00:12:02,995
Call it West or East [NOISE], um,

239
00:12:02,995 --> 00:12:05,560
and either way that's not optimal because if

240
00:12:05,560 --> 00:12:08,275
it's actually here it should be going that way,

241
00:12:08,275 --> 00:12:11,150
not over here and down.

242
00:12:11,310 --> 00:12:14,560
Um, and so it can distinguish whether it's in here or

243
00:12:14,560 --> 00:12:18,205
here but it could just end up moving back and forth,

244
00:12:18,205 --> 00:12:20,560
er, or making very bad decisions.

245
00:12:20,560 --> 00:12:22,420
And so it can get stuck and never be able

246
00:12:22,420 --> 00:12:25,510
to know when it's safe to go down and reach the money.

247
00:12:25,510 --> 00:12:28,540
So, it learns a near-determini- deterministic policy

248
00:12:28,540 --> 00:12:30,325
because that's what we've normally been learning with these,

249
00:12:30,325 --> 00:12:34,720
um, and whether it's greedy or e-greedy and generally it will do very poorly.

250
00:12:34,720 --> 00:12:39,490
But if you have a stochastic policy when you're in a state where you're aliased,

251
00:12:39,490 --> 00:12:40,570
you could just randomize.

252
00:12:40,570 --> 00:12:45,535
You'd say, "I'm not sure whether I'm actually in this state- in this state or this state,

253
00:12:45,535 --> 00:12:48,040
um, so I'll just go, er,

254
00:12:48,040 --> 00:12:51,010
either East or West with 50 percent probability."

255
00:12:51,010 --> 00:12:54,220
And then it'll generally reach the goal state quickly.

256
00:12:54,220 --> 00:12:57,610
Because note, it can tell what it should do when it reaches

257
00:12:57,610 --> 00:13:00,280
here because that looks different than these two states.

258
00:13:00,280 --> 00:13:03,160
So, once it's in the middle it knows exactly what to do.

259
00:13:03,160 --> 00:13:08,050
So, that's just, again, an example where a stochastic policy has a way better value than

260
00:13:08,050 --> 00:13:10,480
a deterministic policy and that's because the domain

261
00:13:10,480 --> 00:13:13,960
here is not Markov, it's partially observable.

262
00:13:13,960 --> 00:13:23,935
[NOISE] Okay.

263
00:13:23,935 --> 00:13:26,650
So, that's sort of one of the reasons why we might want to- some of

264
00:13:26,650 --> 00:13:29,590
the reasons why you want- might wanna be directly policy-based,

265
00:13:29,590 --> 00:13:30,895
and there's a lot of other reasons.

266
00:13:30,895 --> 00:13:33,610
Um, so, so what does this mean?

267
00:13:33,610 --> 00:13:34,960
Well, er, we're gonna have

268
00:13:34,960 --> 00:13:38,200
this parameterized policy and the goal is that we wanna find. Yeah,

269
00:13:38,200 --> 00:13:43,240
Like you said, can we conclude that when the world is not

270
00:13:43,240 --> 00:13:48,610
Markov, it is partially observed, stochastic policy is always better?

271
00:13:48,610 --> 00:13:50,650
Your name is ? I'm sorry yeah.

272
00:13:50,650 --> 00:13:53,020
So what said is can we conclude that, um,

273
00:13:53,020 --> 00:13:56,110
if the world is partially observable stochastic policies are always better.

274
00:13:56,110 --> 00:13:58,525
Um, I think it depends on the modeling you wanna do.

275
00:13:58,525 --> 00:13:59,920
I think, in this case,

276
00:13:59,920 --> 00:14:02,620
better than being stochastic because it's still doing something,

277
00:14:02,620 --> 00:14:05,755
kind of, not very intelligent in the gray states, it's just randomizing,

278
00:14:05,755 --> 00:14:09,850
would be to have a partially observable Markov decision process policy, um,

279
00:14:09,850 --> 00:14:12,295
and then you could track,

280
00:14:12,295 --> 00:14:14,365
uh, an estimate over where you are in the world.

281
00:14:14,365 --> 00:14:17,155
So, you can keep track of a belief state over what state you're in,

282
00:14:17,155 --> 00:14:19,915
and then you could hopefully uniquely identify that,

283
00:14:19,915 --> 00:14:22,630
"Oh, if I was just in this state I have to be in the state now."

284
00:14:22,630 --> 00:14:25,390
And then you can deterministically go to the right or left.

285
00:14:25,390 --> 00:14:26,905
[NOISE] So, it depends on,

286
00:14:26,905 --> 00:14:30,920
on the modeling one's willing to do. Good question.

287
00:14:32,970 --> 00:14:35,290
Okay. So, when we, um,

288
00:14:35,290 --> 00:14:37,900
start to do the- go to parameterize policy search,

289
00:14:37,900 --> 00:14:41,050
what we're gonna wanna do is find the parameters that yield the best value.

290
00:14:41,050 --> 00:14:43,240
The policy in the class with the best value,

291
00:14:43,240 --> 00:14:46,435
and so similar to what we've seen before we can,

292
00:14:46,435 --> 00:14:51,265
we can think about sort of episodic settings and infinite sort of continuing settings.

293
00:14:51,265 --> 00:14:52,810
So, in an episodic setting,

294
00:14:52,810 --> 00:14:56,380
that means that the agent will act for a number of time-steps often,

295
00:14:56,380 --> 00:14:58,540
let's say, H steps.

296
00:14:58,540 --> 00:15:00,580
But it could be variable, like,

297
00:15:00,580 --> 00:15:01,690
it might be until you reach,

298
00:15:01,690 --> 00:15:03,860
you know, a terminal state.

299
00:15:05,400 --> 00:15:09,580
And then we can just consider what is the expected value,

300
00:15:09,580 --> 00:15:10,750
wha- wher- what is the value?

301
00:15:10,750 --> 00:15:12,730
What is the expected discounted sum of rewards we get from

302
00:15:12,730 --> 00:15:15,910
the start state or distribution of start states?

303
00:15:15,910 --> 00:15:20,455
And then what we wanna do is find the parameterized policy that has the highest value.

304
00:15:20,455 --> 00:15:23,110
Um, another option is that if we're in

305
00:15:23,110 --> 00:15:25,720
a continuing environment which means we're in the online setting,

306
00:15:25,720 --> 00:15:28,255
we don't act for H steps we just act forever.

307
00:15:28,255 --> 00:15:31,795
There's no terminal states and we can either use the, um,

308
00:15:31,795 --> 00:15:36,490
average value where we average over the distribution of states.

309
00:15:36,490 --> 00:15:38,110
So, this is, um,

310
00:15:38,110 --> 00:15:40,840
like what we saw before thinking about the distribution,

311
00:15:40,840 --> 00:15:45,580
the stationary distribution over the Markov chain that is induced by a particular policy.

312
00:15:45,580 --> 00:15:49,045
Because we talked about before about the fact that if you fix the policy,

313
00:15:49,045 --> 00:15:50,650
um, then basically, uh,

314
00:15:50,650 --> 00:15:52,630
you get into Markov reward process.

315
00:15:52,630 --> 00:15:56,260
You can also just think of the distribution of states you get is a Markov chain.

316
00:15:56,260 --> 00:15:58,810
So, um, if we're acting forever,

317
00:15:58,810 --> 00:16:01,150
we're gonna say sort of on average what is

318
00:16:01,150 --> 00:16:04,765
the value of the states that we reach under that stationary distribution?

319
00:16:04,765 --> 00:16:07,480
Um, and another way to do it is also to say we

320
00:16:07,480 --> 00:16:10,330
just look at sort of the average reward per time step.

321
00:16:10,330 --> 00:16:14,410
Now, for simplicity today we're gonna focus almost exclusively on the episodic setting,

322
00:16:14,410 --> 00:16:18,770
but we can think about similar techniques for these other forms of settings.

323
00:16:19,380 --> 00:16:23,620
So, as before and this is an optimization problem similar

324
00:16:23,620 --> 00:16:27,055
to what we saw in the value function approximation case, uh,

325
00:16:27,055 --> 00:16:30,115
for linear value functions and using deep neural networks, um,

326
00:16:30,115 --> 00:16:32,620
we're gonna wanna be doing optimization, er,

327
00:16:32,620 --> 00:16:34,240
which means that we need to do some sort of

328
00:16:34,240 --> 00:16:38,000
optimization tool to try to search for the best data.

329
00:16:39,120 --> 00:16:43,510
So, one option is to do gradient free optimization.

330
00:16:43,510 --> 00:16:47,110
We don't tend to do this very much in policy search methods,

331
00:16:47,110 --> 00:16:50,455
um, but there are lots of different methods that are gradient free optimization.

332
00:16:50,455 --> 00:16:55,555
Just for us to find whatever parameters maximize this V Pi Theta.

333
00:16:55,555 --> 00:16:59,875
Um, and just to connect this- just like what we saw for Q functions,

334
00:16:59,875 --> 00:17:03,100
now we have Theta which is specifying a policy.

335
00:17:03,100 --> 00:17:05,994
And it maybe has some interesting landscape,

336
00:17:05,994 --> 00:17:09,279
and then we wanna be able to find where's the max.

337
00:17:09,280 --> 00:17:13,555
So, we're really trying to find the max of a function as efficiently as we can.

338
00:17:13,555 --> 00:17:15,280
And there are lots of methods for doing that that

339
00:17:15,280 --> 00:17:17,755
don't rely on the function being differentiable.

340
00:17:17,755 --> 00:17:22,329
Um, and these actually can be very good in some cases.

341
00:17:22,329 --> 00:17:27,459
Um, so this is some nice work done by a colleague of mine and-

342
00:17:27,460 --> 00:17:30,925
We have developed a method for automatically identifying

343
00:17:30,925 --> 00:17:35,260
the exoskeleton assistance patterns that minimize metabolic energy costs for

344
00:17:35,260 --> 00:17:43,150
individual humans during walking [NOISE].

345
00:17:43,150 --> 00:17:45,700
During optimization the user first experiences

346
00:17:45,700 --> 00:17:48,745
one control law while respiratory measurements are taken.

347
00:17:48,745 --> 00:17:51,430
Steady-state energy cost is estimated by fitting

348
00:17:51,430 --> 00:17:54,190
a first-order dynamical model to two minutes of transient

349
00:17:54,190 --> 00:18:02,350
data. The control law is then changed and metabolic rate is estimated again.

350
00:18:02,350 --> 00:18:07,540
This process is repeated for a prescribed number of control laws forming one generation.

351
00:18:07,540 --> 00:18:12,280
[NOISE]

352
00:18:12,280 --> 00:18:15,850
A covariance matrix adaptation evolution strategy

353
00:18:15,850 --> 00:18:18,190
is then used to create the next generation.

354
00:18:18,190 --> 00:18:20,290
The mean of each generation represents

355
00:18:20,290 --> 00:18:23,320
the best estimate of the optimal control parameter values.

356
00:18:23,320 --> 00:18:25,510
After about an hour of optimization,

357
00:18:25,510 --> 00:18:27,445
energy cost was reduced by an averageg

358
00:18:27,445 --> 00:18:31,130
of 24 percent, compared to no assistance.

359
00:18:34,500 --> 00:18:39,520
So this is work that's done by my colleague uh, Steve Collins, um,

360
00:18:39,520 --> 00:18:41,950
who's over in mechanical engineering and we've been collaborating

361
00:18:41,950 --> 00:18:44,740
some on whether you can train people to- do this better, um.

362
00:18:44,740 --> 00:18:46,885
So, the idea in this case is that, uh,

363
00:18:46,885 --> 00:18:50,020
there's lots of instances for which you'd like to use exoskeletons.

364
00:18:50,020 --> 00:18:51,160
Um, a lot of people have strokes,

365
00:18:51,160 --> 00:18:52,915
a lot of people have mobility problems um,

366
00:18:52,915 --> 00:18:55,150
and of course there's a lot of veterans that lose a limb.

367
00:18:55,150 --> 00:18:59,120
Um, and in these cases one of the challenges has been is how do you,

368
00:18:59,120 --> 00:19:03,835
sort of figure out what are the parameters of these exoskeletons in order to provide um,

369
00:19:03,835 --> 00:19:05,980
support for people walking and generally it

370
00:19:05,980 --> 00:19:08,320
varies on physiology and for many different people.

371
00:19:08,320 --> 00:19:10,480
They're going to need different types of parameters,

372
00:19:10,480 --> 00:19:12,520
um, but you want do this really quickly.

373
00:19:12,520 --> 00:19:15,340
So, you want to be able to figure out very fast for each individual.

374
00:19:15,340 --> 00:19:17,560
What is the right control parameters in order to

375
00:19:17,560 --> 00:19:20,035
help them get the most assistance as they walk.

376
00:19:20,035 --> 00:19:23,275
Um, and so Steve's lab treated this as,

377
00:19:23,275 --> 00:19:25,600
uh, sort of a policy,

378
00:19:25,600 --> 00:19:26,980
a policy search problem,

379
00:19:26,980 --> 00:19:29,785
where what you're doing is you're having somebody wear their device,

380
00:19:29,785 --> 00:19:32,860
you're trying some, uh, control laws,

381
00:19:32,860 --> 00:19:36,445
um, that are providing a particular form of support in terms of their exoskeleton.

382
00:19:36,445 --> 00:19:38,320
You're measuring their sort of, um,

383
00:19:38,320 --> 00:19:41,320
metabolic efficiency, which is, how do you- you know.

384
00:19:41,320 --> 00:19:43,555
How hard are they breathing? How hard are they having to work,

385
00:19:43,555 --> 00:19:46,555
compared to if they weren't wearing this or under different control laws.

386
00:19:46,555 --> 00:19:49,300
And then you can use this information to figure out

387
00:19:49,300 --> 00:19:52,210
what's the next set of control laws you use and do this all,

388
00:19:52,210 --> 00:19:54,685
in a, closed loop fashion as quickly as possible.

389
00:19:54,685 --> 00:19:56,635
Now one of the reasons I bring this up is,

390
00:19:56,635 --> 00:19:58,420
both because it was incredibly effective,

391
00:19:58,420 --> 00:20:00,565
it's a really nice science paper that, um,

392
00:20:00,565 --> 00:20:03,700
illustrates how this could be much more effective than previous techniques.

393
00:20:03,700 --> 00:20:07,045
Um, and second because it was using CMA-ES,

394
00:20:07,045 --> 00:20:08,860
which is a gradient free approach.

395
00:20:08,860 --> 00:20:11,635
So even though most of what we're gonna discuss today in class,

396
00:20:11,635 --> 00:20:13,510
is all with gradient based methods.

397
00:20:13,510 --> 00:20:16,930
There's some really nice examples of not using gradient based methods

398
00:20:16,930 --> 00:20:20,560
also to do policy search for lots of other types of applications.

399
00:20:20,560 --> 00:20:22,300
So, I think it's useful to sort of,

400
00:20:22,300 --> 00:20:23,965
know in your toolbox that,

401
00:20:23,965 --> 00:20:26,470
one doesn't have to be constrained to gradient based methods,

402
00:20:26,470 --> 00:20:27,940
and one of the really nice things about,

403
00:20:27,940 --> 00:20:29,470
things like CMA-ES is that,

404
00:20:29,470 --> 00:20:30,760
they're guaranteed to get towards,

405
00:20:30,760 --> 00:20:32,515
uh, to a global optima.

406
00:20:32,515 --> 00:20:34,270
So in some cases, uh,

407
00:20:34,270 --> 00:20:35,860
you might really want to be guaranteed that you're

408
00:20:35,860 --> 00:20:38,900
doing that because it's high stakes situation.

409
00:20:40,800 --> 00:20:43,615
Um, and in general, it sort of is,

410
00:20:43,615 --> 00:20:46,090
has been noticed repeatedly recently that sometimes these sort of

411
00:20:46,090 --> 00:20:48,550
approaches do work kind of embarrassingly well, um,

412
00:20:48,550 --> 00:20:51,265
uh, that they tend to be in some ways sort of a brute forced,

413
00:20:51,265 --> 00:20:52,480
a smart brute force way,

414
00:20:52,480 --> 00:20:54,070
um, that often can be very effective.

415
00:20:54,070 --> 00:20:55,210
So they're good to consider,

416
00:20:55,210 --> 00:20:57,740
in terms of the applications you look at.

417
00:20:58,110 --> 00:21:00,745
But, you know, despite this,

418
00:21:00,745 --> 00:21:02,575
um, uh, even though,

419
00:21:02,575 --> 00:21:04,765
they can be really good and sometimes, um,

420
00:21:04,765 --> 00:21:07,075
they're very, very helpful for parallelization.

421
00:21:07,075 --> 00:21:10,240
Um, uh, they're generally not very sample efficient.

422
00:21:10,240 --> 00:21:11,650
And so depending on, the domain that you're

423
00:21:11,650 --> 00:21:13,615
looking at and what sort of structure you have,

424
00:21:13,615 --> 00:21:15,820
often it's useful to go to a gradient-based method,

425
00:21:15,820 --> 00:21:19,720
particularly if you might be satisfied with the local solution at the end.

426
00:21:19,720 --> 00:21:21,835
Sort of locally optimal.

427
00:21:21,835 --> 00:21:25,405
So what we're going to talk about mostly today- just like what we did for,

428
00:21:25,405 --> 00:21:28,735
um, value, like, uh, value-based methods is,

429
00:21:28,735 --> 00:21:31,480
gradient descent, um, and gradient based methods,

430
00:21:31,480 --> 00:21:34,270
um and other methods that try to exploit

431
00:21:34,270 --> 00:21:37,585
the sequential structure of decision making problems.

432
00:21:37,585 --> 00:21:41,080
So CMA-ES doesn't know anything about the fact that this- the world might be

433
00:21:41,080 --> 00:21:45,055
an MDP or any form of sort of, sequential stochastic process.

434
00:21:45,055 --> 00:21:46,615
And we're gonna focus on ones that sort of,

435
00:21:46,615 --> 00:21:49,120
leverage the structure of the Markov decision process,

436
00:21:49,120 --> 00:21:51,530
in the decision process itself.

437
00:21:51,600 --> 00:21:55,225
So let's talk about policy gradient methods.

438
00:21:55,225 --> 00:21:58,075
Um, where again just sort of,

439
00:21:58,075 --> 00:21:59,680
um, define things in terms of theta,

440
00:21:59,680 --> 00:22:02,170
so that we're explicit about the parameter and we're

441
00:22:02,170 --> 00:22:04,720
gonna focus on episodic MDPs, which means that,

442
00:22:04,720 --> 00:22:07,210
we're gonna run our policy for a certain number of time steps,

443
00:22:07,210 --> 00:22:10,270
until we reach a terminal state or for certain you know, maybe h steps.

444
00:22:10,270 --> 00:22:12,895
Uh, we're going to get some reward during that time period,

445
00:22:12,895 --> 00:22:14,960
and then we're going to reset.

446
00:22:15,510 --> 00:22:19,855
So, we're going to be looking for a local maximum,

447
00:22:19,855 --> 00:22:21,520
and we're going to be taking the gradient,

448
00:22:21,520 --> 00:22:23,485
with respect to the parameters that,

449
00:22:23,485 --> 00:22:25,225
um, define the policy,

450
00:22:25,225 --> 00:22:29,530
and then use some small learning rate [NOISE].

451
00:22:29,530 --> 00:22:31,795
So just this is- this should look very similar,

452
00:22:31,795 --> 00:22:33,280
very similar to the,

453
00:22:33,280 --> 00:22:41,440
similar to, uh, Q and V based search.

454
00:22:43,650 --> 00:22:45,700
And the main difference here is,

455
00:22:45,700 --> 00:22:47,365
that instead of taking, uh,

456
00:22:47,365 --> 00:22:50,200
the derivative with respect to parameters that define our q function,

457
00:22:50,200 --> 00:22:51,400
we're taking them with respect to,

458
00:22:51,400 --> 00:22:54,170
the parameters that define our policy.

459
00:22:55,470 --> 00:22:58,375
So, the simplest thing to do here,

460
00:22:58,375 --> 00:23:01,720
is, um, to do finite differences.

461
00:23:01,720 --> 00:23:05,020
Um, so for each of your policy parameters,

462
00:23:05,020 --> 00:23:07,630
you just perturb it a little bit, um,

463
00:23:07,630 --> 00:23:11,245
and if you do that for every single one of the dimensions, um,

464
00:23:11,245 --> 00:23:14,290
that define your policy parameters,

465
00:23:14,290 --> 00:23:16,555
then you're going to get an estimate of the gradient.

466
00:23:16,555 --> 00:23:20,480
Here, just doing sort of a finite differences estimate of the gradient.

467
00:23:21,840 --> 00:23:27,280
And you can use a certain number of evaluations for doing this, in each of the cases.

468
00:23:27,280 --> 00:23:29,200
So you can- let's say you have this,

469
00:23:29,200 --> 00:23:31,360
um, k dimensional, uh,

470
00:23:31,360 --> 00:23:33,310
set of parameters that defined your policy,

471
00:23:33,310 --> 00:23:35,140
you try changing one of them a little bit,

472
00:23:35,140 --> 00:23:38,350
you repeat it, you get a bunch of samples for that, new policy.

473
00:23:38,350 --> 00:23:40,765
Um, you do that for all of the different dimensions,

474
00:23:40,765 --> 00:23:43,270
and now you have an approximation of the gradient.

475
00:23:43,270 --> 00:23:45,850
It's very simple, it's quite noisy, um,

476
00:23:45,850 --> 00:23:46,885
it's not particularly efficient,

477
00:23:46,885 --> 00:23:48,415
but, it can sometimes be effective.

478
00:23:48,415 --> 00:23:50,710
I mean it was one of the earlier demonstrations

479
00:23:50,710 --> 00:23:53,050
of how policy gradient methods could be very useful,

480
00:23:53,050 --> 00:23:54,700
in an RL context.

481
00:23:54,700 --> 00:23:56,680
Um, and the nice thing is that the policy itself

482
00:23:56,680 --> 00:23:58,840
doesn't have to be differentiable because,

483
00:23:58,840 --> 00:24:04,420
we're just doing sort of a finite difference approximation of the gradient [NOISE].

484
00:24:04,420 --> 00:24:07,570
So, one of the first examples that- I see- well,

485
00:24:07,570 --> 00:24:09,520
um, I think of when- I think of,

486
00:24:09,520 --> 00:24:11,170
sort of how policy, uh,

487
00:24:11,170 --> 00:24:14,395
gradient methods or how policy search methods can be really effective,

488
00:24:14,395 --> 00:24:17,230
is Peter Stone's work on doing,

489
00:24:17,230 --> 00:24:21,730
uh Robocup and who here's ever seen- like Robocup?

490
00:24:21,730 --> 00:24:23,860
Okay. A few people but not everybody.

491
00:24:23,860 --> 00:24:26,065
So let's see if we can, get up like a

492
00:24:26,065 --> 00:24:28,870
short demonstration of like what these robots look like.

493
00:24:28,870 --> 00:24:36,160
So, let's- ah, okay.

494
00:24:36,160 --> 00:24:37,465
So you probably can't see it do you?

495
00:24:37,465 --> 00:24:39,175
We won't do that right now, um,

496
00:24:39,175 --> 00:24:40,600
but essentially what you have is,

497
00:24:40,600 --> 00:24:42,355
there's a bunch of different leagues of Robocup.

498
00:24:42,355 --> 00:24:43,780
One of the goals has been that, um,

499
00:24:43,780 --> 00:24:45,070
I think by 2050,

500
00:24:45,070 --> 00:24:46,945
the goal is that, we're going to have, uh,

501
00:24:46,945 --> 00:24:49,240
a robotic soccer, uh,

502
00:24:49,240 --> 00:24:52,375
team that is going to be able to defeat- like able to,

503
00:24:52,375 --> 00:24:54,670
you know, win the, the World Cup.

504
00:24:54,670 --> 00:24:57,010
Um, so that's been one of the driving goals,

505
00:24:57,010 --> 00:24:59,035
of this Ro- the Robocup initiative.

506
00:24:59,035 --> 00:25:02,230
Uh, and there's lot of different leagues within this, and one of them is,

507
00:25:02,230 --> 00:25:03,910
these sort of quadruped robots,

508
00:25:03,910 --> 00:25:06,895
um, which try to score goals against each other.

509
00:25:06,895 --> 00:25:08,920
And one of the key challenges for this is,

510
00:25:08,920 --> 00:25:10,570
they look kind of like that.

511
00:25:10,570 --> 00:25:14,800
Um, and, you have to figure out the gait for walking, um,

512
00:25:14,800 --> 00:25:16,435
and you want them to be able to walk,

513
00:25:16,435 --> 00:25:18,535
quickly but you don't want them to fall over.

514
00:25:18,535 --> 00:25:20,680
Um, and so just simply that question of like,

515
00:25:20,680 --> 00:25:22,360
how do you optimize the gait,

516
00:25:22,360 --> 00:25:24,430
is an important question in order, to win,

517
00:25:24,430 --> 00:25:26,875
because you need your robots to move fast on the field.

518
00:25:26,875 --> 00:25:29,305
So, Peter Stone has been a leading person in,

519
00:25:29,305 --> 00:25:30,805
in Robocup for a long time.

520
00:25:30,805 --> 00:25:32,380
Um, and their goal was simply to,

521
00:25:32,380 --> 00:25:34,840
learn a fast way for these AIBOs to walk.

522
00:25:34,840 --> 00:25:37,030
Um, and to do it by,

523
00:25:37,030 --> 00:25:38,905
uh, real experience um,

524
00:25:38,905 --> 00:25:41,740
and data's really important here because it's expensive um,

525
00:25:41,740 --> 00:25:43,750
you have these robots walking back up forth and you

526
00:25:43,750 --> 00:25:46,210
want them to very quickly, optimize their gait.

527
00:25:46,210 --> 00:25:49,660
Um, and you don't want to have to keep changing batteries and things like that,

528
00:25:49,660 --> 00:25:52,270
so you really wanna do this with very little amounts of data.

529
00:25:52,270 --> 00:25:55,030
So, what they thought of doing in this case is sort of to,

530
00:25:55,030 --> 00:25:59,000
to do a parameterized policy and try to optimize those proper policies.

531
00:25:59,070 --> 00:26:02,170
So this is where significant domain knowledge

532
00:26:02,170 --> 00:26:04,270
came in and this is a way to inject domain knowledge.

533
00:26:04,270 --> 00:26:08,350
So they, um, specified it by this sort of continuous ellipse,

534
00:26:08,350 --> 00:26:11,215
of how gait works for,

535
00:26:11,215 --> 00:26:12,760
um, the small robot.

536
00:26:12,760 --> 00:26:16,060
And so they parameterized it by these 12 continuous parameters.

537
00:26:16,060 --> 00:26:20,335
And this completely defines the space of possible policies you could learn.

538
00:26:20,335 --> 00:26:22,225
This might not be optimal.

539
00:26:22,225 --> 00:26:26,200
Peter Stone and his group have a huge amount of experience on doing Robocup,

540
00:26:26,200 --> 00:26:28,180
um, at the time they were doing this paper and so

541
00:26:28,180 --> 00:26:30,400
they really had a lot of knowledge they could inject in here.

542
00:26:30,400 --> 00:26:34,045
And in some ways it's a way to provide sort of this hierarchical structure about,

543
00:26:34,045 --> 00:26:36,175
what sort of policies might be good.

544
00:26:36,175 --> 00:26:39,699
And then what they did is they did just this method of finite differencing,

545
00:26:39,699 --> 00:26:42,130
in order to try to optimize for all of these parameters.

546
00:26:42,130 --> 00:26:46,735
[NOISE] So, one of the important things here, um,

547
00:26:46,735 --> 00:26:50,650
is that all of their policy evaluations were going to be done on actual real robots,

548
00:26:50,650 --> 00:26:52,075
um, and they just wanted to,

549
00:26:52,075 --> 00:26:54,625
have people inter- intervene every once in a while,

550
00:26:54,625 --> 00:26:57,955
in order to, replace batteries which took- happened about once an hour.

551
00:26:57,955 --> 00:27:00,295
Um, and so they did it on three AIBOs,

552
00:27:00,295 --> 00:27:01,420
very small amount of hardware.

553
00:27:01,420 --> 00:27:04,300
Um, they did about 15 policies per iteration,

554
00:27:04,300 --> 00:27:07,375
and, they evaluated each policy three times.

555
00:27:07,375 --> 00:27:10,285
So, it's not very many but it can be a very noisy signal,

556
00:27:10,285 --> 00:27:13,385
um, and each iteration took about 7.5 minutes.

557
00:27:13,385 --> 00:27:16,095
So- and then they had to pick some learning rate.

558
00:27:16,095 --> 00:27:17,850
And so what do we see in this case?

559
00:27:17,850 --> 00:27:19,005
Well, we see that,

560
00:27:19,005 --> 00:27:21,750
in terms of the number of iterations that they have versus how

561
00:27:21,750 --> 00:27:25,710
quickly they're- of course you have to define your optimization criteria in this case,

562
00:27:25,710 --> 00:27:27,735
they're looking at speed of stable walking.

563
00:27:27,735 --> 00:27:30,285
Um, and a lot of people have been trying to

564
00:27:30,285 --> 00:27:32,730
figure out how to do these using hand tuning before,

565
00:27:32,730 --> 00:27:36,345
um, uh, including, so they're the UT Austin Villa team.

566
00:27:36,345 --> 00:27:40,680
Um, including, them- in the past people have found different ways to sort of hand tune,

567
00:27:40,680 --> 00:27:43,315
um, I don't know if we'll be using unsupervised learning et cetera.

568
00:27:43,315 --> 00:27:46,690
And you can see, as they do multiple iterations of trying to,

569
00:27:46,690 --> 00:27:49,525
search for a better policy using this finite difference method,

570
00:27:49,525 --> 00:27:51,775
that they get to faster than everything else.

571
00:27:51,775 --> 00:27:54,520
And this is not that many iterations, um,

572
00:27:54,520 --> 00:27:56,095
so this is something that was happening over,

573
00:27:56,095 --> 00:27:58,130
you know, a few hours.

574
00:27:58,650 --> 00:28:02,514
So, I think this was a really compelling example of how,

575
00:28:02,514 --> 00:28:05,620
policy gradient methods could really do much better than what we-

576
00:28:05,620 --> 00:28:09,385
had happened before and they didn't have to require an enormous amount of data.

577
00:28:09,385 --> 00:28:12,640
That's very different than probably what you're experiencing in assignment two,

578
00:28:12,640 --> 00:28:15,355
so this is, no total number of iterations.

579
00:28:15,355 --> 00:28:19,390
Um, uh, I think this was on the order of, let's see,

580
00:28:19,390 --> 00:28:23,110
like, this is on the order of, you know,

581
00:28:23,110 --> 00:28:24,640
tens to hundreds of policies,

582
00:28:24,640 --> 00:28:26,500
not millions and millions of steps.

583
00:28:26,500 --> 00:28:28,675
So these things can be very data efficient.

584
00:28:28,675 --> 00:28:31,510
But there was also a lot of information that was given.

585
00:28:31,510 --> 00:28:36,130
So, um, if you think about sort of like,

586
00:28:36,130 --> 00:28:37,825
uh, I have a little bit on here.

587
00:28:37,825 --> 00:28:40,360
So, in their paper they discussed

588
00:28:40,360 --> 00:28:42,775
sort of what was actually impacting performance in this case,

589
00:28:42,775 --> 00:28:44,770
and there are a lot of things that impact performance.

590
00:28:44,770 --> 00:28:46,510
So, um, you know,

591
00:28:46,510 --> 00:28:48,010
how do we start?

592
00:28:48,010 --> 00:28:50,650
Um, so I may have a sense of why, you know,

593
00:28:50,650 --> 00:28:56,080
why does the initial policy parameters used matter for this type of method? Yeah,

594
00:28:56,080 --> 00:28:59,040
Well, because we're not guaranteed to have a global optima,

595
00:28:59,040 --> 00:29:01,740
only a local optima, so your starting point is gonna

596
00:29:01,740 --> 00:29:04,955
affect which local optima you are able to find.

597
00:29:04,955 --> 00:29:06,835
Exactly. So what just said is that,

598
00:29:06,835 --> 00:29:08,680
um, because these methods are only guaranteed,

599
00:29:08,680 --> 00:29:10,660
particularly this method is only guaranteed, to,

600
00:29:10,660 --> 00:29:14,245
find a local optima and all of the sort of policy gradient style methods are.

601
00:29:14,245 --> 00:29:16,180
Um, then wherever you start you're gonna get to

602
00:29:16,180 --> 00:29:20,140
the closest local optima and you have no guarantee that that's the best global optima.

603
00:29:20,140 --> 00:29:22,720
Um, so it's important to either try lots of

604
00:29:22,720 --> 00:29:25,690
random restarts here in this case or to have domain knowledge.

605
00:29:25,690 --> 00:29:28,510
Um, another important question here is,

606
00:29:28,510 --> 00:29:31,615
how much you're perturbing sort of the size of your finite differences.

607
00:29:31,615 --> 00:29:35,830
And then I think, really most critical is this policy parameterization.

608
00:29:35,830 --> 00:29:39,070
Like just how are you writing down the space of possible policies

609
00:29:39,070 --> 00:29:42,070
that you can learn within because like, if that's not a good policy space,

610
00:29:42,070 --> 00:29:44,065
then you're just not going to learn anything.

611
00:29:44,065 --> 00:29:48,937
Um, yeah.

612
00:29:48,937 --> 00:29:50,115
on slide 26

613
00:29:50,115 --> 00:29:54,105
What is an open loop policy can you explain a bit more on that.

614
00:29:54,105 --> 00:29:56,460
Yeah. Um, uh, so,

615
00:29:56,460 --> 00:29:58,665
question was about the open loop policy part.

616
00:29:58,665 --> 00:30:01,530
So, these policies that we're learning don't have to be adaptive.

617
00:30:01,530 --> 00:30:03,960
And open loop policy is essentially a plan.

618
00:30:03,960 --> 00:30:06,270
It's a sequence of actions to take, um,

619
00:30:06,270 --> 00:30:09,000
regardless of any additional input that you might have.

620
00:30:09,000 --> 00:30:14,400
So, um, we typically have been thinking about policies as mappings from states to actions,

621
00:30:14,400 --> 00:30:17,610
but they can also just be a series of actions.

622
00:30:17,610 --> 00:30:19,800
And so, when we talk about an open loop policy,

623
00:30:19,800 --> 00:30:22,620
that's a non-reactive policy because it's

624
00:30:22,620 --> 00:30:26,505
just a sequence of actions that regardless of the state of the robot you just keep going.

625
00:30:26,505 --> 00:30:28,770
So, maybe there's a really large wind in the middle,

626
00:30:28,770 --> 00:30:32,070
and the robot's next action is the same whether there's a lot of wind or not.

627
00:30:32,070 --> 00:30:34,450
It doesn't have to be reactive.

628
00:30:39,560 --> 00:30:42,780
Okay. So, but in general,

629
00:30:42,780 --> 00:30:46,050
um, you know, finite differences is a reasonable thing to try.

630
00:30:46,050 --> 00:30:48,810
Um, often we're gonna want to use gradient information and leverage

631
00:30:48,810 --> 00:30:52,990
the fact that our policy for function is actually differentiable.

632
00:30:53,000 --> 00:30:55,845
So, what we're gonna do now is, um,

633
00:30:55,845 --> 00:30:58,500
compute the policy gradient analytically [NOISE] excuse me.

634
00:30:58,500 --> 00:30:59,895
This is most common, um,

635
00:30:59,895 --> 00:31:01,890
in most of the techniques that are used right now.

636
00:31:01,890 --> 00:31:04,860
Um, we're gonna assume it's differentiable wherever it is non-zero,

637
00:31:04,860 --> 00:31:07,275
um, and that we can explicitly compute this.

638
00:31:07,275 --> 00:31:11,920
So, when we say, what we- when we say know that means that this is computable.

639
00:31:12,620 --> 00:31:15,330
And we can compute this explicitly.

640
00:31:15,330 --> 00:31:16,950
And so, now we're gonna be, um,

641
00:31:16,950 --> 00:31:18,915
thinking only about gradient-based methods.

642
00:31:18,915 --> 00:31:20,220
And so, we're only,

643
00:31:20,220 --> 00:31:23,805
[NOISE] we're gonna only converge to a local optima.

644
00:31:23,805 --> 00:31:26,505
Hopefully, hopefully, we'll get to a local optima,

645
00:31:26,505 --> 00:31:29,130
that's the best we can hope for in this case.

646
00:31:29,130 --> 00:31:37,545
Okay. So, we're going to talk- people often talk about likelihood ratio policies,

647
00:31:37,545 --> 00:31:40,005
um, and they're gonna proceed as follows.

648
00:31:40,005 --> 00:31:42,270
So, let- we're thinking about the episodic case.

649
00:31:42,270 --> 00:31:44,550
So, we're gonna think about it as having, um, trajectories.

650
00:31:44,550 --> 00:31:46,365
So, state action reward,

651
00:31:46,365 --> 00:31:47,745
next state, et cetera,

652
00:31:47,745 --> 00:31:49,695
all the way out to some terminal state.

653
00:31:49,695 --> 00:31:52,270
So, this is where we terminate.

654
00:31:53,060 --> 00:31:58,710
And we're gonna use R of Tau to denote the sum of rewards for a trajectory.

655
00:31:58,710 --> 00:32:04,365
Okay. So, the policy value in this case

656
00:32:04,365 --> 00:32:10,170
is just gonna be the expected discounted sum of rewards we get by following this policy.

657
00:32:10,170 --> 00:32:14,100
And we can represent that as the probability that we

658
00:32:14,100 --> 00:32:19,030
observe a particular trajectory times the reward of that trajectory.

659
00:32:19,460 --> 00:32:22,845
So, it just says given under this policy what are the, you know,

660
00:32:22,845 --> 00:32:24,825
what's the probability of seeing any trajectory,

661
00:32:24,825 --> 00:32:27,465
and then what would be the reward of that trajectory?

662
00:32:27,465 --> 00:32:31,140
Because the reward is the deterministic function of the trajectory.

663
00:32:31,140 --> 00:32:33,330
Once you know the state action rewards, et cetera,

664
00:32:33,330 --> 00:32:34,755
then your reward is,

665
00:32:34,755 --> 00:32:37,245
um, just the sum of all of those.

666
00:32:37,245 --> 00:32:40,244
And so now, in this particular notation,

667
00:32:40,244 --> 00:32:43,260
what our goal will be is to find policy parameters Theta,

668
00:32:43,260 --> 00:32:46,770
which, um are the arg max of this.

669
00:32:46,770 --> 00:32:50,580
Uh, and the reason we sort of- what have we changed here, um,

670
00:32:50,580 --> 00:32:55,035
the change now then has been the fact that we've gonna focus on here.

671
00:32:55,035 --> 00:32:59,085
So, notice now that the policy parameters only appear in terms of

672
00:32:59,085 --> 00:33:04,420
the distribution of trajectories that we might encounter under this policy.

673
00:33:04,640 --> 00:33:07,860
And this is, again, a little bit similar to what we talked

674
00:33:07,860 --> 00:33:10,380
about for imitation learning before or where in imitation learning,

675
00:33:10,380 --> 00:33:14,730
we talked a lot about distributions of states and distributions of states and actions,

676
00:33:14,730 --> 00:33:18,540
and trying to find a policy that would match the same state action distribution,

677
00:33:18,540 --> 00:33:20,985
as what was demonstrated by an expert.

678
00:33:20,985 --> 00:33:23,550
Um, today, we're not gonna talk as much about

679
00:33:23,550 --> 00:33:26,070
sort of state action distributions but we are talking

680
00:33:26,070 --> 00:33:27,810
about sort of distributions of

681
00:33:27,810 --> 00:33:31,870
trajectories that we could encounter under our particular policy.

682
00:33:33,080 --> 00:33:35,790
So, what's the gradient of this?

683
00:33:35,790 --> 00:33:39,930
Um, so, we wanna take the gradient of this function with respect to Theta.

684
00:33:39,930 --> 00:33:43,020
So, we're gonna go for this as follows.

685
00:33:43,020 --> 00:33:47,190
We are gonna rewrite what is the probability of a trajectory under Theta.

686
00:33:47,190 --> 00:33:53,625
So, sum over Tau.

687
00:33:53,625 --> 00:34:02,490
I wanna do probability of Tau [NOISE] times.

688
00:34:02,490 --> 00:34:06,010
All right, first actually I'll whip it in here.

689
00:34:21,469 --> 00:34:24,224
And then what we're gonna do is,

690
00:34:24,225 --> 00:34:30,150
make sure I get the notation the same.

691
00:34:30,150 --> 00:34:31,900
Okay.

692
00:34:40,850 --> 00:34:43,350
So, then what we're gonna do is,

693
00:34:43,350 --> 00:34:47,550
we're gonna do something simple where we just multiply and divide by the same thing.

694
00:34:47,550 --> 00:34:51,045
So, we're gonna put in probability of Tau given Theta,

695
00:34:51,045 --> 00:34:54,389
divided by probability of Tau given Theta,

696
00:34:54,389 --> 00:34:59,085
times the derivative of the probability of Tau given Theta.

697
00:34:59,085 --> 00:35:04,755
And the probability- if we instead had a log.

698
00:35:04,755 --> 00:35:10,170
So, if you're taking the derivative of log of probability of Tau given Theta,

699
00:35:10,170 --> 00:35:16,680
that is exactly equal to the one over the probability

700
00:35:16,680 --> 00:35:23,085
of Tau given Theta times the derivative of p of Tau given Theta.

701
00:35:23,085 --> 00:35:26,010
So, we can re-express this as follows;

702
00:35:26,010 --> 00:35:28,950
Sum over Tau r of Tau,

703
00:35:28,950 --> 00:35:39,160
p of Tau given Theta times derivative with respect to log of p of Tau given Theta.

704
00:35:40,910 --> 00:35:45,420
Now, so far that doesn't necessarily seem like that's gonna be very useful.

705
00:35:45,420 --> 00:35:46,590
Um, [LAUGHTER] So, we've done that,

706
00:35:46,590 --> 00:35:47,970
that's a reasonable transformation,

707
00:35:47,970 --> 00:35:51,090
but we'll see shortly why that transformation is helpful.

708
00:35:51,090 --> 00:35:53,250
And in particular, the reason this transformation is

709
00:35:53,250 --> 00:35:55,650
helpful is it's gonna be very useful when we think

710
00:35:55,650 --> 00:36:00,405
about wanting to do all of this without knowing the dynamics or reward models.

711
00:36:00,405 --> 00:36:02,310
Um, so, we're gonna need to be able to, you know,

712
00:36:02,310 --> 00:36:04,740
get reward in terms of, uh, a trajectory,

713
00:36:04,740 --> 00:36:06,945
but we wanna be able to evaluate, um,

714
00:36:06,945 --> 00:36:10,065
the gradient of a policy without knowing the dynamics model,

715
00:36:10,065 --> 00:36:12,940
and this trick is gonna help us get there.

716
00:36:14,270 --> 00:36:17,190
So, when, when we do this, this is often,

717
00:36:17,190 --> 00:36:20,145
this is often referred to as the likelihood ratio.

718
00:36:20,145 --> 00:36:22,320
And we can convert it and just say, "Well,

719
00:36:22,320 --> 00:36:23,670
we noticed that by doing this,

720
00:36:23,670 --> 00:36:26,440
this is actually exactly the same as the log."

721
00:36:26,660 --> 00:36:30,360
Now, why else does this start to look like something that might be useful?

722
00:36:30,360 --> 00:36:31,500
Well, what do we have here?

723
00:36:31,500 --> 00:36:34,725
We have, if we- this is the sum over all trajectories.

724
00:36:34,725 --> 00:36:37,560
Of course, we don't necessarily have access to all possible trajectories,

725
00:36:37,560 --> 00:36:38,940
but we can sample them.

726
00:36:38,940 --> 00:36:41,460
So, you could imagine starting to be able to approximate this

727
00:36:41,460 --> 00:36:44,010
by running your policy a number of times,

728
00:36:44,010 --> 00:36:46,065
sampling a number of trajectories,

729
00:36:46,065 --> 00:36:47,910
looking at the reward of those,

730
00:36:47,910 --> 00:36:50,369
um, and then taking the derivative

731
00:36:50,369 --> 00:36:53,740
with respect to this probability of trajectory given Theta.

732
00:36:55,220 --> 00:37:00,060
So, typically we're gonna do this by just running the policy m times.

733
00:37:00,060 --> 00:37:03,630
Um, and then, that p of the- Tau given Theta,

734
00:37:03,630 --> 00:37:06,630
we're gonna just approximate that by the following.

735
00:37:06,630 --> 00:37:08,595
So, that part drops out,

736
00:37:08,595 --> 00:37:13,305
we're just gonna weigh all of the trajectories that we got during our sampling uniformly,

737
00:37:13,305 --> 00:37:15,660
and then we look at the reward of that trajectory,

738
00:37:15,660 --> 00:37:17,670
and the log of p of, um,

739
00:37:17,670 --> 00:37:20,980
[NOISE] uh, Tau given Theta.

740
00:37:22,370 --> 00:37:24,870
So, what is happening in this case?

741
00:37:24,870 --> 00:37:27,525
Okay. So, this is saying that the gradient is this sort of,

742
00:37:27,525 --> 00:37:30,135
um, [NOISE] uh, the reward that we get

743
00:37:30,135 --> 00:37:33,930
Um, times the log of the probability

744
00:37:33,930 --> 00:37:37,950
of that trajectory for the reward with associated word times Theta.

745
00:37:37,950 --> 00:37:40,620
So, what's happening in that case?

746
00:37:40,620 --> 00:37:43,770
So, in this case,

747
00:37:43,770 --> 00:37:46,215
we have a function which for our case is the reward,

748
00:37:46,215 --> 00:37:49,020
which is sort of measuring how good, um,

749
00:37:49,020 --> 00:37:50,760
that particular, um, you know,

750
00:37:50,760 --> 00:37:52,785
trajectory is or how good that sample is.

751
00:37:52,785 --> 00:37:55,980
And so, what this is doing is we're just moving up and

752
00:37:55,980 --> 00:38:00,915
the trajectory of the log probability of that sample based on how good it is.

753
00:38:00,915 --> 00:38:03,990
So, we wanna sort of push up our parameters,

754
00:38:03,990 --> 00:38:08,955
that, um, are responsible for us getting samples which are good.

755
00:38:08,955 --> 00:38:12,810
So, um, we want to have parameters in our policy

756
00:38:12,810 --> 00:38:16,990
that are gonna cause us to execute trajectories that give us high reward.

757
00:38:18,920 --> 00:38:23,580
So, if we think of just sort of here f of x again is the reward.

758
00:38:23,580 --> 00:38:27,870
And we are- this is gonna be our policy or parameterized policy.

759
00:38:27,870 --> 00:38:35,110
We want to increase the weight of things in our space that lead to high reward.

760
00:38:35,110 --> 00:38:37,110
So, if this is our f of x,

761
00:38:37,110 --> 00:38:40,350
which is our reward function and this is the probability of our trajectories,

762
00:38:40,350 --> 00:38:42,540
then we wanna reweight our policy to try to

763
00:38:42,540 --> 00:38:45,940
increase the probability of trajectories that yield high reward.

764
00:38:45,940 --> 00:38:48,140
So, you would end up having

765
00:38:48,140 --> 00:38:52,830
larger gradients towards things that have high value, high reward.

766
00:38:56,450 --> 00:38:58,710
Okay. So, then the next question is,

767
00:38:58,710 --> 00:39:02,430
if I'm gonna do this then I have to be able to approximate the second term,

768
00:39:02,430 --> 00:39:04,380
which is this log.

769
00:39:04,380 --> 00:39:07,980
You know, the derivative with respect to the probability of a trajectory,

770
00:39:07,980 --> 00:39:09,885
um, under some parameters.

771
00:39:09,885 --> 00:39:11,640
So, I have to be able to figure out what is

772
00:39:11,640 --> 00:39:14,580
the probability of a trajectory under a set of parameters,

773
00:39:14,580 --> 00:39:16,510
and we can do that as follows.

774
00:39:16,510 --> 00:39:21,725
And so, this is gonna be Delta Theta of

775
00:39:21,725 --> 00:39:27,390
log of the pro- of Mu of S_0.

776
00:39:27,390 --> 00:39:29,220
So, this is our initial starting state,

777
00:39:29,220 --> 00:39:32,020
the probability of our initial starting state,

778
00:39:32,300 --> 00:39:39,690
times the product over j equals 0 to t minus 1 of the probability of

779
00:39:39,690 --> 00:39:48,360
observing the next state given the action that was taken,

780
00:39:48,360 --> 00:39:53,230
times the probability of taking that action under our current policy.

781
00:39:59,030 --> 00:40:04,065
So, there's like another bracket at the end.

782
00:40:04,065 --> 00:40:05,610
Um, and so, since this is log,

783
00:40:05,610 --> 00:40:07,140
we can just decompose this.

784
00:40:07,140 --> 00:40:12,510
So, this is gonna be equal to Delta Theta of log of

785
00:40:12,510 --> 00:40:19,320
Mu of S naught plus Delta Theta sum over Delta Theta because it's a log term,

786
00:40:19,320 --> 00:40:26,175
of J equals 0 to t minus 1 of log of [NOISE] the transition model.

787
00:40:26,175 --> 00:40:28,830
Remember, we don't know this in general. This is unknown.

788
00:40:28,830 --> 00:40:30,210
You're just gonna hopefully end up with

789
00:40:30,210 --> 00:40:32,955
an expression which means we don't need to have it.

790
00:40:32,955 --> 00:40:37,480
Um, and what i is indexing here is which trajectory we're on.

791
00:40:40,700 --> 00:40:45,345
Add sum over J equals 0 to t minus 1.

792
00:40:45,345 --> 00:40:50,350
And this is gonna be our actual policy parameters.

793
00:40:54,320 --> 00:40:57,540
All right, can anybody me why this is a useful decomposition?

794
00:40:57,540 --> 00:41:00,435
And whether or not it looks like we're gonna need to, so,

795
00:41:00,435 --> 00:41:04,830
let me just parameterize all these things,

796
00:41:04,830 --> 00:41:10,140
um, does this look hopeful in terms of us not needing to know what the dynamics model is?

797
00:41:10,140 --> 00:41:25,680
[inaudible]

798
00:41:25,680 --> 00:41:26,910
How about everybody just take a second,

799
00:41:26,910 --> 00:41:27,990
talk to your neighbor and,

800
00:41:27,990 --> 00:41:31,450
um, then tell me which of these terms are gonna be zero.

801
00:41:34,010 --> 00:41:37,410
So we're taking the derivative with respect to Theta.

802
00:41:37,410 --> 00:41:52,070
And which of these terms depend on Theta

803
00:41:52,070 --> 00:42:24,660
[OVERLAPPING].

804
00:42:24,660 --> 00:42:27,450
Remember Theta is what determines your policy parameters.

805
00:42:27,450 --> 00:42:31,360
Theta is what determines what action you take in a given state.

806
00:42:32,750 --> 00:42:35,670
All right I'm gonna do a quick poll, um,

807
00:42:35,670 --> 00:42:40,350
so I'm gonna call these items one, two and three.

808
00:42:40,350 --> 00:42:42,240
Does the first term depend on Theta?

809
00:42:42,240 --> 00:42:45,270
Raise your hand if yes, raise your hand if no.

810
00:42:45,270 --> 00:42:49,350
Great, okay, yeah. So this is independent Theta. So this is gonna be zero.

811
00:42:49,350 --> 00:42:53,325
Raise your hand if the second term is independent of Theta.

812
00:42:53,325 --> 00:42:55,800
Great, so this goes to zero.

813
00:42:55,800 --> 00:42:58,875
So the only thing we have less is this, which is great.

814
00:42:58,875 --> 00:43:01,980
So, um, the nice thing and so now it sort of becomes

815
00:43:01,980 --> 00:43:05,415
more clear why we did this weird log, um, transformation,

816
00:43:05,415 --> 00:43:07,530
because when we did this weird log transformation,

817
00:43:07,530 --> 00:43:09,570
it allowed us to take this product of

818
00:43:09,570 --> 00:43:13,455
the probability of the action that we took in the state transitions,

819
00:43:13,455 --> 00:43:16,035
then instead we can decompose it into sums.

820
00:43:16,035 --> 00:43:19,065
And now once we see that we decompose it into sums,

821
00:43:19,065 --> 00:43:22,020
we can apply the derivative separately and that means some of these terms

822
00:43:22,020 --> 00:43:25,035
just directly disappear which is really cool.

823
00:43:25,035 --> 00:43:27,990
So, it means that we don't actually need to know what the transition model is.

824
00:43:27,990 --> 00:43:29,925
Um, we don't need to have a explicit representative.

825
00:43:29,925 --> 00:43:31,540
Yeah question and name first please.

826
00:43:31,540 --> 00:43:33,845
And the question is, I was wondering,

827
00:43:33,845 --> 00:43:38,765
doesn't the dynamics of the system depend on the policy though, um, in general?

828
00:43:38,765 --> 00:43:40,040
Great question. So question is,

829
00:43:40,040 --> 00:43:42,230
does the dynamics of the system depend on the policy?

830
00:43:42,230 --> 00:43:45,460
Absolutely, but only through this part.

831
00:43:45,460 --> 00:43:47,355
So, it's like um,

832
00:43:47,355 --> 00:43:49,050
the agent gets to pick what action they take,

833
00:43:49,050 --> 00:43:50,100
but once they pick that,

834
00:43:50,100 --> 00:43:53,895
the dynamics is independent of the agent and so it's this de-coupling.

835
00:43:53,895 --> 00:43:55,920
So, if you have a different policy,

836
00:43:55,920 --> 00:43:57,810
you will absolutely get different trajectories

837
00:43:57,810 --> 00:44:00,135
but the way in which you get different trajectories,

838
00:44:00,135 --> 00:44:03,270
um, is only affected by the policy in terms of the actions that are

839
00:44:03,270 --> 00:44:06,870
selected and then the environment will determine sort of the next states that you get.

840
00:44:06,870 --> 00:44:09,945
And so we don't need to know that in terms of, um,

841
00:44:09,945 --> 00:44:13,695
estimating the impact of the actions on the environment.

842
00:44:13,695 --> 00:44:16,185
It will also come through in terms of the rewards you get.

843
00:44:16,185 --> 00:44:18,570
Because the rewards you get are also a function of the state.

844
00:44:18,570 --> 00:44:21,390
So you'll absolutely visit different parts of the state depending on the actions you

845
00:44:21,390 --> 00:44:25,000
take. Any other questions?

846
00:44:26,210 --> 00:44:30,390
I'm and I just want to make sure I understand how I get

847
00:44:30,390 --> 00:44:34,500
the estimate for the probability of how given Theta, I mean,

848
00:44:34,500 --> 00:44:38,760
most likely we are just saying if we took m episodes and this one showed up,

849
00:44:38,760 --> 00:44:43,395
you know, i times it's gonna be i over m. It is that what we are doing here is correct?

850
00:44:43,395 --> 00:44:46,395
Great question. So, is asking, um, you know,

851
00:44:46,395 --> 00:44:49,410
this is, what I put here is

852
00:44:49,410 --> 00:44:52,590
just one of those internal terms that would cover one i, yes,

853
00:44:52,590 --> 00:44:54,960
So, what we're doing here is, we're saying, we're gonna take this policy.

854
00:44:54,960 --> 00:44:56,325
We're gonna run it m times.

855
00:44:56,325 --> 00:44:58,785
We're probably not gonna get any trajectories that are identical.

856
00:44:58,785 --> 00:45:01,170
And what we're gonna do is compute this log of

857
00:45:01,170 --> 00:45:03,795
the probability of trajectory for each of those separately.

858
00:45:03,795 --> 00:45:06,490
And then sum them up.

859
00:45:06,800 --> 00:45:08,865
You might end up with that, I mean,

860
00:45:08,865 --> 00:45:10,905
you know in deterministic cases you might, um,

861
00:45:10,905 --> 00:45:13,770
if your domain doesn't have a lot of stochasticity and neither does your policy,

862
00:45:13,770 --> 00:45:16,215
you might end up with multiple trajectories that are identical.

863
00:45:16,215 --> 00:45:18,630
In general your trajectories will be completely different.

864
00:45:18,630 --> 00:45:22,330
And so will your estimate of their local gradients.

865
00:45:23,330 --> 00:45:25,650
So, this is really nice.

866
00:45:25,650 --> 00:45:28,965
We're gonna end up with this situation where we only have to be able to have

867
00:45:28,965 --> 00:45:33,510
an analytic form for the derivative of our policy with respect to our parameters.

868
00:45:33,510 --> 00:45:37,005
So, we still need and we'll talk about this a little bit more later. We still need this.

869
00:45:37,005 --> 00:45:39,310
We have to evaluate this.

870
00:45:39,520 --> 00:45:43,820
This is about how we parametrized our policy.

871
00:45:43,820 --> 00:45:45,965
And if we want this to be analytic,

872
00:45:45,965 --> 00:45:48,530
we need to have parametrized our policy in a way that we can

873
00:45:48,530 --> 00:45:51,625
compute this exactly for any state and action.

874
00:45:51,625 --> 00:45:54,390
So, we'll talk more about some ways, you know,

875
00:45:54,390 --> 00:45:58,905
some policy parameterizations which make this computation analytic and nice.

876
00:45:58,905 --> 00:46:02,145
In other cases you might have to estimate this thing itself by,

877
00:46:02,145 --> 00:46:06,240
you know, brute force or computation or finite differences or something.

878
00:46:06,240 --> 00:46:09,420
But if we choose a particular form of parameterized policy,

879
00:46:09,420 --> 00:46:12,550
then this part is gonna be analytic.

880
00:46:13,040 --> 00:46:16,920
So, another thing is I don't find this, um,

881
00:46:16,920 --> 00:46:18,360
er, I don't find

882
00:46:18,360 --> 00:46:21,090
this additional terminology particularly helpful but it's used all over the place.

883
00:46:21,090 --> 00:46:23,820
So I wanna introduce it which is people often call,

884
00:46:23,820 --> 00:46:26,985
um, this part a score function.

885
00:46:26,985 --> 00:46:30,210
Just the score function which is not particularly helpful,

886
00:46:30,210 --> 00:46:33,480
I think but nevertheless is often used is called this.

887
00:46:33,480 --> 00:46:39,270
So that's the quantity that we were just talking about needing to be able to evaluate.

888
00:46:39,270 --> 00:46:41,160
So this really gets into,

889
00:46:41,160 --> 00:46:45,285
um, er, well, we'll write it out again.

890
00:46:45,285 --> 00:46:53,595
So, um, when we take the derivative of the value function we approximate that by getting

891
00:46:53,595 --> 00:46:57,870
m samples and we sum

892
00:46:57,870 --> 00:47:02,910
over i equals one to m. And we look at the reward for that treject- um,

893
00:47:02,910 --> 00:47:07,470
trajectory and then we sum

894
00:47:07,470 --> 00:47:14,530
over these per step score functions.

895
00:47:23,780 --> 00:47:26,535
Can everybody read that in the back?

896
00:47:26,535 --> 00:47:30,555
Yeah. Okay great. Yeah so these are sort of our score functions.

897
00:47:30,555 --> 00:47:32,400
And these are our score functions, um,

898
00:47:32,400 --> 00:47:35,865
that can be evaluated

899
00:47:35,865 --> 00:47:38,130
over every single state action pair that we

900
00:47:38,130 --> 00:47:41,680
saw and we do not need to know the dynamics model.

901
00:47:41,870 --> 00:47:47,790
So, the policy gradient theorem slightly generalizes this.

902
00:47:47,790 --> 00:47:49,365
How is it gonna generalize this?

903
00:47:49,365 --> 00:47:53,535
Note in this case, what we're doing here is we're- this is for the episodic setting.

904
00:47:53,535 --> 00:47:57,870
And this is for when we just take our raw, our raw reward functions.

905
00:47:57,870 --> 00:48:01,980
So we look at the sum of rewards for that trajectory and then, um,

906
00:48:01,980 --> 00:48:06,870
we weigh it by this sort of derivative with respect to our policy parameters.

907
00:48:06,870 --> 00:48:12,015
Um, it turns out that we can also slightly generalize this.

908
00:48:12,015 --> 00:48:14,010
And let's say I'm gonna call,

909
00:48:14,010 --> 00:48:15,779
so this is a value function,

910
00:48:15,779 --> 00:48:18,900
um, let's say that we had slightly different objective functions.

911
00:48:18,900 --> 00:48:21,720
We talked before about how we could have episodic reward

912
00:48:21,720 --> 00:48:25,005
or average reward per time step or average value.

913
00:48:25,005 --> 00:48:28,545
So, we could either have our objective function be equal to

914
00:48:28,545 --> 00:48:35,070
our normal value for episodic or we could have it equal to

915
00:48:35,070 --> 00:48:39,210
what I'm gonna call J AVR which is average reward per time

916
00:48:39,210 --> 00:48:46,900
step or we could have it as average value.

917
00:48:48,710 --> 00:48:51,780
Let's say we're always continuing and we want to

918
00:48:51,780 --> 00:48:54,480
average over the distribution of states that we encounter.

919
00:48:54,480 --> 00:48:56,760
So we can think about that. It's a good scenario too.

920
00:48:56,760 --> 00:48:59,100
It turns out that on all of those cases you can do

921
00:48:59,100 --> 00:49:02,430
a similar derivation to what we did here for the episodic case.

922
00:49:02,430 --> 00:49:08,205
And what we find is that we have the derivative of our objective function,

923
00:49:08,205 --> 00:49:11,520
which now can be kind of any one of these different objective functions,

924
00:49:11,520 --> 00:49:14,550
is equal to the expected value under that,

925
00:49:14,550 --> 00:49:19,380
the current policy of the derivative with respect to

926
00:49:19,380 --> 00:49:26,080
just those policy parameters times Q.

927
00:49:32,450 --> 00:49:38,255
And Sutton and Barto in Chapter 13 which we also reference on the schedule, um,

928
00:49:38,255 --> 00:49:42,790
have a nice discussion about a number of these different issues, um,

929
00:49:42,790 --> 00:49:45,510
and so again, we're not gonna talk too much about

930
00:49:45,510 --> 00:49:47,910
these slightly other different objective functions but just know that

931
00:49:47,910 --> 00:49:51,310
this all can be extended to the continuing case.

932
00:49:51,800 --> 00:49:56,130
Okay, so what we've said here so far is that we have

933
00:49:56,130 --> 00:49:59,610
this approximation where what we do is we just take our policy,

934
00:49:59,610 --> 00:50:01,785
we run it out phi m times,

935
00:50:01,785 --> 00:50:06,270
for each of those m times we get a whole sequence of states and actions and rewards.

936
00:50:06,270 --> 00:50:08,580
And then we average. And this is

937
00:50:08,580 --> 00:50:13,110
an unbiased estimate of the policy gradient but it's very noisy.

938
00:50:13,110 --> 00:50:19,710
So, this is gonna be unbiased and noisy.

939
00:50:21,810 --> 00:50:24,010
If you think about what we saw

940
00:50:24,010 --> 00:50:26,140
before for things like Monte Carlo methods,

941
00:50:26,140 --> 00:50:27,670
it should look vaguely familiar,

942
00:50:27,670 --> 00:50:28,810
same sort of spirit, right?

943
00:50:28,810 --> 00:50:31,540
We have, um, we're just running out our policy.

944
00:50:31,540 --> 00:50:32,920
We're gonna get some

945
00:50:32,920 --> 00:50:37,120
sum of rewards just like what we got in Monte Carlo, um, estimates.

946
00:50:37,120 --> 00:50:40,030
But, [NOISE] so, it'll be unbiased estimate of the gradient.

947
00:50:40,030 --> 00:50:41,890
So, it's unbiased estimate of the gradient,

948
00:50:41,890 --> 00:50:48,590
estimate of gradient. But noisy.

949
00:50:49,290 --> 00:50:52,660
So, what can make this actually practical?

950
00:50:52,660 --> 00:50:56,425
Um, there's a number of different techniques for doing that.

951
00:50:56,425 --> 00:50:58,600
Um, but some of the things we'll start to talk about today

952
00:50:58,600 --> 00:51:00,760
are temporal structure and baselines.

953
00:51:00,760 --> 00:51:03,420
[NOISE] Okay. So, how do we fix this?

954
00:51:03,420 --> 00:51:09,210
I'm gonna start to look at, you know, fixes, [NOISE] uh,

955
00:51:09,210 --> 00:51:19,810
temporal structure and baselines.

956
00:51:21,090 --> 00:51:23,950
And before we keep going on this, um,

957
00:51:23,950 --> 00:51:26,305
based on what I just said in terms of Monte Carlo estimates,

958
00:51:26,305 --> 00:51:28,840
um, what are some of you guys' ideas for how we

959
00:51:28,840 --> 00:51:32,005
could maybe reduce the variance of this estimate?

960
00:51:32,005 --> 00:51:35,080
Based on stuff we've seen so far in class.

961
00:51:35,080 --> 00:51:41,650
Like, what are the alternative to cut up Monte Carlo methods? Yeah?

962
00:51:41,650 --> 00:51:43,045
We could use bootstraps.

963
00:51:43,045 --> 00:51:44,140
Um, can I get your name first.

964
00:51:44,140 --> 00:51:44,860
Oh, I'm .

965
00:51:44,860 --> 00:51:47,350
? Yeah. What said is exactly, right.

966
00:51:47,350 --> 00:51:49,135
So, said we could use bootstrapping.

967
00:51:49,135 --> 00:51:51,400
Yeah. So, we've repeatedly seen that, um,

968
00:51:51,400 --> 00:51:53,320
we have this trade off between bias and variance,

969
00:51:53,320 --> 00:51:54,835
and that bootstrapping, um,

970
00:51:54,835 --> 00:51:58,570
like temporal difference methods that we saw in Q-learning that you're doing in DQN,

971
00:51:58,570 --> 00:52:00,325
can be helpful in, um,

972
00:52:00,325 --> 00:52:03,910
reducing variance and, and speeding the spread of information.

973
00:52:03,910 --> 00:52:06,985
So, yeah. So, we could absolutely do things like bootstrapping, um,

974
00:52:06,985 --> 00:52:13,329
to kind of replace R with something else or use,

975
00:52:13,329 --> 00:52:19,525
um, a covariate in addition to R. To try to reduce the variance of R. Okay.

976
00:52:19,525 --> 00:52:23,455
All right. So, what we're gonna do now is,

977
00:52:23,455 --> 00:52:25,660
we're first gonna do something that doesn't go all the way to

978
00:52:25,660 --> 00:52:27,910
there but tries to [NOISE] at

979
00:52:27,910 --> 00:52:32,800
least leverage the fact that we're in a temporal, temporal, um, process.

980
00:52:32,800 --> 00:52:35,650
[NOISE] Okay. Um, and for

981
00:52:35,650 --> 00:52:38,935
any of you who have played around with importance sampling before,

982
00:52:38,935 --> 00:52:40,450
this is closely related to,

983
00:52:40,450 --> 00:52:42,775
um, per-decision importance sampling.

984
00:52:42,775 --> 00:52:44,935
And basically, the, um,

985
00:52:44,935 --> 00:52:46,030
thing that we're going to exploit,

986
00:52:46,030 --> 00:52:48,040
is the fact that, um,

987
00:52:48,040 --> 00:52:51,855
the rewards, uh, can only,

988
00:52:51,855 --> 00:52:53,970
um, of the temporal structure domain.

989
00:52:53,970 --> 00:52:56,385
Oh, I'll write it out first. Okay. So, what we had before,

990
00:52:56,385 --> 00:52:58,995
is we said that, um,

991
00:52:58,995 --> 00:53:02,760
the derivative with respect to Theta of the expected value of

992
00:53:02,760 --> 00:53:09,100
Tau of the return is equal to the expected value under

993
00:53:09,100 --> 00:53:13,690
the trajectories you could get of the sum over t equals

994
00:53:13,690 --> 00:53:18,730
0 to t minus 1 of rt such as the sum of rewards you get

995
00:53:18,730 --> 00:53:23,650
[NOISE] times the sum over t equals

996
00:53:23,650 --> 00:53:28,600
0 to t minus 1 of your derivative with respect to your policy parameters.

997
00:53:28,600 --> 00:53:37,915
[NOISE] Um, that's what we had before.

998
00:53:37,915 --> 00:53:40,870
So, we just sum up all of our rewards, and then,

999
00:53:40,870 --> 00:53:44,470
we'd multiply that by the sum over all of the gradients

1000
00:53:44,470 --> 00:53:48,100
of our policy at every single action state pair we got in that trajectory.

1001
00:53:48,100 --> 00:53:52,765
[NOISE] Okay. So, let's think about doing this for a single reward,

1002
00:53:52,765 --> 00:53:56,215
instead of looking at the whole sum of rewards. So, let's just look at.

1003
00:53:56,215 --> 00:54:02,740
We take the derivative with respect to Theta of expected value of rt prime.

1004
00:54:02,740 --> 00:54:04,180
[NOISE] So, this is

1005
00:54:04,180 --> 00:54:08,335
just a single time step reward that we might encounter you know, along our trajectory,

1006
00:54:08,335 --> 00:54:11,605
and that's gonna be equal to the expected value of

1007
00:54:11,605 --> 00:54:19,765
rt prime times sum over t equals zero to t prime of this derivative.

1008
00:54:19,765 --> 00:54:23,470
So, this is gonna look almost exactly the same as before.

1009
00:54:23,470 --> 00:54:32,530
[NOISE] Except, the only key difference here is that I'm only summing up to t prime.

1010
00:54:32,530 --> 00:54:35,545
Okay. So, we're only summing up,

1011
00:54:35,545 --> 00:54:38,785
um, you could think of this as just like a shortened trajectory.

1012
00:54:38,785 --> 00:54:41,530
I'm looking at the product of, um, the states,

1013
00:54:41,530 --> 00:54:43,300
and the actions and the rewards that I,

1014
00:54:43,300 --> 00:54:47,860
I reached all the way up to when I got to, um, rt prime.

1015
00:54:47,860 --> 00:54:51,385
Okay. So, I don't have to sum all the way over the future ones.

1016
00:54:51,385 --> 00:54:53,980
So, we can take this expression,

1017
00:54:53,980 --> 00:54:56,725
and now, we can sum over all time steps.

1018
00:54:56,725 --> 00:54:59,065
So, this says, what's the expected reward, uh,

1019
00:54:59,065 --> 00:55:02,470
or the derivative with respect to the reward for time step t prime?

1020
00:55:02,470 --> 00:55:04,190
Now, I'm gonna just sum that,

1021
00:55:04,190 --> 00:55:07,155
and that's gonna be the same as my first expressions.

1022
00:55:07,155 --> 00:55:08,850
So, what I'm gonna do is I'm gonna say,

1023
00:55:08,850 --> 00:55:14,700
[NOISE] V of Theta is equal to the derivative with respect to Theta of er,

1024
00:55:14,700 --> 00:55:18,370
and I'm gonna sum up that internal expression.

1025
00:55:18,660 --> 00:55:25,240
So, I'm gonna sum over t prime is equal to zero to t minus 1 rt prime,

1026
00:55:25,240 --> 00:55:27,725
and then insert that second expression.

1027
00:55:27,725 --> 00:55:41,980
Okay. So, all I did is I put that in there

1028
00:55:41,980 --> 00:55:44,300
and I summed over t prime is equal to zero,

1029
00:55:44,300 --> 00:55:47,040
all the way up to t minus 1, and then,

1030
00:55:47,040 --> 00:55:49,470
what I'm gonna do is I'm going to

1031
00:55:49,470 --> 00:55:54,495
reorder this and this by making the following observation.

1032
00:55:54,495 --> 00:55:56,420
So, if we think about,

1033
00:55:56,420 --> 00:55:59,650
how many terms one of these particular,

1034
00:55:59,650 --> 00:56:03,790
um, log Pi Theta at st appears in?

1035
00:56:03,790 --> 00:56:05,440
So, if we look at, um,

1036
00:56:05,440 --> 00:56:12,320
[NOISE] log of Phi Theta of a_1 s_1.

1037
00:56:12,480 --> 00:56:15,655
So if you look at how many times that appears,

1038
00:56:15,655 --> 00:56:21,400
that appears for the early rewards and it appears for all the later rewards too.

1039
00:56:21,400 --> 00:56:23,590
Okay. This is going to appear for r_1,

1040
00:56:23,590 --> 00:56:25,135
it's going to appear for r_2,

1041
00:56:25,135 --> 00:56:28,660
it's gonna appear all the way up to rt for t minus 1.

1042
00:56:28,660 --> 00:56:33,745
Because we're always summing over everything before that t prime.

1043
00:56:33,745 --> 00:56:36,940
Okay. So, what we're gonna do now is we're gonna take

1044
00:56:36,940 --> 00:56:40,045
those terms and we're gonna just reorganize those.

1045
00:56:40,045 --> 00:56:42,550
So, some of these terms appear a whole bunch of times,

1046
00:56:42,550 --> 00:56:44,425
some of them, the last one,

1047
00:56:44,425 --> 00:56:49,450
the [NOISE] log of Pi Theta of at minus 1,

1048
00:56:49,450 --> 00:56:53,210
st minus 1, it's only gonna appear once.

1049
00:56:53,790 --> 00:56:59,020
It's only re-responsible for helping dictate the very [NOISE] final reward.

1050
00:56:59,020 --> 00:57:01,690
So, we can use that insight to just slightly,

1051
00:57:01,690 --> 00:57:04,000
um, reorganize this equation as follows.

1052
00:57:04,000 --> 00:57:06,160
[NOISE] So, now, we're gonna say this is equal to

1053
00:57:06,160 --> 00:57:12,325
the expected value of sum over t equals zero to t minus 1.

1054
00:57:12,325 --> 00:57:17,545
So, notice before, I put the t prime on the outside and the t was on the inside,

1055
00:57:17,545 --> 00:57:20,785
and now, what I'm gonna do is put the t on the outside,

1056
00:57:20,785 --> 00:57:22,600
and I'm gonna say,

1057
00:57:22,600 --> 00:57:27,610
[NOISE] got Delta Theta, log Pi Theta,

1058
00:57:27,610 --> 00:57:37,210
at st times sum over t prime is equal to t all the way to t minus 1 of rt prime.

1059
00:57:37,210 --> 00:57:40,570
Okay. So, all I've done is I've reorganized that sum.

1060
00:57:40,570 --> 00:57:41,965
Yes? Is that ?

1061
00:57:41,965 --> 00:57:42,265
Yeah.

1062
00:57:42,265 --> 00:57:42,670
Yeah.

1063
00:57:42,670 --> 00:57:44,905
Um, on the second line from the bottom,

1064
00:57:44,905 --> 00:57:46,720
is it's supposed to be the derivative that- is

1065
00:57:46,720 --> 00:57:48,865
that a value function [NOISE] with respect to Theta?

1066
00:57:48,865 --> 00:57:50,695
Um, at the very [NOISE] left.

1067
00:57:50,695 --> 00:57:51,580
Yeah.

1068
00:57:51,580 --> 00:57:52,795
Okay. Yes.

1069
00:57:52,795 --> 00:57:54,820
Oh sorry. You mean[OVERLAPPING] it's supposed to be the derivative of this?

1070
00:57:54,820 --> 00:57:55,450
Yes. [NOISE]

1071
00:57:55,450 --> 00:57:59,180
Yeah. Thank you.

1072
00:57:59,700 --> 00:58:04,150
Okay. So, what we've done in this case was we've reorganized the sum.

1073
00:58:04,150 --> 00:58:07,525
We-we've just recollected terms in a slightly different way.

1074
00:58:07,525 --> 00:58:10,300
But it's gonna be the- in a useful way.

1075
00:58:10,300 --> 00:58:13,105
So, [NOISE] let's move this up,

1076
00:58:13,105 --> 00:58:14,710
and I'll move this one down.

1077
00:58:14,710 --> 00:58:17,500
[NOISE].

1078
00:58:17,500 --> 00:58:18,970
Okay. So, right now,

1079
00:58:18,970 --> 00:58:20,950
we're still working on the temporal structure.

1080
00:58:20,950 --> 00:58:23,680
[NOISE] And what is this going to allow us to do?

1081
00:58:23,680 --> 00:58:25,555
Well that second term there,

1082
00:58:25,555 --> 00:58:26,860
should look somewhat familiar.

1083
00:58:26,860 --> 00:58:28,839
What that's saying here is that's saying,

1084
00:58:28,839 --> 00:58:32,125
what is the reward we get starting at time step,

1085
00:58:32,125 --> 00:58:34,690
uh, t all the way to the end?

1086
00:58:34,690 --> 00:58:36,835
And that's just the return.

1087
00:58:36,835 --> 00:58:39,295
So, we had previously defined that,

1088
00:58:39,295 --> 00:58:41,590
um, when we are talking about,

1089
00:58:41,590 --> 00:58:43,210
like, Monte Carlo methods, et cetera,

1090
00:58:43,210 --> 00:58:46,160
[NOISE] that we could always just look at,

1091
00:58:47,100 --> 00:58:50,260
um, rt prime at I.

1092
00:58:50,260 --> 00:58:53,605
This is just equal to the return.

1093
00:58:53,605 --> 00:58:59,470
The return for the rest of the episode starting in time step t on episode i.

1094
00:58:59,470 --> 00:59:03,910
So, that, that should look very familiar to what we had seen in Monte Carlo methods,

1095
00:59:03,910 --> 00:59:06,265
where we could always say from this state and action,

1096
00:59:06,265 --> 00:59:08,020
what was the sum of rewards we get

1097
00:59:08,020 --> 00:59:10,660
starting at that state and action until the end of the episode?

1098
00:59:10,660 --> 00:59:15,390
Okay. So, that means we can re-express the derivative [NOISE] with respect to

1099
00:59:15,390 --> 00:59:18,840
Theta as approximately one

1100
00:59:18,840 --> 00:59:22,860
over m sum over all of the trajectories and we're summing over,

1101
00:59:22,860 --> 00:59:25,810
sum over all the time steps.

1102
00:59:25,950 --> 00:59:30,070
The derivative with respect to Theta of our actual policy

1103
00:59:30,070 --> 00:59:36,110
parameter times just the return.

1104
00:59:40,140 --> 00:59:44,935
And this is gonna be a slightly lower variance estimate than before.

1105
00:59:44,935 --> 00:59:52,950
Okay. So, instead of us having to sort of separately sum up all of our words, and then,

1106
00:59:52,950 --> 00:59:58,320
we multiply that by the full sum of all of these derivatives at the logs,

1107
00:59:58,320 --> 01:00:04,585
we are only kind of needing to take the sum of the logs, um,

1108
01:00:04,585 --> 01:00:08,305
for some of the reward terms essentially,

1109
01:00:08,305 --> 01:00:11,530
and so, we can reduce the variance in that case.

1110
01:00:11,530 --> 01:00:14,200
Because in some ways, what this is doing, this is saying, like,

1111
01:00:14,200 --> 01:00:15,460
for every single reward,

1112
01:00:15,460 --> 01:00:17,890
because you could re-express this as a sum of rewards.

1113
01:00:17,890 --> 01:00:19,390
For every single one of those rewards,

1114
01:00:19,390 --> 01:00:20,665
you have to, um,

1115
01:00:20,665 --> 01:00:25,165
sum it by sort of the full trajectory in terms of the derivative of the gradient,

1116
01:00:25,165 --> 01:00:27,400
uh, the derivative of the policy parameters.

1117
01:00:27,400 --> 01:00:29,005
And now, we're saying, you don't have to,

1118
01:00:29,005 --> 01:00:30,670
uh, multiply that by all of those.

1119
01:00:30,670 --> 01:00:34,690
You only have to multiply it by the ones that are relevant for that particular reward.

1120
01:00:34,690 --> 01:00:38,170
That means that you're gonna have a slightly lower variance estimator.

1121
01:00:38,170 --> 01:00:40,840
[NOISE] Okay.

1122
01:00:41,580 --> 01:00:47,020
So, when we do this we can end up with what's known as REINFORCE which,

1123
01:00:47,020 --> 01:00:48,880
um, who here has heard of REINFORCE?

1124
01:00:48,880 --> 01:00:50,680
Yeah. Number of people, not everybody.

1125
01:00:50,680 --> 01:00:55,150
REINFORCE is one of the most common reinforcement learning policy gradient algorithms.

1126
01:00:55,150 --> 01:00:57,070
So, you get the REINFORCE algorithm.

1127
01:00:57,070 --> 01:01:02,065
[NOISE]

1128
01:01:02,065 --> 01:01:04,150
So how it works is you, um,

1129
01:01:04,150 --> 01:01:08,110
then the algorithm is you initialize in it, theta randomly.

1130
01:01:08,110 --> 01:01:14,860
[NOISE] You just always will have to first decide on how you're parameterizing your policy,

1131
01:01:14,860 --> 01:01:18,295
so somewhere you already defied- decided how you're parameterizing your policy.

1132
01:01:18,295 --> 01:01:21,235
Now, you're gonna set the values for that policy randomly.

1133
01:01:21,235 --> 01:01:23,635
And then for each episode,

1134
01:01:23,635 --> 01:01:26,080
so you're going to run an episode with that policy.

1135
01:01:26,080 --> 01:01:28,600
[NOISE] Episode.

1136
01:01:28,600 --> 01:01:33,310
[NOISE] And you're gonna gather a whole bunch of actions and rewards,

1137
01:01:33,310 --> 01:01:40,495
[NOISE] and this is sampled from your current policy.

1138
01:01:40,495 --> 01:01:44,860
So, your sample, your current policy according to,

1139
01:01:44,860 --> 01:01:46,810
um, sample from your current policy,

1140
01:01:46,810 --> 01:01:48,145
you get a trajectory,

1141
01:01:48,145 --> 01:01:51,550
and then for every step in that trajectory,

1142
01:01:51,550 --> 01:01:54,460
you're gonna update your policy parameters.

1143
01:01:54,460 --> 01:02:00,534
[NOISE] So, [NOISE] for every time step inside of that episode,

1144
01:02:00,534 --> 01:02:02,530
we're gonna update our policy parameters.

1145
01:02:02,530 --> 01:02:06,970
[NOISE] So, it's going to be the same as before times some learning rate.

1146
01:02:06,970 --> 01:02:14,390
[NOISE] I will not use W there, I use alpha, um,

1147
01:02:17,400 --> 01:02:22,765
times the derivative with respect to Theta, log Pi Theta,

1148
01:02:22,765 --> 01:02:29,125
st at Gt, where Gt is just in this episode,

1149
01:02:29,125 --> 01:02:33,940
what is the sum of rewards from st at onwards?

1150
01:02:33,940 --> 01:02:36,490
[NOISE].

1151
01:02:36,490 --> 01:02:39,160
So, that's the just the normal return like,

1152
01:02:39,160 --> 01:02:41,545
what we had with, um, Monte Carlo methods.

1153
01:02:41,545 --> 01:02:43,660
So, just like, what we did when we were estimating like,

1154
01:02:43,660 --> 01:02:45,085
the linear value function,

1155
01:02:45,085 --> 01:02:48,370
and we were using rewards from the state and action onwards.

1156
01:02:48,370 --> 01:02:50,770
We're going to do the same thing here except for, um,

1157
01:02:50,770 --> 01:02:54,400
we're going to be updating the policy parameters, and we do this many,

1158
01:02:54,400 --> 01:02:55,930
many, many times and then at the end,

1159
01:02:55,930 --> 01:02:58,420
we return the Theta parameters.

1160
01:02:58,420 --> 01:03:00,520
Yeah? [NOISE]

1161
01:03:00,520 --> 01:03:02,215
I have a question. So, for each episode,

1162
01:03:02,215 --> 01:03:04,960
do you sample from the updated, um, policy?

1163
01:03:04,960 --> 01:03:06,730
We're gonna talk with you. Hope you're ready.

1164
01:03:06,730 --> 01:03:08,590
Yes. Uh-um. [NOISE] Yeah.

1165
01:03:08,590 --> 01:03:10,630
So, what just asked is, right.

1166
01:03:10,630 --> 01:03:12,310
Um, so, in this case you, um,

1167
01:03:12,310 --> 01:03:16,825
I- after you do all the updates for one episode,

1168
01:03:16,825 --> 01:03:19,195
so you could do these incremental updates.

1169
01:03:19,195 --> 01:03:21,100
Um, I- and then,

1170
01:03:21,100 --> 01:03:23,905
a- at the end of doing all of your incremental updates,

1171
01:03:23,905 --> 01:03:29,845
then you get another episode with your new updating parameters. Yeah, ?

1172
01:03:29,845 --> 01:03:32,710
Um, since we're doing every time updates would this be a biased method?

1173
01:03:32,710 --> 01:03:34,900
Um, good question.

1174
01:03:34,900 --> 01:03:37,955
So, since we're doing every time estimates, um,

1175
01:03:37,955 --> 01:03:41,859
this should be an unbiased estimate of the,

1176
01:03:41,859 --> 01:03:45,190
um, I- It should still be an unbiased estimate of the gradient.

1177
01:03:45,190 --> 01:03:47,770
It's stochastic, um, but, um,

1178
01:03:47,770 --> 01:03:52,915
we- there's not a notion of state and actions in the same way.

1179
01:03:52,915 --> 01:03:56,605
Um, this will be asymptotically consistent. It's a good question.

1180
01:03:56,605 --> 01:04:00,475
So, the notion of, um,

1181
01:04:00,475 --> 01:04:02,710
a state and action in this case is different

1182
01:04:02,710 --> 01:04:05,380
because we have just these policy parameters.

1183
01:04:05,380 --> 01:04:08,755
So, we're not estimating the value of a state and action here.

1184
01:04:08,755 --> 01:04:14,500
Um, so, this is certainly asymptotically consistent.

1185
01:04:14,500 --> 01:04:16,285
I think it's still just unbiased.

1186
01:04:16,285 --> 01:04:19,450
Um, if I, if I reconsider that later,

1187
01:04:19,450 --> 01:04:21,130
I'll send a Piazza post, um,

1188
01:04:21,130 --> 01:04:24,920
but I think it's still just an unbiased estimate of the gradient.

1189
01:04:25,080 --> 01:04:34,330
It's a good question. Okay. So, I go back to my slide notes.

1190
01:04:34,330 --> 01:04:37,420
Um, I think the last thing I just wanted- well,

1191
01:04:37,420 --> 01:04:38,500
I'll- I mention the, uh,

1192
01:04:38,500 --> 01:04:39,700
probably the things will, um,

1193
01:04:39,700 --> 01:04:43,495
[NOISE] one critical question here is, whether or not,

1194
01:04:43,495 --> 01:04:47,665
or how to compute this differential with respect to the policy parameters?

1195
01:04:47,665 --> 01:04:49,960
So, I think it's useful to talk about, you know,

1196
01:04:49,960 --> 01:04:53,140
what are the classes of policies that often people consider,

1197
01:04:53,140 --> 01:04:56,365
[NOISE], um, that have nice differentiable forms.

1198
01:04:56,365 --> 01:04:59,950
So, um, some of the classes people considers are things like,

1199
01:04:59,950 --> 01:05:02,080
Softmax, Gaussians, and Neural networks.

1200
01:05:02,080 --> 01:05:03,580
Those are probably the most common.

1201
01:05:03,580 --> 01:05:05,290
So, I- what do I mean by that?

1202
01:05:05,290 --> 01:05:06,370
I mean, that's how we're [NOISE] going to actually,

1203
01:05:06,370 --> 01:05:08,125
just parameterize our policy.

1204
01:05:08,125 --> 01:05:10,825
So, let's just look at an example.

1205
01:05:10,825 --> 01:05:20,155
So, Softmax is where we simply [NOISE] have a linear combination of features,

1206
01:05:20,155 --> 01:05:21,790
and we take sort of, um,

1207
01:05:21,790 --> 01:05:23,230
an exponential weight of them.

1208
01:05:23,230 --> 01:05:28,570
So, what we're gonna do is we're gonna have some features of our state and action space,

1209
01:05:28,570 --> 01:05:33,640
and we're gonna [NOISE] multiply them by some weights or parameters.

1210
01:05:33,640 --> 01:05:35,320
These are our parameters.

1211
01:05:35,320 --> 01:05:41,755
[NOISE] And then, to actually get a probability of taking an action,

1212
01:05:41,755 --> 01:05:44,770
so if we want to have our policy where we say,

1213
01:05:44,770 --> 01:05:47,260
what is the probability of action given the state?

1214
01:05:47,260 --> 01:05:50,950
We're gonna take the exponential of these weighted features.

1215
01:05:50,950 --> 01:05:56,230
So, we have e [NOISE] to the phi t Theta,

1216
01:05:56,230 --> 01:05:57,640
divided by the sum

1217
01:05:57,640 --> 01:06:07,390
over all actions [NOISE].

1218
01:06:07,390 --> 01:06:11,200
So notice, this is a reasonable thing to do when our action space is discrete,

1219
01:06:11,200 --> 01:06:17,950
action space discrete [NOISE].

1220
01:06:17,950 --> 01:06:19,030
So, [NOISE] a lot of  Atari games,

1221
01:06:19,030 --> 01:06:20,140
a lot of different, um,

1222
01:06:20,140 --> 01:06:22,435
scenarios you do have a discrete action space, um,

1223
01:06:22,435 --> 01:06:26,275
so you could just take this exponential, divide by the normalized.

1224
01:06:26,275 --> 01:06:29,635
Um, sum over the exponential,

1225
01:06:29,635 --> 01:06:33,835
and that immediately yields our parameterized policy class.

1226
01:06:33,835 --> 01:06:35,980
And so, then, um,

1227
01:06:35,980 --> 01:06:39,760
if we want to be able to take the derivative of this with respect to the log,

1228
01:06:39,760 --> 01:06:42,730
that's quite nice because we have exponentials here,

1229
01:06:42,730 --> 01:06:43,750
and we have log term.

1230
01:06:43,750 --> 01:06:45,325
We're taking the log of this.

1231
01:06:45,325 --> 01:06:47,980
So, um, we want to be able to compute

1232
01:06:47,980 --> 01:06:51,400
this term from this sort of parameterized policy class.

1233
01:06:51,400 --> 01:06:55,915
What we get is the derivative with respect to Theta of log of this type of

1234
01:06:55,915 --> 01:07:02,710
parameterized policy [NOISE] is just equal to our features [NOISE].

1235
01:07:02,710 --> 01:07:05,455
So, this is whatever feature representation we're using,

1236
01:07:05,455 --> 01:07:07,435
like in the case of the, um, uh,

1237
01:07:07,435 --> 01:07:11,125
locomotion robotic case, this would be like all the different.

1238
01:07:11,125 --> 01:07:14,140
Um, uh, this could be something like,

1239
01:07:14,140 --> 01:07:16,690
you know, angles, or joints, or things like that.

1240
01:07:16,690 --> 01:07:22,300
Um, so, this is whatever featurization we're using minus the expected value for

1241
01:07:22,300 --> 01:07:28,150
Theta of [NOISE] your parameters,

1242
01:07:28,150 --> 01:07:31,495
um, with an exponent with, um,

1243
01:07:31,495 --> 01:07:35,245
your expected value [NOISE] over all the actions you might take under that policy.

1244
01:07:35,245 --> 01:07:36,820
So, it's sort of saying the features that you

1245
01:07:36,820 --> 01:07:39,760
observed versus the sort of average feature,

1246
01:07:39,760 --> 01:07:42,860
average, average over the action.

1247
01:07:43,950 --> 01:07:46,600
Okay? So, that's differentiable.

1248
01:07:46,600 --> 01:07:48,940
Um, and you can solve it,

1249
01:07:48,940 --> 01:07:50,830
then it gives you an analytic form.

1250
01:07:50,830 --> 01:07:53,890
Um, another thing that's really popular is a Gaussian policy.

1251
01:07:53,890 --> 01:07:58,630
[NOISE] And why might this be good?

1252
01:07:58,630 --> 01:08:00,100
Well, this might be good because often,

1253
01:08:00,100 --> 01:08:01,750
we have continuous action spaces.

1254
01:08:01,750 --> 01:08:03,760
So, this is good if we have discrete action spaces.

1255
01:08:03,760 --> 01:08:05,875
Often, we have continuous actions spaces.

1256
01:08:05,875 --> 01:08:08,080
This is very common in,

1257
01:08:08,080 --> 01:08:10,540
um, controls and robotics.

1258
01:08:10,540 --> 01:08:14,170
[NOISE] So, you have a number of

1259
01:08:14,170 --> 01:08:16,465
different parameters and i- in

1260
01:08:16,465 --> 01:08:19,854
their continuous scalar values that you wanna be able to set.

1261
01:08:19,854 --> 01:08:22,629
Um, and what we could say here, let's say,

1262
01:08:22,630 --> 01:08:30,220
we use mu of s. It might be a linear combination of state features, times some parameter.

1263
01:08:30,220 --> 01:08:33,805
Okay. And let's imagine for simplicity right now that,

1264
01:08:33,805 --> 01:08:36,340
um, we have a variance but that, that's static.

1265
01:08:36,340 --> 01:08:40,509
So, we could also consider the case where it's not,

1266
01:08:40,509 --> 01:08:43,180
but we're gonna assume that we have some variance term that is fixed,

1267
01:08:43,180 --> 01:08:45,880
[NOISE] so this is not a parameter.

1268
01:08:45,880 --> 01:08:47,380
This is not something we're gonna try to learn.

1269
01:08:47,380 --> 01:08:51,205
We're just gonna be trying to learn the Theta that's defining our mu function,

1270
01:08:51,205 --> 01:08:53,740
and then our policy is gonna be a Gaussian.

1271
01:08:53,740 --> 01:09:00,354
So, a is going to be drawn from a Gaussian using this mean per features.

1272
01:09:00,354 --> 01:09:06,774
Okay. So, we compare our current state to the mean,

1273
01:09:06,774 --> 01:09:10,179
and then we select an action with respect to that,

1274
01:09:10,180 --> 01:09:14,020
and the score function in this case is the derivative of this Gaussian.

1275
01:09:14,020 --> 01:09:17,680
[NOISE] So, score's a derivative of,

1276
01:09:17,680 --> 01:09:20,020
uh, of this Gaussian function [NOISE],

1277
01:09:20,020 --> 01:09:27,250
which ends up being a minus mu of s times our parameterization

1278
01:09:27,250 --> 01:09:35,080
of the state features divided by sigma squared [NOISE].

1279
01:09:35,080 --> 01:09:37,359
So, again, it can be done analytically.

1280
01:09:37,359 --> 01:09:40,329
And the other really common one is deep neural networks.

1281
01:09:40,330 --> 01:09:44,979
So, those are, um, [NOISE] those are- those are sort of very common forms,

1282
01:09:44,979 --> 01:09:47,664
um, I- that people use.

1283
01:09:47,665 --> 01:09:51,040
We're gonna talk next time about another common way.

1284
01:09:51,040 --> 01:09:53,680
Before we finish, um, we're gonna do,

1285
01:09:53,680 --> 01:09:55,225
um, spend about five minutes,

1286
01:09:55,225 --> 01:09:57,100
um, to do some early class feedback.

1287
01:09:57,100 --> 01:09:59,740
It's really helpful for us to figure out what's helping you learn,

1288
01:09:59,740 --> 01:10:01,270
what things do you think could be improved?

1289
01:10:01,270 --> 01:10:03,880
Um, so, what I'm going to do right now is open,

1290
01:10:03,880 --> 01:10:05,665
if you guys could go to Piazza.

1291
01:10:05,665 --> 01:10:08,560
Um, It would be great if you could fill this out.

1292
01:10:08,560 --> 01:10:09,910
All everything is anonymous.

1293
01:10:09,910 --> 01:10:13,780
Um, and my goal is to get feedback for you guys on Wednesday about this.

1294
01:10:13,780 --> 01:10:16,360
So, just- let me see if I could redo that.

1295
01:10:16,360 --> 01:10:19,675
So, it's posted.

1296
01:10:19,675 --> 01:10:21,850
Okay. Let me see if I can- I'll pin it,

1297
01:10:21,850 --> 01:10:22,870
so it's easier to find.

1298
01:10:22,870 --> 01:10:29,365
[NOISE] So, it should be pinned now at the very top.

1299
01:10:29,365 --> 01:10:31,270
Yeah. So, a class feedback survey,

1300
01:10:31,270 --> 01:10:32,620
if you go to Piazza, um,

1301
01:10:32,620 --> 01:10:33,715
it's a very short survey.

1302
01:10:33,715 --> 01:10:35,875
You can give us information back. That would be great.

1303
01:10:35,875 --> 01:10:39,280
What we'll do next time is we'll continue to talk about policy search.

1304
01:10:39,280 --> 01:10:40,630
We're gonna talk about baselines,

1305
01:10:40,630 --> 01:10:42,565
which is another way to reduce variance.

1306
01:10:42,565 --> 01:10:45,570
Um, and this again is a super active area of research,

1307
01:10:45,570 --> 01:10:49,125
so there's, um, a ton of work on deep reinforcement learning and policy gradients,

1308
01:10:49,125 --> 01:10:50,880
and so we'll talk about some of that work,

1309
01:10:50,880 --> 01:10:52,755
and then you'll have a chance to, uh,

1310
01:10:52,755 --> 01:10:54,360
play around with this a- and, uh,

1311
01:10:54,360 --> 01:10:57,975
do get sort of hands-on experience with policy gradients after the mid-term.

1312
01:10:57,975 --> 01:11:00,915
So we're releasing the assignment about this at post the midterm,

1313
01:11:00,915 --> 01:11:03,190
and that will be the third assignment.

