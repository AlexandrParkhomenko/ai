1
00:00:04,720 --> 00:00:06,680
хорошо, мы собираемся продолжить и

2
00:00:06,680 --> 00:00:07,870
начать

3
00:00:07,870 --> 00:00:11,000
домашнюю работу - она должна идти полным ходом, если у ва 

4
00:00:11,000 --> 00:00:12,920
есть какие-либо вопросы, не стесняйтесь об 

5
00:00:12,920 --> 00:00:16,970
ащаться к нам с проектными предложениями, если у вас ес 

6
00:00:16,970 --> 00:00:18,380
ь вопросы по этому поводу, не стесняйтесь пр 

7
00:00:18,380 --> 00:00:20,540
ходить в наши часы работы или св 

8
00:00:20,540 --> 00:00:22,880
заться  у нашей площади, так что у них есть другие

9
00:00:22,880 --> 00:00:29,600
вопросы, на которые я могу ответить прямо сейчас,

10
00:00:29,600 --> 00:00:31,400
хорошо, так что сегодня мы начнем,

11
00:00:31,400 --> 00:00:33,830
это немного громко, сегодня мы

12
00:00:33,830 --> 00:00:35,660
начнем говорить о

13
00:00:35,660 --> 00:00:37,729
методах градиента политики Методы градиента политики,

14
00:00:37,729 --> 00:00:39,379
вероятно, наиболее часто используются  в

15
00:00:39,379 --> 00:00:41,330
обучении с подкреплением прямо сейчас, так что я

16
00:00:41,330 --> 00:00:43,309
думаю, что это невероятно полезная вещь,

17
00:00:43,309 --> 00:00:46,850
с которой нужно ознакомиться всякий раз, когда мы говорим

18
00:00:46,850 --> 00:00:48,110
об обучении с подкреплением, мы как бы

19
00:00:48,110 --> 00:00:49,370
продолжаем возвращаться к этим основным

20
00:00:49,370 --> 00:00:51,379
свойствам, которые мы хотели бы от агентов,

21
00:00:51,379 --> 00:00:53,479
которые учатся принимать решения о них

22
00:00:53,479 --> 00:00:54,979
возможность выполнять

23
00:00:54,979 --> 00:00:56,690
оптимизацию, обрабатывающую отсроченные

24
00:00:56,690 --> 00:01:00,409
последствия во время исследования, и

25
00:01:00,409 --> 00:01:01,909
делать все это статистически и

26
00:01:01,909 --> 00:01:03,379
эффективно в действительно многомерных

27
00:01:03,379 --> 00:01:07,460
пространствах, и то, с чего мы начинали.  разговор

28
00:01:07,460 --> 00:01:09,079
о прошлом разе с точки зрения имитационного

29
00:01:09,079 --> 00:01:10,759
обучения был своего рода другим способом

30
00:01:10,759 --> 00:01:12,859
обеспечить дополнительную структуру или

31
00:01:12,859 --> 00:01:14,840
дополнительную поддержку для наших агентов,

32
00:01:14,840 --> 00:01:16,159
чтобы они могли попытаться научиться делать

33
00:01:16,159 --> 00:01:19,430
что-то быстрее, а имитационное обучение было

34
00:01:19,430 --> 00:01:21,170
одним из способов обеспечить структуру или  поддержку

35
00:01:21,170 --> 00:01:23,359
, используя демонстрации от людей,

36
00:01:23,359 --> 00:01:25,880
и мы видели другие способы

37
00:01:25,880 --> 00:01:29,240
кодирования структуры или предшествующих знаний человека,

38
00:01:29,240 --> 00:01:31,549
когда мы начали говорить о

39
00:01:31,549 --> 00:01:33,770
приближении функций, поэтому, когда мы думаем

40
00:01:33,770 --> 00:01:37,399
о том, как мы определяем Q, например, когда мы

41
00:01:37,399 --> 00:01:40,729
определяем Q как si и W, где это  был

42
00:01:40,729 --> 00:01:45,499
набором параметров, мы неявно

43
00:01:45,499 --> 00:01:47,479
делали выбор о наложении

44
00:01:47,479 --> 00:01:49,130
некоторой структуры с точки зрения того, как мы

45
00:01:49,130 --> 00:01:50,659
собираемся представлять нашу функцию ценности,

46
00:01:50,659 --> 00:01:53,509
и этот выбор мог бы быть довольно сильным,

47
00:01:53,509 --> 00:01:55,460
как если бы он был линейным, так что это

48
00:01:55,460 --> 00:01:57,499
своего рода довольно сильный  предположение, или

49
00:01:57,499 --> 00:01:59,840
это может быть очень слабое предположение, такое как

50
00:01:59,840 --> 00:02:05,539
использование глубокой нейронной сети, и поэтому, когда мы

51
00:02:05,539 --> 00:02:07,520
указываем ответ на эти функции,

52
00:02:07,520 --> 00:02:09,560
приближенные к представлениям, мы

53
00:02:09,560 --> 00:02:12,440
сортируем  о неявном принятии решений о

54
00:02:12,440 --> 00:02:14,209
том, какую структуру и сколько

55
00:02:14,209 --> 00:02:15,850
знаний в предметной области мы хотим добавить,

56
00:02:15,850 --> 00:02:19,930
чтобы наши агенты могли учиться, поэтому то, о чем

57
00:02:19,930 --> 00:02:21,100
мы собираемся говорить сегодня

58
00:02:21,100 --> 00:02:22,450
и пытаемся говорить на этой неделе,

59
00:02:22,450 --> 00:02:24,580
- это поиск политики, который  это еще одно место,

60
00:02:24,580 --> 00:02:26,560
где может быть очень естественно вводить

61
00:02:26,560 --> 00:02:28,570
знания в предметной области, и мы увидим это сегодня на

62
00:02:28,570 --> 00:02:31,240
некоторых примерах робототехники, и это также может

63
00:02:31,240 --> 00:02:32,890
быть очень эффективным способом попытаться

64
00:02:32,890 --> 00:02:39,400
научиться, поэтому, как я сказал ранее, мы как

65
00:02:39,400 --> 00:02:40,630
бы  приблизиться к тому, где мы проводим

66
00:02:40,630 --> 00:02:42,460
обучение с подкреплением без модели, и

67
00:02:42,460 --> 00:02:43,930
когда мы начали пытаться масштабироваться до

68
00:02:43,930 --> 00:02:46,630
действительно больших пространств состояний, у меня

69
00:02:46,630 --> 00:02:48,130
было это, несколько разных людей спрашивали

70
00:02:48,130 --> 00:02:49,870
меня о действительно больших пространствах действий,

71
00:02:49,870 --> 00:02:52,300
что является действительно важной темой, которую мы

72
00:02:52,300 --> 00:02:53,680
не  мы будем слишком много говорить об этом в

73
00:02:53,680 --> 00:02:54,940
этом квартале, хотя мы

74
00:02:54,940 --> 00:02:56,380
немного поговорим о том, когда ваше пространство действий

75
00:02:56,380 --> 00:02:58,990
является непрерывным, но низкоразмерным, но

76
00:02:58,990 --> 00:03:00,430
мы начали много говорить о том, когда

77
00:03:00,430 --> 00:03:01,930
пространство состояний действительно

78
00:03:01,930 --> 00:03:04,630
многомерно и реально  y большой, и поэтому мы

79
00:03:04,630 --> 00:03:07,780
говорили об аппроксимации вещей с помощью

80
00:03:07,780 --> 00:03:09,370
какой-то параметризации, например,

81
00:03:09,370 --> 00:03:10,990
будь то параметры тета, или мы

82
00:03:10,990 --> 00:03:17,170
предлагаем, или мы часто используем W, но некоторую

83
00:03:17,170 --> 00:03:19,510
параметризацию функции, и поэтому

84
00:03:19,510 --> 00:03:22,870
мы использовали нашу функцию ценности, чтобы определить

85
00:03:22,870 --> 00:03:25,450
ожидаемую дисконтированную сумму вознаграждения.

86
00:03:25,450 --> 00:03:27,250
из определенного состояния или действия состояния, а

87
00:03:27,250 --> 00:03:29,650
затем мы могли бы извлечь политику из этой

88
00:03:29,650 --> 00:03:31,450
функции значения или, по крайней мере, из

89
00:03:31,450 --> 00:03:33,970
функции значения действия состояния, и вместо этого

90
00:03:33,970 --> 00:03:35,320
мы собираемся сделать сегодня просто

91
00:03:35,320 --> 00:03:38,920
параметризовать политику напрямую, поэтому, когда мы

92
00:03:38,920 --> 00:03:41,290
говорили о табличных политиках, наш  политика

93
00:03:41,290 --> 00:03:43,840
была просто отображением от состояний к

94
00:03:43,840 --> 00:03:46,840
действиям, и в табличной настройке мы

95
00:03:46,840 --> 00:03:48,070
могли бы просто посмотреть, как сделать это в виде таблицы поиска

96
00:03:48,070 --> 00:03:49,690
для каждого отдельного состояния, мы могли бы

97
00:03:49,690 --> 00:03:52,780
записать, какие действия мы предпримем, и

98
00:03:52,780 --> 00:03:54,280
что мы собираемся сделать сейчас, это сказать,

99
00:03:54,280 --> 00:03:55,750
что это  будет невозможно

100
00:03:55,750 --> 00:03:59,200
записать нашу таблицу нашей политики, поэтому вместо

101
00:03:59,200 --> 00:04:00,370
этого мы собираемся параметризовать ее,

102
00:04:00,370 --> 00:04:02,740
мы собираемся использовать набор весов или

103
00:04:02,740 --> 00:04:04,540
тета, сегодня мы в основном собираемся т  o используйте

104
00:04:04,540 --> 00:04:07,060
тета, но это хорошо, думайте об этом

105
00:04:07,060 --> 00:04:09,730
как о весах, просто как о способе параметризации

106
00:04:09,730 --> 00:04:12,340
нашей политики.

107
00:04:12,340 --> 00:04:15,790


108
00:04:15,790 --> 00:04:17,709


109
00:04:17,709 --> 00:04:20,440


110
00:04:20,440 --> 00:04:22,180
эффективно

111
00:04:22,180 --> 00:04:24,460
определяя пространство, которое вы не можете

112
00:04:24,460 --> 00:04:27,070
изучить, так что это

113
00:04:27,070 --> 00:04:29,599
определяет класс политик, которые вы могли бы

114
00:04:29,599 --> 00:04:33,169
изучить. Я ношу двигатели.

115
00:04:33,169 --> 00:04:34,550


116
00:04:34,550 --> 00:04:36,080


117
00:04:36,080 --> 00:04:38,149
к априорной

118
00:04:38,149 --> 00:04:40,219
модели динамики или вознаграждения

119
00:04:40,219 --> 00:04:44,899
мира, поэтому мы рассмотрели некоторые из этих

120
00:04:44,899 --> 00:04:46,189
диаграмм в начале квартала,

121
00:04:46,189 --> 00:04:48,619
я просто хочу вернуться к этому, мы

122
00:04:48,619 --> 00:04:50,779
говорили о ценности сортировки, которую мы не

123
00:04:50,779 --> 00:04:51,949
так много говорили о моделях, хотя

124
00:04:51,949 --> 00:04:54,289
модели также очень важны, но

125
00:04:54,289 --> 00:04:55,490
мы много говорили о своего рода

126
00:04:55,490 --> 00:04:57,229
подходах, основанных на функции ценности

127
00:04:57,229 --> 00:04:59,569
, а теперь мы собираемся поговорить о

128
00:04:59,569 --> 00:05:02,839
методах прямого поиска политики

129
00:05:02,839 --> 00:05:05,119
и  s вы можете ожидать, что есть много

130
00:05:05,119 --> 00:05:06,559
работы, которая пытается объединить

131
00:05:06,559 --> 00:05:08,059
два из них, они часто называются

132
00:05:08,059 --> 00:05:11,149
методами дополнительной критики, мы пытаемся явно

133
00:05:11,149 --> 00:05:13,309
поддерживать параметризованную политику и

134
00:05:13,309 --> 00:05:15,529
явно поддерживать параметризованную

135
00:05:15,529 --> 00:05:18,169
критику или функцию значения, так что это

136
00:05:18,169 --> 00:05:24,379
политика, и это  сигнал, который мы собираемся

137
00:05:24,379 --> 00:05:25,279
начать сегодня, и мы собираемся

138
00:05:25,279 --> 00:05:28,459
говорить о методах, основанных на политике, так

139
00:05:28,459 --> 00:05:31,849
почему вы хотите сделать это хорошо, на

140
00:05:31,849 --> 00:05:33,289
самом деле это немного восходит к тому, о

141
00:05:33,289 --> 00:05:34,490
чем мы говорили на прошлой неделе с

142
00:05:34,490 --> 00:05:36,529
имитацией обучения для  имитационное

143
00:05:36,529 --> 00:05:37,879
обучение мы говорили о том, что

144
00:05:37,879 --> 00:05:39,529
иногда людям трудно

145
00:05:39,529 --> 00:05:41,240
записать функцию вознаграждения, и поэтому им может

146
00:05:41,240 --> 00:05:42,619
быть проще просто продемонстрировать,

147
00:05:42,619 --> 00:05:45,409
как выглядит политика аналогично, в

148
00:05:45,409 --> 00:05:47,029
некоторых случаях, может быть, проще

149
00:05:47,029 --> 00:05:49,699
записать своего рода параметризацию

150
00:05:49,699 --> 00:05:51,349
пространства политик, чем

151
00:05:51,349 --> 00:05:53,659
записывать параметризацию пространства

152
00:05:53,659 --> 00:05:57,259
функций ценности действия состояния, кроме того,

153
00:05:57,259 --> 00:05:59,959
они часто гораздо более эффективны в

154
00:05:59,959 --> 00:06:01,610
многомерных или c  непрерывные пространства действий,

155
00:06:01,610 --> 00:06:03,680
и они позволяют нам изучать

156
00:06:03,680 --> 00:06:05,689
стохастические стратегии, о которых мы до сих пор не

157
00:06:05,689 --> 00:06:07,939
говорили очень много, но я

158
00:06:07,939 --> 00:06:08,599
собираюсь дать вам несколько иллюстраций того,

159
00:06:08,599 --> 00:06:10,249
где нам определенно нужны

160
00:06:10,249 --> 00:06:14,180
стохастические политики, и они иногда

161
00:06:14,180 --> 00:06:15,619
имеют лучшие конвергентные свойства конвергенции политик,

162
00:06:15,619 --> 00:06:18,559
которые могут  быть

163
00:06:18,559 --> 00:06:19,909
немного спорным, это зависит именно от

164
00:06:19,909 --> 00:06:21,709
того, сравниваем ли мы это с

165
00:06:21,709 --> 00:06:23,089
моделью для подходов, основанных на вашей модели,

166
00:06:23,089 --> 00:06:24,800
и от того, сколько вычислений

167
00:06:24,800 --> 00:06:26,749
мы делаем, и поэтому это может быть

168
00:06:26,749 --> 00:06:28,490
немного функцией вычислений для

169
00:06:28,490 --> 00:06:33,860
вычислений, независимо от того, что

170
00:06:33,860 --> 00:06:36,139
действительно большой недостаток заключается в том, что они,

171
00:06:36,139 --> 00:06:38,389
как правило, сходятся только

172
00:06:38,389 --> 00:06:41,289
к локальной Оптиме,

173
00:06:41,769 --> 00:06:44,389
поэтому мы собираемся сойтись с

174
00:06:44,389 --> 00:06:45,799
чем-то, что, надеюсь, является довольно

175
00:06:45,799 --> 00:06:47,539
хорошей политикой, но мы, как правило, не

176
00:06:47,539 --> 00:06:48,829
гарантируем сходимость к глобальной

177
00:06:48,829 --> 00:06:51,289
Оптиме сейчас  есть несколько методов

178
00:06:51,289 --> 00:06:53,389
, которые гарантированно объединят

179
00:06:53,389 --> 00:06:55,009
локальные и глобальные оптимумы, и сегодня я

180
00:06:55,009 --> 00:06:56,479
постараюсь выделить некоторые из них, но в

181
00:06:56,479 --> 00:06:58,639
целом почти все  методы, которые

182
00:06:58,639 --> 00:07:00,079
вы видите, такие как глубокое

183
00:07:00,079 --> 00:07:02,289
обучение с подкреплением, что наша политика, основанная на градиенте,

184
00:07:02,289 --> 00:07:06,289
преобразуется только в локальные оптимумы, а

185
00:07:06,289 --> 00:07:08,569
затем другая проблема заключается в том, что обычно

186
00:07:08,569 --> 00:07:09,860
мы собираемся сделать это, пытаясь

187
00:07:09,860 --> 00:07:11,719
оценить политику, а затем оценить ее

188
00:07:11,719 --> 00:07:13,879
градиент, и часто это  может быть несколько

189
00:07:13,879 --> 00:07:16,789
неэффективной выборка, поэтому у нас может быть

190
00:07:16,789 --> 00:07:18,889
довольно много данных для оценки того, что представляет собой этот

191
00:07:18,889 --> 00:07:21,110
градиент, когда мы используем

192
00:07:21,110 --> 00:07:26,509
подход, основанный на градиенте, поэтому зачем нам нужна своего

193
00:07:26,509 --> 00:07:28,939
рода стохастическая политика, как я

194
00:07:28,939 --> 00:07:31,610
упоминал ранее в табличной настройке,

195
00:07:31,610 --> 00:07:35,989
поэтому  позвольте мне просто вернуться сюда, так что теперь,

196
00:07:35,989 --> 00:07:40,849
почему мы хотим этого, если вы

197
00:07:40,849 --> 00:07:42,349
вспомните самые первые

198
00:07:42,349 --> 00:07:45,079
лекции, я сказал, что если у нас

199
00:07:45,079 --> 00:07:52,459
есть табличный MDP, существует пирог, который

200
00:07:52,459 --> 00:08:03,019
является детерминированным и оптимальным, поэтому в

201
00:08:03,019 --> 00:08:08,479
табличная настройка MDP нам не нужна,

202
00:08:08,479 --> 00:08:09,860
нам не нужны стохастические политики,

203
00:08:09,860 --> 00:08:12,019
потому что всегда существует политика,

204
00:08:12,019 --> 00:08:13,639
которая является детерминированной и имеет то же

205
00:08:13,639 --> 00:08:18,079
значение, что и оптимальная политика, поэтому

206
00:08:18,079 --> 00:08:20,059
в табличной форме она не нужна.

207
00:08:20,059 --> 00:08:22,669
Случай марковского процесса принятия решений, но

208
00:08:22,669 --> 00:08:24,019
мы не всегда действуем в

209
00:08:24,019 --> 00:08:25,579
табличном случае марковского процесса принятия решений,

210
00:08:25,579 --> 00:08:28,999
поэтому, например, кто здесь знаком

211
00:08:28,999 --> 00:08:32,179
с камнем-ножницами-бумагой, большинство людей и

212
00:08:32,179 --> 00:08:34,219
я, возможно, если вы не знаете, вы могли бы

213
00:08:34,219 --> 00:08:35,719
играли в нее под другим названием, поэтому в

214
00:08:35,719 --> 00:08:37,759


215
00:08:37,759 --> 00:08:41,958
игре «камень-ножницы-бумага» это игра для двух игроков, каждый может либо выбрать бумагу, либо

216
00:08:41,958 --> 00:08:44,720
ножницы, либо камень, когда вам нужно выбрать

217
00:08:44,720 --> 00:08:46,069
один из них,

218
00:08:46,069 --> 00:08:48,319
и ножницы побеждают бумагу в

219
00:08:48,319 --> 00:08:51,980
камне.  в этом случае, если бы у вас была

220
00:08:51,980 --> 00:08:55,730
детерминированная политика, которую вы могли бы проиграть, другой агент мог

221
00:08:55,730 --> 00:08:57,020
бы легко использовать вас,

222
00:08:57,020 --> 00:09:00,620
но равномерная случайная политика

223
00:09:00,620 --> 00:09:03,710
в основном оптимальна, что я имею в виду под

224
00:09:03,710 --> 00:09:05,360
оптимальностью в этом случае, я имею в виду,

225
00:09:05,360 --> 00:09:07,580
что вы получаете плюс один, если  вы выиграли, и

226
00:09:07,580 --> 00:09:10,970
, скажем, ноль минус один, если вы проиграете сейчас,

227
00:09:10,970 --> 00:09:12,140
мы не будем слишком много говорить о

228
00:09:12,140 --> 00:09:15,440
многоагентных случаях в этом классе, но

229
00:09:15,440 --> 00:09:17,720
это очень интересная область исследований, и

230
00:09:17,720 --> 00:09:20,660
в этом случае вы знаете, что среда

231
00:09:20,660 --> 00:09:23,180
не агностична, и  Окружающая среда  n

232
00:09:23,180 --> 00:09:25,220
реагировать на политику, которую мы проводим,

233
00:09:25,220 --> 00:09:27,560
и которая может быть враждебной, и поэтому нам нужна

234
00:09:27,560 --> 00:09:34,280
политика, устойчивая к противнику,

235
00:09:34,280 --> 00:09:38,450
поэтому второй случай - это псевдоним grid world, так что

236
00:09:38,450 --> 00:09:41,030
в этом случае, поэтому, почему вы знаете,

237
00:09:41,030 --> 00:09:42,650
почему стохастичность важна для вас здесь  хорошо,

238
00:09:42,650 --> 00:09:44,270
потому что мы на самом деле не находимся в стохастической

239
00:09:44,270 --> 00:09:46,070
обстановке, мы находимся в состязательной обстановке,

240
00:09:46,070 --> 00:09:48,110
у нас есть другой агент, который играет

241
00:09:48,110 --> 00:09:49,820
с нами, и они могут быть нестационарными

242
00:09:49,820 --> 00:09:51,560
, менять свою политику и реагировать

243
00:09:51,560 --> 00:09:55,040
звездами, так что это не среда, на которую

244
00:09:55,040 --> 00:09:56,750
не влияет  выбрать следующий, он не выбирает

245
00:09:56,750 --> 00:09:58,310
камень-ножницы-бумагу независимо от наших

246
00:09:58,310 --> 00:10:00,650
действий, и в прошлом он может реагировать

247
00:10:00,650 --> 00:10:03,530
на них, поэтому он как бы приобретает эту

248
00:10:03,530 --> 00:10:06,380
нестационарность или состязательный характер в

249
00:10:06,380 --> 00:10:08,870
других случаях, когда это не Марков, так

250
00:10:08,870 --> 00:10:10,910
что это действительно частично наблюдаемо и  у вас

251
00:10:10,910 --> 00:10:13,070
есть псевдонимы, что означает, что мы не можем

252
00:10:13,070 --> 00:10:14,900
различать несколько состояний с

253
00:10:14,900 --> 00:10:16,940
точки зрения наших датчиков, поэтому мы подумали об этом

254
00:10:16,940 --> 00:10:18,200
немного раньше, когда говорили о

255
00:10:18,200 --> 00:10:20,210
роботах, которые, как вы знаете, могут иметь лазерные

256
00:10:20,210 --> 00:10:22,580
дальномеры и как бы сказать, где  они

257
00:10:22,580 --> 00:10:26,930
были в коридоре в зависимости от того, насколько далеко находилась каждая

258
00:10:26,930 --> 00:10:31,910
из первых точек

259
00:10:31,910 --> 00:10:34,790
препятствия на все их 180 градусов, и поэтому

260
00:10:34,790 --> 00:10:35,870
это выглядело бы одинаково во многих

261
00:10:35,870 --> 00:10:37,910
разных коридорах, так что это простой

262
00:10:37,910 --> 00:10:40,070
пример этого, поэтому в сетке псевдонимов

263
00:10:40,070 --> 00:10:42,320
допустим, что агент из

264
00:10:42,320 --> 00:10:44,180
-за своих сенсоров не может различать

265
00:10:44,180 --> 00:10:46,490
серые состояния, и у них есть особенности

266
00:10:46,490 --> 00:10:49,520
определенной формы, у них есть особенность

267
00:10:49,520 --> 00:10:51,170
, есть ли стена на

268
00:10:51,170 --> 00:10:55,460
севере, или на востоке, или на юге, или на западе, так что он может в

269
00:10:55,460 --> 00:10:57,320
основном нравиться, если это  здесь это может сказать,

270
00:10:57,320 --> 00:10:58,850
как о, у меня есть стены по обе стороны от

271
00:10:58,850 --> 00:11:02,030
меня, а не впереди или позади меня, но

272
00:11:02,030 --> 00:11:03,740
это может быть то же самое, что и

273
00:11:03,740 --> 00:11:07,820
в других классах,

274
00:11:07,820 --> 00:11:10,460
поэтому, если бы мы использовали подход к обучению с подкреплением, основанный на ценности,

275
00:11:10,460 --> 00:11:12,140
используя какой-то вид

276
00:11:12,140 --> 00:11:14,900
функция приблизительного значения, она будет использовать

277
00:11:14,900 --> 00:11:17,120
эти функции, которые представляют собой комбинацию

278
00:11:17,120 --> 00:11:18,800
того, какие действия я собираюсь предпринять, и

279
00:11:18,800 --> 00:11:20,500
есть ли вокруг меня стены или нет,

280
00:11:20,500 --> 00:11:23,420
или мы могли бы использовать подход, основанный на политике,

281
00:11:23,420 --> 00:11:26,210
который также учитывает эти функции, но

282
00:11:26,210 --> 00:11:27,590
затем просто  напрямую пытается принять

283
00:11:27,590 --> 00:11:29,300
решение о том, какое действие предпринять, и

284
00:11:29,300 --> 00:11:33,080
эти действия могут быть стохастическими, поэтому в

285
00:11:33,080 --> 00:11:34,640
этом случае агент пытается

286
00:11:34,640 --> 00:11:36,260
выяснить, как ориентироваться в этом мире, он

287
00:11:36,260 --> 00:11:37,610
действительно хочет добраться сюда,

288
00:11:37,610 --> 00:11:39,380
вот где есть большая награда, так

289
00:11:39,380 --> 00:11:42,320
что это  хорошо, что он хочет избежать черепа

290
00:11:42,320 --> 00:11:44,150
и скрещенных костей, и это будет

291
00:11:44,150 --> 00:11:48,170
отрицательная награда, поэтому из-за

292
00:11:48,170 --> 00:11:50,210
псевдонимов агент не может различить

293
00:11:50,210 --> 00:11:55,040
, здесь он или здесь, и поэтому

294
00:11:55,040 --> 00:11:56,210
он должен делать одно и то же в обоих

295
00:11:56,210 --> 00:11:59,900
состояниях, и поэтому либо он  идти налево

296
00:11:59,900 --> 00:12:02,840
или нужно идти направо на запад или восток, и в

297
00:12:02,840 --> 00:12:05,540
любом случае это не оптимально, потому что, если

298
00:12:05,540 --> 00:12:07,400
он на самом деле здесь, он должен идти в

299
00:12:07,400 --> 00:12:12,740
эту сторону, а не сюда и вниз, и поэтому

300
00:12:12,740 --> 00:12:14,420
он может различать, находится ли он здесь

301
00:12:14,420 --> 00:12:17,480
или здесь, но он может просто закончиться  двигаясь

302
00:12:17,480 --> 00:12:19,730
вперед и назад, они принимают очень плохие

303
00:12:19,730 --> 00:12:21,770
решения, и поэтому он может застрять и

304
00:12:21,770 --> 00:12:23,870
никогда не узнать, что безопасно

305
00:12:23,870 --> 00:12:26,960
спуститься и получить деньги, поэтому он учится

306
00:12:26,960 --> 00:12:28,820
в детерминированной политике, потому что это

307
00:12:28,820 --> 00:12:29,780
то, что мы обычно изучаем.  g с

308
00:12:29,780 --> 00:12:31,940
ними, и независимо от того, жадный он или

309
00:12:31,940 --> 00:12:34,450
жадный в целом, он будет работать очень плохо,

310
00:12:34,450 --> 00:12:38,120
но если у вас есть стохастическая политика, когда

311
00:12:38,120 --> 00:12:39,260
вы находитесь в состоянии, когда вы находитесь в состоянии, где вы псевдоним,

312
00:12:39,260 --> 00:12:40,880
вы можете просто рандомизировать, вы могли бы сказать,

313
00:12:40,880 --> 00:12:42,500
что я не уверен, что  На самом деле я нахожусь в

314
00:12:42,500 --> 00:12:45,220
этом состоянии в этом состоянии или в этом состоянии,

315
00:12:45,220 --> 00:12:48,980
поэтому я просто пойду либо на восток, либо на запад

316
00:12:48,980 --> 00:12:51,830
с вероятностью 50%, и тогда он,

317
00:12:51,830 --> 00:12:54,250
как правило, быстро достигнет целевого состояния,

318
00:12:54,250 --> 00:12:56,510
потому что обратите внимание, что он может сказать, что он должен

319
00:12:56,510 --> 00:12:58,340
делать, когда это произойдет.  достигает здесь, потому что это

320
00:12:58,340 --> 00:13:00,200
выглядит иначе, чем эти два состояния, поэтому,

321
00:13:00,200 --> 00:13:01,790
когда оно находится посередине, оно точно знает,

322
00:13:01,790 --> 00:13:04,760
что делать, так что это снова

323
00:13:04,760 --> 00:13:06,740
пример, когда стохастическая политика

324
00:13:06,740 --> 00:13:08,780
имеет большую ценность, чем детерминированная

325
00:13:08,780 --> 00:13:10,190
политика, и это потому, что область

326
00:13:10,190 --> 00:13:13,430
здесь не Марковская  это частично

327
00:13:13,430 --> 00:13:15,760
наблюдаемо,

328
00:13:22,829 --> 00:13:24,970
хорошо, так что это своего рода одна из

329
00:13:24,970 --> 00:13:26,740
причин, по которой мы можем захотеть некоторые из

330
00:13:26,740 --> 00:13:28,870
причин, почему мы можем захотеть быть непосредственно

331
00:13:28,870 --> 00:13:30,160
основанными на политике, и есть много других

332
00:13:30,160 --> 00:13:33,760
причин, так что это значит, хорошо,

333
00:13:33,760 --> 00:13:35,920
у нас будет это  параметризовать  политика,

334
00:13:35,920 --> 00:13:37,420
и цель состоит в том, что мы хотим найти,

335
00:13:37,420 --> 00:13:53,170
да, так что мы можем сделать вывод, что если

336
00:13:53,170 --> 00:13:54,519
мир частично наблюдаем, стохастическая

337
00:13:54,519 --> 00:13:56,980
политика всегда лучше, я думаю, это

338
00:13:56,980 --> 00:13:58,570
зависит от моделирования, которое вы хотите сделать, я

339
00:13:58,570 --> 00:14:00,610
думаю, что в этом случае лучше, чем быть

340
00:14:00,610 --> 00:14:02,199
стохастическим, потому что это  все еще делая

341
00:14:02,199 --> 00:14:04,029
что-то вроде не очень разумного

342
00:14:04,029 --> 00:14:05,529
в серых состояниях, это просто рандомизация

343
00:14:05,529 --> 00:14:07,480
будет заключаться в том, чтобы иметь частично наблюдаемую

344
00:14:07,480 --> 00:14:10,480
марковскую политику процесса принятия решений, и тогда

345
00:14:10,480 --> 00:14:13,449
вы могли бы отслеживать оценку того, где

346
00:14:13,449 --> 00:14:14,800
вы находитесь в мире, чтобы вы могли

347
00:14:14,800 --> 00:14:16,540
отслеживать состояние убеждения в течение  в каком состоянии

348
00:14:16,540 --> 00:14:18,370
вы находитесь, и тогда вы могли бы, надеюсь,

349
00:14:18,370 --> 00:14:20,589
однозначно определить, что если бы я был просто

350
00:14:20,589 --> 00:14:22,060
в этом состоянии, я должен быть в этом состоянии

351
00:14:22,060 --> 00:14:24,040
сейчас, а затем вы могли бы детерминистически

352
00:14:24,040 --> 00:14:26,740
пойти вправо или влево, так что это зависит

353
00:14:26,740 --> 00:14:29,500
от желания моделирующих  сделать хороший

354
00:14:29,500 --> 00:14:35,800
вопрос, хорошо, поэтому, когда мы начнем

355
00:14:35,800 --> 00:14:37,990
переходить к параметризации поиска политики,

356
00:14:37,990 --> 00:14:39,069
мы собираемся найти

357
00:14:39,069 --> 00:14:41,079
параметры, которые дают наилучшее значение

358
00:14:41,079 --> 00:14:42,850
политики в классе с  наилучшее значение

359
00:14:42,850 --> 00:14:45,579
и так похоже на то, что мы видели до

360
00:14:45,579 --> 00:14:47,319
того, как мы можем это сделать, мы можем думать об обслуживаемых

361
00:14:47,319 --> 00:14:49,959
эпизодических настройках и бесконечных для

362
00:14:49,959 --> 00:14:52,209
продолжающихся настроек, поэтому в эпизодической

363
00:14:52,209 --> 00:14:54,339
настройке это означает, что агент будет

364
00:14:54,339 --> 00:14:56,290
действовать в течение нескольких временных шагов, часто

365
00:14:56,290 --> 00:14:59,800
скажем, H  шаги, но это может быть

366
00:14:59,800 --> 00:15:01,389
переменным, как это может быть, пока

367
00:15:01,389 --> 00:15:06,639
вы не достигнете конечного состояния, и тогда

368
00:15:06,639 --> 00:15:08,139
мы можем просто рассмотреть, каково

369
00:15:08,139 --> 00:15:10,510
ожидаемое значение или каково значение,

370
00:15:10,510 --> 00:15:12,010
каково ожидаемое дисконтированное лето

371
00:15:12,010 --> 00:15:13,930
вознаграждений, которые мы получаем от начального состояния или

372
00:15:13,930 --> 00:15:16,569
распределения  начальных состояний, а затем

373
00:15:16,569 --> 00:15:17,949
то, что мы хотим сделать, это найти

374
00:15:17,949 --> 00:15:19,600
параметризованную политику, которая имеет наибольшее

375
00:15:19,600 --> 00:15:23,079
значение. Другой вариант заключается в том, что если мы находимся

376
00:15:23,079 --> 00:15:24,399
в непрерывной среде, что означает, что

377
00:15:24,399 --> 00:15:26,680
мы находимся в онлайн-настройке, мы не действуем в

378
00:15:26,680 --> 00:15:28,149
течение H шагов, мы  просто действуйте вечно

379
00:15:28,149 --> 00:15:30,069
, нет конечных состояний, и мы можем

380
00:15:30,069 --> 00:15:31,330
либо использовать

381
00:15:31,330 --> 00:15:34,150
среднее значение, где мы усредняем

382
00:15:34,150 --> 00:15:38,230
распределение состояний, так что это похоже на

383
00:15:38,230 --> 00:15:39,910
то, что мы видели, прежде чем думать о

384
00:15:39,910 --> 00:15:41,470
распределении стационарное распределение

385
00:15:41,470 --> 00:15:44,050
Цепь Маркова, которая

386
00:15:44,050 --> 00:15:46,480
индуцируется определенной политикой, потому что мы уже говорили

387
00:15:46,480 --> 00:15:47,860
о том, что если вы

388
00:15:47,860 --> 00:15:50,770
зафиксируете политику, а затем, по сути,

389
00:15:50,770 --> 00:15:52,870
получите прямой процесс Маркова, вы

390
00:15:52,870 --> 00:15:54,340
также можете просто подумать о распределении

391
00:15:54,340 --> 00:15:57,700
состояний, которое вы получаете, является марковским.  цепь, поэтому, если

392
00:15:57,700 --> 00:15:59,260
мы действуем бесконечно, мы собираемся

393
00:15:59,260 --> 00:16:01,570
сказать в среднем, какова

394
00:16:01,570 --> 00:16:03,010
ценность состояний, которых мы достигаем при этом

395
00:16:03,010 --> 00:16:06,160
стационарном распределении, и другой способ

396
00:16:06,160 --> 00:16:07,960
сделать это также состоит в том, чтобы сказать, что мы просто смотрим на

397
00:16:07,960 --> 00:16:09,990
своего рода среднее  вознаграждение за временной шаг

398
00:16:09,990 --> 00:16:12,190
теперь для простоты сегодня мы собираемся

399
00:16:12,190 --> 00:16:13,840
сосредоточиться почти исключительно на эпизодической

400
00:16:13,840 --> 00:16:15,910
настройке, но мы можем подумать об аналогичных

401
00:16:15,910 --> 00:16:17,260
методах для этих других форм

402
00:16:17,260 --> 00:16:22,240
настроек, как и раньше, и это

403
00:16:22,240 --> 00:16:23,950
проблема оптимизации, похожая на то, что мы

404
00:16:23,950 --> 00:16:26,020
видели в значении  случай аппроксимации функции

405
00:16:26,020 --> 00:16:28,540
для функций линейного значения и

406
00:16:28,540 --> 00:16:30,490
использования глубоких нейронных сетей, мы собираемся

407
00:16:30,490 --> 00:16:32,770
провести оптимизацию, что

408
00:16:32,770 --> 00:16:34,210
означает, что нам нужно сделать какой-то

409
00:16:34,210 --> 00:16:35,860
инструмент оптимизации, чтобы попытаться найти

410
00:16:35,860 --> 00:16:41,860
лучшие данные, поэтому один из вариантов - выполнить

411
00:16:41,860 --> 00:16:44,200
оптимизацию без градиента, мы не

412
00:16:44,200 --> 00:16:46,390
склонны делать это очень часто в методах поиска политики,

413
00:16:46,390 --> 00:16:48,280
но есть много разных

414
00:16:48,280 --> 00:16:49,300
методов, которые наша оптимизация замораживания градиента предназначена

415
00:16:49,300 --> 00:16:51,490
только для нас, чтобы найти

416
00:16:51,490 --> 00:16:54,370
любые параметры, максимизирующие это

417
00:16:54,370 --> 00:16:57,670
благочестивые данные  и просто чтобы связать это так же,

418
00:16:57,670 --> 00:17:00,250
как то, что мы видели для функций Q, теперь у нас

419
00:17:00,250 --> 00:17:02,380
есть тета, которая определяет политику,

420
00:17:02,380 --> 00:17:05,200
и у нее может быть какой-то интересный

421
00:17:05,200 --> 00:17:07,030
ландшафт, и затем мы хотим

422
00:17:07,030 --> 00:17:10,480
найти, где находится максимум, поэтому мы действительно

423
00:17:10,480 --> 00:17:12,339
пытаемся найти  максимальное значение функции настолько

424
00:17:12,339 --> 00:17:14,050
эффективно, насколько это возможно, и существует

425
00:17:14,050 --> 00:17:15,400
множество методов для этого, которые не

426
00:17:15,400 --> 00:17:16,510
полагаются на дифференцируемость функции,

427
00:17:16,510 --> 00:17:20,470
и в некоторых случаях они действительно могут быть

428
00:17:20,470 --> 00:17:24,099
очень хорошими, так что это

429
00:17:24,099 --> 00:17:25,690
хорошая работа, выполненная коллегой из  мой,

430
00:17:25,690 --> 00:17:29,380
и мы разработали метод для

431
00:17:29,380 --> 00:17:31,060
автоматического определения

432
00:17:31,060 --> 00:17:32,980
моделей помощи экзоскелета, которые

433
00:17:32,980 --> 00:17:35,260
минимизируют затраты метаболической энергии для

434
00:17:35,260 --> 00:17:39,150
отдельных людей во время ходьбы

435
00:17:42,440 --> 00:17:44,960
во время оптимизации, которую пользователь впервые

436
00:17:44,960 --> 00:17:46,880
испытывает o  закон управления, в то время как

437
00:17:46,880 --> 00:17:48,370
проводятся измерения дыхания в

438
00:17:48,370 --> 00:17:51,110
стационарном состоянии. Стоимость энергии оценивается путем

439
00:17:51,110 --> 00:17:53,000
подгонки динамической модели первого порядка к

440
00:17:53,000 --> 00:17:58,130
двухминутным данным переходного процесса.

441
00:17:58,130 --> 00:18:00,230
Затем закон управления изменяется и

442
00:18:00,230 --> 00:18:02,930
снова оценивается скорость метаболизма. Этот

443
00:18:02,930 --> 00:18:05,000
процесс повторяется в течение заданного

444
00:18:05,000 --> 00:18:06,830
количества контрольных  потеря при формировании одного

445
00:18:06,830 --> 00:18:13,880
поколения

446
00:18:13,880 --> 00:18:16,130
стратегия эволюции адаптации ковариационной матрицы затем

447
00:18:16,130 --> 00:18:18,620
используется для создания следующего поколения

448
00:18:18,620 --> 00:18:20,360
среднее значение каждого поколения представляет собой

449
00:18:20,360 --> 00:18:21,980
наилучшую оценку значений оптимальных

450
00:18:21,980 --> 00:18:24,740
параметров управления после примерно часа

451
00:18:24,740 --> 00:18:26,870
оптимизации затраты энергии были снижены

452
00:18:26,870 --> 00:18:29,360
в среднем на 24 процента по сравнению с  никакой

453
00:18:29,360 --> 00:18:36,470
помощи, так что это работа, которую

454
00:18:36,470 --> 00:18:39,980
проделал мой коллега Стив Коллинз, который

455
00:18:39,980 --> 00:18:41,420
занимается машиностроением, и мы

456
00:18:41,420 --> 00:18:42,680
сотрудничали с кем-то, можете ли вы

457
00:18:42,680 --> 00:18:45,350
научить людей делать это лучше, поэтому

458
00:18:45,350 --> 00:18:47,240
идея в этом случае состоит в том, что есть

459
00:18:47,240 --> 00:18:49,100
много случаев, для которых вы  хотел бы использовать

460
00:18:49,100 --> 00:18:50,780
экзоскелеты у многих людей

461
00:18:50,780 --> 00:18:51,920
инсульт много людей с мобильностью

462
00:18:51,920 --> 00:18:53,810
pr  проблем, и, конечно же, есть много

463
00:18:53,810 --> 00:18:56,510
ветеранов, которые теряют конечность, и в этих

464
00:18:56,510 --> 00:18:58,610
случаях одна из проблем заключалась в

465
00:18:58,610 --> 00:19:00,050
том, как выяснить,

466
00:19:00,050 --> 00:19:01,910
каковы параметры этих экзоскелетов

467
00:19:01,910 --> 00:19:04,700
, чтобы обеспечить поддержку ходячим людям,

468
00:19:04,700 --> 00:19:06,410
а затем, как правило, это  зависит от

469
00:19:06,410 --> 00:19:08,240
физиологии, и для многих разных людей

470
00:19:08,240 --> 00:19:09,620
им понадобятся разные типы

471
00:19:09,620 --> 00:19:11,780
параметров, но вы хотите сделать это

472
00:19:11,780 --> 00:19:13,400
очень быстро, чтобы иметь возможность

473
00:19:13,400 --> 00:19:15,260
очень быстро выяснить для каждого человека,

474
00:19:15,260 --> 00:19:17,120
какие параметры управления являются правильными

475
00:19:17,120 --> 00:19:18,440
, чтобы помочь им  получают наибольшую

476
00:19:18,440 --> 00:19:21,530
помощь при ходьбе, и поэтому

477
00:19:21,530 --> 00:19:25,270
лаборатории Стива рассматривали это как своего рода

478
00:19:25,270 --> 00:19:27,530
проблему поиска политик политики, где то, что вы

479
00:19:27,530 --> 00:19:28,970
делаете, это то, что у вас есть кто-то, на

480
00:19:28,970 --> 00:19:32,240
его устройстве вы пробуете некоторые

481
00:19:32,240 --> 00:19:34,010
законы управления, которые обеспечивают определенную

482
00:19:34,010 --> 00:19:35,540
форму  поддержку с точки зрения их

483
00:19:35,540 --> 00:19:37,820
экзоскелета, вы измеряете их

484
00:19:37,820 --> 00:19:40,790
метаболическую эффективность, то есть

485
00:19:40,790 --> 00:19:42,380
как узнать, насколько тяжело они дышат, насколько

486
00:19:42,380 --> 00:19:44,000
тяжело им приходится работать по сравнению с тем,

487
00:19:44,000 --> 00:19:45,350
если бы они  не носили это или под

488
00:19:45,350 --> 00:19:47,210
другими законами управления, и тогда вы можете

489
00:19:47,210 --> 00:19:49,010
использовать эту информацию, чтобы выяснить,

490
00:19:49,010 --> 00:19:50,990
какой следующий набор законов управления вы

491
00:19:50,990 --> 00:19:52,940
используете, и сделать все это в замкнутом

492
00:19:52,940 --> 00:19:55,280
цикле как можно быстрее, теперь одна

493
00:19:55,280 --> 00:19:56,269
из причин, по которой я привожу  это

494
00:19:56,269 --> 00:19:58,009
связано как с тем, что это было невероятно

495
00:19:58,009 --> 00:19:59,269
эффективно, так и с тем, что это действительно хорошая научная

496
00:19:59,269 --> 00:20:01,639
статья, которая иллюстрирует, как это может быть

497
00:20:01,639 --> 00:20:02,959
намного эффективнее, чем предыдущие

498
00:20:02,959 --> 00:20:05,719
методы, и, во-вторых,

499
00:20:05,719 --> 00:20:08,029
с использованием CM AES, который представляет собой подход без градиента,

500
00:20:08,029 --> 00:20:10,190
поэтому, несмотря на то, что большая часть того, что

501
00:20:10,190 --> 00:20:11,690
мы  Я собираюсь обсудить сегодня в классе

502
00:20:11,690 --> 00:20:13,759
все с методами, основанными на градиенте, есть

503
00:20:13,759 --> 00:20:15,649
несколько действительно хороших примеров

504
00:20:15,649 --> 00:20:17,989
неиспользования методов, основанных на градиенте, а также для

505
00:20:17,989 --> 00:20:19,669
поиска политик для множества других типов

506
00:20:19,669 --> 00:20:21,619
приложений, поэтому я думаю, что

507
00:20:21,619 --> 00:20:24,229
полезно знать в своем наборе инструментов, что один из

508
00:20:24,229 --> 00:20:25,249
них не  не должны быть ограничены

509
00:20:25,249 --> 00:20:27,049
методами, основанными на градиенте, и одна из

510
00:20:27,049 --> 00:20:28,159
действительно приятных особенностей таких вещей, как

511
00:20:28,159 --> 00:20:30,289
CMA, заключается в том, что они гарантированно

512
00:20:30,289 --> 00:20:33,409
приближаются к глобальной Optima, поэтому в некоторых

513
00:20:33,409 --> 00:20:34,999
случаях  вы действительно хотите быть

514
00:20:34,999 --> 00:20:36,229
уверены, что делаете это,

515
00:20:36,229 --> 00:20:42,109
потому что это ситуация с высокими ставками, и

516
00:20:42,109 --> 00:20:43,940
в целом в последнее время

517
00:20:43,940 --> 00:20:45,320
неоднократно замечалось, что

518
00:20:45,320 --> 00:20:46,849
иногда такие подходы действительно

519
00:20:46,849 --> 00:20:48,379
работают, что немного смущает меня, ну

520
00:20:48,379 --> 00:20:50,509
, это, как правило,  быть в некотором роде своего рода

521
00:20:50,509 --> 00:20:52,549
грубой силой умным методом грубой силы, и

522
00:20:52,549 --> 00:20:54,259
это часто может быть очень эффективным, поэтому

523
00:20:54,259 --> 00:20:55,729
их хорошо рассматривать с точки зрения

524
00:20:55,729 --> 00:20:59,809
приложений, на которые вы смотрите, но вы знаете,

525
00:20:59,809 --> 00:21:03,320
несмотря на это, даже если они могут быть

526
00:21:03,320 --> 00:21:04,969
действительно хорошими, а иногда  они очень,

527
00:21:04,969 --> 00:21:08,839
очень полезны для паралича, я, как

528
00:21:08,839 --> 00:21:10,339
правило, не очень эффективны с точки зрения выборки, и

529
00:21:10,339 --> 00:21:11,659
поэтому в зависимости от области, на которую вы

530
00:21:11,659 --> 00:21:12,950
смотрите, и от того, какую структуру

531
00:21:12,950 --> 00:21:14,659
вы имеете, часто бывает полезно перейти к

532
00:21:14,659 --> 00:21:16,639
методу, основанному на градиенте, особенно если

533
00:21:16,639 --> 00:21:18,320
вы можете  быть удовлетворенным локальным

534
00:21:18,320 --> 00:21:20,779
решением в конце, своего рода локально

535
00:21:20,779 --> 00:21:23,029
оптимальным, поэтому то, о чем мы будем говорить

536
00:21:23,029 --> 00:21:24,499
в основном сегодня, точно так же, как то, что мы сделали

537
00:21:24,499 --> 00:21:28,219
для методов, основанных на ценности, основано на

538
00:21:28,219 --> 00:21:30,859
градиентном спуске, основанном на ингредиентах.

539
00:21:30,859 --> 00:21:33,619
hods и другие методы, которые пытаются

540
00:21:33,619 --> 00:21:35,839
использовать последовательную структуру

541
00:21:35,839 --> 00:21:38,929
проблем принятия решений, чтобы CME ничего не

542
00:21:38,929 --> 00:21:40,219
знал о том факте, что

543
00:21:40,219 --> 00:21:42,559
этот мир может быть MDP или каким-

544
00:21:42,559 --> 00:21:44,719
либо последовательным стохастическим процессом Портера,

545
00:21:44,719 --> 00:21:46,309
и мы собираемся сосредоточиться на  те, которые как

546
00:21:46,309 --> 00:21:47,869
бы используют структуру

547
00:21:47,869 --> 00:21:49,700
марковского процесса принятия решений и сам процесс принятия решений

548
00:21:49,700 --> 00:21:53,269
, поэтому давайте поговорим о

549
00:21:53,269 --> 00:21:57,679
методах градиента политики.

550
00:21:57,679 --> 00:21:59,419


551
00:21:59,419 --> 00:22:01,159


552
00:22:01,159 --> 00:22:02,839
мы сосредоточимся на

553
00:22:02,839 --> 00:22:04,789
эпизодических MDP, что означает, что мы будем

554
00:22:04,789 --> 00:22:06,409
применять нашу политику, если пройдет определенное

555
00:22:06,409 --> 00:22:08,659
количество временных шагов, пока мы не достигнем конечного состояния, или, как

556
00:22:08,659 --> 00:22:09,690
вы знаете, может быть,

557
00:22:09,690 --> 00:22:11,789
восемь шагов, мы получим некоторое вознаграждение в

558
00:22:11,789 --> 00:22:13,259
течение этого периода времени, а затем мы

559
00:22:13,259 --> 00:22:18,600
собираемся  reset, поэтому мы собираемся

560
00:22:18,600 --> 00:22:20,250
искать локальный максимум, и мы

561
00:22:20,250 --> 00:22:22,230
собираемся взять градиент по отношению

562
00:22:22,230 --> 00:22:24,809
к параметрам, которые определяют политику,

563
00:22:24,809 --> 00:22:29,669
а затем использовать небольшую скорость обучения, так

564
00:22:29,669 --> 00:22:30,899
что это просто шоу  ld выглядят очень

565
00:22:30,899 --> 00:22:37,769
похожими, очень похожими

566
00:22:37,769 --> 00:22:45,000
на поиск на основе Q и V, и основное

567
00:22:45,000 --> 00:22:46,139
отличие здесь заключается в том, что вместо того, чтобы

568
00:22:46,139 --> 00:22:48,509
брать производную по

569
00:22:48,509 --> 00:22:50,159
параметрам, которые определяют нашу функцию Q,

570
00:22:50,159 --> 00:22:51,299
мы берем их по

571
00:22:51,299 --> 00:22:56,879
параметрам, которые определяют нашу политику.  так что

572
00:22:56,879 --> 00:23:00,360
самое простое, что здесь можно сделать, это

573
00:23:00,360 --> 00:23:02,070
вычислить конечные разности,

574
00:23:02,070 --> 00:23:04,799
поэтому для каждого из параметров вашей политики

575
00:23:04,799 --> 00:23:08,669
вы просто немного возмутите его, и если

576
00:23:08,669 --> 00:23:10,110
вы сделаете это для всех

577
00:23:10,110 --> 00:23:13,289
параметров, определяющих параметры вашей политики

578
00:23:13,289 --> 00:23:15,419
, вы получите

579
00:23:15,419 --> 00:23:17,100
оценка градиента, вы просто

580
00:23:17,100 --> 00:23:18,450
делаете своего рода оценку градиента с конечными разностями,

581
00:23:18,450 --> 00:23:23,820
и вы можете

582
00:23:23,820 --> 00:23:25,860
использовать определенное количество оценок для

583
00:23:25,860 --> 00:23:27,450
этого в каждом из случаев, поэтому вы

584
00:23:27,450 --> 00:23:29,879
можете сказать, что у вас есть этот K-

585
00:23:29,879 --> 00:23:32,159
мерный набор параметров, который

586
00:23:32,159 --> 00:23:34,500
определил  вашей политики, вы пытаетесь немного изменить одну

587
00:23:34,500 --> 00:23:36,000
из них, вы повторяете ее, вы

588
00:23:36,000 --> 00:23:37,350
получаете кучу образцов для этой новой

589
00:23:37,350 --> 00:23:39,750
политики, вы делаете это для всех

590
00:23:39,750 --> 00:23:41,519
различных измерений, и теперь у вас есть

591
00:23:41,519 --> 00:23:43,950
приблизительный  Создание градиента очень

592
00:23:43,950 --> 00:23:46,080
просто, довольно шумно и не

593
00:23:46,080 --> 00:23:47,610
особенно эффективно, но иногда может

594
00:23:47,610 --> 00:23:49,259
быть эффективным, и это была одна из

595
00:23:49,259 --> 00:23:51,299
ранних демонстраций того, как

596
00:23:51,299 --> 00:23:53,129
методы градиента политики могут быть очень полезны в

597
00:23:53,129 --> 00:23:55,919
контексте RL, и приятно то, что

598
00:23:55,919 --> 00:23:57,299
сама политика не  не должны быть

599
00:23:57,299 --> 00:23:59,519
дифференцируемыми, потому что мы просто делаем

600
00:23:59,519 --> 00:24:00,629
своего рода аппроксимацию градиента с помощью конечных разностей,

601
00:24:00,629 --> 00:24:05,460
так что же

602
00:24:05,460 --> 00:24:08,370
это за первые примеры, которые я вижу, о которых я

603
00:24:08,370 --> 00:24:09,990
думаю, когда думаю о том, как

604
00:24:09,990 --> 00:24:12,720
методы градиента политики или

605
00:24:12,720 --> 00:24:14,129
методы поиска политики могут быть  действительно

606
00:24:14,129 --> 00:24:18,179
эффективно Питер Стоун работает над созданием RoboCup,

607
00:24:18,179 --> 00:24:21,679
и кто здесь когда-либо видел, как Робокоп,

608
00:24:21,679 --> 00:24:23,250
хорошо, несколько человек,

609
00:24:23,250 --> 00:24:25,050
не все, так что давайте посмотрим, сможем ли мы

610
00:24:25,050 --> 00:24:27,600
встать, как короткая демонстрация того, как

611
00:24:27,600 --> 00:24:36,090
выглядят эти роботы, так что все в порядке,

612
00:24:36,090 --> 00:24:37,560
так что вы, вероятно, можете  мы

613
00:24:37,560 --> 00:24:38,880
не будем этого делать прямо сейчас,

614
00:24:38,880 --> 00:24:40,770
но, по сути, у вас

615
00:24:40,770 --> 00:24:42,240
есть куча разных лиг Робокопа,

616
00:24:42,240 --> 00:24:44,040
одна из целей, я думаю,

617
00:24:44,040 --> 00:24:45,960
к 2050 году цель  что у

618
00:24:45,960 --> 00:24:50,130
нас будет роботизированная футбольная команда, которая

619
00:24:50,130 --> 00:24:52,590
сможет побеждать, как вы знаете, может

620
00:24:52,590 --> 00:24:55,890
выиграть чемпионат мира, так что это была одна

621
00:24:55,890 --> 00:24:57,570
из движущих целей своего рода

622
00:24:57,570 --> 00:24:59,820
инициативы Робокопа, и в этом есть много

623
00:24:59,820 --> 00:25:01,800
разных лиг.  и один из

624
00:25:01,800 --> 00:25:03,390
них - это такие роботы cuatro ped,

625
00:25:03,390 --> 00:25:05,700
которые пытаются забивать голы

626
00:25:05,700 --> 00:25:07,710
друг против друга, и одна из ключевых

627
00:25:07,710 --> 00:25:09,240
проблем для этого заключается в том, что они выглядят

628
00:25:09,240 --> 00:25:12,840
примерно так, и вам нужно

629
00:25:12,840 --> 00:25:15,120
выяснить ворота для ходьбы, и вы хотите

630
00:25:15,120 --> 00:25:17,280
их  иметь возможность быстро ходить, но вы

631
00:25:17,280 --> 00:25:19,500
не хотите, чтобы они падали, и поэтому

632
00:25:19,500 --> 00:25:21,120
просто такой вопрос, как

633
00:25:21,120 --> 00:25:23,010
оптимизировать ворота, является важным

634
00:25:23,010 --> 00:25:24,660
вопросом для победы, потому что вам

635
00:25:24,660 --> 00:25:26,220
нужно, чтобы ваши роботы быстро перемещались по

636
00:25:26,220 --> 00:25:28,650
полю, поэтому  Питер Стоун долгое время был ведущим

637
00:25:28,650 --> 00:25:30,720
специалистом в Робокопе,

638
00:25:30,720 --> 00:25:32,670
и их цель состояла в том, чтобы просто научиться

639
00:25:32,670 --> 00:25:35,460
быстрому способу передвижения этих ИИ-луков и

640
00:25:35,460 --> 00:25:39,570
сделать это на основе реального опыта, и данные

641
00:25:39,570 --> 00:25:40,770
здесь действительно важны, потому что это

642
00:25:40,770 --> 00:25:42,630
дорого, как у вас есть эти  роботы

643
00:25:42,630 --> 00:25:44,040
ш  ходить взад и вперед, и вы хотите, чтобы

644
00:25:44,040 --> 00:25:46,770
они очень быстро оптимизировали свою походку, и

645
00:25:46,770 --> 00:25:48,390
вам не нужно постоянно менять

646
00:25:48,390 --> 00:25:49,830
батарейки и тому подобное, поэтому вы

647
00:25:49,830 --> 00:25:50,910
действительно хотите сделать это с очень небольшим

648
00:25:50,910 --> 00:25:53,910
количеством данных, так что они думали

649
00:25:53,910 --> 00:25:55,380
сделать в этом случае  является своего рода

650
00:25:55,380 --> 00:25:56,700
параметризованной политикой и попыткой оптимизировать

651
00:25:56,700 --> 00:26:00,750
эти надлежащие политики, так что именно здесь

652
00:26:00,750 --> 00:26:02,610
появились важные знания предметной области, и

653
00:26:02,610 --> 00:26:03,990
это способ внедрить знания предметной области,

654
00:26:03,990 --> 00:26:06,360
поэтому они указали это в виде

655
00:26:06,360 --> 00:26:10,770
непрерывного эллипса того, как ворота работают для

656
00:26:10,770 --> 00:26:13,350
этого небольшого  робота, и поэтому они

657
00:26:13,350 --> 00:26:14,550
параметризуют его этими двенадцатью

658
00:26:14,550 --> 00:26:16,530
непрерывными параметрами, и это

659
00:26:16,530 --> 00:26:18,780
полностью определяет пространство возможных

660
00:26:18,780 --> 00:26:21,120
политик, которые вы могли бы изучить, это может

661
00:26:21,120 --> 00:26:23,370
быть не оптимальным. Питер Стоун и его группа

662
00:26:23,370 --> 00:26:24,900
имеют огромный опыт в

663
00:26:24,900 --> 00:26:27,270
создании RoboCup в то время, когда они делают

664
00:26:27,270 --> 00:26:28,740
это.  документ, и поэтому у них действительно было

665
00:26:28,740 --> 00:26:30,210
много знаний, которые они могли вставить сюда,

666
00:26:30,210 --> 00:26:31,560
и в некотором смысле это способ обеспечить

667
00:26:31,560 --> 00:26:33,390
своего рода эту иерархическую структуру

668
00:26:33,390 --> 00:26:35,220
о том, какие политики  может быть

669
00:26:35,220 --> 00:26:36,250
хорошо,

670
00:26:36,250 --> 00:26:38,080
а затем то, что они сделали, так это то, что они просто использовали

671
00:26:38,080 --> 00:26:39,790
этот метод конечных разностей

672
00:26:39,790 --> 00:26:41,380
, чтобы попытаться оптимизировать все

673
00:26:41,380 --> 00:26:45,700
эти параметры, поэтому одна из важных

674
00:26:45,700 --> 00:26:47,890
вещей здесь заключается в том, что все их

675
00:26:47,890 --> 00:26:49,210
оценки политики мы собираемся сделать.  на

676
00:26:49,210 --> 00:26:51,730
настоящих настоящих роботах, и они просто хотели,

677
00:26:51,730 --> 00:26:53,890
чтобы люди время от времени вмешивались

678
00:26:53,890 --> 00:26:55,450
, чтобы заменить

679
00:26:55,450 --> 00:26:57,160
батареи, которые происходили примерно раз

680
00:26:57,160 --> 00:26:59,770
в час, и поэтому они сделали это на трех I

681
00:26:59,770 --> 00:27:01,930
BOS очень небольшом количестве оборудования, они

682
00:27:01,930 --> 00:27:03,580
сделали около пятнадцати политик  за итерацию,

683
00:27:03,580 --> 00:27:06,760
и они оценивали каждую политику три

684
00:27:06,760 --> 00:27:08,620
раза, так что это не очень много, что может

685
00:27:08,620 --> 00:27:11,140
быть очень шумным сигналом, и каждая итерация

686
00:27:11,140 --> 00:27:14,260
занимала около семи целых пяти минут,

687
00:27:14,260 --> 00:27:15,520
поэтому им пришлось выбрать некоторую скорость обучения,

688
00:27:15,520 --> 00:27:17,620
и что мы видим в этом случае?

689
00:27:17,620 --> 00:27:19,960
мы видим, что с точки зрения

690
00:27:19,960 --> 00:27:21,670
количества итераций, которые они имеют, по сравнению с тем, как

691
00:27:21,670 --> 00:27:23,470
быстро они, конечно, вы должны

692
00:27:23,470 --> 00:27:25,300
определить свои критерии оптимизации, в

693
00:27:25,300 --> 00:27:26,530
этом случае они смотрят на скорость

694
00:27:26,530 --> 00:27:29,560
стабильной ходьбы и множество  люди

695
00:27:29,560 --> 00:27:31,120
пытались выяснить, как сделать

696
00:27:31,120 --> 00:27:34,150
это, используя ручную настройку, прежде чем включить их,

697
00:27:34,150 --> 00:27:36,160
так что они команда UT Austin Villa,

698
00:27:36,160 --> 00:27:38,530
включая их в прошлом, люди

699
00:27:38,530 --> 00:27:39,910
нашли разные способы ручной

700
00:27:39,910 --> 00:27:42,010
настройки, и я не знаю, если я  используя

701
00:27:42,010 --> 00:27:43,840
обучение с учителем и т. д., и вы можете

702
00:27:43,840 --> 00:27:46,030
видеть, как они делают несколько итераций,

703
00:27:46,030 --> 00:27:47,800
пытаясь найти лучшую политику,

704
00:27:47,800 --> 00:27:49,780
используя этот метод конечных разностей, к которому

705
00:27:49,780 --> 00:27:51,250
они приходят быстрее, чем все остальное,

706
00:27:51,250 --> 00:27:54,640
и это не так уж много итераций, так

707
00:27:54,640 --> 00:27:55,750
что это то, что происходило, вы

708
00:27:55,750 --> 00:27:59,800
знаете  через несколько часов, поэтому я

709
00:27:59,800 --> 00:28:00,880
думаю, что это действительно убедительный

710
00:28:00,880 --> 00:28:03,580
пример того, как методы градиента политики

711
00:28:03,580 --> 00:28:05,470
могут действительно работать намного лучше, чем то,

712
00:28:05,470 --> 00:28:07,330
что происходило раньше, и им не нужно

713
00:28:07,330 --> 00:28:08,830
было требовать огромного количества данных,

714
00:28:08,830 --> 00:28:10,960
которые сильно отличаются от того, что

715
00:28:10,960 --> 00:28:12,940
вы, вероятно,  повторно испытывая назначение, так

716
00:28:12,940 --> 00:28:15,930
что это не общее количество итераций, я

717
00:28:15,930 --> 00:28:19,230
думаю, что это было в порядке, давайте посмотрим,

718
00:28:19,230 --> 00:28:22,870
как нет, это в порядке, как вы знаете, от

719
00:28:22,870 --> 00:28:24,760
десятков до сотен политик, а не

720
00:28:24,760 --> 00:28:26,950
миллионов  львы и миллионы шагов, так что эти

721
00:28:26,950 --> 00:28:29,020
вещи могут быть очень эффективными с данными, но

722
00:28:29,020 --> 00:28:30,730
было также много информации, которая

723
00:28:30,730 --> 00:28:35,530
была предоставлена, поэтому, если вы немного подумаете о чем-то

724
00:28:35,530 --> 00:28:39,490
вроде здесь, так что в своей

725
00:28:39,490 --> 00:28:40,870
статье они обсудили это о том, что на

726
00:28:40,870 --> 00:28:42,220
самом деле влияло на производительность  в этом

727
00:28:42,220 --> 00:28:43,900
случае, и есть много вещей, которые

728
00:28:43,900 --> 00:28:46,990
влияют на производительность, поэтому вы знаете, как мы

729
00:28:46,990 --> 00:28:49,200
начинаем,

730
00:28:49,200 --> 00:28:50,760
поэтому может иметь смысл, что вы знаете,

731
00:28:50,760 --> 00:28:52,560
почему используемые начальные параметры политики

732
00:28:52,560 --> 00:28:56,420
имеют значение для этого типа метода,

733
00:28:56,750 --> 00:28:59,250
гарантированно имеющий глобальный оптимум

734
00:28:59,250 --> 00:29:01,440
только локальный оптимум  поэтому ваша отправная

735
00:29:01,440 --> 00:29:03,540
точка будет влиять на то, какое локальное

736
00:29:03,540 --> 00:29:06,570
свойство может, поэтому только что было сказано, что,

737
00:29:06,570 --> 00:29:08,070
поскольку эти методы только

738
00:29:08,070 --> 00:29:09,570
гарантированы, в частности, этот метод

739
00:29:09,570 --> 00:29:11,610
гарантирует только нахождение локальных оптимумов,

740
00:29:11,610 --> 00:29:13,020
и все виды методов стиля градиента политики

741
00:29:13,020 --> 00:29:15,300
затем находятся где угодно  вы

742
00:29:15,300 --> 00:29:16,620
начнете, вы доберетесь до ближайшей

743
00:29:16,620 --> 00:29:18,030
локальной оптимы, и у вас нет гарантии

744
00:29:18,030 --> 00:29:21,090
, что это лучшая глобальная оптима, поэтому

745
00:29:21,090 --> 00:29:22,680
важно либо попробовать много

746
00:29:22,680 --> 00:29:24,360
случайных перезапусков  s здесь, в этом случае, или

747
00:29:24,360 --> 00:29:27,390
иметь знание предметной области, другой важный

748
00:29:27,390 --> 00:29:29,520
вопрос здесь заключается в том, насколько вы

749
00:29:29,520 --> 00:29:30,690
возмущаете размер ваших конечных

750
00:29:30,690 --> 00:29:33,330
различий, и тогда я думаю, что действительно наиболее

751
00:29:33,330 --> 00:29:35,340
важной является эта параметризация политики,

752
00:29:35,340 --> 00:29:37,410
например, как вы записываете

753
00:29:37,410 --> 00:29:39,390
пространство возможных политик.  что вы можете

754
00:29:39,390 --> 00:29:41,190
учиться с этим, потому что это не хорошее

755
00:29:41,190 --> 00:29:42,840
политическое пространство, тогда вы просто

756
00:29:42,840 --> 00:29:55,320
ничего не узнаете, да,

757
00:29:55,320 --> 00:29:57,810
так что особенно было о части политики открытого цикла,

758
00:29:57,810 --> 00:29:59,910
поэтому эти политики, которые мы

759
00:29:59,910 --> 00:30:01,680
изучаем, не должны быть адаптивными и

760
00:30:01,680 --> 00:30:03,780
Политика открытого цикла — это, по сути, план

761
00:30:03,780 --> 00:30:05,870
, это последовательность действий, которые необходимо предпринять

762
00:30:05,870 --> 00:30:08,160
независимо от каких-либо дополнительных входных данных, которые

763
00:30:08,160 --> 00:30:11,310
вы можете иметь, поэтому мы обычно

764
00:30:11,310 --> 00:30:13,170
думали о политиках как о сопоставлении

765
00:30:13,170 --> 00:30:15,870
состояний с действиями, но они также могут

766
00:30:15,870 --> 00:30:18,060
быть просто серией действий, и поэтому, когда мы

767
00:30:18,060 --> 00:30:20,130
говорить о политике открытого цикла, которая является

768
00:30:20,130 --> 00:30:22,830
нереактивной политикой, потому что это просто

769
00:30:22,830 --> 00:30:24,360
последовательность действий, которую

770
00:30:24,360 --> 00:30:25,680
вы просто продолжаете выполнять независимо от состояния робота,

771
00:30:25,680 --> 00:30:27,990
поэтому, возможно, есть  действительно

772
00:30:27,990 --> 00:30:29,670
сильный ветер посередине, и следующее действие робота

773
00:30:29,670 --> 00:30:31,200
одинаково, независимо от

774
00:30:31,200 --> 00:30:32,820
того, сильный ветер или нет, он не должен быть

775
00:30:32,820 --> 00:30:35,300
реактивным,

776
00:30:39,850 --> 00:30:43,880
хорошо, но в целом вы знаете, что конечные

777
00:30:43,880 --> 00:30:45,740
различия - это разумная вещь, чтобы пробовать

778
00:30:45,740 --> 00:30:47,750
часто, мы собираемся  хотите использовать

779
00:30:47,750 --> 00:30:49,160
информацию о градиенте и использовать тот факт, что

780
00:30:49,160 --> 00:30:50,960
наша политика для функции на самом деле

781
00:30:50,960 --> 00:30:54,860
дифференцируема, поэтому сейчас мы собираемся

782
00:30:54,860 --> 00:30:56,929
вычислить градиент политики

783
00:30:56,929 --> 00:30:59,030
аналитически, извините меня, это наиболее

784
00:30:59,030 --> 00:31:00,860
распространено в большинстве методов, которые

785
00:31:00,860 --> 00:31:03,170
используются прямо сейчас, мы собираемся  предположим, что оно

786
00:31:03,170 --> 00:31:05,210
дифференцируемо везде, где оно не равно нулю, и

787
00:31:05,210 --> 00:31:07,520
что мы можем вычислить это явно, поэтому,

788
00:31:07,520 --> 00:31:09,170
когда мы говорим, что мы говорим, что знаем

789
00:31:09,170 --> 00:31:13,760
, что это означает, что это вычислимо, мы

790
00:31:13,760 --> 00:31:15,800
можем вычислить это явно, и поэтому теперь

791
00:31:15,800 --> 00:31:17,570
мы будем думать только о

792
00:31:17,570 --> 00:31:20,380
методах, основанных на градиенте, и  так что

793
00:31:20,380 --> 00:31:22,820
мы только собираемся сходиться только к локальному

794
00:31:22,820 --> 00:31:25,820
оптимуму, надеюсь, надеюсь, мы доберемся

795
00:31:25,820 --> 00:31:27,200
до локального оптимума, это лучшее, на что мы можем

796
00:31:27,200 --> 00:31:34,700
надеяться в этом случае, хорошо, так что мы

797
00:31:34,700 --> 00:31:35,540
будем часто говорить с людьми та  Расскажите о

798
00:31:35,540 --> 00:31:38,690
политике отношения правдоподобия, и они будут

799
00:31:38,690 --> 00:31:39,920
действовать следующим образом,

800
00:31:39,920 --> 00:31:42,020
поэтому мы думаем об эпизодическом случае,

801
00:31:42,020 --> 00:31:43,390
поэтому мы будем думать о нем как о наличии

802
00:31:43,390 --> 00:31:45,890
траекторий, поэтому заявите действие или слово

803
00:31:45,890 --> 00:31:48,590
следующее состояние и т. д. на всем пути к какому-то

804
00:31:48,590 --> 00:31:50,600
терминалу  Укажите, что здесь мы

805
00:31:50,600 --> 00:31:55,790
заканчиваем, и мы собираемся использовать R tau

806
00:31:55,790 --> 00:31:57,800
для обозначения суммы вознаграждений за

807
00:31:57,800 --> 00:32:03,590
траекторию, поэтому значение политики в этом

808
00:32:03,590 --> 00:32:07,820
случае будет просто ожидаемым

809
00:32:07,820 --> 00:32:09,410
количеством вознаграждений, которые мы получаем, следуя этой

810
00:32:09,410 --> 00:32:12,559
политике, и мы можем  представить это как

811
00:32:12,559 --> 00:32:14,570
вероятность того, что мы наблюдали

812
00:32:14,570 --> 00:32:17,179
конкретную траекторию, умноженную на награду

813
00:32:17,179 --> 00:32:21,380
за эту траекторию, поэтому в ней просто говорится, что в

814
00:32:21,380 --> 00:32:22,820
соответствии с этой политикой какова, как вы

815
00:32:22,820 --> 00:32:23,929
знаете, вероятность увидеть любую

816
00:32:23,929 --> 00:32:25,670
траекторию, а затем какова будет

817
00:32:25,670 --> 00:32:28,010
награда за эту траекторию, потому что

818
00:32:28,010 --> 00:32:29,870
награда  это просто детерминированная

819
00:32:29,870 --> 00:32:31,910
функция траектории, когда вы знаете

820
00:32:31,910 --> 00:32:33,920
вознаграждение за действие в состоянии и т. д., тогда ваше

821
00:32:33,920 --> 00:32:36,170
вознаграждение представляет собой просто сумму всех этих значений,

822
00:32:36,170 --> 00:32:39,830
и теперь в этом конкретном обозначении

823
00:32:39,830 --> 00:32:42,170
наша цель будет  быть заключается в том, чтобы найти

824
00:32:42,170 --> 00:32:44,870
параметры политики тета, которые являются

825
00:32:44,870 --> 00:32:48,800
аргументами этого, и причина, по которой мы прошли через

826
00:32:48,800 --> 00:32:50,720
то, что мы изменили здесь,

827
00:32:50,720 --> 00:32:52,230
изменение теперь bin имеет

828
00:32:52,230 --> 00:32:54,120
тот факт, на котором мы собираемся сосредоточиться

829
00:32:54,120 --> 00:32:56,910
здесь, поэтому обратите внимание, что

830
00:32:56,910 --> 00:32:59,340
только параметры политики  появляются с точки зрения

831
00:32:59,340 --> 00:33:01,320
распределения траекторий, с которыми мы

832
00:33:01,320 --> 00:33:05,520
можем столкнуться в соответствии с этой политикой, и

833
00:33:05,520 --> 00:33:07,470
это снова немного похоже на

834
00:33:07,470 --> 00:33:08,580
то, о чем мы говорили для обучения по приглашению,

835
00:33:08,580 --> 00:33:10,110
прежде чем мы столкнулись с имитационным

836
00:33:10,110 --> 00:33:11,240
обучением, мы много говорили о

837
00:33:11,240 --> 00:33:12,960
распределениях состояний и

838
00:33:12,960 --> 00:33:14,820
распределениях состояний и  действий и

839
00:33:14,820 --> 00:33:16,290
пытаясь найти политику, которая соответствовала бы

840
00:33:16,290 --> 00:33:18,390
тому же распределению действий состояния,

841
00:33:18,390 --> 00:33:22,080
которое было продемонстрировано сегодня экспертом,

842
00:33:22,080 --> 00:33:23,340
мы не собираемся говорить так много о

843
00:33:23,340 --> 00:33:25,380
распределении действий состояний, но

844
00:33:25,380 --> 00:33:26,820
мы говорим о типах

845
00:33:26,820 --> 00:33:28,770
распределений траекторий, которые мы

846
00:33:28,770 --> 00:33:30,330
может встретиться в соответствии с нашей конкретной

847
00:33:30,330 --> 00:33:36,390
политикой, так каков градиент этого, поэтому

848
00:33:36,390 --> 00:33:38,040
мы хотим взять градиент этой

849
00:33:38,040 --> 00:33:40,620
функции по отношению к тета, поэтому мы

850
00:33:40,620 --> 00:33:43,950
собираемся  пойдем для этого следующим образом, мы собираемся

851
00:33:43,950 --> 00:33:46,080
переписать, какова вероятность

852
00:33:46,080 --> 00:33:53,600
траектории ниже тета, так что я выше тау,

853
00:33:53,600 --> 00:33:59,360
теперь мы собираемся сделать вероятность тау

854
00:34:00,169 --> 00:34:04,740
раз, хорошо, сначала на самом деле я перенесу

855
00:34:04,740 --> 00:34:06,980
ее,

856
00:34:21,070 --> 00:34:42,469
а затем что мы  что мы собираемся сделать, так это то,

857
00:34:42,469 --> 00:34:43,820
что мы собираемся сделать, это сделать

858
00:34:43,820 --> 00:34:45,739
что-то простое, где мы просто умножаем

859
00:34:45,739 --> 00:34:48,109
и делим на одно и то же, поэтому мы

860
00:34:48,109 --> 00:34:49,879
собираемся ввести вероятность тау, учитывая

861
00:34:49,879 --> 00:34:52,969
тета, деленную на вероятность заданного тау.

862
00:34:52,969 --> 00:34:56,659
тета, умноженная на производную

863
00:34:56,659 --> 00:35:01,790
вероятности тау с учетом тета, и

864
00:35:01,790 --> 00:35:05,000
вероятность, если бы вместо этого у нас был логарифм, поэтому,

865
00:35:05,000 --> 00:35:06,640
если бы мы брали производную от

866
00:35:06,640 --> 00:35:10,849
логарифма вероятности тау с учетом тета,

867
00:35:10,849 --> 00:35:15,920
которая точно равна 1 над

868
00:35:15,920 --> 00:35:19,040
вероятностью тау с учетом тета раз

869
00:35:19,040 --> 00:35:23,660
производная P от тау с учетом тета, поэтому мы

870
00:35:23,660 --> 00:35:25,820
можем повторно выразить это следующим образом:

871
00:35:25,820 --> 00:35:30,500
сумма по тау R от тау P от тау с учетом

872
00:35:30,500 --> 00:35:34,940
тета, умноженная на производную по отношению к

873
00:35:34,940 --> 00:35:43,130
логарифму P от тау, пока что это не

874
00:35:43,130 --> 00:35:44,599
обязательно похоже на то, что это будет  быть

875
00:35:44,599 --> 00:35:46,430
очень полезным, поэтому мы сделали

876
00:35:46,430 --> 00:35:48,230
это  разумная трансформация, но

877
00:35:48,230 --> 00:35:49,550
вскоре мы увидим, почему эта

878
00:35:49,550 --> 00:35:51,440
трансформация полезна, и, в

879
00:35:51,440 --> 00:35:52,430
частности, причина, по которой эта

880
00:35:52,430 --> 00:35:54,109
трансформация полезна, заключается в том, что она

881
00:35:54,109 --> 00:35:55,880
будет очень полезной, когда мы подумаем о

882
00:35:55,880 --> 00:35:57,770
желании сделать все это, не

883
00:35:57,770 --> 00:36:00,980
зная динамики или моделей вознаграждения, поэтому

884
00:36:00,980 --> 00:36:01,820
мы  нам нужно иметь возможность

885
00:36:01,820 --> 00:36:03,619
получить вознаграждение с точки зрения

886
00:36:03,619 --> 00:36:05,720
траектории, но мы хотим иметь возможность

887
00:36:05,720 --> 00:36:08,150
оценить градиент политики,

888
00:36:08,150 --> 00:36:09,980
не зная модели динамики, и

889
00:36:09,980 --> 00:36:13,900
этот трюк поможет нам достичь этого,

890
00:36:14,410 --> 00:36:17,480
поэтому, когда мы это сделаем  это часто это

891
00:36:17,480 --> 00:36:18,830
часто называют отношением правдоподобия,

892
00:36:18,830 --> 00:36:22,099
теперь мы можем преобразовать его и просто сказать

893
00:36:22,099 --> 00:36:23,750
хорошо, мы замечаем, что, делая это, это

894
00:36:23,750 --> 00:36:27,250
на самом деле то же самое, что и журнал,

895
00:36:27,250 --> 00:36:29,020
теперь почему еще это начинает выглядеть

896
00:36:29,020 --> 00:36:30,490
как что-то, что могло бы быть  полезно хорошо,

897
00:36:30,490 --> 00:36:32,950
что у нас есть здесь у нас есть, если мы

898
00:36:32,950 --> 00:36:34,960
это сумма по всем траекториям, конечно,

899
00:36:34,960 --> 00:36:36,400
мы не обязательно имеем доступ ко всем

900
00:36:36,400 --> 00:36:38,230
возможным траекториям, но мы можем попробовать

901
00:36:38,230 --> 00:36:40,359
их, чтобы вы могли представить, что начинаете

902
00:36:40,359 --> 00:36:42,220
быть  Вы можете аппроксимировать это, запустив свою

903
00:36:42,220 --> 00:36:43,900
политику несколько раз,

904
00:36:43,900 --> 00:36:45,640
выбрав несколько траекторий,

905
00:36:45,640 --> 00:36:49,000
глядя на вознаграждение за них, а затем

906
00:36:49,000 --> 00:36:50,890
взяв производную относительно

907
00:36:50,890 --> 00:36:52,300
этой вероятности траектории с учетом

908
00:36:52,300 --> 00:36:57,820
тета, поэтому обычно мы собираемся сделать

909
00:36:57,820 --> 00:36:59,560
это, просто запустив  политику M раз,

910
00:36:59,560 --> 00:37:03,460
а затем P, если они скажут заданную тету,

911
00:37:03,460 --> 00:37:05,950
мы просто аппроксимируем это

912
00:37:05,950 --> 00:37:06,520
следующим

913
00:37:06,520 --> 00:37:09,190
образом, чтобы часть выпадала, мы просто собираемся

914
00:37:09,190 --> 00:37:11,320
равномерно взвесить все траектории, которые мы

915
00:37:11,320 --> 00:37:13,180
получили во время нашей выборки, а

916
00:37:13,180 --> 00:37:14,650
затем мы смотрим на  вознаграждение за эту

917
00:37:14,650 --> 00:37:19,660
траекторию и журнал PF Tau с учетом

918
00:37:19,660 --> 00:37:24,730
тета, так что происходит в этом случае,

919
00:37:24,730 --> 00:37:26,290
хорошо, так что это говорит о том, что градиент

920
00:37:26,290 --> 00:37:30,060
- это своего рода правило, где мы получаем

921
00:37:30,060 --> 00:37:34,119
умноженное на журнал вероятности этой

922
00:37:34,119 --> 00:37:35,800
траектории для вознаграждения  слово,

923
00:37:35,800 --> 00:37:38,530
связанное со словом, умноженное на тета, итак, что

924
00:37:38,530 --> 00:37:43,690
происходит в этом случае, поэтому в этом случае

925
00:37:43,690 --> 00:37:45,580
у нас есть функция, которая для нашего случая

926
00:37:45,580 --> 00:37:47,020
является наградой, которая как бы измеряет,

927
00:37:47,020 --> 00:37:50,740
насколько хорошо эта конкретная траектория вам известна.

928
00:37:50,740 --> 00:37:52,330
i  s или насколько хороша эта выборка,

929
00:37:52,330 --> 00:37:54,910
и что это делает, так это то, что мы просто

930
00:37:54,910 --> 00:37:57,400
движемся вверх по траектории логарифмической

931
00:37:57,400 --> 00:38:00,160
вероятности этой выборки в зависимости от того, насколько

932
00:38:00,160 --> 00:38:03,040
она хороша, поэтому мы хотим увеличить наши

933
00:38:03,040 --> 00:38:06,310
параметры, которые отвечают за то, что мы

934
00:38:06,310 --> 00:38:10,869
получаем  образцы, которые хороши, поэтому мы

935
00:38:10,869 --> 00:38:12,760
хотим иметь параметры в нашей политике

936
00:38:12,760 --> 00:38:14,410
, которые заставят нас выполнять

937
00:38:14,410 --> 00:38:20,109
траектории, которые дают нам высокое вознаграждение, поэтому,

938
00:38:20,109 --> 00:38:22,359
если мы подумаем о чем-то вроде здесь, f of X

939
00:38:22,359 --> 00:38:24,760
снова является вознаграждением, которое мы собираемся

940
00:38:24,760 --> 00:38:27,400
получить  наша политика параметризованная политика,

941
00:38:27,400 --> 00:38:32,200
мы хотим увеличить вес вещей

942
00:38:32,200 --> 00:38:35,980
в нашем пространстве, которые приводят к высокому вознаграждению, поэтому,

943
00:38:35,980 --> 00:38:36,610
если это наше

944
00:38:36,610 --> 00:38:38,350
f of X, которое является нашей функцией вознаграждения, и

945
00:38:38,350 --> 00:38:39,370
это вероятность наших

946
00:38:39,370 --> 00:38:41,410
траекторий, тогда мы хотим переписать нашу

947
00:38:41,410 --> 00:38:42,850
политику так, чтобы  попытайтесь увеличить

948
00:38:42,850 --> 00:38:44,620
вероятность траекторий, которые приносят

949
00:38:44,620 --> 00:38:47,680
высокое вознаграждение, чтобы в конечном итоге у вас были

950
00:38:47,680 --> 00:38:49,960
большие градиенты к вещам, которые

951
00:38:49,960 --> 00:38:57,940
имеют высокую ценность, высокое вознаграждение, хорошо,

952
00:38:57,940 --> 00:38:59,260
тогда следующий вопрос: собираюсь ли я это сделать,

953
00:38:59,260 --> 00:39:00,370
и я должен быть в состоянии

954
00:39:00,370 --> 00:39:03,250
приблизить  второй член, который представляет собой

955
00:39:03,250 --> 00:39:05,860
этот журнал, вы знаете

956
00:39:05,860 --> 00:39:07,120
производную по вероятности

957
00:39:07,120 --> 00:39:10,120
траектории при некоторых параметрах, поэтому я

958
00:39:10,120 --> 00:39:11,560
должен быть в состоянии выяснить,

959
00:39:11,560 --> 00:39:13,480
какова вероятность траектории при

960
00:39:13,480 --> 00:39:15,370
наборе параметров, мы можем сделать это следующим

961
00:39:15,370 --> 00:39:20,050
образом и  так что это будет Delta

962
00:39:20,050 --> 00:39:26,950
Theta логарифм произведения MU на s 0,

963
00:39:26,950 --> 00:39:29,020
так что это наш начальный начальный

964
00:39:29,020 --> 00:39:30,550
этап вероятность нашего начального начального

965
00:39:30,550 --> 00:39:35,920
состояния, умноженная на произведение над J, равна от 0

966
00:39:35,920 --> 00:39:39,190
до t минус 1 вероятности

967
00:39:39,190 --> 00:39:46,420
наблюдения следующего  состояние с учетом

968
00:39:46,420 --> 00:39:49,480
предпринятого действия, умноженного на

969
00:39:49,480 --> 00:39:51,250
вероятность совершения этого действия в соответствии с

970
00:39:51,250 --> 00:40:01,000
нашей текущей политикой, поэтому

971
00:40:01,000 --> 00:40:05,110
в конце есть еще одна скобка, и, поскольку

972
00:40:05,110 --> 00:40:06,610
это логарифм, мы можем просто разложить

973
00:40:06,610 --> 00:40:10,660
это так, чтобы оно было равно дельта

974
00:40:10,660 --> 00:40:15,130
тета логарифма  mu of s ноль плюс

975
00:40:15,130 --> 00:40:16,180
дельта тета

976
00:40:16,180 --> 00:40:18,250
сумма над дельта тета, потому что это логарифмический

977
00:40:18,250 --> 00:40:23,640
термин Джейка будет от 0 до t минус 1 логарифмическая

978
00:40:23,640 --> 00:40:26,800
модель перехода помните, что мы не

979
00:40:26,800 --> 00:40:28,390
знаем этого общее это неизвестно

980
00:40:28,390 --> 00:40:30,190
мы просто будем надеяться  d

981
00:40:30,190 --> 00:40:31,540
с выражением, которое означает, что нам это

982
00:40:31,540 --> 00:40:35,320
не нужно, и то, что я индексирую здесь,

983
00:40:35,320 --> 00:40:42,130
это то, на какой траектории мы находимся, да, это

984
00:40:42,130 --> 00:40:45,100
сумма по J равна от 0 до t минус 1, и

985
00:40:45,100 --> 00:40:48,700
это будут наши фактические параметры политики,

986
00:40:48,700 --> 00:40:51,150


987
00:40:53,800 --> 00:40:56,000
хорошо  кто-нибудь может сказать мне, почему

988
00:40:56,000 --> 00:40:58,100
это полезная декомпозиция и

989
00:40:58,100 --> 00:41:00,110
похоже ли, что нам это понадобится, поэтому

990
00:41:00,110 --> 00:41:04,130
позвольте мне просто параметры все эти

991
00:41:04,130 --> 00:41:07,550
вещи выглядит ли это обнадеживающе с точки

992
00:41:07,550 --> 00:41:09,170
зрения того, что нам не нужно знать, что такое

993
00:41:09,170 --> 00:41:26,240
динамическая модель, как  обо всех,

994
00:41:26,240 --> 00:41:27,440
просто поговорите еще раз со своим соседом,

995
00:41:27,440 --> 00:41:29,690
а затем скажите мне, какой из этих

996
00:41:29,690 --> 00:41:35,450
терминов будет равен 0, поэтому мы берем

997
00:41:35,450 --> 00:41:37,090
производную по отношению к тета, и

998
00:41:37,090 --> 00:41:41,680
какие из этих терминов зависят от тета,

999
00:42:23,960 --> 00:42:26,280
помните, что тета определяет

1000
00:42:26,280 --> 00:42:27,990
параметры вашей политики.  тета — это то, что

1001
00:42:27,990 --> 00:42:29,610
определяет, какое действие вы предпринимаете в

1002
00:42:29,610 --> 00:42:34,320
данном состоянии, хорошо, я проведу

1003
00:42:34,320 --> 00:42:36,510
быстрый опрос, и поэтому я назову эти

1004
00:42:36,510 --> 00:42:41,160
пункты один, два и три, зависит ли первый

1005
00:42:41,160 --> 00:42:42,960
член от тета, поднимите руку, если

1006
00:42:42,960 --> 00:42:46,710
да, ваша рука  не здорово хорошо да так

1007
00:42:46,710 --> 00:42:49,250
это независимо, так как это может быть 0,

1008
00:42:49,250 --> 00:42:51,660
поднимите руку, если второй член не

1009
00:42:51,660 --> 00:42:54,660
зависит от тета, отлично, так что это идет

1010
00:42:54,660 --> 00:42:57,720
к 0, поэтому единственное, что у нас меньше,

1011
00:42:57,720 --> 00:43:00,690
это то, что здорово, так что это хорошо,

1012
00:43:00,690 --> 00:43:02,370
и теперь это вроде как становится более ясным

1013
00:43:02,370 --> 00:43:05,310
почему мы сделали это странное логарифмическое преобразование,

1014
00:43:05,310 --> 00:43:06,600
потому что, когда мы сделали это странное логарифмическое

1015
00:43:06,600 --> 00:43:08,520
преобразование, оно позволило нам взять

1016
00:43:08,520 --> 00:43:11,100
этот продукт вероятности

1017
00:43:11,100 --> 00:43:12,420
действия, которое мы предприняли, и переходов состояний,

1018
00:43:12,420 --> 00:43:14,370
тогда вместо этого мы можем

1019
00:43:14,370 --> 00:43:17,040
разложить его на суммы, и теперь, когда мы

1020
00:43:17,040 --> 00:43:19,200
видим, что мы  разложив его на суммы, мы

1021
00:43:19,200 --> 00:43:20,910
можем применить производную отдельно, и

1022
00:43:20,910 --> 00:43:22,170
это означает, что некоторые из этих терминов

1023
00:43:22,170 --> 00:43:23,420
просто исчезают,

1024
00:43:23,420 --> 00:43:25,830
что действительно круто, поэтому это означает, что нам на

1025
00:43:25,830 --> 00:43:26,820
самом деле не нужно знать, что такое

1026
00:43:26,820 --> 00:43:28,920
модель перехода, нам не нужно

1027
00:43:28,920 --> 00:43:30,060
иметь явный  представление да

1028
00:43:30,060 --> 00:43:31,530
вопрос и имя сначала, пожалуйста,

1029
00:43:31,530 --> 00:43:33,810
и вопрос в том, мне было интересно

1030
00:43:33,810 --> 00:43:35,040
, не зависит ли динамика системы

1031
00:43:35,040 --> 00:43:38,960
от политики, хотя в целом

1032
00:43:38,960 --> 00:43:40,920
вопрос в том, зависит ли динамика

1033
00:43:40,920 --> 00:43:42,330
системы от t  Политика абсолютно,

1034
00:43:42,330 --> 00:43:46,620
но только через эту часть, так что как

1035
00:43:46,620 --> 00:43:48,720
будто агент может выбрать, какое действие он

1036
00:43:48,720 --> 00:43:50,190
предпримет, но как только он выберет,

1037
00:43:50,190 --> 00:43:52,350
динамика не зависит от агента, и

1038
00:43:52,350 --> 00:43:55,230
поэтому это разделение, поэтому, если у вас есть

1039
00:43:55,230 --> 00:43:56,880
другая политика, вы получите абсолютно

1040
00:43:56,880 --> 00:43:58,230
разные траектории  но то, как

1041
00:43:58,230 --> 00:44:00,570
вы получаете разные траектории,

1042
00:44:00,570 --> 00:44:02,640
зависит только от политики с точки

1043
00:44:02,640 --> 00:44:04,050
зрения выбранных действий, а

1044
00:44:04,050 --> 00:44:05,730
затем среда будет определять

1045
00:44:05,730 --> 00:44:07,920
тип следующих состояний, которые вы получите, и поэтому нам

1046
00:44:07,920 --> 00:44:09,300
не нужно знать это с точки зрения

1047
00:44:09,300 --> 00:44:12,720
оценивая влияние действий на

1048
00:44:12,720 --> 00:44:14,700
окружающую среду, это также будет происходить

1049
00:44:14,700 --> 00:44:16,500
с точки зрения вознаграждений, которые вы получаете, потому

1050
00:44:16,500 --> 00:44:17,880
что вознаграждения, которые вы получаете, также являются

1051
00:44:17,880 --> 00:44:19,650
функцией штата, поэтому вы будете посещать

1052
00:44:19,650 --> 00:44:20,790
разные части штата в зависимости

1053
00:44:20,790 --> 00:44:23,490
от действий, которые вы совершаете.  ответить на любые другие

1054
00:44:23,490 --> 00:44:25,850
вопросы,

1055
00:44:27,309 --> 00:44:30,099
а этот убедиться, что я понимаю, как

1056
00:44:30,099 --> 00:44:31,449
получить оценку вероятности

1057
00:44:31,449 --> 00:44:34,689
того, как раздачи и тета, я имею в виду, выглядят так,

1058
00:44:34,689 --> 00:44:37,630
как будто мы просто говорим об эпизодах, а это

1059
00:44:37,630 --> 00:44:38,650
на  я появился,

1060
00:44:38,650 --> 00:44:40,659
вы знаете, я раз буду высоко над тем

1061
00:44:40,659 --> 00:44:42,669
, что то, что мы здесь делаем, это

1062
00:44:42,669 --> 00:44:43,390
правильный

1063
00:44:43,390 --> 00:44:46,119
отличный вопрос, поэтому я спрашиваю, вы знаете,

1064
00:44:46,119 --> 00:44:49,809
это то, что я поставил здесь, это всего лишь один из

1065
00:44:49,809 --> 00:44:51,459
тех внутренних терминов, которые могли бы произойти

1066
00:44:51,459 --> 00:44:53,289
для одного я да  Итак, что мы здесь делаем, так

1067
00:44:53,289 --> 00:44:54,579
это то, что мы говорим, что собираемся принять эту

1068
00:44:54,579 --> 00:44:55,809
политику, мы собираемся запустить ее M раз,

1069
00:44:55,809 --> 00:44:57,339
ну, часть, не собираемся получать какие-либо

1070
00:44:57,339 --> 00:44:59,289
идентичные траектории, и что

1071
00:44:59,289 --> 00:45:01,150
мы собираемся сделать, это вычислить этот журнал

1072
00:45:01,150 --> 00:45:02,529
вероятности траектории для каждого

1073
00:45:02,529 --> 00:45:06,929
из них в отдельности, а затем суммировать их,

1074
00:45:06,929 --> 00:45:08,949
вы можете прийти к тому, что я имею в виду, что вы

1075
00:45:08,949 --> 00:45:11,079
знаете, что в детерминированных случаях вы могли бы, или

1076
00:45:11,079 --> 00:45:12,130
если вы знаете, приятель, не имеет много

1077
00:45:12,130 --> 00:45:13,419
стохастичности, и ваша

1078
00:45:13,419 --> 00:45:14,769
политика также не может  в конечном итоге вы получите несколько

1079
00:45:14,769 --> 00:45:16,269
траекторий, которые в целом идентичны,

1080
00:45:16,269 --> 00:45:17,739
или ваши траектории будут

1081
00:45:17,739 --> 00:45:19,749
совершенно разными, как и ваша

1082
00:45:19,749 --> 00:45:23,400
оценка их локальных градиентов, так

1083
00:45:24,029 --> 00:45:26,140
что это действительно хорошо, мы

1084
00:45:26,140 --> 00:45:27,909
закончим этой ситуацией, когда нам нужно только

1085
00:45:27,909 --> 00:45:30,339
иметь возможность иметь  аналитическая форма f

1086
00:45:30,339 --> 00:45:32,289
или производная от нашей политики по

1087
00:45:32,289 --> 00:45:34,150
отношению к нашим параметрам, так что нам все еще

1088
00:45:34,150 --> 00:45:35,409
нужно, и мы поговорим об этом

1089
00:45:35,409 --> 00:45:37,119
чуть позже, нам все еще нужно это, мы

1090
00:45:37,119 --> 00:45:41,349
должны оценить это, это о том, как

1091
00:45:41,349 --> 00:45:44,829
мы параметризуем нашу политику, и если мы

1092
00:45:44,829 --> 00:45:46,539
хотим, чтобы это  быть аналитическим, нам нужно

1093
00:45:46,539 --> 00:45:48,249
параметризовать нашу политику таким образом, чтобы

1094
00:45:48,249 --> 00:45:50,229
мы могли вычислить это точно для любого

1095
00:45:50,229 --> 00:45:52,779
состояния в действии, поэтому мы поговорим больше о

1096
00:45:52,779 --> 00:45:55,239
некоторых способах, которыми вы знаете некоторые

1097
00:45:55,239 --> 00:45:56,619
параметры политики, которые делают это

1098
00:45:56,619 --> 00:45:59,409
вычисление аналитическим и приятным в других

1099
00:45:59,409 --> 00:46:00,759
случаях, которые вы могли бы  нужно оценить

1100
00:46:00,759 --> 00:46:03,759
саму эту вещь с помощью грубой силы, или

1101
00:46:03,759 --> 00:46:05,529
вычислений, или конечных разностей, или

1102
00:46:05,529 --> 00:46:07,509
чего-то еще, но если мы выберем конкретную

1103
00:46:07,509 --> 00:46:09,789
форму политики параметризации, тогда эта

1104
00:46:09,789 --> 00:46:14,919
часть будет аналитической, так что другое

1105
00:46:14,919 --> 00:46:18,219
дело, я не нахожу это, но я не нахожу

1106
00:46:18,219 --> 00:46:18,999
нахожу эту дополнительную терминологию

1107
00:46:18,999 --> 00:46:20,559
особенно полезной, но она используется

1108
00:46:20,559 --> 00:46:21,549
повсюду, где я хочу ее представить,

1109
00:46:21,549 --> 00:46:25,059
то есть люди часто называют эту часть

1110
00:46:25,059 --> 00:46:28,479
функцией оценки, просто функцией оценки,

1111
00:46:28,479 --> 00:46:30,279
которая не является конкретной  я

1112
00:46:30,279 --> 00:46:32,140
думаю, что это полезно, но, тем не менее, часто используется,

1113
00:46:32,140 --> 00:46:35,169
называется это это количество, о котором

1114
00:46:35,169 --> 00:46:36,759
мы только что говорили, нужно

1115
00:46:36,759 --> 00:46:40,239
иметь возможность оценить, так что это действительно входит

1116
00:46:40,239 --> 00:46:41,460
в

1117
00:46:41,460 --> 00:46:45,030
эй, хорошо, мы напишем это снова,

1118
00:46:45,030 --> 00:46:50,010
поэтому, когда мы возьмем производную от

1119
00:46:50,010 --> 00:46:52,860
функция ценности, которую мы аппроксимируем,

1120
00:46:52,860 --> 00:46:58,410
получая M выборок, и мы суммируем I,

1121
00:46:58,410 --> 00:47:00,990
равное 1 до M, и мы смотрим на награду

1122
00:47:00,990 --> 00:47:06,930
за эту попытку этой траектории, а затем

1123
00:47:06,930 --> 00:47:15,290
суммируем эти функции оценки за шаг,

1124
00:47:24,140 --> 00:47:27,720
кто-нибудь читал это в конце, да, хорошо,

1125
00:47:27,720 --> 00:47:29,430
да  так что это своего рода наши

1126
00:47:29,430 --> 00:47:31,350
функции оценки, это наши

1127
00:47:31,350 --> 00:47:36,960
функции оценки, которые можно оценить, но для каждой

1128
00:47:36,960 --> 00:47:38,730
отдельной пары действий состояния, которую мы видели, и

1129
00:47:38,730 --> 00:47:40,290
нам не нужно знать модель динамики,

1130
00:47:40,290 --> 00:47:45,690
поэтому теорема градиента политики

1131
00:47:45,690 --> 00:47:48,270
немного обобщает это, как она

1132
00:47:48,270 --> 00:47:49,890
собирается обобщать  это примечание в данном

1133
00:47:49,890 --> 00:47:51,840
случае то, что мы здесь делаем, это то, что мы

1134
00:47:51,840 --> 00:47:54,240
это для эпизодического сеттинга, а это

1135
00:47:54,240 --> 00:47:56,790
для того, когда мы просто берем наши сырые исходные данные или

1136
00:47:56,790 --> 00:47:59,400
функции вознаграждения, поэтому мы смотрим на сумму

1137
00:47:59,400 --> 00:48:02,190
вознаграждений для этой траектории и  затем мы

1138
00:48:02,190 --> 00:48:04,470
взвешиваем это своего рода производной

1139
00:48:04,470 --> 00:48:07,860
относительно параметров нашей политики,

1140
00:48:07,860 --> 00:48:10,560
получается, что мы также можем немного

1141
00:48:10,560 --> 00:48:13,320
обобщить это, и скажем, я

1142
00:48:13,320 --> 00:48:15,650
собираюсь позвонить, так что это функция ценности

1143
00:48:15,650 --> 00:48:17,580
, скажем, что у нас были немного разные

1144
00:48:17,580 --> 00:48:19,350
целевые функции, которые мы  мы говорили ранее

1145
00:48:19,350 --> 00:48:21,090
о том, как мы могли бы иметь подопечный posada Kragh

1146
00:48:21,090 --> 00:48:23,820
или среднее вознаграждение за временной шаг или

1147
00:48:23,820 --> 00:48:26,010
среднее значение, чтобы мы могли либо сделать

1148
00:48:26,010 --> 00:48:28,320
нашу целевую функцию равной нашему

1149
00:48:28,320 --> 00:48:34,320
нормальному значению для эпизодического, либо мы могли

1150
00:48:34,320 --> 00:48:35,700
бы сделать ее равной тому, что я собираюсь назвать

1151
00:48:35,700 --> 00:48:39,150
J ABR, который является средним или словом за временной

1152
00:48:39,150 --> 00:48:43,910
шаг, или

1153
00:48:43,910 --> 00:48:49,849
мы могли бы иметь его как среднее значение,

1154
00:48:49,849 --> 00:48:51,710
скажем, мы всегда продолжаем, и мы

1155
00:48:51,710 --> 00:48:53,240
хотим усреднить распределение

1156
00:48:53,240 --> 00:48:54,829
состояний, с которыми мы сталкиваемся, чтобы мы могли

1157
00:48:54,829 --> 00:48:57,500
подумать об этом сценарии - получается во

1158
00:48:57,500 --> 00:48:59,119
всех  таких случаев, как вы можете сделать

1159
00:48:59,119 --> 00:49:01,069
вывод, аналогичный тому, что мы сделали здесь

1160
00:49:01,069 --> 00:49:03,289
для эпизодического случая, и мы обнаружим

1161
00:49:03,289 --> 00:49:07,160
, что у нас есть производная нашей

1162
00:49:07,160 --> 00:49:09,289
целевой функции, которая теперь может быть своего

1163
00:49:09,289 --> 00:49:10,640
рода любой из этих различных o  Объективные

1164
00:49:10,640 --> 00:49:13,130
функции равны ожидаемому значению

1165
00:49:13,130 --> 00:49:17,359
при том, что текущая политика

1166
00:49:17,359 --> 00:49:20,690
производной по отношению только к тем

1167
00:49:20,690 --> 00:49:33,829
параметрам политики, времена Q и sutnar

1168
00:49:33,829 --> 00:49:35,900
umberto в главе тринадцатой, на которые мы

1169
00:49:35,900 --> 00:49:37,880
также ссылаемся в графике

1170
00:49:37,880 --> 00:49:40,609
, хорошо обсуждаем ряд

1171
00:49:40,609 --> 00:49:44,390
этих различных  проблемы, и поэтому снова

1172
00:49:44,390 --> 00:49:45,289
мы не собираемся слишком много говорить об

1173
00:49:45,289 --> 00:49:46,700
этих немного других целевых

1174
00:49:46,700 --> 00:49:48,170
функциях, но просто знайте, что все это

1175
00:49:48,170 --> 00:49:52,029
можно распространить на продолжающийся случай,

1176
00:49:52,029 --> 00:49:55,190
хорошо, так что до сих пор мы говорили здесь,

1177
00:49:55,190 --> 00:49:57,140
что у нас есть это приближение, где

1178
00:49:57,140 --> 00:49:59,809
что мы делаем, так это просто берем нашу политику,

1179
00:49:59,809 --> 00:50:02,180
запускаем ее пять M раз для каждого из

1180
00:50:02,180 --> 00:50:04,279
этих моментов, мы получаем целую последовательность

1181
00:50:04,279 --> 00:50:06,619
состояний, действий и вознаграждений, а

1182
00:50:06,619 --> 00:50:09,230
затем усредняем, и это непредвзятая

1183
00:50:09,230 --> 00:50:11,930
оценка градиента политики, но она

1184
00:50:11,930 --> 00:50:17,109
очень зашумлена.  так что это будет непредвзято

1185
00:50:17,109 --> 00:50:23,930
и шумно, если вы вспомните о том, что мы видели

1186
00:50:23,930 --> 00:50:25,490
раньше для таких вещей, как методы Монте-Карло,

1187
00:50:25,490 --> 00:50:27,319
это должно выглядеть смутно знакомым

1188
00:50:27,319 --> 00:50:30,079
тот же дух, верно, мы только что

1189
00:50:30,079 --> 00:50:32,180
выполнив нашу политику, мы получим

1190
00:50:32,180 --> 00:50:34,730
некоторую сумму вознаграждений, точно такую же, как то, что мы получ 

1191
00:50:34,730 --> 00:50:38,240
ли в оценках Монте-Карло, но это бу

1192
00:50:38,240 --> 00:50:39,559
ет несмещенная оценка гради 

1193
00:50:39,559 --> 00:50:41,119
нта, так что это будет несмещенная оценк 

1194
00:50:41,119 --> 00:50:45,010
градиента, оценка градиента, но заш

1195
00:50:46,160 --> 00:50:51,690
мленная, так что может сделать это  на самом деле это

1196
00:50:51,690 --> 00:50:54,660
практично, для этого существует множество различных

1197
00:50:54,660 --> 00:50:57,180
техник, но некоторые

1198
00:50:57,180 --> 00:50:58,109
из вещей, о которых мы начнем сегодня говорить,

1199
00:50:58,109 --> 00:50:59,880
это временная структура и

1200
00:50:59,880 --> 00:51:01,589
басовые партии, хорошо,

1201
00:51:01,589 --> 00:51:03,930
так как мы это исправим, мы собираемся

1202
00:51:03,930 --> 00:51:09,869
начать смотреть на то, как вы знаете, исправляет временную

1203
00:51:09,869 --> 00:51:22,710
структуру.  базовые линии, и прежде чем

1204
00:51:22,710 --> 00:51:24,720
мы продолжим это, основываясь на том, что я только что сказал

1205
00:51:24,720 --> 00:51:26,490
в отношении оценок Монте-Карло, и

1206
00:51:26,490 --> 00:51:28,470
какие у вас есть идеи, ребята, о

1207
00:51:28,470 --> 00:51:30,450
том, как мы могли бы уменьшить

1208
00:51:30,450 --> 00:51:33,359
дисперсию этой оценки на основе материала, который мы

1209
00:51:33,359 --> 00:51:36,690
видели до сих пор в классе  например, какова

1210
00:51:36,690 --> 00:51:38,309
альтернатива методу Монте-Карло

1211
00:51:38,309 --> 00:51:47,819
, так что покопайтесь в том, что

1212
00:51:47,819 --> 00:51:49,739
можно использовать начальную загрузку, да, так что мы

1213
00:51:49,739 --> 00:51:51,900
неоднократно видели, что у нас есть этот

1214
00:51:51,900 --> 00:51:53,430
компромисс между смещением и дисперсией, и

1215
00:51:53,430 --> 00:51:55,470
эта начальная загрузка  например,

1216
00:51:55,470 --> 00:51:56,940
методы временной разницы, которые мы видели при изучении сигналов,

1217
00:51:56,940 --> 00:51:58,920
которые вы делаете в DQ n, могут

1218
00:51:58,920 --> 00:52:01,619
быть полезны для уменьшения дисперсии и

1219
00:52:01,619 --> 00:52:04,289
ускорения распространения информации, так

1220
00:52:04,289 --> 00:52:05,670
что да, поэтому мы могли бы делать такие вещи,

1221
00:52:05,670 --> 00:52:08,720
как начальная загрузка, чтобы заменить

1222
00:52:08,720 --> 00:52:14,369
R чем-то другим или  используйте ковариант

1223
00:52:14,369 --> 00:52:16,470
в дополнение к нашему, чтобы попытаться уменьшить

1224
00:52:16,470 --> 00:52:21,450
дисперсию R, так что

1225
00:52:21,450 --> 00:52:24,150
сейчас мы собираемся сделать

1226
00:52:24,150 --> 00:52:25,559
то, что мы сначала сделаем что-то, что не идет до конца,

1227
00:52:25,559 --> 00:52:29,099
но пытается, по крайней мере, использовать

1228
00:52:29,099 --> 00:52:30,989
тот факт, что мы находимся во временном временном

1229
00:52:30,989 --> 00:52:36,359
процессе приложения, хорошо, и для любого из вас,

1230
00:52:36,359 --> 00:52:37,980
кто играл с важной

1231
00:52:37,980 --> 00:52:39,630
выборкой до того, как это тесно связано

1232
00:52:39,630 --> 00:52:42,769
с выборкой важности для каждого решения, и в

1233
00:52:42,769 --> 00:52:45,420
основном вещь, которую мы собираемся

1234
00:52:45,420 --> 00:52:50,089
использовать, это тот факт, что вознаграждения  Я

1235
00:52:50,089 --> 00:52:53,910
могу только сделать временную структуру,

1236
00:52:53,910 --> 00:52:55,739
я сначала напишу это, хорошо, итак,

1237
00:52:55,739 --> 00:52:58,660
мы сказали,

1238
00:52:58,660 --> 00:53:02,059
что река по отношению к тета

1239
00:53:02,059 --> 00:53:06,950
ожидаемого значения тау возврата

1240
00:53:06,950 --> 00:53:09,200
равна ожидаемому значению при

1241
00:53:09,200 --> 00:53:12,259
траектории, которые вы могли бы получить от суммы

1242
00:53:12,259 --> 00:53:16,670
по T, равны от нуля до t минус один из RT,

1243
00:53:16,670 --> 00:53:19,269
поэтому только летние форварды, которые вы получите,

1244
00:53:19,269 --> 00:53:24,229
умноженные на эту сумму по T, равны от нуля до t

1245
00:53:24,229 --> 00:53:26,779
минус один, или если ваша производная по

1246
00:53:26,779 --> 00:53:36,680
отношению к параметрам вашей политики, это то,

1247
00:53:36,680 --> 00:53:38,959
что мы  было раньше, поэтому мы просто суммируем

1248
00:53:38,959 --> 00:53:41,209
все наши вознаграждения, а затем умножаем

1249
00:53:41,209 --> 00:53:43,819
это на сумму всех

1250
00:53:43,819 --> 00:53:46,130
градиентов нашей политики в каждой отдельной

1251
00:53:46,130 --> 00:53:47,479
паре состояний действия, которые мы получили на этой

1252
00:53:47,479 --> 00:53:51,049
траектории, поэтому давайте подумаем о том, чтобы сделать

1253
00:53:51,049 --> 00:53:53,390
это для одного вознаграждения.  вместо того, чтобы

1254
00:53:53,390 --> 00:53:55,309
смотреть на всю сумму вознаграждений, поэтому

1255
00:53:55,309 --> 00:53:56,749
давайте просто посмотрим на то, что мы берем

1256
00:53:56,749 --> 00:53:59,019
производную по отношению к тета

1257
00:53:59,019 --> 00:54:04,160
ожидаемого значения нашего простого T, так что это

1258
00:54:04,160 --> 00:54:05,869
всего лишь один шаг вперед, с которым мы

1259
00:54:05,869 --> 00:54:07,430
можем столкнуться, как вы знаете, на нашей

1260
00:54:07,430 --> 00:54:09,440
траектории, и это происходит  быть

1261
00:54:09,440 --> 00:54:14,059
равным ожидаемому значению суммы RT простых

1262
00:54:14,059 --> 00:54:17,239
чисел по T равно нулю до T простого числа

1263
00:54:17,239 --> 00:54:21,499
этой производной, так что это будет выглядеть

1264
00:54:21,499 --> 00:54:28,130
почти точно так же, как и раньше, за исключением того,

1265
00:54:28,130 --> 00:54:30,049
что здесь единственное ключевое отличие состоит в том, что я

1266
00:54:30,049 --> 00:54:34,549
суммирую только T Pr  Итак, мы только

1267
00:54:34,549 --> 00:54:37,249
подводим итог, вы можете думать об этом

1268
00:54:37,249 --> 00:54:39,469
как о коротком вступительном слове. Я смотрю

1269
00:54:39,469 --> 00:54:41,690
на продукт состояний,

1270
00:54:41,690 --> 00:54:43,699
действий и наград, которых я достиг на

1271
00:54:43,699 --> 00:54:46,940
всем пути, до которого я добрался до нашего

1272
00:54:46,940 --> 00:54:49,789
пика.  так что мне не нужно суммировать

1273
00:54:49,789 --> 00:54:52,609
все будущие, поэтому мы можем взять это

1274
00:54:52,609 --> 00:54:55,819
выражение, и теперь мы можем суммировать по всем

1275
00:54:55,819 --> 00:54:56,690
временным шагам,

1276
00:54:56,690 --> 00:54:58,219
так что это говорит о том, каково ожидаемое вознаграждение

1277
00:54:58,219 --> 00:55:00,319
или производная по отношению к

1278
00:55:00,319 --> 00:55:02,690
вознаграждению за временной шаг T простое число  теперь

1279
00:55:02,690 --> 00:55:05,329
я просто суммирую это, и это будет

1280
00:55:05,329 --> 00:55:07,759
то же самое, что и мое первое выражение, поэтому

1281
00:55:07,759 --> 00:55:10,859
я собираюсь сказать, что V от ADA

1282
00:55:10,859 --> 00:55:12,449
равен производной относительно

1283
00:55:12,449 --> 00:55:16,259
тета IVR, и я собираюсь просуммировать  это

1284
00:55:16,259 --> 00:55:20,339
внутреннее выражение, так что я собираюсь просуммировать

1285
00:55:20,339 --> 00:55:22,949
по T Prime, равно нулю до t минус

1286
00:55:22,949 --> 00:55:26,789
1 RT Prime, а затем вставить это второе

1287
00:55:26,789 --> 00:55:41,819
выражение, так что все, что я сделал, это поместил его

1288
00:55:41,819 --> 00:55:43,559
туда, и я суммировал по T Prime,

1289
00:55:43,559 --> 00:55:45,979
равно нулю все  путь до t минус 1, а

1290
00:55:45,979 --> 00:55:49,039
затем я собираюсь

1291
00:55:49,039 --> 00:55:53,069
изменить порядок этого и этого, сделав

1292
00:55:53,069 --> 00:55:55,410
следующее  Если мы подумаем

1293
00:55:55,410 --> 00:55:58,469
о том, в скольких терминах появляется один из этих

1294
00:55:58,469 --> 00:56:02,999
конкретных логарифмических тета H es T

1295
00:56:02,999 --> 00:56:10,979
, если мы посмотрим на логарифм пи тета a 1

1296
00:56:10,979 --> 00:56:14,579
s 1, то есть, если вы посмотрите, сколько раз

1297
00:56:14,579 --> 00:56:18,539
появляется то, что появляется для раннего

1298
00:56:18,539 --> 00:56:20,339
награды, и это появляется для всех более поздних

1299
00:56:20,339 --> 00:56:22,739
наград, хорошо, это появится

1300
00:56:22,739 --> 00:56:24,719
для нашей 1, это появится для наших 2,

1301
00:56:24,719 --> 00:56:26,579
это будет ясно вплоть до нашей

1302
00:56:26,579 --> 00:56:29,640
команды, правильная команда - 1, потому что мы

1303
00:56:29,640 --> 00:56:32,489
всегда суммируем все до

1304
00:56:32,489 --> 00:56:35,999
этого T  Итак, что мы собираемся сделать

1305
00:56:35,999 --> 00:56:37,799
сейчас, так это взять эти термины и

1306
00:56:37,799 --> 00:56:40,380
просто реорганизовать их так, чтобы

1307
00:56:40,380 --> 00:56:42,089
некоторые из этих терминов появлялись целую

1308
00:56:42,089 --> 00:56:44,579
кучу раз, некоторые из них были последними.

1309
00:56:44,579 --> 00:56:50,219
минус 1 st минус

1310
00:56:50,219 --> 00:56:55,339
1 это появится только после того, как оно будет

1311
00:56:55,339 --> 00:56:57,689
отвечать только за то, чтобы помочь определить самое

1312
00:56:57,689 --> 00:57:00,660
окончательное вознаграждение, поэтому мы можем использовать это понимание,

1313
00:57:00,660 --> 00:57:02,579
чтобы просто немного реорганизовать это

1314
00:57:02,579 --> 00:57:04,890
уравнение следующим образом, так что теперь мы

1315
00:57:04,890 --> 00:57:06,749
скажем, что это равно ожидаемому

1316
00:57:06,749 --> 00:57:12,630
значению суммы  над T равно от 0 до t минус 1, поэтому

1317
00:57:12,630 --> 00:57:14,549
обратите внимание перед  e Я поместил T

1318
00:57:14,549 --> 00:57:17,069
Prime снаружи, а T был внутри,

1319
00:57:17,069 --> 00:57:19,410
и теперь я поставлю

1320
00:57:19,410 --> 00:57:22,829
T снаружи, и я скажу, что у меня

1321
00:57:22,829 --> 00:57:24,120
есть журнал Delta Theta

1322
00:57:24,120 --> 00:57:32,400
PI theta a TST x  сумма по T Prime

1323
00:57:32,400 --> 00:57:35,670
равна T вплоть до t минус один из

1324
00:57:35,670 --> 00:57:39,300
наших T Prime, поэтому все, что я сделал, это

1325
00:57:39,300 --> 00:57:42,450
реорганизовал этого сына да, да, да,

1326
00:57:42,450 --> 00:57:45,150
во второй строке внизу это

1327
00:57:45,150 --> 00:57:46,800
должно быть  производная состоит в том,

1328
00:57:46,800 --> 00:57:48,300
что функция значения по отношению к тета

1329
00:57:48,300 --> 00:57:53,280
и в самом левом углу да ладно, извините

1330
00:57:53,280 --> 00:57:54,450
, вы имеете в виду, что это будет производная

1331
00:57:54,450 --> 00:58:01,830
от этого да ладно, так что мы сделали в

1332
00:58:01,830 --> 00:58:03,600
этом случае, мы реорганизовали

1333
00:58:03,600 --> 00:58:06,060
сына мы'  я только что вспомнил термины

1334
00:58:06,060 --> 00:58:08,520
немного по-другому, но это

1335
00:58:08,520 --> 00:58:10,230
будет полезно,

1336
00:58:10,230 --> 00:58:14,160
так что просто переместите это вверх, а я перенесу

1337
00:58:14,160 --> 00:58:16,400
это вниз,

1338
00:58:17,549 --> 00:58:19,839
так что прямо сейчас мы все еще работаем над

1339
00:58:19,839 --> 00:58:22,839
временной структурой, что это

1340
00:58:22,839 --> 00:58:24,849
позволит нам сделать  хорошо, что второй термин

1341
00:58:24,849 --> 00:58:27,010
там должен выглядеть несколько знакомо, что

1342
00:58:27,010 --> 00:58:29,109
это за высказывание здесь, это то, что говоря, что

1343
00:58:29,109 --> 00:58:31,180
значит их слово, которое мы получаем, начиная со времени

1344
00:58:31,180 --> 00:58:35,440
шаг T до конца, и это

1345
00:58:35,440 --> 00:58:38,260
просто возврат, поэтому мы должны были заранее

1346
00:58:38,260 --> 00:58:41,559
определить, что, когда мы говорим

1347
00:58:41,559 --> 00:58:43,480
о методах Монте-Карло и т. д., что

1348
00:58:43,480 --> 00:58:50,680
мы всегда можем просто посмотреть на RT, это

1349
00:58:50,680 --> 00:58:55,180
просто равно возврату возврата

1350
00:58:55,180 --> 00:58:56,829
для остальных  эпизода, начинающегося во временном

1351
00:58:56,829 --> 00:59:01,329
шаге T на Episode by, так что это должно

1352
00:59:01,329 --> 00:59:02,619
быть очень похоже на то, что мы видели

1353
00:59:02,619 --> 00:59:04,329
в методах Монте-Карло, потому что мы

1354
00:59:04,329 --> 00:59:06,010
всегда могли сказать из этого состояния в действии,

1355
00:59:06,010 --> 00:59:07,960
какое лето вознаграждений мы получили,

1356
00:59:07,960 --> 00:59:09,640
начиная это же действие до

1357
00:59:09,640 --> 00:59:12,430
конец эпизода, так что это означает, что мы можем

1358
00:59:12,430 --> 00:59:15,279
перевыразить производную

1359
00:59:15,279 --> 00:59:18,789
по тета как произведение отдельно один

1360
00:59:18,789 --> 00:59:21,640
на m сумму по всем

1361
00:59:21,640 --> 00:59:24,250
траекториям суммируя по сумме по всем временным

1362
00:59:24,250 --> 00:59:28,210
шагам производную по

1363
00:59:28,210 --> 00:59:32,519
тета нашего фактического  параметр политики,

1364
00:59:33,210 --> 00:59:41,650
умноженный только на возврат, и это

1365
00:59:41,650 --> 00:59:43,630
будет немного более низкая оценка дисперсии,

1366
00:59:43,630 --> 00:59:49,829
чем раньше, поэтому вместо того, чтобы нам приходилось как

1367
00:59:49,829 --> 00:59:52,089
бы отдельно суммировать все наши

1368
00:59:52,089 --> 00:59:54,130
слова, а затем мы умножаем это на

1369
00:59:54,130 --> 00:59:57,400
полную сумму o  Для всех этих

1370
00:59:57,400 --> 00:59:57,970
производных журналов

1371
00:59:57,970 --> 01:00:02,410
нам нужно только взять эту

1372
01:00:02,410 --> 01:00:05,740
сумму журналов для некоторых условий вознаграждения

1373
01:00:05,740 --> 01:00:09,549
по существу, и поэтому мы можем

1374
01:00:09,549 --> 01:00:12,039
уменьшить дисперсию в этом случае, потому что в

1375
01:00:12,039 --> 01:00:13,569
некотором смысле то, что это делает, это

1376
01:00:13,569 --> 01:00:15,339
говорит  например, для каждого отдельного вознаграждения,

1377
01:00:15,339 --> 01:00:18,099
потому что вы можете повторно собрать вознаграждения за

1378
01:00:18,099 --> 01:00:19,480
каждое из этих вознаграждений, которые вы

1379
01:00:19,480 --> 01:00:22,089
должны суммировать, как бы по полной

1380
01:00:22,089 --> 01:00:24,339
траектории с точки зрения

1381
01:00:24,339 --> 01:00:26,259
производной градиента, производной

1382
01:00:26,259 --> 01:00:28,119
параметров политики, и теперь мы говорим, что

1383
01:00:28,119 --> 01:00:29,140
вы не  вам не нужно

1384
01:00:29,140 --> 01:00:30,910
умножать это на все те, которые вам

1385
01:00:30,910 --> 01:00:32,559
нужно только умножить на те, которые имеют

1386
01:00:32,559 --> 01:00:35,230
отношение к этому конкретному вознаграждению, что

1387
01:00:35,230 --> 01:00:36,940
означает, что у вас будет немного

1388
01:00:36,940 --> 01:00:43,869
более низкая оценка дисперсии, поэтому, когда мы это сделаем,

1389
01:00:43,869 --> 01:00:45,819
мы можем получить то, что  известный как

1390
01:00:45,819 --> 01:00:47,829
подкрепление, о котором здесь слышали,

1391
01:00:47,829 --> 01:00:50,230
укрепил ряд людей, а не

1392
01:00:50,230 --> 01:00:51,640
всех, мы применяем этот один из

1393
01:00:51,640 --> 01:00:53,109
наиболее распространенных алгоритмов оценки политики обучения с подкреплением,

1394
01:00:53,109 --> 01:00:55,960
поэтому вы получаете

1395
01:00:55,960 --> 01:01:02,799
усиленный алгоритм, так что как он

1396
01:01:02,799 --> 01:01:05,230
работает  s вы алгоритм, вы

1397
01:01:05,230 --> 01:01:11,859
инициализируете данные NIT случайным образом, вам всегда

1398
01:01:11,859 --> 01:01:13,450
придется сначала решить, как вы

1399
01:01:13,450 --> 01:01:15,609
параметризуете свою политику, чтобы где-то

1400
01:01:15,609 --> 01:01:17,109
вы были готовы бороться, решили, как вы

1401
01:01:17,109 --> 01:01:18,700
параметризуете свою политику, теперь вы

1402
01:01:18,700 --> 01:01:20,079
собираетесь установить значения для этой политики

1403
01:01:20,079 --> 01:01:23,309
случайным образом, а затем для каждого эпизода, так

1404
01:01:23,309 --> 01:01:25,569
что вы собираетесь запустить эпизод с этой

1405
01:01:25,569 --> 01:01:31,510
политикой, и вы соберете целую

1406
01:01:31,510 --> 01:01:38,680
кучу действий и наград, и это будет

1407
01:01:38,680 --> 01:01:41,140
выбрано из вашей текущей политики, поэтому вы

1408
01:01:41,140 --> 01:01:44,940
выбираете свою текущую политику в соответствии с

1409
01:01:44,940 --> 01:01:47,049
семью из вашего текущего  политики вы получаете

1410
01:01:47,049 --> 01:01:50,740
траекторию, а затем для каждого шага на

1411
01:01:50,740 --> 01:01:52,660
этой траектории вы будете обновлять

1412
01:01:52,660 --> 01:01:58,900
параметры своей политики, поэтому для каждого временного шага

1413
01:01:58,900 --> 01:02:01,000
внутри этого эпизода мы будем

1414
01:02:01,000 --> 01:02:03,640
обновлять параметры нашей политики, чтобы они были

1415
01:02:03,640 --> 01:02:06,069
такими же, как и раньше, раз некоторая

1416
01:02:06,069 --> 01:02:12,400
скорость обучения  Буп, я не буду использовать W

1417
01:02:12,400 --> 01:02:20,440
там альфа, умноженная на производную спецификацию

1418
01:02:20,440 --> 01:02:27,130
Theda log pi theta dat GT, где GT

1419
01:02:27,130 --> 01:02:30,309
просто в этом эпизоде, какова сумма

1420
01:02:30,309 --> 01:02:37,670
вознаграждений, начиная с 80-го уровня, так что это

1421
01:02:37,670 --> 01:02:39,349
просто  нормальный возврат, как у нас

1422
01:02:39,349 --> 01:02:42,200
было с методами Монте-Карло, так же,

1423
01:02:42,200 --> 01:02:43,609
как и то, что мы сделали неправильно, оценивая, например,

1424
01:02:43,609 --> 01:02:45,619
функцию линейного значения, и мы

1425
01:02:45,619 --> 01:02:47,690
использовали вознаграждения от состояния и действия,

1426
01:02:47,690 --> 01:02:49,280
мы собираемся сделать то же самое

1427
01:02:49,280 --> 01:02:51,470
здесь, за исключением того, что мы будем  обновляя

1428
01:02:51,470 --> 01:02:54,140
параметры политики, и мы делаем это

1429
01:02:54,140 --> 01:02:55,760
много-много раз, а затем в конце

1430
01:02:55,760 --> 01:03:10,010
мы возвращаем тета-параметры, которые являются

1431
01:03:10,010 --> 01:03:14,839
правильными, поэтому в этом случае вы после того, как сделаете

1432
01:03:14,839 --> 01:03:17,450
все обновления для одного эпизода, вы

1433
01:03:17,450 --> 01:03:20,720
дадите вам эти добавочные обновления, а

1434
01:03:20,720 --> 01:03:22,700
затем в  В конце выполнения всех ваших

1435
01:03:22,700 --> 01:03:25,640
добавочных обновлений вы получаете еще один

1436
01:03:25,640 --> 01:03:27,470
эпизод с вашими новыми параметрами обновления,

1437
01:03:27,470 --> 01:03:35,480
да, так что, поскольку мы делаем

1438
01:03:35,480 --> 01:03:39,440
каждый раз, когда я говорю, что это должна быть

1439
01:03:39,440 --> 01:03:43,369
беспристрастная оценка I, это

1440
01:03:43,369 --> 01:03:44,660
все еще должна быть беспристрастная оценка

1441
01:03:44,660 --> 01:03:49,099
градиента, который он  стохастический, но у нас

1442
01:03:49,099 --> 01:03:51,950
нет понятия состояния и действий таким

1443
01:03:51,950 --> 01:03:54,650
же образом, это будет асимптотически

1444
01:03:54,650 --> 01:03:56,510
непротиворечивым, это хороший вопрос,

1445
01:03:56,510 --> 01:04:01,640
поэтому понятие состояния в действии в

1446
01:04:01,640 --> 01:04:03,380
этом случае отличается, потому что w  У меня есть

1447
01:04:03,380 --> 01:04:05,900
только эти параметры политики, поэтому здесь не

1448
01:04:05,900 --> 01:04:07,460
оценивается ценность государства в

1449
01:04:07,460 --> 01:04:12,290
действии, так что это, безусловно,

1450
01:04:12,290 --> 01:04:15,020
асимптотически непротиворечиво. Я думаю, что это все

1451
01:04:15,020 --> 01:04:19,040
еще просто беспристрастно, если я передумаю,

1452
01:04:19,040 --> 01:04:21,410
что позже я сделаю ставку на Piazza Post, но я

1453
01:04:21,410 --> 01:04:22,849
думаю, что это все еще просто беспристрастно

1454
01:04:22,849 --> 01:04:26,329
оценка градиента, это хороший

1455
01:04:26,329 --> 01:04:30,370
вопрос, хорошо,

1456
01:04:30,370 --> 01:04:35,080
поэтому, когда я вернулся к примечаниям к слайду, я

1457
01:04:35,080 --> 01:04:37,330
думаю, что последнее, чего я просто хотел, хорошо,

1458
01:04:37,330 --> 01:04:40,510
я упомянул автомобильные вещи, мы ответим на один

1459
01:04:40,510 --> 01:04:43,120
важный вопрос: нужно ли

1460
01:04:43,120 --> 01:04:45,940
и как вычислить этот дифференциал

1461
01:04:45,940 --> 01:04:48,430
относительно  к параметрам политики, поэтому я

1462
01:04:48,430 --> 01:04:49,960
думаю, что полезно поговорить о

1463
01:04:49,960 --> 01:04:51,970
том, какие классы политик, которые

1464
01:04:51,970 --> 01:04:54,670
часто люди считают, что у меня есть хорошие

1465
01:04:54,670 --> 01:04:58,120
дифференцируемые формы, поэтому некоторые из

1466
01:04:58,120 --> 01:04:59,830
классов, которые люди считают, это такие вещи, как

1467
01:04:59,830 --> 01:05:01,480
мягкие максимальные гауссианы и нейронные сети,

1468
01:05:01,480 --> 01:05:04,060
которые, вероятно,  наиболее распространенный,

1469
01:05:04,060 --> 01:05:05,650
и что я имею в виду под этим?

1470
01:05:05,650 --> 01:05:06,400


1471
01:05:06,400 --> 01:05:09,280


1472
01:05:09,280 --> 01:05:13,450


1473
01:05:13,450 --> 01:05:19,480
просто есть линейная комбинация

1474
01:05:19,480 --> 01:05:21,790
функций, и мы берем

1475
01:05:21,790 --> 01:05:23,710
их экспоненциальный вес, так что

1476
01:05:23,710 --> 01:05:25,870
мы собираемся сделать, у нас будут некоторые

1477
01:05:25,870 --> 01:05:28,230
функции или очень состояние и пространство действия,

1478
01:05:28,230 --> 01:05:31,510
и мы собираемся умножить их на некоторые

1479
01:05:31,510 --> 01:05:34,600
веса наших параметров.  это наши

1480
01:05:34,600 --> 01:05:39,580
параметры, а затем, чтобы фактически получить

1481
01:05:39,580 --> 01:05:42,430
вероятность совершения действия, поэтому, если мы

1482
01:05:42,430 --> 01:05:44,710
хотим иметь нашу политику, в которой мы говорим,

1483
01:05:44,710 --> 01:05:46,540
какова вероятность действия при

1484
01:05:46,540 --> 01:05:48,910
заданном состоянии, мы возьмем

1485
01:05:48,910 --> 01:05:52,030
экспоненту этих взвешенных функций, поэтому мы имеем e

1486
01:05:52,030 --> 01:05:57,850
к 5 Т тета, деленное на сумму

1487
01:05:57,850 --> 01:06:08,200
всех действий, так что обратите внимание, что

1488
01:06:08,200 --> 01:06:09,910
это разумно делать, когда наше

1489
01:06:09,910 --> 01:06:11,140
пространство действий дискретно.

1490
01:06:11,140 --> 01:06:18,550


1491
01:06:18,550 --> 01:06:20,830


1492
01:06:20,830 --> 01:06:22,870


1493
01:06:22,870 --> 01:06:24,190
нельзя просто разделить эту экспоненту

1494
01:06:24,190 --> 01:06:28,480
на нормализованную сумму по

1495
01:06:28,480 --> 01:06:31,300
экспоненте, и это немедленно дает

1496
01:06:31,300 --> 01:06:34,930
наш параметризованный класс политик, и поэтому

1497
01:06:34,930 --> 01:06:37,450
, если мы хотим получить

1498
01:06:37,450 --> 01:06:39,070
производную от этого относительно

1499
01:06:39,070 --> 01:06:41,380
l  og, это очень хорошо, потому что у нас

1500
01:06:41,380 --> 01:06:43,660
здесь есть экспоненциальная и у нас есть логарифмическая диаграмма,

1501
01:06:43,660 --> 01:06:44,140
мы

1502
01:06:44,140 --> 01:06:47,530
берем лог этого, поэтому мы хотим иметь возможность

1503
01:06:47,530 --> 01:06:49,570
вычислить этот член из такого рода

1504
01:06:49,570 --> 01:06:52,270
параметризованного класса политики, что мы получаем,

1505
01:06:52,270 --> 01:06:54,010
является производной по отношению к тета от

1506
01:06:54,010 --> 01:06:57,510
журнал этого типа политики

1507
01:06:57,510 --> 01:07:03,070
параметризации просто равен нашим функциям, так что это

1508
01:07:03,070 --> 01:07:04,660
любое представление функций, которое мы

1509
01:07:04,660 --> 01:07:08,290
используем, как в случае с роботизированным передвижением,

1510
01:07:08,290 --> 01:07:10,450
это будет похоже на все

1511
01:07:10,450 --> 01:07:13,990
другое, это может быть что-то вроде

1512
01:07:13,990 --> 01:07:16,150
вы знаете углы или суставы или  такие вещи,

1513
01:07:16,150 --> 01:07:18,490
так что это то, что

1514
01:07:18,490 --> 01:07:21,610
использовала характеристика минус ожидаемое значение для

1515
01:07:21,610 --> 01:07:30,250
тета ваших параметров с

1516
01:07:30,250 --> 01:07:33,130
показателем степени с вашим ожидаемым значением по

1517
01:07:33,130 --> 01:07:34,480
всем действиям, которые вы могли бы предпринять в соответствии с

1518
01:07:34,480 --> 01:07:36,220
этой политикой, так что это как бы говорит о том, что

1519
01:07:36,220 --> 01:07:38,200
функции, которые вы наблюдали, по сравнению с

1520
01:07:38,200 --> 01:07:40,330
сортировкой  средней функции,

1521
01:07:40,330 --> 01:07:45,490
усредненной по действию, так что это

1522
01:07:45,490 --> 01:07:49,210
дифференцируемо, и вы можете решить его, тогда

1523
01:07:49,210 --> 01:07:51,520
это дает вам аналитическую форму. Еще одна

1524
01:07:51,520 --> 01:07:52,780
вещь, которая действительно популярна, - это

1525
01:07:52,780 --> 01:07:58,360
Политика Гаусса и почему это может быть

1526
01:07:58,360 --> 01:07:59,770
хорошо, это может быть хорошо, потому что

1527
01:07:59,770 --> 01:08:01,660
часто у нас есть лица с непрерывными действиями,

1528
01:08:01,660 --> 01:08:02,950
поэтому это хорошо, если у нас есть дискретные

1529
01:08:02,950 --> 01:08:04,840
пространства действий часто у нас есть

1530
01:08:04,840 --> 01:08:08,400
пространства с непрерывными действиями, это очень распространено в

1531
01:08:08,400 --> 01:08:14,110
элементах управления и робототехнике. У меня есть несколько

1532
01:08:14,110 --> 01:08:16,720
разных  параметры и их

1533
01:08:16,720 --> 01:08:18,340
непрерывные скалярные значения, которые

1534
01:08:18,340 --> 01:08:21,700
вы хотите установить, и что мы могли бы сказать

1535
01:08:21,700 --> 01:08:25,120
здесь, так это то, что, скажем, мы используем mu of s, может

1536
01:08:25,120 --> 01:08:27,899
быть линейной комбинацией функций состояния,

1537
01:08:27,899 --> 01:08:32,050
умноженных на какой-то параметр, и давайте

1538
01:08:32,050 --> 01:08:34,240
для простоты представим прямо сейчас, что у нас есть

1539
01:08:34,240 --> 01:08:38,920
дисперсия, но она статическая, поэтому мы могли бы

1540
01:08:38,920 --> 01:08:40,420
также рассмотреть случай, когда она остановилась,

1541
01:08:40,420 --> 01:08:41,470
но мы собираемся предположить, что у нас есть некоторый

1542
01:08:41,470 --> 01:08:45,100
член дисперсии, который является фиксированным, это

1543
01:08:45,100 --> 01:08:46,689
не параметр, это не то, что мы будем

1544
01:08:46,689 --> 01:08:47,950
пытаться узнать, мы просто собираемся  мы

1545
01:08:47,950 --> 01:08:49,600
пытаемся изучить тета, которая

1546
01:08:49,600 --> 01:08:51,880
определяет нашу новую функцию, и тогда наша

1547
01:08:51,880 --> 01:08:54,370
политика будет гауссовой, поэтому a

1548
01:08:54,370 --> 01:08:57,399
будет извлечена из гауссовой, используя

1549
01:08:57,399 --> 01:08:58,420
эти

1550
01:08:58,420 --> 01:09:04,899
функции, поэтому мы сравниваем наши

1551
01:09:04,899 --> 01:09:07,899
текущие  ent состояние к среднему, а затем мы

1552
01:09:07,899 --> 01:09:09,460
выбираем действие по отношению к этому,

1553
01:09:09,460 --> 01:09:12,040
и функция оценки в этом случае

1554
01:09:12,040 --> 01:09:16,870
является производной этой функции Гаусса, поэтому оценка

1555
01:09:16,870 --> 01:09:18,520
является производной этой функции Гаусса,

1556
01:09:18,520 --> 01:09:22,060
которая в конечном итоге является минус

1557
01:09:22,060 --> 01:09:29,200
мю раз нашей параметризации

1558
01:09:29,200 --> 01:09:33,480
характеристики состояния, разделенные на квадрат сигмы,

1559
01:09:34,200 --> 01:09:37,390
так что опять же, это можно сделать аналитически,

1560
01:09:37,390 --> 01:09:39,430
а другой действительно распространенный - это глубокие

1561
01:09:39,430 --> 01:09:42,609
нейронные сети, так что

1562
01:09:42,609 --> 01:09:45,340
это очень распространенные формы и ИИ,

1563
01:09:45,340 --> 01:09:49,090
которые люди используют, мы собираемся поговорить в следующий

1564
01:09:49,090 --> 01:09:51,580
раз о другом  Прежде чем мы

1565
01:09:51,580 --> 01:09:54,490
закончим, мы потратим около

1566
01:09:54,490 --> 01:09:56,650
пяти минут на то, чтобы дать некоторую

1567
01:09:56,650 --> 01:09:58,240
обратную связь в начале урока. Нам действительно полезно

1568
01:09:58,240 --> 01:09:59,950
выяснить, что помогает вам узнать, какие

1569
01:09:59,950 --> 01:10:01,210
вещи, по вашему мнению, можно было бы улучшить,

1570
01:10:01,210 --> 01:10:03,280
и что я собираюсь сделать прямо сейчас.  открыто,

1571
01:10:03,280 --> 01:10:07,330
если вы, ребята, могли бы пойти на площадь, было

1572
01:10:07,330 --> 01:10:08,680
бы здорово, если бы вы могли заполнить это,

1573
01:10:08,680 --> 01:10:10,840
все анонимно, и моя цель -

1574
01:10:10,840 --> 01:10:12,250
дать вам отзывы

1575
01:10:12,250 --> 01:10:14,920
об этом в среду, поэтому я просто позвольте мне

1576
01:10:14,920 --> 01:10:19,270
посмотреть, смогу ли я повторно  сделайте это, чтобы он был опубликован,

1577
01:10:19,270 --> 01:10:21,940
хорошо, дайте мне посмотреть, смогу ли я, я закреплю его, чтобы

1578
01:10:21,940 --> 01:10:28,090
его было легче найти, поэтому он должен быть закреплен

1579
01:10:28,090 --> 01:10:29,770
сейчас или на самом верху, да,

1580
01:10:29,770 --> 01:10:31,540
так что опрос обратной связи среднего класса, если вы пойдете

1581
01:10:31,540 --> 01:10:33,910
на площадь, это очень короткий опрос может

1582
01:10:33,910 --> 01:10:35,320
дать  нам информацию обратно, это было бы

1583
01:10:35,320 --> 01:10:37,570
здорово, что мы сделаем в следующий раз, мы

1584
01:10:37,570 --> 01:10:39,100
продолжим говорить о поиске политики,

1585
01:10:39,100 --> 01:10:40,480
мы собираемся поговорить о базовых показателях,

1586
01:10:40,480 --> 01:10:42,130
что является еще одним способом уменьшить дисперсию,

1587
01:10:42,130 --> 01:10:45,100
и это снова очень активная область

1588
01:10:45,100 --> 01:10:46,960
исследований, поэтому  есть тонна работы по

1589
01:10:46,960 --> 01:10:48,430
глубокому обучению с подкреплением и градиентам политики,

1590
01:10:48,430 --> 01:10:50,200
и поэтому мы поговорим о некоторых

1591
01:10:50,200 --> 01:10:51,520
из этих работ, а затем у вас будет

1592
01:10:51,520 --> 01:10:54,730
возможность поиграть с этим

1593
01:10:54,730 --> 01:10:56,110
и получить своего рода практический опыт с

1594
01:10:56,110 --> 01:10:58,300
градиентами политики после  промежуточный семестр, поэтому

1595
01:10:58,300 --> 01:10:59,710
мы опубликуем задание об этом

1596
01:10:59,710 --> 01:11:01,510
в промежуточном выпуске, и это будет

1597
01:11:01,510 --> 01:11:04,140
третье задание

