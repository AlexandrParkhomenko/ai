1
00:00:05,450 --> 00:00:08,189
Итак, домашнее задание - вы, ребята, платите за то,

2
00:00:08,189 --> 00:00:09,809
чтобы начать работать, и у нас есть

3
00:00:09,809 --> 00:00:12,030
сессии на этой неделе, которые хороши, если у

4
00:00:12,030 --> 00:00:13,289
вас нет фона и глубокого

5
00:00:13,289 --> 00:00:16,020
обучения, и вы можете свободно протянуть руку и

6
00:00:16,020 --> 00:00:29,010
быть как это, о, да, вопросы отличные

7
00:00:29,010 --> 00:00:31,920
, да  посетите пост на Piazza, вы

8
00:00:31,920 --> 00:00:33,510
всегда можете разработать свой собственный

9
00:00:33,510 --> 00:00:35,010
проект, который всегда в порядке,

10
00:00:35,010 --> 00:00:36,629
и некоторые из вас пришли поговорить со мной

11
00:00:36,629 --> 00:00:38,670
о тех или поговорить с другими днями

12
00:00:38,670 --> 00:00:40,530
, это дополнительные варианты, поэтому, если

13
00:00:40,530 --> 00:00:42,690
люди заинтересованы в том, чтобы посмотреть либо  проект по

14
00:00:42,690 --> 00:00:44,100
умолчанию, который мы выпустили

15
00:00:44,100 --> 00:00:46,350
вчера, связанный с варфарином бандитов,

16
00:00:46,350 --> 00:00:48,660
или если вы хотите ознакомиться с

17
00:00:48,660 --> 00:00:51,960
некоторыми предложениями от старших

18
00:00:51,960 --> 00:00:53,910
аспирантов или постдоков, это отличные

19
00:00:53,910 --> 00:00:56,100
возможности, особенно я думаю, если

20
00:00:56,100 --> 00:00:57,270
вы никогда не занимались

21
00:00:57,270 --> 00:00:58,950
обучением с подкреплением, прежде чем это часто  Я бы не

22
00:00:58,950 --> 00:01:00,329
ожидал, что через три недели вы

23
00:01:00,329 --> 00:01:01,590
сможете бросить вызов современному

24
00:01:01,590 --> 00:01:03,989
проекту, так что, если вы заинтересованы в том, чтобы

25
00:01:03,989 --> 00:01:05,820
узнать больше о наших исследованиях RL, тогда

26
00:01:05,820 --> 00:01:07,110
это может быть действительно здорово.  возможность

27
00:01:07,110 --> 00:01:09,090
посмотреть на некоторые из этих предложенных проектов, а

28
00:01:09,090 --> 00:01:12,600
затем связаться с людьми. Еще одна вещь,

29
00:01:12,600 --> 00:01:15,179
о которой я просто хотел сделать дружеское

30
00:01:15,179 --> 00:01:17,850
напоминание, это то, что мы явно публикуем

31
00:01:17,850 --> 00:01:20,639
реплики FA для каждого из домашних заданий, и некоторые

32
00:01:20,639 --> 00:01:21,990
из CAS упоминают, что некоторые

33
00:01:21,990 --> 00:01:23,069
из студентов  Приходя в рабочее время

34
00:01:23,069 --> 00:01:24,600
прямо сейчас, возможно, у вас не было возможности

35
00:01:24,600 --> 00:01:27,810
взглянуть на них, поэтому, если вы когда-

36
00:01:27,810 --> 00:01:29,039
либо спрашивали, когда вы идете с

37
00:01:29,039 --> 00:01:30,179
домашним заданием, первое, что нужно сделать, это пойти

38
00:01:30,179 --> 00:01:31,829
на площадь и особенно посмотреть на

39
00:01:31,829 --> 00:01:33,630
эти  булавочные заметки в самом верху, в которых

40
00:01:33,630 --> 00:01:36,450
есть очень распространенные подсказки FA о

41
00:01:36,450 --> 00:01:38,840
задании, поэтому обязательно прочитайте их,

42
00:01:38,840 --> 00:01:41,670
прежде чем идти в рабочее время, а затем,

43
00:01:41,670 --> 00:01:43,020
конечно, не стесняйтесь приходить и в рабочее

44
00:01:43,020 --> 00:01:44,219
время, но это действительно

45
00:01:44,219 --> 00:01:47,189
хороший ресурс для  посмотри на любые другие

46
00:01:47,189 --> 00:01:51,719
вопросы, хорошо, так что просто с точки зрения того,

47
00:01:51,719 --> 00:01:53,700
где мы находимся в курсе прямо сейчас, мы

48
00:01:53,700 --> 00:01:56,130
прошли DQ n в понедельник, мы собираемся

49
00:01:56,130 --> 00:01:59,490
поговорить сегодня немного больше о том, что мы

50
00:01:59,490 --> 00:02:00,719
можем подвести итоги некоторых вещей, которые я должен

51
00:02:00,719 --> 00:02:01,950
был  спешить в конце  Понедельник с

52
00:02:01,950 --> 00:02:04,590
точки зрения обучения DQ и некоторых

53
00:02:04,590 --> 00:02:06,119
недавних расширений, а затем мы собираемся

54
00:02:06,119 --> 00:02:07,739
поговорить с некоторыми об имитационном обучении в

55
00:02:07,739 --> 00:02:09,598
больших пространствах состояний, прежде

56
00:02:09,598 --> 00:02:11,250
чем на следующей неделе мы начнем говорить о методах градиента политики,

57
00:02:11,250 --> 00:02:13,520


58
00:02:15,120 --> 00:02:17,160
поэтому мы начнем с

59
00:02:17,160 --> 00:02:20,400
освежения от  то, что dqn делал dqn,

60
00:02:20,400 --> 00:02:22,980
было этой идеей объединения ключевого

61
00:02:22,980 --> 00:02:25,410
обучения и использования глубоких нейронных сетей в

62
00:02:25,410 --> 00:02:28,050
качестве аппроксиматора функций, и два ключевых

63
00:02:28,050 --> 00:02:30,060
типа алгоритмических изменений по сравнению с

64
00:02:30,060 --> 00:02:33,989
предыдущей работой заключались в том, что они использовали

65
00:02:33,989 --> 00:02:38,879
воспроизведение опыта и фиксированные цели-подсказки, а также

66
00:02:38,879 --> 00:02:40,830
фиксированные цели-подсказки, которые  имелось в виду,

67
00:02:40,830 --> 00:02:44,510
что, когда мы использовали наш ARP RT плюс гамма-

68
00:02:44,510 --> 00:02:53,430
максимум в очереди sta st плюс 1

69
00:02:53,430 --> 00:02:56,340
, веса, которые использовались для этого

70
00:02:56,340 --> 00:02:58,680
представления очереди, были фиксированными на некоторое

71
00:02:58,680 --> 00:03:00,480
время, поэтому, возможно, мы обновили бы их каждые

72
00:03:00,480 --> 00:03:03,569
сто шагов или каждые 50 эпизодов или

73
00:03:03,569 --> 00:03:07,079
некоторый интервал, и поэтому это предварительное попадание

74
00:03:07,079 --> 00:03:09,239
обеспечило более стабильную цель для

75
00:03:09,239 --> 00:03:11,700
обучения с учителем, потому что, поскольку часть обучения с учителем

76
00:03:11,700 --> 00:03:13,410
снова заключается в том, что

77
00:03:13,410 --> 00:03:15,569
у нас была эта комбинация, которую мы

78
00:03:15,569 --> 00:03:17,340
хотим  у нас есть сумма ожидания, и мы хотим минимизировать

79
00:03:17,340 --> 00:03:20,359
эту ошибку по сравнению с нашей текущей оценкой,

80
00:03:20,359 --> 00:03:26,400
минимизируя ошибку TD, поэтому

81
00:03:26,400 --> 00:03:28,829
способ, которым это происходило, заключается в том, что мы как

82
00:03:28,829 --> 00:03:30,329
бы сохраняем переход в буфере памяти воспроизведения,

83
00:03:30,329 --> 00:03:33,630
мы делаем мини-пакеты, мы читаем

84
00:03:33,630 --> 00:03:35,970
образец, пакет  Действие состояния вознаграждает

85
00:03:35,970 --> 00:03:38,250
следующие кортежи состояния, а затем делает эти

86
00:03:38,250 --> 00:03:40,829
резервные копии, где мы как бы обновляем нашу

87
00:03:40,829 --> 00:03:42,919
функцию q и переоснащаем нашу функцию q,

88
00:03:42,919 --> 00:03:45,930
и, как и многие методы функции линейного значения,

89
00:03:45,930 --> 00:03:47,639
которые мы видели до того, как она использует

90
00:03:47,639 --> 00:03:49,859
стохастический градиентный спуск и

91
00:03:49,859 --> 00:03:50,970
действительно крутую вещь о  это то, что

92
00:03:50,970 --> 00:03:52,739
они сделали это в 50 играх, они используют

93
00:03:52,739 --> 00:03:54,569
одну и ту же архитектуру для этих 50 игр и

94
00:03:54,569 --> 00:03:56,370
одни и те же гиперпараметры, и они получили

95
00:03:56,370 --> 00:03:58,739
производительность на уровне человека, поэтому мы

96
00:03:58,739 --> 00:04:00,150
довольно много говорили об этом раньше, а затем

97
00:04:00,150 --> 00:04:02,430
мы как бы кратко говорили о трех

98
00:04:02,430 --> 00:04:04,859
основных типах  расширения к этому в

99
00:04:04,859 --> 00:04:07,019
ближайшие годы, и снова

100
00:04:07,019 --> 00:04:09,450
было много расширений и

101
00:04:09,450 --> 00:04:10,650
много работы по глубокому

102
00:04:10,650 --> 00:04:12,810
обучению с подкреплением прямо сейчас, но три из них

103
00:04:12,810 --> 00:04:15,930
были следующими:  сначала было двойное D

104
00:04:15,930 --> 00:04:19,649
QN, и мы говорили, прежде чем перейти к

105
00:04:19,649 --> 00:04:21,870
аппроксимации функций, говоря о

106
00:04:21,870 --> 00:04:24,290
проблеме со смещением максимизации,

107
00:04:24,290 --> 00:04:26,650
что, когда вы используете одно и то же

108
00:04:26,650 --> 00:04:29,750
представление для выбора действия и

109
00:04:29,750 --> 00:04:31,160
оценки значения этого действия, вы

110
00:04:31,160 --> 00:04:32,780
можете столкнуться с проблемой смещения максимизации.

111
00:04:32,780 --> 00:04:35,210
и способ, которым этого избегают в

112
00:04:35,210 --> 00:04:37,250
двойном dqn, и я хотел вернуться к этому

113
00:04:37,250 --> 00:04:38,150
снова, потому что пара вопросов

114
00:04:38,150 --> 00:04:39,890
после урока, у нас не было много времени, чтобы

115
00:04:39,890 --> 00:04:42,770
обсудить, что происходит, так это то, что у нас есть

116
00:04:42,770 --> 00:04:44,510
текущая сеть очередей, которая

117
00:04:44,510 --> 00:04:45,950
параметризована набором  весов, и

118
00:04:45,950 --> 00:04:48,550
это то, что сейчас используется для выбора действий,

119
00:04:48,550 --> 00:04:51,740
просто для ясности здесь часто мы используем

120
00:04:51,740 --> 00:04:54,170
какой-то жадный метод, поэтому

121
00:04:54,170 --> 00:04:55,610
мы будем использовать текущие веса сети очередей,

122
00:04:55,610 --> 00:04:57,620
чтобы выбрать наилучшее действие, и

123
00:04:57,620 --> 00:04:58,910
мы выберем это с помощью  один минус

124
00:04:58,910 --> 00:05:01,370
вероятность эпсилон, а затем есть более

125
00:05:01,370 --> 00:05:02,900
старая сеть очередей, которая используется для

126
00:05:02,900 --> 00:05:06,170
оценки этих действий, поэтому, если мы посмотрим,

127
00:05:06,170 --> 00:05:07,490
как мы собираемся изменить наши

128
00:05:07,490 --> 00:05:10,190
веса, у нас будет оценка действий,

129
00:05:10,190 --> 00:05:14,930
используя эти o  затем веса w-,

130
00:05:14,930 --> 00:05:20,120
а затем выбор действия с помощью w, поэтому,

131
00:05:20,120 --> 00:05:21,560
когда вы посмотрите на это, это может начать

132
00:05:21,560 --> 00:05:24,650
выглядеть очень похоже на то, что делал dqn

133
00:05:24,650 --> 00:05:26,510
, потому что DQ n сказал, что мы

134
00:05:26,510 --> 00:05:29,420
собираемся использовать фиксированный набор весов для

135
00:05:29,420 --> 00:05:31,430
этих целевых обновлений, так что  DQ n

136
00:05:31,430 --> 00:05:38,660
делал это R плюс гамма Q Я

137
00:05:38,660 --> 00:05:59,600
напишу максимум в эй W минус это ток, поэтому

138
00:05:59,600 --> 00:06:01,430
в обычном DQ n они также

139
00:06:01,430 --> 00:06:07,460
использовали W минус, но здесь, в двойном DQ n, это

140
00:06:07,460 --> 00:06:09,050
может быть немного по-другому  и

141
00:06:09,050 --> 00:06:10,310
причина, по которой это немного отличается от

142
00:06:10,310 --> 00:06:12,200
того, что мы только что видели, заключается в том, что вы можете

143
00:06:12,200 --> 00:06:13,910
постоянно поддерживать два набора весов

144
00:06:13,910 --> 00:06:16,100
и переключаться между ними на

145
00:06:16,100 --> 00:06:19,820
каждом шаге или в каждой партии, поэтому, когда был введен DQ n,

146
00:06:19,820 --> 00:06:21,470
это была скорее идея

147
00:06:21,470 --> 00:06:24,320
вы фиксируете свои веса, скажем, от

148
00:06:24,320 --> 00:06:27,650
временного шага T до временного шага T плюс 100, используйте

149
00:06:27,650 --> 00:06:29,780
те же веса, что и весь период времени

150
00:06:29,780 --> 00:06:32,480
для вашей цели в двойном D Q, и вам

151
00:06:32,480 --> 00:06:33,470
не обязательно делать это, вы

152
00:06:33,470 --> 00:06:35,000
можете переключаться между ними,

153
00:06:35,000 --> 00:06:37,190
которые  это то, что мы видели с двойным

154
00:06:37,190 --> 00:06:40,610
q-обучением, которое, как вы знаете,  на первом

155
00:06:40,610 --> 00:06:43,430
этапе вы можете использовать веса один для действия и

156
00:06:43,430 --> 00:06:45,170
веса два для оценки. На втором этапе

157
00:06:45,170 --> 00:06:46,610
вы можете использовать два веса для оценки веса

158
00:06:46,610 --> 00:06:48,230
один для действия, что означает, что вы можете

159
00:06:48,230 --> 00:06:51,080
распространять информацию быстрее, поэтому

160
00:06:51,080 --> 00:06:52,970
вместо ожидания пятидесяти эпизодов или 100

161
00:06:52,970 --> 00:06:55,850
эпизодов для обновления  веса, которые

162
00:06:55,850 --> 00:06:57,740
вы используете для своей цели, так что, опять же,

163
00:06:57,740 --> 00:07:02,210
это ваша цель, вы не можете

164
00:07:02,210 --> 00:07:03,830
переключаться между ними, что позволяет

165
00:07:03,830 --> 00:07:05,570
вам обновлять обе сети, сильно

166
00:07:05,570 --> 00:07:07,400
усиливая вес обеих стимулирующих сетей,

167
00:07:07,400 --> 00:07:10,880
их сети идентичны, да, в

168
00:07:10,880 --> 00:07:12,350
общем, когда вы  мы оцениваем эти

169
00:07:12,350 --> 00:07:14,030
виды различных подходов к улучшению

170
00:07:14,030 --> 00:07:17,210
этих методов, есть ли хороший

171
00:07:17,210 --> 00:07:18,800
компромисс между скоростью

172
00:07:18,800 --> 00:07:20,090
распространения информации и ее

173
00:07:20,090 --> 00:07:22,400
нестабильностью, поэтому мы можем обнаружить, что

174
00:07:22,400 --> 00:07:24,890
если система, которую мы пытаемся изучить,

175
00:07:24,890 --> 00:07:27,200
сама по себе относительно  хорошее поведение и

176
00:07:27,200 --> 00:07:28,310
стабильность, мы хотим выбрать что-то, что

177
00:07:28,310 --> 00:07:30,560
имеет более быстрое распространение информации, но

178
00:07:30,560 --> 00:07:32,750
если это очень шумно или нестабильно, нам

179
00:07:32,750 --> 00:07:33,860
нужно сделать что-то более

180
00:07:33,860 --> 00:07:36,560
консервативное.  e вы говорите Berkshire,

181
00:07:36,560 --> 00:07:37,730
который вы знаете, обычно существует

182
00:07:37,730 --> 00:07:39,530
компромисс с точки зрения этих методов

183
00:07:39,530 --> 00:07:41,480
между своего рода характеристикой

184
00:07:41,480 --> 00:07:43,160
стабильности системы и тем, насколько

185
00:07:43,160 --> 00:07:45,220
быстро вы можете распространять информацию обратно,

186
00:07:45,220 --> 00:07:47,150
к сожалению, я чувствую, что это не очень

187
00:07:47,150 --> 00:07:48,740
хорошо охарактеризовано, поэтому я чувствую  как будто мы в

188
00:07:48,740 --> 00:07:51,590
большинстве случаев это эвристики, и

189
00:07:51,590 --> 00:07:52,790
люди оценивают их, и они оценивают

190
00:07:52,790 --> 00:07:54,260
их по множеству разных критериев, и

191
00:07:54,260 --> 00:07:55,220
таким образом мы получаем

192
00:07:55,220 --> 00:07:56,930
обобщение, но я не думаю, что

193
00:07:56,930 --> 00:07:57,940
есть хорошая

194
00:07:57,940 --> 00:08:00,830
систематическая характеристика того, как охарактеризовать

195
00:08:00,830 --> 00:08:03,260
стабильность  системы с этими глубокими

196
00:08:03,260 --> 00:08:04,850
нейронными сетями, особенно в

197
00:08:04,850 --> 00:08:07,010
контексте RL, так что здесь тоже есть много отличных

198
00:08:07,010 --> 00:08:08,510
возможностей для теоретического анализа

199
00:08:08,510 --> 00:08:09,860
или просто более формального

200
00:08:09,860 --> 00:08:11,330
понимания прямо сейчас, я думаю, что мы находимся

201
00:08:11,330 --> 00:08:12,530
на уровне того, чтобы сказать либо просто

202
00:08:12,530 --> 00:08:14,030
кажется последовательным  поработать с

203
00:08:14,030 --> 00:08:16,550
кучей игр Atari и, возможно, с mojo co

204
00:08:16,550 --> 00:08:18,260
или нет, и попытаться охарактеризовать и

205
00:08:18,260 --> 00:08:32,690
сказать об успехах, да, да, так что

206
00:08:32,690 --> 00:08:34,039
вопрос  вы знаете, как мы могли бы переключаться

207
00:08:34,039 --> 00:08:36,740
между этими W и W - и как бы

208
00:08:36,740 --> 00:08:38,929
вы знали, почему и как бы вы это сделали,

209
00:08:38,929 --> 00:08:43,580
чтобы в настройке dqn вы могли установить

210
00:08:43,580 --> 00:08:46,460
W - так что в начале W - равно

211
00:08:46,460 --> 00:08:49,880
W на нулевом временном шаге  а затем в DQ, и

212
00:08:49,880 --> 00:08:50,930
вы бы оставили мой W

213
00:08:50,930 --> 00:08:52,700
таким же, может быть, для следующих 50

214
00:08:52,700 --> 00:08:55,220
эпизодов, но вы бы

215
00:08:55,220 --> 00:08:57,410
обновили W, а затем через 50 эпизодов вы бы обновили W

216
00:08:57,410 --> 00:08:59,930
- недостатком того, о чем мы

217
00:08:59,930 --> 00:09:02,930
немного говорили ранее, является  что вы

218
00:09:02,930 --> 00:09:04,790
не используете информацию, которую вы получаете,

219
00:09:04,790 --> 00:09:08,420
чтобы обновить эту оценку, потому что вы

220
00:09:08,420 --> 00:09:10,760
используете этот старый устаревший набор w, поэтому, по

221
00:09:10,760 --> 00:09:12,740
сути, вы просто не используете

222
00:09:12,740 --> 00:09:14,060
информацию, которую вы получили за эти 50

223
00:09:14,060 --> 00:09:16,010
эпизодов, чтобы обновить то, что произойдет  если

224
00:09:16,010 --> 00:09:18,580
вы получите s Prime и предпримете действие a,

225
00:09:18,580 --> 00:09:22,760
то альтернативой будет переключение

226
00:09:22,760 --> 00:09:24,560
между, скажем, вами, вместо того, чтобы

227
00:09:24,560 --> 00:09:26,209
думать, что это w w — что вы можете

228
00:09:26,209 --> 00:09:27,200
думать об этом таким образом, вы можете просто подумать

229
00:09:27,200 --> 00:09:28,670
о поддержании двух разных наборов

230
00:09:28,670 --> 00:09:32,810
весов и представить, что я  со временем скажу, что это

231
00:09:32,810 --> 00:09:34,580
тик, потому что один раз он идет

232
00:09:34,580 --> 00:09:38,390
для tenkasi, так что представьте, что мы просто

233
00:09:38,390 --> 00:09:39,830
выбираем между теми весами,

234
00:09:39,830 --> 00:09:41,570
которые мы используем для выбора действия, и теми

235
00:09:41,570 --> 00:09:43,310
весами, которые мы используем для оценки

236
00:09:43,310 --> 00:09:45,200
действия, поэтому на первом временном шаге вы

237
00:09:45,200 --> 00:09:47,270
можете использовать это для оценки и это

238
00:09:47,270 --> 00:09:49,459
для выбора  действие, а затем вы

239
00:09:49,459 --> 00:09:53,600
можете переключать его вперед и назад, что по

240
00:09:53,600 --> 00:09:55,700
существу означает, что оба набора

241
00:09:55,700 --> 00:09:57,080
весов обновляются очень

242
00:09:57,080 --> 00:09:59,029
часто, поэтому вместо обновления только

243
00:09:59,029 --> 00:10:00,830
одного из них каждые 50 эпизодов

244
00:10:00,830 --> 00:10:02,990
вы продолжаете распространять эту

245
00:10:02,990 --> 00:10:05,810
информацию быстро и

246
00:10:05,810 --> 00:10:07,580
конечно, здесь есть множество вариантов диаграмм о

247
00:10:07,580 --> 00:10:09,380
том, как часто вы обновляете, вы знаете,

248
00:10:09,380 --> 00:10:10,400
когда вы переключаетесь

249
00:10:10,400 --> 00:10:12,290
между ними, и вы можете думать обо

250
00:10:12,290 --> 00:10:13,700
всех них как о гиперпараметрах, которые вы

251
00:10:13,700 --> 00:10:16,580
можете настроить, но это вместо того,

252
00:10:16,580 --> 00:10:18,709
чтобы сохранить эту цель  исправлено

253
00:10:18,709 --> 00:10:21,740
для 50 шагов или вы знаете n шагов,

254
00:10:21,740 --> 00:10:23,660
это все параметры, которые вы можете

255
00:10:23,660 --> 00:10:24,800
переключать между ними, и это то, что

256
00:10:24,800 --> 00:10:36,290
двойное обучение Q сделало оценку еще

257
00:10:36,290 --> 00:10:41,660
одной сетью Q от TT Q  N, за исключением

258
00:10:41,660 --> 00:10:43,850
того факта, что бывает немного чаще,

259
00:10:43,850 --> 00:10:47,029
это почти насколько отличается

260
00:10:47,029 --> 00:10:48,020
от предыдущего года, это почти идентично,

261
00:10:48,020 --> 00:10:49,850
поэтому я думаю, что основное отличие здесь

262
00:10:49,850 --> 00:10:52,850
в том, что вы можете переключать его, пока

263
00:10:52,850 --> 00:10:54,110
вы поддерживаете какой-то набор весов.

264
00:10:54,110 --> 00:10:56,329
для вашей цели это говорит о том, что вы могли бы что-

265
00:10:56,329 --> 00:10:57,890
то вроде набора сейчас, у вас действительно просто

266
00:10:57,890 --> 00:10:59,180
есть одна и та же сеть, два набора

267
00:10:59,180 --> 00:11:00,320
весов, которые вы должны поддерживать в

268
00:11:00,320 --> 00:11:01,060
памяти,

269
00:11:01,060 --> 00:11:02,290
и это говорит о том, что вы можете

270
00:11:02,290 --> 00:11:03,880


271
00:11:03,880 --> 00:11:05,920
часто переключаться туда и обратно с этими правами и помогаете избежать

272
00:11:05,920 --> 00:11:09,280
смещение максимизации в то время это

273
00:11:09,280 --> 00:11:10,990
не всегда работает, это часто помогает,

274
00:11:10,990 --> 00:11:12,330
все еще есть проблема со стабильностью,

275
00:11:12,330 --> 00:11:14,710
которая может быть лучше, и это позволяет избежать

276
00:11:14,710 --> 00:11:18,670
смещения максимизации, мы также говорили о

277
00:11:18,670 --> 00:11:20,800
приоритетном воспроизведении опыта, мы

278
00:11:20,800 --> 00:11:22,780
рассмотрели небольшой табличный пример,

279
00:11:22,780 --> 00:11:24,190
где  мы рассмотрели влияние создания

280
00:11:24,190 --> 00:11:26,290
резервных копий, поэтому, если у нас есть этот

281
00:11:26,290 --> 00:11:29,050
буфер воспроизведения кортежей SAR s-prime,

282
00:11:29,050 --> 00:11:31,030
какой из них мы должны использовать для создания наших

283
00:11:31,030 --> 00:11:32,620
резервных копий и как мы распространяем это

284
00:11:32,620 --> 00:11:36,040
в  формирование обратно и в этом

285
00:11:36,040 --> 00:11:37,810
алгоритме они сказали, что в этой

286
00:11:37,810 --> 00:11:40,150
статье они говорили о том факте, что если

287
00:11:40,150 --> 00:11:41,590
вы можете сделать это оптимально, в некоторых случаях

288
00:11:41,590 --> 00:11:43,030
вы можете получить экспоненциальное ускорение

289
00:11:43,030 --> 00:11:45,610
и сходимость, но это сложно сделать,

290
00:11:45,610 --> 00:11:47,800
это требует больших вычислительных ресурсов, так что

291
00:11:47,800 --> 00:11:49,480
Они предложили здесь расставить приоритеты

292
00:11:49,480 --> 00:11:52,600
на основе размера своего рода

293
00:11:52,600 --> 00:11:54,580
ошибки DQ n разницы

294
00:11:54,580 --> 00:11:59,380
между текущей оценкой этого и вашей

295
00:11:59,380 --> 00:12:00,700
целевой оценкой, на которую вы смотрите

296
00:12:00,700 --> 00:12:03,010
, и поэтому мы говорили о том, как вы могли бы

297
00:12:03,010 --> 00:12:05,440
использовать это в качестве  приоритет, и это может быть

298
00:12:05,440 --> 00:12:07,870
стохастический приоритет, чтобы попытаться выбрать

299
00:12:07,870 --> 00:12:10,210
элементы, и мы также говорили о том факте,

300
00:12:10,210 --> 00:12:12,130
что если вы установите альфа, равную 0, это

301
00:12:12,130 --> 00:12:14,260
становится однородным, и сказали, что нет

302
00:12:14,260 --> 00:12:15,970
особого приоритета над вашими

303
00:12:15,970 --> 00:12:19,060
кортежами, а затем то, что у нас

304
00:12:19,060 --> 00:12:20,800
почти не было  пришло время поговорить о дуэлях,

305
00:12:20,800 --> 00:12:24,150
так что дуэли были лучшей статьей, с которой я столкнулся

306
00:12:24,150 --> 00:12:28,270
в 2016 году, я имею в виду, что я вижу его, позвольте

307
00:12:28,270 --> 00:12:30,400
мне немного освежить в памяти это,

308
00:12:30,400 --> 00:12:31,510
потому что мы прошли через это очень-очень

309
00:12:31,510 --> 00:12:34,720
быстро  Интуиция здесь такова,

310
00:12:34,720 --> 00:12:35,950
что функции, которые вам могут понадобиться для

311
00:12:35,950 --> 00:12:38,170
записи значения состояния, могут

312
00:12:38,170 --> 00:12:40,000
отличаться от тех, которые необходимы для указания

313
00:12:40,000 --> 00:12:42,190
относительной выгоды различных действий в

314
00:12:42,190 --> 00:12:44,560
этом состоянии, и вы хотите

315
00:12:44,560 --> 00:12:46,870
понять относительную выгоду действий, чтобы

316
00:12:46,870 --> 00:12:48,730
чтобы решить, какой должна быть ваша политика,

317
00:12:48,730 --> 00:12:53,590
поэтому, глядя на такие вещи, как счет в игре

318
00:12:53,590 --> 00:12:55,260
, очевидно, что это очень важно, значение

319
00:12:55,260 --> 00:12:57,430
, которое может быть, вы можете захотеть, чтобы другие

320
00:12:57,430 --> 00:12:59,020
функции пытались решить, какие действия

321
00:12:59,020 --> 00:13:01,240
делать прямо сейчас в игре, и

322
00:13:01,240 --> 00:13:03,370
поэтому появилась функция преимущества  который был

323
00:13:03,370 --> 00:13:05,320
разработан Бэрдом давным-давно, и

324
00:13:05,320 --> 00:13:06,880
это те же медведи, у которых был этот

325
00:13:06,880 --> 00:13:09,190
контрпример, чтобы показать, почему

326
00:13:09,190 --> 00:13:12,420
аппроксимация функции ценности может быть плохой, поэтому

327
00:13:12,420 --> 00:13:14,550
работа Бэрда, прежде чем она сказала, хорошо, смотрите,

328
00:13:14,550 --> 00:13:17,399
вы можете разложить, вы думаете о своей функции q,

329
00:13:17,399 --> 00:13:19,829
которая представляет

330
00:13:19,829 --> 00:13:21,540
значение политики 30 и состояние и выполнение

331
00:13:21,540 --> 00:13:24,480
определенного действия по сравнению со значением

332
00:13:24,480 --> 00:13:26,820
только этого состояния, так что это своего рода

333
00:13:26,820 --> 00:13:33,660
неявно q PI s PI, так что

334
00:13:33,660 --> 00:13:35,190
в чем разница  Разница между выполнением

335
00:13:35,190 --> 00:13:36,630
этого конкретного действия и просто

336
00:13:36,630 --> 00:13:38,070
следованием вашей политике из текущего

337
00:13:38,070 --> 00:13:40,430
состояния, и он назвал это преимуществом,

338
00:13:40,430 --> 00:13:43,350
было преимуществом этого действия для

339
00:13:43,350 --> 00:13:51,120
этого состояния, поэтому в дуэли DQ n

340
00:13:51,120 --> 00:13:54,120
вместо одной сети, которая просто предсказывает функции Q,

341
00:13:54,120 --> 00:13:56,430
они используют архитектуру, которая

342
00:13:56,430 --> 00:13:58,350
разделяется на прогнозирование значений и

343
00:13:58,350 --> 00:14:00,420
прогнозирование этих функций Advantage, а

344
00:14:00,420 --> 00:14:02,490
затем добавляет их обратно вместе с

345
00:14:02,490 --> 00:14:03,959
идеей, что вы можете получить различные

346
00:14:03,959 --> 00:14:08,399
функции здесь и здесь, поэтому вы должны

347
00:14:08,399 --> 00:14:10,200
немного отделиться,

348
00:14:10,200 --> 00:14:11,339
чтобы убедиться, что вы фиксируете

349
00:14:11,339 --> 00:14:12,899
функции, которые имеют отношение к

350
00:14:12,899 --> 00:14:14,250
захвату существенных вещей, на которые вы хотите обратить внимание

351
00:14:14,250 --> 00:14:20,070
для Q теперь одна вещь, которую я

352
00:14:20,070 --> 00:14:23,670
очень кратко упомянул в прошлый раз, это то,

353
00:14:23,670 --> 00:14:25,500
что функция преимущества идентифицируема, и

354
00:14:25,500 --> 00:14:27,660
что я имею в виду под этим в этом случае я

355
00:14:27,660 --> 00:14:30,300
имею в виду, что если вы  есть функция q, которую

356
00:14:30,300 --> 00:14:31,940
в конечном итоге мы собираемся использовать,

357
00:14:31,940 --> 00:14:35,010
можем ли мы разложить ее на уникальные a PI

358
00:14:35,010 --> 00:14:38,220
и B PI, поэтому здесь, в конечном счете, нам нужен q,

359
00:14:38,220 --> 00:14:40,860
и вопрос в том, i  f мы тогда находимся в

360
00:14:40,860 --> 00:14:42,510
архитектуре, разлагая это на

361
00:14:42,510 --> 00:14:44,820
значение, и преимущество заключается в том, что есть уникальный

362
00:14:44,820 --> 00:14:48,329
способ сделать это, но его нет

363
00:14:48,329 --> 00:14:56,130
, если вы добавите константу как к

364
00:14:56,130 --> 00:15:01,980
Q, так и к B, тогда вы можете получить ту же

365
00:15:01,980 --> 00:15:04,050
функцию преимущества  так что нет единицы, которую

366
00:15:04,050 --> 00:15:06,570
вы всегда можете сдвинуть, сдвиньте свои

367
00:15:06,570 --> 00:15:08,399
слова на константу, и это не

368
00:15:08,399 --> 00:15:10,380
изменит вашу политику, изменит вашу

369
00:15:10,380 --> 00:15:14,070
функцию ценности I, поэтому есть много

370
00:15:14,070 --> 00:15:15,300
разных способов декомпозировать вашу

371
00:15:15,300 --> 00:15:17,790
функцию преимущества и ваши ценности, это

372
00:15:17,790 --> 00:15:21,779
не уникальная

373
00:15:21,779 --> 00:15:24,209
декомпозиция  то, как они определили это, состоит в том, чтобы сказать, что

374
00:15:24,209 --> 00:15:26,370
давайте заставим преимущество для состояния

375
00:15:26,370 --> 00:15:29,819
и действия быть равным нулю, если a - это предпринятое действие,

376
00:15:29,819 --> 00:15:35,970
поэтому здесь они сравнивают его с

377
00:15:35,970 --> 00:15:37,740
предпринятым действием, которое вы используете,

378
00:15:37,740 --> 00:15:42,180
скажем, жадный подход, и это действительно

379
00:15:42,180 --> 00:15:45,240
просто способ ко всему этому, мы можем думать

380
00:15:45,240 --> 00:15:47,100
об этом в некотором смысле, это аналогия обучения с учителем,

381
00:15:47,100 --> 00:15:49,050
и поэтому мы хотим

382
00:15:49,050 --> 00:15:50,699
иметь основную цель, и мы хотим иметь

383
00:15:50,699 --> 00:15:52,379
возможность изучить эти функции преимущества

384
00:15:52,379 --> 00:15:54,660
и эти функции ценности, если у нас есть

385
00:15:54,660 --> 00:15:56,550
много-много данных о них, так что

386
00:15:56,550 --> 00:15:58,230
это своего рода выбор конкретной фиксированной

387
00:15:58,230 --> 00:16:00,120
точки для определения функции преимущества,

388
00:16:00,120 --> 00:16:02,069
а затем они также сказали, что

389
00:16:02,069 --> 00:16:03,959
эмпирически вы можете просто использовать среднее

390
00:16:03,959 --> 00:16:05,910
значение, чтобы вы могли просто усреднить здесь

391
00:16:05,910 --> 00:16:08,670
функции преимущества, это умный круг  здесь

392
00:16:08,670 --> 00:16:12,240
подход палки и то, что они снова находят,

393
00:16:12,240 --> 00:16:14,519
поэтому мы как бы наслаивали

394
00:16:14,519 --> 00:16:15,930
эти дополнительные методы, которые мы начали

395
00:16:15,930 --> 00:16:18,170
с DQ n, затем мы подумали о добавлении

396
00:16:18,170 --> 00:16:20,730
двойного обучения Q к DQ n, а затем мы

397
00:16:20,730 --> 00:16:22,050
подумали о добавлении воспроизведения с приоритетом,

398
00:16:22,050 --> 00:16:24,389
а затем это дуэль и что  они

399
00:16:24,389 --> 00:16:26,550
считают, что делать Лиама по сравнению с двойным DQM

400
00:16:26,550 --> 00:16:28,649
с приоритетным повтором в большинстве случаев намного лучше,

401
00:16:28,649 --> 00:16:31,529
теперь дайте мне посмотреть, смогу ли я

402
00:16:31,529 --> 00:16:36,029
найти да Монтесумы, так что для Монтесумы

403
00:16:36,029 --> 00:16:38,069
этот новый метод в основном ничем не лучше, поскольку

404
00:16:38,069 --> 00:16:39,509
ни один из этих методов действительно не

405
00:16:39,509 --> 00:16:42,509
справляется с трудным исследованием  проблемы, но

406
00:16:42,509 --> 00:16:43,920
они используют лучшие способы

407
00:16:43,920 --> 00:16:45,600
распространения информации, чем Сеть, и

408
00:16:45,600 --> 00:16:47,040
пытаются изменить способ, которым мы

409
00:16:47,040 --> 00:16:48,959
обучаем сеть.

410
00:16:48,959 --> 00:16:54,930
сначала имя, пожалуйста, ладно, небольшое

411
00:16:54,930 --> 00:16:58,470
письмо, могут ли

412
00:16:58,470 --> 00:17:01,370
меня услышать люди сзади, или это просто зависит хорошо,

413
00:17:01,370 --> 00:17:03,600
хорошо, так что это были три

414
00:17:03,600 --> 00:17:05,159
метода, которые в конечном итоге имели большое

415
00:17:05,159 --> 00:17:06,869
значение, мы очень кратко поговорили о

416
00:17:06,869 --> 00:17:09,630
практических советах, я не буду идти  на них слишком

417
00:17:09,630 --> 00:17:11,189
много, главное, что мы

418
00:17:11,189 --> 00:17:12,959
пытаемся активно поощрять вас наращивать

419
00:17:12,959 --> 00:17:14,669
свое представление о остроте, прежде чем

420
00:17:14,669 --> 00:17:17,189
вы попробуете Atari, вы можете попробовать различные

421
00:17:17,189 --> 00:17:20,898
формы потерь, скорость обучения

422
00:17:20,898 --> 00:17:23,069
важна, но в этом случае наша

423
00:17:23,069 --> 00:17:24,390
задача, которую мы будем  использование оптимизатора атома,

424
00:17:24,390 --> 00:17:25,829
что означает, что вам не нужно

425
00:17:25,829 --> 00:17:28,020
слишком беспокоиться об этом,

426
00:17:28,020 --> 00:17:30,330
есть проблема, связанная с попытками

427
00:17:30,330 --> 00:17:31,860
различных схем исследования, это

428
00:17:31,860 --> 00:17:32,670
то, о чем мы поговорим

429
00:17:32,670 --> 00:17:35,160
позже в этом классе, поэтому сейчас

430
00:17:35,160 --> 00:17:36,600
мы все еще думаем  о простом

431
00:17:36,600 --> 00:17:40,860
Egret приближается к хорошей статье, которая вышла

432
00:17:40,860 --> 00:17:45,030
, я думаю, она была начата 2018 была

433
00:17:45,030 --> 00:17:46,920
радугой, которая была бумагой, которая в

434
00:17:46,920 --> 00:17:48,390
основном просто пыталась объединить целую

435
00:17:48,390 --> 00:17:50,790
кучу этих последних методов, чтобы увидеть,

436
00:17:50,790 --> 00:17:52,320
насколько велики  улучшения вы

437
00:17:52,320 --> 00:17:55,470
получаете сейчас снова обратите внимание в этом случае, и

438
00:17:55,470 --> 00:17:56,700
мы вернемся к этому всего через пару

439
00:17:56,700 --> 00:17:59,010
слайдов, это много данных,

440
00:17:59,010 --> 00:18:00,750
в мире много опыта 200

441
00:18:00,750 --> 00:18:03,330
миллионов кадров, которые испытывают, но они

442
00:18:03,330 --> 00:18:04,679
разработали алгоритм под названием  Rainbow,

443
00:18:04,679 --> 00:18:06,510
который сочетает в себе многое из того, что мы

444
00:18:06,510 --> 00:18:08,370
только что говорили о двойном DQ n,

445
00:18:08,370 --> 00:18:11,520
приоритетном и dooli, а также некоторых

446
00:18:11,520 --> 00:18:15,300
других недавних достижениях, Noisy,

447
00:18:15,300 --> 00:18:17,010
также пытается проводить некоторые другие формы

448
00:18:17,010 --> 00:18:19,170
исследования, поэтому они нашли это

449
00:18:19,170 --> 00:18:21,000
, добавив эти  одобрение s-- вместе,

450
00:18:21,000 --> 00:18:22,350
тогда вы могли бы получить значительное

451
00:18:22,350 --> 00:18:25,470
улучшение. Я думаю, что это полезное

452
00:18:25,470 --> 00:18:26,790
понимание, потому что часто неясно, являются

453
00:18:26,790 --> 00:18:28,350
ли эти различные улучшения

454
00:18:28,350 --> 00:18:32,400
аддитивными или они просто что-то вроде вашего

455
00:18:32,400 --> 00:18:33,900
уха, они делают то же самое.  вещь,

456
00:18:33,900 --> 00:18:35,280
но, может быть, немного по-другому,

457
00:18:35,280 --> 00:18:36,870
и поэтому приятно видеть, что в некоторых из

458
00:18:36,870 --> 00:18:38,190
этих случаев эти разные

459
00:18:38,190 --> 00:18:39,660
идеи дополняются с точки зрения

460
00:18:39,660 --> 00:18:41,370
результирующей игры производительности,

461
00:18:41,370 --> 00:18:43,980
это не то, что это все еще

462
00:18:43,980 --> 00:18:49,770
v  очень большой объем данных, хорошо, так что

463
00:18:49,770 --> 00:18:51,330
просто подведем итоги, которые завершаются тем, где

464
00:18:51,330 --> 00:18:53,370
мы находимся, с свободными моделями глубоких нейронных

465
00:18:53,370 --> 00:18:55,920
сетей для RL прямо сейчас, они являются очень

466
00:18:55,920 --> 00:18:57,650
выразительным аппроксиматором функций z',

467
00:18:57,650 --> 00:18:59,820
я должен быть в состоянии понять, как вы

468
00:18:59,820 --> 00:19:01,260
представляете функцию q и  вы можете

469
00:19:01,260 --> 00:19:03,600
просто посмотреть на методы на основе Карло или методы в стиле TD,

470
00:19:03,600 --> 00:19:06,870
и на этом этапе

471
00:19:06,870 --> 00:19:08,160
хорошо убедиться, что вы понимаете,

472
00:19:08,160 --> 00:19:09,750
как вы будете делать это с помощью табличных

473
00:19:09,750 --> 00:19:11,250
методов с методами линейной функции значения

474
00:19:11,250 --> 00:19:13,890
и с глубокими нейронными сетями, поэтому

475
00:19:13,890 --> 00:19:15,960
алгоритмически это выглядит очень

476
00:19:15,960 --> 00:19:17,940
похоже на все из них, но тогда вам

477
00:19:17,940 --> 00:19:19,650
в некоторых случаях вам нужно выполнить этот шаг,

478
00:19:19,650 --> 00:19:21,630
чтобы выполнить аппроксимацию функции, в других

479
00:19:21,630 --> 00:19:24,059
случаях вы этого не сделаете, и тогда было бы

480
00:19:24,059 --> 00:19:25,200
хорошо просто убедиться, что вы можете как бы

481
00:19:25,200 --> 00:19:27,420
перечислить несколько расширений, которые помогают помимо

482
00:19:27,420 --> 00:19:32,610
dqn  и почему они работают хорошо, так что теперь давайте

483
00:19:32,610 --> 00:19:36,330
вернемся к нашему высокоуровневому представлению

484
00:19:36,330 --> 00:19:37,650
о том, что мы хотим от

485
00:19:37,650 --> 00:19:39,179
алгоритма обучения

486
00:19:39,179 --> 00:19:41,520
с

487
00:19:41,520 --> 00:19:43,830
подкреплением.  ализацию проводят исследование, они

488
00:19:43,830 --> 00:19:45,419
делают все это статистически и

489
00:19:45,419 --> 00:19:47,340
вычислительно эффективно, и мы

490
00:19:47,340 --> 00:19:48,510
просто потратили довольно много времени

491
00:19:48,510 --> 00:19:50,340
на рассмотрение обобщения, а также

492
00:19:50,340 --> 00:19:53,010
оптимизации, мы не очень много говорили

493
00:19:53,010 --> 00:19:58,230
об эффективности, поэтому одна из

494
00:19:58,230 --> 00:20:00,330
проблем заключается в том, что  если вы хотите

495
00:20:00,330 --> 00:20:02,370
формально определить эффективность, например, с точки

496
00:20:02,370 --> 00:20:04,440
зрения того, сколько данных и агенту Ли нужно

497
00:20:04,440 --> 00:20:06,660
изучить, чтобы принять правильное решение, есть

498
00:20:06,660 --> 00:20:09,450
результаты твердости, которые известны, поэтому

499
00:20:09,450 --> 00:20:11,970
наша лаборатория разработала некоторые нижние границы, которые

500
00:20:11,970 --> 00:20:14,580
другие люди должны, я думаю, у нас теперь есть

501
00:20:14,580 --> 00:20:16,410
в основном узкие верхняя и нижняя границы

502
00:20:16,410 --> 00:20:20,040
для табличного случая MDB, которые указывают на

503
00:20:20,040 --> 00:20:21,720
то, что есть некоторые действительно патологические

504
00:20:21,720 --> 00:20:23,190
горохи MD, для которых нам просто нужно

505
00:20:23,190 --> 00:20:25,530
много данных за день, которые, как вы знаете,

506
00:20:25,530 --> 00:20:26,970
не очень хорошо масштабируются, когда вы начинаете

507
00:20:26,970 --> 00:20:28,860
подниматься к действительно  огромные домены, поэтому некоторые из

508
00:20:28,860 --> 00:20:30,360
этих задач действительно трудно решить

509
00:20:30,360 --> 00:20:31,800
формально, вам просто нужно много

510
00:20:31,800 --> 00:20:33,480
исследований, мы можем сделать что-то намного

511
00:20:33,480 --> 00:20:35,220
лучше, чем жадный, но мы поговорим

512
00:20:35,220 --> 00:20:36,960
об этом позже, но e  даже когда мы делаем

513
00:20:36,960 --> 00:20:38,370
эти вещи намного лучше, чем жадные,

514
00:20:38,370 --> 00:20:39,960
мы можем доказать, что нам все еще очень трудно

515
00:20:39,960 --> 00:20:41,309
учиться, нам все еще может понадобиться

516
00:20:41,309 --> 00:20:44,460
много данных, поэтому альтернативой будет сказать,

517
00:20:44,460 --> 00:20:45,990
что в мире есть много других надзоров,

518
00:20:45,990 --> 00:20:47,429
которые мы могли бы иметь  чтобы

519
00:20:47,429 --> 00:20:50,340
попытаться узнать, как что-то делать, и как

520
00:20:50,340 --> 00:20:52,020
мы можем использовать эту дополнительную информацию

521
00:20:52,020 --> 00:20:53,790
, чтобы ускорить

522
00:20:53,790 --> 00:20:55,470
обучение с подкреплением, поэтому мы собираемся сделать, это поговорить

523
00:20:55,470 --> 00:20:57,870
сегодня об имитационном обучении, а

524
00:20:57,870 --> 00:20:58,980
затем мы начнем говорить о

525
00:20:58,980 --> 00:21:00,570
поиске политики и  методы градиента политики на

526
00:21:00,570 --> 00:21:03,450
следующей неделе, и те, о которых вы также можете

527
00:21:03,450 --> 00:21:04,919
думать как о другом способе

528
00:21:04,919 --> 00:21:08,010
наложения структуры, потому что в

529
00:21:08,010 --> 00:21:09,360
методах градиента политики вы всегда должны

530
00:21:09,360 --> 00:21:12,419
определять свой класс политики, который иногда

531
00:21:12,419 --> 00:21:14,460
может быть действительно богатым классом политики, и поэтому,

532
00:21:14,460 --> 00:21:15,750
возможно, это не слишком много.

533
00:21:15,750 --> 00:21:17,820
ограничение, но в других случаях вы

534
00:21:17,820 --> 00:21:20,070
кодируете знание предметной области классом, который, по

535
00:21:20,070 --> 00:21:25,890
вашему мнению, вы представляете, и, в

536
00:21:25,890 --> 00:21:26,940
частности, мы будем думать

537
00:21:26,940 --> 00:21:28,380
об имитационном обучении и больших состояниях.

538
00:21:28,380 --> 00:21:30,090
шагов, что является именно тем местом, где

539
00:21:30,090 --> 00:21:31,530
вы могли бы надеяться получить

540
00:21:31,530 --> 00:21:36,510
дополнительную помощь или наблюдение, поэтому, если мы

541
00:21:36,510 --> 00:21:37,800
подумаем о чем-то вроде «Мести Монтесумы»,

542
00:21:37,800 --> 00:21:38,880


543
00:21:38,880 --> 00:21:42,270
есть неплохая работа по рассмотрению

544
00:21:42,270 --> 00:21:44,429
того, как далеко продвинулся DQ и в этом

545
00:21:44,429 --> 00:21:46,679
случае, поэтому месть Монтесумы тем, кто

546
00:21:46,679 --> 00:21:48,140
вы, кто не играл, это своего рода

547
00:21:48,140 --> 00:21:52,050
игра с очень длинным горизонтом, в которой

548
00:21:52,050 --> 00:21:53,070
вы как бы пытаетесь узнать

549
00:21:53,070 --> 00:21:54,990
отрицание в этом мире, и вам нравится

550
00:21:54,990 --> 00:21:57,809
подбирать ключи и принимать решения, и это

551
00:21:57,809 --> 00:21:59,700
включает в себя множество разных комнат, так что вы

552
00:21:59,700 --> 00:22:02,100
можете видеть  вот контур

553
00:22:02,100 --> 00:22:03,480
всех белых квадратов - это в основном

554
00:22:03,480 --> 00:22:07,470
комнаты, а с левой стороны своего

555
00:22:07,470 --> 00:22:09,330
рода dqn, который был обучен на 50 миллионов

556
00:22:09,330 --> 00:22:12,570
кадров, проходит только через первые две

557
00:22:12,570 --> 00:22:14,610
комнаты, как будто он просто очень плохой,

558
00:22:14,610 --> 00:22:17,389
он не очень много делает  прогресс,

559
00:22:17,389 --> 00:22:19,710
тогда как с правой стороны мы видим

560
00:22:19,710 --> 00:22:21,419
что-то, что явно пытается

561
00:22:21,419 --> 00:22:23,880
исследовать, и оно использует некоторые из

562
00:22:23,880 --> 00:22:25,320
методов, о которых мы

563
00:22:25,320 --> 00:22:27,419
поговорим позже, но обратите внимание, что оно все

564
00:22:27,419 --> 00:22:29,330
еще не проходит весь путь.  gh игра,

565
00:22:29,330 --> 00:22:31,740
и поэтому я думаю, что это иллюстрирует

566
00:22:31,740 --> 00:22:32,730
тот факт, что некоторые из этих игр

567
00:22:32,730 --> 00:22:33,929
действительно сложны,

568
00:22:33,929 --> 00:22:35,070
был достигнут некоторый действительно хороший

569
00:22:35,070 --> 00:22:39,059
дополнительный прогресс, так как и я,

570
00:22:39,059 --> 00:22:40,889
и лаборатория Перси Ланге теперь можем в

571
00:22:40,889 --> 00:22:42,840
основном продавать игры Монтесумы, а

572
00:22:42,840 --> 00:22:44,070
также были некоторые действительно хорошие  работа из

573
00:22:44,070 --> 00:22:45,690
лаборатории искусственного интеллекта Uber над решением проблемы

574
00:22:45,690 --> 00:22:47,759
Монтесумы, но многие

575
00:22:47,759 --> 00:22:49,679
люди изначально получили поддержку в

576
00:22:49,679 --> 00:22:51,000
этом, начав использовать

577
00:22:51,000 --> 00:22:56,129
демонстрации имитации обучения, поэтому, в частности,

578
00:22:56,129 --> 00:22:57,809
если мы подумаем о случаях, когда RL может

579
00:22:57,809 --> 00:23:00,059
работать хорошо, RL работает, вы довольно хорошо знаете,

580
00:23:00,059 --> 00:23:02,730
когда это  легко или, конечно, мы видели

581
00:23:02,730 --> 00:23:04,320
большой успех до сих пор, когда данные дешевы,

582
00:23:04,320 --> 00:23:06,840
а распараллеливание легко, и может

583
00:23:06,840 --> 00:23:08,549
быть намного сложнее использовать методы, о которых

584
00:23:08,549 --> 00:23:10,169
мы говорили до сих пор, когда данные

585
00:23:10,169 --> 00:23:13,139
дороги и когда, возможно, отказ недопустим.

586
00:23:13,139 --> 00:23:16,019
поэтому, если вы попытаетесь использовать

587
00:23:16,019 --> 00:23:18,000
методы, которые мы только что описали, чтобы научиться

588
00:23:18,000 --> 00:23:20,399
летать, как вертолет с дистанционным управлением,

589
00:23:20,399 --> 00:23:22,529
обычно требуется много вертолетов,

590
00:23:22,529 --> 00:23:24,240
и, поскольку они очень горячие, я позволю этому.

591
00:23:24,240 --> 00:23:26,009
Это очень дорого, и поэтому есть много

592
00:23:26,009 --> 00:23:28,559
случаев, когда этот тип производительности

593
00:23:28,559 --> 00:23:33,330
просто не будет практичным, поэтому одно

594
00:23:33,330 --> 00:23:34,679
из преимуществ заключается в том, что если вы можете дать

595
00:23:34,679 --> 00:23:36,450
агенту много вознаграждений, вы можете

596
00:23:36,450 --> 00:23:39,059
довольно быстро формировать поведение, поэтому одна из

597
00:23:39,059 --> 00:23:40,740
проблем в методе Монтесумы  месть заключается в

598
00:23:40,740 --> 00:23:42,600
том, что награда очень скудна, вы знаете, что

599
00:23:42,600 --> 00:23:43,679
агент должен попробовать много разных

600
00:23:43,679 --> 00:23:45,269
вещей, прежде чем он получит какой-либо сигнал о

601
00:23:45,269 --> 00:23:47,730
том, правильно ли он поступает, и

602
00:23:47,730 --> 00:23:49,620
откуда берутся эти награды.

603
00:23:49,620 --> 00:23:50,820


604
00:23:50,820 --> 00:23:53,519
теперь давайте

605
00:23:53,519 --> 00:23:55,139
подумаем об этом, просто даже о

606
00:23:55,139 --> 00:23:58,049
проблеме определения вознаграждений, поэтому, если

607
00:23:58,049 --> 00:23:59,309
вы вручную спроектируете их, это может быть

608
00:23:59,309 --> 00:24:02,340
довольно хрупким в зависимости от задачи, и

609
00:24:02,340 --> 00:24:05,250
альтернатива - просто продемонстрировать,

610
00:24:05,250 --> 00:24:06,929
поэтому, если вам пришлось записать функцию вознаграждения

611
00:24:06,929 --> 00:24:08,490
за вождение автомобиля  это довольно

612
00:24:08,490 --> 00:24:10,230
сложно, как будто вы не хотите ехать по

613
00:24:10,230 --> 00:24:13,080
дорогам здесь, бьете по пострадавшим людям, которых вы не

614
00:24:13,080 --> 00:24:14,910
хотите сбивать с дороги, вы хотите

615
00:24:14,910 --> 00:24:16,650
добраться до места назначения, и поэтому это очень

616
00:24:16,650 --> 00:24:18,000
комплиментарно.  функцию вознаграждения, которую нужно

617
00:24:18,000 --> 00:24:20,040
записать, но для большинства из нас довольно

618
00:24:20,040 --> 00:24:21,929
легко просто доехать до пункта назначения и показать

619
00:24:21,929 --> 00:24:24,559
пример, возможно, оптимального поведения,

620
00:24:24,559 --> 00:24:27,000
так что в этом и заключается идея

621
00:24:27,000 --> 00:24:30,570
обучения на демонстрациях,

622
00:24:30,570 --> 00:24:32,460
над этим было проделано много и много работы, но

623
00:24:32,460 --> 00:24:34,230
с тех пор  люди начали думать об

624
00:24:34,230 --> 00:24:35,790
обучении на демонстрациях или

625
00:24:35,790 --> 00:24:37,799
имитации обучения. Я бы сказал,

626
00:24:37,799 --> 00:24:39,750
что это, вероятно, было начато примерно

627
00:24:39,750 --> 00:24:42,630
20 лет назад, примерно в 1999-2000 годах.

628
00:24:42,630 --> 00:24:44,160


629
00:24:44,160 --> 00:24:47,070


630
00:24:47,070 --> 00:24:48,030


631
00:24:48,030 --> 00:24:49,860
об этом для

632
00:24:49,860 --> 00:24:51,840
таких вещей, как вождение по шоссе, или

633
00:24:51,840 --> 00:24:53,910
навигация, или навигация по парковке,

634
00:24:53,910 --> 00:24:54,780
есть много таких случаев,

635
00:24:54,780 --> 00:24:56,760
особенно при вождении прямо сейчас, но

636
00:24:56,760 --> 00:24:58,559
когда люди думали, и

637
00:24:58,559 --> 00:25:01,500
роботы думали о том, как вы делаете

638
00:25:01,500 --> 00:25:03,480
демонстрации, например, как взять

639
00:25:03,480 --> 00:25:05,100
чашку или  такие вещи, чтобы попытаться

640
00:25:05,100 --> 00:25:08,790
научить роботов выполнять эти задачи, есть

641
00:25:08,790 --> 00:25:10,200
также несколько действительно интересных вопросов -

642
00:25:10,200 --> 00:25:12,809
о таких, как вы  знаете, как вы делаете такие вещи,

643
00:25:12,809 --> 00:25:14,549
как планирование пути или цель в отпечатках, и

644
00:25:14,549 --> 00:25:16,320
снова такие случаи, когда

645
00:25:16,320 --> 00:25:17,910
довольно сложно записать функцию вознаграждения

646
00:25:17,910 --> 00:25:19,650
напрямую или она может быть хрупкой,

647
00:25:19,650 --> 00:25:21,720
и проблема с хрупкими

648
00:25:21,720 --> 00:25:23,640
функциями вознаграждения заключается в том, что ваш агент будет

649
00:25:23,640 --> 00:25:25,410
оптимизировать для этого  и это может быть не то

650
00:25:25,410 --> 00:25:29,429
поведение, которое вы хотели, поэтому

651
00:25:29,429 --> 00:25:30,990
установка обучения на демонстрациях,

652
00:25:30,990 --> 00:25:32,730
и сегодня я собираюсь быть несколько

653
00:25:32,730 --> 00:25:34,049
неформальным относительно того, называю ли я вещи

654
00:25:34,049 --> 00:25:36,000
обучением на демонстрациях, хм, есть

655
00:25:36,000 --> 00:25:39,660
также обратное RL, есть также имитация

656
00:25:39,660 --> 00:25:42,080
обучения

657
00:25:46,320 --> 00:25:48,150
и есть своего рода  различия, но

658
00:25:48,150 --> 00:25:49,380
многие из этих вещей в некоторой степени

659
00:25:49,380 --> 00:25:51,090
взаимозаменяемы, большая часть этого

660
00:25:51,090 --> 00:25:52,920
касается идеи сказать, что у вас есть некоторые

661
00:25:52,920 --> 00:25:54,300
демонстрационные данные, а затем вы

662
00:25:54,300 --> 00:25:56,490
собираетесь использовать их, чтобы помочь либо

663
00:25:56,490 --> 00:25:58,650
загрузиться, либо полностью изучить новую

664
00:25:58,650 --> 00:26:00,930
политику, так что идея  что вы могли бы

665
00:26:00,930 --> 00:26:02,730
найти эксперта, и, может быть, он идеальный

666
00:26:02,730 --> 00:26:04,080
эксперт или, может быть, он довольно хороший

667
00:26:04,080 --> 00:26:05,850
эксперт, чтобы показать некоторые демонстрационные

668
00:26:05,850 --> 00:26:09,540
траектории действий  в штатах,

669
00:26:09,540 --> 00:26:13,470
и во многих случаях людям будет

670
00:26:13,470 --> 00:26:14,970
проще это сделать, но полезно

671
00:26:14,970 --> 00:26:17,010
подумать о том, что проще указать

672
00:26:17,010 --> 00:26:19,590
тем или иным и какие ситуации являются

673
00:26:19,590 --> 00:26:23,730
общими для каждого, так что проблема

674
00:26:23,730 --> 00:26:25,230
поставлена проблема настроена  что у нас

675
00:26:25,230 --> 00:26:27,470
есть это пространство состояний и пространство действия

676
00:26:27,470 --> 00:26:29,850
некоторая модель перехода, которая обычно

677
00:26:29,850 --> 00:26:32,910
неизвестна, нет функции вознаграждения, а вместо этого

678
00:26:32,910 --> 00:26:34,890
своего рода набор демонстраций учителей

679
00:26:34,890 --> 00:26:37,350
из какой-то конкретной мы предполагаем на данный момент

680
00:26:37,350 --> 00:26:41,460
оптимальную политику и клонирование поведения,

681
00:26:41,460 --> 00:26:43,770
мы собираемся сказать, как мы это делаем напрямую  изучите

682
00:26:43,770 --> 00:26:46,080
политику учителей, так как мы можем сопоставить

683
00:26:46,080 --> 00:26:47,610
попытку с тем, как мы можем получить своего рода

684
00:26:47,610 --> 00:26:51,660
приближение звезды пи непосредственно из

685
00:26:51,660 --> 00:26:54,000
этой демонстрации, обратная RL

686
00:26:54,000 --> 00:26:55,320
обычно говорит о том, как мы можем

687
00:26:55,320 --> 00:26:57,840
восстановить функцию вознаграждения, когда у нас есть

688
00:26:57,840 --> 00:26:59,220
функция вознаграждения, тогда мы можем ее использовать

689
00:26:59,220 --> 00:27:04,020
компьютерная политика, и что часто этот последний

690
00:27:04,020 --> 00:27:05,730
шаг часто сочетается с

691
00:27:05,730 --> 00:27:07,530
обучением emprender ship, так что мы оба

692
00:27:07,530 --> 00:27:08,910
пытались получить эту функцию вознаграждения, а

693
00:27:08,910 --> 00:27:10,200
затем фактически создали хорошее остроумие политики.

694
00:27:10,200 --> 00:27:12,960
h, что в некоторых случаях вам может понадобиться только

695
00:27:12,960 --> 00:27:15,780
функция вознаграждения. Могу ли я

696
00:27:15,780 --> 00:27:17,070
подумать о случае, когда вас может

697
00:27:17,070 --> 00:27:18,990
заинтересовать только функция вознаграждения,

698
00:27:18,990 --> 00:27:20,280
может быть, вы не хотите восстанавливать

699
00:27:20,280 --> 00:27:22,100
политику, но вам просто интересно,

700
00:27:22,100 --> 00:27:24,330
что такое функция вознаграждения?  это другой

701
00:27:24,330 --> 00:27:41,520
агент, да, это, я думаю, отличный

702
00:27:41,520 --> 00:27:44,940
пример, так что во многих науках, которые вы знаете о

703
00:27:44,940 --> 00:27:46,200
биологии excetera, вы часто хотите

704
00:27:46,200 --> 00:27:47,850
понять поведение организмов,

705
00:27:47,850 --> 00:27:49,410
животных или подобных вещей, и поэтому, если

706
00:27:49,410 --> 00:27:51,390
вы можете просто посмотреть на их поведение, вы

707
00:27:51,390 --> 00:27:53,010
могли бы  сказать Обезьяны Транк или

708
00:27:53,010 --> 00:27:55,320
что-то в этом роде, а затем использовать это для обратного решения,

709
00:27:55,320 --> 00:27:57,960
например, какова их функция вознаграждения,

710
00:27:57,960 --> 00:27:59,600
каковы цели или предпочтения,

711
00:27:59,600 --> 00:28:01,520
я думаю, что есть ряд случаев, когда

712
00:28:01,520 --> 00:28:03,020
это полезно, это может быть в будущем,

713
00:28:03,020 --> 00:28:04,550
вы знаете, может быть, есть какая-то оптимизация,

714
00:28:04,550 --> 00:28:06,170
которая '  произойдет, но, как правило,

715
00:28:06,170 --> 00:28:07,520
это просто понимание

716
00:28:07,520 --> 00:28:08,750
того, какова структура цели или какова

717
00:28:08,750 --> 00:28:10,910
структура предпочтений организма

718
00:28:10,910 --> 00:28:12,710
или индивидуума.

719
00:28:12,710 --> 00:28:13,940
t

720
00:28:13,940 --> 00:28:15,650
понять, какой выбор люди делают с

721
00:28:15,650 --> 00:28:17,500
точки зрения того, что вы знаете, как

722
00:28:17,500 --> 00:28:20,150
добираться до работы, или с точки зрения покупательских

723
00:28:20,150 --> 00:28:21,860
предпочтений, или чего-то подобного, возможно,

724
00:28:21,860 --> 00:28:23,480
позже вы захотите оптимизировать для этого, но

725
00:28:23,480 --> 00:28:26,240
также вам просто любопытно, как

726
00:28:26,240 --> 00:28:28,550
поведение людей раскрывает своего рода

727
00:28:28,550 --> 00:28:30,620
основная структура вознаграждения,

728
00:28:30,620 --> 00:28:34,090
лежащая в основе модели предпочтений, да,

729
00:28:34,420 --> 00:28:37,430
демонстрации, установленные как верхняя граница, я

730
00:28:37,430 --> 00:28:40,310
думаю, были ли случаи,

731
00:28:40,310 --> 00:28:43,670
когда да, подобный агент учится

732
00:28:43,670 --> 00:28:45,560
работать лучше, чем дополнительный, как

733
00:28:45,560 --> 00:28:49,100
мы находим новую позу, да, поскольку хороший

734
00:28:49,100 --> 00:28:51,830
вопрос о подобном - это  поведение

735
00:28:51,830 --> 00:28:53,420
экспертов — верхняя граница или их случай — это также то,

736
00:28:53,420 --> 00:28:55,460
где агент может выйти за рамки этого.

737
00:28:55,460 --> 00:28:56,600


738
00:28:56,600 --> 00:28:57,800


739
00:28:57,800 --> 00:28:59,750


740
00:28:59,750 --> 00:29:03,050
скажем, как

741
00:29:03,050 --> 00:29:07,400
инверсный RL плюс RL, где, например, вы

742
00:29:07,400 --> 00:29:09,920
можете использовать это, чтобы быстро привязать систему,

743
00:29:09,920 --> 00:29:11,720
а затем ваш агент продолжит

744
00:29:11,720 --> 00:29:13,580
учиться вдобавок к этому, есть также

745
00:29:13,580 --> 00:29:15,320
неплохая работа от Peter Be  als, где

746
00:29:15,320 --> 00:29:17,810
они рассмотрели предположение, что

747
00:29:17,810 --> 00:29:19,400
эксперт предоставляет шумную

748
00:29:19,400 --> 00:29:21,590
демонстрацию оптимального пути, и сказали, что цель состоит в том,

749
00:29:21,590 --> 00:29:23,150
чтобы изучить оптимальный путь, а не его шумную

750
00:29:23,150 --> 00:29:25,280
демонстрацию, так что часто вы хотите

751
00:29:25,280 --> 00:29:27,080
выйти за рамки эксперта,

752
00:29:27,080 --> 00:29:28,940
для этого есть ограничения.  и мы поговорим об

753
00:29:28,940 --> 00:29:30,440
этом через секунду, на самом деле,

754
00:29:30,440 --> 00:29:31,730
какие у вас

755
00:29:31,730 --> 00:29:33,680
могут быть ограничения, если вы не сможете продолжать

756
00:29:33,680 --> 00:29:37,030
собирать данные в новой среде,

757
00:29:38,500 --> 00:29:40,430
хорошо, так что давайте начнем с поведенческого

758
00:29:40,430 --> 00:29:41,540
клонирования, которое, вероятно, является самым простым

759
00:29:41,540 --> 00:29:45,110
один, потому что, по сути, в поведенческом

760
00:29:45,110 --> 00:29:46,310
клонировании мы просто собираемся рассматривать это

761
00:29:46,310 --> 00:29:47,480
как стандартную проблему обучения с учителем,

762
00:29:47,480 --> 00:29:49,820
поэтому мы собираемся исправить класс политики,

763
00:29:49,820 --> 00:29:51,650
что означает своего рода способ

764
00:29:51,650 --> 00:29:54,380
представить наше отображение из состояний в

765
00:29:54,380 --> 00:29:56,690
действия, и это может быть глубокая нейронная

766
00:29:56,690 --> 00:29:58,670
сеть, это может быть дерево решений, это

767
00:29:58,670 --> 00:30:00,650
может быть много разных вещей, а

768
00:30:00,650 --> 00:30:01,700
затем мы просто собираемся оценить

769
00:30:01,700 --> 00:30:04,970
политику на примерах обучения, поэтому

770
00:30:04,970 --> 00:30:06,320
мы просто скажем, что видели это время.

771
00:30:06,320 --> 00:30:08,060
мы видели состояние в действии от нашего

772
00:30:08,060 --> 00:30:09,920
эксперта, и это просто наш ввод-вывод

773
00:30:09,920 --> 00:30:11,480
для нашей модели контролируемого обучения, и

774
00:30:11,480 --> 00:30:12,920
мы просто собираемся изучить сопоставление

775
00:30:12,920 --> 00:30:16,190
состояний с действиями и на раннем этапе, так что это

776
00:30:16,190 --> 00:30:18,650
существует уже довольно долгое

777
00:30:18,650 --> 00:30:20,840
время и  Я должен был сказать больше, как 30

778
00:30:20,840 --> 00:30:23,990
лет, было несколько хороших примеров того

779
00:30:23,990 --> 00:30:26,860
, как это сделать, поэтому Альбан был очень ранней

780
00:30:26,860 --> 00:30:28,010
статьей.

781
00:30:28,010 --> 00:30:30,220
Я настаиваю на том, чтобы он думал о

782
00:30:30,220 --> 00:30:33,110
вождении по дороге, видел нейронную

783
00:30:33,110 --> 00:30:35,450
сеть и был обучен, по крайней мере

784
00:30:35,450 --> 00:30:37,340
частично, с использованием поведенческого клонирования или

785
00:30:37,340 --> 00:30:38,740
контролируемого обучения.  хорошо имитировать

786
00:30:38,740 --> 00:30:44,420
траектории, так что давайте подумаем,

787
00:30:44,420 --> 00:30:48,080
почему это может пойти не так, и - сначала

788
00:30:48,080 --> 00:30:49,280
давайте подумаем о том, что происходит в

789
00:30:49,280 --> 00:30:51,440
обучении с учителем, поэтому в

790
00:30:51,440 --> 00:30:54,380
обучении с учителем мы будем предполагать iid пары Si

791
00:30:54,380 --> 00:30:55,940
и будем игнорировать временную

792
00:30:55,940 --> 00:30:58,850
структуру, поэтому мы собираемся  если мы просто

793
00:30:58,850 --> 00:31:00,080
дроиды контролируемое обучение в целом, мы

794
00:31:00,080 --> 00:31:01,190
просто представляем, что у нас есть эти пары состояний и

795
00:31:01,190 --> 00:31:03,350
действий, а затем, возможно, мы изучаем

796
00:31:03,350 --> 00:31:07,100
какой-то классификатор или да, скажем,

797
00:31:07,100 --> 00:31:09,140
классификатор для классификации чего  t действие, которое, как вы

798
00:31:09,140 --> 00:31:11,540
знаете, мы должны выполнить, и оно может иметь какие-

799
00:31:11,540 --> 00:31:13,310
то ошибки, оно может иметь ошибки,

800
00:31:13,310 --> 00:31:15,440
которые вы будете знать с вероятностью эпсилон,

801
00:31:15,440 --> 00:31:18,200
и поэтому, если бы мы думали о том, чтобы сделать

802
00:31:18,200 --> 00:31:20,120
это в течение t временных шагов,

803
00:31:20,120 --> 00:31:21,950
то мы могли бы сообщить вам что-то вроде

804
00:31:21,950 --> 00:31:23,570
ожидаемое общее количество ошибок в

805
00:31:23,570 --> 00:31:28,070
эпсилон раз t, поэтому давайте еще раз

806
00:31:28,070 --> 00:31:29,750
задумаемся о том, что пойдет не так, когда

807
00:31:29,750 --> 00:31:30,740
мы делаем это в

808
00:31:30,740 --> 00:31:33,920
обучении с учителем или в контексте RL, поэтому

809
00:31:33,920 --> 00:31:36,050
под контекстом RL я имею в виду тот факт,

810
00:31:36,050 --> 00:31:38,120
что решения  то, что мы делаем, повлияло

811
00:31:38,120 --> 00:31:39,950
на следующее состояние, так что давайте просто возьмем

812
00:31:39,950 --> 00:31:41,570
одну минуту, может быть, поговорим с вами, вашим

813
00:31:41,570 --> 00:31:42,890
соседом, и скажите, например, что, по вашему мнению,

814
00:31:42,890 --> 00:31:44,540
может быть проблемой с поведенческим

815
00:31:44,540 --> 00:31:47,030
цитированием в такого рода сценариях, и

816
00:31:47,030 --> 00:31:49,790
если это просто подумать,

817
00:31:49,790 --> 00:31:51,050
то, может быть,  подумайте о том, как бы вы

818
00:31:51,050 --> 00:31:53,840
справились с этим, и что вы могли бы сделать в этом

819
00:31:53,840 --> 00:31:55,430
случае, если возникнут проблемы, которые возникают,

820
00:31:55,430 --> 00:31:57,320
когда мы пытаемся применить стандартное

821
00:31:57,320 --> 00:32:00,980
обучение с учителем к этому случаю, когда оно действительно

822
00:32:00,980 --> 00:32:04,630
лежит в основе MDP

823
00:32:29,830 --> 00:32:32,970
[музыка],

824
00:33:00,630 --> 00:33:03,490
хорошо  поэтому, прежде всего, давайте просто

825
00:33:03,490 --> 00:33:04,900
сделаем предположение, я спрошу вас, ребята

826
00:33:04,900 --> 00:33:07,300
, думаете ли вы, что общее количество ожидаемых

827
00:33:07,300 --> 00:33:09,850
ошибок, если мы делаем это в том месте,

828
00:33:09,850 --> 00:33:11,740
где лежащий в основе мир представляет собой MDP,

829
00:33:11,740 --> 00:33:13,570
будет больше или меньше

830
00:33:13,570 --> 00:33:15,250
числа  ошибки, которые мы ожидаем в соответствии

831
00:33:15,250 --> 00:33:18,430
с подходом к обучению с учителем, так что кто

832
00:33:18,430 --> 00:33:19,660
думает, что у нас будет больше

833
00:33:19,660 --> 00:33:23,470
ожидаемых общих ошибок, хорошо, кто думает, что

834
00:33:23,470 --> 00:33:28,390
у нас будет меньше, сколько людей

835
00:33:28,390 --> 00:33:32,200
должно быть сбито с толку, хорошо, так как насчет того

836
00:33:32,200 --> 00:33:33,430
, что так много, кто думает, ответы  что у нас

837
00:33:33,430 --> 00:33:35,020
будет лучше, может быть, кто-то, кто

838
00:33:35,020 --> 00:33:36,790
думает, что это так, мог бы сказать, почему

839
00:33:36,790 --> 00:33:38,770
они думают, что у нас может быть больше ошибок,

840
00:33:38,770 --> 00:33:40,870
если реальный мир - это MDP, и мы пытались

841
00:33:40,870 --> 00:33:42,100
использовать технику обучения с

842
00:33:42,100 --> 00:33:46,360
учителем, да, неточная идея, что-то

843
00:33:46,360 --> 00:33:50,080
вроде  как человек, вы планируете более

844
00:33:50,080 --> 00:33:52,270
долгосрочный горизонт или, как будто она делает

845
00:33:52,270 --> 00:33:54,220
один шаг, и вы знаете, как это

846
00:33:54,220 --> 00:33:55,420
действие затем даст вам другую

847
00:33:55,420 --> 00:33:59,740
последовательность, но поскольку мы просто начинаем

848
00:33:59,740 --> 00:34:01,030
принимать состояние в действии, а затем, например,

849
00:34:01,030 --> 00:34:03,160
предсказывать прямо оттуда w  Мы не можем

850
00:34:03,160 --> 00:34:05,050
планировать эту долгосрочную последовательность, поэтому она

851
00:34:05,050 --> 00:34:07,620
будет усугублять наши ошибки по мере продвижения

852
00:34:07,620 --> 00:34:10,510
, это правильно, поскольку это правильно, мы будем

853
00:34:10,510 --> 00:34:12,630
усугублять эти ошибки, и один из

854
00:34:12,630 --> 00:34:14,530
сложных аспектов этого заключается в том, что

855
00:34:14,530 --> 00:34:16,899
ошибки могут усугубляться во многом, и это

856
00:34:16,899 --> 00:34:19,179
потому что распределение состояний, которые

857
00:34:19,179 --> 00:34:21,149
вы получаете, может зависеть

858
00:34:21,149 --> 00:34:23,309
от действий, которые вы предпринимаете, поэтому, если вы

859
00:34:23,309 --> 00:34:24,809
думаете об этом как о случае навигации,

860
00:34:24,809 --> 00:34:26,609
например, если бы я должен был выйти через

861
00:34:26,609 --> 00:34:29,998
правую дверь, и я смотрел и выходил из

862
00:34:29,998 --> 00:34:31,349
этой двери, и я видел, что  он пошел направо,

863
00:34:31,349 --> 00:34:33,480
а я и я, вы знаете, пытались выучить

864
00:34:33,480 --> 00:34:35,518
классификатор контролируемого обучения для того, что

865
00:34:35,518 --> 00:34:36,268
я должен делать здесь,

866
00:34:36,268 --> 00:34:38,039
мой супервизор-ученик был немного

867
00:34:38,039 --> 00:34:39,899
сломан, и поэтому вместо того, чтобы идти прямо

868
00:34:39,899 --> 00:34:40,589
здесь,

869
00:34:40,589 --> 00:34:43,259
я на самом деле пошел налево, хорошо, теперь я нахожусь в

870
00:34:43,259 --> 00:34:44,668
части  комнату, в которую я никогда не ходил,

871
00:34:44,668 --> 00:34:46,109
потому что он шел туда, чтобы подойти

872
00:34:46,109 --> 00:34:47,879
к двери, и поэтому сейчас я понятия не имею,

873
00:34:47,879 --> 00:34:51,899
что здесь делать, как сейчас, я нахожусь в

874
00:34:51,899 --> 00:34:54,089
государственном распределении, это то, чего

875
00:34:54,089 --> 00:34:55,168
я не видел раньше,

876
00:34:55,168 --> 00:34:56,609
это очень  скорее всего я совершу

877
00:34:56,609 --> 00:34:58,140
ошибку  На самом деле моя вероятность ошибки

878
00:34:58,140 --> 00:35:00,450
сейчас может не совпадать с моей вероятностью ошибки

879
00:35:00,450 --> 00:35:03,269
здесь, она предполагает тот факт,

880
00:35:03,269 --> 00:35:04,680
что данные, которые вы получите в будущем, имеют

881
00:35:04,680 --> 00:35:06,599
то же распределение, что и данные, которые вы

882
00:35:06,599 --> 00:35:08,700
получили в прошлом, наши гарантии контролируемого обучения

883
00:35:08,700 --> 00:35:10,650
обычно говорят, когда мы имеем

884
00:35:10,650 --> 00:35:14,940
им, что я, если ваши данные поступают

885
00:35:14,940 --> 00:35:17,309
из распределения iid, то в

886
00:35:17,309 --> 00:35:18,990
будущем это то, что будет ошибкой вашего теста,

887
00:35:18,990 --> 00:35:20,970
проблема заключается в том, что в

888
00:35:20,970 --> 00:35:22,440
обучении с подкреплением и марковском

889
00:35:22,440 --> 00:35:24,230
решении обрабатываются ваши действия, чтобы

890
00:35:24,230 --> 00:35:26,190
определить, какие данные вы собираетесь

891
00:35:26,190 --> 00:35:27,989
увидеть  так что тот факт, что вместо того, чтобы

892
00:35:27,989 --> 00:35:30,559
выполнить правильное действие здесь, я пошел

893
00:35:30,559 --> 00:35:33,509
сюда, и теперь у меня нет данных, и мое

894
00:35:33,509 --> 00:35:35,460
распределение данных отличается, поэтому теперь

895
00:35:35,460 --> 00:35:37,049
нет никаких гарантий для моего суперсчета,

896
00:35:37,049 --> 00:35:38,940
он покупает алгоритм обучения, потому что моя

897
00:35:38,940 --> 00:35:40,170
дата отличается, на которой он никогда не

898
00:35:40,170 --> 00:35:41,460
обучался  ничего подобного, поэтому он

899
00:35:41,460 --> 00:35:43,710
не может обобщать, так что это именно

900
00:35:43,710 --> 00:35:47,190
проблема, и это было отмечено группой Like Draw

901
00:35:47,190 --> 00:35:49,680
Bag Mouse из CMU в 2011 году, когда они

902
00:35:49,680 --> 00:35:51,269
утверждали, что вы знаете, что это действительно

903
00:35:51,269 --> 00:35:53,009
большой пиар.  проблемой для того, что называется поведенческим

904
00:35:53,009 --> 00:35:54,869
клонированием, поэтому, несмотря на то, что было несколько

905
00:35:54,869 --> 00:35:57,960
хороших эмпирических демонстраций этого, он

906
00:35:57,960 --> 00:35:59,700
и его бывшие ученики Стефан Росс

907
00:35:59,700 --> 00:36:01,319
указали, почему это может

908
00:36:01,319 --> 00:36:03,779
быть фундаментально очень большой проблемой, и поэтому давайте

909
00:36:03,779 --> 00:36:05,940
продемонстрируем некоторые вещи, которые люди

910
00:36:05,940 --> 00:36:08,099
иногда видели эмпирически. идея  заключается в

911
00:36:08,099 --> 00:36:09,869
том, что как только вы отклоняетесь, то есть

912
00:36:09,869 --> 00:36:11,039
это время, когда вы делаете свою ошибку,

913
00:36:11,039 --> 00:36:13,019
тогда, по сути, вся остальная часть

914
00:36:13,019 --> 00:36:14,910
траектории может быть ошибками, которые вы можете

915
00:36:14,910 --> 00:36:17,789
совершить командой или командой, или ошибками, и, таким образом, это

916
00:36:17,789 --> 00:36:20,160
означает, что общее количество ошибок,

917
00:36:20,160 --> 00:36:23,160
которые вы делаете  не ожидается, что это эпсилон,

918
00:36:23,160 --> 00:36:24,960
умноженное на Т, но эпсилон, умноженное на Т в квадрате,

919
00:36:24,960 --> 00:36:28,589
это намного хуже, и это действительно из-за

920
00:36:28,589 --> 00:36:31,140
этих ошибок сложения, ведущих к тому,

921
00:36:31,140 --> 00:36:33,690
что ваше распределение

922
00:36:33,690 --> 00:36:34,270
состояний

923
00:36:34,270 --> 00:36:35,890
сильно отличается от того, о чем у вас есть данные,

924
00:36:35,890 --> 00:36:37,840
и эта проблема, о которой вы столкнетесь

925
00:36:37,840 --> 00:36:39,340
снова, снова своего рода цитата, это

926
00:36:39,340 --> 00:36:41,200
размышление о том, каково

927
00:36:41,200 --> 00:36:44,410
распределение состояний, в которые вы попадаете, вы

928
00:36:44,410 --> 00:36:45,700
знаете политику, которой вы следуете, по сравнению с

929
00:36:45,700 --> 00:36:47,080
политикой  Если вы хотите следовать этой

930
00:36:47,080 --> 00:36:48,580
проблеме, она снова возникает при

931
00:36:48,580 --> 00:36:53,830
обучении с подкреплением, так что это на самом деле именно

932
00:36:53,830 --> 00:36:55,300
эта или фундаментальная проблема, которую вы знаете,

933
00:36:55,300 --> 00:36:56,770
какое распределение данных вы

934
00:36:56,770 --> 00:36:59,020
получите в соответствии с политикой, которую вы

935
00:36:59,020 --> 00:37:01,690
изучили, по сравнению с истинной политикой и

936
00:37:01,690 --> 00:37:04,390
поиском.  при этом несоответствии, поэтому, как только вы сойдете

937
00:37:04,390 --> 00:37:06,040
с трассы, у вас не будет

938
00:37:06,040 --> 00:37:08,770
никаких данных об этом, поэтому одна из

939
00:37:08,770 --> 00:37:10,660
идей, которая привлекла Доуэлла и его

940
00:37:10,660 --> 00:37:12,190
учеников к размышлениям об

941
00:37:12,190 --> 00:37:14,950
этом, заключалась в том, чтобы сказать, что, если бы мы могли

942
00:37:14,950 --> 00:37:19,120
получить больше  данные, так что, если, когда я знаю

943
00:37:19,120 --> 00:37:21,310
мои данные, я могу начать только с Мала,

944
00:37:21,310 --> 00:37:23,260
я узнал мою политику контролируемого обучения,

945
00:37:23,260 --> 00:37:25,270
чтобы сказать, что вы знаете, что я должен делать

946
00:37:25,270 --> 00:37:26,770
на каждом свидании, пока я делаю ошибку,

947
00:37:26,770 --> 00:37:29,410
поэтому иногда вы знаете, что я иду  таким

948
00:37:29,410 --> 00:37:31,270
образом моя гоночная машина уезжает с

949
00:37:31,270 --> 00:37:34,030
ипподрома что, если бы я мог знать, что

950
00:37:34,030 --> 00:37:36,730
делать в этом состоянии в этом состоянии, поэтому я достиг

951
00:37:36,730 --> 00:37:37,840
состояния, в котором у меня нет никаких данных о

952
00:37:37,840 --> 00:37:40,000
том, что, если бы я мог спросить своего эксперта эй,

953
00:37:40,000 --> 00:37:41,890
что мне теперь делать  так что, как будто я иду сюда,

954
00:37:41,890 --> 00:37:43,390
я такой, о боже, я не знаю  теперь вы знаете,

955
00:37:43,390 --> 00:37:45,490
что делать сейчас, а затем вы спрашиваете своего

956
00:37:45,490 --> 00:37:46,750
эксперта там, например, о, просто поверните направо,

957
00:37:46,750 --> 00:37:48,100
все в порядке, вы все еще можете выйти из

958
00:37:48,100 --> 00:37:51,670
нужной двери, все в порядке, так что если бы вы могли попросить у

959
00:37:51,670 --> 00:37:53,020
своего эксперта ярлыки,

960
00:37:53,020 --> 00:37:54,910
ну, теперь вы получаете ярлыки о

961
00:37:54,910 --> 00:37:57,340
состояния, с которыми вы сталкиваетесь, и до

962
00:37:57,340 --> 00:37:59,140
тех пор, пока у вас есть данные

963
00:37:59,140 --> 00:38:00,760
, охватывающие все состояния, которые вы

964
00:38:00,760 --> 00:38:03,670
собираетесь потенциально испытать,

965
00:38:03,670 --> 00:38:04,930
тогда обучение под наблюдением

966
00:38:04,930 --> 00:38:06,820
должно довольно хорошо справляться с этим, но, как правило, возникает более серьезная проблема.

967
00:38:06,820 --> 00:38:08,260
с тем фактом, что

968
00:38:08,260 --> 00:38:09,400
вы сталкиваетесь с состояниями, что у вас

969
00:38:09,400 --> 00:38:11,260
нет никакого охвата ваших тренировочных

970
00:38:11,260 --> 00:38:14,350
данных, поэтому идея кинжала, который служит

971
00:38:14,350 --> 00:38:16,180
для агрегации набора данных, по сути, заключается в том, что вы

972
00:38:16,180 --> 00:38:19,240
просто продолжаете увеличивать свой набор данных, поэтому

973
00:38:19,240 --> 00:38:20,530
в этом случае вы начинаете  у

974
00:38:20,530 --> 00:38:22,000
вас нет никаких данных, вы инициализируете

975
00:38:22,000 --> 00:38:24,880
свою политику, вы следуете своей политике, поэтому в

976
00:38:24,880 --> 00:38:26,290
этом случае мы предполагаем, что у вас

977
00:38:26,290 --> 00:38:33,190
есть эксперт, какая-то экспертная политика, так

978
00:38:33,190 --> 00:38:34,480
что, возможно, вы знаете, что эксперт

979
00:38:34,480 --> 00:38:37,000
предпринимает некоторые шаги, а затем также ваши

980
00:38:37,000 --> 00:38:39,310
другие  эээ, политика, вы могли бы собрать

981
00:38:39,310 --> 00:38:41,680
траекторию, поэтому иногда вы следуете

982
00:38:41,680 --> 00:38:43,750
одному предложению политики, которого вы не придерживаетесь, и

983
00:38:43,750 --> 00:38:47,170
тогда вы получаете это состояние

984
00:38:47,170 --> 00:38:49,359
восемь, вы можете пойти и спросить своего эксперта для

985
00:38:49,359 --> 00:38:51,130
каждого состояния, что бы вы

986
00:38:51,130 --> 00:38:55,540
сделали здесь  так что да, для каждого состояния, с которым

987
00:38:55,540 --> 00:38:57,040
вы столкнулись на этой траектории, а

988
00:38:57,040 --> 00:39:01,299
затем вы добавляете все эти кортежи в свой

989
00:39:01,299 --> 00:39:02,710
набор данных и обучаете свою

990
00:39:02,710 --> 00:39:05,950
политику контролируемого обучения на этом, поэтому все

991
00:39:05,950 --> 00:39:07,690
внутри вашего набора данных, который вы

992
00:39:07,690 --> 00:39:10,150
используете для обучения своей политике,  с

993
00:39:10,150 --> 00:39:12,970
экспертной меткой, и вы медленно

994
00:39:12,970 --> 00:39:15,970
увеличиваете размер этого набора данных, поэтому идея состоит в том, что

995
00:39:15,970 --> 00:39:16,990
вы как бы даете все больше и больше

996
00:39:16,990 --> 00:39:19,180
экспертов все больше меток экспертных

997
00:39:19,180 --> 00:39:21,609
действий в направлениях, которые вы на

998
00:39:21,609 --> 00:39:23,530
самом деле представляли, и есть хорошие формальные

999
00:39:23,530 --> 00:39:24,819
свойства  для этого, чтобы вы могли быть

1000
00:39:24,819 --> 00:39:27,069
гарантированы, что вы придете к

1001
00:39:27,069 --> 00:39:31,180
хорошей политике, следуя этому в соответствии с

1002
00:39:31,180 --> 00:39:34,150
индуцированным распределением состояний, да, просто чтобы

1003
00:39:34,150 --> 00:39:36,040
подтвердить, что это предполагает, что у нас есть

1004
00:39:36,040 --> 00:39:38,859
начало наших падений, что мы делаем в

1005
00:39:38,859 --> 00:39:41,290
случае, если мы  у меня нет экспертов, да,

1006
00:39:41,290 --> 00:39:42,460
это отличный вопрос, поэтому я, когда

1007
00:39:42,460 --> 00:39:43,480
искал, просто мне пришлось бы

1008
00:39:43,480 --> 00:39:44,530
перепроверить, я думаю, это предполагает, что

1009
00:39:44,530 --> 00:39:46,059
ваш эксперт может дать вам действие, которое

1010
00:39:46,059 --> 00:39:48,309
он не знает, предположим, что у вас

1011
00:39:48,309 --> 00:39:50,589
есть явный доступ  к звезде PI, потому что,

1012
00:39:50,589 --> 00:39:51,910
если бы у вас был явный доступ к звезде PI,

1013
00:39:51,910 --> 00:39:53,410
вам не нужно было бы ничего изучать, поэтому я

1014
00:39:53,410 --> 00:39:55,349
думаю, что в этом случае это все равно, что подбрасывать

1015
00:39:55,349 --> 00:39:57,910
монетку о том, дает ли эксперт просто

1016
00:39:57,910 --> 00:39:59,260
прямое действие в этом

1017
00:39:59,260 --> 00:40:00,400
случае или вы следуете другому

1018
00:40:00,400 --> 00:40:03,460
политика, которая, потому что вы

1019
00:40:03,460 --> 00:40:04,990
все равно должны иметь эксперта рядом,

1020
00:40:04,990 --> 00:40:06,130
потому что им всегда придется в

1021
00:40:06,130 --> 00:40:07,270
конечном итоге сказать вам, что они там сделали бы,

1022
00:40:07,270 --> 00:40:09,400
хорошо, вот как вы получаете,

1023
00:40:09,400 --> 00:40:15,119
извините меня, этот набор данных,

1024
00:40:15,450 --> 00:40:19,049
и мне любопытно, как  это делается

1025
00:40:19,049 --> 00:40:21,210
эффективно, я могу себе представить, что в

1026
00:40:21,210 --> 00:40:24,839
некоторых ситуациях, когда человек отправляет

1027
00:40:24,839 --> 00:40:27,960
командную строку, ваш процессор хорошо тренируется,

1028
00:40:27,960 --> 00:40:31,140
и действия ввода не будут эффективными,

1029
00:40:31,140 --> 00:40:34,559
как люди обычно делают это

1030
00:40:34,559 --> 00:40:35,819
таким образом, что не требует ручного управления  аль-

1031
00:40:35,819 --> 00:40:38,430
вмешательство, да, есть отличное,

1032
00:40:38,430 --> 00:40:41,579
которое вы хорошо знаете, это требует, чтобы

1033
00:40:41,579 --> 00:40:43,230
у вас был эксперт либо для

1034
00:40:43,230 --> 00:40:44,730
каждого шага, либо, например, в энергетической

1035
00:40:44,730 --> 00:40:46,020
структуре, которая может вернуться и пометить

1036
00:40:46,020 --> 00:40:47,220
все, и это невероятно

1037
00:40:47,220 --> 00:40:48,720
дорого, и вы, если вы делаете это

1038
00:40:48,720 --> 00:40:49,829
для  вы знаете миллионы и миллионы

1039
00:40:49,829 --> 00:40:51,089
временных шагов, которые совершенно неразрешимы,

1040
00:40:51,089 --> 00:40:54,390
я думаю, именно поэтому я нахожусь в этом направлении

1041
00:40:54,390 --> 00:40:57,990
исследований, которое в определенном смысле оказало меньшее влияние,

1042
00:40:57,990 --> 00:40:59,460
чем некоторые другие

1043
00:40:59,460 --> 00:41:00,750
методы, которые мы увидим далее

1044
00:41:00,750 --> 00:41:03,839
с точки зрения того, как вы это делаете.  что-то вроде обратного

1045
00:41:03,839 --> 00:41:06,540
RL, так что на самом деле это предполагает,

1046
00:41:06,540 --> 00:41:08,579
что у вас есть эта человеческая петля, которая

1047
00:41:08,579 --> 00:41:10,950
действительно находится в петле, а не просто

1048
00:41:10,950 --> 00:41:12,000
просите их предоставить демонстрации

1049
00:41:12,000 --> 00:41:14,099
один раз, а затем ваш алгоритм отключается,

1050
00:41:14,099 --> 00:41:16,500
и я думаю, что практически в большинстве

1051
00:41:16,500 --> 00:41:18,150
случаев это гораздо больше  реалистично сказать, что

1052
00:41:18,150 --> 00:41:20,010
вы знаете, что проедете машину вокруг квартала

1053
00:41:20,010 --> 00:41:21,900
10 раз, но затем вы можете уйти, и

1054
00:41:21,900 --> 00:41:23,880
мы сделаем все наши RL вместо того, чтобы говорить, что мне

1055
00:41:23,880 --> 00:41:26,069
нужно, чтобы вы были в машине, или вы знаете,

1056
00:41:26,069 --> 00:41:27,930
как обозначить все  траектории

1057
00:41:27,930 --> 00:41:29,160
, по которым движется машина, и продолжают говорить вам,

1058
00:41:29,160 --> 00:41:30,270
правильно это или неправильно, я думаю, что

1059
00:41:30,270 --> 00:41:31,770
это просто очень интенсивное использование ярлыков, это очень

1060
00:41:31,770 --> 00:41:34,380
дорого, поэтому я думаю, что в некоторых

1061
00:41:34,380 --> 00:41:36,960
ограниченных случаях, например, если ваша скорость действий

1062
00:41:36,960 --> 00:41:40,680
очень низкая, например, если ваша скорость действий - это

1063
00:41:40,680 --> 00:41:41,880
вы  уметь принимать решения в вооруженных

1064
00:41:41,880 --> 00:41:44,160
силах или никто другой с очень высоким

1065
00:41:44,160 --> 00:41:45,809
уровнем очень редкие решения это может быть

1066
00:41:45,809 --> 00:41:46,950
очень разумно, потому что вы можете в

1067
00:41:46,950 --> 00:41:48,210
основном использовать свои знания бесконечных

1068
00:41:48,210 --> 00:41:50,790
вычислений перед каждым

1069
00:41:50,790 --> 00:41:52,700
принятием решения, если вы делаете это в

1070
00:41:52,700 --> 00:41:56,220
первый раз в режиме реального времени решения уровня боли  Я

1071
00:41:56,220 --> 00:42:00,900
думаю, что это очень сложно просто быть

1072
00:42:00,900 --> 00:42:03,119
совместимым с экспертом, который берет на

1073
00:42:03,119 --> 00:42:04,920
себя управление системой, как будто кто-то

1074
00:42:04,920 --> 00:42:07,470
сидит за рулем, позволяя агенту

1075
00:42:07,470 --> 00:42:09,960
вести машину, а затем все вроде бы признают, что

1076
00:42:09,960 --> 00:42:11,730
возникла чрезвычайная ситуация, когда нужно

1077
00:42:11,730 --> 00:42:15,150
сесть за

1078
00:42:15,150 --> 00:42:17,520
руль.  совместимо с экспертом, вступающим во

1079
00:42:17,520 --> 00:42:20,190
владение да, я имею в виду, я думаю, и это может

1080
00:42:20,190 --> 00:42:21,540
быть более простой способ получить ярлыки, поэтому вы

1081
00:42:21,540 --> 00:42:22,950
можете сказать, если у вас есть эксперт там

1082
00:42:22,950 --> 00:42:24,480
каждый  предпринятое действие такое

1083
00:42:24,480 --> 00:42:25,920
же, как и действие, которое предпримет эксперт,

1084
00:42:25,920 --> 00:42:27,210
может быть, они не вмешиваются, в противном случае

1085
00:42:27,210 --> 00:42:28,970
они только предоставляют ярлыки

1086
00:42:28,970 --> 00:42:30,380
или вмешательства, когда они будут отличаться,

1087
00:42:30,380 --> 00:42:32,420
но это все равно требует, как эксперт,

1088
00:42:32,420 --> 00:42:35,980
чтобы контролировать, что часто все еще

1089
00:42:35,980 --> 00:42:38,210
сложно умственно, по сути, вы

1090
00:42:38,210 --> 00:42:42,260
знаете, что это  все еще высокая стоимость, все в порядке, так

1091
00:42:42,260 --> 00:42:44,420
что это приятно, я имею в виду, там

1092
00:42:44,420 --> 00:42:46,160
очень приятно видеть своего рода их формальную

1093
00:42:46,160 --> 00:42:47,930
характеристику того, почему клонирование поведения

1094
00:42:47,930 --> 00:42:50,030
может быть плохим и в чем причина

1095
00:42:50,030 --> 00:42:53,960
этого, и я думаю, что кинжал может

1096
00:42:53,960 --> 00:42:55,460
быть очень полезен в  определенных обстоятельствах,

1097
00:42:55,460 --> 00:42:57,050
но есть много случаев, когда просто

1098
00:42:57,050 --> 00:42:58,520
практически гораздо проще получить

1099
00:42:58,520 --> 00:43:01,130
демонстрационные примеры, а затем предположить

1100
00:43:01,130 --> 00:43:02,660
, что в цикле больше нет человека,

1101
00:43:02,660 --> 00:43:03,490


1102
00:43:03,490 --> 00:43:06,470
так что инверсное RL больше относится к одной

1103
00:43:06,470 --> 00:43:08,690
из вторых категорий, так что

1104
00:43:08,690 --> 00:43:10,670
же происходит в  инверсный RL, ну,

1105
00:43:10,670 --> 00:43:11,690
сначала давайте просто подумаем о том, что вы знаете

1106
00:43:11,690 --> 00:43:16,609
функцию вознаграждения, основанную на функциях, так что

1107
00:43:16,609 --> 00:43:18,290
хорошо, мы вернемся к этому через секунду, так что

1108
00:43:18,290 --> 00:43:19,430
снова мы думаем об этом  случай,

1109
00:43:19,430 --> 00:43:21,410
когда у нас есть какая-то модель перехода, которую

1110
00:43:21,410 --> 00:43:24,380
мы можем не наблюдать или, может быть, мы наблюдаем,

1111
00:43:24,380 --> 00:43:26,660
и многие методы здесь для начала

1112
00:43:26,660 --> 00:43:28,250
предполагают, что вы знаете

1113
00:43:28,250 --> 00:43:30,290
модель перехода, которая довольно сильна

1114
00:43:30,290 --> 00:43:32,180
для многих областей реального мира, но в

1115
00:43:32,180 --> 00:43:34,069
некоторых случаях это  разумно, так что

1116
00:43:34,069 --> 00:43:35,270
прямо сейчас мы собираемся предположить, что

1117
00:43:35,270 --> 00:43:36,500
единственное, чего мы не знаем, это

1118
00:43:36,500 --> 00:43:38,869
функция вознаграждения, есть некоторые расширения

1119
00:43:38,869 --> 00:43:39,920
, когда вы не знаете модели перехода,

1120
00:43:39,920 --> 00:43:42,500
вы в порядке, поэтому у нас снова есть

1121
00:43:42,500 --> 00:43:44,599
наш набор демонстраций и  цель

1122
00:43:44,599 --> 00:43:47,450
сейчас состоит не в том, чтобы напрямую изучить политику, а

1123
00:43:47,450 --> 00:43:50,780
просто в том, чтобы сделать вывод о функции вознаграждения, поэтому, если

1124
00:43:50,780 --> 00:43:52,970
я ничего не скажу вам об

1125
00:43:52,970 --> 00:43:55,400
оптимальности политики учителей, что

1126
00:43:55,400 --> 00:43:56,630
можно сделать вывод об их явной

1127
00:43:56,630 --> 00:43:59,210
функции, я мог бы сказать, что это не

1128
00:43:59,210 --> 00:44:03,650
эксперт, это просто  демонстрации, если вы

1129
00:44:03,650 --> 00:44:05,900
получите демонстрацию состояния действия и

1130
00:44:05,900 --> 00:44:09,730
т. д., можете ли вы сделать вывод о R,

1131
00:44:09,730 --> 00:44:11,720
если вы ничего не знаете об

1132
00:44:11,720 --> 00:44:14,079
оптимальности,

1133
00:44:17,790 --> 00:44:19,850


1134
00:44:24,269 --> 00:44:26,380
вы не будете такими же, как образец, поэтому я

1135
00:44:26,380 --> 00:44:28,089
думаю, что мы получим больше демонстрации  s

1136
00:44:28,089 --> 00:44:30,880
похожи на нашу звезду, я думаю, что это не предполагает

1137
00:44:30,880 --> 00:44:34,539
никаких предположений об оптимальности, поэтому, если я

1138
00:44:34,539 --> 00:44:36,190
ничего не скажу вам об

1139
00:44:36,190 --> 00:44:38,380
оптимальности политики или не увижу ее,

1140
00:44:38,380 --> 00:44:40,240
есть ли какая-либо информация, которую вы

1141
00:44:40,240 --> 00:44:45,220
не могли бы собрать в этом случае выбор, который вы

1142
00:44:45,220 --> 00:44:54,039
сделали в соответствии с их  политика только для этого

1143
00:44:54,039 --> 00:44:55,480
конкретного человека, вы могли бы сказать

1144
00:44:55,480 --> 00:44:58,599
что-то об их функции вознаграждения,

1145
00:44:58,599 --> 00:45:00,369
предполагая, что они являются рациональными

1146
00:45:00,369 --> 00:45:02,500
агентами, что это было выше по своему

1147
00:45:02,500 --> 00:45:05,140
происхождению, это правда, но если бы вы хотели, чтобы

1148
00:45:05,140 --> 00:45:07,200
это было об общей функции вознаграждения,

1149
00:45:07,200 --> 00:45:10,000
это сказало бы вам, может быть, я  Я окрашиваю

1150
00:45:10,000 --> 00:45:12,599
его так, что он кажется немного более

1151
00:45:12,599 --> 00:45:15,190
тонким, чем я имею в виду, это не

1152
00:45:15,190 --> 00:45:16,630
говорит вам ничего правильного, например, если вы, если

1153
00:45:16,630 --> 00:45:19,779
вы видите меня, любите бродить вокруг, как если бы

1154
00:45:19,779 --> 00:45:21,549
вы видели агента, крутящегося вокруг,

1155
00:45:21,549 --> 00:45:23,470
и вы ничего не знаете  о том, принимает ли он

1156
00:45:23,470 --> 00:45:24,970
правильные решения

1157
00:45:24,970 --> 00:45:26,880
относительно истинной функции вознаграждения.

1158
00:45:26,880 --> 00:45:28,450
Демонстрации ничего не говорят вам,

1159
00:45:28,450 --> 00:45:30,309
как они не дают вам никакой информации

1160
00:45:30,309 --> 00:45:31,809
о функции вознаграждения, если только вы не

1161
00:45:31,809 --> 00:45:33,970
знаете что-то об этом.  вне, если вы не

1162
00:45:33,970 --> 00:45:35,650
пытаетесь сделать предположение, что

1163
00:45:35,650 --> 00:45:37,720
агент действует рационально по отношению

1164
00:45:37,720 --> 00:45:40,660
к истинной функции вознаграждения, и я, возможно,

1165
00:45:40,660 --> 00:45:41,680
вы можете получить некоторую информацию об их

1166
00:45:41,680 --> 00:45:43,329
внутренней функции, например, что говорят, но в

1167
00:45:43,329 --> 00:45:45,579
целом, если мы не делаем никаких предположений

1168
00:45:45,579 --> 00:45:48,460
о поведении агентов, и мы не

1169
00:45:48,460 --> 00:45:50,079
делаем и не предполагаем, что они делают что-то

1170
00:45:50,079 --> 00:45:51,369
оптимальное в отношении глобальной

1171
00:45:51,369 --> 00:45:53,049
функции вознаграждения, нет никакой информации, которую

1172
00:45:53,049 --> 00:45:55,900
вы можете получить сейчас, более сложная задача

1173
00:45:55,900 --> 00:45:58,119
будет следующей, поэтому давайте предположим, что

1174
00:45:58,119 --> 00:45:59,589
политика учителей  оптимальна по отношению

1175
00:45:59,589 --> 00:46:02,019
к истинной глобальной функции вознаграждения,

1176
00:46:02,019 --> 00:46:03,549
которую агент также, возможно, захочет

1177
00:46:03,549 --> 00:46:06,160
оптимизировать в будущем, поэтому вы получите опытного

1178
00:46:06,160 --> 00:46:10,470
водителя для водителя,

1179
00:46:10,920 --> 00:46:15,160
разъезжающего по городу. Я на секунду задумался о том, что

1180
00:46:15,160 --> 00:46:16,869
вы можете сделать вывод о функции вознаграждения

1181
00:46:16,869 --> 00:46:20,019
в  в этом случае и, в частности,

1182
00:46:20,019 --> 00:46:22,299
есть ли более одной функции вознаграждения,

1183
00:46:22,299 --> 00:46:25,900
которая могла бы объяснить их поведение, поэтому

1184
00:46:25,900 --> 00:46:27,400
подумайте о том, в чем ее уникальность, скажем,

1185
00:46:27,400 --> 00:46:30,099
представьте, что данные не проблема. Я даю вам десять

1186
00:46:30,099 --> 00:46:33,160
триллионов экз.  Множество агентов,

1187
00:46:33,160 --> 00:46:36,760
следующих оптимальной политике. 10 триллионов примеров,

1188
00:46:36,760 --> 00:46:39,880
и вы хотите посмотреть, сможете ли вы узнать,

1189
00:46:39,880 --> 00:46:41,440
какова их функция слова, и этот

1190
00:46:41,440 --> 00:46:44,410
вопрос заключается в том, существует ли единственная функция вознаграждения,

1191
00:46:44,410 --> 00:46:46,869
которая согласуется с их

1192
00:46:46,869 --> 00:46:50,920
данными, или многим может понадобиться секунда,

1193
00:46:50,920 --> 00:46:53,050
чтобы поговорить  кому-нибудь из окружающих, я просто

1194
00:46:53,050 --> 00:46:55,630
хочу посмотреть, есть ли вопрос,

1195
00:46:55,630 --> 00:46:57,160
действительно ли есть одна функция вознаграждения,

1196
00:46:57,160 --> 00:46:59,500
есть ли уникальная функция вознаграждения, если у вас

1197
00:46:59,500 --> 00:47:00,880
есть бесконечные данные, так что это не

1198
00:47:00,880 --> 00:47:05,220
проблема разреженности данных по сравнению со многими,

1199
00:48:01,869 --> 00:48:04,640
я собираюсь сделать быстрый выбор, хорошо, мы

1200
00:48:04,640 --> 00:48:05,960
Я собираюсь вытащить вас, ребята, что

1201
00:48:05,960 --> 00:48:07,730
я спрошу, думаете ли вы, что существует одна

1202
00:48:07,730 --> 00:48:09,410
функция вознаграждения или существует более

1203
00:48:09,410 --> 00:48:11,810
одной функции вознаграждения, так что кто думает, что

1204
00:48:11,810 --> 00:48:13,040
есть одна функция вознаграждения

1205
00:48:13,040 --> 00:48:15,349
бесконечные данные единственная функция вознаграждения,

1206
00:48:15,349 --> 00:48:17,450
которая непротиворечива, как вы думаете  есть

1207
00:48:17,450 --> 00:48:20,180
более одной функции вознаграждения, хорошо, может

1208
00:48:20,180 --> 00:48:22,400
кто-нибудь дать мне функцию вознаграждения,

1209
00:48:22,400 --> 00:48:25,579
которая всегда согласуется с любой оптимальной

1210
00:48:25,579 --> 00:48:27,760
политикой,

1211
00:48:33,170 --> 00:48:36,019
я думаю, что вам нужно сделать

1212
00:48:36,019 --> 00:48:38,390
на первом этапе, просто если старше  помимо

1213
00:48:38,390 --> 00:48:41,059
того, что учитель будет иметь

1214
00:48:41,059 --> 00:48:47,359
политику, то, возможно, на том первом шаге,

1215
00:48:47,359 --> 00:48:48,650
который вы могли бы дать, вы можете дать наибольшую

1216
00:48:48,650 --> 00:48:50,089
награду, например, у них есть общий

1217
00:48:50,089 --> 00:48:51,589
опыт, а затем он будет случайным после этого

1218
00:48:51,589 --> 00:48:54,019
, и я мог бы зависеть от состояния, я

1219
00:48:54,019 --> 00:48:55,880
думаю, я  я думал, может только сказать мне,

1220
00:48:55,880 --> 00:49:00,680
например, число, которое позволит или вы

1221
00:49:00,680 --> 00:49:02,029
знаете спецификацию функции вознаграждения,

1222
00:49:02,029 --> 00:49:05,089
например, константу, например,

1223
00:49:05,089 --> 00:49:07,220
выбор константы, которая сделает любую

1224
00:49:07,220 --> 00:49:11,660
политику оптимальной, да, и желтый, вы

1225
00:49:11,660 --> 00:49:13,250
просто даете нулевое вознаграждение за каждый

1226
00:49:13,250 --> 00:49:17,960
действие, которое вы даете нулевое вознаграждение, любая

1227
00:49:17,960 --> 00:49:19,670
политика является оптимальным уважением, что вы никогда не

1228
00:49:19,670 --> 00:49:21,470
получите никакого вознаграждения в любом месте, это печальная жизнь,

1229
00:49:21,470 --> 00:49:24,019
к сожалению, и в этом случае все

1230
00:49:24,019 --> 00:49:28,099
политики оптимальны, поэтому, если вы

1231
00:49:28,099 --> 00:49:30,519
просто наблюдаете траектории, тогда одна

1232
00:49:30,519 --> 00:49:33,410
функция вознаграждения, для которой эта политика является

1233
00:49:33,410 --> 00:49:37,690
оптимальной  на нуле, но это не уникально, поэтому

1234
00:49:38,230 --> 00:49:40,849
эта проблема наблюдалась, я думаю, что это было

1235
00:49:40,849 --> 00:49:45,019
сделано Эндрю Инджем и Стюартом Расселом еще

1236
00:49:45,019 --> 00:49:49,309
в 2000 году, где есть статья, в которой говорится

1237
00:49:49,309 --> 00:49:51,589
об обратном RL, где они отметили эту

1238
00:49:51,589 --> 00:49:54,470
проблему.  Проблема в том, что это не

1239
00:49:54,470 --> 00:49:56,509
уникально без дополнительных предположений,

1240
00:49:56,509 --> 00:49:58,250
есть много функций вознаграждения, которые

1241
00:49:58,250 --> 00:50:05,539
непротиворечивы, поэтому нам

1242
00:50:05,539 --> 00:50:06,680
придется подумать о том, как мы разорвем связи

1243
00:50:06,680 --> 00:50:07,579
и как мы наложим дополнительную

1244
00:50:07,579 --> 00:50:18,200
структуру да, есть закон, чтобы они

1245
00:50:18,200 --> 00:50:21,259
помнили ваш  имя, так что да, что это такое,

1246
00:50:21,259 --> 00:50:23,180
я думаю, что есть много многих или

1247
00:50:23,180 --> 00:50:25,869
словесных функций, поэтому, если у вас есть константа,

1248
00:50:25,869 --> 00:50:29,049
все имеет то же самое, любая константа

1249
00:50:29,049 --> 00:50:34,309
также идентична, так что обычно есть

1250
00:50:34,309 --> 00:50:37,039
много разных функций вознаграждения,

1251
00:50:37,039 --> 00:50:38,990
которые все дадут вам, есть много

1252
00:50:38,990 --> 00:50:40,700
разных функций вознаграждения, для которых  любая

1253
00:50:40,700 --> 00:50:43,519
политика оптимальна, и это будет означать,

1254
00:50:43,519 --> 00:50:44,750
что если вы пытаетесь сделать вывод о том, какая

1255
00:50:44,750 --> 00:50:46,410
функция, учитывая некоторые данные,

1256
00:50:46,410 --> 00:50:47,640
существует множество функций вознаграждения, которые вы

1257
00:50:47,640 --> 00:50:49,290
могли бы записать, чтобы эти данные

1258
00:50:49,290 --> 00:50:50,910
были оптимальными в отношении их функции вознаграждения,

1259
00:50:50,910 --> 00:50:53,220
а вторая часть  на самом деле

1260
00:50:53,220 --> 00:50:54,450
то, что мы пытаемся получить, это то, что мы пытаемся

1261
00:50:54,450 --> 00:50:59,100
вывести, какая функция вознаграждения

1262
00:50:59,100 --> 00:51:02,330
сделает эти данные такими, как будто они

1263
00:51:02,330 --> 00:51:05,640
исходят от оптимальной политики, если мы

1264
00:51:05,640 --> 00:51:09,390
предположим  д., что эксперт оптимален, поэтому

1265
00:51:09,390 --> 00:51:10,920
давайте подумаем также о том, как мы делаем это

1266
00:51:10,920 --> 00:51:13,140
в больших пространствах состояний, поэтому мы

1267
00:51:13,140 --> 00:51:14,280
подумаем об аппроксиматоре линейной функции тела,

1268
00:51:14,280 --> 00:51:16,980
потому что часто

1269
00:51:16,980 --> 00:51:18,450
места, где нам особенно нужно быть

1270
00:51:18,450 --> 00:51:20,010
эффективным сэмплом, например, когда наше пространство состояний

1271
00:51:20,010 --> 00:51:21,120
огромен, и мы не сможем

1272
00:51:21,120 --> 00:51:22,830
эффективно исследовать его, поэтому давайте подумаем

1273
00:51:22,830 --> 00:51:24,300
об аппроксиматоре функции линейного значения,

1274
00:51:24,300 --> 00:51:26,340
и мы подумаем, что это вознаграждение

1275
00:51:26,340 --> 00:51:30,180
также является линейным по функциям, поэтому

1276
00:51:30,180 --> 00:51:32,790
наша функция вознаграждения может быть несколько весов,

1277
00:51:32,790 --> 00:51:35,520
умноженных на представление некоторых функций

1278
00:51:35,520 --> 00:51:37,760
наше пространство состояний и цель состоит в том, чтобы

1279
00:51:37,760 --> 00:51:40,440
вычислить хороший набор весов, учитывая нашу

1280
00:51:40,440 --> 00:51:42,960
демонстрацию. Я уже сказал, что в

1281
00:51:42,960 --> 00:51:45,420
целом это не уникально, но мы

1282
00:51:45,420 --> 00:51:46,710
попытаемся найти способы сделать это

1283
00:51:46,710 --> 00:51:51,090
и различные методы, поэтому функция значения

1284
00:51:51,090 --> 00:51:52,680
для политики  PI может быть

1285
00:51:52,680 --> 00:51:54,840
выражен следующим образом: вы

1286
00:51:54,840 --> 00:51:57,510
просто записываете его как ожидаемое

1287
00:51:57,510 --> 00:51:59,610
значение дисконтированной суммы вознаграждения, и

1288
00:51:59,610 --> 00:52:02,040
это состояния, которых вы достигли бы в

1289
00:52:02,040 --> 00:52:06,090
соответствии с этой политикой в

1290
00:52:06,090 --> 00:52:08,160
тветствии с  Распределение состояний, которые вы получаете в соответствии с этой политикой,

1291
00:52:08,160 --> 00:52:10,950
это вознаграждения, поэтому

1292
00:52:10,950 --> 00:52:14,640
теперь мы собираемся выразить это, поэтому мы

1293
00:52:14,640 --> 00:52:20,130
делаем это сейчас, это будет у нас

1294
00:52:20,130 --> 00:52:21,840
есть линейное представление нашей

1295
00:52:21,840 --> 00:52:26,250
функции вознаграждения, поэтому  мы можем написать, что мы можем

1296
00:52:26,250 --> 00:52:30,210
повторно выразить это так, чтобы мы могли

1297
00:52:30,210 --> 00:52:33,120
записать его с точки зрения характеристик

1298
00:52:33,120 --> 00:52:35,010
состояния, которого мы достигаем на каждом временном шаге, умноженном

1299
00:52:35,010 --> 00:52:37,410
на вес, а затем, поскольку

1300
00:52:37,410 --> 00:52:39,930
вектор веса постоянен для всего, вы

1301
00:52:39,930 --> 00:52:41,610
можете просто переместить его  и затем вы получаете

1302
00:52:41,610 --> 00:52:44,100
это интересное выражение, которое заключается в том, что вы

1303
00:52:44,100 --> 00:52:46,140
просто имеете эту дисконтированную сумму

1304
00:52:46,140 --> 00:52:49,100
признаков состояния, с которыми вы сталкиваетесь,

1305
00:52:49,100 --> 00:52:52,050
мы собираемся назвать это mu, поэтому мы говорили

1306
00:52:52,050 --> 00:52:54,540
об этом очень кратко ранее, но теперь

1307
00:52:54,540 --> 00:52:56,100
мы говорим о том, что mu сортируется

1308
00:52:56,100 --> 00:52:57,780
дисконтированной взвешенной частоты

1309
00:52:57,780 --> 00:53:00,300
функций состояния в соответствии с политикой,

1310
00:53:00,300 --> 00:53:03,030
сколько времени вы тратите на различные

1311
00:53:03,030 --> 00:53:05,700
функции, или вы знаете, в основном, сколько

1312
00:53:05,700 --> 00:53:06,630
времени вы тратите в разных состояниях

1313
00:53:06,630 --> 00:53:08,570
для будущих глаз версия этого

1314
00:53:08,570 --> 00:53:10,440
со скидкой по типу, когда вы  вы достигаете

1315
00:53:10,440 --> 00:53:11,670
их, потому что некоторые состояния вы

1316
00:53:11,670 --> 00:53:12,870
действительно можете достичь очень далеко в будущем,

1317
00:53:12,870 --> 00:53:15,960
а не сейчас, так что это действительно своего

1318
00:53:15,960 --> 00:53:17,340
рода стационарное распределение, о котором мы

1319
00:53:17,340 --> 00:53:19,140
говорили раньше, теперь мы используем

1320
00:53:19,140 --> 00:53:25,080
дисконтирование, так почему это хорошо, хорошо, так

1321
00:53:25,080 --> 00:53:27,780
что бы мы хотели  Теперь я хочу сказать,

1322
00:53:27,780 --> 00:53:29,910
что вместо того, чтобы думать непосредственно о

1323
00:53:29,910 --> 00:53:32,670
функциях вознаграждения, мы можем начать

1324
00:53:32,670 --> 00:53:35,580
думать о распределениях состояний и

1325
00:53:35,580 --> 00:53:38,060
думать о своего рода вероятности

1326
00:53:38,060 --> 00:53:39,960
достижения разных распределений

1327
00:53:39,960 --> 00:53:42,920
состояний, различных распределений состояний как

1328
00:53:42,920 --> 00:53:46,050
представляющих разные разные

1329
00:53:46,050 --> 00:53:47,880
политики, существенно разные политики

1330
00:53:47,880 --> 00:53:50,580
для конкретной награды.  функция будет

1331
00:53:50,580 --> 00:53:52,110
достигать различных распределений состояний,

1332
00:53:52,110 --> 00:53:54,900
и поэтому мы можем подумать об использовании этой

1333
00:53:54,900 --> 00:53:57,020
формулировки для обучения ученичества,

1334
00:53:57,020 --> 00:54:00,390
поэтому в этом случае у нас есть эта хорошая

1335
00:54:00,390 --> 00:54:02,880
настройка для обучения полосы Бренда прямо

1336
00:54:02,880 --> 00:54:04,950
сейчас, мы используем приближение линейной функции ценности,

1337
00:54:04,950 --> 00:54:06,300
мы называем это

1338
00:54:06,300 --> 00:54:07,500
обучением ученичества, мы'  повторное обучение,

1339
00:54:07,500 --> 00:54:09,300
как агенты, получающие ученичество

1340
00:54:09,300 --> 00:54:12,960
от от от  демонстратор, так что

1341
00:54:12,960 --> 00:54:14,220
теперь у нас есть эта дисконтированная взвешенная

1342
00:54:14,220 --> 00:54:17,250
частота состояний признаков, поэтому мы

1343
00:54:17,250 --> 00:54:18,870
всегда как бы движемся в

1344
00:54:18,870 --> 00:54:22,110
пространство признаков состояний сейчас, а затем мы хотим

1345
00:54:22,110 --> 00:54:26,190
отметить следующее, поэтому, если мы определим, что

1346
00:54:26,190 --> 00:54:30,390
является функцией значения для пи-звезды, это

1347
00:54:30,390 --> 00:54:32,670
просто  равно ожидаемому дисконтированному

1348
00:54:32,670 --> 00:54:34,860
летнему вознаграждению, которого мы достигаем, и по

1349
00:54:34,860 --> 00:54:37,530
определению это лучше, чем значение

1350
00:54:37,530 --> 00:54:40,440
для любой другой политики, по крайней мере, хорошо,

1351
00:54:40,440 --> 00:54:42,390
потому что либо pi совпадает с pi, либо это

1352
00:54:42,390 --> 00:54:43,680
то же самое, что и оптимальная политика, либо

1353
00:54:43,680 --> 00:54:46,830
отличается, и это просто равно  та

1354
00:54:46,830 --> 00:54:50,190
же самая функция вознаграждения, которую мы

1355
00:54:50,190 --> 00:54:51,660
не знаем, но при другом

1356
00:54:51,660 --> 00:54:53,970
распределении состояний она находится под

1357
00:54:53,970 --> 00:54:55,380
распределением состояний, в которое вы попадете, если

1358
00:54:55,380 --> 00:54:58,370
будете следовать этой альтернативной политике, и

1359
00:54:58,370 --> 00:55:00,960
поэтому, если мы думаем, что

1360
00:55:00,960 --> 00:55:02,550
демонстрации экспертов взяты из оптимальной

1361
00:55:02,550 --> 00:55:04,920
политики  чтобы идентифицировать W,

1362
00:55:04,920 --> 00:55:07,490
достаточно найти такую звезду W, что если вы 

1363
00:55:07,490 --> 00:55:10,290
аучили скалярному произведению, что с ра 

1364
00:55:10,290 --> 00:55:12,750
пределением состояний, которое вы получили пр 

1365
00:55:12,750 --> 00:55:13,890
оптимальной политике, по 

1366
00:55:13,890 --> 00:55:16,020
ните, что это то, что  мы знаем, что

1367
00:55:16,020 --> 00:55:18,200
мы можем получить это из наших демонстраций,

1368
00:55:18,200 --> 00:55:22,550
это должно выглядеть лучше, чем

1369
00:55:22,550 --> 00:55:25,110
округ, чем тот же вектор весов,

1370
00:55:25,110 --> 00:55:26,850
умноженный на распределение штатов, которое вы

1371
00:55:26,850 --> 00:55:32,640
получили бы при любой другой политике.

1372
00:55:32,640 --> 00:55:34,920


1373
00:55:34,920 --> 00:55:36,630
значение

1374
00:55:36,630 --> 00:55:38,970
оптимальной политики напрямую связано

1375
00:55:38,970 --> 00:55:40,560
с распределением состояний, которые вы получаете под

1376
00:55:40,560 --> 00:55:43,140
ней, умноженных на этот вектор весов, и это

1377
00:55:43,140 --> 00:55:46,200
значение должно быть выше, чем значение

1378
00:55:46,200 --> 00:55:48,630
любой другой политики, которая использует тот

1379
00:55:48,630 --> 00:55:50,160
же вектор весов и просто дает вам

1380
00:55:50,160 --> 00:56:08,000
другое распределение  состояния, такие

1381
00:56:08,000 --> 00:56:11,130
как стационарное распределение,

1382
00:56:11,130 --> 00:56:17,550
пропорция ускользнула, да, оценивая меня,

1383
00:56:17,550 --> 00:56:19,020
какой хороший способ думать об этом,

1384
00:56:19,020 --> 00:56:20,070
что мы должны думать об этом или это похоже

1385
00:56:20,070 --> 00:56:22,170
на стационарное распределение состояний, да,

1386
00:56:22,170 --> 00:56:23,400
я думаю, что это разумно думать, что это

1387
00:56:23,400 --> 00:56:24,420
было по существу стационарное

1388
00:56:24,420 --> 00:56:26,340
распределение состояний, взвешенное с

1389
00:56:26,340 --> 00:56:30,960
этим коэффициентом дисконтирования поверх него, так что

1390
00:56:30,960 --> 00:56:33,060
оно очень похоже на стационарные

1391
00:56:33,060 --> 00:56:36,020
распределения, которые мы видели раньше,

1392
00:56:40,700 --> 00:56:43,050
все в порядке, так что essen  В сущности, это то же самое,

1393
00:56:43,050 --> 00:56:44,520
мы хотим найти функцию вознаграждения,

1394
00:56:44,520 --> 00:56:45,870
чтобы экспертная политика и

1395
00:56:45,870 --> 00:56:47,340
распределение состояний, которых вы достигаете,

1396
00:56:47,340 --> 00:56:49,230
выглядели лучше, когда вы вычисляете

1397
00:56:49,230 --> 00:56:51,840
функцию ценности по сравнению с тем же

1398
00:56:51,840 --> 00:56:53,370
вектором весов при любом другом

1399
00:56:53,370 --> 00:56:58,740
распределении состояний, и поэтому, если мы можем

1400
00:56:58,740 --> 00:57:01,890
найдите политику так, чтобы ее

1401
00:57:01,890 --> 00:57:04,200
распределение состояний совпадало с распределением

1402
00:57:04,200 --> 00:57:06,600
состояний нашего эксперта, тогда у нас все

1403
00:57:06,600 --> 00:57:09,510
будет хорошо, так что здесь говорится,

1404
00:57:09,510 --> 00:57:12,060
что если у нас есть политика,

1405
00:57:12,060 --> 00:57:13,950
дисконтированное распределение состояний, которое

1406
00:57:13,950 --> 00:57:16,860
мы достигаем при ней, равно  близко к

1407
00:57:16,860 --> 00:57:18,360
распределению состояний, которое вы получили из

1408
00:57:18,360 --> 00:57:22,070
своей демонстрации, так что это эксперт,

1409
00:57:22,070 --> 00:57:25,590
поэтому, если это мало

1410
00:57:25,590 --> 00:57:28,170
, ваша ошибка функции значения также

1411
00:57:28,170 --> 00:57:34,500
будет небольшой, поэтому для любого W, если вы можете, если вы в

1412
00:57:34,500 --> 00:57:37,680
основном соответствуете функциям, соответствуете

1413
00:57:37,680 --> 00:57:39,840
ожидаемым функциям или соответствуете

1414
00:57:39,840 --> 00:57:42,450
распределениям  заявляет, что вы в порядке, тогда вы

1415
00:57:42,450 --> 00:57:45,360
обнаружили, что я нашел значение, которое будет

1416
00:57:45,360 --> 00:57:51,390
иметь очень похожее значение на истинное, так что это на

1417
00:57:51,390 --> 00:57:54,600
самом деле означает для любого WT здесь, так что это

1418
00:57:54,600 --> 00:57:56,010
означает, что не имеет значения w  что истинная

1419
00:57:56,010 --> 00:57:58,440
функция вознаграждения заключается в том, что если вы можете найти политику, чтобы

1420
00:57:58,440 --> 00:58:02,850
характеристики вашего состояния совпадали, то независимо

1421
00:58:02,850 --> 00:58:04,260
от того, какова истинная функция вознаграждения,

1422
00:58:04,260 --> 00:58:05,850
вы знаете, что вы будете близки

1423
00:58:05,850 --> 00:58:13,740
к истинному значению, да, они будут использоваться

1424
00:58:13,740 --> 00:58:17,640
для заполнения, я думаю,  ожидание, что

1425
00:58:17,640 --> 00:58:22,400
ваши, я думаю, значения безопасны, да, как

1426
00:58:22,400 --> 00:58:24,810
только вы это сделаете, и W вы можете объединить это

1427
00:58:24,810 --> 00:58:27,480
с вашей музой, чтобы вычислить как

1428
00:58:27,480 --> 00:58:31,490
значение состояния,

1429
00:58:32,300 --> 00:58:35,430
вы можете использовать его для принятия решений,

1430
00:58:35,430 --> 00:58:38,520
когда вы находитесь вне штата  поэтому любой

1431
00:58:38,520 --> 00:58:40,200
вопрос заключается в том, чтобы сказать, например, хорошо, поэтому, если

1432
00:58:40,200 --> 00:58:42,000
мы даем эти w или что-то вроде того, что

1433
00:58:42,000 --> 00:58:43,380
мы решаем для решения для

1434
00:58:43,380 --> 00:58:45,840
политики или решения для W и т. д., в этом

1435
00:58:45,840 --> 00:58:47,610
случае я думаю, что разумный способ думать

1436
00:58:47,610 --> 00:58:50,520
об этом - это решение для W  и решить

1437
00:58:50,520 --> 00:58:52,440
для pi, так что это говорит о том, что

1438
00:58:52,440 --> 00:58:55,080
допустим, вы оптимизируете PI, если

1439
00:58:55,080 --> 00:58:58,260
вы нашли PI, поэтому прямо сейчас мы знаем

1440
00:58:58,260 --> 00:58:59,820
модель перехода, которая не всегда

1441
00:58:59,820 --> 00:59:01,350
верна, но если вы знаете модель перехода

1442
00:59:01,350 --> 00:59:05,120
для данного PI, вы  может вычислить mu,

1443
00:59:05,120 --> 00:59:07,230
потому что это jus  Например, вы

1444
00:59:07,230 --> 00:59:08,640
могли бы выполнить развертывание Монте-Карло,

1445
00:59:08,640 --> 00:59:10,440
поэтому, если вам кто-то дал вам

1446
00:59:10,440 --> 00:59:12,120
PI, и они говорят вам модель перехода,

1447
00:59:12,120 --> 00:59:13,410
вы можете развернуть это, и вы можете

1448
00:59:13,410 --> 00:59:16,110
оценить меня PI, и это говорит, что если

1449
00:59:16,110 --> 00:59:17,580
вы это сделаете, скажем, я  у меня есть какая-то

1450
00:59:17,580 --> 00:59:20,040
политика, я повторяю это несколько раз,

1451
00:59:20,040 --> 00:59:21,960
я оцениваю свой Мью и проверяю

1452
00:59:21,960 --> 00:59:23,910
, похоже ли это на мою демонстрационную политику отключения звука,

1453
00:59:23,910 --> 00:59:26,280
если она маленькая,

1454
00:59:26,280 --> 00:59:28,230
это говорит о том, что независимо от того, какова реальная

1455
00:59:28,230 --> 00:59:30,120
функция вознаграждения, у вас будет

1456
00:59:30,120 --> 00:59:34,050
такое же значение, как то, что вы из себя представляете, как если бы

1457
00:59:34,050 --> 00:59:36,600
вы соответствовали значению, которое вы

1458
00:59:36,600 --> 00:59:38,400
получили бы в соответствии с экспертной политикой, так что вы

1459
00:59:38,400 --> 00:59:39,120
хороши, вы могли бы,

1460
00:59:39,120 --> 00:59:42,050
как указ отца,

1461
00:59:46,220 --> 00:59:49,190
и я смотрю на ограничение

1462
00:59:49,190 --> 00:59:52,550
нормы бесконечности для W да и  Я не

1463
00:59:52,550 --> 00:59:54,050
совсем понимаю, откуда это взялось.

1464
00:59:54,050 --> 00:59:57,230
Мне любопытно, если я пропустил это здесь или

1465
00:59:57,230 --> 00:59:59,630
мы не рассмотрели это, почему у

1466
00:59:59,630 --> 01:00:03,860
нас есть это ограничение в отношении W,

1467
01:00:03,860 --> 01:00:05,210
я должен перепроверить детали.

1468
01:00:05,210 --> 01:00:06,860
быть осторожным с этим, но я почти

1469
01:00:06,860 --> 01:00:09,770
уверен, что это там, так что, как мы  делаем эти

1470
01:00:09,770 --> 01:00:11,570
резервные копии, и мы делаем это приближение,

1471
01:00:11,570 --> 01:00:15,230
что все ваши ошибки ограничены, чтобы

1472
01:00:15,230 --> 01:00:17,780
ничего не взорвалось, и что, когда вы

1473
01:00:17,780 --> 01:00:22,130
делаете это доказательство, я думаю, что мне придется

1474
01:00:22,130 --> 01:00:24,050
перепроверить это, но я думаю, что вы в

1475
01:00:24,050 --> 01:00:25,490
основном используете неравенство держателей и

1476
01:00:25,490 --> 01:00:26,960
затем вы используете тот факт, что W

1477
01:00:26,960 --> 01:00:28,790
ограничено, чтобы гарантировать, что ваше конечное

1478
01:00:28,790 --> 01:00:37,970
значение ограничено, поэтому в целом вы хотите, чтобы

1479
01:00:37,970 --> 01:00:40,250
ваша функция вознаграждения была ограничена,

1480
01:00:40,250 --> 01:00:43,220
и особенно да, даже

1481
01:00:43,220 --> 01:00:45,230
со скидкой, как будто это полезно, вам

1482
01:00:45,230 --> 01:00:46,490
всегда нужно убедиться, что вы

1483
01:00:46,490 --> 01:00:48,080
Оператор посыльного похож на

1484
01:00:48,080 --> 01:00:49,610
сокращение надежды на добро. Я имею в виду, что мы уже

1485
01:00:49,610 --> 01:00:50,690
говорили о том факте, что с

1486
01:00:50,690 --> 01:00:52,580
аппроксимацией линейной функции значения вы не

1487
01:00:52,580 --> 01:00:55,910
всегда сходитесь, но если ваши вознаграждения

1488
01:00:55,910 --> 01:01:02,720
неограничены, это становится хуже, да, пытаясь

1489
01:01:02,720 --> 01:01:04,850
вписать это в  другие вещи,

1490
01:01:04,850 --> 01:01:06,290
с которыми я знаком, это в основном как метод максимальной

1491
01:01:06,290 --> 01:01:08,690
вероятности правильно смотреть на политику,

1492
01:01:08,690 --> 01:01:10,760
например, если мы подбросили монету

1493
01:01:10,760 --> 01:01:13,310
сто раз, и мы получили 99 орлов одна

1494
01:01:13,310 --> 01:01:15,440
решка, возможно, это произошло на

1495
01:01:15,440 --> 01:01:18,920
ярмарке.  вы знаете, что мы не можем

1496
01:01:18,920 --> 01:01:20,720
сбрасывать со счетов это, но это маловероятно, поэтому,

1497
01:01:20,720 --> 01:01:24,050
если мы наблюдаем, как какой-то экспертный агент делает

1498
01:01:24,050 --> 01:01:26,090
одно и то же сотни раз, это

1499
01:01:26,090 --> 01:01:27,680
может исходить из функции переделки на

1500
01:01:27,680 --> 01:01:29,870
нуле везде, но не так вероятно, как

1501
01:01:29,870 --> 01:01:32,990
какая-либо другая функция вознаграждения, это означает

1502
01:01:32,990 --> 01:01:35,120
например, это дает нам какой-то

1503
01:01:35,120 --> 01:01:36,830
способ справиться с тем фактом, что

1504
01:01:36,830 --> 01:01:38,420
вознаграждение может быть нулевым, что у нас есть

1505
01:01:38,420 --> 01:01:41,030
своего рода проблема идентифицируемости,

1506
01:01:41,030 --> 01:01:44,080
это не обрабатывается, что, к сожалению, так что

1507
01:01:44,080 --> 01:01:47,240
это все еще не гарантирует, что мы

1508
01:01:47,240 --> 01:01:49,670
не можем научиться ожиданию  это ноль

1509
01:01:49,670 --> 01:01:50,960
везде, но здесь это говорит о

1510
01:01:50,960 --> 01:01:53,780
том, что вместо того, чтобы думать о попытке

1511
01:01:53,780 --> 01:01:55,760
напрямую изучить функцию вознаграждения, если

1512
01:01:55,760 --> 01:01:59,000
вы соответствуете ожидаемым функциям состояния

1513
01:01:59,000 --> 01:02:01,250
, это еще один способ гарантировать,

1514
01:02:01,250 --> 01:02:02,660
что ваша политика в основном делает то

1515
01:02:02,660 --> 01:02:04,250
же самое для эксперта, поэтому, если у вас есть

1516
01:02:04,250 --> 01:02:06,710
политика, которая в основном выглядит так, как будто

1517
01:02:06,710 --> 01:02:08,359
посещает одни и те же состояния с

1518
01:02:08,359 --> 01:02:09,799
одинаковой частотой, - это то, что делает эксперт,

1519
01:02:09,799 --> 01:02:12,020
тогда вы сопоставили их политику, и вы

1520
01:02:12,020 --> 01:02:13,430
все еще не обязательно  знаете, что

1521
01:02:13,430 --> 01:02:15,740
полученное вами двойное число является точным или является

1522
01:02:15,740 --> 01:02:17,569
хорошей оценкой функции вознаграждения, но,

1523
01:02:17,569 --> 01:02:18,980
возможно, оно вам не нужно, потому что, если вы

1524
01:02:18,980 --> 01:02:20,270
действительно заботитесь только о том, чтобы

1525
01:02:20,270 --> 01:02:23,390
соответствовать политике экспертов, то вы

1526
01:02:23,390 --> 01:02:25,460
сопоставили его, потому что если вы  Если вы

1527
01:02:25,460 --> 01:02:27,140
посещаете все штаты с той

1528
01:02:27,140 --> 01:02:28,700
же частотой, что и у эксперта, у

1529
01:02:28,700 --> 01:02:32,030
вас одинаковые политики, так что это своего

1530
01:02:32,030 --> 01:02:34,130
рода отказ от этого, это говорит о том, что мы все

1531
01:02:34,130 --> 01:02:35,119
еще не знаем, какова реальная функция вознаграждения

1532
01:02:35,119 --> 01:02:36,260
, но это не имеет значения, потому что  мы

1533
01:02:36,260 --> 01:02:47,150
раскрыли вопрос экспертной политики,

1534
01:02:47,150 --> 01:02:48,829
да, поэтому было проделано много работы

1535
01:02:48,829 --> 01:02:50,150
, чтобы сделать это с помощью глубоких нейронных сетей.

1536
01:02:50,150 --> 01:02:51,740
Позже я дам пару указателей на

1537
01:02:51,740 --> 01:02:56,329
другие подходы, хорошо, так что

1538
01:02:56,329 --> 01:02:58,970
это наблюдение привело к алгоритму для

1539
01:02:58,970 --> 01:03:02,660
изучения  политика, которая заключается в том, что вы пытаетесь

1540
01:03:02,660 --> 01:03:04,940
найти какую-то функцию вознаграждения, подобную

1541
01:03:04,940 --> 01:03:08,140
этой, что означает, что вы знаете в W мой выбор W

1542
01:03:08,140 --> 01:03:11,420
, так что учитель выглядит лучше, чем

1543
01:03:11,420 --> 01:03:12,859
все остальное, выглядит лучше, чем

1544
01:03:12,859 --> 01:03:14,859
все другие контроллеры, которые у вас были раньше,

1545
01:03:14,859 --> 01:03:18,940
поэтому это выглядит как  так  rt

1546
01:03:18,940 --> 01:03:22,549
этого W для этого распределения состояния выглядит

1547
01:03:22,549 --> 01:03:24,910
лучше, чем W для всех других

1548
01:03:24,910 --> 01:03:28,880
распределений, а затем вы находите

1549
01:03:28,880 --> 01:03:31,390
оптимальную политику управления для текущего W,

1550
01:03:31,390 --> 01:03:36,710
которая может позволить вам затем получить новую Muse,

1551
01:03:36,710 --> 01:03:38,450
потому что у вас есть ваша модель перехода,

1552
01:03:38,450 --> 01:03:42,710
и вы повторяете это до тех пор, пока  своего

1553
01:03:42,710 --> 01:03:47,690
рода разрыв достаточно мал, сейчас

1554
01:03:47,690 --> 01:03:52,490
это не идеально, если ваша экспертная политика

1555
01:03:52,490 --> 01:03:54,440
неоптимальна, это немного сложно, как их

1556
01:03:54,440 --> 01:03:57,799
комбинировать, я не хочу слишком останавливаться

1557
01:03:57,799 --> 01:03:59,299
на этом конкретном алгоритме, это

1558
01:03:59,299 --> 01:04:01,250
не то, что большинство людей больше использует,

1559
01:04:01,250 --> 01:04:04,460
и люди будут  используйте более глубокие

1560
01:04:04,460 --> 01:04:07,760
подходы к обучению, но я думаю, что ключевые

1561
01:04:07,760 --> 01:04:09,740
вещи, которые нужно понять из этого, - это своего

1562
01:04:09,740 --> 01:04:12,200
рода такой аспект, если вы соответствуете

1563
01:04:12,200 --> 01:04:12,800
состоянию,

1564
01:04:12,800 --> 01:04:15,110
но этого достаточно, чтобы сказать, что

1565
01:04:15,110 --> 01:04:17,570
политики идентичны, это на самом деле

1566
01:04:17,570 --> 01:04:19,610
то, что вы учитесь да, я напоминаю

1567
01:04:19,610 --> 01:04:21,330
вам  имя Брюс Ли

1568
01:04:21,330 --> 01:04:22,860
, есть ли смысл использовать

1569
01:04:22,860 --> 01:04:26,490
нормальную норму по сравнению с любой другой нормальной

1570
01:04:26,490 --> 01:04:29,250
нормой благодарности - ах, хороший вопрос, вопрос

1571
01:04:29,250 --> 01:04:31,500
в том, почему мы используем норму 1 и уравнение 7,

1572
01:04:31,500 --> 01:04:34,320
которые на самом деле  важно, это не

1573
01:04:34,320 --> 01:04:36,300
единственный выбор, но здесь

1574
01:04:36,300 --> 01:04:40,010
говорится, что вы должны сопоставлять все состояния

1575
01:04:40,010 --> 01:04:42,150
, это то, что здесь говорится в норме,

1576
01:04:42,150 --> 01:04:44,100
поэтому вы собираетесь дать им, что вы pi - это

1577
01:04:44,100 --> 01:04:46,230
действительно bean of s я не показываю это

1578
01:04:46,230 --> 01:04:48,660
явно  зависимость здесь, но это от s,

1579
01:04:48,660 --> 01:04:50,640
и поэтому норма 1 говорит о том, что

1580
01:04:50,640 --> 01:04:52,890
когда вы суммируете все эти ошибки,

1581
01:04:52,890 --> 01:04:54,240
она должна быть 1, поэтому вы действительно

1582
01:04:54,240 --> 01:04:56,190
оцениваете ошибку по всему этому,

1583
01:04:56,190 --> 01:04:57,480
вы можете выбрать другие вещи, сделайте это

1584
01:04:57,480 --> 01:05:00,090
изменить норму анализа 1 и норму бесконечности,

1585
01:05:00,090 --> 01:05:01,890
как правило, особенно легко

1586
01:05:01,890 --> 01:05:05,460
рассуждать, когда вы начинаете делать, когда

1587
01:05:05,460 --> 01:05:06,960
вы пытаетесь ограничить ошибку и

1588
01:05:06,960 --> 01:05:18,330
функцию значения, хорошо, так что все еще есть

1589
01:05:18,330 --> 01:05:19,650
эта двусмысленность, о которой мы говорили,

1590
01:05:19,650 --> 01:05:21,450
так что есть такой вид  из бесконечного

1591
01:05:21,450 --> 01:05:22,920
числа различных функций вознаграждения

1592
01:05:22,920 --> 01:05:24,060
алгоритм, о котором мы только что говорили

1593
01:05:24,060 --> 01:05:27,900
, не решает эту проблему, поэтому

1594
01:05:27,900 --> 01:05:31,440
было проделано много работы по имитационному обучению

1595
01:05:31,440 --> 01:05:33,090
в рамках обучения принуждению к годовщине, и

1596
01:05:33,090 --> 01:05:36,150
2 из ключевых статей следующие:

1597
01:05:36,150 --> 01:05:37,560
первая называется максимальной  энтропия

1598
01:05:37,560 --> 01:05:41,400
обратный RL, и идея здесь заключалась в том, чтобы сказать, что

1599
01:05:41,400 --> 01:05:43,520
мы хотим выбрать что-то,

1600
01:05:43,520 --> 01:05:48,150
что имеет максимальную неопределенность, учитывая,

1601
01:05:48,150 --> 01:05:50,700
что все еще учитывает

1602
01:05:50,700 --> 01:05:52,520
ограничения данных, которые у нас есть от нашего эксперта,

1603
01:05:52,520 --> 01:05:54,420
говорящего, что мы действительно не уверены, какова

1604
01:05:54,420 --> 01:05:55,920
функция вознаграждения, которую мы  может быть, не совсем

1605
01:05:55,920 --> 01:05:57,450
уверен, какова политика, давайте попробуем

1606
01:05:57,450 --> 01:05:59,460
выбрать дистрибутивы, которые имеют максимальную

1607
01:05:59,460 --> 01:06:02,160
энтропию, что-то вроде обязательств по аренде,

1608
01:06:02,160 --> 01:06:03,300
есть ли противоположность тому, что мы

1609
01:06:03,300 --> 01:06:05,430
подгоняем, вы как бы удивляетесь, как не подходите

1610
01:06:05,430 --> 01:06:08,460
насколько это возможно, и только убедитесь, что

1611
01:06:08,460 --> 01:06:11,790
вы  совпадают в этих ожидаемых частотах состояний,

1612
01:06:11,790 --> 01:06:16,859
так что оба из них, что

1613
01:06:16,859 --> 01:06:18,210
оба этих метода и многие

1614
01:06:18,210 --> 01:06:20,040
сообщения очень тщательно обдумывают

1615
01:06:20,040 --> 01:06:22,470
ожидаемые частоты состояний, которые вы получаете, и

1616
01:06:22,470 --> 01:06:24,720
сравнивают полученные данные с

1617
01:06:24,720 --> 01:06:26,910
данными, которые у вас есть в демонстрации его

1618
01:06:26,910 --> 01:06:29,280
демонстратора этого типа  методов может

1619
01:06:29,280 --> 01:06:31,260
быть расширен до того места, где модель перехода

1620
01:06:31,260 --> 01:06:33,890
является узлом,

1621
01:06:38,260 --> 01:06:40,550
часто требующим доступа к

1622
01:06:40,550 --> 01:06:42,500
симулятору, часто это означает, что вы можете

1623
01:06:42,500 --> 01:06:44,089
представить себе то, что у нас было раньше, если

1624
01:06:44,089 --> 01:06:45,950
у вас не было доступа к модели перехода,

1625
01:06:45,950 --> 01:06:47,990
но у вас был доступ к действию

1626
01:06:47,990 --> 01:06:49,579
в реальном мире, вы могли просто опробовать

1627
01:06:49,579 --> 01:06:51,290
новые политики, посмотреть, на что

1628
01:06:51,290 --> 01:06:52,730
похоже ваше распределение состояний и как оно соответствует

1629
01:06:52,730 --> 01:06:55,460
вашей экспертной демонстрации, и на самом деле

1630
01:06:55,460 --> 01:06:57,470
это часто то, что  сделано так, что максимальное

1631
01:06:57,470 --> 01:06:59,210
объятие энтропии RL оказало огромное

1632
01:06:59,210 --> 01:07:01,130
влияние, а затем второе,

1633
01:07:01,130 --> 01:07:02,720
и это также примечание, это

1634
01:07:02,720 --> 01:07:04,550
также от группы Zhu Bag Nels, которая является тем

1635
01:07:04,550 --> 01:07:06,109
же человеком, который придумал кинжал, так

1636
01:07:06,109 --> 01:07:07,450
что группы много думали

1637
01:07:07,450 --> 01:07:09,980
и  сделал много хороших вкладов в

1638
01:07:09,980 --> 01:07:12,410
обратное RL, а затем с точки зрения

1639
01:07:12,410 --> 01:07:16,250
расширения этого до своего рода гораздо более широкого

1640
01:07:16,250 --> 01:07:20,780
аппроксиматора функций x 'Стефан Рон,

1641
01:07:20,780 --> 01:07:23,900
который здесь, в Стэнфорде, расширил это,

1642
01:07:23,900 --> 01:07:27,170
используя глубокие нейронные сети, и снова

1643
01:07:27,170 --> 01:07:29,089
соединяется через это сопоставление признаков,

1644
01:07:29,089 --> 01:07:30,950
поэтому идея в этом  В этом случае оба этих

1645
01:07:30,950 --> 01:07:32,750
метода сравниваются с работой типа кинжала

1646
01:07:32,750 --> 01:07:34,190
или предполагают, что у вас есть фиксированный

1647
01:07:34,190 --> 01:07:35,839
набор траекторий в начале, а

1648
01:07:35,839 --> 01:07:37,010
затем вы собираетесь делать больше вещей в

1649
01:07:37,010 --> 01:07:37,520
будущем

1650
01:07:37,520 --> 01:07:40,700
в p  общее генеративно-

1651
01:07:40,700 --> 01:07:43,089
состязательное обратное имитационное обучение

1652
01:07:43,089 --> 01:07:45,710
имеет эти начальные траектории, а затем

1653
01:07:45,710 --> 01:07:46,970
оно позволит агенту выйти

1654
01:07:46,970 --> 01:07:49,040
и собрать больше данных, чтобы он мог собрать

1655
01:07:49,040 --> 01:07:51,560
больше данных, он может вычислить частоты состояний,

1656
01:07:51,560 --> 01:07:54,349
он также может использовать своего рода

1657
01:07:54,349 --> 01:07:56,750
дискриминатор для сравнения одного  одна из

1658
01:07:56,750 --> 01:07:59,599
проблем заключается в том, что вы знаете, что записать

1659
01:07:59,599 --> 01:08:03,050
форму MU может быть сложно, когда у вас

1660
01:08:03,050 --> 01:08:05,030
действительно очень многомерное пространство состояний,

1661
01:08:05,030 --> 01:08:06,680
поэтому записывая, вы знаете, что

1662
01:08:06,680 --> 01:08:10,220
распределение наших изображений сложно, поэтому

1663
01:08:10,220 --> 01:08:12,230
то, что они делают в этом случае, они в

1664
01:08:12,230 --> 01:08:14,000
основном сосредоточены на Lu  Задачи в стиле Джоко, такие как задачи в

1665
01:08:14,000 --> 01:08:15,800
стиле робота, у нас было бы

1666
01:08:15,800 --> 01:08:17,210
много разных суставов, но все еще

1667
01:08:17,210 --> 01:08:18,259
трудно записать, вы знаете, хорошие

1668
01:08:18,259 --> 01:08:19,880
распределения по этому, поэтому

1669
01:08:19,880 --> 01:08:21,319
в этом случае они сосредоточены на том, чтобы думать о

1670
01:08:21,319 --> 01:08:22,939
таких вещах, как дискриминатор, который мог

1671
01:08:22,939 --> 01:08:24,979
бы различать ваши  демонстрации экспертов

1672
01:08:24,979 --> 01:08:26,299
и демонстрация и

1673
01:08:26,299 --> 01:08:27,950
траектории, которые генерируются

1674
01:08:27,950 --> 01:08:30,350
агентом, и поэтому, если вы можете

1675
01:08:30,350 --> 01:08:31,549
отличить их, то вы н

1676
01:08:31,549 --> 01:08:34,370
несоответствие, это хорошее понимание,

1677
01:08:34,370 --> 01:08:36,670
поскольку я говорю, что мы могли бы использовать такого рода

1678
01:08:36,670 --> 01:08:40,040
дискриминаторы Ганс, как вы знаете, в

1679
01:08:40,040 --> 01:08:41,540
функции дискриминатора, чтобы попытаться

1680
01:08:41,540 --> 01:08:43,939
выяснить, как мы количественно определяем, что значит

1681
01:08:43,939 --> 01:08:45,710
иметь такое же распределение состояний и

1682
01:08:45,710 --> 01:08:46,170
действительно высокое

1683
01:08:46,170 --> 01:08:49,830
высокое  -мерные пространства состояний, так что это

1684
01:08:49,830 --> 01:08:52,229
известно как Гейл, и было много

1685
01:08:52,229 --> 01:08:57,540
расширений Гейла, ранее мы

1686
01:08:57,540 --> 01:08:59,220
говорили, что могут быть реальные практические

1687
01:08:59,220 --> 01:09:00,840
преимущества в изучении функции вознаграждения

1688
01:09:00,840 --> 01:09:03,960
в определенных ситуациях, но, похоже,

1689
01:09:03,960 --> 01:09:04,979
вывод заключается в том, что мы можем '  на

1690
01:09:04,979 --> 01:09:06,810
самом деле это не так, это то, что правильный

1691
01:09:06,810 --> 01:09:09,540
вывод здесь, хм, да, поэтому мы

1692
01:09:09,540 --> 01:09:11,040
говорили ранее, что вы утверждаете, что,

1693
01:09:11,040 --> 01:09:12,300
возможно, они работают, мы действительно хотим

1694
01:09:12,300 --> 01:09:14,160
функцию вознаграждения, но, может быть, вы говорите

1695
01:09:14,160 --> 01:09:16,590
нам, что мы не можем этого сделать  Я думаю, что

1696
01:09:16,590 --> 01:09:18,510
в этом случае мы в основном говорили

1697
01:09:18,510 --> 01:09:20,399
о частотной статистике,

1698
01:09:20,399 --> 01:09:22,200
когда мы говорим здесь о статистических

1699
01:09:22,200 --> 01:09:24,029
методах с этой точки зрения,

1700
01:09:24,029 --> 01:09:25,229
часто очень трудно раскрыть слово «

1701
01:09:25,229 --> 01:09:26,819
функция».  часто люди так и

1702
01:09:26,819 --> 01:09:28,950
делают, когда хотят сказать, что понимают

1703
01:09:28,950 --> 01:09:30,990
поведение животных или что-то в этом роде, у

1704
01:09:30,990 --> 01:09:32,729
вас есть априорные данные, вы можете сделать это другим способом:

1705
01:09:32,729 --> 01:09:34,439
у вас есть байесовские априорные

1706
01:09:34,439 --> 01:09:36,510
функции или функции вознаграждения, а затем вы выполняете байесовское

1707
01:09:36,510 --> 01:09:38,130
обновление, чтобы они предоставили данные, которые

1708
01:09:38,130 --> 01:09:39,990
вы видите, что вы пытаетесь усовершенствовать свой апостериор

1709
01:09:39,990 --> 01:09:43,130
по сравнению с другими возможными функциями вознаграждения, чтобы

1710
01:09:43,130 --> 01:09:45,510
он избегал того, что тогда вы могли бы просто не

1711
01:09:45,510 --> 01:09:47,790
иметь своего предыдущего прикрытия, что вознаграждение равно

1712
01:09:47,790 --> 01:09:50,670
нулю, например, везде, и поэтому, если у

1713
01:09:50,670 --> 01:09:52,439
вас есть структурированный априор, это может быть

1714
01:09:52,439 --> 01:09:54,540
одним из способов по-прежнему использовать информацию  чтобы

1715
01:09:54,540 --> 01:09:56,010
попытаться уменьшить вашу неопределенность в отношении

1716
01:09:56,010 --> 01:10:18,060
функций вознаграждения людей, агентов или животных, так

1717
01:10:18,060 --> 01:10:19,110
что вы знаете, каковы

1718
01:10:19,110 --> 01:10:21,060
реалистичные априорные значения для функций вознаграждения, гм

1719
01:10:21,060 --> 01:10:23,490
, это отличный вопрос, я думаю, в основном это

1720
01:10:23,490 --> 01:10:26,550
зависит от области, которую люди используют,

1721
01:10:26,550 --> 01:10:28,110
я думаю, мы  расскажите немного об этом

1722
01:10:28,110 --> 01:10:29,610
для аспектов исследования, и люди

1723
01:10:29,610 --> 01:10:31,680
действительно используют априорные функции вместо функций вознаграждения для

1724
01:10:31,680 --> 01:10:34,020
исследования, а также такие вещи, как выборка Томпсона,

1725
01:10:34,020 --> 01:10:35,610
и требуют, чтобы вы сделали это, если

1726
01:10:35,610 --> 01:10:37,050
хотите.  чтобы быть как можно ближе к

1727
01:10:37,050 --> 01:10:38,910
частоте, и часто люди используют

1728
01:10:38,910 --> 01:10:41,340
распределения Дурсля по

1729
01:10:41,340 --> 01:10:43,320
многочленам или тому подобное, или

1730
01:10:43,320 --> 01:10:45,750
гауссовы и я, и поэтому вы должны использовать

1731
01:10:45,750 --> 01:10:47,390
сопряженные

1732
01:10:47,390 --> 01:10:49,280
экспоненциальные семейства, так что все

1733
01:10:49,280 --> 01:10:50,660
сопряжено, но это не просто

1734
01:10:50,660 --> 01:10:53,240
реалистично, я думаю, в реальных областях

1735
01:10:53,240 --> 01:10:55,370
Преимущество, вероятно, от использования такого рода

1736
01:10:55,370 --> 01:10:57,200
априорных данных будет заключаться в том, чтобы действительно закодировать знания предметной области

1737
01:10:57,200 --> 01:10:59,960
о том, знаете ли вы, чувствительны ли люди к

1738
01:10:59,960 --> 01:11:01,520
запястью, какие вознаграждения

1739
01:11:01,520 --> 01:11:04,190
вы ожидаете быть разумными в этих

1740
01:11:04,190 --> 01:11:05,170
случаях,

1741
01:11:05,170 --> 01:11:08,210
да, когда я думаю, что нужно вернуться к пункту

1742
01:11:08,210 --> 01:11:10,340
два, я думаю, что это  многое зависит

1743
01:11:10,340 --> 01:11:11,390
от того, что вы хотите получить,

1744
01:11:11,390 --> 01:11:12,680
поэтому, если вы действительно хотите просто понять

1745
01:11:12,680 --> 01:11:13,730
функцию вознаграждения, функцию предпочтения,

1746
01:11:13,730 --> 01:11:15,260
тогда нам, возможно, нужно сделать

1747
01:11:15,260 --> 01:11:16,850
что-то байесовское, или нам нужно попытаться

1748
01:11:16,850 --> 01:11:18,320
найти методы, которые могут помочь нам. Я освещаю это, я

1749
01:11:18,320 --> 01:11:19,910
думаю  то, что в конечном итоге говорят многие другие методы,

1750
01:11:19,910 --> 01:11:21,650
может быть, мы заботимся о

1751
01:11:21,650 --> 01:11:23,090
функции вознаграждения, но в основном мы

1752
01:11:23,090 --> 01:11:24,920
заботимся только о получении высокой производительности, поэтому,

1753
01:11:24,920 --> 01:11:26,300
если мы сможем раскрыть политику, которая

1754
01:11:26,300 --> 01:11:27,980
соответствует  g экспертная политика, у нас все в порядке,

1755
01:11:27,980 --> 01:11:29,600
клонирование поведения не было хорошим способом сделать

1756
01:11:29,600 --> 01:11:31,280
это, потому что ошибки накапливаются, но теперь

1757
01:11:31,280 --> 01:11:32,570
есть другие способы, которые могут сделать

1758
01:11:32,570 --> 01:11:35,060
это лучше, и поэтому мы в порядке с этой

1759
01:11:35,060 --> 01:11:36,740
частью, и снова я просто хочу подчеркнуть

1760
01:11:36,740 --> 01:11:38,900
например, Сергей Левин и другие проделали

1761
01:11:38,900 --> 01:11:41,150
работу, которая действительно сочетает в себе, например, вы можете

1762
01:11:41,150 --> 01:11:43,340
взять Гейл, а затем выйти за рамки этого с

1763
01:11:43,340 --> 01:11:45,920
точки зрения ее исследования, чтобы вы могли в конечном

1764
01:11:45,920 --> 01:11:47,240
итоге получить политику, которая лучше, чем ваш

1765
01:11:47,240 --> 01:11:48,770
демонстратор, что я думаю, это хорошо,

1766
01:11:48,770 --> 01:11:50,630
потому что часто нравится, если  ваш демонстратор

1767
01:11:50,630 --> 01:11:53,570
взят с YouTube, что приятно, это

1768
01:11:53,570 --> 01:11:55,010
бесплатное место для

1769
01:11:55,010 --> 01:11:56,570
демонстраций, и вы на самом деле не

1770
01:11:56,570 --> 01:11:58,160
знаете качество, так часто вы можете захотеть

1771
01:11:58,160 --> 01:11:59,660
использовать, что делает их начальное обучение,

1772
01:11:59,660 --> 01:12:03,860
но не обязательно ограничиваться этим, все в

1773
01:12:03,860 --> 01:12:04,190
порядке,

1774
01:12:04,190 --> 01:12:05,810
так что просто подведем итоги  гм, вы знаете, на

1775
01:12:05,810 --> 01:12:07,310
практике было

1776
01:12:07,310 --> 01:12:08,450
проделано огромное количество работы по имитационному обучению,

1777
01:12:08,450 --> 01:12:10,670
особенно в робототехнике, но во многих

1778
01:12:10,670 --> 01:12:13,760
областях, и я думаю, что вы знаете, если

1779
01:12:13,760 --> 01:12:15,110
вы собираетесь оставить класс сегодня и

1780
01:12:15,110 --> 01:12:17,540
пойти в  В промышленности имитационное обучение

1781
01:12:17,540 --> 01:12:20,690
может быть очень полезным на практике, потому что

1782
01:12:20,690 --> 01:12:22,220
часто легче получить демонстрации,

1783
01:12:22,220 --> 01:12:25,450
и оно действительно может ускорить изучение

1784
01:12:25,450 --> 01:12:29,120
сложных игр и т. д., но все

1785
01:12:29,120 --> 01:12:30,350
еще остается много проблем, которые остаются,

1786
01:12:30,350 --> 01:12:33,380
особенно много областей, о которых я

1787
01:12:33,380 --> 01:12:35,090
думаю, мы не  знаю оптимальную

1788
01:12:35,090 --> 01:12:38,720
политику, поэтому я думаю о здравоохранении или о

1789
01:12:38,720 --> 01:12:41,660
клиентах или об образовании, таких как

1790
01:12:41,660 --> 01:12:43,070
интеллектуальные системы обучения и обо всем

1791
01:12:43,070 --> 01:12:44,330
этом, одна из больших проблем, что вы

1792
01:12:44,330 --> 01:12:46,070
не знаете оптимальную политику, и вы,

1793
01:12:46,070 --> 01:12:47,630
возможно, делаете все это, потому что думаете,

1794
01:12:47,630 --> 01:12:49,010
что можете что-то сделать  лучше, чем

1795
01:12:49,010 --> 01:12:51,290
то, что есть в существующих данных, так

1796
01:12:51,290 --> 01:12:53,600
что это большая проблема, и как вы

1797
01:12:53,600 --> 01:12:56,240
комбинируете своего рода обратное RL, а затем,

1798
01:12:56,240 --> 01:12:58,880
возможно, онлайн RL безопасным способом, так что одна

1799
01:12:58,880 --> 01:13:00,480
из мотиваций, которые я назвал для

1800
01:13:00,480 --> 01:13:01,980
ограничения обучения, была ну ладно, если вы

1801
01:13:01,980 --> 01:13:04,739
хотите быть  безопасно, но тогда, если вы

1802
01:13:04,739 --> 01:13:06,180
говорите, что сейчас дела обстоят не очень хорошо,

1803
01:13:06,180 --> 01:13:07,440
тогда вам нужно выяснить, как проводить

1804
01:13:07,440 --> 01:13:12,239
безопасное исследование в будущем, хорошо,

1805
01:13:12,239 --> 01:13:13,710
я думаю, что каждый  на сегодня,

1806
01:13:13,710 --> 01:13:14,850
и увидимся на следующей неделе,

1807
01:13:14,850 --> 01:13:16,140
мы начнем говорить о

1808
01:13:16,140 --> 01:13:18,320
поиске по правилам

