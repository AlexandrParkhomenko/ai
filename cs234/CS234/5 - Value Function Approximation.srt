1
00:00:04,790 --> 00:00:07,935
All right. Good morning, we're gonna go ahead and get started.

2
00:00:07,935 --> 00:00:10,770
Um, homework [NOISE] one is due today,

3
00:00:10,770 --> 00:00:12,680
unless you're using late days, um,

4
00:00:12,680 --> 00:00:14,895
and homework two will be released today.

5
00:00:14,895 --> 00:00:17,490
Homework two is gonna be over, um,

6
00:00:17,490 --> 00:00:20,305
[NOISE] function approximation and reinforcement learning.

7
00:00:20,305 --> 00:00:22,460
Um, we're gonna start to cover that material today,

8
00:00:22,460 --> 00:00:24,540
and then we'll continue next week with deep learning.

9
00:00:24,540 --> 00:00:27,390
[NOISE] Um, deep learning is not a prerequisite for

10
00:00:27,390 --> 00:00:31,200
this class and so we're gonna be releasing a tutorial on TensorFlow,

11
00:00:31,200 --> 00:00:32,460
um, later this week.

12
00:00:32,460 --> 00:00:34,230
[NOISE] Uh, and then next week,

13
00:00:34,230 --> 00:00:36,200
we'll also in sessions [NOISE] have the opportunity to

14
00:00:36,200 --> 00:00:38,450
go into some more of the background to deep learning.

15
00:00:38,450 --> 00:00:40,700
[NOISE] You're not expected to be an expert at it but you

16
00:00:40,700 --> 00:00:43,025
need to know enough of it in order to do the homeworks and,

17
00:00:43,025 --> 00:00:44,480
and do the function approximation.

18
00:00:44,480 --> 00:00:47,755
[NOISE] We will be assuming that you're very familiar with things like,

19
00:00:47,755 --> 00:00:50,820
um, gradient descent, and taking derivatives, and things like that.

20
00:00:50,820 --> 00:00:54,150
Um, TensorFlow and other packages can do that automatically for you,

21
00:00:54,150 --> 00:00:57,590
but you should be familiar with the general [NOISE] process that happens.

22
00:00:57,590 --> 00:00:59,180
Um, before we continue the sim,

23
00:00:59,180 --> 00:01:00,680
may I have any logistic questions.

24
00:01:00,680 --> 00:01:06,330
[NOISE] All right. Let's go ahead and get started.

25
00:01:06,330 --> 00:01:09,530
[NOISE] Um, as you can see I have lost my voice a little bit,

26
00:01:09,530 --> 00:01:12,830
it's coming back but we'll see how we go and if it gets too tricky,

27
00:01:12,830 --> 00:01:13,940
then will take over.

28
00:01:13,940 --> 00:01:20,030
[NOISE] All right, so what we've been talking about so far is thinking about, um,

29
00:01:20,030 --> 00:01:22,370
[NOISE] learning, uh, to be able to evaluate

30
00:01:22,370 --> 00:01:24,710
policies in sequential decision-making cases,

31
00:01:24,710 --> 00:01:26,240
and being able to make decisions.

32
00:01:26,240 --> 00:01:28,655
[NOISE] All of this is when the world is unknown.

33
00:01:28,655 --> 00:01:29,870
And what I mean by that is that,

34
00:01:29,870 --> 00:01:31,355
we're not given in advance,

35
00:01:31,355 --> 00:01:33,560
a dynamics model, or a reward model.

36
00:01:33,560 --> 00:01:35,780
[NOISE] Um, and what we're gonna start to talk

37
00:01:35,780 --> 00:01:38,090
about today is value function approximation.

38
00:01:38,090 --> 00:01:41,040
[NOISE] Um, just so I know actually,

39
00:01:41,040 --> 00:01:43,170
who of you, who of you have seen this before?

40
00:01:43,170 --> 00:01:45,920
Who've seen some form of like value function approximation?

41
00:01:45,920 --> 00:01:49,160
[NOISE] Okay, so, a couple of people, that most people know.

42
00:01:49,160 --> 00:01:53,220
Um, uh, so when I say value function approximation,

43
00:01:53,220 --> 00:01:56,120
what I mean is that so far we've been thinking about domains,

44
00:01:56,120 --> 00:01:59,090
where we tend to have a finite set of states and actions,

45
00:01:59,090 --> 00:02:00,410
and where it is, um,

46
00:02:00,410 --> 00:02:04,730
computationally and memory feasible [NOISE] to just write down a table,

47
00:02:04,730 --> 00:02:06,860
to keep track of what the value is,

48
00:02:06,860 --> 00:02:09,620
of states or the value of state action pairs, [NOISE] um,

49
00:02:09,620 --> 00:02:12,200
or that we could imagine writing data table to write down

50
00:02:12,200 --> 00:02:15,410
the models explicitly of the Reward Model and the dynamics model.

51
00:02:15,410 --> 00:02:19,880
[NOISE] But many real world problems have enormous state and action spaces.

52
00:02:19,880 --> 00:02:23,480
So, if you think about things like the Atari games,

53
00:02:23,480 --> 00:02:25,130
which we can debate about whether or not that's

54
00:02:25,130 --> 00:02:27,650
a real-world problem but it's certainly a challenging problem.

55
00:02:27,650 --> 00:02:32,465
[NOISE] Um, state-space we discussed at the beginning is really sort of a set of pixels.

56
00:02:32,465 --> 00:02:34,730
And so that's gonna be an enormous space and we're not

57
00:02:34,730 --> 00:02:36,920
going to be able to write down that as a table.

58
00:02:36,920 --> 00:02:39,150
[NOISE] And so, in these cases,

59
00:02:39,150 --> 00:02:42,290
we're gonna have to go beyond sort of this tabular representation,

60
00:02:42,290 --> 00:02:44,930
[NOISE] and really think about this issue of generalization.

61
00:02:44,930 --> 00:02:47,780
[NOISE] So, we're going to need to be able to say we

62
00:02:47,780 --> 00:02:50,720
want to be able to make decisions and learn to make good decisions.

63
00:02:50,720 --> 00:02:53,510
We're gonna need to be able to generalize from our prior experience,

64
00:02:53,510 --> 00:02:58,410
so that even if we end up in a state action pair that we've never seen exactly before,

65
00:02:58,410 --> 00:03:01,610
it's like a slightly different set of pixels than we've ever seen before,

66
00:03:01,610 --> 00:03:02,960
that we're still gonna be able to make

67
00:03:02,960 --> 00:03:05,720
good decisions and that's gonna require generalization.

68
00:03:05,720 --> 00:03:09,410
[NOISE] So, um, what we're

69
00:03:09,410 --> 00:03:13,220
gonna talk about today is we're starting with value function approximation,

70
00:03:13,220 --> 00:03:14,960
[NOISE], um, for prediction,

71
00:03:14,960 --> 00:03:16,400
and then talk about control .

72
00:03:16,400 --> 00:03:18,650
[NOISE] Um, and the kind of

73
00:03:18,650 --> 00:03:22,050
the key idea that we're gonna start to talk about in this case is that,

74
00:03:22,050 --> 00:03:25,070
we're gonna be representing the state action value,

75
00:03:25,070 --> 00:03:27,950
uh, value function with a parameterized function.

76
00:03:27,950 --> 00:03:33,230
[NOISE] So, we can think of now as having a function where we input a state,

77
00:03:33,230 --> 00:03:36,260
and instead of looking up in a table to see what its value is,

78
00:03:36,260 --> 00:03:38,510
instead we're gonna have some parameters here.

79
00:03:38,510 --> 00:03:41,090
So, this is, this could be a deep neural network.

80
00:03:41,090 --> 00:03:44,850
[NOISE] This could be,

81
00:03:44,850 --> 00:03:47,340
you know, um, [NOISE] a polynomial.

82
00:03:47,340 --> 00:03:52,460
[NOISE] It can be all sorts of different function approximations but the key here

83
00:03:52,460 --> 00:03:54,980
is that we have some parameters that allow us

84
00:03:54,980 --> 00:03:58,505
to say for any input state, what is the value.

85
00:03:58,505 --> 00:04:00,560
And just like we saw before,

86
00:04:00,560 --> 00:04:02,780
we're gonna both sort of go back and forth between thinking

87
00:04:02,780 --> 00:04:05,120
of there being a state value function,

88
00:04:05,120 --> 00:04:07,100
and a, a state action value function.

89
00:04:07,100 --> 00:04:10,370
[NOISE] Um, and the key thing now is that we have these parameters.

90
00:04:10,370 --> 00:04:13,940
[NOISE] We're mostly gonna be talking about those parameters in terms of w.

91
00:04:13,940 --> 00:04:17,345
[NOISE] So, you can generally think of w as just a vector.

92
00:04:17,345 --> 00:04:20,540
Um, [NOISE] uh, with that vector could

93
00:04:20,540 --> 00:04:23,570
[NOISE] be the parameters of a deep neural network or it could be something much simpler.

94
00:04:23,570 --> 00:04:28,770
[NOISE] So, again, you know,

95
00:04:28,770 --> 00:04:30,560
why do we wanna do this and sort of what are

96
00:04:30,560 --> 00:04:33,335
the forms of approximations we might start to think about?

97
00:04:33,335 --> 00:04:35,720
So, we just don't wanna have explicitly store

98
00:04:35,720 --> 00:04:38,090
learn for every individual state action pair.

99
00:04:38,090 --> 00:04:41,195
[NOISE] So, we don't have to do that in terms of learning the dynamics model,

100
00:04:41,195 --> 00:04:43,640
you don't have to do that in terms of a value function,

101
00:04:43,640 --> 00:04:46,670
or state action value function or even in terms of a policy.

102
00:04:46,670 --> 00:04:48,590
[NOISE] We're gonna need to be able to generalize,

103
00:04:48,590 --> 00:04:51,110
so that we can figure out that, our agents,

104
00:04:51,110 --> 00:04:53,705
our algorithms can figure out good policies for,

105
00:04:53,705 --> 00:04:56,900
um, sort of these enormous state spaces and action spaces.

106
00:04:56,900 --> 00:04:59,240
[NOISE] And so we need these compact representations.

107
00:04:59,240 --> 00:05:04,790
[NOISE] So, once we do this we're gonna get a multiple different benefits.

108
00:05:04,790 --> 00:05:07,280
There would also gonna incur potential problems as well.

109
00:05:07,280 --> 00:05:10,550
So, we're gonna reduce the memory that we need to store all of these things.

110
00:05:10,550 --> 00:05:15,140
We're gonna reduce the computation needed and we might be able to reduce the experience.

111
00:05:15,140 --> 00:05:17,750
[NOISE] And so what I mean by that there is, um,

112
00:05:17,750 --> 00:05:22,490
how much data does our agent need to collect in order to learn to make good decisions.

113
00:05:22,490 --> 00:05:25,880
So, this is really a notion of sort of how much data is needed.

114
00:05:25,880 --> 00:05:35,085
[NOISE] Now, I just wanna highlight here that,

115
00:05:35,085 --> 00:05:37,230
um, you know, there can be really bad,

116
00:05:37,230 --> 00:05:38,990
it would be really bad approximations.

117
00:05:38,990 --> 00:05:42,620
UM, [NOISE] and those can be great in terms of not needing a lot of data,

118
00:05:42,620 --> 00:05:43,990
and not needing a lot of computation,

119
00:05:43,990 --> 00:05:45,530
and not need a lot of memory,

120
00:05:45,530 --> 00:05:48,380
[NOISE] but they may just not allow you to represent very good policies.

121
00:05:48,380 --> 00:05:50,720
[NOISE] Um, so these are,

122
00:05:50,720 --> 00:05:55,265
these choices of representation or defining sort of hypothesis classes.

123
00:05:55,265 --> 00:05:59,900
They're defining spaces over which you couldn't represent policies and value functions,

124
00:05:59,900 --> 00:06:01,115
and so you couldn't,

125
00:06:01,115 --> 00:06:03,590
there's gonna be sort of a bias-variance trade-off here, um,

126
00:06:03,590 --> 00:06:05,690
and add a function approximation trade-off,

127
00:06:05,690 --> 00:06:08,735
in the sense that, if you have a very small representation,

128
00:06:08,735 --> 00:06:11,450
you're not gonna need very much data to learn to fit it,

129
00:06:11,450 --> 00:06:13,250
but then it's also not gonna have

130
00:06:13,250 --> 00:06:17,390
very good capacity in terms of representing complicated value functions or policies.

131
00:06:17,390 --> 00:06:20,210
[NOISE] Um, so, as a simple example,

132
00:06:20,210 --> 00:06:23,450
we could assume that our agent is always in the same state all the time.

133
00:06:23,450 --> 00:06:26,420
You know, all video game frames are always identical,

134
00:06:26,420 --> 00:06:29,215
[NOISE] and that's a really compressed representation,

135
00:06:29,215 --> 00:06:31,460
um, you know, uh, we only have one state,

136
00:06:31,460 --> 00:06:33,380
[NOISE] but it's not gonna allow us to learn to

137
00:06:33,380 --> 00:06:35,360
make different decisions in different parts of the game.

138
00:06:35,360 --> 00:06:37,765
So, it's not gonna allow us to achieve high reward.

139
00:06:37,765 --> 00:06:40,445
So, there's going to generally be a trade-off between

140
00:06:40,445 --> 00:06:43,400
the capacity of the representation we choose,

141
00:06:43,400 --> 00:06:45,890
so sort of the representational capacity

142
00:06:45,890 --> 00:06:53,650
[NOISE] versus all these other things

143
00:06:53,650 --> 00:06:58,270
we would like versus memory, computation, and data.

144
00:06:58,270 --> 00:07:02,420
[NOISE] Others and always,

145
00:07:02,420 --> 00:07:03,945
sometimes one gets lucky and,

146
00:07:03,945 --> 00:07:06,790
and you can choose something that's very, very compact,

147
00:07:06,790 --> 00:07:09,190
[NOISE] and it's still sufficient to represent

148
00:07:09,190 --> 00:07:12,790
the properties you need to represent it in order to make good decisions,

149
00:07:12,790 --> 00:07:15,700
[NOISE] but it's just worth thinking that often there's this explicit trade-off,

150
00:07:15,700 --> 00:07:18,025
and we often don't know in advance what is

151
00:07:18,025 --> 00:07:22,740
a sufficient representational capacity in order to achieve high reward. Yeah?

152
00:07:22,740 --> 00:07:24,580
[NOISE] Is this, um-

153
00:07:24,580 --> 00:07:25,205
What's your name.

154
00:07:25,205 --> 00:07:26,220
Oh, sorry, .

155
00:07:26,220 --> 00:07:26,970


156
00:07:26,970 --> 00:07:29,530
Is this more or less an orthogonal  consideration from

157
00:07:29,530 --> 00:07:33,545
the bias-variance trade-off in inter-functional [NOISE] coordination?

158
00:07:33,545 --> 00:07:35,100
Yeah, and you can think of it as right,

159
00:07:35,100 --> 00:07:36,320
the best question is whether this is

160
00:07:36,320 --> 00:07:38,660
an orthogonal trade-off to sort of bias-variance trade off?

161
00:07:38,660 --> 00:07:40,640
[NOISE] Um, can think of it as related,

162
00:07:40,640 --> 00:07:43,520
i- if you choose a really restricted representational capacity,

163
00:07:43,520 --> 00:07:45,200
you're gonna have, um,

164
00:07:45,200 --> 00:07:49,100
a bias forever because you're just not gonna be able to represent the true function.

165
00:07:49,100 --> 00:07:50,240
[NOISE] Um, so it's,

166
00:07:50,240 --> 00:07:51,800
be- and they all have consuming, uh,

167
00:07:51,800 --> 00:07:54,890
a smaller variance for a long time because it's a smaller representation.

168
00:07:54,890 --> 00:07:57,200
[NOISE] So, it's really didn't shift to, uh, related [NOISE] to that.

169
00:07:57,200 --> 00:07:58,760
If you take a machine learning and then, uh,

170
00:07:58,760 --> 00:08:01,430
talked about things like structural risk minimization,

171
00:08:01,430 --> 00:08:02,855
[NOISE] and thinking about, um,

172
00:08:02,855 --> 00:08:06,170
how you choose your model class capacity versus how much data you have,

173
00:08:06,170 --> 00:08:09,230
in terms of minimizing your tests that are similar to that too.

174
00:08:09,230 --> 00:08:10,520
[NOISE] So, you know,

175
00:08:10,520 --> 00:08:13,159
how do you trade-off in terms of capacity to generalize,

176
00:08:13,159 --> 00:08:17,555
um, [NOISE] versus the expressive power.

177
00:08:17,555 --> 00:08:20,890
All right. So, a natural immediate question that we've started,

178
00:08:20,890 --> 00:08:25,465
I've started alluding to already is what function approximation are we going to use?

179
00:08:25,465 --> 00:08:27,595
Um, there's a huge number of choices.

180
00:08:27,595 --> 00:08:31,675
Um, today we're only gonna start to talk about one particular set.

181
00:08:31,675 --> 00:08:34,390
Um, but there's an enormous number probably

182
00:08:34,390 --> 00:08:37,360
most of the ones you can think of have been tried with reinforcement learning.

183
00:08:37,360 --> 00:08:39,909
So, pretty much anything that you could do in supervised learning.

184
00:08:39,909 --> 00:08:44,199
You could also try as a function approximator for your value function, um,

185
00:08:44,200 --> 00:08:47,770
could be neural networks or deep decision trees or nearest neighbors,

186
00:08:47,770 --> 00:08:50,440
um, wavelet bases, lots of different things.

187
00:08:50,440 --> 00:08:52,990
Um, what we're gonna do in

188
00:08:52,990 --> 00:08:56,080
this class is mostly focused on things that are differentiable.

189
00:08:56,080 --> 00:08:59,455
Um, these are nice for a number of reasons.

190
00:08:59,455 --> 00:09:03,790
Um, but they tend to be a really nice smooth optimization properties.

191
00:09:03,790 --> 00:09:05,695
So, they're easier to optimize for.

192
00:09:05,695 --> 00:09:08,380
That's one of the reasons we're gonna focus on them in this class.

193
00:09:08,380 --> 00:09:10,450
Those are not always the right choice.

194
00:09:10,450 --> 00:09:13,825
Um, uh, can anybody give me example of where,

195
00:09:13,825 --> 00:09:15,805
for those of you that are familiar with decision trees,

196
00:09:15,805 --> 00:09:17,575
where you might want a decision tree

197
00:09:17,575 --> 00:09:21,140
to represent either your value function or your policy?

198
00:09:23,130 --> 00:09:25,945
Yeah.

199
00:09:25,945 --> 00:09:27,550
Yes.

200
00:09:27,550 --> 00:09:29,665
Uh, they tend to be highly interpretable.

201
00:09:29,665 --> 00:09:31,870
You keep them simple [inaudible] us all with trees. All right.

202
00:09:31,870 --> 00:09:36,625
[inaudible] actually understand that could be helpful.

203
00:09:36,625 --> 00:09:39,550
Exactly. So, what he just said is that, um, you know,

204
00:09:39,550 --> 00:09:43,720
depending on where you're-  how you're using this sort of reinforcement learning policy,

205
00:09:43,720 --> 00:09:45,340
this may be interacting directly with people.

206
00:09:45,340 --> 00:09:48,565
So, let's say this is gonna be used as a decision support for doctors.

207
00:09:48,565 --> 00:09:52,570
In those cases, having a deep neural network may not be very effective in terms of

208
00:09:52,570 --> 00:09:54,700
justifying why you want a particular treatment for

209
00:09:54,700 --> 00:09:57,145
a patient but if you use a decision tree,

210
00:09:57,145 --> 00:09:59,950
um, those tend to be highly interpretable.

211
00:09:59,950 --> 00:10:03,280
Um, uh, well, depending on what features you use but

212
00:10:03,280 --> 00:10:06,085
often it's pretty highly interpretable and so that can be really helpful.

213
00:10:06,085 --> 00:10:08,710
So, thinking about what function approximation you

214
00:10:08,710 --> 00:10:11,695
use often depends on how you're gonna use it later on.

215
00:10:11,695 --> 00:10:12,940
Um, there's also been

216
00:10:12,940 --> 00:10:16,660
some really exciting work recently on sort of explainable deep neural networks

217
00:10:16,660 --> 00:10:19,090
where you can fit a deep neural network and then you can

218
00:10:19,090 --> 00:10:21,910
fit a sort of a simpler function approximator on top.

219
00:10:21,910 --> 00:10:23,770
So, you could fit like, first fit

220
00:10:23,770 --> 00:10:26,200
your deep neural network and then try to fit a decision tree to it.

221
00:10:26,200 --> 00:10:28,230
So, you try to get the kind of the best of both worlds.

222
00:10:28,230 --> 00:10:30,390
Super expressive, um, uh,

223
00:10:30,390 --> 00:10:33,690
function approximator and then still get the interpretability later.

224
00:10:33,690 --> 00:10:36,780
Um, but it's worth thinking about sort of the application that you're looking

225
00:10:36,780 --> 00:10:39,900
at because different ones will be more appropriate in different cases.

226
00:10:39,900 --> 00:10:41,320
Um, so, you know,

227
00:10:41,320 --> 00:10:43,690
probably the two most popular classes, um,

228
00:10:43,690 --> 00:10:46,480
these days and in RL in general are, um,

229
00:10:46,480 --> 00:10:50,380
linear value function approximation and deep neuro networks.

230
00:10:50,380 --> 00:10:54,535
Um, and we're gonna start with linear value function approximation for two reasons.

231
00:10:54,535 --> 00:10:56,290
One is that it's been sort of probably

232
00:10:56,290 --> 00:11:00,010
the most well studied function approximators in reinforcement learning but,

233
00:11:00,010 --> 00:11:02,110
um, up to the last few years and second,

234
00:11:02,110 --> 00:11:04,135
is because you can think of deep neural networks

235
00:11:04,135 --> 00:11:06,250
as computing some really complicated set of

236
00:11:06,250 --> 00:11:08,200
features that you're then doing

237
00:11:08,200 --> 00:11:11,410
linear function approximation over at least in a number of cases.

238
00:11:11,410 --> 00:11:16,585
So, it's really provides a nice foundation for the next part anyway. All right.

239
00:11:16,585 --> 00:11:18,520
So, we're gonna do a really quick review of

240
00:11:18,520 --> 00:11:21,685
gradient descent because we're gonna be using a ton over the next few days.

241
00:11:21,685 --> 00:11:24,865
So, let's just think about any sort of general function J,

242
00:11:24,865 --> 00:11:28,645
um, which is a differentiable function of a parameter vector W. So,

243
00:11:28,645 --> 00:11:31,690
you have some vector W, it's gonna be a set of linear week

244
00:11:31,690 --> 00:11:35,230
soon and our goal is to find the parameter,

245
00:11:35,230 --> 00:11:37,960
um, W that minimizes our objective function.

246
00:11:37,960 --> 00:11:41,125
Haven't told you what the objective function is but we'll define it shortly.

247
00:11:41,125 --> 00:11:46,030
Um, so, the gradient of J of W is we're gonna denote that.

248
00:11:46,030 --> 00:11:50,230
It's told to J of W and that's just us taking

249
00:11:50,230 --> 00:11:57,110
the derivative of it with respect to each of the parameters inside of the vector

250
00:12:06,420 --> 00:12:08,785
and so that would be the gradient,

251
00:12:08,785 --> 00:12:13,225
and so a gradient descent way of trying to optimize for a function, uh,

252
00:12:13,225 --> 00:12:17,020
J of W would be to compute the derivative or the gradient of

253
00:12:17,020 --> 00:12:21,565
it and then to move your parameter vector in the direction of the gradient.

254
00:12:21,565 --> 00:12:25,240
So, if your weights and generally we're going

255
00:12:25,240 --> 00:12:28,990
to always assume the weights are vector, um, uh,

256
00:12:28,990 --> 00:12:34,060
we're gonna be equal to your previous value of the weights minus some learning

257
00:12:34,060 --> 00:12:41,960
rate of the derivative of your objective function.

258
00:12:43,950 --> 00:12:47,020
So, we're sort of just we're figuring out the derivative of

259
00:12:47,020 --> 00:12:52,195
our function and then we're gonna take a step size and that and move our,

260
00:12:52,195 --> 00:12:54,115
our parameter weights over a little bit.

261
00:12:54,115 --> 00:12:56,110
Um, and then we're gonna keep going.

262
00:12:56,110 --> 00:12:59,755
So, if we do this enough times,

263
00:12:59,755 --> 00:13:03,500
um, are we guaranteed to find a local optima?

264
00:13:05,340 --> 00:13:08,950
Right. So, [OVERLAPPING] assume it yeah.

265
00:13:08,950 --> 00:13:12,610
So, they could be yeah, there may be some conditions o- on the learning rate.

266
00:13:12,610 --> 00:13:17,095
Um,ah, but yes, if we do this enough we're guaranteed to get to a local optima.

267
00:13:17,095 --> 00:13:21,085
Um, no- notice this is local.

268
00:13:21,085 --> 00:13:25,180
So, we started thinking about this in terms of the polis- uh,

269
00:13:25,180 --> 00:13:26,410
in terms of doing RL,

270
00:13:26,410 --> 00:13:29,440
it's important to think about where are we gonna converge to and if we're

271
00:13:29,440 --> 00:13:32,680
gonna converge and I'll talk more about that throughout class.

272
00:13:32,680 --> 00:13:36,670
So, this is gonna be sort of a local way for us to try to smoothly start changing

273
00:13:36,670 --> 00:13:41,110
our parameter representation at the value function in order to try to get to a better,

274
00:13:41,110 --> 00:13:44,360
um, better approximation of it.

275
00:13:44,760 --> 00:13:48,130
Right. So, let's think

276
00:13:48,130 --> 00:13:50,890
about how this would apply if we're trying to do policy evaluation.

277
00:13:50,890 --> 00:13:53,890
So again, policy evaluation is someone's giving you a policy.

278
00:13:53,890 --> 00:13:57,040
They've given you a mapping of, um,

279
00:13:57,040 --> 00:14:00,685
first date what your action is and this could be,

280
00:14:00,685 --> 00:14:02,500
it could be stochastic.

281
00:14:02,500 --> 00:14:09,985
So, it could be a mapping from states to a probability distribution over actions.

282
00:14:09,985 --> 00:14:12,640
So, but someone's giving you a policy and

283
00:14:12,640 --> 00:14:15,265
what you wanna do is figure out what's the value of that policy.

284
00:14:15,265 --> 00:14:18,970
What's the expected discounted sum of rewards you get by following that policy.

285
00:14:18,970 --> 00:14:21,640
So, let's assume for a second that,um,

286
00:14:21,640 --> 00:14:25,900
we could quer- query a particular state and then an Oracle would just give us the value,

287
00:14:25,900 --> 00:14:27,835
the true value of the policy.

288
00:14:27,835 --> 00:14:29,950
So, I, you know,

289
00:14:29,950 --> 00:14:31,195
I asked you like, you know what's the,

290
00:14:31,195 --> 00:14:34,330
what's the expected discounted sum of returns for starting in

291
00:14:34,330 --> 00:14:36,010
this part of the room and trying to navigate towards

292
00:14:36,010 --> 00:14:37,735
the door under some policy and it says,

293
00:14:37,735 --> 00:14:40,210
okay the expected discounted number of steps it

294
00:14:40,210 --> 00:14:42,985
would take you as on average like 30 for example.

295
00:14:42,985 --> 00:14:46,210
So, um, that would be a way that the Oracle

296
00:14:46,210 --> 00:14:49,990
could return these pairs and so you get sort of this pair of S,

297
00:14:49,990 --> 00:14:54,250
V pie of S and then let's say given that,

298
00:14:54,250 --> 00:14:57,340
we have all this data what we wanna do is we wanna fit a function.

299
00:14:57,340 --> 00:15:01,840
We wanna fit our parameterized function to represent all that data accurately.

300
00:15:01,840 --> 00:15:04,975
So, we wanna find the best representation in our space,

301
00:15:04,975 --> 00:15:09,530
um, of the state value pairs.

302
00:15:10,530 --> 00:15:14,740
So, if you frame this in the context of stochastic gradient descent,

303
00:15:14,740 --> 00:15:18,280
what we're gonna wanna do is just directly try to minimize our loss

304
00:15:18,280 --> 00:15:22,750
between the value that we're predicting and the true value.

305
00:15:22,750 --> 00:15:24,430
So, right now imagine someone's giving us

306
00:15:24,430 --> 00:15:26,680
these true S value pairs and then we

307
00:15:26,680 --> 00:15:29,575
just want to fit a function approximators to fit that data.

308
00:15:29,575 --> 00:15:33,310
So, it's really very similar to just doing sort of supervised learning.

309
00:15:33,310 --> 00:15:35,350
Um, and in general we're going to use

310
00:15:35,350 --> 00:15:37,900
the mean squared loss and we'll return to that later.

311
00:15:37,900 --> 00:15:41,080
So, the mean squared loss in this case is that we're just going to compare

312
00:15:41,080 --> 00:15:44,350
the true value to our approximate value and

313
00:15:44,350 --> 00:15:50,755
our approximate value here is parameterized by a vector of parameters.

314
00:15:50,755 --> 00:15:53,605
Um, and we're just gonna do gradient descent.

315
00:15:53,605 --> 00:15:56,830
So, we're gonna compute the derivative of

316
00:15:56,830 --> 00:16:00,310
our objective function and when we do compute the derivative of that then we're

317
00:16:00,310 --> 00:16:03,250
gonna take a step size and we're gonna do

318
00:16:03,250 --> 00:16:07,165
stochastic gradient descent here which means we're just gonna sample the gradient.

319
00:16:07,165 --> 00:16:13,970
So, what I mean by that is that if we take the derivative of our objective function,

320
00:16:14,130 --> 00:16:17,950
what we would get is we'd get something that looks like this.

321
00:16:17,950 --> 00:16:28,975
[NOISE]

322
00:16:28,975 --> 00:16:31,165
And what we're gonna do is we're going to take,

323
00:16:31,165 --> 00:16:34,900
I'm going to use this as shorthand for updating the weights,

324
00:16:34,900 --> 00:16:38,710
I'm gonna take a small step size in

325
00:16:38,710 --> 00:16:44,030
the direction of this as evaluated for one single point.

326
00:16:48,930 --> 00:16:53,500
So now, there's no expectation and this is just for a single point.

327
00:16:53,500 --> 00:16:58,780
[NOISE] So this is stochastic gradient descent where

328
00:16:58,780 --> 00:17:01,420
we're not trying to compute the average of

329
00:17:01,420 --> 00:17:04,270
this gradient we're going to- we're trying to just sample this gradient,

330
00:17:04,270 --> 00:17:06,040
evaluated at particular states.

331
00:17:06,040 --> 00:17:08,440
And what I've told you right now is that someone's given us

332
00:17:08,440 --> 00:17:11,454
these pairs of states and the true value function.

333
00:17:11,454 --> 00:17:13,269
So you just take one of those pairs,

334
00:17:13,270 --> 00:17:15,444
compute the gradient at that point and then

335
00:17:15,444 --> 00:17:18,384
update your wave function and do that many many times.

336
00:17:18,385 --> 00:17:19,795
And the nice thing is that

337
00:17:19,795 --> 00:17:23,724
the expected stochastic gradient descent is the same as the full gradient update.

338
00:17:23,724 --> 00:17:28,089
Um, so this has nice properties in terms of converging. Yes a name first please.

339
00:17:28,089 --> 00:17:30,280
Um, so just to confirm, uh,

340
00:17:30,280 --> 00:17:33,955
why is the expectation over policy and not over a set of states if you're saying,

341
00:17:33,955 --> 00:17:36,070
if SGD is a single state?

342
00:17:36,070 --> 00:17:40,940
So this is over the distribution of states that you'd encounter onto this policy.

343
00:17:40,940 --> 00:17:43,260
Yeah, the  question was you know wh- why do it

344
00:17:43,260 --> 00:17:45,360
over- what does the expectation mean in this case?

345
00:17:45,360 --> 00:17:47,579
In this case it's the expected distribution

346
00:17:47,579 --> 00:17:49,830
of- of states and values you'd get under this policy.

347
00:17:49,830 --> 00:17:52,165
[NOISE] And that's, uh,

348
00:17:52,165 --> 00:17:53,680
it's an important point, will come up later.

349
00:17:53,680 --> 00:17:56,320
It'll come up again later in terms of sort of what is

350
00:17:56,320 --> 00:18:00,110
the distribution of data that you're going to encounter under a policy.

351
00:18:00,990 --> 00:18:04,360
Of course, you know, in reality we don't actually have access to

352
00:18:04,360 --> 00:18:07,225
an oracle to tell us the true value function for any state.

353
00:18:07,225 --> 00:18:08,590
Um, if we did we'd already know

354
00:18:08,590 --> 00:18:11,245
the true value function and we wouldn't need to do anything else.

355
00:18:11,245 --> 00:18:13,690
Um, so what we're gonna do now is talk about how do we do

356
00:18:13,690 --> 00:18:16,960
model-free function approximation in

357
00:18:16,960 --> 00:18:20,960
order to do prediction evaluation um ah without a model.

358
00:18:21,600 --> 00:18:26,140
Okay. So, if we go back to what we talked

359
00:18:26,140 --> 00:18:29,830
about before we thought about EBV sort of Monte-Carlo style methods or these TD

360
00:18:29,830 --> 00:18:34,270
learning style methods um where we could adaptively learn

361
00:18:34,270 --> 00:18:39,445
online a value function to represent the value of following a particular policy.

362
00:18:39,445 --> 00:18:42,325
Um, and we did this using data.

363
00:18:42,325 --> 00:18:44,470
And we're going to do exactly the same thing now

364
00:18:44,470 --> 00:18:46,630
except for we're gonna have to whenever we're doing

365
00:18:46,630 --> 00:18:51,235
this sort of update step of um do- updating our estimator with new data,

366
00:18:51,235 --> 00:18:53,635
we're also going to have to do function approximation.

367
00:18:53,635 --> 00:18:55,960
So instead of just like um incrementally

368
00:18:55,960 --> 00:18:58,960
updating our table entry about the value of a state,

369
00:18:58,960 --> 00:19:03,230
now we also have to re approximate our function whenever we get new data.

370
00:19:04,230 --> 00:19:07,360
All right. So, when we start doing this we're going to have

371
00:19:07,360 --> 00:19:09,820
to choose a feature vector to represent the state.

372
00:19:09,820 --> 00:19:12,310
Um, let me just ground out what this might mean.

373
00:19:12,310 --> 00:19:15,010
So let's imagine that we're thinking about a robot,

374
00:19:15,010 --> 00:19:16,705
uh, and a robot that,

375
00:19:16,705 --> 00:19:19,390
well robots can have tons of really amazing sensors but let's

376
00:19:19,390 --> 00:19:22,870
imagine that it's old school and it just has a laser range finder.

377
00:19:22,870 --> 00:19:27,220
Um, a lot of laser range finders used to basically be a 180 degrees um,

378
00:19:27,220 --> 00:19:29,724
and so you would get distance to

379
00:19:29,724 --> 00:19:33,790
the first obstacle that you hit along all of this 180 degrees.

380
00:19:33,790 --> 00:19:39,130
So maybe here it's like two feet and this is 1.5 feet,

381
00:19:39,130 --> 00:19:40,840
this is 7 feet.

382
00:19:40,840 --> 00:19:44,170
And this sort of gives you an approximation of what the wall looks

383
00:19:44,170 --> 00:19:48,100
like for example. So here's our robot.

384
00:19:48,100 --> 00:19:50,260
It's giving- it's got a sensor on it which is

385
00:19:50,260 --> 00:19:53,065
the laser range finder and it's telling us the distance towards the walls.

386
00:19:53,065 --> 00:19:56,020
And so what would this feature representation be in this case?

387
00:19:56,020 --> 00:20:01,765
It would just be simply for each of these 180 degrees, what's the distance?

388
00:20:01,765 --> 00:20:04,060
One degree, two degree.

389
00:20:04,060 --> 00:20:09,115
[NOISE] That'll be example of a feature representation.

390
00:20:09,115 --> 00:20:10,795
Now, why?

391
00:20:10,795 --> 00:20:13,030
That sounds like a pretty good of it, maybe slightly

392
00:20:13,030 --> 00:20:15,520
primitive but generally a pretty good feature representation,

393
00:20:15,520 --> 00:20:17,110
um, but what's the problem with that?

394
00:20:17,110 --> 00:20:19,870
Well, probably isn't mark off.

395
00:20:19,870 --> 00:20:25,045
So a lot of buildings have hallways that would say, you know,

396
00:20:25,045 --> 00:20:29,140
on my left and my right there's a wall about two feet away um,

397
00:20:29,140 --> 00:20:31,210
and then there's nothing in front of me at least for

398
00:20:31,210 --> 00:20:33,340
it perhaps out to my laser range finder,

399
00:20:33,340 --> 00:20:35,020
you would say you know out of rage.

400
00:20:35,020 --> 00:20:37,720
And that would be true for many different parts of

401
00:20:37,720 --> 00:20:40,690
the same hallway and it will be true for many different hallways.

402
00:20:40,690 --> 00:20:42,535
And so there'd be a lot of partial aliasing.

403
00:20:42,535 --> 00:20:44,950
So this is a feature representation that probably is not

404
00:20:44,950 --> 00:20:47,335
mark off um, but it might be reasonable.

405
00:20:47,335 --> 00:20:50,230
It might be a reasonable one on which to condition decisions,

406
00:20:50,230 --> 00:20:51,790
maybe if you're in the middle of the hallway and that's

407
00:20:51,790 --> 00:20:54,280
what it looks like you was just wanna go forward.

408
00:20:54,280 --> 00:20:57,610
And that's an example of a type of feature representation.

409
00:20:57,610 --> 00:20:59,950
And again just emphasizes the point that

410
00:20:59,950 --> 00:21:03,760
the choice of the feature representation will end up being really important.

411
00:21:03,760 --> 00:21:07,000
Um, and for those of you who have taken through deep learning classes

412
00:21:07,000 --> 00:21:10,090
you've probably already heard this but it's kinda before deep learning.

413
00:21:10,090 --> 00:21:12,430
There was often amo- a huge amount of work and there's

414
00:21:12,430 --> 00:21:14,920
still a huge amount of work on doing feature engineering to figure

415
00:21:14,920 --> 00:21:16,420
out what's the right way to write down

416
00:21:16,420 --> 00:21:20,035
your state space so that you could make predictions or make decisions.

417
00:21:20,035 --> 00:21:24,130
Now, one of the nice things about deep neural networks is that it kind of pushes back

418
00:21:24,130 --> 00:21:26,200
that feature selection problem so that you can use

419
00:21:26,200 --> 00:21:30,550
really high dimensional sensor input and then do less amount of hand tuning.

420
00:21:30,550 --> 00:21:32,350
So what do I mean by hand tuning?

421
00:21:32,350 --> 00:21:34,150
Well, in this case, you know you could use

422
00:21:34,150 --> 00:21:38,200
the raw features about like how far you are to on each of these

423
00:21:38,200 --> 00:21:40,660
a 180 degrees or you can imagine having

424
00:21:40,660 --> 00:21:44,905
higher level abstract features like trying to understand if there are corners.

425
00:21:44,905 --> 00:21:48,280
So you could already have done some pre-processing on this raw data to

426
00:21:48,280 --> 00:21:52,390
figure out what features you think might be relevant if you're going to make decisions.

427
00:21:52,390 --> 00:21:55,300
And the problem with doing that is that again if you- if you pick

428
00:21:55,300 --> 00:21:58,000
the wrong set you might not be able to make the decisions you want.

429
00:21:58,000 --> 00:21:59,260
Yes, the name first please.

430
00:21:59,260 --> 00:22:02,575
Uh, could you please elaborate why this is not mark off,

431
00:22:02,575 --> 00:22:05,230
um, this [NOISE] ah kind of getting  the 180 degrees.

432
00:22:05,230 --> 00:22:07,570
Is it ?

433
00:22:07,570 --> 00:22:08,035


434
00:22:08,035 --> 00:22:11,620
Yeah. So, the question is can I elaborate why this is not markup?

435
00:22:11,620 --> 00:22:15,820
Um, I, if just have a 180 degrees for a robot,

436
00:22:15,820 --> 00:22:20,170
if you think about something say like a long hallway.

437
00:22:20,170 --> 00:22:22,120
Let's say this is floor one.

438
00:22:22,120 --> 00:22:26,785
This is floor two, like n gates for example.

439
00:22:26,785 --> 00:22:28,930
So if you have your little robot that's walking

440
00:22:28,930 --> 00:22:31,825
along and it's guiding its laser range finder,

441
00:22:31,825 --> 00:22:35,485
to try and tell it to the distance to all of the things,

442
00:22:35,485 --> 00:22:37,750
um, you're not going to be able to distinguish

443
00:22:37,750 --> 00:22:40,060
with that representation whether you're on floor

444
00:22:40,060 --> 00:22:45,490
one or floor two because your immediate sensor readings are gonna look identical.

445
00:22:45,490 --> 00:22:47,380
And in fact you're not even able to tell where you are in

446
00:22:47,380 --> 00:22:49,300
that hallway from this hallway. Yeah?

447
00:22:49,300 --> 00:22:56,170
[inaudible] So, um, can we generalize that ah if we have partial aliasing then,

448
00:22:56,170 --> 00:22:59,095
uh, we say its not Markov?

449
00:22:59,095 --> 00:23:01,540
Great question. [NOISE] ask, can we generalize to say

450
00:23:01,540 --> 00:23:03,700
if we have partial aliasing it's not Markov? Yes.

451
00:23:03,700 --> 00:23:07,900
I mean, you could change the state representation to be mark off by including

452
00:23:07,900 --> 00:23:11,410
the history um and so then each individual observation would

453
00:23:11,410 --> 00:23:15,010
be aliased but the whole state representation would not be but in general yes,

454
00:23:15,010 --> 00:23:17,770
if you have a state representation for which there is,

455
00:23:17,770 --> 00:23:20,245
um, aliasing it's not mark-off.

456
00:23:20,245 --> 00:23:22,360
Might still be that you could could still do pretty well with

457
00:23:22,360 --> 00:23:24,730
that representation or you might not but it's just good to be

458
00:23:24,730 --> 00:23:29,950
aware of in terms of the techniques one has applied. Good questions.

459
00:23:29,950 --> 00:23:33,640
All right. So let's think about doing this with linear value function approximation.

460
00:23:33,640 --> 00:23:37,240
Um, so what do I mean by linear value function approximation?

461
00:23:37,240 --> 00:23:40,300
It means that we're simply going to have a set of weights and we're going

462
00:23:40,300 --> 00:23:44,110
to.product this with um a- a set of features.

463
00:23:44,110 --> 00:23:46,060
[NOISE] So you know maybe it's

464
00:23:46,060 --> 00:23:48,955
my 180 degrees sensor readings

465
00:23:48,955 --> 00:23:52,345
and then I'm just gonna have a weight for each of those 180 features.

466
00:23:52,345 --> 00:23:55,690
Um, and we can either rep- use that to represent ah

467
00:23:55,690 --> 00:23:59,815
a value function or you can do that for a state action value function.

468
00:23:59,815 --> 00:24:02,320
Um, those of you who are already thinking about

469
00:24:02,320 --> 00:24:04,270
state action value functions might notice that there's

470
00:24:04,270 --> 00:24:08,500
at least two ways to do that once you start getting into q just mentioned that briefly.

471
00:24:08,500 --> 00:24:12,940
You could either have a separate weight vector for each action or you could put

472
00:24:12,940 --> 00:24:18,580
the action as sort of an additional um feature essentially, multiple different choices.

473
00:24:18,580 --> 00:24:20,380
You get different forms of sharing.

474
00:24:20,380 --> 00:24:22,630
Okay? But right now we're just thinking

475
00:24:22,630 --> 00:24:26,455
about um er estimating the value of a particular policy.

476
00:24:26,455 --> 00:24:29,815
So we're just going to think about values and we're gonna say that

477
00:24:29,815 --> 00:24:33,880
remember W is a vector and X is a vector.

478
00:24:33,880 --> 00:24:37,360
Now X and S is just going to give us the features of that state.

479
00:24:37,360 --> 00:24:40,030
So it could be like the real state of the world is where the robot

480
00:24:40,030 --> 00:24:43,780
is and the features you get out are those a 180 readings.

481
00:24:43,780 --> 00:24:46,300
So we're again going to focus on mean squared errors,

482
00:24:46,300 --> 00:24:48,730
our objective function is this mean squared error.

483
00:24:48,730 --> 00:24:51,985
The difference between the values we're predicting and the true values.

484
00:24:51,985 --> 00:24:55,120
And this is our weight update which is,

485
00:24:55,120 --> 00:24:59,905
uh we want to update our weight by a learning rate times the derivative of this function.

486
00:24:59,905 --> 00:25:03,580
So what does this look like in the case of linear value function approximation?

487
00:25:03,580 --> 00:25:07,690
[NOISE] So what we're gonna do is we're just gonna take the derivative of J using

488
00:25:07,690 --> 00:25:12,550
the fact that we know that this is actually X times W. Okay?

489
00:25:12,550 --> 00:25:23,365
So, what we're gonna get in this case is W- delta W is equal to 1.5 alpha to P pi of S

490
00:25:23,365 --> 00:25:28,270
minus S W times

491
00:25:28,270 --> 00:25:36,505
X because the derivative of X times W with respect to W is X. Yes.

492
00:25:36,505 --> 00:25:40,435
Is this expected value over all states or for a particular state?

493
00:25:40,435 --> 00:25:42,955
Great question, remind me your name one more time.

494
00:25:42,955 --> 00:25:43,720


495
00:25:43,720 --> 00:25:45,010
Yes. So the question is,

496
00:25:45,010 --> 00:25:48,025
is this is an expected value of all  states or particular state?

497
00:25:48,025 --> 00:25:52,105
When we're doing the update of the W we're going to be evaluating this at one state.

498
00:25:52,105 --> 00:25:56,395
So we're gonna do this per each state, um,

499
00:25:56,395 --> 00:25:59,245
tha- well, we're going to see different algorithms for it but um,

500
00:25:59,245 --> 00:26:01,180
generally we're gonna be doing stochastic gradient descent.

501
00:26:01,180 --> 00:26:02,530
So we're gonna be doing this at each state.

502
00:26:02,530 --> 00:26:05,785
The expected value here you can think about is really over

503
00:26:05,785 --> 00:26:10,735
the state distribution sampled from this policy.

504
00:26:10,735 --> 00:26:15,850
So if you were to execute this policy in your real MDP you would encounter some states.

505
00:26:15,850 --> 00:26:18,190
And if you, um and we'll talk shortly

506
00:26:18,190 --> 00:26:20,710
more about like what that distribution looks like but

507
00:26:20,710 --> 00:26:23,680
that's the- we want to minimize

508
00:26:23,680 --> 00:26:28,210
our error over all- over the state distribution we would encounter under that policy.

509
00:26:28,210 --> 00:26:32,485
They're good questions. Okay. So, if we look at this form, what does this look like?

510
00:26:32,485 --> 00:26:36,340
It looks like we have a step size which we've seen before with TD learning.

511
00:26:36,340 --> 00:26:41,559
And then we have a prediction error which is the difference between the value function,

512
00:26:41,559 --> 00:26:43,960
uh, the true value function and the value function we're predicting

513
00:26:43,960 --> 00:26:46,870
under estimator and then we have a feature value.

514
00:26:46,870 --> 00:26:50,425
So that's one of the nice aspects of linear, uh, uh,

515
00:26:50,425 --> 00:26:54,730
linear value function approximation is that these updates form into this sort of

516
00:26:54,730 --> 00:27:00,370
very natural notion of how far off were you from the true value weighed by the features.

517
00:27:00,370 --> 00:27:03,078
Yeah?

518
00:27:03,078 --> 00:27:07,900
The question about the math  here so that you have the negative [inaudible]

519
00:27:07,900 --> 00:27:16,120
the negative inside V Pi s hat.

520
00:27:16,120 --> 00:27:22,390
So, does the- it should be a negative excess there with the negative [inaudible] outside as well?

521
00:27:22,390 --> 00:27:24,955
We're going to push this into either,

522
00:27:24,955 --> 00:27:28,825
so the question is about just being careful about um the negatives they come out.

523
00:27:28,825 --> 00:27:32,380
Um, yes you could push that negative out into here

524
00:27:32,380 --> 00:27:36,100
in general alpha is a constant so you can flip it and be positive or negative.

525
00:27:36,100 --> 00:27:37,690
Generally, you're going to want your, um,

526
00:27:37,690 --> 00:27:39,955
if you're minimizing this is kinda be, ah,

527
00:27:39,955 --> 00:27:41,950
you're going to be subtracting this from the weights but

528
00:27:41,950 --> 00:27:43,930
you just want to be careful of depending on how you're defining

529
00:27:43,930 --> 00:27:46,630
your alpha to make sure that you're taking

530
00:27:46,630 --> 00:27:51,190
gradient descent- gradient steps in the right direction. Okay.

531
00:27:51,190 --> 00:27:52,780
It's a good question.

532
00:27:52,780 --> 00:27:56,155
Okay, so how would we do this,

533
00:27:56,155 --> 00:27:59,560
remembering again that we don't actually have access to the true value function?

534
00:27:59,560 --> 00:28:00,805
Um, so we don't actually know,

535
00:28:00,805 --> 00:28:02,500
so in this equation, right?

536
00:28:02,500 --> 00:28:04,555
This assumes this is true,

537
00:28:04,555 --> 00:28:09,190
like this is if Oracle has given you the value of a state under that policy,

538
00:28:09,190 --> 00:28:11,005
but of course we don't have access to that.

539
00:28:11,005 --> 00:28:15,370
Um, so what we're gonna do is sort of use the same types of ideas wi- as what we saw,

540
00:28:15,370 --> 00:28:17,710
um, in Tabular learning,

541
00:28:17,710 --> 00:28:20,050
um, now with a value function approximation.

542
00:28:20,050 --> 00:28:23,890
So, the return which is the expected or

543
00:28:23,890 --> 00:28:28,600
the return which is the sum of rewards from timestep t till the end of the episode,

544
00:28:28,600 --> 00:28:30,670
is an unbiased noisy sample of

545
00:28:30,670 --> 00:28:35,530
the true expected return for the current state wherein on time step t. And so,

546
00:28:35,530 --> 00:28:38,995
we can think about doing Monte Carlo value function approximation

547
00:28:38,995 --> 00:28:43,300
as really as if we're doing supervised learning on the set of state returned pairs.

548
00:28:43,300 --> 00:28:44,980
So now, what we're doing here,

549
00:28:44,980 --> 00:28:46,945
is we're substituting in G_t.

550
00:28:46,945 --> 00:28:49,210
It's an estimate of the true value.

551
00:28:49,210 --> 00:28:57,310
[NOISE] So, we don't know what the true value is,

552
00:28:57,310 --> 00:28:58,975
but, uh, we know that the,

553
00:28:58,975 --> 00:29:01,030
the Monte Carlo returned is an unbiased estimator,

554
00:29:01,030 --> 00:29:02,440
so we're gonna substitute that in.

555
00:29:02,440 --> 00:29:06,745
[NOISE] Okay, so what does that mean if we're doing linear value function approximation?

556
00:29:06,745 --> 00:29:08,440
It means inside of our wait update,

557
00:29:08,440 --> 00:29:09,490
we have a G here.

558
00:29:09,490 --> 00:29:12,850
[NOISE] So, we would take the state.

559
00:29:12,850 --> 00:29:15,280
We would take the sum of rewards on that episode.

560
00:29:15,280 --> 00:29:16,960
So again, this can only be applied in

561
00:29:16,960 --> 00:29:20,710
episodic settings just like generally with Monte Carlo,

562
00:29:20,710 --> 00:29:24,475
then we take the derivative and in this case that's just x,

563
00:29:24,475 --> 00:29:26,530
our features because we're using a linear value function

564
00:29:26,530 --> 00:29:28,660
approximation and then on the last line,

565
00:29:28,660 --> 00:29:30,880
I'm just plugging in exactly what our,

566
00:29:30,880 --> 00:29:33,820
um, V hat estimator is.

567
00:29:33,820 --> 00:29:37,720
So, we're comparing our return to our current estimator,

568
00:29:37,720 --> 00:29:40,315
um, and then we're multiplying it by our features.

569
00:29:40,315 --> 00:29:44,320
And as usual, we have the problem that G might be a very noisy estimate of the return.

570
00:29:44,320 --> 00:29:45,430
Yes, the name first, please.

571
00:29:45,430 --> 00:29:45,580


572
00:29:45,580 --> 00:29:49,300
[NOISE] Can we differentiate first time and every time like before?

573
00:29:49,300 --> 00:29:49,810
Sort of.

574
00:29:49,810 --> 00:29:52,840
Do we differentiate first-time and every time visit, uh, like before?

575
00:29:52,840 --> 00:29:54,190
[NOISE] Great question to ask.

576
00:29:54,190 --> 00:29:57,250
Do we, um, distinguish between first-time visit and every time visit?

577
00:29:57,250 --> 00:29:59,320
Yes. The same exact distinctions

578
00:29:59,320 --> 00:30:02,020
apply to Monte Carlo up to, remember that applied before.

579
00:30:02,020 --> 00:30:06,055
[NOISE] So, [NOISE] I'm here,

580
00:30:06,055 --> 00:30:08,185
I'm showing a first-visit variant of it,

581
00:30:08,185 --> 00:30:10,960
but you could also, could also do every visit.

582
00:30:10,960 --> 00:30:19,275
[NOISE] And it would have the same [NOISE] strengths and limitations as before.

583
00:30:19,275 --> 00:30:20,700
Every visit is biased,

584
00:30:20,700 --> 00:30:22,545
asymptotically it's [NOISE] consistent.

585
00:30:22,545 --> 00:30:24,420
Okay, so what does the weights look like?

586
00:30:24,420 --> 00:30:29,550
In this case, we would say weight is equal to the old weights plus

587
00:30:29,550 --> 00:30:38,200
[NOISE] Alpha times G_t of s minus v,

588
00:30:38,480 --> 00:30:44,680
uh, of sw, remembering that this is just x times w for that state,

589
00:30:44,680 --> 00:30:51,910
[NOISE] times x of s. [NOISE] So,

590
00:30:51,910 --> 00:30:54,880
it's very similar to what we saw before for Monte Carlo,

591
00:30:54,880 --> 00:30:58,240
um, uh, approximate Monte Carlo policy evaluation.

592
00:30:58,240 --> 00:31:00,460
[NOISE] Um, what we do is we start off,

593
00:31:00,460 --> 00:31:03,070
in this case now instead o- of having a value function,

594
00:31:03,070 --> 00:31:04,825
we just have a set of weights, um,

595
00:31:04,825 --> 00:31:07,135
which is gonna now be the zero vector to start.

596
00:31:07,135 --> 00:31:08,635
And we sample an episode,

597
00:31:08,635 --> 00:31:12,640
you have to sample all the way to the end of the episode using the policy, [NOISE] um,

598
00:31:12,640 --> 00:31:16,960
and then we step through that episode and if it's the first visit to that state,

599
00:31:16,960 --> 00:31:20,875
then we compute the return from that state till the end of the episode,

600
00:31:20,875 --> 00:31:23,860
and then we update our weights. Yeah?

601
00:31:23,860 --> 00:31:26,590
Um, just to check on  that, [NOISE] are you adding, uh,

602
00:31:26,590 --> 00:31:30,430
the learning rate, uh, because of the mechanism, uh, reward?

603
00:31:30,430 --> 00:31:32,665
[NOISE] Considering that, uh,

604
00:31:32,665 --> 00:31:34,660
question is about, um,

605
00:31:34,660 --> 00:31:36,340
the Alpha where, oh,

606
00:31:36,340 --> 00:31:38,665
in terms of negative versus positive?

607
00:31:38,665 --> 00:31:40,480
Right. Each one [inaudible] gradient.

608
00:31:40,480 --> 00:31:44,800
Yeah. So, in general, this is gonna look like [NOISE] this.

609
00:31:44,800 --> 00:31:47,005
I'm gonna be a little bit loose on those.

610
00:31:47,005 --> 00:31:49,330
Um, Alpha is gonna be a learning rate,

611
00:31:49,330 --> 00:31:50,890
that's, um, a choice.

612
00:31:50,890 --> 00:31:52,225
Generally, we're gonna be, um,

613
00:31:52,225 --> 00:31:55,450
trying to minimize our objective function that we're gonna be reducing our weights,

614
00:31:55,450 --> 00:31:57,880
um, uh, and will need to be able, again,

615
00:31:57,880 --> 00:32:01,360
be a little bit careful about how we pick Alpha over time, um,

616
00:32:01,360 --> 00:32:06,490
and, and this has been evaluated at each of the states that we encounter along the way.

617
00:32:06,490 --> 00:32:07,900
[inaudible]

618
00:32:07,900 --> 00:32:13,590
and just to be, uh,

619
00:32:13,590 --> 00:32:17,415
[NOISE] careful on step six,

620
00:32:17,415 --> 00:32:21,545
read again factor or just adding up of notice now?

621
00:32:21,545 --> 00:32:23,245
Good question. Um, uh,

622
00:32:23,245 --> 00:32:25,005
on step six, um,

623
00:32:25,005 --> 00:32:26,730
uh, was it ?

624
00:32:26,730 --> 00:32:27,135


625
00:32:27,135 --> 00:32:27,775
sorry.

626
00:32:27,775 --> 00:32:30,460
said, um, "Do we need to have a Gamma function?"

627
00:32:30,460 --> 00:32:32,875
Um, it's a good question.

628
00:32:32,875 --> 00:32:34,810
Um, in episodic RL,

629
00:32:34,810 --> 00:32:37,270
you can always get away with Gamma being one.

630
00:32:37,270 --> 00:32:39,250
Um, so if it's an episodic place,

631
00:32:39,250 --> 00:32:40,780
Gamma can always equal one.

632
00:32:40,780 --> 00:32:43,780
It is also fine to include Gamma here.

633
00:32:43,780 --> 00:32:50,725
[NOISE] So here, generally in episodic cases, um,

634
00:32:50,725 --> 00:32:55,375
you will set a Gamma being equal to one because one of the reasons why you set our,

635
00:32:55,375 --> 00:32:57,220
our Gamma to be less than one is to make sure

636
00:32:57,220 --> 00:32:59,650
things are bounded in terms of their value function,

637
00:32:59,650 --> 00:33:01,045
but then the episodic case,

638
00:33:01,045 --> 00:33:03,760
it is always guaranteed to be bounded, um,

639
00:33:03,760 --> 00:33:06,520
but it is also completely fine to include a Gamma here, yeah.

640
00:33:06,520 --> 00:33:10,120
[NOISE] So, I got a couple of questions about same point,

641
00:33:10,120 --> 00:33:12,535
um, about this, this G,

642
00:33:12,535 --> 00:33:15,205
so when we do that, it seems like we'll and, uh,

643
00:33:15,205 --> 00:33:20,800
sam- sampling G's that have reward- rewards over episodes of different lengths,

644
00:33:20,800 --> 00:33:23,440
[NOISE] but, so doesn't that close

645
00:33:23,440 --> 00:33:26,755
their distribution without stationary and more variance?

646
00:33:26,755 --> 00:33:29,560
This question [inaudible] there's a problem with the fact that, um,

647
00:33:29,560 --> 00:33:33,490
the returns you're taking are gonna be sums over different lengths.

648
00:33:33,490 --> 00:33:34,915
[NOISE] It isn't.

649
00:33:34,915 --> 00:33:40,285
Um, so, uh, you're always trying to estimate the value of being in this state,

650
00:33:40,285 --> 00:33:43,210
um, which itself under this policy.

651
00:33:43,210 --> 00:33:45,445
Um, and in episodic case,

652
00:33:45,445 --> 00:33:47,350
you might encounter that state early on in

653
00:33:47,350 --> 00:33:49,885
the trajectory or late in the trajectory, and your,

654
00:33:49,885 --> 00:33:52,045
your value is exactly gonna be

655
00:33:52,045 --> 00:33:55,615
averaged over whether you encountered early or late and one of the returns.

656
00:33:55,615 --> 00:33:58,330
So there's no problem with, um,

657
00:33:58,330 --> 00:34:00,085
we're assuming all of your episodes are bounded,

658
00:34:00,085 --> 00:34:01,435
they have to be finite.

659
00:34:01,435 --> 00:34:03,850
So there has to be with probability,

660
00:34:03,850 --> 00:34:05,455
one, you're episode has to end.

661
00:34:05,455 --> 00:34:07,780
If that is true, then, um, your,

662
00:34:07,780 --> 00:34:09,505
your rewards are always bounded,

663
00:34:09,505 --> 00:34:12,250
and then you can always just average over this and that's fine.

664
00:34:12,250 --> 00:34:13,570
Sometimes you might encounter, um,

665
00:34:13,570 --> 00:34:16,179
a state really early in the trajectory in a lot of rewards,

666
00:34:16,179 --> 00:34:19,569
other times you might encounter at the end and have very few rewards, [NOISE] um,

667
00:34:19,570 --> 00:34:22,210
and the value of interest the expectation over all of them.

668
00:34:22,210 --> 00:34:23,889
[NOISE] Yeah?

669
00:34:23,889 --> 00:34:26,529
[NOISE] um,

670
00:34:26,530 --> 00:34:27,699
on this part of clarification,

671
00:34:27,699 --> 00:34:29,079
so essentially is uplink,

672
00:34:29,080 --> 00:34:33,670
you're updating this little video approximation at the episode.

673
00:34:33,670 --> 00:34:39,130
So, [inaudible]

674
00:34:39,130 --> 00:34:40,989
And not just once as the velocitor is in.

675
00:34:40,989 --> 00:34:44,049
You're not just updating the weight once an episode many times, right?

676
00:34:44,050 --> 00:34:47,710
So, you look at all of the states you encountered in that episode and for each of those,

677
00:34:47,710 --> 00:34:49,360
you update your weight vector.

678
00:34:49,360 --> 00:34:52,900
[NOISE] Which is equivalent of like generating

679
00:34:52,900 --> 00:34:57,970
all the episodes and trying to feed them in a single, in a single-

680
00:34:57,970 --> 00:35:02,560
[inaudible] Well, what if we did this in a batch setting,

681
00:35:02,560 --> 00:35:05,665
so what if you generate it all every data and then afterwards tried to fit it.

682
00:35:05,665 --> 00:35:08,050
So this is an incremental approach to doing that, um,

683
00:35:08,050 --> 00:35:12,160
and now ends up converging to the same thing. Question, yeah?

684
00:35:12,160 --> 00:35:13,390
Um,

685
00:35:13,390 --> 00:35:15,260
[NOISE] I'm just wondering,

686
00:35:15,260 --> 00:35:16,345
do we include Gamma,

687
00:35:16,345 --> 00:35:19,975
should be Gamma our j minus t slowly start discounting,

688
00:35:19,975 --> 00:35:22,990
um, like going forwards.

689
00:35:22,990 --> 00:35:25,240
J minus t, oh, yeah.

690
00:35:25,240 --> 00:35:33,120
Uh-huh. [NOISE] Catch. [NOISE]

691
00:35:33,120 --> 00:35:36,840
Again, you shouldn't need a Gamma in this case.

692
00:35:36,840 --> 00:35:39,930
[NOISE] So, in general in this case there should be

693
00:35:39,930 --> 00:35:43,155
probably knows that there'd be no Gamma from the episodic case.

694
00:35:43,155 --> 00:35:45,870
But it's good to be precise about these things.

695
00:35:45,870 --> 00:35:48,225
Okay. All right.

696
00:35:48,225 --> 00:35:50,190
So, let's think about this for a particular example.

697
00:35:50,190 --> 00:35:53,070
Um, it turns out that when we

698
00:35:53,070 --> 00:35:56,325
start to combine function approximation with making decisions,

699
00:35:56,325 --> 00:35:59,280
um, ah, [NOISE] and doing this sort of incremental update online,

700
00:35:59,280 --> 00:36:00,885
things can start to go bad.

701
00:36:00,885 --> 00:36:02,760
Um, and there's, uh, um, and,

702
00:36:02,760 --> 00:36:05,850
and what I mean by that is that we may not converge and we may not converge to

703
00:36:05,850 --> 00:36:09,195
places that we want to in terms of representing the optimal value function.

704
00:36:09,195 --> 00:36:11,820
So, there's a nice example, um,

705
00:36:11,820 --> 00:36:13,290
when people are really starting to think a lot about

706
00:36:13,290 --> 00:36:17,190
function approximation in the early 1990s, um, uh,

707
00:36:17,190 --> 00:36:21,360
Baird came up with this example where it can illustrate some of the challenges

708
00:36:21,360 --> 00:36:25,965
of doing function approximation when combining it with doing control and decision-making.

709
00:36:25,965 --> 00:36:28,335
So, we're gonna introduce this example now.

710
00:36:28,335 --> 00:36:32,235
We're doing MC policy evaluation and then we'll see it a few times throughout class.

711
00:36:32,235 --> 00:36:33,690
So, what does this example showing?

712
00:36:33,690 --> 00:36:36,015
So, in this example they're going to be two actions.

713
00:36:36,015 --> 00:36:39,030
So, a_1 is gonna be straight lines and those are all

714
00:36:39,030 --> 00:36:42,540
going to deterministically go to what I'm going to call state S seven.

715
00:36:42,540 --> 00:36:45,104
And this is state S1, S2,

716
00:36:45,104 --> 00:36:49,020
S3, S4, S5, S6.

717
00:36:49,020 --> 00:36:51,750
And what you can see inside of the bubbles there

718
00:36:51,750 --> 00:36:54,360
is what its feature value representation is.

719
00:36:54,360 --> 00:36:57,300
So, remember I said that we would have a state and then we could write it down as,

720
00:36:57,300 --> 00:36:58,875
um, a set of features.

721
00:36:58,875 --> 00:37:00,765
So, what does S1 look like?

722
00:37:00,765 --> 00:37:03,330
It looks like two, two,

723
00:37:03,330 --> 00:37:06,765
three, four, five, six, seven.

724
00:37:06,765 --> 00:37:09,270
So, weight one is two,

725
00:37:09,270 --> 00:37:11,145
um, and weight eight is one.

726
00:37:11,145 --> 00:37:12,735
So, what does S2 look like?

727
00:37:12,735 --> 00:37:14,850
S2 looks like zero to one,

728
00:37:14,850 --> 00:37:17,770
two, three, four, five.

729
00:37:17,780 --> 00:37:21,700
S3 looks Like this.

730
00:37:23,090 --> 00:37:29,110
And so on until we get to S7 which looks like this.

731
00:37:29,690 --> 00:37:42,240
Okay.

732
00:37:42,240 --> 00:37:45,360
So, S7 looks a little bit different than the rest of them.

733
00:37:45,360 --> 00:37:48,525
That is the feature representation of those states.

734
00:37:48,525 --> 00:37:52,260
Now notice that it looks pretty similar to a tabular representation.

735
00:37:52,260 --> 00:37:55,380
In fact, there are more features than there are states.

736
00:37:55,380 --> 00:37:58,995
So, there are only seven states here and there are eight features.

737
00:37:58,995 --> 00:38:00,960
That's completely possible, right?

738
00:38:00,960 --> 00:38:02,910
Like your feature representation could be

739
00:38:02,910 --> 00:38:05,460
larger than the number of true states in the world.

740
00:38:05,460 --> 00:38:07,110
So, then we have, um,

741
00:38:07,110 --> 00:38:09,420
action a_1 and action a_1 always takes us

742
00:38:09,420 --> 00:38:12,060
from any state to deterministically to state S7.

743
00:38:12,060 --> 00:38:15,600
And then we have action a_2 which is denoted by dot dot dot.

744
00:38:15,600 --> 00:38:18,455
And what action a_2 does is, um,

745
00:38:18,455 --> 00:38:22,455
with probability one over six,

746
00:38:22,455 --> 00:38:28,260
it takes you to state Si where i is n one to six.

747
00:38:28,260 --> 00:38:32,805
So, basically uniformly spreads you across one of the first six states.

748
00:38:32,805 --> 00:38:34,710
There are only two actions.

749
00:38:34,710 --> 00:38:38,190
Either you deterministlly go to state S7 or if you take

750
00:38:38,190 --> 00:38:42,675
the second action then you go to one of the first six states with equal probability.

751
00:38:42,675 --> 00:38:46,485
And it's a pretty simple control problem because the reward is zero.

752
00:38:46,485 --> 00:38:48,900
Everywhere, for all actions.

753
00:38:48,900 --> 00:38:54,405
So, the value function for this is zero because there's no rewards anywhere.

754
00:38:54,405 --> 00:38:57,690
Um, and yet we can start to run into trouble in some cases.

755
00:38:57,690 --> 00:39:00,615
So, before we get to that part let's first just think about what,

756
00:39:00,615 --> 00:39:03,570
um, like a Monte Carlo update would do.

757
00:39:03,570 --> 00:39:06,300
Um, and, and let's just imagine also that there's

758
00:39:06,300 --> 00:39:08,460
some additional small probability here that

759
00:39:08,460 --> 00:39:11,430
from S7 that we actually go to a terminal state.

760
00:39:11,430 --> 00:39:14,400
So, um, like let's say, you know,

761
00:39:14,400 --> 00:39:17,835
with probability 0.999 we stay in S7 and or like

762
00:39:17,835 --> 00:39:22,395
0.99 we stay in S7 and 0.01 we terminate.

763
00:39:22,395 --> 00:39:25,260
And, uh, this is a slight modification but I'm

764
00:39:25,260 --> 00:39:27,480
doing that just so we can do it for the Monte Carlo case.

765
00:39:27,480 --> 00:39:29,595
So, we can think of episodes ending.

766
00:39:29,595 --> 00:39:32,250
So, if you're in state one through six you can

767
00:39:32,250 --> 00:39:34,950
either go to S7 or you can stay in states one through six.

768
00:39:34,950 --> 00:39:36,450
If you're an S7, um,

769
00:39:36,450 --> 00:39:38,190
you can either go to states one through six.

770
00:39:38,190 --> 00:39:40,980
You can stay in S7 or you can terminate. All Right.

771
00:39:40,980 --> 00:39:43,200
So, what then- what might an episode look like in this case?

772
00:39:43,200 --> 00:39:45,540
So, let's imagine that we are in state S1.

773
00:39:45,540 --> 00:39:47,130
We took action a_1,

774
00:39:47,130 --> 00:39:49,800
that deterministically gets us to state S7.

775
00:39:49,800 --> 00:39:54,600
Actually before I do that, I'll specify we got zero reward. Rewards was zero.

776
00:39:54,600 --> 00:39:56,025
We went to S7.

777
00:39:56,025 --> 00:39:58,380
We took action a_1.

778
00:39:58,380 --> 00:40:00,240
We got zero reward.

779
00:40:00,240 --> 00:40:01,785
We stayed in S7.

780
00:40:01,785 --> 00:40:02,970
We took action a_1.

781
00:40:02,970 --> 00:40:05,620
We got zero reward and then we terminates.

782
00:40:06,230 --> 00:40:10,110
That's our episode. Okay. So, now

783
00:40:10,110 --> 00:40:12,660
we can think about what our Monte Carlo update would be.

784
00:40:12,660 --> 00:40:15,915
So, our Monte Carlo update in this case would

785
00:40:15,915 --> 00:40:20,085
be let's start with state S1 and try to do the Monte Carlo update.

786
00:40:20,085 --> 00:40:24,210
So, for state S1 the return is what?

787
00:40:24,210 --> 00:40:25,710
Zero.

788
00:40:25,710 --> 00:40:28,050
Zero. So, the return is zero.

789
00:40:28,050 --> 00:40:31,815
Um, what is x? Um, I should tell you.

790
00:40:31,815 --> 00:40:36,010
So, let's start with initializing all of our weights to be one.

791
00:40:36,650 --> 00:40:41,490
So, what is our initial estimate of the value function of state S1?

792
00:40:41,490 --> 00:40:46,890
[inaudible]

793
00:40:46,890 --> 00:40:51,885
How many? So, it's all of the weights are one.

794
00:40:51,885 --> 00:40:56,040
The state S1 representation is 200013.

795
00:40:56,040 --> 00:41:00,900
That's right. Okay. So, and that's just equal to our,

796
00:41:00,900 --> 00:41:06,705
ah, X times W. Okay.

797
00:41:06,705 --> 00:41:08,415
So, then what does our update look like and

798
00:41:08,415 --> 00:41:09,990
of course I would have to tell you what alpha is.

799
00:41:09,990 --> 00:41:12,195
So, let's say alpha is equal to 0.5.

800
00:41:12,195 --> 00:41:17,175
So, what our weights are gonna- our change in the weights is gonna be equal to 0.5

801
00:41:17,175 --> 00:41:23,085
times 0 minus 3 times our feature vector for x.

802
00:41:23,085 --> 00:41:26,700
Our feature vector for x is to 20001.

803
00:41:26,700 --> 00:41:32,055
So, that means that we're gonna get simply minus 1.5

804
00:41:32,055 --> 00:41:39,810
times 20001 minus 3 times minus 1.5.

805
00:41:39,810 --> 00:41:42,850
One, two, three, four, five, six.

806
00:41:43,400 --> 00:41:48,180
So, notice this is gonna give us an update for every single weight but

807
00:41:48,180 --> 00:41:50,190
it's only gonna give us an update for the weights that are

808
00:41:50,190 --> 00:41:52,290
non-zero in this particular state,

809
00:41:52,290 --> 00:41:55,365
which is the first weight and weight eight.

810
00:41:55,365 --> 00:41:58,650
And so then if we were to actually get the new weights,

811
00:41:58,650 --> 00:42:01,770
so now we're going to have w is equal to w plus delta

812
00:42:01,770 --> 00:42:06,825
w. Then our new representation would be minus two,

813
00:42:06,825 --> 00:42:08,565
one, two, three, four,

814
00:42:08,565 --> 00:42:11,770
five, six minus 0.5.

815
00:42:13,310 --> 00:42:18,645
So, that would be one update of Monte Carlo for the first state.

816
00:42:18,645 --> 00:42:21,840
Now you would do this for every single state in that episode.

817
00:42:21,840 --> 00:42:23,910
Say, you would then do it for the first time you

818
00:42:23,910 --> 00:42:25,890
see it and the algorithm I've defined before.

819
00:42:25,890 --> 00:42:28,260
So, we'd next to this for state S7 as well,

820
00:42:28,260 --> 00:42:31,875
where the return would also be zero but the value would be something different,

821
00:42:31,875 --> 00:42:33,975
so we would get a different, um,

822
00:42:33,975 --> 00:42:37,620
well actually in this particular case the value is also three.

823
00:42:37,620 --> 00:42:40,455
Um, it depends on if you've already updated your w then your,

824
00:42:40,455 --> 00:42:43,440
your value will already be different. Yeah.

825
00:42:43,440 --> 00:42:47,205
So, we're doing SGD per state not per episode.

826
00:42:47,205 --> 00:42:50,430
questions is are we doing SGD  per episode or state?

827
00:42:50,430 --> 00:42:54,900
We do it per state. Yeah.

828
00:42:54,900 --> 00:42:59,370
In the previous slide where we had before every state- ev- every encounter,

829
00:42:59,370 --> 00:43:00,720
does that mean that-

830
00:43:00,720 --> 00:43:03,900
For every- for every first visit in that episode.

831
00:43:03,900 --> 00:43:06,150
So, yeah. And it's within that specific-

832
00:43:06,150 --> 00:43:09,390
if so then you go to a new episode that would be S7.

833
00:43:09,390 --> 00:43:12,300
question is about through this first visit,

834
00:43:12,300 --> 00:43:16,140
we basically  step along that episode similar to what we did with Monte Carlo before and

835
00:43:16,140 --> 00:43:18,330
the first time we are encountering state in that episode

836
00:43:18,330 --> 00:43:20,865
we update the weights using its return.

837
00:43:20,865 --> 00:43:23,100
And when we do that for every single unique state

838
00:43:23,100 --> 00:43:25,005
and that episode the first time we see it.

839
00:43:25,005 --> 00:43:27,675
And then after all of that we'd get a new episode.

840
00:43:27,675 --> 00:43:30,615
Okay. All right.

841
00:43:30,615 --> 00:43:32,385
So, this is what would happen.

842
00:43:32,385 --> 00:43:35,640
Um, and you can see that the changes can be fairly large

843
00:43:35,640 --> 00:43:38,745
because we're comparing like the full return to our value function.

844
00:43:38,745 --> 00:43:41,085
Um, it depends of course on what our alpha,

845
00:43:41,085 --> 00:43:43,170
alpha is an alpha can change over time.

846
00:43:43,170 --> 00:43:46,470
And generally we'll want alpha to change over time in order to get convergence.

847
00:43:46,470 --> 00:43:49,410
Um, this gives an example of sort of what Monte Carlo update

848
00:43:49,410 --> 00:43:53,080
would look like in this case with linear value function approximator.

849
00:43:53,270 --> 00:43:57,150
Okay. So, a natural question might be,

850
00:43:57,150 --> 00:43:58,965
um, does this do anything reasonable?

851
00:43:58,965 --> 00:44:02,205
Are we guaranteed that this is gonna converge to the right thing?

852
00:44:02,205 --> 00:44:04,230
Um, and what does the right thing mean here?

853
00:44:04,230 --> 00:44:07,560
Um, we're constrained by our linear value function approximator.

854
00:44:07,560 --> 00:44:09,900
So, we're gonna say are we gonna converge to sort of like

855
00:44:09,900 --> 00:44:12,735
the best thing in our linear value function approximator.

856
00:44:12,735 --> 00:44:16,155
Okay. Before we do this let's just talk for a second about,

857
00:44:16,155 --> 00:44:19,600
um, the distribution of states and how that influences the result.

858
00:44:19,600 --> 00:44:21,170
So, if you think back for

859
00:44:21,170 --> 00:44:24,799
maybe the first or second lecture we talked about the relationship between,

860
00:44:24,799 --> 00:44:27,920
um, Markov processes, Markov reward processes,

861
00:44:27,920 --> 00:44:29,585
and Markov decision processes.

862
00:44:29,585 --> 00:44:32,450
And we said that once you define a particular policy,

863
00:44:32,450 --> 00:44:36,335
then your Markov decision process is actually a Markov reward process.

864
00:44:36,335 --> 00:44:38,240
Where you can think of it as, um,

865
00:44:38,240 --> 00:44:42,575
a chain where the next state is determined by your dynamics model,

866
00:44:42,575 --> 00:44:46,030
where you only use the action according to your policy.

867
00:44:46,030 --> 00:44:48,075
So, if you run that,

868
00:44:48,075 --> 00:44:52,125
if you run your sort of Markov chain defined by an MDP with a particular policy,

869
00:44:52,125 --> 00:44:56,100
you will eventually converge to a probability distribution over states.

870
00:44:56,100 --> 00:44:59,940
And that distribution overstates is called the stationary distribution.

871
00:44:59,940 --> 00:45:02,250
It's a probability distribution its sayings are like

872
00:45:02,250 --> 00:45:04,740
what percentage of the time you're going to be in state one,

873
00:45:04,740 --> 00:45:07,005
on average versus state two et cetera.

874
00:45:07,005 --> 00:45:09,660
Has to sum to one because it's a probability distribution.

875
00:45:09,660 --> 00:45:13,590
You always have to be in some state and it satisfies a balanced equation.

876
00:45:13,590 --> 00:45:18,870
So, it says that the probability distribution over states before,

877
00:45:18,870 --> 00:45:24,810
um, I summed- yeah, I guess.

878
00:45:24,810 --> 00:45:27,210
Let me just flip this. I think it's a little bit easier to,

879
00:45:27,210 --> 00:45:29,800
to think about it the other way around.

880
00:45:32,090 --> 00:45:38,775
You've got, um, d of S prime is equal to sum over S sum over a.

881
00:45:38,775 --> 00:45:42,420
We're just doing the sum over a right now so that we can be sure that,

882
00:45:42,420 --> 00:45:46,980
um, we allow ourselves to have stochastic policies.

883
00:45:46,980 --> 00:45:50,100
So, we look at all the actions that we could take under the current state.

884
00:45:50,100 --> 00:45:54,160
And then we look at where we could transition to you on the next state.

885
00:45:56,420 --> 00:45:59,835
So, we're in some distribution over states.

886
00:45:59,835 --> 00:46:02,895
We think of all the actions we could take from each of those states,

887
00:46:02,895 --> 00:46:04,800
where we might transition to.

888
00:46:04,800 --> 00:46:08,610
And then that gives us a new distribution over states S prime.

889
00:46:08,610 --> 00:46:11,490
And those two have to be identical.

890
00:46:11,490 --> 00:46:14,730
So, um, this is often also thought about in terms of

891
00:46:14,730 --> 00:46:18,345
a mixing property when your Markov chain has run for long enough.

892
00:46:18,345 --> 00:46:21,270
Um, this balance equation will eventually hold and this is just that

893
00:46:21,270 --> 00:46:23,820
your distribution over states on the previous time step has to be

894
00:46:23,820 --> 00:46:26,040
exactly the same as your distribution over states on

895
00:46:26,040 --> 00:46:29,895
the next time step after this process is fully mixed.

896
00:46:29,895 --> 00:46:31,860
And it's just telling you on average, you know,

897
00:46:31,860 --> 00:46:33,510
how much time are you spending in,

898
00:46:33,510 --> 00:46:35,460
in, um, what's the probability on

899
00:46:35,460 --> 00:46:37,980
any particular time step you're gonna be in a particular state.

900
00:46:37,980 --> 00:46:41,730
This is not telling us how long it takes for this process to occur.

901
00:46:41,730 --> 00:46:45,345
So, this depends a lot on the underlying dynamics of the system.

902
00:46:45,345 --> 00:46:48,870
So, it might be that this takes millions of steps

903
00:46:48,870 --> 00:46:52,560
until you reach the stationary distribution or it might mix pretty quickly,

904
00:46:52,560 --> 00:46:55,230
it depends on the properties of your transition matrix,

905
00:46:55,230 --> 00:46:56,805
um, under the policy.

906
00:46:56,805 --> 00:46:59,130
I'm not gonna get into any of that in this class.

907
00:46:59,130 --> 00:47:02,625
Um, it's just important to know that you can't- it's not like you can just wait

908
00:47:02,625 --> 00:47:04,080
a 100 steps and definitely you are going to be in

909
00:47:04,080 --> 00:47:06,615
the stationary distribution that depends on the problem. Yeah.

910
00:47:06,615 --> 00:47:08,400
Have there been any proof of-

911
00:47:08,400 --> 00:47:08,814
[inaudible] meaning. Yeah.

912
00:47:08,814 --> 00:47:15,265
Have there been any proof of [inaudible]

913
00:47:15,265 --> 00:47:18,970
Any proven bounds on the mixing time of this type of Monte Carlo methods.

914
00:47:18,970 --> 00:47:20,650
Not that I know of. There might be some.

915
00:47:20,650 --> 00:47:22,990
Um, [NOISE] it's a really tricky issue, often,

916
00:47:22,990 --> 00:47:25,660
because you don't know how long it will take to get to this,

917
00:47:25,660 --> 00:47:27,310
sort of, stationary distribution.

918
00:47:27,310 --> 00:47:29,260
There is a really cool paper that just came out in like,

919
00:47:29,260 --> 00:47:30,790
a month ago at [inaudible] , um,

920
00:47:30,790 --> 00:47:32,680
that talks about how,

921
00:47:32,680 --> 00:47:34,600
when we're thinking about of- policy evaluation,

922
00:47:34,600 --> 00:47:36,280
which we'll talk more about later today.

923
00:47:36,280 --> 00:47:39,580
[NOISE] Um, instead of thinking about, um,

924
00:47:39,580 --> 00:47:42,070
superstep, um, ratios, or whether

925
00:47:42,070 --> 00:47:44,935
you'll be taking a certain action and a certain policy or not.

926
00:47:44,935 --> 00:47:47,050
You can think about these stationary distributions,

927
00:47:47,050 --> 00:47:49,300
and the difference between them, in different policies.

928
00:47:49,300 --> 00:47:51,220
Problem is, you often don't know how long,

929
00:47:51,220 --> 00:47:53,815
and whether your data has got to that stationary distribution.

930
00:47:53,815 --> 00:47:56,875
So, would also be really nice if there were easy test to tell if this was true.

931
00:47:56,875 --> 00:47:58,840
That's also really hard to know. Yeah.

932
00:47:58,840 --> 00:47:59,980
Uh sorry, [inaudible].

933
00:47:59,980 --> 00:48:02,570
And it's.

934
00:48:02,580 --> 00:48:07,030
Yes. [LAUGHTER] Um, [inaudible] Why isn't it [inaudible].

935
00:48:07,030 --> 00:48:09,370
Ah, yes.

936
00:48:09,370 --> 00:48:10,735
So, question is about, uh.

937
00:48:10,735 --> 00:48:12,550
So, um,  I, sort of,

938
00:48:12,550 --> 00:48:13,690
gave a long what

939
00:48:13,690 --> 00:48:15,640
As when you gave a long prelude about saying, like,

940
00:48:15,640 --> 00:48:16,720
that things  might not converge,

941
00:48:16,720 --> 00:48:17,770
but everything looked fine there.

942
00:48:17,770 --> 00:48:20,140
We're gonna go into that bar. Yes, and we're gonna talk about the fact that,

943
00:48:20,140 --> 00:48:23,395
actually, in the on policy setting where we're just doing policy evaluation.

944
00:48:23,395 --> 00:48:27,220
Everything's gonna be fine. It's only when we get into the control case, um,

945
00:48:27,220 --> 00:48:31,615
where we're gonna be using data from one policy to estimate the value of another,

946
00:48:31,615 --> 00:48:34,615
where in this example and many others, things start to go right.

947
00:48:34,615 --> 00:48:36,910
So, we'll use this as a running example, but right now,

948
00:48:36,910 --> 00:48:40,120
there's no reason for you to believe this is pathological.

949
00:48:40,120 --> 00:48:43,315
Okay. So this is the stationary distribution.

950
00:48:43,315 --> 00:48:46,255
And then, the convergence guarantees are related to that.

951
00:48:46,255 --> 00:48:48,070
Okay. So what we're gonna do is to find the mean

952
00:48:48,070 --> 00:48:50,845
squared error of our linear value function approximator,

953
00:48:50,845 --> 00:48:53,290
with respect to the stationary distribution.

954
00:48:53,290 --> 00:48:54,550
Why is this reasonable?

955
00:48:54,550 --> 00:48:56,740
Well, because you probably care more about

956
00:48:56,740 --> 00:48:59,770
your function approximated error in states that you visit a lot.

957
00:48:59,770 --> 00:49:01,420
There's a state that's really, really rare,

958
00:49:01,420 --> 00:49:03,085
probably it's okay to have bigger error there.

959
00:49:03,085 --> 00:49:07,000
You want your overall mean squared error to be defined on that stationary distribution.

960
00:49:07,000 --> 00:49:10,225
[NOISE] So, this is the mean squared,

961
00:49:10,225 --> 00:49:12,835
um, sort of, value prediction error.

962
00:49:12,835 --> 00:49:16,060
Um, and it compares what we predict versus the true value,

963
00:49:16,060 --> 00:49:18,760
um, weighed by this distribution of states.

964
00:49:18,760 --> 00:49:20,170
And what we're assuming for right now,

965
00:49:20,170 --> 00:49:23,380
is that the approximation we're using is a linear value function approximator.

966
00:49:23,380 --> 00:49:25,180
[NOISE] Um, let me just note,

967
00:49:25,180 --> 00:49:26,935
for historical reasons that, um,

968
00:49:26,935 --> 00:49:29,140
John Tsitsiklis and Ben Van Roy.

969
00:49:29,140 --> 00:49:31,150
John is a, um, MIT, ha- um,

970
00:49:31,150 --> 00:49:33,355
I had the pleasure of him teaching me probability, which was great.

971
00:49:33,355 --> 00:49:35,440
And then, um, Ben Van Roy is here.

972
00:49:35,440 --> 00:49:36,730
Um, and was one of,

973
00:49:36,730 --> 00:49:39,085
I think John's, uh, PhD students for postdocs.

974
00:49:39,085 --> 00:49:41,950
Anyway, they, um, in about 1997,

975
00:49:41,950 --> 00:49:44,200
people were getting really interested, in whether or not,

976
00:49:44,200 --> 00:49:46,930
when you combine function approximation with, um,

977
00:49:46,930 --> 00:49:48,640
reinforcement learning, what happened,

978
00:49:48,640 --> 00:49:49,810
and whether things were good or bad.

979
00:49:49,810 --> 00:49:51,220
And- and they're responsible for,

980
00:49:51,220 --> 00:49:53,020
um, this nice analysis.

981
00:49:53,020 --> 00:49:56,670
[NOISE] So, let's assume we have a linear value function approximator.

982
00:49:56,670 --> 00:49:58,245
What you can prove is that,

983
00:49:58,245 --> 00:50:00,615
if you do Monte Carlo policy evaluation,

984
00:50:00,615 --> 00:50:02,610
linear value function approximators,

985
00:50:02,610 --> 00:50:06,690
gonna converge to the wage which have the minimum mean squared error possible.

986
00:50:06,690 --> 00:50:08,515
Just, kind of, best you could hope for.

987
00:50:08,515 --> 00:50:12,415
So, um, this is saying the limit as you have lots and lots of data.

988
00:50:12,415 --> 00:50:15,610
Um, ah then, an- and you run this many, many,

989
00:50:15,610 --> 00:50:17,260
many times, um, then you're, kind of,

990
00:50:17,260 --> 00:50:18,835
just converge to the,

991
00:50:18,835 --> 00:50:20,500
the best weights possible.

992
00:50:20,500 --> 00:50:22,150
Now, this air might not be zero,

993
00:50:22,150 --> 00:50:24,130
because it might be that your value function is not

994
00:50:24,130 --> 00:50:27,115
approximatable with your linears that have weights.

995
00:50:27,115 --> 00:50:29,530
But it's gonna do the best job they can.

996
00:50:29,530 --> 00:50:30,955
It's just gonna find the best.

997
00:50:30,955 --> 00:50:32,950
It's, basically just doing the best linear regression that you

998
00:50:32,950 --> 00:50:34,900
can do, okay, on your, on your data.

999
00:50:34,900 --> 00:50:36,895
[NOISE] So, it's good.

1000
00:50:36,895 --> 00:50:38,905
That's, you know, sort of, a nice sanity check.

1001
00:50:38,905 --> 00:50:41,930
It's gonna converge to the best thing that you could hope to do.

1002
00:50:42,360 --> 00:50:46,375
Um, some people have been asking about, uh, okay, well.

1003
00:50:46,375 --> 00:50:48,280
I've knew me this, sort of, incremental method.

1004
00:50:48,280 --> 00:50:50,050
And maybe, in some cases, that's reasonable.

1005
00:50:50,050 --> 00:50:52,270
And maybe, you're running like a customer recommendation system,

1006
00:50:52,270 --> 00:50:53,500
and you're getting data over time,

1007
00:50:53,500 --> 00:50:55,030
and you're updating this estimator.

1008
00:50:55,030 --> 00:50:58,840
But in some cases, you might have access to just a whole bunch of data from this policy.

1009
00:50:58,840 --> 00:51:00,145
And couldn't you just do that to, kind of,

1010
00:51:00,145 --> 00:51:02,575
more directly. And the answer is, yes.

1011
00:51:02,575 --> 00:51:04,570
So, this is often called Batch,

1012
00:51:04,570 --> 00:51:07,330
uh, Monte Carlo value function approximator.

1013
00:51:07,330 --> 00:51:10,810
And the idea is that you have a whole bunch of episodes from a policy.

1014
00:51:10,810 --> 00:51:12,910
And the nice thing is, now you can just, kind of,

1015
00:51:12,910 --> 00:51:15,190
analytically solve for the best approximator.

1016
00:51:15,190 --> 00:51:20,455
So, again, our, our GI's are gonna be our unbiased sample of the true expected return.

1017
00:51:20,455 --> 00:51:21,940
And what you can do is,

1018
00:51:21,940 --> 00:51:24,530
now, N is just our set of data.

1019
00:51:26,130 --> 00:51:29,245
This is really a linear regression problem.

1020
00:51:29,245 --> 00:51:31,060
We're gonna use our, um,

1021
00:51:31,060 --> 00:51:33,940
unbiased samples as estimates of the true value function.

1022
00:51:33,940 --> 00:51:37,135
We're just gonna find the weights that minimize this mean squared error.

1023
00:51:37,135 --> 00:51:38,770
You take the derivative, you set it to zero.

1024
00:51:38,770 --> 00:51:41,290
It's linear regression. You can solve for this analytically.

1025
00:51:41,290 --> 00:51:44,770
Um, so, a lil- just like how

1026
00:51:44,770 --> 00:51:47,890
we talked about you could do policy evaluation analytically in some cases.

1027
00:51:47,890 --> 00:51:51,730
You can also do it analytically in this case for the linear value function approximator.

1028
00:51:51,730 --> 00:51:54,250
Um, and note again, this is Monte Carlo.

1029
00:51:54,250 --> 00:51:56,110
We're not making any Markov assumptions.

1030
00:51:56,110 --> 00:51:57,885
We're just using the full return.

1031
00:51:57,885 --> 00:52:01,575
So, this is also fine in non-Markov environments.

1032
00:52:01,575 --> 00:52:05,740
Yeah. Can you speak to the  [inaudible] of this approach versus our,

1033
00:52:05,740 --> 00:52:11,350
our [inaudible] that we use [inaudible] policy evaluation.

1034
00:52:11,350 --> 00:52:13,030
Yeah. [inaudible] Okay.

1035
00:52:13,030 --> 00:52:15,820
So whe- wha- when we'd wanna do  this versus the other derivative one.

1036
00:52:15,820 --> 00:52:17,950
This generally has higher computational cost.

1037
00:52:17,950 --> 00:52:20,035
X's can be a very large matrix.

1038
00:52:20,035 --> 00:52:23,185
It may not be possible to even write down X, X. Um,

1039
00:52:23,185 --> 00:52:24,865
is all of your data in the,

1040
00:52:24,865 --> 00:52:26,590
in the future representation form,

1041
00:52:26,590 --> 00:52:28,360
and it requires taking a matrix inverse.

1042
00:52:28,360 --> 00:52:31,315
[NOISE] Um, so that may not be feasible, if you've got, you know,

1043
00:52:31,315 --> 00:52:33,535
huge feature vectors, um, and,

1044
00:52:33,535 --> 00:52:36,100
you know, millions or billions of customers.

1045
00:52:36,100 --> 00:52:38,095
[NOISE] Um, Facebook can't do this,

1046
00:52:38,095 --> 00:52:40,240
um, and do this, er, er, directly.

1047
00:52:40,240 --> 00:52:43,090
Um, and also, you know, if you're doing this,

1048
00:52:43,090 --> 00:52:44,560
you could do this, sort of, incrementally,

1049
00:52:44,560 --> 00:52:46,120
but you're always refitting with all of your data.

1050
00:52:46,120 --> 00:52:47,980
Um, that also could be pretty expensive.

1051
00:52:47,980 --> 00:52:50,470
So most of it it's about memory and computation.

1052
00:52:50,470 --> 00:52:53,305
Um, if you have a really small case, it's probably a good thing to do.

1053
00:52:53,305 --> 00:52:56,290
And it also depends, whether you already have all your data or not. Yeah.

1054
00:52:56,290 --> 00:52:59,020
[NOISE] You could also do some batch as well, right?

1055
00:52:59,020 --> 00:53:01,705
And that could help with convergence and not having your,

1056
00:53:01,705 --> 00:53:04,390
um, radiant estimations fluctuating crazily.

1057
00:53:04,390 --> 00:53:06,430
[inaudible] So, this, of course there's an in-between.

1058
00:53:06,430 --> 00:53:07,960
So, you could do, you don't have to.

1059
00:53:07,960 --> 00:53:09,880
If you have access to quite a bit of data,

1060
00:53:09,880 --> 00:53:12,820
you could either do it completely incrementally or all batch,

1061
00:53:12,820 --> 00:53:14,425
or you could do some batches.

1062
00:53:14,425 --> 00:53:16,360
Um, [NOISE] and there's some nice, uh,

1063
00:53:16,360 --> 00:53:18,310
work by my colleagues.

1064
00:53:18,310 --> 00:53:19,405
And also us showing that in,

1065
00:53:19,405 --> 00:53:20,770
in terms of, um,

1066
00:53:20,770 --> 00:53:22,300
[NOISE] reading into deep learning,

1067
00:53:22,300 --> 00:53:23,920
there can be a lot of benefits to doing, sort of,

1068
00:53:23,920 --> 00:53:26,770
some amount of this analytical aspect over like, you know,

1069
00:53:26,770 --> 00:53:28,675
a sub batch of data [NOISE] because,

1070
00:53:28,675 --> 00:53:30,370
um, you're, sort of,

1071
00:53:30,370 --> 00:53:31,900
particularly when you get into TD learning.

1072
00:53:31,900 --> 00:53:34,675
Or, kind of, proper getting information a lot more quickly than you are,

1073
00:53:34,675 --> 00:53:37,225
um, if you're just doing this, sort of, incremental slow update.

1074
00:53:37,225 --> 00:53:39,745
Because, remember, in TD learning we're also, kind of, only doing like,

1075
00:53:39,745 --> 00:53:43,225
one step of backup compared to kinda propagating all of our information back,

1076
00:53:43,225 --> 00:53:44,500
like we do with Monte Carlo.

1077
00:53:44,500 --> 00:53:47,650
All right.

1078
00:53:47,650 --> 00:53:49,750
So now we're gonna get into temporal difference learning.

1079
00:53:49,750 --> 00:53:51,925
Um, so remember in temporal difference learning,

1080
00:53:51,925 --> 00:53:54,325
we're gonna use both bootstrapping and sampling.

1081
00:53:54,325 --> 00:53:57,610
Monte Carlo only uses sampling to approximate the expectation.

1082
00:53:57,610 --> 00:53:59,830
[NOISE] TD learning also use bootstrapping,

1083
00:53:59,830 --> 00:54:01,855
because we don't have to wait till the end of an episode.

1084
00:54:01,855 --> 00:54:04,195
Um, we just bootstrap and, like,

1085
00:54:04,195 --> 00:54:05,860
combine in our estimated ah,

1086
00:54:05,860 --> 00:54:09,700
expected discounted sum of returns by using our current value function.

1087
00:54:09,700 --> 00:54:11,590
So in this case,

1088
00:54:11,590 --> 00:54:13,960
what we used to do is, we would bootstrap.

1089
00:54:13,960 --> 00:54:16,190
This is the bootstrapping part.

1090
00:54:18,630 --> 00:54:21,310
And our- what we often call our target is

1091
00:54:21,310 --> 00:54:24,160
the reward plus gamma times the value of the next state.

1092
00:54:24,160 --> 00:54:27,100
And I remember the reason this is sampling is, um,

1093
00:54:27,100 --> 00:54:30,550
we're sampling this to approximate our expectation.

1094
00:54:30,550 --> 00:54:32,920
We're not taking the full probability of S prime,

1095
00:54:32,920 --> 00:54:36,040
given as a, and summing over all of S prime.

1096
00:54:36,040 --> 00:54:40,120
So before we did this and we represented everything as a table.

1097
00:54:40,120 --> 00:54:42,820
[NOISE] Now, we wanna not do that anymore.

1098
00:54:42,820 --> 00:54:46,060
Um, so let me just- before we get into this,

1099
00:54:46,060 --> 00:54:48,580
let me just remind us the three forms of like- of the,

1100
00:54:48,580 --> 00:54:50,845
the forms of approximation we're gonna have now.

1101
00:54:50,845 --> 00:54:56,900
Now, we're gonna have a function approximation, bootstrapping and sampling.

1102
00:55:03,420 --> 00:55:05,515
But we're still on policy.

1103
00:55:05,515 --> 00:55:07,420
What do I mean by that right now we're still just doing

1104
00:55:07,420 --> 00:55:09,670
policy evaluation which means we're getting

1105
00:55:09,670 --> 00:55:12,355
data from the policy that we're trying to estimate its value.

1106
00:55:12,355 --> 00:55:15,250
It turns out things are just way easier in that case when you're

1107
00:55:15,250 --> 00:55:18,400
on policy and perhaps they should be somewhat intuitive.

1108
00:55:18,400 --> 00:55:21,010
It's quite similar to the supervised learning case.

1109
00:55:21,010 --> 00:55:23,320
Supervised learning, you're generally assuming your data is

1110
00:55:23,320 --> 00:55:26,920
all IID or data is a little bit more complicated than that.

1111
00:55:26,920 --> 00:55:32,110
But our data's closer to that in this case because we have a single policy.

1112
00:55:32,110 --> 00:55:33,970
It's not sort of this non-stationary aspect

1113
00:55:33,970 --> 00:55:35,995
that comes up when we start changing the policy.

1114
00:55:35,995 --> 00:55:38,740
So, right now we have these three forms of approximation.

1115
00:55:38,740 --> 00:55:42,085
Function approximation, bootstrapping sampling but we're still on policy,

1116
00:55:42,085 --> 00:55:45,355
mostly things are still going to be okay in terms of convergence.

1117
00:55:45,355 --> 00:55:47,290
So, what does that look like?

1118
00:55:47,290 --> 00:55:51,970
We're again going to sort of think about doing the equivalent of supervised learning.

1119
00:55:51,970 --> 00:55:54,940
We'd like to just have our states and the Oracle tells us what

1120
00:55:54,940 --> 00:55:58,240
the value is and fit our function instead of having the oracle,

1121
00:55:58,240 --> 00:56:00,460
we're going to use RTD estimates.

1122
00:56:00,460 --> 00:56:05,995
So, we're going to use our word plus Gamma times our approximate value of the next state.

1123
00:56:05,995 --> 00:56:10,480
And that's going to form our estimate of what the true value is.

1124
00:56:10,480 --> 00:56:12,670
Okay. And then we're going to find

1125
00:56:12,670 --> 00:56:15,890
the weights to minimize the mean squared error in that setting.

1126
00:56:16,950 --> 00:56:19,765
So, if we do that,

1127
00:56:19,765 --> 00:56:23,620
what we can see is that if we're doing this with the linear case,

1128
00:56:23,620 --> 00:56:28,250
we write this out it's just this is the TD target.

1129
00:56:29,430 --> 00:56:31,570
Just as a quick side note,

1130
00:56:31,570 --> 00:56:33,445
I'm gonna use the words TD zero a lot.

1131
00:56:33,445 --> 00:56:36,595
We haven't talked about it in this class but there's actually a whole bunch of

1132
00:56:36,595 --> 00:56:41,575
different slight variance of TD which often called TD Gamma.

1133
00:56:41,575 --> 00:56:44,260
And so if you're reading the book that might be a little bit confusing and so I

1134
00:56:44,260 --> 00:56:46,720
just want to be clear that we're doing the TD zero variant,

1135
00:56:46,720 --> 00:56:50,050
which is probably the most popular and there's a lot of other extensions.

1136
00:56:50,050 --> 00:56:53,080
For simplicity, we're just going to focus on TD zero for now.

1137
00:56:53,080 --> 00:56:55,880
So, this is the TD target.

1138
00:56:55,980 --> 00:57:00,145
This is our current estimate and then we take the derivative.

1139
00:57:00,145 --> 00:57:03,235
In this case that means that we're going to end up plugging in

1140
00:57:03,235 --> 00:57:06,910
our linear value function approximator for both our current state,

1141
00:57:06,910 --> 00:57:12,340
the next state and looking at that difference weighed by the feature vector.

1142
00:57:12,340 --> 00:57:16,120
So, it should look almost identical to the Monte-Carlo update

1143
00:57:16,120 --> 00:57:19,285
except for the fact that now we're bootstrapping.

1144
00:57:19,285 --> 00:57:23,500
So, instead of this being G versus being

1145
00:57:23,500 --> 00:57:28,390
the G return of that we saw before for a particular episode,

1146
00:57:28,390 --> 00:57:32,410
now we're bootstrapping and we're getting the immediate reward plus the estimate of

1147
00:57:32,410 --> 00:57:34,300
the discounted sum of rewards which we're using

1148
00:57:34,300 --> 00:57:37,490
our value function approximator to estimate.

1149
00:57:38,280 --> 00:57:42,310
So, this is what the TD learning

1150
00:57:42,310 --> 00:57:46,030
linear value function approximation for policy evaluation algorithm looks like.

1151
00:57:46,030 --> 00:57:48,100
And again we're gonna initialize our weight vector.

1152
00:57:48,100 --> 00:57:51,190
We're gonna sample a tuple and then we're gonna update our weights.

1153
00:57:51,190 --> 00:57:53,200
So, we get to now update our weights after

1154
00:57:53,200 --> 00:57:56,215
every single tuple just like what we saw for TD learning.

1155
00:57:56,215 --> 00:58:00,070
And what we can see here is that we're just plugging

1156
00:58:00,070 --> 00:58:05,450
in our particular estimator minus old estimator times X.

1157
00:58:08,100 --> 00:58:11,515
So, let's see what this looks like on the Baird example.

1158
00:58:11,515 --> 00:58:15,250
So, again we have the same state feature representation as before.

1159
00:58:15,250 --> 00:58:18,320
State one is 200001.

1160
00:58:18,870 --> 00:58:21,775
We still have zero words for everywhere.

1161
00:58:21,775 --> 00:58:24,145
Let's set our alpha equal to 0.5.

1162
00:58:24,145 --> 00:58:27,160
Now we're in the case through or we can say that there is

1163
00:58:27,160 --> 00:58:32,440
no terminal state because TV learning can handle just continuous online learning.

1164
00:58:32,440 --> 00:58:38,455
So, we're just gonna assume that S7 always stays S7 under action A one.

1165
00:58:38,455 --> 00:58:44,395
So, A one is the solid line and A two is the dashed.

1166
00:58:44,395 --> 00:58:48,640
And we're gonna initialize our weights to 1111.

1167
00:58:48,640 --> 00:58:50,620
And then let's look at this tuple.

1168
00:58:50,620 --> 00:58:54,670
So, just like the first tuple we saw before let's imagine we're in state one.

1169
00:58:54,670 --> 00:58:58,375
We took action A one, we got a reward of zero when we went to state S seven.

1170
00:58:58,375 --> 00:59:01,150
So, why don't we take a minute and you

1171
00:59:01,150 --> 00:59:03,790
calculate what the new weights would be after we do

1172
00:59:03,790 --> 00:59:05,710
that update and maybe compare back to

1173
00:59:05,710 --> 00:59:10,220
the Monte Carlo case in terms of how much they have changed.

1174
00:59:10,860 --> 00:59:13,225
And feel free to talk to a neighbor.

1175
00:59:13,225 --> 00:59:35,870
Let's make this a little bigger so it's easy to remember what S seven is.

1176
01:00:32,250 --> 01:00:35,245
All right. Have they moved a lot, a little?

1177
01:00:35,245 --> 01:00:39,860
How much of the weight changed compared to what we saw with TD with Monte Carlo?

1178
01:00:42,780 --> 01:00:46,660
Seen some people indicates smaller. Yes, that's right.

1179
01:00:46,660 --> 01:00:51,505
Okay. So, the- the- value- the initial value of the states this is gonna

1180
01:00:51,505 --> 01:00:56,755
be so for X of S one times W it's still gonna be three.

1181
01:00:56,755 --> 01:01:00,055
For X S prime S prime is seven.

1182
01:01:00,055 --> 01:01:04,040
We look up what that is. This is also going to be three.

1183
01:01:04,230 --> 01:01:06,250
But now what we're gonna have,

1184
01:01:06,250 --> 01:01:09,700
is we're gonna have Delta W is equal to Alpha times

1185
01:01:09,700 --> 01:01:14,425
zero plus 0.9 times three minus three.

1186
01:01:14,425 --> 01:01:20,960
So, that's gonna be equal to Alpha times minus 0.3.

1187
01:01:21,120 --> 01:01:26,590
So, remember before it was actually minus three so it's a much bigger update.

1188
01:01:26,590 --> 01:01:30,520
And so then when we add that into our- our new weights,

1189
01:01:30,520 --> 01:01:34,795
we're gonna move our weights but we're gonna move them much smaller than we saw before.

1190
01:01:34,795 --> 01:01:38,230
And this shouldn't be too surprising that sort of consistent with what

1191
01:01:38,230 --> 01:01:41,695
we saw for Monte Carlo updating and TD learning.

1192
01:01:41,695 --> 01:01:44,395
TD learning is only updating these sort of

1193
01:01:44,395 --> 01:01:48,310
smaller local changes like one state action or word next highest state.

1194
01:01:48,310 --> 01:01:55,030
The- the Monte Carlo is saying this is an episode full episodic return.

1195
01:01:55,030 --> 01:01:56,125
It's not bootstrapping.

1196
01:01:56,125 --> 01:01:59,260
So, it's a no really the return from starting in state.

1197
01:01:59,260 --> 01:02:02,800
S one is zero. So, we're gonna move it a lot more here.

1198
01:02:02,800 --> 01:02:07,900
It's saying, okay, I'm going to pretend that the return from status one is 2.7,

1199
01:02:07,900 --> 01:02:10,690
which is close to three, it's not zero.

1200
01:02:10,690 --> 01:02:13,450
So, when we move our out weights over here,

1201
01:02:13,450 --> 01:02:16,765
the- the difference is gonna be much smaller than what we saw for Monte Carlo,

1202
01:02:16,765 --> 01:02:20,065
which is similar to what we saw without function approximator.

1203
01:02:20,065 --> 01:02:25,540
All right. Whatever theoretical properties in this case, pretty good.

1204
01:02:25,540 --> 01:02:27,850
So, these are also, uh,

1205
01:02:27,850 --> 01:02:30,100
if you look at TD zero,

1206
01:02:30,100 --> 01:02:33,070
you're gonna converge the weights which aren't necessarily quite as good

1207
01:02:33,070 --> 01:02:36,385
as Monte Carlo but they're within a constant factor.

1208
01:02:36,385 --> 01:02:42,140
So, they're going to be one over one minus Gamma of the minimum possible.

1209
01:02:42,390 --> 01:02:47,720
So, they're not quite as good as Monte Carlo but they're pretty good.

1210
01:02:47,850 --> 01:02:50,200
And depending on your, uh,

1211
01:02:50,200 --> 01:02:53,844
discount factor and the function approximator possible,

1212
01:02:53,844 --> 01:02:55,930
this varies in terms of benefits.

1213
01:02:55,930 --> 01:02:58,225
So, just to check our understanding for a second,

1214
01:02:58,225 --> 01:02:59,890
I've put up both of these results.

1215
01:02:59,890 --> 01:03:02,650
So, one says the Monte Carlo policy evaluator

1216
01:03:02,650 --> 01:03:05,710
converges to the minimum mean squared error one possible under

1217
01:03:05,710 --> 01:03:08,740
your linear value function approximators and TD zero

1218
01:03:08,740 --> 01:03:13,045
converges to within one over one minus Gamma of this minimum error.

1219
01:03:13,045 --> 01:03:15,910
So, again what is this minimum error that says if you could

1220
01:03:15,910 --> 01:03:18,955
pick any linear value function approximator,

1221
01:03:18,955 --> 01:03:24,235
uh, how good could that be at representing your true value of your policy?

1222
01:03:24,235 --> 01:03:28,930
So, let's take just another minute and this is a good one to talk to a neighbor about.

1223
01:03:28,930 --> 01:03:32,455
If the value function approximator is the tabular representation,

1224
01:03:32,455 --> 01:03:37,730
what is the MSVE for both Monte Carlo and TD?

1225
01:03:45,240 --> 01:03:49,075
We guaranteed to converge to the optimal solution,

1226
01:03:49,075 --> 01:03:54,410
optimal value for the true value for V of pi or not

1227
01:04:06,450 --> 01:04:09,670
and if it's not clear what the question is, feel free to ask.

1228
01:04:09,670 --> 01:04:15,160
[NOISE]

1229
01:04:15,160 --> 01:04:17,920
So when you, when you say it's a tabular representation,

1230
01:04:17,920 --> 01:04:22,690
do you mean that you are reducing the repre- representational capacity of the system?

1231
01:04:22,690 --> 01:04:26,410
Last week's session is, if I say it's tabular representation; what do I mean by that?

1232
01:04:26,410 --> 01:04:28,450
I mean that there is one feature for each state,

1233
01:04:28,450 --> 01:04:29,860
it's like a one-hot encoding.

1234
01:04:29,860 --> 01:04:32,980
So it's like the same representations we saw for the first few lectures,

1235
01:04:32,980 --> 01:04:36,910
where like, for each state you have a table lookup for that value of that state.

1236
01:04:36,910 --> 01:04:37,240
[NOISE]

1237
01:04:37,240 --> 01:04:37,640
Yeah?

1238
01:04:37,640 --> 01:04:39,640
Can you please explain-

1239
01:04:39,640 --> 01:04:40,840
And your name?

1240
01:04:40,840 --> 01:04:42,040


1241
01:04:42,040 --> 01:04:46,840
Can you please explain what the barrel mains into?

1242
01:04:46,840 --> 01:04:48,840
Like, if they're [inaudible] into.

1243
01:04:48,840 --> 01:04:50,680
Ah, good question. So TD0,

1244
01:04:50,680 --> 01:04:52,690
I- everything we're seeing in uh,

1245
01:04:52,690 --> 01:04:54,955
um question is about TD versus the TD0.

1246
01:04:54,955 --> 01:04:57,595
Everything we're talking about in class right now is TD0.

1247
01:04:57,595 --> 01:04:59,215
I'm using that because um,

1248
01:04:59,215 --> 01:05:00,850
there are multiple versions of TD.

1249
01:05:00,850 --> 01:05:04,255
And if you look at the book they'll have TD Lambda sometimes too.

1250
01:05:04,255 --> 01:05:05,920
So I'm just making sure it's clear.

1251
01:05:05,920 --> 01:05:10,220
So if you read other resources you'll know which version of TD to know too.

1252
01:05:11,610 --> 01:05:14,140
Alright. Well first question.

1253
01:05:14,140 --> 01:05:17,635
For the um, if we're using a tabular representation,

1254
01:05:17,635 --> 01:05:20,620
can we exactly represent the value of a policy?

1255
01:05:20,620 --> 01:05:28,840
[inaudible]

1256
01:05:28,840 --> 01:05:30,070
Well if we're using the,

1257
01:05:30,070 --> 01:05:32,485
if we- for every single state in the world,

1258
01:05:32,485 --> 01:05:35,830
you can grab a different- different table at representation,

1259
01:05:35,830 --> 01:05:39,050
can we exactly represent the value of the policy?

1260
01:05:40,140 --> 01:05:42,385
Yes. Yeah, we can.

1261
01:05:42,385 --> 01:05:44,245
So if you um, I,

1262
01:05:44,245 --> 01:05:47,170
if you have one feature for every single state in the world,

1263
01:05:47,170 --> 01:05:49,030
it's not going to be practical, we can't actually do this,

1264
01:05:49,030 --> 01:05:51,640
but you can exactly represent the value of that policy.

1265
01:05:51,640 --> 01:05:55,450
How could you do this? You could simply run the policy for every single state.

1266
01:05:55,450 --> 01:05:58,240
Um, I do Monte Carlo returns an

1267
01:05:58,240 --> 01:06:01,195
average and that would give you the true value of the state. So you could do it.

1268
01:06:01,195 --> 01:06:03,610
You could represent the expected discounted sum of returns

1269
01:06:03,610 --> 01:06:06,010
by representing that in every single table.

1270
01:06:06,010 --> 01:06:07,645
So that means that um,

1271
01:06:07,645 --> 01:06:15,280
this error is equal to 0 because um,

1272
01:06:15,280 --> 01:06:20,800
your functional capacity is sufficient to represent the value.

1273
01:06:20,800 --> 01:06:24,460
Let's go.

1274
01:06:24,460 --> 01:06:27,280
So what we're seeing is in expectation, right,

1275
01:06:27,280 --> 01:06:30,070
the difference between what you're function [inaudible] actually gets

1276
01:06:30,070 --> 01:06:33,625
to 0 but it's like any chart is going to be a bit different.

1277
01:06:33,625 --> 01:06:40,225
So there's um, I- i-in expectation at 0 but at any upsets, it-it's different.

1278
01:06:40,225 --> 01:06:44,710
In this case, if you have a tabular representation and this is in the limit,

1279
01:06:44,710 --> 01:06:47,050
so with infinite amounts of- of data, et cetera,

1280
01:06:47,050 --> 01:06:48,790
then this will be um,

1281
01:06:48,790 --> 01:06:51,010
this will be 0 for every single state.

1282
01:06:51,010 --> 01:06:55,135
So this, equals 0 for every state.

1283
01:06:55,135 --> 01:06:57,520
You will converge to the right value for

1284
01:06:57,520 --> 01:07:01,285
every single state if you're using a tabular representation.

1285
01:07:01,285 --> 01:07:03,535
And that's because if you think of just having like

1286
01:07:03,535 --> 01:07:06,805
literally infinite amount of data and you run your policy, just you know,

1287
01:07:06,805 --> 01:07:09,100
infinite-infinite amount of times then for every state you have

1288
01:07:09,100 --> 01:07:11,965
an infinite number of trajectories starting from that state,

1289
01:07:11,965 --> 01:07:16,840
and you can write down that value separately in the table. So it'll be 0.

1290
01:07:16,840 --> 01:07:20,110
So what that means is that the mean squared value error for

1291
01:07:20,110 --> 01:07:24,430
the Monte Carlo estimator is 0 if you're using a tabular representation.

1292
01:07:24,430 --> 01:07:27,415
And because it's 0, that is exactly the same

1293
01:07:27,415 --> 01:07:30,910
as the mean squared value estimator for TD except for -- so

1294
01:07:30,910 --> 01:07:38,635
this is just equal to MSVE of the Monte Carlo times one over one minus Gamma.

1295
01:07:38,635 --> 01:07:42,385
So, that means that this is also 0.

1296
01:07:42,385 --> 01:07:44,650
So if it's a tabular representation,

1297
01:07:44,650 --> 01:07:46,480
just to sort connect back to that,

1298
01:07:46,480 --> 01:07:49,855
um, not- none of these methods have any year.

1299
01:07:49,855 --> 01:07:52,405
Yeah, question at the back? Your name first.

1300
01:07:52,405 --> 01:07:53,110
Me?

1301
01:07:53,110 --> 01:07:53,425
Yeah.

1302
01:07:53,425 --> 01:07:59,305
Uh, I'm wondering where  the one over one minus Gamma constant came from?

1303
01:07:59,305 --> 01:08:02,800
Yes, the question is; where does that one over one minus Gamma constant come from?

1304
01:08:02,800 --> 01:08:04,240
Um, in the interest of time,

1305
01:08:04,240 --> 01:08:05,515
I'm not gonna go through it too much.

1306
01:08:05,515 --> 01:08:08,470
Um, I encourage you to read the Tsitsiklis paper.

1307
01:08:08,470 --> 01:08:12,520
Um, intuitively, there is error that's a propagate-propagating here

1308
01:08:12,520 --> 01:08:17,005
because of the fact that we're Bootstrapping and so if you have a function,

1309
01:08:17,005 --> 01:08:19,660
what this- what this result is sort of trying to highlight is that,

1310
01:08:19,660 --> 01:08:22,585
if your function approximator actually has no error,

1311
01:08:22,585 --> 01:08:25,060
then there's gonna be no difference between Monte Carlo and

1312
01:08:25,060 --> 01:08:28,750
TD because for both of them the mean squared value error,

1313
01:08:28,750 --> 01:08:30,564
um, inside of that sum,

1314
01:08:30,564 --> 01:08:32,229
the minimum over w is going to be 0.

1315
01:08:32,229 --> 01:08:34,674
So it doesn't matter whether you're using TD or Monte Carlo.

1316
01:08:34,675 --> 01:08:35,950
But if that's not true,

1317
01:08:35,950 --> 01:08:38,875
like if you can't actually exactly represent the value function,

1318
01:08:38,875 --> 01:08:42,069
then you're going to get error and that error is going to um,

1319
01:08:42,069 --> 01:08:43,599
so like basically you can think of

1320
01:08:43,600 --> 01:08:46,734
one over one minus gamma is approximately a horizon each.

1321
01:08:46,734 --> 01:08:51,024
And basically that's getting multiplied because you're adding up these errors.

1322
01:08:51,024 --> 01:08:54,204
And the reason those get added up is because you're bootstrapping.

1323
01:08:54,205 --> 01:08:59,845
You're propagating that error back whereas Monte Carlo doesn't suffer that. Yeah.

1324
01:08:59,845 --> 01:09:02,350
Um, my name is . In general,

1325
01:09:02,350 --> 01:09:05,185
the mean squared error has taken over distribution uh,

1326
01:09:05,185 --> 01:09:07,270
of the states but-

1327
01:09:07,270 --> 01:09:08,140
Under the policy yeah.

1328
01:09:08,140 --> 01:09:10,689
-yeah, yeah. Under specific policy, uh,

1329
01:09:10,689 --> 01:09:14,019
but the only specific one we've seen as a stationary distribution.

1330
01:09:14,020 --> 01:09:17,270
Do you ever use another one? Like-

1331
01:09:17,270 --> 01:09:18,930
Great question, - Okay,

1332
01:09:18,930 --> 01:09:22,109
right now we're seeing this under the stationary distribution  of the states that you're

1333
01:09:22,109 --> 01:09:25,409
gonna reach under the policy that you're exit that you care about evaluating.

1334
01:09:25,410 --> 01:09:27,330
For that, I think that's the right choice because

1335
01:09:27,330 --> 01:09:29,819
that really is the state you're gonna get to under this policy.

1336
01:09:29,819 --> 01:09:31,109
We  start to think about control.

1337
01:09:31,109 --> 01:09:32,669
You might want others, like,

1338
01:09:32,670 --> 01:09:34,600
if you're gonna change your policy.

1339
01:09:34,600 --> 01:09:38,274
Okay. All right so let's um,

1340
01:09:38,274 --> 01:09:39,564
I guess just briefly more on this.

1341
01:09:39,564 --> 01:09:41,049
I- are they faster?

1342
01:09:41,050 --> 01:09:43,450
Is one of them better? To my knowledge that's not really understood.

1343
01:09:43,450 --> 01:09:45,670
If you come across any literature on that, I'd love to hear it.

1344
01:09:45,670 --> 01:09:48,475
Um, practically TD often is better,

1345
01:09:48,475 --> 01:09:52,645
is to empirically often the bootstrapping often helps to pull up.

1346
01:09:52,645 --> 01:09:55,480
Alright. Let's move on briefly to control.

1347
01:09:55,480 --> 01:09:58,045
Um, it's going to be pretty similar.

1348
01:09:58,045 --> 01:10:00,370
So instead of representing the value function

1349
01:10:00,370 --> 01:10:02,725
and we're going to represent the state action value function

1350
01:10:02,725 --> 01:10:05,050
which is what we saw before when we wanted to often

1351
01:10:05,050 --> 01:10:07,735
move from policy evaluation to control.

1352
01:10:07,735 --> 01:10:11,140
And now what we're gonna do is we're going to interleave sort of policy evaluation with

1353
01:10:11,140 --> 01:10:15,835
a value function approximator with performing like an e-greedy policy improvement.

1354
01:10:15,835 --> 01:10:19,330
Um, this is where things can start to get unstable.

1355
01:10:19,330 --> 01:10:21,595
Um, what are we doing in this case?

1356
01:10:21,595 --> 01:10:23,860
We generally involving function approximation,

1357
01:10:23,860 --> 01:10:27,715
bootstrapping, we're often a- are also doing sampling.

1358
01:10:27,715 --> 01:10:32,170
But really the- the really big issue seems to be the off-policy learning.

1359
01:10:32,170 --> 01:10:34,210
But when we think about before we had

1360
01:10:34,210 --> 01:10:38,710
this nice stationary distribution or converging to a stationary distribution over states,

1361
01:10:38,710 --> 01:10:41,200
we're not going to be doing that anymore because we are going to be using

1362
01:10:41,200 --> 01:10:43,405
our- changing our control policy over time,

1363
01:10:43,405 --> 01:10:47,260
and so that's changing the distributions of states that we encounter.

1364
01:10:47,260 --> 01:10:50,245
And um, setting the bar to often call it The Deadly Triad.

1365
01:10:50,245 --> 01:10:51,640
If you want to, start combining

1366
01:10:51,640 --> 01:10:54,730
function approximation and bootstrapping and off-policy learning,

1367
01:10:54,730 --> 01:10:56,935
things start to get a little bit um,

1368
01:10:56,935 --> 01:11:00,175
they can fail to converge or converge to something good.

1369
01:11:00,175 --> 01:11:03,070
Alright. But before we get into that let's think about it procedurally.

1370
01:11:03,070 --> 01:11:05,635
So now we're going to have um,

1371
01:11:05,635 --> 01:11:08,560
Q functions that are parameterized by a W,

1372
01:11:08,560 --> 01:11:11,290
and we can again do stochastic gradient descent,

1373
01:11:11,290 --> 01:11:14,305
so its going to look almost identical to what we had before.

1374
01:11:14,305 --> 01:11:17,020
And again, the stochastic gradient descent can sample

1375
01:11:17,020 --> 01:11:20,350
the gradient which means for particular state action pair,

1376
01:11:20,350 --> 01:11:23,210
then we're gonna do these updates.

1377
01:11:25,670 --> 01:11:28,860
So, here what we're gonna do,

1378
01:11:28,860 --> 01:11:30,805
is we're gonna represent, um,

1379
01:11:30,805 --> 01:11:36,275
our Q function by a set of linear state action, um, weights.

1380
01:11:36,275 --> 01:11:38,240
So, that means we're gonna have features that

1381
01:11:38,240 --> 01:11:40,575
sort of both encode the state and the action.

1382
01:11:40,575 --> 01:11:45,390
So, like what I saw when I was turning left as my robot for example.

1383
01:11:45,390 --> 01:11:48,705
Um, so, it's gonna be a combination of these two.

1384
01:11:48,705 --> 01:11:51,020
And then, once we have that then we're gonna just have

1385
01:11:51,020 --> 01:11:54,310
a weight vector on top of that for the Q.

1386
01:11:54,310 --> 01:11:57,040
So, we're not having separate weight vectors for each action.

1387
01:11:57,040 --> 01:11:58,680
Instead, we're having features that try to

1388
01:11:58,680 --> 01:12:01,320
encompass both the state and action themselves.

1389
01:12:01,320 --> 01:12:05,380
And then, we can do our Stochastic gradient descent on top of that.

1390
01:12:07,610 --> 01:12:10,580
So, how does this work for,

1391
01:12:10,580 --> 01:12:12,965
um, uh, like, Monte Carlo?

1392
01:12:12,965 --> 01:12:15,730
Um, it's gonna look almost identical to before.

1393
01:12:15,730 --> 01:12:18,460
We're just gonna again use our return.

1394
01:12:18,460 --> 01:12:22,410
Now, we're gonna be defining returns from a particular state action.

1395
01:12:22,410 --> 01:12:26,190
For doing first visit the first time we reach the state action in that episode,

1396
01:12:26,190 --> 01:12:27,480
we look at the return,

1397
01:12:27,480 --> 01:12:30,565
the sum of rewards till the end of the episode and we use that as our target.

1398
01:12:30,565 --> 01:12:32,360
Use that as our estimate of the oracle,

1399
01:12:32,360 --> 01:12:36,030
the true Q function and we update towards that.

1400
01:12:36,030 --> 01:12:39,370
In SARSA we're, gonna use a TD target,

1401
01:12:39,370 --> 01:12:43,680
so we're gonna look at the immediate reward of our tuple plus gamma times

1402
01:12:43,680 --> 01:12:48,730
Q of the next state that we encountered and the action we took.

1403
01:12:48,730 --> 01:12:51,340
And so, then we're,

1404
01:12:51,340 --> 01:12:56,300
again I'm just gonna just plug that in.

1405
01:12:56,300 --> 01:12:58,265
And then for Q learning,

1406
01:12:58,265 --> 01:13:00,370
it's gonna look almost identical to Q learning

1407
01:13:00,370 --> 01:13:03,290
except for again we're gonna plug in function approximators everywhere.

1408
01:13:03,290 --> 01:13:07,130
So, we're gonna plug in, um, this.

1409
01:13:07,130 --> 01:13:11,575
Remember, is gonna be a function of RX which is gonna be a function of S

1410
01:13:11,575 --> 01:13:13,370
prime and A prime times R

1411
01:13:13,370 --> 01:13:17,340
W. Whereas here this is going to be a function of the state in action.

1412
01:13:17,340 --> 01:13:20,790
Everything's linear and we're just doing different forms of

1413
01:13:20,790 --> 01:13:24,550
bootstrapping and comparing whether or not we take a max or not.

1414
01:13:24,550 --> 01:13:27,695
All right.

1415
01:13:27,695 --> 01:13:30,610
So, I went through that a little bit fast but it's basically exactly

1416
01:13:30,610 --> 01:13:33,850
analogous to the first part which we sort of stepped through more carefully.

1417
01:13:33,850 --> 01:13:36,400
Uh, so, far everything's with Q functions now.

1418
01:13:36,400 --> 01:13:39,575
Why might this gets or weird or tricky?

1419
01:13:39,575 --> 01:13:44,245
So, TD with value function approximation does not really follow in a gradient.

1420
01:13:44,245 --> 01:13:47,060
I don't have time to go into total details on that today,

1421
01:13:47,060 --> 01:13:50,350
but there's a ni- some nice explanations on this in Chapter 11.

1422
01:13:50,350 --> 01:13:52,635
So, certain Umberto Chapter 11,

1423
01:13:52,635 --> 01:13:59,475
um, it's a great resource and we also have lecture notes available online.

1424
01:13:59,475 --> 01:14:03,120
Um, and so, informally we're sort

1425
01:14:03,120 --> 01:14:06,910
of doing this interleaving or doing this like approximate sample Bellman backup

1426
01:14:06,910 --> 01:14:10,695
combined with what's often known as a projection step because we're trying to

1427
01:14:10,695 --> 01:14:15,405
sort of project our value function back into the space of representable functions.

1428
01:14:15,405 --> 01:14:18,910
And intuitively for why this might start to be a problem,

1429
01:14:18,910 --> 01:14:22,345
is that the Bellman operator we showed is a contraction.

1430
01:14:22,345 --> 01:14:25,485
Like when we're doing dynamic programming we showed that if you do

1431
01:14:25,485 --> 01:14:29,640
Bellman opera- Bellman backups you're guaranteed to converge to a fixed point.

1432
01:14:29,640 --> 01:14:32,345
When you do the value function approximator,

1433
01:14:32,345 --> 01:14:34,350
it can be an expansion.

1434
01:14:34,350 --> 01:14:36,040
What does an expansion mean.

1435
01:14:36,040 --> 01:14:37,069
Well, what a contraction,

1436
01:14:37,069 --> 01:14:39,020
just as a reminder what a contraction said is

1437
01:14:39,020 --> 01:14:41,250
let's say for an operator that's a contraction.

1438
01:14:41,250 --> 01:14:47,335
If you apply this operator and this is an operator like the Bellman equation.

1439
01:14:47,335 --> 01:14:50,634
I wanna back up, if you apply it to two different value functions,

1440
01:14:50,634 --> 01:14:53,890
the distance between this feel like a max norm or something

1441
01:14:53,890 --> 01:14:57,230
like that is less than or equal to the previous distance.

1442
01:14:57,230 --> 01:14:59,510
Which means as you apply this operator,

1443
01:14:59,510 --> 01:15:02,320
the distance between your old value function and your new value function keeps

1444
01:15:02,320 --> 01:15:05,560
getting smaller and smaller and smaller and eventually get to a fixed point.

1445
01:15:05,560 --> 01:15:08,385
The problem is this now we're not doing that anymore.

1446
01:15:08,385 --> 01:15:15,140
It's more like we're doing like O V and then we do some sort of projection operator.

1447
01:15:15,140 --> 01:15:18,340
I'm just gonna call it kinda weird P. So,

1448
01:15:18,340 --> 01:15:23,120
this is the projection operator which means when you compute your new value function,

1449
01:15:23,120 --> 01:15:26,380
it may no longer lie in your value function approximators space

1450
01:15:26,380 --> 01:15:29,895
and you're gonna have to refit it back into that space.

1451
01:15:29,895 --> 01:15:31,885
And when you do that, um,

1452
01:15:31,885 --> 01:15:34,550
that operator itself can be an expansion.

1453
01:15:34,550 --> 01:15:38,450
For those of you that are interested in some of the early sort of discussions of this,

1454
01:15:38,450 --> 01:15:44,435
Jeff Gordon, um, has a really nice paper on averages from 1995.

1455
01:15:44,435 --> 01:15:49,475
Um, but they talk about how linear value function approximators can be an expansion.

1456
01:15:49,475 --> 01:15:51,800
So, the Bellman backup is fine.

1457
01:15:51,800 --> 01:15:54,020
It's a contraction but when you do this approximation you

1458
01:15:54,020 --> 01:15:57,085
might expand the distance and that's one of the problems.

1459
01:15:57,085 --> 01:16:01,485
Okay. So, if we go back to our Baird example and,

1460
01:16:01,485 --> 01:16:06,055
um, think about this a little bit more in terms of the, the control case.

1461
01:16:06,055 --> 01:16:08,650
So, let's imagine that we have a setting where,

1462
01:16:08,650 --> 01:16:11,160
um, you have two different policies.

1463
01:16:11,160 --> 01:16:13,860
And the first policy, and this is the policy that you want to

1464
01:16:13,860 --> 01:16:18,470
evaluate you always take the solid line.

1465
01:16:18,470 --> 01:16:24,020
So, you always take A1 and in your behavior data, this is the data that you're,

1466
01:16:24,020 --> 01:16:26,280
this is the policy you're using to gather data,

1467
01:16:26,280 --> 01:16:33,445
you take A2 with six-sevenths of the time and you take A1 one seventh of the time.

1468
01:16:33,445 --> 01:16:35,225
Gamma is point nine nine.

1469
01:16:35,225 --> 01:16:38,630
Um, and what you do is you generate a whole bunch of data.

1470
01:16:38,630 --> 01:16:40,860
So, you generate data,

1471
01:16:41,350 --> 01:16:46,180
data under your behavior policy.

1472
01:16:49,900 --> 01:16:53,970
So, there's some really cool work on how you deal with, um,

1473
01:16:53,970 --> 01:16:55,800
sort of correcting for the,

1474
01:16:55,800 --> 01:16:58,010
the data that you're getting versus the data you wanna evaluate.

1475
01:16:58,010 --> 01:17:01,420
Let's imagine we, we don't go into any of that which I think is super cool and we're,

1476
01:17:01,420 --> 01:17:03,370
instead we're just gonna do something super simple which is

1477
01:17:03,370 --> 01:17:05,735
we're gonna throw out all the data that doesn't match.

1478
01:17:05,735 --> 01:17:13,020
So, imagine you just throw away data if,

1479
01:17:13,020 --> 01:17:18,870
um, A is not equal to Pi of S. So,

1480
01:17:18,870 --> 01:17:20,980
you generated all these data points.

1481
01:17:20,980 --> 01:17:22,840
So, what does it, what do I mean by data points here?

1482
01:17:22,840 --> 01:17:25,345
We had SAR as prime.

1483
01:17:25,345 --> 01:17:27,185
So, you take all these tuples.

1484
01:17:27,185 --> 01:17:30,630
If it turns out that the action that was taken there is not the same as the,

1485
01:17:30,630 --> 01:17:33,890
the policy you wanna evaluate but where you're only ever taking A1,

1486
01:17:33,890 --> 01:17:35,590
just throw out that tuple, you don't update.

1487
01:17:35,590 --> 01:17:40,390
So, now all of your remaining data is consistent with your policy.

1488
01:17:40,390 --> 01:17:44,170
So, let's imagine then you tried to do TD learning with that data.

1489
01:17:44,170 --> 01:17:46,570
The problem is, you can diverge.

1490
01:17:46,570 --> 01:17:48,230
And what do I mean by that?

1491
01:17:48,230 --> 01:17:50,885
It mean that, that, that your weights could blow up.

1492
01:17:50,885 --> 01:17:53,260
Super interesting why this happens.

1493
01:17:53,260 --> 01:17:56,915
Um, the main intuition for it is that your distribution data is

1494
01:17:56,915 --> 01:18:01,285
not the same as the data you'd get under your desired target policy.

1495
01:18:01,285 --> 01:18:05,295
In particular, if you were actually to run the,

1496
01:18:05,295 --> 01:18:07,810
the policy carve out Pi what would happen?

1497
01:18:07,810 --> 01:18:10,960
Let's say you start off in state S1. You take A1.

1498
01:18:10,960 --> 01:18:13,770
That determinant will get you to state seven but you're gonna stay

1499
01:18:13,770 --> 01:18:17,620
in a seven for a really long time because it's deterministic.

1500
01:18:17,930 --> 01:18:20,965
So, you'd get like S1. S7, S7.

1501
01:18:20,965 --> 01:18:22,520
Let's say even you did this maybe you,

1502
01:18:22,520 --> 01:18:24,840
maybe it wasn't episodic case and you have multiple episodes,

1503
01:18:24,840 --> 01:18:28,230
you'd still have cases where you'd like have very little bit amount of data

1504
01:18:28,230 --> 01:18:32,650
about S these S's and lots of data about S7.

1505
01:18:32,650 --> 01:18:34,540
But in the data that you get from

1506
01:18:34,540 --> 01:18:37,690
your behaviour policy because a bunch of the time it takes A2,

1507
01:18:37,690 --> 01:18:41,915
it'll keep teleportating you back to one of S1 through S6.

1508
01:18:41,915 --> 01:18:44,060
Which means the distribution of your data,

1509
01:18:44,060 --> 01:18:45,970
the data you have looks very different.

1510
01:18:45,970 --> 01:18:48,660
The distribution of states you visit looks very different than

1511
01:18:48,660 --> 01:18:52,355
the data that you get and the states you'd visit under Pi.

1512
01:18:52,355 --> 01:18:54,900
And that is the problem. If you,

1513
01:18:54,900 --> 01:18:57,060
if you don't account for this mismatch,

1514
01:18:57,060 --> 01:18:58,870
then the values can diverge.

1515
01:18:58,870 --> 01:19:01,110
Even though they're all sort of compatible,

1516
01:19:01,110 --> 01:19:03,905
all in the sense that you're only ever using

1517
01:19:03,905 --> 01:19:08,210
state action pair if you took the action under your desired policy.

1518
01:19:08,210 --> 01:19:10,640
And this sort of issue can also come up,

1519
01:19:10,640 --> 01:19:12,385
um, when you're using Q learning.

1520
01:19:12,385 --> 01:19:16,640
Um, uh, and you're doing generally like updating this policy over time.

1521
01:19:17,230 --> 01:19:20,040
So, in terms of this,

1522
01:19:20,040 --> 01:19:22,665
um, just to briefly summarize before we finish.

1523
01:19:22,665 --> 01:19:25,630
In the tabular case everything converges, it's beautiful.

1524
01:19:25,630 --> 01:19:27,500
Um, in the linear case,

1525
01:19:27,500 --> 01:19:34,740
[NOISE] I mean by this I mean that,

1526
01:19:34,740 --> 01:19:37,160
um, you can chatter.

1527
01:19:37,300 --> 01:19:40,130
You basically converge but you might,

1528
01:19:40,130 --> 01:19:43,225
um, uh, there might be some oscillation.

1529
01:19:43,225 --> 01:19:48,515
Uh, but Q learning like we are doing this off policy aspect can diverge.

1530
01:19:48,515 --> 01:19:52,130
And once we get into nonlinear value function approximation,

1531
01:19:52,130 --> 01:19:53,750
every- like mostly all bets are off.

1532
01:19:53,750 --> 01:19:56,875
Now, this is a little bit of an oversimplification.

1533
01:19:56,875 --> 01:20:01,010
Um, there has been a huge amount of work and huge amount of interest in this because

1534
01:20:01,010 --> 01:20:05,065
everyone wants to do function approximation with or else we can tackle real problems.

1535
01:20:05,065 --> 01:20:06,490
And so, over the last, like,

1536
01:20:06,490 --> 01:20:08,730
one or two decades, there's been a huge amount of work on this.

1537
01:20:08,730 --> 01:20:12,485
Um, and there are some algorithms now that do have convergence guarantees.

1538
01:20:12,485 --> 01:20:15,890
Um, and there's some coo- super cool really recent work, um,

1539
01:20:15,890 --> 01:20:20,555
where they're looking at batch URL which can converge with nonlinear approximators.

1540
01:20:20,555 --> 01:20:24,155
So, there's definitely a lot of work on this that we're not gonna get to.

1541
01:20:24,155 --> 01:20:25,760
Um, I just wanna highlight that it's

1542
01:20:25,760 --> 01:20:27,720
a really important issue not just whether it converges,

1543
01:20:27,720 --> 01:20:29,275
but what it converges to.

1544
01:20:29,275 --> 01:20:32,895
Like you might converge to a point which is a really bad approximation.

1545
01:20:32,895 --> 01:20:34,040
So, it's stable.

1546
01:20:34,040 --> 01:20:35,770
It doesn't, your dab- your weights aren't blowing

1547
01:20:35,770 --> 01:20:38,425
up but it's just a really bad approximator and,

1548
01:20:38,425 --> 01:20:40,380
and some of the critical choices here are

1549
01:20:40,380 --> 01:20:42,695
your objective functioning and your feature representation.

1550
01:20:42,695 --> 01:20:47,040
So, just before we close I think this is a really nice figure from Sutton and Barto.

1551
01:20:47,040 --> 01:20:50,110
Um, what they're showing here is you can think of it as like you have

1552
01:20:50,110 --> 01:20:52,375
this plane which is where you can represent

1553
01:20:52,375 --> 01:20:54,955
all of your linear value function approximators.

1554
01:20:54,955 --> 01:20:57,415
And what happens when you do a Bellman update,

1555
01:20:57,415 --> 01:20:59,520
um, or like you do a TD backup,

1556
01:20:59,520 --> 01:21:02,080
is that you're gonna now sort of have a value function that might not

1557
01:21:02,080 --> 01:21:04,765
be representable in your plane and the you're gonna,

1558
01:21:04,765 --> 01:21:06,295
you can project it back.

1559
01:21:06,295 --> 01:21:10,615
And these different, you can quantify different forms of error,

1560
01:21:10,615 --> 01:21:12,820
different basically this allows you to find

1561
01:21:12,820 --> 01:21:15,250
different objective functions that you're trying

1562
01:21:15,250 --> 01:21:18,150
to minimize in order to find the best approximator.

1563
01:21:18,150 --> 01:21:20,105
And we've seen one today which is sort of this

1564
01:21:20,105 --> 01:21:22,814
me- minimum mean squared error approximation

1565
01:21:22,814 --> 01:21:25,965
essentially over the like these Bellman errors but that's not

1566
01:21:25,965 --> 01:21:29,105
the only choice and it's not necessary even the best choice.

1567
01:21:29,105 --> 01:21:32,160
Um, because it might be that the one that has the smallest error there

1568
01:21:32,160 --> 01:21:35,560
is not the same one that has the best performance in your real problem.

1569
01:21:35,560 --> 01:21:37,635
So, that's a little bit fast but it,

1570
01:21:37,635 --> 01:21:39,200
it's super and Shane that's covered in,

1571
01:21:39,200 --> 01:21:40,470
um, Sutton and Barto in 11.

1572
01:21:40,470 --> 01:21:41,980
If you wanna go into more detail.

1573
01:21:41,980 --> 01:21:44,030
Just really quick, what are the things you should understand?

1574
01:21:44,030 --> 01:21:45,990
You should, um, you have to implement these ones on

1575
01:21:45,990 --> 01:21:49,245
linear value function approximator both for policy evaluation and control.

1576
01:21:49,245 --> 01:21:52,075
You should understand whether or not things can, uh,

1577
01:21:52,075 --> 01:21:55,045
converge in the policy evaluation case

1578
01:21:55,045 --> 01:21:57,640
and when the solution has zero error and non-zero error.

1579
01:21:57,640 --> 01:22:00,280
Um, and you should just just understand qualitatively

1580
01:22:00,280 --> 01:22:03,270
what are the issues that can come out so that some

1581
01:22:03,270 --> 01:22:05,890
of these solutions may not always converge and it's

1582
01:22:05,890 --> 01:22:09,600
this combination of function approximation bootstrapping and all policy learning.

1583
01:22:09,600 --> 01:22:12,480
All right. So, that's enough just to get started with the homework

1584
01:22:12,480 --> 01:22:14,990
two that we're gonna be releasing this week, today.

1585
01:22:14,990 --> 01:22:17,610
And then, um, next week we're gonna start to talk about deepening.

1586
01:22:17,610 --> 01:22:19,530
Thanks.

