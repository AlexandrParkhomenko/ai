# 12\. Визуализация и понимание сверточных сетей

Всегда очень весело читать эту лекцию, потому что мы видим много красивых картинок. 

К этому моменту мы узнали в классе как обучать сверточные сети. Но есть вопросы, которые вы, возможно, задавали:
- Что именно происходит внутри этих сетей? 
- Как они делали то, что делали? 
- Какие функции они ищут?

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0004.jpg)

ConvNets как маленький черный ящик. На вход подаётся сырое изображение. Происходит ряд свёрток. На выходе ряд оценок или положение ограничивающей рамки или маркированные пиксели или что-то в этом роде.

## Визуализация первого слоя.

Это относительно простая вещь. Например: в AlexNet первый сверточный слой состоит из 64 фильтров, каждый сверточный фильтр имеет форму 3 канала на 11 на 11 пикселов.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0005.jpg)

Эти сверточные фильтры скользят по входному изображению. Мы можем получить то, что ищут эти фильтры просто визуализируя обученные веса этих фильтров как сами изображения. Вы видите, что они ищут ориентированные края под разными углами и в разных положениях, подобно самым ранним уровням зрительной системы человека.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0006.jpg)

## Визуализация промежуточных сверточных слоев.

На самом деле это гораздо менее интерпретируемо. На сайте нашего курса мы используем крошечную демонстрационную сеть ConvNets. Второй сверточный слой, теперь получает 16-канальный вход. И делает свертку 7 на 7 с 20 сверточными фильтрами. Так что проблема в том, что вы не можете визуализировать это прямо как изображения. Входные данные имеют глубину 16 измерений, у нас есть 20 таких сверточных фильтров, которые создают выходные плоскости следующего слоя. Мы просуммировали эти каналы и вывели в градации серого. У нас есть масштабирование веса в диапазоне от нуля до 255, на практике эти веса могут быть неограниченными.


## Последний слой.

У нас есть, может быть, 1000 оценок класса. Непосредственно перед последним слоем у нас часто есть какой-то полносвязный слой.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0008.jpg)

Пытаемся приблизиться к ближайшему соседу.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0009.jpg)

Последний скрытый слой сети имеет 4096-мерный вектор. Вы видели что-то вроде PCA - анализ основных компонентов. Что позволяет вам взять какое-то многомерное представление, а затем сжать его до двух измерений. 

Но есть еще один действительно мощный алгоритм называется t-SNE. Это метод уменьшения нелинейной размерности:

[версии в высоком разрешении](https://cs.stanford.edu/people/karpathy/cnnembed/)

Визуализацию карт активации в некоторых случаях можно интерпретировать [Jason Yosinski на YouTube](https://youtu.be/AgkfIQ4IGaM)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture12_page-0032.jpg)

С оригинальной лекцией можно ознакомиться на [YouTube](https://youtu.be/6wcs6szJWMY).
