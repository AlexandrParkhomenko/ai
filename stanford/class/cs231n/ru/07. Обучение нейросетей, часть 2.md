# 7\. Обучение нейросетей, часть 2

В шестой лекции мы начали рассказывать про обучение нейросетей: выяснили, как выбрать функцию активации, подготавливать данные, настраивать параметры и следить за процессом. Сегодня мы продолжим обсуждение и подробнее рассмотрим оптимизацию, регуляризацию и передачу обучения.

Хотя мы уже рассмотрели наиболее важные аспекты обучения нейросетей, стоит взять на заметку ещё несколько интересных и полезных стратегий. Как упоминалось ранее, для получения хороших и точных результатов важно применять оптимизацию (добиваться минимального значения функции потерь) и регуляризацию (упрощать модель, чтобы избежать переобучения). Выясним, как именно эти процессы выглядят на практике.

## Оптимизация: подводные камни

Мы говорили, что функция потерь показывает, насколько хорошо или плохо выбранные веса справляются с поставленной задачей (например, с классификацией изображений). В двумерном пространстве значения функции можно представить в виде цветного «ландшафта», где наиболее тёплый регион (красный) — наименьшая величина потерь, к которой мы стремимся; а холодный (синий) — наибольшая. Рассмотрим простой пример оптимизации для двух значений функции потерь: **W_1** и **W_2**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0014.jpg)

Самый тривиальный алгоритм оптимизации — стохастический градиентный спуск, реализуемый всего в три строки кода. Он обновляет параметры, вычисляя отрицательное направление градиента, которое соответствует наибольшему уменьшению функции потерь. Выполняя эту операцию много раз, мы, возможно, достигнем красного региона, получим минимум ошибок и будем счастливы.

Но, к сожалению, на практике метод сопровождается множеством проблем. Представим, что мы поменяли одно из значений, например, **W_2**. При этом наши потери стали уменьшаться очень быстро. Тогда мы меняем значение **W_1**, но теперь прогресс идёт гораздо медленнее. Выводя это на график, мы увидим, что функция потерь более чувствительна к изменениям в вертикальном направлении, чем в горизонтальном. Стохастический градиентный спуск будет выглядеть зигзагообразно — и это не очень хорошо, поскольку прогресс оптимизации слишком медленный.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0016.jpg)

Ещё одна распространённая проблема — локальные минимумы и [седловые точки](https://ru.wikipedia.org/wiki/%D0%A1%D0%B5%D0%B4%D0%BB%D0%BE%D0%B2%D0%B0%D1%8F_%D1%82%D0%BE%D1%87%D0%BA%D0%B0). Если функция имеет форму сложной кривой, как показано на рисунке ниже, градиентный спуск может «застрять» в одном из её углублений — локальном экстремуме.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0018.jpg)

Проблема с седловыми точками возникает, когда в одном направлении функция возрастает, а в другом — убывает. Самый простой пример двумерной седловой точки — ноль на графике **y = x<sup>3</sup>**. В трёхмерном пространстве образующаяся поверхность напоминает седло, из-за чего и появилось такое название. При попадании в седловую точку градиентный спуск застопорится на локальном минимуме. И независимо от числа дальнейших итераций функция потерь практически не уменьшится.

<!--
![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0001.jpg)
_Примеры графиков с седловыми точками_
-->

Трудности возникают также из-за зашумлённых данных, которые могут в большом количестве оказаться в пакете образцов, обычно использующемся для стохастического градиентного спуска.

## Улучшенная оптимизация

Глядя на вышеупомянутые затруднения, возникает логичный вопрос: какой же алгоритм оптимизации поможет их избежать? 

### SGD + Momentum

К счастью, есть очень простая стратегия, решающая большинство проблем. Её идея в том, чтобы сохранить скорость градиентного спуска, добавив для этого некоторый импульс (momentum). Теперь вместо простого итеративного вычисления градиента мы прибавляем к нему подсчёт скорости и дописываем две строки в наш код.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0021.jpg)

Таким образом, мы будем двигаться в направлении изменения скорости, а не в направлении градиента. Гиперпараметр **rho** соответствует трению, как при движении по твёрдой поверхности.

Что же теперь произойдёт в локальном минимуме или седловой точке? Здесь можно провести аналогию с шариком, который катится с горки. Как только он дойдёт до точки, где градиент равен нулю, он по-прежнему сохранит некоторую скорость и будет двигаться дальше.

### Nesterov Accelerated Gradient

Одна из разновидностей этого алгоритма — **ускоренный градиент Нестерова** (Nesterov Accelerated Gradient). Посмотрим на рисунок ниже, где градиентный спуск стартует из красной точки. Мы знаем, что импульс собирается приблизить нас к вершине зеленой стрелки. Вместо того, чтобы оценивать градиент в текущей позиции (красная точка), мы оцениваем градиент сразу в новой позиции. Так мы найдём точку в окрестности, где окажемся в следующий момент.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0024.jpg)

Ниже можно увидеть, как это выглядит в виде формул и кода. На графике показано сравнение обычного градиентного спуска (чёрная линия), градиентного спуска с импульсом (синяя линяя) и ускоренного градиента Нестерова (зелёная линия).

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0027.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0028.jpg)

### AdaGrad

Другой популярный метод оптимизации называется [AdaGrad](http://jmlr.org/papers/v12/duchi11a.html). На каждом шаге мы вычисляем промежуточную сумму всех квадратов градиентов, которые считаются во время обучения. Полученное значение используется для нормализации — мы просто делим параметры на корень из суммы квадратов. 

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0029.jpg)

Вспомним ситуацию, когда наша функция слишком быстро менялась в одном направлении и медленно в другом. Это значит, что у одной из координат было большое значение градиента, а у другой — маленькое. Если мы возьмём корень из суммы квадратов небольшого градиента, то будем делить параметры на маленькое число и тем самым ускорим движение в медленном направлении. С большими значениями будет ровно наоборот, что в теории должно сгладить зигзагообразную кривую. Но здесь же кроется загвоздка — если значения градиента постоянно растут, то мы начнём делить параметры на всё большие числа и шаги будут становиться всё медленнее.

### RMSProp

Усовершенствованный алгоритм, называемый RMSProp, отчасти решает эту проблему. Вместо простого суммирования он использует скользящее среднее квадратов градиентов, что фактически уменьшает значение grad_squared и не даёт оптимизации замедляться. На рисунке ниже показана реализация RMSProp.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0032.jpg)

### Adam

Adam объединяет в себе преимущества RMSProp, AdaGrad и SGD+Momentum. Сначала мы выполняем оценку первого импульса и взвешенной суммы градиентов, а затем оцениваем второй импульс и квадрат градиентов. Первый импульс играет роль скорости, а второй служит для оптимизации параметров.

Изначально оба параметра равны нулю, поэтому к ним добавляется корректирующее смещение, чтобы избежать слишком большого шага в самом начале.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0035.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0036.jpg)

На анимациях ниже продемонстрировано, как ведут себя различные алгоритмы оптимизации. Можно увидеть, что обычный SGD работает гораздо медленнее всех упомянутых методов. Справа показано поведение в седловой точке, быстрее всего которую преодолевает AdaDelta — ещё одна вариация AdaGrad, очень похожая на Adam за исключением того, что в ней отсутствует импульс.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/sgd-methods.gif)


![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/sgd-methods-3d.gif)


## Оптимизация второго порядка

Все алгоритмы, описанные выше, относятся к оптимизации первого порядка, поскольку для градиента нам необходимо считать только первые производные ([якобиан](https://ru.wikipedia.org/wiki/%D0%AF%D0%BA%D0%BE%D0%B1%D0%B8%D0%B0%D0%BD)). Но существует также оптимизация второго порядка, учитывающая как первую, так и вторую производные. В ней используется приближение Тейлора, которое локально аппроксимирует функцию к квадратичной и позволяет быстрее достичь минимума.

Обобщая этот подход в многомерное пространство, мы получим так называемый [шаг Ньютона](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). В нём сначала вычисляется [гессиан](http://ru.wikipedia.org/wiki/%D0%93%D0%B5%D1%81%D1%81%D0%B8%D0%B0%D0%BD_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) (вторые производные), а затем берётся обратная матрица для более быстрого перехода к минимуму квадратичной аппроксимации нашей функции.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0047.jpg)

_Шаг Ньютона. Hf(x) — гессиан, ∇f(x) — якобиан._

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0048.jpg)

Однако приведенное выше уравнение нецелесообразно для большинства приложений глубокого обучения, поскольку вычисление (и инвертирование) гессиана — очень дорогостоящий процесс. Поэтому существующие методы пытаются аппроксимировать обратный гессиан. Среди них наиболее популярным является [L-BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS), который использует для этого информацию о нескольких последних значениях градиента.

На практике оптимизация второго порядка редко применяется к масштабным нейронным сетям. Вместо неё используются стандартные алгоритмы первого порядка, основанные на градиентном спуске, потому что они проще вычисляются и легче масштабируются.

## Ансамблевые методы

Иногда кажется, будто мы уже нашли подходящий алгоритм оптимизации и достигли хорошей точности, но этого всё ещё недостаточно. Что же можно сделать, чтобы модель работала лучше?

На помощь придёт способ «одна голова — хорошо, две — лучше, а ещё лучше — десять!», который также известен как ансамблевый метод. Вместо того, чтобы использовать одну нейросеть, можно независимо обучить несколько архитектур, а во время тестирования взять их усреднённый результат. Это помогает уменьшить переобучение и немного улучшить эффективность модели (обычно на пару процентов). Также вместо разных алгоритмов можно взять несколько версий одной нейросети.

## Выбор скорости обучения

Ни один метод не достигнет хороших результатов без правильной настройки гиперпараметров, самый важный из которых — скорость обучения (learning rate). В процессе оптимизации глубоких сетей скорость обычно уменьшают с течением времени. Это связано с тем, что при высоких значениях наша система будет содержать слишком много кинетической энергии. Вектор параметров не сможет приблизиться к минимуму функции потерь и начнёт хаотично перемещаться вокруг него.

Заранее предугадать, когда следует уменьшить скорость обучения, может быть непросто: если она будет затухать медленно, вы потратите слишком много вычислительных ресурсов и долго не будете наблюдать улучшений. Но чересчур агрессивное затухание приведёт к преждевременной остановке обучения. 

Существует три распространенных способа снижения learning rate:

**Пошаговое затухание:** снижение скорости на некоторую величину каждые несколько эпох. Типичные варианты: уменьшение в два раза каждые 5 эпох или в 0.1 раз каждые 20 эпох. Конкретные цифры сильно зависят от задачи и модели.

**Экспоненциальное затухание** и **затухание 1/t:** имеют вид, показанный на рисунке ниже. Здесь **α<sub>0</sub>** и **k** — гиперпараметры, а **t** — номер итерации.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0040.jpg)

## Регуляризация Dropout

Регуляризация используется для упрощения модели, чтобы избежать переобучения и заставить её корректно работать на новых данных. На лекции о функции потерь мы говорили о методах L1, L2 и Elastic net. Но чаще всего в машинном обучении применяется эффективный и простой алгоритм [Dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf).

Каждый раз, выполняя прямой проход по слою, мы устанавливаем некоторые значения активаций нулевыми (нейроны выбираются случайно). В результате мы получим несколько «подсетей» и будем поочерёдно оценивать их эффективность, что делает dropout похожим на ансамбль методов.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0060.jpg)

Самая простая реализация для трёхслойной нейросети выглядит следующим образом:

```
""" Vanilla Dropout: не рекомендуется использовать на практике """

p = 0.5 # вероятность оставить нейрон активным. Чем выше, тем меньше dropout

def train_step(X):
  """ X - данные """

  # прямой проход по трёхслойной нейросети
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # первый dropout
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # второй dropout
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

  # обратный проход: вычисление градиентов... (не показано)
  # обновление параметров... (не показано)

def predict(X):
  # ансамблевый прямой проход
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # масштабируем активации!
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # масштабируем активации!
  out = np.dot(W3, H2) + b3
```

Нежелательное свойство представленной выше схемы состоит в том, что мы должны масштабировать активации во время прогнозирования. Поскольку производительность тестирования критически важна, лучше всего использовать **инвертированный dropout**, который выполняет масштабирование во время обучения. Кроме того, если мы захотим убрать dropout из кода, функция прогнозирования останется без изменений.

```
""" 
Inverted Dropout: Рекомендуемая реализация
"""

p = 0.5 # вероятность оставить нейрон активным. Чем выше, тем меньше dropout

def train_step(X):
  # прямой проход по трёхслойной нейросети
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # первый dropout. Заметьте /p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # второй dropout. Заметьте /p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

  # обратный проход: вычисление градиентов... (не показано)
  # обновление параметров... (не показано)

def predict(X):
  # ансамблевый прямой проход
  H1 = np.maximum(0, np.dot(W1, X) + b1) # масштабирование не нужно
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

Dropout очень полезен тем, что с его помощью можно менять силу регуляризации, просто варьируя значение параметра **p**.

## Дополнение данных

На этом регуляризация не заканчивается! Вы наверняка знаете о том, что нейросети должны находить на фотографии кошку, даже если снимок будет перевёрнут или обрезан. Для этого можно просто взять исходные изображения, выполнить над ними различные преобразования (масштабирование, отражение, поворот, обрезка, коррекция цветов) и дополнить уже существующий датасет. Эта стратегия также может оказаться очень полезной в тех случаях, когда исходных данных для обучения слишком мало.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0075.jpg)

На практике при дополнения датасета также используется смещение пикселей, добавление [дисторсии](http://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%81%D1%82%D0%BE%D1%80%D1%81%D0%B8%D1%8F), [метод главных компонент](http://ru.wikipedia.org/wiki/%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82), увеличение/уменьшение яркости, контрастности, резкости и другие способы. Здесь можно дать волю своей фантазии.

## Передача обучения

При недостаточном количестве исходных образцов также стоит попробовать метод передачи обучения или Transfer Learning. Его суть заключается в том, что сначала мы обучаем нейросеть на большом наборе данных (например, ImageNet), а затем подстраиваем её под свой датасет. Для этого необходимо заново инициализировать матрицу весов последнего слоя и переобучить его, чтобы сеть адаптировалась под наши потребности.

Но это работает, только если данных действительно совсем немного. В других случаях, возможно, потребуется переобучить несколько слоёв или всю нейросеть.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ai/main/stanford/class/cs231n/ru/images/cs231n_2017_lecture7_page-0090.jpg)

Мы заново обучаем только последние слои, оставляя предыдущие без изменений. Это связано с тем, что в первых слоях нейросети содержатся более общие признаки, в то время как последние отвечают за специфичные особенности.

Мы выяснили, какие бывают стратегии оптимизации, чем полезна регуляризация и как можно использовать уже обученные нейросети для новых проектов. 

А на следующей лекции мы расскажем о программном обеспечении, которое используется в машинном обучении. 

С оригинальной лекцией можно ознакомиться на [YouTube](https://youtu.be/_JB0AO7QxSA).
