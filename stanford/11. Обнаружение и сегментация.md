# 11\. Обнаружение и сегментация

Сегодня мы поговорим о различных типах других задач компьютерного зрения и расскажем о пространственных пикселях внутри ваших изображений. Мы увидим **сегментацию, локализацию, обнаружение**.

При классификации изображений, у нас будет приходить какое-то входное изображение и проходить через какую-то глубокую сверточную сеть. Эта сеть даст нам некоторый вектор признаков, возможно, 4096 измерений в случае AlexNet RGB. Дальше подавать эти признаки на полносвязный слой с 1000 выходами - разными оценками классов. Это, пожалуй, самая простая из возможных задач.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture11_page-0017.jpg)

В задаче семантической сегментации мы хотим ввести изображение, а затем вывести решение категории для каждого пикселя в этом изображении. И потом сказать: это кот ходит по полю, он очень милый.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture11_page-0019.jpg)

Есть одна интересная вещь при семантической сегментации, она не различает экземпляры. На примере справа у нас есть изображение с двумя коровами, где они стоят рядом друг к другу. И когда мы говорим о семантических сегментация мы просто маркируем все пиксели независимо от того, к какой категории относится этот пиксель. Вывод не делает различий между этими двумя коровами. Вместо этого мы просто получаем массу пикселей, которые все помечены как корова.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture11_page-0021.jpg)

 Cемантическая сегментация может осуществляться посредством классификации. Итак, вы можете использовать идею скользящего окна. Мы берем наше входное изображение и разбиваем его на множество мелких крошечных локальных изображений. В этом примере мы взяли может быть, три обрезанных изображения головы коровы и применили к ним алгоритмы классификации, которые применяли к целым изображениям.

Возможно окно будет супер-супер долго бегать вперед и назад.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture11_page-0023.jpg)

Поэтому вместо извлечения отдельных участков изображения, мы можем представить себе, что наша сеть станет огромной гигантской стопкой сверточных слоев без полностью связанных слоёв или что-то ещё, поэтому в этом случае у нас просто куча сверточных слоев.


С оригинальной лекцией можно ознакомиться на [YouTube](https://youtu.be/nDPWywWRIRo).
