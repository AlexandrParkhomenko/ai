# 4\. Введение в нейронные сети

В прошлый раз мы выяснили, как работает функция потерь и оптимизация, а также рассказали о пользе градиента и градиентного спуска. Сегодня поговорим о методе обратного распространения ошибки и узнаем, как устроены нейронные сети.

Вспомним определение классификатора, на котором мы остановились. Функция **f** принимает данные **x** и параметры **W** на вход и выдаёт вектор оценок **s** для каждой из категорий, которые вы хотите классифицировать. Также у нас есть функция потерь **L** (например, SVM), определяющая, насколько правильные получились оценки. С её помощью мы можем вычислить потери данных. Узнать «простоту» модели помогает регуляризация. 

Наша цель — найти параметры **W**, соответствующие наименьшим потерям. Для этого мы используем отрицательное направление градиента функции **L**, который указывает путь к её минимуму.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0005.jpg)

Мы выяснили, что есть два основных способа вычислить градиент: аналитический и числовой. Числовой метод достаточно прост, но работает очень медленно и выдаёт приблизительные значения. Аналитический градиент более точный и быстрый, но в при его подсчёте можно легко допустить ошибки. Чтобы избежать этого и легко вычислять градиент даже для сложных функций, лучше всего использовать **вычислительные графы**.

## Введение в теорию графов и backpropagation

Вычислительный граф — это иллюстрированная запись какой-либо функции, состоящая из вершин и рёбер. Вершины (иногда их ещё называют узлы) — вычислительные операции, которые необходимо выполнить, а рёбра связывают их в определённую последовательность. 

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0008.jpg)

На рисунке выше изображён пример графа с нашим классификатором. Узел с операцией (*) означает умножение матриц параметров **W** и данных **x**, результатом которого является вектор весов **s**. Следующая вершина зависимых потерь (hinge loss) определяет потери данных **L**. Узел **R** вычисляет регуляризацию. И, наконец, в самом конце мы получаем общие потери, суммируя регуляризацию и потери данных.

Преимущество графов в том, что они позволяют использовать так называемый метод обратного распространения ошибки (backpropagation). Этот алгоритм рекурсивно использует [правило дифференцирования сложной функции](https://ru.wikipedia.org/wiki/%D0%94%D0%B8%D1%84%D1%84%D0%B5%D1%80%D0%B5%D0%BD%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%BE%D0%B9_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8) для вычисления градиента каждой переменной в графе. Метод становится очень полезным для действительно сложных функций, которые применяются в свёрточных нейросетях. Рассмотрим, как он работает.

Начнём с простого примера:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0012.jpg)

У нас есть функция **_f(x,y,z) = (x+y)z_** и мы хотим найти её градиенты по отношению к каждой из переменных. На вычислительном графе отражены операции **_x+y_** _и_ **_(x+y)z_** в виде вершин сложения (+) и умножения (*). Для примера взяты значения **_x = − 2, y = 5, z = −4_** , поэтому: **_−2 + 5 = 3_** (промежуточный результат после узла сложения), а **_3 * (−4) = −12_** (результат умножения).

Обозначим какими-нибудь буквами наши промежуточные значения. Пусть переменная после (+) называется **_q_** (**_q = x+y_**), тогда функция **_f_** будет равна **_qz_** . Выпишем градиенты (частные производные): градиент **_q_** зависит от переменных **_x_** и **_y_**, а градиент **_f_** — от **_q_** и **z**. Поскольку мы ищем градиент исходной функции **_f_**, которая зависит от всех трёх переменных **_x_**, **_y_** и **_z_**, то нам хотелось бы найти **_df/dx_**, **_df/dy_** и **_df/dz_**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0013.jpg)

Частные производные **_dq/dx_** и **_dq/dy_** равны единице — это означает, что при любом изменении **_x_** или **_y_** **_q_** изменится точно так же.

Попробуем рекурсивно воспользоваться правилом дифференцирования сложной функции. Начнём с конца вычислительного графа и будем двигаться к началу, по пути считая все градиенты. Самое последнее полученное нами значение — функция **_f_** и её промежуточный результат **_−12_**, частная производная по которому равна единице. Далее идёт переменная **_z_**, и мы знаем, что **_df/dz = q = 3\._** Переходим к вершине (+) и переменной **_q_** : производная **_df/dq_** равна **_z_**, то есть **_−4_**. 

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0015.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0017.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0019.jpg)

Получилось довольно интересно: в двух рёбрах, входящих в вершину умножения, градиент каждого из входов равен значению другого входа, умноженному на предыдущий градиент. И это не совпадение — можете взять правило в синей рамке на заметку.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0021.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0023.jpg)

Теперь мы приблизились к производной **_df/dy_**. Правило дифференцирования сложной функции говорит, что она может быть записана как **_df/dq * dq/dy_**. Значения **_df/dq_** и **_dq/dy_** нам известны: они равны **_−4_** и **_1_** соответственно, поэтому производная **_df/dy_** равна **_−4_**. Точно так же это работает и для **_df/dx_**:

Здесь мы нашли ещё одну закономерность: в вершине сложения оказались равными все три градиента: и самой вершины, и входящие в неё. Обратите внимание: значение производной для узла, который следует за текущим, называется **_восходящий градиент_**, а для предыдущего узла — **_локальный градиент_**. Например, для (+) восходящим градиентом является производная **_df/df_**, а локальным — производные **_df/dx_** и **_df/dy_**.

Мы справились с задачей и нашли градиенты каждой переменной. Метод обратного распространения ошибки может сильно упрощать вычисление градиентов для громоздких функций. Убедимся в этом на более сложном примере.

## Обратное распространение ошибки: продвинутый уровень

Возьмём функцию **_f_**, вычислительный граф которой выглядит следующим образом:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0030.jpg)

Здесь сходу посчитать производные уже не так просто, как в предыдущем случае. Нам нужно найти градиенты **_df/dw0_**, **_df/dw1_**, **_df/dw2_**, **_df/dx0_** и **_df/dx1_**. Для вычислений возьмём тестовые значения **_w0 = 2_**, **_w1 = −3**, **_w2 = −3_**, **_x0 = −1_** и **_x1 = −2_**. Если подставить их в функцию, то на выходе получим **_f = 0.73_**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0031.jpg)

Снова начнём с конца и посчитаем производную функции **_df/df_**, которая, очевидно, равна единице.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0033.jpg)

Движемся дальше — первая вершина содержит сложную функцию **_1/x_**. Можно воспользоваться таблицей [производных](https://www.webmath.ru/poleznoe/formules_8_6.php) и вспомнить, что **_1/x = x<sup>-1</sup>_**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0035.jpg)

В следующем узле находится **_c+x_**, производная которой, как вы помните, равна **c**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0037.jpg)

Третья вершина — это экспонента в степени **_x_**.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0039.jpg)

Четвёртая — умножение на **_−1_**. 

Заметьте, что на каждом шаге мы умножаем локальный градиент на восходящий, чтобы выполнялось правило дифференцирования. Локальные градиенты считаются по формулам в нижней части рисунка.

Мы подошли к узлу сложения с двумя входящими в него рёбрами, для которого известен восходящий градиент **_0.2_**. В предыдущем простом примере мы выяснили, что в вершине сложения градиент по отношению к каждому из входов будет равен 1\. Поэтому мы просто берём единицу (локальный градиент) и умножаем её на восходящий градиент для обоих рёбер:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0041.jpg)

Точно так же выполняем и со следующим узлом сложения. 

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0043.jpg)

Продвинемся к первым двум переменным: **_w0_** и **_x0_**. Перед ними находится вершина умножения. Ранее мы обнаружили, что в этом случае градиент по отношению к одному из входов просто является значением другого входа, умноженным на восходящий градиент. Поэтому **_df/dw0 = 2 * 0.2 = 0.4_**, а **_df/dw0 = (−1)*0.2 =  −0.2_**. 

Таким же образом разбираемся со второй вершиной напротив **_w1_** и **_x1_**:
**_x1 = -3 * 0.2 = -0.6_** ; **_w1 = -2 * 0.2 = -0.4_**

Вот и всё! Было непросто, но мы справились.

## Прокачайте свой граф

Во всех предыдущих случаях мы разбивали граф на множество простейших вершин. Но на самом деле это не всегда необходимо: вершины можно группировать друг с другом, если они образуют какую-нибудь легко дифференцируемую функцию. Это может существенно сократить и упростить вычисления.

Возьмём, например, [сигмоиду](https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%B3%D0%BC%D0%BE%D0%B8%D0%B4%D0%B0). Если присмотреться, то она похожа на функцию из предыдущего примера. У сигмоиды есть очень полезное свойство: её производная легко выражается через саму функцию.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0044.jpg)

Переменные степени экспоненты **_w0_**, **_w1_**, **_w2_**, **_x0_** и **_x1_** складываются в вершине сложения, которая следует сразу за **_w2_**. Поэтому мы можем взять ту часть графа, которая полностью совпадает с сигмоидой, и объединить несколько узлов в один. Проверим, что локальные градиенты в этом случае окажутся одинаковыми: просто подставим значение функции **_f_** в выражение производной сигмоиды:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0045.jpg)

Видим, что градиенты совпадают.

Всегда ли необходимо группировать вершины? Это зависит от того, что вам нужно больше: короткий граф со сложными вычислениями, или объёмный с более простыми.

Мы рассмотрели пример со скалярными величинами, но точно так же можно работать с векторами и матрицами: в этом случае на каждом шаге придётся считать матрицу производных, которая также называется [матрица Якоби](https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0_%D0%AF%D0%BA%D0%BE%D0%B1%D0%B8).

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0059.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0061.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0063.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0065.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0067.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0070.jpg)

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0073.jpg)

## Переносим вычислительный граф в код

Все наши предыдущие действия над вычислительными графами делятся на два больших этапа:

1.  Проходим по графу в прямом направлении, подставляя исходные значения переменных и считая все промежуточные результаты;
2.  Идём в обратном направлении, вычисляя градиенты и пользуясь правилом дифференцирования.

В коде их можно описать функциями **forward** и **backward**, которые будут выглядеть примерно так:

```python
class ComputationalGraph(object):
    #...
    def forward(inputs):
        # 1\. [передаём входные данные...]
        # 2\. следуем по графу в прямом направлении:
        for gate in self.graph.nodes_topologically_sorted():
            gate.forward()
        return loss # последняя вершина графа выводит потери
    def backward():
        for gate in reversed(self.graph.nodes_topologically_sorted()):
            gate.backward() # немного обратного распространения с правилом дифференцирования
        return inputs_gradients
```

При этом мы можем установить механику **forward** и **backward** для каждой вершины. Вот пример функции, которая обрабатывает узел умножения по установленным правилам:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0076.jpg)

Рассмотрим популярный фреймворк для глубокого обучения [Caffe](https://caffe.berkeleyvision.org/). Зайдём в его репозиторий и откроем папку [Layers](https://github.com/BVLC/caffe/tree/master/src/caffe/layers):

```
absval_layer, accuracy_layer, argmax_layer, base_conv_layer, base_data_layer, batch_norm_layer, batch_reindex_layer, bias_layer, bnll_layer, clip_layer, concat_layer, contrastive_loss_layer, conv_layer, crop_layer, cudnn_conv_layer, cudnn_deconv_layer, cudnn_lcn_layer, cudnn_lrn_layer, cudnn_pooling_layer, cudnn_relu_layer, cudnn_sigmoid_layer, cudnn_softmax_layer, cudnn_tanh_layer, data_layer, deconv_layer, dropout_layer, dummy_data_layer, eltwise_layer, elu_layer, embed_layer, euclidean_loss_layer, exp_layer, filter_layer, flatten_layer, hdf5_data_layer, hdf5_output_layer, hinge_loss_layer, im2col_layer, image_data_layer, infogain_loss_layer, inner_product_layer, input_layer, log_layer, loss_layer, lrn_layer, lstm_layer, lstm_unit_layer, memory_data_layer, multinomial_logistic_loss_layer, mvn_layer, neuron_layer, parameter_layer, pooling_layer, power_layer, prelu_layer, recurrent_layer, reduction_layer, relu_layer, reshape_layer, rnn_layer, scale_layer, sigmoid_cross_entropy_loss_layer, sigmoid_layer, silence_layer, slice_layer, softmax_layer, softmax_loss_layer, split_layer, spp_layer, swish_layer, tanh_layer, threshold_layer, tile_layer, window_data_layer
```

По сути, все эти слои представляют собой те же самые вершины вычислительного графа. Они могут быть более сложными, чем в наших примерах, но часто встречаются и стандартные функции. Вот, например, слой с сигмоидой:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0079.jpg)

Таким образом, почти любую нейросеть можно представить в виде графа. Теперь мы плавно приблизились к тому, как устроены архитектуры настоящих нейронных сетей.

## Нейронные сети: начинаем с малого

Очень часто люди сравнивают искусственные сети с настоящими нейронами в мозге. Мы обсудим это сравнение чуть позже и сначала посмотрим на нейросети, не погружаясь в биологические аналогии.

Вспомним функцию линейного классификатора, о которой мы так много говорили: **_f = Wx_**. Если мы захотим «превратить» её в нейросеть, то нам надо будет разделить параметры **_W_** на две части: **_W1_** и **_W2_** и применить одно линейное преобразование поверх другого:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0086.jpg)

Мы получили простую двухслойную нейросеть с двумя линейными слоями. На рисунке выше **_x_** — входные данные, **_h_** — промежуточная нелинейность (мы поговорим о ней чуть позже) и **_s_** — выходной вектор оценок. В более широком смысле, нейронные сети — это сложные функции, состоящие из простых. 

В одной из предыдущих лекций мы упоминали, что каждая строка весовой матрицы **_W_** является шаблоном одного из классов. Эти шаблоны выглядели, как некий усреднённый объект (строка цветных изображений на 86 слайде).

Также мы говорили о том, что с такими единичными шаблонами возникает проблема: например, в классе **“car”** машина окрашена в красный цвет, в то время как в реальных данных могут встречаться автомобили и других цветов. Многослойные сети решают этот вопрос: **_W1_** содержит те же единичные шаблоны, но теперь оценки для них хранятся в промежуточной нелинейной переменной **h**. И следующий слой **_W2_** объединит шаблоны с помощью взвешенной суммы, что позволит дать более точные оценки для машин других цветов и прочих разнообразных объектов.

Кстати, ничто не мешает нам добавить ещё один слой, чтобы повысить точность распознавания:

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0087.jpg)

Именно так появляются глубокие нейросети.

Для наглядности посмотрим на код простой двухслойной сети, которую можно реализовать всего в 20 строк:

```python
import numpy as np
from numpy.random import randn

N, D_in, H, D_out = 64, 1000, 100, 10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(H, D_out)

for t in range(2000):
    h = 1 / (1 + np.exp(-x.dot(w1)))
    y_pred = h.dot(w2)
    loss = np.square(y_pred - y).sum()
    print(t, loss)

    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h.T.dot(grad_y_pred)
    grad_h = grad_y_pred.dot(w2.T)
    grad_w1 = x.T.dot(grad_h * h * (1 - h))

    w1 -= 1e-4 * grad_w1
    w2 -= 1e-4 * grad_w2
```

## Artificial vs Biological

На самом деле искусственные и биологические нейронные сети имеют мало общего. Тем не менее, именно изучение человеческого мышления вдохновило учёных на создание AI. 

В нашем мозге присутствует огромное число нейронов, которые соединены друг с другом специальными «ветвями» — аксонами. Когда происходит какая-либо мыслительная деятельность, по нейронам проходят электрические импульсы, передающиеся от одних клеток к другим. У нейронов есть дендриты — это отростки, к которым приходят импульсы. Тело клетки объединяет в себе все поступающие сигналы и посылает их в следующие нейроны через свой аксон. Место, где происходит контакт двух нейронов, называется синапс.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0090.jpg)

Если мы посмотрим на вершины вычислительных графов, то сможем провести параллели: например, данные **_x<sub>i</sub>_** — это исходные импульсы, поступающие в узел через дендрит, а веса **_w<sub>i</sub>_** — синапсы. Тело клетки обрабатывает сигналы и отправляет их к следующим вершинам через выходной аксон с помощью функции активации.

![](https://raw.githubusercontent.com/AlexandrParkhomenko/ml/main/stanford/images/cs231n_2017_lecture4_page-0091.jpg)

Функция активации вычисляет выходное значение текущего нейрона в зависимости от результата взвешенной суммы. Сигмоида, кстати, — одна из её разновидностей. Это и есть та самая нелинейность, которая вводится между линейными слоями нейросети. В биологическом смысле она служит для приведения нейронов в активное состояние.

Хотя искусственные и настоящие нейроны кажутся очень похожими, всё-таки нужно учесть несколько важных моментов:

— биологические нейроны делятся на множество разновидностей;

— реальные дендриты могут выполнять сложные нелинейные вычисления;

— синапсы — это не просто веса, а сложная нелинейная динамическая система;

— обычной функции активации на практике может быть недостаточно.

Поэтому будьте осторожны с биологическими аналогиями.

В следующий раз мы поговорим о свёрточных нейросетях, обсудим их историю, особенности архитектуры и области применения.

С оригинальной лекцией можно ознакомиться на [YouTube](https://youtu.be/d14TUNcbn1k).
