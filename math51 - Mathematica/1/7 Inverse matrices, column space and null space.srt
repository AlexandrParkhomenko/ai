1
00:00:00,199 --> 00:00:15,160
As you can probably tell by now, the bulk
of this series is on understanding matrix

2
00:00:15,160 --> 00:00:16,470
and vector operations

3
00:00:16,470 --> 00:00:20,040
through that more visual lens of linear transformations.

4
00:00:20,040 --> 00:00:24,020
This video is no exception, describing the
concepts of inverse matrices,

5
00:00:24,020 --> 00:00:28,100
column space, rank, and null space through
that lens.

6
00:00:28,100 --> 00:00:32,229
A forewarning though: I'm not going to talk
about the methods for actually computing these

7
00:00:32,229 --> 00:00:33,230
things,

8
00:00:33,230 --> 00:00:34,910
and some would argue that that's pretty important.

9
00:00:34,910 --> 00:00:38,940
There are a lot of very good resources for
learning those methods outside this series.

10
00:00:38,940 --> 00:00:42,290
Keywords: "Gaussian elimination" and "Row
echelon form."

11
00:00:42,290 --> 00:00:46,630
I think most of the value that I actually
have to add here is on the intuition half.

12
00:00:46,630 --> 00:00:50,940
Plus, in practice, we usually get software
to compute this stuff for us anyway.

13
00:00:50,940 --> 00:00:54,460
First, a few words on the usefulness of linear
algebra.

14
00:00:54,460 --> 00:00:57,989
By now, you already have a hint for how it's
used in describing the the manipulation of

15
00:00:57,989 --> 00:00:58,989
space,

16
00:00:58,989 --> 00:01:01,559
which is useful for things like computer graphics
and robotics,

17
00:01:01,559 --> 00:01:05,209
but one of the main reasons that linear algebra
is more broadly applicable,

18
00:01:05,209 --> 00:01:07,829
and required for just about any technical
discipline,

19
00:01:07,829 --> 00:01:11,490
is that it lets us solve certain systems of
equations.

20
00:01:11,490 --> 00:01:15,509
When I say "system of equations," I mean you
have a list of variables, things you don't

21
00:01:15,509 --> 00:01:16,509
know,

22
00:01:16,509 --> 00:01:18,000
and a list of equations relating them.

23
00:01:18,000 --> 00:01:21,939
In a lot of situations, those equations can
get very complicated,

24
00:01:21,939 --> 00:01:26,520
but, if you're lucky, they might take on a
certain special form.

25
00:01:26,520 --> 00:01:31,880
Within each equation, the only thing happening
to each variable is that it's scaled by some

26
00:01:31,880 --> 00:01:32,880
constant,

27
00:01:32,880 --> 00:01:36,159
and the only thing happening to each of those
scaled variables is that they're added to

28
00:01:36,159 --> 00:01:37,209
each other.

29
00:01:37,209 --> 00:01:43,560
So, no exponents or fancy functions, or multiplying
two variables together; things like that.

30
00:01:43,560 --> 00:01:46,649
The typical way to organize this sort of special
system of equations

31
00:01:46,649 --> 00:01:50,009
is to throw all the variables on the left,

32
00:01:50,009 --> 00:01:52,930
and put any lingering constants on the right.

33
00:01:52,930 --> 00:01:56,859
It's also nice to vertically line up the common
variables,

34
00:01:56,859 --> 00:02:00,249
and to do that you might need to throw in
some zero coefficients whenever the variable

35
00:02:00,249 --> 00:02:04,950
doesn't show up in one of the equations.

36
00:02:04,950 --> 00:02:08,169
This is called a "linear system of equations."

37
00:02:08,169 --> 00:02:11,370
You might notice that this looks a lot like
matrix vector multiplication.

38
00:02:11,370 --> 00:02:17,190
In fact, you can package all of the equations
together into a single vector equation,

39
00:02:17,190 --> 00:02:21,459
where you have the matrix containing all of
the constant coefficients, and a vector containing

40
00:02:21,459 --> 00:02:23,040
all of the variables,

41
00:02:23,040 --> 00:02:29,020
and their matrix vector product equals some
different constant vector.

42
00:02:29,020 --> 00:02:31,319
Let's name that constant matrix A,

43
00:02:31,319 --> 00:02:34,879
denote the vector holding the variables with
a boldface x,

44
00:02:34,879 --> 00:02:39,030
and call the constant vector on the right-hand
side v.

45
00:02:39,030 --> 00:02:42,370
This is more than just a notational trick
to get our system of equations written on

46
00:02:42,370 --> 00:02:43,549
one line.

47
00:02:43,549 --> 00:02:47,629
It sheds light on a pretty cool geometric
interpretation for the problem.

48
00:02:47,629 --> 00:02:52,910
The matrix A corresponds with some linear
transformation, so solving Ax = v

49
00:02:52,910 --> 00:02:57,470
means we're looking for a vector x which,
after applying the transformation, lands on

50
00:02:57,470 --> 00:02:58,470
v.

51
00:02:58,470 --> 00:03:02,000
Think about what's happening here for a moment.

52
00:03:02,000 --> 00:03:07,129
You can hold in your head this really complicated
idea of multiple variables all intermingling

53
00:03:07,129 --> 00:03:08,129
with each other

54
00:03:08,129 --> 00:03:11,769
just by thinking about squishing and morphing
space and trying to figure out which vector

55
00:03:11,769 --> 00:03:13,140
lands on another.

56
00:03:13,140 --> 00:03:14,849
Cool, right?

57
00:03:14,849 --> 00:03:19,079
To start simple, let's say you have a system
with two equations and two unknowns.

58
00:03:19,079 --> 00:03:21,909
This means that the matrix A is a 2x2 matrix,

59
00:03:21,909 --> 00:03:24,819
and v and x are each two dimensional vectors.

60
00:03:24,819 --> 00:03:28,489
Now, how we think about the solutions to this
equation

61
00:03:28,489 --> 00:03:33,879
depends on whether the transformation associated
with A squishes all of space into a lower

62
00:03:33,879 --> 00:03:34,879
dimension,

63
00:03:34,879 --> 00:03:35,879
like a line or a point,

64
00:03:35,879 --> 00:03:40,780
or if it leaves everything spanning the full
two dimensions where it started.

65
00:03:40,780 --> 00:03:45,650
In the language of the last video, we subdivide
into the case where A has zero determinant,

66
00:03:45,650 --> 00:03:51,689
and the case where A has nonzero determinant.

67
00:03:51,689 --> 00:03:55,109
Let's start with the most likely case, where
the determinant is nonzero,

68
00:03:55,109 --> 00:03:58,650
meaning space does not get squished into a
zero area region.

69
00:03:58,650 --> 00:04:03,409
In this case, there will always be one and
only one vector that lands on v,

70
00:04:03,409 --> 00:04:07,420
and you can find it by playing the transformation
in reverse.

71
00:04:07,420 --> 00:04:10,219
Following where v goes as we rewind the tape
like this,

72
00:04:10,219 --> 00:04:15,900
you'll find the vector x such that A times
x equals v.

73
00:04:15,900 --> 00:04:20,500
When you play the transformation in reverse,
it actually corresponds to a separate linear

74
00:04:20,500 --> 00:04:21,500
transformation,

75
00:04:21,500 --> 00:04:23,380
commonly called "the inverse of A"

76
00:04:23,380 --> 00:04:25,440
denoted A to the negative one.

77
00:04:25,440 --> 00:04:28,640
For example, if A was a counterclockwise rotation
by 90ยบ

78
00:04:28,640 --> 00:04:34,780
then the inverse of A would be a clockwise
rotation by 90ยบ.

79
00:04:34,780 --> 00:04:38,380
If A was a rightward shear that pushes j-hat
one unit to the right,

80
00:04:38,380 --> 00:04:43,090
the inverse of a would be a leftward shear
that pushes j-hat one unit to the left.

81
00:04:43,090 --> 00:04:48,970
In general, A inverse is the unique transformation
with the property that if you first apply

82
00:04:48,970 --> 00:04:49,970
A,

83
00:04:49,970 --> 00:04:52,370
then follow it with the transformation A inverse,

84
00:04:52,370 --> 00:04:54,760
you end up back where you started.

85
00:04:54,760 --> 00:04:59,640
Applying one transformation after another
is captured algebraically with matrix multiplication,

86
00:04:59,640 --> 00:05:05,160
so the core property of this transformation
A inverse is that A inverse times A

87
00:05:05,160 --> 00:05:08,190
equals the matrix that corresponds to doing
nothing.

88
00:05:08,190 --> 00:05:11,850
The transformation that does nothing is called
the "identity transformation."

89
00:05:11,850 --> 00:05:15,060
It leaves i-hat and j-hat each where they
are, unmoved,

90
00:05:15,060 --> 00:05:20,170
so its columns are one, zero, and zero, one.

91
00:05:20,170 --> 00:05:23,600
Once you find this inverse, which, in practice,
you do with a computer,

92
00:05:23,600 --> 00:05:30,100
you can solve your equation by multiplying
this inverse matrix by v.

93
00:05:30,100 --> 00:05:34,610
And again, what this means geometrically is
that you're playing the transformation in

94
00:05:34,610 --> 00:05:40,550
reverse, and following v.

95
00:05:40,550 --> 00:05:44,650
This nonzero determinant case, which for a
random choice of matrix is by far the most

96
00:05:44,650 --> 00:05:46,080
likely one,

97
00:05:46,080 --> 00:05:49,690
corresponds with the idea that if you have
two unknowns and two equations,

98
00:05:49,690 --> 00:05:54,160
it's almost certainly the case that there's
a single, unique solution.

99
00:05:54,160 --> 00:05:56,190
This idea also makes sense in higher dimensions,

100
00:05:56,190 --> 00:05:59,020
when the number of equations equals the number
of unknowns.

101
00:05:59,020 --> 00:06:04,130
Again, the system of equations can be translated
to the geometric interpretation

102
00:06:04,130 --> 00:06:08,470
where you have some transformation, A,

103
00:06:08,470 --> 00:06:09,680
and some vector, v,

104
00:06:09,680 --> 00:06:16,080
and you're looking for the vector x that lands
on v.

105
00:06:16,080 --> 00:06:20,480
As long as the transformation A doesn't squish
all of space into a lower dimension,

106
00:06:20,480 --> 00:06:22,900
meaning, its determinant is nonzero,

107
00:06:22,900 --> 00:06:26,060
there will be an inverse transformation, A
inverse,

108
00:06:26,060 --> 00:06:28,360
with the property that if you first do A,

109
00:06:28,360 --> 00:06:30,020
then you do A inverse,

110
00:06:30,020 --> 00:06:33,750
it's the same as doing nothing.

111
00:06:33,750 --> 00:06:38,270
And to solve your equation, you just have
to multiply that reverse transformation matrix

112
00:06:38,270 --> 00:06:43,660
by the vector v.

113
00:06:43,660 --> 00:06:47,610
But when the determinant is zero, and the
transformation associated with this system

114
00:06:47,610 --> 00:06:48,610
of equations

115
00:06:48,610 --> 00:06:52,500
squishes space into a smaller dimension, there
is no inverse.

116
00:06:52,500 --> 00:06:55,630
You cannot un-squish a line to turn it into
a plane.

117
00:06:55,630 --> 00:06:58,490
At least, that's not something that a function
can do.

118
00:06:58,490 --> 00:07:01,060
That would require transforming each individual
vector

119
00:07:01,060 --> 00:07:03,860
into a whole line full of vectors.

120
00:07:03,860 --> 00:07:07,860
But functions can only take a single input
to a single output.

121
00:07:07,860 --> 00:07:11,160
Similarly, for three equations in three unknowns,

122
00:07:11,160 --> 00:07:14,360
there will be no inverse if the corresponding
transformation

123
00:07:14,360 --> 00:07:17,030
squishes 3D space onto the plane,

124
00:07:17,030 --> 00:07:20,140
or even if it squishes it onto a line, or
a point.

125
00:07:20,140 --> 00:07:22,420
Those all correspond to a determinant of zero,

126
00:07:22,420 --> 00:07:27,120
since any region is squished into something
with zero volume.

127
00:07:27,120 --> 00:07:31,150
It's still possible that a solution exists
even when there is no inverse,

128
00:07:31,150 --> 00:07:35,070
it's just that when your transformation squishes
space onto, say, a line,

129
00:07:35,070 --> 00:07:43,490
you have to be lucky enough that the vector
v lives somewhere on that line.

130
00:07:43,490 --> 00:07:48,870
You might notice that some of these zero determinant
cases feel a lot more restrictive than others.

131
00:07:48,870 --> 00:07:53,400
Given a 3x3 matrix, for example, it seems
a lot harder for a solution to exist

132
00:07:53,400 --> 00:07:58,190
when it squishes space onto a line compared
to when it squishes things onto a plane,

133
00:07:58,190 --> 00:08:02,750
even though both of those are zero determinant.

134
00:08:02,750 --> 00:08:06,630
We have some language that's a bit more specific
than just saying "zero determinant."

135
00:08:06,630 --> 00:08:10,990
When the output of a transformation is a line,
meaning it's one-dimensional,

136
00:08:10,990 --> 00:08:15,300
we say the transformation has a "rank" of
one.

137
00:08:15,300 --> 00:08:18,170
If all the vectors land on some two-dimensional
plane,

138
00:08:18,170 --> 00:08:23,060
We say the transformation has a "rank" of
two.

139
00:08:23,060 --> 00:08:28,470
So the word "rank" means the number of dimensions
in the output of a transformation.

140
00:08:28,470 --> 00:08:33,169
For instance, in the case of 2x2 matrices,
rank 2 is the best that it can be.

141
00:08:33,169 --> 00:08:37,819
It means the basis vectors continue to span
the full two dimensions of space, and the

142
00:08:37,820 --> 00:08:39,640
determinant is nonzero.

143
00:08:39,640 --> 00:08:43,559
But for 3x3 matrices, rank 2 means that we've
collapsed,

144
00:08:43,559 --> 00:08:47,290
but not as much as they would have collapsed
for a rank 1 situation.

145
00:08:47,290 --> 00:08:52,500
If a 3D transformation has a nonzero determinant,
and its output fills all of 3D space,

146
00:08:52,500 --> 00:08:54,650
it has a rank of 3.

147
00:08:54,650 --> 00:08:57,210
This set of all possible outputs for your
matrix,

148
00:08:57,210 --> 00:09:00,180
whether it's a line, a plane, 3D space, whatever,

149
00:09:00,180 --> 00:09:04,450
is called the "column space" of your matrix.

150
00:09:04,450 --> 00:09:06,760
You can probably guess where that name comes
from.

151
00:09:06,760 --> 00:09:11,190
The columns of your matrix
tell you where the basis vectors land,

152
00:09:11,190 --> 00:09:15,780
and the span of those transformed basis
vectors gives you all possible outputs.

153
00:09:15,780 --> 00:09:22,260
In other words, the column space is the
span of the columns of your matrix.

154
00:09:22,260 --> 00:09:26,070
So, a more precise definition of rank
would be that

155
00:09:26,070 --> 00:09:30,200
it's the number of dimensions in the column
space.

156
00:09:30,200 --> 00:09:32,360
When this rank is as high as it can be,

157
00:09:32,360 --> 00:09:38,090
meaning it equals the number of columns, we
call the matrix "full rank."

158
00:09:38,090 --> 00:09:42,740
Notice, the zero vector will always be
included in the column space,

159
00:09:42,740 --> 00:09:47,040
since linear transformations must keep the
origin fixed in place.

160
00:09:47,040 --> 00:09:51,680
For a full rank transformation, the only vector
that lands at the origin is the zero vector

161
00:09:51,680 --> 00:09:52,680
itself,

162
00:09:52,680 --> 00:09:56,430
but for matrices that aren't full rank,
which squish to a smaller dimension,

163
00:09:56,430 --> 00:10:02,150
you can have a whole bunch of vectors that
land on zero.

164
00:10:02,150 --> 00:10:05,090
If a 2D transformation squishes space onto
a line, for example,

165
00:10:05,090 --> 00:10:08,300
there is a separate line in a different direction,

166
00:10:08,300 --> 00:10:11,930
full of vectors that get squished onto the
origin.

167
00:10:11,930 --> 00:10:14,680
If a 3D transformation squishes space onto
a plane,

168
00:10:14,680 --> 00:10:20,800
there's also a full line of vectors that
land on the origin.

169
00:10:20,800 --> 00:10:24,270
If a 3D transformation squishes all the space
onto a line,

170
00:10:24,270 --> 00:10:33,390
then there's a whole plane full of vectors
that land on the origin.

171
00:10:33,390 --> 00:10:37,960
This set of vectors that lands on the
origin is called the "null space" or the "kernel"

172
00:10:37,960 --> 00:10:39,370
of your matrix.

173
00:10:39,370 --> 00:10:42,250
It's the space of all vectors that become
null,

174
00:10:42,250 --> 00:10:45,750
in the sense that they land on the zero vector.

175
00:10:45,750 --> 00:10:50,310
In terms of the linear system of
equations, when v happens to be the zero vector,

176
00:10:50,310 --> 00:10:56,910
the null space gives you all of
the possible solutions to the equation.

177
00:10:56,910 --> 00:10:58,470
So that's a very high-level overview

178
00:10:58,470 --> 00:11:02,400
of how to think about linear systems of equations
geometrically.

179
00:11:02,400 --> 00:11:05,160
Each system has
some kind of linear transformation associated

180
00:11:05,160 --> 00:11:06,160
with it,

181
00:11:06,160 --> 00:11:08,150
and when that
transformation has an inverse,

182
00:11:08,150 --> 00:11:11,740
you can use that inverse to solve your system.

183
00:11:11,740 --> 00:11:18,200
Otherwise, the idea of column space lets
us understand when a solution even exists,

184
00:11:18,200 --> 00:11:21,370
and the idea of a null space
helps us to understand what the set of

185
00:11:21,370 --> 00:11:25,110
all possible solutions can look like.

186
00:11:25,110 --> 00:11:27,620
Again there's a lot that I haven't
covered here,

187
00:11:27,620 --> 00:11:29,560
most notably how to compute these things.

188
00:11:29,560 --> 00:11:32,870
I also had to limit my scope to examples where
the number of equations

189
00:11:32,870 --> 00:11:35,170
equals the number of unknowns.

190
00:11:35,170 --> 00:11:37,480
But the goal here is not to try to teach everything;

191
00:11:37,480 --> 00:11:41,361
it's that you come away with
a strong intuition for inverse matrices, column

192
00:11:41,361 --> 00:11:43,300
space, and null space,

193
00:11:43,300 --> 00:11:47,800
and that those intuitions make any future
learning that you do more fruitful.

194
00:11:47,800 --> 00:11:51,980
Next video, by popular request, will be a
brief footnote about nonsquare matrices.

195
00:11:51,980 --> 00:11:55,130
Then, after that, I'm going to give you my
take on dot products,

196
00:11:55,130 --> 00:11:56,990
and something pretty cool that
happens when you view them

197
00:11:56,990 --> 00:12:00,550
under the light of linear transformations.

198
00:12:00,550 --> 00:12:05,570
See you then!

