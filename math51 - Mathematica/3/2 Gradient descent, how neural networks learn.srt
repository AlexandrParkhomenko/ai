1
00:00:04,070 --> 00:00:07,059
Last video I laid out the structure of a neural network

2
00:00:07,160 --> 00:00:10,089
I'll give a quick recap here just so that it's fresh in our minds

3
00:00:10,089 --> 00:00:15,368
And then I have two main goals for this video. The first is to introduce the idea of gradient descent,

4
00:00:15,650 --> 00:00:18,219
which underlies not only how neural networks learn,

5
00:00:18,220 --> 00:00:20,439
but how a lot of other machine learning works as well

6
00:00:20,660 --> 00:00:24,609
Then after that we're going to dig in a little more to how this particular network performs

7
00:00:24,609 --> 00:00:27,758
And what those hidden layers of neurons end up actually looking for

8
00:00:28,999 --> 00:00:33,489
As a reminder our goal here is the classic example of handwritten digit recognition

9
00:00:34,129 --> 00:00:36,129
the hello world of neural networks

10
00:00:36,500 --> 00:00:43,090
these digits are rendered on a 28 by 28 pixel grid each pixel with some grayscale value between 0 & 1

11
00:00:43,610 --> 00:00:46,089
those are what determine the activations of

12
00:00:46,850 --> 00:00:50,199
784 neurons in the input layer of the network and

13
00:00:50,840 --> 00:00:55,719
Then the activation for each neuron in the following layers is based on a weighted sum of

14
00:00:56,000 --> 00:01:00,639
All the activations in the previous layer plus some special number called a bias

15
00:01:01,699 --> 00:01:06,338
then you compose that sum with some other function like the sigmoid squishification or

16
00:01:06,400 --> 00:01:08,769
a ReLu the way that I walked through last video

17
00:01:09,110 --> 00:01:15,729
In total given the somewhat arbitrary choice of two hidden layers here with 16 neurons each the network has about

18
00:01:16,579 --> 00:01:24,159
13,000 weights and biases that we can adjust and it's these values that determine what exactly the network you know actually does

19
00:01:24,799 --> 00:01:28,328
Then what we mean when we say that this network classifies a given digit

20
00:01:28,329 --> 00:01:33,429
Is that the brightest of those 10 neurons in the final layer corresponds to that digit

21
00:01:33,950 --> 00:01:38,589
And remember the motivation that we had in mind here for the layered structure was that maybe

22
00:01:38,780 --> 00:01:44,680
The second layer could pick up on the edges and the third layer might pick up on patterns like loops and lines

23
00:01:44,930 --> 00:01:48,729
And the last one could just piece together those patterns to recognize digits

24
00:01:49,369 --> 00:01:52,029
So here we learn how the network learns

25
00:01:52,399 --> 00:01:57,099
What we want is an algorithm where you can show this network a whole bunch of training data

26
00:01:57,229 --> 00:02:03,669
which comes in the form of a bunch of different images of handwritten digits along with labels for what they're supposed to be and

27
00:02:03,890 --> 00:02:05,659
It'll adjust those

28
00:02:05,659 --> 00:02:09,789
13000 weights and biases so as to improve its performance on the training data

29
00:02:10,729 --> 00:02:13,569
Hopefully this layered structure will mean that what it learns

30
00:02:14,269 --> 00:02:16,719
generalizes to images beyond that training data

31
00:02:16,720 --> 00:02:20,289
And the way we test that is that after you train the network

32
00:02:20,290 --> 00:02:26,560
You show it more labeled theta that it's never seen before and you see how accurately it classifies those new images

33
00:02:31,040 --> 00:02:37,000
Fortunately for us and what makes this such a common example to start with is that the good people behind the MNIST base have

34
00:02:37,000 --> 00:02:44,289
put together a collection of tens of thousands of handwritten digit images each one labeled with the numbers that they're supposed to be and

35
00:02:44,720 --> 00:02:49,539
It's provocative as it is to describe a machine as learning once you actually see how it works

36
00:02:49,540 --> 00:02:55,359
It feels a lot less like some crazy sci-fi premise and a lot more like well a calculus exercise

37
00:02:55,390 --> 00:02:59,589
I mean basically it comes down to finding the minimum of a certain function

38
00:03:01,519 --> 00:03:05,199
Remember conceptually we're thinking of each neuron as being connected

39
00:03:05,390 --> 00:03:12,309
to all of the neurons in the previous layer and the weights in the weighted sum defining its activation are kind of like the

40
00:03:12,440 --> 00:03:14,060
strengths of those connections

41
00:03:14,060 --> 00:03:20,440
And the bias is some indication of whether that neuron tends to be active or inactive and to start things off

42
00:03:20,440 --> 00:03:26,919
We're just gonna initialize all of those weights and biases totally randomly needless to say this network is going to perform

43
00:03:26,919 --> 00:03:33,759
pretty horribly on a given training example since it's just doing something random for example you feed in this image of a 3 and the

44
00:03:33,760 --> 00:03:35,799
Output layer it just looks like a mess

45
00:03:36,349 --> 00:03:42,518
So what you do is you define a cost function a way of telling the computer: "No bad computer!

46
00:03:42,739 --> 00:03:50,529
That output should have activations which are zero for most neurons, but one for this neuron what you gave me is utter trash"

47
00:03:51,260 --> 00:03:56,530
To say that a little more mathematically what you do is add up the squares of the differences between

48
00:03:56,720 --> 00:04:01,419
each of those trash output activations and the value that you want them to have and

49
00:04:01,489 --> 00:04:04,599
This is what we'll call the cost of a single training example

50
00:04:05,599 --> 00:04:10,749
Notice this sum is small when the network confidently classifies the image correctly

51
00:04:12,199 --> 00:04:15,639
But it's large when the network seems like it doesn't really know what it's doing

52
00:04:18,329 --> 00:04:25,249
So then what you do is consider the average cost over all of the tens of thousands of training examples at your disposal

53
00:04:27,060 --> 00:04:34,310
This average cost is our measure for how lousy the network is and how bad the computer should feel, and that's a complicated thing

54
00:04:34,830 --> 00:04:38,960
Remember how the network itself was basically a function one that takes in

55
00:04:39,540 --> 00:04:45,890
784 numbers as inputs the pixel values and spits out ten numbers as its output and in a sense

56
00:04:45,890 --> 00:04:48,770
It's parameterised by all these weights and biases

57
00:04:49,140 --> 00:04:54,020
While the cost function is a layer of complexity on top of that it takes as its input

58
00:04:54,450 --> 00:05:02,059
those thirteen thousand or so weights and biases and it spits out a single number describing how bad those weights and biases are and

59
00:05:02,340 --> 00:05:08,749
The way it's defined depends on the network's behavior over all the tens of thousands of pieces of training data

60
00:05:09,150 --> 00:05:11,150
That's a lot to think about

61
00:05:12,000 --> 00:05:15,619
But just telling the computer what a crappy job, it's doing isn't very helpful

62
00:05:15,900 --> 00:05:19,819
You want to tell it how to change those weights and biases so that it gets better?

63
00:05:20,820 --> 00:05:25,129
To make it easier rather than struggling to imagine a function with 13,000 inputs

64
00:05:25,130 --> 00:05:30,409
Just imagine a simple function that has one number as an input and one number as an output

65
00:05:30,960 --> 00:05:34,999
How do you find an input that minimizes the value of this function?

66
00:05:36,270 --> 00:05:40,039
Calculus students will know that you can sometimes figure out that minimum explicitly

67
00:05:40,260 --> 00:05:43,879
But that's not always feasible for really complicated functions

68
00:05:44,310 --> 00:05:52,160
Certainly not in the thirteen thousand input version of this situation for our crazy complicated neural network cost function

69
00:05:52,350 --> 00:05:59,029
A more flexible tactic is to start at any old input and figure out which direction you should step to make that output lower

70
00:06:00,120 --> 00:06:03,710
Specifically if you can figure out the slope of the function where you are

71
00:06:04,020 --> 00:06:09,619
Then shift to the left if that slope is positive and shift the input to the right if that slope is negative

72
00:06:12,130 --> 00:06:16,799
If you do this repeatedly at each point checking the new slope and taking the appropriate step

73
00:06:16,800 --> 00:06:20,039
you're gonna approach some local minimum of the function and

74
00:06:20,280 --> 00:06:24,080
the image you might have in mind here is a ball rolling down a hill and

75
00:06:24,400 --> 00:06:30,900
Notice even for this really simplified single input function there are many possible valleys that you might land in

76
00:06:31,540 --> 00:06:36,220
Depending on which random input you start at and there's no guarantee that the local minimum

77
00:06:36,580 --> 00:06:39,040
You land in is going to be the smallest possible value of the cost function

78
00:06:39,610 --> 00:06:44,009
That's going to carry over to our neural network case as well, and I also want you to notice

79
00:06:44,010 --> 00:06:47,190
How if you make your step sizes proportional to the slope

80
00:06:47,620 --> 00:06:54,540
Then when the slope is flattening out towards the minimum your steps get smaller and smaller and that kind of helps you from overshooting

81
00:06:55,720 --> 00:07:00,449
Bumping up the complexity a bit imagine instead a function with two inputs and one output

82
00:07:01,120 --> 00:07:07,739
You might think of the input space as the XY plane and the cost function as being graphed as a surface above it

83
00:07:08,230 --> 00:07:15,060
Now instead of asking about the slope of the function you have to ask which direction should you step in this input space?

84
00:07:15,310 --> 00:07:22,440
So as to decrease the output of the function most quickly in other words. What's the downhill direction?

85
00:07:22,440 --> 00:07:25,379
And again it's helpful to think of a ball rolling down that hill

86
00:07:26,260 --> 00:07:34,080
Those of you familiar with multivariable calculus will know that the gradient of a function gives you the direction of steepest ascent

87
00:07:34,750 --> 00:07:38,459
Basically, which direction should you step to increase the function most quickly

88
00:07:39,100 --> 00:07:46,439
naturally enough taking the negative of that gradient gives you the direction to step that decreases the function most quickly and

89
00:07:47,020 --> 00:07:53,400
Even more than that the length of this gradient vector is actually an indication for just how steep that steepest slope is

90
00:07:54,130 --> 00:07:56,280
Now if you're unfamiliar with multivariable calculus

91
00:07:56,280 --> 00:08:00,239
And you want to learn more check out some of the work that I did for Khan Academy on the topic

92
00:08:00,910 --> 00:08:03,779
Honestly, though all that matters for you and me right now

93
00:08:03,780 --> 00:08:09,419
Is that in principle there exists a way to compute this vector. This vector that tells you what the

94
00:08:09,520 --> 00:08:15,900
Downhill direction is and how steep it is you'll be okay if that's all you know and you're not rock solid on the details

95
00:08:16,790 --> 00:08:24,580
because if you can get that the algorithm from minimizing the function is to compute this gradient direction then take a small step downhill and

96
00:08:24,740 --> 00:08:26,740
Just repeat that over and over

97
00:08:27,800 --> 00:08:34,600
It's the same basic idea for a function that has 13,000 inputs instead of two inputs imagine organizing all

98
00:08:35,330 --> 00:08:39,400
13,000 weights and biases of our network into a giant column vector

99
00:08:39,679 --> 00:08:43,869
The negative gradient of the cost function is just a vector

100
00:08:43,880 --> 00:08:49,299
It's some Direction inside this insanely huge input space that tells you which

101
00:08:49,400 --> 00:08:55,030
nudges to all of those numbers is going to cause the most rapid decrease to the cost function and

102
00:08:55,460 --> 00:08:58,150
of course with our specially designed cost function

103
00:08:58,580 --> 00:09:04,900
Changing the weights and biases to decrease it means making the output of the network on each piece of training data

104
00:09:05,180 --> 00:09:10,599
Look less like a random array of ten values and more like an actual decision that we want it to make

105
00:09:11,030 --> 00:09:16,030
It's important to remember this cost function involves an average over all of the training data

106
00:09:16,370 --> 00:09:20,590
So if you minimize it it means it's a better performance on all of those samples

107
00:09:23,780 --> 00:09:30,849
The algorithm for computing this gradient efficiently which is effectively the heart of how a neural network learns is called back propagation

108
00:09:31,190 --> 00:09:34,690
And it's what I'm going to be talking about next video

109
00:09:34,690 --> 00:09:36,690
There I really want to take the time to walk through

110
00:09:36,830 --> 00:09:41,439
What exactly happens to each weight and each bias for a given piece of training data?

111
00:09:41,810 --> 00:09:46,960
Trying to give an intuitive feel for what's happening beyond the pile of relevant calculus and formulas

112
00:09:47,510 --> 00:09:52,179
Right here right now the main thing. I want you to know independent of implementation details

113
00:09:52,180 --> 00:09:58,479
is that what we mean when we talk about a network learning is that it's just minimizing a cost function and

114
00:09:58,940 --> 00:10:04,479
Notice one consequence of that is that it's important for this cost function to have a nice smooth output

115
00:10:04,480 --> 00:10:07,810
So that we can find a local minimum by taking little steps downhill

116
00:10:08,810 --> 00:10:10,520
This is why by the way

117
00:10:10,520 --> 00:10:16,749
Artificial neurons have continuously ranging activations rather than simply being active or inactive in a binary way

118
00:10:16,750 --> 00:10:18,750
if the way that biological neurons are

119
00:10:19,940 --> 00:10:26,770
This process of repeatedly nudging an input of a function by some multiple of the negative gradient is called gradient descent

120
00:10:26,930 --> 00:10:32,380
It's a way to converge towards some local minimum of a cost function basically a valley in this graph

121
00:10:32,930 --> 00:10:38,890
I'm still showing the picture of a function with two inputs of course because nudges in a thirteen thousand dimensional input

122
00:10:38,890 --> 00:10:44,049
Space are a little hard to wrap your mind around, but there is actually a nice non-spatial way to think about this

123
00:10:44,630 --> 00:10:51,340
Each component of the negative gradient tells us two things the sign of course tells us whether the corresponding

124
00:10:51,830 --> 00:10:59,139
Component of the input vector should be nudged up or down, but importantly the relative magnitudes of all these components

125
00:10:59,840 --> 00:11:02,530
Kind of tells you which changes matter more

126
00:11:05,150 --> 00:11:09,340
You see in our network an adjustment to one of the weights might have a much greater

127
00:11:09,710 --> 00:11:12,939
impact on the cost function than the adjustment to some other weight

128
00:11:14,450 --> 00:11:17,950
Some of these connections just matter more for our training data

129
00:11:18,920 --> 00:11:22,690
So a way that you can think about this gradient vector of our mind-warpingly

130
00:11:22,690 --> 00:11:27,999
massive cost function is that it encodes the relative importance of each weight and bias

131
00:11:28,250 --> 00:11:32,200
That is which of these changes is going to carry the most bang for your buck

132
00:11:33,560 --> 00:11:36,460
This really is just another way of thinking about direction

133
00:11:36,860 --> 00:11:41,290
To take a simpler example if you have some function with two variables as an input and you

134
00:11:41,690 --> 00:11:46,540
Compute that its gradient at some particular point comes out as (3,1)

135
00:11:47,420 --> 00:11:51,670
Then on the one hand you can interpret that as saying that when you're standing at that input

136
00:11:52,070 --> 00:11:55,150
moving along this direction increases the function most quickly

137
00:11:55,460 --> 00:12:02,229
That when you graph the function above the plane of input points that vector is what's giving you the straight uphill direction

138
00:12:02,600 --> 00:12:06,580
But another way to read that is to say that changes to this first variable

139
00:12:06,740 --> 00:12:13,390
Have three times the importance as changes to the second variable that at least in the neighborhood of the relevant input

140
00:12:13,520 --> 00:12:16,689
Nudging the x value carries a lot more bang for your buck

141
00:12:19,310 --> 00:12:19,930
All right

142
00:12:19,930 --> 00:12:24,940
Let's zoom out and sum up where we are so far the network itself is this function with

143
00:12:25,400 --> 00:12:29,859
784 inputs and 10 outputs defined in terms of all of these weighted sums

144
00:12:30,350 --> 00:12:34,780
the cost function is a layer of complexity on top of that it takes the

145
00:12:35,120 --> 00:12:41,870
13,000 weights and biases as inputs and spits out a single measure of lousyness based on the training examples and

146
00:12:42,180 --> 00:12:47,930
The gradient of the cost function is one more layer of complexity still it tells us

147
00:12:47,930 --> 00:12:53,839
What nudges to all of these weights and biases cause the fastest change to the value of the cost function

148
00:12:53,970 --> 00:12:57,680
Which you might interpret is saying which changes to which weights matter the most

149
00:13:02,550 --> 00:13:09,289
So when you initialize the network with random weights and biases and adjust them many times based on this gradient descent process

150
00:13:09,420 --> 00:13:12,949
How well does it actually perform on images that it's never seen before?

151
00:13:13,680 --> 00:13:19,609
Well the one that I've described here with the two hidden layers of sixteen neurons each chosen mostly for aesthetic reasons

152
00:13:20,579 --> 00:13:26,089
well, it's not bad it classifies about 96 percent of the new images that it sees correctly and

153
00:13:26,759 --> 00:13:32,239
Honestly, if you look at some of the examples that it messes up on you kind of feel compelled to cut it a little slack

154
00:13:35,759 --> 00:13:39,079
Now if you play around with the hidden layer structure and make a couple tweaks

155
00:13:39,079 --> 00:13:43,698
You can get this up to 98% and that's pretty good. It's not the best

156
00:13:43,740 --> 00:13:48,409
You can certainly get better performance by getting more sophisticated than this plain vanilla Network

157
00:13:48,569 --> 00:13:52,669
But given how daunting the initial task is I just think there's something?

158
00:13:52,889 --> 00:13:56,929
Incredible about any network doing this well on images that it's never seen before

159
00:13:57,389 --> 00:14:00,919
Given that we never specifically told it what patterns to look for

160
00:14:02,579 --> 00:14:07,068
Originally the way that I motivated this structure was by describing a hope that we might have

161
00:14:07,259 --> 00:14:09,739
That the second layer might pick up on little edges

162
00:14:09,809 --> 00:14:17,089
That the third layer would piece together those edges to recognize loops and longer lines and that those might be pieced together to recognize digits

163
00:14:17,699 --> 00:14:22,729
So is this what our network is actually doing? Well for this one at least

164
00:14:23,339 --> 00:14:24,449
Not at all

165
00:14:24,449 --> 00:14:27,409
remember how last video we looked at how the weights of the

166
00:14:27,480 --> 00:14:31,849
Connections from all of the neurons in the first layer to a given neuron in the second layer

167
00:14:31,980 --> 00:14:36,829
Can be visualized as a given pixel pattern that that second layer neuron is picking up on

168
00:14:37,350 --> 00:14:43,309
Well when we actually do that for the weights associated with these transitions from the first layer to the next

169
00:14:43,709 --> 00:14:50,209
Instead of picking up on isolated little edges here and there. They look well almost random

170
00:14:50,370 --> 00:14:56,399
Just put some very loose patterns in the middle there it would seem that in the unfathomably large

171
00:14:56,920 --> 00:15:02,580
13,000 dimensional space of possible weights and biases our network found itself a happy little local minimum that

172
00:15:02,860 --> 00:15:08,940
despite successfully classifying most images doesn't exactly pick up on the patterns that we might have hoped for and

173
00:15:09,430 --> 00:15:13,709
To really drive this point home watch what happens when you input a random image

174
00:15:14,019 --> 00:15:21,449
if the system was smart you might expect it to either feel uncertain maybe not really activating any of those 10 output neurons or

175
00:15:21,579 --> 00:15:23,200
Activating them all evenly

176
00:15:23,200 --> 00:15:24,820
But instead it

177
00:15:24,820 --> 00:15:32,010
Confidently gives you some nonsense answer as if it feels as sure that this random noise is a 5 as it does that an actual

178
00:15:32,010 --> 00:15:34,010
image of a 5 is a 5

179
00:15:34,180 --> 00:15:40,829
phrase differently even if this network can recognize digits pretty well it has no idea how to draw them a

180
00:15:41,500 --> 00:15:45,149
Lot of this is because it's such a tightly constrained training setup

181
00:15:45,149 --> 00:15:51,479
I mean put yourself in the network's shoes here from its point of view the entire universe consists of nothing

182
00:15:51,480 --> 00:15:57,539
But clearly defined unmoving digits centered in a tiny grid and its cost function just never gave it any

183
00:15:57,700 --> 00:16:00,959
Incentive to be anything, but utterly confident in its decisions

184
00:16:01,690 --> 00:16:05,070
So if this is the image of what those second layer neurons are really doing

185
00:16:05,140 --> 00:16:09,839
You might wonder why I would introduce this network with the motivation of picking up on edges and patterns

186
00:16:09,839 --> 00:16:11,969
I mean, that's just not at all what it ends up doing

187
00:16:13,029 --> 00:16:17,909
Well, this is not meant to be our end goal, but instead a starting point frankly

188
00:16:17,910 --> 00:16:19,120
This is old technology

189
00:16:19,120 --> 00:16:21,510
the kind researched in the 80s and 90s and

190
00:16:21,640 --> 00:16:29,129
You do need to understand it before you can understand more detailed modern variants and it clearly is capable of solving some interesting problems

191
00:16:29,410 --> 00:16:34,110
But the more you dig in to what those hidden layers are really doing the less intelligent it seems

192
00:16:38,530 --> 00:16:42,359
Shifting the focus for a moment from how networks learn to how you learn

193
00:16:42,580 --> 00:16:46,139
That'll only happen if you engage actively with the material here somehow

194
00:16:46,660 --> 00:16:53,100
One pretty simple thing that I want you to do is just pause right now and think deeply for a moment about what

195
00:16:53,440 --> 00:16:55,230
Changes you might make to this system

196
00:16:55,230 --> 00:17:00,719
And how it perceives images if you wanted it to better pick up on things like edges and patterns?

197
00:17:01,360 --> 00:17:04,410
But better than that to actually engage with the material

198
00:17:04,410 --> 00:17:05,079
I

199
00:17:05,079 --> 00:17:08,969
Highly recommend the book by Michael Nielsen on deep learning and neural networks

200
00:17:09,190 --> 00:17:14,369
In it you can find the code and the data to download and play with for this exact example

201
00:17:14,410 --> 00:17:18,089
And the book will walk you through step by step what that code is doing

202
00:17:18,910 --> 00:17:21,749
What's awesome is that this book is free and publicly available

203
00:17:22,359 --> 00:17:27,539
So if you do get something out of it consider joining me in making a donation towards Nielsen's efforts

204
00:17:27,910 --> 00:17:32,219
I've also linked a couple other resources that I like a lot in the description including the

205
00:17:32,470 --> 00:17:36,390
phenomenal and beautiful blog post by Chris Ola and the articles in distill

206
00:17:38,230 --> 00:17:40,200
To close things off here for the last few minutes

207
00:17:40,200 --> 00:17:43,740
I want to jump back into a snippet of the interview that I had with Leisha Lee

208
00:17:43,930 --> 00:17:49,079
You might remember her from the last video. She did her PhD work in deep learning and in this little snippet

209
00:17:49,080 --> 00:17:55,530
She talks about two recent papers that really dig into how some of the more modern image recognition networks are actually learning

210
00:17:55,810 --> 00:18:01,349
Just to set up where we were in the conversation the first paper took one of these particularly deep neural networks

211
00:18:01,350 --> 00:18:05,910
That's really good at image recognition and instead of training it on a properly labeled data

212
00:18:05,910 --> 00:18:08,579
Set it shuffled all of the labels around before training

213
00:18:08,800 --> 00:18:14,669
Obviously the testing accuracy here was going to be no better than random since everything's just randomly labeled

214
00:18:14,800 --> 00:18:20,879
But it was still able to achieve the same training accuracy as you would on a properly labeled dataset

215
00:18:21,490 --> 00:18:27,540
Basically the millions of weights for this particular network were enough for it to just memorize the random data

216
00:18:27,820 --> 00:18:34,379
Which kind of raises the question for whether minimizing this cost function actually corresponds to any sort of structure in the image?

217
00:18:34,380 --> 00:18:36,380
Or is it just you know?

218
00:18:36,520 --> 00:18:37,420
memorize the entire

219
00:18:37,420 --> 00:18:43,859
Data set of what the correct classification is and so a couple of you know half a year later at ICML this year

220
00:18:44,470 --> 00:18:49,039
There was not exactly rebuttal paper paper that addressed some asked like hey

221
00:18:49,470 --> 00:18:55,279
Actually these networks are doing something a little bit smarter than that if you look at that accuracy curve

222
00:18:55,279 --> 00:18:57,499
if you were just training on a

223
00:18:58,259 --> 00:19:05,179
Random data set that curve sort of went down very you know very slowly in almost kind of a linear fashion

224
00:19:05,179 --> 00:19:09,589
So you're really struggling to find that local minima of possible

225
00:19:09,590 --> 00:19:15,289
you know the right weights that would get you that accuracy whereas if you're actually training on a structured data set one that has the

226
00:19:15,289 --> 00:19:21,439
Right labels. You know you fiddle around a little bit in the beginning, but then you kind of dropped very fast to get to that

227
00:19:22,200 --> 00:19:26,149
Accuracy level and so in some sense it was easier to find that

228
00:19:26,759 --> 00:19:33,949
Local maxima and so it was also interesting about that is it caught brings into light another paper from actually a couple of years ago

229
00:19:34,080 --> 00:19:36,080
Which has a lot more

230
00:19:36,990 --> 00:19:39,169
simplifications about the network layers

231
00:19:39,169 --> 00:19:46,788
But one of the results was saying how if you look at the optimization landscape the local minima that these networks tend to learn are

232
00:19:47,340 --> 00:19:54,079
Actually of equal quality so in some sense if your data set is structure, and you should be able to find that much more easily

233
00:19:58,139 --> 00:20:01,189
My thanks as always to those of you supporting on patreon

234
00:20:01,190 --> 00:20:06,950
I've said before just what a game-changer patreon is but these videos really would not be possible without you I

235
00:20:07,230 --> 00:20:12,889
Also want to give a special. Thanks to the VC firm amplifi partners in their support of these initial videos in the series

236
00:20:13,470 --> 00:20:17,149
They focus on very early stage machine learning and AI companies

237
00:20:17,309 --> 00:20:19,110
and I feel pretty confident in the

238
00:20:19,110 --> 00:20:23,899
Probabilities that some of you watching this and even more likely some of the people that you know are

239
00:20:24,210 --> 00:20:27,949
right now in the early stages of getting such a company off the ground and

240
00:20:28,230 --> 00:20:31,279
The amplifi folks would love to hear from any such founders

241
00:20:31,279 --> 00:20:37,069
and they even set up an email address just for this video that you can reach out to them through three blue one brown at

242
00:20:37,590 --> 00:20:39,590
amplify partners com

