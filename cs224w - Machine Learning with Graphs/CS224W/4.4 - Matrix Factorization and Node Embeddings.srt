1
00:00:03,980 --> 00:00:06,900
So this now concludes,

2
00:00:06,900 --> 00:00:08,640
uh, the first two parts of the lecture.

3
00:00:08,640 --> 00:00:10,305
And what I wanna do in the, uh,

4
00:00:10,305 --> 00:00:12,615
remaining, uh, few minutes,

5
00:00:12,615 --> 00:00:14,940
10 minutes or so, I wanna talk about

6
00:00:14,940 --> 00:00:18,495
connection to know the embeddings and matrix factorization.

7
00:00:18,495 --> 00:00:22,320
So, uh, let me refresh you what we talked,

8
00:00:22,320 --> 00:00:24,735
uh, what we talked, uh, on Tuesday.

9
00:00:24,735 --> 00:00:27,690
So we talked about embeddings and we talked

10
00:00:27,690 --> 00:00:30,930
about how we have this embedding matrix z that, you know,

11
00:00:30,930 --> 00:00:34,439
the number of rows is the e- embedding dimension,

12
00:00:34,439 --> 00:00:37,205
and the number of columns is the number of nodes,

13
00:00:37,205 --> 00:00:38,465
uh, in the graph.

14
00:00:38,465 --> 00:00:41,735
And this means that every column of this matrix z

15
00:00:41,735 --> 00:00:45,080
will store an embedding for that given node.

16
00:00:45,080 --> 00:00:50,615
And our objective was in this encoder-decoder framework is that we wanna maximize,

17
00:00:50,615 --> 00:00:54,310
um, the- the- the dot product between pairs of nodes,

18
00:00:54,310 --> 00:00:55,460
uh, that are similar.

19
00:00:55,460 --> 00:00:57,410
So if two nodes are similar, uh,

20
00:00:57,410 --> 00:01:00,260
then their dot product of their embeddings of

21
00:01:00,260 --> 00:01:03,835
their columns in this matrix z, uh, has to be high.

22
00:01:03,835 --> 00:01:06,315
And that was the- that was the idea.

23
00:01:06,315 --> 00:01:09,140
Then of course, how did you define this notion of similarity?

24
00:01:09,140 --> 00:01:14,495
We said two nodes are similar if they co-appear in the same random walk,

25
00:01:14,495 --> 00:01:16,600
uh, starting at the given node.

26
00:01:16,600 --> 00:01:18,930
So now, you know,

27
00:01:18,930 --> 00:01:20,910
how- how can we,

28
00:01:20,910 --> 00:01:22,985
uh, think about this more broadly, right?

29
00:01:22,985 --> 00:01:25,130
You could say we define the notion of

30
00:01:25,130 --> 00:01:27,860
similarity through random walks in the previous lecture.

31
00:01:27,860 --> 00:01:31,460
What if we define an even simpler notion of, uh, similarity?

32
00:01:31,460 --> 00:01:35,440
What if we say, two nodes are similar if they are connected by an edge.

33
00:01:35,440 --> 00:01:37,720
Then what this means is we say, "Oh,

34
00:01:37,720 --> 00:01:39,950
I want to approximate this matrix,

35
00:01:39,950 --> 00:01:42,920
say like entry in the mai- UV in the matrix,

36
00:01:42,920 --> 00:01:46,155
say this is either 0 or 1 by this dot product?"

37
00:01:46,155 --> 00:01:49,830
Like I'm saying, if two nodes are connect- u and v are connected,

38
00:01:49,830 --> 00:01:51,980
then I want their dot product to be 1.

39
00:01:51,980 --> 00:01:53,525
And if they are not connected,

40
00:01:53,525 --> 00:01:56,940
I want their dot product to be 0, right?

41
00:01:56,940 --> 00:02:01,760
Um, of course, like if I write it this way at the level of the single entry,

42
00:02:01,760 --> 00:02:04,715
if I write it in the- in the matrix form,

43
00:02:04,715 --> 00:02:06,200
this- I'm writing basically saying,

44
00:02:06,200 --> 00:02:10,025
this is Z trans- Z transposed times Z equals A.

45
00:02:10,025 --> 00:02:17,375
So this is now caused matrix factorization because I take my matrix A and I factorize it,

46
00:02:17,375 --> 00:02:20,380
I represent it as a product of,

47
00:02:20,380 --> 00:02:22,760
um- of, uh, two matrices.

48
00:02:22,760 --> 00:02:25,335
Um, Z- Z transposed and Z.

49
00:02:25,335 --> 00:02:28,655
So essentially I'm saying I take my adjacency matrix, uh, A.

50
00:02:28,655 --> 00:02:30,710
Here is, for example, one entry of it.

51
00:02:30,710 --> 00:02:33,140
I want- because there is an edge, I want, you know,

52
00:02:33,140 --> 00:02:37,880
the- the- the dot product of this row and that column to be value 1.

53
00:02:37,880 --> 00:02:40,280
For example, for, uh, for this entry,

54
00:02:40,280 --> 00:02:43,235
I would want the product of this,

55
00:02:43,235 --> 00:02:46,565
uh, row and this particular column to be 0.

56
00:02:46,565 --> 00:02:53,940
So this means that we take matrix A and factorize it as a product of Z transpose times Z.

57
00:02:53,950 --> 00:02:59,520
Of course, because embedding dimension D number of rows in Z,

58
00:02:59,520 --> 00:03:03,470
Z is much, much smaller than the number of, uh- uh, nodes, right?

59
00:03:03,470 --> 00:03:05,240
So the matrix is very wide,

60
00:03:05,240 --> 00:03:06,995
but not too- too deep.

61
00:03:06,995 --> 00:03:10,400
This means that this exact approximation saying

62
00:03:10,400 --> 00:03:15,270
A equals Z transpose times Z is generally not possible, right?

63
00:03:15,270 --> 00:03:17,265
We don't have enough representation power

64
00:03:17,265 --> 00:03:19,740
to really capture each edge but perfectly.

65
00:03:19,740 --> 00:03:23,645
So we can learn this martix Z approximately.

66
00:03:23,645 --> 00:03:25,235
So what we can see is,

67
00:03:25,235 --> 00:03:28,995
let's find the matrix Z such that, you know, the, uh,

68
00:03:28,995 --> 00:03:31,320
Z transpose times Z, uh,

69
00:03:31,320 --> 00:03:35,520
the values of it are as similar to the values of A as possible.

70
00:03:35,520 --> 00:03:38,600
How do I- how do I measure similarity now between

71
00:03:38,600 --> 00:03:41,735
values is to use what is called Frobenius norm,

72
00:03:41,735 --> 00:03:44,485
which is simply take an entry in A,

73
00:03:44,485 --> 00:03:46,680
subtract an entry in this,

74
00:03:46,680 --> 00:03:49,710
uh- in this matrix Z transpose times Z,

75
00:03:49,710 --> 00:03:52,245
um, and take the square value of it and sum it up.

76
00:03:52,245 --> 00:03:55,350
So Frobenius norm is simply, uh,

77
00:03:55,350 --> 00:03:57,405
a sum of the square differences, uh,

78
00:03:57,405 --> 00:04:01,935
between corresponding entries into, uh- into matrices.

79
00:04:01,935 --> 00:04:05,540
Right? So this is now very similar to what we were

80
00:04:05,540 --> 00:04:09,080
also doing in the previous, uh, lecture, right?

81
00:04:09,080 --> 00:04:10,350
In the node embeddings lecture.

82
00:04:10,350 --> 00:04:11,960
In the node embeddings lecture,

83
00:04:11,960 --> 00:04:15,710
we were not using the L2 norm to- to define

84
00:04:15,710 --> 00:04:20,105
the discrepancy between A and its factorization,

85
00:04:20,105 --> 00:04:23,180
we used the softmax,

86
00:04:23,180 --> 00:04:26,075
uh, uh, function instead of the L2 norm.

87
00:04:26,075 --> 00:04:31,710
But the goal of approximating A with Z transpose times Z was the same.

88
00:04:31,710 --> 00:04:37,730
So the conclusion is that the inner product decoder with note similarity defined by

89
00:04:37,730 --> 00:04:40,850
edge- edge connectivity is equivalent to

90
00:04:40,850 --> 00:04:44,570
the factorization of adjacency matrix, uh, A, right?

91
00:04:44,570 --> 00:04:47,510
So we say we wanna directly approximate A by

92
00:04:47,510 --> 00:04:50,780
the embeddings of the nodes such that if two nodes are linked,

93
00:04:50,780 --> 00:04:53,825
then I want their dot product to be equal to 1.

94
00:04:53,825 --> 00:04:55,100
And if they are not linked,

95
00:04:55,100 --> 00:04:57,775
I want their dot product to be equal to 0.

96
00:04:57,775 --> 00:05:02,095
So that's the, uh- the simplest way to define node similarity.

97
00:05:02,095 --> 00:05:05,240
Now, in random walk-based similarity,

98
00:05:05,240 --> 00:05:08,380
it turns out that when we have this more,

99
00:05:08,380 --> 00:05:11,485
um, nuanced, more complex definition of, uh,

100
00:05:11,485 --> 00:05:17,300
similarity, then, um, it is still- the entire process is still equivalent to

101
00:05:17,300 --> 00:05:24,060
matrix factorization of just- of a more complex or transformed, uh, adjacency matrix.

102
00:05:24,060 --> 00:05:26,130
So, um, here's the equation,

103
00:05:26,130 --> 00:05:29,235
let me explain it, uh, on the next slide, what does it mean.

104
00:05:29,235 --> 00:05:32,360
So it means that what we are really trying to do is

105
00:05:32,360 --> 00:05:35,660
we are trying to factorize this particular,

106
00:05:35,660 --> 00:05:39,495
um, transformed graph adjacency matrix.

107
00:05:39,495 --> 00:05:41,775
So how is the the- uh,

108
00:05:41,775 --> 00:05:43,440
what- what is the transformation?

109
00:05:43,440 --> 00:05:48,044
The transformation is- here is the, um, adjacency matrix.

110
00:05:48,044 --> 00:05:51,720
Here is the, um, one over the- the diagonal matrix,

111
00:05:51,720 --> 00:05:53,685
these are the node degrees.

112
00:05:53,685 --> 00:05:55,350
Um, this is now, uh,

113
00:05:55,350 --> 00:05:57,120
raised to the power r,

114
00:05:57,120 --> 00:06:00,585
where this- r goes between 1 and T where, uh,

115
00:06:00,585 --> 00:06:07,010
capital T is the context window length is actually the length of the random walks that,

116
00:06:07,010 --> 00:06:08,960
uh, we are simulating in,

117
00:06:08,960 --> 00:06:11,740
uh- in DeepWalk or, uh, node2vec.

118
00:06:11,740 --> 00:06:16,280
Um, volume of G is simply the sum of the entries of the adjacency matrix,

119
00:06:16,280 --> 00:06:18,790
we see- which is twice the number of edges.

120
00:06:18,790 --> 00:06:22,040
And the log b is a factor that corresponds to

121
00:06:22,040 --> 00:06:26,195
the number of negative samples we are using in the optimization problem.

122
00:06:26,195 --> 00:06:29,585
So basically, what this means is that you can compute,

123
00:06:29,585 --> 00:06:31,920
um- in this case,

124
00:06:31,920 --> 00:06:36,860
deep walk either by what we talked last time by simulating random walks

125
00:06:36,860 --> 00:06:42,720
and defining the gradients and doing gradient descent or taking the adjacency matrix,

126
00:06:42,720 --> 00:06:44,180
A, of your graph,

127
00:06:44,180 --> 00:06:47,585
transforming it according to this equation where

128
00:06:47,585 --> 00:06:50,930
we both take into account the lands of the random walks,

129
00:06:50,930 --> 00:06:53,515
as well as the number of negative samples we use.

130
00:06:53,515 --> 00:06:55,815
And if we factorize this, uh,

131
00:06:55,815 --> 00:06:59,900
transform matrix, what I mean by this is if we go and, uh,

132
00:06:59,900 --> 00:07:04,295
replace this A here with the transform matrix and then we try to solve,

133
00:07:04,295 --> 00:07:06,054
uh, this- this equation,

134
00:07:06,054 --> 00:07:07,875
uh, then the, uh, the solution to it,

135
00:07:07,875 --> 00:07:11,955
this matrix Z, will be exactly the same as what,

136
00:07:11,955 --> 00:07:13,485
uh- what we- what, uh,

137
00:07:13,485 --> 00:07:17,400
our approach that we discussed in the previous lecture arrived to.

138
00:07:17,400 --> 00:07:21,030
So, um, basically that is a very- very nice paper.

139
00:07:21,030 --> 00:07:24,450
Um, if you wanna know more about this called network embedding as

140
00:07:24,450 --> 00:07:28,410
matrix factorization that basically unifies DeepWalk LINE,

141
00:07:28,410 --> 00:07:32,405
um, and a lot of other algorithms including node2vec in this,

142
00:07:32,405 --> 00:07:35,675
uh, one mathematical, uh, framework.

143
00:07:35,675 --> 00:07:38,330
So, um, as I said,

144
00:07:38,330 --> 00:07:44,150
this random walk-based similarity can also be thought as, um, matrix factorization.

145
00:07:44,150 --> 00:07:46,490
The equation I show here is for DeepWalk.

146
00:07:46,490 --> 00:07:53,035
Um, you can derive similar type of matrix transformation for, uh, node2vec.

147
00:07:53,035 --> 00:07:58,325
Just the matrix is even more complex because the random walk process of node2vec is,

148
00:07:58,325 --> 00:07:59,675
uh- is more complex,

149
00:07:59,675 --> 00:08:02,255
is more, uh- more nuanced.

150
00:08:02,255 --> 00:08:05,045
So to conclude the lecture today,

151
00:08:05,045 --> 00:08:07,790
I wanna talk a bit about the limitations and kind of

152
00:08:07,790 --> 00:08:11,160
motivate what are we going to talk about next week.

153
00:08:11,160 --> 00:08:16,490
So the limitations of node embeddings via this matrix factorization or this random walks,

154
00:08:16,490 --> 00:08:21,030
uh, like- like I discussed in terms of, um,

155
00:08:21,030 --> 00:08:24,370
node2vec, DeepWalk for multidimensional embeddings,

156
00:08:24,370 --> 00:08:29,060
you can even think of PageRank as a single-dimensional embedding is that,

157
00:08:29,060 --> 00:08:33,020
um, we cannot obtain embeddings for nodes not in the training set.

158
00:08:33,020 --> 00:08:35,179
So this means if our graph is evolving,

159
00:08:35,179 --> 00:08:38,114
if new nodes are appearing over time, then, uh,

160
00:08:38,115 --> 00:08:40,770
the nodes that are not in the graph, uh,

161
00:08:40,770 --> 00:08:44,260
at the time when we are computing embeddings won't have an embedding.

162
00:08:44,260 --> 00:08:46,100
So if a newly added node 5,

163
00:08:46,100 --> 00:08:48,755
for example, arrives, let's say, at the test time,

164
00:08:48,755 --> 00:08:51,290
lets say- or is a new user in the social network,

165
00:08:51,290 --> 00:08:53,390
we can- we cannot compute its embedding,

166
00:08:53,390 --> 00:08:56,090
we have to redo everything from scratch.

167
00:08:56,090 --> 00:08:59,570
We have to recompute all embeddings of all nodes in the network.

168
00:08:59,570 --> 00:09:02,065
So this is very limiting.

169
00:09:02,065 --> 00:09:04,890
The second important thing is that this, uh,

170
00:09:04,890 --> 00:09:09,450
embeddings cannot capture structural similarity.

171
00:09:09,450 --> 00:09:11,390
Uh, the reason being is, for example,

172
00:09:11,390 --> 00:09:14,795
if I have the graph like I show here and I consider nodes,

173
00:09:14,795 --> 00:09:16,300
uh, 1 and 11.

174
00:09:16,300 --> 00:09:19,260
Even though they are in very different parts of the graph,

175
00:09:19,260 --> 00:09:21,405
their local network structure, uh,

176
00:09:21,405 --> 00:09:24,210
looks- looks quite similar.

177
00:09:24,210 --> 00:09:27,410
Um, and DeepWalk and node2vec will come up with

178
00:09:27,410 --> 00:09:31,895
very different embeddings for 11 and- and 1 because,

179
00:09:31,895 --> 00:09:34,555
you know, 11 is neighbor with 12 and 13,

180
00:09:34,555 --> 00:09:37,265
while 1 is neighbor with 2 and 3.

181
00:09:37,265 --> 00:09:41,540
So this means that they- these types of embeddings

182
00:09:41,540 --> 00:09:45,455
won't be able to capture this notion of local structural similarity,

183
00:09:45,455 --> 00:09:50,570
but it will- more be able to capture who- who- what are the identities of the neighbors,

184
00:09:50,570 --> 00:09:53,525
uh, next, uh- next to a given starting node.

185
00:09:53,525 --> 00:09:57,520
Of course, if you were to define these over anonymous walks,

186
00:09:57,520 --> 00:10:01,890
then, um, that would cap- allow us to capture the structure because, uh,

187
00:10:01,890 --> 00:10:06,605
2 and 3, um, would- would- basically the identities of the nodes,

188
00:10:06,605 --> 00:10:09,305
um, uh, are forgotten, and then we would be able to,

189
00:10:09,305 --> 00:10:11,920
uh, solve this, uh, problem.

190
00:10:11,920 --> 00:10:16,265
And then the last limitation I wanna talk about is- is that

191
00:10:16,265 --> 00:10:20,465
this approaches cannot utilize node edge in graph level features,

192
00:10:20,465 --> 00:10:24,370
meaning, you know, feature vectors attached to nodes,

193
00:10:24,370 --> 00:10:28,810
uh, edges and graphs cannot naturally be incorporated in this framework.

194
00:10:28,810 --> 00:10:31,475
Right? We basically create node embedding separately

195
00:10:31,475 --> 00:10:34,440
from the features that these nodes might have.

196
00:10:34,440 --> 00:10:35,870
So for example, you know, I use it in

197
00:10:35,870 --> 00:10:38,495
a social network may have a set of properties, features,

198
00:10:38,495 --> 00:10:41,690
attributes where are protein has a set of properties,

199
00:10:41,690 --> 00:10:45,600
features that you would want to use them in creating, uh, embeddings.

200
00:10:45,600 --> 00:10:48,480
And really, what are we going to talk next is,

201
00:10:48,480 --> 00:10:51,695
um, you know, what are the solutions to these limitations?

202
00:10:51,695 --> 00:10:56,330
The solution for- to these limitations is deep representation learning and,

203
00:10:56,330 --> 00:10:57,710
uh, graph neural networks.

204
00:10:57,710 --> 00:10:58,940
And in the next week,

205
00:10:58,940 --> 00:11:00,330
we are going to move, uh,

206
00:11:00,330 --> 00:11:01,815
to the topic of, uh,

207
00:11:01,815 --> 00:11:04,430
graph neural networks that will allow us to

208
00:11:04,430 --> 00:11:07,530
resolve these limitations that I have just, uh,

209
00:11:07,530 --> 00:11:10,330
discussed, and will allow us to fuse the feature

210
00:11:10,330 --> 00:11:13,880
information together with the structured information.

211
00:11:13,880 --> 00:11:16,295
So to summarize today's lecture,

212
00:11:16,295 --> 00:11:20,425
we talked about PageRank that measures important soft nodes in a graph.

213
00:11:20,425 --> 00:11:23,720
Uh, we talked about it in three different ways,

214
00:11:23,720 --> 00:11:27,545
we talked about it in terms of flow formulation,

215
00:11:27,545 --> 00:11:29,330
in terms of links, and nodes.

216
00:11:29,330 --> 00:11:31,355
We talked about it in terms of

217
00:11:31,355 --> 00:11:35,080
random walk and stationary distribution of a random walk process.

218
00:11:35,080 --> 00:11:38,495
And we also talked about it from the linear algebra point of view, uh,

219
00:11:38,495 --> 00:11:41,690
by basically computing the eigenvector, uh,

220
00:11:41,690 --> 00:11:43,880
to the particularly transformed,

221
00:11:43,880 --> 00:11:45,590
uh, graph adjacency matrix.

222
00:11:45,590 --> 00:11:48,180
Then we talked about, um,

223
00:11:48,180 --> 00:11:51,290
extensions of PageRank particular random walk

224
00:11:51,290 --> 00:11:53,450
with restarts and personalized PageRank where

225
00:11:53,450 --> 00:11:59,790
basically the difference is in terms of changing the teleportation set.

226
00:11:59,790 --> 00:12:02,645
Um, and then the last part of the lecture,

227
00:12:02,645 --> 00:12:06,200
we talked about node embeddings based on random walks and

228
00:12:06,200 --> 00:12:10,210
how they can be rep- expressed as a form of matrix factorization.

229
00:12:10,210 --> 00:12:15,740
So this means that viewing graphs as matrices is a-plays

230
00:12:15,740 --> 00:12:18,695
a key role in all of the above algorithms

231
00:12:18,695 --> 00:12:21,890
where we can think of this in many different ways.

232
00:12:21,890 --> 00:12:23,270
But mathematically at the end,

233
00:12:23,270 --> 00:12:27,920
it's all about mat- matrix representation of the graph, factorizing this matrix,

234
00:12:27,920 --> 00:12:32,300
computing eigenvectors, eigenvalues, and extracting connectivity information,

235
00:12:32,300 --> 00:12:36,850
uh, out of it with the tools of, uh, linear algebra.

236
00:12:36,850 --> 00:12:39,540
So, um, thank you very much,

237
00:12:39,540 --> 00:12:43,089
everyone, for the- for the lecture.

