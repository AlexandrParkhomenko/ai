1
00:00:04,100 --> 00:00:07,860
Thank you for the opportunity and I'm excited to talk about

2
00:00:07,860 --> 00:00:11,950
my work on pre-training graph neural networks.

3
00:00:13,280 --> 00:00:17,610
Okay, so, um, here I, uh,

4
00:00:17,610 --> 00:00:19,245
I want to talk about applying

5
00:00:19,245 --> 00:00:23,340
graph neural networks to application in scientific domains.

6
00:00:23,340 --> 00:00:25,710
Uh, for example, uh, in chemistry,

7
00:00:25,710 --> 00:00:28,230
we have this molecular graphs where each node is

8
00:00:28,230 --> 00:00:31,800
an atom and each edge represents the chemical bond.

9
00:00:31,800 --> 00:00:36,645
And here we are interested in predicting the properties of molecules.

10
00:00:36,645 --> 00:00:39,075
For example, given this kind of molecular graph,

11
00:00:39,075 --> 00:00:40,350
we want to predict uh,

12
00:00:40,350 --> 00:00:42,225
whether it's toxic or not.

13
00:00:42,225 --> 00:00:44,630
In biology, there's also a graph problem.

14
00:00:44,630 --> 00:00:46,325
For example, uh, we have a

15
00:00:46,325 --> 00:00:51,380
protein-protein association graph where each node is a protein and the edge represents

16
00:00:51,380 --> 00:00:55,130
certain association between proteins and the associations are

17
00:00:55,130 --> 00:00:59,180
very important to determine the functionality of the protein.

18
00:00:59,180 --> 00:01:02,105
And our goal is to given this, uh,

19
00:01:02,105 --> 00:01:04,985
subgraph centered around the protein node,

20
00:01:04,985 --> 00:01:06,590
we want to predict whether

21
00:01:06,590 --> 00:01:10,835
the center protein node has certain biological activity or not.

22
00:01:10,835 --> 00:01:15,065
So what I want to say is that in bio- in many scientific domains, this, uh,

23
00:01:15,065 --> 00:01:21,210
graph problem appears a lot and we want to apply GNNs to solve this problem.

24
00:01:21,470 --> 00:01:27,995
And just to review how GNNs can be used for graph classification.

25
00:01:27,995 --> 00:01:32,455
So GNNs obtain an embedding of the entire graph by following two steps.

26
00:01:32,455 --> 00:01:34,340
First, um, given this,

27
00:01:34,340 --> 00:01:37,174
the molecular graph, let's say graph, let's say molecule,

28
00:01:37,174 --> 00:01:42,844
we obtain the embedding of node by either the aggregate neighboring information.

29
00:01:42,844 --> 00:01:44,450
Once we do that,

30
00:01:44,450 --> 00:01:46,520
we- once we obtain this node embedding,

31
00:01:46,520 --> 00:01:52,430
we can pool this node embedding globally to obtain the embedding of the entire graph.

32
00:01:52,430 --> 00:01:55,680
And once you obtain this embedding of the entire graph, we can use it,

33
00:01:55,680 --> 00:02:01,205
or we can apply a linear function on top of it to predict whether this entire graph is,

34
00:02:01,205 --> 00:02:03,600
let's say, toxic or not.

35
00:02:03,710 --> 00:02:08,350
And just to, [NOISE] uh, say what this means is that, uh,

36
00:02:08,350 --> 00:02:11,150
here the node embedding is trying to capture

37
00:02:11,150 --> 00:02:14,600
the local neighborhood structure, uh, around each node.

38
00:02:14,600 --> 00:02:17,120
If we apply k-hop GNNs,

39
00:02:17,120 --> 00:02:20,980
we will just know that it should capture k-hop local neighborhood.

40
00:02:20,980 --> 00:02:25,010
And then the embedding of the entire graph can, uh,

41
00:02:25,010 --> 00:02:28,025
is basically aggregating this local structure though,

42
00:02:28,025 --> 00:02:33,000
the node- node embedding that is capturing the local neighborhood structure.

43
00:02:33,320 --> 00:02:35,595
And, uh, and, uh,

44
00:02:35,595 --> 00:02:37,200
this is how GNN works,

45
00:02:37,200 --> 00:02:42,105
and now I am going to talk about challenges of how- applying machine learning to,

46
00:02:42,105 --> 00:02:43,805
uh, the scientific domains.

47
00:02:43,805 --> 00:02:46,385
And there are basically two fundamental challenges.

48
00:02:46,385 --> 00:02:49,190
The first challenge of applying machine learning to

49
00:02:49,190 --> 00:02:53,195
the scientific domains is that all the scarcity of labeled data.

50
00:02:53,195 --> 00:02:57,895
Obtaining labels in these scientific domains requires expensive lab experiments.

51
00:02:57,895 --> 00:03:01,850
For example, to, or say whether a molecule is toxic or not,

52
00:03:01,850 --> 00:03:04,415
you need to perform wet lab experiments,

53
00:03:04,415 --> 00:03:05,960
and these are very expensive.

54
00:03:05,960 --> 00:03:08,330
As a result, we cannot get that,

55
00:03:08,330 --> 00:03:13,375
a lot of training data and machinery models tend to overfit to this small training data.

56
00:03:13,375 --> 00:03:18,220
Another important issue is that out-of-distribution prediction, meaning that,

57
00:03:18,220 --> 00:03:20,675
test examples we want to make a prediction on

58
00:03:20,675 --> 00:03:23,365
tend to be very different from training examples.

59
00:03:23,365 --> 00:03:27,770
And this is really the nature of scientific discovery because in scientific discovery,

60
00:03:27,770 --> 00:03:30,710
we want- we want to discover some new molecule that is

61
00:03:30,710 --> 00:03:34,460
inherently very different from- from your training molecules.

62
00:03:34,460 --> 00:03:36,080
And in these domains,

63
00:03:36,080 --> 00:03:39,930
machine learning models are extrapolate poorly.

64
00:03:40,900 --> 00:03:46,670
And especially these two challenges are becoming more challenging for deep learning.

65
00:03:46,670 --> 00:03:50,870
The first, uh, the first point of the label scarcity,

66
00:03:50,870 --> 00:03:53,870
deep learning models have a lot of parameters to train,

67
00:03:53,870 --> 00:03:56,329
uh, typically in the order of millions.

68
00:03:56,329 --> 00:03:58,265
And uh- and in this regime,

69
00:03:58,265 --> 00:04:02,030
the number of training data is much less than the number of parameters.

70
00:04:02,030 --> 00:04:04,220
And deep-learning models are extremely prone to

71
00:04:04,220 --> 00:04:07,490
overfitting on small data- small labeled data.

72
00:04:07,490 --> 00:04:12,605
Another issue is that deep learning models are known to extrapolate poorly.

73
00:04:12,605 --> 00:04:16,235
And that's reported that models often make, uh,

74
00:04:16,235 --> 00:04:19,880
predictions based on spurious correlation in a dataset without

75
00:04:19,880 --> 00:04:24,475
understanding the true causal mechanism of how to make prediction.

76
00:04:24,475 --> 00:04:27,830
So let's consider this a toy example of

77
00:04:27,830 --> 00:04:31,340
a- a image classification between polar bear and brown bear,

78
00:04:31,340 --> 00:04:33,505
uh, this image is shown here.

79
00:04:33,505 --> 00:04:36,120
So during training, let's say, uh,

80
00:04:36,120 --> 00:04:38,675
our training data for- in our training data,

81
00:04:38,675 --> 00:04:42,020
most polar bears have snow background like this,

82
00:04:42,020 --> 00:04:45,895
and most brown bears have a grass background like this.

83
00:04:45,895 --> 00:04:49,370
And as a result, the model can learn to predict,

84
00:04:49,370 --> 00:04:51,680
make prediction of whether it's, uh,

85
00:04:51,680 --> 00:04:57,350
the given image is polar bear or brown bear based on the image background rather than

86
00:04:57,350 --> 00:05:00,380
animal itself because that's sufficient

87
00:05:00,380 --> 00:05:04,025
to make predictions over the- over the training dataset.

88
00:05:04,025 --> 00:05:06,050
But what if at the time same,

89
00:05:06,050 --> 00:05:08,150
if we see polar bear on the grass,

90
00:05:08,150 --> 00:05:09,635
then the model will, uh,

91
00:05:09,635 --> 00:05:15,020
because the model is not understanding the- this prediction task,

92
00:05:15,020 --> 00:05:18,110
the model will perform poorly on the- on

93
00:05:18,110 --> 00:05:21,410
the data or test data that is different from training data.

94
00:05:21,410 --> 00:05:23,810
And the model is, uh, just capturing this kind of

95
00:05:23,810 --> 00:05:28,200
spurious, spurious correlation in the training dataset.

96
00:05:28,670 --> 00:05:33,465
And our key idea or the goal, uh,

97
00:05:33,465 --> 00:05:35,880
given these challenges is that we want to improve

98
00:05:35,880 --> 00:05:39,950
model's out-of-distribution performance even with limited data.

99
00:05:39,950 --> 00:05:44,345
And the way we do this is to inject domain knowledge

100
00:05:44,345 --> 00:05:49,315
into a model before we apply them on scarcely-labeled tasks.

101
00:05:49,315 --> 00:05:52,860
And that, this work- this may work because

102
00:05:52,860 --> 00:05:56,990
the model already knows the domain knowledge before the model is

103
00:05:56,990 --> 00:06:01,010
training on- on our downstream data so that the model can generalize

104
00:06:01,010 --> 00:06:05,190
well without many task-specific labeled data and uh,

105
00:06:05,190 --> 00:06:08,765
and uh, the model may be able to extract essential,

106
00:06:08,765 --> 00:06:12,520
non-spurious, uh, pattern in the data, uh,

107
00:06:12,520 --> 00:06:16,475
which allows the model to better extrapolate to the-

108
00:06:16,475 --> 00:06:21,880
to the test data that still- that still very different from training data.

109
00:06:21,880 --> 00:06:25,220
And a very effective solution to inject

110
00:06:25,220 --> 00:06:28,160
domain knowledge into model is called pre-training.

111
00:06:28,160 --> 00:06:32,360
And we pre-train a model on relevant tasks that's different

112
00:06:32,360 --> 00:06:36,755
from a downstream task where data is abundant.

113
00:06:36,755 --> 00:06:39,320
And after we pre-train the model,

114
00:06:39,320 --> 00:06:41,870
the model's parameter, uh,

115
00:06:41,870 --> 00:06:47,330
already contains some domain knowledge and once this is done,

116
00:06:47,330 --> 00:06:51,980
we can transfer this pre-trained model parameter to the downstream task,

117
00:06:51,980 --> 00:06:55,990
which is what we care about and which where we have small number of data.

118
00:06:55,990 --> 00:07:00,555
And we can start from the pre-trained parameter and fine-tune the-

119
00:07:00,555 --> 00:07:06,000
the parameters on the downstream tasks.

120
00:07:06,000 --> 00:07:13,430
Um, and just to mention that pre-training has been hugely successful in- in the domain of

121
00:07:13,430 --> 00:07:16,370
computer vision and natural language processing and it's

122
00:07:16,370 --> 00:07:20,420
reported that pre-training improves label efficiency.

123
00:07:20,420 --> 00:07:27,320
Also, pre-training improves out-of-distribution performance.

124
00:07:27,320 --> 00:07:33,350
And because of this and within

125
00:07:33,350 --> 00:07:38,780
pre-training can be a very powerful solution to the scientific applications.

126
00:07:38,780 --> 00:07:43,250
And those two challenges are scarce labels and out-of-distribution prediction.

127
00:07:43,250 --> 00:07:45,950
So now we motivated

128
00:07:45,950 --> 00:07:52,115
this pre-training GNNs to solve an important problem in scientific applications.

129
00:07:52,115 --> 00:07:55,595
So let's consider, you know, actually pre-trained GNNs.

130
00:07:55,595 --> 00:07:59,570
And our work is about designing

131
00:07:59,570 --> 00:08:02,120
GNN's pre-training strategies and we want to

132
00:08:02,120 --> 00:08:06,439
systematically embedded- investigate the following two questions.

133
00:08:06,439 --> 00:08:09,005
How effective is pre-training GNNs,

134
00:08:09,005 --> 00:08:12,605
and what is the effective pre-training strategies.

135
00:08:12,605 --> 00:08:15,350
So as a running example,

136
00:08:15,350 --> 00:08:17,960
let's think about molecular property prediction task,

137
00:08:17,960 --> 00:08:20,135
prediction for drug discovery.

138
00:08:20,135 --> 00:08:25,315
For a given molecule, we want to predict its toxicity or biological activity.

139
00:08:25,315 --> 00:08:31,700
And the very naive strategy is multi-task supervised pre-training on relevant labels.

140
00:08:31,700 --> 00:08:34,010
This means that in the, for example,

141
00:08:34,010 --> 00:08:36,679
chemical database we have, uh, all the,

142
00:08:36,679 --> 00:08:39,559
uh, experimental measurements of

143
00:08:39,559 --> 00:08:44,450
tox- various toxicity and biological activities of a lot of molecules.

144
00:08:44,450 --> 00:08:48,380
And we can first pre-train GNNs to predict those,

145
00:08:48,380 --> 00:08:52,595
uh, very diverse biological activity of toxicity.

146
00:08:52,595 --> 00:08:56,570
And then we expect GNNs parameter to capture

147
00:08:56,570 --> 00:08:59,510
some chemistry domain knowledge which can be tran-

148
00:08:59,510 --> 00:09:04,710
and then we can transfer that parameter to our downstream task.

149
00:09:05,600 --> 00:09:09,505
And the setting that we consider is, uh, to,

150
00:09:09,505 --> 00:09:13,279
to study whether this naive strategy is effective,

151
00:09:13,279 --> 00:09:15,335
we consider this setting.

152
00:09:15,335 --> 00:09:18,350
We consider this binary classification of molecules.

153
00:09:18,350 --> 00:09:22,330
Given molecule, we want to judge whether it's negative or positive.

154
00:09:22,330 --> 00:09:26,600
And- and we, for the supervised pre-training part, uh,

155
00:09:26,600 --> 00:09:28,910
we consider- we consider predicting

156
00:09:28,910 --> 00:09:38,685
more than 1000 diverse binary bioassays annotated over 450,000 molecules.

157
00:09:38,685 --> 00:09:39,810
So there are a lot of data,

158
00:09:39,810 --> 00:09:42,065
uh, in this pre-training stage.

159
00:09:42,065 --> 00:09:46,775
And then we apply transfer the parameter to or downstream task,

160
00:09:46,775 --> 00:09:49,625
which is eight molecular classification datasets,

161
00:09:49,625 --> 00:09:51,530
those are relatively small, uh,

162
00:09:51,530 --> 00:09:54,065
about 1,000 to 100,000 molecules.

163
00:09:54,065 --> 00:09:56,590
And- and for the data split,

164
00:09:56,590 --> 00:09:58,730
uh, for the downstream task,

165
00:09:58,730 --> 00:10:00,755
we consider the scaffold split,

166
00:10:00,755 --> 00:10:04,860
which makes the test molecules out of distribution.

167
00:10:04,900 --> 00:10:09,350
And it turns out that the Naive strategy of

168
00:10:09,350 --> 00:10:14,360
this multi-task supervised pre-training on relevant labels doesn't work very well.

169
00:10:14,360 --> 00:10:20,325
It gives a limited performance improvement on downstream tasks and even, uh,

170
00:10:20,325 --> 00:10:22,455
leads to negative transfer,

171
00:10:22,455 --> 00:10:28,135
which means that the pre-trained model performs worse than randomly initialized model.

172
00:10:28,135 --> 00:10:30,275
So here's the, uh, table,

173
00:10:30,275 --> 00:10:31,430
and here is a figure.

174
00:10:31,430 --> 00:10:33,755
So we have our eight, uh,

175
00:10:33,755 --> 00:10:37,035
downstream datasets and -and

176
00:10:37,035 --> 00:10:42,605
the y-axis is the ROCAUC improvement over no pre-training baseline.

177
00:10:42,605 --> 00:10:45,355
So this purple is a no pre-train baseline.

178
00:10:45,355 --> 00:10:49,425
And we see that Naive strategy sometimes work well,

179
00:10:49,425 --> 00:10:52,475
but for these two datasets,

180
00:10:52,475 --> 00:10:57,730
it's actually performing worse than non pre-trained randomly initialized baseline.

181
00:10:57,730 --> 00:11:01,545
So this is kind of not desirable,

182
00:11:01,545 --> 00:11:04,710
we want pre-trained model to perform better.

183
00:11:06,330 --> 00:11:11,170
So how- what is the- then we move on to our next question,

184
00:11:11,170 --> 00:11:13,510
what is the effective pre-training strategy?

185
00:11:13,510 --> 00:11:18,340
And our key idea is to pre-train both node and graph

186
00:11:18,340 --> 00:11:24,310
embeddings so that GNNs can capture domain-specific knowledge about the graph,

187
00:11:24,310 --> 00:11:25,840
uh, at the both, uh,

188
00:11:25,840 --> 00:11:27,595
local and global level.

189
00:11:27,595 --> 00:11:30,655
So- so in the naive strategy,

190
00:11:30,655 --> 00:11:33,325
we pre-train GNNs, uh,

191
00:11:33,325 --> 00:11:35,875
on the- at the level of graph, use this, uh,

192
00:11:35,875 --> 00:11:39,100
graph embedding to make a various prediction.

193
00:11:39,100 --> 00:11:40,720
But we think, uh,

194
00:11:40,720 --> 00:11:43,525
it's important to also pre-training, uh, this, uh,

195
00:11:43,525 --> 00:11:46,165
node embedding because these node embeddings are

196
00:11:46,165 --> 00:11:49,375
aggregated to generate the embedding of the entire graph.

197
00:11:49,375 --> 00:11:52,465
So we- we need to have a high-quality node embedding

198
00:11:52,465 --> 00:11:56,380
that captures  the local neighborhood structure well.

199
00:11:56,380 --> 00:12:04,870
So and the- the intuition behind this is that- so here's the figure,

200
00:12:04,870 --> 00:12:08,005
the- the upper side means the node embedding

201
00:12:08,005 --> 00:12:11,515
and these node embedding are proved to generate the embedding of the graph,

202
00:12:11,515 --> 00:12:13,020
and we want the, uh,

203
00:12:13,020 --> 00:12:18,090
the quality of the node embedding and graph embedding to be both high quality,

204
00:12:18,090 --> 00:12:23,050
capture the semantics in the- in the- this is,

205
00:12:23,050 --> 00:12:27,460
uh, different from the naive pre-training strategy where, for example,

206
00:12:27,460 --> 00:12:30,580
if we- on the node-level pre-training,

207
00:12:30,580 --> 00:12:33,175
we capture the semantic at the level of nodes,

208
00:12:33,175 --> 00:12:37,060
but if those node embeddings they are globally aggregated,

209
00:12:37,060 --> 00:12:39,550
we may not be able to obtain nice, uh,

210
00:12:39,550 --> 00:12:43,330
graph embedding, um, or if we only pre-training,

211
00:12:43,330 --> 00:12:45,430
uh, GNNs at the level of graph,

212
00:12:45,430 --> 00:12:48,220
we can obtain nice graph embedding,

213
00:12:48,220 --> 00:12:52,900
but the- there's no guarantee that the- before aggregation,

214
00:12:52,900 --> 00:12:55,299
those node embeddings are of high-quality,

215
00:12:55,299 --> 00:12:59,170
and this may lead to a negative transfer,

216
00:12:59,170 --> 00:13:03,350
um, because the node embeddings are kind of not robust.

217
00:13:04,140 --> 00:13:08,335
So- so in order to realize this, uh,

218
00:13:08,335 --> 00:13:12,340
strategy to pre-train GNNs at the level of both nodes and graphs,

219
00:13:12,340 --> 00:13:15,790
we- we designed three concrete methods.

220
00:13:15,790 --> 00:13:19,090
Uh, two self-supervised method, uh,

221
00:13:19,090 --> 00:13:23,155
meaning there is no need for external labels for node-level pre-training,

222
00:13:23,155 --> 00:13:29,110
and one graph-level of prediction of pre-training strategy,

223
00:13:29,110 --> 00:13:31,660
pre-training method, um, that's,

224
00:13:31,660 --> 00:13:33,595
uh, that's, uh, supervised.

225
00:13:33,595 --> 00:13:37,060
So let me go through these three concrete methods.

226
00:13:37,060 --> 00:13:38,920
[NOISE] Okay.

227
00:13:38,920 --> 00:13:41,500
So the first method is the attribute masking.

228
00:13:41,500 --> 00:13:42,970
It's a node-level pre-training method.

229
00:13:42,970 --> 00:13:46,585
Um, the algorithm is quite simple.

230
00:13:46,585 --> 00:13:49,930
So given that input graph, we first mask,

231
00:13:49,930 --> 00:13:53,770
uh, uh, rand- randomly mask our node attributes.

232
00:13:53,770 --> 00:13:56,470
So let's say we mask these two node attributes.

233
00:13:56,470 --> 00:13:58,780
In the molecular graphs, this node that we are

234
00:13:58,780 --> 00:14:01,315
basically masking the identity of the atom.

235
00:14:01,315 --> 00:14:05,140
Um, and then we can use GNNs to generate, uh,

236
00:14:05,140 --> 00:14:07,930
node embeddings of these two mask nodes,

237
00:14:07,930 --> 00:14:13,600
and we use these node embeddings to predict the identity of the mask attributes.

238
00:14:13,600 --> 00:14:17,605
So here we want to use the node embedding to predict the- that

239
00:14:17,605 --> 00:14:22,495
this X is the- is the oxygen and this X is the carbon.

240
00:14:22,495 --> 00:14:25,750
And the intuition behind this is that

241
00:14:25,750 --> 00:14:28,780
the- through solving the mask attribute prediction task,

242
00:14:28,780 --> 00:14:31,210
our GNN is forced to learn

243
00:14:31,210 --> 00:14:36,160
the domain knowledge because it kind of needs to solve this quiz, and, uh, and, uh,

244
00:14:36,160 --> 00:14:38,395
and- and through this,

245
00:14:38,395 --> 00:14:40,720
solving this kind of quiz, GNN, uh,

246
00:14:40,720 --> 00:14:47,120
can learn the- the parameter of the GNN can capture the- the chemistry domain.

247
00:14:47,700 --> 00:14:53,590
The next self-supervised task that we propose is called the context prediction.

248
00:14:53,590 --> 00:14:58,225
And, uh, and here the idea- the algorithm is as follows.

249
00:14:58,225 --> 00:14:59,920
So for each graph,

250
00:14:59,920 --> 00:15:01,945
uh, we sample one center node.

251
00:15:01,945 --> 00:15:03,385
Let's say this is a,

252
00:15:03,385 --> 00:15:06,265
um, um, red nodes,

253
00:15:06,265 --> 00:15:08,410
and we, for this, uh,

254
00:15:08,410 --> 00:15:11,440
center node, we extract neighborhood and context graph.

255
00:15:11,440 --> 00:15:17,320
So neighborhood graph is just the k-hop neighborhood graph, uh, in this case,

256
00:15:17,320 --> 00:15:19,570
two-hop neighbor graph like this,

257
00:15:19,570 --> 00:15:21,760
and then the context graph is the- is

258
00:15:21,760 --> 00:15:25,390
the surrounding graph that's directly attached to this,

259
00:15:25,390 --> 00:15:27,200
uh, k-hop neighborhood graph.

260
00:15:27,200 --> 00:15:29,940
Once we do extract this graph,

261
00:15:29,940 --> 00:15:37,020
we can use two separate GNNs to encode this neighborhood graph into, you know, uh,

262
00:15:37,020 --> 00:15:41,670
one- use one GNN to encode this neighborhood graph into a vector,

263
00:15:41,670 --> 00:15:45,440
another GNN is to encode this context graph into a vector.

264
00:15:45,440 --> 00:15:49,045
And our objective is to maximize, uh,

265
00:15:49,045 --> 00:15:53,740
or maximize the inner product between the true neighborhood context pair

266
00:15:53,740 --> 00:15:59,274
while minimizing the inner product between the false or negative,

267
00:15:59,274 --> 00:16:03,490
uh, uh, false on neighborhood context pair,

268
00:16:03,490 --> 00:16:06,865
and these false pairs can be obtained by, uh,

269
00:16:06,865 --> 00:16:11,305
randomly sample context graph from other neighborhoods.

270
00:16:11,305 --> 00:16:15,790
And here the intuition is that we're using- we are assuming that

271
00:16:15,790 --> 00:16:21,685
subgraphs that are surrounded by similar contexts are semantically similar.

272
00:16:21,685 --> 00:16:24,220
So we are basically pushing, uh,

273
00:16:24,220 --> 00:16:27,730
the embedding of the neighborhood to be

274
00:16:27,730 --> 00:16:31,285
similar if they are- have similar context graphs.

275
00:16:31,285 --> 00:16:35,530
And this is the kind of widely used, uh,

276
00:16:35,530 --> 00:16:38,590
assumption in natural language processing,

277
00:16:38,590 --> 00:16:40,690
it's called distributional hypothesis.

278
00:16:40,690 --> 00:16:44,800
So words that are appearing with similar- similar contexts have similar meaning,

279
00:16:44,800 --> 00:16:48,115
and it's exploited in the famous, uh, Word2vec model.

280
00:16:48,115 --> 00:16:53,050
Um, so finally, um,

281
00:16:53,050 --> 00:16:54,730
I'm going to talk about this, uh,

282
00:16:54,730 --> 00:16:59,710
graph-level pre-training task, which is essentially what I introduced before.

283
00:16:59,710 --> 00:17:05,755
So multi-task supervised pre-training on many relevant graph level- labels.

284
00:17:05,755 --> 00:17:08,454
And this is really the direct approach, uh,

285
00:17:08,454 --> 00:17:11,289
to inject, uh, domain knowledge into the model.

286
00:17:11,290 --> 00:17:18,069
[NOISE] So to summarize the overall strategy,

287
00:17:18,069 --> 00:17:21,879
we perform- we first perform node-level pre-training,

288
00:17:21,880 --> 00:17:25,944
uh, to- to obtain good node representation.

289
00:17:25,944 --> 00:17:27,594
We- we use that, uh,

290
00:17:27,595 --> 00:17:31,780
pre-training parameter, uh, to- and then, uh,

291
00:17:31,780 --> 00:17:34,960
to- to further perform pre-training, uh,

292
00:17:34,960 --> 00:17:38,605
at the level of graph using supervised graph-level pre-training.

293
00:17:38,605 --> 00:17:40,765
Once- once we've done,

294
00:17:40,765 --> 00:17:43,030
uh, this, both steps,

295
00:17:43,030 --> 00:17:49,345
we fine-tune, uh, the parameters on the downstream task that we care about.

296
00:17:49,345 --> 00:17:52,030
So this is the overall strategy.

297
00:17:52,030 --> 00:17:56,515
And it turns out that our strategy works out pretty well.

298
00:17:56,515 --> 00:17:58,045
Uh, as you can see, these, uh,

299
00:17:58,045 --> 00:18:00,189
green dots are our strategy,

300
00:18:00,189 --> 00:18:05,365
and our strategy first avoids the negative transfer in these, uh, two datasets,

301
00:18:05,365 --> 00:18:10,825
and also consistently perform better than- than- than this,

302
00:18:10,825 --> 00:18:14,860
uh, orange naive strategy baseline across the datasets.

303
00:18:14,860 --> 00:18:19,270
Uh, and also, um,

304
00:18:19,270 --> 00:18:22,135
another interesting side note is that, uh,

305
00:18:22,135 --> 00:18:27,475
we fix the pre-training strategy and the pre-training different GNNs models,

306
00:18:27,475 --> 00:18:30,745
uh, use different GNNs models for pre-training.

307
00:18:30,745 --> 00:18:34,930
And what we found out is that the most expressive GNN model,

308
00:18:34,930 --> 00:18:37,690
namely GIN, that we've learned in the lecture,

309
00:18:37,690 --> 00:18:41,620
um, um, benefits most from pre-training.

310
00:18:41,620 --> 00:18:43,210
Uh, as you can see here,

311
00:18:43,210 --> 00:18:48,970
the gain of pre-trained model versus non-pre-trained model is the- is the largest,

312
00:18:48,970 --> 00:18:51,220
um, in terms of accuracy.

313
00:18:51,220 --> 00:18:53,680
And- and the intuition here is that

314
00:18:53,680 --> 00:18:56,860
the expressive GNN model can learn to

315
00:18:56,860 --> 00:19:00,100
capture more domain knowledge than less expressive model,

316
00:19:00,100 --> 00:19:02,515
especially learned from large amounts of,

317
00:19:02,515 --> 00:19:05,630
uh, data during pre-training.

318
00:19:05,700 --> 00:19:10,510
So to summarize of our GNNs, we've, uh,

319
00:19:10,510 --> 00:19:14,020
said- learned that the GNNs have important application

320
00:19:14,020 --> 00:19:17,800
in scientific domains like molecular property prediction or,

321
00:19:17,800 --> 00:19:20,214
um, protein function prediction,

322
00:19:20,214 --> 00:19:23,650
but, uh, those application domains present the challenges

323
00:19:23,650 --> 00:19:27,280
of label scarcity and out-of-distribution prediction,

324
00:19:27,280 --> 00:19:30,655
and we argue that pre-training is a promising,

325
00:19:30,655 --> 00:19:34,825
uh, framework to tackle both of the challenges.

326
00:19:34,825 --> 00:19:39,175
However, we found that naive pre-training strategy of this, uh,

327
00:19:39,175 --> 00:19:41,170
supervised graph level pre-training gives

328
00:19:41,170 --> 00:19:44,995
sub-optimal performance and even leads to negative transfer.

329
00:19:44,995 --> 00:19:47,545
And our strategy is to, um,

330
00:19:47,545 --> 00:19:51,475
effective strategy is to pre-train both node and graph embeddings,

331
00:19:51,475 --> 00:19:53,320
and we found this strategy leads to

332
00:19:53,320 --> 00:19:57,730
a significant performance gain on diverse downstream tasks.

333
00:19:57,730 --> 00:20:01,910
Um, yeah, thank you for listening.

