1
00:00:04,160 --> 00:00:08,265
Welcome back to Stanford 244W,

2
00:00:08,265 --> 00:00:10,225
uh, Machine Learning with Graphs.

3
00:00:10,225 --> 00:00:12,480
Um, in this part of the lecture,

4
00:00:12,480 --> 00:00:18,195
I'm going to discuss applications of graph machine learning research and its impact,

5
00:00:18,195 --> 00:00:21,270
uh, across many different, uh, applications.

6
00:00:21,270 --> 00:00:24,275
So in mach- graph machine learning,

7
00:00:24,275 --> 00:00:26,795
we can formulate different types of tasks.

8
00:00:26,795 --> 00:00:30,125
We can formulate tasks at the level of individual nodes.

9
00:00:30,125 --> 00:00:32,795
We can formulate tasks at the level of,

10
00:00:32,795 --> 00:00:35,480
uh, edges, uh, which is pairs of nodes.

11
00:00:35,480 --> 00:00:40,070
We can identify or define tasks at the level of subgraphs of nodes,

12
00:00:40,070 --> 00:00:43,355
as well as the tasks at the level of the entire, um,

13
00:00:43,355 --> 00:00:47,855
graphs like for a graph level prediction or, uh, graph generation.

14
00:00:47,855 --> 00:00:49,730
And what I'm going to talk, uh,

15
00:00:49,730 --> 00:00:53,690
next is go through these different levels of tasks and show you, uh,

16
00:00:53,690 --> 00:00:56,090
different, uh, applications, uh,

17
00:00:56,090 --> 00:01:00,110
and different domains where this type of methods models can be applied.

18
00:01:00,110 --> 00:01:02,225
So for node level tasks,

19
00:01:02,225 --> 00:01:04,805
we generally talk about node classification,

20
00:01:04,805 --> 00:01:07,580
where we are trying to predict a property of a node.

21
00:01:07,580 --> 00:01:09,470
For example, categorize, uh,

22
00:01:09,470 --> 00:01:12,350
online users or categorize items.

23
00:01:12,350 --> 00:01:15,200
In link prediction, we tried to predict whether there

24
00:01:15,200 --> 00:01:18,160
are missing links between a pair of nodes.

25
00:01:18,160 --> 00:01:21,985
One such example of this task is knowledge graph completion.

26
00:01:21,985 --> 00:01:25,670
In, uh, graph level task like graph classification,

27
00:01:25,670 --> 00:01:27,965
we try to categorize different graphs.

28
00:01:27,965 --> 00:01:31,735
Uh, for example, we may want to represent molecules as,

29
00:01:31,735 --> 00:01:35,510
uh, graphs and then predict properties of molecules.

30
00:01:35,510 --> 00:01:38,210
This is especially interesting and important task

31
00:01:38,210 --> 00:01:40,480
for drug design where we try to predict,

32
00:01:40,480 --> 00:01:42,370
uh, properties of different,

33
00:01:42,370 --> 00:01:44,535
uh, molecules, different drugs.

34
00:01:44,535 --> 00:01:48,160
We can also perform clustering or community detection,

35
00:01:48,160 --> 00:01:50,410
where the goal is to identify, um,

36
00:01:50,410 --> 00:01:53,680
closely neat, uh, subparts of the graph, uh,

37
00:01:53,680 --> 00:01:57,530
where nodes are densely connected or highly connected with each other.

38
00:01:57,530 --> 00:02:00,970
Um, and application of these could be social circle detection.

39
00:02:00,970 --> 00:02:04,140
And- and then there are also other types of tasks.

40
00:02:04,140 --> 00:02:09,039
For example, graph generation or graph, um, uh, evolution,

41
00:02:09,039 --> 00:02:11,080
where graph generation could be, for example,

42
00:02:11,080 --> 00:02:14,970
used for drug discovery to generate novel molecular structures.

43
00:02:14,970 --> 00:02:19,030
And graph- and predicting graph evolution is very useful, uh,

44
00:02:19,030 --> 00:02:24,360
in physics where we wanna run accurate simulations of various kinds of physics phenomena,

45
00:02:24,360 --> 00:02:27,420
and that can be represented, um, as a graph.

46
00:02:27,420 --> 00:02:31,625
So in all these machine learning tasks, uh, we use, uh,

47
00:02:31,625 --> 00:02:33,470
we use, uh, graphs, uh,

48
00:02:33,470 --> 00:02:36,500
which leads to high, uh, impact applications.

49
00:02:36,500 --> 00:02:39,475
And now I wanna give you some examples of them.

50
00:02:39,475 --> 00:02:41,990
So first, I'm going to give you some examples of

51
00:02:41,990 --> 00:02:44,800
node level machine learning applications.

52
00:02:44,800 --> 00:02:49,175
So, um, a very recent one announced at the end of December,

53
00:02:49,175 --> 00:02:50,570
uh, this year is,

54
00:02:50,570 --> 00:02:52,130
uh, the following problem.

55
00:02:52,130 --> 00:02:53,720
It's called protein folding,

56
00:02:53,720 --> 00:02:55,540
where basically in our bodies,

57
00:02:55,540 --> 00:02:58,130
we have these molecules called proteins that

58
00:02:58,130 --> 00:03:02,120
regulate various biological processes, and for example,

59
00:03:02,120 --> 00:03:03,365
the way that drugs, uh,

60
00:03:03,365 --> 00:03:07,175
work is to bind or change behavior of

61
00:03:07,175 --> 00:03:12,290
different proteins which then then changes the biological processes in our body,

62
00:03:12,290 --> 00:03:13,850
and this way, uh, for example,

63
00:03:13,850 --> 00:03:16,030
we- we- we get cured or we we heal.

64
00:03:16,030 --> 00:03:19,170
Um, proteins are- are composed,

65
00:03:19,170 --> 00:03:21,075
uh, um, of amino acids.

66
00:03:21,075 --> 00:03:25,175
And we can think of our protein as a sequence of amino acids.

67
00:03:25,175 --> 00:03:28,610
However, due magnetic and different kinds of forces,

68
00:03:28,610 --> 00:03:31,830
these- these proteins are not these, um, uh,

69
00:03:31,830 --> 00:03:36,960
chains or strains, but the- they are actually- they actually fold,

70
00:03:36,960 --> 00:03:39,750
um, in very complex, uh, shapes.

71
00:03:39,750 --> 00:03:44,825
And one of the very important problems in biology, a very, uh, um,

72
00:03:44,825 --> 00:03:49,835
a problem that hasn't yet been solved is given up- a sequence of amino acids,

73
00:03:49,835 --> 00:03:53,570
can you predict the 3D structure of the underlying protein?

74
00:03:53,570 --> 00:03:56,815
So the computational task that, um,

75
00:03:56,815 --> 00:04:01,745
scientists have been running competitions about since, um, uh,

76
00:04:01,745 --> 00:04:05,900
'70s is about how do we computation- computationally predict

77
00:04:05,900 --> 00:04:11,240
protein's 3D structure based solely on its amino acid sequence.

78
00:04:11,240 --> 00:04:14,095
Um, and here I show you a few, um, uh,

79
00:04:14,095 --> 00:04:17,615
the three-dimensional structure of two different proteins,

80
00:04:17,615 --> 00:04:20,360
and what you can see is that- that this folding of

81
00:04:20,360 --> 00:04:23,960
a protein is- is very complex based on its,

82
00:04:23,960 --> 00:04:25,910
uh, amino acid structure.

83
00:04:25,910 --> 00:04:27,155
So the question is,

84
00:04:27,155 --> 00:04:28,790
given a sequence of amino acids,

85
00:04:28,790 --> 00:04:31,355
can we predict the three-dimensional structure,

86
00:04:31,355 --> 00:04:33,170
um, of the protein?

87
00:04:33,170 --> 00:04:36,535
And this is the problem that has been just recently solved.

88
00:04:36,535 --> 00:04:39,625
In the middle of December of, uh, 2020, uh,

89
00:04:39,625 --> 00:04:44,920
DeepMind announced Alpa- AlphaFold that increased the performance, um,

90
00:04:44,920 --> 00:04:47,005
or the accuracy of this,

91
00:04:47,005 --> 00:04:48,850
uh, protein folding, uh,

92
00:04:48,850 --> 00:04:54,490
applications by 30 percent all the way up to the values that are in high 90s.

93
00:04:54,490 --> 00:04:57,585
And here I just show a couple of, um, uh,

94
00:04:57,585 --> 00:05:00,390
titles of articles in media,

95
00:05:00,390 --> 00:05:02,850
uh, about how an important, uh,

96
00:05:02,850 --> 00:05:04,705
achievement this- this has been,

97
00:05:04,705 --> 00:05:07,639
how it changed the biology forever,

98
00:05:07,639 --> 00:05:11,905
how it solved the- one of the largest scientific open problems,

99
00:05:11,905 --> 00:05:16,420
and how this will turbocharge drug discovery and all kinds of,

100
00:05:16,420 --> 00:05:20,285
uh, important, um, implications that this has.

101
00:05:20,285 --> 00:05:23,510
And what is interesting in this, uh, scientific, uh,

102
00:05:23,510 --> 00:05:27,050
AI machine learning breakthrough is that the key idea that

103
00:05:27,050 --> 00:05:30,815
made this possible was to represent the underlying,

104
00:05:30,815 --> 00:05:33,025
uh, protein as a graph.

105
00:05:33,025 --> 00:05:36,325
Uh, and here they represented it as a spatial graph,

106
00:05:36,325 --> 00:05:40,475
where nodes in this graph were amino acids in the protein sequence,

107
00:05:40,475 --> 00:05:44,730
and the edges corresponded to, um,

108
00:05:44,730 --> 00:05:49,115
ami- to nodes- to amino acids that are spatially close to each other.

109
00:05:49,115 --> 00:05:52,639
So this means that now given the positions, um,

110
00:05:52,639 --> 00:05:57,290
of all the amino acids and the edges proximities between them,

111
00:05:57,290 --> 00:05:59,630
the graph neural network, uh,

112
00:05:59,630 --> 00:06:03,680
approach was trained that it predicted the new positions,

113
00:06:03,680 --> 00:06:06,750
uh, of the- of the, um, amino acids.

114
00:06:06,750 --> 00:06:11,480
And this way, uh, the folding of the protein was able to be simulated and the-

115
00:06:11,480 --> 00:06:17,120
and the posi- the final positions of the molecules were able to be, uh, predicted.

116
00:06:17,120 --> 00:06:19,940
So the key ingredient in making this work,

117
00:06:19,940 --> 00:06:24,350
in making this scientific breakthrough in protein folding was

118
00:06:24,350 --> 00:06:29,815
the use of graph representation and the graph neural network, uh, technology.

119
00:06:29,815 --> 00:06:33,150
Now, uh, this was on the level of nodes,

120
00:06:33,150 --> 00:06:34,940
where basically for every node in the graph,

121
00:06:34,940 --> 00:06:37,325
we tried to predict its, um,

122
00:06:37,325 --> 00:06:39,125
uh, position in space,

123
00:06:39,125 --> 00:06:40,880
and this way, uh,

124
00:06:40,880 --> 00:06:45,130
tell what is the three-dimensional organization of a protein.

125
00:06:45,130 --> 00:06:49,310
Now we are going to talk about edge-level machine learning task,

126
00:06:49,310 --> 00:06:52,340
where we are basically doing link prediction or trying to

127
00:06:52,340 --> 00:06:56,075
understand relationship between different nodes.

128
00:06:56,075 --> 00:06:59,735
The first example of this is in recommender systems,

129
00:06:59,735 --> 00:07:04,835
where basically we can think of these as users interacting with items,

130
00:07:04,835 --> 00:07:07,010
items being products, movies,

131
00:07:07,010 --> 00:07:09,415
um, songs, and so on.

132
00:07:09,415 --> 00:07:13,720
And nodes, uh, will be- we'll have two types of nodes.

133
00:07:13,720 --> 00:07:16,005
We will have users, and we would have items.

134
00:07:16,005 --> 00:07:21,870
And there is an edge between a user and an item if a user consumed, bought,

135
00:07:21,870 --> 00:07:26,645
reviewed, uh, a given item or listened to a given song or,

136
00:07:26,645 --> 00:07:29,020
uh, watched a given movie.

137
00:07:29,020 --> 00:07:33,890
And based on the structure of this graph and the properties of the users and the items,

138
00:07:33,890 --> 00:07:36,425
we would like to predict or recommend

139
00:07:36,425 --> 00:07:42,070
what other items given users might be interested in, uh, in the future.

140
00:07:42,070 --> 00:07:46,730
So we naturally have a bipartite graph and, um, a graph problem.

141
00:07:46,730 --> 00:07:51,305
And the modern recommender systems used in companies like, uh,

142
00:07:51,305 --> 00:07:53,900
Pinterest, LinkedIn, uh, Facebook,

143
00:07:53,900 --> 00:07:56,300
uh, Instagram, uh, Alibaba,

144
00:07:56,300 --> 00:08:01,040
um, and elsewhere are all based on these graphical representations

145
00:08:01,040 --> 00:08:06,275
and use graph representation learning and graph neural networks to make predictions.

146
00:08:06,275 --> 00:08:09,410
And the key insight here is that we can basically

147
00:08:09,410 --> 00:08:13,224
learn how to embed or how to represent nodes,

148
00:08:13,224 --> 00:08:16,160
um, of this graph such that related nodes are

149
00:08:16,160 --> 00:08:19,670
embedded closer to each other than nodes that are not related.

150
00:08:19,670 --> 00:08:22,264
And for example, in case of Pinterest,

151
00:08:22,264 --> 00:08:23,510
we can think of, uh,

152
00:08:23,510 --> 00:08:26,075
Pinterest images as nodes in the graph,

153
00:08:26,075 --> 00:08:28,530
and the goal is to embed, um,

154
00:08:28,530 --> 00:08:31,615
nodes that are related- images that are related

155
00:08:31,615 --> 00:08:35,150
closer together than images that are not related.

156
00:08:35,150 --> 00:08:36,605
For example, this, uh,

157
00:08:36,605 --> 00:08:39,070
sweater and the cake.

158
00:08:39,070 --> 00:08:43,130
And the way one can do this is to create this type of bipartite network,

159
00:08:43,130 --> 00:08:45,590
where we have the images on the top, and we can have,

160
00:08:45,590 --> 00:08:49,205
for example, users or Pinterest boards at the bottom.

161
00:08:49,205 --> 00:08:52,820
And then we can define a neural network approach that will take

162
00:08:52,820 --> 00:08:56,990
the feature information or attribute information of these different pins,

163
00:08:56,990 --> 00:08:58,850
so basically the content of the image,

164
00:08:58,850 --> 00:09:04,145
and transform it across the underlying graph to come up with a robust embedding,

165
00:09:04,145 --> 00:09:05,940
uh, of a given, uh, image.

166
00:09:05,940 --> 00:09:08,630
And it turns out that this approach works much,

167
00:09:08,630 --> 00:09:12,440
much better than if you would just consider images by themselves.

168
00:09:12,440 --> 00:09:15,230
So images plus the graph structure leads to

169
00:09:15,230 --> 00:09:18,970
much better recommendations than the image themselves.

170
00:09:18,970 --> 00:09:21,785
So here in this example of the task,

171
00:09:21,785 --> 00:09:26,510
it is about understanding relationships between pairs of nodes or pairs of

172
00:09:26,510 --> 00:09:29,060
images by basically saying that nodes that are

173
00:09:29,060 --> 00:09:31,750
related should be embedded closer together,

174
00:09:31,750 --> 00:09:36,530
the distance between them should be smaller than the distance between,

175
00:09:36,530 --> 00:09:39,935
uh, pairs of images that are not related to each other.

176
00:09:39,935 --> 00:09:45,140
Um, another example of a link level prediction task is very different.

177
00:09:45,140 --> 00:09:46,790
This is about, uh,

178
00:09:46,790 --> 00:09:48,960
drug combination side effects.

179
00:09:48,960 --> 00:09:52,550
Uh, the problem here is that many patients take

180
00:09:52,550 --> 00:09:58,040
multiple drugs simultaneously to trick- to treat complex and coexisting diseases.

181
00:09:58,040 --> 00:10:01,595
For example, in the United States, basically,

182
00:10:01,595 --> 00:10:07,630
fif- 50 percent of people over 70 years of age simultaneously take four or,

183
00:10:07,630 --> 00:10:09,545
uh, five or more drugs.

184
00:10:09,545 --> 00:10:11,750
And there are many patients who take

185
00:10:11,750 --> 00:10:17,030
20- 20 plus drugs to treat many complex coexisting diseases.

186
00:10:17,030 --> 00:10:19,055
For example, somebody who suffers,

187
00:10:19,055 --> 00:10:21,890
uh, insomnia, suffers depression,

188
00:10:21,890 --> 00:10:23,270
and has a heart disease,

189
00:10:23,270 --> 00:10:26,720
all simultaneously will- will take many different drugs,

190
00:10:26,720 --> 00:10:28,760
uh, altogether at once.

191
00:10:28,760 --> 00:10:31,565
And the problem is that these drugs, uh,

192
00:10:31,565 --> 00:10:33,545
interact with each other, um,

193
00:10:33,545 --> 00:10:36,320
and they lead to new adverse side effects.

194
00:10:36,320 --> 00:10:40,220
So basically, the interactions between drugs leads to additional, uh,

195
00:10:40,220 --> 00:10:43,789
diseases, um, uh, or additional problems,

196
00:10:43,789 --> 00:10:45,385
uh, in that human.

197
00:10:45,385 --> 00:10:49,210
Uh, and of course, the number of combinations of different drugs is too big,

198
00:10:49,210 --> 00:10:52,910
so we cannot experimentally or in clinical trials test

199
00:10:52,910 --> 00:10:57,060
every combination of drugs to see what kind of side effects does it lead to.

200
00:10:57,060 --> 00:11:00,125
So the question is, can we build up predictive engine that for

201
00:11:00,125 --> 00:11:04,590
an arbitrary pair of drugs will predict how these drugs are going to interact,

202
00:11:04,590 --> 00:11:06,830
and what kind of adverse side effects,

203
00:11:06,830 --> 00:11:09,055
uh, they may cause?

204
00:11:09,055 --> 00:11:12,765
And this is also a graph problem.

205
00:11:12,765 --> 00:11:14,920
So let me tell you how we formulate it.

206
00:11:14,920 --> 00:11:17,560
Um, we create this, um,

207
00:11:17,560 --> 00:11:22,335
two-level heterogeneous network where triangles are the, uh, uh,

208
00:11:22,335 --> 00:11:24,570
different drugs and, um,

209
00:11:24,570 --> 00:11:27,810
circles are proteins in our bodies.

210
00:11:27,810 --> 00:11:32,070
And then the way drugs work is that they target the different proteins.

211
00:11:32,070 --> 00:11:34,800
So these are the edges between triangles and the circles.

212
00:11:34,800 --> 00:11:38,090
And, um, biologists have been mapping out

213
00:11:38,090 --> 00:11:41,505
the protein-protein interaction network where they

214
00:11:41,505 --> 00:11:44,970
experimentally test whether two proteins physically come

215
00:11:44,970 --> 00:11:49,500
together and interact to regulate a given biological process or function.

216
00:11:49,500 --> 00:11:51,800
So we also know, experimentally,

217
00:11:51,800 --> 00:11:54,575
which proteins interact with each other.

218
00:11:54,575 --> 00:11:57,335
And this is called a protein-protein interaction network,

219
00:11:57,335 --> 00:12:00,280
or also called the inter-rectum.

220
00:12:00,280 --> 00:12:02,615
And then the last set of links we have in

221
00:12:02,615 --> 00:12:05,910
this graph are the known side-effects where basically, for example,

222
00:12:05,910 --> 00:12:08,015
the link between the node C and node M

223
00:12:08,015 --> 00:12:10,695
says that if you take these two drus- drugs together,

224
00:12:10,695 --> 00:12:14,435
the side-effect of type R is knowing- known to occur.

225
00:12:14,435 --> 00:12:20,145
Of course, this network up here of side-effects is notoriously incomplete and,

226
00:12:20,145 --> 00:12:22,540
uh, has a lot of missing connections.

227
00:12:22,540 --> 00:12:25,400
So the question becomes, can we impute,

228
00:12:25,400 --> 00:12:27,560
can we predict the missing edges,

229
00:12:27,560 --> 00:12:30,170
missing connections, um, in this, uh,

230
00:12:30,170 --> 00:12:32,780
network that would basically tell, us um,

231
00:12:32,780 --> 00:12:36,740
how lay- what kind of side-effects can we expect if we take,

232
00:12:36,740 --> 00:12:40,230
uh, or if a person takes two drugs simultaneously?

233
00:12:40,230 --> 00:12:41,715
So the way we think of this,

234
00:12:41,715 --> 00:12:45,680
we think of it as a link prediction between triangular nodes of g- um,

235
00:12:45,680 --> 00:12:49,290
in the graph, where basically the question is, given, uh,

236
00:12:49,290 --> 00:12:53,700
the two drugs, what kind of side effects, uh, may occur?

237
00:12:53,700 --> 00:12:57,230
And what is interesting is that you can apply this method, um,

238
00:12:57,230 --> 00:12:59,630
very accurately and you can discover

239
00:12:59,630 --> 00:13:03,600
new side effects that haven't been known, uh, in the past.

240
00:13:03,600 --> 00:13:05,130
For example, in this, uh,

241
00:13:05,130 --> 00:13:06,780
in this case, um,

242
00:13:06,780 --> 00:13:08,360
the mo- the model, uh,

243
00:13:08,360 --> 00:13:12,980
outputted the top ten predictions it is most, uh, certain about,

244
00:13:12,980 --> 00:13:16,220
where basically the way you read it is to say if you think these two drugs,

245
00:13:16,220 --> 00:13:19,260
then this particular side effect is likely to occur.

246
00:13:19,260 --> 00:13:25,035
And, uh, none of these side-effects are actually in the da- in the official FDA database.

247
00:13:25,035 --> 00:13:26,940
So what the authors did here is they took

248
00:13:26,940 --> 00:13:30,000
the top 10 predictions from the model and then they

249
00:13:30,000 --> 00:13:31,710
looked in the medical literature and

250
00:13:31,710 --> 00:13:35,490
clinical medical notes to see if there- are there any,

251
00:13:35,490 --> 00:13:37,420
um, any reports that could,

252
00:13:37,420 --> 00:13:39,640
uh, tell us whether, uh,

253
00:13:39,640 --> 00:13:42,735
and provide evidence of whether this particular uh,

254
00:13:42,735 --> 00:13:45,120
pair of drugs could lead to a given side-effect.

255
00:13:45,120 --> 00:13:47,940
Then actually, for the five out of top 10,

256
00:13:47,940 --> 00:13:50,510
we actually, um, found, uh,

257
00:13:50,510 --> 00:13:54,290
that there is some research evidence that points that this,

258
00:13:54,290 --> 00:13:56,009
um, that this predictions,

259
00:13:56,009 --> 00:13:58,245
um, might actually be true.

260
00:13:58,245 --> 00:14:03,690
So these were the machine learning tasks at the level of pairs of nodes.

261
00:14:03,690 --> 00:14:08,490
So we talked about recommender systems and I talked about the side effect prediction.

262
00:14:08,490 --> 00:14:12,810
Now, I wanna talk about the sub-graph level machine learning task.

263
00:14:12,810 --> 00:14:14,855
Um, and here is one, um,

264
00:14:14,855 --> 00:14:17,535
very recent that we are all using every day.

265
00:14:17,535 --> 00:14:19,580
It's about traffic prediction.

266
00:14:19,580 --> 00:14:21,605
So for example, if today you open

267
00:14:21,605 --> 00:14:25,005
Google Maps and you say I wanna drive- drive from Stanford, uh,

268
00:14:25,005 --> 00:14:27,150
all the way up to Berkeley, uh,

269
00:14:27,150 --> 00:14:29,730
Google will tell you how long it will take you to get

270
00:14:29,730 --> 00:14:32,610
there and what is your estimated time of arrival.

271
00:14:32,610 --> 00:14:34,470
And I'm not sure you knew,

272
00:14:34,470 --> 00:14:36,705
but actually, uh, in the end,

273
00:14:36,705 --> 00:14:41,505
graph machine learning is used to make these predictions of the travel time,

274
00:14:41,505 --> 00:14:47,105
and the way the graph is created is that nodes represent a road segments and,

275
00:14:47,105 --> 00:14:49,955
uh, connectivity between road segments,

276
00:14:49,955 --> 00:14:53,100
um, is captured by the edges of this network.

277
00:14:53,100 --> 00:14:57,360
And then, um, our graph neural network approach is-

278
00:14:57,360 --> 00:15:01,630
is trained that based on the conditions, uh, uh,

279
00:15:01,630 --> 00:15:04,740
and traffic patterns on each of the road segment, um,

280
00:15:04,740 --> 00:15:09,390
as well as the path between the source and the destination, um,

281
00:15:09,390 --> 00:15:11,760
uh, of the- of the journey, uh,

282
00:15:11,760 --> 00:15:16,155
the graph neural network approach is trained to predict the estimate that,

283
00:15:16,155 --> 00:15:18,795
uh, time of arrival or, uh, travel time.

284
00:15:18,795 --> 00:15:22,290
Um, and this- and it has been announced that actually this, um,

285
00:15:22,290 --> 00:15:27,000
graph-based approach is used in production in Google Maps, so whenever, uh,

286
00:15:27,000 --> 00:15:29,370
you are asking for directions,

287
00:15:29,370 --> 00:15:31,980
there is actually a graph machine learning-based approach

288
00:15:31,980 --> 00:15:34,810
that tells you when are you going to arrive,

289
00:15:34,810 --> 00:15:36,895
uh, to a given location.

290
00:15:36,895 --> 00:15:42,620
And last, I wanna talk about graph-level machine learning tasks,

291
00:15:42,620 --> 00:15:47,065
uh, and some interesting impactful applications of graph-level tasks.

292
00:15:47,065 --> 00:15:51,125
Um, one very recent is around drug discovery.

293
00:15:51,125 --> 00:15:54,000
And actually, graph- graph-based machine learning was

294
00:15:54,000 --> 00:15:57,960
used to discover new drugs, new antibiotics, right?

295
00:15:57,960 --> 00:16:02,615
Antibiotics are small molecular graphs and we can represent molecules

296
00:16:02,615 --> 00:16:07,550
as graphs where the nodes are atoms and edges correspond to chemical bonds.

297
00:16:07,550 --> 00:16:10,575
So each molecule can be represented as a graph.

298
00:16:10,575 --> 00:16:13,334
But then we ca- we have these banks,

299
00:16:13,334 --> 00:16:16,335
uh, or collections of billions of molecules.

300
00:16:16,335 --> 00:16:17,865
And the question is,

301
00:16:17,865 --> 00:16:21,180
which molecules could have, uh, therapeutic effect.

302
00:16:21,180 --> 00:16:24,450
So essentially, which molecules should be prioritized so that

303
00:16:24,450 --> 00:16:28,710
biologists can pass them in the laboratory to validate or,

304
00:16:28,710 --> 00:16:31,515
um, their ther- therapeutic effect.

305
00:16:31,515 --> 00:16:35,900
And actually, a team at MIT was using, um, graph, uh,

306
00:16:35,900 --> 00:16:37,940
based deep learning approach for

307
00:16:37,940 --> 00:16:42,815
antibiotic discovery where they used a graph neural network, uh,

308
00:16:42,815 --> 00:16:48,900
to classify different molecules and predict promising molecules from a pool of,

309
00:16:48,900 --> 00:16:50,385
uh, billions of candidates.

310
00:16:50,385 --> 00:16:52,265
And then these predictions would have further,

311
00:16:52,265 --> 00:16:54,360
uh, validated, uh, in the lab.

312
00:16:54,360 --> 00:16:56,840
And there is a very exciting, um,

313
00:16:56,840 --> 00:16:58,860
breakthrough paper published in,

314
00:16:58,860 --> 00:17:00,350
uh, journal cell, uh,

315
00:17:00,350 --> 00:17:02,070
just this year about how

316
00:17:02,070 --> 00:17:06,429
these graph-based approach allows us to efficiently and quickly discover,

317
00:17:06,429 --> 00:17:09,609
uh, new drugs and new therapeutic uses of different,

318
00:17:09,609 --> 00:17:11,804
uh, types of molecules.

319
00:17:11,805 --> 00:17:16,119
Um, to further talk about drug discovery, uh,

320
00:17:16,119 --> 00:17:20,069
we can think also about graph generation as a way to

321
00:17:20,069 --> 00:17:25,834
discover new molecules that have never been synthesized or considered, uh, before.

322
00:17:25,835 --> 00:17:30,815
And this is very useful because it allows us to generate new structures,

323
00:17:30,815 --> 00:17:34,485
new molecules in various kinds of targeted ways.

324
00:17:34,485 --> 00:17:38,450
For example, we can say generate new molecules that are non-toxic,

325
00:17:38,450 --> 00:17:42,430
generate new molecules that have high solubility,

326
00:17:42,430 --> 00:17:45,740
generate new molecules that have high drug likeness.

327
00:17:45,740 --> 00:17:51,740
So we can generate now molecules as graphs in a targeted way. Not even that.

328
00:17:51,740 --> 00:17:54,285
The second use case is that we can optimize

329
00:17:54,285 --> 00:17:57,485
existing molecules to have a desirable property.

330
00:17:57,485 --> 00:18:02,170
So basically, the use case here is that you have a small part of the molecule that has,

331
00:18:02,170 --> 00:18:04,460
uh, a given therapeutic effect, for example.

332
00:18:04,460 --> 00:18:06,335
And now we wanna complete, uh,

333
00:18:06,335 --> 00:18:10,040
the rest of the molecule scaffold so that you improve,

334
00:18:10,040 --> 00:18:12,480
um, a given property.

335
00:18:12,480 --> 00:18:14,585
For example- for example, uh,

336
00:18:14,585 --> 00:18:17,625
solubility and this type of deep graph, uh,

337
00:18:17,625 --> 00:18:20,920
generative models, uh, can be used for tasks,

338
00:18:20,920 --> 00:18:25,160
uh, like uh, molecule generation and optimization.

339
00:18:25,160 --> 00:18:29,525
So, um, and the last graph-level task that I

340
00:18:29,525 --> 00:18:33,890
want to talk about is a realistic, uh, physics-based simulation.

341
00:18:33,890 --> 00:18:37,685
In this case, we can basically have different materials.

342
00:18:37,685 --> 00:18:41,070
We represent the material as a set of particles and then

343
00:18:41,070 --> 00:18:44,690
we can have a graph defined on top of,

344
00:18:44,690 --> 00:18:50,070
uh, these, um, set of particles that capture which particles interact with each other.

345
00:18:50,070 --> 00:18:54,095
And now the underlying task for the machine learning is to say,

346
00:18:54,095 --> 00:18:57,395
predict how this graph is going to evolve in the future.

347
00:18:57,395 --> 00:19:02,385
And this allows us to predict how this material is going to deform.

348
00:19:02,385 --> 00:19:05,070
Um, so let me tell you how this is done.

349
00:19:05,070 --> 00:19:08,115
The way this is done is that essentially we iterate,

350
00:19:08,115 --> 00:19:09,750
um, the following approach.

351
00:19:09,750 --> 00:19:14,230
We take the material and we represent it as a set of particles.

352
00:19:14,230 --> 00:19:15,720
Based on the proximities,

353
00:19:15,720 --> 00:19:17,565
interactions between the particles,

354
00:19:17,565 --> 00:19:20,040
we generated the proximity graph.

355
00:19:20,040 --> 00:19:23,150
Now, that we have this, uh, proximity graph,

356
00:19:23,150 --> 00:19:25,490
we apply graph machine learning,

357
00:19:25,490 --> 00:19:26,880
a graph neural network,

358
00:19:26,880 --> 00:19:30,914
that takes the current properties, meaning positions,

359
00:19:30,914 --> 00:19:35,525
as well as velocities of the particles and predict what will be the,

360
00:19:35,525 --> 00:19:38,900
uh, positions and velocities of the particles in the future.

361
00:19:38,900 --> 00:19:41,670
And now based on this prediction, we can move,

362
00:19:41,670 --> 00:19:45,920
evolve the particles to their new positions, and then again,

363
00:19:45,920 --> 00:19:49,529
we go to the first step where now based on this new proximities,

364
00:19:49,529 --> 00:19:50,780
we create the new graph,

365
00:19:50,780 --> 00:19:52,495
predict the new positions,

366
00:19:52,495 --> 00:19:55,865
um, move the particles and keep iterating this.

367
00:19:55,865 --> 00:20:02,670
And this allows for very fast and very accurate physics-based simulations.

368
00:20:02,670 --> 00:20:06,620
So these were some examples of graph,

369
00:20:06,620 --> 00:20:09,605
uh, of a graph-level tasks and, uh,

370
00:20:09,605 --> 00:20:14,905
important applications of graph machine learning to various domains, um,

371
00:20:14,905 --> 00:20:16,865
across, uh, across, uh,

372
00:20:16,865 --> 00:20:21,850
sciences, industry, as well as different consumer products.

