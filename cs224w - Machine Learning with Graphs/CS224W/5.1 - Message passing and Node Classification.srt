1
00:00:04,010 --> 00:00:07,110
Thank you so much everyone for attending,

2
00:00:07,110 --> 00:00:09,510
exciting to be here and to talk about,

3
00:00:09,510 --> 00:00:11,445
uh, the next topic in this class.

4
00:00:11,445 --> 00:00:13,380
Today we are going to discuss, uh,

5
00:00:13,380 --> 00:00:16,125
message passing and node classification.

6
00:00:16,125 --> 00:00:20,660
Um, and this is an intermediate topic that we are going to use, um,

7
00:00:20,660 --> 00:00:23,270
uh, so that we can then move to,

8
00:00:23,270 --> 00:00:25,270
uh, graph, uh, neural networks.

9
00:00:25,270 --> 00:00:28,260
Um, so let's talk about how,

10
00:00:28,260 --> 00:00:30,135
uh, we are going to think about this.

11
00:00:30,135 --> 00:00:35,630
So the idea for today is that we are given a network with labels on some nodes.

12
00:00:35,630 --> 00:00:39,790
And the question is, how do we assign labels to all other nodes in the network?

13
00:00:39,790 --> 00:00:42,870
Um, so the example is that in a network,

14
00:00:42,870 --> 00:00:44,280
some nodes are, let's say,

15
00:00:44,280 --> 00:00:47,150
fraudsters or un- untrustworthy nodes,

16
00:00:47,150 --> 00:00:50,185
and some other nodes are trusted, fully trusted.

17
00:00:50,185 --> 00:00:55,610
The question becomes, how do we find other fraudsters or trustworthy nodes,

18
00:00:55,610 --> 00:00:56,905
uh, in the network?

19
00:00:56,905 --> 00:00:58,850
Um, and we have already, for example,

20
00:00:58,850 --> 00:01:00,860
discussed node embeddings method,

21
00:01:00,860 --> 00:01:03,050
um, in lecture 3.

22
00:01:03,050 --> 00:01:06,650
And we could say, let's use this node embedding methods to simply build

23
00:01:06,650 --> 00:01:10,770
a classifier that predicts which node is trusted,

24
00:01:10,770 --> 00:01:13,170
and which node is, uh, not trusted.

25
00:01:13,170 --> 00:01:17,875
Uh, however, today, we are going to think about this a bit differently.

26
00:01:17,875 --> 00:01:19,970
We are going to de- think about this in what is

27
00:01:19,970 --> 00:01:23,120
called semi-supervised node classification,

28
00:01:23,120 --> 00:01:25,600
where we will be given both, uh,

29
00:01:25,600 --> 00:01:27,335
some nodes that are labeled,

30
00:01:27,335 --> 00:01:30,320
let's say labeled with the green and the red color,

31
00:01:30,320 --> 00:01:34,640
and some other nodes that are unlabeled and they will be all part of the same network.

32
00:01:34,640 --> 00:01:35,915
So the question will be,

33
00:01:35,915 --> 00:01:39,590
how do we predict labels of the unlabeled node?

34
00:01:39,590 --> 00:01:41,270
Uh, and this is called,

35
00:01:41,270 --> 00:01:43,775
uh, semi-supervised node classification,

36
00:01:43,775 --> 00:01:48,530
semi-supervised because we are both given the supervised signal, so the labels,

37
00:01:48,530 --> 00:01:50,570
as well as the unsupervised signal,

38
00:01:50,570 --> 00:01:52,235
as well as the non-labels,

39
00:01:52,235 --> 00:01:54,535
uh, all at the same time.

40
00:01:54,535 --> 00:01:57,685
And the idea is that we wanna do this

41
00:01:57,685 --> 00:02:00,400
in what is called a message passing framework, right?

42
00:02:00,400 --> 00:02:02,020
We would like to- basically,

43
00:02:02,020 --> 00:02:04,300
given one big network partially labeled,

44
00:02:04,300 --> 00:02:06,130
we'd like to infer the labels,

45
00:02:06,130 --> 00:02:08,225
uh, of the unlabeled nodes.

46
00:02:08,225 --> 00:02:13,600
Uh, and we are going to do this by doing what is called message passing over the network.

47
00:02:13,600 --> 00:02:16,840
Uh, and the intuition here will be that we wanna exploit,

48
00:02:16,840 --> 00:02:19,810
uh, correlations that exist in the network.

49
00:02:19,810 --> 00:02:22,105
So what I mean by correlations,

50
00:02:22,105 --> 00:02:27,100
I mean that nodes that share labels tend to be connected, right?

51
00:02:27,100 --> 00:02:30,445
So we will have this notion of collective classification,

52
00:02:30,445 --> 00:02:34,500
where basically the idea will be that nodes are going to, uh, um,

53
00:02:34,500 --> 00:02:39,539
update what they believe is their own labels based on the labels of the neighbors,

54
00:02:39,539 --> 00:02:40,760
uh, in the network.

55
00:02:40,760 --> 00:02:46,470
And here, conceptually, this is also similar to what we were discussing, uh, in, uh,

56
00:02:46,470 --> 00:02:50,809
pager link lecture, where the pager link score has- was updated,

57
00:02:50,809 --> 00:02:52,370
uh, based on the scores,

58
00:02:52,370 --> 00:02:53,380
uh, of the neighbors.

59
00:02:53,380 --> 00:02:55,450
But here we are not going to update the score,

60
00:02:55,450 --> 00:02:58,625
we're going to update the belief,

61
00:02:58,625 --> 00:03:01,790
the prediction about what is the label about a given node.

62
00:03:01,790 --> 00:03:06,075
And we are going to talk about three classical techniques.

63
00:03:06,075 --> 00:03:08,715
One is called relational classification,

64
00:03:08,715 --> 00:03:11,055
the other one is iterative classification,

65
00:03:11,055 --> 00:03:14,465
and then last we are going to talk about belief propagation.

66
00:03:14,465 --> 00:03:16,130
And all these methods,

67
00:03:16,130 --> 00:03:18,080
are kind of a bit old school methods,

68
00:03:18,080 --> 00:03:21,860
but they give us a lot of intuition for what we are going to talk in

69
00:03:21,860 --> 00:03:24,440
the next few weeks when we are going to focus

70
00:03:24,440 --> 00:03:27,740
on deep learning on graphs and in particular,

71
00:03:27,740 --> 00:03:29,200
uh, graph neural networks.

72
00:03:29,200 --> 00:03:32,405
So today, kind of as a starter topic into that,

73
00:03:32,405 --> 00:03:34,520
we are going to talk about, um, uh,

74
00:03:34,520 --> 00:03:38,335
these three topics of, uh, collective classification.

75
00:03:38,335 --> 00:03:41,885
So when I said correlations exist in networks,

76
00:03:41,885 --> 00:03:44,434
what I mean by this is that individual behaviors

77
00:03:44,434 --> 00:03:47,075
are often correlated in the network structure.

78
00:03:47,075 --> 00:03:51,570
And correlation means that nearby nodes tend to have the same color,

79
00:03:51,570 --> 00:03:53,030
tend to have the same label.

80
00:03:53,030 --> 00:03:54,440
They tend to be- uh- uh,

81
00:03:54,440 --> 00:03:56,875
belong to the same class.

82
00:03:56,875 --> 00:04:00,920
Um, and there are two reasons, specially, for example,

83
00:04:00,920 --> 00:04:04,070
in social networks, but also in other types of networks,

84
00:04:04,070 --> 00:04:06,300
for why this might be happening.

85
00:04:06,300 --> 00:04:08,630
First is this notion of homophily,

86
00:04:08,630 --> 00:04:11,900
where basically individual characteristics, um,

87
00:04:11,900 --> 00:04:18,300
uh, um mean that people of similar characteristics tend to link each other.

88
00:04:18,300 --> 00:04:20,795
This is notion of homophily from social science.

89
00:04:20,795 --> 00:04:23,344
And then there is also this notion of influence,

90
00:04:23,344 --> 00:04:24,935
where the idea is that, uh,

91
00:04:24,935 --> 00:04:29,550
social connections influence our own characteristics or our own behaviors.

92
00:04:29,550 --> 00:04:34,010
So let me kind of give you a bit of motivation from the social science point of view,

93
00:04:34,010 --> 00:04:38,250
why these correlations exist in networks and why network data,

94
00:04:38,250 --> 00:04:39,600
um, is so useful.

95
00:04:39,600 --> 00:04:43,160
So homophily is defined as the tendency of

96
00:04:43,160 --> 00:04:48,260
individuals to associate or bond with similar others.

97
00:04:48,260 --> 00:04:50,390
And one way to think of this is to say,

98
00:04:50,390 --> 00:04:53,015
birds of feather flock together, so right?

99
00:04:53,015 --> 00:04:55,490
People that are similar tend to bond,

100
00:04:55,490 --> 00:04:57,355
they tend to link with each other.

101
00:04:57,355 --> 00:05:01,790
And this phenomenon has been observed in a vast array of different,

102
00:05:01,790 --> 00:05:03,635
uh, social networks studies, uh,

103
00:05:03,635 --> 00:05:06,230
on a variety of attributes in terms of age,

104
00:05:06,230 --> 00:05:10,140
gender, organization, a little social status, um,

105
00:05:10,140 --> 00:05:12,260
any kind of preferences,

106
00:05:12,260 --> 00:05:16,160
political preferences, food preferences, and so on.

107
00:05:16,160 --> 00:05:18,470
Um, and one example would be, for example,

108
00:05:18,470 --> 00:05:20,595
that researchers who focus, uh,

109
00:05:20,595 --> 00:05:25,120
on the same research area are more li- more likely to collaborate with each other.

110
00:05:25,120 --> 00:05:29,255
Or researchers who focus on the same area are more likely to know each other

111
00:05:29,255 --> 00:05:33,740
or be friends with each other naturally because they attend the same conference,

112
00:05:33,740 --> 00:05:35,495
they interact with each other, and, uh,

113
00:05:35,495 --> 00:05:37,995
connections between them, uh, get formed.

114
00:05:37,995 --> 00:05:40,895
So this is in terms of, um, homophily.

115
00:05:40,895 --> 00:05:42,815
And to give you an example,

116
00:05:42,815 --> 00:05:45,365
this is an online social network,

117
00:05:45,365 --> 00:05:48,375
uh, from a high school, uh, where,

118
00:05:48,375 --> 00:05:49,920
uh, nodes are people,

119
00:05:49,920 --> 00:05:51,915
uh, edges are friendships,

120
00:05:51,915 --> 00:05:56,360
and color denotes their interests in terms of sports and arts.

121
00:05:56,360 --> 00:05:59,420
And what you notice immediately from this visualization is that there

122
00:05:59,420 --> 00:06:02,705
seems to be kind of four groups, uh, of people.

123
00:06:02,705 --> 00:06:04,910
And it seems that they are very much grouped

124
00:06:04,910 --> 00:06:07,710
based on this node color or based on interests, right?

125
00:06:07,710 --> 00:06:11,450
Is that green nodes tend to link with each other and, um,

126
00:06:11,450 --> 00:06:14,060
and ye- uh, yellow nodes,

127
00:06:14,060 --> 00:06:16,320
uh, tend to leak- link, uh, with each other.

128
00:06:16,320 --> 00:06:19,070
So it means that people with same interests are more

129
00:06:19,070 --> 00:06:22,550
likely or more closely connected due to this,

130
00:06:22,550 --> 00:06:27,095
um, effect of or this phenomena called homophily.

131
00:06:27,095 --> 00:06:31,580
Another, um, phenomena, or another, uh,

132
00:06:31,580 --> 00:06:36,860
force that creates these correlations in networks is kind of the other way around, right?

133
00:06:36,860 --> 00:06:40,025
If in homophily we say people have characteristics

134
00:06:40,025 --> 00:06:43,590
and people with similar characteristics tend to link to each other,

135
00:06:43,590 --> 00:06:48,620
the notion of social influence kind of flips the a- the,

136
00:06:48,620 --> 00:06:50,480
uh, the- the arrow in some sense, right?

137
00:06:50,480 --> 00:06:52,775
So it says that social connections can

138
00:06:52,775 --> 00:06:56,120
influence the individual characteristics of a person, right?

139
00:06:56,120 --> 00:07:01,865
So for example, if I recommend my musical preferences to my friends and I'm, you know,

140
00:07:01,865 --> 00:07:04,194
very, very, uh, persistent,

141
00:07:04,194 --> 00:07:08,885
perhaps one of them will- will grow to like my favorite genres, my favorite music.

142
00:07:08,885 --> 00:07:12,680
So this means that now this person just became more similar to me, right?

143
00:07:12,680 --> 00:07:14,075
It means we're very connected.

144
00:07:14,075 --> 00:07:17,180
And I influence them to kind of change their behavior,

145
00:07:17,180 --> 00:07:20,975
to change their preference so that the two of us are more similar.

146
00:07:20,975 --> 00:07:23,480
Which- which one explanation would be is, you know,

147
00:07:23,480 --> 00:07:25,520
this makes our bond even stronger,

148
00:07:25,520 --> 00:07:26,900
even easier to maintain.

149
00:07:26,900 --> 00:07:29,825
So here it was the social connection that

150
00:07:29,825 --> 00:07:33,750
affected or influenced the individual characteristic.

151
00:07:33,750 --> 00:07:36,215
And both of these phenomena are very,

152
00:07:36,215 --> 00:07:40,174
very common and very strong in, um, social networks.

153
00:07:40,174 --> 00:07:44,080
Um, and the correlations also exist in many other types of networks.

154
00:07:44,080 --> 00:07:46,940
And this is really the main intuition that we

155
00:07:46,940 --> 00:07:50,275
are going to exploit in today's, uh, lecture.

156
00:07:50,275 --> 00:07:52,115
So the question will be,

157
00:07:52,115 --> 00:07:56,690
how do we leverage this notion of correlation across the edges of the network,

158
00:07:56,690 --> 00:07:58,415
observe the networks, uh,

159
00:07:58,415 --> 00:08:00,650
to help to predict node labels, right?

160
00:08:00,650 --> 00:08:05,250
When I say correlation, I mean nodes that are connected tend to have the same label,

161
00:08:05,250 --> 00:08:07,385
tend to have the same preferences.

162
00:08:07,385 --> 00:08:09,125
So the question is,

163
00:08:09,125 --> 00:08:12,530
given this partially labeled network, you know, green,

164
00:08:12,530 --> 00:08:16,055
let's call that a positive class a- a- a label 1,

165
00:08:16,055 --> 00:08:18,980
and red will be what I'll call our negative class,

166
00:08:18,980 --> 00:08:21,425
and let's label it with label 0.

167
00:08:21,425 --> 00:08:24,680
And the gray nodes are the nodes that don't have the color yet.

168
00:08:24,680 --> 00:08:29,160
And the question is, how would I come up with an algorithm to learn,

169
00:08:29,160 --> 00:08:34,005
uh, or to predict the colors of, um, gray nodes?

170
00:08:34,005 --> 00:08:37,700
So the motivation is that similar nodes are

171
00:08:37,700 --> 00:08:41,965
typically close together or directly connected in the network.

172
00:08:41,965 --> 00:08:44,535
So the principal we are going to, uh,

173
00:08:44,535 --> 00:08:47,570
use is also known as guilt by association,

174
00:08:47,570 --> 00:08:51,325
in a sense that if I'm connected to a node we'd label X,

175
00:08:51,325 --> 00:08:54,860
then I'm likely to have that label X as well.

176
00:08:54,860 --> 00:08:57,470
And that's this notion of correlation I was saying about, right?

177
00:08:57,470 --> 00:08:59,420
So if you could say about, let's say,

178
00:08:59,420 --> 00:09:02,480
um, the malicious and benign web pages, you could say,

179
00:09:02,480 --> 00:09:07,450
malicious web- webpages tend to link to one another to increase visibility,

180
00:09:07,450 --> 00:09:08,774
uh, and look credible,

181
00:09:08,774 --> 00:09:11,030
and rank higher in search engines.

182
00:09:11,030 --> 00:09:13,849
So we find out that one web page is malicious,

183
00:09:13,849 --> 00:09:18,350
then perhaps other pages that link towards it also tend to be,

184
00:09:18,350 --> 00:09:20,240
uh, malicious. All right?

185
00:09:20,240 --> 00:09:22,010
Um, that's intuition.

186
00:09:22,010 --> 00:09:25,850
So the way we are going to think of this is that we are going to, uh,

187
00:09:25,850 --> 00:09:29,725
determine the classification label of a node v in the network,

188
00:09:29,725 --> 00:09:32,300
that it will depend on two factors.

189
00:09:32,300 --> 00:09:33,890
It will depend on the properties,

190
00:09:33,890 --> 00:09:38,785
features of the node V. And it will also depend on the labels,

191
00:09:38,785 --> 00:09:44,685
um, of the neighbors of- of the node v of interest.

192
00:09:44,685 --> 00:09:51,020
Uh, and of course, because the v's label depends on the labels of, um,

193
00:09:51,020 --> 00:09:52,985
nodes in the neighborhood,

194
00:09:52,985 --> 00:09:57,200
those labels will also depend on the features of those nodes in the neighborhood.

195
00:09:57,200 --> 00:09:58,940
So- so kind of- um,

196
00:09:58,940 --> 00:10:03,345
this means that also the label of node v will depend on the features of the nodes,

197
00:10:03,345 --> 00:10:05,645
uh, in its, uh, neighborhood.

198
00:10:05,645 --> 00:10:10,150
So here is how we are thinking about this, uh, graphically.

199
00:10:10,150 --> 00:10:11,590
Given a graph and a few,

200
00:10:11,590 --> 00:10:13,555
uh, labeled nodes, um,

201
00:10:13,555 --> 00:10:18,490
right we want to find labeled class of, ah, remaining nodes.

202
00:10:18,490 --> 00:10:21,280
When I say label, in this case it will be positive or negative,

203
00:10:21,280 --> 00:10:22,630
it will be, um,

204
00:10:22,630 --> 00:10:24,490
green or it will be red.

205
00:10:24,490 --> 00:10:26,020
Um, and the main assumption,

206
00:10:26,020 --> 00:10:27,595
the main modeling assumption,

207
00:10:27,595 --> 00:10:31,315
the main inductive bias in this, in our approach,

208
00:10:31,315 --> 00:10:35,680
will be to assume that there is some degree of homophily in the network.

209
00:10:35,680 --> 00:10:37,045
So that basically these, uh,

210
00:10:37,045 --> 00:10:39,324
labels tend to cluster,

211
00:10:39,324 --> 00:10:43,915
meaning that nodes of the same label tend to link to each other.

212
00:10:43,915 --> 00:10:46,960
So to give you an example task,

213
00:10:46,960 --> 00:10:51,025
let A be an adjacency matrix over n nodes.

214
00:10:51,025 --> 00:10:53,695
This is basically captures the structure of the graph.

215
00:10:53,695 --> 00:10:56,169
This adjacency matrix can be unweighted,

216
00:10:56,169 --> 00:10:58,060
can be also weighted,

217
00:10:58,060 --> 00:11:01,075
all methods generalize to weighted graphs as well,

218
00:11:01,075 --> 00:11:03,310
can be undirected or directed.

219
00:11:03,310 --> 00:11:06,160
All the methods generalize to both types of graphs.

220
00:11:06,160 --> 00:11:10,360
And we will use the Y as a vector of node labels, right?

221
00:11:10,360 --> 00:11:14,110
So we'll say Y_v equals 1, if the, um,

222
00:11:14,110 --> 00:11:16,540
node V belongs to class 1,

223
00:11:16,540 --> 00:11:17,770
to the green color,

224
00:11:17,770 --> 00:11:20,170
and Y_v equals 0.

225
00:11:20,170 --> 00:11:23,065
If the node V belongs to the class 0,

226
00:11:23,065 --> 00:11:25,990
meaning it is labeled with a red color.

227
00:11:25,990 --> 00:11:29,395
And of course there may be also other unlabelled nodes

228
00:11:29,395 --> 00:11:32,800
that- that need to be- whose label needs to be predicted,

229
00:11:32,800 --> 00:11:35,140
whose labeling needs to be classified.

230
00:11:35,140 --> 00:11:38,740
And right now, they- we don't know, ah, the label.

231
00:11:38,740 --> 00:11:42,760
And the goal is predict which labeled nodes are likely to be of class

232
00:11:42,760 --> 00:11:46,450
1 and which ones are likely to be of class 0.

233
00:11:46,450 --> 00:11:49,585
So that's the idea of what we wanna do.

234
00:11:49,585 --> 00:11:55,420
Um, there are many examples of this notion of collective classification, right?

235
00:11:55,420 --> 00:11:56,890
You can think about, I wanna do

236
00:11:56,890 --> 00:12:00,070
document classification and documents linked to each other.

237
00:12:00,070 --> 00:12:04,030
I wanna do link prediction in, in, in graphs.

238
00:12:04,030 --> 00:12:06,580
And the links will depend on the properties,

239
00:12:06,580 --> 00:12:08,650
labels of the neighbors.

240
00:12:08,650 --> 00:12:11,905
Um, even like you can take certain other domains where you have

241
00:12:11,905 --> 00:12:16,600
optical character recognition and you can represent that as a graph and say, you know,

242
00:12:16,600 --> 00:12:19,150
the label of what character I am also

243
00:12:19,150 --> 00:12:22,660
depends on the labels what are the characters around me,

244
00:12:22,660 --> 00:12:23,995
meaning that I can,

245
00:12:23,995 --> 00:12:27,310
I-I know how to form let's say English,

246
00:12:27,310 --> 00:12:29,350
English words and you know what,

247
00:12:29,350 --> 00:12:32,530
some random character, sequence of characters is very unlikely.

248
00:12:32,530 --> 00:12:36,000
So, you know, whether I'm letter A or letter

249
00:12:36,000 --> 00:12:39,825
G will depends what my neighbors here in the,

250
00:12:39,825 --> 00:12:41,850
let's say the line graph think about.

251
00:12:41,850 --> 00:12:45,330
So there is a lot of different cases where you can basically want to

252
00:12:45,330 --> 00:12:50,275
make prediction about one object based on the relationships of the object to its,

253
00:12:50,275 --> 00:12:53,335
uh, nearby, uh, objects in terms of nodes,

254
00:12:53,335 --> 00:12:57,250
images, letters in OCR, part-of-speech tagging.

255
00:12:57,250 --> 00:12:59,694
In many other cases,

256
00:12:59,694 --> 00:13:03,130
knowing what- what- what are the labels of

257
00:13:03,130 --> 00:13:06,565
the nodes around you helps you determine your own label.

258
00:13:06,565 --> 00:13:08,605
That's essentially the idea.

259
00:13:08,605 --> 00:13:14,365
So collective classification that we are going to talk about today is going to have,

260
00:13:14,365 --> 00:13:17,050
um, three different parts, right?

261
00:13:17,050 --> 00:13:22,105
So the intuition we are going to have is that you wanna simultaneously classify, ah,

262
00:13:22,105 --> 00:13:28,240
linked nodes using address and propagate information across the edges of the network.

263
00:13:28,240 --> 00:13:30,520
And this will be our probabilistic framework.

264
00:13:30,520 --> 00:13:34,435
Where do we will be making what is called a Markov assumption.

265
00:13:34,435 --> 00:13:37,840
And a Markov assumption means that the label of a node,

266
00:13:37,840 --> 00:13:41,590
um, only depends on the labels of its neighbors, right?

267
00:13:41,590 --> 00:13:44,920
So this is a first-order Markov assumption because we only

268
00:13:44,920 --> 00:13:48,504
assume that a label depends on the label of the neighbors.

269
00:13:48,504 --> 00:13:50,140
And we don't- for example,

270
00:13:50,140 --> 00:13:54,655
assume that the label depends on the label of neighbors, of neighbors, right.

271
00:13:54,655 --> 00:13:56,050
Like degree 2 neighborhood.

272
00:13:56,050 --> 00:13:58,120
We only look at the degree 1 neighborhood.

273
00:13:58,120 --> 00:14:00,700
And this notion of collective classification,

274
00:14:00,700 --> 00:14:03,670
the reason why we use this experiment is because we are- we will be

275
00:14:03,670 --> 00:14:07,825
altogether classifying all the nodes on the graph because every,

276
00:14:07,825 --> 00:14:10,990
every nodes labeled depends on other nodes labeled.

277
00:14:10,990 --> 00:14:12,640
So we are going to iteratively,

278
00:14:12,640 --> 00:14:16,180
ah, reclassify, reclassify nodes.

279
00:14:16,180 --> 00:14:19,060
Nodes are going to update the belief for a prediction about

280
00:14:19,060 --> 00:14:22,750
the labels until the process will converge.

281
00:14:22,750 --> 00:14:26,515
And in order for us to do this kind of collective iterative classification,

282
00:14:26,515 --> 00:14:30,070
we will need three types of, ah, classifiers.

283
00:14:30,070 --> 00:14:34,870
We'll have these local classifier that assigns the initial label to the node,

284
00:14:34,870 --> 00:14:38,665
we'll then have what we call a relational classifier that

285
00:14:38,665 --> 00:14:42,490
captures between- correlations between nodes and you will basically say,

286
00:14:42,490 --> 00:14:44,620
aha, what are the labels of other nodes in

287
00:14:44,620 --> 00:14:47,905
the network of the neighbors of the node of interest.

288
00:14:47,905 --> 00:14:50,545
And then we'll have this notion of collective inference,

289
00:14:50,545 --> 00:14:54,249
where we will be- where we will be propagating these correlations,

290
00:14:54,249 --> 00:14:58,090
these beliefs over the network until the labels wi- will

291
00:14:58,090 --> 00:15:03,145
converge to some stable state or until some fixed number of iterations,

292
00:15:03,145 --> 00:15:05,215
um, will be achieved.

293
00:15:05,215 --> 00:15:10,360
So what are these three pieces that we need to define?

294
00:15:10,360 --> 00:15:13,390
First, we have this notion of our local classifier that

295
00:15:13,390 --> 00:15:16,855
will assign initial labels to unlabeled nodes.

296
00:15:16,855 --> 00:15:19,765
So this is used for initial label assignment.

297
00:15:19,765 --> 00:15:25,530
And it will predict label of node based on its attributes or features.

298
00:15:25,530 --> 00:15:29,970
Um, it is just a standard classification task where given a set of,

299
00:15:29,970 --> 00:15:31,530
ah, given features of a node,

300
00:15:31,530 --> 00:15:32,985
we wanna predict it's labeled.

301
00:15:32,985 --> 00:15:35,790
And these does not use the network information yet.

302
00:15:35,790 --> 00:15:42,610
So this is applied only once at the beginning to give initial labels to the gray nodes.

303
00:15:42,610 --> 00:15:45,400
Then we will define this notion of

304
00:15:45,400 --> 00:15:50,500
a relational classifier that will capture the correlations between the nodes.

305
00:15:50,500 --> 00:15:52,270
So what does this mean?

306
00:15:52,270 --> 00:15:56,950
Is that we learn another predictor that will predict a label of

307
00:15:56,950 --> 00:16:02,620
one node based on the labels or attributes of other nodes in its neighborhood.

308
00:16:02,620 --> 00:16:05,590
And this is where the network information will be used

309
00:16:05,590 --> 00:16:08,620
because this relational classifier will say what is,

310
00:16:08,620 --> 00:16:13,765
what is given nodes label based on the labels of the nodes that are connected to it.

311
00:16:13,765 --> 00:16:16,495
And this is where the network information is used.

312
00:16:16,495 --> 00:16:21,040
And then we won't only apply this relational classifier once,

313
00:16:21,040 --> 00:16:23,560
but we are going to apply it in rounds.

314
00:16:23,560 --> 00:16:26,680
So we'll- we will have this collective inference stretch,

315
00:16:26,680 --> 00:16:28,840
where we are going to keep updating

316
00:16:28,840 --> 00:16:32,980
the predictions based on the updated predictions on the neighbors, right?

317
00:16:32,980 --> 00:16:37,180
So we are going to apply a relational classifier to each node iteratively

318
00:16:37,180 --> 00:16:42,010
and iterate until the inconsistency between neighboring nodes is minimize,

319
00:16:42,010 --> 00:16:45,760
meaning network structure is going to affect the predictions and

320
00:16:45,760 --> 00:16:50,395
these predictions are going to converge and the predictions are going to stabilize.

321
00:16:50,395 --> 00:16:53,140
And usually we will either run this, ah,

322
00:16:53,140 --> 00:16:59,500
iteration until it stabilizes or until some maximum number of iterations, ah, is reached.

323
00:16:59,500 --> 00:17:01,660
And I will give you specific examples,

324
00:17:01,660 --> 00:17:03,700
ah, what I mean by that.

325
00:17:03,700 --> 00:17:11,185
So the problem setting is how do we predict labels Y_v of unlabeled node V?

326
00:17:11,185 --> 00:17:13,089
Here denoted in Grey color.

327
00:17:13,089 --> 00:17:19,659
Each node V will have a feature vector F_v. Labels of some nodes will be given to us.

328
00:17:19,660 --> 00:17:24,849
Ah, you know, we'll use label 1 for green nodes and labeled 0 for red nodes.

329
00:17:24,849 --> 00:17:31,045
Ah, and the task is find the probability that a given node, um, is,

330
00:17:31,045 --> 00:17:35,080
let's say positive is green based on the features it has,

331
00:17:35,080 --> 00:17:38,110
as well as the network structure and the colors,

332
00:17:38,110 --> 00:17:40,060
ah, of the nodes around it.

333
00:17:40,060 --> 00:17:42,775
So that's the problem we are trying to solve.

334
00:17:42,775 --> 00:17:46,555
And we are going to solve this by propagating the beliefs,

335
00:17:46,555 --> 00:17:48,550
the propagating the information, ah,

336
00:17:48,550 --> 00:17:53,605
across the underlying network structure in an iterative way.

337
00:17:53,605 --> 00:17:56,875
So what's the overview of what is coming?

338
00:17:56,875 --> 00:18:00,850
We are going to focus on this notion of semi-supervised node classification.

339
00:18:00,850 --> 00:18:03,070
Semi-supervised in a sense that we are given

340
00:18:03,070 --> 00:18:06,340
both labeled and unlabeled data at the same time,

341
00:18:06,340 --> 00:18:08,590
we are given a partially labeled network.

342
00:18:08,590 --> 00:18:12,639
Ah, we are going to use this notion- this intuition of the notion of homophily,

343
00:18:12,639 --> 00:18:18,100
that similar nodes are typically close together or directly connected in the network.

344
00:18:18,100 --> 00:18:20,454
And we are going to talk about three techniques,

345
00:18:20,454 --> 00:18:22,780
about the relational classification,

346
00:18:22,780 --> 00:18:25,165
iterative classification, and then last,

347
00:18:25,165 --> 00:18:28,729
I'm going to talk about belief propagation.

