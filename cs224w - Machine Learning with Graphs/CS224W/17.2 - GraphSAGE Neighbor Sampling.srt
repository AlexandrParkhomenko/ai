1
00:00:04,130 --> 00:00:07,500
Let's talk about neighborhood sampling.

2
00:00:07,500 --> 00:00:08,970
And this is really,

3
00:00:08,970 --> 00:00:11,730
the key idea of the GraphSAGE,

4
00:00:11,730 --> 00:00:14,219
uh, paper or the GraphSAGE architecture.

5
00:00:14,219 --> 00:00:16,410
So what the GraphSAGE, uh,

6
00:00:16,410 --> 00:00:19,215
what brought to the field of graph neural networks,

7
00:00:19,215 --> 00:00:22,650
is a way to think about mini-batch implementations.

8
00:00:22,650 --> 00:00:25,920
Before all the implementations were full batch and

9
00:00:25,920 --> 00:00:30,030
people would then run- run graph neural networks on you know, 3,

10
00:00:30,030 --> 00:00:34,700
4, 5,000 node graphs because that's what they could fit into GPU memory.

11
00:00:34,700 --> 00:00:36,825
And what GraphSAGE , uh,

12
00:00:36,825 --> 00:00:41,480
changed is they, it changed the way we think of graph neural networks,

13
00:00:41,480 --> 00:00:44,285
the way we can think of creating minibatches,

14
00:00:44,285 --> 00:00:47,740
and this also means that we can scale them up to graphs of,

15
00:00:47,740 --> 00:00:49,850
uh, tens of billions of nodes and edges.

16
00:00:49,850 --> 00:00:53,180
So, uh, let me now tell you about what do I mean by,

17
00:00:53,180 --> 00:00:56,565
um, a- a neighborhood sampling.

18
00:00:56,565 --> 00:00:59,250
So, uh, let's recall,

19
00:00:59,250 --> 00:01:01,270
let- let's remember how, uh,

20
00:01:01,270 --> 00:01:05,600
graph neural networks work and they operate through this notion of a computational graph.

21
00:01:05,600 --> 00:01:09,920
Where GNN generate node embeddings via neighborhood aggregation,

22
00:01:09,920 --> 00:01:13,400
meaning they aggregate features information from the neighbors,

23
00:01:13,400 --> 00:01:16,385
and then they create a new message and pass it on.

24
00:01:16,385 --> 00:01:17,600
So the way you can think of it,

25
00:01:17,600 --> 00:01:20,685
if you say I want to create an embedding for this node, uh,

26
00:01:20,685 --> 00:01:24,080
0, I will take neighbors and neighbors of neighbors.

27
00:01:24,080 --> 00:01:27,230
Here is now my computation graph, um,

28
00:01:27,230 --> 00:01:31,550
where nodes start with their individual feature vectors at the level 0.

29
00:01:31,550 --> 00:01:34,474
Uh, these feature vectors get aggregated,

30
00:01:34,474 --> 00:01:37,760
transformed, passed to the next level, um.

31
00:01:37,760 --> 00:01:39,800
And then again, the same thing gets aggregated

32
00:01:39,800 --> 00:01:42,680
transformed all the way to node- node of interest,

33
00:01:42,680 --> 00:01:44,675
node 0, that makes the prediction.

34
00:01:44,675 --> 00:01:47,330
Notice here that these nodes, you know, er,

35
00:01:47,330 --> 00:01:48,935
the node 0 here,

36
00:01:48,935 --> 00:01:51,815
means this is the feature vector of the node, uh,

37
00:01:51,815 --> 00:01:54,760
0, um, er, for example,

38
00:01:54,760 --> 00:01:57,410
here this means that this will be now a layer 1,

39
00:01:57,410 --> 00:01:59,750
er, representation of nodes,

40
00:01:59,750 --> 00:02:01,430
er, 1, 2, 3.

41
00:02:01,430 --> 00:02:05,840
And here I have now a layer 2 representation of node ze- of node 0.

42
00:02:05,840 --> 00:02:07,760
So what- what do I mean by this is that,

43
00:02:07,760 --> 00:02:11,820
actually nodes have multiple representations,

44
00:02:11,820 --> 00:02:14,370
uh, one for each, uh, layer.

45
00:02:14,370 --> 00:02:17,030
And that's the- that's why when you do full batch,

46
00:02:17,030 --> 00:02:20,480
you want to take level 0 embeddings to create level

47
00:02:20,480 --> 00:02:24,245
1 embeddings of everyone, to create level 2 embeddings from everyone.

48
00:02:24,245 --> 00:02:26,900
But here, because we have this local view,

49
00:02:26,900 --> 00:02:31,370
we will only need to create level 1 embeddings for nodes 1, 2,

50
00:02:31,370 --> 00:02:35,000
and 3, because we need those to create a level 2 embedding,

51
00:02:35,000 --> 00:02:36,995
uh, for node uh, 0.

52
00:02:36,995 --> 00:02:39,560
So that's, uh, the idea, right?

53
00:02:39,560 --> 00:02:45,770
So the important observation is that a two layer GNN generates embedding for this node 0,

54
00:02:45,770 --> 00:02:50,020
using got two-hop neighborhood structure and features around, uh, this graph.

55
00:02:50,020 --> 00:02:52,490
So if I wanted to compute the layer, uh,

56
00:02:52,490 --> 00:02:54,785
the embedding of node 0,

57
00:02:54,785 --> 00:02:57,124
what I need is the graph structure,

58
00:02:57,124 --> 00:03:02,330
plus features of this two-hop neighborhood around the- the node,

59
00:03:02,330 --> 00:03:04,715
and I can ignore the rest of the graph.

60
00:03:04,715 --> 00:03:07,370
I don't need to know the rest of the graph, right?

61
00:03:07,370 --> 00:03:10,790
The embedding of node 0 will be independent of how

62
00:03:10,790 --> 00:03:14,600
the graph structure is beyond the two-hop, uh, neighborhood.

63
00:03:14,600 --> 00:03:16,525
And that's an important insight.

64
00:03:16,525 --> 00:03:21,650
Because this means if I want to have a K layer GNN to emb- to generate

65
00:03:21,650 --> 00:03:26,405
an embedding of a node using these K-hop neighborhood structure and features,

66
00:03:26,405 --> 00:03:28,850
this means I only need to, um,

67
00:03:28,850 --> 00:03:33,050
know at the time and I'm generating that the embedding that K-hop neighborhood.

68
00:03:33,050 --> 00:03:35,560
And I can ignore the rest of the network.

69
00:03:35,560 --> 00:03:38,365
And now, if the level k is, uh,

70
00:03:38,365 --> 00:03:42,185
a relatively small or the neighborhood size is not too big,

71
00:03:42,185 --> 00:03:44,780
which means that it is let say constant size,

72
00:03:44,780 --> 00:03:47,015
but the entire graph can be much, much bigger,

73
00:03:47,015 --> 00:03:49,730
then it means I need relatively little information,

74
00:03:49,730 --> 00:03:54,800
or relatively little memory to use to generate or create the embedding,

75
00:03:54,800 --> 00:03:56,875
uh, of node, uh, 0.

76
00:03:56,875 --> 00:03:59,120
And, uh, this is the main insight.

77
00:03:59,120 --> 00:04:04,490
The main insight is that we wanna- that to compute the embedding of a single node,

78
00:04:04,490 --> 00:04:07,580
all we need is the K-hop neighborhood structure, uh,

79
00:04:07,580 --> 00:04:11,060
around that node, and we can ignore the rest of the network.

80
00:04:11,060 --> 00:04:16,875
The rest of the network does not affect the embedding of this node of interest,

81
00:04:16,875 --> 00:04:21,305
um, all that affects it is the K-hop neighborhood around that node.

82
00:04:21,305 --> 00:04:26,720
So what this means is that now we can generate minibatches in such a way that we say,

83
00:04:26,720 --> 00:04:30,020
let's sample M different nodes in a mini-batch,

84
00:04:30,020 --> 00:04:31,835
but we won't only put,

85
00:04:31,835 --> 00:04:34,370
we won't put nodes into the mini-batch,

86
00:04:34,370 --> 00:04:38,390
but we are going to put entire computation graphs into the mini-batch.

87
00:04:38,390 --> 00:04:40,940
So it means that I would pick a first node and I'll

88
00:04:40,940 --> 00:04:43,660
take its K-hop neighborhood computation graph,

89
00:04:43,660 --> 00:04:46,275
and this is one element in the batch.

90
00:04:46,275 --> 00:04:48,440
And then I'm going to sample the second node,

91
00:04:48,440 --> 00:04:52,055
create its computation graph and put this into my mini-batch.

92
00:04:52,055 --> 00:04:53,690
And so on and so forth.

93
00:04:53,690 --> 00:04:56,540
So now this means perhaps my batches will be smaller

94
00:04:56,540 --> 00:04:59,780
because batches are not now composed of individual nodes,

95
00:04:59,780 --> 00:05:02,870
but batches are composed of network neighborhoods,

96
00:05:02,870 --> 00:05:06,665
batches are composed from computation graphs.

97
00:05:06,665 --> 00:05:10,250
So we are going to sample M computation graphs, um,

98
00:05:10,250 --> 00:05:13,550
and then put these M computation graphs into the GPU memory,

99
00:05:13,550 --> 00:05:18,895
so that we can compute the loss over this mini batch of M computation graphs.

100
00:05:18,895 --> 00:05:21,260
So to emphasize again,

101
00:05:21,260 --> 00:05:23,015
you know, what is the key idea?

102
00:05:23,015 --> 00:05:24,755
The key idea is the following.

103
00:05:24,755 --> 00:05:29,810
It starts with the insight that in order to compute the embedding of a given node,

104
00:05:29,810 --> 00:05:34,300
all that we need to know is the K-hop neighborhood of that node.

105
00:05:34,300 --> 00:05:37,370
So this means if I create a mini

106
00:05:37,370 --> 00:05:41,810
batching not based on the nodes but based on the K-hop neighborhoods,

107
00:05:41,810 --> 00:05:44,390
then I will be able to compute- um,

108
00:05:44,390 --> 00:05:47,320
I will be able to compute the gradient in a reliable way.

109
00:05:47,320 --> 00:05:50,680
So basically, rather than putting nodes into mini-batches,

110
00:05:50,680 --> 00:05:51,920
we are putting, uh,

111
00:05:51,920 --> 00:05:54,695
computational graphs, or in other words,

112
00:05:54,695 --> 00:05:56,540
K-hop neighborhoods into the-,

113
00:05:56,540 --> 00:05:59,455
uh, into the mini-batches.

114
00:05:59,455 --> 00:06:03,540
Now, um, because we have put, um,

115
00:06:03,540 --> 00:06:05,310
uh, we have put, uh,

116
00:06:05,310 --> 00:06:07,550
uh, entire computation graphs,

117
00:06:07,550 --> 00:06:12,275
entire network neighborhoods of K-hops into the, into the mini-batch,

118
00:06:12,275 --> 00:06:14,540
we can, uh, consider the following, uh,

119
00:06:14,540 --> 00:06:18,860
stochastic gradient descent strategy to train the model parameters, right?

120
00:06:18,860 --> 00:06:20,650
We are going to sample,

121
00:06:20,650 --> 00:06:23,540
um, let say M nodes.

122
00:06:23,540 --> 00:06:25,730
For each node, we are going now to sample

123
00:06:25,730 --> 00:06:30,290
the entire K-hop neighborhood to construct the computation graph.

124
00:06:30,290 --> 00:06:31,850
And the we are, uh,

125
00:06:31,850 --> 00:06:34,490
assuming that we have enough memory, um,

126
00:06:34,490 --> 00:06:36,470
that we can fit into the mini-batch

127
00:06:36,470 --> 00:06:39,965
both the nodes as well as their entire computation graphs.

128
00:06:39,965 --> 00:06:43,625
Now, we have the complete set of information we need

129
00:06:43,625 --> 00:06:47,074
to compute an embedding of every- every node in the mini-batch,

130
00:06:47,074 --> 00:06:50,945
so we can then compute the loss over the- this mini-batch,

131
00:06:50,945 --> 00:06:54,110
and we can then perform stochastic gradient descent to

132
00:06:54,110 --> 00:06:57,880
basically update the model parameter with respect to the gradient,

133
00:06:57,880 --> 00:06:59,250
um, er, to that,

134
00:06:59,250 --> 00:07:00,890
uh, mini-batch, uh, loss.

135
00:07:00,890 --> 00:07:03,185
And this is stochastic gradient because,

136
00:07:03,185 --> 00:07:05,810
um, the batches are randomly created, so uh,

137
00:07:05,810 --> 00:07:08,930
the- the gradient will have a bit of,

138
00:07:08,930 --> 00:07:10,955
uh, randomness in it, but that's all fine.

139
00:07:10,955 --> 00:07:12,800
It just means we can do, uh, great,

140
00:07:12,800 --> 00:07:14,735
um, updates very, very fast.

141
00:07:14,735 --> 00:07:17,270
So that's the, that's the idea.

142
00:07:17,270 --> 00:07:20,345
So, um, if we, uh,

143
00:07:20,345 --> 00:07:22,175
do it the way I explained it,

144
00:07:22,175 --> 00:07:25,460
then we have, uh, still an issue with this notion of, uh,

145
00:07:25,460 --> 00:07:29,045
mini-batches and stochastic training because for each node,

146
00:07:29,045 --> 00:07:33,020
we need to get the entire K-hop neighborhood and pass it through

147
00:07:33,020 --> 00:07:37,490
the computation graph and load it into the GPU memory.

148
00:07:37,490 --> 00:07:43,250
So this means we need to aggregate a lot of information just to compute one- one node,

149
00:07:43,250 --> 00:07:45,650
uh, e- e- embedding for a single node.

150
00:07:45,650 --> 00:07:47,974
So computation will be expensive.

151
00:07:47,974 --> 00:07:50,375
So let me tell you why it will be expensive.

152
00:07:50,375 --> 00:07:52,340
First, it will be expensive because,

153
00:07:52,340 --> 00:07:54,710
um, um, uh, the deeper I go,

154
00:07:54,710 --> 00:07:56,390
the bigger these computation graphs and

155
00:07:56,390 --> 00:07:59,150
these computation graphs are going to increase- um,

156
00:07:59,150 --> 00:08:01,860
their size is going to increase exponentially, uh,

157
00:08:01,860 --> 00:08:04,250
with the- with the depth of

158
00:08:04,250 --> 00:08:08,480
the computation graph because even if every node has just three children,

159
00:08:08,480 --> 00:08:10,640
it's going to increase, uh,

160
00:08:10,640 --> 00:08:13,070
exponentially with the number of layers.

161
00:08:13,070 --> 00:08:15,260
So that's one issue.

162
00:08:15,260 --> 00:08:19,370
So the computation graphs are going to get very big if they get

163
00:08:19,370 --> 00:08:23,165
very deep and then the second thing is that in natural graphs,

164
00:08:23,165 --> 00:08:25,565
think of the lecture when we talked about

165
00:08:25,565 --> 00:08:29,149
Microsoft Instant Messenger network when we talked about degree distribution,

166
00:08:29,149 --> 00:08:31,190
we have these celebrity nodes,

167
00:08:31,190 --> 00:08:33,110
these high degree nodes, uh,

168
00:08:33,110 --> 00:08:37,610
that a lot of other people connect to or a lot of other nodes collect- connect to.

169
00:08:37,610 --> 00:08:39,740
We have such nodes even in knowledge graphs.

170
00:08:39,740 --> 00:08:41,255
If you think about, you know,

171
00:08:41,255 --> 00:08:43,309
a node corresponding to a large country,

172
00:08:43,309 --> 00:08:45,560
let's say like, uh, USA, um,

173
00:08:45,560 --> 00:08:47,360
it will have a huge degree because there is

174
00:08:47,360 --> 00:08:49,925
so many other entities related to it and of course,

175
00:08:49,925 --> 00:08:52,265
a small country will have a much smaller degree

176
00:08:52,265 --> 00:08:55,115
because there will be a smaller number of entities related to it.

177
00:08:55,115 --> 00:08:57,440
So the point is, we'll have these hub nodes.

178
00:08:57,440 --> 00:09:00,530
And now if a hub node has degree 1 million, uh,

179
00:09:00,530 --> 00:09:02,135
which is nothing out of ordinary,

180
00:09:02,135 --> 00:09:05,435
then you'll have to aggregate here information from 1 million nodes.

181
00:09:05,435 --> 00:09:09,290
So this computation graph will get huge very quickly and you are

182
00:09:09,290 --> 00:09:13,430
most likely going to hit these hub nodes, uh, very often.

183
00:09:13,430 --> 00:09:14,810
So the point is,

184
00:09:14,810 --> 00:09:20,540
you cannot take the entire K-hop neighborhood in most of the cases. So what do you do?

185
00:09:20,540 --> 00:09:23,555
What do you do, is to do- is to

186
00:09:23,555 --> 00:09:26,810
ap- apply on a project that is called neighborhood sampling.

187
00:09:26,810 --> 00:09:29,120
Where the key idea is to cra- construct

188
00:09:29,120 --> 00:09:33,080
a computational graph by sampling at most H neighbors,

189
00:09:33,080 --> 00:09:34,655
uh, of every node.

190
00:09:34,655 --> 00:09:38,750
So it means that every- every node- every node in the tree- in

191
00:09:38,750 --> 00:09:43,205
the computation graph is going to have at most- is going to aggregate from at most,

192
00:09:43,205 --> 00:09:45,740
uh, H other nodes.

193
00:09:45,740 --> 00:09:48,155
So in our case, just to give you an example,

194
00:09:48,155 --> 00:09:50,480
if I say, let's say H equals 3,

195
00:09:50,480 --> 00:09:54,785
then my original computation graph is now going to be pruned in such a way that

196
00:09:54,785 --> 00:10:00,020
every- every aggregation is going to aggregate from at most two other nodes.

197
00:10:00,020 --> 00:10:04,970
So here, this entire branch is going to be cut out and you know,

198
00:10:04,970 --> 00:10:08,645
some other nodes are going to be cut out as well.

199
00:10:08,645 --> 00:10:11,570
And what this means is that now our, um,

200
00:10:11,570 --> 00:10:16,070
computation graph will be much more manageable because we have just,

201
00:10:16,070 --> 00:10:18,440
um, even if we hit the high degree hub node,

202
00:10:18,440 --> 00:10:19,835
we are only to take,

203
00:10:19,835 --> 00:10:22,460
uh, a fixed number of its neighbors,

204
00:10:22,460 --> 00:10:24,634
uh, in the aggregation.

205
00:10:24,634 --> 00:10:27,500
So the point is that you can use these print

206
00:10:27,500 --> 00:10:31,280
computational graphs to more efficiently compute, uh, node embedding.

207
00:10:31,280 --> 00:10:32,750
Um, so now, uh,

208
00:10:32,750 --> 00:10:36,110
how do you do this computational graph sampling, right?

209
00:10:36,110 --> 00:10:39,155
Basically the idea is for every node, for every layer, uh,

210
00:10:39,155 --> 00:10:41,240
for every internal, um,

211
00:10:41,240 --> 00:10:43,580
node of the- of the computation graph,

212
00:10:43,580 --> 00:10:45,920
uh, we are going to, uh,

213
00:10:45,920 --> 00:10:48,545
first basically compute the K-hop neighborhood, uh,

214
00:10:48,545 --> 00:10:51,200
from the starting node and then for every, uh,

215
00:10:51,200 --> 00:10:52,490
node in the computation graph,

216
00:10:52,490 --> 00:10:55,325
we are going to pick at most, uh,

217
00:10:55,325 --> 00:10:58,580
K, uh, H random, uh, neighbors.

218
00:10:58,580 --> 00:11:03,860
Um, and this means that the K-layer GNN will, uh, involve, uh,

219
00:11:03,860 --> 00:11:06,590
at most, uh, um, uh,

220
00:11:06,590 --> 00:11:08,645
the product of the- of, uh,

221
00:11:08,645 --> 00:11:10,850
H leaf nodes in the computation graph.

222
00:11:10,850 --> 00:11:14,075
So our computation graphs are still gro- gro- going to grow, uh,

223
00:11:14,075 --> 00:11:16,760
exponentially but the, uh,

224
00:11:16,760 --> 00:11:18,785
but the point will be that,

225
00:11:18,785 --> 00:11:20,180
uh, their fan out,

226
00:11:20,180 --> 00:11:21,740
will be- will be, uh,

227
00:11:21,740 --> 00:11:25,490
upper bounded by H. So the- the growth won't be uh,

228
00:11:25,490 --> 00:11:28,280
that bad or that fast and we'll still be able to go,

229
00:11:28,280 --> 00:11:30,560
uh, quite, uh, deep.

230
00:11:30,560 --> 00:11:32,840
Now, let me make a few,

231
00:11:32,840 --> 00:11:34,040
uh, remarks about these.

232
00:11:34,040 --> 00:11:36,830
Uh, first remark is that there is, uh, the trade-off,

233
00:11:36,830 --> 00:11:39,860
uh, in- in how many neighbors do we sample, right?

234
00:11:39,860 --> 00:11:43,880
The smaller, uh, H leads to more efficient neighborhood aggregation

235
00:11:43,880 --> 00:11:48,065
because computation graphs would be smaller but results in more unstable,

236
00:11:48,065 --> 00:11:50,420
uh, training because we are ignoring

237
00:11:50,420 --> 00:11:54,380
entire subparts of the network when we are doing message aggregation.

238
00:11:54,380 --> 00:11:57,110
So our, uh, um, uh,

239
00:11:57,110 --> 00:11:59,120
gradient estimates will be more, uh,

240
00:11:59,120 --> 00:12:01,790
noisy, they will have higher, uh, variance.

241
00:12:01,790 --> 00:12:05,255
Uh, another thing is that in terms of computation time,

242
00:12:05,255 --> 00:12:07,085
even the neighborhood sampling,

243
00:12:07,085 --> 00:12:09,035
the size of the computational graph,

244
00:12:09,035 --> 00:12:13,220
as I said is still exponential with respect to the number of layers but

245
00:12:13,220 --> 00:12:17,705
if H is not that large and we don't go too deep in terms of K,

246
00:12:17,705 --> 00:12:19,230
uh, then it is still man-, uh,

247
00:12:19,230 --> 00:12:22,235
uh, uh, manageable, right?

248
00:12:22,235 --> 00:12:25,910
Um, so, you know, adding one more layer to the, uh,

249
00:12:25,910 --> 00:12:29,390
to the GNN makes the computation H times,

250
00:12:29,390 --> 00:12:30,935
uh, more expensive and now,

251
00:12:30,935 --> 00:12:32,150
you know, if, uh,

252
00:12:32,150 --> 00:12:35,120
H is maybe an order of 5-10,

253
00:12:35,120 --> 00:12:37,820
and K is also, I don't know, on order of, you know,

254
00:12:37,820 --> 00:12:40,325
5 plus minus, then you can still,

255
00:12:40,325 --> 00:12:42,425
uh, keep, uh, doing this.

256
00:12:42,425 --> 00:12:44,855
And then, uh, the last, uh,

257
00:12:44,855 --> 00:12:48,500
the last important thing I want to mention is remark number 3 which is

258
00:12:48,500 --> 00:12:52,820
this- this approach gives you a lot of freedom how you select the nodes,

259
00:12:52,820 --> 00:12:57,680
uh, to sample and so far I don't- I call it a random sampling, right?

260
00:12:57,680 --> 00:13:01,340
Just uniformly at random pick e- at H neighbors,

261
00:13:01,340 --> 00:13:03,350
uh, of a given node.

262
00:13:03,350 --> 00:13:07,175
Uh, but the- the issue is with this approach is that,

263
00:13:07,175 --> 00:13:10,580
uh, real-world networks have these highly skewed degree distributions.

264
00:13:10,580 --> 00:13:16,250
So there is a lot of kind of single- single nodes or low degree nodes in the network.

265
00:13:16,250 --> 00:13:18,935
And if you are taking an, uh,

266
00:13:18,935 --> 00:13:20,090
a given node of interest,

267
00:13:20,090 --> 00:13:21,980
and sample H of its neighbors,

268
00:13:21,980 --> 00:13:25,280
you are most likely going to sample this like degree 1, uh,

269
00:13:25,280 --> 00:13:27,680
nodes that are not perhaps the most important nodes in

270
00:13:27,680 --> 00:13:31,055
the network and perhaps very noisy, not the most informative.

271
00:13:31,055 --> 00:13:33,605
This could be users that are not very engaged,

272
00:13:33,605 --> 00:13:35,540
this could be pieces of content that are

273
00:13:35,540 --> 00:13:38,645
not very important and you don't have a good signal on.

274
00:13:38,645 --> 00:13:40,130
So what you can do,

275
00:13:40,130 --> 00:13:41,840
and this works much better in practice,

276
00:13:41,840 --> 00:13:46,940
is that you'll do a random walk with restart from the node of interest,

277
00:13:46,940 --> 00:13:48,320
here, the green node.

278
00:13:48,320 --> 00:13:51,935
And then your sampling strategy is to take, uh, uh,

279
00:13:51,935 --> 00:13:56,000
H neighbors but not at- let's say not at random,

280
00:13:56,000 --> 00:13:57,365
but based on, uh,

281
00:13:57,365 --> 00:13:59,945
their random walk with restart scores.

282
00:13:59,945 --> 00:14:05,359
So it means that at every layer you are going to take H most important neighbors,

283
00:14:05,359 --> 00:14:06,920
uh, of a given node.

284
00:14:06,920 --> 00:14:10,400
And, uh this means that the graph

285
00:14:10,400 --> 00:14:13,820
you are going to select will be kind of much more representative,

286
00:14:13,820 --> 00:14:15,830
much better connected, um,

287
00:14:15,830 --> 00:14:17,810
and it will have- it will be based on

288
00:14:17,810 --> 00:14:22,279
these more important nodes which have better and more reliable feature information,

289
00:14:22,279 --> 00:14:23,735
these are more active users,

290
00:14:23,735 --> 00:14:26,780
so they kind of provide you more information for predictions.

291
00:14:26,780 --> 00:14:30,980
So in practice, this strategy of sampling the computation graph,

292
00:14:30,980 --> 00:14:33,455
um, works much better,

293
00:14:33,455 --> 00:14:36,020
um, and, you know, there is- I think here to say,

294
00:14:36,020 --> 00:14:39,545
there is room to do a proper investigation about

295
00:14:39,545 --> 00:14:43,910
how would you define what are different strategies to sample,

296
00:14:43,910 --> 00:14:45,590
to define the computation graphs,

297
00:14:45,590 --> 00:14:48,125
to sample the computation graphs and ha-

298
00:14:48,125 --> 00:14:51,050
what are their strategies in which kind of cases?

299
00:14:51,050 --> 00:14:52,700
I think this still hasn't been, uh,

300
00:14:52,700 --> 00:14:56,990
systematically investigated but such a study would be very important,

301
00:14:56,990 --> 00:14:58,460
uh, for the field of,

302
00:14:58,460 --> 00:15:00,860
uh, graph machine learning.

303
00:15:00,860 --> 00:15:06,080
So, uh, to summarize the pro- the neighborhood sampling approach,

304
00:15:06,080 --> 00:15:10,340
the idea is that the computational graph is constructed for each node, um,

305
00:15:10,340 --> 00:15:14,540
and the computational graphs are put into the mini- mini-batch because

306
00:15:14,540 --> 00:15:16,970
computational graphs can become very big very

307
00:15:16,970 --> 00:15:20,180
quickly by hitting a high degree hub node, um,

308
00:15:20,180 --> 00:15:22,910
neighborhoods- we then proposed neighborhood sampling

309
00:15:22,910 --> 00:15:25,730
which is where the computational graph is created

310
00:15:25,730 --> 00:15:31,750
stochastically or is pruned sub-sampled to increase computational efficiency.

311
00:15:31,750 --> 00:15:36,620
It also increases the model robustness because now the GNN architecture is,

312
00:15:36,620 --> 00:15:38,270
uh, stochastic by it's self,

313
00:15:38,270 --> 00:15:42,020
so it's almost like a form of dropout if you want to think of it that way, uh,

314
00:15:42,020 --> 00:15:47,315
and the computa- pruned computational graph is used to generate node embeddings.

315
00:15:47,315 --> 00:15:51,860
Um, here, uh, caveat is that if- if your network,

316
00:15:51,860 --> 00:15:55,040
uh, GNN- number of GNN layers is very deep,

317
00:15:55,040 --> 00:15:58,040
these computation graphs may still become large which means

318
00:15:58,040 --> 00:16:01,010
your batch sizes will have to be smaller, um,

319
00:16:01,010 --> 00:16:03,890
which means, uh, your gradient will be, um,

320
00:16:03,890 --> 00:16:07,445
uh, uh, kind of more, uh, less reliable.

321
00:16:07,445 --> 00:16:10,375
So if the batch size is small,

322
00:16:10,375 --> 00:16:14,065
the, uh, the gradient is less reliable,

323
00:16:14,065 --> 00:16:16,660
and if the pruning is too much,

324
00:16:16,660 --> 00:16:19,880
then again the, uh,

325
00:16:19,880 --> 00:16:22,460
gradient, uh, gradients are not too reliable.

326
00:16:22,460 --> 00:16:25,870
So it's important to find a good balance between the batch size and, uh,

327
00:16:25,870 --> 00:16:30,505
the pruning factor or sampling factor for the computation graphs.

328
00:16:30,505 --> 00:16:33,295
But that's essentially the idea and this is really, I would say,

329
00:16:33,295 --> 00:16:37,850
what most of the large-scale industrial implementations of graph neural networks use,

330
00:16:37,850 --> 00:16:39,335
uh, to achieve, uh,

331
00:16:39,335 --> 00:16:41,660
scaling gap to industrial size graphs.

332
00:16:41,660 --> 00:16:43,480
For example, this is what is used at Pinterest,

333
00:16:43,480 --> 00:16:45,920
at Alibaba and so on.

