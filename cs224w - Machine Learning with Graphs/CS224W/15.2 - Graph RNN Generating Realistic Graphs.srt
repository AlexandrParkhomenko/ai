1
00:00:00,000 --> 00:00:05,060


2
00:00:05,060 --> 00:00:05,720
So.

3
00:00:05,720 --> 00:00:10,340
Now, we are ready to look
about our generative model

4
00:00:10,340 --> 00:00:14,300
for graphs, and this generative
model is called graphRNN.

5
00:00:14,300 --> 00:00:18,170
And, it will allow us to
generate realistic graphs

6
00:00:18,170 --> 00:00:23,420
without making any kind of
inductive bias assumptions.

7
00:00:23,420 --> 00:00:25,670
And the model will
be as we will see--

8
00:00:25,670 --> 00:00:28,760
extremely general and
scalable, and being

9
00:00:28,760 --> 00:00:31,940
able to generate
a vast diversity

10
00:00:31,940 --> 00:00:35,570
of different types of graphs,
of different sizes and shapes,

11
00:00:35,570 --> 00:00:38,370
and structures, and so on.

12
00:00:38,370 --> 00:00:42,290
So the key idea that
this is-- that we

13
00:00:42,290 --> 00:00:44,360
are going to harness
here, is that we

14
00:00:44,360 --> 00:00:47,180
want to generate
graphs via sequential

15
00:00:47,180 --> 00:00:48,800
adding of nodes and edges.

16
00:00:48,800 --> 00:00:51,290
And why do we want
to do it this way?

17
00:00:51,290 --> 00:00:54,020
We want to do it this
way-- exactly what I

18
00:00:54,020 --> 00:00:56,600
explained at the end of
the previous segment.

19
00:00:56,600 --> 00:00:58,190
Which is, that
basically, we want

20
00:00:58,190 --> 00:01:01,340
to model a complex distribution
that captures distribution

21
00:01:01,340 --> 00:01:02,360
over graphs.

22
00:01:02,360 --> 00:01:04,170
Modeling that complex
distribution--

23
00:01:04,170 --> 00:01:05,330
we don't know how to do.

24
00:01:05,330 --> 00:01:07,760
So we are going to break
that complex distribution

25
00:01:07,760 --> 00:01:09,380
into many small parts.

26
00:01:09,380 --> 00:01:11,960
And the way we break
creation of the graph,

27
00:01:11,960 --> 00:01:14,000
we are going to break it
into many small parts,

28
00:01:14,000 --> 00:01:15,590
into many small actions.

29
00:01:15,590 --> 00:01:16,610
And we are simply--

30
00:01:16,610 --> 00:01:20,660
all we'll have to do is model a
model transition probabilities.

31
00:01:20,660 --> 00:01:22,610
Basically saying,
given the graph,

32
00:01:22,610 --> 00:01:26,090
given the actions we have made
so far, conditioning on that,

33
00:01:26,090 --> 00:01:28,760
what is the next
action we want to take?

34
00:01:28,760 --> 00:01:30,990
And this way, by
multiplying these together,

35
00:01:30,990 --> 00:01:34,160
we are able to model the
entire distribution--

36
00:01:34,160 --> 00:01:36,170
all the dependencies in it.

37
00:01:36,170 --> 00:01:40,250
So in our case, the
idea is that, imagine I

38
00:01:40,250 --> 00:01:42,440
have this graph, and
I want to generate it.

39
00:01:42,440 --> 00:01:45,320
The way I'm going to describe
the generation process,

40
00:01:45,320 --> 00:01:47,360
is that, I'll start
with the first node,

41
00:01:47,360 --> 00:01:49,640
and then, I'll add
the second node.

42
00:01:49,640 --> 00:01:52,080
The second node will
link to the first one.

43
00:01:52,080 --> 00:01:54,830
Now I have already built
a bit of the graph,

44
00:01:54,830 --> 00:01:57,440
a third node is added and
links to the first node,

45
00:01:57,440 --> 00:02:00,560
then node number 4 is added
and links to two and three,

46
00:02:00,560 --> 00:02:04,980
and then node number 5 is added
and it links to three and four.

47
00:02:04,980 --> 00:02:06,710
So that's basically,
the generative

48
00:02:06,710 --> 00:02:09,650
process I want to model
and I want to capture.

49
00:02:09,650 --> 00:02:11,960
And in this part
of the lecture, I'm

50
00:02:11,960 --> 00:02:18,440
going to explain to you
the model called graphRNN.

51
00:02:18,440 --> 00:02:21,090
And, here is the
link to the paper,

52
00:02:21,090 --> 00:02:24,290
so if you want more details
you can read the paper

53
00:02:24,290 --> 00:02:27,280
that I'm citing here.

54
00:02:27,280 --> 00:02:31,470
So let's think about this
modeling graphs or generating

55
00:02:31,470 --> 00:02:33,030
graphs as sequences.

56
00:02:33,030 --> 00:02:35,940
A graph with node
ordering pi can

57
00:02:35,940 --> 00:02:39,660
be uniquely mapped into a
sequence of nodes and edge

58
00:02:39,660 --> 00:02:43,110
additions S. What
do I mean by this?

59
00:02:43,110 --> 00:02:46,320
Graph is a set of nodes
and a set of objects,

60
00:02:46,320 --> 00:02:50,140
and apriori graph has
no ordering to it.

61
00:02:50,140 --> 00:02:53,580
So we need to this node
ordering, this permutation pi,

62
00:02:53,580 --> 00:02:57,300
that determines the order
in which the nodes appear.

63
00:02:57,300 --> 00:03:00,540
And given that ordering
in which the nodes appear,

64
00:03:00,540 --> 00:03:04,590
then the sequence to generate
the graph is uniquely defined.

65
00:03:04,590 --> 00:03:07,030
Because, first, I
add the second node.

66
00:03:07,030 --> 00:03:07,530
Sorry.

67
00:03:07,530 --> 00:03:09,863
First, I add the first node,
then I add the second node,

68
00:03:09,863 --> 00:03:10,950
I add the third node.

69
00:03:10,950 --> 00:03:14,370
And then, in terms of adding
edges I can always say I go--

70
00:03:14,370 --> 00:03:15,950
I ask, should the
link the node 1?

71
00:03:15,950 --> 00:03:16,480
Yes, no.

72
00:03:16,480 --> 00:03:17,480
Should I link to node 1?

73
00:03:17,480 --> 00:03:18,120
Yes, no.

74
00:03:18,120 --> 00:03:22,210
Until all the nodes after
already present in the graph.

75
00:03:22,210 --> 00:03:25,860
So that is basically the
key-- the key idea is

76
00:03:25,860 --> 00:03:28,260
that we need some node
ordering, and given

77
00:03:28,260 --> 00:03:30,810
that no node ordering,
then generating the graph

78
00:03:30,810 --> 00:03:34,400
is simply a sequence problem.

79
00:03:34,400 --> 00:03:37,520
Another observation that is
important for graph generation,

80
00:03:37,520 --> 00:03:41,210
is that, this sequence is
actually a two-level sequence.

81
00:03:41,210 --> 00:03:43,370
There is a sequence
of adding nodes,

82
00:03:43,370 --> 00:03:45,740
and there is a sequence
of adding edges.

83
00:03:45,740 --> 00:03:51,890
Right at node level sequence,
at each step we add a new node.

84
00:03:51,890 --> 00:03:55,040
And then, we also
have the second level

85
00:03:55,040 --> 00:03:59,690
after a node is added the edges
need to be added for that given

86
00:03:59,690 --> 00:04:00,290
node.

87
00:04:00,290 --> 00:04:03,590
So this means we s a two-level
sequence where we will first

88
00:04:03,590 --> 00:04:06,420
have the-- we have the high
level node level sequence.

89
00:04:06,420 --> 00:04:09,230
And then, the low level
edge level sequence.

90
00:04:09,230 --> 00:04:11,840
Node level, a step--

91
00:04:11,840 --> 00:04:14,390
we do one node
level step and then

92
00:04:14,390 --> 00:04:15,903
we do a lot of edge level steps.

93
00:04:15,903 --> 00:04:17,320
And then, we do a
node level step,

94
00:04:17,320 --> 00:04:19,190
and we do a lot of
edge level steps.

95
00:04:19,190 --> 00:04:21,860
And each edge level
step is simply

96
00:04:21,860 --> 00:04:23,720
an addition of a new edge.

97
00:04:23,720 --> 00:04:28,280
So you can think of node
level steps being, add node 1,

98
00:04:28,280 --> 00:04:30,770
add node 2, and node 3.

99
00:04:30,770 --> 00:04:36,680
While the sequence elements
in the edge level are,

100
00:04:36,680 --> 00:04:38,690
should I add this node--

101
00:04:38,690 --> 00:04:41,720
this new node that I have added,
should be connect it to node 1?

102
00:04:41,720 --> 00:04:43,310
Should I connect it to node 2?

103
00:04:43,310 --> 00:04:44,790
Should I connect it to node 3?

104
00:04:44,790 --> 00:04:46,730
And you can think of
this as generating

105
00:04:46,730 --> 00:04:51,080
a binary sequence that says,
don't connect, connect,

106
00:04:51,080 --> 00:04:54,810
connect, So that's the idea.

107
00:04:54,810 --> 00:04:57,660
So a summary of what
we have learned so far,

108
00:04:57,660 --> 00:04:59,790
is that, a graph
plus a node ordering

109
00:04:59,790 --> 00:05:02,700
gives us a unique
sequence of sequences

110
00:05:02,700 --> 00:05:04,410
to generate a given graph.

111
00:05:04,410 --> 00:05:07,860
A node ordering for now, let's
assume is randomly selected

112
00:05:07,860 --> 00:05:10,800
or somehow it's given to us,
will come to this question

113
00:05:10,800 --> 00:05:11,782
later.

114
00:05:11,782 --> 00:05:13,740
And the way you can think
of this is right if--

115
00:05:13,740 --> 00:05:16,290
basically is to say, Oh, if I
have a partially built graph

116
00:05:16,290 --> 00:05:19,410
and I want to add a new
node, then adding this node

117
00:05:19,410 --> 00:05:22,560
to the graph simply
means I have to print out

118
00:05:22,560 --> 00:05:25,325
this particular column
of the adjacency matrix.

119
00:05:25,325 --> 00:05:27,510
So I'd basically
saying, node 4 doesn't

120
00:05:27,510 --> 00:05:30,180
link to node 1, node
4 links to node 2,

121
00:05:30,180 --> 00:05:32,370
and node 4 links to node 3.

122
00:05:32,370 --> 00:05:35,010
This is first node,
second node, third node,

123
00:05:35,010 --> 00:05:38,700
and we just generated the
column of the adjacency matrix

124
00:05:38,700 --> 00:05:40,380
for the node number 4.

125
00:05:40,380 --> 00:05:44,520
So node level sequence is
going to add one new column

126
00:05:44,520 --> 00:05:46,110
to the adjacency matrix.

127
00:05:46,110 --> 00:05:49,620
And edge level sequence is
going to kind of print out

128
00:05:49,620 --> 00:05:52,440
the rows-- the entries
of this column, where

129
00:05:52,440 --> 00:05:55,290
0 means the edge
does not exist and 1

130
00:05:55,290 --> 00:05:58,510
means that the edge exists.

131
00:05:58,510 --> 00:05:59,740
So.

132
00:05:59,740 --> 00:06:02,380
So far we have transformed
the graph generation problem

133
00:06:02,380 --> 00:06:04,300
into a sequenced
generation problem.

134
00:06:04,300 --> 00:06:09,310
Now, we need to model the
two processes, the process

135
00:06:09,310 --> 00:06:11,830
of adding new nodes, where
basically, we want to generate

136
00:06:11,830 --> 00:06:13,690
a state for a new node.

137
00:06:13,690 --> 00:06:17,230
And then, we want to generate
edges for the new node

138
00:06:17,230 --> 00:06:18,610
based on it's state.

139
00:06:18,610 --> 00:06:20,860
And will be the
edge level sequence.

140
00:06:20,860 --> 00:06:24,070
So the point will be that, an
approach we are going to use

141
00:06:24,070 --> 00:06:25,480
is-- we are going
to use what are

142
00:06:25,480 --> 00:06:27,700
called recurrent neural
networks to model

143
00:06:27,700 --> 00:06:29,380
this two-level sequence.

144
00:06:29,380 --> 00:06:33,640
And we are going to do use a
nested recurrent neural network

145
00:06:33,640 --> 00:06:35,690
as I'm going to explain.

146
00:06:35,690 --> 00:06:38,110
So the idea is the
following, what

147
00:06:38,110 --> 00:06:40,560
are recurrent neural networks?

148
00:06:40,560 --> 00:06:44,230
Recurrent neural networks are
designed for sequential data.

149
00:06:44,230 --> 00:06:46,270
And a recurrent neural
network sequentially

150
00:06:46,270 --> 00:06:50,480
takes an input sequence to
update its hidden state.

151
00:06:50,480 --> 00:06:52,623
And based on the hidden state--

152
00:06:52,623 --> 00:06:54,040
the idea is that
this hidden state

153
00:06:54,040 --> 00:06:58,390
summarizes all the information
input to the RNN so far.

154
00:06:58,390 --> 00:07:02,050
And then, the
update is conducted

155
00:07:02,050 --> 00:07:04,850
via what is called RNN
cells, for every time

156
00:07:04,850 --> 00:07:06,760
step I have a new RNN cell.

157
00:07:06,760 --> 00:07:08,410
So the way you can
think of it is,

158
00:07:08,410 --> 00:07:11,020
I initialize the RNN
with some hidden state,

159
00:07:11,020 --> 00:07:13,410
I give it the input.

160
00:07:13,410 --> 00:07:17,620
The RNN is going to update its
hidden state and put it here,

161
00:07:17,620 --> 00:07:19,990
and it's going also
to create an output.

162
00:07:19,990 --> 00:07:23,500
And now, the second time
step this hidden state--

163
00:07:23,500 --> 00:07:25,900
next cell is going
to take as the input.

164
00:07:25,900 --> 00:07:29,770
It's going to take as the input
the input from the environment,

165
00:07:29,770 --> 00:07:34,840
it's going to update its hidden
state, here denoted as S2,

166
00:07:34,840 --> 00:07:38,140
and it's going also to
produce me an output.

167
00:07:38,140 --> 00:07:40,390
So basically, the idea
is that this hidden state

168
00:07:40,390 --> 00:07:45,340
s keeps memory of what the
RNN-- what kind of inputs

169
00:07:45,340 --> 00:07:47,890
the RNN has seen so far.

170
00:07:47,890 --> 00:07:52,200
So to tell you more and
give you more mathematics,

171
00:07:52,200 --> 00:07:55,990
S sub t is the state of
the RNN at the step t.

172
00:07:55,990 --> 00:07:59,590
X sub t is the input to
the RNN at that time,

173
00:07:59,590 --> 00:08:03,550
and Y sub t is the output
of the RNN under that time.

174
00:08:03,550 --> 00:08:06,670
And the RNN is-- has
several parameters,

175
00:08:06,670 --> 00:08:09,370
and these parameters
are called W, U, and V.

176
00:08:09,370 --> 00:08:11,350
These are trainable
parameters, this

177
00:08:11,350 --> 00:08:14,950
could be trainable vectors,
trainable matrices.

178
00:08:14,950 --> 00:08:16,690
And the way the
update equations go

179
00:08:16,690 --> 00:08:21,040
is the following, hidden state
gets updated by basically

180
00:08:21,040 --> 00:08:24,430
saying, what is the hidden
state in the previous time step?

181
00:08:24,430 --> 00:08:27,070
Let's transform it.

182
00:08:27,070 --> 00:08:30,700
What is the new input
of the current time?

183
00:08:30,700 --> 00:08:32,370
Let's transform it.

184
00:08:32,370 --> 00:08:34,299
Pass It through
the nonlinearity,

185
00:08:34,299 --> 00:08:36,760
and that is my
updated hidden state.

186
00:08:36,760 --> 00:08:40,840
And then, the output Y is
simply a transformation

187
00:08:40,840 --> 00:08:44,006
of the hidden state
and this is what the--

188
00:08:44,006 --> 00:08:45,820
what we are going to output.

189
00:08:45,820 --> 00:08:50,110
In our case, S, X,
o t will be scalars.

190
00:08:50,110 --> 00:08:52,090
So basically, we
are going to output

191
00:08:52,090 --> 00:08:59,170
a scalar that will tell us
whether there is an edge

192
00:08:59,170 --> 00:09:00,310
or not.

193
00:09:00,310 --> 00:09:02,320
Of course you can have
more expressive cells

194
00:09:02,320 --> 00:09:05,330
like GRU, LSTM, and so on.

195
00:09:05,330 --> 00:09:08,050
But here I am talking
about RNN, which

196
00:09:08,050 --> 00:09:12,810
basically, is the most basic
of this sequence based models.

197
00:09:12,810 --> 00:09:13,710
So.

198
00:09:13,710 --> 00:09:16,560
So far we learned what is RNN.

199
00:09:16,560 --> 00:09:18,420
Now, we want to talk
about how to use

200
00:09:18,420 --> 00:09:22,860
the RNN to model the node level
and the edge level sequence.

201
00:09:22,860 --> 00:09:26,400
And the relationship between
two RNNs is the following,

202
00:09:26,400 --> 00:09:29,670
is that, node level RNN
generates the initial state

203
00:09:29,670 --> 00:09:33,750
for the edge level RNN,
and then, edge level RNN

204
00:09:33,750 --> 00:09:38,260
is going to produce the sequence
of edges for that new node.

205
00:09:38,260 --> 00:09:41,820
And then, it's going to pass
its state back to the node level

206
00:09:41,820 --> 00:09:46,560
RNN, who's going to generate
a new node, update the state,

207
00:09:46,560 --> 00:09:50,050
and push it down to
the edge level RNN.

208
00:09:50,050 --> 00:09:53,710
So this is how this
is going to work.

209
00:09:53,710 --> 00:09:58,580
So to give you an idea, we will
initialize the node level RNN

210
00:09:58,580 --> 00:10:02,750
with a start of a
sequence, and we will--

211
00:10:02,750 --> 00:10:05,840
the node level RNN is
going to create a node.

212
00:10:05,840 --> 00:10:09,050
Now, the edge level RNN--

213
00:10:09,050 --> 00:10:10,850
given this node is
created a node level

214
00:10:10,850 --> 00:10:14,090
RNN will add a new node, and
then, ask the edge level RNN

215
00:10:14,090 --> 00:10:15,470
to generate edges for it.

216
00:10:15,470 --> 00:10:17,660
So node 1 is already there.

217
00:10:17,660 --> 00:10:19,790
Now, node level
RNN decides to add

218
00:10:19,790 --> 00:10:21,470
one more node, node number 2.

219
00:10:21,470 --> 00:10:25,130
Now, the edge level RNN has
to say, does 2 link to 1,

220
00:10:25,130 --> 00:10:31,460
and this number 1 here,
would say that 1 links to 2.

221
00:10:31,460 --> 00:10:32,970
And, here is now the new graph.

222
00:10:32,970 --> 00:10:37,850
Now the node RNN says, OK,
let's add a new node, number 3.

223
00:10:37,850 --> 00:10:40,820
Now, we ask the edge
level RNN to generate

224
00:10:40,820 --> 00:10:43,490
the edges for the node 3.

225
00:10:43,490 --> 00:10:46,760
And node 3 will say, it
links to the first node,

226
00:10:46,760 --> 00:10:48,690
but does not link
to the second node.

227
00:10:48,690 --> 00:10:50,800
So now our new
graph is like this.

228
00:10:50,800 --> 00:10:52,820
And again, the node
level RNN decides,

229
00:10:52,820 --> 00:10:55,370
let's add one more
node, let's add node 4.

230
00:10:55,370 --> 00:10:58,550
And the edge level RNN
prints out-- basically,

231
00:10:58,550 --> 00:11:01,400
a sequence of zeros
and ones that will mean

232
00:11:01,400 --> 00:11:03,350
does falling to node 1?

233
00:11:03,350 --> 00:11:04,520
Does it link to node two?

234
00:11:04,520 --> 00:11:05,810
Or does it link to node 3?

235
00:11:05,810 --> 00:11:07,760
And 4 links to 2 and 3.

236
00:11:07,760 --> 00:11:09,200
So, here is now
the current graph.

237
00:11:09,200 --> 00:11:12,530
And then, node level RNN says,
OK, let's add one more node,

238
00:11:12,530 --> 00:11:13,820
node 5.

239
00:11:13,820 --> 00:11:16,550
Edge level RNN
outputs the-- four

240
00:11:16,550 --> 00:11:19,550
every previous node it tells
us, whether 5 should link to it

241
00:11:19,550 --> 00:11:21,250
or not.

242
00:11:21,250 --> 00:11:23,630
And that's how we get the graph.

243
00:11:23,630 --> 00:11:25,100
And the generation
will stop when

244
00:11:25,100 --> 00:11:27,200
the node level RNN
we'll say, I'm done,

245
00:11:27,200 --> 00:11:29,510
I won't create any new nodes.

246
00:11:29,510 --> 00:11:33,140
So this is the idea, where
edge level RNN sequentially

247
00:11:33,140 --> 00:11:35,180
predicts if a new
node will connect

248
00:11:35,180 --> 00:11:37,790
to each of the previous nodes.

249
00:11:37,790 --> 00:11:41,840
Now, how do we do this with RNN?

250
00:11:41,840 --> 00:11:44,790
So let me give you more ideas.

251
00:11:44,790 --> 00:11:49,490
So in terms of how we use the
RNN to generate the sequence,

252
00:11:49,490 --> 00:11:51,980
we are basically going
to take the output

253
00:11:51,980 --> 00:11:54,320
from-- of the previous
cell, and put it

254
00:11:54,320 --> 00:11:56,360
as an input of the next cell.

255
00:11:56,360 --> 00:11:59,720
So whatever the
previous decision output

256
00:11:59,720 --> 00:12:01,850
was, for example, whatever
was the previous edge--

257
00:12:01,850 --> 00:12:04,850
that would be the input to
the next cell or the next time

258
00:12:04,850 --> 00:12:05,700
step.

259
00:12:05,700 --> 00:12:08,030
How do we initialize
the input sequence?

260
00:12:08,030 --> 00:12:10,310
We initialize it to
have a special token.

261
00:12:10,310 --> 00:12:11,990
We basically train
the neural network

262
00:12:11,990 --> 00:12:14,120
that when it gets
this special token--

263
00:12:14,120 --> 00:12:17,120
we'll call this token
SOS, so start of sequence.

264
00:12:17,120 --> 00:12:21,410
As the initial
input, the network

265
00:12:21,410 --> 00:12:22,730
started generating the output.

266
00:12:22,730 --> 00:12:25,850
Usually, SOS-- when I
say a special token,

267
00:12:25,850 --> 00:12:27,590
it could be a
vector of full zeros

268
00:12:27,590 --> 00:12:29,270
or it could be a
vector full ones.

269
00:12:29,270 --> 00:12:31,670
For example, just
something you reserve.

270
00:12:31,670 --> 00:12:34,310
And then, when do we
stop the generation--

271
00:12:34,310 --> 00:12:39,230
we stop the generation when
the end of sequence token

272
00:12:39,230 --> 00:12:40,370
is produced.

273
00:12:40,370 --> 00:12:44,090
And if end of sequence is
0, then RNN will continue

274
00:12:44,090 --> 00:12:48,980
generation , and if end of
sequence is one the RNN will

275
00:12:48,980 --> 00:12:51,810
stop the generation.

276
00:12:51,810 --> 00:12:53,930
So to give you an idea now.

277
00:12:53,930 --> 00:12:56,510
How do this-- how does
this fit together?

278
00:12:56,510 --> 00:13:00,020
Will somehow initialize the
hidden state, will input

279
00:13:00,020 --> 00:13:02,030
the start of sequence token.

280
00:13:02,030 --> 00:13:04,250
The RNN is going to
generate some output

281
00:13:04,250 --> 00:13:08,480
and then we are going
to connect this output

282
00:13:08,480 --> 00:13:10,070
as the input to the next step.

283
00:13:10,070 --> 00:13:12,560
And of course, the hidden
state will get updated.

284
00:13:12,560 --> 00:13:14,600
And now, given the
hidden state, and given

285
00:13:14,600 --> 00:13:16,760
the output from
the previous state,

286
00:13:16,760 --> 00:13:18,800
this is again going
to be combined here,

287
00:13:18,800 --> 00:13:20,660
another output is
going to be produced,

288
00:13:20,660 --> 00:13:24,920
and another hidden state
is going to be updated.

289
00:13:24,920 --> 00:13:26,553
This is all good.

290
00:13:26,553 --> 00:13:28,970
But the problem with the model
as I've wrote it right now,

291
00:13:28,970 --> 00:13:30,620
is that, it's all deterministic.

292
00:13:30,620 --> 00:13:32,750
It's all a set of
deterministic equations

293
00:13:32,750 --> 00:13:36,920
that are in a deterministic
way generating these edges

294
00:13:36,920 --> 00:13:37,760
and nodes.

295
00:13:37,760 --> 00:13:39,950
So what we want is, we
want the stochastic model.

296
00:13:39,950 --> 00:13:43,300
We want a model that will
have some randomness to it.

297
00:13:43,300 --> 00:13:52,090
So our goal is to use RNN to
do-- to model this p model

298
00:13:52,090 --> 00:13:54,850
and RNN really is--
we are using it

299
00:13:54,850 --> 00:13:59,640
to model this product of
conditional distributions.

300
00:13:59,640 --> 00:14:05,980
So this marginal distribution of
p of Xt given everything else.

301
00:14:05,980 --> 00:14:10,120
And let's write that
Y sub t is the p

302
00:14:10,120 --> 00:14:16,360
model of X sub t
given all the X's that

303
00:14:16,360 --> 00:14:18,400
were previously generated.

304
00:14:18,400 --> 00:14:22,990
Then this means we need to
be able to sample Xt plus 1

305
00:14:22,990 --> 00:14:24,040
from Yt.

306
00:14:24,040 --> 00:14:26,590
So essentially, we
want to be able to get

307
00:14:26,590 --> 00:14:31,630
to sample from this p model,
from this single dimensional

308
00:14:31,630 --> 00:14:32,650
distribution.

309
00:14:32,650 --> 00:14:34,930
This means that
each step of RNN--

310
00:14:34,930 --> 00:14:38,380
Now, it's going to output a
probability of a single edge.

311
00:14:38,380 --> 00:14:41,020
So each step of
the edge level RNN

312
00:14:41,020 --> 00:14:43,540
is going to output the
probability of a single edge.

313
00:14:43,540 --> 00:14:45,400
And then, based on
that probability,

314
00:14:45,400 --> 00:14:48,460
we are going to
flip a coin, if we

315
00:14:48,460 --> 00:14:50,440
are going to sample from
the Bernoulli defined

316
00:14:50,440 --> 00:14:52,070
by that probability.

317
00:14:52,070 --> 00:14:55,030
And then, whether
we get 0 or 1, that

318
00:14:55,030 --> 00:14:57,600
is actually going to be
input to the next step.

319
00:14:57,600 --> 00:15:01,750
So rather than saying the
RNN generates an edge,

320
00:15:01,750 --> 00:15:03,970
RNN generates a
probability, and then, we

321
00:15:03,970 --> 00:15:08,320
have a stochastic event,
a coin flip without bias,

322
00:15:08,320 --> 00:15:11,320
that then lands heads
or tails, zero or one,

323
00:15:11,320 --> 00:15:14,330
and we use that as an
input to the next step.

324
00:15:14,330 --> 00:15:20,030
So that is essentially the idea.

325
00:15:20,030 --> 00:15:22,730
So at the generation
time, at the test time,

326
00:15:22,730 --> 00:15:26,030
let's assume we have
already trained the model.

327
00:15:26,030 --> 00:15:27,860
Then, how this is
going to work, is

328
00:15:27,860 --> 00:15:31,370
that, our Y sub t will be a
probability, will be a scalar,

329
00:15:31,370 --> 00:15:33,890
will be basically a
Bernoulli distribution.

330
00:15:33,890 --> 00:15:40,250
And let's use this square to
mean that this is a probability

331
00:15:40,250 --> 00:15:44,270
and this square takes
value 1 with probability p,

332
00:15:44,270 --> 00:15:47,510
and it takes value 0 with
probability 1 minus p.

333
00:15:47,510 --> 00:15:49,970
So the idea will be
that, we will start--

334
00:15:49,970 --> 00:15:53,690
we'll start our RNN, it
will output a probability.

335
00:15:53,690 --> 00:15:57,290
And now, we are going to
flip a coin without bias

336
00:15:57,290 --> 00:15:59,750
to create-- to decide whether
there is an edge or not.

337
00:15:59,750 --> 00:16:02,030
And whatever is the
output of this coin,

338
00:16:02,030 --> 00:16:05,330
we are going to use this as
an input to the next state--

339
00:16:05,330 --> 00:16:06,290
to the next cell.

340
00:16:06,290 --> 00:16:08,570
And again, we are going to
get another probability,

341
00:16:08,570 --> 00:16:11,930
flip the coin, get
the realization,

342
00:16:11,930 --> 00:16:15,680
and include that as an input
to the RNN cell who's going

343
00:16:15,680 --> 00:16:17,943
to give me another probability.

344
00:16:17,943 --> 00:16:19,610
So this is how we are
going to generate,

345
00:16:19,610 --> 00:16:22,640
or how we are going
to unroll this RNN.

346
00:16:22,640 --> 00:16:26,120
Now, the question is, how
do we use the training data?

347
00:16:26,120 --> 00:16:29,560
How do we use the training
graphs that we are given?

348
00:16:29,560 --> 00:16:31,270
Write this Xs.

349
00:16:31,270 --> 00:16:34,150
And, in order to
train the model,

350
00:16:34,150 --> 00:16:36,760
we are assuming that
we are observing

351
00:16:36,760 --> 00:16:38,510
the graph that was given to us.

352
00:16:38,510 --> 00:16:40,930
So this means we observe
a sequence of edges,

353
00:16:40,930 --> 00:16:42,070
we basically assume--

354
00:16:42,070 --> 00:16:45,320
we observe how the given
graph was generated.

355
00:16:45,320 --> 00:16:47,800
So we observe zeros
and ones whether--

356
00:16:47,800 --> 00:16:50,650
that corresponds to where
the edges exist or edges

357
00:16:50,650 --> 00:16:51,640
don't exist.

358
00:16:51,640 --> 00:16:54,670
And we are going to use this
notion of teacher forcing,

359
00:16:54,670 --> 00:16:57,400
this technique of feature
forcing that place--

360
00:16:57,400 --> 00:17:02,830
that replaces the input and the
output by the real sequence.

361
00:17:02,830 --> 00:17:04,790
So the point will
be the following.

362
00:17:04,790 --> 00:17:08,618
We are going to start
the model, and the model

363
00:17:08,618 --> 00:17:11,559
is going to output some
probability of an edge.

364
00:17:11,560 --> 00:17:14,710
But at the training time, we
are not going now to flip a coin

365
00:17:14,710 --> 00:17:16,810
and use that as an
input to the next cell.

366
00:17:16,810 --> 00:17:20,020
We're actually going to say, OK,
what was the true value there?

367
00:17:20,020 --> 00:17:22,630
Oh, the true value
was, there was no edge.

368
00:17:22,630 --> 00:17:28,640
So we are going to force this
as an input to the next step.

369
00:17:28,640 --> 00:17:31,930
So teacher is kind of,
forcing the student, whatever

370
00:17:31,930 --> 00:17:34,150
the student does
in the next step--

371
00:17:34,150 --> 00:17:36,220
teacher correct the
student and the student

372
00:17:36,220 --> 00:17:38,590
starts from the right--

373
00:17:38,590 --> 00:17:41,160
with the right input
for the next step.

374
00:17:41,160 --> 00:17:43,210
So the point, is
that, the inputs will

375
00:17:43,210 --> 00:17:46,210
be the correct sequence,
not the sequence generated

376
00:17:46,210 --> 00:17:51,110
by the model, and this is
called teacher forcing.

377
00:17:51,110 --> 00:17:54,320
Now, of course we need
to define the laws that

378
00:17:54,320 --> 00:17:57,110
measures the discrepancy
between the output of the model,

379
00:17:57,110 --> 00:17:59,900
the output of the student,
and the ground truth

380
00:17:59,900 --> 00:18:07,100
sequence that we try to
teach the model to generate.

381
00:18:07,100 --> 00:18:12,090
And we are going to use binary
cross entropy, loss function,

382
00:18:12,090 --> 00:18:13,980
which you can write
out the following.

383
00:18:13,980 --> 00:18:16,820
You can-- the star is the--

384
00:18:16,820 --> 00:18:19,430
why is a binary variable 0, 1.

385
00:18:19,430 --> 00:18:23,030
1 means edge exist, 0 doesn't--
means that does not exist.

386
00:18:23,030 --> 00:18:27,590
And Y without the star is
the probability of the edge.

387
00:18:27,590 --> 00:18:31,250
And basically, the idea is
that because Y star is either

388
00:18:31,250 --> 00:18:32,270
0 or 1--

389
00:18:32,270 --> 00:18:35,300
if only one of
these two terms is

390
00:18:35,300 --> 00:18:38,750
going to survive when we
actually implement it,

391
00:18:38,750 --> 00:18:41,480
because when the
Y star comes in.

392
00:18:41,480 --> 00:18:45,080
And basically, the idea is, if Y
star is 1, then we are really--

393
00:18:45,080 --> 00:18:48,860
the loss really boils
down to minus log Y1.

394
00:18:48,860 --> 00:18:51,440
So basically, in order for
us to minimize the loss,

395
00:18:51,440 --> 00:19:00,180
we want to make log of Y1 to
be as close to 0 as possible.

396
00:19:00,180 --> 00:19:04,160
Which means we want to make
Y as close to 1 as possible.

397
00:19:04,160 --> 00:19:08,040
And then, if we want to--
now, on the other hand

398
00:19:08,040 --> 00:19:10,250
if the edge is not
present, then we

399
00:19:10,250 --> 00:19:15,620
want to minimize minus
log of 1 minus Y1.

400
00:19:15,620 --> 00:19:18,350
And here we want
to make Y1 lower so

401
00:19:18,350 --> 00:19:21,290
that the entire expression
again is log of something

402
00:19:21,290 --> 00:19:24,200
close to 1, which
will be close to 0.

403
00:19:24,200 --> 00:19:30,320
So this way, Y1 is fitting
to the data samples Y star.

404
00:19:30,320 --> 00:19:34,040
And again, just to remind you
Y1 or this Y's are computed

405
00:19:34,040 --> 00:19:38,540
by RNN, and the loss is going
to adjust the RNN parameters,

406
00:19:38,540 --> 00:19:42,320
those matrices W, U, and
V, using back propagation

407
00:19:42,320 --> 00:19:43,880
to try to minimize the loss.

408
00:19:43,880 --> 00:19:47,090
Minimize the discrepancy
between the true sequence

409
00:19:47,090 --> 00:19:48,890
and the generated
probabilities .

410
00:19:48,890 --> 00:19:51,080
And basically, the point
is that the loss will say,

411
00:19:51,080 --> 00:19:53,540
wherever there is a 0,
generate a probability

412
00:19:53,540 --> 00:19:56,180
that's close to 0, and
wherever there is a 1,

413
00:19:56,180 --> 00:20:00,670
generate a probability
that is close to 1.

414
00:20:00,670 --> 00:20:01,420
OK.

415
00:20:01,420 --> 00:20:04,390
So we do this, let's
put things together.

416
00:20:04,390 --> 00:20:06,950
So do-- our plan
is the following,

417
00:20:06,950 --> 00:20:09,460
we want to have two RNNs.

418
00:20:09,460 --> 00:20:12,430
First, we want to
have an RNN that

419
00:20:12,430 --> 00:20:16,330
will add one node
at each step, and we

420
00:20:16,330 --> 00:20:22,450
will use the output of it to
initialize the edge level RNN.

421
00:20:22,450 --> 00:20:24,700
And then, the edge
level RNN is going

422
00:20:24,700 --> 00:20:29,450
to predict what other nodes--
what existing nodes does

423
00:20:29,450 --> 00:20:31,720
the new connect to?

424
00:20:31,720 --> 00:20:34,270
And then, we are going
to add another node,

425
00:20:34,270 --> 00:20:37,960
and we will use the last hidden
state of the edge level RNN

426
00:20:37,960 --> 00:20:41,980
to initialize the node
level RNN for one more step.

427
00:20:41,980 --> 00:20:43,450
And then, how is this--

428
00:20:43,450 --> 00:20:46,450
how are we going to
stop the generation--

429
00:20:46,450 --> 00:20:49,930
if the edge level RNN is going
to output the end of sequence

430
00:20:49,930 --> 00:20:53,620
at step 1, we know that no edges
are connected to the new node,

431
00:20:53,620 --> 00:20:55,570
and we are going to
stop the generation.

432
00:20:55,570 --> 00:20:59,540
So it's actually the edge level
RNN, and that, we have decided

433
00:20:59,540 --> 00:21:04,080
will determine whether we stop
generating the graph or not.

434
00:21:04,080 --> 00:21:06,760
So let me give you
now an example.

435
00:21:06,760 --> 00:21:08,920
So that you see how
all this fits together.

436
00:21:08,920 --> 00:21:13,170
So this is what is going to
happen under the training time.

437
00:21:13,170 --> 00:21:15,480
For, let's say, a
given training--

438
00:21:15,480 --> 00:21:17,430
observed training graph.

439
00:21:17,430 --> 00:21:23,100
We are starting with the start
of sequence and a hidden state.

440
00:21:23,100 --> 00:21:26,760
The node level RNN
will add the node,

441
00:21:26,760 --> 00:21:29,610
and then, the edge
level RNN will be asked,

442
00:21:29,610 --> 00:21:32,490
shall this node that
has just been added,

443
00:21:32,490 --> 00:21:34,960
shall it link to
the previous nodes?

444
00:21:34,960 --> 00:21:35,790
Yes or no.

445
00:21:35,790 --> 00:21:37,990
It will update the probability.

446
00:21:37,990 --> 00:21:43,450
And we are then going to flip
a coin that will determine--

447
00:21:43,450 --> 00:21:46,500
with this given bias that will
determine whether the edge is

448
00:21:46,500 --> 00:21:47,080
added or not.

449
00:21:47,080 --> 00:21:51,510
And then, and then, we'll take
this and use it as an input--

450
00:21:51,510 --> 00:21:54,150
as an initialization
back to the node level

451
00:21:54,150 --> 00:21:57,580
RNN who's now going to
add the second node,

452
00:21:57,580 --> 00:21:59,290
and this would be node number 3.

453
00:21:59,290 --> 00:22:01,210
And then, the edge
level RNN is going

454
00:22:01,210 --> 00:22:04,060
to tell us, will node
3 link to node 1,

455
00:22:04,060 --> 00:22:06,340
will node 3 link to node 2.

456
00:22:06,340 --> 00:22:08,350
And again, it's
outputting probabilities,

457
00:22:08,350 --> 00:22:10,300
we are flipping
the coins, whatever

458
00:22:10,300 --> 00:22:13,210
is the output of that coin is
the input to the next level

459
00:22:13,210 --> 00:22:13,840
RNN.

460
00:22:13,840 --> 00:22:16,480
So here are the
probabilities 0.6.

461
00:22:16,480 --> 00:22:18,910
Perhaps you were lucky,
the output was 1,

462
00:22:18,910 --> 00:22:22,850
so this is the input
for the next state.

463
00:22:22,850 --> 00:22:26,800
And then, after
we have traversed

464
00:22:26,800 --> 00:22:29,620
with all the previous
edges, we are going over

465
00:22:29,620 --> 00:22:31,210
all the previous
nodes, we are again

466
00:22:31,210 --> 00:22:34,540
going to ask node RNN
to generate a new node.

467
00:22:34,540 --> 00:22:36,830
And so on and so forth.

468
00:22:36,830 --> 00:22:41,980
And this is going to continue
right until the last node has

469
00:22:41,980 --> 00:22:45,160
been added, then the node
RNN will add one more node,

470
00:22:45,160 --> 00:22:47,320
but the edge level
RNN will say, I'm

471
00:22:47,320 --> 00:22:49,700
not willing to connect
it to anyone else.

472
00:22:49,700 --> 00:22:51,880
So this is the end of
sequence and we stop.

473
00:22:51,880 --> 00:22:54,890
So basically, it's like,
when we add an isolated node,

474
00:22:54,890 --> 00:22:58,780
we know that's the signal
that we want to stop.

475
00:22:58,780 --> 00:23:02,140
So that's the idea.

476
00:23:02,140 --> 00:23:05,260
And then, for each
prediction, here we

477
00:23:05,260 --> 00:23:07,420
are going to get
supervision from the ground,

478
00:23:07,420 --> 00:23:09,070
from the ground truth.

479
00:23:09,070 --> 00:23:11,560
So the point is that, we will
be doing teacher forcing.

480
00:23:11,560 --> 00:23:14,420
So, even for example,
here, when this was 0.6

481
00:23:14,420 --> 00:23:16,420
and we did a coin flip
and maybe we were unlucky

482
00:23:16,420 --> 00:23:22,780
and we got a zero, we are going
to output the true edge as we

483
00:23:22,780 --> 00:23:24,070
said, the teacher--

484
00:23:24,070 --> 00:23:25,360
the teacher forcing.

485
00:23:25,360 --> 00:23:27,520
And then, the structure
of our neural network

486
00:23:27,520 --> 00:23:29,920
when we do back propagation,
will be basically doing back

487
00:23:29,920 --> 00:23:32,500
propagation through time.

488
00:23:32,500 --> 00:23:36,400
We are going to basically back
prop from all these events all

489
00:23:36,400 --> 00:23:40,150
the way to the beginning--
to the beginning of time,

490
00:23:40,150 --> 00:23:44,450
to the firr-- to the first
node in the graph to update

491
00:23:44,450 --> 00:23:48,250
the parameters of the RNN.

492
00:23:48,250 --> 00:23:51,850
And then, how about at the test
time, at the generation time?

493
00:23:51,850 --> 00:23:54,640
We are basically going to
sample the connectivity based

494
00:23:54,640 --> 00:23:57,760
on the predicted distributions,
predicted probabilities,

495
00:23:57,760 --> 00:24:00,310
and we are going to replace
the input at each step

496
00:24:00,310 --> 00:24:04,090
by the RNNs own
prediction or own output.

497
00:24:04,090 --> 00:24:08,050
So here we are going
to flip the coin,

498
00:24:08,050 --> 00:24:11,203
the coin will say what it will
say, and it will go as an input

499
00:24:11,203 --> 00:24:11,870
to the next one.

500
00:24:11,870 --> 00:24:14,800
And I think, here, we are
going to do the coin, whatever

501
00:24:14,800 --> 00:24:18,030
is the output here, should be
the input to the next step,

502
00:24:18,030 --> 00:24:20,210
so it should be a 0
here, it's a mistake.

503
00:24:20,210 --> 00:24:20,710
OK.

504
00:24:20,710 --> 00:24:22,850
So that's the idea.

505
00:24:22,850 --> 00:24:27,640
So the summary is we have costed
the problem of graph generation

506
00:24:27,640 --> 00:24:29,380
as a problem of
sequence generation.

507
00:24:29,380 --> 00:24:32,710
Actually, a problem of a
two-level sequence generation,

508
00:24:32,710 --> 00:24:35,710
node level sequence, and
an edge level sequence,

509
00:24:35,710 --> 00:24:37,780
and we use recurrent
neural networks

510
00:24:37,780 --> 00:24:39,970
to generate the sequences.

511
00:24:39,970 --> 00:24:42,820
And what I want to
discuss next is, how do we

512
00:24:42,820 --> 00:24:45,040
make the RNN tractable?

513
00:24:45,040 --> 00:24:47,670
And how do we evaluate?

514
00:24:47,670 --> 00:24:52,000


