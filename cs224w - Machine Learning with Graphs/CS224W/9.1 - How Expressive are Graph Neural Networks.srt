1
00:00:03,980 --> 00:00:12,735
So now we are shifting gears and we are talking about theory of graph neural networks.

2
00:00:12,735 --> 00:00:18,930
And we are in particular going to ask ourselves how expressive are graph neural networks,

3
00:00:18,930 --> 00:00:21,795
what are graph neural networks able to learn,

4
00:00:21,795 --> 00:00:24,900
and what are they not uh, able to learn?

5
00:00:24,900 --> 00:00:27,870
This is really the question for uh,

6
00:00:27,870 --> 00:00:29,490
this part of the lecture.

7
00:00:29,490 --> 00:00:32,685
So we talk about deep uh,

8
00:00:32,685 --> 00:00:35,055
neural networks applied to graphs,

9
00:00:35,055 --> 00:00:37,130
where through several um,

10
00:00:37,130 --> 00:00:39,420
layers of non-linear propagation,

11
00:00:39,420 --> 00:00:43,980
we are trying to come up with the embeddings of nodes, embeddings of networks.

12
00:00:43,980 --> 00:00:48,865
So we can do various kinds of machine learning prediction tasks.

13
00:00:48,865 --> 00:00:55,505
The key idea that graph neural networks have is this idea of aggregating

14
00:00:55,505 --> 00:00:59,600
local neighborhoods around a given node

15
00:00:59,600 --> 00:01:02,850
of interest to generate the embedding for that node.

16
00:01:02,850 --> 00:01:06,095
Right, So this is kind of the classical picture we've been showing

17
00:01:06,095 --> 00:01:10,985
several times in our discussions so far.

18
00:01:10,985 --> 00:01:13,220
Right, so the intuition is that nodes

19
00:01:13,220 --> 00:01:17,750
aggregate information from their neighbors using neural networks.

20
00:01:17,750 --> 00:01:24,200
And so far we discussed what kind of design choices you have when deciding how to,

21
00:01:24,200 --> 00:01:27,695
how to operationalize or how to design

22
00:01:27,695 --> 00:01:32,785
this message transformation and aggregation operations.

23
00:01:32,785 --> 00:01:38,465
So today, right now we are going to talk about the theory of graph neural networks.

24
00:01:38,465 --> 00:01:43,150
And in particular, we are going to ask how powerful are graph neural networks.

25
00:01:43,150 --> 00:01:44,930
How expressive are they?

26
00:01:44,930 --> 00:01:48,425
What can they learn and what can they not learn?

27
00:01:48,425 --> 00:01:53,770
This is especially important because there are many different and GNN models.

28
00:01:53,770 --> 00:01:57,350
Right, we talked about the GCN, graph convolution neural network.

29
00:01:57,350 --> 00:01:59,795
We talked about that, the graph attention network.

30
00:01:59,795 --> 00:02:03,110
We talked about the GraphSAGE network and we talked about

31
00:02:03,110 --> 00:02:07,195
the entire design space of these types of models.

32
00:02:07,195 --> 00:02:11,044
So the question is, what is their expressive power?

33
00:02:11,044 --> 00:02:15,470
Which basically means what is their ability to distinguish different nodes,

34
00:02:15,470 --> 00:02:18,170
different graph structures, and how

35
00:02:18,170 --> 00:02:21,685
expressive are they in learning different types of patterns?

36
00:02:21,685 --> 00:02:25,670
And then what would be super cool today is that we will be actually able

37
00:02:25,670 --> 00:02:29,650
to design the maximally expressive GNN models.

38
00:02:29,650 --> 00:02:31,700
So in some sense, we'll be able to design

39
00:02:31,700 --> 00:02:37,935
the most powerful graph neural network there is which is super cool.

40
00:02:37,935 --> 00:02:40,680
So that's the- that's the plan.

41
00:02:40,680 --> 00:02:45,215
So background is, we have many graph neural network models,

42
00:02:45,215 --> 00:02:47,195
they all have different,

43
00:02:47,195 --> 00:02:50,900
they all differ in terms of how they propagate,

44
00:02:50,900 --> 00:02:54,035
aggregate, and transform messages.

45
00:02:54,035 --> 00:02:55,925
And the question is,

46
00:02:55,925 --> 00:03:00,995
can we understand what is their expressive power and how these different design choices

47
00:03:00,995 --> 00:03:06,835
actually lead to different type of models.

48
00:03:06,835 --> 00:03:09,560
So for example, in a graph,

49
00:03:09,560 --> 00:03:12,200
convolutional neural networks, GCN.

50
00:03:12,200 --> 00:03:14,405
It's using what is called mean pooling.

51
00:03:14,405 --> 00:03:15,635
Right, so basically when,

52
00:03:15,635 --> 00:03:19,490
when we aggregate information from neighbors,

53
00:03:19,490 --> 00:03:22,715
we use  element-wise mean pooling.

54
00:03:22,715 --> 00:03:28,835
And then we use a linear transformation plus a ReLU, nonlinearity.

55
00:03:28,835 --> 00:03:31,225
That's for example, what a GCN is.

56
00:03:31,225 --> 00:03:38,900
For example, GraphSAGE uses a multi-layer perceptron plus element-wise,

57
00:03:38,900 --> 00:03:42,410
let say maximum pooling which is- which is different.

58
00:03:42,410 --> 00:03:44,430
And the question is, what is better?

59
00:03:44,430 --> 00:03:46,890
Is max better than, than average?

60
00:03:46,890 --> 00:03:50,810
Or what's the difference between the two in terms of,

61
00:03:50,810 --> 00:03:54,790
let's say, theoretical properties and expressive power?

62
00:03:54,790 --> 00:04:02,185
Um, there is an important note I wanna- I wanna make so that we don't get confused later.

63
00:04:02,185 --> 00:04:03,940
In graph neural networks,

64
00:04:03,940 --> 00:04:05,229
we have two aspects.

65
00:04:05,229 --> 00:04:07,420
We have the aspect of node features,

66
00:04:07,420 --> 00:04:12,175
node properties, and we have the aspect of a graph structure.

67
00:04:12,175 --> 00:04:14,575
And for the purpose of this lecture,

68
00:04:14,575 --> 00:04:19,880
I'm going to use colors of nodes to represent their feature vectors.

69
00:04:19,880 --> 00:04:22,765
What I mean by this, if two nodes are of the same color,

70
00:04:22,765 --> 00:04:25,615
then they have the same feature vector,

71
00:04:25,615 --> 00:04:27,895
they have the same feature representation.

72
00:04:27,895 --> 00:04:29,200
Right? So for example,

73
00:04:29,200 --> 00:04:30,925
in this graph here,

74
00:04:30,925 --> 00:04:33,580
numbers represent node IDs.

75
00:04:33,580 --> 00:04:35,800
So I can say I'm talking about node 1,

76
00:04:35,800 --> 00:04:37,660
I'm talking about node 2,

77
00:04:37,660 --> 00:04:41,300
but the features these nodes have are all the same.

78
00:04:41,300 --> 00:04:44,660
So there is no featured information that would

79
00:04:44,660 --> 00:04:48,140
allow me to distinguish nodes from one another, right?

80
00:04:48,140 --> 00:04:54,185
So we don't, the color means what is the feature vector of the node.

81
00:04:54,185 --> 00:04:56,555
So now for example, the question would be,

82
00:04:56,555 --> 00:05:01,160
how well can a GNN distinguish different graphical structures, right?

83
00:05:01,160 --> 00:05:02,585
Because if every node has

84
00:05:02,585 --> 00:05:06,530
its own unique feature vector then it's easy to distinguish the nodes.

85
00:05:06,530 --> 00:05:08,560
You just look at their feature vectors.

86
00:05:08,560 --> 00:05:12,035
But if all the feature vectors are the same, like in this case,

87
00:05:12,035 --> 00:05:13,220
all nodes are yellow,

88
00:05:13,220 --> 00:05:15,850
then the question is, can you still distinguish the nodes?

89
00:05:15,850 --> 00:05:20,200
Can you learn that node 5 is different than node 4,

90
00:05:20,200 --> 00:05:22,460
for example, in this case.

91
00:05:22,460 --> 00:05:26,225
So and in graph neural networks,

92
00:05:26,225 --> 00:05:31,340
we are particularly interested in this notion of local neighborhood structures.

93
00:05:31,340 --> 00:05:34,400
Where basically we are interested in quantifying

94
00:05:34,400 --> 00:05:39,355
the local network neighborhood around each node in the graph.

95
00:05:39,355 --> 00:05:41,360
So for example here,

96
00:05:41,360 --> 00:05:44,180
let's say I'm interested in nodes 1 and 5 and I

97
00:05:44,180 --> 00:05:47,980
say could I learn to distinguish nodes 1 and 5?

98
00:05:47,980 --> 00:05:50,735
Distinguishing them would be quite easy

99
00:05:50,735 --> 00:05:53,915
because they have different neighborhood structures,

100
00:05:53,915 --> 00:05:58,310
even if you look at the number of edges that is adjacent to each of them,

101
00:05:58,310 --> 00:06:02,605
you know, node 1 has degree 2 and node 5 has degree 3.

102
00:06:02,605 --> 00:06:05,885
So it'll be very easy to distinguish them.

103
00:06:05,885 --> 00:06:10,115
If you can capture the degree of the node in a graph neural network,

104
00:06:10,115 --> 00:06:14,595
then you can differentiate between nodes 1 and 5.

105
00:06:14,595 --> 00:06:17,705
Let's look now at the second example.

106
00:06:17,705 --> 00:06:20,945
How about nodes 1 and 4?

107
00:06:20,945 --> 00:06:22,660
Are they distinguishable?

108
00:06:22,660 --> 00:06:29,390
Right? If you look at it from the single layer,

109
00:06:29,390 --> 00:06:32,950
single hop neighborhood, then node 1 has degree 2.

110
00:06:32,950 --> 00:06:34,755
And node 4 has degree 2.

111
00:06:34,755 --> 00:06:38,450
So if I only am able to capture the degree of the node itself,

112
00:06:38,450 --> 00:06:41,540
I cannot differentiate between 1 and 2, right?

113
00:06:41,540 --> 00:06:44,695
They have the same feature vector and they have the same degree.

114
00:06:44,695 --> 00:06:49,040
However, 1 and 2 are still different because if I look at,

115
00:06:49,040 --> 00:06:51,740
let's say the second-degree neighborhood.

116
00:06:51,740 --> 00:06:55,970
Right? You could say, ah-ha,  node 1 has two neighbors.

117
00:06:55,970 --> 00:06:59,230
One has degree 2 and 1 has degree 3.

118
00:06:59,230 --> 00:07:02,300
While node 4 also has two neighbors,

119
00:07:02,300 --> 00:07:05,660
but one has degree 1 and the other one has degree 3.

120
00:07:05,660 --> 00:07:08,090
So if I'm able to capture the degree of the node

121
00:07:08,090 --> 00:07:12,379
4 of the node itself plus the degrees of the neighbors,

122
00:07:12,379 --> 00:07:18,350
then 1 and 4 are distinguishable because their neighbors have different degrees.

123
00:07:18,350 --> 00:07:23,690
Right? So now you see how maybe immediately two- two nodes look the same.

124
00:07:23,690 --> 00:07:25,925
But if you go deeper into the network here,

125
00:07:25,925 --> 00:07:27,275
I go to the neighbors,

126
00:07:27,275 --> 00:07:31,185
then the two nodes become distinguishable.

127
00:07:31,185 --> 00:07:33,810
And that is very interesting.

128
00:07:33,810 --> 00:07:38,990
So now let's continue this investigation and look at another pair of nodes.

129
00:07:38,990 --> 00:07:42,120
Let's look at nodes 1 and 2.

130
00:07:42,120 --> 00:07:45,985
What is interesting is that 1 and 2- actually, um,

131
00:07:45,985 --> 00:07:47,305
in this graph neural,

132
00:07:47,305 --> 00:07:50,575
in this network are indistinguishable from one another.

133
00:07:50,575 --> 00:07:53,920
Because they are kind of symmetric in the graph, right?

134
00:07:53,920 --> 00:07:56,545
They both have degree 2, um, uh,

135
00:07:56,545 --> 00:08:00,055
their neighbor, um, they both have two neighbors.

136
00:08:00,055 --> 00:08:03,550
Uh, one of degree- of degree 2 and one of degree 3.

137
00:08:03,550 --> 00:08:06,099
Um, if you go to the second hop neighborhood,

138
00:08:06,099 --> 00:08:08,080
it's, uh, the node number 4,

139
00:08:08,080 --> 00:08:09,520
uh, that has degree 2.

140
00:08:09,520 --> 00:08:11,875
So basically their- their, uh,

141
00:08:11,875 --> 00:08:14,965
network neighborhood is identical

142
00:08:14,965 --> 00:08:18,805
regardless how deep or how far do we explore the network.

143
00:08:18,805 --> 00:08:20,800
Because in- in both cases,

144
00:08:20,800 --> 00:08:22,075
you know, they have,

145
00:08:22,075 --> 00:08:25,210
each- each of them has one node of degree 2,

146
00:08:25,210 --> 00:08:26,740
one node of degree 3.

147
00:08:26,740 --> 00:08:28,465
At two-hop neighborhood, um,

148
00:08:28,465 --> 00:08:31,720
they both have one neighbor of degree 2.

149
00:08:31,720 --> 00:08:34,960
Three hops away they both have one neighbor of degree 1.

150
00:08:34,960 --> 00:08:39,370
So you cannot distinguish one and two unless somebody gives you

151
00:08:39,370 --> 00:08:43,750
some feature information that would allow you to s- tell 1 from 2.

152
00:08:43,750 --> 00:08:45,325
But based on the graph structure,

153
00:08:45,325 --> 00:08:48,205
you cannot distinguish them because they're kind of symmetric.

154
00:08:48,205 --> 00:08:50,515
Their- their positions are isomorphic,

155
00:08:50,515 --> 00:08:51,820
uh, in the graph, right?

156
00:08:51,820 --> 00:08:54,234
So that's an exa- an- an example,

157
00:08:54,234 --> 00:08:56,920
kind of trying to build up intuition how this,

158
00:08:56,920 --> 00:08:59,380
uh, will all, uh, work out.

159
00:08:59,380 --> 00:09:02,580
So the key question we wanna,

160
00:09:02,580 --> 00:09:04,350
uh, look at is,

161
00:09:04,350 --> 00:09:10,785
can a GNN node embedding distinguish different local neighborhood structures, right?

162
00:09:10,785 --> 00:09:14,555
Local meaning neighborhoods structures around a given node.

163
00:09:14,555 --> 00:09:18,100
And if it can, the question is when and if not,

164
00:09:18,100 --> 00:09:21,520
what are the failure cases of graph neural networks?

165
00:09:21,520 --> 00:09:24,895
So- so what we'll do next is we need to understand

166
00:09:24,895 --> 00:09:28,900
how a GNN captures local neighborhood structures.

167
00:09:28,900 --> 00:09:33,640
And we are going to understand this through this key concept of a computational graph.

168
00:09:33,640 --> 00:09:35,935
So let me now talk about, uh,

169
00:09:35,935 --> 00:09:39,220
what is a computational graph, right?

170
00:09:39,220 --> 00:09:41,665
The way you think of this is that each, uh,

171
00:09:41,665 --> 00:09:44,290
layer, uh, a GNN aggregates,

172
00:09:44,290 --> 00:09:46,240
uh, neighborho- neighboring embeddings.

173
00:09:46,240 --> 00:09:48,490
So in a GNN, uh,

174
00:09:48,490 --> 00:09:49,780
we generate an embedding through

175
00:09:49,780 --> 00:09:53,890
a computational graph defined on the node neighborhood structure.

176
00:09:53,890 --> 00:09:56,635
So for example, if I say here is node 1,

177
00:09:56,635 --> 00:09:59,080
the computational graph- let's say if I do

178
00:09:59,080 --> 00:10:03,085
a two-layer GNN for node 1 is created here, right?

179
00:10:03,085 --> 00:10:07,930
Node 1 aggregates information from nodes 2 and 5. Here they are.

180
00:10:07,930 --> 00:10:12,325
Node 5- node 5 aggregates information from its neighbors,

181
00:10:12,325 --> 00:10:15,325
5 has neighbors 1, 2, and 4.

182
00:10:15,325 --> 00:10:17,350
And node 2 here,

183
00:10:17,350 --> 00:10:19,450
aggregates information from its neighbors,

184
00:10:19,450 --> 00:10:20,980
node 1 and node 5.

185
00:10:20,980 --> 00:10:23,665
So this is what we call a computation graph.

186
00:10:23,665 --> 00:10:28,000
It simply shows us how the messages gets- get aggregated from level,

187
00:10:28,000 --> 00:10:31,195
uh, level 0 to level 1 to level 2.

188
00:10:31,195 --> 00:10:34,120
And this is now the computation graph that describes

189
00:10:34,120 --> 00:10:37,240
the two-layer graph neural network for,

190
00:10:37,240 --> 00:10:40,165
uh, node, uh, node ID, uh, 1.

191
00:10:40,165 --> 00:10:42,040
That's the idea here.

192
00:10:42,040 --> 00:10:46,180
And what is interesting is that now if I take, for example, uh,

193
00:10:46,180 --> 00:10:48,070
node I- node, uh,

194
00:10:48,070 --> 00:10:51,535
number 2 and I create a computation graph for itself,

195
00:10:51,535 --> 00:10:52,930
uh, here it is, right?

196
00:10:52,930 --> 00:10:56,665
Two aggregates from nodes 1 and 5, uh,

197
00:10:56,665 --> 00:10:58,990
5 again aggregates from,

198
00:10:58,990 --> 00:11:00,805
uh, uh, 1, 2 and 4.

199
00:11:00,805 --> 00:11:04,810
And, uh, node number 1 aggregates from 2 and 5, right?

200
00:11:04,810 --> 00:11:11,770
What you notice is that computational graphs for nodes 1 and 2 are actually identical.

201
00:11:11,770 --> 00:11:13,600
They both have, uh,

202
00:11:13,600 --> 00:11:15,190
two children at- uh,

203
00:11:15,190 --> 00:11:17,110
at level 1 and they have,

204
00:11:17,110 --> 00:11:18,400
you know, one- one has 2,

205
00:11:18,400 --> 00:11:19,510
and one has fi- uh,

206
00:11:19,510 --> 00:11:20,890
one has 3, uh,

207
00:11:20,890 --> 00:11:22,990
at level, uh, 0.

208
00:11:22,990 --> 00:11:25,465
So what this means is,

209
00:11:25,465 --> 00:11:29,800
because a GNN is only doing message-passing information,

210
00:11:29,800 --> 00:11:32,290
uh, without any node IDs,

211
00:11:32,290 --> 00:11:34,720
it only uses node feature vectors.

212
00:11:34,720 --> 00:11:36,190
This means that, you know,

213
00:11:36,190 --> 00:11:37,900
if you look at these propagation,

214
00:11:37,900 --> 00:11:39,700
uh, trees, these computation graphs,

215
00:11:39,700 --> 00:11:41,740
right now they are different because you say, oh,

216
00:11:41,740 --> 00:11:44,845
obviously here is node number 1 and here is node number 2.

217
00:11:44,845 --> 00:11:47,125
So obviously these trees are different.

218
00:11:47,125 --> 00:11:51,715
But if you only look at the colors- if you only look at the node feature information,

219
00:11:51,715 --> 00:11:53,770
then this is how these trees look like.

220
00:11:53,770 --> 00:11:59,635
They look identical and there is no way to tell nodes apart from each other.

221
00:11:59,635 --> 00:12:01,210
So in all cases,

222
00:12:01,210 --> 00:12:02,680
all the graph neural network can do,

223
00:12:02,680 --> 00:12:04,315
can aggregate, you know,

224
00:12:04,315 --> 00:12:05,710
the information from these nodes.

225
00:12:05,710 --> 00:12:09,880
They all have yellow color and here it can aggregate yellow color.

226
00:12:09,880 --> 00:12:11,605
So all it can do it is to say,

227
00:12:11,605 --> 00:12:13,600
oh, I have three yellow children.

228
00:12:13,600 --> 00:12:15,895
This guy can say I have two yellow children.

229
00:12:15,895 --> 00:12:18,625
And then this- here we can say, uh-huh you know,

230
00:12:18,625 --> 00:12:20,980
I have two children an- and one of them has two and

231
00:12:20,980 --> 00:12:23,440
the other one has three, uh, further children.

232
00:12:23,440 --> 00:12:26,425
And that's how we can describe this computation graph.

233
00:12:26,425 --> 00:12:28,705
But the point is that for two different nodes,

234
00:12:28,705 --> 00:12:31,435
1 and 2, the computation graphs are the same.

235
00:12:31,435 --> 00:12:34,195
So without any feature information,

236
00:12:34,195 --> 00:12:36,930
without any node attribute information, uh,

237
00:12:36,930 --> 00:12:38,430
these two- these two nodes,

238
00:12:38,430 --> 00:12:40,440
these two computation graphs are the same.

239
00:12:40,440 --> 00:12:45,435
So these two nodes will be embedded into the same point in the embedding space.

240
00:12:45,435 --> 00:12:50,645
And what this means is that they will overlap so the graph neural network won't be able,

241
00:12:50,645 --> 00:12:52,090
uh, to distinguish them,

242
00:12:52,090 --> 00:12:56,785
uh, and won't be able to classify node 1 into a different class than node 2,

243
00:12:56,785 --> 00:12:59,320
because their embeddings will be exactly the same.

244
00:12:59,320 --> 00:13:04,420
They will overlap because the- the computation graphs are the same,

245
00:13:04,420 --> 00:13:06,310
and there is no distinguishable, uh,

246
00:13:06,310 --> 00:13:11,125
node feature information because that's kind of our assumption, uh, going in.

247
00:13:11,125 --> 00:13:14,425
So if there is an important slide of this lecture,

248
00:13:14,425 --> 00:13:16,090
this is the most important slide,

249
00:13:16,090 --> 00:13:18,670
is that basically we- GNNs capture

250
00:13:18,670 --> 00:13:21,850
the local neighborhood structure through the computation graph.

251
00:13:21,850 --> 00:13:25,135
And if computation graph of two nodes are the same,

252
00:13:25,135 --> 00:13:29,905
then the two nodes will be embedded exactly into the same point in the embedding space,

253
00:13:29,905 --> 00:13:34,570
which means that we are not able to classify one into one class,

254
00:13:34,570 --> 00:13:37,900
and the other one into the other because they are- they are identical,

255
00:13:37,900 --> 00:13:42,250
they are overlapping, so we cannot distinguish, uh, between them.

256
00:13:42,250 --> 00:13:45,655
So this means just kind of to summarize,

257
00:13:45,655 --> 00:13:47,605
is that in this simple example,

258
00:13:47,605 --> 00:13:53,545
a GNN will generate the same embedding for nodes 1 and 2 because of two facts.

259
00:13:53,545 --> 00:13:58,150
First is that because the computational graphs are the same, they are identical.

260
00:13:58,150 --> 00:14:00,895
And the second important part is that

261
00:14:00,895 --> 00:14:04,600
node feature information in this case is identical, right?

262
00:14:04,600 --> 00:14:07,360
All- all nodes the assumption of this lecture

263
00:14:07,360 --> 00:14:10,390
is that node features are not useful in this case,

264
00:14:10,390 --> 00:14:12,325
so all nodes have the same feature.

265
00:14:12,325 --> 00:14:14,200
They are all yellow, right?

266
00:14:14,200 --> 00:14:18,460
And because GNN does not care about node IDs,

267
00:14:18,460 --> 00:14:20,530
it cares about the attributes,

268
00:14:20,530 --> 00:14:22,750
features of the nodes and aggregates them.

269
00:14:22,750 --> 00:14:27,970
This means that this GNN is not able to distinguish nodes 1 and 2.

270
00:14:27,970 --> 00:14:29,920
So 1 and 2 will always have

271
00:14:29,920 --> 00:14:33,925
exactly the same embedding so they will always be put into the same class,

272
00:14:33,925 --> 00:14:35,380
or they will be assigned,

273
00:14:35,380 --> 00:14:37,255
uh, the same label.

274
00:14:37,255 --> 00:14:40,330
Which, uh, which is interesting and, uh,

275
00:14:40,330 --> 00:14:41,650
which now seems, uh,

276
00:14:41,650 --> 00:14:43,210
quite- uh, quite daunting,

277
00:14:43,210 --> 00:14:44,440
a bit disappointing, right?

278
00:14:44,440 --> 00:14:47,215
That we so quickly found a corner case,

279
00:14:47,215 --> 00:14:50,080
or a- or a failure case for graph neural networks

280
00:14:50,080 --> 00:14:53,830
where they basically cannot, uh, distinguish nodes.

281
00:14:53,830 --> 00:14:58,585
So the important point that I wanted to make here is that in general,

282
00:14:58,585 --> 00:15:03,190
different local neighborhoods define different computation graphs, right?

283
00:15:03,190 --> 00:15:05,380
So, uh, here are computation graphs,

284
00:15:05,380 --> 00:15:06,745
uh, for different nodes.

285
00:15:06,745 --> 00:15:09,385
These are computation graphs for nodes 1 and 2, uh,

286
00:15:09,385 --> 00:15:12,205
computation graphs for nodes 3 and 4,

287
00:15:12,205 --> 00:15:13,690
as well as computation graph,

288
00:15:13,690 --> 00:15:15,055
uh, for node 5.

289
00:15:15,055 --> 00:15:17,740
So now we already know that we won't be able to

290
00:15:17,740 --> 00:15:21,160
distinguish 1 and 2 because they have the same computation graphs.

291
00:15:21,160 --> 00:15:23,500
That's- that's some- that's fact of life.

292
00:15:23,500 --> 00:15:24,970
There's not much we can do.

293
00:15:24,970 --> 00:15:27,380
But the question still remains,

294
00:15:27,380 --> 00:15:29,375
how about 3 and 4?

295
00:15:29,375 --> 00:15:30,885
Or 3 and 5?

296
00:15:30,885 --> 00:15:33,800
Will our graph neural network be able to distinguish these nodes,

297
00:15:33,800 --> 00:15:36,890
because obviously they have different computational graphs.

298
00:15:36,890 --> 00:15:40,400
So perhaps the graph neural network is able to remember,

299
00:15:40,400 --> 00:15:44,255
or capture the structure of the computation graph,

300
00:15:44,255 --> 00:15:47,420
which means that nodes 3 and 4 will get a different embedding,

301
00:15:47,420 --> 00:15:50,030
because their computation graphs are different, right?

302
00:15:50,030 --> 00:15:51,320
That's the- in some sense,

303
00:15:51,320 --> 00:15:53,585
the big question, right?

304
00:15:53,585 --> 00:15:56,650
So basically what I'm- what is the point is?

305
00:15:56,650 --> 00:15:59,990
The point is that computational graphs are identical to

306
00:15:59,990 --> 00:16:03,410
the rooted subtree structures around each node, right?

307
00:16:03,410 --> 00:16:07,285
So we can think of this rooted subtree as it defines

308
00:16:07,285 --> 00:16:11,470
the topological structure of the neighborhood around, uh,

309
00:16:11,470 --> 00:16:15,730
each node and two nodes will be able to distinguish

310
00:16:15,730 --> 00:16:20,740
them in the best case if they have different rooted subtree structures,

311
00:16:20,740 --> 00:16:22,840
if they have different computation graphs.

312
00:16:22,840 --> 00:16:27,280
Of course, maybe our graph neural network is so imperfect that is

313
00:16:27,280 --> 00:16:31,810
not even able to distinguish nodes that have different computation graphs,

314
00:16:31,810 --> 00:16:35,335
meaning that the structure of these rooted trees is different.

315
00:16:35,335 --> 00:16:38,604
And what we are going to look at next is under what cases,

316
00:16:38,604 --> 00:16:40,915
you know, can 2 and 3 be distinguished,

317
00:16:40,915 --> 00:16:43,810
and in what cases 2 and 3 will simply be lumped

318
00:16:43,810 --> 00:16:48,820
together into the same, uh, embedding. So-

319
00:16:48,820 --> 00:16:51,430
Kind of to continue on this, right?

320
00:16:51,430 --> 00:16:55,810
GNN's node embeddings capture rooted subtree structures.

321
00:16:55,810 --> 00:17:00,655
They basically cap- they wanna capture the structure of the graphing- of the,

322
00:17:00,655 --> 00:17:04,585
uh, computational graph of the network neighborhood around a given node.

323
00:17:04,585 --> 00:17:10,420
And the most possible expressive graph neural network will map different, uh,

324
00:17:10,420 --> 00:17:13,645
rooted subtrees into different node embeddings,

325
00:17:13,645 --> 00:17:14,829
uh, here, for example,

326
00:17:14,829 --> 00:17:17,199
represented by different, uh, colors, right?

327
00:17:17,200 --> 00:17:18,310
So one and two,

328
00:17:18,310 --> 00:17:23,454
because they have exactly identical computation graphs and exactly identical features,

329
00:17:23,454 --> 00:17:25,329
will be mapped to the same point.

330
00:17:25,329 --> 00:17:26,829
There is nothing we can do about

331
00:17:26,829 --> 00:17:30,580
that with the current definition of graph neural networks.

332
00:17:30,580 --> 00:17:32,080
Um, but for example,

333
00:17:32,080 --> 00:17:34,270
nodes 3, 4 and 5,

334
00:17:34,270 --> 00:17:37,810
they don't have identical computation graph structures,

335
00:17:37,810 --> 00:17:40,390
so they should be mapped into different,

336
00:17:40,390 --> 00:17:42,880
uh, points in the embedding space, right?

337
00:17:42,880 --> 00:17:47,170
So the most expressive graph neural network will basically be

338
00:17:47,170 --> 00:17:51,625
able to learn or capture what is the structure of the computation graph,

339
00:17:51,625 --> 00:17:54,820
and based on the structure of the computation graph assign

340
00:17:54,820 --> 00:17:58,480
a different embedding for each computation graph.

341
00:17:58,480 --> 00:18:01,090
Um, that's the main, uh,

342
00:18:01,090 --> 00:18:03,685
uh, premise, uh, that,

343
00:18:03,685 --> 00:18:04,885
uh, we are making here.

344
00:18:04,885 --> 00:18:09,010
So we wanna ensure that if two nodes have different computation graphs,

345
00:18:09,010 --> 00:18:12,835
then they are mapped to different points in the embedding space.

346
00:18:12,835 --> 00:18:14,515
And the question is,

347
00:18:14,515 --> 00:18:17,905
can graph neural networks, uh, do that?

348
00:18:17,905 --> 00:18:23,890
There is an important concept for mathematics that will allow us, uh,

349
00:18:23,890 --> 00:18:27,040
to make further progress in understanding whether

350
00:18:27,040 --> 00:18:31,060
a graph neural network can take two different computation graphs,

351
00:18:31,060 --> 00:18:33,520
two different, um, rooted subtrees

352
00:18:33,520 --> 00:18:36,595
and map them into different points in the embedding space.

353
00:18:36,595 --> 00:18:38,560
And that is this notion, uh,

354
00:18:38,560 --> 00:18:42,040
or definition of what an injective function is.

355
00:18:42,040 --> 00:18:46,240
And a function that maps from the- the- from the domain X,

356
00:18:46,240 --> 00:18:49,345
uh, to domain Y is called injective.

357
00:18:49,345 --> 00:18:53,440
If it maps different elements into the different outputs.

358
00:18:53,440 --> 00:18:58,030
So what this basically means that f retains the information of the input, right?

359
00:18:58,030 --> 00:19:01,225
It means that whatever- whatever inputs you get,

360
00:19:01,225 --> 00:19:03,490
you'll always map them into distinct,

361
00:19:03,490 --> 00:19:06,400
um, distinct points, sort of distinct outputs.

362
00:19:06,400 --> 00:19:09,340
Meaning, for example, it's not that 2 and 3 would

363
00:19:09,340 --> 00:19:12,310
collide and you would give the same output A.

364
00:19:12,310 --> 00:19:15,760
So every input maps to a different output.

365
00:19:15,760 --> 00:19:18,055
That's a definition of an injective function.

366
00:19:18,055 --> 00:19:21,985
And we will- this is a very important concept because we will use it,

367
00:19:21,985 --> 00:19:25,705
uh, for the lo- rest- rest of the lecture, uh, quite heavily.

368
00:19:25,705 --> 00:19:31,885
So we wanna know how expressive is a graph neural network.

369
00:19:31,885 --> 00:19:37,465
And most expressive graph neural network should map these subtrees,

370
00:19:37,465 --> 00:19:41,815
these, uh, computation graphs to node embeddings injectively,

371
00:19:41,815 --> 00:19:44,544
meaning that for every different subtree,

372
00:19:44,544 --> 00:19:48,190
we should map it into a different point in the embedding space,

373
00:19:48,190 --> 00:19:50,890
um, and if this mapping is not injective,

374
00:19:50,890 --> 00:19:53,020
meaning that two different inputs,

375
00:19:53,020 --> 00:19:56,665
two different subtrees get mapped to the same point,

376
00:19:56,665 --> 00:19:59,815
then, um, this is not an

377
00:19:59,815 --> 00:20:03,475
injective mapping and that is, uh, the issue.

378
00:20:03,475 --> 00:20:08,860
So we wanna have and show that graphed- what we wanna develop

379
00:20:08,860 --> 00:20:12,010
a graph neural network that has this injective mapping where

380
00:20:12,010 --> 00:20:15,385
different subtrees get mapped into different points,

381
00:20:15,385 --> 00:20:17,660
uh, in the embedding space.

382
00:20:18,360 --> 00:20:21,850
So the key observation, uh,

383
00:20:21,850 --> 00:20:24,820
that will allow us to make progress is that trees of

384
00:20:24,820 --> 00:20:28,345
the same depth can be recursively characterized,

385
00:20:28,345 --> 00:20:29,785
uh, from the leaf nodes,

386
00:20:29,785 --> 00:20:31,075
uh, to the root nodes.

387
00:20:31,075 --> 00:20:35,650
So what I mean by this is if we are able to distinguish one level of the tree,

388
00:20:35,650 --> 00:20:37,420
then we can recursively,

389
00:20:37,420 --> 00:20:38,770
uh, take these, uh,

390
00:20:38,770 --> 00:20:42,805
levels and aggregate them together into a unique description of the tree.

391
00:20:42,805 --> 00:20:45,670
So what I mean by this is, for example, um,

392
00:20:45,670 --> 00:20:48,610
the way you can characterize the tree is simply by the number of

393
00:20:48,610 --> 00:20:51,850
children each node has, all right?

394
00:20:51,850 --> 00:20:54,325
So for example, here you could say, aha,

395
00:20:54,325 --> 00:20:56,980
at the lower level one node has three neighbors,

396
00:20:56,980 --> 00:21:00,385
three children, and the other node has two children.

397
00:21:00,385 --> 00:21:04,580
Um, and then you can say a-ha and then the node, uh, uh,

398
00:21:04,580 --> 00:21:07,675
the- the- the- the root has,

399
00:21:07,675 --> 00:21:09,760
uh, has, uh, um, has,

400
00:21:09,760 --> 00:21:11,140
uh, two children as well.

401
00:21:11,140 --> 00:21:14,410
So I can characterize this by saying, aha, um,

402
00:21:14,410 --> 00:21:16,000
you know, uh, we have,

403
00:21:16,000 --> 00:21:17,680
uh, two neighbors at, uh,

404
00:21:17,680 --> 00:21:20,725
level 0, we have three neighbors at level 0, and, uh,

405
00:21:20,725 --> 00:21:22,420
we have, uh, two neighbors,

406
00:21:22,420 --> 00:21:24,355
uh, at level, uh, at level 1.

407
00:21:24,355 --> 00:21:27,685
While for example, for this particular computation graph,

408
00:21:27,685 --> 00:21:30,235
I have one child here, uh,

409
00:21:30,235 --> 00:21:31,750
three- three children here,

410
00:21:31,750 --> 00:21:33,190
and then, again, two children here.

411
00:21:33,190 --> 00:21:36,520
So this description is different than their description.

412
00:21:36,520 --> 00:21:39,930
So it means I'm able to separate out, um,

413
00:21:39,930 --> 00:21:44,715
or to distinguish between these two- these two different, uh, uh trees.

414
00:21:44,715 --> 00:21:48,315
Uh, the important thing is that trees can be decomposed level by level,

415
00:21:48,315 --> 00:21:52,235
so if I'm able to capture the structure of a single level of the tree,

416
00:21:52,235 --> 00:21:54,250
perhaps even just this level,

417
00:21:54,250 --> 00:21:56,110
then I can recursively do this,

418
00:21:56,110 --> 00:21:57,355
uh, level by level.

419
00:21:57,355 --> 00:22:00,070
So what I mean is, um,

420
00:22:00,070 --> 00:22:04,615
we only need to focus on how do we characterize one level of this, uh,

421
00:22:04,615 --> 00:22:06,490
computation graph or this, uh,

422
00:22:06,490 --> 00:22:10,915
rooted subtree around a given node, uh, of interest.

423
00:22:10,915 --> 00:22:15,550
So, um, let's continue thinking and setting up the problem.

424
00:22:15,550 --> 00:22:19,090
So if each step of GNN, uh,

425
00:22:19,090 --> 00:22:23,110
aggregation process can fully retain the neighborhood information,

426
00:22:23,110 --> 00:22:24,970
meaning how many children,

427
00:22:24,970 --> 00:22:27,175
uh, neighbors does a given node have?

428
00:22:27,175 --> 00:22:31,330
Then, uh, the generated node embeddings can distinguish different,

429
00:22:31,330 --> 00:22:33,250
uh, subtree structures, right?

430
00:22:33,250 --> 00:22:35,140
If I can say, um,

431
00:22:35,140 --> 00:22:36,910
at level, uh, 1, uh,

432
00:22:36,910 --> 00:22:41,650
in- in one tree I have two- two children in the other one I have three.

433
00:22:41,650 --> 00:22:43,600
Um, and if I can kind of capture

434
00:22:43,600 --> 00:22:46,990
this information and propagate it all the way up to the node 1.

435
00:22:46,990 --> 00:22:51,040
And in this other tree I can kind of capture the information that one,

436
00:22:51,040 --> 00:22:54,895
uh, one node has one child and the other node has, uh, three children.

437
00:22:54,895 --> 00:22:57,850
And again, I'm able to retain this information, uh,

438
00:22:57,850 --> 00:22:59,575
all the way to the top layer,

439
00:22:59,575 --> 00:23:03,955
then obviously the- the number of children, um, is different.

440
00:23:03,955 --> 00:23:06,415
So these two, uh, these two trees, uh,

441
00:23:06,415 --> 00:23:09,310
we are able to, uh, distinguish them, right?

442
00:23:09,310 --> 00:23:11,005
So the point is, in some sense,

443
00:23:11,005 --> 00:23:14,380
are we able to aggregate information from the children and somehow

444
00:23:14,380 --> 00:23:18,400
store it so that when we pass it on to our parent in the tree,

445
00:23:18,400 --> 00:23:21,175
this information, uh, gets retained?

446
00:23:21,175 --> 00:23:23,305
As in this case, the information that

447
00:23:23,305 --> 00:23:27,505
two and three got retained all the way up to the root of the tree.

448
00:23:27,505 --> 00:23:31,675
That's, uh, the question, uh, we wanna answer.

449
00:23:31,675 --> 00:23:34,780
So in other words,

450
00:23:34,780 --> 00:23:37,420
what we wanna do is we wanna say that

451
00:23:37,420 --> 00:23:40,660
the most expressive graph neural network would use

452
00:23:40,660 --> 00:23:44,530
an injective neighborhood aggregation for each step,

453
00:23:44,530 --> 00:23:49,075
for each layer of the or for each level of the computation graph.

454
00:23:49,075 --> 00:23:50,500
So this means that it will map

455
00:23:50,500 --> 00:23:54,550
different neighborhoods into different, um, embeddings, right?

456
00:23:54,550 --> 00:23:58,315
So we want to be able to capture the number of children at level 1,

457
00:23:58,315 --> 00:24:00,460
oh sorry, at level 0, at level 1,

458
00:24:00,460 --> 00:24:04,840
and then aggregate this- kind of retain this information as we are pushing it

459
00:24:04,840 --> 00:24:09,685
up the tree so that the tree knows how many children each of its,

460
00:24:09,685 --> 00:24:11,110
um, each of its, uh,

461
00:24:11,110 --> 00:24:13,480
uh, inner nodes, uh, have.

462
00:24:13,480 --> 00:24:16,360
So that's essentially the idea.

463
00:24:16,360 --> 00:24:19,600
So the summary so far is the following.

464
00:24:19,600 --> 00:24:21,415
To generate a node embedding,

465
00:24:21,415 --> 00:24:24,310
GNN uses a computational graph that

466
00:24:24,310 --> 00:24:28,420
corresponds to a rooted subtree structure around each node.

467
00:24:28,420 --> 00:24:30,550
So if I have a node,

468
00:24:30,550 --> 00:24:33,565
I have this notion of a computational graph that is simply a-

469
00:24:33,565 --> 00:24:36,054
a rooted subtree structure that describes

470
00:24:36,054 --> 00:24:39,145
the local neighborhood structure around this node.

471
00:24:39,145 --> 00:24:42,130
And then different rooted subtrees,

472
00:24:42,130 --> 00:24:44,995
different computation graphs will be distinguishable

473
00:24:44,995 --> 00:24:48,115
if we are using injective neighborhood aggregation,

474
00:24:48,115 --> 00:24:51,205
meaning we are able to distinguish different subtrees.

475
00:24:51,205 --> 00:24:55,015
And GNNs can- as we are going to see, um,

476
00:24:55,015 --> 00:24:58,254
GNNs can fully distinguish different subtree structures

477
00:24:58,254 --> 00:25:01,720
if at every level its neighborhood aggregation,

478
00:25:01,720 --> 00:25:05,080
meaning it's aggregation for the children, is injective,

479
00:25:05,080 --> 00:25:09,190
which means that no information, uh, gets lost.

480
00:25:09,190 --> 00:25:11,485
So then we can fully characterize, uh,

481
00:25:11,485 --> 00:25:16,730
the computation graph and distinguish one computation graph, uh, from the other.

