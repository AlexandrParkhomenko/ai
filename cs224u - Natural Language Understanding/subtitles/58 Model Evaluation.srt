1
00:00:00,000 --> 00:00:04,058


2
00:00:04,058 --> 00:00:05,600
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,600 --> 00:00:08,080
This is Part 5 in our series
on methods and metrics.

4
00:00:08,080 --> 00:00:10,240
We're going to be talking
about essential selected

5
00:00:10,240 --> 00:00:14,790
topics in model
evaluation in our field.

6
00:00:14,790 --> 00:00:15,800
Here's our overview.

7
00:00:15,800 --> 00:00:19,230
I'd like to start by talking
about baselines and their role

8
00:00:19,230 --> 00:00:21,390
in experimental comparisons.

9
00:00:21,390 --> 00:00:23,520
Then we'll discuss
hyperparameter optimization,

10
00:00:23,520 --> 00:00:25,740
both the process
and the motivations,

11
00:00:25,740 --> 00:00:28,170
as well as compromises
that you might

12
00:00:28,170 --> 00:00:30,000
have to make due to
resource constraints

13
00:00:30,000 --> 00:00:31,670
and other constraints.

14
00:00:31,670 --> 00:00:33,820
We'll touch briefly on
classifier comparison,

15
00:00:33,820 --> 00:00:37,320
which is a topic we covered in
the sentiment analysis unit.

16
00:00:37,320 --> 00:00:39,540
And then we'll close with
two topics that are really

17
00:00:39,540 --> 00:00:42,930
pressing for deep learning
models, which is assessing

18
00:00:42,930 --> 00:00:45,990
models without convergence and
the role of random parameter

19
00:00:45,990 --> 00:00:48,930
initialization in shaping
experimental results.

20
00:00:48,930 --> 00:00:51,480


21
00:00:51,480 --> 00:00:54,020
So let's begin with baselines.

22
00:00:54,020 --> 00:00:56,660
The fundamental insight
here is that in our field

23
00:00:56,660 --> 00:00:59,540
evaluation numbers can
never be understood properly

24
00:00:59,540 --> 00:01:00,380
in isolation.

25
00:01:00,380 --> 00:01:02,670
Let's consider
two extreme cases.

26
00:01:02,670 --> 00:01:05,150
Suppose your system
gets 0.95 F1,

27
00:01:05,150 --> 00:01:08,210
then you might feel like you can
declare victory at that point.

28
00:01:08,210 --> 00:01:10,460
But it will be
natural for people

29
00:01:10,460 --> 00:01:13,140
who are consuming your
results to ask, well,

30
00:01:13,140 --> 00:01:14,510
is the task too easy?

31
00:01:14,510 --> 00:01:17,360
Is it really an achievement
that you've got 0.95?

32
00:01:17,360 --> 00:01:19,790
Or would even
simpler systems have

33
00:01:19,790 --> 00:01:21,595
achieved something similar?

34
00:01:21,595 --> 00:01:22,970
At the other end
of the spectrum,

35
00:01:22,970 --> 00:01:25,945
suppose your system
gets 0.6 F1, you

36
00:01:25,945 --> 00:01:28,070
might think that means you
haven't gotten traction.

37
00:01:28,070 --> 00:01:29,520
But we should ask two questions.

38
00:01:29,520 --> 00:01:32,870
First, what do humans get
as a kind of upper bound?

39
00:01:32,870 --> 00:01:35,090
And also, what would a
random classifier get?

40
00:01:35,090 --> 00:01:36,980
And if your 0.6 is
really different

41
00:01:36,980 --> 00:01:39,740
from the random classifier
and human performance

42
00:01:39,740 --> 00:01:42,680
is kind of low, we might
then see that this 0.6

43
00:01:42,680 --> 00:01:45,875
F1 is a real achievement.

44
00:01:45,875 --> 00:01:47,500
Now it kind of shows
you that baselines

45
00:01:47,500 --> 00:01:50,690
are just crucial for strong
experiments in our field.

46
00:01:50,690 --> 00:01:52,390
So defining baselines
for you should not

47
00:01:52,390 --> 00:01:54,790
be an afterthought, but
rather central to how you

48
00:01:54,790 --> 00:01:57,640
define your overall hypothesis.

49
00:01:57,640 --> 00:02:01,000
Baselines are really important
for building a persuasive case.

50
00:02:01,000 --> 00:02:03,790
And they can be used to
illuminate specific aspects

51
00:02:03,790 --> 00:02:05,560
of the problem that
you're tackling

52
00:02:05,560 --> 00:02:08,458
and specific virtues of
your proposed system.

53
00:02:08,458 --> 00:02:10,750
But this really comes down
to is, right from the start,

54
00:02:10,750 --> 00:02:12,880
you might be saying,
for example, here's

55
00:02:12,880 --> 00:02:16,790
a baseline model, here's my
proposed modification of it.

56
00:02:16,790 --> 00:02:18,640
And the way we test
your hypothesis

57
00:02:18,640 --> 00:02:21,700
is by comparing the performance
of those two systems.

58
00:02:21,700 --> 00:02:24,160
In that context, you can see
that the baseline is playing

59
00:02:24,160 --> 00:02:27,100
a crucial role in quantifying
the extent to which

60
00:02:27,100 --> 00:02:28,750
your hypothesis is true.

61
00:02:28,750 --> 00:02:32,020
And therefore, careful model
comparisons at that level

62
00:02:32,020 --> 00:02:33,460
are going to be
really fundamental

63
00:02:33,460 --> 00:02:38,000
to successful pursuit
of the hypothesis.

64
00:02:38,000 --> 00:02:40,640
When in doubt, you could
include random baselines

65
00:02:40,640 --> 00:02:41,810
in your results table.

66
00:02:41,810 --> 00:02:43,485
They are very easy
to set up and can

67
00:02:43,485 --> 00:02:45,110
illuminate what it's
like if we're just

68
00:02:45,110 --> 00:02:46,940
making random predictions.

69
00:02:46,940 --> 00:02:49,040
And here I'm showing you
that scikit-learn kind of

70
00:02:49,040 --> 00:02:50,430
has you covered on this point.

71
00:02:50,430 --> 00:02:52,820
They have two classes,
DummyClassifier

72
00:02:52,820 --> 00:02:56,210
and DummyRegressor, each with
a wide range of different ways

73
00:02:56,210 --> 00:02:59,355
that they can make random
guesses based on the data.

74
00:02:59,355 --> 00:03:01,730
And I would encourage you to
use these classes because it

75
00:03:01,730 --> 00:03:04,670
will make it easy for you
to fit the random baselines

76
00:03:04,670 --> 00:03:07,840
into your overall experimental
pipeline, which will reduce

77
00:03:07,840 --> 00:03:10,340
the amount of code that you
have to write and possibly avoid

78
00:03:10,340 --> 00:03:13,160
bugs that might come from
implementing these baselines

79
00:03:13,160 --> 00:03:14,000
yourself.

80
00:03:14,000 --> 00:03:16,245
So strongly encouraged.

81
00:03:16,245 --> 00:03:18,120
And kind of at the other
end of the spectrum,

82
00:03:18,120 --> 00:03:19,995
you might think, for
your task, whether there

83
00:03:19,995 --> 00:03:23,342
are task-specific baselines
that you should be considering.

84
00:03:23,342 --> 00:03:24,800
Because they might
reveal something

85
00:03:24,800 --> 00:03:27,050
about the dataset, or
the problem, or the way

86
00:03:27,050 --> 00:03:29,540
people are going about
modeling the problem.

87
00:03:29,540 --> 00:03:31,800
We saw an example of this
before in the context

88
00:03:31,800 --> 00:03:33,050
of natural language inference.

89
00:03:33,050 --> 00:03:37,280
We saw that hypothesis-only
baselines tended to make

90
00:03:37,280 --> 00:03:41,480
predictions that were as
good as 0.65 to 0.70 F1,

91
00:03:41,480 --> 00:03:45,080
which is substantially better
than the baseline random chance

92
00:03:45,080 --> 00:03:47,600
which would be at about 0.33.

93
00:03:47,600 --> 00:03:50,720
And that's revealing to us that
when we measure performance,

94
00:03:50,720 --> 00:03:52,430
we should really be
thinking about gains

95
00:03:52,430 --> 00:03:55,040
above that
hypothesis-only baseline.

96
00:03:55,040 --> 00:03:56,780
Comparisons against
random chance

97
00:03:56,780 --> 00:03:59,180
are going to vastly overstate
the extent to which we

98
00:03:59,180 --> 00:04:04,070
have made meaningful
progress on those datasets.

99
00:04:04,070 --> 00:04:06,980
The story of the Story Cloze
task is somewhat similar.

100
00:04:06,980 --> 00:04:09,710
Here the task is to distinguish
between a coherent and

101
00:04:09,710 --> 00:04:12,020
incoherent ending for a story.

102
00:04:12,020 --> 00:04:13,700
And people observed
that systems that

103
00:04:13,700 --> 00:04:15,533
looked only at
the ending options

104
00:04:15,533 --> 00:04:16,700
were able to do really well.

105
00:04:16,700 --> 00:04:21,350
There is some bias in coherent
and incoherent continuations

106
00:04:21,350 --> 00:04:24,140
that leads them to be pretty
good evidence for making

107
00:04:24,140 --> 00:04:26,180
this classification decision.

108
00:04:26,180 --> 00:04:27,890
Again, you might
think that reveals

109
00:04:27,890 --> 00:04:30,140
that there's a fundamental
problem with the data set.

110
00:04:30,140 --> 00:04:31,140
And that might be true.

111
00:04:31,140 --> 00:04:33,218
But another
perspective is simply

112
00:04:33,218 --> 00:04:35,510
that when we do comparisons
and think about the model's

113
00:04:35,510 --> 00:04:38,360
performance, it should be
with this as the baseline

114
00:04:38,360 --> 00:04:39,620
and not random guessing.

115
00:04:39,620 --> 00:04:43,300


116
00:04:43,300 --> 00:04:45,970
OK, let's talk about
hyperparameter optimization.

117
00:04:45,970 --> 00:04:48,800
We discussed this in our
unit on sentiment analysis.

118
00:04:48,800 --> 00:04:50,740
And we walked through
some of the rationale.

119
00:04:50,740 --> 00:04:53,650
Let me quickly reiterate
the full case for this.

120
00:04:53,650 --> 00:04:55,630
First, hyperparameter
optimization

121
00:04:55,630 --> 00:04:58,480
might be crucial for obtaining
the best version of your model

122
00:04:58,480 --> 00:05:01,450
that you can, which might
be your fundamental goal.

123
00:05:01,450 --> 00:05:04,420
Probably for any modern model
that you're looking at there

124
00:05:04,420 --> 00:05:06,550
is a wide range of
hyperparameters.

125
00:05:06,550 --> 00:05:08,800
And we might know that
different settings of them

126
00:05:08,800 --> 00:05:10,820
lead to very different outcomes.

127
00:05:10,820 --> 00:05:13,300
So it's in your best interest
to do hyperparameter search

128
00:05:13,300 --> 00:05:16,360
to put your model in
the very best light.

129
00:05:16,360 --> 00:05:17,950
We also talked at
length about how

130
00:05:17,950 --> 00:05:21,520
this is a crucial step in
conducting fair comparisons

131
00:05:21,520 --> 00:05:22,720
between models.

132
00:05:22,720 --> 00:05:25,330
It's really important that
when you conduct a comparison

133
00:05:25,330 --> 00:05:28,510
you not put one model in
its best light with its best

134
00:05:28,510 --> 00:05:31,390
hyperparameter settings and
have all the other models

135
00:05:31,390 --> 00:05:34,420
be kind of randomly chosen
or even poorly chosen

136
00:05:34,420 --> 00:05:36,130
hyperparameter
settings, because that

137
00:05:36,130 --> 00:05:39,520
would lead to unfair comparisons
and exaggerate differences

138
00:05:39,520 --> 00:05:40,820
between the models.

139
00:05:40,820 --> 00:05:43,030
What we want to do
is compare the models

140
00:05:43,030 --> 00:05:45,850
all with their best possible
hyperparameter settings.

141
00:05:45,850 --> 00:05:50,075
And that implies doing extensive
search to find those settings.

142
00:05:50,075 --> 00:05:51,700
And the third motivation
you might have

143
00:05:51,700 --> 00:05:54,310
is just to understand the
stability of your architecture.

144
00:05:54,310 --> 00:05:57,520
We might want to know for some
large space of hyperparameters

145
00:05:57,520 --> 00:06:00,250
which ones really matter
for final performance,

146
00:06:00,250 --> 00:06:03,220
maybe which ones lead to
really degenerate solutions,

147
00:06:03,220 --> 00:06:05,410
and which space
of hyperparameters

148
00:06:05,410 --> 00:06:06,790
overall perform the best.

149
00:06:06,790 --> 00:06:09,370
So that we have more than just
a single set of parameters

150
00:06:09,370 --> 00:06:11,530
that work well, but
maybe real insights

151
00:06:11,530 --> 00:06:16,060
into the overall settings of
the models that are really good.

152
00:06:16,060 --> 00:06:18,970
There's one more rule that
I need to reiterate here.

153
00:06:18,970 --> 00:06:21,610
All hyperparameter
tuning must be done only

154
00:06:21,610 --> 00:06:23,530
on train and development data.

155
00:06:23,530 --> 00:06:26,980
It is a sin in our field to
do any kind of hyperparameter

156
00:06:26,980 --> 00:06:28,570
tuning on a test set.

157
00:06:28,570 --> 00:06:31,420
All of that tuning should
happen outside of the test set.

158
00:06:31,420 --> 00:06:33,310
And then as usual,
you get one run

159
00:06:33,310 --> 00:06:35,380
on the test set with
your chosen parameters

160
00:06:35,380 --> 00:06:38,500
and that is the number that
you report as performance

161
00:06:38,500 --> 00:06:39,940
on the test data.

162
00:06:39,940 --> 00:06:42,100
That's the only way
that we can really

163
00:06:42,100 --> 00:06:44,260
get a look at how
these systems behave

164
00:06:44,260 --> 00:06:45,920
on completely unseen data.

165
00:06:45,920 --> 00:06:49,000
So this is really crucial
for understanding progress

166
00:06:49,000 --> 00:06:51,700
in our field.

167
00:06:51,700 --> 00:06:55,180
Now hyperparameter optimization,
as you can imagine,

168
00:06:55,180 --> 00:06:56,397
can get very expensive.

169
00:06:56,397 --> 00:06:58,855
And let's review that and then
talk about some compromises,

170
00:06:58,855 --> 00:06:59,740
right?

171
00:06:59,740 --> 00:07:02,230
The ideal for
hyperparameter optimization

172
00:07:02,230 --> 00:07:06,020
is that you identify a large
set of values for your model.

173
00:07:06,020 --> 00:07:08,800
You create a list of all the
combinations of those values.

174
00:07:08,800 --> 00:07:11,180
This will be the cross
product of all the values

175
00:07:11,180 --> 00:07:13,880
of the features
that you identified.

176
00:07:13,880 --> 00:07:15,310
And for each of
the settings, you

177
00:07:15,310 --> 00:07:18,730
should cross validate it on
the available training data.

178
00:07:18,730 --> 00:07:21,580
And then choose the settings
that are the best at step

179
00:07:21,580 --> 00:07:25,250
three, train on all the training
data using those settings.

180
00:07:25,250 --> 00:07:28,060
And then finally,
evaluate on the test data.

181
00:07:28,060 --> 00:07:29,098
That is the ideal here.

182
00:07:29,098 --> 00:07:31,640
And let's just think about how
that's actually going to work.

183
00:07:31,640 --> 00:07:34,240
Suppose for our example that
we have one hyperparameter

184
00:07:34,240 --> 00:07:36,600
and it has five values.

185
00:07:36,600 --> 00:07:39,415
And we have a second
hyperparameter with ten values.

186
00:07:39,415 --> 00:07:41,040
Then the cross product
is going to lead

187
00:07:41,040 --> 00:07:44,910
us to have 50 total settings
for those hyperparameters.

188
00:07:44,910 --> 00:07:47,520
Suppose we add a third
hyperparameter with two values.

189
00:07:47,520 --> 00:07:52,620
Now the number of settings that
we have has jumped up to 100.

190
00:07:52,620 --> 00:07:54,840
If we want to do
5-fold cross-validation

191
00:07:54,840 --> 00:07:56,820
to select those
optimal parameters,

192
00:07:56,820 --> 00:08:01,320
then we are talking about doing
500 different experiments.

193
00:08:01,320 --> 00:08:03,150
That's probably
perfectly fine if you're

194
00:08:03,150 --> 00:08:04,980
dealing with a
small linear model

195
00:08:04,980 --> 00:08:06,720
with some hand-built features.

196
00:08:06,720 --> 00:08:09,750
But if you are fitting a
large transformer-based model

197
00:08:09,750 --> 00:08:12,540
where each experiment
takes you up to one day,

198
00:08:12,540 --> 00:08:14,760
this is going to be
prohibitively expensive

199
00:08:14,760 --> 00:08:17,220
in terms of time or
compute resources.

200
00:08:17,220 --> 00:08:19,830
And that's going to compel
us to make some compromises.

201
00:08:19,830 --> 00:08:21,720
This is the bottom line here.

202
00:08:21,720 --> 00:08:23,910
The above picture,
that ideal, is

203
00:08:23,910 --> 00:08:27,450
untenable as a set of laws
for our scientific community.

204
00:08:27,450 --> 00:08:30,270
If we adopted it,
then complex models

205
00:08:30,270 --> 00:08:33,090
trained on large data sets
would end up disfavored

206
00:08:33,090 --> 00:08:36,580
and only the very wealthy
would be able to participate.

207
00:08:36,580 --> 00:08:38,933
And just to give you a
glimpse of just how expensive

208
00:08:38,933 --> 00:08:40,350
this could get,
here's a quotation

209
00:08:40,350 --> 00:08:42,750
from this nice paper
on NLP and machine

210
00:08:42,750 --> 00:08:44,100
learning for health care.

211
00:08:44,100 --> 00:08:45,840
In their supplementary
materials,

212
00:08:45,840 --> 00:08:48,180
they report that
performance on all

213
00:08:48,180 --> 00:08:51,180
of the above neural networks
was tuned automatically using

214
00:08:51,180 --> 00:08:57,000
Google Vizier with a total
of over 200,000 GPU hours.

215
00:08:57,000 --> 00:08:58,500
For me as a private
citizen, that

216
00:08:58,500 --> 00:09:01,100
could easily cost a
half a million dollars

217
00:09:01,100 --> 00:09:05,010
just for the process of
hyperparameter optimization.

218
00:09:05,010 --> 00:09:07,500
And that's what I mean by this
being kind of fundamentally

219
00:09:07,500 --> 00:09:09,180
untenable for us.

220
00:09:09,180 --> 00:09:10,990
So what should we
do in response?

221
00:09:10,990 --> 00:09:13,410
We need a pragmatic
response here.

222
00:09:13,410 --> 00:09:14,880
Here are some
steps that you take

223
00:09:14,880 --> 00:09:18,210
to alleviate the problem in what
I view as kind of descending

224
00:09:18,210 --> 00:09:19,920
order of attractiveness.

225
00:09:19,920 --> 00:09:21,930
So starting with
the best option,

226
00:09:21,930 --> 00:09:23,730
you could do some
random sampling

227
00:09:23,730 --> 00:09:26,160
and maybe guided
sampling to explore

228
00:09:26,160 --> 00:09:29,520
a large space of hyperparameters
on a fixed computational

229
00:09:29,520 --> 00:09:32,000
budget.

230
00:09:32,000 --> 00:09:34,340
You could do search based
on just a few epochs

231
00:09:34,340 --> 00:09:36,480
of training, right, rather
than allowing your model

232
00:09:36,480 --> 00:09:40,010
to run for many epochs,
which could take a whole day.

233
00:09:40,010 --> 00:09:43,370
You might select hyperparameters
based on one or two epochs,

234
00:09:43,370 --> 00:09:47,090
on the assumption that settings
that are good at the start

235
00:09:47,090 --> 00:09:49,550
will remain good and settings
that are bad at the start

236
00:09:49,550 --> 00:09:50,450
will remain bad.

237
00:09:50,450 --> 00:09:51,830
That's a heuristic assumption.

238
00:09:51,830 --> 00:09:53,360
But it seems reasonable.

239
00:09:53,360 --> 00:09:55,700
You could possibly bolster
it with some learning curves

240
00:09:55,700 --> 00:09:56,600
and so forth.

241
00:09:56,600 --> 00:09:58,910
And that could vastly
cut down on the amount

242
00:09:58,910 --> 00:10:02,870
that you have to spend
in this search process.

243
00:10:02,870 --> 00:10:05,270
You can also search based
on subsets of the data.

244
00:10:05,270 --> 00:10:07,350
This would be another
kind of compromise.

245
00:10:07,350 --> 00:10:10,310
However because a lot of
hyperparameters are dependent

246
00:10:10,310 --> 00:10:13,920
on dataset size,z think
of regularization terms.

247
00:10:13,920 --> 00:10:16,610
This might be riskier
than the version in 2

248
00:10:16,610 --> 00:10:20,560
there where we just
trained for a few epochs.

249
00:10:20,560 --> 00:10:23,320
You also might do some
heuristic search, maybe

250
00:10:23,320 --> 00:10:25,450
by defining which
hyperparameters matter less

251
00:10:25,450 --> 00:10:28,450
and then set those by hand
based on this heuristic search.

252
00:10:28,450 --> 00:10:30,910
And then you might just describe
that process in the paper

253
00:10:30,910 --> 00:10:34,150
that via few observations
you made some guesses

254
00:10:34,150 --> 00:10:36,280
about parameters
that you could fix

255
00:10:36,280 --> 00:10:39,100
and therefore explored a smaller
subset of the space that you

256
00:10:39,100 --> 00:10:40,660
might have liked to explore.

257
00:10:40,660 --> 00:10:42,130
Again, I think if
you make the case

258
00:10:42,130 --> 00:10:43,930
and you're clear
about this, readers

259
00:10:43,930 --> 00:10:47,770
will be receptive because
we're aware of the costs.

260
00:10:47,770 --> 00:10:49,720
You can also find
optimal hyperparameters

261
00:10:49,720 --> 00:10:52,060
via a single split of
your data and use them

262
00:10:52,060 --> 00:10:54,220
for all subsequent splits.

263
00:10:54,220 --> 00:10:56,710
That would be justified if
the splits are very similar

264
00:10:56,710 --> 00:10:59,080
and your model performance
is very stable.

265
00:10:59,080 --> 00:11:01,450
And that would reduce all
that cross-validation that

266
00:11:01,450 --> 00:11:03,310
did cause the number
of experiments

267
00:11:03,310 --> 00:11:07,450
we had to run to jump
up by a large amount.

268
00:11:07,450 --> 00:11:10,150
And finally, you might
adopt other's choices.

269
00:11:10,150 --> 00:11:12,850
Now the skeptic will complain
that these findings don't

270
00:11:12,850 --> 00:11:16,420
translate to new data sets,
but it could be the only option

271
00:11:16,420 --> 00:11:18,100
that you just
observe for example

272
00:11:18,100 --> 00:11:21,220
that for some very large
model, the original authors use

273
00:11:21,220 --> 00:11:23,590
settings X, Y, and Z,
then you might simply

274
00:11:23,590 --> 00:11:27,610
adopt them even knowing that
your dataset or your test

275
00:11:27,610 --> 00:11:30,550
might call for different
optimal settings.

276
00:11:30,550 --> 00:11:31,700
It isn't the best.

277
00:11:31,700 --> 00:11:34,700
But if it's the only
thing that you can afford,

278
00:11:34,700 --> 00:11:38,940
it's certainly a
reasonable case to make.

279
00:11:38,940 --> 00:11:41,190
Finally some tools for
hyperparameter search.

280
00:11:41,190 --> 00:11:42,930
As usual scikit-learn
has a bunch

281
00:11:42,930 --> 00:11:46,170
of great tools for this,
GridSearch, RandomizedSearch,

282
00:11:46,170 --> 00:11:48,180
and HalvingGridSearch.

283
00:11:48,180 --> 00:11:49,920
GridSearch will be
the most expensive,

284
00:11:49,920 --> 00:11:51,630
RandomizedSearch
the least expensive,

285
00:11:51,630 --> 00:11:53,130
and HalvingGridSearch
will help you

286
00:11:53,130 --> 00:11:56,420
kind of strategically
navigate through the space

287
00:11:56,420 --> 00:11:57,780
of hyperparameters.

288
00:11:57,780 --> 00:12:00,090
And if you want to go even
further in that direction,

289
00:12:00,090 --> 00:12:03,840
the scikit-optimize
package offers a bunch

290
00:12:03,840 --> 00:12:08,040
of tools for doing model-based
performance-driven exploration

291
00:12:08,040 --> 00:12:09,470
of a space of hyperparameters.

292
00:12:09,470 --> 00:12:13,600
And that could be
very effective indeed.

293
00:12:13,600 --> 00:12:16,370
All right, let's talk briefly
about classifier comparison.

294
00:12:16,370 --> 00:12:18,530
We've-- it's a topic
we've reviewed before,

295
00:12:18,530 --> 00:12:20,230
but I'll just briefly recap.

296
00:12:20,230 --> 00:12:21,380
The scenario is this.

297
00:12:21,380 --> 00:12:23,710
Suppose you have assessed
two classifier models.

298
00:12:23,710 --> 00:12:26,200
Their performance is probably
different to some degree

299
00:12:26,200 --> 00:12:27,430
numerically.

300
00:12:27,430 --> 00:12:29,830
What can be done to establish
whether those models are

301
00:12:29,830 --> 00:12:32,310
different in some
meaningful sense?

302
00:12:32,310 --> 00:12:35,140
As we've discussed, I think
guidance from the literature

303
00:12:35,140 --> 00:12:37,630
is that first we could
cover practical differences.

304
00:12:37,630 --> 00:12:41,260
If you just observed that one
model makes 10,000 more highly

305
00:12:41,260 --> 00:12:43,160
important predictions
than another,

306
00:12:43,160 --> 00:12:45,130
then that might be
sufficient to make the case

307
00:12:45,130 --> 00:12:47,260
that it's the better model.

308
00:12:47,260 --> 00:12:49,000
For differences
that are narrower,

309
00:12:49,000 --> 00:12:52,870
again the guidance is that we
might use confidence intervals

310
00:12:52,870 --> 00:12:54,040
on repeated runs.

311
00:12:54,040 --> 00:12:57,340
Or the Wilcoxon signed-rank
test to give a single summary

312
00:12:57,340 --> 00:13:00,250
statistic of whether or
not the different runs are

313
00:13:00,250 --> 00:13:03,810
truly different in their
means and variance.

314
00:13:03,810 --> 00:13:06,600
You could use McNemar's
test if you can only

315
00:13:06,600 --> 00:13:09,900
afford to run one experiment,
whereas the Wilcoxon

316
00:13:09,900 --> 00:13:11,910
and confidence
intervals will require

317
00:13:11,910 --> 00:13:15,570
you to run 10 to 20 different
experiments, which again, could

318
00:13:15,570 --> 00:13:17,010
be prohibitively expensive.

319
00:13:17,010 --> 00:13:19,560
And in those situations,
you might fall back

320
00:13:19,560 --> 00:13:23,250
to McNemar's test, because it's
less expensive and arguably

321
00:13:23,250 --> 00:13:25,800
better than nothing,
especially in scenarios

322
00:13:25,800 --> 00:13:28,987
where it's hard to tell whether
there are practical differences

323
00:13:28,987 --> 00:13:29,820
between the systems.

324
00:13:29,820 --> 00:13:32,910


325
00:13:32,910 --> 00:13:34,580
Finally, let's talk
about two topics

326
00:13:34,580 --> 00:13:37,490
that seem especially pressing
in the context of large scale

327
00:13:37,490 --> 00:13:38,390
deep learning models.

328
00:13:38,390 --> 00:13:41,105
And the first is assessing
models without convergence,

329
00:13:41,105 --> 00:13:42,320
right?

330
00:13:42,320 --> 00:13:45,080
When working with linear
models, convergence issues

331
00:13:45,080 --> 00:13:48,110
rarely arise, because
the models seem

332
00:13:48,110 --> 00:13:51,380
to converge quickly based on
whatever threshold you've set.

333
00:13:51,380 --> 00:13:54,290
And convergence implies kind
of maximized performance

334
00:13:54,290 --> 00:13:56,750
in a wide range of cases.

335
00:13:56,750 --> 00:13:59,270
With neural networks
however, convergence issues

336
00:13:59,270 --> 00:14:00,600
really take center stage.

337
00:14:00,600 --> 00:14:03,920
The models rarely converge even
based on liberal thresholds

338
00:14:03,920 --> 00:14:05,880
that you might set.

339
00:14:05,880 --> 00:14:08,040
They converge at different
rates between runs,

340
00:14:08,040 --> 00:14:09,330
so it's hard to predict.

341
00:14:09,330 --> 00:14:11,520
And their performance
on the test data

342
00:14:11,520 --> 00:14:16,560
is often heavily dependent
on these differences, right?

343
00:14:16,560 --> 00:14:18,780
Sometimes a model
with a low final error

344
00:14:18,780 --> 00:14:20,370
turns out to be great.

345
00:14:20,370 --> 00:14:23,280
And sometimes it turns out
to be worse than one that

346
00:14:23,280 --> 00:14:24,660
finished with a higher error.

347
00:14:24,660 --> 00:14:26,880
Who really knows
what's going on?

348
00:14:26,880 --> 00:14:28,890
Our only fallback
in these situations

349
00:14:28,890 --> 00:14:31,230
is just do experiments
and observe

350
00:14:31,230 --> 00:14:33,970
what seems to work the best.

351
00:14:33,970 --> 00:14:36,660
So I think a very natural
and easy to implement

352
00:14:36,660 --> 00:14:38,850
response to this that
proves highly effective

353
00:14:38,850 --> 00:14:42,310
is what I'm calling your
incremental dev-set testing.

354
00:14:42,310 --> 00:14:45,240
This is just the idea
that as training proceeds,

355
00:14:45,240 --> 00:14:48,420
we will regularly collect
information about performance

356
00:14:48,420 --> 00:14:52,270
on some held out dev-set as
part of the training process.

357
00:14:52,270 --> 00:14:54,720
For example, at every
100th iteration,

358
00:14:54,720 --> 00:14:56,790
you could make predictions
on that dev-set

359
00:14:56,790 --> 00:15:00,990
and store those predictions
for some kind of assessment.

360
00:15:00,990 --> 00:15:03,150
All the PyTorch
models for our course

361
00:15:03,150 --> 00:15:05,130
have an early stopping
parameter that

362
00:15:05,130 --> 00:15:07,800
will allow you to conduct
experiments in this way

363
00:15:07,800 --> 00:15:10,200
and keep hold of what
seemed to be the best

364
00:15:10,200 --> 00:15:12,240
model, performance-wise,
and then

365
00:15:12,240 --> 00:15:15,150
report that based on
the stopping criteria

366
00:15:15,150 --> 00:15:16,230
that you've set up.

367
00:15:16,230 --> 00:15:18,030
And with luck,
heuristically, that

368
00:15:18,030 --> 00:15:21,410
will give you the best
model in the fewest epochs.

369
00:15:21,410 --> 00:15:23,490
And the early_stopping
parameter has a bunch

370
00:15:23,490 --> 00:15:24,990
of different other
settings that you

371
00:15:24,990 --> 00:15:27,750
can use to control
exactly how it behaves,

372
00:15:27,750 --> 00:15:30,840
which might be important for
particular models structures

373
00:15:30,840 --> 00:15:34,000
or data sets.

374
00:15:34,000 --> 00:15:36,622
Here's a bit of our
motivation for early stopping.

375
00:15:36,622 --> 00:15:38,080
You might be
thinking, why not just

376
00:15:38,080 --> 00:15:41,350
let my model run to
convergence if I possibly can.

377
00:15:41,350 --> 00:15:45,640
In the context of these large,
very difficult optimization

378
00:15:45,640 --> 00:15:48,760
processes, that could lead
you really far astray, right?

379
00:15:48,760 --> 00:15:51,590
So here is a picture of
a deep learning model.

380
00:15:51,590 --> 00:15:53,710
And you can see its error
going down very quickly

381
00:15:53,710 --> 00:15:55,062
over many iterations.

382
00:15:55,062 --> 00:15:57,020
And it looks like you
might want to iterate out

383
00:15:57,020 --> 00:15:59,840
to even to 80
epochs of training.

384
00:15:59,840 --> 00:16:01,420
However, if you
look at performance

385
00:16:01,420 --> 00:16:04,210
on that held-out dev set you
see that this model actually

386
00:16:04,210 --> 00:16:07,360
very quickly reached
its peak of performance.

387
00:16:07,360 --> 00:16:08,890
And then all that
remaining training

388
00:16:08,890 --> 00:16:13,930
was just either wasting time
or eroding the performance

389
00:16:13,930 --> 00:16:15,790
that you saw early
on in the process.

390
00:16:15,790 --> 00:16:18,940
And this is exactly why since
this is our real goal here,

391
00:16:18,940 --> 00:16:22,930
you might want to do some
kind of dev-set testing

392
00:16:22,930 --> 00:16:26,030
with early stopping.

393
00:16:26,030 --> 00:16:28,370
The final thing I'd want to
say here is that all of this

394
00:16:28,370 --> 00:16:30,590
might lead us to get out
of the mode of assuming

395
00:16:30,590 --> 00:16:32,270
that we should always
be recording one

396
00:16:32,270 --> 00:16:34,320
number to summarize our models.

397
00:16:34,320 --> 00:16:36,530
We're dealing with
very powerful models.

398
00:16:36,530 --> 00:16:38,390
In the limit, they
might be able to learn

399
00:16:38,390 --> 00:16:39,830
very complicated things.

400
00:16:39,830 --> 00:16:42,920
And we might want to ask
different questions like how

401
00:16:42,920 --> 00:16:46,250
quickly can they learn, and how
effectively, and how robustly.

402
00:16:46,250 --> 00:16:48,710
And that might imply that
what we really want to do

403
00:16:48,710 --> 00:16:51,740
is not report summary
tables of statistics,

404
00:16:51,740 --> 00:16:55,130
but rather full learning curves
with confidence intervals.

405
00:16:55,130 --> 00:16:57,770
This is a picture from a paper
that I was involved with.

406
00:16:57,770 --> 00:16:59,390
And I think it's
illuminating to see

407
00:16:59,390 --> 00:17:02,840
a by category breakdown of
how the model is performing

408
00:17:02,840 --> 00:17:04,910
in addition to the
overall average.

409
00:17:04,910 --> 00:17:07,310
Because you can see that
while this red model

410
00:17:07,310 --> 00:17:11,569
is arguably much better than
the yellow and the gray overall.

411
00:17:11,569 --> 00:17:13,819
It's kind of hard to
distinguish it globally

412
00:17:13,819 --> 00:17:15,410
from this blue model.

413
00:17:15,410 --> 00:17:17,240
But for various of
the subcategories,

414
00:17:17,240 --> 00:17:18,838
you do see some differences.

415
00:17:18,838 --> 00:17:20,630
Whereas for others,
you do see that they're

416
00:17:20,630 --> 00:17:22,310
kind of indistinguishable.

417
00:17:22,310 --> 00:17:23,740
It's a very rich picture.

418
00:17:23,740 --> 00:17:26,730
You can also see that early on
for some of these categories

419
00:17:26,730 --> 00:17:28,700
some of these models are
really differentiated.

420
00:17:28,700 --> 00:17:30,680
They learn more efficiently.

421
00:17:30,680 --> 00:17:33,890
Whereas by the time you've
run out to 100,000 epochs,

422
00:17:33,890 --> 00:17:36,770
many of the model
distinctions have disappeared.

423
00:17:36,770 --> 00:17:39,350
That's the kind of rich picture
that is already giving us

424
00:17:39,350 --> 00:17:42,330
a sense for how different
values and different goals

425
00:17:42,330 --> 00:17:45,260
we have might guide
different choices about which

426
00:17:45,260 --> 00:17:48,908
model to use and different
choices about how to optimize.

427
00:17:48,908 --> 00:17:50,450
And I would just
love it if our field

428
00:17:50,450 --> 00:17:54,230
got into the habit of reporting
this very full picture, as

429
00:17:54,230 --> 00:17:58,420
opposed to reducing
everything to a single number.

430
00:17:58,420 --> 00:18:00,850
The final topic is the
role of random parameter

431
00:18:00,850 --> 00:18:01,780
initialization.

432
00:18:01,780 --> 00:18:04,390
This is kind of yet another
hyperparameter that's

433
00:18:04,390 --> 00:18:07,960
in the background that's much
more difficult to think about.

434
00:18:07,960 --> 00:18:10,780
Most deep learning models have
their parameters initialized

435
00:18:10,780 --> 00:18:13,150
randomly, or many
of those parameters

436
00:18:13,150 --> 00:18:15,170
are initialized randomly.

437
00:18:15,170 --> 00:18:18,640
This is clearly meaningful for
these non-convex optimization

438
00:18:18,640 --> 00:18:20,200
problems that we're posing.

439
00:18:20,200 --> 00:18:21,940
But even simple
models can also be

440
00:18:21,940 --> 00:18:24,310
impacted if you're dealing
with very small data sets

441
00:18:24,310 --> 00:18:27,480
with very large feature spaces.

442
00:18:27,480 --> 00:18:29,670
In this classic paper
here, these authors

443
00:18:29,670 --> 00:18:31,800
just observed that
different initializations

444
00:18:31,800 --> 00:18:33,600
for neural sequence
models that were doing

445
00:18:33,600 --> 00:18:37,830
named entity recognition led
to statistically significantly

446
00:18:37,830 --> 00:18:38,910
different results.

447
00:18:38,910 --> 00:18:42,210
That is one and the same model
with a different random seed

448
00:18:42,210 --> 00:18:45,030
was performing in ways that
looked significantly different

449
00:18:45,030 --> 00:18:46,470
on these data sets.

450
00:18:46,470 --> 00:18:48,630
And a number of recent
systems actually

451
00:18:48,630 --> 00:18:50,850
turned out to be
indistinguishable in terms

452
00:18:50,850 --> 00:18:54,030
of their raw performance
once this source of variation

453
00:18:54,030 --> 00:18:55,440
was taken into account.

454
00:18:55,440 --> 00:19:00,030
That's just a powerful example
of how much a random seed could

455
00:19:00,030 --> 00:19:04,140
shape final performance in the
context of models like this.

456
00:19:04,140 --> 00:19:06,160
Relatedly, at the other
end of the spectrum,

457
00:19:06,160 --> 00:19:09,040
you can see catastrophic
failure as a result

458
00:19:09,040 --> 00:19:11,010
of unlucky initialization.

459
00:19:11,010 --> 00:19:14,790
Some settings are great and
some can be miserable failures.

460
00:19:14,790 --> 00:19:17,460
We don't really know ahead
of time which will be which.

461
00:19:17,460 --> 00:19:20,250
And that means that we just have
to be really attentive to how

462
00:19:20,250 --> 00:19:24,280
we're initializing these systems
in a wide range of settings.

463
00:19:24,280 --> 00:19:26,700
And you'll notice that
in the evaluation methods

464
00:19:26,700 --> 00:19:30,100
notebook that I've distributed
as a companion to this lecture,

465
00:19:30,100 --> 00:19:32,670
I fit a simple feed-forward
network, a very small one,

466
00:19:32,670 --> 00:19:34,530
on the classic
XOR problem, which

467
00:19:34,530 --> 00:19:36,930
is one of the original
motivating problems

468
00:19:36,930 --> 00:19:39,390
for using deep
learning models at all.

469
00:19:39,390 --> 00:19:42,150
And what you see is that it
succeeds about 8 out of 10

470
00:19:42,150 --> 00:19:44,280
times, where the
only thing that we're

471
00:19:44,280 --> 00:19:47,310
changing across these
models is the way they

472
00:19:47,310 --> 00:19:48,720
are randomly initialized.

473
00:19:48,720 --> 00:19:50,820
And that again just
shows you that this

474
00:19:50,820 --> 00:19:54,660
can be powerfully shaping final
performance for our systems.

475
00:19:54,660 --> 00:19:56,340
And probably what
we need to do is

476
00:19:56,340 --> 00:19:59,730
be thinking about this as
yet another hyperparameter

477
00:19:59,730 --> 00:20:02,580
that we need to tune
and optimize along

478
00:20:02,580 --> 00:20:04,550
with all the rest.

479
00:20:04,550 --> 00:20:08,427


