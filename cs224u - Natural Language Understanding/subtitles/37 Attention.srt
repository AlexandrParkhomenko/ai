1
00:00:00,000 --> 00:00:04,158


2
00:00:04,158 --> 00:00:05,700
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,700 --> 00:00:08,000
This is part 5 in our series
on natural language inference.

4
00:00:08,000 --> 00:00:10,208
We're going to be talking
about attention mechanisms.

5
00:00:10,208 --> 00:00:12,830
Attention was an important
source of innovation in the NLI

6
00:00:12,830 --> 00:00:14,570
literature, and, of
course, it's only

7
00:00:14,570 --> 00:00:17,160
grown in prominence since then.

8
00:00:17,160 --> 00:00:18,960
Let's begin with
some guiding ideas.

9
00:00:18,960 --> 00:00:20,850
In the context of
the NLI problem,

10
00:00:20,850 --> 00:00:24,060
we might have an intuition that
we just need more connections

11
00:00:24,060 --> 00:00:26,580
for a lot of our architectures
between the premise

12
00:00:26,580 --> 00:00:28,110
and hypothesis, right?

13
00:00:28,110 --> 00:00:30,270
Possibly, in processing
the hypothesis,

14
00:00:30,270 --> 00:00:33,360
we just need the model to
have some reminders about what

15
00:00:33,360 --> 00:00:35,220
the premise actually contained.

16
00:00:35,220 --> 00:00:37,260
And whatever summary
representation we have

17
00:00:37,260 --> 00:00:38,910
of that premise
might just not be

18
00:00:38,910 --> 00:00:42,300
enough from the point of view
of processing the hypothesis

19
00:00:42,300 --> 00:00:46,230
and feeding the representation
into the classifier layer.

20
00:00:46,230 --> 00:00:50,160
Relatedly, there's a persistent
intuition in the NLI literature

21
00:00:50,160 --> 00:00:53,520
that it's useful to softly align
the premise and the hypothesis

22
00:00:53,520 --> 00:00:56,700
to find corresponding words
and phrases between those two

23
00:00:56,700 --> 00:00:57,480
texts.

24
00:00:57,480 --> 00:01:00,180
It can be difficult to do
that at a mechanical level,

25
00:01:00,180 --> 00:01:02,760
but attention
mechanisms might allow

26
00:01:02,760 --> 00:01:05,129
us, via our data-driven
learning process,

27
00:01:05,129 --> 00:01:08,220
to find soft connections in
the weights for these attention

28
00:01:08,220 --> 00:01:10,590
layers between the
premise and hypothesis

29
00:01:10,590 --> 00:01:12,900
and achieve some of the
effects that we would get

30
00:01:12,900 --> 00:01:16,430
from a real alignment process.

31
00:01:16,430 --> 00:01:18,170
So let's begin with
global attention.

32
00:01:18,170 --> 00:01:19,900
This is the simplest
attention mechanism

33
00:01:19,900 --> 00:01:21,720
that you see in
the NLI literature,

34
00:01:21,720 --> 00:01:23,058
but it's already quite powerful.

35
00:01:23,058 --> 00:01:25,600
And as you'll see, it has deep
connections with the attention

36
00:01:25,600 --> 00:01:28,160
mechanisms in the transformer.

37
00:01:28,160 --> 00:01:30,760
So to make this concrete, let's
start with a simple example.

38
00:01:30,760 --> 00:01:33,460
We have "every dog
danced" as our premise,

39
00:01:33,460 --> 00:01:35,830
"some poodle danced"
as our hypothesis,

40
00:01:35,830 --> 00:01:39,770
and they're fit together into
this chained RNN model for NLI.

41
00:01:39,770 --> 00:01:41,200
Now, standardly,
what we would do

42
00:01:41,200 --> 00:01:44,080
is take this final
representation, hC,

43
00:01:44,080 --> 00:01:46,840
as the summary representation
for the entire sequence

44
00:01:46,840 --> 00:01:49,840
and feed that directly
into the classifier.

45
00:01:49,840 --> 00:01:52,690
What we're going to do when
we add attention mechanisms is

46
00:01:52,690 --> 00:01:56,080
instead offer some connections
back from this state

47
00:01:56,080 --> 00:01:57,920
into the premise states.

48
00:01:57,920 --> 00:01:59,380
The way that
process gets started

49
00:01:59,380 --> 00:02:00,922
is via a series of dot products.

50
00:02:00,922 --> 00:02:02,630
So we're going to take
our target vector,

51
00:02:02,630 --> 00:02:05,770
hC, and take its dot
product with each one

52
00:02:05,770 --> 00:02:08,889
of the hidden representations
corresponding to tokens

53
00:02:08,889 --> 00:02:10,210
in the premise.

54
00:02:10,210 --> 00:02:12,910
And that gives us this vector
of unnormalized scores,

55
00:02:12,910 --> 00:02:14,570
just the dot products.

56
00:02:14,570 --> 00:02:16,210
And it's common,
then, to softmax

57
00:02:16,210 --> 00:02:19,970
normalize those scores into
our attention weights, alpha.

58
00:02:19,970 --> 00:02:22,557
What we do with alpha is then
create our context vector,

59
00:02:22,557 --> 00:02:24,140
and the way that
happens is that we're

60
00:02:24,140 --> 00:02:27,580
going to get a weighted view
of all those premise states.

61
00:02:27,580 --> 00:02:29,660
Each one-- h1, h2, and h3--

62
00:02:29,660 --> 00:02:32,240
is weighted by its corresponding
attention weight, which

63
00:02:32,240 --> 00:02:34,970
is capturing its kind
of unnormalized notion

64
00:02:34,970 --> 00:02:38,143
of similarity with
our target vector hC.

65
00:02:38,143 --> 00:02:40,310
And then to get a fixed
dimensional version of that,

66
00:02:40,310 --> 00:02:42,830
we take the mean-- or
it could be the sum--

67
00:02:42,830 --> 00:02:45,970
of all of those weighted
views of the premise.

68
00:02:45,970 --> 00:02:48,280
Next, we get our attention
combination layer,

69
00:02:48,280 --> 00:02:50,160
and there are various
ways to do this.

70
00:02:50,160 --> 00:02:52,870
One simple one would be to
simply concatenate our context

71
00:02:52,870 --> 00:02:55,450
vector with our
original context--

72
00:02:55,450 --> 00:02:59,080
target vector hC and feed those
through a kind of dense layer

73
00:02:59,080 --> 00:03:00,868
of learned parameters.

74
00:03:00,868 --> 00:03:02,410
Another perspective,
kind of similar,

75
00:03:02,410 --> 00:03:05,410
is to give the context
vector and our target vector

76
00:03:05,410 --> 00:03:07,720
hC, each one their
own weights, and have

77
00:03:07,720 --> 00:03:09,590
an additive combination
of those two,

78
00:03:09,590 --> 00:03:13,370
and, again, feed it through
some kind of non-linearity.

79
00:03:13,370 --> 00:03:15,620
And you could think of various
other designs for this.

80
00:03:15,620 --> 00:03:19,330
And that gives us this
attention combination, h tilde.

81
00:03:19,330 --> 00:03:21,610
And then, finally,
the classifier layer

82
00:03:21,610 --> 00:03:24,820
is a simple dense layer, just
as before, except instead

83
00:03:24,820 --> 00:03:28,990
of using just hC, we now use
this h tilde representation,

84
00:03:28,990 --> 00:03:33,700
which incorporates both hC and
that kind of weighted mixture

85
00:03:33,700 --> 00:03:36,980
of premise states.

86
00:03:36,980 --> 00:03:38,660
It might be useful
to go through this

87
00:03:38,660 --> 00:03:40,783
with some specific
numerical values here.

88
00:03:40,783 --> 00:03:42,200
So what I've done
is, just imagine

89
00:03:42,200 --> 00:03:44,510
that we have two-dimensional
representations for all

90
00:03:44,510 --> 00:03:45,620
of these vectors.

91
00:03:45,620 --> 00:03:48,440
And you can see, what I've done
here is kind of ensure that,

92
00:03:48,440 --> 00:03:52,520
proportionally, "every" is a lot
like this final representation

93
00:03:52,520 --> 00:03:53,590
here.

94
00:03:53,590 --> 00:03:55,757
And then that kind of
similarity drops off

95
00:03:55,757 --> 00:03:57,340
as we move through
the premise states,

96
00:03:57,340 --> 00:04:00,150
and you'll see what happens when
we take the dot products here.

97
00:04:00,150 --> 00:04:02,750
So the first step gives us
the unnormalized scores,

98
00:04:02,750 --> 00:04:05,930
and you can see that the highest
unnormalized similarity is

99
00:04:05,930 --> 00:04:08,660
with the first token, followed
by the second, and then

100
00:04:08,660 --> 00:04:09,440
the third.

101
00:04:09,440 --> 00:04:12,650
The softmax normalization step
kind of just flattens out those

102
00:04:12,650 --> 00:04:14,240
dot products a
little bit, but we

103
00:04:14,240 --> 00:04:18,902
get the same proportional
ranking with respect to hC.

104
00:04:18,902 --> 00:04:20,839
Here's that context
vector, and you

105
00:04:20,839 --> 00:04:23,960
can see it's just a mean of
the weighted values of all

106
00:04:23,960 --> 00:04:25,070
of these vectors.

107
00:04:25,070 --> 00:04:26,540
That gives us k.

108
00:04:26,540 --> 00:04:29,708
And that k is then fed into this
attention combination layer.

109
00:04:29,708 --> 00:04:31,250
And you can see, in
orange here, this

110
00:04:31,250 --> 00:04:33,770
is the context vector,
two dimensions.

111
00:04:33,770 --> 00:04:36,560
Down here, we have hC,
just faithfully repeated.

112
00:04:36,560 --> 00:04:39,320
And then this
matrix of weights Wk

113
00:04:39,320 --> 00:04:42,830
is going to give us, in the
end, after this non-linearity, h

114
00:04:42,830 --> 00:04:44,120
tilde.

115
00:04:44,120 --> 00:04:46,860
And then the classifier
is as before.

116
00:04:46,860 --> 00:04:48,350
So that's a simple
worked example

117
00:04:48,350 --> 00:04:50,180
of how these attention
mechanisms work.

118
00:04:50,180 --> 00:04:54,230
And the idea is that we are
kind of fundamentally weighting

119
00:04:54,230 --> 00:04:58,310
this target representation
hC by its similarity

120
00:04:58,310 --> 00:05:00,650
with the previous
premise states.

121
00:05:00,650 --> 00:05:02,810
But all of them are mixed
in, and the influence

122
00:05:02,810 --> 00:05:07,042
is kind of proportional to
that unnormalized similarity.

123
00:05:07,042 --> 00:05:09,250
There are other scoring
functions that you could use,

124
00:05:09,250 --> 00:05:09,750
of course.

125
00:05:09,750 --> 00:05:12,080
We've just done a simple
dot product up here,

126
00:05:12,080 --> 00:05:15,370
but you can also imagine having
learned parameters in there

127
00:05:15,370 --> 00:05:17,450
or doing concatenation of
the learned parameters.

128
00:05:17,450 --> 00:05:19,300
This does a kind
of bilinear form,

129
00:05:19,300 --> 00:05:22,270
and this is just a concatenation
of those two states fed

130
00:05:22,270 --> 00:05:23,590
through these learned weights.

131
00:05:23,590 --> 00:05:25,450
And once you see this
kind of design space,

132
00:05:25,450 --> 00:05:27,550
you can imagine there
are a lot of other ways

133
00:05:27,550 --> 00:05:29,380
in which you could
mix in parameters

134
00:05:29,380 --> 00:05:34,220
and have different views of
this global attention mechanism.

135
00:05:34,220 --> 00:05:35,900
We could go one
step further here.

136
00:05:35,900 --> 00:05:37,220
That was global attention.

137
00:05:37,220 --> 00:05:38,960
In word-by-word
attention, we're going

138
00:05:38,960 --> 00:05:41,360
to have a lot more learned
parameters and a lot

139
00:05:41,360 --> 00:05:44,390
more connections between
the hypothesis back

140
00:05:44,390 --> 00:05:45,920
into the premise.

141
00:05:45,920 --> 00:05:47,510
So to make this
kind of tractable,

142
00:05:47,510 --> 00:05:50,900
I've picked one pretty simple
view of how this could work.

143
00:05:50,900 --> 00:05:52,970
And the way we should
track these computations

144
00:05:52,970 --> 00:05:56,405
is focus on this vector
B here, because we're

145
00:05:56,405 --> 00:05:57,530
going to move through time.

146
00:05:57,530 --> 00:06:00,770
But let's imagine that we've
already processed the A state,

147
00:06:00,770 --> 00:06:03,470
and we will subsequently
process the C state.

148
00:06:03,470 --> 00:06:05,390
So we're focused on B.

149
00:06:05,390 --> 00:06:07,250
And the way we establish
these connections

150
00:06:07,250 --> 00:06:09,227
is by taking the
previous context

151
00:06:09,227 --> 00:06:10,310
vector that we've created.

152
00:06:10,310 --> 00:06:12,200
That's kA here.

153
00:06:12,200 --> 00:06:15,170
We're going to multiply that by
repeated copies of the B state,

154
00:06:15,170 --> 00:06:18,200
and that's simply so that we
get the same dimensionality

155
00:06:18,200 --> 00:06:20,030
as we have in the
premise over here,

156
00:06:20,030 --> 00:06:22,640
where I've simply copied
over into a matrix all three

157
00:06:22,640 --> 00:06:24,200
of those states.

158
00:06:24,200 --> 00:06:26,930
And we have a matrix of
learned parameters here

159
00:06:26,930 --> 00:06:28,940
and an additive
combination of the two,

160
00:06:28,940 --> 00:06:30,560
followed by a non-linearity.

161
00:06:30,560 --> 00:06:33,440
That's going to give us this M
here, which kind of corresponds

162
00:06:33,440 --> 00:06:36,080
to the attention weights in
the previous global attention

163
00:06:36,080 --> 00:06:37,910
mechanisms.

164
00:06:37,910 --> 00:06:39,603
We're going to softmax
normalize those,

165
00:06:39,603 --> 00:06:41,270
and that literally
gives us the weights.

166
00:06:41,270 --> 00:06:43,645
And you can see that there
are some additional parameters

167
00:06:43,645 --> 00:06:47,280
in here to create the
right dimensionalities.

168
00:06:47,280 --> 00:06:49,550
And then, finally, we
have the context at B.

169
00:06:49,550 --> 00:06:52,050
So that's going to be a repeated
view of all these premises,

170
00:06:52,050 --> 00:06:54,510
weighted by our context
vector, as before,

171
00:06:54,510 --> 00:06:58,500
and then fed through some
additional parameters Wa here.

172
00:06:58,500 --> 00:07:01,180
And that gives us, as you
can see here, the context

173
00:07:01,180 --> 00:07:03,870
representation for the state B.

174
00:07:03,870 --> 00:07:05,730
When we move to
state C, of course,

175
00:07:05,730 --> 00:07:08,160
that will be used in
the place of A here.

176
00:07:08,160 --> 00:07:10,380
And C will go in for
all these purple values,

177
00:07:10,380 --> 00:07:12,563
and the computation
will proceed as before.

178
00:07:12,563 --> 00:07:15,230
And in that way, because we have
all of these additional learned

179
00:07:15,230 --> 00:07:17,280
parameters, we can
meaningfully move

180
00:07:17,280 --> 00:07:20,250
through the entire sequence,
updating our parameters

181
00:07:20,250 --> 00:07:23,370
and learning connections from
each hypothesis token back

182
00:07:23,370 --> 00:07:24,510
into the premise.

183
00:07:24,510 --> 00:07:27,000
So it's much more powerful
than the previous view, where

184
00:07:27,000 --> 00:07:30,420
we had relatively few learned
parameters in our attention

185
00:07:30,420 --> 00:07:33,210
mechanisms, and, therefore, we
could only really meaningfully

186
00:07:33,210 --> 00:07:35,550
connect that from the
state that we're going

187
00:07:35,550 --> 00:07:37,080
to feed into the classifier.

188
00:07:37,080 --> 00:07:40,500
So this is much more
expressive, right?

189
00:07:40,500 --> 00:07:43,570
And then once we have done the
entire sequence processing,

190
00:07:43,570 --> 00:07:46,005
finally, we get the
representation for C

191
00:07:46,005 --> 00:07:47,910
here, as fed through
these mechanisms,

192
00:07:47,910 --> 00:07:50,280
and that becomes the
input to the classifier

193
00:07:50,280 --> 00:07:53,440
that we ultimately use.

194
00:07:53,440 --> 00:07:55,768
The connection with the
transformer should be apparent.

195
00:07:55,768 --> 00:07:58,060
This is going to return us
back to the global attention

196
00:07:58,060 --> 00:07:58,770
mechanism.

197
00:07:58,770 --> 00:08:00,490
Recall that for
the transformer, we

198
00:08:00,490 --> 00:08:02,990
have these sequences
of tokens with

199
00:08:02,990 --> 00:08:04,600
their positional encodings.

200
00:08:04,600 --> 00:08:06,410
That gives us an embedding here.

201
00:08:06,410 --> 00:08:08,350
And at that point,
we establish a lot

202
00:08:08,350 --> 00:08:10,017
of dot product connections.

203
00:08:10,017 --> 00:08:12,100
And I showed you in the
lecture on the transformer

204
00:08:12,100 --> 00:08:15,640
that the mechanisms here are
identical to the mechanisms

205
00:08:15,640 --> 00:08:17,420
that we used for dot
product attention.

206
00:08:17,420 --> 00:08:19,730
It's just that in the
context of the transformer,

207
00:08:19,730 --> 00:08:21,990
we do it from every state
to every other state.

208
00:08:21,990 --> 00:08:25,568


209
00:08:25,568 --> 00:08:27,110
And then, of course,
the computations

210
00:08:27,110 --> 00:08:30,290
proceed through subsequent
steps in the transformer layer

211
00:08:30,290 --> 00:08:32,510
and on through multiple
transformer layers,

212
00:08:32,510 --> 00:08:34,892
potentially.

213
00:08:34,892 --> 00:08:36,600
And there are some
other variants, right?

214
00:08:36,600 --> 00:08:38,909
This is just the beginning
of a very large design

215
00:08:38,909 --> 00:08:40,340
space for attention mechanisms.

216
00:08:40,340 --> 00:08:41,700
Let me just mention a few.

217
00:08:41,700 --> 00:08:43,135
We could have local attention.

218
00:08:43,135 --> 00:08:44,760
This was actually an
early contribution

219
00:08:44,760 --> 00:08:46,990
in the context of
machine translation.

220
00:08:46,990 --> 00:08:49,560
And this would build connections
between selected points

221
00:08:49,560 --> 00:08:51,630
and the premise and
hypothesis, based

222
00:08:51,630 --> 00:08:54,480
on some possibly a
priori notion we have

223
00:08:54,480 --> 00:08:58,030
of which things are likely to
be important for our problem.

224
00:08:58,030 --> 00:08:59,680
Word-by-word attention,
as I've said,

225
00:08:59,680 --> 00:09:02,920
can be set up in many ways, with
many more learned parameters.

226
00:09:02,920 --> 00:09:04,810
And the classic paper
is the one that I'm

227
00:09:04,810 --> 00:09:07,060
recommending for reading
for this unit, Rocktaschel

228
00:09:07,060 --> 00:09:11,110
et al., where they do a
really pioneering view of this

229
00:09:11,110 --> 00:09:14,140
in using even more complex
attention mechanisms than I

230
00:09:14,140 --> 00:09:16,180
presented under
word-by-word attention

231
00:09:16,180 --> 00:09:20,340
but following a lot of the
same intuitions, I would say.

232
00:09:20,340 --> 00:09:22,230
The attention
representation at a time t

233
00:09:22,230 --> 00:09:24,180
could be appended to the
hidden representation

234
00:09:24,180 --> 00:09:25,335
at time t plus 1.

235
00:09:25,335 --> 00:09:27,930
This would give us another
way of moving sequentially

236
00:09:27,930 --> 00:09:30,450
through the sequence,
having meaningful attention

237
00:09:30,450 --> 00:09:32,462
at each one of those
points, as opposed

238
00:09:32,462 --> 00:09:34,170
to the global attention,
which would just

239
00:09:34,170 --> 00:09:36,172
be for that final state.

240
00:09:36,172 --> 00:09:37,880
And then there are
other connections even

241
00:09:37,880 --> 00:09:38,840
further afield.

242
00:09:38,840 --> 00:09:40,550
For example, memory
networks can be

243
00:09:40,550 --> 00:09:42,320
used to address similar
issues, and they

244
00:09:42,320 --> 00:09:45,980
have similar intuitions behind
them as attention mechanisms

245
00:09:45,980 --> 00:09:47,870
as applied to the NLI problem.

246
00:09:47,870 --> 00:09:50,300
And that's kind of more
explicitly drawing on this idea

247
00:09:50,300 --> 00:09:53,030
that we might, in late
states in processing,

248
00:09:53,030 --> 00:09:54,980
need a bit of a
reminder about what

249
00:09:54,980 --> 00:09:58,420
was in the previous
context that we processed.

250
00:09:58,420 --> 00:10:02,000


