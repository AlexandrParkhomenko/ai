1
00:00:05,839 --> 00:00:08,320
привет всем, добро пожаловать в часть 5 нашей

2
00:00:08,320 --> 00:00:10,559
серии о nlu и ir.

3
00:00:10,559 --> 00:00:12,480
Этот скринкаст будет третьим среди

4
00:00:12,480 --> 00:00:17,199
трех наших видеороликов о нейронном ir.

5
00:00:17,199 --> 00:00:19,279
В предыдущем скринкасте мы обсуждали

6
00:00:19,279 --> 00:00:21,359
изучение весов терминов как парадигму

7
00:00:21,359 --> 00:00:23,359
построения нейронных ir-моделей, которые одновременно

8
00:00:23,359 --> 00:00:25,359
эффективны и действенны.

9
00:00:25,359 --> 00:00:27,279
мы упомянули две такие модели из их

10
00:00:27,279 --> 00:00:31,920
литературы: deep ct и dr quiri,

11
00:00:31,920 --> 00:00:34,079
обе из которых, несмотря на то, что они превосходят bm

12
00:00:34,079 --> 00:00:38,160
25 в mrr, все же оставляют очень большой запас

13
00:00:38,160 --> 00:00:41,520
по качеству, которое мы видим в bert,

14
00:00:41,520 --> 00:00:43,840
мы спросили себя, можем ли мы достичь высокого

15
00:00:43,840 --> 00:00:46,399
mrr и низких вычислительных затрат.  в то

16
00:00:46,399 --> 00:00:48,480
же время,

17
00:00:48,480 --> 00:00:50,480
можем ли мы сделать лучше,

18
00:00:50,480 --> 00:00:52,640
чтобы ответить на этот вопрос, давайте начнем

19
00:00:52,640 --> 00:00:54,879
исследовать более выразительные парадигмы для

20
00:00:54,879 --> 00:00:57,520
эффективной нейронной сети?

21
00:00:57,520 --> 00:00:59,840
Следующая парадигма здесь - парадигма

22
00:00:59,840 --> 00:01:03,039


23
00:01:03,280 --> 00:01:04,640
подобия представления в парадигме подобия представления, которую

24
00:01:04,640 --> 00:01:06,640
мы начинаем с токенизации

25
00:01:06,640 --> 00:01:08,560
запроса и документа,

26
00:01:08,560 --> 00:01:11,040
и мы подаем каждый из  их независимо друг от друга с

27
00:01:11,040 --> 00:01:13,119
помощью кодировщика, такого как рождение,

28
00:01:13,119 --> 00:01:14,479
например,

29
00:01:14,479 --> 00:01:16,560
этот кодировщик затем используется для создания

30
00:01:16,560 --> 00:01:18,960
одного вектора, представляющего  для

31
00:01:18,960 --> 00:01:22,560
запроса и для документа отдельно,

32
00:01:22,560 --> 00:01:24,240
поэтому для рождения мы могли бы взять это

33
00:01:24,240 --> 00:01:26,640
через токен класса, например, и взять

34
00:01:26,640 --> 00:01:28,799
выходные вложения, или мы могли бы усреднить

35
00:01:28,799 --> 00:01:32,479
все выходные данные конечного уровня,

36
00:01:32,560 --> 00:01:34,320
когда у нас есть те, которые мы, наконец, вычисляем

37
00:01:34,320 --> 00:01:36,400
показатель релевантности этого документа для

38
00:01:36,400 --> 00:01:39,040
наш запрос как одноточечное произведение

39
00:01:39,040 --> 00:01:41,840
между двумя векторами.

40
00:01:44,720 --> 00:01:46,640
Эта парадигма очень эффективна для

41
00:01:46,640 --> 00:01:48,240
поиска.

42
00:01:48,240 --> 00:01:50,320
Сначала каждый документ может быть представлен

43
00:01:50,320 --> 00:01:53,040
в виде вектора в автономном режиме,

44
00:01:53,040 --> 00:01:54,960
и это предварительно вычисленное представление может

45
00:01:54,960 --> 00:01:57,280
быть сохранено на диске, прежде чем мы даже

46
00:01:57,280 --> 00:02:00,159
начнем проводить поиск,

47
00:02:00,159 --> 00:02:02,240
более того, вычисление подобия

48
00:02:02,240 --> 00:02:04,159
между  запрос и документ здесь

49
00:02:04,159 --> 00:02:06,479
очень дешевы и очень эффективны, так

50
00:02:06,479 --> 00:02:08,160
как это просто скалярное произведение

51
00:02:08,160 --> 00:02:11,119
между двумя

52
00:02:11,920 --> 00:02:13,920
векторами очень большое количество ir-моделей являются

53
00:02:13,920 --> 00:02:16,319
моделями сходства представлений,

54
00:02:16,319 --> 00:02:18,879
многие из которых на самом деле предшествуют рождению,

55
00:02:18,879 --> 00:02:22,800
такие как double sm и snrm, но последний

56
00:02:22,800 --> 00:02:24,400
год и  В половине случаев мы видели множество

57
00:02:24,400 --> 00:02:26,640
моделей подобия, основанных на рождении, для тестов ir,

58
00:02:26,640 --> 00:02:27,680


59
00:02:27,680 --> 00:02:31,519
включая рождение косатки dpr d рождения

60
00:02:31,519 --> 00:02:34,480
и  Среди прочего,

61
00:02:34,480 --> 00:02:36,480
многие из этих моделей

62
00:02:36,480 --> 00:02:38,239
фактически были предложены одновременно

63
00:02:38,239 --> 00:02:39,280
друг с другом,

64
00:02:39,280 --> 00:02:40,879
и их основные различия заключаются в

65
00:02:40,879 --> 00:02:43,040
конкретных задачах, на которые нацелена каждая из них, и

66
00:02:43,040 --> 00:02:44,640
подходе к надзору, который каждая из них

67
00:02:44,640 --> 00:02:46,319
предлагает,

68
00:02:46,319 --> 00:02:48,319
поэтому давайте углубимся в

69
00:02:48,319 --> 00:02:50,000
репрезентативную и одну из более ранних

70
00:02:50,000 --> 00:02:51,519
и самых популярных моделей.

71
00:02:51,519 --> 00:02:53,840
моделей среди них это

72
00:02:53,840 --> 00:02:56,959
ретривер плотного прохода или dpr by carboquinital,

73
00:02:56,959 --> 00:02:59,360
который появился на emlp всего несколько

74
00:02:59,360 --> 00:03:01,120
месяцев назад

75
00:03:01,120 --> 00:03:04,400
dpr кодирует каждое сообщение или документ

76
00:03:04,400 --> 00:03:07,680
как 768-мерный вектор и аналогично

77
00:03:07,680 --> 00:03:09,840
для каждого запроса

78
00:03:09,840 --> 00:03:12,239
во время обучения dpr выдает

79
00:03:12,239 --> 00:03:14,560
оценку сходства между запросом  и

80
00:03:14,560 --> 00:03:16,400
положительный пассаж, так что это соответствующий

81
00:03:16,400 --> 00:03:18,560
пассаж, который мы хотели получить, а также

82
00:03:18,560 --> 00:03:21,040
между запросом и несколькими отрицательными значениями,

83
00:03:21,040 --> 00:03:23,120
некоторые из которых взяты из

84
00:03:23,120 --> 00:03:25,200
топ-100 bm25, а другие являются отрицательными в пакете,

85
00:03:25,200 --> 00:03:27,280
которые на самом деле являются положительными,

86
00:03:27,280 --> 00:03:28,799
но для других запросов в  тот же

87
00:03:28,799 --> 00:03:31,200
тренировочный пакет, как

88
00:03:31,200 --> 00:03:33,440
только dpr получает все эти оценки во время

89
00:03:33,440 --> 00:03:35,360
обучения, он оптимизирует

90
00:03:35,360 --> 00:03:37,840
потерю классификации, а именно n-way

91
00:03:37,840 --> 00:03:39,599
потеря классификации с мягкой

92
00:03:39,599 --> 00:03:41,840
кросс-энтропийной потерей с мягким максимумом

93
00:03:41,840 --> 00:03:44,000
по оценкам одного положительного и

94
00:03:44,000 --> 00:03:44,840
всех этих

95
00:03:44,840 --> 00:03:47,280
отрицательных с целью

96
00:03:47,280 --> 00:03:50,560
выбора положительного прохода, конечно,

97
00:03:50,959 --> 00:03:53,200
dpr не тестировался на

98
00:03:53,200 --> 00:03:55,599
наборе данных ms marco первоначальными авторами,

99
00:03:55,599 --> 00:03:58,720
но последующая работа Чанга

100
00:03:58,720 --> 00:04:02,080
Guitar тестирует dpr-подобный ретривер на

101
00:04:02,080 --> 00:04:05,040
MS Marco и достигает 31 MRR, они также

102
00:04:05,040 --> 00:04:07,599
предлагают более сложные

103
00:04:07,599 --> 00:04:09,599
подходы для наблюдения, которые могут

104
00:04:09,599 --> 00:04:12,159
увеличить этот MRR на пару пунктов,

105
00:04:12,159 --> 00:04:14,159
поэтому оба они демонстрируют

106
00:04:14,159 --> 00:04:16,560
значительный прогресс по сравнению с изученными

107
00:04:16,560 --> 00:04:18,478
моделями веса термина, которые мы рассматривали.

108
00:04:18,478 --> 00:04:21,918
раньше, как глубокий кт или доктор королева,

109
00:04:21,918 --> 00:04:23,759
но они по-прежнему значительно

110
00:04:23,759 --> 00:04:27,520
отстают от рождений, гораздо более высокая эффективность,

111
00:04:27,520 --> 00:04:30,639
так почему же,

112
00:04:30,639 --> 00:04:32,000
как выясняется,

113
00:04:32,000 --> 00:04:34,320
модели подобия представления страдают от двух основных

114
00:04:34,320 --> 00:04:38,400
недостатков, когда дело доходит до их задач.

115
00:04:38,400 --> 00:04:40,320


116
00:04:40,320 --> 00:04:42,240


117
00:04:42,240 --> 00:04:44,960
и каждый

118
00:04:44,960 --> 00:04:47,600
документ в один довольно низкомерный

119
00:04:47,600 --> 00:04:49,600
вектор, во-

120
00:04:49,600 --> 00:04:51,440
вторых, это их недостаток мелкозернистости.  Во

121
00:04:51,440 --> 00:04:53,600
время сопоставления

122
00:04:53,600 --> 00:04:55,280
модели сходства представления

123
00:04:55,280 --> 00:04:57,600
оценивают релевантность как одно скалярное произведение

124
00:04:57,600 --> 00:05:00,160
между двумя векторами, и поскольку они теряют

125
00:05:00,160 --> 00:05:02,000
взаимодействия на уровне терминов между

126
00:05:02,000 --> 00:05:04,080
терминами запроса и терминами документа, которые

127
00:05:04,080 --> 00:05:05,840
мы имели в моделях взаимодействия документа запроса,

128
00:05:05,840 --> 00:05:08,800
таких как рождение, на самом деле даже простые

129
00:05:08,800 --> 00:05:11,440
модели взвешивания терминов, такие как  bm25 или deep

130
00:05:11,440 --> 00:05:13,120
ct имели

131
00:05:13,120 --> 00:05:15,199
по замыслу какой-то элемент сопоставления на уровне терминов,

132
00:05:15,199 --> 00:05:18,880
который мы здесь теряем,

133
00:05:19,280 --> 00:05:21,440
поэтому наш следующий естественный вопрос

134
00:05:21,440 --> 00:05:23,919
заключается в том, можем ли мы получить эти

135
00:05:23,919 --> 00:05:26,240
преимущества в эффективности предварительных вычислений,

136
00:05:26,240 --> 00:05:27,520
которые мы получаем от моделей подобия представлений,

137
00:05:27,520 --> 00:05:29,039


138
00:05:29,039 --> 00:05:31,120
сохраняя при этом детализированные

139
00:05:31,120 --> 00:05:33,280
взаимодействия на уровне терминов.  которые мы

140
00:05:33,280 --> 00:05:36,000
использовали раньше с моделью, такой как рождение или

141
00:05:36,000 --> 00:05:39,240
глубокое КТ,

142
00:05:42,000 --> 00:05:44,240
для ответа на этот вопрос,

143
00:05:44,240 --> 00:05:46,320
я думаю, что это помогает проанализировать нейронные

144
00:05:46,320 --> 00:05:49,840
парадигмы, которые мы видели до сих пор,

145
00:05:50,400 --> 00:05:53,600
в левой части мы рассмотрели

146
00:05:53,600 --> 00:05:56,080
изученную парадигму весов терминов, которую

147
00:05:56,080 --> 00:05:57,840
предложили эти модели.  независимая

148
00:05:57,840 --> 00:05:59,600
независимая кодировка запросов и

149
00:05:59,600 --> 00:06:01,840
документов, которая отлично подходила для эффективности,

150
00:06:01,840 --> 00:06:03,680
но  Они заставляли нас работать с запросом из

151
00:06:03,680 --> 00:06:06,800
набора слов, который терял весь контекст

152
00:06:06,800 --> 00:06:08,880
и, таким образом, не был столь конкурентоспособен, как мы

153
00:06:08,880 --> 00:06:10,560
хотели,

154
00:06:10,560 --> 00:06:12,400
мы затем исследовали модели подобия представлений,

155
00:06:12,400 --> 00:06:14,800
которые также позволили

156
00:06:14,800 --> 00:06:16,560
нам вычислять независимые кодировки

157
00:06:16,560 --> 00:06:18,400
запросов и документов, что опять-таки было

158
00:06:18,400 --> 00:06:21,840
действительно полезно для эффективности,

159
00:06:22,080 --> 00:06:23,919
но на этот раз мы были вынуждены работать

160
00:06:23,919 --> 00:06:26,160
с одиночными векторными представлениями, и

161
00:06:26,160 --> 00:06:28,479
мы потеряли наши детализированные взаимодействия на уровне терминов,

162
00:06:28,479 --> 00:06:30,080


163
00:06:30,080 --> 00:06:32,160
которые мы интуитивно считаем очень

164
00:06:32,160 --> 00:06:36,960
полезными для сопоставления в своих задачах

165
00:06:37,440 --> 00:06:39,360
с правой стороны.

166
00:06:39,360 --> 00:06:41,520


167
00:06:41,520 --> 00:06:43,440
модели взаимодействия с запросом документа, такие как стандартные классификаторы рождения,

168
00:06:43,440 --> 00:06:45,280


169
00:06:45,280 --> 00:06:48,400
они предлагали очень высокую точность,

170
00:06:48,400 --> 00:06:50,720
но были чрезвычайно дороги в использовании,

171
00:06:50,720 --> 00:06:52,960
потому что все вычисления для одного

172
00:06:52,960 --> 00:06:55,120
документа зависели как от запроса, так и

173
00:06:55,120 --> 00:06:57,759
от документа, мы просто не могли заранее выполнить какие

174
00:06:57,759 --> 00:07:00,400
-либо предварительные вычисления в этом случае в автономном режиме.

175
00:07:00,400 --> 00:07:02,720


176
00:07:02,720 --> 00:07:05,039
так можем ли мы как-то объединить

177
00:07:05,039 --> 00:07:08,880
преимущества всех этих трех парадигм сразу,

178
00:07:08,880 --> 00:07:10,800
прежде чем мы ответим на этот квест  На самом деле есть еще

179
00:07:10,800 --> 00:07:13,039
одна последняя функция,

180
00:07:13,039 --> 00:07:15,599
одна последняя возможность первых двух

181
00:07:15,599 --> 00:07:17,840
парадигм, которую мы должны обсудить,

182
00:07:17,840 --> 00:07:20,000
поэтому запрашивайте модели взаимодействия документов,

183
00:07:20,000 --> 00:07:22,240
которые довольно дороги, они вынудили

184
00:07:22,240 --> 00:07:24,479
нас использовать

185
00:07:24,479 --> 00:07:26,720
конвейер переранжирования. Это конвейер, в котором мы переоценили

186
00:07:26,720 --> 00:07:28,560
тысячу лучших документов, которые мы  уже

187
00:07:28,560 --> 00:07:31,840
полученный bm25,

188
00:07:32,880 --> 00:07:35,199
иногда это нормально, но во многих случаях

189
00:07:35,199 --> 00:07:36,880
это может быть проблемой,

190
00:07:36,880 --> 00:07:39,680
потому что он связывает наш отзыв с

191
00:07:39,680 --> 00:07:42,639
отзывом bm25, который в конечном итоге является моделью,

192
00:07:42,639 --> 00:07:44,879
основанной на поиске терминов, которые

193
00:07:44,879 --> 00:07:47,440
точно совпадают в запросах и документах,

194
00:07:47,440 --> 00:07:48,879
и поэтому он может быть довольно ограничительным.  во

195
00:07:48,879 --> 00:07:50,960
многих случаях,

196
00:07:50,960 --> 00:07:52,400
когда отзыв является важным

197
00:07:52,400 --> 00:07:54,160
соображением,

198
00:07:54,160 --> 00:07:56,240
мы часто хотим, чтобы наша нейронная модель, которую мы

199
00:07:56,240 --> 00:07:59,039
обучили, выполняла сквозной

200
00:07:59,039 --> 00:08:02,080
поиск, то есть быстрый поиск по

201
00:08:02,080 --> 00:08:03,840
всем документам в нашей коллекции

202
00:08:03,840 --> 00:08:08,160
напрямую без какого-либо ранжирования

203
00:08:08,160 --> 00:08:09,759


204
00:08:09,759 --> 00:08:12,000
.  Модели сходства представлений,

205
00:08:12,000 --> 00:08:14,240
которые мы рассматривали до сих пор, смягчают это

206
00:08:14,240 --> 00:08:16,560
ограничение, и это большое преимущество

207
00:08:16,560 --> 00:08:17,680
для них,

208
00:08:17,680 --> 00:08:19,680
так как они специфичны.  Когда мы изучаем веса терминов,

209
00:08:19,680 --> 00:08:20,960


210
00:08:20,960 --> 00:08:22,240
мы можем сохранять эти веса в

211
00:08:22,240 --> 00:08:25,599
инвертированном индексе, как и в случае с bm25,

212
00:08:25,599 --> 00:08:27,680
и это позволяет нам

213
00:08:27,680 --> 00:08:30,000
получать быстрый поиск,

214
00:08:30,000 --> 00:08:32,159
когда мы изучаем представления векторов.

215
00:08:32,159 --> 00:08:34,399
Также оказывается, что мы можем индексировать эти

216
00:08:34,399 --> 00:08:37,039
векторы, используя библиотеки для быстрого

217
00:08:37,039 --> 00:08:42,000
поиска сходства векторов, такие как  face fai double s

218
00:08:42,000 --> 00:08:44,240
это основано на эффективных структурах данных,

219
00:08:44,240 --> 00:08:46,080
которые поддерживают сокращение,

220
00:08:46,080 --> 00:08:48,160
которое в основном находит лучшие k

221
00:08:48,160 --> 00:08:50,399
совпадений, скажем, 10 или 100 лучших

222
00:08:50,399 --> 00:08:53,040
совпадений, без необходимости исчерпывающего

223
00:08:53,040 --> 00:08:57,040
перечисления всех возможных кандидатов.

224
00:08:57,040 --> 00:08:59,360
Детали поиска с этими

225
00:08:59,360 --> 00:09:02,080
структурами данных сокращения находятся за пределами наших возможностей.  объем,

226
00:09:02,080 --> 00:09:03,600
но действительно полезно знать об этой

227
00:09:03,600 --> 00:09:06,000
важной возможности для сквозного

228
00:09:06,000 --> 00:09:09,000
поиска,

229
00:09:09,440 --> 00:09:10,320
хорошо,

230
00:09:10,320 --> 00:09:12,000
поэтому давайте вернемся к нашему последнему основному

231
00:09:12,000 --> 00:09:14,240
вопросу, можем ли мы получить

232
00:09:14,240 --> 00:09:16,880
преимущества эффективности от предварительного вычисления, сохраняя при

233
00:09:16,880 --> 00:09:18,800
этом детализированные взаимодействия на уровне терминов,

234
00:09:18,800 --> 00:09:22,080
которые мы использовали

235
00:09:22,080 --> 00:09:24,160
иметь нейронную парадигму, которая позволит

236
00:09:24,160 --> 00:09:26,800
нам сделать это, называется поздним взаимодействием,

237
00:09:26,800 --> 00:09:28,000
и это то, что  я работал

238
00:09:28,000 --> 00:09:29,920
над этим здесь, в Стэнфорде,

239
00:09:29,920 --> 00:09:32,240
так что давайте создадим позднее взаимодействие с

240
00:09:32,240 --> 00:09:33,760
нуля,

241
00:09:33,760 --> 00:09:36,080
мы начнем, как обычно, с

242
00:09:36,080 --> 00:09:39,200
токенизации запроса и документа,

243
00:09:39,200 --> 00:09:41,440
мы будем стремиться независимо кодировать

244
00:09:41,440 --> 00:09:43,519
запрос и документ, но на

245
00:09:43,519 --> 00:09:47,519
этот раз в мелкозернистых представлениях

246
00:09:47,680 --> 00:09:49,600
так что, как вы можете видеть в левой части,

247
00:09:49,600 --> 00:09:52,720
это на самом деле не сложно, как показано,

248
00:09:52,720 --> 00:09:54,720
мы можем передать две копии рождения

249
00:09:54,720 --> 00:09:56,800
запроса и документа отдельно и

250
00:09:56,800 --> 00:09:58,720
сохранить все выходные вложения,

251
00:09:58,720 --> 00:10:01,040
соответствующие всем токенам, в качестве нашего

252
00:10:01,040 --> 00:10:03,440
мелкозернистого представления

253
00:10:03,440 --> 00:10:06,880
для  запрос и для документа,

254
00:10:06,880 --> 00:10:09,839
хорошо, поэтому мы закончим здесь

255
00:10:09,839 --> 00:10:12,560
только после того, как мы действительно закроем этот цикл правильно,

256
00:10:12,560 --> 00:10:14,640
нам все еще нужно оценить релевантность

257
00:10:14,640 --> 00:10:17,519
между этим запросом и этим документом, по

258
00:10:17,519 --> 00:10:19,839
сути, у нас есть две матрицы, и нам

259
00:10:19,839 --> 00:10:21,760
нужно понятие сходства между

260
00:10:21,760 --> 00:10:24,800
этими двумя  матрицы или эти два пакета

261
00:10:24,800 --> 00:10:27,279
векторов,

262
00:10:28,079 --> 00:10:30,720
однако не каждый подход будет достаточным,

263
00:10:30,720 --> 00:10:32,800
мы настаиваем на том, чтобы мы получили масштабируемый

264
00:10:32,800 --> 00:10:34,720
механизм, который позволяет нам использовать

265
00:10:34,720 --> 00:10:37,200
поиск сходства векторов с обрезкой

266
00:10:37,200 --> 00:10:39,519
для проведения сквозного

267
00:10:39,519 --> 00:10:41,680
поиска масштабируемым образом по всей

268
00:10:41,680 --> 00:10:44,240
коллекции

269
00:10:44,480 --> 00:10:47,200
при выполнении этого или для этого

270
00:10:47,200 --> 00:10:48,959
оказывается, что очень простой

271
00:10:48,959 --> 00:10:51,920
механизм взаимодействия предлагает как масштабирование, так и высокое

272
00:10:51,920 --> 00:10:54,399
качество,

273
00:10:54,640 --> 00:10:56,480
поэтому вот что мы будем делать для

274
00:10:56,480 --> 00:10:58,720
Каждое вложение запроса,

275
00:10:58,720 --> 00:11:00,959
как я показываю здесь, мы вычисляем максимальную

276
00:11:00,959 --> 00:11:02,800
оценку сходства для всех

277
00:11:02,800 --> 00:11:05,760
вложений документа,

278
00:11:06,160 --> 00:11:08,160
так что это будет просто косинусное

279
00:11:08,160 --> 00:11:09,839
сходство,

280
00:11:09,839 --> 00:11:12,640
дающее нам одну частичную частичную оценку

281
00:11:12,640 --> 00:11:15,760
для этого термина запроса, которая является максимальным

282
00:11:15,760 --> 00:11:17,839
косинусным сходством для всех вложений.  синие

283
00:11:17,839 --> 00:11:20,880
вложения в этом случае

284
00:11:23,760 --> 00:11:25,760
мы повторим это здесь для всех вложений запроса,

285
00:11:25,760 --> 00:11:27,279


286
00:11:27,279 --> 00:11:29,519
и мы просто просуммируем все эти

287
00:11:29,519 --> 00:11:31,680
максимальные оценки подобия,

288
00:11:31,680 --> 00:11:36,000
чтобы получить нашу окончательную оценку для этого документа,

289
00:11:36,079 --> 00:11:37,839
поэтому мы будем ссылаться на эту общую

290
00:11:37,839 --> 00:11:40,320
парадигму здесь как на позднее взаимодействие и на

291
00:11:40,320 --> 00:11:42,720
эта конкретная модель, показанная здесь

292
00:11:42,720 --> 00:11:45,440
поверх рождения как Кольбер,

293
00:11:45,440 --> 00:11:47,839
и интуиция проста для каждого

294
00:11:47,839 --> 00:11:50,399
термина в запросе, мы просто пытаемся

295
00:11:50,399 --> 00:11:54,079
мягко и контекстуально найти этот термин

296
00:11:54,079 --> 00:11:55,440
в документе

297
00:11:55,440 --> 00:11:57,600
присвоение оценки тому, насколько успешным было это

298
00:11:57,600 --> 00:12:00,320
сопоставление,

299
00:12:00,720 --> 00:12:02,560
позвольте мне проиллюстрировать это на реальном

300
00:12:02,560 --> 00:12:04,639
примере из набора для разработки ранжирования ms mark marco,

301
00:12:04,639 --> 00:12:06,160
и я надеюсь, что это будет

302
00:12:06,160 --> 00:12:08,880
довольно интуитивно понятно, как только вы увидите, что

303
00:12:08,880 --> 00:12:11,120
вверху находится запрос, а внизу

304
00:12:11,120 --> 00:12:12,959
—  Часть правильного отрывка, которую

305
00:12:12,959 --> 00:12:15,920
Колберт извлекает в первой позиции,

306
00:12:15,920 --> 00:12:17,279
потому что у нас есть этот простой

307
00:12:17,279 --> 00:12:19,040
механизм позднего взаимодействия,

308
00:12:19,040 --> 00:12:21,519
мы можем фактически исследовать поведение, и

309
00:12:21,519 --> 00:12:23,760
мы можем видеть в этом конкретном примере,

310
00:12:23,760 --> 00:12:25,519
что Колберт сопоставляет

311
00:12:25,519 --> 00:12:28,639
через операторы максимального сходства

312
00:12:28,639 --> 00:12:31,040
слово when в вопросе со

313
00:12:31,040 --> 00:12:33,839
словом on  во фразе 8 августа,

314
00:12:33,839 --> 00:12:37,360
которая является датой, как мы могли ожидать,

315
00:12:37,360 --> 00:12:39,200
она соответствует слову «трансформеры» с

316
00:12:39,200 --> 00:12:40,959
тем же словом в документе,

317
00:12:40,959 --> 00:12:43,440
она соответствует «мультфильму» с «анимацией»,

318
00:12:43,440 --> 00:12:45,440
а отдельные слова

319
00:12:45,440 --> 00:12:48,480
соответствуют термину «выпущено».

320
00:12:48,480 --> 00:12:51,120
фраза, которая была опубликована в документе

321
00:12:51,120 --> 00:12:53,200
8 августа,

322
00:12:53,200 --> 00:12:55,120


323
00:12:55,120 --> 00:12:57,440
как мы могли бы интуитивно ожидать, поэтому мы

324
00:12:57,440 --> 00:12:58,959
просто пытаемся контекстуально

325
00:12:58,959 --> 00:13:01,200
сопоставить эти термины quiddity

326
00:13:01,200 --> 00:13:02,880
в документе

327
00:13:02,880 --> 00:13:05,200
и назначьте некоторый балл соответствия для каждого

328
00:13:05,200 --> 00:13:06,800
из этих терминов,

329
00:13:06,800 --> 00:13:09,120
поэтому обратите внимание здесь и помните, что

330
00:13:09,120 --> 00:13:11,120
covaria представляет каждый документ как

331
00:13:11,120 --> 00:13:13,519
плотную матрицу многих векторов и, в

332
00:13:13,519 --> 00:13:15,680
частности, один вектор на токен,

333
00:13:15,680 --> 00:13:17,519
и это отличается от

334
00:13:17,519 --> 00:13:20,240
моделей подобия представления, которые мы рассматривали ранее.

335
00:13:20,240 --> 00:13:22,079
который пытался втиснуть каждый документ в

336
00:13:22,079 --> 00:13:24,480
один вектор, и что делает это возможным, так

337
00:13:24,480 --> 00:13:26,399
это операторы максимального подобия,

338
00:13:26,399 --> 00:13:29,519
которые у нас есть поверх этих

339
00:13:29,519 --> 00:13:31,600
матричных представлений,

340
00:13:31,600 --> 00:13:35,519
так что, насколько хорошо работает Кольбер

341
00:13:35,519 --> 00:13:37,360
и как он справляется с этим разрывом, который мы

342
00:13:37,360 --> 00:13:39,760
имеем здесь между эффективными моделями и

343
00:13:39,760 --> 00:13:42,399
высокоэффективные,

344
00:13:42,399 --> 00:13:44,560
а также перепроектирование

345
00:13:44,560 --> 00:13:46,079
архитектуры модели и предложение парадигмы позднего

346
00:13:46,079 --> 00:13:48,240
взаимодействия. Colbert позволяет нам

347
00:13:48,240 --> 00:13:50,079
достичь качества, конкурентоспособного с

348
00:13:50,079 --> 00:13:54,320
рождением, при небольшой доле затрат, что,

349
00:13:54,560 --> 00:13:56,800
возможно, более важно, Colbert может

350
00:13:56,800 --> 00:13:59,120
масштабироваться до всей коллекции

351
00:13:59,120 --> 00:14:01,440
благодаря сквозной обрезке.

352
00:14:01,440 --> 00:14:03,600
в этом случае извлекать все девять миллионов пассажей,

353
00:14:03,600 --> 00:14:05,920
сохраняя при этом задержку менее секунды,

354
00:14:05,920 --> 00:14:08,160


355
00:14:08,160 --> 00:14:10,560
гм и чт  Это позволяет гораздо более высокий отзыв,

356
00:14:10,560 --> 00:14:12,560
чем позволяют традиционные конвейеры

357
00:14:12,560 --> 00:14:15,560
переранжирования.

358
00:14:18,000 --> 00:14:19,920
Хорошо, до сих пор мы рассматривали в

359
00:14:19,920 --> 00:14:21,920
оценках эффективности предметной области в

360
00:14:21,920 --> 00:14:24,160
основном случаи, когда у нас были

361
00:14:24,160 --> 00:14:26,720
данные обучения и оценки для поставленной задачи,

362
00:14:26,720 --> 00:14:30,480
которой до сих пор была мисс Марко, но мы

363
00:14:30,480 --> 00:14:32,560
часто хотим

364
00:14:32,560 --> 00:14:34,560
использовать поиск в новых настройках вне домена

365
00:14:34,560 --> 00:14:36,079
, мы просто хотим бросить нашу

366
00:14:36,079 --> 00:14:37,279
поисковую систему

367
00:14:37,279 --> 00:14:39,360
на сложную проблему без обучающих

368
00:14:39,360 --> 00:14:42,160
данных без данных проверки и увидеть, как она работает

369
00:14:42,160 --> 00:14:44,480
хорошо,

370
00:14:44,480 --> 00:14:47,519
мы кратко обсудили воздух, перед

371
00:14:47,519 --> 00:14:48,800
которым

372
00:14:48,800 --> 00:14:51,920
была недавняя попытка протестировать наши модели в  настройка с

373
00:14:51,920 --> 00:14:53,120
нулевым выстрелом,

374
00:14:53,120 --> 00:14:55,360
когда модели обучаются на

375
00:14:55,360 --> 00:14:57,680
одной задаче за задачей, а затем

376
00:14:57,680 --> 00:14:59,760
фиксируются, а затем тестируются на

377
00:14:59,760 --> 00:15:01,600
совершенно другом

378
00:15:01,600 --> 00:15:03,279
наборе задач.

379
00:15:03,279 --> 00:15:06,320
Пиво включает 17 наборов данных

380
00:15:06,320 --> 00:15:08,079
и девять различных

381
00:15:08,079 --> 00:15:10,800
задач или сценариев, а также  авторы

382
00:15:10,800 --> 00:15:13,120
nand и ital сравнили множество ir-

383
00:15:13,120 --> 00:15:14,959
моделей, которые мы обсуждали сегодня, с

384
00:15:14,959 --> 00:15:17,279
нулевой точностью друг против друга

385
00:15:17,279 --> 00:15:19,360
во всех этих задачах,

386
00:15:19,360 --> 00:15:22,079
так что давайте посмотрим,

387
00:15:22,079 --> 00:15:23,440


388
00:15:23,440 --> 00:15:25,839
что у нас есть.

389
00:15:25,839 --> 00:15:28,399
Результаты bm25 для модели взаимодействия,

390
00:15:28,399 --> 00:15:30,639
которой в данном случае является электра, которая

391
00:15:30,639 --> 00:15:32,320
имеет тенденцию работать немного лучше, чем

392
00:15:32,320 --> 00:15:34,320
рождение для ранжирования, у нас есть две

393
00:15:34,320 --> 00:15:36,959
модели подобия представления dpr и

394
00:15:36,959 --> 00:15:38,480
s-bert,

395
00:15:38,480 --> 00:15:39,920
и у нас есть модель позднего взаимодействия,

396
00:15:39,920 --> 00:15:41,519
которая является

397
00:15:41,519 --> 00:15:42,639


398
00:15:42,639 --> 00:15:44,399
лучшей

399
00:15:44,399 --> 00:15:46,800
в каждой строке.  в каждой задаче ir

400
00:15:46,800 --> 00:15:49,680
выделено жирным шрифтом, и мы видим, что во всех задачах

401
00:15:49,680 --> 00:15:53,360
самая сильная модель в ndcg на

402
00:15:53,360 --> 00:15:55,279
10 всегда является одной из трех моделей, которые

403
00:15:55,279 --> 00:15:57,279
включают взаимодействия на уровне поворота, которые

404
00:15:57,279 --> 00:16:01,199
представляют собой electro colbert и bm25, что

405
00:16:01,199 --> 00:16:03,040
интересно, одновекторные

406
00:16:03,040 --> 00:16:05,279
подходы, которые казались весьма многообещающими.

407
00:16:05,279 --> 00:16:06,639
до сих пор

408
00:16:06,639 --> 00:16:08,560
не удалось надежно обобщить в соответствии

409
00:16:08,560 --> 00:16:10,959
с этими результатами,

410
00:16:10,959 --> 00:16:12,880
тогда как colbert, который также является быстрой

411
00:16:12,880 --> 00:16:14,320
моделью,

412
00:16:14,320 --> 00:16:15,839
почти соответствует качеству

413
00:16:15,839 --> 00:16:19,519
дорогой электры,

414
00:16:20,480 --> 00:16:22,639
якорь результаты до сих пор, где на метрике

415
00:16:22,639 --> 00:16:24,720
ndcg присутствует, которая является метрикой, ориентированной на точность,

416
00:16:24,720 --> 00:16:27,360
смотрит на лучшие результаты,

417
00:16:27,360 --> 00:16:30,320
но  здесь у меня есть результаты автора,

418
00:16:30,320 --> 00:16:31,279
гм,

419
00:16:31,279 --> 00:16:33,680
после агрегирования уровня задачи, гм, с

420
00:16:33,680 --> 00:16:37,440
учетом отзыва на 100,

421
00:16:37,440 --> 00:16:40,240
и здесь, хотя

422
00:16:41,680 --> 00:16:44,240
мы видим  при этом вы знаете, что

423
00:16:44,240 --> 00:16:46,560
результаты довольно похожи, когда мы

424
00:16:46,560 --> 00:16:48,959
рассматриваем отзыв,

425
00:16:48,959 --> 00:16:50,399
но одно главное отличие состоит в том, что

426
00:16:50,399 --> 00:16:52,320
механизм позднего взаимодействия Колберта,

427
00:16:52,320 --> 00:16:53,839
который позволяет ему проводить сквозной

428
00:16:53,839 --> 00:16:55,120
поиск

429
00:16:55,120 --> 00:16:57,519
с высоким качеством, позволяет ему достичь

430
00:16:57,519 --> 00:17:00,880
самого сильного отзыва в этом случае,

431
00:17:00,880 --> 00:17:03,040
и поэтому  мы можем заключить в основном, что

432
00:17:03,040 --> 00:17:05,919
масштабируемое мелкозернистое взаимодействие является ключом

433
00:17:05,919 --> 00:17:09,199
к надежному более высокому вызову,

434
00:17:09,599 --> 00:17:11,760
конечно, обратите внимание, что bm25 и

435
00:17:11,760 --> 00:17:13,599
отзыв электрона

436
00:17:13,599 --> 00:17:15,919
здесь одинаковы, поскольку электро всего на три

437
00:17:15,919 --> 00:17:18,400
балла выше 100 лучших в данном случае от

438
00:17:18,400 --> 00:17:20,959
bm25,

439
00:17:20,959 --> 00:17:23,280
так что это завершает наш нейронный раздел ir

440
00:17:23,280 --> 00:17:26,160
из серии nlu plus ir

441
00:17:26,160 --> 00:17:28,480
в следующем скринкасте мы

442
00:17:28,480 --> 00:17:30,640
обсудим, как масштабируемость

443
00:17:30,640 --> 00:17:32,720
с этими моделями поиска может на самом деле

444
00:17:32,720 --> 00:17:35,200
привести к значительному приросту качества, а не только

445
00:17:35,200 --> 00:17:36,320
скорости,

446
00:17:36,320 --> 00:17:38,080
чего мы до сих пор не видели, за

447
00:17:38,080 --> 00:17:39,679
исключением случая отдачи

448
00:17:39,679 --> 00:17:42,000
и как настройка нейронного ir  модель вписывается

449
00:17:42,000 --> 00:17:44,400
в более крупный нисходящий открытый домен

450
00:17:44,400 --> 00:17:47,840
в задаче liu

