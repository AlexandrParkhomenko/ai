1
00:00:04,960 --> 00:00:06,720
приветствуем всех, это вторая часть нашей

2
00:00:06,720 --> 00:00:08,080
серии статей о контекстных представлениях слов,

3
00:00:08,080 --> 00:00:09,280
мы собираемся

4
00:00:09,280 --> 00:00:10,719
поговорить об архитектуре преобразователя,

5
00:00:10,719 --> 00:00:12,960
которая является центральной

6
00:00:12,960 --> 00:00:14,799
частью всех моделей, которые мы будем изучать в

7
00:00:14,799 --> 00:00:15,839
этом

8
00:00:15,839 --> 00:00:17,600
разделе, давайте углубимся в структуру модели, которую

9
00:00:17,600 --> 00:00:19,279
мы будем  поработайте над этим, используя простой

10
00:00:19,279 --> 00:00:21,119
пример внизу, здесь у меня есть

11
00:00:21,119 --> 00:00:23,519
входная последовательность правил рока, и я

12
00:00:23,519 --> 00:00:25,039
указал красным, что мы будем

13
00:00:25,039 --> 00:00:26,720
отслеживать позиции

14
00:00:26,720 --> 00:00:29,359
каждого из этих токенов в последовательности, которую

15
00:00:29,359 --> 00:00:31,039
первый шаг уже знаком, мы

16
00:00:31,039 --> 00:00:32,719
собираемся искать как слова, так и

17
00:00:32,719 --> 00:00:35,600
позиции в отдельных пространствах вложений,

18
00:00:35,600 --> 00:00:37,200
это фиксированные пространства вложений, которые

19
00:00:37,200 --> 00:00:38,719
мы изучим как часть изучения всех

20
00:00:38,719 --> 00:00:41,280
параметров в этой модели, я дал

21
00:00:41,280 --> 00:00:43,040
вложения слов  в светло-сером и

22
00:00:43,040 --> 00:00:45,760
позиционные вложения в темно-сером,

23
00:00:45,760 --> 00:00:47,440
чтобы сформировать то, что мы считаем фактическим

24
00:00:47,440 --> 00:00:49,280
вложением для этой модели, мы делаем

25
00:00:49,280 --> 00:00:51,199
поэлементное добавление слова

26
00:00:51,199 --> 00:00:53,199
вложение с позиционным вложением,

27
00:00:53,199 --> 00:00:54,879
и это дает нам представление  сообщения

28
00:00:54,879 --> 00:00:56,800
, которые здесь выделены зеленым, вы можете видеть, что

29
00:00:56,800 --> 00:00:58,160
в правой части слайда я

30
00:00:58,160 --> 00:00:59,440
буду отслеживать все

31
00:00:59,440 --> 00:01:02,000
вычисления в отношении этого

32
00:01:02,000 --> 00:01:03,520
столбца c здесь, и они полностью

33
00:01:03,520 --> 00:01:05,680
параллельны для столбцов a и b,

34
00:01:05,680 --> 00:01:07,840
поэтому для  form c input мы делаем поэлементное

35
00:01:07,840 --> 00:01:10,159
добавление x34 для встраивания

36
00:01:10,159 --> 00:01:12,799
слова rules и p3, которое является вложением

37
00:01:12,799 --> 00:01:16,240
для позиции три в этой модели,

38
00:01:16,240 --> 00:01:18,320
следующий слой действительно является отличительной чертой

39
00:01:18,320 --> 00:01:19,840
этой архитектуры, и

40
00:01:19,840 --> 00:01:21,680
все, что вам нужно, это то, что придает статье внимание.

41
00:01:21,680 --> 00:01:23,680
мы собираемся сформировать кучу

42
00:01:23,680 --> 00:01:26,000
плотных соединений скалярного произведения между

43
00:01:26,000 --> 00:01:28,080
всеми этими представлениями, чтобы вы могли

44
00:01:28,080 --> 00:01:29,840
думать о них как о формировании этих

45
00:01:29,840 --> 00:01:31,600
соединений, которые выглядят вот так: плотная

46
00:01:31,600 --> 00:01:32,880
заросль

47
00:01:32,880 --> 00:01:34,479
справа здесь я дал основной

48
00:01:34,479 --> 00:01:36,240
расчет, и он  должно быть знакомо

49
00:01:36,240 --> 00:01:38,400
из первой части этого модуля, это именно

50
00:01:38,400 --> 00:01:40,479
тот расчет, который я представил там,

51
00:01:40,479 --> 00:01:43,119
всего с двумя небольшими изменениями, но в основном,

52
00:01:43,119 --> 00:01:45,360
если наш целевой вектор является входом c здесь,

53
00:01:45,360 --> 00:01:46,880
мы обращаем внимание на

54
00:01:46,880 --> 00:01:48,479
входы a и b,

55
00:01:48,479 --> 00:01:50,799
мы делаем это с помощью  здесь мы формируем скалярные произведения,

56
00:01:50,799 --> 00:01:52,640
и один поворот из предыдущего заключается в

57
00:01:52,640 --> 00:01:54,240
том, что вместо того, чтобы просто брать эти скалярные

58
00:01:54,240 --> 00:01:56,320
произведения, мы нормализуем их

59
00:01:56,320 --> 00:01:58,159
квадратным корнем из размерности

60
00:01:58,159 --> 00:01:59,680
модели dk

61
00:01:59,680 --> 00:02:01,840
dk является важным значением здесь из-за

62
00:02:01,840 --> 00:02:03,680
того, как мы комбинируем представления в

63
00:02:03,680 --> 00:02:06,159
все выходные данные

64
00:02:06,159 --> 00:02:08,160
всех слоев, на которые мы смотрим, должны

65
00:02:08,160 --> 00:02:10,878
иметь одинаковую размерность, заданную параметром dk,

66
00:02:10,878 --> 00:02:12,000
и поэтому то, что мы здесь делаем, по

67
00:02:12,000 --> 00:02:13,599
сути, масштабирует эти скалярные произведения,

68
00:02:13,599 --> 00:02:15,760
чтобы как бы удерживать их в разумном

69
00:02:15,760 --> 00:02:17,200
диапазоне,

70
00:02:17,200 --> 00:02:19,840
что дает нам  счет вектора альфа тильда,

71
00:02:19,840 --> 00:02:21,920
мы мягко макс нормализуем их, а затем

72
00:02:21,920 --> 00:02:23,440
другой поворот заключается в том, что вместо того, чтобы использовать

73
00:02:23,440 --> 00:02:24,959
здесь среднее значение, как мы делали до того, как мы использовали

74
00:02:24,959 --> 00:02:27,200
суммирование, но фактический вектор - это

75
00:02:27,200 --> 00:02:28,879
тот, который мы вычислили, прежде чем мы собираемся

76
00:02:28,879 --> 00:02:31,440
взять взвешенные версии входных данных и  b

77
00:02:31,440 --> 00:02:33,360
ввод в соответствии с этим вектором

78
00:02:33,360 --> 00:02:35,599
весов, который мы создали здесь,

79
00:02:35,599 --> 00:02:37,519
который дает нам представление c

80
00:02:37,519 --> 00:02:40,000
внимания, как показано здесь оранжевым цветом, и мы

81
00:02:40,000 --> 00:02:41,280
делаем это, конечно, для всех других

82
00:02:41,280 --> 00:02:43,599
позиций в модели

83
00:02:43,599 --> 00:02:45,280
t  Следующий шаг довольно интересен,

84
00:02:45,280 --> 00:02:47,120
мы создаем то, что называется остаточным

85
00:02:47,120 --> 00:02:49,599
соединением, поэтому, чтобы получить слой ca здесь,

86
00:02:49,599 --> 00:02:50,560
выделенный желтым цветом,

87
00:02:50,560 --> 00:02:53,760
мы добавляем входные данные c, и это представление внимания,

88
00:02:53,760 --> 00:02:55,840
которое мы только что создали и

89
00:02:55,840 --> 00:02:57,760
применили, выпадает как шаг регуляризации,

90
00:02:57,760 --> 00:03:00,000
и это дает нам  ca

91
00:03:00,000 --> 00:03:01,440
уровень интересная вещь, конечно,

92
00:03:01,440 --> 00:03:03,120
это остаточное соединение вместо того, чтобы

93
00:03:03,120 --> 00:03:05,920
просто продвигать вперед, обратите внимание, мы

94
00:03:05,920 --> 00:03:07,440
передаем вперед на самом деле его версию,

95
00:03:07,440 --> 00:03:09,599
которая сочетается с нашим начальным

96
00:03:09,599 --> 00:03:13,040
позиционно-кодированным встраиванием,

97
00:03:13,280 --> 00:03:14,959
затем мы следуем за этим с шагом нормализации слоя,

98
00:03:14,959 --> 00:03:16,560
который должен помочь с

99
00:03:16,560 --> 00:03:18,400
оптимизацией  он собирается как бы

100
00:03:18,400 --> 00:03:21,120
масштабировать веса и эти представления,

101
00:03:21,120 --> 00:03:22,800
следующий шаг более значим,

102
00:03:22,800 --> 00:03:25,360
это серия из двух плотных слоев, поэтому мы

103
00:03:25,360 --> 00:03:27,440
возьмем норму ca и пропустим ее через

104
00:03:27,440 --> 00:03:30,000
этот плотный слой с нелинейностью,

105
00:03:30,000 --> 00:03:32,319
за которой следует еще один линейный  слой, чтобы

106
00:03:32,319 --> 00:03:35,599
дать нам cfx, как показано темно-синим здесь,

107
00:03:35,599 --> 00:03:36,959
и за ним следует еще одно из

108
00:03:36,959 --> 00:03:38,720
этих интересных остаточных соединений,

109
00:03:38,720 --> 00:03:41,280
поэтому мы применим  выпадите в cff,

110
00:03:41,280 --> 00:03:43,280
а затем добавьте это с нормой ca, как

111
00:03:43,280 --> 00:03:44,720
указано здесь внизу, и это

112
00:03:44,720 --> 00:03:46,239
дает нам это второе желтое

113
00:03:46,239 --> 00:03:47,760
представление,

114
00:03:47,760 --> 00:03:50,000
за которым мы следуем еще одним шагом нормализации слоя,

115
00:03:50,000 --> 00:03:52,000
и это дает нам

116
00:03:52,000 --> 00:03:54,080
вывод для этого блока представлений преобразователя

117
00:03:54,080 --> 00:03:56,319


118
00:03:56,319 --> 00:03:57,840
и  вы, конечно, можете представить, как вы увидите,

119
00:03:57,840 --> 00:03:59,200
что мы можем сложить эти

120
00:03:59,200 --> 00:04:00,959
блоки трансформатора, и способ, которым мы

121
00:04:00,959 --> 00:04:02,000
это делаем,

122
00:04:02,000 --> 00:04:04,080
заключается в том, чтобы взять эти темно-

123
00:04:04,080 --> 00:04:05,920
зеленые представления вверху

124
00:04:05,920 --> 00:04:07,599
и использовать их в качестве входных данных, и все

125
00:04:07,599 --> 00:04:09,200
вычисления одинаковы, поэтому  вы можете

126
00:04:09,200 --> 00:04:11,280
представить, что мы могли бы продолжить здесь,

127
00:04:11,280 --> 00:04:13,040
просто выполнив плотную серию связей внимания

128
00:04:13,040 --> 00:04:14,959
между ними, а затем

129
00:04:14,959 --> 00:04:16,959
продолжив вычисления, которые я

130
00:04:16,959 --> 00:04:18,959
только что представил, и таким образом мы могли бы

131
00:04:18,959 --> 00:04:20,959
сложить блоки преобразователя, и я

132
00:04:20,959 --> 00:04:23,199
вернусь к этому позже.

133
00:04:23,199 --> 00:04:24,400
несколько других вещей, на которые

134
00:04:24,400 --> 00:04:25,680
стоит обратить внимание, которые

135
00:04:25,680 --> 00:04:27,919
заслуживают внимания в этой модели,

136
00:04:27,919 --> 00:04:29,680
она выглядит как сложная серия

137
00:04:29,680 --> 00:04:31,120
вычислений, но я бы сказал, что

138
00:04:31,120 --> 00:04:33,440
принципиально  что здесь происходит, так это

139
00:04:33,440 --> 00:04:35,520
то, что мы выполняем позиционное кодирование, чтобы получить

140
00:04:35,520 --> 00:04:37,600
вложения, чтобы они были

141
00:04:37,600 --> 00:04:40,479
чувствительными к положению представлениями слов,

142
00:04:40,479 --> 00:04:42,560
за которыми мы следим за этим с помощью слоя внимания,

143
00:04:42,560 --> 00:04:44,320
который создает эту плотную чащу

144
00:04:44,320 --> 00:04:46,400
связей между всеми словами, как

145
00:04:46,400 --> 00:04:48,160
позиционно закодированные,

146
00:04:48,160 --> 00:04:49,840
тогда у нас есть эти вещи оптимизации

147
00:04:49,840 --> 00:04:51,440
вплетены, но в основном мы

148
00:04:51,440 --> 00:04:53,680
следуем этому шагу внимания с двумя

149
00:04:53,680 --> 00:04:56,000
сериями шагов прямого слоя,

150
00:04:56,000 --> 00:04:58,800
за которыми следует тот же процесс исключения

151
00:04:58,800 --> 00:05:00,880
и нормализации слоя, поэтому, если вы как

152
00:05:00,880 --> 00:05:03,280
бы выделили желтый и фиолетовый,

153
00:05:03,280 --> 00:05:04,720
вы бы увидели, что мы  на самом деле

154
00:05:04,720 --> 00:05:06,560
это напряжение, за которым следуют ноги

155
00:05:06,560 --> 00:05:08,320
вперед, а затем, когда мы складываем эти

156
00:05:08,320 --> 00:05:09,840
вещи, это будет подача

157
00:05:09,840 --> 00:05:11,919
внимания вперед, подача внимания вперед, когда мы

158
00:05:11,919 --> 00:05:14,479
поднимаемся и переплетаемся, есть

159
00:05:14,479 --> 00:05:15,840
некоторые вещи, которые, я бы сказал, помогают с

160
00:05:15,840 --> 00:05:17,919
оптимизацией.

161
00:05:17,919 --> 00:05:19,600
еще одна примечательная вещь в этой

162
00:05:19,600 --> 00:05:22,240
модели:  что единственный смысл, в котором

163
00:05:22,240 --> 00:05:24,160
мы отслеживаем линейный

164
00:05:24,160 --> 00:05:26,479
порядок последовательности, заключается в тех позиционных

165
00:05:26,479 --> 00:05:28,880
вложениях  dings, если бы не они,

166
00:05:28,880 --> 00:05:30,720
порядок столбцов был бы совершенно неуместным,

167
00:05:30,720 --> 00:05:32,160
потому что, конечно, мы создали все

168
00:05:32,160 --> 00:05:33,840
эти симметричные соединения на

169
00:05:33,840 --> 00:05:35,919
уровне внимания, и нет никаких других

170
00:05:35,919 --> 00:05:38,240
соединений между этими столбцами, поэтому

171
00:05:38,240 --> 00:05:40,960
единственный смысл, в котором порядок столбцов, это

172
00:05:40,960 --> 00:05:43,199
порядок слов имеет значение  вот через эти

173
00:05:43,199 --> 00:05:46,320
позиционные вложения

174
00:05:47,600 --> 00:05:49,280
вот более подробный взгляд на

175
00:05:49,280 --> 00:05:51,520
сами расчеты внимания я просто

176
00:05:51,520 --> 00:05:52,960
хочу показать, как это на самом деле

177
00:05:52,960 --> 00:05:54,800
работает на механическом уровне, так что

178
00:05:54,800 --> 00:05:56,639
это расчет, как я представил его на

179
00:05:56,639 --> 00:05:58,639
предыдущем слайде и в первой части этого

180
00:05:58,639 --> 00:06:00,000
модуля

181
00:06:00,000 --> 00:06:02,000
в статье, и теперь обычно это

182
00:06:02,000 --> 00:06:03,919
представлено в этом матричном формате, и если

183
00:06:03,919 --> 00:06:06,319
вы похожи на меня, сразу не очевидно,

184
00:06:06,319 --> 00:06:07,840
что это эквивалентные

185
00:06:07,840 --> 00:06:09,600
вычисления, поэтому то, что я сделал для

186
00:06:09,600 --> 00:06:12,560
следующих двух слайдов, это просто показать вам на

187
00:06:12,560 --> 00:06:14,560
проработанных примерах, как эти

188
00:06:14,560 --> 00:06:16,720
расчеты работают и как они получают

189
00:06:16,720 --> 00:06:19,039
точно такие же значения, я не собираюсь

190
00:06:19,039 --> 00:06:20,800
тратить на это слишком много времени, это

191
00:06:20,800 --> 00:06:22,639
действительно только здесь для вас, если вы

192
00:06:22,639 --> 00:06:24,240
хотите  Ке подробно проработать расчеты,

193
00:06:24,240 --> 00:06:26,000
которые я настоятельно рекомендую,

194
00:06:26,000 --> 00:06:28,160
потому что это действительно фундаментальный

195
00:06:28,160 --> 00:06:30,080
шаг в этой модели,

196
00:06:30,080 --> 00:06:31,440
и вот все детали, которые вам

197
00:06:31,440 --> 00:06:33,039
понадобятся, чтобы получить практические результаты с этими

198
00:06:33,039 --> 00:06:34,960
идеями.

199
00:06:34,960 --> 00:06:37,199


200
00:06:37,199 --> 00:06:39,440
В упрощенном виде отличительной чертой

201
00:06:39,440 --> 00:06:41,680
внимания в преобразователе является то, что

202
00:06:41,680 --> 00:06:43,759
это, как правило, многоголовое внимание,

203
00:06:43,759 --> 00:06:45,840
поэтому позвольте мне раскрыть эту идею немного

204
00:06:45,840 --> 00:06:48,639
конкретнее, мы начнем с нашей нашей

205
00:06:48,639 --> 00:06:50,319
входной последовательности из предыдущего, и мы будем

206
00:06:50,319 --> 00:06:52,080
смотреть на эти  Зеленые представления

207
00:06:52,080 --> 00:06:52,880
здесь

208
00:06:52,880 --> 00:06:55,199
и идея, лежащая в основе механизмов многократного задержания

209
00:06:55,199 --> 00:06:56,800
, что мы собираемся внедрить

210
00:06:56,800 --> 00:06:58,560
в этот процесс кучу изученных параметров,

211
00:06:58,560 --> 00:07:01,280
чтобы поощрять разнообразие как

212
00:07:01,280 --> 00:07:02,639
часть процесса обучения, и это

213
00:07:02,639 --> 00:07:03,919
действительно разнообразные и интересные

214
00:07:03,919 --> 00:07:05,440
представления,

215
00:07:05,440 --> 00:07:07,360
так что вот как это работает.  мы собираемся

216
00:07:07,360 --> 00:07:09,280
сформировать здесь три представления, используя

217
00:07:09,280 --> 00:07:11,120
тот же механизм скалярного произведения, что и

218
00:07:11,120 --> 00:07:13,039
раньше, и в основном это тот же

219
00:07:13,039 --> 00:07:14,319
расчет,

220
00:07:14,319 --> 00:07:16,000
за исключением того, что теперь у нас будет булочка

221
00:07:16,000 --> 00:07:18,080
ch изученных параметров веса, как

222
00:07:18,080 --> 00:07:20,080
показано здесь оранжевым цветом, и они помогут нам с

223
00:07:20,080 --> 00:07:22,000
двумя вещами: во-первых, внести разнообразие

224
00:07:22,000 --> 00:07:24,319
в этот процесс, а также уменьшить

225
00:07:24,319 --> 00:07:26,240
размерность

226
00:07:26,240 --> 00:07:28,160
представлений до одной трети размера, на который мы

227
00:07:28,160 --> 00:07:30,400
ориентируемся как dk для нашей модели

228
00:07:30,400 --> 00:07:31,919
размерность, и вы поймете, почему это

229
00:07:31,919 --> 00:07:33,520
происходит через секунду,

230
00:07:33,520 --> 00:07:35,280
но в основном то, что мы делаем, - это

231
00:07:35,280 --> 00:07:37,120
точно то же вычисление, которое мы делали раньше,

232
00:07:37,120 --> 00:07:39,039
но теперь с этими параметрами обучения,

233
00:07:39,039 --> 00:07:40,960
введенными в него, поэтому, если вы прищуритесь, вы

234
00:07:40,960 --> 00:07:42,400
можете увидеть, что это действительно точечный

235
00:07:42,400 --> 00:07:44,639
продукт  c ввод с вводом, как и

236
00:07:44,639 --> 00:07:47,280
раньше, но теперь преобразуется этими

237
00:07:47,280 --> 00:07:48,800
изученными параметрами, которые выделены

238
00:07:48,800 --> 00:07:50,720
оранжевым цветом, и это повторяется для этих других

239
00:07:50,720 --> 00:07:52,720
вычислений,

240
00:07:52,720 --> 00:07:54,319
поэтому мы собираемся сделать это для позиции a,

241
00:07:54,319 --> 00:07:56,479
и мы делаем это также для позиции b,

242
00:07:56,479 --> 00:07:58,400
и это то же самое  расчет, но теперь

243
00:07:58,400 --> 00:08:00,160
с новыми параметрами

244
00:08:00,160 --> 00:08:01,919
для второй позиции,

245
00:08:01,919 --> 00:08:03,759
а затем для третьей головки точно такой

246
00:08:03,759 --> 00:08:05,680
же расчет, но новые

247
00:08:05,680 --> 00:08:07,919
параметры обучения вверху здесь, так что это

248
00:08:07,919 --> 00:08:10,080
трехголовое внимание  И способ, которым мы на

249
00:08:10,080 --> 00:08:12,639
самом деле формируем представления, которые

250
00:08:12,639 --> 00:08:14,720
продолжаются с остальной частью расчета

251
00:08:14,720 --> 00:08:16,240
архитектуры преобразователя, как было

252
00:08:16,240 --> 00:08:17,919
представлено ранее,

253
00:08:17,919 --> 00:08:19,919
заключается в объединении трех

254
00:08:19,919 --> 00:08:21,759
представлений, которые мы создали для каждого

255
00:08:21,759 --> 00:08:24,879
из этих блоков, так что столбец a является

256
00:08:24,879 --> 00:08:26,639
первым представлением в каждом из

257
00:08:26,639 --> 00:08:27,680
них.

258
00:08:27,680 --> 00:08:29,280
столбец b является вторым

259
00:08:29,280 --> 00:08:31,199
представлением каждой головы и

260
00:08:31,199 --> 00:08:33,360
аналогично для столбца c является третьим

261
00:08:33,360 --> 00:08:34,880
представлением в каждой из этих

262
00:08:34,880 --> 00:08:36,799
головок, и поэтому каждый из них

263
00:08:36,799 --> 00:08:38,159
должен иметь одну треть

264
00:08:38,159 --> 00:08:40,399
размерности нашей полной модели, чтобы

265
00:08:40,399 --> 00:08:42,559
мы могли  объединить их, а затем передать

266
00:08:42,559 --> 00:08:45,519
их в последующие вычисления.

267
00:08:45,519 --> 00:08:46,959
Идея здесь, конечно, заключается в том, что

268
00:08:46,959 --> 00:08:48,240
вводя все эти изученные

269
00:08:48,240 --> 00:08:50,560
параметры во все эти разные

270
00:08:50,560 --> 00:08:52,560
головы, мы даем модели возможность

271
00:08:52,560 --> 00:08:54,560
изучить множество различных способов

272
00:08:54,560 --> 00:08:58,480
связывания слов в последовательности.

273
00:08:58,480 --> 00:08:59,920
последний момент, о котором я уже

274
00:08:59,920 --> 00:09:01,519
упоминал ранее, заключается в том, что обычно

275
00:09:01,519 --> 00:09:03,519
у нас есть не один блок трансформатора,

276
00:09:03,519 --> 00:09:05,600
а целая станция.  После них мы можем

277
00:09:05,600 --> 00:09:08,080
повторить их n раз для моделей, с которыми вы

278
00:09:08,080 --> 00:09:10,480
работаете, у вас может быть 12, 24 или

279
00:09:10,480 --> 00:09:11,519
даже больше

280
00:09:11,519 --> 00:09:13,920
блоков в архитектуре трансформатора,

281
00:09:13,920 --> 00:09:15,519
и способ, которым мы это делаем, как я уже сказал, заключается в том, что мы

282
00:09:15,519 --> 00:09:17,519
просто берем темно-зеленые

283
00:09:17,519 --> 00:09:19,519
представления на выходе.  слоя здесь

284
00:09:19,519 --> 00:09:21,600
и используя их в качестве входных данных для последующего

285
00:09:21,600 --> 00:09:23,680
блока, чтобы они учитывались, когда мы

286
00:09:23,680 --> 00:09:25,040
приступаем к последующим

287
00:09:25,040 --> 00:09:27,519
шагам регуляризации и прямой связи,

288
00:09:27,519 --> 00:09:29,760
как и раньше, и когда вы работаете с

289
00:09:29,760 --> 00:09:32,000
этими моделями в обнимающем лице, если вы

290
00:09:32,000 --> 00:09:33,839
запрашиваете все скрытые состояния  то, что вы

291
00:09:33,839 --> 00:09:36,080
получаете, представляет собой сетку представлений,

292
00:09:36,080 --> 00:09:38,080
соответствующих этим блокам вывода, выделенным

293
00:09:38,080 --> 00:09:39,920
здесь зеленым цветом,

294
00:09:39,920 --> 00:09:41,760
и, конечно, просто в качестве напоминания, я не

295
00:09:41,760 --> 00:09:43,600
указываю это здесь, но на самом деле

296
00:09:43,600 --> 00:09:45,440
к каждому из

297
00:09:45,440 --> 00:09:46,959
этих блоков через каждый из

298
00:09:46,959 --> 00:09:48,640
слоев, так что

299
00:09:48,640 --> 00:09:50,959
в этой модели есть много изученных параметров, особенно если у

300
00:09:50,959 --> 00:09:55,360
вас есть 12 или 24 головы внимания

301
00:09:55,680 --> 00:09:57,279
на данный момент, я надеюсь, что теперь вы можете

302
00:09:57,279 --> 00:09:59,440
плодотворно вернуться к исходной

303
00:09:59,440 --> 00:10:01,360
vasonia все бумаги и вот  хорошо на их

304
00:10:01,360 --> 00:10:03,519
диаграмме модели и получить больше от этого для

305
00:10:03,519 --> 00:10:05,519
меня, это своего рода гиперсжатие, но теперь

306
00:10:05,519 --> 00:10:07,200
, когда мы глубоко погрузились во все

307
00:10:07,200 --> 00:10:09,040
части, я думаю, что это служит своего рода

308
00:10:09,040 --> 00:10:11,120
полезным сокращением того, как все части

309
00:10:11,120 --> 00:10:12,640
подходят друг к другу, так что  давайте просто сделаем это

310
00:10:12,640 --> 00:10:14,880
быстро, так как у нас есть позиционные

311
00:10:14,880 --> 00:10:17,440
кодировки и вложения входных слов, и

312
00:10:17,440 --> 00:10:18,959
они складываются, чтобы дать нам

313
00:10:18,959 --> 00:10:20,720
интуитивное представление о вложении в этой

314
00:10:20,720 --> 00:10:21,839
модели

315
00:10:21,839 --> 00:10:23,519
, за которым следует слой внимания,

316
00:10:23,519 --> 00:10:25,760
как мы обсуждали, и он имеет остаточную

317
00:10:25,760 --> 00:10:27,839
связь здесь с этим

318
00:10:27,839 --> 00:10:30,399
часть нормализации уровня, которая подается в

319
00:10:30,399 --> 00:10:32,079
блоки прямой связи, за

320
00:10:32,079 --> 00:10:34,480
которой следует такой же процесс, как выпадение

321
00:10:34,480 --> 00:10:35,279
и

322
00:10:35,279 --> 00:10:37,040
нормализация уровня,

323
00:10:37,040 --> 00:10:39,200
и это, по сути, означает, что каждый

324
00:10:39,200 --> 00:10:41,600
из них повторяется для каждого шага в

325
00:10:41,600 --> 00:10:43,440
процессе кодирования каждого из

326
00:10:43,440 --> 00:10:46,320
столбцов, которые мы  посмотрел

327
00:10:46,320 --> 00:10:48,800
справа, и если потому, что в документе

328
00:10:48,800 --> 00:10:50,560
они работают с моделью декодера кодировщика,

329
00:10:50,560 --> 00:10:52,720
каждое состояние декодера самообслуживается

330
00:10:52,720 --> 00:10:54,640
со всеми другими состояниями декодера

331
00:10:54,640 --> 00:10:56,560
и со всеми o  Если состояния кодировщика так

332
00:10:56,560 --> 00:10:58,480
правильны, представьте себе эту двойную последовательность,

333
00:10:58,480 --> 00:11:00,079
у нас есть плотная серия потенциальных

334
00:11:00,079 --> 00:11:02,399
соединений соединений между обеими

335
00:11:02,399 --> 00:11:05,279
частями представления

336
00:11:05,279 --> 00:11:06,720
с правой стороны, когда мы снова делаем

337
00:11:06,720 --> 00:11:09,519
декодирование, мы повторяем этот блок для

338
00:11:09,519 --> 00:11:11,360
каждого состояния декодера,

339
00:11:11,360 --> 00:11:13,760
и если каждое состояние там  имеет вывод,

340
00:11:13,760 --> 00:11:15,839
как для машинного перевода или какого-либо

341
00:11:15,839 --> 00:11:17,760
процесса генерации, тогда у нас будет что-

342
00:11:17,760 --> 00:11:19,600
то вроде этого выходного стека в

343
00:11:19,600 --> 00:11:22,160
каждом из этих выходных состояний, если,

344
00:11:22,160 --> 00:11:24,160
напротив, мы делаем что-то вроде nli

345
00:11:24,160 --> 00:11:25,839
или сентимент, это проблема классификации,

346
00:11:25,839 --> 00:11:27,920
может быть, только одна из  эти состояния

347
00:11:27,920 --> 00:11:30,959
будут иметь один из этих выходов,

348
00:11:30,959 --> 00:11:32,399
а затем внимание становится немного

349
00:11:32,399 --> 00:11:34,560
сложнее, если вы выполняете декодирование

350
00:11:34,560 --> 00:11:36,000
для такой модели, как генерация естественного языка

351
00:11:36,000 --> 00:11:37,920
или машинный перевод в

352
00:11:37,920 --> 00:11:40,399
декодере, который вы не можете посещать в будущем,

353
00:11:40,399 --> 00:11:42,480
как вы делаете  поколений, так что есть

354
00:11:42,480 --> 00:11:44,079
процесс маскировки, который

355
00:11:44,079 --> 00:11:46,240
ограничивает внимание к предыдущим словам

356
00:11:46,240 --> 00:11:50,120
в последовательности, которую вы создаете

