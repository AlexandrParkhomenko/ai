1
00:00:05,600 --> 00:00:07,200
приветствуем всех, это шестая часть

2
00:00:07,200 --> 00:00:08,720
нашей серии статей о контекстных представлениях слов,

3
00:00:08,720 --> 00:00:09,840
мы собираемся

4
00:00:09,840 --> 00:00:11,759
поговорить о практической тонкой настройке, пришло

5
00:00:11,759 --> 00:00:13,440
время заняться этими

6
00:00:13,440 --> 00:00:15,519
параметрами, о которых мы говорили, так что

7
00:00:15,519 --> 00:00:17,279
вот основная идея, которую

8
00:00:17,279 --> 00:00:18,960
говорит ваша существующая архитектура.  для

9
00:00:18,960 --> 00:00:20,720
текущей исходной системы и выпекания,

10
00:00:20,720 --> 00:00:23,119
вероятно, может выиграть от контекстного

11
00:00:23,119 --> 00:00:25,199
представления, мы видели, что во

12
00:00:25,199 --> 00:00:27,840
многих контекстах в nlu в наши дни

13
00:00:27,840 --> 00:00:29,840
тонкая настройка ноутбука показывает вам, как

14
00:00:29,840 --> 00:00:31,679
ввести представления преобразователя

15
00:00:31,679 --> 00:00:33,440
двумя способами: сначала с простой

16
00:00:33,440 --> 00:00:35,920
характеристикой, а затем с полным  о точной

17
00:00:35,920 --> 00:00:37,280
настройке, и я собираюсь поговорить об обоих

18
00:00:37,280 --> 00:00:39,600
из них в скринкасте. Суть

19
00:00:39,600 --> 00:00:41,280
этой идеи заключается в том, что,

20
00:00:41,280 --> 00:00:43,920
расширяя существующие модули pi torch из

21
00:00:43,920 --> 00:00:46,399
дистрибутива кода курса, вы можете

22
00:00:46,399 --> 00:00:47,920
очень легко создавать настраиваемые

23
00:00:47,920 --> 00:00:49,840
модели точной настройки всего несколькими строками.

24
00:00:49,840 --> 00:00:50,879
кода,

25
00:00:50,879 --> 00:00:52,160
и это должно быть действительно полезно с

26
00:00:52,160 --> 00:00:53,680
точки зрения изучения множества различных

27
00:00:53,680 --> 00:00:55,760
дизайнов и понимания того, как лучше всего использовать эти

28
00:00:55,760 --> 00:00:58,079
параметры для вашего p  проблема, я просто

29
00:00:58,079 --> 00:01:00,160
хочу упомянуть, что на самом деле это

30
00:01:00,160 --> 00:01:02,160
возможно только из-за потрясающей

31
00:01:02,160 --> 00:01:03,840
работы, которую проделала команда обнимающего лица,

32
00:01:03,840 --> 00:01:05,680
чтобы сделать эти параметры доступными для

33
00:01:05,680 --> 00:01:07,760
всех нас,

34
00:01:07,760 --> 00:01:09,840
поэтому давайте начнем с простой

35
00:01:09,840 --> 00:01:11,280
футуризации и на самом деле хотим вернуться к нашему

36
00:01:11,280 --> 00:01:13,040
обсуждению.  рекуррентных нейронных сетей

37
00:01:13,040 --> 00:01:14,960
и подумать о том, как мы представляем

38
00:01:14,960 --> 00:01:17,119
примеры для этих моделей в

39
00:01:17,119 --> 00:01:19,200
стандартном режиме, у нас есть в качестве примеров

40
00:01:19,200 --> 00:01:21,280
списки токенов, здесь

41
00:01:21,280 --> 00:01:23,439
мы конвертируем их в списки индексов,

42
00:01:23,439 --> 00:01:25,840
и эти индексы помогают нам искать векторные

43
00:01:25,840 --> 00:01:28,080
представления этих слов в некотором

44
00:01:28,080 --> 00:01:30,240
фиксированном встраивании  пространстве, и результатом

45
00:01:30,240 --> 00:01:32,640
этого является то, что каждый пример

46
00:01:32,640 --> 00:01:34,560
представлен списком векторов, и это

47
00:01:34,560 --> 00:01:36,640
важно иметь в виду, мы склонны

48
00:01:36,640 --> 00:01:38,479
думать о модели как о

49
00:01:38,479 --> 00:01:40,880
входных данных списков токенов и наличии

50
00:01:40,880 --> 00:01:42,799
встраивания, но с точки зрения

51
00:01:42,799 --> 00:01:44,640
сама модель, которую она действительно хочет

52
00:01:44,640 --> 00:01:47,439
обрабатывать в качестве входных списков векторов, и

53
00:01:47,439 --> 00:01:49,680
это расширяющая возможности идея, потому что, если мы

54
00:01:49,680 --> 00:01:51,439
используем ее в фиксированном встраивании, конечно,

55
00:01:51,439 --> 00:01:53,360
тогда  Два вхождения a будут одним и тем

56
00:01:53,360 --> 00:01:54,399
же вектором,

57
00:01:54,399 --> 00:01:56,159
и эти два вхождения b в

58
00:01:56,159 --> 00:01:58,320
примерах будут одним и тем же вектором,

59
00:01:58,320 --> 00:01:59,920
но модель на самом деле не заботится о том, чтобы

60
00:01:59,920 --> 00:02:01,920
они были одним и тем же вектором, который мы могли бы, если бы

61
00:02:01,920 --> 00:02:04,000
захотели напрямую преобразовать

62
00:02:04,000 --> 00:02:07,119
последовательности токенов в  списки векторов с использованием такого

63
00:02:07,119 --> 00:02:09,280
устройства, как модель vert, и это позволило бы,

64
00:02:09,280 --> 00:02:11,520
чтобы a в первой позиции и a

65
00:02:11,520 --> 00:02:13,120
в третьей могли соответствовать

66
00:02:13,120 --> 00:02:15,360
разным векторам или b в этих двух

67
00:02:15,360 --> 00:02:17,280
примерах могли бы соответствовать разным

68
00:02:17,280 --> 00:02:19,040
векторам, которые были бы

69
00:02:19,040 --> 00:02:22,000
частью контекстуального представления этих  модели, и

70
00:02:22,000 --> 00:02:23,520
опять же, с точки зрения rnn,

71
00:02:23,520 --> 00:02:25,280
мы можем передать их косвенно, это

72
00:02:25,280 --> 00:02:27,040
просто, это полный

73
00:02:27,040 --> 00:02:30,400
рецепт для того, чтобы сделать это, используя код sst

74
00:02:30,400 --> 00:02:32,080
и модули pry torch из этого

75
00:02:32,080 --> 00:02:33,920
дистрибутива кода курса, чтобы вы могли видеть,

76
00:02:33,920 --> 00:02:34,800
что

77
00:02:34,800 --> 00:02:36,879
помимо материала установки, который  мы

78
00:02:36,879 --> 00:02:39,440
сделали несколько раз, функция признаков просто

79
00:02:39,440 --> 00:02:41,840
будет использовать функциональность vert для

80
00:02:41,840 --> 00:02:43,920
поиска индексов примеров, а затем преобразовать

81
00:02:43,920 --> 00:02:45,760
их в векторные представления и  d

82
00:02:45,760 --> 00:02:48,800
здесь в качестве резюме мы собираемся использовать

83
00:02:48,800 --> 00:02:51,040
представление над токеном класса, но

84
00:02:51,040 --> 00:02:52,400
в этот момент возможно многое,

85
00:02:52,400 --> 00:02:53,360


86
00:02:53,360 --> 00:02:54,959
а затем, когда у нас есть наша модель-оболочка,

87
00:02:54,959 --> 00:02:57,120
мы настраиваем классификатор torch rnn,

88
00:02:57,120 --> 00:02:58,560
и это всего лишь две вещи.  обратите внимание,

89
00:02:58,560 --> 00:03:01,120
сначала мы говорим, что использование встраивания равно false,

90
00:03:01,120 --> 00:03:02,480
потому что мы собираемся передавать векторы

91
00:03:02,480 --> 00:03:04,480
косвенно, здесь нет встраивания,

92
00:03:04,480 --> 00:03:06,400
и нам также не нужен

93
00:03:06,400 --> 00:03:08,319
словарь, который вы могли бы указать, но

94
00:03:08,319 --> 00:03:10,239
он не задействован, потому что, по сути,

95
00:03:10,239 --> 00:03:12,560
модель снова имеет дело непосредственно с

96
00:03:12,560 --> 00:03:14,159
векторами

97
00:03:14,159 --> 00:03:16,159
а затем в эксперименте с ssd вы снова говорите, что

98
00:03:16,159 --> 00:03:18,720
векторизация равна false, и это

99
00:03:18,720 --> 00:03:21,120
полный рецепт для ввода

100
00:03:21,120 --> 00:03:24,319
представлений bert со стандартным rnn,

101
00:03:24,319 --> 00:03:26,159
хотя это не совсем тонкая настройка, поэтому

102
00:03:26,159 --> 00:03:27,760
давайте подумаем о том, как мы могли бы получить дополнительные

103
00:03:27,760 --> 00:03:29,840
преимущества от фактического обновления этих

104
00:03:29,840 --> 00:03:31,760
параметров  вместо того, чтобы просто использовать их в

105
00:03:31,760 --> 00:03:34,319
качестве входных данных замороженных представлений для

106
00:03:34,319 --> 00:03:36,480
другой модели,

107
00:03:36,480 --> 00:03:38,159
я бы посоветовал вам подумать

108
00:03:38,159 --> 00:03:40,400
о подклассах включенных модулей pi torch.

109
00:03:40,400 --> 00:03:42,000
в нашем распределении кода курса,

110
00:03:42,000 --> 00:03:44,080
потому что тогда вы

111
00:03:44,080 --> 00:03:46,239
сможете писать код, ориентированный

112
00:03:46,239 --> 00:03:48,319
на архитектуру вашей модели, и

113
00:03:48,319 --> 00:03:50,239
многие детали оптимизации и

114
00:03:50,239 --> 00:03:52,480
обработки данных будут обработаны за вас.

115
00:03:52,480 --> 00:03:54,159
Я надеюсь, что это яркий пример того,

116
00:03:54,159 --> 00:03:56,159
что он исходит из  учебник Pytorch Models Notebook

117
00:03:56,159 --> 00:03:57,599


118
00:03:57,599 --> 00:04:00,000
это классификатор torch softmax, и

119
00:04:00,000 --> 00:04:01,760
единственное, что нам нужно сделать, это переписать эту

120
00:04:01,760 --> 00:04:04,159
функцию графа сборки, чтобы указать один

121
00:04:04,159 --> 00:04:06,959
единственный слой плотного слоя, который мы используем в качестве

122
00:04:06,959 --> 00:04:08,640
нашего базового класса, нейронный классификатор оболочки torch,

123
00:04:08,640 --> 00:04:10,560
который обрабатывает все остальное,

124
00:04:10,560 --> 00:04:12,000
связанное с настройкой этого  модель и

125
00:04:12,000 --> 00:04:14,959
оптимизировать ее, если бы мы хотели пойти в

126
00:04:14,959 --> 00:04:16,560
другом направлении и вместо этого подогнать действительно

127
00:04:16,560 --> 00:04:18,399
глубокую модель, мы могли бы снова начать с

128
00:04:18,399 --> 00:04:20,639
мелкого нейронного классификатора и

129
00:04:20,639 --> 00:04:22,320
переписать функцию построения графика, чтобы

130
00:04:22,320 --> 00:04:24,240
она просто имела больше слоев, а

131
00:04:24,240 --> 00:04:25,680
затем то, что происходит в этом init

132
00:04:25,680 --> 00:04:27,600
метод заключается в том, что мы просто даем пользователю

133
00:04:27,600 --> 00:04:29,680
доступ к различным гиперпараметрам,

134
00:04:29,680 --> 00:04:31,520
которые они могут выбрать для окончательной настройки этой

135
00:04:31,520 --> 00:04:32,960
модели.

136
00:04:32,960 --> 00:04:35,120
Вот более сложный пример. В

137
00:04:35,120 --> 00:04:37,280
этом мы начинаем с модуля pi torch nn, который

138
00:04:37,280 --> 00:04:39,199
идет вниз к

139
00:04:39,199 --> 00:04:41,199
основанию, здесь это модель линейной регрессии факела,

140
00:04:41,199 --> 00:04:42,400


141
00:04:42,400 --> 00:04:44,000
здесь мы устанавливаем параметры веса, а

142
00:04:44,000 --> 00:04:45,600
затем у нас есть единственный прямой проход,

143
00:04:45,600 --> 00:04:47,759
который соответствует  структура

144
00:04:47,759 --> 00:04:49,759
простой линейной регрессии

145
00:04:49,759 --> 00:04:50,639
теперь

146
00:04:50,639 --> 00:04:52,479
для фактического интерфейса, нам нужно проделать

147
00:04:52,479 --> 00:04:54,479
немного больше работы здесь, поэтому

148
00:04:54,479 --> 00:04:56,080
мы настраиваем потери так, чтобы они

149
00:04:56,080 --> 00:04:58,080
подходили для модели регрессии, в

150
00:04:58,080 --> 00:04:59,600
отличие от классификаторов, которые мы

151
00:04:59,600 --> 00:05:02,000
рассматривали до сих пор  построить график просто

152
00:05:02,000 --> 00:05:03,919
использует модуль nn, который я показал вам

153
00:05:03,919 --> 00:05:06,000
секунду назад, нам нужно немного

154
00:05:06,000 --> 00:05:08,240
поработать над переписыванием набора данных сборки, чтобы

155
00:05:08,240 --> 00:05:10,240
мы правильно обрабатывали данные линейной регрессии,

156
00:05:10,240 --> 00:05:11,360


157
00:05:11,360 --> 00:05:12,720
а затем нам нужно переписать

158
00:05:12,720 --> 00:05:14,479
функции прогнозирования и оценки, чтобы они были  своего

159
00:05:14,479 --> 00:05:16,240
рода хорошие граждане кодовой базы, которые

160
00:05:16,240 --> 00:05:18,560
позволяют оптимизировать гиперпараметры

161
00:05:18,560 --> 00:05:20,320
, перекрестную проверку и т. д., но

162
00:05:20,320 --> 00:05:21,600
это опять же просто и в

163
00:05:21,600 --> 00:05:23,440
основном для прогнозирования, мы

164
00:05:23,440 --> 00:05:24,960
фактически используем базу  классы

165
00:05:24,960 --> 00:05:27,120
подчеркивают метод прогнозирования для тяжелой работы

166
00:05:27,120 --> 00:05:29,199
здесь, а затем оценка, конечно

167
00:05:29,199 --> 00:05:30,720
, просто переводит нас из режима

168
00:05:30,720 --> 00:05:33,039
оценки классификаторов в

169
00:05:33,039 --> 00:05:35,440
режим оценки моделей регрессии, это

170
00:05:35,440 --> 00:05:36,400
все, что вам нужно сделать, и снова

171
00:05:36,400 --> 00:05:38,479
явно отсутствует в этом

172
00:05:38,479 --> 00:05:40,720
большинство аспектов  обработки данных и

173
00:05:40,720 --> 00:05:42,639
всех деталей оптимизации.

174
00:05:42,639 --> 00:05:45,680
База модели факела базового класса здесь имеет

175
00:05:45,680 --> 00:05:47,680
очень полнофункциональный метод подбора, который вы

176
00:05:47,680 --> 00:05:49,440
можете использовать для оптимизации этих моделей и

177
00:05:49,440 --> 00:05:52,240
исследования гиперпараметров,

178
00:05:52,240 --> 00:05:53,919
и это приводит нас к звезде

179
00:05:53,919 --> 00:05:55,759
шоу, которая  Будьте осторожны с тонкой настройкой

180
00:05:55,759 --> 00:05:57,919
с обнимающими параметрами лица здесь, мы

181
00:05:57,919 --> 00:06:00,800
начнем с pi torch nn.module, который

182
00:06:00,800 --> 00:06:03,199
мы загружаем в модуль модели vert, как мы

183
00:06:03,199 --> 00:06:04,960
делали раньше, и обязательно настраиваем его для

184
00:06:04,960 --> 00:06:07,280
обучения, чтобы его можно было обновить,

185
00:06:07,280 --> 00:06:08,720
а затем  новые параметры здесь на

186
00:06:08,720 --> 00:06:10,240
самом деле просто этот слой классификатора

187
00:06:10,240 --> 00:06:12,080
плотный слой, который будет ориентирован

188
00:06:12,080 --> 00:06:13,759
на структуру классификации, которую

189
00:06:13,759 --> 00:06:15,440
мы хотим, на то, что мы хотим, чтобы наша модель

190
00:06:15,440 --> 00:06:16,400


191
00:06:16,400 --> 00:06:18,160
имела прямой метод cal  ls прямой

192
00:06:18,160 --> 00:06:19,600
метод модели отрыжки, и вы получаете

193
00:06:19,600 --> 00:06:21,360
кучу представлений, здесь есть

194
00:06:21,360 --> 00:06:23,600
много вариантов, что я решил сделать,

195
00:06:23,600 --> 00:06:25,440
это просто использовать вывод пула на основе объятий,

196
00:06:25,440 --> 00:06:27,360
который представляет собой некоторые параметры

197
00:06:27,360 --> 00:06:29,840
поверх токена класса в качестве входных данных  к

198
00:06:29,840 --> 00:06:31,199
классификатору,

199
00:06:31,199 --> 00:06:33,280
и когда мы оптимизируем эту модель, если

200
00:06:33,280 --> 00:06:35,360
повезет, продуктивным образом, будут обновлены не только

201
00:06:35,360 --> 00:06:37,199
эти параметры классификатора,

202
00:06:37,199 --> 00:06:39,039
но и все параметры модели рывка,

203
00:06:39,039 --> 00:06:42,000
которую вы загрузили в режиме обучения,

204
00:06:42,000 --> 00:06:44,000
интерфейс здесь немного задействован,

205
00:06:44,000 --> 00:06:46,080
так что мы  сделать, это предоставить

206
00:06:46,080 --> 00:06:48,240
пользователю некоторую гибкость в отношении того, какие варианты

207
00:06:48,240 --> 00:06:49,199


208
00:06:49,199 --> 00:06:50,880
построения графика снова просто загружаются в

209
00:06:50,880 --> 00:06:52,479
модуль, который я показал вам всего секунду

210
00:06:52,479 --> 00:06:55,039
назад, а затем строить набор данных немного

211
00:06:55,039 --> 00:06:57,280


212
00:06:57,280 --> 00:07:00,000
сложнее, но в основном мы используем токенизатор burp для  пакетно кодируем

213
00:07:00,000 --> 00:07:01,120
наши данные,

214
00:07:01,120 --> 00:07:02,160
а затем мы немного

215
00:07:02,160 --> 00:07:04,160
обрабатываем выходные метки, чтобы убедиться,

216
00:07:04,160 --> 00:07:06,319
что pi torch может понять их

217
00:07:06,319 --> 00:07:07,759
, это действительно так, и суть этого

218
00:07:07,759 --> 00:07:09,680
заключается в том, что мы снова используем обнимание

219
00:07:09,680 --> 00:07:12,319
функциональность лица для представления наших данных

220
00:07:12,319 --> 00:07:14,560
в модели отрыжки, а затем это

221
00:07:14,560 --> 00:07:16,400
действительно интересная часть,

222
00:07:16,400 --> 00:07:18,160
вызывающая прямой метод, а затем

223
00:07:18,160 --> 00:07:20,160
подгоняющая классификатор сверху — это

224
00:07:20,160 --> 00:07:22,000
почти все, что вам нужно сделать, и, конечно,

225
00:07:22,000 --> 00:07:24,080
это открывает мир вариантов, которые

226
00:07:24,080 --> 00:07:25,599
здесь есть у представителей.  множество других вещей, которые вы

227
00:07:25,599 --> 00:07:27,280
могли бы использовать в качестве входных данных для этого

228
00:07:27,280 --> 00:07:29,039
слоя классификатора,

229
00:07:29,039 --> 00:07:30,479
и многие из них на самом деле могут быть более

230
00:07:30,479 --> 00:07:32,160
продуктивными, чем простой подход, который

231
00:07:32,160 --> 00:07:35,639
я использовал здесь.

