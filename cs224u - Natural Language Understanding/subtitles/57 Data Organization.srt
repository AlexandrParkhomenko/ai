1
00:00:00,000 --> 00:00:04,148


2
00:00:04,148 --> 00:00:05,690
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,690 --> 00:00:07,900
This is part 4 in our series
on methods and metrics.

4
00:00:07,900 --> 00:00:09,358
We're going to be
talking about how

5
00:00:09,358 --> 00:00:12,070
we organize datasets for
the purposes of conducting

6
00:00:12,070 --> 00:00:14,350
evaluations in NLP.

7
00:00:14,350 --> 00:00:17,770
Let's begin with the classic
train/dev/test split.

8
00:00:17,770 --> 00:00:20,140
This is a very common
format for datasets

9
00:00:20,140 --> 00:00:22,120
in our field, especially
for the very large

10
00:00:22,120 --> 00:00:24,040
publicly available ones.

11
00:00:24,040 --> 00:00:25,450
And it's really
good in the sense

12
00:00:25,450 --> 00:00:28,930
that in releasing datasets
with these splits predefined,

13
00:00:28,930 --> 00:00:31,450
we do ensure some consistency
across the different

14
00:00:31,450 --> 00:00:33,730
evaluations that people run.

15
00:00:33,730 --> 00:00:36,820
It does presuppose that you
have a fairly large datasets

16
00:00:36,820 --> 00:00:39,100
because after all,
right from the get go,

17
00:00:39,100 --> 00:00:41,755
you are setting aside a whole
lot of examples in the dev

18
00:00:41,755 --> 00:00:43,690
and test splits
that you can't use

19
00:00:43,690 --> 00:00:45,410
at all to train your systems.

20
00:00:45,410 --> 00:00:48,250
So even though your system might
benefit from those examples,

21
00:00:48,250 --> 00:00:49,780
you can't use them
in that context.

22
00:00:49,780 --> 00:00:51,860
They can be used
only for evaluations.

23
00:00:51,860 --> 00:00:56,500
You're just giving up a lot of
potentially useful examples.

24
00:00:56,500 --> 00:00:59,348
As we've discussed many times,
we're all on the honor system

25
00:00:59,348 --> 00:01:00,640
when it comes to that test set.

26
00:01:00,640 --> 00:01:02,620
It's distributed as
part of the data set,

27
00:01:02,620 --> 00:01:04,750
but it has a privileged status.

28
00:01:04,750 --> 00:01:08,320
The test set can be used only
once all of system development

29
00:01:08,320 --> 00:01:12,520
is complete, and then you do a
single evaluation on the test

30
00:01:12,520 --> 00:01:15,250
set and report that number
completely hands off.

31
00:01:15,250 --> 00:01:17,860
This is vital for our field
because it's the only way

32
00:01:17,860 --> 00:01:20,860
that we can even hope to
get a true picture of how

33
00:01:20,860 --> 00:01:25,210
our systems are truly
generalizing to new examples.

34
00:01:25,210 --> 00:01:29,020
That said, the downside
of having predefined

35
00:01:29,020 --> 00:01:32,050
train/dev/test splits is
that inevitably everyone

36
00:01:32,050 --> 00:01:35,170
is using those same
dev and test sets.

37
00:01:35,170 --> 00:01:37,450
And what that means
is that over time

38
00:01:37,450 --> 00:01:41,320
as we see consistent
progress on a benchmark test,

39
00:01:41,320 --> 00:01:42,880
we're taking that
same measurement

40
00:01:42,880 --> 00:01:44,020
on that same test set.

41
00:01:44,020 --> 00:01:45,850
And it can be hard to
be sure whether we're

42
00:01:45,850 --> 00:01:47,860
seeing true progress
on the underlying

43
00:01:47,860 --> 00:01:51,790
task or the result of a
lot of implicit lessons

44
00:01:51,790 --> 00:01:53,620
that people have
learned about what works

45
00:01:53,620 --> 00:01:56,590
and what doesn't for
that particular test set.

46
00:01:56,590 --> 00:01:59,950
And that's true even if everyone
is obeying that honor code

47
00:01:59,950 --> 00:02:03,430
and using the test set only
for truly final evaluations.

48
00:02:03,430 --> 00:02:06,130
Nonetheless, information
can leak out,

49
00:02:06,130 --> 00:02:09,562
and we might start to
mistake true progress when

50
00:02:09,562 --> 00:02:11,020
we're actually just
seeing progress

51
00:02:11,020 --> 00:02:12,468
on that particular test set.

52
00:02:12,468 --> 00:02:14,260
And I think the only
way that we can really

53
00:02:14,260 --> 00:02:16,690
combat this is by
continually setting

54
00:02:16,690 --> 00:02:20,350
new benchmark tests for
ourselves with new test sets

55
00:02:20,350 --> 00:02:22,120
so that we see how
systems perform

56
00:02:22,120 --> 00:02:25,920
in truly unseen environments.

57
00:02:25,920 --> 00:02:28,980
As you leave NLP, it's
common to find datasets

58
00:02:28,980 --> 00:02:31,950
that don't come with that
predefined train/dev/test

59
00:02:31,950 --> 00:02:34,790
split, and that poses some
methodological questions

60
00:02:34,790 --> 00:02:35,290
for you.

61
00:02:35,290 --> 00:02:37,560
This is especially true
for small public data

62
00:02:37,560 --> 00:02:39,720
sets that you see out there.

63
00:02:39,720 --> 00:02:41,670
And this poses a
challenge for assessment.

64
00:02:41,670 --> 00:02:43,740
For robust
comparisons, you really

65
00:02:43,740 --> 00:02:47,020
have to run all your models
using your same assessment

66
00:02:47,020 --> 00:02:47,520
regime.

67
00:02:47,520 --> 00:02:49,590
That is, the same splits.

68
00:02:49,590 --> 00:02:52,320
And that's especially important
if the dataset is small

69
00:02:52,320 --> 00:02:53,975
because, of course,
in a small dataset,

70
00:02:53,975 --> 00:02:55,350
you're probably
going to get more

71
00:02:55,350 --> 00:02:57,630
variance across different runs.

72
00:02:57,630 --> 00:02:59,610
And this can make it
really hard to compare

73
00:02:59,610 --> 00:03:01,980
outside of the experimental
work that you're doing.

74
00:03:01,980 --> 00:03:06,120
If someone has published the
results of some random 70/30

75
00:03:06,120 --> 00:03:11,160
train-test split, unless you can
reconstruct exactly the splits

76
00:03:11,160 --> 00:03:13,020
that they used, it
might be unclear

77
00:03:13,020 --> 00:03:15,850
whether you're doing a true
apples-to-apples comparison.

78
00:03:15,850 --> 00:03:17,400
So that's something
to keep in mind.

79
00:03:17,400 --> 00:03:20,790
And it does mean that if you
can for your own experiments,

80
00:03:20,790 --> 00:03:24,240
you might impose a split right
at the start of your project.

81
00:03:24,240 --> 00:03:27,030
This is probably feasible
if the data set is large.

82
00:03:27,030 --> 00:03:29,460
And what it will mean is
that you have a simplified

83
00:03:29,460 --> 00:03:33,690
experimental setup, and you
have to do less hyperparameter

84
00:03:33,690 --> 00:03:35,340
optimization just
because there are

85
00:03:35,340 --> 00:03:39,480
fewer moving parts in your
underlying experimental setup.

86
00:03:39,480 --> 00:03:41,982
It does presuppose that you
have a pretty large data set

87
00:03:41,982 --> 00:03:43,440
because, as I said
before, you have

88
00:03:43,440 --> 00:03:46,650
to give up a whole bunch of
examples to dev and test.

89
00:03:46,650 --> 00:03:49,710
But it will simplify other
aspects of your project

90
00:03:49,710 --> 00:03:51,330
if it's feasible.

91
00:03:51,330 --> 00:03:53,580
For small datasets
though, imposing a split

92
00:03:53,580 --> 00:03:55,830
might leave too
little data, leading

93
00:03:55,830 --> 00:03:57,455
to highly variable performance.

94
00:03:57,455 --> 00:03:59,580
And in that context, if
that's the kind of behavior

95
00:03:59,580 --> 00:04:01,590
that you observe, you
might want to move

96
00:04:01,590 --> 00:04:04,600
into the mode of
cross-validation.

97
00:04:04,600 --> 00:04:06,450
So cross-validation,
in this context

98
00:04:06,450 --> 00:04:09,420
we take a set of examples,
say our entire data set,

99
00:04:09,420 --> 00:04:13,590
and we partition them into
two or more train-test splits.

100
00:04:13,590 --> 00:04:15,510
And you might do that
repeatedly and then

101
00:04:15,510 --> 00:04:18,209
average over the results
of evaluations on those

102
00:04:18,209 --> 00:04:21,600
splits in some way to give
a holistic summary of system

103
00:04:21,600 --> 00:04:22,360
performance.

104
00:04:22,360 --> 00:04:25,020
And in that way, even
as those numbers vary--

105
00:04:25,020 --> 00:04:26,610
they might have a
lot of variance--

106
00:04:26,610 --> 00:04:28,110
we're still getting
in the average,

107
00:04:28,110 --> 00:04:30,000
we hope, a pretty
reliable measure

108
00:04:30,000 --> 00:04:33,780
of how the system performs in
general on the available data.

109
00:04:33,780 --> 00:04:35,280
And I'm going to
talk about two ways

110
00:04:35,280 --> 00:04:38,040
to do cross-validation,
each with its own strengths

111
00:04:38,040 --> 00:04:39,280
and weaknesses.

112
00:04:39,280 --> 00:04:41,710
Let's begin with what I
call random splits here.

113
00:04:41,710 --> 00:04:45,000
So under the random splits
regime, you take your dataset.

114
00:04:45,000 --> 00:04:49,500
And let's say k times, you
shuffle it, and you split it.

115
00:04:49,500 --> 00:04:52,320
And you have t% for train
then probably the rest

116
00:04:52,320 --> 00:04:53,760
left out for test.

117
00:04:53,760 --> 00:04:55,320
And on each one of
those splits, you

118
00:04:55,320 --> 00:04:59,070
conduct some kind of evaluation,
get back your metrics.

119
00:04:59,070 --> 00:05:01,860
And then at the end of
all these k evaluations,

120
00:05:01,860 --> 00:05:03,600
you probably average
those metrics

121
00:05:03,600 --> 00:05:06,480
in some way to give a
single summary number

122
00:05:06,480 --> 00:05:09,030
for system performance.

123
00:05:09,030 --> 00:05:11,550
In general, but not always,
when we do these splits,

124
00:05:11,550 --> 00:05:14,460
we want them to be
stratified in the sense

125
00:05:14,460 --> 00:05:17,340
that the train and test splits
should have approximately

126
00:05:17,340 --> 00:05:22,080
the same distribution over the
classes in the underlying data.

127
00:05:22,080 --> 00:05:24,770
But I have been careful to say
that this is not always true.

128
00:05:24,770 --> 00:05:26,490
There could, for
example, be contexts

129
00:05:26,490 --> 00:05:29,070
in which you would like
your test set to stress test

130
00:05:29,070 --> 00:05:31,740
your system by having a very
different distribution, maybe

131
00:05:31,740 --> 00:05:34,800
an even distribution, or
one that's heavily skewed

132
00:05:34,800 --> 00:05:37,950
towards some of the smaller
but more important classes.

133
00:05:37,950 --> 00:05:41,730
And that will pose a challenge
for train-test regimes

134
00:05:41,730 --> 00:05:44,580
because the system's
experiences at train time

135
00:05:44,580 --> 00:05:47,550
will be different in this high
level distributional sense

136
00:05:47,550 --> 00:05:49,260
from what it sees at test time.

137
00:05:49,260 --> 00:05:51,090
But that, of course,
might be part

138
00:05:51,090 --> 00:05:52,950
of what you're trying
to pursue as part

139
00:05:52,950 --> 00:05:56,650
of your overall hypothesis.

140
00:05:56,650 --> 00:05:59,470
The trade-offs for this kind
of cross-validation-- the good

141
00:05:59,470 --> 00:06:01,510
is that you can
create as many splits

142
00:06:01,510 --> 00:06:04,450
as you want without having this
impact the ratio of training

143
00:06:04,450 --> 00:06:07,510
to testing examples, right,
because k times-- we're just

144
00:06:07,510 --> 00:06:09,130
going to do a random split.

145
00:06:09,130 --> 00:06:11,740
And it can be consistent
that we do independent

146
00:06:11,740 --> 00:06:16,150
of k 70% train,
30% test or 50-50,

147
00:06:16,150 --> 00:06:18,400
or whatever we
decide we want that's

148
00:06:18,400 --> 00:06:21,430
independent of the number
of splits that we set.

149
00:06:21,430 --> 00:06:23,980
The bad of this, of course,
is that there's no guarantee

150
00:06:23,980 --> 00:06:27,370
that every example will be
used the same number of times

151
00:06:27,370 --> 00:06:29,230
for training and for testing.

152
00:06:29,230 --> 00:06:31,850
And for small data sets,
this could, of course,

153
00:06:31,850 --> 00:06:34,360
be a concern because
you might be introducing

154
00:06:34,360 --> 00:06:39,190
unwanted correlations across
the splits in, for example,

155
00:06:39,190 --> 00:06:41,200
never having certain
hard examples be

156
00:06:41,200 --> 00:06:43,750
part of your test set just
as a matter of chance.

157
00:06:43,750 --> 00:06:45,750
So that's something
to keep in mind.

158
00:06:45,750 --> 00:06:47,565
But of course, for
very large data sets,

159
00:06:47,565 --> 00:06:48,940
it's very unlikely
that you'll be

160
00:06:48,940 --> 00:06:51,070
susceptible to the
bad part of this,

161
00:06:51,070 --> 00:06:52,810
and then you do get
a lot of the benefits

162
00:06:52,810 --> 00:06:56,050
of the freedom of being able
to run lots of experiments

163
00:06:56,050 --> 00:06:59,350
with a fixed train-test ratio.

164
00:06:59,350 --> 00:07:02,320
And of course, as
usual, scikit has a lot

165
00:07:02,320 --> 00:07:03,970
of tools to help you with this.

166
00:07:03,970 --> 00:07:06,100
So I've just given some
classic examples down

167
00:07:06,100 --> 00:07:08,320
here from the
model_selection package.

168
00:07:08,320 --> 00:07:12,070
You might import ShuffleSplit,
StratifiedShuffleSplit.

169
00:07:12,070 --> 00:07:14,740
And of course, train_test_split
is a useful utility

170
00:07:14,740 --> 00:07:18,130
for very quickly and flexibly
creating splits of your data.

171
00:07:18,130 --> 00:07:22,530
And I make heavy use of
these throughout my own code.

172
00:07:22,530 --> 00:07:24,330
The second reason
for cross-validation

173
00:07:24,330 --> 00:07:27,330
that I'd like to discuss is
K-folds cross-validation.

174
00:07:27,330 --> 00:07:29,550
Here, the method is
slightly different.

175
00:07:29,550 --> 00:07:32,580
We're going to take our dataset
and split it into three folds,

176
00:07:32,580 --> 00:07:34,890
in this case for three-fold
cross-validation.

177
00:07:34,890 --> 00:07:37,980
You could, of course, pick any
fold number that you wanted.

178
00:07:37,980 --> 00:07:40,230
And then, given that it's
three-fold cross-validation,

179
00:07:40,230 --> 00:07:43,110
we're going to conduct three
experiments-- one where fold 1

180
00:07:43,110 --> 00:07:46,350
is used for testing and 2
and 3 are merged together

181
00:07:46,350 --> 00:07:49,560
for training, the second
experiment where we hold out

182
00:07:49,560 --> 00:07:52,680
fold 2 for testing and
the union of 1 and 3

183
00:07:52,680 --> 00:07:55,620
is used for training, and then
finally a third experiment

184
00:07:55,620 --> 00:07:59,280
where fold 3 is used for
testing and folds 1 and 2

185
00:07:59,280 --> 00:08:03,437
are concatenated
for the train set.

186
00:08:03,437 --> 00:08:06,020
The trade-offs here are slightly
different from the trade-offs

187
00:08:06,020 --> 00:08:07,390
for random splits.

188
00:08:07,390 --> 00:08:10,340
So the good of this
is that every example

189
00:08:10,340 --> 00:08:13,670
appears in a train set
exactly k minus 1 times

190
00:08:13,670 --> 00:08:15,470
and in a test set exactly once.

191
00:08:15,470 --> 00:08:18,080
We have that guarantee
in virtue of the fact

192
00:08:18,080 --> 00:08:20,210
that we use a single
split over here

193
00:08:20,210 --> 00:08:23,480
to conduct our three
experimental paradigms.

194
00:08:23,480 --> 00:08:26,660
The bad of this, of course,
can be really difficult.

195
00:08:26,660 --> 00:08:30,170
The size of K is determining
the size of the training test

196
00:08:30,170 --> 00:08:30,860
split.

197
00:08:30,860 --> 00:08:33,500
But just consider that for
three-fold cross-validation,

198
00:08:33,500 --> 00:08:36,558
we're going to use 67%
of the data for training

199
00:08:36,558 --> 00:08:38,629
and 33 for testing.

200
00:08:38,630 --> 00:08:42,169
But if three experiments is not
enough, if we want 10 folds,

201
00:08:42,169 --> 00:08:43,610
the result of that
will be that we

202
00:08:43,610 --> 00:08:47,810
use 90% of our data for
training and 10% for testing.

203
00:08:47,810 --> 00:08:49,190
And the bottom
line is that those

204
00:08:49,190 --> 00:08:51,470
are very different
experimental scenarios

205
00:08:51,470 --> 00:08:53,720
from the point of view of
the amount of training data

206
00:08:53,720 --> 00:08:57,320
that your system has and
probably the variance that you

207
00:08:57,320 --> 00:08:59,120
see in testing
because of the way

208
00:08:59,120 --> 00:09:01,640
you're changing the
size of the test set.

209
00:09:01,640 --> 00:09:04,500
Whereas for the random splits
that we just discussed,

210
00:09:04,500 --> 00:09:06,950
we have an independence
of the number of folds

211
00:09:06,950 --> 00:09:09,740
and the percentage of
train and test examples

212
00:09:09,740 --> 00:09:10,790
that we're going to have.

213
00:09:10,790 --> 00:09:13,520
And that can be very freeing,
especially for large data

214
00:09:13,520 --> 00:09:16,550
sets where the value
up here in the good

215
00:09:16,550 --> 00:09:19,160
is really less pressing.

216
00:09:19,160 --> 00:09:22,135
And, again, scikit-learn
has lots of tools for this.

217
00:09:22,135 --> 00:09:24,260
And I've actually just
given a sample of them here.

218
00:09:24,260 --> 00:09:28,460
You have KFold, StratifiedKFold,
and then cross_val_score

219
00:09:28,460 --> 00:09:31,010
is a nice wrapper utility
that will, again, give you

220
00:09:31,010 --> 00:09:33,470
flexible access to
lots of different ways

221
00:09:33,470 --> 00:09:37,750
of conceptualizing
K-folds cross-validation.

222
00:09:37,750 --> 00:09:41,000


