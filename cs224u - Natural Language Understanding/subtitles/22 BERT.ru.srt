1
00:00:04,880 --> 00:00:06,480
приветствуем всех, это третья часть

2
00:00:06,480 --> 00:00:07,839
нашей серии статей о контекстных представлениях слов,

3
00:00:07,839 --> 00:00:09,040
мы собираемся

4
00:00:09,040 --> 00:00:10,880
поговорить о модели bert, которая является

5
00:00:10,880 --> 00:00:12,559
инновационным и мощным приложением

6
00:00:12,559 --> 00:00:14,719
архитектуры трансформатора, которое мы

7
00:00:14,719 --> 00:00:16,880
рассмотрели в предыдущем скринкасте,

8
00:00:16,880 --> 00:00:18,800
давайте углубимся в структуру базовой модели

9
00:00:18,800 --> 00:00:21,119
но мы начнем с входных данных, как

10
00:00:21,119 --> 00:00:22,880
обычно, мы будем работать с простым примером

11
00:00:22,880 --> 00:00:24,880
правила рока, который имеет три токена, но

12
00:00:24,880 --> 00:00:27,119
вы заметите, что для bert мы начинаем

13
00:00:27,119 --> 00:00:29,119
каждую последовательность с маркера назначенного класса,

14
00:00:29,119 --> 00:00:31,359
а последовательности заканчиваются

15
00:00:31,359 --> 00:00:33,440
маркером назначенного набора.  и токен sep

16
00:00:33,440 --> 00:00:35,280
также можно использовать в качестве граничного маркера

17
00:00:35,280 --> 00:00:38,719
между частями входной последовательности,

18
00:00:38,719 --> 00:00:40,559
мы изучим позиционные вложения для

19
00:00:40,559 --> 00:00:43,040
каждого из этих токенов, и, кроме того,

20
00:00:43,040 --> 00:00:44,399
как вы можете видеть на темно-бордовом здесь, мы

21
00:00:44,399 --> 00:00:45,840
собираемся изучить второе понятие

22
00:00:45,840 --> 00:00:47,120
позиция, которую

23
00:00:47,120 --> 00:00:49,200
он отправил для всех этих токенов, поэтому

24
00:00:49,200 --> 00:00:50,879
он не будет способствовать в этом конкретном

25
00:00:50,879 --> 00:00:52,879
случае, но если бы у нас была проблема, такая как

26
00:00:52,879 --> 00:00:54,719
вывод на естественном языке, где

27
00:00:54,719 --> 00:00:57,039
примеры состоят из предпосылки s

28
00:00:57,039 --> 00:00:59,520
мы могли бы использовать

29
00:00:59,520 --> 00:01:00,960
изучение отдельных вложений для

30
00:01:00,960 --> 00:01:02,719
предпосылки и гипотезы и, таким образом,

31
00:01:02,719 --> 00:01:04,720
надеяться, что мы сможем уловить такое

32
00:01:04,720 --> 00:01:06,640
второе понятие положения в наших

33
00:01:06,640 --> 00:01:08,799
входных данных,

34
00:01:08,799 --> 00:01:10,479
мы собираемся выучить вложения

35
00:01:10,479 --> 00:01:12,240
для каждого из этих понятий для

36
00:01:12,240 --> 00:01:14,400
слово и два понятия позиции,

37
00:01:14,400 --> 00:01:16,159
и, как и в преобразователе, фактические

38
00:01:16,159 --> 00:01:18,320
вложения, данные здесь, в моем зеленом,

39
00:01:18,320 --> 00:01:20,960
будут аддитивными комбинациями этих трех

40
00:01:20,960 --> 00:01:24,960
изученных представлений вложений

41
00:01:25,200 --> 00:01:27,280
оттуда, мы просто делаем много работы

42
00:01:27,280 --> 00:01:29,040
с преобразователем, у нас есть повторяющиеся

43
00:01:29,040 --> 00:01:30,880
блоки преобразователя, это могло бы  может быть 12,

44
00:01:30,880 --> 00:01:33,280
может быть 24, а может быть даже больше,

45
00:01:33,280 --> 00:01:34,799
и выход всех этих

46
00:01:34,799 --> 00:01:36,560
блоков трансформатора в конце концов представляет собой

47
00:01:36,560 --> 00:01:39,119
последовательность выходных представлений,

48
00:01:39,119 --> 00:01:41,759
это векторы, не обозначенные темно-зеленым цветом,

49
00:01:41,759 --> 00:01:43,840
что является основной структурой модели для

50
00:01:43,840 --> 00:01:45,119
сгоревшего,

51
00:01:45,119 --> 00:01:46,880
и это приносит нам  к тому, как эта модель

52
00:01:46,880 --> 00:01:48,880
обучается, цель моделирования языка маски

53
00:01:48,880 --> 00:01:50,560
является фундаментальной

54
00:01:50,560 --> 00:01:52,960
целью обучения для этой модели, в

55
00:01:52,960 --> 00:01:54,720
основном то, что модель

56
00:01:54,720 --> 00:01:56,880
пытается сделать, это работать как автоматический кодировщик и

57
00:01:56,880 --> 00:01:59,759
воспроизводить целые входные последовательности,

58
00:01:59,759 --> 00:02:01,680
чтобы сделать эту проблему нетривиальной, хотя

59
00:02:01,680 --> 00:02:03,680
мы собираемся использовать эту идею моделирования массового языка,

60
00:02:03,680 --> 00:02:05,280
и это означает,

61
00:02:05,280 --> 00:02:06,320
что мы собираемся пройти через эти

62
00:02:06,320 --> 00:02:08,399
входные последовательности  и случайным образом замените

63
00:02:08,399 --> 00:02:11,520
некоторый небольшой процент от 10 до

64
00:02:11,520 --> 00:02:13,680
15 входных токенов этим назначенным

65
00:02:13,680 --> 00:02:15,040
токеном маски,

66
00:02:15,040 --> 00:02:16,720
а затем задача модели состоит в том, чтобы

67
00:02:16,720 --> 00:02:18,800
научиться для этих замаскированных входных данных

68
00:02:18,800 --> 00:02:21,040
реконструировать то, что было фактическим правильным вводом,

69
00:02:21,040 --> 00:02:23,120
поэтому в этом случае мы замаскировали

70
00:02:23,120 --> 00:02:25,440
правила и  задача модели состоит в том, чтобы использовать

71
00:02:25,440 --> 00:02:27,280
этот двунаправленный контекст,

72
00:02:27,280 --> 00:02:28,879
поступающий от всех этих

73
00:02:28,879 --> 00:02:31,519
механизмов внимания, чтобы выяснить, что правила на

74
00:02:31,519 --> 00:02:33,280
самом деле были токеном, принадлежащим этой

75
00:02:33,280 --> 00:02:35,840
начальной позиции,

76
00:02:35,840 --> 00:02:37,840
команда ver также сделала вариант этого

77
00:02:37,840 --> 00:02:39,920
, который является маскировкой.  случайным

78
00:02:39,920 --> 00:02:42,160
словом, так что в этом случае мы могли бы заменить

79
00:02:42,160 --> 00:02:44,239
правила на такое слово, как каждый, выбранное

80
00:02:44,239 --> 00:02:46,560
случайным образом из нашего словаря, но

81
00:02:46,560 --> 00:02:48,400
опять же основная задача модели

82
00:02:48,400 --> 00:02:51,040
состоит в том, чтобы научиться вычислять это правило  s

83
00:02:51,040 --> 00:02:53,360
был фактическим токеном в этой позиции,

84
00:02:53,360 --> 00:02:54,879
поэтому в этом случае он сделает некоторый прогноз,

85
00:02:54,879 --> 00:02:57,200
если он отличается от правил,

86
00:02:57,200 --> 00:02:59,040
и сигнал ошибки будет проходить обратно

87
00:02:59,040 --> 00:03:00,879
через все параметры этой модели,

88
00:03:00,879 --> 00:03:02,239
влияющие, мы надеемся, на все

89
00:03:02,239 --> 00:03:04,000
представления из-за этого плотного

90
00:03:04,000 --> 00:03:06,319
билета  связи внимания, которые

91
00:03:06,319 --> 00:03:08,560
существуют на этих временных шагах, и

92
00:03:08,560 --> 00:03:10,159
таким образом модель научится эффективно обновлять

93
00:03:10,159 --> 00:03:12,640
себя, изучая, как

94
00:03:12,640 --> 00:03:15,040
реконструировать недостающие фрагменты из

95
00:03:15,040 --> 00:03:16,560
этих входных данных, которые мы создали во время

96
00:03:16,560 --> 00:03:18,959
обучения,

97
00:03:18,959 --> 00:03:20,879
поэтому давайте немного глубже погрузимся в эту цель моделирования языка маски

98
00:03:20,879 --> 00:03:23,519


99
00:03:23,519 --> 00:03:26,640
для трансформатора  параметры h тета

100
00:03:26,640 --> 00:03:28,799
и некоторая последовательность токенов x с

101
00:03:28,799 --> 00:03:31,440
соответствующей замаскированной версией x шляпа

102
00:03:31,440 --> 00:03:33,440
это цель здесь, давайте увеличим

103
00:03:33,440 --> 00:03:35,440
масштаб некоторого временного шага t

104
00:03:35,440 --> 00:03:37,120
uh фундаментальная вещь оценки заключается в том, что

105
00:03:37,120 --> 00:03:38,720
мы собираемся искать векторное

106
00:03:38,720 --> 00:03:40,480
представление во встраивании  для этого

107
00:03:40,480 --> 00:03:41,920
временного шага t,

108
00:03:41,920 --> 00:03:44,000
и мы возьмем скалярное произведение этого

109
00:03:44,000 --> 00:03:46,239
с выходным представлением в момент времени t

110
00:03:46,239 --> 00:03:47,599
из всего  В этой модели

111
00:03:47,599 --> 00:03:49,680


112
00:03:49,680 --> 00:03:51,599
преобразования процедура подсчета очков

113
00:03:51,599 --> 00:03:53,120
очень похожа на то, что вы получаете от

114
00:03:53,120 --> 00:03:54,640
условных языковых моделей, вам просто

115
00:03:54,640 --> 00:03:56,879
нужно помнить, что из-за всех

116
00:03:56,879 --> 00:03:58,640
этих механизмов внимания, соединяющих

117
00:03:58,640 --> 00:04:00,959
каждый токен с каждым другим токеном, это

118
00:04:00,959 --> 00:04:03,040
не просто предшествующий контекст до

119
00:04:03,040 --> 00:04:05,200
временного шага t.  а скорее весь

120
00:04:05,200 --> 00:04:08,720
окружающий контекст для этой позиции,

121
00:04:08,720 --> 00:04:10,560
а затем, как обычно, мы нормализуем это,

122
00:04:10,560 --> 00:04:12,720
рассматривая все альтернативные токены x

123
00:04:12,720 --> 00:04:15,200
prime, которые могут быть в этой позиции,

124
00:04:15,200 --> 00:04:16,560
теперь вы заметите, что здесь есть

125
00:04:16,560 --> 00:04:19,519
индикаторная переменная mt mt равна 1, если токен t

126
00:04:19,519 --> 00:04:21,440
был массирован  l0, так что это все равно, что сказать,

127
00:04:21,440 --> 00:04:23,600
что мы собираемся включить эту потерю

128
00:04:23,600 --> 00:04:25,520
только для токенов, которые мы замаскировали

129
00:04:25,520 --> 00:04:27,120
,

130
00:04:27,120 --> 00:04:29,199
а затем последнее — это

131
00:04:29,199 --> 00:04:30,800
не определяющий выбор в отношении этой модели,

132
00:04:30,800 --> 00:04:32,240
но что-то, что стоит отметить, вы увидите,

133
00:04:32,240 --> 00:04:34,160
что мы  эффективно используя встраивание для этого

134
00:04:34,160 --> 00:04:35,120
токена

135
00:04:35,120 --> 00:04:37,520
в качестве параметров softmax,

136
00:04:37,520 --> 00:04:39,040
здесь могут быть отдельные параметры,

137
00:04:39,040 --> 00:04:41,360
которые мы изучили для части классификатора,

138
00:04:41,360 --> 00:04:43,360
которая учится быть  Я

139
00:04:43,360 --> 00:04:44,960
думаю, что языковая модель, но я думаю, что люди со временем обнаружили,

140
00:04:44,960 --> 00:04:46,880
что, связывая эти параметры с

141
00:04:46,880 --> 00:04:48,800
помощью транспонирования этих параметров

142
00:04:48,800 --> 00:04:51,040
для создания выходного пространства, мы получаем некоторую

143
00:04:51,040 --> 00:04:53,040
статистическую силу и более эффективное

144
00:04:53,040 --> 00:04:55,440
обучение.

145
00:04:55,440 --> 00:04:57,199
Есть вторая цель в модели глагола,

146
00:04:57,199 --> 00:04:59,440
и это  бинарная задача предсказания следующего предложения,

147
00:04:59,440 --> 00:05:00,960


148
00:05:00,960 --> 00:05:02,560
и я думаю, что это была попытка

149
00:05:02,560 --> 00:05:04,800
найти некоторую согласованность, э-э, за пределами

150
00:05:04,800 --> 00:05:07,039
простого предложения или уровня последовательности,

151
00:05:07,039 --> 00:05:08,800
так что это довольно просто для

152
00:05:08,800 --> 00:05:10,720
положительных примеров для этого класса, мы

153
00:05:10,720 --> 00:05:12,320
собираемся взять фактические последовательности

154
00:05:12,320 --> 00:05:14,160
предложений в  корпус, который мы используем

155
00:05:14,160 --> 00:05:16,080
для обучения, поэтому здесь вы можете видеть, что они

156
00:05:16,080 --> 00:05:17,440
действительно произошли вместе, и они

157
00:05:17,440 --> 00:05:19,520
помечены как следующие, а для отрицательных

158
00:05:19,520 --> 00:05:21,440
примеров мы просто случайным образом выбираем

159
00:05:21,440 --> 00:05:23,600
второе предложение и помечаем его как не

160
00:05:23,600 --> 00:05:25,600
следующее, и я думаю, что стремление здесь было в

161
00:05:25,600 --> 00:05:27,600
том, что это  поможет модели изучить

162
00:05:27,600 --> 00:05:29,600
некоторое понятие связности дискурса

163
00:05:29,600 --> 00:05:31,680
за пределами локальной связности

164
00:05:31,680 --> 00:05:34,880
отдельных последовательностей,

165
00:05:34,960 --> 00:05:36,960
чего вы, вероятно, хотите.  o делать с

166
00:05:36,960 --> 00:05:38,639
моделью отрыжки - не тренировать ее с

167
00:05:38,639 --> 00:05:40,639
нуля, а скорее настраивать ее для

168
00:05:40,639 --> 00:05:42,960
конкретной задачи, которая у вас есть, есть

169
00:05:42,960 --> 00:05:44,560
много режимов, о которых вы можете подумать для

170
00:05:44,560 --> 00:05:47,120
этого. своего рода выбор по умолчанию,

171
00:05:47,120 --> 00:05:48,800
стандартный и простой выбор -

172
00:05:48,800 --> 00:05:50,880
использовать  токен класса,

173
00:05:50,880 --> 00:05:52,560
точнее, он выводится на

174
00:05:52,560 --> 00:05:54,800
последнем уровне модели отрыжки в качестве

175
00:05:54,800 --> 00:05:57,280
основы для установки некоторых конкретных параметров задачи,

176
00:05:57,280 --> 00:05:59,440
а затем использует эти

177
00:05:59,440 --> 00:06:01,360
метки, которые у вас есть для

178
00:06:01,360 --> 00:06:02,319
контроля здесь,

179
00:06:02,319 --> 00:06:03,600
и это может быть эффективным, потому что

180
00:06:03,600 --> 00:06:05,440
токен класса появляется в этой позиции  в

181
00:06:05,440 --> 00:06:07,360
каждой из этих последовательностей, и

182
00:06:07,360 --> 00:06:08,800
поэтому вы можете думать об этом как о хорошем

183
00:06:08,800 --> 00:06:10,960
сводном представлении всей

184
00:06:10,960 --> 00:06:12,960
последовательности, а затем, когда вы выполняете точную

185
00:06:12,960 --> 00:06:14,400
настройку, вы, конечно, будете обновлять

186
00:06:14,400 --> 00:06:16,560
эти параметры задачи, а затем вы могли бы,

187
00:06:16,560 --> 00:06:18,960
если хотите, также  обновить некоторые или

188
00:06:18,960 --> 00:06:20,479
все фактические параметры из

189
00:06:20,479 --> 00:06:22,400
предварительно обученной модели, что было бы истинным

190
00:06:22,400 --> 00:06:24,319
понятием тонкой настройки,

191
00:06:24,319 --> 00:06:25,919
теперь вы можете беспокоиться о том, что токен класса

192
00:06:25,919 --> 00:06:27,840
является недостаточной сводкой  всю

193
00:06:27,840 --> 00:06:29,199
последовательность, и поэтому вы, конечно, можете

194
00:06:29,199 --> 00:06:31,360
подумать об объединении всех выходных

195
00:06:31,360 --> 00:06:33,360
состояний в последовательности с помощью какой-либо функции,

196
00:06:33,360 --> 00:06:36,240
такой как сумма, среднее или максимальное значение, и использовать их

197
00:06:36,240 --> 00:06:38,560
в качестве входных данных для любых конкретных параметров теста,

198
00:06:38,560 --> 00:06:42,319
которые у вас есть здесь вверху,

199
00:06:42,400 --> 00:06:43,759
я просто хочу напомнить  нам, что

200
00:06:43,759 --> 00:06:46,160
инвертирование токенизации немного необычно,

201
00:06:46,160 --> 00:06:47,919
мы уже рассматривали это несколько раз,

202
00:06:47,919 --> 00:06:49,360
но просто помните, что мы

203
00:06:49,360 --> 00:06:51,759
фактически получаем не полные слова, а части слов,

204
00:06:51,759 --> 00:06:54,639
поэтому для таких случаев, как encode me, вы

205
00:06:54,639 --> 00:06:56,080
можете видеть, что слово encode было

206
00:06:56,080 --> 00:06:57,840
разделено на  две части слова, и

207
00:06:57,840 --> 00:07:00,240
мы неявно надеемся, что модель

208
00:07:00,240 --> 00:07:01,759
может узнать, что это в каком-то глубоком

209
00:07:01,759 --> 00:07:03,360
смысле все еще слово, даже если оно

210
00:07:03,360 --> 00:07:05,520
было разделено на части, и это должно опираться

211
00:07:05,520 --> 00:07:07,280
на действительно контекстную природу этих

212
00:07:07,280 --> 00:07:08,240
моделей.

213
00:07:08,240 --> 00:07:10,080


214
00:07:10,080 --> 00:07:12,560
релизы птичьей базы состоят из 12

215
00:07:12,560 --> 00:07:13,840
трансформирующих слоев и имеют

216
00:07:13,840 --> 00:07:17,280
представления размерности 768 с 12

217
00:07:17,280 --> 00:07:19,360
головками внимания, всего 110

218
00:07:19,360 --> 00:07:21,199
миллионов параметров, это, конечно,

219
00:07:21,199 --> 00:07:23,120
очень большая модель, но это мана  Это удобно

220
00:07:23,120 --> 00:07:25,039
для вас, чтобы выполнять локальную работу, особенно если

221
00:07:25,039 --> 00:07:26,479
вы просто хотите выполнить простую точную

222
00:07:26,479 --> 00:07:28,840
настройку или использовать эту модель для

223
00:07:28,840 --> 00:07:31,360
вывода. Крупный выпуск птицы намного

224
00:07:31,360 --> 00:07:33,919
больше, он имеет 24 слоя, в

225
00:07:33,919 --> 00:07:35,440
два раза больше размерности для своих

226
00:07:35,440 --> 00:07:37,680
представлений, и 16 головок внимания

227
00:07:37,680 --> 00:07:40,639
, всего 340.  миллионов параметров,

228
00:07:40,639 --> 00:07:42,160
это достаточно велико, и с ним может быть

229
00:07:42,160 --> 00:07:44,400
трудно работать локально, но,

230
00:07:44,400 --> 00:07:45,680
конечно, вы можете получить гораздо больше

231
00:07:45,680 --> 00:07:48,319
репрезентативных возможностей от его использования

232
00:07:48,319 --> 00:07:49,599
для обеих этих моделей. У нас есть

233
00:07:49,599 --> 00:07:52,400
ограничение до 512 токенов, и это

234
00:07:52,400 --> 00:07:54,240
потому, что это размер

235
00:07:54,240 --> 00:07:55,759
пространство для позиционного встраивания, которое они

236
00:07:55,759 --> 00:07:57,520
узнали, есть много новых выпусков,

237
00:07:57,520 --> 00:07:59,039
конечно, вы можете найти их на сайте проекта,

238
00:07:59,039 --> 00:08:01,199
и обнимание лица

239
00:08:01,199 --> 00:08:02,800
упростило доступ к этим моделям, и

240
00:08:02,800 --> 00:08:05,120
это очень помогло

241
00:08:05,120 --> 00:08:06,639
закрыть это, позвольте мне просто упомянуть несколько

242
00:08:06,639 --> 00:08:08,319
известных ограничений  Берка, к которому мы

243
00:08:08,319 --> 00:08:09,919
собираемся вернуться, когда будем рассматривать некоторые

244
00:08:09,919 --> 00:08:12,560
последующие модели этого устройства, поэтому сначала

245
00:08:12,560 --> 00:08:14,800
в исходной статье есть большое,

246
00:08:14,800 --> 00:08:16,960
но все же частичное онемение.  Что касается

247
00:08:16,960 --> 00:08:19,199
исследований абляции и оптимизационных исследований,

248
00:08:19,199 --> 00:08:21,919
здесь задействован огромный ландшафт, и

249
00:08:21,919 --> 00:08:23,919
лишь небольшие его части исследуются

250
00:08:23,919 --> 00:08:25,520
в исходной статье, поэтому мы можем беспокоиться о том,

251
00:08:25,520 --> 00:08:27,039
что есть лучший выбор, который мы могли

252
00:08:27,039 --> 00:08:30,319
бы сделать в этом пространстве.

253
00:08:30,319 --> 00:08:32,159
В исходной статье также указывается, что

254
00:08:32,159 --> 00:08:33,839
в этом массовом токене есть некоторая неестественность,

255
00:08:33,839 --> 00:08:36,320
они говорят, что первый

256
00:08:36,320 --> 00:08:38,159
недостаток цели млм заключается в том, что мы

257
00:08:38,159 --> 00:08:40,240
создаем несоответствие между предварительной подготовкой

258
00:08:40,240 --> 00:08:42,479
и тонкой настройкой, потому что массовый

259
00:08:42,479 --> 00:08:44,399
токен никогда не виден во время тонкой настройки, так

260
00:08:44,399 --> 00:08:45,519
что это то, что мы могли бы решить

261
00:08:45,519 --> 00:08:47,040


262
00:08:47,040 --> 00:08:48,320
они  также укажите, что есть

263
00:08:48,320 --> 00:08:50,160
обратная сторона использования цели млм,

264
00:08:50,160 --> 00:08:51,600
которая заключается в том, что данные

265
00:08:51,600 --> 00:08:54,240
неэффективны, мы можем замаскировать только небольшой

266
00:08:54,240 --> 00:08:56,000
процент токенов, потому что нам

267
00:08:56,000 --> 00:08:57,680
нужен окружающий контекст, чтобы

268
00:08:57,680 --> 00:09:00,240
разумно воспроизвести эти токены,

269
00:09:00,240 --> 00:09:01,680
и это означает, что это  данные

270
00:09:01,680 --> 00:09:03,600
неэффективны,

271
00:09:03,600 --> 00:09:05,200
и, наконец, это из чистой бумаги Excel,

272
00:09:05,200 --> 00:09:07,200
я думаю, что это довольно проницательно, но

273
00:09:07,200 --> 00:09:09,040
предполагает, что прогнозируемый ток  ns

274
00:09:09,040 --> 00:09:10,640
не зависят друг от друга, учитывая

275
00:09:10,640 --> 00:09:13,200
незамаскированные токены, которые чрезмерно упрощены,

276
00:09:13,200 --> 00:09:15,200
поскольку в естественном языке преобладает дальняя зависимость высокого порядка,

277
00:09:15,200 --> 00:09:17,200
что они

278
00:09:17,200 --> 00:09:18,800
имеют в виду здесь, по сути, если у вас

279
00:09:18,800 --> 00:09:21,360
есть идиома, подобная вне этого мира,

280
00:09:21,360 --> 00:09:23,120
и случается, что оба  первое

281
00:09:23,120 --> 00:09:25,120
и последнее слова в этой идиоме замаскированы

282
00:09:25,120 --> 00:09:26,720
, тогда Берд попытается

283
00:09:26,720 --> 00:09:28,240
воспроизвести их так, как если бы они были

284
00:09:28,240 --> 00:09:29,920
независимы друг от друга, тогда как на самом деле

285
00:09:29,920 --> 00:09:32,000
мы знаем, что между ними существует статистическая зависимость, возникающая

286
00:09:32,000 --> 00:09:34,160
из-за

287
00:09:34,160 --> 00:09:35,839
того, что они участвуют в  эта

288
00:09:35,839 --> 00:09:36,720
идиома,

289
00:09:36,720 --> 00:09:38,160
так что есть какое-то понятие

290
00:09:38,160 --> 00:09:39,680
репрезентативной

291
00:09:39,680 --> 00:09:41,839
согласованности, которую берт просто не

292
00:09:41,839 --> 00:09:46,120
улавливает с его целью МЛМ

