1
00:00:00,000 --> 00:00:04,168


2
00:00:04,168 --> 00:00:05,710
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,710 --> 00:00:07,860
This is part three in our
series on contextual word

4
00:00:07,860 --> 00:00:08,582
representations.

5
00:00:08,582 --> 00:00:10,540
We're going to be talking
about the BERT model.

6
00:00:10,540 --> 00:00:12,498
Which is an innovative
and powerful application

7
00:00:12,498 --> 00:00:15,060
of the transformer
architecture which we covered

8
00:00:15,060 --> 00:00:16,980
in the previous screencast.

9
00:00:16,980 --> 00:00:19,290
Let's dive into the core
model structure of BERT.

10
00:00:19,290 --> 00:00:20,970
We'll begin with the inputs.

11
00:00:20,970 --> 00:00:23,430
As usual we'll work with a
simple example; "the Rock

12
00:00:23,430 --> 00:00:24,840
rules" that has three tokens.

13
00:00:24,840 --> 00:00:26,790
But you'll notice
that for BERT, we

14
00:00:26,790 --> 00:00:29,970
begin every sequence with
the designated class token.

15
00:00:29,970 --> 00:00:32,729
And sequences end with
a designated SEP token.

16
00:00:32,729 --> 00:00:34,380
And the SEP token
can also be used

17
00:00:34,380 --> 00:00:38,810
as a boundary marker between
sub-parts of an input sequence.

18
00:00:38,810 --> 00:00:40,520
We'll learn
positional embeddings

19
00:00:40,520 --> 00:00:42,410
for each one of those tokens.

20
00:00:42,410 --> 00:00:44,390
And in addition, as you
can see in maroon here,

21
00:00:44,390 --> 00:00:47,180
we're going to learn a
second notion of position.

22
00:00:47,180 --> 00:00:49,160
It sent A for all
of these tokens,

23
00:00:49,160 --> 00:00:51,390
so it won't contribute
in this particular case.

24
00:00:51,390 --> 00:00:54,290
But if we had a problem like
natural language inference

25
00:00:54,290 --> 00:00:56,420
where examples
consist of a premise

26
00:00:56,420 --> 00:00:58,850
sentence and a
hypothesis sentence,

27
00:00:58,850 --> 00:01:00,800
we could learn
separate embeddings

28
00:01:00,800 --> 00:01:02,390
for the premise
and the hypothesis.

29
00:01:02,390 --> 00:01:05,630
And thereby hope that we can
capture that second notion

30
00:01:05,630 --> 00:01:08,940
of position within our input.

31
00:01:08,940 --> 00:01:10,560
We're going to have
learned embeddings

32
00:01:10,560 --> 00:01:12,600
for each one of these
notions for the word

33
00:01:12,600 --> 00:01:14,520
and the two notions of position.

34
00:01:14,520 --> 00:01:16,920
And as in the transformer,
the actual embeddings

35
00:01:16,920 --> 00:01:19,980
given here in light green
will be additive combinations

36
00:01:19,980 --> 00:01:25,230
of those three learned
embedding representations.

37
00:01:25,230 --> 00:01:28,230
From there we just do a lot
of work with the transformer.

38
00:01:28,230 --> 00:01:30,140
Have we have had repeated
transformer blocks,

39
00:01:30,140 --> 00:01:33,360
it could be 12, it could be
24, it could be even more.

40
00:01:33,360 --> 00:01:36,420
And the output of all of those
transformer blocks in the end

41
00:01:36,420 --> 00:01:39,100
is a sequence of
output representations.

42
00:01:39,100 --> 00:01:41,820
These are vectors, and I've
given that in dark green.

43
00:01:41,820 --> 00:01:45,210
That is the core model
structure for BERT.

44
00:01:45,210 --> 00:01:47,550
And that brings us to how
this model is trained.

45
00:01:47,550 --> 00:01:49,440
The masked language
modeling objective

46
00:01:49,440 --> 00:01:53,060
is the fundamental training
objective for this model.

47
00:01:53,060 --> 00:01:55,280
Fundamentally, what the
model is trying to do

48
00:01:55,280 --> 00:01:58,490
is work as an auto-encoder
and reproduce the entire input

49
00:01:58,490 --> 00:01:59,870
sequences.

50
00:01:59,870 --> 00:02:01,790
To make that problem
non-trivial though,

51
00:02:01,790 --> 00:02:04,285
we're going to employ this
masked language modeling idea.

52
00:02:04,285 --> 00:02:05,660
And what that
means is that we're

53
00:02:05,660 --> 00:02:07,410
going to go through
these input sequences,

54
00:02:07,410 --> 00:02:11,570
and randomly replace some
small percentage, 10% to 15%

55
00:02:11,570 --> 00:02:15,080
of the input tokens, with
this designated mask token.

56
00:02:15,080 --> 00:02:17,210
And then the job of
the model is to learn,

57
00:02:17,210 --> 00:02:19,790
for those masked inputs
to reconstruct what

58
00:02:19,790 --> 00:02:21,490
was the actual input, right?

59
00:02:21,490 --> 00:02:25,040
So in this case, we masked out
rules and the job of the model

60
00:02:25,040 --> 00:02:27,320
is to use this bidirectional
context, that's

61
00:02:27,320 --> 00:02:30,020
flowing in from all those
attention mechanisms

62
00:02:30,020 --> 00:02:32,000
to figure out that
"rules" was actually

63
00:02:32,000 --> 00:02:35,820
the token that belonged
to that initial position.

64
00:02:35,820 --> 00:02:37,710
The BERT team did
a variant of this

65
00:02:37,710 --> 00:02:40,390
as well, which is
masking by a random word.

66
00:02:40,390 --> 00:02:42,630
So in this case, we
might replace "rules"

67
00:02:42,630 --> 00:02:46,080
with a word like "every" picked
randomly from our vocabulary.

68
00:02:46,080 --> 00:02:48,480
But there again, the
fundamental job of the model

69
00:02:48,480 --> 00:02:52,140
is to learn to figure out that
"rules" was the actual token

70
00:02:52,140 --> 00:02:53,620
in that position.

71
00:02:53,620 --> 00:02:55,980
So it's going to make some
prediction in this case.

72
00:02:55,980 --> 00:02:58,260
If it's different from
"rules," then the error signal

73
00:02:58,260 --> 00:03:00,930
will flow back down through all
the parameters of this model.

74
00:03:00,930 --> 00:03:03,180
Affecting, we hope,
all the representations

75
00:03:03,180 --> 00:03:06,240
because of that dense thicket
of attention connections

76
00:03:06,240 --> 00:03:08,310
that exists across
these timesteps.

77
00:03:08,310 --> 00:03:10,980
And in that way the model
will learn to update itself,

78
00:03:10,980 --> 00:03:14,430
effectively learning how
to reconstruct the missing

79
00:03:14,430 --> 00:03:19,190
pieces from these inputs that
we created during training.

80
00:03:19,190 --> 00:03:21,970
So let's dive into that masked
language modeling objective,

81
00:03:21,970 --> 00:03:23,650
a little more deeply.

82
00:03:23,650 --> 00:03:26,710
For transformer
parameters H theta,

83
00:03:26,710 --> 00:03:29,980
and some sequence of tokens x,
with its corresponding masked

84
00:03:29,980 --> 00:03:31,360
version x hat.

85
00:03:31,360 --> 00:03:32,685
This is the objective here.

86
00:03:32,685 --> 00:03:35,568
Well, let's zoom in
on some timestep t.

87
00:03:35,568 --> 00:03:37,360
The fundamental scoring
thing is that we're

88
00:03:37,360 --> 00:03:39,490
going to look up the
vector representation

89
00:03:39,490 --> 00:03:41,980
and the embedding
for that timestep t.

90
00:03:41,980 --> 00:03:44,950
And we'll take the dot product
of that with the output

91
00:03:44,950 --> 00:03:49,660
representation at time t, from
the entire transformer model.

92
00:03:49,660 --> 00:03:51,700
That much there, that
scoring procedure

93
00:03:51,700 --> 00:03:54,100
looks a lot like what you
get from conditional language

94
00:03:54,100 --> 00:03:54,600
models.

95
00:03:54,600 --> 00:03:57,730
You just have to remember that
because of all those attention

96
00:03:57,730 --> 00:04:00,670
mechanisms, connecting every
token to every other token.

97
00:04:00,670 --> 00:04:04,030
This is not just the preceding
context before timestep t,

98
00:04:04,030 --> 00:04:05,890
but rather the
entire surrounding

99
00:04:05,890 --> 00:04:08,870
context for this position.

100
00:04:08,870 --> 00:04:11,180
And then as usual we
normalize that by considering

101
00:04:11,180 --> 00:04:13,130
all the alternative
tokens x prime,

102
00:04:13,130 --> 00:04:15,153
that could be in this position.

103
00:04:15,153 --> 00:04:17,570
Now, you'll notice over here
there's an indicator variable

104
00:04:17,570 --> 00:04:20,910
mt, mt is 1 if token
t was masked, else 0.

105
00:04:20,910 --> 00:04:23,600
So that's like saying we're
going to turn on this loss,

106
00:04:23,600 --> 00:04:27,320
only for the tokens
that we have masked out.

107
00:04:27,320 --> 00:04:30,055
And then the final thing is kind
of not a definitional choice

108
00:04:30,055 --> 00:04:31,930
about this model, but
something worth noting.

109
00:04:31,930 --> 00:04:35,110
You'll see that we're using
the embedding for this token,

110
00:04:35,110 --> 00:04:37,593
effectively as the
softmax parameters.

111
00:04:37,593 --> 00:04:39,010
There could be
separate parameters

112
00:04:39,010 --> 00:04:41,710
here that we learn for the
classifier part, that's

113
00:04:41,710 --> 00:04:43,673
learning to be a language model.

114
00:04:43,673 --> 00:04:45,340
But I think people
have found over time,

115
00:04:45,340 --> 00:04:47,290
that by tying these
parameters, by using

116
00:04:47,290 --> 00:04:48,880
the transpose of
these parameters

117
00:04:48,880 --> 00:04:52,240
to create the output space, we
get some statistical strength

118
00:04:52,240 --> 00:04:55,550
and more efficient learning.

119
00:04:55,550 --> 00:04:57,650
There is a second
objective in the BERT model

120
00:04:57,650 --> 00:05:01,310
and it is the binary next
sentence prediction task.

121
00:05:01,310 --> 00:05:02,960
And I think this was
an attempt to find

122
00:05:02,960 --> 00:05:06,260
some coherence beyond just the
simple sentence or sequence

123
00:05:06,260 --> 00:05:07,243
level.

124
00:05:07,243 --> 00:05:08,660
So this is pretty
straightforward.

125
00:05:08,660 --> 00:05:10,710
For positive instances
for this class,

126
00:05:10,710 --> 00:05:13,070
we're going to take actual
sequences of sentences

127
00:05:13,070 --> 00:05:15,127
in the corpus that we're
using for training.

128
00:05:15,127 --> 00:05:17,210
So here you can see these
actually occur together,

129
00:05:17,210 --> 00:05:18,860
and they are labeled as next.

130
00:05:18,860 --> 00:05:21,450
And for negative examples
we just randomly choose

131
00:05:21,450 --> 00:05:24,212
the second sentence, and
label that as not next.

132
00:05:24,212 --> 00:05:25,670
And I think the
aspiration here was

133
00:05:25,670 --> 00:05:29,090
that this would help the model
learn some notion of discourse

134
00:05:29,090 --> 00:05:31,640
coherence beyond
the local coherence

135
00:05:31,640 --> 00:05:35,010
of individual sequences.

136
00:05:35,010 --> 00:05:37,770
Now what you probably want
to do with the BERT model

137
00:05:37,770 --> 00:05:40,410
is not train it from scratch,
but rather fine tune it

138
00:05:40,410 --> 00:05:42,760
on a particular
task that you have.

139
00:05:42,760 --> 00:05:46,098
There are many modes that you
can think about for doing this.

140
00:05:46,098 --> 00:05:48,390
Kind of default choice, the
standard and simple choice,

141
00:05:48,390 --> 00:05:50,900
would be to use the class token.

142
00:05:50,900 --> 00:05:53,280
More specifically, its
output in the final layer

143
00:05:53,280 --> 00:05:57,330
of the BERT model, as the basis
for setting some task specific

144
00:05:57,330 --> 00:05:58,140
parameters.

145
00:05:58,140 --> 00:06:00,240
And then using that,
whatever labels

146
00:06:00,240 --> 00:06:02,252
you have for
supervision up here.

147
00:06:02,252 --> 00:06:04,710
And that could be effective
because the class token appears

148
00:06:04,710 --> 00:06:07,320
in this position in every
single one of these sequences.

149
00:06:07,320 --> 00:06:10,290
And so you might think of it as
a good summary representation

150
00:06:10,290 --> 00:06:11,868
of the entire sequence.

151
00:06:11,868 --> 00:06:13,410
And then when you
do the fine tuning,

152
00:06:13,410 --> 00:06:15,870
you'll of course be updating
these task parameters.

153
00:06:15,870 --> 00:06:17,610
And then you could,
if you wanted to,

154
00:06:17,610 --> 00:06:20,295
also update some or all
of the actual parameters

155
00:06:20,295 --> 00:06:21,420
from the pre-trained model.

156
00:06:21,420 --> 00:06:24,293
And that would be a true
notion of fine tuning.

157
00:06:24,293 --> 00:06:25,710
Now you might worry
that the class

158
00:06:25,710 --> 00:06:28,490
token is an insufficient
summary of the entire sequence.

159
00:06:28,490 --> 00:06:31,470
And so you could of course think
about pooling all the output

160
00:06:31,470 --> 00:06:32,700
states in the sequence.

161
00:06:32,700 --> 00:06:35,550
Via some function like
sum, or mean, or max,

162
00:06:35,550 --> 00:06:38,190
and using those as
the input to whatever

163
00:06:38,190 --> 00:06:42,620
task-specific parameters
you have up here at the top.

164
00:06:42,620 --> 00:06:45,260
I just want to remind us
that tokenization in BERT

165
00:06:45,260 --> 00:06:46,440
is a little unusual.

166
00:06:46,440 --> 00:06:48,050
We've covered this
a few times before,

167
00:06:48,050 --> 00:06:50,217
but just remember that we're
getting effectively not

168
00:06:50,217 --> 00:06:52,550
full words, but word pieces.

169
00:06:52,550 --> 00:06:54,595
So for cases like
"encode me", you

170
00:06:54,595 --> 00:06:56,720
can see that the word
"encode" has been split apart

171
00:06:56,720 --> 00:06:57,890
into two word pieces.

172
00:06:57,890 --> 00:07:00,320
And we're hoping,
implicitly, that the model

173
00:07:00,320 --> 00:07:02,930
can learn that that is in
some deep sense still a word.

174
00:07:02,930 --> 00:07:05,120
Even though it has been
split apart and that

175
00:07:05,120 --> 00:07:07,070
should draw on the
truly contextual nature

176
00:07:07,070 --> 00:07:08,390
of these models.

177
00:07:08,390 --> 00:07:11,000
The BERT team did two
initial model releases.

178
00:07:11,000 --> 00:07:13,670
BERT base consists of
12 transformer layers

179
00:07:13,670 --> 00:07:17,645
and has representations of
dimension 768 with 12 attention

180
00:07:17,645 --> 00:07:20,630
heads for a total of
110 million parameters.

181
00:07:20,630 --> 00:07:22,190
That's of course,
a very large model,

182
00:07:22,190 --> 00:07:24,620
but this is manageable
for you to do local work.

183
00:07:24,620 --> 00:07:27,320
Especially if you just want
to do some simple fine tuning,

184
00:07:27,320 --> 00:07:29,930
or use this model for inference.

185
00:07:29,930 --> 00:07:33,980
The BERT large release is
much larger, it has 24 layers.

186
00:07:33,980 --> 00:07:36,350
Twice the dimensionality
for its representations,

187
00:07:36,350 --> 00:07:38,330
and 16 attention
heads for a total

188
00:07:38,330 --> 00:07:40,723
of 340 million parameters.

189
00:07:40,723 --> 00:07:42,140
This is large
enough that it might

190
00:07:42,140 --> 00:07:44,180
be difficult to do
local work with.

191
00:07:44,180 --> 00:07:46,910
But of course you might get
much more representational power

192
00:07:46,910 --> 00:07:48,293
from using it.

193
00:07:48,293 --> 00:07:50,210
For both of these models,
we have a limitation

194
00:07:50,210 --> 00:07:52,010
to 512 tokens.

195
00:07:52,010 --> 00:07:54,140
And that is because
that is the size

196
00:07:54,140 --> 00:07:56,367
of the positional embedding
space that they learned.

197
00:07:56,367 --> 00:07:57,950
There are many new
releases of course.

198
00:07:57,950 --> 00:07:59,533
You can find those
at the project site

199
00:07:59,533 --> 00:08:01,520
and Hugging Face has
made it very easy

200
00:08:01,520 --> 00:08:05,270
to access these models, and
that's been very empowering.

201
00:08:05,270 --> 00:08:07,760
To close this let me just
mention a few known limitations

202
00:08:07,760 --> 00:08:09,177
of BERT that we're
going to return

203
00:08:09,177 --> 00:08:10,940
to as we go through
some subsequent models

204
00:08:10,940 --> 00:08:12,210
for this unit.

205
00:08:12,210 --> 00:08:13,820
So first in the
original paper, there

206
00:08:13,820 --> 00:08:17,480
is a large, but still partial
number of ablation studies

207
00:08:17,480 --> 00:08:19,100
and optimization studies.

208
00:08:19,100 --> 00:08:21,830
There's a huge landscape
that's in play here,

209
00:08:21,830 --> 00:08:24,807
and only small parts of it are
explored in the original paper.

210
00:08:24,807 --> 00:08:27,140
So we might worry that there
are better choices we could

211
00:08:27,140 --> 00:08:30,470
be making within this space.

212
00:08:30,470 --> 00:08:32,210
The original paper
also points out

213
00:08:32,210 --> 00:08:34,669
that there's some unnaturalness
about this MASK token.

214
00:08:34,669 --> 00:08:37,700
They say, the first downside
of the MLM objective

215
00:08:37,700 --> 00:08:39,289
is that we are
creating a mismatch

216
00:08:39,289 --> 00:08:41,120
between pre-training
and fine tuning,

217
00:08:41,120 --> 00:08:44,330
because the MASK token is
never seen during fine tuning.

218
00:08:44,330 --> 00:08:47,158
So that's something we
might want to address.

219
00:08:47,158 --> 00:08:48,950
They also point out
that there's a downside

220
00:08:48,950 --> 00:08:51,117
to using the MLM objective
which is to say that it's

221
00:08:51,117 --> 00:08:52,940
kind of data inefficient.

222
00:08:52,940 --> 00:08:55,580
We can only mask out a small
percentage of the tokens.

223
00:08:55,580 --> 00:08:57,350
Because we need the
surrounding context

224
00:08:57,350 --> 00:09:00,290
that would ostensibly
reproduce those tokens.

225
00:09:00,290 --> 00:09:03,640
And that means that it's
kind of data inefficient.

226
00:09:03,640 --> 00:09:05,590
And finally this is
from the XLNet paper,

227
00:09:05,590 --> 00:09:07,240
I think this is
quite perceptive.

228
00:09:07,240 --> 00:09:09,130
BERT assumes that
the predicted tokens

229
00:09:09,130 --> 00:09:12,040
are independent of each other
given the unmasked tokens,

230
00:09:12,040 --> 00:09:14,020
which is oversimplified
as high-order,

231
00:09:14,020 --> 00:09:16,837
long-range dependency is
prevalent in natural language.

232
00:09:16,837 --> 00:09:18,670
What they have in mind
here, is essentially,

233
00:09:18,670 --> 00:09:21,460
if you have an idiom
like "Out of this world"

234
00:09:21,460 --> 00:09:23,980
and it happens that both
the first and the last words

235
00:09:23,980 --> 00:09:25,990
in that idiom are
masked out, then

236
00:09:25,990 --> 00:09:28,240
BERT is going to try to
reproduce them as though they

237
00:09:28,240 --> 00:09:29,640
were independent of each other.

238
00:09:29,640 --> 00:09:32,710
And in fact, we know that there
is a statistical dependency

239
00:09:32,710 --> 00:09:34,480
between them coming
from the fact

240
00:09:34,480 --> 00:09:36,790
that they are participating
in this idiom.

241
00:09:36,790 --> 00:09:40,810
So there's some notion of
representational coherence

242
00:09:40,810 --> 00:09:42,460
that BERT is simply
not capturing

243
00:09:42,460 --> 00:09:45,060
with its MLM objective.

244
00:09:45,060 --> 00:09:49,000


