1
00:00:00,000 --> 00:00:04,757


2
00:00:04,757 --> 00:00:05,590
BILL MACCARTNEY: OK.

3
00:00:05,590 --> 00:00:06,700
We're underway.

4
00:00:06,700 --> 00:00:09,700
We have a simple model with
reasonable performance.

5
00:00:09,700 --> 00:00:12,950
Where do we go from here?

6
00:00:12,950 --> 00:00:14,530
Well, to make
further gains we need

7
00:00:14,530 --> 00:00:16,970
to stop treating the
model as a black box.

8
00:00:16,970 --> 00:00:19,900
We need to open it
up and get visibility

9
00:00:19,900 --> 00:00:23,330
into what it's learned
and more importantly,

10
00:00:23,330 --> 00:00:25,480
where it still falls down.

11
00:00:25,480 --> 00:00:28,060
And then we can begin to
look at some ideas for how

12
00:00:28,060 --> 00:00:28,740
to improve it.

13
00:00:28,740 --> 00:00:31,720


14
00:00:31,720 --> 00:00:34,760
One important way to gain
understanding of our trained

15
00:00:34,760 --> 00:00:38,250
models is to inspect
the model weights.

16
00:00:38,250 --> 00:00:40,620
Which features are strong
positive indicators

17
00:00:40,620 --> 00:00:43,170
for each relation,
and what features

18
00:00:43,170 --> 00:00:45,330
are strong negative indicators?

19
00:00:45,330 --> 00:00:47,760
The rel_ext model
contains a function

20
00:00:47,760 --> 00:00:52,680
called examine_model_weights
that makes it easy to inspect.

21
00:00:52,680 --> 00:00:56,940
So here, I show results for
just 4 of our 16 relations.

22
00:00:56,940 --> 00:00:58,860
And in general, I
think the features

23
00:00:58,860 --> 00:01:02,050
with large positive weight
are pretty intuitive.

24
00:01:02,050 --> 00:01:05,280
So for the author relation,
the biggest weights

25
00:01:05,280 --> 00:01:08,856
are author, books, and by.

26
00:01:08,856 --> 00:01:12,360
For film performance, we
have starring, alongside,

27
00:01:12,360 --> 00:01:14,047
and opposite.

28
00:01:14,047 --> 00:01:15,630
By the way, I was a
little bit puzzled

29
00:01:15,630 --> 00:01:19,320
when I first saw alongside and
opposite, because I thought

30
00:01:19,320 --> 00:01:21,420
that those are words
that would naturally

31
00:01:21,420 --> 00:01:24,000
appear between the
names of two actors,

32
00:01:24,000 --> 00:01:29,280
not between the name of a
film and the name of an actor.

33
00:01:29,280 --> 00:01:31,320
What I did was I wrote
a little bit of code

34
00:01:31,320 --> 00:01:36,240
to pull up the actual examples
that caused these weights

35
00:01:36,240 --> 00:01:37,950
to wind up being large.

36
00:01:37,950 --> 00:01:40,830
And what I realized was
there's a very common pattern

37
00:01:40,830 --> 00:01:48,270
which is like X appeared
in Y alongside Z.

38
00:01:48,270 --> 00:01:50,610
So X and Z are actors.

39
00:01:50,610 --> 00:01:51,690
Y is a film.

40
00:01:51,690 --> 00:01:56,040
X appeared in Y
alongside Z. So you

41
00:01:56,040 --> 00:01:58,860
have Y alongside
Z. That indicates

42
00:01:58,860 --> 00:02:03,420
that Z is an actor that
appeared in film Y.

43
00:02:03,420 --> 00:02:05,820
And I think something
similar happens for opposite.

44
00:02:05,820 --> 00:02:10,320
So it does make
sense that these are

45
00:02:10,320 --> 00:02:14,460
strong indicators of the
film performance relation.

46
00:02:14,460 --> 00:02:18,570
For has spouse, we have
wife, married, and husband.

47
00:02:18,570 --> 00:02:21,280
I think this makes
perfect sense.

48
00:02:21,280 --> 00:02:23,760
The one that's a bit
surprising is adjoins.

49
00:02:23,760 --> 00:02:28,080
So for adjoins we have
Cordoba, Taluks and Valais.

50
00:02:28,080 --> 00:02:31,560
It's odd to see specific
place names here.

51
00:02:31,560 --> 00:02:35,750
They certainly don't seem to
express the adjoins relation.

52
00:02:35,750 --> 00:02:38,340
I wonder if anyone has
a guess what's going on.

53
00:02:38,340 --> 00:02:40,900
I was really puzzled by this.

54
00:02:40,900 --> 00:02:44,610
And so again, I
wrote a bit of code

55
00:02:44,610 --> 00:02:48,360
to find the specific
examples that contributed

56
00:02:48,360 --> 00:02:51,150
to this result. I
looked for examples

57
00:02:51,150 --> 00:02:55,050
where the two entity mentions,
stand in the adjoins relation.

58
00:02:55,050 --> 00:02:59,770
And these terms, these specific
terms, appear in the middle.

59
00:02:59,770 --> 00:03:01,350
And when I looked
at the examples,

60
00:03:01,350 --> 00:03:03,990
I realized that
what's going on is

61
00:03:03,990 --> 00:03:08,890
that it's very common to have
lists of geographic locations.

62
00:03:08,890 --> 00:03:14,010
So A, B, C, D.
And in such lists,

63
00:03:14,010 --> 00:03:19,950
it's not uncommon that just
by chance A and C or A and D

64
00:03:19,950 --> 00:03:21,720
stand in the adjoins relation.

65
00:03:21,720 --> 00:03:25,620
Maybe it's a list of
provinces in a country.

66
00:03:25,620 --> 00:03:27,780
And of course, some
of those provinces

67
00:03:27,780 --> 00:03:29,680
are adjacent to each other.

68
00:03:29,680 --> 00:03:34,650
So if A adjoins
C or A adjoins D,

69
00:03:34,650 --> 00:03:40,500
that will tend to make B
appear as a positive indicator

70
00:03:40,500 --> 00:03:42,990
for the adjoins
relation and especially

71
00:03:42,990 --> 00:03:47,880
if the corpus just happens to
contain several such examples.

72
00:03:47,880 --> 00:03:49,880
So I think that's
what contributed

73
00:03:49,880 --> 00:03:55,080
to this puzzling result.

74
00:03:55,080 --> 00:03:57,660
The features with
large negative weights

75
00:03:57,660 --> 00:04:00,840
look a bit more haphazard.

76
00:04:00,840 --> 00:04:02,370
But I think that's
not surprising.

77
00:04:02,370 --> 00:04:06,990
It's kind of what you expect
for this kind of linear model.

78
00:04:06,990 --> 00:04:10,110


79
00:04:10,110 --> 00:04:15,450
By the way, you can fiddle with
the code that prints this out.

80
00:04:15,450 --> 00:04:18,425
And here it just
prints the top three.

81
00:04:18,425 --> 00:04:19,800
But you can fiddle
with the code,

82
00:04:19,800 --> 00:04:21,300
there's actually
a parameter that

83
00:04:21,300 --> 00:04:24,750
tells you-- it tells it how
many of the top of the list

84
00:04:24,750 --> 00:04:25,680
to print.

85
00:04:25,680 --> 00:04:27,940
And so you can print
much longer lists.

86
00:04:27,940 --> 00:04:33,840
And for many of the relations,
the top 20, even the top 50,

87
00:04:33,840 --> 00:04:37,200
features all look very
plausible and intuitive.

88
00:04:37,200 --> 00:04:40,910
And it's quite satisfying to
see those results come out.

89
00:04:40,910 --> 00:04:44,430


90
00:04:44,430 --> 00:04:46,230
Another way to gain
insight into our model

91
00:04:46,230 --> 00:04:50,100
is to use it to discover new
relation instances that don't

92
00:04:50,100 --> 00:04:52,500
currently appear in the KB.

93
00:04:52,500 --> 00:04:54,030
In fact, as we
discussed last time,

94
00:04:54,030 --> 00:04:56,880
this is the whole point of
building a relation extraction

95
00:04:56,880 --> 00:05:00,750
system, to augment
a KB with knowledge

96
00:05:00,750 --> 00:05:04,830
extracted from natural
language text at scale.

97
00:05:04,830 --> 00:05:09,940
So the decisive question is, can
our model do this effectively?

98
00:05:09,940 --> 00:05:13,020
We can't really evaluate this
capability automatically,

99
00:05:13,020 --> 00:05:17,160
because we have no other source
of ground truth than the KB

100
00:05:17,160 --> 00:05:18,300
itself.

101
00:05:18,300 --> 00:05:22,050
But we can evaluate it
manually by examining

102
00:05:22,050 --> 00:05:24,900
KB triples that aren't
in the KB but which

103
00:05:24,900 --> 00:05:29,250
our model really, really
thinks should be in the KB.

104
00:05:29,250 --> 00:05:32,080
So we wrote a
function to do this.

105
00:05:32,080 --> 00:05:34,770
It's called
find_new_relation_instances.

106
00:05:34,770 --> 00:05:38,010
And you can go look at the code.

107
00:05:38,010 --> 00:05:39,520
Here's how it works.

108
00:05:39,520 --> 00:05:43,200
It starts from corpus
examples containing

109
00:05:43,200 --> 00:05:45,480
pairs of entities
that don't belong

110
00:05:45,480 --> 00:05:47,580
to any relation in the KB.

111
00:05:47,580 --> 00:05:53,870
So these are what we described
last time as negative examples.

112
00:05:53,870 --> 00:05:56,930
We'll consider each
such pair of entities

113
00:05:56,930 --> 00:06:00,410
as a candidate to
join each relation.

114
00:06:00,410 --> 00:06:04,310
So we'll take the cross product
of all of those entity pairs

115
00:06:04,310 --> 00:06:07,180
and relations.

116
00:06:07,180 --> 00:06:12,850
We'll apply our model to all
of those candidate KB triples.

117
00:06:12,850 --> 00:06:16,330
And we'll just sort the results
by the probability assigned

118
00:06:16,330 --> 00:06:20,560
by the model in order to find
the most likely new instances

119
00:06:20,560 --> 00:06:22,000
of each relation.

120
00:06:22,000 --> 00:06:25,780
So we'll find the candidate KB
triples that aren't currently

121
00:06:25,780 --> 00:06:28,960
in the KB, but which
the model believes

122
00:06:28,960 --> 00:06:33,920
have really high
probability of being valid.

123
00:06:33,920 --> 00:06:36,220
So let's see what we
get when we run it.

124
00:06:36,220 --> 00:06:40,000
Here are the results for
the adjoins relation.

125
00:06:40,000 --> 00:06:44,140
Notice that the model
assigned a probability of 1.0

126
00:06:44,140 --> 00:06:45,820
to each of these pairs.

127
00:06:45,820 --> 00:06:49,180
It is totally convinced
that these pairs

128
00:06:49,180 --> 00:06:52,330
belong to the adjoins relation.

129
00:06:52,330 --> 00:06:56,490
But the results are,
well let's be honest,

130
00:06:56,490 --> 00:06:58,800
the results are terrible.

131
00:06:58,800 --> 00:07:03,780
Almost all of these pairs
belong to the contains relation,

132
00:07:03,780 --> 00:07:06,780
which by the way isn't actually
one of our 16 relations.

133
00:07:06,780 --> 00:07:11,310
But intuitively, they should
belong to a contains relation

134
00:07:11,310 --> 00:07:13,560
not the adjoins relation.

135
00:07:13,560 --> 00:07:17,310
You could make a case maybe
for Mexico and Atlantic

136
00:07:17,310 --> 00:07:20,400
Ocean belonging to
the adjoins relation.

137
00:07:20,400 --> 00:07:26,820
But I mean, to be honest,
even that one is a stretch.

138
00:07:26,820 --> 00:07:28,980
One other thing worth
noting, whenever the model

139
00:07:28,980 --> 00:07:32,040
predicts that X
adjoins Y, it also

140
00:07:32,040 --> 00:07:35,910
predicts that Y adjoins
X. You might for a moment

141
00:07:35,910 --> 00:07:39,420
think that this shows that
the model has understood

142
00:07:39,420 --> 00:07:43,290
that adjoins is a
symmetric relation.

143
00:07:43,290 --> 00:07:46,320
Unfortunately no, that's
not what's going on.

144
00:07:46,320 --> 00:07:50,370
It's just an artifact of how we
wrote the simple bag of words

145
00:07:50,370 --> 00:07:51,060
featurizer.

146
00:07:51,060 --> 00:07:52,920
That simple bag of
words featurizer

147
00:07:52,920 --> 00:07:57,430
makes no distinction between
forward and reverse examples.

148
00:07:57,430 --> 00:07:59,940
So it has no idea
which one comes first

149
00:07:59,940 --> 00:08:02,010
and which one comes second.

150
00:08:02,010 --> 00:08:04,470
And that will be true
for asymmetric relations

151
00:08:04,470 --> 00:08:08,510
just like for
symmetric relations.

152
00:08:08,510 --> 00:08:10,310
So this is not a
very promising start.

153
00:08:10,310 --> 00:08:17,510
It's-- I mean, we saw a pretty
good quantitative evaluation

154
00:08:17,510 --> 00:08:18,930
for this model.

155
00:08:18,930 --> 00:08:21,630
So this is a little
bit surprising.

156
00:08:21,630 --> 00:08:25,230
Let's see what we get
for some other relations.

157
00:08:25,230 --> 00:08:28,340
So here are the results
for the author relation.

158
00:08:28,340 --> 00:08:30,530
And these look a lot better.

159
00:08:30,530 --> 00:08:34,400
Once again, all of the
probabilities are 1.

160
00:08:34,400 --> 00:08:38,058
But this time, every single
one of these predictions

161
00:08:38,058 --> 00:08:40,149
is correct.

162
00:08:40,150 --> 00:08:43,330
Well not quite actually,
because the book

163
00:08:43,330 --> 00:08:46,180
is supposed to appear
first, like "Oliver

164
00:08:46,180 --> 00:08:49,270
Twist" and the author
second, Charles Dickens.

165
00:08:49,270 --> 00:08:51,830
So this first one
actually is correct.

166
00:08:51,830 --> 00:08:53,680
The second one is backwards.

167
00:08:53,680 --> 00:08:57,130
It has the author first
and the book second.

168
00:08:57,130 --> 00:09:00,140
Our model is completely
ignorant of order.

169
00:09:00,140 --> 00:09:04,660
So it's just as likely
to put things in reverse.

170
00:09:04,660 --> 00:09:07,180
But if you ignore
that, if you're

171
00:09:07,180 --> 00:09:11,180
willing to imagine that
we could easily fix that,

172
00:09:11,180 --> 00:09:14,020
then the results look great.

173
00:09:14,020 --> 00:09:17,290
We could put all of these
triples right into our KB

174
00:09:17,290 --> 00:09:21,760
and we'd have a bigger and
better KB because of it.

175
00:09:21,760 --> 00:09:25,450
This is relation
extraction at its finest.

176
00:09:25,450 --> 00:09:26,410
This is what we wanted.

177
00:09:26,410 --> 00:09:28,960


178
00:09:28,960 --> 00:09:30,850
Here are the results
for the capital relation

179
00:09:30,850 --> 00:09:33,100
and it's a similar picture.

180
00:09:33,100 --> 00:09:35,320
All of the
probabilities are 1.0.

181
00:09:35,320 --> 00:09:39,580
The ordering is
frequently reversed.

182
00:09:39,580 --> 00:09:41,140
It's very haphazard.

183
00:09:41,140 --> 00:09:45,910
But if you put that aside,
the results look very good.

184
00:09:45,910 --> 00:09:49,720
You could quibble
perhaps with Delhi here.

185
00:09:49,720 --> 00:09:52,930
I mean, the capital of
India is really New Delhi.

186
00:09:52,930 --> 00:09:58,780
But New Delhi is part of
Delhi, so you know, it's close.

187
00:09:58,780 --> 00:10:03,188
Still overall, I think
this looks really good.

188
00:10:03,188 --> 00:10:04,230
Let me show you one more.

189
00:10:04,230 --> 00:10:05,970
This is the last one I'll show.

190
00:10:05,970 --> 00:10:09,360
These are results for
the worked_at relation.

191
00:10:09,360 --> 00:10:12,510
And here, the results
are more mixed.

192
00:10:12,510 --> 00:10:16,880
So we have Stan Lee
and Marvel Comics.

193
00:10:16,880 --> 00:10:21,440
Sure, if you can say that Elon
Musk worked at Tesla Motors,

194
00:10:21,440 --> 00:10:25,460
then you can say that Stan
Lee worked at Marvel Comics.

195
00:10:25,460 --> 00:10:28,310
And while we're at
it, Genghis Khan

196
00:10:28,310 --> 00:10:30,500
worked at the Mongol Empire.

197
00:10:30,500 --> 00:10:33,170
Sure, why not?

198
00:10:33,170 --> 00:10:36,650
But the rest are nonsense.

199
00:10:36,650 --> 00:10:38,210
So why?

200
00:10:38,210 --> 00:10:39,980
What happened here?

201
00:10:39,980 --> 00:10:43,130
Well, when you encounter
surprising and mysterious

202
00:10:43,130 --> 00:10:45,260
results in your
model output, it's

203
00:10:45,260 --> 00:10:51,020
really good practice to go dig
into the data and investigate.

204
00:10:51,020 --> 00:10:52,790
And this is called
error analysis.

205
00:10:52,790 --> 00:10:56,875
And I want to show you a
couple of examples of that now.

206
00:10:56,875 --> 00:10:58,500
So first let's see
if we can figure out

207
00:10:58,500 --> 00:11:04,390
what happened with Lewis
Chevrolet and William C Durant.

208
00:11:04,390 --> 00:11:07,530
First, let's look up
the corpus examples

209
00:11:07,530 --> 00:11:10,590
containing these two entities.

210
00:11:10,590 --> 00:11:12,540
I'm only going to look
up the examples that

211
00:11:12,540 --> 00:11:14,970
have them in this order.

212
00:11:14,970 --> 00:11:17,590
I should look them up in
the other order as well.

213
00:11:17,590 --> 00:11:18,900
And as a matter of fact, I did.

214
00:11:18,900 --> 00:11:21,150
I'm just not going to
put that on the slide.

215
00:11:21,150 --> 00:11:25,390
I'm just going to focus on
what happens in this order.

216
00:11:25,390 --> 00:11:27,780
So I'm going to look
up these examples

217
00:11:27,780 --> 00:11:29,740
and print out what
they look like.

218
00:11:29,740 --> 00:11:31,890
And here's what we get.

219
00:11:31,890 --> 00:11:33,330
There are 12 examples.

220
00:11:33,330 --> 00:11:35,590
And they all look identical.

221
00:11:35,590 --> 00:11:37,630
Actually, I didn't print
the full context here.

222
00:11:37,630 --> 00:11:39,255
If you look at the
code closely, you'll

223
00:11:39,255 --> 00:11:42,150
see that I'm printing
the suffix of the left

224
00:11:42,150 --> 00:11:43,650
and the prefix of the right.

225
00:11:43,650 --> 00:11:46,890
So there's more context
further out on left and right.

226
00:11:46,890 --> 00:11:49,200
And if you did see
the full context,

227
00:11:49,200 --> 00:11:52,740
you would realize that the
examples do differ slightly.

228
00:11:52,740 --> 00:11:54,220
But they're very, very similar.

229
00:11:54,220 --> 00:11:55,388
They're near duplicates.

230
00:11:55,388 --> 00:11:57,180
I mentioned this last
time that this is one

231
00:11:57,180 --> 00:11:58,710
of the warts of this dataset.

232
00:11:58,710 --> 00:12:01,920
It contains a lot of
near duplicate examples.

233
00:12:01,920 --> 00:12:04,500
And I think this is an
unfortunate consequence

234
00:12:04,500 --> 00:12:08,760
of the way the sample was
constructed, the way the web

235
00:12:08,760 --> 00:12:11,130
documents that this
corpus was based on

236
00:12:11,130 --> 00:12:13,350
were sampled from the web.

237
00:12:13,350 --> 00:12:17,220
And it seems like
that's bitten us here.

238
00:12:17,220 --> 00:12:19,950
But it still leaves
the question, why?

239
00:12:19,950 --> 00:12:22,410
Why did that repetition
leave the model

240
00:12:22,410 --> 00:12:25,230
to predict that
this pair belongs

241
00:12:25,230 --> 00:12:26,940
to the worked_at relation?

242
00:12:26,940 --> 00:12:30,030
Because it doesn't look
obvious that that's

243
00:12:30,030 --> 00:12:31,730
the right relation here.

244
00:12:31,730 --> 00:12:37,080
I suspect that it's because of
the word "founder", because X

245
00:12:37,080 --> 00:12:43,260
being a founder of Y strongly
implies that X worked at Y.

246
00:12:43,260 --> 00:12:45,480
And actually, we can check this.

247
00:12:45,480 --> 00:12:48,330
It's not that hard
to write some code

248
00:12:48,330 --> 00:12:51,570
to inspect the weight
that was assigned

249
00:12:51,570 --> 00:12:54,990
to the word "founder"
in the model

250
00:12:54,990 --> 00:12:57,450
for the worked_at relation.

251
00:12:57,450 --> 00:13:01,130
So here's a little bit
of code that does that.

252
00:13:01,130 --> 00:13:03,200
And sure enough, in the
model for worked at,

253
00:13:03,200 --> 00:13:08,990
the word "founder" gets a weight
of 2.05, which is pretty large.

254
00:13:08,990 --> 00:13:11,750
If you look at the
distribution of weights,

255
00:13:11,750 --> 00:13:13,490
it's a relatively large one.

256
00:13:13,490 --> 00:13:16,310
I forget exactly, but I
think it's in the top 10.

257
00:13:16,310 --> 00:13:20,720
It's a relatively significant,
quite significant,

258
00:13:20,720 --> 00:13:23,200
feature for this model.

259
00:13:23,200 --> 00:13:24,200
So that's what happened.

260
00:13:24,200 --> 00:13:25,820
We've got 12 examples.

261
00:13:25,820 --> 00:13:28,730
Each of them is contributing
a sizable weight.

262
00:13:28,730 --> 00:13:32,630
And the result is that the
model is completely convinced

263
00:13:32,630 --> 00:13:39,200
that this is the right-- that
the worked_at relation holds

264
00:13:39,200 --> 00:13:40,310
here.

265
00:13:40,310 --> 00:13:41,600
By the way, I didn't check.

266
00:13:41,600 --> 00:13:46,010
But I'm confident that
the founder, the model

267
00:13:46,010 --> 00:13:49,820
for the founders relation, will
also predict that the founder

268
00:13:49,820 --> 00:13:51,350
relation holds here.

269
00:13:51,350 --> 00:13:54,050
An understanding
of what went wrong

270
00:13:54,050 --> 00:13:59,307
here could help to stimulate
some ideas for how to fix it.

271
00:13:59,307 --> 00:14:00,140
I don't think I'll--

272
00:14:00,140 --> 00:14:00,950
I think I won't.

273
00:14:00,950 --> 00:14:02,150
I mean, I have some ideas.

274
00:14:02,150 --> 00:14:03,770
I think I won't give them away.

275
00:14:03,770 --> 00:14:09,163
But I hope this underscores
the value of error analysis.

276
00:14:09,163 --> 00:14:10,580
If you really want
to understand--

277
00:14:10,580 --> 00:14:12,860
when you see weird
results, you really

278
00:14:12,860 --> 00:14:15,950
want to understand what's
going on in your data that

279
00:14:15,950 --> 00:14:19,560
led to these weird results.

280
00:14:19,560 --> 00:14:21,423
Let me show you one
more example that has

281
00:14:21,423 --> 00:14:22,590
a bit of a different flavor.

282
00:14:22,590 --> 00:14:26,210
Let's look at what's going
on with Homer and the Iliad.

283
00:14:26,210 --> 00:14:28,980
I wrote a little bit of code
to investigate this one too.

284
00:14:28,980 --> 00:14:32,100
And I'm not going to show
the whole investigation

285
00:14:32,100 --> 00:14:33,540
but I'm just going
to cherry pick

286
00:14:33,540 --> 00:14:36,963
the most informative results.

287
00:14:36,963 --> 00:14:38,630
So one thing that I
notice is that there

288
00:14:38,630 --> 00:14:42,480
are a lot of examples
for Homer and Iliad.

289
00:14:42,480 --> 00:14:47,540
In fact, there are 118 of
them just in that direction.

290
00:14:47,540 --> 00:14:50,800
There's more in the
reverse direction.

291
00:14:50,800 --> 00:14:52,730
That's impressive.

292
00:14:52,730 --> 00:14:57,320
But again, by itself, it
doesn't explain why worked_at

293
00:14:57,320 --> 00:14:59,510
looked like a good prediction.

294
00:14:59,510 --> 00:15:01,670
By the way, I did
check to see if it

295
00:15:01,670 --> 00:15:05,270
was the same explanation as last
time, a lot of near duplicates.

296
00:15:05,270 --> 00:15:05,930
It's not.

297
00:15:05,930 --> 00:15:08,070
That's not what's going on here.

298
00:15:08,070 --> 00:15:12,710
But the next thing I did
was to write some code

299
00:15:12,710 --> 00:15:16,250
to count up the most
common middles that

300
00:15:16,250 --> 00:15:20,840
join Homer and Iliad
across these 118 examples.

301
00:15:20,840 --> 00:15:24,710
And so that code
looks like this.

302
00:15:24,710 --> 00:15:26,350
And here are the results.

303
00:15:26,350 --> 00:15:29,210
And there was one middle
that strongly dominated.

304
00:15:29,210 --> 00:15:34,240
And it's apostrophe s,
so as in "Homer's Iliad."

305
00:15:34,240 --> 00:15:37,000


306
00:15:37,000 --> 00:15:41,080
So that makes sense because
clearly the possessive can

307
00:15:41,080 --> 00:15:43,570
indicate the author relation.

308
00:15:43,570 --> 00:15:47,500
You expect to see Homer's
Iliad and Jane Austen's Pride

309
00:15:47,500 --> 00:15:53,560
and Prejudice and many
other similar formulations.

310
00:15:53,560 --> 00:15:56,170
But the apostrophe
s can equally well

311
00:15:56,170 --> 00:16:01,570
indicate the worked_at relation,
as in Tesla's Elon Musk

312
00:16:01,570 --> 00:16:04,930
or Microsoft's Bill Gates.

313
00:16:04,930 --> 00:16:08,140
So this apostrophe s is
really highly ambiguous.

314
00:16:08,140 --> 00:16:10,720


315
00:16:10,720 --> 00:16:15,610
Just to confirm that
this is actually

316
00:16:15,610 --> 00:16:18,430
significant to the
result that we saw let's

317
00:16:18,430 --> 00:16:21,910
check what weight was
assigned to apostrophe s

318
00:16:21,910 --> 00:16:23,690
in the model for worked_at.

319
00:16:23,690 --> 00:16:26,540
So this code is similar to the
code on the previous slide.

320
00:16:26,540 --> 00:16:30,250
But this time we're looking for
the weight for apostrophe s.

321
00:16:30,250 --> 00:16:33,890
And it turns out that
the weight was 0.58.

322
00:16:33,890 --> 00:16:34,390
OK.

323
00:16:34,390 --> 00:16:36,280
It's not a huge weight.

324
00:16:36,280 --> 00:16:38,410
But it's not small either.

325
00:16:38,410 --> 00:16:43,610
And this feature occurred
51 times across the corpus.

326
00:16:43,610 --> 00:16:45,160
So I think that's what happened.

327
00:16:45,160 --> 00:16:51,940
We had a non-trivial
amount of weight

328
00:16:51,940 --> 00:16:54,850
that got added up 51 times.

329
00:16:54,850 --> 00:16:57,760
And we wound up with a
really big contribution

330
00:16:57,760 --> 00:17:02,478
and the model feeling really
confident about this relation.

331
00:17:02,478 --> 00:17:04,020
So again, thinking
about this problem

332
00:17:04,020 --> 00:17:06,118
might suggest some
strategies for how

333
00:17:06,118 --> 00:17:09,039
to reduce that ambiguity.

334
00:17:09,040 --> 00:17:11,670
The fundamental problem
here is that apostrophe s

335
00:17:11,670 --> 00:17:15,030
is highly ambiguous.

336
00:17:15,030 --> 00:17:16,500
But a good question
to ask yourself

337
00:17:16,500 --> 00:17:20,579
is, is there other information
in the sentence that

338
00:17:20,579 --> 00:17:24,089
could help to distinguish
the author relation

339
00:17:24,089 --> 00:17:26,368
from the worked_at relation?

340
00:17:26,368 --> 00:17:27,539
And I think there is.

341
00:17:27,540 --> 00:17:30,030
Again, I don't want
to give too much away.

342
00:17:30,030 --> 00:17:32,790
But I think there
is other evidence

343
00:17:32,790 --> 00:17:36,240
in these sentences that could
help to tease apart these two

344
00:17:36,240 --> 00:17:37,110
relations.

345
00:17:37,110 --> 00:17:39,000
And this kind of
error analysis is

346
00:17:39,000 --> 00:17:41,640
really indispensable to the
model development process.

347
00:17:41,640 --> 00:17:46,080


348
00:17:46,080 --> 00:17:48,000
Now for the homework
and the bake off.

349
00:17:48,000 --> 00:17:50,570
We're going to turn
you loose to find ways

350
00:17:50,570 --> 00:17:53,120
to improve this baseline model.

351
00:17:53,120 --> 00:17:56,010
And there are a lot
of possibilities.

352
00:17:56,010 --> 00:17:59,780
One area for innovation is in
the feature representation we

353
00:17:59,780 --> 00:18:02,370
pass to the learning algorithm.

354
00:18:02,370 --> 00:18:05,690
So far, we've just used a simple
bag of words representation.

355
00:18:05,690 --> 00:18:08,330
But you can imagine lots
of ways to enhance this.

356
00:18:08,330 --> 00:18:11,180
So you could use
word embeddings,

357
00:18:11,180 --> 00:18:12,890
like the glove embeddings.

358
00:18:12,890 --> 00:18:14,810
You could use a bag of
words representation

359
00:18:14,810 --> 00:18:19,250
that distinguishes between
forward and reverse contexts.

360
00:18:19,250 --> 00:18:23,570
You could use bigrams
or longer n-grams.

361
00:18:23,570 --> 00:18:25,580
You could leverage the
part of speech tags

362
00:18:25,580 --> 00:18:31,160
that we have in the corpus
or information from WordNet.

363
00:18:31,160 --> 00:18:33,230
Much of the early work
on relation extraction

364
00:18:33,230 --> 00:18:35,840
relied heavily on
syntactic features.

365
00:18:35,840 --> 00:18:38,000
So maybe try that.

366
00:18:38,000 --> 00:18:40,100
And so far, we've
used features based

367
00:18:40,100 --> 00:18:43,965
only on the middle phrase, the
phrase between the two entity

368
00:18:43,965 --> 00:18:44,570
mentions.

369
00:18:44,570 --> 00:18:47,270
You could also try using
information about the entity

370
00:18:47,270 --> 00:18:51,680
mentions themselves, for
example, the entity types.

371
00:18:51,680 --> 00:18:53,120
Or you could try
deriving features

372
00:18:53,120 --> 00:18:55,680
from the left and right context.

373
00:18:55,680 --> 00:18:59,750
There are a lot of
possibilities for richer feature

374
00:18:59,750 --> 00:19:02,860
representations.

375
00:19:02,860 --> 00:19:06,670
There's also a lot of room for
innovation with the model type.

376
00:19:06,670 --> 00:19:09,550
Our baseline model is a
simple linear model optimized

377
00:19:09,550 --> 00:19:11,140
with logistic regression.

378
00:19:11,140 --> 00:19:12,440
That's a good place to start.

379
00:19:12,440 --> 00:19:14,590
But there are many
other possibilities.

380
00:19:14,590 --> 00:19:17,740
If you want to stick with linear
models, you could use an SVM.

381
00:19:17,740 --> 00:19:21,040
And sklearn makes that easy.

382
00:19:21,040 --> 00:19:23,920
Or you could experiment
with neural networks.

383
00:19:23,920 --> 00:19:26,260
You could use a simple
feed-forward neural network

384
00:19:26,260 --> 00:19:29,590
as a drop in replacement
for our linear model.

385
00:19:29,590 --> 00:19:32,930
Or since examples can
be a variable length,

386
00:19:32,930 --> 00:19:37,148
you might consider a recurrent
neural network, like an LSTM.

387
00:19:37,148 --> 00:19:39,190
If you go this way you'll
have to think carefully

388
00:19:39,190 --> 00:19:41,950
about how to encode the input.

389
00:19:41,950 --> 00:19:44,950
If the input is just
the middle phrase,

390
00:19:44,950 --> 00:19:48,760
things are probably
relatively straightforward.

391
00:19:48,760 --> 00:19:51,490
But if you want to include
the entity mentions

392
00:19:51,490 --> 00:19:54,040
or the left and
right context, you

393
00:19:54,040 --> 00:19:55,840
might need to think
carefully about how

394
00:19:55,840 --> 00:19:59,300
to demarcate the segments.

395
00:19:59,300 --> 00:20:01,550
Or you could use a
transformer-based architecture

396
00:20:01,550 --> 00:20:03,740
like BERTs, although
the quantity

397
00:20:03,740 --> 00:20:05,570
of training data that
we have available

398
00:20:05,570 --> 00:20:08,240
here might be a bit small.

399
00:20:08,240 --> 00:20:10,370
I think all of these
are potentially

400
00:20:10,370 --> 00:20:14,310
interesting and fruitful
directions for exploration.

401
00:20:14,310 --> 00:20:18,190
And I think you can have
a lot of fun with this.

402
00:20:18,190 --> 00:20:22,038


