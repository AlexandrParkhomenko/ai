1
00:00:00,000 --> 00:00:04,128


2
00:00:04,128 --> 00:00:05,670
CHRISTOPHER POTTS:
Welcome, everyone.

3
00:00:05,670 --> 00:00:07,420
This is the first
screencast in our series

4
00:00:07,420 --> 00:00:09,200
on natural language
inference, or NLI.

5
00:00:09,200 --> 00:00:11,055
This is one of my
favorite problems.

6
00:00:11,055 --> 00:00:12,680
What I'd like to do
is give you a sense

7
00:00:12,680 --> 00:00:14,840
for how the task is
formulated and then

8
00:00:14,840 --> 00:00:16,490
situate the task
within the broader

9
00:00:16,490 --> 00:00:20,110
landscape of ideas for NLU.

10
00:00:20,110 --> 00:00:21,958
As usual, we have a
bunch of materials

11
00:00:21,958 --> 00:00:24,250
that would allow you to get
hands-on with this problem.

12
00:00:24,250 --> 00:00:28,090
The core module is nli.py, and
then there are two notebooks.

13
00:00:28,090 --> 00:00:29,920
The first introduces
the three data

14
00:00:29,920 --> 00:00:33,570
sets that we'll be exploring
in detail, SNLI, multiNLI,

15
00:00:33,570 --> 00:00:35,560
and adversarial NLI.

16
00:00:35,560 --> 00:00:38,230
And the second notebook offers
a bunch of different modeling

17
00:00:38,230 --> 00:00:39,700
approaches.

18
00:00:39,700 --> 00:00:41,860
It really covers core
approaches that people

19
00:00:41,860 --> 00:00:43,990
have taken to NLI
in the past, and I

20
00:00:43,990 --> 00:00:46,570
hope it points to some
avenues for modifying

21
00:00:46,570 --> 00:00:49,480
those architectures, possibly
in the service of developing

22
00:00:49,480 --> 00:00:52,860
an original system
for a final project.

23
00:00:52,860 --> 00:00:55,320
There's also an associated
homework and bakeoff.

24
00:00:55,320 --> 00:00:58,470
I emphasize that this is not
required for us this quarter.

25
00:00:58,470 --> 00:01:01,080
I'm mentioning it because
I think the problem is

26
00:01:01,080 --> 00:01:02,050
an interesting one.

27
00:01:02,050 --> 00:01:03,480
It's a word
entailment task which

28
00:01:03,480 --> 00:01:05,940
is an interesting
small-scale problem

29
00:01:05,940 --> 00:01:08,520
that I believe could be
used to stress-test an NLI

30
00:01:08,520 --> 00:01:11,790
system in interesting ways.

31
00:01:11,790 --> 00:01:14,040
The core readings cover
the three datasets

32
00:01:14,040 --> 00:01:15,730
that will be in focus for us.

33
00:01:15,730 --> 00:01:17,640
And then the final
reading listed here,

34
00:01:17,640 --> 00:01:20,850
was I think, the paper that
introduced attention mechanisms

35
00:01:20,850 --> 00:01:23,970
into the study of NLI and
that had an impact that

36
00:01:23,970 --> 00:01:26,540
went well beyond this task.

37
00:01:26,540 --> 00:01:28,030
And then, for
additional readings,

38
00:01:28,030 --> 00:01:30,470
I'm kind of suggesting
a range of things.

39
00:01:30,470 --> 00:01:32,512
Some of these readings
cover core fundamentals

40
00:01:32,512 --> 00:01:33,970
for deep learning
that I think will

41
00:01:33,970 --> 00:01:36,970
be useful in the
context of studying NLI.

42
00:01:36,970 --> 00:01:39,850
Some of them help you with kind
of a foundational understanding

43
00:01:39,850 --> 00:01:42,460
of the NLI task and how
you might think about it.

44
00:01:42,460 --> 00:01:44,800
And then some of them
are meant to push

45
00:01:44,800 --> 00:01:48,430
us to stress test our system,
think adversarially, and maybe

46
00:01:48,430 --> 00:01:50,140
find artifacts in our datasets.

47
00:01:50,140 --> 00:01:52,930
And those are going to be
themes of later screencasts

48
00:01:52,930 --> 00:01:55,770
in the series.

49
00:01:55,770 --> 00:01:58,230
To begin getting a sense for
how the task is formulated,

50
00:01:58,230 --> 00:02:01,090
let's start with some
simple examples here.

51
00:02:01,090 --> 00:02:04,260
So in NLI, we have as our
inputs a premise sentence

52
00:02:04,260 --> 00:02:06,300
and a hypothesis
sentence, and the task

53
00:02:06,300 --> 00:02:07,920
is a classification one.

54
00:02:07,920 --> 00:02:10,086
So in this simple example
here, the premise sentence

55
00:02:10,086 --> 00:02:11,790
is, "a turtle danced."

56
00:02:11,790 --> 00:02:13,830
The hypothesis sentence
is "a turtle moved."

57
00:02:13,830 --> 00:02:16,290
Both of those are system
inputs, and our task

58
00:02:16,290 --> 00:02:17,940
is to assign one
of three labels.

59
00:02:17,940 --> 00:02:22,310
In this case, the correct
label would be "entails."

60
00:02:22,310 --> 00:02:24,410
The second example looks
simple, but it actually

61
00:02:24,410 --> 00:02:27,140
begins to suggest how the
task is actually formulated.

62
00:02:27,140 --> 00:02:29,270
We have as our premise, turtle.

63
00:02:29,270 --> 00:02:31,250
And as our
hypothesis, linguists.

64
00:02:31,250 --> 00:02:33,710
And what we would like to
do in the context of NLI

65
00:02:33,710 --> 00:02:36,170
is assign that the
contradicts label.

66
00:02:36,170 --> 00:02:37,760
Now, you might pause
there and think,

67
00:02:37,760 --> 00:02:41,450
it's not a logical fact that
turtles can be linguists,

68
00:02:41,450 --> 00:02:43,310
so surely contradiction
is too strong,

69
00:02:43,310 --> 00:02:45,620
but it is a common
sense that to kind

70
00:02:45,620 --> 00:02:47,270
of natural inference
about the world

71
00:02:47,270 --> 00:02:49,640
we live in that no
turtles are linguists,

72
00:02:49,640 --> 00:02:51,530
and it's for that reason
that we would choose

73
00:02:51,530 --> 00:02:54,200
the contradicts label, and that
begins to key into the fact

74
00:02:54,200 --> 00:02:57,560
that fundamentally NLI is
not a logical reasoning

75
00:02:57,560 --> 00:03:03,070
task, but a more general
common sense reasoning task.

76
00:03:03,070 --> 00:03:05,632
"Every reptile danced"
is neutral with respect

77
00:03:05,632 --> 00:03:07,840
to "a turtle ate" which is
just to say that these two

78
00:03:07,840 --> 00:03:11,720
sentences can be true or false
independently of each other.

79
00:03:11,720 --> 00:03:13,570
And now, with entails
contradiction neutral,

80
00:03:13,570 --> 00:03:16,360
we have the three labels
that are standardly used

81
00:03:16,360 --> 00:03:20,200
for NLI datasets at this point.

82
00:03:20,200 --> 00:03:21,880
Look at some
additional examples.

83
00:03:21,880 --> 00:03:24,370
"Some turtles walk"
contradicts "no turtles move."

84
00:03:24,370 --> 00:03:26,740
I think that's straightforward.

85
00:03:26,740 --> 00:03:29,080
Here's one that shows how
intricate this could get.

86
00:03:29,080 --> 00:03:31,750
So the premise is "James
Byron Dean refused

87
00:03:31,750 --> 00:03:33,340
to move without blue jeans."

88
00:03:33,340 --> 00:03:36,048
Entails "James Dean didn't
dance without pants."

89
00:03:36,048 --> 00:03:37,840
This highlights two
aspects of the problem.

90
00:03:37,840 --> 00:03:41,050
First, you might have to do
some complex named entity

91
00:03:41,050 --> 00:03:44,320
recognition on James
Byron Dean and James Dean

92
00:03:44,320 --> 00:03:47,530
to figure out that these
are coreferring expressions.

93
00:03:47,530 --> 00:03:50,230
And you also might encounter
real linguistic complexity,

94
00:03:50,230 --> 00:03:53,170
in this case, emphasizing
things involving how negations

95
00:03:53,170 --> 00:03:56,080
interact with each other.

96
00:03:56,080 --> 00:03:59,160
This next example begins to show
how much common sense reasoning

97
00:03:59,160 --> 00:04:01,390
could be brought into the task.

98
00:04:01,390 --> 00:04:05,070
So the premise is Mitsubishi's
new vehicle sales in the US

99
00:04:05,070 --> 00:04:08,400
fell 46% in June,
and the hypothesis is

100
00:04:08,400 --> 00:04:11,180
Mitsubishi's sales rose 46%.

101
00:04:11,180 --> 00:04:14,100
And we would standardly say that
that is in the contradiction

102
00:04:14,100 --> 00:04:14,640
relation.

103
00:04:14,640 --> 00:04:16,709
Now again, you might
pause and think

104
00:04:16,709 --> 00:04:19,380
it is certainly possible
even in our world

105
00:04:19,380 --> 00:04:22,050
that Mitsubishi
could see a 46% rise

106
00:04:22,050 --> 00:04:23,790
and fall in the same month.

107
00:04:23,790 --> 00:04:26,550
So surely these should
be labeled neutral.

108
00:04:26,550 --> 00:04:29,940
But I think what you'll find
in NLI datasets is that these

109
00:04:29,940 --> 00:04:32,940
are called contradiction
on the informal assumption

110
00:04:32,940 --> 00:04:34,740
that the premise
and hypothesis are

111
00:04:34,740 --> 00:04:36,750
talking about the same event.

112
00:04:36,750 --> 00:04:39,630
And in that context, we would
say that these are common sense

113
00:04:39,630 --> 00:04:41,597
contradictions.

114
00:04:41,597 --> 00:04:43,180
Here's another example
that highlights

115
00:04:43,180 --> 00:04:45,460
how much pragmatics could
be brought into the problem.

116
00:04:45,460 --> 00:04:49,000
The premise is Acme reported
that its CEO resigned,

117
00:04:49,000 --> 00:04:52,090
and the hypothesis is
that Acme's CEO resigned.

118
00:04:52,090 --> 00:04:54,370
We would probably
say entailment there.

119
00:04:54,370 --> 00:04:56,590
Even though, in a
strict logical sense,

120
00:04:56,590 --> 00:04:59,770
the premise does not entail the
hypothesis because, of course,

121
00:04:59,770 --> 00:05:02,680
the company could be reporting
things that are false.

122
00:05:02,680 --> 00:05:04,630
But here, we kind of
make an assumption

123
00:05:04,630 --> 00:05:07,510
that the company is an
authority and will likely

124
00:05:07,510 --> 00:05:10,277
report true things
about facts like this,

125
00:05:10,277 --> 00:05:11,860
and therefore we
allow that this would

126
00:05:11,860 --> 00:05:13,420
be in the entailment relation.

127
00:05:13,420 --> 00:05:17,943
Again not logical but much
more like common sense.

128
00:05:17,943 --> 00:05:19,360
So just to emphasize
this, here is

129
00:05:19,360 --> 00:05:22,240
kind of the fundamental
question that we confront.

130
00:05:22,240 --> 00:05:25,150
Does the premise justify an
inference to the hypothesis?

131
00:05:25,150 --> 00:05:29,980
Commonsense reasoning,
rather than strict logic.

132
00:05:29,980 --> 00:05:31,948
Two other characteristics
of this task

133
00:05:31,948 --> 00:05:33,490
in the modern era
are, first, there's

134
00:05:33,490 --> 00:05:35,560
a focus on local
inference steps.

135
00:05:35,560 --> 00:05:38,290
That is just one premise
and one hypothesis,

136
00:05:38,290 --> 00:05:41,080
rather than long
deductive chains.

137
00:05:41,080 --> 00:05:42,640
And the second is
that the emphasis

138
00:05:42,640 --> 00:05:45,070
is really on the variability
of linguistic expressions,

139
00:05:45,070 --> 00:05:47,980
so when people have created
the large benchmark tasks

140
00:05:47,980 --> 00:05:51,220
in this space, they have largely
focused on just collecting

141
00:05:51,220 --> 00:05:54,700
a lot of data and not
placed any special emphasis

142
00:05:54,700 --> 00:05:57,700
on collecting examples that
have a lot of negations

143
00:05:57,700 --> 00:05:59,890
or quantifiers or
something that would really

144
00:05:59,890 --> 00:06:03,702
shine a spotlight on linguistic
and semantic complexity.

145
00:06:03,702 --> 00:06:05,410
So that's worth keeping
in mind about how

146
00:06:05,410 --> 00:06:09,790
we're thinking about the
task in the present day.

147
00:06:09,790 --> 00:06:11,530
If you would like
additional perspectives

148
00:06:11,530 --> 00:06:14,455
on this, including some disputes
about exactly how to think

149
00:06:14,455 --> 00:06:16,830
about the problem, and what
would be the most productive,

150
00:06:16,830 --> 00:06:19,163
I would encourage you to check
out of these three papers

151
00:06:19,163 --> 00:06:21,550
by a lot of Stanford
researchers.

152
00:06:21,550 --> 00:06:23,230
I think the fundamental
outcome of this

153
00:06:23,230 --> 00:06:25,660
is that we do want to focus
on common sense reasoning,

154
00:06:25,660 --> 00:06:28,180
even though that's
a kind of amorphous

155
00:06:28,180 --> 00:06:30,220
and difficult to define concept.

156
00:06:30,220 --> 00:06:33,850
It's nonetheless arguably
the useful one for us

157
00:06:33,850 --> 00:06:35,935
when we think about
developing practical systems.

158
00:06:35,935 --> 00:06:38,890


159
00:06:38,890 --> 00:06:41,500
Now, in a visionary paper
that really set the agenda

160
00:06:41,500 --> 00:06:45,070
for NLI, Dagan et
al, 2006, they make

161
00:06:45,070 --> 00:06:47,890
a lot of connections
between NLI and the broader

162
00:06:47,890 --> 00:06:49,340
landscape of NLU.

163
00:06:49,340 --> 00:06:51,460
So let me just read this
opening statement here.

164
00:06:51,460 --> 00:06:54,010
It seems that major
inferences, as needed

165
00:06:54,010 --> 00:06:56,470
by multiple
applications, can indeed

166
00:06:56,470 --> 00:06:59,110
be cast in terms of
textual entailment.

167
00:06:59,110 --> 00:07:01,930
Consequently, we hypothesize
that textual entailment

168
00:07:01,930 --> 00:07:04,540
recognition is a
suitable generic task

169
00:07:04,540 --> 00:07:07,500
for evaluating and comparing
applied semantic inference

170
00:07:07,500 --> 00:07:08,000
models.

171
00:07:08,000 --> 00:07:10,840
Eventually, such efforts
can promote the development

172
00:07:10,840 --> 00:07:12,880
of entailment
recognition engines

173
00:07:12,880 --> 00:07:17,060
which may provide useful generic
modules across applications.

174
00:07:17,060 --> 00:07:19,390
So wonderful vision and
a spin we might put on

175
00:07:19,390 --> 00:07:22,870
in the present day is that
what we might hope from NLI

176
00:07:22,870 --> 00:07:25,930
is that since reasoning about
entailment and contradiction

177
00:07:25,930 --> 00:07:29,050
are truly fundamental
to our use of language,

178
00:07:29,050 --> 00:07:31,150
that pre-training
on the NLI task

179
00:07:31,150 --> 00:07:34,780
might give us representations
that are useful in lots

180
00:07:34,780 --> 00:07:37,000
of different contexts.

181
00:07:37,000 --> 00:07:38,710
And Dagan et al
actually continue

182
00:07:38,710 --> 00:07:42,460
by showing that we can formulate
a lot of traditional tasks

183
00:07:42,460 --> 00:07:43,870
as NLI tasks.

184
00:07:43,870 --> 00:07:45,940
And here are just a
few examples of that.

185
00:07:45,940 --> 00:07:48,220
If our task is
paraphrased, we might

186
00:07:48,220 --> 00:07:50,050
say that in the
NLI context, that

187
00:07:50,050 --> 00:07:53,770
means we want equality or mutual
entailment between the text

188
00:07:53,770 --> 00:07:54,670
and the paraphrase.

189
00:07:54,670 --> 00:07:57,430
That is premise
and the hypothesis.

190
00:07:57,430 --> 00:07:59,510
For summarization, we
would do something weaker.

191
00:07:59,510 --> 00:08:02,140
We would hope just that the
text, the original text,

192
00:08:02,140 --> 00:08:04,810
entailed the summaries
allowing that the summary might

193
00:08:04,810 --> 00:08:07,630
be weaker or more general.

194
00:08:07,630 --> 00:08:09,910
For information retrieval,
we kind of do the reverse.

195
00:08:09,910 --> 00:08:14,330
Here we want to find documents
that entail the query.

196
00:08:14,330 --> 00:08:16,580
And then for question
answering, it's kind of similar.

197
00:08:16,580 --> 00:08:18,455
We could formulate that
as an entailment task

198
00:08:18,455 --> 00:08:21,590
by saying that what we want
is to find answers that entail

199
00:08:21,590 --> 00:08:24,080
the question, and the way we
might think about entailment

200
00:08:24,080 --> 00:08:26,190
for questions is kind
of illustrated here,

201
00:08:26,190 --> 00:08:28,950
where we would informally
convert a question like,

202
00:08:28,950 --> 00:08:29,900
who left?

203
00:08:29,900 --> 00:08:32,570
Into someone left, to
give us a statement.

204
00:08:32,570 --> 00:08:36,860
And then we could say that Sandy
left is an answer to, who left?

205
00:08:36,860 --> 00:08:40,010
In the sense that it
entails someone left.

206
00:08:40,010 --> 00:08:41,960
And I think there are
many other tasks that we

207
00:08:41,960 --> 00:08:43,350
could formulate in this way.

208
00:08:43,350 --> 00:08:46,520
And it does show you just
how fundamental entailment

209
00:08:46,520 --> 00:08:51,600
and contradiction are to
reasoning and language.

210
00:08:51,600 --> 00:08:54,100
And finally, let me give you a
sense for the model landscape

211
00:08:54,100 --> 00:08:55,060
and how it has changed.

212
00:08:55,060 --> 00:08:59,090
NLI is a pretty old problem
in the field, and as a result,

213
00:08:59,090 --> 00:09:01,930
we've seen a wide spectrum
of different approaches.

214
00:09:01,930 --> 00:09:03,820
In the earliest
days, you had a lot

215
00:09:03,820 --> 00:09:06,460
of systems that were kind of
focused on logic and theorem

216
00:09:06,460 --> 00:09:07,360
proving.

217
00:09:07,360 --> 00:09:09,070
And I've characterized
those systems here

218
00:09:09,070 --> 00:09:12,250
as offering really
deep representations,

219
00:09:12,250 --> 00:09:14,530
but they weren't especially
effective in the sense

220
00:09:14,530 --> 00:09:17,440
that they worked only for
the domains and examples

221
00:09:17,440 --> 00:09:20,170
that the system designers
had been able to anticipate.

222
00:09:20,170 --> 00:09:22,760
So they're kind of brittle.

223
00:09:22,760 --> 00:09:25,460
Following that, you have
a kind of exploration

224
00:09:25,460 --> 00:09:28,580
of what Bill MacCartney called
natural logic approaches.

225
00:09:28,580 --> 00:09:31,723
Bill was one of the early
innovators in this space.

226
00:09:31,723 --> 00:09:33,890
I think he actually coined
the term natural language

227
00:09:33,890 --> 00:09:34,700
inference.

228
00:09:34,700 --> 00:09:36,290
And he explored
natural logic, which

229
00:09:36,290 --> 00:09:39,650
has some of the aspects of
logic and theorem proving,

230
00:09:39,650 --> 00:09:42,050
but it's kind of more
open and easily amenable

231
00:09:42,050 --> 00:09:44,787
to tackling a lot of data.

232
00:09:44,787 --> 00:09:46,370
And so those systems
were consequently

233
00:09:46,370 --> 00:09:48,297
a little less steep but
also more effective,

234
00:09:48,297 --> 00:09:50,630
and a similar thing happened
with these semantic graphs,

235
00:09:50,630 --> 00:09:53,900
which is providing rich,
conceptual representations

236
00:09:53,900 --> 00:09:57,530
of the underlying domain
that we want to reason about.

237
00:09:57,530 --> 00:10:02,630
Another interesting thing
here is that, until recently,

238
00:10:02,630 --> 00:10:05,510
it was the case that clever
hand-built features, which

239
00:10:05,510 --> 00:10:08,910
I'll show you some a bit later
in the screencast series.

240
00:10:08,910 --> 00:10:11,600
They were really in the lead
and simple n-gram variations,

241
00:10:11,600 --> 00:10:14,690
traditional models with
hand-built features,

242
00:10:14,690 --> 00:10:16,700
they were the best models.

243
00:10:16,700 --> 00:10:19,760
There was a kind of faith
early on in the deep learning

244
00:10:19,760 --> 00:10:22,160
revolution that eventually,
those models would prove

245
00:10:22,160 --> 00:10:23,870
to be the best at this task.

246
00:10:23,870 --> 00:10:26,630
But at the time, we just
didn't have the data sets

247
00:10:26,630 --> 00:10:27,860
that would support--

248
00:10:27,860 --> 00:10:31,190
that would provide evidence
for that kind of claim.

249
00:10:31,190 --> 00:10:33,470
And so as a result, for a
while, deep learning systems

250
00:10:33,470 --> 00:10:36,350
really lagged behind more
traditional approaches.

251
00:10:36,350 --> 00:10:38,857
And I would say that it
was really in about 2017

252
00:10:38,857 --> 00:10:40,190
that deep learning pulled ahead.

253
00:10:40,190 --> 00:10:42,740
And that's a result of
modeling innovations,

254
00:10:42,740 --> 00:10:45,890
and also the arrival of
some really large benchmark

255
00:10:45,890 --> 00:10:49,460
datasets that would allow
us to train systems that

256
00:10:49,460 --> 00:10:50,747
were effective for the task.

257
00:10:50,747 --> 00:10:53,330
And it's at that point you see
that deep learning kind of took

258
00:10:53,330 --> 00:10:56,570
over, and as a result, in
subsequent screencasts,

259
00:10:56,570 --> 00:10:59,600
we too will be focused on
deep learning architectures

260
00:10:59,600 --> 00:11:01,870
for the NLI problem.

261
00:11:01,870 --> 00:11:06,070


