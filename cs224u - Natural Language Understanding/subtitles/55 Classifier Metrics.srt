1
00:00:00,000 --> 00:00:04,400


2
00:00:04,400 --> 00:00:06,150
CHRISTOPHER POTTS:
Welcome back, everyone.

3
00:00:06,150 --> 00:00:08,470
This is part 2 in our series
on methods and metrics.

4
00:00:08,470 --> 00:00:11,050
We're going to be talking
about classifier metrics.

5
00:00:11,050 --> 00:00:13,480
I'm sort of assuming that the
metrics I'll be discussing

6
00:00:13,480 --> 00:00:15,670
are broadly familiar to us.

7
00:00:15,670 --> 00:00:18,550
I think that's a chance
for us to step back and be

8
00:00:18,550 --> 00:00:21,850
reflective about what values
these familiar metrics actually

9
00:00:21,850 --> 00:00:23,020
encode.

10
00:00:23,020 --> 00:00:25,330
Because that really is
the name of the game here.

11
00:00:25,330 --> 00:00:27,250
No matter what kind of
task you're working on

12
00:00:27,250 --> 00:00:28,708
or what the structure
of your model

13
00:00:28,708 --> 00:00:32,170
is like, it's just fundamentally
true that different evaluation

14
00:00:32,170 --> 00:00:35,410
metrics will encode different
values, different goals

15
00:00:35,410 --> 00:00:38,440
you have for your system, and
different kinds of hypotheses

16
00:00:38,440 --> 00:00:39,910
that you might be pursuing.

17
00:00:39,910 --> 00:00:43,060
You can hear in that, that
really fundamentally choosing

18
00:00:43,060 --> 00:00:46,450
a metric is a crucial aspect to
any kind of experimental work.

19
00:00:46,450 --> 00:00:49,300
It's a fundamental step
in how we operationalize

20
00:00:49,300 --> 00:00:54,640
hypotheses in terms of data, and
models, and model comparisons.

21
00:00:54,640 --> 00:00:57,550
As a result, you should
feel free, for whatever task

22
00:00:57,550 --> 00:01:01,090
you're working on, to motivate
new metrics or specific uses

23
00:01:01,090 --> 00:01:03,460
of existing metrics,
depending on what

24
00:01:03,460 --> 00:01:07,180
your actual goals for your
experiments actually are.

25
00:01:07,180 --> 00:01:09,130
Relatedly, for
established tasks,

26
00:01:09,130 --> 00:01:11,140
you'll probably
feel some pressure

27
00:01:11,140 --> 00:01:14,290
to use specific
well-established metrics.

28
00:01:14,290 --> 00:01:16,420
But you should always,
as a scientist,

29
00:01:16,420 --> 00:01:19,810
feel empowered to push back
if you feel that the accepted

30
00:01:19,810 --> 00:01:23,140
metrics are not reflective
of your hypothesis

31
00:01:23,140 --> 00:01:26,140
or are distorting our
notions of progress somehow.

32
00:01:26,140 --> 00:01:28,720
Because remember,
areas of research

33
00:01:28,720 --> 00:01:30,700
can stagnate due
to poor metrics.

34
00:01:30,700 --> 00:01:31,970
And so we have to be vigilant.

35
00:01:31,970 --> 00:01:33,700
We have to be on the
lookout for cases

36
00:01:33,700 --> 00:01:35,920
in which the metrics
we've accepted

37
00:01:35,920 --> 00:01:38,492
might be at odds with
the actual goals we have

38
00:01:38,492 --> 00:01:39,700
for the research we're doing.

39
00:01:39,700 --> 00:01:42,710


40
00:01:42,710 --> 00:01:45,020
Let's begin our discussion
of classifier metrics

41
00:01:45,020 --> 00:01:48,200
by talking about confusion
matrices, a pretty fundamental

42
00:01:48,200 --> 00:01:50,810
data structure for a
lot of the calculations

43
00:01:50,810 --> 00:01:52,280
that we'll perform.

44
00:01:52,280 --> 00:01:54,440
So by convention, for
my confusion matrices,

45
00:01:54,440 --> 00:01:58,130
I'll have the actual labels
going across the rows here.

46
00:01:58,130 --> 00:02:00,470
And across the columns,
I'll have the predictions

47
00:02:00,470 --> 00:02:02,280
from some classifier model.

48
00:02:02,280 --> 00:02:04,040
So you can see in
this confusion matrix,

49
00:02:04,040 --> 00:02:07,250
that there were 15 cases in
which the model predicted

50
00:02:07,250 --> 00:02:08,150
positive.

51
00:02:08,150 --> 00:02:10,310
And the actual
label was positive.

52
00:02:10,310 --> 00:02:13,370
Where there-- whereas there are
10 cases where the actual label

53
00:02:13,370 --> 00:02:15,170
was positive and
the model predicted

54
00:02:15,170 --> 00:02:19,400
negative, and so forth for the
other values in this table.

55
00:02:19,400 --> 00:02:20,655
I think that seems familiar.

56
00:02:20,655 --> 00:02:22,280
It's something we
can take for granted.

57
00:02:22,280 --> 00:02:25,550
But we should remember that
behind the scenes here,

58
00:02:25,550 --> 00:02:28,070
a threshold was imposed
in order to create

59
00:02:28,070 --> 00:02:30,050
these categorical predictions.

60
00:02:30,050 --> 00:02:33,320
By and large, classifier
models that we use today

61
00:02:33,320 --> 00:02:36,620
predict probability
distributions over the labels.

62
00:02:36,620 --> 00:02:39,650
And so in order to create an
actual categorical prediction,

63
00:02:39,650 --> 00:02:42,200
we decided, for
example, that the label

64
00:02:42,200 --> 00:02:45,500
with the maximum probability
would be the true one.

65
00:02:45,500 --> 00:02:47,810
And that was-- the result
of that decision was

66
00:02:47,810 --> 00:02:49,620
used to aggregate this table.

67
00:02:49,620 --> 00:02:52,040
But, of course, different
choices of that threshold

68
00:02:52,040 --> 00:02:54,000
might give very
different results.

69
00:02:54,000 --> 00:02:55,640
And there might be
contexts in which

70
00:02:55,640 --> 00:02:57,470
we want to explore
the full range

71
00:02:57,470 --> 00:02:59,360
of probabilistic predictions.

72
00:02:59,360 --> 00:03:03,050
That's something I'll return to
at the end of the screencast.

73
00:03:03,050 --> 00:03:03,980
Final note about this.

74
00:03:03,980 --> 00:03:06,620
It can be helpful in the
context of confusion matrices

75
00:03:06,620 --> 00:03:09,710
to add a column for what's
called support, which is simply

76
00:03:09,710 --> 00:03:12,380
the number of actual
true instances

77
00:03:12,380 --> 00:03:13,720
that fall into each class.

78
00:03:13,720 --> 00:03:16,340
So there are 125
positive instances

79
00:03:16,340 --> 00:03:20,090
in this corpus, 35 negative,
and over 1,000 that's

80
00:03:20,090 --> 00:03:21,320
all in the neutral category.

81
00:03:21,320 --> 00:03:24,770
And that's already illuminating
about how specific metrics

82
00:03:24,770 --> 00:03:28,650
might deal with that extremely
imbalanced vector of support

83
00:03:28,650 --> 00:03:29,150
values.

84
00:03:29,150 --> 00:03:32,000


85
00:03:32,000 --> 00:03:33,730
So let's start with
accuracy-- by far,

86
00:03:33,730 --> 00:03:36,970
the most famous and familiar
of all the classifier metrics.

87
00:03:36,970 --> 00:03:40,120
Accuracy is simply the number
of correct predictions divided

88
00:03:40,120 --> 00:03:42,310
by the total number of examples.

89
00:03:42,310 --> 00:03:44,200
In terms of our
confusion matrices,

90
00:03:44,200 --> 00:03:46,030
that is just the sum
of all the values

91
00:03:46,030 --> 00:03:49,240
along the diagonal divided
by the sum of all the values

92
00:03:49,240 --> 00:03:51,740
that are in this table.

93
00:03:51,740 --> 00:03:54,470
The bounds are 0 and 1, of
course, with 0 the worst

94
00:03:54,470 --> 00:03:56,150
and 1 the best.

95
00:03:56,150 --> 00:03:58,760
In terms of the value
encoded by accuracy,

96
00:03:58,760 --> 00:04:01,350
I would say it's an attempt
to answer the question,

97
00:04:01,350 --> 00:04:04,250
how often is the system correct?

98
00:04:04,250 --> 00:04:06,600
And that kind of feeds
into the weaknesses here.

99
00:04:06,600 --> 00:04:08,990
So the weaknesses are,
first, there's no per class

100
00:04:08,990 --> 00:04:10,490
notions of accuracy.

101
00:04:10,490 --> 00:04:11,510
Not directly.

102
00:04:11,510 --> 00:04:14,180
We just get a single
holistic number.

103
00:04:14,180 --> 00:04:16,670
And relatedly, there is
just a complete failure

104
00:04:16,670 --> 00:04:19,290
to control for class size.

105
00:04:19,290 --> 00:04:21,860
So you can see, for example,
in this confusion matrix,

106
00:04:21,860 --> 00:04:24,320
that performance on
the neutral class

107
00:04:24,320 --> 00:04:28,130
will completely dominate
the accuracy values.

108
00:04:28,130 --> 00:04:30,020
And it's to the
point, in this table,

109
00:04:30,020 --> 00:04:32,240
where no matter
how much progress

110
00:04:32,240 --> 00:04:34,400
we make on the positive
and negative classes

111
00:04:34,400 --> 00:04:37,040
because they are so much smaller
in terms of their support

112
00:04:37,040 --> 00:04:40,940
for neutral, that progress
is unlikely to be reflected

113
00:04:40,940 --> 00:04:42,800
in our accuracy values.

114
00:04:42,800 --> 00:04:44,970
And that's why if you
return to the value encoded,

115
00:04:44,970 --> 00:04:47,210
you can see that just at
a raw fundamental level,

116
00:04:47,210 --> 00:04:52,300
it is simply answering how
often is the system correct?

117
00:04:52,300 --> 00:04:55,830
Another thing to keep in mind
is that for many classifier

118
00:04:55,830 --> 00:04:58,860
models, the loss
for those models

119
00:04:58,860 --> 00:05:00,900
is what's called the
cross-entropy loss.

120
00:05:00,900 --> 00:05:05,670
It's also called the negative
log-loss in Scikit-learn.

121
00:05:05,670 --> 00:05:10,680
And that value is inversely
proportional to accuracy.

122
00:05:10,680 --> 00:05:12,870
The takeaway there
is that even as we

123
00:05:12,870 --> 00:05:15,960
might choose other metrics to
compare models and evaluate

124
00:05:15,960 --> 00:05:19,770
models, we should keep in mind
that our classifiers themselves

125
00:05:19,770 --> 00:05:23,460
are kind of engines for
trying to maximize accuracy.

126
00:05:23,460 --> 00:05:26,790
And so they are likely to
inherit whatever properties,

127
00:05:26,790 --> 00:05:28,890
and values, and
strengths, and weaknesses

128
00:05:28,890 --> 00:05:31,710
are inherent in the
accuracy calculation, which

129
00:05:31,710 --> 00:05:34,840
as we'll see, could be at
odds with our actual values

130
00:05:34,840 --> 00:05:38,480
for the system that
we're developing.

131
00:05:38,480 --> 00:05:41,440
And that kind of feeds nicely
into precision, recall,

132
00:05:41,440 --> 00:05:44,590
and F scores which are
attempts to make up

133
00:05:44,590 --> 00:05:47,380
for some of the weaknesses
that you see in accuracy.

134
00:05:47,380 --> 00:05:48,730
We'll start with precision.

135
00:05:48,730 --> 00:05:50,680
This is a per class notion.

136
00:05:50,680 --> 00:05:52,960
For a class k, it's
the correct predictions

137
00:05:52,960 --> 00:05:56,320
for k divided by the sum
of all the guesses for k

138
00:05:56,320 --> 00:05:58,900
that were made by your model.

139
00:05:58,900 --> 00:06:00,630
So in terms of this
confusion matrix,

140
00:06:00,630 --> 00:06:02,650
if we focus on the
positive class here,

141
00:06:02,650 --> 00:06:04,950
the numerator is the number
of correct predictions

142
00:06:04,950 --> 00:06:08,370
for that class divided by
the sum of all the values

143
00:06:08,370 --> 00:06:09,900
that are in this column.

144
00:06:09,900 --> 00:06:12,300
And for the negative class,
we would repeat that.

145
00:06:12,300 --> 00:06:15,370
The numerator would be 15, and
we would sum over the column.

146
00:06:15,370 --> 00:06:17,790
And finally, for neutral,
the numerator would be 1,000.

147
00:06:17,790 --> 00:06:19,890
And we would again
sum over this column.

148
00:06:19,890 --> 00:06:22,560
And that leads to this
vector of precision values

149
00:06:22,560 --> 00:06:26,110
that you see along
the bottom here.

150
00:06:26,110 --> 00:06:29,050
The bounds of precision
are 0 and 1 approximately.

151
00:06:29,050 --> 00:06:30,550
With 0 the worst and 1 the best.

152
00:06:30,550 --> 00:06:32,920
There is an important
caveat here though.

153
00:06:32,920 --> 00:06:36,370
Precision is technically
undefined in situations

154
00:06:36,370 --> 00:06:38,510
where a model makes no
predictions about a given

155
00:06:38,510 --> 00:06:39,010
class.

156
00:06:39,010 --> 00:06:42,160
Because in that situation,
you're dividing by 0.

157
00:06:42,160 --> 00:06:44,110
And that's
technically undefined.

158
00:06:44,110 --> 00:06:46,803
It is common practice
to map those to 0,

159
00:06:46,803 --> 00:06:48,220
but we should keep
in mind that we

160
00:06:48,220 --> 00:06:50,440
are making that extra decision.

161
00:06:50,440 --> 00:06:53,590
The value encoded is a
kind of conservative one.

162
00:06:53,590 --> 00:06:56,020
We're going to penalize
incorrect guesses

163
00:06:56,020 --> 00:06:58,510
for a certain class.

164
00:06:58,510 --> 00:07:01,750
So you can imagine that a
failure mode there is to just

165
00:07:01,750 --> 00:07:03,340
rarely guess a certain class.

166
00:07:03,340 --> 00:07:04,750
That is the core weakness.

167
00:07:04,750 --> 00:07:07,890
You can achieve high
precision for a class k

168
00:07:07,890 --> 00:07:10,510
simply by rarely guessing k.

169
00:07:10,510 --> 00:07:12,010
So we'll obviously
need to offset

170
00:07:12,010 --> 00:07:13,690
that with some other pressure.

171
00:07:13,690 --> 00:07:17,480
And by and large, the
offset pressure is recall.

172
00:07:17,480 --> 00:07:19,450
Recall is, again,
a per class notion.

173
00:07:19,450 --> 00:07:22,540
For a class k, it's going to be
the correct predictions for k

174
00:07:22,540 --> 00:07:26,110
divided by the sum of all
the true members of k.

175
00:07:26,110 --> 00:07:28,090
So now we're going
to operate row--

176
00:07:28,090 --> 00:07:29,170
rowwise.

177
00:07:29,170 --> 00:07:30,880
We focus on the positive class.

178
00:07:30,880 --> 00:07:33,670
Our numerator is 15, the
number of true predictions

179
00:07:33,670 --> 00:07:37,000
for the positive class, divided
by the sum of all the values

180
00:07:37,000 --> 00:07:38,200
along the rows.

181
00:07:38,200 --> 00:07:41,410
That is all the true members
of that class for positive.

182
00:07:41,410 --> 00:07:43,990
That gives us a
recall value of 0.12.

183
00:07:43,990 --> 00:07:47,280
And we can repeat that
for the other two rows.

184
00:07:47,280 --> 00:07:48,710
The bounds of 0 and 1.

185
00:07:48,710 --> 00:07:50,910
0 the worst and 1 the best.

186
00:07:50,910 --> 00:07:53,250
The value encoded
is a permissive one.

187
00:07:53,250 --> 00:07:56,820
We want to penalize
missed true cases.

188
00:07:56,820 --> 00:07:59,340
We would like to make a lot
of predictions about a class

189
00:07:59,340 --> 00:08:02,400
in order to avoid leaving
any out, so to speak.

190
00:08:02,400 --> 00:08:04,290
And that leads into
the core weakness.

191
00:08:04,290 --> 00:08:07,840
We can achieve higher
recall for a class k

192
00:08:07,840 --> 00:08:09,840
simply by always guessing k.

193
00:08:09,840 --> 00:08:11,190
Never mind the mistakes.

194
00:08:11,190 --> 00:08:14,280
As long as we get all the actual
cases into our predictions,

195
00:08:14,280 --> 00:08:15,893
we're doing well by recall.

196
00:08:15,893 --> 00:08:17,310
And you can hear
in that that it's

197
00:08:17,310 --> 00:08:20,490
important to offset this
pressure by something else.

198
00:08:20,490 --> 00:08:23,500
And that's standard precision.

199
00:08:23,500 --> 00:08:25,450
And the way we offset
these two pressures

200
00:08:25,450 --> 00:08:27,550
is typically with F scores.

201
00:08:27,550 --> 00:08:31,240
So F scores are a harmonic mean
of the precision and recall

202
00:08:31,240 --> 00:08:31,760
scores.

203
00:08:31,760 --> 00:08:33,580
It's again a per class notion.

204
00:08:33,580 --> 00:08:35,620
And it has this
weighting value, beta.

205
00:08:35,620 --> 00:08:38,500
If we want to evenly balance
precision and recall,

206
00:08:38,500 --> 00:08:41,400
then we set beta to 1.

207
00:08:41,400 --> 00:08:43,350
So here's that
confusion matrix again.

208
00:08:43,350 --> 00:08:45,690
And along this column here,
I've given the per class

209
00:08:45,690 --> 00:08:48,320
F1 values here.

210
00:08:48,320 --> 00:08:51,200
The bounds are 0 and
1 as before with 0

211
00:08:51,200 --> 00:08:52,700
the worst and 1 the best.

212
00:08:52,700 --> 00:08:55,580
And you can count on the fact
that the F1 score for a class

213
00:08:55,580 --> 00:08:58,580
will always fall between the
precision and recall classes,

214
00:08:58,580 --> 00:09:00,320
because it's a,
kind of, an average.

215
00:09:00,320 --> 00:09:02,920
It's the harmonic mean.

216
00:09:02,920 --> 00:09:04,270
What's the value encoded?

217
00:09:04,270 --> 00:09:06,830
The best way I can say this
is that we're essentially

218
00:09:06,830 --> 00:09:08,830
trying to answer the
question, for a given class

219
00:09:08,830 --> 00:09:11,830
k, how much do
predictions for k align

220
00:09:11,830 --> 00:09:13,780
with a true instance of k?

221
00:09:13,780 --> 00:09:16,780
That is aligning with
both precision and recall

222
00:09:16,780 --> 00:09:17,930
as pressures.

223
00:09:17,930 --> 00:09:19,480
And then we can
use the beta value

224
00:09:19,480 --> 00:09:21,280
to control how much
weight we place

225
00:09:21,280 --> 00:09:24,220
on precision versus recall.

226
00:09:24,220 --> 00:09:25,850
What are the
weaknesses of F scores?

227
00:09:25,850 --> 00:09:27,580
Well, I can really think of two.

228
00:09:27,580 --> 00:09:29,560
The first is that
there's no normalization

229
00:09:29,560 --> 00:09:31,510
for the size of
the dataset because

230
00:09:31,510 --> 00:09:33,850
of the way we use the
denominators for the row

231
00:09:33,850 --> 00:09:35,680
and column sums.

232
00:09:35,680 --> 00:09:39,160
And relatedly, for a given class
that we decide to focus on,

233
00:09:39,160 --> 00:09:42,160
we actually ignore most of
the data that's in the table.

234
00:09:42,160 --> 00:09:44,230
Consider the fact
that if we decided

235
00:09:44,230 --> 00:09:46,780
to calculate the F1 score
for the positive class,

236
00:09:46,780 --> 00:09:49,220
we pay attention to
these column values

237
00:09:49,220 --> 00:09:51,130
and these are row
values, but we completely

238
00:09:51,130 --> 00:09:53,080
ignore these four values here.

239
00:09:53,080 --> 00:09:55,670
They're just not involved
in the calculation at all.

240
00:09:55,670 --> 00:09:59,680
And as a result, the
positive class F1 score

241
00:09:59,680 --> 00:10:02,770
might give a distorted picture
of what the model's predictions

242
00:10:02,770 --> 00:10:04,780
are actually like in
virtue of the fact

243
00:10:04,780 --> 00:10:10,570
that they leave out so much of
the data here as you can see.

244
00:10:10,570 --> 00:10:14,032
Now because F scores
are a per class notion,

245
00:10:14,032 --> 00:10:15,490
I think, that's
useful in the sense

246
00:10:15,490 --> 00:10:17,865
that it gives us a perspective
on each one of the classes

247
00:10:17,865 --> 00:10:18,640
separately.

248
00:10:18,640 --> 00:10:20,620
But for many kinds
of model evaluations,

249
00:10:20,620 --> 00:10:22,570
we need a summary
number, a single number

250
00:10:22,570 --> 00:10:25,030
that we can use to
compare models and assess

251
00:10:25,030 --> 00:10:26,437
overall progress.

252
00:10:26,437 --> 00:10:28,270
So we're going to do
some kind of averaging.

253
00:10:28,270 --> 00:10:29,950
And I'd like to
offer you three ways

254
00:10:29,950 --> 00:10:32,050
that we might average
these F scores.

255
00:10:32,050 --> 00:10:35,450
Macro-averaging, weighted
averaging, and micro-averaging.

256
00:10:35,450 --> 00:10:37,750
And as you'll see, these
encode quite different

257
00:10:37,750 --> 00:10:42,510
values about how we want to
think about the F scores.

258
00:10:42,510 --> 00:10:44,760
Macro-averaging is a
averaging that we've

259
00:10:44,760 --> 00:10:47,730
done at various points
throughout the quarter.

260
00:10:47,730 --> 00:10:51,210
It is simply the
arithmetic mean of all the

261
00:10:51,210 --> 00:10:53,200
per category F1 scores.

262
00:10:53,200 --> 00:10:56,850
So it's just the mean of the
values along this column.

263
00:10:56,850 --> 00:11:00,930
Its bounds are 0 and 1 with
0 the worst and 1 the best.

264
00:11:00,930 --> 00:11:02,280
What value does encode?

265
00:11:02,280 --> 00:11:05,550
Well, it's the same values
that we get from F scores,

266
00:11:05,550 --> 00:11:08,100
plus the additional and
non-trivial assumption

267
00:11:08,100 --> 00:11:10,170
that all of the
classes are equal

268
00:11:10,170 --> 00:11:13,890
regardless of size differences
between them, right?

269
00:11:13,890 --> 00:11:16,050
And that kind of feeds
into the weaknesses here.

270
00:11:16,050 --> 00:11:19,470
A classifier that does well
only on the small classes

271
00:11:19,470 --> 00:11:22,020
might not actually do
well in the real world.

272
00:11:22,020 --> 00:11:25,440
If you imagine counterfactually
that for our given model here,

273
00:11:25,440 --> 00:11:27,660
we had really
outstanding F1 scores

274
00:11:27,660 --> 00:11:30,960
for positive and negative
and really low for neutral.

275
00:11:30,960 --> 00:11:33,780
That might really be at odds
with how this classifier would

276
00:11:33,780 --> 00:11:36,390
behave in the world,
assuming that most

277
00:11:36,390 --> 00:11:38,370
of the examples that
are streaming in

278
00:11:38,370 --> 00:11:40,980
are in the neutral category.

279
00:11:40,980 --> 00:11:44,970
Relatedly, a classifier that
does well only on large classes

280
00:11:44,970 --> 00:11:48,210
might do poorly on the
small, but nonetheless vital

281
00:11:48,210 --> 00:11:50,170
classes that are in our data.

282
00:11:50,170 --> 00:11:53,430
And that just reflects the fact
that very often in NLP, it's

283
00:11:53,430 --> 00:11:56,250
the small classes that are the
most precious, the ones that we

284
00:11:56,250 --> 00:11:57,570
care about the most.

285
00:11:57,570 --> 00:12:01,020
And we're not reflecting that
kind of asymmetry in our values

286
00:12:01,020 --> 00:12:05,570
by simply taking the average
of all these F scores.

287
00:12:05,570 --> 00:12:07,520
Weighted average
F scores will give

288
00:12:07,520 --> 00:12:10,550
a very different perspective
on model performance.

289
00:12:10,550 --> 00:12:12,230
In this case, we
are again just going

290
00:12:12,230 --> 00:12:14,060
to take an average
of the F1 scores,

291
00:12:14,060 --> 00:12:16,190
but now weighted by
the amount of support

292
00:12:16,190 --> 00:12:18,500
for each one of the classes.

293
00:12:18,500 --> 00:12:21,140
That, again, has bound
0 to 1 with 0 the worst

294
00:12:21,140 --> 00:12:22,415
and 1 the best.

295
00:12:22,415 --> 00:12:25,010
The value encoded is
the same as the values

296
00:12:25,010 --> 00:12:27,980
that we get for the F scores,
but now with the added

297
00:12:27,980 --> 00:12:31,010
assumption that the size of the
class as the amount of support

298
00:12:31,010 --> 00:12:32,645
really does matter.

299
00:12:32,645 --> 00:12:34,520
And that's going to feed
into the weaknesses.

300
00:12:34,520 --> 00:12:37,460
And the fundamental thing
here is that large classes

301
00:12:37,460 --> 00:12:38,420
will dominate.

302
00:12:38,420 --> 00:12:41,335
Just as with accuracy,
the larger our classes,

303
00:12:41,335 --> 00:12:42,710
the more it's
going to contribute

304
00:12:42,710 --> 00:12:44,510
to our overall summary number.

305
00:12:44,510 --> 00:12:47,930
And that can lead to the kind
of problematic situation,

306
00:12:47,930 --> 00:12:51,530
where the small classes are just
not relevant for the evaluation

307
00:12:51,530 --> 00:12:52,430
metric.

308
00:12:52,430 --> 00:12:54,470
That could reflect
your values because.

309
00:12:54,470 --> 00:12:56,540
If what you really
care about is raw rate

310
00:12:56,540 --> 00:12:58,400
of correct
predictions, you might

311
00:12:58,400 --> 00:13:01,070
want to weight the larger
classes more heavily.

312
00:13:01,070 --> 00:13:03,130
But again, for many
contexts in NLP

313
00:13:03,130 --> 00:13:05,030
we really care about
how much progress

314
00:13:05,030 --> 00:13:08,780
we can make on the small, but
nonetheless important classes.

315
00:13:08,780 --> 00:13:11,030
And so in those contexts,
weighted averaging

316
00:13:11,030 --> 00:13:14,567
is probably not
the right choice.

317
00:13:14,567 --> 00:13:16,900
The final averaging scheme
that I would like to consider

318
00:13:16,900 --> 00:13:19,010
is micro-average F scores.

319
00:13:19,010 --> 00:13:22,660
This will be very similar to
weighted averaging of F1 scores

320
00:13:22,660 --> 00:13:25,840
and is directly
connected to accuracy.

321
00:13:25,840 --> 00:13:27,700
The way this works is
a little bit involved.

322
00:13:27,700 --> 00:13:30,790
We start with this
core confusion matrix.

323
00:13:30,790 --> 00:13:34,630
And we're going to break it down
into three smaller confusion

324
00:13:34,630 --> 00:13:36,652
matrices, one per class.

325
00:13:36,652 --> 00:13:38,110
So you can see this
one on the left

326
00:13:38,110 --> 00:13:40,120
here is for the positive class.

327
00:13:40,120 --> 00:13:44,200
The yes'es are 15 and the no's
are the sum of these two values

328
00:13:44,200 --> 00:13:46,120
here along this row.

329
00:13:46,120 --> 00:13:49,390
The no's are the 20, which is
the sum of these two values.

330
00:13:49,390 --> 00:13:51,250
And then there's
no no categories,

331
00:13:51,250 --> 00:13:54,400
for all the remaining data
in this quadrant here.

332
00:13:54,400 --> 00:13:57,520
We repeat that same procedure
for the negative class

333
00:13:57,520 --> 00:13:59,320
and for the neutral class.

334
00:13:59,320 --> 00:14:01,660
And then we simply sum
up those three smaller

335
00:14:01,660 --> 00:14:05,920
tables into one big
yes-no confusion matrix

336
00:14:05,920 --> 00:14:09,490
and calculate the F1
scores per category.

337
00:14:09,490 --> 00:14:10,930
That gives us two scores here.

338
00:14:10,930 --> 00:14:14,130
One for yes and one for no.

339
00:14:14,130 --> 00:14:15,810
The bounds on this are 0 and 1.

340
00:14:15,810 --> 00:14:18,210
With 0 the worst and 1 the best.

341
00:14:18,210 --> 00:14:21,690
The value encoded is
really easy to state.

342
00:14:21,690 --> 00:14:24,870
Macro-averaged F1 scores
for the yes category

343
00:14:24,870 --> 00:14:27,555
are equivalent to accuracy
scores numerically.

344
00:14:27,555 --> 00:14:30,420
So this is identical in
terms of that metric.

345
00:14:30,420 --> 00:14:32,910
And we have this additional
problem now that--

346
00:14:32,910 --> 00:14:35,550
well, we have the same
kind of value reflected

347
00:14:35,550 --> 00:14:38,482
as we have for the weighted
F scores or for accuracy,

348
00:14:38,482 --> 00:14:40,440
but now we have brought
in an additional source

349
00:14:40,440 --> 00:14:43,320
of uncertainty, which is we
have a number for the yes

350
00:14:43,320 --> 00:14:45,360
category and the no category.

351
00:14:45,360 --> 00:14:47,890
And hence, no single
summary number.

352
00:14:47,890 --> 00:14:49,890
The convention in
the literature is

353
00:14:49,890 --> 00:14:53,190
to focus on the yes category,
but that simply brings us back

354
00:14:53,190 --> 00:14:56,200
to accuracy with a more
involved calculation.

355
00:14:56,200 --> 00:14:57,940
So that's obviously
not very productive.

356
00:14:57,940 --> 00:14:59,550
And I would say
as a result here,

357
00:14:59,550 --> 00:15:01,680
the two real choices
that you want to make

358
00:15:01,680 --> 00:15:05,190
are between macro-averaging
and weighted averaging

359
00:15:05,190 --> 00:15:06,570
of your F1 scores.

360
00:15:06,570 --> 00:15:08,550
And again, that will
come down to what

361
00:15:08,550 --> 00:15:10,710
your fundamental
values are and what

362
00:15:10,710 --> 00:15:13,920
hypotheses you're pursuing.

363
00:15:13,920 --> 00:15:16,260
The final point I want to
make is that thus far, we

364
00:15:16,260 --> 00:15:19,170
have operated in terms of
the confusion matrix, which

365
00:15:19,170 --> 00:15:22,710
involved imposing a threshold
on probabilistic predictions

366
00:15:22,710 --> 00:15:26,040
in order to create categorical
values that we could then

367
00:15:26,040 --> 00:15:28,800
compare with precision,
and recall, and so forth.

368
00:15:28,800 --> 00:15:32,430
Precision and recall curves
offer a fundamentally different

369
00:15:32,430 --> 00:15:33,570
perspective.

370
00:15:33,570 --> 00:15:36,630
In this case, instead of
imposing one threshold,

371
00:15:36,630 --> 00:15:39,360
we'll take every
possible value that's

372
00:15:39,360 --> 00:15:43,020
predicted by our classifier
to be a potential threshold,

373
00:15:43,020 --> 00:15:45,600
and essentially create a
bunch of confusion matrices

374
00:15:45,600 --> 00:15:49,530
based on that successive
series of thresholds.

375
00:15:49,530 --> 00:15:52,350
And then we can plot the
trade-offs between precision

376
00:15:52,350 --> 00:15:55,080
here along the y-axis
and recall along

377
00:15:55,080 --> 00:15:58,380
the x-axis for all those
different notions of threshold.

378
00:15:58,380 --> 00:16:00,570
And that can be really
illuminating in terms

379
00:16:00,570 --> 00:16:02,925
of helping us see how our
system trades precision

380
00:16:02,925 --> 00:16:05,010
and recall against each other.

381
00:16:05,010 --> 00:16:07,710
And help us find based
on values that we

382
00:16:07,710 --> 00:16:10,080
have about our
problem and our goals,

383
00:16:10,080 --> 00:16:12,810
what the optimal balance
between precision and recall

384
00:16:12,810 --> 00:16:14,253
actually is.

385
00:16:14,253 --> 00:16:15,670
And then if you
do need to summary

386
00:16:15,670 --> 00:16:18,700
a number of this entire
table, average precision,

387
00:16:18,700 --> 00:16:20,610
which is implemented
in Scikit-learn

388
00:16:20,610 --> 00:16:23,850
is a standard way of
summarizing the entire curve

389
00:16:23,850 --> 00:16:26,820
with a single number
without, though, imposing

390
00:16:26,820 --> 00:16:30,120
that single threshold
that was so much shaping

391
00:16:30,120 --> 00:16:33,740
all of the previous
metrics that we discussed.

392
00:16:33,740 --> 00:16:38,000


