1
00:00:05,839 --> 00:00:08,160
привет всем, добро пожаловать в третью

2
00:00:08,160 --> 00:00:09,679
часть серии,

3
00:00:09,679 --> 00:00:11,840
скринкаст будет первым из двух

4
00:00:11,840 --> 00:00:14,320
или трех, посвященных нейронной сети,

5
00:00:14,320 --> 00:00:16,880
и в ней мы будем изучать входы,

6
00:00:16,880 --> 00:00:19,439
выходы, обучение и вывод в

7
00:00:19,439 --> 00:00:22,800
контексте нейронной сети.

8
00:00:24,640 --> 00:00:26,640
Давайте быстро начнем с напоминания о

9
00:00:26,640 --> 00:00:29,760
нашем  настройка из предыдущего скринкаста в

10
00:00:29,760 --> 00:00:32,479
автономном режиме, нам дается большой корпус

11
00:00:32,479 --> 00:00:34,079
текстовых документов, которые

12
00:00:34,079 --> 00:00:36,239
мы предварительно обработаем и проиндексируем этот

13
00:00:36,239 --> 00:00:39,600
корпус для быстрого поиска в

14
00:00:39,600 --> 00:00:42,000
Интернете, нам дан запрос, на который мы хотим

15
00:00:42,000 --> 00:00:43,600
ответить,

16
00:00:43,600 --> 00:00:45,920
нашим результатом будет список k

17
00:00:45,920 --> 00:00:50,160
наиболее релевантных  документы для этого запроса

18
00:00:51,920 --> 00:00:54,000
в классическом скринкасте ir мы

19
00:00:54,000 --> 00:00:57,360
обсуждаем bm25 как строгую модель поиска сопоставления терминов,

20
00:00:57,360 --> 00:00:59,440


21
00:00:59,440 --> 00:01:02,640
поэтому если мы просто используем bm25,

22
00:01:02,640 --> 00:01:05,680
краткий ответ таков: мы могли бы,

23
00:01:05,680 --> 00:01:07,600
но если наш интерес заключается в получении самого

24
00:01:07,600 --> 00:01:10,000
высокого качества, которое мы можем,

25
00:01:10,000 --> 00:01:11,680
тогда мы, вероятно, должны использовать  нейронный

26
00:01:11,680 --> 00:01:12,960
ir,

27
00:01:12,960 --> 00:01:15,920
как мы увидим, url ir

28
00:01:15,920 --> 00:01:18,479
широко использует нашу работу nlu творчески и

29
00:01:18,479 --> 00:01:21,200
интересно.

30
00:01:21,520 --> 00:01:23,840
Длинный ответ на вопрос, следует ли нам

31
00:01:23,840 --> 00:01:26,560
использовать bm25, заключается в том, что это зависит,

32
00:01:26,560 --> 00:01:28,799
среди прочего, от других факторов.  это зависит от нашего

33
00:01:28,799 --> 00:01:31,200
бюджета

34
00:01:31,439 --> 00:01:33,600
каждая модель ir представляет собой различный

35
00:01:33,600 --> 00:01:37,119
компромисс между эффективностью и эффективностью

36
00:01:37,119 --> 00:01:38,960
во многих случаях мы заинтересованы в

37
00:01:38,960 --> 00:01:41,200
максимизации эффективности максимизации

38
00:01:41,200 --> 00:01:43,280
качества до тех пор, пока эффективность

39
00:01:43,280 --> 00:01:45,759
приемлема,

40
00:01:45,759 --> 00:01:49,119
давайте начнем исследовать это на коллекции ms

41
00:01:49,119 --> 00:01:50,960
marco, которую мы представили

42
00:01:50,960 --> 00:01:54,399
в предыдущем скринкасте

43
00:01:54,399 --> 00:01:56,960
здесь мы  мы будем измерять эффективность,

44
00:01:56,960 --> 00:01:59,360
используя средний обратный ранг в автомобиле

45
00:01:59,360 --> 00:02:00,240
10,

46
00:02:00,240 --> 00:02:02,000
и мы будем измерять эффективность и, в

47
00:02:02,000 --> 00:02:03,759
частности, задержку,

48
00:02:03,759 --> 00:02:07,079
используя миллисекунды. На

49
00:02:08,000 --> 00:02:11,280
этом рисунке здесь показано извлечение bm25

50
00:02:11,280 --> 00:02:14,160
с использованием популярного набора инструментов под названием ancerini

51
00:02:14,160 --> 00:02:17,360
как одна точка данных в широком диапазоне

52
00:02:17,360 --> 00:02:22,599
значений mrr и задержки  Возможности

53
00:02:24,480 --> 00:02:26,800
просто в качестве напоминания, чем меньше задержка, тем

54
00:02:26,800 --> 00:02:27,680
лучше,

55
00:02:27,680 --> 00:02:30,080
и задержка здесь показана в

56
00:02:30,080 --> 00:02:33,120
логарифмической шкале,

57
00:02:33,200 --> 00:02:34,959
и чем

58
00:02:34,959 --> 00:02:37,920
выше

59
00:02:37,920 --> 00:02:42,080
mrr, тем лучше качество модели,

60
00:02:42,319 --> 00:02:44,560
так что еще может существовать в этом большом

61
00:02:44,560 --> 00:02:47,120
пустом пространстве, на данный момент

62
00:02:47,120 --> 00:02:48,480
мы  мы увидим, как

63
00:02:48,480 --> 00:02:50,640
это пространство заполнится множеством различных

64
00:02:50,640 --> 00:02:53,120
нейрофизиологических моделей в течение следующих нескольких

65
00:02:53,120 --> 00:02:55,280
скринов.  sts,

66
00:02:55,280 --> 00:02:57,680
и центральный вопрос время от времени

67
00:02:57,680 --> 00:03:00,159
, как правило, будет заключаться в том, как мы можем улучшить нашу

68
00:03:00,159 --> 00:03:02,959
mrr10 или любую другую метрику эффективности, с которой

69
00:03:02,959 --> 00:03:04,800
мы решим работать,

70
00:03:04,800 --> 00:03:06,720
возможно, за счет увеличения

71
00:03:06,720 --> 00:03:09,599
задержки,

72
00:03:11,280 --> 00:03:12,239
хорошо,

73
00:03:12,239 --> 00:03:14,319
так что давайте на самом деле посмотрим, как

74
00:03:14,319 --> 00:03:16,800
нейронные ir-модели будут работать

75
00:03:16,800 --> 00:03:18,959
конкретно.  в их поведении на входе и выходе

76
00:03:18,959 --> 00:03:20,959


77
00:03:20,959 --> 00:03:22,480
для целей этого короткого

78
00:03:22,480 --> 00:03:24,879
скринкаста мы будем рассматривать нейронный ранкер

79
00:03:24,879 --> 00:03:27,280
как черный ящик

80
00:03:27,280 --> 00:03:29,200
мы рассмотрим различные реализации

81
00:03:29,200 --> 00:03:31,599
для этой функции черного ящика в следующем

82
00:03:31,599 --> 00:03:34,239
скринкасте

83
00:03:34,560 --> 00:03:38,159
мы снабдим этот нейронный капитал черного ящика

84
00:03:38,159 --> 00:03:40,239
и документ

85
00:03:40,239 --> 00:03:41,920
и модель сделает свое дело и

86
00:03:41,920 --> 00:03:44,720
вернет нам единую оценку,

87
00:03:44,720 --> 00:03:46,959
которая оценивает релевантность этого

88
00:03:46,959 --> 00:03:50,400
запроса этому документу

89
00:03:50,400 --> 00:03:52,480
для того же запроса, мы повторим этот

90
00:03:52,480 --> 00:03:54,159
процесс для каждого документа, который мы хотим

91
00:03:54,159 --> 00:03:55,840
оценить,

92
00:03:55,840 --> 00:03:57,760
и, наконец, мы отсортируем все эти

93
00:03:57,760 --> 00:04:00,959
документы.  за счет уменьшения показателя релевантности

94
00:04:00,959 --> 00:04:02,959
, который даст нам лучший список k

95
00:04:02,959 --> 00:04:05,439
результатов на

96
00:04:06,640 --> 00:04:09,680
данный момент, это звучит достаточно просто,

97
00:04:09,680 --> 00:04:11,599
но как мы должны обучить эту нейронную

98
00:04:11,599 --> 00:04:14,560
модель для r

99
00:04:14,560 --> 00:04:16,478
это может быть не очень очевидно, но один

100
00:04:16,478 --> 00:04:18,478
довольно эффективный выбор — это просто

101
00:04:18,478 --> 00:04:20,639
двусторонняя классификация,

102
00:04:20,639 --> 00:04:22,800
попарная классификация,

103
00:04:22,800 --> 00:04:25,040
здесь каждый обучающий пример будет

104
00:04:25,040 --> 00:04:26,400
тройкой, в

105
00:04:26,400 --> 00:04:28,800
частности, каждый обучающий экземпляр будет

106
00:04:28,800 --> 00:04:30,400
содержать

107
00:04:30,400 --> 00:04:32,960
запрос релевантного или положительного документа и

108
00:04:32,960 --> 00:04:36,800
нерелевантного документа или отрицательного

109
00:04:36,800 --> 00:04:39,040
в  прямой проход во время обучения

110
00:04:39,040 --> 00:04:40,400
мы будем передавать модели запрос и

111
00:04:40,400 --> 00:04:42,639
положительный документ,

112
00:04:42,639 --> 00:04:44,560
и отдельно мы будем передавать запрос

113
00:04:44,560 --> 00:04:46,240
и отрицательный документ нейронному

114
00:04:46,240 --> 00:04:48,639
ранкеру,

115
00:04:49,040 --> 00:04:50,960
и мы будем оптимизировать всю всю

116
00:04:50,960 --> 00:04:53,120
нейронную сеть от начала до конца с градиентным

117
00:04:53,120 --> 00:04:54,080
спуском,

118
00:04:54,080 --> 00:04:57,199
используя простой  потеря классификации

119
00:04:57,199 --> 00:04:59,360
в данном случае перекрестная энтропийная потеря с

120
00:04:59,360 --> 00:05:02,000
мягким

121
00:05:02,080 --> 00:05:03,919
максимумом цель здесь состоит в том, чтобы максимизировать

122
00:05:03,919 --> 00:05:05,680
оценку положительного документа

123
00:05:05,680 --> 00:05:07,360
и

124
00:05:07,360 --> 00:05:09,039
минимизировать оценку, присвоенную отрицательному

125
00:05:09,039 --> 00:05:11,840
документу,

126
00:05:11,919 --> 00:05:14,000
вспомнить, что мы можем получить положительные результаты для

127
00:05:14,000 --> 00:05:15,199
каждого запроса

128
00:05:15,199 --> 00:05:17,280
из наших оценок релевантности, и что

129
00:05:17,280 --> 00:05:19,199
и  что каждый документ, который не был

130
00:05:19,199 --> 00:05:21,759
помечен как положительный, часто можно рассматривать

131
00:05:21,759 --> 00:05:23,919
как неявно отрицательный, поэтому мы могли  используйте

132
00:05:23,919 --> 00:05:24,800
это

133
00:05:24,800 --> 00:05:27,360
при создании троек для

134
00:05:27,360 --> 00:05:29,680
обучения двусторонней классификации для нашего

135
00:05:29,680 --> 00:05:32,400
нейронного ранжировщика, как

136
00:05:33,680 --> 00:05:35,680
только наш нейронный ранг ограничен

137
00:05:35,680 --> 00:05:37,680
выводом или фактическим проведением

138
00:05:37,680 --> 00:05:40,000
ранжирования, очень легко,

139
00:05:40,000 --> 00:05:41,919
учитывая запрос, мы просто выбираем каждый

140
00:05:41,919 --> 00:05:44,320
документ, передаем запрос и документ

141
00:05:44,320 --> 00:05:45,919
через нейронный  сеть

142
00:05:45,919 --> 00:05:47,759
получает оценку, а затем мы отсортируем все

143
00:05:47,759 --> 00:05:49,680
документы по оценке,

144
00:05:49,680 --> 00:05:51,120
это даст нам список k лучших

145
00:05:51,120 --> 00:05:52,400
документов,

146
00:05:52,400 --> 00:05:56,080
однако есть только небольшая,

147
00:05:56,080 --> 00:05:58,400
но очень серьезная проблема,

148
00:05:58,400 --> 00:06:00,319
коллекции часто содержат многие миллионы, если

149
00:06:00,319 --> 00:06:02,160
не миллиарды документов,

150
00:06:02,160 --> 00:06:04,560
даже если наш  модель настолько быстра, что

151
00:06:04,560 --> 00:06:06,319
обрабатывает каждый документ за одну

152
00:06:06,319 --> 00:06:09,440
микросекунду, одну миллионную секунды,

153
00:06:09,440 --> 00:06:11,440
вам все равно потребуется девять секунд на

154
00:06:11,440 --> 00:06:13,039
запрос

155
00:06:13,039 --> 00:06:15,600
для набора данных, такого как ms marco, с девятью

156
00:06:15,600 --> 00:06:18,319
миллионами проходов, что слишком медленно

157
00:06:18,319 --> 00:06:21,840
для большинства практических приложений,

158
00:06:22,319 --> 00:06:24,400
чтобы справиться с этим в  На практике нейронные

159
00:06:24,400 --> 00:06:27,120
модели ir часто используются в качестве

160
00:06:27,120 --> 00:06:29,440
моделей повторного ранжирования, которые пересчитывают только k лучших

161
00:06:29,440 --> 00:06:31,919
документов, полученных другой моделью,

162
00:06:31,919 --> 00:06:34,960
чтобы улучшить окончательный рейтинг.

163
00:06:34,960 --> 00:06:37,199
Если наиболее распространенным дизайном пайплайна

164
00:06:37,199 --> 00:06:39,520
является повторное ранжирование тысячи лучших документов,

165
00:06:39,520 --> 00:06:42,160
полученных bm25,

166
00:06:42,160 --> 00:06:44,400
это может быть здорово, потому что это

167
00:06:44,400 --> 00:06:45,759
сокращает работу

168
00:06:45,759 --> 00:06:47,280
для коллекции с 10 миллионами

169
00:06:47,280 --> 00:06:49,520
проходов в десять тысяч раз,

170
00:06:49,520 --> 00:06:50,960
потому что нам нужно только ранжировать тысячу

171
00:06:50,960 --> 00:06:53,440
документов.  с нейронной моделью,

172
00:06:53,440 --> 00:06:55,440
но он также вводит искусственный

173
00:06:55,440 --> 00:06:57,759
потолок отзыва, он искусственно ограничивает ваш

174
00:06:57,759 --> 00:07:00,479
запрос, поскольку теперь все

175
00:07:00,479 --> 00:07:03,199
соответствующие документы, которые

176
00:07:03,199 --> 00:07:06,000
bm25 не может получить наш ранжировщик первого этапа, не

177
00:07:06,000 --> 00:07:08,400
могут быть переоценены

178
00:07:08,400 --> 00:07:12,479
нашим новым блестящим ранжировщиком ir,

179
00:07:14,240 --> 00:07:15,039
поэтому

180
00:07:15,039 --> 00:07:16,560
может  мы делаем

181
00:07:16,560 --> 00:07:18,400
лучше получается, что ответ - да,

182
00:07:18,400 --> 00:07:20,560
мы обсудим понятие сквозного

183
00:07:20,560 --> 00:07:23,360
поиска позже, когда наша нейронная модель

184
00:07:23,360 --> 00:07:24,960
сможет быстро проводить

185
00:07:24,960 --> 00:07:26,960
поиск по всей

186
00:07:26,960 --> 00:07:29,440
коллекции без какого-либо конвейера ранжирования,

187
00:07:29,440 --> 00:07:30,400
но сначала

188
00:07:30,400 --> 00:07:31,840
мы

189
00:07:31,840 --> 00:07:33,919
в следующем

190
00:07:33,919 --> 00:07:36,919
скринкасте мы подробно обсудим ряд нейронных переранжировщиков.

