1
00:00:04,230 --> 00:00:07,120
The hard assumption here is that you’ve watched part 3,

2
00:00:07,120 --> 00:00:10,230
giving an intuitive walkthrough of the backpropagation algorithm.

3
00:00:11,040 --> 00:00:14,770
Here, we get a bit more formal and dive into the relevant calculus.

4
00:00:14,770 --> 00:00:17,040
It’s normal for this to be a little confusing,

5
00:00:17,040 --> 00:00:21,480
so the mantra to regularly pause and ponder certainly applies as much here as anywhere else.

6
00:00:21,920 --> 00:00:25,180
Our main goal is to show how people in machine learning

7
00:00:25,180 --> 00:00:29,440
commonly think about the chain rule from the calculus in the context of networks,

8
00:00:29,440 --> 00:00:33,820
which has a different feel for how much most introductory calculus courses approach the subject.

9
00:00:34,500 --> 00:00:36,890
For those of you uncomfortable with the relevant calculus,

10
00:00:36,890 --> 00:00:39,040
I do have a whole series on the topic.

11
00:00:40,340 --> 00:00:43,150
Let’s just start off with an extremely simple network,

12
00:00:43,150 --> 00:00:45,730
one where each layer has a single neuron in it.

13
00:00:46,270 --> 00:00:50,680
So this particular network is determined by 3 weights and 3 biases,

14
00:00:50,680 --> 00:00:55,070
and our goal is to understand how sensitive the cost function is to these variables.

15
00:00:55,550 --> 00:00:57,830
That way we know which adjustments to these terms

16
00:00:57,830 --> 00:01:00,940
is going to cause the most efficient decrease to the cost function.

17
00:01:01,920 --> 00:01:05,170
And we're just focus on the connection between the last two neurons.

18
00:01:05,880 --> 00:01:11,370
Let's label the activation of that last neuron a with a superscript L, indicating which layer it’s in,

19
00:01:11,690 --> 00:01:15,720
so the activation of this previous neuron is a^(L-1).

20
00:01:16,430 --> 00:01:20,030
There are not exponents, they're just a way of indexing what we’re talking about,

21
00:01:20,030 --> 00:01:22,970
since I want to save subscripts for different indices later on.

22
00:01:23,740 --> 00:01:29,710
Let’s say that the value we want this last activation to be for a given training example is y.

23
00:01:30,170 --> 00:01:32,360
For example, y might be 0 or 1.

24
00:01:32,940 --> 00:01:39,470
So the cost of this simple network for a single training example is (a^(L) - y)^2.

25
00:01:40,250 --> 00:01:44,650
We’ll denote the cost of this one training example as C_0.

26
00:01:46,030 --> 00:01:51,520
As a reminder, this last activation is determined by a weight, which I'm going to call w^(L)

27
00:01:51,980 --> 00:01:54,220
times the previous neuron’s activation,

28
00:01:54,530 --> 00:01:56,940
plus some bias, which I’ll call b^(L),

29
00:01:57,480 --> 00:01:59,900
then you pump that through some special nonlinear function

30
00:01:59,900 --> 00:02:01,520
like a sigmoid or a ReLU.

31
00:02:01,850 --> 00:02:06,980
It's actually going to make things easier for us if we give a special name to this weighted sum, like z,

32
00:02:06,980 --> 00:02:09,550
with the same superscript as the relevant activations.

33
00:02:10,389 --> 00:02:11,480
So there are a lot of terms.

34
00:02:11,480 --> 00:02:16,960
And a way you might conceptualize this is that the weight, the previous activation, and the bias

35
00:02:16,960 --> 00:02:21,400
altogether are used to compute z, which in turn lets us compute a,

36
00:02:21,740 --> 00:02:25,610
which finally, along with the constant y, let us compute the cost.

37
00:02:27,260 --> 00:02:31,660
And of course, a^(L-1) is influenced by its own weight and bias, and such.

38
00:02:32,810 --> 00:02:34,840
But we are not gonna focus on that right now.

39
00:02:35,680 --> 00:02:38,040
All of these are just numbers, right?

40
00:02:38,040 --> 00:02:41,230
And it can be nice to think of each one as having its own little number line.

41
00:02:41,900 --> 00:02:43,990
Our first goal is to understand

42
00:02:43,990 --> 00:02:48,940
how sensitive the cost function is to small changes in our weight w^(L).

43
00:02:49,640 --> 00:02:54,880
Or phrased differently, what’s the derivative of C with respect to w^(L).

44
00:02:55,630 --> 00:02:58,070
When you see this “∂w” term,

45
00:02:58,070 --> 00:03:02,750
think of it as meaning “some tiny nudge to w”, like a change by 0.01.

46
00:03:03,150 --> 00:03:08,210
And think of this “∂C” term as meaning “whatever the resulting nudge to the cost is”.

47
00:03:08,710 --> 00:03:10,420
What we want is their ratio.

48
00:03:11,210 --> 00:03:16,520
Conceptually, this tiny nudge to w^(L) causes some nudge to z^(L)

49
00:03:16,520 --> 00:03:21,380
which in turn causes some change to a^(L), which directly influences the cost.

50
00:03:23,100 --> 00:03:28,930
So we break this up by first looking at the ratio of a tiny change to z^(L) to the tiny change in w^(L).

51
00:03:29,290 --> 00:03:33,030
That is, the derivative of z^(L) with respect to w^(L).

52
00:03:33,760 --> 00:03:39,410
Likewise, you then consider the ratio of a change to a^(L) to the tiny change in z^(L) that caused it,

53
00:03:39,850 --> 00:03:44,880
as well as the ratio between the final nudge to C and this intermediate nudge to a^(L).

54
00:03:45,670 --> 00:03:47,850
This right here is the chain rule,

55
00:03:47,850 --> 00:03:54,950
where multiplying together these three ratios gives us the sensitivity of C to small changes in w^(L).

56
00:03:57,190 --> 00:04:00,040
So on screen right now, there’s kinda lot of symbols,

57
00:04:00,040 --> 00:04:03,000
so take a moment to make sure it’s clear what they all are,

58
00:04:03,600 --> 00:04:06,560
because now we are gonna compute the relevant derivatives.

59
00:04:07,400 --> 00:04:13,230
The derivative of C with respect to a^(L) works out to be 2(a^(L) - y).

60
00:04:13,960 --> 00:04:16,880
Notice, this means that its size is proportional to

61
00:04:16,880 --> 00:04:20,880
the difference between the network’s output, and the thing we want it to be.

62
00:04:21,360 --> 00:04:23,340
So if that output was very different,

63
00:04:23,340 --> 00:04:27,150
even slight changes stand to have a big impact on the cost function.

64
00:04:28,300 --> 00:04:33,880
The derivative of a^(L) with respect to z^(L) is just the derivative of our sigmoid function,

65
00:04:33,880 --> 00:04:36,370
or whatever nonlinearity you choose to use.

66
00:04:37,310 --> 00:04:40,370
And the derivative of z^(L) with respect to w^(L),

67
00:04:41,470 --> 00:04:44,530
in this case comes out just to be a^(L-1).

68
00:04:46,070 --> 00:04:49,570
Now I don't know about you, but I think it’s easy to get stuck head-down in these formulas

69
00:04:49,570 --> 00:04:53,690
without taking a moment to sit back and remind yourself what they all actually mean.

70
00:04:54,120 --> 00:04:56,040
In the case of this last derivative,

71
00:04:56,040 --> 00:05:00,060
the amount that a small nudge to this weight influences the last layer

72
00:05:00,060 --> 00:05:02,850
depends on how strong the previous neuron is.

73
00:05:03,310 --> 00:05:07,520
Remember, this is where that “neurons that fire together wire together” idea comes in.

74
00:05:09,210 --> 00:05:15,940
And all of this is the derivative with respect to w^(L) only of the cost for a specific training example.

75
00:05:16,410 --> 00:05:22,150
Since the full cost function involves averaging together all those costs across many training examples,

76
00:05:22,150 --> 00:05:27,610
its derivative requires averaging this expression that we found over all training examples.

77
00:05:28,430 --> 00:05:31,930
And of course that is just one component of the gradient vector,

78
00:05:31,930 --> 00:05:33,890
which itself is built up from

79
00:05:33,890 --> 00:05:38,480
the partial derivatives of the cost function with respect to all those weights and biases.

80
00:05:40,710 --> 00:05:43,550
But even though it was just one of those partial derivatives we need,

81
00:05:43,550 --> 00:05:45,390
it's more than 50% of the work.

82
00:05:46,420 --> 00:05:49,940
The sensitivity to the bias, for example, is almost identical.

83
00:05:50,250 --> 00:05:55,120
We just need to change out this ∂z/∂w term for a ∂z/∂b,

84
00:05:58,760 --> 00:06:02,590
And if you look at the relevant formula, that derivative comes to be 1.

85
00:06:06,210 --> 00:06:09,880
Also, and this is where the idea of propagating backwards comes in,

86
00:06:10,230 --> 00:06:15,670
you can see how sensitive this cost function is to the activation of the previous layer;

87
00:06:16,250 --> 00:06:19,650
namely, this initial derivative in the chain rule expansion,

88
00:06:19,650 --> 00:06:23,100
the sensitivity of z to the previous activation,

89
00:06:23,480 --> 00:06:25,670
comes out to be the weight w^(L).

90
00:06:26,580 --> 00:06:31,500
And again, even though we won’t be able to directly influence that activation,

91
00:06:31,500 --> 00:06:33,080
it’s helpful to keep track of,

92
00:06:33,080 --> 00:06:38,200
because now we can just keep iterating this chain rule idea backwards

93
00:06:38,200 --> 00:06:42,750
to see how sensitive the cost function is to previous weights and to previous biases.

94
00:06:43,630 --> 00:06:45,980
And you might think this is an overly simple example,

95
00:06:45,980 --> 00:06:47,880
since all layers just have 1 neuron,

96
00:06:47,880 --> 00:06:51,220
and things are just gonna get exponentially more complicated in the real network.

97
00:06:51,680 --> 00:06:56,270
But honestly, not that much changes when we give the layers multiple neurons.

98
00:06:56,270 --> 00:06:58,710
Really it's just a few more indices to keep track of.

99
00:06:59,340 --> 00:07:02,880
Rather than the activation of a given layer simply being a^(L),

100
00:07:02,880 --> 00:07:07,210
it's also going to have a subscript indicating which neuron of that layer it is.

101
00:07:07,780 --> 00:07:14,470
Let’s go ahead and use the letter k to index the layer (L-1), and j to index the layer (L).

102
00:07:15,290 --> 00:07:18,910
For the the cost, again we look at what the desired output is.

103
00:07:18,910 --> 00:07:19,380
But this time

104
00:07:19,380 --> 00:07:25,260
we add up the squares of the differences between these last layer activations and the desired output.

105
00:07:26,060 --> 00:07:31,070
That is, you take a sum over (a_j^(L) - y_j)^2

106
00:07:33,110 --> 00:07:34,520
Since there are a lot more weights,

107
00:07:34,520 --> 00:07:37,650
each one has to have a couple more indices to keep track of where it is.

108
00:07:38,010 --> 00:07:44,990
So let’s call the weight of the edge connecting this k-th neuron to the j-th neuron w_{jk}^(L).

109
00:07:45,660 --> 00:07:48,260
Those indices might feel a little backwards at first,

110
00:07:48,260 --> 00:07:52,940
but it lines up with how you’d index the weight matrix that I talked about in the Part 1 video.

111
00:07:53,680 --> 00:07:58,350
Just as before, it’s still nice to give a name to the relevant weighted sum, like z,

112
00:07:58,350 --> 00:08:04,310
so that the activation of the last layer is just your special function, like the sigmoid, applied to z.

113
00:08:05,040 --> 00:08:06,230
You can kinda see what I mean, right?

114
00:08:06,230 --> 00:08:11,680
These are all essentially the same equations we had before in the one-neuron-per-layer case;

115
00:08:11,680 --> 00:08:13,870
it just looks a little more complicated.

116
00:08:15,370 --> 00:08:18,220
And indeed, the chain-rule derivative expression

117
00:08:18,220 --> 00:08:21,980
describing how sensitive the cost is to a specific weight

118
00:08:21,980 --> 00:08:23,890
looks essentially the same.

119
00:08:23,890 --> 00:08:26,880
I’ll leave it to you to pause and think about each of these terms if you want.

120
00:08:29,310 --> 00:08:31,320
What does change here, though,

121
00:08:31,320 --> 00:08:36,830
is the derivative of the cost with respect to one of the activations in the layer (L-1).

122
00:08:37,760 --> 00:08:43,120
In this case, the difference is the neuron influences the cost function through multiple paths.

123
00:08:44,660 --> 00:08:50,540
That is, on the one hand, it influences a_0^(L), which plays a role in the cost function,

124
00:08:51,010 --> 00:08:56,320
but it also has an influence on a_1^(L), which also plays a role in the cost function.

125
00:08:56,320 --> 00:08:57,410
And you have to add those up.

126
00:09:00,170 --> 00:09:02,980
And that... well that is pretty much it.

127
00:09:03,560 --> 00:09:08,520
Once you know how sensitive the cost function is to the activations in this second to last layer,

128
00:09:08,840 --> 00:09:12,940
you can just repeat the process for all the weights and biases feeding into that layer.

129
00:09:13,850 --> 00:09:15,360
So pat yourself on the back!

130
00:09:15,360 --> 00:09:16,950
If this all of these makes sense,

131
00:09:16,950 --> 00:09:20,440
you have now looked deep into the heart of backpropagation,

132
00:09:20,440 --> 00:09:22,830
the workhorse behind how neural networks learn.

133
00:09:23,590 --> 00:09:29,300
These chain rule expressions give you the derivatives that determine each component in the gradient

134
00:09:29,300 --> 00:09:33,550
that helps minimize the cost of the network by repeatedly stepping downhill.

135
00:09:34,280 --> 00:09:36,850
Hhhhpf. If you sit back and think about all that,

136
00:09:36,850 --> 00:09:40,090
that’s a lot of layers of complexity to wrap your mind around.

137
00:09:40,090 --> 00:09:43,090
So don't worry if it takes time for your mind to digest it all.

