1
00:00:04,019 --> 00:00:10,680
This is a three. It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels.

2
00:00:10,680 --> 00:00:15,660
But your brain has no trouble recognizing it as a three and I want you to take a moment to appreciate

3
00:00:15,900 --> 00:00:18,949
How crazy it is that brains can do this so effortlessly?

4
00:00:18,949 --> 00:00:23,160
I mean this this and this are also recognizable as threes,

5
00:00:23,160 --> 00:00:28,060
even though the specific values of each pixel is very different from one image to the next.

6
00:00:28,080 --> 00:00:33,780
The particular light-sensitive cells in your eye that are firing when you see this three

7
00:00:33,780 --> 00:00:36,800
are very different from the ones firing when you see this three.

8
00:00:37,140 --> 00:00:40,610
But something in that crazy smart visual cortex of yours

9
00:00:41,129 --> 00:00:48,139
resolves these as representing the same idea while at the same time recognizing other images as their own distinct ideas

10
00:00:48,840 --> 00:00:55,039
But if I told you hey sit down and write for me a program that takes in a grid of 28 by 28

11
00:00:55,379 --> 00:01:01,759
pixels like this and outputs a single number between 0 and 10 telling you what it thinks the digit is

12
00:01:02,250 --> 00:01:06,139
Well the task goes from comically trivial to dauntingly difficult

13
00:01:06,750 --> 00:01:08,270
Unless you've been living under a rock

14
00:01:08,270 --> 00:01:14,599
I think I hardly need to motivate the relevance and importance of machine learning and neural networks to the present into the future

15
00:01:14,640 --> 00:01:18,410
But what I want to do here is show you what a neural network actually is

16
00:01:18,660 --> 00:01:24,229
Assuming no background and to help visualize what it's doing not as a buzzword but as a piece of math

17
00:01:24,570 --> 00:01:28,310
My hope is just that you come away feeling like this structure itself is

18
00:01:28,380 --> 00:01:34,399
Motivated and to feel like you know what it means when you read or you hear about a neural network quote-unquote learning

19
00:01:34,950 --> 00:01:40,249
This video is just going to be devoted to the structure component of that and the following one is going to tackle learning

20
00:01:40,530 --> 00:01:45,950
What we're going to do is put together a neural network that can learn to recognize handwritten digits

21
00:01:49,270 --> 00:01:51,329
This is a somewhat classic example for

22
00:01:51,520 --> 00:01:56,759
Introducing the topic and I'm happy to stick with the status quo here because at the end of the two videos I want to point

23
00:01:56,760 --> 00:02:02,099
You to a couple good resources where you can learn more and where you can download the code that does this and play with it?

24
00:02:02,100 --> 00:02:04,100
on your own computer

25
00:02:04,750 --> 00:02:08,970
There are many many variants of neural networks and in recent years

26
00:02:08,970 --> 00:02:11,970
There's been sort of a boom in research towards these variants

27
00:02:12,130 --> 00:02:19,019
But in these two introductory videos you and I are just going to look at the simplest plain-vanilla form with no added frills

28
00:02:19,300 --> 00:02:21,040
This is kind of a necessary

29
00:02:21,040 --> 00:02:24,510
prerequisite for understanding any of the more powerful modern variants and

30
00:02:24,760 --> 00:02:28,199
Trust me it still has plenty of complexity for us to wrap our minds around

31
00:02:28,690 --> 00:02:32,820
But even in this simplest form it can learn to recognize handwritten digits

32
00:02:32,820 --> 00:02:36,180
Which is a pretty cool thing for a computer to be able to do.

33
00:02:37,120 --> 00:02:41,960
And at the same time you'll see how it does fall short of a couple hopes that we might have for it

34
00:02:43,090 --> 00:02:48,179
As the name suggests neural networks are inspired by the brain, but let's break that down

35
00:02:48,520 --> 00:02:51,389
What are the neurons and in what sense are they linked together?

36
00:02:52,090 --> 00:02:57,750
Right now when I say neuron all I want you to think about is a thing that holds a number

37
00:02:58,209 --> 00:03:02,129
Specifically a number between 0 & 1 it's really not more than that

38
00:03:03,430 --> 00:03:11,130
For example the network starts with a bunch of neurons corresponding to each of the 28 times 28 pixels of the input image

39
00:03:11,400 --> 00:03:12,460
which is

40
00:03:12,460 --> 00:03:20,240
784 neurons in total each one of these holds a number that represents the grayscale value of the corresponding pixel

41
00:03:20,769 --> 00:03:24,299
ranging from 0 for black pixels up to 1 for white pixels

42
00:03:24,910 --> 00:03:30,419
This number inside the neuron is called its activation and the image you might have in mind here

43
00:03:30,420 --> 00:03:33,959
Is that each neuron is lit up when its activation is a high number?

44
00:03:36,260 --> 00:03:41,559
So all of these 784 neurons make up the first layer of our network

45
00:03:45,990 --> 00:03:51,289
Now jumping over to the last layer this has ten neurons each representing one of the digits

46
00:03:51,570 --> 00:03:56,239
the activation in these neurons again some number that's between zero and one

47
00:03:56,880 --> 00:04:00,049
Represents how much the system thinks that a given image?

48
00:04:00,720 --> 00:04:05,990
Corresponds with a given digit. There's also a couple layers in between called the hidden layers

49
00:04:06,180 --> 00:04:07,770
Which for the time being?

50
00:04:07,770 --> 00:04:13,549
Should just be a giant question mark for how on earth this process of recognizing digits is going to be handled

51
00:04:13,740 --> 00:04:20,209
In this network I chose two hidden layers each one with 16 neurons and admittedly that's kind of an arbitrary choice

52
00:04:20,608 --> 00:04:24,889
to be honest I chose two layers based on how I want to motivate the structure in just a moment and

53
00:04:25,350 --> 00:04:29,179
16 well that was just a nice number to fit on the screen in practice

54
00:04:29,180 --> 00:04:32,209
There is a lot of room for experiment with a specific structure here

55
00:04:32,730 --> 00:04:38,329
The way the network operates activations in one layer determine the activations of the next layer

56
00:04:38,760 --> 00:04:45,349
And of course the heart of the network as an information processing mechanism comes down to exactly how those

57
00:04:45,570 --> 00:04:48,409
activations from one layer bring about activations in the next layer

58
00:04:48,900 --> 00:04:54,859
It's meant to be loosely analogous to how in biological networks of neurons some groups of neurons firing

59
00:04:55,410 --> 00:04:57,410
cause certain others to fire

60
00:04:57,570 --> 00:04:58,340
Now the network

61
00:04:58,340 --> 00:05:03,019
I'm showing here has already been trained to recognize digits and let me show you what I mean by that

62
00:05:03,140 --> 00:05:06,580
It means if you feed in an image lighting up all

63
00:05:06,640 --> 00:05:11,780
784 neurons of the input layer according to the brightness of each pixel in the image

64
00:05:12,330 --> 00:05:17,029
That pattern of activations causes some very specific pattern in the next layer

65
00:05:17,190 --> 00:05:19,309
Which causes some pattern in the one after it?

66
00:05:19,440 --> 00:05:22,190
Which finally gives some pattern in the output layer and?

67
00:05:22,350 --> 00:05:29,359
The brightest neuron of that output layer is the network's choice so to speak for what digit this image represents?

68
00:05:32,070 --> 00:05:36,859
And before jumping into the math for how one layer influences the next or how training works?

69
00:05:37,140 --> 00:05:43,069
Let's just talk about why it's even reasonable to expect a layered structure like this to behave intelligently

70
00:05:43,800 --> 00:05:48,260
What are we expecting here? What is the best hope for what those middle layers might be doing?

71
00:05:48,860 --> 00:05:56,720
Well when you or I recognize digits we piece together various components a nine has a loop up top and a line on the right

72
00:05:57,260 --> 00:06:01,280
an 8 also has a loop up top, but it's paired with another loop down low

73
00:06:02,020 --> 00:06:06,599
A 4 basically breaks down into three specific lines and things like that

74
00:06:07,180 --> 00:06:11,970
Now in a perfect world we might hope that each neuron in the second-to-last layer

75
00:06:12,640 --> 00:06:14,729
corresponds with one of these sub components

76
00:06:14,890 --> 00:06:19,740
That anytime you feed in an image with say a loop up top like a 9 or an 8

77
00:06:19,870 --> 00:06:21,220
There's some specific

78
00:06:21,220 --> 00:06:27,749
Neuron whose activation is going to be close to one and I don't mean this specific loop of pixels the hope would be that any

79
00:06:28,090 --> 00:06:35,039
Generally loopy pattern towards the top sets off this neuron that way going from the third layer to the last one

80
00:06:35,380 --> 00:06:39,960
just requires learning which combination of sub components corresponds to which digits

81
00:06:40,510 --> 00:06:42,810
Of course that just kicks the problem down the road

82
00:06:42,910 --> 00:06:49,019
Because how would you recognize these sub components or even learn what the right sub components should be and I still haven't even talked about

83
00:06:49,020 --> 00:06:52,829
How one layer influences the next but run with me on this one for a moment

84
00:06:53,650 --> 00:06:56,340
recognizing a loop can also break down into subproblems

85
00:06:56,860 --> 00:07:02,550
One reasonable way to do this would be to first recognize the various little edges that make it up

86
00:07:03,520 --> 00:07:08,910
Similarly a long line like the kind you might see in the digits 1 or 4 or 7

87
00:07:08,910 --> 00:07:14,279
Well that's really just a long edge or maybe you think of it as a certain pattern of several smaller edges

88
00:07:14,740 --> 00:07:19,379
So maybe our hope is that each neuron in the second layer of the network

89
00:07:20,290 --> 00:07:22,650
corresponds with the various relevant little edges

90
00:07:23,230 --> 00:07:28,259
Maybe when an image like this one comes in it lights up all of the neurons

91
00:07:28,720 --> 00:07:31,649
associated with around eight to ten specific little edges

92
00:07:31,930 --> 00:07:36,930
which in turn lights up the neurons associated with the upper loop and a long vertical line and

93
00:07:37,300 --> 00:07:39,599
Those light up the neuron associated with a nine

94
00:07:40,300 --> 00:07:41,100
whether or not

95
00:07:41,100 --> 00:07:47,070
This is what our final network actually does is another question, one that I'll come back to once we see how to train the network

96
00:07:47,350 --> 00:07:52,170
But this is a hope that we might have. A sort of goal with the layered structure like this

97
00:07:53,020 --> 00:07:59,340
Moreover you can imagine how being able to detect edges and patterns like this would be really useful for other image recognition tasks

98
00:07:59,740 --> 00:08:06,749
And even beyond image recognition there are all sorts of intelligent things you might want to do that break down into layers of abstraction

99
00:08:07,690 --> 00:08:14,670
Parsing speech for example involves taking raw audio and picking out distinct sounds which combine to make certain syllables

100
00:08:15,070 --> 00:08:19,829
Which combine to form words which combine to make up phrases and more abstract thoughts etc

101
00:08:20,770 --> 00:08:25,710
But getting back to how any of this actually works picture yourself right now designing

102
00:08:25,710 --> 00:08:30,449
How exactly the activations in one layer might determine the activations in the next?

103
00:08:30,670 --> 00:08:35,879
The goal is to have some mechanism that could conceivably combine pixels into edges

104
00:08:35,880 --> 00:08:41,430
Or edges into patterns or patterns into digits and to zoom in on one very specific example

105
00:08:41,950 --> 00:08:44,189
Let's say the hope is for one particular

106
00:08:44,380 --> 00:08:50,430
Neuron in the second layer to pick up on whether or not the image has an edge in this region here

107
00:08:50,950 --> 00:08:54,960
The question at hand is what parameters should the network have

108
00:08:55,270 --> 00:09:02,490
what dials and knobs should you be able to tweak so that it's expressive enough to potentially capture this pattern or

109
00:09:02,590 --> 00:09:07,290
Any other pixel pattern or the pattern that several edges can make a loop and other such things?

110
00:09:08,290 --> 00:09:15,389
Well, what we'll do is assign a weight to each one of the connections between our neuron and the neurons from the first layer

111
00:09:15,850 --> 00:09:17,850
These weights are just numbers

112
00:09:18,190 --> 00:09:25,590
then take all those activations from the first layer and compute their weighted sum according to these weights I

113
00:09:27,370 --> 00:09:31,680
Find it helpful to think of these weights as being organized into a little grid of their own

114
00:09:31,680 --> 00:09:37,079
And I'm going to use green pixels to indicate positive weights and red pixels to indicate negative weights

115
00:09:37,240 --> 00:09:41,670
Where the brightness of that pixel is some loose depiction of the weights value?

116
00:09:42,400 --> 00:09:45,840
Now if we made the weights associated with almost all of the pixels zero

117
00:09:46,150 --> 00:09:49,079
except for some positive weights in this region that we care about

118
00:09:49,480 --> 00:09:51,310
then taking the weighted sum of

119
00:09:51,310 --> 00:09:57,690
all the pixel values really just amounts to adding up the values of the pixel just in the region that we care about

120
00:09:58,870 --> 00:10:04,440
And, if you really want it to pick up on whether there's an edge here what you might do is have some negative weights

121
00:10:04,900 --> 00:10:06,900
associated with the surrounding pixels

122
00:10:07,030 --> 00:10:12,660
Then the sum is largest when those middle pixels are bright, but the surrounding pixels are darker

123
00:10:14,279 --> 00:10:18,169
When you compute a weighted sum like this you might come out with any number

124
00:10:18,240 --> 00:10:23,180
but for this network what we want is for activations to be some value between 0 & 1

125
00:10:23,730 --> 00:10:26,599
so a common thing to do is to pump this weighted sum

126
00:10:26,910 --> 00:10:32,000
Into some function that squishes the real number line into the range between 0 & 1 and

127
00:10:32,190 --> 00:10:37,249
A common function that does this is called the sigmoid function also known as a logistic curve

128
00:10:37,980 --> 00:10:43,339
basically very negative inputs end up close to zero very positive inputs end up close to 1

129
00:10:43,339 --> 00:10:46,398
and it just steadily increases around the input 0

130
00:10:49,080 --> 00:10:56,029
So the activation of the neuron here is basically a measure of how positive the relevant weighted sum is

131
00:10:57,450 --> 00:11:01,819
But maybe it's not that you want the neuron to light up when the weighted sum is bigger than 0

132
00:11:02,100 --> 00:11:06,260
Maybe you only want it to be active when the sum is bigger than say 10

133
00:11:06,630 --> 00:11:10,279
That is you want some bias for it to be inactive

134
00:11:10,860 --> 00:11:16,099
what we'll do then is just add in some other number like negative 10 to this weighted sum

135
00:11:16,529 --> 00:11:19,669
Before plugging it through the sigmoid squishification function

136
00:11:20,220 --> 00:11:22,730
That additional number is called the bias

137
00:11:23,310 --> 00:11:29,060
So the weights tell you what pixel pattern this neuron in the second layer is picking up on and the bias

138
00:11:29,220 --> 00:11:35,450
tells you how high the weighted sum needs to be before the neuron starts getting meaningfully active

139
00:11:35,910 --> 00:11:37,910
And that is just one neuron

140
00:11:38,120 --> 00:11:41,940
Every other neuron in this layer is going to be connected to all

141
00:11:42,320 --> 00:11:50,620
784 pixels neurons from the first layer and each one of those 784 connections has its own weight associated with it

142
00:11:51,330 --> 00:11:57,739
also each one has some bias some other number that you add on to the weighted sum before squishing it with the sigmoid and

143
00:11:58,020 --> 00:12:01,909
That's a lot to think about with this hidden layer of 16 neurons

144
00:12:02,010 --> 00:12:08,270
that's a total of 784 times 16 weights along with 16 biases

145
00:12:08,490 --> 00:12:14,029
And all of that is just the connections from the first layer to the second the connections between the other layers

146
00:12:14,029 --> 00:12:17,208
Also, have a bunch of weights and biases associated with them

147
00:12:17,760 --> 00:12:20,680
All said and done this network has almost exactly

148
00:12:21,280 --> 00:12:23,920
13,000 total weights and biases

149
00:12:24,280 --> 00:12:29,540
13,000 knobs and dials that can be tweaked and turned to make this network behave in different ways

150
00:12:30,520 --> 00:12:32,520
So when we talk about learning?

151
00:12:32,530 --> 00:12:40,199
What that's referring to is getting the computer to find a valid setting for all of these many many numbers so that it'll actually solve

152
00:12:40,200 --> 00:12:42,190
the problem at hand

153
00:12:42,190 --> 00:12:43,000
one thought

154
00:12:43,000 --> 00:12:49,979
Experiment that is at once fun and kind of horrifying is to imagine sitting down and setting all of these weights and biases by hand

155
00:12:50,380 --> 00:12:56,159
Purposefully tweaking the numbers so that the second layer picks up on edges the third layer picks up on patterns etc

156
00:12:56,350 --> 00:13:01,440
I personally find this satisfying rather than just reading the network as a total black box

157
00:13:01,870 --> 00:13:04,349
Because when the network doesn't perform the way you

158
00:13:04,600 --> 00:13:11,370
anticipate if you've built up a little bit of a relationship with what those weights and biases actually mean you have a starting place for

159
00:13:11,680 --> 00:13:16,289
Experimenting with how to change the structure to improve or when the network does work?

160
00:13:16,290 --> 00:13:18,290
But not for the reasons you might expect

161
00:13:18,310 --> 00:13:25,169
Digging into what the weights and biases are doing is a good way to challenge your assumptions and really expose the full space of possible

162
00:13:25,180 --> 00:13:26,350
solutions

163
00:13:26,350 --> 00:13:30,600
By the way the actual function here is a little cumbersome to write down. Don't you think?

164
00:13:32,350 --> 00:13:38,460
So let me show you a more notationally compact way that these connections are represented. This is how you'd see it

165
00:13:38,460 --> 00:13:40,460
If you choose to read up more about neural networks

166
00:13:41,110 --> 00:13:45,810
Organize all of the activations from one layer into a column as a vector

167
00:13:47,470 --> 00:13:52,320
Then organize all of the weights as a matrix where each row of that matrix

168
00:13:52,900 --> 00:13:57,659
corresponds to the connections between one layer and a particular neuron in the next layer

169
00:13:58,060 --> 00:14:03,599
What that means is that taking the weighted sum of the activations in the first layer according to these weights?

170
00:14:04,000 --> 00:14:09,330
Corresponds to one of the terms in the matrix vector product of everything we have on the left here

171
00:14:13,540 --> 00:14:18,380
By the way so much of machine learning just comes down to having a good grasp of linear algebra

172
00:14:18,380 --> 00:14:26,940
So for any of you who want a nice visual understanding for matrices and what matrix vector multiplication means take a look at the series I did on linear algebra

173
00:14:27,250 --> 00:14:28,839
especially chapter three

174
00:14:28,839 --> 00:14:35,759
Back to our expression instead of talking about adding the bias to each one of these values independently we represent it by

175
00:14:36,010 --> 00:14:42,209
Organizing all those biases into a vector and adding the entire vector to the previous matrix vector product

176
00:14:42,910 --> 00:14:44,040
Then as a final step

177
00:14:44,040 --> 00:14:47,250
I'll rap a sigmoid around the outside here

178
00:14:47,250 --> 00:14:51,899
And what that's supposed to represent is that you're going to apply the sigmoid function to each specific

179
00:14:52,420 --> 00:14:54,570
component of the resulting vector inside

180
00:14:55,510 --> 00:15:00,749
So once you write down this weight matrix and these vectors as their own symbols you can

181
00:15:01,000 --> 00:15:07,589
communicate the full transition of activations from one layer to the next in an extremely tight and neat little expression and

182
00:15:07,930 --> 00:15:15,000
This makes the relevant code both a lot simpler and a lot faster since many libraries optimize the heck out of matrix multiplication

183
00:15:17,560 --> 00:15:21,359
Remember how earlier I said these neurons are simply things that hold numbers

184
00:15:21,790 --> 00:15:26,250
Well of course the specific numbers that they hold depends on the image you feed in

185
00:15:27,790 --> 00:15:32,940
So it's actually more accurate to think of each neuron as a function one that takes in the

186
00:15:33,070 --> 00:15:38,070
outputs of all the neurons in the previous layer and spits out a number between zero and one

187
00:15:38,800 --> 00:15:42,270
Really the entire network is just a function one that takes in

188
00:15:42,760 --> 00:15:47,010
784 numbers as an input and spits out ten numbers as an output

189
00:15:47,470 --> 00:15:48,700
It's an absurdly

190
00:15:48,700 --> 00:15:56,249
Complicated function one that involves thirteen thousand parameters in the forms of these weights and biases that pick up on certain patterns and which involves

191
00:15:56,250 --> 00:16:00,270
iterating many matrix vector products and the sigmoid squish evocation function

192
00:16:00,610 --> 00:16:06,390
But it's just a function nonetheless and in a way it's kind of reassuring that it looks complicated

193
00:16:06,390 --> 00:16:12,239
I mean if it were any simpler what hope would we have that it could take on the challenge of recognizing digits?

194
00:16:12,960 --> 00:16:19,559
And how does it take on that challenge? How does this network learn the appropriate weights and biases just by looking at data? Oh?

195
00:16:20,080 --> 00:16:26,039
That's what I'll show in the next video, and I'll also dig a little more into what this particular network we are seeing is really doing

196
00:16:27,130 --> 00:16:32,640
Now is the point I suppose I should say subscribe to stay notified about when that video or any new videos come out

197
00:16:32,760 --> 00:16:37,560
But realistically most of you don't actually receive notifications from YouTube, do you ?

198
00:16:37,560 --> 00:16:42,260
Maybe more honestly I should say subscribe so that the neural networks that underlie YouTube's

199
00:16:42,459 --> 00:16:47,639
Recommendation algorithm are primed to believe that you want to see content from this channel get recommended to you

200
00:16:48,250 --> 00:16:50,250
anyway stay posted for more

201
00:16:50,410 --> 00:16:53,550
Thank you very much to everyone supporting these videos on patreon

202
00:16:53,589 --> 00:16:56,759
I've been a little slow to progress in the probability series this summer

203
00:16:56,760 --> 00:17:01,379
But I'm jumping back into it after this project so patrons you can look out for updates there

204
00:17:03,310 --> 00:17:05,550
To close things off here I have with me Lisha Li

205
00:17:05,550 --> 00:17:12,029
Lee who did her PhD work on the theoretical side of deep learning and who currently works at a venture capital firm called amplify partners

206
00:17:12,030 --> 00:17:16,530
Who kindly provided some of the funding for this video so Lisha one thing

207
00:17:16,530 --> 00:17:19,109
I think we should quickly bring up is this sigmoid function

208
00:17:19,180 --> 00:17:24,780
As I understand it early networks used this to squish the relevant weighted sum into that interval between zero and one

209
00:17:24,980 --> 00:17:30,340
You know kind of motivated by this biological analogy of neurons either being inactive or active
(Lisha) - Exactly

210
00:17:30,360 --> 00:17:36,320
(3B1B) - But relatively few modern networks actually use sigmoid anymore. That's kind of old school right ?
(Lisha) - Yeah or rather

211
00:17:36,370 --> 00:17:42,780
ReLU seems to be much easier to train
(3B1B) - And ReLU really stands for rectified linear unit

212
00:17:42,780 --> 00:17:48,839
(Lisha) - Yes it's this kind of function where you're just taking a max of 0 and a where a is given by

213
00:17:49,120 --> 00:17:53,670
what you were explaining in the video and what this was sort of motivated from I think was a

214
00:17:54,610 --> 00:17:56,610
partially by a biological

215
00:17:56,620 --> 00:17:58,179
Analogy with how

216
00:17:58,179 --> 00:18:03,089
Neurons would either be activated or not and so if it passes a certain threshold

217
00:18:03,250 --> 00:18:05,250
It would be the identity function

218
00:18:05,290 --> 00:18:10,439
But if it did not then it would just not be activated so be zero so it's kind of a simplification

219
00:18:10,720 --> 00:18:14,429
Using sigmoids didn't help training, or it was very difficult to train

220
00:18:14,429 --> 00:18:19,589
It's at some point and people just tried relu and it happened to work

221
00:18:20,110 --> 00:18:22,140
Very well for these incredibly

222
00:18:22,690 --> 00:18:25,090
Deep neural networks.
(3B1B) - All right

223
00:18:25,090 --> 00:18:26,060
Thank You Lisha

224
00:18:26,060 --> 00:18:33,429
for background amplify partners in early-stage VC invests in technical founders building the next generation of companies focused on the

225
00:18:33,590 --> 00:18:38,409
applications of AI if you or someone that you know has ever thought about starting a company someday

226
00:18:38,450 --> 00:18:43,179
Or if you're working on an early-stage one right now the Amplify folks would love to hear from you

227
00:18:43,240 --> 00:18:48,800
they even set up a specific email for this video 3blue1brown@amplifypartners.com

228
00:18:48,800 --> 00:18:50,780
so feel free to reach out to them through that

229
00:19:12,130 --> 00:19:14,190
 

