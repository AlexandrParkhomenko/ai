1
00:00:04,350 --> 00:00:06,410
Here we tackle backpropagation,

2
00:00:06,410 --> 00:00:09,400
the core algorithm behind how neural networks learn.

3
00:00:09,400 --> 00:00:11,210
After a quick recap for where we are,

4
00:00:11,210 --> 00:00:15,470
the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing

5
00:00:15,470 --> 00:00:17,270
without any reference to the formulas,

6
00:00:17,640 --> 00:00:20,310
Then for those of you who do want to dive into the math,

7
00:00:20,310 --> 00:00:23,140
the next video goes into the calculus underlying all this.

8
00:00:23,940 --> 00:00:25,550
If you watched the last two videos

9
00:00:25,550 --> 00:00:27,920
or if you're just jumping in with the appropriate background,

10
00:00:27,920 --> 00:00:31,290
you know what a neural network is and how it feeds forward information.

11
00:00:31,660 --> 00:00:35,100
Here we're doing the classic example of recognizing handwritten digits,

12
00:00:35,100 --> 00:00:39,930
whose pixel values get fed into the first layer of the network with 784 neurons.

13
00:00:39,930 --> 00:00:44,000
And I've been showing a network with two hidden layers having just 16 neurons each,

14
00:00:44,000 --> 00:00:49,250
and an output layer of 10 neurons, indicating which digit the network is choosing as its answer.

15
00:00:50,020 --> 00:00:54,340
I'm also expecting you to understand gradient descent as described in the last video,

16
00:00:54,340 --> 00:00:56,890
and how what we mean by learning is that

17
00:00:56,890 --> 00:01:01,450
we want to find which weights and biases minimize a certain cost function.

18
00:01:02,010 --> 00:01:05,470
As a quick reminder, for the cost of a single training example,

19
00:01:05,470 --> 00:01:08,400
what you do is take the output that the network gives,

20
00:01:08,400 --> 00:01:10,850
along with the output that you wanted it to give,

21
00:01:11,200 --> 00:01:14,820
and you just add up the squares of the differences between each component.

22
00:01:15,370 --> 00:01:20,020
Doing this for all of your tens of thousands of training examples, and averaging the results,

23
00:01:20,020 --> 00:01:22,410
this gives you the total cost of the network.

24
00:01:22,910 --> 00:01:26,010
And as if that's not enough to think about, as described in the last video,

25
00:01:26,010 --> 00:01:30,870
the thing that we're looking for is the negative gradient of this cost function,

26
00:01:30,870 --> 00:01:35,720
which tells you how you need to change all of the weights and biases, all of these connections,

27
00:01:35,720 --> 00:01:38,270
so as to most efficiently decrease the cost.

28
00:01:42,950 --> 00:01:45,210
Backpropagation, the topic of this video,

29
00:01:45,210 --> 00:01:48,800
is an algorithm for computing that crazy complicated gradient.

30
00:01:49,490 --> 00:01:54,010
And the one idea from the last video that I really want you to hold firmly in your mind right now

31
00:01:54,010 --> 00:01:58,910
is that because thinking of the gradient vector as a direction in 13000 dimensions is,

32
00:01:58,910 --> 00:02:02,090
to put it lightly, beyond the scope of our imaginations,

33
00:02:02,090 --> 00:02:03,510
there's another way you can think about it:

34
00:02:04,580 --> 00:02:07,710
The magnitude of each component here is telling you

35
00:02:07,710 --> 00:02:11,140
how sensitive the cost function is to each weight and bias.

36
00:02:11,810 --> 00:02:14,580
For example, let's say you go through the process I'm about to describe,

37
00:02:14,580 --> 00:02:16,370
and you compute the negative gradient,

38
00:02:16,370 --> 00:02:21,470
and the component associated with the weight on this edge here comes out to be 3.2,

39
00:02:21,870 --> 00:02:26,370
while the component associated with this edge here comes out as 0.1.

40
00:02:26,910 --> 00:02:28,420
The way you would interpret that is that

41
00:02:28,420 --> 00:02:33,080
the cost of the function is 32 times more sensitive to changes in that first weight.

42
00:02:33,640 --> 00:02:35,930
So if you were to wiggle that value just a little bit,

43
00:02:35,930 --> 00:02:38,190
it's gonna cause some change to the cost,

44
00:02:38,190 --> 00:02:43,200
and that change is 32 times greater than what the same wiggle to that second weight would give.

45
00:02:48,520 --> 00:02:51,440
Personally, when I was first learning about backpropagation,

46
00:02:51,440 --> 00:02:55,740
I think the most confusing aspect was just the notation and the index chasing of it all.

47
00:02:56,180 --> 00:02:59,450
But once you unwrap what each part of this algorithm is really doing,

48
00:02:59,450 --> 00:03:02,870
each individual effect that it's having is actually pretty intuitive.

49
00:03:03,180 --> 00:03:06,740
It's just that there's a lot of little adjustments getting layered on top of each other.

50
00:03:07,660 --> 00:03:11,290
So I'm gonna start things off here with a complete disregard for the notation,

51
00:03:11,290 --> 00:03:13,370
and just step through those effects that

52
00:03:13,370 --> 00:03:16,350
each training example is having on the weights and biases.

53
00:03:17,090 --> 00:03:18,590
Because the cost function involves

54
00:03:18,590 --> 00:03:23,640
averaging a certain cost per example over all the tens of thousands of training examples,

55
00:03:23,970 --> 00:03:28,640
the way that we adjust the weights and biases for a single gradient descent step

56
00:03:28,640 --> 00:03:31,140
also depends on every single example,

57
00:03:31,680 --> 00:03:33,200
or rather in principle it should,

58
00:03:33,200 --> 00:03:35,930
but for computational efficiency we're going to do a little trick later

59
00:03:35,930 --> 00:03:39,370
to keep you from needing to hit every single example for every single step.

60
00:03:39,790 --> 00:03:41,330
Another case right now,

61
00:03:41,330 --> 00:03:46,160
all we're gonna do is focus our attention on one single example: this image of a 2.

62
00:03:46,670 --> 00:03:51,650
What effect should this one training example have on how the weights and biases get adjusted?

63
00:03:52,680 --> 00:03:55,240
Let's say we're at a point where the network is not well trained yet,

64
00:03:55,240 --> 00:03:57,970
so the activations in the output are gonna look pretty random,

65
00:03:57,970 --> 00:04:02,040
maybe something like 0.5, 0.8, 0.2, on and on.

66
00:04:02,640 --> 00:04:07,450
Now we can't directly change those activations, we only have influence on the weights and biases,

67
00:04:07,790 --> 00:04:12,670
but it is helpful to keep track of which adjustments we wish should take place to that output layer,

68
00:04:13,270 --> 00:04:15,710
and since we want it to classify the image as a 2,

69
00:04:16,040 --> 00:04:21,360
we want that third value to get nudged up, while all of the others get nudged down.

70
00:04:22,040 --> 00:04:26,020
Moreover, the sizes of these nudges should be proportional to

71
00:04:26,020 --> 00:04:29,630
how far away each current value is from its target value.

72
00:04:30,220 --> 00:04:34,350
For example, the increase to that number 2 neurons activation is,

73
00:04:34,350 --> 00:04:38,490
in a sense, more important than the decrease to the number 8 neuron,

74
00:04:38,490 --> 00:04:40,630
which is already pretty close to where it should be.

75
00:04:41,990 --> 00:04:45,250
So zooming in further, let's focus just on this one neuron,

76
00:04:45,250 --> 00:04:47,530
the one whose activation we wish to increase.

77
00:04:48,160 --> 00:04:50,550
Remember, that activation is defined as

78
00:04:50,550 --> 00:04:56,430
a certain weighted sum of all of the activations in the previous layer, plus a bias,

79
00:04:56,430 --> 00:05:01,290
which has all been plugged into something like the sigmoid squishification function or a ReLU,

80
00:05:01,810 --> 00:05:07,360
So there are three different avenues that can team up together to help increase that activation:

81
00:05:07,680 --> 00:05:10,970
you can increase the bias, you can increase the weights,

82
00:05:10,970 --> 00:05:14,030
and you can change the activations from the previous layer.

83
00:05:14,950 --> 00:05:17,770
Focusing just on how the weights should be adjusted,

84
00:05:17,770 --> 00:05:21,410
notice how the weights actually have differing levels of influence:

85
00:05:21,410 --> 00:05:25,750
the connections with the brightest neurons from the preceding layer have the biggest effect,

86
00:05:25,750 --> 00:05:29,240
since those weights are multiplied by larger activation values.

87
00:05:31,330 --> 00:05:33,480
So if you were to increase one of those weights,

88
00:05:33,480 --> 00:05:37,370
it actually has a stronger influence on the ultimate cost function

89
00:05:37,370 --> 00:05:40,820
than increasing the weights of connections with dimmer neurons,

90
00:05:40,820 --> 00:05:43,650
at least as far as this one training example is concerned.

91
00:05:44,380 --> 00:05:46,890
Remember when we talked about gradient descent,

92
00:05:46,890 --> 00:05:50,620
we don't just care about whether each component should get nudged up or down,

93
00:05:50,620 --> 00:05:53,370
we care about which ones give you the most bang for your buck.

94
00:05:55,270 --> 00:05:59,310
This, by the way, is at least somewhat reminiscent of a theory in neuroscience

95
00:05:59,310 --> 00:06:01,870
for how biological networks of neurons learn

96
00:06:01,870 --> 00:06:06,820
Hebbian theory - often summed up in the phrase “neurons that fire together wire together”.

97
00:06:07,260 --> 00:06:12,200
Here, the biggest increases to weights, the biggest strengthening of connections,

98
00:06:12,200 --> 00:06:14,840
happens between neurons which are the most active,

99
00:06:14,840 --> 00:06:17,590
and the ones which we wish to become more active.

100
00:06:18,020 --> 00:06:21,060
In a sense, the neurons that are firing while seeing a 2,

101
00:06:21,060 --> 00:06:24,680
get more strongly linked to those firing when thinking about a 2.

102
00:06:25,420 --> 00:06:28,780
To be clear, I really am not in a position to make statements one way or another

103
00:06:28,780 --> 00:06:33,080
about whether artificial networks of neurons behave anything like biological brains,

104
00:06:33,080 --> 00:06:37,250
and this fires-together-wire-together idea comes with a couple meaningful asterisks.

105
00:06:37,250 --> 00:06:41,260
But taken as a very loose analogy, I do find it interesting to note.

106
00:06:41,890 --> 00:06:46,020
Anyway, the third way that we can help increase this neuron's activation

107
00:06:46,020 --> 00:06:49,060
is by changing all the activations in the previous layer,

108
00:06:49,560 --> 00:06:54,970
namely, if everything connected to that digit 2 neuron with a positive weight got brighter,

109
00:06:54,970 --> 00:06:57,960
and if everything connected with a negative weight got dimmer,

110
00:06:58,340 --> 00:07:00,890
then that digit 2 neuron would become more active.

111
00:07:02,450 --> 00:07:06,130
And similar to the weight changes, you're going to get the most bang for your buck

112
00:07:06,130 --> 00:07:10,550
by seeking changes that are proportional to the size of the corresponding weights.

113
00:07:12,120 --> 00:07:15,360
Now of course, we cannot directly influence those activations,

114
00:07:15,360 --> 00:07:17,780
we only have control over the weights and biases.

115
00:07:18,220 --> 00:07:23,610
But just as with the last layer, it's helpful to just keep a note of what those desired changes are.

116
00:07:24,450 --> 00:07:29,720
But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants.

117
00:07:29,720 --> 00:07:34,840
Remember, we also want all of the other neurons in the last layer to become less active,

118
00:07:34,840 --> 00:07:36,500
and each of those other output neurons

119
00:07:36,500 --> 00:07:39,840
has its own thoughts about what should happen to that second-to-last layer.

120
00:07:43,110 --> 00:07:46,140
So, the desire of this digit 2 neuron

121
00:07:46,140 --> 00:07:50,520
is added together with the desires of all the other output neurons

122
00:07:50,520 --> 00:07:53,240
for what should happen to this second-to-last layer.

123
00:07:53,580 --> 00:07:56,400
Again, in proportion to the corresponding weights,

124
00:07:56,400 --> 00:08:00,910
and in proportion to how much each of those neurons needs to change.

125
00:08:01,480 --> 00:08:05,510
This right here is where the idea of propagating backwards comes in.

126
00:08:05,960 --> 00:08:08,730
By adding together all these desired effects,

127
00:08:08,730 --> 00:08:13,560
you basically get a list of nudges that you want to happen to the second-to-last layer.

128
00:08:14,180 --> 00:08:15,390
And once you have those,

129
00:08:15,390 --> 00:08:17,850
you can recursively apply the same process

130
00:08:17,850 --> 00:08:21,180
to the relevant weights and biases that determine those values,

131
00:08:21,180 --> 00:08:25,140
repeating the same process I just walked through and moving backwards through the network.

132
00:08:29,030 --> 00:08:30,370
And zooming out a bit further,

133
00:08:30,370 --> 00:08:31,920
remember that this is all just

134
00:08:31,920 --> 00:08:37,400
how a single training example wishes to nudge each one of those weights and biases.

135
00:08:37,400 --> 00:08:39,700
If we only listen to what that 2 wanted,

136
00:08:39,700 --> 00:08:43,400
the network would ultimately be incentivized just to classify all images as a 2.

137
00:08:44,030 --> 00:08:49,420
So what you do is you go through this same backprop routine for every other training example,

138
00:08:49,420 --> 00:08:53,200
recording how each of them would like to change the weights and the biases,

139
00:08:53,650 --> 00:08:56,220
and you averaged together those desired changes.

140
00:09:02,050 --> 00:09:06,940
This collection here of the averaged nudges to each weight and bias is,

141
00:09:06,940 --> 00:09:11,910
loosely speaking, the negative gradient of the cost function referenced in the last video,

142
00:09:11,910 --> 00:09:13,740
or at least something proportional to it.

143
00:09:14,360 --> 00:09:19,570
I say “loosely speaking”, only because I have yet to get quantitatively precise about those nudges.

144
00:09:19,570 --> 00:09:22,190
But if you understood every change that I just referenced,

145
00:09:22,190 --> 00:09:24,770
why some are proportionally bigger than others,

146
00:09:24,770 --> 00:09:27,160
and how they all need to be added together,

147
00:09:27,160 --> 00:09:31,170
you understand the mechanics for what backpropagation is actually doing.

148
00:09:34,050 --> 00:09:37,400
By the way, in practice it takes computers an extremely long time

149
00:09:37,400 --> 00:09:42,490
to add up the influence of every single training example, every single gradient descent step.

150
00:09:43,010 --> 00:09:44,960
So here's what's commonly done instead:

151
00:09:45,440 --> 00:09:50,280
You randomly shuffle your training data, and then divide it into a whole bunch of mini-batches,

152
00:09:50,280 --> 00:09:52,680
let's say, each one having 100 training examples.

153
00:09:53,240 --> 00:09:56,430
Then you compute a step according to the mini-batch.

154
00:09:56,850 --> 00:09:59,390
It's not going to be the actual gradient of the cost function,

155
00:09:59,390 --> 00:10:02,630
which depends on all of the training data, not this tiny subset.

156
00:10:03,100 --> 00:10:05,640
So it's not the most efficient step downhill.

157
00:10:06,080 --> 00:10:08,970
But each mini batch does give you a pretty good approximation,

158
00:10:08,970 --> 00:10:12,250
and more importantly, it gives you a significant computational speed up.

159
00:10:12,820 --> 00:10:16,810
If you were to plot the trajectory of your network under the relevant cost surface,

160
00:10:16,810 --> 00:10:22,030
it would be a little more like a drunk man stumbling aimlessly down a hill, but taking quick steps;

161
00:10:22,030 --> 00:10:27,180
rather than a carefully calculating man determining the exact downhill direction of each step

162
00:10:27,180 --> 00:10:30,350
before taking a very slow and careful step in that direction.

163
00:10:31,460 --> 00:10:34,940
This technique is referred to as “stochastic gradient descent”.

164
00:10:36,000 --> 00:10:39,800
There's kind of a lot going on here, so let's just sum it up for ourselves, shall we?

165
00:10:40,240 --> 00:10:42,270
Backpropagation is the algorithm

166
00:10:42,270 --> 00:10:47,370
for determining how a single training example would like to nudge the weights and biases,

167
00:10:47,370 --> 00:10:49,930
not just in terms of whether they should go up or down,

168
00:10:49,930 --> 00:10:55,700
but in terms of what relative proportions to those changes cause the most rapid decrease to the cost.

169
00:10:56,240 --> 00:10:58,270
A true gradient descent step

170
00:10:58,270 --> 00:11:01,820
would involve doing this for all your tens and thousands of training examples

171
00:11:01,820 --> 00:11:04,260
and averaging the desired changes that you get.

172
00:11:04,830 --> 00:11:06,340
But that's computationally slow.

173
00:11:06,690 --> 00:11:10,480
So instead you randomly subdivide the data into these mini-batches

174
00:11:10,480 --> 00:11:13,460
and compute each step with respect to a mini-batch.

175
00:11:13,900 --> 00:11:17,690
Repeatedly going through all of the mini batches and making these adjustments,

176
00:11:17,690 --> 00:11:21,050
you will converge towards a local minimum of the cost function,

177
00:11:21,430 --> 00:11:25,740
which is to say, your network is going to end up doing a really good job on the training examples.

178
00:11:27,450 --> 00:11:32,290
So with all of that said, every line of code that would go into implementing backprop

179
00:11:32,290 --> 00:11:36,970
actually corresponds with something that you have now seen, at least in informal terms.

180
00:11:37,570 --> 00:11:40,960
But sometimes knowing what the math does is only half the battle,

181
00:11:40,960 --> 00:11:44,460
and just representing the damn thing is where it gets all muddled and confusing.

182
00:11:44,930 --> 00:11:47,620
So for those of you who do want to go deeper,

183
00:11:47,620 --> 00:11:50,670
the next video goes through the same ideas that were just presented here

184
00:11:50,670 --> 00:11:52,750
but in terms of the underlying calculus,

185
00:11:52,750 --> 00:11:56,760
which should hopefully make it a little more familiar as you see the topic in other resources.

186
00:11:57,210 --> 00:11:59,440
Before that, one thing worth emphasizing is that

187
00:11:59,440 --> 00:12:04,320
for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks,

188
00:12:04,320 --> 00:12:06,120
you need a lot of training data.

189
00:12:06,430 --> 00:12:09,860
In our case, one thing that makes handwritten digits such a nice example

190
00:12:09,860 --> 00:12:12,110
is that there exists the MNIST database

191
00:12:12,110 --> 00:12:15,290
with so many examples that have been labeled by humans.

192
00:12:15,290 --> 00:12:19,000
So a common challenge that those of you working in machine learning will be familiar with

193
00:12:19,000 --> 00:12:21,930
is just getting the labeled training data that you actually need,

194
00:12:22,240 --> 00:12:25,080
whether that's having people label tens of thousands of images

195
00:12:25,080 --> 00:12:27,550
or whatever other data type you might be dealing with.

196
00:12:27,870 --> 00:12:33,160
And this actually transitions really nicely to today's extremely relevant sponsor - CrowdFlower,

197
00:12:33,160 --> 00:12:34,340
which is a software platform

198
00:12:34,340 --> 00:12:38,090
where data scientists and machine learning teams can create training data.

199
00:12:38,600 --> 00:12:41,780
They allow you to upload text or audio or image data,

200
00:12:41,780 --> 00:12:43,880
and have it annotated by real people.

201
00:12:44,190 --> 00:12:46,940
You may have heard of the human-in-the-loop approach before,

202
00:12:46,940 --> 00:12:49,330
and this is essentially what we're talking about here:

203
00:12:49,330 --> 00:12:52,960
“leveraging human intelligence to train machine intelligence”.

204
00:12:53,480 --> 00:12:56,380
They employ a whole bunch of pretty smart quality control mechanisms

205
00:12:56,380 --> 00:12:58,170
to keep the data clean and accurate,

206
00:12:58,170 --> 00:13:02,600
and they've helped to train test and tune thousands of data and AI projects.

207
00:13:02,600 --> 00:13:06,360
And what's most fun, there's actually a free t-shirt in this for you guys.

208
00:13:06,360 --> 00:13:10,570
If you go to 3b1b.co/crowdflower,

209
00:13:10,570 --> 00:13:13,340
or follow the link on screen and in the description,

210
00:13:13,340 --> 00:13:16,110
you can create a free account and run a project,

211
00:13:16,110 --> 00:13:18,540
and they'll send you a free shirt once you've done the job.

212
00:13:19,000 --> 00:13:21,070
And the shirt it's actually pretty cool, I quite like it.

213
00:13:21,490 --> 00:13:23,720
So thanks to CrowdFlower for supporting this video,

214
00:13:23,720 --> 00:13:27,050
and thank you also to everyone on Patreon helping support these videos.

