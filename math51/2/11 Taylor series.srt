1
00:00:15,039 --> 00:00:19,240
When I first learned about Taylor series,
I definitely didn’t appreciate how important

2
00:00:19,240 --> 00:00:22,119
they are.
But time and time again they come up in math,

3
00:00:22,119 --> 00:00:26,219
physics, and many fields of engineering because
they’re one of the most powerful tools that

4
00:00:26,219 --> 00:00:29,289
math has to offer for approximating functions.

5
00:00:29,289 --> 00:00:34,220
One of the first times this clicked for me
as a student was not in a calculus class,

6
00:00:34,220 --> 00:00:37,560
but in a physics class.
We were studying some problem that had to

7
00:00:37,560 --> 00:00:42,280
do with the potential energy of a pendulum,
and for that you need an expression for how

8
00:00:42,280 --> 00:00:48,750
high the weight of the pendulum is above its
lowest point, which works out to be proportional

9
00:00:48,750 --> 00:00:54,010
to one minus the cosine of the angle between
the pendulum and the vertical.

10
00:00:54,010 --> 00:00:57,879
The specifics of the problem we were trying
to solve are beyond the point here, but I’ll

11
00:00:57,879 --> 00:01:07,610
just say that this cosine function made the
problem awkward and unwieldy.

12
00:01:07,610 --> 00:01:15,060
But by approximating cos(theta) as 1 - theta2/2,
of all things, everything fell into place

13
00:01:15,060 --> 00:01:18,780
much more easily.
If you’ve never seen anything like this

14
00:01:18,780 --> 00:01:23,870
before, an approximation like that might seem
completely out of left field.

15
00:01:23,870 --> 00:01:30,890
If you graph cos(theta) along with this function
1 - theta2/2, they do seem rather close to

16
00:01:30,890 --> 00:01:36,420
each other for small angles near 0, but how
would you even think to make this approximation?

17
00:01:36,420 --> 00:01:43,280
And how would you find this particular quadratic?
The study of Taylor series is largely about

18
00:01:43,280 --> 00:01:48,440
taking non-polynomial functions, and finding
polynomials that approximate them near some

19
00:01:48,440 --> 00:01:51,549
input.
The motive is that polynomials tend to be

20
00:01:51,549 --> 00:01:56,530
much easier to deal with than other functions:
They’re easier to compute, easier to take

21
00:01:56,530 --> 00:02:00,710
derivatives, easier to integrate...they’re
just all around friendly.

22
00:02:00,710 --> 00:02:06,240
So let’s look at the function cos(x), and
take a moment to think about how you might

23
00:02:06,240 --> 00:02:14,510
find a quadratic approximation near x = 0.
That is, among all the polynomials that look

24
00:02:14,510 --> 00:02:25,909
c0 + c1x + c2x2 for some choice of the constants
c0, c1 and c2, find the one that most resembles

25
00:02:25,909 --> 00:02:33,359
cos(x) near x=0; whose graph kind of spoons
with the graph of cos(x) at that point.

26
00:02:33,359 --> 00:02:40,090
Well, first of all, at the input 0 the value
of cos(x) is 1, so if our approximation is

27
00:02:40,090 --> 00:02:46,629
going to be any good at all, it should also
equal 1 when you plug in 0. Plugging in 0

28
00:02:46,629 --> 00:02:53,239
just results in whatever c0 is, so we can
set that equal to 1.

29
00:02:53,239 --> 00:02:57,549
This leaves us free to choose constant c1
and c2 to make this approximation as good

30
00:02:57,549 --> 00:03:02,499
as we can, but nothing we do to them will
change the fact that the polynomial equals

31
00:03:02,499 --> 00:03:07,379
1 at x=0.
It would also be good if our approximation

32
00:03:07,379 --> 00:03:13,240
had the same tangent slope as as cos(x) at
this point of interest. Otherwise, the approximation

33
00:03:13,240 --> 00:03:18,329
drifts away from the cosine graph even fro
value of x very close to 0.

34
00:03:18,329 --> 00:03:24,840
The derivative of cos(x) is -sin(x), and at
x=0 that equals 0, meaning its tangent line

35
00:03:24,840 --> 00:03:30,840
is flat.
Working out the derivative of our quadratic,

36
00:03:30,840 --> 00:03:42,939
you get c1 + 2c2x. At x=0 that equals whatever
we choose for c1. So this constant c1 controls

37
00:03:42,939 --> 00:03:48,939
the derivative of our approximation around
x=0. Setting it equal to 0 ensures that our

38
00:03:48,939 --> 00:03:53,504
approximation has the same derivative as cos(x),
and hence the same tangent slope.

39
00:03:53,504 --> 00:03:59,501
This leaves us free to change c2, but the
value and slope of our polynomial at x=0 are

40
00:03:59,501 --> 00:04:06,639
locked in place to match that of cos(x).

41
00:04:06,639 --> 00:04:13,510
The cosine graph curves downward above x=0,
it has a negative second derivative. Or in

42
00:04:13,510 --> 00:04:18,529
other words, even though the rate of change
is 0 at that point, the rate of change itself

43
00:04:18,529 --> 00:04:25,270
is decreasing around that point.
Specifically, since its derivative is -sin(x)

44
00:04:25,270 --> 00:04:32,630
its second derivative is -cos(x), so at x=0
its second derivative is -1.

45
00:04:32,630 --> 00:04:37,970
In the same way that we wanted the derivative
of our approximation to match that of cosine,

46
00:04:37,970 --> 00:04:42,130
so that their values wouldn’t drift apart
needlessly quickly, making sure that their

47
00:04:42,130 --> 00:04:47,650
second derivatives match will ensure that
they curve at the same rate; that the slope

48
00:04:47,650 --> 00:04:52,430
of our polynomial doesn’t drift away from
the slope of cos(x) any more quickly than

49
00:04:52,430 --> 00:04:56,730
it needs to.
Pulling out that same derivative we had before,

50
00:04:56,730 --> 00:05:02,900
then taking its derivative, we see that the
second derivative of this polynomial is exactly

51
00:05:02,900 --> 00:05:13,160
2c2, so to make sure this second derivative
also equals -1 at x=0, 2c2 must equal -1,

52
00:05:13,160 --> 00:05:21,620
meaning c2 itself has to be -½.
This gives us the approximation 1 + 0x - ½

53
00:05:21,620 --> 00:05:22,620
x2.

54
00:05:22,620 --> 00:05:30,580
To get a feel for how good this is, if you
estimated cos(0.1) with this polynomial, you’d

55
00:05:30,580 --> 00:05:40,440
get 0.995. And this is the true value of cos(0.1).
It’s a really good approximation.

56
00:05:40,440 --> 00:05:45,320
Take a moment to reflect on what just happened.
You had three degrees of freedom with a quadratic

57
00:05:45,320 --> 00:05:51,870
approximation, the constants c0, c1, and c2.
c0 was responsible for making sure that the

58
00:05:51,870 --> 00:05:58,650
output of the approximation matches that of
cos(x) at x=0, c1 was in charge of making

59
00:05:58,650 --> 00:06:06,530
sure the derivatives match at that point,
and c2 was responsible for making sure the

60
00:06:06,530 --> 00:06:11,420
second derivatives match up.
This ensures that the way your approximation

61
00:06:11,420 --> 00:06:17,840
changes as you move away from x=0, and the
way that the rate of change itself changes,

62
00:06:17,840 --> 00:06:24,360
is as similar as possible to behavior of cos(x),
given the amount of control you have.

63
00:06:24,360 --> 00:06:28,870
You could give yourself more control by allowing
more terms in your polynomial, and matching

64
00:06:28,870 --> 00:06:35,120
higher order derivatives of cos(x).
For example, add on the term c3x3 for some

65
00:06:35,120 --> 00:06:41,180
constant c3.
If you take the third derivative of a cubic

66
00:06:41,180 --> 00:06:45,700
polynomial, anything quadratic or smaller
goes to 0.

67
00:06:45,700 --> 00:06:54,030
As for that last term, after three iterations
of the power rule it looks like 1*2*3*c3.

68
00:06:54,030 --> 00:07:04,040
On the other hand, the third derivative of
cos(x) is sin(x), which equals 0 at x=0, so

69
00:07:04,040 --> 00:07:09,920
to make the third derivatives match, the constant
c3 should be 0.

70
00:07:09,920 --> 00:07:16,450
In other words, not only is 1 - ½ x2 the
best possible quadratic approximation of cos(x)

71
00:07:16,450 --> 00:07:21,580
around x=0, it’s also the best possible
cubic approximation.

72
00:07:21,580 --> 00:07:28,860
You can actually make an improvement by adding
a fourth order term, c4x4. The fourth derivative

73
00:07:28,860 --> 00:07:35,670
of cos(x) is itself, which equals 1 at x=0.
And what’s the fourth derivative of our

74
00:07:35,670 --> 00:07:41,160
polynomial with this new term? Well, when
you keep applying the power rule over and

75
00:07:41,160 --> 00:07:49,150
over, with those exponents all hopping down
front, you end up with 1*2*3*4*c4, which is

76
00:07:49,150 --> 00:07:53,970
24c4
So if we want this to match the fourth derivative

77
00:07:53,970 --> 00:08:05,750
of cos(x), which is 1, c4 must be 1/24.
And indeed, the polynomial 1 - ½ x2 + 1/24

78
00:08:05,750 --> 00:08:13,820
x4, which looks like this, is a very close
approximation for cos(x) around x = 0.

79
00:08:13,820 --> 00:08:18,600
In any physics problem involving the cosine
of some small angle, for example, predictions

80
00:08:18,600 --> 00:08:24,980
would be almost unnoticeably different if
you substituted this polynomial for cos(x).

81
00:08:24,980 --> 00:08:30,270
Now, step back and notice a few things about
this process.

82
00:08:30,270 --> 00:08:35,070
First, factorial terms naturally come up in
this process.

83
00:08:35,070 --> 00:08:43,010
When you take n derivatives of xn, letting
the power rule just keep cascading, what you’re

84
00:08:43,010 --> 00:08:51,249
left with is 1*2*3 and on up to n.
So you don’t simply set the coefficients

85
00:08:51,249 --> 00:08:55,990
of the polynomial equal to whatever derivative
value you want, you have to divide by the

86
00:08:55,990 --> 00:09:02,870
appropriate factorial to cancel out this effect.
For example, that x4 coefficient is the fourth

87
00:09:02,870 --> 00:09:09,540
derivative of cosine, 1, divided by 4 factorial,
24.

88
00:09:09,540 --> 00:09:16,010
The second thing to notice is that adding
new terms, like this c4x4, doesn’t mess

89
00:09:16,010 --> 00:09:20,129
up what old terms should be, and that’s
important.

90
00:09:20,129 --> 00:09:26,230
For example, the second derivative of this
polynomial at x = 0 is still equal to 2 times

91
00:09:26,230 --> 00:09:31,059
the second coefficient, even after introducing
higher order terms to the polynomial.

92
00:09:31,059 --> 00:09:36,500
And it’s because we’re plugging in x=0,
so the second derivative of any higher order

93
00:09:36,500 --> 00:09:43,240
terms, which all include an x, will wash away.
The same goes for any other derivative, which

94
00:09:43,240 --> 00:09:52,680
is why each derivative of a polynomial at
x=0 is controlled by one and only one coefficient.

95
00:09:52,680 --> 00:09:58,360
If instead you were approximating near an
input other than 0, like x=pi, in order to

96
00:09:58,360 --> 00:10:03,880
get the same effect you would have to write
your polynomial in terms of powers of (x - pi),

97
00:10:03,880 --> 00:10:09,050
or whatever input you’re looking at.
This makes it look notably more complicated,

98
00:10:09,050 --> 00:10:17,069
but all it’s doing is making the point pi
look like 0, so that plugging in x = pi will

99
00:10:17,069 --> 00:10:22,699
result in a lot of nice cancelation that leaves
only one constant.

100
00:10:22,699 --> 00:10:27,550
And finally, on a more philosophical level,
notice how what we’re doing here is essentially

101
00:10:27,550 --> 00:10:33,100
taking information about the higher order
derivatives of a function at a single point,

102
00:10:33,100 --> 00:10:39,620
and translating it into information about
the value of that function near that point.

103
00:10:39,620 --> 00:10:46,290
We can take as many derivatives of cos(x)
as we want, it follows this nice cyclic pattern

104
00:10:46,290 --> 00:10:56,100
cos(x), -sin(x), -cos(x), sin(x), and repeat.
So the value of these derivative of x=0 have

105
00:10:56,100 --> 00:11:04,519
the cyclic pattern 1, 0, -1, 0, and repeat.
And knowing the values of all those higher-order

106
00:11:04,519 --> 00:11:09,990
derivatives is a lot of information about
cos(x), even though it only involved plugging

107
00:11:09,990 --> 00:11:18,619
in a single input, x=0.
That information is leveraged to get an approximation

108
00:11:18,619 --> 00:11:25,690
around this input by creating a polynomial
whose higher order derivatives, match up with

109
00:11:25,690 --> 00:11:31,769
those of cos(x), following this same 1, 0,
-1, 0 cyclic pattern.

110
00:11:31,769 --> 00:11:37,640
To do that, make each coefficient of this
polynomial follow this same pattern, but divide

111
00:11:37,640 --> 00:11:42,300
each one by the appropriate factorial, like
I mentioned before, so as to cancel out the

112
00:11:42,300 --> 00:11:49,649
cascading effects of many power rule applications.
The polynomials you get by stopping this process

113
00:11:49,649 --> 00:11:53,980
at any point are called “Taylor polynomials”
for cos(x) around the input x=0.

114
00:11:53,980 --> 00:11:58,649
More generally, and hence more abstractly,
if we were dealing with some function other

115
00:11:58,649 --> 00:12:04,189
than cosine, you would compute its derivative,
second derivative, and so on, getting as many

116
00:12:04,189 --> 00:12:09,749
terms as you’d like, and you’d evaluate
each one at x=0.

117
00:12:09,749 --> 00:12:16,509
Then for your polynomial approximation, the
coefficient of each xn term should be the

118
00:12:16,509 --> 00:12:23,910
value of the nth derivative of the function
at 0, divided by (n!).

119
00:12:23,910 --> 00:12:29,839
This rather abstract formula is something
you’ll likely see in any text or course

120
00:12:29,839 --> 00:12:34,689
touching on Taylor polynomials.
And when you see it, think to yourself that

121
00:12:34,689 --> 00:12:38,809
the constant term ensures that the value of
the polynomial matches that of f(x) at x=0,

122
00:12:38,809 --> 00:12:46,420
the next term ensures that the slope of the
polynomial matches that of the function, the

123
00:12:46,420 --> 00:12:52,290
next term ensure the rate at which that slope
changes is the same, and so on, depending

124
00:12:52,290 --> 00:12:57,029
on how many terms you want.
The more terms you choose, the closer the

125
00:12:57,029 --> 00:13:02,759
approximation, but the tradeoff is that your
polynomial is more complicated.

126
00:13:02,759 --> 00:13:11,300
And if you want to approximate near some input
a other than 0, you write the polynomial in

127
00:13:11,300 --> 00:13:19,279
terms of (x-a) instead, and evaluate all the
derivatives of f at that input a.

128
00:13:19,279 --> 00:13:24,889
This is what Taylor series look like in their
fullest generality. Changing the value of

129
00:13:24,889 --> 00:13:30,630
a changes where the approximation is hugging
the original function; where its higher order

130
00:13:30,630 --> 00:13:34,680
derivatives will be equal to those of the
original function.

131
00:13:34,680 --> 00:13:44,069
One of the simplest meaningful examples is
ex, around the input x=0. Computing its derivatives

132
00:13:44,069 --> 00:13:52,110
is nice, since the derivative of ex is itself,
so its second derivative is also ex, as is

133
00:13:52,110 --> 00:13:59,689
its third, and so on.
So at the point x=0, these are all 1. This

134
00:13:59,689 --> 00:14:16,290
means our polynomial approximation looks like
1 + x + ½ x2 + 1/(3!) x3 + 1/(4!) x4, and

135
00:14:16,290 --> 00:14:28,239
so on, depending on how many terms you want.
These are the Taylor polynomials 

136
00:14:28,239 --> 00:14:29,249
for ex.

137
00:14:29,249 --> 00:14:34,819
In the spirit of showing you just how connected
the topics of calculus are, let me turn to

138
00:14:34,819 --> 00:14:41,829
a completely different way to understand this
second order term geometrically. It’s related

139
00:14:41,829 --> 00:14:48,190
to the fundamental theorem of calculus, which
I talked about in chapters 1 and 8.

140
00:14:48,190 --> 00:14:53,350
Like we did in those videos, consider a function
that gives the area under some graph between

141
00:14:53,350 --> 00:14:58,540
a fixed left point and a variable right point.
What we’re going to do is think about how

142
00:14:58,540 --> 00:15:03,759
to approximate this area function, not the
function for the graph like we were doing

143
00:15:03,759 --> 00:15:10,029
before. Focusing on that area is what will
make the second order term pop out.

144
00:15:10,029 --> 00:15:16,459
Remember, the fundamental theorem of calculus
is that this graph itself represents the derivative

145
00:15:16,459 --> 00:15:21,660
of the area function, and as a reminder it’s
because a slight nudge dx to the right bound

146
00:15:21,660 --> 00:15:28,970
on the area gives a new bit of area approximately
equal to the height of the graph times dx,

147
00:15:28,970 --> 00:15:33,193
in a way that’s increasingly accurate for
smaller choice of dx.

148
00:15:33,193 --> 00:15:34,899
So df over dx, the change in area divided
by that nudge dx, approaches the height of

149
00:15:34,899 --> 00:15:37,639
the graph as dx approaches 0.
But if you wanted to be more accurate about

150
00:15:37,639 --> 00:15:42,899
the change to the area given some change to
x that isn’t mean to approach 0, you would

151
00:15:42,899 --> 00:15:48,480
take into account this portion right here,
which is approximately a triangle.

152
00:15:48,480 --> 00:15:56,350
Let’s call the starting input a, and the
nudged input above it x, so that this change

153
00:15:56,350 --> 00:16:01,759
is (x-a).
The base of that little triangle is that change

154
00:16:01,759 --> 00:16:10,290
(x-a), and its height is the slope of the
graph times (x-a). Since this graph is the

155
00:16:10,290 --> 00:16:16,149
derivative of the area function, that slope
is the second derivative of the area function,

156
00:16:16,149 --> 00:16:20,959
evaluated at the input a.
So the area of that triangle, ½ base times

157
00:16:20,959 --> 00:16:27,559
height, is one half times the second derivative
of the area function, evaluated at a, multiplied

158
00:16:27,559 --> 00:16:34,339
by (x-a)2.
And this is exactly what you see with Taylor

159
00:16:34,339 --> 00:16:39,399
polynomials. If you knew the various derivative
information about the area function at the

160
00:16:39,399 --> 00:16:50,630
point a, you would approximate this area at
x to be the area up to a, f(a), plus the area

161
00:16:50,630 --> 00:16:58,170
of this rectangle, which is the first derivative
times (x-a), plus the area of this triangle,

162
00:16:58,170 --> 00:17:05,060
which is ½ (the second derivative) * (x - a)2.
I like this, because even though it looks

163
00:17:05,060 --> 00:17:14,329
a bit messy all written out, each term has
a clear meaning you can point to on the diagram.

164
00:17:14,329 --> 00:17:17,669
We could call it an end here, and you’d
have you’d have a phenomenally useful tool

165
00:17:17,670 --> 00:17:24,500
for approximations with these Taylor polynomials.
But if you’re thinking like a mathematician,

166
00:17:24,500 --> 00:17:29,640
one question you might ask is if it makes
sense to never stop, and add up infinitely

167
00:17:29,640 --> 00:17:35,120
many terms.
In math, an infinite sum is called a “series”,

168
00:17:35,120 --> 00:17:39,290
so even though one of the approximations with
finitely many terms is called a “Taylor

169
00:17:39,290 --> 00:17:43,440
polynomial” for your function, adding all
infinitely many terms gives what’s called

170
00:17:43,440 --> 00:17:47,270
a “Taylor series”.
Now you have to be careful with the idea of

171
00:17:47,270 --> 00:17:53,170
an infinite series, because it doesn’t actually
make sense to add infinitely many things;

172
00:17:53,170 --> 00:17:57,800
you can only hit the plus button on the calculator
so many times.

173
00:17:57,800 --> 00:18:04,340
But if you have a series where adding more
and more terms gets you increasingly close

174
00:18:04,340 --> 00:18:11,960
to some specific value, you say the series
converges to that value. Or, if you’re comfortable

175
00:18:11,960 --> 00:18:16,910
extending the definition of equality to include
this kind of series convergence, you’d say

176
00:18:16,910 --> 00:18:23,890
the series as a whole, this infinite sum,
equals the value it converges to.

177
00:18:23,890 --> 00:18:31,290
For example, look at the Taylor polynomials
for ex, and plug in some input like x = 1.

178
00:18:31,290 --> 00:18:37,460
As you add more and more polynomial terms,
the total sum gets closer and closer to the

179
00:18:37,460 --> 00:18:44,010
value e, so we say that the infinite series
converges to the number e. Or, what’s saying

180
00:18:44,010 --> 00:18:46,380
the same thing, that it equals the number
e.

181
00:18:46,380 --> 00:18:54,000
In fact, it turns out that if you plug in
any other value of x, like x=2, and look at

182
00:18:54,000 --> 00:19:00,210
the value of higher and higher order Taylor
polynomials at this value, they will converge

183
00:19:00,210 --> 00:19:07,420
towards ex, in this case e2.
This is true for any input, no matter how

184
00:19:07,420 --> 00:19:13,460
far away from 0 it is, even though these Taylor
polynomials are constructed only from derivative

185
00:19:13,460 --> 00:19:22,410
information gathered at the input 0.
In a case like this, we say ex equals its

186
00:19:22,410 --> 00:19:28,200
Taylor series at all inputs x, which is kind
of a magical thing to have happen.

187
00:19:28,200 --> 00:19:34,240
Although this is also true for some other
important functions, like sine and cosine,

188
00:19:34,240 --> 00:19:39,350
sometimes these series only converge within
a certain range around the input whose derivative

189
00:19:39,350 --> 00:19:43,400
information you’re using.
If you work out the Taylor series for the

190
00:19:43,400 --> 00:19:49,790
natural log of x around the input x = 1, which
is built from evaluating the higher order

191
00:19:49,790 --> 00:19:56,310
derivatives of ln(x) at x=1, this is what
it looks like.

192
00:19:56,310 --> 00:20:01,490
When you plug in an input between 0 and 2,
adding more and more terms of this series

193
00:20:01,490 --> 00:20:06,450
will indeed get you closer and closer to the
natural log of that input.

194
00:20:06,450 --> 00:20:12,590
But outside that range, even by just a bit,
the series fails to approach anything.

195
00:20:12,590 --> 00:20:20,630
As you add more and more terms the sum bounces
back and forth wildly, it does not approaching

196
00:20:20,630 --> 00:20:26,010
the natural log of that value, even though
the natural log of x is perfectly well defined

197
00:20:26,010 --> 00:20:30,880
for inputs above 2.
In some sense, the derivative information

198
00:20:30,880 --> 00:20:36,740
of ln(x) at x=1 doesn’t propagate out that
far.

199
00:20:36,740 --> 00:20:41,240
In a case like this, where adding more terms
of the series doesn’t approach anything,

200
00:20:41,240 --> 00:20:46,420
you say the series diverges.
And that maximum distance between the input

201
00:20:46,420 --> 00:20:50,890
you’re approximating near, and points where
the outputs of these polynomials actually

202
00:20:50,890 --> 00:20:57,030
do converge, is called the “radius of convergence”
for the Taylor series.

203
00:20:57,030 --> 00:21:01,870
There remains more to learn about Taylor series,
their many use cases, tactics for placing

204
00:21:01,870 --> 00:21:06,430
bounds on the error of these approximations,
tests for understanding when these series

205
00:21:06,430 --> 00:21:10,420
do and don’t converge.
For that matter there remains more to learn

206
00:21:10,420 --> 00:21:15,600
about calculus as a whole, and the countless
topics not touched by this series.

207
00:21:15,600 --> 00:21:19,930
The goal with these videos is to give you
the fundamental intuitions that make you feel

208
00:21:19,930 --> 00:21:25,610
confident and efficient learning more on your
own, and potentially even rediscovering more

209
00:21:25,610 --> 00:21:31,110
of the topic for yourself.
In the case of Taylor series, the fundamental

210
00:21:31,110 --> 00:21:36,870
intuition to keep in mind as you explore more
is that they translate derivative information

211
00:21:36,870 --> 00:21:47,300
at a single point to approximation information
around that point.

212
00:21:47,300 --> 00:21:51,520
The next series like this will be on probability,
and if you want early access as those videos

213
00:21:51,520 --> 00:21:53,020
are made, you know where to go.

