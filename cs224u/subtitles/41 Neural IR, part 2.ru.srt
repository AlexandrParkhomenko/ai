1
00:00:05,600 --> 00:00:08,080
привет всем, добро пожаловать в четвертую часть

2
00:00:08,080 --> 00:00:11,040
нашей серии о nlu nir,

3
00:00:11,040 --> 00:00:12,960
скринкаст будет вторым среди

4
00:00:12,960 --> 00:00:14,880
трех наших видеороликов о

5
00:00:14,880 --> 00:00:17,920
поиске нейронной информации,

6
00:00:19,039 --> 00:00:21,119
просто чтобы напомнить, что это функциональный

7
00:00:21,119 --> 00:00:23,039
взгляд на нейронный ir, который мы оставили в

8
00:00:23,039 --> 00:00:25,279
предыдущем скринкасте,

9
00:00:25,279 --> 00:00:27,039
наша модель возьмет  запрос и

10
00:00:27,039 --> 00:00:30,240
документ, а затем выведет оценку

11
00:00:30,240 --> 00:00:32,079
, которая оценит релевантность этого

12
00:00:32,079 --> 00:00:34,320
документа запросу,

13
00:00:34,320 --> 00:00:36,640
мы отсортируем документы по уменьшению

14
00:00:36,640 --> 00:00:37,600
оценки,

15
00:00:37,600 --> 00:00:41,040
чтобы получить лучшие результаты k,

16
00:00:41,360 --> 00:00:42,960
давайте начнем

17
00:00:42,960 --> 00:00:45,039
с очень эффективной парадигмы для

18
00:00:45,039 --> 00:00:47,840
построения нейронных моделей,

19
00:00:47,840 --> 00:00:52,079
а именно запроса документа  взаимодействие,

20
00:00:53,360 --> 00:00:55,199
поэтому при заданном запросе и документе они будут

21
00:00:55,199 --> 00:00:57,120
токенизированы,

22
00:00:57,120 --> 00:00:59,280
затем мы встроим токены каждого

23
00:00:59,280 --> 00:01:02,719
в статическое векторное представление,

24
00:01:02,719 --> 00:01:04,720
чтобы это могли быть, например, векторы GloVe

25
00:01:04,720 --> 00:01:06,000


26
00:01:06,000 --> 00:01:07,360
или начальные

27
00:01:07,360 --> 00:01:10,640
представления о рождении,

28
00:01:10,960 --> 00:01:12,960
мы затем создадим то, что называется документом запроса

29
00:01:12,960 --> 00:01:15,439
матрица взаимодействия

30
00:01:15,439 --> 00:01:17,040
обычно представляет собой не что иное,

31
00:01:17,040 --> 00:01:20,240
как матрицу косинусного сходства между

32
00:01:20,240 --> 00:01:21,759
каждой парой слов,

33
00:01:21,759 --> 00:01:24,479
каждой парой слов в запросе

34
00:01:24,479 --> 00:01:27,720
и  документ

35
00:01:28,640 --> 00:01:30,640
теперь, когда у нас есть эта матрица, нам просто

36
00:01:30,640 --> 00:01:32,960
нужно уменьшить ее до одной оценки, которая

37
00:01:32,960 --> 00:01:35,360
оценивает релевантность нашего

38
00:01:35,360 --> 00:01:38,159
документа этому запросу,

39
00:01:38,240 --> 00:01:40,079
чтобы сделать это, мы просто изучим кучу

40
00:01:40,079 --> 00:01:42,799
нейронных слоев, таких как свертка или линейные

41
00:01:42,799 --> 00:01:45,680
слои с объединением,

42
00:01:45,680 --> 00:01:48,159
пока мы  в конечном итоге получить единую оценку для

43
00:01:48,159 --> 00:01:51,520
этой пары документов запроса.

44
00:01:52,799 --> 00:01:55,200
Многие модели ir попадают в эту

45
00:01:55,200 --> 00:01:57,280
категорию, особенно те, которые были

46
00:01:57,280 --> 00:02:00,479
представлены в период с 2016 по 2018 или

47
00:02:00,479 --> 00:02:03,200
2019 год.

48
00:02:04,399 --> 00:02:06,960
при достаточном количестве обучающих данных

49
00:02:06,960 --> 00:02:09,280
модели взаимодействия документов запроса могут

50
00:02:09,280 --> 00:02:11,520
достичь значительно лучшего качества, чем

51
00:02:11,520 --> 00:02:14,160
модели набора слов.  как bm25,

52
00:02:14,160 --> 00:02:15,599
и они действительно могут сделать это при

53
00:02:15,599 --> 00:02:17,599
разумном увеличении

54
00:02:17,599 --> 00:02:22,280
умеренного увеличения вычислительных затрат,

55
00:02:23,040 --> 00:02:24,560
поэтому, как обсуждалось в предыдущих

56
00:02:24,560 --> 00:02:26,239
скринкастах,

57
00:02:26,239 --> 00:02:28,400
эти модели обычно используются в

58
00:02:28,400 --> 00:02:31,280
качестве последнего этапа конвейера переранжирования,

59
00:02:31,280 --> 00:02:33,280
и, в частности,

60
00:02:33,280 --> 00:02:34,239
на этом

61
00:02:34,239 --> 00:02:35,840
рисунке здесь они  используется для повторного ранжирования

62
00:02:35,840 --> 00:02:39,360
тысячи лучших пассажей, извлеченных bm25,

63
00:02:39,360 --> 00:02:40,879
и это делается для того, чтобы убедиться, что их

64
00:02:40,879 --> 00:02:43,040
задержка приемлема

65
00:02:43,040 --> 00:02:45,680
, но все еще импровизирована.  Сравнивая mrr

66
00:02:45,680 --> 00:02:50,360
и качество с поиском bm25,

67
00:02:55,840 --> 00:02:59,280
совсем недавно, в 2019 году, их сообщество

68
00:02:59,280 --> 00:03:01,040
обнаружило силу рождения для

69
00:03:01,040 --> 00:03:02,720
ранжирования

70
00:03:02,720 --> 00:03:06,239
функционально, это очень похоже

71
00:03:06,239 --> 00:03:07,920
на парадигму, которую мы только что видели при взаимодействии с запросом

72
00:03:07,920 --> 00:03:09,840
документа,

73
00:03:09,840 --> 00:03:11,519
так что здесь мы собираемся  передать

74
00:03:11,519 --> 00:03:13,440
и запрос, и документ как одну

75
00:03:13,440 --> 00:03:15,840
последовательность с двумя сегментами, один сегмент

76
00:03:15,840 --> 00:03:17,599
для запроса и один сегмент для

77
00:03:17,599 --> 00:03:19,599
документа, как показано,

78
00:03:19,599 --> 00:03:21,200
мы пропустим это через все уровни

79
00:03:21,200 --> 00:03:22,800
рождения

80
00:03:22,800 --> 00:03:24,799
и, наконец,

81
00:03:24,799 --> 00:03:27,120
извлечем встраивание токена класса из рождения и  уменьшите его

82
00:03:27,120 --> 00:03:29,360
до единого балла с помощью окончательной линейной

83
00:03:29,360 --> 00:03:30,840
головы поверх

84
00:03:30,840 --> 00:03:33,920
рождения, поскольку вы, вероятно, можете сказать, что это

85
00:03:33,920 --> 00:03:36,480
не что иное, как стандартный классификатор рождения,

86
00:03:36,480 --> 00:03:39,120
где мы собираемся взять баллы

87
00:03:39,120 --> 00:03:40,879
или уверенность, что это

88
00:03:40,879 --> 00:03:42,959
результат классификатора, и использовать его для  ранжирование

89
00:03:42,959 --> 00:03:44,720
наших отрывков

90
00:03:44,720 --> 00:03:47,200
и, как и любая другая задача с bert, мы

91
00:03:47,200 --> 00:03:49,599
должны сначала точно настроить эту модель рождения

92
00:03:49,599 --> 00:03:51,599
с соответствующими обучающими данными, прежде чем

93
00:03:51,599 --> 00:03:54,560
использовать ее для нашей задачи,

94
00:03:54,799 --> 00:03:56,959
мы обсудили, как обучать нейронные

95
00:03:56,959 --> 00:03:58,879
модели ir  в предыдущем скринкасте, так что

96
00:03:58,879 --> 00:04:02,879
обращайтесь к этому, если хотите,

97
00:04:03,680 --> 00:04:05,680
чтобы эта действительно простая модель в дополнение к

98
00:04:05,680 --> 00:04:08,000
рождению была основой для огромного

99
00:04:08,000 --> 00:04:10,000
прогресса в поиске за последние два

100
00:04:10,000 --> 00:04:12,000
года,

101
00:04:12,000 --> 00:04:14,480
и, в частности

102
00:04:14,480 --> 00:04:16,238
, стоит упомянуть первый публичный

103
00:04:16,238 --> 00:04:18,560
экземпляр этого, который был  В январе

104
00:04:18,560 --> 00:04:21,279
2019 года в задаче рейтинга прохождения мисс Марко

105
00:04:21,279 --> 00:04:22,639


106
00:04:22,639 --> 00:04:24,880
здесь Ногейра и Чо

107
00:04:24,880 --> 00:04:26,880
сделали простую заявку на основе рождения

108
00:04:26,880 --> 00:04:29,440
в таблицу лидеров мисс Марко,

109
00:04:29,440 --> 00:04:31,440
которая продемонстрировала значительный прогресс по сравнению

110
00:04:31,440 --> 00:04:33,600
с предыдущим уровнем техники, представленным

111
00:04:33,600 --> 00:04:36,000
всего за несколько дней до этого,

112
00:04:36,000 --> 00:04:38,479
к октябрю 2019 года почти  ровно через

113
00:04:38,479 --> 00:04:40,800
год после того, как рождение впервые появилось,

114
00:04:40,800 --> 00:04:42,639
Google публично обсудил

115
00:04:42,639 --> 00:04:44,080
использование рождения в поиске

116
00:04:44,080 --> 00:04:46,400
и вскоре последовал за ним

117
00:04:46,400 --> 00:04:50,000
в ноябре того же года,

118
00:04:54,080 --> 00:04:55,520
но на самом деле история немного

119
00:04:55,520 --> 00:04:56,960
сложнее,

120
00:04:56,960 --> 00:04:59,600
этот очень большой прирост качества привел

121
00:04:59,600 --> 00:05:01,680
к резкому увеличению  вычислительные

122
00:05:01,680 --> 00:05:03,039
затраты,

123
00:05:03,039 --> 00:05:05,680
которые диктуют задержку

124
00:05:05,680 --> 00:05:07,840
и что очень важно для взаимодействия с

125
00:05:07,840 --> 00:05:09,600
пользователем в задачах поиска, как мы

126
00:05:09,600 --> 00:05:12,400
обсуждали ранее,

127
00:05:13,120 --> 00:05:14,160


128
00:05:14,160 --> 00:05:15,919
по сравнению с простыми

129
00:05:15,919 --> 00:05:17,840
запросами.  Все модели взаимодействия с документами, такие как

130
00:05:17,840 --> 00:05:20,720
duet или config nrm

131
00:05:20,720 --> 00:05:23,039
noguera, и выбранные модели рождения увеличили

132
00:05:23,039 --> 00:05:25,600
mrr более чем на восемь пунктов, но также

133
00:05:25,600 --> 00:05:27,680
увеличили задержку

134
00:05:27,680 --> 00:05:31,120
до нескольких секунд на каждый

135
00:05:31,120 --> 00:05:33,520
запрос, поэтому здесь для нас естественно задаться

136
00:05:33,520 --> 00:05:36,639
вопросом

137
00:05:36,639 --> 00:05:39,360
, можем ли мы достичь высокого mrr и низкого

138
00:05:39,360 --> 00:05:42,320
задержка сразу,

139
00:05:42,639 --> 00:05:44,720
и оказывается, что ответ да,

140
00:05:44,720 --> 00:05:46,240
но для этого потребуется много прогресса,

141
00:05:46,240 --> 00:05:47,280


142
00:05:47,280 --> 00:05:48,880
и мы постараемся осветить это в остальной

143
00:05:48,880 --> 00:05:51,039
части скринкаста в следующем, так что

144
00:05:51,039 --> 00:05:54,160
давайте начнем с этого,

145
00:05:54,240 --> 00:05:56,160
чтобы искать  лучший компромисс между

146
00:05:56,160 --> 00:05:58,560
качеством и задержкой, что является нашей целью,

147
00:05:58,560 --> 00:06:00,639
давайте подумаем о том, почему системы ранжирования рождений

148
00:06:00,639 --> 00:06:02,080
такие медленные.

149
00:06:02,080 --> 00:06:04,240
Наше первое наблюдение здесь будет заключаться в том, что системы

150
00:06:04,240 --> 00:06:06,240
ранжирования рождений довольно избыточны в

151
00:06:06,240 --> 00:06:08,479
своих вычислениях,

152
00:06:08,479 --> 00:06:11,520
если вы думаете о том, что они делают, т. е.

153
00:06:11,520 --> 00:06:13,840
им нужно вычислять

154
00:06:13,840 --> 00:06:15,680
контекст контекстуализированное представление

155
00:06:15,680 --> 00:06:18,240
запроса для каждого документа, который мы ранжируем так,

156
00:06:18,240 --> 00:06:20,240
что это тысяча раз для 1000

157
00:06:20,240 --> 00:06:21,759
документов,

158
00:06:21,759 --> 00:06:24,160
и они также должны кодировать каждый документ

159
00:06:24,160 --> 00:06:26,800
для каждого отдельного запроса, который  появляется,

160
00:06:26,800 --> 00:06:30,479
что требуется оценка для этого документа,

161
00:06:30,800 --> 00:06:33,280
конечно, у нас есть документы в наших

162
00:06:33,280 --> 00:06:35,600
коллекциях заранее, и мы можем выполнять

163
00:06:35,600 --> 00:06:37,360
столько предварительной обработки, сколько захотим, в

164
00:06:37,360 --> 00:06:40,400
автономном режиме, прежде чем мы получим какие-либо запросы,

165
00:06:40,400 --> 00:06:42,400
поэтому возникает вопрос, можем ли мы как

166
00:06:42,400 --> 00:06:44,479
-то предварительно вычислить  какую-то форму представления документов

167
00:06:44,479 --> 00:06:46,639
заранее раз и

168
00:06:46,639 --> 00:06:48,639
навсегда, используя эти мощные модели, которые у нас

169
00:06:48,639 --> 00:06:50,800
есть, такие как bert, и храним эти

170
00:06:50,800 --> 00:06:53,199
представления или кэшируем их где-то,

171
00:06:53,199 --> 00:06:55,120
чтобы мы могли просто использовать их быстро каждый

172
00:06:55,120 --> 00:06:57,280
раз, когда у нас есть запрос на ответ,

173
00:06:57,280 --> 00:06:59,039
это будет нашим направляющим вопросом для

174
00:06:59,039 --> 00:07:00,560
остальная часть этого и следующих

175
00:07:00,560 --> 00:07:03,560
скринкастов,

176
00:07:03,599 --> 00:07:06,479
конечно, на самом деле еще не очевидна,

177
00:07:06,479 --> 00:07:08,400
по крайней мере, если мы сможем заранее вычислить такие

178
00:07:08,400 --> 00:07:10,639
представления без большой

179
00:07:10,639 --> 00:07:13,840
потери качества, поскольку все, что мы знаем до сих

180
00:07:13,840 --> 00:07:16,800
пор, может иметь большую эмпирическую ценность

181
00:07:16,800 --> 00:07:18,720
в совместном представлении  запросы и

182
00:07:18,720 --> 00:07:21,680
документы одновременно,

183
00:07:22,400 --> 00:07:24,000
но мы проверим эту гипотезу.

184
00:07:24,000 --> 00:07:25,919


185
00:07:25,919 --> 00:07:27,440
Первый подход к укрощению

186
00:07:27,440 --> 00:07:30,000
вычислительной задержки рождения для ir заключается в

187
00:07:30,000 --> 00:07:32,560
изучении терминов,

188
00:07:32,560 --> 00:07:34,800
взвешивающих ke  Ваше наблюдение заключается в том, что

189
00:07:34,800 --> 00:07:37,840
модели набора слов, такие как bm25, разлагают

190
00:07:37,840 --> 00:07:40,000
оценку каждого документа на

191
00:07:40,000 --> 00:07:41,919
сумму весов документов терминов,

192
00:07:41,919 --> 00:07:44,800
и, возможно, мы можем сделать то же самое, поэтому

193
00:07:44,800 --> 00:07:46,319
можем ли мы узнать эти веса терминов с помощью

194
00:07:46,319 --> 00:07:48,479
bert, в частности,

195
00:07:48,479 --> 00:07:50,479
простой способ сделать это было бы  чтобы

196
00:07:50,479 --> 00:07:53,840
токенизировать запрос и ленту документа, создайте

197
00:07:53,840 --> 00:07:56,160


198
00:07:56,160 --> 00:07:59,440
только документ и используйте линейный слой

199
00:07:59,440 --> 00:08:01,680
для проецирования каждого токена

200
00:08:01,680 --> 00:08:03,440
в документе в одну числовую

201
00:08:03,440 --> 00:08:05,599
оценку,

202
00:08:05,599 --> 00:08:07,360
идея здесь заключается в том, что мы можем сохранить эти

203
00:08:07,360 --> 00:08:09,680
веса терминов документа в инвертированном

204
00:08:09,680 --> 00:08:12,639
индексе, как мы это сделали с  bm25

205
00:08:12,639 --> 00:08:15,280
в классическом ir и быстро искать

206
00:08:15,280 --> 00:08:16,960
эти веса терминов при ответе на

207
00:08:16,960 --> 00:08:18,639
запрос,

208
00:08:18,639 --> 00:08:20,800
это гарантирует, что нам вообще не нужно использовать

209
00:08:20,800 --> 00:08:23,360
bert при ответе на

210
00:08:23,360 --> 00:08:25,280
справедливость,

211
00:08:25,280 --> 00:08:28,000
поскольку мы только что перевели всю нашу работу по рождению в

212
00:08:28,000 --> 00:08:31,759
автономном режиме на этап индексации,

213
00:08:33,919 --> 00:08:36,000
так что это может быть действительно здорово  теперь мы

214
00:08:36,000 --> 00:08:38,320
можем использовать рождение, чтобы узнать гораздо более сильные

215
00:08:38,320 --> 00:08:40,559
веса терминов, чем bm25

216
00:08:40,559 --> 00:08:41,839
um,

217
00:08:41,839 --> 00:08:44,399
а deep city и doctorquity являются двумя

218
00:08:44,399 --> 00:08:45,839
основными моделями в рамках этой эффективной

219
00:08:45,839 --> 00:08:47,839
парадигмы,

220
00:08:47,839 --> 00:08:50,640
как показано на рисунке внизу,

221
00:08:50,640 --> 00:08:52,959
они  действительно значительно превосходят bm-25 и

222
00:08:52,959 --> 00:08:55,120
mrr, но на самом деле они имеют сравнимую

223
00:08:55,120 --> 00:08:56,560
задержку, потому что мы все еще используем

224
00:08:56,560 --> 00:08:59,600
инвертированный индекс для поиска,

225
00:08:59,600 --> 00:09:00,800


226
00:09:00,800 --> 00:09:03,200
однако недостатком является то, что наш запрос

227
00:09:03,200 --> 00:09:05,680
снова становится набором слов, и мы теряем

228
00:09:05,680 --> 00:09:07,839
более глубокое понимание нашего  запросов,

229
00:09:07,839 --> 00:09:10,480
помимо этого,

230
00:09:11,040 --> 00:09:13,519
поэтому наш главный вопрос остается,

231
00:09:13,519 --> 00:09:16,800
можем ли мы совместно достичь высокого mrr и низкой

232
00:09:16,800 --> 00:09:19,120
вычислительной стоимости, и, как мы сказали ранее,

233
00:09:19,120 --> 00:09:20,800
ответ положительный,

234
00:09:20,800 --> 00:09:22,720
и для этого

235
00:09:22,720 --> 00:09:24,720
мы обсудим в следующем скринкасте две

236
00:09:24,720 --> 00:09:27,040
очень интересные парадигмы нейронных ir-

237
00:09:27,040 --> 00:09:31,800
моделей, которые получают  нам близко к размаху

