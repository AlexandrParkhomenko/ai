1
00:00:04,799 --> 00:00:06,480
приветствую всех, это пятая часть

2
00:00:06,480 --> 00:00:07,919
нашей серии статей о выводе естественного языка,

3
00:00:07,919 --> 00:00:08,880
мы собираемся поговорить о

4
00:00:08,880 --> 00:00:10,880
механизмах внимания, внимание было

5
00:00:10,880 --> 00:00:12,480
важным источником инноваций в

6
00:00:12,480 --> 00:00:14,559
литературе по НЛИ, и, конечно,

7
00:00:14,559 --> 00:00:16,960
с тех пор его известность только возросла.

8
00:00:16,960 --> 00:00:19,039
давайте начнем с некоторых руководящих идей.

9
00:00:19,039 --> 00:00:21,039
в контексте проблемы nli у нас может

10
00:00:21,039 --> 00:00:23,279
быть интуиция, что нам просто нужно больше

11
00:00:23,279 --> 00:00:24,560
связей для многих наших

12
00:00:24,560 --> 00:00:26,640
архитектур между предпосылкой и

13
00:00:26,640 --> 00:00:29,199
гипотезой, возможно, при

14
00:00:29,199 --> 00:00:31,279
обработке гипотезы нам просто нужна модель, чтобы

15
00:00:31,279 --> 00:00:33,360
иметь некоторые напоминания о том, что на

16
00:00:33,360 --> 00:00:35,760
самом деле содержала предпосылка  и любое

17
00:00:35,760 --> 00:00:37,520
сводное представление, которое у нас есть для этой

18
00:00:37,520 --> 00:00:39,760
предпосылки, может быть просто недостаточным

19
00:00:39,760 --> 00:00:41,440
с точки зрения обработки

20
00:00:41,440 --> 00:00:42,719
гипотезы и подачи

21
00:00:42,719 --> 00:00:46,160
представления в слой классификатора,

22
00:00:46,160 --> 00:00:48,399
соответственно, вы знаете, что

23
00:00:48,399 --> 00:00:50,160
в литературе по nli есть устойчивая интуиция, что

24
00:00:50,160 --> 00:00:52,559
полезно мягко выровнять предпосылку

25
00:00:52,559 --> 00:00:54,559
и гипотеза, чтобы найти соответствующие

26
00:00:54,559 --> 00:00:56,640
слова и фразы между этими двумя

27
00:00:56,640 --> 00:00:59,120
текстами  Это может быть трудно сделать

28
00:00:59,120 --> 00:01:00,879
на механическом уровне, но

29
00:01:00,879 --> 00:01:03,359
механизмы внимания могут позволить нам с помощью нашего

30
00:01:03,359 --> 00:01:05,680
процесса обучения, управляемого данными, найти

31
00:01:05,680 --> 00:01:07,439
мягкие связи в весах для

32
00:01:07,439 --> 00:01:09,280
этих слоев внимания между

33
00:01:09,280 --> 00:01:11,119
предпосылкой и гипотезой и достичь

34
00:01:11,119 --> 00:01:13,119
некоторых эффектов, которые мы бы  получить от

35
00:01:13,119 --> 00:01:16,159
реального процесса выравнивания,

36
00:01:16,159 --> 00:01:17,920
так что давайте начнем с глобального внимания,

37
00:01:17,920 --> 00:01:19,840
это простейший механизм внимания,

38
00:01:19,840 --> 00:01:21,680
который вы видите в литературе нои, но

39
00:01:21,680 --> 00:01:23,200
он уже достаточно мощный и, как

40
00:01:23,200 --> 00:01:24,880
вы увидите, имеет глубокие связи с

41
00:01:24,880 --> 00:01:26,640
механизмами внимания в

42
00:01:26,640 --> 00:01:27,920
преобразователе,

43
00:01:27,920 --> 00:01:29,600
поэтому  это конкретное давайте начнем

44
00:01:29,600 --> 00:01:31,920
с простого примера у нас есть каждая собака

45
00:01:31,920 --> 00:01:34,400
танцевала как наша предпосылка какой-то пудель танцевал

46
00:01:34,400 --> 00:01:36,320
как наша гипотеза и они вписываются

47
00:01:36,320 --> 00:01:38,640
в эту связанную модель rnn для

48
00:01:38,640 --> 00:01:39,680
nli

49
00:01:39,680 --> 00:01:41,520
теперь стандартно то, что мы сделали бы, это взяли

50
00:01:41,520 --> 00:01:44,000
это окончательное представление hc

51
00:01:44,000 --> 00:01:45,840
как резюме  представление

52
00:01:45,840 --> 00:01:47,840
всей последовательности и передать его непосредственно

53
00:01:47,840 --> 00:01:49,759
в классификатор.

54
00:01:49,759 --> 00:01:50,960
Что мы собираемся делать, когда добавим

55
00:01:50,960 --> 00:01:53,840
механизмы внимания, так это  вместо этого предложите

56
00:01:53,840 --> 00:01:56,159
некоторые соединения обратно из этого состояния

57
00:01:56,159 --> 00:01:58,240
в исходные состояния. Способ, которым этот

58
00:01:58,240 --> 00:02:00,240
процесс начинается, — это серия

59
00:02:00,240 --> 00:02:01,759
скалярных произведений, поэтому мы возьмем наш

60
00:02:01,759 --> 00:02:03,520
целевой вектор hc

61
00:02:03,520 --> 00:02:05,680
и возьмем его скалярное произведение с каждым

62
00:02:05,680 --> 00:02:07,439
из скрытых представлений,

63
00:02:07,439 --> 00:02:10,080
соответствующих  токенов в предпосылке,

64
00:02:10,080 --> 00:02:11,599
и это дает нам этот вектор

65
00:02:11,599 --> 00:02:13,360
ненормализованных оценок, просто точечные

66
00:02:13,360 --> 00:02:14,400
произведения,

67
00:02:14,400 --> 00:02:16,160
и обычно затем soft max

68
00:02:16,160 --> 00:02:17,680
нормализует эти оценки в наши

69
00:02:17,680 --> 00:02:19,840
веса внимания alpha.

70
00:02:19,840 --> 00:02:21,680
Что мы делаем с alpha, затем создаем наш

71
00:02:21,680 --> 00:02:23,599
вектор контекста, и способ, которым это происходит,

72
00:02:23,599 --> 00:02:24,959
таков:  мы собираемся получить взвешенное

73
00:02:24,959 --> 00:02:27,760
представление всех этих предпосылок, каждое

74
00:02:27,760 --> 00:02:30,560
из которых h1 h2 и h3 взвешивается

75
00:02:30,560 --> 00:02:32,319
соответствующим весом внимания, который

76
00:02:32,319 --> 00:02:34,560
фиксирует своего рода ненормализованное

77
00:02:34,560 --> 00:02:36,720
понятие сходства с нашим целевым

78
00:02:36,720 --> 00:02:38,239
вектором hc,

79
00:02:38,239 --> 00:02:39,599
а затем получить версию с фиксированной размерностью

80
00:02:39,599 --> 00:02:41,599
из которых мы берем среднее значение, или это

81
00:02:41,599 --> 00:02:43,440
может быть сумма всех этих

82
00:02:43,440 --> 00:02:45,920
взвешенных представлений о предпосылке,

83
00:02:45,920 --> 00:02:47,840
затем мы получаем наш слой комбинации внимания,

84
00:02:47,840 --> 00:02:49,440
и есть v  Есть несколько способов сделать

85
00:02:49,440 --> 00:02:51,440
это, один простой из них состоит в том, чтобы просто

86
00:02:51,440 --> 00:02:54,000
соединить наш вектор контекста с нашим исходным вектором контекста и

87
00:02:54,000 --> 00:02:57,040
вектором цели hc и

88
00:02:57,040 --> 00:02:58,959
передать их через некий плотный

89
00:02:58,959 --> 00:03:00,720
слой изученных параметров.

90
00:03:00,720 --> 00:03:02,400


91
00:03:02,400 --> 00:03:04,560


92
00:03:04,560 --> 00:03:06,959
вектор hc каждый имеет свой собственный

93
00:03:06,959 --> 00:03:08,720
вес и имеет аддитивную

94
00:03:08,720 --> 00:03:10,800
комбинацию этих двух и снова подает его через

95
00:03:10,800 --> 00:03:13,440
какую-то нелинейность,

96
00:03:13,440 --> 00:03:14,640
и вы можете придумать различные другие

97
00:03:14,640 --> 00:03:16,239
конструкции для этого, и это дает нам эту

98
00:03:16,239 --> 00:03:19,280
комбинацию внимания h тильда,

99
00:03:19,280 --> 00:03:21,840
а затем, наконец, классификатор  слой —

100
00:03:21,840 --> 00:03:23,840
это простой плотный слой, как и раньше,

101
00:03:23,840 --> 00:03:26,959
за исключением того, что вместо использования только hc мы теперь

102
00:03:26,959 --> 00:03:29,120
используем это представление с тильдой h, которое

103
00:03:29,120 --> 00:03:31,200
включает в себя как hc, так

104
00:03:31,200 --> 00:03:33,840
и взвешенную смесь

105
00:03:33,840 --> 00:03:36,720
предпосылочных состояний

106
00:03:36,879 --> 00:03:38,480
, может быть полезно пройти это здесь

107
00:03:38,480 --> 00:03:40,720
с некоторыми конкретными числовыми значениями,

108
00:03:40,720 --> 00:03:42,159
поэтому  то, что я сделал, это просто представьте, что

109
00:03:42,159 --> 00:03:44,080
у нас есть двумерные представления

110
00:03:44,080 --> 00:03:46,239
для всех этих векторов, и вы можете видеть,

111
00:03:46,239 --> 00:03:48,159
что я сделал здесь:  удостоверьтесь,

112
00:03:48,159 --> 00:03:51,040
что пропорционально все во многом похоже на

113
00:03:51,040 --> 00:03:53,439
это окончательное представление здесь,

114
00:03:53,439 --> 00:03:55,439
а затем это сходство

115
00:03:55,439 --> 00:03:56,879
уменьшается по мере того, как мы продвигаемся по исходным

116
00:03:56,879 --> 00:03:58,480
состояниям, и вы увидите, что произойдет, когда

117
00:03:58,480 --> 00:04:00,239
мы возьмем здесь скалярные произведения, поэтому

118
00:04:00,239 --> 00:04:02,239
первый шаг дает нам  ненормализованные

119
00:04:02,239 --> 00:04:04,000
оценки, и вы можете видеть, что наибольшее

120
00:04:04,000 --> 00:04:06,080
ненормализованное сходство наблюдается с

121
00:04:06,080 --> 00:04:08,319
первым токеном, за которым следует второй, а

122
00:04:08,319 --> 00:04:09,599
затем

123
00:04:09,599 --> 00:04:11,519
третий шаг нормализации softmax

124
00:04:11,519 --> 00:04:13,360
просто немного сглаживает эти точечные произведения,

125
00:04:13,360 --> 00:04:14,720
но мы получаем такое же

126
00:04:14,720 --> 00:04:18,959
пропорциональное ранжирование по отношению к hc

127
00:04:18,959 --> 00:04:20,880
вот этот вектор контекста, и вы можете

128
00:04:20,880 --> 00:04:23,040
видеть, что это просто среднее значение взвешенных

129
00:04:23,040 --> 00:04:25,199
значений всех этих векторов, которое

130
00:04:25,199 --> 00:04:26,479
дает нам k,

131
00:04:26,479 --> 00:04:28,320
и это k затем подается в этот

132
00:04:28,320 --> 00:04:30,000
комбинированный слой внимания, и вы можете

133
00:04:30,000 --> 00:04:31,840
видеть оранжевым цветом здесь это

134
00:04:31,840 --> 00:04:34,479
двухмерный вектор контекста  здесь у нас есть

135
00:04:34,479 --> 00:04:36,880
hc, только что точно повторенный, и затем

136
00:04:36,880 --> 00:04:39,680
эта матрица весов wk собирается

137
00:04:39,680 --> 00:04:41,199
дать нам в конце после этой

138
00:04:41,199 --> 00:04:42,479
нелинейности

139
00:04:42,479 --> 00:04:44,080
h тильду,

140
00:04:44,080 --> 00:04:46,639
а затем cl  assifier такой же, как и раньше,

141
00:04:46,639 --> 00:04:48,479
так что это простой проработанный пример того, как

142
00:04:48,479 --> 00:04:50,400
работают эти механизмы внимания, и

143
00:04:50,400 --> 00:04:52,240
идея состоит в том, что мы как бы

144
00:04:52,240 --> 00:04:53,600
фундаментально

145
00:04:53,600 --> 00:04:56,960
взвешиваем это целевое представление hc

146
00:04:56,960 --> 00:04:59,040
по его сходству

147
00:04:59,040 --> 00:05:00,960
с предыдущими предпосылками, но все

148
00:05:00,960 --> 00:05:02,800
они смешаны, и влияние

149
00:05:02,800 --> 00:05:04,080
пропорционально этому

150
00:05:04,080 --> 00:05:07,120
ненормализованному сходству,

151
00:05:07,120 --> 00:05:08,560
есть и другие функции подсчета очков, которые

152
00:05:08,560 --> 00:05:10,160
вы могли бы использовать.

153
00:05:10,160 --> 00:05:12,320


154
00:05:12,320 --> 00:05:14,320


155
00:05:14,320 --> 00:05:15,199


156
00:05:15,199 --> 00:05:16,800


157
00:05:16,800 --> 00:05:18,639
билинейная

158
00:05:18,639 --> 00:05:20,800
форма, и это просто конкатенация

159
00:05:20,800 --> 00:05:22,880
этих двух состояний, подаваемых через эти третьи

160
00:05:22,880 --> 00:05:24,639
веса, и как только вы увидите такое

161
00:05:24,639 --> 00:05:26,720
пространство проектирования, вы сможете представить очень

162
00:05:26,720 --> 00:05:28,479
много других способов, которыми вы могли бы смешивать

163
00:05:28,479 --> 00:05:30,479
параметры и иметь разные представления об

164
00:05:30,479 --> 00:05:34,080
этом глобальном внимании.  механизм

165
00:05:34,080 --> 00:05:35,919
мы могли бы сделать еще один шаг здесь это

166
00:05:35,919 --> 00:05:38,160
было глобальное внимание слово за словом

167
00:05:38,160 --> 00:05:39,680
внимание мы собираемся узнать намного больше

168
00:05:39,680 --> 00:05:41,600
ed параметры и гораздо больше

169
00:05:41,600 --> 00:05:44,320
связей между гипотезой обратно

170
00:05:44,320 --> 00:05:45,840
в предпосылку,

171
00:05:45,840 --> 00:05:47,600
поэтому, чтобы сделать этот вид податливым, я

172
00:05:47,600 --> 00:05:49,680
выбрал одно довольно простое представление о том, как

173
00:05:49,680 --> 00:05:51,520
это могло бы работать, и способ, которым мы должны

174
00:05:51,520 --> 00:05:54,240
отслеживать эти вычисления, заключается в том, чтобы сосредоточиться на

175
00:05:54,240 --> 00:05:56,560
этом векторе b здесь, потому что мы  re собираемся

176
00:05:56,560 --> 00:05:58,560
двигаться во времени, но давайте представим,

177
00:05:58,560 --> 00:06:00,639
что мы уже обработали состояние a

178
00:06:00,639 --> 00:06:02,560
и впоследствии обработаем состояние c,

179
00:06:02,560 --> 00:06:05,280
чтобы мы сосредоточились на b,

180
00:06:05,280 --> 00:06:06,639
и способ, которым мы устанавливаем эти

181
00:06:06,639 --> 00:06:08,639
связи, заключается в том,

182
00:06:08,639 --> 00:06:11,039
что мы берем предыдущий вектор контекста, который мы создали

183
00:06:11,039 --> 00:06:12,000
вот так,

184
00:06:12,000 --> 00:06:13,840
мы собираемся умножить это на повторяющиеся

185
00:06:13,840 --> 00:06:15,759
копии состояния b, и это просто для

186
00:06:15,759 --> 00:06:18,160
того, чтобы мы получили ту же размерность

187
00:06:18,160 --> 00:06:19,919
, что и в предпосылке здесь,

188
00:06:19,919 --> 00:06:21,520
где я просто скопировал в

189
00:06:21,520 --> 00:06:24,080
матрицу все три из этих состояний

190
00:06:24,080 --> 00:06:26,319
и здесь у нас есть матрица параметров обучения

191
00:06:26,319 --> 00:06:28,479
и аддитивная комбинация этих

192
00:06:28,479 --> 00:06:30,720
двух параметров, за которой следует нелинейность, которая

193
00:06:30,720 --> 00:06:32,479
даст нам вот это m, которое

194
00:06:32,479 --> 00:06:34,479
соответствует весам внимания

195
00:06:34,479 --> 00:06:36,000
в предыдущем  Механизмы глобального внимания

196
00:06:36,000 --> 00:06:37,759
, которые

197
00:06:37,759 --> 00:06:39,520
мы собираемся мягко максимизировать, нормализуют их,

198
00:06:39,520 --> 00:06:41,199
и это буквально дает нам веса,

199
00:06:41,199 --> 00:06:42,319
и вы можете видеть, что здесь есть некоторые

200
00:06:42,319 --> 00:06:44,800
дополнительные параметры для

201
00:06:44,800 --> 00:06:47,199
создания правильных размерностей,

202
00:06:47,199 --> 00:06:48,960
и, наконец, у нас есть контекст в точке

203
00:06:48,960 --> 00:06:50,880
b, так что это будет  повторный

204
00:06:50,880 --> 00:06:52,800
просмотр всех этих предпосылок, взвешенных нашим

205
00:06:52,800 --> 00:06:55,280
вектором контекста, как и раньше, а затем переданных

206
00:06:55,280 --> 00:06:57,520
через некоторые дополнительные параметры wa

207
00:06:57,520 --> 00:06:58,479
здесь,

208
00:06:58,479 --> 00:07:00,400
и это дает нам, как вы можете видеть здесь,

209
00:07:00,400 --> 00:07:02,960
представление контекста для состояния

210
00:07:02,960 --> 00:07:03,759
b,

211
00:07:03,759 --> 00:07:05,919
когда мы перейдем к состоянию c, конечно, которое

212
00:07:05,919 --> 00:07:08,319
будет  используется вместо здесь и

213
00:07:08,319 --> 00:07:10,240
с будет использоваться для всех этих фиолетовых значений,

214
00:07:10,240 --> 00:07:11,840
и вычисление будет продолжаться, как и

215
00:07:11,840 --> 00:07:14,000
раньше, и таким образом, поскольку у нас есть

216
00:07:14,000 --> 00:07:15,919
все эти дополнительные параметры обучения,

217
00:07:15,919 --> 00:07:17,520
мы можем осмысленно перемещаться по

218
00:07:17,520 --> 00:07:18,960
всей последовательности,

219
00:07:18,960 --> 00:07:20,800
обновляя наши параметры и обучая

220
00:07:20,800 --> 00:07:22,960
связи от каждого маркера гипотезы

221
00:07:22,960 --> 00:07:25,360
возвращаются в предпосылку, поэтому это намного

222
00:07:25,360 --> 00:07:26,960
мощнее, чем предыдущее представление, где у нас

223
00:07:26,960 --> 00:07:29,680
было относительно мало изученных параметров.  в

224
00:07:29,680 --> 00:07:31,759
наших механизмах внимания, и поэтому

225
00:07:31,759 --> 00:07:33,520
мы можем действительно осмысленно связать

226
00:07:33,520 --> 00:07:35,520
это только с состоянием, которое мы собираемся

227
00:07:35,520 --> 00:07:37,680
передать в классификатор, так что это намного

228
00:07:37,680 --> 00:07:40,160
более выразительно,

229
00:07:40,160 --> 00:07:41,599
и затем, когда мы выполнили

230
00:07:41,599 --> 00:07:44,400
всю обработку последовательности, наконец, мы

231
00:07:44,400 --> 00:07:46,639
получаем представление  для c здесь, как подается

232
00:07:46,639 --> 00:07:48,319
через эти механизмы, и это

233
00:07:48,319 --> 00:07:50,319
становится входными данными для классификатора, который

234
00:07:50,319 --> 00:07:53,199
мы в конечном итоге используем,

235
00:07:53,280 --> 00:07:54,879
связь с преобразователем

236
00:07:54,879 --> 00:07:56,080
должна быть очевидной, это

237
00:07:56,080 --> 00:07:57,919
вернет нас обратно к глобальному

238
00:07:57,919 --> 00:07:59,520
механизму внимания, вспомните, что для

239
00:07:59,520 --> 00:08:02,319
преобразователя у нас есть эти последовательности

240
00:08:02,319 --> 00:08:04,479
токены с их позиционными кодировками,

241
00:08:04,479 --> 00:08:06,560
которые дают нам вложение здесь, и в

242
00:08:06,560 --> 00:08:08,639
этот момент мы устанавливаем множество соединений скалярного

243
00:08:08,639 --> 00:08:10,720
произведения, и я показал вам

244
00:08:10,720 --> 00:08:12,400
в лекции о преобразователе, что

245
00:08:12,400 --> 00:08:14,879
механизмы здесь идентичны

246
00:08:14,879 --> 00:08:16,720
механизмам, которые мы использовали для внимания скалярного произведения,

247
00:08:16,720 --> 00:08:18,319
это  просто в

248
00:08:18,319 --> 00:08:20,560
контексте преобразователя мы делаем это из каждого

249
00:08:20,560 --> 00:08:23,919
состояния в любое другое состояние,

250
00:08:25,599 --> 00:08:26,960
а затем, конечно, c  вычисления

251
00:08:26,960 --> 00:08:29,120
проходят через последовательные шаги на

252
00:08:29,120 --> 00:08:31,039
уровне преобразователя и, возможно, через

253
00:08:31,039 --> 00:08:34,958
несколько слоев преобразователя,

254
00:08:34,958 --> 00:08:36,399
и есть несколько других вариантов,

255
00:08:36,399 --> 00:08:38,080
это только начало очень

256
00:08:38,080 --> 00:08:39,679
большого пространства для проектирования

257
00:08:39,679 --> 00:08:41,679
механизмов внимания, позвольте мне упомянуть некоторые из них, на которые мы

258
00:08:41,679 --> 00:08:43,279
могли бы обратить внимание на местном уровне.  на

259
00:08:43,279 --> 00:08:44,880
самом деле был ранним вкладом в

260
00:08:44,880 --> 00:08:47,120
контекст машинного перевода, и это

261
00:08:47,120 --> 00:08:49,040
должно было установить связь между выбранными

262
00:08:49,040 --> 00:08:51,200
точками в посылке и гипотезе,

263
00:08:51,200 --> 00:08:53,680
основанной на некотором, возможно, априорном представлении, которое

264
00:08:53,680 --> 00:08:55,760
у нас есть, о том, какие вещи, вероятно, будут

265
00:08:55,760 --> 00:08:58,000
важны для нашей проблемы.

266
00:08:58,000 --> 00:08:59,680
Я сказал, что

267
00:08:59,680 --> 00:09:01,680
можно настроить разными способами с большим количеством

268
00:09:01,680 --> 00:09:04,080
изученных параметров, и классическая статья

269
00:09:04,080 --> 00:09:05,519
— это та, которую я рекомендую для

270
00:09:05,519 --> 00:09:07,920
чтения для этого модуля reptessio вообще,

271
00:09:07,920 --> 00:09:10,080
где они делают действительно новаторский

272
00:09:10,080 --> 00:09:11,120
взгляд на это

273
00:09:11,120 --> 00:09:13,120
и используют еще более сложное внимание

274
00:09:13,120 --> 00:09:14,959
механизмы, чем я представил под слово

275
00:09:14,959 --> 00:09:17,040
за словом внимание, но следуя

276
00:09:17,040 --> 00:09:20,240
той же интуиции, я бы сказал,

277
00:09:20,240 --> 00:09:22,160
что в  репрезентация внимания в момент времени t

278
00:09:22,160 --> 00:09:23,440
может быть добавлена к скрытому

279
00:09:23,440 --> 00:09:25,519
представлению в момент времени t плюс один, это

280
00:09:25,519 --> 00:09:27,120
дало бы нам другой способ

281
00:09:27,120 --> 00:09:29,360
последовательного перемещения по последовательности,

282
00:09:29,360 --> 00:09:30,959
уделяя осмысленное внимание каждой из

283
00:09:30,959 --> 00:09:33,040
этих точек, в отличие от глобального

284
00:09:33,040 --> 00:09:34,720
внимания, которое было бы просто для  это

285
00:09:34,720 --> 00:09:36,399
конечное состояние,

286
00:09:36,399 --> 00:09:37,519
а затем есть другие связи

287
00:09:37,519 --> 00:09:39,279
еще дальше, например,

288
00:09:39,279 --> 00:09:41,120
сети памяти могут использоваться для решения

289
00:09:41,120 --> 00:09:42,800
аналогичных проблем, и

290
00:09:42,800 --> 00:09:45,200
за ними стоят аналогичные интуиции в

291
00:09:45,200 --> 00:09:47,760
качестве механизмов внимания применительно к проблеме nli,

292
00:09:47,760 --> 00:09:49,200
и это как бы более явно

293
00:09:49,200 --> 00:09:51,360
опирается на это  идея о том, что на

294
00:09:51,360 --> 00:09:53,600
поздних этапах обработки нам может понадобиться

295
00:09:53,600 --> 00:09:55,200
небольшое напоминание о том, что было в

296
00:09:55,200 --> 00:09:59,560
предыдущем контексте, который мы обрабатывали

