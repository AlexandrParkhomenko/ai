1
00:00:05,680 --> 00:00:07,200
приветствую всех, это третья часть

2
00:00:07,200 --> 00:00:08,880
нашей серии статей о методах и метриках, мы

3
00:00:08,880 --> 00:00:10,400
собираемся поговорить о метриках для

4
00:00:10,400 --> 00:00:12,160
оценки систем генерации естественного языка,

5
00:00:12,160 --> 00:00:14,240
мы ранее говорили о

6
00:00:14,240 --> 00:00:16,079
метриках классификатора, и вопросы кажутся

7
00:00:16,079 --> 00:00:18,720
относительно простыми, поскольку вы увидите, что

8
00:00:18,720 --> 00:00:20,640
оценка для энергетических систем

9
00:00:20,640 --> 00:00:22,800
значительно сложнее, давайте

10
00:00:22,800 --> 00:00:24,480
начнем с этих фундаментальных

11
00:00:24,480 --> 00:00:26,320
проблем, возможно, самая фундаментальная из

12
00:00:26,320 --> 00:00:28,400
всех заключается в том, что в естественном языке существует

13
00:00:28,400 --> 00:00:30,400
более одного эффективного способа сказать большинство

14
00:00:30,400 --> 00:00:32,719
вещей. Наборы данных, которые у нас есть, могут иметь

15
00:00:32,719 --> 00:00:34,640
один или несколько хороших примеров того, как

16
00:00:34,640 --> 00:00:36,239
что-то должно быть.

17
00:00:36,239 --> 00:00:38,000
но это всего лишь пример многих

18
00:00:38,000 --> 00:00:39,600
способов, которыми мы могли бы эффективно общаться,

19
00:00:39,600 --> 00:00:41,440
и это оставляет нас с

20
00:00:41,440 --> 00:00:43,280
фундаментальными открытыми вопросами о том, какие

21
00:00:43,280 --> 00:00:45,360
сравнения мы должны делать и как мы

22
00:00:45,360 --> 00:00:48,239
должны оценивать так называемые ошибки.

23
00:00:48,239 --> 00:00:50,079


24
00:00:50,079 --> 00:00:51,920
на самом деле пытаясь измерить,

25
00:00:51,920 --> 00:00:53,440
является ли это беглостью

26
00:00:53,440 --> 00:00:55,600
, правдивостью, коммуникативной

27
00:00:55,600 --> 00:00:58,079
эффективностью или какой-то смесью

28
00:00:58,079 --> 00:00:59,920
Когда мы думаем о различных показателях, мы

29
00:00:59,920 --> 00:01:02,239
можем обнаружить, что они охватывают один или

30
00:01:02,239 --> 00:01:04,400
несколько из них и полностью игнорируют

31
00:01:04,400 --> 00:01:06,320
другие, и это обязательно сформирует

32
00:01:06,320 --> 00:01:08,159
траекторию нашего проекта и фактические

33
00:01:08,159 --> 00:01:09,680
цели, которых мы достигаем, поэтому мы должны быть

34
00:01:09,680 --> 00:01:11,600
действительно обдуманы.  мы на

35
00:01:11,600 --> 00:01:15,200
самом деле пытаемся измерить в пространстве,

36
00:01:15,200 --> 00:01:16,960
давайте начнем с недоумения, я бы сказал,

37
00:01:16,960 --> 00:01:19,280
что недоумение имеет значение, так это то,

38
00:01:19,280 --> 00:01:21,439
что оно, по крайней мере, очень тесно связано

39
00:01:21,439 --> 00:01:23,759
со структурой многих моделей, с которыми

40
00:01:23,759 --> 00:01:25,680
мы работаем в nlg,

41
00:01:25,680 --> 00:01:27,759
так что ядро  Расчет

42
00:01:27,759 --> 00:01:30,640
заключается в том, что для некоторой последовательности x длины

43
00:01:30,640 --> 00:01:31,360
n

44
00:01:31,360 --> 00:01:33,759
и распределения вероятностей

45
00:01:33,759 --> 00:01:35,759
p недоумение x по отношению к этому

46
00:01:35,759 --> 00:01:37,759
распределению p является произведением,

47
00:01:37,759 --> 00:01:38,960
обратным всем заданным

48
00:01:38,960 --> 00:01:40,880
вероятностям, а затем мы берем

49
00:01:40,880 --> 00:01:42,880
среднее здесь есть много способов

50
00:01:42,880 --> 00:01:44,960
выразить это вычисление  и многие

51
00:01:44,960 --> 00:01:46,799
способы связи с информационными теоретическими

52
00:01:46,799 --> 00:01:49,200
измерениями позволяют мне отложить эти вопросы

53
00:01:49,200 --> 00:01:50,799
всего на секунду, и я попытаюсь

54
00:01:50,799 --> 00:01:53,360
развить интуицию сразу после

55
00:01:53,360 --> 00:01:55,280
прохождения основного расчета.  Так что это

56
00:01:55,280 --> 00:01:57,200
недоумение, а затем, когда мы делаем

57
00:01:57,200 --> 00:01:59,280
правильное недоумение на уровне токенов, мы хотим назначить

58
00:01:59,280 --> 00:02:01,600
недоумение отдельным примерам, которые нам

59
00:02:01,600 --> 00:02:03,439
нужно нормализовать по длине этих

60
00:02:03,439 --> 00:02:06,079
примеров, и мы делаем это в логарифмическом пространстве

61
00:02:06,079 --> 00:02:08,080
, чтобы зафиксировать своего рода среднее геометрическое,

62
00:02:08,080 --> 00:02:10,399
которое, возможно,  больше подходит

63
00:02:10,399 --> 00:02:13,280
для сравнения значений вероятности,

64
00:02:13,280 --> 00:02:14,959
а затем, если мы хотим

65
00:02:14,959 --> 00:02:17,280
сложить недоумение для всего корпуса, мы снова

66
00:02:17,280 --> 00:02:20,080
используем среднее геометрическое всех

67
00:02:20,080 --> 00:02:23,920
прогнозов недоумения на уровне токенов,

68
00:02:23,920 --> 00:02:25,680
и это дает нам единую величину

69
00:02:25,680 --> 00:02:28,879
для всей серии примеров.

70
00:02:28,879 --> 00:02:30,480
каковы свойства  недоумение

71
00:02:30,480 --> 00:02:32,560
хорошо, его границы от одного до бесконечности с

72
00:02:32,560 --> 00:02:33,840
одним лучшим, поэтому мы хотели бы

73
00:02:33,840 --> 00:02:34,840
минимизировать

74
00:02:34,840 --> 00:02:37,120
недоумение, это эквивалентно

75
00:02:37,120 --> 00:02:39,360
возведению в степень потери перекрестной энтропии

76
00:02:39,360 --> 00:02:41,680
, это тесная связь с моделями,

77
00:02:41,680 --> 00:02:44,080
которые я хотел вызвать, мы часто работаем

78
00:02:44,080 --> 00:02:45,599
с языковыми моделями, которые используют  через

79
00:02:45,599 --> 00:02:47,519
потерю энтропии, и вы можете видеть, что

80
00:02:47,519 --> 00:02:49,599
они напрямую оптимизируют количество

81
00:02:49,599 --> 00:02:51,840
, пропорциональное недоумению, и

82
00:02:51,840 --> 00:02:53,360
что  может быть полезно как своего рода

83
00:02:53,360 --> 00:02:55,760
получение прямого представления о характере

84
00:02:55,760 --> 00:02:57,840
предсказаний вашей модели,

85
00:02:57,840 --> 00:02:59,599
какое значение она хорошо кодирует, я думаю,

86
00:02:59,599 --> 00:03:01,519
что это просто, присваивает ли модель высокую

87
00:03:01,519 --> 00:03:04,000
вероятность входным последовательностям, то

88
00:03:04,000 --> 00:03:06,159
есть присваивает ли она низкую степень недоумения

89
00:03:06,159 --> 00:03:08,239
входным последовательностям?

90
00:03:08,239 --> 00:03:10,640
слабых мест на самом деле много, во-

91
00:03:10,640 --> 00:03:12,319
первых, это сильно зависит от

92
00:03:12,319 --> 00:03:14,480
основного словаря, чтобы увидеть, что

93
00:03:14,480 --> 00:03:16,560
представьте крайний случай, когда мы берем каждое

94
00:03:16,560 --> 00:03:18,800
слово в словаре и сопоставляем его с

95
00:03:18,800 --> 00:03:21,200
одним токеном unk, в этом случае мы

96
00:03:21,200 --> 00:03:23,440
абсолютно минимизируем недоумение, но наша

97
00:03:23,440 --> 00:03:26,000
система будет бесполезна.  в этом пограничном случае

98
00:03:26,000 --> 00:03:28,239
вы можете видеть, что я мог бы уменьшить

99
00:03:28,239 --> 00:03:30,879
недоумение, просто изменив

100
00:03:30,879 --> 00:03:32,799
размер своего словаря, таким образом вы

101
00:03:32,799 --> 00:03:34,319
могли бы получить эту метрику

102
00:03:34,319 --> 00:03:36,000
непреднамеренно

103
00:03:36,000 --> 00:03:37,519
, потому что мы не можем на самом

104
00:03:37,519 --> 00:03:39,360
деле сравнивать наборы данных,

105
00:03:39,360 --> 00:03:40,400
потому что, конечно  у них может быть

106
00:03:40,400 --> 00:03:42,480
разная лексика и разные

107
00:03:42,480 --> 00:03:45,120
внутренние понятия недоумения,

108
00:03:45,120 --> 00:03:46,720
а также сложно проводить

109
00:03:46,720 --> 00:03:48,720
сравнения между моделями, которые вы можете  видите,

110
00:03:48,720 --> 00:03:51,599
что в моей первой слабости есть то, что

111
00:03:51,599 --> 00:03:53,840
если мы сравним модели, нам нужно

112
00:03:53,840 --> 00:03:55,920
исправить набор данных и убедиться, что

113
00:03:55,920 --> 00:03:58,000
различия между моделями по

114
00:03:58,000 --> 00:04:00,000
своей сути не формируют диапазон

115
00:04:00,000 --> 00:04:01,840
значений недоумения, которые мы, вероятно,

116
00:04:01,840 --> 00:04:03,280
увидим,

117
00:04:03,280 --> 00:04:05,120
давайте теперь перейдем к  семейство того, что

118
00:04:05,120 --> 00:04:06,720
вы могли бы представить как основанные на n-граммах

119
00:04:06,720 --> 00:04:08,879
методы для оценки систем nlg,

120
00:04:08,879 --> 00:04:11,360
начиная с частоты ошибок в словах,

121
00:04:11,360 --> 00:04:13,200
поэтому фундаментальной вещью здесь будет

122
00:04:13,200 --> 00:04:15,680
мера расстояния редактирования,

123
00:04:15,680 --> 00:04:17,358
и поэтому вы можете рассматривать частоту ошибок в словах

124
00:04:17,358 --> 00:04:19,120
как своего рода семейство мер,

125
00:04:19,120 --> 00:04:20,798
зависящих от  при выборе функции расстояния редактирования,

126
00:04:20,798 --> 00:04:22,320
которую мы просто

127
00:04:22,320 --> 00:04:24,160
вставляем,

128
00:04:24,160 --> 00:04:26,560
частота ошибок в словах — это расстояние

129
00:04:26,560 --> 00:04:28,880
между фактической последовательностью x и некоторым

130
00:04:28,880 --> 00:04:31,520
прогнозируемым разбросом последовательности, нормализованным

131
00:04:31,520 --> 00:04:33,840
по длине фактической последовательности,

132
00:04:33,840 --> 00:04:35,360
и если мы хотим, чтобы частота ошибок в словах

133
00:04:35,360 --> 00:04:37,600
для  всего корпуса, его легко масштабировать

134
00:04:37,600 --> 00:04:39,680
, но здесь есть одна особенность

135
00:04:39,680 --> 00:04:41,600
: стандартный способ расчета заключается в том, что

136
00:04:41,600 --> 00:04:42,400
числитель

137
00:04:42,400 --> 00:04:44,639
представляет собой сумму всех расстояний

138
00:04:44,639 --> 00:04:47,120
между фактическим и предиктивным  ted последовательности

139
00:04:47,120 --> 00:04:49,199
не нормализованы, как это было здесь для

140
00:04:49,199 --> 00:04:51,199
частоты ошибок в словах нормализация, которая

141
00:04:51,199 --> 00:04:52,960
происходит по всему корпусу, это

142
00:04:52,960 --> 00:04:55,040
сумма всех длин фактических

143
00:04:55,040 --> 00:04:57,120
строк в корпусе, поэтому у нас есть одно

144
00:04:57,120 --> 00:04:58,960
среднее значение, а не среднее значение

145
00:04:58,960 --> 00:05:01,199
средних значений.

146
00:05:01,199 --> 00:05:02,880
свойства частоты ошибок слова,

147
00:05:02,880 --> 00:05:04,639
его границы от нуля до бесконечности, и мы

148
00:05:04,639 --> 00:05:06,479
хотели бы минимизировать его, чтобы ноль был

149
00:05:06,479 --> 00:05:07,600
лучшим,

150
00:05:07,600 --> 00:05:10,000
закодированное значение похоже на f баллов,

151
00:05:10,000 --> 00:05:11,919
мы хотели бы ответить на вопрос,

152
00:05:11,919 --> 00:05:14,479
как строка является предсказанной последовательностью

153
00:05:14,479 --> 00:05:16,400
с фактической последовательностью  и я задействовал здесь баллы f,

154
00:05:16,400 --> 00:05:18,800
потому что, если наша мера расстояния редактирования

155
00:05:18,800 --> 00:05:20,560
имеет понятия вставки и

156
00:05:20,560 --> 00:05:22,479
удаления, они играют роли,

157
00:05:22,479 --> 00:05:25,759
аналогичные точности, и

158
00:05:25,759 --> 00:05:28,000
хорошо запоминают слабые места, во-первых, у нас есть только

159
00:05:28,000 --> 00:05:30,560
один справочный текст, который я назвал

160
00:05:30,560 --> 00:05:32,639
ранее, что часто их много  хорошие

161
00:05:32,639 --> 00:05:34,400
способы сказать что-то, в то время как здесь мы

162
00:05:34,400 --> 00:05:36,639
можем сделать только одно сравнение,

163
00:05:36,639 --> 00:05:38,080
а также, возможно, это более

164
00:05:38,080 --> 00:05:40,000
фундаментальное слово airid - очень

165
00:05:40,000 --> 00:05:42,320
синтаксическое понятие, просто рассмотрите c  сравнивая

166
00:05:42,320 --> 00:05:44,960
текст, как будто это было хорошо, это было не хорошо,

167
00:05:44,960 --> 00:05:47,039
и это было здорово, они, вероятно, будут

168
00:05:47,039 --> 00:05:49,039
иметь одинаковые коэффициенты ошибок в словах,

169
00:05:49,039 --> 00:05:50,880
хотя первые два сильно различаются

170
00:05:50,880 --> 00:05:52,720
по своим значениям, а

171
00:05:52,720 --> 00:05:54,240
первый и третий на самом деле довольно

172
00:05:54,240 --> 00:05:56,560
похожи по своим значениям, что  семантическое

173
00:05:56,560 --> 00:05:58,639
понятие сходства вряд ли будет

174
00:05:58,639 --> 00:06:02,560
отражено в частоте ошибок в словах,

175
00:06:02,560 --> 00:06:04,240
давайте теперь перейдем к синим баллам, это еще

176
00:06:04,240 --> 00:06:05,199
одна

177
00:06:05,199 --> 00:06:06,880
основанная метрика, но она попытается

178
00:06:06,880 --> 00:06:08,479
учесть тот факт, что мы хотим сделать

179
00:06:08,479 --> 00:06:10,960
сравнения с несколькими

180
00:06:10,960 --> 00:06:14,080
созданными людьми справочными текстами, у

181
00:06:14,080 --> 00:06:15,840
нее есть понятие  точности в нем, но

182
00:06:15,840 --> 00:06:17,759
это называется точностью модифицированной инграммы,

183
00:06:17,759 --> 00:06:19,440
позвольте мне показать вам пример, и,

184
00:06:19,440 --> 00:06:21,680
надеюсь, это мотивирует его представить,

185
00:06:21,680 --> 00:06:23,520
что у нас есть кандидат, в котором есть только семь

186
00:06:23,520 --> 00:06:25,680
экземпляров слова «мысль», у

187
00:06:25,680 --> 00:06:27,680
нас есть два справочных текста, предположительно

188
00:06:27,680 --> 00:06:29,919
написанных людьми кошка  находится на коврике,

189
00:06:29,919 --> 00:06:32,400
и на коврике есть кошка

190
00:06:32,400 --> 00:06:34,400
модифицированная точность принимает для

191
00:06:34,400 --> 00:06:37,120
токена максимальное количество раз,

192
00:06:37,120 --> 00:06:39,280
которое встречается в любом референсе  ce текст,

193
00:06:39,280 --> 00:06:42,000
и это два эталона, один здесь,

194
00:06:42,000 --> 00:06:43,680
и он делит это на количество

195
00:06:43,680 --> 00:06:45,520
раз, которое появляется в кандидате,

196
00:06:45,520 --> 00:06:47,840
которое равно семи, что даст нам 2 из

197
00:06:47,840 --> 00:06:50,479
7 в качестве модифицированной инграммы.

198
00:06:50,479 --> 00:06:53,919
Оценка точности в один грамм для этого

199
00:06:53,919 --> 00:06:55,759
кандидата, а также штраф за краткость.  который

200
00:06:55,759 --> 00:06:57,280
будет играть роль чего-то вроде

201
00:06:57,280 --> 00:06:59,840
отзыва в синей оценке, поэтому у нас есть

202
00:06:59,840 --> 00:07:01,759
величина r, которая является суммой всех

203
00:07:01,759 --> 00:07:03,599
минимальных абсолютных различий длины

204
00:07:03,599 --> 00:07:05,840
между кандидатами и ссылкой,

205
00:07:05,840 --> 00:07:07,680
у нас есть c, которая является общей длиной

206
00:07:07,680 --> 00:07:09,840
всех кандидатов, и затем мы сказали  что

207
00:07:09,840 --> 00:07:12,080
штраф за краткость равен 1, если c больше,

208
00:07:12,080 --> 00:07:14,240
чем r, в противном случае это экспоненциальное

209
00:07:14,240 --> 00:07:18,080
убывание отношения r и c,

210
00:07:18,080 --> 00:07:19,440
и снова это будет играть своего рода

211
00:07:19,440 --> 00:07:21,599
понятие отзыва, и тогда синяя оценка

212
00:07:21,599 --> 00:07:23,840
является просто продуктом этого

213
00:07:23,840 --> 00:07:24,880
штрафа за краткость

214
00:07:24,880 --> 00:07:27,120
с  сумма взвешенных модифицированных

215
00:07:27,120 --> 00:07:29,599
значений точности n-грамм для каждого

216
00:07:29,599 --> 00:07:32,479
значения инграммы n, поэтому мы, вероятно, пройдем от

217
00:07:32,479 --> 00:07:34,319
одного до четырех, это стандартный набор

218
00:07:34,319 --> 00:07:36,720
из n граммов для рассмотрения, мы суммируем

219
00:07:36,720 --> 00:07:39,039
все эти  понятия модифицированной точности n-грамм

220
00:07:39,039 --> 00:07:40,720
для каждого n

221
00:07:40,720 --> 00:07:42,319
и, возможно, их взвешивание по-разному в

222
00:07:42,319 --> 00:07:44,479
зависимости от того, как мы хотим оценить один

223
00:07:44,479 --> 00:07:46,319
грамм, два грамма, три грамма и четыре

224
00:07:46,319 --> 00:07:47,840
грамма,

225
00:07:47,840 --> 00:07:49,440
так что это синий балл, каковы его

226
00:07:49,440 --> 00:07:51,599
свойства, его границы равны нулю, единице

227
00:07:51,599 --> 00:07:53,440
и единице.  лучший, но мы действительно

228
00:07:53,440 --> 00:07:55,199
не ожидаем, что какая-либо система на

229
00:07:55,199 --> 00:07:57,440
самом деле достигнет этого, потому что даже

230
00:07:57,440 --> 00:07:59,680
сравнения между человеческими переводами или

231
00:07:59,680 --> 00:08:02,240
созданными людьми текстами не будут иметь синего

232
00:08:02,240 --> 00:08:03,840
балла,

233
00:08:03,840 --> 00:08:06,560
равного единице, закодированное значение представляет собой соответствующий

234
00:08:06,560 --> 00:08:09,840
баланс модифицированной точности и отзыва

235
00:08:09,840 --> 00:08:12,720
под видом этого  Штраф за краткость

236
00:08:12,720 --> 00:08:14,800


237
00:08:14,800 --> 00:08:16,160
в этом смысле очень похож на процент ошибок в слове, но он стремится

238
00:08:16,160 --> 00:08:17,440
учесть тот факт, что

239
00:08:17,440 --> 00:08:20,000
обычно существует несколько подходящих выходных данных

240
00:08:20,000 --> 00:08:22,080
для данного ввода, и это реальная

241
00:08:22,080 --> 00:08:24,000
сила синего в

242
00:08:24,000 --> 00:08:26,479
оценке слабых сторон. Эта команда утверждала,

243
00:08:26,479 --> 00:08:27,919
что синий оценивает только  не удалось

244
00:08:27,919 --> 00:08:30,720
коррелировать с человеческими баллами за

245
00:08:30,720 --> 00:08:32,640
переводы, и это немного беспокоит,

246
00:08:32,640 --> 00:08:34,240
потому что синие баллы изначально были

247
00:08:34,240 --> 00:08:36,479
мотивированы в  Контекст машинного

248
00:08:36,479 --> 00:08:38,559
перевода и проблемы, которые они

249
00:08:38,559 --> 00:08:40,799
выявляют, как будто он очень чувствителен к

250
00:08:40,799 --> 00:08:42,880
порядку инграмм в отличие от человеческой

251
00:08:42,880 --> 00:08:44,399
интуиции

252
00:08:44,399 --> 00:08:46,640
и нечувствителен к типу

253
00:08:46,640 --> 00:08:48,720
инграмм, поэтому снова просто рассмотрите

254
00:08:48,720 --> 00:08:51,519
сравнения, такие как эта собака с собакой и

255
00:08:51,519 --> 00:08:54,000
этот тостер, которые будут  вероятно, у них очень

256
00:08:54,000 --> 00:08:56,640
похожие баллы по синему, но эта собака и

257
00:08:56,640 --> 00:08:58,399
та собака по своей природе гораздо более

258
00:08:58,399 --> 00:09:00,720
похожи, чем та собака и этот тостер,

259
00:09:00,720 --> 00:09:02,880
и в силу того факта, что это и

260
00:09:02,880 --> 00:09:04,160
просто разница на уровне

261
00:09:04,160 --> 00:09:06,399
функционального словаря по сравнению с собакой и

262
00:09:06,399 --> 00:09:09,760
тостером действительно  изменения содержания,

263
00:09:09,760 --> 00:09:11,600
а затем, когда мы переходим к темам, которые

264
00:09:11,600 --> 00:09:14,320
более тесно связаны с nlu, у нас,

265
00:09:14,320 --> 00:09:16,240
возможно, появляется еще более тревожная

266
00:09:16,240 --> 00:09:18,720
картина, поэтому эта команда утверждает, что синий цвет — это

267
00:09:18,720 --> 00:09:21,279
просто в корне неправильная мера

268
00:09:21,279 --> 00:09:23,760
для оценки диалоговых систем, и это

269
00:09:23,760 --> 00:09:25,440
может быть индикатором того, что это не так.

270
00:09:25,440 --> 00:09:27,120
чтобы быть подходящим для многих видов

271
00:09:27,120 --> 00:09:31,680
задач nlg в

272
00:09:31,680 --> 00:09:33,839
nlu, это всего лишь образец двух

273
00:09:33,839 --> 00:09:35,680
показателей, основанных на инграммах, я подумал, что упомяну несколько месяцев.

274
00:09:35,680 --> 00:09:37,360
чтобы дать вам основу для проведения некоторых

275
00:09:37,360 --> 00:09:39,120
сравнений, поэтому я упомянул слово «

276
00:09:39,120 --> 00:09:41,120
ошибка», которое в основном означает

277
00:09:41,120 --> 00:09:43,760
расстояние редактирования от одного справочного текста,

278
00:09:43,760 --> 00:09:46,080
синий, как мы видели, — это измененная точность,

279
00:09:46,080 --> 00:09:48,080
а штраф за краткость — вид движения припоминания,

280
00:09:48,080 --> 00:09:50,560
сравнивающий со многими справочными

281
00:09:50,560 --> 00:09:52,000
текстами,

282
00:09:52,000 --> 00:09:54,560
румяна — это  вспомните сфокусированный вариант

283
00:09:54,560 --> 00:09:56,640
синего, который ориентирован на оценку

284
00:09:56,640 --> 00:09:59,440
систем обобщения.

285
00:09:59,440 --> 00:10:01,279
Метеор интересно отличается,

286
00:10:01,279 --> 00:10:03,200
потому что он пытается выйти за рамки простого

287
00:10:03,200 --> 00:10:04,720
сопоставления инграмм и зафиксировать некоторые

288
00:10:04,720 --> 00:10:07,440
семантические понятия.

289
00:10:07,440 --> 00:10:10,160


290
00:10:10,160 --> 00:10:12,000


291
00:10:12,000 --> 00:10:14,640
и

292
00:10:14,640 --> 00:10:16,800
синонимы действительно пытаются привнести некоторые

293
00:10:16,800 --> 00:10:18,800
семантические аспекты,

294
00:10:18,800 --> 00:10:20,640
и сидр похож, это будет

295
00:10:20,640 --> 00:10:22,480
еще более семантическое понятие, потому что он

296
00:10:22,480 --> 00:10:24,160
будет выполнять свои сравнения в векторном

297
00:10:24,160 --> 00:10:26,320
пространстве, это своего рода приблизительно

298
00:10:26,320 --> 00:10:29,200
взвешенное косинусное сходство между

299
00:10:29,200 --> 00:10:33,839
векторами tf idf, созданными из корпуса

300
00:10:34,240 --> 00:10:36,079
наконец, в заключение я просто хотел

301
00:10:36,079 --> 00:10:38,079
призвать вас всех подумать о  больше

302
00:10:38,079 --> 00:10:39,839
коммуникационных показателей в

303
00:10:39,839 --> 00:10:42,800
контексте nlu для nlu стоит спросить

304
00:10:42,800 --> 00:10:44,399
, можете ли вы оценить свою систему,

305
00:10:44,399 --> 00:10:45,760
основываясь на том, насколько хорошо она на самом деле

306
00:10:45,760 --> 00:10:47,680
взаимодействует в контексте

307
00:10:47,680 --> 00:10:50,079
реальной цели, а не просто сравнивая

308
00:10:50,079 --> 00:10:52,480
различные строки, которые являются входными данными и

309
00:10:52,480 --> 00:10:54,800
справочными текстами, и мы  на самом деле видели

310
00:10:54,800 --> 00:10:56,959
пример этого в нашем задании и

311
00:10:56,959 --> 00:10:59,279
выпекали по эталону цвета, мы на

312
00:10:59,279 --> 00:11:01,200
самом деле не оценивали, насколько хорошо ваша система может

313
00:11:01,200 --> 00:11:03,040
воспроизводить высказывания, которые были

314
00:11:03,040 --> 00:11:05,600
в корпусе, а нашим основным понятием

315
00:11:05,600 --> 00:11:07,920
была точность слушателя, которая была ключом

316
00:11:07,920 --> 00:11:10,079
к цели общения  насколько хорошо

317
00:11:10,079 --> 00:11:12,160
ваша система на самом деле способна принимать

318
00:11:12,160 --> 00:11:14,720
сообщения и использовать их, чтобы понять,

319
00:11:14,720 --> 00:11:16,880
что говорящий имел в виду в простом

320
00:11:16,880 --> 00:11:19,360
цветовом контексте, и чтобы узнать больше об этом

321
00:11:19,360 --> 00:11:20,800
и взглянуть на многие из этих

322
00:11:20,800 --> 00:11:22,399
вопросов, я рекомендую вам ознакомиться с этой

323
00:11:22,399 --> 00:11:24,560
статьей  который возглавлял Бен Ньюман, он

324
00:11:24,560 --> 00:11:27,200
начался как курсовой проект для этого класса

325
00:11:27,200 --> 00:11:32,040
и превратился в действительно успешную работу.

