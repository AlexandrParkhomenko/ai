1
00:00:04,799 --> 00:00:06,319
приветствую всех, это четвертая часть

2
00:00:06,319 --> 00:00:07,919
нашей серии статей о выводе естественного языка,

3
00:00:07,919 --> 00:00:08,880
мы собираемся поговорить о

4
00:00:08,880 --> 00:00:10,639
различных стратегиях моделирования, вы можете

5
00:00:10,639 --> 00:00:12,559
рассматривать этот скринкаст как дополнение

6
00:00:12,559 --> 00:00:14,799
к соответствующей записной книжке, в которой

7
00:00:14,799 --> 00:00:16,560
исследуется множество различных идей моделирования и

8
00:00:16,560 --> 00:00:18,320
предлагаются некоторые архитектурные вариации.

9
00:00:18,320 --> 00:00:20,480
что вы могли бы исследовать сами,

10
00:00:20,480 --> 00:00:21,680
я думал, что мы начнем просто с

11
00:00:21,680 --> 00:00:23,600
рассмотрения функций функций, созданных вручную,

12
00:00:23,600 --> 00:00:25,680
потому что они все еще могут быть достаточно мощными

13
00:00:25,680 --> 00:00:27,439
для проблемы nli,

14
00:00:27,439 --> 00:00:29,279
поэтому некоторые стандартные идеи функций, созданных вручную,

15
00:00:29,279 --> 00:00:32,079
включают в качестве своего рода базовое

16
00:00:32,079 --> 00:00:33,760
совпадение слов между предпосылкой и

17
00:00:33,760 --> 00:00:36,000
гипотезой это  дает вам своего рода

18
00:00:36,000 --> 00:00:38,559
довольно небольшое функциональное пространство, по

19
00:00:38,559 --> 00:00:40,320
существу измеряющее, насколько

20
00:00:40,320 --> 00:00:43,120
похожи предпосылка и гипотеза,

21
00:00:43,120 --> 00:00:44,719
если вы хотите поднять его на уровень выше, вы

22
00:00:44,719 --> 00:00:46,640
могли бы рассматривать функции кросс-произведения слова,

23
00:00:46,640 --> 00:00:48,480
которые вы просто рассматриваете как его

24
00:00:48,480 --> 00:00:49,760
пространство признаков

25
00:00:49,760 --> 00:00:51,760
каждую пару слова в  предпосылка

26
00:00:51,760 --> 00:00:53,840
и слово в гипотезе, это

27
00:00:53,840 --> 00:00:56,640
даст вам массивное функциональное пространство, очень

28
00:00:56,640 --> 00:00:59,120
большое, очень с  parse, но интуиция,

29
00:00:59,120 --> 00:01:00,719
стоящая за этим, может заключаться в том, что вы даете

30
00:01:00,719 --> 00:01:02,879
своей модели шанс обнаружить

31
00:01:02,879 --> 00:01:04,799
точки совпадения и несоответствия

32
00:01:04,799 --> 00:01:06,960
между предпосылкой и гипотезой,

33
00:01:06,960 --> 00:01:09,439
и поэтому это может быть очень мощным,

34
00:01:09,439 --> 00:01:11,520
вы также можете рассмотреть дополнительные

35
00:01:11,520 --> 00:01:13,760
сетевые отношения слов, вносящие те, что в

36
00:01:13,760 --> 00:01:15,360
них будут  могут быть такие вещи, как следствие

37
00:01:15,360 --> 00:01:18,320
, противоречие и синоним автономии,

38
00:01:18,320 --> 00:01:19,840
и они, конечно, могут быть хорошо

39
00:01:19,840 --> 00:01:21,680
включены в основную логику

40
00:01:21,680 --> 00:01:23,920
проблемы nli.

41
00:01:23,920 --> 00:01:26,320
Расстояние редактирования - еще одна общая черта,

42
00:01:26,320 --> 00:01:28,400
но просто необработанное значение с плавающей запятой между

43
00:01:28,400 --> 00:01:30,479
предпосылкой и гипотезой как своего рода

44
00:01:30,479 --> 00:01:33,920
высокоуровневый способ сравнения  эти два

45
00:01:33,920 --> 00:01:35,680
различия слов в тексте могут быть хорошим

46
00:01:35,680 --> 00:01:37,680
сопоставлением с перекрытием слов, вы

47
00:01:37,680 --> 00:01:39,200
могли бы рассмотреть способы, которыми

48
00:01:39,200 --> 00:01:41,520
предпосылка и гипотеза контрастируют

49
00:01:41,520 --> 00:01:44,000
друг с другом в этом пространстве признаков,

50
00:01:44,000 --> 00:01:46,159
и мы также можем перейти к функциям, основанным на выравнивании,

51
00:01:46,159 --> 00:01:47,840
я упомянул, что перекрестное

52
00:01:47,840 --> 00:01:49,840
произведение слов является своего рода  попытка

53
00:01:49,840 --> 00:01:51,680
заставить модель изучить точки совпадения

54
00:01:51,680 --> 00:01:53,439
между предпосылкой и гипотезой,

55
00:01:53,439 --> 00:01:55,520
но не  с другой стороны, мы могли бы также сделать некоторые вещи

56
00:01:55,520 --> 00:01:57,280
еще до того, как мы начнем изучать

57
00:01:57,280 --> 00:01:59,040
веса признаков, пытаясь выяснить,

58
00:01:59,040 --> 00:02:01,040
какие части в предпосылке

59
00:02:01,040 --> 00:02:03,920
соответствуют какой части в гипотезе,

60
00:02:03,920 --> 00:02:05,759
мы могли бы рассмотреть отрицание, мы видели,

61
00:02:05,759 --> 00:02:07,040
конечно, что это важная

62
00:02:07,040 --> 00:02:09,199
индикаторная функция  во многих наборах данных nli

63
00:02:09,199 --> 00:02:11,440
за этим стоит мощная интуиция

64
00:02:11,440 --> 00:02:12,959
, особенно в том, что касается

65
00:02:12,959 --> 00:02:14,879
противоречия, и поэтому, возможно, мы могли бы

66
00:02:14,879 --> 00:02:16,319
написать некоторые функции признаков, которые были бы

67
00:02:16,319 --> 00:02:18,640
явно привязаны к наличию или

68
00:02:18,640 --> 00:02:20,800
отсутствию отрицания в различных

69
00:02:20,800 --> 00:02:23,520
местах предпосылки и гипотезы,

70
00:02:23,520 --> 00:02:25,120
и мы могли бы  также поднимите этот уровень на более высокий уровень

71
00:02:25,120 --> 00:02:27,360
и рассмотрите в более общем плане все

72
00:02:27,360 --> 00:02:28,560
виды различных интересных

73
00:02:28,560 --> 00:02:30,560
количественных отношений, которые,

74
00:02:30,560 --> 00:02:33,120
возможно, сохранятся на уровне выравнивания, как

75
00:02:33,120 --> 00:02:35,519
в пункте шесть здесь, между предпосылкой и

76
00:02:35,519 --> 00:02:37,599
гипотезой, и это своего рода ключ

77
00:02:37,599 --> 00:02:40,400
к основной логике  проблема nli,

78
00:02:40,400 --> 00:02:41,760


79
00:02:41,760 --> 00:02:43,040
а затем, наконец, названное распознавание сущностей,

80
00:02:43,040 --> 00:02:45,040
мы увидели, что эти

81
00:02:45,040 --> 00:02:46,800
функции могут быть важны для понимания

82
00:02:46,800 --> 00:02:49,120
нашей  какие сущности соотносятся с

83
00:02:49,120 --> 00:02:51,280
предпосылкой и гипотезой, и поэтому наличие

84
00:02:51,280 --> 00:02:53,760
некоторых устройств для выяснения этого может

85
00:02:53,760 --> 00:02:56,239
быть полезным, э-э, в качестве своего рода низкоуровневого

86
00:02:56,239 --> 00:02:59,440
заземления для вашей системы,

87
00:03:00,720 --> 00:03:02,800
теперь давайте перейдем в режим, который больше

88
00:03:02,800 --> 00:03:04,400
похож на режим глубокого обучения, потому что  как

89
00:03:04,400 --> 00:03:06,560
мы видели ранее в серии скринкастов,

90
00:03:06,560 --> 00:03:08,720
эти модели оказались на данный

91
00:03:08,720 --> 00:03:11,040
момент самыми мощными моделями

92
00:03:11,040 --> 00:03:13,040
для проблем nli, поэтому продуктивно

93
00:03:13,040 --> 00:03:14,800
подумать также о различных архитектурах глубокого обучения,

94
00:03:14,800 --> 00:03:16,720
и я хотел бы начать с

95
00:03:16,720 --> 00:03:18,239
того, что я назвал  здесь предложение и

96
00:03:18,239 --> 00:03:20,800
кодирование моделируют самую основную форму

97
00:03:20,800 --> 00:03:23,360
, которая возвращается к идее распределенных

98
00:03:23,360 --> 00:03:25,440
представлений как признаков,

99
00:03:25,440 --> 00:03:27,360
поэтому идея здесь состоит в том, что у нас есть на этой

100
00:03:27,360 --> 00:03:30,319
диаграмме предпосылка и гипотеза,

101
00:03:30,319 --> 00:03:32,319
поэтому предпосылки, которые танцевала каждая собака, и

102
00:03:32,319 --> 00:03:35,120
гипотеза, это то, что каждый пудель двигался

103
00:03:35,120 --> 00:03:37,040
и  наш подход с использованием распределенных

104
00:03:37,040 --> 00:03:39,360
представлений самый простой будет

105
00:03:39,360 --> 00:03:40,879
заключаться в том, что мы будем просто искать

106
00:03:40,879 --> 00:03:42,480
все эти слова в некотором фиксированном

107
00:03:42,480 --> 00:03:44,080
пространстве встраивания, которое будет похоже на

108
00:03:44,080 --> 00:03:46,560
GloVe, например, встраивание пространства,

109
00:03:46,560 --> 00:03:47,840
а затем мы собираемся отдельно

110
00:03:47,840 --> 00:03:50,400
закодировать предпосылку и гипотезу,

111
00:03:50,400 --> 00:03:52,879
например, выполнив сумму или

112
00:03:52,879 --> 00:03:55,040
среднее значение векторов в каждом из этих двух

113
00:03:55,040 --> 00:03:58,080
текстов, чтобы получить вектор xp и

114
00:03:58,080 --> 00:03:59,120
xh,

115
00:03:59,120 --> 00:04:01,599
а затем мы могли бы объединить эти  два

116
00:04:01,599 --> 00:04:03,200
или сделать что-то другое

117
00:04:03,200 --> 00:04:05,599
сравнение, такое как разница или максимальное значение или

118
00:04:05,599 --> 00:04:06,400
среднее значение,

119
00:04:06,400 --> 00:04:08,319
чтобы получить одно

120
00:04:08,319 --> 00:04:10,959
представление с фиксированной размерностью x, которое тогда является входом

121
00:04:10,959 --> 00:04:13,200
для типа, может быть простым классификатором softmax,

122
00:04:13,200 --> 00:04:14,560


123
00:04:14,560 --> 00:04:16,880
поэтому все, что мы здесь сделали, это взяли наш старый

124
00:04:16,880 --> 00:04:18,160
Подход с использованием распределенных

125
00:04:18,160 --> 00:04:20,639
представлений в качестве функций и перенос его

126
00:04:20,639 --> 00:04:22,400
в проблему nli, где у нас есть

127
00:04:22,400 --> 00:04:24,400
как предпосылка, так и гипотеза,

128
00:04:24,400 --> 00:04:26,080
и я назвал это моделью кодирования предложений,

129
00:04:26,080 --> 00:04:28,080
потому что мы отдельно кодируем

130
00:04:28,080 --> 00:04:30,240
два предложения, а затем модель

131
00:04:30,240 --> 00:04:32,000
узнает, что мы надеемся на что-то  о

132
00:04:32,000 --> 00:04:36,080
том, как эти две репрезентации взаимодействуют

133
00:04:36,240 --> 00:04:38,160
на этом и следующем слайдах, я дал

134
00:04:38,160 --> 00:04:40,560
полный рецепт того, как сделать то, что

135
00:04:40,560 --> 00:04:42,639
я только что описал,

136
00:04:42,639 --> 00:04:44,080
потому что он также есть в

137
00:04:44,080 --> 00:04:45,600
блокнотах, и он просто показывает вам, как

138
00:04:45,600 --> 00:04:47,440
с помощью нашего кода курса можно

139
00:04:47,440 --> 00:04:49,680
относительно легко настроить такие модели, как

140
00:04:49,680 --> 00:04:51,919
эта, большая часть кода посвящена

141
00:04:51,919 --> 00:04:53,680
низкоуровневой обработке

142
00:04:53,680 --> 00:04:58,320
слов в их пространстве для встраивания.

143
00:04:58,720 --> 00:05:00,479


144
00:05:00,479 --> 00:05:02,000
модели кодирования предложений Я думаю, что это довольно

145
00:05:02,000 --> 00:05:03,280
интересное право, мы могли бы захотеть

146
00:05:03,280 --> 00:05:05,199
закодировать посылку и гипотезу

147
00:05:05,199 --> 00:05:07,600
отдельно, чтобы дать модели

148
00:05:07,600 --> 00:05:09,280
возможность найти богатые абстрактные

149
00:05:09,280 --> 00:05:12,639
отношения между ними,

150
00:05:13,120 --> 00:05:14,720
подход к кодированию предложений может

151
00:05:14,720 --> 00:05:16,800
также облегчить переход к другим

152
00:05:16,800 --> 00:05:18,639
видам задач правильно  в той мере, в какой мы

153
00:05:18,639 --> 00:05:20,880
отдельно кодируем два предложения, мы

154
00:05:20,880 --> 00:05:22,160
могли бы иметь

155
00:05:22,160 --> 00:05:24,800
репрезентации на уровне предложений, которые полезны даже для

156
00:05:24,800 --> 00:05:26,320
проблем, которые не вписываются в

157
00:05:26,320 --> 00:05:28,800
специфический режим NLI, имеющий единую

158
00:05:28,800 --> 00:05:31,039
предпосылку и единственную гипотезу

159
00:05:31,039 --> 00:05:32,720
ради классификации, и которые могли бы  быть

160
00:05:32,720 --> 00:05:34,639
важной частью того видения от

161
00:05:34,639 --> 00:05:37,360
dagen вообще, что nli является своего рода

162
00:05:37,360 --> 00:05:39,680
источником эффективной предварительной подготовки для

163
00:05:39,680 --> 00:05:42,560
более общего  проблемы,

164
00:05:42,560 --> 00:05:44,720
давайте перейдем к более сложной модели, мы будем

165
00:05:44,720 --> 00:05:46,160
следовать тому же повествованию, которое мы

166
00:05:46,160 --> 00:05:48,000
использовали до того, как у нас была простая

167
00:05:48,000 --> 00:05:49,520
фиксированная модель, которая собиралась

168
00:05:49,520 --> 00:05:51,600
объединить предпосылку и гипотезу с помощью некоторой

169
00:05:51,600 --> 00:05:54,560
фиксированной функции, такой как сумма или среднее значение или

170
00:05:54,560 --> 00:05:55,520
максимум

171
00:05:55,520 --> 00:05:56,960
здесь мы'  у нас будут функции, которые

172
00:05:56,960 --> 00:05:58,560
узнают о том, как

173
00:05:58,560 --> 00:05:59,919
должны происходить эти взаимодействия, но мы собираемся

174
00:05:59,919 --> 00:06:02,160
следовать предложению в режиме кодирования, поэтому у меня

175
00:06:02,160 --> 00:06:04,240
есть тот же пример, когда каждая собака танцевала и

176
00:06:04,240 --> 00:06:06,400
каждый пудель двигался, и идея состоит в том, что

177
00:06:06,400 --> 00:06:08,639
каждый из них обрабатывается  свою

178
00:06:08,639 --> 00:06:11,199
собственную отдельную рекуррентную нейронную сеть,

179
00:06:11,199 --> 00:06:12,960
и я указал зеленым цветом, что,

180
00:06:12,960 --> 00:06:14,479
хотя эти две модели будут иметь

181
00:06:14,479 --> 00:06:16,000
одинаковую структуру, это разные

182
00:06:16,000 --> 00:06:17,759
параметры для предпосылки и

183
00:06:17,759 --> 00:06:20,479
гипотезы, поэтому они функционируют отдельно,

184
00:06:20,479 --> 00:06:22,240
а затем в простейшем подходе мы

185
00:06:22,240 --> 00:06:23,919
бы взяли окончательное скрытое

186
00:06:23,919 --> 00:06:26,240
представление из  каждый из них и

187
00:06:26,240 --> 00:06:28,080
каким-то образом комбинировать их, вероятно, посредством

188
00:06:28,080 --> 00:06:30,080
конкатенации, и это будет

189
00:06:30,080 --> 00:06:32,400
вводом для окончательного слоя или слоев классификатора,

190
00:06:32,400 --> 00:06:34,880
которые фактически изучают nli

191
00:06:34,880 --> 00:06:36,160
проблема,

192
00:06:36,160 --> 00:06:38,160
так что это подход к кодированию предложений в

193
00:06:38,160 --> 00:06:41,120
том смысле, что h3 и hd считаются

194
00:06:41,120 --> 00:06:43,280
своего рода отдельными сводными

195
00:06:43,280 --> 00:06:45,120
представлениями предпосылки и гипотезы

196
00:06:45,120 --> 00:06:47,199
соответственно, и у нас есть видение, что

197
00:06:47,199 --> 00:06:48,800
эти представления могут быть

198
00:06:48,800 --> 00:06:50,800
независимо полезны даже вне

199
00:06:50,800 --> 00:06:53,440
контекста nli

200
00:06:53,440 --> 00:06:56,639
сейчас в  связанных моделей ноутбуков nli

201
00:06:56,639 --> 00:06:58,560
2

202
00:06:58,560 --> 00:06:59,680
существует множество различных

203
00:06:59,680 --> 00:07:01,919
реализаций, включая полную

204
00:07:01,919 --> 00:07:04,240
реализацию pi torch с использованием наших

205
00:07:04,240 --> 00:07:06,800
классов pi torch, основанных на

206
00:07:06,800 --> 00:07:08,479
подходе кодирования предложений rnn, который я только что

207
00:07:08,479 --> 00:07:10,319
описал вам, и я подумал, что он

208
00:07:10,319 --> 00:07:11,759
просто даст вам

209
00:07:11,759 --> 00:07:14,080
краткий обзор высокого уровня  как работает этот подход к моделированию,

210
00:07:14,080 --> 00:07:15,280


211
00:07:15,280 --> 00:07:16,479
потому что на самом деле это всего лишь несколько

212
00:07:16,479 --> 00:07:18,160
движущихся частей, а остальное — это своего рода

213
00:07:18,160 --> 00:07:20,560
детали реализации низкого уровня,

214
00:07:20,560 --> 00:07:22,319
поэтому первое, что вам нужно сделать,

215
00:07:22,319 --> 00:07:25,440
это изменить класс набора данных,

216
00:07:25,440 --> 00:07:27,280
чтобы концептуально он собирался

217
00:07:27,280 --> 00:07:30,319
создавать списки  пары примеров с

218
00:07:30,319 --> 00:07:31,840
их длиной и связанными с ними

219
00:07:31,840 --> 00:07:34,319
метками по умолчанию базовый код,

220
00:07:34,319 --> 00:07:36,400
который мы используем, ожидает один  последовательность

221
00:07:36,400 --> 00:07:38,800
токенов одной длины и одной метки, и здесь

222
00:07:38,800 --> 00:07:40,639
нам просто нужно поднять ее, чтобы у нас

223
00:07:40,639 --> 00:07:42,639
было два, как вы можете видеть здесь, каждая собака

224
00:07:42,639 --> 00:07:44,800
танцевала, каждый пудель двигался, у обоих

225
00:07:44,800 --> 00:07:46,400
есть ссылка три, и их метка является

226
00:07:46,400 --> 00:07:48,639
следствием, поэтому мы вносим некоторые изменения в

227
00:07:48,639 --> 00:07:50,560
класс набора данных, чтобы приспособиться к этому

228
00:07:50,560 --> 00:07:53,919
изменению формата, по существу,

229
00:07:53,919 --> 00:07:56,879
тогда основная модель для этого

230
00:07:56,879 --> 00:08:00,080
концептуально представляет собой всего две rnns, а

231
00:08:00,080 --> 00:08:02,160
метод forward просто

232
00:08:02,160 --> 00:08:04,319
объединяет эти две части и

233
00:08:04,319 --> 00:08:05,759
передает их последующим

234
00:08:05,759 --> 00:08:07,759
слоям классификатора, и поэтому это очень

235
00:08:07,759 --> 00:08:09,520
концептуально естественно, и это просто  вплоть

236
00:08:09,520 --> 00:08:11,599
до наличия двух отдельных rnns, которые вы

237
00:08:11,599 --> 00:08:12,479
реализуете

238
00:08:12,479 --> 00:08:14,319
с использованием исходных материалов, которые уже

239
00:08:14,319 --> 00:08:16,160
есть в коде,

240
00:08:16,160 --> 00:08:17,599
а затем, наконец, для фактического

241
00:08:17,599 --> 00:08:20,160
интерфейса классификатора кодировщика предложений torch rnn

242
00:08:20,160 --> 00:08:23,199
это в основном не изменилось

243
00:08:23,199 --> 00:08:25,280
с одной модификацией, которая вам нужна,

244
00:08:25,280 --> 00:08:27,599
чтобы изменить метод прогнозирования проблемы

245
00:08:27,599 --> 00:08:29,520
фундаментальный метод прогнозирования,

246
00:08:29,520 --> 00:08:31,520
потому что он также должен иметь дело с этим

247
00:08:31,520 --> 00:08:33,039
другим форматом набора данных, который мы

248
00:08:33,039 --> 00:08:35,120
оцениваем  здесь убрано, и это снова своего

249
00:08:35,120 --> 00:08:36,640
рода низкоуровневое изменение, и поэтому я

250
00:08:36,640 --> 00:08:39,039
надеюсь, что вы видите, что первый и

251
00:08:39,039 --> 00:08:41,279
третий шаги — это своего рода управление

252
00:08:41,279 --> 00:08:42,159
данными,

253
00:08:42,159 --> 00:08:43,760
а средний шаг — это тот, который

254
00:08:43,760 --> 00:08:46,160
фактически изменяет граф вычислений,

255
00:08:46,160 --> 00:08:48,000
но это  шаг очень интуитивен, потому что

256
00:08:48,000 --> 00:08:50,080
мы в основном просто отражаем в коде

257
00:08:50,080 --> 00:08:52,000
нашу идею о том, что у нас есть отдельные rnns

258
00:08:52,000 --> 00:08:55,839
для предпосылки и гипотезы,

259
00:08:55,839 --> 00:08:57,360
а затем, наконец, я просто хочу упомянуть,

260
00:08:57,360 --> 00:08:58,640
что общий подход, который вы видите,

261
00:08:58,640 --> 00:09:00,880
особенно в ранней литературе, - это

262
00:09:00,880 --> 00:09:03,200
дерево кодирования предложений nnn  за этим стоит

263
00:09:03,200 --> 00:09:05,440
точно такая же интуиция, как и

264
00:09:05,440 --> 00:09:07,760
за rnns, которые мы только что рассмотрели, за исключением

265
00:09:07,760 --> 00:09:10,080
того, что предпосылка и гипотеза

266
00:09:10,080 --> 00:09:12,880
обрабатываются древовидными

267
00:09:12,880 --> 00:09:15,120
рекурсивными рекурсивными нейронными сетями, и поскольку

268
00:09:15,120 --> 00:09:17,279
базовые наборы данных часто имеют

269
00:09:17,279 --> 00:09:20,000
представления полного разбора, это путь,

270
00:09:20,000 --> 00:09:22,080
который вы могли бы  изучить

271
00:09:22,080 --> 00:09:23,519
, может быть сложно реализовать их

272
00:09:23,519 --> 00:09:25,360
эффективно, но концептуально

273
00:09:25,360 --> 00:09:27,200
это очень естественная вещь, когда вы просто

274
00:09:27,200 --> 00:09:29,680
постоянно имеете плотный слой в каждом

275
00:09:29,680 --> 00:09:32,240
один из составляющих узлов вплоть до

276
00:09:32,240 --> 00:09:35,839
окончательного представления здесь pb и pd,

277
00:09:35,839 --> 00:09:38,160
который затем передается на уровень классификатора

278
00:09:38,160 --> 00:09:40,080
точно так же, как мы делали для предыдущих

279
00:09:40,080 --> 00:09:42,480
моделей,

280
00:09:42,800 --> 00:09:44,640
так что это предложения, кодирующие rnns,

281
00:09:44,640 --> 00:09:46,240
теперь давайте перейдем к другому видению и

282
00:09:46,240 --> 00:09:48,000
я назвал эти цепные модели, потому

283
00:09:48,000 --> 00:09:49,360
что они собираются просто

284
00:09:49,360 --> 00:09:51,519
смешать предпосылку и гипотезу,

285
00:09:51,519 --> 00:09:53,279
а не кодировать их по отдельности.

286
00:09:53,279 --> 00:09:54,080


287
00:09:54,080 --> 00:09:56,240
Конечно, самое простое, что мы могли бы сделать

288
00:09:56,240 --> 00:09:58,160
в цепном режиме, это по

289
00:09:58,160 --> 00:10:00,240
существу игнорировать тот факт, что у нас есть

290
00:10:00,240 --> 00:10:03,040
два  выводит в текст предпосылку и гипотезу и

291
00:10:03,040 --> 00:10:05,279
просто передает их в виде одной длинной последовательности

292
00:10:05,279 --> 00:10:08,000
в стандартную рекуррентную нейронную сеть,

293
00:10:08,000 --> 00:10:09,920
и, поскольку это не требует изменений ни в

294
00:10:09,920 --> 00:10:11,839
одном коде, который мы использовали для классификаторов rnn,

295
00:10:11,839 --> 00:10:13,920
это кажется довольно

296
00:10:13,920 --> 00:10:16,160
естественной базовой линией, и так это изображено

297
00:10:16,160 --> 00:10:17,279
здесь

298
00:10:17,279 --> 00:10:19,040
и на самом деле это может быть удивительно

299
00:10:19,040 --> 00:10:20,240
эффективным,

300
00:10:20,240 --> 00:10:22,399
правильное обоснование для этого

301
00:10:22,399 --> 00:10:24,720
в этом контексте здесь вы могли бы сказать,

302
00:10:24,720 --> 00:10:26,079
что предпосылкой

303
00:10:26,079 --> 00:10:28,320
является просто установление контекста для

304
00:10:28,320 --> 00:10:30,240
обработки  гипотеза, и это

305
00:10:30,240 --> 00:10:31,920
кажется очень естественным понятием

306
00:10:31,920 --> 00:10:34,000
обусловленности одного текста, когда вы

307
00:10:34,000 --> 00:10:36,240
обрабатываете второй, и, соответственно,

308
00:10:36,240 --> 00:10:37,760
на уровне обработки человеческого языка

309
00:10:37,760 --> 00:10:39,279
это может фактически соответствовать тому,

310
00:10:39,279 --> 00:10:40,640
что мы делаем,

311
00:10:40,640 --> 00:10:42,000
когда читаем текст нашей предпосылки

312
00:10:42,000 --> 00:10:44,000
гипотезы и выясняем  какова

313
00:10:44,000 --> 00:10:46,399
взаимосвязь,

314
00:10:46,399 --> 00:10:48,640
и вот простой рецепт для

315
00:10:48,640 --> 00:10:50,320
этого. Единственное отличие от диаграммы

316
00:10:50,320 --> 00:10:52,160
, о котором вы могли бы подумать, это то, что я сделал

317
00:10:52,160 --> 00:10:54,480
, представляя примеры здесь,

318
00:10:54,480 --> 00:10:56,480
выровняв их, конечно,

319
00:10:56,480 --> 00:10:58,240
но также вставив этот граничный маркер

320
00:10:58,240 --> 00:10:59,440
, который, по крайней мере, дал бы модель

321
00:10:59,440 --> 00:11:01,120
шанс узнать, что произошло

322
00:11:01,120 --> 00:11:02,800
разделение, происходит какой-то

323
00:11:02,800 --> 00:11:04,399
переход между предпосылкой и

324
00:11:04,399 --> 00:11:05,680
гипотезой,

325
00:11:05,680 --> 00:11:06,880
но это всего лишь уровень

326
00:11:06,880 --> 00:11:08,880
характеристики, и с точки зрения моделирования

327
00:11:08,880 --> 00:11:10,640
вам вряд ли нужно вносить какие-либо изменения

328
00:11:10,640 --> 00:11:14,480
, чтобы запустить такой эксперимент,

329
00:11:14,480 --> 00:11:16,480
мы могли бы также подумать  о модификации

330
00:11:16,480 --> 00:11:17,839
, которая объединила бы

331
00:11:17,839 --> 00:11:19,440
кодирование предложений с цепочкой, и это было бы

332
00:11:19,440 --> 00:11:22,640
там, где у нас есть два набора r  nn параметры,

333
00:11:22,640 --> 00:11:24,000
один для предпосылки и один для

334
00:11:24,000 --> 00:11:26,000
гипотезы, но мы, тем не менее, связываем их

335
00:11:26,000 --> 00:11:28,000
вместе, а не кодируем по отдельности,

336
00:11:28,000 --> 00:11:30,880
так что до того, как у меня есть предпосылка

337
00:11:30,880 --> 00:11:32,000
rnn зеленым цветом, у

338
00:11:32,000 --> 00:11:35,040
меня есть гипотеза rnn и фиолетовый, они

339
00:11:35,040 --> 00:11:36,800
имеют одинаковую структуру, но разные

340
00:11:36,800 --> 00:11:39,279
изученные параметры и  передача обслуживания, по

341
00:11:39,279 --> 00:11:40,959
сути, заключается в том, что начальное скрытое

342
00:11:40,959 --> 00:11:43,600
состояние для гипотезы является окончательным

343
00:11:43,600 --> 00:11:45,680
выходным состоянием для предпосылки, и таким

344
00:11:45,680 --> 00:11:47,279
образом вы получаете плавный переход

345
00:11:47,279 --> 00:11:49,040
между этими двумя моделями,

346
00:11:49,040 --> 00:11:50,639
и это дает модели некоторое

347
00:11:50,639 --> 00:11:53,360
пространство для изучения того, что токены

348
00:11:53,360 --> 00:11:55,279
предпосылок и последовательности предпосылок имеют  другой

349
00:11:55,279 --> 00:11:56,959
статус, чем те, которые появляются в

350
00:11:56,959 --> 00:11:59,680
гипотезе,

351
00:11:59,839 --> 00:12:01,440
и позвольте мне в завершение упомянуть

352
00:12:01,440 --> 00:12:03,200
несколько других стратегий, потому что это

353
00:12:03,200 --> 00:12:05,120
ни в коем случае не было исчерпывающим, но это довольно

354
00:12:05,120 --> 00:12:07,040
интересно на высоком уровне

355
00:12:07,040 --> 00:12:08,800
архитектуры, размышляя о кодировании приговоров по

356
00:12:08,800 --> 00:12:11,920
сравнению с этими цепными моделями,

357
00:12:11,920 --> 00:12:14,959
поэтому сначала факел  Классификатор rnn передает

358
00:12:14,959 --> 00:12:16,399
свое скрытое состояние непосредственно на

359
00:12:16,399 --> 00:12:18,320
слой классификатора, но у нас есть такие опции,

360
00:12:18,320 --> 00:12:20,639
как bi-dire  ctional равно true, что

361
00:12:20,639 --> 00:12:22,720
будет использовать в качестве сводного представления

362
00:12:22,720 --> 00:12:24,480
как конечное, так и начальное скрытые

363
00:12:24,480 --> 00:12:26,560
состояния, по существу, и передавать их

364
00:12:26,560 --> 00:12:28,160
в классификатор, так что это другое

365
00:12:28,160 --> 00:12:30,959
понятие кодирования приговора или

366
00:12:30,959 --> 00:12:33,279
кодирования последовательности

367
00:12:33,279 --> 00:12:35,360
и других идей здесь, поэтому мы могли бы

368
00:12:35,360 --> 00:12:37,279
вместо того, чтобы ограничиваться только  одно или

369
00:12:37,279 --> 00:12:39,519
несколько конечных состояний выполняют какое-то

370
00:12:39,519 --> 00:12:41,519
объединение с максимальным или средним значением для

371
00:12:41,519 --> 00:12:43,120
всех выходных состояний,

372
00:12:43,120 --> 00:12:44,880
и различные параметры объединения могут быть

373
00:12:44,880 --> 00:12:46,320
объединены с различными версиями

374
00:12:46,320 --> 00:12:48,720
этих моделей либо с кодированием предложений,

375
00:12:48,720 --> 00:12:50,800
либо с цепочкой, мы также, конечно, можем иметь

376
00:12:50,800 --> 00:12:53,519
дополнительные скрытые слои  между

377
00:12:53,519 --> 00:12:55,440
слоем классификатора и встраиванием я

378
00:12:55,440 --> 00:12:56,880
показал вам только один для

379
00:12:56,880 --> 00:12:59,040
простоты, но более глубокий может быть лучше,

380
00:12:59,040 --> 00:13:01,040
особенно для очень больших наборов данных nli,

381
00:13:01,040 --> 00:13:02,399
которые у нас есть,

382
00:13:02,399 --> 00:13:04,079
и, наконец, важным источником

383
00:13:04,079 --> 00:13:06,560
инноваций в этом и многих других областях

384
00:13:06,560 --> 00:13:08,079
является  идея добавления

385
00:13:08,079 --> 00:13:10,079
механизмов внимания к этим моделям, и это

386
00:13:10,079 --> 00:13:11,519
настолько важная идея, что я собираюсь

387
00:13:11,519 --> 00:13:13,920
приберечь ее для следующих  репетиция в этой

388
00:13:13,920 --> 00:13:16,920
серии

