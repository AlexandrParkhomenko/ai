1
00:00:00,000 --> 00:00:05,120


2
00:00:05,120 --> 00:00:06,870
CHRISTOPHER POTTS:
Welcome back, everyone.

3
00:00:06,870 --> 00:00:08,870
This is part 4 on our
series on distributed word

4
00:00:08,870 --> 00:00:09,550
representations.

5
00:00:09,550 --> 00:00:12,050
We're going to be talking about
basical reweighting schemes.

6
00:00:12,050 --> 00:00:13,190
Essentially, I feel
like we've been

7
00:00:13,190 --> 00:00:15,260
faithful to the underlying
counts of our matrices

8
00:00:15,260 --> 00:00:16,020
for too long.

9
00:00:16,020 --> 00:00:17,740
It's time to start
messing with them.

10
00:00:17,740 --> 00:00:20,240
Here are some high-level goals
that we have for reweighting.

11
00:00:20,240 --> 00:00:21,698
And we would like
in these matrices

12
00:00:21,698 --> 00:00:23,450
to amplify the
associations that are

13
00:00:23,450 --> 00:00:27,390
important and trustworthy and
unusual while correspondingly

14
00:00:27,390 --> 00:00:30,110
deemphasizing the things
that are mundane or quirky

15
00:00:30,110 --> 00:00:32,360
or reflect errors or
idiosyncrasies in the data

16
00:00:32,360 --> 00:00:33,530
that we use.

17
00:00:33,530 --> 00:00:36,470
Now, of course, absent the
defined objective function

18
00:00:36,470 --> 00:00:37,720
of the machine-learning sense.

19
00:00:37,720 --> 00:00:39,680
This is going to
remain a fuzzy goal,

20
00:00:39,680 --> 00:00:41,810
but we do have some
quantitative hooks, I think.

21
00:00:41,810 --> 00:00:44,030
We have this guiding
intuition that we

22
00:00:44,030 --> 00:00:45,980
would like to move
away from raw counts

23
00:00:45,980 --> 00:00:48,470
because frequency
alone is generally

24
00:00:48,470 --> 00:00:51,110
a poor proxy for the kind
of semantic information

25
00:00:51,110 --> 00:00:53,130
that we hope to extract.

26
00:00:53,130 --> 00:00:55,250
So we can ask for each of
the reweighting schemes

27
00:00:55,250 --> 00:00:57,020
that we consider,
first, how does

28
00:00:57,020 --> 00:00:59,870
it compare to the
underlying raw count values?

29
00:00:59,870 --> 00:01:02,900
If the scheme is just rescaling
the underlying counts,

30
00:01:02,900 --> 00:01:04,525
it's probably not
worth the effort.

31
00:01:04,525 --> 00:01:05,900
On the other hand,
if it gives us

32
00:01:05,900 --> 00:01:07,595
a very different
distribution, then

33
00:01:07,595 --> 00:01:09,470
at least we know that
we're cooking with fire

34
00:01:09,470 --> 00:01:12,655
when it comes to moving
away from raw frequency.

35
00:01:12,655 --> 00:01:14,030
There's a related
question that I

36
00:01:14,030 --> 00:01:15,710
would like us to
have in mind, what

37
00:01:15,710 --> 00:01:17,630
is the overall
distribution of values

38
00:01:17,630 --> 00:01:19,490
that the reweighting
scheme delivers?

39
00:01:19,490 --> 00:01:22,190
Count distributions are
very skewed in a way that

40
00:01:22,190 --> 00:01:24,110
can make them difficult
to deal with for lots

41
00:01:24,110 --> 00:01:26,220
of analytic and
machine-learning methods.

42
00:01:26,220 --> 00:01:28,610
So we might hope that in
reweighting, in addition

43
00:01:28,610 --> 00:01:30,230
to capturing things
that are important

44
00:01:30,230 --> 00:01:32,390
and deemphasizing
things that are mundane,

45
00:01:32,390 --> 00:01:34,880
would also give us an overall
distribution of values

46
00:01:34,880 --> 00:01:38,600
that was more tractable for
these downstream applications.

47
00:01:38,600 --> 00:01:40,070
And then finally
I personally have

48
00:01:40,070 --> 00:01:42,320
a goal that we would like
to do, no feature selection

49
00:01:42,320 --> 00:01:44,900
based on counts or
outside resources

50
00:01:44,900 --> 00:01:46,620
like stopword dictionaries.

51
00:01:46,620 --> 00:01:49,700
I don't want to be filtering
off parts of the vocabulary

52
00:01:49,700 --> 00:01:51,790
a priori, because
for all I know,

53
00:01:51,790 --> 00:01:54,320
something that's a boring
stopword for one genre is

54
00:01:54,320 --> 00:01:56,900
actually an important
content word for another.

55
00:01:56,900 --> 00:01:59,240
We would like the method to
sort of make that decision.

56
00:01:59,240 --> 00:02:02,360


57
00:02:02,360 --> 00:02:04,480
So let's start with
the most basic scheme.

58
00:02:04,480 --> 00:02:06,610
And this is a scheme that
will pay attention only

59
00:02:06,610 --> 00:02:08,080
to the row context.

60
00:02:08,080 --> 00:02:09,220
This is normalization.

61
00:02:09,220 --> 00:02:11,620
So this is actually a repeat
from the lecture on vector

62
00:02:11,620 --> 00:02:13,600
comparison L2 norming.

63
00:02:13,600 --> 00:02:15,880
We've calculated the
L2 length as a fixed

64
00:02:15,880 --> 00:02:17,740
quantity for each row vector.

65
00:02:17,740 --> 00:02:19,300
And then the length
normalization

66
00:02:19,300 --> 00:02:20,950
of that row vector
is just taking

67
00:02:20,950 --> 00:02:22,810
each value in the
original vector

68
00:02:22,810 --> 00:02:27,127
and dividing it by that fixed
quantity of the L2 lengths.

69
00:02:27,127 --> 00:02:28,960
There's a related, and
perhaps more familiar

70
00:02:28,960 --> 00:02:31,293
notion, which I've called
probability distribution where

71
00:02:31,293 --> 00:02:32,920
we follow the same logic.

72
00:02:32,920 --> 00:02:34,810
We just replace that
normalizing constant,

73
00:02:34,810 --> 00:02:37,060
the L2 length with
the sum of all

74
00:02:37,060 --> 00:02:38,290
the elements in the vectors.

75
00:02:38,290 --> 00:02:40,960
But again we do this
element-wise division

76
00:02:40,960 --> 00:02:43,000
by that fixed
quantity to normalize

77
00:02:43,000 --> 00:02:46,390
the vector into a
probability distribution.

78
00:02:46,390 --> 00:02:48,400
I think both of these
methods can be powerful,

79
00:02:48,400 --> 00:02:51,160
but the shame of them is that
they are paying attention only

80
00:02:51,160 --> 00:02:52,570
to the row context.

81
00:02:52,570 --> 00:02:54,640
For a given cell, IJ,
we're looking just

82
00:02:54,640 --> 00:02:58,030
across the row I. We're not
considering the context that

83
00:02:58,030 --> 00:03:00,580
could come from the
column J. So let's begin

84
00:03:00,580 --> 00:03:02,740
to correct that omission.

85
00:03:02,740 --> 00:03:05,243
Here is kind of the star of
our show in the quiet sense.

86
00:03:05,243 --> 00:03:07,660
This is the first scheme we'll
look at that pays attention

87
00:03:07,660 --> 00:03:09,250
to both row and column context.

88
00:03:09,250 --> 00:03:11,442
This is observed over expected.

89
00:03:11,442 --> 00:03:13,150
Let's just go through
this notation here.

90
00:03:13,150 --> 00:03:14,170
We have the row sum.

91
00:03:14,170 --> 00:03:15,430
I think that's intuitive.

92
00:03:15,430 --> 00:03:17,630
Correspondingly, the
column sum, the sum of all

93
00:03:17,630 --> 00:03:19,540
values along the column.

94
00:03:19,540 --> 00:03:21,640
And then the sum for
some matrix x is just

95
00:03:21,640 --> 00:03:24,610
the sum of all the cell
values in that matrix.

96
00:03:24,610 --> 00:03:26,740
Those are the raw materials
for calculating what's

97
00:03:26,740 --> 00:03:28,600
called the expected value.

98
00:03:28,600 --> 00:03:32,350
The expected value given
a matrix x for cell i, j

99
00:03:32,350 --> 00:03:35,440
is the rowsum
times the columnsum

100
00:03:35,440 --> 00:03:37,930
as the numerator divided
by the sum of all

101
00:03:37,930 --> 00:03:39,580
the values in the matrix.

102
00:03:39,580 --> 00:03:41,950
This is an expected quasi count.

103
00:03:41,950 --> 00:03:44,770
It is giving us the
number we would expect

104
00:03:44,770 --> 00:03:47,470
if the row and column were
independent of each other

105
00:03:47,470 --> 00:03:48,760
in the statistical sense.

106
00:03:48,760 --> 00:03:51,910
And that's the sense in
which this is an expectation.

107
00:03:51,910 --> 00:03:54,520
The observed over expected
value simply compared

108
00:03:54,520 --> 00:03:57,850
the observed value in the
numerator by that expected

109
00:03:57,850 --> 00:03:59,180
value.

110
00:03:59,180 --> 00:04:01,930
So in a bit more detail, here's
how the calculations work.

111
00:04:01,930 --> 00:04:03,850
We've got this tiny
little count matrix here.

112
00:04:03,850 --> 00:04:04,960
Let's look at cell XA.

113
00:04:04,960 --> 00:04:06,890
It's got a count of 34.

114
00:04:06,890 --> 00:04:10,160
That's our observed count
over here in the numerator.

115
00:04:10,160 --> 00:04:12,880
The denominator is the
product of the rowsum

116
00:04:12,880 --> 00:04:15,970
and the columnsum,
45 by 81 divided

117
00:04:15,970 --> 00:04:19,570
by the sum of all the values
in this matrix, which is 99.

118
00:04:19,570 --> 00:04:22,610
We repeat that calculation
for all the other cells,

119
00:04:22,610 --> 00:04:24,340
making the corresponding
adjustments,

120
00:04:24,340 --> 00:04:29,070
and that gives us a
completely reweighted matrix.

121
00:04:29,070 --> 00:04:30,170
Here's the intuition.

122
00:04:30,170 --> 00:04:31,252
That was the calculation.

123
00:04:31,252 --> 00:04:33,210
Let's think about why we
might want to do this.

124
00:04:33,210 --> 00:04:36,920
So I've got here a highly
idealized little count matrix.

125
00:04:36,920 --> 00:04:40,250
And the conceit of this example
is that "keep tabs" in English

126
00:04:40,250 --> 00:04:41,360
is an idiom.

127
00:04:41,360 --> 00:04:43,130
And otherwise, the
word "tabs" alone

128
00:04:43,130 --> 00:04:45,000
doesn't appear with
many other words.

129
00:04:45,000 --> 00:04:48,140
It's kind of constrained
to this idiomatic context.

130
00:04:48,140 --> 00:04:51,110
So we get a really high
count for "keep tabs"

131
00:04:51,110 --> 00:04:53,570
and a relatively low
count for "enjoy tabs"

132
00:04:53,570 --> 00:04:56,240
again because "tabs" doesn't
really associate with the word

133
00:04:56,240 --> 00:04:57,640
"enjoy."

134
00:04:57,640 --> 00:05:00,640
On the right here, I've got
the expected calculation.

135
00:05:00,640 --> 00:05:02,500
And it comes out just
like we would hope.

136
00:05:02,500 --> 00:05:06,670
The expected count for
"keep tabs" is mere 12.48%,

137
00:05:06,670 --> 00:05:08,660
compare that with the
observed count of 20.

138
00:05:08,660 --> 00:05:12,580
"Keep tabs" is overrepresented
relative to our expectations

139
00:05:12,580 --> 00:05:15,580
in virtue of the fact that the
independence assumption built

140
00:05:15,580 --> 00:05:18,520
into the expected calculation
is just not met here because

141
00:05:18,520 --> 00:05:20,570
of the collocational effect.

142
00:05:20,570 --> 00:05:24,290
Similarly, the expected count
for "enjoy tabs" is 8.5.

143
00:05:24,290 --> 00:05:26,470
That's much larger
than our observation,

144
00:05:26,470 --> 00:05:29,140
again because these are
kind of disassociated

145
00:05:29,140 --> 00:05:31,990
with each other in virtue of
the restricted distribution

146
00:05:31,990 --> 00:05:33,900
of tabs.

147
00:05:33,900 --> 00:05:36,150
And that brings us to
really the star of our show,

148
00:05:36,150 --> 00:05:38,890
and in fact the star of a lot
of the remainder of this unit.

149
00:05:38,890 --> 00:05:42,030
This is pointwise mutual
information, or PMI.

150
00:05:42,030 --> 00:05:45,690
PMI is simply observed
over expected in log-space

151
00:05:45,690 --> 00:05:48,570
where we stipulate
that the log of 0 is 0.

152
00:05:48,570 --> 00:05:52,350
In a bit more detail for
matrix x, given cell i, j,

153
00:05:52,350 --> 00:05:55,380
the PMI value is the
log of the observed

154
00:05:55,380 --> 00:05:57,130
count over the expected count.

155
00:05:57,130 --> 00:05:58,140
And that's it.

156
00:05:58,140 --> 00:06:00,210
Many people find it
more intuitive to think

157
00:06:00,210 --> 00:06:01,515
of this in probabilistic terms.

158
00:06:01,515 --> 00:06:03,390
That's what I've done
over here on the right.

159
00:06:03,390 --> 00:06:06,910
It's equivalent numerically, but
for this kind of calculation,

160
00:06:06,910 --> 00:06:09,810
we first form a joint
probability table

161
00:06:09,810 --> 00:06:12,660
by just dividing all the cell
values by the total number

162
00:06:12,660 --> 00:06:14,580
of values in all the cells.

163
00:06:14,580 --> 00:06:16,470
That gives us the
joint probability table

164
00:06:16,470 --> 00:06:19,710
and then the row probability
and the column probability

165
00:06:19,710 --> 00:06:22,930
are just summing across the row
and the column respectively.

166
00:06:22,930 --> 00:06:24,055
And again we multiply them.

167
00:06:24,055 --> 00:06:25,763
And that's kind of
nice, because then you

168
00:06:25,763 --> 00:06:28,260
can see we really are testing
an independence assumption.

169
00:06:28,260 --> 00:06:31,200
It's as though we say we can
multiply these probabilities

170
00:06:31,200 --> 00:06:32,760
because they're independent.

171
00:06:32,760 --> 00:06:34,710
If the distribution
is truly independent,

172
00:06:34,710 --> 00:06:36,530
that ought to match
what we observed.

173
00:06:36,530 --> 00:06:38,280
And of course discrepancies
are the things

174
00:06:38,280 --> 00:06:41,010
that these matrices
will highlight.

175
00:06:41,010 --> 00:06:42,060
Let's look at an example.

176
00:06:42,060 --> 00:06:43,050
And there's one
thing that I want

177
00:06:43,050 --> 00:06:45,150
to track because we work
through this example.

178
00:06:45,150 --> 00:06:48,190
And that's the cell down
here, this lonely little 1.

179
00:06:48,190 --> 00:06:49,410
So this is a count matrix.

180
00:06:49,410 --> 00:06:51,327
I've got this as a word
by document matrix.

181
00:06:51,327 --> 00:06:53,160
This is a very flexible
method, and we apply

182
00:06:53,160 --> 00:06:55,230
to lots of matrix designs.

183
00:06:55,230 --> 00:06:58,020
Over here, I form the
joint probability table.

184
00:06:58,020 --> 00:07:01,140
And I've got here the columnsum
and the rowsum corresponding

185
00:07:01,140 --> 00:07:02,700
to the column and
row probability.

186
00:07:02,700 --> 00:07:06,240
These are the raw ingredients
for the PMI matrix, which

187
00:07:06,240 --> 00:07:09,580
is derived down here by
applying this calculation to all

188
00:07:09,580 --> 00:07:11,010
of these values.

189
00:07:11,010 --> 00:07:12,900
Notice what's
happened, that lonely 1

190
00:07:12,900 --> 00:07:16,140
down here because it's in a very
infrequent row and a relatively

191
00:07:16,140 --> 00:07:17,460
infrequent column.

192
00:07:17,460 --> 00:07:21,150
It has the largest PMI value
in the resulting matrix.

193
00:07:21,150 --> 00:07:22,650
Now that could be
good, because this

194
00:07:22,650 --> 00:07:25,110
could be a very important
event, in which case

195
00:07:25,110 --> 00:07:26,400
we want to amplify it.

196
00:07:26,400 --> 00:07:29,790
On the other hand,
NLP being what it is,

197
00:07:29,790 --> 00:07:32,310
this could be just a mistake
in the data or something.

198
00:07:32,310 --> 00:07:34,110
And then this
exaggerated value here

199
00:07:34,110 --> 00:07:36,040
could turn out to
be problematic.

200
00:07:36,040 --> 00:07:37,690
It's difficult to
strike this balance.

201
00:07:37,690 --> 00:07:40,260
But it's worth keeping in mind
as you work with this method

202
00:07:40,260 --> 00:07:43,470
that it could amplify not
only important things but also

203
00:07:43,470 --> 00:07:45,300
idiosyncratic things.

204
00:07:45,300 --> 00:07:48,252
Positive PMI is an important
variant of PMI, so important,

205
00:07:48,252 --> 00:07:49,710
in fact, that I
would like to think

206
00:07:49,710 --> 00:07:51,085
of it as the kind
of default view

207
00:07:51,085 --> 00:07:54,000
that we take on PMI for
the following reason.

208
00:07:54,000 --> 00:07:57,000
PMI is actually undefined where
the count is 0, because we

209
00:07:57,000 --> 00:07:59,340
need to take the log of 0.

210
00:07:59,340 --> 00:08:01,065
So we had to stipulate
that the log of 0

211
00:08:01,065 --> 00:08:04,650
was 0 for this calculation.

212
00:08:04,650 --> 00:08:06,870
However, that's
arguably not coherent

213
00:08:06,870 --> 00:08:09,300
if you think about what the
underlying matrix represents.

214
00:08:09,300 --> 00:08:11,430
What we're saying with
PMI is that larger

215
00:08:11,430 --> 00:08:14,340
than expected values
get a large PMI.

216
00:08:14,340 --> 00:08:17,340
Smaller than expected
values get a smaller PMI.

217
00:08:17,340 --> 00:08:18,340
That's good.

218
00:08:18,340 --> 00:08:21,120
But when we encounter a 0, we
place it right in the middle.

219
00:08:21,120 --> 00:08:23,790
And that's just strange
because a 0 isn't evidence

220
00:08:23,790 --> 00:08:25,372
of anything larger or smaller.

221
00:08:25,372 --> 00:08:27,330
It doesn't deserve to be
in the middle of this.

222
00:08:27,330 --> 00:08:31,030
If anything, we just don't know
what to do with the 0 values.

223
00:08:31,030 --> 00:08:33,270
So this is arguably
sort of incoherent,

224
00:08:33,270 --> 00:08:35,250
and the standard
response to it is

225
00:08:35,250 --> 00:08:38,850
to simply turn all of the
negative values into 0.

226
00:08:38,850 --> 00:08:41,830
And that's positive PMI
that's defined here.

227
00:08:41,830 --> 00:08:43,770
So we simply lop off
all the negative values

228
00:08:43,770 --> 00:08:45,810
by mapping them to 0.

229
00:08:45,810 --> 00:08:48,630
And that at least restores
the overall coherence

230
00:08:48,630 --> 00:08:50,100
of the claims where
all we're doing

231
00:08:50,100 --> 00:08:53,250
is reflecting the fact that
larger than expected counts

232
00:08:53,250 --> 00:08:58,210
have large positive PMI
and the rest are put in 0.

233
00:08:58,210 --> 00:09:00,550
Let's look briefly at a few
other reweighting schemes,

234
00:09:00,550 --> 00:09:01,858
starting with the t-test.

235
00:09:01,858 --> 00:09:03,400
The t-test is
something that you work

236
00:09:03,400 --> 00:09:05,290
with on the first
assignment to implement it.

237
00:09:05,290 --> 00:09:07,420
It turns out to be a very
good reweighting scheme.

238
00:09:07,420 --> 00:09:09,340
And I like it because
it obviously reflects

239
00:09:09,340 --> 00:09:11,890
many of the same intuitions
that guide the PMI

240
00:09:11,890 --> 00:09:15,010
and observed over
expected calculations.

241
00:09:15,010 --> 00:09:16,670
TF-IDF is quite different.

242
00:09:16,670 --> 00:09:18,580
So this is typically
performed on word

243
00:09:18,580 --> 00:09:21,430
by document matrices in
the context of information

244
00:09:21,430 --> 00:09:22,450
retrieval.

245
00:09:22,450 --> 00:09:24,160
Given some corpus
of documents D,

246
00:09:24,160 --> 00:09:27,310
we're going to say that the
term frequency for a given cell

247
00:09:27,310 --> 00:09:29,840
is that value divided by
the sum of all the values

248
00:09:29,840 --> 00:09:31,840
in the column, giving us
the kind of probability

249
00:09:31,840 --> 00:09:34,880
of the word given the
document that we're in.

250
00:09:34,880 --> 00:09:38,330
And then the IDF value is the
log of this quantity here.

251
00:09:38,330 --> 00:09:41,360
This is the number of
documents in our corpus

252
00:09:41,360 --> 00:09:44,210
that is the column
dimensionality divided

253
00:09:44,210 --> 00:09:47,220
by the number of documents
that contain the target word.

254
00:09:47,220 --> 00:09:49,530
And again we met log of 0 to 0.

255
00:09:49,530 --> 00:09:52,730
The TF-IDF is the product
of those two values.

256
00:09:52,730 --> 00:09:54,980
I think this can be
an outstanding method

257
00:09:54,980 --> 00:09:58,790
for very large sparse matrices,
like the Word Document one.

258
00:09:58,790 --> 00:10:02,540
Conversely it is typically
not well-behaved for very

259
00:10:02,540 --> 00:10:04,910
dense matrices, like
the word-by-word ones

260
00:10:04,910 --> 00:10:06,950
that we were favoring
in this course.

261
00:10:06,950 --> 00:10:09,530
The reason, this is IDF value.

262
00:10:09,530 --> 00:10:11,030
It's very unlikely
that you would

263
00:10:11,030 --> 00:10:14,330
have a word that appeared
literally in every document.

264
00:10:14,330 --> 00:10:18,350
However, in the context of very
dense word by word matrices,

265
00:10:18,350 --> 00:10:21,140
it is possible for
some words to co-occur

266
00:10:21,140 --> 00:10:23,120
with every single other
word, in which case

267
00:10:23,120 --> 00:10:26,187
you'll get an IDF of value
of 0, which is probably not

268
00:10:26,187 --> 00:10:28,520
the intended outcome for
something that's high frequency

269
00:10:28,520 --> 00:10:31,760
but might nonetheless be
important in the context

270
00:10:31,760 --> 00:10:33,770
of individual documents.

271
00:10:33,770 --> 00:10:35,930
So I'd probably steer
away from TF-IDF

272
00:10:35,930 --> 00:10:39,137
unless you're working with
a sparse matrix design.

273
00:10:39,137 --> 00:10:40,970
And then even further
afield from the things

274
00:10:40,970 --> 00:10:43,220
we've discussed, you might
explore using, for example,

275
00:10:43,220 --> 00:10:46,970
pairwise distance matrices where
I calculate the cosine distance

276
00:10:46,970 --> 00:10:49,430
between every pair of
words along the rows

277
00:10:49,430 --> 00:10:51,590
and form a matrix on that basis.

278
00:10:51,590 --> 00:10:55,100
Really different in its approach
and probably in its outcomes,

279
00:10:55,100 --> 00:10:58,380
but it could be
very interesting.

280
00:10:58,380 --> 00:11:00,110
Let's return to our
essential questions.

281
00:11:00,110 --> 00:11:02,193
Remember for each one of
these reweighting schemes

282
00:11:02,193 --> 00:11:04,560
we want to ask, how does
it compare to the raw count

283
00:11:04,560 --> 00:11:07,800
values, and what overall
distribution of values

284
00:11:07,800 --> 00:11:08,800
does it deliver?

285
00:11:08,800 --> 00:11:10,600
So let's do a bit of
an assessment of that.

286
00:11:10,600 --> 00:11:12,647
I'm working with
the giga5 matrix

287
00:11:12,647 --> 00:11:14,730
that you can load as part
of the course materials.

288
00:11:14,730 --> 00:11:19,080
That's Gigaword with a window
of 5 and a scaling of 1 over n.

289
00:11:19,080 --> 00:11:22,620
Up here in the left, I have the
raw counts, and the cell value

290
00:11:22,620 --> 00:11:25,020
along the x-axis and the
number of things that have

291
00:11:25,020 --> 00:11:27,490
that value along the y-axis.

292
00:11:27,490 --> 00:11:29,550
And you can see that
raw counts, it's

293
00:11:29,550 --> 00:11:31,300
a very difficult distribution.

294
00:11:31,300 --> 00:11:35,820
First of all, this goes all the
way up to about 100 million,

295
00:11:35,820 --> 00:11:37,350
and starting from 0.

296
00:11:37,350 --> 00:11:40,440
Most things have quantities
that are close to 0.

297
00:11:40,440 --> 00:11:42,480
And then you have this
very long thin tail

298
00:11:42,480 --> 00:11:44,790
of things that are
very high frequency.

299
00:11:44,790 --> 00:11:46,740
This highly skewed
distribution is

300
00:11:46,740 --> 00:11:49,140
difficult for many
machine-learning methods,

301
00:11:49,140 --> 00:11:52,560
both in terms of the skew
towards 0 and very low values

302
00:11:52,560 --> 00:11:55,807
and also in terms of the
range of these x-axis values.

303
00:11:55,807 --> 00:11:57,390
So we would like to
move away from it.

304
00:11:57,390 --> 00:11:59,640
That's one motivating reason.

305
00:11:59,640 --> 00:12:02,190
When we look at L2 norming
and probability distributions,

306
00:12:02,190 --> 00:12:03,930
they do kind of the same thing.

307
00:12:03,930 --> 00:12:07,530
They're constraining the cell
values to be between 0 and 1,

308
00:12:07,530 --> 00:12:10,590
or roughly about
between 0 and 1.

309
00:12:10,590 --> 00:12:12,840
But they still have a
heavy skew toward things

310
00:12:12,840 --> 00:12:15,300
that are very small in
their adjusted values

311
00:12:15,300 --> 00:12:17,520
and their reweighted values.

312
00:12:17,520 --> 00:12:21,210
Observed over expected is more
extreme in that as is TF-IDF.

313
00:12:21,210 --> 00:12:24,210
So again the observed
over expected values

314
00:12:24,210 --> 00:12:27,435
range quite high, up
to about almost 50,000

315
00:12:27,435 --> 00:12:29,310
which is somewhat better
than the raw counts,

316
00:12:29,310 --> 00:12:32,340
but it's still very large
in terms of its spread.

317
00:12:32,340 --> 00:12:35,040
And we still have that
heavy skew towards 0.

318
00:12:35,040 --> 00:12:37,932
TF-IDF solves the range
problem down here,

319
00:12:37,932 --> 00:12:40,140
because it's highly constrained
in the set of values,

320
00:12:40,140 --> 00:12:42,000
but it still has
a very heavy skew,

321
00:12:42,000 --> 00:12:45,830
looking a lot like the
raw count distribution.

322
00:12:45,830 --> 00:12:48,740
From this perspective, it
looks like PMI and positive PMI

323
00:12:48,740 --> 00:12:50,040
are really steps forward.

324
00:12:50,040 --> 00:12:53,600
First of all, for PMI the
distribution of cell values

325
00:12:53,600 --> 00:12:55,940
has this nice sort of
normal distribution.

326
00:12:55,940 --> 00:12:58,070
And the values themselves
are pretty constrained

327
00:12:58,070 --> 00:13:00,320
to like negative 10 to 10.

328
00:13:00,320 --> 00:13:02,990
And then for positive
PMI, we simply

329
00:13:02,990 --> 00:13:05,690
lop off all the negative
values and make it back to 0.

330
00:13:05,690 --> 00:13:09,170
So it's more skewed towards
0, but not nearly as skewed

331
00:13:09,170 --> 00:13:11,700
as all these other methods
that we're looking at.

332
00:13:11,700 --> 00:13:15,440
So this is looking like PMI,
and PPMI are good choices here,

333
00:13:15,440 --> 00:13:19,700
just from the point of view of
departing from the raw counts

334
00:13:19,700 --> 00:13:22,720
and giving us a
tractable distribution.

335
00:13:22,720 --> 00:13:24,700
There's another perspective
where we directly

336
00:13:24,700 --> 00:13:28,330
compare in these matrices the
co-occurrence count on log

337
00:13:28,330 --> 00:13:31,870
scale, so it's doable, with the
result the new weighted cell

338
00:13:31,870 --> 00:13:32,470
value.

339
00:13:32,470 --> 00:13:34,660
What we're looking
for here presumably

340
00:13:34,660 --> 00:13:38,020
is an overall lack
of correlation.

341
00:13:38,020 --> 00:13:40,570
I think we find that L2
norming and probabilities are

342
00:13:40,570 --> 00:13:41,860
pretty good on this score.

343
00:13:41,860 --> 00:13:44,440
They have kind of
low correlations.

344
00:13:44,440 --> 00:13:47,230
And they make good use of
a large part of the scale

345
00:13:47,230 --> 00:13:48,910
that they operate on.

346
00:13:48,910 --> 00:13:51,190
Observed over expected
has a low correlation

347
00:13:51,190 --> 00:13:53,650
with the cell counts,
which looks initially good,

348
00:13:53,650 --> 00:13:56,500
but it has its biggest problem
that the cell values are

349
00:13:56,500 --> 00:13:59,740
strangely distributed and this
correlation value might not

350
00:13:59,740 --> 00:14:01,690
even be especially
meaningful, given

351
00:14:01,690 --> 00:14:04,210
that we have a few outliers
and then a whole lot of things

352
00:14:04,210 --> 00:14:05,590
that are close to 0.

353
00:14:05,590 --> 00:14:09,040
And TF-IDF is frankly similar
low correlation but maybe not

354
00:14:09,040 --> 00:14:11,800
so trustworthy in terms
of that correlation value.

355
00:14:11,800 --> 00:14:14,650
Fundamentally, again, these look
like difficult distributions

356
00:14:14,650 --> 00:14:16,420
of values to work with.

357
00:14:16,420 --> 00:14:19,600
Again, PMI and positive
PMI look really good.

358
00:14:19,600 --> 00:14:21,550
Relatively low
correlations, so we've

359
00:14:21,550 --> 00:14:22,930
done something meaningful.

360
00:14:22,930 --> 00:14:25,330
And both of these are
making meaningful use

361
00:14:25,330 --> 00:14:28,780
of a substantial part
of the overall space

362
00:14:28,780 --> 00:14:29,530
that they operate.

363
00:14:29,530 --> 00:14:31,450
And we have lots of
different combinations

364
00:14:31,450 --> 00:14:34,930
of cell values and underlying
co-occurrence counts.

365
00:14:34,930 --> 00:14:37,420
Something of a correlation,
but that could be good.

366
00:14:37,420 --> 00:14:39,590
But we're not locked
into that correlation.

367
00:14:39,590 --> 00:14:41,920
So we've done
something meaningful.

368
00:14:41,920 --> 00:14:43,960
To wrap up, let's do
some relationships

369
00:14:43,960 --> 00:14:46,140
and generalizations,
just some reminders here.

370
00:14:46,140 --> 00:14:48,730
So a theme running through
nearly all of these schemes

371
00:14:48,730 --> 00:14:50,230
is that we want to
reweight the cell

372
00:14:50,230 --> 00:14:52,240
value relative to
the values we expect,

373
00:14:52,240 --> 00:14:53,720
given the row and the column.

374
00:14:53,720 --> 00:14:55,630
And we would like
to make use of both

375
00:14:55,630 --> 00:14:58,290
of those notions of context.

376
00:14:58,290 --> 00:15:01,020
The magnitude of the counts
might be important, just

377
00:15:01,020 --> 00:15:04,500
think about how 1, 10
as a bit of evidence

378
00:15:04,500 --> 00:15:07,320
and 1,000, 10,000
as a bit of evidence

379
00:15:07,320 --> 00:15:09,630
might be very different
situations in terms

380
00:15:09,630 --> 00:15:11,910
of the evidence that
you have gathered.

381
00:15:11,910 --> 00:15:14,160
Creating probability
distributions and lengths

382
00:15:14,160 --> 00:15:16,710
normalizing will
obscure that difference.

383
00:15:16,710 --> 00:15:19,170
And that might be something
that you want to dwell on.

384
00:15:19,170 --> 00:15:22,070


385
00:15:22,070 --> 00:15:24,170
PMI and its variants
will amplify the values

386
00:15:24,170 --> 00:15:26,690
of counts that are tiny
relative to their rows

387
00:15:26,690 --> 00:15:28,168
and their columns.

388
00:15:28,168 --> 00:15:30,710
That could be good because that
might be what you want to do,

389
00:15:30,710 --> 00:15:33,320
find the things that are
really important and unusual.

390
00:15:33,320 --> 00:15:34,980
Unfortunately,
with language data,

391
00:15:34,980 --> 00:15:39,360
we have to watch out
that they might be noise.

392
00:15:39,360 --> 00:15:41,970
And finally, TF-IDF
severely punishes words

393
00:15:41,970 --> 00:15:43,530
that appear in many documents.

394
00:15:43,530 --> 00:15:45,580
It behaves oddly
for dense matrices,

395
00:15:45,580 --> 00:15:47,850
which can include the
word by word matrices

396
00:15:47,850 --> 00:15:48,995
that we're working with.

397
00:15:48,995 --> 00:15:50,370
So you might
proceed with caution

398
00:15:50,370 --> 00:15:52,290
with that particular
reweighting scheme

399
00:15:52,290 --> 00:15:54,883
in the context of this course.

400
00:15:54,883 --> 00:15:56,050
Finally, some code snippets.

401
00:15:56,050 --> 00:15:58,780
I'm just showing off that
our VSM module in the course

402
00:15:58,780 --> 00:16:01,750
repository makes it really
easy to do these reweighting

403
00:16:01,750 --> 00:16:02,620
schemes, a lot--

404
00:16:02,620 --> 00:16:05,800
all the ones that we've
talked about and more in fact.

405
00:16:05,800 --> 00:16:08,470
And returning to the end of
our vector comparison method,

406
00:16:08,470 --> 00:16:11,080
you might recall that I
looked at the neighbors

407
00:16:11,080 --> 00:16:13,060
of "bad" in this yelp5 matrix.

408
00:16:13,060 --> 00:16:14,470
And it really didn't look good.

409
00:16:14,470 --> 00:16:17,500
This does not look especially
semantically coherent.

410
00:16:17,500 --> 00:16:19,630
When I take those
underlying counts

411
00:16:19,630 --> 00:16:22,140
and I just adjust
them by positive PMI,

412
00:16:22,140 --> 00:16:24,760
I start to see something
that looks quite semantically

413
00:16:24,760 --> 00:16:25,395
coherent.

414
00:16:25,395 --> 00:16:26,770
And I think we're
starting to see

415
00:16:26,770 --> 00:16:28,258
the promise of these methods.

416
00:16:28,258 --> 00:16:29,800
And this is really
just the beginning

417
00:16:29,800 --> 00:16:31,510
in terms of surfacing
semantically

418
00:16:31,510 --> 00:16:33,430
coherent and
interesting information

419
00:16:33,430 --> 00:16:35,970
from these underlying counts.

420
00:16:35,970 --> 00:16:41,000


