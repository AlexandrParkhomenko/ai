1
00:00:00,000 --> 00:00:03,882


2
00:00:03,882 --> 00:00:05,340
CHRISTOPHER POTTS:
Hello, everyone.

3
00:00:05,340 --> 00:00:07,730
Welcome to part 6 in our
series on supervised sentiment

4
00:00:07,730 --> 00:00:08,330
analysis.

5
00:00:08,330 --> 00:00:11,000
This screencast is going to
cover two important methods

6
00:00:11,000 --> 00:00:14,030
in this space, hyperparameter
search and classifier

7
00:00:14,030 --> 00:00:15,900
comparisons.

8
00:00:15,900 --> 00:00:17,680
So let's begin with
hyperparameter search.

9
00:00:17,680 --> 00:00:20,390
And first, I'll just
offer the rationale.

10
00:00:20,390 --> 00:00:22,850
Let's say that the
parameters of a model

11
00:00:22,850 --> 00:00:25,160
are those whose values
are learned as part

12
00:00:25,160 --> 00:00:26,850
of optimizing the model itself.

13
00:00:26,850 --> 00:00:29,000
So for the classifiers
we've been studying,

14
00:00:29,000 --> 00:00:31,640
the parameters are
really just the weights

15
00:00:31,640 --> 00:00:34,675
that you learn on each of
the individual features.

16
00:00:34,675 --> 00:00:36,800
And those are the things
that are directly targeted

17
00:00:36,800 --> 00:00:39,053
by the optimization process.

18
00:00:39,053 --> 00:00:40,970
The parameters of a model
are typically pretty

19
00:00:40,970 --> 00:00:42,980
crisply defined because
they kind of follow

20
00:00:42,980 --> 00:00:45,680
from the structure
mathematically of the model

21
00:00:45,680 --> 00:00:47,450
under investigation.

22
00:00:47,450 --> 00:00:49,730
Much more diffuse are
the hyperparameters.

23
00:00:49,730 --> 00:00:51,650
We can say the
hyperparameters of a model

24
00:00:51,650 --> 00:00:54,890
are any settings that are
outside of the optimization

25
00:00:54,890 --> 00:00:56,690
process mentioned in 1.

26
00:00:56,690 --> 00:00:59,660
So examples from models
we've seen, our GloVe and LSA

27
00:00:59,660 --> 00:01:01,850
have that
dimensionality setting.

28
00:01:01,850 --> 00:01:03,530
The model itself
gives you no guidance

29
00:01:03,530 --> 00:01:06,050
about what to choose
for the dimensionality.

30
00:01:06,050 --> 00:01:08,030
And the dimensionality
is not selected

31
00:01:08,030 --> 00:01:10,460
as part of the optimization
of the model itself.

32
00:01:10,460 --> 00:01:13,580
You have to choose it via
some external mechanism,

33
00:01:13,580 --> 00:01:16,382
making it a hyperparameter.

34
00:01:16,382 --> 00:01:18,590
And GloVe actually has two
other additional prominent

35
00:01:18,590 --> 00:01:20,880
hyperparameters, xmax and alpha.

36
00:01:20,880 --> 00:01:23,360
Again, those are not
optimized by the model.

37
00:01:23,360 --> 00:01:26,510
You have to select them via
some external mechanism.

38
00:01:26,510 --> 00:01:28,730
And for the classifiers
that we've been studying,

39
00:01:28,730 --> 00:01:30,380
we have regularization terms.

40
00:01:30,380 --> 00:01:32,210
Those are classic
hyperparameters.

41
00:01:32,210 --> 00:01:33,830
If you have a deep
classifier, then

42
00:01:33,830 --> 00:01:35,990
the hidden dimensionalities
in the model

43
00:01:35,990 --> 00:01:38,120
could also be considered
hyperparameters.

44
00:01:38,120 --> 00:01:41,750
Learning rates, any core feature
of the optimization method

45
00:01:41,750 --> 00:01:44,420
itself could be considered
hyperparameters.

46
00:01:44,420 --> 00:01:46,280
And even things that
might be considered

47
00:01:46,280 --> 00:01:49,040
kind of architectural,
like the activation

48
00:01:49,040 --> 00:01:51,020
function and the
deep classifier,

49
00:01:51,020 --> 00:01:54,202
you might think of it as kind of
an intrinsic part of the model

50
00:01:54,202 --> 00:01:55,160
that you're evaluating.

51
00:01:55,160 --> 00:01:58,350
But since it's an easy choice
point for us at this point,

52
00:01:58,350 --> 00:02:00,890
you'll be tempted to explore
a few different options

53
00:02:00,890 --> 00:02:03,200
for that particular
architectural choice.

54
00:02:03,200 --> 00:02:05,960
And in that way, it could
become a hyperparameter.

55
00:02:05,960 --> 00:02:08,960
And at this point, even
the optimization methods

56
00:02:08,960 --> 00:02:11,270
could also emerge
as a hyperparameter

57
00:02:11,270 --> 00:02:14,060
that you would like
to do search over.

58
00:02:14,060 --> 00:02:16,460
And so forth and so
on, you should probably

59
00:02:16,460 --> 00:02:19,370
take a fairly expansive view
of what the hyperparameters

60
00:02:19,370 --> 00:02:22,160
of your model are if you can.

61
00:02:22,160 --> 00:02:24,350
Now here's the crux
of the argument.

62
00:02:24,350 --> 00:02:27,170
Hyperparameter optimization
is crucial to building

63
00:02:27,170 --> 00:02:28,370
a persuasive argument.

64
00:02:28,370 --> 00:02:31,490
Fundamentally, for any
kind of comparison we make,

65
00:02:31,490 --> 00:02:35,180
we want to put every model
in its very best light.

66
00:02:35,180 --> 00:02:37,550
We could take it for granted
that for any sufficiently

67
00:02:37,550 --> 00:02:40,952
complicated model there's some
setting of its hyperparameters

68
00:02:40,952 --> 00:02:42,410
that's kind of
degenerate and would

69
00:02:42,410 --> 00:02:44,653
make the model look very bad.

70
00:02:44,653 --> 00:02:46,070
And so you certainly
wouldn't want

71
00:02:46,070 --> 00:02:48,920
to do any comparisons against
that really problematic set

72
00:02:48,920 --> 00:02:50,000
of choices.

73
00:02:50,000 --> 00:02:51,840
Rather, what we
want to do is say,

74
00:02:51,840 --> 00:02:54,140
let's put all the models
in their best light

75
00:02:54,140 --> 00:02:56,390
by choosing optimal
hyperparameters for them

76
00:02:56,390 --> 00:02:57,862
to the best of our ability.

77
00:02:57,862 --> 00:03:00,320
And then we can say that one
model is better than the other

78
00:03:00,320 --> 00:03:05,487
if it emerges victorious in
that very rigorous setting.

79
00:03:05,487 --> 00:03:07,820
And the final thing I'll say
about this methodologically

80
00:03:07,820 --> 00:03:10,640
is that, of course, all
hyperparameter tuning

81
00:03:10,640 --> 00:03:13,760
must be done only on train
and development data.

82
00:03:13,760 --> 00:03:17,210
You can consider that
all fair game in terms

83
00:03:17,210 --> 00:03:19,250
of using it however
you want to choose

84
00:03:19,250 --> 00:03:21,080
the optimal hyperparameters.

85
00:03:21,080 --> 00:03:23,300
But once that choice
is set, it is fixed.

86
00:03:23,300 --> 00:03:25,910
And those are the parameters
that you use at test time.

87
00:03:25,910 --> 00:03:27,920
And that is the
fundamental evaluation

88
00:03:27,920 --> 00:03:30,890
that you would use for any
kind of model comparison.

89
00:03:30,890 --> 00:03:34,010
And at no point should you be
tuning these hyperparameters

90
00:03:34,010 --> 00:03:35,670
on the test data itself.

91
00:03:35,670 --> 00:03:39,720
That would be
completely illegitimate.

92
00:03:39,720 --> 00:03:41,513
I hope we've made
it really easy to do

93
00:03:41,513 --> 00:03:42,930
this kind of
hyperparameter search

94
00:03:42,930 --> 00:03:44,305
in the context of
the work you're

95
00:03:44,305 --> 00:03:46,450
doing for supervised
sentiment analysis.

96
00:03:46,450 --> 00:03:49,470
Here are some code snippets
that show how that can happen.

97
00:03:49,470 --> 00:03:51,810
In loading my libraries,
I have a pointer

98
00:03:51,810 --> 00:03:53,130
to our sentiment data.

99
00:03:53,130 --> 00:03:55,500
And here I have a fixed
feature function, which is just

100
00:03:55,500 --> 00:03:57,990
a unigram feature function.

101
00:03:57,990 --> 00:04:01,260
The change happens
inside the model wrapper.

102
00:04:01,260 --> 00:04:03,180
Whereas before,
essentially, all we did

103
00:04:03,180 --> 00:04:05,100
was set up a logistic
regression model

104
00:04:05,100 --> 00:04:08,730
and then call its fit method,
here we set up that model

105
00:04:08,730 --> 00:04:11,830
but also established a
grid of hyperparameters.

106
00:04:11,830 --> 00:04:13,230
These are different
choice points

107
00:04:13,230 --> 00:04:14,830
for this logistic
regression model

108
00:04:14,830 --> 00:04:16,829
like whether or
not I have a bias

109
00:04:16,829 --> 00:04:20,310
term, the value of the
regularization parameter,

110
00:04:20,310 --> 00:04:23,040
and even the algorithm
used for regularization

111
00:04:23,040 --> 00:04:25,320
itself, L1 or L2.

112
00:04:25,320 --> 00:04:28,170
The model will explore the
full grid of these options.

113
00:04:28,170 --> 00:04:31,080
It's going to do five-fold
cross validation, so test one

114
00:04:31,080 --> 00:04:34,320
each five times on different
splits of the data.

115
00:04:34,320 --> 00:04:36,850
And in that very
long search process,

116
00:04:36,850 --> 00:04:39,870
it will find what it takes
to be the best setting of all

117
00:04:39,870 --> 00:04:42,420
of these hyperparameters,
of all the combinations

118
00:04:42,420 --> 00:04:45,510
that can logically be set.

119
00:04:45,510 --> 00:04:48,340
And that is the model that we
finally return here, right?

120
00:04:48,340 --> 00:04:50,340
So now you can see the
value of having a wrapper

121
00:04:50,340 --> 00:04:51,540
around these fit methods.

122
00:04:51,540 --> 00:04:54,510
Because then I could do
all of this extra work

123
00:04:54,510 --> 00:04:57,720
without changing the interface
to sst.experiment at all.

124
00:04:57,720 --> 00:05:01,230
The experiments look just as
they did in the previous mode.

125
00:05:01,230 --> 00:05:03,540
It's just that they will
take a lot longer because you

126
00:05:03,540 --> 00:05:06,000
are running dozens and
dozens of experiments

127
00:05:06,000 --> 00:05:08,860
as part of this
exhaustive search of all

128
00:05:08,860 --> 00:05:10,930
the possible settings.

129
00:05:10,930 --> 00:05:13,380
OK, part 2 is
classifier comparison.

130
00:05:13,380 --> 00:05:15,180
Once again, begin
with the rationale.

131
00:05:15,180 --> 00:05:17,850
Suppose you've assessed
a baseline Model B

132
00:05:17,850 --> 00:05:21,300
and your favorite model M. And
your chosen assessment metric

133
00:05:21,300 --> 00:05:24,850
favors M. And this seems like
a little victory for you.

134
00:05:24,850 --> 00:05:27,270
But you should still ask
yourself, is M really better,

135
00:05:27,270 --> 00:05:28,560
right?

136
00:05:28,560 --> 00:05:30,780
Now if the difference
between B and M

137
00:05:30,780 --> 00:05:32,887
is clearly of
practical significance,

138
00:05:32,887 --> 00:05:35,220
then you might not need to
do anything beyond presenting

139
00:05:35,220 --> 00:05:36,450
the numbers, right?

140
00:05:36,450 --> 00:05:38,820
If each one of your
classification decisions

141
00:05:38,820 --> 00:05:41,520
corresponds to something
really important in the world

142
00:05:41,520 --> 00:05:43,410
and your classifier
makes thousands

143
00:05:43,410 --> 00:05:45,570
more good predictions
than the other model,

144
00:05:45,570 --> 00:05:48,030
that might be enough
for the argument.

145
00:05:48,030 --> 00:05:49,950
But even in that
situation, you might

146
00:05:49,950 --> 00:05:52,540
ask whether there's variation
in how these two models

147
00:05:52,540 --> 00:05:53,520
B and M perform.

148
00:05:53,520 --> 00:05:55,020
Did you just get
lucky when you saw

149
00:05:55,020 --> 00:05:56,645
what looked like a
practical difference

150
00:05:56,645 --> 00:05:59,670
and with minor changes to the
initialization or something,

151
00:05:59,670 --> 00:06:01,650
you would see very
different outcomes?

152
00:06:01,650 --> 00:06:03,780
If the answer is
possibly yes, then you

153
00:06:03,780 --> 00:06:08,280
might still want to do some
kind of classifier comparison.

154
00:06:08,280 --> 00:06:11,250
Now there's this nice
paper by Demsar 2006

155
00:06:11,250 --> 00:06:14,910
that advises using the Wilcoxon
signed-rank test for situations

156
00:06:14,910 --> 00:06:17,850
in which you can afford
to repeatedly assess

157
00:06:17,850 --> 00:06:20,835
your two models B and M on
different train test splits,

158
00:06:20,835 --> 00:06:22,050
right?

159
00:06:22,050 --> 00:06:24,880
And we'll talk later in the
term about the precise rationale

160
00:06:24,880 --> 00:06:25,380
for this.

161
00:06:25,380 --> 00:06:28,470
But the idea is just that you
would do a lot of experiments

162
00:06:28,470 --> 00:06:30,750
on slightly different
views of your data

163
00:06:30,750 --> 00:06:32,700
and then kind of
average across them

164
00:06:32,700 --> 00:06:37,920
to get a sense for how the two
models compare with each other.

165
00:06:37,920 --> 00:06:42,225
In situations where you can't
repeatedly assess B and M,

166
00:06:42,225 --> 00:06:45,000
McNemar's test is a
reasonable alternative.

167
00:06:45,000 --> 00:06:47,820
It operates on the confusion
matrices produced by the two

168
00:06:47,820 --> 00:06:50,820
models, testing the null
hypothesis that the two models

169
00:06:50,820 --> 00:06:53,220
have the same error rate.

170
00:06:53,220 --> 00:06:55,250
The reason you might
opt for McNemar's test

171
00:06:55,250 --> 00:06:57,360
is, for example, if you're
doing a deep learning

172
00:06:57,360 --> 00:07:00,990
experiment where all the models
take a few weeks to optimize.

173
00:07:00,990 --> 00:07:02,430
Then of course,
you can't probably

174
00:07:02,430 --> 00:07:06,280
afford to do dozens and dozens
of experiments with each one.

175
00:07:06,280 --> 00:07:09,030
So you might be compelled to
use McNemar's based on one

176
00:07:09,030 --> 00:07:10,830
single run of the two models.

177
00:07:10,830 --> 00:07:12,960
It's a much weaker argument
because, of course,

178
00:07:12,960 --> 00:07:15,630
precisely the point is
that we might see variation

179
00:07:15,630 --> 00:07:16,650
across different runs.

180
00:07:16,650 --> 00:07:18,180
And McNemar's is
not really going

181
00:07:18,180 --> 00:07:20,760
to grapple with that in
the way that the Wilcoxon

182
00:07:20,760 --> 00:07:22,650
signed-rank test will.

183
00:07:22,650 --> 00:07:25,235
But this is arguably better
than nothing in most situations.

184
00:07:25,235 --> 00:07:28,710
So you might default to
McNemar's if the Wilcoxon

185
00:07:28,710 --> 00:07:30,815
is too expensive.

186
00:07:30,815 --> 00:07:32,190
And let me just
show you how easy

187
00:07:32,190 --> 00:07:34,230
this can be in the
context of our code base.

188
00:07:34,230 --> 00:07:37,800
So by way of illustration, what
we're essentially going to do

189
00:07:37,800 --> 00:07:41,730
is compare logistic
regression and naive Bayes.

190
00:07:41,730 --> 00:07:44,620
I encourage you, when you're
doing these comparisons,

191
00:07:44,620 --> 00:07:47,200
to have only one
point of variation.

192
00:07:47,200 --> 00:07:48,960
So we're going to fix
the data and we're

193
00:07:48,960 --> 00:07:50,730
going to fix the
feature function

194
00:07:50,730 --> 00:07:53,790
and compare only the
model architectures.

195
00:07:53,790 --> 00:07:55,380
You could separately
say I'm going

196
00:07:55,380 --> 00:07:58,170
to have a single fixed model
like logistic regression

197
00:07:58,170 --> 00:08:01,050
and explore a few different
feature functions.

198
00:08:01,050 --> 00:08:03,870
But I would advise against
exploring two different feature

199
00:08:03,870 --> 00:08:06,030
functions as combined
with two different models

200
00:08:06,030 --> 00:08:08,530
because when you observe
differences in the end,

201
00:08:08,530 --> 00:08:11,880
you won't be sure whether that
was caused by the model choice

202
00:08:11,880 --> 00:08:13,360
or by the feature functions.

203
00:08:13,360 --> 00:08:18,220
We want to isolate these things
and do systematic comparisons.

204
00:08:18,220 --> 00:08:20,220
So here, I'm going to do
a systematic comparison

205
00:08:20,220 --> 00:08:22,710
of logistic regression
and naive Bayes

206
00:08:22,710 --> 00:08:25,650
on the SST using
the Wilcoxon test.

207
00:08:25,650 --> 00:08:26,880
And here's the setup.

208
00:08:26,880 --> 00:08:29,620
The function is
sst_compare_models.

209
00:08:29,620 --> 00:08:31,992
I point it to my training data.

210
00:08:31,992 --> 00:08:33,450
You can have two
feature functions.

211
00:08:33,450 --> 00:08:36,330
But in that case, you should
have just one model wrapper.

212
00:08:36,330 --> 00:08:38,705
Here, I've got one feature
function used for both models.

213
00:08:38,705 --> 00:08:40,455
And I'll have these
two different wrappers

214
00:08:40,455 --> 00:08:42,120
corresponding to the
evaluation that I

215
00:08:42,120 --> 00:08:45,300
want to do of those
two model classes.

216
00:08:45,300 --> 00:08:47,760
I'm going to use the
Wilcoxon as advised.

217
00:08:47,760 --> 00:08:52,770
I'll do 10 trials of each on the
train size of 70% of the data.

218
00:08:52,770 --> 00:08:54,660
And as always, in
this setting, I'll

219
00:08:54,660 --> 00:08:57,480
use the macro F1 as my score.

220
00:08:57,480 --> 00:08:59,010
So what this will
do internally is

221
00:08:59,010 --> 00:09:02,500
run 10 experiments on
different train test

222
00:09:02,500 --> 00:09:04,440
splits for each one
of these models.

223
00:09:04,440 --> 00:09:09,300
That gives us a score vector,
10 numbers for each model.

224
00:09:09,300 --> 00:09:10,890
And then what the
Wilcoxon is doing

225
00:09:10,890 --> 00:09:12,930
is comparing whether
the-- or assessing

226
00:09:12,930 --> 00:09:15,420
whether the means of
those two score vectors

227
00:09:15,420 --> 00:09:18,327
are statistically
significantly different.

228
00:09:18,327 --> 00:09:20,160
And here it looks like
we have some evidence

229
00:09:20,160 --> 00:09:23,100
that we can reject the null
hypothesis that these models

230
00:09:23,100 --> 00:09:26,220
are identical, which is
presumably the argument that we

231
00:09:26,220 --> 00:09:28,260
were trying to build.

232
00:09:28,260 --> 00:09:30,030
Now, of course,
that's very expensive

233
00:09:30,030 --> 00:09:32,797
because we had to run 20
experiments in this situation.

234
00:09:32,797 --> 00:09:34,380
And of course, you
could run many more

235
00:09:34,380 --> 00:09:37,590
if you were also doing
hyperparameter tuning as part

236
00:09:37,590 --> 00:09:39,810
of your experimental workflow.

237
00:09:39,810 --> 00:09:42,690
So in situations where you can't
afford to do something that

238
00:09:42,690 --> 00:09:44,700
involves so many
experiments, as I said,

239
00:09:44,700 --> 00:09:47,310
you couldn't default
to McNemar's.

240
00:09:47,310 --> 00:09:50,550
That is included
in utils.mcnemar.

241
00:09:50,550 --> 00:09:53,400
And the return values
of SST experiment

242
00:09:53,400 --> 00:09:55,200
will give you all the
information you need.

243
00:09:55,200 --> 00:09:58,520
Essentially from McNemar's you
need the actual goal vector

244
00:09:58,520 --> 00:10:00,570
of labels and then
the two vectors

245
00:10:00,570 --> 00:10:02,700
of predictions for each
one of your experiments.

246
00:10:02,700 --> 00:10:04,860
So that's a simple
alternative in the situation

247
00:10:04,860 --> 00:10:08,330
in which Wilcoxon's
was just too expensive.

248
00:10:08,330 --> 00:10:12,000


