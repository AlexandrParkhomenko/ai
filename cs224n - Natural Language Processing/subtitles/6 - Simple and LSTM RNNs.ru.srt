1
00:00:05,600 --> 00:00:10,800
Хорошо, привет всем, добро пожаловать обратно в cs224n,

2
00:00:10,800 --> 00:00:13,840
так что сегодня это очень ключевая лекция, в которой

3
00:00:13,840 --> 00:00:16,400
мы рассмотрим ряд важных

4
00:00:16,400 --> 00:00:19,359
тем для нейронных сетей, особенно

5
00:00:19,359 --> 00:00:21,600
применительно к обработке естественного языка,

6
00:00:21,600 --> 00:00:23,519
поэтому в самом конце прошлого раза я

7
00:00:23,519 --> 00:00:26,000
начал с текущих нейронных сетей, поэтому

8
00:00:26,000 --> 00:00:27,920
мы '  Я подробно расскажу о текущих

9
00:00:27,920 --> 00:00:30,160
нейронных сетях в первой части

10
00:00:30,160 --> 00:00:32,079
класса, и

11
00:00:32,079 --> 00:00:34,719
мы выделим языковые модели, но

12
00:00:34,719 --> 00:00:37,520
затем также выйдем за рамки этого, а

13
00:00:37,520 --> 00:00:40,160
затем посмотрим на более продвинутые виды

14
00:00:40,160 --> 00:00:42,239
рекуррентных нейронных сетей ближе к

15
00:00:42,239 --> 00:00:44,879
конечной части класса.

16
00:00:44,879 --> 00:00:47,520
эм, я просто хотел сказать пару слов

17
00:00:47,520 --> 00:00:50,000
перед тем, как приступить к окончательному

18
00:00:50,000 --> 00:00:52,320
проекту, так что, надеюсь, к настоящему времени вы

19
00:00:52,320 --> 00:00:54,879
начали смотреть на третье задание,

20
00:00:54,879 --> 00:00:56,320
которое является серединой из пяти

21
00:00:56,320 --> 00:00:57,920
заданий для первой половины

22
00:00:57,920 --> 00:01:00,320
курса, а затем - для второй.

23
00:01:00,320 --> 00:01:02,320
Конечно, большая часть ваших усилий уходит

24
00:01:02,320 --> 00:01:04,080
на финальный проект,

25
00:01:04,080 --> 00:01:06,640
поэтому на следующей неделе в четверг лекция

26
00:01:06,640 --> 00:01:08,479
будет посвящена финальным проектам,

27
00:01:08,479 --> 00:01:10,479
выбору финального проекта и советам по

28
00:01:10,479 --> 00:01:13,360
окончательной оценке.  проектов и т.

29
00:01:25,200 --> 00:01:26,880
думают о финальных проектах, вы можете

30
00:01:26,880 --> 00:01:29,840
найти некоторую информацию на веб-сайте, но

31
00:01:29,840 --> 00:01:31,439
обратите внимание, что информация, которая есть на

32
00:01:31,439 --> 00:01:34,400
данный момент, все еще является прошлогодней

33
00:01:34,400 --> 00:01:36,240
и будет обновляться в течение

34
00:01:36,240 --> 00:01:37,840
следующей недели,

35
00:01:37,840 --> 00:01:40,000
мы также поговорим о наставниках проекта, если

36
00:01:40,000 --> 00:01:42,479
вы '  У меня есть идеи о людях, которые

37
00:01:42,479 --> 00:01:44,640
самостоятельно могут стать наставниками, и

38
00:01:44,640 --> 00:01:46,799
сейчас самое время спросить их об этом, и

39
00:01:46,799 --> 00:01:48,159
мы как бы поговорим о том, какие

40
00:01:48,159 --> 00:01:51,040
альтернативы

41
00:01:51,040 --> 00:01:52,960
приемлемы, так что на

42
00:01:52,960 --> 00:01:54,399
прошлой лекции

43
00:01:54,399 --> 00:01:57,360
я представил идею языка  модели

44
00:01:57,360 --> 00:01:59,840
настолько вероятностные модели, которые предсказывают

45
00:01:59,840 --> 00:02:02,240
вероятность следующих слов после последовательности слов,

46
00:02:02,240 --> 00:02:04,640
а затем мы посмотрели на

47
00:02:04,640 --> 00:02:06,719
модели языка инграмм и начали с

48
00:02:06,719 --> 00:02:09,520
текущих моделей нейронных сетей, поэтому сегодня

49
00:02:09,520 --> 00:02:11,520
мы собираемся поговорить больше о

50
00:02:11,520 --> 00:02:14,400
простых RNN, которые мы видели ранее  мы говорим об

51
00:02:14,400 --> 00:02:17,280
обучении RNN и использовании RNN, но затем

52
00:02:17,280 --> 00:02:19,680
мы также рассмотрим проблемы,

53
00:02:19,680 --> 00:02:22,319
которые возникают с RNN, и способы

54
00:02:22,319 --> 00:02:24,720
их решения, которые будут мотивировать более

55
00:02:24,720 --> 00:02:26,879
сложную архитектуру RNN под названием

56
00:02:26,879 --> 00:02:29,520
lstms, и мы поговорим о других более

57
00:02:29,520 --> 00:02:33,040
сложных параметрах RNN.  -направленные РНН

58
00:02:33,040 --> 00:02:35,680
и многослойные РНН,

59
00:02:35,680 --> 00:02:38,000
то в следующий вторник

60
00:02:38,000 --> 00:02:39,840
мы, по сути, собираемся

61
00:02:39,840 --> 00:02:43,040
использовать дальнейшие эксплойты и построим архитектуры на основе RNN,

62
00:02:43,040 --> 00:02:45,120
которые мы

63
00:02:45,120 --> 00:02:47,200
рассматривали, чтобы обсудить, как построить

64
00:02:47,200 --> 00:02:49,760
новую систему машинного перевода

65
00:02:49,760 --> 00:02:52,160
с последовательностью в последовательность и

66
00:02:52,160 --> 00:02:55,200
внимательно и эффективно, поскольку эта

67
00:02:55,200 --> 00:02:57,840
модель - это то, что вы будете использовать в задании,

68
00:02:57,840 --> 00:02:59,599
но это также означает, что вы будете использовать

69
00:02:59,599 --> 00:03:00,879
все, о чем мы

70
00:03:00,879 --> 00:03:03,360
говорим сегодня,

71
00:03:03,360 --> 00:03:05,680
хорошо, так что, если вы помните из прошлого раза,

72
00:03:05,680 --> 00:03:08,640
это был  идея простой рекуррентной

73
00:03:08,640 --> 00:03:11,760
языковой модели нейронной сети, поэтому у нас

74
00:03:11,760 --> 00:03:15,200
была последовательность слов в качестве нашего контекста, для

75
00:03:15,200 --> 00:03:17,599
которого мы искали вложения слов,

76
00:03:17,599 --> 00:03:19,360
а затем их текущая модель нейронной сети

77
00:03:19,360 --> 00:03:22,800
запускала этот повторяющийся слой,

78
00:03:22,800 --> 00:03:25,920
где  в каждой точке у нас есть предыдущее

79
00:03:25,920 --> 00:03:28,879
скрытое состояние, которое может быть просто нулем

80
00:03:28,879 --> 00:03:31,120
в начале последовательности,

81
00:03:31,120 --> 00:03:34,239
и вы вводите его

82
00:03:34,239 --> 00:03:36,319
в следующее скрытое состояние, предыдущее

83
00:03:36,319 --> 00:03:39,360
скрытое состояние и кодирование

84
00:03:39,360 --> 00:03:42,480
и преобразование кодирования слова

85
00:03:42,480 --> 00:03:44,640
с помощью этого рекуррентного уравнения нейронной сети

86
00:03:44,640 --> 00:03:46,799
что у меня слева, это

87
00:03:46,799 --> 00:03:49,040
очень центральное место, и на его основе вы

88
00:03:49,040 --> 00:03:52,239
вычисляете новое скрытое представление

89
00:03:52,239 --> 00:03:54,159
для следующего временного шага,

90
00:03:54,159 --> 00:03:56,879
и вы можете повторять это для

91
00:03:56,879 --> 00:03:59,120
последовательных временных шагов.Теперь

92
00:03:59,120 --> 00:04:00,400
мы также

93
00:04:00,400 --> 00:04:02,640
обычно хотим, чтобы наши рекуррентные нейронные

94
00:04:02,640 --> 00:04:05,760
сети производили выходные данные, поэтому

95
00:04:05,760 --> 00:04:08,400
я показываю только  это в конце здесь, но на

96
00:04:08,400 --> 00:04:11,120
каждом временном шаге мы также собираемся

97
00:04:11,120 --> 00:04:13,680
генерировать вывод, и для этого

98
00:04:13,680 --> 00:04:16,880
мы загружаем скрытый слой в

99
00:04:16,880 --> 00:04:18,720
мягкий максимальный слой, поэтому мы делаем еще одну

100
00:04:18,720 --> 00:04:21,440
матрицу, умножаем, добавляем на смещение, положив  это

101
00:04:21,440 --> 00:04:23,680
через уравнение мягкого максимума, и это

102
00:04:23,680 --> 00:04:25,280
затем даст распределение вероятностей

103
00:04:25,280 --> 00:04:28,080
по словам, и мы можем использовать

104
00:04:28,080 --> 00:04:30,320
это, чтобы предсказать, насколько вероятно, что

105
00:04:30,320 --> 00:04:33,040
разные слова будут встречаться после того,

106
00:04:33,040 --> 00:04:36,560
как ученики откроют их

107
00:04:36,800 --> 00:04:39,520
хорошо, я не представил эту

108
00:04:39,520 --> 00:04:41,120
модель, но я действительно не разбирался

109
00:04:41,120 --> 00:04:43,680
в особенностях того, как мы обучаем эту

110
00:04:43,680 --> 00:04:46,479
модель, как мы ее используем, и оцениваем ее, так что

111
00:04:46,479 --> 00:04:50,240
позвольте мне пройти через это сейчас,

112
00:04:50,240 --> 00:04:53,360
так вот как мы обучаем  В языковой

113
00:04:53,360 --> 00:04:55,919
модели rnn мы получаем большой корпус текста, просто

114
00:04:55,919 --> 00:04:58,400
много текста, и поэтому мы можем рассматривать это как

115
00:04:58,400 --> 00:05:02,880
просто длинную последовательность слов от x1 до xt,

116
00:05:02,880 --> 00:05:05,120
и то, что мы собираемся сделать, это

117
00:05:05,120 --> 00:05:07,520
передать ее в rnnlm,

118
00:05:07,520 --> 00:05:10,240
поэтому для каждой позиции мы  мы собираемся взять

119
00:05:10,240 --> 00:05:14,080
префиксы этой последовательности и на основе

120
00:05:14,080 --> 00:05:16,560
каждого префикса мы собираемся

121
00:05:16,560 --> 00:05:17,680
предсказать

122
00:05:17,680 --> 00:05:20,400
распределение вероятностей для

123
00:05:20,400 --> 00:05:23,520
следующего слова,

124
00:05:23,520 --> 00:05:26,720
а затем мы собираемся обучить нашу модель,

125
00:05:26,720 --> 00:05:29,360
оценивая, насколько хорошо мы выполняем свою работу.

126
00:05:29,360 --> 00:05:32,479
что и поэтому функция потерь, которую мы используем,

127
00:05:32,479 --> 00:05:34,960
является функцией потерь, обычно

128
00:05:34,960 --> 00:05:36,960
называемой кросс-энтропийной потерей в

129
00:05:36,960 --> 00:05:39,360
литературе, которая заключается в том, что это отрицательное логарифмическое

130
00:05:39,360 --> 00:05:42,080
правдоподобие длится, поэтому мы собираемся

131
00:05:42,080 --> 00:05:43,120
предсказать

132
00:05:43,120 --> 00:05:46,639
какое-то слово, которое будет следующим, хорошо, у нас есть

133
00:05:46,639 --> 00:05:48,800
распределение вероятностей по

134
00:05:48,800 --> 00:05:51,280
предсказаниям  какое слово будет следующим и на

135
00:05:51,280 --> 00:05:53,039
самом деле там ва  это фактическое следующее слово

136
00:05:53,039 --> 00:05:55,600
в тексте, и поэтому мы хорошо говорим, какую

137
00:05:55,600 --> 00:05:57,440
вероятность вы дали этому слову,

138
00:05:57,440 --> 00:05:59,039
и, возможно, мы дали ему оценку вероятности

139
00:05:59,039 --> 00:06:01,759
0,01 хорошо, было бы

140
00:06:01,759 --> 00:06:03,440
здорово, если бы мы дали оценку вероятности

141
00:06:03,440 --> 00:06:05,840
почти в единицу, потому что это

142
00:06:05,840 --> 00:06:08,160
означало, что мы почти уверены, что то, что

143
00:06:08,160 --> 00:06:09,440
произошло дальше

144
00:06:09,440 --> 00:06:11,840
в нашей модели, и поэтому мы

145
00:06:11,840 --> 00:06:14,000
возьмем убыток в той степени, в которой мы

146
00:06:14,000 --> 00:06:17,360
даем фактическому следующему слову прогнозируемую

147
00:06:17,360 --> 00:06:20,720
вероятность меньше единицы,

148
00:06:20,720 --> 00:06:22,960
чтобы затем получить представление о том, насколько хорошо мы

149
00:06:24,319 --> 00:06:28,560
мы работаем над всем корпусом um, мы вычисляем эту потерю

150
00:06:28,560 --> 00:06:31,120
в каждой позиции, а затем мы вычисляем

151
00:06:31,120 --> 00:06:33,600
среднюю потерю всего обучающего

152
00:06:33,600 --> 00:06:34,560
набора,

153
00:06:34,560 --> 00:06:36,639
поэтому давайте просто рассмотрим это еще раз

154
00:06:36,639 --> 00:06:38,720
более наглядно на следующих

155
00:06:38,720 --> 00:06:41,039
двух слайдах, так что внизу

156
00:06:41,039 --> 00:06:44,319
внизу наши  корпус текста,

157
00:06:44,319 --> 00:06:46,080
мы пропускаем его через

158
00:06:46,080 --> 00:06:48,560
нашу простую рекуррентную нейронную сеть, и

159
00:06:48,560 --> 00:06:50,560
в каждой позиции

160
00:06:50,560 --> 00:06:52,479
мы прогнозируем распределение вероятностей для

161
00:06:52,479 --> 00:06:55,039
слов, которые

162
00:06:55,039 --> 00:06:56,080
мы затем

163
00:06:56,880 --> 00:06:58,960
хорошо произносим, на самом деле

164
00:06:58,960 --> 00:07:00,960
в каждой позиции мы знаем, какое слово на

165
00:07:00,960 --> 00:07:03,680
самом деле будет следующим, поэтому, когда мы находимся на временном шаге

166
00:07:04,800 --> 00:07:07,199
Фактически следующее слово - студенты, потому что

167
00:07:07,199 --> 00:07:08,880
мы можем видеть его здесь справа от нас,

168
00:07:08,880 --> 00:07:10,960
и мы говорим, какую

169
00:07:10,960 --> 00:07:13,120
оценку вероятности вы дали студентам, и в

170
00:07:13,120 --> 00:07:16,080
той степени, в которой она не высока, это не

171
00:07:16,080 --> 00:07:18,960
та, которую мы принимаем на себя, а затем мы продолжаем

172
00:07:18,960 --> 00:07:22,319
ко второму временному шагу, и мы говорим, что на

173
00:07:22,319 --> 00:07:24,400
втором временном шаге вы предсказали

174
00:07:24,400 --> 00:07:26,560
распределение вероятностей по словам,

175
00:07:26,560 --> 00:07:28,960
фактическое следующее слово открывается так, что в той

176
00:07:28,960 --> 00:07:30,479
степени, в которой вы не дали высокой

177
00:07:30,479 --> 00:07:33,680
вероятности открытия, вы понесете убыток, а

178
00:07:33,680 --> 00:07:36,800
затем это повторяется со временем  Шаг третий,

179
00:07:36,800 --> 00:07:39,199
мы надеемся, что модель предсказывает там, на

180
00:07:39,199 --> 00:07:41,680
временном шаге четыре, мы надеемся, что модель

181
00:07:41,680 --> 00:07:44,240
будет предсказывать экзамены, а затем, чтобы вычислить

182
00:07:44,240 --> 00:07:46,400
нашу общую потерю,

183
00:07:46,400 --> 00:07:47,840
мы затем

184
00:07:47,840 --> 00:07:51,599
усредняем потерю за каждый временной шаг,

185
00:07:51,599 --> 00:07:52,560
так что

186
00:07:52,560 --> 00:07:54,720
в некотором смысле это довольно  очевидная вещь, которую

187
00:07:54,720 --> 00:07:57,680
нужно сделать, но обратите внимание, что здесь есть небольшая

188
00:07:57,680 --> 00:08:00,879
тонкость, и, в частности, этот

189
00:08:00,879 --> 00:08:02,400
алгоритм упоминается в

190
00:08:02,400 --> 00:08:04,879
литературе как принудительное принуждение учителя,

191
00:08:04,879 --> 00:08:07,680
и что это значит?

192
00:08:13,039 --> 00:08:15,680
Ok  ай, просто начни генерировать, может быть,

193
00:08:15,680 --> 00:08:17,520
я подскажу, с чего начать,

194
00:08:17,520 --> 00:08:19,199
я скажу, что предложение запускает

195
00:08:19,199 --> 00:08:21,599
учеников, а затем

196
00:08:21,599 --> 00:08:23,440
дайте ему поработать и посмотрите, что оно генерирует в

197
00:08:23,440 --> 00:08:26,400
следующем эм, он может начать говорить,

198
00:08:26,400 --> 00:08:28,879
что ученики заблокированы

199
00:08:28,879 --> 00:08:32,320
классная комната или что-то в этом роде, гм,

200
00:08:32,320 --> 00:08:34,159
и мы могли бы сказать, что это не

201
00:08:34,159 --> 00:08:36,320
очень близко к тому, что говорит настоящий текст,

202
00:08:36,320 --> 00:08:38,479
и каким-то образом мы хотим извлечь уроки из этого,

203
00:08:38,479 --> 00:08:40,479
и если вы пойдете в этом направлении,

204
00:08:40,479 --> 00:08:42,240
есть пространство вещей, которые вы можете сделать,

205
00:08:42,240 --> 00:08:44,640
что приведет  в более сложные алгоритмы,

206
00:08:44,640 --> 00:08:47,360
такие как обучение с подкреплением, эм, но

207
00:08:47,360 --> 00:08:50,160
с точки зрения обучения этих

208
00:08:50,160 --> 00:08:54,240
нейронных моделей это чрезмерно сложное и

209
00:08:54,240 --> 00:08:57,519
ненужное, поэтому у нас есть очень простой

210
00:08:57,519 --> 00:09:00,720
способ делать вещи, которые мы делаем,

211
00:09:00,720 --> 00:09:03,680
просто прогнозируем на один шаг вперед, поэтому

212
00:09:03,680 --> 00:09:07,279
мы говорим, что знаем  что префикс - это

213
00:09:07,279 --> 00:09:09,360
предсказание распределения вероятности

214
00:09:09,360 --> 00:09:11,760
по следующему слову, это хорошо,

215
00:09:11,760 --> 00:09:14,160
если вы даете мастер вероятности открытому

216
00:09:14,160 --> 00:09:16,320
хорошо, теперь префикс - это открытый ученик,

217
00:09:16,320 --> 00:09:17,920
предсказать

218
00:09:17,920 --> 00:09:20,240
распределение вероятности  Ион над следующим

219
00:09:20,240 --> 00:09:22,560
словом, это хорошо в той мере, в какой вы

220
00:09:22,560 --> 00:09:25,760
указываете там мастер вероятностей, и так

221
00:09:25,760 --> 00:09:29,440
эффективно на каждом этапе мы сбрасываем

222
00:09:29,440 --> 00:09:31,440
то, что было на самом деле в корпусе, чтобы

223
00:09:31,440 --> 00:09:32,720
вы знали, что это возможно после того, как

224
00:09:32,720 --> 00:09:35,360
студенты открыли модель, которая считает, что,

225
00:09:35,360 --> 00:09:38,000
безусловно, наиболее  вероятная вещь, которая будет дальше,

226
00:09:38,000 --> 00:09:40,880
ах или так сказать, я имею в виду, что мы на

227
00:09:40,880 --> 00:09:43,839
самом деле не используем то, что предлагала модель, мы

228
00:09:43,839 --> 00:09:45,440
наказываем модель за то, что она не

229
00:09:45,440 --> 00:09:47,760
предлагала там, но затем мы просто идем с

230
00:09:47,760 --> 00:09:49,920
тем, что на самом деле находится в корпусе, и просим ее

231
00:09:49,920 --> 00:09:52,800
снова предсказать, что

232
00:09:55,600 --> 00:09:58,080
это  просто небольшая

233
00:09:58,080 --> 00:10:00,800
побочная вещь, но это важная часть, чтобы

234
00:10:00,800 --> 00:10:01,680
знать,

235
00:10:01,680 --> 00:10:03,279
действительно ли вы тренируете свою собственную

236
00:10:03,279 --> 00:10:05,120
нейронную языковую модель, которую я

237
00:10:05,120 --> 00:10:07,440
как бы представлен как один огромный

238
00:10:07,440 --> 00:10:10,480
корпус, который мы пихаем, но на

239
00:10:10,480 --> 00:10:13,200
практике мы не прогоняем весь

240
00:10:13,200 --> 00:10:16,640
корпус ни на один шаг за  время то, что мы делаем,

241
00:10:16,640 --> 00:10:19,600
мы разрезаем весь корпус на более короткие

242
00:10:19,600 --> 00:10:22,800
части, которые обычно могут быть предложениями

243
00:10:22,800 --> 00:10:24,640
или документами, а иногда они

244
00:10:24,640 --> 00:10:26,880
буквально просто кусочки, которые нарезаны

245
00:10:26,880 --> 00:10:28,720
правильно, чтобы вы вспомнили этот

246
00:10:28,720 --> 00:10:30,959
запах стохастического градиента  позволяет нам вычислять

247
00:10:30,959 --> 00:10:33,120
потери и градиенты из небольшого фрагмента

248
00:10:33,120 --> 00:10:36,160
данных и обновлять, поэтому мы берем

249
00:10:36,160 --> 00:10:39,200
эти небольшие фрагменты, вычисляем градиенты

250
00:10:39,200 --> 00:10:41,839
из них, обновляем веса и повторяем,

251
00:10:41,839 --> 00:10:43,600
и, в частности,

252
00:10:43,600 --> 00:10:46,000
мы получаем намного больше скорости, эффективности

253
00:10:46,000 --> 00:10:48,640
и обучения, если мы  на самом деле

254
00:10:48,640 --> 00:10:51,360
не обновляют только одно предложение

255
00:10:51,360 --> 00:10:54,560
за раз, а на самом деле пакет предложений,

256
00:10:54,560 --> 00:10:56,959
поэтому обычно то, что мы на самом деле делаем, это

257
00:10:56,959 --> 00:11:00,079
мы загружаем модели 32 предложения, скажем,

258
00:11:00,079 --> 00:11:02,560
одинаковой длины, в то же время

259
00:11:02,560 --> 00:11:05,440
вычисляя градиенты для  они обновляют

260
00:11:05,440 --> 00:11:07,440
веса, а затем получают еще одну партию

261
00:11:07,440 --> 00:11:10,720
предложений для обучения,

262
00:11:11,040 --> 00:11:13,440
как мы тренируемся, я вроде как не

263
00:11:13,440 --> 00:11:16,880
вдавался в подробности этого, я имею в виду, что

264
00:11:16,880 --> 00:11:19,120
в определенном смысле ответ такой же, как мы

265
00:11:19,120 --> 00:11:21,760
говорили в лекции три, которые мы используем

266
00:11:21,760 --> 00:11:23,680
обратное распространение для получения градиентов и

267
00:11:23,680 --> 00:11:26,000
обновления параметров um, но давайте потратим хотя

268
00:11:26,000 --> 00:11:28,560
бы минуту, чтобы пройти через

269
00:11:28,560 --> 00:11:30,800
различия и тонкости

270
00:11:30,800 --> 00:11:34,000
текущего случая нейронной сети um и

271
00:11:34,000 --> 00:11:36,880
центральную вещь, которую вы немного знаете, как и

272
00:11:36,880 --> 00:11:40,000
прежде, мы собираемся t  возьмем нашу потерю, и

273
00:11:40,000 --> 00:11:42,959
мы собираемся распространить ее обратно на

274
00:11:42,959 --> 00:11:45,040
все параметры сети,

275
00:11:45,040 --> 00:11:46,880
от встраивания слов до

276
00:11:46,880 --> 00:11:50,399
смещений и т. д., но центральный бит, который

277
00:11:50,399 --> 00:11:52,480
немного отличается и более

278
00:11:52,480 --> 00:11:55,120
сложен, заключается в

279
00:11:55,120 --> 00:11:58,399
том, что у нас есть эта матрица wh, которая работает

280
00:11:58,399 --> 00:12:00,720
вместе  последовательность, которую мы продолжаем

281
00:12:00,720 --> 00:12:05,040
применять для обновления нашего скрытого состояния,

282
00:12:05,040 --> 00:12:09,200
так какова производная jt от теты

283
00:12:09,200 --> 00:12:11,040
по отношению к повторяющейся

284
00:12:11,040 --> 00:12:13,120
матрице весов wh,

285
00:12:13,120 --> 00:12:16,720
и ответ на это - то,

286
00:12:16,720 --> 00:12:18,800
что мы делаем, мы

287
00:12:18,800 --> 00:12:22,480
смотрим на нее в каждой позиции

288
00:12:22,480 --> 00:12:27,680
и  выяснить, какие частичные элементы имеют gt

289
00:12:27,680 --> 00:12:31,920
по отношению к wh в позиции один или

290
00:12:31,920 --> 00:12:34,320
позиции два, позиции три, позиции

291
00:12:34,320 --> 00:12:36,800
четыре и т. д. прямо вдоль последовательности,

292
00:12:36,800 --> 00:12:39,680
и мы просто суммируем все эти частичные,

293
00:12:39,680 --> 00:12:43,680
и это дает нам частичное значение для jt

294
00:12:43,680 --> 00:12:46,480
относительно wh в целом

295
00:12:46,480 --> 00:12:48,000
um

296
00:12:48,000 --> 00:12:51,200
поэтому ответ для текущей нейронной сети

297
00:12:51,200 --> 00:12:53,120
- это градиент по отношению к

298
00:12:53,120 --> 00:12:55,760
повторяющемуся весу в нашей текущей сети,

299
00:12:55,760 --> 00:12:58,079
это сумма градиента по отношению

300
00:12:58,079 --> 00:13:00,240
к каждому разу, когда он появляется

301
00:13:00,240 --> 00:13:01,279
um,

302
00:13:01,279 --> 00:13:02,880
и позвольте мне просто

303
00:13:02,880 --> 00:13:06,320
пройти через  тьфу, немного, почему это так,

304
00:13:06,320 --> 00:13:09,600
но прежде чем я это сделаю, позвольте мне просто

305
00:13:09,600 --> 00:13:12,000
отметить одну ошибку, я имею в виду,

306
00:13:12,000 --> 00:13:14,800
что это просто не тот случай, что это означает, что

307
00:13:14,800 --> 00:13:20,320
она равна t раз частичному jt по

308
00:13:20,320 --> 00:13:23,600
отношению к wh, потому что мы используем wh

309
00:13:23,600 --> 00:13:26,160
здесь, здесь, здесь  здесь, здесь, через

310
00:13:26,160 --> 00:13:28,880
последовательность, и для каждого из мест, где мы

311
00:13:28,880 --> 00:13:31,279
его используем, есть свой восходящий

312
00:13:31,279 --> 00:13:33,519
градиент, который подается в него, поэтому

313
00:13:33,519 --> 00:13:36,000
каждое из значений в этой сумме будет

314
00:13:36,000 --> 00:13:39,680
полностью отличаться друг от друга,

315
00:13:40,880 --> 00:13:45,040
поэтому мы получаем этот ответ, по сути, является

316
00:13:45,040 --> 00:13:47,519
следствием  то, о чем мы говорили

317
00:13:47,519 --> 00:13:49,360
в третьей лекции,

318
00:13:49,360 --> 00:13:51,920
чтобы взять простейший случай этого

319
00:13:51,920 --> 00:13:54,639
права: если у вас есть функция с несколькими переменными

320
00:13:54,639 --> 00:13:57,519
f от xy,

321
00:13:57,519 --> 00:13:59,279
и у вас есть две

322
00:13:59,279 --> 00:14:02,000
функции с одной переменной x от t и y

323
00:14:02,000 --> 00:14:02,959
от t,

324
00:14:02,959 --> 00:14:05,440
которые хорошо подаются на один вход

325
00:14:05,440 --> 00:14:08,320
t  тогда

326
00:14:08,320 --> 00:14:11,279
простой вариант вычисления

327
00:14:11,279 --> 00:14:14,160
производной этой функции состоит в том, что

328
00:14:14,160 --> 00:14:16,800
вы берете производную по одному пути, а

329
00:14:16,800 --> 00:14:19,360
производную по другому пути,

330
00:14:19,360 --> 00:14:21,839
и поэтому на слайдах в

331
00:14:21,839 --> 00:14:24,240
третьей лекции это было то, что было

332
00:14:24,240 --> 00:14:26,639
резюмировано на  пара слайдов по

333
00:14:26,639 --> 00:14:30,240
лозунгу градиентная сумма на внешних ветвях,

334
00:14:30,240 --> 00:14:33,760
так что у t есть внешние ветки, и поэтому вы

335
00:14:33,760 --> 00:14:36,800
берете градиент здесь, на левом градиенте

336
00:14:36,800 --> 00:14:39,519
справа, и вы суммируете их вместе,

337
00:14:39,519 --> 00:14:41,680
и поэтому на самом деле то, что происходит с

338
00:14:41,680 --> 00:14:44,800
рекуррентной нейронной сетью, - это просто

339
00:14:44,800 --> 00:14:47,920
обобщение множества частей  это, поэтому у нас

340
00:14:47,920 --> 00:14:51,600
есть одна матрица wh, и мы используем ее, чтобы

341
00:14:51,600 --> 00:14:54,160
продолжать обновлять скрытое состояние во

342
00:14:54,160 --> 00:14:57,680
время 1, 2, 3, прямо через время

343
00:14:57,680 --> 00:14:58,880
t,

344
00:14:58,880 --> 00:15:02,639
и поэтому мы собираемся получить, что у

345
00:15:02,639 --> 00:15:06,560
нее много внешних

346
00:15:06,560 --> 00:15:09,120
ветвей  и мы собираемся

347
00:15:09,120 --> 00:15:12,880
просуммировать путь градиента для каждого из них,

348
00:15:12,880 --> 00:15:13,760
но

349
00:15:13,760 --> 00:15:15,600
что это за

350
00:15:15,600 --> 00:15:18,000
путь градиента здесь, он как бы идет здесь,

351
00:15:18,000 --> 00:15:20,399
а затем идет вниз, но вы знаете, что на

352
00:15:20,399 --> 00:15:23,199
самом деле нижняя часть заключается в том, что мы

353
00:15:23,199 --> 00:15:27,199
просто используем wh для каждого  позиции, поэтому у нас

354
00:15:27,199 --> 00:15:30,480
есть партиал wh, используемый в позиции

355
00:15:30,480 --> 00:15:34,320
i по отношению к партиалу wh,

356
00:15:34,320 --> 00:15:36,480
который является просто нашей матрицей весов для нашей

357
00:15:36,480 --> 00:15:38,880
текущей нейронной сети, так что это только

358
00:15:38,880 --> 00:15:41,120
один, потому что вы знаете, что мы просто используем

359
00:15:41,120 --> 00:15:44,240
одну и ту же матрицу везде, и поэтому мы

360
00:15:44,240 --> 00:15:45,279
именно тогда 

361
00:15:47,360 --> 00:15:49,360
суммируя частички в каждой позиции, которую мы

362
00:15:49,360 --> 00:15:51,839
используем,

363
00:15:52,480 --> 00:15:55,440
хорошо, на практике, что это значит

364
00:15:55,440 --> 00:15:59,519
с точки зрения того, как вы хорошо вычисляете это,

365
00:15:59,519 --> 00:16:02,079
если вы делаете это вручную, что

366
00:16:02,079 --> 00:16:05,199
происходит, вы начинаете с конца,

367
00:16:05,199 --> 00:16:07,920
как в этой общей лекции три  story

368
00:16:07,920 --> 00:16:10,399
вы разрабатываете

369
00:16:10,399 --> 00:16:12,480
производные

370
00:16:12,480 --> 00:16:14,639
по отношению к скрытому слою, а

371
00:16:14,639 --> 00:16:18,320
затем по отношению к wh на последнем временном

372
00:16:18,320 --> 00:16:20,880
шаге, и это дает вам

373
00:16:20,880 --> 00:16:24,160
одно обновление для wh, но затем вы продолжаете

374
00:16:24,160 --> 00:16:26,880
передавать градиент обратно к t минус

375
00:16:26,880 --> 00:16:29,440
один временной шаг и через пару  больше

376
00:16:29,440 --> 00:16:32,639
шагов правила цепочки вы получаете еще одно

377
00:16:32,639 --> 00:16:35,680
обновление для wh, и вы просто суммируете это с

378
00:16:35,680 --> 00:16:38,399
вашим предыдущим обновлением для wh, а

379
00:16:38,399 --> 00:16:39,600
затем вы переходите к

380
00:16:39,600 --> 00:16:43,199
ht минус 2, вы получаете еще одно обновление для wh,

381
00:16:43,199 --> 00:16:45,759
и вы суммируете это со своим обновлением для wh,

382
00:16:45,759 --> 00:16:48,160
и вы возвращаетесь назад  все время,

383
00:16:48,160 --> 00:16:50,480
и вы суммируете градиенты по мере

384
00:16:50,480 --> 00:16:53,920
продвижения, и это дает вам полное обновление

385
00:16:53,920 --> 00:16:55,360
um для чего,

386
00:16:55,360 --> 00:16:58,880
и поэтому здесь есть два трюка, и

387
00:16:58,880 --> 00:17:01,759
я просто упомяну два трюка, которые вы

388
00:17:01,759 --> 00:17:03,680
должны как бы отдельно суммировать

389
00:17:03,680 --> 00:17:06,079
обновления  для чего, а потом еще раз  вы

390
00:17:06,079 --> 00:17:09,119
закончили применять их все сразу, вы

391
00:17:09,119 --> 00:17:10,640
не хотите на самом деле изменять

392
00:17:10,640 --> 00:17:13,760
матрицу wh по мере продвижения, потому что тогда это

393
00:17:13,760 --> 00:17:16,000
недействительно, потому что прямые

394
00:17:16,000 --> 00:17:18,880
вычисления были выполнены с константой

395
00:17:18,880 --> 00:17:22,559
wh, которая была у вас из предыдущего

396
00:17:22,559 --> 00:17:25,599
состояния по всей сети

397
00:17:25,599 --> 00:17:28,160
второй трюк хорош, если вы делаете

398
00:17:28,160 --> 00:17:30,880
это для предложений, вы обычно можете просто

399
00:17:30,880 --> 00:17:33,440
вернуться к началу предложения,

400
00:17:33,440 --> 00:17:36,320
но если у вас очень длинные последовательности,

401
00:17:36,320 --> 00:17:38,400
это может действительно замедлить вас, если

402
00:17:38,400 --> 00:17:40,080
вам нужно как бы запустить это  алгоритм

403
00:17:40,080 --> 00:17:43,679
обратно в течение огромного количества времени,

404
00:17:43,679 --> 00:17:45,760
поэтому то, что люди обычно делают, это

405
00:17:45,760 --> 00:17:48,559
то, что называется усеченным обратным распространением

406
00:17:48,559 --> 00:17:50,640
во времени, где вы выбираете некоторую

407
00:17:50,640 --> 00:17:53,440
константу, скажем 20, и вы говорите хорошо, я

408
00:17:53,440 --> 00:17:56,160
просто собираюсь запустить это обратное распространение

409
00:17:56,160 --> 00:17:59,840
для 20 временных шагов, суммируйте эти 20 градиентов

410
00:17:59,840 --> 00:18:03,039
а затем я просто сделал это то, чем я

411
00:18:03,039 --> 00:18:06,240
обновлю матрицу wh,

412
00:18:06,240 --> 00:18:09,840
и это работает нормально,

413
00:18:09,919 --> 00:18:13,280
хорошо, поэтому теперь, учитывая корпус,

414
00:18:13,280 --> 00:18:14,720
мы можем обучить

415
00:18:16,559 --> 00:18:20,240
простой rnn, и это хороший прогресс,

416
00:18:20,240 --> 00:18:21,840
но

417
00:18:21,840 --> 00:18:23,919
это модель, которая также может генерировать

418
00:18:23,919 --> 00:18:26,559
t  ext в целом, так как же нам

419
00:18:26,559 --> 00:18:29,679
хорошо сгенерировать текст точно так же, как в нашей

420
00:18:29,679 --> 00:18:31,360
языковой модели инграмм, мы собираемся генерировать

421
00:18:31,360 --> 00:18:34,640
текст путем повторной выборки, поэтому мы собираемся

422
00:18:34,640 --> 00:18:38,799
начать с начального состояния,

423
00:18:39,760 --> 00:18:40,840
и

424
00:18:40,840 --> 00:18:44,720
да, этот слайд несовершенен,

425
00:18:44,720 --> 00:18:46,799
так что начальный  состояние для скрытого

426
00:18:46,799 --> 00:18:48,480
состояния

427
00:18:48,480 --> 00:18:51,280
um обычно просто берется как нулевой

428
00:18:51,280 --> 00:18:53,520
вектор, и тогда нам нужно что-

429
00:18:53,520 --> 00:18:56,240
то для первого ввода, и на этом

430
00:18:56,240 --> 00:18:58,080
слайде первый ввод отображается как

431
00:18:58,080 --> 00:19:00,559
первое слово my, и если вы хотите

432
00:19:00,559 --> 00:19:03,760
указать начальную точку  вы могли бы скормить мне, но

433
00:19:03,760 --> 00:19:05,840
большую часть времени вы хотели бы сгенерировать

434
00:19:05,840 --> 00:19:07,840
предложение из ничего, и если вы хотите

435
00:19:07,840 --> 00:19:10,080
сделать это, то обычно нужно

436
00:19:10,080 --> 00:19:12,080
дополнительно иметь

437
00:19:12,080 --> 00:19:14,720
токен начала последовательности, который является специальным токеном,

438
00:19:14,720 --> 00:19:16,160
поэтому вы будете кормить в  начало

439
00:19:16,160 --> 00:19:18,400
токена последовательности в начале в

440
00:19:18,400 --> 00:19:21,360
качестве первого токена, у которого есть встраивание, а

441
00:19:21,360 --> 00:19:23,600
затем вы используете

442
00:19:23,600 --> 00:19:26,400
обновление rnn, а затем вы генерируете,

443
00:19:26,400 --> 00:19:29,600
используя softmax и следующее слово, и,

444
00:19:29,600 --> 00:19:32,080
ну, вы генерируете

445
00:19:32,080 --> 00:19:34,640
вероятность распределения вероятностей по следующим словам

446
00:19:34,640 --> 00:19:36,720
и t  в этот момент вы пробуете из

447
00:19:36,720 --> 00:19:39,200
этого, и он выбирает какое-то слово, например

448
00:19:39,200 --> 00:19:42,320
избранное, и тогда трюк состоит в том, чтобы

449
00:19:42,320 --> 00:19:44,960
выполнить генерацию, вы берете это слово,

450
00:19:44,960 --> 00:19:47,520
которое вы выбрали, и копируете его

451
00:19:47,520 --> 00:19:50,160
обратно на вход, а затем вы вводите

452
00:19:50,160 --> 00:19:52,720
его как импорт  следующий шаг, если вы

453
00:19:53,520 --> 00:19:56,240
образец n из softmax, возьмите другое слово

454
00:19:56,240 --> 00:19:58,559
и просто продолжайте повторять это снова и

455
00:19:58,559 --> 00:20:01,520
снова, и вы начинаете генерировать

456
00:20:01,520 --> 00:20:02,559
текст,

457
00:20:02,559 --> 00:20:05,760
и как вы заканчиваете, а также имея

458
00:20:05,760 --> 00:20:07,520
начало последовательности

459
00:20:07,520 --> 00:20:10,159
um специальный символ, у вас обычно есть

460
00:20:10,159 --> 00:20:12,640
конец  специального символа последовательности, и в

461
00:20:12,640 --> 00:20:14,799
какой-то момент рекуррентная нейронная

462
00:20:14,799 --> 00:20:17,520
сеть сгенерирует конец

463
00:20:17,520 --> 00:20:20,720
символа последовательности um, а затем вы говорите, хорошо,

464
00:20:20,720 --> 00:20:25,200
я закончил, я закончил генерировать текст,

465
00:20:25,200 --> 00:20:27,440
поэтому, прежде чем переходить к

466
00:20:27,440 --> 00:20:29,120
более

467
00:20:29,120 --> 00:20:31,440
сложному содержанию лекции  мы можем

468
00:20:31,440 --> 00:20:33,520
просто немного повеселиться с этим

469
00:20:33,520 --> 00:20:34,960
и попробовать

470
00:20:34,960 --> 00:20:37,679
потренироваться и сгенерировать текст с нашей

471
00:20:37,679 --> 00:20:40,000
текущей моделью нейронной сети, чтобы вы могли

472
00:20:40,000 --> 00:20:42,480
сгенерировать, вы можете обучать rnn на любом

473
00:20:42,480 --> 00:20:44,960
типе текста, и это означает, что это одна

474
00:20:44,960 --> 00:20:47,039
из забавных вещей  Вы можете

475
00:20:47,039 --> 00:20:48,720
создавать текст

476
00:20:48,720 --> 00:20:50,799
в разных стилях в зависимости от того, что вы

477
00:20:50,799 --> 00:20:53,760
могли бы использовать для его обучения,

478
00:20:54,400 --> 00:20:56,320
так что здесь

479
00:20:56,320 --> 00:20:58,799
есть большой объем

480
00:20:58,799 --> 00:21:01,520
текста, так что вы можете обучать

481
00:21:01,520 --> 00:21:03,919
rnn lm на книгах о Гарри Поттере, а затем

482
00:21:03,919 --> 00:21:06,559
сказать dolph  и сгенерируйте какой-то текст, и

483
00:21:06,559 --> 00:21:09,360
он сгенерирует такой текст, извините, как

484
00:21:09,360 --> 00:21:11,679
Гарри кричал в панике, я оставлю эти

485
00:21:11,679 --> 00:21:13,919
метлы в Лондоне, неужели они

486
00:21:13,919 --> 00:21:15,840
не догадывались, сказали, что почти обезглавленный Ник

487
00:21:15,840 --> 00:21:18,320
низко бросается рядом с Седриком, несущим

488
00:21:18,320 --> 00:21:20,320
последний кусочек патоки с

489
00:21:20,320 --> 00:21:22,400
плеча Гарри и на  ответьте ему,

490
00:21:22,400 --> 00:21:24,960
гостиная толкнула его, четыре руки держали

491
00:21:24,960 --> 00:21:27,200
блестящую ручку, когда паук не

492
00:21:27,200 --> 00:21:30,720
чувствовал, что казалось, что он слишком хорошо добрался до команд,

493
00:21:30,720 --> 00:21:34,159
так что, с одной стороны, это все еще

494
00:21:34,159 --> 00:21:37,200
немного бессвязно, как рассказ,

495
00:21:37,200 --> 00:21:39,520
с другой стороны  это вроде как звучит как

496
00:21:39,520 --> 00:21:41,679
гарри поттер и, конечно, из тех,

497
00:21:41,679 --> 00:21:44,159
кто знает лексику и конструкции в

498
00:21:44,159 --> 00:21:47,679
пользователях, и я думаю, вы согласитесь, что

499
00:21:47,679 --> 00:21:49,360
знаете, даже если это становится своего рода

500
00:21:49,360 --> 00:21:52,559
бессвязным, это вроде более связно,

501
00:21:52,559 --> 00:21:54,880
чем то, что мы получили от ингра.  m языковая

502
00:21:54,880 --> 00:21:57,600
модель эм, когда я показал поколение

503
00:21:57,600 --> 00:21:58,799
на прошлой

504
00:21:58,799 --> 00:22:00,559
лекции,

505
00:22:00,559 --> 00:22:03,520
вы можете выбрать совсем другой стиль

506
00:22:03,520 --> 00:22:04,640
текста,

507
00:22:04,640 --> 00:22:06,559
чтобы вместо этого вы могли

508
00:22:06,559 --> 00:22:09,600
обучить модель на кучке кулинарных книг,

509
00:22:09,600 --> 00:22:11,919
и если вы это сделаете, вы можете затем сказать

510
00:22:11,919 --> 00:22:13,200
генерировать

511
00:22:13,200 --> 00:22:14,640
на основе того, что вы  узнал о

512
00:22:14,640 --> 00:22:16,000
поваренных книгах,

513
00:22:16,000 --> 00:22:17,919
и он просто сгенерирует рецепт, так что

514
00:22:17,919 --> 00:22:19,760
вот рецепт

515
00:22:19,760 --> 00:22:22,960
шоколадное ранчо барбекю

516
00:22:22,960 --> 00:22:25,520
категории дают шесть порций

517
00:22:25,520 --> 00:22:28,080
две столовые ложки нарезанного сыра пармезан

518
00:22:28,080 --> 00:22:31,039
и одна чашка кокосового молока

519
00:22:31,039 --> 00:22:32,880
три взбитых яйца

520
00:22:32,880 --> 00:22:35,760
поместите каждую пасту поверх слоев

521
00:22:35,760 --> 00:22:38,000
смеси в форме комков в умеренную духовку

522
00:22:38,000 --> 00:22:41,200
и тушить, пока не станет твердой подача. Горячие воплощенные

523
00:22:41,200 --> 00:22:44,080
свежие горчицы, апельсин и сыр.

524
00:22:44,080 --> 00:22:46,799
Смешайте сыр и соль.

525
00:22:46,799 --> 00:22:48,960
Тесто в большой сковороде. Добавьте

526
00:22:48,960 --> 00:22:51,120
ингредиенты и перемешайте шоколад

527
00:22:51,120 --> 00:22:52,880
и перец.

528
00:22:58,000 --> 00:22:59,679
на самом деле даже нет

529
00:22:59,679 --> 00:23:01,760
опасности, что вы попробуете приготовить это

530
00:23:01,760 --> 00:23:04,159
дома, но вы знаете, что кое-что

531
00:23:04,159 --> 00:23:07,200
интересное, хотя вы знаете, что это

532
00:23:07,200 --> 00:23:08,559
действительно просто

533
00:23:08,559 --> 00:23:11,440
не  Рецепт и то, что

534
00:23:11,440 --> 00:23:14,240
делается в инструкциях, не имеет

535
00:23:14,240 --> 00:23:17,919
отношения к ингредиентам.

536
00:23:17,919 --> 00:23:19,919
Интересно то, что он

537
00:23:19,919 --> 00:23:22,240
узнал, это рекуррентная модель нейронной сети,

538
00:23:22,240 --> 00:23:25,200
что она действительно освоила

539
00:23:25,200 --> 00:23:27,600
общую структуру рецепта, который, как он знает,

540
00:23:27,600 --> 00:23:30,559
рецепт имеет  название, которое он часто говорит

541
00:23:30,559 --> 00:23:33,360
вам о том, сколько людей он обслуживает, он

542
00:23:33,360 --> 00:23:35,280
перечисляет ингредиенты, а затем в нем есть

543
00:23:35,280 --> 00:23:37,679
инструкции, ммм, чтобы сделать это так, что

544
00:23:37,679 --> 00:23:40,559
это довольно впечатляюще в некотором смысле

545
00:23:40,559 --> 00:23:43,919
для высокоуровневого структурирования текста,

546
00:23:43,919 --> 00:23:46,559
поэтому еще одна вещь, которую я хотел

547
00:23:46,559 --> 00:23:48,400
упомянуть, это

548
00:23:48,400 --> 00:23:50,960
когда я говорю, что вы можете обучить языковую модель rnn

549
00:23:50,960 --> 00:23:53,039
в любом виде текста, другое

550
00:23:53,039 --> 00:23:54,880
отличие от того, где мы были в

551
00:23:54,880 --> 00:23:57,279
моделях грамматического языка, было в языковых

552
00:23:57,279 --> 00:23:59,760
моделях инграмм, что означало просто подсчет инграмм

553
00:23:59,760 --> 00:24:01,840
и означало, что это заняло

554
00:24:01,840 --> 00:24:04,240
две минуты или даже большой корпус

555
00:24:04,240 --> 00:24:06,559
при любом современном компьютерном обучении ваш

556
00:24:06,559 --> 00:24:09,039
rnn lm на самом деле может быть

557
00:24:09,039 --> 00:24:11,200
занятием, требующим много времени, и вы можете тратить на

558
00:24:11,200 --> 00:24:13,679
это часы, как вы можете обнаружить на следующей

559
00:24:13,679 --> 00:24:15,520
неделе, когда будете тренировать

560
00:24:15,520 --> 00:24:18,400
ма.  китайские модели перевода

561
00:24:18,400 --> 00:24:19,760
хорошо,

562
00:24:19,760 --> 00:24:22,799
как нам решить, хороши наши модели

563
00:24:22,799 --> 00:24:24,159
или нет,

564
00:24:26,080 --> 00:24:29,520
так что стандартная метрика оценки для

565
00:24:29,520 --> 00:24:31,679
языковых моделей - это то, что называется

566
00:24:31,679 --> 00:24:33,679
недоумением,

567
00:24:33,679 --> 00:24:36,960
а то, что такое недоумение,

568
00:24:37,840 --> 00:24:40,880
похоже на то, как когда вы

569
00:24:40,880 --> 00:24:43,840
тренируете свою модель, вы используете учитель,

570
00:24:45,039 --> 00:24:47,679
заставляющий кусок  текста, который представляет собой другой

571
00:24:47,679 --> 00:24:50,720
фрагмент тестового текста, который не является текстом, который

572
00:24:50,720 --> 00:24:52,960
был в обучающих данных, и вы хорошо говорите,

573
00:24:52,960 --> 00:24:57,440
учитывая последовательность из t слов,

574
00:24:57,440 --> 00:24:59,360
какую вероятность вы даете

575
00:24:59,360 --> 00:25:02,480
фактическому t плюс одно слово, и вы

576
00:25:02,480 --> 00:25:04,960
повторяете это в каждой позиции

577
00:25:04,960 --> 00:25:07,200
и  затем вы берете обратную

578
00:25:07,200 --> 00:25:10,000
вероятность этой вероятности и увеличиваете ее до

579
00:25:10,000 --> 00:25:13,039
единицы на t для длины вашего тестового

580
00:25:13,039 --> 00:25:14,400
образца текста,

581
00:25:14,400 --> 00:25:17,360
и это число является недоумением, так

582
00:25:17,360 --> 00:25:20,480
что это среднее геометрическое обратных

583
00:25:20,480 --> 00:25:22,159
вероятностей

584
00:25:22,159 --> 00:25:25,360
теперь после этого объяснения, возможно, более

585
00:25:25,360 --> 00:25:28,240
простой способ думать  из-за того, что

586
00:25:28,240 --> 00:25:29,919
недоумение

587
00:25:29,919 --> 00:25:32,240
- это

588
00:25:32,240 --> 00:25:34,480
просто потеря кросс-энтропии, которую я ввел

589
00:25:34,480 --> 00:25:38,080
перед возведением в степень,

590
00:25:38,080 --> 00:25:40,000
так

591
00:25:40,000 --> 00:25:41,679
что теперь вы знаете, что сейчас все наоборот,

592
00:25:42,720 --> 00:25:46,159
так что низкая недоумение лучше, так

593
00:25:46,159 --> 00:25:47,679
что  На самом деле это интересная история

594
00:25:47,679 --> 00:25:51,360
об этих затруднениях, гм, так что известным

595
00:25:51,360 --> 00:25:53,760
деятелем в разработке

596
00:25:53,760 --> 00:25:55,360
вероятностных подходов и подходов машинного обучения

597
00:25:55,360 --> 00:25:56,960
к обработке естественного языка

598
00:25:56,960 --> 00:25:59,679
является Фред Гелланек, который умер

599
00:25:59,679 --> 00:26:01,600
несколько лет назад

600
00:26:01,600 --> 00:26:05,520
и пытался

601
00:26:06,000 --> 00:26:08,880
заинтересовать людей идеей использования

602
00:26:08,880 --> 00:26:11,279
вероятностных моделей.  и машинное обучение

603
00:26:11,279 --> 00:26:14,000
для обработки естественного языка в

604
00:26:14,000 --> 00:26:18,480
то время. Это 1970-е и начало 1980-х годов,

605
00:26:18,480 --> 00:26:22,720
когда почти все в области искусственного интеллекта

606
00:26:22,720 --> 00:26:25,679
все еще были в плену логических

607
00:26:25,679 --> 00:26:28,720
моделей, архитектур классной доски и

608
00:26:28,720 --> 00:26:30,159
тому подобного для

609
00:26:30,159 --> 00:26:32,480
систем искусственного интеллекта и т. д.

610
00:26:32,480 --> 00:26:35,039
Гелларнек на самом деле был теоретиком информации

611
00:26:35,039 --> 00:26:39,360
по фону э-э, а

612
00:26:39,360 --> 00:26:41,600
затем заинтересовался работой с

613
00:26:41,600 --> 00:26:44,559
речью, а затем с языковыми данными,

614
00:26:44,559 --> 00:26:47,919
так что в то время вещи, которые были такого

615
00:26:47,919 --> 00:26:50,640
рода экспоненциальными или использующими

616
00:26:50,640 --> 00:26:53,039
кросс-энтропийные потери, были полностью

617
00:26:53,039 --> 00:26:55,919
хлебом с маслом для Фреда Джелинека, но

618
00:26:55,919 --> 00:26:58,720
он  обнаружил, что никто из искусственного интеллекта не может

619
00:26:58,720 --> 00:27:01,520
понять нижнюю половину слайда,

620
00:27:01,520 --> 00:27:03,039
поэтому он решил придумать

621
00:27:03,039 --> 00:27:05,520
что-то простое, что люди в то

622
00:27:05,520 --> 00:27:08,640
время могли понять, а недоумение имеет

623
00:27:08,640 --> 00:27:10,799
своего рода простую

624
00:27:10,799 --> 00:27:13,840
интерпретацию, которую вы можете сказать людям, поэтому, если

625
00:27:13,840 --> 00:27:16,840
вы получаете недоумение

626
00:27:16,840 --> 00:27:21,039
53, это означает, что ваша

627
00:27:21,039 --> 00:27:23,760
неуверенность в следующем слове эквивалентна

628
00:27:23,760 --> 00:27:26,919
неуверенности в том, что вы ''  перебрасывает

629
00:27:26,919 --> 00:27:30,880
53-гранный кубик, и это становится

630
00:27:30,880 --> 00:27:34,559
правильным, так что это была легкая

631
00:27:34,559 --> 00:27:37,760
простая метрика, и поэтому он представил

632
00:27:37,760 --> 00:27:39,840
эту идею,

633
00:27:39,840 --> 00:27:42,880
но вы знаете, я думаю, что все остается, и

634
00:27:42,880 --> 00:27:46,480
по сей день все оценивают свои

635
00:27:46,480 --> 00:27:48,720
языковые модели  предоставляя числа недоумения,

636
00:27:48,720 --> 00:27:50,720
и вот некоторые числа недоумения,

637
00:27:52,080 --> 00:27:55,039
так что традиционные модели языка инграмм

638
00:27:55,039 --> 00:27:58,080
обычно имели затруднения более сотни,

639
00:27:58,080 --> 00:27:59,919
но если вы сделали их действительно большими и

640
00:27:59,919 --> 00:28:02,399
очень осторожными, вы могли

641
00:28:02,399 --> 00:28:06,240
бы их уменьшить до числа, например, 67, поскольку

642
00:28:06,240 --> 00:28:09,039
люди начали строить больше  продвинутые

643
00:28:09,039 --> 00:28:12,159
рекуррентные нейронные сети, особенно по мере

644
00:28:12,159 --> 00:28:14,559
того, как они вышли за рамки простых

645
00:28:14,559 --> 00:28:16,880
rnn, которые я вам показал до сих пор,

646
00:28:16,880 --> 00:28:19,440
какая из них находится во второй

647
00:28:19,440 --> 00:28:21,720
строке сайта слайда  е в

648
00:28:21,720 --> 00:28:25,360
lstms, о которых я расскажу позже в этом

649
00:28:25,360 --> 00:28:28,480
курсе, что люди начали создавать

650
00:28:28,480 --> 00:28:31,360
гораздо более серьезные затруднения, а здесь мы

651
00:28:31,360 --> 00:28:34,640
получаем количество затруднений до 30, и

652
00:28:34,640 --> 00:28:37,520
это результат, фактически полученный несколько лет назад,

653
00:28:37,520 --> 00:28:39,679
так что в настоящее время люди получают затруднения

654
00:28:39,679 --> 00:28:42,720
даже ниже 30. um  вы должны быть

655
00:28:42,720 --> 00:28:44,960
реалистами в том, чего вы можете ожидать правильно,

656
00:28:44,960 --> 00:28:46,480
потому что, если вы просто обычно

657
00:28:46,480 --> 00:28:49,360
создаете текст, некоторые слова почти

658
00:28:49,360 --> 00:28:52,640
определены, так что вы знаете, если это что-

659
00:28:52,640 --> 00:28:55,440
то вроде,

660
00:28:55,440 --> 00:28:58,399
вы знаете, зум дал человеку салфетку, которую он

661
00:28:58,399 --> 00:28:59,679
сказал, спасибо,

662
00:28:59,679 --> 00:29:01,520
в основном  на сто процентов вы

663
00:29:01,520 --> 00:29:02,880
должны быть в состоянии сказать следующее слово

664
00:29:02,880 --> 00:29:06,080
- это вы, э-э, и чтобы вы могли

665
00:29:06,080 --> 00:29:09,279
очень хорошо предсказать, но э-э, вы знаете, если

666
00:29:09,279 --> 00:29:12,000
это много других предложений, например, э-э, он

667
00:29:12,000 --> 00:29:14,960
выглянул в окно и увидел

668
00:29:14,960 --> 00:29:17,200
что-то правильное без всякой вероятности в

669
00:29:17,200 --> 00:29:19,840
модельная модель в мире может дать очень

670
00:29:19,840 --> 00:29:21,360
хорошую оценку того, что на самом деле

671
00:29:21,360 --> 00:29:23,679
будет дальше от этой точки, и, таким образом,

672
00:29:23,679 --> 00:29:26,080
это дает нам своего рода остаточную

673
00:29:26,080 --> 00:29:28,240
неопределенность, которая приводит

674
00:29:28,240 --> 00:29:30,799
к затруднениям, которые  ярость может быть около 20

675
00:29:30,799 --> 00:29:33,279
или что-то в этом роде,

676
00:29:35,279 --> 00:29:37,520
так что мы много говорили о языковых

677
00:29:37,520 --> 00:29:40,399
моделях, почему мы должны заботиться о

678
00:29:40,399 --> 00:29:42,480
языковом моделировании, вы хорошо знаете, есть своего

679
00:29:42,480 --> 00:29:43,520
рода

680
00:29:43,520 --> 00:29:47,279
интеллектуальный научный ответ, который говорит, что

681
00:29:47,279 --> 00:29:50,080
это контрольная задача, если мы

682
00:29:50,080 --> 00:29:52,399
то, что хотим  нам предстоит создать

683
00:29:52,399 --> 00:29:54,799
модели машинного обучения языка и нашу

684
00:29:54,799 --> 00:29:56,559
способность предсказывать, какое слово будет

685
00:29:56,559 --> 00:29:59,600
следующим, в контексте, который показывает, насколько хорошо

686
00:29:59,600 --> 00:30:02,080
мы понимаем и структуру

687
00:30:02,080 --> 00:30:04,640
языка, и структуру человеческого

688
00:30:04,640 --> 00:30:05,840
мира,

689
00:30:05,840 --> 00:30:08,720
о котором говорит язык, но есть

690
00:30:08,720 --> 00:30:11,600
гораздо больше  практический ответ, чем тот эм,

691
00:30:11,600 --> 00:30:14,720
который, как вы знаете, языковые модели на

692
00:30:14,720 --> 00:30:17,039
самом деле являются секретным инструментом обработки естественного

693
00:30:17,039 --> 00:30:20,399
языка, поэтому, если вы

694
00:30:20,399 --> 00:30:24,720
разговариваете с любым человеком из НЛП и у вас есть

695
00:30:24,720 --> 00:30:27,360
почти любая

696
00:30:27,360 --> 00:30:30,399
задача, вполне вероятно, что они скажут: о, я

697
00:30:30,399 --> 00:30:32,240
уверен, мы могли бы  использовать для этого языковую модель,

698
00:30:32,240 --> 00:30:35,360
и поэтому языковые модели как бы

699
00:30:35,360 --> 00:30:38,960
используются не как полное решение, а как

700
00:30:38,960 --> 00:30:42,159
часть почти любой задачи, любой задачи, которая

701
00:30:42,159 --> 00:30:44,960
включает в себя создание или оценку

702
00:30:44,960 --> 00:30:47,360
вероятности te  xt, чтобы вы могли использовать его

703
00:30:47,360 --> 00:30:50,880
для прогнозирующего набора текста, исправления грамматики распознавания речи,

704
00:30:50,880 --> 00:30:53,600
определения авторов,

705
00:30:55,760 --> 00:30:57,840
диалога резюмирования машинного перевода, практически все, что вы делаете с

706
00:30:57,840 --> 00:30:59,440
естественным языком, включает языковые

707
00:30:59,440 --> 00:31:02,640
модели, и мы увидим примеры этого

708
00:31:02,640 --> 00:31:04,799
на следующих занятиях, включая следующий

709
00:31:04,799 --> 00:31:06,720
вторник, где мы будем использовать язык

710
00:31:06,720 --> 00:31:09,919
модели для машинного перевода

711
00:31:09,919 --> 00:31:12,159
хорошо, поэтому языковая модель

712
00:31:12,159 --> 00:31:14,640
- это просто система, которая предсказывает следующее

713
00:31:14,640 --> 00:31:18,640
слово um рекуррентная нейронная сеть - это

714
00:31:18,640 --> 00:31:22,159
семейство нейронных сетей, которые могут

715
00:31:22,159 --> 00:31:25,120
принимать последовательные входные данные любой длины,

716
00:31:25,120 --> 00:31:28,240
они повторно используют одни и те же веса для

717
00:31:28,240 --> 00:31:30,799
генерации скрытого состояния и, возможно, но

718
00:31:30,799 --> 00:31:33,760
обычно вывод на каждом шаге,

719
00:31:33,760 --> 00:31:37,200
обратите внимание, что эти две вещи разные,

720
00:31:38,640 --> 00:31:40,320
поэтому мы говорили о двух способах

721
00:31:40,320 --> 00:31:42,640
построения языковых моделей, но один из

722
00:31:42,640 --> 00:31:46,000
них - это отличный способ rnn, но rnns

723
00:31:46,000 --> 00:31:47,679
также можно использовать для многих других

724
00:31:47,679 --> 00:31:50,159
вещей, поэтому  позвольте мне быстро просмотреть

725
00:31:50,159 --> 00:31:53,279
несколько вещей, которые вы можете делать с помощью rnns,

726
00:31:53,279 --> 00:31:55,279
так что есть много задач, которые люди

727
00:31:55,279 --> 00:31:57,919
хотят выполнять в nlp, которые

728
00:31:57,919 --> 00:32:00,799
называются задачами маркировки последовательностей, когда мы

729
00:32:00,799 --> 00:32:04,320
хотели бы взять слова из текста и провести

730
00:32:04,320 --> 00:32:06,320
некоторую классификацию по

731
00:32:06,320 --> 00:32:09,840
последовательности, поэтому одна простая распространенная задача -

732
00:32:09,840 --> 00:32:12,480
дать словам части речи, которая является

733
00:32:12,480 --> 00:32:14,960
определяющим фактором, испугавшись, поскольку прилагательное кошка

734
00:32:14,960 --> 00:32:18,240
- это существительное  knocked как глагол um, и

735
00:32:18,240 --> 00:32:20,640
вы можете сделать это напрямую,

736
00:32:20,640 --> 00:32:24,320
используя текущую нейронную сеть в качестве

737
00:32:24,320 --> 00:32:26,799
последовательного классификатора, где теперь она

738
00:32:26,799 --> 00:32:29,360
будет генерировать части речи, а

739
00:32:29,360 --> 00:32:31,840
не следующее слово,

740
00:32:31,840 --> 00:32:33,919
вы можете использовать рекуррентную нейронную сеть,

741
00:32:33,919 --> 00:32:36,159
классификацию настроений, на

742
00:32:37,120 --> 00:32:38,799
этот раз мы  на самом деле не хотим

743
00:32:38,799 --> 00:32:42,080
генерировать um и выводить каждое слово

744
00:32:42,080 --> 00:32:44,000
обязательно, но мы хотим знать, как

745
00:32:44,000 --> 00:32:46,799
выглядит общее настроение, поэтому каким-

746
00:32:46,799 --> 00:32:49,279
то образом мы хотим получить кодировку предложения,

747
00:32:49,279 --> 00:32:51,760
которую мы, возможно, можем пропустить через другой

748
00:32:51,760 --> 00:32:54,080
уровень нейронной сети, чтобы определить, является

749
00:32:54,080 --> 00:32:56,399
ли предложение  положительный

750
00:32:56,399 --> 00:32:59,440
или отрицательный, самый простой способ сделать

751
00:32:59,440 --> 00:33:01,519
это - хорошо подумать

752
00:33:01,519 --> 00:33:02,960
после того, как

753
00:33:02,960 --> 00:33:03,840
я

754
00:33:03,840 --> 00:33:05,200
пропустил свой

755
00:33:05,200 --> 00:33:06,720
lstm

756
00:33:06,720 --> 00:33:09,440
через все предложение, на самом деле это

757
00:33:09,440 --> 00:33:11,919
окончательное скрытое состояние, это  закодировал

758
00:33:11,919 --> 00:33:13,440
все предложение, потому что помните, я

759
00:33:13,440 --> 00:33:15,679
обновил это скрытое состояние на основе каждого

760
00:33:15,679 --> 00:33:18,320
предыдущего слова, и поэтому вы могли бы сказать, что

761
00:33:18,320 --> 00:33:19,679
это все значение

762
00:33:19,679 --> 00:33:22,480
предложения, поэтому давайте просто скажем, что это

763
00:33:22,480 --> 00:33:24,480
кодировка предложения,

764
00:33:24,480 --> 00:33:26,880
а затем поместите на него дополнительный

765
00:33:26,880 --> 00:33:28,960
слой классификатора с чем-то

766
00:33:28,960 --> 00:33:32,000
как классификатор softmax, этот

767
00:33:32,000 --> 00:33:34,000
метод использовался, и он действительно работает

768
00:33:34,000 --> 00:33:36,480
достаточно хорошо, и если вы как бы хорошо обучите

769
00:33:36,480 --> 00:33:38,720
эту модель от начала до конца, на самом деле это

770
00:33:38,720 --> 00:33:42,159
будет мотивировано для сохранения

771
00:33:42,159 --> 00:33:44,240
информации о настроениях в скрытом состоянии

772
00:33:44,240 --> 00:33:46,159
текущей нейронной сети, потому что это

773
00:33:46,159 --> 00:33:48,640
позволит ей  чтобы лучше предсказать

774
00:33:48,640 --> 00:33:50,399
тональность всего предложения,

775
00:33:50,399 --> 00:33:53,440
которое является последней задачей и, следовательно, функцией потерь,

776
00:33:53,440 --> 00:33:55,600
которую мы передаем сети,

777
00:33:55,600 --> 00:33:57,760
но оказывается, что вы обычно можете

778
00:33:57,760 --> 00:34:00,399
добиться большего, фактически выполняя

779
00:34:00,399 --> 00:34:03,519
такие вещи, как ввод всех скрытых состояний

780
00:34:03,519 --> 00:34:05,919
в предложение и  кодирование, возможно, путем

781
00:34:05,919 --> 00:34:08,639
создания предложения,

782
00:34:08,639 --> 00:34:11,440
кодирующего поэлементный максимум или

783
00:34:11,440 --> 00:34:14,399
поэлементное среднее всех скрытых состояний,

784
00:34:14,399 --> 00:34:17,040
потому что это больше s  ymmetrically

785
00:34:18,399 --> 00:34:22,800
кодирует скрытое состояние на каждом временном шаге,

786
00:34:24,639 --> 00:34:26,719
еще одно большое использование рекуррентных нейронных

787
00:34:26,719 --> 00:34:28,879
сетей - это

788
00:34:28,879 --> 00:34:32,000
то, что я назову модулем языкового кодирования,

789
00:34:32,000 --> 00:34:36,000
поэтому каждый раз, когда у вас есть

790
00:34:36,000 --> 00:34:38,079
текст, например, здесь, у нас есть вопрос

791
00:34:38,079 --> 00:34:41,599
о том, какой национальности был Бетховен,

792
00:34:41,599 --> 00:34:44,239
мы бы хотели построить  какое-то

793
00:34:44,239 --> 00:34:47,199
более новое представление этого, поэтому один из способов

794
00:34:47,199 --> 00:34:50,480
сделать это -

795
00:34:50,480 --> 00:34:52,800
запустить текущую нейронную сеть поверх нее, а

796
00:34:52,800 --> 00:34:55,199
затем, как и в прошлый раз,

797
00:34:55,199 --> 00:34:57,520
либо принять окончательное скрытое состояние, либо

798
00:34:57,520 --> 00:34:59,599
взять какую-то

799
00:34:59,599 --> 00:35:01,839
функцию всех скрытых состояний и

800
00:35:01,839 --> 00:35:05,040
сказать, что  представление предложения,

801
00:35:05,040 --> 00:35:07,200
и мы могли бы сделать то же самое

802
00:35:07,200 --> 00:35:09,760
для контекста, поэтому для

803
00:35:09,760 --> 00:35:11,440
ответа на вопрос мы собираемся построить дополнительную

804
00:35:11,440 --> 00:35:13,119
структуру нейронной

805
00:35:13,119 --> 00:35:14,880
сети поверх этого, и мы узнаем больше

806
00:35:14,880 --> 00:35:17,520
об этом через пару недель, когда

807
00:35:17,520 --> 00:35:19,680
у нас будет вопрос  отвечая на лекцию,

808
00:35:20,400 --> 00:35:23,280
но ключевым моментом является то, что мы построили до сих пор, мы

809
00:35:23,280 --> 00:35:26,079
используем для получения представления предложения, поэтому

810
00:35:26,079 --> 00:35:28,720
это модуль языкового кодирования,

811
00:35:28,720 --> 00:35:31,520
так что это была часть языкового кодирования,

812
00:35:31,520 --> 00:35:35,680
мы также можем использовать rnns для декодирования i  на

813
00:35:35,680 --> 00:35:38,079
язык, и это обычно используется в

814
00:35:40,640 --> 00:35:43,040
обобщении машинного перевода для распознавания речи, поэтому, если у нас есть распознаватель речи,

815
00:35:43,040 --> 00:35:46,480
входом является аудиосигнал,

816
00:35:46,480 --> 00:35:48,880
и мы хотим хорошо декодировать его

817
00:35:48,880 --> 00:35:51,760
на язык, что мы могли бы сделать, это

818
00:35:51,760 --> 00:35:54,560
использовать некоторую функцию ввода, которая является

819
00:35:54,560 --> 00:35:56,800
вероятно, сама будет нейронной сетью

820
00:35:56,800 --> 00:35:58,560
в качестве

821
00:35:58,560 --> 00:36:00,400
начального

822
00:36:00,400 --> 00:36:02,960
скрытого состояния

823
00:36:02,960 --> 00:36:04,880
нашего rnn lm,

824
00:36:04,880 --> 00:36:08,320
а затем мы говорим начать генерацию текста

825
00:36:08,320 --> 00:36:11,520
на основе этого, и поэтому мы должны затем

826
00:36:11,520 --> 00:36:13,760
генерировать слово за раз с помощью метода,

827
00:36:13,760 --> 00:36:15,359
который мы только что рассмотрели,

828
00:36:15,359 --> 00:36:18,079
мы превращаем речь в  текст,

829
00:36:18,079 --> 00:36:20,320
так что это пример модели условного

830
00:36:20,320 --> 00:36:22,160
языка, потому что теперь мы

831
00:36:22,160 --> 00:36:24,640
генерируем текст, обусловленный

832
00:36:24,640 --> 00:36:28,240
речевым сигналом, и большую часть времени вы

833
00:36:28,240 --> 00:36:29,119
можете делать

834
00:36:29,119 --> 00:36:31,040
интересные более сложные вещи с

835
00:36:31,040 --> 00:36:33,599
рекуррентными нейронными сетями, создавая

836
00:36:33,599 --> 00:36:36,560
модели условного языка в другом

837
00:36:36,560 --> 00:36:38,400
месте, которое вы можете использовать  условные языковые

838
00:36:38,400 --> 00:36:41,920
модели предназначены для задач классификации текста

839
00:36:41,920 --> 00:36:44,720
и включают классификацию тональности,

840
00:36:44,720 --> 00:36:47,520
поэтому, если вы можете обусловить

841
00:36:47,520 --> 00:36:48,640
свою

842
00:36:48,640 --> 00:36:50,560
языковую модель на основе своего рода

843
00:36:50,560 --> 00:36:52,079
настроения, вы можете создать своего рода

844
00:36:52,079 --> 00:36:54,400
классификатор для этого и другого использования, которое

845
00:36:54,400 --> 00:36:56,480
мы увидим много в следующем классе, для

846
00:36:56,480 --> 00:36:59,280
машинного перевода,

847
00:36:59,280 --> 00:37:00,160
хорошо,

848
00:37:00,160 --> 00:37:03,040
так что это конец вступления

849
00:37:03,040 --> 00:37:05,280
к работе

850
00:37:06,079 --> 00:37:07,760
с

851
00:37:07,760 --> 00:37:09,520
повторяющимися нейронными сетями и языковыми

852
00:37:09,520 --> 00:37:13,200
моделями, теперь я хочу переместить  и расскажем

853
00:37:13,200 --> 00:37:15,920
вам о том факте, что все

854
00:37:15,920 --> 00:37:18,960
не идеально, и у этих повторяющихся нейронных

855
00:37:18,960 --> 00:37:21,359
сетей, как правило, есть несколько

856
00:37:21,359 --> 00:37:22,560
проблем,

857
00:37:22,560 --> 00:37:25,760
и мы поговорим о них, а затем

858
00:37:25,760 --> 00:37:28,079
частично, что затем будет мотивировать

859
00:37:28,079 --> 00:37:29,680
придумывать более продвинутую архитектуру рекуррентной нейронной

860
00:37:29,680 --> 00:37:31,599
сети.

861
00:37:31,599 --> 00:37:34,800
Итак, первая проблема, которую следует упомянуть, -

862
00:37:34,800 --> 00:37:37,680
это идея того, что называется исчезающими

863
00:37:37,680 --> 00:37:38,960
градиентами,

864
00:37:38,960 --> 00:37:42,000
и что это значит. Что ж, в

865
00:37:42,000 --> 00:37:46,079
конце нашей последовательности мы имеем некоторую общую

866
00:37:46,079 --> 00:37:49,040
потерю, которую мы вычисляем, и

867
00:37:49,040 --> 00:37:51,920
что мы хотим сделать, это распространить

868
00:37:51,920 --> 00:37:53,760
эту потерю в обратном направлении

869
00:37:53,760 --> 00:37:55,920
и  мы хотим распространить его в обратном направлении прямо

870
00:37:55,920 --> 00:37:58,480
по последовательности, поэтому мы

871
00:37:58,480 --> 00:38:02,240
прорабатываем частичные значения j4

872
00:38:02,240 --> 00:38:04,480
относительно скрытого состояния во время

873
00:38:04,480 --> 00:38:06,560
один, и когда у нас будет более длинная последовательность,

874
00:38:06,560 --> 00:38:09,440
мы будем работать над  но частичные значения j20

875
00:38:09,440 --> 00:38:11,760
относительно скрытого состояния в момент времени

876
00:38:11,760 --> 00:38:13,200
один, и

877
00:38:13,200 --> 00:38:17,200
как мы это делаем хорошо, как мы это делаем, так это

878
00:38:17,200 --> 00:38:20,079
по композиции и правилу цепочки, у нас

879
00:38:20,079 --> 00:38:23,040
есть большая длинная цепочка, прокручивающаяся по

880
00:38:23,040 --> 00:38:24,800
всей последовательности,

881
00:38:24,800 --> 00:38:25,760
ну

882
00:38:25,760 --> 00:38:28,800
хорошо, если мы '  Делая это,

883
00:38:28,800 --> 00:38:31,520
вы знаете, что мы умножаем тонны

884
00:38:31,520 --> 00:38:33,680
вещей вместе,

885
00:38:33,680 --> 00:38:36,560
и поэтому опасность того, что имеет тенденцию

886
00:38:36,560 --> 00:38:37,599
происходить,

887
00:38:37,599 --> 00:38:40,200
состоит в том, что по мере того, как мы делаем

888
00:38:40,200 --> 00:38:43,040
это умножение,

889
00:38:43,040 --> 00:38:45,760
эти частичные

890
00:38:45,760 --> 00:38:49,119
последовательные скрытые состояния становятся небольшими,

891
00:38:49,119 --> 00:38:52,160
и поэтому происходит то, что мы идем дальше

892
00:38:52,160 --> 00:38:54,800
градиент становится все меньше и меньше

893
00:38:54,800 --> 00:38:58,720
и меньше и начинает исчезать,

894
00:38:58,720 --> 00:39:01,920
и в той степени, в которой он исчезает

895
00:39:01,920 --> 00:39:04,320
хорошо, у нас как бы нет восходящего

896
00:39:04,320 --> 00:39:06,560
градиента, и поэтому мы вообще не будем

897
00:39:06,560 --> 00:39:09,520
изменять параметры, и это

898
00:39:09,520 --> 00:39:12,839
оказывается  довольно проблематично,

899
00:39:14,800 --> 00:39:17,760
так что следующая пара слайдов как

900
00:39:17,760 --> 00:39:22,320
бы немного расскажет о том, почему и как

901
00:39:22,320 --> 00:39:24,320
это происходит,

902
00:39:24,320 --> 00:39:27,440
то, что представлено здесь, является своего рода лишь

903
00:39:27,440 --> 00:39:29,119
полуформальным

904
00:39:30,240 --> 00:39:32,320
взмахом руки над проблемами,

905
00:39:32,320 --> 00:39:34,960
которых вы могли бы ожидать, если действительно

906
00:39:34,960 --> 00:39:37,359
хотите  так  Чтобы вникнуть во все

907
00:39:37,359 --> 00:39:39,280
детали этого эм, вы должны взглянуть на

908
00:39:39,280 --> 00:39:41,599
пару статей, которые упомянуты мелким

909
00:39:41,599 --> 00:39:44,400
шрифтом внизу слайда, но в

910
00:39:44,400 --> 00:39:46,079
любом случае, если вы помните, что это

911
00:39:46,079 --> 00:39:49,040
наше основное уравнение рекуррентной нейронной сети,

912
00:39:49,040 --> 00:39:51,599
давайте рассмотрим  простой

913
00:39:51,599 --> 00:39:55,920
случай, предположим, что мы как бы избавляемся от нашей

914
00:39:55,920 --> 00:39:58,720
нелинейности и просто предполагаем, что это

915
00:39:58,720 --> 00:40:00,640
функция

916
00:40:00,640 --> 00:40:02,960
идентичности, и тогда, когда мы

917
00:40:02,960 --> 00:40:04,720
прорабатываем части скрытого состояния по

918
00:40:04,720 --> 00:40:07,680
отношению к предыдущему скрытому состоянию,

919
00:40:07,680 --> 00:40:10,800
мы можем обработать их в  обычным способом в

920
00:40:10,800 --> 00:40:13,119
соответствии с правилом цепочки,

921
00:40:13,119 --> 00:40:15,440
а затем, если

922
00:40:15,440 --> 00:40:16,480
сигма

923
00:40:16,480 --> 00:40:17,920
- это

924
00:40:17,920 --> 00:40:20,880
просто функция идентичности,

925
00:40:20,880 --> 00:40:22,800
тогда все становится

926
00:40:22,800 --> 00:40:24,400
для нас действительно легко, поэтому

927
00:40:25,920 --> 00:40:28,960
только сигма просто уходит, и только

928
00:40:28,960 --> 00:40:30,319
первый член

929
00:40:30,319 --> 00:40:34,160
включает um h в момент времени t минус один, поэтому более

930
00:40:34,160 --> 00:40:37,680
поздние члены  уйти, и поэтому наш

931
00:40:37,680 --> 00:40:40,960
градиент заканчивается так,

932
00:40:40,960 --> 00:40:43,599
что он делает это только для одного временного

933
00:40:43,599 --> 00:40:45,920
шага, что происходит, когда вы хотите

934
00:40:45,920 --> 00:40:48,560
проработать эти частичные шаги несколько раз,

935
00:40:48,560 --> 00:40:50,319
так что

936
00:40:50,319 --> 00:40:53,920
мы хотим отработать его часть

937
00:40:53,920 --> 00:40:57,760
времени  шаг i в отношении j

938
00:40:57,760 --> 00:40:59,040
um

939
00:40:59,040 --> 00:41:02,880
хорошо то, что мы в итоге получаем, является

940
00:41:02,880 --> 00:41:06,880
продуктом частичных последовательных временных шагов,

941
00:41:06,880 --> 00:41:10,400
и хотя каждый из

942
00:41:10,400 --> 00:41:15,520
них выходит как wh, и поэтому мы в конечном итоге

943
00:41:15,520 --> 00:41:21,280
получаем wh возведены в силу эльфов

944
00:41:21,280 --> 00:41:24,240
и, что ж, наша потенциальная проблема  состоит в том, что

945
00:41:24,240 --> 00:41:28,400
если wh в некотором смысле мало, то этот

946
00:41:28,400 --> 00:41:31,040
член становится экспоненциально проблематичным, и он

947
00:41:31,040 --> 00:41:33,839
становится исчезающе малым, поскольку

948
00:41:33,839 --> 00:41:36,480
длина нашей последовательности становится большой,

949
00:41:36,480 --> 00:41:39,920
что мы можем понимать под маленьким колодцем,

950
00:41:39,920 --> 00:41:41,839
матрица мала,

951
00:41:41,839 --> 00:41:44,960
если все ее собственные значения меньше единицы,

952
00:41:44,960 --> 00:41:47,680
поэтому мы можем переписать  что происходит с

953
00:41:47,680 --> 00:41:50,319
этим успехом умножения с использованием

954
00:41:50,319 --> 00:41:53,040
собственных значений и собственных векторов

955
00:41:53,040 --> 00:41:55,520
um и я должен сказать, что все, что я могу

956
00:41:55,520 --> 00:41:57,599
векторные значения меньше единицы, является

957
00:41:57,599 --> 00:41:59,520
достаточным, но не необходимым условием

958
00:41:59,520 --> 00:42:02,720
для того, что я собираюсь сказать um right, поэтому мы

959
00:42:02,720 --> 00:42:04,319
можем переписать

960
00:42:04,319 --> 00:42:08,160
вещи, используя собственные векторы  в качестве основы,

961
00:42:08,160 --> 00:42:11,280
и если мы это сделаем,

962
00:42:11,280 --> 00:42:13,440
мы получим

963
00:42:14,440 --> 00:42:16,880
собственные значения, возведенные в восьмую

964
00:42:16,880 --> 00:42:20,960
степень, и поэтому, если все наши собственные

965
00:42:20,960 --> 00:42:22,960
значения меньше единицы, если мы берем

966
00:42:22,960 --> 00:42:25,200
число меньше единицы, а затем  возведя его

967
00:42:25,200 --> 00:42:27,119
в восьмую степень, которая будет

968
00:42:27,119 --> 00:42:30,160
приближаться к нулю по мере увеличения длины последовательности,

969
00:42:30,160 --> 00:42:33,680
и поэтому градиент исчезает,

970
00:42:33,680 --> 00:42:36,160
хорошо, теперь реальность более сложна,

971
00:42:36,160 --> 00:42:38,319
чем это, потому что на самом деле мы всегда

972
00:42:38,319 --> 00:42:40,880
используем нелинейную сигму активации, но вы

973
00:42:40,880 --> 00:42:43,520
знаете, что в принципе это своего рода

974
00:42:43,520 --> 00:42:46,480
То же самое, кроме того, что мы должны

975
00:42:46,480 --> 00:42:48,400
учитывать влияние нелинейной

976
00:42:48,400 --> 00:42:50,960
активации,

977
00:42:50,960 --> 00:42:53,920
хорошо, так почему это проблема, что

978
00:42:53,920 --> 00:42:55,680
градиенты хорошо исчезают?

979
00:42:56,640 --> 00:42:58,480
Предположим, мы хотим посмотреть на

980
00:42:58,480 --> 00:43:01,359
влияние временных шагов в

981
00:43:01,359 --> 00:43:02,560
будущем

982
00:43:03,760 --> 00:43:06,319
на представления  мы хотим, чтобы в

983
00:43:06,319 --> 00:43:10,000
начале предложения было хорошо

984
00:43:10,000 --> 00:43:12,480
то, что происходит в конце предложения,

985
00:43:12,480 --> 00:43:14,400
просто не будет давать много

986
00:43:14,400 --> 00:43:17,920
информации о том, что мы должны

987
00:43:17,920 --> 00:43:21,680
хранить в векторе h в момент времени 1,

988
00:43:21,680 --> 00:43:24,400
тогда как, с другой стороны, потеря на

989
00:43:24,400 --> 00:43:26,800
временном шаге два равна  будет давать

990
00:43:26,800 --> 00:43:29,760
много информации о том, что

991
00:43:29,760 --> 00:43:31,760
должно храниться в скрытом векторе на

992
00:43:39,599 --> 00:43:44,079
первом временном шаге, поэтому конечный результат заключается в том, что эти простые rnn очень хороши при моделировании вблизи  y,

993
00:43:45,200 --> 00:43:48,000
но они совсем не годятся для моделирования

994
00:43:48,000 --> 00:43:50,800
долгосрочных эффектов, потому что сигнал градиента

995
00:43:50,800 --> 00:43:53,760
издалека просто теряется слишком

996
00:43:53,760 --> 00:43:54,480
сильно,

997
00:43:54,480 --> 00:43:57,280
и поэтому модель никогда не сможет

998
00:43:57,280 --> 00:43:59,920
эффективно узнать,

999
00:43:59,920 --> 00:44:03,040
какую информацию издалека было

1000
00:44:03,040 --> 00:44:05,920
бы полезно сохранить в  Будущее,

1001
00:44:05,920 --> 00:44:08,640
так что давайте рассмотрим это конкретно на

1002
00:44:10,000 --> 00:44:12,720
примере языковых моделей, над которыми мы работали,

1003
00:44:12,720 --> 00:44:16,160
так что вот фрагмент текста ммм, когда она

1004
00:44:16,160 --> 00:44:18,000
пыталась распечатать свои билеты, она обнаружила,

1005
00:44:18,000 --> 00:44:20,480
что в принтере закончился тонер, она

1006
00:44:20,480 --> 00:44:22,560
пошла в магазин канцелярских товаров, чтобы купить  больше

1007
00:44:22,560 --> 00:44:25,440
тонера цена на него была очень завышена после

1008
00:44:25,440 --> 00:44:27,200
установки тонера в принтер,

1009
00:44:27,200 --> 00:44:29,440
она наконец напечатала ее,

1010
00:44:29,440 --> 00:44:32,240
и вы все умные люди, я

1011
00:44:32,240 --> 00:44:34,480
надеюсь, вы все можете догадаться, какое

1012
00:44:34,480 --> 00:44:38,319
слово будет дальше, это должны быть билеты,

1013
00:44:38,319 --> 00:44:41,760
но проблема в том, что для  rnn,

1014
00:44:41,760 --> 00:44:44,720
чтобы начать изучать такие случаи, ему

1015
00:44:44,720 --> 00:44:46,880
пришлось бы в своем

1016
00:44:46,880 --> 00:44:49,920
скрытом состоянии передать память словарных

1017
00:44:49,920 --> 00:44:52,800
билетов для чего-то вроде того, что это около

1018
00:44:52,800 --> 00:44:55,680
30 скрытых обновлений состояния,

1019
00:44:55,680 --> 00:44:56,720
и

1020
00:44:56,720 --> 00:45:00,400
мы будем тренироваться на этом  um пример, и поэтому

1021
00:45:00,400 --> 00:45:03,440
мы захотим, чтобы он предсказывал, что билеты

1022
00:45:03,440 --> 00:45:06,000
- это следующее слово, и поэтому обновление градиента

1023
00:45:06,000 --> 00:45:08,400
будет отправлено обратно

1024
00:45:08,400 --> 00:45:11,440
через скрытые состояния lstm,

1025
00:45:11,440 --> 00:45:13,920
соответствующие этому предложению,

1026
00:45:13,920 --> 00:45:15,280
и

1027
00:45:15,280 --> 00:45:17,680
это должно сообщить модели,

1028
00:45:17,680 --> 00:45:19,680
что хорошо сохранять информацию о

1029
00:45:19,680 --> 00:45:21,359
тикеры слов, потому что это может быть

1030
00:45:21,359 --> 00:45:23,680
полезно в будущем, здесь это было полезно

1031
00:45:23,680 --> 00:45:25,280
в будущем,

1032
00:45:25,280 --> 00:45:27,119
но проблема в том, что

1033
00:45:27,119 --> 00:45:29,839
сигнал градиента станет

1034
00:45:29,839 --> 00:45:33,200
слишком слабым после набора слов,

1035
00:45:33,200 --> 00:45:36,640
и он просто никогда не узнает эту зависимость

1036
00:45:36,640 --> 00:45:40,160
и, следовательно, то, что мы  найти на практике,

1037
00:45:40,160 --> 00:45:42,560
модель просто неспособна предсказать аналогичные

1038
00:45:42,560 --> 00:45:45,520
зависимости на большом расстоянии во время тестирования

1039
00:45:45,520 --> 00:45:48,240
Я потратил довольно много времени на

1040
00:45:48,240 --> 00:45:50,240
исчезающие градиенты, а затем действительно

1041
00:45:50,240 --> 00:45:53,359
исчезающие градиенты являются большой проблемой

1042
00:45:53,359 --> 00:45:57,839
на практике с использованием

1043
00:45:57,839 --> 00:45:59,680
рекуррентных нейронных сетей на длинных

1044
00:45:59,680 --> 00:46:01,440
последовательностях

1045
00:46:01,440 --> 00:46:03,920
но вы знаете, что я должен отдать должное тому

1046
00:46:03,920 --> 00:46:05,359
факту, что на самом деле у вас может быть и

1047
00:46:05,359 --> 00:46:07,599
противоположная проблема, у вас также могут быть

1048
00:46:07,599 --> 00:46:09,520
взрывающиеся градиенты,

1049
00:46:09,520 --> 00:46:10,319
поэтому,

1050
00:46:10,319 --> 00:46:14,240
если градиент становится слишком би  g, это

1051
00:46:14,240 --> 00:46:17,280
также проблема, и это проблема,

1052
00:46:17,280 --> 00:46:20,079
потому что

1053
00:46:20,079 --> 00:46:23,200
шаг обновления стохастического градиента становится слишком большим, так

1054
00:46:23,200 --> 00:46:26,880
что помните, что наше обновление параметров

1055
00:46:26,880 --> 00:46:29,359
основано на произведении скорости обучения

1056
00:46:29,359 --> 00:46:31,440
и градиента, поэтому, если ваш

1057
00:46:31,440 --> 00:46:34,079
градиент огромен, вы правильно рассчитали,

1058
00:46:34,079 --> 00:46:36,319
о, это  здесь большой уклон, это

1059
00:46:36,319 --> 00:46:38,960
имеет уклон 10000,

1060
00:46:38,960 --> 00:46:40,880
тогда ваше

1061
00:46:40,880 --> 00:46:43,680
обновление параметров может быть произвольно

1062
00:46:43,680 --> 00:46:45,440
большим,

1063
00:46:45,440 --> 00:46:47,760
и это потенциально проблематично, что

1064
00:46:47,760 --> 00:46:50,000
может вызвать плохое обновление, когда вы делаете

1065
00:46:50,000 --> 00:46:53,839
огромный шаг, и вы в конечном итоге получаете странную и

1066
00:46:53,839 --> 00:46:54,720
плохую

1067
00:46:54,720 --> 00:46:57,119
конфигурацию параметров, поэтому вы  Вы вроде как

1068
00:46:57,119 --> 00:46:59,040
думаете, что вы подходите

1069
00:46:59,040 --> 00:47:01,599
к крутому холму, чтобы подняться, и вы

1070
00:47:01,599 --> 00:47:03,680
хотите подняться на холм с высокой

1071
00:47:03,680 --> 00:47:05,920
вероятностью, но на самом деле градиент

1072
00:47:05,920 --> 00:47:10,640
настолько крутой, что вы делаете огромное

1073
00:47:10,640 --> 00:47:12,880
обновление, а затем внезапно ваши

1074
00:47:12,880 --> 00:47:14,960
параметры заканчиваются  Айова, и вы вообще потеряли свой

1075
00:47:14,960 --> 00:47:16,880
холм, есть также

1076
00:47:16,880 --> 00:47:18,560
практическая трудность, заключающаяся в том, что у нас сейчас только

1077
00:47:18,560 --> 00:47:20,559
такое большое разрешение чисел с плавающей запятой,

1078
00:47:21,680 --> 00:47:22,640
поэтому,

1079
00:47:22,640 --> 00:47:25,359
если ваш градиент становится слишком крутым, вы

1080
00:47:25,359 --> 00:47:26,720
начинаете получать  Не говоря уже

1081
00:47:26,720 --> 00:47:29,040
о числах в ваших расчетах, которые

1082
00:47:29,040 --> 00:47:31,920
разрушают всю вашу тяжелую тренировочную работу,

1083
00:47:31,920 --> 00:47:35,280
мы используем своего рода простое решение,

1084
00:47:35,280 --> 00:47:38,079
которое называется градиентной обрезкой, то

1085
00:47:38,079 --> 00:47:41,599
есть мы выбираем какое-то разумное

1086
00:47:41,599 --> 00:47:44,160
число и говорим, что просто не собираемся

1087
00:47:44,160 --> 00:47:45,760
иметь дело с градиентами  которые больше,

1088
00:47:45,760 --> 00:47:47,839
чем это число.

1089
00:47:58,319 --> 00:48:01,040
градиент

1090
00:48:01,040 --> 00:48:03,599
больше, чем этот порог, мы

1091
00:48:03,599 --> 00:48:06,400
просто уменьшаем его, что означает,

1092
00:48:06,400 --> 00:48:09,200
что затем мы делаем меньшее обновление градиента,

1093
00:48:09,200 --> 00:48:12,400
поэтому мы все еще движемся в

1094
00:48:12,400 --> 00:48:13,680
том же

1095
00:48:13,680 --> 00:48:16,079
направлении, но мы делаем меньший

1096
00:48:16,079 --> 00:48:17,839
шаг,

1097
00:48:17,839 --> 00:48:19,839
поэтому выполнение этого градиентного отсечения

1098
00:48:19,839 --> 00:48:21,920
важно, как

1099
00:48:21,920 --> 00:48:23,760
вы знаете  но эту проблему легко

1100
00:48:23,760 --> 00:48:26,079
решить,

1101
00:48:27,040 --> 00:48:30,160
ладно, эм, поэтому

1102
00:48:30,160 --> 00:48:34,720
нам еще предстоит решить, как на самом деле решить

1103
00:48:34,720 --> 00:48:38,240
эту проблему исчезающих градиентов.

1104
00:48:45,680 --> 00:48:47,280
s,

1105
00:48:47,280 --> 00:48:48,800
и

1106
00:48:48,800 --> 00:48:51,920
один из способов подумать об этом интуитивно

1107
00:48:51,920 --> 00:48:55,280
: на каждом временном шаге у нас есть скрытое

1108
00:48:55,280 --> 00:48:56,400
состояние,

1109
00:48:56,400 --> 00:49:00,160
и скрытое состояние полностью

1110
00:49:00,160 --> 00:49:03,520
изменяется на каждом временном шаге, и оно

1111
00:49:03,520 --> 00:49:06,800
изменяется мультипликативным образом

1112
00:49:06,800 --> 00:49:09,200
путем умножения на wh, а затем

1113
00:49:09,200 --> 00:49:10,079
через

1114
00:49:10,079 --> 00:49:11,200
um

1115
00:49:11,200 --> 00:49:13,599
и  нелинейность,

1116
00:49:13,599 --> 00:49:16,000
например, мы могли бы добиться большего

1117
00:49:16,000 --> 00:49:17,839
прогресса,

1118
00:49:17,839 --> 00:49:20,960
если бы могли более гибко

1119
00:49:20,960 --> 00:49:24,319
поддерживать память в нашей рекуррентной

1120
00:49:24,319 --> 00:49:25,920
нейронной сети,

1121
00:49:25,920 --> 00:49:27,319
которой мы могли бы

1122
00:49:27,319 --> 00:49:30,800
манипулировать более гибко,

1123
00:49:30,800 --> 00:49:33,440
что позволяет нам легче сохранять

1124
00:49:33,440 --> 00:49:34,800
информацию,

1125
00:49:34,800 --> 00:49:36,480
и поэтому эту

1126
00:49:36,480 --> 00:49:39,599
идею начали  думали,

1127
00:49:39,599 --> 00:49:41,440
и на самом деле они начали думать об

1128
00:49:41,440 --> 00:49:43,680
этом очень давно,

1129
00:49:43,680 --> 00:49:46,290
в конце 1990-х

1130
00:49:46,290 --> 00:49:48,559
[Музыка],

1131
00:49:48,559 --> 00:49:51,040
и Хак Райт и Шмидт Гувер

1132
00:49:51,040 --> 00:49:52,880
придумали эту идею,

1133
00:49:52,880 --> 00:49:55,920
которую назвали долгосрочной краткосрочной памятью

1134
00:49:57,359 --> 00:49:59,440
как решение проблемы

1135
00:49:59,440 --> 00:50:01,119
исчезающих градиентов,

1136
00:50:01,119 --> 00:50:04,640
я имею в виду  так что эта статья 1997 года - это статья, которую

1137
00:50:04,640 --> 00:50:07,680
вы всегда видите процитированной для lstms, но вы

1138
00:50:07,680 --> 00:50:09,440
знаете, что на самом деле

1139
00:50:09,440 --> 00:50:11,680
с точки зрения того, что мы теперь понимаем как

1140
00:50:11,680 --> 00:50:13,200
lstm, в

1141
00:50:13,200 --> 00:50:16,240
ней отсутствовала часть  Фактически

1142
00:50:16,240 --> 00:50:18,319
отсутствует то, что в ретроспективе

1143
00:50:18,319 --> 00:50:21,520
оказалось самой важной частью

1144
00:50:22,480 --> 00:50:25,920
современного lstm, так что в некотором смысле

1145
00:50:25,920 --> 00:50:29,359
настоящая бумага, из-за которой появился современный lstm,

1146
00:50:29,359 --> 00:50:32,960
- это немного более поздняя статья Герша,

1147
00:50:32,960 --> 00:50:34,880
все еще Schmidt hoover and cummins from

1148
00:50:34,880 --> 00:50:37,680
2000 um, которая  дополнительно вводит

1149
00:50:37,680 --> 00:50:40,000
ворота забывания, которые я объясню через

1150
00:50:40,000 --> 00:50:42,400
минуту,

1151
00:50:42,400 --> 00:50:45,839
да, так что это был очень умный

1152
00:50:45,839 --> 00:50:49,359
материал, который был представлен, и позже выяснилось,

1153
00:50:49,359 --> 00:50:52,720
что он окажет огромное влияние,

1154
00:50:52,720 --> 00:50:54,319
если я просто

1155
00:50:54,319 --> 00:50:56,559
отклонюсь от технической части еще на

1156
00:50:56,559 --> 00:50:58,400
один момент

1157
00:50:58,400 --> 00:51:00,880
это вы знаете для тех из вас, кто в наши

1158
00:51:00,880 --> 00:51:04,319
дни думает, что освоение ваших

1159
00:51:04,319 --> 00:51:08,640
сетей - это путь к славе и богатству,

1160
00:51:08,640 --> 00:51:11,119
что забавно, вы знаете, что в то время,

1161
00:51:11,119 --> 00:51:12,960
когда эта работа была сделана,

1162
00:51:12,960 --> 00:51:16,559
это было неправдой.

1163
00:51:18,720 --> 00:51:22,240
сетей, и хотя долгосрочные краткосрочные

1164
00:51:22,240 --> 00:51:24,079
воспоминания оказались одной

1165
00:51:24,079 --> 00:51:26,079
из самых важных успешных и

1166
00:51:26,079 --> 00:51:29,359
влиятельных идей в нейронных сетях на

1167
00:51:30,240 --> 00:51:32,960
последующие 25 лет, на

1168
00:51:32,960 --> 00:51:35,520
самом деле оригинальные авторы  не получил

1169
00:51:35,520 --> 00:51:38,079
признания за это, поэтому они оба

1170
00:51:38,079 --> 00:51:41,280
теперь профессора в немецких университетах,

1171
00:51:41,280 --> 00:51:43,599
но ястреб-писатель

1172
00:51:43,599 --> 00:51:46,240
занялся биоинформатикой,

1173
00:51:47,280 --> 00:51:49,040
чтобы найти

1174
00:51:49,040 --> 00:51:52,720
что-то, чем им заняться, и предположить, что на самом деле

1175
00:51:52,720 --> 00:51:56,319
занимается своего рода мультимедийными исследованиями,

1176
00:51:56,319 --> 00:52:00,400
так что это судьбы истории

1177
00:52:01,599 --> 00:52:05,440
Итак, что такое lstm, так

1178
00:52:05,440 --> 00:52:08,720
что ключевое нововведение lstm - это

1179
00:52:08,720 --> 00:52:12,079
сказать хорошо, а не просто иметь один

1180
00:52:12,079 --> 00:52:13,839
скрытый вектор

1181
00:52:13,839 --> 00:52:17,760
в рекуррентной модели, мы собираемся

1182
00:52:17,760 --> 00:52:19,760
построить модель с

1183
00:52:19,760 --> 00:52:21,599
двумя

1184
00:52:21,599 --> 00:52:25,200
скрытыми векторами на каждом временном шаге, один из

1185
00:52:25,200 --> 00:52:28,079
которых все еще  называется скрытым состоянием h,

1186
00:52:28,079 --> 00:52:30,319
а другое из которых называется

1187
00:52:30,319 --> 00:52:32,880
состоянием ячейки,

1188
00:52:32,880 --> 00:52:35,760
теперь вы знаете, что, возможно, ретроспективно

1189
00:52:35,760 --> 00:52:37,760
они были названы неправильно, потому что, как

1190
00:52:37,760 --> 00:52:39,200
вы увидите, когда мы рассмотрим более

1191
00:52:39,200 --> 00:52:42,160
подробно, в некотором смысле ячейка более

1192
00:52:42,160 --> 00:52:44,400
эквивалентна скрытому состоянию

1193
00:52:44,400 --> 00:52:47,359
простого rnn тогда наоборот, но

1194
00:52:47,359 --> 00:52:49,280
мы просто используем имена, которые все

1195
00:52:49,280 --> 00:52:51,920
используют, так что оба они являются векторами

1196
00:52:51,920 --> 00:52:53,200
длины n

1197
00:52:53,200 --> 00:52:55,280
um, и это будет ячейка, которая

1198
00:52:55,280 --> 00:52:58,800
хранит долгосрочную информацию,

1199
00:52:58,800 --> 00:53:00,800
и поэтому мы  Чтобы иметь что-то, что

1200
00:53:00,800 --> 00:53:03,599
больше похоже на память, так что

1201
00:53:03,599 --> 00:53:06,480
значение, такое как барабан и компьютер, так

1202
00:53:06,480 --> 00:53:09,599
что ячейка спроектирована так, чтобы вы могли читать

1203
00:53:09,599 --> 00:53:12,160
из нее, вы можете стирать ее части, и

1204
00:53:12,160 --> 00:53:14,160
вы можете записывать новую информацию в

1205
00:53:15,440 --> 00:53:18,880
ячейку и интересную часть

1206
00:53:18,880 --> 00:53:22,240
lstm - это управляющие структуры, которые

1207
00:53:22,240 --> 00:53:25,440
решают, как вы это делаете, поэтому

1208
00:53:25,440 --> 00:53:28,160
выбор информации для стирания записи и

1209
00:53:28,160 --> 00:53:30,800
чтения контролируется вероятностными

1210
00:53:30,800 --> 00:53:31,760
вентилями,

1211
00:53:31,760 --> 00:53:34,400
поэтому вентили также являются векторами длины

1212
00:53:34,400 --> 00:53:35,200
n,

1213
00:53:35,200 --> 00:53:39,200
и на каждом временном шаге um мы определяем

1214
00:53:39,200 --> 00:53:41,440
состояние для  векторов вентилей, поэтому каждый

1215
00:53:41,440 --> 00:53:43,520
элемент векторов вентилей является

1216
00:53:43,520 --> 00:53:45,440
вероятностью, поэтому они могут иметь вероятность открытия,

1217
00:53:45,440 --> 00:53:48,720
одну близкую вероятность, ноль

1218
00:53:48,720 --> 00:53:52,000
или где-то посередине, и их значение

1219
00:53:52,000 --> 00:53:54,880
будет говорить, сколько вы стираете,

1220
00:53:54,880 --> 00:53:58,640
сколько вы пишете, сколько вы читаете

1221
00:53:58,640 --> 00:54:00,960
и так  это динамические ворота со

1222
00:54:00,960 --> 00:54:03,280
значением, которое вычисляется на основе

1223
00:54:03,280 --> 00:54:06,319
текущего контекста.

1224
00:54:06,400 --> 00:54:09,119
Хорошо, поэтому в следующем слайде мы

1225
00:54:09,119 --> 00:54:11,920
рассмотрим уравнения lstm, но после

1226
00:54:11,920 --> 00:54:14,079
этого есть еще несколько графических слайдов,

1227
00:54:14,079 --> 00:54:17,040
которые будут про  возможно, будет легче усвоить

1228
00:54:17,040 --> 00:54:19,680
правильно, поэтому мы снова, как и прежде, чем это

1229
00:54:19,680 --> 00:54:21,839
наша текущая нейронная сеть, у нас есть

1230
00:54:21,839 --> 00:54:25,280
последовательность входных данных x и t,

1231
00:54:25,280 --> 00:54:27,280
и мы собираемся на каждом временном шаге

1232
00:54:27,280 --> 00:54:30,480
вычислять состояние ячейки и скрытое состояние,

1233
00:54:30,480 --> 00:54:33,839
так как мы это делаем?  поэтому сначала мы

1234
00:54:33,839 --> 00:54:36,839
собираемся вычислить значения трех

1235
00:54:36,839 --> 00:54:39,839
вентилей, и поэтому мы вычисляем значения вентилей,

1236
00:54:39,839 --> 00:54:42,799
используя уравнение, которое

1237
00:54:42,799 --> 00:54:46,160
идентично уравнению

1238
00:54:46,160 --> 00:54:49,839
для простой рекуррентной нейронной сети,

1239
00:54:49,839 --> 00:54:53,280
но, в частности, ой ой, извините, я

1240
00:54:53,280 --> 00:54:55,200
просто скажу, что  ворота являются первыми,

1241
00:54:55,200 --> 00:54:58,319
поэтому есть ворота забвения um, с помощью которых мы

1242
00:54:58,319 --> 00:55:00,880
можем контролировать то, что хранится

1243
00:55:00,880 --> 00:55:03,520
в ячейке на следующем временном шаге, по сравнению с

1244
00:55:03,520 --> 00:55:06,480
тем, что забыто, есть входной шлюз,

1245
00:55:06,480 --> 00:55:09,520
который будет определять, какие

1246
00:55:09,520 --> 00:55:12,160
части вычисленного нового содержимого ячейки будут

1247
00:55:12,160 --> 00:55:14,319
записаны в  ячейка памяти, и

1248
00:55:14,319 --> 00:55:16,880
есть выходной вентиль, который будет контролировать,

1249
00:55:16,880 --> 00:55:19,599
какие части ячейки памяти

1250
00:55:19,599 --> 00:55:21,760
перемещаются в скрытое состояние,

1251
00:55:21,760 --> 00:55:24,640
и поэтому каждый из них использует

1252
00:55:24,640 --> 00:55:27,200
логистическую функцию, потому что мы хотим, чтобы

1253
00:55:27,200 --> 00:55:30,640
они были в каждом элементе этого вектора.  ra

1254
00:55:30,640 --> 00:55:33,599
вероятность, которая скажет, следует ли

1255
00:55:33,599 --> 00:55:36,960
полностью забыть, частично забыть или

1256
00:55:36,960 --> 00:55:39,200
полностью вспомнить,

1257
00:55:39,200 --> 00:55:41,680
да, и уравнение для каждого из них

1258
00:55:41,680 --> 00:55:43,760
в точности похоже на простое уравнение r и n,

1259
00:55:43,760 --> 00:55:46,160
но, конечно, обратите внимание, что у нас

1260
00:55:46,160 --> 00:55:48,480
есть разные параметры для каждого из них, поэтому

1261
00:55:48,480 --> 00:55:52,160
мы  получили матрицу весов забвения w

1262
00:55:52,160 --> 00:55:55,119
со смещением

1263
00:55:55,119 --> 00:55:57,040
забвения и множителем забвения

1264
00:55:57,040 --> 00:56:00,559
входных данных,

1265
00:56:00,960 --> 00:56:04,079
хорошо, так что тогда у нас есть другие уравнения,

1266
00:56:04,079 --> 00:56:07,839
которые действительно являются механикой

1267
00:56:07,839 --> 00:56:09,280
lstm,

1268
00:56:09,280 --> 00:56:12,799
поэтому у нас есть что-то, что будет

1269
00:56:12,799 --> 00:56:15,440
вычислять новое содержимое ячейки, так что это наше

1270
00:56:15,440 --> 00:56:18,160
обновление кандидата  и поэтому

1271
00:56:18,160 --> 00:56:20,720
для вычисления кандидата обновления

1272
00:56:20,720 --> 00:56:23,440
мы снова, по сути, используем точно

1273
00:56:23,440 --> 00:56:26,880
такое же простое уравнение rnn, за исключением того, что

1274
00:56:26,880 --> 00:56:29,839
теперь обычно используется tanh, поэтому вы получаете

1275
00:56:29,839 --> 00:56:32,480
что-то, что, как обсуждалось в прошлый раз,

1276
00:56:32,480 --> 00:56:34,720
сбалансировано около нуля,

1277
00:56:34,720 --> 00:56:38,240
хорошо, поэтому для фактического обновления вещей

1278
00:56:38,240 --> 00:56:42,000
мы используем наши  ворота, поэтому для нашего нового

1279
00:56:42,000 --> 00:56:45,119
содержимого ячейки идея состоит в том, что мы

1280
00:56:45,119 --> 00:56:49,040
хотим запомнить некоторые, но, вероятно, не все,

1281
00:56:49,040 --> 00:56:51,040
что у нас было в ячейке с предыдущих

1282
00:56:51,040 --> 00:56:54,640
временных шагов, и мы хотим, чтобы  o

1283
00:56:54,640 --> 00:56:55,920
сохранить

1284
00:56:55,920 --> 00:56:59,040
некоторые, но, вероятно, не все значения,

1285
00:56:59,040 --> 00:57:02,960
которые мы вычислили в качестве обновления новой

1286
00:57:02,960 --> 00:57:03,839
ячейки,

1287
00:57:03,839 --> 00:57:07,839
и поэтому мы делаем это так: мы

1288
00:57:07,839 --> 00:57:11,359
берем предыдущее содержимое ячейки, а

1289
00:57:11,359 --> 00:57:14,640
затем берем его продукт хадамара

1290
00:57:14,640 --> 00:57:17,520
с вектором забывания,

1291
00:57:17,520 --> 00:57:20,400
а затем добавляем  к нему произведение хадамара

1292
00:57:20,400 --> 00:57:24,559
входных ворот, умноженное на

1293
00:57:24,559 --> 00:57:27,839
обновление ячейки-кандидата,

1294
00:57:30,160 --> 00:57:32,720
а затем для разработки нового скрытого

1295
00:57:32,720 --> 00:57:36,079
состояния мы затем решаем, какие части

1296
00:57:38,079 --> 00:57:42,319
ячейки отображать в скрытом состоянии, и поэтому

1297
00:57:42,319 --> 00:57:45,440
после преобразования tan h

1298
00:57:45,440 --> 00:57:48,160
ячейки мы  затем возьмите продукт хадамара

1299
00:57:48,160 --> 00:57:50,400
с выходным вентилем, и это дает нам

1300
00:57:50,400 --> 00:57:52,799
наше скрытое представление и в качестве этого

1301
00:57:52,799 --> 00:57:54,400
скрытого представления,

1302
00:57:54,400 --> 00:57:55,680
которое мы затем

1303
00:57:55,680 --> 00:57:58,000
пропускаем через слой soft soft max для

1304
00:57:58,000 --> 00:58:01,520
генерации следующего вывода нашей lstm

1305
00:58:01,520 --> 00:58:03,119
или текущей нейронной сети.

1306
00:58:07,040 --> 00:58:10,000
вещи, которые они

1307
00:58:10,000 --> 00:58:13,359
помещают, являются векторами размера n, и что

1308
00:58:13,359 --> 00:58:15,599
мы делаем, мы берем каждый

1309
00:58:15,599 --> 00:58:17,920
из них и умножаем их

1310
00:58:17,920 --> 00:58:19,040
поэлементно,

1311
00:58:19,040 --> 00:58:22,079
чтобы выработать новый вектор, а затем мы получаем

1312
00:58:22,079 --> 00:58:23,440
два вектора, и что мы  сложить

1313
00:58:23,440 --> 00:58:27,040
вместе, так что этот способ делать что

1314
00:58:27,040 --> 00:58:29,440
-то поэлементно, что вы вроде как действительно не

1315
00:58:29,440 --> 00:58:32,880
видите в стандартном курсе линейной алгебры,

1316
00:58:32,880 --> 00:58:35,920
он называется продуктом Хадамара,

1317
00:58:35,920 --> 00:58:38,799
он представлен каким-то кругом,

1318
00:58:38,799 --> 00:58:40,559
я имею в виду, что на самом деле в более современной работе это

1319
00:58:40,559 --> 00:58:42,880
было больше  обычно представлял это с помощью

1320
00:58:42,880 --> 00:58:44,720
этого немного большего круга с

1321
00:58:44,720 --> 00:58:47,280
точкой посередине в качестве символа продукта Хадамар,

1322
00:58:48,799 --> 00:58:50,960
и когда-нибудь я изменю эти слайды, чтобы они

1323
00:58:50,960 --> 00:58:53,440
были такими, но я был ленив и

1324
00:58:53,440 --> 00:58:55,839
переделал уравнения, но другая запись, которую

1325
00:58:55,839 --> 00:58:58,240
вы видите довольно часто, это  просто используя тот

1326
00:58:58,240 --> 00:59:00,240
же маленький кружок, который вы используете для

1327
00:59:00,240 --> 00:59:02,319
композиции функций для представления

1328
00:59:02,319 --> 00:59:04,400
продукта хадама,

1329
00:59:04,400 --> 00:59:07,040
хорошо, поэтому все эти вещи

1330
00:59:07,040 --> 00:59:10,720
выполняются как векторы одинаковой длины n,

1331
00:59:10,720 --> 00:59:12,799
а другая вещь, которую вы можете

1332
00:59:12,799 --> 00:59:16,799
заметить, это то, что обновление кандидата

1333
00:59:16,799 --> 00:59:19,359
и забыть

1334
00:59:19,359 --> 00:59:22,480
импорт и  Все выходные ворота имеют очень

1335
00:59:22,480 --> 00:59:25,040
похожую форму, единственное отличие состоит в том, что

1336
00:59:25,040 --> 00:59:28,720
три логических элемента и один tanh, и ни один из

1337
00:59:28,720 --> 00:59:31,520
них не зависит друг от друга, поэтому все четыре из

1338
00:59:31,520 --> 00:59:34,559
них могут быть рассчитаны параллельно.  И

1339
00:59:34,559 --> 00:59:36,880
если вы хотите иметь эффективную

1340
00:59:36,880 --> 00:59:39,599
реализацию lstm, это то, что вы делаете,

1341
00:59:39,599 --> 00:59:42,480
хорошо, так что вот более графическое

1342
00:59:42,480 --> 00:59:45,280
представление этого, поэтому эти изображения

1343
00:59:45,280 --> 00:59:48,160
взяты от Криса Ола, и я думаю, он проделал

1344
00:59:48,160 --> 00:59:50,720
такую хорошую работу по созданию изображений

1345
00:59:50,720 --> 00:59:54,480
для lstms, что почти все их используют

1346
00:59:54,480 --> 00:59:57,359
дней, и поэтому этот вид

1347
00:59:58,960 --> 01:00:02,559
разбивает график вычислений модуля lstm, так что,

1348
01:00:02,559 --> 01:00:04,960
взорвав его,

1349
01:00:04,960 --> 01:00:08,319
вы получили с предыдущего временного шага

1350
01:00:08,319 --> 01:00:11,520
и свою ячейку, и скрытые

1351
01:00:11,520 --> 01:00:12,839
рекуррентные

1352
01:00:12,839 --> 01:00:17,920
векторы, и поэтому вы скармливаете скрытый

1353
01:00:18,400 --> 01:00:21,040
вектор из предыдущего временного шага

1354
01:00:21,040 --> 01:00:24,640
и новый  введите xt в

1355
01:00:24,640 --> 01:00:26,640
вычисление ворот, которое происходит

1356
01:00:26,640 --> 01:00:29,680
внизу, так что вы вычисляете ворота забывания,

1357
01:00:29,680 --> 01:00:32,240
а затем вы используете ворота

1358
01:00:32,240 --> 01:00:34,720
забывания в продукте Хадамара, здесь нарисованном как

1359
01:00:34,720 --> 01:00:35,920
фактический

1360
01:00:35,920 --> 01:00:39,440
символ времени, чтобы забыть некоторое содержимое ячейки,

1361
01:00:39,440 --> 01:00:42,240
вы разрабатываете входные ворота  а затем,

1362
01:00:42,240 --> 01:00:46,720
используя входной вентиль и обычную

1363
01:00:46,720 --> 01:00:48,160
рекуррентную нейронную сеть, такую как

1364
01:00:48,160 --> 01:00:51,200
вычисления, вы можете вычислить

1365
01:00:51,200 --> 01:00:54,000
содержание новой ячейки кандидата,

1366
01:00:54,000 --> 01:00:58,240
и затем вы складываете эти два вместе,

1367
01:00:58,240 --> 01:01:01,440
чтобы получить новую ячейку  содержимое, которое затем

1368
01:01:01,440 --> 01:01:04,079
выводится как новое содержимое ячейки в

1369
01:01:04,079 --> 01:01:05,440
момент времени t,

1370
01:01:05,440 --> 01:01:07,280
но затем вы

1371
01:01:07,280 --> 01:01:10,799
также разработали выходной вентиль, и

1372
01:01:10,799 --> 01:01:14,079
затем вы берете содержимое ячейки,

1373
01:01:14,079 --> 01:01:18,480
пропускаете его через другой нелинейный и

1374
01:01:18,480 --> 01:01:20,880
мультиадамарный продукт с выходным

1375
01:01:20,880 --> 01:01:23,200
вентилем, и это затем дает  Вы новое

1376
01:01:23,200 --> 01:01:25,680
скрытое состояние,

1377
01:01:25,680 --> 01:01:29,359
так что это все довольно сложно,

1378
01:01:29,359 --> 01:01:32,880
но что касается понимания того, почему

1379
01:01:32,880 --> 01:01:35,359
здесь происходит что-то другое, следует

1380
01:01:35,359 --> 01:01:39,520
отметить, что состояние ячейки от t

1381
01:01:39,520 --> 01:01:41,040
минус 1

1382
01:01:41,040 --> 01:01:44,559
проходит прямо через это, чтобы быть

1383
01:01:44,559 --> 01:01:46,480
состоянием ячейки в момент времени t

1384
01:01:46,480 --> 01:01:50,000
с этим ничего не происходит, поэтому

1385
01:01:50,000 --> 01:01:52,400
некоторые из них

1386
01:01:52,400 --> 01:01:55,200
удаляются шлюзом забывания,

1387
01:01:55,200 --> 01:01:58,799
а затем в него записывается какой-то новый материал

1388
01:01:58,799 --> 01:02:01,839
в результате

1389
01:02:01,839 --> 01:02:04,799
использования этого кандидата нового содержимого ячейки,

1390
01:02:04,799 --> 01:02:06,480
но настоящий секрет

1391
01:02:07,839 --> 01:02:09,599
um

1392
01:02:09,599 --> 01:02:11,039
lstm

1393
01:02:11,039 --> 01:02:14,559
заключается в том, что новый материал  просто добавляется

1394
01:02:14,559 --> 01:02:17,520
в ячейку с правом сложения, поэтому

1395
01:02:17,520 --> 01:02:21,200
в простом rnn на каждом последующем шаге

1396
01:02:21,200 --> 01:02:23,440
вы выполняете умножение,

1397
01:02:23,440 --> 01:02:26,720
и это делает невероятно

1398
01:02:26,720 --> 01:02:29,760
трудным научиться сохранять информацию в

1399
01:02:29,760 --> 01:02:32,960
скрытом состоянии ov  в течение длительного периода времени

1400
01:02:32,960 --> 01:02:35,280
это не совсем невозможно, но

1401
01:02:35,280 --> 01:02:37,760
это очень сложная вещь для изучения,

1402
01:02:37,760 --> 01:02:41,440
тогда как с этой новой архитектурой

1403
01:02:41,440 --> 01:02:43,920
lstm тривиально сохранять информацию в

1404
01:02:43,920 --> 01:02:46,799
ячейке от одного временного шага до следующего, вы

1405
01:02:46,799 --> 01:02:49,280
просто не забываете об этом,

1406
01:02:49,280 --> 01:02:51,520
и она будет  довести до конца,

1407
01:02:51,520 --> 01:02:54,480
возможно, добавив что-то новое, чтобы

1408
01:02:54,480 --> 01:02:57,200
также запомнить, и поэтому в этом смысле

1409
01:02:58,480 --> 01:03:01,760
ячейка ведет себя больше как барана в

1410
01:03:01,760 --> 01:03:03,839
обычном компьютере, который хранит

1411
01:03:03,839 --> 01:03:07,039
материал, и в него можно хранить дополнительные данные,

1412
01:03:07,039 --> 01:03:08,960
а другие материалы можно удалять  из

1413
01:03:08,960 --> 01:03:12,319
этого, когда вы продвигаетесь

1414
01:03:12,720 --> 01:03:15,839
хорошо, поэтому архитектура lstm

1415
01:03:15,839 --> 01:03:19,119
значительно упрощает сохранение информации из

1416
01:03:19,119 --> 01:03:22,240
многих временных шагов,

1417
01:03:22,480 --> 01:03:24,799
поэтому в частности,

1418
01:03:24,799 --> 01:03:28,160
стандартная практика с lstms заключается в

1419
01:03:28,160 --> 01:03:30,720
инициализации шлюза забывания одним

1420
01:03:30,720 --> 01:03:33,119
вектором, который просто так, что

1421
01:03:33,119 --> 01:03:35,440
отправной точкой является  скажем сохранить

1422
01:03:35,440 --> 01:03:39,280
все из предыдущих временных шагов,

1423
01:03:39,280 --> 01:03:40,319
а

1424
01:03:40,319 --> 01:03:42,000
затем он учится, когда

1425
01:03:42,000 --> 01:03:44,640
уместно забыть вещи,

1426
01:03:44,640 --> 01:03:48,400
и, наоборот, очень трудно получить

1427
01:03:48,400 --> 01:03:51,599
или простой rnn для сохранения  в течение

1428
01:03:51,599 --> 01:03:54,400
очень долгого времени я имею в виду, что это на

1429
01:03:54,400 --> 01:03:56,319
самом деле означает

1430
01:03:56,319 --> 01:03:58,559
хорошо, вы знаете, я записал здесь некоторые цифры,

1431
01:03:58,559 --> 01:04:00,240
я имею в виду, что

1432
01:04:00,240 --> 01:04:03,039
вы знаете, как то, что вы получаете на практике,

1433
01:04:03,039 --> 01:04:05,039
вы знаете, зависит от миллиона вещей, это

1434
01:04:05,039 --> 01:04:07,119
зависит от характера ваших данных и

1435
01:04:07,119 --> 01:04:08,960
сколько у вас данных и какой

1436
01:04:08,960 --> 01:04:11,280
размерности ваши

1437
01:04:11,280 --> 01:04:14,240
скрытые состояния являются размытыми кровавыми пятнами, но

1438
01:04:14,240 --> 01:04:17,359
просто чтобы дать вам некоторое представление о том, что

1439
01:04:17,359 --> 01:04:20,720
происходит, обычно, если вы тренируете

1440
01:04:20,720 --> 01:04:23,599
простую рекуррентную нейронную сеть, ее

1441
01:04:23,599 --> 01:04:26,000
эффект памяти ее способность иметь

1442
01:04:26,000 --> 01:04:28,000
возможность использовать вещи в  прошлое, чтобы

1443
01:04:28,000 --> 01:04:30,640
обусловить будущее, занимает около семи временных

1444
01:04:30,640 --> 01:04:32,799
шагов, вы просто не можете заставить его

1445
01:04:32,799 --> 01:04:35,119
вспомнить вещи более

1446
01:04:37,680 --> 01:04:40,559
ранние, чем это, тогда как для lstm

1447
01:04:40,559 --> 01:04:42,880
это не полная магия, это не

1448
01:04:42,880 --> 01:04:45,200
работает вечно, но вы знаете, что это

1449
01:04:45,200 --> 01:04:48,319
эффективно запоминать и использовать

1450
01:04:48,319 --> 01:04:50,799
вещи из гораздо более

1451
01:04:50,799 --> 01:04:54,079
далекого прошлого, поэтому обычно вы обнаруживаете, что с lstm вы

1452
01:04:54,079 --> 01:04:56,720
можете эффективно запоминать и использовать вещи

1453
01:04:56,720 --> 01:04:59,039
примерно в сто раз назад, и

1454
01:04:59,039 --> 01:05:01,920
это просто намного полезнее для

1455
01:05:01,920 --> 01:05:03,200
многие задачи по пониманию естественного языка,

1456
01:05:03,200 --> 01:05:05,680
которые

1457
01:05:05,680 --> 01:05:08,079
мы хотим выполнить,

1458
01:05:08,079 --> 01:05:11,119
и поэтому именно для этого

1459
01:05:11,119 --> 01:05:14,000
был разработан lstm, и я имею в виду, в

1460
01:05:14,000 --> 01:05:16,720
частности, просто возвращаясь к его названию, я

1461
01:05:16,720 --> 01:05:19,680
довольно много людей пропустили его название,

1462
01:05:19,680 --> 01:05:22,400
идею его  Это была

1463
01:05:22,400 --> 01:05:24,480
концепция кратковременной памяти, пришедшая

1464
01:05:24,480 --> 01:05:26,559
из психологии, и было

1465
01:05:26,559 --> 01:05:29,680
предложено для простых МНН, что

1466
01:05:29,680 --> 01:05:31,760
скрытое состояние РНН

1467
01:05:31,760 --> 01:05:33,280
могло быть моделью

1468
01:05:33,280 --> 01:05:36,319
кратковременной памяти человека, и тогда было

1469
01:05:36,319 --> 01:05:38,720
бы что-то еще, что

1470
01:05:38,720 --> 01:05:41,200
могло бы иметь дело.  с человеческой долговременной памятью,

1471
01:05:41,200 --> 01:05:43,680
но люди обнаружили, что это

1472
01:05:43,680 --> 01:05:46,480
дает вам только очень короткую кратковременную память,

1473
01:05:46,480 --> 01:05:49,680
так что гм Хок Райт и Шмидт

1474
01:05:49,680 --> 01:05:52,559
Убер интересовались, как мы могли бы

1475
01:05:54,000 --> 01:05:57,039
создать модели с долговременной краткосрочной

1476
01:05:57,039 --> 01:05:59,359
памятью и т. д.  который затем дал нам это

1477
01:05:59,359 --> 01:06:02,240
имя lstm

1478
01:06:02,240 --> 01:06:05,359
um lstms не гарантирует, что

1479
01:06:05,359 --> 01:06:08,160
нет исчезающих или взрывающихся градиентов, но

1480
01:06:08,160 --> 01:06:10,880
на практике они обеспечивают,

1481
01:06:10,880 --> 01:06:13,839
что они не имеют тенденции снова взорваться

1482
01:06:13,839 --> 01:06:16,400
почти так же, как знак

1483
01:06:16,400 --> 01:06:18,720
плюса cr

1484
01:06:18,720 --> 01:06:21,039
они являются гораздо более эффективным

1485
01:06:21,039 --> 01:06:25,039
способом изучения зависимостей на большом расстоянии.

1486
01:06:40,480 --> 01:06:42,880
они были успешными, так что это было

1487
01:06:42,880 --> 01:06:46,480
примерно с 2013 по 2015 год,

1488
01:06:46,480 --> 01:06:50,000
когда lstms своего рода поразил мир,

1489
01:06:50,000 --> 01:06:52,160
достигнув самых современных результатов по

1490
01:06:52,160 --> 01:06:54,319
всем видам проблем. Одна из

1491
01:06:54,319 --> 01:06:56,160
первых крупных демонстраций была для

1492
01:06:56,160 --> 01:06:58,880
распознавания рукописного ввода, затем распознавания речи,

1493
01:06:58,880 --> 01:07:01,680
но затем  к

1494
01:07:01,680 --> 01:07:04,799
множеству задач естественного языка, включая

1495
01:07:04,799 --> 01:07:07,760
машинный перевод, синтаксический анализ видения

1496
01:07:07,760 --> 01:07:10,079
и языковые задачи, такие как создание подписей к изображениям,

1497
01:07:10,079 --> 01:07:11,760
а также, конечно, их использование для

1498
01:07:11,760 --> 01:07:15,440
языковых моделей, и примерно в эти годы

1499
01:07:15,440 --> 01:07:18,400
lstms стал доминирующим подходом для

1500
01:07:18,400 --> 01:07:20,960
большинства задач

1501
01:07:20,960 --> 01:07:23,920
nlp.  сильная модель заключалась в том, чтобы подойти к

1502
01:07:23,920 --> 01:07:26,480
проблеме с lstm,

1503
01:07:26,480 --> 01:07:30,319
поэтому теперь, в 2021 году, фактически lstms

1504
01:07:30,319 --> 01:07:32,480
начинают вытесняться или были

1505
01:07:32,480 --> 01:07:34,480
вытеснены другими подходами  В

1506
01:07:34,480 --> 01:07:37,599
частности, модели трансформеров, к которым

1507
01:07:37,599 --> 01:07:39,359
мы доберемся в классе через пару

1508
01:07:39,359 --> 01:07:40,720
недель,

1509
01:07:40,720 --> 01:07:42,480
так что это та картина, которую вы можете

1510
01:07:42,480 --> 01:07:45,200
видеть, так что в течение многих лет проводилась

1511
01:07:45,200 --> 01:07:47,599
конференция по машинному переводу и своего

1512
01:07:47,599 --> 01:07:50,240
рода соревнование по запеканию, называемое

1513
01:07:50,240 --> 01:07:52,960
мастерской wmt on  машинный перевод,

1514
01:07:52,960 --> 01:07:55,520
так что если вы посмотрите на историю того, что в

1515
01:07:55,520 --> 01:07:57,650
wmt 2014

1516
01:07:57,650 --> 01:07:58,799
[Музыка]

1517
01:08:00,960 --> 01:08:03,480
на конкурсе не было систем нейронного машинного перевода,

1518
01:08:03,480 --> 01:08:06,640
2014 год был фактически первым годом,

1519
01:08:06,640 --> 01:08:09,200
когда успех

1520
01:08:09,200 --> 01:08:12,799
um lstms для машинного перевода был

1521
01:08:12,799 --> 01:08:15,760
доказан в документе конференции, но

1522
01:08:15,760 --> 01:08:18,640
в этом соревновании ничего не произошло

1523
01:08:18,640 --> 01:08:20,180
к 2016 году

1524
01:08:20,180 --> 01:08:22,080
[Музыка]

1525
01:08:22,080 --> 01:08:25,520
все запрыгали на ls dms работает

1526
01:08:25,520 --> 01:08:28,560
отлично, и многие люди,

1527
01:08:28,560 --> 01:08:30,399
включая победителя конкурса, использовали

1528
01:08:30,399 --> 01:08:33,920
модель lstm ммм

1529
01:08:39,679 --> 01:08:42,000
lstms, и подавляющее большинство

1530
01:08:42,000 --> 01:08:44,319
людей сейчас используют трансформаторы,

1531
01:08:44,319 --> 01:08:46,640
поэтому в вашей сети все быстро меняется,

1532
01:08:46,640 --> 01:08:49,120
и мне постоянно приходится переписывать

1533
01:08:49,120 --> 01:08:51,520
эти лекции,

1534
01:08:51,520 --> 01:08:52,799
эм

1535
01:08:52,799 --> 01:08:55,359
так быстро  Другое примечание об исчезающих и

1536
01:08:55,359 --> 01:08:58,000
взрывающихся градиентах - это проблема только

1537
01:08:58,000 --> 01:09:00,960
с повторяющимися нейронными сетями, это не

1538
01:09:00,960 --> 01:09:04,000
на самом деле проблема, которая также возникает

1539
01:09:04,000 --> 01:09:06,479
везде, где у вас есть большая глубина,

1540
01:09:06,479 --> 01:09:08,719
включая прямую связь и сверточные

1541
01:09:08,719 --> 01:09:10,560
нейронные сети,

1542
01:09:10,560 --> 01:09:11,359
как в

1543
01:09:11,359 --> 01:09:13,759
любое время, когда у вас есть длинные последовательности

1544
01:09:14,479 --> 01:09:16,520
правил цепочки, которые дают вам

1545
01:09:16,520 --> 01:09:19,359
умножения, градиент может стать

1546
01:09:19,359 --> 01:09:22,080
исчезающе маленьким по мере того, как он распространяется обратно

1547
01:09:22,080 --> 01:09:24,238
um, и поэтому

1548
01:09:24,238 --> 01:09:26,319
обычно нижние уровни

1549
01:09:26,319 --> 01:09:28,479
изучаются очень медленно и их трудно

1550
01:09:28,479 --> 01:09:31,439
обучать, поэтому было много усилий в

1551
01:09:31,439 --> 01:09:33,359
других местах,

1552
01:09:33,359 --> 01:09:36,799
чтобы придумать  различные архитектуры,

1553
01:09:36,799 --> 01:09:38,238
которые позволяют вам

1554
01:09:38,238 --> 01:09:41,120
более эффективно учиться в более глубоких

1555
01:09:41,120 --> 01:09:43,759
сетях, и самый распространенный способ сделать

1556
01:09:43,759 --> 01:09:44,640
это

1557
01:09:44,640 --> 01:09:47,279
- добавить больше прямых соединений, которые

1558
01:09:47,279 --> 01:09:50,799
позволяют течь градиенту, так что большой

1559
01:09:50,799 --> 01:09:52,799
вещью и видением в последние несколько

1560
01:09:52,799 --> 01:09:55,040
лет были пересети, где res

1561
01:09:55,040 --> 01:09:58,080
означает  остаточные соединения и то, как

1562
01:09:58,080 --> 01:09:59,760
они сделаны, эта

1563
01:09:59,760 --> 01:10:01,920
картинка перевернута, так что ввод находится

1564
01:10:01,920 --> 01:10:03,760
наверху, гм,

1565
01:10:03,760 --> 01:10:05,520
у вас

1566
01:10:05,520 --> 01:10:07,840
есть эти виды  f два пути, которые

1567
01:10:07,840 --> 01:10:10,239
суммируются, один путь - это просто

1568
01:10:10,239 --> 01:10:12,640
путь идентичности, а другой проходит

1569
01:10:12,640 --> 01:10:14,880
через некоторые уровни нейронной сети,

1570
01:10:14,880 --> 01:10:17,600
поэтому его поведение по умолчанию -

1571
01:10:17,600 --> 01:10:20,880
просто сохранить входной um,

1572
01:10:20,880 --> 01:10:22,960
что может немного походить на то, что

1573
01:10:22,960 --> 01:10:26,080
мы только что видели для lstms  гм, есть и другие

1574
01:10:26,080 --> 01:10:28,239
методы, которые тогда были плотными сетями, где

1575
01:10:28,239 --> 01:10:30,719
вы добавляете пропускающие соединения вперед к

1576
01:10:30,719 --> 01:10:33,199
каждому лазерному слою

1577
01:10:33,199 --> 01:10:34,719
сети шоссе,

1578
01:10:34,719 --> 01:10:36,239
также были фактически разработаны

1579
01:10:36,239 --> 01:10:37,760
schmidhuber

1580
01:10:37,760 --> 01:10:39,760
и вроде как напоминают то, что было

1581
01:10:39,760 --> 01:10:42,480
сделано с lstms, так что вместо того, чтобы просто

1582
01:10:42,480 --> 01:10:45,440
иметь соединение с идентификацией в качестве

1583
01:10:45,440 --> 01:10:49,199
resnet  он вводит дополнительный вентиль,

1584
01:10:49,199 --> 01:10:52,320
поэтому он больше похож на lstm, который говорит,

1585
01:10:52,320 --> 01:10:54,560
сколько нужно отправить вход через

1586
01:10:54,560 --> 01:10:56,320
магистраль по

1587
01:10:56,320 --> 01:10:59,280
сравнению с тем, сколько мкм, чтобы пропустить его через

1588
01:10:59,280 --> 01:11:01,760
слой нейронной сети, и эти два затем

1589
01:11:01,760 --> 01:11:05,040
объединяются в выход,

1590
01:11:06,159 --> 01:11:07,120
так что, по

1591
01:11:07,120 --> 01:11:08,800
сути,

1592
01:11:08,800 --> 01:11:11,440
эта проблема  происходит где угодно, когда у вас

1593
01:11:11,440 --> 01:11:13,520
есть большая глубина в ваших

1594
01:11:13,520 --> 01:11:15,679
слоях нейронной сети,

1595
01:11:15,679 --> 01:11:16,640
но

1596
01:11:16,640 --> 01:11:19,040
она впервые возникла и оказывается

1597
01:11:19,040 --> 01:11:21,520
особенно проблематичной

1598
01:11:21,520 --> 01:11:23,760
с повторяющимися нейронными сетями.  сетей, они

1599
01:11:23,760 --> 01:11:26,239
особенно нестабильны из-за

1600
01:11:26,239 --> 01:11:28,800
того, что у вас есть одна

1601
01:11:28,800 --> 01:11:30,480
матрица весов, которую вы

1602
01:11:30,480 --> 01:11:32,960
постоянно используете во временной

1603
01:11:32,960 --> 01:11:35,520
последовательности.

1604
01:11:44,080 --> 01:11:45,440
или менее о том, захотите ли вы

1605
01:11:45,440 --> 01:11:47,840
когда-нибудь использовать rn, например, простой rnn,

1606
01:11:47,840 --> 01:11:49,679
вместо lstm,

1607
01:11:49,679 --> 01:11:52,159
как lstm узнает, что делать со

1608
01:11:52,159 --> 01:11:53,520
своими воротами,

1609
01:11:53,520 --> 01:11:55,840
вы можете высказать свое мнение об этих вещах,

1610
01:11:55,840 --> 01:11:56,880
конечно,

1611
01:11:57,679 --> 01:12:01,360
так что я думаю, в основном ответ: ммм,

1612
01:12:01,360 --> 01:12:03,679
вы никогда не должны  используйте простой rnn, в наши

1613
01:12:03,679 --> 01:12:06,960
дни вы всегда должны использовать lstm, я

1614
01:12:06,960 --> 01:12:09,280
имею в виду, что вы, очевидно, знаете, что это зависит от

1615
01:12:09,280 --> 01:12:10,640
того, что вы делаете, если вы хотите

1616
01:12:10,640 --> 01:12:13,040
сделать какую-то аналитическую статью или

1617
01:12:13,040 --> 01:12:17,040
что-то, что вы можете предпочесть простому rnn,

1618
01:12:17,040 --> 01:12:20,719
и это так  что вы

1619
01:12:20,719 --> 01:12:22,960
действительно можете получить достойные результаты с простыми

1620
01:12:22,960 --> 01:12:25,600
РНН, при условии, что вы очень внимательно

1621
01:12:25,600 --> 01:12:28,880
следите за тем, чтобы вещи не взрывались и

1622
01:12:28,880 --> 01:12:32,000
не исчезали,

1623
01:12:32,880 --> 01:12:36,320
но на практике вы знаете, что заставить простые

1624
01:12:36,320 --> 01:12:39,440
РНН работать и сохранять длинный

1625
01:12:39,440 --> 01:12:41,760
контекст невероятно сложно там, где вы можете

1626
01:12:41,760 --> 01:12:44,800
тренировать lst  мс, и они будут просто работать,

1627
01:12:44,800 --> 01:12:46,880
так что на самом деле вы всегда должны просто использовать

1628
01:12:46,880 --> 01:12:49,520
lstm,

1629
01:12:49,520 --> 01:12:52,480
теперь подождите, второй вопрос был:

1630
01:12:52,480 --> 01:12:54,400
эм, я думаю, есть небольшая путаница,

1631
01:12:54,400 --> 01:12:55,840
например,

1632
01:12:55,840 --> 01:12:57,600
узнаются ли ворота по-

1633
01:12:57,600 --> 01:13:01,040
другому.

1634
01:13:01,040 --> 01:13:04,800
мы

1635
01:13:04,800 --> 01:13:07,120
возвращаемся к этим уравнениям,

1636
01:13:08,880 --> 01:13:11,679
вы знаете, что это полная модель, и

1637
01:13:11,679 --> 01:13:14,080
когда мы обучаем модель,

1638
01:13:14,080 --> 01:13:17,280
каждый из этих параметров, поэтому все

1639
01:13:17,280 --> 01:13:20,560
эти wu и b,

1640
01:13:20,560 --> 01:13:23,199
все одновременно

1641
01:13:23,199 --> 01:13:25,600
обучаются с помощью обратного распространения,

1642
01:13:25,600 --> 01:13:26,400
так

1643
01:13:26,400 --> 01:13:27,760
что то, на

1644
01:13:27,760 --> 01:13:30,800
что вы надеетесь, и действительно работает

1645
01:13:30,800 --> 01:13:33,840
модель изучает, какие вещи я должен

1646
01:13:33,840 --> 01:13:36,400
запоминать надолго, а какие -

1647
01:13:36,400 --> 01:13:38,159
забыть,

1648
01:13:38,159 --> 01:13:40,320
какие вещи во входных данных важны,

1649
01:13:40,320 --> 01:13:42,560
а какие во входных данных на самом деле не имеют

1650
01:13:42,560 --> 01:13:44,960
значения, поэтому она может изучать такие вещи,

1651
01:13:44,960 --> 01:13:48,080
как наши функциональные слова, такие как

1652
01:13:48,080 --> 01:13:49,920
don '  это действительно имеет значение, даже если все

1653
01:13:49,920 --> 01:13:51,440
используют их на английском языке,

1654
01:13:51,440 --> 01:13:54,239
поэтому вы можете просто не беспокоиться о них,

1655
01:13:54,239 --> 01:13:56,960
поэтому все это изучено, и модели

1656
01:13:56,960 --> 01:13:59,120
действительно успешно узнают

1657
01:13:59,120 --> 01:14:01,840
значения ворот о том, какая информация  полезно

1658
01:14:01,840 --> 01:14:04,400
для сохранения в долгосрочной перспективе по сравнению с тем, какая

1659
01:14:04,400 --> 01:14:05,840
информация

1660
01:14:05,840 --> 01:14:08,159
на самом деле полезна только в краткосрочной перспективе для

1661
01:14:08,159 --> 01:14:11,679
предсказания следующих одного или двух слов,

1662
01:14:11,679 --> 01:14:12,840
наконец,

1663
01:14:12,840 --> 01:14:16,480
э-э, улучшения градиента из-

1664
01:14:16,480 --> 01:14:18,480
за того, что вы сказали, что добавление

1665
01:14:18,480 --> 01:14:20,080
действительно важно между новым

1666
01:14:20,080 --> 01:14:21,920
кандидатом в ячейку и состоянием ячейки  Я не

1667
01:14:21,920 --> 01:14:23,520
думаю, что по крайней мере пара студентов

1668
01:14:23,520 --> 01:14:24,560
задалась этим

1669
01:14:24,560 --> 01:14:26,159
вопросом, поэтому, если вы хотите

1670
01:14:26,159 --> 01:14:28,880
повторить это еще раз, это может быть полезно,

1671
01:14:28,880 --> 01:14:30,640
конечно,

1672
01:14:30,640 --> 01:14:32,640
ммм,

1673
01:14:32,640 --> 01:14:37,120
так что нам бы хотелось, чтобы это был простой способ

1674
01:14:37,120 --> 01:14:40,880
сохранить память на долгое время

1675
01:14:40,880 --> 01:14:43,199
ммм, а

1676
01:14:43,199 --> 01:14:44,239
вы  Знайте, что

1677
01:14:44,239 --> 01:14:46,719
один способ, который используют ресети, - это

1678
01:14:46,719 --> 01:14:49,280
просто как бы полностью иметь прямой

1679
01:14:49,280 --> 01:14:52,960
путь от ct минус один до ct и

1680
01:14:54,239 --> 01:14:56,719
полностью сохранять историю, так что это своего

1681
01:14:56,719 --> 01:15:00,320
рода действие по умолчанию для сохранения

1682
01:15:00,320 --> 01:15:03,840
информации um о прошлом долгосрочном

1683
01:15:03,840 --> 01:15:08,000
lstms не  Это вполне возможно, но они

1684
01:15:08,000 --> 01:15:10,960
позволяют упростить эту функцию, поэтому

1685
01:15:10,960 --> 01:15:12,960
вы начинаете с предыдущего

1686
01:15:12,960 --> 01:15:16,080
состояния ячейки и можете забыть некоторые из них с

1687
01:15:16,080 --> 01:15:18,159
помощью ворот забывания, чтобы вы могли удалить

1688
01:15:18,159 --> 01:15:19,520
из своей памяти то, что используется  Полная

1689
01:15:19,520 --> 01:15:21,280
операция,

1690
01:15:21,280 --> 01:15:22,640
а затем, пока вы

1691
01:15:22,640 --> 01:15:25,440
сможете обновить содержимое ячейки с помощью

1692
01:15:25,440 --> 01:15:26,800
этой

1693
01:15:27,600 --> 01:15:30,400
правильной операции, которая происходит в плюсе,

1694
01:15:30,400 --> 01:15:33,199
где в зависимости от входного шлюза

1695
01:15:33,199 --> 01:15:35,360
будут добавлены некоторые части того, что находится в ячейке,

1696
01:15:36,560 --> 01:15:38,719
но вы можете  подумайте об этом добавлении как

1697
01:15:38,719 --> 01:15:41,920
наложении дополнительной информации, все,

1698
01:15:41,920 --> 01:15:43,600
что было в ячейке, что не было

1699
01:15:43,600 --> 01:15:46,719
забыто, по-прежнему продолжается до

1700
01:15:46,719 --> 01:15:49,840
следующего временного шага

1701
01:15:50,000 --> 01:15:52,239
и, в частности, когда вы

1702
01:15:52,239 --> 01:15:54,880
выполняете обратное распространение во времени,

1703
01:15:54,880 --> 01:15:57,360
которого нет,

1704
01:15:58,239 --> 01:15:59,280
я хочу  чтобы сказать, что нет

1705
01:15:59,280 --> 01:16:02,159
умножения между ct и ct минус

1706
01:16:02,159 --> 01:16:05,840
один, и здесь есть этот неудачный символ времени um,

1707
01:16:05,840 --> 01:16:07,360
но помните, что это

1708
01:16:07,360 --> 01:16:09,920
произведение Хадамара, которое обнуляет его

1709
01:16:09,920 --> 01:16:12,159
часть с помощью элемента забывания, это

1710
01:16:12,159 --> 01:16:14,800
не умножение на матрицу,

1711
01:16:14,800 --> 01:16:18,600
как в простом  пнн,

1712
01:16:22,400 --> 01:16:24,880
я надеюсь, что это хорошо, ну

1713
01:16:24,880 --> 01:16:27,040
ладно, есть еще пара

1714
01:16:27,040 --> 01:16:29,440
вещей, которые я хотел бы сделать

1715
01:16:29,440 --> 01:16:31,360
до конца, я думаю, у меня не

1716
01:16:31,360 --> 01:16:33,199
будет времени сделать и то, и другое, я думаю, что

1717
01:16:33,199 --> 01:16:35,440
сделаю последнее  вероятно  В следующий раз,

1718
01:16:35,440 --> 01:16:37,920
так что это на самом деле простые и легкие

1719
01:16:37,920 --> 01:16:39,760
вещи,

1720
01:16:39,760 --> 01:16:43,199
но они дополняют нашу картину,

1721
01:16:43,199 --> 01:16:45,760
так что я как бы кратко сослался на этот

1722
01:16:45,760 --> 01:16:48,080
пример классификации настроений,

1723
01:16:48,080 --> 01:16:51,600
где то, что мы могли бы сделать, это запустить rnn,

1724
01:16:51,600 --> 01:16:55,120
может быть, lstm над предложением

1725
01:16:55,120 --> 01:16:58,400
назовем это нашим представлением  предложения

1726
01:16:59,920 --> 01:17:01,040
и

1727
01:17:01,040 --> 01:17:04,400
передать его в мягкий классификатор максимума, чтобы

1728
01:17:04,400 --> 01:17:06,480
классифицировать его по

1729
01:17:06,480 --> 01:17:09,199
настроениям, так что на самом деле мы

1730
01:17:09,199 --> 01:17:11,840
говорим, что мы можем рассматривать скрытое

1731
01:17:11,840 --> 01:17:16,000
состояние как представление слова в

1732
01:17:16,000 --> 01:17:17,600
контексте,

1733
01:17:17,600 --> 01:17:18,400
который

1734
01:17:18,400 --> 01:17:20,960
ниже у нас есть просто вектор слов

1735
01:17:20,960 --> 01:17:23,920
для  ужасно, но затем мы

1736
01:17:23,920 --> 01:17:27,199
посмотрели на наш контекст и сказали, что теперь мы

1737
01:17:28,000 --> 01:17:31,199
создали представление скрытого состояния

1738
01:17:31,199 --> 01:17:34,159
для слова, ужасно в

1739
01:17:34,159 --> 01:17:37,360
контексте фильма, и это оказалось

1740
01:17:37,360 --> 01:17:40,080
действительно полезной идеей, потому что слова имеют

1741
01:17:40,080 --> 01:17:43,520
разные значения в разных контекстах,

1742
01:17:43,520 --> 01:17:45,360
но это  похоже, что в

1743
01:17:45,360 --> 01:17:47,920
том, что мы здесь сделали, есть дефект, потому что

1744
01:17:47,920 --> 01:17:51,199
наш контекст содержит информацию

1745
01:17:51,199 --> 01:17:53,920
только слева, а что насчет правого контекста,

1746
01:17:53,920 --> 01:17:56,560
конечно, также было бы полезно иметь

1747
01:17:56,560 --> 01:17:58,400
значение t  ошибочно

1748
01:17:58,400 --> 01:18:02,000
зависят от захватывающего, потому что часто слова

1749
01:18:02,000 --> 01:18:04,000
означают разные вещи в зависимости от того, что

1750
01:18:04,000 --> 01:18:05,760
следует за ними,

1751
01:18:05,760 --> 01:18:08,080
поэтому вы знаете, что если у вас есть что-то вроде

1752
01:18:08,080 --> 01:18:10,480
красного вина, это означает что-то совсем

1753
01:18:10,480 --> 01:18:14,159
иное, чем красный свет,

1754
01:18:14,159 --> 01:18:16,880
так как мы можем справиться с этим колодцем?

1755
01:18:19,360 --> 01:18:22,320
хорошо сказать, если мы просто хотим

1756
01:18:22,320 --> 01:18:24,880
придумать нейронное кодирование предложения, мы могли

1757
01:18:24,880 --> 01:18:27,679
бы получить второй rnn с полностью

1758
01:18:27,679 --> 01:18:30,560
отдельными изученными параметрами, и мы могли бы

1759
01:18:30,560 --> 01:18:33,440
запустить его в обратном направлении по предложению,

1760
01:18:33,440 --> 01:18:35,840
чтобы получить обратное представление каждого

1761
01:18:35,840 --> 01:18:38,480
слова, а затем мы можем  получить общее

1762
01:18:38,480 --> 01:18:41,280
представление каждого слова в контексте,

1763
01:18:41,280 --> 01:18:43,280
просто объединив эти два

1764
01:18:43,280 --> 01:18:44,880
представления,

1765
01:18:44,880 --> 01:18:46,800
и теперь у нас есть представление

1766
01:18:46,800 --> 01:18:50,400
ужасно, которое имеет как левый, так и правый

1767
01:18:50,400 --> 01:18:52,800
контекст,

1768
01:18:52,800 --> 01:18:56,640
поэтому мы просто запускаем вперед rnn,

1769
01:18:56,640 --> 01:18:58,800
и когда я говорю здесь rnn,

1770
01:18:58,800 --> 01:19:00,560
это просто означает  любой тип рекуррентной

1771
01:19:00,560 --> 01:19:02,560
нейронной сети, так что обычно это будет

1772
01:19:02,560 --> 01:19:04,640
lstm

1773
01:19:04,640 --> 01:19:06,320
и обратная,

1774
01:19:06,320 --> 01:19:08,040
а затем на каждом временном шаге мы просто

1775
01:19:08,040 --> 01:19:11,360
объединяем их представления

1776
01:19:11,360 --> 01:19:14,320
um с каждым из  они имеют разные

1777
01:19:14,320 --> 01:19:16,640
веса, и поэтому мы рассматриваем эту

1778
01:19:16,640 --> 01:19:19,520
объединенную вещь как скрытое состояние,

1779
01:19:19,520 --> 01:19:22,239
контекстное представление токена

1780
01:19:22,239 --> 01:19:24,640
в конкретное время, которое мы передаем

1781
01:19:24,640 --> 01:19:26,080
вперед,

1782
01:19:26,080 --> 01:19:29,520
это настолько распространено, что люди используют

1783
01:19:29,520 --> 01:19:30,960
ярлык,

1784
01:19:30,960 --> 01:19:32,880
чтобы обозначить это, и они просто нарисуют

1785
01:19:32,880 --> 01:19:36,000
эту картинку  с двусторонними стрелками, и

1786
01:19:36,000 --> 01:19:38,400
когда вы видите это изображение с двусторонними

1787
01:19:38,400 --> 01:19:42,400
стрелками, это означает, что вы запускаете два

1788
01:19:42,400 --> 01:19:44,239
um

1789
01:19:44,239 --> 01:19:47,120
rnn по одному в каждом направлении, а затем

1790
01:19:47,120 --> 01:19:49,440
объединяете их результаты на каждом временном

1791
01:19:49,440 --> 01:19:51,679
шаге, и это то, что вы собираетесь использовать

1792
01:19:51,679 --> 01:19:54,480
позже в  модель в

1793
01:19:54,480 --> 01:19:56,800
порядке, но

1794
01:19:56,800 --> 01:19:58,640
если

1795
01:19:58,640 --> 01:20:01,040
вы решаете проблему с кодированием, например, для

1796
01:20:01,040 --> 01:20:03,199
классификации настроений или ответов на вопросы

1797
01:20:06,080 --> 01:20:08,400
с использованием двунаправленных rnns, это отличная

1798
01:20:08,400 --> 01:20:10,080
вещь,

1799
01:20:10,080 --> 01:20:12,320
но они применимы только в том случае, если у вас есть

1800
01:20:12,320 --> 01:20:15,679
доступ ко всей входной последовательности,

1801
01:20:15,679 --> 01:20:17,600
они  неприменимо

1802
01:20:17,600 --> 01:20:20,239
к языковому моделированию, потому что в

1803
01:20:20,239 --> 01:20:22,320
языковой модели вам обязательно нужно

1804
01:20:22,320 --> 01:20:24,960
сгенерировать следующее слово на основе только

1805
01:20:24,960 --> 01:20:27,600
предыдущего контекста,

1806
01:20:27,600 --> 01:20:29,440
но если у вас есть вся входная

1807
01:20:29,440 --> 01:20:31,679
последовательность, которая двунаправленная  Иональность дает

1808
01:20:31,679 --> 01:20:35,280
вам большую мощность, и это действительно

1809
01:20:35,280 --> 01:20:37,840
была идея, которую люди развили в

1810
01:20:37,840 --> 01:20:41,280
последующей работе, поэтому, когда мы дойдем до

1811
01:20:41,280 --> 01:20:43,600
трансформаторов через пару недель,

1812
01:20:43,600 --> 01:20:46,000
мы потратим много времени на модель Берта,

1813
01:20:47,040 --> 01:20:49,120
где этот акроним означает

1814
01:20:49,120 --> 01:20:51,679
двух-  представления направленного кодировщика

1815
01:20:51,679 --> 01:20:55,440
от трансформаторов, поэтому часть того, что

1816
01:20:55,440 --> 01:20:57,440
важно в этой модели, - это

1817
01:20:57,440 --> 01:21:00,480
трансформатор, но на самом деле центральным

1818
01:21:00,480 --> 01:21:03,280
моментом статьи было сказать, что вы можете

1819
01:21:03,280 --> 01:21:05,520
построить более мощные модели, используя

1820
01:21:05,520 --> 01:21:08,239
трансформаторы, снова

1821
01:21:08,239 --> 01:21:10,040
используя

1822
01:21:10,040 --> 01:21:11,760
двунаправленность,

1823
01:21:12,880 --> 01:21:15,600
хорошо, на rnn остался один крошечный бит  но

1824
01:21:15,600 --> 01:21:18,159
я пробираюсь в следующий класс, и я

1825
01:21:18,159 --> 01:21:20,800
назову это концом на сегодня, и если

1826
01:21:20,800 --> 01:21:22,719
есть другие вещи, о которых вы хотели бы задать

1827
01:21:22,719 --> 01:21:24,159
вопросы,

1828
01:21:24,159 --> 01:21:27,600
вы можете снова найти меня в укромных уголках и

1829
01:21:27,600 --> 01:21:29,120
всего через минуту,

1830
01:21:29,120 --> 01:21:34,280
хорошо, так что увидимся  снова в следующий вторник

