1
00:00:00,000 --> 00:00:05,140


2
00:00:05,140 --> 00:00:05,950
Hi, everyone.

3
00:00:05,950 --> 00:00:07,150
I'll get started.

4
00:00:07,150 --> 00:00:13,900
OK, so we're now back to
the second week of CS224N

5
00:00:13,900 --> 00:00:18,020
on Natural Language
Processing with Deep Learning.

6
00:00:18,020 --> 00:00:20,950
OK, so for today's lecture what
we're going to be looking at

7
00:00:20,950 --> 00:00:26,560
is all the math details of
doing neural net learning.

8
00:00:26,560 --> 00:00:29,500
First of all, looking
at how we can work out

9
00:00:29,500 --> 00:00:33,610
by hand gradients for
training neural networks,

10
00:00:33,610 --> 00:00:35,320
and then looking
at how it's done

11
00:00:35,320 --> 00:00:38,200
more algorithmically, which is
known as the back propagation

12
00:00:38,200 --> 00:00:40,260
algorithm.

13
00:00:40,260 --> 00:00:43,740
And correspondingly,
by you guys,

14
00:00:43,740 --> 00:00:46,980
well, I hope you remember
that one minute ago

15
00:00:46,980 --> 00:00:50,380
was when assignment one was due
and everyone has handed that

16
00:00:50,380 --> 00:00:50,880
in.

17
00:00:50,880 --> 00:00:54,450
If by some chance you
haven't handed it in,

18
00:00:54,450 --> 00:00:56,770
really should hand it
in as soon as possible.

19
00:00:56,770 --> 00:00:59,850
Best to preserve those late
days for the harder assignments.

20
00:00:59,850 --> 00:01:02,520
So I mean, I actually forgot
to mention we actually

21
00:01:02,520 --> 00:01:05,610
did make one change
for this year

22
00:01:05,610 --> 00:01:08,430
to make it a bit easier
when occasionally people

23
00:01:08,430 --> 00:01:10,230
joined the class a week late.

24
00:01:10,230 --> 00:01:13,260
If you want to this
year in the grading,

25
00:01:13,260 --> 00:01:16,320
assignment one can be
discounted and we'll just

26
00:01:16,320 --> 00:01:18,310
use your other four assignments.

27
00:01:18,310 --> 00:01:22,560
But if you've been in the class
so far, for that 98% of people,

28
00:01:22,560 --> 00:01:25,800
well, since assignment one
is the easiest assignment,

29
00:01:25,800 --> 00:01:28,050
again, it's silly not
to do it and have it

30
00:01:28,050 --> 00:01:30,030
as part of your grade.

31
00:01:30,030 --> 00:01:34,920
OK, so starting today
we've put out assignment 2.

32
00:01:34,920 --> 00:01:38,160
And assignment 2 is all
about making sure you really

33
00:01:38,160 --> 00:01:40,860
understand the math
of neural networks,

34
00:01:40,860 --> 00:01:45,450
and then the software that
we use to do that math.

35
00:01:45,450 --> 00:01:49,090
So this is going to be a bit
of a tough week for some.

36
00:01:49,090 --> 00:01:51,510
So for some people
who are great on all

37
00:01:51,510 --> 00:01:54,690
their math and
backgrounds, now feel

38
00:01:54,690 --> 00:01:58,500
like this is stuff they know
well, nothing very difficult.

39
00:01:58,500 --> 00:02:02,130
But I know there are
quite a few of you who

40
00:02:02,130 --> 00:02:06,330
this lecture and week is the
biggest struggle of the course.

41
00:02:06,330 --> 00:02:08,910
We really do want
people to actually

42
00:02:08,910 --> 00:02:11,730
have an understanding of what
goes on in neural network

43
00:02:11,730 --> 00:02:16,210
learning rather than viewing
it as some kind of deep magic.

44
00:02:16,210 --> 00:02:19,140
And I hope that some of
the material we give today

45
00:02:19,140 --> 00:02:23,010
and that you read up on and use
in the assignment will really

46
00:02:23,010 --> 00:02:26,520
give you more of a sense of
what these neural networks are

47
00:02:26,520 --> 00:02:29,700
doing, and how it
is just math that's

48
00:02:29,700 --> 00:02:33,240
applied in a systematic
large scale that works out

49
00:02:33,240 --> 00:02:35,190
the answers, and
that this will be

50
00:02:35,190 --> 00:02:39,070
valuable in giving you a deeper
sense of what's going on.

51
00:02:39,070 --> 00:02:44,610
But if this material seems
very scary and difficult,

52
00:02:44,610 --> 00:02:46,530
you can take some
refuge in the fact

53
00:02:46,530 --> 00:02:49,590
that there's first light
at the end of the tunnel

54
00:02:49,590 --> 00:02:52,260
since this is really
the only lecture that's

55
00:02:52,260 --> 00:02:56,010
heavily going through the math
details of neural networks.

56
00:02:56,010 --> 00:02:59,220
After that, we'll be kind of
popping back up to a higher

57
00:02:59,220 --> 00:03:03,420
level and by and
large, after this week

58
00:03:03,420 --> 00:03:05,910
we'll be making use of
software to do a lot

59
00:03:05,910 --> 00:03:09,690
of the complicated math for us.

60
00:03:09,690 --> 00:03:12,120
But nevertheless, I
hope this is valuable.

61
00:03:12,120 --> 00:03:14,610
I'll go through
everything quickly today,

62
00:03:14,610 --> 00:03:17,820
but if this isn't stuff
that you know backwards,

63
00:03:17,820 --> 00:03:22,230
I really do encourage
you to work through it

64
00:03:22,230 --> 00:03:23,860
and get help as you need it.

65
00:03:23,860 --> 00:03:26,940
So do come along to
our office hours.

66
00:03:26,940 --> 00:03:30,180
There are also a number of
pieces of tutorial material

67
00:03:30,180 --> 00:03:31,710
given in the syllabus.

68
00:03:31,710 --> 00:03:33,840
So there's both
the lecture notes,

69
00:03:33,840 --> 00:03:37,110
there's some
materials from CS231.

70
00:03:37,110 --> 00:03:39,960
In the list of readings,
the very top reading

71
00:03:39,960 --> 00:03:44,730
is some material put together
by Kevin Clark a couple of years

72
00:03:44,730 --> 00:03:47,820
ago, and actually that
one's my favorite.

73
00:03:47,820 --> 00:03:51,150
The presentation there
fairly closely follows

74
00:03:51,150 --> 00:03:53,910
the presentation in
this lecture of going

75
00:03:53,910 --> 00:03:55,290
through matrix calculus.

76
00:03:55,290 --> 00:03:58,320
So personally, I'd recommend
starting with that one

77
00:03:58,320 --> 00:03:59,820
but there are four
different ones

78
00:03:59,820 --> 00:04:05,220
you can choose from if one of
them seems more helpful to you.

79
00:04:05,220 --> 00:04:07,830
Two other things on
what's coming up.

80
00:04:07,830 --> 00:04:12,210
Actually for our Thursday's
lecture, we make a big change.

81
00:04:12,210 --> 00:04:13,920
And Thursday's
lecture is probably

82
00:04:13,920 --> 00:04:16,470
the most linguistic
lecture of the whole class

83
00:04:16,470 --> 00:04:18,630
where we go through the
details of dependency

84
00:04:18,630 --> 00:04:20,579
grammar and dependency parsing.

85
00:04:20,579 --> 00:04:23,040
Some people find that
tough as well, but at least

86
00:04:23,040 --> 00:04:25,200
it will be tough
in a different way.

87
00:04:25,200 --> 00:04:29,520
And then one other really good
opportunity is this Friday,

88
00:04:29,520 --> 00:04:32,340
we have our second
tutorial at 10:00 AM, which

89
00:04:32,340 --> 00:04:35,160
is an introduction to PyTorch,
which is the deep learning

90
00:04:35,160 --> 00:04:39,150
framework that we'll be using
for the rest of the class

91
00:04:39,150 --> 00:04:41,520
once we've gone through
these first two assignments

92
00:04:41,520 --> 00:04:44,292
where you do things by yourself.

93
00:04:44,292 --> 00:04:46,500
So this is a great chance
to get an intro to PyTorch,

94
00:04:46,500 --> 00:04:50,890
it will be really useful
for later in the class.

95
00:04:50,890 --> 00:04:55,270
OK, today's material
is really all

96
00:04:55,270 --> 00:04:58,690
about sort of the math
of neural networks,

97
00:04:58,690 --> 00:05:03,100
but just to introduce a setting
where we can work through this,

98
00:05:03,100 --> 00:05:06,040
I'm going to
introduce a simple NLP

99
00:05:06,040 --> 00:05:08,770
task and a simple
form of classifier

100
00:05:08,770 --> 00:05:10,210
that we can use for it.

101
00:05:10,210 --> 00:05:12,640
So the task of named
entity recognition

102
00:05:12,640 --> 00:05:15,760
is a very common basic NLP task.

103
00:05:15,760 --> 00:05:19,960
And the goal of this is you're
looking through pieces of text

104
00:05:19,960 --> 00:05:23,440
and you're wanting to label
by labeling the words which

105
00:05:23,440 --> 00:05:26,650
words belong to entity
categories like persons,

106
00:05:26,650 --> 00:05:31,030
locations, products,
dates, times, et cetera.

107
00:05:31,030 --> 00:05:33,370
So for this piece
of text, last night

108
00:05:33,370 --> 00:05:36,460
Paris Hilton wowed
in a sequin gown.

109
00:05:36,460 --> 00:05:39,580
Samuel Quinn was arrested
in the Hilton Hotel in Paris

110
00:05:39,580 --> 00:05:41,620
in April 1989.

111
00:05:41,620 --> 00:05:44,080
The-- some words
are being labeled

112
00:05:44,080 --> 00:05:46,610
as named entities as shown.

113
00:05:46,610 --> 00:05:48,190
These two sentences
don't actually

114
00:05:48,190 --> 00:05:50,260
belong together in
the same article

115
00:05:50,260 --> 00:05:55,150
but I chose those two sentences
to illustrate the basic point

116
00:05:55,150 --> 00:05:58,330
that it's not that you can
just do this task by using

117
00:05:58,330 --> 00:05:59,260
a dictionary.

118
00:05:59,260 --> 00:06:03,220
Yes, a dictionary is helpful
to know that Paris can possibly

119
00:06:03,220 --> 00:06:07,300
be a location, but Paris
can also be a person name.

120
00:06:07,300 --> 00:06:10,060
So you have to
use context to get

121
00:06:10,060 --> 00:06:11,650
named entity recognition right.

122
00:06:11,650 --> 00:06:14,330


123
00:06:14,330 --> 00:06:18,950
OK, well, how might we do
that with a neural network?

124
00:06:18,950 --> 00:06:22,100
There are much more
advanced ways of doing this.

125
00:06:22,100 --> 00:06:25,130
But a simple yet
already pretty good

126
00:06:25,130 --> 00:06:29,270
way of doing named
entity recognition

127
00:06:29,270 --> 00:06:33,300
with a simple neural
net is to say, well,

128
00:06:33,300 --> 00:06:36,650
what we're going to do
is use the word vectors

129
00:06:36,650 --> 00:06:38,540
that we've learned
about and we're

130
00:06:38,540 --> 00:06:43,670
going to build up a context
window of word vectors.

131
00:06:43,670 --> 00:06:47,750
And then we're going to put
those through a neural network

132
00:06:47,750 --> 00:06:52,130
layer and then feed it through
a softmax classifier of the kind

133
00:06:52,130 --> 00:06:55,400
that we, sorry, I
said that wrong,

134
00:06:55,400 --> 00:06:58,400
and then we're going to feed it
through a logistic classifier

135
00:06:58,400 --> 00:07:00,350
of the kind that
we saw when looking

136
00:07:00,350 --> 00:07:02,840
at negative sampling,
which is going

137
00:07:02,840 --> 00:07:07,460
to say for a particular
entity type, such as location,

138
00:07:07,460 --> 00:07:10,730
is that high
probability location

139
00:07:10,730 --> 00:07:13,340
or is that not a high
probability location?

140
00:07:13,340 --> 00:07:16,370
So for a sentence like
the museums in Paris

141
00:07:16,370 --> 00:07:20,120
are amazing to see, what we're
going to do is for each word,

142
00:07:20,120 --> 00:07:22,220
say we're doing the
word Paris, we're

143
00:07:22,220 --> 00:07:24,260
going to form a
window around that,

144
00:07:24,260 --> 00:07:26,810
say a plus or minus
2 word window.

145
00:07:26,810 --> 00:07:29,150
And so for those
five words, we're

146
00:07:29,150 --> 00:07:31,730
going to get word
vectors for them

147
00:07:31,730 --> 00:07:35,210
from the kind of Word2Vec
or GLoVe word vectors

148
00:07:35,210 --> 00:07:36,110
we've learned.

149
00:07:36,110 --> 00:07:38,930
And we're going to
make a long vector out

150
00:07:38,930 --> 00:07:41,730
of the concatenation of
those five word vectors

151
00:07:41,730 --> 00:07:43,820
so the word of interest
is in the middle.

152
00:07:43,820 --> 00:07:48,080
And then we're going
to feed this vector

153
00:07:48,080 --> 00:07:50,570
to a classifier
which is at the end

154
00:07:50,570 --> 00:07:55,460
going to have a probability
of the word being a location.

155
00:07:55,460 --> 00:07:57,290
And then we could have
another classifier

156
00:07:57,290 --> 00:08:00,643
that says the probability of
the word being a person name.

157
00:08:00,643 --> 00:08:02,060
And so once we've
done that, we're

158
00:08:02,060 --> 00:08:03,810
then going to run it
at the next position.

159
00:08:03,810 --> 00:08:07,430
So we then say, well, is
the word "a" a location?

160
00:08:07,430 --> 00:08:09,620
And we'd feed a
window of five words,

161
00:08:09,620 --> 00:08:12,320
it's then, in Paris
are amazing to,

162
00:08:12,320 --> 00:08:16,030
and put it through the
same kind of classifier.

163
00:08:16,030 --> 00:08:18,630
And so this is the
classifier that we'll use.

164
00:08:18,630 --> 00:08:23,090
So its input will
be this word window.

165
00:08:23,090 --> 00:08:25,380
So if we have D
dimensional word vectors,

166
00:08:25,380 --> 00:08:27,600
this will be a 5D vector.

167
00:08:27,600 --> 00:08:30,300
And then we're going to
put it through a layer

168
00:08:30,300 --> 00:08:31,900
of a neural network.

169
00:08:31,900 --> 00:08:34,049
So the layer of
the neural network

170
00:08:34,049 --> 00:08:39,000
is going to multiply
this vector by a matrix,

171
00:08:39,000 --> 00:08:42,299
add on a bias vector,
and then put that

172
00:08:42,299 --> 00:08:49,340
through a non-linearity such
as the softmax transformation

173
00:08:49,340 --> 00:08:50,810
that we've seen before.

174
00:08:50,810 --> 00:08:53,930
And that will give
us a hidden vector

175
00:08:53,930 --> 00:08:57,260
which might be of a smaller
dimensionality such as this one

176
00:08:57,260 --> 00:08:58,350
here.

177
00:08:58,350 --> 00:09:02,540
And so then with
that hidden vector,

178
00:09:02,540 --> 00:09:05,300
we're then going to
take the dot product

179
00:09:05,300 --> 00:09:10,110
of that with an extra
vector here, here it's u.

180
00:09:10,110 --> 00:09:12,830
So we take u dot product h.

181
00:09:12,830 --> 00:09:17,180
And so when we do that, we're
getting out a single number.

182
00:09:17,180 --> 00:09:20,010
And that number can
be any real number.

183
00:09:20,010 --> 00:09:23,600
And so then finally, we're
going to put that number

184
00:09:23,600 --> 00:09:28,580
through a logistic
transform of the same kind

185
00:09:28,580 --> 00:09:31,400
that we saw when doing
negative sampling.

186
00:09:31,400 --> 00:09:34,940
The logistic transform
will take any real number,

187
00:09:34,940 --> 00:09:38,240
and it'll transform
it into a probability

188
00:09:38,240 --> 00:09:40,490
that that word is the location.

189
00:09:40,490 --> 00:09:44,270
So its output is the
predicted probability

190
00:09:44,270 --> 00:09:47,060
of the word belonging
to a particular class.

191
00:09:47,060 --> 00:09:50,510
And so this could be
our location classifier

192
00:09:50,510 --> 00:09:53,450
which could classify
each word in a window

193
00:09:53,450 --> 00:09:57,860
as to what the probability is
that it's the location word.

194
00:09:57,860 --> 00:09:59,990
And so this little
neural network

195
00:09:59,990 --> 00:10:04,100
here is the neural network
I'm going to use today when

196
00:10:04,100 --> 00:10:06,440
going through some of the math.

197
00:10:06,440 --> 00:10:09,230
But actually, I'm going to
make it even easier on myself.

198
00:10:09,230 --> 00:10:14,960
I'm going to throw away the
logistic function at the top,

199
00:10:14,960 --> 00:10:16,370
and I'm really
just going to work

200
00:10:16,370 --> 00:10:19,910
through the math of the
bottom three-quarters of this.

201
00:10:19,910 --> 00:10:24,500
If you look at Kevin Clark's
handout that I just mentioned,

202
00:10:24,500 --> 00:10:27,170
he includes when he
works through it, also

203
00:10:27,170 --> 00:10:30,450
working through the
logistic function.

204
00:10:30,450 --> 00:10:34,400
And we also saw working through
a softmax in the first lecture

205
00:10:34,400 --> 00:10:38,460
when I was working through
some of the Word2Vec model.

206
00:10:38,460 --> 00:10:44,040
OK, so the overall question
we want to be able to answer

207
00:10:44,040 --> 00:10:48,270
is, so here's our stochastic
gradient descent equation,

208
00:10:48,270 --> 00:10:53,700
that we have existing
parameters of our model,

209
00:10:53,700 --> 00:11:00,240
and we want to update them
based on our current loss, which

210
00:11:00,240 --> 00:11:02,170
is the j of theta.

211
00:11:02,170 --> 00:11:07,260
So for getting our loss
here, that the true answer

212
00:11:07,260 --> 00:11:09,450
as to whether a word
is a location or not

213
00:11:09,450 --> 00:11:15,420
will be 1 if it is a location
or 0 if it isn't, our logistic

214
00:11:15,420 --> 00:11:19,380
classifier will return
some number like 0.9,

215
00:11:19,380 --> 00:11:21,660
and we'll use the distance
away from what it should

216
00:11:21,660 --> 00:11:24,910
have been squared as our loss.

217
00:11:24,910 --> 00:11:29,340
So we work out a loss and then
we're moving a little distance

218
00:11:29,340 --> 00:11:31,740
in the negative of
the gradient, which

219
00:11:31,740 --> 00:11:36,300
will be changing our parameter
estimates in such a way

220
00:11:36,300 --> 00:11:38,470
that they reduce the loss.

221
00:11:38,470 --> 00:11:41,070
And so this is
already being written

222
00:11:41,070 --> 00:11:43,960
in terms of a whole
vector of parameters,

223
00:11:43,960 --> 00:11:48,030
which is being updated as to
a new vector of parameters.

224
00:11:48,030 --> 00:11:52,440
But you can also think about it,
for each individual parameter,

225
00:11:52,440 --> 00:11:56,460
theta J, that we're working
out the partial derivative

226
00:11:56,460 --> 00:12:00,270
of the loss with respect
to that parameter.

227
00:12:00,270 --> 00:12:02,490
And then we're
moving a little bit

228
00:12:02,490 --> 00:12:06,220
in the negative
direction of that.

229
00:12:06,220 --> 00:12:11,860
That's going to give us a new
value for parameter theta J.

230
00:12:11,860 --> 00:12:15,730
And we're going to update all
of the parameters of our model

231
00:12:15,730 --> 00:12:17,080
as we learn.

232
00:12:17,080 --> 00:12:19,630
I mean, in particular,
in contrast

233
00:12:19,630 --> 00:12:24,250
to what commonly happens
in statistics, we also--

234
00:12:24,250 --> 00:12:29,080
we update not only the sort
of parameters of our model

235
00:12:29,080 --> 00:12:31,360
that are sort of weights
in the classifier,

236
00:12:31,360 --> 00:12:34,490
but we also will update
our data representation.

237
00:12:34,490 --> 00:12:38,770
So we'll also be changing
our word vectors as we learn.

238
00:12:38,770 --> 00:12:40,960
OK, so to build
neural nets, i.e.,

239
00:12:40,960 --> 00:12:44,050
to train neural
nets based on data,

240
00:12:44,050 --> 00:12:47,590
what we need is to
be able to compute

241
00:12:47,590 --> 00:12:52,030
this gradient of the parameters
so that we can then iteratively

242
00:12:52,030 --> 00:12:55,510
update the weights of the
model and efficiently train

243
00:12:55,510 --> 00:12:58,540
the model that has
good weights, i.e.

244
00:12:58,540 --> 00:13:00,700
that has high accuracy.

245
00:13:00,700 --> 00:13:03,710
And so how can we do that?

246
00:13:03,710 --> 00:13:06,850
Well, what I'm going
to talk about today

247
00:13:06,850 --> 00:13:11,660
is, first of all, how
you can do it by hand.

248
00:13:11,660 --> 00:13:15,020
And so for doing it by
hand, this is basically

249
00:13:15,020 --> 00:13:19,930
a review of matrix
calculus, and that will take

250
00:13:19,930 --> 00:13:21,730
quite a bit of the lecture.

251
00:13:21,730 --> 00:13:26,740
And then after we've talked
about that for a while,

252
00:13:26,740 --> 00:13:30,550
I'll then shift gears and
introduce the back propagation

253
00:13:30,550 --> 00:13:35,380
algorithm, which is the central
technology for neural networks.

254
00:13:35,380 --> 00:13:39,040
And that technology
is essentially

255
00:13:39,040 --> 00:13:42,580
the efficient application
of calculus on a large scale

256
00:13:42,580 --> 00:13:45,320
as we'll come to
talking about soon.

257
00:13:45,320 --> 00:13:49,840
So for computing gradients
by hand, what we're doing

258
00:13:49,840 --> 00:13:52,520
is matrix calculus.

259
00:13:52,520 --> 00:13:55,840
So we're working with
vectors and matrices

260
00:13:55,840 --> 00:13:58,070
and working out gradients.

261
00:13:58,070 --> 00:14:04,120
And this can seem like
pretty scary stuff.

262
00:14:04,120 --> 00:14:07,660
And well, to the extent
that you're kind of scared

263
00:14:07,660 --> 00:14:11,860
and don't know what's
going on, one choice

264
00:14:11,860 --> 00:14:17,680
is to work out a non-vectorized
gradient by just working out

265
00:14:17,680 --> 00:14:22,330
what the partial derivative
is for one parameter at a time

266
00:14:22,330 --> 00:14:26,470
and I showed a little example
of that in the first lecture.

267
00:14:26,470 --> 00:14:31,270
But it's much, much
faster and more useful

268
00:14:31,270 --> 00:14:37,210
to actually be able to work
with vectorized gradients.

269
00:14:37,210 --> 00:14:40,780
And in some sense, if
you're not very confident,

270
00:14:40,780 --> 00:14:43,540
this is kind of almost
a leap of faith,

271
00:14:43,540 --> 00:14:48,100
but it really is the case that
multivariable calculus is just

272
00:14:48,100 --> 00:14:50,770
like single variable
calculus except you're

273
00:14:50,770 --> 00:14:52,820
using vectors and matrices.

274
00:14:52,820 --> 00:14:55,240
So providing you
remember some basics

275
00:14:55,240 --> 00:14:58,300
of single variable
calculus, you really

276
00:14:58,300 --> 00:15:02,560
should be able to do this
stuff and get it to work out.

277
00:15:02,560 --> 00:15:07,070
Lots of other sources are
mentioned in the notes.

278
00:15:07,070 --> 00:15:10,780
You can also look at
the textbook for Math 51

279
00:15:10,780 --> 00:15:14,100
which also has quite a
lot of material on this.

280
00:15:14,100 --> 00:15:17,970
I know some of you have
bad memories of Math 51.

281
00:15:17,970 --> 00:15:19,770
OK, so let's go
through this and see

282
00:15:19,770 --> 00:15:22,600
how it works ramping
up from the beginning.

283
00:15:22,600 --> 00:15:26,850
So the beginning of calculus
is, we have a function

284
00:15:26,850 --> 00:15:29,190
with one input and one output.

285
00:15:29,190 --> 00:15:31,440
f(x) equals x cubed.

286
00:15:31,440 --> 00:15:34,830
And so then its gradient
is its slope, right?

287
00:15:34,830 --> 00:15:36,000
So its derivative.

288
00:15:36,000 --> 00:15:40,300
So its derivative is 3x squared.

289
00:15:40,300 --> 00:15:42,930
And the way to
think about this is

290
00:15:42,930 --> 00:15:46,470
how much will the output
change if we change

291
00:15:46,470 --> 00:15:48,450
the input a little bit, right?

292
00:15:48,450 --> 00:15:51,540
So what we're wanting to
do in our neural net models

293
00:15:51,540 --> 00:15:55,440
is change what they output
so that they do a better

294
00:15:55,440 --> 00:15:58,470
job of predicting the
correct answers when we're

295
00:15:58,470 --> 00:16:00,430
doing supervised learning.

296
00:16:00,430 --> 00:16:03,330
And so what we want to
know is if we fiddle

297
00:16:03,330 --> 00:16:06,090
different parameters of the
model, how much of an effect

298
00:16:06,090 --> 00:16:07,530
will that have on the output?

299
00:16:07,530 --> 00:16:10,890
Because then we can choose how
to fiddle them in the right way

300
00:16:10,890 --> 00:16:12,880
to move things down, right?

301
00:16:12,880 --> 00:16:17,920
So when we're saying that the
derivative here is 3x squared,

302
00:16:17,920 --> 00:16:23,340
well, what we're saying is
that if you're at x equals 1,

303
00:16:23,340 --> 00:16:26,490
if you fiddle the
input a little bit,

304
00:16:26,490 --> 00:16:30,960
the output will change 3 times
as much, 3 times 1 squared,

305
00:16:30,960 --> 00:16:31,720
and it does.

306
00:16:31,720 --> 00:16:36,420
So if I say what's the value
at 1.01, it's about 1.03.

307
00:16:36,420 --> 00:16:39,960
It's changed three times as
much, and that's its slope.

308
00:16:39,960 --> 00:16:47,370
But at x equals 4, the
derivative is 16 times 348.

309
00:16:47,370 --> 00:16:49,740
So if we fiddle
the input a little,

310
00:16:49,740 --> 00:16:52,080
it will change 48 times
as much, and that's

311
00:16:52,080 --> 00:16:56,820
roughly what happens,
4.01 cubed is 64.48.

312
00:16:56,820 --> 00:16:58,860
Now, of course,
this is just sort

313
00:16:58,860 --> 00:17:03,840
of showing it for a small fiddle
but that's an approximation

314
00:17:03,840 --> 00:17:06,640
to the actual truth.

315
00:17:06,640 --> 00:17:10,990
Okay, so then we sort of ramp
up to the more complex cases,

316
00:17:10,990 --> 00:17:12,490
which are more
reflective of what we

317
00:17:12,490 --> 00:17:14,240
do with neural networks.

318
00:17:14,240 --> 00:17:19,310
So if we have a function
with one output and n inputs,

319
00:17:19,310 --> 00:17:20,930
then we have a gradient.

320
00:17:20,930 --> 00:17:24,460
So a gradient is a vector
of partial derivatives

321
00:17:24,460 --> 00:17:26,089
with respect to each input.

322
00:17:26,089 --> 00:17:29,620
So we've got n inputs x1 to
xn, and we're working out

323
00:17:29,620 --> 00:17:31,700
the partial derivative
of f with respect

324
00:17:31,700 --> 00:17:35,650
to x1, the partial derivative
of f with respect to x2,

325
00:17:35,650 --> 00:17:41,620
et cetera, and we then get a
vector of partial derivatives

326
00:17:41,620 --> 00:17:43,660
where each element
of this vector

327
00:17:43,660 --> 00:17:49,360
is just like a simple derivative
with respect to one variable.

328
00:17:49,360 --> 00:17:51,970
OK, so from that
point, we just keep

329
00:17:51,970 --> 00:17:56,030
on ramping up for what we
do with neural networks.

330
00:17:56,030 --> 00:17:58,480
So commonly, when
we have something

331
00:17:58,480 --> 00:18:01,150
like layer in a
neural network, we'll

332
00:18:01,150 --> 00:18:04,030
have a function
within n inputs that

333
00:18:04,030 --> 00:18:06,310
will be like our
word vectors, then

334
00:18:06,310 --> 00:18:10,510
we do something like
multiply by a matrix,

335
00:18:10,510 --> 00:18:12,640
and then we'll have m outputs.

336
00:18:12,640 --> 00:18:15,940
So we have a function now
which is taking n inputs

337
00:18:15,940 --> 00:18:18,970
and is producing m outputs.

338
00:18:18,970 --> 00:18:23,260
So at this point, what
we're calculating for

339
00:18:23,260 --> 00:18:27,070
the gradient is what's
called a Jacobian matrix.

340
00:18:27,070 --> 00:18:31,120
So for m inputs and n
outputs, the Jacobian

341
00:18:31,120 --> 00:18:35,200
is an m by n matrix
of every combination

342
00:18:35,200 --> 00:18:36,950
of partial derivatives.

343
00:18:36,950 --> 00:18:43,600
So function f splits up into
these different sub functions

344
00:18:43,600 --> 00:18:48,770
f1 through fm, which generate
each of the m outputs.

345
00:18:48,770 --> 00:18:51,820
And so then we're taking
the partial derivative of f1

346
00:18:51,820 --> 00:18:55,870
with respect to x1 through
the partial derivative of f1

347
00:18:55,870 --> 00:18:59,170
with respect to xn,
then heading down,

348
00:18:59,170 --> 00:19:02,350
we can make it up to the partial
derivative of fm with respect

349
00:19:02,350 --> 00:19:03,730
to x1, et cetera.

350
00:19:03,730 --> 00:19:06,910
So we have every possible
partial derivative

351
00:19:06,910 --> 00:19:09,460
of an output
variable with respect

352
00:19:09,460 --> 00:19:13,200
to one of the input variables.

353
00:19:13,200 --> 00:19:20,340
OK, so in simple calculus,
when you have a composition

354
00:19:20,340 --> 00:19:22,480
of one variable functions.

355
00:19:22,480 --> 00:19:29,760
So that if you have y equals x
squared, and then z equals 3y,

356
00:19:29,760 --> 00:19:30,360
that's--

357
00:19:30,360 --> 00:19:35,760
then z is a composition
of two functions of--

358
00:19:35,760 --> 00:19:37,560
or you're composing
two functions,

359
00:19:37,560 --> 00:19:39,640
to get z as a function of x.

360
00:19:39,640 --> 00:19:42,240
Then you can work out
the derivative of z

361
00:19:42,240 --> 00:19:43,680
with respect to x.

362
00:19:43,680 --> 00:19:46,360
And the way you do that
is with the chain rule.

363
00:19:46,360 --> 00:19:49,530
And so in the chain rule,
you multiply derivatives.

364
00:19:49,530 --> 00:19:54,630
So dz dx equals
dz dy times dy dx.

365
00:19:54,630 --> 00:20:02,040
So dz dy is just
3, and dy dx is 2x.

366
00:20:02,040 --> 00:20:08,910
So we get 3 times 2x so that
overall, the derivative here

367
00:20:08,910 --> 00:20:10,350
is 6x.

368
00:20:10,350 --> 00:20:13,530
And since if we
multiply this together,

369
00:20:13,530 --> 00:20:17,250
we're really saying that
z equals 3x squared.

370
00:20:17,250 --> 00:20:19,620
You should trivially
be able to see again.

371
00:20:19,620 --> 00:20:22,350
Aha, its derivative is 6x.

372
00:20:22,350 --> 00:20:23,680
So that works.

373
00:20:23,680 --> 00:20:30,090
OK, so once we move into vectors
and matrices and Jacobians,

374
00:20:30,090 --> 00:20:32,250
it's actually the same game.

375
00:20:32,250 --> 00:20:34,260
So when we're
working with those,

376
00:20:34,260 --> 00:20:37,470
we can compose functions and
work out their derivatives

377
00:20:37,470 --> 00:20:40,110
by simply multiplying Jacobians.

378
00:20:40,110 --> 00:20:44,430
So if we start with an
input x and then put it

379
00:20:44,430 --> 00:20:48,120
through the simplest form
of neural network layer

380
00:20:48,120 --> 00:20:51,660
and say that z equals Wx plus b.

381
00:20:51,660 --> 00:20:54,840
So we multiply the
x vector by matrix W

382
00:20:54,840 --> 00:20:57,090
and then add on a bias vector b.

383
00:20:57,090 --> 00:20:58,770
And then typically,
we put things

384
00:20:58,770 --> 00:21:01,140
through a non-linearity f.

385
00:21:01,140 --> 00:21:03,720
So f could be a
sigmoid function.

386
00:21:03,720 --> 00:21:05,880
We'll then say h equals f(z).

387
00:21:05,880 --> 00:21:09,990
So this is the composition
of two functions in terms

388
00:21:09,990 --> 00:21:13,240
of vectors and matrices.

389
00:21:13,240 --> 00:21:15,180
So we can use
Jacobians and we can

390
00:21:15,180 --> 00:21:17,580
say the partial
of h with respect

391
00:21:17,580 --> 00:21:23,050
to x is going to be the
product of the partial h

392
00:21:23,050 --> 00:21:27,450
with respect to z, and the
partial z with respect to x.

393
00:21:27,450 --> 00:21:29,380
And this all does work out.

394
00:21:29,380 --> 00:21:34,290
So let's start going
through some examples of how

395
00:21:34,290 --> 00:21:37,920
these things work
slightly more concretely.

396
00:21:37,920 --> 00:21:42,270
First, just particular
Jacobians and then

397
00:21:42,270 --> 00:21:44,290
composing them together.

398
00:21:44,290 --> 00:21:48,720
So one case we look at
is the nonlinearities

399
00:21:48,720 --> 00:21:51,150
that we put a vector
through, so this

400
00:21:51,150 --> 00:21:53,400
is something like
putting a vector

401
00:21:53,400 --> 00:21:57,120
through the sigmoid function f.

402
00:21:57,120 --> 00:22:01,050
And so if we have an
intermediate vector z and we

403
00:22:01,050 --> 00:22:05,770
turn into vector h by putting
it through a logistic function,

404
00:22:05,770 --> 00:22:09,035
we can say what is dh / dz?.

405
00:22:09,035 --> 00:22:11,980


406
00:22:11,980 --> 00:22:18,130
Well, for this, formally
this is a function that

407
00:22:18,130 --> 00:22:21,560
has n inputs and n outputs.

408
00:22:21,560 --> 00:22:27,310
So at the end of the day, we're
computing an n by n Jacobian.

409
00:22:27,310 --> 00:22:32,890
And so what that's meaning is
the elements of this n by n

410
00:22:32,890 --> 00:22:36,510
Jacobian are going to take
the partial derivative

411
00:22:36,510 --> 00:22:42,610
of each output with
respect to each input.

412
00:22:42,610 --> 00:22:45,950
And well, what is that
going to be in this case?

413
00:22:45,950 --> 00:22:49,090
Well, in this case,
because we are actually

414
00:22:49,090 --> 00:22:54,440
just computing element
wise transformation,

415
00:22:54,440 --> 00:22:59,020
such as a logistic
transform of each element zi

416
00:22:59,020 --> 00:23:04,000
like the second equation
here, if i equals j,

417
00:23:04,000 --> 00:23:06,040
we've got something to compute.

418
00:23:06,040 --> 00:23:10,870
Whereas, if i doesn't
equal J, the input

419
00:23:10,870 --> 00:23:15,280
has no influence on the output
and so the derivative is 0.

420
00:23:15,280 --> 00:23:18,700
So if i doesn't equal j
we're going to get a 0,

421
00:23:18,700 --> 00:23:21,430
and if i does equal
j, well, then we're

422
00:23:21,430 --> 00:23:25,450
going to get the regular
one variable derivative

423
00:23:25,450 --> 00:23:31,170
of the logistic function,
which if I remember correctly,

424
00:23:31,170 --> 00:23:33,402
you were asked to compute--

425
00:23:33,402 --> 00:23:35,360
now I can't remember
whether it's assignment, 1

426
00:23:35,360 --> 00:23:38,790
or assignment 2, but one of the
two asked you to compute it.

427
00:23:38,790 --> 00:23:42,540
So our Jacobian for this
case looks like this.

428
00:23:42,540 --> 00:23:50,075
We have a diagonal matrix with
the derivatives of each element

429
00:23:50,075 --> 00:23:54,490
element along the diagonal
and everything else is 0.

430
00:23:54,490 --> 00:23:58,840
OK, so let's look at a
couple of other Jacobians.

431
00:23:58,840 --> 00:24:04,330
So if we are asking, if we've
got this Wx plus b basic neural

432
00:24:04,330 --> 00:24:08,890
network layer and we're asking
for the gradient with respect

433
00:24:08,890 --> 00:24:14,080
to x, then what are we
going to have coming out

434
00:24:14,080 --> 00:24:16,780
is that that's actually
going to be the matrix W.

435
00:24:16,780 --> 00:24:18,610
So this is where--

436
00:24:18,610 --> 00:24:24,160
what I hope you can do is
look at the notes at home

437
00:24:24,160 --> 00:24:30,010
and work through this exactly,
and see that this is actually

438
00:24:30,010 --> 00:24:31,610
the right answer.

439
00:24:31,610 --> 00:24:36,700
But this is the way in which if
you just have faith and think,

440
00:24:36,700 --> 00:24:40,540
this is just like
single variable calculus

441
00:24:40,540 --> 00:24:44,140
except I've now got vectors
and matrices, the answer you

442
00:24:44,140 --> 00:24:46,000
get is actually what
you expected to get.

443
00:24:46,000 --> 00:24:51,280
Because this is just like
the derivative of ax plus b

444
00:24:51,280 --> 00:24:54,190
with respect to x where it's a.

445
00:24:54,190 --> 00:24:57,370
So similarly, if we take the
partial derivative with respect

446
00:24:57,370 --> 00:25:05,380
to b of Wx plus b we get
out the identity matrix.

447
00:25:05,380 --> 00:25:10,330
OK, then one other Jacobian that
we mentioned while in the first

448
00:25:10,330 --> 00:25:14,710
lecture while working through
Word2Vec is if you have the dot

449
00:25:14,710 --> 00:25:22,360
product of two vectors,
i.e., that's a number,

450
00:25:22,360 --> 00:25:25,550
that what you get
coming out of it--

451
00:25:25,550 --> 00:25:31,240
so the partial derivative
of uTh with respect to u

452
00:25:31,240 --> 00:25:33,550
is h transpose.

453
00:25:33,550 --> 00:25:36,430
And at this point,
there's some fine print

454
00:25:36,430 --> 00:25:40,940
that I'm going to come
back to in a minute.

455
00:25:40,940 --> 00:25:43,300
So this is the correct
Jacobian, right?

456
00:25:43,300 --> 00:25:50,620
Because in this case, we have
the dimension of h inputs

457
00:25:50,620 --> 00:25:57,460
and we have one output, and so
we want to have a row vector.

458
00:25:57,460 --> 00:25:59,380
But there's a little
bit more to say on

459
00:25:59,380 --> 00:26:03,280
that that I'll come back
to in about 20 slides,

460
00:26:03,280 --> 00:26:06,090
but this is the
correct Jacobian.

461
00:26:06,090 --> 00:26:10,470
OK, so if you are not familiar
with these kind of Jacobians,

462
00:26:10,470 --> 00:26:15,960
do please look at some of
the notes that are available

463
00:26:15,960 --> 00:26:19,740
and try and compute these
in more detail element wise

464
00:26:19,740 --> 00:26:22,500
and convince yourself that
they really are right.

465
00:26:22,500 --> 00:26:25,350
But I'm going to assume
these now and show you

466
00:26:25,350 --> 00:26:28,470
what happens when we
actually then work out

467
00:26:28,470 --> 00:26:31,440
gradients for at least a
mini little neural net.

468
00:26:31,440 --> 00:26:35,470


469
00:26:35,470 --> 00:26:42,320
OK, so here is most
of this neural net.

470
00:26:42,320 --> 00:26:47,330
I mean, as I commented
that really we'd

471
00:26:47,330 --> 00:26:50,790
be working out the partial
derivative of the loss

472
00:26:50,790 --> 00:26:54,350
j with respect to these
variables, but for the example

473
00:26:54,350 --> 00:26:56,090
I'm doing here, I just--

474
00:26:56,090 --> 00:26:58,880
I've locked that off to keep
it a little simpler and more

475
00:26:58,880 --> 00:27:00,440
manageable for the lecture.

476
00:27:00,440 --> 00:27:02,690
And so we're going
to just work out

477
00:27:02,690 --> 00:27:05,630
the partial derivative
of the score s, which

478
00:27:05,630 --> 00:27:09,650
is a real number with respect
to the different parameters

479
00:27:09,650 --> 00:27:12,980
of this model where the
parameters of this model

480
00:27:12,980 --> 00:27:19,310
are going to be the W
and the b and the u,

481
00:27:19,310 --> 00:27:23,840
and also the input because
we can update the weight

482
00:27:23,840 --> 00:27:28,490
vectors of the word
vectors of different words

483
00:27:28,490 --> 00:27:33,170
based on tuning them to better
predict the classification

484
00:27:33,170 --> 00:27:35,130
outputs that we desire.

485
00:27:35,130 --> 00:27:37,760
So let's start off
with a fairly easy one

486
00:27:37,760 --> 00:27:42,740
where we want to update
the bias vector b to have

487
00:27:42,740 --> 00:27:45,620
our system classify better.

488
00:27:45,620 --> 00:27:48,470
So to be able to do that,
what we want to work out

489
00:27:48,470 --> 00:27:51,410
is the partial derivatives
of s with respect

490
00:27:51,410 --> 00:27:56,570
to b, so we know how to put that
into our stochastic gradient

491
00:27:56,570 --> 00:28:00,290
update for the b parameters.

492
00:28:00,290 --> 00:28:03,720
OK, so how do we go
about doing these things?

493
00:28:03,720 --> 00:28:07,700
So the first step is we
want to sort of break things

494
00:28:07,700 --> 00:28:12,860
up into different functions
of minimal complexity

495
00:28:12,860 --> 00:28:14,640
that compose together.

496
00:28:14,640 --> 00:28:16,940
So in particular,
this neural net layer

497
00:28:16,940 --> 00:28:21,480
h equals f of Wx plus b, it's
still a little bit complex.

498
00:28:21,480 --> 00:28:24,350
So let's decompose
that one further step.

499
00:28:24,350 --> 00:28:30,560
So we have the input
x, we then calculate

500
00:28:30,560 --> 00:28:35,600
the linear transformation
z equals Wx plus b

501
00:28:35,600 --> 00:28:40,820
and then we put things
through the sort of element

502
00:28:40,820 --> 00:28:46,070
wise non-linearity h
equals f(z) and then we

503
00:28:46,070 --> 00:28:49,250
do the dot product with u.

504
00:28:49,250 --> 00:28:53,860
And it's useful for
working these things out

505
00:28:53,860 --> 00:28:58,310
to split into pieces like
this, have straight what

506
00:28:58,310 --> 00:29:00,680
your different variables
are, and to know

507
00:29:00,680 --> 00:29:04,730
what the dimensionality of
each of these variables is.

508
00:29:04,730 --> 00:29:07,280
It's well worth just writing
out the dimensionality

509
00:29:07,280 --> 00:29:09,350
of every variable
and making sure

510
00:29:09,350 --> 00:29:11,240
that the answers
that you're computing

511
00:29:11,240 --> 00:29:14,070
are of the right dimensionality.

512
00:29:14,070 --> 00:29:18,980
So at this point though, what
we can see is that calculating s

513
00:29:18,980 --> 00:29:22,490
is the product of three--

514
00:29:22,490 --> 00:29:28,010
I'm sorry, it's the composition
of three functions around x.

515
00:29:28,010 --> 00:29:34,340
So for working out the partials
of s with respect to b,

516
00:29:34,340 --> 00:29:36,710
it's the composition
of the three

517
00:29:36,710 --> 00:29:39,230
functions shown on the left.

518
00:29:39,230 --> 00:29:44,180
And so therefore, the gradient
of x with respect to b we're

519
00:29:44,180 --> 00:29:50,420
going to take the
product of these three

520
00:29:50,420 --> 00:29:53,880
partial derivatives.

521
00:29:53,880 --> 00:29:58,290
OK, so how do-- what do we--

522
00:29:58,290 --> 00:30:01,350
so we've got the s equals uTh.

523
00:30:01,350 --> 00:30:05,220
So that's sort of the
top corresponding partial

524
00:30:05,220 --> 00:30:08,100
derivative, partial
derivative h with respect

525
00:30:08,100 --> 00:30:12,060
to z, partial derivative
of z with respect

526
00:30:12,060 --> 00:30:16,050
to b, which is the first
one that we're working out.

527
00:30:16,050 --> 00:30:18,790
OK, so we want to work this out.

528
00:30:18,790 --> 00:30:21,690
And if we're lucky, we
remember those Jacobians

529
00:30:21,690 --> 00:30:26,560
I showed previously about
the Jacobian for a vector dot

530
00:30:26,560 --> 00:30:31,560
product, the Jacobian
for the non-linearity

531
00:30:31,560 --> 00:30:36,370
and the Jacobian for the
simple linear transformation.

532
00:30:36,370 --> 00:30:37,920
And so we can use those.

533
00:30:37,920 --> 00:30:45,450
So for the partials of s
with respect to h, well,

534
00:30:45,450 --> 00:30:49,530
that's going to be uT
using the first one.

535
00:30:49,530 --> 00:30:52,530
The partials of h
with respect to z, OK,

536
00:30:52,530 --> 00:30:55,650
so that's the
non-linearity, and so that's

537
00:30:55,650 --> 00:30:59,760
going to be the matrix
that's the diagonal matrix

538
00:30:59,760 --> 00:31:05,280
with the element-wise derivative
f prime of z and 0 elsewhere.

539
00:31:05,280 --> 00:31:09,930
And then for the Wx
plus b, when we're

540
00:31:09,930 --> 00:31:14,520
taking the partials with respect
to b, that's just the identity

541
00:31:14,520 --> 00:31:15,820
matrix.

542
00:31:15,820 --> 00:31:18,000
So we can simplify
that down a little,

543
00:31:18,000 --> 00:31:27,760
the identity matrix disappears
and since uT is a vector

544
00:31:27,760 --> 00:31:30,190
and this is a
diagonal matrix, we

545
00:31:30,190 --> 00:31:36,010
can rewrite this as uT Hadamard
product of f prime of z.

546
00:31:36,010 --> 00:31:38,230
I think this is
the first time I've

547
00:31:38,230 --> 00:31:41,230
used this little circle
for Hadamard product

548
00:31:41,230 --> 00:31:43,900
but it's something
that you'll see

549
00:31:43,900 --> 00:31:48,140
quite a bit in neural network
work since it's often used.

550
00:31:48,140 --> 00:31:56,420
So when we have two vectors uT
and this vector here, sometimes

551
00:31:56,420 --> 00:31:59,360
you want to do an
element wise product.

552
00:31:59,360 --> 00:32:01,400
So the output of
this will be a vector

553
00:32:01,400 --> 00:32:03,410
where you've taken the
first element of each

554
00:32:03,410 --> 00:32:05,510
and multiplied them, the
second element of each

555
00:32:05,510 --> 00:32:08,340
and multiplied them,
et cetera downwards.

556
00:32:08,340 --> 00:32:10,460
And so that's called
the Hadamard product

557
00:32:10,460 --> 00:32:12,350
and that's what
we're calculating

558
00:32:12,350 --> 00:32:19,390
as to calculate a vector,
which is the gradient of s

559
00:32:19,390 --> 00:32:22,820
with respect to b.

560
00:32:22,820 --> 00:32:25,850
OK, so that's good.

561
00:32:25,850 --> 00:32:29,180
So we now have a gradient
of s with respect

562
00:32:29,180 --> 00:32:33,170
to b and we could use that
in our stochastic gradient.

563
00:32:33,170 --> 00:32:36,350
But we don't stop
there, we also want

564
00:32:36,350 --> 00:32:40,250
to work out the
gradient with respect

565
00:32:40,250 --> 00:32:42,510
to others of our parameters.

566
00:32:42,510 --> 00:32:48,080
So we might want to next go on
and work out the gradients of s

567
00:32:48,080 --> 00:32:50,900
with respect to w.

568
00:32:50,900 --> 00:32:57,110
Well, we can use the chain rule
just like we did before, right?

569
00:32:57,110 --> 00:33:00,980
So we've got the same product
of functions and everything

570
00:33:00,980 --> 00:33:03,560
is going to be the
same apart from me now

571
00:33:03,560 --> 00:33:09,470
taking the derivatives with
respect to w rather than b.

572
00:33:09,470 --> 00:33:14,810
So it's now going to be the
partial of s with respect to h,

573
00:33:14,810 --> 00:33:19,130
h with respect to z,
and z with respect to w.

574
00:33:19,130 --> 00:33:23,930
And the important thing to
notice here and this leads

575
00:33:23,930 --> 00:33:27,830
into what we do with the
back propagation algorithm

576
00:33:27,830 --> 00:33:33,470
is wait a minute, this is very
similar to what we've already

577
00:33:33,470 --> 00:33:34,280
done.

578
00:33:34,280 --> 00:33:37,010
So when we are working
out the gradients of s

579
00:33:37,010 --> 00:33:42,170
with respect to b, the first
two terms were exactly the same,

580
00:33:42,170 --> 00:33:45,410
it's only the last
one that differs.

581
00:33:45,410 --> 00:33:50,690
So to be able to
build or to train

582
00:33:50,690 --> 00:33:53,300
neural networks
efficiently, this

583
00:33:53,300 --> 00:33:57,320
is what happens all the time
and it's absolutely essential

584
00:33:57,320 --> 00:34:02,720
that we use an algorithm that
avoids repeated computation.

585
00:34:02,720 --> 00:34:05,960
And so the idea we're
going to develop

586
00:34:05,960 --> 00:34:08,840
is when we have
this equation stack

587
00:34:08,840 --> 00:34:13,639
that this sort of stuff that's
above where we compute z,

588
00:34:13,639 --> 00:34:17,360
and we're going to be sort of,
that'll be the same each time

589
00:34:17,360 --> 00:34:20,989
and we want to compute something
from that that we can then

590
00:34:20,989 --> 00:34:24,799
sort of feed downwards
when working out

591
00:34:24,800 --> 00:34:29,989
the gradients with
respect to Wx or b.

592
00:34:29,989 --> 00:34:34,880
And so we do that
by defining delta,

593
00:34:34,880 --> 00:34:38,780
which is delta is the
partials composed that

594
00:34:38,780 --> 00:34:42,290
are above the linear transform.

595
00:34:42,290 --> 00:34:44,810
And that's referred
to as the local error

596
00:34:44,810 --> 00:34:46,520
signal, that's
what's being passed

597
00:34:46,520 --> 00:34:50,270
in from above to the
linear transform.

598
00:34:50,270 --> 00:34:54,889
And we've already computed
the gradient of that

599
00:34:54,889 --> 00:34:56,960
in the preceding slides.

600
00:34:56,960 --> 00:35:02,300
And so the final form of the
partial with respect to b

601
00:35:02,300 --> 00:35:09,270
will be delta times
the remaining part.

602
00:35:09,270 --> 00:35:13,650
And well, we've seen
that for partial of s

603
00:35:13,650 --> 00:35:16,110
with respect to b,
the partial of z

604
00:35:16,110 --> 00:35:18,810
with respect to b is
just the identity,

605
00:35:18,810 --> 00:35:20,610
so the end result was delta.

606
00:35:20,610 --> 00:35:22,560
But in this time,
we're then going

607
00:35:22,560 --> 00:35:25,470
to have to work out the
partial of z with respect

608
00:35:25,470 --> 00:35:28,650
to w and multiply that by delta.

609
00:35:28,650 --> 00:35:32,910
So that's the part that
we still haven't yet done.

610
00:35:32,910 --> 00:35:39,930
So and this is where things
get in some sense, a little bit

611
00:35:39,930 --> 00:35:44,220
hairier, and so
there's something

612
00:35:44,220 --> 00:35:46,700
that's important to explain.

613
00:35:46,700 --> 00:35:54,360
So what should we have for
the Jacobian of ds / dW?

614
00:35:54,360 --> 00:36:01,050
Well, that's a function
that has one output,

615
00:36:01,050 --> 00:36:04,120
the output is just a
score of real number,

616
00:36:04,120 --> 00:36:07,020
and then it has n by m inputs.

617
00:36:07,020 --> 00:36:15,240
So that Jacobian is a 1
by n by m matrix, i.e.

618
00:36:15,240 --> 00:36:21,660
a very long low vector
but that's correct math.

619
00:36:21,660 --> 00:36:24,210
But it turns out
that that's kind of

620
00:36:24,210 --> 00:36:26,700
bad for our neural networks.

621
00:36:26,700 --> 00:36:29,370
Because remember, what we want
to do with our neural networks

622
00:36:29,370 --> 00:36:32,200
is do stochastic
gradient descent.

623
00:36:32,200 --> 00:36:36,150
And we want to say theta
new equals theta old

624
00:36:36,150 --> 00:36:41,640
minus a small multiplier
times the gradient.

625
00:36:41,640 --> 00:36:53,050
And well, actually the W
matrix is an n by m matrix,

626
00:36:53,050 --> 00:36:56,170
and so we couldn't
actually do the subtraction

627
00:36:56,170 --> 00:37:00,900
if this gradient we calculate
is just a huge row vector.

628
00:37:00,900 --> 00:37:06,420
We'd like to have it as the
same shape as the W matrix.

629
00:37:06,420 --> 00:37:09,390
In neural network
land when we do this,

630
00:37:09,390 --> 00:37:12,540
we depart from pure
math at this point

631
00:37:12,540 --> 00:37:15,720
and we use what we call
the shape convention.

632
00:37:15,720 --> 00:37:19,530
So what we're going
to say is and you're

633
00:37:19,530 --> 00:37:22,230
meant to use this for
answers in the assignment,

634
00:37:22,230 --> 00:37:24,510
that the shape of
the gradient we're

635
00:37:24,510 --> 00:37:28,650
always going to make to be
the shape of the parameters.

636
00:37:28,650 --> 00:37:34,020
And so therefore,
ds / dW, we are also

637
00:37:34,020 --> 00:37:38,640
going to represent as an
n by m matrix just like W,

638
00:37:38,640 --> 00:37:43,560
and we're going to reshape
the Jacobian to place it

639
00:37:43,560 --> 00:37:47,230
into this matrix shape.

640
00:37:47,230 --> 00:37:55,270
OK, so if we want to place it
into this matrix shape, what

641
00:37:55,270 --> 00:37:59,920
are we going to want
to get for ds / dW?

642
00:37:59,920 --> 00:38:06,310
Well, we know that it's
going to involve delta,

643
00:38:06,310 --> 00:38:15,000
our local error signal and
then we have to work out

644
00:38:15,000 --> 00:38:16,732
something for dz / dW.

645
00:38:16,732 --> 00:38:19,350


646
00:38:19,350 --> 00:38:23,520
Well, since z equals
Wx plus b, you'd

647
00:38:23,520 --> 00:38:28,660
kind of expect that
the answer should be x.

648
00:38:28,660 --> 00:38:30,030
And that's right.

649
00:38:30,030 --> 00:38:35,520
So the answer to
ds / dW is going

650
00:38:35,520 --> 00:38:40,230
to be delta transpose
times x transpose.

651
00:38:40,230 --> 00:38:43,740
And so the form that we're
getting for this derivative is

652
00:38:43,740 --> 00:38:49,890
going to be the product of
the local error signal that

653
00:38:49,890 --> 00:38:56,310
comes from above versus what we
calculate from the local input

654
00:38:56,310 --> 00:38:58,290
x.

655
00:38:58,290 --> 00:39:01,330
So that shouldn't yet be
obvious why that is true.

656
00:39:01,330 --> 00:39:04,560
So let me just go through
in a bit more detail

657
00:39:04,560 --> 00:39:06,190
why that's true.

658
00:39:06,190 --> 00:39:12,300
So when we want to
work out the ds / dW,

659
00:39:12,300 --> 00:39:14,120
right, it's sort
of delta times dz /

660
00:39:14,120 --> 00:39:20,400
dW where that's computing
for z is Wx plus b.

661
00:39:20,400 --> 00:39:23,250
So let's just
consider for a moment

662
00:39:23,250 --> 00:39:25,890
what the derivative
is with respect

663
00:39:25,890 --> 00:39:29,190
to a single weight, Wij.

664
00:39:29,190 --> 00:39:35,580
So Wij might be W23 that's shown
in my little neural network

665
00:39:35,580 --> 00:39:36,640
here.

666
00:39:36,640 --> 00:39:42,090
And so the first thing to
notice is that Wij only

667
00:39:42,090 --> 00:39:50,190
contributes to zi, so
it's going into to z2,

668
00:39:50,190 --> 00:39:57,130
which then computes h2 and it
has no effect whatsoever on h1.

669
00:39:57,130 --> 00:40:01,960
OK, so when we're
working out dzi,

670
00:40:01,960 --> 00:40:10,000
dWij, it's going to be
dWix that sort of row,

671
00:40:10,000 --> 00:40:16,490
that row of the matrix plus
bi, which means that for--

672
00:40:16,490 --> 00:40:21,250
we've got a kind of a
sum of Wik times xk.

673
00:40:21,250 --> 00:40:25,930
And then for this sum, this
is like one variable calculus

674
00:40:25,930 --> 00:40:28,930
that when we're taking the
derivative of this with respect

675
00:40:28,930 --> 00:40:35,260
to Wij, every term in
the sum is going to be 0,

676
00:40:35,260 --> 00:40:38,020
the derivative is going to
be 0 except for the one that

677
00:40:38,020 --> 00:40:40,000
involves Wij.

678
00:40:40,000 --> 00:40:42,550
And then the derivative
of that is just

679
00:40:42,550 --> 00:40:46,480
like ax with respect
to a, it's going

680
00:40:46,480 --> 00:40:51,600
to be x so you get
xj out as the answer.

681
00:40:51,600 --> 00:40:56,930
And so the end result of that
is that when we're working out

682
00:40:56,930 --> 00:40:59,690
what we want as the
answer is that we're

683
00:40:59,690 --> 00:41:08,060
going to get these columns
where x1 is all that's left,

684
00:41:08,060 --> 00:41:12,810
x2 is all that's left through
xm is all that's left.

685
00:41:12,810 --> 00:41:17,960
And then that's multiplied by
the vectors of the local error

686
00:41:17,960 --> 00:41:19,490
signal from above.

687
00:41:19,490 --> 00:41:24,320
And what we want to compute is
this outer product matrix where

688
00:41:24,320 --> 00:41:28,310
we're getting the different
combinations of the delta

689
00:41:28,310 --> 00:41:29,690
and the x.

690
00:41:29,690 --> 00:41:33,350
And so we can get
the n by m matrix

691
00:41:33,350 --> 00:41:36,890
that we'd like to have
via our shape convention

692
00:41:36,890 --> 00:41:41,840
by taking delta transpose,
which is n by 1 times

693
00:41:41,840 --> 00:41:44,840
x transpose, which
is n1 by m and then

694
00:41:44,840 --> 00:41:48,040
we get this out
of product matrix.

695
00:41:48,040 --> 00:41:52,520
So like that's the kind of a
hacky argument that I've made.

696
00:41:52,520 --> 00:41:54,380
It's certainly a
way of doing things

697
00:41:54,380 --> 00:41:58,670
that the dimensions work out
and it sort of makes sense,

698
00:41:58,670 --> 00:42:01,340
there's a more detailed
run through this that

699
00:42:01,340 --> 00:42:03,960
appears on the lecture notes.

700
00:42:03,960 --> 00:42:06,440
And I encourage
you to sort of also

701
00:42:06,440 --> 00:42:09,860
look at the more
mathy version of that.

702
00:42:09,860 --> 00:42:12,170
Here's a little bit
more information

703
00:42:12,170 --> 00:42:14,700
about the shape convention.

704
00:42:14,700 --> 00:42:21,290
So well, first of
all one more example

705
00:42:21,290 --> 00:42:30,700
of this, so when you're working
ds / db that comes out as--

706
00:42:30,700 --> 00:42:34,570
its Jacobian is a row vector.

707
00:42:34,570 --> 00:42:37,900
But similarly, you know
according to the shape

708
00:42:37,900 --> 00:42:44,590
convention we want our gradient
to be the same shape as b and b

709
00:42:44,590 --> 00:42:47,500
is column vector, so
that's sort of again,

710
00:42:47,500 --> 00:42:50,950
they're different shapes and
you have to transpose one

711
00:42:50,950 --> 00:42:52,370
to get the other.

712
00:42:52,370 --> 00:42:55,360
And so effectively, what
we have is a disagreement

713
00:42:55,360 --> 00:42:58,000
between the Jacobian form.

714
00:42:58,000 --> 00:43:02,860
So the Jacobian form makes
sense for calculus and math.

715
00:43:02,860 --> 00:43:07,720
Because if you want to have
that like I claimed, that matrix

716
00:43:07,720 --> 00:43:10,780
calculus is just like
single variable calculus

717
00:43:10,780 --> 00:43:13,090
apart from using
vectors and matrices,

718
00:43:13,090 --> 00:43:15,730
you can just multiply
together the parftials,

719
00:43:15,730 --> 00:43:19,660
that only works out if
you're using Jacobians.

720
00:43:19,660 --> 00:43:21,670
But on the other
hand, if you want

721
00:43:21,670 --> 00:43:26,020
to do stochastic
gradient descent

722
00:43:26,020 --> 00:43:30,640
and be able to sort of subtract
off a piece of the gradient,

723
00:43:30,640 --> 00:43:34,030
that only works if you
have the same shape

724
00:43:34,030 --> 00:43:41,240
matrix for the gradient as you
do for the original matrix.

725
00:43:41,240 --> 00:43:45,310
And so this is a bit confusing,
but that's just the reality,

726
00:43:45,310 --> 00:43:48,440
there are both of
these two things.

727
00:43:48,440 --> 00:43:54,910
So the Jacobian form is
useful in doing the calculus.

728
00:43:54,910 --> 00:43:58,300
But for the answers
in the assignment,

729
00:43:58,300 --> 00:44:02,350
we want the answers to be
presented using the shape

730
00:44:02,350 --> 00:44:07,270
convention so that
the gradient is shown

731
00:44:07,270 --> 00:44:11,350
in the same shape
as the parameters

732
00:44:11,350 --> 00:44:13,330
and therefore, you'll
be able to-- it's

733
00:44:13,330 --> 00:44:17,200
the right shape for doing
a gradient update by just

734
00:44:17,200 --> 00:44:21,620
subtracting a small
amount of the gradient.

735
00:44:21,620 --> 00:44:26,170
So for working through things
there are then basically

736
00:44:26,170 --> 00:44:28,280
two choices.

737
00:44:28,280 --> 00:44:34,060
One choice is to work through
all the math using Jacobians

738
00:44:34,060 --> 00:44:38,950
and then right at the end, to
reshape following the shape

739
00:44:38,950 --> 00:44:40,940
convention to give the answer.

740
00:44:40,940 --> 00:44:45,640
So that's what I did
when I worked out ds/ db.

741
00:44:45,640 --> 00:44:50,770
We worked through
it using Jacobians,

742
00:44:50,770 --> 00:44:54,760
we got an answer but it
turned out to be a row vector,

743
00:44:54,760 --> 00:44:57,430
and so well then we
have to transpose that

744
00:44:57,430 --> 00:45:02,620
at the end to get into the right
shape for the shape convention.

745
00:45:02,620 --> 00:45:10,870
The alternative is to always
follow the shape convention.

746
00:45:10,870 --> 00:45:14,410
And that's kind of what I did
when I was then working out

747
00:45:14,410 --> 00:45:18,790
ds / dW, I didn't
fully use Jacobians.

748
00:45:18,790 --> 00:45:25,660
I said, Oh, well, when we work
out whatever it was, dz / dW,

749
00:45:25,660 --> 00:45:28,580
let's work out what
shape we want it to be

750
00:45:28,580 --> 00:45:31,330
and what to fill
in the cells with.

751
00:45:31,330 --> 00:45:36,010
And if you're sort of
trying to do it immediately

752
00:45:36,010 --> 00:45:41,410
with the shape convention,
it's a little bit more hacky

753
00:45:41,410 --> 00:45:43,420
in a way, since
you know you have

754
00:45:43,420 --> 00:45:46,000
to look at the dimensions
for what you want and figure

755
00:45:46,000 --> 00:45:49,360
out when to transpose
or to reshape the matrix

756
00:45:49,360 --> 00:45:50,865
to be at the right shape.

757
00:45:50,865 --> 00:45:55,720
But the kind of informal
reasoning that I gave

758
00:45:55,720 --> 00:45:58,060
is what you do and what works.

759
00:45:58,060 --> 00:46:02,890
And one way, and there are
sort of hints that you can use,

760
00:46:02,890 --> 00:46:05,680
right, that you know that
your gradient should always

761
00:46:05,680 --> 00:46:08,830
be the same shape
as your parameters

762
00:46:08,830 --> 00:46:12,460
and you know that the
error message coming in

763
00:46:12,460 --> 00:46:15,040
will always have the
same dimensionality

764
00:46:15,040 --> 00:46:18,670
as that hidden layer, and
you can work it out always

765
00:46:18,670 --> 00:46:20,260
following the shape convention.

766
00:46:20,260 --> 00:46:23,180


767
00:46:23,180 --> 00:46:34,640
OK, so that is hey, doing
this is all matrix calculus.

768
00:46:34,640 --> 00:46:40,290
So after pausing for
breath for a second,

769
00:46:40,290 --> 00:46:45,170
the rest of the
lecture is then OK.

770
00:46:45,170 --> 00:46:51,200
Let's look at how our software
trains neural networks using

771
00:46:51,200 --> 00:46:56,150
what's referred to as the
backpropagation algorithm.

772
00:46:56,150 --> 00:47:04,370


773
00:47:04,370 --> 00:47:11,180
So the short answer is
basically we've already done it.

774
00:47:11,180 --> 00:47:13,700
The rest of the lecture is easy.

775
00:47:13,700 --> 00:47:18,710
So essentially, I've just shown
you what the backpropagation

776
00:47:18,710 --> 00:47:20,930
algorithm does.

777
00:47:20,930 --> 00:47:25,130
So the backpropagation
algorithm is

778
00:47:25,130 --> 00:47:34,010
judiciously taking and
propagating derivatives

779
00:47:34,010 --> 00:47:38,750
using the matrix chain rule.

780
00:47:38,750 --> 00:47:41,480
The rest of the back
propagation algorithm

781
00:47:41,480 --> 00:47:46,010
is to say, OK, when we
have these neural networks,

782
00:47:46,010 --> 00:47:50,750
we have a lot of shared
structure and shared

783
00:47:50,750 --> 00:47:52,230
derivatives.

784
00:47:52,230 --> 00:47:56,090
So what we want
to do is maximally

785
00:47:56,090 --> 00:48:01,790
efficiently re-use
derivatives of higher layers

786
00:48:01,790 --> 00:48:05,420
when we're computing
derivatives for lower layers

787
00:48:05,420 --> 00:48:07,890
so that we minimize computation.

788
00:48:07,890 --> 00:48:10,550
And I already pointed
that out in the first half

789
00:48:10,550 --> 00:48:14,940
but we want to
systematically exploit that.

790
00:48:14,940 --> 00:48:19,730
And so the way we do that
in our computational systems

791
00:48:19,730 --> 00:48:23,780
is they construct
computation graphs.

792
00:48:23,780 --> 00:48:27,530
So this maybe looks a
little bit like what

793
00:48:27,530 --> 00:48:30,830
you saw in a compilers
class if you did one, right,

794
00:48:30,830 --> 00:48:33,640
that you're creating--

795
00:48:33,640 --> 00:48:36,650
I call it here computation graph
but it's really a tree, right?

796
00:48:36,650 --> 00:48:41,360
So you're creating here
this tree of computations

797
00:48:41,360 --> 00:48:44,090
in this case but in
a more general case

798
00:48:44,090 --> 00:48:50,810
it's some kind of directed
graph of computations which has

799
00:48:50,810 --> 00:48:54,260
source nodes, which are inputs.

800
00:48:54,260 --> 00:48:58,700
Either inputs like x or
input parameters like W

801
00:48:58,700 --> 00:49:03,710
and b and its interior
nodes are operations.

802
00:49:03,710 --> 00:49:06,780
And so then once we've
constructed a graph,

803
00:49:06,780 --> 00:49:09,950
and so this graph corresponds
to exactly the example

804
00:49:09,950 --> 00:49:11,030
I did before, right?

805
00:49:11,030 --> 00:49:13,880
That this was our little neural
net that's in the top right

806
00:49:13,880 --> 00:49:17,630
and here's the corresponding
computation graph of computing

807
00:49:17,630 --> 00:49:24,230
Wx plus b put it through
the sigmoid non-linearity f

808
00:49:24,230 --> 00:49:27,320
multiply the resulting dot
product with the resulting

809
00:49:27,320 --> 00:49:32,850
vector with u, gives
us our output score s.

810
00:49:32,850 --> 00:49:36,620
OK, so what we do
to compute this

811
00:49:36,620 --> 00:49:40,510
is we pass along the edges
the results of operations.

812
00:49:40,510 --> 00:49:45,410
So this is Wx then z then
h and then our output is s.

813
00:49:45,410 --> 00:49:47,870
And so the first
thing we want to be

814
00:49:47,870 --> 00:49:50,600
able to do to compute
with neural networks

815
00:49:50,600 --> 00:49:54,290
is to be able to compute
four different inputs, what

816
00:49:54,290 --> 00:49:55,730
the output is.

817
00:49:55,730 --> 00:49:59,690
And so that's referred to
as forward propagation.

818
00:49:59,690 --> 00:50:05,510
And so we simply run
this expression much

819
00:50:05,510 --> 00:50:09,500
like you standardly
do in a compiler

820
00:50:09,500 --> 00:50:11,900
to compute the value
of s and that's

821
00:50:11,900 --> 00:50:14,300
the forward propagation phase.

822
00:50:14,300 --> 00:50:18,350
But the essential additional
element of neural networks

823
00:50:18,350 --> 00:50:22,760
is that we then also want
to be able to send back

824
00:50:22,760 --> 00:50:26,000
gradients which will
tell us how to update

825
00:50:26,000 --> 00:50:27,990
the parameters of the model.

826
00:50:27,990 --> 00:50:33,440
And so it's this ability to send
back gradients which gives us

827
00:50:33,440 --> 00:50:38,090
the ability for these models
to learn once we have a loss

828
00:50:38,090 --> 00:50:40,310
function at the
end, we can work out

829
00:50:40,310 --> 00:50:42,620
how to change the
parameters of the model

830
00:50:42,620 --> 00:50:48,020
so that they more accurately
produce the desired output,

831
00:50:48,020 --> 00:50:50,640
i.e. they minimize the loss.

832
00:50:50,640 --> 00:50:56,540
And so it's doing that part that
then is called backpropagation.

833
00:50:56,540 --> 00:51:00,860
So we then-- once we
forward propagated a value

834
00:51:00,860 --> 00:51:04,910
with our current
parameters, we then

835
00:51:04,910 --> 00:51:08,960
head backwards reversing
the direction of the arrows

836
00:51:08,960 --> 00:51:14,330
and pass along gradients down
to the different parameters

837
00:51:14,330 --> 00:51:17,720
like b, and W, and
u that we can use

838
00:51:17,720 --> 00:51:21,500
to change using stochastic
gradient descent what

839
00:51:21,500 --> 00:51:24,390
the value of b is and
what the value of W is.

840
00:51:24,390 --> 00:51:29,000
So we start off with ds
/ ds, which is just 1,

841
00:51:29,000 --> 00:51:32,450
and then we run
our backpropagation

842
00:51:32,450 --> 00:51:36,950
and we're using the sort
of same kind of composition

843
00:51:36,950 --> 00:51:38,150
of Jacobian.

844
00:51:38,150 --> 00:51:41,330
So we have ds / dh
here and ds / dz

845
00:51:41,330 --> 00:51:46,130
and we progressively pass
back those gradients.

846
00:51:46,130 --> 00:51:51,050
So we just need to work out how
to efficiently and cleanly do

847
00:51:51,050 --> 00:51:53,850
this in a computational system.

848
00:51:53,850 --> 00:51:56,990
And so let's sort of
work through again a few

849
00:51:56,990 --> 00:51:58,410
of these cases.

850
00:51:58,410 --> 00:52:04,860
So the general situation is
we have a particular node,

851
00:52:04,860 --> 00:52:10,580
so a node is where some kind of
operation like multiplication

852
00:52:10,580 --> 00:52:13,890
or a non-linearity happens.

853
00:52:13,890 --> 00:52:16,700
And so the simplest
case is that we've

854
00:52:16,700 --> 00:52:19,490
got one output and one input.

855
00:52:19,490 --> 00:52:20,730
So we'll do that first.

856
00:52:20,730 --> 00:52:23,630
So that's like h equals f of z.

857
00:52:23,630 --> 00:52:31,220
So what we have is an
upstream gradient ds / dh

858
00:52:31,220 --> 00:52:36,290
and what we want to do is
compute the downstream gradient

859
00:52:36,290 --> 00:52:39,000
of ds / dz.

860
00:52:39,000 --> 00:52:42,070
And the way we're going
to do that is say,

861
00:52:42,070 --> 00:52:46,570
well for this function
f it's a function,

862
00:52:46,570 --> 00:52:49,150
it's got a derivative
for a gradient.

863
00:52:49,150 --> 00:52:53,112
So what we want to do is work
out that local gradient dh /

864
00:52:53,112 --> 00:53:00,090
dz, and then that gives us
everything that we need to work

865
00:53:00,090 --> 00:53:03,870
out ds/ dz because that's
precisely we're going to use

866
00:53:03,870 --> 00:53:04,800
the chain rule.

867
00:53:04,800 --> 00:53:09,420
We're going to say that ds / dz
equals the product of ds / dh

868
00:53:09,420 --> 00:53:14,610
times dh / dz where this
is, again, using Jacobians.

869
00:53:14,610 --> 00:53:17,940
OK, so the general principle
that we're going to use

870
00:53:17,940 --> 00:53:21,690
is the downstream gradient
equals the upstream gradient

871
00:53:21,690 --> 00:53:24,000
times the local gradient.

872
00:53:24,000 --> 00:53:27,700
OK, sometimes it gets a
little bit more complicated.

873
00:53:27,700 --> 00:53:31,000
So we might have multiple
inputs to a function.

874
00:53:31,000 --> 00:53:36,780
So this is the matrix vector
multiply, so z equals Wx.

875
00:53:36,780 --> 00:53:39,780
OK, when there are
multiple inputs,

876
00:53:39,780 --> 00:53:45,450
we still have an upstream
gradient ds / dz,

877
00:53:45,450 --> 00:53:50,010
but what we're going to do
is work out a local gradient

878
00:53:50,010 --> 00:53:52,330
with respect to each input.

879
00:53:52,330 --> 00:53:56,040
So we have dz / dw and dz / dx.

880
00:53:56,040 --> 00:53:59,730
And so then at that point,
it's exactly the same

881
00:53:59,730 --> 00:54:01,200
for each piece of it.

882
00:54:01,200 --> 00:54:04,980
We're going to work out the
downstream gradients ds / dW

883
00:54:04,980 --> 00:54:09,000
and ds / dx by using the
chain rule with respect

884
00:54:09,000 --> 00:54:14,250
to the particular
local gradient.

885
00:54:14,250 --> 00:54:17,480
So let's go through
an example of this.

886
00:54:17,480 --> 00:54:20,690
I mean, this is kind
of a silly example,

887
00:54:20,690 --> 00:54:24,590
it's not really an example that
looks like a typical neural net

888
00:54:24,590 --> 00:54:26,690
but it's sort of a
simple example where

889
00:54:26,690 --> 00:54:29,760
we can show some of the
components of what we do.

890
00:54:29,760 --> 00:54:31,780
So what we're going
to do is we're

891
00:54:31,780 --> 00:54:36,260
going to calculate
f of xyz, which

892
00:54:36,260 --> 00:54:42,560
is being calculated as x plus
y times the max of y and z.

893
00:54:42,560 --> 00:54:45,980
And we've got particular
values that we're

894
00:54:45,980 --> 00:54:50,480
starting off with x equals 1,
y equals 2, and z equals 0.

895
00:54:50,480 --> 00:54:54,470
So these are the current
values of our parameters.

896
00:54:54,470 --> 00:54:57,710
And so we can say,
OK, well, we want

897
00:54:57,710 --> 00:55:00,830
to build an expression
tree for that.

898
00:55:00,830 --> 00:55:03,140
Here's our expression tree.

899
00:55:03,140 --> 00:55:07,640
We're taking x plus y, we're
taking the max of y and z,

900
00:55:07,640 --> 00:55:10,170
and then we're multiplying them.

901
00:55:10,170 --> 00:55:15,210
And so our forward propagation
phase is just to run this.

902
00:55:15,210 --> 00:55:18,380
So we take the values
of our parameters

903
00:55:18,380 --> 00:55:21,240
and we simply start to
compute with them, right?

904
00:55:21,240 --> 00:55:25,880
So we have 1, 2, 2, 0
and we add them as 3,

905
00:55:25,880 --> 00:55:29,510
the max is 2 we multiply
them and that gives us 6.

906
00:55:29,510 --> 00:55:32,130


907
00:55:32,130 --> 00:55:36,030
OK, so then at
that point, we then

908
00:55:36,030 --> 00:55:43,620
want to go and work out how to
do things for back propagation

909
00:55:43,620 --> 00:55:46,900
and how these back
propagation steps work.

910
00:55:46,900 --> 00:55:49,950
And so the first
part of that is sort

911
00:55:49,950 --> 00:55:53,010
of working out what our local
gradients are going to be.

912
00:55:53,010 --> 00:55:56,310


913
00:55:56,310 --> 00:56:00,210
So this is a here
and this is x and y.

914
00:56:00,210 --> 00:56:05,010
So da / dx since a equals x
plus y is just going to be 1.

915
00:56:05,010 --> 00:56:10,590
And da / dy is
also going to be 1.

916
00:56:10,590 --> 00:56:19,330
Then for b equals the max of
yz, so this is this max node,

917
00:56:19,330 --> 00:56:25,070
So the local gradients for that
is it's going to depend on y--

918
00:56:25,070 --> 00:56:27,110
where the y is greater than z.

919
00:56:27,110 --> 00:56:32,540
So db / dy is going
to be 1 if and only

920
00:56:32,540 --> 00:56:36,450
if y is greater than z, which
it is at our particular point

921
00:56:36,450 --> 00:56:36,950
here.

922
00:56:36,950 --> 00:56:38,210
So that's 1.

923
00:56:38,210 --> 00:56:45,260
And dbdz is going to be 1
only if z is greater than y.

924
00:56:45,260 --> 00:56:52,880
So for our particular values
here that one is going to be 0.

925
00:56:52,880 --> 00:56:56,060
And then finally here
we are calculating

926
00:56:56,060 --> 00:56:59,240
the product f equals ab.

927
00:56:59,240 --> 00:57:06,440
So for that, we're going to--

928
00:57:06,440 --> 00:57:09,630
wait, sorry, that slide
is a little imperfect.

929
00:57:09,630 --> 00:57:14,210
OK, so for the product, the
derivative f with respect to a

930
00:57:14,210 --> 00:57:17,300
is equal to b, which is
2, and the derivative

931
00:57:17,300 --> 00:57:20,450
of f with respect
to b is a equals 3.

932
00:57:20,450 --> 00:57:25,220
So that gives us all of the
local gradients at each node.

933
00:57:25,220 --> 00:57:28,250
And so then to run
back propagation,

934
00:57:28,250 --> 00:57:32,090
we start with df /
df, which is just 1,

935
00:57:32,090 --> 00:57:38,360
and then we're going to work
out that the downstream equals

936
00:57:38,360 --> 00:57:40,730
the upstream times the local.

937
00:57:40,730 --> 00:57:43,280
OK, so the local--

938
00:57:43,280 --> 00:57:46,220
so when you have a
product like this,

939
00:57:46,220 --> 00:57:49,520
note that sort of
the gradients flip.

940
00:57:49,520 --> 00:57:55,840
So we take upstream times
the local, which is 2, oops.

941
00:57:55,840 --> 00:57:59,580


942
00:57:59,580 --> 00:58:09,470
So the downstream is 2,
on this side df / db is 3.

943
00:58:09,470 --> 00:58:13,910
So we're taking upstream
times local, that gives us 3,

944
00:58:13,910 --> 00:58:15,590
and so that gives us--

945
00:58:15,590 --> 00:58:20,480
that propagates values to
the plus and max nodes.

946
00:58:20,480 --> 00:58:22,520
And so then we continue along.

947
00:58:22,520 --> 00:58:30,020
So for the max node, the local
gradient db / dy equals 1.

948
00:58:30,020 --> 00:58:33,290
So we're going to
take upstream is 3,

949
00:58:33,290 --> 00:58:39,800
so it only takes 3 times
1 and that gives us 3.

950
00:58:39,800 --> 00:58:45,050
db / dz is 0 because of the fact
that z value is not the max.

951
00:58:45,050 --> 00:58:50,060
So we're taking 3 times 0 and
saying the gradient there is 0.

952
00:58:50,060 --> 00:58:53,240
So finally, during
the plus node,

953
00:58:53,240 --> 00:58:57,710
the local gradients for
both x and y there are 1.

954
00:58:57,710 --> 00:59:00,890
So we're just getting
2 times 1 in both cases

955
00:59:00,890 --> 00:59:04,400
and we're saying that the
gradients there are 2.

956
00:59:04,400 --> 00:59:08,340
OK, and so again at
the end of the day,

957
00:59:08,340 --> 00:59:13,520
the interpretation here is that
this is giving us information

958
00:59:13,520 --> 00:59:18,110
as to if we wiggle the
values of x, y, and z,

959
00:59:18,110 --> 00:59:21,490
how much of a difference
does it make to the output?

960
00:59:21,490 --> 00:59:26,640
What is the slope, the gradient
with respect to the variable?

961
00:59:26,640 --> 00:59:35,150
So what we've seen is that since
z isn't the max of y and z,

962
00:59:35,150 --> 00:59:40,190
if I change the value of z a
little, like I make the z 0.1

963
00:59:40,190 --> 00:59:42,920
or minus 0.1, it
makes no difference

964
00:59:42,920 --> 00:59:45,510
at all to what I
compute as the output.

965
00:59:45,510 --> 00:59:49,520
So therefore, the
gradient there is 0.

966
00:59:49,520 --> 00:59:55,520
If I change the
value of x a little,

967
00:59:55,520 --> 00:59:58,670
then that is going
to have an effect.

968
00:59:58,670 --> 01:00:04,580
And it's going to affect
the output by twice as much

969
01:00:04,580 --> 01:00:06,290
as the amount I change it.

970
01:00:06,290 --> 01:00:12,350


971
01:00:12,350 --> 01:00:19,310
Right, so and that's because
the df / dz equals 2.

972
01:00:19,310 --> 01:00:26,460
So interestingly, so I mean we
can basically work that out.

973
01:00:26,460 --> 01:00:31,650
So if we imagine
making sort of x 2.1,

974
01:00:31,650 --> 01:00:37,930
well, then what we'd
calculate the max is 2--

975
01:00:37,930 --> 01:00:44,290
I'm so sorry, if we make x 1.1,
we then get the max here is 2,

976
01:00:44,290 --> 01:00:50,710
and we get 1.1 plus 2 is
3.1, so we get 3.1 times

977
01:00:50,710 --> 01:00:54,380
2 so they'd be about 6.2.

978
01:00:54,380 --> 01:01:00,820
So changing x by 0.1 has
added 0.2 to the value of f.

979
01:01:00,820 --> 01:01:07,000
Conversely, for the value of
y, we find that df dy equals 5.

980
01:01:07,000 --> 01:01:10,960
So, what we do when we've got
two things coming out here,

981
01:01:10,960 --> 01:01:13,060
as I'll go through
again in a moment,

982
01:01:13,060 --> 01:01:14,810
is we're summing the gradient.

983
01:01:14,810 --> 01:01:17,230
So, again, 3 plus 2 equals 5.

984
01:01:17,230 --> 01:01:19,360
And empirically
that's what happens.

985
01:01:19,360 --> 01:01:22,900
So, if we consider fiddling
the value of y a little,

986
01:01:22,900 --> 01:01:27,580
let's say we make
it a value of 2.1,

987
01:01:27,580 --> 01:01:30,700
then the prediction is
they'll have 5 times

988
01:01:30,700 --> 01:01:34,330
as big an effect on the
output value that we compute.

989
01:01:34,330 --> 01:01:36,130
And, well, what do we compute?

990
01:01:36,130 --> 01:01:39,680
So, we compute 1 plus 2.1.

991
01:01:39,680 --> 01:01:41,620
So that's 3.1.

992
01:01:41,620 --> 01:01:47,660
Then we compute the max
of 2.1 and 0, is 2.1.

993
01:01:47,660 --> 01:01:51,580
So we'll take the
product of 2.1 and 3.1.

994
01:01:51,580 --> 01:01:54,220
And I calculate that in
advance, since I can't really

995
01:01:54,220 --> 01:01:55,960
do this arithmetic in my head.

996
01:01:55,960 --> 01:01:59,230
And the product of
those two is 6.51.

997
01:01:59,230 --> 01:02:03,050
So it has gone up about by 0.5.

998
01:02:03,050 --> 01:02:08,260
So we've multiplied by
fiddling it by 0.1, by 5 times

999
01:02:08,260 --> 01:02:12,040
to work out the magnitude
of the effect on the output.

1000
01:02:12,040 --> 01:02:12,940
OK.

1001
01:02:12,940 --> 01:02:15,070
So for this--

1002
01:02:15,070 --> 01:02:24,820
Before I did the case of when
we had one in and one out here,

1003
01:02:24,820 --> 01:02:31,300
and multiple ins
and one out here.

1004
01:02:31,300 --> 01:02:35,200
The case that I hadn't
actually dealt with

1005
01:02:35,200 --> 01:02:40,120
is the case of when you have
multiple outward branches, that

1006
01:02:40,120 --> 01:02:43,510
then turned up in
the computation of y.

1007
01:02:43,510 --> 01:02:47,410
So, once you have multiple
outward branches, what

1008
01:02:47,410 --> 01:02:50,380
you're doing is you're summing.

1009
01:02:50,380 --> 01:02:58,840
So that when you want
to work out the df dy,

1010
01:02:58,840 --> 01:03:02,500
you've got a local
gradient, you've

1011
01:03:02,500 --> 01:03:07,330
got two upstream
gradients, and you're

1012
01:03:07,330 --> 01:03:10,390
working it out with
respect to each of them

1013
01:03:10,390 --> 01:03:12,050
as in the chain rule.

1014
01:03:12,050 --> 01:03:14,050
And then you're
summing them together

1015
01:03:14,050 --> 01:03:16,650
to work out the
impact at the end.

1016
01:03:16,650 --> 01:03:20,960


1017
01:03:20,960 --> 01:03:24,860
So, we also saw some of the
other node intuitions, which

1018
01:03:24,860 --> 01:03:28,320
it's useful to have doing this.

1019
01:03:28,320 --> 01:03:32,300
So when you have
an addition, that

1020
01:03:32,300 --> 01:03:35,210
distributes the
upstream gradient

1021
01:03:35,210 --> 01:03:39,340
to each of the things below it.

1022
01:03:39,340 --> 01:03:42,060
When you have max, it's
like a routing node.

1023
01:03:42,060 --> 01:03:45,460
So when you have max, you
have the upstream gradient

1024
01:03:45,460 --> 01:03:48,760
and it goes to one of
the branches below it,

1025
01:03:48,760 --> 01:03:50,500
and the rest of them
get no gradient.

1026
01:03:50,500 --> 01:03:53,620


1027
01:03:53,620 --> 01:03:56,830
When you then have
a multiplication,

1028
01:03:56,830 --> 01:04:00,480
it has this effect of
switching the gradient.

1029
01:04:00,480 --> 01:04:07,120
So, if you're taking 3 by 2,
the gradient on the 2 side

1030
01:04:07,120 --> 01:04:09,760
is 3 and on the 3 side is 2.

1031
01:04:09,760 --> 01:04:13,450
And if you think about it in
terms of how much effect you

1032
01:04:13,450 --> 01:04:16,840
get from when you're doing this
sort of wiggling, that totally

1033
01:04:16,840 --> 01:04:17,860
makes sense, right?

1034
01:04:17,860 --> 01:04:21,250
Because if you're multiplying
another number by 3,

1035
01:04:21,250 --> 01:04:25,810
then any change here is
going to be multiplied by 3,

1036
01:04:25,810 --> 01:04:26,705
and vice versa.

1037
01:04:26,705 --> 01:04:30,120


1038
01:04:30,120 --> 01:04:32,070
OK.

1039
01:04:32,070 --> 01:04:34,860
So, this is the kind
of computation graph

1040
01:04:34,860 --> 01:04:41,010
that we want to use to work
out derivatives in an automated

1041
01:04:41,010 --> 01:04:44,610
computational fashion,
which is the basis

1042
01:04:44,610 --> 01:04:47,160
of the back
propagation algorithm.

1043
01:04:47,160 --> 01:04:50,770
But at that point, this
is what we're doing,

1044
01:04:50,770 --> 01:04:54,300
but there's still one
mistake that we can make.

1045
01:04:54,300 --> 01:04:58,120
It would be wrong for us to sort
of say, OK, well, first of all,

1046
01:04:58,120 --> 01:05:00,640
we want to work out ds / db.

1047
01:05:00,640 --> 01:05:03,150
So, look, we can start up here.

1048
01:05:03,150 --> 01:05:07,560
We can propagate
our upstream errors,

1049
01:05:07,560 --> 01:05:09,450
work out local gradients.

1050
01:05:09,450 --> 01:05:13,650
Upstream our local gradient
and keep all the way down,

1051
01:05:13,650 --> 01:05:19,030
and get the ds / db down here.

1052
01:05:19,030 --> 01:05:19,810
OK.

1053
01:05:19,810 --> 01:05:23,800
Next we want to
do it for ds / dw.

1054
01:05:23,800 --> 01:05:26,230
Let's just run it
all over again.

1055
01:05:26,230 --> 01:05:30,670
Because if we did that, we'd
be doing repeated computation,

1056
01:05:30,670 --> 01:05:32,740
as I showed in the first half.

1057
01:05:32,740 --> 01:05:36,220
That this term is
the same both times.

1058
01:05:36,220 --> 01:05:38,530
This term is the
same both times.

1059
01:05:38,530 --> 01:05:40,750
This term is the
same both times.

1060
01:05:40,750 --> 01:05:43,570
That only the bits
at the end differ.

1061
01:05:43,570 --> 01:05:48,730
So, what we want to do is
avoid duplicated computation

1062
01:05:48,730 --> 01:05:53,440
and compute all the
gradients that we're

1063
01:05:53,440 --> 01:05:58,660
going to need successively,
so that we only do them once.

1064
01:05:58,660 --> 01:06:00,520
And so that was
analogous to when

1065
01:06:00,520 --> 01:06:05,200
I introduced this delta variable
when we computed gradients

1066
01:06:05,200 --> 01:06:06,190
by hand.

1067
01:06:06,190 --> 01:06:08,350
So, starting off here from--

1068
01:06:08,350 --> 01:06:12,540


1069
01:06:12,540 --> 01:06:15,930
Starting off here
with ds / ds is 1.

1070
01:06:15,930 --> 01:06:22,680
We then want to one time compute
gradient in the green here.

1071
01:06:22,680 --> 01:06:25,770
One time compute the
gradient in green here.

1072
01:06:25,770 --> 01:06:28,020
That's all common work.

1073
01:06:28,020 --> 01:06:35,640
Then we're going to take the
local gradient for dz / db

1074
01:06:35,640 --> 01:06:38,265
and multiply that by
the upstream gradient

1075
01:06:38,265 --> 01:06:40,620
to have worked out ds / db.

1076
01:06:40,620 --> 01:06:44,070
And then we're going to take
the same upstream gradient,

1077
01:06:44,070 --> 01:06:51,080
and then work out the
local gradient here.

1078
01:06:51,080 --> 01:06:55,810
And then sort of propagate
that down to give us ds / dw.

1079
01:06:55,810 --> 01:07:00,400
So, the end result is, we
want to sort of systematically

1080
01:07:00,400 --> 01:07:03,280
work to forward
computation forward

1081
01:07:03,280 --> 01:07:06,670
in the graph, and
backward computation,

1082
01:07:06,670 --> 01:07:10,000
back propagation backward
in the graph in a way

1083
01:07:10,000 --> 01:07:13,190
that we do things efficiently.

1084
01:07:13,190 --> 01:07:17,710
So this is the general
form of the algorithm,

1085
01:07:17,710 --> 01:07:23,780
which works for an
arbitrary computation graph.

1086
01:07:23,780 --> 01:07:30,830
So, at the end of the day, we've
got a single scalar output, z.

1087
01:07:30,830 --> 01:07:38,230
And then we have inputs and
parameters, which compute z.

1088
01:07:38,230 --> 01:07:41,590
And so once we have
this computation graph--

1089
01:07:41,590 --> 01:07:45,040
And I added in this
funky extra arrow

1090
01:07:45,040 --> 01:07:48,520
here to make it a more
general computation graph.

1091
01:07:48,520 --> 01:07:52,450
Well, we can always say
that we can work out

1092
01:07:52,450 --> 01:07:56,020
a starting point, something
that doesn't depend on anything.

1093
01:07:56,020 --> 01:07:59,830
So in this case, both of
these bottom two nodes

1094
01:07:59,830 --> 01:08:01,840
don't depend on anything else.

1095
01:08:01,840 --> 01:08:04,420
So we can start
with them, and we

1096
01:08:04,420 --> 01:08:06,130
can start to compute forward.

1097
01:08:06,130 --> 01:08:10,450
We can compute values for
all of these second row

1098
01:08:10,450 --> 01:08:11,920
from the bottom nodes.

1099
01:08:11,920 --> 01:08:16,840
And then we're able to
compute the third ones up.

1100
01:08:16,840 --> 01:08:20,350
So, we can have a
topological sort

1101
01:08:20,350 --> 01:08:22,840
of the nodes based
on the dependencies

1102
01:08:22,840 --> 01:08:24,520
in this directed graph.

1103
01:08:24,520 --> 01:08:28,300
And we can compute the
value of each node,

1104
01:08:28,300 --> 01:08:31,810
given some subset
of its predecessors

1105
01:08:31,810 --> 01:08:33,310
which it depends on.

1106
01:08:33,310 --> 01:08:36,729
And so doing that is referred
to as the forward propagation

1107
01:08:36,729 --> 01:08:40,540
phase, and gives us a
computation of the scalar

1108
01:08:40,540 --> 01:08:43,870
output, z, using our
current parameters

1109
01:08:43,870 --> 01:08:46,000
and our current inputs.

1110
01:08:46,000 --> 01:08:50,290
And so then after that,
we run back propagation.

1111
01:08:50,290 --> 01:08:55,104
So for back propagation, we
initialize the output gradient,

1112
01:08:55,104 --> 01:08:57,849
dz / dz, as 1.

1113
01:08:57,850 --> 01:09:02,590
And then we visit nodes
in the reverse order

1114
01:09:02,590 --> 01:09:07,810
of the topological sort, and we
compute the gradients downward.

1115
01:09:07,810 --> 01:09:12,890
And so our recipe is that for
each node as we head down,

1116
01:09:12,890 --> 01:09:16,420
we're going to
compute the gradient

1117
01:09:16,420 --> 01:09:21,310
of the node with respect to
its successors the things

1118
01:09:21,310 --> 01:09:22,840
that it feeds into.

1119
01:09:22,840 --> 01:09:28,510
And how we compute that gradient
is using this chain rule

1120
01:09:28,510 --> 01:09:29,450
that we've looked at.

1121
01:09:29,450 --> 01:09:32,319
So this is sort of the
generalized form of the chain

1122
01:09:32,319 --> 01:09:35,689
rule where we have
multiple outputs.

1123
01:09:35,689 --> 01:09:38,120
And so we're summing over
the different outputs.

1124
01:09:38,120 --> 01:09:40,720
And then for each
output, we're computing

1125
01:09:40,720 --> 01:09:43,029
the product of the
upstream gradient

1126
01:09:43,029 --> 01:09:46,930
and the local gradient
with respect to that node.

1127
01:09:46,930 --> 01:09:49,060
And so we head downwards.

1128
01:09:49,060 --> 01:09:53,740
And we continue down the
reverse topological sort order,

1129
01:09:53,740 --> 01:09:57,460
and we work out the
gradient with respect

1130
01:09:57,460 --> 01:10:01,620
to each variable in this graph.

1131
01:10:01,620 --> 01:10:07,370
And so it hopefully
looks kind of intuitive

1132
01:10:07,370 --> 01:10:08,420
looking at this picture.

1133
01:10:08,420 --> 01:10:12,090


1134
01:10:12,090 --> 01:10:15,990
If you think of it like
this, the big O complexity

1135
01:10:15,990 --> 01:10:18,960
of forward propagation
and backward propagation

1136
01:10:18,960 --> 01:10:20,640
is the same, right?

1137
01:10:20,640 --> 01:10:23,970
In both cases, you're
doing a linear path

1138
01:10:23,970 --> 01:10:27,420
through all of these
nodes and calculating

1139
01:10:27,420 --> 01:10:32,790
values given predecessors, and
then values given successors.

1140
01:10:32,790 --> 01:10:36,930
I mean, you have to do a little
bit more work for working out

1141
01:10:36,930 --> 01:10:40,290
the gradients sort of as
shown by this chain rule,

1142
01:10:40,290 --> 01:10:43,120
but it's the same
big O complexity.

1143
01:10:43,120 --> 01:10:45,390
So, if somehow you're
implementing stuff for yourself

1144
01:10:45,390 --> 01:10:47,640
rather than relying
on the software,

1145
01:10:47,640 --> 01:10:51,060
and you're calculating the
gradients of a different order

1146
01:10:51,060 --> 01:10:54,090
of complexity of
forward propagation,

1147
01:10:54,090 --> 01:10:56,450
it means that you're
doing something wrong.

1148
01:10:56,450 --> 01:10:59,500
You're doing repeated work
that you shouldn't have to do.

1149
01:10:59,500 --> 01:11:00,000
OK.

1150
01:11:00,000 --> 01:11:05,640
So, this algorithm works
for a completely arbitrary

1151
01:11:05,640 --> 01:11:06,960
computation graph.

1152
01:11:06,960 --> 01:11:12,360
Any directed acyclic graph
you can apply this algorithm.

1153
01:11:12,360 --> 01:11:17,310
In general, what we find is that
we build neural networks that

1154
01:11:17,310 --> 01:11:19,120
have a regular layer structure.

1155
01:11:19,120 --> 01:11:21,780
So we have things like
a vector of inputs,

1156
01:11:21,780 --> 01:11:24,370
and then that's
multiplied by a matrix.

1157
01:11:24,370 --> 01:11:27,570
It's transformed into
another vector, which

1158
01:11:27,570 --> 01:11:29,880
might be multiplied
by another matrix,

1159
01:11:29,880 --> 01:11:32,370
or summed with another
matrix or something, right?

1160
01:11:32,370 --> 01:11:36,150
So, once we're using that kind
of regular layer structure,

1161
01:11:36,150 --> 01:11:39,060
we can then parallelize
the computation

1162
01:11:39,060 --> 01:11:46,470
by working out the gradients in
terms of Jacobians of vectors

1163
01:11:46,470 --> 01:11:51,370
and matrices, and do things in
parallel much more efficiently.

1164
01:11:51,370 --> 01:11:52,510
OK.

1165
01:11:52,510 --> 01:11:56,320
So, doing this is then
referred to as automatic

1166
01:11:56,320 --> 01:11:57,910
differentiation.

1167
01:11:57,910 --> 01:12:04,180
And so, essentially, if you
know the computation graph,

1168
01:12:04,180 --> 01:12:08,350
you should be able to have
your clever computer system

1169
01:12:08,350 --> 01:12:14,630
work out what the
derivatives of everything is,

1170
01:12:14,630 --> 01:12:19,730
and then apply back
propagation to work out

1171
01:12:19,730 --> 01:12:22,550
how to update the
parameters and learn.

1172
01:12:22,550 --> 01:12:25,320
And there's actually a
sort of an interesting sort

1173
01:12:25,320 --> 01:12:33,020
of thing of how history has gone
backwards here, which I'll just

1174
01:12:33,020 --> 01:12:35,150
note.

1175
01:12:35,150 --> 01:12:38,060
So, some of you
might be familiar

1176
01:12:38,060 --> 01:12:42,690
with symbolic
computation packages,

1177
01:12:42,690 --> 01:12:44,970
so those are things
like Mathematica.

1178
01:12:44,970 --> 01:12:49,560
So Mathematica, you can
give it a symbolic form

1179
01:12:49,560 --> 01:12:52,740
of a computation, and
then it can work out

1180
01:12:52,740 --> 01:12:54,490
derivatives for you.

1181
01:12:54,490 --> 01:12:56,730
So it should be the
case that if you

1182
01:12:56,730 --> 01:13:01,710
give a complete symbolic form
of a computation graph, then

1183
01:13:01,710 --> 01:13:05,140
it should be able to work out
all the derivatives for you.

1184
01:13:05,140 --> 01:13:09,660
And you never have to work out a
derivative by hand, whatsoever.

1185
01:13:09,660 --> 01:13:14,100
And that was actually attempted
in a famous deep learning

1186
01:13:14,100 --> 01:13:16,410
library called
Theano, which came out

1187
01:13:16,410 --> 01:13:20,970
of Yoshua Bengio's group at
the University of Montreal.

1188
01:13:20,970 --> 01:13:23,670
That it had a compiler
that did that kind

1189
01:13:23,670 --> 01:13:26,520
of symbolic manipulation.

1190
01:13:26,520 --> 01:13:33,410
But somehow, that
proved a little bit

1191
01:13:33,410 --> 01:13:35,580
too hard a road to follow.

1192
01:13:35,580 --> 01:13:38,650
I imagine that it actually might
come back again in the future.

1193
01:13:38,650 --> 01:13:42,780
And so, for modern deep
learning frameworks,

1194
01:13:42,780 --> 01:13:46,560
which includes both
TensorFlow, or PyTorch,

1195
01:13:46,560 --> 01:13:54,030
they do 90% of this computation
of automatic differentiation

1196
01:13:54,030 --> 01:13:57,870
for you, but they don't
actually symbolically compute

1197
01:13:57,870 --> 01:13:59,070
derivatives.

1198
01:13:59,070 --> 01:14:05,040
So, for each particular node
or layer of your deep learning

1199
01:14:05,040 --> 01:14:09,300
system, somebody,
either you or the person

1200
01:14:09,300 --> 01:14:15,240
who wrote that layer,
has handwritten

1201
01:14:15,240 --> 01:14:17,070
the local derivatives.

1202
01:14:17,070 --> 01:14:20,370
But then everything
from that point on,

1203
01:14:20,370 --> 01:14:23,220
the sort of the taking,
doing the chain rule

1204
01:14:23,220 --> 01:14:27,330
of combining upstream
gradients with local gradients

1205
01:14:27,330 --> 01:14:29,820
to work out downstream
gradients, that's then

1206
01:14:29,820 --> 01:14:33,210
all been done automatically
for back propagation

1207
01:14:33,210 --> 01:14:36,020
on the computation graph.

1208
01:14:36,020 --> 01:14:40,730
And so what that means is,
for a whole neural network,

1209
01:14:40,730 --> 01:14:43,150
you have a computation
graph, and it's

1210
01:14:43,150 --> 01:14:47,590
going to have a forward
pass and a backward pass.

1211
01:14:47,590 --> 01:14:50,360
And so for the
forward pass, you're

1212
01:14:50,360 --> 01:14:54,970
topologically sorting the nodes
based on their dependencies

1213
01:14:54,970 --> 01:14:56,740
in the computation graph.

1214
01:14:56,740 --> 01:15:01,750
And then for each node,
you're running forward

1215
01:15:01,750 --> 01:15:04,480
the forward computation
on that node.

1216
01:15:04,480 --> 01:15:06,580
And then for
backward propagation,

1217
01:15:06,580 --> 01:15:09,730
you're reversing the
topological sort of the graph.

1218
01:15:09,730 --> 01:15:12,310
And then for each
node in the graph,

1219
01:15:12,310 --> 01:15:15,140
you're running the
backward propagation,

1220
01:15:15,140 --> 01:15:17,320
which is the little bit
of back prop, the chain

1221
01:15:17,320 --> 01:15:19,090
rule at that node.

1222
01:15:19,090 --> 01:15:21,340
And then the result
of doing that is you

1223
01:15:21,340 --> 01:15:28,120
have gradients for your
inputs and parameters.

1224
01:15:28,120 --> 01:15:30,810
And so, this is--

1225
01:15:30,810 --> 01:15:33,790
The overall software
runs this for you.

1226
01:15:33,790 --> 01:15:38,430
And so what you want
to do is then actually

1227
01:15:38,430 --> 01:15:43,690
have stuff for particular
nodes or layers in the graph.

1228
01:15:43,690 --> 01:15:47,010
So, if I have a
multiply gate, it's

1229
01:15:47,010 --> 01:15:49,530
going to have a forward
algorithm, which

1230
01:15:49,530 --> 01:15:53,760
just computes that the
output is x times y

1231
01:15:53,760 --> 01:15:55,650
in terms of the two inputs.

1232
01:15:55,650 --> 01:15:59,010
And then I'm going to
want to tell it also

1233
01:15:59,010 --> 01:16:01,480
how to calculate the
local derivative.

1234
01:16:01,480 --> 01:16:05,940
So I want to say, what
is the local derivative.

1235
01:16:05,940 --> 01:16:12,160
So, dl / dx and dl / dy in
terms of the upstream gradient,

1236
01:16:12,160 --> 01:16:13,780
dl / dz.

1237
01:16:13,780 --> 01:16:19,240
And so, I will then manually
work out how to calculate that.

1238
01:16:19,240 --> 01:16:22,220
And normally what
I have to do is,

1239
01:16:22,220 --> 01:16:27,120
I assume the forward
pass is being run first.

1240
01:16:27,120 --> 01:16:31,890
And I'm going to shove into some
local variables for my class

1241
01:16:31,890 --> 01:16:35,440
the values that we used in
the forward computation.

1242
01:16:35,440 --> 01:16:39,150
So as well as computing
z equals x times y,

1243
01:16:39,150 --> 01:16:43,020
I'm going to sort of
remember what x and y were.

1244
01:16:43,020 --> 01:16:47,670
So that then when I'm asked
to compute the backward pass,

1245
01:16:47,670 --> 01:16:51,090
I'm then going to
have implemented here

1246
01:16:51,090 --> 01:16:54,390
what we saw earlier of--

1247
01:16:54,390 --> 01:16:59,220
That when it's xy, you're going
to sort of swap the y and the x

1248
01:16:59,220 --> 01:17:02,400
to work out the local gradients.

1249
01:17:02,400 --> 01:17:04,620
And so then I'm going
to multiply those

1250
01:17:04,620 --> 01:17:06,090
by the upstream gradient.

1251
01:17:06,090 --> 01:17:08,070
And I'm going to return--

1252
01:17:08,070 --> 01:17:10,560
I've just written it here
as a sort of a little list,

1253
01:17:10,560 --> 01:17:16,990
but really it's going to be a
NumPy vector of the gradients.

1254
01:17:16,990 --> 01:17:18,550
OK.

1255
01:17:18,550 --> 01:17:23,530
So that's 98% of what I
wanted to cover today.

1256
01:17:23,530 --> 01:17:28,090
Just a couple of
quick comments left.

1257
01:17:28,090 --> 01:17:32,620
So, that can and should
all be automated.

1258
01:17:32,620 --> 01:17:35,530
Sometimes you want to
just check if you're

1259
01:17:35,530 --> 01:17:37,690
computing the right gradients.

1260
01:17:37,690 --> 01:17:40,030
And so the standard
way of checking

1261
01:17:40,030 --> 01:17:42,610
that you're computing
the right gradients

1262
01:17:42,610 --> 01:17:45,280
is to manually work
out the gradient

1263
01:17:45,280 --> 01:17:48,880
by doing a numeric
calculation of the gradient.

1264
01:17:48,880 --> 01:17:51,770
And so, you can do that--

1265
01:17:51,770 --> 01:17:56,720
So you can work out what the
derivative of f with respect

1266
01:17:56,720 --> 01:18:01,780
to x should be, by choosing
some sort of small number,

1267
01:18:01,780 --> 01:18:06,790
like 10 to the minus 4, adding
it to x, subtracting it from x.

1268
01:18:06,790 --> 01:18:10,000
And then so the difference
between these numbers is 2h.

1269
01:18:10,000 --> 01:18:12,160
Dividing it through by 2h.

1270
01:18:12,160 --> 01:18:14,230
And you're simply
working out the rise

1271
01:18:14,230 --> 01:18:18,070
over the run, which is the
slope at that point with respect

1272
01:18:18,070 --> 01:18:18,910
to x.

1273
01:18:18,910 --> 01:18:23,110
And that's an approximation of
the gradient of f with respect

1274
01:18:23,110 --> 01:18:26,980
to x at that value of x.

1275
01:18:26,980 --> 01:18:28,820
So, this is so simple.

1276
01:18:28,820 --> 01:18:31,060
You can't make a
mistake implementing it.

1277
01:18:31,060 --> 01:18:32,860
And so therefore,
you can use this

1278
01:18:32,860 --> 01:18:38,740
to check whether your gradient
values are correct or not.

1279
01:18:38,740 --> 01:18:41,860
This isn't something that
you'd want to use much.

1280
01:18:41,860 --> 01:18:43,570
Because not only
is it approximate,

1281
01:18:43,570 --> 01:18:45,550
but it's extremely slow.

1282
01:18:45,550 --> 01:18:47,050
Because to work
this out, you have

1283
01:18:47,050 --> 01:18:50,890
to run the forward computation
for every parameter

1284
01:18:50,890 --> 01:18:52,040
of the model.

1285
01:18:52,040 --> 01:18:54,370
So, if you have a model
with a million parameters,

1286
01:18:54,370 --> 01:18:56,770
you're now doing a
million times as much work

1287
01:18:56,770 --> 01:19:00,700
to run back prop as you
would do if you're actually

1288
01:19:00,700 --> 01:19:01,690
using calculus.

1289
01:19:01,690 --> 01:19:03,910
So, calculus is a
good thing to know.

1290
01:19:03,910 --> 01:19:07,990
But it can be really useful to
check that the right values are

1291
01:19:07,990 --> 01:19:10,300
being calculated.

1292
01:19:10,300 --> 01:19:13,180
In the old days when we
handwrote everything,

1293
01:19:13,180 --> 01:19:15,160
this was kind of
the key unit test

1294
01:19:15,160 --> 01:19:17,260
that people used everywhere.

1295
01:19:17,260 --> 01:19:18,970
These days, most
of the time you're

1296
01:19:18,970 --> 01:19:22,040
reusing layers that
are built into PyTorch,

1297
01:19:22,040 --> 01:19:23,960
or some other deep
learning framework.

1298
01:19:23,960 --> 01:19:25,900
So it's much less needed.

1299
01:19:25,900 --> 01:19:28,240
But sometimes you're
implementing your own layer,

1300
01:19:28,240 --> 01:19:30,130
and you really do want
to check that things

1301
01:19:30,130 --> 01:19:32,320
are implemented correctly.

1302
01:19:32,320 --> 01:19:34,930
There's a fine point in
the way this is written.

1303
01:19:34,930 --> 01:19:40,180
If you saw this sort of in
high school calculus class,

1304
01:19:40,180 --> 01:19:44,530
you will have seen rise
over run of f(x) plus h

1305
01:19:44,530 --> 01:19:50,290
minus f(x) divided by h.

1306
01:19:50,290 --> 01:19:54,130
It turns out that doing this
two sided estimate like this,

1307
01:19:54,130 --> 01:19:58,420
is much, much more accurate
than doing a one sided estimate.

1308
01:19:58,420 --> 01:20:01,030
And so you're really
much encouraged to use

1309
01:20:01,030 --> 01:20:03,290
this approximation.

1310
01:20:03,290 --> 01:20:03,790
OK.

1311
01:20:03,790 --> 01:20:07,180
So at that point, we've
mastered the core technology

1312
01:20:07,180 --> 01:20:08,710
of neural nets.

1313
01:20:08,710 --> 01:20:12,970
Back propagation is recursively
and hence efficiently

1314
01:20:12,970 --> 01:20:16,630
applying the chain rule
along the computation graph,

1315
01:20:16,630 --> 01:20:22,510
with this key step that
downstream gradient equals

1316
01:20:22,510 --> 01:20:25,480
upstream gradient
times local gradient.

1317
01:20:25,480 --> 01:20:28,510
And so for calculating
with neural nets,

1318
01:20:28,510 --> 01:20:30,850
we do the forward
pass to work out

1319
01:20:30,850 --> 01:20:35,500
values with current parameters,
then run back propagation

1320
01:20:35,500 --> 01:20:39,130
to work out the
gradient of the loss,

1321
01:20:39,130 --> 01:20:45,180
currently computed loss with
respect to those parameters.

1322
01:20:45,180 --> 01:20:50,000
Now, to some extent, with
modern deep learning frameworks,

1323
01:20:50,000 --> 01:20:53,380
you don't actually have to know
how to do any of this, right?

1324
01:20:53,380 --> 01:20:55,310
It's the same as you
don't have to know

1325
01:20:55,310 --> 01:20:58,250
how to implement a C compiler.

1326
01:20:58,250 --> 01:21:03,680
You can just write C code and
say gcc, and it'll compile it

1327
01:21:03,680 --> 01:21:07,430
and it'll run the
right stuff for you.

1328
01:21:07,430 --> 01:21:09,770
And that's the kind
of functionality

1329
01:21:09,770 --> 01:21:11,990
you get from the
PyTorch framework.

1330
01:21:11,990 --> 01:21:15,650
So, do come along to the
PyTorch tutorial this Friday

1331
01:21:15,650 --> 01:21:17,900
and get a sense
about how easy it

1332
01:21:17,900 --> 01:21:21,930
is to write neural networks
using a framework like PyTorch

1333
01:21:21,930 --> 01:21:23,390
or TensorFlow.

1334
01:21:23,390 --> 01:21:26,780
And it's so easy,
that's why high school

1335
01:21:26,780 --> 01:21:28,880
students across
the nation are now

1336
01:21:28,880 --> 01:21:33,020
doing their science projects
training deep learning systems.

1337
01:21:33,020 --> 01:21:35,840
Because you don't actually
have to understand

1338
01:21:35,840 --> 01:21:39,860
very much to bung a few neural
network layers together and set

1339
01:21:39,860 --> 01:21:42,620
it computing on some data.

1340
01:21:42,620 --> 01:21:45,230
But we hope in this
class that you actually

1341
01:21:45,230 --> 01:21:50,130
are also learning how these
things are implemented.

1342
01:21:50,130 --> 01:21:53,540
So you have a deeper
understanding than that.

1343
01:21:53,540 --> 01:21:55,550
And it turns out that
sometimes you need

1344
01:21:55,550 --> 01:21:57,480
to have a deeper understanding.

1345
01:21:57,480 --> 01:22:01,770
So, back propagation doesn't
always work perfectly.

1346
01:22:01,770 --> 01:22:03,890
And so understanding
what it's really doing

1347
01:22:03,890 --> 01:22:06,737
can be crucial to
debugging things.

1348
01:22:06,737 --> 01:22:08,570
And so we'll actually
see an example of that

1349
01:22:08,570 --> 01:22:11,090
fairly soon when
we start looking

1350
01:22:11,090 --> 01:22:14,490
at recurrent models and some
of the problems that they have,

1351
01:22:14,490 --> 01:22:17,780
which will require us to think
a bit more deeply about what's

1352
01:22:17,780 --> 01:22:20,580
happening in our
gradient computations.

1353
01:22:20,580 --> 01:22:22,100
OK.

1354
01:22:22,100 --> 01:22:24,190
That's it for today.

1355
01:22:24,190 --> 01:22:29,000


