1
00:00:00,000 --> 00:00:05,410


2
00:00:05,410 --> 00:00:07,280
Hello, everybody.

3
00:00:07,280 --> 00:00:10,720
Welcome to the
CS224N, lecture 10.

4
00:00:10,720 --> 00:00:12,760
This is going to be
primarily on pretraining,

5
00:00:12,760 --> 00:00:17,050
but we will also discuss
subword models a little bit,

6
00:00:17,050 --> 00:00:19,370
and review transformers.

7
00:00:19,370 --> 00:00:19,870
OK.

8
00:00:19,870 --> 00:00:24,460
So we have a lot of exciting
things to get into today,

9
00:00:24,460 --> 00:00:27,940
but some reminders
about the class.

10
00:00:27,940 --> 00:00:32,470
Assignment 5 is
being released today,

11
00:00:32,470 --> 00:00:34,400
assignment 4 was
due a minute ago,

12
00:00:34,400 --> 00:00:37,040
so if you're done with
that, congratulations.

13
00:00:37,040 --> 00:00:40,870
If not, I hope that
the late days go well.

14
00:00:40,870 --> 00:00:45,400
Assignment 5 is on
pretraining and transformers.

15
00:00:45,400 --> 00:00:48,980
So these lectures are going to
be very useful to you for that.

16
00:00:48,980 --> 00:00:53,050
And it doesn't cover anything
after these lectures.

17
00:00:53,050 --> 00:00:53,770
All right.

18
00:00:53,770 --> 00:00:58,240
So today let's kind of take
a little peek through what

19
00:00:58,240 --> 00:01:00,040
the outline will be.

20
00:01:00,040 --> 00:01:02,770
We haven't talked about subword
modeling yet, and sort of we

21
00:01:02,770 --> 00:01:03,958
should have.

22
00:01:03,958 --> 00:01:06,250
And so we're going to talk
a little bit about subwords.

23
00:01:06,250 --> 00:01:10,357
You saw these in
assignment 4, just as

24
00:01:10,357 --> 00:01:12,940
the data that we provided to you
with your machine translation

25
00:01:12,940 --> 00:01:15,640
system, but we're going to
talk a little bit about why

26
00:01:15,640 --> 00:01:17,860
they're so ubiquitous in NLP.

27
00:01:17,860 --> 00:01:21,892
Because they are used
in pretrained models.

28
00:01:21,892 --> 00:01:24,100
I mean, they're used in a
number of different models.

29
00:01:24,100 --> 00:01:26,470
But when we discuss
pretraining, it's

30
00:01:26,470 --> 00:01:29,728
important to know that
subwords are a part of it,

31
00:01:29,728 --> 00:01:31,270
then we'll sort of
motivate-- we will

32
00:01:31,270 --> 00:01:34,600
go on another journey of
motivation, of motivating model

33
00:01:34,600 --> 00:01:36,070
pretraining from
word embeddings.

34
00:01:36,070 --> 00:01:37,930
So we've already
seen pretraining

35
00:01:37,930 --> 00:01:41,200
in some sense in the very
first lecture of this course,

36
00:01:41,200 --> 00:01:44,110
because we pretrained
individual word embeddings that

37
00:01:44,110 --> 00:01:47,170
don't take into account their
contexts on very large text

38
00:01:47,170 --> 00:01:49,180
corpora and saw that
they were able to encode

39
00:01:49,180 --> 00:01:53,270
a lot of useful
things about language.

40
00:01:53,270 --> 00:01:55,480
So after we do the
motivation, we'll

41
00:01:55,480 --> 00:01:57,370
go through model
pretraining three ways.

42
00:01:57,370 --> 00:01:59,140
And we're going to
reference actually

43
00:01:59,140 --> 00:02:00,098
the lecture on Tuesday.

44
00:02:00,098 --> 00:02:01,806
So this is when we'll
review a little bit

45
00:02:01,806 --> 00:02:02,890
of the transformers stuff.

46
00:02:02,890 --> 00:02:05,468
We'll talk about model
pretraining and decoders,

47
00:02:05,468 --> 00:02:07,510
like a transformer decoder
that we saw last week,

48
00:02:07,510 --> 00:02:09,975
and encoders, and
then encoder-decoders.

49
00:02:09,975 --> 00:02:11,350
And in each of
these three cases,

50
00:02:11,350 --> 00:02:13,142
we're going to talk a
little bit about sort

51
00:02:13,142 --> 00:02:14,890
of what things you
could be doing,

52
00:02:14,890 --> 00:02:18,595
and then popular models that
are in use across research

53
00:02:18,595 --> 00:02:20,350
and in industry.

54
00:02:20,350 --> 00:02:22,240
Then we're going to
talk a little bit about,

55
00:02:22,240 --> 00:02:24,730
what do we think
pretraining is teaching?

56
00:02:24,730 --> 00:02:25,965
This going to be very brief.

57
00:02:25,965 --> 00:02:28,090
Actually a lot of the
interpretability and analysis

58
00:02:28,090 --> 00:02:30,220
lecture in two weeks
is going to talk more

59
00:02:30,220 --> 00:02:33,850
about sort of the mystery
and the scientific problem

60
00:02:33,850 --> 00:02:36,550
of figuring out what
these models are learning

61
00:02:36,550 --> 00:02:39,280
about language through
pretraining objectives,

62
00:02:39,280 --> 00:02:40,570
but we'll sort of get a peek.

63
00:02:40,570 --> 00:02:42,490
And then we'll talk
about very large models

64
00:02:42,490 --> 00:02:43,600
and in-context learning.

65
00:02:43,600 --> 00:02:47,380
So if you've heard of
GPT3, for example, we're

66
00:02:47,380 --> 00:02:49,400
going to just briefly
touch on that here,

67
00:02:49,400 --> 00:02:51,160
and I think we'll
discuss more about it

68
00:02:51,160 --> 00:02:53,510
in the course later on as well.

69
00:02:53,510 --> 00:02:54,250
OK.

70
00:02:54,250 --> 00:02:57,500
So we've got a lot to
do, let's jump right in.

71
00:02:57,500 --> 00:03:00,190
So word structure
and subword models.

72
00:03:00,190 --> 00:03:02,333
Lets think about sort
of the assumptions

73
00:03:02,333 --> 00:03:04,000
we've been making in
this course so far.

74
00:03:04,000 --> 00:03:05,500
When we give you
an assignment, when

75
00:03:05,500 --> 00:03:09,010
we talk about training
Word2vec for example,

76
00:03:09,010 --> 00:03:11,530
we made this assumption about
a language's vocabulary.

77
00:03:11,530 --> 00:03:13,210
In particular, we've
made the assumption

78
00:03:13,210 --> 00:03:15,250
that it has a fixed
vocabulary of something

79
00:03:15,250 --> 00:03:17,500
like tens of thousands,
maybe 100,000,

80
00:03:17,500 --> 00:03:19,810
I don't know, a number of yeah.

81
00:03:19,810 --> 00:03:22,810
Some relatively large, it
seems, number of words, and that

82
00:03:22,810 --> 00:03:26,560
seems sort of like pretty
good so far at least,

83
00:03:26,560 --> 00:03:27,520
and what we've done.

84
00:03:27,520 --> 00:03:29,980
And we build this
vocabulary from the set

85
00:03:29,980 --> 00:03:32,360
that we train say, Word2vec on.

86
00:03:32,360 --> 00:03:33,850
And then, here's
the crucial thing.

87
00:03:33,850 --> 00:03:36,100
Any novel word, any
word that you did not

88
00:03:36,100 --> 00:03:42,203
see at training time, is sort
of mapped to a single UNK token.

89
00:03:42,203 --> 00:03:43,870
And there are other
ways to handle this,

90
00:03:43,870 --> 00:03:46,060
but you sort of
have to do something

91
00:03:46,060 --> 00:03:49,120
and a frequent method is
to map them all to UNK.

92
00:03:49,120 --> 00:03:53,140
So let's walk through what
this sort of means in English.

93
00:03:53,140 --> 00:03:56,050
You learn in embeddings,
you map them, it all works.

94
00:03:56,050 --> 00:03:59,620
Then you have a variation
on a word, like "taaaaasty."

95
00:03:59,620 --> 00:04:01,443
With a bunch of As.

96
00:04:01,443 --> 00:04:03,110
Right, and your model
isn't smart enough

97
00:04:03,110 --> 00:04:07,760
to that sort of means
like very tasty maybe.

98
00:04:07,760 --> 00:04:09,740
And so it maps it
to UNK because it's

99
00:04:09,740 --> 00:04:11,930
just a dictionary look up miss.

100
00:04:11,930 --> 00:04:15,740
And then, you have a typo
like "laern," and that maps

101
00:04:15,740 --> 00:04:19,339
to UNK as well potentially, if
it wasn't in your training set.

102
00:04:19,339 --> 00:04:21,829
Some people make typos
but not all of them

103
00:04:21,829 --> 00:04:23,173
will be seen at training time.

104
00:04:23,173 --> 00:04:24,840
And then you'll have
novel items, right?

105
00:04:24,840 --> 00:04:27,090
So this could be the first
time that you've ever seen,

106
00:04:27,090 --> 00:04:29,420
you as the students
in 224N have seen

107
00:04:29,420 --> 00:04:31,940
the word, "transformerify."

108
00:04:31,940 --> 00:04:34,932
But I get the feeling you
sort of have a notion of what

109
00:04:34,932 --> 00:04:36,140
it's supposed to mean, right?

110
00:04:36,140 --> 00:04:38,660
Like maybe add
transformers to, or turn

111
00:04:38,660 --> 00:04:41,120
into using transformers,
or turn into a transformer,

112
00:04:41,120 --> 00:04:42,240
or something like that.

113
00:04:42,240 --> 00:04:44,032
And this is also going
to be mapped to UNK.

114
00:04:44,032 --> 00:04:48,030
Even though you've seen
"transformer," and "ify."

115
00:04:48,030 --> 00:04:48,530
OK.

116
00:04:48,530 --> 00:04:51,920
And so somehow, the
conclusion we have to come to

117
00:04:51,920 --> 00:04:54,620
is that, looking
at words as just

118
00:04:54,620 --> 00:04:57,710
like the individual
sequences of characters

119
00:04:57,710 --> 00:04:59,722
uniquely identifies
that word, and that's

120
00:04:59,722 --> 00:05:01,430
sort of how we should
parameterize things

121
00:05:01,430 --> 00:05:04,290
is just wrong.

122
00:05:04,290 --> 00:05:08,360
And so, not only is this true in
English, but in many languages,

123
00:05:08,360 --> 00:05:09,740
this finite
vocabulary assumption

124
00:05:09,740 --> 00:05:10,880
makes even less sense.

125
00:05:10,880 --> 00:05:12,980
So already it doesn't
make sense in English.

126
00:05:12,980 --> 00:05:16,550
But English is-- it's not
the worst for English.

127
00:05:16,550 --> 00:05:21,860
So morphology is the study
of the structure of words.

128
00:05:21,860 --> 00:05:24,590
And English is known to have
pretty simple morphology

129
00:05:24,590 --> 00:05:27,980
in kind of specific ways.

130
00:05:27,980 --> 00:05:31,340
And when languages have
complex morphology,

131
00:05:31,340 --> 00:05:34,610
it means you have longer
words, more complex words that

132
00:05:34,610 --> 00:05:38,417
get modified more, and each one
of them occurs less frequently.

133
00:05:38,417 --> 00:05:40,250
And that should sound
like a problem, right?

134
00:05:40,250 --> 00:05:41,920
If a word occurs
less frequently,

135
00:05:41,920 --> 00:05:46,310
it will be less likely to
show up in your training set,

136
00:05:46,310 --> 00:05:48,230
and maybe it'll show
up in your test set,

137
00:05:48,230 --> 00:05:49,355
never in your training set.

138
00:05:49,355 --> 00:05:51,810
Now it's mapped to UNK, and
you don't know what to do.

139
00:05:51,810 --> 00:05:56,240
So an example, Swahili verbs can
have hundreds of conjugations.

140
00:05:56,240 --> 00:06:01,010
So each conjugation encodes
important information

141
00:06:01,010 --> 00:06:03,830
about the sentence that in
English might be represented

142
00:06:03,830 --> 00:06:05,630
through say, more words.

143
00:06:05,630 --> 00:06:09,875
But in Swahili it's mapped
onto the verb as prefixes

144
00:06:09,875 --> 00:06:11,000
and suffixes, and the like.

145
00:06:11,000 --> 00:06:12,950
This is called
inflectional morphology.

146
00:06:12,950 --> 00:06:14,825
And so you can have
hundreds of conjugations.

147
00:06:14,825 --> 00:06:18,290
I have just sort of pasted
this Wiktionary block just

148
00:06:18,290 --> 00:06:21,080
to give you a small sample
of just the huge number

149
00:06:21,080 --> 00:06:22,970
of conjugations there
are-- and so trying

150
00:06:22,970 --> 00:06:24,890
to memorize
independently a meaning

151
00:06:24,890 --> 00:06:27,460
of each one of these words
is just not the right answer.

152
00:06:27,460 --> 00:06:31,980


153
00:06:31,980 --> 00:06:34,670
So this is going to be
a very brief overview.

154
00:06:34,670 --> 00:06:36,200
And so what we're
going to do, is

155
00:06:36,200 --> 00:06:43,520
take one, let's say a class of
algorithms for subword modeling

156
00:06:43,520 --> 00:06:46,700
that have been kind
of developed to try

157
00:06:46,700 --> 00:06:50,670
to take a middle ground
between two options.

158
00:06:50,670 --> 00:06:54,860
One option is saying, everything
is just like individual words--

159
00:06:54,860 --> 00:06:57,170
either I know the word and
I saw it at training time,

160
00:06:57,170 --> 00:06:59,210
or I don't know the
word and it's like UNK.

161
00:06:59,210 --> 00:07:01,820
And then sort of another
extreme option is to say,

162
00:07:01,820 --> 00:07:03,680
it's just characters.

163
00:07:03,680 --> 00:07:04,180
Right?

164
00:07:04,180 --> 00:07:06,050
So like I get the
sequence of characters,

165
00:07:06,050 --> 00:07:07,700
and then my neural
network on top

166
00:07:07,700 --> 00:07:10,220
of my sequence of
characters has to learn

167
00:07:10,220 --> 00:07:13,800
everything-- has to learn how
to combine words and stuff.

168
00:07:13,800 --> 00:07:15,855
So subword models
in general just

169
00:07:15,855 --> 00:07:18,230
means looking at the sort of
internal structure of words,

170
00:07:18,230 --> 00:07:20,360
somehow looking
below the word level.

171
00:07:20,360 --> 00:07:23,150
But this group of
models is going

172
00:07:23,150 --> 00:07:24,980
to try to meet a middle ground.

173
00:07:24,980 --> 00:07:28,280
So byte-pair encoding.

174
00:07:28,280 --> 00:07:30,410
What we're going to
do is we're going

175
00:07:30,410 --> 00:07:33,782
to learn a vocabulary from
a training data set again.

176
00:07:33,782 --> 00:07:36,240
So now we have a training data
set, instead of just saying,

177
00:07:36,240 --> 00:07:39,740
oh, everything that was split
by my heuristic word splitter

178
00:07:39,740 --> 00:07:42,570
like spaces in
English for example,

179
00:07:42,570 --> 00:07:45,470
is going to be a word
in my vocabulary,

180
00:07:45,470 --> 00:07:48,470
we're going to learn
the vocabulary using

181
00:07:48,470 --> 00:07:50,400
a greedy algorithm in this case.

182
00:07:50,400 --> 00:07:52,550
So here's what
we're going to do.

183
00:07:52,550 --> 00:07:55,390
We start with a vocabulary
containing only characters.

184
00:07:55,390 --> 00:07:56,600
So that's our extreme, right?

185
00:07:56,600 --> 00:07:58,850
That's our-- at the
very least, if you've

186
00:07:58,850 --> 00:08:02,540
seen all the characters,
then you know that you can--

187
00:08:02,540 --> 00:08:03,800
you'll never have an UNK.

188
00:08:03,800 --> 00:08:05,810
Right? 'Cause you see a word,
you've never seen it before,

189
00:08:05,810 --> 00:08:07,460
you just split it
into its characters,

190
00:08:07,460 --> 00:08:11,580
and then you try to
deal with it that way.

191
00:08:11,580 --> 00:08:13,870
And then also an
end of word symbol.

192
00:08:13,870 --> 00:08:15,620
And then we'll iterate
over this algorithm

193
00:08:15,620 --> 00:08:17,720
we'll say, use the
corpus of text,

194
00:08:17,720 --> 00:08:20,360
find common adjacent letters--

195
00:08:20,360 --> 00:08:24,000
so maybe a and b are
very frequently adjacent.

196
00:08:24,000 --> 00:08:28,560
Add the pair of them
together as a single subword

197
00:08:28,560 --> 00:08:30,330
into your vocabulary.

198
00:08:30,330 --> 00:08:32,340
Now replace instances
of that character pair

199
00:08:32,340 --> 00:08:34,950
with the new subword, repeat
until your desired vocabulary

200
00:08:34,950 --> 00:08:35,450
size.

201
00:08:35,450 --> 00:08:39,299
So maybe you start with a
small character vocabulary,

202
00:08:39,299 --> 00:08:41,880
and then you end up with
that same small character

203
00:08:41,880 --> 00:08:46,350
vocabulary plus a bunch
of sort of entire words

204
00:08:46,350 --> 00:08:47,100
or parts of words.

205
00:08:47,100 --> 00:08:50,280
So notice how "apple," an
entire word, looks like "apple."

206
00:08:50,280 --> 00:08:53,310
But then, "app," maybe this
is sort of the first part,

207
00:08:53,310 --> 00:08:57,030
the first subword of
application or "app."

208
00:08:57,030 --> 00:09:01,650
Yeah, and then "ly,"
I guess I should

209
00:09:01,650 --> 00:09:05,950
have not put the hash there.

210
00:09:05,950 --> 00:09:08,850
But maybe you learned "ly"
as like the end of a word,

211
00:09:08,850 --> 00:09:10,540
for example.

212
00:09:10,540 --> 00:09:15,120
And so, what you end up
with is a vocabulary where

213
00:09:15,120 --> 00:09:18,010
common things, you get
to map to themselves,

214
00:09:18,010 --> 00:09:20,070
and then rare sequences
of characters,

215
00:09:20,070 --> 00:09:22,957
you kind of split as
little as possible.

216
00:09:22,957 --> 00:09:24,540
And it doesn't always
end up so nicely

217
00:09:24,540 --> 00:09:26,940
that you learn like
morphologically relevant

218
00:09:26,940 --> 00:09:32,100
suffixes like "ly," but you can
try to split things somewhat

219
00:09:32,100 --> 00:09:34,380
reasonably and if
you have enough data,

220
00:09:34,380 --> 00:09:37,140
the subword vocabulary
you learn tends to be OK.

221
00:09:37,140 --> 00:09:40,180
So this is originally used
in machine translation,

222
00:09:40,180 --> 00:09:42,840
and now a similar
method, WordPiece,

223
00:09:42,840 --> 00:09:44,640
which we won't go
over in this lecture

224
00:09:44,640 --> 00:09:47,640
is used in pretrained models,
but the idea is effectively

225
00:09:47,640 --> 00:09:49,470
the same and you end
up with vocabularies

226
00:09:49,470 --> 00:09:50,800
that look a lot like this.

227
00:09:50,800 --> 00:09:54,660
So if we go back to our--

228
00:09:54,660 --> 00:09:58,860
if we go back to our examples
of where word level NLP was

229
00:09:58,860 --> 00:10:03,960
failing us, then you
have "hat" mapping

230
00:10:03,960 --> 00:10:05,500
to "hat," OK that's good.

231
00:10:05,500 --> 00:10:07,470
We've "hat" mapping
to "hat," because that

232
00:10:07,470 --> 00:10:09,750
was a common enough
sequence of characters

233
00:10:09,750 --> 00:10:11,130
that it was actually
incorporated

234
00:10:11,130 --> 00:10:13,060
into our subword vocabulary.

235
00:10:13,060 --> 00:10:13,560
Right?

236
00:10:13,560 --> 00:10:15,435
And then you have "learn"
mapping to "learn."

237
00:10:15,435 --> 00:10:16,418
So common words, good.

238
00:10:16,418 --> 00:10:18,960
And that means that the model,
the neural network that you're

239
00:10:18,960 --> 00:10:21,240
going to process this
text with, does not

240
00:10:21,240 --> 00:10:25,380
need to say, combine the
letters of "learn" and "hat"

241
00:10:25,380 --> 00:10:29,342
in order to try to derive
the meaning of these words

242
00:10:29,342 --> 00:10:31,050
from the letters,
because you can imagine

243
00:10:31,050 --> 00:10:32,880
that might be
difficult. But then

244
00:10:32,880 --> 00:10:36,450
when you get a word that
you have not seen before,

245
00:10:36,450 --> 00:10:39,200
you are able to decompose it.

246
00:10:39,200 --> 00:10:42,140
And so, if you've seen
tasty with varying numbers

247
00:10:42,140 --> 00:10:46,460
of As at training
time, maybe you

248
00:10:46,460 --> 00:10:49,850
actually get some of the same
subwords, or similar subwords

249
00:10:49,850 --> 00:10:52,220
that you're splitting it
into at evaluation time.

250
00:10:52,220 --> 00:10:55,280
So we never saw tasty enough
to work with however many

251
00:10:55,280 --> 00:10:58,770
As in order to add it into
our subword vocabulary.

252
00:10:58,770 --> 00:11:00,728
But we're still able to
split it into things,

253
00:11:00,728 --> 00:11:02,270
and then the neural
network that runs

254
00:11:02,270 --> 00:11:04,670
on top of these
subword embeddings

255
00:11:04,670 --> 00:11:06,770
could be able to sort
of induce that oh, yeah.

256
00:11:06,770 --> 00:11:10,490
This is one of those things
where people chain letters

257
00:11:10,490 --> 00:11:15,110
together, chain vowels together
in English for emphasis.

258
00:11:15,110 --> 00:11:18,260
So misspellings still
pretty much mess you up.

259
00:11:18,260 --> 00:11:20,990
So now, "laern,"
this misspelling

260
00:11:20,990 --> 00:11:22,700
might be mapped to two subwords.

261
00:11:22,700 --> 00:11:25,490
But if you saw misspellings
like this frequently enough,

262
00:11:25,490 --> 00:11:28,560
maybe you could learn
sort of to handle it.

263
00:11:28,560 --> 00:11:30,080
It still messes up
the model though.

264
00:11:30,080 --> 00:11:33,770
And-- but at the very least,
it's not just an UNK, right?

265
00:11:33,770 --> 00:11:35,850
It seems clearly
better than that.

266
00:11:35,850 --> 00:11:38,723
And then, "transformerify,"
may be in the best,

267
00:11:38,723 --> 00:11:41,015
this is sort of optimistic,
but maybe in the best case,

268
00:11:41,015 --> 00:11:43,490
right, you were able
to say, oh, yes this

269
00:11:43,490 --> 00:11:47,360
is "transformer," and "ify."

270
00:11:47,360 --> 00:11:49,220
Again, the subwords
that you learn

271
00:11:49,220 --> 00:11:51,890
don't actually tend to be this
well morphologically motivated,

272
00:11:51,890 --> 00:11:52,430
I think.

273
00:11:52,430 --> 00:11:56,330
So "ify" is like a clear
like suffix in English that

274
00:11:56,330 --> 00:11:59,240
has a very common and replicable
meaning when you apply it

275
00:11:59,240 --> 00:12:02,660
to nouns, that's
derivational morphology.

276
00:12:02,660 --> 00:12:06,200
But you're able to sort of
compose the word of tran--

277
00:12:06,200 --> 00:12:08,780
the meaning of "transformerify"
possibly from its two

278
00:12:08,780 --> 00:12:10,850
subword constituents.

279
00:12:10,850 --> 00:12:12,860
And so, when we talk
about words being

280
00:12:12,860 --> 00:12:15,590
input to transformer models,
pretrained to transformer

281
00:12:15,590 --> 00:12:17,970
models, throughout the
entirety of this lecture,

282
00:12:17,970 --> 00:12:20,330
we will be talking
about subwords.

283
00:12:20,330 --> 00:12:23,000
So I might say a
word, and what I mean

284
00:12:23,000 --> 00:12:27,597
is possibly a full word,
also possibly a subword.

285
00:12:27,597 --> 00:12:29,930
OK, so when we say a sequence
of words, the transformer,

286
00:12:29,930 --> 00:12:32,600
the pretrained
transformer has no idea,

287
00:12:32,600 --> 00:12:37,190
sort of whether it's dealing
with words or subwords, when

288
00:12:37,190 --> 00:12:40,290
it's doing its
self-attention operations.

289
00:12:40,290 --> 00:12:41,720
And so, this can be a problem.

290
00:12:41,720 --> 00:12:44,600
You can imagine if you
have really weird sequences

291
00:12:44,600 --> 00:12:47,750
of characters, you can actually
have an individual single word

292
00:12:47,750 --> 00:12:51,350
mapped to as many words
as it has characters.

293
00:12:51,350 --> 00:12:53,270
That can be a problem,
because suddenly you

294
00:12:53,270 --> 00:12:55,940
have a 10-word sentence
but one of the words

295
00:12:55,940 --> 00:12:59,090
is mapped to 20
subwords, and now you

296
00:12:59,090 --> 00:13:00,290
have a 30-word sentence.

297
00:13:00,290 --> 00:13:03,860
Where 20 of the 30 words
are just one real word.

298
00:13:03,860 --> 00:13:06,890
So keep this in
mind, but I think

299
00:13:06,890 --> 00:13:09,680
it's important for sort of this
open vocabulary assumption,

300
00:13:09,680 --> 00:13:11,972
it's important in English,
and it's even more important

301
00:13:11,972 --> 00:13:15,020
in many other languages.

302
00:13:15,020 --> 00:13:16,430
And the actual
algorithm, I mean,

303
00:13:16,430 --> 00:13:18,845
you can go into the actual
algorithms done for this,

304
00:13:18,845 --> 00:13:23,120
byte-pair encoding is sort of my
favorite for going over briefly

305
00:13:23,120 --> 00:13:27,090
word piece you can
also take a look at.

306
00:13:27,090 --> 00:13:28,160
OK.

307
00:13:28,160 --> 00:13:29,875
Any questions on subwords?

308
00:13:29,875 --> 00:13:32,300
I guess, John,
somebody was asking,

309
00:13:32,300 --> 00:13:34,730
what does the hashtag mean?

310
00:13:34,730 --> 00:13:35,900
Oh great, great point.

311
00:13:35,900 --> 00:13:39,420
So this means that you should
be combining this subword.

312
00:13:39,420 --> 00:13:41,870
So this subword it is
not the end of a word.

313
00:13:41,870 --> 00:13:45,300
"taa##" is sort of
telling the model.

314
00:13:45,300 --> 00:13:48,530
So if I had "taa"
with no hashes,

315
00:13:48,530 --> 00:13:50,510
that's a separate
subword that means

316
00:13:50,510 --> 00:13:53,472
there's an entire word that is
"taa," or at the very least,

317
00:13:53,472 --> 00:13:54,680
it's not the end of the word.

318
00:13:54,680 --> 00:13:57,560
See how here, I don't have
the hashes at the end,

319
00:13:57,560 --> 00:13:59,270
it's because this is
indicating that this

320
00:13:59,270 --> 00:14:00,560
is at the end of the word.

321
00:14:00,560 --> 00:14:02,537
Different subword
schemes differ on

322
00:14:02,537 --> 00:14:05,120
whether you should put something
at the beginning of the word,

323
00:14:05,120 --> 00:14:08,240
if it does begin a word, or
if you should put something

324
00:14:08,240 --> 00:14:10,910
at the end of the word if
it doesn't end the word.

325
00:14:10,910 --> 00:14:13,550
Right, so when the tokenizer
is running over your data,

326
00:14:13,550 --> 00:14:16,730
so you've got something that's
tokenizing this sentence,

327
00:14:16,730 --> 00:14:20,120
"in the worst case--" Oh!

328
00:14:20,120 --> 00:14:20,630
Yeah.

329
00:14:20,630 --> 00:14:23,570
"In the worst case--"
it says like, "in,"

330
00:14:23,570 --> 00:14:24,650
that's a whole word.

331
00:14:24,650 --> 00:14:26,880
Give it just the
word "in," no hashes.

332
00:14:26,880 --> 00:14:27,920
That's a whole word.

333
00:14:27,920 --> 00:14:30,170
Give it just the word
"the," no hashes.

334
00:14:30,170 --> 00:14:34,220
And then, maybe over
here at subwords, right,

335
00:14:34,220 --> 00:14:36,560
we've got this
weird word subwords,

336
00:14:36,560 --> 00:14:38,990
and it splits it
into sub and words.

337
00:14:38,990 --> 00:14:42,110
And so, "sub," it's going
to give it the subword with

338
00:14:42,110 --> 00:14:47,840
S-U-B-#-# to indicate that
it's part of this larger word,

339
00:14:47,840 --> 00:14:52,340
subwords, as opposed to the
word "sub," like "submarine,"

340
00:14:52,340 --> 00:14:54,030
which would be different.

341
00:14:54,030 --> 00:14:55,220
That was a great question.

342
00:14:55,220 --> 00:15:02,160


343
00:15:02,160 --> 00:15:06,270
OK, great.

344
00:15:06,270 --> 00:15:10,140
So that was our note on
subword modeling, and you can--

345
00:15:10,140 --> 00:15:12,810
subwords are
important for example

346
00:15:12,810 --> 00:15:16,200
in a lot of translation
applications, that's

347
00:15:16,200 --> 00:15:19,312
why we gave you subwords on
the translation assignment.

348
00:15:19,312 --> 00:15:21,270
Now let's talk about
model pretraining and word

349
00:15:21,270 --> 00:15:22,090
embeddings.

350
00:15:22,090 --> 00:15:23,790
So I love--

351
00:15:23,790 --> 00:15:25,490
I love being able
to go to this slide.

352
00:15:25,490 --> 00:15:28,110
So we saw this quote at
the beginning of the class.

353
00:15:28,110 --> 00:15:30,270
"You shall know a word
by the company it keeps."

354
00:15:30,270 --> 00:15:32,070
And this was sort
of one of the things

355
00:15:32,070 --> 00:15:34,840
that we used to summarize
distributional semantics.

356
00:15:34,840 --> 00:15:38,670
This idea that Word2vec was sort
of well motivated in some way,

357
00:15:38,670 --> 00:15:40,710
because the meaning of
a word can be thought

358
00:15:40,710 --> 00:15:43,500
of as being derived
from the kind

359
00:15:43,500 --> 00:15:47,160
of co-occurrence statistics of
words that co-occur around it,

360
00:15:47,160 --> 00:15:51,960
and that was just fascinatingly
effective, I think.

361
00:15:51,960 --> 00:15:54,760
But there's this other quote
actually from the same person.

362
00:15:54,760 --> 00:15:58,200
So we have J.R.
Firth 1935 compared

363
00:15:58,200 --> 00:16:00,450
to our quote before from 1957.

364
00:16:00,450 --> 00:16:03,630
And the second quote says,
"the complete meaning of a word

365
00:16:03,630 --> 00:16:06,240
is always contextual,
and no study

366
00:16:06,240 --> 00:16:08,100
of meaning apart from
a complete context

367
00:16:08,100 --> 00:16:10,320
can be taken seriously."

368
00:16:10,320 --> 00:16:12,720
Now again, these are just
things that we can sort of

369
00:16:12,720 --> 00:16:14,680
think about and chew on.

370
00:16:14,680 --> 00:16:17,910
But it comes to
mind, right, when

371
00:16:17,910 --> 00:16:20,520
you embed words with
Word2vec, one of the issues

372
00:16:20,520 --> 00:16:23,160
is that, you don't actually
look at its neighbors

373
00:16:23,160 --> 00:16:25,860
as you're giving
it an embedding.

374
00:16:25,860 --> 00:16:30,360
So if I have the sentence,
"I record the record."

375
00:16:30,360 --> 00:16:34,320
The two instances of "rec,"
"ord" mean different things,

376
00:16:34,320 --> 00:16:37,120
but they're given the
same Word2vec embedding.

377
00:16:37,120 --> 00:16:37,620
Right?

378
00:16:37,620 --> 00:16:40,320
Because in Word2vec, you take
the string, you map it to,

379
00:16:40,320 --> 00:16:42,030
oh I've seen the
word "record" before.

380
00:16:42,030 --> 00:16:46,050
You get that sort of vector
from your learned matrix,

381
00:16:46,050 --> 00:16:49,420
and you give it the same
thing in both cases.

382
00:16:49,420 --> 00:16:51,360
And so, what we're
going to be doing today,

383
00:16:51,360 --> 00:16:54,660
is actually not conceptually
all that different

384
00:16:54,660 --> 00:16:56,430
from training Word2vec.

385
00:16:56,430 --> 00:16:59,340
Word2vec training you can
think of as pretraining

386
00:16:59,340 --> 00:17:01,860
just a very simple
model that only assigns

387
00:17:01,860 --> 00:17:04,530
an individual vector
to each unique word

388
00:17:04,530 --> 00:17:07,319
type, each unique element
in your vocabulary.

389
00:17:07,319 --> 00:17:10,030
Today, we'll be going a
lot further than that.

390
00:17:10,030 --> 00:17:11,910
But the idea is very similar.

391
00:17:11,910 --> 00:17:15,210
So back in 2017, we would
start with pretrained word

392
00:17:15,210 --> 00:17:16,650
embeddings.

393
00:17:16,650 --> 00:17:18,480
And again, remember,
no context there.

394
00:17:18,480 --> 00:17:21,660
So you give a word an embedding
independent of the context

395
00:17:21,660 --> 00:17:22,560
that it shows up in.

396
00:17:22,560 --> 00:17:25,358
And then, you learn how to
incorporate the context.

397
00:17:25,358 --> 00:17:28,409
It's not like our NLP models
never used context, right?

398
00:17:28,410 --> 00:17:30,990
Instead, you would learn to
incorporate the context using

399
00:17:30,990 --> 00:17:34,740
your LSTM, or if it's
later in 2017, you know,

400
00:17:34,740 --> 00:17:36,900
your transformer.

401
00:17:36,900 --> 00:17:40,560
And you will learn to
incorporate context while

402
00:17:40,560 --> 00:17:41,670
training on the task.

403
00:17:41,670 --> 00:17:43,410
So you have some supervision.

404
00:17:43,410 --> 00:17:45,210
Maybe it's machine
translation supervision,

405
00:17:45,210 --> 00:17:47,343
maybe sentiment, maybe
question answering.

406
00:17:47,343 --> 00:17:49,260
And you would learn how
to incorporate context

407
00:17:49,260 --> 00:17:53,400
in your LSTM or otherwise
through the signal

408
00:17:53,400 --> 00:17:56,530
of the training, instead of say,
through the Word2vec signal.

409
00:17:56,530 --> 00:17:58,950
And so, sort of
pictographically,

410
00:17:58,950 --> 00:18:00,450
you have these word
embeddings here.

411
00:18:00,450 --> 00:18:02,790
So the red are sort of
your Word2vec embeddings

412
00:18:02,790 --> 00:18:04,690
and those are pretrained.

413
00:18:04,690 --> 00:18:08,040
And those take up some of the
parameters of your network.

414
00:18:08,040 --> 00:18:10,000
And then, you've got
your contextualization.

415
00:18:10,000 --> 00:18:12,250
Now, this looks like an LSTM
but it could be whatever.

416
00:18:12,250 --> 00:18:15,060
So this maybe
bidirectional encoder thing

417
00:18:15,060 --> 00:18:16,290
here is not pretrained.

418
00:18:16,290 --> 00:18:18,180
And now that's a
lot of parameters

419
00:18:18,180 --> 00:18:20,040
that are not pretrained,
and then maybe you

420
00:18:20,040 --> 00:18:22,740
have some sort of readout
function at the end, right?

421
00:18:22,740 --> 00:18:25,410
To predict whatever thing
you're trying to predict again.

422
00:18:25,410 --> 00:18:28,650
Maybe it's sentiment, maybe
you're doing, I don't know,

423
00:18:28,650 --> 00:18:29,880
topic labeling.

424
00:18:29,880 --> 00:18:32,197
Whatever you want to do,
this is sort of the paradigm,

425
00:18:32,197 --> 00:18:33,780
like you set some
code of architecture

426
00:18:33,780 --> 00:18:36,570
and you only pretrained
the word embeddings.

427
00:18:36,570 --> 00:18:39,930
And so, this isn't
actually conceptually

428
00:18:39,930 --> 00:18:45,330
necessarily the biggest
problem, because we

429
00:18:45,330 --> 00:18:47,340
like to think in
deep learning stuff

430
00:18:47,340 --> 00:18:50,850
that we have a lot of training
data for our objectives.

431
00:18:50,850 --> 00:18:55,367
I mean one of the things that
we motivated big deep neural

432
00:18:55,367 --> 00:18:57,450
networks for, is that they
can take a lot of data,

433
00:18:57,450 --> 00:18:59,100
and then we can learn
patterns from it.

434
00:18:59,100 --> 00:19:02,700
But it does put the onus
on our downstream data

435
00:19:02,700 --> 00:19:06,870
to be sort of sufficient to
teach the contextual aspects

436
00:19:06,870 --> 00:19:07,870
of language.

437
00:19:07,870 --> 00:19:09,750
So you can imagine
if you only have

438
00:19:09,750 --> 00:19:12,900
a little bit of labeled
data for fine tuning,

439
00:19:12,900 --> 00:19:15,952
you're putting a pretty big
role on that data to say,

440
00:19:15,952 --> 00:19:17,910
"hey, maybe here's some
pretrained embeddings."

441
00:19:17,910 --> 00:19:19,680
But how you handle
like sentences,

442
00:19:19,680 --> 00:19:23,330
and how they compose, and all
that stuff, that's up to you.

443
00:19:23,330 --> 00:19:25,080
So if you don't have
a lot of labeled data

444
00:19:25,080 --> 00:19:27,180
for your downstream
task, you're asking

445
00:19:27,180 --> 00:19:30,930
it to do a lot with a large
number of parameters that

446
00:19:30,930 --> 00:19:34,340
have been initialized randomly.

447
00:19:34,340 --> 00:19:36,410
OK, so like a small
portion of the parameters

448
00:19:36,410 --> 00:19:39,090
have been pretrained

449
00:19:39,090 --> 00:19:40,050
OK.

450
00:19:40,050 --> 00:19:43,470
So where we're going is
pretraining whole models.

451
00:19:43,470 --> 00:19:46,980
I mean, conceptually we're
pretty close to there.

452
00:19:46,980 --> 00:19:51,150
So nowadays, almost
all parameters

453
00:19:51,150 --> 00:19:53,070
in your neural
network, and let's say

454
00:19:53,070 --> 00:19:55,710
a lot of research settings,
and increasingly in industry,

455
00:19:55,710 --> 00:19:58,260
are initialized via pretraining.

456
00:19:58,260 --> 00:20:00,600
Just like Word2vec
parameters were initialized.

457
00:20:00,600 --> 00:20:03,510
And pretraining
methods in general,

458
00:20:03,510 --> 00:20:08,790
hide parts of the input
from the model itself,

459
00:20:08,790 --> 00:20:11,710
and then train the model
to reconstruct those parts.

460
00:20:11,710 --> 00:20:14,730
How does this
connect to Word2vec?

461
00:20:14,730 --> 00:20:17,640
In Word2vec people don't
usually make this connection,

462
00:20:17,640 --> 00:20:19,030
but it's the following.

463
00:20:19,030 --> 00:20:23,970
You have an individual word,
and it knows itself, right?

464
00:20:23,970 --> 00:20:26,190
Because you have the
embedding for the center word.

465
00:20:26,190 --> 00:20:26,690
Right?

466
00:20:26,690 --> 00:20:27,780
From assignment 2.

467
00:20:27,780 --> 00:20:29,530
You have the embedding
for the center word

468
00:20:29,530 --> 00:20:31,140
and it knows itself,
and you've masked

469
00:20:31,140 --> 00:20:33,308
out all of its neighbors.

470
00:20:33,308 --> 00:20:35,100
You've hidden all of
its neighbors from it.

471
00:20:35,100 --> 00:20:35,310
Right?

472
00:20:35,310 --> 00:20:37,440
All of its window neighbors,
you've hidden from it.

473
00:20:37,440 --> 00:20:40,680
You ask the center words
to predict its neighbors.

474
00:20:40,680 --> 00:20:41,570
Right?

475
00:20:41,570 --> 00:20:47,093
And so, this falls under
the category of pretraining.

476
00:20:47,093 --> 00:20:48,510
All of these methods
look similar.

477
00:20:48,510 --> 00:20:50,940
You hide parts of the
input from the model,

478
00:20:50,940 --> 00:20:53,940
and train the model to
reconstruct those parts.

479
00:20:53,940 --> 00:20:56,340
The differences with
full model pretraining

480
00:20:56,340 --> 00:20:59,127
is that you don't give the
model just the individual word

481
00:20:59,127 --> 00:21:00,960
and have it learn an
embedding of that word,

482
00:21:00,960 --> 00:21:03,000
you give it much
more of the sequence

483
00:21:03,000 --> 00:21:06,588
and have that predict held
out parts of the sequence.

484
00:21:06,588 --> 00:21:08,130
And we'll get into
the details there.

485
00:21:08,130 --> 00:21:11,340
But the takeaway is
that, everything here

486
00:21:11,340 --> 00:21:13,110
is pretrained jointly.

487
00:21:13,110 --> 00:21:16,620
Possibly with the exception
of the very last layer

488
00:21:16,620 --> 00:21:19,280
that predicts the label.

489
00:21:19,280 --> 00:21:19,780
OK.

490
00:21:19,780 --> 00:21:22,180
And this has just
been exceptionally

491
00:21:22,180 --> 00:21:25,990
effective at building
representations of language.

492
00:21:25,990 --> 00:21:28,660
That just maps similar
things in language,

493
00:21:28,660 --> 00:21:31,090
to similar representations
in these encoders,

494
00:21:31,090 --> 00:21:33,850
just like how Word2vec
maps similar words

495
00:21:33,850 --> 00:21:35,320
to similar vectors.

496
00:21:35,320 --> 00:21:38,020
That's been exceptionally
effective at making parameter

497
00:21:38,020 --> 00:21:40,870
initializations, where you
start with these parameters that

498
00:21:40,870 --> 00:21:43,450
have been pretrained,
and then you fine

499
00:21:43,450 --> 00:21:45,700
tune them on your labeled data.

500
00:21:45,700 --> 00:21:47,530
And then third, they've
been exceptionally

501
00:21:47,530 --> 00:21:49,750
effective at defining
probability distributions

502
00:21:49,750 --> 00:21:52,210
over language, like
in language modeling.

503
00:21:52,210 --> 00:21:54,550
They are actually
really useful to sample

504
00:21:54,550 --> 00:21:56,407
from in certain cases.

505
00:21:56,407 --> 00:21:58,240
So these are three ways
in which we interact

506
00:21:58,240 --> 00:21:59,590
with pretrained models.

507
00:21:59,590 --> 00:22:02,410
We use the representations
just to compute similarities,

508
00:22:02,410 --> 00:22:04,360
we use them for parameter
initializations,

509
00:22:04,360 --> 00:22:07,690
and we actually just use them
as probability distributions.

510
00:22:07,690 --> 00:22:10,960
Sort of how we trained them.

511
00:22:10,960 --> 00:22:12,610
OK.

512
00:22:12,610 --> 00:22:16,150
So lets-- we're going to get
into some technical parts here,

513
00:22:16,150 --> 00:22:18,640
but I sort of want to
think broad thoughts

514
00:22:18,640 --> 00:22:20,830
about what we could
do with pretraining,

515
00:22:20,830 --> 00:22:24,160
and what kind of things we could
expect to potentially learn

516
00:22:24,160 --> 00:22:27,640
from this general method
of hide part of the input,

517
00:22:27,640 --> 00:22:29,712
and then see other
parts of the input,

518
00:22:29,712 --> 00:22:31,670
and then try to predict
the parts that you hid.

519
00:22:31,670 --> 00:22:32,440
OK.

520
00:22:32,440 --> 00:22:36,640
So Stanford University is
located in blank California.

521
00:22:36,640 --> 00:22:39,220
If we gave a model everything
that was not blanked out here,

522
00:22:39,220 --> 00:22:41,200
and asked to predict
the middle, right?

523
00:22:41,200 --> 00:22:45,490
The loss function
would train the model

524
00:22:45,490 --> 00:22:49,070
to predict Palo
Alto here, I expect.

525
00:22:49,070 --> 00:22:49,570
OK.

526
00:22:49,570 --> 00:22:52,462
So this is an
instance of something

527
00:22:52,462 --> 00:22:54,670
that you could imagine being
a pretraining objective.

528
00:22:54,670 --> 00:22:57,220
You take in a sentence,
you remove part of it,

529
00:22:57,220 --> 00:22:59,530
and you say, recreate
the part that I removed.

530
00:22:59,530 --> 00:23:01,780
And in this case, if I just
gave a bunch of examples

531
00:23:01,780 --> 00:23:05,930
that looked like this, it might
learn this sort of trivia thing

532
00:23:05,930 --> 00:23:08,080
here.

533
00:23:08,080 --> 00:23:08,580
OK.

534
00:23:08,580 --> 00:23:12,490
Here's another one I put
blank fork down on the table.

535
00:23:12,490 --> 00:23:14,110
This one is under-specified.

536
00:23:14,110 --> 00:23:14,610
Right?

537
00:23:14,610 --> 00:23:21,660
So this could be "the fork," "my
fork," "his fork," "her fork,"

538
00:23:21,660 --> 00:23:24,480
"some fork," "a fork."

539
00:23:24,480 --> 00:23:29,100
So this is specifying the
kinds of syntactic categories

540
00:23:29,100 --> 00:23:32,015
of things that can sort
of appear in this context.

541
00:23:32,015 --> 00:23:33,390
So this is another
thing that you

542
00:23:33,390 --> 00:23:37,230
might be able to learn
from such an objective.

543
00:23:37,230 --> 00:23:39,660
Here-- so you have, the woman
walked across the street,

544
00:23:39,660 --> 00:23:41,942
checking for traffic
over blank shoulder.

545
00:23:41,942 --> 00:23:43,650
One of the things that
could go over here

546
00:23:43,650 --> 00:23:47,560
is, "her," that's a
co-reference statement.

547
00:23:47,560 --> 00:23:50,160
So you could learn sort of
connections between entities

548
00:23:50,160 --> 00:23:54,390
in a text, where one
word, woman can also

549
00:23:54,390 --> 00:23:57,840
co-refer to the same entity
in the world, as this word,

550
00:23:57,840 --> 00:23:58,650
this pronoun her.

551
00:23:58,650 --> 00:24:01,160


552
00:24:01,160 --> 00:24:03,740
Here, you could think
about, I went to the ocean

553
00:24:03,740 --> 00:24:06,537
to see the fish, turtles,
seals, and blank.

554
00:24:06,537 --> 00:24:08,870
Now here, I don't think there's
a single correct answer,

555
00:24:08,870 --> 00:24:11,607
as to what we could see
going into that blank.

556
00:24:11,607 --> 00:24:13,190
But a model could
learn a distribution

557
00:24:13,190 --> 00:24:14,750
of the kinds of
things that people

558
00:24:14,750 --> 00:24:18,050
might be talking about when
they-- one go to the ocean,

559
00:24:18,050 --> 00:24:20,600
and two are excited
to see marine life.

560
00:24:20,600 --> 00:24:21,100
Right?

561
00:24:21,100 --> 00:24:22,725
So this is sort of
a semantic category.

562
00:24:22,725 --> 00:24:25,010
A lexical semantic
category of things

563
00:24:25,010 --> 00:24:29,090
that might sort of be in the
same set of interest as fish,

564
00:24:29,090 --> 00:24:31,430
turtles, and seals
in the context

565
00:24:31,430 --> 00:24:34,430
of, "I went to the ocean."

566
00:24:34,430 --> 00:24:36,100
OK?

567
00:24:36,100 --> 00:24:38,260
So and you know, I
expect that there

568
00:24:38,260 --> 00:24:41,440
would be examples of this in
a large corpus of text, maybe

569
00:24:41,440 --> 00:24:44,120
maybe a book.

570
00:24:44,120 --> 00:24:46,520
OK, here's another example.

571
00:24:46,520 --> 00:24:50,210
Overall, the value I got from
the two hours watching it

572
00:24:50,210 --> 00:24:53,510
was the sum total of the
popcorn and the drink.

573
00:24:53,510 --> 00:24:55,440
The movie was blank.

574
00:24:55,440 --> 00:24:55,940
Right?

575
00:24:55,940 --> 00:24:57,815
And this is when I was
sort of like, look out

576
00:24:57,815 --> 00:25:00,950
into the audience and say,
was the movie bad or good?

577
00:25:00,950 --> 00:25:04,440
But the movie was bad,
is my prediction here.

578
00:25:04,440 --> 00:25:04,940
Right?

579
00:25:04,940 --> 00:25:08,420
And so, this is teaching you
something about sentiment.

580
00:25:08,420 --> 00:25:10,960
About how people express
sentiment in language.

581
00:25:10,960 --> 00:25:12,320
And so, this is--

582
00:25:12,320 --> 00:25:15,950
even looks like a task itself.

583
00:25:15,950 --> 00:25:17,870
Like do sentiment
analysis, is sort

584
00:25:17,870 --> 00:25:19,820
of what you need to do
in order to figure out

585
00:25:19,820 --> 00:25:21,500
whether the movie
was bad or good.

586
00:25:21,500 --> 00:25:24,070
Or maybe the word is
neither bad or good,

587
00:25:24,070 --> 00:25:25,820
the movie was over or
something like that.

588
00:25:25,820 --> 00:25:27,500
But like, if you
had to, if you had

589
00:25:27,500 --> 00:25:29,765
to choose between is
bad or good more likely,

590
00:25:29,765 --> 00:25:31,400
right, you sort of
have to figure out

591
00:25:31,400 --> 00:25:34,470
the sentiment of the text.

592
00:25:34,470 --> 00:25:37,120
Now that's really fascinating.

593
00:25:37,120 --> 00:25:38,110
OK.

594
00:25:38,110 --> 00:25:39,400
Here's another one.

595
00:25:39,400 --> 00:25:42,550
Iroh went into the
kitchen to make some tea.

596
00:25:42,550 --> 00:25:46,270
Standing next to Iroh,
Zuko pondered his destiny.

597
00:25:46,270 --> 00:25:48,850
Zuko left the blank.

598
00:25:48,850 --> 00:25:49,360
OK.

599
00:25:49,360 --> 00:25:53,570
So this is a little easy because
we really only show one place.

600
00:25:53,570 --> 00:25:55,870
So I guess we have
another noun, destiny.

601
00:25:55,870 --> 00:25:59,080
But this is sort of reasoning
about spatial location,

602
00:25:59,080 --> 00:26:02,440
and the movement of sort of
agents in an imagined world.

603
00:26:02,440 --> 00:26:05,710
We could imagine text
that has lines like this,

604
00:26:05,710 --> 00:26:07,150
person went into
the place and was

605
00:26:07,150 --> 00:26:09,950
next to so-and-so,
who left and did that,

606
00:26:09,950 --> 00:26:12,230
and so you have
these relationships.

607
00:26:12,230 --> 00:26:15,310
So here, Zuko left the kitchen,
is the most likely thing

608
00:26:15,310 --> 00:26:17,260
that I think would go here.

609
00:26:17,260 --> 00:26:20,230
And it sort of indicates
that in order for a model

610
00:26:20,230 --> 00:26:25,720
to learn to perform this,
fill in the missing part task,

611
00:26:25,720 --> 00:26:30,070
it might need to, in
general, figure out sort

612
00:26:30,070 --> 00:26:35,410
of where things are, and
whether statements mean or imply

613
00:26:35,410 --> 00:26:36,740
that locality.

614
00:26:36,740 --> 00:26:38,560
So standing next to.

615
00:26:38,560 --> 00:26:40,060
Iroh went into the kitchen.

616
00:26:40,060 --> 00:26:41,807
Now Iroh is in the kitchen.

617
00:26:41,807 --> 00:26:43,390
And then, standing
next to Iroh, means

618
00:26:43,390 --> 00:26:44,960
Zuko is now in the kitchen.

619
00:26:44,960 --> 00:26:45,460
Right?

620
00:26:45,460 --> 00:26:48,280
And then, Zuko now leaves where?

621
00:26:48,280 --> 00:26:50,080
Well, he was in
the kitchen before.

622
00:26:50,080 --> 00:26:53,990
So this is sort of a very
basic sense of reasoning.

623
00:26:53,990 --> 00:26:55,817
Now, this one.

624
00:26:55,817 --> 00:26:56,650
Here's the sentence.

625
00:26:56,650 --> 00:26:58,483
I was thinking about
the sequence that goes,

626
00:26:58,483 --> 00:27:02,750
1, 1, 2, 3, 5, 8, 13, 21, blank.

627
00:27:02,750 --> 00:27:03,250
Right?

628
00:27:03,250 --> 00:27:05,290
So I don't know.

629
00:27:05,290 --> 00:27:07,040
I can imagine people
writing stuff.

630
00:27:07,040 --> 00:27:08,680
So this is the
Fibonacci sequence.

631
00:27:08,680 --> 00:27:11,540
And sort of you sum these
two to get the next one,

632
00:27:11,540 --> 00:27:13,930
sum these two to get the next
one, sum these two, right?

633
00:27:13,930 --> 00:27:16,300
And so, you have
this running sum,

634
00:27:16,300 --> 00:27:17,800
it's a famous
sequence that shows up

635
00:27:17,800 --> 00:27:20,480
in a lot of text
on the internet.

636
00:27:20,480 --> 00:27:23,920
And in general, you have to
learn the algorithm, or just

637
00:27:23,920 --> 00:27:27,040
the formula, I guess that
defines the Fibonacci sequence

638
00:27:27,040 --> 00:27:29,560
in order to keep going.

639
00:27:29,560 --> 00:27:31,810
Do models learn
this in practice?

640
00:27:31,810 --> 00:27:32,980
Wait and find out.

641
00:27:32,980 --> 00:27:35,620
But you would have
to learn it in order

642
00:27:35,620 --> 00:27:40,550
to get the sequence to keep
going, and going, and going.

643
00:27:40,550 --> 00:27:41,050
OK.

644
00:27:41,050 --> 00:27:45,220
So we're going to get into
specific pretrained models,

645
00:27:45,220 --> 00:27:47,440
specific methods
of pretraining now.

646
00:27:47,440 --> 00:27:52,900
So I'm going to go over a brief
review of transformer encoders,

647
00:27:52,900 --> 00:27:56,650
decoders, and
encoder-decoders, because we're

648
00:27:56,650 --> 00:27:58,780
going to get into the sort
of technical bits now.

649
00:27:58,780 --> 00:28:01,720
So before I do that,
I'm going to pause,

650
00:28:01,720 --> 00:28:05,480
are there any questions?

651
00:28:05,480 --> 00:28:05,980
Oh, yeah.

652
00:28:05,980 --> 00:28:07,730
There's an important
question asked about,

653
00:28:07,730 --> 00:28:11,950
the risk of overfitting our
model on our input training

654
00:28:11,950 --> 00:28:13,600
data when they're
doing pretraining.

655
00:28:13,600 --> 00:28:15,390
Then you could also
answer this question

656
00:28:15,390 --> 00:28:17,530
in the light of the
huge pretrained models

657
00:28:17,530 --> 00:28:20,797
that we're seeing nowadays.

658
00:28:20,797 --> 00:28:22,380
Sorry the first part
of that question,

659
00:28:22,380 --> 00:28:26,097
was it are we overfitting
our models to what?

660
00:28:26,097 --> 00:28:29,640
The risk of overfitting model
on our input training data

661
00:28:29,640 --> 00:28:31,540
when they're doing pretraining.

662
00:28:31,540 --> 00:28:32,040
Got it.

663
00:28:32,040 --> 00:28:34,290
Yeah, so that's a good point.

664
00:28:34,290 --> 00:28:36,420
So we're using
very large models,

665
00:28:36,420 --> 00:28:39,960
and we might imagine that
there's a risk of overfitting.

666
00:28:39,960 --> 00:28:42,450
And in practice,
yeah, it's actually

667
00:28:42,450 --> 00:28:46,232
one of the more crucial things
to do to make training work.

668
00:28:46,232 --> 00:28:48,690
So that turns out that you need
to have a lot, a lot, a lot

669
00:28:48,690 --> 00:28:51,190
of data, like a lot of data.

670
00:28:51,190 --> 00:28:53,670
And in fact, we'll
show results later on,

671
00:28:53,670 --> 00:28:56,040
where people built
a pretrained model.

672
00:28:56,040 --> 00:28:59,810
Pretrained it on a lot of data,
and then like six months later,

673
00:28:59,810 --> 00:29:01,560
someone else came along
and was like, hey,

674
00:29:01,560 --> 00:29:03,227
if you pretrained on
it 10 months later,

675
00:29:03,227 --> 00:29:06,120
and changed almost nothing else,
it would have gone even better.

676
00:29:06,120 --> 00:29:07,530
Now was it overfitting?

677
00:29:07,530 --> 00:29:12,370
I mean, you can sort of hold out
some text during pretraining,

678
00:29:12,370 --> 00:29:12,870
right?

679
00:29:12,870 --> 00:29:15,002
And sort of evaluate
the perplexity, right?

680
00:29:15,002 --> 00:29:16,710
The language modeling
performance on that

681
00:29:16,710 --> 00:29:19,140
held out text,
and it tends to be

682
00:29:19,140 --> 00:29:22,410
the case that actually these
models are underfitting, right?

683
00:29:22,410 --> 00:29:24,660
That we need even
larger and larger models

684
00:29:24,660 --> 00:29:27,720
to express the complex
interactions that

685
00:29:27,720 --> 00:29:30,600
allow us to fit these
data sets better,

686
00:29:30,600 --> 00:29:33,182
and so we'll talk about that
when we talk about BERT.

687
00:29:33,182 --> 00:29:34,890
And one of the really
interesting results

688
00:29:34,890 --> 00:29:38,220
is that BERT is underfit, not
overfit, but in principle,

689
00:29:38,220 --> 00:29:39,870
yes, it's a problem to--

690
00:29:39,870 --> 00:29:42,060
It's potentially a
problem to overfit,

691
00:29:42,060 --> 00:29:45,060
but we end up having a ton
of text in English at least,

692
00:29:45,060 --> 00:29:46,710
although not in every language.

693
00:29:46,710 --> 00:29:49,710
And so, yeah, it's
important to scale them,

694
00:29:49,710 --> 00:29:51,717
but currently our models
don't seem overfit

695
00:29:51,717 --> 00:29:52,800
to their pretraining text.

696
00:29:52,800 --> 00:29:58,760


697
00:29:58,760 --> 00:29:59,580
OK.

698
00:29:59,580 --> 00:30:00,865
Any other questions?

699
00:30:00,865 --> 00:30:03,660


700
00:30:03,660 --> 00:30:05,520
All right.

701
00:30:05,520 --> 00:30:07,830
So we saw this figure
before, right here.

702
00:30:07,830 --> 00:30:10,830
We saw this figure of a
transformer encoder-decoder,

703
00:30:10,830 --> 00:30:13,930
from this paper Attention
Is All You Need.

704
00:30:13,930 --> 00:30:17,430
And so we have a
couple of things.

705
00:30:17,430 --> 00:30:20,160
We're not going to go over
the form of attention again

706
00:30:20,160 --> 00:30:22,290
today, because we
have a lot to go over,

707
00:30:22,290 --> 00:30:25,380
but I'm happy to chat
about it more on Ed.

708
00:30:25,380 --> 00:30:28,800
But so in our encoder, we
have some input sequence.

709
00:30:28,800 --> 00:30:31,580
Remember this is a
sequence of subwords now.

710
00:30:31,580 --> 00:30:34,040
If subword gets a
word embeddding,

711
00:30:34,040 --> 00:30:36,530
and each index in
the transformer

712
00:30:36,530 --> 00:30:38,750
gets a position
embedding, now, remember

713
00:30:38,750 --> 00:30:42,230
that we have a finite length
that our sequence can possibly

714
00:30:42,230 --> 00:30:44,840
be, like 512 tokens.

715
00:30:44,840 --> 00:30:47,277
That was that capital
T from last lecture.

716
00:30:47,277 --> 00:30:48,860
So you have some
finite length, so you

717
00:30:48,860 --> 00:30:53,930
have one embedding of a position
for every index for all 512

718
00:30:53,930 --> 00:30:57,410
indices, and then you have
all your word embeddings.

719
00:30:57,410 --> 00:30:59,630
And then the transformer
encoder, right,

720
00:30:59,630 --> 00:31:03,680
was this combination
of sort of submodules

721
00:31:03,680 --> 00:31:08,210
that we walked through
line by line on Tuesday.

722
00:31:08,210 --> 00:31:12,300
Multi-headed attention was sort
of the core building block.

723
00:31:12,300 --> 00:31:14,540
And then, we had
residual and LayerNorm,

724
00:31:14,540 --> 00:31:16,340
right, to help with
passing gradients,

725
00:31:16,340 --> 00:31:18,990
and to help make training
go better and faster.

726
00:31:18,990 --> 00:31:24,200
We had that feed-forward
layer to process

727
00:31:24,200 --> 00:31:26,250
sort of the result of the
multi-headed attention,

728
00:31:26,250 --> 00:31:28,400
another residual and
LayerNorm, and then

729
00:31:28,400 --> 00:31:31,370
pass to an identical
transformer encoder block here.

730
00:31:31,370 --> 00:31:32,593
And these will be stacked.

731
00:31:32,593 --> 00:31:34,760
We'll see a number of
different configurations here,

732
00:31:34,760 --> 00:31:39,740
but I think 6, 12, of these
sort of stack together.

733
00:31:39,740 --> 00:31:42,260
OK, so that's a
transformer encoder.

734
00:31:42,260 --> 00:31:45,680
And we're actually going to
see whole models today that

735
00:31:45,680 --> 00:31:48,360
are just transformer encoders.

736
00:31:48,360 --> 00:31:48,860
OK.

737
00:31:48,860 --> 00:31:50,590
So when we talked about
machine translation,

738
00:31:50,590 --> 00:31:52,090
when we talked about
the transformer

739
00:31:52,090 --> 00:31:54,430
itself, the transformer
encoder-decoder,

740
00:31:54,430 --> 00:31:55,870
we talked about
this whole thing.

741
00:31:55,870 --> 00:31:58,060
But you could actually
just have this left column,

742
00:31:58,060 --> 00:32:01,710
and you could actually just
have this right column as well.

743
00:32:01,710 --> 00:32:03,460
Although the right
column changes a little

744
00:32:03,460 --> 00:32:04,510
bit if you just have it.

745
00:32:04,510 --> 00:32:07,060
So remember the
right column, we had

746
00:32:07,060 --> 00:32:10,160
this masked multi-head
self-attention, right,

747
00:32:10,160 --> 00:32:13,930
so where you can't
look at the future.

748
00:32:13,930 --> 00:32:15,760
And someone asked
actually about how

749
00:32:15,760 --> 00:32:18,190
we decode from
transformers, given

750
00:32:18,190 --> 00:32:20,290
that you have this sort
of big chunking operation.

751
00:32:20,290 --> 00:32:21,250
It's a great
question, I won't be

752
00:32:21,250 --> 00:32:23,140
able to get into
it in detail today,

753
00:32:23,140 --> 00:32:26,980
but you have to run it once
during the decoding process.

754
00:32:26,980 --> 00:32:29,740
For every time that
you decode to sort of

755
00:32:29,740 --> 00:32:31,840
predict the next word.

756
00:32:31,840 --> 00:32:34,000
I'll write out something
on Ed for this.

757
00:32:34,000 --> 00:32:36,652
So in the masked
multi-head self-attention,

758
00:32:36,652 --> 00:32:38,360
you're not allowed to
look at the future,

759
00:32:38,360 --> 00:32:41,680
so that you sort of have
this well defined objective

760
00:32:41,680 --> 00:32:44,050
of trying to do
language modeling,

761
00:32:44,050 --> 00:32:45,880
then we have residual
and LayerNorm,

762
00:32:45,880 --> 00:32:47,590
the multi-head
cross-attention remember

763
00:32:47,590 --> 00:32:50,860
goes back to the last layer
of the transformer encoder,

764
00:32:50,860 --> 00:32:53,350
or the last transformer
encoder block.

765
00:32:53,350 --> 00:32:54,850
And then, more
residual LayerNorm,

766
00:32:54,850 --> 00:32:57,730
another feed-foward layer,
more residual LayerNorm.

767
00:32:57,730 --> 00:33:01,120
Now, if we don't have
an encoder here, right?

768
00:33:01,120 --> 00:33:03,692
Then we get rid of the
cross-attention and residual

769
00:33:03,692 --> 00:33:05,610
and LayerNorm here, right?

770
00:33:05,610 --> 00:33:07,498
So if we didn't have
this stack of encoders,

771
00:33:07,498 --> 00:33:09,040
the decoders get
simpler, because you

772
00:33:09,040 --> 00:33:10,545
don't have to attend to them.

773
00:33:10,545 --> 00:33:12,670
But then again, you also
have these word embeddings

774
00:33:12,670 --> 00:33:14,860
at the bottom and
position representations

775
00:33:14,860 --> 00:33:17,560
for the output sequence.

776
00:33:17,560 --> 00:33:18,190
OK.

777
00:33:18,190 --> 00:33:20,548
So that's been review.

778
00:33:20,548 --> 00:33:22,840
Let's talk about pretraining
through language modeling.

779
00:33:22,840 --> 00:33:25,690
So we've actually talked maybe
a little bit about this before,

780
00:33:25,690 --> 00:33:29,290
and we've seen language
modeling in the context of maybe

781
00:33:29,290 --> 00:33:30,850
just wanting to do it apriori.

782
00:33:30,850 --> 00:33:33,080
So language models were
useful, for example,

783
00:33:33,080 --> 00:33:35,170
in automatic speech
recognition systems.

784
00:33:35,170 --> 00:33:38,170
They were useful in statistical
machine translation systems.

785
00:33:38,170 --> 00:33:41,950
So let's recall the
language modeling task.

786
00:33:41,950 --> 00:33:44,380
You can say it's
defined as modeling

787
00:33:44,380 --> 00:33:46,960
the probability of a
word at a given index

788
00:33:46,960 --> 00:33:49,090
T, of any word at
any given index.

789
00:33:49,090 --> 00:33:51,700
Given all the words before it.

790
00:33:51,700 --> 00:33:54,130
And this probability
distribution--

791
00:33:54,130 --> 00:33:56,650
this is a distribution of words
given their past contexts.

792
00:33:56,650 --> 00:33:59,490


793
00:33:59,490 --> 00:34:02,960
And so this is just saying,
for any prefix here,

794
00:34:02,960 --> 00:34:05,060
Iroh goes to make--

795
00:34:05,060 --> 00:34:07,440
I want a probability of whatever
the next word should be.

796
00:34:07,440 --> 00:34:09,739
So the observed
next word is tasty,

797
00:34:09,739 --> 00:34:13,940
but maybe this goes to
make tea, goes to make

798
00:34:13,940 --> 00:34:15,620
hot water, et cetera.

799
00:34:15,620 --> 00:34:18,170
You can have a distribution
of what the next word should

800
00:34:18,170 --> 00:34:20,600
be in this decoder,
and remember that,

801
00:34:20,600 --> 00:34:22,670
because of the masked
self-attention,

802
00:34:22,670 --> 00:34:26,179
"make" can look back to
the word "to," or "goes,"

803
00:34:26,179 --> 00:34:30,373
or "Iroh," but it can't
look forward to "tasty."

804
00:34:30,373 --> 00:34:31,790
So there's a lot
of data for this.

805
00:34:31,790 --> 00:34:34,800
Right, you just
have text and voila!

806
00:34:34,800 --> 00:34:36,290
You have language modeling data.

807
00:34:36,290 --> 00:34:38,150
It's free.

808
00:34:38,150 --> 00:34:40,820
Once you have the text,
it's freely available.

809
00:34:40,820 --> 00:34:42,199
You don't need to label it.

810
00:34:42,199 --> 00:34:43,699
And in English you
have a lot of it.

811
00:34:43,699 --> 00:34:47,090
Right, this is not true of
every language by any means,

812
00:34:47,090 --> 00:34:51,870
but in English you have a
lot of pretraining data.

813
00:34:51,870 --> 00:34:56,300
And so, the simple thing
about sort of pretraining is,

814
00:34:56,300 --> 00:34:58,070
well what we're
going to do, is we're

815
00:34:58,070 --> 00:34:59,987
going to train a neural
network to do language

816
00:34:59,987 --> 00:35:01,580
modeling on a large
amount of text,

817
00:35:01,580 --> 00:35:04,130
and we'll just
save the parameters

818
00:35:04,130 --> 00:35:06,600
of our trained network to disk.

819
00:35:06,600 --> 00:35:08,448
So conceptually,
it's not actually

820
00:35:08,448 --> 00:35:10,490
different from the things
that we've done before.

821
00:35:10,490 --> 00:35:11,870
It's just sort of the intent.

822
00:35:11,870 --> 00:35:12,410
Right?

823
00:35:12,410 --> 00:35:14,510
We're training these
parameters to start

824
00:35:14,510 --> 00:35:16,940
using them for something
else later down the line.

825
00:35:16,940 --> 00:35:20,030
But the language modeling
itself doesn't change,

826
00:35:20,030 --> 00:35:22,170
the decoder here
doesn't change, right?

827
00:35:22,170 --> 00:35:25,670
It's a transformer in
pretrained models in a modern,

828
00:35:25,670 --> 00:35:30,230
because this is sort of
a newly popular concept.

829
00:35:30,230 --> 00:35:34,040
Although back in 2015, was
sort of when this, I think,

830
00:35:34,040 --> 00:35:36,320
was first effectively
tried out and got

831
00:35:36,320 --> 00:35:39,750
some interesting results.

832
00:35:39,750 --> 00:35:42,297
But this could be anything here.

833
00:35:42,297 --> 00:35:44,630
Today it's mostly going to
be transformers in the models

834
00:35:44,630 --> 00:35:47,510
that we actually observe

835
00:35:47,510 --> 00:35:48,942
OK.

836
00:35:48,942 --> 00:35:50,650
So once you have your
pretrained network,

837
00:35:50,650 --> 00:35:52,240
what's the sort of
the default thing

838
00:35:52,240 --> 00:35:54,620
you do to take to use it, right?

839
00:35:54,620 --> 00:35:57,340
And if you take anything away
from this lecture in terms

840
00:35:57,340 --> 00:35:58,960
of just like
engineering practices

841
00:35:58,960 --> 00:36:01,720
that will be broadly
useful to you as you go off

842
00:36:01,720 --> 00:36:05,650
and build things and
study things maybe

843
00:36:05,650 --> 00:36:09,250
as a machine learning engineer,
or a computational social

844
00:36:09,250 --> 00:36:12,310
scientist or et cetera,
what people tend to do,

845
00:36:12,310 --> 00:36:15,520
is you pretrain your network
on just a lot of data.

846
00:36:15,520 --> 00:36:18,170
Lots of text, learn
very general things.

847
00:36:18,170 --> 00:36:22,720
And then, you adapt the network
to whatever you wanted to do.

848
00:36:22,720 --> 00:36:24,950
So we had a bunch
of pretraining data,

849
00:36:24,950 --> 00:36:26,680
and then maybe this
is a movie review

850
00:36:26,680 --> 00:36:29,020
that we're taking as
input here, and we just

851
00:36:29,020 --> 00:36:33,850
apply the decoder that we
sort of pretrained right.

852
00:36:33,850 --> 00:36:37,150
Start the parameters
there, and then

853
00:36:37,150 --> 00:36:41,500
fine tune it on whatever we
were sort of wanting to do.

854
00:36:41,500 --> 00:36:43,760
Maybe this is a
sentiment analysis task,

855
00:36:43,760 --> 00:36:47,452
so we run the whole sequence
through the decoder, right?

856
00:36:47,452 --> 00:36:49,660
Get a hidden state at the
end of the very last thing,

857
00:36:49,660 --> 00:36:53,233
and then we predict maybe
plus or minus sentiment.

858
00:36:53,233 --> 00:36:55,900
And this is sort of adapting the
pretrained network to the task.

859
00:36:55,900 --> 00:37:01,360
So this pretrain, fine tune
paradigm is wildly successful

860
00:37:01,360 --> 00:37:05,200
and you should really try it
whenever you're doing any NLP

861
00:37:05,200 --> 00:37:08,830
task nowadays effectively.

862
00:37:08,830 --> 00:37:10,960
Because this tends to be what--

863
00:37:10,960 --> 00:37:14,820
some variant of this tends
to be what works best.

864
00:37:14,820 --> 00:37:16,680
OK, so we've got a
technical note now.

865
00:37:16,680 --> 00:37:19,770


866
00:37:19,770 --> 00:37:24,600
So if you don't like to think
about optimization, or gradient

867
00:37:24,600 --> 00:37:27,660
descent, maybe take
a pass on this slide,

868
00:37:27,660 --> 00:37:30,300
but I encourage you to just
think for a second about,

869
00:37:30,300 --> 00:37:34,296
like why should this help?

870
00:37:34,296 --> 00:37:37,350
Training neural nets, we're
using gradient descent

871
00:37:37,350 --> 00:37:40,170
to try to find some
global minimum, right,

872
00:37:40,170 --> 00:37:43,230
of this optimism-- of
this loss function,

873
00:37:43,230 --> 00:37:47,080
and we're sort of doing
this in two steps.

874
00:37:47,080 --> 00:37:50,610
And the first step is we
have-- we get some parameters,

875
00:37:50,610 --> 00:37:55,470
theta hat, by approximating
min over our--

876
00:37:55,470 --> 00:37:57,890
sorry theta is the parameters
of the neural network.

877
00:37:57,890 --> 00:38:00,953
So all of the KQV vectors
in our transformer,

878
00:38:00,953 --> 00:38:03,120
right, the word embeddings,
the position embeddings,

879
00:38:03,120 --> 00:38:06,910
it's just all of the parameters
of our neural network.

880
00:38:06,910 --> 00:38:09,660
And so, we're doing min over
all the parameters of our theta

881
00:38:09,660 --> 00:38:11,910
we're trying to approximate,
min over the parameters

882
00:38:11,910 --> 00:38:14,220
of our neural network of
our pretraining loss, which

883
00:38:14,220 --> 00:38:17,760
here was language modeling
of our parameters.

884
00:38:17,760 --> 00:38:21,300
And we just get this
sort of estimate

885
00:38:21,300 --> 00:38:23,790
of some parameters theta hat.

886
00:38:23,790 --> 00:38:27,270
And then we fine
tune by approximating

887
00:38:27,270 --> 00:38:31,380
this, min over theta of the
fine tune loss, maybe that's

888
00:38:31,380 --> 00:38:32,700
sentiment, right?

889
00:38:32,700 --> 00:38:33,940
Starting at theta hat.

890
00:38:33,940 --> 00:38:36,210
So we initialize our gradient
descent at theta hat.

891
00:38:36,210 --> 00:38:38,460
And then we just sort of
like let it do what it wants.

892
00:38:38,460 --> 00:38:42,870
And it's just like,
it just works.

893
00:38:42,870 --> 00:38:46,440
In part it has to
be, because something

894
00:38:46,440 --> 00:38:49,020
about where we start
is so important,

895
00:38:49,020 --> 00:38:52,110
not just in terms of sort of
gradient flow, although that

896
00:38:52,110 --> 00:38:57,120
is a big part of it, but also it
seems like stochastic gradient

897
00:38:57,120 --> 00:39:01,560
descent sticks relatively
close to that pretraining

898
00:39:01,560 --> 00:39:03,880
initialization
during fine tuning.

899
00:39:03,880 --> 00:39:07,170
This is something that we seem
to observe in practice, right,

900
00:39:07,170 --> 00:39:11,100
that somehow the locality of
stochastic gradient descent,

901
00:39:11,100 --> 00:39:14,010
finding local minima that are
close to this theta hat that

902
00:39:14,010 --> 00:39:18,090
was good for such a general
problem of language modeling.

903
00:39:18,090 --> 00:39:21,270
It seems like, yeah, the local
minima of the fine tuning loss,

904
00:39:21,270 --> 00:39:22,390
because we don't find--

905
00:39:22,390 --> 00:39:22,890
oh, yeah.

906
00:39:22,890 --> 00:39:24,557
The local minima of
the fine tuning loss

907
00:39:24,557 --> 00:39:27,918
tend to generalize well, when
they're near to this theta hat

908
00:39:27,918 --> 00:39:28,710
that we pretrained.

909
00:39:28,710 --> 00:39:30,918
And this is sort of a mystery
that we're still trying

910
00:39:30,918 --> 00:39:32,440
to figure out more about.

911
00:39:32,440 --> 00:39:34,290
And then also, yeah,
maybe the gradients.

912
00:39:34,290 --> 00:39:36,582
Right, the gradients of the
fine tuning loss near theta

913
00:39:36,582 --> 00:39:39,090
propagate nicely so our
network training goes really

914
00:39:39,090 --> 00:39:41,000
well as well.

915
00:39:41,000 --> 00:39:41,500
OK.

916
00:39:41,500 --> 00:39:45,780
So this is something to chew
on, but in practice it works.

917
00:39:45,780 --> 00:39:49,520
I think it's just still
fascinating that it works.

918
00:39:49,520 --> 00:39:50,180
OK.

919
00:39:50,180 --> 00:39:56,460
So we've talked about mainly
the transformer encoder-decoder.

920
00:39:56,460 --> 00:39:59,850
And in fact, right,
I said that we

921
00:39:59,850 --> 00:40:01,530
could have just sort
of the left hand

922
00:40:01,530 --> 00:40:05,160
side encoders to be
pretrained, or just

923
00:40:05,160 --> 00:40:08,310
decoders to be pretrained,
or encoder-decoders.

924
00:40:08,310 --> 00:40:10,800
And there are actually
really popular sort

925
00:40:10,800 --> 00:40:13,450
of famous models in each
of these three categories.

926
00:40:13,450 --> 00:40:15,780
The kinds of
pretraining you can do,

927
00:40:15,780 --> 00:40:21,840
and the kinds of applications or
uses of those pretrained models

928
00:40:21,840 --> 00:40:24,600
that are most natural,
actually depend strongly

929
00:40:24,600 --> 00:40:28,380
on whether you choose to
pretrain an encoder, a decoder,

930
00:40:28,380 --> 00:40:31,195
or an encoder-decoder.

931
00:40:31,195 --> 00:40:32,570
And so, I think
it's useful as we

932
00:40:32,570 --> 00:40:36,830
go through some of these
popular sort of model names

933
00:40:36,830 --> 00:40:39,833
that you need to know,
and what the sort of, what

934
00:40:39,833 --> 00:40:41,750
their innovations were
to actually split it up

935
00:40:41,750 --> 00:40:44,260
into these categories.

936
00:40:44,260 --> 00:40:46,687
So we've already--
so here's the thing.

937
00:40:46,687 --> 00:40:48,270
We're going to go
through these three,

938
00:40:48,270 --> 00:40:51,420
and they all have sort of
benefits, and in some sense

939
00:40:51,420 --> 00:40:52,335
drawbacks.

940
00:40:52,335 --> 00:40:56,085
So for the decoders, right?

941
00:40:56,085 --> 00:40:58,710
Really what we're talking about
here mainly is language models.

942
00:40:58,710 --> 00:41:00,085
And we've seen
this so far, we've

943
00:41:00,085 --> 00:41:02,370
talked about
pretrained decoders,

944
00:41:02,370 --> 00:41:04,420
and these are nice
to generate from.

945
00:41:04,420 --> 00:41:04,920
Right?

946
00:41:04,920 --> 00:41:07,830
So you can just sample from
your pretrained language model,

947
00:41:07,830 --> 00:41:09,540
and get things that
look like the text

948
00:41:09,540 --> 00:41:11,730
that you were pretraining on.

949
00:41:11,730 --> 00:41:13,680
But one problem
is that you can't

950
00:41:13,680 --> 00:41:15,030
condition on future words.

951
00:41:15,030 --> 00:41:15,530
Right?

952
00:41:15,530 --> 00:41:19,110
So we mentioned in our
modeling with LSTMs,

953
00:41:19,110 --> 00:41:20,970
that just like if you could--

954
00:41:20,970 --> 00:41:25,620
when you can do it, we said that
having a bidirectional LSTM,

955
00:41:25,620 --> 00:41:28,350
was actually just way more
useful than having a one

956
00:41:28,350 --> 00:41:29,120
directional LSTM.

957
00:41:29,120 --> 00:41:31,780
Well, it's sort of true
for transformers as well.

958
00:41:31,780 --> 00:41:35,910
So if you can see how the
arrows are pointing here,

959
00:41:35,910 --> 00:41:38,040
the arrows are pointing
up and to the right.

960
00:41:38,040 --> 00:41:42,570
So this word is sort of looking
back at its past history,

961
00:41:42,570 --> 00:41:48,700
but this word can't see, can't
contextualize with the future.

962
00:41:48,700 --> 00:41:51,720
Whereas in the encoder block
here in blue just below it,

963
00:41:51,720 --> 00:41:54,030
you sort of have all
pairs of interactions.

964
00:41:54,030 --> 00:41:55,905
And so, when you're
building representations,

965
00:41:55,905 --> 00:41:57,613
that can actually be
super useful to know

966
00:41:57,613 --> 00:41:58,770
what the future words are.

967
00:41:58,770 --> 00:42:00,690
So so that's what
encoders get you, right?

968
00:42:00,690 --> 00:42:02,582
You get bidirectional context.

969
00:42:02,582 --> 00:42:04,290
So you can condition
on the future, maybe

970
00:42:04,290 --> 00:42:06,748
that helps you build up better
representations of language,

971
00:42:06,748 --> 00:42:09,690
but the question that will
actually go through here

972
00:42:09,690 --> 00:42:11,918
is, well, how do
you pretrain them?

973
00:42:11,918 --> 00:42:13,710
You can't pretrain them
as language models,

974
00:42:13,710 --> 00:42:15,700
because you have
access to the future.

975
00:42:15,700 --> 00:42:18,930
So if you try to do that, the
loss will just immediately be

976
00:42:18,930 --> 00:42:21,000
0, because you can just
see what the future is,

977
00:42:21,000 --> 00:42:22,410
that's not useful.

978
00:42:22,410 --> 00:42:26,070
And then, we'll talk about
pretrained encoder-decoders,

979
00:42:26,070 --> 00:42:29,340
which like may be the
best of both worlds.

980
00:42:29,340 --> 00:42:33,180
But also maybe unclear what's
the best way to pretrain them,

981
00:42:33,180 --> 00:42:36,640
they definitely have
benefits for both.

982
00:42:36,640 --> 00:42:41,070
So let's get into
sort of some general--

983
00:42:41,070 --> 00:42:43,500
a more-- yeah, let's
get into decoders first,

984
00:42:43,500 --> 00:42:45,880
we'll go through all three.

985
00:42:45,880 --> 00:42:46,380
OK.

986
00:42:46,380 --> 00:42:52,105
So when we're pretraining
a language model,

987
00:42:52,105 --> 00:42:54,570
right, we're pretraining
it on this objective,

988
00:42:54,570 --> 00:42:57,220
we're trying to make it
approximate this probability

989
00:42:57,220 --> 00:43:01,060
of a word given all
of its previous words.

990
00:43:01,060 --> 00:43:02,692
What we end up
doing, and I showed

991
00:43:02,692 --> 00:43:05,150
this sort of pictographically,
but that's some math, right?

992
00:43:05,150 --> 00:43:09,400
We get a hidden state,
h1 to hT for each

993
00:43:09,400 --> 00:43:11,540
of the words in
the input w1 to wT,

994
00:43:11,540 --> 00:43:14,180
and remember words again
mean subwords here.

995
00:43:14,180 --> 00:43:14,680
OK.

996
00:43:14,680 --> 00:43:16,510
And then, when we're
fine tuning this,

997
00:43:16,510 --> 00:43:19,330
right, we can take
the representation--

998
00:43:19,330 --> 00:43:22,990
this should be hT
Aht plus b, and then

999
00:43:22,990 --> 00:43:24,400
the picture here is right here.

1000
00:43:24,400 --> 00:43:29,500
So hT, it's the very
last encoder state.

1001
00:43:29,500 --> 00:43:31,180
And now, this has sort of the--

1002
00:43:31,180 --> 00:43:33,430
it's seen all of
its history, right,

1003
00:43:33,430 --> 00:43:39,190
and so, you can apply a linear
layer here maybe multiplying it

1004
00:43:39,190 --> 00:43:42,910
by some parameters A and B,
that were not pretrained,

1005
00:43:42,910 --> 00:43:45,190
and then you're predicting
sentiment maybe.

1006
00:43:45,190 --> 00:43:47,620
Plus or minus
sentiment, perhaps.

1007
00:43:47,620 --> 00:43:49,573
And so, look at the
red and the gray,

1008
00:43:49,573 --> 00:43:51,490
so most of the parameters
of my neural network

1009
00:43:51,490 --> 00:43:54,820
have now been pretrained,
the very last layer that's

1010
00:43:54,820 --> 00:43:59,583
learning the sentiment
say decision has not

1011
00:43:59,583 --> 00:44:00,250
been pretrained.

1012
00:44:00,250 --> 00:44:01,930
So those have been
randomly initialized.

1013
00:44:01,930 --> 00:44:04,525
And when you take the loss
of the sentiment loss,

1014
00:44:04,525 --> 00:44:07,870
right, you train not just
the linear layer here,

1015
00:44:07,870 --> 00:44:09,970
but you actually back
propagate the gradients

1016
00:44:09,970 --> 00:44:13,030
all the way through the
entire pretrained network,

1017
00:44:13,030 --> 00:44:15,930
and fine tune all
of those parameters.

1018
00:44:15,930 --> 00:44:16,430
Right?

1019
00:44:16,430 --> 00:44:18,222
So it's not like you're
just training this,

1020
00:44:18,222 --> 00:44:20,205
at fine tuning time,
this linear layer.

1021
00:44:20,205 --> 00:44:21,580
You're training
the whole network

1022
00:44:21,580 --> 00:44:25,580
as a function of this
fine tuning loss.

1023
00:44:25,580 --> 00:44:29,990
And maybe it's bad that the
linear layer wasn't pretrained,

1024
00:44:29,990 --> 00:44:32,960
in the grand scheme of things,
it's not that many parameters

1025
00:44:32,960 --> 00:44:34,755
also.

1026
00:44:34,755 --> 00:44:36,380
So this is useful,
this is just one way

1027
00:44:36,380 --> 00:44:38,570
to interact with
pretrained models, right?

1028
00:44:38,570 --> 00:44:40,940
And so, what I want you
to take away from this

1029
00:44:40,940 --> 00:44:42,500
is that, there was
a contract that we

1030
00:44:42,500 --> 00:44:43,980
had with the original model.

1031
00:44:43,980 --> 00:44:44,480
Right?

1032
00:44:44,480 --> 00:44:46,370
The contract was
that it was defining

1033
00:44:46,370 --> 00:44:48,237
probability distributions.

1034
00:44:48,237 --> 00:44:49,820
But when we're fine
tuning, when we're

1035
00:44:49,820 --> 00:44:52,430
interacting with the pretrained
model, what we also have

1036
00:44:52,430 --> 00:44:54,560
are just like the trained
weights, and the network

1037
00:44:54,560 --> 00:44:55,370
architecture.

1038
00:44:55,370 --> 00:44:57,320
We don't need to use
it as a language model,

1039
00:44:57,320 --> 00:44:59,570
we don't need to use it as
a probability distribution.

1040
00:44:59,570 --> 00:45:02,510
When we're actually fine tuning
it, we're really just using it

1041
00:45:02,510 --> 00:45:05,300
for its initialization of
its parameters, and saying,

1042
00:45:05,300 --> 00:45:08,420
oh, this is just a
transformer decoder

1043
00:45:08,420 --> 00:45:10,460
that was pretrained by--

1044
00:45:10,460 --> 00:45:12,290
and it happens to
be really great,

1045
00:45:12,290 --> 00:45:15,290
in that when you fine tune
on some sentiment data

1046
00:45:15,290 --> 00:45:17,840
it does a really good job.

1047
00:45:17,840 --> 00:45:20,540
OK, but there's a second way
to interact with decoder--

1048
00:45:20,540 --> 00:45:21,860
with pretrained decoders.

1049
00:45:21,860 --> 00:45:24,080
Which is in some sense
even more natural,

1050
00:45:24,080 --> 00:45:28,050
it actually is closer to the
contract that we started with.

1051
00:45:28,050 --> 00:45:30,590
So we don't have to just
ignore the fact that it

1052
00:45:30,590 --> 00:45:32,690
was a probability
distribution entirely,

1053
00:45:32,690 --> 00:45:35,400
we can make use of it
while still fine tuning it.

1054
00:45:35,400 --> 00:45:37,100
So here's what
we're going to do.

1055
00:45:37,100 --> 00:45:40,310
So we can use them as a
generator at fine tuning time.

1056
00:45:40,310 --> 00:45:44,240
By generator, I mean, it's going
to define this distribution

1057
00:45:44,240 --> 00:45:47,120
of words given their contexts.

1058
00:45:47,120 --> 00:45:49,400
And then we'll
actually just fine tune

1059
00:45:49,400 --> 00:45:51,870
that probability distribution.

1060
00:45:51,870 --> 00:45:56,210
So in a task like some kind
of turn-based dialogue,

1061
00:45:56,210 --> 00:46:01,550
we might encode the dialogue
history as your past context.

1062
00:46:01,550 --> 00:46:05,113
Right, so you have a dialogue
history of some things

1063
00:46:05,113 --> 00:46:07,280
that people are saying back
and forth to each other,

1064
00:46:07,280 --> 00:46:09,230
you encode it as
words, and you try

1065
00:46:09,230 --> 00:46:11,810
to predict the next
words in the dialogue.

1066
00:46:11,810 --> 00:46:13,730
Right, and maybe your
pretraining objective,

1067
00:46:13,730 --> 00:46:16,550
you looked at very general
purpose text from I don't know,

1068
00:46:16,550 --> 00:46:18,350
Wikipedia, or
books, or something.

1069
00:46:18,350 --> 00:46:20,750
And you're fine tuning
it as a language model,

1070
00:46:20,750 --> 00:46:23,360
but you're fine tuning
as a language model

1071
00:46:23,360 --> 00:46:26,870
on this sort of domain-specific
distribution of text,

1072
00:46:26,870 --> 00:46:29,390
like dialogue, or maybe
summarization, where

1073
00:46:29,390 --> 00:46:33,140
you paste in the whole
document and then say

1074
00:46:33,140 --> 00:46:35,840
a specific word, and
then the summary,

1075
00:46:35,840 --> 00:46:37,700
and say predict the summary.

1076
00:46:37,700 --> 00:46:39,750
And so what this
looks like is, again,

1077
00:46:39,750 --> 00:46:42,110
at fine tuning time here.

1078
00:46:42,110 --> 00:46:45,860
You have your h1 to hT is equal
to the decoder of the words.

1079
00:46:45,860 --> 00:46:48,800
And then, you have
this distribution

1080
00:46:48,800 --> 00:46:53,240
that you're fine tuning, of wT
is Ah, there's a typo again,

1081
00:46:53,240 --> 00:46:55,130
ht minus 1 plus b.

1082
00:46:55,130 --> 00:46:58,790
So now, every time I have this--

1083
00:46:58,790 --> 00:47:00,470
I'm predicting these
words-- from word 1

1084
00:47:00,470 --> 00:47:03,310
I predict word 2, word 2 I
predict word 3, et cetera,

1085
00:47:03,310 --> 00:47:04,040
right?

1086
00:47:04,040 --> 00:47:06,230
The actual last
layer of the network

1087
00:47:06,230 --> 00:47:08,540
unlike before, the last
layer of the network

1088
00:47:08,540 --> 00:47:10,070
has been pretrained.

1089
00:47:10,070 --> 00:47:12,230
But I'm still fine
tuning the whole thing.

1090
00:47:12,230 --> 00:47:14,630
Right, so A and B
here are mapping

1091
00:47:14,630 --> 00:47:16,550
to sort of a
probability distribution

1092
00:47:16,550 --> 00:47:19,640
over my vocabulary, or the
logits of a probability

1093
00:47:19,640 --> 00:47:20,390
distribution.

1094
00:47:20,390 --> 00:47:23,485
And I just get to sort
of like tweak them now.

1095
00:47:23,485 --> 00:47:24,860
In order to have
the distribution

1096
00:47:24,860 --> 00:47:28,310
that I'm going to use, reflect
the thing like dialogue

1097
00:47:28,310 --> 00:47:29,420
that I want it to reflect.

1098
00:47:29,420 --> 00:47:32,370


1099
00:47:32,370 --> 00:47:32,870
OK.

1100
00:47:32,870 --> 00:47:34,430
So those are two
ways of interacting

1101
00:47:34,430 --> 00:47:36,770
with a pretrained decoder.

1102
00:47:36,770 --> 00:47:39,560
Now, here's an example
of what is ended up

1103
00:47:39,560 --> 00:47:43,460
being the first of
your line of wildly

1104
00:47:43,460 --> 00:47:49,010
successful, or at least talked
about, pretrained decoders.

1105
00:47:49,010 --> 00:47:54,530
So the generative
pretrained decoder, GPT,

1106
00:47:54,530 --> 00:47:57,740
was a huge success
in some sense,

1107
00:47:57,740 --> 00:48:00,410
or at least it
got a lot of buzz.

1108
00:48:00,410 --> 00:48:04,250
So it's a transformer decoder,
no encoder, with 12 layers.

1109
00:48:04,250 --> 00:48:05,900
I'm giving you
the details so you

1110
00:48:05,900 --> 00:48:08,660
can start to get a feeling
for how the size of things

1111
00:48:08,660 --> 00:48:13,460
changes over the years as we'll
continue to progress here.

1112
00:48:13,460 --> 00:48:18,080
Had each of-- each of the hidden
states with dimensionality 768.

1113
00:48:18,080 --> 00:48:19,770
So if you remember
back to last lecture,

1114
00:48:19,770 --> 00:48:24,530
we had a term D, which was our
dimensionality, so D is 768.

1115
00:48:24,530 --> 00:48:26,930
And then, an
interesting statement

1116
00:48:26,930 --> 00:48:29,622
that you should keep in mind
for the engineering-minded folks

1117
00:48:29,622 --> 00:48:31,580
is that, the actual
feed-forward layers, right.

1118
00:48:31,580 --> 00:48:33,320
You've got a hidden
layer in the feed--

1119
00:48:33,320 --> 00:48:34,910
excuse me, in the
feed-forward layer,

1120
00:48:34,910 --> 00:48:36,775
and this is actually very large.

1121
00:48:36,775 --> 00:48:38,150
So you had these
sort of position

1122
00:48:38,150 --> 00:48:39,980
wise feed-forward layers, right?

1123
00:48:39,980 --> 00:48:44,570
And the feed-forward layer
would take the 768 dimensional

1124
00:48:44,570 --> 00:48:48,620
vector, sort of like project
it to 3,000 dimensional space

1125
00:48:48,620 --> 00:48:50,210
through the sort
of non-linearity,

1126
00:48:50,210 --> 00:48:52,280
and then project it back to 768.

1127
00:48:52,280 --> 00:48:54,980
This ends up being
because you can squash

1128
00:48:54,980 --> 00:48:57,200
a lot more parameters
in, for not too much more

1129
00:48:57,200 --> 00:49:01,160
compute in this way
but-- so that's curious.

1130
00:49:01,160 --> 00:49:03,800
OK, and then,
byte-pair encoding.

1131
00:49:03,800 --> 00:49:06,200
It's actually-- was this
one byte-pair encoding?

1132
00:49:06,200 --> 00:49:08,960
Was a subword vocabulary
with 40,000 merges.

1133
00:49:08,960 --> 00:49:11,780
So 40,000 merges, so that's
not the size of the vocabulary,

1134
00:49:11,780 --> 00:49:14,270
because you started with
a bunch of characters,

1135
00:49:14,270 --> 00:49:17,330
and I don't remember how many
characters they started with.

1136
00:49:17,330 --> 00:49:21,260
But so it's a relatively small
vocabulary you can see, right?

1137
00:49:21,260 --> 00:49:23,810
And compared to if
you tried to say,

1138
00:49:23,810 --> 00:49:26,915
have every word have a
unique representation.

1139
00:49:26,915 --> 00:49:28,790
Now, it's going to be
trained on BooksCorpus.

1140
00:49:28,790 --> 00:49:32,600
It's got 7,000 unique
books, and it contains

1141
00:49:32,600 --> 00:49:34,310
long spans of contiguous text.

1142
00:49:34,310 --> 00:49:37,160
So you have, instead
of say training it

1143
00:49:37,160 --> 00:49:40,730
on individual sentences,
just small short sentences,

1144
00:49:40,730 --> 00:49:43,730
the model is able to learn
long distance dependencies,

1145
00:49:43,730 --> 00:49:47,180
because you haven't split like
a book into random sentences,

1146
00:49:47,180 --> 00:49:48,590
and shuffled them all around.

1147
00:49:48,590 --> 00:49:50,460
You've sort of
kept it contiguous.

1148
00:49:50,460 --> 00:49:53,780
So it can have that
sort of consistency.

1149
00:49:53,780 --> 00:49:56,150
And then, a little
treat here, yeah.

1150
00:49:56,150 --> 00:49:58,250
So GPT never showed up
in the original paper,

1151
00:49:58,250 --> 00:50:00,872
or in the original blog
post like as an acronym.

1152
00:50:00,872 --> 00:50:02,330
And it could actually
sort of refer

1153
00:50:02,330 --> 00:50:04,927
to like, Generative
PreTraining, just

1154
00:50:04,927 --> 00:50:07,010
sort of what the title of
the paper would suggest,

1155
00:50:07,010 --> 00:50:09,415
or Generative
PreTrained Transformer.

1156
00:50:09,415 --> 00:50:11,540
And I sort of decided to
say, Generative PreTrained

1157
00:50:11,540 --> 00:50:15,170
Transformer because this
seemed like way too general.

1158
00:50:15,170 --> 00:50:17,640
So, GPT.

1159
00:50:17,640 --> 00:50:18,140
OK.

1160
00:50:18,140 --> 00:50:22,010
So they pretrained this huge
language model transformer,

1161
00:50:22,010 --> 00:50:25,280
this huge transformer
decoder, just on 7,000 books.

1162
00:50:25,280 --> 00:50:27,860
And they fine tuned it on a
number of different tasks.

1163
00:50:27,860 --> 00:50:30,318
And I want to talk a little
bit about the details about how

1164
00:50:30,318 --> 00:50:31,430
they fine tuned it.

1165
00:50:31,430 --> 00:50:35,420
And so, they fine tuned it on
one particular task, or family

1166
00:50:35,420 --> 00:50:38,160
of tasks, called Natural
Language Inference.

1167
00:50:38,160 --> 00:50:39,830
So in Natural
Language Inference,

1168
00:50:39,830 --> 00:50:41,810
we're labeling
pairs of sentences

1169
00:50:41,810 --> 00:50:44,720
as entailing, or contradictory
to each other or neutral.

1170
00:50:44,720 --> 00:50:46,920
So you have a premise,
and you hold the premise

1171
00:50:46,920 --> 00:50:48,080
that's sort of true.

1172
00:50:48,080 --> 00:50:51,710
"The man is in the doorway,"
and you have a hypothesis,

1173
00:50:51,710 --> 00:50:53,810
"the person is near the door."

1174
00:50:53,810 --> 00:50:57,320
If this person is
referring to that man,

1175
00:50:57,320 --> 00:50:59,450
then it's sort of
like, Oh, yeah.

1176
00:50:59,450 --> 00:51:02,330
So this is sort of entailed,
because there's a person,

1177
00:51:02,330 --> 00:51:03,770
because a man is a person.

1178
00:51:03,770 --> 00:51:06,410
And they're in the doorway,
then they are near the door.

1179
00:51:06,410 --> 00:51:10,570
So you have this sort of logical
reasoning that you're doing,

1180
00:51:10,570 --> 00:51:12,320
or you're supposed to
be able to be doing,

1181
00:51:12,320 --> 00:51:14,030
and you're labeling
these sentences.

1182
00:51:14,030 --> 00:51:15,860
So it's a labeled "task."

1183
00:51:15,860 --> 00:51:18,920
You've got sort of an input
that's cut into two parts,

1184
00:51:18,920 --> 00:51:20,920
and then one of three outputs.

1185
00:51:20,920 --> 00:51:21,420
OK.

1186
00:51:21,420 --> 00:51:25,140
So the GPT paper
evaluates on this task.

1187
00:51:25,140 --> 00:51:28,340
But what they've got is
a transformer decoder.

1188
00:51:28,340 --> 00:51:30,440
So what do they do?

1189
00:51:30,440 --> 00:51:36,020
This is sort of one of the
earlier examples of taking--

1190
00:51:36,020 --> 00:51:39,380
instead of changing your
neural network architecture

1191
00:51:39,380 --> 00:51:42,500
to adapt to the kind
of task you're doing,

1192
00:51:42,500 --> 00:51:47,060
you're going to just format the
task as like a bunch of tokens

1193
00:51:47,060 --> 00:51:49,580
and not change
your architecture.

1194
00:51:49,580 --> 00:51:51,890
Because the pretraining
was so useful.

1195
00:51:51,890 --> 00:51:54,770
It's probably better to
keep the architecture fixed,

1196
00:51:54,770 --> 00:51:57,480
pretrain it, and
then change the task

1197
00:51:57,480 --> 00:52:00,320
specification to sort of fit
the pretrained architecture.

1198
00:52:00,320 --> 00:52:03,320
So what they did right,
they put this token "START,"

1199
00:52:03,320 --> 00:52:04,970
this is a special token.

1200
00:52:04,970 --> 00:52:06,560
"The man is in the doorway."

1201
00:52:06,560 --> 00:52:08,848
Some delimiter token, right?

1202
00:52:08,848 --> 00:52:10,640
So this is just a linear
sequence of tokens

1203
00:52:10,640 --> 00:52:14,930
that we're giving as
one big prefix to GPT.

1204
00:52:14,930 --> 00:52:17,120
And then, "the person
is near the door,"

1205
00:52:17,120 --> 00:52:19,460
and then some extra token here.

1206
00:52:19,460 --> 00:52:21,380
Like "EXTRACT."

1207
00:52:21,380 --> 00:52:24,860
And then, the linear
classifier that we talked about

1208
00:52:24,860 --> 00:52:29,030
in sort of the first way
to interact with models,

1209
00:52:29,030 --> 00:52:31,970
with decoder models, it's
applied to the represent

1210
00:52:31,970 --> 00:52:34,430
of the EXTRACT token, right?

1211
00:52:34,430 --> 00:52:37,880
So you have the last hidden
state on top of extract,

1212
00:52:37,880 --> 00:52:39,800
and then you fine
tune the whole network

1213
00:52:39,800 --> 00:52:41,700
to predict these labels.

1214
00:52:41,700 --> 00:52:44,150
And so this sort
of input formatting

1215
00:52:44,150 --> 00:52:49,400
is increasingly used to keep
the model architecture the same,

1216
00:52:49,400 --> 00:52:51,740
and allow for a variety
of different problems

1217
00:52:51,740 --> 00:52:53,930
to be solved with it.

1218
00:52:53,930 --> 00:52:55,310
OK, and so, did it work?

1219
00:52:55,310 --> 00:52:58,280
On Natural Language
Inference, the answer is yes.

1220
00:52:58,280 --> 00:53:00,242
So there's a number of
different numbers here,

1221
00:53:00,242 --> 00:53:01,700
I wouldn't worry
too much about it.

1222
00:53:01,700 --> 00:53:03,650
The fine tuned
transformer language model

1223
00:53:03,650 --> 00:53:06,840
is sort of what you
should pay attention to.

1224
00:53:06,840 --> 00:53:09,350
There's a lot of effort that
went into the other models,

1225
00:53:09,350 --> 00:53:09,590
right?

1226
00:53:09,590 --> 00:53:11,507
And sort of, this is the
story of pretraining.

1227
00:53:11,507 --> 00:53:13,130
People put a lot of
effort into models

1228
00:53:13,130 --> 00:53:15,170
that do various sort
of careful things,

1229
00:53:15,170 --> 00:53:17,540
and then you take a
single transformer

1230
00:53:17,540 --> 00:53:20,840
and you say, I'm going to
pretrain it on a ton of text,

1231
00:53:20,840 --> 00:53:23,030
and not worry too much
about anything else,

1232
00:53:23,030 --> 00:53:27,810
and just fine tune it, and you
end up doing super, super well.

1233
00:53:27,810 --> 00:53:30,570
Sometimes not too much
better in the GPT case

1234
00:53:30,570 --> 00:53:34,260
than sort of the best known
state of the art methods,

1235
00:53:34,260 --> 00:53:35,828
but usually a little bit better.

1236
00:53:35,828 --> 00:53:37,620
And again, the amount
of effort, the amount

1237
00:53:37,620 --> 00:53:40,410
of task-specific effort that
you have to put into it,

1238
00:53:40,410 --> 00:53:42,560
is very low.

1239
00:53:42,560 --> 00:53:43,060
OK.

1240
00:53:43,060 --> 00:53:45,850
And so, what about the other way
of interacting with decoders?

1241
00:53:45,850 --> 00:53:48,100
Right, so we said that we
could interact with the code

1242
00:53:48,100 --> 00:53:50,210
just by sampling from
them, just by saying,

1243
00:53:50,210 --> 00:53:51,980
well, they're probability
distributions.

1244
00:53:51,980 --> 00:53:55,390
So we can use them in their
capacities as language models.

1245
00:53:55,390 --> 00:54:00,370
And so, GPT-2, this is just,
really just a bigger GPT.

1246
00:54:00,370 --> 00:54:02,170
Don't worry too much about it.

1247
00:54:02,170 --> 00:54:05,050
With larger hidden
units, more layers.

1248
00:54:05,050 --> 00:54:06,670
When it was trained
on more data,

1249
00:54:06,670 --> 00:54:10,900
it was shown to produce sort of
relatively convincing samples

1250
00:54:10,900 --> 00:54:11,960
of natural language.

1251
00:54:11,960 --> 00:54:14,530
So this is something that
went around Twitter a lot.

1252
00:54:14,530 --> 00:54:17,560
Right, so you have this
sort of contrived example

1253
00:54:17,560 --> 00:54:20,980
that probably didn't show
up in the training data that

1254
00:54:20,980 --> 00:54:24,610
has scientists discovering
a herd of unicorns,

1255
00:54:24,610 --> 00:54:31,028
and then they sort of sample
from almost the distribution

1256
00:54:31,028 --> 00:54:31,570
of the model.

1257
00:54:31,570 --> 00:54:36,870
They sort of give that model
some extra credit here.

1258
00:54:36,870 --> 00:54:38,650
They do something
called, truncating

1259
00:54:38,650 --> 00:54:40,450
the distribution of
the language models,

1260
00:54:40,450 --> 00:54:44,500
to sort of cut out
noise in GPT-2.

1261
00:54:44,500 --> 00:54:46,300
So it's not exactly
a perfect sample,

1262
00:54:46,300 --> 00:54:51,820
but more or less
GPT-2 generated this.

1263
00:54:51,820 --> 00:54:54,580
And so, you have the scientists
discovering unicorns.

1264
00:54:54,580 --> 00:54:57,730
And then, you have this
consistency, OK, there's

1265
00:54:57,730 --> 00:55:03,340
the scientist, you have
them giving it a name.

1266
00:55:03,340 --> 00:55:08,110
You have-- you refer
back to this well--

1267
00:55:08,110 --> 00:55:11,050
yeah, you refer back to
the scientist's name,

1268
00:55:11,050 --> 00:55:13,270
you sort of have these like
topic consistency things,

1269
00:55:13,270 --> 00:55:16,550
also the syntax is really good.

1270
00:55:16,550 --> 00:55:19,273
It looks vaguely like
English, and so this is sort

1271
00:55:19,273 --> 00:55:20,440
of continuing to be a trend.

1272
00:55:20,440 --> 00:55:21,970
As we get larger and
larger language models,

1273
00:55:21,970 --> 00:55:23,830
we actually sample
from them, even when

1274
00:55:23,830 --> 00:55:26,110
we give them prompts
that look sort of odd,

1275
00:55:26,110 --> 00:55:29,740
and they seem to be
increasingly convincing.

1276
00:55:29,740 --> 00:55:31,450
OK.

1277
00:55:31,450 --> 00:55:35,530
So pretraining encoders.

1278
00:55:35,530 --> 00:55:37,660
OK, pretraining encoders.

1279
00:55:37,660 --> 00:55:40,390
Let's take another second
because I need some more water

1280
00:55:40,390 --> 00:55:41,552
here.

1281
00:55:41,552 --> 00:55:43,260
If there's another
question, let me know.

1282
00:55:43,260 --> 00:55:50,030


1283
00:55:50,030 --> 00:55:50,530
All right.

1284
00:55:50,530 --> 00:55:53,200


1285
00:55:53,200 --> 00:55:56,890
So the benefit of encoders
that we talked about

1286
00:55:56,890 --> 00:55:58,900
was that, they get this
bidirectional context.

1287
00:55:58,900 --> 00:56:01,660
So you can-- while you're
building representations

1288
00:56:01,660 --> 00:56:04,900
of your sentence, of
your parts of sentences,

1289
00:56:04,900 --> 00:56:06,460
you can look to the
future, and that

1290
00:56:06,460 --> 00:56:08,770
can help you build a better
representation of the word

1291
00:56:08,770 --> 00:56:10,240
that you're looking
at right now.

1292
00:56:10,240 --> 00:56:12,910
But the big problem is that we
can't do language modeling now.

1293
00:56:12,910 --> 00:56:14,690
So we've pretty
much only said we

1294
00:56:14,690 --> 00:56:16,990
like we've relied on these
tasks that we already

1295
00:56:16,990 --> 00:56:19,550
knew about language modeling
to do our pretraining.

1296
00:56:19,550 --> 00:56:21,520
But now we want to
pretrain encoders,

1297
00:56:21,520 --> 00:56:23,305
and so, we can't use it.

1298
00:56:23,305 --> 00:56:24,430
So what are we going to do?

1299
00:56:24,430 --> 00:56:27,010


1300
00:56:27,010 --> 00:56:29,020
Here's the solution
that was come up

1301
00:56:29,020 --> 00:56:33,490
with in a paper that introduced
the language model of the model

1302
00:56:33,490 --> 00:56:34,810
called BERT.

1303
00:56:34,810 --> 00:56:36,820
It's called masked
language modeling.

1304
00:56:36,820 --> 00:56:39,200
So here's the idea.

1305
00:56:39,200 --> 00:56:41,260
We get the sentence
and then we just

1306
00:56:41,260 --> 00:56:43,900
take a fraction of the
words, and we replace them

1307
00:56:43,900 --> 00:56:46,090
with a sort of a masked token.

1308
00:56:46,090 --> 00:56:50,020
A token that's-- that means you
don't know what this is right

1309
00:56:50,020 --> 00:56:50,530
now.

1310
00:56:50,530 --> 00:56:52,723
And then you
predict these words.

1311
00:56:52,723 --> 00:56:54,640
Some details we'll get
into in the next slide.

1312
00:56:54,640 --> 00:56:56,590
But so here's what
it looks like, right?

1313
00:56:56,590 --> 00:57:01,098
We have the sentence,
I mask to the mask.

1314
00:57:01,098 --> 00:57:03,140
We get some hidden states
for all of them, right?

1315
00:57:03,140 --> 00:57:06,370
So we haven't trained-- we
haven't changed the transformer

1316
00:57:06,370 --> 00:57:08,380
encoder at all.

1317
00:57:08,380 --> 00:57:10,605
We've just said, OK, here
is like this sequence.

1318
00:57:10,605 --> 00:57:11,980
You get to see
everything, right?

1319
00:57:11,980 --> 00:57:13,790
Look at all the arrows
going everywhere.

1320
00:57:13,790 --> 00:57:18,070
But then, we have
this prediction layer,

1321
00:57:18,070 --> 00:57:20,170
that we were pretraining, right?

1322
00:57:20,170 --> 00:57:24,190
And we're using it, we
only have loss on the words

1323
00:57:24,190 --> 00:57:26,170
where we had masks here.

1324
00:57:26,170 --> 00:57:28,300
So I had this masked,
and then I have

1325
00:57:28,300 --> 00:57:29,780
to predict that
it was "went" that

1326
00:57:29,780 --> 00:57:32,650
went here, and "store"
that went here.

1327
00:57:32,650 --> 00:57:34,900
And now, this is a lot
like language modeling

1328
00:57:34,900 --> 00:57:35,855
you might say.

1329
00:57:35,855 --> 00:57:38,230
But now you don't need to have
this sort of left to right

1330
00:57:38,230 --> 00:57:40,713
decomposition,
you're saying, I'm

1331
00:57:40,713 --> 00:57:42,130
going to remove
some of the words,

1332
00:57:42,130 --> 00:57:44,178
and you have to
predict what they are.

1333
00:57:44,178 --> 00:57:45,970
So this is called masked
language modeling.

1334
00:57:45,970 --> 00:57:49,630
And it's been very, very, very
effective with a quick caveat,

1335
00:57:49,630 --> 00:57:51,260
it gets a little
more complicated.

1336
00:57:51,260 --> 00:57:54,280
So what did they actually do?

1337
00:57:54,280 --> 00:57:56,283
They proposed masked
language modeling,

1338
00:57:56,283 --> 00:57:58,825
and they released the weights
of this pretrained transformer,

1339
00:57:58,825 --> 00:58:01,190
but with a little
bit more complexity

1340
00:58:01,190 --> 00:58:03,400
so we get masked language
modeling to work.

1341
00:58:03,400 --> 00:58:07,060
So you are going to
take a random 15%

1342
00:58:07,060 --> 00:58:09,100
of the subword tokens.

1343
00:58:09,100 --> 00:58:10,660
So that was true.

1344
00:58:10,660 --> 00:58:14,260
But you're not always going
to replace them with mask.

1345
00:58:14,260 --> 00:58:18,400
You can think of it like, if
the model sees a mask token,

1346
00:58:18,400 --> 00:58:21,490
it gets a guarantee that it
needs to predict something,

1347
00:58:21,490 --> 00:58:23,980
and if the model doesn't
see a mask token,

1348
00:58:23,980 --> 00:58:25,810
it gets a guarantee
that it doesn't

1349
00:58:25,810 --> 00:58:26,980
need to predict anything.

1350
00:58:26,980 --> 00:58:30,100
So why should it bother
building strong representations

1351
00:58:30,100 --> 00:58:32,820
of the words that aren't masked?

1352
00:58:32,820 --> 00:58:34,950
And I want my model to
build strong representations

1353
00:58:34,950 --> 00:58:36,400
of everything.

1354
00:58:36,400 --> 00:58:38,990
So we're going to add some sort
of uncertainty to the model.

1355
00:58:38,990 --> 00:58:41,460
So what we're going to do
is, for those 15% of tokens,

1356
00:58:41,460 --> 00:58:44,670
80% of the time, we're going
to replace it with a mask.

1357
00:58:44,670 --> 00:58:48,180
That was our original idea
of masked language modeling.

1358
00:58:48,180 --> 00:58:50,190
Then 10% of the
time, we're actually

1359
00:58:50,190 --> 00:58:52,680
going to replace the word
with just a random token, just

1360
00:58:52,680 --> 00:58:55,832
a random vocabulary
item, can be anything.

1361
00:58:55,832 --> 00:58:57,540
And then the other
10% of the time, we're

1362
00:58:57,540 --> 00:58:59,620
going to leave the
word unchanged.

1363
00:58:59,620 --> 00:59:04,950
So now, when it sees a word,
it could be a random token,

1364
00:59:04,950 --> 00:59:07,700
or it could be unchanged.

1365
00:59:07,700 --> 00:59:10,870
Then if I see a mask, I
know I need to predict it.

1366
00:59:10,870 --> 00:59:13,050
So what these two
things do here is say,

1367
00:59:13,050 --> 00:59:15,320
you have to sort
of be doing this--

1368
00:59:15,320 --> 00:59:17,820
you have to be on your
toes for every word

1369
00:59:17,820 --> 00:59:18,820
in your representation.

1370
00:59:18,820 --> 00:59:21,520
So here, "I pizza to the mask."

1371
00:59:21,520 --> 00:59:22,020
Right?

1372
00:59:22,020 --> 00:59:23,895
It turns out, and the
model didn't know this,

1373
00:59:23,895 --> 00:59:28,060
but it's getting three loss
terms for the sentence.

1374
00:59:28,060 --> 00:59:29,640
It only has one
mask, but it's going

1375
00:59:29,640 --> 00:59:32,650
to be penalized for predicting
three different things.

1376
00:59:32,650 --> 00:59:35,790
It needs to predict that this
word is actually, "went," so I

1377
00:59:35,790 --> 00:59:37,245
replaced this one.

1378
00:59:37,245 --> 00:59:40,090
It needs to predict that this
word is "to," is in fact,

1379
00:59:40,090 --> 00:59:42,330
the word "to,"
and then, it needs

1380
00:59:42,330 --> 00:59:46,350
to predict that this
word is in fact "store."

1381
00:59:46,350 --> 00:59:48,390
Now as a short interlude,
you might be thinking,

1382
00:59:48,390 --> 00:59:51,015
maybe thinking, John, there's no
way the model could know this,

1383
00:59:51,015 --> 00:59:53,470
it's so under-specified.

1384
00:59:53,470 --> 00:59:53,970
OK.

1385
00:59:53,970 --> 00:59:55,785
"I pizza," is a
little weird, I admit.

1386
00:59:55,785 --> 00:59:58,410
But there's just no way to know
that there's a store, and went,

1387
00:59:58,410 --> 01:00:00,690
and to, I mean, the same thing
is true of language modeling,

1388
01:00:00,690 --> 01:00:01,190
right?

1389
01:00:01,190 --> 01:00:03,900
So it's going to end up learning
these average statistics

1390
01:00:03,900 --> 01:00:06,600
about what things tend
to be in a given context,

1391
01:00:06,600 --> 01:00:08,972
and it's going to sort
of hedge its bets,

1392
01:00:08,972 --> 01:00:11,430
and try to build a distribution
of what things could appear

1393
01:00:11,430 --> 01:00:11,935
there.

1394
01:00:11,935 --> 01:00:14,310
So for the people who are
thinking that if there was any,

1395
01:00:14,310 --> 01:00:15,300
that's what you
should be thinking.

1396
01:00:15,300 --> 01:00:17,550
It has to sort of know
what kinds of things

1397
01:00:17,550 --> 01:00:20,340
will end up in these slots, it
has other uncertainty, right,

1398
01:00:20,340 --> 01:00:22,590
because it can't be sure
that any of the other words

1399
01:00:22,590 --> 01:00:25,500
are necessarily right.

1400
01:00:25,500 --> 01:00:29,690
And then, it's predicting
these three words.

1401
01:00:29,690 --> 01:00:30,240
OK.

1402
01:00:30,240 --> 01:00:35,060
And so, that's-- you can see why
it's important to not just have

1403
01:00:35,060 --> 01:00:38,270
masks potentially, to have these
sort of token randomization

1404
01:00:38,270 --> 01:00:41,270
things, because again we don't
actually care about its ability

1405
01:00:41,270 --> 01:00:42,660
to predict the masks.

1406
01:00:42,660 --> 01:00:43,160
Right?

1407
01:00:43,160 --> 01:00:45,350
Like I'm not going
to usually, I'm

1408
01:00:45,350 --> 01:00:48,260
not going to actually sample
from the models distribution

1409
01:00:48,260 --> 01:00:50,480
over what should go here.

1410
01:00:50,480 --> 01:00:54,260
Instead, I am going
to use the parameters

1411
01:00:54,260 --> 01:00:55,820
of the neural
network, and expect

1412
01:00:55,820 --> 01:00:58,310
that it built strong
representations of language.

1413
01:00:58,310 --> 01:00:59,810
So I don't want it
to think it's got

1414
01:00:59,810 --> 01:01:01,940
a free pass for
representing something,

1415
01:01:01,940 --> 01:01:05,630
if it doesn't have a mask there.

1416
01:01:05,630 --> 01:01:06,210
OK.

1417
01:01:06,210 --> 01:01:13,050
So there was one extra thing
with the BERT pretraining,

1418
01:01:13,050 --> 01:01:15,440
which is a next sentence
prediction objective.

1419
01:01:15,440 --> 01:01:17,540
So the inputs of
BERT looks like this.

1420
01:01:17,540 --> 01:01:19,130
This is straight
from the BERT paper.

1421
01:01:19,130 --> 01:01:23,090
You have a label here
before your first sentence.

1422
01:01:23,090 --> 01:01:25,440
And then a separation, and
then a second sentence.

1423
01:01:25,440 --> 01:01:29,690
So you had all these two
contiguous chunks of text.

1424
01:01:29,690 --> 01:01:33,260
You had the first chunk of
text here, "my dog is cute."

1425
01:01:33,260 --> 01:01:36,120
And then a second chunk of
text, "he likes playing."

1426
01:01:36,120 --> 01:01:36,620
Right?

1427
01:01:36,620 --> 01:01:37,912
And you see the subwords there.

1428
01:01:37,912 --> 01:01:42,350
And now, these would actually
be both be much longer, right?

1429
01:01:42,350 --> 01:01:45,550
So this whole thing
would be 512 words,

1430
01:01:45,550 --> 01:01:47,940
this would be about half,
and that would be about half,

1431
01:01:47,940 --> 01:01:51,680
and they'd be contiguous
chunks of text.

1432
01:01:51,680 --> 01:01:52,820
But here is the deal.

1433
01:01:52,820 --> 01:01:55,220
What they wanted to
do was they wanted

1434
01:01:55,220 --> 01:01:57,740
to try to teach the
system to understand

1435
01:01:57,740 --> 01:02:01,580
sort of relationships between
different whole pieces of text.

1436
01:02:01,580 --> 01:02:03,980
In order to better
pretrain for objectives--

1437
01:02:03,980 --> 01:02:06,710
for downstream applications,
like question answering,

1438
01:02:06,710 --> 01:02:10,010
where you have two pretty
different pieces of text,

1439
01:02:10,010 --> 01:02:12,330
and you need to know how
they relate to each other.

1440
01:02:12,330 --> 01:02:16,550
So the objective they came up
with was, you should sometimes,

1441
01:02:16,550 --> 01:02:18,620
have the second chunk of text.

1442
01:02:18,620 --> 01:02:22,670
The actual chunk of text
that directly follows

1443
01:02:22,670 --> 01:02:27,020
the first in your data
set, and sometimes

1444
01:02:27,020 --> 01:02:30,697
have the second check of
text be randomly sampled

1445
01:02:30,697 --> 01:02:31,530
from somewhere else.

1446
01:02:31,530 --> 01:02:32,780
So unrelated.

1447
01:02:32,780 --> 01:02:35,600
And the model should
predict whether it's

1448
01:02:35,600 --> 01:02:38,480
the first case or the second
in order, again, to sort of,

1449
01:02:38,480 --> 01:02:40,730
have to reason about the
relationships between the two

1450
01:02:40,730 --> 01:02:42,420
chunks of text.

1451
01:02:42,420 --> 01:02:44,193
So this is next
sentence prediction.

1452
01:02:44,193 --> 01:02:46,610
I think it's important to think
about, because again, it's

1453
01:02:46,610 --> 01:02:49,520
a very different idea
of pretraining objective

1454
01:02:49,520 --> 01:02:53,660
than language modeling and
masked language modeling.

1455
01:02:53,660 --> 01:02:55,640
Even though, later
work sort of argued

1456
01:02:55,640 --> 01:02:59,690
that in the case of BERT,
it's not necessary or useful.

1457
01:02:59,690 --> 01:03:01,190
And one of the
arguments is actually

1458
01:03:01,190 --> 01:03:07,010
because it's actually way better
to have a single context that's

1459
01:03:07,010 --> 01:03:08,130
twice as long.

1460
01:03:08,130 --> 01:03:11,090
So you can learn even
longer distance dependencies

1461
01:03:11,090 --> 01:03:12,030
and things.

1462
01:03:12,030 --> 01:03:13,550
And so, whether the
objective itself

1463
01:03:13,550 --> 01:03:16,400
would be useful if you could
always just double the context

1464
01:03:16,400 --> 01:03:18,625
size, I'm not sure if anyone's
done research on that.

1465
01:03:18,625 --> 01:03:20,750
But again, it's like a
different kind of objective.

1466
01:03:20,750 --> 01:03:23,390
And it's still noising something
about the input, right?

1467
01:03:23,390 --> 01:03:26,480
The input was this
big chunk of text.

1468
01:03:26,480 --> 01:03:28,317
And you've noised it
to say like, now you

1469
01:03:28,317 --> 01:03:29,900
don't know whether
it really was that,

1470
01:03:29,900 --> 01:03:32,840
or whether you sort of replaced
it with a bunch of garbage.

1471
01:03:32,840 --> 01:03:35,300
This sort of second
portion here.

1472
01:03:35,300 --> 01:03:38,720
Whether the second portion has
been replaced with something,

1473
01:03:38,720 --> 01:03:39,302
yeah.

1474
01:03:39,302 --> 01:03:41,510
Has been replaced with
something that didn't actually

1475
01:03:41,510 --> 01:03:42,677
come from the same sequence.

1476
01:03:42,677 --> 01:03:46,120


1477
01:03:46,120 --> 01:03:46,620
OK.

1478
01:03:46,620 --> 01:03:49,112
So let's talk some
details about BERT.

1479
01:03:49,112 --> 01:03:51,960
So BERT had 12 or
24 layers depending

1480
01:03:51,960 --> 01:03:53,305
on BERT-base or BERT-large.

1481
01:03:53,305 --> 01:03:54,930
You'll probably use
one of these models

1482
01:03:54,930 --> 01:03:57,013
or one of the sort of
descendants of these models,

1483
01:03:57,013 --> 01:04:00,300
if you choose to do something
with the custom final project

1484
01:04:00,300 --> 01:04:04,380
potentially, or if you choose
the version of the default

1485
01:04:04,380 --> 01:04:05,880
final project.

1486
01:04:05,880 --> 01:04:10,260
And you had 600 or 1,000
dimension hidden states,

1487
01:04:10,260 --> 01:04:11,460
a bunch of attention heads.

1488
01:04:11,460 --> 01:04:13,627
So this is that multi-headed
attention you remember,

1489
01:04:13,627 --> 01:04:14,680
had a bunch of them.

1490
01:04:14,680 --> 01:04:18,270
So you're splitting all your
dimensions into those 16 heads,

1491
01:04:18,270 --> 01:04:20,940
and we're talking on the
order of a couple 100

1492
01:04:20,940 --> 01:04:22,980
million parameters.

1493
01:04:22,980 --> 01:04:25,980
At the time, right in
2018, we were like, whoa!

1494
01:04:25,980 --> 01:04:28,162
That's a lot of parameters.

1495
01:04:28,162 --> 01:04:31,710
How do you-- that's
a lot of parameters.

1496
01:04:31,710 --> 01:04:35,040
And now, models are way,
way, way, way bigger.

1497
01:04:35,040 --> 01:04:37,460
So let's keep track of
sort of the model sizes

1498
01:04:37,460 --> 01:04:38,840
as we're going through this.

1499
01:04:38,840 --> 01:04:42,150
And let's come back now to
the corpus sizes as well.

1500
01:04:42,150 --> 01:04:44,784
So we have BooksCorpus, this
is the number of words there.

1501
01:04:44,784 --> 01:04:46,242
This is the same
thing that GPT-2--

1502
01:04:46,242 --> 01:04:50,060
GPT-1 was trained on,
800 million words.

1503
01:04:50,060 --> 01:04:53,780
Now we're going to train
on also English Wikipedia.

1504
01:04:53,780 --> 01:04:56,750
It's 250, sorry
that's 2,500 million.

1505
01:04:56,750 --> 01:04:59,030
So that's 2.5 billion words.

1506
01:04:59,030 --> 01:05:03,230
And again, to give
you an idea of what

1507
01:05:03,230 --> 01:05:04,760
is done in practice, right?

1508
01:05:04,760 --> 01:05:09,650
Pretraining is expensive and
impractical for most users,

1509
01:05:09,650 --> 01:05:10,850
let's say.

1510
01:05:10,850 --> 01:05:14,810
So if you are a researcher
with the GPU, or five GPUs,

1511
01:05:14,810 --> 01:05:16,280
or something like that.

1512
01:05:16,280 --> 01:05:19,100
You tend to not really be
pretraining your whole own BERT

1513
01:05:19,100 --> 01:05:22,250
model unless you're willing
to spend a long time doing it.

1514
01:05:22,250 --> 01:05:25,670
BERT itself was pretrained
with 64 TPU chips.

1515
01:05:25,670 --> 01:05:28,580
A TPU is a special kind
of hardware accelerator

1516
01:05:28,580 --> 01:05:31,940
that accelerates the tensor
operations effectively,

1517
01:05:31,940 --> 01:05:34,910
it was developed by Google.

1518
01:05:34,910 --> 01:05:39,380
So TPUs are just fast,
and can hold a lot.

1519
01:05:39,380 --> 01:05:41,900
And for four days,
they had 64 chips.

1520
01:05:41,900 --> 01:05:43,520
So if you have
one GPU, which you

1521
01:05:43,520 --> 01:05:46,135
can think of as less
than a single TPU,

1522
01:05:46,135 --> 01:05:48,260
you're going to be waiting
a long time to pretrain.

1523
01:05:48,260 --> 01:05:50,690
But fine tuning actually.

1524
01:05:50,690 --> 01:05:52,140
Fine tuning is so fast.

1525
01:05:52,140 --> 01:05:55,640
It's so fast and practical,
it's common on a single GPU.

1526
01:05:55,640 --> 01:05:58,490
You'll see how much faster
fine tuning is than pretraining

1527
01:05:58,490 --> 01:06:00,590
in assignment 5.

1528
01:06:00,590 --> 01:06:04,400
And so, this becomes, I think
like a refrain of the field.

1529
01:06:04,400 --> 01:06:06,800
You pretrain once or
a handful of times.

1530
01:06:06,800 --> 01:06:10,192
Like a couple of people
release big pretrained models,

1531
01:06:10,192 --> 01:06:11,900
and then you fine tune
many times, right?

1532
01:06:11,900 --> 01:06:15,020
So you you save those parameters
from pretraining, you fine

1533
01:06:15,020 --> 01:06:16,910
tune on all kinds of
different problems.

1534
01:06:16,910 --> 01:06:20,470


1535
01:06:20,470 --> 01:06:22,000
And that paradigm, right?

1536
01:06:22,000 --> 01:06:23,920
Taking something like
BERT, or whatever

1537
01:06:23,920 --> 01:06:28,270
the best descendant of BERT
is, and taking it pretrained

1538
01:06:28,270 --> 01:06:33,310
and then fine tuning it and what
you want, is pretty close to--

1539
01:06:33,310 --> 01:06:36,910
it's a very, very strong
baseline in NLP right now.

1540
01:06:36,910 --> 01:06:37,410
Right?

1541
01:06:37,410 --> 01:06:40,660
So and the simplicity
is pretty fascinating,

1542
01:06:40,660 --> 01:06:44,620
and there's one code
base called Transformers

1543
01:06:44,620 --> 01:06:46,510
from a company called
Hugging Face, that

1544
01:06:46,510 --> 01:06:50,590
makes this just really, just
a couple of lines of Python

1545
01:06:50,590 --> 01:06:51,770
to try out as well.

1546
01:06:51,770 --> 01:06:55,270
So it's sort of opened
up very strong baselines

1547
01:06:55,270 --> 01:06:59,000
without too, too much
effort for a lot of tasks

1548
01:06:59,000 --> 01:06:59,500
OK.

1549
01:06:59,500 --> 01:07:01,490
So let's talk about evaluation.

1550
01:07:01,490 --> 01:07:04,000
So pretraining is
pitched as requiring

1551
01:07:04,000 --> 01:07:06,160
all this different kind
of language understanding.

1552
01:07:06,160 --> 01:07:08,110
And the field is--

1553
01:07:08,110 --> 01:07:11,720
the field of NLP has a
hard time doing evaluation.

1554
01:07:11,720 --> 01:07:13,518
But we try our best,
and we build datasets

1555
01:07:13,518 --> 01:07:15,310
that we think are hard
for various reasons,

1556
01:07:15,310 --> 01:07:16,893
because they require
you to know stuff

1557
01:07:16,893 --> 01:07:19,690
about language, and about the
world, and about reasoning.

1558
01:07:19,690 --> 01:07:22,060
And so, when we evaluate
whether retraining

1559
01:07:22,060 --> 01:07:26,350
is getting you a lot of
sort of general knowledge,

1560
01:07:26,350 --> 01:07:30,710
we evaluate on a
lot of these tasks.

1561
01:07:30,710 --> 01:07:34,810
So we evaluate on things
like-- paraphrase detection,

1562
01:07:34,810 --> 01:07:39,010
on Quora Questions, natural
language inference we saw,

1563
01:07:39,010 --> 01:07:41,560
we have hard sentiment
analysis datasets, or what

1564
01:07:41,560 --> 01:07:45,280
were hard sentiment analysis
datasets a couple of years ago.

1565
01:07:45,280 --> 01:07:47,860
Actually figuring out if
sentences are grammatical

1566
01:07:47,860 --> 01:07:49,480
tends to be hard.

1567
01:07:49,480 --> 01:07:51,400
Determining the
textual similarity,

1568
01:07:51,400 --> 01:07:53,710
the semantic similarity
of text can be hard.

1569
01:07:53,710 --> 01:07:56,200
Paraphrasing again,
natural language inference

1570
01:07:56,200 --> 01:07:57,820
on a very, very small data set.

1571
01:07:57,820 --> 01:07:59,680
So this is this--
this pretraining help

1572
01:07:59,680 --> 01:08:01,600
you train on smaller
data sets, the answer is

1573
01:08:01,600 --> 01:08:03,170
yes, sort of thing.

1574
01:08:03,170 --> 01:08:06,340
And so, right, so
the BERT folks,

1575
01:08:06,340 --> 01:08:09,130
released their paper
after GPT was released,

1576
01:08:09,130 --> 01:08:11,800
and there were a lot of sort
of state of the art results

1577
01:08:11,800 --> 01:08:13,720
that came from various
things that you

1578
01:08:13,720 --> 01:08:15,670
were supposed to be doing.

1579
01:08:15,670 --> 01:08:20,890
And the results that you get
sort of with pretraining.

1580
01:08:20,890 --> 01:08:23,470
So here's OpenAI's GPT,
here is BERT-base and large.

1581
01:08:23,470 --> 01:08:25,630
The last three rows
are all pretrained.

1582
01:08:25,630 --> 01:08:28,423
ELMo is sort of in the middle.

1583
01:08:28,423 --> 01:08:30,340
It's sort of in the
middle between pretraining

1584
01:08:30,340 --> 01:08:32,381
the whole model, and just
having word embeddings,

1585
01:08:32,381 --> 01:08:33,999
that's what this is.

1586
01:08:34,000 --> 01:08:36,160
And the numbers
you get are just,

1587
01:08:36,160 --> 01:08:38,923
I think to the field were
quite astounding actually.

1588
01:08:38,923 --> 01:08:40,840
We were all surprised
that there was that much

1589
01:08:40,840 --> 01:08:43,810
left to even be gotten on
some of these datasets.

1590
01:08:43,810 --> 01:08:47,270
And taking here, so
this line in the table

1591
01:08:47,270 --> 01:08:49,478
is unmarked, which is actually
the number of training

1592
01:08:49,479 --> 01:08:50,260
examples.

1593
01:08:50,260 --> 01:08:53,590
This dataset has 2.5
thousand training examples.

1594
01:08:53,590 --> 01:08:57,279
And before sort of the big
transformers came around

1595
01:08:57,279 --> 01:08:59,740
we had 60% accuracy on it.

1596
01:08:59,740 --> 01:09:02,080
We run transformers on
it, we get 10 points

1597
01:09:02,080 --> 01:09:03,430
just by pretraining.

1598
01:09:03,430 --> 01:09:07,029
And this has been a trend
that is just continued.

1599
01:09:07,029 --> 01:09:10,720
So why do anything,
but pretrain encoders?

1600
01:09:10,720 --> 01:09:12,460
But like we know
encoders are good,

1601
01:09:12,460 --> 01:09:15,410
and we like the fact that you
have bidirectional context.

1602
01:09:15,410 --> 01:09:18,580
We also saw that BERT
did better than GPT.

1603
01:09:18,580 --> 01:09:22,149
But it has-- if you
want to actually get

1604
01:09:22,149 --> 01:09:24,380
it to do things, right?

1605
01:09:24,380 --> 01:09:26,255
You can't just generate it--

1606
01:09:26,255 --> 01:09:27,880
generate sequences
from it the same way

1607
01:09:27,880 --> 01:09:31,224
that you would from a model
like GPT, a pretrained decoder,

1608
01:09:31,224 --> 01:09:32,337
right?

1609
01:09:32,337 --> 01:09:34,629
You can sort of sample what
things should go in a mask.

1610
01:09:34,630 --> 01:09:37,210
So here's a mask, you
can put a mask somewhere,

1611
01:09:37,210 --> 01:09:39,276
sample the words
that should go there.

1612
01:09:39,276 --> 01:09:41,109
But if you want to
sample the whole context,

1613
01:09:41,109 --> 01:09:43,442
right, if you want to get
that story about the unicorns,

1614
01:09:43,442 --> 01:09:46,640
for example, the encoder
is not what you want to do.

1615
01:09:46,640 --> 01:09:49,710
So they have sort of
different contracts in there.

1616
01:09:49,710 --> 01:09:52,450
It can be used naturally
at least in different ways.

1617
01:09:52,450 --> 01:09:54,900


1618
01:09:54,900 --> 01:09:55,400
OK.

1619
01:09:55,400 --> 01:09:57,800
So let's talk very briefly
about extensions of BERT.

1620
01:09:57,800 --> 01:10:00,770
So there are variants
like RoBERTa and SpanBERT,

1621
01:10:00,770 --> 01:10:02,270
and there's just
a bunch of papers

1622
01:10:02,270 --> 01:10:04,700
with the word BERT in the
title, that did various things.

1623
01:10:04,700 --> 01:10:06,450
Two very strong takeaways.

1624
01:10:06,450 --> 01:10:09,770
RoBERTa, train BERT
longer, BERT is under fit.

1625
01:10:09,770 --> 01:10:12,710
Train it on more data,
train it four more steps.

1626
01:10:12,710 --> 01:10:18,590
SpanBERT, mask contiguous
spans of subwords--

1627
01:10:18,590 --> 01:10:21,345
of words makes a harder,
more useful pretraining task.

1628
01:10:21,345 --> 01:10:22,970
So this is the idea
that we can come up

1629
01:10:22,970 --> 01:10:25,970
with better ways of noising
the input, of hiding stuff

1630
01:10:25,970 --> 01:10:28,460
in the input, or breaking stuff
in the input for our model

1631
01:10:28,460 --> 01:10:30,260
to correct.

1632
01:10:30,260 --> 01:10:33,020
So, for example, if
you have the sentence,

1633
01:10:33,020 --> 01:10:36,800
"Mask irr esi mask good."

1634
01:10:36,800 --> 01:10:39,350
It's just not that
hard to know that this

1635
01:10:39,350 --> 01:10:41,810
is "irresistibly," right?

1636
01:10:41,810 --> 01:10:43,400
Because like what
could this possibly

1637
01:10:43,400 --> 01:10:44,630
be after these subwords?

1638
01:10:44,630 --> 01:10:49,670
So this is "irresisti--"
something's got to come here,

1639
01:10:49,670 --> 01:10:52,280
and it's probably
the end of that word.

1640
01:10:52,280 --> 01:10:55,040
Whereas if you mask a
long sequence of things.

1641
01:10:55,040 --> 01:10:57,530
Right, now this is much
harder, and actually you're

1642
01:10:57,530 --> 01:11:00,590
getting a useful signal
that it's irresistibly good.

1643
01:11:00,590 --> 01:11:02,810
And you sort of need
to mask all of them

1644
01:11:02,810 --> 01:11:04,190
to make the task interesting.

1645
01:11:04,190 --> 01:11:07,790
So SpanBERT was like
oh, we should do this.

1646
01:11:07,790 --> 01:11:10,110
So this was super
useful as well.

1647
01:11:10,110 --> 01:11:11,990
So RoBERTa-- just to
point you at the fact

1648
01:11:11,990 --> 01:11:16,100
that RoBERTa showed
that BERT was under fit,

1649
01:11:16,100 --> 01:11:19,550
said BERT was trained on
about 13 gigabytes of text,

1650
01:11:19,550 --> 01:11:21,290
it got some accuracies.

1651
01:11:21,290 --> 01:11:23,810
You can get above
the amazing results

1652
01:11:23,810 --> 01:11:26,780
of BERT, four extra
points or so here,

1653
01:11:26,780 --> 01:11:30,080
right, just by taking
the identical model

1654
01:11:30,080 --> 01:11:32,870
and training it on more
data, with a larger batch

1655
01:11:32,870 --> 01:11:36,410
size for a long time.

1656
01:11:36,410 --> 01:11:38,930
And if you train it,
yeah, on even longer

1657
01:11:38,930 --> 01:11:41,660
without sort of more data,
you don't get any better.

1658
01:11:41,660 --> 01:11:44,980


1659
01:11:44,980 --> 01:11:46,810
Very briefly OK, so--

1660
01:11:46,810 --> 01:11:49,150
very briefly on the
encoder-decoders.

1661
01:11:49,150 --> 01:11:51,510
So we've seen,
decoders can be good

1662
01:11:51,510 --> 01:11:54,010
because we get to play with the
contracts that they gave us,

1663
01:11:54,010 --> 01:11:55,843
we get to play with
them as language models,

1664
01:11:55,843 --> 01:11:58,390
encoders give us that
bidirectional context.

1665
01:11:58,390 --> 01:12:01,870
So encoder-decoders
maybe we get both.

1666
01:12:01,870 --> 01:12:04,720
In practice they're actually,
yeah, pretty, pretty strong.

1667
01:12:04,720 --> 01:12:08,352
So there was a--

1668
01:12:08,352 --> 01:12:10,560
right, we could-- so but I
guess one of the questions

1669
01:12:10,560 --> 01:12:12,872
is like, what do we
do to pre-train them?

1670
01:12:12,872 --> 01:12:14,872
So we could do something
like language modeling,

1671
01:12:14,872 --> 01:12:22,790
right, where we take a sequence
of words, word 1 to word 2T,

1672
01:12:22,790 --> 01:12:26,180
instead of T. Right,
and instead of word 1

1673
01:12:26,180 --> 01:12:28,850
here, dot, dot, dot,
word T, we provide those

1674
01:12:28,850 --> 01:12:32,190
all through our encoder, and
we predict on none of them.

1675
01:12:32,190 --> 01:12:34,430
And then we have, word
T plus 1 to word 2T,

1676
01:12:34,430 --> 01:12:36,830
here in our decoder, right?

1677
01:12:36,830 --> 01:12:38,210
And we predict on these.

1678
01:12:38,210 --> 01:12:40,790
So we're doing language
modeling on half the sequence,

1679
01:12:40,790 --> 01:12:42,830
and we're taking the
other half to have

1680
01:12:42,830 --> 01:12:45,278
our bidirectional
encoder, right?

1681
01:12:45,278 --> 01:12:47,570
So we're building strong
representations on the encoder

1682
01:12:47,570 --> 01:12:51,860
side, not predicting language
modeling on any of this,

1683
01:12:51,860 --> 01:12:53,810
and then we on the other
half of the tokens,

1684
01:12:53,810 --> 01:12:57,230
we predict as a
language model would do.

1685
01:12:57,230 --> 01:13:00,110
And the hope is that you sort
of pretrained both of these well

1686
01:13:00,110 --> 01:13:04,070
through the one language
modeling loss up here.

1687
01:13:04,070 --> 01:13:06,380
And this is actually-- so
this works pretty well.

1688
01:13:06,380 --> 01:13:09,410
The encoder benefits from
bidirectionality, the decoder,

1689
01:13:09,410 --> 01:13:10,920
you can use to train the model.

1690
01:13:10,920 --> 01:13:17,180
But what-- this paper showed
that introduced the Model T5

1691
01:13:17,180 --> 01:13:20,540
Raffel et al., found to work
best was actually a very--

1692
01:13:20,540 --> 01:13:22,728
well, at least a somewhat
different objective.

1693
01:13:22,728 --> 01:13:24,770
And this should keep in
your mind sort of that we

1694
01:13:24,770 --> 01:13:27,380
have different ways of
specifying the pretraining

1695
01:13:27,380 --> 01:13:29,540
objectives, and they will
really work differently

1696
01:13:29,540 --> 01:13:30,360
from each other.

1697
01:13:30,360 --> 01:13:31,902
So what they said,
let's say you have

1698
01:13:31,902 --> 01:13:33,290
an original text like this.

1699
01:13:33,290 --> 01:13:37,670
Thank you for inviting me
to your party last week.

1700
01:13:37,670 --> 01:13:41,450
We're going to define variable
length spans in the text,

1701
01:13:41,450 --> 01:13:44,060
to replace with a
unique symbol that

1702
01:13:44,060 --> 01:13:46,070
says something is missing here.

1703
01:13:46,070 --> 01:13:48,860
And then we'll replace,
and then we'll remove that.

1704
01:13:48,860 --> 01:13:50,900
So now our input
to our encoder is,

1705
01:13:50,900 --> 01:13:57,660
Thank you, symbol 1, me to your
party, symbol 2, week, right?

1706
01:13:57,660 --> 01:14:00,860
So we've noised the input,
we've hidden stuff in the input.

1707
01:14:00,860 --> 01:14:02,480
Also really
interestingly, right, this

1708
01:14:02,480 --> 01:14:05,180
doesn't say how long
this is supposed to be.

1709
01:14:05,180 --> 01:14:06,590
That's different from BERT.

1710
01:14:06,590 --> 01:14:07,250
Right?

1711
01:14:07,250 --> 01:14:10,550
BERT said, "Oh, you masked
this many subwords."

1712
01:14:10,550 --> 01:14:12,530
This says, well I got
some token that says,

1713
01:14:12,530 --> 01:14:13,760
Something's missing here.

1714
01:14:13,760 --> 01:14:15,468
And I don't know what
it is, I don't even

1715
01:14:15,468 --> 01:14:17,480
know how many subwords it is.

1716
01:14:17,480 --> 01:14:19,340
And then, so you have
this in your encoder,

1717
01:14:19,340 --> 01:14:25,160
and then your decoder predicts
the first special word,

1718
01:14:25,160 --> 01:14:30,070
this X here, and then what
was missing, for inviting.

1719
01:14:30,070 --> 01:14:33,010
So, "Thank you X,
X for inviting,"

1720
01:14:33,010 --> 01:14:35,500
and then it predicts
Y, here's this Y here.

1721
01:14:35,500 --> 01:14:38,400
And then what was missing
from the Y, "last."

1722
01:14:38,400 --> 01:14:40,280
"Last week."

1723
01:14:40,280 --> 01:14:42,560
This is called span
corruption, and it's

1724
01:14:42,560 --> 01:14:45,080
really interesting to
me because, in terms

1725
01:14:45,080 --> 01:14:47,010
of the actual
encoder-decoder, we

1726
01:14:47,010 --> 01:14:49,010
don't have to change it
compared to whether we--

1727
01:14:49,010 --> 01:14:51,230
if we were just doing
language modeling pretraining.

1728
01:14:51,230 --> 01:14:53,182
Because I just do
language modeling

1729
01:14:53,182 --> 01:14:54,140
on all of these things.

1730
01:14:54,140 --> 01:14:57,440
I just predict these words
as if I'm a language model.

1731
01:14:57,440 --> 01:15:00,720
I've just done a text
pre-processing step.

1732
01:15:00,720 --> 01:15:01,220
Right?

1733
01:15:01,220 --> 01:15:03,560
So the actual--

1734
01:15:03,560 --> 01:15:06,230
I've just pre-processed the
text to look like, oh, yeah.

1735
01:15:06,230 --> 01:15:08,030
Take the input, make
it look like this.

1736
01:15:08,030 --> 01:15:10,640
Then make an output that
looks like that up there.

1737
01:15:10,640 --> 01:15:14,540
And the model gets to do what is
effectively language modeling,

1738
01:15:14,540 --> 01:15:16,050
but it actually works better.

1739
01:15:16,050 --> 01:15:18,680
So these are a lot of
numbers I realize, but take

1740
01:15:18,680 --> 01:15:20,190
a look at the star here.

1741
01:15:20,190 --> 01:15:23,210
This encoder-decoder with
a denoising objective

1742
01:15:23,210 --> 01:15:25,080
that tends to work the best.

1743
01:15:25,080 --> 01:15:29,270
And they tried similar models
like a prefix language model,

1744
01:15:29,270 --> 01:15:32,792
that was sort of the
first try that we had at

1745
01:15:32,792 --> 01:15:35,000
defining a pretraining
objective for language models,

1746
01:15:35,000 --> 01:15:38,420
sorry, for encoder-decoders.

1747
01:15:38,420 --> 01:15:40,820
And then they had another--
a number of other options,

1748
01:15:40,820 --> 01:15:43,280
but what worked best for
the encoder-decoders.

1749
01:15:43,280 --> 01:15:45,770
And one of the fascinating
things about T5

1750
01:15:45,770 --> 01:15:49,040
is that you could pretrain it
and fine tune it on questions.

1751
01:15:49,040 --> 01:15:52,190
Like, When was Franklin
D, Roosevelt born?

1752
01:15:52,190 --> 01:15:55,660
And fine tune it to
produce the answer.

1753
01:15:55,660 --> 01:15:58,690
And then, you could ask it
new questions at test time.

1754
01:15:58,690 --> 01:16:01,240
And then it would retrieve
the answer from its parameters

1755
01:16:01,240 --> 01:16:02,530
with some accuracy.

1756
01:16:02,530 --> 01:16:05,620
And it would do so
relatively well actually.

1757
01:16:05,620 --> 01:16:08,890
And it would do so maybe 25% of
the time on some of these data

1758
01:16:08,890 --> 01:16:11,120
sets with 220
million parameters.

1759
01:16:11,120 --> 01:16:13,540
Then at 11 billion
parameters, this

1760
01:16:13,540 --> 01:16:16,810
is way bigger than BERT-large,
it would do so even better,

1761
01:16:16,810 --> 01:16:19,420
sometimes even doing
as well as systems

1762
01:16:19,420 --> 01:16:21,487
that were allowed to
look at stuff other

1763
01:16:21,487 --> 01:16:22,570
than their own parameters.

1764
01:16:22,570 --> 01:16:24,970
So again, this is just
making this answer

1765
01:16:24,970 --> 01:16:28,300
come from its parameters.

1766
01:16:28,300 --> 01:16:30,800
Yeah, we have to skip this.

1767
01:16:30,800 --> 01:16:34,090
So if you look back at
this slide after class,

1768
01:16:34,090 --> 01:16:35,860
I have each of the
examples of the things

1769
01:16:35,860 --> 01:16:37,860
that we could imagine
learning from pretraining,

1770
01:16:37,860 --> 01:16:40,010
with the label of what
you might be learning.

1771
01:16:40,010 --> 01:16:42,850
So this example is, Stanford
University is located in blank.

1772
01:16:42,850 --> 01:16:44,170
You might learn trivia.

1773
01:16:44,170 --> 01:16:46,810
In all these cases, there's
all these things you can learn.

1774
01:16:46,810 --> 01:16:50,560
One thing I will say, is that
models also learn and can

1775
01:16:50,560 --> 01:16:52,030
make even worse--

1776
01:16:52,030 --> 01:16:54,670
racism, sexism, all
manner of bad biases

1777
01:16:54,670 --> 01:16:56,170
that are encoded in our text.

1778
01:16:56,170 --> 01:16:59,663
When I say-- yeah, they do this.

1779
01:16:59,663 --> 01:17:02,080
And so, we'll learn more about
this in our later lectures,

1780
01:17:02,080 --> 01:17:04,300
but it's important to keep
in mind that when you're

1781
01:17:04,300 --> 01:17:06,400
doing pretraining, you're
learning a lot of stuff

1782
01:17:06,400 --> 01:17:09,710
and not all of it is good.

1783
01:17:09,710 --> 01:17:12,440
So with GPT-3, the
last thing here

1784
01:17:12,440 --> 01:17:15,920
is that there's this
third way of interacting

1785
01:17:15,920 --> 01:17:18,260
with models that's
related to treating them

1786
01:17:18,260 --> 01:17:19,490
as language models.

1787
01:17:19,490 --> 01:17:23,360
So GPT-3 is this
very, very large model

1788
01:17:23,360 --> 01:17:25,550
that was released
by OpenAI, and it

1789
01:17:25,550 --> 01:17:29,600
seems to be able to learn from
examples in their contexts--

1790
01:17:29,600 --> 01:17:32,810
their decoder contexts,
without gradient steps.

1791
01:17:32,810 --> 01:17:36,500
Simply by looking sort
of within their history.

1792
01:17:36,500 --> 01:17:40,280
And now, GPT-3 has 175
billion parameters, right?

1793
01:17:40,280 --> 01:17:43,520
The last T5 model we saw
was 11 billion parameters.

1794
01:17:43,520 --> 01:17:47,240
And it seems to be sort
of the canonical example

1795
01:17:47,240 --> 01:17:48,330
of this working.

1796
01:17:48,330 --> 01:17:49,940
And so what it
looks like is, you

1797
01:17:49,940 --> 01:17:52,190
give it as part of its prefix--

1798
01:17:52,190 --> 01:17:55,190
"thanks" goes to "merci,"
"hello" goes to--

1799
01:17:55,190 --> 01:17:57,890
"mint" goes to-- so you've got
these translation examples.

1800
01:17:57,890 --> 01:18:00,350
You ask it for the last
one, and it comes up

1801
01:18:00,350 --> 01:18:03,050
with the correct translation.

1802
01:18:03,050 --> 01:18:05,390
Seemingly because it's learned
something about the task

1803
01:18:05,390 --> 01:18:08,257
that you're sort of telling
it to do through its prefix.

1804
01:18:08,257 --> 01:18:10,340
And so, you might do the
same thing with addition.

1805
01:18:10,340 --> 01:18:11,990
So something plus something--

1806
01:18:11,990 --> 01:18:13,400
5 plus 8 is 13.

1807
01:18:13,400 --> 01:18:16,010
Give it addition examples,
it might do the next addition

1808
01:18:16,010 --> 01:18:17,990
example for you.

1809
01:18:17,990 --> 01:18:22,980
Or maybe trying to figure out
grammatical or spelling errors,

1810
01:18:22,980 --> 01:18:23,970
for example.

1811
01:18:23,970 --> 01:18:27,540
And here is the French
to English-- the English

1812
01:18:27,540 --> 01:18:29,600
to French case.

1813
01:18:29,600 --> 01:18:33,420
So again, you're learning
just to do pretraining.

1814
01:18:33,420 --> 01:18:35,930
But when you're evaluating
it, you don't even

1815
01:18:35,930 --> 01:18:39,770
fine tune the model, you
just provide prefixes.

1816
01:18:39,770 --> 01:18:43,587
And so, this especially
is not well understood

1817
01:18:43,587 --> 01:18:45,170
and there's a lot
of research is going

1818
01:18:45,170 --> 01:18:46,850
into sort of what
the limitations

1819
01:18:46,850 --> 01:18:49,880
of this so-called
in-context learning are.

1820
01:18:49,880 --> 01:18:53,510
But it's a fascinating
direction for future work.

1821
01:18:53,510 --> 01:18:56,800
In total, these models
are not well understood.

1822
01:18:56,800 --> 01:19:00,110
However, "small," in air
quotes, models like BERT

1823
01:19:00,110 --> 01:19:02,720
have become general tools
in a wide range of settings,

1824
01:19:02,720 --> 01:19:05,390
and they do have these issues
about learning all these biases

1825
01:19:05,390 --> 01:19:06,230
about the world.

1826
01:19:06,230 --> 01:19:10,430
That will go into and further
lectures in this course,

1827
01:19:10,430 --> 01:19:11,420
and so, yeah.

1828
01:19:11,420 --> 01:19:14,000
What you've learned this week,
transformers and pretraining

1829
01:19:14,000 --> 01:19:17,270
form the basis, or at
least the baselines

1830
01:19:17,270 --> 01:19:19,652
for much of natural
language processing today.

1831
01:19:19,652 --> 01:19:21,110
And assignment 5
is out, and you'll

1832
01:19:21,110 --> 01:19:24,672
be able to look more into it.

1833
01:19:24,672 --> 01:19:25,880
And I'm over time, all right.

1834
01:19:25,880 --> 01:19:31,990


1835
01:19:31,990 --> 01:19:33,220
Yeah.

1836
01:19:33,220 --> 01:19:37,360
I guess, I can take a
question if there is any.

1837
01:19:37,360 --> 01:19:38,960
But people can
get going as well.

1838
01:19:38,960 --> 01:19:45,460


1839
01:19:45,460 --> 01:19:48,155
Well, I think there is
a question about T5.

1840
01:19:48,155 --> 01:19:51,280
Which was, how does the decoder
know that it's currently

1841
01:19:51,280 --> 01:19:53,830
predicting X or Y?

1842
01:19:53,830 --> 01:19:55,840
Could you repeat that?

1843
01:19:55,840 --> 01:19:56,425
Yeah.

1844
01:19:56,425 --> 01:19:58,270
About T5, there
was a question that

1845
01:19:58,270 --> 01:20:00,900
asking, how does the decoder
know that it's currently

1846
01:20:00,900 --> 01:20:03,940
predicting X or Y?

1847
01:20:03,940 --> 01:20:05,950
Its hierarchy of
predicting X or Y?

1848
01:20:05,950 --> 01:20:09,250


1849
01:20:09,250 --> 01:20:11,692
I guess if you can
specify it sort of says,

1850
01:20:11,692 --> 01:20:14,860
how does it know that it's
currently predicting X or Y?

1851
01:20:14,860 --> 01:20:15,360
OK.

1852
01:20:15,360 --> 01:20:15,760
Yeah.

1853
01:20:15,760 --> 01:20:16,718
Yeah, that makes sense.

1854
01:20:16,718 --> 01:20:20,800
So what it does, right so,
it knows from the encoder,

1855
01:20:20,800 --> 01:20:23,170
that it has to at
some point predict X,

1856
01:20:23,170 --> 01:20:26,753
and at some point predict Y.
Because the encoder can just

1857
01:20:26,753 --> 01:20:28,920
remember that, oh, yeah,
there's two things missing.

1858
01:20:28,920 --> 01:20:31,070
And if there were
more spans replaced,

1859
01:20:31,070 --> 01:20:34,350
there would be a Z, and then an
A, and then a B, and whatever.

1860
01:20:34,350 --> 01:20:38,160
Just a bunch of
unique identifiers.

1861
01:20:38,160 --> 01:20:44,330
And then up here, it gets to
say, OK, I have attention,

1862
01:20:44,330 --> 01:20:44,930
I suppose.

1863
01:20:44,930 --> 01:20:47,360
I can look and I know that
first I have to predict this,

1864
01:20:47,360 --> 01:20:48,770
first masked thing.

1865
01:20:48,770 --> 01:20:51,150
So I'm going to generate
that in my decoder.

1866
01:20:51,150 --> 01:20:52,740
And then it gets
that symbol, right?

1867
01:20:52,740 --> 01:20:55,490
So we're doing training by
giving it the right symbol.

1868
01:20:55,490 --> 01:20:59,010
Now, it gets that X, and it
says OK, I'm predicting X now.

1869
01:20:59,010 --> 01:20:59,510
right?

1870
01:20:59,510 --> 01:21:01,910
Now it can predict, predict,
predict, predict, then

1871
01:21:01,910 --> 01:21:04,280
it gets Y. So we're doing
this teacher-forcing training

1872
01:21:04,280 --> 01:21:06,488
where we give it the right
answer after penalizing it

1873
01:21:06,488 --> 01:21:07,580
if it's wrong.

1874
01:21:07,580 --> 01:21:10,990
Now it gets this Y,
right, and says OK, now I

1875
01:21:10,990 --> 01:21:14,160
have to predict what should
go in Y, and it can attend

1876
01:21:14,160 --> 01:21:16,780
into the natural parts of this,
as well as what it's already

1877
01:21:16,780 --> 01:21:19,810
predicted here, because
the decoder has attention

1878
01:21:19,810 --> 01:21:21,935
within itself, and it can
see what should go there.

1879
01:21:21,935 --> 01:21:24,143
So what's fascinating here,
is you're doing something

1880
01:21:24,143 --> 01:21:26,785
like language modeling, but
when you're predicting Y,

1881
01:21:26,785 --> 01:21:29,028
right, you get to see
what came after it.

1882
01:21:29,028 --> 01:21:31,570
And that's, I think, one of the
benefits of span corruptions.

1883
01:21:31,570 --> 01:21:33,945
You're doing this thing where
you don't know how long you

1884
01:21:33,945 --> 01:21:36,220
should be predicting for,
like language modeling,

1885
01:21:36,220 --> 01:21:40,890
but you get to know what came
after the thing that's missing.

1886
01:21:40,890 --> 01:21:45,238


