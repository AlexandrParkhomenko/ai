1
00:00:05,200 --> 00:00:06,640
Итак, что мы собираемся делать на

2
00:00:06,640 --> 00:00:10,240
сегодня, так что основной контент на сегодня -

3
00:00:10,240 --> 00:00:11,519
это пройти

4
00:00:13,679 --> 00:00:16,560
еще кое-что о векторах слов,

5
00:00:16,560 --> 00:00:19,039
включая прикосновение к датчикам слов, а

6
00:00:19,039 --> 00:00:21,119
затем введение понятия

7
00:00:21,119 --> 00:00:23,840
классификаторов нейронных сетей, так что наша самая большая

8
00:00:23,840 --> 00:00:26,640
цель состоит в том, чтобы  В конце сегодняшнего урока

9
00:00:26,640 --> 00:00:28,080
вы должны почувствовать, что можете с

10
00:00:28,080 --> 00:00:30,160
уверенностью взглянуть на одну из

11
00:00:30,160 --> 00:00:32,479
документов для встраивания слов, такую как бумага Google

12
00:00:32,479 --> 00:00:35,760
word2vect, GloVe или

13
00:00:35,760 --> 00:00:37,600
бумага санджива авроры, к которой мы вернемся

14
00:00:37,600 --> 00:00:39,520
позже, и почувствуете, что да, я могу

15
00:00:39,520 --> 00:00:41,520
понять  это я знаю, что они

16
00:00:41,520 --> 00:00:43,760
делают, и это имеет смысл, поэтому давайте

17
00:00:43,760 --> 00:00:46,640
вернемся к тому месту, где мы были, так что

18
00:00:46,640 --> 00:00:48,640
это было своего рода введение этой

19
00:00:48,640 --> 00:00:51,120
модели слова devec, и

20
00:00:51,120 --> 00:00:52,320
третья

21
00:00:52,320 --> 00:00:55,199
строка, ваша идея заключалась в том, что мы начали со

22
00:00:55,199 --> 00:00:57,120
случайных векторов слов, а затем мы

23
00:00:57,120 --> 00:00:58,960
собираясь отсортировать это, у нас есть большой корпус

24
00:00:58,960 --> 00:01:00,879
текста, и мы собираемся перебирать

25
00:01:00,879 --> 00:01:03,199
каждое слово во всем корпусе, и для

26
00:01:03,199 --> 00:01:05,438
каждой позиции мы собираемся попытаться

27
00:01:05,438 --> 00:01:08,960
предсказать, какие слова окружают это наше

28
00:01:08,960 --> 00:01:11,040
центральное слово, и мы собираемся  к d  o что

29
00:01:11,040 --> 00:01:13,520
с распределением вероятностей, которое

30
00:01:13,520 --> 00:01:16,960
определяется в терминах скалярного произведения

31
00:01:16,960 --> 00:01:19,840
между векторами слов для центрального

32
00:01:19,840 --> 00:01:22,479
слова и контекстными словами

33
00:01:22,479 --> 00:01:24,400
um, и таким образом, это даст оценку вероятности

34
00:01:24,400 --> 00:01:26,159
появления слова в

35
00:01:26,159 --> 00:01:29,119
контексте в хорошо, что реальные слова действительно

36
00:01:29,119 --> 00:01:31,520
встречались в  контекст into в этом

37
00:01:31,520 --> 00:01:33,520
случае, поэтому мы

38
00:01:33,520 --> 00:01:36,560
хотим сделать более вероятным, что

39
00:01:36,560 --> 00:01:39,360
превращение проблем, связанных с банковским сектором и кризисами,

40
00:01:39,360 --> 00:01:42,159
в контексте в, и поэтому

41
00:01:42,159 --> 00:01:44,640
мы учимся обновлять векторы слов,

42
00:01:44,640 --> 00:01:46,560
чтобы они могли предсказывать  фактические

43
00:01:46,560 --> 00:01:48,720
окружающие слова лучше,

44
00:01:49,520 --> 00:01:52,399
а затем почти волшебство

45
00:01:52,399 --> 00:01:54,799
заключается в том, что не более чем этот простой

46
00:01:54,799 --> 00:01:57,360
алгоритм позволяет нам изучать векторы слов,

47
00:01:57,360 --> 00:01:59,520
которые хорошо фиксируют

48
00:01:59,520 --> 00:02:02,399
сходство слов и значимые направления в

49
00:02:02,399 --> 00:02:04,880
пространстве слов,

50
00:02:04,880 --> 00:02:08,239
поэтому более точно подходят для этой

51
00:02:08,239 --> 00:02:11,360
модели единственные параметры  этой модели -

52
00:02:11,360 --> 00:02:14,080
векторы слов, поэтому у нас есть векторы внешних слов

53
00:02:14,080 --> 00:02:16,239
и векторы центральных слов для каждого

54
00:02:16,239 --> 00:02:18,800
слова, а затем мы берем их скалярное

55
00:02:18,800 --> 00:02:22,239
произведение um, чтобы получить  Вероятность хорошо,

56
00:02:22,239 --> 00:02:24,879
мы берем скалярное произведение, чтобы получить оценку

57
00:02:24,879 --> 00:02:27,520
того, насколько вероятно, что конкретное внешнее

58
00:02:27,520 --> 00:02:29,440
слово должно произойти с центральным словом, а

59
00:02:29,440 --> 00:02:30,879
затем мы используем мягкое преобразование максимума,

60
00:02:30,879 --> 00:02:33,280
чтобы преобразовать эти оценки

61
00:02:33,280 --> 00:02:35,840
в вероятности, как я обсуждал в прошлый

62
00:02:35,840 --> 00:02:38,319
раз, и я вроде  Я вернусь в

63
00:02:38,319 --> 00:02:40,160
конце, на этот раз,

64
00:02:40,160 --> 00:02:43,519
на пару вещей, на которые стоит обратить внимание. Эту

65
00:02:43,519 --> 00:02:47,040
модель мы называем nlp. Мешок слов

66
00:02:47,040 --> 00:02:49,760
моделей. Мешок слов моделей - это модели,

67
00:02:49,760 --> 00:02:52,319
которые на самом деле не обращают никакого внимания на

68
00:02:52,319 --> 00:02:54,400
порядок слов и не позиционируют его.  Неважно,

69
00:02:54,400 --> 00:02:56,080
рядом ли вы с центральным словом или

70
00:02:56,080 --> 00:02:58,560
немного дальше слева или справа,

71
00:02:58,560 --> 00:03:00,400
оценка вероятности будет такой

72
00:03:00,400 --> 00:03:01,920
же,

73
00:03:01,920 --> 00:03:04,480
и это похоже на очень грубую

74
00:03:04,480 --> 00:03:06,720
модель языка, которая оскорбит любого

75
00:03:06,720 --> 00:03:09,120
лингвиста, и это очень  грубая модель

76
00:03:09,120 --> 00:03:10,959
языка, и

77
00:03:10,959 --> 00:03:13,280
по мере продолжения мы перейдем к более совершенным моделям языка, но даже

78
00:03:13,280 --> 00:03:15,680
этой грубой модели языка достаточно,

79
00:03:15,680 --> 00:03:18,800
чтобы с большой долей вероятности выучить

80
00:03:18,800 --> 00:03:21,040
извинения о

81
00:03:21,040 --> 00:03:23,599
свойствах слов,

82
00:03:23,599 --> 00:03:25,599
а затем второе примечание

83
00:03:25,599 --> 00:03:28,640
хорошо ж  Используя эту модель,

84
00:03:28,640 --> 00:03:30,720
мы хотим, чтобы она давала

85
00:03:30,720 --> 00:03:33,440
достаточно высокие вероятности

86
00:03:33,440 --> 00:03:36,000
словам, которые действительно встречаются в

87
00:03:36,000 --> 00:03:38,480
контексте центрального слова, по крайней мере, если они делают это

88
00:03:38,480 --> 00:03:40,959
вообще часто, но очевидно, что

89
00:03:40,959 --> 00:03:43,360
может встречаться много разных слов, поэтому мы не

90
00:03:43,360 --> 00:03:45,680
говорим о вероятностях, подобных  0,3 и

91
00:03:45,680 --> 00:03:48,159
0,5, мы, скорее всего, будем

92
00:03:48,159 --> 00:03:50,799
говорить о вероятностях, таких как 0,01,

93
00:03:50,799 --> 00:03:53,360
и подобных числах,

94
00:03:53,360 --> 00:03:56,400
а также о том, как мы этого достигаем, и о том,

95
00:03:56,400 --> 00:03:58,959
как это достигается с помощью модели словесного дефекта,

96
00:03:58,959 --> 00:04:01,360
и это этап

97
00:04:01,360 --> 00:04:04,159
обучения модели, заключающийся в размещении

98
00:04:04,159 --> 00:04:06,959
слов  которые похожи по смыслу

99
00:04:06,959 --> 00:04:09,200
близко друг к другу в этом

100
00:04:09,200 --> 00:04:11,599
многомерном векторном пространстве, так что вы снова не

101
00:04:11,599 --> 00:04:13,840
можете прочитать это, но если мы перейдем

102
00:04:13,840 --> 00:04:15,760
к этому,

103
00:04:15,760 --> 00:04:18,160
мы увидим

104
00:04:18,160 --> 00:04:20,238
много похожих слов, означающих группу

105
00:04:20,238 --> 00:04:22,000
близко друг к другу в пространстве, так что вот

106
00:04:22,000 --> 00:04:24,160
дни недели, такие как вторник, четверг,

107
00:04:24,160 --> 00:04:27,520
воскресенье, а также рождество, а

108
00:04:29,120 --> 00:04:31,759
что еще у нас есть,

109
00:04:31,759 --> 00:04:33,440
у нас есть

110
00:04:33,440 --> 00:04:36,240
samsung и nokia, это

111
00:04:36,240 --> 00:04:38,400
диаграмма, которую я сделал несколько лет назад, так

112
00:04:38,400 --> 00:04:40,720
что тогда Nokia все еще была важным

113
00:04:40,720 --> 00:04:43,199
производителем c  У нас есть

114
00:04:43,199 --> 00:04:44,960
разные области, такие как математика и

115
00:04:44,960 --> 00:04:46,400
экономика,

116
00:04:46,400 --> 00:04:49,120
поэтому мы группируем слова, которые

117
00:04:49,120 --> 00:04:51,919
схожи по значению, на

118
00:04:51,919 --> 00:04:53,520
самом деле еще одно замечание, которое я хотел сделать

119
00:04:53,520 --> 00:04:55,600
по этому поводу, я снова имею в виду, что это

120
00:04:55,600 --> 00:04:58,080
двухмерное изображение, которое все, что я

121
00:04:58,080 --> 00:05:01,039
могу вам показать  на слайде,

122
00:05:01,039 --> 00:05:02,560
и это делается с помощью

123
00:05:02,560 --> 00:05:04,880
проекции основных компонентов, которую вы также будете

124
00:05:04,880 --> 00:05:07,919
использовать в задании. Что-

125
00:05:07,919 --> 00:05:10,240
то важное, что нужно запомнить, но

126
00:05:10,240 --> 00:05:13,520
трудно запомнить, - это то, что многомерные

127
00:05:13,520 --> 00:05:16,320
пространства имеют очень разные свойства

128
00:05:16,320 --> 00:05:18,160
по сравнению с двухмерными пространствами, на которые мы можем

129
00:05:18,160 --> 00:05:21,120
смотреть и  так, в частности,

130
00:05:21,840 --> 00:05:25,039
слово вектор может быть близко ко многим другим

131
00:05:25,039 --> 00:05:27,600
вещам в многомерном пространстве, но

132
00:05:27,600 --> 00:05:31,360
близко к ним в разных измерениях,

133
00:05:31,360 --> 00:05:34,240
хорошо, поэтому я упомянул об

134
00:05:34,240 --> 00:05:38,080
обучении, поэтому следующий вопрос

135
00:05:38,080 --> 00:05:41,600
: как мы можем выучить хорошие

136
00:05:41,600 --> 00:05:44,080
векторы слов и это  был бит, который я

137
00:05:44,080 --> 00:05:47,039
не совсем подключил в конце прошлого

138
00:05:47,039 --> 00:05:51,199
класса, поэтому какое-то время в последнем я сказал

139
00:05:51,199 --> 00:05:54,320
исчисление, и мы должны вычислить

140
00:05:54,320 --> 00:05:55,840
градиент функции потерь

141
00:05:55,840 --> 00:05:57,680
относительно  параметры, которые

142
00:05:57,680 --> 00:06:00,000
позволят нам добиться прогресса, но я

143
00:06:00,000 --> 00:06:01,520
не собирал их

144
00:06:01,520 --> 00:06:04,000
вместе, поэтому

145
00:06:04,000 --> 00:06:08,080
мы собираемся начать со случайных векторов слов, которые

146
00:06:08,080 --> 00:06:10,080
мы инициализируем небольшими

147
00:06:10,080 --> 00:06:12,880
числами, близкими к нулю в каждом измерении, которое

148
00:06:12,880 --> 00:06:15,039
мы '  Мы определили нашу

149
00:06:15,039 --> 00:06:17,680
функцию потерь j, которую мы рассматривали в прошлый

150
00:06:17,680 --> 00:06:19,919
раз, а затем мы собираемся использовать

151
00:06:19,919 --> 00:06:22,400
алгоритм градиентного спуска, который является

152
00:06:22,400 --> 00:06:24,960
итеративным итеративным алгоритмом, который

153
00:06:24,960 --> 00:06:27,600
учится максимизировать j тэты,

154
00:06:27,600 --> 00:06:30,560
изменяя тэту, и поэтому идея этого

155
00:06:30,560 --> 00:06:32,800
алгоритма заключается в том, что

156
00:06:32,800 --> 00:06:35,520
от  текущие значения теты вы

157
00:06:35,520 --> 00:06:38,400
вычисляете градиент j теты, а

158
00:06:38,400 --> 00:06:40,560
затем то, что вы собираетесь сделать, это сделать

159
00:06:40,560 --> 00:06:42,960
небольшой шаг в направлении

160
00:06:42,960 --> 00:06:45,280
отрицательного градиента, чтобы градиент был

161
00:06:45,280 --> 00:06:47,520
направлен вверх, и мы делаем

162
00:06:47,520 --> 00:06:49,520
небольшой шаг в направлении

163
00:06:49,520 --> 00:06:52,160
отрицательного градиента, чтобы постепенно

164
00:06:52,160 --> 00:06:54,960
опускаться к минимуму,

165
00:06:54,960 --> 00:06:57,360
и поэтому одним из параметров нейронных

166
00:06:57,360 --> 00:06:59,280
сетей, которые вы можете использовать в своем

167
00:06:59,280 --> 00:07:02,000
программном пакете, является размер шага,

168
00:07:02,000 --> 00:07:04,400
поэтому, если вы возьмете действительно очень немного

169
00:07:04,400 --> 00:07:08,080
шагу вам может потребоваться много времени,

170
00:07:08,080 --> 00:07:11,280
чтобы свести к минимуму функцию, вы выполняете много

171
00:07:11,280 --> 00:07:14,000
бесполезных вычислений, с другой стороны, если

172
00:07:14,000 --> 00:07:15,840
ваш размер шага

173
00:07:15,840 --> 00:07:18,240
слишком

174
00:07:18,240 --> 00:07:21,039
велик, вы можете действительно отклониться и

175
00:07:21,039 --> 00:07:24,080
начать идти в худшие места или даже если

176
00:07:24,080 --> 00:07:26,400
вы спускаетесь под гору  Немного о

177
00:07:26,400 --> 00:07:27,759
том, что произойдет, так это то, что

178
00:07:27,759 --> 00:07:29,759
вы в конечном итоге будете подпрыгивать взад и вперед,

179
00:07:29,759 --> 00:07:31,599
и вам понадобится гораздо больше времени, чтобы добраться

180
00:07:31,599 --> 00:07:33,280
до минимума

181
00:07:33,280 --> 00:07:36,880
хорошо на этой картинке у меня красивая

182
00:07:36,880 --> 00:07:38,400
квадратичная,

183
00:07:38,400 --> 00:07:40,880
и ее легко минимизировать что-то

184
00:07:40,880 --> 00:07:42,319
что вы, возможно, знаете о нейронных

185
00:07:42,319 --> 00:07:44,639
сетях, так это то, что в целом они не

186
00:07:44,639 --> 00:07:47,199
выпуклые, поэтому вы могли подумать,

187
00:07:47,199 --> 00:07:49,919
что все будет хорошо, но

188
00:07:49,919 --> 00:07:51,680
правда в том, что на практике жизнь

189
00:07:51,680 --> 00:07:52,720
идет хорошо,

190
00:07:52,720 --> 00:07:54,720
но я думаю, что я не пойму

191
00:07:54,720 --> 00:07:57,120
прямо сейчас и вернемся к этому um в

192
00:07:57,120 --> 00:07:59,120
более позднем классе,

193
00:07:59,120 --> 00:08:01,520
так что это наш градиентный спуск, поэтому у нас

194
00:08:01,520 --> 00:08:03,039
есть текущие значения

195
00:08:03,039 --> 00:08:07,120
параметров theta, мы затем

196
00:08:07,120 --> 00:08:08,879
немного пройдемся в отрицательном

197
00:08:08,879 --> 00:08:11,840
направлении градиента, используя нашу

198
00:08:11,840 --> 00:08:14,240
скорость обучения или шаг  размер  alpha, и

199
00:08:14,240 --> 00:08:17,680
это дает нам новые значения параметров, где

200
00:08:17,680 --> 00:08:19,440
это означает, что вы знаете, что это

201
00:08:19,440 --> 00:08:21,280
векторы, но для каждого отдельного

202
00:08:21,280 --> 00:08:24,240
параметра мы немного обновляем его,

203
00:08:24,240 --> 00:08:26,160
вычисляя частную

204
00:08:26,160 --> 00:08:28,960
производную j по этому

205
00:08:28,960 --> 00:08:31,599
параметру,

206
00:08:32,000 --> 00:08:34,080
так что простой алгоритм градиентного спуска

207
00:08:34,080 --> 00:08:36,880
никто  использует его, и вы

208
00:08:36,880 --> 00:08:38,958
не должны его

209
00:08:38,958 --> 00:08:42,719
использовать, проблема в том, что наш j является

210
00:08:42,719 --> 00:08:45,839
функцией всех окон в корпусе, помните,

211
00:08:45,839 --> 00:08:49,360
что мы делаем эту сумму по каждому центральному

212
00:08:49,360 --> 00:08:51,839
слову во всем корпусе, и у нас

213
00:08:51,839 --> 00:08:53,440
часто есть миллиарды слов в

214
00:08:53,440 --> 00:08:54,399
корпус,

215
00:08:54,399 --> 00:08:57,760
поэтому фактическая разработка j теты

216
00:08:57,760 --> 00:09:00,000
или градиента j теты будет

217
00:09:00,000 --> 00:09:02,160
чрезвычайно дорогостоящей, потому что мы

218
00:09:02,160 --> 00:09:04,399
должны перебирать весь наш корпус,

219
00:09:04,399 --> 00:09:06,720
поэтому вам придется очень долго ждать, прежде чем

220
00:09:06,720 --> 00:09:09,279
вы сделаете одно обновление градиента, и поэтому

221
00:09:09,279 --> 00:09:11,760
оптимизация будет чрезвычайно  медленный

222
00:09:11,760 --> 00:09:12,880
и поэтому в

223
00:09:12,880 --> 00:09:15,279
основном сто процентов времени

224
00:09:15,279 --> 00:09:17,760
в нейронных сетях мы не используем

225
00:09:17,760 --> 00:09:20,160
градиентный спуск, мы вместо этого используем то, что

226
00:09:20,160 --> 00:09:22,720
называется стохастическим градиентным спуском и

227
00:09:22,720 --> 00:09:25,200
стохастическим градиентным спуском  является очень

228
00:09:25,200 --> 00:09:27,760
простой модификацией этого, поэтому вместо того,

229
00:09:27,760 --> 00:09:31,040
чтобы вычислять

230
00:09:31,040 --> 00:09:34,399
оценку градиента на основе всего корпуса,

231
00:09:34,399 --> 00:09:37,440
вы просто берете одно центральное слово или

232
00:09:37,440 --> 00:09:40,720
небольшую партию, например 32 центральных слова, и

233
00:09:40,720 --> 00:09:42,959
вы вычисляете оценку градиента

234
00:09:42,959 --> 00:09:44,560
на основе них

235
00:09:44,560 --> 00:09:46,399
теперь, когда  оценка градиента будет

236
00:09:47,120 --> 00:09:50,480
шумной и плохой, потому что вы просмотрели

237
00:09:50,480 --> 00:09:52,640
только небольшую часть корпуса, а

238
00:09:52,640 --> 00:09:55,120
не весь корпус, но, тем не менее,

239
00:09:55,120 --> 00:09:56,720
вы можете использовать эту оценку

240
00:09:56,720 --> 00:09:59,680
градиента для обновления ваших тета-

241
00:09:59,680 --> 00:10:02,480
параметров точно таким же образом, и поэтому это

242
00:10:02,480 --> 00:10:05,440
это алгоритм, который мы можем сделать, и поэтому,

243
00:10:05,440 --> 00:10:08,720
если у нас есть корпус из миллиарда слов, мы

244
00:10:08,720 --> 00:10:12,240
можем, если мы сделаем это для каждого центрального слова, мы

245
00:10:12,240 --> 00:10:13,920
сможем сделать миллиард обновлений

246
00:10:13,920 --> 00:10:16,480
параметров, которые мы передаем через корпус

247
00:10:16,480 --> 00:10:19,279
один раз, а не только сделать одно более

248
00:10:19,279 --> 00:10:20,800
точным

249
00:10:20,800 --> 00:10:22,959
обновите

250
00:10:22,959 --> 00:10:25,040
параметры сразу после того, как вы прошли через корпус,

251
00:10:25,040 --> 00:10:26,720
чтобы в целом

252
00:10:26,720 --> 00:10:28,000
мы могли изучить на

253
00:10:28,000 --> 00:10:30,800
несколько порядков быстрее,

254
00:10:30,800 --> 00:10:32,800
и поэтому это алгоритм, который вы

255
00:10:32,800 --> 00:10:34,800
будете использовать везде,

256
00:10:34,800 --> 00:10:37,600
включая вас  знать с

257
00:10:37,600 --> 00:10:40,800
самого начала из наших заданий

258
00:10:40,800 --> 00:10:42,720
эээ, еще

259
00:10:42,720 --> 00:10:45,120
раз просто дополнительный комментарий о более

260
00:10:45,120 --> 00:10:47,360
сложных вещах, к которым мы вернемся

261
00:10:47,360 --> 00:10:49,600
хорошо,

262
00:10:52,160 --> 00:10:55,440
это градиентный спуск, это своего рода

263
00:10:55,440 --> 00:10:57,200
хак производительности, он позволяет вам учиться намного

264
00:10:57,200 --> 00:10:59,680
быстрее, оказывается, это не так

265
00:10:59,680 --> 00:11:02,160
только нейронные сети для взлома производительности имеют некоторые

266
00:11:02,160 --> 00:11:04,399
довольно противоречащие интуиции свойства um,

267
00:11:04,399 --> 00:11:07,680
и на самом деле тот факт, что

268
00:11:07,680 --> 00:11:10,240
стохастический градиентный спуск немного

269
00:11:10,240 --> 00:11:12,880
шумный и подпрыгивает, когда он делает свое

270
00:11:12,880 --> 00:11:15,839
дело, на самом деле означает, что в сложных

271
00:11:15,839 --> 00:11:19,760
сетях он изучает лучшие решения um,

272
00:11:19,760 --> 00:11:21,839
чем если бы вы  запускать

273
00:11:21,839 --> 00:11:24,480
простой градиентный спуск очень медленно, чтобы

274
00:11:24,480 --> 00:11:26,800
вы могли вычислять намного быстрее

275
00:11:26,800 --> 00:11:29,680
и лучше выполнять свою работу.

276
00:11:29,680 --> 00:11:30,640
Хорошо,

277
00:11:30,640 --> 00:11:32,959
одно последнее замечание по запуску стохастических

278
00:11:32,959 --> 00:11:35,120
градиентов с векторами слов, это своего

279
00:11:35,120 --> 00:11:36,880
рода отступление,

280
00:11:36,880 --> 00:11:39,120
но следует отметить, что если мы

281
00:11:39,120 --> 00:11:41,760
выполняем стохастический анализ  обновление градиента на

282
00:11:41,760 --> 00:11:45,360
основе одного окна, тогда на самом деле в этом

283
00:11:45,360 --> 00:11:47,600
окне мы не увидели почти ни одного из

284
00:11:47,600 --> 00:11:49,680
наших параметров, потому что если у нас есть

285
00:11:49,680 --> 00:11:51,680
окно чего-то вроде

286
00:11:51,680 --> 00:11:53,600
пяти слов  s по обе стороны от центрального

287
00:11:53,600 --> 00:11:57,440
слова мы видели не более 11 различных типов слов,

288
00:11:57,440 --> 00:11:59,120
поэтому

289
00:11:59,120 --> 00:12:01,040
у нас будет информация о градиенте для

290
00:12:01,040 --> 00:12:04,560
этих 11 слов, но для других 100000 нечетных

291
00:12:04,560 --> 00:12:06,880
слов теперь словарь не будет иметь

292
00:12:06,880 --> 00:12:09,680
информации об обновлении градиента, так что это

293
00:12:09,680 --> 00:12:13,680
будет очень-очень  разреженное обновление градиента,

294
00:12:13,680 --> 00:12:15,519
поэтому, если вы думаете только

295
00:12:15,519 --> 00:12:17,200
о математике,

296
00:12:17,200 --> 00:12:18,560
вы можете просто

297
00:12:18,560 --> 00:12:22,480
иметь весь свой градиент и использовать

298
00:12:22,480 --> 00:12:24,800
уравнение, которое я показал ранее,

299
00:12:24,800 --> 00:12:26,079
но если вы

300
00:12:26,079 --> 00:12:28,800
думаете об оптимизации системы,

301
00:12:28,800 --> 00:12:31,600
тогда вам нужно хорошо подумать, на самом деле я

302
00:12:31,600 --> 00:12:34,399
хочу только обновить

303
00:12:34,399 --> 00:12:37,120
параметры  для нескольких слов, и

304
00:12:37,120 --> 00:12:39,600
они должны быть, и есть гораздо более

305
00:12:39,600 --> 00:12:42,000
эффективные способы, которыми я мог бы это сделать,

306
00:12:43,279 --> 00:12:45,200
и так

307
00:12:45,200 --> 00:12:47,680
вот, так что это еще одно в сторону, будет

308
00:12:47,680 --> 00:12:49,760
полезно для задания, поэтому я скажу

309
00:12:49,760 --> 00:12:50,639
это

310
00:12:50,639 --> 00:12:53,279
до сих пор, когда я представил

311
00:12:53,279 --> 00:12:56,000
векторы слов  Я представил их как векторы-столбцы,

312
00:12:57,600 --> 00:13:00,160
и это имеет наибольший смысл, если вы

313
00:13:00,160 --> 00:13:03,519
думаете об этом как о части математики,

314
00:13:03,519 --> 00:13:05,680
тогда как на самом деле

315
00:13:05,680 --> 00:13:07,200
во всех

316
00:13:07,200 --> 00:13:09,519
распространенных пакетах глубокого обучения, включая

317
00:13:09,519 --> 00:13:11,680
pytorch, которые мы используем

318
00:13:11,680 --> 00:13:14,480
векторы слов, на самом деле воспроизводятся  представлены как

319
00:13:14,480 --> 00:13:18,000
векторы-строки, и если вы вспомните

320
00:13:18,000 --> 00:13:21,040
о представлении матриц и

321
00:13:21,040 --> 00:13:23,440
cs107 или что-то в этом роде, то вы

322
00:13:23,440 --> 00:13:25,200
знаете, что тогда это очевидно

323
00:13:25,200 --> 00:13:28,720
эффективно для представления слов,

324
00:13:28,720 --> 00:13:30,959
потому что тогда вы можете получить доступ ко всему

325
00:13:30,959 --> 00:13:32,959
вектору слова как к непрерывному диапазону

326
00:13:32,959 --> 00:13:34,560
памяти

327
00:13:34,560 --> 00:13:36,800
разные, если вы все равно находитесь в фортране, так что на

328
00:13:36,800 --> 00:13:38,639
самом деле наши

329
00:13:38,639 --> 00:13:41,839
векторы слов будут векторами-строками,

330
00:13:41,839 --> 00:13:44,240
когда вы посмотрите на эти мм внутри пи-

331
00:13:44,240 --> 00:13:46,639
факела,

332
00:13:46,800 --> 00:13:49,839
хорошо, теперь я хотел сказать немного больше

333
00:13:49,839 --> 00:13:52,880
о семействе алгоритмов word2vec,

334
00:13:52,880 --> 00:13:56,880
а также о том, что вы  собираюсь сделать

335
00:13:56,880 --> 00:13:59,360
в домашнем задании 2. эм так что если вы все еще

336
00:13:59,360 --> 00:14:01,199
должны работать над домашним заданием 1, которое

337
00:14:01,199 --> 00:14:04,639
помнит эм до следующего вторника, что на самом

338
00:14:04,639 --> 00:14:06,800
деле на самом деле с сегодняшним содержанием мы

339
00:14:06,800 --> 00:14:09,199
начинаем домашнее задание два, и я как бы пройдусь

340
00:14:09,199 --> 00:14:11,040
по первому  часть второго домашнего задания

341
00:14:11,040 --> 00:14:13,760
сегодня и другие вещи, которые вам

342
00:14:13,760 --> 00:14:16,240
нужно знать для второго домашнего задания, поэтому я

343
00:14:16,240 --> 00:14:18,800
кратко упомянул идею о том, что у нас есть два

344
00:14:18,800 --> 00:14:21,920
отдельных вектора для каждого типа слова:

345
00:14:21,920 --> 00:14:24,959
центральный вектор и внешние векторы,

346
00:14:24,959 --> 00:14:26,959
и мы просто усредняем  они оба в конце

347
00:14:26,959 --> 00:14:29,199
они похожи, но не идентичны по

348
00:14:29,199 --> 00:14:31,240
нескольким причинам, включая случайную

349
00:14:31,240 --> 00:14:33,360
инициализацию и стохастический

350
00:14:33,360 --> 00:14:35,839
градиентный спуск.

351
00:14:35,839 --> 00:14:37,040
Вы можете

352
00:14:37,040 --> 00:14:38,320
реализовать

353
00:14:38,320 --> 00:14:40,560
алгоритм дефекта слова только с одним

354
00:14:40,560 --> 00:14:43,519
вектором на слово, и на самом деле, если вы это сделаете,

355
00:14:43,519 --> 00:14:45,920
он будет работать немного лучше,

356
00:14:45,920 --> 00:14:48,079
но делает  алгоритм намного

357
00:14:48,079 --> 00:14:51,760
сложнее, и причина этого в том, что

358
00:14:51,760 --> 00:14:55,440
иногда у вас будет тот же тип слова,

359
00:14:55,440 --> 00:14:58,480
что и центральное слово и контекстное слово,

360
00:14:58,480 --> 00:15:01,120
а это означает, что когда вы выполняете

361
00:15:01,120 --> 00:15:03,600
свое вычисление в этой точке, вы

362
00:15:03,600 --> 00:15:06,079
получаете такого рода  беспорядочный случай, когда только

363
00:15:06,079 --> 00:15:08,079
для этого слова вы получаете термин x в

364
00:15:08,079 --> 00:15:10,959
квадрате, извините, скалярный продукт,

365
00:15:10,959 --> 00:15:12,959
вы получаете точечный продукт x, точка x,

366
00:15:12,959 --> 00:15:15,199
что делает его более беспорядочным

367
00:15:15,199 --> 00:15:17,120
для работы, и поэтому мы используем

368
00:15:17,120 --> 00:15:19,199
этот вид  простой оптимизации

369
00:15:19,199 --> 00:15:21,279
с использованием двух векторов на слово - это

370
00:15:21,279 --> 00:15:22,240
нормально,

371
00:15:22,240 --> 00:15:23,600
поэтому

372
00:15:23,600 --> 00:15:26,880
для модели слова-вектора,

373
00:15:26,880 --> 00:15:30,240
представленной в miklov в нашей статье в

374
00:15:30,240 --> 00:15:31,839
2013

375
00:15:31,839 --> 00:15:33,519
году, на самом деле это был не

376
00:15:33,519 --> 00:15:35,040
один

377
00:15:35,040 --> 00:15:38,240
алгоритм, а семейство алгоритмов,

378
00:15:38,240 --> 00:15:41,680
поэтому есть два основных  Варианты модели

379
00:15:41,680 --> 00:15:44,000
один назывался моделью пропуска грамматики,

380
00:15:44,000 --> 00:15:46,240
которая, как я вам объяснил

381
00:15:46,240 --> 00:15:48,480
,

382
00:15:49,600 --> 00:15:52,560
предсказывала положение внешних слов

383
00:15:52,560 --> 00:15:55,360
независимо от центрального слова

384
00:15:55,360 --> 00:15:57,519
в модели стиля мешка слов,

385
00:15:57,519 --> 00:15:59,519
другая называлась моделью непрерывного мешка

386
00:15:59,519 --> 00:16:02,560
слов sibo  и в этом вы

387
00:16:02,560 --> 00:16:04,560
предсказываете центральное слово из набора

388
00:16:04,560 --> 00:16:07,279
контекстных слов,

389
00:16:07,279 --> 00:16:10,160
оба из них дают схожие результаты,

390
00:16:10,160 --> 00:16:12,720
скипграмма более естественна во многих

391
00:16:12,720 --> 00:16:14,480
отношениях, так что это вроде того, к

392
00:16:14,480 --> 00:16:16,320
которому люди

393
00:16:16,320 --> 00:16:18,880
тяготеют в последующей работе,

394
00:16:18,880 --> 00:16:21,600
но потом, как  о том, как вы тренируете эту

395
00:16:21,600 --> 00:16:22,480
модель ...

396
00:16:22,480 --> 00:16:25,680
то, что я представил до сих пор, - это

397
00:16:25,680 --> 00:16:28,079
наивное уравнение softmax,

398
00:16:28,079 --> 00:16:29,680
которое

399
00:16:29,680 --> 00:16:32,320
является простым, но относительно дорогим

400
00:16:32,320 --> 00:16:35,759
методом обучения, и поэтому на самом деле это не то,

401
00:16:35,759 --> 00:16:38,079
что они предлагают использовать в вашей статье

402
00:16:38,079 --> 00:16:40,240
в статье, которую они предлагают использовать метод

403
00:16:40,240 --> 00:16:42,320
это называется отрицательной выборкой, поэтому

404
00:16:42,320 --> 00:16:45,839
вы иногда будете видеть аббревиатуру sgns,

405
00:16:45,839 --> 00:16:48,880
что означает пропустить грамм отрицательной выборки,

406
00:16:48,880 --> 00:16:52,079
поэтому позвольте мне сказать немного

407
00:16:52,079 --> 00:16:55,279
о том, что это такое, но на самом деле я

408
00:16:55,279 --> 00:16:56,800
делаю s  Модель cript gram с

409
00:16:56,800 --> 00:16:58,720
отрицательной выборкой также является частью

410
00:16:58,720 --> 00:17:00,480
домашнего задания, так что вы узнаете эту

411
00:17:00,480 --> 00:17:03,279
модель хорошо, так что суть в том, что если вы

412
00:17:03,279 --> 00:17:05,919
используете этот наивный softmax, вы знаете,

413
00:17:05,919 --> 00:17:08,240
хотя люди обычно используют этот наивный

414
00:17:08,240 --> 00:17:11,119
softmax в различных моделях нейронных сетей,

415
00:17:11,119 --> 00:17:13,839
которые  вычисление знаменателя

416
00:17:13,839 --> 00:17:16,160
довольно дорого, и это потому, что вам

417
00:17:16,160 --> 00:17:18,799
нужно перебирать каждое слово в

418
00:17:18,799 --> 00:17:21,599
словаре и вычислять

419
00:17:21,599 --> 00:17:23,439
эти скалярные произведения, поэтому, если у вас есть словарный запас в

420
00:17:23,439 --> 00:17:24,959
сто тысяч слов,

421
00:17:26,000 --> 00:17:28,079
вам нужно сделать сто

422
00:17:28,079 --> 00:17:31,120
тысяч скалярных произведений, чтобы вычислить

423
00:17:31,120 --> 00:17:33,360
знаменатель, и это кажется

424
00:17:33,360 --> 00:17:36,160
немного стыдным, поэтому вместо этого

425
00:17:36,160 --> 00:17:38,880
идея отрицательной выборки заключается в том, что

426
00:17:38,880 --> 00:17:40,960
вместо использования

427
00:17:40,960 --> 00:17:45,600
этого мягкого максимума мы собираемся обучать

428
00:17:45,600 --> 00:17:48,559
модели бинарной логистической регрессии

429
00:17:48,559 --> 00:17:50,880
как для отряда, так

430
00:17:50,880 --> 00:17:54,559
и для истинной пары центрального слова

431
00:17:54,559 --> 00:17:56,720
и  контекстное слово

432
00:17:56,720 --> 00:17:58,160
против

433
00:17:58,160 --> 00:18:00,480
пар шума, где мы сохраняем истинное

434
00:18:00,480 --> 00:18:03,679
центральное слово, и мы просто случайным образом выбираем

435
00:18:03,679 --> 00:18:06,559
слова из словаря,

436
00:18:06,559 --> 00:18:09,280
чтобы, как представлено в документе,

437
00:18:09,280 --> 00:18:11,840
идея была такой, так что в целом, что w  Мы

438
00:18:11,840 --> 00:18:12,799
хотим

439
00:18:12,799 --> 00:18:15,600
оптимизировать по-

440
00:18:15,600 --> 00:18:18,160
прежнему среднее значение

441
00:18:18,160 --> 00:18:21,520
потерь для каждого отдельного центрального слова,

442
00:18:21,520 --> 00:18:23,360
но когда мы вычисляем потери для

443
00:18:23,360 --> 00:18:25,760
каждого конкретного центрального слова, мы

444
00:18:25,760 --> 00:18:28,720
собираемся вычислить, извините, потери для каждого

445
00:18:28,720 --> 00:18:30,160
конкретного центрального слова и каждого

446
00:18:30,160 --> 00:18:32,799
отдельного слова.  Мы собираемся

447
00:18:32,799 --> 00:18:35,520
взять скалярное произведение, как и раньше, из

448
00:18:35,520 --> 00:18:37,840
центрального слова

449
00:18:37,840 --> 00:18:40,640
и внешнего слова, и это своего рода

450
00:18:40,640 --> 00:18:43,440
основное количество, но теперь вместо использования

451
00:18:43,440 --> 00:18:45,600
этого внутри softmax мы собираемся пропустить

452
00:18:45,600 --> 00:18:46,960
его

453
00:18:46,960 --> 00:18:49,120
через логистическую функцию, которая иногда

454
00:18:49,120 --> 00:18:51,360
также часто также называется сигмоидной

455
00:18:51,360 --> 00:18:53,360
функцией, имя логистической является более

456
00:18:53,360 --> 00:18:55,520
точным, поэтому здесь эта функция,

457
00:18:55,520 --> 00:18:57,520
поэтому логистическая функция является удобной

458
00:18:57,520 --> 00:19:00,559
функцией, которая будет отображать любое действительное число

459
00:19:00,559 --> 00:19:02,320
с вероятностью

460
00:19:02,320 --> 00:19:05,360
между нулем и одним открытым интервалом, поэтому в

461
00:19:05,360 --> 00:19:09,120
основном, если скалярное произведение велико,

462
00:19:09,120 --> 00:19:11,520
логистика скалярного произведения будет

463
00:19:11,520 --> 00:19:13,039
практически

464
00:19:13,039 --> 00:19:16,559
приемлемой, поэтому мы хотим, чтобы она была большой, а

465
00:19:16,559 --> 00:19:19,679
затем, в среднем, мы

466
00:19:19,679 --> 00:19:22,160
хотели бы получить скалярное произведение между центральным

467
00:19:22,160 --> 00:19:24,799
словом и словами, которые мы оправдываем.  t выбрали

468
00:19:24,799 --> 00:19:27,679
случайным образом, т.е. они, скорее всего, на

469
00:19:27,679 --> 00:19:29,840
самом деле не встречались в контексте

470
00:19:29,840 --> 00:19:32,799
центрального слова, чтобы быть маленьким,

471
00:19:32,799 --> 00:19:35,840
и есть только одна небольшая хитрость,

472
00:19:35,840 --> 00:19:38,240
как это делается: эта

473
00:19:38,240 --> 00:19:40,080
сигмовидная функция

474
00:19:40,080 --> 00:19:43,520
является симметричной, и поэтому, если

475
00:19:44,320 --> 00:19:46,320
мы хотим, чтобы

476
00:19:46,320 --> 00:19:49,120
эта вероятность была  small, мы можем

477
00:19:49,120 --> 00:19:52,000
взять отрицательное значение скалярного произведения, поэтому мы

478
00:19:52,000 --> 00:19:54,480
хотим, чтобы

479
00:19:54,480 --> 00:19:57,360
произведение скалярного произведения случайного слова

480
00:19:57,360 --> 00:20:00,720
в центральном слове было отрицательным числом,

481
00:20:00,720 --> 00:20:02,799
поэтому мы собираемся принять

482
00:20:02,799 --> 00:20:05,440
отрицание  это, а затем снова, как только

483
00:20:05,440 --> 00:20:07,440
мы пропустим это через сигмоид, мы хотели бы, чтобы

484
00:20:07,440 --> 00:20:08,720
большое число было

485
00:20:08,720 --> 00:20:10,640
хорошо, чтобы то, как они представляют

486
00:20:10,640 --> 00:20:13,360
вещи, на самом деле максимизирует это

487
00:20:13,360 --> 00:20:16,720
количество, но если я вернусь к тому, чтобы сделать его

488
00:20:16,720 --> 00:20:18,240
немного более похожим на то, как мы

489
00:20:18,240 --> 00:20:19,919
написал вещи, с

490
00:20:19,919 --> 00:20:22,000
которыми мы работали,

491
00:20:22,000 --> 00:20:25,120
минимизируя отрицательную логарифмическую вероятность

492
00:20:25,120 --> 00:20:26,960
um, так что

493
00:20:26,960 --> 00:20:29,520
это выглядит так, поэтому мы берем

494
00:20:29,520 --> 00:20:32,400
отрицательную логарифмическую вероятность

495
00:20:32,400 --> 00:20:35,520
этого сигмоида скалярного произведения um

496
00:20:35,520 --> 00:20:37,919
снова отрицательная логарифмическая вероятность, мы

497
00:20:37,919 --> 00:20:41,360
используем ту же точку отрицателя  продукт

498
00:20:41,360 --> 00:20:43,760
через  сигмоид, а затем мы собираемся вычислить

499
00:20:43,760 --> 00:20:46,840
это количество для небольшого

500
00:20:46,840 --> 00:20:49,360
числа брендов,

501
00:20:49,360 --> 00:20:53,760
мы k отрицательных образцов um, и насколько

502
00:20:53,760 --> 00:20:55,840
вероятно, что они будут выбирать слово, зависит

503
00:20:55,840 --> 00:20:59,120
от их вероятности и от того, где эта

504
00:20:59,120 --> 00:21:00,880
функция потерь будет

505
00:21:00,880 --> 00:21:04,640
минимизирована с учетом этого отрицания с помощью  делая

506
00:21:04,640 --> 00:21:07,760
эти скалярные произведения большими, а эти скалярные

507
00:21:07,760 --> 00:21:09,440
произведения

508
00:21:09,440 --> 00:21:12,799
уменьшают отрицательные,

509
00:21:14,000 --> 00:21:15,280
так

510
00:21:15,280 --> 00:21:16,960
что они просто еще

511
00:21:16,960 --> 00:21:19,039
один трюк, который они

512
00:21:19,039 --> 00:21:20,799
используют, на самом деле есть больше, чем один другой

513
00:21:20,799 --> 00:21:22,880
трюк, который используется в документе о дефектах слов,

514
00:21:22,880 --> 00:21:25,039
чтобы заставить его работать хорошо, но я

515
00:21:25,039 --> 00:21:26,799
только упомяну  одна из их других уловок,

516
00:21:26,799 --> 00:21:30,159
когда они выбирают слова, они

517
00:21:30,159 --> 00:21:33,919
не просто выбирают слова, основываясь

518
00:21:33,919 --> 00:21:37,120
на их вероятности появления

519
00:21:37,120 --> 00:21:40,240
в корпусе, или единообразно то, что они делают, -

520
00:21:40,240 --> 00:21:42,240
они начинают с того, что мы называем распределением слов по униграмме,

521
00:21:42,240 --> 00:21:45,679
так что  это то,

522
00:21:45,679 --> 00:21:48,240
как часто слова на самом деле встречаются в нашем

523
00:21:48,240 --> 00:21:50,799
большом корпусе, поэтому, если у вас есть корпус из миллиардов слов,

524
00:21:50,799 --> 00:21:54,559
и конкретное слово встречается

525
00:21:54,559 --> 00:21:57,200
в нем 90 раз, вы берете 90, деленное

526
00:21:57,200 --> 00:21:59,360
на миллиард, и это униграмма

527
00:21:59,360 --> 00:22:01,679
Вероятность слова, но

528
00:22:01,679 --> 00:22:02,880
затем

529
00:22:02,880 --> 00:22:04,400
они принимают это в

530
00:22:04,400 --> 00:22:06,720
степени трех четвертей и эффект

531
00:22:06,720 --> 00:22:08,799
этой степени в три четверти, который затем

532
00:22:08,799 --> 00:22:10,640
повторно нормализуется, чтобы получить распределение вероятности

533
00:22:10,640 --> 00:22:13,360
с z вроде того, что мы видели в

534
00:22:13,360 --> 00:22:15,520
прошлый раз с  мягкий максимум,

535
00:22:15,520 --> 00:22:18,159
взяв мощность в три четверти,

536
00:22:18,159 --> 00:22:20,960
которая имеет эффект сглаживания

537
00:22:20,960 --> 00:22:23,280
разницы между обычными и редкими словами,

538
00:22:23,280 --> 00:22:26,240
так что менее частые слова

539
00:22:26,240 --> 00:22:28,400
выбираются несколько чаще, но все же

540
00:22:28,400 --> 00:22:31,360
не так много, как

541
00:22:31,360 --> 00:22:33,520
если бы вы просто использовали что-то вроде  равномерное

542
00:22:33,520 --> 00:22:36,559
распределение по словарю,

543
00:22:36,559 --> 00:22:38,080
хорошо, так

544
00:22:38,080 --> 00:22:41,760
что это в основном все, что

545
00:22:41,760 --> 00:22:42,880
можно сказать

546
00:22:45,120 --> 00:22:47,679
об основах того, как у нас есть этот очень

547
00:22:47,679 --> 00:22:49,039
простой

548
00:22:49,039 --> 00:22:51,120
алгоритм нейронной сети

549
00:22:51,120 --> 00:22:54,320
word2vec и как мы можем его обучить и

550
00:22:54,320 --> 00:22:56,480
изучить векторы слов,

551
00:22:56,480 --> 00:22:59,520
поэтому в следующем бите я хочу сделать

552
00:22:59,520 --> 00:23:01,600
сделайте шаг назад и скажите хорошо, вот

553
00:23:01,600 --> 00:23:04,080
алгоритм, который я вам показал, который отлично работает ...

554
00:23:06,000 --> 00:23:08,320
что еще мы могли сделать и что

555
00:23:08,320 --> 00:23:11,840
мы можем сказать об этом, ммм и о первом

556
00:23:11,840 --> 00:23:14,320
том, что вы можете подумать

557
00:23:14,320 --> 00:23:15,919
вот этот

558
00:23:15,919 --> 00:23:18,240
забавный итеративный алгоритм,

559
00:23:18,240 --> 00:23:19,440
который дает вам

560
00:23:19,440 --> 00:23:21,120
векторы слов,

561
00:23:21,120 --> 00:23:25,200
вы знаете, если у нас много слов

562
00:23:25,200 --> 00:23:26,559
и корпус,

563
00:23:26,559 --> 00:23:28,320
кажется, более очевидная вещь, которую

564
00:23:28,320 --> 00:23:29,520
мы могли бы сделать,

565
00:23:29,520 --> 00:23:33,679
это просто посмотреть на количество слов, которые

566
00:23:33,679 --> 00:23:37,120
встречаются друг с другом  и построить

567
00:23:37,120 --> 00:23:39,280
матрицу подсчетов,

568
00:23:39,280 --> 00:23:41,600
матрицу совместной встречаемости, так что вот идея

569
00:23:42,480 --> 00:23:45,200
матрицы совместной встречаемости, так что у меня есть

570
00:23:45,200 --> 00:23:47,520
крохотный корпус, мне нравится глубокое обучение, мне

571
00:23:47,520 --> 00:23:50,640
нравится nlp, мне нравится летать,

572
00:23:50,640 --> 00:23:53,440
и я могу определить размер окна, который я сделал своим

573
00:23:53,440 --> 00:23:56,720
окно просто размером один, чтобы

574
00:23:56,720 --> 00:23:57,840
упростить заполнение

575
00:23:57,840 --> 00:23:59,440
моей матрицы

576
00:23:59,440 --> 00:24:01,760
симметричной, как наш алгоритм word to back,

577
00:24:01,760 --> 00:24:05,200
и поэтому

578
00:24:05,200 --> 00:24:08,640
счетчики в этих ячейках просто показывают, как

579
00:24:08,640 --> 00:24:11,360
часто вещи, которые одновременно происходят в

580
00:24:11,360 --> 00:24:14,240
окне размера один, так что мне нравится,

581
00:24:14,240 --> 00:24:15,840
происходит дважды,

582
00:24:15,840 --> 00:24:18,080
поэтому  мы получаем двойки в этих ячейках, потому

583
00:24:18,080 --> 00:24:19,679
что это симметричное

584
00:24:19,679 --> 00:24:22,799
глубокое обучение происходит один, поэтому мы получаем

585
00:24:22,799 --> 00:24:25,279
один здесь, и многие другие вещи происходят с

586
00:24:25,279 --> 00:24:29,440
нулем, поэтому мы можем построить такую матрицу совместной встречаемости,

587
00:24:29,440 --> 00:24:32,400
как эта, и что на

588
00:24:32,400 --> 00:24:34,720
самом деле это дает нам

589
00:24:34,720 --> 00:24:36,640
представление слов как

590
00:24:36,640 --> 00:24:39,360
совместных  векторы возникновения  поэтому я могу взять

591
00:24:39,360 --> 00:24:40,559
слово i

592
00:24:40,559 --> 00:24:42,880
либо со строкой, либо с вектором-столбцом,

593
00:24:42,880 --> 00:24:45,840
поскольку оно симметрично, и сказать: хорошо, мое

594
00:24:45,840 --> 00:24:48,559
представление слова i

595
00:24:48,559 --> 00:24:50,400
- это вектор-строка,

596
00:24:50,400 --> 00:24:52,960
и это представление слова

597
00:24:52,960 --> 00:24:53,760
i,

598
00:24:53,760 --> 00:24:56,240
и я думаю, вы можете убедить

599
00:24:56,240 --> 00:24:58,720
себя, что  степень, в которой слова

600
00:24:58,720 --> 00:25:02,480
имеют схожее значение и использование, вы как

601
00:25:02,480 --> 00:25:04,080
бы ожидаете, что они будут иметь несколько

602
00:25:04,080 --> 00:25:06,480
схожие векторы, так что если бы у меня было

603
00:25:06,480 --> 00:25:09,200
слово u также в большем корпусе, вы

604
00:25:09,200 --> 00:25:11,679
могли бы ожидать, что i и u будут иметь похожие

605
00:25:11,679 --> 00:25:14,240
векторы, потому что мне нравится, что вы любите  Мне нравится, что

606
00:25:14,240 --> 00:25:17,279
вы и радость, вы бы увидели такие же

607
00:25:17,279 --> 00:25:20,159
возможности, эй, Крис, не могли бы вы

608
00:25:20,159 --> 00:25:22,080
продолжить поиск ответов на некоторые вопросы,

609
00:25:22,080 --> 00:25:23,120
конечно,

610
00:25:23,120 --> 00:25:24,720
хорошо, так что у нас есть несколько вопросов из

611
00:25:24,720 --> 00:25:26,320
негативных, э-э,

612
00:25:26,320 --> 00:25:28,799
слайдов с образцами негативных штампов,

613
00:25:29,679 --> 00:25:31,120
в частности,

614
00:25:31,120 --> 00:25:32,080
э-э,

615
00:25:32,080 --> 00:25:33,840
как это может  вы даете некоторую интуицию

616
00:25:33,840 --> 00:25:35,039
для отрицательной выборки, что

617
00:25:35,039 --> 00:25:38,240
делает отрицательная выборка, и почему мы

618
00:25:38,240 --> 00:25:40,240
берем только один положительный пример, это

619
00:25:40,240 --> 00:25:41,440
два вопроса, на которые можно ответить

620
00:25:42,400 --> 00:25:45,039
тандемно, хорошо, это хороший вопрос, хорошо  Я

621
00:25:45,039 --> 00:25:47,760
постараюсь дать больше интуиции, так

622
00:25:47,760 --> 00:25:50,720
что нужно разработать что-то вроде того, что

623
00:25:50,720 --> 00:25:53,520
softmax

624
00:25:54,880 --> 00:25:57,120
сделал гораздо более

625
00:25:57,120 --> 00:25:59,360
эффективным способом,

626
00:26:00,240 --> 00:26:02,799
так что в soft max well

627
00:26:02,799 --> 00:26:06,320
вы хотели дать высокую

628
00:26:07,840 --> 00:26:10,720
вероятность предсказания контекста контекстному слову,

629
00:26:10,720 --> 00:26:12,480
которое действительно сделало  появляются с центральным

630
00:26:12,480 --> 00:26:16,880
словом um, и хорошо, как вы это делаете,

631
00:26:16,880 --> 00:26:19,279
если скалярное произведение между этими

632
00:26:19,279 --> 00:26:21,919
двумя словами должно быть как можно большим

633
00:26:21,919 --> 00:26:23,279
и является

634
00:26:23,279 --> 00:26:25,600
частью того, как, но вы знаете, что собираетесь

635
00:26:25,600 --> 00:26:28,320
быть чем-то большим, чем это, потому что

636
00:26:28,320 --> 00:26:30,880
в  знаменатель, вы также

637
00:26:30,880 --> 00:26:32,640
разрабатываете скалярное произведение с каждым другим

638
00:26:32,640 --> 00:26:34,960
словом в словаре, поэтому, помимо того,

639
00:26:36,880 --> 00:26:39,679
что вы хотите, чтобы скалярное произведение с фактическим словом, которое вы видите в контексте, было

640
00:26:39,679 --> 00:26:44,159
большим, вы максимизируете свою вероятность,

641
00:26:44,159 --> 00:26:46,720
создавая скалярные произведения

642
00:26:46,720 --> 00:26:49,200
других слов  которые не были в контексте

643
00:26:49,200 --> 00:26:51,279
меньше, потому что это сужает ваш

644
00:26:51,279 --> 00:26:52,480
знаменатель,

645
00:26:52,480 --> 00:26:55,440
и, следовательно, у вас выходит большее

646
00:26:55,440 --> 00:26:57,760
число, и вы

647
00:26:57,760 --> 00:27:00,400
максимизируете потери, поэтому даже для softmax

648
00:27:00,400 --> 00:27:02,400
общая вещь, которую вы хотите сделать,

649
00:27:02,400 --> 00:27:04,720
чтобы максимизировать  ze это есть

650
00:27:04,720 --> 00:27:06,559
точечный продукт со словами действие

651
00:27:06,559 --> 00:27:08,240
контекстное большое

652
00:27:08,240 --> 00:27:10,000
точечное произведение со словами не в

653
00:27:10,000 --> 00:27:13,919
контексте быть маленьким, насколько это возможно,

654
00:27:13,919 --> 00:27:15,279
и, очевидно, вы должны

655
00:27:15,279 --> 00:27:17,120
усреднить это как можно лучше для всех

656
00:27:17,120 --> 00:27:18,640
видов разных контекстов, потому что

657
00:27:18,640 --> 00:27:20,320
иногда разные слова появляются в

658
00:27:20,320 --> 00:27:23,679
разных  контексты, очевидно,

659
00:27:23,679 --> 00:27:24,799
так

660
00:27:26,640 --> 00:27:28,720
что отрицательная выборка - это способ,

661
00:27:28,720 --> 00:27:32,080
следовательно, попытаться максимизировать ту же

662
00:27:32,080 --> 00:27:35,760
цель, теперь вы знаете, что для

663
00:27:36,880 --> 00:27:39,600
вас есть только один положительный термин, потому

664
00:27:39,600 --> 00:27:41,039
что вы на самом деле хотите использовать

665
00:27:41,039 --> 00:27:43,360
фактические данные, так что вы не ждете,

666
00:27:43,360 --> 00:27:46,000
желая  изобретать данные, чтобы для

667
00:27:46,000 --> 00:27:48,559
вычисления всего j мы

668
00:27:48,559 --> 00:27:51,440
действительно вычисляем это количество для каждого

669
00:27:51,440 --> 00:27:54,480
центрального слова и каждого контекстного слова, чтобы

670
00:27:54,480 --> 00:27:55,919
вы знали, что мы перебираем

671
00:27:55,919 --> 00:27:56,799
разные

672
00:27:56,799 --> 00:27:59,200
слова в контекстном окне, а затем

673
00:27:59,200 --> 00:28:00,960
мы перемещаемся по позициям в

674
00:28:00,960 --> 00:28:03,520
корпусе  поэтому мы делаем разные vcs, чтобы

675
00:28:03,520 --> 00:28:05,760
вы знали, что мы делаем это постепенно, но для

676
00:28:05,760 --> 00:28:07,600
одного конкретного центрального слова и одного

677
00:28:07,600 --> 00:28:10,320
конкретного контекстного слова у нас есть только один

678
00:28:10,320 --> 00:28:12,799
реальный фрагмент данных, который положителен  e, так

679
00:28:12,799 --> 00:28:14,960
что это все, что мы используем, потому что мы не знаем,

680
00:28:14,960 --> 00:28:17,520
какие еще слова

681
00:28:17,520 --> 00:28:20,159
следует считать положительными словами, теперь

682
00:28:20,159 --> 00:28:22,640
для отрицательных слов

683
00:28:22,640 --> 00:28:24,799
вы можете просто выбрать

684
00:28:24,799 --> 00:28:25,679
одно

685
00:28:25,679 --> 00:28:27,520
отрицательное слово, и

686
00:28:27,520 --> 00:28:29,679
это, вероятно, сработает, но если вы

687
00:28:29,679 --> 00:28:31,600
хотите немного

688
00:28:31,600 --> 00:28:34,640
лучшего, более стабильного

689
00:28:36,640 --> 00:28:38,559
В общем, мы хотели бы, чтобы другие слова имели низкую

690
00:28:38,559 --> 00:28:40,799
вероятность; похоже, что вы

691
00:28:40,799 --> 00:28:43,120
могли бы получить более стабильные результаты,

692
00:28:43,120 --> 00:28:45,440
если вместо этого скажете: давайте возьмем

693
00:28:45,440 --> 00:28:48,399
10 или 15 образцов отрицательных слов, и

694
00:28:48,399 --> 00:28:51,679
действительно, это оказалось правдой,

695
00:28:51,679 --> 00:28:54,000
но  а для отрицательных слов

696
00:28:54,000 --> 00:28:55,760
легко выбрать любое количество случайных

697
00:28:55,760 --> 00:28:57,919
слов, которые вы хотите, и в этот момент это своего

698
00:28:57,919 --> 00:29:00,000
рода вероятностный аргумент:

699
00:29:00,000 --> 00:29:02,240
слова, которые вы отбираете,

700
00:29:02,240 --> 00:29:04,320
могут быть на самом деле не плохими словами, чтобы

701
00:29:04,320 --> 00:29:06,240
появиться в контексте,

702
00:29:06,240 --> 00:29:07,679
они могут быть другими  слова, которые

703
00:29:07,679 --> 00:29:10,720
находятся в контексте, но в 99,9 случаях

704
00:29:10,720 --> 00:29:13,200
они вряд ли

705
00:29:13,200 --> 00:29:15,279
будут встречаться в контексте,

706
00:29:15,279 --> 00:29:18,399
поэтому они хороши для использования, и да,

707
00:29:18,399 --> 00:29:21,440
вы выбираете только 10 или 15 из них, но

708
00:29:21,440 --> 00:29:25,039
этого достаточно, чтобы добиться прогресса  потому

709
00:29:25,039 --> 00:29:27,279
что центральное слово будет появляться в

710
00:29:27,279 --> 00:29:29,679
других случаях, и когда это произойдет, вы будете

711
00:29:29,679 --> 00:29:32,080
пробовать разные слова здесь, чтобы

712
00:29:32,080 --> 00:29:34,000
вы постепенно пробовали разные

713
00:29:34,000 --> 00:29:36,000
части пространства и начинали узнавать,

714
00:29:36,000 --> 00:29:39,840
что у нас была эта матрица совместной встречаемости, и она

715
00:29:39,840 --> 00:29:43,600
дает представление  слов как

716
00:29:43,600 --> 00:29:46,320
векторов совместной встречаемости

717
00:29:46,320 --> 00:29:49,600
и еще одно замечание о том, что я имею в виду,

718
00:29:49,600 --> 00:29:51,520
что на самом деле существует два способа, которыми

719
00:29:51,520 --> 00:29:53,200
люди обычно делают эти матрицы совместной встречаемости,

720
00:29:54,399 --> 00:29:56,159
одна из которых соответствует тому, что мы уже видели,

721
00:29:56,159 --> 00:29:58,640
когда вы используете окно вокруг

722
00:29:58,640 --> 00:29:59,840
слова,

723
00:29:59,840 --> 00:30:02,480
которое является  похоже на слово в век,

724
00:30:02,480 --> 00:30:04,240
и это позволяет вам уловить некоторую

725
00:30:04,240 --> 00:30:06,320
локальность и некоторую

726
00:30:06,320 --> 00:30:08,960
синтаксическую и семантическую близость, которая

727
00:30:08,960 --> 00:30:11,520
более детализирована, в отличие от того, как

728
00:30:15,760 --> 00:30:17,679
часто возникают эти коматричные заболевания, заключающиеся в том, что

729
00:30:17,679 --> 00:30:19,840
обычно документы имеют некоторую структуру,

730
00:30:19,840 --> 00:30:22,320
будь то абзацы или  эм, просто

731
00:30:22,320 --> 00:30:24,880
реальные веб-страницы сортируют размер документов,

732
00:30:24,880 --> 00:30:26,880
так что вы можете просто сделать размер своего окна

733
00:30:26,880 --> 00:30:30,080
абзацем или целой веб-страницей и

734
00:30:30,080 --> 00:30:32,480
подсчитать их совместное появление, и

735
00:30:32,480 --> 00:30:34,399
это своего рода мет  od, который часто

736
00:30:34,399 --> 00:30:36,960
используется при поиске информации в таких методах,

737
00:30:36,960 --> 00:30:40,720
как скрытый семантический анализ,

738
00:30:40,720 --> 00:30:42,640
хорошо, поэтому вопрос в

739
00:30:42,640 --> 00:30:46,320
том, хороши ли эти типы векторов подсчета

740
00:30:46,320 --> 00:30:47,840
слов

741
00:30:47,840 --> 00:30:52,000
для использования хорошо, что

742
00:30:52,000 --> 00:30:53,760
люди их использовали, они не

743
00:30:53,760 --> 00:30:54,799
ужасны,

744
00:30:54,799 --> 00:30:57,200
но у них есть определенные проблемы

745
00:30:57,200 --> 00:30:59,519
такого рода проблемы  что у них есть

746
00:30:59,519 --> 00:31:02,720
ну во-первых, они огромные, хотя и очень

747
00:31:02,720 --> 00:31:04,480
редкие, так что это то, что я сказал

748
00:31:04,480 --> 00:31:06,960
раньше, если бы у нас был словарный запас в

749
00:31:06,960 --> 00:31:10,080
полмиллиона слов, тогда у нас есть

750
00:31:10,080 --> 00:31:13,039
полмиллион мерный вектор для каждого слова,

751
00:31:13,039 --> 00:31:16,480
который намного больше, чем  слово

752
00:31:16,480 --> 00:31:19,039
векторы, которые мы обычно используем

753
00:31:19,039 --> 00:31:19,840
um,

754
00:31:19,840 --> 00:31:22,559
и это также означает, что, поскольку

755
00:31:22,559 --> 00:31:24,320
у нас есть эти очень многомерные

756
00:31:24,320 --> 00:31:27,840
векторы um, у нас много

757
00:31:27,840 --> 00:31:31,039
разреженности и много случайности, поэтому

758
00:31:31,039 --> 00:31:33,840
результаты, которые вы получаете, имеют тенденцию быть более шумными

759
00:31:33,840 --> 00:31:35,519
и менее надежными в зависимости от того, что

760
00:31:35,519 --> 00:31:38,559
конкретный материал был в корпусе,

761
00:31:38,559 --> 00:31:41,440
и поэтому в целом люди обнаружили, что

762
00:31:41,440 --> 00:31:43,840
вы можете получить гораздо лучшие результаты,

763
00:31:43,840 --> 00:31:46,880
работая с векторами низкой размерности,

764
00:31:46,880 --> 00:31:50,240
поэтому идея состоит в том, что мы можем хранить большую

765
00:31:50,240 --> 00:31:52,559
часть  важная информация о

766
00:31:52,559 --> 00:31:55,200
распределении слов в контексте

767
00:31:55,200 --> 00:31:57,919
других слов в фиксированном небольшом количестве

768
00:31:57,919 --> 00:32:00,880
измерений, дающих плотный вектор,

769
00:32:00,880 --> 00:32:02,960
и на практике

770
00:32:02,960 --> 00:32:05,360
размерность используемых векторов обычно находится

771
00:32:05,360 --> 00:32:08,159
где-то между 25 и тысячей,

772
00:32:08,159 --> 00:32:11,679
и поэтому в этот момент мы  нужно использовать два,

773
00:32:11,679 --> 00:32:13,919
нам нужно использовать какой-то способ уменьшить

774
00:32:13,919 --> 00:32:16,159
размерность наших

775
00:32:16,159 --> 00:32:19,120
векторов совпадения числа,

776
00:32:19,679 --> 00:32:20,640
поэтому,

777
00:32:20,640 --> 00:32:22,960
если у вас есть хорошая память

778
00:32:22,960 --> 00:32:26,159
из класса линейной алгебры,

779
00:32:26,159 --> 00:32:28,559
вы, надеюсь, видели разложение по сингулярным значениям,

780
00:32:28,559 --> 00:32:30,799
и

781
00:32:30,799 --> 00:32:33,360
оно имеет различные математические свойства,

782
00:32:33,360 --> 00:32:36,080
которые я не  Мы собираемся поговорить здесь

783
00:32:36,080 --> 00:32:39,440
о проекции единственного сингулярного значения,

784
00:32:39,440 --> 00:32:41,679
дающей вам оптимальный способ в соответствии с

785
00:32:41,679 --> 00:32:44,000
определенным определением оптимальности

786
00:32:44,000 --> 00:32:47,039
создания матрицы уменьшенной размерности,

787
00:32:47,039 --> 00:32:50,159
которая максимально или, к

788
00:32:50,159 --> 00:32:53,200
сожалению, пара матриц, которая максимально

789
00:32:53,200 --> 00:32:55,200
хорошо позволяет вам восстановить исходную

790
00:32:55,200 --> 00:32:56,399
матрицу,

791
00:32:56,399 --> 00:32:58,559
но идею сингулярного значения

792
00:32:58,559 --> 00:33:00,960
разложение - вы можете взять любую матрицу,

793
00:33:00,960 --> 00:33:02,720
такую как наша

794
00:33:02,720 --> 00:33:05,559
матрица подсчета, и

795
00:33:05,559 --> 00:33:08,240
разложить ее на

796
00:33:08,240 --> 00:33:10,960
три матрицы  es

797
00:33:10,960 --> 00:33:14,240
ua диагональная матрица sigma и av.

798
00:33:35,600 --> 00:33:38,720
вы хотите получить меньшие

799
00:33:38,720 --> 00:33:41,120
размерные представления,

800
00:33:41,120 --> 00:33:43,120
что вы делаете, это пользуетесь тем

801
00:33:43,120 --> 00:33:47,200
фактом, что сингулярные значения внутри

802
00:33:47,200 --> 00:33:50,399
диагональной сигма-матрицы упорядочены от

803
00:33:50,399 --> 00:33:53,440
наибольшего вниз к наименьшему, поэтому мы

804
00:33:53,440 --> 00:33:58,080
можем просто удалить большую часть

805
00:33:58,080 --> 00:34:01,519
матрицы удаления  out некоторые сингулярные значения,

806
00:34:01,519 --> 00:34:04,000
что фактически означает, что в этом

807
00:34:04,000 --> 00:34:05,279
произведении

808
00:34:05,279 --> 00:34:06,480
сумма u

809
00:34:06,480 --> 00:34:09,760
и сумма v также не используются, и

810
00:34:09,760 --> 00:34:12,560
поэтому в результате этого мы получаем

811
00:34:12,560 --> 00:34:15,280
представления um более низкой размерности

812
00:34:16,079 --> 00:34:17,199
для

813
00:34:17,199 --> 00:34:19,359
наших слов, если мы хотим иметь векторы слов

814
00:34:20,399 --> 00:34:24,000
которые по-прежнему выполняют максимально хорошую работу в

815
00:34:24,000 --> 00:34:26,879
пределах заданной размерности,

816
00:34:26,879 --> 00:34:30,639
позволяя вам восстановить исходную

817
00:34:30,639 --> 00:34:33,760
матрицу совместной встречаемости,

818
00:34:33,760 --> 00:34:36,960
поэтому из фона линейной алгебры um

819
00:34:36,960 --> 00:34:40,239
это  очевидная вещь, которую нужно использовать,

820
00:34:40,239 --> 00:34:42,399
так как это

821
00:34:42,399 --> 00:34:45,040
хорошо работает, если вы просто создаете

822
00:34:46,000 --> 00:34:48,800
матрицу совпадения необработанных подсчетов

823
00:34:48,800 --> 00:34:52,079
и запускаете svd на этом и пытаетесь использовать

824
00:34:52,079 --> 00:34:55,119
их в качестве векторов слов, это на самом деле работает

825
00:34:55,119 --> 00:34:57,119
плохо, и

826
00:34:57,119 --> 00:34:59,040
это работает плохо,

827
00:34:59,040 --> 00:35:00,960
потому что, если вы попадете в  математические

828
00:35:00,960 --> 00:35:03,280
предположения svd, которые вы ожидаете

829
00:35:03,280 --> 00:35:04,079
иметь

830
00:35:04,079 --> 00:35:08,079
эти нормально распределенные ошибки, и

831
00:35:08,079 --> 00:35:10,720
то, что вы получаете с подсчетом слов,

832
00:35:10,720 --> 00:35:14,160
выглядело совсем не так,

833
00:35:14,880 --> 00:35:16,480
как что-то нормальное, чего вы не делали,

834
00:35:16,480 --> 00:35:18,640
потому что у вас есть чрезвычайно общие

835
00:35:18,640 --> 00:35:21,839
слова, такие как артур и и и у вас

836
00:35:21,839 --> 00:35:24,640
очень большой  количество редких слов, так что

837
00:35:24,640 --> 00:35:26,640
это не очень хорошо работает, но вы действительно

838
00:35:26,640 --> 00:35:29,599
получаете что-то, что работает намного лучше,

839
00:35:29,599 --> 00:35:32,480
если вы масштабируете счетчики в ячейках,

840
00:35:32,480 --> 00:35:33,200
поэтому

841
00:35:33,200 --> 00:35:35,200
для решения этой проблемы очень

842
00:35:35,200 --> 00:35:37,200
частых слов есть некоторые вещи, которые мы

843
00:35:37,200 --> 00:35:39,760
можем сделать, мы могли бы просто  возьмите журнал

844
00:35:39,760 --> 00:35:41,200
необработанных подсчетов,

845
00:35:41,200 --> 00:35:44,480
мы могли бы как бы ограничить максимальное количество, которое

846
00:35:44,480 --> 00:35:46,960
мы могли бы отбросить функциональные слова,

847
00:35:46,960 --> 00:35:50,240
и любая из этих идей позволит вам

848
00:35:50,240 --> 00:35:52,960
построить, а затем получить матрицу совместной встречаемости,

849
00:35:52,960 --> 00:35:55,280
которая станет более полезной  векторов слов

850
00:35:55,280 --> 00:35:58,400
от запуска чего-то вроде svd,

851
00:35:58,400 --> 00:36:00,480
и действительно, такие модели были

852
00:36:00,480 --> 00:36:04,200
исследованы в 1990-х и в

853
00:36:04,200 --> 00:36:08,560
2000-х, и, в частности, гм Дуг Роди

854
00:36:08,560 --> 00:36:11,119
исследовал ряд этих идей, а также то,

855
00:36:11,119 --> 00:36:14,160
как улучшить матрицу совместной встречаемости

856
00:36:14,160 --> 00:36:16,640
в модели, которую он  построенный, который назывался

857
00:36:16,640 --> 00:36:17,839
Коля,

858
00:36:17,839 --> 00:36:21,359
и вы знаете, что на самом деле в его модели Коля

859
00:36:22,560 --> 00:36:26,560
он заметил тот факт, что вы можете

860
00:36:26,560 --> 00:36:29,440
получить такие же

861
00:36:29,440 --> 00:36:31,680
линейные компоненты, которые имеют семантические

862
00:36:31,680 --> 00:36:34,960
компоненты, которые мы видели вчера, когда

863
00:36:34,960 --> 00:36:36,320
говорили об

864
00:36:36,320 --> 00:36:39,760
аналогиях, так что, например, это

865
00:36:39,760 --> 00:36:42,480
фигура из его статьи и  вы можете видеть,

866
00:36:42,480 --> 00:36:45,680
что у нас, кажется, есть компонент значения,

867
00:36:45,680 --> 00:36:49,280
идущий от глагола к человеку, который произносит

868
00:36:49,280 --> 00:36:51,920
этот глагол, так побуждай к плаванию, чтобы

869
00:36:51,920 --> 00:36:54,320
пловец учил учителя жениться на

870
00:36:54,320 --> 00:36:56,880
священнике, и что эти

871
00:36:56,880 --> 00:36:59,920
компоненты вектора не идеальны,

872
00:36:59,920 --> 00:37:01,839
но примерно

873
00:37:01,839 --> 00:37:04,800
параллельны и примерно  того же размера, и

874
00:37:04,800 --> 00:37:07,280
поэтому у нас есть компонент значения,

875
00:37:07,280 --> 00:37:09,920
который мы могли бы добавить к другому слову

876
00:37:09,920 --> 00:37:12,240
так же, как мы делали ранее, для

877
00:37:12,240 --> 00:37:15,520
аналогий мы могли бы сказать драйверы водителю

878
00:37:15,520 --> 00:37:19,440
как mari  это то, к чему, и мы добавили бы на этот

879
00:37:19,440 --> 00:37:21,680
экранный векторный компонент, который примерно

880
00:37:21,680 --> 00:37:23,920
такой же, как этот, и мы бы сказали о

881
00:37:23,920 --> 00:37:26,240
священник, чтобы это пространство могло действительно

882
00:37:26,240 --> 00:37:28,800
получить некоторые

883
00:37:30,560 --> 00:37:32,960
аналогии векторов слов,

884
00:37:32,960 --> 00:37:34,560
и поэтому это показалось

885
00:37:34,560 --> 00:37:37,760
нам действительно интересным  Примерно в то время

886
00:37:37,760 --> 00:37:40,320
word2vec появилось из-за желания

887
00:37:40,320 --> 00:37:42,160
лучше понять, что делает алгоритм итеративного

888
00:37:42,160 --> 00:37:44,880
обновления word2vec

889
00:37:44,880 --> 00:37:47,359
и как он соотносится с

890
00:37:47,359 --> 00:37:49,520
методами, более основанными на линейной алгебре, которые были

891
00:37:50,400 --> 00:37:52,960
исследованы пару десятилетий назад, и

892
00:37:52,960 --> 00:37:55,520
поэтому для следующего бита i  хочу рассказать вам

893
00:37:55,520 --> 00:37:57,760
немного об алгоритме GloVe,

894
00:37:57,760 --> 00:37:59,359
который представлял собой алгоритм

895
00:37:59,359 --> 00:38:02,400
для векторов слов, который был

896
00:38:02,400 --> 00:38:05,119
разработан мной и Джеффри Пеннингтоном Ричардом Сохером

897
00:38:05,119 --> 00:38:07,119
в 2014 г.

898
00:38:14,800 --> 00:38:17,119
-матрицы встречаемости, такие как lsa и

899
00:38:17,119 --> 00:38:20,320
coles, с такими моделями, как skip grand

900
00:38:20,320 --> 00:38:22,960
sibo и другие их друзья, которые были

901
00:38:22,960 --> 00:38:26,400
алгоритмами итеративного нейронного обновления, так

902
00:38:26,400 --> 00:38:28,720
что, с одной стороны, вы

903
00:38:28,720 --> 00:38:30,720
знаете линейный a  На самом деле

904
00:38:30,720 --> 00:38:33,599
казалось, что у методов lgebra есть преимущества для быстрого

905
00:38:33,599 --> 00:38:34,960
обучения и эффективного использования

906
00:38:34,960 --> 00:38:36,400
статистики,

907
00:38:36,400 --> 00:38:37,760
но,

908
00:38:37,760 --> 00:38:39,119
несмотря на то, что проводилась работа по

909
00:38:39,119 --> 00:38:40,320
выявлению

910
00:38:40,320 --> 00:38:44,079
сходства слов с ними в целом,

911
00:38:44,079 --> 00:38:46,000
результаты были не такими хорошими, возможно,

912
00:38:46,000 --> 00:38:47,760
из-за непропорционального значения,

913
00:38:47,760 --> 00:38:49,920
придаваемого крупным счетам в основном.

914
00:38:49,920 --> 00:38:51,200
и наоборот,

915
00:38:52,079 --> 00:38:53,920
модели,

916
00:38:53,920 --> 00:38:57,040
нейронные модели, кажется, что

917
00:38:57,040 --> 00:38:58,480
если вы просто выполняете эти

918
00:38:58,480 --> 00:39:01,280
обновления градиента в окнах, вы каким-то образом

919
00:39:01,280 --> 00:39:03,599
неэффективно используете статистику по сравнению с

920
00:39:03,599 --> 00:39:05,520
матрицей совместной встречаемости,

921
00:39:05,520 --> 00:39:07,520
но с другой стороны,

922
00:39:07,520 --> 00:39:09,839
на самом деле ее легче масштабировать до очень

923
00:39:09,839 --> 00:39:12,400
больших  корпус, обменивая

924
00:39:12,400 --> 00:39:15,680
время на пространство, но

925
00:39:15,680 --> 00:39:18,000
в то время казалось, что новые

926
00:39:18,000 --> 00:39:20,079
методы просто лучше работают для людей,

927
00:39:20,079 --> 00:39:22,320
что они повышают

928
00:39:22,320 --> 00:39:24,240
производительность многих задач не только из-за сходства слов,

929
00:39:24,240 --> 00:39:26,480
и что они могут улавливать

930
00:39:26,480 --> 00:39:29,760
сложные шаблоны, такие как аналогии,

931
00:39:29,760 --> 00:39:33,280
выходящие за рамки слов  сходство,

932
00:39:33,280 --> 00:39:35,200
и поэтому мы хотели

933
00:39:35,200 --> 00:39:37,839
немного больше понять, что вам нужно,

934
00:39:37,839 --> 00:39:40,839
какие свойства вам нужны  чтобы эти

935
00:39:40,839 --> 00:39:44,720
аналогии работали, как я показал в прошлый раз,

936
00:39:44,720 --> 00:39:45,920
и поэтому

937
00:39:45,920 --> 00:39:48,880
мы поняли, что если вы хотите

938
00:39:50,240 --> 00:39:53,520
иметь такого рода векторные

939
00:39:53,520 --> 00:39:55,359
вычитания и

940
00:39:55,359 --> 00:39:56,880
сложения,

941
00:39:56,880 --> 00:39:59,920
работающие по аналогии, свойство, которое

942
00:39:59,920 --> 00:40:00,880
вы

943
00:40:00,880 --> 00:40:04,480
хотите, предназначено для обозначения компонентов, поэтому

944
00:40:04,480 --> 00:40:06,960
Смысловой компонент - это что-то вроде

945
00:40:06,960 --> 00:40:08,720
перехода от

946
00:40:08,720 --> 00:40:12,800
мужчины к королеве-женщине к королю или

947
00:40:12,800 --> 00:40:15,040
от

948
00:40:16,079 --> 00:40:19,520
его возраста и от грузовика к водителю, эм,

949
00:40:19,520 --> 00:40:21,440
эти значимые компоненты

950
00:40:21,440 --> 00:40:24,000
должны быть представлены как отношения

951
00:40:24,000 --> 00:40:25,920
вероятностей совместного появления,

952
00:40:25,920 --> 00:40:29,200
поэтому вот пример, который показывает, что

953
00:40:29,200 --> 00:40:32,079
хорошо, поэтому предположим, что значимый компонент

954
00:40:32,079 --> 00:40:35,040
то, что мы хотим получить, -

955
00:40:35,040 --> 00:40:38,480
это спектр от твердого тела к газу, так как в

956
00:40:38,480 --> 00:40:39,520
физике

957
00:40:40,319 --> 00:40:41,920
вы могли бы подумать, что

958
00:40:41,920 --> 00:40:44,560
вы можете получить твердую его часть,

959
00:40:44,560 --> 00:40:47,200
возможно, сказав, встречается ли слово вместе

960
00:40:47,200 --> 00:40:49,839
со льдом, а слово твердое тело встречается со

961
00:40:49,839 --> 00:40:52,480
льдом, поэтому  это выглядит обнадеживающе, и газ

962
00:40:52,480 --> 00:40:54,400
не возникает со льдом, так что это

963
00:40:54,400 --> 00:40:56,960
выглядит обнадеживающим, но проблема в том, что

964
00:40:56,960 --> 00:40:59,680
слово вода также часто встречается со

965
00:40:59,680 --> 00:41:01,760
льдом, и если вы просто возьмете какое-то другое

966
00:41:01,760 --> 00:41:04,079
случайное слово, например слово случайный, это

967
00:41:04,079 --> 00:41:06,400
про  Возможно, со льдом не так

968
00:41:06,400 --> 00:41:07,839
много,

969
00:41:07,839 --> 00:41:10,640
по контрасту, если вы посмотрите на

970
00:41:10,640 --> 00:41:13,599
слова, встречающиеся вместе с паром,

971
00:41:13,599 --> 00:41:15,920
твердое тело не будет встречаться с паром много, но

972
00:41:15,920 --> 00:41:17,119
газ будет,

973
00:41:17,119 --> 00:41:19,680
но вода будет снова, и случайное значение будет

974
00:41:19,680 --> 00:41:20,720
маленьким,

975
00:41:20,720 --> 00:41:22,720
чтобы получить значимый компонент, который мы

976
00:41:22,720 --> 00:41:24,560
хотим  При переходе от

977
00:41:24,560 --> 00:41:27,200
газа к твердому телу действительно

978
00:41:27,200 --> 00:41:29,839
полезно посмотреть на соотношение этих

979
00:41:29,839 --> 00:41:32,480
вероятностей совместного возникновения,

980
00:41:32,480 --> 00:41:35,280
потому что тогда мы получаем спектр

981
00:41:35,280 --> 00:41:39,040
от большого к малому между твердым телом и газом,

982
00:41:39,040 --> 00:41:41,520
тогда как для воды в случайном слове он в

983
00:41:41,520 --> 00:41:45,119
основном сокращает и дает вам один

984
00:41:45,119 --> 00:41:47,599
эм, я просто записал эти числа, но если

985
00:41:47,599 --> 00:41:48,560
вы

986
00:41:48,560 --> 00:41:51,280
пересчитаете их в большом корпусе, это в

987
00:41:51,280 --> 00:41:53,440
основном то, что вы получите, так что вот

988
00:41:53,440 --> 00:41:57,040
фактические вероятности совпадения и

989
00:41:57,040 --> 00:41:58,240
что для

990
00:41:58,240 --> 00:42:00,160
воды и моего случайного слова, которое

991
00:42:00,160 --> 00:42:03,040
здесь было модным, это примерно один

992
00:42:03,040 --> 00:42:04,960
мкм, тогда как для

993
00:42:04,960 --> 00:42:07,200
отношение

994
00:42:07,200 --> 00:42:09,920
вероятности совместного присутствия твердого тела

995
00:42:09,920 --> 00:42:13,680
со льдом или паром составляет около десяти, а четыре

996
00:42:13,680 --> 00:42:14,640
предполагают,

997
00:42:14,640 --> 00:42:16,400
что это около десятой части,

998
00:42:16,400 --> 00:42:18,400
так

999
00:42:18,400 --> 00:42:19,920
как мы можем

1000
00:42:19,920 --> 00:42:22,560
зафиксировать эти отношения

1001
00:42:22,560 --> 00:42:25,040
вероятностей соекон как компоненты линейного значения,

1002
00:42:25,040 --> 00:42:27,520
чтобы  в нашем векторном

1003
00:42:27,520 --> 00:42:30,240
пространстве слов мы можем просто складывать и вычитать

1004
00:42:30,240 --> 00:42:32,400
компоненты линейного значения,

1005
00:42:33,280 --> 00:42:35,680
это похоже на то, как мы можем добиться

1006
00:42:35,680 --> 00:42:38,560
этого, если мы построим журнал по линейной

1007
00:42:38,560 --> 00:42:41,680
модели, так что скалярное произведение между

1008
00:42:41,680 --> 00:42:43,920
двумя векторами слов

1009
00:42:43,920 --> 00:42:46,480
пытается аппроксимировать журнал

1010
00:42:46,480 --> 00:42:48,640
вероятности  совпадения,

1011
00:42:48,640 --> 00:42:51,440
поэтому, если вы это сделаете, вы получите это

1012
00:42:51,440 --> 00:42:53,200
свойство, заключающееся в

1013
00:42:53,200 --> 00:42:55,119
том, что

1014
00:42:55,119 --> 00:42:58,319
разница между двумя векторами,

1015
00:42:58,319 --> 00:43:00,960
его сходство с другим словом

1016
00:43:00,960 --> 00:43:02,960
соответствует логарифму

1017
00:43:02,960 --> 00:43:05,520
отношения вероятностей, показанному на предыдущем

1018
00:43:05,520 --> 00:43:06,880
слайде,

1019
00:43:06,880 --> 00:43:10,640
поэтому модель GloVe хотела попытаться

1020
00:43:11,599 --> 00:43:14,560
объединить мышление  между

1021
00:43:14,560 --> 00:43:17,599
матричными моделями совместной встречаемости и

1022
00:43:17,599 --> 00:43:19,200
нейронными моделями

1023
00:43:19,200 --> 00:43:20,000
,

1024
00:43:20,000 --> 00:43:22,000
поскольку они в некотором роде похожи на более новую

1025
00:43:22,000 --> 00:43:25,839
модель, но фактически вычисляются поверх

1026
00:43:26,800 --> 00:43:29,200
текущего количества матриц,

1027
00:43:29,200 --> 00:43:32,640
поэтому у нас была явная функция потерь,

1028
00:43:32,640 --> 00:43:35,040
а наша явная функция потерь

1029
00:43:35,040 --> 00:43:38,160
заключается в том, что мы хотели скалярное произведение  чтобы быть

1030
00:43:38,160 --> 00:43:41,760
похожим на журнал совпадений, который

1031
00:43:41,760 --> 00:43:43,839
мы на самом деле добавили в некоторые термины смещения

1032
00:43:43,839 --> 00:43:45,280
здесь, но я проигнорирую их на

1033
00:43:45,280 --> 00:43:48,240
данный момент, и мы хотели не иметь очень

1034
00:43:48,240 --> 00:43:51,920
общего слова  s доминируют, и поэтому мы сохранили

1035
00:43:51,920 --> 00:43:54,560
эффект большого количества слов,

1036
00:43:54,560 --> 00:43:57,599
используя эту функцию f, которая показана здесь,

1037
00:43:57,599 --> 00:43:58,880
а затем мы могли

1038
00:43:58,880 --> 00:44:01,920
оптимизировать эту функцию j

1039
00:44:01,920 --> 00:44:04,800
непосредственно на

1040
00:44:04,800 --> 00:44:06,960
матрице подсчета совпадений, что дало нам быстрое

1041
00:44:06,960 --> 00:44:10,640
обучение, масштабируемое до огромных корпусов,

1042
00:44:10,640 --> 00:44:13,599
и поэтому этот алгоритм  работал

1043
00:44:13,599 --> 00:44:15,040
очень хорошо,

1044
00:44:15,040 --> 00:44:16,640
поэтому, если вы спросите,

1045
00:44:16,640 --> 00:44:18,480
запускаете ли вы этот алгоритм, спросите, какие

1046
00:44:18,480 --> 00:44:21,280
слова ближе всего к лягушке, вы получите лягушку,

1047
00:44:21,280 --> 00:44:23,520
жабу, а затем вы получите несколько сложных

1048
00:44:23,520 --> 00:44:25,920
слов, но оказывается, что все они

1049
00:44:25,920 --> 00:44:28,480
лягушки, ммм, пока вы не перейдете к ящерицам,

1050
00:44:28,480 --> 00:44:31,280
так что латоя такая милая  древесная лягушка,

1051
00:44:31,280 --> 00:44:32,079
гм,

1052
00:44:32,079 --> 00:44:34,079
и так что это на самом деле, казалось, сработало

1053
00:44:34,079 --> 00:44:35,839
довольно хорошо,

1054
00:44:35,839 --> 00:44:38,640
насколько хорошо это сработало, гм, чтобы обсудить

1055
00:44:38,640 --> 00:44:40,800
это еще немного, теперь я хочу

1056
00:44:40,800 --> 00:44:43,200
кое-что сказать о том, как мы оцениваем векторы слов,

1057
00:44:45,680 --> 00:44:47,040
годны ли мы до этого для

1058
00:44:47,040 --> 00:44:50,040
вопросов, которые

1059
00:44:50,079 --> 00:44:52,079
мы  У меня есть несколько вопросов, а что вы имеете в

1060
00:44:52,079 --> 00:44:54,000
виду, говоря о неэффективном использовании статистики

1061
00:44:54,000 --> 00:44:56,000
в качестве мошенничества для пропуска грамма,

1062
00:44:56,800 --> 00:44:58,400
я имею в виду,

1063
00:44:58,400 --> 00:44:59,760
что

1064
00:44:59,760 --> 00:45:00,880
вы

1065
00:45:00,880 --> 00:45:05,040
знаете слово, чтобы убрать, вы просто знаете,

1066
00:45:05,040 --> 00:45:08,560
глядя на одно центральное слово за раз и

1067
00:45:08,560 --> 00:45:11,760
ген  оцениваете несколько отрицательных образцов, и поэтому

1068
00:45:11,760 --> 00:45:14,800
кажется,

1069
00:45:14,800 --> 00:45:17,119
что там всегда делается что-то точное,

1070
00:45:17,119 --> 00:45:20,480
тогда как если вы выполняете

1071
00:45:20,480 --> 00:45:23,119
алгоритм оптимизации для всей

1072
00:45:23,119 --> 00:45:25,760
матрицы сразу, вы на самом деле знаете

1073
00:45:25,760 --> 00:45:27,440
все о матрице сразу,

1074
00:45:27,440 --> 00:45:30,160
вы не просто смотрите на то, что  слова,

1075
00:45:30,160 --> 00:45:32,800
какие еще слова произошли в этом одном

1076
00:45:32,800 --> 00:45:35,520
контексте центрального слова, у вас

1077
00:45:35,520 --> 00:45:37,920
есть весь вектор совместной встречаемости,

1078
00:45:37,920 --> 00:45:40,400
учитывает центральное слово и другое

1079
00:45:40,400 --> 00:45:43,280
слово, и поэтому вы можете гораздо более

1080
00:45:43,280 --> 00:45:46,400
эффективно и менее шумно

1081
00:45:46,400 --> 00:45:50,400
решить, как минимизировать свои потери

1082
00:45:50,400 --> 00:45:52,560
хорошо,

1083
00:45:52,560 --> 00:45:53,920
я продолжу,

1084
00:45:53,920 --> 00:45:55,680
хорошо, так что

1085
00:45:55,680 --> 00:45:57,680
я вроде как сказал, посмотрите на эти векторы слов,

1086
00:45:57,680 --> 00:45:59,760
они великолепны, и я вроде как

1087
00:45:59,760 --> 00:46:01,839
показал вам несколько вещей в

1088
00:46:01,839 --> 00:46:04,000
конце последнего урока, в которых утверждалось, что

1089
00:46:04,000 --> 00:46:06,480
это здорово, вы знаете, что они работают  Из

1090
00:46:06,480 --> 00:46:09,680
этих аналогий они показывают сходство

1091
00:46:09,680 --> 00:46:12,480
и такие вещи, как этот, мы хотим сделать

1092
00:46:12,480 --> 00:46:14,480
это немного точнее,

1093
00:46:14,480 --> 00:46:16,400
и действительно, для обработки естественного языка,

1094
00:46:16,400 --> 00:46:18,800
как и в других областях машинного

1095
00:46:18,800 --> 00:46:21,200
обучения, большая часть того, что люди

1096
00:46:21,200 --> 00:46:23,760
делают, работает  хорошие способы

1097
00:46:23,760 --> 00:46:27,040
оценки имеющихся знаний,

1098
00:46:27,040 --> 00:46:29,040
так как мы можем действительно оценивать векторы слов.

1099
00:46:29,040 --> 00:46:32,800
В общем, для оценки НЛП

1100
00:46:32,800 --> 00:46:35,760
люди говорят о двух способах оценки:

1101
00:46:35,760 --> 00:46:39,200
внутреннем и внешнем, так что внутренняя

1102
00:46:39,200 --> 00:46:41,119
оценка

1103
00:46:41,119 --> 00:46:45,119
означает, что вы оцениваете непосредственно

1104
00:46:45,119 --> 00:46:48,000
конкретные или промежуточные подзадачи, которые

1105
00:46:48,000 --> 00:46:50,160
вы выполняете.  я работал, поэтому мне нужна

1106
00:46:50,160 --> 00:46:52,480
мера, с помощью которой я могу напрямую оценить, насколько

1107
00:46:52,480 --> 00:46:55,599
хороши мои векторы слов, и обычно

1108
00:46:55,599 --> 00:46:58,560
внутренние оценки вычисляются быстро,

1109
00:46:58,560 --> 00:47:01,200
они помогли вам

1110
00:47:01,200 --> 00:47:03,760
понять компонент, над которым вы работали, но

1111
00:47:03,760 --> 00:47:04,960
часто

1112
00:47:04,960 --> 00:47:08,640
просто попытка оптимизировать этот компонент

1113
00:47:08,640 --> 00:47:11,280
может  или может не иметь очень большого хорошего

1114
00:47:11,280 --> 00:47:14,079
эффекта на общую систему, которую вы

1115
00:47:14,079 --> 00:47:16,319
пытаетесь построить,

1116
00:47:16,319 --> 00:47:19,440
так что люди также очень

1117
00:47:19,440 --> 00:47:22,240
интересовались внешними оценками,

1118
00:47:22,240 --> 00:47:25,520
поэтому внешняя оценка заключается в том, что вы берете на себя

1119
00:47:25,520 --> 00:47:28,160
какую-то реальную задачу, представляющую интерес для

1120
00:47:28,160 --> 00:47:30,400
людей, будь то  веб-поиск,

1121
00:47:30,400 --> 00:47:32,880
машинный перевод или что-то в этом роде,

1122
00:47:32,880 --> 00:47:35,760
и вы говорите, что ваша цель -

1123
00:47:35,760 --> 00:47:38,240
действительно улучшить производительность этой

1124
00:47:38,240 --> 00:47:42,319
задачи,  Это реальное доказательство того, что

1125
00:47:42,319 --> 00:47:45,280
это делает что-то полезное, так что в некотором

1126
00:47:45,280 --> 00:47:47,680
смысле это явно лучше,

1127
00:47:47,680 --> 00:47:50,319
но, с другой стороны, у него также есть некоторые

1128
00:47:50,319 --> 00:47:52,000
недостатки:

1129
00:47:52,000 --> 00:47:54,960
требуется намного больше времени для

1130
00:47:54,960 --> 00:47:57,680
оценки внешней задачи, потому

1131
00:47:57,680 --> 00:48:01,040
что это гораздо большая система, и иногда

1132
00:48:01,040 --> 00:48:03,760
вы знаете  когда вы меняете что-

1133
00:48:03,760 --> 00:48:06,240
то, неясно, был ли факт, что

1134
00:48:06,240 --> 00:48:09,040
числа снизились, потому что у вас теперь

1135
00:48:09,040 --> 00:48:11,839
есть худшие векторы слов, или

1136
00:48:11,839 --> 00:48:14,480
просто каким-то образом другие компоненты

1137
00:48:14,480 --> 00:48:16,400
системы

1138
00:48:16,400 --> 00:48:18,400
лучше взаимодействовали с вашими старыми векторами слов,

1139
00:48:18,400 --> 00:48:19,839
и если вы измените другие

1140
00:48:19,839 --> 00:48:22,400
компоненты, а также вещи

1141
00:48:22,400 --> 00:48:24,079
снова станет лучше, так что

1142
00:48:24,079 --> 00:48:26,319
в некотором смысле иногда может быть

1143
00:48:26,319 --> 00:48:29,040
сложнее увидеть, достигаете ли вы прогресса,

1144
00:48:29,040 --> 00:48:32,839
но я коснусь обоих этих методов

1145
00:48:32,839 --> 00:48:36,400
здесь, так что для внутренней оценки

1146
00:48:36,400 --> 00:48:37,839
векторов слов в

1147
00:48:37,839 --> 00:48:38,880
одну сторону,

1148
00:48:38,880 --> 00:48:42,800
о которой мы упоминали в прошлый раз, было это

1149
00:48:42,800 --> 00:48:45,280
аналогия вектора слова, чтобы мы могли просто

1150
00:48:45,280 --> 00:48:48,480
дать нашим моделям большой набор

1151
00:48:48,480 --> 00:48:50,880
задач аналогии вектора слова, чтобы мы могли сказать, что

1152
00:48:50,880 --> 00:48:54,079
мужчина - это женщина, поскольку король - это что, и

1153
00:48:54,079 --> 00:48:57,119
попросить модель определить  Найдите слово, которое

1154
00:48:57,119 --> 00:49:00,160
ближе всего, используя подобного рода вычисление аналогии со словами,

1155
00:49:00,160 --> 00:49:03,200
и надейтесь, что то, что выходит,

1156
00:49:03,200 --> 00:49:05,680
- королева,

1157
00:49:05,680 --> 00:49:07,760
и это то, что люди сделали

1158
00:49:07,760 --> 00:49:10,240
и выработали оценку точности того,

1159
00:49:10,240 --> 00:49:13,680
как часто вы правы

1160
00:49:13,920 --> 00:49:16,559
в этот момент, я просто должен упомянуть  одна

1161
00:49:16,559 --> 00:49:18,960
маленькая уловка из этих словесных векторных

1162
00:49:18,960 --> 00:49:22,079
аналогий, которую все используют, но не

1163
00:49:22,079 --> 00:49:24,559
все о ней много говорят в первую

1164
00:49:24,559 --> 00:49:26,720
очередь, я имею в виду,

1165
00:49:26,720 --> 00:49:28,319
что есть небольшая уловка, которую вы можете

1166
00:49:28,319 --> 00:49:30,880
найти в коде симулятора, если вы посмотрите на него,

1167
00:49:30,880 --> 00:49:34,240
что когда это действительно удается женщине как

1168
00:49:34,240 --> 00:49:35,920
королю

1169
00:49:40,079 --> 00:49:43,119
часто

1170
00:49:43,119 --> 00:49:45,760
случается то, что на самом деле слово, когда вы делаете

1171
00:49:45,760 --> 00:49:48,240
свои плюсы и минусы,

1172
00:49:48,240 --> 00:49:52,400
слово, которое на самом деле будет самым близким, по-

1173
00:49:52,400 --> 00:49:54,480
прежнему является королем,

1174
00:49:55,200 --> 00:49:57,440
поэтому люди всегда делают это так, что

1175
00:49:57,440 --> 00:50:00,079
они не позволяют ни одному из трех  введите

1176
00:50:00,079 --> 00:50:03,440
слова um в процессе выбора, поэтому

1177
00:50:03,440 --> 00:50:05,760
вы выбираете ближайшее слово,

1178
00:50:05,760 --> 00:50:09,200
которое не является одним из тех, что слова

1179
00:50:09,520 --> 00:50:11,200
um ok,

1180
00:50:11,200 --> 00:50:12,480
так как

1181
00:50:12,480 --> 00:50:14,400
um здесь показывает результаты из

1182
00:50:14,400 --> 00:50:17,200
векторов GloVe, поэтому факторы GloVe

1183
00:50:17,200 --> 00:50:20,480
имеют этот штрих  ng свойство линейного компонента

1184
00:50:20,480 --> 00:50:24,000
точно так же, как я показал ранее um

1185
00:50:24,000 --> 00:50:24,800
для

1186
00:50:25,680 --> 00:50:28,000
угля um, так что это

1187
00:50:28,000 --> 00:50:31,280
для мужского и женского измерения, и

1188
00:50:31,280 --> 00:50:34,319
поэтому во многих случаях можно ожидать,

1189
00:50:34,319 --> 00:50:36,559
что аналогии слов будут работать, потому что я

1190
00:50:36,559 --> 00:50:38,880
могу взять векторную разность человека

1191
00:50:38,880 --> 00:50:41,520
и  женщина, а затем, если я добавлю эту векторную

1192
00:50:41,520 --> 00:50:44,079
разницу к брату, я ожидаю

1193
00:50:44,079 --> 00:50:45,359
получить сестру

1194
00:50:45,359 --> 00:50:47,760
и король-королеву и из любого из этих

1195
00:50:47,760 --> 00:50:50,400
примеров, но, конечно, они могут не

1196
00:50:50,400 --> 00:50:52,559
всегда работать правильно, потому что если я начну

1197
00:50:52,559 --> 00:50:55,040
с императора, это будет

1198
00:50:55,040 --> 00:50:57,599
скорее  и поэтому может оказаться, что

1199
00:50:57,599 --> 00:51:02,079
вместо этого выйдет графиня или герцогиня,

1200
00:51:02,240 --> 00:51:04,319
вы можете сделать это для различных

1201
00:51:04,319 --> 00:51:06,079
отношений, таких разных семантических

1202
00:51:06,079 --> 00:51:08,079
отношений, так что

1203
00:51:08,079 --> 00:51:09,520
эти типы словарных векторов на самом деле

1204
00:51:09,520 --> 00:51:10,960
узнают довольно много всего лишь мировых

1205
00:51:10,960 --> 00:51:14,240
знаний, так что вот генеральный директор компании

1206
00:51:14,240 --> 00:51:16,839
или это  является генеральным директором компании примерно с

1207
00:51:16,839 --> 00:51:20,160
2010 по 2014 год, когда данные были взяты

1208
00:51:20,160 --> 00:51:21,359
из

1209
00:51:21,359 --> 00:51:23,359
векторов слов,

1210
00:51:23,359 --> 00:51:25,680
и они, а также семантические или

1211
00:51:25,680 --> 00:51:27,839
прагматические вещи, подобные этому, также

1212
00:51:27,839 --> 00:51:30,000
изучают синтаксические вещи, поэтому вот

1213
00:51:30,000 --> 00:51:32,640
векторы для po  сравнительные и

1214
00:51:32,640 --> 00:51:35,520
превосходные формы прилагательных, и вы

1215
00:51:35,520 --> 00:51:38,800
можете видеть, что они также перемещаются и примерно

1216
00:51:38,800 --> 00:51:42,400
линейные компоненты, так что слово, чтобы

1217
00:51:42,400 --> 00:51:44,559
направить людей, построили набор данных

1218
00:51:44,559 --> 00:51:46,800
аналогий, чтобы вы могли оценивать

1219
00:51:46,800 --> 00:51:48,400
различные модели на предмет точности

1220
00:51:48,400 --> 00:51:50,559
их аналогий,

1221
00:51:50,559 --> 00:51:52,319
и

1222
00:51:52,319 --> 00:51:54,079
вот как вы  может это сделать, и это

1223
00:51:54,079 --> 00:51:56,400
дает некоторые числа, поэтому есть семантические

1224
00:51:56,400 --> 00:51:58,720
и синтаксические аналогии, я просто посмотрю

1225
00:51:58,720 --> 00:52:00,800
на итоги,

1226
00:52:00,800 --> 00:52:03,040
хорошо, так что я сказал раньше, если вы

1227
00:52:03,040 --> 00:52:06,079
просто используете немасштабированные

1228
00:52:06,079 --> 00:52:08,319
счетчики совпадений

1229
00:52:08,319 --> 00:52:11,040
и передаете их через svd, все работает

1230
00:52:11,040 --> 00:52:12,800
ужасно, и вы  видите, что вы

1231
00:52:12,800 --> 00:52:16,160
получаете только 7,3, но затем, как я также указал,

1232
00:52:16,160 --> 00:52:18,319
если вы выполните некоторое масштабирование, вы действительно можете

1233
00:52:18,319 --> 00:52:19,599
получить svd

1234
00:52:19,599 --> 00:52:22,400
для масштабированной матрицы подсчета, чтобы работать

1235
00:52:22,400 --> 00:52:26,640
достаточно хорошо, поэтому этот

1236
00:52:26,640 --> 00:52:29,040
spdl похож на модель Коля, и теперь

1237
00:52:29,040 --> 00:52:31,280
мы встаем  до 60,1, что на самом

1238
00:52:31,280 --> 00:52:33,040
деле неплохой результат, так что вы

1239
00:52:33,040 --> 00:52:34,880
действительно можете достойно работать без

1240
00:52:34,880 --> 00:52:37,839
нейронной сети, а затем вот

1241
00:52:37,839 --> 00:52:41,040
два варианта модели

1242
00:52:41,040 --> 00:52:43,839
um word2vec, и вот наши

1243
00:52:44,880 --> 00:52:46,800
результаты  модель GloVe и, конечно же, в

1244
00:52:48,000 --> 00:52:52,000
2014 году мы восприняли это как абсолютное доказательство того,

1245
00:52:52,000 --> 00:52:54,640
что наша модель была лучше, и наше более

1246
00:52:54,640 --> 00:52:57,440
эффективное использование статистики действительно

1247
00:52:57,440 --> 00:53:00,640
работало в нашу пользу эм с семью

1248
00:53:00,640 --> 00:53:03,280
годами ретроспективы, я думаю, что это не

1249
00:53:03,280 --> 00:53:05,839
совсем так, оказывается  Я думаю, что

1250
00:53:05,839 --> 00:53:08,480
основная причина того, почему мы набрали больше очков, заключается в

1251
00:53:08,480 --> 00:53:10,880
том, что у нас на самом деле были лучшие данные, и поэтому

1252
00:53:10,880 --> 00:53:13,760
на следующем слайде есть немного доказательств

1253
00:53:13,760 --> 00:53:16,720
этого, поэтому здесь рассматривается

1254
00:53:17,920 --> 00:53:20,400
семантическая синтаксическая и общая

1255
00:53:20,400 --> 00:53:24,319
производительность на словесных аналогиях моделей GloVe,

1256
00:53:24,319 --> 00:53:27,040
которые были обучены  на разных

1257
00:53:27,040 --> 00:53:30,079
подмножествах данных, поэтому, в частности, двое

1258
00:53:30,079 --> 00:53:34,319
слева обучаются в Википедии,

1259
00:53:34,319 --> 00:53:35,760
и вы можете видеть, что обучение в

1260
00:53:35,760 --> 00:53:38,800
Википедии помогает вам хорошо

1261
00:53:38,800 --> 00:53:41,520
разбираться в семантических аналогиях, что, возможно, имеет

1262
00:53:41,520 --> 00:53:43,520
смысл, потому что Википедия просто сообщает вам

1263
00:53:43,520 --> 00:53:45,280
много семантических фактов, я имею в виду, что  что-то вроде

1264
00:53:45,280 --> 00:53:45,920
того,

1265
00:53:45,920 --> 00:53:48,000
что делают энциклопедии,

1266
00:53:48,000 --> 00:53:50,640
и поэтому одним из наших больших преимуществ

1267
00:53:50,640 --> 00:53:54,240
была эта википедия,

1268
00:53:54,240 --> 00:53:56,800
в которой модель GloVe была частично обучена

1269
00:53:56,800 --> 00:53:59,280
в Википедии, а также в других тексах.  ts, в

1270
00:53:59,280 --> 00:54:01,119
то время как модель word2vec, которая была

1271
00:54:01,119 --> 00:54:03,440
выпущена, была обучена исключительно в

1272
00:54:03,440 --> 00:54:06,800
новостях Google, поэтому данные ленты новостей, и если вы

1273
00:54:06,800 --> 00:54:10,240
тренируетесь только на небольшом количестве

1274
00:54:10,240 --> 00:54:13,280
данных ленты новостей, вы можете видеть, что для

1275
00:54:13,280 --> 00:54:16,160
семантики это просто не так хорошо, как

1276
00:54:17,520 --> 00:54:20,240
даже четверть  от

1277
00:54:20,240 --> 00:54:23,040
размера данных Википедии, хотя, если вы получаете

1278
00:54:23,040 --> 00:54:25,119
много данных, вы можете компенсировать это, поэтому

1279
00:54:25,119 --> 00:54:27,599
здесь, справа, у вас

1280
00:54:27,599 --> 00:54:30,079
были общие веб-данные для сканирования, и поэтому,

1281
00:54:30,079 --> 00:54:32,400
когда веб-данных много, так что теперь 42

1282
00:54:32,400 --> 00:54:35,119
миллиарда слов гм  затем вы снова начинаете

1283
00:54:35,119 --> 00:54:37,280
добавлять хорошие оценки с семантической

1284
00:54:37,280 --> 00:54:39,440
стороны.

1285
00:54:39,440 --> 00:54:42,960
График справа показывает, насколько

1286
00:54:42,960 --> 00:54:45,440
хорошо вы делаете это при увеличении

1287
00:54:45,440 --> 00:54:49,200
размерности вектора, и вы можете видеть то,

1288
00:54:49,200 --> 00:54:51,680
что вы знаете, что 25-мерные

1289
00:54:51,680 --> 00:54:54,240
векторы не очень  хорошо, что они доходят до

1290
00:54:54,240 --> 00:54:57,440
50, а затем до 100, и поэтому 100 размерных

1291
00:54:57,440 --> 00:54:59,920
векторов уже работают достаточно хорошо,

1292
00:54:59,920 --> 00:55:01,359
поэтому я использовал сотни размерных

1293
00:55:01,359 --> 00:55:02,319
векторов,

1294
00:55:02,319 --> 00:55:06,960
когда показывал свой пример в классе, но

1295
00:55:06,960 --> 00:55:10,400
это сладкий запас, слишком долгая загрузка и

1296
00:55:10,400 --> 00:55:12,160
работа достаточно хорошо  Но вы все равно

1297
00:55:12,160 --> 00:55:14,480
получаете значительный прирост за 200, а это

1298
00:55:14,480 --> 00:55:17,119
несколько до 300, так что, по крайней мере,

1299
00:55:17,119 --> 00:55:20,240
в 2013–15 гг. все

1300
00:55:20,240 --> 00:55:22,480
тяготели к тому факту, что 300-

1301
00:55:22,480 --> 00:55:25,200
мерные векторы - это золотая середина, ммм

1302
00:55:25,200 --> 00:55:27,599
так почти часто, если вы

1303
00:55:27,599 --> 00:55:29,760
просматриваете самые известные наборы  векторов слов, которые

1304
00:55:29,760 --> 00:55:32,319
включают в себя векторы слов divec и

1305
00:55:32,319 --> 00:55:35,200
векторы GloVe, которые обычно

1306
00:55:35,200 --> 00:55:38,960
представляют собой 300-мерные векторы слов.

1307
00:55:38,960 --> 00:55:41,359
Это не единственная внутренняя

1308
00:55:41,359 --> 00:55:43,280
оценка, которую вы можете сделать.

1309
00:55:43,280 --> 00:55:45,599
Еще одна внутренняя оценка, которую вы можете сделать,

1310
00:55:45,599 --> 00:55:47,599
это посмотреть, как

1311
00:55:47,599 --> 00:55:50,720
эти модели моделируют человеческие суждения о

1312
00:55:50,720 --> 00:55:54,559
сходстве слов.  гм, поэтому психологи в течение

1313
00:55:54,559 --> 00:55:57,680
нескольких десятилетий фактически

1314
00:55:57,680 --> 00:56:00,559
принимали человеческие суждения о подобии слов,

1315
00:56:00,559 --> 00:56:03,920
когда буквально вы просите людей

1316
00:56:06,720 --> 00:56:09,280
называть пары слов, такие как профессор и доктор, чтобы дать им оценку сходства, которая

1317
00:56:09,280 --> 00:56:10,640
вроде бы измеряется как некоторая

1318
00:56:10,640 --> 00:56:13,280
непрерывная величина, дающая вам оценку

1319
00:56:13,280 --> 00:56:17,040
между, скажем,  0 и десять мкм, и поэтому

1320
00:56:17,040 --> 00:56:19,040
есть человеческие суждения, которые затем

1321
00:56:19,040 --> 00:56:21,280
усредняются по множеству человеческих суждений

1322
00:56:21,280 --> 00:56:24,079
о том, насколько похожи разные  Такие слова такие

1323
00:56:24,079 --> 00:56:26,640
тигр и кот очень похожи,

1324
00:56:26,640 --> 00:56:28,960
компьютер и интернет очень похожи,

1325
00:56:28,960 --> 00:56:31,680
самолет, а машина меньше похожа, и

1326
00:56:31,680 --> 00:56:34,480
компакт-диск совсем не похожи, но стоковая

1327
00:56:34,480 --> 00:56:38,319
и ягуар еще менее похожи,

1328
00:56:38,319 --> 00:56:39,920
так что мы могли бы сказать,

1329
00:56:40,960 --> 00:56:44,240
что наши модели делают  у них одинаковые

1330
00:56:44,240 --> 00:56:47,119
суждения о сходстве, и, в частности,

1331
00:56:47,119 --> 00:56:49,920
мы можем измерить коэффициент корреляции

1332
00:56:49,920 --> 00:56:51,599
того, дают ли они одинаковый порядок

1333
00:56:52,319 --> 00:56:54,960
суждений о сходстве, и тогда мы можем

1334
00:56:54,960 --> 00:56:57,359
получить данные для этого, и, таким образом, существуют

1335
00:56:57,359 --> 00:56:59,760
различные наборы данных о сходстве слов,

1336
00:56:59,760 --> 00:57:02,319
и мы можем оценивать разные

1337
00:57:02,319 --> 00:57:04,880
модели относительно того, насколько хорошо они справляются с

1338
00:57:04,880 --> 00:57:07,440
подобием, и снова

1339
00:57:07,440 --> 00:57:10,559
вы видите здесь, что простые svds

1340
00:57:10,559 --> 00:57:14,000
и здесь работают сравнительно лучше для

1341
00:57:14,000 --> 00:57:16,000
сходства, чем для аналогий,

1342
00:57:16,000 --> 00:57:17,520
вы знаете, что это не очень хорошо, но теперь это не

1343
00:57:17,520 --> 00:57:19,119
совсем ужасно,

1344
00:57:19,119 --> 00:57:20,960
потому что нам больше не нужно это линейное

1345
00:57:20,960 --> 00:57:23,760
свойство, но снова  масштабированные svds работают

1346
00:57:23,760 --> 00:57:25,760
намного лучше

1347
00:57:25,760 --> 00:57:28,400
word2vec работает немного лучше,

1348
00:57:28,400 --> 00:57:30,000
и мы получили некоторые из тех же

1349
00:57:30,000 --> 00:57:33,760
незначительных преимуществ от модели GloVe

1350
00:57:33,760 --> 00:57:35,440
эй chr  извините, что прерываю,

1351
00:57:35,440 --> 00:57:37,520
многие студенты спрашивали, можете ли вы

1352
00:57:37,520 --> 00:57:39,680
заново объяснить целевую функцию для

1353
00:57:39,680 --> 00:57:42,240
модели GloVe, а также что

1354
00:57:42,240 --> 00:57:45,119
означает логарифмическая билинейность.

1355
00:57:55,920 --> 00:57:58,640
так что один слайд

1356
00:57:58,640 --> 00:58:02,720
перед этим правым, так что свойство, которое

1357
00:58:02,720 --> 00:58:04,559
мы хотим,

1358
00:58:04,559 --> 00:58:08,640
состоит в том, что мы хотим, чтобы скалярное произведение

1359
00:58:08,640 --> 00:58:12,720
um представляло логарифмическую вероятность

1360
00:58:12,720 --> 00:58:14,319
совместной встречаемости,

1361
00:58:14,319 --> 00:58:15,200
поэтому

1362
00:58:15,200 --> 00:58:18,559
um, и это дает мне мой сложный

1363
00:58:18,559 --> 00:58:22,160
лог-билинейный, поэтому покупка заключается в том, что есть своего

1364
00:58:22,160 --> 00:58:24,640
рода wi  и wj, так что есть

1365
00:58:24,640 --> 00:58:26,640
как бы две

1366
00:58:26,640 --> 00:58:27,760
линейные

1367
00:58:27,760 --> 00:58:30,319
вещи, и это линейно в каждой из

1368
00:58:30,319 --> 00:58:33,920
них, так что это похоже на наличие и,

1369
00:58:33,920 --> 00:58:37,040
а не на своего рода топор, где у

1370
00:58:37,040 --> 00:58:39,280
вас просто есть что-то линейное

1371
00:58:39,280 --> 00:58:42,880
по x, а a - константа  это билинейно,

1372
00:58:42,880 --> 00:58:45,839
потому что у нас есть wiwj, и

1373
00:58:45,839 --> 00:58:48,160
в них обоих есть линейный, и это затем

1374
00:58:48,160 --> 00:58:51,200
связано с логарифмом вероятности, и

1375
00:58:51,200 --> 00:58:55,520
поэтому это дает нам журнал по линейной модели,

1376
00:58:56,799 --> 00:58:58,000
и так

1377
00:58:58,000 --> 00:59:01,200
как мы, поскольку мы хотим, чтобы эти вещи

1378
00:59:01,200 --> 00:59:03,200
были равны

1379
00:59:03,200 --> 00:59:05,440
тем, что мы  делаем ее  e, если вы проигнорируете

1380
00:59:05,440 --> 00:59:07,839
эти два центральных члена, это означает, что мы

1381
00:59:07,839 --> 00:59:11,359
хотим сказать, что разница между

1382
00:59:11,359 --> 00:59:12,319
этими

1383
00:59:12,319 --> 00:59:15,040
двумя минимальна, поэтому

1384
00:59:15,040 --> 00:59:16,799
мы берем эту разницу и

1385
00:59:16,799 --> 00:59:19,200
возводим ее в квадрат, так что она всегда положительна, и

1386
00:59:19,200 --> 00:59:20,720
мы хотим, чтобы этот

1387
00:59:20,720 --> 00:59:24,640
член в квадрате был  быть как можно меньше,

1388
00:59:24,640 --> 00:59:27,920
и вы знаете, что это 90 процентов, и

1389
00:59:27,920 --> 00:59:30,559
вы можете в основном остановиться на этом, но

1390
00:59:30,559 --> 00:59:32,480
другая часть, которая здесь присутствует,

1391
00:59:32,480 --> 00:59:34,319
- это много времени,

1392
00:59:34,319 --> 00:59:35,520
когда вы

1393
00:59:35,520 --> 00:59:39,040
строите модели, а не

1394
00:59:39,040 --> 00:59:42,559
просто имеете своего рода модель топора, кажется

1395
00:59:42,559 --> 00:59:45,839
полезно иметь термин смещения, который может

1396
00:59:45,839 --> 00:59:47,839
перемещать вещи вверх и вниз

1397
00:59:48,640 --> 00:59:51,920
для слова в целом, и поэтому мы добавляем его

1398
00:59:51,920 --> 00:59:54,240
в термин смещения модели, чтобы

1399
00:59:54,240 --> 00:59:57,680
для обоих слов был термин смещения, поэтому, если в

1400
00:59:57,680 --> 01:00:00,000
целом вероятность высока для

1401
01:00:00,000 --> 01:00:02,640
определенного слова, это смещение  термин может смоделировать

1402
01:00:02,640 --> 01:00:05,280
это, а для другого слова этот термин предвзятости,

1403
01:00:05,280 --> 01:00:07,280
затем смоделировать его,

1404
01:00:07,280 --> 01:00:08,720
хорошо,

1405
01:00:08,720 --> 01:00:10,559
так что теперь я вернусь,

1406
01:00:10,559 --> 01:00:12,240
и после того, как

1407
01:00:12,240 --> 01:00:13,440
э-э, на

1408
01:00:13,440 --> 01:00:16,240
самом деле, я только что увидел, как кто-то сказал, почему

1409
01:00:16,240 --> 01:00:19,440
умножение на f,

1410
01:00:19,440 --> 01:00:22,000
извините, я пропустил этот последний термин,

1411
01:00:23,680 --> 01:00:26,960
ладно, почему  модифицируя этим f элемента

1412
01:00:26,960 --> 01:00:30,960
xij, так что t  его последняя часть заключалась в том,

1413
01:00:30,960 --> 01:00:32,000
чтобы

1414
01:00:32,000 --> 01:00:36,720
масштабировать вещи в зависимости от частоты

1415
01:00:36,720 --> 01:00:40,319
употребления слова, потому что

1416
01:00:40,319 --> 01:00:41,520
вы хотите

1417
01:00:41,520 --> 01:00:43,760
уделять больше внимания

1418
01:00:44,640 --> 01:00:47,760
более распространенным словам или парам слов,

1419
01:00:47,760 --> 01:00:50,400
которые встречаются чаще, потому что вы знаете, что если

1420
01:00:50,400 --> 01:00:52,880
вы думаете об этом, вы понимаете,

1421
01:00:52,880 --> 01:00:55,040
что вы  видя, есть

1422
01:00:55,040 --> 01:00:57,119
ли у вещей счет совместного

1423
01:00:57,119 --> 01:00:59,359
возникновения 50 против

1424
01:00:59,359 --> 01:01:00,640
3,

1425
01:01:00,640 --> 01:01:03,680
вы хотите лучше

1426
01:01:03,680 --> 01:01:07,440
моделировать совместное возникновение вещей, которые

1427
01:01:07,440 --> 01:01:11,200
произошли вместе 50 раз,

1428
01:01:11,200 --> 01:01:12,400
и поэтому

1429
01:01:12,400 --> 01:01:14,640
вы хотите учитывать при подсчете

1430
01:01:14,640 --> 01:01:16,079
совпадения,

1431
01:01:16,079 --> 01:01:18,559
но тогда  аргумент состоит в том, что на

1432
01:01:18,559 --> 01:01:20,960
самом деле это сбивает вас с пути, когда у вас есть

1433
01:01:20,960 --> 01:01:23,119
чрезвычайно распространенные слова, такие как функциональные

1434
01:01:23,119 --> 01:01:26,319
слова, и вы так эффективно обращали больше

1435
01:01:26,319 --> 01:01:28,079
внимания

1436
01:01:28,079 --> 01:01:29,119
на

1437
01:01:29,119 --> 01:01:31,760
слова, которые сочетались вместе

1438
01:01:31,760 --> 01:01:34,160
до определенной точки, а затем кривая просто

1439
01:01:34,160 --> 01:01:36,480
сглаживалась, поэтому не имело значения, если  это

1440
01:01:36,480 --> 01:01:38,880
было чрезвычайно распространенное слово,

1441
01:01:38,880 --> 01:01:40,000
так что тогда

1442
01:01:40,000 --> 01:01:42,640
гм для

1443
01:01:42,640 --> 01:01:45,680
внешней оценки вектора слов, так что на

1444
01:01:45,680 --> 01:01:47,200
этом этапе

1445
01:01:47,200 --> 01:01:49,680
вы теперь хотите вроде как сказать,

1446
01:01:49,680 --> 01:01:53,119
можем ли мы встроить наши векторы слов в какую-то

1447
01:01:53,119 --> 01:01:57,680
задачу конечного пользователя, и помогают ли они

1448
01:01:57,680 --> 01:01:59,839
и  работают ли разные векторы слов

1449
01:01:59,839 --> 01:02:03,200
лучше или хуже, чем другие векторы слов,

1450
01:02:03,200 --> 01:02:05,280
так что это то, что мы увидим

1451
01:02:05,280 --> 01:02:07,839
много позже в классе, я имею в виду, в

1452
01:02:07,839 --> 01:02:09,680
частности,

1453
01:02:09,680 --> 01:02:11,520
когда вы перейдете к выполнению

1454
01:02:11,520 --> 01:02:13,920
третьего задания, которое вы получите для

1455
01:02:13,920 --> 01:02:16,400
создания синтаксических анализаторов зависимостей и  затем вы можете

1456
01:02:18,079 --> 01:02:19,920
использовать векторы слов в

1457
01:02:19,920 --> 01:02:22,559
анализаторе зависимостей и посмотреть, насколько они помогают, мы на

1458
01:02:22,559 --> 01:02:23,920
самом деле не заставляем вас тестировать

1459
01:02:23,920 --> 01:02:26,079
разные наборы векторов слов, но вы

1460
01:02:26,079 --> 01:02:28,240
могли бы

1461
01:02:28,240 --> 01:02:30,240
вот только один пример этого, чтобы дать

1462
01:02:30,240 --> 01:02:32,960
вам смысл, поэтому задача named

1463
01:02:32,960 --> 01:02:35,440
Распознавание сущностей проходит через фрагмент

1464
01:02:35,440 --> 01:02:38,400
текста и идентифицирует упоминания имени

1465
01:02:38,400 --> 01:02:40,960
человека или названия организации, например,

1466
01:02:40,960 --> 01:02:46,079
компании или местоположения, и поэтому,

1467
01:02:46,079 --> 01:02:49,760
если у вас есть хорошие векторы слов, ммм, они

1468
01:02:49,760 --> 01:02:52,240
помогают вам лучше распознавать именованные сущности,

1469
01:02:52,240 --> 01:02:55,039
и ответ на это  да, так что

1470
01:02:55,039 --> 01:02:57,680
если кто-то начинает с модели, которая

1471
01:02:57,680 --> 01:03:01,119
просто имеет дискретные функции, поэтому она использует

1472
01:03:01,119 --> 01:03:03,839
идентичность слов в качестве функций, вы можете

1473
01:03:03,839 --> 01:03:05,760
построить довольно хорошую именованную модель сущности, делая

1474
01:03:05,760 --> 01:03:08,960
это, но если вы добавите в нее векторы слов,

1475
01:03:08,960 --> 01:03:11,119
вы  получить лучшее представление о

1476
01:03:11,119 --> 01:03:13,440
значении слов, чтобы вы

1477
01:03:13,440 --> 01:03:16,480
могли немного увеличить числа, а

1478
01:03:16,480 --> 01:03:19,119
затем вы могли сравнивать разные модели, чтобы

1479
01:03:19,119 --> 01:03:21,760
увидеть, какой выигрыш они дают вам с точки

1480
01:03:21,760 --> 01:03:24,559
зрения этой внешней задачи,

1481
01:03:24,559 --> 01:03:26,640
так что забегая вперед,

1482
01:03:26,640 --> 01:03:28,400
это был вопрос  что после урока меня спросили,

1483
01:03:30,559 --> 01:03:31,599
что это за

1484
01:03:31,599 --> 01:03:35,039
слова, потому что до сих пор

1485
01:03:35,039 --> 01:03:36,799
у нас было только

1486
01:03:36,799 --> 01:03:37,760
одно

1487
01:03:37,760 --> 01:03:40,160
слово,

1488
01:03:40,160 --> 01:03:42,720
извините за одну конкретную строку, у нас

1489
01:03:42,720 --> 01:03:45,200
есть некоторый строковый дом, и мы собираемся

1490
01:03:45,200 --> 01:03:47,920
сказать для каждой из этих строк есть

1491
01:03:47,920 --> 01:03:49,359
вектор слов,

1492
01:03:49,359 --> 01:03:52,319
и если  вы думаете об этом немного больше,

1493
01:03:52,319 --> 01:03:54,319
что кажется

1494
01:03:54,319 --> 01:03:55,359
очень

1495
01:03:55,359 --> 01:03:59,599
странным, потому что на самом деле большинство слов,

1496
01:03:59,599 --> 01:04:01,920
особенно распространенные слова и особенно

1497
01:04:01,920 --> 01:04:04,480
слова, которые существуют в течение долгого времени, на

1498
01:04:04,480 --> 01:04:06,880
самом деле имеют много значений, которые

1499
01:04:06,880 --> 01:04:09,119
очень разные, так как это можно

1500
01:04:09,119 --> 01:04:11,039
уловить, если у вас есть только один

1501
01:04:11,039 --> 01:04:13,039
вектор слова для слова, потому что вы не можете на

1502
01:04:13,039 --> 01:04:15,119
самом деле уловить тот факт, что у вас

1503
01:04:15,119 --> 01:04:17,359
есть разные значения слова,

1504
01:04:17,359 --> 01:04:18,960
потому что ваше значение для слова - это

1505
01:04:18,960 --> 01:04:21,920
всего лишь одна точка в пространстве, один вектор

1506
01:04:21,920 --> 01:04:23,039
и так,

1507
01:04:23,039 --> 01:04:24,799
например,  об этом

1508
01:04:24,799 --> 01:04:27,599
вот слово "пик"

1509
01:04:27,599 --> 01:04:28,799
теперь

1510
01:04:28,799 --> 01:04:30,400
это на самом деле,

1511
01:04:30,400 --> 01:04:32,000
но это старое немецкое

1512
01:04:32,000 --> 01:04:33,680
слово хорошо, какое значение

1513
01:04:33,680 --> 01:04:35,200
имеет слово щука,

1514
01:04:36,400 --> 01:04:38,799
так что, может быть, вы можете просто подумать на минуту

1515
01:04:38,799 --> 01:04:41,440
и подумать,

1516
01:04:41,440 --> 01:04:46,480
какие значения имеет слово щука,

1517
01:04:46,559 --> 01:04:47,920
и оно на самом деле оказывается  как вы знаете, это

1518
01:04:47,920 --> 01:04:50,720
имеет много разных значений,

1519
01:04:50,720 --> 01:04:54,559
так что, возможно, самое основное значение - эм,

1520
01:04:54,559 --> 01:04:57,039
если вы играли в фэнтезийные игры или какое-то

1521
01:04:57,039 --> 01:05:00,160
средневековое оружие, эм, остроконечный

1522
01:05:00,160 --> 01:05:02,480
посох, есть пика, эм, но

1523
01:05:02,480 --> 01:05:04,400
есть рыба, которая имеет похожую

1524
01:05:04,400 --> 01:05:08,319
удлиненную форму  это щука, эм, она

1525
01:05:08,319 --> 01:05:09,520
использовалась для

1526
01:05:09,520 --> 01:05:13,039
железных дорог, эм, возможно, это употребление

1527
01:05:13,039 --> 01:05:15,119
больше не используется, но, безусловно, все еще

1528
01:05:15,119 --> 01:05:18,160
используется в отношении дорог, так что

1529
01:05:18,160 --> 01:05:20,559
это похоже на то, когда у вас есть магистрали, у нас есть

1530
01:05:20,559 --> 01:05:23,039
выражения, где щука означает будущее,

1531
01:05:23,039 --> 01:05:25,440
например, спуск по пике

1532
01:05:25,440 --> 01:05:27,680
это положение в дайвинге,

1533
01:05:27,680 --> 01:05:30,079
когда ныряльщики ловят щуку,

1534
01:05:30,079 --> 01:05:32,319
это все теперь петли,

1535
01:05:32,319 --> 01:05:34,799
они также используются словесно, так что вы можете поймать

1536
01:05:34,799 --> 01:05:37,599
кого-то своей щукой,

1537
01:05:37,599 --> 01:05:39,839
вы знаете, что в Австралии у разных видов использования может быть

1538
01:05:39,839 --> 01:05:41,359
другая валюта

1539
01:05:41,359 --> 01:05:43,520
вы  также можно использовать «

1540
01:05:43,520 --> 01:05:45,520
щуку», чтобы обозначить, что вы отказываетесь от выполнения

1541
01:05:45,520 --> 01:05:47,200
чего-то вроде, я думаю, он собирается

1542
01:05:47,200 --> 01:05:49,280
щучать,

1543
01:05:49,280 --> 01:05:50,880
я не думаю, что это употребление используется в

1544
01:05:50,880 --> 01:05:52,640
Америке, но много значений и, на

1545
01:05:52,640 --> 01:05:54,400
самом деле, для слов, которые встречаются чаще, если

1546
01:05:54,400 --> 01:05:56,799
вы начинаете думать о таких словах, как линия или

1547
01:05:56,799 --> 01:05:59,039
поле я имею в виду, что у них просто еще больше

1548
01:05:59,039 --> 01:06:01,200
значений, чем это, так

1549
01:06:01,200 --> 01:06:03,599
что мы на самом деле делаем только с одним

1550
01:06:03,599 --> 01:06:05,599
вектором для слова,

1551
01:06:05,599 --> 01:06:06,559
и что

1552
01:06:06,559 --> 01:06:07,359
ж,

1553
01:06:07,359 --> 01:06:10,160
один из способов, которым вы могли бы пойти, - это сказать хорошо,

1554
01:06:10,160 --> 01:06:12,559
до сих пор то, что мы делали, - это сумасшедшая

1555
01:06:12,559 --> 01:06:14,720
щука и другие слова  имеют все

1556
01:06:14,720 --> 01:06:17,200
эти разные значения, так что, возможно, нам

1557
01:06:18,480 --> 01:06:21,599
следует иметь разные векторы слов для

1558
01:06:21,599 --> 01:06:24,079
разных значений пики, чтобы у нас был

1559
01:06:24,079 --> 01:06:25,119
один

1560
01:06:25,119 --> 01:06:27,760
вектор слов для средневекового остроконечного

1561
01:06:27,760 --> 01:06:30,880
оружия, другой вектор слов для

1562
01:06:30,880 --> 01:06:33,680
вида рыбы, другой вектор слова для вида

1563
01:06:33,680 --> 01:06:36,240
дороги, так что тогда они будут векторами смыслового слова,

1564
01:06:39,280 --> 01:06:42,400
и вы можете это сделать, я имею в виду, что на самом деле

1565
01:06:42,400 --> 01:06:45,799
мы работали над этим в начале

1566
01:06:45,799 --> 01:06:48,960
2010-х, на самом деле, даже до того, как word2vec

1567
01:06:48,960 --> 01:06:50,559
появилось,

1568
01:06:50,559 --> 01:06:51,440
так что

1569
01:06:51,440 --> 01:06:54,000
эта картинка немного мала,

1570
01:06:54,000 --> 01:06:54,960
чтобы увидеть,

1571
01:06:54,960 --> 01:06:57,520
чем мы были  для

1572
01:06:57,520 --> 01:07:00,799
слов, которые мы работаем, кластеризовали

1573
01:07:00,799 --> 01:07:03,280
экземпляры слова в надежде, что эти

1574
01:07:03,280 --> 01:07:04,559
кластеры

1575
01:07:04,559 --> 01:07:07,119
будут кластеризовать словарные токены, надеясь, что

1576
01:07:07,119 --> 01:07:08,720
те кластеры, которые были похожи,

1577
01:07:08,720 --> 01:07:11,599
представляли сенсоры, а затем для

1578
01:07:11,599 --> 01:07:14,160
кластеров словесных токенов мы как бы

1579
01:07:14,160 --> 01:07:15,760
относились к ним, как к отдельным

1580
01:07:15,760 --> 01:07:18,079
словам, и изучая  вектор слова для

1581
01:07:18,079 --> 01:07:20,720
каждого, и вы знаете, что в основном

1582
01:07:20,720 --> 01:07:24,079
это работает, поэтому в зеленом цвете у нас есть два

1583
01:07:24,079 --> 01:07:26,559
смысла для слова банк, и поэтому есть

1584
01:07:26,559 --> 01:07:28,640
одно значение для слова банк, которое

1585
01:07:28,640 --> 01:07:30,880
здесь, где оно близко к таким словам, как

1586
01:07:30,880 --> 01:07:32,799
транзакция банковского финансирования и

1587
01:07:32,799 --> 01:07:34,799
отмывание, а затем у нас есть другое

1588
01:07:34,799 --> 01:07:36,960
смысл слова банк здесь, в

1589
01:07:36,960 --> 01:07:39,039
то время как близко к таким словам, как

1590
01:07:39,039 --> 01:07:41,440
территория границы плато, которая является

1591
01:07:41,440 --> 01:07:44,720
смыслом слова берег реки у берега реки, и

1592
01:07:44,720 --> 01:07:49,039
для слова ягуар, выделенного пурпурным цветом, ну, у

1593
01:07:49,039 --> 01:07:51,920
jq есть несколько датчиков, и поэтому

1594
01:07:51,920 --> 01:07:54,880
у нас есть и они, так что  это чувство

1595
01:07:54,880 --> 01:07:57,440
здесь близко к охотнику, так что

1596
01:07:57,440 --> 01:08:00,160
это чувство крупного зверя,

1597
01:08:00,160 --> 01:08:03,200
ягуара на вершине, здесь проявляется

1598
01:08:03,200 --> 01:08:05,440
рядом с роскошью и кабриолетом.  блин, это

1599
01:08:05,440 --> 01:08:07,599
чувство автомобиля ягуара,

1600
01:08:07,599 --> 01:08:11,839
тогда ягуар здесь рядом со струнной

1601
01:08:11,839 --> 01:08:14,160
клавиатурой и такими словами, так что ягуар

1602
01:08:14,160 --> 01:08:17,839
- это название своего рода клавиатуры, а

1603
01:08:17,839 --> 01:08:20,880
затем этот последний ягуар здесь

1604
01:08:20,880 --> 01:08:23,439
близок к программному обеспечению и Microsoft, а затем,

1605
01:08:23,439 --> 01:08:25,439
если вы  достаточно старый, вы помните,

1606
01:08:25,439 --> 01:08:27,359
что была старая версия mac os,

1607
01:08:27,359 --> 01:08:29,600
поэтому она называлась jaguar um, так что

1608
01:08:29,600 --> 01:08:32,080
это компьютерный смысл, так что в основном это

1609
01:08:32,080 --> 01:08:35,759
работает, и мы можем изучать векторы слов

1610
01:08:35,759 --> 01:08:38,560
для разных датчиков слова, но на

1611
01:08:38,560 --> 01:08:40,880
самом деле это не  в большинстве

1612
01:08:40,880 --> 01:08:44,719
случаев это было сделано на практике,

1613
01:08:44,719 --> 01:08:46,000
и для

1614
01:08:46,000 --> 01:08:48,000
этого есть несколько

1615
01:08:48,000 --> 01:08:51,279
причин, я имею в виду, что одна из них - это просто простота,

1616
01:08:51,279 --> 01:08:54,399
если вы делаете это,

1617
01:08:54,399 --> 01:08:56,479
это довольно сложно, потому что вам в

1618
01:08:56,479 --> 01:08:58,399
первую очередь нужно выучить значения слов, а

1619
01:08:58,399 --> 01:09:00,000
затем начать изучать векторы слов  с

1620
01:09:00,000 --> 01:09:02,399
точки зрения смысла слов,

1621
01:09:02,399 --> 01:09:04,799
но другая причина в том, что хотя эта

1622
01:09:04,799 --> 01:09:08,560
модель наличия датчиков слов

1623
01:09:08,560 --> 01:09:10,799
является традиционной, это то, что вы видите в

1624
01:09:10,799 --> 01:09:13,279
словарях, это обычно то, что

1625
01:09:13,279 --> 01:09:15,759
используется при обработке естественного языка, я

1626
01:09:15,759 --> 01:09:18,479
имею в виду, что это обычно  несовершенный по-своему,

1627
01:09:18,479 --> 01:09:20,399
потому что мы пытаемся взять все

1628
01:09:20,399 --> 01:09:22,799
варианты использования слова щука и как бы разрезать

1629
01:09:22,799 --> 01:09:27,759
их на ключевые различные датчики, где

1630
01:09:29,839 --> 01:09:32,238
разница в некотором роде перекрывается,

1631
01:09:32,238 --> 01:09:34,158
и часто неясно, какие из них

1632
01:09:34,158 --> 01:09:37,040
считать отдельными, например  здесь прямо

1633
01:09:37,040 --> 01:09:39,040
железнодорожная линия и тип дороги, ну

1634
01:09:39,040 --> 01:09:41,120
вроде как это то же самое чувство

1635
01:09:41,120 --> 01:09:42,479
щуки, просто это разные

1636
01:09:42,479 --> 01:09:44,640
виды транспорта, и поэтому вы знаете, что

1637
01:09:44,640 --> 01:09:46,399
это может быть вы знаете тип

1638
01:09:46,399 --> 01:09:48,399
транспортной линии и покрываете их обе,

1639
01:09:48,399 --> 01:09:51,279
так что  всегда очень непонятно,

1640
01:09:51,279 --> 01:09:54,560
как вы разрезаете значение слова на

1641
01:09:54,560 --> 01:09:56,719
разные датчики, и действительно, если вы посмотрите

1642
01:09:56,719 --> 01:09:58,640
на разные словари, все делают

1643
01:09:58,640 --> 01:10:01,520
это по-разному,

1644
01:10:01,520 --> 01:10:03,440
так

1645
01:10:03,440 --> 01:10:06,239
что на самом деле оказывается, что на практике

1646
01:10:06,239 --> 01:10:07,520
вы можете

1647
01:10:07,520 --> 01:10:11,600
неплохо справиться, просто имея один

1648
01:10:11,600 --> 01:10:14,239
вектор слова для каждого типа слова

1649
01:10:14,239 --> 01:10:17,440
и  что произойдет, если вы сделаете это

1650
01:10:17,440 --> 01:10:21,360
хорошо, вы обнаружите

1651
01:10:21,440 --> 01:10:22,239
,

1652
01:10:22,239 --> 01:10:24,000
что

1653
01:10:24,000 --> 01:10:26,719
то, что вы изучаете как вектор

1654
01:10:27,440 --> 01:10:30,400
слов, в модных разговорах называется

1655
01:10:33,760 --> 01:10:36,480
суперпозицией диф векторов слов для

1656
01:10:36,480 --> 01:10:39,360
ди  различные значения слова um, где

1657
01:10:39,360 --> 01:10:42,480
слово суперпозиция означает не больше и не меньше,

1658
01:10:42,480 --> 01:10:45,600
чем взвешенная сумма, поэтому наш вектор,

1659
01:10:45,600 --> 01:10:48,560
который мы выучили для пики, будет

1660
01:10:48,560 --> 01:10:50,560
средневзвешенным векторов, которые вы

1661
01:10:50,560 --> 01:10:52,719
выучили бы для средневекового

1662
01:10:52,719 --> 01:10:55,600
чувства оружия плюс чувство рыбы

1663
01:10:55,600 --> 01:10:57,440
плюс чувство дороги плюс любые другие

1664
01:10:57,440 --> 01:10:59,120
ощущения, которые у вас есть,

1665
01:10:59,120 --> 01:11:01,280
где вес, который дается

1666
01:11:01,280 --> 01:11:03,440
этим различным векторам чувств,

1667
01:11:03,440 --> 01:11:06,159
соответствует частотам

1668
01:11:06,159 --> 01:11:09,280
использования различных датчиков, поэтому мы получаем

1669
01:11:10,400 --> 01:11:14,320
слово um вектор для pike um, являющийся своего

1670
01:11:14,320 --> 01:11:16,560
рода средним  вектор,

1671
01:11:16,560 --> 01:11:18,560
и поэтому, если вы

1672
01:11:21,760 --> 01:11:23,920
говорите хорошо, вы только что сложили несколько

1673
01:11:23,920 --> 01:11:26,320
разных векторов в среднее, вы

1674
01:11:26,320 --> 01:11:29,520
можете подумать, что это бесполезно,

1675
01:11:29,520 --> 01:11:31,679
потому что вы знаете, что потеряли истинное

1676
01:11:31,679 --> 01:11:33,520
значение слова, и вы '  у нас только что есть

1677
01:11:33,520 --> 01:11:36,320
какой-то забавный средний вектор,

1678
01:11:36,320 --> 01:11:39,120
который находится между ними,

1679
01:11:39,120 --> 01:11:40,719
но на самом деле

1680
01:11:40,719 --> 01:11:42,719
оказывается, что если вы используете этот

1681
01:11:42,719 --> 01:11:44,400
средний вектор

1682
01:11:44,400 --> 01:11:48,600
в приложениях, он имеет тенденцию к

1683
01:11:48,600 --> 01:11:52,159
самоопределению, потому что если вы скажете, что

1684
01:11:52,159 --> 01:11:56,480
это слово p  Подобно слову

1685
01:11:56,480 --> 01:11:57,440
рыба,

1686
01:11:57,440 --> 01:12:01,040
часть этого вектора представляет рыбу,

1687
01:12:01,040 --> 01:12:03,920
чувство рыбы щуки, и поэтому в этих

1688
01:12:03,920 --> 01:12:06,239
компонентах он будет похож на

1689
01:12:06,239 --> 01:12:08,960
вектор рыбы, так что да, вы скажете,

1690
01:12:08,960 --> 01:12:10,320
что сущность

1691
01:12:10,320 --> 01:12:12,239
есть существенное

1692
01:12:12,239 --> 01:12:15,920
сходство, тогда как если  в другом

1693
01:12:15,920 --> 01:12:18,880
фрагменте текста, в котором говорится, что вы знаете, что люди

1694
01:12:18,880 --> 01:12:21,440
были нацелены, были вооружены пиками и

1695
01:12:21,440 --> 01:12:23,840
копьями, пиками и столами или любым

1696
01:12:23,840 --> 01:12:26,560
другим средневековым оружием, которое вы хорошо помните, на

1697
01:12:26,560 --> 01:12:28,080
самом деле

1698
01:12:28,080 --> 01:12:30,000
часть этого значения также присутствует в

1699
01:12:30,000 --> 01:12:32,320
векторе пики, и поэтому он скажет  да,

1700
01:12:32,320 --> 01:12:34,239
есть хорошее сходство

1701
01:12:34,239 --> 01:12:35,440
с

1702
01:12:35,440 --> 01:12:37,679
булавой, посохом и такими словами,

1703
01:12:37,679 --> 01:12:41,360
и на самом деле мы можем выяснить, какое

1704
01:12:41,360 --> 01:12:44,159
значение имеет смысл пики,

1705
01:12:44,159 --> 01:12:46,239
просто увидев, какие

1706
01:12:46,239 --> 01:12:48,320
компоненты похожи на другие слова,

1707
01:12:48,320 --> 01:12:51,600
которые используются в том же контексте,

1708
01:12:51,600 --> 01:12:54,080
и действительно есть  гораздо более

1709
01:12:54,080 --> 01:12:56,480
удивительный результат, чем это, и

1710
01:12:56,480 --> 01:12:59,040
это результат, который гм евреи - это санджив

1711
01:12:59,040 --> 01:13:00,239
аврора

1712
01:13:00,239 --> 01:13:02,560
тангума, который сейчас учится на нашем стэнфордском

1713
01:13:02,560 --> 01:13:06,080
факультете и других в 2018 году, и

1714
01:13:06,080 --> 01:13:07,520
это следующий результат, который я не знаю.

1715
01:13:07,520 --> 01:13:11,280
В общем, собираюсь объяснить,

1716
01:13:11,280 --> 01:13:14,560
но если вы думаете, что вектор для щуки

1717
01:13:14,560 --> 01:13:17,920
- это просто сумма векторов

1718
01:13:17,920 --> 01:13:20,719
для разных датчиков,

1719
01:13:22,480 --> 01:13:24,880
вы должны подумать, что просто

1720
01:13:26,320 --> 01:13:29,440
невозможно восстановить смысловые векторы

1721
01:13:29,440 --> 01:13:34,719
из вектора для гм  тип слова, потому что

1722
01:13:34,719 --> 01:13:38,080
обычно, если я говорю, что у меня два числа,

1723
01:13:38,080 --> 01:13:40,800
их сумма равна 17, у вас просто нет

1724
01:13:40,800 --> 01:13:42,719
информации о том, какие мои два числа

1725
01:13:42,719 --> 01:13:45,280
верны, вы не можете решить его, и

1726
01:13:45,280 --> 01:13:47,199
даже хуже, если я скажу вам, что у меня есть  три

1727
01:13:47,199 --> 01:13:50,560
числа, и их сумма составляет 17,

1728
01:13:50,560 --> 01:13:51,440
но

1729
01:13:51,440 --> 01:13:53,440
оказывается, что когда у нас есть эти

1730
01:13:53,440 --> 01:13:56,000
многомерные векторные пространства

1731
01:13:56,000 --> 01:13:56,880
,

1732
01:13:56,880 --> 01:13:59,280
в этих многомерных векторных пространствах вещи настолько редки,

1733
01:13:59,280 --> 01:14:01,840
что вы можете

1734
01:14:01,840 --> 01:14:05,600
использовать идеи разреженного кодирования для фактического

1735
01:14:05,600 --> 01:14:08,159
разделения различных датчиков при

1736
01:14:08,159 --> 01:14:11,360
условии, что они  относительно распространены,

1737
01:14:11,360 --> 01:14:13,840
поэтому они показывают в своей статье, что вы можете

1738
01:14:13,840 --> 01:14:16,560
начать с вектора, скажем, щука, и

1739
01:14:16,560 --> 01:14:19,440
фактически отделить компоненты этого

1740
01:14:19,440 --> 01:14:21,440
вектора, которые соответствуют различным

1741
01:14:21,440 --> 01:14:23,760
датчикам слова щука, и вот

1742
01:14:23,760 --> 01:14:26,000
пример в бо  В начале этого слайда,

1743
01:14:26,000 --> 01:14:29,520
который представляет собой слово,

1744
01:14:29,520 --> 01:14:31,840
он разделил этот вектор на пять

1745
01:14:31,840 --> 01:14:34,159
различных датчиков, и поэтому в одном

1746
01:14:34,159 --> 01:14:36,880
смысле он близок к словам брюки,

1747
01:14:36,880 --> 01:14:38,800
блузка, жилет, так что это вид

1748
01:14:38,800 --> 01:14:42,560
одежды, чувство галстука, другие датчики,

1749
01:14:42,560 --> 01:14:44,480
близкие к проводам, кабелям, проводке,

1750
01:14:44,480 --> 01:14:46,880
электричеству.  Так что это своего рода

1751
01:14:46,880 --> 01:14:48,719
тайский стиль одежды, используемый в

1752
01:14:48,719 --> 01:14:50,960
электрике, а затем у нас есть своего

1753
01:14:50,960 --> 01:14:53,760
рода уравнитель голов, так что

1754
01:14:53,760 --> 01:14:56,640
это ощущение ничьей в

1755
01:14:59,280 --> 01:15:02,080
спортивной игре.

1756
01:15:02,080 --> 01:15:03,920
вот, наконец, вот этот

