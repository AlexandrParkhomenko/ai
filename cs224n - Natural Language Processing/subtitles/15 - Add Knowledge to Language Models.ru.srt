1
00:00:05,200 --> 00:00:08,080
добро пожаловать на лекцию 15 cs224n, так что

2
00:00:08,080 --> 00:00:09,440
я Меган, я один из городов, участвующих в этом

3
00:00:09,440 --> 00:00:10,960
курсе, и я также докторант,

4
00:00:10,960 --> 00:00:12,480
работающий с Крисом Рэем,

5
00:00:12,480 --> 00:00:13,519
и сегодня я буду говорить об

6
00:00:13,519 --> 00:00:14,639
интеграции знаний и языковых

7
00:00:14,639 --> 00:00:17,639
моделей,

8
00:00:18,640 --> 00:00:20,080
так что немного поскорее  Напоминает, что

9
00:00:20,080 --> 00:00:21,760
основные этапы вашего проекта должны были быть выполнены сегодня, поэтому, надеюсь,

10
00:00:21,760 --> 00:00:23,119
вы уже предоставите их или мы

11
00:00:23,119 --> 00:00:25,119
изменим их в ближайшие пару дней,

12
00:00:25,119 --> 00:00:26,560
и мы постараемся получить отзывы о них

13
00:00:26,560 --> 00:00:28,880
как можно быстрее,

14
00:00:28,880 --> 00:00:30,320
чтобы знать, что нужно знать об изменении

15
00:00:30,320 --> 00:00:32,079
Основания для выставления оценок и

16
00:00:32,079 --> 00:00:33,760
крайний срок отмены курса - эта пятница,

17
00:00:33,760 --> 00:00:35,840
поэтому, если вы хотите внести какие-либо изменения в

18
00:00:35,840 --> 00:00:37,520
свою оценку, обязательно сделайте это к тому времени, и к тому времени

19
00:00:37,520 --> 00:00:39,200
мы вернем вам оценки

20
00:00:39,200 --> 00:00:40,719
за пятое задание,

21
00:00:40,719 --> 00:00:41,760
если это будет полезно в  Принятие

22
00:00:41,760 --> 00:00:44,000
решения,

23
00:00:44,000 --> 00:00:45,600
и, наконец, ваши финальные проекты должны быть выполнены

24
00:00:45,600 --> 00:00:47,120
через две недели, поэтому, надеюсь, они

25
00:00:47,120 --> 00:00:49,120
пройдут гладко,

26
00:00:49,120 --> 00:00:50,640
поэтому темой дня является интеграция

27
00:00:50,640 --> 00:00:52,320
знаний и языковых моделей, которые вы

28
00:00:52,320 --> 00:00:54,160
немного видели об этой идее в задании

29
00:00:54,160 --> 00:00:56,000
5, а также в лекции Колина Раффла

30
00:00:56,000 --> 00:00:58,480
последний урок, поэтому в задании 5 задача

31
00:00:58,480 --> 00:01:00,399
заключалась в том, чтобы обучить модель предсказывать

32
00:01:00,399 --> 00:01:02,879
место рождения человека по его имени,

33
00:01:02,879 --> 00:01:04,400
и вы увидели это, предварительно обучившись на более

34
00:01:04,400 --> 00:01:06,080
крупном наборе данных, и вы действительно можете

35
00:01:06,080 --> 00:01:07,760
лучше справиться с этой задачей, так как вы можете

36
00:01:07,760 --> 00:01:09,680
закодировать некоторые мировые знания в

37
00:01:09,680 --> 00:01:11,360
языковую модель,

38
00:01:11,360 --> 00:01:13,280
а затем на последней лекции Колин Розыгрыш

39
00:01:13,280 --> 00:01:15,360
представил, как на самом деле t5 можно

40
00:01:15,360 --> 00:01:17,360
настроить для задачи ответа на вопрос в закрытой области,

41
00:01:17,360 --> 00:01:20,080
чтобы вы могли задать

42
00:01:20,080 --> 00:01:22,000
t5 вопрос на естественном языке, и он

43
00:01:22,000 --> 00:01:23,759
вернет ответ,

44
00:01:23,759 --> 00:01:24,880
чтобы они  опираться на эти

45
00:01:24,880 --> 00:01:26,400
темы и изучать методы,

46
00:01:26,400 --> 00:01:27,600
которые недавно

47
00:01:27,600 --> 00:01:29,360
разработали исследователи для увеличения объема

48
00:01:29,360 --> 00:01:32,400
знаний о языковых моделях,

49
00:01:32,400 --> 00:01:33,439
поэтому мы собираемся начать с быстрого

50
00:01:33,439 --> 00:01:34,880
обзора языковых моделей, чтобы убедиться,

51
00:01:34,880 --> 00:01:36,720
что мы все на одной странице

52
00:01:36,720 --> 00:01:37,680
затем мы поговорим о том, какие

53
00:01:37,680 --> 00:01:39,360
типы языковых моделей знаний

54
00:01:39,360 --> 00:01:40,799
уже можно кодировать и с чем они могут

55
00:01:40,799 --> 00:01:42,079
столкнуться,

56
00:01:42,079 --> 00:01:43,920
мы также мотивируем, почему исследователи

57
00:01:43,920 --> 00:01:45,520
заинтересованы в увеличении объема

58
00:01:45,520 --> 00:01:47,360
знаний  преимущества в языковых моделях и что

59
00:01:47,360 --> 00:01:49,520
это может сделать для будущих систем

60
00:01:54,479 --> 00:01:56,000
искусственного интеллекта, если у нас есть языковые модели, которые действительно могут надежно вспоминать знания, мы поговорим о трех широких классах

61
00:01:56,000 --> 00:01:57,439
методов, которые исследователи

62
00:01:57,439 --> 00:01:58,719
использовали для добавления знаний в языковые

63
00:01:58,719 --> 00:02:00,799
модели, включая добавление предварительных знаний.

64
00:02:00,799 --> 00:02:03,119
встраивание обученных объектов с использованием внешней памяти

65
00:02:03,119 --> 00:02:05,040
или хранилища значений ключа или даже простого

66
00:02:05,040 --> 00:02:06,960
изменения обучающих данных,

67
00:02:06,960 --> 00:02:08,318
и для каждого из этих методов мы

68
00:02:08,318 --> 00:02:10,399
поговорим по крайней мере об одной недавней работе, в которой

69
00:02:10,399 --> 00:02:12,080
использовалась эта техника, поэтому, надеюсь,

70
00:02:12,080 --> 00:02:13,520
ясно, как на самом деле использовать ее

71
00:02:13,520 --> 00:02:15,599
на практике

72
00:02:15,599 --> 00:02:16,879
а затем, наконец, мы подведем итоги,

73
00:02:16,879 --> 00:02:18,400
поговорив о том, как оценивать

74
00:02:18,400 --> 00:02:19,840
знания в языковых моделях, и о

75
00:02:19,840 --> 00:02:21,200
проблемах, которые возникают при попытке сделать

76
00:02:21,200 --> 00:02:23,840
это,

77
00:02:24,640 --> 00:02:26,480
так что давайте углубимся сразу,

78
00:02:26,480 --> 00:02:27,520
мы начнем с разговора о

79
00:02:27,520 --> 00:02:28,879
стандартных языковых моделях, о которых вы

80
00:02:28,879 --> 00:02:30,080
узнали  это в начале

81
00:02:30,080 --> 00:02:32,000
курса, и задача состоит в том, чтобы предсказать

82
00:02:32,000 --> 00:02:34,000
следующее слово и последовательность текста и

83
00:02:34,000 --> 00:02:36,160
вычислить вероятность последовательности, чтобы

84
00:02:36,160 --> 00:02:37,760
вы могли  Помните пример, когда

85
00:02:37,760 --> 00:02:39,680
студенты открывали свои бланки, и мы

86
00:02:39,680 --> 00:02:42,239
говорили о том, что экзамены могут принести

87
00:02:42,239 --> 00:02:43,599
сюда обе книги,

88
00:02:43,599 --> 00:02:44,879
а задача стандартной языковой

89
00:02:44,879 --> 00:02:46,480
модели состоит в том, чтобы предсказать наиболее вероятное следующее

90
00:02:46,480 --> 00:02:48,879
слово в

91
00:02:48,879 --> 00:02:50,720
последовательности пару лекций назад Джон также

92
00:02:50,720 --> 00:02:52,160
ввел понятие  в моделях массового языка

93
00:02:52,160 --> 00:02:53,840
вместо предсказания следующего

94
00:02:53,840 --> 00:02:55,920
слова в последовательности текста задачи

95
00:02:55,920 --> 00:02:57,840
предсказывают массовый токен,

96
00:02:57,840 --> 00:02:59,360
и это делается с использованием двунаправленного

97
00:02:59,360 --> 00:03:00,800
контекста,

98
00:03:00,800 --> 00:03:02,720
поэтому вы можете вспомнить пример, когда я

99
00:03:02,720 --> 00:03:04,800
замаскировал маску, и цель модели массового языка

100
00:03:04,800 --> 00:03:06,879
состоит в том, чтобы сделать  наиболее вероятный токен

101
00:03:06,879 --> 00:03:09,360
для каждого из замаскированных слов,

102
00:03:09,360 --> 00:03:11,680
поэтому, возможно, я пошел в магазин,

103
00:03:11,680 --> 00:03:12,879
поэтому, хотя в

104
00:03:12,879 --> 00:03:14,480
этих двух типах языковых моделей есть некоторые различия,

105
00:03:14,480 --> 00:03:16,159
предсказываете ли вы следующее слово

106
00:03:16,159 --> 00:03:17,519
или предсказываете ли вы токен massdot,

107
00:03:18,480 --> 00:03:19,840
они похожи  и что они оба могут

108
00:03:19,840 --> 00:03:21,760
быть обучены на больших объемах

109
00:03:21,760 --> 00:03:23,280
немаркированного текста,

110
00:03:23,280 --> 00:03:24,400
и это одна из причин, почему

111
00:03:24,400 --> 00:03:26,080
они получили такое широкое распространение, что

112
00:03:26,080 --> 00:03:30,080
не требуют никаких аннотированных данных от человека.

113
00:03:30,239 --> 00:03:31,599
Итак, вы увидели, что языковые модели

114
00:03:31,599 --> 00:03:33,840
можно использовать для множества задач, от

115
00:03:33,840 --> 00:03:36,239
обобщения до диалога до задач оценки беглости,

116
00:03:37,440 --> 00:03:38,879
которые включают либо создание

117
00:03:38,879 --> 00:03:40,879
текста, либо оценку вероятности

118
00:03:40,879 --> 00:03:42,959
текста,

119
00:03:42,959 --> 00:03:44,159
а недавно мы увидели, что

120
00:03:44,159 --> 00:03:45,920
языковые модели также могут использоваться для

121
00:03:45,920 --> 00:03:47,599
генерировать предварительно измененные представления

122
00:03:47,599 --> 00:03:49,440
текста, которые кодируют некоторое представление о

123
00:03:49,440 --> 00:03:51,200
понимании языка и, как было

124
00:03:51,200 --> 00:03:52,959
показано, широко используются для различных

125
00:03:52,959 --> 00:03:56,000
последующих задач nlp,

126
00:03:56,000 --> 00:03:57,599
и, наконец, сегодня мы собираемся

127
00:03:57,599 --> 00:03:59,360
затронуть идею о том, что если языковые

128
00:03:59,360 --> 00:04:01,360
модели обучаются в большом

129
00:04:01,360 --> 00:04:03,519
количестве  Можно ли их даже использовать в качестве

130
00:04:03,519 --> 00:04:06,239
базы знаний,

131
00:04:06,879 --> 00:04:08,319
поэтому мы собираемся начать с рассмотрения

132
00:04:08,319 --> 00:04:09,680
того, какие типы фактических знаний

133
00:04:09,680 --> 00:04:11,680
уже могут быть известны языковой модели,

134
00:04:11,680 --> 00:04:13,040
и эти примеры взяты из

135
00:04:13,040 --> 00:04:14,959
статьи Petroni вообще

136
00:04:14,959 --> 00:04:17,358
в emlp пару лет назад и  цель

137
00:04:17,358 --> 00:04:19,358
состоит в том, чтобы проверить фактические знания или здравый смысл

138
00:04:19,358 --> 00:04:21,918
в существующих языковых моделях,

139
00:04:21,918 --> 00:04:23,919
таких как Bert Large,

140
00:04:23,919 --> 00:04:25,040
поэтому давайте посмотрим, что Bert Large предсказывает, что такое

141
00:04:26,400 --> 00:04:29,360
ipod touch.  производство Apple london

142
00:04:29,360 --> 00:04:32,160
jazz festival находится в Лондоне.

143
00:04:32,160 --> 00:04:35,199
Дэнни Алвес играет с Сантосом Карлом

144
00:04:35,199 --> 00:04:37,759
III, который раньше общался на немецком языке, а

145
00:04:37,759 --> 00:04:40,400
вороны умеют летать,

146
00:04:40,400 --> 00:04:41,680
так что здесь у нас есть правильные

147
00:04:41,680 --> 00:04:43,440
прогнозы зеленым цветом и неправильные

148
00:04:43,440 --> 00:04:44,479
прогнозы красным цветом,

149
00:04:44,479 --> 00:04:46,080
и если вы что-то знаете о спорте,

150
00:04:46,080 --> 00:04:47,680
вы  может знать, что Дэнни Алвес -

151
00:04:47,680 --> 00:04:50,320
футболист Сантос - футбольная команда,

152
00:04:50,320 --> 00:04:51,440
здесь они надеялись, что она будет

153
00:04:51,440 --> 00:04:53,360
предсказывать Барселону, потому что, по крайней мере,

154
00:04:53,360 --> 00:04:54,800
во время этого набора данных, очевидно, он

155
00:04:54,800 --> 00:04:57,040
играл за Барселону, а Карл III на

156
00:04:57,040 --> 00:04:58,479
самом деле общался на шведском, а

157
00:04:58,479 --> 00:05:00,800
не на немецком языке,

158
00:05:00,800 --> 00:05:02,800
поэтому  Что хорошего в этих примерах, так

159
00:05:02,800 --> 00:05:04,080
это то, что предсказания в целом

160
00:05:04,080 --> 00:05:06,000
разумны, если вы не знали основную

161
00:05:06,000 --> 00:05:06,880
истину,

162
00:05:06,880 --> 00:05:08,400
все они имеют смысл, когда вы хотите

163
00:05:08,400 --> 00:05:09,919
создать, когда вы хотите предсказать

164
00:05:09,919 --> 00:05:11,759
язык, вы на самом деле предсказываете

165
00:05:11,759 --> 00:05:13,600
язык,

166
00:05:13,600 --> 00:05:15,199
но, конечно, они  не все фактически

167
00:05:15,199 --> 00:05:17,199
верны,

168
00:05:17,199 --> 00:05:19,039
так почему это может происходить

169
00:05:19,039 --> 00:05:21,039
хорошо? Для одного факт, возможно, не был

170
00:05:21,039 --> 00:05:22,720
замечен при обучении, и вы не можете ожидать,

171
00:05:22,720 --> 00:05:24,240
что языковая модель будет делать больше, чем

172
00:05:24,240 --> 00:05:25,680
вспоминать  факты, которые он видел во время

173
00:05:25,680 --> 00:05:27,280
обучения, он не может составить фактов

174
00:05:27,280 --> 00:05:29,360
о мире,

175
00:05:29,360 --> 00:05:30,720
например, также возможно, что этот факт

176
00:05:30,720 --> 00:05:32,720
действительно очень редок, поэтому, возможно, языковая

177
00:05:32,720 --> 00:05:34,639
модель видела обучение факторингу,

178
00:05:34,639 --> 00:05:36,080
но не видела достаточно раз, чтобы фактически

179
00:05:36,080 --> 00:05:38,720
запомнить  факт,

180
00:05:38,720 --> 00:05:40,000
и последняя проблема немного более

181
00:05:40,000 --> 00:05:40,960
тонкая,

182
00:05:40,960 --> 00:05:42,160
модель может быть очень

183
00:05:42,160 --> 00:05:44,160
чувствительной к формулировке заполнения

184
00:05:44,160 --> 00:05:45,680
в пустом заявлении,

185
00:05:45,680 --> 00:05:47,120
и поэтому, например, у вас могут быть

186
00:05:47,120 --> 00:05:49,440
такие утверждения, как x был создан пустым,

187
00:05:49,440 --> 00:05:51,360
что модель не может правильно предсказать

188
00:05:51,360 --> 00:05:53,280
но если вы измените его на x,

189
00:05:53,280 --> 00:05:54,960
он внезапно стал пустым, он сможет правильно его предсказать,

190
00:05:54,960 --> 00:05:56,800
и мы вернемся к этому,

191
00:05:56,800 --> 00:05:59,039
чтобы на самом деле оценивать знания

192
00:05:59,039 --> 00:06:02,000
в этих языковых моделях,

193
00:06:02,319 --> 00:06:04,400
поэтому эта неспособность надежно вспомнить

194
00:06:04,400 --> 00:06:06,400
знания является ключевой проблемой, с которой сталкиваются

195
00:06:06,400 --> 00:06:07,919
языковые модели.  сегодня это будет в

196
00:06:07,919 --> 00:06:09,600
центре внимания этого выступления,

197
00:06:09,600 --> 00:06:10,960
недавние работы показали, что языковые

198
00:06:10,960 --> 00:06:13,039
модели могут восстановить некоторые знания,

199
00:06:13,039 --> 00:06:15,039
включая работу, которую Колин представил на

200
00:06:15,039 --> 00:06:16,880
последнем занятии, у них были очень обнадеживающие

201
00:06:16,880 --> 00:06:18,240
результаты

202
00:06:18,240 --> 00:06:20,000
но есть еще путь, как мы видели,

203
00:06:20,000 --> 00:06:21,440
с заполнением пустых утверждений

204
00:06:21,440 --> 00:06:22,639
и с этими проблемами, которые мы только что

205
00:06:22,639 --> 00:06:24,560
обсудили выше,

206
00:06:24,560 --> 00:06:26,400
поэтому в результате за последние пару лет

207
00:06:26,400 --> 00:06:28,479
в этой области исследований был достигнут огромный прогресс

208
00:06:28,479 --> 00:06:30,400
с точки зрения попыток

209
00:06:30,400 --> 00:06:32,240
выяснить, как вы на самом деле кодируете

210
00:06:32,240 --> 00:06:35,840
больше знаний в языковых моделях,

211
00:06:37,440 --> 00:06:38,880
поэтому я также хочу мотивировать, почему

212
00:06:38,880 --> 00:06:40,639
исследователи заинтересованы в создании

213
00:06:40,639 --> 00:06:42,880
языковых моделей, которые могут более надежно

214
00:06:42,880 --> 00:06:44,720
или вызывать знания,

215
00:06:44,720 --> 00:06:46,400
и одна из этих причин заключается в том, что

216
00:06:46,400 --> 00:06:48,240
предварительно обученные представления используются

217
00:06:48,240 --> 00:06:50,240
во множестве  нижестоящих задач и некоторые

218
00:06:50,240 --> 00:06:51,680
из этих нижестоящих тестов требуют большого объема знаний,

219
00:06:53,599 --> 00:06:55,199
поэтому, например, у вас может быть

220
00:06:55,199 --> 00:06:57,440
нижележащая задача для извлечения отношений

221
00:06:57,440 --> 00:06:59,680
между двумя объектами в предложении,

222
00:06:59,680 --> 00:07:01,120
и это обычно известно как извлечение отношений,

223
00:07:01,120 --> 00:07:03,360
и это намного проще, если у

224
00:07:03,360 --> 00:07:05,840
вас есть некоторые знания о  сущности,

225
00:07:05,840 --> 00:07:07,919
которые потенциально могут быть предоставлены

226
00:07:07,919 --> 00:07:09,199
этим предварительно обученным представлением языковой модели,

227
00:07:11,599 --> 00:07:13,280
и мы говорим об оценке, о которой мы

228
00:07:13,280 --> 00:07:14,880
поговорим  какие типы задач с наибольшей

229
00:07:14,880 --> 00:07:16,880
вероятностью выиграют от этих

230
00:07:16,880 --> 00:07:20,479
богатых знаниями предварительно обученных представлений,

231
00:07:20,560 --> 00:07:22,160
а затем в качестве сложной цели некоторые

232
00:07:22,160 --> 00:07:23,599
исследователи начинают предлагать

233
00:07:23,599 --> 00:07:24,479
идею о том,

234
00:07:24,479 --> 00:07:26,319
что можно в

235
00:07:26,319 --> 00:07:27,840
конечном итоге использовать языковые модели для замены

236
00:07:27,840 --> 00:07:30,720
традиционных баз знаний, чтобы

237
00:07:30,720 --> 00:07:32,240
вместо создания знаний  база для факта,

238
00:07:32,240 --> 00:07:34,080
как вы могли бы прямо сейчас с sql, вы

239
00:07:34,080 --> 00:07:35,440
бы создали языковую модель с подсказкой на

240
00:07:35,440 --> 00:07:38,080
естественном языке,

241
00:07:38,080 --> 00:07:39,520
и, конечно, это требует, чтобы

242
00:07:39,520 --> 00:07:41,360
языковая модель имела высокое качество в

243
00:07:41,360 --> 00:07:43,520
соответствии с фактами вызова, поэтому нас, возможно, еще не было,

244
00:07:44,400 --> 00:07:45,919
но это интересное направление

245
00:07:45,919 --> 00:07:48,800
чтобы мы двигались,

246
00:07:49,039 --> 00:07:50,400
поэтому я хочу прояснить, что я

247
00:07:50,400 --> 00:07:52,000
имею в виду под базой знаний,

248
00:07:52,000 --> 00:07:53,039
здесь мы просто говорим о

249
00:07:53,039 --> 00:07:55,120
графе знаний, где узлы в

250
00:07:55,120 --> 00:07:57,120
графе будут объектами,

251
00:07:57,120 --> 00:07:58,879
а края будут отношениями

252
00:07:58,879 --> 00:08:01,120
между  сущности, так что, например, здесь

253
00:08:01,120 --> 00:08:03,039
у нас есть подмножество графа знаний

254
00:08:03,039 --> 00:08:05,280
для Франклина д-рузвельта, и вы видите

255
00:08:05,280 --> 00:08:07,280
информацию о его супруге, его

256
00:08:07,280 --> 00:08:10,240
месте рождения  его дата рождения и так

257
00:08:10,240 --> 00:08:11,919
далее, важно отметить, что это

258
00:08:11,919 --> 00:08:13,919
структурированный способ хранения знаний,

259
00:08:13,919 --> 00:08:16,160
поскольку он просто в форме графика,

260
00:08:16,160 --> 00:08:17,440
и вы можете описать эти

261
00:08:17,440 --> 00:08:19,360
графики с помощью троек графов знаний,

262
00:08:19,360 --> 00:08:21,120
которые будут важным словарным

263
00:08:21,120 --> 00:08:23,039
словом на протяжении всего выступления, поэтому

264
00:08:23,039 --> 00:08:26,000
Тройка графа знаний будет состоять из

265
00:08:26,000 --> 00:08:28,639
субъектной сущности, отношения и затем

266
00:08:28,639 --> 00:08:30,960
сущности объекта, поэтому, например, здесь у нас

267
00:08:30,960 --> 00:08:33,360
может быть дата

268
00:08:33,360 --> 00:08:36,000
рождения Франклина Друзвельта 30 января 1882 года, и это будет

269
00:08:36,000 --> 00:08:38,000
формировать тройку графа знаний, которую

270
00:08:38,000 --> 00:08:39,760
мы также будем называть  родительская

271
00:08:39,760 --> 00:08:43,519
сущность отношение и хвостовая сущность,

272
00:08:43,519 --> 00:08:45,120
поэтому викиданные - одна очень популярная

273
00:08:45,120 --> 00:08:46,399
база знаний, с которой вы можете столкнуться, если

274
00:08:46,399 --> 00:08:47,839
вы работаете в этой области,

275
00:08:47,839 --> 00:08:49,120
это бесплатная база знаний, которая на

276
00:08:49,120 --> 00:08:50,880
самом деле заполнена людьми, поэтому они

277
00:08:50,880 --> 00:08:52,000
заполняют

278
00:08:52,000 --> 00:08:54,240
эти отношения и сущности, и это

279
00:08:54,240 --> 00:08:56,800
также многоязычный,

280
00:08:56,800 --> 00:08:58,160
поэтому, если вам нужна информация из этой

281
00:08:58,160 --> 00:09:00,240
базы знаний, вы пишете, как

282
00:09:00,240 --> 00:09:02,000
если бы вы писали запрос sql,

283
00:09:02,000 --> 00:09:04,480
это упрощенный, но идея в том, что

284
00:09:04,480 --> 00:09:06,080
вы  я бы хотел

285
00:09:06,080 --> 00:09:08,080
выяснить дату рождения Франклина Рузвельта,

286
00:09:08,080 --> 00:09:11,680
поэтому вы должны написать такой запрос, как

287
00:09:12,000 --> 00:09:13,279
сейчас, если вместо этого вы хотите запросить

288
00:09:13,279 --> 00:09:14,959
языковую модель в качестве базы знаний, у

289
00:09:14,959 --> 00:09:16,880
вас будет что-то вроде этой диаграммы,

290
00:09:16,880 --> 00:09:18,320
которую вы на самом деле, вероятно, видели  в

291
00:09:18,320 --> 00:09:20,240
нескольких лекциях,

292
00:09:20,240 --> 00:09:22,399
и идея состоит в том, что вы обучите языковую

293
00:09:22,399 --> 00:09:24,720
модель на этом неструктурированном тексте,

294
00:09:24,720 --> 00:09:26,560
а затем вы будете использовать языковую модель, чтобы

295
00:09:26,560 --> 00:09:28,240
просто ответить на эти операторы запроса на естественном языке,

296
00:09:29,760 --> 00:09:32,720
так что вот эта работа над t5, где

297
00:09:32,720 --> 00:09:34,720
они обучают t5  над

298
00:09:34,720 --> 00:09:36,480
естественным языком или просто неструктурированным

299
00:09:36,480 --> 00:09:38,720
текстом с задачей искажения диапазона,

300
00:09:38,720 --> 00:09:40,720
а затем они спрашивают t5, когда

301
00:09:40,720 --> 00:09:42,560
родился Франклин Рузвельт,

302
00:09:42,560 --> 00:09:44,240
и идея состоит в том, что t5 выдаст

303
00:09:44,240 --> 00:09:46,160
текстовый ответ,

304
00:09:46,160 --> 00:09:48,080
так что вы можете увидеть этот контраст очень сильно

305
00:09:48,080 --> 00:09:49,360
со старым подходом использования

306
00:09:49,360 --> 00:09:50,959
традиционная база

307
00:09:50,959 --> 00:09:52,399
знаний, в которой структурирована база знаний, и у вас

308
00:09:52,399 --> 00:09:56,000
есть операторы sql для запроса,

309
00:09:57,839 --> 00:09:59,279
так каковы преимущества использования

310
00:09:59,279 --> 00:10:00,480
языковых моделей по сравнению с традиционными

311
00:10:00,480 --> 00:10:02,160
базами знаний и почему люди

312
00:10:02,160 --> 00:10:04,480
могут  чернила, это может быть хорошей идеей,

313
00:10:04,480 --> 00:10:06,160
поскольку языковые модели

314
00:10:06,160 --> 00:10:07,600
предварительно обучены на больших объемах

315
00:10:07,600 --> 00:10:10,240
неструктурированного и немаркированного текста,

316
00:10:10,240 --> 00:10:11,600
тогда как традиционные базы знаний

317
00:10:11,600 --> 00:10:13,440
требуют ручных аннотаций, как в случае с

318
00:10:13,440 --> 00:10:16,000
людьми, которые фактически заполняют их викиданными,

319
00:10:16,000 --> 00:10:18,720
или сложных конвейеров nlp для извлечения из

320
00:10:18,720 --> 00:10:21,120
неструктурированного текста в  структурированная форма,

321
00:10:21,120 --> 00:10:24,320
которая формирует базу знаний,

322
00:10:24,320 --> 00:10:25,839
языковые модели могут также поддерживать более

323
00:10:25,839 --> 00:10:28,480
гибкие запросы на естественном языке,

324
00:10:28,480 --> 00:10:30,480
поэтому, если мы возьмем пример, что означает

325
00:10:30,480 --> 00:10:34,079
окончательное f в песне ufo f означает

326
00:10:34,079 --> 00:10:35,760
базу знаний, вероятно, не будет

327
00:10:35,760 --> 00:10:37,839
поля для окончательного f, поэтому  он не сможет

328
00:10:37,839 --> 00:10:39,279
ответить на ваш запрос,

329
00:10:39,279 --> 00:10:40,959
но есть шанс, что языковая

330
00:10:40,959 --> 00:10:42,800
модель действительно сможет изучить и получить

331
00:10:42,800 --> 00:10:46,160
ответ на этот запрос естественного языка, у

332
00:10:46,160 --> 00:10:47,920
них также был менее экстремальный пример в

333
00:10:47,920 --> 00:10:50,160
этой статье Петрони и других, где,

334
00:10:50,160 --> 00:10:52,160
возможно, ваше отношение было бы  is

335
00:10:52,160 --> 00:10:54,240
работает в вашей базе знаний, а затем вы

336
00:10:54,240 --> 00:10:56,399
запрашиваете, что работает,

337
00:10:56,399 --> 00:10:58,000
а база знаний не имеет

338
00:10:58,000 --> 00:10:59,760
точного соответствия в этой области, поэтому

339
00:10:59,760 --> 00:11:01,600
возвращает пустой ответ,

340
00:11:01,600 --> 00:11:02,640
и

341
00:11:02,640 --> 00:11:04,000
разумно полагать, что ваша

342
00:11:04,000 --> 00:11:05,920
языковая модель может определить, что

343
00:11:05,920 --> 00:11:08,160
эти отношения похожи, поэтому, если я

344
00:11:08,160 --> 00:11:09,680
знаю ответ на один из них, я, вероятно,

345
00:11:09,680 --> 00:11:12,720
знаю ответ на другой,

346
00:11:12,959 --> 00:11:14,880
конечно, это не все

347
00:11:14,880 --> 00:11:16,480
преимущества, также есть много  открытые проблемы с

348
00:11:16,480 --> 00:11:19,200
использованием языковых моделей в качестве баз знаний,

349
00:11:19,200 --> 00:11:21,519
поэтому, для одного, труднее интерпретировать,

350
00:11:21,519 --> 00:11:23,040
когда традиционная база знаний дает

351
00:11:23,040 --> 00:11:24,480
ответ, на самом деле существует информация о происхождении,

352
00:11:24,480 --> 00:11:26,560
связанная с тем, почему она

353
00:11:26,560 --> 00:11:28,959
вернула этот конкретный запрос,

354
00:11:28,959 --> 00:11:30,880
но с языковой моделью действительно

355
00:11:30,880 --> 00:11:32,240
не ясно,

356
00:11:32,240 --> 00:11:34,240
почему она может давать  предсказание,

357
00:11:34,240 --> 00:11:35,279
знание которого просто закодировано в

358
00:11:35,279 --> 00:11:38,079
параметрах модели,

359
00:11:38,079 --> 00:11:40,079
которому также труднее доверять, поэтому вы видели

360
00:11:40,079 --> 00:11:42,160
это в задании 5, где языковая

361
00:11:42,160 --> 00:11:43,920
модель может давать реалистичные

362
00:11:43,920 --> 00:11:46,399
прогнозы, но они неверны, поэтому

363
00:11:46,399 --> 00:11:48,160
нелегко узнать, когда языковая

364
00:11:48,160 --> 00:11:50,240
модель действительно знает  факт против того, что

365
00:11:50,240 --> 00:11:52,079
он использует некоторые подобные предубеждения, чтобы сделать свой

366
00:11:52,079 --> 00:11:53,680
прогноз,

367
00:11:53,680 --> 00:11:54,720
и в случае традиционного

368
00:11:54,720 --> 00:11:56,560
база знаний, если она не знает факта,

369
00:11:56,560 --> 00:11:57,600
она просто получит пустой

370
00:11:57,600 --> 00:11:59,920
ответ,

371
00:11:59,920 --> 00:12:02,160
и, наконец, базы знаний или

372
00:12:02,160 --> 00:12:05,040
языковые модели сложнее изменить,

373
00:12:05,040 --> 00:12:06,720
поэтому в базе знаний, если вы хотите

374
00:12:06,720 --> 00:12:08,800
обновить факт, вы просто измените факт

375
00:12:08,800 --> 00:12:11,680
прямо в  структурированные данные,

376
00:12:11,680 --> 00:12:13,120
но в языковой модели не совсем

377
00:12:13,120 --> 00:12:15,040
понятно, как вы это сделаете, вы можете

378
00:12:15,040 --> 00:12:16,639
дольше настраивать модель на

379
00:12:16,639 --> 00:12:19,200
обновленных данных, но как узнать, есть ли в

380
00:12:19,200 --> 00:12:21,519
ней какие-то запоминания старого

381
00:12:21,519 --> 00:12:23,200
факта,

382
00:12:23,200 --> 00:12:24,880
поэтому есть много открытых  бросает вызов

383
00:12:24,880 --> 00:12:26,959
этой цели фактического использования языковых

384
00:12:26,959 --> 00:12:29,120
моделей в качестве традиционных баз знаний,

385
00:12:29,120 --> 00:12:30,880
но, надеюсь, вы понимаете, почему некоторые люди

386
00:12:30,880 --> 00:12:32,880
думают, что это действительно может быть хорошей идеей,

387
00:12:32,880 --> 00:12:35,920
и почему исследователи заинтересованы в

388
00:12:35,920 --> 00:12:37,040
обучении языковых моделей, которые могут

389
00:12:37,040 --> 00:12:40,480
фактически интегрировать больше знаний,

390
00:12:41,279 --> 00:12:42,959
что подводит нас ко второму разделу  в

391
00:12:42,959 --> 00:12:44,720
разговоре, поэтому я хочу остановиться здесь на всякий

392
00:12:44,720 --> 00:12:48,440
случай, если есть какие-то вопросы,

393
00:12:52,079 --> 00:12:53,279
хорошо,

394
00:12:53,279 --> 00:12:55,279
я думаю, что все в порядке, да,

395
00:12:55,279 --> 00:12:57,200
хорошо, здорово, так что теперь мы

396
00:12:57,200 --> 00:12:58,399
поговорим о том, какие методы

397
00:12:58,399 --> 00:13:00,320
исследования  Chers используют, чтобы на самом деле добавить

398
00:13:00,320 --> 00:13:03,600
больше знаний к языковым моделям,

399
00:13:03,600 --> 00:13:04,959
поэтому мы собираемся поговорить о трех широких

400
00:13:04,959 --> 00:13:07,040
классах техник, это никоим

401
00:13:07,040 --> 00:13:09,040
образом не является исчерпывающим, но, надеюсь, это дает

402
00:13:09,040 --> 00:13:10,560
вам хороший обзор, так что если вы

403
00:13:10,560 --> 00:13:12,000
хотите погрузиться глубже,

404
00:13:12,000 --> 00:13:13,360
вы можете,

405
00:13:13,360 --> 00:13:15,120
поэтому мы  Начнем с разговора о добавлении

406
00:13:15,120 --> 00:13:17,120
предварительно обученных встраиваний сущностей, и для

407
00:13:17,120 --> 00:13:18,959
каждого раздела мы сосредоточимся на

408
00:13:18,959 --> 00:13:20,800
первой работе, которую вы видите в маркерах,

409
00:13:20,800 --> 00:13:22,480
но мы также кратко поговорим о

410
00:13:22,480 --> 00:13:24,800
некоторых вариантах, чтобы вы увидели,

411
00:13:24,800 --> 00:13:26,880
как они работают.  внутри каждого класса может

412
00:13:26,880 --> 00:13:30,399
отличаться и то, какие ручки вы можете повернуть,

413
00:13:31,440 --> 00:13:33,200
поэтому для добавления предварительно обученных встраиваний нам

414
00:13:33,200 --> 00:13:34,639
сначала нужно выяснить, какие

415
00:13:34,639 --> 00:13:36,480
предварительно обученные вложения на самом деле были

416
00:13:36,480 --> 00:13:38,000
бы наиболее полезными для добавления знаний в

417
00:13:38,000 --> 00:13:39,519
языковые модели,

418
00:13:39,519 --> 00:13:41,360
и это может начаться с наблюдения,

419
00:13:41,360 --> 00:13:42,880
но фактов  о мире обычно выражаются в

420
00:13:42,880 --> 00:13:45,040
терминах сущностей,

421
00:13:45,040 --> 00:13:46,800
поэтому, если у нас есть такой факт, как вашингтон

422
00:13:46,800 --> 00:13:48,959
был первым президентом Соединенных Штатов,

423
00:13:48,959 --> 00:13:50,959
у нас есть сущности вашингтон, соединенные

424
00:13:50,959 --> 00:13:52,800
штаты,

425
00:13:52,800 --> 00:13:54,320
но предварительно обученные вложения слов не

426
00:13:54,320 --> 00:13:56,800
имеют  это понятие сущностей,

427
00:13:56,800 --> 00:13:58,160
поэтому у нас будут разные вложения слов

428
00:13:58,160 --> 00:14:00,880
для сша, соединенных штатов америки и

429
00:14:00,880 --> 00:14:02,800
америки, даже если все они относятся к

430
00:14:02,800 --> 00:14:04,880
одному и тому же объекту,

431
00:14:04,880 --> 00:14:06,079
и это усложняет для

432
00:14:06,079 --> 00:14:08,000
языковой модели фактическое изучение любых

433
00:14:08,000 --> 00:14:10,160
представлений над этими объектами,

434
00:14:10,160 --> 00:14:12,240
поскольку на них можно ссылаться  по-разному

435
00:14:12,240 --> 00:14:14,800
в тексте,

436
00:14:15,440 --> 00:14:17,680
так что, если вместо этого у нас будет одно

437
00:14:17,680 --> 00:14:19,440
встраивание для каждой сущности, и мы будем

438
00:14:19,440 --> 00:14:22,320
называть их встраиваниями сущностей,

439
00:14:22,320 --> 00:14:23,680
чтобы теперь у вас было встраивание одной сущности

440
00:14:23,680 --> 00:14:26,160
для США, Соединенных

441
00:14:26,160 --> 00:14:28,560
Штатов Америки и Америки,

442
00:14:28,560 --> 00:14:30,160
и всякий раз, когда вы видите  фразу в тексте,

443
00:14:30,160 --> 00:14:32,240
относящуюся к этой сущности,

444
00:14:32,240 --> 00:14:35,440
вы должны использовать то же встраивание сущностей,

445
00:14:35,440 --> 00:14:37,040
и эти вложения сущностей на самом деле могут

446
00:14:37,040 --> 00:14:39,440
быть предварительно обучены для кодирования этих фактических

447
00:14:39,440 --> 00:14:41,040
знаний о мире,

448
00:14:41,040 --> 00:14:42,560
и эти первоклассные методы, которые мы

449
00:14:42,560 --> 00:14:44,240
рассмотрим, будут состоять в том, как вы на самом деле

450
00:14:44,240 --> 00:14:45,600
лучше всего  используйте эти предварительно обученные

451
00:14:45,600 --> 00:14:49,360
вложения сущностей в языковую модель,

452
00:14:50,160 --> 00:14:52,000
поэтому мне нужно быстро отметить, что

453
00:14:52,000 --> 00:14:53,920
эти вложения сущностей полезны только

454
00:14:53,920 --> 00:14:56,000
для языковых моделей, хотя, если вы  n выполнить

455
00:14:56,000 --> 00:14:58,399
еще одну задачу nlp, называемую "линковкой сущностей",

456
00:15:00,320 --> 00:15:02,079
поэтому я собираюсь отвлечься и

457
00:15:02,079 --> 00:15:05,040
объяснить, что такое связывание сущностей,

458
00:15:05,040 --> 00:15:06,880
поэтому определение связывания сущностей состоит в том,

459
00:15:06,880 --> 00:15:09,040
чтобы связывать упоминания в тексте с сущностями в

460
00:15:09,040 --> 00:15:10,959
базе знаний,

461
00:15:10,959 --> 00:15:12,320
мне нравится думать об этом в  с точки зрения того,

462
00:15:12,320 --> 00:15:14,000
как вы используете встраивание слов,

463
00:15:14,000 --> 00:15:15,440
поэтому, если вы хотите использовать встраивание слов

464
00:15:15,440 --> 00:15:16,880
и у вас есть предложение, вы собираетесь

465
00:15:16,880 --> 00:15:19,120
сначала токенизировать это предложение в слова,

466
00:15:19,120 --> 00:15:20,720
а затем для каждого слова вы будете искать

467
00:15:20,720 --> 00:15:22,560
их соответствующий идентификатор и некоторую

468
00:15:22,560 --> 00:15:24,160
матрицу встраивания слов и  теперь у вас есть

469
00:15:24,160 --> 00:15:24,839
слово, которое

470
00:15:24,839 --> 00:15:27,199
хорошо встраивается в сущность, и, держу пари,

471
00:15:27,199 --> 00:15:29,759
поиск по словарю не так прост, у

472
00:15:29,759 --> 00:15:31,920
вас могут быть предложения, например, вашингтон

473
00:15:31,920 --> 00:15:34,160
- первый президент США

474
00:15:34,160 --> 00:15:35,519
хорошо, у Вашингтона есть два разных

475
00:15:35,519 --> 00:15:37,440
кандидата, мы говорим о Джорджем

476
00:15:37,440 --> 00:15:38,959
Вашингтоне или мы говорим о

477
00:15:38,959 --> 00:15:40,959
штате Вашингтон  и это разные

478
00:15:40,959 --> 00:15:42,480
сущности, которые имеют разные вложения сущностей,

479
00:15:44,000 --> 00:15:46,079
и идентификаторы очередей здесь будут просто

480
00:15:46,079 --> 00:15:49,120
их идентификаторами и данными вики,

481
00:15:49,120 --> 00:15:50,639
а затем в Соединенных Штатах просто есть  единая

482
00:15:50,639 --> 00:15:52,240
сущность,

483
00:15:52,240 --> 00:15:54,079
поэтому задача связывания сущностей состоит в том, чтобы

484
00:15:54,079 --> 00:15:56,560
правильно определить эти неоднозначные упоминания, на

485
00:15:56,560 --> 00:15:58,240
какие сущности они фактически ссылаются в

486
00:15:58,240 --> 00:16:00,079
базе знаний,

487
00:16:00,079 --> 00:16:01,360
и есть много разных способов, которыми вы можете

488
00:16:01,360 --> 00:16:04,000
связать эту сущность, поэтому один из способов, которым вы

489
00:16:04,000 --> 00:16:05,360
могли бы это сделать, - это вычислить

490
00:16:05,360 --> 00:16:06,880
из этого, о, я вижу контекстное слово

491
00:16:06,880 --> 00:16:09,040
президента, поэтому вашингтон, вероятно, ссылается

492
00:16:09,040 --> 00:16:11,600
на Джорджа Вашингтона,

493
00:16:11,600 --> 00:16:13,199
просто еще несколько определений, которые мы собираемся

494
00:16:13,199 --> 00:16:14,880
сослаться на Вашингтон, когда я

495
00:16:14,880 --> 00:16:16,800
упоминал Соединенные Штаты, как я уже упоминал, а

496
00:16:16,800 --> 00:16:18,240
затем вещи, на которые это упоминание может

497
00:16:18,240 --> 00:16:20,480
ссылаться, так что  два варианта

498
00:16:20,480 --> 00:16:23,839
Вашингтона будут кандидатами,

499
00:16:23,839 --> 00:16:25,360
так что это отдельная область исследований,

500
00:16:25,360 --> 00:16:26,639
и я рекомендую вам проверить

501
00:16:26,639 --> 00:16:28,079
ресурсы внизу, если вы

502
00:16:28,079 --> 00:16:29,839
заинтересованы в получении дополнительной информации,

503
00:16:29,839 --> 00:16:31,279
но сейчас самое важное, что

504
00:16:31,279 --> 00:16:33,120
нужно понять, это то, что  Связывание сущностей - это

505
00:16:33,120 --> 00:16:34,720
то, что скажет нам, какие

506
00:16:34,720 --> 00:16:36,079
вложения сущностей действительно имеют отношение к

507
00:16:36,079 --> 00:16:38,240
тексту и какие из них вы хотите использовать

508
00:16:38,240 --> 00:16:41,199
при итерации по последовательности

509
00:16:41,199 --> 00:16:43,279
меган, есть

510
00:16:43,279 --> 00:16:46,320
несколько несколько вопросов здесь один из них

511
00:16:46,320 --> 00:16:49,199
таков, что это связь сущностей, но как

512
00:16:49,199 --> 00:16:51,839
насчет отношений,

513
00:16:52,800 --> 00:16:54,320
да, так что некоторые из работ, о которых мы будем говорить,

514
00:16:54,320 --> 00:16:55,600
будут использовать только

515
00:16:55,600 --> 00:16:58,160
вложения сущностей, так что некоторые из них были

516
00:16:58,160 --> 00:17:00,320
предварительно обучены с информацией о взаимосвязях

517
00:17:00,320 --> 00:17:01,680
но, в конце концов, у вас есть только встраивание сущности,

518
00:17:03,680 --> 00:17:05,199
поэтому извлечение отношений - это еще один

519
00:17:05,199 --> 00:17:07,199
тест nlp, который вы также можете выполнить, но да, здесь

520
00:17:07,199 --> 00:17:09,359
мы просто говорим о связывании NC,

521
00:17:09,359 --> 00:17:11,280
но если у вас есть граф знаний, который вы

522
00:17:11,280 --> 00:17:13,839
показали ранее, у него есть отношения в нем

523
00:17:13,839 --> 00:17:16,000
правильно  вы получаете какую-либо связь между

524
00:17:16,000 --> 00:17:19,199
этим и текстом,

525
00:17:20,240 --> 00:17:21,760
я имею в виду, что цель права извлечения отношения

526
00:17:24,000 --> 00:17:25,599
состоит в том, чтобы выяснить, как с учетом сущностей,

527
00:17:25,599 --> 00:17:26,959
что является отношением между ними,

528
00:17:26,959 --> 00:17:29,360
которое затем сформирует полную тройку из

529
00:17:29,360 --> 00:17:33,840
хвостовой сущности и отношения головной сущности,

530
00:17:36,320 --> 00:17:37,919
ну ладно,

531
00:17:37,919 --> 00:17:39,200
тогда я  думаю, что люди хотят узнать больше

532
00:17:39,200 --> 00:17:40,559
о доме, который он будет использовать, но,

533
00:17:40,559 --> 00:17:41,840
возможно, вам стоит продолжить и показать несколько

534
00:17:41,840 --> 00:17:43,520
примеров,

535
00:17:43,520 --> 00:17:48,960
да, я точно буду, хорошо,

536
00:17:49,600 --> 00:17:51,679
так что встраивание сущностей просто для

537
00:17:51,679 --> 00:17:53,039
обобщения  например, вложения слов, но

538
00:17:53,039 --> 00:17:54,480
они для сущностей в базе знаний,

539
00:17:54,480 --> 00:17:55,919
поэтому у вас будет какой-то связанный с

540
00:17:55,919 --> 00:17:57,840
вектором Джордж вашингтон, и это должно иметь

541
00:17:57,840 --> 00:17:59,440
смысл во

542
00:17:59,440 --> 00:18:00,960
встраиваемом пространстве, так что, возможно,

543
00:18:00,960 --> 00:18:02,480
вектор Джорджа Вашингтона близок к

544
00:18:02,480 --> 00:18:05,440
векторам для других отцов-основателей,

545
00:18:05,440 --> 00:18:06,559
поэтому мы  Мы собираемся кратко поговорить о

546
00:18:06,559 --> 00:18:08,000
некоторых методах для обучающей сущности в

547
00:18:08,000 --> 00:18:09,120
ставках.

548
00:18:09,120 --> 00:18:10,240
Есть методы встраивания графа знаний,

549
00:18:10,240 --> 00:18:11,600
которые вы, возможно, слышали о

550
00:18:11,600 --> 00:18:13,600
временном методе ставок, так что это начинается

551
00:18:13,600 --> 00:18:15,280
с идеи наличия троек графа знаний,

552
00:18:16,480 --> 00:18:18,160
и вы хотите изучить предварительно обученную сущность.

553
00:18:18,160 --> 00:18:20,080
и предварительно обученные вложения отношений, и

554
00:18:20,080 --> 00:18:22,160
вы хотите, чтобы

555
00:18:22,160 --> 00:18:23,840
вложение субъекта и отношение, встраивающее

556
00:18:23,840 --> 00:18:25,120
сумму этих двух,

557
00:18:25,120 --> 00:18:26,799
были близки к встраиванию объекта в

558
00:18:26,799 --> 00:18:28,799
векторное пространство, поэтому это алгоритм для

559
00:18:28,799 --> 00:18:31,200
изучения этого ограничения,

560
00:18:31,200 --> 00:18:32,720
есть также методы сопряжения сущностей слов,

561
00:18:32,720 --> 00:18:34,320
поэтому  они строятся на словах,

562
00:18:34,320 --> 00:18:35,760
одна из них даже называется

563
00:18:35,760 --> 00:18:38,240
wikipedia to back, и идея

564
00:18:38,240 --> 00:18:39,919
дает сущность, которую вы хотите выяснить  какие

565
00:18:39,919 --> 00:18:42,160
слова с наибольшей вероятностью будут встречаться вокруг

566
00:18:42,160 --> 00:18:44,080
него,

567
00:18:44,080 --> 00:18:45,919
а затем последний метод или один из

568
00:18:45,919 --> 00:18:47,919
других распространенных сейчас методов на

569
00:18:47,919 --> 00:18:50,240
самом деле просто использует преобразователь для

570
00:18:50,240 --> 00:18:52,240
изучения представлений сущности путем

571
00:18:52,240 --> 00:18:54,320
кодирования описания сущности, и поэтому

572
00:18:54,320 --> 00:18:56,160
мигание из facebook - это  подход,

573
00:18:56,160 --> 00:18:57,760
который делает это,

574
00:18:57,760 --> 00:18:59,360
поэтому методы, о которых мы поговорим сегодня,

575
00:18:59,360 --> 00:19:01,200
на самом деле не зависят от того, как вы

576
00:19:01,200 --> 00:19:03,039
обучаете встраиванию предварительно обученных сущностей, но я

577
00:19:03,039 --> 00:19:04,240
думаю, что важно знать, что

578
00:19:04,240 --> 00:19:05,840
на самом деле существует широкий спектр

579
00:19:05,840 --> 00:19:07,360
методов для обучения этих встраиваний проповедующих сущностей,

580
00:19:07,360 --> 00:19:10,080
и это  на самом деле неясно,

581
00:19:10,080 --> 00:19:12,080
какой метод лучше всего подходит для их

582
00:19:12,080 --> 00:19:15,360
последующего использования и языковые модели,

583
00:19:16,240 --> 00:19:17,679
поэтому одна из ключевых проблем использования

584
00:19:17,679 --> 00:19:19,039
предварительно обученных встраиваний сущностей в

585
00:19:19,039 --> 00:19:21,280
языковые модели - выяснить, как

586
00:19:21,280 --> 00:19:22,720
их включить, если они из

587
00:19:22,720 --> 00:19:24,000
другого пространства встраивания, чем

588
00:19:24,000 --> 00:19:25,840
языковая модель

589
00:19:25,840 --> 00:19:27,600
и поэтому мы сделаем или подходы, которые мы

590
00:19:27,600 --> 00:19:28,799
рассмотрим сегодня,

591
00:19:28,799 --> 00:19:30,400
мы изучим слой слияния, чтобы объединить

592
00:19:30,400 --> 00:19:32,880
этот контекст и информацию о сущности, чтобы

593
00:19:32,880 --> 00:19:35,280
у нас была сущность embe  ddings, и у нас

594
00:19:35,280 --> 00:19:36,640
есть контекстуализированные вложения слов из

595
00:19:36,640 --> 00:19:39,039
нашей языковой модели,

596
00:19:39,039 --> 00:19:41,280
поэтому, если мы возьмем последовательность текста

597
00:19:41,280 --> 00:19:43,440
и представим, что j указывает на

598
00:19:43,440 --> 00:19:45,679
элемент j в последовательности, то проблема

599
00:19:45,679 --> 00:19:47,280
здесь в том, что мы хотим выяснить, как мы

600
00:19:47,280 --> 00:19:50,000
объединяем некоторые вложения слов wj  с некоторой

601
00:19:50,000 --> 00:19:52,400
выровненной сущностью, встраиваемой, например,

602
00:19:52,400 --> 00:19:54,080
здесь выравнивание может быть таким, как

603
00:19:54,080 --> 00:19:56,080
в примере, где у нас был вашингтон

604
00:19:56,080 --> 00:19:57,520
был первым президентом

605
00:19:57,520 --> 00:19:59,120
вашингтон был бы вашим вложением слова,

606
00:19:59,120 --> 00:20:00,880
а джордж вашингтон был бы

607
00:20:00,880 --> 00:20:03,200
выровненной сущностью, встраиваемой туда,

608
00:20:03,200 --> 00:20:04,799
так что вы можете представить в этом случае,

609
00:20:04,799 --> 00:20:07,679
скажем, ваш  wj - это вашингтон, а ваш ek

610
00:20:07,679 --> 00:20:08,960
- это вложение вашей сущности для Джорджа

611
00:20:08,960 --> 00:20:10,640
Вашингтона, и вы хотите выровнять их

612
00:20:10,640 --> 00:20:12,799
вместе, поэтому вы можете выучить

613
00:20:12,799 --> 00:20:16,320
весовую матрицу wt для текста, а мы

614
00:20:16,320 --> 00:20:18,159
для сущности, чтобы проецировать эти

615
00:20:18,159 --> 00:20:20,559
вложения в то же измерение, прежде

616
00:20:20,559 --> 00:20:22,400
чем суммировать  их и, наконец, взять на себя

617
00:20:22,400 --> 00:20:24,799
функцию активации над ними,

618
00:20:24,799 --> 00:20:27,200
поэтому идея состоит в том, что, имея некоторый

619
00:20:27,200 --> 00:20:29,360
механизм слоя слияния, подобный этому, вы действительно можете

620
00:20:30,320 --> 00:20:32,159
использовать эти объекты emb  eddings и эти

621
00:20:32,159 --> 00:20:34,000
контекстные вложения слов, которые находятся в

622
00:20:34,000 --> 00:20:36,320
разных пространствах встраивания и объединяют их

623
00:20:36,320 --> 00:20:38,320
вместе, чтобы получить единое скрытое

624
00:20:38,320 --> 00:20:40,000
представление

625
00:20:40,000 --> 00:20:43,280
для элемента в последовательности,

626
00:20:43,760 --> 00:20:45,120
поэтому подходы, о которых мы сегодня поговорим,

627
00:20:45,120 --> 00:20:47,200
имеют какой-то механизм,

628
00:20:47,200 --> 00:20:48,559
очень похожий на этот, или некоторые

629
00:20:48,559 --> 00:20:50,640
варианты  это, чтобы сделать эту

630
00:20:50,640 --> 00:20:54,480
комбинацию информации контекста и сущности,

631
00:20:55,520 --> 00:20:56,480
поэтому первый подход, о котором мы собираемся

632
00:20:56,480 --> 00:20:58,720
поговорить, называется расширенным

633
00:20:58,720 --> 00:21:00,480
языковым представлением Эрни с информативными

634
00:21:00,480 --> 00:21:02,400
сущностями, и поэтому он просто основан на том, о чем

635
00:21:02,400 --> 00:21:04,000
мы уже говорили, он использует

636
00:21:04,000 --> 00:21:06,640
предварительно обученную сущность  embeddings, и

637
00:21:06,640 --> 00:21:08,320
он также использует это понятие слоя слияния,

638
00:21:09,280 --> 00:21:11,039
поэтому первый блок в ernie - это текстовый

639
00:21:11,039 --> 00:21:12,080
кодировщик,

640
00:21:12,080 --> 00:21:14,080
который представляет собой многослойный двунаправленный

641
00:21:14,080 --> 00:21:15,919
преобразователь-кодировщик для своих

642
00:21:15,919 --> 00:21:17,919
экспериментов, которые они используют bert, но его не

643
00:21:17,919 --> 00:21:20,080
нужно сжигать,

644
00:21:20,080 --> 00:21:21,280
и это  за которым следует кодировщик знаний, у

645
00:21:21,280 --> 00:21:23,120
которого сложены блоки,

646
00:21:23,120 --> 00:21:26,000
состоящие из двух многоголовых вниманий,

647
00:21:26,000 --> 00:21:27,840
один находится над встраиваемыми объектами, а

648
00:21:27,840 --> 00:21:29,600
другой - над вашим токеном o  r вложений подслов,

649
00:21:31,600 --> 00:21:33,039
а затем выходные данные этих

650
00:21:33,039 --> 00:21:34,400
контекстуализированных сущностей и

651
00:21:34,400 --> 00:21:35,760
встраивания токенов из многоголовых

652
00:21:35,760 --> 00:21:36,960
вниманий

653
00:21:36,960 --> 00:21:39,120
передаются на слой слияния,

654
00:21:39,120 --> 00:21:40,480
который очень похож на то, что мы только что

655
00:21:40,480 --> 00:21:41,919
рассмотрели,

656
00:21:41,919 --> 00:21:43,840
но теперь у вас также есть новые вложения слов и

657
00:21:43,840 --> 00:21:45,840
сущностей, которые вы  создание

658
00:21:45,840 --> 00:21:48,080
uh в качестве вывода вашего слоя слияния, поэтому вы

659
00:21:48,080 --> 00:21:49,600
видите этот wj

660
00:21:49,600 --> 00:21:52,240
um и этот ek, которые создаются

661
00:21:52,240 --> 00:21:54,000
как следующий слой встраивания слов и сущностей,

662
00:21:55,679 --> 00:21:58,000
поэтому здесь i указывает, что

663
00:21:58,000 --> 00:21:59,679
это i-й блок в кодировщике знаний,

664
00:21:59,679 --> 00:22:01,440
поэтому у вас действительно будет несколько

665
00:22:01,440 --> 00:22:03,520
стеки этих кодировщиков знаний, и

666
00:22:03,520 --> 00:22:05,600
вы будете выполнять

667
00:22:05,600 --> 00:22:07,600
объединение встраивания сущности слова, создавая вложения новых слов и

668
00:22:07,600 --> 00:22:09,440
сущностей, а затем передавая это

669
00:22:09,440 --> 00:22:10,720
в следующий блок кодировщика знаний,

670
00:22:14,640 --> 00:22:16,159
так что вот как выглядит диаграмма архитектуры

671
00:22:16,159 --> 00:22:18,559
слева, мы  есть

672
00:22:18,559 --> 00:22:20,799
кодировщик t или кодировщик текста,

673
00:22:20,799 --> 00:22:22,320
за которым следует кодировщик k или

674
00:22:22,320 --> 00:22:24,000
кодировщик знаний,

675
00:22:24,000 --> 00:22:25,120
а затем с правой стороны у нас есть

676
00:22:25,120 --> 00:22:26,320
увеличенная версия вашего кодировщика знаний

677
00:22:27,440 --> 00:22:28,799
s  o вы видите многоголовое внимание

678
00:22:28,799 --> 00:22:30,799
над жетонами оранжевого цвета, а затем

679
00:22:30,799 --> 00:22:32,799
над объектами желтым цветом, а затем у вас есть

680
00:22:32,799 --> 00:22:34,320
это выравнивание между словом и

681
00:22:34,320 --> 00:22:35,600
объектами

682
00:22:35,600 --> 00:22:37,280
с пунктирными линиями,

683
00:22:37,280 --> 00:22:39,360
так что у них есть этот пример, как писал Боб Дилан,

684
00:22:39,360 --> 00:22:42,080
дующий на ветру в 1962 году

685
00:22:42,080 --> 00:22:43,919
сущности здесь - это Боб Дилан и

686
00:22:43,919 --> 00:22:45,440
дуют на ветру,

687
00:22:45,440 --> 00:22:47,360
и у них есть простое правило выравнивания, в

688
00:22:47,360 --> 00:22:48,880
котором вы хотите выровнять сущность

689
00:22:48,880 --> 00:22:51,200
по первому слову во фразе сущности,

690
00:22:51,200 --> 00:22:53,360
поэтому вы хотите выровнять Боб Дилан с Бобом,

691
00:22:53,360 --> 00:22:54,720
это то, что пунктирная линия пытается

692
00:22:54,720 --> 00:22:56,799
обозначить  и вы хотите,

693
00:22:56,799 --> 00:22:58,960
чтобы дул ветер,

694
00:22:58,960 --> 00:23:00,559
так что здесь уже предполагается, что

695
00:23:00,559 --> 00:23:02,240
связывание сущностей было выполнено, и вы знаете свои

696
00:23:02,240 --> 00:23:03,840
сущности заранее, чтобы вы могли видеть,

697
00:23:03,840 --> 00:23:05,679
что сущности фактически вводятся в

698
00:23:05,679 --> 00:23:08,000
модель,

699
00:23:08,000 --> 00:23:10,080
поэтому после того, как вы выполнили выравнивание слова nt,

700
00:23:10,080 --> 00:23:11,520
это идет  к слою слияния информации

701
00:23:11,520 --> 00:23:14,320
и к этому светло-пурпурно-серому цвету,

702
00:23:14,320 --> 00:23:15,840
а затем, наконец, он создает эти новые

703
00:23:15,840 --> 00:23:18,480
словесные объекты, вставляемые в качестве выходных данных,

704
00:23:18,480 --> 00:23:20,080
а затем помните, что у вас есть несколько

705
00:23:20,080 --> 00:23:21,520
блоков этих, поэтому  ose будет передан

706
00:23:21,520 --> 00:23:22,799
в следующий блок вашего кодировщика знаний,

707
00:23:26,480 --> 00:23:28,400
так что как вы на самом деле это тренируете, это

708
00:23:28,400 --> 00:23:29,919
очень похоже на то, что у вас есть

709
00:23:29,919 --> 00:23:31,760
потеря модели массового языка, и у вас есть

710
00:23:31,760 --> 00:23:33,679
потеря предсказания следующего предложения,

711
00:23:33,679 --> 00:23:35,039
и они также вводят

712
00:23:35,039 --> 00:23:37,120
задачу предварительной подготовки знаний, которая  они называют

713
00:23:37,120 --> 00:23:38,960
задачу dea,

714
00:23:38,960 --> 00:23:40,720
она названа в честь автоэнкодера сущности

715
00:23:40,720 --> 00:23:43,520
с шумоподавлением из статьи icml

716
00:23:43,520 --> 00:23:45,600
в 2008 году,

717
00:23:45,600 --> 00:23:46,640
и идея состоит в том, что они собираются

718
00:23:46,640 --> 00:23:48,159
случайным образом замаскировать эти выравнивания сущностей токенов,

719
00:23:48,159 --> 00:23:50,799
чтобы идея о том, что Боб переходит к

720
00:23:50,799 --> 00:23:52,720
Бобу Дилану, они собираются замаскировать  это

721
00:23:52,720 --> 00:23:54,480
с некоторым случайным процентом,

722
00:23:54,480 --> 00:23:55,440
а затем они собираются предсказать

723
00:23:55,440 --> 00:23:57,760
соответствующую сущность для токена

724
00:23:57,760 --> 00:24:00,400
из сущностей в этой последовательности,

725
00:24:00,400 --> 00:24:02,240
так что это выглядит следующим образом:

726
00:24:02,240 --> 00:24:04,159
суммирование проводится по m сущностям в

727
00:24:04,159 --> 00:24:06,720
последовательности, поэтому это будет больше, чем bob  Дилан

728
00:24:06,720 --> 00:24:08,480
и дует на ветру в предыдущем

729
00:24:08,480 --> 00:24:09,440
примере,

730
00:24:09,440 --> 00:24:11,279
и, учитывая конкретное слово, они хотят

731
00:24:11,279 --> 00:24:13,520
выяснить, к какой сущности он, скорее

732
00:24:13,520 --> 00:24:15,520
всего, присоединится в этой последовательности, так

733
00:24:15,520 --> 00:24:18,480
же как и Боб выравнивается с Бобом Диланом или d  oes bob

734
00:24:18,480 --> 00:24:20,960
согласны с ветром,

735
00:24:20,960 --> 00:24:22,799
и их мотивация для этого заключается в

736
00:24:22,799 --> 00:24:24,559
том, что если у вас нет этой задачи, все,

737
00:24:24,559 --> 00:24:25,840
что вы когда-либо будете предсказывать, -

738
00:24:25,840 --> 00:24:27,760
это токен с потерей модели массового языка,

739
00:24:28,640 --> 00:24:30,400
и вы действительно должны знать 10 кодов.

740
00:24:30,400 --> 00:24:31,520
Вероятно, они предсказывают

741
00:24:31,520 --> 00:24:32,880
сущности,

742
00:24:32,880 --> 00:24:34,640
поэтому, добавляя эту задачу, у них есть какая-

743
00:24:34,640 --> 00:24:36,240
то задача, которая на самом деле

744
00:24:36,240 --> 00:24:38,159
предсказывает сущность,

745
00:24:38,159 --> 00:24:39,440
и они также предполагают, что это могло бы

746
00:24:39,440 --> 00:24:42,400
лучше объединить знания или сущность

747
00:24:42,400 --> 00:24:44,640
и представления слов, чем просто

748
00:24:44,640 --> 00:24:47,840
использование слоя слияния,

749
00:24:47,919 --> 00:24:50,000
их окончательная потеря  затем

750
00:24:50,000 --> 00:24:52,000
суммирование модели массового языка потеряло следующую

751
00:24:52,000 --> 00:24:54,240
потерю предсказания предложения и потерю этой

752
00:24:54,240 --> 00:24:57,760
задачи предварительной подготовки знаний,

753
00:24:59,679 --> 00:25:01,279
поэтому они показали эксперимент по удалению,

754
00:25:01,279 --> 00:25:02,640
что на самом деле очень важно

755
00:25:02,640 --> 00:25:04,640
иметь эту задачу предварительной подготовки знаний,

756
00:25:04,640 --> 00:25:08,159
поэтому она не отображается слева

757
00:25:08,159 --> 00:25:10,240
-костнейший бар Эрни как второй столбец слева,

758
00:25:10,240 --> 00:25:11,520
и это со всеми функциями

759
00:25:11,520 --> 00:25:13,520
Эрни, а затем они пытаются удалить

760
00:25:13,520 --> 00:25:15,039
предварительно обученные вложения сущностей и

761
00:25:15,039 --> 00:25:16,400
удалить те  это задача предварительной подготовки знаний,

762
00:25:17,840 --> 00:25:19,520
поэтому вы видите, что Берт выполняет худшее,

763
00:25:19,520 --> 00:25:21,039
это не очень удивительно, и что

764
00:25:21,039 --> 00:25:23,039
Эрни работает лучше всего, но что

765
00:25:23,039 --> 00:25:24,400
интересно, если вы удалите

766
00:25:24,400 --> 00:25:26,159
вложения сущностей или удалите

767
00:25:26,159 --> 00:25:28,720
задачу предварительной подготовки, они сделают только немного

768
00:25:28,720 --> 00:25:30,640
лучше, чем bert,

769
00:25:30,640 --> 00:25:33,120
и поэтому действительно необходимо

770
00:25:33,120 --> 00:25:35,279
использовать эту предварительную тренировку, чтобы

771
00:25:35,279 --> 00:25:36,799
максимально использовать вашу предварительно обученную сущность и

772
00:25:36,799 --> 00:25:39,200
ставки,

773
00:25:40,799 --> 00:25:42,320
поэтому некоторые сильные стороны этой работы заключались в том, что

774
00:25:42,320 --> 00:25:43,840
они представили некоторый способ объединить

775
00:25:43,840 --> 00:25:45,760
сущность и контекстную информацию с помощью

776
00:25:45,760 --> 00:25:47,520
этого слияния  слой и эта

777
00:25:47,520 --> 00:25:48,880
задача предварительного обучения знаний,

778
00:25:48,880 --> 00:25:50,080
а затем они также показывают улучшенную

779
00:25:50,080 --> 00:25:52,240
производительность для последующих задач, к которым

780
00:25:52,240 --> 00:25:53,360
мы вернемся, когда будем говорить об

781
00:25:53,360 --> 00:25:55,360
оценке,

782
00:25:55,360 --> 00:25:56,400
но, конечно, есть также некоторые

783
00:25:56,400 --> 00:25:58,000
ограничения,

784
00:25:58,000 --> 00:25:59,919
поэтому ему нужны текстовые данные с объектами,

785
00:25:59,919 --> 00:26:02,080
аннотированными в качестве входных, и это  верно даже

786
00:26:02,080 --> 00:26:03,919
для последующих задач, поэтому, если вы помните

787
00:26:03,919 --> 00:26:05,840
на диаграмме архитектуры, у нас была информация о

788
00:26:05,840 --> 00:26:08,240
сущности сущности, фактически вводимая

789
00:26:08,240 --> 00:26:10,559
в архитектуру, но это не  очень

790
00:26:10,559 --> 00:26:12,080
реалистично, что у вас

791
00:26:12,080 --> 00:26:14,080
обязательно будет хороший компоновщик сущностей для любых

792
00:26:14,080 --> 00:26:15,279
последующих задач, на которых вы хотите использовать

793
00:26:15,279 --> 00:26:17,840
Эрни,

794
00:26:18,000 --> 00:26:19,520
и следующая проблема заключается в том, что это требует

795
00:26:19,520 --> 00:26:21,360
дополнительной предварительной подготовки вашей языковой модели,

796
00:26:21,360 --> 00:26:22,960
поэтому теперь вам не нужно просто притворяться

797
00:26:22,960 --> 00:26:24,640
Burt, но вам также необходимо предварительно обучить

798
00:26:24,640 --> 00:26:27,039
кодировщик знаний наверху

799
00:26:27,039 --> 00:26:28,159
для первой задачи, мы

800
00:26:28,159 --> 00:26:29,760
собираемся поговорить о работе, которая

801
00:26:29,760 --> 00:26:31,520
представляет решение для решения этой

802
00:26:31,520 --> 00:26:32,880
второй задачи, я рекомендую вам

803
00:26:32,880 --> 00:26:35,120
проверить сноску внизу

804
00:26:35,120 --> 00:26:37,600
это вводит работу, которая на

805
00:26:37,600 --> 00:26:38,880
самом деле использует предварительно обученные вложения сущностей,

806
00:26:38,880 --> 00:26:40,559
использует их в языковой модели

807
00:26:40,559 --> 00:26:41,600
и не требует дополнительной

808
00:26:41,600 --> 00:26:44,960
предварительной подготовки, так что это довольно круто,

809
00:26:45,279 --> 00:26:46,880
я думаю, это все, что у меня есть для Эрни,

810
00:26:46,880 --> 00:26:50,760
поэтому я хочу остановиться здесь, чтобы задать

811
00:26:53,360 --> 00:26:56,799
вопросы  вот тот, который находится здесь, поэтому на

812
00:26:56,799 --> 00:26:59,200
уровне слияния он заметил, что

813
00:26:59,200 --> 00:27:00,799
передача объекта, встраиваемого в

814
00:27:00,799 --> 00:27:02,400
слой слияния, для объединения с

815
00:27:02,400 --> 00:27:04,480
внедрением слов, более эффективна, чем просто

816
00:27:04,480 --> 00:27:06,640
конкатенация объекта, встраиваемого

817
00:27:06,640 --> 00:27:08,240
в  конец слова встраивание вопросительного

818
00:27:08,240 --> 00:27:10,640
знака да, поэтому я думаю, что люди все еще

819
00:27:10,640 --> 00:27:12,799
немного сбиты с толку

820
00:27:12,799 --> 00:27:15,600
относительно мотивации этого слоя слияния, и

821
00:27:15,600 --> 00:27:18,080
поэтому я думаю, что это самая простая

822
00:27:18,080 --> 00:27:20,320
стратегия, поскольку у вас есть

823
00:27:20,320 --> 00:27:22,000
объект, связывающий, вы можете просто

824
00:27:22,000 --> 00:27:24,559
объединить  сущность встраивается в

825
00:27:24,559 --> 00:27:26,720
конец встраивания слов и делает это регулярно,

826
00:27:26,720 --> 00:27:29,919
это сработает так же хорошо,

827
00:27:32,880 --> 00:27:35,279
я думаю, идея в том, что это не так, потому что

828
00:27:35,279 --> 00:27:37,120
если вы представите, что, скажем, ваши

829
00:27:37,120 --> 00:27:40,000
величины очень разные,

830
00:27:40,000 --> 00:27:42,320
вам нужен какой-то способ, я думаю, выровнять

831
00:27:42,320 --> 00:27:44,320
пробелы  так что что-либо значимое

832
00:27:44,320 --> 00:27:45,600
в пространстве встраивания сущностей по-прежнему

833
00:27:45,600 --> 00:27:47,120
имеет смысл в пространстве встраивания слова,

834
00:27:47,120 --> 00:27:48,799
поэтому, если вы близки к пространству встраивания слова,

835
00:27:48,799 --> 00:27:50,559
вы также хотели бы быть

836
00:27:50,559 --> 00:27:52,640
рядом в пространстве вложения сущностей, поэтому я

837
00:27:52,640 --> 00:27:55,600
думаю, что это один аргумент,

838
00:27:58,720 --> 00:28:00,960
да

839
00:28:00,960 --> 00:28:03,360
я имею в виду, я имею в виду, я думаю, что вопрос, разве

840
00:28:03,360 --> 00:28:05,279
вы не знаете, что это хороший вопрос, как

841
00:28:05,279 --> 00:28:08,080
говорят люди, я имею в виду, что не совсем очевидно,

842
00:28:08,080 --> 00:28:10,080
что это не сработает, что

843
00:28:10,080 --> 00:28:11,919
кажется, что одна из потенциальных проблем

844
00:28:11,919 --> 00:28:12,640
- это кто-то

845
00:28:12,640 --> 00:28:15,120
У слов есть

846
00:28:15,120 --> 00:28:17,919
ссылки на сущности, а у некоторых слов нет, и

847
00:28:17,919 --> 00:28:19,440
тогда у вас будут нулевые

848
00:28:19,440 --> 00:28:20,880
векторы для тех, у которых нет

849
00:28:20,880 --> 00:28:22,880
ничего хорошего связанного, и

850
00:28:22,880 --> 00:28:24,320
это может

851
00:28:24,320 --> 00:28:27,120
вести себя немного странно, но

852
00:28:27,120 --> 00:28:30,240
да в этом  случай, когда у них нет

853
00:28:30,240 --> 00:28:32,559
связанных сущностей, что является отличным моментом,

854
00:28:32,559 --> 00:28:34,799
да, первое уравнение просто

855
00:28:34,799 --> 00:28:36,640
упрощается до первого члена плюс

856
00:28:36,640 --> 00:28:39,039
смещение, так что есть очевидное решение

857
00:28:39,039 --> 00:28:39,919
в том случае, когда вы не

858
00:28:39,919 --> 00:28:41,360
объединяете, что вы просто не добавляете

859
00:28:41,360 --> 00:28:43,360
о термине эм

860
00:28:43,360 --> 00:28:46,640
да, это может быть одной из причин, тоже

861
00:28:55,440 --> 00:28:59,720
хорошо, есть ли еще вопросы

862
00:28:59,720 --> 00:29:01,360
[Музыка],

863
00:29:01,360 --> 00:29:03,200
я думаю, вы можете продолжать,

864
00:29:03,200 --> 00:29:05,440
хорошо, круто,

865
00:29:05,440 --> 00:29:07,120
ну,

866
00:29:07,120 --> 00:29:11,200
сейчас мы говорим о новерте,

867
00:29:11,360 --> 00:29:13,120
и это от тех же людей, которые

868
00:29:13,120 --> 00:29:14,880
представили работу elmo

869
00:29:14,880 --> 00:29:16,640
и идея здесь в том, что они собираются

870
00:29:17,840 --> 00:29:19,520
предварительно обучить интегрированный компоновщик сущностей в

871
00:29:19,520 --> 00:29:22,559
качестве расширения для Bird,

872
00:29:23,360 --> 00:29:24,720
и поэтому

873
00:29:24,720 --> 00:29:26,159
их функция потерь теперь будет

874
00:29:26,159 --> 00:29:27,520
суммированием следующего предложения,

875
00:29:27,520 --> 00:29:29,120
предсказывающего потерю модели массового языка,

876
00:29:29,120 --> 00:29:30,720
и этой сущности, связывающей потерю, поэтому вместо этого

877
00:29:30,720 --> 00:29:32,960
знания p  переобучая задачу

878
00:29:32,960 --> 00:29:34,640
от Эрни, у нас будет сущность, связывающая

879
00:29:34,640 --> 00:29:35,520
потерю,

880
00:29:35,520 --> 00:29:38,080
и идея компоновщика сущностей заключается в том,

881
00:29:38,080 --> 00:29:39,919
что теперь у вас будет такая же обычная последовательность,

882
00:29:39,919 --> 00:29:42,240
как и ввод, а интегрированный пустой компоновщик

883
00:29:42,240 --> 00:29:44,000
определит, какие сущности

884
00:29:44,000 --> 00:29:46,480
в предложении и  эм или какие

885
00:29:46,480 --> 00:29:48,159
упоминания в предложении, какие

886
00:29:48,159 --> 00:29:49,840
кандидаты на эти упоминания, а затем

887
00:29:49,840 --> 00:29:52,399
каковы должны быть оценки этих сущностей

888
00:29:52,399 --> 00:29:54,480
или кандидатов с учетом

889
00:29:54,480 --> 00:29:56,480
контекста предложения, и поэтому все это сейчас делается

890
00:29:56,480 --> 00:29:57,919
как часть модели, а не

891
00:29:57,919 --> 00:30:00,320
требует этого  в качестве некоторого внешнего этапа конвейера,

892
00:30:00,320 --> 00:30:01,760
прежде чем вы могли даже использовать,

893
00:30:01,760 --> 00:30:03,600
например, ernie,

894
00:30:03,600 --> 00:30:05,039
поэтому теперь для последующих задач вам больше

895
00:30:05,039 --> 00:30:06,720
не нужны эти аннотации сущностей,

896
00:30:06,720 --> 00:30:08,799
ваш интегрированный анти-компоновщик

897
00:30:08,799 --> 00:30:10,799
определит, что такое правильный объект, и

898
00:30:10,799 --> 00:30:14,240
сможет использовать правильное встраивание объекта,

899
00:30:14,240 --> 00:30:15,919
поэтому есть также  эта идея о том, что обучение

900
00:30:15,919 --> 00:30:17,360
- это соединение сущностей, может на самом деле лучше

901
00:30:17,360 --> 00:30:19,039
кодировать знания, чем эта

902
00:30:19,039 --> 00:30:21,120
предварительная тренировочная задача, потому что они показывают, что на

903
00:30:21,120 --> 00:30:22,720
самом деле никто не превосходит Эрни в

904
00:30:22,720 --> 00:30:24,880
последующих задачах.  ks,

905
00:30:24,880 --> 00:30:26,720
поэтому одна из причин, по которой это может произойти, заключается в том, что если

906
00:30:26,720 --> 00:30:29,039
вы думаете о задаче dea, это на

907
00:30:29,039 --> 00:30:31,120
самом деле немного проще, чем просто связывание сущностей,

908
00:30:31,120 --> 00:30:33,039
поэтому вы пытаетесь, например, предсказать, с

909
00:30:34,559 --> 00:30:36,399
чем связан боб из Боба Дилана и

910
00:30:36,399 --> 00:30:38,399
дует ветер и

911
00:30:38,399 --> 00:30:40,399
даже человеку гораздо легче увидеть, что Боб Дилан с

912
00:30:40,399 --> 00:30:42,000
большей вероятностью будет ссылаться на Боба Дилана или что Боб с

913
00:30:42,000 --> 00:30:43,840
большей вероятностью будет ссылаться на Боба Дилана,

914
00:30:43,840 --> 00:30:45,200
чем этот Боб будет ссылаться на дующий

915
00:30:45,200 --> 00:30:46,240
на ветру

916
00:30:46,240 --> 00:30:48,159
и анти-линковую задачу, у вас на самом деле

917
00:30:48,159 --> 00:30:49,440
есть гораздо более сложный набор кандидатов  Чтобы

918
00:30:49,440 --> 00:30:50,960
предсказать, вы не просто смотрите

919
00:30:50,960 --> 00:30:52,720
на те, что в предложении, поэтому

920
00:30:52,720 --> 00:30:55,200
связь Вашингтона с Джорджем Вашингтоном или

921
00:30:55,200 --> 00:30:57,039
штатом Вашингтон на самом деле требует от вас

922
00:30:57,039 --> 00:30:59,600
использования дополнительной информации о сущности,

923
00:30:59,600 --> 00:31:01,679
поэтому, учитывая более сложную задачу,

924
00:31:01,679 --> 00:31:04,559
неудивительно, что она может работать лучше,

925
00:31:04,559 --> 00:31:06,399
чем  просто эта более простая

926
00:31:06,399 --> 00:31:07,760
задача предварительной подготовки знаний, которую ввел Эрни,

927
00:31:09,919 --> 00:31:11,360
так что в остальном nobert имеет много

928
00:31:11,360 --> 00:31:13,440
общего с Эрни, он использует слой слияния,

929
00:31:13,440 --> 00:31:15,120
который объединяет этот контекст и

930
00:31:15,120 --> 00:31:16,640
информацию об объектах,

931
00:31:16,640 --> 00:31:18,080
и вводит s  Задача предварительной подготовки знаний,

932
00:31:19,519 --> 00:31:21,039
поэтому я бы сказал, что вывод высокого уровня заключается в том,

933
00:31:21,039 --> 00:31:22,159
что если вы хотите использовать предварительно обученные

934
00:31:22,159 --> 00:31:23,840
встраивания сущностей в языковую модель, вы,

935
00:31:23,840 --> 00:31:25,760
вероятно, по крайней мере захотите рассмотреть

936
00:31:25,760 --> 00:31:27,919
оба этих компонента с точки зрения фактического

937
00:31:27,919 --> 00:31:29,519
перехода к  интегрировать вложения проповедующих сущностей

938
00:31:31,039 --> 00:31:32,720
и максимально использовать имеющиеся

939
00:31:32,720 --> 00:31:35,600
в них знания,

940
00:31:37,200 --> 00:31:38,720
чтобы перейти к следующему классу

941
00:31:38,720 --> 00:31:40,320
техник, использующих внешнюю

942
00:31:40,320 --> 00:31:42,720
память,

943
00:31:42,720 --> 00:31:44,399
и здесь мы в основном сосредоточимся на этой работе

944
00:31:44,399 --> 00:31:46,240
под названием kglm, а затем мы также  Вкратце

945
00:31:46,240 --> 00:31:49,600
поговорим о kn lm,

946
00:31:49,679 --> 00:31:50,799
поэтому предыдущие методы, о которых мы

947
00:31:50,799 --> 00:31:52,320
говорили, полагались на

948
00:31:52,320 --> 00:31:54,240
встраивание сущностей до изменения для кодирования фактических

949
00:31:54,240 --> 00:31:56,799
знаний из баз знаний,

950
00:31:56,799 --> 00:31:58,559
и одна проблема с этим или одна

951
00:31:58,559 --> 00:32:00,799
из проблем с этим заключается в том, что вы хотите,

952
00:32:00,799 --> 00:32:02,720
чтобы  скажем, измените свою базу знаний,

953
00:32:02,720 --> 00:32:04,240
теперь вам нужно переобучить свои вложения сущностей,

954
00:32:04,240 --> 00:32:05,519
а затем переобучить свою

955
00:32:05,519 --> 00:32:06,960
языковую модель поверх этих встраиваний сущностей,

956
00:32:08,559 --> 00:32:10,720
поэтому возникает вопрос, есть ли более

957
00:32:10,720 --> 00:32:12,159
прямые способы в предварительно обученных электронных  ntity,

958
00:32:12,159 --> 00:32:14,480
чтобы предоставить модели фактические

959
00:32:14,480 --> 00:32:16,880
знания,

960
00:32:16,880 --> 00:32:18,320
и поэтому мы собираемся поговорить о том, как

961
00:32:18,320 --> 00:32:20,000
вы можете фактически использовать внешнюю память

962
00:32:20,000 --> 00:32:22,159
или хранилище ключевых значений, чтобы предоставить модели

963
00:32:22,159 --> 00:32:24,240
доступ либо к троек графа знаний,

964
00:32:24,240 --> 00:32:26,480
либо к контекстной информации,

965
00:32:26,480 --> 00:32:27,919
а также к ключевой вещи об этом  внешняя

966
00:32:27,919 --> 00:32:30,159
память заключается в том, что она не зависит от

967
00:32:30,159 --> 00:32:32,799
параметров изученной модели,

968
00:32:32,799 --> 00:32:34,960
поэтому это означает, что вы действительно можете поддерживать

969
00:32:34,960 --> 00:32:36,799
внедрение и обновление фактических знаний,

970
00:32:36,799 --> 00:32:38,480
вы можете сделать это непосредственно в символической

971
00:32:38,480 --> 00:32:40,559
внешней памяти, скажем,

972
00:32:40,559 --> 00:32:42,720
изменив значение для определенного ключа или, возможно,

973
00:32:42,720 --> 00:32:44,480
добавив еще один ключ,

974
00:32:44,480 --> 00:32:45,760
и вы  не нужно

975
00:32:45,760 --> 00:32:47,760
предварительно обучать или переобучать встраивание вашей сущности,

976
00:32:47,760 --> 00:32:50,559
когда вы вносите это изменение,

977
00:32:50,559 --> 00:32:51,600
и подходы, о которых мы поговорим

978
00:32:51,600 --> 00:32:53,679
сегодня, могут фактически даже иметь эти

979
00:32:53,679 --> 00:32:55,519
обновления во внешней памяти без

980
00:32:55,519 --> 00:32:58,720
дополнительной предварительной подготовки языковой модели,

981
00:32:58,720 --> 00:33:00,799
так что это довольно аккуратно

982
00:33:00,799 --> 00:33:02,480
а затем еще одно преимущество использования

983
00:33:02,480 --> 00:33:04,159
внешней памяти по сравнению с этими предварительно обученными

984
00:33:04,159 --> 00:33:06,240
подходами к встраиванию nc в том, что они также могут

985
00:33:06,240 --> 00:33:08,799
быть более интерпретируемыми, поэтому, если у вас есть

986
00:33:08,799 --> 00:33:11,679
ошибка или не ошибка воздух в вашей модели,

987
00:33:11,679 --> 00:33:14,559
где он не предсказывает правильный

988
00:33:14,559 --> 00:33:15,919
факт, очень сложно выяснить с

989
00:33:15,919 --> 00:33:18,080
предварительно обученными анти-встраиваемыми

990
00:33:21,440 --> 00:33:22,720
программами.  Вложения сущностей - это то, как

991
00:33:22,720 --> 00:33:24,240
языковая модель использует вложения сущностей,

992
00:33:24,240 --> 00:33:26,080
и здесь у вас есть немного

993
00:33:26,080 --> 00:33:28,480
больше информации о внешней памяти,

994
00:33:28,480 --> 00:33:29,679
и что вы можете посмотреть во внешней

995
00:33:29,679 --> 00:33:31,279
памяти и увидеть, был ли факт во

996
00:33:31,279 --> 00:33:32,960
внешней памяти,

997
00:33:32,960 --> 00:33:34,880
был ли он не во внешней памяти  и так

998
00:33:34,880 --> 00:33:37,039
далее, поэтому он добавляет немного больше

999
00:33:37,039 --> 00:33:39,120
интерпретируемости, чем просто использование этих

1000
00:33:39,120 --> 00:33:40,960
предварительно обученных ставок ncm в качестве косвенного

1001
00:33:40,960 --> 00:33:44,399
способа кодирования базы знаний,

1002
00:33:45,600 --> 00:33:46,720
поэтому первая работа, о которой мы собираемся поговорить,

1003
00:33:46,720 --> 00:33:49,120
называется kglm и в отличие от

1004
00:33:49,120 --> 00:33:50,799
других подходов, которые мы '  Мы уже говорили о том, что

1005
00:33:50,799 --> 00:33:53,279
на самом деле здесь используются lstms, а не

1006
00:33:53,279 --> 00:33:55,519
трансформаторы,

1007
00:33:55,519 --> 00:33:57,760
поэтому ключевая идея здесь состоит в том, чтобы обусловить

1008
00:33:57,760 --> 00:34:00,640
языковую модель на графе знаний,

1009
00:34:00,640 --> 00:34:02,559
поэтому вспомните, что с помощью стандартной языковой модели

1010
00:34:02,559 --> 00:34:04,240
мы хотим предсказать следующий  слово,

1011
00:34:04,240 --> 00:34:07,120
учитывая предыдущие слова в последовательности,

1012
00:34:07,120 --> 00:34:08,320
теперь мы также хотим предсказать

1013
00:34:08,320 --> 00:34:10,399
следующую сущность, учитывая предыдущие слова

1014
00:34:10,399 --> 00:34:12,000
в последовательности и учитывая предыдущие

1015
00:34:12,000 --> 00:34:14,399
сущности в предложении

1016
00:34:14,399 --> 00:34:15,918
или сущности, которые имеют отношение к

1017
00:34:15,918 --> 00:34:18,878
предложению, я должен сказать,

1018
00:34:19,119 --> 00:34:21,199
что kglm будет строить  граф локальных

1019
00:34:21,199 --> 00:34:22,960
знаний при итерации по

1020
00:34:22,960 --> 00:34:24,239
последовательности,

1021
00:34:24,239 --> 00:34:25,760
а граф локальных знаний - это всего лишь

1022
00:34:25,760 --> 00:34:27,679
подмножество полного графа знаний, в котором

1023
00:34:27,679 --> 00:34:28,960
есть только те сущности, которые действительно имеют

1024
00:34:28,960 --> 00:34:31,119
отношение к последовательности.

1025
00:34:34,239 --> 00:34:36,320
документ о

1026
00:34:36,320 --> 00:34:37,839
том, что super mario land - это игра,

1027
00:34:37,839 --> 00:34:40,480
разработанная пустой и super mario land,

1028
00:34:40,480 --> 00:34:42,800
вот сущность,

1029
00:34:42,800 --> 00:34:44,399
вам нужен график местных знаний, как

1030
00:34:44,399 --> 00:34:46,239
показано ниже, где вы видите, что super mario

1031
00:34:46,239 --> 00:34:48,320
land находится в графе местных знаний, но

1032
00:34:48,320 --> 00:34:50,079
у нас также есть отношения с super

1033
00:34:50,079 --> 00:34:52,159
mario land другим объектам, которые

1034
00:34:52,159 --> 00:34:53,679
копируются из полного графа знаний

1035
00:34:53,679 --> 00:34:56,159
в этот граф местных знаний,

1036
00:34:56,159 --> 00:34:57,200
и вы можете построить этот

1037
00:34:57,200 --> 00:34:58,560
граф местных знаний по мере выполнения итерации по

1038
00:34:58,560 --> 00:35:00,880
предложение, поэтому всякий раз, когда вы видите объект,

1039
00:35:00,880 --> 00:35:02,320
вы должны добавлять его в граф местных знаний,

1040
00:35:02,320 --> 00:35:04,160
а также его отношения с другими

1041
00:35:04,160 --> 00:35:06,240
объектами,

1042
00:35:06,240 --> 00:35:08,240
поэтому очевидно, что это гораздо меньший

1043
00:35:08,240 --> 00:35:10,000
пример, чем тот, который действительно имел бы

1044
00:35:10,000 --> 00:35:12,000
все отношения с землей супер марио только

1045
00:35:12,000 --> 00:35:13,760
для целей  пример,

1046
00:35:13,760 --> 00:35:14,880
но, надеюсь, ясно, что все

1047
00:35:14,880 --> 00:35:18,640
они имеют отношение к последовательности,

1048
00:35:19,599 --> 00:35:20,960
и здесь важно отметить, что

1049
00:35:20,960 --> 00:35:22,720
здесь предполагается, что сущности

1050
00:35:22,720 --> 00:35:24,160
известны во время обучения,

1051
00:35:24,160 --> 00:35:25,520
так что у вас есть

1052
00:35:25,520 --> 00:35:26,960
аннотированные данные этой сущности для обучения, и

1053
00:35:26,960 --> 00:35:28,400
поэтому ваш локальный граф знаний

1054
00:35:28,400 --> 00:35:30,079
всегда  граф местных знаний на основе истины по

1055
00:35:30,079 --> 00:35:33,599
мере того, как вы повторяете последовательность,

1056
00:35:33,599 --> 00:35:35,280
так почему это может быть хорошей идеей, чтобы сделать

1057
00:35:35,280 --> 00:35:36,160
это

1058
00:35:36,160 --> 00:35:37,760
хорошо? Здесь следующее слово, которое вы хотите

1059
00:35:37,760 --> 00:35:39,359
предсказать, - это nintendo,

1060
00:35:39,359 --> 00:35:41,119
и вы можете заметить, что nintendo находится в

1061
00:35:41,119 --> 00:35:42,880
вашем графе местных знаний,

1062
00:35:42,880 --> 00:35:44,400
поэтому иногда этот местный  График знаний

1063
00:35:44,400 --> 00:35:45,920
может на самом деле служить очень сильным

1064
00:35:45,920 --> 00:35:47,920
сигналом для того, что вы хотите предсказать для

1065
00:35:47,920 --> 00:35:50,240
своего следующего слова,

1066
00:35:50,240 --> 00:35:51,520
теперь вы, возможно, хорошо думаете, что это

1067
00:35:51,520 --> 00:35:53,520
не  всегда быть полезным, и это

1068
00:35:53,520 --> 00:35:55,359
правда, тоже все,

1069
00:35:55,359 --> 00:35:56,720
поэтому, если вы посмотрите точно так же, как на третье

1070
00:35:56,720 --> 00:35:57,839
слово в последовательности, и вы хотите

1071
00:35:57,839 --> 00:36:00,800
предсказать это слово, например, игра

1072
00:36:02,079 --> 00:36:03,520
хороша, если его нет в

1073
00:36:03,520 --> 00:36:04,720
графе местных знаний, это не  Не

1074
00:36:04,720 --> 00:36:06,560
обязательно быть настолько полезным, что

1075
00:36:06,560 --> 00:36:07,920
вы просто сделаете прогноз стандартной языковой

1076
00:36:07,920 --> 00:36:10,000
модели,

1077
00:36:10,000 --> 00:36:11,119
или если вы находитесь в начале

1078
00:36:11,119 --> 00:36:13,200
последовательности, ваш граф местных знаний

1079
00:36:13,200 --> 00:36:14,480
пуст, поэтому, конечно, вы не

1080
00:36:14,480 --> 00:36:16,640
получите от него никакого сигнала,

1081
00:36:16,640 --> 00:36:19,119
поэтому первый вопрос  они спрашивают в kglm,

1082
00:36:19,119 --> 00:36:21,119
как может языковая модель узнать, когда

1083
00:36:21,119 --> 00:36:22,880
использовать граф местных знаний и когда он

1084
00:36:22,880 --> 00:36:24,800
может быть полезен для

1085
00:36:24,800 --> 00:36:27,839
предсказания следующего слова,

1086
00:36:29,119 --> 00:36:30,480
поэтому мы собираемся сохранить тот же пример,

1087
00:36:30,480 --> 00:36:32,160
что и рабочий пример, и у нас есть наши

1088
00:36:32,160 --> 00:36:33,920
местные знания  График здесь

1089
00:36:33,920 --> 00:36:35,680
у нас теперь есть lstm, который похож

1090
00:36:35,680 --> 00:36:36,960
на представления, которые вы видели

1091
00:36:36,960 --> 00:36:38,240
в этом классе,

1092
00:36:38,240 --> 00:36:39,839
и обычно вы видели, что lstm хорошо

1093
00:36:39,839 --> 00:36:42,000
предсказывает следующее слово,

1094
00:36:42,000 --> 00:36:43,440
теперь мы также собираемся использовать

1095
00:36:43,440 --> 00:36:45,839
lstm для прогнозирования следующего типа

1096
00:36:45,839 --> 00:36:46,640
слово

1097
00:36:46,640 --> 00:36:48,079
так есть  следующее слово будет

1098
00:36:48,079 --> 00:36:50,320
связанным объектом, означающим, что оно уже находится в

1099
00:36:50,320 --> 00:36:51,839
графе местных знаний,

1100
00:36:51,839 --> 00:36:53,839
это будет новый объект, означающий,

1101
00:36:53,839 --> 00:36:55,920
что его нет в графе местных знаний, или

1102
00:36:55,920 --> 00:36:57,839
это будет не объект, и в этом

1103
00:36:57,839 --> 00:36:59,760
случае вы просто вернетесь к  нормальное предсказание lstm,

1104
00:37:01,760 --> 00:37:03,280
и они собираются использовать скрытое состояние lstm,

1105
00:37:03,280 --> 00:37:05,119
чтобы сделать это предсказание

1106
00:37:05,119 --> 00:37:06,480
типа следующего слова

1107
00:37:06,480 --> 00:37:07,760
по этим трём трём различным

1108
00:37:07,760 --> 00:37:11,200
классам, которые они могут захотеть рассмотреть,

1109
00:37:11,359 --> 00:37:13,280
поэтому в случае super mario land как

1110
00:37:13,280 --> 00:37:15,520
игры  Разработанный Nintendo, мы увидели, что

1111
00:37:15,520 --> 00:37:17,599
это будет случай связанной сущности,

1112
00:37:17,599 --> 00:37:19,280
потому что вы видели, что nintendo была в

1113
00:37:19,280 --> 00:37:21,119
графе местных знаний,

1114
00:37:21,119 --> 00:37:23,280
для других случаев super mario land

1115
00:37:23,280 --> 00:37:25,359
будет новым случаем сущности, поскольку

1116
00:37:25,359 --> 00:37:26,640
в этой точке граф локальных знаний пуст

1117
00:37:26,640 --> 00:37:27,680
,

1118
00:37:27,680 --> 00:37:29,599
а затем  любое из слов между super

1119
00:37:29,599 --> 00:37:31,440
mario land и nintendo не было бы

1120
00:37:31,440 --> 00:37:34,000
сущностью, поскольку это просто стандартное

1121
00:37:34,000 --> 00:37:36,640
предсказание языковой модели lstm, которое

1122
00:37:36,640 --> 00:37:39,839
не включает никаких сущностей,

1123
00:37:40,079 --> 00:37:41,760
поэтому теперь нам нужно поговорить о том, что задействует

1124
00:37:41,760 --> 00:37:43,280
языковая модель.  ally в этих

1125
00:37:43,280 --> 00:37:45,280
трех разных сценариях предсказывает

1126
00:37:45,280 --> 00:37:50,119
следующую сущность и следующее слово,

1127
00:37:50,880 --> 00:37:52,160
поэтому мы собираемся оставить пример

1128
00:37:52,160 --> 00:37:53,440
вверху на тот случай, если вы захотите вернуться

1129
00:37:53,440 --> 00:37:55,520
к трем различным случаям,

1130
00:37:55,520 --> 00:37:56,240
и мы собираемся начать с

1131
00:37:56,240 --> 00:37:58,960
связанных  case entity,

1132
00:37:58,960 --> 00:38:01,520
поэтому здесь мы предполагаем, что следующее слово или

1133
00:38:01,520 --> 00:38:03,280
объект на самом деле находится в вашем локальном

1134
00:38:03,280 --> 00:38:05,040
графе знаний, и помним, что мы можем

1135
00:38:05,040 --> 00:38:06,400
описать граф знаний в терминах

1136
00:38:06,400 --> 00:38:09,440
троек, таким образом, в терминах uh пар

1137
00:38:09,440 --> 00:38:11,040
отношений родительских сущностей и хвостовых

1138
00:38:11,040 --> 00:38:12,240
сущностей

1139
00:38:12,240 --> 00:38:13,680
и в случае  предсказывая следующее

1140
00:38:13,680 --> 00:38:15,760
слово как

1141
00:38:15,760 --> 00:38:17,599
Nintendo,

1142
00:38:17,599 --> 00:38:19,280
в графе местных знаний может быть только одна родительская сущность, которая является

1143
00:38:19,280 --> 00:38:21,040
супер-марионеткой,

1144
00:38:21,040 --> 00:38:22,640
и ваша цель - выяснить,

1145
00:38:22,640 --> 00:38:24,880
какая тройка является наиболее релевантной, которая

1146
00:38:24,880 --> 00:38:26,560
будет полезна для предсказания

1147
00:38:26,560 --> 00:38:28,000
следующего слова,

1148
00:38:28,000 --> 00:38:29,200
так что в  В этом случае у

1149
00:38:31,200 --> 00:38:33,280
вас может быть Nintendo, издатель Triple Super Mario Land, у вас может быть

1150
00:38:33,280 --> 00:38:35,680
платформенная игра в жанре Triple Super Mario Land, которая на

1151
00:38:35,680 --> 00:38:37,200
самом деле помогает предсказать,

1152
00:38:37,200 --> 00:38:40,560
что Nintendo должна  быть следующим словом,

1153
00:38:40,560 --> 00:38:42,960
так что здесь, что вы хотели бы, чтобы kglm сделал,

1154
00:38:42,960 --> 00:38:44,640
это предсказать, что родительский

1155
00:38:44,640 --> 00:38:47,200
объект с наибольшим количеством очков - это super mario land, а

1156
00:38:47,200 --> 00:38:49,680
отношение с наибольшим количеством очков - издатель, и вы

1157
00:38:49,680 --> 00:38:51,280
можете видеть, что на самом деле

1158
00:38:51,280 --> 00:38:52,720
в предложении есть контекстные подсказки, которые могут помочь вам

1159
00:38:52,720 --> 00:38:54,640
выяснить  о какой тройке вы

1160
00:38:54,640 --> 00:38:56,400
говорите,

1161
00:38:56,400 --> 00:38:57,920
а затем, учитывая, что ваша

1162
00:38:57,920 --> 00:38:59,839
родительская сущность с наибольшим количеством очков - это земля супер марио, а

1163
00:38:59,839 --> 00:39:02,000
ваше отношение с наивысшей оценкой - издатель,

1164
00:39:02,000 --> 00:39:04,000
вы можете выяснить, что с помощью

1165
00:39:04,000 --> 00:39:06,240
графа знаний втрое, хвостовая сущность должна быть

1166
00:39:06,240 --> 00:39:07,359
Nintendo,

1167
00:39:07,359 --> 00:39:08,640
и, следовательно, это дает вам  сильный

1168
00:39:08,640 --> 00:39:10,400
сигнал о том, что следующим словом будет

1169
00:39:10,400 --> 00:39:13,400
nintendo,

1170
00:39:14,960 --> 00:39:16,079
поэтому цель состоит в том, чтобы найти

1171
00:39:16,079 --> 00:39:17,839
родительский объект с наибольшей

1172
00:39:17,839 --> 00:39:19,680
оценкой и соотношение с наивысшей оценкой, используя узлы в вашем

1173
00:39:19,680 --> 00:39:21,599
графе местных знаний, и вы можете сделать

1174
00:39:21,599 --> 00:39:23,599
это, используя скрытое состояние lstm в

1175
00:39:23,599 --> 00:39:24,960
сочетании с  предварительно обученные вложения сущностей и

1176
00:39:24,960 --> 00:39:26,720
отношений,

1177
00:39:26,720 --> 00:39:28,160
поэтому я признаю, что немного обманул здесь,

1178
00:39:28,160 --> 00:39:29,920
и что здесь используются предварительно обученные

1179
00:39:29,920 --> 00:39:31,839
вложения, но, надеюсь, вы увидите

1180
00:39:31,839 --> 00:39:33,599
к концу этого обсуждения  почему я думаю, что

1181
00:39:33,599 --> 00:39:35,200
он немного лучше подходит для этого

1182
00:39:35,200 --> 00:39:38,320
варианта использования внешней памяти,

1183
00:39:38,720 --> 00:39:39,520
поэтому они

1184
00:39:39,520 --> 00:39:41,119
собираются сделать softmax, используя

1185
00:39:41,119 --> 00:39:43,280
скрытое состояние lstm и встраивание сущностей

1186
00:39:43,280 --> 00:39:44,560
для каждой из потенциальных родительских

1187
00:39:44,560 --> 00:39:46,160
сущностей и  тогда мы возьмем этот

1188
00:39:46,160 --> 00:39:48,400
объект с самым высоким рейтингом в качестве родительского объекта,

1189
00:39:48,400 --> 00:39:49,520
и они сделают то же самое для

1190
00:39:49,520 --> 00:39:52,000
отношения, встраиваемого

1191
00:39:52,000 --> 00:39:53,839
в следующий объект, тогда это будет только этот хвостовой

1192
00:39:53,839 --> 00:39:55,920
объект из тройки графа знаний,

1193
00:39:55,920 --> 00:39:57,359
поэтому относительно тривиально выяснить,

1194
00:39:57,359 --> 00:39:58,400
что следующий

1195
00:39:58,400 --> 00:40:00,240
объект  должно быть после того, как вы

1196
00:40:00,240 --> 00:40:02,000
выяснили родительский объект с наибольшей оценкой и ваше

1197
00:40:02,000 --> 00:40:04,640
отношение с наибольшей оценкой,

1198
00:40:04,640 --> 00:40:06,480
а затем, наконец, чтобы предсказать следующее

1199
00:40:06,480 --> 00:40:09,040
слово, они берут словарь и

1200
00:40:09,040 --> 00:40:11,359
расширяют его, включая различные псевдонимы,

1201
00:40:11,359 --> 00:40:13,760
которые могут относиться к этому объекту,

1202
00:40:13,760 --> 00:40:15,760
так что мы подразумеваем под псевдонимами  вот

1203
00:40:15,760 --> 00:40:17,440
фразы, которые могут относиться к объекту

1204
00:40:17,440 --> 00:40:18,720
и тексту,

1205
00:40:18,720 --> 00:40:20,560
так что вы можете не просто называть его nintendo,

1206
00:40:20,560 --> 00:40:22,560
вы также можете сказать nintendo company или

1207
00:40:22,560 --> 00:40:24,640
copai, и вы хотите, чтобы любое из этих слов было

1208
00:40:24,640 --> 00:40:26,640
возможными словами, которые вы могли бы предсказать

1209
00:40:26,640 --> 00:40:28,640
следующее слово,

1210
00:40:28,640 --> 00:40:31,119
поэтому цель этого расширения словаря

1211
00:40:31,119 --> 00:40:33,119
- увеличить вероятность того, что

1212
00:40:33,119 --> 00:40:34,720
следующее слово, которое вы прогнозируете, действительно будет

1213
00:40:34,720 --> 00:40:39,079
связано с этой следующей сущностью,

1214
00:40:40,000 --> 00:40:42,319
поэтому случай новой сущности немного проще, это

1215
00:40:42,319 --> 00:40:43,520
означает, что сущность, которую вы

1216
00:40:43,520 --> 00:40:45,040
предсказываете, не находится в  граф локальных знаний,

1217
00:40:45,040 --> 00:40:46,560
чтобы вы не получали никакого сигнала

1218
00:40:46,560 --> 00:40:47,760
от этого графа местных знаний, который

1219
00:40:47,760 --> 00:40:50,079
вы создавали,

1220
00:40:50,079 --> 00:40:51,760
и все, что вам нужно сделать, это найти объект с наивысшей

1221
00:40:51,760 --> 00:40:53,200
оценкой в полном графе знаний,

1222
00:40:53,200 --> 00:40:55,440
и вы можете сделать это с помощью lstm

1223
00:40:55,440 --> 00:40:57,440
hidden  заявить и притвориться, что ставки ntm

1224
00:40:57,440 --> 00:40:58,880
похожи на то, как мы нашли оценку

1225
00:40:58,880 --> 00:41:01,760
для верхней родительской сущности,

1226
00:41:01,760 --> 00:41:03,359
ваша следующая сущность будет просто сущностью с наивысшей

1227
00:41:03,359 --> 00:41:04,880
оценкой из полного графа знаний,

1228
00:41:04,880 --> 00:41:07,599
а затем вашим следующим словом

1229
00:41:07,599 --> 00:41:09,200
снова будет этот словарь, расширенный за счет

1230
00:41:09,200 --> 00:41:13,040
включения псевдонимов этой сущности

1231
00:41:13,040 --> 00:41:15,680
регистр not in empty - это самый простой вариант:

1232
00:41:15,680 --> 00:41:18,000
вы просто вернетесь к обычному lstm, у вас

1233
00:41:18,000 --> 00:41:20,160
нет объекта x, который нужно предсказать, и ваше

1234
00:41:20,160 --> 00:41:22,000
следующее слово является наиболее вероятным следующим

1235
00:41:22,000 --> 00:41:25,839
токеном по сравнению с вашим обычным словарным запасом,

1236
00:41:26,800 --> 00:41:28,800
поэтому здесь '  диаграмма из их статьи,

1237
00:41:28,800 --> 00:41:30,640
которая, надеюсь, суммирует и делает еще

1238
00:41:30,640 --> 00:41:31,520
более ясным

1239
00:41:31,520 --> 00:41:33,359
то, что я только что рассмотрел,

1240
00:41:33,359 --> 00:41:35,440
поэтому у них есть более длинный пример, чем тот, на

1241
00:41:35,440 --> 00:41:36,640
который мы смотрим, но это тот же

1242
00:41:36,640 --> 00:41:38,880
прогноз, что и Nintendo - следующее слово,

1243
00:41:38,880 --> 00:41:40,400
и у них есть свои прогнозы красным,

1244
00:41:40,400 --> 00:41:41,920
так что это  они хотят, чтобы kglm

1245
00:41:41,920 --> 00:41:42,960
предсказал,

1246
00:41:42,960 --> 00:41:44,960
что три разных случая находятся в

1247
00:41:44,960 --> 00:41:47,280
горизонтальной плоскости, и мы видим, что здесь вы находитесь

1248
00:41:47,280 --> 00:41:49,119
в случае связанной сущности, поскольку

1249
00:41:49,119 --> 00:41:50,880
nintendo находится в вашем локальном графике знаний,

1250
00:41:52,240 --> 00:41:54,079
поэтому они хотят, чтобы kglm предсказал, что

1251
00:41:54,079 --> 00:41:56,400
nintendo должна быть связанным типом сущности

1252
00:41:56,400 --> 00:41:57,359
слова,

1253
00:41:57,359 --> 00:41:59,280
что super mario land должна быть его

1254
00:41:59,280 --> 00:42:01,680
родительской сущностью, что издатель должен

1255
00:42:01,680 --> 00:42:04,079
быть соответствующим родственником, и в

1256
00:42:04,079 --> 00:42:06,240
результате следующая сущность - nintendo, а затем

1257
00:42:06,240 --> 00:42:08,160
они расширяют свой словарный запас, вы видите

1258
00:42:08,160 --> 00:42:09,119
псевдонимы

1259
00:42:09,119 --> 00:42:11,040
псевдонимы nintendo внизу, и,

1260
00:42:11,040 --> 00:42:12,160
наконец, они фактически предсказывают

1261
00:42:12,160 --> 00:42:14,480
nintendo  поскольку следующее слово

1262
00:42:14,480 --> 00:42:16,000
и другие случаи просто резюмируют то, что

1263
00:42:16,000 --> 00:42:19,280
мы уже прошли,

1264
00:42:20,000 --> 00:42:22,319
поэтому я обнаружил, что kglm на самом деле превосходит

1265
00:42:22,319 --> 00:42:26,720
gpt2 и awd lstm, что как

1266
00:42:26,720 --> 00:42:29,599
Языковая модель trong lstm в задаче полного завершения,

1267
00:42:29,599 --> 00:42:30,720
аналогичной заполнению пустых

1268
00:42:30,720 --> 00:42:31,839
примеров, которые мы рассмотрели в

1269
00:42:31,839 --> 00:42:34,160
начале выступления,

1270
00:42:34,160 --> 00:42:35,839
они также обнаружили качественно, что

1271
00:42:35,839 --> 00:42:38,640
по сравнению с gpt 2 kglm имеет тенденцию предсказывать

1272
00:42:38,640 --> 00:42:40,560
более конкретные токены, поскольку он может

1273
00:42:40,560 --> 00:42:42,400
предсказать эти токены из

1274
00:42:42,400 --> 00:42:43,760
просто копирование из графа местных знаний,

1275
00:42:44,640 --> 00:42:46,560
тогда как gpt2 будет иметь тенденцию предсказывать более

1276
00:42:46,560 --> 00:42:48,480
общие токены, поэтому, если вы хотите предсказать

1277
00:42:48,480 --> 00:42:50,560
место рождения кого-то, gpt2 с большей

1278
00:42:50,560 --> 00:42:52,560
вероятностью предскажет, например, Нью-Йорк,

1279
00:42:52,560 --> 00:42:54,800
а kglm может предсказать какое-то неясное

1280
00:42:54,800 --> 00:42:56,880
место,

1281
00:42:56,880 --> 00:42:58,079
и тогда у них были эти действительно крутые

1282
00:42:58,079 --> 00:42:59,680
набор экспериментов, в которых они показали, что

1283
00:42:59,680 --> 00:43:01,599
kglm на самом деле поддерживает изменение или

1284
00:43:01,599 --> 00:43:03,520
обновление фактов,

1285
00:43:03,520 --> 00:43:04,960
поэтому они внесли прямое изменение в

1286
00:43:04,960 --> 00:43:06,800
граф знаний, а затем они увидели,

1287
00:43:06,800 --> 00:43:10,000
как изменились прогнозы kglm,

1288
00:43:10,000 --> 00:43:12,400
так что у них есть этот пример, где

1289
00:43:12,400 --> 00:43:14,160
последовательность была рождена Бараком Обама

1290
00:43:14,160 --> 00:43:15,440
пусто,

1291
00:43:15,440 --> 00:43:17,040
у них был график знаний втрое больше,

1292
00:43:17,040 --> 00:43:19,040
чем исходная дата рождения Барака Обамы, и

1293
00:43:19,040 --> 00:43:20,880
тогда их наиболее вероятные следующие жетоны были

1294
00:43:20,880 --> 00:43:23,920
такими, как ожидалось  4 августа 1961 года,

1295
00:43:23,920 --> 00:43:25,040
а затем они просто изменили свой

1296
00:43:25,040 --> 00:43:26,560
график знаний, так что они изменили

1297
00:43:26,560 --> 00:43:28,800
дату рождения Обамы, они сказали, что

1298
00:43:28,800 --> 00:43:30,560
теперь он родился в 2013 году.

1299
00:43:35,839 --> 00:43:38,240
в локальном графе знаний,

1300
00:43:38,240 --> 00:43:39,599
так что это то, что довольно круто,

1301
00:43:39,599 --> 00:43:41,200
и что на самом деле только подходы к внешней памяти, которые

1302
00:43:41,200 --> 00:43:44,319
um может делать по сравнению с

1303
00:43:44,319 --> 00:43:45,280
этими исходными предварительно обученными

1304
00:43:45,280 --> 00:43:46,480
подходами против внедрения, о которых мы говорили,

1305
00:43:47,280 --> 00:43:48,720
и я думаю, что это одна из

1306
00:43:48,720 --> 00:43:50,640
причин того, что  kglm, по крайней мере, на мой взгляд,

1307
00:43:50,640 --> 00:43:52,240
лучше подходит для этих случаев использования внешней памяти,

1308
00:43:56,319 --> 00:43:58,319
так что следующий слайд - это другая

1309
00:43:58,319 --> 00:44:00,000
статья, поэтому я думаю, что отвечу на вопросы по

1310
00:44:00,000 --> 00:44:01,599
kglm,

1311
00:44:01,599 --> 00:44:04,240
если они есть,

1312
00:44:07,200 --> 00:44:08,800
это довольно сложный метод, поэтому не

1313
00:44:08,800 --> 00:44:10,400
стесняйтесь задавать вопросы,

1314
00:44:10,400 --> 00:44:13,200
да, мм  не могли бы вы еще раз объяснить,

1315
00:44:13,200 --> 00:44:14,800
какое определение графа локальных

1316
00:44:14,800 --> 00:44:16,480
знаний соотносится

1317
00:44:16,480 --> 00:44:18,240
с глобальным графом

1318
00:44:20,560 --> 00:44:21,839
знаний.

1319
00:44:21,839 --> 00:44:24,079
График знаний,

1320
00:44:24,079 --> 00:44:26,160
и он должен состоять только из

1321
00:44:26,160 --> 00:44:27,920
сущностей, которые на самом деле действительно

1322
00:44:27,920 --> 00:44:29,599
были замечены в последовательности

1323
00:44:32,480 --> 00:44:33,440
мм, а также

1324
00:44:33,440 --> 00:44:35,440
их соответствующие сущности

1325
00:44:35,440 --> 00:44:38,720
хорошо

1326
00:44:38,720 --> 00:44:40,640
ой хорошо, так что здесь вы видите, что

1327
00:44:40,640 --> 00:44:42,400
земля супер марио находится в графе местных знаний,

1328
00:44:42,400 --> 00:44:43,760
потому что земля супер марио  - это

1329
00:44:43,760 --> 00:44:45,760
объект, который виден в последовательности, а

1330
00:44:45,760 --> 00:44:47,920
затем вы также хотите скопировать все

1331
00:44:47,920 --> 00:44:50,560
грани из земли супер марио, которая

1332
00:44:50,560 --> 00:44:52,160
будет в полном графе знаний,

1333
00:44:52,160 --> 00:44:53,839
так что это всего лишь их подмножество для

1334
00:44:53,839 --> 00:44:55,359
целей примера, но вы видите, что

1335
00:44:55,359 --> 00:44:57,599
super mario land имеет преимущество от Nintendo до

1336
00:44:57,599 --> 00:44:59,599
Game Boy и платформенной игры, поэтому вы

1337
00:44:59,599 --> 00:45:01,040
должны скопировать все грани, которые имеет super mario

1338
00:45:01,040 --> 00:45:03,119
land, в другой узел в полном

1339
00:45:03,119 --> 00:45:04,720
графе знаний,

1340
00:45:04,720 --> 00:45:06,160
и они знают заранее, как будто у них

1341
00:45:06,160 --> 00:45:08,079
есть метки здесь для того, что

1342
00:45:08,079 --> 00:45:10,079
сущности во время  обучения, так что именно так они

1343
00:45:10,079 --> 00:45:12,160
могут создать этот граф достоверных

1344
00:45:12,160 --> 00:45:14,079
знаний,

1345
00:45:14,079 --> 00:45:17,200
а затем вкратце студент спросил, почему мы

1346
00:45:17,200 --> 00:45:19,359
не можем просто использовать весь граф знаний,

1347
00:45:19,359 --> 00:45:21,440
и я дал ответ, но, возможно, вы знаете  Как

1348
00:45:21,440 --> 00:45:22,640
лучше,

1349
00:45:22,640 --> 00:45:24,960
да, я думаю, что идея в том, что сигнал

1350
00:45:24,960 --> 00:45:26,800
будет намного сильнее, если вы просто используете

1351
00:45:26,800 --> 00:45:29,680
граф локальных знаний, поэтому в softmax

1352
00:45:29,680 --> 00:45:32,560
для случая связанной сущности вы

1353
00:45:32,560 --> 00:45:34,319
просто

1354
00:45:34,319 --> 00:45:36,240
прогнозируете потенциальные родительские

1355
00:45:36,240 --> 00:45:37,440
сущности в вашем локальном графе знаний,

1356
00:45:37,440 --> 00:45:38,800
который является  намного меньше, чем то, что находится

1357
00:45:38,800 --> 00:45:41,280
в вашем полном графике знаний,

1358
00:45:41,280 --> 00:45:42,960
поэтому я думаю, что более вероятно, что вы

1359
00:45:42,960 --> 00:45:44,319
собираетесь предсказать что-то, что является

1360
00:45:44,319 --> 00:45:46,319
правильным в этом случае, тогда, когда у вас

1361
00:45:46,319 --> 00:45:48,240
есть около пяти миллионов объектов в вашем

1362
00:45:48,240 --> 00:45:49,599
полном графе знаний, это также намного

1363
00:45:49,599 --> 00:45:50,960
дешевле  compute

1364
00:45:50,960 --> 00:45:52,400
um в этом случае есть только один

1365
00:45:52,400 --> 00:45:53,760
родительский объект, но у вас может быть

1366
00:45:53,760 --> 00:45:54,880
несколько родительских объектов, которые вы

1367
00:45:54,880 --> 00:45:56,319
пытаетесь вычислить, из которых, скорее

1368
00:45:56,319 --> 00:45:58,400
всего,

1369
00:45:58,400 --> 00:46:01,599
закончится то, о чем вы также думали.

1370
00:46:04,160 --> 00:46:06,240
Сигнал

1371
00:46:06,240 --> 00:46:07,280
тоже круто,

1372
00:46:08,079 --> 00:46:09,920
вот захватывающий вопрос, а как насчет

1373
00:46:09,920 --> 00:46:12,560
запросов, которые требуют более одного

1374
00:46:12,560 --> 00:46:16,400
шага в графе знаний, например,

1375
00:46:16,400 --> 00:46:18,880
местонахождение издателя super

1376
00:46:18,880 --> 00:46:21,440
mario l  и

1377
00:46:22,720 --> 00:46:25,280
да, это хороший вопрос,

1378
00:46:25,280 --> 00:46:26,880
поэтому идея в том, может ли он поддерживать эти

1379
00:46:26,880 --> 00:46:30,160
типы, например, поддерживает ли он многопоточное

1380
00:46:30,160 --> 00:46:32,880
построение графа знаний?

1381
00:46:38,160 --> 00:46:39,760
вопрос, они строят

1382
00:46:39,760 --> 00:46:41,520
граф знаний так, что это всего лишь один переход,

1383
00:46:41,520 --> 00:46:43,839
насколько я знаю, но как если бы вы видели

1384
00:46:43,839 --> 00:46:45,520
другие объекты,

1385
00:46:45,520 --> 00:46:46,880
если бы вы видели объекты

1386
00:46:46,880 --> 00:46:48,640
на переходах, они были бы в локальном

1387
00:46:48,640 --> 00:46:50,400
графике знаний

1388
00:46:50,400 --> 00:46:51,599
да, это хорошо  вопрос, я не знаю,

1389
00:46:51,599 --> 00:46:55,160
исследовали ли они это

1390
00:47:00,560 --> 00:47:02,240
великое

1391
00:47:02,240 --> 00:47:03,359
хорошо, ладно,

1392
00:47:03,359 --> 00:47:07,040
давайте продолжим, тогда

1393
00:47:09,599 --> 00:47:10,640
хорошо,

1394
00:47:10,640 --> 00:47:11,760
так что следующая часть работы, о которой мы собираемся

1395
00:47:11,760 --> 00:47:13,200
поговорить, вы, ребята, на самом деле

1396
00:47:13,200 --> 00:47:15,440
кратко видели в лекции по созданию естественного языка,

1397
00:47:17,040 --> 00:47:18,480
но я собираюсь пойти  Повторите это снова

1398
00:47:18,480 --> 00:47:20,720
быстро, так что в отличие от других

1399
00:47:20,720 --> 00:47:22,000
работ, о которых мы говорили, что использованный

1400
00:47:22,000 --> 00:47:23,680
граф знаний утроился, это на самом деле

1401
00:47:23,680 --> 00:47:25,280
будет иметь своего рода более свободное представление о

1402
00:47:25,280 --> 00:47:27,440
знаниях, поскольку знания

1403
00:47:27,440 --> 00:47:28,880
будут просто закодированы в тексте во время

1404
00:47:28,880 --> 00:47:30,480
обучения  набор данных,

1405
00:47:30,480 --> 00:47:33,680
поэтому он называется kn lm, и идея состоит в

1406
00:47:33,680 --> 00:47:35,520
том, что мы строим идею о том, что

1407
00:47:35,520 --> 00:47:37,119
языковые модели не только учатся

1408
00:47:37,119 --> 00:47:38,960
предсказывать следующее слово и текст, но

1409
00:47:38,960 --> 00:47:41,920
также изучают эти представления текста,

1410
00:47:41,920 --> 00:47:43,280
и авторы предполагают, что на

1411
00:47:43,280 --> 00:47:45,760
самом деле это может быть проще  Чтобы узнать сходство

1412
00:47:45,760 --> 00:47:47,599
между текстовыми последовательностями, чем

1413
00:47:47,599 --> 00:47:50,559
предсказать следующее слово в тексте, у

1414
00:47:50,559 --> 00:47:52,559
вас есть этот пример, что Диккенс

1415
00:47:52,559 --> 00:47:54,240
является автором пробела, а Диккенс написал

1416
00:47:54,240 --> 00:47:56,559
пробел, и они утверждают, что это легче

1417
00:47:56,559 --> 00:47:59,200
сказать для человека, но также и для модели,

1418
00:47:59,200 --> 00:48:00,960
которая  эти последовательности похожи, и

1419
00:48:00,960 --> 00:48:02,480
они, вероятно, должны иметь такое же следующее

1420
00:48:02,480 --> 00:48:04,160
слово, даже если вы не знаете, какое

1421
00:48:04,160 --> 00:48:05,520
следующее слово является

1422
00:48:05,520 --> 00:48:07,440
эм, поэтому это предполагает, что вы знаете,

1423
00:48:07,440 --> 00:48:09,040
что легче узнать эти сходства,

1424
00:48:09,040 --> 00:48:10,640
чем на самом деле предсказать следующее

1425
00:48:10,640 --> 00:48:12,319
слово,

1426
00:48:12,319 --> 00:48:13,599
и они спорят  что это еще более

1427
00:48:13,599 --> 00:48:15,200
верно для шаблонов с длинным хвостом, где

1428
00:48:15,200 --> 00:48:16,640
для модели очень сложно

1429
00:48:16,640 --> 00:48:18,319
предсказать, что следующее слово является каким-то

1430
00:48:18,319 --> 00:48:20,800
редко встречающимся токеном или редкой сущностью,

1431
00:48:20,800 --> 00:48:22,640
чем найти другое подобное

1432
00:48:22,640 --> 00:48:24,720
последовательность, которую он уже видел, и просто

1433
00:48:24,720 --> 00:48:28,319
скопируйте следующее слово из этой последовательности,

1434
00:48:28,319 --> 00:48:30,079
поэтому они предлагают сохранить все

1435
00:48:30,079 --> 00:48:32,000
представления текстовых последовательностей в

1436
00:48:32,000 --> 00:48:33,680
хранилище данных ближайшего соседа,

1437
00:48:33,680 --> 00:48:35,440
а затем, при заключении,

1438
00:48:35,440 --> 00:48:36,800
вы захотите найти k

1439
00:48:36,800 --> 00:48:39,040
наиболее  подобная последовательность, как текст,

1440
00:48:39,040 --> 00:48:40,400
вы затем извлекаете соответствующие

1441
00:48:40,400 --> 00:48:41,680
значения, чтобы вы просто смотрели на эти

1442
00:48:41,680 --> 00:48:43,200
последовательности и видели, какими были их следующие

1443
00:48:43,200 --> 00:48:44,960
слова,

1444
00:48:44,960 --> 00:48:46,880
а затем вы объединяете вероятность

1445
00:48:46,880 --> 00:48:47,920
из этого

1446
00:48:47,920 --> 00:48:49,760
хранилища данных ближайшего соседа с

1447
00:48:49,760 --> 00:48:52,079
обычным прогнозом языковой модели, и поэтому

1448
00:48:52,079 --> 00:48:53,680
они называют это  шаг интерполяции в

1449
00:48:53,680 --> 00:48:55,760
том, что они ждут, сколько обратить

1450
00:48:55,760 --> 00:48:57,760
внимание на вероятность этого

1451
00:48:57,760 --> 00:48:59,599
подхода kn и сколько обратить внимание

1452
00:48:59,599 --> 00:49:02,160
на этот подход к языковой модели,

1453
00:49:02,160 --> 00:49:03,920
а лямбда здесь - это просто

1454
00:49:03,920 --> 00:49:07,040
гиперпараметр, который они настраивают,

1455
00:49:07,760 --> 00:49:08,960
поэтому у них есть эта диаграмма из своей

1456
00:49:08,960 --> 00:49:10,559
статьи, где  они хотят предсказать

1457
00:49:10,559 --> 00:49:12,640
следующее слово в последовательности

1458
00:49:12,640 --> 00:49:14,319
пьесы Шекспира,

1459
00:49:14,319 --> 00:49:15,760
поэтому они делают то, что у них есть все

1460
00:49:15,760 --> 00:49:17,520
обучающие контакты, уже закодированные в

1461
00:49:17,520 --> 00:49:18,960
t  хранилище данных, так что у них есть

1462
00:49:18,960 --> 00:49:20,480
представления всех обучающих

1463
00:49:20,480 --> 00:49:21,599
контактов,

1464
00:49:21,599 --> 00:49:23,040
а затем они вычисляют

1465
00:49:23,040 --> 00:49:24,880
представление своего текстового контекста, и они хотят

1466
00:49:24,880 --> 00:49:26,160
выяснить,

1467
00:49:26,160 --> 00:49:27,520
какие представления в обучающем

1468
00:49:27,520 --> 00:49:29,440
контексте больше всего похожи на это

1469
00:49:29,440 --> 00:49:32,559
представление контекста текстового теста,

1470
00:49:32,559 --> 00:49:34,559
и поэтому здесь, во внешней памяти

1471
00:49:34,559 --> 00:49:36,880
вид вещей, ключи будут

1472
00:49:36,880 --> 00:49:39,040
представлениями обучающего контекста,

1473
00:49:39,040 --> 00:49:42,480
а значения будут следующими словами,

1474
00:49:42,480 --> 00:49:44,400
поэтому для получения k ближайших

1475
00:49:44,400 --> 00:49:47,200
обучающих представлений они затем копируют

1476
00:49:47,200 --> 00:49:48,720
свои значения, так что то, что вы видите в

1477
00:49:48,720 --> 00:49:51,440
этом примере Macbeth Hamlet Macbeth, у

1478
00:49:51,440 --> 00:49:52,960
них есть  шаг нормализации, на котором они

1479
00:49:52,960 --> 00:49:55,280
преобразуют это в вероятностное пространство

1480
00:49:55,280 --> 00:49:56,880
um, а затем, наконец, у них есть

1481
00:49:56,880 --> 00:49:59,119
шаг агрегирования, поэтому, если

1482
00:49:59,119 --> 00:50:00,880
слово рассматривается как следующее слово и

1483
00:50:00,880 --> 00:50:02,960
несколько из этих uh k ближайших соседей,

1484
00:50:02,960 --> 00:50:04,559
тогда они хотят подсчитать больше для

1485
00:50:04,559 --> 00:50:06,480
этого, поэтому они объединяют, скажем,  c Макбет

1486
00:50:06,480 --> 00:50:08,960
дважды, это означает, что макбет более вероятен,

1487
00:50:10,000 --> 00:50:11,119
а затем, наконец, у них есть этот

1488
00:50:11,119 --> 00:50:13,119
шаг интерполяции, на котором они пытаются

1489
00:50:13,119 --> 00:50:14,960
уравновесить  ce между

1490
00:50:14,960 --> 00:50:16,880
вероятностями классификации из языковой модели

1491
00:50:16,880 --> 00:50:18,480
и из подхода kn,

1492
00:50:20,720 --> 00:50:22,800
так что некоторые немедленные наблюдения, которые вы могли бы

1493
00:50:22,800 --> 00:50:25,359
иметь, это кажется действительно дорогим,

1494
00:50:25,359 --> 00:50:27,599
они действительно предлагают способы

1495
00:50:27,599 --> 00:50:29,599
попытаться свести к минимуму затраты на

1496
00:50:29,599 --> 00:50:30,720
фактическое хранение всех

1497
00:50:30,720 --> 00:50:32,480
обучающих контактов в этих данных  store,

1498
00:50:32,480 --> 00:50:34,400
потому что они фактически хранят его для каждого

1499
00:50:34,400 --> 00:50:36,559
отдельного окна следующего слова в

1500
00:50:36,559 --> 00:50:38,319
контексте обучения,

1501
00:50:38,319 --> 00:50:40,079
и вы можете выполнить квантование для некоторых

1502
00:50:40,079 --> 00:50:41,920
подходов к ближайшему соседу, чтобы попытаться

1503
00:50:41,920 --> 00:50:43,760
сделать это менее затратным,

1504
00:50:43,760 --> 00:50:45,359
но я предполагаю, что это все равно будет довольно

1505
00:50:45,359 --> 00:50:47,040
дорого для действительно больших наборов обучающих данных,

1506
00:50:47,040 --> 00:50:48,559
которые

1507
00:50:48,559 --> 00:50:50,000
они  также есть несколько интересных экспериментов,

1508
00:50:50,000 --> 00:50:51,359
которые показывают, что это очень хорошо для

1509
00:50:51,359 --> 00:50:53,599
адаптации домена, поэтому, если вы возьмете свою

1510
00:50:53,599 --> 00:50:55,839
языковую модель и у вас есть новый домен, к

1511
00:50:55,839 --> 00:50:56,880
которому вы хотите применить свою языковую

1512
00:50:56,880 --> 00:50:59,520
модель, вы можете просто создать

1513
00:50:59,520 --> 00:51:02,240
хранилище данных ближайшего соседа вашего нового домена

1514
00:51:02,240 --> 00:51:03,040
поэтому вы

1515
00:51:03,040 --> 00:51:04,559
кодируете все представления этого

1516
00:51:04,559 --> 00:51:07,119
нового домена, которые вы вставляете в хранилище данных,

1517
00:51:07,119 --> 00:51:09,440
а затем вы можете просто использовать свой LAN  Модель guage

1518
00:51:10,559 --> 00:51:12,559
с этими вероятностями kn также

1519
00:51:12,559 --> 00:51:14,800
сразу в этом новом домене

1520
00:51:14,800 --> 00:51:16,240
без необходимости дальнейшего обучения

1521
00:51:16,240 --> 00:51:17,920
вашей языковой модели,

1522
00:51:17,920 --> 00:51:20,319
поэтому я подумал, что это довольно крутой

1523
00:51:20,319 --> 00:51:21,839
вариант использования этого подхода с внешней памятью,

1524
00:51:23,280 --> 00:51:25,040
поэтому, хотя он не использует

1525
00:51:25,040 --> 00:51:27,119
базы знаний напрямую, он делает  иметь это

1526
00:51:27,119 --> 00:51:29,920
слабое знание или нечеткое представление о кодировании

1527
00:51:29,920 --> 00:51:31,440
знаний, которые находятся в форме текстового

1528
00:51:31,440 --> 00:51:33,440
представления, в некоторую внешнюю

1529
00:51:33,440 --> 00:51:35,119
память, которую модель может затем использовать в

1530
00:51:35,119 --> 00:51:37,839
своих интересах,

1531
00:51:39,760 --> 00:51:41,680
это все, что у меня есть для этого подхода,

1532
00:51:41,680 --> 00:51:44,640
есть ли какие-либо вопросы по этому

1533
00:51:48,839 --> 00:51:50,880
подходу,

1534
00:51:50,880 --> 00:51:53,280
так что только один человек  спрашивает, как

1535
00:51:53,280 --> 00:51:55,359
kn делает прогнозы для следующего

1536
00:51:55,359 --> 00:51:56,400
слова,

1537
00:51:56,400 --> 00:51:58,240
k соседей для контекста,

1538
00:51:58,240 --> 00:52:00,720
а не для следующего слова

1539
00:52:00,720 --> 00:52:03,760
о, хорошо, это было не ясно, так что ключи

1540
00:52:03,760 --> 00:52:05,680
являются представлениями контекста,

1541
00:52:05,680 --> 00:52:07,680
значения в вашей внешней памяти

1542
00:52:07,680 --> 00:52:10,079
являются следующими  слова, поэтому, когда вы выясняете,

1543
00:52:10,079 --> 00:52:11,520
вы выясняете своих ближайших соседей,

1544
00:52:11,520 --> 00:52:13,280
используя свои ключи, а затем копируете

1545
00:52:13,280 --> 00:52:15,040
их значения, чтобы он действительно знал,

1546
00:52:15,040 --> 00:52:16,079
что  следующие слова предназначены для каждого из

1547
00:52:16,079 --> 00:52:19,240
этих представлений,

1548
00:52:23,440 --> 00:52:24,400
хорошо,

1549
00:52:24,400 --> 00:52:26,880
итак, наконец, мы собираемся поговорить о

1550
00:52:26,880 --> 00:52:28,400
том, как вы можете просто изменить данные обучения,

1551
00:52:28,400 --> 00:52:30,000
чтобы лучше кодировать знания и

1552
00:52:30,000 --> 00:52:32,000
языковые модели,

1553
00:52:32,000 --> 00:52:33,599
поэтому подходы, о которых мы говорили до сих пор,

1554
00:52:33,599 --> 00:52:35,839
фактически включают знания

1555
00:52:35,839 --> 00:52:37,839
явно с помощью  либо предварительно обученные

1556
00:52:37,839 --> 00:52:40,559
вложения, либо внешняя память,

1557
00:52:40,559 --> 00:52:42,160
мы также хотим поговорить о том, как вы можете

1558
00:52:42,160 --> 00:52:44,480
просто неявно включить знания

1559
00:52:44,480 --> 00:52:47,839
через неструктурированный текст,

1560
00:52:48,000 --> 00:52:49,440
поэтому мы собираемся либо замаскировать,

1561
00:52:49,440 --> 00:52:51,119
либо повредить данные, чтобы ввести

1562
00:52:51,119 --> 00:52:52,960
дополнительные обучающие задачи, требующие

1563
00:52:52,960 --> 00:52:54,960
фактических знаний  чтобы

1564
00:52:54,960 --> 00:52:56,240
выяснить, какие данные были замаскированы,

1565
00:52:56,240 --> 00:52:57,760
например,

1566
00:52:57,760 --> 00:52:59,599
поэтому у этого есть некоторые очевидные преимущества: у него

1567
00:52:59,599 --> 00:53:00,960
нет дополнительной памяти или

1568
00:53:00,960 --> 00:53:02,720
требований к вычислениям, у вас нет

1569
00:53:02,720 --> 00:53:04,640
хранилища данных, с которым можно было бы иметь дело, у вас нет

1570
00:53:04,640 --> 00:53:06,720
дополнительных слоев кодировщика знаний для обучения

1571
00:53:06,720 --> 00:53:09,760
всех вас  do - это изменение данных обучения,

1572
00:53:09,760 --> 00:53:10,800
и вам также не нужно изменять свою

1573
00:53:10,800 --> 00:53:12,400
архитектуру, чтобы вы могли продолжать

1574
00:53:12,400 --> 00:53:14,800
использовать свою любимую модель Bert и просто

1575
00:53:14,800 --> 00:53:18,240
сделать  эти изменения в данных обучения,

1576
00:53:18,240 --> 00:53:19,440
поэтому первая работа, которую мы собираемся рассмотреть,

1577
00:53:19,440 --> 00:53:22,000
называется

1578
00:53:22,000 --> 00:53:23,760
языковая модель предварительного обучения с еженедельным контролируемым знанием wklm или

1579
00:53:23,760 --> 00:53:25,359
предварительно обученная языковая модель,

1580
00:53:25,359 --> 00:53:27,280
и ключевая идея здесь состоит в том, чтобы научить

1581
00:53:27,280 --> 00:53:28,960
модель различать истинное и

1582
00:53:28,960 --> 00:53:30,960
ложное  знания,

1583
00:53:30,960 --> 00:53:32,720
поэтому они собираются испортить данные,

1584
00:53:32,720 --> 00:53:34,400
заменив упоминания в тексте

1585
00:53:34,400 --> 00:53:35,599
упоминаниями, которые относятся к различным

1586
00:53:35,599 --> 00:53:38,240
объектам одного и того же типа, чтобы создать то, что

1587
00:53:38,240 --> 00:53:39,680
они называют отрицательными заявлениями о знаниях,

1588
00:53:41,680 --> 00:53:43,440
а затем модель просто предскажет, был

1589
00:53:43,440 --> 00:53:47,440
ли объект заменен или  повреждено

1590
00:53:47,440 --> 00:53:49,040
ограничение этого типа необходимо, чтобы

1591
00:53:49,040 --> 00:53:50,240
убедиться, что

1592
00:53:50,240 --> 00:53:52,079
или побудить модель действительно

1593
00:53:52,079 --> 00:53:53,520
использовать фактические знания, чтобы выяснить, имеет ли

1594
00:53:53,520 --> 00:53:55,440
место это повреждение, чтобы вы

1595
00:53:55,440 --> 00:53:56,880
могли представить, если вы замените его

1596
00:53:56,880 --> 00:53:58,319
чем-то, что совсем нереально,

1597
00:53:58,319 --> 00:53:59,680
модель могла бы

1598
00:53:59,680 --> 00:54:01,359
просто базироваться  его предсказание основано на том, является ли

1599
00:54:01,359 --> 00:54:04,400
это предложение лингвистически правильным,

1600
00:54:04,400 --> 00:54:06,079
так что в качестве примера у нас есть истинное

1601
00:54:06,079 --> 00:54:07,839
утверждение знания, поскольку

1602
00:54:07,839 --> 00:54:11,680
jk rowling является автором harry  potter,

1603
00:54:11,680 --> 00:54:13,200
а затем мы хотим изменить это, чтобы

1604
00:54:13,200 --> 00:54:14,960
заменить его другим автором, поэтому, допустим,

1605
00:54:14,960 --> 00:54:17,440
мы изменим это на jr tolkien как

1606
00:54:17,440 --> 00:54:19,520
автора книги о Гарри Поттере,

1607
00:54:19,520 --> 00:54:21,359
чтобы вы могли видеть, что это требует некоторого

1608
00:54:21,359 --> 00:54:23,040
количества

1609
00:54:23,040 --> 00:54:24,079
фоновых знаний, чтобы на самом деле иметь

1610
00:54:24,079 --> 00:54:25,920
возможность выяснить, какой  утверждение истинно, а

1611
00:54:25,920 --> 00:54:27,839
какое утверждение ложно. Идея состоит в

1612
00:54:27,839 --> 00:54:29,520
том, что модель сможет предсказать

1613
00:54:29,520 --> 00:54:31,200
для каждого из этих упоминаний,

1614
00:54:31,200 --> 00:54:35,200
истинное ли это или ложное упоминание,

1615
00:54:36,559 --> 00:54:38,400
поэтому эта диаграмма взята из статьи

1616
00:54:38,400 --> 00:54:40,079
и, надеюсь, объясняет это немного лучше, у

1617
00:54:40,079 --> 00:54:41,599
них есть исходная статья

1618
00:54:41,599 --> 00:54:43,200
слева, а затем у них есть замененная

1619
00:54:43,200 --> 00:54:44,400
статья с повреждениями

1620
00:54:44,400 --> 00:54:47,200
справа, а сущности отмечены синим цветом,

1621
00:54:47,200 --> 00:54:49,440
поэтому они сначала ищут для данной сущности

1622
00:54:49,440 --> 00:54:51,440
ее тип, находят

1623
00:54:51,440 --> 00:54:53,520
другие сущности этого типа,

1624
00:54:53,520 --> 00:54:56,400
а затем случайным образом выбирают  сущность

1625
00:54:56,400 --> 00:54:58,480
и получить псевдоним для замены в

1626
00:54:58,480 --> 00:55:00,400
тексте, чтобы они собирались играть в Стэнли,

1627
00:55:00,400 --> 00:55:02,559
например, с Брайаном Джонсоном и

1628
00:55:02,559 --> 00:55:04,960
комиксами Marvel с комиксами DC, и их

1629
00:55:04,960 --> 00:55:08,079
размещение  красным цветом справа,

1630
00:55:08,079 --> 00:55:10,000
а затем идея состоит в том, что модель

1631
00:55:10,000 --> 00:55:11,119
сможет предсказать для каждого из этих

1632
00:55:11,119 --> 00:55:14,079
упоминаний гм, была ли она заменена или нет.

1633
00:55:14,079 --> 00:55:15,520
В случае с Брайаном Джонсоном у них есть

1634
00:55:15,520 --> 00:55:18,799
красный крестик, потому что это ложное упоминание и

1635
00:55:18,799 --> 00:55:20,000
в случае  Из истинных упоминаний у них

1636
00:55:20,000 --> 00:55:22,079
есть галочка,

1637
00:55:22,079 --> 00:55:23,920
так что это довольно простой подход, но

1638
00:55:23,920 --> 00:55:25,760
они на самом деле показывают, что это может помочь

1639
00:55:25,760 --> 00:55:26,960
модели

1640
00:55:26,960 --> 00:55:28,400
увеличить объем знаний,

1641
00:55:28,400 --> 00:55:31,960
закодированных в параметрах.

1642
00:55:35,680 --> 00:55:39,040
Хорошо, поэтому wklm использует потерю замены объекта,

1643
00:55:39,040 --> 00:55:40,559
чтобы обучить модель

1644
00:55:40,559 --> 00:55:42,319
различать эти  Истинные и ложные упоминания,

1645
00:55:42,319 --> 00:55:43,760
и это просто выглядит как

1646
00:55:43,760 --> 00:55:45,760
потеря двоичной классификации, где ваши истинные

1647
00:55:45,760 --> 00:55:47,920
упоминания находятся слева, а ваши ложные

1648
00:55:47,920 --> 00:55:49,599
упоминания - справа, и вы

1649
00:55:49,599 --> 00:55:52,480
хотите увеличить вероятность того, что это p

1650
00:55:52,480 --> 00:55:54,799
of e задано c, чтобы объект вероятности с

1651
00:55:54,799 --> 00:55:56,400
учетом контекста  вы хотите увеличить

1652
00:55:56,400 --> 00:55:58,400
это значение для истинных упоминаний и уменьшить

1653
00:55:58,400 --> 00:56:01,280
его для ложных упоминаний,

1654
00:56:01,280 --> 00:56:02,640
тогда общая потеря - это просто

1655
00:56:02,640 --> 00:56:04,319
комбинация потери модели массового языка

1656
00:56:04,319 --> 00:56:07,920
и потери замены этой сущности

1657
00:56:07,920 --> 00:56:10,240
В модификациях массового языка

1658
00:56:10,240 --> 00:56:12,799
потеря модели массового языка определяется на уровне токена,

1659
00:56:12,799 --> 00:56:14,640
а потеря замещения сущности

1660
00:56:14,640 --> 00:56:16,640
определяется на уровне сущности, что

1661
00:56:16,640 --> 00:56:18,559
означает, что это не только над подсловами,

1662
00:56:18,559 --> 00:56:20,480
но даже потенциально над словами, если у вас

1663
00:56:20,480 --> 00:56:21,359
есть,

1664
00:56:21,359 --> 00:56:24,880
например, фразы из нескольких слов,

1665
00:56:24,880 --> 00:56:26,640
и это  является важным моментом

1666
00:56:26,640 --> 00:56:28,799
или важной темой, которую мы действительно

1667
00:56:28,799 --> 00:56:30,079
видим в этих работах, которые

1668
00:56:30,079 --> 00:56:31,920
мы рассмотрим в том, что

1669
00:56:31,920 --> 00:56:34,000
изменение данных на уровне сущности

1670
00:56:34,000 --> 00:56:35,760
кажется важным компонентом

1671
00:56:35,760 --> 00:56:37,119
фактического увеличения объема

1672
00:56:37,119 --> 00:56:38,640
знаний, которые может кодировать языковая модель.

1673
00:56:42,640 --> 00:56:44,799
поэтому они обнаружили, что wklm лучше, чем

1674
00:56:44,799 --> 00:56:47,359
bert, а gpt2 на самом деле задачи завершения,

1675
00:56:47,359 --> 00:56:48,640
такие как заполнение пустых операторов,

1676
00:56:48,640 --> 00:56:50,559
которые мы рассматривали в начале,

1677
00:56:50,559 --> 00:56:52,480
они также обнаружили, что он

1678
00:56:52,480 --> 00:56:54,319
лучше, чем статья Эрни, о которой мы говорили о

1679
00:56:54,319 --> 00:56:56,079
последующей задаче,

1680
00:56:56,079 --> 00:56:57,280
и у них была  набор экспериментов по абляции, на

1681
00:56:57,280 --> 00:56:59,280
которые они смотрели, можете ли вы

1682
00:56:59,280 --> 00:57:00,720
просто удалить эту потерю модели массового языка

1683
00:57:00,720 --> 00:57:02,559
сейчас,

1684
00:57:02,559 --> 00:57:04,799
и если вы просто тренируете Берта подольше,

1685
00:57:04,799 --> 00:57:06,319
вам действительно нужно  d эта потеря замещения сущности,

1686
00:57:07,680 --> 00:57:09,040
так что вот на что смотрит таблица,

1687
00:57:09,040 --> 00:57:11,200
вторая строка смотрит, если мы

1688
00:57:11,200 --> 00:57:12,880
удалим модель массового языка, потеряли то, что

1689
00:57:12,880 --> 00:57:14,000
происходит,

1690
00:57:14,000 --> 00:57:15,520
мы видим, что она работает намного хуже

1691
00:57:15,520 --> 00:57:17,119
без потери модели массового языка, поэтому

1692
00:57:17,119 --> 00:57:19,280
вам действительно нужны обе потери  их

1693
00:57:19,280 --> 00:57:21,200
интуиция заключалась в том, что массовая

1694
00:57:21,200 --> 00:57:22,559
потеря модели языка

1695
00:57:22,559 --> 00:57:24,640
помогает кодировать только общее понимание языка,

1696
00:57:26,640 --> 00:57:28,720
а затем обучение Берта на более длительное время

1697
00:57:28,720 --> 00:57:30,160
работает намного хуже, чем использование

1698
00:57:30,160 --> 00:57:31,839
потери замещения его сущности, так что это

1699
00:57:31,839 --> 00:57:33,839
мотивирует еще больше, что вам

1700
00:57:33,839 --> 00:57:36,319
действительно нужно гм, или потеря замещения сущности

1701
00:57:36,319 --> 00:57:38,160
на самом деле  действительно помогает закодировать

1702
00:57:38,160 --> 00:57:41,839
больше знаний в этих языковых моделях,

1703
00:57:43,119 --> 00:57:44,960
поэтому, помимо искажения данных, которые

1704
00:57:44,960 --> 00:57:46,559
мы также собираемся рассмотреть, можем ли мы просто

1705
00:57:46,559 --> 00:57:48,160
замаскировать данные по-другому, можем ли мы быть более

1706
00:57:48,160 --> 00:57:50,480
умными в том, как мы делаем маскировку,

1707
00:57:50,480 --> 00:57:51,920
и это тема в нескольких недавних

1708
00:57:51,920 --> 00:57:53,200
работает,

1709
00:57:53,200 --> 00:57:54,640
поэтому на самом деле есть еще одна статья, называемая

1710
00:57:54,640 --> 00:57:56,319
Эрни, так что она отличается от той, о которой

1711
00:57:56,319 --> 00:57:58,160
мы говорили ранее, и это

1712
00:57:58,160 --> 00:57:59,599
улучшенное представление t  через

1713
00:57:59,599 --> 00:58:01,119
интеграцию знаний,

1714
00:58:01,119 --> 00:58:03,440
и то, что они делают, демонстрируют улучшения в

1715
00:58:03,440 --> 00:58:05,920
последующих китайских nlp-задачах, выполняя

1716
00:58:05,920 --> 00:58:08,480
маскирование на уровне фраз и на уровне сущности, поэтому

1717
00:58:08,480 --> 00:58:10,480
вместо того, чтобы просто маскировать подслова,

1718
00:58:10,480 --> 00:58:12,319
они собираются замаскировать фразы из

1719
00:58:12,319 --> 00:58:14,720
нескольких слов и сущностей,

1720
00:58:14,720 --> 00:58:16,000
полную фразу сущности  который

1721
00:58:16,000 --> 00:58:17,440
соответствует

1722
00:58:17,440 --> 00:58:18,880
некоторому объекту и тексту, который они могут

1723
00:58:18,880 --> 00:58:20,480
найти, например, с помощью подобных повествовательных техник,

1724
00:58:23,440 --> 00:58:25,119
а вторая работа - это

1725
00:58:25,119 --> 00:58:26,319
то, о чем вы слышали в последней

1726
00:58:26,319 --> 00:58:28,480
лекции, а именно об идее использования

1727
00:58:28,480 --> 00:58:31,359
маскировки выступающего промежутка для маскировки выступающих

1728
00:58:31,359 --> 00:58:33,839
промежутков и промежутка солевого раствора.  - это просто именованная

1729
00:58:33,839 --> 00:58:35,920
сущность или дата, поэтому вы можете видеть, что это

1730
00:58:35,920 --> 00:58:37,920
очень похоже на то, что делает Эрни,

1731
00:58:37,920 --> 00:58:39,760
и они обнаружили, что использование маскировки значимого диапазона на

1732
00:58:39,760 --> 00:58:42,240
самом деле значительно улучшило

1733
00:58:42,240 --> 00:58:44,079
производительность t5 в этих задачах с ответами на вопросы закрытого домена,

1734
00:58:48,160 --> 00:58:49,359
поэтому просто чтобы убедиться, что мы все  на той

1735
00:58:49,359 --> 00:58:50,960
же странице с различными

1736
00:58:50,960 --> 00:58:52,799
методами маскировки эта диаграмма из

1737
00:58:52,799 --> 00:58:54,880
статьи Эрни сравнивает с тем, что делает

1738
00:58:54,880 --> 00:58:57,040
Берт, и с тем, что делает Эрни.  op показывает,

1739
00:58:57,040 --> 00:58:59,680
что Эрни замаскировал жетоны

1740
00:58:59,680 --> 00:59:01,040
подслов или этот

1741
00:59:01,040 --> 00:59:03,680
Берт собрал жетоны метро, в то время как Эрни собрал фразы,

1742
00:59:03,680 --> 00:59:05,839
такие как ряд, а также такие сущности,

1743
00:59:05,839 --> 00:59:08,799
как jk rowling,

1744
00:59:10,799 --> 00:59:12,319
есть некоторые интересные результаты,

1745
00:59:12,319 --> 00:59:14,960
показывающие, что маскирование заметного промежутка

1746
00:59:14,960 --> 00:59:17,119
помогает кодировать больше знаний в этих

1747
00:59:17,119 --> 00:59:18,480
представления,

1748
00:59:18,480 --> 00:59:20,000
так что слева мы смотрим на

1749
00:59:20,000 --> 00:59:21,520
результаты исходной статьи, в которой

1750
00:59:21,520 --> 00:59:23,760
предлагалось маскирование заметного песчаного выступа,

1751
00:59:23,760 --> 00:59:27,119
так что это основная работа,

1752
00:59:27,119 --> 00:59:29,040
и идея здесь заключалась в том, что они

1753
00:59:29,040 --> 00:59:30,799
обучали извлекающего знания, так что на

1754
00:59:30,799 --> 00:59:33,040
самом деле это скорее класс внешней памяти

1755
00:59:33,040 --> 00:59:34,799
техник, но они обнаружили, что,

1756
00:59:34,799 --> 00:59:36,319
используя технику маскирования заметного промежутка

1757
00:59:36,319 --> 00:59:38,240
,

1758
00:59:38,240 --> 00:59:39,520
они могут на самом деле обучить гораздо более качественный инструмент

1759
00:59:39,520 --> 00:59:41,440
извлечения знаний, так что это хороший

1760
00:59:41,440 --> 00:59:43,680
пример того,

1761
00:59:44,799 --> 00:59:46,720
как эти методы действительно дополняют друг друга.

1762
00:59:49,920 --> 00:59:52,000
вместе,

1763
00:59:52,000 --> 00:59:53,599
и они обнаружили, что маскирование тишины вентилятора

1764
00:59:53,599 --> 00:59:55,520
по сравнению с использованием маскировки из

1765
00:59:55,520 --> 00:59:57,599
б  Это были бы случайные равномерные

1766
00:59:57,599 --> 00:59:59,359
маски или

1767
00:59:59,359 --> 01:00:01,440
случайное маскирование промежутков из бумаги

1768
01:00:01,440 --> 01:00:04,799
под названием spanbert um, он работает намного

1769
01:00:04,799 --> 01:00:06,799
лучше, маскируя полосу заметности, так что

1770
01:00:06,799 --> 01:00:08,559
вы видите, как

1771
01:00:08,559 --> 01:00:10,880
точное совпадение 38 баллов по сравнению, например, 32

1772
01:00:10,880 --> 01:00:13,440
балла точного совпадения

1773
01:00:13,440 --> 01:00:15,920
и на  Итак, у нас есть результаты

1774
01:00:15,920 --> 01:00:19,040
тонкой настройки t5 либо с

1775
01:00:19,040 --> 01:00:21,200
маскированием потолочного диапазона, либо с задачей искажения диапазона,

1776
01:00:21,200 --> 01:00:23,040
которую вы видели в задании 5, и вы

1777
01:00:23,040 --> 01:00:24,880
можете видеть, что на этих различных наборах данных qa

1778
01:00:24,880 --> 01:00:26,559
маскирование научного диапазона работает

1779
01:00:26,559 --> 01:00:28,400
значительно лучше, чем просто использование

1780
01:00:29,760 --> 01:00:31,680
техники искажения диапазона,

1781
01:00:31,680 --> 01:00:34,000
поэтому  это действительно говорит о том, что

1782
01:00:34,000 --> 01:00:35,839
маскирование заметных промежутков и

1783
01:00:35,839 --> 01:00:37,440
маскирование этих заметных промежутков этих

1784
01:00:37,440 --> 01:00:39,680
сущностей на самом деле помогает закодировать

1785
01:00:39,680 --> 01:00:44,319
больше знаний в этих языковых моделях,

1786
01:00:46,240 --> 01:00:47,680
поэтому, резюмируя, мы говорили о трех

1787
01:00:47,680 --> 01:00:49,280
разных классах методов для добавления

1788
01:00:49,280 --> 01:00:51,680
знаний в языковые модели, о которых

1789
01:00:51,680 --> 01:00:53,040
мы говорили.  с использованием предварительно обученных

1790
01:00:53,040 --> 01:00:55,040
встраиваний сущностей их было

1791
01:00:55,040 --> 01:00:57,599
несложно применить к существующим архитектурам и

1792
01:00:57,599 --> 01:00:58,960
как способ использовать это

1793
01:00:58,960 --> 01:01:00,720
предварительное обучение графа знаний,

1794
01:01:00,720 --> 01:01:02,000
но это довольно косвенный способ

1795
01:01:02,000 --> 01:01:03,680
включения знаний, и его может быть

1796
01:01:03,680 --> 01:01:06,079
трудно интерпретировать,

1797
01:01:06,079 --> 01:01:07,520
мы также говорили о подходах к

1798
01:01:07,520 --> 01:01:09,839
добавлению внешней памяти,

1799
01:01:09,839 --> 01:01:11,839
которая может поддерживать изменение

1800
01:01:11,839 --> 01:01:13,680
базы знаний, ее также было легче

1801
01:01:13,680 --> 01:01:15,200
интерпретировать,

1802
01:01:15,200 --> 01:01:16,799
но они, как правило, были более сложными.  сложны в

1803
01:01:16,799 --> 01:01:19,440
реализации, как мы видели на kglm, и

1804
01:01:19,440 --> 01:01:21,359
им также требовалось больше памяти, как мы

1805
01:01:21,359 --> 01:01:24,400
видели с подходом k nlm,

1806
01:01:24,400 --> 01:01:25,920
а затем, наконец, мы говорим об

1807
01:01:25,920 --> 01:01:27,760
изменении обучающих данных,

1808
01:01:27,760 --> 01:01:29,760
поэтому это не требует изменений модели или

1809
01:01:29,760 --> 01:01:31,920
дополнительных вычислений, это также может

1810
01:01:31,920 --> 01:01:34,000
быть проще всего теоретически проанализировать  так

1811
01:01:34,000 --> 01:01:35,440
что на самом деле сейчас это активное исследование,

1812
01:01:37,359 --> 01:01:39,359
но все еще остается открытым вопрос,

1813
01:01:39,359 --> 01:01:41,359
всегда ли изменение обучающих данных так же эффективно,

1814
01:01:41,359 --> 01:01:44,160
как и изменение модели, и каковы

1815
01:01:44,160 --> 01:01:45,839
компромиссы с точки зрения количества требуемых данных по

1816
01:01:45,839 --> 01:01:47,760
сравнению с использованием одного из этих других

1817
01:01:47,760 --> 01:01:51,559
подходов к расширению знаний,

1818
01:01:52,480 --> 01:01:55,039
поэтому  Это приводит нас к третьему разделу, так что

1819
01:01:55,039 --> 01:01:58,880
я думаю, я снова сделаю паузу, чтобы ответить на вопросы,

1820
01:02:02,880 --> 01:02:04,880
я думаю, мы можем быть хорошими,

1821
01:02:04,880 --> 01:02:07,839
круто, хорошо  Итак, в третьем разделе

1822
01:02:07,839 --> 01:02:09,359
рассказывается о том, как исследователи на самом деле

1823
01:02:09,359 --> 01:02:10,960
собираются оценивать знания и

1824
01:02:10,960 --> 01:02:13,680
языковые модели, и я думаю, как некоторые

1825
01:02:13,680 --> 01:02:14,799
из техник, о которых мы на самом деле только что

1826
01:02:14,799 --> 01:02:17,680
говорили, используются в этой оценке,

1827
01:02:17,680 --> 01:02:19,039
поэтому сначала мы поговорим о

1828
01:02:19,039 --> 01:02:20,400
пробах, которые

1829
01:02:20,400 --> 01:02:22,160
не работают.  Это требует какой-либо тонкой настройки

1830
01:02:22,160 --> 01:02:23,359
языковой модели, а затем мы собираемся

1831
01:02:23,359 --> 01:02:25,359
поговорить о последующих задачах, которые посмотрят,

1832
01:02:25,359 --> 01:02:27,280
насколько хорошо эти предварительно обученные

1833
01:02:27,280 --> 01:02:29,280
представления действительно передают свои

1834
01:02:29,280 --> 01:02:32,400
знания другим задачам,

1835
01:02:32,480 --> 01:02:34,480
поэтому одна из первых работ в этой области

1836
01:02:34,480 --> 01:02:37,280
была названа llama  и это действительно положило

1837
01:02:37,280 --> 01:02:39,440
начало серии работ по изучению того, сколько

1838
01:02:39,440 --> 01:02:41,520
знаний уже закодировано в этих

1839
01:02:41,520 --> 01:02:43,440
языковых моделях,

1840
01:02:43,440 --> 01:02:44,880
поэтому их вопрос заключался в том, сколько

1841
01:02:44,880 --> 01:02:46,799
реляционного здравого смысла и фактических

1842
01:02:46,799 --> 01:02:48,559
знаний содержится в готовых языковых

1843
01:02:48,559 --> 01:02:50,400
моделях, таких как использование предварительно обученных

1844
01:02:50,400 --> 01:02:52,000
языковых моделей  и оценка

1845
01:02:52,000 --> 01:02:54,079
знаний в них,

1846
01:02:54,079 --> 01:02:55,280
и это без какого-либо дополнительного

1847
01:02:55,280 --> 01:02:57,440
обучения или тонкой настройки,

1848
01:02:57,440 --> 01:02:59,119
поэтому они в основном построили набор того, что

1849
01:02:59,119 --> 01:03:00,960
они называют закрытым персоналом.  Это

1850
01:03:00,960 --> 01:03:02,559
просто заполнение пустых

1851
01:03:02,559 --> 01:03:04,319
утверждений, которые мы фактически использовали

1852
01:03:04,319 --> 01:03:05,760
в начале выступления, у нас есть еще

1853
01:03:05,760 --> 01:03:08,720
несколько примеров,

1854
01:03:10,640 --> 01:03:11,760
и они вручную создали эти

1855
01:03:11,760 --> 01:03:13,760
шаблоны закрытых утверждений, используя

1856
01:03:13,760 --> 01:03:15,119
тройки графов знаний и

1857
01:03:15,119 --> 01:03:19,039
пары с ответами на вопросы из существующих наборов данных, которые

1858
01:03:19,039 --> 01:03:20,640
они  хотел сравнить предварительно обученные

1859
01:03:20,640 --> 01:03:22,960
языковые модели для надзора за

1860
01:03:22,960 --> 01:03:24,400
извлечением отношений и системами ответов на вопросы,

1861
01:03:24,400 --> 01:03:26,559
чтобы увидеть, как эти языковые

1862
01:03:26,559 --> 01:03:28,000
модели, которые были обучены

1863
01:03:28,000 --> 01:03:30,480
неконтролируемым образом, по сравнению с этими

1864
01:03:30,480 --> 01:03:32,480
базовыми системами, которые не только

1865
01:03:32,480 --> 01:03:34,160
контролируются, но и действительно предназначены для этой

1866
01:03:34,160 --> 01:03:37,359
задачи извлечения знаний

1867
01:03:37,359 --> 01:03:38,720
и  их цель состояла в том, чтобы оценить

1868
01:03:38,720 --> 01:03:40,400
знания в существующих предварительно обученных

1869
01:03:40,400 --> 01:03:42,640
языковых моделях, и ключевой момент в

1870
01:03:42,640 --> 01:03:44,640
этом заключается в том, что они просто используют

1871
01:03:44,640 --> 01:03:46,400
языковые модели, поскольку они доступны

1872
01:03:46,400 --> 01:03:48,480
исследователям, поэтому это означает, что могут быть

1873
01:03:48,480 --> 01:03:50,000
различия в корпусах предварительного обучения

1874
01:03:50,000 --> 01:03:51,200
для  пример,

1875
01:03:51,200 --> 01:03:52,480
поэтому, когда вы смотрите на следующую таблицу

1876
01:03:52,480 --> 01:03:54,000
и сравниваете языковые модели,

1877
01:03:54,000 --> 01:03:55,920
также k  Имейте в виду, что

1878
01:03:55,920 --> 01:03:57,200
они не учитывают

1879
01:03:57,200 --> 01:04:00,559
различия в предварительно обученных корпусах,

1880
01:04:00,559 --> 01:04:01,839
поэтому многие из этих языковых моделей,

1881
01:04:01,839 --> 01:04:03,200
вероятно, покажутся вам знакомыми либо

1882
01:04:03,200 --> 01:04:04,160
из

1883
01:04:04,160 --> 01:04:05,760
предыдущих лекций, либо, возможно, из ваших последних

1884
01:04:05,760 --> 01:04:07,200
проектов,

1885
01:04:07,200 --> 01:04:09,760
и мы видим общую

1886
01:04:09,760 --> 01:04:11,200
базу и  Большие предварительно обученные

1887
01:04:11,200 --> 01:04:14,480
модели bert работают намного лучше,

1888
01:04:14,480 --> 01:04:15,839
чем предыдущий язык или другие

1889
01:04:15,839 --> 01:04:17,200
языковые модели, здесь

1890
01:04:17,200 --> 01:04:18,720
я думаю, я забыл упомянуть, какая средняя

1891
01:04:18,720 --> 01:04:20,720
точность на одном из них - это довольно

1892
01:04:20,720 --> 01:04:23,599
простая метрика, идея в том, если вы посмотрите

1893
01:04:23,599 --> 01:04:24,640
на пустое поле и вы  посмотрите на верхние

1894
01:04:24,640 --> 01:04:26,559
прогнозы для верхнего прогноза

1895
01:04:26,559 --> 01:04:28,400
для бланка, правильно это или нет, так

1896
01:04:28,400 --> 01:04:30,240
что точность в 1 означает, что точность в

1897
01:04:30,240 --> 01:04:32,160
10 будет, давайте посмотрим на 10 лучших

1898
01:04:32,160 --> 01:04:34,000
прогнозов, это правильный прогноз

1899
01:04:34,000 --> 01:04:36,079
в верхних 10.

1900
01:04:36,079 --> 01:04:37,359
хм,

1901
01:04:37,359 --> 01:04:39,599
так что в дополнение к  bert large и base в

1902
01:04:39,599 --> 01:04:42,720
целом работают хорошо, мы действительно видим, что

1903
01:04:42,720 --> 01:04:44,640
в наборе данных t-rex

1904
01:04:44,640 --> 01:04:46,160
базовая линия извлечения отношений работает немного

1905
01:04:46,160 --> 01:04:48,559
лучше, чем bert,

1906
01:04:48,559 --> 01:04:50,000
одна вещь, которую они здесь замечают, что довольно

1907
01:04:50,000 --> 01:04:52,400
интересно, это то, что  в этом наборе данных есть

1908
01:04:52,400 --> 01:04:54,240
много различных типов отношений, и

1909
01:04:54,240 --> 01:04:56,240
отношения можно классифицировать с точки зрения того,

1910
01:04:56,240 --> 01:04:58,240
являются ли они взаимно-однозначными отношениями, являются ли

1911
01:04:58,240 --> 01:05:00,240
они сквозными отношениями, являются ли они

1912
01:05:00,240 --> 01:05:01,839
сквозными отношениями,

1913
01:05:01,839 --> 01:05:04,000
примером  однозначное отношение

1914
01:05:04,000 --> 01:05:06,559
будет вашим отношением идентификатора ученика, поэтому у вас

1915
01:05:06,559 --> 01:05:09,119
будет уникальный идентификатор ученика.

1916
01:05:09,119 --> 01:05:11,520
Примером сквозного отношения будет, если они

1917
01:05:11,520 --> 01:05:13,599
зарегистрированы в связи, поэтому есть много

1918
01:05:13,599 --> 01:05:15,520
учеников, зарегистрированных во множестве классов, так что

1919
01:05:15,520 --> 01:05:17,440
это будет  сквозное отношение,

1920
01:05:17,440 --> 01:05:18,880
и они обнаружили, что bert действительно борется

1921
01:05:18,880 --> 01:05:21,680
с этими сквозными отношениями,

1922
01:05:21,680 --> 01:05:22,960
поэтому, хотя он работает лучше, чем

1923
01:05:22,960 --> 01:05:24,720
базовая линия извлечения отношений для некоторых

1924
01:05:24,720 --> 01:05:26,960
типов отношений в целом, он

1925
01:05:26,960 --> 01:05:28,160
довольно ужасно справляется с этими сквозными

1926
01:05:28,160 --> 01:05:30,079
отношениями, так что в целом  он работает немного хуже,

1927
01:05:30,079 --> 01:05:33,599
чем базовый уровень на этом наборе данных trx, который

1928
01:05:33,599 --> 01:05:36,559
они также сравнивают с отрядом на docker qa,

1929
01:05:36,559 --> 01:05:38,799
и они обнаруживают, что он работает намного

1930
01:05:38,799 --> 01:05:40,720
хуже, они отмечают, что языковая модель

1931
01:05:40,720 --> 01:05:42,880
здесь не настроена, а также не имеет

1932
01:05:42,880 --> 01:05:44,480
доступа к  информационно-поисковая

1933
01:05:44,480 --> 01:05:45,440
система,

1934
01:05:45,440 --> 01:05:46,799
а затем они  посмотрите на точность в

1935
01:05:46,799 --> 01:05:48,799
10, они обнаруживают, что этот разрыв между

1936
01:05:48,799 --> 01:05:50,799
производительностью docker qa и bert на

1937
01:05:50,799 --> 01:05:52,880
самом деле немного

1938
01:05:52,880 --> 01:05:54,880
сокращается, что говорит о том, что эти языковые модели действительно

1939
01:05:54,880 --> 01:05:56,640
содержат некоторый объем знаний, закодированных в

1940
01:05:56,640 --> 01:05:59,119
них, и что они даже

1941
01:05:59,119 --> 01:06:00,720
конкурентоспособны с этими

1942
01:06:00,720 --> 01:06:05,000
контролируемыми базовыми уровнями извлечения знаний

1943
01:06:06,240 --> 01:06:07,760
так что вы также можете попробовать примеры в

1944
01:06:07,760 --> 01:06:10,480
их репозитории github для зондирования ламы,

1945
01:06:10,480 --> 01:06:11,920
у нас есть пример,

1946
01:06:11,920 --> 01:06:13,280
который был из их репо, где

1947
01:06:13,280 --> 01:06:14,799
кошка на маске,

1948
01:06:14,799 --> 01:06:16,640
вы можете увидеть, какие 10 лучших прогнозов

1949
01:06:17,520 --> 01:06:19,680
должны заполнить закрытый оператор здесь, который у

1950
01:06:19,680 --> 01:06:22,240
них есть  кот разговаривает по телефону,

1951
01:06:22,240 --> 01:06:23,839
так что это может быть интересный способ просто

1952
01:06:23,839 --> 01:06:26,559
выяснить, какие фактические знания здравого смысла заложены

1953
01:06:26,559 --> 01:06:28,799
в существующих языковых моделях, и его

1954
01:06:28,799 --> 01:06:29,920
довольно

1955
01:06:29,920 --> 01:06:33,280
легко использовать с этой интерактивной подсказкой,

1956
01:06:33,280 --> 01:06:35,440
поэтому некоторые ограничения ламы-зонда

1957
01:06:35,440 --> 01:06:36,960
заключаются в том, что это может быть сложно  чтобы понять,

1958
01:06:36,960 --> 01:06:40,240
почему модели работают хорошо, когда они это делают,

1959
01:06:40,240 --> 01:06:41,839
например, Берт может просто

1960
01:06:41,839 --> 01:06:43,599
предсказывать самый популярный токен, и

1961
01:06:43,599 --> 01:06:45,359
это оказывается правильным, может быть, это просто

1962
01:06:45,359 --> 01:06:47,200
память  zing и

1963
01:06:47,200 --> 01:06:49,440
не совсем понимает

1964
01:06:49,440 --> 01:06:50,799
формулировку знания и не

1965
01:06:50,799 --> 01:06:52,000
понимает,

1966
01:06:52,000 --> 01:06:54,400
что это за факт,

1967
01:06:54,400 --> 01:06:55,760
это также может быть просто идентификация

1968
01:06:55,760 --> 01:06:57,839
сходства между поверхностными

1969
01:06:57,839 --> 01:07:00,160
формами субъекта и объекта, так,

1970
01:07:00,160 --> 01:07:02,640
например, папа Климент vii имеет

1971
01:07:02,640 --> 01:07:04,160
пустую позицию

1972
01:07:04,160 --> 01:07:05,599
даже если вы ничего не знаете о

1973
01:07:05,599 --> 01:07:07,440
Папе Клименте VII, вы можете

1974
01:07:07,440 --> 01:07:09,839
понять, что Папа является вероятным следующим

1975
01:07:09,839 --> 01:07:11,200
словом

1976
01:07:11,200 --> 01:07:14,960
для этой эээ тройки или для этого шаблона,

1977
01:07:14,960 --> 01:07:16,880
поэтому проблема в том, что

1978
01:07:16,880 --> 01:07:18,559
модель просто делает эти прогнозы на

1979
01:07:18,559 --> 01:07:20,720
основе  эти поверхностные формы или паттерны совместного появления

1980
01:07:22,000 --> 01:07:23,359
трудно узнать для реальной

1981
01:07:23,359 --> 01:07:24,880
оценки знаний в модели,

1982
01:07:24,880 --> 01:07:26,160
возможно, они просто делают правильные

1983
01:07:26,160 --> 01:07:29,599
прогнозы по другим причинам,

1984
01:07:29,599 --> 01:07:30,880
и более тонкая проблема, которую мы

1985
01:07:30,880 --> 01:07:32,559
подняли, заключается в том, что языковые модели могут

1986
01:07:32,559 --> 01:07:34,160
быть просто чувствительны к  формулировка

1987
01:07:34,160 --> 01:07:35,280
утверждения

1988
01:07:35,280 --> 01:07:36,960
таким образом, чтобы для каждой

1989
01:07:36,960 --> 01:07:38,559
тройки в их наборе данных или для каждого

1990
01:07:38,559 --> 01:07:40,160
отношения их набора данных у них был только

1991
01:07:40,160 --> 01:07:42,240
один вручную определенный шаблон и

1992
01:07:42,240 --> 01:07:43,599
качественный  Они обнаружили, что если они

1993
01:07:43,599 --> 01:07:45,680
просто внесут небольшие изменения в шаблон, это

1994
01:07:45,680 --> 01:07:47,599
может фактически изменить, может ли

1995
01:07:47,599 --> 01:07:49,200
модель вспомнить правильный

1996
01:07:49,200 --> 01:07:51,200
прогноз или нет,

1997
01:07:51,200 --> 01:07:52,640
и поэтому это означает, что результаты проверки

1998
01:07:52,640 --> 01:07:54,400
действительно являются нижней границей

1999
01:07:54,400 --> 01:07:55,920
знаний, которые закодированы на языке.

2000
01:07:55,920 --> 01:07:57,760
модель,

2001
01:07:57,760 --> 01:07:59,119
поэтому, если вы измените формулировку,

2002
01:07:59,119 --> 01:08:01,200
возможно, что модель может показать, что

2003
01:08:01,200 --> 01:08:02,720
действительно имеет закодированные в ней знания,

2004
01:08:04,319 --> 01:08:05,680
поэтому следующие направления работы, о которых мы будем

2005
01:08:05,680 --> 01:08:08,000
говорить, действительно основаны на этих

2006
01:08:08,000 --> 01:08:09,920
двух ограничениях этого исходного зонда ламы,

2007
01:08:12,319 --> 01:08:14,400
поэтому первое  один называется лама ун или

2008
01:08:14,400 --> 01:08:17,279
бесполезными именами ламы, и ключевая идея

2009
01:08:17,279 --> 01:08:19,040
состоит в том, чтобы убрать эти примеры из ламы, на

2010
01:08:19,040 --> 01:08:20,158
которые можно ответить без

2011
01:08:20,158 --> 01:08:21,920
знания отношений, так что это своего рода

2012
01:08:21,920 --> 01:08:23,439
устранение первого ограничения на

2013
01:08:23,439 --> 01:08:25,439
последнем слайде,

2014
01:08:25,439 --> 01:08:27,120
поэтому они заметили, что Берт полагается на

2015
01:08:27,120 --> 01:08:29,040
поверхностные формы  сущности могут не

2016
01:08:29,040 --> 01:08:30,158
использовать знания, чтобы делать

2017
01:08:30,158 --> 01:08:32,158
эти прогнозы, включая

2018
01:08:32,158 --> 01:08:33,520
ситуацию совпадения строк, о которой мы говорили

2019
01:08:33,520 --> 01:08:35,359
с Папой,

2020
01:08:35,359 --> 01:08:36,560
это

2021
01:08:36,560 --> 01:08:37,920
тоже деа  Из-за проблемы с раскрытием имени человека,

2022
01:08:37,920 --> 01:08:39,520
которую вы видели в пятом задании,

2023
01:08:40,640 --> 01:08:42,080
здесь имя может быть

2024
01:08:42,080 --> 01:08:44,238
неправильным предшествующим для родного

2025
01:08:44,238 --> 01:08:46,158
языка кого-то, места его рождения, его

2026
01:08:46,158 --> 01:08:47,679
национальности, у

2027
01:08:47,679 --> 01:08:49,359
них есть этот пример из таблицы или

2028
01:08:49,359 --> 01:08:50,719
из бумаги, где они смотрят  на

2029
01:08:51,520 --> 01:08:53,759
имена разных людей или имена людей, а затем

2030
01:08:53,759 --> 01:08:55,198
они смотрят на предсказание рождений для своего

2031
01:08:55,198 --> 01:08:57,120
родного языка, и все это

2032
01:08:57,120 --> 01:08:59,359
франкоязычные актеры, и Burt просто предсказывает

2033
01:08:59,359 --> 01:09:01,520
очень предвзятые и стереотипные

2034
01:09:01,520 --> 01:09:04,560
языки для этих конкретных имен,

2035
01:09:04,560 --> 01:09:06,479
так что это действительно может работать в обоих направлениях, что может

2036
01:09:06,479 --> 01:09:08,000
привести Берта к  делать неверные прогнозы,

2037
01:09:08,000 --> 01:09:10,319
а иногда или в некоторых случаях, но это

2038
01:09:10,319 --> 01:09:12,560
также может сработать, чтобы сделать или позволить Берту

2039
01:09:12,560 --> 01:09:14,319
делать правильные прогнозы, даже если у него

2040
01:09:14,319 --> 01:09:16,319
нет фактических знаний об этих людях,

2041
01:09:16,319 --> 01:09:17,359
поэтому проблема, которую они пытаются

2042
01:09:17,359 --> 01:09:19,679
решить, заключается в том, знаем ли мы, что  Берт действительно

2043
01:09:19,679 --> 01:09:21,759
знает этот факт, или он просто использует

2044
01:09:21,759 --> 01:09:24,319
некоторую предвзятость, чтобы сделать свой прогноз,

2045
01:09:24,319 --> 01:09:25,439
поэтому они вводят

2046
01:09:25,439 --> 01:09:27,120
пару эвристик, чтобы просто

2047
01:09:27,120 --> 01:09:29,359
отфильтровать эти примеры из

2048
01:09:29,359 --> 01:09:31,439
зонда ламы, которые могут быть решены либо

2049
01:09:31,439 --> 01:09:32,719
установкой соответствия строк,

2050
01:09:32,719 --> 01:09:35,040
либо установкой имени наблюдателя,

2051
01:09:35,040 --> 01:09:36,560
так что они усложняют подмножество

2052
01:09:36,560 --> 01:09:39,439
набора данных ламы, по сути,

2053
01:09:39,439 --> 01:09:41,040
они обнаруживают, что при тестировании берта на

2054
01:09:41,040 --> 01:09:43,040
этом более сложном подмножестве  производительность

2055
01:09:43,040 --> 01:09:44,960
падает примерно на восемь процентов,

2056
01:09:44,960 --> 01:09:46,080
но когда они тестируют свою

2057
01:09:46,080 --> 01:09:48,000
модель с расширенными знаниями, которую они называют ebert,

2058
01:09:48,000 --> 01:09:50,158
оценка падает только примерно на один процент,

2059
01:09:50,158 --> 01:09:52,158
так что возможно, что по мере того, как мы будем проводить более сложные

2060
01:09:52,158 --> 01:09:54,000
исследования знаний, мы действительно увидим еще

2061
01:09:54,000 --> 01:09:56,239
большие различия в производительности

2062
01:09:56,239 --> 01:09:58,080
моделей с расширенными знаниями.  модели

2063
01:09:58,080 --> 01:10:01,880
без этих расширений знаний

2064
01:10:02,640 --> 01:10:04,080
следующая часть работы, о которой мы поговорим, на

2065
01:10:04,080 --> 01:10:07,760
самом деле касается этой проблемы, потому

2066
01:10:07,760 --> 01:10:09,679
что

2067
01:10:09,679 --> 01:10:11,760
формулировка приглашения может фактически вызвать

2068
01:10:11,760 --> 01:10:13,120
разные ответы от языковой

2069
01:10:13,120 --> 01:10:14,880
модели, поэтому языковая модель может знать

2070
01:10:14,880 --> 01:10:16,719
этот факт, но может потерпеть неудачу

2071
01:10:16,719 --> 01:10:19,120
по задаче из-за формулировки,

2072
01:10:19,120 --> 01:10:20,480
одна из причин, по которой это может произойти, заключается в том, что

2073
01:10:20,480 --> 01:10:22,400
предварительное обучение проводится в разных контекстах

2074
01:10:22,400 --> 01:10:24,400
и предложениях  e структуры в запросе,

2075
01:10:24,400 --> 01:10:25,840
так что, например, у вас может быть в вашем

2076
01:10:25,840 --> 01:10:28,000
корпусе предварительной подготовки, место рождения

2077
01:10:28,000 --> 01:10:30,560
Барака Обамы - Гонолулу, Гавайи, и это

2078
01:10:30,560 --> 01:10:31,840
может быть что-то, что вы видите в Википедии,

2079
01:10:31,840 --> 01:10:33,120
например, это общий

2080
01:10:33,120 --> 01:10:34,640
набор данных для обучения,

2081
01:10:34,640 --> 01:10:36,159
а затем как исследователь вы пишете

2082
01:10:36,159 --> 01:10:38,320
barack  Обама рожден пустым, и вы

2083
01:10:38,320 --> 01:10:39,440
можете видеть, что эти структуры предложений

2084
01:10:39,440 --> 01:10:41,520
довольно разные, поэтому модель

2085
01:10:41,520 --> 01:10:43,120
могла видеть первый факт, но

2086
01:10:43,120 --> 01:10:44,320
разница в структуре предложений на

2087
01:10:44,320 --> 01:10:46,320
самом деле достаточно, чтобы запутать ее, поэтому он

2088
01:10:46,320 --> 01:10:49,199
не может ответить на этот запрос,

2089
01:10:49,199 --> 01:10:50,640
поэтому они делают то, что  генерировать намного

2090
01:10:50,640 --> 01:10:52,400
больше таких запросов с помощью шаблонов интеллектуального анализа данных

2091
01:10:52,400 --> 01:10:54,239
из Википедии, один из их

2092
01:10:54,239 --> 01:10:55,600
методов фактически использует

2093
01:10:55,600 --> 01:10:58,159
синтаксический анализ зависимостей, а также генерирует запросы перефразирования,

2094
01:10:58,159 --> 01:11:00,960
основываясь на литературе

2095
01:11:00,960 --> 01:11:02,640
по машинному переводу

2096
01:11:02,640 --> 01:11:04,880
и используя обратный перевод,

2097
01:11:04,880 --> 01:11:07,120
поэтому генерируйте гораздо больше

2098
01:11:07,120 --> 01:11:08,800
запросов, чтобы попытаться запросить языковые модели  и выяснить,

2099
01:11:08,800 --> 01:11:10,960
делают ли небольшие изменения в

2100
01:11:10,960 --> 01:11:12,719
подсказке правильное предсказание от

2101
01:11:12,719 --> 01:11:14,560
языковой модели, которую

2102
01:11:14,560 --> 01:11:16,080
они также использовали  periment затем мрачные

2103
01:11:16,080 --> 01:11:17,840
подсказки, поэтому, если мы дадим модели несколько

2104
01:11:17,840 --> 01:11:19,600
подсказок, а затем возьмем

2105
01:11:19,600 --> 01:11:21,280
некоторую вероятность, усредненную по этим

2106
01:11:21,280 --> 01:11:23,280
различным подсказкам,

2107
01:11:23,280 --> 01:11:24,880
можем ли мы улучшить производительность

2108
01:11:24,880 --> 01:11:26,400
модели, возвращающей правильный прогноз,

2109
01:11:26,400 --> 01:11:27,840
чтобы мы дали ей больше шансов

2110
01:11:27,840 --> 01:11:29,440
увидеть контекст, который мог бы быть на самом деле

2111
01:11:29,440 --> 01:11:32,480
во время предварительного обучения

2112
01:11:32,480 --> 01:11:34,400
они обнаруживают, что производительность ламы

2113
01:11:34,400 --> 01:11:36,560
увеличивается, когда они либо используют наиболее

2114
01:11:36,560 --> 01:11:38,159
эффективную подсказку, либо когда они используют этот

2115
01:11:38,159 --> 01:11:39,679
подход к объединению,

2116
01:11:39,679 --> 01:11:41,199
поэтому это говорит о том, что исходная лама

2117
01:11:41,199 --> 01:11:42,960
действительно была нижней границей

2118
01:11:42,960 --> 01:11:44,719
количества знаний, закодированных в этих языковых

2119
01:11:44,719 --> 01:11:46,400
моделях  и

2120
01:11:46,400 --> 01:11:48,560
изменение формулировки может действительно помочь

2121
01:11:49,520 --> 01:11:52,640
модели вспомнить правильный ответ,

2122
01:11:52,640 --> 01:11:54,000
эта таблица немного пугает, но

2123
01:11:54,000 --> 01:11:55,679
они обнаруживают, что небольшие изменения в

2124
01:11:55,679 --> 01:11:57,600
запросе могут привести к действительно большому увеличению

2125
01:11:57,600 --> 01:11:59,840
производительности, поэтому, если у вас просто есть такой запрос,

2126
01:11:59,840 --> 01:12:02,320
как x играет в позиции y

2127
01:12:02,320 --> 01:12:04,000
и  затем вы меняете это значение на x, которое играет в этой

2128
01:12:04,000 --> 01:12:06,000
позиции y, это может фактически привести

2129
01:12:06,000 --> 01:12:07,920
к увеличению точности в 23 раза в этом

2130
01:12:07,920 --> 01:12:09,600
конкретном случае.

2131
01:12:09,600 --> 01:12:10,800
С точки зрения модели, действительно

2132
01:12:10,800 --> 01:12:13,280
способной вспомнить правильный ответ

2133
01:12:13,280 --> 01:12:16,000
или даже просто x был создан в y, чтобы x было

2134
01:12:16,000 --> 01:12:19,440
создано с увеличением точности y 10,

2135
01:12:19,440 --> 01:12:21,280
поэтому я думаю, что это мотивирует необходимость

2136
01:12:21,280 --> 01:12:23,040
не только разработать более эффективные способы запроса

2137
01:12:23,040 --> 01:12:24,800
этих моделей, но, вероятно,  также создавать

2138
01:12:24,800 --> 01:12:26,239
языковые модели, которые на самом деле более

2139
01:12:26,239 --> 01:12:29,840
устойчивы к самому запросу,

2140
01:12:31,440 --> 01:12:33,679
поэтому в дополнение к зондам другой способ

2141
01:12:33,679 --> 01:12:35,199
оценки этих языковых моделей - это

2142
01:12:35,199 --> 01:12:37,600
посмотреть, насколько хорошо они переходят

2143
01:12:37,600 --> 01:12:39,520
от предварительно обученного представления к

2144
01:12:39,520 --> 01:12:42,080
последующим задачам,

2145
01:12:42,080 --> 01:12:43,440
и поэтому идея здесь - вы '  На самом деле мы

2146
01:12:43,440 --> 01:12:44,560
собираемся точно настроить предварительно обученное

2147
01:12:44,560 --> 01:12:46,400
представление для различных последующих

2148
01:12:46,400 --> 01:12:48,400
задач, аналогично тому, как вы оценили бы

2149
01:12:48,400 --> 01:12:51,440
bert в задачах склеивания.

2150
01:12:51,440 --> 01:12:53,280
Некоторые общие задачи, которые используются для этого,

2151
01:12:53,280 --> 01:12:56,320
- это типирование сущности извлечения

2152
01:12:56,320 --> 01:12:58,000
отношения и извлечение отношения ответа на вопрос

2153
01:12:58,000 --> 01:12:59,920
- это то, где вы хотите предсказать

2154
01:12:59,920 --> 01:13:01,600
отношения между двумя объектами, поэтому

2155
01:13:01,600 --> 01:13:03,040
мы возвращаемся к одному из

2156
01:13:03,040 --> 01:13:04,480
вопросов, заданных ранее в разговоре, с точки

2157
01:13:04,480 --> 01:13:06,000
зрения того, как получить отношение,

2158
01:13:06,000 --> 01:13:07,120
которое '  s края в этих базах знаний,

2159
01:13:08,080 --> 01:13:10,320
поэтому, учитывая две сущности, вы изучаете модель,

2160
01:13:10,320 --> 01:13:12,080
чтобы предсказать, какая связь между

2161
01:13:12,080 --> 01:13:12,960
ними.

2162
01:13:18,080 --> 01:13:19,760
как преступник,

2163
01:13:19,760 --> 01:13:21,040
а вы, ребята, хорошо знакомы с

2164
01:13:21,040 --> 01:13:23,280
ответами на вопросы,

2165
01:13:23,280 --> 01:13:25,199
поэтому идея этого комментария этих

2166
01:13:25,199 --> 01:13:26,719
задач заключается в том, что они требуют больших знаний,

2167
01:13:26,719 --> 01:13:28,480
поэтому они являются хорошими кандидатами, чтобы

2168
01:13:28,480 --> 01:13:30,000
увидеть, насколько хорошо эти предварительно обученные

2169
01:13:30,000 --> 01:13:31,760
представления на самом деле передают свои

2170
01:13:31,760 --> 01:13:35,360
знания  эти нижестоящие задачи

2171
01:13:36,239 --> 01:13:37,600
здесь мы смотрим на производительность

2172
01:13:37,600 --> 01:13:39,440
на тесте извлечения отношений, называемом

2173
01:13:39,440 --> 01:13:42,000
tacrid, и все модели, которые мы

2174
01:13:42,000 --> 01:13:43,920
здесь показываем, в какой-то момент были современными

2175
01:13:43,920 --> 01:13:45,120
на tacrid,

2176
01:13:45,120 --> 01:13:48,480
поэтому этот cgcn представляет собой сверточную

2177
01:13:48,480 --> 01:13:50,640
нейронную сеть графа над деревьями зависимостей.

2178
01:13:50,640 --> 01:13:53,520
База lstm - это одна из

2179
01:13:53,520 --> 01:13:54,800
первых работ, которые показали, что вы

2180
01:13:54,800 --> 01:13:56,000
действительно можете получить современную

2181
01:13:56,000 --> 01:13:57,520
производительность с помощью bert при извлечении отношений,

2182
01:13:57,520 --> 01:13:59,600
и это просто помещает lstm

2183
01:13:59,600 --> 01:14:01,600
laye  Результат r over bert

2184
01:14:01,600 --> 01:14:02,960
ernie - это работа, о которой мы говорили

2185
01:14:02,960 --> 01:14:05,280
с предварительно обученными встраиваемыми сущностями,

2186
01:14:05,280 --> 01:14:06,640
соответствующими пробелам, до которых мы не дошли

2187
01:14:06,640 --> 01:14:08,239
сегодня, но это действительно интересная работа

2188
01:14:08,239 --> 01:14:09,920
по изучению значимых представлений отношений,

2189
01:14:09,920 --> 01:14:12,480
и она больше относится

2190
01:14:12,480 --> 01:14:13,440
к

2191
01:14:13,440 --> 01:14:14,960
подходам к изменению обучающих данных

2192
01:14:14,960 --> 01:14:16,800
и что они на самом деле

2193
01:14:16,800 --> 01:14:18,880
снова маскируют сущности,

2194
01:14:18,880 --> 01:14:21,600
а затем нет рождения - это то, о чем мы говорили.

2195
01:14:21,600 --> 01:14:24,000
w и w здесь означает, что они фактически

2196
01:14:24,000 --> 01:14:26,000
кодируют две базы знаний в novert, поэтому

2197
01:14:26,000 --> 01:14:27,360
они кодируют wordnet, а также они

2198
01:14:27,360 --> 01:14:30,000
кодируют википедию

2199
01:14:30,000 --> 01:14:31,360
и высокий уровень  Вывод из этой

2200
01:14:31,360 --> 01:14:33,280
таблицы заключается в том, что вы можете видеть, что

2201
01:14:33,280 --> 01:14:35,280
последние модели с расширенными знаниями

2202
01:14:35,280 --> 01:14:37,199
достигли уровня искусства по сравнению

2203
01:14:37,199 --> 01:14:39,040
с исходными моделями, которые когда-то

2204
01:14:39,040 --> 01:14:40,800
очень хорошо работали на сетке галсов, и у нас есть

2205
01:14:40,800 --> 01:14:43,600
около пяти приростов f1, здесь есть

2206
01:14:43,600 --> 01:14:44,880
еще один интересный вывод из этой

2207
01:14:44,880 --> 01:14:46,640
таблицы  похоже, это компромисс

2208
01:14:46,640 --> 01:14:48,719
в размере языковой модели, который

2209
01:14:48,719 --> 01:14:50,719
необходим для достижения определенной производительности,

2210
01:14:50,719 --> 01:14:52,560
поэтому, если вы просто учитываете размер

2211
01:14:52,560 --> 01:14:53,600
языковая модель тогда никакое рождение

2212
01:14:53,600 --> 01:14:54,960
работает лучше всего,

2213
01:14:54,960 --> 01:14:56,400
но если вы

2214
01:14:56,400 --> 01:14:58,000
не учитываете, что тогда она максимальна с

2215
01:14:58,000 --> 01:15:00,640
сопоставлением пробелов, в

2216
01:15:00,640 --> 01:15:02,560
целом это довольно хорошее свидетельство того,

2217
01:15:02,560 --> 01:15:04,800
что эти методы

2218
01:15:04,800 --> 01:15:06,480
расширения знаний фактически переносятся на эти

2219
01:15:06,480 --> 01:15:08,400
наукоемкие последующие задачи,

2220
01:15:08,400 --> 01:15:10,480
которые действительно могут потребовать  Преимущество этих

2221
01:15:10,480 --> 01:15:13,679
предварительно обученных представлений:

2222
01:15:14,000 --> 01:15:16,239
у нас также есть результаты по типированию сущностей, поэтому

2223
01:15:16,239 --> 01:15:17,280
здесь мы сравниваем несколько

2224
01:15:17,280 --> 01:15:18,960
другой набор моделей, некоторые из

2225
01:15:18,960 --> 01:15:21,120
базовых - это модели lstm, которые были

2226
01:15:21,120 --> 01:15:23,520
разработаны для типизации сущностей, и у нас есть

2227
01:15:23,520 --> 01:15:26,640
ernie и nobert um, ведущие

2228
01:15:26,640 --> 01:15:28,719
i  угадайте таблицу лидеров здесь по

2229
01:15:28,719 --> 01:15:30,960
задаче типизации сущности открытой сущности, и мы видим

2230
01:15:30,960 --> 01:15:33,440
выигрыш примерно в 15 очков F1 с Эрни

2231
01:15:33,440 --> 01:15:35,520
и без Берта, так что еще раз мы действительно

2232
01:15:35,520 --> 01:15:37,600
видим, что эти богатые знаниями

2233
01:15:37,600 --> 01:15:39,199
предварительно обученные представления

2234
01:15:39,199 --> 01:15:40,800
передаются и помогают в этих

2235
01:15:40,800 --> 01:15:44,400
интенсивных знаниях ниже по течению  задачи,

2236
01:15:45,679 --> 01:15:48,000
поэтому просто напомним, что мы говорили о пробах,

2237
01:15:48,000 --> 01:15:49,360
которые оценивают знания, уже

2238
01:15:49,360 --> 01:15:51,199
присутствующие в моделях, они не требуют

2239
01:15:51,199 --> 01:15:52,560
каких-либо  повторное обучение,

2240
01:15:52,560 --> 01:15:54,000
но может быть сложно построить

2241
01:15:54,000 --> 01:15:56,000
тесты, чтобы на самом деле

2242
01:15:56,000 --> 01:15:57,280
убедиться, что вы проверяете знания

2243
01:15:57,280 --> 01:15:59,120
в этих языковых моделях, также может быть

2244
01:15:59,120 --> 01:16:00,880
сложно построить запросы,

2245
01:16:00,880 --> 01:16:02,800
используемые в тесте, о котором

2246
01:16:02,800 --> 01:16:05,120
мы тогда говорили о последующих задачах,

2247
01:16:05,120 --> 01:16:06,560
это немного косвенный  способ

2248
01:16:06,560 --> 01:16:07,679
оценить знания и что у них есть

2249
01:16:07,679 --> 01:16:09,679
этот дополнительный компонент тонкой настройки, но

2250
01:16:09,679 --> 01:16:11,840
это хороший способ оценить, насколько

2251
01:16:11,840 --> 01:16:13,679
полезно это богатое знаниями предварительно обученное

2252
01:16:13,679 --> 01:16:17,600
представление в реальных приложениях,

2253
01:16:18,719 --> 01:16:21,199
поэтому я просто коснулся захватывающей

2254
01:16:21,199 --> 01:16:23,040
работы в этой области, но есть много  другие

2255
01:16:23,040 --> 01:16:24,960
направления, если вы хотите углубиться в

2256
01:16:24,960 --> 01:16:26,800
это, поэтому есть поисковые расширенные

2257
01:16:26,800 --> 01:16:28,400
языковые модели, которые изучают извлекающие знания,

2258
01:16:29,679 --> 01:16:31,440
чтобы выяснить, какие документы могут иметь

2259
01:16:31,440 --> 01:16:33,679
отношение к предсказанию следующего слова.

2260
01:16:33,679 --> 01:16:35,679
есть работа по изменению знаний

2261
01:16:35,679 --> 01:16:37,600
в языковых моделях, поэтому я рассказал о том, как

2262
01:16:37,600 --> 01:16:39,360
это  препятствий и

2263
01:16:39,360 --> 01:16:41,520
проблем на пути использования языковых моделей в качестве

2264
01:16:41,520 --> 01:16:43,520
баз знаний, поэтому в последнее время

2265
01:16:43,520 --> 01:16:45,040
в этой области

2266
01:16:45,040 --> 01:16:47,520
мы также  увидел, насколько важна

2267
01:16:47,520 --> 01:16:49,520
задача предварительной подготовки знаний; хорошо, что есть много

2268
01:16:49,520 --> 01:16:51,440
статей, в которых предлагаются различные

2269
01:16:51,440 --> 01:16:53,120
задачи для предварительной подготовки знаний,

2270
01:16:53,120 --> 01:16:54,800
так что это все еще открытый вопрос с точки

2271
01:16:54,800 --> 01:16:57,679
зрения того, какие задачи лучше всего добавить, чтобы закодировать

2272
01:16:57,679 --> 01:16:59,199
больше знаний,

2273
01:16:59,199 --> 01:17:00,960
над которыми также работали  более эффективные

2274
01:17:00,960 --> 01:17:02,960
системы знаний, поэтому теперь у медсестры

2275
01:17:02,960 --> 01:17:05,120
есть эффективная задача по обеспечению качества, которая направлена

2276
01:17:05,120 --> 01:17:07,760
на создание минимальной системы обеспечения качества,

2277
01:17:07,760 --> 01:17:09,199
а затем, наконец, была проведена работа по

2278
01:17:09,199 --> 01:17:10,719
созданию лучших тестов знаний,

2279
01:17:10,719 --> 01:17:11,920
основанных

2280
01:17:11,920 --> 01:17:15,960
на тестах, которые мы видели сегодня,

2281
01:17:16,080 --> 01:17:17,840
это все, что у меня есть на сегодня, и я надеюсь

2282
01:17:17,840 --> 01:17:21,880
ваши последние проекты идут хорошо

