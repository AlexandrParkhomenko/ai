1
00:00:05,279 --> 00:00:05,600
хорошо,

2
00:00:05,600 --> 00:00:07,440
я собираюсь поговорить об эм Бёрте, а

3
00:00:07,440 --> 00:00:08,720
также о некоторой,

4
00:00:08,720 --> 00:00:11,360
э-э, работе-предшественнике, а затем о некоторых

5
00:00:11,360 --> 00:00:12,880
последующих работах, которые произошли

6
00:00:12,880 --> 00:00:14,320
в прошлом году, теперь мы не будем продолжать,

7
00:00:14,320 --> 00:00:16,720
но э-э, более поздние достижения, эм,

8
00:00:16,720 --> 00:00:17,520
это произошло

9
00:00:17,520 --> 00:00:20,640
с тех пор  тогда

10
00:00:20,640 --> 00:00:21,760
сначала мы поговорим об истории

11
00:00:21,760 --> 00:00:24,320
и предыстории, так что

12
00:00:24,320 --> 00:00:26,320
все знают и любят вложения

13
00:00:26,320 --> 00:00:27,920
вардов в nlp, верно, они вроде основы

14
00:00:27,920 --> 00:00:28,640
для

15
00:00:28,640 --> 00:00:33,040
нейронных сетей, работающих для nlp,

16
00:00:33,040 --> 00:00:35,360
потому что нейронные сети работают в

17
00:00:35,360 --> 00:00:36,960
непрерывном пространстве,

18
00:00:36,960 --> 00:00:40,399
векторы и матрицы  и, очевидно,

19
00:00:40,399 --> 00:00:42,800
текст - это дискретное пространство, и поэтому

20
00:00:42,800 --> 00:00:44,559
должно быть что-то, чтобы восполнить пробел,

21
00:00:44,559 --> 00:00:46,719
и оказывается, что средство для

22
00:00:46,719 --> 00:00:48,160
преодоления разрыва

23
00:00:48,160 --> 00:00:49,440
на самом деле довольно просто, это просто

24
00:00:49,440 --> 00:00:52,079
таблица поиска от

25
00:00:52,079 --> 00:00:54,160
каждого из них от набора дискретных словаря

26
00:00:54,160 --> 00:00:55,840
до  вектор, который изучался избирательно от

27
00:00:55,840 --> 00:00:57,440
конца до конца вправо, поэтому

28
00:00:57,440 --> 00:00:59,120
изначально они были просто изучены, как

29
00:00:59,120 --> 00:01:02,239
в оригинальном нейронном языке Bengio 2003

30
00:01:02,239 --> 00:01:03,600
на бумаге, они были просто

31
00:01:03,600 --> 00:01:06,000
обучены дискриминационно от конца до конца,

32
00:01:06,000 --> 00:01:07,760
и это были  Фактически, и поэтому

33
00:01:07,760 --> 00:01:09,439
люди тренируют языковые модели, а затем

34
00:01:09,439 --> 00:01:12,400
используют эти предварительно обученные, используют слой встраивания в

35
00:01:12,400 --> 00:01:14,640
качестве предварительно обученных представлений

36
00:01:14,640 --> 00:01:16,880
для других задач, но они

37
00:01:16,880 --> 00:01:17,759
не будут использовать остальную языковую

38
00:01:17,759 --> 00:01:18,880
модель, они просто будут использовать слой встраивания

39
00:01:19,280 --> 00:01:21,200
и  затем появилось слово «разработка», «перчатка» и

40
00:01:21,200 --> 00:01:22,479
прочее,

41
00:01:22,479 --> 00:01:24,799
где люди нашли гораздо более дешевый и

42
00:01:24,799 --> 00:01:25,920
более масштабируемый способ обучения,

43
00:01:25,920 --> 00:01:29,040
где вы можете просто использовать

44
00:01:29,040 --> 00:01:31,680
статистику корпуса, где это просто

45
00:01:31,680 --> 00:01:32,640
линейная модель, поэтому вам не нужно их

46
00:01:32,640 --> 00:01:33,840
вычислять  дорогостоящие

47
00:01:33,840 --> 00:01:34,799
слои прямого распространения, которые вы собираетесь

48
00:01:34,799 --> 00:01:36,960
выбросить в любом случае, и поэтому вы можете масштабировать

49
00:01:36,960 --> 00:01:38,560
до миллиардов

50
00:01:38,560 --> 00:01:43,119
токенов на одном процессоре, так

51
00:01:43,119 --> 00:01:44,640
что проблема в том, что эти

52
00:01:44,640 --> 00:01:46,479
вложения слов

53
00:01:46,479 --> 00:01:47,840
применяются в контексте, свободном от контекста,

54
00:01:47,840 --> 00:01:49,920
так что для  как своего рода простой игрушечный

55
00:01:49,920 --> 00:01:50,560
пример,

56
00:01:50,560 --> 00:01:52,799
слово банк, если вы говорите открыть банковский

57
00:01:52,799 --> 00:01:54,240
счет и на берегу реки,

58
00:01:54,240 --> 00:01:55,680
это будет то же самое вложение, поэтому

59
00:01:55,680 --> 00:01:57,759
люди пытаются делать такие вещи, как

60
00:01:57,759 --> 00:01:59,920
встраивание смысла слова, где это не

61
00:01:59,920 --> 00:02:01,360
всего одно слово, это полный смысл слова,

62
00:02:01,920 --> 00:02:04,399
но этот вид банка - это

63
00:02:04,399 --> 00:02:06,320
немного игрушечный пример, так что

64
00:02:06,320 --> 00:02:08,000
почти любое слово имеет разное значение в

65
00:02:08,000 --> 00:02:09,758
зависимости от контекста

66
00:02:09,758 --> 00:02:13,040
эм, это очень даже похоже на открытие

67
00:02:13,040 --> 00:02:14,319
банковского счета, и я пошел  для банка

68
00:02:14,319 --> 00:02:15,520
они все еще находятся

69
00:02:15,520 --> 00:02:18,959
в полуразличных смыслах слова

70
00:02:18,959 --> 00:02:20,560
банк один из них означает, что у них

71
00:02:20,560 --> 00:02:22,720
другая часть текста речи,

72
00:02:22,720 --> 00:02:24,160
ну, я думаю, не совсем, но как

73
00:02:24,160 --> 00:02:25,120
будто они вроде как используют

74
00:02:25,120 --> 00:02:26,000
разные  чувствует правильно,

75
00:02:26,000 --> 00:02:28,800
и так что да, поэтому нам действительно нужно

76
00:02:28,800 --> 00:02:30,000
правильное контекстное представление, поэтому мы

77
00:02:30,000 --> 00:02:30,800
хотим что-то,

78
00:02:30,800 --> 00:02:33,440
где это представление слова

79
00:02:33,440 --> 00:02:34,160
после

80
00:02:34,160 --> 00:02:36,879
того, как оно было помещено в контекст

81
00:02:36,879 --> 00:02:37,760
предложения, которое мы видели в правильном,

82
00:02:37,760 --> 00:02:39,360
что было бы как в  внизу вот

83
00:02:39,360 --> 00:02:41,680
так что вроде по истории контекстных

84
00:02:42,480 --> 00:02:45,280
представлений первой большой статьей для этого типа

85
00:02:45,280 --> 00:02:46,959
контекстной организации была статья

86
00:02:46,959 --> 00:02:47,920
от Google

87
00:02:47,920 --> 00:02:50,319
в 2015 году под названием полу-контролируемое последовательное

88
00:02:50,319 --> 00:02:52,080
обучение от Эндрю Дай и

89
00:02:52,080 --> 00:02:55,280
Крякли и так в этом  во-первых,

90
00:02:55,280 --> 00:02:56,879
это было на самом деле очень похоже на

91
00:02:56,879 --> 00:02:58,159
статьи, которые вышли после него, ему не

92
00:02:58,159 --> 00:02:59,680
уделяли столько внимания по разным причинам,

93
00:02:59,680 --> 00:03:01,840
поэтому, в основном, у них были некоторые

94
00:03:01,840 --> 00:03:03,599
задачи классификации, такие как

95
00:03:03,599 --> 00:03:05,040
классификация настроений в обзорах фильмов,

96
00:03:05,040 --> 00:03:06,560
и у них был большой корпус

97
00:03:06,560 --> 00:03:07,440
фильмов  обзоры,

98
00:03:07,440 --> 00:03:08,640
и затем они сказали, что произойдет, если мы

99
00:03:08,640 --> 00:03:10,959
просто возьмем нашу существующую модель lstm

100
00:03:10,959 --> 00:03:12,159
и вместо того, чтобы просто использовать предварительно обученные

101
00:03:12,159 --> 00:03:13,200
вложения, которые все

102
00:03:13,200 --> 00:03:15,040
уже делали с тех пор, по крайней мере,

103
00:03:15,040 --> 00:03:16,640
для подобных, на самом деле, вероятно, с

104
00:03:16,640 --> 00:03:18,560
2003 года.

105
00:03:18,560 --> 00:03:19,840
но

106
00:03:19,840 --> 00:03:21,120
они сказали, что давайте на самом деле предварительно обучим

107
00:03:21,120 --> 00:03:23,200
всю модель в качестве языковой модели,

108
00:03:23,200 --> 00:03:24,799
а затем давайте доработаем ее для нашего

109
00:03:24,799 --> 00:03:28,560
классификационного теста, и они получили

110
00:03:28,560 --> 00:03:30,400
довольно хорошие результаты, но не такие, как звездные,

111
00:03:30,400 --> 00:03:32,239
и теперь мы знаем, что

112
00:03:32,239 --> 00:03:33,519
причина, по которой они этого не сделали  получить результаты продаж,

113
00:03:33,519 --> 00:03:34,720
они не обучили его на достаточном количестве данных, и

114
00:03:34,720 --> 00:03:35,680
они потому, что они в основном использовали

115
00:03:35,680 --> 00:03:36,480
те же корпорации,

116
00:03:36,480 --> 00:03:37,040
на которых

117
00:03:37,040 --> 00:03:38,720
они обучались, и они обучили модель того же размера

118
00:03:38,720 --> 00:03:40,480
t  что они тренировались, и теперь мы

119
00:03:40,480 --> 00:03:41,440
знаем, что это должно быть больше,

120
00:03:41,440 --> 00:03:43,280
но это вроде как, это

121
00:03:43,280 --> 00:03:44,959
уже было немного впереди

122
00:03:44,959 --> 00:03:46,480
своего времени,

123
00:03:46,480 --> 00:03:47,599
частично потому что, как вы знаете, все

124
00:03:47,599 --> 00:03:49,440
было не так, как будто у нас не было этого вычисления

125
00:03:49,440 --> 00:03:49,920
тогда,

126
00:03:49,920 --> 00:03:51,840
даже при том, что это было всего пять лет назад,

127
00:03:51,840 --> 00:03:53,120
и это было бы, знаете ли,

128
00:03:53,120 --> 00:03:54,080
дороже,

129
00:03:54,080 --> 00:03:57,760
так что э-э, а затем в 2017 году

130
00:03:57,760 --> 00:04:00,400
вышел Elmo, который был из

131
00:04:00,400 --> 00:04:01,439
университета

132
00:04:01,439 --> 00:04:04,640
Вашингтона и ai2, и вот этот

133
00:04:04,640 --> 00:04:06,239
они сделали что-то довольно умное, где

134
00:04:06,239 --> 00:04:08,239
они взяли, что

135
00:04:08,239 --> 00:04:09,439
вы обучаете языковую модель на большом

136
00:04:09,439 --> 00:04:10,799
корпусе, поэтому они торговали корпусом из миллиардов слов,

137
00:04:11,519 --> 00:04:14,480
и они сделали большую модель lstm с 4000

138
00:04:14,480 --> 00:04:16,238
скрытых измерений, что довольно

139
00:04:16,238 --> 00:04:17,120
дорого,

140
00:04:17,120 --> 00:04:19,040
и они обучили двунаправленную модель,

141
00:04:19,040 --> 00:04:20,320
так что они, но

142
00:04:20,320 --> 00:04:22,240
это  был, э-э, еженедельно двунаправленным,

143
00:04:22,240 --> 00:04:23,840
где они обучили модель слева направо,

144
00:04:23,840 --> 00:04:26,160
а затем модель справа налево-направо, а

145
00:04:26,160 --> 00:04:28,080
затем они объединили их

146
00:04:28,080 --> 00:04:29,360
и назвали эти контекстные

147
00:04:29,360 --> 00:04:31,040
предварительно обученные вложения, и поэтому

148
00:04:31,040 --> 00:04:32,080
идея elmo

149
00:04:32,080 --> 00:04:34,080
заключается в том, что это  делает  на самом деле не

150
00:04:34,080 --> 00:04:35,919
меняйте существующую архитектуру модели,

151
00:04:35,919 --> 00:04:38,639
вы как бы берете любую имеющуюся у

152
00:04:38,639 --> 00:04:40,160
вас архитектуру модели для конкретной задачи,

153
00:04:40,160 --> 00:04:42,400
которая может быть вам известна для ответа на вопрос,

154
00:04:42,400 --> 00:04:43,840
это может быть какая-то причудливая

155
00:04:43,840 --> 00:04:45,680
модель, в которой вы выполняете lstm над

156
00:04:45,680 --> 00:04:47,360
источником и над или над  вопрос

157
00:04:47,360 --> 00:04:49,199
и ответ, тогда вы затем

158
00:04:49,199 --> 00:04:50,080
обращаетесь друг к другу

159
00:04:50,080 --> 00:04:51,680
и в какой бы архитектуре у вас

160
00:04:51,680 --> 00:04:53,199
ни была, и где бы вы ни

161
00:04:53,199 --> 00:04:55,040
вставляли перчаток до этого, вы

162
00:04:55,040 --> 00:04:57,440
вставляете вложения elmo,

163
00:04:57,440 --> 00:05:00,400
ммм, и так что это стало современным,

164
00:05:00,400 --> 00:05:00,880
вы знаете

165
00:05:00,880 --> 00:05:02,400
все в то время полумесяц,

166
00:05:02,400 --> 00:05:04,479
отвечая на семантические части и синтетический

167
00:05:04,479 --> 00:05:05,120
синтаксический анализ,

168
00:05:05,120 --> 00:05:06,639
потому что это было, и поэтому, если вы

169
00:05:06,639 --> 00:05:07,600
просто возьмете любую существующую

170
00:05:07,600 --> 00:05:09,120
современную модель,

171
00:05:09,120 --> 00:05:11,199
вы можете поместить ее в mlm и получить

172
00:05:11,199 --> 00:05:12,800
состояние искусства правильно, но но но  они

173
00:05:12,800 --> 00:05:13,280
не были,

174
00:05:13,280 --> 00:05:15,199
но это были своего рода модели, которые

175
00:05:15,199 --> 00:05:16,479
были своего рода фиксированными,

176
00:05:16,479 --> 00:05:21,440
а затем, после этого,

177
00:05:21,440 --> 00:05:23,680
openai опубликовал улучшение

178
00:05:23,680 --> 00:05:24,479
понимания языка с помощью генеративного

179
00:05:24,479 --> 00:05:25,520
предварительного обучения, которое

180
00:05:25,520 --> 00:05:29,440
просто называется gpt-1 и  поэтому в этом они

181
00:05:29,440 --> 00:05:33,440
взяли такой же большой

182
00:05:34,479 --> 00:05:37,039
корпус из миллиарда слов, и они обучили

183
00:05:37,039 --> 00:05:38,800
очень большую языковую модель, так что 12-слойная

184
00:05:38,800 --> 00:05:39,440
языковая модель,

185
00:05:39,440 --> 00:05:41,840
которая в то время была, может быть, я не знаю,

186
00:05:41,840 --> 00:05:42,880
действительно ли это был большой язык

187
00:05:42,880 --> 00:05:43,680
а не между ними в то время,

188
00:05:43,680 --> 00:05:44,639
безусловно, было крупнейшее предприятие,

189
00:05:44,639 --> 00:05:45,919
которое было обучено на таком большом количестве данных

190
00:05:45,919 --> 00:05:49,360
для своего рода модели с открытым исходным кодом,

191
00:05:49,360 --> 00:05:50,880
и когда я впервые прочитал его, я действительно

192
00:05:50,880 --> 00:05:52,479
подумал, что он слишком большой,

193
00:05:52,800 --> 00:05:54,400
не то чтобы это было хуже, а что  они как

194
00:05:54,400 --> 00:05:55,440
бы просто хвастались, показывая, насколько

195
00:05:55,440 --> 00:05:56,560
велика модель, которую они могут обучить,

196
00:05:56,560 --> 00:05:58,400
но теперь мы знаем, что на самом деле

197
00:05:58,400 --> 00:05:59,759
эта глубина, которая у них была, была на самом деле

198
00:05:59,759 --> 00:06:00,639
решающим,

199
00:06:00,639 --> 00:06:02,400
решающим элементом, поэтому они сделали

200
00:06:02,400 --> 00:06:03,600
что-то довольно простое.

201
00:06:03,600 --> 00:06:04,880
это обучить языковую модель

202
00:06:04,880 --> 00:06:06,319
очень большой, а затем они просто

203
00:06:06,319 --> 00:06:07,120
настроили ее,

204
00:06:07,120 --> 00:06:09,120
взяв последний токен, а затем

205
00:06:09,120 --> 00:06:10,960
точно настроив ее для задачи классификации,

206
00:06:10,960 --> 00:06:13,520
так что это положительное или отрицательное,

207
00:06:13,520 --> 00:06:14,319
и они получили в

208
00:06:14,319 --> 00:06:16,080
основном состояние  искусства по множеству

209
00:06:16,080 --> 00:06:18,240
различных задач классификации,

210
00:06:18,240 --> 00:06:20,400
но я собираюсь принять

211
00:06:21,840 --> 00:06:24,000
здесь какую-то сторону, прежде чем я перейду в

212
00:06:24,000 --> 00:06:26,240
причал, который касается трансформатора, так

213
00:06:26,240 --> 00:06:27,280
что это был другой, э-э,

214
00:06:27,280 --> 00:06:29,120
большой  такая вещь, как большой

215
00:06:29,120 --> 00:06:30,960
предшественник, э-э, который позволил

216
00:06:30,960 --> 00:06:35,680
bert и gbt работать хорошо, так что um

217
00:06:35,680 --> 00:06:37,280
burton gbt оба использовались для преобразования, о чем

218
00:06:37,280 --> 00:06:38,400
я уверен, что вы, ребята, узнали, и

219
00:06:38,400 --> 00:06:39,520
поэтому

220
00:06:39,520 --> 00:06:41,120
мне не нужно вдаваться во все

221
00:06:41,120 --> 00:06:42,560
подробности об этом, но

222
00:06:42,560 --> 00:06:46,160
гм, так что

223
00:06:46,160 --> 00:06:47,360
вы знаете, что многоголовое внимание с

224
00:06:47,360 --> 00:06:49,280
прямой связью слоев норма слоя я не буду вдаваться

225
00:06:49,280 --> 00:06:50,319
во все детали, потому что

226
00:06:50,319 --> 00:06:51,120
я думаю, что вы, ребята, уже узнали об

227
00:06:51,120 --> 00:06:53,599
этом, но главный вопрос в том,

228
00:06:53,599 --> 00:06:56,479
почему этот вид взял верх,

229
00:06:56,479 --> 00:06:57,680
это действительно  два преимущества по сравнению с

230
00:06:57,680 --> 00:06:58,400
lstm:

231
00:06:58,400 --> 00:07:00,840
одно состоит в том, что нет смещения по местности, и

232
00:07:00,840 --> 00:07:02,560
поэтому

233
00:07:02,560 --> 00:07:05,120
контекст с большим расстоянием имеет равные

234
00:07:05,120 --> 00:07:06,720
возможности с контекстом с коротким расстоянием,

235
00:07:06,720 --> 00:07:07,919
что

236
00:07:07,919 --> 00:07:10,080
важно, так как, как и при нормальном

237
00:07:10,080 --> 00:07:11,120
понимании языка,

238
00:07:11,120 --> 00:07:14,560
смещение местоположения lstms обычно

239
00:07:14,560 --> 00:07:17,440
считается хорошей вещью

240
00:07:17,440 --> 00:07:20,639
потому что локальный контекст более

241
00:07:20,639 --> 00:07:22,400
актуален, чем контекст на большом расстоянии,

242
00:07:22,400 --> 00:07:24,000
но способ, которым работают gpt, bert и другие

243
00:07:24,000 --> 00:07:24,720
модели,

244
00:07:24,720 --> 00:07:27,520
состоит в том, что они фактически объединяют

245
00:07:27,520 --> 00:07:28,319
контекст,

246
00:07:28,319 --> 00:07:31,120
и поэтому, если у вас есть такая модель,

247
00:07:31,120 --> 00:07:32,479
которая говорит, действительно ли это

248
00:07:32,479 --> 00:07:34,479
предложение влечет за собой предложение два,

249
00:07:34,479 --> 00:07:36,080
способ, которым это было сделано исторически,

250
00:07:36,080 --> 00:07:37,520
означает, что, как и до

251
00:07:37,520 --> 00:07:39,440
gbt, вы хотели бы закодировать их оба,

252
00:07:39,440 --> 00:07:40,560
скажем, с помощью lstm,

253
00:07:40,560 --> 00:07:41,919
тогда вы обращали бы внимание от одного

254
00:07:41,919 --> 00:07:44,080
к другому с

255
00:07:44,080 --> 00:07:46,240
помощью трансформатора, вы можете просто поместить их в

256
00:07:46,240 --> 00:07:47,520
одну и ту же последовательность

257
00:07:47,520 --> 00:07:49,599
и дать им отдельную последовательность

258
00:07:49,599 --> 00:07:51,199
ставки или добавить токен разделителя,

259
00:07:51,199 --> 00:07:54,160
и он научится, а затем он

260
00:07:54,479 --> 00:07:56,080
может что-то найти в своем собственном предложении

261
00:07:56,080 --> 00:07:57,599
локально, но он также может присутствовать на

262
00:07:57,599 --> 00:07:59,759
всем пути до другого предложения, ну, потому что

263
00:07:59,759 --> 00:08:01,520
почти

264
00:08:01,520 --> 00:08:03,599
нет, это так же легко для него  стремиться полностью

265
00:08:03,599 --> 00:08:04,639
к другому предложению,

266
00:08:04,639 --> 00:08:06,160
и поэтому, когда вы делаете это, вы можете

267
00:08:06,160 --> 00:08:07,120
просто упаковать все в одну

268
00:08:07,120 --> 00:08:08,080
последовательность, и

269
00:08:08,080 --> 00:08:09,520
тогда все будет изучено, а

270
00:08:09,520 --> 00:08:10,879
не вместо того, чтобы делать это  его как

271
00:08:10,879 --> 00:08:11,440
часть

272
00:08:11,440 --> 00:08:13,280
архитектуры модели, которая в конечном

273
00:08:13,280 --> 00:08:14,879
итоге оказывается довольно важной

274
00:08:14,879 --> 00:08:17,360
для упрощения этих моделей, и

275
00:08:17,360 --> 00:08:18,160
поэтому другая вещь

276
00:08:18,160 --> 00:08:22,000
- это гм, что

277
00:08:22,000 --> 00:08:25,120
с трансформаторами с lstms, допустим,

278
00:08:25,440 --> 00:08:26,960
это партия, и это

279
00:08:26,960 --> 00:08:28,400
слова сзади  поэтому у вас есть два предложения

280
00:08:28,400 --> 00:08:29,759
и четыре слова в предложении,

281
00:08:29,759 --> 00:08:32,080
каждый шаг должен вычисляться по одному,

282
00:08:32,080 --> 00:08:34,159
поэтому вы получаете только пакетный размер

283
00:08:34,159 --> 00:08:35,440
эффективно,

284
00:08:35,440 --> 00:08:38,399
и так на современном оборудовании, которое является tpus

285
00:08:38,399 --> 00:08:39,279
и gpus,

286
00:08:39,279 --> 00:08:40,640
чем больше умножение матрицы, тем

287
00:08:40,640 --> 00:08:41,919
лучше  вы хотите, чтобы все три

288
00:08:41,919 --> 00:08:43,440
измерения были большими,

289
00:08:43,440 --> 00:08:45,680
поэтому, даже если у вас есть большие скрытые слои,

290
00:08:45,680 --> 00:08:47,200
размер вашей партии все равно будет

291
00:08:47,200 --> 00:08:47,680
маленьким,

292
00:08:47,680 --> 00:08:49,040
если у вас нет огромной партии, но тогда

293
00:08:49,040 --> 00:08:51,040
это слишком дорого для длинных последовательностей,

294
00:08:51,040 --> 00:08:53,680
но с трансформаторами это всего,

295
00:08:53,680 --> 00:08:54,080
потому

296
00:08:54,080 --> 00:08:57,040
что это слой мудрый  внимание, общее

297
00:08:57,040 --> 00:08:57,519
число,

298
00:08:57,519 --> 00:08:58,560
размер пакета - это общее количество

299
00:08:58,560 --> 00:09:01,040
слов, поэтому, если у вас есть 500 слов, а затем

300
00:09:01,040 --> 00:09:04,160
32 предложения, это на самом деле 32 раза по 512

301
00:09:04,160 --> 00:09:05,279
- это общий размер пакета, поэтому вы получите эти

302
00:09:05,279 --> 00:09:06,480
огромные матричные умножения,

303
00:09:06,480 --> 00:09:08,399
и вы можете воспользоваться преимуществами современного

304
00:09:08,399 --> 00:09:10,480
оборудования,

305
00:09:10,480 --> 00:09:11,279
и вот почему

306
00:09:11,279 --> 00:09:12,959
трансформатор взял верх из-за

307
00:09:12,959 --> 00:09:13,760
этих двух вещей,

308
00:09:13,760 --> 00:09:14,959
и вот почему он использовался в gpd и

309
00:09:14,959 --> 00:09:17,519
почему он использует отрыжку, поэтому

310
00:09:17,519 --> 00:09:22,560
теперь я собираюсь поговорить о bert  так

311
00:09:22,560 --> 00:09:25,360
что проблема с предыдущей моделью

312
00:09:25,360 --> 00:09:26,880
была elmo, gpt и

313
00:09:26,880 --> 00:09:29,920
um bloods до этого в том, что языковые

314
00:09:29,920 --> 00:09:32,160
модели использовали только левый контекст или правый

315
00:09:32,160 --> 00:09:33,680
контекст

316
00:09:33,680 --> 00:09:37,519
или их конкатенацию, но на самом деле,

317
00:09:37,519 --> 00:09:38,399
но понимание языка является

318
00:09:38,399 --> 00:09:39,519
двунаправленным, так что есть это

319
00:09:39,519 --> 00:09:41,600
явное, э-э,

320
00:09:41,600 --> 00:09:43,680
несоответствие  между почему, почему

321
00:09:43,680 --> 00:09:45,360
все тренировались, но на однонаправленных

322
00:09:45,360 --> 00:09:46,720
моделях, где вы могли видеть только

323
00:09:46,720 --> 00:09:48,560
влево или только вправо,

324
00:09:48,560 --> 00:09:50,399
когда на самом деле нам все равно, когда мы знаем, что

325
00:09:50,399 --> 00:09:51,519
для понимания языка вам нужно

326
00:09:51,519 --> 00:09:52,959
смотреть в обоих направлениях вправо,

327
00:09:52,959 --> 00:09:56,240
поэтому есть два  Одна из причин заключается в том, что

328
00:09:56,240 --> 00:09:57,760
исторически языковые модели

329
00:09:57,760 --> 00:09:59,760
использовались для um,

330
00:09:59,760 --> 00:10:01,920
как правило, в качестве функций в других системах,

331
00:10:01,920 --> 00:10:03,200
так что самое прямое применение

332
00:10:03,200 --> 00:10:04,160
языкового моделирования могло бы  быть похожим на

333
00:10:04,160 --> 00:10:05,760
интеллектуальный текст, который

334
00:10:05,760 --> 00:10:07,200
прямо говорит, что предсказывает следующее слово,

335
00:10:07,200 --> 00:10:08,560
другое, но другие приложения,

336
00:10:08,560 --> 00:10:10,079
которые на самом деле более распространены, - это использовать

337
00:10:10,079 --> 00:10:11,360
их в системе машинного перевода или

338
00:10:11,360 --> 00:10:12,640
системе распознавания речи,

339
00:10:12,640 --> 00:10:14,480
где у вас есть эти функции, такие как

340
00:10:14,480 --> 00:10:16,000
функции перевода или

341
00:10:16,000 --> 00:10:17,839
акустические функции  а затем вы добавляете

342
00:10:17,839 --> 00:10:19,279
языковую модель, которая говорит, какова

343
00:10:19,279 --> 00:10:20,480
вероятность предложения,

344
00:10:20,480 --> 00:10:21,519
и поэтому для этого вы хотите, чтобы это было

345
00:10:21,519 --> 00:10:23,360
правильно сформированное распределение для этих

346
00:10:23,360 --> 00:10:24,560
предварительно обученных моделей, на самом деле нас это не

347
00:10:24,560 --> 00:10:25,839
волнует, но это было

348
00:10:25,839 --> 00:10:26,480
что-то вроде того, что  были,

349
00:10:26,480 --> 00:10:29,360
эээ, люди, вроде как,

350
00:10:30,320 --> 00:10:31,920
я думаю, зациклились на этой идее о том, что языковые

351
00:10:31,920 --> 00:10:33,040
модели должны иметь

352
00:10:33,040 --> 00:10:34,399
распределение и вероятностное описание,

353
00:10:34,399 --> 00:10:35,360
хотя нас на самом деле это не заботит

354
00:10:35,360 --> 00:10:36,160
,

355
00:10:36,160 --> 00:10:38,000
но другая более важная причина заключается в

356
00:10:38,000 --> 00:10:39,440
том, что слова  могут видеть себя в

357
00:10:39,440 --> 00:10:41,760
двунаправленном кодировщике,

358
00:10:41,760 --> 00:10:44,480
и это означает, что вы создаете

359
00:10:44,959 --> 00:10:48,640
представление постепенно,

360
00:10:48,640 --> 00:10:50,480
так что у вас есть свой ввод, а затем у вас есть

361
00:10:50,480 --> 00:10:52,000
свой o  utput, и он всегда смещается на

362
00:10:52,000 --> 00:10:52,720
единицу,

363
00:10:52,720 --> 00:10:55,839
поэтому мы можем иметь токен начального

364
00:10:55,839 --> 00:10:57,680
предложения, мы предсказываем первое слово,

365
00:10:57,680 --> 00:10:59,040
затем мы вводим второе слово, которое

366
00:10:59,040 --> 00:11:00,720
мы помещаем в первое слово, второе слово,

367
00:11:00,720 --> 00:11:02,959
и поэтому мы можем один раз закодировать предложение

368
00:11:02,959 --> 00:11:04,160
и предсказать все  слов в

369
00:11:04,160 --> 00:11:05,440
предложении с однонаправленной моделью,

370
00:11:05,440 --> 00:11:06,640
и, таким образом, это дает нам хорошую эффективность выборки,

371
00:11:06,640 --> 00:11:08,079
потому что, если у нас есть

372
00:11:08,079 --> 00:11:09,680
512 измерений,

373
00:11:09,680 --> 00:11:11,600
таких как последовательность из 500 слов, мы не

374
00:11:11,600 --> 00:11:12,959
хотим, чтобы нам нужно было

375
00:11:12,959 --> 00:11:14,320
предсказывать только одно слово, потому что оно

376
00:11:14,320 --> 00:11:16,320
будет 500 раз

377
00:11:16,320 --> 00:11:18,720
столько же вычислений, чтобы получить такое же

378
00:11:18,720 --> 00:11:20,240
количество прогнозов,

379
00:11:20,240 --> 00:11:21,680
но если бы мы просто действительно сделали

380
00:11:21,680 --> 00:11:23,839
двунаправленный lcm или преобразователь,

381
00:11:23,839 --> 00:11:25,600
у нас была бы ситуация, когда вы

382
00:11:25,600 --> 00:11:27,360
кодируете свое предложение,

383
00:11:27,360 --> 00:11:29,120
все является двунаправленным, и поэтому

384
00:11:29,120 --> 00:11:30,560
после первого слоя

385
00:11:30,560 --> 00:11:32,399
все может видеть себя, так что это

386
00:11:32,399 --> 00:11:34,399
слово открыто там есть путь

387
00:11:34,399 --> 00:11:35,680
назад, чтобы открыть, и поэтому тривиально

388
00:11:35,680 --> 00:11:36,640
предсказать слово,

389
00:11:36,640 --> 00:11:38,079
которое может, может там, где оно во

390
00:11:38,079 --> 00:11:39,920
входных данных, также правильно, там нет никакого

391
00:11:39,920 --> 00:11:41,760
фактического предсказания, происходящего там

392
00:11:41,760 --> 00:11:44,640
Итак, простое решение, которое,

393
00:11:44,640 --> 00:11:46,720
по сути, является основной поддержкой, заключается в том, что

394
00:11:46,720 --> 00:11:50,000
давайте вместо

395
00:11:50,320 --> 00:11:52,480
обучения нормальной языковой модели давайте

396
00:11:52,480 --> 00:11:53,760
просто предскажем замаскировать

397
00:11:53,760 --> 00:11:56,800
k процентов слов, чтобы человек

398
00:11:56,800 --> 00:11:57,519
пошел к

399
00:11:57,519 --> 00:12:01,279
маске, чтобы купить маску с молоком и так

400
00:12:01,279 --> 00:12:02,639
и так  теперь вы можете запустить двунаправленную

401
00:12:02,639 --> 00:12:03,839
модель для этого, и поскольку слов

402
00:12:03,839 --> 00:12:05,440
нет во входных данных, вы не

403
00:12:05,440 --> 00:12:08,720
можете обмануть все, и

404
00:12:08,720 --> 00:12:10,399
поэтому обратная сторона этого заключается в том, что вы не

405
00:12:10,399 --> 00:12:11,920
получаете столько прогнозов

406
00:12:11,920 --> 00:12:13,839
на предложение, как вы ''  вы можете предсказывать только

407
00:12:13,839 --> 00:12:15,200
15 процентов слов вместо

408
00:12:15,760 --> 00:12:17,519
100 слов, но положительным моментом является то,

409
00:12:17,519 --> 00:12:18,720
что вы получаете гораздо более богатую

410
00:12:18,720 --> 00:12:19,680
модель, потому что вы видите в обоих

411
00:12:19,680 --> 00:12:20,880
направлениях правильно,

412
00:12:20,880 --> 00:12:22,880
поэтому это значение k является гиперпараметром,

413
00:12:22,880 --> 00:12:24,160
который мы  нужно

414
00:12:24,160 --> 00:12:26,560
просто принять решение эмпирически, поэтому мы используем

415
00:12:26,560 --> 00:12:27,519
15,

416
00:12:27,519 --> 00:12:28,560
оказывается, что это на самом деле своего

417
00:12:28,560 --> 00:12:30,160
рода оптимальная ценность, которую имеют люди, поэтому мы,

418
00:12:30,160 --> 00:12:31,839
а также люди с тех пор

419
00:12:31,839 --> 00:12:34,320
провели более тщательные эксперименты по абляции

420
00:12:34,320 --> 00:12:35,120
и обнаружили, что

421
00:12:35,120 --> 00:12:37,279
это 15 хорошо, поэтому причина причина

422
00:12:37,279 --> 00:12:38,639
для того, чтобы делать ac  Определенный процент

423
00:12:38,639 --> 00:12:39,920
по сравнению с другим заключается в том, что если бы вы сделали,

424
00:12:39,920 --> 00:12:42,079
скажем, 50 маскировок, вы бы получили намного

425
00:12:42,079 --> 00:12:42,880
больше предсказаний,

426
00:12:42,880 --> 00:12:44,880
но вы также замаскировали бы, как и весь

427
00:12:44,880 --> 00:12:46,880
ваш контекст, и поэтому

428
00:12:46,880 --> 00:12:50,560
вы можете, э-э, если вы замаскируете весь

429
00:12:50,560 --> 00:12:51,600
свой контекст, и вы  не получая ничего,

430
00:12:51,600 --> 00:12:53,200
вы не можете изучать контекстные модели, и если

431
00:12:53,200 --> 00:12:54,320
вы только это

432
00:12:54,320 --> 00:12:55,600
сделаете, скажем, вы могли бы замаскировать одно

433
00:12:55,600 --> 00:12:57,600
слово, которое могло бы быть оптимальным,

434
00:12:57,600 --> 00:12:59,920
может быть, ммм, но вам придется

435
00:12:59,920 --> 00:13:01,279
обрабатывать гораздо больше данных, так что это будет намного

436
00:13:01,279 --> 00:13:02,320
дороже  для обучения, и мы знаем, что

437
00:13:02,320 --> 00:13:03,440
эти модели в основном

438
00:13:03,440 --> 00:13:05,680
просто вычисляются, поэтому, если у вас просто

439
00:13:05,680 --> 00:13:06,880
достаточно данных, вы можете просто

440
00:13:06,880 --> 00:13:08,800
обучать их бесконечно, и это всегда будет

441
00:13:08,800 --> 00:13:10,399
лучше, так что на самом деле это просто

442
00:13:10,399 --> 00:13:11,760
компромисс между этими двумя этими двумя

443
00:13:11,760 --> 00:13:13,440
вещами.

444
00:13:13,440 --> 00:13:15,519
так что еще одна маленькая деталь, инвертированная,

445
00:13:15,519 --> 00:13:18,880
которая оказалась очень важной,

446
00:13:19,120 --> 00:13:20,560
заключается в том, что, поскольку массовый токен никогда не

447
00:13:20,560 --> 00:13:22,880
виден во время точной настройки вместо того, чтобы

448
00:13:22,880 --> 00:13:25,279
всегда заменять слово замаскированным

449
00:13:25,279 --> 00:13:26,399
токеном,

450
00:13:26,399 --> 00:13:29,200
поскольку в этом случае мы иногда случайным образом

451
00:13:29,920 --> 00:13:31,360
предсказывали бы это с помощью  случайное слово и

452
00:13:31,360 --> 00:13:33,120
иногда сохраняйте одно и то же слово, так, например,

453
00:13:33,120 --> 00:13:34,880
17 раз скажет, что мы пошли в магазин

454
00:13:34,880 --> 00:13:36,079
и пошли направо,

455
00:13:36,079 --> 00:13:39,760
и поэтому мы не сказали модели, в

456
00:13:39,760 --> 00:13:43,040
каком случае был какой эм,

457
00:13:43,040 --> 00:13:44,880
мы бы просто получили его мы

458
00:13:46,160 --> 00:13:47,839
мы бы просто сказали, что это

459
00:13:47,839 --> 00:13:49,279
слово должно быть правильным, и не знали,

460
00:13:49,279 --> 00:13:50,160
правильное оно или нет, поэтому

461
00:13:50,160 --> 00:13:51,519
это могло быть одно и то же слово, потому что в 10

462
00:13:51,519 --> 00:13:52,720
% случаев это одно и то же слово, это

463
00:13:52,720 --> 00:13:53,600
могло быть случайное слово

464
00:13:53,600 --> 00:13:55,760
и так  он должен быть в состоянии

465
00:13:55,760 --> 00:13:57,360
поддерживать хорошее

466
00:13:57,360 --> 00:13:59,440
представление каждого слова, потому что он не знает,

467
00:13:59,440 --> 00:14:00,959
действительно ли это правильное слово, поэтому я на

468
00:14:00,959 --> 00:14:02,000
самом деле смотрю на всех и выясняю,

469
00:14:02,000 --> 00:14:03,199
правильное ли это слово, чтобы мы

470
00:14:03,199 --> 00:14:04,240
потенциально могли даже просто уйти

471
00:14:04,240 --> 00:14:05,600
с тем, чтобы вообще не использовать математический токен и

472
00:14:05,600 --> 00:14:06,720
просто делать то же самое 50

473
00:14:06,720 --> 00:14:08,800
раз и 50 раз, но

474
00:14:08,800 --> 00:14:10,320
причина этого не в том, что,

475
00:14:10,320 --> 00:14:12,160
вы знаете, мы бы испортили

476
00:14:12,160 --> 00:14:13,360
много наших данных, и мы не хотим  это

477
00:14:13,360 --> 00:14:14,079
обязательно

478
00:14:14,079 --> 00:14:15,519
повредить данные, потому что тот факт, что  t

479
00:14:15,519 --> 00:14:17,680
это неправильное слово может испортить наше

480
00:14:17,680 --> 00:14:18,880
предсказание для какого-то другого слова здесь

481
00:14:18,880 --> 00:14:19,360
правильно,

482
00:14:19,360 --> 00:14:21,440
поэтому, в то время как массовый токен, по крайней мере,

483
00:14:21,440 --> 00:14:22,720
знает, что это неправильное слово, поэтому

484
00:14:22,720 --> 00:14:24,000
он не использует его как часть своего

485
00:14:24,000 --> 00:14:26,720
контекста,

486
00:14:26,720 --> 00:14:30,000
так что другое  Другой вид

487
00:14:30,000 --> 00:14:33,040
деталей бурта, который также сейчас и впоследствии

488
00:14:33,040 --> 00:14:33,600
может быть

489
00:14:33,600 --> 00:14:36,560
не таким важным, заключается в том, что многие из

490
00:14:36,560 --> 00:14:37,839
этих задач, которые мы выполняем, мы не

491
00:14:37,839 --> 00:14:38,720
просто изучаем

492
00:14:38,720 --> 00:14:41,279
слова, мы хотим предсказать

493
00:14:41,279 --> 00:14:42,959
отношения между предложениями, поэтому

494
00:14:42,959 --> 00:14:44,560
ответы на вопросы  в частности, у нас

495
00:14:44,560 --> 00:14:46,399
есть запрос, который

496
00:14:46,399 --> 00:14:48,959
обычно является предложением, а затем у нас есть

497
00:14:48,959 --> 00:14:50,399
ответ, который является абзацем,

498
00:14:50,399 --> 00:14:53,440
предложением или документом, и мы хотим, чтобы

499
00:14:53,440 --> 00:14:55,839
вы знали, отвечает ли он на

500
00:14:55,839 --> 00:14:56,639
вопрос,

501
00:14:56,639 --> 00:15:00,000
поэтому, делая это, мы, поэтому мы хотим

502
00:15:00,000 --> 00:15:01,519
есть некоторая предтренировочная задача,

503
00:15:01,519 --> 00:15:04,720
которая на самом деле делает предсказание на уровне предложения,

504
00:15:04,720 --> 00:15:05,920
а не просто предсказание на уровне слов,

505
00:15:05,920 --> 00:15:07,279
поэтому способ, которым мы это сделали,

506
00:15:07,839 --> 00:15:08,800
и нам нужно, чтобы это было как

507
00:15:08,800 --> 00:15:09,760
бесконечное количество данных, прямо там, где мы

508
00:15:09,760 --> 00:15:11,040
можем просто сгенерировать бесконечное am  количество

509
00:15:11,040 --> 00:15:11,440
данных,

510
00:15:11,440 --> 00:15:12,959
поэтому мы не хотим, чтобы это была

511
00:15:12,959 --> 00:15:14,720
аннотированная задача,

512
00:15:14,720 --> 00:15:17,839
поэтому мы сделали это следующим образом: мы

513
00:15:17,839 --> 00:15:20,399
просто выполнили тест прогнозирования следующего предложения, в

514
00:15:20,399 --> 00:15:21,440
котором мы взяли

515
00:15:21,440 --> 00:15:24,399
два предложения из одного и того же корпуса из

516
00:15:24,399 --> 00:15:25,360
того же документа

517
00:15:25,360 --> 00:15:26,560
и 50 раз  они из того же

518
00:15:26,560 --> 00:15:28,399
документа 50 раз из случайного

519
00:15:28,399 --> 00:15:29,199
документа,

520
00:15:29,199 --> 00:15:31,440
а затем мы просто сказали, будет ли это реальным

521
00:15:31,440 --> 00:15:32,399
следующим предложением,

522
00:15:32,399 --> 00:15:34,480
ну или нет, и поэтому, если вам нравится, что

523
00:15:34,480 --> 00:15:35,920
человек пошел в магазин, он купил пистолет

524
00:15:35,920 --> 00:15:37,440
на молоке, которое является  В следующем предложении он

525
00:15:37,440 --> 00:15:39,040
сказал, что человек пошел в магазин,

526
00:15:39,040 --> 00:15:39,759
пингвины нелетающие,

527
00:15:39,759 --> 00:15:41,040
это не следующее предложение, поэтому в

528
00:15:41,040 --> 00:15:42,959
основном сейчас мы заставляем модель

529
00:15:42,959 --> 00:15:44,720
во время подготовки к тренировке фактически заставить

530
00:15:44,720 --> 00:15:46,240
взглянуть на полные предложения,

531
00:15:46,240 --> 00:15:48,160
а затем составить какое-то предложение

532
00:15:48,160 --> 00:15:49,519
предсказание, и мы надеемся, что это

533
00:15:49,519 --> 00:15:53,040
в некотором роде обобщенное,

534
00:15:53,040 --> 00:15:54,399
это что-то вроде ответа на вопрос,

535
00:15:54,399 --> 00:15:56,160
где у вас есть вопрос и ответ в виде

536
00:15:56,160 --> 00:15:58,720
предложения, и вы отправляете его,

537
00:15:58,720 --> 00:16:00,800
так что с точки зрения нашего входного

538
00:16:00,800 --> 00:16:03,440
представления

539
00:16:04,160 --> 00:16:05,440
он выглядит очень похоже на обычный

540
00:16:05,440 --> 00:16:08,079
преобразователь, но у нас есть эти

541
00:16:08,079 --> 00:16:10,399
дополнительные вложения, которые называются

542
00:16:10,399 --> 00:16:11,759
встраиваниями сегментов,

543
00:16:11,759 --> 00:16:14,399
так что обычный преобразователь вы получите

544
00:16:14,399 --> 00:16:15,440
свой ввод,

545
00:16:15,440 --> 00:16:16,560
а затем вы будете делать

546
00:16:16,560 --> 00:16:18,800
сегментацию wordpress прямо там, где вы разделяете, повторно

547
00:16:18,800 --> 00:16:20,800
применяйте это неконтролируемое

548
00:16:20,800 --> 00:16:23,920
разбиение слов на

549
00:16:23,920 --> 00:16:26,160
своего рода морфологические расщепления, но

550
00:16:26,160 --> 00:16:27,920
они обычно часто не морфологические

551
00:16:27,920 --> 00:16:29,839
так что это неконтролируемое, но вы в конечном итоге

552
00:16:31,680 --> 00:16:33,360
получаете что-то, что примерно морфологически правильно, и теперь у вас есть как нет нет из

553
00:16:33,360 --> 00:16:35,040
словарных токенов,

554
00:16:35,040 --> 00:16:36,480
все представлено, по

555
00:16:36,480 --> 00:16:38,000
крайней мере, вы всегда разбиваете на

556
00:16:38,000 --> 00:16:38,880
символы,

557
00:16:38,880 --> 00:16:41,680
поэтому мы используем словарь из 30 000 слов,

558
00:16:41,680 --> 00:16:42,959
а затем  у нас есть

559
00:16:42,959 --> 00:16:45,920
вложения токенов, тогда у нас есть

560
00:16:45,920 --> 00:16:47,199
вложения нормального положения, которое является нижним,

561
00:16:47,199 --> 00:16:47,920
и поэтому они являются

562
00:16:47,920 --> 00:16:49,680
частью трансформатора, где, поскольку

563
00:16:49,680 --> 00:16:51,519
трансформаторы, в отличие от lstms, не имеют какой-

564
00:16:51,519 --> 00:16:53,600
либо информации о

565
00:16:53,600 --> 00:16:56,240
местонахождении, поэтому единственный

566
00:16:56,240 --> 00:16:58,000
способ кодировать это то, что  вы

567
00:16:58,000 --> 00:17:00,160
кодируете фактическое вложение для каждой

568
00:17:00,160 --> 00:17:01,600
позиции, поэтому это называется

569
00:17:01,600 --> 00:17:02,959
встраиванием абсолютной позиции, есть другой вид

570
00:17:02,959 --> 00:17:05,520
te  chniques в наши дни

571
00:17:05,520 --> 00:17:07,439
и так далее, а затем у вас есть вложение сегмента,

572
00:17:07,439 --> 00:17:08,799
которое

573
00:17:08,799 --> 00:17:10,400
представляет собой предложение a или предложение b, и

574
00:17:10,400 --> 00:17:13,359
поэтому этот вид обобщает в более

575
00:17:13,359 --> 00:17:14,799
общем контексте, так что вы можете представить,

576
00:17:14,799 --> 00:17:16,079
говорите ли вы, пытаетесь ли вы сказать,

577
00:17:16,079 --> 00:17:17,359
как вы '  повторяя попытку сделать что-то похожее на веб-поиск,

578
00:17:17,359 --> 00:17:18,000
вы могли бы сказать, что

579
00:17:18,000 --> 00:17:20,400
вот мой запрос, вот заголовок,

580
00:17:20,400 --> 00:17:21,919
вот URL-адрес, вот

581
00:17:21,919 --> 00:17:25,119
содержание документа, и поэтому вы можете

582
00:17:25,119 --> 00:17:26,319
просто упаковать их в одну

583
00:17:26,319 --> 00:17:27,359
последовательность, а затем просто дать им

584
00:17:27,359 --> 00:17:28,480
различные

585
00:17:28,480 --> 00:17:30,880
вложения сегментов или встраивания типов, так что

586
00:17:30,880 --> 00:17:32,480
что теперь у вас

587
00:17:32,480 --> 00:17:36,000
есть возможность иметь гораздо более сильную

588
00:17:36,000 --> 00:17:38,880
эм, вы можете просто представить

589
00:17:38,880 --> 00:17:39,520
все

590
00:17:39,520 --> 00:17:41,840
в такой же единой последовательности,

591
00:17:41,840 --> 00:17:43,120
где вы как бы просто дифференцируете, но

592
00:17:43,120 --> 00:17:44,320
только одно встраивание, которое отличается,

593
00:17:44,320 --> 00:17:47,200
и все это, конечно, усвоено

594
00:17:47,200 --> 00:17:48,559
и поэтому это контрастирует с

595
00:17:48,559 --> 00:17:49,840
более старым стилем, где у вас

596
00:17:49,840 --> 00:17:51,039
обычно будет другой кодировщик для

597
00:17:51,039 --> 00:17:52,080
каждой его части, так что у вас

598
00:17:52,080 --> 00:17:53,200
будет другой код для запроса, а затем,

599
00:17:53,200 --> 00:17:55,039
возможно, для заголовка и  возможно URL-адрес,

600
00:17:55,039 --> 00:17:56,160
но в этом случае это всего лишь одна

601
00:17:56,160 --> 00:17:58,320
последовательность, поэтому

602
00:17:58,320 --> 00:18:00,559
мы тренировались примерно на трех миллиардах слов,

603
00:18:00,559 --> 00:18:01,360
которые

604
00:18:01,360 --> 00:18:03,200
в то время были большими, теперь это даже не так

605
00:18:03,200 --> 00:18:04,559
уж и много из того, что люди

606
00:18:04,559 --> 00:18:06,960
тренируют,

607
00:18:07,200 --> 00:18:10,160
мы использовали размер пакета, который  был

608
00:18:10,160 --> 00:18:11,280
также большим,

609
00:18:11,280 --> 00:18:13,919
мы тренировались около 40 эпох

610
00:18:13,919 --> 00:18:15,440
по данным, и мы обучили эти две модели,

611
00:18:15,440 --> 00:18:17,200
которые все еще относительно велики,

612
00:18:17,200 --> 00:18:20,880
поэтому одна из них - это 12-слойный слой 768, а

613
00:18:20,880 --> 00:18:22,720
затем другой 24-слойный 10 24. Так что в то

614
00:18:22,720 --> 00:18:24,559
время это было в основном.  как

615
00:18:24,559 --> 00:18:25,919
одна из самых больших моделей, которые были

616
00:18:25,919 --> 00:18:27,440
обучены, хотя сейчас люди тренируют

617
00:18:27,440 --> 00:18:28,559
модели, которые, как я думаю, в

618
00:18:28,559 --> 00:18:32,240
30 или более раз больше, чем эта

619
00:18:32,240 --> 00:18:33,760
в более поздних статьях, так что в последнее время

620
00:18:33,760 --> 00:18:35,600
произошел взрыв в плане

621
00:18:35,600 --> 00:18:37,440
вычислительных вычислений.  Не знаю,

622
00:18:37,440 --> 00:18:39,120
три года,

623
00:18:39,120 --> 00:18:42,799
но да, так

624
00:18:42,799 --> 00:18:46,480
что процедура тонкой настройки

625
00:18:46,480 --> 00:18:48,799
очень проста, так что

626
00:18:48,799 --> 00:18:50,240
мы предварительно обучили эту модель для этих двух

627
00:18:50,240 --> 00:18:51,520
задач, и теперь

628
00:18:51,520 --> 00:18:53,360
у нас есть входная последовательность, которая представляет собой

629
00:18:53,360 --> 00:18:54,559
несколько предложений с разным типом

630
00:18:54,559 --> 00:18:55,440
embe  ddings,

631
00:18:55,440 --> 00:18:57,840
мы кормим их через нашу

632
00:18:58,799 --> 00:19:01,200
модель трансформатора, и теперь у нас есть специальное

633
00:19:01,200 --> 00:19:01,840
вложение,

634
00:19:01,840 --> 00:19:03,840
которое, я думаю, я не упоминал, так что

635
00:19:03,840 --> 00:19:05,919
это специальное вложение, это в

636
00:19:05,919 --> 00:19:08,320
основном, оно научилось

637
00:19:08,320 --> 00:19:10,080
предсказывать следующую задачу прогнозирования предложения,

638
00:19:10,080 --> 00:19:13,039
а затем это  используется также для

639
00:19:13,039 --> 00:19:14,080
текста классификации,

640
00:19:14,080 --> 00:19:15,440
но дело не только в том, что

641
00:19:15,440 --> 00:19:17,280
мы используем право встраивания,

642
00:19:17,280 --> 00:19:18,799
мы правильно настраиваем всю модель,

643
00:19:18,799 --> 00:19:20,720
так что на самом деле это не значит, что это

644
00:19:20,720 --> 00:19:21,919
вложение по сути полезно или

645
00:19:21,919 --> 00:19:22,960
что вложения слов

646
00:19:22,960 --> 00:19:25,919
полезны  Дело в том, что веса

647
00:19:25,919 --> 00:19:28,640
внутри всей 12- или 24-слойной модели

648
00:19:28,640 --> 00:19:30,720
полезны, и поэтому, тонко настраивая

649
00:19:30,720 --> 00:19:32,400
всю модель, вы можете как бы

650
00:19:32,400 --> 00:19:33,919
выделить основные части,

651
00:19:33,919 --> 00:19:37,200
которые важны для некоторой последующей задачи,

652
00:19:37,200 --> 00:19:40,559
и поэтому это  своего

653
00:19:40,559 --> 00:19:42,400
рода тонкая настройка груза для конкретного класса,

654
00:19:42,400 --> 00:19:44,080
поэтому, если у нас есть одна

655
00:19:44,080 --> 00:19:44,880
задача

656
00:19:44,880 --> 00:19:48,559
классификации, например, скажем, анализ настроений,

657
00:19:48,559 --> 00:19:50,480
где я говорю, положительный или

658
00:19:50,480 --> 00:19:52,160
отрицательный отзыв, мы

659
00:19:52,160 --> 00:19:54,880
кодируем наше предложение  е с моделью отрыжки,

660
00:19:54,880 --> 00:19:56,400
то единственные параметры, которые мы добавляем,

661
00:19:56,400 --> 00:19:57,039
это

662
00:19:57,039 --> 00:19:58,799
окончательная матрица выходных данных, так что, возможно, если у нас

663
00:19:58,799 --> 00:20:00,240
есть три, скажем,

664
00:20:00,240 --> 00:20:02,000
положительный отрицательный или нейтральный, это может

665
00:20:02,000 --> 00:20:03,280
быть тысячу раз

666
00:20:03,280 --> 00:20:04,640
три правильно, так что это всего лишь три тысячи

667
00:20:04,640 --> 00:20:07,360
параметров и 300 миллионов,

668
00:20:07,360 --> 00:20:10,559
так что 3000  новых параметров и 300 миллионов

669
00:20:10,559 --> 00:20:11,440
старых параметров,

670
00:20:11,440 --> 00:20:14,320
и мы совместно обучаем все 300 миллионов

671
00:20:14,320 --> 00:20:16,400
плюс 3000

672
00:20:16,400 --> 00:20:18,320
для этой последующей задачи, но поскольку

673
00:20:18,320 --> 00:20:19,760
подавляющее большинство из них предварительно обучено,

674
00:20:19,760 --> 00:20:21,760
мы можем отчасти адаптироваться к нему только

675
00:20:21,760 --> 00:20:23,120
как несколько тысяч

676
00:20:23,120 --> 00:20:25,280
помеченных примеров и аналогично для

677
00:20:25,280 --> 00:20:26,480
предложение справедливый класс

678
00:20:26,480 --> 00:20:28,720
мы делаем эм, мы просто продолжаем делать

679
00:20:28,720 --> 00:20:30,000
предложения с различными встраиваемыми типами,

680
00:20:30,000 --> 00:20:31,360
так что у нас есть, если вы хотите сказать,

681
00:20:31,360 --> 00:20:33,120
влечет ли это предложение за собой другое предложение, которое вы

682
00:20:33,120 --> 00:20:34,480
говорите, предложение a,

683
00:20:34,480 --> 00:20:37,200
вы помещаете его объединить предложение b, а

684
00:20:37,200 --> 00:20:37,679
затем

685
00:20:37,679 --> 00:20:39,440
также предсказывает из этого токена  и

686
00:20:39,440 --> 00:20:41,200
настроить все это так, чтобы

687
00:20:41,200 --> 00:20:43,520
очень мало дополнительных параметров для задач прогнозирования диапазона, у

688
00:20:44,799 --> 00:20:48,159
вас просто было своего рода начало

689
00:20:48,159 --> 00:20:49,600
начала диапазона и диапазона, чтобы вы  только

690
00:20:49,600 --> 00:20:51,760
добавление нескольких тысяч новых параметров,

691
00:20:51,760 --> 00:20:53,919
а затем для тегирования задач, таких как тегирование части

692
00:20:53,919 --> 00:20:54,799
речи, у

693
00:20:54,799 --> 00:20:58,320
вас просто есть

694
00:20:58,320 --> 00:21:02,000
одно предложение, которое у вас есть, вы добавляете

695
00:21:02,000 --> 00:21:03,760
каждый токен или, может быть, каждый токен,

696
00:21:03,760 --> 00:21:06,080
кроме частей слова, но, как и

697
00:21:06,080 --> 00:21:08,000
вы, это своего рода предварительный  - обрабатывая вас,

698
00:21:08,000 --> 00:21:09,280
вы предсказываете,

699
00:21:09,280 --> 00:21:11,520
что это за часть речи, и поэтому на

700
00:21:11,520 --> 00:21:12,960
самом деле именно

701
00:21:12,960 --> 00:21:17,440
поэтому само слово - это своего

702
00:21:17,440 --> 00:21:17,840
рода,

703
00:21:17,840 --> 00:21:19,679
я бы сказал, постепенное улучшение

704
00:21:19,679 --> 00:21:21,679
того, что существует искусство, так что для этого потребовались

705
00:21:21,679 --> 00:21:25,200
трансформеры, э-э, elmo

706
00:21:25,200 --> 00:21:28,480
gpt действительно  эти три идеи

707
00:21:28,799 --> 00:21:32,640
внесли довольно простое изменение поверх них,

708
00:21:32,640 --> 00:21:36,320
но причина, по которой они имели такое большое

709
00:21:36,320 --> 00:21:37,600
влияние, - это не просто цифры, которые я

710
00:21:37,600 --> 00:21:39,120
покажу на нескольких слайдах, это действительно

711
00:21:39,120 --> 00:21:42,840
то, потому что

712
00:21:42,840 --> 00:21:45,440
эм с Эльмо

713
00:21:45,440 --> 00:21:46,880
там  На самом деле не было принципиальной

714
00:21:46,880 --> 00:21:47,840
разницы между тем, что это было просто

715
00:21:47,840 --> 00:21:49,039
контекстное встраивание, так что у вас

716
00:21:49,039 --> 00:21:50,880
все еще было, потому что, как и глубокое

717
00:21:50,880 --> 00:21:52,400
обучение,

718
00:21:52,400 --> 00:21:55,039
исторически было весело создавать новые модели,

719
00:21:55,039 --> 00:21:56,159
так что у вас есть все эти

720
00:21:56,159 --> 00:21:56,720
компоненты

721
00:21:56,720 --> 00:21:59,120
вроде как блоки лего, у вас есть

722
00:21:59,120 --> 00:22:00,960
натяжные слои, подача слоев, нормализация слоев

723
00:22:14,080 --> 00:22:17,440
lstms и т.  elmo, на самом деле

724
00:22:17,440 --> 00:22:18,400
это не было ничего принципиально ничего не изменилось,

725
00:22:18,400 --> 00:22:20,720
это было просто потому, что

726
00:22:21,039 --> 00:22:22,240
вы просто загрузили это в свою существующую модель

727
00:22:22,240 --> 00:22:24,080
и получили самое современное для

728
00:22:24,080 --> 00:22:27,919
gpt-1, это было не

729
00:22:27,919 --> 00:22:29,280
очень похоже  эти вещи на самом деле не

730
00:22:29,280 --> 00:22:30,240
работали правильно, потому что это была

731
00:22:30,240 --> 00:22:31,440
языковая модель с написанием слева направо,

732
00:22:31,440 --> 00:22:32,640
и поэтому вы могли просто взять

733
00:22:32,640 --> 00:22:34,400
последний токен, а затем предсказать

734
00:22:34,400 --> 00:22:35,440
тест классификации,

735
00:22:35,440 --> 00:22:36,880
но на самом деле не имело никакого смысла

736
00:22:36,880 --> 00:22:39,039
предсказать гм как часть  речевых тегов,

737
00:22:39,039 --> 00:22:39,440
потому что

738
00:22:39,440 --> 00:22:41,440
для первого слова нет контекста, поэтому

739
00:22:41,440 --> 00:22:42,960
нет смысла предсказывать без

740
00:22:42,960 --> 00:22:44,320
контекста

741
00:22:44,320 --> 00:22:47,440
um with bert, причина, по которой это оказало такое

742
00:22:47,440 --> 00:22:49,360
сильное влияние,

743
00:22:49,360 --> 00:22:51,200
заключалась в том, что это было своего рода упрощением,

744
00:22:51,200 --> 00:22:52,480
и поэтому я не говорю, что это

745
00:22:52,480 --> 00:22:53,360
обязательно

746
00:22:53,360 --> 00:22:56,480
хорошо, потому что как  исследователь или что-

747
00:22:56,480 --> 00:22:57,280
то плохое,

748
00:22:57,280 --> 00:22:59,679
я так как исследователь, по

749
00:22:59,679 --> 00:23:01,120
иронии судьбы, конечная цель

750
00:23:01,120 --> 00:23:02,400
исследования часто похожа на исследование себя без

751
00:23:02,400 --> 00:23:03,200
работы, как

752
00:23:03,200 --> 00:23:05,360
будто вы знаете, если физик,

753
00:23:05,360 --> 00:23:06,640
я не говорю, что у Берта был

754
00:23:06,640 --> 00:23:08,320
кто-то  рядом с этим столкновением, как физик,

755
00:23:08,320 --> 00:23:09,440
который придумал великую

756
00:23:09,440 --> 00:23:10,240
теорию физики, как

757
00:23:10,240 --> 00:23:11,679
они хотели бы, это было бы

758
00:23:11,679 --> 00:23:13,280
как величайший момент в физике, но

759
00:23:13,280 --> 00:23:14,480
также это как бы

760
00:23:14,480 --> 00:23:16,159
исключило многие исследования, правильно,

761
00:23:16,159 --> 00:23:18,000
так что это вроде как  Конечная

762
00:23:18,000 --> 00:23:19,760
цель Ричарда просто как бы решила

763
00:23:19,760 --> 00:23:21,360
проблему правильно, так что у

764
00:23:21,360 --> 00:23:24,720
Берта вроде

765
00:23:24,720 --> 00:23:28,240
бы есть шаг, на котором теперь,

766
00:23:28,240 --> 00:23:29,520
как и все эти различные виды

767
00:23:29,520 --> 00:23:31,840
проблем, на самом деле он вроде

768
00:23:31,840 --> 00:23:34,799
как убит, как и большая часть необходимости делать

769
00:23:34,799 --> 00:23:36,400
дизайн архитектуры модели  что отчасти

770
00:23:36,640 --> 00:23:37,840
прискорбно, потому что это действительно

771
00:23:37,840 --> 00:23:40,320
весело, и

772
00:23:40,320 --> 00:23:42,559
это своего рода влияние, и я так, что

773
00:23:42,559 --> 00:23:43,440
я не собираюсь говорить,

774
00:23:43,440 --> 00:23:44,960
хорошее это или плохое влияние, это своего

775
00:23:44,960 --> 00:23:46,240
рода объективное воздействие, и вот почему это

776
00:23:46,240 --> 00:23:47,919
было так много  влияние, потому что

777
00:23:47,919 --> 00:23:51,279
это как бы оказало такое влияние

778
00:23:51,279 --> 00:23:53,600
на многие вещи, которые раньше были похожи на

779
00:23:53,600 --> 00:23:54,320
создание забавных

780
00:23:54,320 --> 00:23:56,799
моделей, которые вы знаете, это просто подкармливает его и

781
00:23:56,799 --> 00:23:57,600
использует один из этих

782
00:23:57,600 --> 00:23:59,039
четырех рецептов, и он вроде как просто работает

783
00:23:59,039 --> 00:24:01,760
для всех этих различных задач,

784
00:24:01,760 --> 00:24:03,760
поэтому  с точки зрения фактических эмпирических результатов,

785
00:24:03,760 --> 00:24:05,039
так что это было в то время, когда статья была

786
00:24:05,039 --> 00:24:08,080
опубликована, конечно,

787
00:24:08,080 --> 00:24:10,480
с тех пор все стало лучше, так что эта

788
00:24:10,480 --> 00:24:11,200
склеивающая задача

789
00:24:11,200 --> 00:24:13,600
представляет собой набор эм, они все вроде

790
00:24:13,600 --> 00:24:14,799
похожи в том, что они все

791
00:24:14,799 --> 00:24:16,400
пары предложений  или тест классификации предложений,

792
00:24:16,400 --> 00:24:19,520
например, мульти-нлай будет чем-то

793
00:24:19,520 --> 00:24:20,000
вроде

794
00:24:20,000 --> 00:24:21,279
холмов и гор, особенно

795
00:24:21,279 --> 00:24:22,880
освященных в джайнизме, а затем

796
00:24:22,880 --> 00:24:24,880
гипотеза, что джайнизм ненавидит природу,

797
00:24:24,880 --> 00:24:26,240
это противоречие, поэтому для того,

798
00:24:26,240 --> 00:24:28,720
чтобы модель НЛП могла понять

799
00:24:28,720 --> 00:24:29,200
это

800
00:24:29,200 --> 00:24:30,640
или иметь возможность  ответьте на этот вопрос правильно

801
00:24:30,640 --> 00:24:32,480
и дайте ему ярлык противоречия,

802
00:24:32,480 --> 00:24:33,520
он должен знать,

803
00:24:33,520 --> 00:24:37,120
что холмы и горы являются

804
00:24:37,120 --> 00:24:39,360
частью природы, освященные - это

805
00:24:39,360 --> 00:24:40,880
хорошо, а ненавидеть что-то -

806
00:24:40,880 --> 00:24:42,000
плохо, и уметь делать  все эти

807
00:24:42,000 --> 00:24:42,720
рассуждения верны,

808
00:24:42,720 --> 00:24:44,799
так что это довольно сложное рассуждение,

809
00:24:44,799 --> 00:24:46,080
как и для колы, вы должны быть в

810
00:24:46,080 --> 00:24:47,039
состоянии сказать,

811
00:24:47,039 --> 00:24:49,039
как фургон, грохотавший по дороге, по сравнению

812
00:24:49,039 --> 00:24:50,559
с машиной, гудящей по дороге,

813
00:24:50,559 --> 00:24:54,159
и так что эти штуки,

814
00:24:54,159 --> 00:24:56,559
э-э, вы знаете одну из них местному жителю

815
00:24:56,559 --> 00:24:57,200
говорящий по-английски

816
00:24:57,200 --> 00:24:58,480
звучит совершенно нормально, другой звучит

817
00:24:58,480 --> 00:25:00,400
странно, и поэтому он похож, и

818
00:25:00,400 --> 00:25:01,440
ни один из них не имеет очень большого количества правильных данных,

819
00:25:01,440 --> 00:25:02,000
поэтому вы должны иметь возможность

820
00:25:02,000 --> 00:25:03,520
обобщать только

821
00:25:03,520 --> 00:25:07,200
на нескольких тысячах примеров

822
00:25:09,840 --> 00:25:10,880
open ai

823
00:25:10,880 --> 00:25:14,000
uh он значительно превзошел open ai, который

824
00:25:14,000 --> 00:25:16,240
был предыдущим уровнем техники,

825
00:25:16,240 --> 00:25:19,440
а затем uh

826
00:25:19,440 --> 00:25:21,200
bert large, который был больше, конечно,

827
00:25:21,200 --> 00:25:22,559
получил лучшие результаты, что

828
00:25:22,559 --> 00:25:24,000
только удивительно, что он получил лучшие

829
00:25:24,000 --> 00:25:25,919
результаты по всем направлениям,

830
00:25:25,919 --> 00:25:27,360
включая очень крошечные данные  это всего

831
00:25:27,360 --> 00:25:28,960
несколько тысяч примеров, которые представляли собой своего

832
00:25:28,960 --> 00:25:29,840
рода более интересный

833
00:25:29,840 --> 00:25:31,520
результат, чем просто тот факт, что

834
00:25:31,520 --> 00:25:33,200
у него были лучшие результаты, потому что

835
00:25:33,200 --> 00:25:35,120
исторически, когда вы, э-э, вы

836
00:25:35,120 --> 00:25:38,000
знали  w практических правил о том, если у вас есть

837
00:25:38,000 --> 00:25:39,919
несколько примеров, как вы

838
00:25:39,919 --> 00:25:41,279
проектируете размер модели,

839
00:25:41,279 --> 00:25:42,799
оптимальный для этого, и поэтому, если вы

840
00:25:42,799 --> 00:25:44,960
не проводите предварительное обучение, например, если вы продолжаете

841
00:25:44,960 --> 00:25:45,919
увеличивать модель без

842
00:25:45,919 --> 00:25:46,720
предварительного обучения, в

843
00:25:46,720 --> 00:25:48,559
конечном итоге вы  получить худшие результаты,

844
00:25:48,559 --> 00:25:49,840
потому что ваша модель будет чрезмерно соответствовать вашим

845
00:25:49,840 --> 00:25:50,960
тренировочным данным

846
00:25:50,960 --> 00:25:52,080
с предварительным обучением,

847
00:25:52,080 --> 00:25:54,159
в любом случае вы в основном делаете только один проход в день,

848
00:25:54,159 --> 00:25:56,400
поэтому, похоже, почти нет предела тому,

849
00:25:56,400 --> 00:25:57,600
насколько большим вы можете его достичь

850
00:25:57,600 --> 00:25:59,039
и при этом получать хорошие результаты даже с

851
00:25:59,039 --> 00:26:00,480
крошечный объем данных точной настройки, и

852
00:26:00,480 --> 00:26:01,760
это действительно похоже на один из

853
00:26:01,760 --> 00:26:03,039
главных выводов,

854
00:26:03,039 --> 00:26:06,320
так что я не пойду, да, так

855
00:26:06,799 --> 00:26:10,640
э-э, команда 2.0, я так сказал, причина, по которой

856
00:26:10,640 --> 00:26:11,679
это числа,

857
00:26:11,679 --> 00:26:13,279
и эти ранги ниже, потому что я

858
00:26:13,279 --> 00:26:14,880
взял  снимок

859
00:26:14,880 --> 00:26:17,200
экрана после того, как это, вы знаете

860
00:26:17,200 --> 00:26:18,159
значительно после

861
00:26:18,159 --> 00:26:20,159
того, как группа других людей

862
00:26:20,159 --> 00:26:21,600
представила системы,

863
00:26:21,600 --> 00:26:25,760
но это вопрос и набор данных,

864
00:26:25,760 --> 00:26:26,960
так что это было бы похоже на то, какое действие

865
00:26:26,960 --> 00:26:28,240
мы предприняли, что вызвало второй нефтяной

866
00:26:28,240 --> 00:26:28,720
шок,

867
00:26:28,720 --> 00:26:30,799
так что в этом  случае нет  поверните направо,

868
00:26:30,799 --> 00:26:31,520
так что вы должны быть в состоянии

869
00:26:31,520 --> 00:26:32,559
предсказать, что на

870
00:26:32,559 --> 00:26:32,880
этом

871
00:26:32,880 --> 00:26:34,559
этапе ответа нет, поэтому мы должны либо

872
00:26:34,559 --> 00:26:36,000
предсказать ответ, либо сказать, что ответа нет

873
00:26:40,720 --> 00:26:42,159
что он был представлен

874
00:26:42,159 --> 00:26:43,440
примерно шестью пунктами, что, как вы знаете,

875
00:26:43,440 --> 00:26:44,320
довольно большой выигрыш,

876
00:26:44,320 --> 00:26:46,000
сейчас он вроде как вышел за пределы человеческого

877
00:26:46,000 --> 00:26:47,520
уровня,

878
00:26:47,520 --> 00:26:50,320
но в то время, да, он был большим,

879
00:26:50,320 --> 00:26:51,440
поэтому я как бы проведу несколько

880
00:26:51,440 --> 00:26:52,559
экспериментов по абляции

881
00:26:52,559 --> 00:26:53,440
или пройду через некоторые из  страхование отбеливания,

882
00:26:53,440 --> 00:26:55,600
так что здесь есть

883
00:26:55,600 --> 00:26:57,679
четыре вещи, которые я сравниваю, так

884
00:26:57,679 --> 00:26:58,720
что это все

885
00:26:58,720 --> 00:27:02,240
модели размера отрыжки, поэтому синяя линия

886
00:27:02,240 --> 00:27:02,880
- это своего

887
00:27:02,880 --> 00:27:06,640
рода модель на основе Берта, красная линия

888
00:27:06,640 --> 00:27:08,559
- если мы вынимаем следующее предсказание предложения,

889
00:27:08,559 --> 00:27:10,080
поэтому  в нашем случае, даже несмотря на то,

890
00:27:10,080 --> 00:27:11,200
что впоследствии люди говорили, что они

891
00:27:11,200 --> 00:27:12,480
не думают, что это важно

892
00:27:12,480 --> 00:27:13,679
в нашем случае, мы действительно измерили это,

893
00:27:13,679 --> 00:27:15,120
и оказалось, что казалось

894
00:27:15,760 --> 00:27:16,960
важным иметь эту задачу прогнозирования следующего предложения,

895
00:27:19,279 --> 00:27:22,399
особенно для

896
00:27:22,399 --> 00:27:23,919
типа qu  Например, эта задача,

897
00:27:23,919 --> 00:27:26,000
но она, кажется, немного помогает,

898
00:27:26,000 --> 00:27:27,440
по крайней мере, для всех четырех,

899
00:27:27,440 --> 00:27:29,279
ну, чтобы она была, так что это не

900
00:27:29,279 --> 00:27:30,559
дает силы в

901
00:27:30,559 --> 00:27:34,399
изучении какой-то модели, которая учится, а

902
00:27:34,399 --> 00:27:36,320
сустав, который изучает  отношения между

903
00:27:36,320 --> 00:27:38,559
предложениями, так что

904
00:27:38,559 --> 00:27:40,799
это тот, который делает

905
00:27:40,799 --> 00:27:43,360
сравнение яблок с яблоками между

906
00:27:43,360 --> 00:27:46,640
open ai и open asgpt1

907
00:27:46,640 --> 00:27:49,200
и bert правильно, потому что я также сделал

908
00:27:49,200 --> 00:27:50,799
модель больше, но не для базы, поэтому

909
00:27:50,799 --> 00:27:52,080
база птицы была точно такого же размера, но

910
00:27:52,080 --> 00:27:53,200
он обучал больше данных,

911
00:27:53,200 --> 00:27:54,960
поэтому, чтобы сделать его честным сравнением, я в

912
00:27:54,960 --> 00:27:56,559
основном переучил свою собственную

913
00:27:56,559 --> 00:27:57,760
реализацию

914
00:27:57,760 --> 00:28:00,880
open ai gpt one, и я,

915
00:28:00,880 --> 00:28:04,000
это эта желтая линия,

916
00:28:04,000 --> 00:28:05,760
и мы можем видеть, что на некоторых

917
00:28:05,760 --> 00:28:07,679
тестах это не так уж далеко, хотя это  на

918
00:28:07,679 --> 00:28:09,200
самом деле довольно большой разрыв, как этот, это

919
00:28:09,200 --> 00:28:09,919
падение,

920
00:28:09,919 --> 00:28:11,760
вы знаете, как четыре очка,

921
00:28:11,760 --> 00:28:13,840
что много, но на некоторых тестах, таких как отряд

922
00:28:13,840 --> 00:28:18,559
и mrc, было намного хуже, и поэтому

923
00:28:20,000 --> 00:28:22,320
для отряда это имеет смысл, потому что отряд

924
00:28:22,320 --> 00:28:24,240
- это обозначение, что это  маркировка промежутка ta  sk

925
00:28:24,240 --> 00:28:26,880
и поэтому, если вы оставили только контекст,

926
00:28:26,880 --> 00:28:27,919
тогда слова в начале в

927
00:28:27,919 --> 00:28:29,200
основном не имеют контекста,

928
00:28:29,200 --> 00:28:30,640
и поэтому вы просите его сделать

929
00:28:30,640 --> 00:28:32,240
разметку для слов с почти без

930
00:28:32,240 --> 00:28:33,279
контекста,

931
00:28:33,279 --> 00:28:34,960
поэтому это действительно не имеет никакого смысла, и

932
00:28:34,960 --> 00:28:36,399
поэтому, конечно, это  будет намного хуже,

933
00:28:36,399 --> 00:28:38,320
поэтому мы также добавили, чтобы было справедливо,

934
00:28:38,320 --> 00:28:39,840
мы также добавили lstm

935
00:28:39,840 --> 00:28:41,039
поверх него, который обучается с

936
00:28:41,039 --> 00:28:43,120
нуля, и это немного помогает

937
00:28:43,120 --> 00:28:45,039
в некоторых задачах, но в

938
00:28:45,039 --> 00:28:46,399
других не помогает  так что я могу прихлопнуть, это помогает,

939
00:28:46,399 --> 00:28:47,360
потому что теперь у вас есть

940
00:28:47,360 --> 00:28:49,279
двунаправленный контекст, но на mrbc,

941
00:28:49,279 --> 00:28:50,640
потому что это очень небольшая задача, у него есть только

942
00:28:50,640 --> 00:28:52,240
3000 помеченных примеров,

943
00:28:52,240 --> 00:28:55,279
это совсем не помогает, поэтому,

944
00:28:55,279 --> 00:28:56,559
если это действительно показывает, что эта

945
00:28:56,559 --> 00:28:58,240
модель массового языка и следующая

946
00:28:58,240 --> 00:28:58,880
предсказания

947
00:28:58,880 --> 00:29:00,880
важны, особенно модель массового

948
00:29:00,880 --> 00:29:03,440
языка,

949
00:29:03,840 --> 00:29:06,320
поэтому другая вещь, одна из других

950
00:29:06,320 --> 00:29:06,960
абляций,

951
00:29:06,960 --> 00:29:10,080
заключается в том, что, когда мы применяем

952
00:29:10,080 --> 00:29:12,320
модель массового языка, мы прогнозируем только

953
00:29:13,520 --> 00:29:15,279
15 слов в предложении, но когда вы

954
00:29:15,279 --> 00:29:16,480
делаете левостороннее  -правильная языковая модель ты

955
00:29:16,480 --> 00:29:17,520
прогнозирование каждого отдельного слова,

956
00:29:17,520 --> 00:29:19,840
обусловливающего все слова слева,

957
00:29:19,840 --> 00:29:22,080
поэтому один вопрос может заключаться в том, насколько

958
00:29:22,080 --> 00:29:23,840
это заставляет дольше сходиться,

959
00:29:23,840 --> 00:29:24,880
хотя в конечном итоге мы знаем, что оно

960
00:29:24,880 --> 00:29:26,480
сходится в гораздо лучшей точке,

961
00:29:26,480 --> 00:29:28,240
если у вас ограниченный бюджет на обучение,

962
00:29:28,240 --> 00:29:30,080
лучше ли  сделайте модель слева направо,

963
00:29:30,080 --> 00:29:33,679
и мы видим, что

964
00:29:33,679 --> 00:29:34,960
когда вы делаете эту модель на массовом языке,

965
00:29:34,960 --> 00:29:37,039
двунаправленность начинает улучшаться, как

966
00:29:37,039 --> 00:29:38,000
в самом начале, потому

967
00:29:38,000 --> 00:29:39,440
что вы делаете гораздо больше прогнозов,

968
00:29:39,440 --> 00:29:42,159
это правда, что лево-право  модель

969
00:29:42,159 --> 00:29:43,440
действительно работает лучше,

970
00:29:43,440 --> 00:29:46,480
как и для той же эпохи, но затем очень

971
00:29:46,480 --> 00:29:48,080
скоро, потому что

972
00:29:48,080 --> 00:29:48,960
двунаправленность настолько важна, что

973
00:29:48,960 --> 00:29:50,960
она начинает преобладать, и поэтому в

974
00:29:50,960 --> 00:29:52,720
основном лучше почти с самого начала

975
00:29:52,720 --> 00:29:54,720
делать двунаправленность, а затем это

976
00:29:54,720 --> 00:29:56,000
занимает больше времени  чтобы сойтись,

977
00:29:56,000 --> 00:29:57,440
но общая сходимость, конечно,

978
00:29:57,440 --> 00:29:59,919
намного выше,

979
00:30:00,080 --> 00:30:04,240
а затем, наконец, для

980
00:30:04,240 --> 00:30:08,720
этой эм абляции, мы можем видеть, что

981
00:30:08,720 --> 00:30:11,200
переход от меньшей модели со 100

982
00:30:11,200 --> 00:30:12,640
миллионами к 300 миллионам параметров помогает

983
00:30:12,640 --> 00:30:14,240
Многое, что неудивительно,

984
00:30:14,240 --> 00:30:15,600
но более удивительно то, что

985
00:30:15,600 --> 00:30:17,120
одна из этих кривых, это не они,

986
00:30:17,120 --> 00:30:18,399
не сопоставимы,

987
00:30:18,399 --> 00:30:21,440
ммм, вам не следует сравнивать

988
00:30:21,440 --> 00:30:22,000
кривые друг с другом,

989
00:30:22,000 --> 00:30:23,919
главное - смотреть на кривые как на

990
00:30:23,919 --> 00:30:25,120
функцию

991
00:30:25,120 --> 00:30:28,480
количества параметров, и вы

992
00:30:28,480 --> 00:30:31,360
увидите, что это всего

993
00:30:31,360 --> 00:30:32,159
лишь три тысячи

994
00:30:32,159 --> 00:30:34,640
помеченных примеров, а у этого

995
00:30:34,640 --> 00:30:35,520
400000 помеченных примеров,

996
00:30:35,520 --> 00:30:37,840
так что в обоих случаях кривые выглядят очень

997
00:30:37,840 --> 00:30:40,960
похожими, что удивительно, потому что

998
00:30:40,960 --> 00:30:42,240
вы знаете эмпирическое правило, что  вы

999
00:30:42,240 --> 00:30:43,600
собираетесь чрезмерно подогнать свои данные, если у вас

1000
00:30:43,600 --> 00:30:45,679
всего несколько цифр, несколько

1001
00:30:45,679 --> 00:30:47,120
примеров с пометками больше не соответствуют

1002
00:30:47,120 --> 00:30:50,000
действительности, и, как

1003
00:30:50,000 --> 00:30:51,120
вы знаете, эти и эти кривые

1004
00:30:51,120 --> 00:30:52,640
продолжают расти, так что сейчас с последующими

1005
00:30:52,640 --> 00:30:54,399
статьями, которые мы '  Я расскажу об

1006
00:30:54,399 --> 00:30:56,000
этом, вот этот большой: 300

1007
00:30:56,000 --> 00:30:57,039
миллионов параметров, люди подняли

1008
00:30:57,039 --> 00:30:58,320
до 11 миллиардов параметров

1009
00:30:58,320 --> 00:30:59,919
и все еще видели подобное поведение, которое вы все

1010
00:30:59,919 --> 00:31:01,519
еще видели, как кривые идут вверх и

1011
00:31:01,519 --> 00:31:02,799
получают, так что результаты, которые кажутся

1012
00:31:02,799 --> 00:31:03,760
сумасшедшими, потому что

1013
00:31:03,760 --> 00:31:06,480
сейчас  мы знаем, эм, вы знаете, в

1014
00:31:06,480 --> 00:31:08,799
основном почти нет предела,

1015
00:31:08,799 --> 00:31:10,080
поэтому я хочу поговорить еще о одной вещи,

1016
00:31:10,080 --> 00:31:11,919
прежде чем я расскажу о

1017
00:31:11,919 --> 00:31:14,640
том, что произошло, потому что burt um -

1018
00:31:14,640 --> 00:31:15,279
это вроде

1019
00:31:15,279 --> 00:31:18,480
как, хотя сам bert

1020
00:31:18,480 --> 00:31:21,519
um был в некотором смысле очень простым,

1021
00:31:21,519 --> 00:31:22,159
что

1022
00:31:22,159 --> 00:31:24,880
вы знаете  неплохо, это было очень

1023
00:31:24,880 --> 00:31:26,000
успешным сразу,

1024
00:31:26,000 --> 00:31:27,279
и вы знаете, что частью этого является

1025
00:31:27,279 --> 00:31:28,640
бренд Google, и, как вы знаете, у него

1026
00:31:28,640 --> 00:31:29,760
симпатичное имя и тому подобное, но я

1027
00:31:29,760 --> 00:31:30,799
думаю, что я

1028
00:31:30,799 --> 00:31:31,600
провел много времени с выпуском с открытым исходным кодом,

1029
00:31:31,600 --> 00:31:33,279
в частности  глядя на другие

1030
00:31:33,279 --> 00:31:34,399
выпуски с открытым исходным кодом

1031
00:31:34,399 --> 00:31:36,159
и выясняя, что людям не нравилось

1032
00:31:36,159 --> 00:31:38,320
в этих эм,

1033
00:31:38,320 --> 00:31:39,360
и поэтому я думаю, что это важно, например,

1034
00:31:39,360 --> 00:31:41,519
когда вы эээ, когда вы сезон

1035
00:31:41,519 --> 00:31:42,480
или даже

1036
00:31:42,480 --> 00:31:44,240
работаете в индустрии как a, и пытаетесь

1037
00:31:44,240 --> 00:31:46,080
выпустить что-то так  эм,

1038
00:31:46,080 --> 00:31:47,840
я как бы просто перечислил здесь вещи,

1039
00:31:47,840 --> 00:31:50,240
которые я считал важными,

1040
00:31:50,240 --> 00:31:52,720
например, почему он был успешным

1041
00:31:52,720 --> 00:31:53,360
по сравнению с

1042
00:31:53,360 --> 00:31:56,240
другими вещами, эм так, как будто я не

1043
00:31:56,240 --> 00:31:57,760
пытаюсь называть их просто для того, чтобы быть злым, потому что

1044
00:31:57,760 --> 00:31:59,679
но как открытый AI  -1 релиз  se был

1045
00:31:59,679 --> 00:32:01,120
действительно э-э, был не очень хорош,

1046
00:32:01,120 --> 00:32:02,559
а потом, как вы и сказали,

1047
00:32:02,559 --> 00:32:04,320
это было потому, что выпуск open eye

1048
00:32:04,320 --> 00:32:05,279
gbt2 был очень хорош,

1049
00:32:05,279 --> 00:32:08,559
и так что да, потому что его было очень

1050
00:32:08,559 --> 00:32:09,519
трудно запускать

1051
00:32:09,519 --> 00:32:10,880
и не было комментариев,

1052
00:32:10,880 --> 00:32:12,640
код тензорного потока был очень гм  как будто он

1053
00:32:12,640 --> 00:32:14,000
работал нормально, как я

1054
00:32:14,000 --> 00:32:16,640
его реплицировал, но, как и код тензорного потока, был

1055
00:32:16,640 --> 00:32:17,760
очень неидиоматическим,

1056
00:32:17,760 --> 00:32:18,880
он использовал всевозможные странные вещи,

1057
00:32:18,880 --> 00:32:20,320
питон был странным, не было комментариев,

1058
00:32:20,320 --> 00:32:21,679
в основном не было инструкций,

1059
00:32:21,679 --> 00:32:24,320
таких как um, а затем другие кодовые базы

1060
00:32:24,320 --> 00:32:25,200
также

1061
00:32:25,200 --> 00:32:26,640
uh  отчасти слишком большой, как будто люди просто

1062
00:32:26,640 --> 00:32:28,720
хотят сказать, будто мы хотим иметь

1063
00:32:28,720 --> 00:32:30,320
единую базу кода для всей нашей

1064
00:32:30,320 --> 00:32:32,559
языковой команды, и поэтому они просто

1065
00:32:32,559 --> 00:32:33,519
помещают вещи как часть этого, и людям

1066
00:32:33,519 --> 00:32:34,480
это тоже не очень нравится,

1067
00:32:34,480 --> 00:32:36,480
так что я был очень  настаивая на том, что мы сделаем

1068
00:32:36,480 --> 00:32:38,159
минимальный выпуск, так что вот так

1069
00:32:38,159 --> 00:32:39,279
мы просто выпустим burt, он не

1070
00:32:39,279 --> 00:32:40,480
будет частью чего-либо, там точно есть какие-то

1071
00:32:40,480 --> 00:32:42,320
внешние зависимости,

1072
00:32:42,320 --> 00:32:43,360
и это будет очень хорошо

1073
00:32:43,360 --> 00:32:45,279
прокомментировано, я думаю, что люди и я

1074
00:32:45,279 --> 00:32:47,039
Было также легко добавить

1075
00:32:47,039 --> 00:32:48,320
только часть моделирования и только

1076
00:32:48,320 --> 00:32:50,480
часть токенизации, и только

1077
00:32:50,480 --> 00:32:51,600
внешний интерфейс, который работает как

1078
00:32:51,600 --> 00:32:53,519
цикл обучения и как бы разделяет все

1079
00:32:53,519 --> 00:32:54,240
это,

1080
00:32:54,240 --> 00:32:56,080
потому что так, и поэтому я думаю

1081
00:32:56,080 --> 00:32:57,600
из-за этого  эм,

1082
00:32:57,600 --> 00:32:59,519
люди начали использовать его намного

1083
00:32:59,519 --> 00:33:01,200
быстрее, и, конечно, как будто вся

1084
00:33:01,200 --> 00:33:02,240
реклама помогла,

1085
00:33:02,240 --> 00:33:04,240
но я думаю, вы знаете, что это могло

1086
00:33:04,240 --> 00:33:05,360
бы легко

1087
00:33:05,360 --> 00:33:07,919
быть не таким успешным, если бы

1088
00:33:07,919 --> 00:33:09,840
вы знали, что это было сделано по-другому, так

1089
00:33:09,840 --> 00:33:11,200
что это просто

1090
00:33:11,200 --> 00:33:14,240
своего рода совет, так

1091
00:33:14,240 --> 00:33:17,519
что да, так что теперь я собираюсь поговорить о пяти

1092
00:33:17,519 --> 00:33:18,480
моделях, которые

1093
00:33:18,480 --> 00:33:20,159
вышли со времен Bert, которые все

1094
00:33:20,159 --> 00:33:21,840
улучшились по сравнению с Burt различными способами,

1095
00:33:21,840 --> 00:33:23,039
их было больше пяти,

1096
00:33:23,039 --> 00:33:24,480
но я собираюсь выделить эти пять, так что

1097
00:33:24,480 --> 00:33:26,799
я  думаю, что они

1098
00:33:26,799 --> 00:33:27,840
интересны многие из них пришли не из Google,

1099
00:33:27,840 --> 00:33:30,159
но это не потому, что это хорошо,

1100
00:33:30,159 --> 00:33:32,720
что многие из них участвовали в Google, я бы

1101
00:33:32,720 --> 00:33:34,320
сказал, что многие из них на самом деле не

1102
00:33:34,320 --> 00:33:37,039
были они были стажерами в Google

1103
00:33:37,039 --> 00:33:38,399
из разных университетов,

1104
00:33:38,399 --> 00:33:40,080
которые были  под руководством г  Исследователи oogle,

1105
00:33:40,080 --> 00:33:41,679
а также используют вычисления Google,

1106
00:33:41,679 --> 00:33:42,720
я имею в виду, что причина, по которой многие из них пришли

1107
00:33:42,720 --> 00:33:44,720
из Google, заключается в том, что, откровенно говоря, как и

1108
00:33:44,720 --> 00:33:45,600
все, кроме

1109
00:33:45,600 --> 00:33:47,120
Facebook, Google и Microsoft, на

1110
00:33:47,120 --> 00:33:48,960
самом деле не так много

1111
00:33:48,960 --> 00:33:51,440
людей, которые могут компании, у которых

1112
00:33:51,440 --> 00:33:53,039
есть ресурсы для обучения этого

1113
00:33:53,039 --> 00:33:56,240
огромного государства  -современных моделей, и

1114
00:33:56,480 --> 00:33:57,919
почти по необходимости он

1115
00:33:57,919 --> 00:33:59,840
будет поступать из одной из этих, эм,

1116
00:33:59,840 --> 00:34:02,960
одной из этих лабораторий, так что первой

1117
00:34:02,960 --> 00:34:05,200
была Роберта, и это, вероятно,

1118
00:34:05,200 --> 00:34:06,880
та, в которой было меньше всего, э-э,

1119
00:34:06,880 --> 00:34:08,159
нового материала.  это было действительно

1120
00:34:08,159 --> 00:34:10,320
просто э-э, и это был

1121
00:34:10,320 --> 00:34:11,679
университет Вашингтона, facebook

1122
00:34:11,679 --> 00:34:13,679
э, это вышло не так давно после Берта,

1123
00:34:13,679 --> 00:34:16,000
и они показали, что Берт

1124
00:34:16,000 --> 00:34:17,440
был действительно недостаточно обучен,

1125
00:34:17,440 --> 00:34:20,399
и поэтому в основном они взяли даже тот

1126
00:34:20,399 --> 00:34:22,000
же объем данных, который был

1127
00:34:22,000 --> 00:34:23,679
это было даже при том, что я сделал 40 эпох

1128
00:34:23,679 --> 00:34:25,440
на данных, если вы сделаете это примерно 200

1129
00:34:25,440 --> 00:34:27,199
эпох, вы получите еще лучшие результаты, например,

1130
00:34:27,199 --> 00:34:28,639
значительно

1131
00:34:28,639 --> 00:34:31,520
ммм, так что они, ну, в основном они обучили

1132
00:34:31,918 --> 00:34:33,679
больше электронных книг на тех же данных, и

1133
00:34:33,679 --> 00:34:34,719
они  ИСО показал, что больше данных помогает,

1134
00:34:34,719 --> 00:34:36,320
что тоже не удивительно,

1135
00:34:36,320 --> 00:34:37,918
и они улучшили маскировку и

1136
00:34:37,918 --> 00:34:39,440
предварительное обучение, используя пару

1137
00:34:39,440 --> 00:34:40,480
-тройку настроек для этого, и они,

1138
00:34:40,480 --> 00:34:42,239
и они, смогли получить

1139
00:34:42,239 --> 00:34:44,239
э-э ...  круто,

1140
00:34:44,239 --> 00:34:46,639
ну и так да, но это была довольно

1141
00:34:46,639 --> 00:34:49,199
простая статья,

1142
00:34:49,199 --> 00:34:51,839
поэтому следующая - xlnet, которую сделали

1143
00:34:53,040 --> 00:34:54,639
некоторые стажеры в CMU, когда они были в

1144
00:34:54,639 --> 00:34:56,480
Google Brain, и поэтому в ней действительно были

1145
00:34:56,480 --> 00:34:57,440
некоторые действительно

1146
00:34:57,440 --> 00:35:00,480
крутые изменения, поэтому

1147
00:35:00,480 --> 00:35:02,400
одно из них было ими использовано  этот

1148
00:35:02,400 --> 00:35:04,160
трансформатор xl, который на самом деле был

1149
00:35:04,160 --> 00:35:05,839
предшественником, созданный теми же людьми,

1150
00:35:05,839 --> 00:35:08,480
которые, э-э, где они просто проводили

1151
00:35:08,480 --> 00:35:10,640
тесты только для ссылок, выполняли предварительное обучение,

1152
00:35:10,640 --> 00:35:13,040
но самая большая из больших

1153
00:35:13,040 --> 00:35:14,320
инноваций трансформатора xl

1154
00:35:14,320 --> 00:35:15,680
- это идея встраивания относительного положения

1155
00:35:15,680 --> 00:35:17,520
и, следовательно,

1156
00:35:17,520 --> 00:35:19,760
с абсолютным  позиция встраивается

1157
00:35:19,760 --> 00:35:21,040
проблема в том, что

1158
00:35:21,040 --> 00:35:24,160
каждое слово выглядит так, как это слово,

1159
00:35:24,160 --> 00:35:26,000
это слово пять, это слово шесть,

1160
00:35:26,000 --> 00:35:27,119
и поэтому они встраиваются, поэтому они

1161
00:35:27,119 --> 00:35:27,760
обобщают,

1162
00:35:27,760 --> 00:35:29,920
но на практике есть квадратичное

1163
00:35:29,920 --> 00:35:31,200
количество отношений  например, как

1164
00:35:31,200 --> 00:35:33,520
слово 83 соотносится со словом 76, верно,

1165
00:35:33,520 --> 00:35:35,760
это так, и как только вы

1166
00:35:35,760 --> 00:35:37,359
увеличиваете количество, например, 500 на тысячу,

1167
00:35:37,359 --> 00:35:38,880
теперь у вас есть тысяча квадратов общих

1168
00:35:38,880 --> 00:35:40,079
отношений, например, вы должны сказать, как

1169
00:35:40,079 --> 00:35:41,760
слово 997 относится ко всему

1170
00:35:41,760 --> 00:35:43,040
правильному, и поэтому это, очевидно, не

1171
00:35:43,040 --> 00:35:46,240
оптимально  как только вы доберетесь до большого размера

1172
00:35:46,240 --> 00:35:49,280
и, таким образом, с помощью

1173
00:35:49,280 --> 00:35:50,880
встраивания относительного видения вы в

1174
00:35:50,880 --> 00:35:52,640
основном можете

1175
00:35:52,640 --> 00:35:55,680
сказать, сколько

1176
00:35:55,680 --> 00:35:58,480
собака уделяет горячее и сколько

1177
00:35:58,480 --> 00:36:00,480
слово собака должно уделять внимание предыдущему слову,

1178
00:36:00,480 --> 00:36:01,520
а затем вы получите, и они являются

1179
00:36:01,520 --> 00:36:03,280
нелинейными для или  так что сначала они линейны,

1180
00:36:03,760 --> 00:36:04,960
но затем вы их комбинируете, а затем вы

1181
00:36:04,960 --> 00:36:06,800
получаете нелинейную

1182
00:36:06,800 --> 00:36:08,079
контекстную ориентацию, затем вы делаете это

1183
00:36:08,079 --> 00:36:09,839
на многих уровнях, и это в конечном итоге

1184
00:36:10,160 --> 00:36:11,839
так, тогда вы говорите, как эта контекстная

1185
00:36:11,839 --> 00:36:13,119
организация уклонения, сколько это делает

1186
00:36:13,119 --> 00:36:14,480
обнаруживаете предыдущее слово,

1187
00:36:14,480 --> 00:36:16,000
и тогда вы как бы получаете возможность накапливаться,

1188
00:36:16,000 --> 00:36:17,280
и поэтому это гораздо лучше обобщается для

1189
00:36:17,280 --> 00:36:18,079
длинных последовательностей,

1190
00:36:18,079 --> 00:36:20,880
так что это крутое нововведение, а

1191
00:36:20,880 --> 00:36:21,680
затем другое,

1192
00:36:21,680 --> 00:36:23,520
которое относится к предварительному обучению и

1193
00:36:23,520 --> 00:36:25,119
не только сама модель

1194
00:36:25,119 --> 00:36:26,400
- это идея моделирования языка перестановок,

1195
00:36:26,400 --> 00:36:28,160
так что это немного сложно

1196
00:36:28,160 --> 00:36:30,640
объяснить,

1197
00:36:30,800 --> 00:36:33,200
я думаю, что в статье это объясняется очень

1198
00:36:33,200 --> 00:36:34,400
формально, я думаю,

1199
00:36:34,400 --> 00:36:37,760
и так, но в основном

1200
00:36:37,760 --> 00:36:39,119
есть трюк, когда в

1201
00:36:39,119 --> 00:36:41,119
языковой модели слева направо каждый

1202
00:36:41,119 --> 00:36:43,280
предсказание готового слова основано на

1203
00:36:43,280 --> 00:36:44,160
слове слева направо,

1204
00:36:44,160 --> 00:36:45,839
но представьте себе, что вместо того, чтобы

1205
00:36:45,839 --> 00:36:47,359
предсказывать все каждое слово, вы

1206
00:36:47,359 --> 00:36:48,800
можете использовать любую перестановку, так что это

1207
00:36:48,800 --> 00:36:49,839
похоже на то, что я собираюсь

1208
00:36:49,839 --> 00:36:51,359
предсказать первое слово, тогда я собираюсь

1209
00:36:51,359 --> 00:36:53,680
выбрать третье  слово, затем второе слово,

1210
00:36:53,680 --> 00:36:54,800
затем четвертое слово,

1211
00:36:54,800 --> 00:36:58,000
и так что это вполне допустимый способ, и

1212
00:36:58,000 --> 00:36:58,960
вы по-прежнему получаете распределение благосостояния и

1213
00:36:58,960 --> 00:36:59,920
вероятности, потому что он все

1214
00:36:59,920 --> 00:37:01,359
еще предсказывает одно слово за раз,

1215
00:37:01,359 --> 00:37:04,000
учитывая некоторую перестановку входных данных,

1216
00:37:04,000 --> 00:37:05,359
с преобразователями и вниманием, вы

1217
00:37:05,359 --> 00:37:06,800
действительно можете  делайте это очень эффективно,

1218
00:37:06,800 --> 00:37:08,160
просто маскируя вероятности вашего внимания,

1219
00:37:08,160 --> 00:37:10,000
и поэтому

1220
00:37:10,000 --> 00:37:12,400
каждое предложение, которое у вас есть, вы можете как

1221
00:37:12,400 --> 00:37:14,800
бы пробовать одну его перестановку,

1222
00:37:14,800 --> 00:37:18,079
и вы можете uh n

1223
00:37:18,079 --> 00:37:19,280
Теперь вы можете эффективно обучать

1224
00:37:19,280 --> 00:37:21,760
двунаправленную модель, потому что это слово

1225
00:37:21,760 --> 00:37:23,280
не будет зависеть от каждого кадра

1226
00:37:23,280 --> 00:37:24,400
в среднем каждое слово будет

1227
00:37:24,400 --> 00:37:25,599
обусловлено только в половине слов,

1228
00:37:25,599 --> 00:37:27,599
но это слово будет обусловлено тем, что вы

1229
00:37:27,599 --> 00:37:28,640
знаете

1230
00:37:28,640 --> 00:37:29,599
все эти слова слева  и все

1231
00:37:29,599 --> 00:37:30,640
эти слова справа, возможно, в них не будет

1232
00:37:30,640 --> 00:37:32,400
этих слов, но это нормально,

1233
00:37:32,400 --> 00:37:34,800
и вы получите гораздо лучшую эффективность выборки,

1234
00:37:34,800 --> 00:37:36,000
поэтому я подумал, что это

1235
00:37:36,000 --> 00:37:37,280
действительно умная идея,

1236
00:37:37,280 --> 00:37:39,520
и это было своего рода

1237
00:37:39,520 --> 00:37:40,880
основным нововведением xlnet

1238
00:37:40,880 --> 00:37:43,680
и  так что да, они в основном получают лучшую

1239
00:37:43,680 --> 00:37:44,960
эффективность выборки, потому

1240
00:37:44,960 --> 00:37:48,160
что они могут выполнять эту случайную

1241
00:37:48,160 --> 00:37:49,599
перестановку и как бы использовать это в своих интересах,

1242
00:37:49,599 --> 00:37:51,280
чтобы это не сработало с lftms,

1243
00:37:52,079 --> 00:37:55,119
потому что из-за этого порядка, а из-

1244
00:37:55,119 --> 00:37:56,320
за способа, которым выполняется маскирование  в

1245
00:37:56,320 --> 00:37:59,200
трансформаторах это просто эм,

1246
00:37:59,200 --> 00:38:00,560
это просто маска для

1247
00:38:00,560 --> 00:38:02,560
привлечения внимания, так что на самом деле это работает

1248
00:38:02,560 --> 00:38:03,599
очень хорошо,

1249
00:38:03,599 --> 00:38:06,720
и они также получили,

1250
00:38:06,720 --> 00:38:09,200
так что да, цифры по сравнению с Робертой

1251
00:38:09,200 --> 00:38:10,079
они на самом деле оказались довольно

1252
00:38:10,079 --> 00:38:10,880
похожими

1253
00:38:10,880 --> 00:38:14,079
хм,

1254
00:38:14,079 --> 00:38:14,960
но многие из этих вещей трудно

1255
00:38:14,960 --> 00:38:15,920
сравнивать, потому что люди меняют

1256
00:38:15,920 --> 00:38:17,359
набор данных и

1257
00:38:17,359 --> 00:38:19,200
меняют размер модели, поэтому

1258
00:38:19,200 --> 00:38:20,640
трудно сравнивать яблоки с яблоками, но

1259
00:38:20,640 --> 00:38:21,760
эти две техники оказались

1260
00:38:21,760 --> 00:38:22,800
довольно похожими, но я думаю, вы знаете

1261
00:38:22,800 --> 00:38:25,040
exonet  было больше инноваций с точки зрения

1262
00:38:25,839 --> 00:38:30,000
техники эм, так что, Альберт, это

1263
00:38:30,000 --> 00:38:32,800
называется легким занятием для самоуправляемого

1264
00:38:32,800 --> 00:38:33,280
обучения,

1265
00:38:33,280 --> 00:38:36,400
и поэтому здесь также есть пара крутых

1266
00:38:36,400 --> 00:38:37,040
нововведений,

1267
00:38:37,040 --> 00:38:39,520
и поэтому идея здесь действительно массового

1268
00:38:39,520 --> 00:38:40,480
обмена параметрами

1269
00:38:40,480 --> 00:38:42,079
с идеей в том, что если вы поделитесь

1270
00:38:42,079 --> 00:38:43,200
параметров вы не получите лучшую

1271
00:38:43,200 --> 00:38:44,560
языковую модель, но вы получите

1272
00:38:44,560 --> 00:38:45,200
лучшую

1273
00:38:45,200 --> 00:38:46,640
эффективность выборки, вы получите меньше

1274
00:38:46,640 --> 00:38:48,160
переобучения, когда вы правильно настроите,

1275
00:38:48,160 --> 00:38:49,280
потому что, если у вас

1276
00:38:49,280 --> 00:38:50,720
есть миллиард параметров, и вы точно настраиваете

1277
00:38:50,720 --> 00:38:52,640
их на 300 на данных  набор с

1278
00:38:52,640 --> 00:38:53,119
тысячей

1279
00:38:53,119 --> 00:38:54,480
размеченных примеров вы все равно

1280
00:38:54,480 --> 00:38:56,240
собираетесь очень быстро переобучать правильно, но если у вас

1281
00:38:56,800 --> 00:38:58,320
есть гораздо меньшее количество параметров,

1282
00:38:58,320 --> 00:38:59,359
вы получите меньше переобучения, поэтому

1283
00:38:59,359 --> 00:39:00,480
мы получим аналогичный p  эффективная модель с

1284
00:39:00,480 --> 00:39:01,280
меньшим количеством параметров,

1285
00:39:01,280 --> 00:39:03,680
вы получите меньше переоснащения, и

1286
00:39:03,680 --> 00:39:04,480
поэтому они,

1287
00:39:04,480 --> 00:39:07,200
ну, так что два основных нововведения заключались в том, что

1288
00:39:07,200 --> 00:39:08,160
вместо использования слова,

1289
00:39:08,160 --> 00:39:09,520
потому что таблица формулировок велика, правильно,

1290
00:39:09,520 --> 00:39:11,280
потому что это размер вашего

1291
00:39:11,280 --> 00:39:12,480
словаря, количество

1292
00:39:12,480 --> 00:39:15,440
слов  штук, умноженных на скрытый размер, и

1293
00:39:15,440 --> 00:39:16,400
поэтому он будет намного больше, чем

1294
00:39:17,040 --> 00:39:19,359
скрытый скрытый слой, поэтому во-первых,

1295
00:39:19,359 --> 00:39:21,119
они использовали факторизованную таблицу встраивания,

1296
00:39:21,119 --> 00:39:22,400
поэтому, если у них был скрытый размер в

1297
00:39:22,400 --> 00:39:25,680
тысячу, они

1298
00:39:25,680 --> 00:39:28,640
использовали только 128-мерное встраивание ввода,

1299
00:39:28,640 --> 00:39:29,680
а затем они  спроектировал это до

1300
00:39:29,680 --> 00:39:30,400
тысячи,

1301
00:39:30,400 --> 00:39:34,640
используя матрицу, и поэтому

1302
00:39:34,640 --> 00:39:36,880
вместо 1024 на сто

1303
00:39:36,880 --> 00:39:38,640
тысяч у них будет 128 на 100000

1304
00:39:38,640 --> 00:39:39,760
плюс 1024 на

1305
00:39:39,760 --> 00:39:41,520
128, и вы умножите их

1306
00:39:41,520 --> 00:39:43,280
вместе и умножите две матрицы

1307
00:39:43,280 --> 00:39:44,800
вместе, и тогда фактически у вас будет

1308
00:39:44,800 --> 00:39:46,880
10 24  на сто тысяч

1309
00:39:46,880 --> 00:39:49,359
встраиваемых матриц, но у вас гораздо

1310
00:39:49,359 --> 00:39:50,640
меньше параметров, но вы

1311
00:39:50,640 --> 00:39:52,320
хорошо связываете параметры, на самом деле

1312
00:39:52,320 --> 00:39:53,280
это не проблема, но вы выполняете

1313
00:39:53,280 --> 00:39:54,320
уменьшение параметров

1314
00:39:54,320 --> 00:39:57,359
на умном способе другой - это

1315
00:39:57,359 --> 00:39:58,960
межуровневое разделение параметров, так что это

1316
00:39:58,960 --> 00:40:02,160
похоже на простое, и это также

1317
00:40:02,160 --> 00:40:03,599
было сделано в предыдущих статьях,

1318
00:40:03,599 --> 00:40:06,640
особенно универсальный трансформатор,

1319
00:40:06,640 --> 00:40:08,880
и идея состоит в том, что вы запускаете кучу

1320
00:40:08,880 --> 00:40:09,839
слоев трансформатора, но

1321
00:40:09,839 --> 00:40:11,359
все  скажем, если у вас есть 12 слоев, все

1322
00:40:11,359 --> 00:40:12,720
12 слоев просто имеют одни и те же параметры,

1323
00:40:13,359 --> 00:40:16,880
и так что в итоге у вас может

1324
00:40:16,880 --> 00:40:17,119
быть

1325
00:40:17,119 --> 00:40:20,880
гораздо большая модель с

1326
00:40:20,880 --> 00:40:22,400
меньшим количеством параметров, чем у птицы, и поэтому

1327
00:40:22,400 --> 00:40:24,160
вы получаете меньше переобучения,

1328
00:40:24,160 --> 00:40:26,720
и поэтому у них есть  современное состояние по

1329
00:40:26,720 --> 00:40:27,520
сравнению с

1330
00:40:27,520 --> 00:40:30,880
x в этом и Роберта, но одна важная

1331
00:40:30,880 --> 00:40:32,560
вещь, которую следует иметь в виду, заключается в том, что Альберт

1332
00:40:32,560 --> 00:40:34,079
легкий с точки зрения параметров, а не с

1333
00:40:34,079 --> 00:40:35,040
точки зрения скорости,

1334
00:40:35,040 --> 00:40:38,160
поэтому

1335
00:40:38,160 --> 00:40:42,000
для смешанной модели,

1336
00:40:42,319 --> 00:40:45,440
которая на самом деле сопоставима с  но

1337
00:40:48,720 --> 00:40:50,560
они на самом деле им немного понравились эта

1338
00:40:50,560 --> 00:40:52,960
модель, и эта модель была

1339
00:40:52,960 --> 00:40:54,240
примерно такой же, но эта была на самом деле

1340
00:40:54,240 --> 00:40:57,200
медленнее, так что только когда они начали

1341
00:40:57,200 --> 00:40:59,280
делать модели, которые были намного больше с

1342
00:40:59,280 --> 00:41:00,720
точки зрения вычислений,

1343
00:41:00,720 --> 00:41:03,760
чем bert, но выполняли  больше привязки параметров,

1344
00:41:03,760 --> 00:41:05,359
тогда они начали получать хорошие результаты,

1345
00:41:05,359 --> 00:41:07,680
и

1346
00:41:07,680 --> 00:41:09,520
поэтому последствия этого заключаются в том, что, как и

1347
00:41:09,520 --> 00:41:10,560
вы, вы можете уменьшить количество

1348
00:41:10,560 --> 00:41:11,200
параметров,

1349
00:41:11,200 --> 00:41:14,960
но все же, гм, никто не понял, как

1350
00:41:14,960 --> 00:41:16,000
уменьшить количество

1351
00:41:16,000 --> 00:41:17,119
требуемых предварительных вычислений,

1352
00:41:17,119 --> 00:41:20,560
что вы знаете  вроде прискорбно, так

1353
00:41:20,560 --> 00:41:23,680
что следующий - t5, который

1354
00:41:23,680 --> 00:41:24,960
исследует пределы передачи

1355
00:41:24,960 --> 00:41:26,079
обучения с унифицированным первым обнаружением текста,

1356
00:41:26,079 --> 00:41:28,240
так что это

1357
00:41:28,240 --> 00:41:31,200
была статья мозга Google и других групп

1358
00:41:31,200 --> 00:41:31,760
в Google,

1359
00:41:31,760 --> 00:41:33,599
где они используют только они использовали

1360
00:41:33,599 --> 00:41:35,520
много вычислений, и они и  они сделали тонны

1361
00:41:35,520 --> 00:41:37,200
абляции на

1362
00:41:37,200 --> 00:41:38,800
предтренировке им не понравились они

1363
00:41:38,800 --> 00:41:40,000
не их цель заключалась в том, чтобы придумать что-то

1364
00:41:40,000 --> 00:41:40,880
с какой-то

1365
00:41:40,880 --> 00:41:42,720
супер умной новой техникой предтренировки

1366
00:41:42,720 --> 00:41:44,640
правильно на самом деле просто аккуратно аблятировать

1367
00:41:44,640 --> 00:41:45,599
каждый аспект

1368
00:41:45,599 --> 00:41:46,880
насколько важен размер модели

1369
00:41:46,880 --> 00:41:48,000
насколько важно обучение, насколько важна

1370
00:41:48,000 --> 00:41:49,200
чистота данных,

1371
00:41:49,200 --> 00:41:50,480
например, насколько важен точный способ

1372
00:41:50,480 --> 00:41:51,680
достижения цели перед обучением,

1373
00:41:51,680 --> 00:41:53,119
например, как выполняется маскировка, например,

1374
00:41:53,119 --> 00:41:54,880
сколько интервалов  вы маскируете, и поэтому они

1375
00:41:54,880 --> 00:41:56,480
хотели как бы очень

1376
00:41:56,480 --> 00:41:59,520
четко сделать это, и они также хотели

1377
00:41:59,520 --> 00:42:00,880
раздвинуть границы размера и сказать, что

1378
00:42:00,880 --> 00:42:01,760
произойдет, если у нас есть

1379
00:42:03,760 --> 00:42:06,160
правильные 300 миллионов миллиардов 10 миллиардов параметров,

1380
00:42:06,160 --> 00:42:08,720
а затем они сделали тонны и

1381
00:42:08,720 --> 00:42:10,800
тонны  абляции,

1382
00:42:10,800 --> 00:42:11,760
и они получили современное состояние и все такое,

1383
00:42:11,760 --> 00:42:12,560
и они все еще сидят

1384
00:42:12,560 --> 00:42:13,839
там, и все,

1385
00:42:13,839 --> 00:42:16,400
и результаты, хотя и немного

1386
00:42:16,400 --> 00:42:17,599
мрачные,

1387
00:42:17,599 --> 00:42:21,680
в том смысле, что на самом деле ничего не

1388
00:42:21,680 --> 00:42:23,920
имело значения, кроме как сделать данные такими,

1389
00:42:23,920 --> 00:42:25,200
как все абляции, которых

1390
00:42:25,200 --> 00:42:27,920
не было  как, о, вы знаете, что Берт

1391
00:42:27,920 --> 00:42:29,359
все сделал идеально, это

1392
00:42:29,359 --> 00:42:31,200
было неважно, как вы могли бы сделать 20

1393
00:42:31,200 --> 00:42:33,839
25, вы можете сделать этот рецепт тонкой настройки,

1394
00:42:33,839 --> 00:42:34,880
этот функциональный

1395
00:42:34,880 --> 00:42:36,560
рецепт, похоже, все, что действительно важно, - это

1396
00:42:36,560 --> 00:42:37,760
увеличение модели и торговля

1397
00:42:37,760 --> 00:42:38,319
большим количеством данных

1398
00:42:38,319 --> 00:42:42,480
и  чистые данные, ну и так

1399
00:42:42,480 --> 00:42:44,480
да, это немного мрачная

1400
00:42:44,480 --> 00:42:46,800
бумага, если

1401
00:42:46,800 --> 00:42:48,880
вы надеетесь, что существует какая

1402
00:42:48,880 --> 00:42:49,920
-то предварительная тренировочная

1403
00:42:49,920 --> 00:42:51,119
техника, которая супер

1404
00:42:51,119 --> 00:42:52,560
вычислительно эффективна, а также может

1405
00:42:52,560 --> 00:42:54,160
познакомить вас с очень впечатляющим r  результаты,

1406
00:42:54,160 --> 00:42:55,520
которых я не говорю, что их нет, но,

1407
00:42:55,520 --> 00:42:58,079
как и большинство этих доказательств, это

1408
00:42:58,079 --> 00:43:00,319
указывает на это, так что единственный вид

1409
00:43:00,319 --> 00:43:02,160
новейшей статьи, которая, возможно, является наиболее

1410
00:43:02,160 --> 00:43:03,280
позитивной в этом направлении,

1411
00:43:03,280 --> 00:43:06,640
- это статья под названием elektra,

1412
00:43:06,640 --> 00:43:10,960
ну и так  так что это делает э-э

1413
00:43:10,960 --> 00:43:13,280
Кев Кларк отсюда и э-э и гугл

1414
00:43:13,280 --> 00:43:15,200
мозг, и так

1415
00:43:15,200 --> 00:43:16,640
что да, и это довольно умная

1416
00:43:16,640 --> 00:43:18,880
идея, так что в основном их идея заключается в том,

1417
00:43:18,880 --> 00:43:22,240
чтобы вместо обучения вместо

1418
00:43:22,240 --> 00:43:23,280
обучения генерировать результат,

1419
00:43:23,280 --> 00:43:24,400
вы просто тренируете его как

1420
00:43:24,400 --> 00:43:26,319
дискриминатор  Итак, у

1421
00:43:26,319 --> 00:43:29,200
вас есть локальная языковая модель, вы выполняете

1422
00:43:29,200 --> 00:43:30,560
некоторую маскировку, у вас есть локальная языковая

1423
00:43:30,560 --> 00:43:32,319
модель, которая заменяет ее,

1424
00:43:32,319 --> 00:43:33,520
а затем вы пытаетесь различить, является ли

1425
00:43:33,520 --> 00:43:34,880
она оригинальной или нет, и поэтому

1426
00:43:34,880 --> 00:43:36,000
идея здесь в том, что

1427
00:43:36,000 --> 00:43:39,200
вы делаете много,

1428
00:43:39,200 --> 00:43:40,480
 вы получаете лучшую

1429
00:43:40,480 --> 00:43:42,480
эффективность выборки для предварительного обучения,

1430
00:43:42,480 --> 00:43:45,680
потому что вы предсказываете каждое э-э,

1431
00:43:45,680 --> 00:43:47,839
каждое слово, что на самом деле я имею в виду, что я

1432
00:43:47,839 --> 00:43:48,960
точно не знаю, почему это будет так

1433
00:43:48,960 --> 00:43:50,880
отличаться от from from, потому что Берт

1434
00:43:50,880 --> 00:43:52,480
все еще э-э,

1435
00:43:52,480 --> 00:43:54,079
с точки зрения  потому что ты не представляешь  зашнуровать

1436
00:43:54,079 --> 00:43:55,200
эр с маской с каждым словом, которое

1437
00:43:55,200 --> 00:43:56,880
вы также случайным образом повредите,

1438
00:43:56,880 --> 00:43:58,319
но самая большая разница в

1439
00:43:58,319 --> 00:44:00,240
том,

1440
00:44:00,240 --> 00:44:01,760
что они как бы контекстно

1441
00:44:01,760 --> 00:44:03,680
заменены, поэтому я конвертирую, когда я выполнял

1442
00:44:03,680 --> 00:44:05,599
случайную маскировку, и заменяю случайным

1443
00:44:05,599 --> 00:44:06,720
словом, это было действительно  случайное слово, поэтому в

1444
00:44:06,720 --> 00:44:08,000
большинстве случаев совершенно тривиально

1445
00:44:08,000 --> 00:44:09,359
сказать, что это было неправильное слово,

1446
00:44:09,359 --> 00:44:10,640
вы не обязательно знали, какое слово

1447
00:44:10,640 --> 00:44:11,520
следует заменить,

1448
00:44:11,520 --> 00:44:13,680
но в этом случае они фактически использовали

1449
00:44:13,680 --> 00:44:15,520
намеренно слабую, но все же нетривиальную

1450
00:44:15,520 --> 00:44:17,119
языковую модель для прогнозирования  какое слово так

1451
00:44:17,119 --> 00:44:18,720
похоже на это локально имеет смысл, повар

1452
00:44:18,720 --> 00:44:19,359
съел блюдо,

1453
00:44:19,359 --> 00:44:21,359
но это не имеет никакого смысла, так как

1454
00:44:21,359 --> 00:44:22,720
очень сильная модель не может предсказать это

1455
00:44:22,720 --> 00:44:23,359
правильно, так

1456
00:44:23,359 --> 00:44:24,640
что идея заключается в том, что вы используете слабую

1457
00:44:24,640 --> 00:44:26,960
модель, чтобы сделать замену, чтобы

1458
00:44:26,960 --> 00:44:28,880
использовать, когда  вы тренируете сильную модель для

1459
00:44:28,880 --> 00:44:33,119
этого, поэтому эти результаты такие, что

1460
00:44:33,119 --> 00:44:34,480
я знаю, что это большая таблица, но эти

1461
00:44:34,480 --> 00:44:36,400
результаты определенно

1462
00:44:36,400 --> 00:44:39,440
положительны по сравнению с предыдущими

1463
00:44:39,440 --> 00:44:41,359
результатами с точки зрения вычислений по сравнению с

1464
00:44:41,359 --> 00:44:44,960
такими, как если бы

1465
00:44:44,960 --> 00:44:49,440
мы сравниваем эту строку так, что она составляет

1466
00:44:49,440 --> 00:44:51,040
одну десятую вычислений от

1467
00:44:51,040 --> 00:44:52,560
большого берта до базы, которая также составляет одну десятую часть

1468
00:44:52,560 --> 00:44:53,359
компьютера, но большая,

1469
00:44:53,359 --> 00:44:54,560
это, безусловно, намного лучше, чем

1470
00:44:54,560 --> 00:44:57,119
база отрыжки,

1471
00:44:57,119 --> 00:45:01,119
но когда они,

1472
00:45:01,119 --> 00:45:03,839
если вы, но с точки зрения состояния  искусство

1473
00:45:03,839 --> 00:45:04,960
ммм,

1474
00:45:04,960 --> 00:45:07,359
когда они знают то же количество

1475
00:45:07,359 --> 00:45:09,839
вычислений, что и их большой причал,

1476
00:45:09,839 --> 00:45:11,680
который является этим, или по сравнению с другими

1477
00:45:11,680 --> 00:45:13,119
современными моделями, они не

1478
00:45:13,119 --> 00:45:14,319
для того, чтобы получить современное состояние или

1479
00:45:14,319 --> 00:45:15,280
становятся похожими, чтобы сказать, что они в

1480
00:45:15,280 --> 00:45:16,800
основном должны выполнять столько вычислений, сколько

1481
00:45:16,800 --> 00:45:18,240
современное состояние, так что, например, четыре четыре четыре

1482
00:45:18,240 --> 00:45:19,599
x пять целых четыре десятых x,

1483
00:45:19,599 --> 00:45:22,000
я имею в виду, что при уменьшении значений они

1484
00:45:22,000 --> 00:45:23,440
смогли добиться большего, но это все еще

1485
00:45:23,440 --> 00:45:25,119
довольно большой  разрыв, например, четыре балла

1486
00:45:25,119 --> 00:45:27,520
ммм, так что это положительно, но это

1487
00:45:27,520 --> 00:45:28,560
не совсем похоже

1488
00:45:28,560 --> 00:45:31,599
на серебряную пулю с точки зрения

1489
00:45:31,599 --> 00:45:35,119
демонстрации того, что мы можем, вы знаете

1490
00:45:35,119 --> 00:45:38,480
модели перед тренировкой намного лучше

1491
00:45:38,480 --> 00:45:41,200
по более низкой цене, так что это последнее, о чем я

1492
00:45:41,200 --> 00:45:42,960
хочу поговорить  гм,

1493
00:45:42,960 --> 00:45:44,400
как мы на самом деле обслуживаем эти модели,

1494
00:45:44,400 --> 00:45:46,560
потому что вы знаете, что я сказал, что li

1495
00:45:46,560 --> 00:45:48,560
они невероятно дороги в обучении,

1496
00:45:48,560 --> 00:45:50,480
и

1497
00:45:50,480 --> 00:45:51,440
никто не смог придумать, как сделать

1498
00:45:51,440 --> 00:45:54,160
это быстрее, но вы знаете, что они

1499
00:45:54,160 --> 00:45:55,920
используются повсюду, так что, как

1500
00:45:55,920 --> 00:45:57,839
вы знаете, есть новости,

1501
00:45:57,839 --> 00:45:59,680
Google улучшил 10

1502
00:45:59,680 --> 00:46:01,119
поисковых запросов по языкам  посылают

1503
00:46:01,119 --> 00:46:02,400
поздороваться с bert, а затем приносят говорит, что

1504
00:46:02,400 --> 00:46:03,680
он применяется с апреля,

1505
00:46:03,680 --> 00:46:05,520
так что это в прямом эфире в

1506
00:46:05,520 --> 00:46:06,800
поиске google и поиске bing, и поэтому они

1507
00:46:06,800 --> 00:46:08,400
похожи на действительно низкие службы ANC, которые

1508
00:46:08,400 --> 00:46:09,920
имеют задержку в несколько миллисекунд,

1509
00:46:09,920 --> 00:46:12,960
и они обслуживают  вы знаете

1510
00:46:12,960 --> 00:46:15,760
миллиарды запросов в день,

1511
00:46:15,760 --> 00:46:18,079
так как они это делают? Это

1512
00:46:18,079 --> 00:46:18,960
похоже на

1513
00:46:18,960 --> 00:46:22,000
то, что вы знаете, что Google и

1514
00:46:22,000 --> 00:46:23,040
Microsoft тратят миллиарды долларов на

1515
00:46:23,040 --> 00:46:24,480
оборудование, чем они являются, но не только

1516
00:46:24,480 --> 00:46:25,200
для этого

1517
00:46:28,800 --> 00:46:29,760
будет ли это стоить миллиарды

1518
00:46:29,760 --> 00:46:30,960
долларов, если вы на

1519
00:46:30,960 --> 00:46:32,000
самом деле обслуживаете берт,

1520
00:46:32,000 --> 00:46:33,920
но мы обслуживаем э-э, не вместо того,

1521
00:46:33,920 --> 00:46:35,440
чтобы использовать, мы используем

1522
00:46:35,440 --> 00:46:36,400
правильную модельную дистилляцию, так что это было

1523
00:46:36,400 --> 00:46:37,599
уже какое-то время

1524
00:46:37,599 --> 00:46:41,280
u  м, так что это называется дистилляция или модельное

1525
00:46:41,280 --> 00:46:42,079
сжатие.

1526
00:46:42,079 --> 00:46:44,079
Одной из первых статей была эта

1527
00:46:44,079 --> 00:46:45,359
модель сжатия.

1528
00:46:49,200 --> 00:46:51,200
Я забыл, какая именно задача.

1529
00:46:53,440 --> 00:46:54,800
версия

1530
00:46:54,800 --> 00:46:57,119
не версий, а более известная статья

1531
00:46:57,839 --> 00:47:00,240
о дистилляции, но на самом деле

1532
00:47:00,240 --> 00:47:01,280
та версия,

1533
00:47:01,280 --> 00:47:02,720
которую мы используем в Google, в той версии,

1534
00:47:02,720 --> 00:47:03,839
которую использует большинство людей, когда они говорят, что

1535
00:47:03,839 --> 00:47:04,880
дистилляция моделей для

1536
00:47:04,880 --> 00:47:08,480
предварительно обученных языковых моделей, это  эм,

1537
00:47:08,480 --> 00:47:10,160
это очень простая техника, но

1538
00:47:10,160 --> 00:47:11,520
легко неверно истолковать то,

1539
00:47:11,520 --> 00:47:14,960
что мы имеем в виду, поэтому мы делаем

1540
00:47:14,960 --> 00:47:15,760
предварительное обучение,

1541
00:47:15,760 --> 00:47:16,800
мы обучаем новейшие модели,

1542
00:47:16,800 --> 00:47:18,800
те из них, которые мы больше всего можем себе позволить,

1543
00:47:18,800 --> 00:47:19,280
тренировать

1544
00:47:19,280 --> 00:47:20,480
правильно, потому что, конечно, мы можем  просто увеличиваем

1545
00:47:20,480 --> 00:47:22,319
его, но мы устанавливаем некоторый бюджет, вы

1546
00:47:22,319 --> 00:47:23,520
знаете, что мы хотим тренировать его в течение дня на

1547
00:47:23,520 --> 00:47:25,520
определенном количестве tpus,

1548
00:47:25,520 --> 00:47:27,359
а затем мы точно настраиваем его так, чтобы мы

1549
00:47:27,359 --> 00:47:28,800
получили модель с максимальной точностью, и

1550
00:47:28,800 --> 00:47:29,680
это наша модель учителя, это

1551
00:47:29,680 --> 00:47:30,400
exp  Следовательно,

1552
00:47:30,400 --> 00:47:32,160
у нас есть большой объем немаркированного

1553
00:47:32,160 --> 00:47:34,400
ввода, который обычно используется для большинства

1554
00:47:34,400 --> 00:47:36,319
отраслевых приложений, у вас есть немаркированный

1555
00:47:36,319 --> 00:47:37,520
ввод, потому что вы знаете, что в

1556
00:47:37,520 --> 00:47:38,319
поиске у вас есть

1557
00:47:38,319 --> 00:47:39,680
это то, что они используют поиск, это

1558
00:47:39,680 --> 00:47:40,559
то, что они нажимали, это то, как он

1559
00:47:40,559 --> 00:47:41,760
ищет они  вы обучены,

1560
00:47:41,760 --> 00:47:44,640
а затем вы можете просто взять их,

1561
00:47:45,040 --> 00:47:47,440
а затем вы просто помечаете ими свои

1562
00:47:47,440 --> 00:47:49,119
примеры,

1563
00:47:49,119 --> 00:47:50,880
чтобы вы могли получить миллиарды из них, если вы

1564
00:47:50,880 --> 00:47:52,640
действительно запускаете реальный сервис,

1565
00:47:52,640 --> 00:47:54,960
а затем вы, так что тогда вы запускаете

1566
00:47:54,960 --> 00:47:55,680
их,

1567
00:47:55,680 --> 00:47:58,400
вы знаете  запросить пары ответов через вашего

1568
00:47:58,400 --> 00:47:59,359
учителя,

1569
00:47:59,359 --> 00:48:01,440
и вы получите псевдометку и просто

1570
00:48:01,440 --> 00:48:02,960
тренируете гораздо меньшую модель, что

1571
00:48:02,960 --> 00:48:04,480
много значит, например, в 50 раз

1572
00:48:04,480 --> 00:48:06,640
меньше, чтобы

1573
00:48:06,640 --> 00:48:08,400
предсказать результат вашего ученика, который выдает учитель,

1574
00:48:08,400 --> 00:48:09,760
и поэтому, как правило, вы можете сделать

1575
00:48:09,760 --> 00:48:10,559
это для большинства

1576
00:48:10,559 --> 00:48:13,119
методов, я имею в виду для большинства задач  вы можете

1577
00:48:13,119 --> 00:48:13,760
сделать это

1578
00:48:13,760 --> 00:48:15,760
довольно легко и получить огромное сжатие от 50 до

1579
00:48:15,760 --> 00:48:17,280
100x

1580
00:48:17,280 --> 00:48:20,000
без ухудшения качества, но

1581
00:48:20,000 --> 00:48:21,119
важно понимать, что мы

1582
00:48:21,119 --> 00:48:22,640
не сжимаем саму предварительно обученную модель

1583
00:48:22,640 --> 00:48:23,200
,

1584
00:48:23,200 --> 00:48:24,720
у нас нет r  Мне очень повезло с этим,

1585
00:48:24,720 --> 00:48:26,160
так что вы не можете просто

1586
00:48:26,160 --> 00:48:26,480
взять

1587
00:48:26,480 --> 00:48:28,480
bert, а затем сжать его до меньшей

1588
00:48:28,480 --> 00:48:29,680
модели, которую вы затем можете настроить для

1589
00:48:29,680 --> 00:48:31,040
всех этих других задач, только

1590
00:48:31,040 --> 00:48:32,640
после того, как вы выбрали задачу и после

1591
00:48:32,640 --> 00:48:34,400
того, как вы ее найдете  настроить его для этой задачи,

1592
00:48:34,400 --> 00:48:37,280
чтобы вы, чтобы мы смогли это

1593
00:48:37,280 --> 00:48:38,640
сделать,

1594
00:48:38,640 --> 00:48:41,599
чтобы показать некоторые конкретные результаты, так что,

1595
00:48:41,599 --> 00:48:42,720
допустим, у нас есть, допустим, у нас есть

1596
00:48:42,720 --> 00:48:44,160
большой учитель, это курс обзоров книг Amazon,

1597
00:48:44,160 --> 00:48:45,920
это статья, которую

1598
00:48:45,920 --> 00:48:47,760
я должен  процитируйте это, но это статья,

1599
00:48:47,760 --> 00:48:49,359
которую опубликовала моя группа, э-э,

1600
00:48:49,359 --> 00:48:52,800
Джулия Терк, э-э, написала, и, таким

1601
00:48:52,800 --> 00:48:56,240
образом, у нее есть пятьдесят тысяч помеченных

1602
00:48:56,240 --> 00:48:57,760
примеров и восемь миллионов

1603
00:48:57,760 --> 00:49:00,800
немаркированных примеров, так что вы точно

1604
00:49:00,800 --> 00:49:02,480
настраиваете себя перед тренировкой очень большой

1605
00:49:02,480 --> 00:49:04,160
нормальный вы принимаете белок, но большой

1606
00:49:04,160 --> 00:49:06,400
вы э-э, вы точно настраиваете его на этих

1607
00:49:06,400 --> 00:49:08,480
50000 примеров, вы получаете эту точность 88

1608
00:49:08,480 --> 00:49:09,680
прямо

1609
00:49:09,680 --> 00:49:12,880
тогда, ну и так,

1610
00:49:12,880 --> 00:49:14,400
но теперь давайте скажем, что вместо использования

1611
00:49:14,400 --> 00:49:15,440
bert large вы использовали гораздо меньшую

1612
00:49:15,440 --> 00:49:16,720
версию, так что

1613
00:49:16,720 --> 00:49:18,000
это четверть от размера, который вы

1614
00:49:18,000 --> 00:49:20,720
знаете  шестнадцатый размер независимо от этого

1615
00:49:20,720 --> 00:49:22,319
примерно одну сотую размера,

1616
00:49:22,319 --> 00:49:23,359
так что эта строка в

1617
00:49:23,359 --> 00:49:25,200
сотую часть размера, если бы вы просто

1618
00:49:25,200 --> 00:49:25,680
тренировали ее,

1619
00:49:25,680 --> 00:49:26,880
если бы вы предварительно натренировали ее в том

1620
00:49:26,880 --> 00:49:30,079
же книжном магазине Википедии, как burt,

1621
00:49:30,079 --> 00:49:33,280
а затем точно настроите ее, вы получите 82

1622
00:49:33,280 --> 00:49:34,240
процента  точность,

1623
00:49:34,240 --> 00:49:36,960
которую вы знаете, намного хуже 60 60, например,

1624
00:49:36,960 --> 00:49:38,240
шесть, шесть, абсолютное хуже, что является довольно

1625
00:49:38,240 --> 00:49:39,040
большим падением, верно,

1626
00:49:39,040 --> 00:49:42,319
но тогда, если вы возьмете этот

1627
00:49:42,319 --> 00:49:44,480
ярлык учителя 88, эти восемь миллионов

1628
00:49:44,480 --> 00:49:46,319
примеров, которые, конечно, выдержаны,

1629
00:49:46,319 --> 00:49:47,119
это тест,

1630
00:49:47,119 --> 00:49:50,400
это точность теста  гм, а затем

1631
00:49:50,400 --> 00:49:53,520
э-э, а затем обучите эту модель классификации,

1632
00:49:54,160 --> 00:49:55,760
которая говорит, что это хороший

1633
00:49:55,760 --> 00:49:57,280
обзор этих восьми миллионов примеров,

1634
00:49:57,280 --> 00:49:58,400
вы можете взять эту модель, которая в сто

1635
00:49:58,400 --> 00:49:59,760
раз меньше, и получить ту же точность,

1636
00:49:59,760 --> 00:50:01,359
что и учитель, верно, вы получите такую же

1637
00:50:01,359 --> 00:50:04,319
точность, так что это действительно

1638
00:50:04,319 --> 00:50:05,520
Самое классное в дистилляции

1639
00:50:05,520 --> 00:50:06,880
то, что вы можете получить модели, которые намного

1640
00:50:06,880 --> 00:50:08,160
меньше, но вам все равно нужно обучать

1641
00:50:08,160 --> 00:50:09,119
большую модель в первую очередь, поэтому это

1642
00:50:09,119 --> 00:50:10,240
не помогает затратам на обучение, а просто

1643
00:50:10,240 --> 00:50:11,200
помогает в действительности  Это намного дороже,

1644
00:50:11,200 --> 00:50:12,079
потому что тогда вам нужно использовать эту большую

1645
00:50:12,079 --> 00:50:13,280
модель для маркировки

1646
00:50:13,280 --> 00:50:15,040
миллионов или миллиардов примеров, так что в

1647
00:50:15,040 --> 00:50:16,160
конечном итоге это будет дороже, чем просто

1648
00:50:16,160 --> 00:50:17,280
торговля рождением, но на самом деле вы можете

1649
00:50:17,280 --> 00:50:18,880
использовать эту модель

1650
00:50:18,880 --> 00:50:22,880
во время вывода за крошечные затраты,

1651
00:50:22,880 --> 00:50:24,480
поэтому вопрос в том, почему

1652
00:50:24,480 --> 00:50:26,640
так ли хорошо работает дистилляция,

1653
00:50:26,640 --> 00:50:29,760
так что большая гипотеза состоит в том, что языковое

1654
00:50:29,760 --> 00:50:31,200
моделирование - это своего рода конечная задача НЛП,

1655
00:50:31,680 --> 00:50:33,760
верно? идеальная языковая модель - это также

1656
00:50:33,760 --> 00:50:35,280
идеальная система ответов на вопросы?

1657
00:50:39,599 --> 00:50:41,040
чтобы делать эти вещи,

1658
00:50:41,040 --> 00:50:42,079
вы должны быть в состоянии

1659
00:50:42,079 --> 00:50:44,640
сконструировать ее как языковую модель,

1660
00:50:44,640 --> 00:50:46,319
поэтому, когда вы тренируете массивную

1661
00:50:46,319 --> 00:50:48,160
языковую модель, вы

1662
00:50:48,160 --> 00:50:49,520
изучаете многие миллионы скрытых

1663
00:50:49,520 --> 00:50:51,119
функций, которые могут быть фактически

1664
00:50:51,119 --> 00:50:52,319
теми же функциями, которые вам нужны для любого

1665
00:50:52,319 --> 00:50:53,520
другая задача,

1666
00:50:53,520 --> 00:50:55,119
и поэтому, когда вы выполняете более простую

1667
00:50:55,119 --> 00:50:57,040
тонкую настройку более конкретной задачи,

1668
00:50:57,040 --> 00:50:58,400
какая тонкая настройка в основном

1669
00:50:58,400 --> 00:50:59,760
принимает эти ярлыки этой скрытой функции

1670
00:50:59,760 --> 00:51:00,960
что ваша система выучила

1671
00:51:00,960 --> 00:51:02,880
некоторые закодированные где-то в ваших весах,

1672
00:51:02,880 --> 00:51:04,480
а вы просто настраиваете

1673
00:51:04,480 --> 00:51:05,520
их, поэтому я могу сделать это с помощью

1674
00:51:05,520 --> 00:51:05,920
одного

1675
00:51:05,920 --> 00:51:09,119
прохода по данным точной настройки, и поэтому,

1676
00:51:09,119 --> 00:51:10,400
но как только вы выяснили, какие

1677
00:51:10,400 --> 00:51:11,680
части  важно

1678
00:51:11,680 --> 00:51:13,359
то, что существует гипотетически гораздо

1679
00:51:13,359 --> 00:51:15,359
меньший размер модели, которая все еще может

1680
00:51:15,359 --> 00:51:17,040
получить то же самое представление, то же самое

1681
00:51:17,040 --> 00:51:18,640
обобщение, так что затем вы

1682
00:51:18,640 --> 00:51:20,559
помечаете кучу примеров с этой точно настроенной

1683
00:51:20,559 --> 00:51:22,079
моделью, и теперь вы можете изучить модель, которая

1684
00:51:22,079 --> 00:51:23,040
действительно может отточить только эти

1685
00:51:23,040 --> 00:51:24,240
функции, которые важны, и поэтому

1686
00:51:24,240 --> 00:51:26,960
он может понять, что он может

1687
00:51:26,960 --> 00:51:28,400
обучить модель

1688
00:51:28,400 --> 00:51:31,599
размером 100 и

1689
00:51:32,000 --> 00:51:33,280
просто оттачивать эти функции, если у

1690
00:51:33,280 --> 00:51:36,000
вас много данных псевдо-меток,

1691
00:51:36,000 --> 00:51:38,559
и именно поэтому он работает, и поэтому

1692
00:51:38,559 --> 00:51:39,680
На самом деле есть свидетельства того, что

1693
00:51:39,680 --> 00:51:40,559
самодистилляция просто не работает

1694
00:51:40,559 --> 00:51:42,079
правильно, и поэтому должно

1695
00:51:42,079 --> 00:51:43,760
быть так, что

1696
00:51:43,760 --> 00:51:44,960
на самом деле это просто изучение

1697
00:51:44,960 --> 00:51:47,359
подмножества функций для большинства этих задач,

1698
00:51:47,359 --> 00:51:51,359
ну и поэтому в основном каждая задача, кроме

1699
00:51:51,359 --> 00:51:52,720
языкового моделирования,  мы смогли

1700
00:51:52,720 --> 00:51:53,920
заставить работать дистилляцию, поэтому это

1701
00:51:53,920 --> 00:51:55,200
включает в себя задачи, которые кажутся действительно

1702
00:51:55,200 --> 00:51:55,920
сложными, такими как

1703
00:51:55,920 --> 00:51:58,800
ответы на вопросы и поиск, так

1704
00:51:58,800 --> 00:51:59,920
что это означает,

1705
00:51:59,920 --> 00:52:01,359
что сам этот язык моделирует и

1706
00:52:01,359 --> 00:52:03,040
прогнозирует, и который в основном является

1707
00:52:03,040 --> 00:52:04,640
генерацией языка, потому что это

1708
00:52:04,640 --> 00:52:06,079
просто форма языка

1709
00:52:06,079 --> 00:52:08,480
моделирование фундаментально сложнее, чем понимание языка,

1710
00:52:09,200 --> 00:52:12,240
что не так уж сложно

1711
00:52:12,240 --> 00:52:13,520
купить или, по крайней мере, может быть, это не

1712
00:52:13,520 --> 00:52:14,800
принципиально сложнее, но с учетом

1713
00:52:14,800 --> 00:52:15,520
современного

1714
00:52:15,520 --> 00:52:17,280
состояния современные модели

1715
00:52:17,280 --> 00:52:19,280
понимания языка фундаментально

1716
00:52:19,280 --> 00:52:20,960
проще, чем то, что они делают правильно  так что, по-

1717
00:52:20,960 --> 00:52:22,559
видимому, это просто

1718
00:52:22,559 --> 00:52:23,359
распознавание образов, а

1719
00:52:23,359 --> 00:52:26,880
не модели, которые генерируют язык,

1720
00:52:26,880 --> 00:52:28,559
и вот почему все эти

1721
00:52:28,559 --> 00:52:30,400
классификационные модели могут быть отчасти

1722
00:52:30,400 --> 00:52:33,119
переработаны так хорошо, в общем,

1723
00:52:33,119 --> 00:52:36,400
в заключение,

1724
00:52:36,400 --> 00:52:37,839
предварительно обученные модели работают очень хорошо,

1725
00:52:37,839 --> 00:52:40,480
они  очень дорого, мы знаем, как

1726
00:52:40,480 --> 00:52:42,960
решить эту проблему за время вывода,

1727
00:52:42,960 --> 00:52:44,319
и мы можем сделать быстрый вывод,

1728
00:52:44,319 --> 00:52:48,480
но это все еще не решено  э-э,

1729
00:52:48,480 --> 00:52:51,839
как сделать это быстро во время тренировки,

1730
00:52:51,839 --> 00:52:55,119
и, более того,

1731
00:52:55,119 --> 00:52:57,839
кажется, что многие детали

1732
00:52:57,839 --> 00:52:59,680
алгоритмических улучшений, чтобы

1733
00:52:59,680 --> 00:53:00,880
сделать обучение более эффективным,

1734
00:53:00,880 --> 00:53:03,920
кажется, не имеют большого преимущества

1735
00:53:03,920 --> 00:53:04,640
с точки зрения, по крайней мере, получения

1736
00:53:04,640 --> 00:53:05,920
состояния-  выдающиеся результаты,

1737
00:53:05,920 --> 00:53:08,079
и кажется, что многие варианты на

1738
00:53:08,079 --> 00:53:09,280
самом деле не имеют большого значения, и на самом деле

1739
00:53:10,480 --> 00:53:13,040
вы знаете пару подобных, по сравнению

1740
00:53:13,040 --> 00:53:14,000
с простой

1741
00:53:14,000 --> 00:53:15,359
мужской басовой партией, которую довольно

1742
00:53:15,359 --> 00:53:17,040
сложно превзойти  в

1743
00:53:17,040 --> 00:53:18,319
устоявшемся сравнении

1744
00:53:18,319 --> 00:53:21,680
яблока, так что да, это немного,

1745
00:53:21,680 --> 00:53:23,200
я имею в виду, что это немного неудачно, потому

1746
00:53:23,200 --> 00:53:24,559
что с точки зрения исследования это

1747
00:53:24,559 --> 00:53:25,920
определенно хорошо

1748
00:53:25,920 --> 00:53:28,160
от людей, которые хотят создавать

1749
00:53:28,160 --> 00:53:29,599
системы nlp и которые хотят

1750
00:53:29,599 --> 00:53:31,359
особенно предметно-ориентированного nlp  такие системы,

1751
00:53:31,359 --> 00:53:32,880
как люди, которые хотят, чтобы вы знали, адаптируются

1752
00:53:32,880 --> 00:53:34,000
к медицинской области,

1753
00:53:34,000 --> 00:53:35,280
или люди, у которых есть лишь крошечный

1754
00:53:35,280 --> 00:53:36,960
объем данных, или люди, которые хотят

1755
00:53:36,960 --> 00:53:38,319
создавать стартапы или они хотят, чтобы вы знали,

1756
00:53:38,319 --> 00:53:39,440
создадите реальный продукт, а у них есть

1757
00:53:39,440 --> 00:53:40,640
только  Я использую данные, так что это определенно хорошо

1758
00:53:40,640 --> 00:53:41,359
с этой точки зрения, но

1759
00:53:41,359 --> 00:53:44,480
я определенно думаю, что с

1760
00:53:44,480 --> 00:53:46,079
точки зрения иногда вы знаете,

1761
00:53:46,079 --> 00:53:48,960
эээ, исследования, как я уже говорил,

1762
00:53:48,960 --> 00:53:49,920
цель исследования - это как бы

1763
00:53:49,920 --> 00:53:51,119
исследовать себя без работы,

1764
00:53:51,119 --> 00:53:54,240
тогда это своего рода  эээ, вы знаете, это

1765
00:53:54,240 --> 00:53:56,079
немного прискорбно с этой точки зрения,

1766
00:53:56,079 --> 00:53:57,599
но вы знаете, я все еще думаю, что

1767
00:53:57,599 --> 00:53:59,040
есть вероятность

1768
00:53:59,040 --> 00:53:59,680
того, что

1769
00:53:59,680 --> 00:54:01,920
произойдет прорыв, который покажет, как

1770
00:54:01,920 --> 00:54:03,520
добиться вычислительной эффективности

1771
00:54:03,520 --> 00:54:06,319
без этого, может показывать

1772
00:54:06,319 --> 00:54:07,920
убедительные результаты, которые вы  не нужно, чтобы

1773
00:54:07,920 --> 00:54:10,480
вы знали такую абсурдно большую модель или на

1774
00:54:10,480 --> 00:54:11,680
самом деле, помимо модели, вам

1775
00:54:11,680 --> 00:54:12,880
не нужно быть уверенным, что дорогая

1776
00:54:12,880 --> 00:54:13,520
модель

1777
00:54:13,520 --> 00:54:16,079
э-э, чтобы хорошо себя чувствовать, эм, может быть, это будет

1778
00:54:16,079 --> 00:54:17,359
из-за редкости или чего-то в этом роде,

1779
00:54:17,359 --> 00:54:18,160
где

1780
00:54:18,160 --> 00:54:19,200
вы на самом деле делаете  есть действительно большая

1781
00:54:19,200 --> 00:54:21,280
модель, просто в некоторых она редко активируется

1782
00:54:21,280 --> 00:54:22,839
с помощью каких-то трюков с эффективностью или

1783
00:54:22,839 --> 00:54:25,839
чего-то еще

