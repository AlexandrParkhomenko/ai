{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_qNSzzyaCbD"
   },
   "source": [
    "##### Copyright 2019 The [TensorFlow](https://www.tensorflow.org/text/guide/subwords_tokenizer) Authors.\n",
    "\n",
    "\n",
    "Пархоменко Александр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-07-26T12:01:42.278126Z",
     "iopub.status.busy": "2022-07-26T12:01:42.277619Z",
     "iopub.status.idle": "2022-07-26T12:01:42.281412Z",
     "shell.execute_reply": "2022-07-26T12:01:42.280881Z"
    },
    "id": "jmjh290raIky"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES8iTKcdPCLt"
   },
   "source": [
    "# Токенизаторы на уровне слов\n",
    "\n",
    "В этом руководстве показано, как создать словарь из набора данных, используя [`text.BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer).\n",
    "\n",
    "Основным преимуществом токенизатора уровня слов является то, что он выполняет интерполяцию между словарной и символьной разметкой. Обычные слова получают место в словарном запасе, но токенизатор может вернуться к частям слова и отдельным символам для неизвестных слов.\n",
    "\n",
    "Цель: В конце этого урока вы построите полный уровня слова токенизатор от начала до конца и детокенизатор с нуля, и сохраните его как saved_model, вы сможете загружать и использовать его в руководстве по [трансформерам](https://tensorflow.org/text/tutorials/transformer).\n",
    "\n",
    "\\* токены = лексемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHfrtG1YPJdR"
   },
   "source": [
    "## Обзор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIMuBnQO6ZoV"
   },
   "source": [
    "Пакет `tensorflow_text` включает в себя реализацию TensorFlow многих распространенных токенизаторов. Сюда входят три токенизатора на уровне слов:\n",
    "\n",
    "* [`text.BertTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/BertTokenizer) - класс является высокоуровневым интерфейсом. Он включает в себя алгоритм расщепления на BERT-токены и `WordPieceTokenizer`. Он принимает **предложения** в качестве входных данных и возвращает **токен-идентификаторы**.\n",
    "* `text.WordpieceTokenizer` - класс является низкоуровневым интерфейсом. Он только реализует [WordPiece алгоритм](#applying_wordpiece). Перед вызовом необходимо стандартизировать и разбить текст на слова. Он принимает **слова** в качестве входных данных и возвращает токен-идентификаторы.\n",
    "* [`text.SentencepieceTokenizer`](https://www.tensorflow.org/text/api_docs/python/text/SentencepieceTokenizer) - требует более сложной настройки. Для его инициализатора требуется предварительно обученная уровня предложений модель. См [google/sentencepiece repository](https://github.com/google/sentencepiece#train-sentencepiece-model) для получения инструкций о том , как построить одну из этих моделей. Он может принимать **предложения** в качестве входных данных при токенизации.\n",
    "\n",
    "В этом руководстве словарь Wordpiece создается сверху вниз, начиная с существующих слов. Этот процесс не работает для японского, китайского или корейского языков, поскольку в этих языках нет четких многосимвольных единиц. Чтобы разметить эти языки рассмотрите возможность использования `text.SentencepieceTokenizer`, `text.UnicodeCharTokenizer` или [этот подход](https://tfhub.dev/google/zh_segmentation/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "## Настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:01:42.284548Z",
     "iopub.status.busy": "2022-07-26T12:01:42.284080Z",
     "iopub.status.idle": "2022-07-26T12:02:06.824395Z",
     "shell.execute_reply": "2022-07-26T12:02:06.823386Z"
    },
    "id": "rJTYbk1E9QOk"
   },
   "outputs": [],
   "source": [
    "#!pip install -q -U \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:06.828313Z",
     "iopub.status.busy": "2022-07-26T12:02:06.828083Z",
     "iopub.status.idle": "2022-07-26T12:02:08.186355Z",
     "shell.execute_reply": "2022-07-26T12:02:08.185513Z"
    },
    "id": "XFG0NDRu5mYQ"
   },
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:08.190272Z",
     "iopub.status.busy": "2022-07-26T12:02:08.190026Z",
     "iopub.status.idle": "2022-07-26T12:02:11.162898Z",
     "shell.execute_reply": "2022-07-26T12:02:11.162238Z"
    },
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:11.167285Z",
     "iopub.status.busy": "2022-07-26T12:02:11.166528Z",
     "iopub.status.idle": "2022-07-26T12:02:11.170129Z",
     "shell.execute_reply": "2022-07-26T12:02:11.169568Z"
    },
    "id": "QZi9RstHxO_Z"
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzJbGA5N5mXr"
   },
   "source": [
    "## Загрузка набора данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kC9TeTd47j8p"
   },
   "source": [
    "Скачиваем Русский/English датасет переводов из [tfds](https://tensorflow.org/datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:11.173391Z",
     "iopub.status.busy": "2022-07-26T12:02:11.172886Z",
     "iopub.status.idle": "2022-07-26T12:02:14.964037Z",
     "shell.execute_reply": "2022-07-26T12:02:14.963376Z"
    },
    "id": "qDaAOTKHNy8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 16:15:27.208900: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 16:15:27.209552: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GHc3O2W8Hgg"
   },
   "source": [
    "Этот набор данных создает пары предложений на русском и английском языках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:14.968263Z",
     "iopub.status.busy": "2022-07-26T12:02:14.967745Z",
     "iopub.status.idle": "2022-07-26T12:02:15.416288Z",
     "shell.execute_reply": "2022-07-26T12:02:15.415604Z"
    },
    "id": "-_ezZT8w8GqD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian:  к : успех , перемены возможны только с оружием в руках .\n",
      "English:    c : success , the change is only coming through the barrel of the gun .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 16:15:27.335031: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for ru, en in train_examples.take(1):\n",
    "  print(\"Russian: \", ru.numpy().decode('utf-8'))\n",
    "  print(\"English:   \", en.numpy().decode('utf-8'))\n",
    "# TODO: улучшить пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNGwm45vKttj"
   },
   "source": [
    "Обратите внимание на несколько моментов в приведенных выше примерах предложений:\n",
    "* Они строчные.\n",
    "* Вокруг знаков препинания есть пробелы.\n",
    "* Неясно, используется ли нормализация Unicode и какая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:15.453462Z",
     "iopub.status.busy": "2022-07-26T12:02:15.452901Z",
     "iopub.status.idle": "2022-07-26T12:02:15.477941Z",
     "shell.execute_reply": "2022-07-26T12:02:15.477241Z"
    },
    "id": "Pm5Eah5F6B1I"
   },
   "outputs": [],
   "source": [
    "train_en = train_examples.map(lambda ru, en: en)\n",
    "train_ru = train_examples.map(lambda ru, en: ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCD57yALsF0D"
   },
   "source": [
    "## Создание словаря\n",
    "\n",
    "В этом разделе создается словарный запас из набора данных. Если у вас уже есть файл словаря и вы просто хотите увидеть, как построить `text.BertTokenizer` или `text.WordpieceTokenizer` с ним, то вы можете перейти вперед к разделу [Инициализация токенизатора](#build_the_tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4CX7_KlO8lX"
   },
   "source": [
    "Примечание: Код генерации словаря, используемый в этом руководстве оптимизирован для **простоты**. Если вам нужно более масштабируемое решение рассмотрите вопрос об использовании реализации Apache Beam, доступной в [tools/wordpiece_vocab/generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R74W3QabgWmX"
   },
   "source": [
    "Код генерации словаря входит в `tensorflow_text` pip пакет. По умолчанию он не импортируется, его нужно импортировать вручную:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:15.481424Z",
     "iopub.status.busy": "2022-07-26T12:02:15.480924Z",
     "iopub.status.idle": "2022-07-26T12:02:15.484987Z",
     "shell.execute_reply": "2022-07-26T12:02:15.484457Z"
    },
    "id": "iqX1fYdpnLS2"
   },
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaWSnj8xFgI7"
   },
   "source": [
    "Функция `bert_vocab.bert_vocab_from_dataset` будет генерировать словарь. \n",
    "\n",
    "Вы можете задать множество аргументов, чтобы изменить её поведение. В этом руководстве вы в основном будете использовать значения по умолчанию. Если вы хотите узнать больше о возможностях, сначала прочтите раздел [алгоритм](#algorithm), а затем посмотреть на [код](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/bert_vocab_from_dataset.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gTty2Wh-dHm"
   },
   "source": [
    "Это займет около 2 минут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:15.488367Z",
     "iopub.status.busy": "2022-07-26T12:02:15.487884Z",
     "iopub.status.idle": "2022-07-26T12:02:15.491466Z",
     "shell.execute_reply": "2022-07-26T12:02:15.490934Z"
    },
    "id": "FwFzYjBy-h8W"
   },
   "outputs": [],
   "source": [
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # Размер целевого словаря\n",
    "    vocab_size = 8000,\n",
    "    # Зарезервированные токены, которые должны быть включены в словарь\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Аргументы для `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Аргументы для `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:02:15.494643Z",
     "iopub.status.busy": "2022-07-26T12:02:15.494137Z",
     "iopub.status.idle": "2022-07-26T12:03:31.339766Z",
     "shell.execute_reply": "2022-07-26T12:03:31.339076Z"
    },
    "id": "PMN6Lli_3sJW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 39s, sys: 5.37 s, total: 8min 44s\n",
      "Wall time: 8min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ru_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_ru.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cl4d2O34gkH"
   },
   "source": [
    "Вот несколько кусочков получившейся лексики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:03:31.343265Z",
     "iopub.status.busy": "2022-07-26T12:03:31.342993Z",
     "iopub.status.idle": "2022-07-26T12:03:31.347201Z",
     "shell.execute_reply": "2022-07-26T12:03:31.346635Z"
    },
    "id": "mfaPmX54FvhW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'і', '՛']\n",
      "['трудно', 'хотела', 'далеко', 'качестве', 'мою', '##3', '##де', '##ила', 'планеты', 'большие']\n",
      "['##’', '##“', '##”', '##„', '##•', '##′', '##⁄', '##∇', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(ru_vocab[:10])\n",
    "print(ru_vocab[100:110])\n",
    "print(ru_vocab[1000:1010])\n",
    "print(ru_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owkP3wbYVQv0"
   },
   "source": [
    "Сохранение словарного файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:03:31.350158Z",
     "iopub.status.busy": "2022-07-26T12:03:31.349921Z",
     "iopub.status.idle": "2022-07-26T12:03:31.353419Z",
     "shell.execute_reply": "2022-07-26T12:03:31.352862Z"
    },
    "id": "VY6v1ThkKDyZ"
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "  with open(filepath, 'w') as f:\n",
    "    for token in vocab:\n",
    "      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:03:31.356472Z",
     "iopub.status.busy": "2022-07-26T12:03:31.356237Z",
     "iopub.status.idle": "2022-07-26T12:03:31.361942Z",
     "shell.execute_reply": "2022-07-26T12:03:31.361475Z"
    },
    "id": "X_TR5U1xWvAV"
   },
   "outputs": [],
   "source": [
    "write_vocab_file('ru_vocab.txt', ru_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ag3qcx54nii"
   },
   "source": [
    "Используем эту функцию для создания словаря из английских данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:03:31.364802Z",
     "iopub.status.busy": "2022-07-26T12:03:31.364414Z",
     "iopub.status.idle": "2022-07-26T12:04:23.266397Z",
     "shell.execute_reply": "2022-07-26T12:04:23.265693Z"
    },
    "id": "R3cMumvHWWtl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 1.36 s, total: 1min 55s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.269535Z",
     "iopub.status.busy": "2022-07-26T12:04:23.269249Z",
     "iopub.status.idle": "2022-07-26T12:04:23.273339Z",
     "shell.execute_reply": "2022-07-26T12:04:23.272767Z"
    },
    "id": "NxOpzMd8ol5B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '#', '$', '%', '&', \"'\"]\n",
      "['##s', 'have', 'but', 'what', 'on', 'do', 'with', 'can', 'there', 'about']\n",
      "['revolution', '200', 'basic', 'potential', 'english', 'led', 'message', 'perfect', '##ce', 'nine']\n",
      "['##–', '##—', '##‘', '##’', '##“', '##”', '##•', '##∇', '##♪', '##♫']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck3LG_f34wCs"
   },
   "source": [
    "Вот два файла словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.276050Z",
     "iopub.status.busy": "2022-07-26T12:04:23.275827Z",
     "iopub.status.idle": "2022-07-26T12:04:23.281456Z",
     "shell.execute_reply": "2022-07-26T12:04:23.280870Z"
    },
    "id": "xfc2jxPznM6H"
   },
   "outputs": [],
   "source": [
    "write_vocab_file('en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.283980Z",
     "iopub.status.busy": "2022-07-26T12:04:23.283767Z",
     "iopub.status.idle": "2022-07-26T12:04:23.446582Z",
     "shell.execute_reply": "2022-07-26T12:04:23.445833Z"
    },
    "id": "djehfEL6Zn-I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_vocab.txt  ru_vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls *_vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vb5ddYLTBJhk"
   },
   "source": [
    "## Инициализация токенизатора\n",
    "<a id=\"build_the_tokenizer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qgp5gvR-2tQ"
   },
   "source": [
    "`text.BertTokenizer` можно инициализировать, передавая путь словарного файла в качестве первого аргумента (смотрите раздел [tf.lookup](#tf.lookup) для других вариантов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.450491Z",
     "iopub.status.busy": "2022-07-26T12:04:23.449911Z",
     "iopub.status.idle": "2022-07-26T12:04:23.460418Z",
     "shell.execute_reply": "2022-07-26T12:04:23.459860Z"
    },
    "id": "gdMpt9ZEjVGu"
   },
   "outputs": [],
   "source": [
    "ru_tokenizer = text.BertTokenizer('ru_vocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = text.BertTokenizer('en_vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhPZafCUds86"
   },
   "source": [
    "Теперь вы можете использовать его для кодирования текста. Возьмите партию из 3 примеров из английских данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.463447Z",
     "iopub.status.busy": "2022-07-26T12:04:23.463240Z",
     "iopub.status.idle": "2022-07-26T12:04:23.767782Z",
     "shell.execute_reply": "2022-07-26T12:04:23.767113Z"
    },
    "id": "NKF0QJjtUm9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'c : success , the change is only coming through the barrel of the gun .'\n",
      "b'the documentation and the hands-on teaching methodology is also open-source and released as the creative commons .'\n",
      "b\"( video ) didi pickles : it 's four o'clock in the morning .\"\n"
     ]
    }
   ],
   "source": [
    "for ru_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  for ex in en_examples:\n",
    "    print(ex.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9OEIBWopMxW"
   },
   "source": [
    "Запустите его через `BertTokenizer.tokenize` метод. Первоначально он возвращает `tf.RaggedTensor` с осями `(batch, word, word-piece)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.771307Z",
     "iopub.status.busy": "2022-07-26T12:04:23.770787Z",
     "iopub.status.idle": "2022-07-26T12:04:23.795615Z",
     "shell.execute_reply": "2022-07-26T12:04:23.795023Z"
    },
    "id": "AeTM81lAc8q1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 28, 1103, 14, 84, 243, 93, 200, 389, 218, 84, 6405, 87, 84, 2473, 16]\n",
      "[84, 3914, 464, 85, 84, 702, 15, 104, 1495, 2346, 2024, 93, 187, 435, 15, 942, 85, 2533, 111, 84, 1068, 5725, 16]\n",
      "[10, 400, 11, 168, 379, 1026, 1125, 28, 90, 9, 57, 316, 53, 9, 2501, 89, 84, 813, 16]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the examples -> (batch, word, word-piece)\n",
    "token_batch = en_tokenizer.tokenize(en_examples)\n",
    "# Merge the word and word-piece axes -> (batch, tokens)\n",
    "token_batch = token_batch.merge_dims(-2,-1)\n",
    "\n",
    "for ex in token_batch.to_list():\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbdIaW6kX8hu"
   },
   "source": [
    "Если заменить токен-идентификаторы с текстовыми представлениями ( с помощью `tf.gather` ) вы можете увидеть, что в первом примере слова `\"searchability\"` и `\"serendipity\"` были разложенного на `\"search ##ability\"` и `\"s ##ere ##nd ##ip ##ity\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.798277Z",
     "iopub.status.busy": "2022-07-26T12:04:23.798060Z",
     "iopub.status.idle": "2022-07-26T12:04:23.819568Z",
     "shell.execute_reply": "2022-07-26T12:04:23.818939Z"
    },
    "id": "FA6nKYx5U3Nj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'c : success , the change is only coming through the barrel of the gun .',\n",
       "       b'the document ##ation and the hands - on teaching method ##ology is also open - source and released as the creative commons .',\n",
       "       b\"( video ) did ##i pick ##les : it ' s four o ' clock in the morning .\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup each token id in the vocabulary.\n",
    "txt_tokens = tf.gather(en_vocab, token_batch)\n",
    "# Join with spaces.\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY2XrhyRem2O"
   },
   "source": [
    "Для того, чтобы повторно собрать слова из извлеченных токенов, используйте метод `BertTokenizer.detokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.822483Z",
     "iopub.status.busy": "2022-07-26T12:04:23.822036Z",
     "iopub.status.idle": "2022-07-26T12:04:23.844290Z",
     "shell.execute_reply": "2022-07-26T12:04:23.843753Z"
    },
    "id": "toBXQSrgemRw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'c : success , the change is only coming through the barrel of the gun .',\n",
       "       b'the documentation and the hands - on teaching methodology is also open - source and released as the creative commons .',\n",
       "       b\"( video ) didi pickles : it ' s four o ' clock in the morning .\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(token_batch)\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIZWWy_iueQY"
   },
   "source": [
    "> Примечание: `BertTokenizer.tokenize`/`BertTokenizer.detokenize` не совершает преобразование туда и обратно без потерь. Результат `detokenize`, как правило, не будет иметь того же содержимого или смещений, что и ввод для `tokenize`. Это происходит потому , что на стадии «базовой токенизации», которая разделяет строки на слова перед применением `WordpieceTokenizer`, включает в себя необратимые шаги, как нижний регистр и расщепление на знаки препинания. `WordpieceTokenizer` с другой стороны, **является** обратимым."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bN30iCexTPY"
   },
   "source": [
    "## Настройка и экспорт\n",
    "\n",
    "Это руководство строит текстовый токенайзер и детокенайзер используемый в руководстве [Transformer](https://tensorflow.org/text/tutorials/transformer). Этот раздел добавляет методы и этапы обработки, чтобы упростить это руководство, а также экспорт токенайзера с помощью `tf.saved_model` таким образом они могут быть импортированы в других руководствах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wpc7oFkwgni"
   },
   "source": [
    "### Настройка токенизатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaUR9hHj0PUy"
   },
   "source": [
    "Ниже в руководстве ожидаются в токенизированном тексте включение двух `[START]` и `[END]` токенов.\n",
    "\n",
    "`reserved_tokens` резервируют место в начале словаря, так `[START]` и `[END]` имеют одинаковые индексы для обоих языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.847388Z",
     "iopub.status.busy": "2022-07-26T12:04:23.846928Z",
     "iopub.status.idle": "2022-07-26T12:04:23.852201Z",
     "shell.execute_reply": "2022-07-26T12:04:23.851657Z"
    },
    "id": "gyyoa5De0WQu"
   },
   "outputs": [],
   "source": [
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START)\n",
    "  ends = tf.fill([count,1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.854982Z",
     "iopub.status.busy": "2022-07-26T12:04:23.854535Z",
     "iopub.status.idle": "2022-07-26T12:04:23.884015Z",
     "shell.execute_reply": "2022-07-26T12:04:23.883479Z"
    },
    "id": "MrZjQIwZ6NHu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] c : success , the change is only coming through the barrel of the gun . [END]',\n",
       "       b'[START] the documentation and the hands - on teaching methodology is also open - source and released as the creative commons . [END]',\n",
       "       b\"[START] ( video ) didi pickles : it ' s four o ' clock in the morning . [END]\"],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(add_start_end(token_batch))\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMmHS5VT_suH"
   },
   "source": [
    "### Настройка детокенизатора\n",
    "\n",
    "Перед экспортом токенизаторов есть несколько вещей, которые вы можете прояснить для последующих руководств:\n",
    "\n",
    "1. Мы хотим генерировать чистый вывод текста, так что отбрасываем зарезервированные лексемы, как `[START]`, `[END]` и `[PAD]`.\n",
    "2. Мы заинтересованы в полных строках, поэтому применяем строчное присоединение по `words` оси результата.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.886621Z",
     "iopub.status.busy": "2022-07-26T12:04:23.886402Z",
     "iopub.status.idle": "2022-07-26T12:04:23.890493Z",
     "shell.execute_reply": "2022-07-26T12:04:23.889907Z"
    },
    "id": "x9vXUQPX1ZFA"
   },
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "  # Отбрасываем зарезервированные токены, кроме \"[UNK]\".\n",
    "  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "  bad_token_re = \"|\".join(bad_tokens)\n",
    "    \n",
    "  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "  # Соединяем их в строки.\n",
    "  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.893187Z",
     "iopub.status.busy": "2022-07-26T12:04:23.892737Z",
     "iopub.status.idle": "2022-07-26T12:04:23.896627Z",
     "shell.execute_reply": "2022-07-26T12:04:23.896103Z"
    },
    "id": "NMSpZUV7sQYw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'c : success , the change is only coming through the barrel of the gun .',\n",
       "       b'the documentation and the hands-on teaching methodology is also open-source and released as the creative commons .',\n",
       "       b\"( video ) didi pickles : it 's four o'clock in the morning .\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_examples.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.899200Z",
     "iopub.status.busy": "2022-07-26T12:04:23.898950Z",
     "iopub.status.idle": "2022-07-26T12:04:23.925515Z",
     "shell.execute_reply": "2022-07-26T12:04:23.924948Z"
    },
    "id": "yB3MJhNvkuBb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'c', b':', b'success', b',', b'the', b'change', b'is', b'only',\n",
       "  b'coming', b'through', b'the', b'barrel', b'of', b'the', b'gun', b'.'],\n",
       " [b'the', b'documentation', b'and', b'the', b'hands', b'-', b'on',\n",
       "  b'teaching', b'methodology', b'is', b'also', b'open', b'-', b'source',\n",
       "  b'and', b'released', b'as', b'the', b'creative', b'commons', b'.']    ,\n",
       " [b'(', b'video', b')', b'didi', b'pickles', b':', b'it', b\"'\", b's',\n",
       "  b'four', b'o', b\"'\", b'clock', b'in', b'the', b'morning', b'.']    ]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_examples).merge_dims(-2,-1)\n",
    "words = en_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.928152Z",
     "iopub.status.busy": "2022-07-26T12:04:23.927655Z",
     "iopub.status.idle": "2022-07-26T12:04:23.943664Z",
     "shell.execute_reply": "2022-07-26T12:04:23.943143Z"
    },
    "id": "ED5rMeZE6HT3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'c : success , the change is only coming through the barrel of the gun .',\n",
       "       b'the documentation and the hands - on teaching methodology is also open - source and released as the creative commons .',\n",
       "       b\"( video ) didi pickles : it ' s four o ' clock in the morning .\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEfEdRi11Re4"
   },
   "source": [
    "### Экспорт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFuo1KZjpEPR"
   },
   "source": [
    "Следующий блок кода создает `CustomTokenizer` класс, чтобы содержать `text.BertTokenizer` экземпляры, пользовательскую логику, и `@tf.function` обертки, необходимых для экспорта. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.946328Z",
     "iopub.status.busy": "2022-07-26T12:04:23.946110Z",
     "iopub.status.idle": "2022-07-26T12:04:23.955162Z",
     "shell.execute_reply": "2022-07-26T12:04:23.954663Z"
    },
    "id": "f1q1hCpH72Vj"
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "  def __init__(self, reserved_tokens, vocab_path):\n",
    "    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "    self._reserved_tokens = reserved_tokens\n",
    "    self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "    self.vocab = tf.Variable(vocab)\n",
    "\n",
    "    ## Create the signatures for export:   \n",
    "\n",
    "    # Include a tokenize signature for a batch of strings. \n",
    "    self.tokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "    \n",
    "    # Include `detokenize` and `lookup` signatures for:\n",
    "    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "    #   * `RaggedTensors` with shape [batch, tokens]\n",
    "    self.detokenize.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.detokenize.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    self.lookup.get_concrete_function(\n",
    "        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "    self.lookup.get_concrete_function(\n",
    "          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "    # These `get_*` methods take no arguments\n",
    "    self.get_vocab_size.get_concrete_function()\n",
    "    self.get_vocab_path.get_concrete_function()\n",
    "    self.get_reserved_tokens.get_concrete_function()\n",
    "    \n",
    "  @tf.function\n",
    "  def tokenize(self, strings):\n",
    "    enc = self.tokenizer.tokenize(strings)\n",
    "    # Merge the `word` and `word-piece` axes.\n",
    "    enc = enc.merge_dims(-2,-1)\n",
    "    enc = add_start_end(enc)\n",
    "    return enc\n",
    "\n",
    "  @tf.function\n",
    "  def detokenize(self, tokenized):\n",
    "    words = self.tokenizer.detokenize(tokenized)\n",
    "    return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "  @tf.function\n",
    "  def lookup(self, token_ids):\n",
    "    return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_size(self):\n",
    "    return tf.shape(self.vocab)[0]\n",
    "\n",
    "  @tf.function\n",
    "  def get_vocab_path(self):\n",
    "    return self._vocab_path\n",
    "\n",
    "  @tf.function\n",
    "  def get_reserved_tokens(self):\n",
    "    return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHzEnTQM6nBD"
   },
   "source": [
    "Построение `CustomTokenizer` для каждого языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:23.957914Z",
     "iopub.status.busy": "2022-07-26T12:04:23.957474Z",
     "iopub.status.idle": "2022-07-26T12:04:26.067679Z",
     "shell.execute_reply": "2022-07-26T12:04:26.067006Z"
    },
    "id": "cU8yFBCSruz4"
   },
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYfrmDhy6syT"
   },
   "source": [
    "Экспорт токенайзеров как `saved_model`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:26.071371Z",
     "iopub.status.busy": "2022-07-26T12:04:26.071145Z",
     "iopub.status.idle": "2022-07-26T12:04:28.071450Z",
     "shell.execute_reply": "2022-07-26T12:04:28.070768Z"
    },
    "id": "aieDGooa9ms7"
   },
   "outputs": [],
   "source": [
    "model_name = 'ted_hrlr_translate_ru_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoCMz2Fm61v6"
   },
   "source": [
    "Обновление `saved_model` и проверка методов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:28.077146Z",
     "iopub.status.busy": "2022-07-26T12:04:28.076887Z",
     "iopub.status.idle": "2022-07-26T12:04:28.816399Z",
     "shell.execute_reply": "2022-07-26T12:04:28.815804Z"
    },
    "id": "9SB_BHwqsHkb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7796"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:28.819455Z",
     "iopub.status.busy": "2022-07-26T12:04:28.819216Z",
     "iopub.status.idle": "2022-07-26T12:04:29.072026Z",
     "shell.execute_reply": "2022-07-26T12:04:29.071420Z"
    },
    "id": "W_Ze3WL3816x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2, 3372, 2214,  691,  952, 2669,    4,    3]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.075193Z",
     "iopub.status.busy": "2022-07-26T12:04:29.074599Z",
     "iopub.status.idle": "2022-07-26T12:04:29.094254Z",
     "shell.execute_reply": "2022-07-26T12:04:29.093727Z"
    },
    "id": "v9o93bzcuhyC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'[START]', b'hello', b'tens', b'##or', b'##f', b'##low', b'!',\n",
       "  b'[END]']]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens = reloaded_tokenizers.en.lookup(tokens)\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.098202Z",
     "iopub.status.busy": "2022-07-26T12:04:29.097751Z",
     "iopub.status.idle": "2022-07-26T12:04:29.215552Z",
     "shell.execute_reply": "2022-07-26T12:04:29.214960Z"
    },
    "id": "Y0205N_8dDT5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello tensorflow !\n"
     ]
    }
   ],
   "source": [
    "round_trip = reloaded_tokenizers.en.detokenize(tokens)\n",
    "\n",
    "print(round_trip.numpy()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSKFDQoBjnNp"
   },
   "source": [
    "Архивирование для [руководств по переводу](https://tensorflow.org/text/tutorials/transformer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ted_hrlr_translate_ru_en_converter\n"
     ]
    }
   ],
   "source": [
    "print(model_name) # ted_hrlr_translate_ru_en_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.218831Z",
     "iopub.status.busy": "2022-07-26T12:04:29.218275Z",
     "iopub.status.idle": "2022-07-26T12:04:29.419791Z",
     "shell.execute_reply": "2022-07-26T12:04:29.418983Z"
    },
    "id": "eY0SoE3Yj2it"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: ted_hrlr_translate_ru_en_converter/ (stored 0%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/saved_model.pb (deflated 91%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/variables/ (stored 0%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/variables/variables.index (deflated 33%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/variables/variables.data-00000-of-00001 (deflated 59%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/assets/ (stored 0%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/assets/ru_vocab.txt (deflated 70%)\r\n",
      "updating: ted_hrlr_translate_ru_en_converter/assets/en_vocab.txt (deflated 54%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r {model_name}.zip {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.423155Z",
     "iopub.status.busy": "2022-07-26T12:04:29.422866Z",
     "iopub.status.idle": "2022-07-26T12:04:29.589306Z",
     "shell.execute_reply": "2022-07-26T12:04:29.588484Z"
    },
    "id": "0Synq0RekAXe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184K\tted_hrlr_translate_ru_en_converter.zip\r\n"
     ]
    }
   ],
   "source": [
    "!du -h *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtmGkGBuGHa2"
   },
   "source": [
    "<a id=\"algorithm\"></a>\n",
    "\n",
    "## Дополнительно: алгоритм\n",
    "\n",
    "\n",
    "Здесь стоит отметить, что существует две версии алгоритма WordPiece: снизу вверх и сверху вниз. В обоих случаях цель одна и та же: \"Учитывая обучающий корпус и количество желаемых токенов D, задача оптимизации состоит в том, чтобы выбрать D словесных частей таким образом, чтобы результирующий корпус был минимальным по количеству частей слова при сегментировании в соответствии с выбранной моделью словаря.\"\n",
    "\n",
    "Оригинальный алгоритм [WordPiece снизу вверх](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), основан на [кодировании байт-пар](https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10). Как и BPE, он начинается с алфавита и итеративно комбинирует общие биграммы для образования частей и слов.\n",
    "\n",
    "Генератор словаря TensorFlow Text следует нисходящеё реализации из [BERT](https://arxiv.org/pdf/1810.04805.pdf). Начинает со слов и разбивает их на более мелкие компоненты, пока они не достигнут порога частоты или не могут быть разбиты дальше. В следующем разделе это подробно описано. Для японского, китайского и корейского языков этот подход сверху вниз не работает, поскольку нет явных единиц слова, с которых можно было бы начать. Для этого вам нужен [другой подход](https://tfhub.dev/google/zh_segmentation/1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLA2QhffYEo0"
   },
   "source": [
    "### Выбор словаря\n",
    "\n",
    "WordPiece алгоритм генерации cверху вниз берет в наборе (слово, количество) пары и порог `T` и возвращает словарь `V`.\n",
    "\n",
    "Алгоритм итерационный. Он выполняет `k` итераций, где обычно`k = 4`, но только первые два действительно важны. Третий и четвертый (и последующие) просто идентичны второму. Обратите внимание, что каждый шаг алгоритма двоичного поиска работает с нуля для `k` итераций.\n",
    "\n",
    "Итерации описанны ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqfY0p3PYIKr"
   },
   "source": [
    "#### Первая итерация\n",
    "\n",
    "1.  Итерация по каждой (слово и счетчик) паре на входе, обозначенной как `(w, c)`.\n",
    "2.  Для каждого слова `w`, генерируется подстрока, обозначаемая `s`. Например, для слова `human`, мы генерируем \n",
    "    `{h, hu, hum, huma, human, ##u, ##um, ##uma, ##uman, ##m, ##ma, ##man, #a, ##an, ##n}`.\n",
    "3.  Ведем хеш-карту подстрока-в-счетчик, и увеличиваем счетчик `c` для каждой `s`. \n",
    "    Например, если мы имеем `(human, 113)` и `(humas, 3)` на нашем входе, счетчик \n",
    "    `s = huma` будет `113+3=116`.\n",
    "4.  После того, как мы собрали значения счетчика для каждой подстроки, перебираем \n",
    "    `(s, c)` пары *начиная с самой длинной `s`*.\n",
    "5.  Храним любую `s` что имеет `c > T`. Например: если `T = 100` и мы имеем \n",
    "    `(pers, 231); (dogs, 259); (##rint; 76)`, тогда мы будем хранили `pers` и `dogs`.\n",
    "6.  Когда `s` сохранена, вычитаем её счетчик из всех её префиксов. \n",
    "    Это является причиной для сортировки всех `s` по длине в шаге 4. Это критическая часть алгоритма, \n",
    "    поскольку в противном случае слова будут подсчитаны дважды.\n",
    "     Например, допустим , что мы сохранили `human` и мы получаем\n",
    "    `(huma, 116)`. Мы знаем, что `113` из этих `116` пришли от `human`, и `3`\n",
    "    пришли от `humas`. Однако теперь, когда `human` находится в нашем словаре, мы знаем\n",
    "    что мы никогда не сегментируем `human` в `huma ##n`. Так как `human` был сохранен,\n",
    "    то `huma` только имеет *эффективный* счетчик `3`.\n",
    "\n",
    "Этот алгоритм будет генерировать набор кусочков слов `s` (многие из которых будет целыми словами `w`), которые мы *могли* бы использовать в качестве нашего WordPiece словаря.\n",
    "\n",
    "\n",
    "Therefore, if we keep the word `human`, we will subtract off the count for `h,\n",
    "hu, hu, huma`, but not for `##u, ##um, ##uma, ##uman` and so on. So we might\n",
    "generate both `human` and `##uman` as word pieces, even though `##uman` will\n",
    "never be applied.\n",
    "\n",
    "Однако есть проблема: этот алгоритм будет сильно генерировать фрагменты слов. Причина в том, что мы вычитаем только счетчики токенов префикса. Поэтому, если мы держим слово `human`, мы вычитаем из счетчика для `h, hu, hu, huma` , но не для `##u, ##um, ##uma, ##uman` и так далее. Таким образом, мы могли бы генерировать как `human` и `##uman` как куски слов, даже если `##uman` никогда не будет применяться.\n",
    "\n",
    "Так почему бы не вычитать из счетчиков для каждой *подстроки*, а не только каждый *префикс*? Потому что тогда мы могли бы вычесть счетчик несколько раз. Допустим, что мы обрабатываем `s` длины 5, и мы продолжаем как `(##denia, 129)` и `(##eniab, 137)`, где `65` из этих подсчетов произошло от слова `undeniable`. Если вычесть из *каждого* подстроки, мы вычитаем `65` из подстроки `##enia` дважды, несмотря на то, что мы должны только вычесть один раз. Однако, если мы будем вычитать только из префиксов, оно будет правильно вычтено только один раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNCtKR8xT9wX"
   },
   "source": [
    "#### Вторая (и третья ...) итерация\n",
    "\n",
    "Чтобы решить проблему избыточной генерации, упомянутую выше, мы выполняем несколько итераций алгоритма.\n",
    "\n",
    "Последующие итерации идентичны первому, но с одним важным отличием: в шаге 2, вместо того, чтобы рассматривать *каждую* подстроку, мы применяем алгоритм WordPiece лексемизации с использованием словаря из предыдущей итерации, и рассматривать только подстроки, которые *начинаются* на точках разделения.\n",
    "\n",
    "Например, допустим , что мы выполняем шаг 2 алгоритма и сталкиваются слово `undeniable`. \n",
    "В первой итерации, мы будем рассматривать каждую подстроку, например,\n",
    "`{u, un, und, ..., undeniable, ##n, ##nd, ..., ##ndeniable, ...}`.\n",
    "\n",
    "Теперь, для второй итерации, мы рассмотрим только их подмножество. Предположим, что после первой итерации соответствующие части слова:\n",
    "\n",
    "`un, ##deni, ##able, ##ndeni, ##iable`\n",
    "\n",
    "Алгоритм WordPiece сегментирует это в `un ##deni ##able` (смотрите раздел [Применение WordPiece](#applying-wordpiece) для получения дополнительной информации). В этом случае мы будем рассматривать только подстроки, которые *начинаются* в точке сегментации. \n",
    "Мы еще рассмотрим каждое возможное *конечное* положение. Таким образом, во второй итерации, множество `s` для `undeniable` является:\n",
    "\n",
    "`{u, un, und, unden, undeni, undenia, undeniab, undeniabl,\n",
    "undeniable, ##d, ##de, ##den, ##deni, ##denia, ##deniab, ##deniabl\n",
    ", ##deniable, ##a, ##ab, ##abl, ##able}`\n",
    "\n",
    "В остальном алгоритм идентичен. В этом примере, в первой итерации, алгоритм производит ложные токены `##ndeni` и `##iable`. Теперь эти токены никогда не учитываются, поэтому они не будут сгенерированы на второй итерации. Мы выполняем несколько итераций, чтобы убедиться, что результаты сходятся (хотя буквальной гарантии сходимости нет)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdUkqe84YQA5"
   },
   "source": [
    "### Применение WordPiece\n",
    "\n",
    "<a id=\"applying_wordpiece\"></a>\n",
    "\n",
    "После создания словаря WordPiece нам нужно иметь возможность применять его к новым данным. Алгоритм представляет собой простое «жадное» приложение с поиском наиболее длинного совпадения.\n",
    "\n",
    "Например, рассмотрим сегментирование слово `undeniable`.\n",
    "\n",
    "We first lookup `undeniable` in our WordPiece dictionary, and if it's present,\n",
    "we're done. If not, we decrement the end point by one character, and repeat,\n",
    "e.g., `undeniabl`.\n",
    "\n",
    "Сначала ищем `undeniable` в нашем WordPiece словаре, и если оно присутствует, готово. Если нет, то мы уменьшаем конечную точку на один символ, и повторяем, например, `undeniabl` .\n",
    "\n",
    "В конце концов, мы либо найдем подтокен в нашем словаре, либо перейдем к подтокену, состоящему из одного символа. (В целом, мы считаем, что каждый символ в нашем словаре, хотя это может быть не так для редких символов Unicode. Если мы встречаем редкий символ Unicode, что не в словаре, мы просто отображаем всё слово `<unk>`).\n",
    "\n",
    "В этом случае, мы находим `un` в нашем словаре. Итак, это наш первый отрывок из слов. Затем мы переходим к концу `un` и повторяем обработку, например, попытаемся найти `##deniable`, затем `##deniabl` и т.д. Это повторяется до тех пор, пока не сегментируем слово целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjRQKQzpYMl2"
   },
   "source": [
    "### Объяснение\n",
    "\n",
    "Интуитивно понятно, что токенизация уровня сслова пытается решить две разные задачи:\n",
    "\n",
    "1.  Токенизировать данные в наименьшее число частей, как это возможно. Важно помнить, что алгоритм WordPiece не «хочет» разбивать слова. Иначе, он бы просто разделил каждое слово на символы, например,  `люди -> {л, ##ю, ##д, #и}`. Это критическая вещь, которая делает WordPiece отличным от морфологических разветвителей, которые разделят лингвистические морфемы даже для общих слов (например: `любимые -> {лю, бим, ые}`).\n",
    "\n",
    "2.  Когда слово действительно нужно разбить на части, разбейте его на части, которые\n",
    "    имеют максимальный счетчик в обучающих данных. Например, причина , почему слово\n",
    "    `undeniable` будет разбито на `{un, ##deni, ##able}` в отличии от альтернативы `{unde, ##niab, ##le}` является то, что счетчики для `un` и `##able` в частности, будет очень высоким, так как это общие префиксы и суффиксы. Даже несмотря на то, счетчик для `##le` должен быть выше , чем `##able` , низкие счетчики `unde` и `##niab` сделает это менее «желательной» токенизацией для алгоритма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQZ38Uus-Xv1"
   },
   "source": [
    "## Дополнительно: tf.lookup\n",
    "\n",
    "<a id=\"tf.lookup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NreDSRmJNG_h"
   },
   "source": [
    "Если вам нужен доступ или больше контроля над словарем стоит отметить, что вы можете создать таблицу поиска сами и передать в `BertTokenizer`.\n",
    "\n",
    "Когда вы передаете строку, `BertTokenizer` выполняет следующие действия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.594123Z",
     "iopub.status.busy": "2022-07-26T12:04:29.593505Z",
     "iopub.status.idle": "2022-07-26T12:04:29.600088Z",
     "shell.execute_reply": "2022-07-26T12:04:29.599529Z"
    },
    "id": "thAF1DzQOQXl"
   },
   "outputs": [],
   "source": [
    "ru_lookup = tf.lookup.StaticVocabularyTable(\n",
    "    num_oov_buckets=1,\n",
    "    initializer=tf.lookup.TextFileInitializer(\n",
    "        filename='ru_vocab.txt',\n",
    "        key_dtype=tf.string,\n",
    "        key_index = tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "        value_dtype = tf.int64,\n",
    "        value_index=tf.lookup.TextFileIndex.LINE_NUMBER)) \n",
    "ru_tokenizer = text.BertTokenizer(ru_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERY4FYN7O66R"
   },
   "source": [
    "Теперь у вас есть прямой доступ к таблице поиска, используемой в токенизаторе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.602914Z",
     "iopub.status.busy": "2022-07-26T12:04:29.602472Z",
     "iopub.status.idle": "2022-07-26T12:04:29.608636Z",
     "shell.execute_reply": "2022-07-26T12:04:29.608127Z"
    },
    "id": "337_DcAMOs6N"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([7832, 7832, 7832, 7832, 7832])>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_lookup.lookup(tf.constant(['é', 'um', 'uma', 'para', 'não']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdZ82x5mPDE9"
   },
   "source": [
    "Вам не нужно использовать словарный файл, `tf.lookup` имеет другие параметры инициализатора. Если у вас есть словарный запас в памяти можно использовать `lookup.KeyValueTensorInitializer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-26T12:04:29.611608Z",
     "iopub.status.busy": "2022-07-26T12:04:29.611191Z",
     "iopub.status.idle": "2022-07-26T12:04:29.621098Z",
     "shell.execute_reply": "2022-07-26T12:04:29.620503Z"
    },
    "id": "mzkrmO9H-b9i"
   },
   "outputs": [],
   "source": [
    "ru_lookup = tf.lookup.StaticVocabularyTable(\n",
    "    num_oov_buckets=1,\n",
    "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=ru_vocab,\n",
    "        values=tf.range(len(ru_vocab), dtype=tf.int64))) \n",
    "ru_tokenizer = text.BertTokenizer(ru_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конец"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "subwords_tokenizer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
