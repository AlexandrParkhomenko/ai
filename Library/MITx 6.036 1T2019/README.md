# MIT 6.036 Introduction to Machine Learning

## Week 1: Basics
### Introduction to ML
- [Introduction to ML - Perspective and history](https://youtu.be/kXOsRyIVAdo)
- [Estimation and generalization](https://youtu.be/zdSiYs8zSJQ)
- [Supervised learning -setting](https://youtu.be/wrfC_0EiUow)
- [Supervised learning - hypotheses](https://youtu.be/92hkMDcpwkQ)
- [Evaluating predictions - loss functions](https://youtu.be/JGFC-3XCUQs)
- [Evaluating hypotheses - training set error](https://youtu.be/gLuo7u-5ezs)
- [Learning algorithms](https://youtu.be/O924ba_ztu8)

### Linear classifiers 
- [Linear classifiers - one level](https://youtu.be/yOKDzd73KgM)
- [Linear classifiers - The random linear classifier algorithm](https://youtu.be/Mm314LUrNN8)

## Week 2: Perceptrons
- [The perceptron algorithm](https://youtu.be/Rn_x12VHplM)
- [Action - an example](https://youtu.be/QLJa1g9n6Ms)
- [Evaluating learning algorithms - validation](https://youtu.be/X_nOMVxfK08)
- [Overview of plan](https://youtu.be/dmmlllblM1Y)
- [Through origin algorithm](https://youtu.be/tFdk5BZhMrU)
- [Linear separability](https://youtu.be/uDClajMTmZw)
- [Margin of a dataset](https://youtu.be/Nsn82ZQt4Yc)
- [Convergence theorem](https://youtu.be/fjByzXxgYgk)
- [Proof sketch of the theorem](https://youtu.be/DRr3zUmI9WU)

## Week 3: Features
- [Transforming through-origin to not-through-origin](https://youtu.be/5TW1A1ToaXg)
- [Polynomial basis](https://youtu.be/RwOx668Jt9E)
- [Example of perceptron algorithm with polynomial basis transformations](https://youtu.be/KXJ9sUsKXP4)
- [Strategies for dealing with varied data](https://youtu.be/kC6mFgyeAtQ)

## Week 4: Margin Maximization
### Logistic Regression
- [ML as optimization - framework](https://youtu.be/ajjRNuYS_aM)
- [ML as optimization - setting and sigmoid function](https://youtu.be/KDwtUUpuQnY)
- [Linear logistic classifier - hypothesis class](https://youtu.be/P41lDuEmEoo)
- [Linear logistic classifier - negative log likelihood](https://youtu.be/ELm-D3MPclQ)
- [ML as optimization - gradient descent in one dimension](https://youtu.be/_bWaKhe0t6Y)
- [ML as optimization - gradient descent in multiple dimensions](https://youtu.be/JHXxx3jf8ec)
- [One-dimensional linear regression - demo](https://youtu.be/62vmnK0Cl1c)
- [Two-dimensional linear regression - demo](https://youtu.be/KWYZfYsCi4Y)
- [Linear logistic classifier - a few comments about regularization](https://youtu.be/AHwdS1XgKjk)

### Gradient Descent
- [Gradient descent optimization - algorithm in one dimension](https://youtu.be/8g2cFGnVyS0)
- [Gradient descent optimization - local optima](https://youtu.be/gaIOqD3Btmo)
- [Gradient descent optimization - algorithm in multiple dimension](https://youtu.be/0Y4jQJwVW1M)
- [Gradient descent optimization - parameters and demo](https://youtu.be/nD90J-FuoEw)

## Week 5: Regression
- [Regression and the ordinary least squares problem](https://youtu.be/GYY_VzamnaY)
- [Regression - ordinary least squares solution using optimization](https://youtu.be/ZdZnwnXHllM)
- [Regression - OLS analytical solution setup](https://youtu.be/bmCSFd4d_Cc)
- [Regression - OLS analytical solution using gradients](https://youtu.be/sg9jErBC_pA)
- [Regression - beauty of the closed form OLS solution](https://youtu.be/zZkH5u6f1Hw)
- [Regression - regularization by ridge regression](https://youtu.be/DpVEKoBcQHY)
- [Regression - analytical minimization of the ridge regression objective](https://youtu.be/aUYKpLZGvu4)
- [Regression - ridge regression using gradient descent](https://youtu.be/e2m444vvVf0)
- [Regression - stochastic gradient descent](https://youtu.be/O7M7xyUcxTU)
- [Regression - OLS and gradient descent demo example](https://youtu.be/-72TMOHAQnc)
- [Regression - structural error and estimation error](https://youtu.be/7-G8ZFL2oiE)

## Week 6: Neural Networks I
- [Neural Networks - basic element](https://youtu.be/jUcdIVQyXow)
- [Neural Networks - layer definition](https://youtu.be/30b8FPTPVik)
- [Neural Networks - many layers](https://youtu.be/z-rkib02AuI)
- [Neural Networks - activation functions](https://youtu.be/SrV8aS20698)
- [Neural Networks - training and back-propagation](https://youtu.be/2N7N6PJWqM0)
- [Neural Networks - backprop with the chain rule](https://youtu.be/B9BHcTxUMMc)
- [Neural Networks - weight initialization](https://youtu.be/xOy85gT5wOo)
- [Neural Networks - output layer activation functions](https://youtu.be/AtYktvqEgNA)

 ## Week 7: Neural Networks II, Making NN's Work 
- [Neural Networks - brief review of layers and backprop](https://youtu.be/S4l7XuMVxwA)
- [Neural Networks - optimizing parameters - batch gradient descent training](https://youtu.be/9ETYna-tOMQ)
- [Neural Networks - optimizing parameters - adaptive step-size](https://youtu.be/gQafJTkYfv4)
- [Neural Networks - optimizing parameters - running averages](https://youtu.be/tO_lQilOutA)
- [Neural Networks - optimizing parameters - momentum](https://youtu.be/OXdtfhiLsWk)
- [Neural Networks - optimizing parameters - adagrad and adadelta](https://youtu.be/B0Zixpgysns)
- [Neural Networks - optimizing parameters - adam step-size update strategy](https://youtu.be/y6eDig5zgZM)
- [Neural Networks - regularization by weight decay](https://youtu.be/81nv6dx0HQE)
- [Neural Networks - regularization by early stopping and dropout](https://youtu.be/4-kgCZ6S6rs)
- [Neural Networks - regularization by batch normalization](https://youtu.be/1mhciX6XfAo)

## Week 8: Convolutional Neural Networks
- [CNNs - intro](https://youtu.be/hm7lfUg3obs)
- [CNNs - one-dimensional filters](https://youtu.be/oqMwT1P7u3Y)
- [CNNs - two-dimensional filters](https://youtu.be/I0RZ7jF9_H4)
- [CNNs - a specific illustrative example filter](https://youtu.be/RRRCff0w-84)
- [CNNs - layers](https://youtu.be/b73F9gJ4JjE)
- [CNNs - max pooling](https://youtu.be/9QMhPxvgX7c)
- [CNNs - typical architecture](https://youtu.be/9l7EiBobcTs)
- [CNNs - backprop and gradient descent](https://youtu.be/wjsRjggZcu0)

## Week 9: State Machines and Markov Decision Processes
- [Sequential models - state machines](https://youtu.be/tocmxrAmfAs)
- [SM - State machine as a transducer](https://youtu.be/vohmh6XIx1c)
- [SM - Towards recurrent neural networks](https://youtu.be/iWBtDjw8wqw)
- [SM - Markov decision processes - states and actions](https://youtu.be/rxk1BoFRghk)
- [SM - MDP - the transition function](https://youtu.be/n0MvChqTumQ)
- [SM - MDP - the reward and policy functions](https://youtu.be/kyvWUCDOrfc)
- [SM - MDP - finite horizon and the value function](https://youtu.be/PWt-TF0puSY)
- [SM - MDP - computing the value function](https://youtu.be/MOESKBfuxSc)
- [SM - MDP - finding an optimal policy](https://youtu.be/R6mmXwNYmeg)
- [SM - MDP - infinite-horizons and the value iteration algorithm](https://youtu.be/Nuo_8ZacVXo)
- [SM - MDP - Grid world example demos](https://youtu.be/0eFJvy2U0BU)

 ## Week 10: Reinforcement Learning 
- [Intro to Reinforcement Learning](https://youtu.be/RD8c9atplDU)
- [K-armed bandits](https://youtu.be/Jq15p4Jt3x8)
- [Objectives of the RL problem](https://youtu.be/0DAzFI69Q44)
- [Model-based learning](https://youtu.be/gTnTyPRPUJo)
- [Policy search](https://youtu.be/-oGUCUbWHdQ)
- [Q-learning](https://youtu.be/YMQlGyB37g8)
- [Q-learning select-action strategies](https://youtu.be/fEcP3XtoLDc)
- [Neural networks and Q-learning ](https://youtu.be/bm6Nqrtsv8Q)
- [Reinforcement learning demos](https://youtu.be/t6aKxyaE4s4)

## Week 11: Recurrent Neural Networks
- [Recurrent neural network model](https://youtu.be/BIZRv1goajo)
- [Sequence-to-sequence RNN](https://youtu.be/KfEVnCsCrN4)
- [Back-propagation through time - forward pass](https://youtu.be/HaUxM6PvRQ4)
- [Back-propagation through time - backward pass](https://youtu.be/I3bJyH1j0vQ)
- [Back-propagation through time - weight updates](https://youtu.be/Yt6bPJ-WRtI)
- [RNNs - training a language model](https://youtu.be/z5hF3tQVQpY)
- [RNNs - gating mechanisms and LSTM](https://youtu.be/XLepfA7iPgE)

## Week 12: Recommender Systems
- [Introduction](https://youtu.be/EOZd_kY9sLs)
- [Collaborative filtering - framework](https://youtu.be/wKzdFan5FeU)
- [Collaborative filtering - strategy](https://youtu.be/U8fIX3aNvLU)
- [Collaborative filtering - hypothesis space](https://youtu.be/nrl_RL6isxQ)
- [Collaborative filtering - objective function](https://youtu.be/E4NwAfdaLbs)
- [Collaborative filtering - alternating least squares idea](https://youtu.be/nSDrIgBVgkQ)
- [Collaborative filtering - alternating least squares algorithm](https://youtu.be/LwFSize-9s0)
- [Collaborative filtering - stochastic gradient descent](https://youtu.be/MCMVGQgecNY)
- [Recommender Systems - Demo example - RNNs](https://youtu.be/K9MkK8FylwA)

## Week 13: Decision Trees and Nearest Neighbors
- [Intro to non-parametric models](https://youtu.be/wJ63CNfgSDY)
- [Decision trees](https://youtu.be/hDNFt7-Gvlc)
- [Regression trees](https://youtu.be/0kpbar28_h8)
- [Building a tree - greedy algorithm](https://youtu.be/b81imrBlVKE)
- [Building a tree - minimum error split](https://youtu.be/e5l1Ycfc9p8)
- [Building a tree - pruning](https://youtu.be/qGUFy_h8cZw)
- [Classification trees](https://youtu.be/DV2YYIhFNwM)
- [Classification trees - impurity measures - gini index](https://youtu.be/iKEtdZUHmJ0)
- [Classification trees - impurity measures - entropy](https://youtu.be/F80O7uBkgPg)
- [Decision trees - the good and the bad](https://youtu.be/cHdgFU5blCE)
- [Bagging - bootstrap aggregation of models](https://youtu.be/2XFoq1NRiIg)
- [Random forests models](https://youtu.be/p5PTeED4fFY)
- [Nearest neighbor models](https://youtu.be/7B2eHRSt284)

