1
00:00:04,100 --> 00:00:06,180
Right. We're going to get started.

2
00:00:06,180 --> 00:00:07,965
This is the last lecture for the term.

3
00:00:07,965 --> 00:00:12,210
Um, uh, just a few logistics things that we're getting at the beginning.

4
00:00:12,210 --> 00:00:17,865
Um, so just a friendly reminder that the project write up is due on the 20th at 11:59 PM.

5
00:00:17,865 --> 00:00:23,340
There are no late days, and then the poster presentations are on Friday at 8:30 AM.

6
00:00:23,340 --> 00:00:26,040
[NOISE] Uh, you need to submit your poster as

7
00:00:26,040 --> 00:00:29,550
well and that should be submitted online to Gradescope by the same time.

8
00:00:29,550 --> 00:00:32,400
Um, we'll open up a submission for that in advance.

9
00:00:32,400 --> 00:00:34,935
Um, and there's also no late dates for that.

10
00:00:34,935 --> 00:00:38,385
Uh, you should have received an email with some details about the poster session.

11
00:00:38,385 --> 00:00:39,855
Any questions? Reach out to us.

12
00:00:39,855 --> 00:00:41,745
Does anybody have any questions right now?

13
00:00:41,745 --> 00:00:44,685
It's the last week of office hours.

14
00:00:44,685 --> 00:00:46,170
We won't have office hours next week.

15
00:00:46,170 --> 00:00:47,970
It's finals week for most people.

16
00:00:47,970 --> 00:00:52,180
Um, but you can reach out to us on Piazza or if you have extra questions, you know,

17
00:00:52,180 --> 00:00:55,240
we're happy to find the time.

18
00:00:55,240 --> 00:00:58,020
Okay. All right.

19
00:00:58,020 --> 00:00:59,385
So what we're going to do today is,

20
00:00:59,385 --> 00:01:01,465
uh, so last time, of course, was the quiz.

21
00:01:01,465 --> 00:01:04,510
Uh, and we're gonna be sending out- our goal is to send

22
00:01:04,510 --> 00:01:07,300
out grades for everybody that took it on Monday to send them out today.

23
00:01:07,300 --> 00:01:08,620
We're almost done grading those.

24
00:01:08,620 --> 00:01:10,780
Um, [NOISE] and for the- there are a

25
00:01:10,780 --> 00:01:13,225
few people who are still taking that late so though- uh,

26
00:01:13,225 --> 00:01:14,980
we have to grade the SCPDs.

27
00:01:14,980 --> 00:01:16,930
But everybody else should get their quiz scores

28
00:01:16,930 --> 00:01:18,805
who took it Monday, should get back today.

29
00:01:18,805 --> 00:01:21,400
And then today what we're gonna do is we're going to talk just a little bit about

30
00:01:21,400 --> 00:01:25,400
Monte-Carlo tree search as well as discuss some end-of-course stuff.

31
00:01:25,400 --> 00:01:27,720
So why Monte-Carlo tree search?

32
00:01:27,720 --> 00:01:31,305
Um, who here has heard of AlphaGo? All right.

33
00:01:31,305 --> 00:01:32,545
Yes. So I mean AlphaGo,

34
00:01:32,545 --> 00:01:36,190
you could argue is one of the major AI achievements of the last,

35
00:01:36,190 --> 00:01:38,130
you know, 10 to 20 years, um,

36
00:01:38,130 --> 00:01:39,470
and it really has been

37
00:01:39,470 --> 00:01:43,160
a spectacular achievement that was achieved much faster than was anticipated,

38
00:01:43,160 --> 00:01:46,415
uh, to beat humans on the board game Go,

39
00:01:46,415 --> 00:01:48,710
which is considered an extremely hard game.

40
00:01:48,710 --> 00:01:51,970
So the Monte Carlo tree search was a critical part of,

41
00:01:51,970 --> 00:01:55,040
you know, to achieve this success plus a lot of other additional things.

42
00:01:55,040 --> 00:01:58,475
But it's one of the aspects that we have not talked about very much so far in class.

43
00:01:58,475 --> 00:02:00,260
So I think talking about Monte-Carlo tree search,

44
00:02:00,260 --> 00:02:02,120
so you're familiar with some of the ideas behind

45
00:02:02,120 --> 00:02:04,595
that and therefore some of the ideas behind AlphaGo,

46
00:02:04,595 --> 00:02:06,890
um, are useful, uh, to be aware of.

47
00:02:06,890 --> 00:02:10,699
Uh, and then also because when we start to think about Monte-Carlo tree search,

48
00:02:10,699 --> 00:02:13,820
it's a way for us to think about model-based reinforcement learning,

49
00:02:13,820 --> 00:02:17,780
which is a very powerful tool that we haven't talked about as much in part

50
00:02:17,780 --> 00:02:23,195
because we haven't seen as much success in the deep learning case with models.

51
00:02:23,195 --> 00:02:26,465
And I'm happy to talk more about that either today or offline.

52
00:02:26,465 --> 00:02:30,380
But I think that going forward it's likely to be a really productive avenue of research.

53
00:02:30,380 --> 00:02:35,140
And we can talk about why that might be particularly useful in alpha in AlphaGo.

54
00:02:35,140 --> 00:02:38,570
Okay. So what we're gonna do first is we're gonna sort

55
00:02:38,570 --> 00:02:41,540
of talk a little bit again about sort of model-based reinforcement learning.

56
00:02:41,540 --> 00:02:44,550
And then we'll talk about simulates- simulation-based search,

57
00:02:44,550 --> 00:02:46,755
which is where Monte Carlo tree search comes up.

58
00:02:46,755 --> 00:02:49,595
Actually, just because everyone takes different classes and I'm curious,

59
00:02:49,595 --> 00:02:53,700
who here has covered Monte Carlo tree search in a, in another class?

60
00:02:53,740 --> 00:02:56,420
Okay. Just two. What class was it?

61
00:02:56,420 --> 00:02:57,210
[inaudible].

62
00:02:57,210 --> 00:02:59,540
238.

63
00:02:59,540 --> 00:02:59,750
Yeah.

64
00:02:59,750 --> 00:03:00,490
Yeah.

65
00:03:00,490 --> 00:03:01,430
Same?

66
00:03:01,430 --> 00:03:04,440
Same one. It was mentioned a little bit like [NOISE]

67
00:03:04,440 --> 00:03:05,160
It was mentioned briefly.

68
00:03:05,160 --> 00:03:06,645
Ah, yeah. Very brief.

69
00:03:06,645 --> 00:03:08,025
Yeah. And yeah?

70
00:03:08,025 --> 00:03:09,105
217.

71
00:03:09,105 --> 00:03:09,735
Which is?

72
00:03:09,735 --> 00:03:10,755
General game play.

73
00:03:10,755 --> 00:03:13,260
Oh, yeah. General game play would be a good one to come and bring it in.

74
00:03:13,260 --> 00:03:17,400
Okay. Cool. Awesome. Oh, and also I just think people are interested.

75
00:03:17,400 --> 00:03:18,920
At the end, I'll also mention some other classes

76
00:03:18,920 --> 00:03:20,945
where you can learn more about reinforcement learning.

77
00:03:20,945 --> 00:03:23,440
All right. So model-based reinforcement learning.

78
00:03:23,440 --> 00:03:26,450
Um, [NOISE], most of what we've talked about this term though not all of it,

79
00:03:26,450 --> 00:03:28,010
but most of what we've talked about this term [BACKGROUND]

80
00:03:28,010 --> 00:03:29,630
particularly when we're talking about learning,

81
00:03:29,630 --> 00:03:31,700
which means we don't know how the world works, um,

82
00:03:31,700 --> 00:03:34,280
is that we're thinking about either learning a policy

83
00:03:34,280 --> 00:03:37,355
or a value function or both directly from data.

84
00:03:37,355 --> 00:03:39,320
Um, and what we're gonna talk about more today is

85
00:03:39,320 --> 00:03:41,060
talking about learning a specific model.

86
00:03:41,060 --> 00:03:44,470
So just to remind ourselves because it has been a little while.

87
00:03:44,470 --> 00:03:52,470
We're gonna be talking about learning the transition and/or reward model,

88
00:03:53,300 --> 00:03:56,915
and we talked about this a little bit maybe, I don't know,

89
00:03:56,915 --> 00:03:58,730
a number of- ah, a few weeks ago,

90
00:03:58,730 --> 00:04:00,620
It came up also in exploration.

91
00:04:00,620 --> 00:04:02,060
But once you have a model,

92
00:04:02,060 --> 00:04:03,725
you can use planning with that.

93
00:04:03,725 --> 00:04:05,180
And just to refresh our memories,

94
00:04:05,180 --> 00:04:07,100
planning is where we take a known model of

95
00:04:07,100 --> 00:04:10,160
the world and then we use value iteration or policy

96
00:04:10,160 --> 00:04:13,085
iteration or dynamic programming in general

97
00:04:13,085 --> 00:04:17,430
to try to compute a policy for those given models.

98
00:04:18,320 --> 00:04:20,329
In contrast to that, of course,

99
00:04:20,329 --> 00:04:22,250
we've talked a lot about Model-Free RL,

100
00:04:22,250 --> 00:04:26,185
where there's no model and we just directly learn a value function from experience.

101
00:04:26,185 --> 00:04:31,030
And now, we are going to learn a model from experience and then plan using that.

102
00:04:31,030 --> 00:04:33,755
Now, the planning that we do in addition to sort of

103
00:04:33,755 --> 00:04:36,575
the approaches that are known from classical decision, uh,

104
00:04:36,575 --> 00:04:39,500
like dy- dynamic programming also can be

105
00:04:39,500 --> 00:04:42,500
any of the other techniques that we've talked about so far in class.

106
00:04:42,500 --> 00:04:44,020
So, you know, once we have this,

107
00:04:44,020 --> 00:04:47,000
this is a- this can act as a simulator.

108
00:04:47,000 --> 00:04:49,050
And so once you have that,

109
00:04:49,050 --> 00:04:54,415
you can do model-free RL using that simulator,

110
00:04:54,415 --> 00:04:57,290
or you can do policy search or anything

111
00:04:57,290 --> 00:05:00,320
else you would like to do given that you have a model of the world.

112
00:05:00,320 --> 00:05:02,240
It basically just acts as a simulator,

113
00:05:02,240 --> 00:05:08,355
and you can use it to generate experience as well as do things like dynamic programming.

114
00:05:08,355 --> 00:05:10,880
Okay. You can do sort of all of those things.

115
00:05:10,880 --> 00:05:13,205
So once you have a simulator of the world, that's great.

116
00:05:13,205 --> 00:05:15,980
The downside of course can be if that simulator is not very good.

117
00:05:15,980 --> 00:05:19,500
What does that do in terms of the resulting estimates?

118
00:05:19,600 --> 00:05:22,940
Okay. So just to think of it again.

119
00:05:22,940 --> 00:05:24,100
We have our world.

120
00:05:24,100 --> 00:05:27,310
It's generating actions and rewards and states.

121
00:05:27,310 --> 00:05:31,625
Um, and now we're going to think about sort of explicitly trying to model those.

122
00:05:31,625 --> 00:05:36,740
So in a lot of cases you may know the reward function, not always.

123
00:05:36,740 --> 00:05:38,390
But in a lot of practical applications,

124
00:05:38,390 --> 00:05:39,680
you'll know the reward function.

125
00:05:39,680 --> 00:05:41,570
So if you're designing like a reinforcement learning

126
00:05:41,570 --> 00:05:44,340
based system for something like customer service,

127
00:05:44,340 --> 00:05:46,789
you probably have a reward function in mind,

128
00:05:46,789 --> 00:05:50,240
like engagement or purchases or things like that.

129
00:05:50,240 --> 00:05:52,925
But you may not have a very good model of the customer.

130
00:05:52,925 --> 00:05:56,030
So there are a lot of practical examples where you'll need to

131
00:05:56,030 --> 00:05:59,615
learn the dynamics model either implicitly or explicitly.

132
00:05:59,615 --> 00:06:02,640
But the reward function itself might be known.

133
00:06:05,540 --> 00:06:08,890
All right. So how do we think about this in terms of a loop?

134
00:06:08,890 --> 00:06:11,110
We think about having,

135
00:06:11,110 --> 00:06:12,880
um, [NOISE] some experience.

136
00:06:12,880 --> 00:06:14,510
So this could be things like,

137
00:06:14,510 --> 00:06:16,390
you know, state, action, reward,

138
00:06:16,390 --> 00:06:19,350
next state tuples that we then feed into a model,

139
00:06:19,350 --> 00:06:23,515
and this is going to output either a reward or a transition model.

140
00:06:23,515 --> 00:06:25,060
We do planning with that,

141
00:06:25,060 --> 00:06:28,705
which can be dynamic programming or Q learning

142
00:06:28,705 --> 00:06:33,235
or many of the other techniques we've seen here, policy search.

143
00:06:33,235 --> 00:06:35,575
And then that has to give us a way to pick an action.

144
00:06:35,575 --> 00:06:40,135
So that has to give us an action that we can then use over here,

145
00:06:40,135 --> 00:06:43,765
and we don't have to necessarily compute a full value function.

146
00:06:43,765 --> 00:06:46,615
All we need to know is what is the next action to take,

147
00:06:46,615 --> 00:06:49,385
and we're going to exploit that when we get to Monte Carlo tree search.

148
00:06:49,385 --> 00:06:51,980
That we don't necessarily have to compute a full value function for

149
00:06:51,980 --> 00:06:54,605
the world nor do we have to have a complete policy.

150
00:06:54,605 --> 00:07:01,620
All we have to know is what should we do for this particular action next.

151
00:07:03,850 --> 00:07:07,940
So some of the advantages about this is, um,

152
00:07:07,940 --> 00:07:11,060
we've a lot of supervised learning methods including from deep,

153
00:07:11,060 --> 00:07:13,925
uh, deep learning which we can use to learn models.

154
00:07:13,925 --> 00:07:17,165
Some of them are better or worse super, uh, suited.

155
00:07:17,165 --> 00:07:18,820
So our transition dynamics,

156
00:07:18,820 --> 00:07:20,765
we're generally going to think of a stochastic.

157
00:07:20,765 --> 00:07:24,710
So we're going to need supervised learning methods that can predict distributions.

158
00:07:24,710 --> 00:07:27,830
For reward models we can often treat them as scalars.

159
00:07:27,830 --> 00:07:32,075
So then we can use very classic regression-based approaches.

160
00:07:32,075 --> 00:07:33,710
And the, the other nice thing about

161
00:07:33,710 --> 00:07:37,265
model-based reinforcement learning is like what we talked about for exploration,

162
00:07:37,265 --> 00:07:39,920
um, that we can often have explicit models over

163
00:07:39,920 --> 00:07:42,965
our uncertainty of how good are our models.

164
00:07:42,965 --> 00:07:45,620
And once we have uncertainty over our models of the world,

165
00:07:45,620 --> 00:07:49,985
we can use that to propagate into uncertainty over the decisions we make.

166
00:07:49,985 --> 00:07:52,670
So in the bandit case that was pretty direct,

167
00:07:52,670 --> 00:07:55,700
because in the bandit case- so for bandits,

168
00:07:55,700 --> 00:08:01,265
we had uncertainty over the reward of an arm,

169
00:08:01,265 --> 00:08:04,280
and that just directly represented our uncertainty over

170
00:08:04,280 --> 00:08:06,670
the value because it was only a single timestep.

171
00:08:06,670 --> 00:08:09,190
In the case of MDPs,

172
00:08:09,190 --> 00:08:11,885
we could represent uncertainty over the,

173
00:08:11,885 --> 00:08:16,190
the reward and the dynamics model and forms of these sort of bonuses,

174
00:08:16,190 --> 00:08:19,010
and then propagate the- that information during planning.

175
00:08:19,010 --> 00:08:22,490
And that again allowed us to think about sort of how well do we

176
00:08:22,490 --> 00:08:25,940
know the value of different states and actions and what could it be,

177
00:08:25,940 --> 00:08:28,420
what sort of could it be optimistically.

178
00:08:28,420 --> 00:08:30,915
Now the downsides is that, you know,

179
00:08:30,915 --> 00:08:33,770
first we're gonna learn a model and then we're gonna construct a value function,

180
00:08:33,770 --> 00:08:37,200
and there could be two sources of approximation error there.

181
00:08:37,450 --> 00:08:41,720
Because we're going to get an approximate model and then we're gonna do

182
00:08:41,720 --> 00:08:45,230
approximate planning in general for large state spaces,

183
00:08:45,230 --> 00:08:48,760
and so we can get compounding errors in that case.

184
00:08:48,760 --> 00:08:51,580
Now, another place that we saw compounding errors earlier in

185
00:08:51,580 --> 00:08:53,710
this course was when we talked about imitation learning.

186
00:08:53,710 --> 00:08:57,010
And we talked about if you had a trajectory and then you tried to

187
00:08:57,010 --> 00:09:00,680
do behavior cloning and learn mappings from states to actions,

188
00:09:00,680 --> 00:09:03,760
and how if you then got that policy and followed it in the real world,

189
00:09:03,760 --> 00:09:06,250
you might end up in parts of the state space where you didn't have

190
00:09:06,250 --> 00:09:11,045
much data and you could have sort of these escalating errors because,

191
00:09:11,045 --> 00:09:12,975
um, again it could compound.

192
00:09:12,975 --> 00:09:15,345
Once you get in parts of the state space where you don't have much data,

193
00:09:15,345 --> 00:09:16,470
and then you're extrapolating,

194
00:09:16,470 --> 00:09:17,770
then things can go badly.

195
00:09:17,770 --> 00:09:19,210
So similarly in this case,

196
00:09:19,210 --> 00:09:23,110
if you build a model and you compute a policy that ends up getting you to

197
00:09:23,110 --> 00:09:24,940
parts of the world where you don't have

198
00:09:24,940 --> 00:09:27,710
very much data and where your model estimate is poor,

199
00:09:27,710 --> 00:09:32,680
then again your resulting value function in your policy might be bad.

200
00:09:33,890 --> 00:09:37,760
I guess I'll just mention one other big advantage I think of with

201
00:09:37,760 --> 00:09:42,005
model-based reinforcement learning is that it can also be very powerful for transfer.

202
00:09:42,005 --> 00:09:45,350
So when Chelsea was here and talking about meta-learning,

203
00:09:45,350 --> 00:09:48,020
one of the nice benefits of model-based RL is

204
00:09:48,020 --> 00:09:50,810
that if you learn a dynamics model of the world,

205
00:09:50,810 --> 00:09:54,050
then if someone changes the reward function,

206
00:09:54,050 --> 00:09:57,415
implicitly you can just do zero shot transfer,

207
00:09:57,415 --> 00:10:01,275
because you can just take your learned model of the dynamics and then your reward function,

208
00:10:01,275 --> 00:10:02,960
you can just compute a new plan.

209
00:10:02,960 --> 00:10:05,920
So like if I'm a robot and I learned how to navigate in this room,

210
00:10:05,920 --> 00:10:07,580
and so like now I know like, you know,

211
00:10:07,580 --> 00:10:09,530
what it's like to turn and what it's like to go forward,

212
00:10:09,530 --> 00:10:11,990
etc., and before I was always trying to get to that exit.

213
00:10:11,990 --> 00:10:16,535
But now I know what dynamic- I know my dynamics model in general for this room.

214
00:10:16,535 --> 00:10:19,435
And then someone says, "No. No. No. I don't want to go to that exit because that one's,

215
00:10:19,435 --> 00:10:20,570
you know, closed or something.

216
00:10:20,570 --> 00:10:22,010
So go to that other exit."

217
00:10:22,010 --> 00:10:23,805
And they tell me the reward function.

218
00:10:23,805 --> 00:10:27,170
They say, you know, there's a +1 for that exit now instead of there.

219
00:10:27,170 --> 00:10:30,590
Then I can just re-plan with my, my dynamics model.

220
00:10:30,590 --> 00:10:33,080
So I don't need any more experience.

221
00:10:33,080 --> 00:10:34,595
I can get zero shot transfer.

222
00:10:34,595 --> 00:10:36,500
So that can be really useful.

223
00:10:36,500 --> 00:10:38,630
So that's one of the other reasons why you

224
00:10:38,630 --> 00:10:41,525
might want to just build models of the world in general.

225
00:10:41,525 --> 00:10:44,980
And there's some interesting evidence that when people play Atari games,

226
00:10:44,980 --> 00:10:47,690
that they are probably systematically building models.

227
00:10:47,690 --> 00:10:50,960
What happens when I move the iceberg next to the polar bear?

228
00:10:50,960 --> 00:10:55,445
And because then you can generalize those models to other experience.

229
00:10:55,445 --> 00:10:57,630
Okay. So how are we gonna write,

230
00:10:57,630 --> 00:10:58,920
write down our model in this case.

231
00:10:58,920 --> 00:11:00,460
We're again just gonna have our normal state,

232
00:11:00,460 --> 00:11:03,310
action, transition, dynamics and reward,

233
00:11:03,310 --> 00:11:08,670
and we're gonna assume that our model approximately represents our,

234
00:11:08,670 --> 00:11:11,245
our transition model and our reward model.

235
00:11:11,245 --> 00:11:15,220
So we're assuming the Markov assumption here.

236
00:11:15,220 --> 00:11:17,590
So we can represent our next state is

237
00:11:17,590 --> 00:11:20,615
just the previous state and action in a distribution over that,

238
00:11:20,615 --> 00:11:24,095
and we'll similarly have that for the- for the reward.

239
00:11:24,095 --> 00:11:28,150
And we, we typically assume things are conditionally independent,

240
00:11:28,150 --> 00:11:29,170
like what we've done before.

241
00:11:29,170 --> 00:11:32,440
So we just have a particular dynamics model that's conditioned on

242
00:11:32,440 --> 00:11:37,220
the state and action and a reward that is conditioned on the previous state and action.

243
00:11:38,670 --> 00:11:41,380
And so if we wanted to do model learning,

244
00:11:41,380 --> 00:11:43,270
then we have the supervised learning problem that we've

245
00:11:43,270 --> 00:11:45,190
talked about a little bit before of,

246
00:11:45,190 --> 00:11:46,690
uh, you have the state and action,

247
00:11:46,690 --> 00:11:49,000
and you want to predict your reward in next state.

248
00:11:49,000 --> 00:11:53,290
And so we have this regression problem and this density estimation problem,

249
00:11:53,290 --> 00:11:55,840
then you can do it in all sorts of ways.

250
00:11:55,840 --> 00:11:57,760
You can, uh, you know,

251
00:11:57,760 --> 00:12:00,910
use mean squared error, you can use different forms of losses.

252
00:12:00,910 --> 00:12:02,740
Um, and in fact,

253
00:12:02,740 --> 00:12:04,660
one of the ways we've recently made progress on

254
00:12:04,660 --> 00:12:06,970
our off-policy reinforcement learning is by using

255
00:12:06,970 --> 00:12:12,280
different forms of losses than standard sort of maximum likelihood losses.

256
00:12:12,280 --> 00:12:13,690
Uh, but generally here,

257
00:12:13,690 --> 00:12:15,820
we're gonna talk about maximum likelihood losses.

258
00:12:15,820 --> 00:12:17,650
So we can just do this, and of course,

259
00:12:17,650 --> 00:12:20,440
in the- in the tabular case this is just [NOISE] counting.

260
00:12:20,440 --> 00:12:23,740
So if you just have a discrete set of states and actions,

261
00:12:23,740 --> 00:12:27,205
you can just count how many times did I start in this state and action,

262
00:12:27,205 --> 00:12:28,330
and go to state one,

263
00:12:28,330 --> 00:12:31,660
versus how many times they start in this state and action and go to state two.

264
00:12:31,660 --> 00:12:34,820
And so you just count those up and then normalize.

265
00:12:36,420 --> 00:12:40,765
And in general, there's a huge number of different ways that you can represent these.

266
00:12:40,765 --> 00:12:45,085
Uh, and I think one of the ones that I think is particularly interesting is Bayesian,

267
00:12:45,085 --> 00:12:47,035
not the- Bayesian Deep Neural Networks.

268
00:12:47,035 --> 00:12:49,225
They've been pretty hard to tune so far.

269
00:12:49,225 --> 00:12:50,575
Oh, another policy, you know,

270
00:12:50,575 --> 00:12:55,030
Bayesian deep neural networks.

271
00:12:55,030 --> 00:12:57,310
[NOISE] Um, I think one of the reasons those could be really

272
00:12:57,310 --> 00:13:00,070
powerful is they can explicitly represent uncertainty,

273
00:13:00,070 --> 00:13:02,125
but so far they've been pretty hard to train.

274
00:13:02,125 --> 00:13:03,490
But I think that there's, you know,

275
00:13:03,490 --> 00:13:06,190
a lot of really- there's some really simple models we can use as

276
00:13:06,190 --> 00:13:10,160
well as some really rich function approximators for these models.

277
00:13:10,470 --> 00:13:13,780
Okay. So if we're in the table lookup case,

278
00:13:13,780 --> 00:13:15,625
we're just averaging counts.

279
00:13:15,625 --> 00:13:18,370
So we're just counting as I said this state action,

280
00:13:18,370 --> 00:13:21,760
next state tuples dividing by the number of times we've taken,

281
00:13:21,760 --> 00:13:24,130
uh, that action in that state,

282
00:13:24,130 --> 00:13:27,130
and we similarly just average all the rewards,

283
00:13:27,130 --> 00:13:29,995
so this should be the reward scene,

284
00:13:29,995 --> 00:13:33,790
for the times that we were in that state, took that action,

285
00:13:33,790 --> 00:13:36,770
and what was the reward we got on that time step.

286
00:13:39,780 --> 00:13:43,300
So let's think about an example for what that looks like here.

287
00:13:43,300 --> 00:13:46,540
So a long time ago, we introduced this AB example,

288
00:13:46,540 --> 00:13:50,590
where we have, um, a state that goes to the- a state A

289
00:13:50,590 --> 00:13:53,200
that goes to action in state B, and then after that,

290
00:13:53,200 --> 00:13:55,960
it either goes to a terminal state where it got a reward of

291
00:13:55,960 --> 00:13:58,990
1 with 75% probability,

292
00:13:58,990 --> 00:14:01,180
or it goes to a terminal state where it gets

293
00:14:01,180 --> 00:14:04,060
a reward of 0 with 25% probability.

294
00:14:04,060 --> 00:14:07,165
And imagine that we've experienced something in this world,

295
00:14:07,165 --> 00:14:10,315
so there's no actions here, there's a single action.

296
00:14:10,315 --> 00:14:15,715
It's really a Markov reward process rather than a decision process,

297
00:14:15,715 --> 00:14:17,545
but we can get our observations.

298
00:14:17,545 --> 00:14:19,720
So let's say, we start in state A,

299
00:14:19,720 --> 00:14:21,100
and then we got a reward of 0,

300
00:14:21,100 --> 00:14:22,555
and went to B and got a 0.

301
00:14:22,555 --> 00:14:24,430
And then we had a whole set of times,

302
00:14:24,430 --> 00:14:27,385
we have 6 times, where we started at B,

303
00:14:27,385 --> 00:14:28,750
and we've got a reward of 1,

304
00:14:28,750 --> 00:14:32,770
and then we got started in state B and got a reward of 0.

305
00:14:32,770 --> 00:14:37,270
And now we can construct a table lookup model from this.

306
00:14:37,270 --> 00:14:39,790
And just to refresh our memories, um,

307
00:14:39,790 --> 00:14:42,880
so we talked about the fact that if you do temporal difference

308
00:14:42,880 --> 00:14:46,165
learning in this problem with a tabular representation,

309
00:14:46,165 --> 00:14:48,130
meaning one row for each state,

310
00:14:48,130 --> 00:14:50,410
so that's just two states; A and B.

311
00:14:50,410 --> 00:14:55,780
That if you do infinite replay on this set of experience that it's equivalent if

312
00:14:55,780 --> 00:15:02,770
you took this data and estimated a Markov decision process model with it,

313
00:15:02,770 --> 00:15:07,585
and then did planning with that to evaluate the optimal policy,

314
00:15:07,585 --> 00:15:11,680
or the policy that you're using to gather the data in this case.

315
00:15:11,680 --> 00:15:15,235
So that was an interesting equivalence that the TD is, um,

316
00:15:15,235 --> 00:15:17,890
giving you exactly the same solution as what if you

317
00:15:17,890 --> 00:15:20,965
compute what's often called a certainty equivalence model,

318
00:15:20,965 --> 00:15:22,525
because you take your data,

319
00:15:22,525 --> 00:15:26,650
you estimate, you take the empirical as average of that data.

320
00:15:26,650 --> 00:15:28,870
So you can say, "If this was all the data in the world,

321
00:15:28,870 --> 00:15:31,810
what would be the model that could be associated with that,

322
00:15:31,810 --> 00:15:35,620
with a maximum likelihood estimate." And then we do planning.

323
00:15:35,620 --> 00:15:37,540
So TD makes that assumption.

324
00:15:37,540 --> 00:15:39,325
Let's just do a quick check of memory.

325
00:15:39,325 --> 00:15:43,970
Do Monte-Carlo methods converge to the same solution on this data?

326
00:15:47,160 --> 00:15:50,590
So maybe take a minute, turn to somebody next to you,

327
00:15:50,590 --> 00:15:58,360
and decide whether or not they do, and why or why not.

328
00:15:58,360 --> 00:16:13,840
Do you have a question?

329
00:16:13,840 --> 00:16:15,880
Uh, as an offering, yes.

330
00:16:15,880 --> 00:16:17,110
[LAUGHTER].

331
00:16:17,110 --> 00:16:17,725
Okay.

332
00:16:17,725 --> 00:16:22,315
I think that Monte-Carlo methods will converge to solution with

333
00:16:22,315 --> 00:16:26,935
the minimum MSE's opposed to have MLE effect?

334
00:16:26,935 --> 00:16:31,315
Correct. The Monte-Carlo methods do not make an assumption of Markovian.

335
00:16:31,315 --> 00:16:37,030
Um, so the- they are suitable in cases where the domain is not Markovian.

336
00:16:37,030 --> 00:16:41,890
So in this case, they will converge to the- well in all cases for this policy evaluation.

337
00:16:41,890 --> 00:16:43,660
They're gonna converge to the minimum mean squared error.

338
00:16:43,660 --> 00:16:44,980
Yeah, question?

339
00:16:44,980 --> 00:16:49,315
So you're saying that if you used an ML model you probably converge them into MSE,

340
00:16:49,315 --> 00:16:51,070
what if you are using a different loss? [OVERLAPPING]

341
00:16:51,070 --> 00:16:53,200
Good question. This is- this is- this is going to

342
00:16:53,200 --> 00:16:55,540
converge to the minimum MSE not the MLE.

343
00:16:55,540 --> 00:17:04,030
[inaudible] If you are using a different loss [inaudible].

344
00:17:04,030 --> 00:17:05,920
Would the Monte Carlo methods converge to the-

345
00:17:05,920 --> 00:17:07,990
I mean, depending on the loss or if you regularize.

346
00:17:07,990 --> 00:17:11,050
It's a great question, if you regularize it may converge to

347
00:17:11,050 --> 00:17:12,970
different solutions than the minimum mean squared error

348
00:17:12,970 --> 00:17:15,099
depending on how you regularize or the loss you use.

349
00:17:15,099 --> 00:17:17,979
But in general, it will not converge to the same thing as if

350
00:17:17,980 --> 00:17:20,935
you got the maximum likelihood estimate model,

351
00:17:20,935 --> 00:17:24,025
and then did planning with that or policy evaluation.

352
00:17:24,025 --> 00:17:28,089
And the key difference is Monte Carlo is not making a Markovian assumption.

353
00:17:28,089 --> 00:17:34,180
So it does- it does not assume Markov.

354
00:17:34,180 --> 00:17:38,245
And so in particular in this case, um,

355
00:17:38,245 --> 00:17:43,760
because I may have a guess of what the value of A will be under the Monte Carlo estimate.

356
00:17:47,460 --> 00:17:50,410
There's only one sample of it.

357
00:17:50,410 --> 00:17:51,505
0.

358
00:17:51,505 --> 00:17:54,295
Yeah, um, yes. So there's only- for Monte Carlo here,

359
00:17:54,295 --> 00:17:58,930
we'll say I- I'm only looking at full returns that started with this particular state,

360
00:17:58,930 --> 00:18:00,490
and there's no bootstrapping.

361
00:18:00,490 --> 00:18:05,620
So, um, the only time we saw A was when the return from A was 0.

362
00:18:05,620 --> 00:18:07,450
Uh, but, you know,

363
00:18:07,450 --> 00:18:08,560
if the system is really Markov,

364
00:18:08,560 --> 00:18:10,930
that's not a very good solution because we have all this other evidence

365
00:18:10,930 --> 00:18:13,330
that B is actually normally has a higher value,

366
00:18:13,330 --> 00:18:15,280
and we're not able to take advantage of that,

367
00:18:15,280 --> 00:18:17,335
um, whereas TD does.

368
00:18:17,335 --> 00:18:19,030
So TD can say, "Well,

369
00:18:19,030 --> 00:18:20,830
I know that V of A was 0 this one time."

370
00:18:20,830 --> 00:18:27,070
But in general, we think that V of A is equal to the immediate reward plus,

371
00:18:27,070 --> 00:18:30,040
in this case there's no discounting, so value of B.

372
00:18:30,040 --> 00:18:34,540
And I have all this other- other evidence that the value of B is, in this case,

373
00:18:34,540 --> 00:18:37,675
actually exactly equal to 0.75, um,

374
00:18:37,675 --> 00:18:40,120
because we have six examples of it being 1,

375
00:18:40,120 --> 00:18:41,770
and two examples of it being 0.

376
00:18:41,770 --> 00:18:43,210
So we would, uh,

377
00:18:43,210 --> 00:18:46,600
have V of B is equal to 0.75,

378
00:18:46,600 --> 00:18:48,940
and both Monte Carlo and,

379
00:18:48,940 --> 00:18:51,100
um, TD would agree on that.

380
00:18:51,100 --> 00:18:55,285
Because for- if you look at every time you started on B,

381
00:18:55,285 --> 00:18:57,220
75% or how- you know,

382
00:18:57,220 --> 00:18:58,855
75% of time you got a 1,

383
00:18:58,855 --> 00:19:00,010
the rest of the time you got a 0.

384
00:19:00,010 --> 00:19:03,130
So the- the value of that is 0.75.

385
00:19:03,130 --> 00:19:13,490
So the TD estimate would say also V of A is equal to 0.75, the TD estimate.

386
00:19:13,590 --> 00:19:16,300
So one of the reasons this comes up here,

387
00:19:16,300 --> 00:19:21,115
a- and notice this is a- this is not due to a sort of finite number of backup,

388
00:19:21,115 --> 00:19:24,220
or sorry, I'll be careful, a finite amount of use of the data.

389
00:19:24,220 --> 00:19:28,030
So this is saying, if you sort of run this through TD many,

390
00:19:28,030 --> 00:19:32,050
many times, and the Monte Carlo estimate is also getting access to all of the data.

391
00:19:32,050 --> 00:19:34,375
It's just saying this is all the data there is.

392
00:19:34,375 --> 00:19:36,190
So an alternative would be,

393
00:19:36,190 --> 00:19:38,695
if you take this data and you build a model.

394
00:19:38,695 --> 00:19:40,930
So now we have a model that says,

395
00:19:40,930 --> 00:19:45,145
the probability of going to B given you started in A is 1,

396
00:19:45,145 --> 00:19:48,190
you always go from- from, um, A to B.

397
00:19:48,190 --> 00:19:49,810
In fact, you've only ever seen this once,

398
00:19:49,810 --> 00:19:51,115
but the one time you saw it,

399
00:19:51,115 --> 00:19:53,065
you went to B, uh,

400
00:19:53,065 --> 00:19:56,830
and we can use this to try to get simulated data.

401
00:19:56,830 --> 00:20:00,820
So let me just- well, I'll go a couple more.

402
00:20:00,820 --> 00:20:04,179
So the idea in this case is that once you have your simulator,

403
00:20:04,179 --> 00:20:06,610
you can use it to simulate samples,

404
00:20:06,610 --> 00:20:09,625
and then you can plan using that simulated data.

405
00:20:09,625 --> 00:20:12,565
Now, initially, that might sound like why would you do that

406
00:20:12,565 --> 00:20:15,970
because you hide your previous data and maybe you could have directly, you know,

407
00:20:15,970 --> 00:20:18,850
put it through a model-free based approach,

408
00:20:18,850 --> 00:20:21,025
like why would you first build a model,

409
00:20:21,025 --> 00:20:23,500
and then generate data from it.

410
00:20:23,500 --> 00:20:25,915
But we'll see an example right now from

411
00:20:25,915 --> 00:20:28,900
that sort of AB example of why that might be beneficial.

412
00:20:28,900 --> 00:20:32,770
So you- what we can do is we can get this- we can get

413
00:20:32,770 --> 00:20:36,880
the maximum likelihood estimate of the model or other estimates you might want to use,

414
00:20:36,880 --> 00:20:38,305
and then you can sample from it.

415
00:20:38,305 --> 00:20:45,435
So in that example we just had here, our estimated transition model,

416
00:20:45,435 --> 00:20:47,610
is that whenever we're in A we go to B.

417
00:20:47,610 --> 00:20:49,875
So when I'm in A,

418
00:20:49,875 --> 00:20:52,740
I can sample and I will go to B,

419
00:20:52,740 --> 00:20:55,500
and that generates me a fake data point.

420
00:20:55,500 --> 00:20:58,720
Okay? And I could do this a whole bunch of times,

421
00:20:58,720 --> 00:21:00,145
we get lots of fake data.

422
00:21:00,145 --> 00:21:02,980
Now, this fake data may or may not look like what the real-world does,

423
00:21:02,980 --> 00:21:04,585
it depends how good my model is.

424
00:21:04,585 --> 00:21:06,775
But it's certainly data I can train with,

425
00:21:06,775 --> 00:21:08,590
and- and we'll see, in a minute,

426
00:21:08,590 --> 00:21:10,105
in a second why that's beneficial.

427
00:21:10,105 --> 00:21:12,145
Okay? So if we go back to here,

428
00:21:12,145 --> 00:21:15,760
this is the real experience on the left.

429
00:21:15,760 --> 00:21:18,745
So on the left-hand side we had all this real experience,

430
00:21:18,745 --> 00:21:23,455
and then what we did is we built a model from that,

431
00:21:23,455 --> 00:21:24,805
and then we could sample from it.

432
00:21:24,805 --> 00:21:26,680
So we could have experience that looks very

433
00:21:26,680 --> 00:21:28,885
similar to the data that we've actually seen,

434
00:21:28,885 --> 00:21:32,450
but we could also have experience like this.

435
00:21:35,190 --> 00:21:38,950
Now, why could we have that, because we now have a simulator, and, uh,

436
00:21:38,950 --> 00:21:41,230
in our simulated- In our model,

437
00:21:41,230 --> 00:21:45,955
we've seen cases where we started in A and we went to B.

438
00:21:45,955 --> 00:21:48,550
And in our model there have

439
00:21:48,550 --> 00:21:51,055
been other times where we've started in B, and we've got a 1.

440
00:21:51,055 --> 00:21:53,905
So essentially we can kind of chain that experience together,

441
00:21:53,905 --> 00:21:56,830
and simulate something that we never observed in the real world.

442
00:21:56,830 --> 00:22:00,140
And we're leveraging the fact here that it's Markov.

443
00:22:00,720 --> 00:22:03,565
So if the domain isn't really Markov,

444
00:22:03,565 --> 00:22:05,440
we could end up getting data that looks very

445
00:22:05,440 --> 00:22:08,005
very different than what you could ever get in the real world.

446
00:22:08,005 --> 00:22:10,825
But if it's Markov, then, um,

447
00:22:10,825 --> 00:22:13,390
it may still be an approximate model because

448
00:22:13,390 --> 00:22:15,715
we only had a limited amount of data training our model.

449
00:22:15,715 --> 00:22:19,945
But now we can start to see conjunctions of states and actions,

450
00:22:19,945 --> 00:22:21,895
uh, that we maybe never saw in our data.

451
00:22:21,895 --> 00:22:25,120
But we could update [NOISE] our model as we sample?

452
00:22:25,120 --> 00:22:27,370
Uh, well, okay great question.

453
00:22:27,370 --> 00:22:28,825
Could you update your model as you sample?

454
00:22:28,825 --> 00:22:32,140
You could, but right now we're just sampling from our model.

455
00:22:32,140 --> 00:22:34,960
So this, this is not real-world experience.

456
00:22:34,960 --> 00:22:37,120
So that could lead to confirmation bias,

457
00:22:37,120 --> 00:22:38,920
because it's like your model is giving you data,

458
00:22:38,920 --> 00:22:40,600
and if you treat that as real data,

459
00:22:40,600 --> 00:22:42,490
and put that like into your model.

460
00:22:42,490 --> 00:22:44,005
It's not from the real world.

461
00:22:44,005 --> 00:22:47,305
So you could end up sort of being overly confident, in,

462
00:22:47,305 --> 00:22:51,355
um, uh, because you're generating fake data and then treating it as if it's real.

463
00:22:51,355 --> 00:22:54,615
How do we judge how confident we would be in our sample's experience?

464
00:22:54,615 --> 00:22:57,750
I guess like relative to how much training data we'd have to put in the model.

465
00:22:57,750 --> 00:22:59,865
Exactly. So that's like, how would be,

466
00:22:59,865 --> 00:23:01,545
how do we know how confident to be and,

467
00:23:01,545 --> 00:23:04,500
And in general this is the issue of your models are gonna be pretty bad.

468
00:23:04,500 --> 00:23:06,090
Sometimes if you have limited amounts of data.

469
00:23:06,090 --> 00:23:08,955
So some of the techniques we talked about for exploration, where we could, uh,

470
00:23:08,955 --> 00:23:11,670
drive these confidence intervals over how good the models are,

471
00:23:11,670 --> 00:23:13,040
they apply here as well.

472
00:23:13,040 --> 00:23:17,005
So, um, if you only have a little bit of data you can use things like Hoeffding to say,

473
00:23:17,005 --> 00:23:20,365
how sure am I about this reward model for example.

474
00:23:20,365 --> 00:23:21,970
Um, for most of today,

475
00:23:21,970 --> 00:23:23,260
we're not gonna talk about that that much,

476
00:23:23,260 --> 00:23:25,990
but you can use that information to try to quantify

477
00:23:25,990 --> 00:23:29,230
how uncertain should you be, and how would that error kind of propagate.

478
00:23:29,230 --> 00:23:31,540
Yeah.

479
00:23:31,540 --> 00:23:37,150
Um, so I guess I'm trying to like conceptually think about the next step is that we're,

480
00:23:37,150 --> 00:23:38,320
we're building this model.

481
00:23:38,320 --> 00:23:43,405
We're gonna use a method to learn some sort of a policy or some sort of like,

482
00:23:43,405 --> 00:23:45,490
way to act in the real world.

483
00:23:45,490 --> 00:23:47,125
If we have the model,

484
00:23:47,125 --> 00:23:49,600
can we just used a model when you are acting and

485
00:23:49,600 --> 00:23:52,645
just basically run our state through the model and get,

486
00:23:52,645 --> 00:23:56,634
maybe like a distribution and just take the maximum action,

487
00:23:56,634 --> 00:23:59,695
the m- The action that maximizes our reward?

488
00:23:59,695 --> 00:24:02,820
So, I guess, once you have the model

489
00:24:02,820 --> 00:24:05,310
you could use it in lots of different ways to do planning.

490
00:24:05,310 --> 00:24:06,585
So one is you could do,

491
00:24:06,585 --> 00:24:09,090
if it's a small case, like here it's a table.

492
00:24:09,090 --> 00:24:13,155
So you could use value iteration and solve this exactly.

493
00:24:13,155 --> 00:24:15,255
There's no reason to simulate data.

494
00:24:15,255 --> 00:24:17,830
Um, but when you start to think about doing like

495
00:24:17,830 --> 00:24:20,245
Atari or other really high-dimensional problems,

496
00:24:20,245 --> 00:24:22,600
the planning problem alone is super expensive.

497
00:24:22,600 --> 00:24:24,070
And so you might still want to do

498
00:24:24,070 --> 00:24:27,730
model-free methods for your planning with your simulated data.

499
00:24:27,730 --> 00:24:31,810
And one of the reasons you might want to do that is because, um, we've,

500
00:24:31,810 --> 00:24:34,090
we've talked about different ways in Q learning to be more sample

501
00:24:34,090 --> 00:24:37,285
efficient like you have a replay buffer and you can do episodic replay.

502
00:24:37,285 --> 00:24:41,440
But another alternative is you just generate a lot of data from your simulated model,

503
00:24:41,440 --> 00:24:43,675
and then you replay over that a ton of times.

504
00:24:43,675 --> 00:24:46,135
And so that's another way to kind of, um,

505
00:24:46,135 --> 00:24:49,345
make more use out of your data. Yeah, question in the back.

506
00:24:49,345 --> 00:24:55,090
If, um, if you don't want to make the Markov assumption,

507
00:24:55,090 --> 00:24:57,400
can you so do the same but, uh,

508
00:24:57,400 --> 00:25:01,480
condition on the past of [inaudible]?

509
00:25:01,480 --> 00:25:04,165
Yeah. Question, what if you don't wanna make the Markov assumption?

510
00:25:04,165 --> 00:25:06,850
Yes, and can you condition on the past, you absolutely can.

511
00:25:06,850 --> 00:25:09,895
Um, that means you would build models that are a full function of the history.

512
00:25:09,895 --> 00:25:12,295
The problem with that is, you don't have very much data.

513
00:25:12,295 --> 00:25:16,855
So you have to, if you want to condition on the entire history as essentially your state,

514
00:25:16,855 --> 00:25:19,150
you're always fine in terms of the Markov assumption,

515
00:25:19,150 --> 00:25:22,555
but you'll just have really really little data to estimate the transition models.

516
00:25:22,555 --> 00:25:24,400
Particularly as the horizon goes on.

517
00:25:24,400 --> 00:25:27,130
So it's often a trade off, like do you want to have better models?

518
00:25:27,130 --> 00:25:29,005
Well, it depends on your domain. Maybe it's really Markov.

519
00:25:29,005 --> 00:25:30,220
If it's not really Markov,

520
00:25:30,220 --> 00:25:32,890
do you want better models with very little data?

521
00:25:32,890 --> 00:25:35,500
So, um, in general this sort of gets to

522
00:25:35,500 --> 00:25:38,995
the function approximation error versus the sample estimation error.

523
00:25:38,995 --> 00:25:41,260
You can have error due to the fact that you don't have much data.

524
00:25:41,260 --> 00:25:44,485
Or you can have error due to the fact that your function approximation is wrong.

525
00:25:44,485 --> 00:25:47,365
And often you're going to want to trade off between those two.

526
00:25:47,365 --> 00:25:48,940
So, so this example, you know,

527
00:25:48,940 --> 00:25:51,175
you can end up getting this sampled experience

528
00:25:51,175 --> 00:25:53,960
that looks different than anything you've seen in reality.

529
00:25:53,960 --> 00:26:00,420
Um, and then in this case if you now run Monte Carlo on the new data,

530
00:26:00,420 --> 00:26:02,670
you may, you can get something that looks very

531
00:26:02,670 --> 00:26:05,680
similar if you've done TD on the original data.

532
00:26:06,030 --> 00:26:10,510
So basically, instead of taking our real experience and then

533
00:26:10,510 --> 00:26:14,890
doing Monte Carlo using that for policy evaluation.

534
00:26:14,890 --> 00:26:16,660
An alternative is to say,

535
00:26:16,660 --> 00:26:19,585
we really think that this is a Markov system, let's simulate a bunch of

536
00:26:19,585 --> 00:26:23,725
data and then you could run Monte Carlo learning on this, or TD learning on it.

537
00:26:23,725 --> 00:26:26,515
Um, and you would get probably the same answer.

538
00:26:26,515 --> 00:26:30,730
So this is, you know, in contrast to what Monte Carlo would've converged to

539
00:26:30,730 --> 00:26:35,200
before which was V(A) = 0 and V(B) = 0.75.

540
00:26:35,200 --> 00:26:38,410
Now again you might say, okay but do I really want to do this, like if,

541
00:26:38,410 --> 00:26:40,870
I, if I really didn't think the system was Markov,

542
00:26:40,870 --> 00:26:45,130
I wouldn't have run Monte Carlo on my fake data either.

543
00:26:45,130 --> 00:26:48,430
But I think this illustrates, um,

544
00:26:48,430 --> 00:26:50,920
what you can do once you have this sampling and just

545
00:26:50,920 --> 00:26:53,500
shows that allows you to make all sorts of choices.

546
00:26:53,500 --> 00:26:55,570
So maybe there you wanna have

547
00:26:55,570 --> 00:26:58,180
sort of a two-step Markov process or you want to do different,

548
00:26:58,180 --> 00:26:59,965
make different assumptions in your model.

549
00:26:59,965 --> 00:27:02,500
And then after that you wanna do a lot of planning with it.

550
00:27:02,500 --> 00:27:05,755
So that just allows you to first take your data, compute a model,

551
00:27:05,755 --> 00:27:09,940
and then you can decide how you're going to use that to try to do planning.

552
00:27:09,940 --> 00:27:12,595
And we'll see a particular way to do that shortly.

553
00:27:12,595 --> 00:27:15,835
Now as you guys were just asking me about, um,

554
00:27:15,835 --> 00:27:21,895
if we have a bad model then we're gonna compute a sub-optimal policy in general.

555
00:27:21,895 --> 00:27:23,620
You know, we might be incredibly lucky.

556
00:27:23,620 --> 00:27:26,020
Um, because ultimately for your decisions,

557
00:27:26,020 --> 00:27:28,000
we only need to decide whether,

558
00:27:28,000 --> 00:27:33,325
you know, V(s,a1) is greater than V(s,a2).

559
00:27:33,325 --> 00:27:37,390
So ultimately we're gonna be making comparison, pairwise comparison decisions.

560
00:27:37,390 --> 00:27:41,170
So you might be, have a really bad model and still end up with a good policy.

561
00:27:41,170 --> 00:27:42,865
Um, but in general,

562
00:27:42,865 --> 00:27:44,830
if your model is bad and you do planning in

563
00:27:44,830 --> 00:27:47,635
general, your policy is also gonna be sub-optimal.

564
00:27:47,635 --> 00:27:52,765
Um, and so one approach if the model is wrong,

565
00:27:52,765 --> 00:27:55,960
um, is to do model-free reinforcement

566
00:27:55,960 --> 00:27:57,490
learning on the original data.

567
00:27:57,490 --> 00:28:00,775
So not to do model-based planning.

568
00:28:00,775 --> 00:28:04,060
It's not clear that always solves the issue.

569
00:28:04,060 --> 00:28:06,190
So it depends why your model is wrong.

570
00:28:06,190 --> 00:28:08,830
Um, if your model is wrong because you've picked

571
00:28:08,830 --> 00:28:12,565
the wrong parametric class or because the system is not Markov,

572
00:28:12,565 --> 00:28:14,980
you're doing Q learning that's not gonna solve your problem

573
00:28:14,980 --> 00:28:17,590
because Q learning also assumes the world is Markov.

574
00:28:17,590 --> 00:28:19,240
So model-free, you know,

575
00:28:19,240 --> 00:28:21,925
it depends on why, you know, why is it wrong?

576
00:28:21,925 --> 00:28:25,970
It depends on why? Whether or not this helps.

577
00:28:28,260 --> 00:28:32,695
Um, another is to reason explicitly about your model uncertainty.

578
00:28:32,695 --> 00:28:35,965
And this is goes back to the exploration, exploitation.

579
00:28:35,965 --> 00:28:39,295
Now again this only deals with a particular form of wrongness.

580
00:28:39,295 --> 00:28:43,045
Um, it deals with the fact that you might have sampling estimation error.

581
00:28:43,045 --> 00:28:47,095
But it's still making the basic assumption that your model class is correct.

582
00:28:47,095 --> 00:28:51,040
So for example, if you're modeling the world as, um,

583
00:28:51,040 --> 00:28:53,050
a multinomial distribution, and you don't have

584
00:28:53,050 --> 00:28:55,945
very much data then your prior metric estimates will be off.

585
00:28:55,945 --> 00:28:58,270
But if the world really isn't a multinomial,

586
00:28:58,270 --> 00:29:01,390
um, then all bets are off.

587
00:29:01,390 --> 00:29:04,780
So it's always good to know sort of what assumptions we're making in

588
00:29:04,780 --> 00:29:08,990
our algorithms and then what sort of forms of uncertainty we're accounting for.

589
00:29:09,540 --> 00:29:12,400
Now I guess I'll just say, say one other thing which is a

590
00:29:12,400 --> 00:29:14,980
little bit subtle which I find super interesting which is,

591
00:29:14,980 --> 00:29:17,110
um, if you have a really good model,

592
00:29:17,110 --> 00:29:21,910
it is, a generally said it or if you have a perfect model,

593
00:29:21,910 --> 00:29:25,570
and perfect estimate of the parameters, that is sufficient to make good decisions.

594
00:29:25,570 --> 00:29:28,090
If you were trying to train your model and you have

595
00:29:28,090 --> 00:29:32,305
a restricted amount of data or restricted sort of expressivity of your model,

596
00:29:32,305 --> 00:29:36,010
um, then a model that has higher predictive accuracy can

597
00:29:36,010 --> 00:29:39,820
in fact be worse when you use it to make decisions.

598
00:29:39,820 --> 00:29:42,250
And the intuition I like to think of is this, we,

599
00:29:42,250 --> 00:29:43,885
we discovered this a few years ago, um,

600
00:29:43,885 --> 00:29:45,040
others have discovered it too,

601
00:29:45,040 --> 00:29:47,290
we were thinking about it for an intelligent tutoring system.

602
00:29:47,290 --> 00:29:52,285
Um, the challenges, let's imagine that you have like a really complicated state-space.

603
00:29:52,285 --> 00:29:56,455
Say, um, you're trying to model what a kitchen looks like when someone's making tea,

604
00:29:56,455 --> 00:29:57,760
and there's all sorts of features,

605
00:29:57,760 --> 00:29:59,170
you know, like there is steam,

606
00:29:59,170 --> 00:30:02,920
maybe it's a sunset outside or when there's also the temperature of the water.

607
00:30:02,920 --> 00:30:06,865
And in order to make tea you need the water to be over 100 degrees.

608
00:30:06,865 --> 00:30:08,680
And in fact that's the only feature you need to pay

609
00:30:08,680 --> 00:30:11,155
attention to in order to successfully make tea.

610
00:30:11,155 --> 00:30:13,480
But if you were trying to just do, um,

611
00:30:13,480 --> 00:30:15,190
build a model of the world,

612
00:30:15,190 --> 00:30:16,450
you're trying to model the sunset,

613
00:30:16,450 --> 00:30:18,250
you're trying to model the steam etc.

614
00:30:18,250 --> 00:30:21,880
And there can be a huge number of features that you're trying to capture in your sort of,

615
00:30:21,880 --> 00:30:24,625
you know, maybe slightly improvisional network.

616
00:30:24,625 --> 00:30:29,005
And so if you just try to fit the best maximum-likelihood estimate model,

617
00:30:29,005 --> 00:30:33,635
it might not be the one that captures the features that you need to make decisions.

618
00:30:33,635 --> 00:30:36,450
And so models that look better in terms of

619
00:30:36,450 --> 00:30:40,095
predictive accuracy may not always be better in terms of decision making.

620
00:30:40,095 --> 00:30:42,975
So that was this issue we encountered a few years ago,

621
00:30:42,975 --> 00:30:46,290
and I think it just illustrates why the models that we need to build when we want to make

622
00:30:46,290 --> 00:30:50,660
decisions are not necessarily the same models we need for predictive accuracy.

623
00:30:50,660 --> 00:30:53,170
So it's important where possible to know which of

624
00:30:53,170 --> 00:30:57,020
your features do you care about in terms of utility and value.

625
00:30:57,810 --> 00:31:01,420
Okay. So let's talk a little bit about simulation-based search.

626
00:31:01,420 --> 00:31:04,810
Um, who here has seen forward search before in one of their classes?

627
00:31:04,810 --> 00:31:06,865
Okay, a number people, but not everybody.

628
00:31:06,865 --> 00:31:09,400
Um, so forward search algorithms.

629
00:31:09,400 --> 00:31:11,800
What we're going to talk about now is different ways instead of

630
00:31:11,800 --> 00:31:14,320
doing Q learning on our simulated data.

631
00:31:14,320 --> 00:31:17,785
Okay, we've got a model, what's another way we can use it to try to make decisions?

632
00:31:17,785 --> 00:31:19,570
One way is to do forward search.

633
00:31:19,570 --> 00:31:20,965
So how does forward search work?

634
00:31:20,965 --> 00:31:25,990
The idea in forward search is that we're gonna think about all the actions we could take.

635
00:31:25,990 --> 00:31:29,545
So let's say here we just have two actions, A1 and A2.

636
00:31:29,545 --> 00:31:32,860
And then we're going to think about all the possible next states we might get to.

637
00:31:32,860 --> 00:31:34,975
So let's say it's a fairly small world,

638
00:31:34,975 --> 00:31:37,670
so we just have S1 and S2.

639
00:31:38,130 --> 00:31:42,150
So in this current state,

640
00:31:42,150 --> 00:31:45,090
I could either take action one or action two and after I

641
00:31:45,090 --> 00:31:48,390
take those actions I could either transition to state one or state two.

642
00:31:48,390 --> 00:31:50,940
And then after I get to whatever I state- I get state,

643
00:31:50,940 --> 00:31:54,530
I get to, I again can make a decision A1 or A2.

644
00:31:54,530 --> 00:31:57,580
Because that's my action space here.

645
00:31:57,580 --> 00:31:59,995
And then after I take that action,

646
00:31:59,995 --> 00:32:03,010
I again my transition or maybe sometimes I terminate it.

647
00:32:03,010 --> 00:32:05,240
So this is a terminal state.

648
00:32:06,300 --> 00:32:10,435
Maybe my robot falls off or falls down or things like that,

649
00:32:10,435 --> 00:32:12,790
or maybe else I go to another state.

650
00:32:12,790 --> 00:32:17,979
[NOISE] And I can think of sort of like these branching set of futures,

651
00:32:17,979 --> 00:32:20,380
and I can roll them out as much as I want to.

652
00:32:20,380 --> 00:32:25,960
Let's say I want to think about the next h steps for example. And then I halt.

653
00:32:25,960 --> 00:32:27,340
And once I have that,

654
00:32:27,340 --> 00:32:29,440
I can use that information and the-

655
00:32:29,440 --> 00:32:32,785
my sort of reward- well let me just say one more thing which is,

656
00:32:32,785 --> 00:32:36,760
um, as I do these sort of like simulated features,

657
00:32:36,760 --> 00:32:40,990
I can think about what the reward would I get under these different features.

658
00:32:40,990 --> 00:32:42,925
Because right now I'm assuming I have a model.

659
00:32:42,925 --> 00:32:45,740
So this is given a model.

660
00:32:45,870 --> 00:32:49,450
So I can think about if I took this state as t and

661
00:32:49,450 --> 00:32:53,200
took action a2 what reward would I get?

662
00:32:53,200 --> 00:32:58,360
And then down here, I can think about if I got what reward I would get in s2, a2.

663
00:32:58,360 --> 00:33:03,340
So I can think of sort of generating different features and then summing up the rewards,

664
00:33:03,340 --> 00:33:07,240
um, it will give me a series of reward along any of those different paths.

665
00:33:07,240 --> 00:33:10,090
And then in order to figure out the value of

666
00:33:10,090 --> 00:33:13,210
these different sort of actions or the best action I should take,

667
00:33:13,210 --> 00:33:22,120
what I can do is I can take a max over actions and I can take an expectation over states.

668
00:33:22,120 --> 00:33:25,855
And I always know how to take that expectation because I'm assuming I have a model.

669
00:33:25,855 --> 00:33:30,280
So I can always think about what would be the probability of me

670
00:33:30,280 --> 00:33:35,900
getting to that particular state given my parent action and the state I'm coming from.

671
00:33:37,290 --> 00:33:39,460
So what happens, in this case,

672
00:33:39,460 --> 00:33:43,780
is as I roll out all these potential futures up until a certain depth.

673
00:33:43,780 --> 00:33:45,520
In the case of Go or something like that,

674
00:33:45,520 --> 00:33:47,800
it would be until you win the game or lose the game.

675
00:33:47,800 --> 00:33:49,450
And then I want to back up.

676
00:33:49,450 --> 00:33:51,520
So you can think of this [BACKGROUND] as sort of doing

677
00:33:51,520 --> 00:33:54,280
a not very efficient form of dynamic programming [NOISE].

678
00:33:54,280 --> 00:33:55,795
Because, why is it not so efficient?

679
00:33:55,795 --> 00:34:00,370
Because there might be many states in here which are identical.

680
00:34:00,370 --> 00:34:04,735
And I'm not- I'm not aliasing those or I'm not treating them as identical.

681
00:34:04,735 --> 00:34:06,760
I'm saying I'm going to separately think about

682
00:34:06,760 --> 00:34:08,770
for each of those dates I would reach what would be

683
00:34:08,770 --> 00:34:10,870
the future reward I would get from that state under

684
00:34:10,870 --> 00:34:14,020
different sequences of actions and resulting states.

685
00:34:14,020 --> 00:34:16,105
And then if I want to figure out how to act,

686
00:34:16,105 --> 00:34:20,965
I go from my leaves and I take a max over all the,

687
00:34:20,965 --> 00:34:27,550
so let's say in this case I just made up for like a symbol small one.

688
00:34:27,550 --> 00:34:33,885
This is a1, a2, and let's say at that point I terminate.

689
00:34:33,885 --> 00:34:39,449
So I got like r(s, a1) and r(s, a2).

690
00:34:39,449 --> 00:34:41,774
So anytime I have a structure that looks like that,

691
00:34:41,775 --> 00:34:47,725
what I do is I take a max and the reward here is equal to whichever of those was bigger.

692
00:34:47,725 --> 00:34:50,949
Let's imagine that it was a2 that was bigger.

693
00:34:50,949 --> 00:34:54,849
So if I want to compute I basically roll all of these out,

694
00:34:54,850 --> 00:34:58,105
computing like getting a sample of the reward,

695
00:34:58,105 --> 00:35:00,085
and the next state is I go all the way out.

696
00:35:00,085 --> 00:35:03,025
And then to get the value at the root,

697
00:35:03,025 --> 00:35:07,330
whenever I see two- two, like a series of action nodes,

698
00:35:07,330 --> 00:35:08,875
I take a max over all of them,

699
00:35:08,875 --> 00:35:11,695
which was just like in our Bellman backup we're taking a max

700
00:35:11,695 --> 00:35:15,760
over the action that allows us to have the best future rewards.

701
00:35:15,760 --> 00:35:19,465
And then whenever I get to states- I'll do another one here.

702
00:35:19,465 --> 00:35:24,100
So let's say now I have two states s1 and s2,

703
00:35:24,100 --> 00:35:29,125
one of them has this value s1 and this one has value s2.

704
00:35:29,125 --> 00:35:32,035
And I want to figure out for this action,

705
00:35:32,035 --> 00:35:37,465
what my new value is then this is going to be equal to the probability of s1 given.

706
00:35:37,465 --> 00:35:38,830
Let's say I came from s0,

707
00:35:38,830 --> 00:35:44,830
s0, a1 times V(s1) plus probability of s2,

708
00:35:44,830 --> 00:35:48,530
s0, a1 times V(s2).

709
00:35:49,140 --> 00:35:51,580
This is exactly like, uh,

710
00:35:51,580 --> 00:35:53,260
um, when we do a Bellman backup,

711
00:35:53,260 --> 00:35:55,900
that we think about all the next states I could get two under

712
00:35:55,900 --> 00:35:59,020
the current action and current state times the value of each of those.

713
00:35:59,020 --> 00:36:01,570
Does that make sense?

714
00:36:01,570 --> 00:36:03,745
So we construct this tree out,

715
00:36:03,745 --> 00:36:05,530
and then in order to compute the value,

716
00:36:05,530 --> 00:36:10,420
we do one of two operations for either we take the max over the actions,

717
00:36:10,420 --> 00:36:15,670
if it's, uh, um, an action nodes, or we take an expectation.

718
00:36:15,670 --> 00:36:18,860
So these are called expecting max trees.

719
00:36:20,340 --> 00:36:24,640
Some of you guys might have seen these before in AI.

720
00:36:24,640 --> 00:36:27,055
Sometimes people often talk about minimax trees.

721
00:36:27,055 --> 00:36:29,680
So if you're playing game theory where the other agent gets

722
00:36:29,680 --> 00:36:32,590
to try to minimize your value and you get to maximize it.

723
00:36:32,590 --> 00:36:36,100
This is very similar except for we're doing an expectation over

724
00:36:36,100 --> 00:36:40,255
next states and a max over actions, okay?

725
00:36:40,255 --> 00:36:43,900
And it's exactly like dynamic program but more inefficient.

726
00:36:43,900 --> 00:36:46,735
But we're gonna see why we would want to do that in a second.

727
00:36:46,735 --> 00:36:48,920
So does anyone have questions about this?

728
00:36:49,290 --> 00:36:54,175
Okay. All right. So this is all- and the way we do this is that we have to have a model,

729
00:36:54,175 --> 00:36:57,115
because if we don't have a model right now we can't, uh,

730
00:36:57,115 --> 00:37:00,265
compute this expecting max exact- exactly because

731
00:37:00,265 --> 00:37:03,865
we're using that we know- like we're only expanding two states here.

732
00:37:03,865 --> 00:37:06,100
Um, and we- we- in order to figure

733
00:37:06,100 --> 00:37:08,650
out how much weight we want to put on each of those two states,

734
00:37:08,650 --> 00:37:11,395
we need to know the probability of getting to each of them.

735
00:37:11,395 --> 00:37:14,110
And so that's where we- we're using the model here,

736
00:37:14,110 --> 00:37:16,495
and we're using the model to get the rewards.

737
00:37:16,495 --> 00:37:19,855
So simulation-based search is similar,

738
00:37:19,855 --> 00:37:22,825
um, [NOISE] except for we're just going to simulate out with a model.

739
00:37:22,825 --> 00:37:25,730
We're not going to compute all of these sort of,

740
00:37:25,730 --> 00:37:28,710
um, exponentially growing number of futures.

741
00:37:28,710 --> 00:37:30,840
Instead, we're just gonna say I'm gonna start here,

742
00:37:30,840 --> 00:37:34,995
and I have a model and I need to have some policy here.

743
00:37:34,995 --> 00:37:36,755
But let's say I have a policy pi,

744
00:37:36,755 --> 00:37:37,990
and then I just use that.

745
00:37:37,990 --> 00:37:42,130
So I look at my policy for the current state and it tells me something to do.

746
00:37:42,130 --> 00:37:44,185
So I followed that action,

747
00:37:44,185 --> 00:37:48,385
and then I go into my model and I sample in s prime.

748
00:37:48,385 --> 00:37:50,845
So I look up my model and I say,

749
00:37:50,845 --> 00:37:54,340
"What would be the next state, given that I was in

750
00:37:54,340 --> 00:37:58,405
this particular state and took that action and I just simulate one next state?"

751
00:37:58,405 --> 00:38:02,200
This is just like how we could have simulated data from our models before.

752
00:38:02,200 --> 00:38:03,655
So let's say that got me to here,

753
00:38:03,655 --> 00:38:06,730
which was state s1. And then I look up again.

754
00:38:06,730 --> 00:38:10,135
I look up to my policy and I say, "What is the policy for s1?"

755
00:38:10,135 --> 00:38:14,005
Let's say that's a2 and then I also- then I follow it down to here.

756
00:38:14,005 --> 00:38:16,810
So just simulate out a trajectory.

757
00:38:16,810 --> 00:38:18,760
Just following my policy,

758
00:38:18,760 --> 00:38:21,265
simulating it out and I go until it terminates.

759
00:38:21,265 --> 00:38:25,700
And that gives me one return of how good that policy is.

760
00:38:28,290 --> 00:38:34,270
So in these sort of cases, we can just simulate complete trajectories with the model.

761
00:38:34,270 --> 00:38:37,300
Uh, and once we have those you could do something like

762
00:38:37,300 --> 00:38:40,120
model-free RL over those simulated trajectories,

763
00:38:40,120 --> 00:38:44,690
which either can be Monte Carlo or it could be something like TD learning.

764
00:38:45,510 --> 00:38:50,425
So if we think of this as sort of doing- like,

765
00:38:50,425 --> 00:38:53,335
in order to do that simulation we need some sort of policy.

766
00:38:53,335 --> 00:38:56,050
So you have to have some way to pick actions in our sort of

767
00:38:56,050 --> 00:38:59,035
simulated world when we think about being in the state and picking an action,

768
00:38:59,035 --> 00:39:00,295
how do we know what action to take?

769
00:39:00,295 --> 00:39:02,590
We follow our current simulation policy.

770
00:39:02,590 --> 00:39:07,375
So let's say we wanted to do effectively one step of policy improvement.

771
00:39:07,375 --> 00:39:10,180
So you have a policy, you have your model,

772
00:39:10,180 --> 00:39:12,730
and then you start off in a state and for each

773
00:39:12,730 --> 00:39:15,715
of the possible actions you simulate out trajectories.

774
00:39:15,715 --> 00:39:19,130
So this is like doing Monte Carlo rollouts.

775
00:39:20,760 --> 00:39:23,240
So I started my state.

776
00:39:23,240 --> 00:39:25,935
So this is, let's say I'm really in a state s_t,

777
00:39:25,935 --> 00:39:27,735
and I will need to figure out what to do next.

778
00:39:27,735 --> 00:39:29,295
So I start in that state s_t,

779
00:39:29,295 --> 00:39:31,980
and then in my head before I take an action in the real world,

780
00:39:31,980 --> 00:39:33,630
I think about all the actions I could take,

781
00:39:33,630 --> 00:39:37,935
and then I just do roll outs from each of them under a behavior policy pi,

782
00:39:37,935 --> 00:39:39,655
then I pick the max over those.

783
00:39:39,655 --> 00:39:43,310
So I'm really in state s_t, and then in my brain,

784
00:39:43,310 --> 00:39:44,470
I think about doing a_1,

785
00:39:44,470 --> 00:39:48,035
and then I do lots of roll outs from that under my policy pi.

786
00:39:48,035 --> 00:39:51,510
And then I do a_2 and do lots of roll outs out of that.

787
00:39:51,510 --> 00:39:54,275
a_3, this is all in my head,

788
00:39:54,275 --> 00:40:02,530
and that basically gives me now an estimate of Q s_t, a_1 under pi.

789
00:40:04,120 --> 00:40:07,430
So it's as if I was to take this action then follow pi,

790
00:40:07,430 --> 00:40:09,060
what would be my estimated Q function,

791
00:40:09,060 --> 00:40:12,145
then I do that for each of the actions, and then I can take a max.

792
00:40:12,145 --> 00:40:14,945
So this is sort of like doing one step of policy improvement,

793
00:40:14,945 --> 00:40:20,190
because this is going to depend on whatever my simulation policy is, does that make sense?

794
00:40:20,190 --> 00:40:22,800
So we have some existing simulation policy,

795
00:40:22,800 --> 00:40:27,290
I haven't told you how we get it, and then we use it to simulate out experience.

796
00:40:28,040 --> 00:40:30,940
Okay. So the question is whether or not we can

797
00:40:30,940 --> 00:40:33,475
actually do better than one step of policy improvement,

798
00:40:33,475 --> 00:40:35,855
because like how do we get these simulation policies?

799
00:40:35,855 --> 00:40:38,300
Like, okay, if we had a simulation policy and it was good,

800
00:40:38,300 --> 00:40:39,575
we could do this one step,

801
00:40:39,575 --> 00:40:42,320
but how could we do this in a more general setting?

802
00:40:42,320 --> 00:40:46,335
Well, the idea is that, um, if you have this model,

803
00:40:46,335 --> 00:40:50,415
you could actually compute the optimal values by doing this Expectimax Tree.

804
00:40:50,415 --> 00:40:52,795
So like I was in this state St,

805
00:40:52,795 --> 00:40:57,070
and instead of just thinking about- so remember in simulation

806
00:40:57,070 --> 00:40:59,745
based search we're just going to follow out one trajectory,

807
00:40:59,745 --> 00:41:02,030
but in the Expectimax tree we can think of,

808
00:41:02,030 --> 00:41:04,155
well, what if I did a1 or a2,

809
00:41:04,155 --> 00:41:08,550
and after that, whether I went to S1 or S2,

810
00:41:08,550 --> 00:41:11,350
and then what action should I do there?

811
00:41:11,980 --> 00:41:14,790
And I can think of basically trying to compute

812
00:41:14,790 --> 00:41:20,540
the optimal Q function under my approximate model for the current state, okay?

813
00:41:20,540 --> 00:41:23,915
The problem with that is that this tree gets really big.

814
00:41:23,915 --> 00:41:29,635
So in general, um, the size of the tree is going to scale at least with

815
00:41:29,635 --> 00:41:34,555
S times A to the H. If H is the horizon,

816
00:41:34,555 --> 00:41:38,540
because at each step, this is why it's not as efficient at dynamic programming,

817
00:41:38,540 --> 00:41:41,580
at- at each step you're going to think about all the possible next states,

818
00:41:41,580 --> 00:41:43,365
and then all the possible next actions.

819
00:41:43,365 --> 00:41:45,670
And so this tree is growing exponentially with the horizon.

820
00:41:45,670 --> 00:41:48,300
And if you think of something like AlphaGo,

821
00:41:48,300 --> 00:41:49,630
um, that are playing, you know,

822
00:41:49,630 --> 00:41:52,545
for a number of time steps before someone wins or loses,

823
00:41:52,545 --> 00:41:55,845
this H might be somewhere between 50 to 200.

824
00:41:55,845 --> 00:41:59,590
So if you have anything larger than an extremely small state space like one,

825
00:41:59,590 --> 00:42:04,360
then this is not gonna be, not gonna be feasible, okay?

826
00:42:04,360 --> 00:42:07,640
So the idea with a Monte Carlo Tree Search is that,

827
00:42:07,640 --> 00:42:09,015
okay we'd like to do better.

828
00:42:09,015 --> 00:42:12,060
In any case we need some sort of simulation policy if

829
00:42:12,060 --> 00:42:14,140
we're going to do this at all, and we can't be as

830
00:42:14,140 --> 00:42:16,960
computationally intractable as full Expectimax.

831
00:42:16,960 --> 00:42:19,560
So how do we do something in between?

832
00:42:19,740 --> 00:42:22,830
So- so the idea with Monte-Carlo Tree Search is

833
00:42:22,830 --> 00:42:24,980
to try to kind of get the best of both worlds,

834
00:42:24,980 --> 00:42:27,580
where what we'd really like is the Expectimax Tree where we

835
00:42:27,580 --> 00:42:30,755
think about all possible futures and take a max over those,

836
00:42:30,755 --> 00:42:35,220
um, but instead, we need to do that in a little bit more computationally tractable way,

837
00:42:35,220 --> 00:42:37,080
and why might that be possible?

838
00:42:37,080 --> 00:42:40,800
Well, let's think about this. If we have our initial starting state,

839
00:42:40,800 --> 00:42:45,610
let's say this is our general tree, that's going to all of these nodes.

840
00:42:45,610 --> 00:42:49,210
Some of these potential ways of acting might be really really bad.

841
00:42:49,210 --> 00:42:52,090
So some of these that might be clear, very early,

842
00:42:52,090 --> 00:42:53,960
like with very little amounts of data

843
00:42:53,960 --> 00:42:57,925
that, or very little amounts of roll outs that in fact,

844
00:42:57,925 --> 00:42:59,950
these are ways you would never want to play Go,

845
00:42:59,950 --> 00:43:01,775
because you're going to immediately lose.

846
00:43:01,775 --> 00:43:05,375
And so then, you don't need to bother sort of continuing to spend a lot of

847
00:43:05,375 --> 00:43:09,475
computational effort fleshing out that tree when something else might look much better.

848
00:43:09,475 --> 00:43:11,840
So that's kinda the intuition here,

849
00:43:11,840 --> 00:43:16,570
is that what we're gonna do is we're going to get us, construct a partial search tree.

850
00:43:16,570 --> 00:43:19,570
So we're going to start at the current state,

851
00:43:19,570 --> 00:43:21,735
and we can sample actions in next state,

852
00:43:21,735 --> 00:43:23,790
just like in simulation based search.

853
00:43:23,790 --> 00:43:26,010
So maybe we first sample A1,

854
00:43:26,010 --> 00:43:28,105
and then we sample S1 again,

855
00:43:28,105 --> 00:43:30,015
and then we sample A2.

856
00:43:30,015 --> 00:43:31,650
So we start off and we're kind of,

857
00:43:31,650 --> 00:43:34,830
the very first round it looks exactly like simulation based search,

858
00:43:34,830 --> 00:43:38,800
but the idea is that then we can do this multiple times and slowly fill out the tree.

859
00:43:38,800 --> 00:43:41,445
So maybe next time we happen to sample A2,

860
00:43:41,445 --> 00:43:44,125
and then maybe we sample S2,

861
00:43:44,125 --> 00:43:49,790
and then sample A1, and so you can think of sort of slowly filling in this Expectimax Tree.

862
00:43:49,790 --> 00:43:52,195
And in the limit, um,

863
00:43:52,195 --> 00:43:54,370
you will fill in the entire Expectimax tree.

864
00:43:54,370 --> 00:43:56,250
It's just that in practice you almost never will

865
00:43:56,250 --> 00:43:59,110
because it's computationally intractable.

866
00:43:59,250 --> 00:44:03,640
So what we're gonna do is do this, and we'll do this for,

867
00:44:03,640 --> 00:44:05,840
sort of a number of simulation episodes,

868
00:44:05,840 --> 00:44:08,800
each simulation episode can be thought of is you start at the root.

869
00:44:08,800 --> 00:44:11,495
This is the root node and current state you're really at,

870
00:44:11,495 --> 00:44:16,355
and then you roll out until you reach a terminal state or a horizon H,

871
00:44:16,355 --> 00:44:20,140
and then you go back to the start state and you again make another trajectory.

872
00:44:20,140 --> 00:44:22,340
And when you're done with all of this,

873
00:44:22,340 --> 00:44:24,975
you can do the same thing you would do in Expectimax,

874
00:44:24,975 --> 00:44:31,700
in that you're always gonna take a max over actions and an expectation over states.

875
00:44:33,060 --> 00:44:35,635
You only will have filled in part of the tree,

876
00:44:35,635 --> 00:44:37,970
so part of the tree might be missing.

877
00:44:40,310 --> 00:44:42,715
So in order to do this,

878
00:44:42,715 --> 00:44:44,260
there's two key aspects.

879
00:44:44,260 --> 00:44:48,260
One is, what do you do in parts of the tree where you already have some data?

880
00:44:48,260 --> 00:44:51,295
So like if you already have tried both actions in a state,

881
00:44:51,295 --> 00:44:53,070
which action should you pick again,

882
00:44:53,070 --> 00:44:56,590
and then what should you do when you reach like a node where there's

883
00:44:56,590 --> 00:44:59,810
nothing else there or like there's only been one thing tried so far?

884
00:44:59,810 --> 00:45:03,550
So this is often called the tree policy and the roll out policy.

885
00:45:03,550 --> 00:45:07,265
The roll out policy is for when you get to a node where,

886
00:45:07,265 --> 00:45:08,920
you know, you've only tried one thing,

887
00:45:08,920 --> 00:45:11,110
or there's no more data, or you've never been there before.

888
00:45:11,110 --> 00:45:14,400
So for example, maybe you sample a state that you'd never reached before,

889
00:45:14,400 --> 00:45:15,910
and so that's now a new node,

890
00:45:15,910 --> 00:45:17,980
and from then on you do a roll out policy.

891
00:45:17,980 --> 00:45:20,590
We'll show an example of this in a second.

892
00:45:20,600 --> 00:45:24,635
And then the idea is, when we're thinking about computing the Q function,

893
00:45:24,635 --> 00:45:29,430
we're just gonna average over all rewards that are received from that node onwards.

894
00:45:30,130 --> 00:45:32,655
This should seem a little bit weird,

895
00:45:32,655 --> 00:45:34,690
because we're not talking about maxes anymore,

896
00:45:34,690 --> 00:45:37,380
and we're not talking about doing- considering

897
00:45:37,380 --> 00:45:41,170
explicitly like the expectation over states  in a formal way,

898
00:45:41,170 --> 00:45:43,330
we're just gonna average this.

899
00:45:43,330 --> 00:45:47,610
The reason why this is okay is because, um,

900
00:45:47,610 --> 00:45:51,750
we're gonna, sort of sample actions in a way such that over time,

901
00:45:51,750 --> 00:45:54,300
we're gonna sample actions that look better much more,

902
00:45:54,300 --> 00:45:56,115
and so we expect that, uh,

903
00:45:56,115 --> 00:45:58,440
eventually, the distribution of data is

904
00:45:58,440 --> 00:46:02,465
gonna converge to the true Q. Yeah.

905
00:46:02,465 --> 00:46:06,200
Just to confirm, is it [inaudible] the simulation before, um,

906
00:46:06,200 --> 00:46:09,500
there's different kind of averaging and moving parts because it seemed before we

907
00:46:09,500 --> 00:46:13,215
were also doing a bunch of roll outs and then combining this,

908
00:46:13,215 --> 00:46:14,860
so that part is still the same, yes?

909
00:46:14,860 --> 00:46:18,590
Yes, great question, in many cases it's very similar though.

910
00:46:18,590 --> 00:46:20,330
We're still gonna be sort of doing

911
00:46:20,330 --> 00:46:22,300
a bunch of simulations where we're gonna be averaging them.

912
00:46:22,300 --> 00:46:25,890
The question is, what is the policy we're using to do the, the, uh,

913
00:46:25,890 --> 00:46:28,990
the roll outs is generally going to be changing for each roll out,

914
00:46:28,990 --> 00:46:31,160
instead of being identical across all roll outs,

915
00:46:31,160 --> 00:46:34,240
and then the way we are gonna average them is also different.

916
00:46:34,340 --> 00:46:38,420
And really the key part I think is that, instead of using,

917
00:46:38,420 --> 00:46:42,340
um, like when I said for, um,

918
00:46:42,340 --> 00:46:44,010
simple Monte Carlo search,

919
00:46:44,010 --> 00:46:47,920
that assumes that you fix a policy and get a whole bunch of roll outs from that policy,

920
00:46:47,920 --> 00:46:50,475
just starting with different initial actions,

921
00:46:50,475 --> 00:46:51,780
but then always following it.

922
00:46:51,780 --> 00:46:56,050
When you do Monte Carlo Tree Search and you also do say k roll outs, generally,

923
00:46:56,050 --> 00:46:58,340
the policy will be different for each of the k roll outs,

924
00:46:58,340 --> 00:47:02,500
and that's on purpose so that you can hopefully get to a better policy.

925
00:47:02,680 --> 00:47:05,610
So again, just to, and just to step back again,

926
00:47:05,610 --> 00:47:07,750
what are, you know, what's the whole loop of happening here?

927
00:47:07,750 --> 00:47:10,970
So what's happening in this case is like you have your agent,

928
00:47:10,970 --> 00:47:15,785
is our robot and it's trying to figure out an action to take,

929
00:47:15,785 --> 00:47:19,315
and then the real-world gives back a S prime and an R prime,

930
00:47:19,315 --> 00:47:23,820
and then what we're talking about right now is everything it needs to do in its head,

931
00:47:23,820 --> 00:47:29,780
like all these roll outs in order to decide the next action to take.

932
00:47:29,780 --> 00:47:34,250
So it's going to do a whole bunch of planning before it takes its next action,

933
00:47:34,250 --> 00:47:36,410
generally it may then throw that whole tree away,

934
00:47:36,410 --> 00:47:38,710
and then the world is gonna give it a new state and

935
00:47:38,710 --> 00:47:42,105
a new reward and then it's gonna do this whole process together again.

936
00:47:42,105 --> 00:47:46,490
And so the key thing is that we need to decide what action to take next.

937
00:47:47,600 --> 00:47:52,520
And we want to do so in a way that we're gonna get the best expected value,

938
00:47:52,520 --> 00:47:54,890
given the information we have so far.

939
00:47:54,890 --> 00:47:58,230
Now in general, um, in Monte Carlo Tree Search,

940
00:47:58,230 --> 00:48:03,170
you might also have another step here where you might compute a new model.

941
00:48:04,160 --> 00:48:06,670
So if you're doing this online,

942
00:48:06,670 --> 00:48:08,445
you might take your most recent data,

943
00:48:08,445 --> 00:48:10,200
retrain your model, given

944
00:48:10,200 --> 00:48:12,335
that model rerun Monte Carlo Tree Search

945
00:48:12,335 --> 00:48:15,410
and now decide what you're gonna do on the next time step.

946
00:48:17,550 --> 00:48:21,545
All right. So the key thing is really this tree policy and roll out,

947
00:48:21,545 --> 00:48:22,885
both of them make a difference.

948
00:48:22,885 --> 00:48:26,965
Um, the roll out often is done randomly at least in the most basic vanilla version,

949
00:48:26,965 --> 00:48:28,420
there's tons of extensions for,

950
00:48:28,420 --> 00:48:31,900
for this and the key part is that tree policy.

951
00:48:32,920 --> 00:48:38,295
So one of the really common ways to do this is called upper confidence tree search,

952
00:48:38,295 --> 00:48:42,460
and this relates to what we're talking about for the last few weeks with exploration.

953
00:48:42,460 --> 00:48:47,055
So the idea is, is that when we're rolling out, so let's say,

954
00:48:47,055 --> 00:48:50,740
we're in s_t and we're using our simulated models to

955
00:48:50,740 --> 00:48:55,120
think about the next actions and states we would be in.

956
00:48:55,290 --> 00:48:59,475
So let's say, um, I get to state one.

957
00:48:59,475 --> 00:49:01,480
And at this point, I want to, let's say,

958
00:49:01,480 --> 00:49:04,770
I've taken a1 and a2 in the past and I've ended up with,

959
00:49:04,770 --> 00:49:08,050
you know, I've done lots of roll outs from these and a number of roll outs from here.

960
00:49:08,050 --> 00:49:10,890
And let's say, three times I got, let's say, I won the game.

961
00:49:10,890 --> 00:49:13,970
So three 1s and I got one 1 and one 0.

962
00:49:13,970 --> 00:49:16,170
These guys.

963
00:49:16,170 --> 00:49:17,600
So, so the key is what,

964
00:49:17,600 --> 00:49:21,480
what action should I take next time I encounter s1 when I'm doing, where I roll

965
00:49:21,480 --> 00:49:26,600
out and the idea is to be optimistic with respect to the data you have so far.

966
00:49:26,600 --> 00:49:29,695
So essentially, we're going to treat this as a bandit.

967
00:49:29,695 --> 00:49:34,100
Uh, think of each decision point as its own bandit,

968
00:49:34,100 --> 00:49:36,400
its own independent bandit and say, well,

969
00:49:36,400 --> 00:49:39,130
if I've taken all the actions I could for my current state,

970
00:49:39,130 --> 00:49:42,945
what is my average reward I got from each of those actions under

971
00:49:42,945 --> 00:49:47,175
any roll out that I've taken this from that particular node in the graph and action,

972
00:49:47,175 --> 00:49:49,040
and how many times did I take it?

973
00:49:49,040 --> 00:49:56,400
So you can get your empirical average for that state one in the,

974
00:49:56,400 --> 00:50:00,490
in the graph a1 plus a discount factor that

975
00:50:00,490 --> 00:50:04,500
often looks like the number of times you've been in that particular node.

976
00:50:04,500 --> 00:50:12,530
Okay. This is really a node in the graph and this is also for that node.

977
00:50:13,830 --> 00:50:15,935
So we think about sort of,

978
00:50:15,935 --> 00:50:17,775
every time I've been in that node what,

979
00:50:17,775 --> 00:50:19,020
and taken that particular action,

980
00:50:19,020 --> 00:50:21,785
what's the average reward I've gotten plus how many times if I done that?

981
00:50:21,785 --> 00:50:24,610
And it just allows you to be, um, uses optimism again.

982
00:50:24,610 --> 00:50:26,895
Says, well, when I've reached different parts of the graph

983
00:50:26,895 --> 00:50:30,040
before using my simulated model, what things look good?

984
00:50:30,040 --> 00:50:33,980
I'm gonna focus on making sure that that's the part of the tree that I flesh out more.

985
00:50:33,980 --> 00:50:35,900
Because that's the one where I think I'm gonna get

986
00:50:35,900 --> 00:50:38,590
likely reach policies that have higher value.

987
00:50:38,590 --> 00:50:41,070
And so hopefully, that's gonna mean that I have le- need

988
00:50:41,070 --> 00:50:44,350
less computation in order to compute a good policy.

989
00:50:44,620 --> 00:50:47,960
Does that make sense? So like if you had an oracle,

990
00:50:47,960 --> 00:50:50,200
that could tell you what the optimal policy was,

991
00:50:50,200 --> 00:50:53,255
then, you would only need to fill in that part of the tree.

992
00:50:53,255 --> 00:50:57,000
And what we're using here is we're saying well, given the data we've seen so far,

993
00:50:57,000 --> 00:50:58,785
we, kind of, only, you know,

994
00:50:58,785 --> 00:51:01,140
focus on the parts of the tree that seem like they're going to be

995
00:51:01,140 --> 00:51:03,775
the ones that when we take our max over our actions,

996
00:51:03,775 --> 00:51:06,150
are gonna be the ones that we end up, uh,

997
00:51:06,150 --> 00:51:09,390
propagating the values back up to the root.

998
00:51:10,830 --> 00:51:15,950
All right. So we've maintained an upper confidence bound over the reward of each arm,

999
00:51:15,950 --> 00:51:18,410
um, using, sort of,

1000
00:51:18,410 --> 00:51:20,320
exactly the same thing as what we've done before.

1001
00:51:20,320 --> 00:51:22,280
And we're treating each of the nodes,

1002
00:51:22,280 --> 00:51:25,970
so each state node as a separate bandit.

1003
00:51:29,060 --> 00:51:31,705
And so that means essentially that, you know,

1004
00:51:31,705 --> 00:51:33,720
the next time we reach that same node in the tree,

1005
00:51:33,720 --> 00:51:39,335
we might do something different because the accounts will change.

1006
00:51:39,335 --> 00:51:41,025
TD, uh, uh, [inaudible] this is kind of like TD in,

1007
00:51:41,025 --> 00:51:44,605
in the sense that [NOISE] if you reward for the bandit problem,

1008
00:51:44,605 --> 00:51:46,025
the value of that,

1009
00:51:46,025 --> 00:51:48,210
of the, of the state or if the, um,

1010
00:51:48,210 --> 00:51:52,420
existing action pair or is it the reward for just that transition?

1011
00:51:52,420 --> 00:51:55,720
It's a great question. So, and what is the reward for this bandit?

1012
00:51:55,720 --> 00:51:57,605
We are gonna treat it as, um,

1013
00:51:57,605 --> 00:52:01,135
essentially the full roll out from that node,

1014
00:52:01,135 --> 00:52:04,305
um, because that's what we're averaging over and we're cou- doing counts.

1015
00:52:04,305 --> 00:52:07,375
It's not like TD in the sense that we're doing this per node.

1016
00:52:07,375 --> 00:52:10,795
So as I mentioned before, you know, you might have s1,

1017
00:52:10,795 --> 00:52:13,720
a1 appear in this part of the graph and s1 a1 appear over here,

1018
00:52:13,720 --> 00:52:15,425
and we're not combining their accounts.

1019
00:52:15,425 --> 00:52:17,945
We're treating every node as if it's totally distinct.

1020
00:52:17,945 --> 00:52:21,805
Even though often the model we'll be using to simulate will be Markov.

1021
00:52:21,805 --> 00:52:23,790
But in ter- in sort of the tree, you can do it,

1022
00:52:23,790 --> 00:52:27,575
it's mostly that it becomes a lot more complicated for implementation.

1023
00:52:27,575 --> 00:52:30,785
If you wanted to basically treat it as a graph instead of a tree.

1024
00:52:30,785 --> 00:52:33,950
Now, I think the other point that you're bringing up,

1025
00:52:33,950 --> 00:52:35,785
we'll come back to in a second which is,

1026
00:52:35,785 --> 00:52:37,680
is this a good idea [LAUGHTER] is,

1027
00:52:37,680 --> 00:52:40,175
is well, what are the limitations of the bandit setting?

1028
00:52:40,175 --> 00:52:41,440
So we'll come back to that in a second.

1029
00:52:41,440 --> 00:52:45,020
All right, so let's talk about this in the context of Go.

1030
00:52:45,020 --> 00:52:47,750
For those of you that haven't played Go or aren't too familiar with it.

1031
00:52:47,750 --> 00:52:49,210
It's at least 2,500 years old.

1032
00:52:49,210 --> 00:52:51,455
It's considered the classic hardest board game.

1033
00:52:51,455 --> 00:52:56,175
Um, and it's been known as a grand challenge task in AI for a very long period of time.

1034
00:52:56,175 --> 00:53:00,195
Um, [NOISE] just to remind ourselves, um,

1035
00:53:00,195 --> 00:53:02,310
this does not involve decision-making in

1036
00:53:02,310 --> 00:53:04,725
the case where the dynamics and reward model are unknown,

1037
00:53:04,725 --> 00:53:06,310
but the rules of Go are known.

1038
00:53:06,310 --> 00:53:07,420
The reward structure is known,

1039
00:53:07,420 --> 00:53:10,400
but it's an incredibly large search space.

1040
00:53:10,400 --> 00:53:14,235
So if we think about the combinatorics of the number of possible,

1041
00:53:14,235 --> 00:53:18,810
um, boards that you can see, it's, it's extremely large.

1042
00:53:19,230 --> 00:53:24,570
So uh, just briefly there's two different types of stones.

1043
00:53:24,570 --> 00:53:25,970
Um, most people probably know this.

1044
00:53:25,970 --> 00:53:27,980
Uh, and typically it's played on a 19

1045
00:53:27,980 --> 00:53:30,020
by 19 board though people have also thought about,

1046
00:53:30,020 --> 00:53:32,005
you know, some people play on smaller boards.

1047
00:53:32,005 --> 00:53:34,425
And you want to capture the most territory,

1048
00:53:34,425 --> 00:53:36,680
and it's a finite game because there's

1049
00:53:36,680 --> 00:53:40,255
a finite number of places to put stones on the board.

1050
00:53:40,255 --> 00:53:47,555
So it's a finite horizon [NOISE].

1051
00:53:47,555 --> 00:53:50,760
I'm gonna go through this part sort of briefly there's different ways to write down.

1052
00:53:50,760 --> 00:53:52,535
You can write down the reward function, um,

1053
00:53:52,535 --> 00:53:54,030
for this game in terms of,

1054
00:53:54,030 --> 00:53:57,375
uh, you know, different, you could do different features.

1055
00:53:57,375 --> 00:54:00,670
The simplest one is just to look at whether or not white wins or black wins on

1056
00:54:00,670 --> 00:54:02,130
the game and in that case it's

1057
00:54:02,130 --> 00:54:05,480
a very sparse reward signal and you only get reward at the very end.

1058
00:54:05,480 --> 00:54:08,035
You just have to play out all the way and see which ga- which,

1059
00:54:08,035 --> 00:54:10,095
uh, which person won.

1060
00:54:10,095 --> 00:54:12,850
And then your value function is essentially what's

1061
00:54:12,850 --> 00:54:16,840
the expected probability of you winning in the current state.

1062
00:54:18,680 --> 00:54:22,225
So how does this work if we do Monte  Carlo evaluation?

1063
00:54:22,225 --> 00:54:24,425
So let's imagine this is your current board.

1064
00:54:24,425 --> 00:54:27,225
And you have a particular policy.

1065
00:54:27,225 --> 00:54:31,330
Then you play out against,

1066
00:54:31,330 --> 00:54:33,965
um, a stationary opponent is normally assumed.

1067
00:54:33,965 --> 00:54:38,365
And then, at the end you see your outcomes and maybe you won twice and you lost twice.

1068
00:54:38,365 --> 00:54:42,683
And so then, the value for this current start state is a half.

1069
00:54:42,683 --> 00:54:48,030
Okay, so how would we do Monte-Carlo tree search in this case?

1070
00:54:48,030 --> 00:54:51,030
So you start off and you have one single start state.

1071
00:54:51,030 --> 00:54:54,325
So at this point you haven't simulated any part of the tree.

1072
00:54:54,325 --> 00:54:56,265
And so you've never taken any action,

1073
00:54:56,265 --> 00:54:58,695
so you just sample randomly.

1074
00:54:58,695 --> 00:55:03,380
So maybe you take a1. And then you follow your default policy and this is often random.

1075
00:55:03,380 --> 00:55:08,550
Though for things like AlphaGo you often want much better policies but, um,

1076
00:55:08,550 --> 00:55:12,000
you can just use a random policy and you'll just take random actions and get to

1077
00:55:12,000 --> 00:55:16,160
next states and you do this until the end and you see you either win or lose.

1078
00:55:16,160 --> 00:55:19,500
Now, um, in this case it's a two player game.

1079
00:55:19,500 --> 00:55:22,450
So we do a minimax tree instead of expectimax,

1080
00:55:22,450 --> 00:55:25,720
but the basic ideas are exactly the same.

1081
00:55:25,720 --> 00:55:28,025
So that's my first roll out.

1082
00:55:28,025 --> 00:55:30,090
I'm gonna do lots of these roll outs before I

1083
00:55:30,090 --> 00:55:32,265
figure out how I'm going to actually place my piece.

1084
00:55:32,265 --> 00:55:35,810
All right. So the next time, so this is my second roll out.

1085
00:55:35,810 --> 00:55:38,060
This is the second roll out of my head [NOISE].

1086
00:55:38,060 --> 00:55:42,710
And say okay well, last time, um, you know, I, I took this action.

1087
00:55:42,710 --> 00:55:44,490
So now I have my default policy.

1088
00:55:44,490 --> 00:55:46,710
So this time I'm gonna take the other action.

1089
00:55:46,710 --> 00:55:48,460
Generally, you want to fill in all,

1090
00:55:48,460 --> 00:55:51,865
try all the actions at least once from a current node.

1091
00:55:51,865 --> 00:55:55,975
Now, the particular order in which you try actions can make a big difference and, um,

1092
00:55:55,975 --> 00:55:58,355
early on there were significant savings by putting in

1093
00:55:58,355 --> 00:56:00,855
action heuristics of what order to try actions in.

1094
00:56:00,855 --> 00:56:05,110
For right now let's just imagine you have to try all actions, um, equally.

1095
00:56:05,110 --> 00:56:07,230
So in this case, now, you take the other action and then

1096
00:56:07,230 --> 00:56:09,360
after that you've never tried anything from that action.

1097
00:56:09,360 --> 00:56:10,890
So you just do a roll out.

1098
00:56:10,890 --> 00:56:13,180
Again, just acting randomly.

1099
00:56:13,460 --> 00:56:16,255
Okay, and then, you repeat this.

1100
00:56:16,255 --> 00:56:18,160
So now, when you get to here,

1101
00:56:18,160 --> 00:56:19,860
so let's say I've tried this before.

1102
00:56:19,860 --> 00:56:22,305
So now, when I get to this node,

1103
00:56:22,305 --> 00:56:24,160
I have to pick, um,

1104
00:56:24,160 --> 00:56:27,785
I have to do a max maybe using my UCT.

1105
00:56:27,785 --> 00:56:30,870
So I look at what was the reward for this one and the reward for

1106
00:56:30,870 --> 00:56:35,130
this one, plus you know something about the counts.

1107
00:56:35,310 --> 00:56:37,580
All right. This is Q to make it clear.

1108
00:56:37,580 --> 00:56:39,740
Okay. And so I pick

1109
00:56:39,740 --> 00:56:41,470
whichever action happened to have looked better

1110
00:56:41,470 --> 00:56:43,210
in the roll outs I've done from it so far.

1111
00:56:43,210 --> 00:56:46,365
And then, I'm gonna focus on expanding that part of the tree.

1112
00:56:46,365 --> 00:56:50,175
And you keep doing this, and you're gonna slowly sort of build out

1113
00:56:50,175 --> 00:56:53,625
the tree and you do this until your computational budget expires.

1114
00:56:53,625 --> 00:56:56,660
And then, you go to the bottom of the tree and you go all the way back

1115
00:56:56,660 --> 00:57:00,255
up where for each of the action nodes you're taking a max.

1116
00:57:00,255 --> 00:57:03,640
And each of this state nodes you're doing an expecti- or you're doing a,

1117
00:57:03,640 --> 00:57:05,980
in this case minimax.

1118
00:57:07,330 --> 00:57:09,490
You just construct a [inaudible].

1119
00:57:09,490 --> 00:57:13,035
So, so what does the opponent do, is it like a stationary opponent, like, what does that mean?

1120
00:57:13,035 --> 00:57:15,295
Great question. Okay. So, um,

1121
00:57:15,295 --> 00:57:19,915
in reality, I think one of the other really big insights to why people got

1122
00:57:19,915 --> 00:57:21,415
Go to work is self play.

1123
00:57:21,415 --> 00:57:26,210
So typically in this, you would use the current agent as the opponent.

1124
00:57:26,760 --> 00:57:31,930
So I take whatever policy I just computed for the other.

1125
00:57:31,930 --> 00:57:34,000
It- Look, particularly, let's imagine that like, I kept that tree.

1126
00:57:34,000 --> 00:57:35,200
So it already knows what it could do.

1127
00:57:35,200 --> 00:57:36,760
So, um, at each point,

1128
00:57:36,760 --> 00:57:38,050
I would look at I would have

1129
00:57:38,050 --> 00:57:41,530
the other agent tell me what action it would do in that state.

1130
00:57:41,530 --> 00:57:43,825
But one of the really so,

1131
00:57:43,825 --> 00:57:47,350
so I think self play was an incredibly important, um, insight for this.

1132
00:57:47,350 --> 00:57:48,535
And why is it so important?

1133
00:57:48,535 --> 00:57:51,580
Because if I play against a grand-master in Go,

1134
00:57:51,580 --> 00:57:54,460
I get no reward for a really long period of time.

1135
00:57:54,460 --> 00:57:57,460
Um, and that's an incredibly hard thing for an agent to learn from,

1136
00:57:57,460 --> 00:57:59,440
because there's no other reward signal.

1137
00:57:59,440 --> 00:58:02,440
And so basically, you're just playing these tons and tons and tons of games and like,

1138
00:58:02,440 --> 00:58:04,480
there's just no signal for a really, really long time.

1139
00:58:04,480 --> 00:58:06,280
And so you need some sort of signal so you can

1140
00:58:06,280 --> 00:58:08,695
start to bootstrap and actually get to a good policy.

1141
00:58:08,695 --> 00:58:10,750
If I play against me like,

1142
00:58:10,750 --> 00:58:12,400
five minutes ago, I'm probably gonna beat them [LAUGHTER].

1143
00:58:12,400 --> 00:58:15,520
Um, and or at least half the time maybe I'll beat them.

1144
00:58:15,520 --> 00:58:19,210
And so that allows because you can have two players

1145
00:58:19,210 --> 00:58:20,890
that are both bad and one of them's

1146
00:58:20,890 --> 00:58:23,900
gonna win and one of them is gonna lose and you start to get signal.

1147
00:58:24,060 --> 00:58:27,700
And so the self play idea has been

1148
00:58:27,700 --> 00:58:31,375
hugely helpful in the context of games, like, two-player games.

1149
00:58:31,375 --> 00:58:33,520
Because it can mean that you can start to get

1150
00:58:33,520 --> 00:58:37,495
some reward signal about what things are successful or not and then you- It's both so,

1151
00:58:37,495 --> 00:58:39,820
like, it both gives you, helps with

1152
00:58:39,820 --> 00:58:42,820
this sparse reward problem and it gives you a curriculum.

1153
00:58:42,820 --> 00:58:44,755
Because you're always kind of,

1154
00:58:44,755 --> 00:58:46,990
only playing in an environment that's a little bit

1155
00:58:46,990 --> 00:58:49,960
more difficult than perhaps what you can tolerate, can manage.

1156
00:58:49,960 --> 00:58:52,630
And actually I think, um, it would be really,

1157
00:58:52,630 --> 00:58:56,185
really cool if we could figure out how to take the same ideas to a lot of other domains.

1158
00:58:56,185 --> 00:59:00,910
Like if there are other ways to essentially make self play for things like medicine or,

1159
00:59:00,910 --> 00:59:03,865
[LAUGHTER] um, uh, customer relations or things like that.

1160
00:59:03,865 --> 00:59:05,545
It would be really great because it's often

1161
00:59:05,545 --> 00:59:08,095
really hard to get the sort of reward signal you want.

1162
00:59:08,095 --> 00:59:10,405
And that's one of the really nice things here.

1163
00:59:10,405 --> 00:59:14,260
Self-play, what if we get stuck in some local extrema?

1164
00:59:14,260 --> 00:59:18,610
Absolutely can happen, yes. So what in self play, how does it arrive if you get stuck in,

1165
00:59:18,610 --> 00:59:20,335
you could but you always try to max.

1166
00:59:20,335 --> 00:59:21,985
So it's a little bit like policy improvement.

1167
00:59:21,985 --> 00:59:23,650
You're always trying to do a little bit better.

1168
00:59:23,650 --> 00:59:25,690
You're still trying to win. Um, so it's

1169
00:59:25,690 --> 00:59:28,300
possible you could get stuck in a case where you're both just,

1170
00:59:28,300 --> 00:59:29,800
you know, winning half the time,

1171
00:59:29,800 --> 00:59:31,930
but then there should be something that you can exploit.

1172
00:59:31,930 --> 00:59:33,520
And if there's something you could exploit,

1173
00:59:33,520 --> 00:59:37,440
if you do enough planning you should be able to identify it. Yeah.

1174
00:59:37,440 --> 00:59:42,270
Do you imagine that there would be this kind of transition point where you might get

1175
00:59:42,270 --> 00:59:47,825
added benefit from transitioning to a more expert player to play against versus yourself?

1176
00:59:47,825 --> 00:59:51,150
You need to kind of start slowly and ease into it but then you might actually

1177
00:59:51,150 --> 00:59:54,360
do learn faster by playing against somebody harder modes.

1178
00:59:54,360 --> 00:59:57,190
Yeah, question is, like, you know, would you also always wanna kinda just,

1179
00:59:57,190 --> 00:59:58,330
like, self play against, you know,

1180
00:59:58,330 --> 01:00:00,880
yourself five minutes ago or maybe at some point you- It will be

1181
01:00:00,880 --> 01:00:03,610
more efficient to go at someone harder. I think it's a great question.

1182
01:00:03,610 --> 01:00:05,245
I think probably that's the case.

1183
01:00:05,245 --> 01:00:09,400
Like, probably there will be cases where you could do bigger curriculum jumps,

1184
01:00:09,400 --> 01:00:11,290
um, and that might accelerate learning.

1185
01:00:11,290 --> 01:00:13,690
But I think it's a tricky sweet spot there if like,

1186
01:00:13,690 --> 01:00:16,420
you still need to have enough reward signal to bootstrap from.

1187
01:00:16,420 --> 01:00:19,525
Absolutely. All right.

1188
01:00:19,525 --> 01:00:21,670
So, you know, the benefits of doing this is it

1189
01:00:21,670 --> 01:00:23,725
becomes this highly selective best-first search,

1190
01:00:23,725 --> 01:00:25,930
because you're sort of constructing part of the tree but

1191
01:00:25,930 --> 01:00:28,240
you're constructing it in a very specific way.

1192
01:00:28,240 --> 01:00:32,830
And the goal is that you should be way more sample efficient than doing Expectimax,

1193
01:00:32,830 --> 01:00:35,110
making the whole tree but you're gonna be much better than

1194
01:00:35,110 --> 01:00:37,225
doing just a single step of policy improvement,

1195
01:00:37,225 --> 01:00:39,070
with some fixed, um,

1196
01:00:39,070 --> 01:00:42,040
you know, simulation-based policy.

1197
01:00:42,040 --> 01:00:45,520
And it's also, you know, parallelizable anytime,

1198
01:00:45,520 --> 01:00:47,005
anytime in the sense that like,

1199
01:00:47,005 --> 01:00:50,785
whether you have one minute or you have three hours to compute the next action.

1200
01:00:50,785 --> 01:00:54,040
And you know, three hours can be very realistic if it's something like, you know,

1201
01:00:54,040 --> 01:00:58,360
a customer recommendation article or a thing that's gonna make or maybe you're

1202
01:00:58,360 --> 01:01:00,370
gonna make one decision per day and so you can run it

1203
01:01:00,370 --> 01:01:02,710
overnight for eight hours and then compute that one decision.

1204
01:01:02,710 --> 01:01:04,570
So, um, it allows you to,

1205
01:01:04,570 --> 01:01:07,120
to take advantage of the computation you have but then

1206
01:01:07,120 --> 01:01:10,180
always provide an answer no matter how quickly you need to do that,

1207
01:01:10,180 --> 01:01:12,440
cause you just do less roll outs.

1208
01:01:12,660 --> 01:01:15,370
Okay, um, I'm gonna skip this for now.

1209
01:01:15,370 --> 01:01:16,555
I just want to mention briefly,

1210
01:01:16,555 --> 01:01:18,340
um, I think it was a question.

1211
01:01:18,340 --> 01:01:21,985
It's a little weird that we do bandits in each of the nodes.

1212
01:01:21,985 --> 01:01:26,485
And intuitively, the reason it's a little bit weird is because in bandits,

1213
01:01:26,485 --> 01:01:28,720
why do we do optimism under uncertainty?

1214
01:01:28,720 --> 01:01:32,950
We do it because we're actually incurring the pain of making bad decisions.

1215
01:01:32,950 --> 01:01:35,815
And so the idea with optimism under uncertainty is that

1216
01:01:35,815 --> 01:01:38,695
either you really do get high reward or you learn something.

1217
01:01:38,695 --> 01:01:40,840
The weird thing about doing that for planning is

1218
01:01:40,840 --> 01:01:44,485
that we're not suffering if we make bad decisions in our head.

1219
01:01:44,485 --> 01:01:47,005
Um, essentially, we're just trying to figure out

1220
01:01:47,005 --> 01:01:50,470
what actions can I take as quickly as possible in terms of,

1221
01:01:50,470 --> 01:01:54,370
like, value of information so that I know what the right action is to get the route.

1222
01:01:54,370 --> 01:01:58,585
And so it doesn't actually matter if I simulate out bad actions.

1223
01:01:58,585 --> 01:02:01,090
If it allows me to get a better decision of the route.

1224
01:02:01,090 --> 01:02:03,625
We just give a really quick example of where that could be different.

1225
01:02:03,625 --> 01:02:07,730
So if you have something like this.

1226
01:02:09,060 --> 01:02:13,720
Let's say this is the potential value of Q.

1227
01:02:13,720 --> 01:02:19,015
Okay. So this is the value of A1 and this is the value of A2 and this is our uncertainty.

1228
01:02:19,015 --> 01:02:22,285
Well, if you're being optimistic, you're always gonna select this,

1229
01:02:22,285 --> 01:02:23,800
because it's got a higher value.

1230
01:02:23,800 --> 01:02:27,250
But if you wanna be really confident that A1 is better,

1231
01:02:27,250 --> 01:02:30,310
you should select A2 because likely when you do that,

1232
01:02:30,310 --> 01:02:32,410
you're gonna update your confidence intervals and now you're

1233
01:02:32,410 --> 01:02:35,600
gonna be totally sure that A1 is best.

1234
01:02:35,700 --> 01:02:38,440
But this approach won't do that.

1235
01:02:38,440 --> 01:02:40,000
Okay, because it's, it's, um,

1236
01:02:40,000 --> 01:02:42,145
it's like, no I'm gonna suffer the cost in my head

1237
01:02:42,145 --> 01:02:44,440
of taking the wrong action so I'm gonna take A1.

1238
01:02:44,440 --> 01:02:47,755
But if ultimately, you just need to know what the right action is to do,

1239
01:02:47,755 --> 01:02:51,370
then sometimes in terms of computation you should take A2 because now,

1240
01:02:51,370 --> 01:02:53,410
your confidence intervals will likely separate.

1241
01:02:53,410 --> 01:02:55,060
You don't need to do any more computation.

1242
01:02:55,060 --> 01:02:58,165
Um, so it's not clear that the bandits,

1243
01:02:58,165 --> 01:03:02,390
um, at each node is the optimal thing to do but it's pretty efficient.

1244
01:03:02,640 --> 01:03:06,370
All right. So that's basically all we're g- I'm gonna say about Go.

1245
01:03:06,370 --> 01:03:08,740
There's, um, some really beautiful papers,

1246
01:03:08,740 --> 01:03:11,335
uh, about this including the new recent extensions.

1247
01:03:11,335 --> 01:03:14,275
Um, and they have applications to chess as well as, you know,

1248
01:03:14,275 --> 01:03:15,775
a number of other games, uh,

1249
01:03:15,775 --> 01:03:17,860
which I think are really, they're, they're amazing results.

1250
01:03:17,860 --> 01:03:20,110
So I highly encourage you to look at some of those papers.

1251
01:03:20,110 --> 01:03:23,425
Let me just briefly talk about sort of popping back up to the end of the course,

1252
01:03:23,425 --> 01:03:26,335
um, because it's the last lecture. All right.

1253
01:03:26,335 --> 01:03:28,090
So I just wanted to sort of refresh on,

1254
01:03:28,090 --> 01:03:29,935
you know, what were the goals of the course, um,

1255
01:03:29,935 --> 01:03:34,180
as we end and some of these you guys had a chance to practice on, on Monday.

1256
01:03:34,180 --> 01:03:36,340
Um, but I just wanted to say

1257
01:03:36,340 --> 01:03:38,800
what I think of as kinda the key things that I hope you got out of it.

1258
01:03:38,800 --> 01:03:42,670
So then one is sort of, what is the key features of RL versus everything else?

1259
01:03:42,670 --> 01:03:45,370
Um, both AI and supervised learning.

1260
01:03:45,370 --> 01:03:48,280
And to me is that really this key, uh, issue of, um,

1261
01:03:48,280 --> 01:03:51,595
the agents being able to gather their own data and make decisions in the world.

1262
01:03:51,595 --> 01:03:54,430
And so it's the censored data

1263
01:03:54,430 --> 01:03:57,580
that's very different than the IID assumption for supervised learning.

1264
01:03:57,580 --> 01:04:00,760
And it's also very different from planning because you are

1265
01:04:00,760 --> 01:04:04,825
reliant on the data you get about the world in order to make decisions.

1266
01:04:04,825 --> 01:04:08,140
Um, this is probably- The second thing is

1267
01:04:08,140 --> 01:04:11,020
probably the thing that for many of you might end up being the most useful,

1268
01:04:11,020 --> 01:04:14,170
which is just how do you figure out given a problem, um,

1269
01:04:14,170 --> 01:04:17,545
whether you should even write it down as an RL problem and how to formulate it.

1270
01:04:17,545 --> 01:04:19,720
And we had you practice this a little bit on, uh,

1271
01:04:19,720 --> 01:04:23,380
Monday and also it's a chance to think about this a lot in some of your projects.

1272
01:04:23,380 --> 01:04:25,465
But I think this is often one of the really hard parts.

1273
01:04:25,465 --> 01:04:29,455
It has huge implications for how easy or hard it is to solve the problem.

1274
01:04:29,455 --> 01:04:30,985
Um, and it's often unclear.

1275
01:04:30,985 --> 01:04:33,744
There's lots of ways to write down the state space of describing

1276
01:04:33,744 --> 01:04:36,790
a patient or describing a student or describing a customer.

1277
01:04:36,790 --> 01:04:39,100
And in some ways it goes back to this issue

1278
01:04:39,100 --> 01:04:42,040
of function approximation versus sample efficiency.

1279
01:04:42,040 --> 01:04:44,890
If I treat all customers as the same, I have a lot of data.

1280
01:04:44,890 --> 01:04:46,240
That's probably a pretty bad model.

1281
01:04:46,240 --> 01:04:48,700
So there's a lot of different trade-offs that come up

1282
01:04:48,700 --> 01:04:51,325
in these cases and I'm sure all of you guys will,

1283
01:04:51,325 --> 01:04:54,235
um, think about interesting, exciting new ways to address that.

1284
01:04:54,235 --> 01:04:56,650
And then the other three things were, you know,

1285
01:04:56,650 --> 01:04:59,350
to be very familiar with a number of common RL algorithms,

1286
01:04:59,350 --> 01:05:01,090
which you guys have implemented a lot.

1287
01:05:01,090 --> 01:05:05,065
Um, to understand how we should even decide if an RL algorithm is good,

1288
01:05:05,065 --> 01:05:07,210
whether it's empirically, compu- you know,

1289
01:05:07,210 --> 01:05:09,610
in terms of its computational complexity or things like

1290
01:05:09,610 --> 01:05:12,805
how much data it takes or its performance guarantees,

1291
01:05:12,805 --> 01:05:16,330
um, and to understand this exploration exploitation challenge,

1292
01:05:16,330 --> 01:05:18,370
which really is quite unique to RL.

1293
01:05:18,370 --> 01:05:21,190
It doesn't come up in planning, doesn't come up in ML.

1294
01:05:21,190 --> 01:05:23,710
Um, and again, it's this critical issue of like, how do you

1295
01:05:23,710 --> 01:05:26,485
gather data quickly in order to make good decisions.

1296
01:05:26,485 --> 01:05:30,220
Um, if you wanna learn more about reinforcement learning,

1297
01:05:30,220 --> 01:05:32,080
there's a bunch of other classes, uh, particularly,

1298
01:05:32,080 --> 01:05:34,075
Mykel Kochenderfer has some really nice ones.

1299
01:05:34,075 --> 01:05:36,280
And also Ben Van Roy does some nice ones

1300
01:05:36,280 --> 01:05:39,070
particularly looking at some of the more theoretical aspects of it.

1301
01:05:39,070 --> 01:05:41,410
And then I do an advanced survey of it where we

1302
01:05:41,410 --> 01:05:44,350
do current topics and it's a project-based class.

1303
01:05:44,350 --> 01:05:47,650
Um, and I'll just I guess I'll, um, two more things.

1304
01:05:47,650 --> 01:05:50,695
One is that I think, you know, we see some really amazing results,

1305
01:05:50,695 --> 01:05:52,270
uh, Go is one example.

1306
01:05:52,270 --> 01:05:54,850
Uh, and we're seeing-starting to see some really exciting results in

1307
01:05:54,850 --> 01:05:57,295
robotics but I think we're missing,

1308
01:05:57,295 --> 01:05:59,305
most of us do not have RL on our phone yet

1309
01:05:59,305 --> 01:06:01,690
in the way that we have face recognition on our phone.

1310
01:06:01,690 --> 01:06:04,480
And so I think that the potential of using these types of

1311
01:06:04,480 --> 01:06:07,540
ideas for a lot of other types of applications is still enormous.

1312
01:06:07,540 --> 01:06:10,750
Um, and so if you go off and you do some of that I would love to hear back about it.

1313
01:06:10,750 --> 01:06:14,485
Um, uh, in my lab we think a lot about these other forms of applications.

1314
01:06:14,485 --> 01:06:17,080
And I think another really critical aspect of this is thinking

1315
01:06:17,080 --> 01:06:20,125
about when we do these RL agents, um,

1316
01:06:20,125 --> 01:06:21,610
how do we do it in sort of safe,

1317
01:06:21,610 --> 01:06:23,710
fair and accountable ways because typically,

1318
01:06:23,710 --> 01:06:25,060
these systems are going to be part of,

1319
01:06:25,060 --> 01:06:26,845
you know, a human in the loop system.

1320
01:06:26,845 --> 01:06:32,020
And so allowing the agents to sort of expose their reasoning and expose,

1321
01:06:32,020 --> 01:06:34,810
um, their limitations will be critical.

1322
01:06:34,810 --> 01:06:36,760
So the final thing is that,

1323
01:06:36,760 --> 01:06:38,695
um, it's really helpful to get you guys this feedback.

1324
01:06:38,695 --> 01:06:41,965
Um, it allows us to improve the class for future years, um,

1325
01:06:41,965 --> 01:06:44,260
either to make sure we continue to do things that you found

1326
01:06:44,260 --> 01:06:46,930
helpful or that we stopped doing things that you didn't find helpful.

1327
01:06:46,930 --> 01:06:49,375
So I'd really appreciate it if we could take about 10 minutes now

1328
01:06:49,375 --> 01:06:52,015
to go through the course evaluations, um,

1329
01:06:52,015 --> 01:06:55,105
and just feed it, uh, let us know what helped you learn,

1330
01:06:55,105 --> 01:06:56,800
what things we could do even better next year.

1331
01:06:56,800 --> 01:06:57,280
Thanks.

1332
01:06:57,280 --> 01:07:12,000
[APPLAUSE]

