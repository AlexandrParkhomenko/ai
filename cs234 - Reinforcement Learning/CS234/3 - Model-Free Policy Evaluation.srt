1
00:00:04,130 --> 00:00:07,320
So, what we're gonna do today is we're gonna start to

2
00:00:07,320 --> 00:00:10,125
talk about Model-Free Policy Evaluation.

3
00:00:10,125 --> 00:00:13,395
Um, so, what we were discussing last time

4
00:00:13,395 --> 00:00:17,100
is we started formally defining Markov processes,

5
00:00:17,100 --> 00:00:20,204
Markov reward processes and Markov decision processes,

6
00:00:20,204 --> 00:00:22,160
and we're looking at the relationship between

7
00:00:22,160 --> 00:00:23,960
these different forms of processes which are

8
00:00:23,960 --> 00:00:28,225
ways for us to model sequential decision-making under uncertainty problems.

9
00:00:28,225 --> 00:00:30,710
So, what we're thinking about last week was,

10
00:00:30,710 --> 00:00:33,560
well what if someone gives us a model of how the world works?

11
00:00:33,560 --> 00:00:35,330
So, we know what the reward model is,

12
00:00:35,330 --> 00:00:37,175
we know what the dynamics model is.

13
00:00:37,175 --> 00:00:40,585
It still might be hard to figure out what's the right thing to do.

14
00:00:40,585 --> 00:00:44,060
So, how do we take actions or how do we find a policy

15
00:00:44,060 --> 00:00:47,795
that can maximize our expected discounted sum of rewards?

16
00:00:47,795 --> 00:00:50,435
Um, if- even if we're given a model,

17
00:00:50,435 --> 00:00:54,205
then we still need to do some computation to try to identify that policy.

18
00:00:54,205 --> 00:00:57,380
So, what we're gonna get to very shortly is how do we do all of

19
00:00:57,380 --> 00:01:00,605
that when we don't get a model of the world in advance.

20
00:01:00,605 --> 00:01:02,390
But, let's just first a recap,

21
00:01:02,390 --> 00:01:06,140
um, sort of this general problem of policy evaluation.

22
00:01:06,140 --> 00:01:09,830
So, we heard a little bit about policy evaluation last time when we talked

23
00:01:09,830 --> 00:01:13,535
about policy evaluation as being one step inside a policy,

24
00:01:13,535 --> 00:01:18,410
um, iteration which alternated between policy evaluation and policy improvement.

25
00:01:18,410 --> 00:01:22,010
So, the idea in policy evaluation is somebody gives you

26
00:01:22,010 --> 00:01:26,050
a way to act and then you want to figure out how good that policy is.

27
00:01:26,050 --> 00:01:30,485
So, what is the expected discounted sum of rewards for that particular policy?

28
00:01:30,485 --> 00:01:34,610
And what we're gonna be talking about today is dynamic programming,

29
00:01:34,610 --> 00:01:38,115
Monte Carlo policy evaluation, and TD learning.

30
00:01:38,115 --> 00:01:40,100
As well as some of the ways that we should think

31
00:01:40,100 --> 00:01:43,200
about trying to compare between these algorithms.

32
00:01:43,220 --> 00:01:46,185
So, just as a brief recall, um,

33
00:01:46,185 --> 00:01:51,325
remember that last time we defined what a return is for Markov reward process.

34
00:01:51,325 --> 00:01:55,600
And a return for a Markov reward process that we defined by G_t was

35
00:01:55,600 --> 00:02:02,170
the discounted sum of rewards we get from that particular time point t onwards.

36
00:02:02,170 --> 00:02:07,590
So, we're gonna get an immediate reward of Rt and then after that,

37
00:02:07,590 --> 00:02:09,104
we're gonna get Gamma,

38
00:02:09,104 --> 00:02:10,749
where Gamma was our discount factor.

39
00:02:10,750 --> 00:02:15,560
And remember we're gonna assume that's gonna be somewhere between zero and one.

40
00:02:16,070 --> 00:02:21,280
And so, we're sort of weighing future awards generally less than the immediate rewards.

41
00:02:21,280 --> 00:02:25,945
The definition of a state value function was the expected return.

42
00:02:25,945 --> 00:02:28,820
And in general, the expected return is gonna be different from

43
00:02:28,820 --> 00:02:31,715
a particular return if the domain is stochastic,

44
00:02:31,715 --> 00:02:35,840
because the [NOISE] reward you might get when you try to drive to the airport

45
00:02:35,840 --> 00:02:37,460
today is likely gonna be different than

46
00:02:37,460 --> 00:02:39,650
the reward you get when you drive to the airport tomorrow,

47
00:02:39,650 --> 00:02:41,450
because traffic will be slightly different,

48
00:02:41,450 --> 00:02:43,565
and it's stochastic, varies over time.

49
00:02:43,565 --> 00:02:45,770
And so, you can compare whether, you know,

50
00:02:45,770 --> 00:02:49,850
on a particular day if it took you two hours to get to the airport versus on average,

51
00:02:49,850 --> 00:02:51,725
it might take you only an hour.

52
00:02:51,725 --> 00:02:57,605
We also defined the state action value function which was the expected reward,

53
00:02:57,605 --> 00:03:02,570
um, if we're following a particular policy Pi but we start off by taking an action a.

54
00:03:02,570 --> 00:03:05,325
[NOISE] So, we're saying if you're in a state s,

55
00:03:05,325 --> 00:03:07,950
you take an action a, and from then onwards,

56
00:03:07,950 --> 00:03:10,445
you follow this policy Pi that someone's given you.

57
00:03:10,445 --> 00:03:13,450
What is the expected discounted sum of rewards?

58
00:03:13,450 --> 00:03:16,280
And we saw that Q functions were useful because we

59
00:03:16,280 --> 00:03:18,860
can use them for things like policy improvement,

60
00:03:18,860 --> 00:03:21,350
because they allowed us to think about, well,

61
00:03:21,350 --> 00:03:25,500
if we wanna follow a policy later but we do something slightly different to start,

62
00:03:25,500 --> 00:03:30,540
can we see how that would help us improve in terms of the amount of reward we'd obtain?

63
00:03:31,450 --> 00:03:35,375
So, we talked about this somewhat but as a recap, um,

64
00:03:35,375 --> 00:03:38,900
we talked about doing dynamic programming for policy evaluation.

65
00:03:38,900 --> 00:03:43,745
So, dynamic programming was something we could apply when we know how the world works.

66
00:03:43,745 --> 00:03:48,715
So, this is when we're given the dynamics,

67
00:03:48,715 --> 00:03:53,450
and I'll use the word dynamics or transition model interchangeably in this course.

68
00:03:53,450 --> 00:03:59,220
So, if you're given the dynamics or the transition model p and the reward model,

69
00:04:01,730 --> 00:04:06,305
then you can do dynamic programming to evaluate how good a policy is.

70
00:04:06,305 --> 00:04:10,129
And so, the way we talked about doing this is that you initialize your value function,

71
00:04:10,129 --> 00:04:11,950
which you could think of generally as a vector.

72
00:04:11,950 --> 00:04:15,320
Right now, we're thinking about there being a finite set of states and actions.

73
00:04:15,320 --> 00:04:19,700
So, you can initialize your value function for this particular policy to be zero,

74
00:04:19,700 --> 00:04:22,805
um, and then you iterate until convergence.

75
00:04:22,805 --> 00:04:26,750
Where we say the value of a state is exactly equal to

76
00:04:26,750 --> 00:04:30,530
the immediate reward we get from following that policy in that state plus

77
00:04:30,530 --> 00:04:34,400
the discounted sum of future rewards we get [NOISE]

78
00:04:34,400 --> 00:04:40,385
using our transition model and the value that we've computed from a previous iteration.

79
00:04:40,385 --> 00:04:43,220
And we talked about defining convergence here.

80
00:04:43,220 --> 00:04:46,370
Convergence generally we're gonna use some sort of norm to compare

81
00:04:46,370 --> 00:04:49,685
the difference between our value functions on one iteration and next.

82
00:04:49,685 --> 00:04:51,920
So, we do things like this,

83
00:04:51,920 --> 00:04:57,200
V_Pi_k minus V_Pi at k minus one [NOISE].

84
00:04:57,200 --> 00:04:59,540
And wait for this to be smaller than some Epsilon.

85
00:04:59,540 --> 00:05:08,335
Okay. So, just as a reminder to what is this quantity that we're computing representing?

86
00:05:08,335 --> 00:05:11,605
Well, we can think of this quantity that we're computing, um,

87
00:05:11,605 --> 00:05:16,700
as being an exact value of the k horizon value of state s under that policy.

88
00:05:16,700 --> 00:05:18,430
So, on any particular iteration,

89
00:05:18,430 --> 00:05:21,640
it's as if we know exactly what value we would get if we could

90
00:05:21,640 --> 00:05:25,390
only act for a finite number of time steps like k time steps.

91
00:05:25,390 --> 00:05:28,060
Says, you know, how good would it be if you followed

92
00:05:28,060 --> 00:05:31,025
this particular policy for the next k time steps?

93
00:05:31,025 --> 00:05:34,195
Equivalently, you can think of it as an approximation

94
00:05:34,195 --> 00:05:37,250
of what the value would be if you acted forever.

95
00:05:37,250 --> 00:05:39,240
So, if k is really large,

96
00:05:39,240 --> 00:05:40,635
k is 20 billion,

97
00:05:40,635 --> 00:05:42,835
then it's probably gonna be a pretty good approximation

98
00:05:42,835 --> 00:05:45,310
to the value you'd get if you'd act forever.

99
00:05:45,310 --> 00:05:46,935
And if k is one,

100
00:05:46,935 --> 00:05:48,980
that's probably gonna be a pretty bad estimate.

101
00:05:48,980 --> 00:05:51,750
This will converge over time.

102
00:05:51,950 --> 00:05:55,550
So, I think it's useful to think about some of these things graphically as well.

103
00:05:55,550 --> 00:05:58,790
So, let's think about this as you're in a state s,

104
00:05:58,790 --> 00:06:03,115
which I'm denoting with that white circle at the top and then you can take an action.

105
00:06:03,115 --> 00:06:06,560
So, what dynamic programming is doing is it's computing

106
00:06:06,560 --> 00:06:10,070
an estimate of the V_Pi here at the top by saying,

107
00:06:10,070 --> 00:06:11,330
"What is the expectation,

108
00:06:11,330 --> 00:06:16,355
expectation over Pi of RT plus Gamma V_k minus one.

109
00:06:16,355 --> 00:06:21,770
And what's that expectation over it's gonna be the probability of s prime given s,

110
00:06:21,770 --> 00:06:24,650
Pi of s. Okay.

111
00:06:24,650 --> 00:06:26,120
So, how do we think about this graphically?

112
00:06:26,120 --> 00:06:27,230
Well, we started in this state,

113
00:06:27,230 --> 00:06:31,730
we take an action and then we think about the next states that we could reach.

114
00:06:31,730 --> 00:06:34,670
We're kind of again assuming that we're in a stochastic process.

115
00:06:34,670 --> 00:06:38,240
So, maybe, you know, sometimes the red light is on and sometimes the red light is off.

116
00:06:38,240 --> 00:06:40,580
So, depending on that, we are gonna be at a different next state,

117
00:06:40,580 --> 00:06:43,050
we're trying to drive to the airport.

118
00:06:43,090 --> 00:06:46,460
And then we can think about after we reach that state,

119
00:06:46,460 --> 00:06:48,170
then we can take some other actions.

120
00:06:48,170 --> 00:06:50,480
And in particular, we can take one action in this case because

121
00:06:50,480 --> 00:06:52,955
we're assuming we're fixing what the policy is.

122
00:06:52,955 --> 00:06:54,845
And then from those, that,

123
00:06:54,845 --> 00:06:57,520
those actions would lead us to other possible states.

124
00:06:57,520 --> 00:07:00,680
So, we can think of sort of drawing the tree of trajectories that we might

125
00:07:00,680 --> 00:07:04,130
reach if we started in a state and start following our policy,

126
00:07:04,130 --> 00:07:06,200
where whenever we get to make a choice,

127
00:07:06,200 --> 00:07:09,410
there's a single action we take because we're doing policy evaluation.

128
00:07:09,410 --> 00:07:12,020
And whenever there's sort of nature's choice,

129
00:07:12,020 --> 00:07:14,980
then there's like a distribution over next states that we might reach.

130
00:07:14,980 --> 00:07:17,360
So, you can think of these as the S-prime and

131
00:07:17,360 --> 00:07:21,210
the S double-primes kind of time is going down like this.

132
00:07:21,760 --> 00:07:24,065
So, this is sort of you know the,

133
00:07:24,065 --> 00:07:27,740
the potential futures that your agent could arise in.

134
00:07:27,740 --> 00:07:29,810
And I think it's useful to think about this graphically because

135
00:07:29,810 --> 00:07:32,210
then we can think about how those potential futures,

136
00:07:32,210 --> 00:07:35,150
um, how we can use those to compute what is the value,

137
00:07:35,150 --> 00:07:37,165
a difference of this policy.

138
00:07:37,165 --> 00:07:40,850
So, um, in what dynamic program what we're doing and

139
00:07:40,850 --> 00:07:43,970
in general when we're trying to compute the value of a policy is,

140
00:07:43,970 --> 00:07:49,075
we're gonna take an expectation over next states.

141
00:07:49,075 --> 00:07:55,730
So, the value is the expected discounted sum of future rewards if we follow this policy,

142
00:07:55,730 --> 00:07:59,560
and the expectation is exactly over these distributions of futures.

143
00:07:59,560 --> 00:08:01,655
So, whenever we see

144
00:08:01,655 --> 00:08:05,090
an action and then we think about all the next possible nodes we could get to,

145
00:08:05,090 --> 00:08:06,590
we want to take an expectation over

146
00:08:06,590 --> 00:08:10,480
those features and expectation over all the rewards we could get.

147
00:08:10,480 --> 00:08:12,515
So, that's what dynamic programming is

148
00:08:12,515 --> 00:08:14,480
or that's what we can think of this graph is doing.

149
00:08:14,480 --> 00:08:17,450
And when we think about what dynamic programming is doing,

150
00:08:17,450 --> 00:08:19,850
is it estimates this expectation over

151
00:08:19,850 --> 00:08:21,920
all those possible futures by

152
00:08:21,920 --> 00:08:26,495
bootstrapping and computing a one timestep expectation exactly.

153
00:08:26,495 --> 00:08:29,225
So, what does it do? Again, it says,

154
00:08:29,225 --> 00:08:35,745
"My V_Pi of s is exactly equal to r of s, Pi of s,

155
00:08:35,745 --> 00:08:41,049
my immediate reward plus Gamma sum over probability of s prime given

156
00:08:41,049 --> 00:08:47,275
s a V_Pi k minus one, the best part.

157
00:08:47,275 --> 00:08:51,860
So, it bootstraps, and we're using the word bootstraps there because it's not

158
00:08:51,860 --> 00:08:56,355
actually summing up all of these lower down potential rewards.

159
00:08:56,355 --> 00:08:57,900
It's saying, "I don't need to do that."

160
00:08:57,900 --> 00:09:01,010
Previously, I computed what it would be like if I started say

161
00:09:01,010 --> 00:09:04,445
in this state and continued on for the future.

162
00:09:04,445 --> 00:09:07,810
And so, I, now I already know what the value is at that state,

163
00:09:07,810 --> 00:09:09,710
and I'm gonna bootstrap and use that as

164
00:09:09,710 --> 00:09:12,455
a substitute for actually doing all that roll-out.

165
00:09:12,455 --> 00:09:17,290
And also here, because I know what the expected discounted or I know what the,

166
00:09:17,290 --> 00:09:18,950
um, sorry, the model is,

167
00:09:18,950 --> 00:09:21,785
that it can also just take a direct expectation over s prime.

168
00:09:21,785 --> 00:09:25,430
So, my question is, is there an implicit assumption here that

169
00:09:25,430 --> 00:09:27,860
the reward at a given state and

170
00:09:27,860 --> 00:09:31,475
thus the value function of evaluated states doesn't change over time.

171
00:09:31,475 --> 00:09:35,870
So, like because you're using it from the prior iteration?

172
00:09:35,870 --> 00:09:38,115
So, I think that question is saying, um,

173
00:09:38,115 --> 00:09:41,660
is there an explicit assumption here that the value doesn't change over time?

174
00:09:41,660 --> 00:09:44,210
Yes. The idea in this case is that the value that we're

175
00:09:44,210 --> 00:09:47,750
computing is for the infinite horizon case and therefore that it's stationary.

176
00:09:47,750 --> 00:09:49,355
It doesn't depend on the time step.

177
00:09:49,355 --> 00:09:52,650
From that way we're not gonna talk very much about the finite horizon case today,

178
00:09:52,650 --> 00:09:54,005
in that case it's different.

179
00:09:54,005 --> 00:09:56,510
In this situation, we're saying at all time

180
00:09:56,510 --> 00:09:59,330
steps you always have an infinite number more time steps to go.

181
00:09:59,330 --> 00:10:02,940
So, the value function itself is a stationary quantity.

182
00:10:05,600 --> 00:10:10,085
So, why is this an okay thing to do like we're bootstrapping?

183
00:10:10,085 --> 00:10:12,620
Um, the reason that this is okay is because we actually have

184
00:10:12,620 --> 00:10:15,955
an exact representation of this V_k minus one.

185
00:10:15,955 --> 00:10:19,190
You're not getting any approximation error of putting that in

186
00:10:19,190 --> 00:10:22,520
instead of sort of explicitly summing over lots of different histories.

187
00:10:22,520 --> 00:10:25,410
Sorry, lots of different future rewards.

188
00:10:26,450 --> 00:10:29,330
So, when we're doing dynamic programming

189
00:10:29,330 --> 00:10:31,775
the things to sort of think about here is if we know the model,

190
00:10:31,775 --> 00:10:33,890
then know dynamic model and know the reward model,

191
00:10:33,890 --> 00:10:36,725
that we can compute the immediate reward exactly.

192
00:10:36,725 --> 00:10:40,670
We can compute our expected sum over future states exactly,

193
00:10:40,670 --> 00:10:44,215
and then we substitute in instead of thinking about,

194
00:10:44,215 --> 00:10:48,590
we, instead of thinking about expanding this out as being a sum over rewards,

195
00:10:48,590 --> 00:10:53,240
we can just bootstrap and use our current estimate of V_k minus one.

196
00:10:53,240 --> 00:10:56,690
And the reason that I'm emphasizing this a lot is that when we start to

197
00:10:56,690 --> 00:10:59,720
look at these other methods like Monte Carlo methods and TD methods,

198
00:10:59,720 --> 00:11:01,280
they're not gonna do this anymore.

199
00:11:01,280 --> 00:11:04,900
They're gonna do other forms of approximation of trying to compute this tree.

200
00:11:04,900 --> 00:11:08,000
So, ultimately to compute the value of a policy,

201
00:11:08,000 --> 00:11:09,980
what we're essentially doing is we're thinking about

202
00:11:09,980 --> 00:11:14,860
all possible futures and what is the return we'd get under each of those futures.

203
00:11:14,860 --> 00:11:18,800
And we're trying to make it tractable to compute that particularly when we don't know how

204
00:11:18,800 --> 00:11:23,310
the world works and we don't have access to the dynamics model or the reward model.

205
00:11:26,600 --> 00:11:30,570
Okay. So, just to summarize dynamic programming,

206
00:11:30,570 --> 00:11:32,340
we should talk a little- a little bit about last time,

207
00:11:32,340 --> 00:11:34,755
but we didn't really talk about the bootstrapping aspect.

208
00:11:34,755 --> 00:11:39,720
Dynamic programming says the value of a policy is approximately equal to

209
00:11:39,720 --> 00:11:43,320
the expected next- the expectation over pi of

210
00:11:43,320 --> 00:11:48,030
immediate reward plus gamma times the previous value you computed requires a model,

211
00:11:48,030 --> 00:11:51,195
it bootstraps the future return using an estimate,

212
00:11:51,195 --> 00:11:53,280
using your V_k minus 1.

213
00:11:53,280 --> 00:11:56,115
And it requires the Markov assumption.

214
00:11:56,115 --> 00:11:59,985
And what- what I mean by that there is that, um,

215
00:11:59,985 --> 00:12:03,915
you're not thinking about all the past you got to reach a certain state.

216
00:12:03,915 --> 00:12:06,225
You're saying no matter how I got to that previous state,

217
00:12:06,225 --> 00:12:08,670
my value of that state is identical,

218
00:12:08,670 --> 00:12:11,070
um, and I can sort of assume that,

219
00:12:11,070 --> 00:12:14,860
and I can compute that singly based on the current observation.

220
00:12:15,260 --> 00:12:17,550
So, may I have any questions about this.

221
00:12:17,550 --> 00:12:20,310
So, right now we're mostly recap of last time, um,

222
00:12:20,310 --> 00:12:24,100
but sort of slightly pointing out some things that I didn't point out before.

223
00:12:25,490 --> 00:12:28,890
Okay. So, those things are useful now that we're gonna be

224
00:12:28,890 --> 00:12:32,040
talking about policy evaluation without a model.

225
00:12:32,040 --> 00:12:36,194
So, what we're going to talk about now is Monte Carlo policy evaluation

226
00:12:36,194 --> 00:12:39,840
which is something that we can apply when we don't know what the models are of the world,

227
00:12:39,840 --> 00:12:42,210
and we're gonna talk a little bit about how we can start to think

228
00:12:42,210 --> 00:12:44,595
about comparing these different forms of estimators,

229
00:12:44,595 --> 00:12:47,710
estimators of the value of a policy.

230
00:12:48,290 --> 00:12:52,185
So, in Monte Carlo policy evaluation,

231
00:12:52,185 --> 00:12:54,420
um, we can again think about the return.

232
00:12:54,420 --> 00:12:59,430
So, the returning and G_t are discounted sum of future rewards under a policy,

233
00:12:59,430 --> 00:13:04,275
and the value of a policy we can represent now is just,

234
00:13:04,275 --> 00:13:07,785
let's think about all the possible trajectories we could get,

235
00:13:07,785 --> 00:13:11,070
um, under our policy and what's average all their returns.

236
00:13:11,070 --> 00:13:14,265
So, we can again think about that tree we just constructed.

237
00:13:14,265 --> 00:13:18,690
Each of those different sort of branches would have had a particular reward,

238
00:13:18,690 --> 00:13:20,925
um, and then we're just going to get the average over all of them.

239
00:13:20,925 --> 00:13:22,590
So, it's a pretty simple idea.

240
00:13:22,590 --> 00:13:28,000
The idea is that the value is just equal to your expected return.

241
00:13:28,160 --> 00:13:31,245
And if all your trajectories are finite,

242
00:13:31,245 --> 00:13:34,570
you just can take a whole bunch of these and you average.

243
00:13:36,110 --> 00:13:41,310
So, the nice thing about Monte Carlo policy evaluation is it doesn't require you to

244
00:13:41,310 --> 00:13:45,735
have a sp- a specific model of the dynamics or reward.

245
00:13:45,735 --> 00:13:48,735
It just requires you to be able to sample from the environment.

246
00:13:48,735 --> 00:13:53,160
So, I don't need to know a particular like parametric model of how traffic works.

247
00:13:53,160 --> 00:13:55,500
All I have to do is drive from here to the airport, you know,

248
00:13:55,500 --> 00:13:58,530
hundreds of times, and then average how long it takes me.

249
00:13:58,530 --> 00:14:00,660
And if I'm always driving with the same policy,

250
00:14:00,660 --> 00:14:02,505
let's say I always take the highway,

251
00:14:02,505 --> 00:14:04,350
um, then if I do that,

252
00:14:04,350 --> 00:14:05,550
you know, 100 times, then I have

253
00:14:05,550 --> 00:14:07,740
a pretty good estimate of what is

254
00:14:07,740 --> 00:14:10,620
my expected time to get to the airport if I drive on the highway.

255
00:14:10,620 --> 00:14:12,660
Well that is my policy.

256
00:14:12,660 --> 00:14:14,909
So, it doesn't do bootstrapping,

257
00:14:14,909 --> 00:14:18,495
it doesn't try to maintain at this root V_k minus 1.

258
00:14:18,495 --> 00:14:20,880
Um, it's simply sums up all the rewards from each of

259
00:14:20,880 --> 00:14:23,730
your trajectories and then averages across those.

260
00:14:23,730 --> 00:14:26,625
It doesn't assume the state is Markov.

261
00:14:26,625 --> 00:14:29,640
Just averaging doesn't- there's

262
00:14:29,640 --> 00:14:33,075
no notion of the next state and whether or not that sufficient to,

263
00:14:33,075 --> 00:14:36,510
um, to summarize the future returns.

264
00:14:36,510 --> 00:14:41,475
An important thing is that it can only be applied to what are known as episodic MDPs.

265
00:14:41,475 --> 00:14:43,860
You act forever if there's no notion

266
00:14:43,860 --> 00:14:47,460
of- if this is sort of like averaging over your life this doesn't work,

267
00:14:47,460 --> 00:14:49,230
[LAUGHTER] because, you only get one.

268
00:14:49,230 --> 00:14:51,750
So, you need to have a process where you can

269
00:14:51,750 --> 00:14:55,365
repeatedly do this many times and the process will end each time.

270
00:14:55,365 --> 00:14:58,095
So, like driving to the airport might be really long,

271
00:14:58,095 --> 00:15:00,810
but you'll get there eventually and then you can try again tomorrow.

272
00:15:00,810 --> 00:15:03,480
So, this doesn't work for all processes like if

273
00:15:03,480 --> 00:15:06,015
you have a robot that's just going to be acting forever,

274
00:15:06,015 --> 00:15:09,790
can't do Monte Carlo policy evaluation.

275
00:15:09,980 --> 00:15:13,590
Okay. So, we also often do this in

276
00:15:13,590 --> 00:15:17,370
an incremental fashion which means that after we maintain a running estimate,

277
00:15:17,370 --> 00:15:20,865
after each episode, we update our current estimate as V_pi.

278
00:15:20,865 --> 00:15:23,160
And our hope is that as we get more and more data,

279
00:15:23,160 --> 00:15:26,025
this estimate will converge to the true value.

280
00:15:26,025 --> 00:15:29,445
So, let's look at, um, what the algorithm for this would be.

281
00:15:29,445 --> 00:15:31,410
So, one algorithm which is known as

282
00:15:31,410 --> 00:15:34,605
the First-Visit Monte Carlo on policy evaluation algorithm,

283
00:15:34,605 --> 00:15:37,290
as we start off and we assume that we haven't- N here

284
00:15:37,290 --> 00:15:40,810
is essentially the number of times we visited a state.

285
00:15:44,420 --> 00:15:47,265
So, we start off and this is zero.

286
00:15:47,265 --> 00:15:52,365
Also the return- the- or average return from starting in any state is also zero.

287
00:15:52,365 --> 00:15:55,590
So, we initialize say right now or we think that

288
00:15:55,590 --> 00:15:58,950
you know we get no reward from a state and we haven't visited any state.

289
00:15:58,950 --> 00:16:00,720
And then what we do is we loop.

290
00:16:00,720 --> 00:16:04,170
And for each loop we sample an episode which is we start in

291
00:16:04,170 --> 00:16:08,040
the starting state and we act until our process terminates.

292
00:16:08,040 --> 00:16:11,955
I start off at my house and I drive until I can get to the airport.

293
00:16:11,955 --> 00:16:14,340
And then I compute my return.

294
00:16:14,340 --> 00:16:18,135
So, I say okay, well maybe that took me two hours to get there.

295
00:16:18,135 --> 00:16:21,105
So, now my G-i is two hours.

296
00:16:21,105 --> 00:16:23,910
Um, but you've just compute your return and you compute it for

297
00:16:23,910 --> 00:16:26,760
every time step t inside of the episode.

298
00:16:26,760 --> 00:16:31,275
So, G_i,t here is defined from the t time step in that episode,

299
00:16:31,275 --> 00:16:34,890
what is the remaining reward you got from that time step onwards,

300
00:16:34,890 --> 00:16:38,595
and we'll instantiate this in our Mars Rover example in a second.

301
00:16:38,595 --> 00:16:42,030
And then for every state that you visited in that particular episode,

302
00:16:42,030 --> 00:16:44,805
for the first time you encountered a state,

303
00:16:44,805 --> 00:16:50,260
you look- you increment the counter and you update your total return.

304
00:16:51,860 --> 00:16:55,170
And you use, then you just take an average of those estimates to

305
00:16:55,170 --> 00:16:58,290
compute your current estimate of the value for the state.

306
00:16:58,290 --> 00:17:02,610
Now why you might be in the same state for more than one time step in an episode.

307
00:17:02,610 --> 00:17:06,270
Well let's say I get to the red light, let's say I've discredited my time steps.

308
00:17:06,270 --> 00:17:08,775
So, I look at my state every one minute.

309
00:17:08,775 --> 00:17:11,685
Well, I got to a red light and there was a traffic accident.

310
00:17:11,685 --> 00:17:14,265
So, on time step one I'm at the red light,

311
00:17:14,265 --> 00:17:15,780
time step two I'm on the red light,

312
00:17:15,780 --> 00:17:17,579
time step three I'm on the red light.

313
00:17:17,579 --> 00:17:20,924
And so you can be in the same state for multiple time steps during the episode.

314
00:17:20,925 --> 00:17:25,920
And what this is saying is that you only use the first time step you saw that state.

315
00:17:25,920 --> 00:17:30,915
And then you sum up the rewards you get til the end of that episode. Okay.

316
00:17:30,915 --> 00:17:33,435
We saw the state but in,

317
00:17:33,435 --> 00:17:36,090
I guess like different time steps and the same episode,

318
00:17:36,090 --> 00:17:40,010
we'd still be incremented twice because it's not- there's gonna be a gap between them?

319
00:17:40,010 --> 00:17:41,760
The question is, what happens if we,

320
00:17:41,760 --> 00:17:43,635
um, see the same state in the same episode?

321
00:17:43,635 --> 00:17:45,810
In first visit, you only use the first occurrence.

322
00:17:45,810 --> 00:17:47,700
So, you drop all other ones.

323
00:17:47,700 --> 00:17:50,010
So, the first time I got to my red light then I would

324
00:17:50,010 --> 00:17:52,455
sum up the future rewards till the end of the episode.

325
00:17:52,455 --> 00:17:56,775
If I happen to get to the same red light during the same episode, I ignore that data.

326
00:17:56,775 --> 00:17:59,860
We'll see a different way of doing that in a second.

327
00:18:00,140 --> 00:18:04,425
Okay. So, how do we estimate whether or not this is a good thing to do.

328
00:18:04,425 --> 00:18:09,120
How do we evaluate whether or not this particular- this is an estimate.

329
00:18:09,120 --> 00:18:12,630
It's likely wrong at least at the beginning where we don't have much data.

330
00:18:12,630 --> 00:18:14,130
So, how do we understand whether or not

331
00:18:14,130 --> 00:18:15,930
this estimate is good and how are we going to compare

332
00:18:15,930 --> 00:18:19,650
all of the estimators and these algorithms that we're going to be talking about today.

333
00:18:19,650 --> 00:18:24,015
So, um, actually just raise your hand because I'm curious.

334
00:18:24,015 --> 00:18:28,830
Um, who here has sort of formally seen definitions of bias and variance in other classes.

335
00:18:28,830 --> 00:18:31,230
Okay. Most people but not quite everybody.

336
00:18:31,230 --> 00:18:33,150
So, just as a quick recap, um,

337
00:18:33,150 --> 00:18:37,200
let's think about sort of having a statistical model that is parameterized by theta,

338
00:18:37,200 --> 00:18:42,960
um, and that we also have some distribution over some observed data p of x given theta.

339
00:18:42,960 --> 00:18:47,670
So, we want to have a statistic theta hat which is a function.

340
00:18:47,670 --> 00:18:54,375
So, theta hat is a function of the observed data and it provides an estimate of theta.

341
00:18:54,375 --> 00:18:57,000
So, in our case, we're going to have this value,

342
00:18:57,000 --> 00:18:58,875
this estimate of the value we're computing.

343
00:18:58,875 --> 00:19:01,920
This is a function of our episodes and this is an estimate of the

344
00:19:01,920 --> 00:19:06,360
true discounted expected rewards of following this policy.

345
00:19:06,360 --> 00:19:10,890
So, the definition of a bias of an estimator is to compare what is

346
00:19:10,890 --> 00:19:16,390
the expected value of our statistic versus the true value,

347
00:19:16,820 --> 00:19:19,815
for any set of data.

348
00:19:19,815 --> 00:19:22,185
So, this would say,

349
00:19:22,185 --> 00:19:23,655
if I compute, you know,

350
00:19:23,655 --> 00:19:25,200
the expected amount of time for me to get to

351
00:19:25,200 --> 00:19:27,870
the airport based on trying to drive there three times.

352
00:19:27,870 --> 00:19:30,660
Does the algorithm that I just showed you is that unbiased?

353
00:19:30,660 --> 00:19:35,745
On average is that the same is the true expected time for me to get to the airport.

354
00:19:35,745 --> 00:19:39,150
The definition of a variance of an estimator compares

355
00:19:39,150 --> 00:19:42,780
my statistic to its expected value squared.

356
00:19:42,780 --> 00:19:45,495
Expected over the, er, the, um,

357
00:19:45,495 --> 00:19:47,190
the type of data I could get under

358
00:19:47,190 --> 00:19:51,075
the true parameter and the mean squared error combines these two.

359
00:19:51,075 --> 00:19:54,000
Mean squared error is normally what we care about.

360
00:19:54,000 --> 00:19:57,330
Normally, we ultimately care about sort of how far away is

361
00:19:57,330 --> 00:20:00,825
our estimate of the quantity we care about versus the true quantity?

362
00:20:00,825 --> 00:20:03,255
And that's the sum of its bias and its variance.

363
00:20:03,255 --> 00:20:04,890
And generally, different algorithms and

364
00:20:04,890 --> 00:20:09,010
different estimators will have different trade offs between bias and variance.

365
00:20:09,610 --> 00:20:12,980
Okay. So, if we go back to our First-Visit Monte Carlo

366
00:20:12,980 --> 00:20:15,920
algorithm the V_pi estimator that we use there is

367
00:20:15,920 --> 00:20:21,635
an unbiased estimator of the true expected discounted sum of rewards from our policy.

368
00:20:21,635 --> 00:20:24,975
It's just a simple average, um, and it's unbiased.

369
00:20:24,975 --> 00:20:26,490
And by the law of large numbers,

370
00:20:26,490 --> 00:20:27,540
as you get more and more data,

371
00:20:27,540 --> 00:20:29,190
it converges to the true value.

372
00:20:29,190 --> 00:20:32,265
So, it's also what is known as as consistent.

373
00:20:32,265 --> 00:20:40,030
Consistent means that it converges to the true value as the- as data goes to infinity.

374
00:20:40,160 --> 00:20:42,840
So, this is reasonable, um,

375
00:20:42,840 --> 00:20:44,955
but it might not be very efficient.

376
00:20:44,955 --> 00:20:46,920
So, ah, as we just talked about,

377
00:20:46,920 --> 00:20:48,270
you might be in the same state, you might be at

378
00:20:48,270 --> 00:20:50,280
the same stoplight for many, many time steps.

379
00:20:50,280 --> 00:20:54,790
Um, and you're only going to use the first state in an episode to update.

380
00:20:54,890 --> 00:20:57,915
So, every visit at Monte Carlo,

381
00:20:57,915 --> 00:21:01,095
simply says well every time you visit a state during the episode,

382
00:21:01,095 --> 00:21:03,240
look at how much reward you got from that state

383
00:21:03,240 --> 00:21:05,595
till the end and average over all of those.

384
00:21:05,595 --> 00:21:07,815
So, essentially every time you reach a state,

385
00:21:07,815 --> 00:21:10,320
you always look at the sum of discounted rewards

386
00:21:10,320 --> 00:21:13,455
from there to the end of the episode and you average all of that.

387
00:21:13,455 --> 00:21:16,125
Which is generally going to be more data efficient.

388
00:21:16,125 --> 00:21:20,520
Bias definition, I guess I'm just a little confused how we would get biased,

389
00:21:20,520 --> 00:21:22,500
even if we don't actually know theta.

390
00:21:22,500 --> 00:21:24,480
How we compute bias. [NOISE]

391
00:21:24,480 --> 00:21:25,865
Yeah, given that we don't know theta.

392
00:21:25,865 --> 00:21:27,770
It's a great- the question is how do you compute bias?

393
00:21:27,770 --> 00:21:29,000
Yes, if you, uh,

394
00:21:29,000 --> 00:21:32,330
if you can compute bias exactly that normally means you know what theta is,

395
00:21:32,330 --> 00:21:34,475
in which case why are you doing an estimator?

396
00:21:34,475 --> 00:21:36,515
Generally, we do not know what bias is,

397
00:21:36,515 --> 00:21:39,380
um we can often bound it.

398
00:21:39,380 --> 00:21:42,440
So, often using things like concentration inequalities we can,

399
00:21:42,440 --> 00:21:45,365
um, well concentration qualities are more for variance.

400
00:21:45,365 --> 00:21:48,125
Often, um, we don't know exactly what the bias is,

401
00:21:48,125 --> 00:21:50,075
unless you know what the ground truth is.

402
00:21:50,075 --> 00:21:54,085
And there are different ways for us to get estimates of bias in practice.

403
00:21:54,085 --> 00:21:57,165
So, as you compare across different forms of parametric models,

404
00:21:57,165 --> 00:22:00,350
um, sometimes you can do is structural risk, ah, ah,

405
00:22:00,350 --> 00:22:02,880
structural risk maximization and things like that to try to get

406
00:22:02,880 --> 00:22:06,840
sort of a quantity of how you compare your estimator and your model class.

407
00:22:06,840 --> 00:22:08,780
I'm not going to go very much

408
00:22:08,780 --> 00:22:11,760
into that here but I'm happy to talk about it in office hours.

409
00:22:13,200 --> 00:22:16,180
So, in every visit Monte Carlo,

410
00:22:16,180 --> 00:22:18,565
we're just gonna update it every single time.

411
00:22:18,565 --> 00:22:20,665
And that's gonna give us another estimator.

412
00:22:20,665 --> 00:22:23,260
And note that that's gonna give us generally a lot more counts.

413
00:22:23,260 --> 00:22:26,080
Because every time you see a state,

414
00:22:26,080 --> 00:22:31,075
you can update the counts. But it's biased.

415
00:22:31,075 --> 00:22:34,510
So, you can show that this is a biased estimator of

416
00:22:34,510 --> 00:22:39,160
V_pi. May have intuition of why it might be biased.

417
00:22:39,160 --> 00:22:41,800
So, in the first case for those of you that have seen this

418
00:22:41,800 --> 00:22:46,195
before or not necessarily this particularly but seen this sort of analysis.

419
00:22:46,195 --> 00:22:48,400
First visit Monte Carlo,

420
00:22:48,400 --> 00:22:53,060
is you're getting IID estimates of a state,

421
00:22:53,760 --> 00:22:55,810
of a state's return right?

422
00:22:55,810 --> 00:22:57,610
Because you only take that, um,

423
00:22:57,610 --> 00:23:00,220
each episode is, is

424
00:23:00,220 --> 00:23:03,610
IID because you're starting at a certain state and you're estimating from there.

425
00:23:03,610 --> 00:23:08,360
Ah, and you only use the return for the first time you saw that state.

426
00:23:09,180 --> 00:23:13,840
If you see a state multiple times in the same episode,

427
00:23:13,840 --> 00:23:17,245
are their returns correlated or uncorrelated?

428
00:23:17,245 --> 00:23:22,405
Correlated. Okay. So, your data is no longer IID.

429
00:23:22,405 --> 00:23:27,580
So, that's sort of the intuition for why when you mod- move to every visit Monte Carlo,

430
00:23:27,580 --> 00:23:31,645
your estimator can be biased 'cause you're not averaging over IID variables anymore.

431
00:23:31,645 --> 00:23:38,350
Is it biased for an obvious reasons to inspectors paradox? [inaudible]

432
00:23:38,350 --> 00:23:39,580
I don't know. That's a good question.

433
00:23:39,580 --> 00:23:42,220
I'm happy to look at it and return.

434
00:23:42,220 --> 00:23:46,660
However, the nice thing about this is that it is a consistent estimator.

435
00:23:46,660 --> 00:23:48,700
So, as you get more and more data,

436
00:23:48,700 --> 00:23:52,375
it will converge to the true estimate.

437
00:23:52,375 --> 00:23:55,810
And empirically, it often has way lower variance.

438
00:23:55,810 --> 00:23:58,060
And intuitively, it should have way lower variance.

439
00:23:58,060 --> 00:24:00,460
We're averaging over a lot more data points,

440
00:24:00,460 --> 00:24:02,110
uh, typically in the same.

441
00:24:02,110 --> 00:24:04,795
Now, you know, if you only visit one-

442
00:24:04,795 --> 00:24:07,690
if you- if you're very unlikely to repeatedly visit the same state,

443
00:24:07,690 --> 00:24:12,295
these two estimators are generally very close to the same thing in an episode.

444
00:24:12,295 --> 00:24:15,595
Because you're not gonna have multiple visits to the same state.

445
00:24:15,595 --> 00:24:17,500
But in some cases you're gonna visit the same state a

446
00:24:17,500 --> 00:24:19,330
lot of times and you get a lot more data

447
00:24:19,330 --> 00:24:23,365
and these estimators will generally be much better if you use every visit,

448
00:24:23,365 --> 00:24:25,705
but it's biased. So, there's this trade-off.

449
00:24:25,705 --> 00:24:28,490
Empirically, this is often much better.

450
00:24:29,120 --> 00:24:31,890
Now, of course in practice often instead

451
00:24:31,890 --> 00:24:35,220
of the- often you may wanna do this incrementally.

452
00:24:35,220 --> 00:24:38,250
You may just want to kind of keep track of a running mean and then

453
00:24:38,250 --> 00:24:42,260
you keep track of your running mean and update your counts sort of incrementally.

454
00:24:42,260 --> 00:24:44,980
And you can do that if also as you visit you don't

455
00:24:44,980 --> 00:24:47,275
have to wait until the end lessons- oh, that's wrong.

456
00:24:47,275 --> 00:24:49,810
You do have to wait till the end because you always have to wait till you

457
00:24:49,810 --> 00:24:54,385
get the full return before you can update. Yeah, in the back.

458
00:24:54,385 --> 00:24:58,540
So, a question on that, if you could like- if you condition on the fact that you have

459
00:24:58,540 --> 00:25:03,250
the same number of estimates approximately in each of the states,

460
00:25:03,250 --> 00:25:08,905
would then the two be more or less equivalent but the other one would be less biased.

461
00:25:08,905 --> 00:25:10,720
For example, if you did I guess there is no way you could

462
00:25:10,720 --> 00:25:12,880
have for example a same number of episodes,

463
00:25:12,880 --> 00:25:17,920
ah, the same number of count in each state with the first visit approximation.

464
00:25:17,920 --> 00:25:19,195
But if you did have that,

465
00:25:19,195 --> 00:25:21,565
would you imagine that the episode would be lower in that case?

466
00:25:21,565 --> 00:25:24,040
I would- expressions about if you have

467
00:25:24,040 --> 00:25:28,060
the same number of counts to a state across the two algorithms.

468
00:25:28,060 --> 00:25:29,770
And in terms of the episodes,

469
00:25:29,770 --> 00:25:31,795
you couldn't have that be the case

470
00:25:31,795 --> 00:25:35,965
unless- so they'd need to be identical if you only visit one state,

471
00:25:35,965 --> 00:25:39,910
um, once in an episode and then they'd be totally identical.

472
00:25:39,910 --> 00:25:41,920
If it's not the case, if you visit, um,

473
00:25:41,920 --> 00:25:43,300
a state multiple times in,

474
00:25:43,300 --> 00:25:45,370
in one episode, then, uh,

475
00:25:45,370 --> 00:25:47,365
by the time you get to the same counts,

476
00:25:47,365 --> 00:25:49,420
the one for the single visit would be

477
00:25:49,420 --> 00:25:52,075
better 'cause it's unbiased and it would have basically the same variance.

478
00:25:52,075 --> 00:25:56,660
Any other questions about that?

479
00:25:56,940 --> 00:26:02,125
Cool. Um, so, incremental Monte Carlo, um,

480
00:26:02,125 --> 00:26:05,950
on policy evaluation is essentially the same as before except where you can

481
00:26:05,950 --> 00:26:09,955
just sort of slowly move your running average for each of the states.

482
00:26:09,955 --> 00:26:11,980
And the important thing about this is that,

483
00:26:11,980 --> 00:26:14,154
um, as you slowly move your estimator,

484
00:26:14,154 --> 00:26:16,270
if you set your alpha to be 1 over Ns,

485
00:26:16,270 --> 00:26:19,345
it's identical to every visit Monte Carlo.

486
00:26:19,345 --> 00:26:22,240
Essentially, you're just exactly computing the average.

487
00:26:22,240 --> 00:26:24,025
Um, but you don't have to do that.

488
00:26:24,025 --> 00:26:29,665
So, you can skew it so that you're running average is more weighted towards recent data.

489
00:26:29,665 --> 00:26:32,290
And the reason why you might wanna do that is

490
00:26:32,290 --> 00:26:35,990
because if your real domain is non-stationary.

491
00:26:36,470 --> 00:26:39,075
We have a guess of where,

492
00:26:39,075 --> 00:26:41,710
where domains might be non-stationary.

493
00:26:44,520 --> 00:26:46,600
It's kind of an advanced topic.

494
00:26:46,600 --> 00:26:49,240
We're not gonna really talk about non-stationary domains for most of

495
00:26:49,240 --> 00:26:52,330
this class though in reality, they're incredibly important.

496
00:26:52,330 --> 00:26:56,020
Um, I don't know if your mechanical parts are breaking down or something's off.

497
00:26:56,020 --> 00:26:58,480
Example of like if you're in a manufacturing process and

498
00:26:58,480 --> 00:27:00,670
your parts are changing- are breaking down over time.

499
00:27:00,670 --> 00:27:03,985
So, your dynamics model is actually changing over time.

500
00:27:03,985 --> 00:27:07,210
Then you don't want to reuse your old data because

501
00:27:07,210 --> 00:27:10,270
you're- actually your MDP has changed over time.

502
00:27:10,270 --> 00:27:12,700
So, this is one of the reasons often empirically like when

503
00:27:12,700 --> 00:27:15,295
people train recommender systems and things like that,

504
00:27:15,295 --> 00:27:18,310
you know, the, the news all these things are non-stationary.

505
00:27:18,310 --> 00:27:21,775
And so people often retrain them a lot to deal with this non-stationarity process.

506
00:27:21,775 --> 00:27:23,410
Do I see a question on the back?

507
00:27:23,410 --> 00:27:29,545
Okay. Yeah. So, empirically that's often really helpful for non-stationary domains,

508
00:27:29,545 --> 00:27:32,725
but if it's non-stationary there's all- there's a bunch of different concerns.

509
00:27:32,725 --> 00:27:34,975
So, we're going to mostly ignore that for now.

510
00:27:34,975 --> 00:27:37,390
Okay. So, let's just check our understanding for a second.

511
00:27:37,390 --> 00:27:39,430
For Monte Carlo, for on policy evaluation.

512
00:27:39,430 --> 00:27:41,380
Let's go back to our Mars rover domain.

513
00:27:41,380 --> 00:27:43,060
So, in our Mars rover,

514
00:27:43,060 --> 00:27:46,060
we had these seven states. Our rover dropped down.

515
00:27:46,060 --> 00:27:47,229
It was gonna explore,

516
00:27:47,229 --> 00:27:49,704
a reward is in state S_1,

517
00:27:49,704 --> 00:27:53,860
one and state S_7 it's plus 10 everywhere else at zero.

518
00:27:53,860 --> 00:27:57,775
And our policy is gonna be A_1 for all states.

519
00:27:57,775 --> 00:28:00,385
And now imagine we don't know what the dynamics model is.

520
00:28:00,385 --> 00:28:02,860
So, we're just gonna observe trajectories.

521
00:28:02,860 --> 00:28:06,445
And if you get to either state one or state seven,

522
00:28:06,445 --> 00:28:09,265
the next action you take terminates the reward.

523
00:28:09,265 --> 00:28:11,920
I don't know. Maybe it falls off a cliff or something like that.

524
00:28:11,920 --> 00:28:14,200
But whenever you get to S_7 or S_1,

525
00:28:14,200 --> 00:28:17,470
then the next action you take so you get whatever reward.

526
00:28:17,470 --> 00:28:22,075
You either get the one or you get the 10 and then your process terminates.

527
00:28:22,075 --> 00:28:25,840
So, let's imagine a trajectory under this policy would be you start in S_3.

528
00:28:25,840 --> 00:28:27,745
You go to action- take action A_1,

529
00:28:27,745 --> 00:28:30,565
you get a reward of zero. This is for reward.

530
00:28:30,565 --> 00:28:32,785
Then you transition to state S_2,

531
00:28:32,785 --> 00:28:35,245
you take an action of A_1, you get a zero.

532
00:28:35,245 --> 00:28:37,480
You stay in the same state.

533
00:28:37,480 --> 00:28:39,130
So, you stay in S_2 again.

534
00:28:39,130 --> 00:28:43,240
Take action A_1, you get another reward of zero and then you reach state S_1,

535
00:28:43,240 --> 00:28:44,320
take an action A_1,

536
00:28:44,320 --> 00:28:46,165
you get a 1 and then it terminates.

537
00:28:46,165 --> 00:28:49,480
So, it's one experience of your Mars rover's life.

538
00:28:49,480 --> 00:28:51,580
So, in this case,

539
00:28:51,580 --> 00:28:53,320
how about we just take a minute or two,

540
00:28:53,320 --> 00:28:55,120
feel free to talk to a neighbor and compute what

541
00:28:55,120 --> 00:28:57,070
is the first visit Monte Carlo estimate of

542
00:28:57,070 --> 00:29:02,270
the value of each state and what is the every visit Monte Carlo estimate of state S_2?

543
00:29:02,370 --> 00:29:08,290
Then I put the algorithm for both first visit at every visit above just depends on

544
00:29:08,290 --> 00:29:10,690
whether you update the state only once for

545
00:29:10,690 --> 00:29:13,450
this episode or whether you can potentially update it multiple times.

546
00:29:13,450 --> 00:29:20,830
[NOISE] You may ask the question too if we have not seen it yet what is the value we use.

547
00:29:20,830 --> 00:29:24,805
So, the value you can also say that you initialize V_Pi of S

548
00:29:24,805 --> 00:29:28,720
equal to zero for all S if you haven't seen it yet.

549
00:29:28,720 --> 00:29:29,830
[NOISE] All right.

550
00:29:29,830 --> 00:29:33,620
Raise your hand if you'd like a little bit more time otherwise we'll go ahead.

551
00:29:33,840 --> 00:29:37,840
Okay. So what- someone wanna to share what they and maybe somebody nearby them

552
00:29:37,840 --> 00:29:41,920
thought was the first visit Monte Carlo estimate of V for every state.

553
00:29:41,920 --> 00:29:45,850
I think the first- this estimates is

554
00:29:45,850 --> 00:29:51,500
really one for every single state except for the last one that's sent.

555
00:29:51,570 --> 00:29:55,960
Which states? We've only updated a few of them so far.

556
00:29:55,960 --> 00:29:59,155
Which why don't you give me the full vector.

557
00:29:59,155 --> 00:30:01,060
Like okay we'll just start here.

558
00:30:01,060 --> 00:30:03,445
So, V of S_1 is what?

559
00:30:03,445 --> 00:30:05,125
Is one.

560
00:30:05,125 --> 00:30:06,745
Okay. And V of S_2?

561
00:30:06,745 --> 00:30:08,335
Is also one.

562
00:30:08,335 --> 00:30:09,745
And V of S_3?

563
00:30:09,745 --> 00:30:10,870
Is also one.

564
00:30:10,870 --> 00:30:12,160
And V of S_4?

565
00:30:12,160 --> 00:30:14,200
Also one. [NOISE].

566
00:30:14,200 --> 00:30:15,220
Anybody disagree.

567
00:30:15,220 --> 00:30:15,790
Zero.

568
00:30:15,790 --> 00:30:23,260
Zero. Okay and V of S_5? Zero. And V of S_6?

569
00:30:23,260 --> 00:30:24,925
Zero.

570
00:30:24,925 --> 00:30:28,660
And V of S_7? [OVERLAPPING] Yeah.

571
00:30:28,660 --> 00:30:33,370
So, we only get to update in this one that the states we've actually visited.

572
00:30:33,370 --> 00:30:35,740
Okay. So, here it's one,

573
00:30:35,740 --> 00:30:37,420
one, one. Zero, zero, zero, zero.

574
00:30:37,420 --> 00:30:42,370
Now what about for every visit the Monte Carlo estimate of just S_2.

575
00:30:42,370 --> 00:30:45,340
So, I picked only S_2 'cause that's the only state we visit twice.

576
00:30:45,340 --> 00:30:47,780
What's its, what's its estimate?

577
00:30:52,380 --> 00:30:59,055
Well, we increment. Yeah.

578
00:30:59,055 --> 00:31:00,750
Is it still gonna be one?

579
00:31:00,750 --> 00:31:03,190
Yeah, yes it is and why?

580
00:31:03,440 --> 00:31:10,995
Because incremental also we have Ns is two at the end of it but Gs is also two.

581
00:31:10,995 --> 00:31:13,235
So, the increment both twice.

582
00:31:13,235 --> 00:31:16,570
Exactly. So, the return from both times when you started in

583
00:31:16,570 --> 00:31:20,875
S_2 and got an added up till the end of the episode was one in both cases.

584
00:31:20,875 --> 00:31:26,425
So, it was one twice and then you average over that so it's still one. Yeah.

585
00:31:26,425 --> 00:31:30,280
Is the reason that they're all one because gamma's one?

586
00:31:30,280 --> 00:31:31,990
'Cause like shouldn't there be some gamma terms in there.

587
00:31:31,990 --> 00:31:34,885
Oh, good question. So, here we've assumed gamma equals one,

588
00:31:34,885 --> 00:31:39,310
otherwise there would be- there'd be a gamma multiplied into some of those two.

589
00:31:39,310 --> 00:31:40,825
Yeah, good question.

590
00:31:40,825 --> 00:31:43,300
I chosen gamma equal to one just to make the math a little bit easier.

591
00:31:43,300 --> 00:31:46,420
Otherwise, it'd be a gamma factor tpo. Okay great.

592
00:31:46,420 --> 00:31:48,670
So, you know, the, the second question is a little bit of

593
00:31:48,670 --> 00:31:51,325
a red herring because in this case it's exactly the same.

594
00:31:51,325 --> 00:31:54,180
But if the return had been different from S_2, um,

595
00:31:54,180 --> 00:31:56,430
like let's say there was a penalty for being in a state,

596
00:31:56,430 --> 00:31:58,215
then they could have had different returns

597
00:31:58,215 --> 00:32:00,290
and then we would have gotten something different there.

598
00:32:00,290 --> 00:32:04,975
Okay. So, Monte Carlo in this case updated- we had to wait till the end of the episode,

599
00:32:04,975 --> 00:32:07,525
but when we updated it till the end of the episode,

600
00:32:07,525 --> 00:32:11,540
we updated S_3, S_2, and S_1.

601
00:32:12,960 --> 00:32:16,060
So, what is Monte Carlo doing when we think about

602
00:32:16,060 --> 00:32:18,550
how we're averaging over possible futures.

603
00:32:18,550 --> 00:32:20,755
So, what Monte Carlo is doing, um,

604
00:32:20,755 --> 00:32:23,230
I've put this sort of incremental version here which you could

605
00:32:23,230 --> 00:32:26,260
use for non-stationary cases but you can think of it in the other way too.

606
00:32:26,260 --> 00:32:30,970
Um, so, and remember if you want this just to be equal to every visit,

607
00:32:30,970 --> 00:32:35,830
you're just plugging in 1 over N of S here for alpha.

608
00:32:35,830 --> 00:32:38,530
So, this is what Monte Carlo Evaluation is doing

609
00:32:38,530 --> 00:32:41,815
is it's just averaging over these returns.

610
00:32:41,815 --> 00:32:46,285
So, what we're doing is if we think about sort of what our tree is doing,

611
00:32:46,285 --> 00:32:48,190
in our case our tree is gonna be finite.

612
00:32:48,190 --> 00:32:52,045
We're gonna assume that each of these sort of branches eventually terminate.

613
00:32:52,045 --> 00:32:55,600
They have to because we can only evaluate a return once we reach it.

614
00:32:55,600 --> 00:32:59,710
So, at some point like here when we got to state S_1 or

615
00:32:59,710 --> 00:33:04,375
S_7 in our Mars example, the process terminated.

616
00:33:04,375 --> 00:33:07,585
And so what does Monte Carlo policy evaluation do?

617
00:33:07,585 --> 00:33:12,879
It approximates averaging over all possible futures by summing up one,

618
00:33:12,879 --> 00:33:15,130
uh, trajectory through the tree.

619
00:33:15,130 --> 00:33:19,945
So, it samples the return all the way down till it gets to a terminal state.

620
00:33:19,945 --> 00:33:22,285
It adds up all of the rewards along the way.

621
00:33:22,285 --> 00:33:24,130
So, like reward, reward, reward.

622
00:33:24,130 --> 00:33:26,485
Well, I'll be more careful than that.

623
00:33:26,485 --> 00:33:29,230
Reward, reward.

624
00:33:29,230 --> 00:33:31,645
Here you get a reward for each state action pair.

625
00:33:31,645 --> 00:33:33,940
So, you sum up all the rewards in this case.

626
00:33:33,940 --> 00:33:37,945
Um, and that is its sample, um, of the value.

627
00:33:37,945 --> 00:33:41,245
So, notice it's not doing any, um, er,

628
00:33:41,245 --> 00:33:43,975
the way it's gonna get into the expectation over states,

629
00:33:43,975 --> 00:33:45,865
is by averaging and across trajectories.

630
00:33:45,865 --> 00:33:48,520
It's not explicitly looking at the probability of

631
00:33:48,520 --> 00:33:52,165
next state given S and A and it's not bootstrapping.

632
00:33:52,165 --> 00:33:54,700
It is only able to update,

633
00:33:54,700 --> 00:33:57,920
when you get all the way out and see the full return.

634
00:33:59,430 --> 00:34:02,920
So, so, this is it samples.

635
00:34:02,920 --> 00:34:06,025
It doesn't use an explicit representation of a dynamics model,

636
00:34:06,025 --> 00:34:10,150
and it does not bootstrap because there's no notion of VK minus 1 here.

637
00:34:10,150 --> 00:34:13,570
It's only summing up a- all of the returns. Questions? Scotty.

638
00:34:13,570 --> 00:34:19,945
[inaudible] policy evaluation like this would do a very poor job in rare occurrences?

639
00:34:19,945 --> 00:34:21,730
Well, it's interesting. Question is,

640
00:34:21,730 --> 00:34:25,330
is it fair to say that this would do a really bad job in very rare occurrences?

641
00:34:25,330 --> 00:34:28,675
It's intriguing. They're very high variance estimators.

642
00:34:28,675 --> 00:34:30,850
So if you're- Monte Carlo,

643
00:34:30,850 --> 00:34:33,159
in general, you essentially just like rolling out futures, right?

644
00:34:33,159 --> 00:34:37,299
And often you need a lot of possible futures until you can get a good expectation.

645
00:34:37,300 --> 00:34:40,900
On the other hand, for things like AlphaGo which is one of

646
00:34:40,900 --> 00:34:45,145
the algorithms that was used to solve the board game Go, they use Monte Carlo.

647
00:34:45,145 --> 00:34:48,010
So, you know, I think, um,

648
00:34:48,010 --> 00:34:49,750
you wanna be careful in how you're doing some of

649
00:34:49,750 --> 00:34:51,880
this roll out when you start to get into control.

650
00:34:51,880 --> 00:34:54,850
And when you start to- because then you get to pick the actions, um,

651
00:34:54,850 --> 00:34:56,170
and you often kind of want to play between,

652
00:34:56,170 --> 00:34:58,930
but it- it's not horrible even if there's rare events.

653
00:34:58,930 --> 00:35:02,485
Um, er, but if you have other information you can use, it's often good.

654
00:35:02,485 --> 00:35:05,005
It depends w-what your other options are.

655
00:35:05,005 --> 00:35:07,600
So, generally this is a pretty high variance estimator.

656
00:35:07,600 --> 00:35:08,665
You can require a lot of data,

657
00:35:08,665 --> 00:35:11,170
and it requires an episodic setting because you

658
00:35:11,170 --> 00:35:14,260
can't do this if you're acting forever because there is no way to terminate.

659
00:35:14,260 --> 00:35:16,690
So, you have to be able to tell processes to terminate.

660
00:35:16,690 --> 00:35:20,470
So, in the DP Policy Evaluation we had the gamma factors,

661
00:35:20,470 --> 00:35:23,740
because we wanted to take care of the cases

662
00:35:23,740 --> 00:35:28,240
where state were seen in-between that started with a probability equals to one.

663
00:35:28,240 --> 00:35:30,505
But in this case, um,

664
00:35:30,505 --> 00:35:33,220
if we had such a case that would never terminate,

665
00:35:33,220 --> 00:35:35,170
right, because the episode would never end.

666
00:35:35,170 --> 00:35:40,120
So, technically, do we still need a gamma factor to evaluate policy equation,

667
00:35:40,120 --> 00:35:41,740
uh, policy evaluation on?

668
00:35:41,740 --> 00:35:43,240
The question was about,

669
00:35:43,240 --> 00:35:45,400
do we still need a gamma factor in these cases,

670
00:35:45,400 --> 00:35:49,120
and what about cases where you could have self-loops or small loops in your process?

671
00:35:49,120 --> 00:35:52,030
So, um, this G in general can,

672
00:35:52,030 --> 00:35:54,415
you know, can use a gamma factor.

673
00:35:54,415 --> 00:35:57,655
So, this can include a gamma when you compute those.

674
00:35:57,655 --> 00:36:00,310
You're right, that if the process is known to terminate,

675
00:36:00,310 --> 00:36:03,340
you don't have to have a gamma less than one because

676
00:36:03,340 --> 00:36:06,640
your reward can't be infinity because your process will always terminate.

677
00:36:06,640 --> 00:36:11,170
Um, this could not handle cases where there's some probability it will terminate.

678
00:36:11,170 --> 00:36:15,505
So, if there is a self-loop inside of- or a small loop inside of your process,

679
00:36:15,505 --> 00:36:18,685
such that you could go round it forever and never terminate,

680
00:36:18,685 --> 00:36:23,710
you can't do Monte Carlo, and having a good discount there won't help.

681
00:36:23,710 --> 00:36:27,205
There are physical reasons why you might have a gamma models like that, which is great,

682
00:36:27,205 --> 00:36:29,605
say you model the fuel cost or something,

683
00:36:29,605 --> 00:36:32,185
or something would interact, would that be reasonable?

684
00:36:32,185 --> 00:36:34,750
The question is whether or not there might be a physical reason for

685
00:36:34,750 --> 00:36:38,065
gamma like fuel costs or things like that.

686
00:36:38,065 --> 00:36:40,660
I mean, I think normally I would put that into the reward function.

687
00:36:40,660 --> 00:36:43,450
Good. So, if you have something like- you can have it.

688
00:36:43,450 --> 00:36:44,950
So, I keep thinking about cases where

689
00:36:44,950 --> 00:36:47,410
basically you want to get to a goal as quickly as possible,

690
00:36:47,410 --> 00:36:51,115
um, and you want to sort of do a stochastic shortest paths type problem.

691
00:36:51,115 --> 00:36:55,330
Um, I think generally there I would probably rather pick making it a terminal

692
00:36:55,330 --> 00:36:56,860
state and then having like

693
00:36:56,860 --> 00:37:00,310
a negative one cost if you really have a notion of how much fuel costs.

694
00:37:00,310 --> 00:37:05,935
Um, but you can also use it as a proxy to try to encourage quick progress towards a goal.

695
00:37:05,935 --> 00:37:11,590
The challenge is that how you set it is often pretty subtle because if you set it

696
00:37:11,590 --> 00:37:14,080
too high you can get weird behavior where

697
00:37:14,080 --> 00:37:16,780
your agent has sort of effectively like too scared to do anything,

698
00:37:16,780 --> 00:37:18,610
it will stay at really safe areas.

699
00:37:18,610 --> 00:37:21,220
Um, and if it's too high in some cases,

700
00:37:21,220 --> 00:37:23,380
if it's possible to get sort of trivial reward,

701
00:37:23,380 --> 00:37:25,330
your agent can be misled by that.

702
00:37:25,330 --> 00:37:29,755
So, it's often a little bit tricky to set in real-world cases.

703
00:37:29,755 --> 00:37:34,360
Okay. So, they're high variance estimators that require these episodic settings,

704
00:37:34,360 --> 00:37:39,760
um, and, um, there's no bootstrapping.

705
00:37:39,760 --> 00:37:42,280
And generally, they converge to the true value under some,

706
00:37:42,280 --> 00:37:45,260
uh, generally mild assumptions.

707
00:37:46,260 --> 00:37:50,275
We're gonna talk about important sampling at the end of class if we have time.

708
00:37:50,275 --> 00:37:53,080
Otherwise, we'll probably end up pushing that towards later.

709
00:37:53,080 --> 00:37:56,005
That's for what how we do this if you have off policy data,

710
00:37:56,005 --> 00:37:58,255
data that's collected from another policy.

711
00:37:58,255 --> 00:38:01,795
Okay. Now let's talk about temporal difference learning.

712
00:38:01,795 --> 00:38:05,770
So, if you look at Sutton and Barto, um,

713
00:38:05,770 --> 00:38:07,675
and if you talk to Rich Sutton or, ah,

714
00:38:07,675 --> 00:38:11,140
number of, uh, and a number of other people that are very influential in the field,

715
00:38:11,140 --> 00:38:13,840
they would probably argue that these central, um,

716
00:38:13,840 --> 00:38:17,410
contribution to reinforcement learning or contribution to reinforcement learning

717
00:38:17,410 --> 00:38:20,965
that makes it different perhaps than some other ways of thinking about adaptive control,

718
00:38:20,965 --> 00:38:24,010
is the notion of temporal difference learning.

719
00:38:24,010 --> 00:38:26,770
And essentially, it's going to just combine between

720
00:38:26,770 --> 00:38:29,845
Monte Carlo estimates and dynamic programming methods.

721
00:38:29,845 --> 00:38:31,420
And it's model-free.

722
00:38:31,420 --> 00:38:35,560
We're not going to explicitly compute a dynamics model or reward model or

723
00:38:35,560 --> 00:38:40,540
an estimator of that from data and it both bootstraps and samples.

724
00:38:40,540 --> 00:38:44,110
So, remember, dynamic programming as we've defined it so far,

725
00:38:44,110 --> 00:38:46,300
um, it bootstraps, er,

726
00:38:46,300 --> 00:38:48,580
and the way we have thought about it so far you actually have access

727
00:38:48,580 --> 00:38:50,890
to the real dynamics model and the real reward model,

728
00:38:50,890 --> 00:38:53,695
but it bootstraps by using that VK minus one.

729
00:38:53,695 --> 00:38:56,350
Monte Carlo estimators do not bootstrap.

730
00:38:56,350 --> 00:38:59,200
They go all the way out to the end of the trajectory and sum up the rewards,

731
00:38:59,200 --> 00:39:02,990
but they sample to approximate the expectation.

732
00:39:03,000 --> 00:39:08,260
So, bootstrapping is used to approximate the future discounted sum of rewards.

733
00:39:08,260 --> 00:39:13,310
Sampling is often done to approximate your expectation over states.

734
00:39:13,680 --> 00:39:17,110
The nice thing about temporal difference learning is you can do it

735
00:39:17,110 --> 00:39:19,945
in episodic processes or continual processes.

736
00:39:19,945 --> 00:39:23,740
And the other nice aspect about it is that you don't have to wait till the end of the,

737
00:39:23,740 --> 00:39:25,180
uh, the episode to update.

738
00:39:25,180 --> 00:39:28,555
So as soon as you get a new observation, taking, ah,

739
00:39:28,555 --> 00:39:32,305
starting in a state taking an action and going to a next state and getting some reward,

740
00:39:32,305 --> 00:39:34,285
um, you can immediately update your value.

741
00:39:34,285 --> 00:39:36,130
And this can be really useful because you can

742
00:39:36,130 --> 00:39:38,810
kind of immediately start to use that knowledge.

743
00:39:39,180 --> 00:39:42,940
Okay. So, what are we gonna do in temporal difference learning?

744
00:39:42,940 --> 00:39:45,745
Again, our aim is to compute our estimate of v pi.

745
00:39:45,745 --> 00:39:48,250
And we still have the same definition of return,

746
00:39:48,250 --> 00:39:52,030
um, and we're gonna look at remind ourselves of the Bellman operator.

747
00:39:52,030 --> 00:39:54,265
So, if we know our MDP models,

748
00:39:54,265 --> 00:39:56,470
our Bellman operator said we're gonna get

749
00:39:56,470 --> 00:40:00,830
our immediate reward plus our discounted sum of future rewards.

750
00:40:01,170 --> 00:40:04,915
And in incremental every visit Monte Carlo,

751
00:40:04,915 --> 00:40:09,160
what we're doing is we're updating our estimate using one sample of the return.

752
00:40:09,160 --> 00:40:13,135
So, this is where we said our va-

753
00:40:13,135 --> 00:40:16,915
our new value estimate of the value is equal to our old estimate

754
00:40:16,915 --> 00:40:20,860
plus alpha times the return we just saw minus

755
00:40:20,860 --> 00:40:25,975
V. But this is where we had to wait till the end of the episode to do that update.

756
00:40:25,975 --> 00:40:29,650
What the inside of temporal difference learning is, well,

757
00:40:29,650 --> 00:40:32,380
why don't we just use our old estimator of

758
00:40:32,380 --> 00:40:36,715
v pi for that state and then you don't have to wait till the end of the episode.

759
00:40:36,715 --> 00:40:41,980
So, instead of using GI there you use the reward you just saw plus

760
00:40:41,980 --> 00:40:47,320
gamma times the value of your next state. So, you bootstrap.

761
00:40:47,320 --> 00:40:50,185
Say I'm not going to wait till I get only an episode,

762
00:40:50,185 --> 00:40:52,240
started my state, I got a reward,

763
00:40:52,240 --> 00:40:53,395
I went to some next state.

764
00:40:53,395 --> 00:40:55,390
What is the value of that next state? I don't know.

765
00:40:55,390 --> 00:40:58,930
I'll go look it up in my estimator and I'll plug that in and I'll treat that as,

766
00:40:58,930 --> 00:41:01,640
uh, as an estimate of the return.

767
00:41:04,500 --> 00:41:09,205
So, the simplest TD learning algorithm is exactly that,

768
00:41:09,205 --> 00:41:11,770
where you just take your immediate reward plus your

769
00:41:11,770 --> 00:41:14,200
discounted expected future value

770
00:41:14,200 --> 00:41:17,155
where you plug that in for the state that you actually reached.

771
00:41:17,155 --> 00:41:20,170
Now, notice that this is sampling.

772
00:41:20,170 --> 00:41:24,505
There is no- normally we would have like that nice sum.

773
00:41:24,505 --> 00:41:28,090
The Bellman operator we would normally have a sum over

774
00:41:28,090 --> 00:41:34,330
S prime probability of S prime given s a of v pi of S prime.

775
00:41:34,330 --> 00:41:35,860
We don't have that here.

776
00:41:35,860 --> 00:41:38,035
We're only giving you a single next state.

777
00:41:38,035 --> 00:41:40,045
And we're plugging that in as our estimator.

778
00:41:40,045 --> 00:41:43,870
So we're still going to be doing sampling to approximate that expectation.

779
00:41:43,870 --> 00:41:46,630
But just like dynamic programming we're going to bootstrap

780
00:41:46,630 --> 00:41:49,225
because we're gonna using our previous estimate of v pi.

781
00:41:49,225 --> 00:41:54,535
We also write this as like a sub a and sub k minus one to show like the iterations.

782
00:41:54,535 --> 00:41:58,330
Yeah. I might have down there if you want to see. No, I don't in this case.

783
00:41:58,330 --> 00:41:59,905
You could also write this with- um,

784
00:41:59,905 --> 00:42:03,805
question is if we want just to be clear about what is happening in terms of iterations.

785
00:42:03,805 --> 00:42:09,775
You can also think of this as p of k plus one and this is V of k,

786
00:42:09,775 --> 00:42:13,810
for example, you're updating this over time.

787
00:42:13,810 --> 00:42:15,640
The thing is is that you're doing this for

788
00:42:15,640 --> 00:42:18,415
every single state compared to dynamic programming,

789
00:42:18,415 --> 00:42:20,965
where you do this in ways where for all states-

790
00:42:20,965 --> 00:42:23,725
so you have sort of a consistent VK and then you're updating.

791
00:42:23,725 --> 00:42:27,085
Here we can think of there as just being a value function and you're just sort of

792
00:42:27,085 --> 00:42:30,880
updating one entry of that value function depending on which state you just reached.

793
00:42:30,880 --> 00:42:32,995
So there's not kind of this nice notion of

794
00:42:32,995 --> 00:42:36,025
the whole previous value function of any value function.

795
00:42:36,025 --> 00:42:38,545
I'll keep that there just for that reason.

796
00:42:38,545 --> 00:42:40,690
Now, people often talk about the TD error,

797
00:42:40,690 --> 00:42:41,935
the temporal difference error.

798
00:42:41,935 --> 00:42:45,625
What that is is it's comparing what is your estimate here.

799
00:42:45,625 --> 00:42:48,900
So, your new estimate,

800
00:42:48,900 --> 00:42:52,410
which is your immediate reward plus gamma times your value of the state you actually

801
00:42:52,410 --> 00:42:56,475
reached minus your current estimate of your value.

802
00:42:56,475 --> 00:42:59,645
Now, notice this one should have been sort of essentially

803
00:42:59,645 --> 00:43:05,620
approximating the expectation over S prime.

804
00:43:05,620 --> 00:43:08,095
Because for that one you're going to be averaging.

805
00:43:08,095 --> 00:43:10,195
And so this looks at the temporal difference.

806
00:43:10,195 --> 00:43:12,910
So this is saying how different is

807
00:43:12,910 --> 00:43:16,030
your immediate reward plus gamma times your value of your next state,

808
00:43:16,030 --> 00:43:21,140
versus your sort of current estimate of the value of your current state.

809
00:43:22,950 --> 00:43:26,920
Now note that that doesn't have to go to zero because

810
00:43:26,920 --> 00:43:30,695
that first thing is always ever just a sample, it's one future.

811
00:43:30,695 --> 00:43:34,830
The only time this would be defined to go to zero is if this is deterministic,

812
00:43:34,830 --> 00:43:36,705
so there's only one next state.

813
00:43:36,705 --> 00:43:39,240
So, you know, if half the time when I try to drive to

814
00:43:39,240 --> 00:43:41,840
the airport I hit traffic and half the time I don't,

815
00:43:41,840 --> 00:43:44,050
then that's sort of two different next states

816
00:43:44,050 --> 00:43:45,790
that I could go to for my current start state,

817
00:43:45,790 --> 00:43:47,785
either hit traffic or don't hit traffic.

818
00:43:47,785 --> 00:43:50,470
Um, and so I'm either going to be getting that v

819
00:43:50,470 --> 00:43:53,425
pi of hitting traffic or v pi of not hitting traffic.

820
00:43:53,425 --> 00:43:58,390
So this TD error will not necessarily go to zero even with infinite data because one is

821
00:43:58,390 --> 00:44:00,760
an expected thing from the current state and the

822
00:44:00,760 --> 00:44:04,030
other is which actual next state did you reach.

823
00:44:04,030 --> 00:44:06,610
So, the nice thing is that you can immediately update this

824
00:44:06,610 --> 00:44:08,560
value estimate after your state action

825
00:44:08,560 --> 00:44:12,850
reward s prime tuple and you don't need episodic settings. Yeah, Scotty?

826
00:44:12,850 --> 00:44:16,695
Does that affect convergence if you keep alpha constant?

827
00:44:16,695 --> 00:44:20,490
Yes, good question. Does this affect convergence if you keep alpha constant?

828
00:44:20,490 --> 00:44:24,945
Yes, and you normally have to have some mild assumptions on decaying alpha.

829
00:44:24,945 --> 00:44:28,030
So, things like one over T is normally

830
00:44:28,030 --> 00:44:31,390
sufficient to ensure these estimates convert. Yeah, question?

831
00:44:31,390 --> 00:44:36,175
Um, can you say anything about the bias of this estimator?

832
00:44:36,175 --> 00:44:39,970
Yeah. The question was whether- question was a good one,

833
00:44:39,970 --> 00:44:42,070
what can you say anything about the bias of this estimator?

834
00:44:42,070 --> 00:44:45,860
Am I having a sense of whether this is going to be a biased estimator?

835
00:44:48,420 --> 00:44:56,330
What of your previous or we have a sense of whether it's going to be biased?

836
00:45:03,150 --> 00:45:06,055
Well think back to dynamic programming,

837
00:45:06,055 --> 00:45:08,785
was V_k minus one.

838
00:45:08,785 --> 00:45:13,090
Um, an unbiased estimator of infinite horizon.

839
00:45:13,090 --> 00:45:18,190
Like, let's say, k is equal to two if we want the infinite horizon value.

840
00:45:18,190 --> 00:45:22,645
Is that- no matter how you've done those updates, it's not going to be cool.

841
00:45:22,645 --> 00:45:24,700
Generally, when you bootstrap, um,

842
00:45:24,700 --> 00:45:27,040
it's going to be a biased estimator because you're

843
00:45:27,040 --> 00:45:29,590
relying on your previous estimator which is generally wrong.

844
00:45:29,590 --> 00:45:33,025
[LAUGHTER]. So, that's going to be biasing you in one particular direction.

845
00:45:33,025 --> 00:45:34,810
So, it's a definitely a biased estimator.

846
00:45:34,810 --> 00:45:37,480
Um, it also can have fairly high variance.

847
00:45:37,480 --> 00:45:40,405
[LAUGHTER]. So, it can both be high-variance and be biased.

848
00:45:40,405 --> 00:45:43,465
But on the other hand, you can update it really, really quickly.

849
00:45:43,465 --> 00:45:47,200
Um, you don't have to wait till the end of the episode and you can use a lot of information.

850
00:45:47,200 --> 00:45:50,915
So, it's generally much less high-variance than, um, im- um,

851
00:45:50,915 --> 00:45:53,410
Monte Carlo estimates because you're

852
00:45:53,410 --> 00:45:58,210
bootstrapping and that sort of helping average over a lot of your of variability.

853
00:45:58,210 --> 00:46:06,400
[inaudible]

854
00:46:06,400 --> 00:46:09,445
Now, this question is whether or not it's a function of the initialization. It's not.

855
00:46:09,445 --> 00:46:11,110
It's a, it's a function of the different properties of

856
00:46:11,110 --> 00:46:13,030
the estimators you could initialize differently.

857
00:46:13,030 --> 00:46:18,175
Um, the, the bootstrapping is because you're using a- by bootstrapping and

858
00:46:18,175 --> 00:46:25,240
using this V_Pi as a proxy for your real expected discounted sum of returns,

859
00:46:25,240 --> 00:46:27,280
um, unless this is the true value,

860
00:46:27,280 --> 00:46:29,510
it's just going to bias you.

861
00:46:29,550 --> 00:46:33,100
Note that this, um, this doesn't- you don't get biased in

862
00:46:33,100 --> 00:46:36,580
dynamic programming when you know the models because that V_Pi,

863
00:46:36,580 --> 00:46:38,350
when you bootstrap it's actually V_Pi.

864
00:46:38,350 --> 00:46:40,480
This is actually the real value.

865
00:46:40,480 --> 00:46:41,980
So, the problem is the- here is that it's

866
00:46:41,980 --> 00:46:45,235
an approximation of the real value and that's why it's biasing you.

867
00:46:45,235 --> 00:46:48,520
So bootstrapping is fine if you know the real dynamics model.

868
00:46:48,520 --> 00:46:49,930
The real reward functions,

869
00:46:49,930 --> 00:46:52,750
you need computed the Pi of k minus one exactly,

870
00:46:52,750 --> 00:46:57,770
um, but it's not okay here because we're introducing bias.

871
00:47:00,930 --> 00:47:04,870
So, how does TD zero learning work?

872
00:47:04,870 --> 00:47:08,365
Um, I do zero here because there's sort of some interesting, um,

873
00:47:08,365 --> 00:47:12,955
in-between between TD learning and Monte-Carlo learning where instead of doing

874
00:47:12,955 --> 00:47:15,220
an immediate reward plus the discounted sum of

875
00:47:15,220 --> 00:47:18,190
future rewards versus summing all of the rewards,

876
00:47:18,190 --> 00:47:20,830
you can imagine continuums between these two where you

877
00:47:20,830 --> 00:47:23,560
may be- some up the first two rewards and then Bootstrap.

878
00:47:23,560 --> 00:47:25,990
[NOISE]. So, there's, um,

879
00:47:25,990 --> 00:47:27,490
there's a continuum of models,

880
00:47:27,490 --> 00:47:30,280
there's a continuum of algorithms between just taking

881
00:47:30,280 --> 00:47:33,685
your immediate reward and then bootstrapping versus never bootstrapping.

882
00:47:33,685 --> 00:47:35,530
Um, but we're just gonna talk right now about

883
00:47:35,530 --> 00:47:37,540
taking your immediate reward and then bootstrapping.

884
00:47:37,540 --> 00:47:41,545
So TD learning works as follows: You have to pick a particular alpha,

885
00:47:41,545 --> 00:47:43,630
um, which can be a function of the time-step.

886
00:47:43,630 --> 00:47:45,550
Um, you initialize your value function,

887
00:47:45,550 --> 00:47:48,910
you sample a state action reward, next state.

888
00:47:48,910 --> 00:47:50,200
Now in this case,

889
00:47:50,200 --> 00:47:53,200
because we're doing policy evaluation,

890
00:47:53,200 --> 00:47:59,770
let me- this will be equal to Pi of st,

891
00:47:59,770 --> 00:48:03,440
and then you update your value.

892
00:48:06,930 --> 00:48:09,670
Okay. So let's look, um,

893
00:48:09,670 --> 00:48:11,590
again at that example we had before.

894
00:48:11,590 --> 00:48:14,200
So we said that for first visit Monte Carlo,

895
00:48:14,200 --> 00:48:16,525
you will get 1110000,

896
00:48:16,525 --> 00:48:18,610
for every visit it would be one.

897
00:48:18,610 --> 00:48:23,930
What is the TD estimate of all states at the end of this episode?

898
00:48:25,500 --> 00:48:28,195
So, notice what we're doing here.

899
00:48:28,195 --> 00:48:31,270
We loop, we sample a tuple,

900
00:48:31,270 --> 00:48:33,970
we update the value of the state we just reached.

901
00:48:33,970 --> 00:48:36,550
We get another tuple, we sample it.

902
00:48:36,550 --> 00:48:38,515
So, what would that look like in this case?

903
00:48:38,515 --> 00:48:41,870
We would start off and we'd have S3,

904
00:48:43,140 --> 00:48:48,580
we'd have S3, A1, zero, S2.

905
00:48:48,580 --> 00:48:53,695
You'd have S2, A1, zero, S2, S2,

906
00:48:53,695 --> 00:48:57,280
A1, zero, S1, S1,

907
00:48:57,280 --> 00:49:00,590
A1 plus one, terminate.

908
00:49:01,470 --> 00:49:03,745
So, why don't you spend a minute and,

909
00:49:03,745 --> 00:49:07,970
and think about what the value would be under TD learning,

910
00:49:09,090 --> 00:49:11,860
and what implications this might have too.

911
00:49:11,860 --> 00:49:17,530
[NOISE].

912
00:49:17,530 --> 00:49:19,270
Does anybody wanna say what the value is,

913
00:49:19,270 --> 00:49:23,590
that you get? [NOISE]. Yeah.

914
00:49:23,590 --> 00:49:25,630
Uh, one followed by all zeros.

915
00:49:25,630 --> 00:49:29,360
That's right. Okay. One followed by all zeros.

916
00:49:29,730 --> 00:49:34,550
So, we only updated the final state in this case.

917
00:49:34,620 --> 00:49:37,750
I also just wanted to- yeah, question.

918
00:49:37,750 --> 00:49:40,040
Um, explain why that happens.

919
00:49:40,040 --> 00:49:42,460
Yeah, because, um, what we are doing in

920
00:49:42,460 --> 00:49:45,610
this case is that we get a data point so what- we're in a state,

921
00:49:45,610 --> 00:49:47,815
we take an action, we get a reward, we get next state.

922
00:49:47,815 --> 00:49:49,930
We update the value only for that state.

923
00:49:49,930 --> 00:49:53,095
So what we did here is we got S3,

924
00:49:53,095 --> 00:49:55,120
we update it, we did action A1,

925
00:49:55,120 --> 00:49:56,650
we got a zero S2.

926
00:49:56,650 --> 00:50:00,040
So our new value for S3 was also equal to zero.

927
00:50:00,040 --> 00:50:02,860
Then we went to S2, we took action A1,

928
00:50:02,860 --> 00:50:04,240
we got a zero, we went to S2,

929
00:50:04,240 --> 00:50:05,800
we got- so we updated S2,

930
00:50:05,800 --> 00:50:08,875
it was also zero. We did that again.

931
00:50:08,875 --> 00:50:12,550
We finally got to state S1 and we got a one.

932
00:50:12,550 --> 00:50:16,240
So, the thing about this that can be beneficial

933
00:50:16,240 --> 00:50:20,590
or not beneficial is you throw away your data in the most naive format.

934
00:50:20,590 --> 00:50:25,435
You have a SAR S-prime tuple and then it goes away again. You don't keep it.

935
00:50:25,435 --> 00:50:27,340
So when you finally see that reward,

936
00:50:27,340 --> 00:50:28,465
you don't back up,

937
00:50:28,465 --> 00:50:31,030
you don't propagate that information backwards.

938
00:50:31,030 --> 00:50:32,590
So what Monte Carlo did is,

939
00:50:32,590 --> 00:50:36,220
it waited until he got all the way there and then it computed the return for every state

940
00:50:36,220 --> 00:50:40,255
along the episode which meant that that's why we got 1111.

941
00:50:40,255 --> 00:50:41,380
But here you don't do that.

942
00:50:41,380 --> 00:50:43,240
By the time you get to, um,

943
00:50:43,240 --> 00:50:46,570
[NOISE] S1, you've thrown away the fact that you were ever in S3

944
00:50:46,570 --> 00:50:49,405
or S2 and then you, you don't update those states.

945
00:50:49,405 --> 00:50:52,000
I mean total reward is proportional to

946
00:50:52,000 --> 00:50:55,765
the number of samples you need to get a good estimate of value function?

947
00:50:55,765 --> 00:50:57,400
Say that again.

948
00:50:57,400 --> 00:50:59,740
Ah, I'm assuming that like the longer it

949
00:50:59,740 --> 00:51:02,305
takes for you to get a rewards, the more samples,

950
00:51:02,305 --> 00:51:05,080
you'd need to like properly estimate,

951
00:51:05,080 --> 00:51:07,450
uh, value of the function.

952
00:51:07,450 --> 00:51:09,640
Question out [inaudible] is sort of, you know,

953
00:51:09,640 --> 00:51:11,320
how long does it take you to get a reward and

954
00:51:11,320 --> 00:51:14,500
how many samples do you need to get a good estimate of the value function?

955
00:51:14,500 --> 00:51:16,555
Um, you mean for all states?

956
00:51:16,555 --> 00:51:18,280
It's a little nuanced.

957
00:51:18,280 --> 00:51:21,295
Um, it depends on the transition dynamics as well.

958
00:51:21,295 --> 00:51:26,395
Um, you couldn't- say for a particular,

959
00:51:26,395 --> 00:51:28,000
like how, how many, um,

960
00:51:28,000 --> 00:51:31,510
samples you need for a particular state to get a good estimate of its reward?

961
00:51:31,510 --> 00:51:33,085
Let's say your rewards are stochastic.

962
00:51:33,085 --> 00:51:36,400
But in terms of how long it takes you to propagate this information back,

963
00:51:36,400 --> 00:51:38,020
it depends on the dynamics.

964
00:51:38,020 --> 00:51:39,670
Um, so in this case, you know,

965
00:51:39,670 --> 00:51:43,780
if you had exactly the same trajectory and you did it again,

966
00:51:43,780 --> 00:51:48,325
then you'd end up updating that S2 and then if you got that same trajectory again,

967
00:51:48,325 --> 00:51:50,650
then you would propagate that information back again to

968
00:51:50,650 --> 00:51:53,215
S2 and then one more time and then you get it back to S3.

969
00:51:53,215 --> 00:51:54,985
I should S3 and there's the third one.

970
00:51:54,985 --> 00:51:58,600
So, you can slowly this- propagate this information back, um,

971
00:51:58,600 --> 00:52:03,115
but you don't get to kind of immediately like what Monte Carlo would do. Question.

972
00:52:03,115 --> 00:52:05,440
I was wondering if you could highlight the differences between this

973
00:52:05,440 --> 00:52:08,080
and the Q learning that we talked about last time?

974
00:52:08,080 --> 00:52:10,105
Because they seem like kind of similar ideas.

975
00:52:10,105 --> 00:52:11,500
That's great. So, exactly right.

976
00:52:11,500 --> 00:52:15,340
In fact, TD learning and Q learning are really close analogs.

977
00:52:15,340 --> 00:52:18,220
Q learning is, um, when you're gonna do control.

978
00:52:18,220 --> 00:52:19,615
So, we're going to look at actions.

979
00:52:19,615 --> 00:52:24,380
TD learning is basically Q learning where you're fixing in the policy,

980
00:52:24,480 --> 00:52:26,875
Yeah. Next question back there.

981
00:52:26,875 --> 00:52:30,340
Like you're actually like implement this so you would

982
00:52:30,340 --> 00:52:32,320
you would keep looping right,

983
00:52:32,320 --> 00:52:36,190
and updating or you just run through, uh, rewards?

984
00:52:36,190 --> 00:52:40,000
It depends. So, um, it depends who asked you.

985
00:52:40,000 --> 00:52:42,475
So if you're really, really compare- concerned about memory,

986
00:52:42,475 --> 00:52:43,885
um, you just drop data,

987
00:52:43,885 --> 00:52:45,370
so then you're on [inaudible].

988
00:52:45,370 --> 00:52:48,370
If, um, in a lot of the existing sort of deep learning methods,

989
00:52:48,370 --> 00:52:49,720
you maintain a sort of a,

990
00:52:49,720 --> 00:52:53,170
a episodic replay buffer and then you would re-sample

991
00:52:53,170 --> 00:52:56,995
samples from that and then you would do this update for the samples from there.

992
00:52:56,995 --> 00:53:00,655
So you could revisit sort of past stuff and use it to update your value function.

993
00:53:00,655 --> 00:53:02,500
Um, you could also- it can,

994
00:53:02,500 --> 00:53:04,150
it can matter the order in which you do that.

995
00:53:04,150 --> 00:53:06,790
So in this case, you could do a pass through your data

996
00:53:06,790 --> 00:53:09,640
and then do it- another pass or maybe go backwards from the end.

997
00:53:09,640 --> 00:53:14,455
[inaudible] it will end up propagating.

998
00:53:14,455 --> 00:53:17,500
Some alpha back to S_2 there.

999
00:53:17,500 --> 00:53:18,250
Yeah.

1000
00:53:18,250 --> 00:53:21,520
So, you just go into like convergence or-

1001
00:53:21,520 --> 00:53:23,965
We'll talk about that very shortly. Yes. That's a great question.

1002
00:53:23,965 --> 00:53:25,330
Like so what happens is you do this for

1003
00:53:25,330 --> 00:53:27,385
convergence and we'll talk about that in a second. Yeah.

1004
00:53:27,385 --> 00:53:30,190
So, just so I make sure I understand.

1005
00:53:30,190 --> 00:53:32,665
So, when we talk about sampling of tuple,

1006
00:53:32,665 --> 00:53:34,870
what's really happening is you're going to

1007
00:53:34,870 --> 00:53:38,455
a trajectory and you're iterating through the SAR,

1008
00:53:38,455 --> 00:53:42,190
the SAR tuples in that trajectory in order.

1009
00:53:42,190 --> 00:53:46,390
Right. But we're thinking of this really as acting as- to repeat the question.

1010
00:53:46,390 --> 00:53:47,770
The question is like we're going through

1011
00:53:47,770 --> 00:53:49,600
this trajectory we're updating in terms of tuples.

1012
00:53:49,600 --> 00:53:52,150
Yes, but we're really thinking about this as like your agent being in

1013
00:53:52,150 --> 00:53:54,700
some state taking an action getting reward again and getting to a next state.

1014
00:53:54,700 --> 00:53:56,635
So, there doesn't exist a full trajectory.

1015
00:53:56,635 --> 00:53:58,675
It just like I'm driving my car,

1016
00:53:58,675 --> 00:54:01,255
what's gonna happen to me in the next like two minutes?

1017
00:54:01,255 --> 00:54:05,230
So, I don't have the full trajectory and that I'm iterating through it.

1018
00:54:05,230 --> 00:54:09,175
It's like this is after every single time step inside of that trajectory, I update.

1019
00:54:09,175 --> 00:54:11,350
So, I don't have to wait till I have the full trajectory.

1020
00:54:11,350 --> 00:54:16,855
Right and, and I guess I'll just the order in which those tuples are chosen.

1021
00:54:16,855 --> 00:54:21,985
I- I'm guessing it matters or with the values that you're getting and estimates.

1022
00:54:21,985 --> 00:54:23,770
Yes. So, the question is like, you know,

1023
00:54:23,770 --> 00:54:25,555
the order in which you receive tuples,

1024
00:54:25,555 --> 00:54:27,860
that absolutely affects your value.

1025
00:54:27,860 --> 00:54:30,510
Um, so in, uh,

1026
00:54:30,510 --> 00:54:32,610
if you're getting this in terms of how you experience this in the world,

1027
00:54:32,610 --> 00:54:34,755
it's just the order you experience these in the world.

1028
00:54:34,755 --> 00:54:40,295
So, this S_t plus prime- T plus one prime becomes your ST on the next time-step.

1029
00:54:40,295 --> 00:54:42,820
So, these aren't being sampled from a trajectory.

1030
00:54:42,820 --> 00:54:44,635
It's like that's just wherever you are now.

1031
00:54:44,635 --> 00:54:46,960
Um, if you have access to batch data,

1032
00:54:46,960 --> 00:54:50,905
then you can choose which ones to pick and it absolutely affects your convergence.

1033
00:54:50,905 --> 00:54:54,070
The problem is you don't have to know which ones to pick in advance.

1034
00:54:54,070 --> 00:54:58,570
Questions. The other thing I just want to mention there is it's a little bit subtle, um,

1035
00:54:58,570 --> 00:55:01,090
that if you set alpha equal to, like, you know,

1036
00:55:01,090 --> 00:55:02,590
1 over T or things like that,

1037
00:55:02,590 --> 00:55:03,940
you can be guaranteed to,

1038
00:55:03,940 --> 00:55:05,920
um, for these things to converge.

1039
00:55:05,920 --> 00:55:08,065
Uh, sometimes if alpha is really small, um,

1040
00:55:08,065 --> 00:55:12,130
also these are going to be guaranteed to converge under minor conditions.

1041
00:55:12,130 --> 00:55:14,230
Um, but if you said something like alpha equals one,

1042
00:55:14,230 --> 00:55:16,075
it can definitely oscillate.

1043
00:55:16,075 --> 00:55:19,840
Alpha equals one means that essentially,

1044
00:55:19,840 --> 00:55:21,700
you're ignoring your previous estimate, right?

1045
00:55:21,700 --> 00:55:26,900
So, if you set alpha equal to one then you're just using your TD target.

1046
00:55:28,530 --> 00:55:34,230
All right. Okay. So, what is temporal policy difference policy evaluation

1047
00:55:34,230 --> 00:55:36,420
doing if we think about it in terms of this diagram and

1048
00:55:36,420 --> 00:55:39,345
thinking about us as taking an expectation over futures.

1049
00:55:39,345 --> 00:55:42,525
So, it's, um, this is the equation for it up there.

1050
00:55:42,525 --> 00:55:46,860
And what it does is it updates its value estimate by using a sample of S_t plus 1

1051
00:55:46,860 --> 00:55:51,535
to approximate that expected next state distribution or next future distribution.

1052
00:55:51,535 --> 00:55:54,145
And then it bootstraps because it plugs in

1053
00:55:54,145 --> 00:55:57,430
your previous estimate of V_pi for this plus 1.

1054
00:55:57,430 --> 00:55:58,900
So, that's why it's a hybrid between

1055
00:55:58,900 --> 00:56:02,200
dynamic programming because it bootstraps and Monte Carlo

1056
00:56:02,200 --> 00:56:07,820
because it doesn't do an explicit expectation over all the next states, just samples one.

1057
00:56:11,520 --> 00:56:15,085
Okay. So, now why don't we think about some of these things that, like,

1058
00:56:15,085 --> 00:56:17,110
allow us to compare between these different algorithms and

1059
00:56:17,110 --> 00:56:19,930
their strengths and weaknesses and it sometimes depends on the application.

1060
00:56:19,930 --> 00:56:23,680
Um, uh, you've had to pick which one is most popular,

1061
00:56:23,680 --> 00:56:26,410
probably TD learning is the most popular but it depends on the domain.

1062
00:56:26,410 --> 00:56:30,520
It depends on, um, whether you're constrained by data or,

1063
00:56:30,520 --> 00:56:34,480
um, you know, computation or memory et cetera. All right.

1064
00:56:34,480 --> 00:56:39,130
So, um, why don't we spend a few minutes on this briefly.

1065
00:56:39,130 --> 00:56:43,270
So, let us spend a minute and think about which

1066
00:56:43,270 --> 00:56:47,545
of these properties from what you remember so far apply to these three algorithms.

1067
00:56:47,545 --> 00:56:51,715
So, whether they're usable when you have no models of the current domain, um,

1068
00:56:51,715 --> 00:56:54,385
whether they handle continuing non-episodic domains,

1069
00:56:54,385 --> 00:56:56,515
they can handle non-Markovian domains.

1070
00:56:56,515 --> 00:56:58,765
They converge to the true value in the limit.

1071
00:56:58,765 --> 00:57:00,310
We're assuming everything's tabular right now,

1072
00:57:00,310 --> 00:57:02,350
we're not in function approximation land.

1073
00:57:02,350 --> 00:57:05,800
And whether or not you think they give you an unbiased estimate of the value.

1074
00:57:05,800 --> 00:57:09,430
So, if at any time point if you were to take your estimator if it's unbiased.

1075
00:57:09,430 --> 00:57:11,890
So, why don't you would just spend a minute see if you can fill in this table.

1076
00:57:11,890 --> 00:57:13,960
Feel free to talk to someone next to you and then we'll step through it.

1077
00:57:13,960 --> 00:57:17,620
[NOISE] All right which of

1078
00:57:17,620 --> 00:57:21,370
these are usable when you have no models of the current domain?

1079
00:57:21,370 --> 00:57:28,750
[NOISE] Does dynamic programming need a model of the current domain?

1080
00:57:28,750 --> 00:57:29,680
Yes.

1081
00:57:29,680 --> 00:57:32,230
Yes. Okay. What about Monte Carlo?

1082
00:57:32,230 --> 00:57:33,280
Usable.

1083
00:57:33,280 --> 00:57:34,715
Usable. What about TD?

1084
00:57:34,715 --> 00:57:35,565
Usable.

1085
00:57:35,565 --> 00:57:38,605
Usable. Yeah. Do either of those,

1086
00:57:38,605 --> 00:57:39,805
TD is known as what?

1087
00:57:39,805 --> 00:57:41,560
As a model free algorithm,

1088
00:57:41,560 --> 00:57:43,390
doesn't need an explicit notion.

1089
00:57:43,390 --> 00:57:46,540
It relies on sampling of the next state from the real world.

1090
00:57:46,540 --> 00:57:51,500
[NOISE] Which of these can be used for continuing non-episodic domains?

1091
00:57:51,980 --> 00:57:56,205
So, like, your process might not terminate, ever.

1092
00:57:56,205 --> 00:57:59,130
Okay. Well, can TD learning be used?

1093
00:57:59,130 --> 00:57:59,865
Yes.

1094
00:57:59,865 --> 00:58:01,470
Yes. Can Monte Carlo be used?

1095
00:58:01,470 --> 00:58:01,905
No.

1096
00:58:01,905 --> 00:58:04,650
No. Can DP be used?

1097
00:58:04,650 --> 00:58:05,570
Yes.

1098
00:58:05,570 --> 00:58:07,315
Yes. Okay. Which of these,

1099
00:58:07,315 --> 00:58:10,370
um, does DP require Markovian?

1100
00:58:11,340 --> 00:58:12,440
Yes.

1101
00:58:12,440 --> 00:58:15,895
It does. Which- does Monte Carlo require Markovian?

1102
00:58:15,895 --> 00:58:20,300
No. Does TD require Markovian?

1103
00:58:20,780 --> 00:58:25,465
Yeah, it does. So, um, uh,

1104
00:58:25,465 --> 00:58:28,660
temporal difference and dynamic programming rely on the fact that

1105
00:58:28,660 --> 00:58:32,530
your value of the current state does not depend on the history.

1106
00:58:32,530 --> 00:58:34,765
So, however you got to the current state,

1107
00:58:34,765 --> 00:58:36,970
it ignores that, um,

1108
00:58:36,970 --> 00:58:39,550
and then it uses that when it bootstraps too,

1109
00:58:39,550 --> 00:58:41,410
it assumes that doesn't- so,

1110
00:58:41,410 --> 00:58:43,270
Monte Carlo just adds up your return from

1111
00:58:43,270 --> 00:58:45,310
wherever you are at now till the end of the episode.

1112
00:58:45,310 --> 00:58:48,250
And note that depending on when you got to that particular state,

1113
00:58:48,250 --> 00:58:51,325
your return may be different and it might depend on the history.

1114
00:58:51,325 --> 00:58:54,985
So, Monte Carlo doesn't rely on the world being Markov.

1115
00:58:54,985 --> 00:58:57,685
Um, you can use it in partially observable environments.

1116
00:58:57,685 --> 00:59:00,025
TD assumes that the world is Markovian,

1117
00:59:00,025 --> 00:59:03,460
so does dynamic programming in the ways we've defined it so far.

1118
00:59:03,460 --> 00:59:05,710
So, you bootstrap you say, um,

1119
00:59:05,710 --> 00:59:08,020
for this current state my prediction of

1120
00:59:08,020 --> 00:59:11,515
the future value only depends on this current state.

1121
00:59:11,515 --> 00:59:16,600
So, I can say I get my immediate reward plus whatever state I transition to.

1122
00:59:16,600 --> 00:59:19,120
But that's sort of a sufficient statistic of

1123
00:59:19,120 --> 00:59:22,420
the history and I can plug-in my bootstrap estimator.

1124
00:59:22,420 --> 00:59:24,520
So, it relies on the Markovian assumption.

1125
00:59:24,520 --> 00:59:27,430
What about non-Markovian domain where do we apply it?

1126
00:59:27,430 --> 00:59:29,080
Um, the question is well,

1127
00:59:29,080 --> 00:59:30,190
what do you mean by non-Markovian?

1128
00:59:30,190 --> 00:59:32,440
Like, these are algorithms you could apply them.

1129
00:59:32,440 --> 00:59:34,495
Um, so yeah. You can apply these algorithms to anything.

1130
00:59:34,495 --> 00:59:36,430
The question is whether or not they're guaranteed

1131
00:59:36,430 --> 00:59:38,725
to converge in the limit to the right value.

1132
00:59:38,725 --> 00:59:41,950
And they're not, if the world is not Markovian and they don't.

1133
00:59:41,950 --> 00:59:45,640
Like [LAUGHTER] we've seen in some of our work on intelligent tutoring systems,

1134
00:59:45,640 --> 00:59:47,935
earlier on we were using some data, um,

1135
00:59:47,935 --> 00:59:51,640
from a fractions tutor and we're applying Markovian techniques and they don't converge.

1136
00:59:51,640 --> 00:59:53,650
I mean, they converge to something that's just totally

1137
00:59:53,650 --> 00:59:55,735
wrong and it doesn't matter how much data you have

1138
00:59:55,735 --> 01:00:00,970
because you're- you're using methods that rely on assumption that is incorrect.

1139
01:00:00,970 --> 01:00:03,730
So, you need to be able to evaluate whether they're not

1140
01:00:03,730 --> 01:00:06,775
Markovian or try to bound the bias or do something.

1141
01:00:06,775 --> 01:00:09,730
Um, otherwise your estimators of what the value is of

1142
01:00:09,730 --> 01:00:13,880
a policy can just be wrong even in the limit of infinite data.

1143
01:00:14,910 --> 01:00:18,760
Um, what about converging to the true value in the limit?

1144
01:00:18,760 --> 01:00:21,520
Let's assume we're in the Markovian case again.

1145
01:00:21,520 --> 01:00:23,710
So, for Markovian domains, does,

1146
01:00:23,710 --> 01:00:26,515
um, DP converge to the true value in the limit?

1147
01:00:26,515 --> 01:00:26,840
Yes.

1148
01:00:26,840 --> 01:00:28,525
What about Monte Carlo?

1149
01:00:28,525 --> 01:00:29,125
Yes.

1150
01:00:29,125 --> 01:00:30,250
Yes. What about TD?

1151
01:00:30,250 --> 01:00:30,955
Yes.

1152
01:00:30,955 --> 01:00:32,725
Yes. They certainly do.

1153
01:00:32,725 --> 01:00:35,755
The world is really Markovian, um, everything converges.

1154
01:00:35,755 --> 01:00:37,690
Asymptotically no under minor assumptions,

1155
01:00:37,690 --> 01:00:39,325
all of these require minor assumptions.

1156
01:00:39,325 --> 01:00:44,935
Um, uh, so under minor assumptions it will converge to the true value of the limit,

1157
01:00:44,935 --> 01:00:46,540
depends on, like, the alpha value.

1158
01:00:46,540 --> 01:00:50,260
Um, uh, what about being an unbiased estimate of the value,

1159
01:00:50,260 --> 01:00:52,825
is Monte Carlo an unbiased estimator?

1160
01:00:52,825 --> 01:00:53,320
Yes.

1161
01:00:53,320 --> 01:00:56,425
Yes. Okay. TD is not.

1162
01:00:56,425 --> 01:00:58,570
DP is a little bit weird.

1163
01:00:58,570 --> 01:01:00,970
It's a little bit not quite fair question there.

1164
01:01:00,970 --> 01:01:05,470
DP is always giving you the exact VK minus one value for that policy.

1165
01:01:05,470 --> 01:01:07,150
So, that is perfect,

1166
01:01:07,150 --> 01:01:08,905
that's the exact value.

1167
01:01:08,905 --> 01:01:13,255
If you have K1- K minus 1 more time steps to act,

1168
01:01:13,255 --> 01:01:19,240
that is not going to be the same as the infinite horizon value function. Yeah.

1169
01:01:19,240 --> 01:01:22,750
Can you explain how the last two lines are different.

1170
01:01:22,750 --> 01:01:25,600
Like I don't understand the difference between unbiased estimator of

1171
01:01:25,600 --> 01:01:28,960
value and something that converges to the true value of order.

1172
01:01:28,960 --> 01:01:31,270
Your question's great. So, the question is what's the difference

1173
01:01:31,270 --> 01:01:33,850
between something being unbiased and consistent?

1174
01:01:33,850 --> 01:01:37,060
Um, so when we say converges to the true value in limit,

1175
01:01:37,060 --> 01:01:40,315
that's also known as formally being a consistent estimator.

1176
01:01:40,315 --> 01:01:44,485
So, the unbiased estimate of the value means,

1177
01:01:44,485 --> 01:01:47,650
if you have a finite amount of data and you

1178
01:01:47,650 --> 01:01:50,200
compute your statistic, in this case the value,

1179
01:01:50,200 --> 01:01:53,215
and you compare it to the true value, then on average,

1180
01:01:53,215 --> 01:01:58,195
that difference will be zero and that is not true for things like TD.

1181
01:01:58,195 --> 01:02:03,340
But, um, and that can be- that's being evaluated for finite amounts of data.

1182
01:02:03,340 --> 01:02:07,075
What consistency says if you have an infinite amount of data,

1183
01:02:07,075 --> 01:02:09,085
will you get to the true value?

1184
01:02:09,085 --> 01:02:11,395
So, what that implies is that,

1185
01:02:11,395 --> 01:02:15,865
say for TD, that asymptotically the bias has to go to zero.

1186
01:02:15,865 --> 01:02:18,010
If you have infinite amounts of data,

1187
01:02:18,010 --> 01:02:20,110
eventually its bias will be zero.

1188
01:02:20,110 --> 01:02:21,490
But for small amount, you know,

1189
01:02:21,490 --> 01:02:23,155
for finite amounts of data and really,

1190
01:02:23,155 --> 01:02:25,240
you know, you don't know what that N is.

1191
01:02:25,240 --> 01:02:28,450
Uh, it could be a biased estimator but as the amount of

1192
01:02:28,450 --> 01:02:31,720
data you have that goes to infinity then it has to converge to the true value.

1193
01:02:31,720 --> 01:02:35,725
So, you can have things that are biased estimators that are consistent. Yeah.

1194
01:02:35,725 --> 01:02:39,400
For Monte Carlo, I thought you said that the implementation has an impact on

1195
01:02:39,400 --> 01:02:41,950
whether or not it's biased is- I thought you said if

1196
01:02:41,950 --> 01:02:44,620
it's every visit then it is unbiased [OVERLAPPING]

1197
01:02:44,620 --> 01:02:47,080
Good question. So, I, um, the question is good.

1198
01:02:47,080 --> 01:02:51,650
So, this is, um, it's an unbiased estimate for the, um, first visit.

1199
01:02:52,020 --> 01:02:55,055
And for every visit,

1200
01:02:55,055 --> 01:02:59,860
it's biased. Great. Question?

1201
01:02:59,860 --> 01:03:01,240
Um, this might be a dumb, uh,

1202
01:03:01,240 --> 01:03:04,450
a dumb question but are there any,

1203
01:03:04,450 --> 01:03:09,295
uh, you know, model free policy evaluations models that aren't actually convergent?

1204
01:03:09,295 --> 01:03:10,960
Yes. Question was, are there

1205
01:03:10,960 --> 01:03:14,065
any model free policy evaluation methods that are not convergent?

1206
01:03:14,065 --> 01:03:17,305
Yes, and we will see them a lot when we get into function approximation.

1207
01:03:17,305 --> 01:03:20,320
When you start- so right now we're in the tabular case which means we can

1208
01:03:20,320 --> 01:03:22,945
write down as a table or as a vector what a value is.

1209
01:03:22,945 --> 01:03:24,970
We move up to infinite state spaces.

1210
01:03:24,970 --> 01:03:28,690
Um, a lot of the methods are not even guaranteed to converge to

1211
01:03:28,690 --> 01:03:30,700
anything [LAUGHTER] Not even- we're not

1212
01:03:30,700 --> 01:03:32,830
even talking about whether they converge to the right value,

1213
01:03:32,830 --> 01:03:35,860
they're not even guaranteed to stop oscillating.

1214
01:03:35,860 --> 01:03:37,960
And they can just keep changing.

1215
01:03:37,960 --> 01:03:39,715
Okay. Yeah. Question.

1216
01:03:39,715 --> 01:03:46,435
So, is there any specific explanation why TD is not unbiased?

1217
01:03:46,435 --> 01:03:47,515
Is- is what?

1218
01:03:47,515 --> 01:03:49,930
Why TD is not unbiased?

1219
01:03:49,930 --> 01:03:52,870
Why it's not unbiased? Yeah. Great question.

1220
01:03:52,870 --> 01:03:54,280
So, the question was to say, you know,

1221
01:03:54,280 --> 01:03:56,455
why is TD biased.

1222
01:03:56,455 --> 01:03:58,570
TD is biased because you're plugging in

1223
01:03:58,570 --> 01:04:02,785
this estimator of the value of the next state, that is wrong.

1224
01:04:02,785 --> 01:04:07,190
And that's generally going to leave to- lead to some bias.

1225
01:04:07,440 --> 01:04:10,240
You're plugging in an estimator that is not

1226
01:04:10,240 --> 01:04:15,200
the true V pi for S prime. It's going to lead to a bit of bias.

1227
01:04:15,420 --> 01:04:19,420
So, it's really the-the bootstrapping part that's the problem.

1228
01:04:19,420 --> 01:04:23,170
The Monte Carlo was also sampling the expectation and it's unbiased,

1229
01:04:23,170 --> 01:04:25,420
at least in the first visit case.

1230
01:04:25,420 --> 01:04:27,205
Problem here is that, um,

1231
01:04:27,205 --> 01:04:31,670
you're plugging in unexpected discounted sum of rewards that is wrong.

1232
01:04:33,700 --> 01:04:36,770
All right. So, um,

1233
01:04:36,770 --> 01:04:38,710
that just summarizes those there.

1234
01:04:38,710 --> 01:04:41,630
I think the important properties to think- to compare between them.

1235
01:04:41,630 --> 01:04:44,785
Um, how would you pick between these two algorithms.

1236
01:04:44,785 --> 01:04:48,265
So, I think thinking about bias and variance characteristics is important.

1237
01:04:48,265 --> 01:04:51,810
Um, data efficiency is also important as well as computational efficiency,

1238
01:04:51,810 --> 01:04:53,620
and there's going to be trade offs between these.

1239
01:04:53,620 --> 01:04:57,140
Um, so if we think

1240
01:04:57,140 --> 01:05:00,530
about sort of the general bias-variance of these different forms of algorithms,

1241
01:05:00,530 --> 01:05:02,800
Um, Monte Carlo is unbiased,

1242
01:05:02,800 --> 01:05:05,670
generally high-variance, um, and it's consistent.

1243
01:05:05,670 --> 01:05:07,115
TD has some bias,

1244
01:05:07,115 --> 01:05:09,295
much lower variance than Monte Carlo.

1245
01:05:09,295 --> 01:05:13,755
TD zero converges to the true value with tabular representations,

1246
01:05:13,755 --> 01:05:16,420
and as I was saying it does not always converge once

1247
01:05:16,420 --> 01:05:19,040
we get into function approximation and um,

1248
01:05:19,040 --> 01:05:20,260
we'll see more about that shortly.

1249
01:05:20,260 --> 01:05:22,235
I think with the last few minutes,

1250
01:05:22,235 --> 01:05:23,805
we won't have a chance to get through,

1251
01:05:23,805 --> 01:05:26,090
um, little bit about how these methods are

1252
01:05:26,090 --> 01:05:28,975
related to each other when we start to think about the batch setting.

1253
01:05:28,975 --> 01:05:33,265
So as we saw in this particular case for the Mars Rover just again contrast them.

1254
01:05:33,265 --> 01:05:36,790
Um, Monte Carlo estimate waited till the end

1255
01:05:36,790 --> 01:05:40,565
of the episode and then updated every state that was visited in that episode.

1256
01:05:40,565 --> 01:05:44,115
TD only used each data point once,

1257
01:05:44,115 --> 01:05:50,130
and so it only ended up changing the value of the final state in this case.

1258
01:05:51,080 --> 01:05:54,685
So what if- happens if we want to go over our data more than once.

1259
01:05:54,685 --> 01:05:57,150
So if they we're willing to spend a little bit more computation,

1260
01:05:57,150 --> 01:05:59,995
so we can actually get better estimates and be more sample efficient.

1261
01:05:59,995 --> 01:06:01,810
Meaning that we want to use our data more

1262
01:06:01,810 --> 01:06:04,210
efficiently so that we can get a better estimate.

1263
01:06:04,210 --> 01:06:07,530
So often we call this batch or offline, um,

1264
01:06:07,530 --> 01:06:11,625
mal- policy evaluation where we've got some data and we're willing to go through it as

1265
01:06:11,625 --> 01:06:13,760
much as we can in order to try to get an estimate

1266
01:06:13,760 --> 01:06:15,920
of the policy that was used to gather that data.

1267
01:06:15,920 --> 01:06:19,055
So let's imagine that we have a set of k episodes,

1268
01:06:19,055 --> 01:06:21,140
and we can repeatedly sample an episode.

1269
01:06:21,140 --> 01:06:26,375
um, and then we either apply Monte Carlo or TD to the whole episode.

1270
01:06:26,375 --> 01:06:28,160
What happens in this case?

1271
01:06:28,160 --> 01:06:32,215
Um, so there's this nice example from Sutton and Barto.

1272
01:06:32,215 --> 01:06:34,880
Um, let's say there's just two states.

1273
01:06:34,880 --> 01:06:36,455
So there is states A and B,

1274
01:06:36,455 --> 01:06:39,860
and Gamma is equal to one, and you have eight episodes of experience.

1275
01:06:39,860 --> 01:06:45,940
So you either the first episode you saw A, 0, B, 0.

1276
01:06:45,940 --> 01:06:47,910
So this is the reward.

1277
01:06:47,910 --> 01:06:50,570
In B, you saw,

1278
01:06:50,570 --> 01:06:53,950
in the sorry- in the, and then another set of episodes you just started in B,

1279
01:06:53,950 --> 01:06:56,750
and you got one, and you observe that six times,

1280
01:06:56,750 --> 01:07:01,380
and then in the eighth episode you started in B and you got a zero.

1281
01:07:02,180 --> 01:07:05,010
So first of all,

1282
01:07:05,010 --> 01:07:10,885
can we compute what V of B is in this case?

1283
01:07:10,885 --> 01:07:15,770
So the, the model of the world is going to look something like this.

1284
01:07:18,810 --> 01:07:22,745
A to B and the B sometimes goes to one,

1285
01:07:22,745 --> 01:07:26,510
and B sometimes goes to zero and then we always terminate.

1286
01:07:28,170 --> 01:07:31,840
So in all eight episodes we saw B.

1287
01:07:31,840 --> 01:07:34,155
In six of those we got a one,

1288
01:07:34,155 --> 01:07:36,520
in two of them we got a zero.

1289
01:07:37,130 --> 01:07:41,565
So, if we were doing Monte Carlo,

1290
01:07:41,565 --> 01:07:45,090
what would be our estimate of B, value of B.

1291
01:07:45,300 --> 01:07:49,030
So we do a Monte Carlo estimate

1292
01:07:49,250 --> 01:07:54,105
using these eight episodes and we can go over them as many times as we want.

1293
01:07:54,105 --> 01:07:56,090
We don't just have to experience each episode once.

1294
01:07:56,090 --> 01:07:57,240
This is the batch data set.

1295
01:07:57,240 --> 01:07:58,870
Someone already collected this for us,

1296
01:07:58,870 --> 01:08:02,155
can do Monte Carlo updates of this data set as much as you want.

1297
01:08:02,155 --> 01:08:08,090
What will be the estimate of V of B in this case?

1298
01:08:08,090 --> 01:08:13,675
[NOISE] which is just equal to six divided by eight.

1299
01:08:13,675 --> 01:08:18,024
In the Monte Carlo estimate or do TD,

1300
01:08:18,024 --> 01:08:21,179
what will be the TD estimate of B?

1301
01:08:23,109 --> 01:08:31,374
Remember what TD does is they get this S-A-R-S prime and you bootstrap.

1302
01:08:31,375 --> 01:08:36,240
You do Alpha times R plus Gamma V of S prime,

1303
01:08:37,620 --> 01:08:41,359
and then do one minus Alpha of your previous estimate.

1304
01:08:41,359 --> 01:08:44,069
What is the [inaudible].

1305
01:08:44,069 --> 01:08:46,654
Um, here in this case you can make

1306
01:08:46,654 --> 01:08:50,094
Alpha anything small you're gonna do it in infinite number of times.

1307
01:08:50,095 --> 01:08:53,200
So this is the batch data settings so for TD you're just going to

1308
01:08:53,200 --> 01:08:56,750
run over your data like millions and millions of times.

1309
01:08:57,399 --> 01:09:00,594
Until convergence basically.

1310
01:09:00,595 --> 01:09:04,450
Somebody have any guesses of what V of B would be for TD.

1311
01:09:04,450 --> 01:09:06,915
It's also three quarters.

1312
01:09:06,915 --> 01:09:08,625
It's also three quarters.

1313
01:09:08,625 --> 01:09:15,370
Okay, so for, for TD it's the same because whenever you're in B you always terminate.

1314
01:09:15,370 --> 01:09:18,005
So it's really like just a one step problem from B,

1315
01:09:18,005 --> 01:09:21,330
and so for TD it'll also say V of B

1316
01:09:21,330 --> 01:09:24,710
is equal to six divided by eight which is equal to three quarters.

1317
01:09:24,710 --> 01:09:27,475
So the two methods agree in the batch setting for this.

1318
01:09:27,475 --> 01:09:30,220
If you can go for your data an infinite amount of time,

1319
01:09:30,220 --> 01:09:34,255
V of B is equal to 3- 6 8ths since so is um,

1320
01:09:34,255 --> 01:09:36,555
ah, under both methods.

1321
01:09:36,555 --> 01:09:45,250
Um, does anybody know what V of A would be under Monte Carlo? Okay, yeah.

1322
01:09:45,250 --> 01:09:48,319
V of A under Monte Carlo is going to be 0.

1323
01:09:48,319 --> 01:09:54,269
Why? Yeah good.

1324
01:09:54,270 --> 01:09:58,980
Because the only [NOISE] only trajectory where you have an A and I will be [inaudible] from here.

1325
01:09:58,980 --> 01:10:02,625
okay there's only one trajectory we have an A and it got a zero reward.

1326
01:10:02,625 --> 01:10:05,850
What do you think might happen with TD?

1327
01:10:05,890 --> 01:10:08,610
Is it going to be 0 or non-zero?

1328
01:10:08,610 --> 01:10:09,550
Its non-zero.

1329
01:10:09,550 --> 01:10:11,290
Non-zero, why?

1330
01:10:11,620 --> 01:10:14,035
Because you bootstrap the value from A.

1331
01:10:14,035 --> 01:10:15,370
Could you bootstrap right so,

1332
01:10:15,370 --> 01:10:19,795
so yes there's only time you're in A you happen to get zero in that trajectory,

1333
01:10:19,795 --> 01:10:21,780
but this is- in TD you would say, well,

1334
01:10:21,780 --> 01:10:25,200
you got immediate reward of zero plus Gamma times V of B,

1335
01:10:25,200 --> 01:10:27,270
and V of B is three quarters.

1336
01:10:27,270 --> 01:10:28,990
So here Gamma is equal to one.

1337
01:10:28,990 --> 01:10:33,465
So your estimate of this under a TD would be three quarters.

1338
01:10:33,465 --> 01:10:37,215
We don't converge to the same thing in this case.

1339
01:10:37,215 --> 01:10:39,310
So why does this um, ah,

1340
01:10:39,310 --> 01:10:41,140
so this is what we just went through and we can

1341
01:10:41,140 --> 01:10:43,210
think about it in terms of these probabilities.

1342
01:10:43,210 --> 01:10:46,270
Um, So what is- what's happening here?

1343
01:10:46,270 --> 01:10:50,705
Monte-Carlo in the batch settings converges to the minimum mean squared error estimate.

1344
01:10:50,705 --> 01:10:54,360
So it minimizes loss with respect to the observed returns.

1345
01:10:54,360 --> 01:10:57,875
Um, and in this example V of A is equal to zero.

1346
01:10:57,875 --> 01:11:02,095
TD zero converges to the dynamic programming policy

1347
01:11:02,095 --> 01:11:06,870
with a maximum likelihood estimates for the dynamics and the reward model.

1348
01:11:07,130 --> 01:11:09,330
So it's equivalent to

1349
01:11:09,330 --> 01:11:15,110
if you essentially just- just through counting you estimated P hat of S prime given S a.

1350
01:11:15,110 --> 01:11:20,200
So for this would say the probability of going B given A is equal to 1.

1351
01:11:20,200 --> 01:11:24,525
[inaudible] Because the only time you've been on A you've went to B and then

1352
01:11:24,525 --> 01:11:30,275
the reward for B is equal to three quarters and the reward for A is equal to 0,

1353
01:11:30,275 --> 01:11:32,755
and then you would do dynamic programming with that,

1354
01:11:32,755 --> 01:11:35,660
and you would get out, get out the value.

1355
01:11:35,900 --> 01:11:42,730
So, TD is converging to this sort of maximum likelihood MDP value estimation,

1356
01:11:42,730 --> 01:11:45,930
and Monte Carlo is just converging to the mean squared error.

1357
01:11:45,930 --> 01:11:48,690
It's ignoring- well it doesn't assume Markovian.

1358
01:11:48,690 --> 01:11:51,310
So it's not using them this Markov structure. Question.

1359
01:11:51,310 --> 01:11:55,165
Just to confirm on the previous slide um,if I'm going over data

1360
01:11:55,165 --> 01:11:59,620
many times because for TD learning on the first iteration V of A would be zero, right?

1361
01:11:59,620 --> 01:12:01,720
Because V of A has [inaudible] just assuming.

1362
01:12:01,720 --> 01:12:04,810
But after a while V of B has converged to three quarters? [OVERLAPPING].

1363
01:12:04,810 --> 01:12:08,335
So, In, in, in the online setting, um,

1364
01:12:08,335 --> 01:12:10,445
if you just saw this once, ah,

1365
01:12:10,445 --> 01:12:15,010
then this- then V of A would be zero for that particular update.

1366
01:12:15,010 --> 01:12:16,370
It just that if you did it many, many,

1367
01:12:16,370 --> 01:12:19,340
many times then it would converge to this other thing.

1368
01:12:19,380 --> 01:12:21,665
So you know which one is better?

1369
01:12:21,665 --> 01:12:24,205
Well if your world is not Markovian you don't

1370
01:12:24,205 --> 01:12:27,955
want to converge to something as if it's Markovian so Monte Carlo was better.

1371
01:12:27,955 --> 01:12:30,140
But if you're world really is Markov,

1372
01:12:30,140 --> 01:12:33,515
um, then you're getting a big benefit from TD here.

1373
01:12:33,515 --> 01:12:35,515
Because it can leverage the Markov structure,

1374
01:12:35,515 --> 01:12:37,910
and so even though you never got a reward from A,

1375
01:12:37,910 --> 01:12:39,700
you can leverage the fact that you got lots of data

1376
01:12:39,700 --> 01:12:42,040
about B and use that to get better estimates.

1377
01:12:42,040 --> 01:12:44,780
Um, I encourage you to think about how you would

1378
01:12:44,780 --> 01:12:48,555
compute these sort of models like it's called certainty equivalence.

1379
01:12:48,555 --> 01:12:50,520
Where what you can do is take your data,

1380
01:12:50,520 --> 01:12:53,355
compute your estimated dynamics and reward model,

1381
01:12:53,355 --> 01:12:55,075
and then do dynamic programming with that,

1382
01:12:55,075 --> 01:12:58,530
and that often can be much more data efficient than these other methods.

1383
01:12:58,530 --> 01:13:02,670
Um, next time we'll start to talk a little bit about control. Thanks.

