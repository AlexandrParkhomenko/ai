1
00:00:05,030 --> 00:00:08,210
так что домашняя работа 2 уже вышла, я понимаю,

2
00:00:08,210 --> 00:00:10,130
что существует действительно широкий спектр

3
00:00:10,130 --> 00:00:11,809
фона с точки зрения того

4
00:00:11,809 --> 00:00:13,880
, видели ли люди глубокое обучение или нет, или посещали

5
00:00:13,880 --> 00:00:17,300
занятия, или широко использовали его,

6
00:00:17,300 --> 00:00:19,759
просто быстрый хамбо, который, если вы

7
00:00:19,759 --> 00:00:23,480
использовали тензорный поток или круговые диаграммы  перед

8
00:00:23,480 --> 00:00:26,000
некоторыми из вас, но не перед всеми, так что

9
00:00:26,000 --> 00:00:26,900
мы собираемся делать на этой неделе на

10
00:00:26,900 --> 00:00:28,970
сессиях, у нас будет

11
00:00:28,970 --> 00:00:30,830
больше информации о глубоком обучении, которым вы

12
00:00:30,830 --> 00:00:33,110
не должны стать или быть

13
00:00:33,110 --> 00:00:35,000
экспертом по глубокому обучению, чтобы быть в  этот класс, но

14
00:00:35,000 --> 00:00:36,710
мы должны иметь некоторые базовые

15
00:00:36,710 --> 00:00:38,780
навыки, чтобы делать домашнюю работу 2. Я

16
00:00:38,780 --> 00:00:40,160
смогу использовать

17
00:00:40,160 --> 00:00:41,960
аппроксимацию функций с помощью глубокой нейронной сети,

18
00:00:41,960 --> 00:00:43,700
поэтому я призываю вас пойти на занятие на этой

19
00:00:43,700 --> 00:00:45,110
неделе, если у вас нет  предыстория

20
00:00:45,110 --> 00:00:47,270
этого, сегодня мы собираемся

21
00:00:47,270 --> 00:00:49,910
немного рассказать о глубоком обучении, но очень-очень мало,

22
00:00:49,910 --> 00:00:51,829
и больше сосредоточимся на глубоком

23
00:00:51,829 --> 00:00:54,559
обучении с подкреплением, но эти

24
00:00:54,559 --> 00:00:55,850
занятия будут хорошим шансом наверстать

25
00:00:55,850 --> 00:00:58,820
упущенное в том материале, который мы также

26
00:00:58,820 --> 00:01:01,100
собираемся достичь, выпуская  б  к концу

27
00:01:01,100 --> 00:01:03,050
завтрашнего дня, каким будет проект по умолчанию

28
00:01:03,050 --> 00:01:06,050
для этого класса, и вы, ребята,

29
00:01:06,050 --> 00:01:07,369
сможете выбрать, хотите ли вы сделать

30
00:01:07,369 --> 00:01:09,470
свой собственный строительный проект или проект по

31
00:01:09,470 --> 00:01:13,369
умолчанию, и эти предложения

32
00:01:13,369 --> 00:01:16,369
будут представлены довольно скоро, чуть более чем через

33
00:01:16,369 --> 00:01:18,409
неделю.  есть любые другие вопросы, которые у

34
00:01:18,409 --> 00:01:25,970
людей есть прямо сейчас, да, вопрос

35
00:01:25,970 --> 00:01:27,890
о том, ограничено ли

36
00:01:27,890 --> 00:01:28,460
задание тензорным потоком,

37
00:01:28,460 --> 00:01:30,110
я почти уверен, что все зависит от того,

38
00:01:30,110 --> 00:01:31,700
что вы используете тензорный поток,

39
00:01:31,700 --> 00:01:34,579
так что да, просто не стесняйтесь обращаться к

40
00:01:34,579 --> 00:01:35,689
Пьяцце, чтобы перепроверить это, но  Я

41
00:01:35,689 --> 00:01:37,159
почти уверен, что все наши другие грейдеры

42
00:01:37,159 --> 00:01:38,600
только что настроили для tensorflow, поэтому для этого

43
00:01:38,600 --> 00:01:39,979
, даже если вы обычно используете факел,

44
00:01:39,979 --> 00:01:42,829
пожалуйста, используйте тензорный клей, все вы, ребята,

45
00:01:42,829 --> 00:01:44,930
также должны иметь доступ к лазурному

46
00:01:44,930 --> 00:01:47,000
кредиту, если у вас есть какие-либо вопросы о

47
00:01:47,000 --> 00:01:48,200
получении  настройте с этим, не стесняйтесь

48
00:01:48,200 --> 00:01:50,509
использовать канал Piazza Piazza, мы также

49
00:01:50,509 --> 00:01:52,310
выпустили руководство о том, как

50
00:01:52,310 --> 00:01:54,350
настроить вашу машину на прошлой неделе, поэтому, если у

51
00:01:54,350 --> 00:01:55,549
вас есть какие-либо вопросы

52
00:01:55,549 --> 00:01:57,740
, это отличное место для начала работы.

53
00:01:57,740 --> 00:01:59,030
Вы можете посмотреть учебник, вы можете посмотреть

54
00:01:59,030 --> 00:02:00,470
видео или связаться с нами по

55
00:02:00,470 --> 00:02:06,409
Piazza.

56
00:02:06,409 --> 00:02:07,219


57
00:02:07,219 --> 00:02:09,289


58
00:02:09,289 --> 00:02:11,239
очень краткий обзор

59
00:02:11,239 --> 00:02:13,280
глубокого обучения, а также глубокого ключевого

60
00:02:13,280 --> 00:02:18,080
обучения, поэтому с точки зрения того, что мы были в

61
00:02:18,080 --> 00:02:18,810
классе, мы

62
00:02:18,810 --> 00:02:20,580
обсуждали, как научиться принимать

63
00:02:20,580 --> 00:02:21,989
решения из мира, когда мы заранее не

64
00:02:21,989 --> 00:02:23,520
знаем модель динамики модели вознаграждения.

65
00:02:23,520 --> 00:02:26,310
и на прошлой неделе мы

66
00:02:26,310 --> 00:02:27,989
обсуждали аппроксимацию функции ценности, в

67
00:02:27,989 --> 00:02:29,640
частности линейную аппроксимацию функции ценности,

68
00:02:29,640 --> 00:02:31,290
и сегодня мы собираемся

69
00:02:31,290 --> 00:02:32,610
начать говорить о других формах аппроксимации функции ценности,

70
00:02:32,610 --> 00:02:34,940
в частности,

71
00:02:34,940 --> 00:02:39,660
с использованием глубоких нейронных сетей, так почему мы

72
00:02:39,660 --> 00:02:41,220
вообще хотим это сделать, причины

73
00:02:41,220 --> 00:02:43,769
мы хотели начать думать об использовании

74
00:02:43,769 --> 00:02:45,390
функции аппроксимации. Ошибка заключается в том, что если мы

75
00:02:45,390 --> 00:02:46,890
хотим иметь возможность использовать

76
00:02:46,890 --> 00:02:48,720
обучение с подкреплением для решения действительно сложных сложных

77
00:02:48,720 --> 00:02:51,630
проблем, мы должны быть в состоянии иметь дело с

78
00:02:51,630 --> 00:02:52,799
тем фактом, что часто мы собираемся  иметь

79
00:02:52,799 --> 00:02:55,110
очень многомерные входные сигналы или

80
00:02:55,110 --> 00:02:57,750
наблюдения, поэтому мы хотим иметь возможность

81
00:02:57,750 --> 00:02:59,190
иметь дело с пиксельным вводом, таким как

82
00:02:59,190 --> 00:03:00,690
изображения, или мы хотим иметь возможность иметь дело

83
00:03:00,690 --> 00:03:02,970
с действительно сложной информацией о

84
00:03:02,970 --> 00:03:05,790
клиентах, пациентах или студентах, где

85
00:03:05,790 --> 00:03:07,349
у нас может быть огромное состояние и наши

86
00:03:07,349 --> 00:03:10,110
действия  Я замечу, что сегодня

87
00:03:10,110 --> 00:03:11,459
мы не будем так много говорить об

88
00:03:11,459 --> 00:03:13,530
огромных пространствах действий, но мы будем

89
00:03:13,530 --> 00:03:14,910
много думать о действительно больших пространствах состояний,

90
00:03:14,910 --> 00:03:17,880
и поэтому, когда мы начали говорить

91
00:03:17,880 --> 00:03:19,560
о них, я утверждал, что нам либо

92
00:03:19,560 --> 00:03:21,630
нужны представления  моделей, которые

93
00:03:21,630 --> 00:03:23,400
означают тип динамики или

94
00:03:23,400 --> 00:03:29,609
модели вознаграждения T или R или R значения действия состояния

95
00:03:29,609 --> 00:03:33,840
Q или V или наши политики, которые могут

96
00:03:33,840 --> 00:03:35,670
обобщать состояния и наши действия

97
00:03:35,670 --> 00:03:37,500
с идеей, что мы фактически

98
00:03:37,500 --> 00:03:39,660
никогда не столкнемся с точно таким же состоянием

99
00:03:39,660 --> 00:03:41,340
опять же, вы можете никогда больше не увидеть то же

100
00:03:41,340 --> 00:03:43,500
самое изображение мира, но мы хотим

101
00:03:43,500 --> 00:03:44,700
иметь возможность обобщать наш прошлый

102
00:03:44,700 --> 00:03:46,920
опыт, и поэтому мы подумали о том,

103
00:03:46,920 --> 00:03:48,480
чтобы вместо таблицы представлять

104
00:03:48,480 --> 00:03:50,579
наши  функции значений, мы собирались использовать

105
00:03:50,579 --> 00:03:52,829
этот общий аппроксиматор функций, где

106
00:03:52,829 --> 00:03:54,630
у нас теперь есть W, который является некоторыми

107
00:03:54,630 --> 00:03:59,880
параметрами, и когда мы подумали об

108
00:03:59,880 --> 00:04:00,840
этом, мы сказали, что мы собираемся

109
00:04:00,840 --> 00:04:02,069
сосредоточиться на том, что мы собираемся сосредоточиться на

110
00:04:02,069 --> 00:04:03,750
аппроксимациях функций, которые являются

111
00:04:03,750 --> 00:04:06,090
дифференцируемыми и  Хорошая вещь

112
00:04:06,090 --> 00:04:08,160
о дифференцируемых представлениях заключается в

113
00:04:08,160 --> 00:04:10,319
том, что мы можем использовать наши данные, мы можем оценить

114
00:04:10,319 --> 00:04:12,540
наши параметры, а затем мы можем использовать

115
00:04:12,540 --> 00:04:13,980
градиентный спуск, чтобы попытаться подогнать нашу

116
00:04:13,980 --> 00:04:15,750
функцию, чтобы попытаться написать это и

117
00:04:15,750 --> 00:04:17,579
написать или представить нашу функцию q или

118
00:04:17,579 --> 00:04:20,820
функцию значения  поэтому я упомянул в прошлый раз,

119
00:04:20,820 --> 00:04:21,988
но большую часть времени мы будем думать

120
00:04:21,988 --> 00:04:23,910
о попытке количественно определить соответствие нашей

121
00:04:23,910 --> 00:04:26,280
функции по сравнению с истинным значением как

122
00:04:26,280 --> 00:04:28,470
среднеквадратичную ошибку, чтобы мы могли определить нашу

123
00:04:28,470 --> 00:04:30,450
потерю J, и мы можем использовать градиентный

124
00:04:30,450 --> 00:04:32,460
спуск, чтобы попытаться  чтобы найти

125
00:04:32,460 --> 00:04:35,430
параметры W, которые оптимизируют, и просто как

126
00:04:35,430 --> 00:04:37,500
напоминание, стохастический градиентный спуск был

127
00:04:37,500 --> 00:04:39,330
полезен, потому что когда мы могли просто медленно

128
00:04:39,330 --> 00:04:42,030
обновлять наши параметры по мере получения дополнительной

129
00:04:42,030 --> 00:04:44,160
информации, и эта информация сейчас

130
00:04:44,160 --> 00:04:45,990
может быть в форме эпизодов или это

131
00:04:45,990 --> 00:04:48,240
могут быть отдельные кортежи, когда я говорю

132
00:04:48,240 --> 00:04:51,590
кортеж, я обычно имею в виду пару состояний,

133
00:04:51,590 --> 00:04:56,940
вознаграждение за действие в состоянии, и приятно

134
00:04:56,940 --> 00:04:58,650
то, что ожидаемый стохастический

135
00:04:58,650 --> 00:05:00,840
градиентный спуск такой же, как и полное

136
00:05:00,840 --> 00:05:06,060
обновление градиента, поэтому просто  напомните

137
00:05:06,060 --> 00:05:07,410
себе в прошлый раз, когда мы говорили

138
00:05:07,410 --> 00:05:09,000
об аппроксиматоре функции линейного значения

139
00:05:09,000 --> 00:05:10,920
x', и это означало, что

140
00:05:10,920 --> 00:05:12,120
мы собираемся сделать целую кучу

141
00:05:12,120 --> 00:05:15,600
функций для описания нашего мира, так что вы

142
00:05:15,600 --> 00:05:17,400
знаете эти функции, которые мы будем вводить в наше

143
00:05:17,400 --> 00:05:19,140
состояние состояния как  реальное состояние

144
00:05:19,140 --> 00:05:20,780
мира, и мы выводим наши характеристики,

145
00:05:20,780 --> 00:05:24,450
и это могут быть такие вещи, как наш

146
00:05:24,450 --> 00:05:27,300
лазерный дальномер для нашего робота, который

147
00:05:27,300 --> 00:05:29,460
сообщает нам, как далеко находятся стены во

148
00:05:29,460 --> 00:05:32,970
всех направлениях на 180 градусов, мы говорили

149
00:05:32,970 --> 00:05:34,530
о том факте, что это была

150
00:05:34,530 --> 00:05:36,690
версия псевдонима  мира, потому что несколько

151
00:05:36,690 --> 00:05:39,420
коридоров могут выглядеть одинаково, поэтому наша

152
00:05:39,420 --> 00:05:40,800
функция ценности теперь представляет собой точечный продукт

153
00:05:40,800 --> 00:05:42,630
между теми характеристиками, которые мы

154
00:05:42,630 --> 00:05:45,660
получили о мире, с весами, которые наша

155
00:05:45,660 --> 00:05:47,100
целевая функция  on снова является

156
00:05:47,100 --> 00:05:48,780
среднеквадратической ошибкой, а затем мы могли бы сделать то

157
00:05:48,780 --> 00:05:51,000
же самое обновление веса, и ключевой трудностью

158
00:05:51,000 --> 00:05:52,920
было то, что мы не знали, что это

159
00:05:52,920 --> 00:05:57,870
такое, поэтому это истинная ценность политики,

160
00:05:57,870 --> 00:05:59,610
и проблема в том, что мы не знаем, что

161
00:05:59,610 --> 00:06:01,200
истинная ценность политики заключается в том, что в противном случае

162
00:06:01,200 --> 00:06:02,910
нам не пришлось бы заниматься всем этим

163
00:06:02,910 --> 00:06:04,710
обучением, и поэтому нам нужно было иметь

164
00:06:04,710 --> 00:06:07,680
разные способы приблизить ее, и

165
00:06:07,680 --> 00:06:09,060
поэтому два способа, о которых мы говорили в прошлый раз,

166
00:06:09,060 --> 00:06:11,580
были вдохновлены нашей работой по Монте-Карло

167
00:06:11,580 --> 00:06:13,830
или  в обучении TD мы могли бы либо

168
00:06:13,830 --> 00:06:18,440
подключить отдачу от полного эпизода,

169
00:06:18,440 --> 00:06:25,710
это сумма вознаграждений, либо мы могли

170
00:06:25,710 --> 00:06:28,320
бы добавить возврат с начальной загрузкой, так что теперь

171
00:06:28,320 --> 00:06:31,160
мы делаем загрузку,

172
00:06:34,600 --> 00:06:37,090
получаем награду за следующее состояние и

173
00:06:37,090 --> 00:06:39,100
значение нашего следующего состояния и  в этом случае

174
00:06:39,100 --> 00:06:40,450
мы используем аппроксиматор функции линейного значения

175
00:06:40,450 --> 00:06:42,100
x 'для всего, что

176
00:06:42,100 --> 00:06:44,380
дало нам действительно простую форму того, что представляет собой

177
00:06:44,380 --> 00:06:46,060
производная этой функции по

178
00:06:46,060 --> 00:06:47,770
отношению к W, в основном это просто наши

179
00:06:47,770 --> 00:06:52,000
функции x, по сути, это ошибка предсказания,

180
00:06:52,000 --> 00:06:55,570
поэтому люди иногда называют это

181
00:06:55,570 --> 00:06:59,200
пиар  ошибка, потому что это

182
00:06:59,200 --> 00:07:02,080
разница между значением или прямо

183
00:07:02,080 --> 00:07:04,180
сейчас мы используем GT как истинное значение,

184
00:07:04,180 --> 00:07:05,950
конечно, на самом деле это просто

185
00:07:05,950 --> 00:07:07,960
образец значения, но это разница

186
00:07:07,960 --> 00:07:11,560
между истинным значением и нашим оценочным

187
00:07:11,560 --> 00:07:12,760
значением, которое я собираюсь  уменьшить эту

188
00:07:12,760 --> 00:07:15,910
разницу, поэтому в этом случае я написал

189
00:07:15,910 --> 00:07:17,920
все эти уравнения для аппроксимации линейной функции ценности,

190
00:07:17,920 --> 00:07:19,330
но есть

191
00:07:19,330 --> 00:07:20,950
некоторые ограничения на использование аппроксимации линейной функции ценности,

192
00:07:20,950 --> 00:07:22,810
хотя

193
00:07:22,810 --> 00:07:26,070
они, вероятно, были наиболее хорошо изучены,

194
00:07:26,310 --> 00:07:29,770
поэтому, если у вас есть правильный набор функций

195
00:07:29,770 --> 00:07:31,870
и  исторически было много работы,

196
00:07:31,870 --> 00:07:33,310
я выясняю, что это за правильный набор

197
00:07:33,310 --> 00:07:35,380
функций, они часто работают очень хорошо,

198
00:07:35,380 --> 00:07:37,780
и на самом деле, когда мы углубимся, я думаю, что я

199
00:07:37,780 --> 00:07:39,460
кратко упомянул раньше, когда мы

200
00:07:39,460 --> 00:07:40,690
начали говорить о глубоких нейронных сетях, о которых вы

201
00:07:40,690 --> 00:07:42,640
можете подумать  глубокая нейронная сеть — это

202
00:07:42,640 --> 00:07:44,200
просто очень сложный способ получить

203
00:07:44,200 --> 00:07:46,060
функции, плюс последний слой представляет собой

204
00:07:46,060 --> 00:07:48,090
линейную комбинацию этих функций, но в

205
00:07:48,090 --> 00:07:50,380
большинстве случаев, когда мы говорим о

206
00:07:50,380 --> 00:07:53,320
глубоких  RL с глубокой нейронной сетью для

207
00:07:53,320 --> 00:07:54,550
представления ключевой функции, это

208
00:07:54,550 --> 00:07:55,900
тип представления, на который мы будем смотреть

209
00:07:55,900 --> 00:07:57,820
, поэтому функция линейного значения часто

210
00:07:57,820 --> 00:07:59,710
действительно работает очень хорошо, если у вас есть

211
00:07:59,710 --> 00:08:00,910
правильный набор функций, но это

212
00:08:00,910 --> 00:08:02,380
проблема того, что такое правильный набор

213
00:08:02,380 --> 00:08:04,810
функций, и есть всевозможные

214
00:08:04,810 --> 00:08:06,700
последствия того, сможем ли мы

215
00:08:06,700 --> 00:08:08,530
вообще записать

216
00:08:08,530 --> 00:08:10,930
функцию истинного значения, используя наш набор

217
00:08:10,930 --> 00:08:13,030
функций, и насколько легко нам

218
00:08:13,030 --> 00:08:16,630
сойтись к этому, поэтому одна альтернатива, которую

219
00:08:16,630 --> 00:08:17,980
мы не  не стоит так много говорить о последнем разе,

220
00:08:17,980 --> 00:08:20,080
это использовать своего рода действительно очень богатую

221
00:08:20,080 --> 00:08:23,050
функцию, аппроксимирующую наш класс, где нам

222
00:08:23,050 --> 00:08:24,340
не нужно иметь прямое

223
00:08:24,340 --> 00:08:26,740
представление функций, и некоторые

224
00:08:26,740 --> 00:08:29,950
из этих подходов основаны на ядре.

225
00:08:29,950 --> 00:08:31,930
кто-нибудь видел такие подходы, основанные на ядре

226
00:08:31,930 --> 00:08:33,789
до или как

227
00:08:33,789 --> 00:08:35,460
подходы типа k ближайших соседей или типов,

228
00:08:35,460 --> 00:08:37,360
возможно, с использованием машинного обучения вы

229
00:08:37,360 --> 00:08:38,710
слышали о K ближайших соседей,

230
00:08:38,710 --> 00:08:40,900
это непараметрические подходы, в которых

231
00:08:40,900 --> 00:08:43,030
размер вашего представления имеет тенденцию

232
00:08:43,030 --> 00:08:45,910
расти  Количество точек данных, и

233
00:08:45,910 --> 00:08:47,410
они могут быть действительно хорошими, и у них есть

234
00:08:47,410 --> 00:08:48,160


235
00:08:48,160 --> 00:08:49,600
действительно хорошие свойства сходимости для

236
00:08:49,600 --> 00:08:51,579
обучения с подкреплением, проблема в

237
00:08:51,579 --> 00:08:53,560
том, что количество необходимых вам точек данных

238
00:08:53,560 --> 00:08:57,100
имеет тенденцию масштабироваться с измерением, поэтому, если у

239
00:08:57,100 --> 00:09:01,180
вас есть, скажем, эти 180

240
00:09:01,180 --> 00:09:03,370
имеет количество точек, которое вам нужно, чтобы разбить

241
00:09:03,370 --> 00:09:04,829
это пространство на сто восемьдесят градусов,

242
00:09:04,829 --> 00:09:07,120
как правило, экспоненциально масштабируется с

243
00:09:07,120 --> 00:09:10,629
размером, поэтому это не так привлекательно

244
00:09:10,629 --> 00:09:12,370
как с точки зрения вычислительной

245
00:09:12,370 --> 00:09:14,230
сложности, требований к памяти, так и

246
00:09:14,230 --> 00:09:16,569
сложности выборки, так что они на самом деле

247
00:09:16,569 --> 00:09:18,610
имеют гораздо более сильные результаты сходимости

248
00:09:18,610 --> 00:09:20,410
по сравнению с линейным  функция значений

249
00:09:20,410 --> 00:09:22,540
приближает ошибки, но они не

250
00:09:22,540 --> 00:09:26,050
использовались очень широко, да, я

251
00:09:26,050 --> 00:09:28,240
имею в виду всех, кого только что назвали Крисом, пожалуйста,

252
00:09:28,240 --> 00:09:35,230
просто помогите мне, да, да, вопрос был в том, почему

253
00:09:35,230 --> 00:09:36,970
происходит экспоненциальное поведение,

254
00:09:36,970 --> 00:09:38,019
и многие из таких

255
00:09:38,019 --> 00:09:40,600
аппроксиматоров на основе ядра Zoar непараметрические

256
00:09:40,600 --> 00:09:44,259
интуиция подсказывает, что если вы хотите получить

257
00:09:44,259 --> 00:09:45,939
точное представление о

258
00:09:45,939 --> 00:09:47,980
своей ценности  функцию, и вы

259
00:09:47,980 --> 00:09:50,529
представляете ее, скажем, локальными точками

260
00:09:50,529 --> 00:09:52,120
вокруг нее, например, как с подходом K

261
00:09:52,120 --> 00:09:53,860
ближайших соседей, тогда

262
00:09:53,860 --> 00:09:55,209
количество точек, которое вам нужно, чтобы

263
00:09:55,209 --> 00:09:57,130
все было близко, как в эпсилон-

264
00:09:57,130 --> 00:10:00,130
шаре, масштабируется с размерностью,

265
00:10:00,130 --> 00:10:01,720
поэтому в основном вы просто  создание

266
00:10:01,720 --> 00:10:08,050
пространства, поэтому, если вы думаете о том, если вы думаете о

267
00:10:08,050 --> 00:10:10,509
том, хотите ли вы, чтобы какая-либо точка на

268
00:10:10,509 --> 00:10:13,240
этой линии была близко, тогда вы можете поставить

269
00:10:13,240 --> 00:10:16,149
точку здесь и точку здесь,

270
00:10:16,149 --> 00:10:18,040
чтобы все было как эпсилон близко

271
00:10:18,040 --> 00:10:20,470
для всех точек  на этой линии, чтобы иметь

272
00:10:20,470 --> 00:10:22,230
соседа, который находится в пределах расстояния эпсилон,

273
00:10:22,230 --> 00:10:24,430
если вы хотите, чтобы он был в квадрате,

274
00:10:24,430 --> 00:10:26,769
вам понадобятся четыре точки, чтобы

275
00:10:26,769 --> 00:10:28,810
все могло быть несколько близко к одной

276
00:10:28,810 --> 00:10:30,579
из точек, и обычно

277
00:10:30,579 --> 00:10:31,809
количество необходимых вам точек будет масштабироваться

278
00:10:31,809 --> 00:10:35,529
экспоненциально с размерностью, но

279
00:10:35,529 --> 00:10:38,769
они действительно хороши, потому что они могут

280
00:10:38,769 --> 00:10:40,480
быть гарантированно средними, о которых мы

281
00:10:40,480 --> 00:10:42,040
очень кратко говорили в прошлый раз,

282
00:10:42,040 --> 00:10:44,110
которые используют аппроксиматор функции линейного значения,

283
00:10:44,110 --> 00:10:45,149


284
00:10:45,149 --> 00:10:47,350
когда вы делаете сборку, вернулись  up это больше

285
00:10:47,350 --> 00:10:49,120
не обязательно оператор сжатия

286
00:10:49,120 --> 00:10:50,860
, вот почему вы иногда можете

287
00:10:50,860 --> 00:10:52,600
взорваться, поскольку вы знаете больше больше резервных копий

288
00:10:52,600 --> 00:10:54,639
действительно крутая вещь в апертурах - это

289
00:10:54,639 --> 00:10:57,309
своего рода их имя, когда вы используете этот

290
00:10:57,309 --> 00:10:59,439
тип приближения, вы не

291
00:10:59,439 --> 00:11:01,449
гарантируете, что они B  быть нерасширением, что

292
00:11:01,449 --> 00:11:02,200
означает, что когда вы

293
00:11:02,200 --> 00:11:03,490
покупаете их с помощью посыльного,

294
00:11:03,490 --> 00:11:05,110
поддерживающего зубы скаррана, все равно будет сокращение,

295
00:11:05,110 --> 00:11:07,360
действительно круто, так что это означает, что такие

296
00:11:07,360 --> 00:11:08,860
приблизительные ошибки гарантированно

297
00:11:08,860 --> 00:11:11,460
сойдутся по сравнению со многими другими,

298
00:11:11,460 --> 00:11:13,330
все в порядке, но они  говоря о шкале

299
00:11:13,330 --> 00:11:15,010
очень хорошо, и на практике вы, как правило,

300
00:11:15,010 --> 00:11:16,270
их не видите, хотя есть действительно

301
00:11:16,270 --> 00:11:17,740
крутая работа моего коллеги finale

302
00:11:17,740 --> 00:11:19,480
joshi из Гарварда, который думает

303
00:11:19,480 --> 00:11:20,620
об использовании их для таких вещей, как

304
00:11:20,620 --> 00:11:22,810
приложения для здравоохранения, и как вы можете

305
00:11:22,810 --> 00:11:24,370
обобщать связанные  пациентов,

306
00:11:24,370 --> 00:11:26,530
поэтому они могут быть полезны, но, как правило

307
00:11:26,530 --> 00:11:28,570
, не так хорошо масштабируются, поэтому

308
00:11:28,570 --> 00:11:30,370
сегодня мы поговорим о глубоких

309
00:11:30,370 --> 00:11:32,590
нейронных сетях, которые также имеют очень

310
00:11:32,590 --> 00:11:34,750
гибкую репутацию.  обиды, но мы надеемся,

311
00:11:34,750 --> 00:11:37,330
что теперь мы будем масштабироваться намного лучше, в

312
00:11:37,330 --> 00:11:38,590
общем, у нас почти не будет

313
00:11:38,590 --> 00:11:40,120
теоретических гарантий до конца

314
00:11:40,120 --> 00:11:44,320
сегодняшнего дня, и, но на практике они часто

315
00:11:44,320 --> 00:11:45,970
работают действительно очень хорошо, поэтому они

316
00:11:45,970 --> 00:11:47,290
стали невероятно полезным инструментом и

317
00:11:47,290 --> 00:11:49,330
обучение с подкреплением и везде на

318
00:11:49,330 --> 00:11:50,830
самом деле с точки зрения машинного обучения,

319
00:11:50,830 --> 00:11:52,480
так что мы подразумеваем под глубокими нейронными

320
00:11:52,480 --> 00:11:53,890
сетями, многие из вас, ребята, являются

321
00:11:53,890 --> 00:11:56,350
экспертами, но что это обычно означает в

322
00:11:56,350 --> 00:11:57,580
этом случае, так это то, что мы просто собираемся подумать

323
00:11:57,580 --> 00:12:00,310
о создании аппроксиматора функции,

324
00:12:00,310 --> 00:12:01,690
который  композиция из ряда

325
00:12:01,690 --> 00:12:03,250
функций, так что у нас будет наш вход

326
00:12:03,250 --> 00:12:06,370
X, и я собираюсь передать его в некоторую

327
00:12:06,370 --> 00:12:09,370
функцию, которая будет принимать некоторые

328
00:12:09,370 --> 00:12:13,240
пшеницы, так что в целом эти вещи могут

329
00:12:13,240 --> 00:12:14,680
быть векторами, так что вы должны принять некоторые

330
00:12:14,680 --> 00:12:17,350
веса  и объедините их с вашим X, а

331
00:12:17,350 --> 00:12:19,090
затем вы собираетесь вставить их в какую-то

332
00:12:19,090 --> 00:12:21,100
функцию, а затем вы собираетесь вывести

333
00:12:21,100 --> 00:12:22,480
что-то, что, вероятно,

334
00:12:22,480 --> 00:12:24,910
также будет вектором, а затем вы можете вставить это

335
00:12:24,910 --> 00:12:28,720
в другую функцию и добавить что-то

336
00:12:28,720 --> 00:12:31,330
Еще больше весов, мы будем делать это

337
00:12:31,330 --> 00:12:33,510
много раз,

338
00:12:33,510 --> 00:12:36,370
а затем, в самом конце, мы

339
00:12:36,370 --> 00:12:38,080
собираемся вывести некоторый Y, который вы можете

340
00:12:38,080 --> 00:12:40,660
представить как наш Q, а затем мы

341
00:12:40,660 --> 00:12:42,840
можем вывести это в какую-то функцию потерь.  J

342
00:12:42,840 --> 00:12:44,950
так что это значит здесь, это означает,

343
00:12:44,950 --> 00:12:51,040
что Y равно H и H n минус

344
00:12:51,040 --> 00:12:59,020
1 tot каждого из X, и я не

345
00:12:59,020 --> 00:12:59,480
написал

346
00:12:59,480 --> 00:13:00,740
весов, которые туда входят, но

347
00:13:00,740 --> 00:13:02,800
есть еще целая куча весов, а

348
00:13:02,800 --> 00:13:04,790
затем  это наше последнее, что должно понравиться

349
00:13:04,790 --> 00:13:09,680
раньше, и вы можете думать об этом, например, как о

350
00:13:09,680 --> 00:13:12,680
нашей подсказке, но это

351
00:13:12,680 --> 00:13:14,209
заглавные буквы и контролируемое обучение,

352
00:13:14,209 --> 00:13:16,070
такое как предсказание, является ли что-

353
00:13:16,070 --> 00:13:18,800
то кошкой или нет, или вы знаете

354
00:13:18,800 --> 00:13:21,560
изображение определенного объекта или для  регрессия

355
00:13:21,560 --> 00:13:23,779
и поэтому, почему мы хотим сделать это хорошо, во-

356
00:13:23,779 --> 00:13:25,339
первых, должно быть ясно, что, когда

357
00:13:25,339 --> 00:13:27,800
вы составляете множество функций, вы можете

358
00:13:27,800 --> 00:13:29,389
представлять действительно сложные функции

359
00:13:29,389 --> 00:13:31,790
, вы знаете, складывать, вычитать и

360
00:13:31,790 --> 00:13:33,410
брать полиномы. Я все виды

361
00:13:33,410 --> 00:13:34,699
вещей, которые вы могли бы делать  просто составляя

362
00:13:34,699 --> 00:13:36,079
функции вместе  э., что это может быть

363
00:13:36,079 --> 00:13:38,449
действительно мощное пространство функций, которые вы

364
00:13:38,449 --> 00:13:40,490
можете представить, но хорошая причина

365
00:13:40,490 --> 00:13:41,870
записать его в таком виде заключается в том, что вы можете

366
00:13:41,870 --> 00:13:43,430
использовать цепное правило, чтобы попытаться выполнить

367
00:13:43,430 --> 00:13:46,639
стохастический градиентный спуск, так как

368
00:13:46,639 --> 00:13:48,980
это хорошо работает, что означает, что мы можем

369
00:13:48,980 --> 00:13:52,850
запишите этого ди-джея, поэтому мы действительно хотим, чтобы вы

370
00:13:52,850 --> 00:13:54,230
знали ди-джея по всем этим

371
00:13:54,230 --> 00:13:56,690
различным параметрам, и поэтому мы можем

372
00:13:56,690 --> 00:13:58,610
записать здесь то, что мы можем записать ди-

373
00:13:58,610 --> 00:14:05,209
джея H N и D H группы d WN, и мы можем

374
00:14:05,209 --> 00:14:07,519
делать это везде.  поэтому DJ

375
00:14:07,519 --> 00:14:15,410
от H до D H от H до DHT или DW, чтобы вы могли

376
00:14:15,410 --> 00:14:17,209
использовать цепное правило для распространения всего

377
00:14:17,209 --> 00:14:20,420
этого, градиент вашей

378
00:14:20,420 --> 00:14:22,130
функции потерь будет говорить с W на всем

379
00:14:22,130 --> 00:14:23,750
пути вниз по всем этим различным

380
00:14:23,750 --> 00:14:26,360
композициям, выписав цепочку

381
00:14:26,360 --> 00:14:28,850
правило, и так что ночь будет хорошей, потому что

382
00:14:28,850 --> 00:14:30,230
это означает, что вы можете взять свой выходной

383
00:14:30,230 --> 00:14:32,959
сигнал, а затем распространить его обратно с

384
00:14:32,959 --> 00:14:35,019
точки зрения обновления всех ваших весов,

385
00:14:35,019 --> 00:14:37,790
теперь я собираюсь встречаться с собой, поэтому, когда я

386
00:14:37,790 --> 00:14:38,990
впервые узнал о глубоких нейронных сетях,

387
00:14:38,990 --> 00:14:42,170
вам нужно было сделать  это вручную и  так что, как вы

388
00:14:42,170 --> 00:14:43,730
можете себе представить, это было менее популярное

389
00:14:43,730 --> 00:14:47,720
задание, и вы назвали это обратным

390
00:14:47,720 --> 00:14:49,970
распространением, поэтому вы можете получить это

391
00:14:49,970 --> 00:14:52,279
вручную, и я расскажу через секунду о

392
00:14:52,279 --> 00:14:53,839
том, что это за функции. вы

393
00:14:53,839 --> 00:14:55,370
знаете, что вам нужны дифференцируемые функции

394
00:14:55,370 --> 00:14:57,260
для H, но я  подумайте, что одно из главных крупных

395
00:14:57,260 --> 00:14:58,639
нововведений, которые произошли там

396
00:14:58,639 --> 00:15:00,019
, примерно за последние пять

397
00:15:00,019 --> 00:15:02,029
-восемь лет, заключается в том, что есть автоматическая

398
00:15:02,029 --> 00:15:05,180
дифференциация, так что теперь вам не

399
00:15:05,180 --> 00:15:07,550
нужно управлять всеми этими градиентами

400
00:15:07,550 --> 00:15:09,350
вручную, вместо этого вы можете просто записать

401
00:15:09,350 --> 00:15:11,020
свой  сетевой параметр,

402
00:15:11,020 --> 00:15:13,870
а затем ваша сеть Prayer, которая

403
00:15:13,870 --> 00:15:15,460
включает в себя кучу параметров, а затем у

404
00:15:15,460 --> 00:15:18,010
вас есть программное обеспечение, такое как tensorflow, чтобы сделать

405
00:15:18,010 --> 00:15:20,530
все различия за вас, поэтому я

406
00:15:20,530 --> 00:15:22,600
думаю, что такие инструменты сделали его

407
00:15:22,600 --> 00:15:24,610
намного более практичным для многих

408
00:15:24,610 --> 00:15:27,250
и многих людей.  использовать глубокие нейронные

409
00:15:27,250 --> 00:15:28,930
сети, потому что вы не можете иметь

410
00:15:28,930 --> 00:15:30,340
очень-очень сложные сети с очень-

411
00:15:30,340 --> 00:15:31,510
очень большим количеством слоев, и нет

412
00:15:31,510 --> 00:15:32,860
никакого способа записать вручную,

413
00:15:32,860 --> 00:15:37,660
что  градиенты, так что же это за функции H,

414
00:15:37,660 --> 00:15:39,790
как правило, они сочетают в себе как

415
00:15:39,790 --> 00:15:41,490
линейные, так и нелинейные преобразования, в

416
00:15:41,490 --> 00:15:43,360
основном они просто должны быть

417
00:15:43,360 --> 00:15:45,990
дифференцируемыми, так что вы знаете, что эти H

418
00:15:45,990 --> 00:15:49,240
должны быть дифференцируемыми, если мы собираемся

419
00:15:49,240 --> 00:15:50,950
использовать градиентный спуск, подходит ли он им,

420
00:15:50,950 --> 00:15:55,630
и поэтому общий  варианты либо

421
00:15:55,630 --> 00:15:57,640
линейны, поэтому вы можете думать, что HN

422
00:15:57,640 --> 00:16:02,770
равно W HN минус 1, либо нелинейны, когда мы

423
00:16:02,770 --> 00:16:04,870
можем думать, что HN равно некоторой

424
00:16:04,870 --> 00:16:08,170
функции HN минус 1, и если это

425
00:16:08,170 --> 00:16:09,640
нелинейно, мы часто называем это

426
00:16:09,640 --> 00:16:16,480
функцией активации из-за времени, которое я  не

427
00:16:16,480 --> 00:16:19,150
буду много говорить в классе о

428
00:16:19,150 --> 00:16:21,070
связях с нейронными сетями,

429
00:16:21,070 --> 00:16:22,600
которые находятся внутри нашего мозга,

430
00:16:22,600 --> 00:16:23,800
что вдохновляет людей на создание таких

431
00:16:23,800 --> 00:16:26,230
искусственных нейронных сетей, но

432
00:16:26,230 --> 00:16:28,150
внутри мозга люди думают о

433
00:16:28,150 --> 00:16:29,350
таких нелинейных функциях активации,

434
00:16:29,350 --> 00:16:31,240
где, если  сигнал проходит

435
00:16:31,240 --> 00:16:32,890
определенный порог, тогда, например,

436
00:16:32,890 --> 00:16:35,530
нейрон срабатывает, и поэтому такого рода

437
00:16:35,530 --> 00:16:37,240
нелинейные функции активации могут быть

438
00:16:37,240 --> 00:16:42,160
такими, как сигмоидные функции или раллийные

439
00:16:42,160 --> 00:16:43,870
реальные  ly был особенно популярен прямо

440
00:16:43,870 --> 00:16:48,040
сейчас, поэтому вы можете выбирать различные

441
00:16:48,040 --> 00:16:49,990
комбинации линейных функций или

442
00:16:49,990 --> 00:16:52,540
нелинейных функций, и, как обычно, нам нужна

443
00:16:52,540 --> 00:16:54,700
функция потерь в конце, обычно мы

444
00:16:54,700 --> 00:16:55,930
используем среднеквадратичную ошибку, вы также можете

445
00:16:55,930 --> 00:16:57,550
использовать логарифмическую вероятность, но нам нужно что-то,

446
00:16:57,550 --> 00:16:59,860
что тогда мы можем дифференцировать  насколько

447
00:16:59,860 --> 00:17:02,320
мы близки к достижению этой цели,

448
00:17:02,320 --> 00:17:07,290
чтобы обновить наши веса, да, сначала имя,

449
00:17:07,290 --> 00:17:10,270
так что эта функция Брайлера не

450
00:17:10,270 --> 00:17:12,000
дифференцируема, грязь,

451
00:17:12,000 --> 00:17:14,909
она переменная, как вы можете, вы

452
00:17:14,909 --> 00:17:17,339
можете выбрать другое, и в последнее время

453
00:17:17,339 --> 00:17:18,959
она стала намного популярнее, чем

454
00:17:18,959 --> 00:17:25,849
сигмоид  хотя я чувствую, что он плоский

455
00:17:25,849 --> 00:17:29,490
хорошо, если он плоский, это теорема, так что в

456
00:17:29,490 --> 00:17:35,010
итоге это просто здорово, просто

457
00:17:35,010 --> 00:17:36,990
вопрос в том, как на самом деле, о, его

458
00:17:36,990 --> 00:17:38,880
много там, где он плоский, и поэтому, если ваш

459
00:17:38,880 --> 00:17:40,380
градиент 0, тогда ваши градиенты могут

460
00:17:40,380 --> 00:17:43,140
исчезнуть там и в  в общем, на самом деле

461
00:17:43,140 --> 00:17:44,490
мы вообще не будем говорить об этом

462
00:17:44,490 --> 00:17:47,130
на уроке, но, безусловно, есть проблема,

463
00:17:47,130 --> 00:17:48,690
поскольку у вас начинаются очень-очень глубокие

464
00:17:48,690 --> 00:17:51,059
нейронные сети, но из-за некоторых o  При использовании

465
00:17:51,059 --> 00:17:52,559
этих функций вы иногда можете

466
00:17:52,559 --> 00:17:54,960
получить почти полное отсутствие сигнала,

467
00:17:54,960 --> 00:17:58,559
возвращающегося к более ранним уровням, но я не

468
00:17:58,559 --> 00:17:59,970
буду говорить ни о чем из того, что будет

469
00:17:59,970 --> 00:18:01,289
нацелено, я буду говорить с некоторыми об этом на

470
00:18:01,289 --> 00:18:02,850
сеансах, и им полезно знать об этом.  of

471
00:18:02,850 --> 00:18:04,440
и мы также рады дать другие

472
00:18:04,440 --> 00:18:06,510
указатели, но да, если он плоский, все в порядке,

473
00:18:06,510 --> 00:18:15,990
вы все равно можете просто иметь 0 хорошо, хорошо,

474
00:18:15,990 --> 00:18:18,210
так почему мы хотим сделать это хорошо,

475
00:18:18,210 --> 00:18:19,770
это хорошо, что мы можем использовать это как

476
00:18:19,770 --> 00:18:21,380
гораздо более сложное представление

477
00:18:21,380 --> 00:18:24,840
другое дело, что если у вас есть хотя

478
00:18:24,840 --> 00:18:27,059
бы один скрытый слой, если у вас есть

479
00:18:27,059 --> 00:18:29,909
достаточное количество узлов, вы

480
00:18:29,909 --> 00:18:31,169
можете думать, как будто вы не знакомы

481
00:18:31,169 --> 00:18:32,669
с этим, в основном просто какой-то

482
00:18:32,669 --> 00:18:35,510
достаточно сложный набор

483
00:18:35,510 --> 00:18:38,070
комбинации функций и  функций

484
00:18:38,070 --> 00:18:40,020
это универсальный аппроксиматор функций,

485
00:18:40,020 --> 00:18:41,640
что означает, что вы можете

486
00:18:41,640 --> 00:18:42,960
представить любую функцию с помощью глубокой

487
00:18:42,960 --> 00:18:44,490
нейронной сети, так что это действительно хорошо,

488
00:18:44,490 --> 00:18:45,659
у нас не будет проблем с пропускной способностью,

489
00:18:45,659 --> 00:18:47,370
если мы будем использовать достаточно

490
00:18:47,370 --> 00:18:50,100
выразительный аппроксиматор функций и

491
00:18:50,100 --> 00:18:52,530
Это важно, потому что если вы думаете

492
00:18:52,530 --> 00:18:54,090
о том, что мы делаем с

493
00:18:54,090 --> 00:18:55,950
аппроксиматором линейной функции значения x', то ясно,

494
00:18:55,950 --> 00:18:57,330
что иногда у вас могут быть

495
00:18:57,330 --> 00:18:58,740
две ограниченные функции, и вы просто

496
00:18:58,740 --> 00:19:00,120
не сможете выразить истинную

497
00:19:00,120 --> 00:19:02,640
функцию значения для некоторых состояний и  то,

498
00:19:02,640 --> 00:19:04,820
что утверждает свойство аппроксиматора универсальной функции,

499
00:19:04,820 --> 00:19:07,740
заключается в том, что это

500
00:19:07,740 --> 00:19:11,460
не произойдет для глубокой нейронной сети, если

501
00:19:11,460 --> 00:19:15,179
она достаточно богата сейчас, конечно,

502
00:19:15,179 --> 00:19:16,169
вы всегда можете подумать о создании

503
00:19:16,169 --> 00:19:17,850
аппроксиматора функции линейного значения с очень

504
00:19:17,850 --> 00:19:19,230
очень богатыми функциями, и тогда это становится

505
00:19:19,230 --> 00:19:21,870
эквивалентным, так что учитывая, что  вы знаете, в чем еще

506
00:19:21,870 --> 00:19:23,700
одно преимущество, еще одно преимущество заключается в том, что

507
00:19:23,700 --> 00:19:25,470
потенциально вы можете

508
00:19:25,470 --> 00:19:27,510
экспоненциально меньше узлов или параметров

509
00:19:27,510 --> 00:19:29,429
по сравнению с использованием мелкой сети, что

510
00:19:29,429 --> 00:19:31,169
означает, что не так много из этих

511
00:19:31,169 --> 00:19:34,200
композиций представляют одну и ту же функцию, которая

512
00:19:34,200 --> 00:19:36,030
довольно элегантна, и я рад поговорить

513
00:19:36,030 --> 00:19:37,470
об этом в автономном режиме  или мы можем поговорить

514
00:19:37,470 --> 00:19:39,419
о Piazza, и, наконец

515
00:19:39,419 --> 00:19:40,590
, вы можете узнать параметры, используя

516
00:19:40,590 --> 00:19:45,330
стохастическое градиентное описание  хорошо, так

517
00:19:45,330 --> 00:19:47,220
что это в значительной степени то, что вы знаете сети D bro

518
00:19:47,220 --> 00:19:49,679
примерно за пять секунд, теперь мы

519
00:19:49,679 --> 00:19:50,669
собираемся немного поговорить о

520
00:19:50,669 --> 00:19:54,179
сверточных нейронных сетях, и снова

521
00:19:54,179 --> 00:19:55,500
все это будет довольно легким

522
00:19:55,500 --> 00:19:57,390
введением, потому что вам не

523
00:19:57,390 --> 00:19:58,950
понадобится  чтобы знать детали, ботаник должен

524
00:19:58,950 --> 00:20:00,600
делать домашнюю работу, за исключением в основном

525
00:20:00,600 --> 00:20:02,580
эффективного понимания того, что это

526
00:20:02,580 --> 00:20:04,799
своего рода очень выразительная функция,

527
00:20:04,799 --> 00:20:08,820
приблизительная простота, так почему мы заботимся о

528
00:20:08,820 --> 00:20:10,830
сверточных нейронных сетях, и

529
00:20:10,830 --> 00:20:12,030
они очень широко используются в

530
00:20:12,030 --> 00:20:13,740
компьютерном зрении, и если мы  Мы заинтересованы

531
00:20:13,740 --> 00:20:16,140
в том, чтобы роботы и другие виды

532
00:20:16,140 --> 00:20:17,490
агентов могли взаимодействовать в реальном

533
00:20:17,490 --> 00:20:19,230
мире. Одной из наших основных сенсорных

534
00:20:19,230 --> 00:20:21,539
модальностей является зрение, и весьма

535
00:20:21,539 --> 00:20:22,559
вероятно, что мы захотим иметь

536
00:20:22,559 --> 00:20:24,090
возможность использовать аналогичные виды входных данных для

537
00:20:24,090 --> 00:20:29,730
наших роботов и искусственных  агенты, поэтому, если

538
00:20:29,730 --> 00:20:31,830
вы думаете об этом, подумайте о том,

539
00:20:31,830 --> 00:20:33,240
что есть изображение этого случая с Эйнштейном,

540
00:20:33,240 --> 00:20:36,000
и есть целая куча разных

541
00:20:36,000 --> 00:20:38,340
пикселей на Эйнштейне, и это изображение

542
00:20:38,340 --> 00:20:40,020
Эйнштейна и  допустим, это тысяча

543
00:20:40,020 --> 00:20:41,640
на тысячу, так что это тысяча на

544
00:20:41,640 --> 00:20:45,270
тысячу, вы знаете x и y, и поэтому у нас есть от

545
00:20:45,270 --> 00:20:51,210
десяти до шести пикселей, поэтому в стандарте, который

546
00:20:51,210 --> 00:20:53,280
часто называют глубокой нейронной сетью с прямой связью, у

547
00:20:53,280 --> 00:20:56,480
вас будут все эти

548
00:20:56,480 --> 00:20:59,789
пиксели, а затем они  будет поступать в качестве

549
00:20:59,789 --> 00:21:02,610
входных данных для другого слоя, и вы можете

550
00:21:02,610 --> 00:21:03,990
захотеть иметь кучу разных узлов,

551
00:21:03,990 --> 00:21:05,789
которые получают входные данные от всех них,

552
00:21:05,789 --> 00:21:08,039
и поэтому вы можете получить огромное количество

553
00:21:08,039 --> 00:21:11,130
весов, поэтому у вас может быть от 10 до 6

554
00:21:11,130 --> 00:21:14,820
весов 4, что часто  мы

555
00:21:14,820 --> 00:21:16,140
думаем об этом, я знаю, что не дал вам

556
00:21:16,140 --> 00:21:18,270
достаточно подробностей об этом, но часто мы

557
00:21:18,270 --> 00:21:19,530
думаем, что это своего рода

558
00:21:19,530 --> 00:21:21,480
глубокая нейронная сеть, где у нас есть много

559
00:21:21,480 --> 00:21:24,120
функций параллельно, так что это не

560
00:21:24,120 --> 00:21:25,740
просто одна, но у нас может быть X

561
00:21:25,740 --> 00:21:31,679
переходя к F h1 h2 h3 h4, тогда

562
00:21:31,679 --> 00:21:33,059
все они затем каким-то

563
00:21:33,059 --> 00:21:35,120
сложным образом переходят к некоторым другим функциям,

564
00:21:35,120 --> 00:21:37,110
поэтому вы можете иметь множество

565
00:21:37,110 --> 00:21:39,780
функций, вычисляемых параллельно,

566
00:21:39,780 --> 00:21:42,130
и поэтому вы можете представить, что ваше изображение идет,

567
00:21:42,130 --> 00:21:43,510
и у вас есть один  функция, которая

568
00:21:43,510 --> 00:21:45,100
вычисляет  тестирует некоторые аспекты изображения и

569
00:21:45,100 --> 00:21:46,210
другую функцию, которая вычисляет некоторые

570
00:21:46,210 --> 00:21:47,380
другие аспекты изображения, а затем

571
00:21:47,380 --> 00:21:48,700
вы собираетесь сражаться, комбинируя эти и

572
00:21:48,700 --> 00:21:51,070
всевозможные сложные способы, так что то, что

573
00:21:51,070 --> 00:21:52,510
здесь говорится, хорошо для того самого

574
00:21:52,510 --> 00:21:53,920
первого, что, возможно, будет  вы

575
00:21:53,920 --> 00:21:55,270
знаете целую кучу n различных

576
00:21:55,270 --> 00:21:56,710
функций для вычисления изображения,

577
00:21:56,710 --> 00:21:59,200
здесь будет от 10 до 6 параметров,

578
00:21:59,200 --> 00:22:03,670
поэтому, если у нас есть только веса, умноженные на X, тогда

579
00:22:03,670 --> 00:22:06,010
это будет от 10 до 6 параметров, чтобы

580
00:22:06,010 --> 00:22:09,040
принять все это X, но  много, а

581
00:22:09,040 --> 00:22:10,630
затем, если мы хотим сделать это для параллельного выполнения

582
00:22:10,630 --> 00:22:12,070
различных типов весов,

583
00:22:12,070 --> 00:22:13,510
тогда это будет очень,

584
00:22:13,510 --> 00:22:16,420
очень большое количество параметров, и

585
00:22:16,420 --> 00:22:18,400
у нас сейчас есть много данных, но

586
00:22:18,400 --> 00:22:19,720
это все еще огромное количество

587
00:22:19,720 --> 00:22:22,150
параметров для обработки.  для представления, и это также как

588
00:22:22,150 --> 00:22:23,620
бы упускает часть того, о чем

589
00:22:23,620 --> 00:22:27,790
мы часто думаем с помощью видения, поэтому, если

590
00:22:27,790 --> 00:22:29,470
мы подумаем о том, чтобы делать это много раз и

591
00:22:29,470 --> 00:22:30,850
иметь много скрытых единиц, мы можем получить

592
00:22:30,850 --> 00:22:33,660
действительно огромное количество параметров

593
00:22:33,660 --> 00:22:37,450
, чтобы избежать своего рода  это  сложность пространства-времени

594
00:22:37,450 --> 00:22:39,310
и тот факт, что мы как

595
00:22:39,310 --> 00:22:40,960
бы игнорируем структуру изображений,

596
00:22:40,960 --> 00:22:42,820
сверточные нейронные сети пытаются

597
00:22:42,820 --> 00:22:45,070
создать особую форму глубокой нейронной

598
00:22:45,070 --> 00:22:46,930
сети, они пытаются думать о

599
00:22:46,930 --> 00:22:51,130
свойствах изображений, поэтому в конкретных

600
00:22:51,130 --> 00:22:53,320
изображениях часто есть структура и  способ,

601
00:22:53,320 --> 00:22:55,630
которым наш мозг обещает изображения, также имеет

602
00:22:55,630 --> 00:22:57,250
структуру и своего рода отличительные

603
00:22:57,250 --> 00:23:01,450
черты, пространство и частоту, поэтому, когда у

604
00:23:01,450 --> 00:23:02,830
вас есть сверточная нейронная сеть,

605
00:23:02,830 --> 00:23:04,450
мы думаем, что существуют определенные

606
00:23:04,450 --> 00:23:07,930
типы операторов, и поэтому операторы здесь снова

607
00:23:07,930 --> 00:23:10,750
похожи на наши функции H 1 и H n  как

608
00:23:10,750 --> 00:23:12,280
я уже сказал, это может быть либо линейная,

609
00:23:12,280 --> 00:23:14,500
либо нелинейная нейронная

610
00:23:14,500 --> 00:23:15,970
сеть

611
00:23:15,970 --> 00:23:19,480


612
00:23:19,480 --> 00:23:20,950


613
00:23:20,950 --> 00:23:22,210
, которую можно реализовать.

614
00:23:22,210 --> 00:23:24,910


615
00:23:24,910 --> 00:23:26,050
здесь мы собираемся разделить много

616
00:23:26,050 --> 00:23:29,160
весов, чтобы уменьшить параметр,

617
00:23:29,160 --> 00:23:32,260
поэтому вместо того, чтобы говорить, что у меня будет

618
00:23:32,260 --> 00:23:35,320
совершенно другой параметр  Каждый из них

619
00:23:35,320 --> 00:23:37,600
принимает все пиксели, и в конечном итоге у меня будут

620
00:23:37,600 --> 00:23:39,790
локальные параметры, которые

621
00:23:39,790 --> 00:23:41,320
идентичны, а затем я применяю их к

622
00:23:41,320 --> 00:23:43,060
разным частям изображения, чтобы попытаться

623
00:23:43,060 --> 00:23:47,350
извлечь, например, функции, потому что, в

624
00:23:47,350 --> 00:23:49,030
конечном счете, смысл этого в том,

625
00:23:49,030 --> 00:23:50,320
чтобы  попытаться извлечь функции,

626
00:23:50,320 --> 00:23:51,700
которые, по нашему мнению, будут полезны

627
00:23:51,700 --> 00:23:52,720
либо для предсказания вещей,

628
00:23:52,720 --> 00:23:54,549
независимо от того, знаете ли вы лица на

629
00:23:54,549 --> 00:23:56,620
изображении, либо для того, чтобы помочь нам

630
00:23:56,620 --> 00:23:58,210
понять, какой

631
00:23:58,210 --> 00:24:01,750
должна быть функция подсказки, поэтому ключевая идея, одна из  ключевая

632
00:24:01,750 --> 00:24:03,100
идея состоит в том, чтобы сказать, что у нас будет

633
00:24:03,100 --> 00:24:05,650
фильтр или восприимчивое поле, то есть

634
00:24:05,650 --> 00:24:07,120
у нас будет какая-то скрытая единица,

635
00:24:07,120 --> 00:24:08,740
так что это будет функция, которая

636
00:24:08,740 --> 00:24:11,409
применяется к какому-то предыдущему входу в

637
00:24:11,409 --> 00:24:12,280
начале, это просто будет

638
00:24:12,280 --> 00:24:14,289
подмножество нашего изображения, и вместо того, чтобы

639
00:24:14,289 --> 00:24:16,299
брать все изображение, мы просто

640
00:24:16,299 --> 00:24:17,679
возьмем часть, поэтому мы просто

641
00:24:17,679 --> 00:24:20,679
возьмем патч, так что теперь мы

642
00:24:20,679 --> 00:24:22,750
возьмем верхний угол, мы возьмем

643
00:24:22,750 --> 00:24:25,360
середину, так что это как  мы просто

644
00:24:25,360 --> 00:24:27,490
попытаемся  вычислить некоторые свойства

645
00:24:27,490 --> 00:24:31,750
определенного фрагмента изображения, и

646
00:24:31,750 --> 00:24:33,909
тогда мы можем представить себе, что он часто

647
00:24:33,909 --> 00:24:36,520
называется фильтром, который уменьшает набор

648
00:24:36,520 --> 00:24:37,929
весов, которые были применены к этому фрагменту,

649
00:24:37,929 --> 00:24:40,140
и мы могли бы делать это по всему изображению,

650
00:24:40,140 --> 00:24:42,909
и мы часто называем это фильтром.  шаг,

651
00:24:42,909 --> 00:24:44,710
который означает, насколько вы перемещаете

652
00:24:44,710 --> 00:24:47,370
этот маленький патч в каждый момент времени,

653
00:24:47,370 --> 00:24:49,270
есть также такая вещь, как нулевое

654
00:24:49,270 --> 00:24:50,679
заполнение, которое определяет, сколько ошибок нужно добавить

655
00:24:50,679 --> 00:24:52,600
на каждый входной слой, и это определяет,

656
00:24:52,600 --> 00:24:56,200
какая помощь помогает определить, каков ваш

657
00:24:56,200 --> 00:24:58,059
вывод, поэтому в этом случае  если у вас есть

658
00:24:58,059 --> 00:25:00,190
ввод 28 на 28, то у вас есть небольшой

659
00:25:00,190 --> 00:25:02,470
патч 5x5, который вы собираетесь скользить

660
00:25:02,470 --> 00:25:04,840
по всему изображению, тогда вы

661
00:25:04,840 --> 00:25:08,200
получите 24 на 24 и следующий слой,

662
00:25:08,200 --> 00:25:09,880
потому что в основном вы просто берете это, а

663
00:25:09,880 --> 00:25:12,130
затем вы  переместите его немного, вы

664
00:25:12,130 --> 00:25:15,610
переместите его, и каждый раз

665
00:25:15,610 --> 00:25:18,909
вы будете брать эти 25, так что это

666
00:25:18,909 --> 00:25:21,370
5 на 5, так что у вас будет 25 входных X,

667
00:25:21,370 --> 00:25:23,350
и вы будете умножать их на

668
00:25:23,350 --> 00:25:25,539
некоторые веса, и это будет  дать

669
00:25:25,539 --> 00:25:28,780
вам вывод так  здесь, в этом случае, это

670
00:25:28,780 --> 00:25:33,669
означает, что нам понадобится 25 недель, поэтому одна

671
00:25:33,669 --> 00:25:35,470
вещь заключается в том, что вместо того, чтобы иметь полный ввод X,

672
00:25:35,470 --> 00:25:37,120
мы просто собираемся принять его, мы

673
00:25:37,120 --> 00:25:39,159
собираемся направить разные части ввода X

674
00:25:39,159 --> 00:25:41,350
на разные нейроны, которые вы можете

675
00:25:41,350 --> 00:25:43,600
думаю, это просто разные функции,

676
00:25:43,600 --> 00:25:46,090
но другая хорошая идея здесь заключается в том, что у нас

677
00:25:46,090 --> 00:25:48,850
будут одинаковые веса для

678
00:25:48,850 --> 00:25:50,909
всего, поэтому, когда мы возьмем эти веса,

679
00:25:50,909 --> 00:25:53,530
у нас будет что-то вроде того, что вы можете думать

680
00:25:53,530 --> 00:25:54,700
о них как о попытке извлечь функцию

681
00:25:54,700 --> 00:25:56,409
из  этот подучасток изображения,

682
00:25:56,409 --> 00:25:58,980
например, есть ли край,

683
00:25:58,980 --> 00:26:00,970
чтобы вы могли себе представить, что я пытаюсь

684
00:26:00,970 --> 00:26:02,590
определить, есть ли что-

685
00:26:02,590 --> 00:26:04,330
то похожее на горизонтальный край в

686
00:26:04,330 --> 00:26:05,760
этой части изображения, и

687
00:26:05,760 --> 00:26:07,470
я тоже, и это определяется  поздно,

688
00:26:07,470 --> 00:26:09,120
поэтому я указываю, и я просто перемещаю это

689
00:26:09,120 --> 00:26:10,950
по всему моему изображению, чтобы увидеть, присутствует ли оно или

690
00:26:10,950 --> 00:26:13,110
нет, так что теперь веса

691
00:26:13,110 --> 00:26:14,790
идентичны, и вы просто перемещаете их

692
00:26:14,790 --> 00:26:16,560
по всему изображению, поэтому вместо

693
00:26:16,560 --> 00:26:18,870
того, чтобы знать вес от десяти до шести

694
00:26:18,870 --> 00:26:21,720
у меня только двадцать пять  веса, и

695
00:26:21,720 --> 00:26:24,030
я применяю их к одному и тому же, просто

696
00:26:24,030 --> 00:26:25,080
применяя их к множеству разных

697
00:26:25,080 --> 00:26:29,520
частей изображения, хорошо, так

698
00:26:29,520 --> 00:26:30,630
что это похоже на то, что у вас

699
00:26:30,630 --> 00:26:32,610
есть входные данные, вы переходите к скрытому

700
00:26:32,610 --> 00:26:34,890
слою, и да, вы сортируете  а также уменьшать

701
00:26:34,890 --> 00:26:41,520
выборку изображения, почему вы хотите

702
00:26:41,520 --> 00:26:42,630
сделать это хорошо, мы думаем, что часто

703
00:26:42,630 --> 00:26:43,800
мозг делает это, пытаясь

704
00:26:43,800 --> 00:26:45,420
уловить различные виды функций, на самом деле

705
00:26:45,420 --> 00:26:46,800
много компьютерного зрения, прежде чем глубокое

706
00:26:46,800 --> 00:26:49,080
обучение пыталось построить эти

707
00:26:49,080 --> 00:26:50,760
специальные виды  включает в себя такие вещи, как

708
00:26:50,760 --> 00:26:53,280
функции просеивания или другие функции, или которые, по

709
00:26:53,280 --> 00:26:54,960
нашему мнению, захватывают важные

710
00:26:54,960 --> 00:26:57,000
свойства изображения, которые также могут быть

711
00:26:57,000 --> 00:26:59,210
инвариантными по отношению к таким вещам, как перевод,

712
00:26:59,210 --> 00:27:01,380
потому что мы также думаем, что вы знаете

713
00:27:01,380 --> 00:27:03,390
, смотрю ли я на мир вот

714
00:27:03,390 --> 00:27:05,640
так, или я двигаю своим  немного голову, что

715
00:27:05,640 --> 00:27:07,470
особенности, которые я вижу, часто

716
00:27:07,470 --> 00:27:09,240
будут идентичными, двигаюсь ли я влево

717
00:27:09,240 --> 00:27:10,440
или вправо немного есть

718
00:27:10,440 --> 00:27:12,030
определенные существенные аспекты мира

719
00:27:12,030 --> 00:27:13,680
, которые будут иметь значение для

720
00:27:13,680 --> 00:27:15,060
обнаружения  независимо от того, есть ли лицо

721
00:27:15,060 --> 00:27:16,950
и релевантно для определения моей функции ценности,

722
00:27:16,950 --> 00:27:18,720
поэтому мы хотим отсортировать функции извлечения,

723
00:27:18,720 --> 00:27:19,920
которые, как мы думаем, будут

724
00:27:19,920 --> 00:27:21,780
представлять такого рода инвариантность перевода,

725
00:27:21,780 --> 00:27:26,760
так что это также означает, что

726
00:27:26,760 --> 00:27:28,830
вместо того, чтобы просто вычислять, что вы можете

727
00:27:28,830 --> 00:27:30,300
сделать это, вы используете то же самое  веса

728
00:27:30,300 --> 00:27:32,520
по всему объекту по всему

729
00:27:32,520 --> 00:27:33,930
изображению, а затем вы можете сделать это для

730
00:27:33,930 --> 00:27:36,900
нескольких различных типов объектов, так

731
00:27:36,900 --> 00:27:40,050
что есть действительно хорошее обсуждение этого,

732
00:27:40,050 --> 00:27:42,000
которое углубляется с 2:31 и

733
00:27:42,000 --> 00:27:43,340
которое некоторые из вас, ребята, могли взять

734
00:27:43,340 --> 00:27:45,690
и  есть хорошая анимация, где они

735
00:27:45,690 --> 00:27:47,250
показывают хорошо, представьте, что вы вводите данные,

736
00:27:47,250 --> 00:27:49,560
можете думать, что это изображение, а затем

737
00:27:49,560 --> 00:27:51,390
вы можете применить эти различные фильтры

738
00:27:51,390 --> 00:27:53,070
поверх него, но вы можете думать о

739
00:27:53,070 --> 00:27:54,660
попытке обнаружить различные функции, а

740
00:27:54,660 --> 00:27:56,100
затем вы перемещаете их по своему  изображение и

741
00:27:56,100 --> 00:27:57,510
посмотрите, присутствует ли эта функция

742
00:27:57,510 --> 00:27:59,640
где-либо, чтобы вы могли сделать это с

743
00:27:59,640 --> 00:28:01,350
несколькими различными типами фильтров, вы

744
00:28:01,350 --> 00:28:02,460
могли бы думать об этом как о попытке

745
00:28:02,460 --> 00:28:03,930
найти что-то  например, или

746
00:28:03,930 --> 00:28:06,060
некоторые вещи, которые вы знаете по горизонтали или по

747
00:28:06,060 --> 00:28:08,600
вертикали, различные типы краев, и

748
00:28:08,600 --> 00:28:10,650
они дают вам различные функции, по

749
00:28:10,650 --> 00:28:14,160
существу, которые были извлечены.

750
00:28:14,160 --> 00:28:16,050
Другая действительно важная вещь в cnn - это

751
00:28:16,050 --> 00:28:17,820
то, что известно как объединение слоев, они

752
00:28:17,820 --> 00:28:19,350
часто используются как способ сортировки  уменьшите

753
00:28:19,350 --> 00:28:21,480
образец изображения, чтобы вы могли делать такие вещи,

754
00:28:21,480 --> 00:28:23,460
как максимальное издевательство, чтобы определить, присутствует ли

755
00:28:23,460 --> 00:28:26,250
конкретная функция или нет, или

756
00:28:26,250 --> 00:28:28,530
взять средние значения или другие способы, чтобы

757
00:28:28,530 --> 00:28:30,600
просто уменьшить и сжать

758
00:28:30,600 --> 00:28:32,880
информацию, которую вы получили, просто

759
00:28:32,880 --> 00:28:34,740
помните в этом случае и в  во многих случаях

760
00:28:34,740 --> 00:28:36,120
мы собираемся начать с действительно

761
00:28:36,120 --> 00:28:38,340
многомерных входных данных, таких как X, может быть

762
00:28:38,340 --> 00:28:42,539
изображением, выходным масштабатором, как вы знаете

763
00:28:42,539 --> 00:28:44,309
значение Q, поэтому нам каким-то образом

764
00:28:44,309 --> 00:28:45,510
придется пойти на действительно многомерный

765
00:28:45,510 --> 00:28:47,549
вход и своего рода среднее значение.  и замедляться до

766
00:28:47,549 --> 00:28:50,070
тех пор, пока мы не сможем получить низкоразмерный

767
00:28:50,070 --> 00:28:54,450
вывод, чтобы последний слой, как правило, был

768
00:28:54,450 --> 00:28:56,309
полностью подключен, чтобы мы могли снова подумать

769
00:28:56,309 --> 00:28:58,530
обо всех этих предыдущих процессах, по

770
00:28:58,530 --> 00:29:00,510
сути, вычисляя какую-то новую функцию.

771
00:29:00,510 --> 00:29:04,409
представление, так что, по сути, отсюда

772
00:29:04,409 --> 00:29:06,179
и сюда мы как бы вычисляем это новое

773
00:29:06,179 --> 00:29:08,100
представление функции изображения, и

774
00:29:08,100 --> 00:29:10,740
в самом конце мы можем взять некоторый полностью

775
00:29:10,740 --> 00:29:13,440
связанный слой, который мы как будто делаем линейной

776
00:29:13,440 --> 00:29:15,360
регрессией, и снова использовать его для вывода

777
00:29:15,360 --> 00:29:19,200
прогнозов или скаляров. Я знаю

778
00:29:19,200 --> 00:29:20,909
либо  для некоторых из вас, ребята, это своего

779
00:29:20,909 --> 00:29:22,919
рода краткое освежение для других

780
00:29:22,919 --> 00:29:24,929
из вас, это явно не в этом, это

781
00:29:24,929 --> 00:29:28,080
было бы вихревое введение, но

782
00:29:28,080 --> 00:29:29,760
мы не будем требовать, чтобы вы знали много

783
00:29:29,760 --> 00:29:31,080
этих деталей, и снова просто перейдите на

784
00:29:31,080 --> 00:29:32,700
сессию.  если у вас есть какие-то вопросы, и не

785
00:29:32,700 --> 00:29:36,659
стесняйтесь обращаться к нам, хорошо, так что

786
00:29:36,659 --> 00:29:38,909
эти типы представлений, как глубокие

787
00:29:38,909 --> 00:29:40,320
нейронные сети, так и сверточные нейронные

788
00:29:40,320 --> 00:29:42,090
сети, широко используются и

789
00:29:42,090 --> 00:29:45,240
глубокое обучение с подкреплением, так что это было

790
00:29:45,240 --> 00:29:50,400
примерно в 2014 году, когда я семинар,

791
00:29:50,400 --> 00:29:51,780
где Дэвид Сильвер начал говорить  о

792
00:29:51,780 --> 00:29:53,520
том, как мы могли использовать эти

793
00:29:53,520 --> 00:29:56,070
приближения для Atari, так почему же это было

794
00:29:56,070 --> 00:29:57,210
удивительно, я просто хочу

795
00:29:57,210 --> 00:30:01,710
вернуться, так что примерно в 1994 году примерно в

796
00:30:01,710 --> 00:30:07,289
1994 году у нас была телевизионная ба  ckgammon, в котором использовались

797
00:30:07,289 --> 00:30:09,570
глубокие нейронные сети, хорошо, что они использовали, знают,

798
00:30:09,570 --> 00:30:11,010
как это работает. Я думаю, что кто-то был

799
00:30:11,010 --> 00:30:14,159
слишком глубоким, и из этого они стали как

800
00:30:14,159 --> 00:30:16,380
игрок в нарды мирового класса, так что это

801
00:30:16,380 --> 00:30:18,960
было довольно рано, и тогда у нас были

802
00:30:18,960 --> 00:30:20,280
результаты, которые как бы происходили.

803
00:30:20,280 --> 00:30:24,440
примерно в 1995 году - может быть, в 1998 году, в

804
00:30:24,440 --> 00:30:28,520
котором говорилось, что аппроксимация функции

805
00:30:28,520 --> 00:30:34,159
плюс автономный контроль политик

806
00:30:34,159 --> 00:30:41,669
Плюс начальная загрузка может быть плохой, вы можете

807
00:30:41,669 --> 00:30:45,090
не сойтись, мы говорили об этом

808
00:30:45,090 --> 00:30:48,360
немного в прошлый раз, что в целом, как только

809
00:30:48,360 --> 00:30:49,620
мы начинаем делать эту аппроксимацию функции,

810
00:30:49,620 --> 00:30:51,390
даже  аппроксиматор линейной функции,

811
00:30:51,390 --> 00:30:53,429
который, когда вы

812
00:30:53,429 --> 00:30:55,440
комбинируете начальную загрузку управления политикой, что

813
00:30:55,440 --> 00:30:57,059
означает, что мы делаем как TD-обучение или Q-

814
00:30:57,059 --> 00:31:00,090
обучение, и аппроксиматор функции,

815
00:31:00,090 --> 00:31:02,240
тогда вы можете начать иметь эту

816
00:31:02,240 --> 00:31:04,980
сложную триаду, которая часто означает, что

817
00:31:04,980 --> 00:31:07,169
мы не гарантируем сходимость  и

818
00:31:07,169 --> 00:31:08,700
даже мы гарантированно сойдемся,

819
00:31:08,700 --> 00:31:11,190
решение может быть не очень хорошим, так что

820
00:31:11,190 --> 00:31:12,510
вроде как был этот ранний обнадеживающий

821
00:31:12,510 --> 00:31:13,880
успех, а затем были эти результаты.

822
00:31:13,880 --> 00:31:16,110
Это было где-то в середине 90-х

823
00:31:16,110 --> 00:31:17,429
, когда пытались лучше понять

824
00:31:17,429 --> 00:31:19,230
это, что указывало на то, что все может быть

825
00:31:19,230 --> 00:31:20,640
очень плохо, и в

826
00:31:20,640 --> 00:31:21,990
дополнение к теоретическим

827
00:31:21,990 --> 00:31:24,200
результатам были такие простые тестовые примеры,

828
00:31:24,200 --> 00:31:27,240
которые вы знаете, эти простые случаи, которые

829
00:31:27,240 --> 00:31:28,980
пошло не так, так что это не просто в

830
00:31:28,980 --> 00:31:31,590
принципе могло случиться, но

831
00:31:31,590 --> 00:31:36,929
были случаи, которые потерпели неудачу, и поэтому я думаю,

832
00:31:36,929 --> 00:31:38,460
что долгое время после этого

833
00:31:38,460 --> 00:31:40,049
сообщество как бы отступало от

834
00:31:40,049 --> 00:31:41,580
глубоких нейронных сетей на какое-то время люди

835
00:31:41,580 --> 00:31:43,260
были весьма осторожны  об их использовании,

836
00:31:43,260 --> 00:31:44,640
потому что были явно даже простые

837
00:31:44,640 --> 00:31:45,929
случаи, когда дела шли очень

838
00:31:45,929 --> 00:31:47,510
плохо с аппроксимацией функций, и

839
00:31:47,510 --> 00:31:49,679
теоретически люди могли доказать, что это

840
00:31:49,679 --> 00:31:51,720
могло пойти плохо, и

841
00:31:51,720 --> 00:31:53,460
поэтому некоторое время этому уделялось меньше внимания, а

842
00:31:53,460 --> 00:31:55,950
затем возник глубокий рост  нейронные

843
00:31:55,950 --> 00:31:57,720
сети и, как вы знаете,

844
00:31:57,720 --> 00:32:04,980
середина 2000-х, как сейчас, так что глубокие нейронные

845
00:32:04,980 --> 00:32:07,919
сети стали огромными, и

846
00:32:07,919 --> 00:32:09,510
в них явно был огромный успех для таких вещей,

847
00:32:09,510 --> 00:32:11,220
как зрение и  в других областях есть

848
00:32:11,220 --> 00:32:12,330
целая куча данных, есть целая

849
00:32:12,330 --> 00:32:13,860
куча вычислений, они получают

850
00:32:13,860 --> 00:32:15,750
действительно выдающиеся результаты, и тогда,

851
00:32:15,750 --> 00:32:17,340
возможно, было естественно, что примерно

852
00:32:17,340 --> 00:32:24,390
в 2014 году, не могли бы вы объединить

853
00:32:24,390 --> 00:32:25,890
их, и у Atari были действительно потрясающие

854
00:32:25,890 --> 00:32:28,260
успехи и так далее?  Я думаю,

855
00:32:28,260 --> 00:32:29,610
что это действительно изменило историю того,

856
00:32:29,610 --> 00:32:31,620
как люди воспринимают использование такого

857
00:32:31,620 --> 00:32:32,580
рода сложных функций,

858
00:32:32,580 --> 00:32:33,240
аппроксимирующих

859
00:32:33,240 --> 00:32:36,720
матерей и RL, и что да, это может

860
00:32:36,720 --> 00:32:38,730
не сойтись, да, все может пойти очень

861
00:32:38,730 --> 00:32:40,020
плохо, и иногда они действительно идут очень плохо

862
00:32:40,020 --> 00:32:42,990
на практике, но это так.  также

863
00:32:42,990 --> 00:32:44,850
часто возможно, что, несмотря на то, что вы

864
00:32:44,850 --> 00:32:46,169
знаете тот факт, что мы не всегда полностью

865
00:32:46,169 --> 00:32:47,070
понимаем, что

866
00:32:47,070 --> 00:32:49,110
они всегда работают, что часто на практике

867
00:32:49,110 --> 00:32:50,760
мы все еще можем получить довольно хорошие

868
00:32:50,760 --> 00:32:52,440
политики сейчас, мы часто не знаем, являются ли они

869
00:32:52,440 --> 00:32:53,760
оптимальными, часто мы знаем, что они  не

870
00:32:53,760 --> 00:32:55,110
оптимальны, потому что мы знаем, что люди могут

871
00:32:55,110 --> 00:32:56,700
играть лучше, но это не значит, что

872
00:32:56,700 --> 00:32:58,620
они могут быть не очень хороши, и поэтому мы как

873
00:32:58,620 --> 00:33:00,000
бы заметили возрождение

874
00:33:00,000 --> 00:33:04,070
интереса к глубокому армированию.  да,

875
00:33:04,070 --> 00:33:08,220
я думаю, с вашей

876
00:33:08,220 --> 00:33:10,440
точки зрения, есть что-то, что глубокое обучение

877
00:33:10,440 --> 00:33:12,390
решило проблемы, которые они

878
00:33:12,390 --> 00:33:14,190
придумали в середине 90-х, или

879
00:33:14,190 --> 00:33:16,470
это просто за счет увеличения

880
00:33:16,470 --> 00:33:18,330
вычислительной мощности и способности

881
00:33:18,330 --> 00:33:21,210
собирать много данных?  что, когда он потерпел неудачу,

882
00:33:21,210 --> 00:33:23,100
это не имеет значения, и мы можем попробовать что-

883
00:33:23,100 --> 00:33:25,950
то другое, как мы, вы знаете, попробовать это

884
00:33:25,950 --> 00:33:27,720
снова, как бы собрать это вместе, а затем

885
00:33:27,720 --> 00:33:29,190
просто продолжать пытаться, пока это не сработает. Я думаю,

886
00:33:29,190 --> 00:33:30,810
мой вопрос в том, действительно ли мы преодолели

887
00:33:30,810 --> 00:33:32,250
какие-либо проблемы.  которые возникли в

888
00:33:32,250 --> 00:33:33,840
конце 90-х, или это просто то, что мы просто как

889
00:33:33,840 --> 00:33:36,390
бы прорабатываем вопрос, как вы

890
00:33:36,390 --> 00:33:37,440
знаете, помогая как бы фундаментально

891
00:33:37,440 --> 00:33:38,670
решить некоторые проблемы конца

892
00:33:38,670 --> 00:33:41,400
90-х, или мы как бы грубо форсируем это,

893
00:33:41,400 --> 00:33:43,980
я думаю, что некоторые  из проблем, которые

894
00:33:43,980 --> 00:33:46,650
возникали в

895
00:33:46,650 --> 00:33:47,970
1995-1988 годах в плане сходимости, сейчас есть некоторые

896
00:33:47,970 --> 00:33:50,070
алгоритмы, которые являются более истинными

897
00:33:50,070 --> 00:33:51,420
алгоритмами стохастического градиента, которые

898
00:33:51,420 --> 00:33:54,330
рассматриваются в главе 11, так что я

899
00:33:54,330 --> 00:33:57,810
гарантированно сойдусь, они могут  не

900
00:33:57,810 --> 00:33:58,860
гарантируется сходимость оптимальной

901
00:33:58,860 --> 00:34:00,750
политики, так что

902
00:34:00,750 --> 00:34:02,190
еще много работы, которую, я думаю, нужно сделать,

903
00:34:02,190 --> 00:34:03,720
чтобы понять функцию, аппроксимирующую

904
00:34:03,720 --> 00:34:07,470
управление политикой и начальную загрузку. Я думаю, что

905
00:34:07,470 --> 00:34:09,210
есть также пара алгоритмических идей,

906
00:34:09,210 --> 00:34:10,860
которые мы увидим позже в  эта

907
00:34:10,860 --> 00:34:13,290
лекция, которая помогает производительности

908
00:34:13,290 --> 00:34:14,790
избежать некоторых из этих

909
00:34:14,790 --> 00:34:16,889
проблем с конвергенцией, поэтому я думаю, что люди знали об

910
00:34:16,889 --> 00:34:18,389
этом, когда они начали входить в 2013

911
00:34:18,389 --> 00:34:20,489
2014, и поэтому они пытались хорошо подумать о том,

912
00:34:20,489 --> 00:34:22,679
когда может произойти эта проблема и

913
00:34:22,679 --> 00:34:24,179
как мы можем избежать некоторых из этих вещей

914
00:34:24,179 --> 00:34:25,739
как то, что вызывает это, и поэтому, по крайней мере,

915
00:34:25,739 --> 00:34:27,270
алгоритмически, можем ли мы попытаться сделать то,

916
00:34:27,270 --> 00:34:29,070
что люди часто говорят о стабильности,

917
00:34:29,070 --> 00:34:31,350
поэтому можем ли мы попытаться убедиться, что глубокая

918
00:34:31,350 --> 00:34:32,730
нейронная сеть, кажется, не начинает

919
00:34:32,730 --> 00:34:34,199
иметь веса, которые уходят

920
00:34:34,199 --> 00:34:35,429
в бесконечность и, по крайней мере,

921
00:34:35,429 --> 00:34:36,870
эмпирически  иметь более стабильную

922
00:34:36,870 --> 00:34:43,500
производительность да, вы

923
00:34:43,500 --> 00:34:45,239
избежали проблем или они действительно были опробованы с помощью

924
00:34:45,239 --> 00:34:47,370
контроля политики, я просто не

925
00:34:47,370 --> 00:34:48,750
знаю, я думал, что я для  что-то, где это

926
00:34:48,750 --> 00:34:51,179
было не так, что на самом деле и в обычном

927
00:34:51,179 --> 00:34:53,699
эксперименте они обновили производительность,

928
00:34:53,699 --> 00:34:56,030
чтобы измерить оптимальные качели, раскрывающие пирог,

929
00:34:56,030 --> 00:35:00,680
или проблемный мальчик, милый, это или

930
00:35:00,680 --> 00:35:02,960
нет, если я должен в настоящее время

931
00:35:02,960 --> 00:35:04,610
в случае с Atari, как вы знаете, где

932
00:35:04,610 --> 00:35:05,690
они  измените вещи, чтобы они были больше связаны с

933
00:35:05,690 --> 00:35:08,300
политикой или которые, как мы знаем, могут быть намного более

934
00:35:08,300 --> 00:35:10,400
стабильными, и они проводят глубокое обучение,

935
00:35:10,400 --> 00:35:12,830
в этом случае глубокое обучение Q, и поэтому оно

936
00:35:12,830 --> 00:35:14,240
все еще может быть очень нестабильным, но они

937
00:35:14,240 --> 00:35:16,010
собираются что-то сделать с тем, как они делают

938
00:35:16,010 --> 00:35:17,930
частоту  обновлений в сетях, чтобы

939
00:35:17,930 --> 00:35:20,510
попытаться сделать его более стабильным, и ваши

940
00:35:20,510 --> 00:35:22,190
замечательные вопросы позвольте мне посмотреть, как это

941
00:35:22,190 --> 00:35:27,350
работает, хорошо, круто, хорошо, мы

942
00:35:27,350 --> 00:35:29,000
вскоре увидим пример для прорыва того, что

943
00:35:29,000 --> 00:35:31,610
они сделали, поэтому снова прямо сейчас мы собираемся

944
00:35:31,610 --> 00:35:32,840
говорить об использовании глубоких нейронных

945
00:35:32,840 --> 00:35:34,720
сетей для представления функции ценности

946
00:35:34,720 --> 00:35:36,830
говорить об использовании глубоких нейронных сетей для

947
00:35:36,830 --> 00:35:38,690
представления политики довольно коротко

948
00:35:38,690 --> 00:35:41,840
мило, так что мы будем делать, у

949
00:35:41,840 --> 00:35:43,250
нас снова будут наши веса, у нас будут те

950
00:35:43,250 --> 00:35:44,930
же приближения  т. е. ошибаюсь, теперь мы

951
00:35:44,930 --> 00:35:47,950
собираемся использовать глубокие нейронные сети,

952
00:35:47,950 --> 00:35:52,760
и в этом случае, опять же, просто для ясности,

953
00:35:52,760 --> 00:35:54,200
мы собираемся использовать функцию подсказки,

954
00:35:54,200 --> 00:35:56,060
потому что мы хотим иметь возможность

955
00:35:56,060 --> 00:35:59,020
управлять, поэтому мы собираемся

956
00:35:59,020 --> 00:36:02,360
в этом случае мы делаем контроль, и поэтому нам

957
00:36:02,360 --> 00:36:04,340
нужно будет изучить значения

958
00:36:04,340 --> 00:36:06,380
действий, просто чтобы быть ясным здесь Atari,

959
00:36:06,380 --> 00:36:07,790
как правило, не имеет действительно

960
00:36:07,790 --> 00:36:09,650
многомерного пространства действий, оно дискретно

961
00:36:09,650 --> 00:36:11,360
, обычно где-то между

962
00:36:11,360 --> 00:36:13,970
4 и 18 зависит  в игре, так что это довольно

963
00:36:13,970 --> 00:36:16,970
низкая деменция, довольно дискретная и

964
00:36:16,970 --> 00:36:19,430
довольно маленькая, поэтому, хотя пространство состояний

965
00:36:19,430 --> 00:36:20,960
огромно, потому что это пиксели, это

966
00:36:20,960 --> 00:36:23,930
изображения. Я пространство действия довольно

967
00:36:23,930 --> 00:36:28,130
дымное, так что просто как напоминание для

968
00:36:28,130 --> 00:36:30,650
q-обучения, что мы видели, это q-обучение

969
00:36:30,650 --> 00:36:34,250
выглядит так: для наших весов у нас

970
00:36:34,250 --> 00:36:35,360
должна быть производная от нашей функции,

971
00:36:35,360 --> 00:36:37,070
она больше не обязательно будет

972
00:36:37,070 --> 00:36:38,030
линейной,

973
00:36:38,030 --> 00:36:40,400
но способ, которым мы обновили наши веса, заключался в том, что

974
00:36:40,400 --> 00:36:43,490
мы сделали резервную копию TD, где у нас есть

975
00:36:43,490 --> 00:36:46,610
эта цель, но теперь мы собираемся

976
00:36:46,610 --> 00:36:48,920
бери их по максимуму  r a над нашим следующим

977
00:36:48,920 --> 00:36:52,790
состоянием в нашем действии, и наши веса теперь

978
00:36:52,790 --> 00:36:55,100
замечают, что в этом уравнении все W, которые вы

979
00:36:55,100 --> 00:36:56,990
видите, идентичны в правой части,

980
00:36:56,990 --> 00:37:00,410
поэтому мы используем те же веса для

981
00:37:00,410 --> 00:37:02,450
представления нашего текущего значения, теперь мы

982
00:37:02,450 --> 00:37:04,160
используем наши те же веса для  подключитесь и

983
00:37:04,160 --> 00:37:06,590
получите оценку нашей будущей стоимости,

984
00:37:06,590 --> 00:37:08,300
а также нашей производной, и одна вещь, которую

985
00:37:08,300 --> 00:37:09,500
мы увидим,

986
00:37:09,500 --> 00:37:14,090
это альтернатива этому, хорошо, поэтому их

987
00:37:14,090 --> 00:37:15,950
идея заключалась в том, что мы действительно хотели бы иметь возможность

988
00:37:15,950 --> 00:37:17,690
использовать этот тип аппроксиматора функций.

989
00:37:17,690 --> 00:37:19,099
в этих глубоких функциях аппроксиматор

990
00:37:19,099 --> 00:37:20,270
должен сделать Atari,

991
00:37:20,270 --> 00:37:22,670
они выбрали Atari частично, я думаю,

992
00:37:22,670 --> 00:37:24,590
по крайней мере, Джеймисон, я думаю, что он был Дэвидом, у

993
00:37:24,590 --> 00:37:27,190
обоих был своего рода совместный стартап по

994
00:37:27,190 --> 00:37:30,109
видеоиграм давным-давно, я думаю, что это

995
00:37:30,109 --> 00:37:31,280
было до того, как Дэвид вернулся в

996
00:37:31,280 --> 00:37:32,990
аспирантуру  если я правильно помню,

997
00:37:32,990 --> 00:37:34,250
то они оба заинтересованы в этом,

998
00:37:34,250 --> 00:37:37,070
очевидно, что игры часто сложны для изучения людьми,

999
00:37:37,070 --> 00:37:39,040
так что это хорошая

1000
00:37:39,040 --> 00:37:41,240
демонстрация интеллекта, и они

1001
00:37:41,240 --> 00:37:43,070
подумали, что мы можем получить к этому доступ,

1002
00:37:43,070 --> 00:37:44,590
и была опубликована статья.

1003
00:37:44,590 --> 00:37:48,290
забывчивый человек  может быть, 2011-2013 говорят

1004
00:37:48,290 --> 00:37:50,450
об играх и эмуляторах Atari как о

1005
00:37:50,450 --> 00:37:53,540
своего рода интересном вызове для RL, так

1006
00:37:53,540 --> 00:37:55,010
что что происходит в этом случае, состояние

1007
00:37:55,010 --> 00:37:57,710
будет просто полным изображением,

1008
00:37:57,710 --> 00:37:59,210
действие будет эквивалентно

1009
00:37:59,210 --> 00:38:00,590
действиям, которые вы могли бы обычно  делать

1010
00:38:00,590 --> 00:38:01,849
в игре это обычно где-то

1011
00:38:01,849 --> 00:38:04,820
от 4 до 18, примерно от 4 до 18

1012
00:38:04,820 --> 00:38:08,690
действий, и награда может быть на

1013
00:38:08,690 --> 00:38:10,130
самом деле какой угодно, но вы можете

1014
00:38:10,130 --> 00:38:13,550
использовать счет или какой-то другой аспект.

1015
00:38:13,550 --> 00:38:15,260


1016
00:38:15,260 --> 00:38:19,550
произойдет

1017
00:38:19,550 --> 00:38:21,290
хорошо, они будут использовать определенное

1018
00:38:21,290 --> 00:38:23,359
состояние ввода, о котором мы говорили ранее, о

1019
00:38:23,359 --> 00:38:26,000
том, является ли представление марковским или нет,

1020
00:38:26,000 --> 00:38:29,330
в этих играх вам обычно

1021
00:38:29,330 --> 00:38:32,690
нужна скорость, поэтому, поскольку вам нужна

1022
00:38:32,690 --> 00:38:34,190
скорость, вам нужно больше, чем просто

1023
00:38:34,190 --> 00:38:36,650
текущее изображение, поэтому  то, что они решили сделать,

1024
00:38:36,650 --> 00:38:40,040
это то, что вам нужно использовать для предыдущих кадров,

1025
00:38:40,040 --> 00:38:42,200
так что это меньше всего позволит вам

1026
00:38:42,200 --> 00:38:45,200
поймать скорость в положении. Я

1027
00:38:45,200 --> 00:38:48,260
вроде шаров и тому подобного, этого не

1028
00:38:48,260 --> 00:38:52,040
всегда достаточно.  кто-нибудь

1029
00:38:52,040 --> 00:38:53,990
может привести пример, где может быть игра Atari, я

1030
00:38:53,990 --> 00:38:55,190
не знаю, сколько людей играло в Atari,

1031
00:38:55,190 --> 00:38:58,910
эм, я этого может быть недостаточно, где

1032
00:38:58,910 --> 00:39:00,920
вы знаете, что для последних четырех изображений все еще

1033
00:39:00,920 --> 00:39:02,780
может быть недостаточно, или тип

1034
00:39:02,780 --> 00:39:06,460
игры, где этого может быть недостаточно  Достаточно

1035
00:39:16,380 --> 00:39:18,490
моих чипсов точно так же, как в таких вещах,

1036
00:39:18,490 --> 00:39:19,660
как «Месть Монтесумы», где

1037
00:39:19,660 --> 00:39:21,160
вам часто нужно получить что-то вроде ключа, а

1038
00:39:21,160 --> 00:39:22,680
затем вы должны взять этот ключ, и тогда,

1039
00:39:22,680 --> 00:39:25,420
возможно, он будет виден в фильмах на экране,

1040
00:39:25,420 --> 00:39:27,310
а может быть, он хранится

1041
00:39:27,310 --> 00:39:29,070
где-то в инвентаре, так что вам нужно сортировать

1042
00:39:29,070 --> 00:39:31,330
помнить, что он у вас есть, чтобы

1043
00:39:31,330 --> 00:39:32,500
принять правильное решение намного позже

1044
00:39:32,500 --> 00:39:33,700
или может быть какая-то информация, которую вы

1045
00:39:33,700 --> 00:39:35,290
видели раньше, так что есть много

1046
00:39:35,290 --> 00:39:37,990
игр много задач, где даже

1047
00:39:37,990 --> 00:39:39,280
последние четыре кадра не дадут мне

1048
00:39:39,280 --> 00:39:41,590
информация, которая вам нужна, но это неплохое

1049
00:39:41,590 --> 00:39:42,940
приближение, это намного проще, чем

1050
00:39:42,940 --> 00:39:44,650
представление всей истории, поэтому они

1051
00:39:44,650 --> 00:39:48,310
начали с этого, так что здесь, в этом случае,

1052
00:39:48,310 --> 00:39:50,980
есть 18 позиций кнопок джойстика,

1053
00:39:50,980 --> 00:39:52,210
может или не нужно использовать их все  в

1054
00:39:52,210 --> 00:39:54,160
конкретной игре, и наградой может

1055
00:39:54,160 --> 00:39:57,100
быть изменение счета. Теперь обратите внимание, что это

1056
00:39:57,100 --> 00:39:59,590
может быть очень полезно, а может и нет, это

1057
00:39:59,590 --> 00:40:01,210
зависит от игры, поэтому в некоторых

1058
00:40:01,210 --> 00:40:02,350
играх вам требуется очень много времени,

1059
00:40:02,350 --> 00:40:03,910
чтобы добраться до места, где может быть ваш счет.

1060
00:40:03,910 --> 00:40:06,220
возможно измениться, так что в этом случае у вас

1061
00:40:06,220 --> 00:40:08,050
может быть очень редкое вознаграждение, в

1062
00:40:08,050 --> 00:40:09,400
других случаях вы собираетесь вознаграждать много,

1063
00:40:09,400 --> 00:40:11,020
и поэтому будет намного легче

1064
00:40:11,020 --> 00:40:13,060
узнать, что делать, одна из важных

1065
00:40:13,060 --> 00:40:15,250
вещей, которые они сделали в своей работе,

1066
00:40:15,250 --> 00:40:18,730
это  документ о природе от 2015 года заключается в том, что они используют

1067
00:40:18,730 --> 00:40:20,740
одну и ту же архитектуру и

1068
00:40:20,740 --> 00:40:23,230
гиперпараметры во всех играх, просто чтобы

1069
00:40:23,230 --> 00:40:24,850
было ясно, что они собираются затем изучить

1070
00:40:24,850 --> 00:40:26,560
разные функции подсказок и разные

1071
00:40:26,560 --> 00:40:29,200
политики для каждой игры, но их точка зрения

1072
00:40:29,200 --> 00:40:31,030
заключалась в том, что они не должны были использовать  совершенно

1073
00:40:31,030 --> 00:40:32,500
разные архитектуры и выполнять совершенно

1074
00:40:32,500 --> 00:40:33,790
разные настройки гиперпараметров для

1075
00:40:33,790 --> 00:40:35,380
каждой отдельной игры, чтобы

1076
00:40:35,380 --> 00:40:37,270
заставить ее работать, на самом деле

1077
00:40:37,270 --> 00:40:40,180
такой общей архитектуры и настройки

1078
00:40:40,180 --> 00:40:41,980
было достаточно, чтобы они могли

1079
00:40:41,980 --> 00:40:44,260
научиться принимать правильные решения для

1080
00:40:44,260 --> 00:40:46,150
всех игр, я думаю, что это был еще один хороший

1081
00:40:46,150 --> 00:40:47,530
вклад в эту статью, чтобы сказать,

1082
00:40:47,530 --> 00:40:48,970
что мы попытаемся получить своего рода

1083
00:40:48,970 --> 00:40:50,890
общий алгоритм и настройку, которая

1084
00:40:50,890 --> 00:40:52,420
выйдет далеко за рамки обычных

1085
00:40:52,420 --> 00:40:53,860
трех примеров.  что мы видим, и

1086
00:40:53,860 --> 00:40:55,570
документы по обучению с подкреплением, просто постарайтесь

1087
00:40:55,570 --> 00:40:57,370
снова попытаться преуспеть во всех 50 играх,

1088
00:40:57,370 --> 00:41:00,520
каждый агент будет учиться с

1089
00:41:00,520 --> 00:41:03,250
нуля в каждой из 50 игр, но он

1090
00:41:03,250 --> 00:41:04,300
будет делать это с теми же основными

1091
00:41:04,300 --> 00:41:05,920
параметрами, тот же тип Краудера и

1092
00:41:05,920 --> 00:41:06,970
тот же  нейронная сеть, это тот же

1093
00:41:06,970 --> 00:41:11,200
аппроксиматор функций, хорошо, и

1094
00:41:11,200 --> 00:41:12,520
приятно то, что я думаю, что это на

1095
00:41:12,520 --> 00:41:14,170
самом деле требуется по природе, они также

1096
00:41:14,170 --> 00:41:15,520
выпускают исходный код, так что вы можете поиграть

1097
00:41:15,520 --> 00:41:19,150
с этим, так как они сделали это

1098
00:41:19,150 --> 00:41:20,830
хорошо, они собираются принести пользу  аппроксиматор функции,

1099
00:41:20,830 --> 00:41:22,000
так что они

1100
00:41:22,000 --> 00:41:23,590
представляют функцию метки, они

1101
00:41:23,590 --> 00:41:24,940
собираются минимизировать средний квадрат, потерянный

1102
00:41:24,940 --> 00:41:27,849
при стохастическом градиентном спуске,

1103
00:41:27,849 --> 00:41:30,170
но мы знаем, что это может расходиться с

1104
00:41:30,170 --> 00:41:32,180
аппроксиматором функции значения x 'и что

1105
00:41:32,180 --> 00:41:34,190
две проблемы для этого хорошо,

1106
00:41:34,190 --> 00:41:36,380
одна из них - это своего рода

1107
00:41:36,380 --> 00:41:39,140
корреляция между выборками, что означает,

1108
00:41:39,140 --> 00:41:46,280
что если у вас есть s a are s простое число

1109
00:41:46,280 --> 00:41:50,570
R, это двойное число Prime, если вы думаете

1110
00:41:50,570 --> 00:41:54,380
о том, какова функция значения или

1111
00:41:54,380 --> 00:41:57,290
возврат для s  и функция ценности

1112
00:41:57,290 --> 00:41:59,930
и доход для простого числа s они

1113
00:41:59,930 --> 00:42:03,380
независимы, нет права на самом деле, как вы

1114
00:42:03,380 --> 00:42:04,310
ожидаете, что они будут сильно коррелированы,

1115
00:42:04,310 --> 00:42:06,440
вероятно, вы знаете, я имею в виду, что это

1116
00:42:06,440 --> 00:42:07,790
зависит от вероятности простого числа s, если

1117
00:42:07,790 --> 00:42:09,470
это детерминированная система единственная

1118
00:42:09,470 --> 00:42:11,869
разница между  будет R, так

1119
00:42:11,869 --> 00:42:13,040
что они сильно коррелированы, это

1120
00:42:13,040 --> 00:42:14,930
не образцы iid, когда мы делаем обновления,

1121
00:42:14,930 --> 00:42:17,300
есть много корреляций, а также

1122
00:42:17,300 --> 00:42:18,859
эта проблема с нестационарными целями

1123
00:42:18,859 --> 00:42:20,599
, что это означает, что когда

1124
00:42:20,599 --> 00:42:22,010
вы пытаетесь выполнить контролируемое

1125
00:42:22,010 --> 00:42:23,690
обучение  и обучите свой предсказатель функции ценности,

1126
00:42:23,690 --> 00:42:26,510
это не значит, что у вас всегда есть один и

1127
00:42:26,510 --> 00:42:28,790
тот же V PI Oracle, который говорит вам,

1128
00:42:28,790 --> 00:42:30,260
каково истинное значение, которое меняется с

1129
00:42:30,260 --> 00:42:32,210
течением времени, потому что вы изучаете Q,

1130
00:42:32,210 --> 00:42:33,800
чтобы попытаться  оцените, что это такое,

1131
00:42:33,800 --> 00:42:35,990
и ваша политика меняется, и поэтому это

1132
00:42:35,990 --> 00:42:37,580
огромное количество нестационарности, поэтому у вас

1133
00:42:37,580 --> 00:42:38,990
нет стационарной цели, когда

1134
00:42:38,990 --> 00:42:40,099
вы даже просто пытаетесь соответствовать своей

1135
00:42:40,099 --> 00:42:41,869
функции, потому что она может постоянно

1136
00:42:41,869 --> 00:42:44,420
меняться на каждом шагу, поэтому вы меняете своего

1137
00:42:44,420 --> 00:42:45,710
пилота  вы меняете свои веса, затем вы

1138
00:42:45,710 --> 00:42:46,880
меняете свою политику, а затем теперь вы

1139
00:42:46,880 --> 00:42:48,290
снова собираетесь менять свои веса, и поэтому,

1140
00:42:48,290 --> 00:42:50,420
возможно, неудивительно, что вещи могут

1141
00:42:50,420 --> 00:42:53,210
быть очень сложными с точки зрения конвергенции, так

1142
00:42:53,210 --> 00:42:56,119
что способ, которым DQ и глубокое Q-

1143
00:42:56,119 --> 00:42:57,470
обучение решают эти проблемы, основан на

1144
00:42:57,470 --> 00:43:00,970
опыте.  воспроизвести и исправить q мишени

1145
00:43:00,970 --> 00:43:02,990
опыт воспроизведения, вероятно, нумер, если вы,

1146
00:43:02,990 --> 00:43:04,010
ребята, слышали об этом, если вы

1147
00:43:04,010 --> 00:43:06,170
узнали о DQ n раньше, мы просто

1148
00:43:06,170 --> 00:43:08,180
собираемся хранить старые данные, о которых мы

1149
00:43:08,180 --> 00:43:10,040
говорили немного раньше, как

1150
00:43:10,040 --> 00:43:12,140
обучение TD в их стандартном подходе  просто

1151
00:43:12,140 --> 00:43:13,970
использует точку данных сейчас, что я имею в виду под

1152
00:43:13,970 --> 00:43:15,710
точкой данных, здесь действительно один из этих

1153
00:43:15,710 --> 00:43:19,280
простых кортежей s AR в простейшем способе

1154
00:43:19,280 --> 00:43:20,810
обучения TD или ключевого обучения, используйте это

1155
00:43:20,810 --> 00:43:23,060
один раз, и вы выбрасываете его, это здорово

1156
00:43:23,060 --> 00:43:24,980
для хранения данных это не так хорошо для

1157
00:43:24,980 --> 00:43:27,920
производительности, поэтому идея состоит в том, что мы

1158
00:43:27,920 --> 00:43:29,630
просто собираемся хранить это, мы собираемся

1159
00:43:29,630 --> 00:43:31,790
сохранить некоторый конечный буфер предыдущего

1160
00:43:31,790 --> 00:43:35,690
опыта, и мы собираемся действительно переделывать

1161
00:43:35,690 --> 00:43:38,089
обновления Q Learning, давайте просто вспомним Q

1162
00:43:38,089 --> 00:43:39,740
Learning  обновление здесь будет

1163
00:43:39,740 --> 00:43:40,550
выглядеть так:

1164
00:43:40,550 --> 00:43:41,840
мы обновим наши веса, которые

1165
00:43:41,840 --> 00:43:43,850
считаются одним обновлением, возьмем кортеж и

1166
00:43:43,850 --> 00:43:45,620
обновим вес, это похоже на одно

1167
00:43:45,620 --> 00:43:47,930
обновление стохастического градиентного спуска, и

1168
00:43:47,930 --> 00:43:49,760
поэтому вы можете просто взять образец из своего

1169
00:43:49,760 --> 00:43:52,310
опыта воспроизведения с помощью вашего буфера воспроизведения,

1170
00:43:52,310 --> 00:43:53,900
вы вычислите заданное целевое значение

1171
00:43:53,900 --> 00:43:56,360
ваша текущая функция-метка, а затем

1172
00:43:56,360 --> 00:43:59,150
вы выполняете стохастический градиентный спуск, теперь

1173
00:43:59,150 --> 00:44:02,030
обратите внимание, потому что ваша функция-метка

1174
00:44:02,030 --> 00:44:03,560
будет меняться со временем

1175
00:44:03,560 --> 00:44:06,110
каждый раз, когда вы обновляете

1176
00:44:06,110 --> 00:44:07,910
бюджет одной и той же пары, у вас может быть

1177
00:44:07,910 --> 00:44:10,130
другое целевое значение, потому что ваша

1178
00:44:10,130 --> 00:44:15,650
функция-метка изменилась для  что

1179
00:44:15,650 --> 00:44:16,880
это хорошо, потому что в основном это означает, что

1180
00:44:16,880 --> 00:44:18,530
вы повторно используете свои данные, а не просто

1181
00:44:18,530 --> 00:44:20,210
используете каждую точку данных, как только вы можете ее повторно использовать,

1182
00:44:20,210 --> 00:44:21,950
и это может быть  полезно, и что

1183
00:44:21,950 --> 00:44:27,830
мы рассмотрим подробнее через минуту, поэтому,

1184
00:44:27,830 --> 00:44:29,210
хотя мы рассматриваем цель

1185
00:44:29,210 --> 00:44:30,500
как скаляр, веса будут обновляться

1186
00:44:30,500 --> 00:44:31,760
в следующем раунде, что означает, что наше целевое

1187
00:44:31,760 --> 00:44:34,220
значение изменяется, и поэтому вы можете как бы

1188
00:44:34,220 --> 00:44:35,540
распространять эту информацию и

1189
00:44:35,540 --> 00:44:37,250
по сути, основная идея заключается в том, что

1190
00:44:37,250 --> 00:44:38,750
мы собираемся использовать данные более одного раза,

1191
00:44:38,750 --> 00:44:41,380
и это часто очень полезно, да и

1192
00:44:41,380 --> 00:44:46,570
счет, мой вопрос в том, является ли это

1193
00:44:46,570 --> 00:44:50,650
привилегией хранить больше кадров в нашем

1194
00:44:50,650 --> 00:44:55,820
представлении, это

1195
00:44:55,820 --> 00:44:57,230
эквивалентно тому, чтобы держать больше кадров в

1196
00:44:57,230 --> 00:44:59,060
нашем  представление, это не

1197
00:44:59,060 --> 00:45:00,890
очень интересный вопрос, больше кадров

1198
00:45:00,890 --> 00:45:02,780
будет похоже на сохранение более сложного

1199
00:45:02,780 --> 00:45:04,730
представления состояния, но вы все равно можете

1200
00:45:04,730 --> 00:45:07,160
просто использовать следующий кортеж состояния, присуждающий действие состояния,

1201
00:45:07,160 --> 00:45:09,320
один раз и выбрасывать эти данные таким образом, что это

1202
00:45:09,320 --> 00:45:11,810
все равно, что сказать, что периодически мне нравится

1203
00:45:11,810 --> 00:45:14,120
, скажем, я'  я пошел s1 a 1 R 1 s 2, а

1204
00:45:14,120 --> 00:45:15,890
затем я продолжаю, и теперь я как

1205
00:45:15,890 --> 00:45:18,710
s 3 a 3 R 3 S 4, так что это действительно то, где

1206
00:45:18,710 --> 00:45:21,080
я нахожусь в мире, я сейчас в состоянии 4, это

1207
00:45:21,080 --> 00:45:23,150
как я  внезапно притворись, о, подожди  Я

1208
00:45:23,150 --> 00:45:25,220
собираюсь притвориться, что я вернулся в s1, взял

1209
00:45:25,220 --> 00:45:27,680
1, получил R 1, я пошел в s 2, и я собираюсь

1210
00:45:27,680 --> 00:45:30,200
снова обновить свои веса, и причина, по

1211
00:45:30,200 --> 00:45:31,700
которой это обновление будет отличаться от

1212
00:45:31,700 --> 00:45:34,610
предыдущего, заключается в том, что я сейчас  обновлено с использованием

1213
00:45:34,610 --> 00:45:37,010
моего второго обновления моего третьего обновления, поэтому

1214
00:45:37,010 --> 00:45:38,330
моя функция Q в целом будет

1215
00:45:38,330 --> 00:45:40,370
отличаться от предыдущей, поэтому она вызовет

1216
00:45:40,370 --> 00:45:42,530
другое обновление веса, поэтому, даже если

1217
00:45:42,530 --> 00:45:44,300
это та же точка данных, что и раньше, она

1218
00:45:44,300 --> 00:45:45,620
вызовет другое обновление веса

1219
00:45:45,620 --> 00:45:47,600
и вообще одно  вещь, о которой мы

1220
00:45:47,600 --> 00:45:50,330
говорили давным-давно, заключается в том, что если вы

1221
00:45:50,330 --> 00:45:51,980
изучаете TD для конвергенции, что означает, что

1222
00:45:51,980 --> 00:45:53,430
вы просматриваете свои данные

1223
00:45:53,430 --> 00:45:56,340
как бесконечное количество времени, по крайней мере,

1224
00:45:56,340 --> 00:45:58,230
в табличном случае, что эквивалентно тому,

1225
00:45:58,230 --> 00:46:00,840
если вы изучили модель MDP, вы

1226
00:46:00,840 --> 00:46:02,220
изучаете  динамика перехода в модели вознаграждения,

1227
00:46:02,220 --> 00:46:04,320
и вы только что выполнили планирование MDP с

1228
00:46:04,320 --> 00:46:04,680
этим

1229
00:46:04,680 --> 00:46:06,870
, это то, к чему сходится обучение TD, как я

1230
00:46:06,870 --> 00:46:08,940
сказал, если вы как бы неоднократно

1231
00:46:08,940 --> 00:46:10,140
просматриваете свои данные бесконечное количество

1232
00:46:10,140 --> 00:46:10,590
времени

1233
00:46:10,590 --> 00:46:11,940
Бенчли, это будет сходиться к тому, как если бы вы

1234
00:46:11,940 --> 00:46:13,920
научились моделировать и  динамичный  мала

1235
00:46:13,920 --> 00:46:15,480
в модели вознаграждения, а затем

1236
00:46:15,480 --> 00:46:17,400
планировала то, что довольно круто, так что это

1237
00:46:17,400 --> 00:46:21,990
приближает нас к этому, но мы не

1238
00:46:21,990 --> 00:46:23,100
хотим делать это все время, потому что

1239
00:46:23,100 --> 00:46:25,020
есть компромисс между вычислениями, и

1240
00:46:25,020 --> 00:46:26,630
особенно здесь, потому что мы в играх

1241
00:46:26,630 --> 00:46:28,470
существует прямой компромисс между

1242
00:46:28,470 --> 00:46:30,320
вычислениями и получением большего опыта,

1243
00:46:30,320 --> 00:46:31,860
это на самом деле действительно интересный

1244
00:46:31,860 --> 00:46:33,420
компромисс, потому что в этих случаях

1245
00:46:33,420 --> 00:46:34,560
это похоже на то, должны ли вы больше думать и

1246
00:46:34,560 --> 00:46:36,120
планировать использовать свои старые данные, или вы должны

1247
00:46:36,120 --> 00:46:38,820
просто набраться больше опыта, но мы можем

1248
00:46:38,820 --> 00:46:39,900
поговорить об этом подробнее.  что позже, да, вопрос

1249
00:46:39,900 --> 00:46:43,230
и главный пресс, пожалуйста, поэтому, если

1250
00:46:43,230 --> 00:46:44,970
буфер воспроизведения опыта имеет

1251
00:46:44,970 --> 00:46:48,420
фиксированный размер, просто для

1252
00:46:48,420 --> 00:46:50,730
уточнения понимания, эти сэмплы

1253
00:46:50,730 --> 00:46:53,160
заменяются новыми сэмплами после

1254
00:46:53,160 --> 00:46:55,380
окончания времени или есть какой-то особый

1255
00:46:55,380 --> 00:46:57,660
способ выбрать, какие сэмплы

1256
00:46:57,660 --> 00:47:00,090
хранение буфера отличный вопрос, который в порядке,

1257
00:47:00,090 --> 00:47:01,470
это, по-видимому, будет буфер фиксированного размера,

1258
00:47:01,470 --> 00:47:03,630
и если это буфер фиксированного размера,

1259
00:47:03,630 --> 00:47:05,850
как вы выбираете, что в нем, и есть

1260
00:47:05,850 --> 00:47:07,800
r  и как вы получаете что-то из

1261
00:47:07,800 --> 00:47:09,900
того, как вы удаляете элементы из него,

1262
00:47:09,900 --> 00:47:11,310
это действительно интересный вопрос, разные

1263
00:47:11,310 --> 00:47:12,810
люди делают разные вещи, обычно это

1264
00:47:12,810 --> 00:47:15,540
часто самый последний буфер может быть,

1265
00:47:15,540 --> 00:47:16,800
например, последним миллионом образцов,

1266
00:47:16,800 --> 00:47:18,630
который дает вам представление о том, как

1267
00:47:18,630 --> 00:47:20,780
мы будем говорить о многих примерах,

1268
00:47:20,780 --> 00:47:23,130
но вы можете сделать другой выбор, и

1269
00:47:23,130 --> 00:47:24,510
есть интересные вопросы о том, какие

1270
00:47:24,510 --> 00:47:27,930
вещи вы должны выбросить, это также

1271
00:47:27,930 --> 00:47:29,160
зависит от того, действительно ли ваша проблема

1272
00:47:29,160 --> 00:47:31,440
нестационарна или нет, и я имею в виду, что

1273
00:47:31,440 --> 00:47:32,700
это похоже на реальный мир.  не стационарно,

1274
00:47:32,700 --> 00:47:35,780
как ваша клиентская база меняется, да,

1275
00:47:35,780 --> 00:47:38,180
попытайтесь найти правильный баланс между

1276
00:47:38,180 --> 00:47:40,440
продолжением опыта, как новые точки данных,

1277
00:47:40,440 --> 00:47:42,150
и повторным применением их, делаем ли мы что-

1278
00:47:42,150 --> 00:47:43,770
то похожее на эксплуатацию

1279
00:47:43,770 --> 00:47:46,890
или исследование, и, по сути,

1280
00:47:46,890 --> 00:47:48,810
со случайной вероятностью, просто решаем

1281
00:47:48,810 --> 00:47:51,420
воспроизвести решения о том, как  мы

1282
00:47:51,420 --> 00:47:53,730
выбираем между тем, что вы знаете,

1283
00:47:53,730 --> 00:47:55,230
чтобы получить данные, и тем, сколько нужно воспроизвести и

1284
00:47:55,230 --> 00:47:57,150
т. д., и можем ли мы сделать это в качестве

1285
00:47:57,150 --> 00:47:58,650
примера?  Я

1286
00:47:58,650 --> 00:48:00,330
думаю, что это, как правило, недостаточно изучено.

1287
00:48:00,330 --> 00:48:01,230
Существует множество различных эвристик, которые

1288
00:48:01,230 --> 00:48:03,780
люди используют, часто у людей есть какое-

1289
00:48:03,780 --> 00:48:05,700
то фиксированное соотношение того, сколько они

1290
00:48:05,700 --> 00:48:06,970
обновляют, исходя

1291
00:48:06,970 --> 00:48:08,920
из опыта, в который мы играли, по сравнению с добавлением

1292
00:48:08,920 --> 00:48:12,250
новых сэмплов, так что

1293
00:48:12,250 --> 00:48:13,180
как правило, прямо сейчас это своего рода

1294
00:48:13,180 --> 00:48:15,369
компромисс вашего состояния, который, безусловно, можно представить,

1295
00:48:15,369 --> 00:48:16,869
пытаясь оптимально понять это, но

1296
00:48:16,869 --> 00:48:19,180
это также требует вычислений, и это

1297
00:48:19,180 --> 00:48:20,109
приводит нас к действительно интересному

1298
00:48:20,109 --> 00:48:21,760
вопросу метавычисления и

1299
00:48:21,760 --> 00:48:22,540
метапознания.

1300
00:48:22,540 --> 00:48:24,310
вы знаете, что ваш агент думает о

1301
00:48:24,310 --> 00:48:25,900
том, как расставить приоритеты для своих собственных  вычисление,

1302
00:48:25,900 --> 00:48:27,910
которое является очень крутой проблемой, которую

1303
00:48:27,910 --> 00:48:31,060
мы решаем все время, хорошо, поэтому

1304
00:48:31,060 --> 00:48:33,310
второе, что делает dqn, это первая

1305
00:48:33,310 --> 00:48:34,900
половина, она сначала хранит эти старые данные,

1306
00:48:34,900 --> 00:48:37,420
второе, что он делает, это

1307
00:48:37,420 --> 00:48:39,750
фиксированные цели подсказок, так что это делает

1308
00:48:39,750 --> 00:48:42,190
мы имеем в виду, чтобы улучшить стабильность, и

1309
00:48:42,190 --> 00:48:43,599
под стабильностью здесь мы подразумеваем то, что мы не хотим, чтобы

1310
00:48:43,599 --> 00:48:44,740
наши веса взорвались и ушли в

1311
00:48:44,740 --> 00:48:46,630
бесконечность, что, как мы видели, может произойти  n в

1312
00:48:46,630 --> 00:48:49,000
функции линейного значения мы собираемся

1313
00:48:49,000 --> 00:48:50,770
зафиксировать целевые веса, которые используются в

1314
00:48:50,770 --> 00:48:52,839
целевом расчете для нескольких обновлений,

1315
00:48:52,839 --> 00:48:54,790
поэтому помните здесь, что я имею в виду под

1316
00:48:54,790 --> 00:48:57,460
целевым расчетом здесь вознаграждение

1317
00:48:57,460 --> 00:49:03,310
плюс гамма V от простого числа S, так что это само по себе

1318
00:49:03,310 --> 00:49:05,829
является функцией  W, и мы собираемся

1319
00:49:05,829 --> 00:49:08,920
исправить W, который мы используем в этом значении s, в

1320
00:49:08,920 --> 00:49:11,859
течение нескольких раундов, поэтому вместо того, чтобы

1321
00:49:11,859 --> 00:49:13,900
всегда обновлять, принимая самое

1322
00:49:13,900 --> 00:49:15,310
последнее, мы просто исправим его

1323
00:49:15,310 --> 00:49:16,869
на некоторое время, и это в основном похоже на то, чтобы

1324
00:49:16,869 --> 00:49:21,460
сделать это более  стабильный, потому что в

1325
00:49:21,460 --> 00:49:24,940
целом это приближение к

1326
00:49:24,940 --> 00:49:30,369
звезде Oracle of V, поэтому вам бы очень хотелось, чтобы

1327
00:49:30,369 --> 00:49:31,839
Oracle просто давал вам это каждый

1328
00:49:31,839 --> 00:49:33,849
раз, когда вы достигаете S Prime

1329
00:49:33,849 --> 00:49:35,950
или предпринимаете действия, чтобы заставить его перейти к s

1330
00:49:35,950 --> 00:49:37,329
Prime.  хотелось бы, чтобы Oracle дал вам

1331
00:49:37,329 --> 00:49:38,829
истинное значение, которого у вас нет

1332
00:49:38,829 --> 00:49:41,170
, и оно может меняться на каждом

1333
00:49:41,170 --> 00:49:42,790
этапе, потому что вы можете обновлять

1334
00:49:42,790 --> 00:49:44,380
веса, о чем здесь говорится, не делайте

1335
00:49:44,380 --> 00:49:47,680
этого, сохраняйте фиксированные веса, которые использовали

1336
00:49:47,680 --> 00:49:49,690
вычисления  PS премьер на некоторое время,

1337
00:49:49,690 --> 00:49:52,000
может быть, для  r 10 шагов, может быть, сто

1338
00:49:52,000 --> 00:49:55,359
шагов, и это просто делает цель чем-

1339
00:49:55,359 --> 00:49:56,680
то вроде того, что вы пытаетесь

1340
00:49:56,680 --> 00:49:58,660
минимизировать свои потери по отношению к более

1341
00:49:58,660 --> 00:50:02,890
стабильной, поэтому мы собираемся увидеть нашу

1342
00:50:02,890 --> 00:50:03,970
единую сеть, но мы просто собираемся  чтобы

1343
00:50:03,970 --> 00:50:05,349
поддерживать два разных набора весов

1344
00:50:05,349 --> 00:50:08,680
для этой сети, одним из них будет этот

1345
00:50:08,680 --> 00:50:12,490
вес - вы называете это - потому

1346
00:50:12,490 --> 00:50:13,720
что могут быть другие соглашения, но, в

1347
00:50:13,720 --> 00:50:15,069
частности, это старый набор весов,

1348
00:50:15,069 --> 00:50:16,510
поэтому те, которые мы не обновляем прямо сейчас,

1349
00:50:16,510 --> 00:50:17,710
и те  те, которые мы используем

1350
00:50:17,710 --> 00:50:20,410
в этом целевом расчете,

1351
00:50:20,410 --> 00:50:21,640
так что это те, которые мы собираемся использовать,

1352
00:50:21,640 --> 00:50:23,019
когда мы хотим выяснить значение

1353
00:50:23,019 --> 00:50:25,420
s Prime, а затем у нас есть другие W,

1354
00:50:25,420 --> 00:50:28,470
которые мы используем для обновления, и

1355
00:50:28,470 --> 00:50:31,569
так далее.  когда мы вычисляем наше целевое значение, мы

1356
00:50:31,569 --> 00:50:34,059
снова можем привести пример кортежа опыта

1357
00:50:34,059 --> 00:50:36,009
из набора данных из нашего

1358
00:50:36,009 --> 00:50:38,410
буфера воспроизведения опыта, вычислить целевое значение,

1359
00:50:38,410 --> 00:50:41,380
используя наш W, а затем мы используем стохастический

1360
00:50:41,380 --> 00:50:42,880
градиентный спуск, обновляем веса сети,

1361
00:50:42,880 --> 00:50:46,839
поэтому это используется с - это

1362
00:50:46,839 --> 00:50:53,259
используется  с собакой  арендовать один я думаю,

1363
00:50:53,259 --> 00:50:55,569
супер псих интуитивно, почему это чувствуется,

1364
00:50:55,569 --> 00:50:57,220
и вроде почему это делает его более

1365
00:50:57,220 --> 00:50:59,500
стабильным, а во-вторых, есть ли

1366
00:50:59,500 --> 00:51:04,029
какие-либо другие способности - один

1367
00:51:04,029 --> 00:51:05,680
интуитивно, почему это помогает M, что

1368
00:51:05,680 --> 00:51:06,940
является большим вопросом, а во-вторых,

1369
00:51:06,940 --> 00:51:08,529
за гранью  стабильность есть какие-либо другие

1370
00:51:08,529 --> 00:51:11,079
преимущества, поэтому интуитивно понятно, почему это

1371
00:51:11,079 --> 00:51:13,960
помогает с точки зрения стабильности с точки зрения

1372
00:51:13,960 --> 00:51:15,460
стабильности, это помогает, потому что вы в

1373
00:51:15,460 --> 00:51:17,440
основном уменьшаете шум в своей

1374
00:51:17,440 --> 00:51:19,539
цели, если вы думаете о Монте-

1375
00:51:19,539 --> 00:51:22,000
Карло и вместо того, чтобы использовать эту

1376
00:51:22,000 --> 00:51:24,789
цель, как  эта цель начальной загрузки

1377
00:51:24,789 --> 00:51:27,579
мы использовали GT, поэтому в Монте-Карло мы использовали

1378
00:51:27,579 --> 00:51:29,710
GT, и я сказал вам

1379
00:51:29,710 --> 00:51:31,210
, что это была непредвзятая

1380
00:51:31,210 --> 00:51:33,759
оценка PI, но недостатком

1381
00:51:33,759 --> 00:51:35,710
была высокая дисперсия, просто

1382
00:51:35,710 --> 00:51:36,819
суммируя награды до конца

1383
00:51:36,819 --> 00:51:39,490
эпизода, и поэтому вещи сильно различаются,

1384
00:51:39,490 --> 00:51:41,049
когда вы пытаетесь регрессировать на них

1385
00:51:41,049 --> 00:51:43,359
, это будет более шумно, и вас

1386
00:51:43,359 --> 00:51:45,250
могут попросить irradiance представить, что мы делаем

1387
00:51:45,250 --> 00:51:46,990
что-то доведенное до крайности, если мы

1388
00:51:46,990 --> 00:51:49,329
хотим  для стабильности вы

1389
00:51:49,329 --> 00:51:50,619
всегда можете сделать свою цель равной

1390
00:51:50,619 --> 00:51:52,809
константе, всегда можете сделать ее равной

1391
00:51:52,809 --> 00:51:54,819
нулю, например, и если вы навсегда сохраните свою

1392
00:51:54,819 --> 00:51:57,009
цель фиксированной, вы узнаете

1393
00:51:57,009 --> 00:51:58,750
веса, которые минимизируют ошибку

1394
00:51:58,750 --> 00:52:01,210
до постоянной функции, и тогда это

1395
00:52:01,210 --> 00:52:03,190
будет стабильно  потому что у вас всегда есть одно и то

1396
00:52:03,190 --> 00:52:04,900
же целевое значение, которое они всегда

1397
00:52:04,900 --> 00:52:06,309
пытаются предсказать, и в конечном итоге вы

1398
00:52:06,309 --> 00:52:07,779
узнаете, что они должны просто при вашем W,

1399
00:52:07,779 --> 00:52:10,089
равном нулю, и это было бы хорошо, так что

1400
00:52:10,089 --> 00:52:11,859
это просто уменьшает шум и

1401
00:52:11,859 --> 00:52:13,990
цель, которую мы пытаемся  вроде как, если

1402
00:52:13,990 --> 00:52:15,069
вы думаете об этом как о

1403
00:52:15,069 --> 00:52:16,660
проблеме обучения с учителем, у нас есть вход X и

1404
00:52:16,660 --> 00:52:19,240
выход Y, проблема в RL заключается в том, что наш

1405
00:52:19,240 --> 00:52:21,039
Y меняется, если вы делаете его таким,

1406
00:52:21,039 --> 00:52:23,259
чтобы ваш Y не менялся, его намного

1407
00:52:23,259 --> 00:52:26,259
легче подогнать, а второй  есть

1408
00:52:26,259 --> 00:52:27,670
ли какая-либо польза помимо

1409
00:52:27,670 --> 00:52:30,720
стабильности, я думаю, в основном нет,

1410
00:52:30,720 --> 00:52:33,060
это также как бы снижает

1411
00:52:33,060 --> 00:52:34,770
скорость распространения информации,

1412
00:52:34,770 --> 00:52:37,590
потому что вы используете устаревший набор

1413
00:52:37,590 --> 00:52:39,420
весов для представления значения

1414
00:52:39,420 --> 00:52:41,400
статистики.  e, поэтому вы можете пропустить оценку

1415
00:52:41,400 --> 00:52:42,420
значения состояния, потому что вы не

1416
00:52:42,420 --> 00:52:43,890
обновили его с помощью старой работы с новой

1417
00:52:43,890 --> 00:52:50,330
информацией, которую мы хотим сделать, или

1418
00:52:50,330 --> 00:52:54,560
примерно есть этот сложный

1419
00:52:54,560 --> 00:52:56,790
вопрос, который специфичен для

1420
00:52:56,790 --> 00:52:57,900
глубоких нейронных сетей или мог бы использовать это

1421
00:52:57,900 --> 00:52:59,280
с линейным значением  аппроксимация функции

1422
00:52:59,280 --> 00:53:00,330
или любая часть значения, которую вы могли бы использовать с

1423
00:53:00,330 --> 00:53:02,400
любым аппроксиматором функции значения, да,

1424
00:53:02,400 --> 00:53:04,140
это не конкретно, это на самом деле просто

1425
00:53:04,140 --> 00:53:05,520
о стабильности, и это верно

1426
00:53:05,520 --> 00:53:07,440
для воспроизведения опыта для воспроизведения опыта

1427
00:53:07,440 --> 00:53:08,310
просто будет

1428
00:53:08,310 --> 00:53:11,130
более эффективно распространять информацию, и это

1429
00:53:11,130 --> 00:53:13,320
просто собирается  сделать его более стабильным, чтобы

1430
00:53:13,320 --> 00:53:15,150
они не были уникальными для использования глубокой

1431
00:53:15,150 --> 00:53:16,260
нейронной сети, я думаю, они просто

1432
00:53:16,260 --> 00:53:17,849
больше беспокоились о стабильности с

1433
00:53:17,849 --> 00:53:18,900
этим действительно сложным

1434
00:53:18,900 --> 00:53:24,150
аппроксиматором функций z 'да в красном да -

1435
00:53:24,150 --> 00:53:27,000
это всегда было для нас отличным видением, так

1436
00:53:27,000 --> 00:53:29,330
что

1437
00:53:30,230 --> 00:53:32,300
независимо от того, обновляем ли мы когда-либо w - да,

1438
00:53:32,300 --> 00:53:34,609
мы бы сравнили обновление eaat eclis w -

1439
00:53:34,609 --> 00:53:38,330
также, поэтому в фиксированном графике скажем, каждые 50

1440
00:53:38,330 --> 00:53:41,119
или вы знаете, каждый и ep  isodes или каждые

1441
00:53:41,119 --> 00:53:46,400
n шагов, которые вы обновляете так же, как каждый, и вы

1442
00:53:46,400 --> 00:53:54,859
бы установили W минус 2w, мы знаем, что

1443
00:53:54,859 --> 00:53:57,710
это для создания вашего набора, и вы не

1444
00:53:57,710 --> 00:54:00,050
используете ту же структуру, что и

1445
00:54:00,050 --> 00:54:02,030
градиенты, которые вы используете отлично,

1446
00:54:02,030 --> 00:54:03,670
да, но другая функция

1447
00:54:03,670 --> 00:54:06,410
вычитая значение времени, но у нас есть

1448
00:54:06,410 --> 00:54:08,690
проект, как это должно не

1449
00:54:08,690 --> 00:54:10,970
сильно ломаться, как и все эти предположения,

1450
00:54:10,970 --> 00:54:15,410
он говорит, хорошо, это действительно

1451
00:54:15,410 --> 00:54:16,730
работает, хорошие условия градиентов, что

1452
00:54:16,730 --> 00:54:18,950
это не так, я имею в виду, что это отличный вопрос

1453
00:54:18,950 --> 00:54:21,109
и такие  q-обучение не является

1454
00:54:21,109 --> 00:54:22,880
истинным методом градиентного спуска,

1455
00:54:22,880 --> 00:54:24,680
это приближение к тому, что они

1456
00:54:24,680 --> 00:54:27,170
часто делают поразительно хорошо, учитывая, что некоторые

1457
00:54:27,170 --> 00:54:29,630
из более поздних методов, в главе

1458
00:54:29,630 --> 00:54:31,460
одиннадцатой, хорошо обсуждают это,

1459
00:54:31,460 --> 00:54:33,290
или G TDS, или градиент во временной

1460
00:54:33,290 --> 00:54:35,119
разнице.  обучение - это более верные

1461
00:54:35,119 --> 00:54:36,470
алгоритмы градиентного спуска, это на

1462
00:54:36,470 --> 00:54:37,970
самом деле просто приближения, и именно

1463
00:54:37,970 --> 00:54:41,030
это, как близкая точка, это не дает

1464
00:54:41,030 --> 00:54:42,830
гарантий сходимости, но,

1465
00:54:42,830 --> 00:54:47,300
надеюсь, это поможет, но так что это

1466
00:54:47,300 --> 00:54:49,369
будет практикой.  люди как какой-то циклический

1467
00:54:49,369 --> 00:54:51,109
шаблон и говорили, и они обновляют

1468
00:54:51,109 --> 00:54:53,740
те ожидания, которые используются для вычисления

1469
00:54:53,740 --> 00:54:58,070
радианов, которые вы знаете на практике, есть

1470
00:54:58,070 --> 00:54:59,570
какой-то циклический шаблон того, как

1471
00:54:59,570 --> 00:55:02,210
часто вы обновляете W - да,

1472
00:55:02,210 --> 00:55:04,220
часто есть определенные шаблоны или

1473
00:55:04,220 --> 00:55:05,660
гипер это выбор гиперпараметра того, как быстро

1474
00:55:05,660 --> 00:55:08,869
как часто вы обновляете это, и это

1475
00:55:08,869 --> 00:55:10,820
будет компромисс между

1476
00:55:10,820 --> 00:55:13,310
более быстрым распространением информации и, возможно,

1477
00:55:13,310 --> 00:55:16,160
менее стабильной, поэтому, если вы сообщите,

1478
00:55:16,160 --> 00:55:18,290
если n здесь тот, который вы вернулись к

1479
00:55:18,290 --> 00:55:20,210
стандартному обучению TD, если n равно бесконечности,

1480
00:55:20,210 --> 00:55:23,060
это означает, что вы никогда не  обновил его, так

1481
00:55:23,060 --> 00:55:24,710
что есть плавное продолжение

1482
00:55:24,710 --> 00:55:28,450
там, которое мы используем, есть лучшие

1483
00:55:28,450 --> 00:55:29,660
инициализации,

1484
00:55:29,660 --> 00:55:32,150
чем просто ноль, что-то вроде того, что

1485
00:55:32,150 --> 00:55:33,290
вы приняли во внимание, я думаю, что

1486
00:55:33,290 --> 00:55:36,590
среднее значение и дисперсия, которые мы делаем, это было W -

1487
00:55:36,590 --> 00:55:38,960
просто W или там, я думаю, вы сделали

1488
00:55:38,960 --> 00:55:43,310
инициализация о том, что вы знаете,

1489
00:55:43,310 --> 00:55:46,310
что влияние того, как мы инициализируем w, может

1490
00:55:46,310 --> 00:55:49,310
иметь значение, и заключается в том, как мы инициализируем

1491
00:55:49,310 --> 00:55:51,560
w - обычно мы - есть w - чтобы быть

1492
00:55:51,560 --> 00:55:53,830
точно таким же, как w в be

1493
00:55:53,830 --> 00:55:55,820
выбор этого также,

1494
00:55:55,820 --> 00:55:59,510
безусловно, повлияет на раннюю производительность. отличные

1495
00:55:59,510 --> 00:56:00,740
вопросы, позвольте мне продолжить, я хочу

1496
00:56:00,740 --> 00:56:01,670
убедиться, что мы также доберемся до некоторых

1497
00:56:01,670 --> 00:56:03,859
расширений, поэтому просто подытожим,

1498
00:56:03,859 --> 00:56:06,290
как работает dqn. два основных нововведения, которые

1499
00:56:06,290 --> 00:56:08,119
он сделал, это использование воспроизведения опыта  они

1500
00:56:08,119 --> 00:56:11,359
выполняют цели, в которых он сохраняет переход,

1501
00:56:11,359 --> 00:56:13,369
и этот буфер воспроизведения представляет собой память воспроизведения, в которой

1502
00:56:13,369 --> 00:56:16,490
вы отбираете случайные мини-пакеты из

1503
00:56:16,490 --> 00:56:17,990
дела, обычно это образец в мини-пакете,

1504
00:56:17,990 --> 00:56:20,510
а не один пример 10 или

1505
00:56:20,510 --> 00:56:23,599
любой другой параметр, который вы делаете свой

1506
00:56:23,599 --> 00:56:25,970
градиентный спуск, учитывая те, которые вы вычисляете

1507
00:56:25,970 --> 00:56:27,710
q-  обучение с использованием этих старых целей, и

1508
00:56:27,710 --> 00:56:29,000
вы оптимизируете среднеквадратичную ошибку

1509
00:56:29,000 --> 00:56:30,410
между сетью очередей и

1510
00:56:30,410 --> 00:56:31,940
целями обучения очереди, используя стохастический градиентный

1511
00:56:31,940 --> 00:56:33,530
спуск, и я не

1512
00:56:33,530 --> 00:56:35,210
упомянул здесь то, что мы обычно проводим

1513
00:56:35,210 --> 00:56:38,720
жадное исследование, и поэтому вам нужно какое-то

1514
00:56:38,720 --> 00:56:41,560
расписание здесь  - за то, как разлагаться жадно,

1515
00:56:41,560 --> 00:56:43,810
чтобы они не проводили сложных

1516
00:56:43,810 --> 00:56:47,720
исследований в оригинальной статье, так

1517
00:56:47,720 --> 00:56:49,040
что это похоже на то, что вы вроде

1518
00:56:49,040 --> 00:56:50,330
г  если вы выполняете несколько разных

1519
00:56:50,330 --> 00:56:52,910
сверток, у них есть изображения, и

1520
00:56:52,910 --> 00:56:54,800
они делают несколько полностью связанных слоев, а

1521
00:56:54,800 --> 00:56:56,450
затем выводят значение очереди для каждого

1522
00:56:56,450 --> 00:57:01,760
действия, позвольте мне просто показать это для

1523
00:57:01,760 --> 00:57:03,310
тех из вас, кто не видел этого раньше,

1524
00:57:03,310 --> 00:57:06,470
так что хорошая вещь  что они, так что вы

1525
00:57:06,470 --> 00:57:09,080
собираетесь увидеть прорыв, который является игрой Atari,

1526
00:57:09,080 --> 00:57:11,000
и что они делают, так это они показывают вам

1527
00:57:11,000 --> 00:57:12,710
своего рода производительность того, что

1528
00:57:12,710 --> 00:57:14,599
делает агент, так что помните, что агенты

1529
00:57:14,599 --> 00:57:16,190
просто учатся здесь с помощью пикселей, как это сделать,

1530
00:57:16,190 --> 00:57:17,540
так что это было красиво  необычно, когда

1531
00:57:17,540 --> 00:57:21,260
они показали это примерно в 2014 году и в

1532
00:57:21,260 --> 00:57:22,460
начале своего обучения этой

1533
00:57:22,460 --> 00:57:24,349
политике, вы можете видеть, что она не

1534
00:57:24,349 --> 00:57:26,300
очень много делает правильные вещи, а затем со

1535
00:57:26,300 --> 00:57:28,310
временем, когда она получает больше эпизодов, она

1536
00:57:28,310 --> 00:57:30,410
начинает учиться принимать более правильные

1537
00:57:30,410 --> 00:57:35,060
решения о том, как  делать это, и одна

1538
00:57:35,060 --> 00:57:36,680
из интересных вещей в этом заключается в том, что,

1539
00:57:36,680 --> 00:57:38,930
как вы и надеетесь, по мере того, как он получает все больше и больше

1540
00:57:38,930 --> 00:57:41,410
данных, он учится принимать более правильные решения,

1541
00:57:41,410 --> 00:57:44,330
но одна из вещей, которые людям очень нравятся в

1542
00:57:44,330 --> 00:57:47,150
этом, это то, что вы можете научиться

1543
00:57:47,150 --> 00:57:50,300
использовать их.  функция вознаграждения,

1544
00:57:50,300 --> 00:57:54,660
так что в эти большие годы, если вы

1545
00:57:54,660 --> 00:57:56,010
действительно просто хотите, чтобы я максимизировал

1546
00:57:56,010 --> 00:57:57,660
ожидаемое вознаграждение, что лучше всего для

1547
00:57:57,660 --> 00:57:59,370
меня сделать, это просто проделать дыру

1548
00:57:59,370 --> 00:58:00,960
в этом, а затем, как только я смогу

1549
00:58:00,960 --> 00:58:05,030
начать просто это  вокруг максимальной загрузки,

1550
00:58:05,030 --> 00:58:09,180
и поэтому это одна из вещей, где

1551
00:58:09,180 --> 00:58:10,980
вы знаете, если вы попросите агента

1552
00:58:10,980 --> 00:58:12,840
максимизировать вознаграждение, немного узнали

1553
00:58:12,840 --> 00:58:13,980
правильный способ максимизировать вознаграждение, учитывая

1554
00:58:13,980 --> 00:58:15,990
достаточно данных, и поэтому это было действительно

1555
00:58:15,990 --> 00:58:17,130
круто, что он мог обнаружить

1556
00:58:17,130 --> 00:58:18,510
вещи, которые могут быть, или стратегии, которым

1557
00:58:18,510 --> 00:58:20,250
людям требуется некоторое время, чтобы изучить, когда

1558
00:58:20,250 --> 00:58:23,360
они впервые изучают игру, а также

1559
00:58:27,110 --> 00:58:29,580
то, что они сделали, они затем показали

1560
00:58:29,580 --> 00:58:31,800
довольно удивительную производительность во

1561
00:58:31,800 --> 00:58:33,480
множестве разных игр, во многих играх, которые они могли

1562
00:58:33,480 --> 00:58:35,670
сделать, а также  люди теперь, чтобы быть точным

1563
00:58:35,670 --> 00:58:38,330
здесь, о да,

1564
00:58:38,630 --> 00:58:40,279
мне просто интересно, почему хорошо - это

1565
00:58:40,279 --> 00:58:42,230
спланировано, это было похоже на то, что мяч

1566
00:58:42,230 --> 00:58:43,940
много шевелился, как будто он не был уверен, что некоторые

1567
00:58:43,940 --> 00:58:45,920
движения, как он перемещается вокруг места

1568
00:58:45,920 --> 00:58:48,200
часто, как да,

1569
00:58:48,200 --> 00:58:50,450
да, так что вы можете видеть, что я  думаю, я мог бы

1570
00:58:50,450 --> 00:58:51,950
воздержитесь от того факта, что весло

1571
00:58:51,950 --> 00:58:54,109
много двигалось, поскольку агенты пытались

1572
00:58:54,109 --> 00:58:55,490
учиться, например, когда мы видели, что

1573
00:58:55,490 --> 00:58:57,289
пользователи назад, почему вы должны много

1574
00:58:57,289 --> 00:58:58,579
дергаться с точки зрения возраста, особенно если

1575
00:58:58,579 --> 00:59:00,950
есть стоимость перемещения, тогда это может

1576
00:59:00,950 --> 00:59:02,599
быть просто любезно  болтать и проводить

1577
00:59:02,599 --> 00:59:04,430
исследования, просто чтобы посмотреть, что работает,

1578
00:59:04,430 --> 00:59:05,839
и с нашей точки зрения это

1579
00:59:05,839 --> 00:59:07,339
явно своего рода неопытный игрок,

1580
00:59:07,339 --> 00:59:08,690
чтобы сделать это, это было бы странно,

1581
00:59:08,690 --> 00:59:10,430
но с точки зрения агентов это

1582
00:59:10,430 --> 00:59:12,559
совершенно разумно, это не дает

1583
00:59:12,559 --> 00:59:13,940
положительного или отрицательного вознаграждения от этого, так

1584
00:59:13,940 --> 00:59:15,950
он не может различить между университетом

1585
00:59:15,950 --> 00:59:17,450
и стационарным движением по сравнению с движением влево или

1586
00:59:17,450 --> 00:59:19,220
вправо, если вы введете стоимость движения,

1587
00:59:19,220 --> 00:59:28,400
которая могла бы помочь там. Я не

1588
00:59:28,400 --> 00:59:30,470
помню полную архитектуру сети и был

1589
00:59:30,470 --> 00:59:32,210
ли там слой пула,

1590
00:59:32,210 --> 00:59:33,349
я думаю, проблема  может

1591
00:59:33,349 --> 00:59:34,819
быть внутри их дома, они должны

1592
00:59:34,819 --> 00:59:36,710
идти от изображений до самого верха, но у

1593
00:59:36,710 --> 00:59:37,880
них есть полный

1594
00:59:37,880 --> 00:59:41,150
вопрос об архитектуре, так что хорошая вещь, которую вы можете

1595
00:59:41,150 --> 00:59:43,700
увидеть здесь, это  у них есть своего рода

1596
00:59:43,700 --> 00:59:45,230
производительность на человеческом уровне в ряде

1597
00:59:45,230 --> 00:59:46,700
различных игр Atari, здесь около 50

1598
00:59:46,700 --> 00:59:49,069
игр, просто чтобы было ясно, когда

1599
00:59:49,069 --> 00:59:50,450
они говорят о производительности на человеческом уровне, это

1600
00:59:50,450 --> 00:59:52,700
означает асимптотически, поэтому после того, как они

1601
00:59:52,700 --> 00:59:55,069
обучили своего агента, они говорят, что не

1602
00:59:55,069 --> 00:59:56,960
говорят о  сколько времени потребовалось им или

1603
00:59:56,960 --> 00:59:59,059
их агенту, чтобы научиться, и, как вы, ребята

1604
00:59:59,059 --> 01:00:00,470
, узнаете из домашнего задания, может

1605
01:00:00,470 --> 01:00:03,529
потребоваться много времени,

1606
01:00:03,529 --> 01:00:05,269
чтобы научиться хорошо работать,

1607
01:00:05,269 --> 01:00:06,470
но, тем не менее, есть много

1608
01:00:06,470 --> 01:00:08,000
случаев, когда это может быть  быть разумными с

1609
01:00:08,000 --> 01:00:10,250
точки зрения игр, и поэтому они сделали переменные

1610
01:00:10,250 --> 01:00:11,960
в некоторых областях, в некоторых областях они справились

1611
01:00:11,960 --> 01:00:13,579
очень плохо,

1612
01:00:13,579 --> 01:00:15,319
к такого рода играм был большой интерес в

1613
01:00:15,319 --> 01:00:17,509
нижней части хвоста, которые часто называют

1614
01:00:17,509 --> 01:00:19,730
играми с жесткими исследованиями.

1615
01:00:19,730 --> 01:00:20,900
я буду говорить намного больше об

1616
01:00:20,900 --> 01:00:24,049
исследованиях позже в ходе курса, так

1617
01:00:24,049 --> 01:00:26,869
что что было критическим, так что мне нравится много

1618
01:00:26,869 --> 01:00:28,099
действительно замечательных вещей в этой статье,

1619
01:00:28,099 --> 01:00:29,299
и одна из действительно приятных вещей заключается в

1620
01:00:29,299 --> 01:00:30,859
том, что они сделали хорошую абляцию  tudy и

1621
01:00:30,859 --> 01:00:32,720
для нас, чтобы понять, какие

1622
01:00:32,720 --> 01:00:34,880
были важные функции, и если вы посмотрите

1623
01:00:34,880 --> 01:00:37,220
на эти цифры, я думаю, станет ясно,

1624
01:00:37,220 --> 01:00:39,400
что действительно важной функцией является воспроизведение,

1625
01:00:39,400 --> 01:00:41,809
так что это их производительность с использованием

1626
01:00:41,809 --> 01:00:44,150
линейной сети, глубоко связанной сетью, похоже,

1627
01:00:44,150 --> 01:00:45,049
не так сильно помогает,

1628
01:00:45,049 --> 01:00:48,019
используя это  фиксированная очередь фиксированная очередь здесь

1629
01:00:48,019 --> 01:00:49,640
означает, что вы кажетесь фиксированной целью, хорошо,

1630
01:00:49,640 --> 01:00:50,960
что вы получаете немного три

1631
01:00:50,960 --> 01:00:52,110
из десяти, вы

1632
01:00:52,110 --> 01:00:55,620
внезапно играете в нее на 241, поэтому выбрасывать

1633
01:00:55,620 --> 01:00:56,970
каждую точку данных после того, как вы ее

1634
01:00:56,970 --> 01:00:59,220
однажды использовали, не очень хорошая вещь.  вы

1635
01:00:59,220 --> 01:01:02,010
хотите повторно использовать эти данные, а затем, если вы

1636
01:01:02,010 --> 01:01:04,050
объедините повтор и фиксированную очередь, вы

1637
01:01:04,050 --> 01:01:06,570
получите улучшение по сравнению с этим, но на

1638
01:01:06,570 --> 01:01:08,780
самом деле вы получите,

1639
01:01:08,780 --> 01:01:11,040
по крайней мере, это огромное увеличение и прорветесь в некоторых

1640
01:01:11,040 --> 01:01:13,860
других играх, сделав повтор сейчас в некоторых

1641
01:01:13,860 --> 01:01:15,330
других.  вы начинаете получать

1642
01:01:15,330 --> 01:01:16,860
значительное улучшение, как только используете

1643
01:01:16,860 --> 01:01:18,570
более сложный аппроксиматор функций,

1644
01:01:18,570 --> 01:01:21,150
но в целом воспроизведение чрезвычайно

1645
01:01:21,150 --> 01:01:23,040
важно, и оно просто дает нам гораздо

1646
01:01:23,040 --> 01:01:25,680
лучший способ использования данных, да, да,

1647
01:01:25,680 --> 01:01:27,270
только здесь  в этой таблице кажется, что

1648
01:01:27,270 --> 01:01:29,100
вы хотели бы использовать повтор в фиксированной

1649
01:01:29,100 --> 01:01:31,440
очереди с линейной моделью, и тогда

1650
01:01:31,440 --> 01:01:33,630
может быть ошибкой использовать здесь глубокую

1651
01:01:33,630 --> 01:01:38,820
модель. вы согласны с этим?

1652
01:01:38,820 --> 01:01:40,590


1653
01:01:40,590 --> 01:01:41,820
будьте ясны, так что это

1654
01:01:41,820 --> 01:01:44,850
все на следующих четырех, мы все

1655
01:01:44,850 --> 01:01:47,490
глубоко, поэтому у них нет здесь линейного

1656
01:01:47,490 --> 01:01:49,860
воспроизведения плюс повтор, но вы, безусловно, можете представить себе, что

1657
01:01:49,860 --> 01:01:52,080
попробуйте линейный плюс повтор, и кажется,

1658
01:01:52,080 --> 01:01:53,520
что вы могли бы очень хорошо здесь может

1659
01:01:53,520 --> 01:01:54,780
зависеть от того, какие функции  вы используете,

1660
01:01:54,780 --> 01:01:57,510
есть кое-какая крутая работа за последние несколько

1661
01:01:57,510 --> 01:01:59,310
лет, смотрите также, можете ли вы

1662
01:01:59,310 --> 01:02:01,320
объединить эти два, поэтому мы проделали некоторую

1663
01:02:01,320 --> 01:02:03,780
работу, используя байесовский последний слой, используя,

1664
01:02:03,780 --> 01:02:05,070
например, байесовскую линейную регрессию, которая

1665
01:02:05,070 --> 01:02:06,810
полезна для неопределенности. другие люди

1666
01:02:06,810 --> 01:02:08,220
только что сделали линейную  регрессия, где

1667
01:02:08,220 --> 01:02:10,680
идея заключается в том, что вы используете своего рода глубокую нейронную

1668
01:02:10,680 --> 01:02:12,660
сеть до определенного момента, а затем

1669
01:02:12,660 --> 01:02:16,080
вы выполняете своего рода прямую линейную регрессию,

1670
01:02:16,080 --> 01:02:17,460
чтобы точно соответствовать весам

1671
01:02:17,460 --> 01:02:19,050
на последнем слое, и поэтому может быть гораздо

1672
01:02:19,050 --> 01:02:21,270
больше данных  эффективно, но у вас все еще есть

1673
01:02:21,270 --> 01:02:26,610
сложное представление, все в порядке, поэтому

1674
01:02:26,610 --> 01:02:28,740
с тех пор было огромное

1675
01:02:28,740 --> 01:02:31,680
количество интереса к этой области, поэтому, снова

1676
01:02:31,680 --> 01:02:32,970
встречаясь с самим собой, обучение с подкреплением

1677
01:02:32,970 --> 01:02:34,290
имел обыкновение ходить и рассказывать об

1678
01:02:34,290 --> 01:02:35,820
обучении с подкреплением.

1679
01:02:35,820 --> 01:02:37,260
большинство из них

1680
01:02:37,260 --> 01:02:40,050
вы знали, а потом я, а потом все

1681
01:02:40,050 --> 01:02:41,580
начало действительно меняться, я думаю, это было,

1682
01:02:41,580 --> 01:02:45,930
может быть, в 2016 году, когда ICML был в Нью-Йорке,

1683
01:02:45,930 --> 01:02:47,490
и внезапно

1684
01:02:47,490 --> 01:02:48,690
в комнате для лекций по обучению с подкреплением

1685
01:02:48,690 --> 01:02:51,660
собралось 400 человек, а затем в этом году в nurbs,

1686
01:02:51,660 --> 01:02:53,040
который  на одной из крупных

1687
01:02:53,040 --> 01:02:54,540
конференций по машинному обучению билеты были распроданы примерно за восемь

1688
01:02:54,540 --> 01:02:57,120
минут, поэтому там было 8000 человек,

1689
01:02:57,120 --> 01:02:58,410
и был огромный интерес

1690
01:02:58,410 --> 01:02:59,940
к глубокому обучению, а для семинара по глубокому

1691
01:02:59,940 --> 01:03:00,960
обучению у вас как бы есть

1692
01:03:00,960 --> 01:03:03,120
аудит на 2000 человек,

1693
01:03:03,120 --> 01:03:04,680
так что было огромное  количество

1694
01:03:04,680 --> 01:03:06,360
волнений, основанных на этой работе, что, я

1695
01:03:06,360 --> 01:03:08,250
думаю, действительно является огромной заслугой глубокого разума

1696
01:03:08,250 --> 01:03:09,540
и работы Дэвида Сильвера и

1697
01:03:09,540 --> 01:03:11,520
других, которые проделали, чтобы показать,

1698
01:03:11,520 --> 01:03:14,100
что это  возможно, некоторые из медиа-

1699
01:03:14,100 --> 01:03:15,300
доказательств, которые мы собираемся пройти

1700
01:03:15,300 --> 01:03:18,270
очень быстро, это двойная очередь D

1701
01:03:18,270 --> 01:03:20,400
и приоритет повторного воспроизведения, а Джулиан dqn

1702
01:03:20,400 --> 01:03:22,950
было намного больше статей,

1703
01:03:22,950 --> 01:03:24,960
чем это, но это некоторые из

1704
01:03:24,960 --> 01:03:27,210
первых действительно больших улучшений поверх

1705
01:03:27,210 --> 01:03:31,410
DQ.  Таким образом, двойное D QN похоже на

1706
01:03:31,410 --> 01:03:33,840
двойное Q обучение, которое мы очень

1707
01:03:33,840 --> 01:03:35,670
кратко рассмотрели в конце пары уроков

1708
01:03:35,670 --> 01:03:37,530
назад, и то, что мы обсуждали

1709
01:03:37,530 --> 01:03:39,510
там, было своего рода смещением максимизации, заключающимся

1710
01:03:39,510 --> 01:03:42,600
в том, что максимальное значение

1711
01:03:42,600 --> 01:03:44,460
оцениваемых действий состояния может быть  предвзятая

1712
01:03:44,460 --> 01:03:45,720
оценка истинного максимума,

1713
01:03:45,720 --> 01:03:48,150
и поэтому мы очень кратко говорили об

1714
01:03:48,150 --> 01:03:52,170
обучении двойного Q, поэтому идея обучения двойного Q

1715
01:03:52,170 --> 01:03:53,340
заключалась в том, что мы собираемся

1716
01:03:53,340 --> 01:03:57,330
поддерживать две разные сети очередей, которые мы

1717
01:03:57,330 --> 01:03:59,130
можем выбрать наше действие, используя как

1718
01:03:59,130 --> 01:04:00,780
жадную политику, где мы усредняем между

1719
01:04:00,780 --> 01:04:03,150
эти сети Q, а затем мы будем наблюдать

1720
01:04:03,150 --> 01:04:06,030
за вознаграждением в виде состояния, и мы в основном используем

1721
01:04:06,030 --> 01:04:08,790
один из Q в качестве цели для

1722
01:04:08,790 --> 01:04:12,480
другого, поэтому, если вы знаете с вероятностью 50%,

1723
01:04:12,480 --> 01:04:14,040
мы собираемся обновить одну

1724
01:04:14,040 --> 01:04:18,150
сеть, и мы получаем  Чтобы сделать это,

1725
01:04:18,150 --> 01:04:20,670
используя выбор действия из другой

1726
01:04:20,670 --> 01:04:26,640
сети, мы пытаемся отделить то, как

1727
01:04:26,640 --> 01:04:29,910
мы выбираем наше действие, от нашей

1728
01:04:29,910 --> 01:04:32,460
оценки ценности этого действия, чтобы справиться с

1729
01:04:32,460 --> 01:04:34,710
такой проблемой смещения максимизации, а

1730
01:04:34,710 --> 01:04:37,560
затем с другой вероятностью 50% мы

1731
01:04:37,560 --> 01:04:40,950
обновите q2, и мы выберем следующее действие

1732
01:04:40,950 --> 01:04:47,340
из другой сети, так что это

1733
01:04:47,340 --> 01:04:49,290
довольно небольшое изменение означает, что у вас есть четыре,

1734
01:04:49,290 --> 01:04:51,060
вы должны поддерживать две разные

1735
01:04:51,060 --> 01:04:52,380
сети или два разных набора

1736
01:04:52,380 --> 01:04:56,780
весов, и это может быть очень полезно, поэтому,

1737
01:04:56,780 --> 01:04:59,700
если вы распространите эту идею на DQ  n у вас есть

1738
01:04:59,700 --> 01:05:02,250
своего рода наши текущие действия Q Network W, которые не понравились,

1739
01:05:02,250 --> 01:05:04,200
и это более старое для оценки

1740
01:05:04,200 --> 01:05:08,520
действий, поэтому вы можете поместить это туда, чтобы

1741
01:05:08,520 --> 01:05:10,620
сделать выбор действий, а затем вы можете

1742
01:05:10,620 --> 01:05:12,350
оценить его ценность с другими

1743
01:05:12,350 --> 01:05:14,140
сетями в

1744
01:05:14,140 --> 01:05:17,019
другие недели сети, так что это довольно

1745
01:05:17,019 --> 01:05:18,970
небольшое изменение, это очень похоже на то, что

1746
01:05:18,970 --> 01:05:20,440
мы уже делали для

1747
01:05:20,440 --> 01:05:24,309
весов целевой сети, и оказывается,

1748
01:05:24,309 --> 01:05:25,989
что во многих случаях это дает вам огромное преимущество

1749
01:05:25,989 --> 01:05:29,739
для игр Atari, так что это

1750
01:05:29,739 --> 01:05:31,239
что-то  at обычно очень

1751
01:05:31,239 --> 01:05:33,759
полезно делать и дает пользователям

1752
01:05:33,759 --> 01:05:34,930
немедленный значительный прирост

1753
01:05:34,930 --> 01:05:37,299
производительности, поэтому вы знаете, что

1754
01:05:37,299 --> 01:05:38,739
эквивалентно небольшому количеству

1755
01:05:38,739 --> 01:05:41,109
кодирования, это одна идея,

1756
01:05:41,109 --> 01:05:42,609
и такой прямой подъем от

1757
01:05:42,609 --> 01:05:44,829
того, что вы знаете, двойной Q, изучая

1758
01:05:44,829 --> 01:05:47,769
вторую вещь.  является приоритетом B, так что

1759
01:05:47,769 --> 01:05:49,930
давайте вернемся к примеру с марсоходом

1760
01:05:49,930 --> 01:05:53,650
I, поэтому в марсоходе у нас была очень

1761
01:05:53,650 --> 01:05:55,210
маленькая область, мы говорим о табличных

1762
01:05:55,210 --> 01:05:58,509
настройках всего через семь состояний, и мы

1763
01:05:58,509 --> 01:06:00,339
говорили о политике, которая просто

1764
01:06:00,339 --> 01:06:02,319
всегда предпринимала действия a1, которые превращались

1765
01:06:02,319 --> 01:06:04,089
чтобы в основном идти влево, поэтому у нас была эта

1766
01:06:04,089 --> 01:06:07,869
траектория, мы начали с состояния s3, мы

1767
01:06:07,869 --> 01:06:10,450
предприняли действие a1, мы получили нулевое вознаграждение,

1768
01:06:10,450 --> 01:06:14,529
мы перешли к s2, мы остались в s2 на один

1769
01:06:14,529 --> 01:06:16,029
раунд, когда мы сделали 1, а затем, в

1770
01:06:16,029 --> 01:06:18,279
конце концов, перешли к s1, а затем  мы

1771
01:06:18,279 --> 01:06:22,900
закончили так, что это было так, и

1772
01:06:22,900 --> 01:06:25,210
первая оценка Монте-Карло посещения

1773
01:06:25,210 --> 01:06:28,869
для каждого состояния была 1 1 1 0 0 0 0 0, а

1774
01:06:28,869 --> 01:06:32,049
оценка TP с альфа, равной 1, была

1775
01:06:32,049 --> 01:06:35,200
такой, и это было, когда мы говорили о

1776
01:06:35,200 --> 01:06:37,089
том факте, что телевидение использует только каждый  данные

1777
01:06:37,089 --> 01:06:38,739
пои  nt один раз, и он не передал

1778
01:06:38,739 --> 01:06:41,200
информацию обратно, поэтому единственным обновлением для

1779
01:06:41,200 --> 01:06:43,749
обучения TD было то, что когда мы достигли состояния s1,

1780
01:06:43,749 --> 01:06:46,089
мы выполнили действие 1, мы получили вознаграждение 1,

1781
01:06:46,089 --> 01:06:48,009
а затем мы завершили работу, поэтому мы только

1782
01:06:48,009 --> 01:06:51,130
обновили значение состояния, так что теперь

1783
01:06:51,130 --> 01:06:53,319
давайте  представьте, что вы можете это сделать,

1784
01:06:53,319 --> 01:06:55,029
теперь вы думаете о том, какими будут ваши лайки или

1785
01:06:55,029 --> 01:06:56,410
буфер воспроизведения, в этом случае у

1786
01:06:56,410 --> 01:06:57,849
вас будет что-то вроде этого, у вас есть

1787
01:06:57,849 --> 01:07:05,589
s3 a1 0 s 2 s 2 a 1 0 s 2 s 2 a 1 0 s 1

1788
01:07:05,589 --> 01:07:10,420
s 1 a 1 1 terminate вот как будет выглядеть ваш

1789
01:07:10,420 --> 01:07:14,079
буфер воспроизведения, поэтому,

1790
01:07:14,079 --> 01:07:16,299
скажем, вы можете выбрать воспроизведение резервных копий,

1791
01:07:16,299 --> 01:07:18,249
чтобы сделать это, чтобы у вас было четыре возможных

1792
01:07:18,249 --> 01:07:20,559
резервных копии воспроизведения, вы можете выбрать одну и ту же дважды,

1793
01:07:20,559 --> 01:07:22,809
если хотите, и я собираюсь  попросить вас

1794
01:07:22,809 --> 01:07:25,329
выбрать для повторного воспроизведения резервных копий, чтобы каким-то образом

1795
01:07:25,329 --> 01:07:26,890
улучшить функцию ценности

1796
01:07:26,890 --> 01:07:31,299
, и я хотел бы, чтобы вы на

1797
01:07:31,299 --> 01:07:32,710
секунду задумались или поговорили с нашим соседом о том,

1798
01:07:32,710 --> 01:07:34,480
какую из двух вы должны выбрать, почему

1799
01:07:34,480 --> 01:07:38,019
и в каком порядке вы бы их сделали.

1800
01:07:38,019 --> 01:07:39,549
также и имеет ли это какое-то значение,

1801
01:07:39,549 --> 01:07:41,170
может быть, на самом деле это не имеет значения, вы просто

1802
01:07:41,170 --> 01:07:42,609
выбираете любой из этих y  вы получите

1803
01:07:42,609 --> 01:07:43,779
одну и ту же функцию значения независимо от того, что вы

1804
01:07:43,779 --> 01:07:46,900
делаете, так что есть ли два обновления, которые

1805
01:07:46,900 --> 01:07:48,970
особенно хороши, если да, то почему и в каком

1806
01:07:48,970 --> 01:08:59,560
порядке, надеюсь, подумать об этом

1807
01:08:59,560 --> 01:09:02,439
на секунду, во-первых, имеет ли это значение,

1808
01:09:02,439 --> 01:09:05,469
поэтому я собираюсь сначала  задайте вам, ребята,

1809
01:09:05,469 --> 01:09:07,359
вопрос, проголосуйте, если вы считаете, что имеет значение,

1810
01:09:07,359 --> 01:09:09,040
какие из них вы выберете с точки зрения

1811
01:09:09,040 --> 01:09:11,620
функции значения, которую вы получите, это правильно,

1812
01:09:11,620 --> 01:09:13,359
поэтому абсолютно важно, какие два вы

1813
01:09:13,359 --> 01:09:14,649
выберете с точки зрения результирующей функции значения,

1814
01:09:14,649 --> 01:09:16,210
вы не получите ту же

1815
01:09:16,210 --> 01:09:20,139
функцию значения нет  независимо от того, что вы выберете сейчас,

1816
01:09:20,139 --> 01:09:21,279
я попрошу еще один, Бодинс спросит,

1817
01:09:21,279 --> 01:09:22,600
какой из них мы должны сделать первым,

1818
01:09:22,600 --> 01:09:24,880
мы должны обновить его, мы должны сделать для первого или

1819
01:09:24,880 --> 01:09:26,560
он последний в нашем буфере воспроизведения.

1820
01:09:26,560 --> 01:09:29,469


1821
01:09:29,469 --> 01:09:31,139


1822
01:09:31,139 --> 01:09:32,679


1823
01:09:32,679 --> 01:09:34,359
правильно, так что у троек есть это - кто-то

1824
01:09:34,359 --> 01:09:40,029
хочет объяснить, почему да, я вижу, у вас

1825
01:09:40,029 --> 01:09:42,310
есть спина, распространяйте информацию, которую вы

1826
01:09:42,310 --> 01:09:45,448
уже знаете, в состоянии один, чтобы указать два,

1827
01:09:45,448 --> 01:09:48,670
да, так что правильно, так что если вы заберете

1828
01:09:48,670 --> 01:09:51,130
резервную копию три, так что резервная копия три это

1829
01:09:51,130 --> 01:09:55,480
s 2 A 1 0  s 1, поэтому, если вы сделаете резервную

1830
01:09:55,480 --> 01:09:59,080
копию, это будет 0 плюс гамма V из s простого числа s

1831
01:09:59,080 --> 01:10:02,260
1, а это 1, так что это означает, что теперь

1832
01:10:02,260 --> 01:10:04,390
вы получите 2 резервных копии, и поэтому теперь ваш B

1833
01:10:04,390 --> 01:10:06,880
из s 2 будет равен  на 1, так что вы

1834
01:10:06,880 --> 01:10:11,590
можете создать резервную копию этой информации, поэтому

1835
01:10:11,590 --> 01:10:14,380
я не был особо конкретен в том, что

1836
01:10:14,380 --> 01:10:16,330
здесь правильно делать,

1837
01:10:16,330 --> 01:10:19,060
но главное, что я

1838
01:10:19,060 --> 01:10:20,140
хотел подчеркнуть, что это имеет большое

1839
01:10:20,140 --> 01:10:20,679
значение

1840
01:10:20,679 --> 01:10:22,360
и что это будет  иметь значение с точки зрения

1841
01:10:22,360 --> 01:10:23,800
порядка что будет следующим, что мы должны сделать,

1842
01:10:23,800 --> 01:10:25,630
если мы действительно поднимем руку, если мы

1843
01:10:25,630 --> 01:10:28,540
должны сделать три, еще раз поднимите руку, если

1844
01:10:28,540 --> 01:10:31,210
мы должны сделать поднять вашу руку, если мы

1845
01:10:31,210 --> 01:10:34,380
должны сделать один да, у тех есть это

1846
01:10:34,380 --> 01:10:46,780
кто-то объяснит, почему это  верно, да, поэтому,

1847
01:10:46,780 --> 01:10:48,940
если вы хотите пройти весь путь до

1848
01:10:48,940 --> 01:10:50,380
оценки Монте-Карло, то, что

1849
01:10:50,380 --> 01:10:53,710
вы хотели бы сделать здесь, это сделать s 3 a 1 0

1850
01:10:53,710 --> 01:10:56,350
s 2, что позволит обновить B of s 3

1851
01:10:56,350 --> 01:10:58,840
до 1  и в этот момент ваша

1852
01:10:58,840 --> 01:11:00,400
функция ценности будет точно такой же,

1853
01:11:00,400 --> 01:11:03,790
как Монте-Карло, поэтому определенно

1854
01:11:03,790 --> 01:11:05,199
имеет значение порядок, в котором

1855
01:11:05,199 --> 01:11:07,090
вы сделали это, сделайте это, если бы вы сделали s 3

1856
01:11:07,090 --> 01:11:09,960
a-10s, чтобы ваш s 3 не изменился,

1857
01:11:09,960 --> 01:11:12,340
поэтому порядок может иметь большое значение, поэтому мы

1858
01:11:12,340 --> 01:11:14,320
не только хотим подумать о

1859
01:11:14,320 --> 01:11:16,270
том, что поднималось раньше, но я

1860
01:11:16,270 --> 01:11:17,860
думаю, что тогда  сказать, что мы должны

1861
01:11:17,860 --> 01:11:20,560
поместить в наш буфер воспроизведения, вы не только

1862
01:11:20,560 --> 01:11:21,520
не думаете о том, что должно быть в

1863
01:11:21,520 --> 01:11:22,960
буфере воспроизведения, но и в каком порядке мы

1864
01:11:22,960 --> 01:11:24,520
их семплируем, может иметь большое значение с

1865
01:11:24,520 --> 01:11:28,060
точки зрения скорости сходимости, и, в

1866
01:11:28,060 --> 01:11:29,560
частности, есть некоторые действительно  классная

1867
01:11:29,560 --> 01:11:30,880
работа пару лет назад, рассматривающая это

1868
01:11:30,880 --> 01:11:32,590
формально, например, как важен порядок,

1869
01:11:32,590 --> 01:11:36,159
поэтому есть эта статья еще в

1870
01:11:36,159 --> 01:11:38,170
2016 году, в которой пытались посмотреть, каким

1871
01:11:38,170 --> 01:11:40,210
будет оптимальный порядок, так что представьте, что у

1872
01:11:40,210 --> 01:11:42,190
вас есть Oracle, который может точно

1873
01:11:42,190 --> 01:11:44,469
вычислить сейчас это  будет трудновыполнимым с

1874
01:11:44,469 --> 01:11:45,610
вычислительной точки зрения, мы не

1875
01:11:45,610 --> 01:11:46,929
сможем сделать это в общем,

1876
01:11:46,929 --> 01:11:48,580
представьте, что Oracle может пройти и

1877
01:11:48,580 --> 01:11:50,110
выбрать и выяснить, какой именно

1878
01:11:50,110 --> 01:11:52,630
правильный порядок, то что они обнаружили

1879
01:11:52,630 --> 01:11:54,370
в этом случае, так это то, что для этого или небольшого

1880
01:11:54,370 --> 01:11:56,560
цепь как  Пример, и вы получаете это

1881
01:11:56,560 --> 01:11:58,150
экспоненциальное улучшение сходимости,

1882
01:11:58,150 --> 01:12:00,010
что довольно круто, так что

1883
01:12:00,010 --> 01:12:02,800
это означает, что количество обновлений, которые вам нужно

1884
01:12:02,800 --> 01:12:04,239
сделать, пока ваша функция значения не

1885
01:12:04,239 --> 01:12:05,679
сойдется к правильной вещи, может быть

1886
01:12:05,679 --> 01:12:07,600
экспоненциально меньше, если вы тщательно обновляете,

1887
01:12:07,600 --> 01:12:10,000
и у вас может быть Oracle

1888
01:12:10,000 --> 01:12:11,739
сказать вам, какой именно кортеж сэмплировать,

1889
01:12:11,739 --> 01:12:14,230
что очень круто, так что вы могли бы быть

1890
01:12:14,230 --> 01:12:16,000
намного лучше, но вы не можете этого сделать, вы

1891
01:12:16,000 --> 01:12:17,949
не собираетесь тратить все это, это

1892
01:12:17,949 --> 01:12:19,510
очень дорого в вычислительном отношении или

1893
01:12:19,510 --> 01:12:20,770
невозможно в некоторых случаях выяснить,

1894
01:12:20,770 --> 01:12:23,170
что именно  что я должен упорядочивать Oracle

1895
01:12:23,170 --> 01:12:25,989
, но это иллюстрирует,

1896
01:12:25,989 --> 01:12:27,130
что мы могли бы быть осторожны

1897
01:12:27,130 --> 01:12:29,350
с порядком, в котором мы это делаем, поэтому

1898
01:12:29,350 --> 01:12:33,610
их интуиция для этого была такой: давайте попробуем

1899
01:12:33,610 --> 01:12:35,710
определить приоритет кортежа для воспроизведения в

1900
01:12:35,710 --> 01:12:40,989
соответствии с его ошибкой DQ n, поэтому  Ошибка DQ n

1901
01:12:40,989 --> 01:12:43,239
в этом случае - это просто наша

1902
01:12:43,239 --> 01:12:44,800
ошибка обучения TD, поэтому это будет

1903
01:12:44,800 --> 01:12:46,929
разница между нашим текущим, это в

1904
01:12:46,929 --> 01:12:49,360
основном наша ошибка прогноза, так что

1905
01:12:49,360 --> 01:12:52,510
это наша ошибка прогноза, почти наша

1906
01:12:52,510 --> 01:12:54,580
ошибка прогнозирования, я просто назову это TD,

1907
01:12:54,580 --> 01:12:55,750
потому что это не совсем так, потому что мы

1908
01:12:55,750 --> 01:12:58,060
делаем это максимально, так что это похоже на наш

1909
01:12:58,060 --> 01:13:07,960
прогноз или - наш текущий - скажем, если у

1910
01:13:07,960 --> 01:13:09,489
вас действительно очень большая ошибка, тогда

1911
01:13:09,489 --> 01:13:10,929
мы расставим приоритеты по обновлению этого

1912
01:13:10,929 --> 01:13:13,630
больше, и вы обновляете это тихое количество

1913
01:13:13,630 --> 01:13:14,949
при каждом обновлении, которое вы устанавливаете для новых

1914
01:13:14,949 --> 01:13:17,080
кортежей равным нулю, и один метод, у них

1915
01:13:17,080 --> 01:13:19,030
есть два разных метода для попытки

1916
01:13:19,030 --> 01:13:21,250
сделать такого рода расстановку приоритетов, но один

1917
01:13:21,250 --> 01:13:25,300
метод в основном берет эти приоритеты,

1918
01:13:25,300 --> 01:13:28,270
повышает их до некоторой мощности альфа, а затем

1919
01:13:28,270 --> 01:13:30,850
нормализует, а затем это

1920
01:13:30,850 --> 01:13:34,090
вероятность выбора этого кортежа, поэтому

1921
01:13:34,090 --> 01:13:35,380
вы расставляете приоритеты для большего количества вещей, которые ждут,

1922
01:13:35,380 --> 01:13:40,449
так что да, я замораживаю старые

1923
01:13:40,449 --> 01:13:43,239
веса или счетчик для распространения

1924
01:13:43,239 --> 01:13:44,770
обратно информации, как если бы вы

1925
01:13:44,770 --> 01:13:47,260
заморозили старые способы, и пример, который мы

1926
01:13:47,260 --> 01:13:49,090
просто пройдя после того, как вы только

1927
01:13:49,090 --> 01:13:51,040
распространили один раз, когда вы

1928
01:13:51,040 --> 01:13:52,780
больше не сможете этого делать,

1929
01:13:52,780 --> 01:13:54,270
потому что ваше значение по-прежнему будет равно нулю,

1930
01:13:54,270 --> 01:13:56,560
это имеет большое значение, а именно, если

1931
01:13:56,560 --> 01:13:58,300
вы исправили

1932
01:13:58,300 --> 01:14:01,270
свой w-  n если бы вы смотрели на наш

1933
01:14:01,270 --> 01:14:03,580
случай, который у нас был раньше, вы

1934
01:14:03,580 --> 01:14:04,540
бы не смогли продолжить распространение

1935
01:14:04,540 --> 01:14:05,800
этого назад, потому что вы еще не

1936
01:14:05,800 --> 01:14:07,510
обновились бы, да, это совершенно правильно, так

1937
01:14:07,510 --> 01:14:08,530
что будет это напряжение между тем,

1938
01:14:08,530 --> 01:14:10,630
когда вы исправляете вещи по сравнению с или

1939
01:14:10,630 --> 01:14:15,220
распространение информации обратно, и

1940
01:14:15,220 --> 01:14:16,870
это внимание, что нужно прямо

1941
01:14:16,870 --> 01:14:17,680
понять, что не обязательно есть

1942
01:14:17,680 --> 01:14:19,480
принципиальные способы для того, чтобы именно то, что

1943
01:14:19,480 --> 01:14:20,830
правильное расписание должно сделать это, но это

1944
01:14:20,830 --> 01:14:32,380
гиперпараметр, чтобы сделать изменение хорошо,

1945
01:14:32,380 --> 01:14:34,510
так что порядок имеет значение вообще в этом случае

1946
01:14:34,510 --> 01:14:36,070
он все еще  имеет значение, потому что мы все еще

1947
01:14:36,070 --> 01:14:38,440
будем задерживаться, и повторение

1948
01:14:38,440 --> 01:14:39,940
весов будет меняться в течение периода времени, в

1949
01:14:39,940 --> 01:14:41,290
течение которого будет воспроизводиться

1950
01:14:41,290 --> 01:14:43,240
этот буфер, так что буфер может быть примерно

1951
01:14:43,240 --> 01:14:44,950
миллион, и вы можете обновлять свои

1952
01:14:44,950 --> 01:14:46,360
веса, например, каждые пятьдесят шагов или

1953
01:14:46,360 --> 01:14:47,410
что-то вроде  так что в вашем буфере воспроизведения все еще

1954
01:14:47,410 --> 01:14:48,610
будет целая куча точек данных

1955
01:14:48,610 --> 01:14:51,190
в I, о которых

1956
01:14:51,190 --> 01:14:52,690
полезно подумать теперь, когда ваши

1957
01:14:52,690 --> 01:14:54,490
веса изменили порядок

1958
01:14:54,490 --> 01:14:55,870
хотите ли вы пройтись по ним, это

1959
01:14:55,870 --> 01:14:59,710
отличный вопрос, хорошо, так что это за метод

1960
01:14:59,710 --> 01:15:02,830
, позвольте мне просто уточнить, сказали ли мы,

1961
01:15:02,830 --> 01:15:04,600
что альфа равна нулю,

1962
01:15:04,600 --> 01:15:06,250
каково правило выбора среди

1963
01:15:06,250 --> 01:15:12,280
существующих кортежей, так что P - это наш вид,

1964
01:15:12,280 --> 01:15:14,860
в основном, наш DQ n  ошибка, если мы установим

1965
01:15:14,860 --> 01:15:18,640
альфу равной нулю, вы знаете, что это

1966
01:15:18,640 --> 01:15:22,150
правильно, да, так что этот вид компромисса

1967
01:15:22,150 --> 01:15:24,990
между униформой без приоритетов и

1968
01:15:24,990 --> 01:15:27,610
полным выбором того, что мне нравится,

1969
01:15:27,610 --> 01:15:29,350
если альфа бесконечность, то это будет

1970
01:15:29,350 --> 01:15:30,520
выбор тот, у которого самый высокий внутренний DQ,

1971
01:15:30,520 --> 01:15:32,050
поэтому  это компромисс это

1972
01:15:32,050 --> 01:15:36,190
стохастический ладно, тогда они комбинируют

1973
01:15:36,190 --> 01:15:37,780
это с какой-то причиной, по которой я выбираю

1974
01:15:37,780 --> 01:15:39,160
эти три порядка, это то, что они все как

1975
01:15:39,160 --> 01:15:40,330
бы накладываются друг на друга, так что

1976
01:15:40,330 --> 01:15:43,060
отдайте предпочтение повторению, а не я думаю, что это

1977
01:15:43,060 --> 01:15:45,400
повтор неполный рабочий день плюс  DM двойной DQ n

1978
01:15:45,400 --> 01:15:47,710
против просто двойного D QN в большинстве случаев

1979
01:15:47,710 --> 01:15:49,900
это ноль, если бы они оба были

1980
01:15:49,900 --> 01:15:52,960
одинаковыми внизу означает плоский ванильный DQ

1981
01:15:52,960 --> 01:15:55,300
двойной D QN лучше выше означает, что

1982
01:15:55,300 --> 01:15:56,530
приоритет повтора лучше в

1983
01:15:56,530 --> 01:15:58,390
большинстве случаев, когда приоритет повтора

1984
01:15:58,390 --> 01:16:00,970
лучше  э, и здесь есть некоторые гиперпараметры, с которыми

1985
01:16:00,970 --> 01:16:02,290
можно поиграться, но в большинстве случаев это

1986
01:16:02,290 --> 01:16:03,820
полезно, и, безусловно, полезно

1987
01:16:03,820 --> 01:16:05,410
подумать о том, что вы знаете, где порядок может иметь

1988
01:16:05,410 --> 01:16:07,930
значение, хорошо, у нас осталось не так много

1989
01:16:07,930 --> 01:16:09,610
времени, так что я просто собираюсь

1990
01:16:09,610 --> 01:16:11,380
вкратце  это просто для того, чтобы вы знали об

1991
01:16:11,380 --> 01:16:15,750
этом лучшие документы с ICML 2016 были должным

1992
01:16:15,930 --> 01:16:20,170
образом идея состоит в том, что если вы хотите принимать

1993
01:16:20,170 --> 01:16:21,940
решения в мире, могут быть

1994
01:16:21,940 --> 01:16:23,950
некоторые государства, которые лучше или хуже, и

1995
01:16:23,950 --> 01:16:25,060
они просто будут иметь более высокую ценность или

1996
01:16:25,060 --> 01:16:26,290
более низкое значение, но то, что вы действительно

1997
01:16:26,290 --> 01:16:27,730
хотите сделать, это выяснить,

1998
01:16:27,730 --> 01:16:29,140
какое правильное действие должно быть сделано в

1999
01:16:29,140 --> 01:16:31,300
конкретном состоянии, и поэтому то, что вы

2000
01:16:31,300 --> 01:16:33,340
хотите понять, это

2001
01:16:33,340 --> 01:16:35,710
функция преимущества, которую вы хотите знать,

2002
01:16:35,710 --> 01:16:37,240
насколько лучше или хуже  выполнение определенного

2003
01:16:37,240 --> 01:16:39,460
действия противопоставляется следованию текущей

2004
01:16:39,460 --> 01:16:42,730
политике, но на самом деле, как будто я не забочусь

2005
01:16:42,730 --> 01:16:44,320
о том, чтобы принять ценность состояния, я

2006
01:16:44,320 --> 01:16:45,880
забочусь о том, чтобы понять,

2007
01:16:45,880 --> 01:16:47,290
какое из действий имеет лучшую

2008
01:16:47,290 --> 01:16:49,840
ценность, упуская из виду эту функцию преимущества,

2009
01:16:49,840 --> 01:16:53,830
так что они  Что нужно сделать для этого, так это

2010
01:16:53,830 --> 01:16:56,020
то, что в отличие от dqn, где вы выводите

2011
01:16:56,020 --> 01:16:58,270
все реплики, они будут разделены,

2012
01:16:58,270 --> 01:16:59,680
и они собираются сначала оценить

2013
01:16:59,680 --> 01:17:00,970
значение состояния, и они собираются

2014
01:17:00,970 --> 01:17:02,950
оценить эту функцию преимущества, которая

2015
01:17:02,950 --> 01:17:09,520
равна Q of s a 1 минус  V of s Q of s a 2

2016
01:17:09,520 --> 01:17:12,580
минус V of S, они просто собираются разделить

2017
01:17:12,580 --> 01:17:14,350
это, это выбор архитектуры, и

2018
01:17:14,350 --> 01:17:16,440
они собираются рекомбинировать их для Q,

2019
01:17:16,440 --> 01:17:18,610
а затем утверждают, что это поможет нам

2020
01:17:18,610 --> 01:17:20,050
сфокусироваться на сигнале, который нас волнует.

2021
01:17:20,050 --> 01:17:22,060
этого достаточно, чтобы точно

2022
01:17:22,060 --> 01:17:25,710
оценить, какое действие лучше или хуже,

2023
01:17:26,460 --> 01:17:28,210
есть интересные вопросы о

2024
01:17:28,210 --> 01:17:29,710
том, можно ли это идентифицировать, у

2025
01:17:29,710 --> 01:17:31,000
меня нет достаточно времени, чтобы вдаваться в них

2026
01:17:31,000 --> 01:17:33,280
сегодня, это невозможно идентифицировать, я рад

2027
01:17:33,280 --> 01:17:35,890
поговорить обо всем этом в автономном режиме и

2028
01:17:35,890 --> 01:17:38,650
Я причина, по которой это важно, заключается в том, что они

2029
01:17:38,650 --> 01:17:40,330
просто вынуждают делать какие-то

2030
01:17:40,330 --> 01:17:43,900
предположения по умолчанию об определении

2031
01:17:43,900 --> 01:17:47,890
функций преимущества эмпирически, это

2032
01:17:47,890 --> 01:17:48,910
часто очень полезно,

2033
01:17:48,910 --> 01:17:51,730
так что еще раз по сравнению с двойным DQ n с

2034
01:17:51,730 --> 01:17:53,140
приоритетным повтором, который мы только что видели,

2035
01:17:53,140 --> 01:17:55,210
который был  уже лучше, чем W double

2036
01:17:55,210 --> 01:17:57,880
DQ n, который также лучше, чем DQ n, это

2037
01:17:57,880 --> 01:17:59,290
снова дает вам еще один существенный прирост производительности,

2038
01:17:59,290 --> 01:18:01,450
и поэтому в основном

2039
01:18:01,450 --> 01:18:04,030
это три три разных,

2040
01:18:04,030 --> 01:18:05,800
которые появились в течение двух

2041
01:18:05,800 --> 01:18:07,360
лет после DQ n, которые начали делать

2042
01:18:07,360 --> 01:18:09,100
некоторые  действительно большие производительные игры

2043
01:18:09,100 --> 01:18:10,510
по сравнению с тем, чтобы просто делать полностью

2044
01:18:10,510 --> 01:18:13,450
ванильный наш DQ n для домашнего задания два, которые вы

2045
01:18:13,450 --> 01:18:15,280
собираетесь реализовывать DQ n не

2046
01:18:15,280 --> 01:18:16,390
другие, которые вы можете

2047
01:18:16,390 --> 01:18:17,890
реализовать некоторые из других,

2048
01:18:17,890 --> 01:18:19,300
просто хорошо знать, что они

2049
01:18:19,300 --> 01:18:21,100
какие-то основные первоначальные улучшения

2050
01:18:21,100 --> 01:18:22,510
для значительного повышения

2051
01:18:22,510 --> 01:18:23,579
производительности на Atari,

2052
01:18:23,579 --> 01:18:26,219
я оставлю их, у нас почти нет

2053
01:18:26,219 --> 01:18:28,290
времени, не стесняйтесь посмотреть на последние

2054
01:18:28,290 --> 01:18:29,639
пару слайдов, чтобы получить несколько практических

2055
01:18:29,639 --> 01:18:31,650
советов, которые исходил от Джона Шульмана Джон

2056
01:18:31,650 --> 01:18:33,270
Шульман  был аспирантом в Беркли

2057
01:18:33,270 --> 01:18:34,920
, который теперь является одним из руководителей открытия.

2058
01:18:34,920 --> 01:18:38,190
Я только одно, что я

2059
01:18:38,190 --> 01:18:40,739
обязательно подчеркну: может быть очень заманчиво

2060
01:18:40,739 --> 01:18:43,050
попытаться начать с внедрения ключевого

2061
01:18:43,050 --> 01:18:45,360
обучения.  g непосредственно на Atari, я настоятельно

2062
01:18:45,360 --> 01:18:47,010
рекомендую вам сначала пройтись

2063
01:18:47,010 --> 01:18:48,480
по порядку задания и,

2064
01:18:48,480 --> 01:18:50,309
как и в линейном случае, убедиться, что ваше

2065
01:18:50,309 --> 01:18:52,110
обучение репликам полностью работает, и прежде

2066
01:18:52,110 --> 01:18:54,510
чем развертывать на Atari даже

2067
01:18:54,510 --> 01:18:56,280
небольшие игры, такие как понг, которые мы  Я постоянно

2068
01:18:56,280 --> 01:18:58,619
работаю над этим, и это отнимает огромное количество

2069
01:18:58,619 --> 01:19:00,630
времени, и поэтому с точки зрения простого

2070
01:19:00,630 --> 01:19:02,040
понимания и отладки

2071
01:19:02,040 --> 01:19:03,420
гораздо лучше убедиться, что вы знаете, что ваш

2072
01:19:03,420 --> 01:19:05,219
метод обучения сигналам работает, прежде

2073
01:19:05,219 --> 01:19:07,139
чем ждать 12 часов, чтобы увидеть, не

2074
01:19:07,139 --> 01:19:09,840
сработал он или нет.  узнайте что-нибудь о понге,

2075
01:19:09,840 --> 01:19:11,639
чтобы была причина, по которой мы как бы

2076
01:19:11,639 --> 01:19:13,639
строим то, что мы делаем в задании,

2077
01:19:13,639 --> 01:19:15,989
просто еще один практический совет из нескольких других

2078
01:19:15,989 --> 01:19:18,150
практических советов, не стесняйтесь смотреть на

2079
01:19:18,150 --> 01:19:21,690
них, и в четверг больше не встретимся.

2080
01:19:21,690 --> 01:19:24,500
Спасибо.

