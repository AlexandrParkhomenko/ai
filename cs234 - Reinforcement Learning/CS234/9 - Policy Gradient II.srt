1
00:00:05,000 --> 00:00:07,560
All right. Welcome back everybody.

2
00:00:07,560 --> 00:00:10,050
Um, before we get started today,

3
00:00:10,050 --> 00:00:13,110
I- does anybody have any questions about logistics,

4
00:00:13,110 --> 00:00:14,670
or midterm, or anything like that?

5
00:00:14,670 --> 00:00:16,710
We'll be doing a midterm review on Monday,

6
00:00:16,710 --> 00:00:18,870
and the midterm will be on Wednesday.

7
00:00:18,870 --> 00:00:20,910
Because there's a number of people in the class,

8
00:00:20,910 --> 00:00:23,430
we're gonna be spreading everybody across a couple of rooms,

9
00:00:23,430 --> 00:00:26,310
and will be, er, sending instructions out about that.

10
00:00:26,310 --> 00:00:31,380
Does anybody have any other logistics questions? Yeah.

11
00:00:31,380 --> 00:00:32,970
Midterm is during class time right?

12
00:00:32,970 --> 00:00:34,485
Midterm is during class time.

13
00:00:34,485 --> 00:00:36,500
Um, instructions will also be on the web,

14
00:00:36,500 --> 00:00:39,650
but you're allowed to bring a one-page, um, of

15
00:00:39,650 --> 00:00:43,130
written notes, um, aside from that everything is closed book. Yeah.

16
00:00:43,130 --> 00:00:47,040
Is it okay to type said notes [NOISE] or it has to be handwritten?

17
00:00:47,040 --> 00:00:48,900
I think we've already issued a policy on that,

18
00:00:48,900 --> 00:00:50,220
Let me just double-check and see what it is.

19
00:00:50,220 --> 00:00:50,580
Okay.

20
00:00:50,580 --> 00:00:56,820
[BACKGROUND]

21
00:00:56,820 --> 00:00:59,170
All right. Okay. Lets go ahead and get started.

22
00:00:59,170 --> 00:01:01,060
Um, before we do that,

23
00:01:01,060 --> 00:01:02,960
I just want to say thank you to all of you that, ah,

24
00:01:02,960 --> 00:01:05,050
participated in the class feedback survey.

25
00:01:05,050 --> 00:01:06,860
It's really helpful to me and to everybody else to

26
00:01:06,860 --> 00:01:10,380
understand what's helping you learn and what's not helping you learn.

27
00:01:10,910 --> 00:01:13,830
Okay. So in terms of,

28
00:01:13,830 --> 00:01:15,435
um, the responses, so note,

29
00:01:15,435 --> 00:01:19,220
you know, all of these things there's about 230 people registered in the class.

30
00:01:19,220 --> 00:01:21,260
Um, so for some of you if you didn't give me feedback,

31
00:01:21,260 --> 00:01:24,200
it's hard for me to know what- what's helping you or not helping you learn.

32
00:01:24,200 --> 00:01:27,005
So we're just gonna go with what people gave us feedback on.

33
00:01:27,005 --> 00:01:30,945
Um, about 65% of you thought it's the right pace,

34
00:01:30,945 --> 00:01:33,600
about 27% of you thought it's going too fast,

35
00:01:33,600 --> 00:01:36,995
and there's only about 8% of people that think it's going too slowly.

36
00:01:36,995 --> 00:01:42,095
Um, so we're gonna keep roughly in the same pace as what we've had before.

37
00:01:42,095 --> 00:01:46,400
Um, a number of people noted that they wished it was like a semester-long course.

38
00:01:46,400 --> 00:01:49,250
Um, I will mention there's a number of other classes that

39
00:01:49,250 --> 00:01:52,170
do reinforcement learning and I highly encourage you to take them.

40
00:01:52,170 --> 00:01:53,915
I- I offer an advanced class,

41
00:01:53,915 --> 00:01:58,420
and also Ben Van Roy offers a class normally in the spring that's more theoretical.

42
00:01:58,420 --> 00:02:00,320
This was super controversial.

43
00:02:00,320 --> 00:02:02,330
I didn't think it would be this split.

44
00:02:02,330 --> 00:02:04,880
Um, so we offer sessions on Thursday and Friday.

45
00:02:04,880 --> 00:02:06,770
Attendance has been really low.

46
00:02:06,770 --> 00:02:08,389
Um, I think we've had around like,

47
00:02:08,389 --> 00:02:11,575
three to seven people showing up for these sessions.

48
00:02:11,575 --> 00:02:15,335
Um, so we thought this was gonna be something that everybody wanted to take out.

49
00:02:15,335 --> 00:02:17,210
Because it's about evenly split,

50
00:02:17,210 --> 00:02:20,060
I asked all the TAs to compare that people that are coming

51
00:02:20,060 --> 00:02:23,165
to their office hours versus the people that are coming to the sessions.

52
00:02:23,165 --> 00:02:24,650
Um, we probably have

53
00:02:24,650 --> 00:02:29,195
about 4 to 5x people trying to go to office hours than trying to go to sessions.

54
00:02:29,195 --> 00:02:31,615
So we're going to switch this to office hours.

55
00:02:31,615 --> 00:02:33,460
Just so that we can kind of,

56
00:02:33,460 --> 00:02:35,655
serve as many people as we can.

57
00:02:35,655 --> 00:02:38,270
Um, so let me just write that there.

58
00:02:38,270 --> 00:02:41,255
So we're gonna switch to office hours.

59
00:02:41,255 --> 00:02:44,650
Now, the sessions will still be offered on the other days,

60
00:02:44,650 --> 00:02:45,890
and they'll still be recorded.

61
00:02:45,890 --> 00:02:49,445
So for anybody who wanted to go to them on Thursday and Friday, you can still go,

62
00:02:49,445 --> 00:02:53,180
you can still participate on Zoom or you can watch the recorded lecture.

63
00:02:53,180 --> 00:02:55,250
Um, but we're gonna switch this to office hours

64
00:02:55,250 --> 00:02:57,710
because my TAs have been saying they've had

65
00:02:57,710 --> 00:02:59,720
a number of office hours where they've either had to stay

66
00:02:59,720 --> 00:03:02,090
really late or they feel like they're not getting to some people.

67
00:03:02,090 --> 00:03:05,465
And so again, just in terms of serving the most people.

68
00:03:05,465 --> 00:03:08,330
I will say when I was going through these responses,

69
00:03:08,330 --> 00:03:10,190
it really made me think about the fact that in

70
00:03:10,190 --> 00:03:13,130
reinforcement learning and sort of sequential decision making in general,

71
00:03:13,130 --> 00:03:16,070
um, we're always optimizing for expected rewards [LAUGHTER]

72
00:03:16,070 --> 00:03:19,580
And so that's kind of exactly the same thing we're doing here is that,

73
00:03:19,580 --> 00:03:21,770
I know that everybody needs slightly different things,

74
00:03:21,770 --> 00:03:24,275
and we're just trying to do our best job and expectation.

75
00:03:24,275 --> 00:03:26,615
Um, but it's exactly why things like

76
00:03:26,615 --> 00:03:29,255
intelligent tutoring systems and other stuff might be better.

77
00:03:29,255 --> 00:03:30,720
Um, okay.

78
00:03:30,720 --> 00:03:33,950
In terms of things that people thought were working well for them,

79
00:03:33,950 --> 00:03:36,410
we've got a number of positive remarks about

80
00:03:36,410 --> 00:03:39,365
doing worked examples in class, doing derivations.

81
00:03:39,365 --> 00:03:43,800
Um, a lot of people really like the fact that my iPad had problems on Monday,

82
00:03:43,800 --> 00:03:46,005
um, and so we did things on the board.

83
00:03:46,005 --> 00:03:50,420
So we'll try to keep doing the same amount or more of derivations.

84
00:03:50,420 --> 00:03:54,010
Um, people also were generally really positive about the homeworks.

85
00:03:54,010 --> 00:03:56,175
In terms of things, er,

86
00:03:56,175 --> 00:03:58,545
we saw repeatedly, so- so what I did is I got,

87
00:03:58,545 --> 00:03:59,785
I just co- collated,

88
00:03:59,785 --> 00:04:03,020
sort of all the free responses and tried to look for common themes.

89
00:04:03,020 --> 00:04:04,865
And anything that came up, you know,

90
00:04:04,865 --> 00:04:06,665
three or five times or more,

91
00:04:06,665 --> 00:04:09,995
I considered was a common issue that people would love addressed.

92
00:04:09,995 --> 00:04:13,850
Um, people would love even more focused on the big picture explaining,

93
00:04:13,850 --> 00:04:17,300
um, as well as connecting from the toy examples to real-world examples.

94
00:04:17,300 --> 00:04:19,055
So we'll try to do that where we can.

95
00:04:19,055 --> 00:04:22,850
Um, I'll also try to make sure that I'm speaking loudly throughout.

96
00:04:22,850 --> 00:04:25,865
Several people said that sometimes it was occasionally hard to hear,

97
00:04:25,865 --> 00:04:28,010
so I'll try to do a better job on that.

98
00:04:28,010 --> 00:04:30,575
If you can't hear me in the back please feel free to raise your hand.

99
00:04:30,575 --> 00:04:34,900
Um, and people would like even more examples, um, worked examples.

100
00:04:34,900 --> 00:04:36,070
And so in particular,

101
00:04:36,070 --> 00:04:38,540
we're going to try to make sure that in sessions,

102
00:04:38,540 --> 00:04:41,790
we emphasize worked examples even more.

103
00:04:44,240 --> 00:04:47,180
And let me just say again, if this was not one of

104
00:04:47,180 --> 00:04:49,250
the things that you were most concerned about,

105
00:04:49,250 --> 00:04:51,335
I'm sorry, we can't address all of them this term.

106
00:04:51,335 --> 00:04:54,890
Um, er, we definitely and it was kind of amusing to go through this,

107
00:04:54,890 --> 00:04:57,275
would have people saying exactly opposite things.

108
00:04:57,275 --> 00:04:58,670
Sometimes right in a row, um,

109
00:04:58,670 --> 00:05:01,190
in terms of how it was collated about things like,

110
00:05:01,190 --> 00:05:05,490
some people don't like the fact that the slides have gaps and I do derivations in class.

111
00:05:05,490 --> 00:05:09,410
And a number of other people really like that I do derivations in class.

112
00:05:09,410 --> 00:05:11,840
Um, some people felt like it was moving too slowly,

113
00:05:11,840 --> 00:05:13,830
other people said it was moving way too fast.

114
00:05:13,830 --> 00:05:17,180
Um, so again, we're just gonna try to do the best job we can to address

115
00:05:17,180 --> 00:05:21,130
everybody's needs. All right.

116
00:05:21,130 --> 00:05:23,390
So today we're going to continue talking about policy search,

117
00:05:23,390 --> 00:05:27,020
which as I said before, is probably the most important

118
00:05:27,020 --> 00:05:30,260
reinforcement learning thing you'll learn this term. [LAUGHTER]

119
00:05:30,260 --> 00:05:33,790
Um, I think this is used really very widely right now,

120
00:05:33,790 --> 00:05:36,035
um, in order to optimize functions.

121
00:05:36,035 --> 00:05:38,710
And we can again think of policy search here as

122
00:05:38,710 --> 00:05:41,120
a lot of things are gonna sound similar to

123
00:05:41,120 --> 00:05:43,490
when we were doing value function approximation.

124
00:05:43,490 --> 00:05:46,925
And what we're thinking about here is having a parameterized policy.

125
00:05:46,925 --> 00:05:49,670
Often we're going to use theta to parameterize It,

126
00:05:49,670 --> 00:05:51,380
but we could use W or anything else.

127
00:05:51,380 --> 00:05:53,750
But we have a policy that's parameterized.

128
00:05:53,750 --> 00:05:57,010
Um, and then we have some value of that policy.

129
00:05:57,010 --> 00:05:58,680
And what we're going to want to be doing,

130
00:05:58,680 --> 00:06:00,255
is trying to find, you know,

131
00:06:00,255 --> 00:06:03,600
a good optima, trying to maximize the value,

132
00:06:03,600 --> 00:06:05,900
um, of that policy.

133
00:06:05,900 --> 00:06:08,450
And one of the reasons why we did this right after

134
00:06:08,450 --> 00:06:10,950
imitation learning was to connect it with the idea of, um,

135
00:06:10,950 --> 00:06:13,220
you have to choose a policy class,

136
00:06:13,220 --> 00:06:16,040
a way to specify that parameterization.

137
00:06:16,040 --> 00:06:18,305
And so because of that inherently,

138
00:06:18,305 --> 00:06:20,440
it's a place to put in structure.

139
00:06:20,440 --> 00:06:22,975
Okay. So just as a recap,

140
00:06:22,975 --> 00:06:25,100
I mentioned that we've done a lot of work so far,

141
00:06:25,100 --> 00:06:26,870
on model-free value-based methods.

142
00:06:26,870 --> 00:06:30,425
We're starting to do work on direct policy search methods now.

143
00:06:30,425 --> 00:06:33,575
Um, and today we'll also start to talk more about Actor-Critic methods,

144
00:06:33,575 --> 00:06:35,045
where we maintain both.

145
00:06:35,045 --> 00:06:38,045
We maintain both an explicit parameterized policy

146
00:06:38,045 --> 00:06:41,180
and an explicit parameterized value function.

147
00:06:41,180 --> 00:06:43,520
And also throughout all of, you know,

148
00:06:43,520 --> 00:06:46,970
the last couple of weeks including yesterday or Monday and today,

149
00:06:46,970 --> 00:06:49,220
we're mostly going to be talking about cases where we want to

150
00:06:49,220 --> 00:06:52,470
be able to work in really really large state spaces.

151
00:06:53,030 --> 00:06:56,330
So I'm just gonna do a brief refresher of last time.

152
00:06:56,330 --> 00:06:57,450
Why do we want to do this?

153
00:06:57,450 --> 00:07:01,400
Well, we're going to generally be able to guarantee that we converge to a local optima.

154
00:07:01,400 --> 00:07:05,645
We don't always have those guarantees for value function based methods.

155
00:07:05,645 --> 00:07:08,270
Um, er, and that might be important.

156
00:07:08,270 --> 00:07:10,625
Er, it's a nice property to have.

157
00:07:10,625 --> 00:07:12,715
Um, the downside is that,

158
00:07:12,715 --> 00:07:14,510
if you use policy gradient methods,

159
00:07:14,510 --> 00:07:16,955
typically we only converge to a local optima.

160
00:07:16,955 --> 00:07:19,580
The last time I showed you the exoskeleton example,

161
00:07:19,580 --> 00:07:22,505
where they are using a global optima approach.

162
00:07:22,505 --> 00:07:26,120
So there are other ones which can policy based RL

163
00:07:26,120 --> 00:07:29,510
isn't inherently always going to get you to a local solution,

164
00:07:29,510 --> 00:07:31,775
but the gradient based methods typically will.

165
00:07:31,775 --> 00:07:34,610
And the other issue that we were talking about, uh,

166
00:07:34,610 --> 00:07:37,320
um, as ways to try to address this is, um,

167
00:07:37,320 --> 00:07:39,890
or- or as tools to address the fact that evaluating

168
00:07:39,890 --> 00:07:44,010
a policy itself might be rather inefficient, and high-variance.

169
00:07:45,290 --> 00:07:51,575
So what we are defining before is a policy gradient where, um, now, er,

170
00:07:51,575 --> 00:07:55,359
before we'd sort of thought about these things being parameterized by a theta,

171
00:07:55,359 --> 00:07:57,295
so we can either think of the value being, uh,

172
00:07:57,295 --> 00:08:01,870
the policy being parameterized by a theta or pi of theta means parameterized.

173
00:08:01,870 --> 00:08:05,365
But we're often going to talk about value functions because ultimately,

174
00:08:05,365 --> 00:08:09,950
the value function depends on the policy and the policy depends on the parameters.

175
00:08:11,640 --> 00:08:15,595
And when we think about what we want out of these algorithms,

176
00:08:15,595 --> 00:08:19,340
typically what we'd really like is to try to converge to a really good local optima.

177
00:08:19,340 --> 00:08:21,595
Often we don't have very much control over that.

178
00:08:21,595 --> 00:08:24,730
Um, but the things we often do have control over is things

179
00:08:24,730 --> 00:08:27,955
like how quickly we converge to that local optima.

180
00:08:27,955 --> 00:08:32,840
Um, and so we want to use sort of go as quickly as we can down that gradient,

181
00:08:32,840 --> 00:08:34,580
if we're doing a gradient-based method,

182
00:08:34,580 --> 00:08:37,980
um, and use our data as well as we can.

183
00:08:40,299 --> 00:08:44,239
So one of the things we're gonna talk more about today,

184
00:08:44,240 --> 00:08:47,975
is when we're doing this sort of policy gradient technique.

185
00:08:47,975 --> 00:08:50,870
So we're going to be sort of moving down.

186
00:08:50,870 --> 00:08:53,120
Now, we're gonna have our gradient.

187
00:08:53,120 --> 00:08:54,260
We're gonna have our functions.

188
00:08:54,260 --> 00:08:57,605
This is B pi, and this is our parameterized pi.

189
00:08:57,605 --> 00:09:03,784
And as we're moving down towards some local gradient,

190
00:09:03,784 --> 00:09:08,675
um, it would be nice if when we update our policy,

191
00:09:08,675 --> 00:09:11,405
that it is monotonically improving.

192
00:09:11,405 --> 00:09:17,440
So can anybody give me a reason why we might want monotonic improvement? Yeah.

193
00:09:17,440 --> 00:09:19,830
To help guarantee convergence.

194
00:09:19,830 --> 00:09:22,760
Answer is right. Can help guarantee convergence absolutely.

195
00:09:22,760 --> 00:09:24,875
And while I love math as much as many of you,

196
00:09:24,875 --> 00:09:26,240
um, that- that is a great reason.

197
00:09:26,240 --> 00:09:27,830
But perhaps, I was also thinking of like

198
00:09:27,830 --> 00:09:29,720
an empirical reason why we might want that as well.

199
00:09:29,720 --> 00:09:31,620
Yeah, in the back.

200
00:09:36,270 --> 00:09:39,130
[inaudible] like in a high-stakes situation.

201
00:09:39,130 --> 00:09:41,065
So what we've seen before in fact,

202
00:09:41,065 --> 00:09:43,060
um, one of my students, ah,

203
00:09:43,060 --> 00:09:45,730
[inaudible] was giving a practice job talk yesterday,

204
00:09:45,730 --> 00:09:48,490
and he was showing this graph for DQN,

205
00:09:48,490 --> 00:09:51,200
which looks something like this.

206
00:09:51,780 --> 00:09:54,280
Where this is, like, the performance,

207
00:09:54,280 --> 00:09:58,190
this is the reward, and this is time.

208
00:10:00,090 --> 00:10:02,440
Of course, it doesn't always look like this.

209
00:10:02,440 --> 00:10:05,080
And typically when you go to- um, when you read papers,

210
00:10:05,080 --> 00:10:06,430
people smooth over many,

211
00:10:06,430 --> 00:10:09,025
many runs, but often it looks something like that.

212
00:10:09,025 --> 00:10:12,670
That as you're going across multiple episodes or across multiple time steps, like,

213
00:10:12,670 --> 00:10:16,275
you're really getting a very jagged up and down performance of your-

214
00:10:16,275 --> 00:10:20,415
of your val- of the policy that you're running as you do DQN.

215
00:10:20,415 --> 00:10:22,470
So why might that not be good in,

216
00:10:22,470 --> 00:10:23,925
like, a high-stakes situation.

217
00:10:23,925 --> 00:10:26,680
Yeah, over there. And name first, please.

218
00:10:26,790 --> 00:10:31,450
If, like, it's something high-stakes and you have something good and then it goes down,

219
00:10:31,450 --> 00:10:34,900
people are going to be upset with you that now it's done something worse,

220
00:10:34,900 --> 00:10:36,955
even if it will later go back up.

221
00:10:36,955 --> 00:10:40,390
Yeah, what was said is that if the system is a high-stakes scenario,

222
00:10:40,390 --> 00:10:42,130
um, if you do- you know,

223
00:10:42,130 --> 00:10:44,140
if your policy works pretty well and then the next one,

224
00:10:44,140 --> 00:10:46,045
next episode, it works really badly,

225
00:10:46,045 --> 00:10:48,100
even if it might go up later, um, you know,

226
00:10:48,100 --> 00:10:50,410
your boss still might fire you [LAUGHTER].

227
00:10:50,410 --> 00:10:52,810
I mean, I'm joking, but I think that people are often

228
00:10:52,810 --> 00:10:55,375
loss averse and also it's often not tolerable,

229
00:10:55,375 --> 00:10:58,120
um, in- it might not be okay, you know,

230
00:10:58,120 --> 00:10:59,140
in a company to say, well,

231
00:10:59,140 --> 00:11:01,825
this quarter we did really well and next quarter we're going to do, you know,

232
00:11:01,825 --> 00:11:03,580
worse, but then eventually, you know,

233
00:11:03,580 --> 00:11:05,950
after many, ah, quarters we're going to do well.

234
00:11:05,950 --> 00:11:09,295
Like we often may wanna ensure that we're sort of monotonically going up.

235
00:11:09,295 --> 00:11:12,100
Um, and in the case of something like patient treatment or

236
00:11:12,100 --> 00:11:15,385
other sorts of high-stakes scenarios or airplanes or stuff like that,

237
00:11:15,385 --> 00:11:17,650
um, it just probably will not be tolerable to people,

238
00:11:17,650 --> 00:11:21,220
if you say we're- we're going to do much worse for this period of time.

239
00:11:21,220 --> 00:11:23,815
Ah, no- now there are exceptions to all of this,

240
00:11:23,815 --> 00:11:24,940
but I think there are many cases,

241
00:11:24,940 --> 00:11:26,710
where you'd really like monotonic improvement,

242
00:11:26,710 --> 00:11:27,880
i- if you can.

243
00:11:27,880 --> 00:11:30,760
Ah, so I think it's a really- in addition to the theoretical,

244
00:11:30,760 --> 00:11:32,920
ah, benefits, ah, it can help us prove things.

245
00:11:32,920 --> 00:11:35,290
Ah, it also can just be,

246
00:11:35,290 --> 00:11:38,740
um, something that's sort of appealing for people to be actually be able to deploy.

247
00:11:38,740 --> 00:11:42,895
And we know that in general that people are very risk- like, very loss averse.

248
00:11:42,895 --> 00:11:46,210
So having policies that are monotonically improving, um,

249
00:11:46,210 --> 00:11:50,905
can be very nice and DQN and a lot of the value based methods do not have those guarantees.

250
00:11:50,905 --> 00:11:54,985
Um, we can talk more also about whether that's always possible,

251
00:11:54,985 --> 00:11:59,420
um, in terms of if you wanted to get to a global optima. Yes.

252
00:11:59,420 --> 00:12:02,440
Just to be clear, the monotonic improvement in these cases

253
00:12:02,440 --> 00:12:05,140
are data that you have access to or have seen, right?

254
00:12:05,140 --> 00:12:08,200
So technically like if there's a distribution in terms of

255
00:12:08,200 --> 00:12:14,005
your life environment where it may differ somewhat from your actual simulation or,

256
00:12:14,005 --> 00:12:16,690
ah, environment, you may not necessarily quantify or improve it,

257
00:12:16,690 --> 00:12:18,980
given all that secret data. Is that right?

258
00:12:18,980 --> 00:12:21,135
Yeah, which is to say,

259
00:12:21,135 --> 00:12:22,230
when we're gonna be- what's- you know,

260
00:12:22,230 --> 00:12:23,790
what is this monotonic improvement?

261
00:12:23,790 --> 00:12:26,775
What are the conditions under which this will be guaranteed or possible?

262
00:12:26,775 --> 00:12:29,610
And are we sort of doing this based on our previous data and

263
00:12:29,610 --> 00:12:32,720
making some assumptions about the future da- future data that's collected.

264
00:12:32,720 --> 00:12:34,630
Absolutely. We're gonna assume that we're still on

265
00:12:34,630 --> 00:12:37,105
the same decision process and that it's stationary.

266
00:12:37,105 --> 00:12:39,100
And what I mean by that is that the transition model and

267
00:12:39,100 --> 00:12:42,250
the reward model is the same across- you know,

268
00:12:42,250 --> 00:12:43,630
you might not have observed all the states yet,

269
00:12:43,630 --> 00:12:45,325
but it's the same across episodes.

270
00:12:45,325 --> 00:12:48,190
So we're not dealing with the fact that, you know, um,

271
00:12:48,190 --> 00:12:50,785
customer preferences have totally changed or,

272
00:12:50,785 --> 00:12:53,995
um, you know, climate change is changing your environment.

273
00:12:53,995 --> 00:12:57,130
We are dealing with the fact that if the world is stationary,

274
00:12:57,130 --> 00:13:00,145
that then we're going to be guaranteed to have monotonic improvement.

275
00:13:00,145 --> 00:13:04,285
Now the other thing that I'm going to show you that in some cases we can guarantee that.

276
00:13:04,285 --> 00:13:06,520
Um, the other really important thing to know is,

277
00:13:06,520 --> 00:13:11,485
this is going to- we're going to hope to show monotonic improvement in expectation.

278
00:13:11,485 --> 00:13:13,975
So- so the value function has expected reward.

279
00:13:13,975 --> 00:13:15,940
So what we're going to be able to hope to say is

280
00:13:15,940 --> 00:13:18,160
the series of policies that we're deploying in

281
00:13:18,160 --> 00:13:22,780
the environment that their value function is going up. So what does that mean?

282
00:13:22,780 --> 00:13:26,800
That means that V_Pi1 we would like that to be less than or equal

283
00:13:26,800 --> 00:13:31,075
to V_Pi2 less than or equal to V_Pi3 dot-dot-dot,

284
00:13:31,075 --> 00:13:32,980
where this is, sort of, um,

285
00:13:32,980 --> 00:13:41,270
the policy we deploy on each iteration or each round.

286
00:13:45,240 --> 00:13:51,625
Ah, but it doesn't guarantee that for a single run this policy is better.

287
00:13:51,625 --> 00:13:56,695
So you could easily have it that it on average you're deploying a policy that is better,

288
00:13:56,695 --> 00:13:58,930
you know, for your airplanes or for patient treatment,

289
00:13:58,930 --> 00:14:01,690
et cetera, but for individual patients that might be worse.

290
00:14:01,690 --> 00:14:06,190
A- and I think a really interesting active area of research right now is,

291
00:14:06,190 --> 00:14:08,185
um, safe reinforcement learning,

292
00:14:08,185 --> 00:14:09,925
um, and safe exploration.

293
00:14:09,925 --> 00:14:11,905
And a lot of different people are thinking about this,

294
00:14:11,905 --> 00:14:13,840
um, including a number of people here at Stanford.

295
00:14:13,840 --> 00:14:16,030
And one of the things that we're looking at in our group is,

296
00:14:16,030 --> 00:14:18,820
how do you really efficiently get to a safe solution?

297
00:14:18,820 --> 00:14:20,560
What do you mean by safe in this case?

298
00:14:20,560 --> 00:14:23,440
I mean you might not want to max- maximize expected reward.

299
00:14:23,440 --> 00:14:25,210
You might want to be able to maximize,

300
00:14:25,210 --> 00:14:27,340
um, some sort of risk averse criteria.

301
00:14:27,340 --> 00:14:30,715
And we'd like to figure out ways to really efficiently get to that solution.

302
00:14:30,715 --> 00:14:32,815
But there's lots of really interesting stuff that says,

303
00:14:32,815 --> 00:14:35,110
you know, how do we try to do policy search?

304
00:14:35,110 --> 00:14:36,910
Or how do we do this improvement in cases

305
00:14:36,910 --> 00:14:39,280
where we don't just care about expected outcomes?

306
00:14:39,280 --> 00:14:44,580
All right. So what we're gonna be trying to do today is move towards,

307
00:14:44,580 --> 00:14:46,770
sort of, ideally, not just monotonic improvements,

308
00:14:46,770 --> 00:14:48,615
but large monotonic improvements.

309
00:14:48,615 --> 00:14:49,995
Um, as you might guess,

310
00:14:49,995 --> 00:14:51,870
it is easier to try to achieve

311
00:14:51,870 --> 00:14:54,180
small monotonic improvements than it would be to

312
00:14:54,180 --> 00:14:56,970
guarantee really large monotonic improvements.

313
00:14:56,970 --> 00:15:00,730
Um, does anybody have any intuition for why that would be true?

314
00:15:02,290 --> 00:15:04,600
That might be harder.

315
00:15:04,600 --> 00:15:08,440
Um, this kind of goes back to the state distributions.

316
00:15:08,440 --> 00:15:11,815
So if you change your policy a lot,

317
00:15:11,815 --> 00:15:14,290
um, does the state- can the state

318
00:15:14,290 --> 00:15:17,840
distribution change a lot in terms of the states you visit?

319
00:15:19,110 --> 00:15:22,630
So- so intuitively that should- the answer should be yes.

320
00:15:22,630 --> 00:15:26,275
So we've talked some about how any policy induces, um,

321
00:15:26,275 --> 00:15:28,690
a state distribution, like if you run it for a long time

322
00:15:28,690 --> 00:15:31,510
you're going to have sort of a stationary distribution over states.

323
00:15:31,510 --> 00:15:35,605
Um, and if your policy is really different than your old policy,

324
00:15:35,605 --> 00:15:38,200
then that state distribution might look really different,

325
00:15:38,200 --> 00:15:40,225
which means you might not have very much data.

326
00:15:40,225 --> 00:15:44,815
Um, whereas if you have almost exactly the same policy as before,

327
00:15:44,815 --> 00:15:46,360
um, you're probably going to be able to have

328
00:15:46,360 --> 00:15:49,195
a really good notion of what that value is in the estimate.

329
00:15:49,195 --> 00:15:50,785
But we'll get more into that later.

330
00:15:50,785 --> 00:15:52,900
So what we're going do today is to try to think

331
00:15:52,900 --> 00:15:54,880
about moving beyond what we were talking about last time,

332
00:15:54,880 --> 00:15:56,980
where we're trying to do policy gradient methods.

333
00:15:56,980 --> 00:15:59,785
And we're trying to do it in a way that was sort of efficient.

334
00:15:59,785 --> 00:16:02,845
We're going to talk about other ways to make it more efficient,

335
00:16:02,845 --> 00:16:07,190
ah, and less noisy, and then try to go towards monotonic improvement.

336
00:16:08,160 --> 00:16:11,155
All right. So that things that we talked about last time,

337
00:16:11,155 --> 00:16:14,320
is we started off when we said what can we do in terms of policy gradient?

338
00:16:14,320 --> 00:16:16,210
One thing we could do is,

339
00:16:16,210 --> 00:16:18,595
kind of, you do Monte Carlo returns.

340
00:16:18,595 --> 00:16:21,320
So these are Monte Carlo returns.

341
00:16:22,590 --> 00:16:26,080
And sometimes people use big R of Tau,

342
00:16:26,080 --> 00:16:28,640
where Tau, this is a trajectory.

343
00:16:29,430 --> 00:16:33,370
So you could just run out your policy till

344
00:16:33,370 --> 00:16:37,090
the world terminates or each steps or however you're defining your episodes,

345
00:16:37,090 --> 00:16:40,525
um, and then look at the reward and you'll look at their reward across,

346
00:16:40,525 --> 00:16:41,830
you know, per time step.

347
00:16:41,830 --> 00:16:45,260
We can also use, um,

348
00:16:45,450 --> 00:16:51,520
G_i_t to denote the reward we get from time-step t and- and in Episode i.

349
00:16:51,520 --> 00:16:54,340
And what we've said before is that, um,

350
00:16:54,340 --> 00:16:57,505
this is an unbiased estimate of the gradient, but it's really noisy.

351
00:16:57,505 --> 00:16:59,980
And so we started talking about additional structure we

352
00:16:59,980 --> 00:17:02,425
could use in the reinforcement learning problem,

353
00:17:02,425 --> 00:17:04,359
where we were assuming the world is Markov,

354
00:17:04,359 --> 00:17:07,644
um, to try to reduce the variance of this estimate.

355
00:17:07,645 --> 00:17:11,740
And so what we talked about last time was using temporal structure,

356
00:17:11,740 --> 00:17:13,930
which we did some on the board.

357
00:17:13,930 --> 00:17:17,935
And the intuition there was the reward you get at some time point,

358
00:17:17,935 --> 00:17:22,944
um, uh, can be- is not influenced by the later decisions you make.

359
00:17:22,944 --> 00:17:24,969
So you didn't have to take kind of

360
00:17:24,970 --> 00:17:28,735
this complete product of the probability of action given the states,

361
00:17:28,735 --> 00:17:31,900
because future actions don't retroactively change

362
00:17:31,900 --> 00:17:35,080
our earlier rewards for intuition.

363
00:17:35,080 --> 00:17:37,690
So what we're going to start talking about now is, um, other things,

364
00:17:37,690 --> 00:17:44,020
which is baselines and alternatives to Monte Carlo. Okay.

365
00:17:44,020 --> 00:17:45,805
So what's the baseline?

366
00:17:45,805 --> 00:17:48,505
Well, a baseline is as let's still think about looking at,

367
00:17:48,505 --> 00:17:51,295
um, the sum of rewards we get from this time-step onwards.

368
00:17:51,295 --> 00:17:56,050
So this is the same thing that we've often called GT.

369
00:17:56,050 --> 00:18:00,250
So this is the reward we get from this time-step till the end of the episode.

370
00:18:00,250 --> 00:18:04,825
And we're gonna subtract a baseline that depends on the state.

371
00:18:04,825 --> 00:18:08,200
And what I'm going to show shortly is that by

372
00:18:08,200 --> 00:18:11,110
subtracting this baseline which depends only on the state,

373
00:18:11,110 --> 00:18:14,125
your resulting gradient estimator can still be unbiased,

374
00:18:14,125 --> 00:18:16,525
but it might not, have much lower variance.

375
00:18:16,525 --> 00:18:19,960
And in particular often a really good choice is the expected return,

376
00:18:19,960 --> 00:18:22,330
which is basically the value function.

377
00:18:22,330 --> 00:18:24,760
And so why would we do this?

378
00:18:24,760 --> 00:18:28,795
We'll then we can kind of look at we- increasing the log probability of an action,

379
00:18:28,795 --> 00:18:32,870
proportional to how much better it is than a baseline.

380
00:18:33,300 --> 00:18:35,860
Which in general is going to end up being sort of a

381
00:18:35,860 --> 00:18:38,140
little bit like an advantage function.

382
00:18:38,140 --> 00:18:40,495
So why is this true?

383
00:18:40,495 --> 00:18:44,080
Okay. So, um, what are we going to try to do here?

384
00:18:44,080 --> 00:18:47,455
We're gonna say we have this high-variance estimate right now.

385
00:18:47,455 --> 00:18:50,995
If we don't have, so imagine we didn't have this, we don't have that.

386
00:18:50,995 --> 00:18:53,905
We have our, the standard estimate we were talking about last time.

387
00:18:53,905 --> 00:18:56,500
And what I want to convince you is that if we

388
00:18:56,500 --> 00:18:59,440
subtract off this thing which is a function of the state,

389
00:18:59,440 --> 00:19:03,805
that an expectation that additional term we're subtracting off is zero.

390
00:19:03,805 --> 00:19:07,480
Meaning that our estimator is still unbiased.

391
00:19:07,480 --> 00:19:10,165
So I said our original estimator is unbiased.

392
00:19:10,165 --> 00:19:11,770
We're subtracting off this weird thing,

393
00:19:11,770 --> 00:19:14,620
we want to show that the resulting estimator is still unbiased.

394
00:19:14,620 --> 00:19:17,155
And the way we do that is by showing there's,

395
00:19:17,155 --> 00:19:18,580
the goal is to show,

396
00:19:18,580 --> 00:19:25,160
[NOISE] that this is equal to zero.

397
00:19:25,470 --> 00:19:27,625
So that's what we're gonna try and do,

398
00:19:27,625 --> 00:19:32,080
and if we can show that then that's gonna justify why we can subtract this random thing.

399
00:19:32,080 --> 00:19:34,510
And then we can start to talk about what that random thing should be.

400
00:19:34,510 --> 00:19:36,880
But first, we're just going to show no matter what that random thing is.

401
00:19:36,880 --> 00:19:38,605
If it's a function only of the state,

402
00:19:38,605 --> 00:19:40,945
that this, um, expectation is 0.

403
00:19:40,945 --> 00:19:43,090
So how do we do this?

404
00:19:43,090 --> 00:19:47,125
Well, first of all noted on the outside there's an expectation over tau.

405
00:19:47,125 --> 00:19:51,700
That is all the trajectories we might encounter by running our current policy.

406
00:19:51,700 --> 00:19:55,870
Okay. So what I'm gonna do first is I'm just going to split it into two parts.

407
00:19:55,870 --> 00:19:58,180
So this is still tau.

408
00:19:58,180 --> 00:20:00,640
And all I've done here is I've split it into

409
00:20:00,640 --> 00:20:04,180
the first part which is all the way up to time step t,

410
00:20:04,180 --> 00:20:06,640
and the second part which is on time step,

411
00:20:06,640 --> 00:20:08,950
um, t, all the way to the end.

412
00:20:08,950 --> 00:20:11,515
So I've just sort of decomposed, that,

413
00:20:11,515 --> 00:20:13,180
I-I I've just written out what, um, ah,

414
00:20:13,180 --> 00:20:15,910
a trajectory is, and then decomposed into two parts.

415
00:20:15,910 --> 00:20:17,650
So I'm just decomposing the trajectory.

416
00:20:17,650 --> 00:20:22,585
[NOISE] And once we do that,

417
00:20:22,585 --> 00:20:26,680
then we can see that the baseline term is only a function of S_t.

418
00:20:26,680 --> 00:20:30,410
[NOISE] So we can pull it out of this inner term.

419
00:20:33,660 --> 00:20:37,240
Right. So we're gonna pull this out because it doesn't depend on

420
00:20:37,240 --> 00:20:40,435
any of these future time steps.

421
00:20:40,435 --> 00:20:42,280
It's independent of those.

422
00:20:42,280 --> 00:20:52,840
[BACKGROUND]

423
00:20:52,840 --> 00:20:53,635
Okay.

424
00:20:53,635 --> 00:20:57,985
Then the next thing we're gonna do is we're going to,

425
00:20:57,985 --> 00:21:02,650
uh, write out or note the fact that in this case.

426
00:21:02,650 --> 00:21:07,885
That all we have in this inner term here is S_t and a_t.

427
00:21:07,885 --> 00:21:11,380
So we can drop all the future terms.

428
00:21:11,380 --> 00:21:14,020
Again that's sort of the prob- the the only thing in

429
00:21:14,020 --> 00:21:16,675
here is the probability of the action a_time-step t,

430
00:21:16,675 --> 00:21:18,715
given the state t and theta.

431
00:21:18,715 --> 00:21:22,630
So we don't need to worry about the future states or the future actions that are taken,

432
00:21:22,630 --> 00:21:24,865
um, we- we're independent of those.

433
00:21:24,865 --> 00:21:26,620
So now, so we just pull it up,

434
00:21:26,620 --> 00:21:28,880
first we pulled out baseline.

435
00:21:29,820 --> 00:21:34,480
And now, we're going to drop those things that we don't need to depend on.

436
00:21:34,480 --> 00:21:38,170
[NOISE]

437
00:21:38,170 --> 00:21:42,440
So all we have here is an expectation over the action that's taken.

438
00:21:46,980 --> 00:21:51,670
Okay. And now what I'm gonna do is I'm going to read it,

439
00:21:51,670 --> 00:21:52,900
so what is this expectation?

440
00:21:52,900 --> 00:21:54,820
It's an expectation over a_t.

441
00:21:54,820 --> 00:21:57,310
What is the problem, you know, what is that expectation?

442
00:21:57,310 --> 00:21:58,915
We're just going to write that out explicitly,

443
00:21:58,915 --> 00:22:01,970
that depends on the policy that we have.

444
00:22:05,700 --> 00:22:09,100
So we're going to sum over a_t,

445
00:22:09,100 --> 00:22:11,320
the probability of that a_t,

446
00:22:11,320 --> 00:22:14,815
which of course just depends on the policy that we're following,

447
00:22:14,815 --> 00:22:18,050
times the derivative of log.

448
00:22:22,290 --> 00:22:25,645
So that is me writing out the expectation,

449
00:22:25,645 --> 00:22:27,730
and I'm gonna take the derivative of the log.

450
00:22:27,730 --> 00:22:45,610
[NOISE]

451
00:22:45,610 --> 00:22:50,210
So that's just gonna be the derivative with respect to the policy itself.

452
00:22:51,890 --> 00:22:58,575
Divided by pi of a_t, s t theta.

453
00:22:58,575 --> 00:23:03,765
Okay. But now we note that there's a term on the numerator and a term on the,

454
00:23:03,765 --> 00:23:05,925
denominator that we can cancel.

455
00:23:05,925 --> 00:23:15,810
So this starts to simplify [NOISE] b of S_t times the sum over a_t,

456
00:23:15,810 --> 00:23:19,120
just the derivative of the policy.

457
00:23:21,450 --> 00:23:24,700
Just canceled numerator and denominator there.

458
00:23:24,700 --> 00:23:30,620
And now we note that we can reverse the sum and the derivative.

459
00:23:33,510 --> 00:23:38,240
This is the others, that kind of critical step of this proof.

460
00:23:38,730 --> 00:23:42,280
So now what we're gonna do here is we're going to move the derivative out.

461
00:23:42,280 --> 00:23:51,415
[NOISE] Well, this is just 1,

462
00:23:51,415 --> 00:23:54,595
because the probability that we select some action,

463
00:23:54,595 --> 00:23:57,025
under our policy always has to be 1.

464
00:23:57,025 --> 00:24:01,400
And so now we see that we're just taking the derivative of 1.

465
00:24:05,790 --> 00:24:08,665
So we are trying to take the derivative of 1,

466
00:24:08,665 --> 00:24:11,080
and of course that's a constant so this is equal to 0.

467
00:24:11,080 --> 00:24:14,860
[NOISE] So that's pretty cool.

468
00:24:14,860 --> 00:24:17,530
So that means that we have added in this baseline.

469
00:24:17,530 --> 00:24:20,305
That is some function that depends on the state,

470
00:24:20,305 --> 00:24:21,730
and we haven't said we told me, you know,

471
00:24:21,730 --> 00:24:23,680
we haven't talked about all the different ways we

472
00:24:23,680 --> 00:24:26,125
could compute that gap or we say it doesn't matter what it is.

473
00:24:26,125 --> 00:24:31,120
No matter what you added there it's always unbiased.

474
00:24:31,120 --> 00:24:36,940
So just to check our understanding for a second if we go back to this equation,

475
00:24:36,940 --> 00:24:40,195
if I set b of S_t,

476
00:24:40,195 --> 00:24:43,450
to be a constant everywhere,

477
00:24:43,450 --> 00:24:46,060
is the gradient estimator still unbiased.

478
00:24:46,060 --> 00:24:54,700
[NOISE]

479
00:24:54,700 --> 00:24:55,705
Just take like one minute and talk to your neighbor,

480
00:24:55,705 --> 00:24:56,860
and say,

481
00:24:56,860 --> 00:25:02,095
so based on what I just said if b of S_t is equal to a constant,

482
00:25:02,095 --> 00:25:04,255
this is like a constant,

483
00:25:04,255 --> 00:25:15,339
for all S_t is the gradient estimator unbiased,

484
00:25:15,339 --> 00:25:18,400
just take like one second or one minute and talk to your neighbor.

485
00:25:18,400 --> 00:26:06,735
[BACKGROUND]

486
00:26:06,735 --> 00:26:09,195
All right. So let's start with,

487
00:26:09,195 --> 00:26:11,370
um, everybody here thinks it's still unbiased, vote?

488
00:26:11,370 --> 00:26:13,500
Yes. Great. Okay, yes.

489
00:26:13,500 --> 00:26:14,520
It has to still be unbiased,

490
00:26:14,520 --> 00:26:17,175
now it's a function not even of s, just a constant.

491
00:26:17,175 --> 00:26:19,410
And so it's definitely a function of an s,

492
00:26:19,410 --> 00:26:22,770
it's a tri- trivial function where it doesn't matter what the value of s is,

493
00:26:22,770 --> 00:26:24,790
and so it's still unbiased.

494
00:26:24,890 --> 00:26:27,780
Just to note here, um,

495
00:26:27,780 --> 00:26:29,565
if s was a fu- if, um,

496
00:26:29,565 --> 00:26:32,190
the baseline was a function of state and action,

497
00:26:32,190 --> 00:26:33,945
do you think this proof would go through?

498
00:26:33,945 --> 00:26:34,740
No.

499
00:26:34,740 --> 00:26:35,130
No.

500
00:26:35,130 --> 00:26:37,350
No. Right. Because one of the things that we did at the very beginning is

501
00:26:37,350 --> 00:26:39,930
we moved b of S_t all the way out.

502
00:26:39,930 --> 00:26:43,125
And if it depended on the action too, we couldn't have done that.

503
00:26:43,125 --> 00:26:48,750
So this is specific to this being only a function of the state. Yes.

504
00:26:48,750 --> 00:26:54,390
[inaudible] functions b that do not give you an un-

505
00:26:54,390 --> 00:26:56,745
an unbiased estimate state.

506
00:26:56,745 --> 00:26:58,875
So I don't know, is there any functions of b?

507
00:26:58,875 --> 00:27:01,845
b is only a function of the state, all of them are unbiased.

508
00:27:01,845 --> 00:27:03,525
Yeah, so it is always unbiased.

509
00:27:03,525 --> 00:27:04,890
There could be really- I mean,

510
00:27:04,890 --> 00:27:06,420
just like what I put here,

511
00:27:06,420 --> 00:27:10,770
um, you could just put in a constant and it might not reduce your variance at all.

512
00:27:10,770 --> 00:27:13,605
So there's certainly unuseful definitions of a baseline,

513
00:27:13,605 --> 00:27:15,210
um, but all of them are unbiased.

514
00:27:15,210 --> 00:27:17,115
So they're not gonna affect, ah,

515
00:27:17,115 --> 00:27:19,490
whether or not your estimator is unbiased.

516
00:27:19,490 --> 00:27:22,385
They could make your estimator potentially worse if they're really bad,

517
00:27:22,385 --> 00:27:24,680
um, or [NOISE] they're really themselves, um,

518
00:27:24,680 --> 00:27:26,720
very bad estimators potentially, um,

519
00:27:26,720 --> 00:27:31,960
and they certainly couldn't make it better [NOISE] by choosing good choices.

520
00:27:31,960 --> 00:27:34,830
Okay. All right.

521
00:27:34,830 --> 00:27:37,680
So this ends up allowing us

522
00:27:37,680 --> 00:27:40,785
to define what's sort of known as the vanilla policy gradient algorithm.

523
00:27:40,785 --> 00:27:44,505
Um, so vanilla policy gradient operates by,

524
00:27:44,505 --> 00:27:47,370
we collect a bunch of trajectories using our current policy.

525
00:27:47,370 --> 00:27:50,070
And then for each time step inside of the trajectory we

526
00:27:50,070 --> 00:27:53,070
compute the return from that time step to the end.

527
00:27:53,070 --> 00:27:56,175
Um, and then we compute the advantage estimate.

528
00:27:56,175 --> 00:28:00,270
So, all right, we'll write out vanilla policy gradient.

529
00:28:00,270 --> 00:28:07,440
[NOISE] Okay.

530
00:28:07,440 --> 00:28:10,185
So vanilla policy gradient works as follows.

531
00:28:10,185 --> 00:28:12,600
You start up, you initialize the policy with

532
00:28:12,600 --> 00:28:16,995
some parameter theta and you need to start with some estimate of the baseline.

533
00:28:16,995 --> 00:28:19,770
Okay. So what happens with vanilla policy gradient,

534
00:28:19,770 --> 00:28:22,650
is for iterations i = 1,

535
00:28:22,650 --> 00:28:24,075
2 dot, dot, dot.

536
00:28:24,075 --> 00:28:34,120
We're gonna collect a set of trajectories using your current policy.

537
00:28:35,900 --> 00:28:38,910
And then for each time step,

538
00:28:38,910 --> 00:28:42,000
for t = 1 dot,

539
00:28:42,000 --> 00:28:45,195
dot, dot, the length of your trajectory i,

540
00:28:45,195 --> 00:28:47,160
then you do two things.

541
00:28:47,160 --> 00:28:49,350
You compute the return,

542
00:28:49,350 --> 00:28:56,740
which is just equal to the sum of all the returns till the end of the episode.

543
00:28:57,350 --> 00:29:03,270
And then you compute the advantage A-hat_i_t,

544
00:29:03,270 --> 00:29:05,835
which is just equal to this return.

545
00:29:05,835 --> 00:29:09,660
I'll parameterize it with i just to show that that's the i'th trajectory.

546
00:29:09,660 --> 00:29:13,600
Um, [NOISE] - b of S_t.

547
00:29:14,510 --> 00:29:17,790
So just to note here for a second,

548
00:29:17,790 --> 00:29:23,055
um, this is a return of the sum of rewards till the end of the episode.

549
00:29:23,055 --> 00:29:26,670
This is a baseline which is like a fixed function.

550
00:29:26,670 --> 00:29:29,280
Um, so this could be, you know,

551
00:29:29,280 --> 00:29:30,435
a deep neural network,

552
00:29:30,435 --> 00:29:31,950
this could be a table lookup.

553
00:29:31,950 --> 00:29:34,530
Ah, but this is a function and you input

554
00:29:34,530 --> 00:29:39,330
the state at time step t and trajectory i [NOISE] and you output a scalar.

555
00:29:39,330 --> 00:29:41,775
So that's what the baseline is doing there.

556
00:29:41,775 --> 00:29:44,820
And then wha- um, in vanilla policy gradient we do is,

557
00:29:44,820 --> 00:29:46,965
then we refit the baseline.

558
00:29:46,965 --> 00:29:50,400
So in this case,

559
00:29:50,400 --> 00:29:54,570
the baseline is gonna be an estimate of the average of the Gs [NOISE].

560
00:29:54,570 --> 00:29:56,475
So in vanilla policy base,

561
00:29:56,475 --> 00:29:58,515
bu- um, vanilla policy gradient.

562
00:29:58,515 --> 00:30:01,530
What we do is the next step is,

563
00:30:01,530 --> 00:30:04,590
we sum over all the trajectories we've got so far.

564
00:30:04,590 --> 00:30:06,585
We just sum over all the time steps.

565
00:30:06,585 --> 00:30:10,140
Um, we do basically, just a least squares fit.

566
00:30:10,140 --> 00:30:16,230
[NOISE]

567
00:30:16,230 --> 00:30:18,810
So note this can be done with like su- this is supervised learning.

568
00:30:18,810 --> 00:30:24,030
We just have some a baseline function that can be parameterized [NOISE].

569
00:30:24,030 --> 00:30:25,770
I'll make sure to put an i there.

570
00:30:25,770 --> 00:30:27,510
So the baseline function that can be

571
00:30:27,510 --> 00:30:30,630
parameterized with some totally other weights or parameters,

572
00:30:30,630 --> 00:30:33,030
um, and then we have our returns g that we've

573
00:30:33,030 --> 00:30:35,715
seen so far and we just try to minimize that distance.

574
00:30:35,715 --> 00:30:37,845
And so then the baseline is really, er,

575
00:30:37,845 --> 00:30:40,620
representing the expected sum of rewards.

576
00:30:40,620 --> 00:30:44,130
Um, note that this is in some ways a little bit funny, right?

577
00:30:44,130 --> 00:30:47,715
Because we're using all of our data that we've ever seen.

578
00:30:47,715 --> 00:30:50,835
So this can either be done over, um,

579
00:30:50,835 --> 00:30:52,170
all the data you you've ever,

580
00:30:52,170 --> 00:30:55,140
ever seen or it can be done over just the most recent round.

581
00:30:55,140 --> 00:30:57,645
There's lots of choices for how to do the baseline.

582
00:30:57,645 --> 00:31:01,800
Um, I- if you use all the data you've ever,

583
00:31:01,800 --> 00:31:05,145
ever seen, um, then which is what this would do.

584
00:31:05,145 --> 00:31:10,110
Um, then you could be averaging over lots of different policies,

585
00:31:10,110 --> 00:31:12,375
because you've got data from different policies.

586
00:31:12,375 --> 00:31:15,015
If you just do this over the most recent round,

587
00:31:15,015 --> 00:31:20,085
then you're just gonna be getting an estimate of essentially V_Pi i- V_Pi i,

588
00:31:20,085 --> 00:31:21,540
like the- the iteration.

589
00:31:21,540 --> 00:31:25,610
This is gonna end up approximating. All right.

590
00:31:25,610 --> 00:31:28,790
If you ju- are only doing it over, um,

591
00:31:28,790 --> 00:31:30,650
the trajectories, if you don't do this,

592
00:31:30,650 --> 00:31:33,870
but you sum it over the trajectories for this round.

593
00:31:34,160 --> 00:31:37,320
I guess the way I've written this is a little bit unclear.

594
00:31:37,320 --> 00:31:38,940
So let me see if I can make that a bit clearer.

595
00:31:38,940 --> 00:31:42,150
So let's say that we have a- a- a- we have d trajectories.

596
00:31:42,150 --> 00:31:46,680
So if we do it this way,

597
00:31:46,680 --> 00:31:48,570
then that's exactly equal to V_Pi i.

598
00:31:48,570 --> 00:31:50,460
So i now is the iteration,

599
00:31:50,460 --> 00:31:54,450
d is the trajectories we've gathered just on iteration i.

600
00:31:54,450 --> 00:31:57,165
So this is only averaging over, um,

601
00:31:57,165 --> 00:32:02,410
the policies for this particular- the- the trajectories for this particular policy.

602
00:32:04,520 --> 00:32:06,660
I said a lot a bit out of order.

603
00:32:06,660 --> 00:32:10,080
Does anybody have any questions about exactly what we're doing in this case?

604
00:32:10,080 --> 00:32:13,935
So normally, in this situation there's a number of series of rounds.

605
00:32:13,935 --> 00:32:19,695
This is for each- So we're gonna have a series of pi's, basically.

606
00:32:19,695 --> 00:32:21,885
And then for each policy,

607
00:32:21,885 --> 00:32:23,385
we're gonna have a set of trajectories.

608
00:32:23,385 --> 00:32:26,070
And for each trajectory we have a set of time steps.

609
00:32:26,070 --> 00:32:28,380
And what this is saying here is average

610
00:32:28,380 --> 00:32:31,410
over all the trajectories you have for the current policy,

611
00:32:31,410 --> 00:32:33,790
and fit the baseline to that.

612
00:32:34,190 --> 00:32:36,585
All right. And then once you have that,

613
00:32:36,585 --> 00:32:38,070
so this gives you the baseline.

614
00:32:38,070 --> 00:32:48,460
Um, and then we do update the policy using your gradient.

615
00:32:52,610 --> 00:32:58,530
And it's gonna be a sum of terms that include these derivatives with respect to

616
00:32:58,530 --> 00:33:05,055
the policy and your advantage function.

617
00:33:05,055 --> 00:33:10,350
Okay. So you're gonna take in this advantage function that you computed over here.

618
00:33:10,350 --> 00:33:13,380
And then you're gonna be multiplying it by what was

619
00:33:13,380 --> 00:33:17,220
the probability of the actions given the state and theta.

620
00:33:17,220 --> 00:33:19,455
The log derivative of that.

621
00:33:19,455 --> 00:33:23,565
And then we plug that,

622
00:33:23,565 --> 00:33:25,740
this gr- this estimate of the gradient,

623
00:33:25,740 --> 00:33:34,900
into something like stochastic gradient descent or ADAM or something else.

624
00:33:35,000 --> 00:33:37,980
So this has been vanilla policy gradient.

625
00:33:37,980 --> 00:33:45,565
[NOISE]

626
00:33:45,565 --> 00:33:48,010
And what we're going to see during the rest of today

627
00:33:48,010 --> 00:33:51,865
is just a number of different slight variants on this basic template.

628
00:33:51,865 --> 00:33:53,500
So I'll get to you in just a second,

629
00:33:53,500 --> 00:33:56,020
but I just want to emphasize that if you- if

630
00:33:56,020 --> 00:33:59,320
you- when you walk away from unders- like from what I'd like you to understand,

631
00:33:59,320 --> 00:34:01,630
from the- the main idea for policy gradient,

632
00:34:01,630 --> 00:34:03,835
is essentially what's on the board right now.

633
00:34:03,835 --> 00:34:06,460
Is that, what we're doing is we are running,

634
00:34:06,460 --> 00:34:07,825
we take one policy,

635
00:34:07,825 --> 00:34:09,475
we get a bunch of data from it,

636
00:34:09,475 --> 00:34:12,774
and then we have to fit something like an advantage,

637
00:34:12,774 --> 00:34:14,620
and there's going to be different ways to compute that.

638
00:34:14,620 --> 00:34:16,239
We could end up doing bootstrapping,

639
00:34:16,239 --> 00:34:18,024
to do some sort of TD estimate,

640
00:34:18,025 --> 00:34:20,139
or we can just directly use the returns.

641
00:34:20,139 --> 00:34:22,014
We often use a baseline,

642
00:34:22,014 --> 00:34:24,534
um, that we're fitting over time.

643
00:34:24,534 --> 00:34:27,564
And then we're going to update the policy,

644
00:34:27,565 --> 00:34:30,864
and have to choose some step size with respect to the gradient.

645
00:34:30,864 --> 00:34:32,934
So this is sort of the most important thing.

646
00:34:32,935 --> 00:34:36,610
Is to say, hey, there's this basic template for almost all policy gradient algorithms,

647
00:34:36,610 --> 00:34:39,295
I can choose different things to kind of plug in here,

648
00:34:39,295 --> 00:34:42,250
and I can choose different ways to take my step sizes.

649
00:34:42,250 --> 00:34:44,050
Um, and that's going to define

650
00:34:44,050 --> 00:34:47,545
a whole bunch of the different policy gradient algorithms that you see.

651
00:34:47,545 --> 00:34:53,320
So what function are we using to represent i so that we can take its gradient?

652
00:34:53,320 --> 00:34:55,540
Great question. So, um, is asking, you know,

653
00:34:55,540 --> 00:34:56,980
how- how do we represent,

654
00:34:56,980 --> 00:34:58,870
um, the policy so we can take its gradient.

655
00:34:58,870 --> 00:35:00,700
We have to be able to take this here.

656
00:35:00,700 --> 00:35:03,490
Um, we talked briefly about this last time, um,

657
00:35:03,490 --> 00:35:06,340
but it was also on the board near the end, Gaussians work,

658
00:35:06,340 --> 00:35:10,300
Softmaxes both are- are- are- are- both of those you can analytically take the derivative

659
00:35:10,300 --> 00:35:14,695
and often we use deep neural networks or shallow deep neural networks.

660
00:35:14,695 --> 00:35:18,265
Yes. I saw a question back there? And name first, please.

661
00:35:18,265 --> 00:35:21,250
Ah, I was wondering if there's any, ah,

662
00:35:21,250 --> 00:35:23,560
issue with like non-states- if we're

663
00:35:23,560 --> 00:35:26,140
getting like b of- the baseline with the neural networks,

664
00:35:26,140 --> 00:35:29,710
there's like non-stationary issues with that?

665
00:35:29,710 --> 00:35:31,000
Yeah, it's a great question.

666
00:35:31,000 --> 00:35:33,400
So, ah, the question I believe is to say,

667
00:35:33,400 --> 00:35:35,530
um, you're asking me about the baseline, right?

668
00:35:35,530 --> 00:35:38,515
So like how- are there non-stationary issues with that?

669
00:35:38,515 --> 00:35:41,395
Empirically what a lot of people, including myself,

670
00:35:41,395 --> 00:35:43,840
have wondered is, um,

671
00:35:43,840 --> 00:35:45,925
we have all this other data.

672
00:35:45,925 --> 00:35:47,890
So when we're estimating the gradient right now,

673
00:35:47,890 --> 00:35:49,990
typically we're running the policy, just you know,

674
00:35:49,990 --> 00:35:53,125
for D trajectories and then we're estimating a gradient with that.

675
00:35:53,125 --> 00:35:56,710
Um, and could we maybe use other data to do that,

676
00:35:56,710 --> 00:35:58,180
but then ends up being off policy,

677
00:35:58,180 --> 00:36:01,195
because then you're mixing together data you've gathered from different policies.

678
00:36:01,195 --> 00:36:06,535
Empirically, I think people often end up using only the data from the current run,

679
00:36:06,535 --> 00:36:08,920
and then you're essentially just estimating V_Pi,

680
00:36:08,920 --> 00:36:13,315
with this and you're not necessarily mixing data for many other policies.

681
00:36:13,315 --> 00:36:15,400
Empirically, it seems like often,

682
00:36:15,400 --> 00:36:17,365
it's really helpful to be on policy.

683
00:36:17,365 --> 00:36:19,570
A- and you could reweight the old data,

684
00:36:19,570 --> 00:36:21,550
ah, but that introduces variance.

685
00:36:21,550 --> 00:36:24,550
And so empirically often, it's best.

686
00:36:24,550 --> 00:36:25,795
I think the jury is still out on it.

687
00:36:25,795 --> 00:36:27,175
There's ongoing research on it.

688
00:36:27,175 --> 00:36:29,830
We've looked at it, Sergey Levine's group has looked at it,

689
00:36:29,830 --> 00:36:33,565
but most of the time using the on-policy data makes sense.

690
00:36:33,565 --> 00:36:35,890
Yeah, is there another question? And name first, please.

691
00:36:35,890 --> 00:36:38,950
I just want to confirm, so when you saying refit baseline,

692
00:36:38,950 --> 00:36:42,790
we're setting baseline equal to the value that minimizes the function error.

693
00:36:42,790 --> 00:36:44,500
[NOISE]

694
00:36:44,500 --> 00:36:47,440
Perfect. Yeah, for error, if we do this if we're

695
00:36:47,440 --> 00:36:50,380
only averaging over the data points that we have for this current policy,

696
00:36:50,380 --> 00:36:53,050
when we do this, it's essentially- it- it's

697
00:36:53,050 --> 00:36:56,470
essentially the same as when we were doing Monte Carlo policy evaluation.

698
00:36:56,470 --> 00:37:00,655
So this is almost exactly like Monte Carlo policy eval.

699
00:37:00,655 --> 00:37:06,115
Where we have a fixed policy and then we have a parameterized function to represent it.

700
00:37:06,115 --> 00:37:08,950
Um, and then we just want to fit those parameters so we can best

701
00:37:08,950 --> 00:37:13,790
estimate the policy value using Monte Carlo.

702
00:37:16,590 --> 00:37:19,330
Okay. So I'm going to- there's a little bit of

703
00:37:19,330 --> 00:37:21,790
information about auto diff you can check it in the slides.

704
00:37:21,790 --> 00:37:25,855
Um, uh, the things we're going to go through next, um,

705
00:37:25,855 --> 00:37:29,980
is thinking about this aspect as I was saying,

706
00:37:29,980 --> 00:37:32,605
and then we'll talk some about this.

707
00:37:32,605 --> 00:37:39,280
So this part is going to be where we think about monotonic improvement.

708
00:37:39,280 --> 00:37:41,110
Because once we have a gradient,

709
00:37:41,110 --> 00:37:42,595
we have to figure out how far to go.

710
00:37:42,595 --> 00:37:44,545
An- and can we guarantee, um,

711
00:37:44,545 --> 00:37:46,060
depending on how far we go,

712
00:37:46,060 --> 00:37:48,355
whether or not we're going to get a monotonic improvement.

713
00:37:48,355 --> 00:37:52,780
Um, and this part is about sort of giving better estimates of our gradient,

714
00:37:52,780 --> 00:37:54,505
ideally with less data,

715
00:37:54,505 --> 00:37:56,620
um, and reducing the variance of it.

716
00:37:56,620 --> 00:37:57,985
So they're both important,

717
00:37:57,985 --> 00:38:01,705
they're doing slightly different things. All right.

718
00:38:01,705 --> 00:38:03,220
So let's talk about,

719
00:38:03,220 --> 00:38:09,740
ah, could we move this up, please? Thank you.

720
00:38:12,810 --> 00:38:17,440
Okay. So like we sort of started talking about before,

721
00:38:17,440 --> 00:38:21,085
um, well, let's- let's talk about first the baseline.

722
00:38:21,085 --> 00:38:23,780
So how should we choose the baseline?

723
00:38:26,610 --> 00:38:30,550
Um, one thing that we can do for the baseline,

724
00:38:30,550 --> 00:38:33,010
is just to- like what that what we're seeing there,

725
00:38:33,010 --> 00:38:35,350
which is an empirical estimate of V_Pi i.

726
00:38:35,350 --> 00:38:36,850
So we could say, in general,

727
00:38:36,850 --> 00:38:41,450
we wanna just have- use V_Pi i as a baseline.

728
00:38:41,450 --> 00:38:43,870
That means we have to compute it somehow.

729
00:38:43,870 --> 00:38:47,470
And the way we estimate that could be from Monte Carlo

730
00:38:47,470 --> 00:38:51,550
or it could be from TD methods. All right.

731
00:38:51,550 --> 00:38:54,310
So what we've seen so far,

732
00:38:54,310 --> 00:38:59,590
is using these as a- so- so I guess just to be clear here,

733
00:38:59,590 --> 00:39:02,020
there's a couple different places we're going to be able to maybe switch

734
00:39:02,020 --> 00:39:05,590
between doing Monte Carlo returns and doing something TD-like.

735
00:39:05,590 --> 00:39:09,970
One is here, and another is our baseline.

736
00:39:09,970 --> 00:39:13,900
Okay. So we have a baseline function here that we're sort of subtracting off.

737
00:39:13,900 --> 00:39:17,090
And we also have a G_t prime here, okay?

738
00:39:17,970 --> 00:39:21,910
And so if we think about our general equation again,

739
00:39:21,910 --> 00:39:25,840
so what we have in this case is we have delta theta, v of theta.

740
00:39:25,840 --> 00:39:29,215
This is our parameter, this is specifying our policy parameters.

741
00:39:29,215 --> 00:39:33,934
And we've said this is approximately equal to 1 over m sum over i = 1

742
00:39:33,934 --> 00:39:40,030
to m of some reward- Well, I'll put this inside.

743
00:39:40,030 --> 00:39:44,560
Sum over t = 0 to t - 1,

744
00:39:44,560 --> 00:39:48,355
of,- I change my mind.

745
00:39:48,355 --> 00:39:50,350
Okay. I'm going to put this out here because it's going to end up being

746
00:39:50,350 --> 00:39:52,990
sort of a function we can use in lots of different ways.

747
00:39:52,990 --> 00:39:54,710
Okay.

748
00:40:05,670 --> 00:40:07,120
So this is

749
00:40:07,120 --> 00:40:08,830
our basic equation we've been working with.

750
00:40:08,830 --> 00:40:12,700
We've said the derivative of the value with respect to our policy parameter is

751
00:40:12,700 --> 00:40:16,600
approximately as sum over m trajectories,

752
00:40:16,600 --> 00:40:19,210
where we've sampled those trajectories from that policy,

753
00:40:19,210 --> 00:40:22,374
times the total reward we've gotten on that trajectory,

754
00:40:22,374 --> 00:40:27,910
times the sum over all time steps of the derivative of the policy with respect to,

755
00:40:27,910 --> 00:40:31,105
um- given the action we took in the state we were in.

756
00:40:31,105 --> 00:40:35,050
All right. And we said it was very noisy, um, but unbiased.

757
00:40:35,050 --> 00:40:40,135
And now we can think of changing this as a target.

758
00:40:40,135 --> 00:40:44,380
So this here was an unbiased estimator of the value of the policy.

759
00:40:44,380 --> 00:40:48,220
And now we can think about substituting other things in. All right.

760
00:40:48,220 --> 00:40:51,640
So we can imagine doing all sorts of things here.

761
00:40:51,640 --> 00:40:54,130
We cou- we could do, um, you know,

762
00:40:54,130 --> 00:40:57,770
TD or MC methods.

763
00:40:58,920 --> 00:41:03,220
If we do it with a value function or if we try to

764
00:41:03,220 --> 00:41:07,720
explicitly compute a value function or a- or a state action value function,

765
00:41:07,720 --> 00:41:09,970
then we typically call this a critic.

766
00:41:09,970 --> 00:41:20,290
So a critic computes V or Q.

767
00:41:20,290 --> 00:41:22,330
So when we talk about actor-critic methods,

768
00:41:22,330 --> 00:41:25,930
that's when we have an explicit parameterized representation of the policy,

769
00:41:25,930 --> 00:41:29,035
and we have an explicit generally parameterized representation

770
00:41:29,035 --> 00:41:31,435
of the value or the state-action value.

771
00:41:31,435 --> 00:41:33,250
And if we have that,

772
00:41:33,250 --> 00:41:37,340
then we can imagine using that to change what our target is.

773
00:41:39,150 --> 00:41:41,695
I want to emphasize here that so,

774
00:41:41,695 --> 00:41:44,575
actor-critic methods combine these two,

775
00:41:44,575 --> 00:41:47,095
combine policy plus critics.

776
00:41:47,095 --> 00:41:51,070
And probably the most popular one of this is A3C,

777
00:41:51,070 --> 00:41:53,035
which is by Mnich et al,

778
00:41:53,035 --> 00:41:58,400
this was introduced in 2016 ICML.

779
00:41:58,770 --> 00:42:01,060
And it's been hugely popular.

780
00:42:01,060 --> 00:42:03,100
This is a version for deep neural networks.

781
00:42:03,100 --> 00:42:06,895
Um, but actor-critic ideas themselves have been around for a lot longer than that.

782
00:42:06,895 --> 00:42:12,925
But A3C is one of the most popular versions of this for deep neural nets. All right.

783
00:42:12,925 --> 00:42:17,920
So how do we do sort of policy gradient formulas with value functions?

784
00:42:17,920 --> 00:42:23,890
What you could do instead here, I shall put on this side.

785
00:42:23,890 --> 00:42:28,405
What you could do is, you could have almost the same equation as we had before.

786
00:42:28,405 --> 00:42:32,860
So derivative with respect to the value function is equal to

787
00:42:32,860 --> 00:42:38,005
an expectation with respect to the trajectories that you might encounter,

788
00:42:38,005 --> 00:42:41,690
the sum over all the time steps in that trajectory,

789
00:42:42,570 --> 00:42:52,780
times the derivative with respect to your policy parameters,

790
00:42:54,050 --> 00:42:59,085
times Q of S_t,

791
00:42:59,085 --> 00:43:02,650
w - b of S_t.

792
00:43:03,840 --> 00:43:07,330
So instead of having your Monte Carlo estimates in here,

793
00:43:07,330 --> 00:43:10,460
you could plug in your estimate of the Q function.

794
00:43:12,560 --> 00:43:20,475
And another way to represent that here is if we think this is an estimate of the value,

795
00:43:20,475 --> 00:43:24,180
and this is basically our advantage function again.

796
00:43:24,180 --> 00:43:28,125
But it could be our- so we had an advantage function over here.

797
00:43:28,125 --> 00:43:30,240
You define that advantage function here,

798
00:43:30,240 --> 00:43:34,425
but this one was a function of the Monte Carlo returns for that episode.

799
00:43:34,425 --> 00:43:37,455
This is a different advantage function which

800
00:43:37,455 --> 00:43:41,730
is the Q function which- where this could be maintained by a critic,

801
00:43:41,730 --> 00:43:45,430
minus your baseline, which is an estimate the value function.

802
00:43:45,830 --> 00:43:48,030
So they look pretty similar,

803
00:43:48,030 --> 00:43:50,055
but you can plug in different choices here.

804
00:43:50,055 --> 00:43:51,750
And these are going to have different trade offs.

805
00:43:51,750 --> 00:43:53,940
So the Monte Carlo estimate of

806
00:43:53,940 --> 00:43:57,300
the return is an unbiased estimate of the value of the current policy.

807
00:43:57,300 --> 00:44:00,495
This is going to be biased generally,

808
00:44:00,495 --> 00:44:06,015
but lower variance. All right.

809
00:44:06,015 --> 00:44:10,425
So I also want to emphasize here that when we think about,

810
00:44:10,425 --> 00:44:14,040
um, kind of getting this estimator,

811
00:44:14,040 --> 00:44:19,080
which we often say the critic is going to compute this estimator, um,

812
00:44:19,080 --> 00:44:24,645
It doesn't have to be only either a TD estimate or a Monte Carlo estimate,

813
00:44:24,645 --> 00:44:26,595
but you can interpolate between these.

814
00:44:26,595 --> 00:44:29,205
It's often known as n-step returns.

815
00:44:29,205 --> 00:44:31,710
So what does that mean?

816
00:44:31,710 --> 00:44:36,600
So let's call- let's write this in a slightly, well, I'll call this here.

817
00:44:36,600 --> 00:44:38,145
So let's put this is a hat.

818
00:44:38,145 --> 00:44:41,670
Okay? Just to note that you can think of this as kind of just a function.

819
00:44:41,670 --> 00:44:45,630
It's going to be an estimate of your state action value function.

820
00:44:45,630 --> 00:44:50,590
And so what we could have is we could have an estimator.

821
00:44:51,590 --> 00:44:57,495
I could have estimator of the value from time-step t onwards,

822
00:44:57,495 --> 00:44:59,970
which is equal to the actual,

823
00:44:59,970 --> 00:45:03,945
this is the actual one we got on time-step i, so I'm going to call this.

824
00:45:03,945 --> 00:45:07,410
I'm going to call this sort of i, 1.

825
00:45:07,410 --> 00:45:13,050
And this is going to be then the actual reward you got on time-step t in episode i,

826
00:45:13,050 --> 00:45:19,125
+ gamma V of S_t + 1 i.

827
00:45:19,125 --> 00:45:23,880
So this should look almost exactly like TD(0) style estimates.

828
00:45:23,880 --> 00:45:26,070
We talked about this before.

829
00:45:26,070 --> 00:45:28,845
So this says, I got- I look at the actual,

830
00:45:28,845 --> 00:45:32,205
immediate one-step reward I got and then I bootstrap.

831
00:45:32,205 --> 00:45:34,005
I add in the value.

832
00:45:34,005 --> 00:45:37,455
So this would be- I get this value function for my critic.

833
00:45:37,455 --> 00:45:40,410
And I would plug that in and then that would be my target.

834
00:45:40,410 --> 00:45:42,690
That then I would use in this equation.

835
00:45:42,690 --> 00:45:47,100
Okay. So that would be one thing you do- you can do.

836
00:45:47,100 --> 00:45:48,525
So we've seen this,

837
00:45:48,525 --> 00:45:50,145
and we've seen a lot of this,

838
00:45:50,145 --> 00:45:55,465
which I'm going to call the infinite or Monte Carlo version.

839
00:45:55,465 --> 00:45:59,960
And this one is you sum over all t prime,

840
00:45:59,960 --> 00:46:04,010
all the way to the end of the episode of gamma 2.

841
00:46:04,010 --> 00:46:10,030
The t prime minus t times r t prime.

842
00:46:10,030 --> 00:46:12,165
So this is the Monte Carlo return,

843
00:46:12,165 --> 00:46:14,100
where we just sum up, we don't do any bootstrapping,

844
00:46:14,100 --> 00:46:16,050
and we sum up all the rewards at the end of the episodes.

845
00:46:16,050 --> 00:46:17,970
But as you can see here, there should be, you know,

846
00:46:17,970 --> 00:46:20,475
there's probably some way to interpolate between these two.

847
00:46:20,475 --> 00:46:23,985
And these are often known as n step returns.

848
00:46:23,985 --> 00:46:26,324
And so for example,

849
00:46:26,324 --> 00:46:27,540
you could do this, you could say,

850
00:46:27,540 --> 00:46:30,660
I'm going to add in the reward at time step i and

851
00:46:30,660 --> 00:46:34,830
the reward I got at time step t + 1.

852
00:46:34,830 --> 00:46:38,200
And then I'm going to bootstrap.

853
00:46:43,010 --> 00:46:48,195
So this is just sort of one of the estimators that are in between these two extremes.

854
00:46:48,195 --> 00:46:51,015
One is you only take in one step of reward,

855
00:46:51,015 --> 00:46:52,650
another one is do you sum up all the rewards,

856
00:46:52,650 --> 00:46:54,660
and then there's a whole bunch of interpolation you could do

857
00:46:54,660 --> 00:46:56,940
between those. Why would you want to do that?

858
00:46:56,940 --> 00:47:01,500
Well, this one is generally going to be somewhat biased, but low variance.

859
00:47:01,500 --> 00:47:02,850
This is going to be unbiased,

860
00:47:02,850 --> 00:47:04,140
but really high variance.

861
00:47:04,140 --> 00:47:05,730
And there's no reason to assume that

862
00:47:05,730 --> 00:47:08,070
the best solution is on either of those two extremes.

863
00:47:08,070 --> 00:47:14,110
And so you could interpolate between a TD estimate and a Monte Carlo estimate.

864
00:47:14,540 --> 00:47:18,060
And all of these just form

865
00:47:18,060 --> 00:47:21,960
returns that then you could subtract off a baseline for too.

866
00:47:21,960 --> 00:47:24,735
So traditional and all this would probably be

867
00:47:24,735 --> 00:47:28,545
sort of hyper-parameter that we can choose through validation or cross-validation.

868
00:47:28,545 --> 00:47:28,890
Right.

869
00:47:28,890 --> 00:47:30,000
Is that what people do here?

870
00:47:30,000 --> 00:47:32,100
Is that kinda too computationally intensive?

871
00:47:32,100 --> 00:47:33,660
So you just have to pick something.

872
00:47:33,660 --> 00:47:36,540
Question is if we were doing standard machine-learning,

873
00:47:36,540 --> 00:47:38,850
this would be just considered as some sort of hyper parameter.

874
00:47:38,850 --> 00:47:40,950
You could turn this into n and you would decide like

875
00:47:40,950 --> 00:47:43,770
how many steps do you do- do people do.

876
00:47:43,770 --> 00:47:45,390
And the question was do- do people do that in

877
00:47:45,390 --> 00:47:47,940
reinforcement learning or is it that considered too expensive?

878
00:47:47,940 --> 00:47:51,255
You certainly could. I- I think it's an interesting question.

879
00:47:51,255 --> 00:47:54,540
I feel like the tricks that people do in this case,

880
00:47:54,540 --> 00:47:59,940
[NOISE] I think I probably see more of this but it varies [NOISE] more using the TD(0),

881
00:47:59,940 --> 00:48:01,305
doing a lot of bootstrapping,

882
00:48:01,305 --> 00:48:03,285
but it probably depends on your application domain.

883
00:48:03,285 --> 00:48:08,175
Another thing that it would likely depend on is if your domain is really Markov or not.

884
00:48:08,175 --> 00:48:10,770
So this case this is still working,

885
00:48:10,770 --> 00:48:12,540
and this is giving you a real estimate of the return of

886
00:48:12,540 --> 00:48:14,625
your policy even if your domain isn't Markov.

887
00:48:14,625 --> 00:48:18,060
This case you're making a much stronger Markovian assumption.

888
00:48:18,060 --> 00:48:22,845
So you also might want to make- do different things depending on your domain.

889
00:48:22,845 --> 00:48:28,290
And also how expensive it is to collect data. All right.

890
00:48:28,290 --> 00:48:31,650
So this gives us sort of a different way to plug things

891
00:48:31,650 --> 00:48:35,355
in to that vanilla policy gradient algorithm I put over there.

892
00:48:35,355 --> 00:48:38,820
So you could plug in these sort of targets instead,

893
00:48:38,820 --> 00:48:42,720
over there to trade off between bias and variance,

894
00:48:42,720 --> 00:48:45,360
when you're doing this estimate of the gradient.

895
00:48:45,360 --> 00:48:48,165
So what this is doing here is it's changing what our targets are,

896
00:48:48,165 --> 00:48:52,140
and it's changing how we're computing our gradient [NOISE].

897
00:48:52,140 --> 00:48:55,905
But then the next thing I wanna talk about is this part.

898
00:48:55,905 --> 00:48:58,770
Which is once we've actually got our gradient,

899
00:48:58,770 --> 00:49:00,435
however we've chosen to get it.

900
00:49:00,435 --> 00:49:04,050
We have to actually figure out how far to go along that gradient.

901
00:49:04,050 --> 00:49:09,780
[NOISE] So why might this be important?

902
00:49:09,780 --> 00:49:14,940
Well, it might be important because,

903
00:49:14,940 --> 00:49:18,000
this is just a local approximation.

904
00:49:18,000 --> 00:49:20,130
You're giving your local estimate of the gradient. Yeah.

905
00:49:20,130 --> 00:49:22,500
How often do you update the parameters of the critic?

906
00:49:22,500 --> 00:49:28,155
The question was how often do you update the parameters of the critic?

907
00:49:28,155 --> 00:49:31,080
It's a great question, again, it depends.

908
00:49:31,080 --> 00:49:33,585
So you can either- you can do this often asynchronously.

909
00:49:33,585 --> 00:49:36,090
So you can have different threads and different networks for

910
00:49:36,090 --> 00:49:39,510
your critic and for your policy and principle,

911
00:49:39,510 --> 00:49:41,640
you could just be updating your critic all the time like, you know,

912
00:49:41,640 --> 00:49:44,985
you can be using DQN for this and doing lots and lots of backups.

913
00:49:44,985 --> 00:49:47,250
In general, it depends,

914
00:49:47,250 --> 00:49:49,725
I think you'd have a schedule.

915
00:49:49,725 --> 00:49:52,710
Yeah. So often you might do something like 10 or 100,

916
00:49:52,710 --> 00:49:54,840
it really varies by application, um.

917
00:49:54,840 --> 00:49:57,870
Uh, but there's no reason that the critic

918
00:49:57,870 --> 00:50:01,035
needs to be updated only on the same schedule at which you're updating the policy.

919
00:50:01,035 --> 00:50:04,450
And doing it asynchronously often makes a lot of sense.

920
00:50:05,060 --> 00:50:08,520
All right. So if we think about what's happening here,

921
00:50:08,520 --> 00:50:09,780
here's our parameterized policy.

922
00:50:09,780 --> 00:50:12,930
Here's our value. We have some crazy function.

923
00:50:12,930 --> 00:50:16,330
Okay. And then we are computing our gradient.

924
00:50:17,240 --> 00:50:24,660
And this gradient, locally is pretty good.

925
00:50:24,660 --> 00:50:28,800
So kinda round here things like linear and things look pretty good.

926
00:50:28,800 --> 00:50:32,295
But of course, as we get further out like here, it's gonna be bad.

927
00:50:32,295 --> 00:50:35,550
Like if we- if we try to follow the gradient too far,

928
00:50:35,550 --> 00:50:37,320
we're going to get an estimate that's very different

929
00:50:37,320 --> 00:50:40,150
than the real va- real value function.

930
00:50:41,630 --> 00:50:44,670
So when we're taking step sizes in this case,

931
00:50:44,670 --> 00:50:46,920
it ends up being important to consider this fact of sort of

932
00:50:46,920 --> 00:50:49,665
how far out do you want your step sizes to be.

933
00:50:49,665 --> 00:50:55,660
Let me just get this back to one [inaudible] Okay.

934
00:51:00,950 --> 00:51:06,390
So we want to figure out how far we should go in the gradient and that's important.

935
00:51:06,390 --> 00:51:09,150
Now, you might say,

936
00:51:09,150 --> 00:51:10,350
okay this is always true.

937
00:51:10,350 --> 00:51:12,720
Right? Like you always need to be careful when you're doing

938
00:51:12,720 --> 00:51:16,050
gradient descent or ascent in any supervised learning problem.

939
00:51:16,050 --> 00:51:18,165
Whenever you're using stochastic gradient descent.

940
00:51:18,165 --> 00:51:21,584
Of course, you don't wanna go too far along your step size because you could overshoot

941
00:51:21,584 --> 00:51:25,770
and you're using this linear approximation and it's bad.

942
00:51:25,770 --> 00:51:29,070
Why does anybody- does anybody have any sense about why this might

943
00:51:29,070 --> 00:51:32,310
be even worse in the reinforcement learning case?

944
00:51:32,310 --> 00:51:35,685
Why might it be even more important to think about this step size.

945
00:51:35,685 --> 00:51:42,225
And it has to do with where the data comes from. Yeah.

946
00:51:42,225 --> 00:51:45,510
So when you have a bad policy

947
00:51:45,510 --> 00:51:49,020
that affects the data you collect, and you might just go down a  bad road.

948
00:51:49,020 --> 00:51:51,675
Sure. So she's exactly correct.

949
00:51:51,675 --> 00:51:53,730
In a supervised learning case,

950
00:51:53,730 --> 00:51:56,535
your data is being generated by an IID distribution,

951
00:51:56,535 --> 00:52:00,855
it doesn't matter what choices you just made for your stochastic gradient descent.

952
00:52:00,855 --> 00:52:03,240
In RL that is determining the next policy we're

953
00:52:03,240 --> 00:52:06,555
using inside of our iteration to gather data.

954
00:52:06,555 --> 00:52:10,695
So we're not going to, you know, if we take a really bad, if we get really,

955
00:52:10,695 --> 00:52:12,780
really bad policies, we just may be getting

956
00:52:12,780 --> 00:52:15,645
no data towards the actual optima of this function.

957
00:52:15,645 --> 00:52:18,795
So it's even more important to sort of carefully

958
00:52:18,795 --> 00:52:23,565
think about where we're going along here and ideally hopefully get monotonic improvement.

959
00:52:23,565 --> 00:52:26,070
So this is- this is the really,

960
00:52:26,070 --> 00:52:28,470
it's very important in the reinforcement learning case,

961
00:52:28,470 --> 00:52:31,304
to think about how we're doing this step size because

962
00:52:31,304 --> 00:52:35,800
this determines the data we collect.

963
00:52:35,900 --> 00:52:39,850
pi and therefore data.

964
00:52:44,420 --> 00:52:46,575
And one version of, um,

965
00:52:46,575 --> 00:52:49,200
one of my colleagues talks about a similar problem.

966
00:52:49,200 --> 00:52:52,140
He sort of has the picture of the Roadrunner running off the cliff, right?

967
00:52:52,140 --> 00:52:53,790
And like them you if you're in a part where your

968
00:52:53,790 --> 00:52:55,890
I- your policy is just really really bad.

969
00:52:55,890 --> 00:52:57,645
You may get no more useful data.

970
00:52:57,645 --> 00:53:01,350
Then you can't get a good estimate there of the gradient and then you're just stuck.

971
00:53:01,350 --> 00:53:03,600
You might get in a really really bad optima.

972
00:53:03,600 --> 00:53:06,990
Okay. So we'd like to think carefully about this part.

973
00:53:06,990 --> 00:53:09,990
So one thing that we could do,

974
00:53:09,990 --> 00:53:11,970
is do something like line search.

975
00:53:11,970 --> 00:53:14,700
So we're talking about right now sort of how do we do,

976
00:53:14,700 --> 00:53:20,590
so how to do step sizes.

977
00:53:25,700 --> 00:53:30,450
So one thing we could do is try to do some sort of line search along the gradient.

978
00:53:30,450 --> 00:53:39,450
[BACKGROUND]

979
00:53:39,450 --> 00:53:41,590
And this is, um, okay

980
00:53:41,590 --> 00:53:42,985
but it's a little bit expensive.

981
00:53:42,985 --> 00:53:49,100
So it's simple but it's expensive.

982
00:53:51,000 --> 00:53:58,045
And it tends to sort of ignore where the linear approximation is good.

983
00:53:58,045 --> 00:54:01,310
So we'd like to do better than this.

984
00:54:01,800 --> 00:54:08,170
Okay. So now we're gonna go back to that point that I mentioned at the start

985
00:54:08,170 --> 00:54:10,330
which is what we'd really like to be able to do is when we're doing

986
00:54:10,330 --> 00:54:13,885
this updating we would like to ensure monotonic improvement.

987
00:54:13,885 --> 00:54:17,470
And so can we kind of choose our step sizes in a way or

988
00:54:17,470 --> 00:54:21,640
choose how far along the gradient to go in order to achieve monotonic improvement.

989
00:54:21,640 --> 00:54:25,390
So what our goal is gonna be is we'd like to have it.

990
00:54:25,390 --> 00:54:31,190
So that V pi of i + 1 is greater than or equal to V pi i.

991
00:54:33,780 --> 00:54:39,220
And we're hoping to achieve this by changing how big of a step size we take.

992
00:54:39,220 --> 00:54:43,870
All right. So let's think about what our- our objective function is again.

993
00:54:43,870 --> 00:54:48,790
Um so we're getting- we have our value, our parameterized value.

994
00:54:48,790 --> 00:54:57,850
So V of theta is equal to the expected value under our policy that's defined by theta of

995
00:54:57,850 --> 00:55:07,975
just the sum over t = 0 to infinity of gamma t_r S_t a_t under our policy.

996
00:55:07,975 --> 00:55:11,530
Okay. And this is where we just sort of

997
00:55:11,530 --> 00:55:14,245
look at the series of states we get under our policy.

998
00:55:14,245 --> 00:55:16,360
So that's our basic equation here in terms of

999
00:55:16,360 --> 00:55:19,225
expressing the value of a parameterized policy.

1000
00:55:19,225 --> 00:55:22,930
And what we would like to do here is we would like

1001
00:55:22,930 --> 00:55:26,680
to get a new policy that has a better value.

1002
00:55:26,680 --> 00:55:30,115
But the problem is that we have samples from an old policy.

1003
00:55:30,115 --> 00:55:34,330
So when we're doing this we're gathering policies with

1004
00:55:34,330 --> 00:55:38,575
pi i and then we're trying to figure out what our pi i + 1 should be.

1005
00:55:38,575 --> 00:55:41,425
So this is gonna fundamentally involve.

1006
00:55:41,425 --> 00:55:44,980
Um so we have access to- we have access to

1007
00:55:44,980 --> 00:55:50,545
trajectories that are sampled from pi of theta.

1008
00:55:50,545 --> 00:55:59,155
And we now wanna sort of predict the value of v of pi of theta.

1009
00:55:59,155 --> 00:56:02,140
I'll put pi i, i + 1.

1010
00:56:02,140 --> 00:56:06,070
So we'd like to sort of now figure out what a new value would be

1011
00:56:06,070 --> 00:56:10,570
if we update these, update these parameters in some way and take like a max.

1012
00:56:10,570 --> 00:56:14,540
You know we'd like to figure out what the new parameters are.

1013
00:56:15,900 --> 00:56:20,110
But this is fundamentally an off policy problem because we have

1014
00:56:20,110 --> 00:56:24,590
data from our last policy and we wanna figure out what our next policy should be.

1015
00:56:25,650 --> 00:56:32,860
Okay. So what we're gonna do is we're gonna first re-express um the value

1016
00:56:32,860 --> 00:56:35,875
of our policy in terms of the advantage oh- the value

1017
00:56:35,875 --> 00:56:39,730
of our new policy in terms of the advantage over our old policy.

1018
00:56:39,730 --> 00:56:44,680
So I'm gonna move down to vanilla policy gradient for now.

1019
00:56:44,680 --> 00:56:49,600
[NOISE]

1020
00:56:49,600 --> 00:56:54,310
Okay. So what we have is we have V of Theta tilde.

1021
00:56:54,310 --> 00:56:58,480
So that could be like our new parameterized policy is

1022
00:56:58,480 --> 00:57:02,320
gonna be equal to the value of our old parameterized policy.

1023
00:57:02,320 --> 00:57:05,840
So whatever we had before plus the following.

1024
00:57:06,270 --> 00:57:10,180
The distribution over the states and actions we'd get if we

1025
00:57:10,180 --> 00:57:13,465
were to run our new policy. Now we don't know that.

1026
00:57:13,465 --> 00:57:17,830
But let's ima- let's ignore that for a second of a sum over

1027
00:57:17,830 --> 00:57:23,180
t = 0 to infinity gamma to the t, the advantage pi.

1028
00:57:24,930 --> 00:57:31,270
Okay. So this- this just generally holds.

1029
00:57:31,270 --> 00:57:35,530
Um, this doesn't have to do anything necessarily with being um, parameterized.

1030
00:57:35,530 --> 00:57:39,010
This is just saying the value of any policy which is here

1031
00:57:39,010 --> 00:57:43,135
parameterized by pi tilde is equal to the value of another policy plus

1032
00:57:43,135 --> 00:57:45,550
the sum over the states and actions you'd reach under

1033
00:57:45,550 --> 00:57:48,190
your target policy of the advantage

1034
00:57:48,190 --> 00:57:51,920
you get of taking this new policy over the old policy.

1035
00:57:54,030 --> 00:57:58,440
Okay. So that um, that just expresses how we can say what the

1036
00:57:58,440 --> 00:58:01,995
va- how uh, the value of a new policy relates to the value of the old policy.

1037
00:58:01,995 --> 00:58:04,080
It's exactly the same as the old values policy

1038
00:58:04,080 --> 00:58:06,600
plus the advantage you'd get if you were to

1039
00:58:06,600 --> 00:58:11,700
run the new policy and look at the state action distribution you'd encounter. Yes?

1040
00:58:11,700 --> 00:58:14,890
Should the subscript be Pi tilde on the advantage?

1041
00:58:14,890 --> 00:58:18,190
Should the subscript be Pi tilde on the advantage?

1042
00:58:18,190 --> 00:58:19,375
[OVERLAPPING] policy?

1043
00:58:19,375 --> 00:58:22,660
Yeah. So we're doing in it- let me write this up thing.

1044
00:58:22,660 --> 00:58:25,600
So [inaudible] question is a good one, let me just write this out to be for- for long.

1045
00:58:25,600 --> 00:58:31,340
Okay, so we've got V of theta plus sum over all the states

1046
00:58:31,340 --> 00:58:37,435
and we're gonna use mu pi tilde of s. So remember this was the stationary distribution.

1047
00:58:37,435 --> 00:58:40,630
Um we use this to denote the stationary distribution over states that we'd

1048
00:58:40,630 --> 00:58:46,105
reach um if we were to run our new policy which is parameterized tilde.

1049
00:58:46,105 --> 00:58:53,600
This is theta tilde. Okay? Um times the advantage function.

1050
00:58:56,490 --> 00:59:07,090
Okay. So what this is saying here is this S_t and a_t are under our desired policy.

1051
00:59:07,090 --> 00:59:10,285
And the advantage here is using the old one.

1052
00:59:10,285 --> 00:59:17,005
Okay? So this is allowing us to compare.

1053
00:59:17,005 --> 00:59:20,530
So what does this do? It's allowing us to compare

1054
00:59:20,530 --> 00:59:26,515
s_a S_t a_t minus

1055
00:59:26,515 --> 00:59:32,090
Q of S_t our old policy.

1056
00:59:40,980 --> 00:59:45,640
And this is under- so

1057
00:59:45,640 --> 00:59:50,635
it's allowing us to compare how much better is that if we take like our new action.

1058
00:59:50,635 --> 00:59:52,975
Okay? All right.

1059
00:59:52,975 --> 00:59:56,110
But one of the problems of this is we don't know this.

1060
00:59:56,110 --> 00:59:59,650
Yeah?

1061
00:59:59,650 --> 01:00:03,220
Sum over from t = 0 to infinity somewhere?

1062
01:00:03,220 --> 01:00:04,330
Oh, thank you um.

1063
01:00:04,330 --> 01:00:05,770
And answering- yeah. Yeah.

1064
01:00:05,770 --> 01:00:06,010
Yeah.

1065
01:00:06,010 --> 01:00:07,000
And also again [OVERLAPPING]

1066
01:00:07,000 --> 01:00:09,850
So the question is is [inaudible]?

1067
01:00:09,850 --> 01:00:13,105
No. And thank you for make me uh allowing me to clarify that.

1068
01:00:13,105 --> 01:00:15,310
So in this case what we're doing is we're taking um

1069
01:00:15,310 --> 01:00:17,770
an expectation over all time-steps and

1070
01:00:17,770 --> 01:00:22,645
this is saying over the trajectories that we'd get to under our new policy.

1071
01:00:22,645 --> 01:00:27,940
I've now reformulated and said well we have a stationary distribution over states.

1072
01:00:27,940 --> 01:00:30,670
If we look at what is the probability of reaching

1073
01:00:30,670 --> 01:00:34,525
those states and then we weight that by the advantage.

1074
01:00:34,525 --> 01:00:39,040
So we've went from a time averaging to a state averaging.

1075
01:00:39,040 --> 01:00:41,275
Does that makes sense?

1076
01:00:41,275 --> 01:00:45,670
So we can either think of our value function or averaging our value function across

1077
01:00:45,670 --> 01:00:47,680
time-steps where we can think here is

1078
01:00:47,680 --> 01:00:50,605
averaging across all the states and what is for each state.

1079
01:00:50,605 --> 01:00:54,460
What is the relative value you get by following your new policy versus your old policy?

1080
01:00:54,460 --> 01:00:58,210
[inaudible].

1081
01:00:58,210 --> 01:01:01,610
Oh sorry. Thank you. Then that- those are typos.

1082
01:01:04,140 --> 01:01:07,180
Okay. All right.

1083
01:01:07,180 --> 01:01:10,520
So this would be under-

1084
01:01:15,690 --> 01:01:20,680
so we look at the states that- for each state what is

1085
01:01:20,680 --> 01:01:22,600
the probability we'd reached that state under

1086
01:01:22,600 --> 01:01:26,330
our new policy and what is the relative advantage?

1087
01:01:32,790 --> 01:01:36,850
The thing that you're pointing to should not have

1088
01:01:36,850 --> 01:01:46,180
[inaudible].

1089
01:01:46,180 --> 01:01:49,615
Oh sorry. We have a tilde over here, this is our value of our original one

1090
01:01:49,615 --> 01:01:53,860
plus this we get the advantage term over all the states. Yeah.

1091
01:01:53,860 --> 01:02:00,880
Is there a difference between pi theta tilde and tilde pi?

1092
01:02:00,880 --> 01:02:03,835
I was just doing this here to make it clear.

1093
01:02:03,835 --> 01:02:07,450
Pi L- say Pi tilde is also- Pi tilde is

1094
01:02:07,450 --> 01:02:12,265
parameterized by, this is a policy parameterized by theta tilde.

1095
01:02:12,265 --> 01:02:14,980
Yeah. I'm just saying it's like in the expectation they wrote in the first slide

1096
01:02:14,980 --> 01:02:20,680
[inaudible]

1097
01:02:20,680 --> 01:02:23,110
Okay. We can vote on that side.

1098
01:02:23,110 --> 01:02:27,550
But either- I- I wanted to just be clear um, often this notation goes back

1099
01:02:27,550 --> 01:02:29,440
and forth between using do you wanna make

1100
01:02:29,440 --> 01:02:32,065
the policy explicit as opposed to just the parameters.

1101
01:02:32,065 --> 01:02:37,750
Um I think it's more clear to have a policy um and parameters here but often we also

1102
01:02:37,750 --> 01:02:40,600
use the- you can just directly parameterize the value function in terms of

1103
01:02:40,600 --> 01:02:44,110
theta as opposed to V pi tilde of theta.

1104
01:02:44,110 --> 01:02:49,810
But I'm gonna just use any of these is fine.

1105
01:02:49,810 --> 01:02:52,300
Is anybody confused about what this is?

1106
01:02:52,300 --> 01:02:54,100
I mean if it's easier I can just go like this.

1107
01:02:54,100 --> 01:02:56,395
Okay. So I can just remove all of this.

1108
01:02:56,395 --> 01:02:59,740
Okay. So this is just whenever I say pi tilde

1109
01:02:59,740 --> 01:03:03,190
that's the policy that's parameterized by the new- that's your new policy.

1110
01:03:03,190 --> 01:03:05,260
Okay. And that's this.

1111
01:03:05,260 --> 01:03:09,280
I know it's a lot of notation.

1112
01:03:09,280 --> 01:03:13,555
Does anybody have any questions about that notation last? Yeah?

1113
01:03:13,555 --> 01:03:13,780
Yeah.

1114
01:03:13,780 --> 01:03:15,460
Name first please.

1115
01:03:15,460 --> 01:03:17,230
So I guess I'm a little bit confused mostly

1116
01:03:17,230 --> 01:03:19,225
just because it's a little bit different from the slides.

1117
01:03:19,225 --> 01:03:20,920
And I'm just wondering-

1118
01:03:20,920 --> 01:03:23,050
Shouldn't be different by the slides but I'm trying to go-

1119
01:03:23,050 --> 01:03:26,170
[LAUGHTER] I was wondering sort in this case do we

1120
01:03:26,170 --> 01:03:29,425
sum over the possible actions for a given state or

1121
01:03:29,425 --> 01:03:32,590
as we've noted here do we assume that we take

1122
01:03:32,590 --> 01:03:37,775
a single action per uh by using the policy which is what we have here on the board?

1123
01:03:37,775 --> 01:03:41,530
You are right. I forgot that right.

1124
01:03:41,530 --> 01:03:46,790
I'm gonna go. Repeat that again.

1125
01:03:46,790 --> 01:03:50,890
Okay. Okay. Let's say V of theta

1126
01:03:50,890 --> 01:03:56,980
tilde I'll go with the same exact same notation as the slides is equal

1127
01:03:56,980 --> 01:04:03,190
to V of Theta plus sum over states or

1128
01:04:03,190 --> 01:04:10,345
stationary distribution over Pi tilde of s. This is the distribution we get.

1129
01:04:10,345 --> 01:04:16,850
Um this is the discounted weight of distribution under our target policy.

1130
01:04:21,990 --> 01:04:29,990
Under pi tilde. Okay. Sum over A.

1131
01:04:36,930 --> 01:04:40,025
Okay.

1132
01:04:40,025 --> 01:04:42,135
So this is the weighted affair,

1133
01:04:42,135 --> 01:04:44,145
this is the weighted distribution under the states.

1134
01:04:44,145 --> 01:04:47,610
We went from the time domain to thinking about the distribution over

1135
01:04:47,610 --> 01:04:52,560
states times looking at all the actions we might take under

1136
01:04:52,560 --> 01:04:56,205
our target policy and the relative advantage of each of those

1137
01:04:56,205 --> 01:05:01,740
over our previous policy, [NOISE] okay?

1138
01:05:01,740 --> 01:05:05,370
And this should look very, very similar to imitation learning in certain ways, right?

1139
01:05:05,370 --> 01:05:07,980
Like, so we're again sort of thinking about,

1140
01:05:07,980 --> 01:05:10,740
um, instead of thinking about subbing rewards over time steps,

1141
01:05:10,740 --> 01:05:14,610
we're thinking about what is the stationary distribution we might get to under

1142
01:05:14,610 --> 01:05:16,410
a new policy and how that compares to

1143
01:05:16,410 --> 01:05:19,830
the stationary distribution we would have had under our old policy.

1144
01:05:19,830 --> 01:05:22,500
Um, and what we're looking at so far is,

1145
01:05:22,500 --> 01:05:26,355
we're looking at the [NOISE] stationary distribution under our target policy.

1146
01:05:26,355 --> 01:05:28,155
The problem is, we don't know this,

1147
01:05:28,155 --> 01:05:30,045
so we can't calculate this.

1148
01:05:30,045 --> 01:05:31,725
This is just an expression.

1149
01:05:31,725 --> 01:05:40,935
Um, uh, but this is unknown [NOISE] because we don't have any samples from pi tilde,

1150
01:05:40,935 --> 01:05:45,490
we only have samples from pi, okay?

1151
01:05:46,010 --> 01:05:48,720
So we can't compute this.

1152
01:05:48,720 --> 01:05:50,070
Why would we, and just to go back,

1153
01:05:50,070 --> 01:05:51,540
why are we trying to do any of this?

1154
01:05:51,540 --> 01:05:54,750
We're trying to do this because when we do vanilla policy gradient,

1155
01:05:54,750 --> 01:06:00,935
[NOISE] we're gonna be trying to figure out a new policy at a,

1156
01:06:00,935 --> 01:06:03,650
that has a value that's better than our old policy.

1157
01:06:03,650 --> 01:06:05,900
What we did here is, we tried to estimate the

1158
01:06:05,900 --> 01:06:09,995
derivative out of the current pol- of the current policy,

1159
01:06:09,995 --> 01:06:14,215
um, but we don't know anything yet about the value once we take that step.

1160
01:06:14,215 --> 01:06:16,470
And so what we're trying to do here is to say, well,

1161
01:06:16,470 --> 01:06:22,140
can we somehow understand what the value will be of a new policy before we execute it?

1162
01:06:22,140 --> 01:06:25,440
And we're gonna do that by trying to relate it to what is our value of

1163
01:06:25,440 --> 01:06:31,965
the previous policy plus some sort of distance between the old policy and the new policy,

1164
01:06:31,965 --> 01:06:36,495
ideally computed in terms of things we can actually evaluate using our current samples.

1165
01:06:36,495 --> 01:06:38,400
That's where we're trying to go to, okay?

1166
01:06:38,400 --> 01:06:40,905
[NOISE] Right.

1167
01:06:40,905 --> 01:06:44,040
So we have this nice expression, but we can't compute it.

1168
01:06:44,040 --> 01:06:49,290
So we're gonna make up a new objective function, okay?

1169
01:06:49,290 --> 01:06:52,410
We're gonna do this one sort of backwards because we're [NOISE] gonna make it up,

1170
01:06:52,410 --> 01:06:54,105
and then, we're gonna show why it's a good thing to do.

1171
01:06:54,105 --> 01:06:55,845
So what are we trying to do?

1172
01:06:55,845 --> 01:06:57,660
We'd like to use something like this.

1173
01:06:57,660 --> 01:06:59,100
If we had this,

1174
01:06:59,100 --> 01:07:02,835
then we could compare the value of the new policy to the value of the old policy.

1175
01:07:02,835 --> 01:07:05,235
The problem is, we don't have this because we don't have the, uh,

1176
01:07:05,235 --> 01:07:09,555
the stationary distribution under the new policy.

1177
01:07:09,555 --> 01:07:11,910
So what we're gonna do instead is,

1178
01:07:11,910 --> 01:07:14,910
we're going to define an objective function L_Pi,

1179
01:07:14,910 --> 01:07:17,595
[NOISE] which is as follows.

1180
01:07:17,595 --> 01:07:22,725
It's the value of your old policy plus a sum over all your states,

1181
01:07:22,725 --> 01:07:27,885
the stationary distribu- discounted distribution of your old policy.

1182
01:07:27,885 --> 01:07:30,120
This is where it's different.

1183
01:07:30,120 --> 01:07:32,595
So this is [NOISE] the old,

1184
01:07:32,595 --> 01:07:36,075
your current policy, okay?

1185
01:07:36,075 --> 01:07:39,060
And then, the rest of the expression looks the same.

1186
01:07:39,060 --> 01:07:48,180
[NOISE] Now, notice, we can compute this, okay?

1187
01:07:48,180 --> 01:07:50,250
This is, um, we could just

1188
01:07:50,250 --> 01:07:53,820
average over all the trajectories we have for the current episode,

1189
01:07:53,820 --> 01:07:58,275
we could estimate our stationary distribution from our current data,

1190
01:07:58,275 --> 01:08:02,700
we could know for a new policy what its action or,

1191
01:08:02,700 --> 01:08:05,670
so if someone gives me a new policy pi,

1192
01:08:05,670 --> 01:08:07,095
I could evaluate this,

1193
01:08:07,095 --> 01:08:09,825
and I could also evaluate the advantage, okay,

1194
01:08:09,825 --> 01:08:14,890
because this advantage is defined only in terms of my previous Pi.

1195
01:08:15,050 --> 01:08:17,250
So as long as if I have a, uh,

1196
01:08:17,250 --> 01:08:20,444
a representation of the state action value function

1197
01:08:20,444 --> 01:08:22,874
for my old policy, I could evaluate this.

1198
01:08:22,875 --> 01:08:24,990
So now, all of this is evaluatable.

1199
01:08:24,990 --> 01:08:26,970
[NOISE] This might not be a good thing to do,

1200
01:08:26,970 --> 01:08:28,200
but we can compute this.

1201
01:08:28,200 --> 01:08:34,140
[inaudible]

1202
01:08:34,140 --> 01:08:37,410
and then, like, giving that to pi itself?

1203
01:08:37,410 --> 01:08:40,080
Yes. [inaudible] in terms of sort of notation where, like, yeah,

1204
01:08:40,080 --> 01:08:42,270
I'm using pi tilde interchangeably with,

1205
01:08:42,270 --> 01:08:44,805
you can, they- they're just some new parameters for computing.

1206
01:08:44,805 --> 01:08:47,819
So our policy is always parameterized by some set of parameters.

1207
01:08:47,819 --> 01:08:49,979
You can either think of that as just being a new policy,

1208
01:08:49,979 --> 01:08:53,339
or you can think of that as the new parameters, either is fine.

1209
01:08:53,340 --> 01:08:56,955
Uh, so could quickly explain why [NOISE] if you're given, um,

1210
01:08:56,955 --> 01:09:04,319
the pi tilde, why you won't be able to calculate the, uh, new [OVERLAPPING]?

1211
01:09:04,319 --> 01:09:06,119
You don't have to. So, um, so the question is,

1212
01:09:06,120 --> 01:09:07,859
if you're given pi to, uh, the new policy,

1213
01:09:07,859 --> 01:09:09,029
why can you not compute mu?

1214
01:09:09,029 --> 01:09:11,369
It's a good question because you don't want any data from that.

1215
01:09:11,370 --> 01:09:13,830
So someone has given you a new polic- [NOISE]

1216
01:09:13,830 --> 01:09:16,035
the- there could be ways to approximate [NOISE] this,

1217
01:09:16,035 --> 01:09:18,660
but the only data that you have right now is from [NOISE]

1218
01:09:18,660 --> 01:09:21,660
the current old policy, [NOISE] from Pi.

1219
01:09:21,660 --> 01:09:23,430
So you've run this out M times,

1220
01:09:23,430 --> 01:09:24,795
you've got M trajectories,

1221
01:09:24,795 --> 01:09:29,399
which are M trajectories gathered under your old policy pi, okay?

1222
01:09:29,399 --> 01:09:32,354
You don't have [NOISE] any data from pi tilde.

1223
01:09:32,354 --> 01:09:35,984
And in general, if pi tilde is not the same as pi,

1224
01:09:35,984 --> 01:09:37,979
you're going to get different trajectories.

1225
01:09:37,979 --> 01:09:41,174
So you don't have any direct estimate of this.

1226
01:09:41,175 --> 01:09:43,200
Does that makes sense to everybody?

1227
01:09:43,200 --> 01:09:46,500
So if we go back to [NOISE] the vanilla policy gradient, what do we do?

1228
01:09:46,500 --> 01:09:48,825
We had a policy pi_i,

1229
01:09:48,825 --> 01:09:50,069
we ran it out,

1230
01:09:50,069 --> 01:09:52,619
and we got D trajectories from that pi_i.

1231
01:09:52,620 --> 01:09:55,185
We could use that to estimate that mu.

1232
01:09:55,185 --> 01:09:58,340
That just gives us on policy data of what are

1233
01:09:58,340 --> 01:10:01,790
the states and actions we experience when we're [NOISE] following Pi_i.

1234
01:10:01,790 --> 01:10:04,205
We don't have any data of Pi_i + 1 yet,

1235
01:10:04,205 --> 01:10:06,875
we haven't run it. Okay. Yeah.

1236
01:10:06,875 --> 01:10:08,180
Uh, just to be clear,

1237
01:10:08,180 --> 01:10:12,060
to get an estimate of the stationary distribution for the old policy, [NOISE] uh,

1238
01:10:12,060 --> 01:10:16,020
you basically [NOISE] look at all the data that you have, uh, like,

1239
01:10:16,020 --> 01:10:18,600
all the trajectories, and see basically what fraction of

1240
01:10:18,600 --> 01:10:21,450
the time you're spending at one state. Okay.

1241
01:10:21,450 --> 01:10:23,910
Exactly. So [inaudible] is exactly right.

1242
01:10:23,910 --> 01:10:26,160
[NOISE] How would you go from this just raw data,

1243
01:10:26,160 --> 01:10:28,560
these te- D trajectories to mu?

1244
01:10:28,560 --> 01:10:30,165
You could just count, you know.

1245
01:10:30,165 --> 01:10:31,890
I mean, in general, if you're in really high dimensions,

1246
01:10:31,890 --> 01:10:33,420
you want to do something smoother than that,

1247
01:10:33,420 --> 01:10:35,085
you want to approximate the density function.

1248
01:10:35,085 --> 01:10:38,040
Um, but essentially, you can just directly,

1249
01:10:38,040 --> 01:10:39,060
i- in a type of so,

1250
01:10:39,060 --> 01:10:40,110
you could just count and [NOISE] just like,

1251
01:10:40,110 --> 01:10:42,510
how many times did I get to this state and take this action, and then,

1252
01:10:42,510 --> 01:10:45,210
that would give you a direct estimate of, um, the mu's.

1253
01:10:45,210 --> 01:10:47,070
[NOISE] In general, you're gonna want some sort of

1254
01:10:47,070 --> 01:10:49,064
parametric function in high dimensions,

1255
01:10:49,064 --> 01:10:51,345
but you couldn't fit that using, you could,

1256
01:10:51,345 --> 01:10:52,995
you could imagine this is parameterized itself,

1257
01:10:52,995 --> 01:10:56,235
and you can fit that using your existing on policy data.

1258
01:10:56,235 --> 01:10:59,460
Uh, intuitively, does this work because we

1259
01:10:59,460 --> 01:11:03,660
assume that distribution of the states [NOISE] won't change too much between policies?

1260
01:11:03,660 --> 01:11:06,030
Oh, yeah. The question is, intuitively, why does this work?

1261
01:11:06,030 --> 01:11:07,365
I've not told you why this works,

1262
01:11:07,365 --> 01:11:10,200
I've just said this is something we could do and that it's computable.

1263
01:11:10,200 --> 01:11:13,965
[NOISE] Um, and I haven't told you yet why this is a good thing to do.

1264
01:11:13,965 --> 01:11:16,095
But we're gonna show that, um,

1265
01:11:16,095 --> 01:11:19,695
that this is going to allow us to get to something which is a lower bound,

1266
01:11:19,695 --> 01:11:21,885
um, and then, we can improve on those lower bounds.

1267
01:11:21,885 --> 01:11:25,125
Okay. [NOISE] So just a quick thing to notice here,

1268
01:11:25,125 --> 01:11:27,195
which is, if you do this,

1269
01:11:27,195 --> 01:11:29,370
if you do L_pi of pi,

1270
01:11:29,370 --> 01:11:32,550
[NOISE] so that's just what is

1271
01:11:32,550 --> 01:11:36,135
this objective function if you plug in the old policy there,

1272
01:11:36,135 --> 01:11:42,585
this is just equal to [NOISE] V of the theta, [NOISE] okay?

1273
01:11:42,585 --> 01:11:48,014
So if you evaluate [NOISE] this function under the same policy,

1274
01:11:48,014 --> 01:11:52,560
it just gives you the value, okay? All right?

1275
01:11:52,560 --> 01:11:54,135
[NOISE] All right.

1276
01:11:54,135 --> 01:11:57,210
So conservative, [NOISE] I'll just briefly,

1277
01:11:57,210 --> 01:12:00,000
we'll have to continue this further later, but, um,

1278
01:12:00,000 --> 01:12:04,725
[NOISE] so we can use this to do what's known as conservative policy iteration.

1279
01:12:04,725 --> 01:12:11,500
So [NOISE] conservative policy iteration,

1280
01:12:11,570 --> 01:12:14,670
um, and the intuition here is,

1281
01:12:14,670 --> 01:12:16,110
let's just first just [NOISE] start with mixed,

1282
01:12:16,110 --> 01:12:17,790
um, a mixed policy.

1283
01:12:17,790 --> 01:12:19,860
[NOISE] So imagine that you have a new policy,

1284
01:12:19,860 --> 01:12:21,210
which is a mix of an,

1285
01:12:21,210 --> 01:12:25,020
of, um, an old policy and something different.

1286
01:12:25,020 --> 01:12:26,670
So you have 1 minus alpha times

1287
01:12:26,670 --> 01:12:35,730
your old policy plus alpha times some new policy pi prime, okay?

1288
01:12:35,730 --> 01:12:37,485
So that just means, you take some ol-

1289
01:12:37,485 --> 01:12:41,265
your current existing policy and you [NOISE] mix in something else, okay?

1290
01:12:41,265 --> 01:12:42,810
Then, in this case,

1291
01:12:42,810 --> 01:12:48,225
you can guarantee that the value [NOISE] of your new policy is greater than or equal to,

1292
01:12:48,225 --> 01:12:54,164
if you'd to take this objective function here and you evaluate it with your new policy,

1293
01:12:54,164 --> 01:12:56,550
so you take your new policy,

1294
01:12:56,550 --> 01:12:59,055
you evaluate under your old policy, you plug that in,

1295
01:12:59,055 --> 01:13:00,720
that's computable because you have data from

1296
01:13:00,720 --> 01:13:05,115
your old policy minus [NOISE] 2 epsilon gamma,

1297
01:13:05,115 --> 01:13:09,900
1 minus gamma squared alpha squared, okay?

1298
01:13:09,900 --> 01:13:12,810
So you can lower bound [NOISE] the value of

1299
01:13:12,810 --> 01:13:15,630
your new policy in terms of whatever this objective function

1300
01:13:15,630 --> 01:13:21,330
is when you compute it minus this expression, okay?

1301
01:13:21,330 --> 01:13:25,005
Um, I just wanna close with two other thoughts, which is,

1302
01:13:25,005 --> 01:13:30,990
note again that if you plug in alpha = 0,

1303
01:13:30,990 --> 01:13:37,800
[NOISE] that means that pi new is the same as pi old,

1304
01:13:37,800 --> 01:13:39,660
and this goes to 0,

1305
01:13:39,660 --> 01:13:42,225
[NOISE] which means that,

1306
01:13:42,225 --> 01:13:44,070
and since we know that this is equal to that,

1307
01:13:44,070 --> 01:13:48,330
that just says that your new policy has to be greater than or equal to your old policy.

1308
01:13:48,330 --> 01:13:52,300
And since the same, their policies are all the same, this is tight.

1309
01:13:52,460 --> 01:13:55,590
Okay. So we, um, this is a,

1310
01:13:55,590 --> 01:13:57,495
a little bit different than we expected because of,

1311
01:13:57,495 --> 01:13:59,700
um, [NOISE] the technical challenges with PDF.

1312
01:13:59,700 --> 01:14:03,960
Uh, so what I'll just close with here is that the next steps we'll go from this is to

1313
01:14:03,960 --> 01:14:08,745
show we can use this to essentially derive a lower bound on the new value function.

1314
01:14:08,745 --> 01:14:13,230
And we can show basically that if we improve across the lower bounds,

1315
01:14:13,230 --> 01:14:17,370
that we're guaranteed that the actual value function is monotonically improving.

1316
01:14:17,370 --> 01:14:20,295
[NOISE] So we will go through that.

1317
01:14:20,295 --> 01:14:22,260
Um, I haven't decided yet whether we'll go through

1318
01:14:22,260 --> 01:14:24,315
that on Monday because that's [NOISE] the midterm review

1319
01:14:24,315 --> 01:14:28,200
or if we'll wait on that until the following week after the midterm.

1320
01:14:28,200 --> 01:14:29,850
Um, the policy gradient,

1321
01:14:29,850 --> 01:14:32,430
uh, homework won't be released until after the midterm.

1322
01:14:32,430 --> 01:14:34,110
So we have a bit more time for that.

1323
01:14:34,110 --> 01:14:37,260
Um, and I'll go through also, like, the main takeaways

1324
01:14:37,260 --> 01:14:41,410
with policy gradient stuff when we conclude this part. Thanks.

