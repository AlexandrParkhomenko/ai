1
00:00:04,350 --> 00:00:06,990
All right, we're gonna go ahead and get started.

2
00:00:06,990 --> 00:00:10,290
Um, I want to start the class with some stuff about some logistics,

3
00:00:10,290 --> 00:00:12,840
um, as well as sort of to address some questions, um,

4
00:00:12,840 --> 00:00:16,400
that have come up on Piazza about the grades on the midterm and some people have

5
00:00:16,400 --> 00:00:20,185
concerns about what that might mean for their grades on the final class.

6
00:00:20,185 --> 00:00:23,170
Um, it was interesting to go back and compare

7
00:00:23,170 --> 00:00:26,990
the means and the distributions on the midterm last year, it's near identical.

8
00:00:26,990 --> 00:00:30,030
Um, so last year the mean was about 69%,

9
00:00:30,030 --> 00:00:32,025
this year it was about 71%.

10
00:00:32,025 --> 00:00:34,295
Um, and you see pretty similar distributions.

11
00:00:34,295 --> 00:00:38,270
Uh, the one on the top is our- Oh,

12
00:00:38,270 --> 00:00:39,450
no, the one on the bottom is ours.

13
00:00:39,450 --> 00:00:42,465
So you can see this is 2019,

14
00:00:42,465 --> 00:00:45,075
this is 2018. [NOISE] Okay.

15
00:00:45,075 --> 00:00:47,595
So they look pretty similar distributions.

16
00:00:47,595 --> 00:00:50,110
Um, we don't do an official curve for the class.

17
00:00:50,110 --> 00:00:52,445
If anybody's getting over 90%,

18
00:00:52,445 --> 00:00:55,020
I always consider, even if everybody gets over 90%,

19
00:00:55,020 --> 00:00:57,240
that that means that those people all

20
00:00:57,240 --> 00:00:59,270
understand the material well enough to deserve an A.

21
00:00:59,270 --> 00:01:01,860
Um, and then if we have really, um,

22
00:01:01,860 --> 00:01:04,724
abnormal distributions that sometimes we curve below that.

23
00:01:04,724 --> 00:01:06,770
But just to give you a sense, um, last year,

24
00:01:06,770 --> 00:01:09,200
about 42% of people got an A in the class.

25
00:01:09,200 --> 00:01:12,300
So for those of you that are concerned about your midterm performance and concerned about

26
00:01:12,300 --> 00:01:14,290
your final grade and whether it's still possible to do

27
00:01:14,290 --> 00:01:16,575
well in the class, it definitely is.

28
00:01:16,575 --> 00:01:19,410
So, so does anybody have any questions about the midterm?

29
00:01:19,410 --> 00:01:21,610
I know we've had some regrade- regrade requests

30
00:01:21,610 --> 00:01:24,175
and we're going through those as quickly as we can. Yeah.

31
00:01:24,175 --> 00:01:28,870
Is there any [inaudible] I'm supposed [NOISE] I'm very curious [inaudible]

32
00:01:28,870 --> 00:01:33,540
but is there any distribution available per, [NOISE] per question?

33
00:01:33,540 --> 00:01:35,550
Cause like- I feel like, for example, for me,

34
00:01:35,550 --> 00:01:38,940
I just ran out of time on the last question and I wonder if that's.

35
00:01:38,940 --> 00:01:40,560
Yes, we have that information.

36
00:01:40,560 --> 00:01:42,340
Um, I'll double-check with the TAs that there's

37
00:01:42,340 --> 00:01:44,230
no reason we shouldn't release that, I don't think there is.

38
00:01:44,230 --> 00:01:46,130
So, um, we, Gradescope gives us

39
00:01:46,130 --> 00:01:48,620
full distributions for all the questions. So we can release that.

40
00:01:48,620 --> 00:01:50,280
Um, a lot of people ran out of time,

41
00:01:50,280 --> 00:01:51,970
uh, the last problem was definitely the hardest.

42
00:01:51,970 --> 00:01:54,150
So that was where we saw

43
00:01:54,150 --> 00:01:55,750
the biggest variation and so that's where we

44
00:01:55,750 --> 00:01:57,580
tried to be particularly careful on that rubric.

45
00:01:57,580 --> 00:01:59,810
Um, and we very much tried to make sure that if you're

46
00:01:59,810 --> 00:02:01,960
doing algebraic mistakes throughout the exam,

47
00:02:01,960 --> 00:02:06,110
that that was worth very little and we were focusing on the conceptual understanding.

48
00:02:06,400 --> 00:02:08,900
Any other questions about the midterm?

49
00:02:08,900 --> 00:02:12,650
So I'll just write down per- per problem breakdown.

50
00:02:12,650 --> 00:02:14,010
Basically, when we're going through it,

51
00:02:14,010 --> 00:02:16,795
we try to look at any problem that had really high variance,

52
00:02:16,795 --> 00:02:20,730
um, and then step through the rubric again to make sure that we're being fair.

53
00:02:20,730 --> 00:02:23,560
Oh, one other thing I- which is we're gonna

54
00:02:23,560 --> 00:02:26,995
continue to accept regrade requests for the midterm through Friday.

55
00:02:26,995 --> 00:02:29,080
And after that, it will be closed.

56
00:02:29,080 --> 00:02:30,950
[NOISE] Okay.

57
00:02:30,950 --> 00:02:32,095
So that's the midterm.

58
00:02:32,095 --> 00:02:33,985
Um, hopefully, that helps, sort of,

59
00:02:33,985 --> 00:02:37,540
quell some concerns from at least some of the people in my class.

60
00:02:37,540 --> 00:02:41,300
The other thing that I wanted to bring up right now is, um, the quiz.

61
00:02:41,300 --> 00:02:44,330
So the quiz is gonna be in about two weeks,

62
00:02:44,330 --> 00:02:45,620
a little less than two weeks.

63
00:02:45,620 --> 00:02:47,025
It's a weird format.

64
00:02:47,025 --> 00:02:50,920
We do this for a reason. Um, I think one of the big tensions in classes that have

65
00:02:50,920 --> 00:02:55,410
big final projects is whether to do a big final project and a big final exam,

66
00:02:55,410 --> 00:02:59,065
uh, which I think is a lot of [LAUGHTER] a lot for students to do both on,

67
00:02:59,065 --> 00:03:02,300
um, and otherwise why go to class after the midterm?

68
00:03:02,300 --> 00:03:03,620
[LAUGHTER] I mean, now why, why,

69
00:03:03,620 --> 00:03:04,800
you know, how do we, uh,

70
00:03:04,800 --> 00:03:06,630
make sure that there's a reason

71
00:03:06,630 --> 00:03:08,610
to learn about the material in the second half of the course

72
00:03:08,610 --> 00:03:10,490
which we do think is valuable and particularly

73
00:03:10,490 --> 00:03:12,915
is often covering [NOISE] more important recent topics,

74
00:03:12,915 --> 00:03:16,085
um, but without doing a really high-stakes large exam.

75
00:03:16,085 --> 00:03:19,765
So I, I talked to a number of people in the sort of teaching,

76
00:03:19,765 --> 00:03:22,585
uh, the teaching center here called VPTL.

77
00:03:22,585 --> 00:03:27,265
Um, and the idea that we came up with is to do a low-stakes quiz,

78
00:03:27,265 --> 00:03:29,295
and the idea is that it's fun.

79
00:03:29,295 --> 00:03:31,420
Um, and I have heard from multiple people that this

80
00:03:31,420 --> 00:03:33,525
is actually true, that's the design of it.

81
00:03:33,525 --> 00:03:36,950
The design is it's gonna be a two-part quiz. It's multiple choice.

82
00:03:36,950 --> 00:03:39,950
It's all supposed to be about, sort of, high-level conceptual questions.

83
00:03:39,950 --> 00:03:41,510
We'll release last year's,

84
00:03:41,510 --> 00:03:43,640
so you guys can see an exa, uh, example.

85
00:03:43,640 --> 00:03:45,610
And so the idea is you do it in two parts.

86
00:03:45,610 --> 00:03:46,800
You first do it individually,

87
00:03:46,800 --> 00:03:48,385
that takes around 45 minutes,

88
00:03:48,385 --> 00:03:52,365
and then you'll be paired up with random groups and you will,

89
00:03:52,365 --> 00:03:54,925
um, have to do one joint quiz.

90
00:03:54,925 --> 00:03:59,920
And your grade will be composed both of your individual part and your group part,

91
00:03:59,920 --> 00:04:02,325
but you can only do better on your group part.

92
00:04:02,325 --> 00:04:04,790
So if your group does worse,

93
00:04:04,790 --> 00:04:07,415
then you're gonna just get the same as your individual grade.

94
00:04:07,415 --> 00:04:10,455
So the reason that we do this is that, um,

95
00:04:10,455 --> 00:04:13,315
for the group, then it's a scratch off exam.

96
00:04:13,315 --> 00:04:18,240
So you scratch off answers as you de- decide on them as a group,

97
00:04:18,240 --> 00:04:21,300
and the point is to- that you should be able to articulate why you

98
00:04:21,300 --> 00:04:25,000
believe some of these answers are true or false and convince your classmates,

99
00:04:25,000 --> 00:04:26,970
and in doing so, that can be a really useful way

100
00:04:26,970 --> 00:04:29,270
to think about really knowing the material well,

101
00:04:29,270 --> 00:04:32,475
um, and also hearing the perspectives of other people.

102
00:04:32,475 --> 00:04:35,810
So that's how the last quiz goes.

103
00:04:35,810 --> 00:04:38,790
Um, [NOISE] again last year everybody- there was

104
00:04:38,790 --> 00:04:42,015
some concern about it before on Piazza, people were concerned.

105
00:04:42,015 --> 00:04:45,135
There are certain game theoretic aspects that can come up.

106
00:04:45,135 --> 00:04:49,625
Um, we carefully design this so that it's a very small part of your grade.

107
00:04:49,625 --> 00:04:53,970
Um, again you could only do better with the group than you can on your individual,

108
00:04:53,970 --> 00:04:56,155
so it's carefully constructed.

109
00:04:56,155 --> 00:04:58,455
And empirically when we did this,

110
00:04:58,455 --> 00:05:02,290
there was lots of laughter and lots of people seem to really enjoy this aspect,

111
00:05:02,290 --> 00:05:05,520
and we've thought about whether we'll do it in multiple parts of the class.

112
00:05:05,520 --> 00:05:08,545
But it's different, almost nobody has ever done an exam like this.

113
00:05:08,545 --> 00:05:10,320
So does anybody have any questions about that?

114
00:05:10,320 --> 00:05:12,065
It's about 5% of your grade. Yeah.

115
00:05:12,065 --> 00:05:15,840
Uh, I remember you saying something earlier on in the course [NOISE] that you guys

116
00:05:15,840 --> 00:05:19,740
have already decided the teams or will have decided the teams, is that true?

117
00:05:19,740 --> 00:05:22,540
Yeah. So we haven't already decided that the question, um,

118
00:05:22,540 --> 00:05:25,230
was how are the teams assigned and have we already decided them,

119
00:05:25,230 --> 00:05:26,610
we'll do that by random assignment.

120
00:05:26,610 --> 00:05:30,910
It'll depend also on which SCPD students are taking the exam on campus or not, uh,

121
00:05:30,910 --> 00:05:32,790
but we'll release this a few days before, um,

122
00:05:32,790 --> 00:05:35,160
and you'll be randomly allocated to

123
00:05:35,160 --> 00:05:37,940
a team and then you'll just sit with that team for that part of the exam.

124
00:05:37,940 --> 00:05:41,970
[NOISE] Anybody have any other questions about that?

125
00:05:41,970 --> 00:05:44,100
And we'll- we'll release the, the sample one.

126
00:05:44,100 --> 00:05:46,015
It will cover all of the course.

127
00:05:46,015 --> 00:05:49,390
So it'll be more heavily weighted to stuff that's happened since the midterm,

128
00:05:49,390 --> 00:05:51,435
but anything from the whole course will be game,

129
00:05:51,435 --> 00:05:55,660
and the idea is that someone who has been attending lectures, um, or,

130
00:05:55,660 --> 00:05:57,155
or watching lectures online,

131
00:05:57,155 --> 00:05:59,150
um, should have to study for,

132
00:05:59,150 --> 00:06:02,750
you know, on the order of a few hours and then be pretty well prepared for the exam.

133
00:06:02,750 --> 00:06:04,560
You'll also be able to bring, uh, a cheat sheet,

134
00:06:04,560 --> 00:06:05,850
just like what you did for the midterm.

135
00:06:05,850 --> 00:06:10,870
[NOISE] Any other questions about that? Yeah.

136
00:06:11,230 --> 00:06:14,690
And I feel like you probably mentioned this,

137
00:06:14,690 --> 00:06:20,920
but I'm just missing which part of the quiz is individual versus with other people?

138
00:06:20,920 --> 00:06:25,965
Do we sit down individually and then after like 20 minutes go be with other people?

139
00:06:25,965 --> 00:06:27,860
Yes. So, um, that was a good one.

140
00:06:27,860 --> 00:06:29,570
So how does- what is this individual group thing?

141
00:06:29,570 --> 00:06:31,125
So the idea is that you come in,

142
00:06:31,125 --> 00:06:32,240
everybody gets an exam,

143
00:06:32,240 --> 00:06:35,450
you work on that exam, probably from around 45 minutes.

144
00:06:35,450 --> 00:06:37,450
Um, you hand it in when you're done,

145
00:06:37,450 --> 00:06:39,655
then when everybody- when that part of the,

146
00:06:39,655 --> 00:06:41,835
the class is done, most people finish early,

147
00:06:41,835 --> 00:06:45,155
it depends, um, then you as a group get a new exam.

148
00:06:45,155 --> 00:06:47,280
And you do exactly the same exam as before,

149
00:06:47,280 --> 00:06:50,100
but you just have to jointly agree on the answers.

150
00:06:51,020 --> 00:06:54,205
And you scratch it off so we can see how many- it's,

151
00:06:54,205 --> 00:06:57,115
uh- how many you have to scratch off until you got the right answer.

152
00:06:57,115 --> 00:06:58,880
But essentially, we can just see whether or not you got

153
00:06:58,880 --> 00:07:02,320
the answer on the first time or, or it took more than one.

154
00:07:03,560 --> 00:07:05,760
Any other questions about the quiz?

155
00:07:05,760 --> 00:07:07,490
If you have any concerns about that,

156
00:07:07,490 --> 00:07:09,420
just write, um, email us on Piazza.

157
00:07:09,420 --> 00:07:11,190
I'll just say briefly there- about what

158
00:07:11,190 --> 00:07:13,200
was the one of the concerns that came up last year.

159
00:07:13,200 --> 00:07:14,775
Um, the concern was game theory.

160
00:07:14,775 --> 00:07:16,830
So you always get the max of, uh,

161
00:07:16,830 --> 00:07:19,935
your score versus the, the group score.

162
00:07:19,935 --> 00:07:23,050
So what people said is well what you should do is you should

163
00:07:23,050 --> 00:07:26,510
answer the best you can on your individual part because that's worth the most credits,

164
00:07:26,510 --> 00:07:27,890
and then on the group part,

165
00:07:27,890 --> 00:07:29,920
you should- if you were torn between two,

166
00:07:29,920 --> 00:07:33,890
you should get people to agree on the second answer to hedge your bets.

167
00:07:33,890 --> 00:07:37,290
Um, [NOISE] you can do that if you want to [LAUGHTER] or try to,

168
00:07:37,290 --> 00:07:39,065
your or your group members can outweigh you.

169
00:07:39,065 --> 00:07:43,050
Again, the group part last year was about 0.5%, so it's very small.

170
00:07:43,050 --> 00:07:46,840
Um, so there, there is that possibility- you would only want to do

171
00:07:46,840 --> 00:07:50,620
that if you were genuinely really torn between two options, um, and again,

172
00:07:50,620 --> 00:07:53,260
there's only one right answer, so I,

173
00:07:53,260 --> 00:07:55,590
I think that we've observed in practice,

174
00:07:55,590 --> 00:07:58,510
there was very little need to do game theoretic analysis of this.

175
00:07:58,510 --> 00:08:00,375
[NOISE] But, no.

176
00:08:00,375 --> 00:08:04,160
Again, it's- we're always welcome to hear how people interpret these things.

177
00:08:04,160 --> 00:08:05,905
All right.

178
00:08:05,905 --> 00:08:08,270
Any other questions about the quiz or logistics right now?

179
00:08:08,270 --> 00:08:11,555
We also- most of you who have already turned in your, uh,

180
00:08:11,555 --> 00:08:13,740
m- milestone for the project,

181
00:08:13,740 --> 00:08:15,200
we'll be giving feedback on those over

182
00:08:15,200 --> 00:08:18,510
the next few days for those of you that are not doing the default.

183
00:08:18,690 --> 00:08:22,830
Okay. All right. So I put this up before.

184
00:08:22,830 --> 00:08:25,380
This is for everybody that didn't see it before,

185
00:08:25,380 --> 00:08:28,085
the grade distribution is basically identical.

186
00:08:28,085 --> 00:08:31,890
So today, we're gonna do the last part on fast learning.

187
00:08:31,890 --> 00:08:33,720
Um, this is, uh,

188
00:08:33,720 --> 00:08:34,929
a really big topic,

189
00:08:34,929 --> 00:08:36,244
there's tons of work on it.

190
00:08:36,245 --> 00:08:39,299
Um, well- we'll spend some more time on it today,

191
00:08:39,299 --> 00:08:40,679
and then on Monday, Chelsea Finn,

192
00:08:40,679 --> 00:08:43,039
who's, uh, just finished her P- PhD at Berkeley and

193
00:08:43,039 --> 00:08:45,510
she'll be joining the faculty here in the summer, um,

194
00:08:45,510 --> 00:08:47,020
will come and talk about meta-learning,

195
00:08:47,020 --> 00:08:48,520
which is also a really exciting area,

196
00:08:48,520 --> 00:08:51,495
and she'll be talking about meta-learning for reinforcement learning,

197
00:08:51,495 --> 00:08:53,220
where meta-learning is relevant to, sort of,

198
00:08:53,220 --> 00:08:55,080
multitask or transfer learning tasks.

199
00:08:55,080 --> 00:08:58,850
[NOISE] So just to

200
00:08:58,850 --> 00:09:02,720
go refresh our minds about what we're talking about in terms of this fast learning,

201
00:09:02,720 --> 00:09:04,730
we're thinking about cases where data matters.

202
00:09:04,730 --> 00:09:06,030
So things like healthcare,

203
00:09:06,030 --> 00:09:08,095
and education, and customers.

204
00:09:08,095 --> 00:09:11,430
Um, I was getting an invite to talk at Pinterest on Monday,

205
00:09:11,430 --> 00:09:14,305
they definitely care about these types of ideas as well.

206
00:09:14,305 --> 00:09:15,790
Um, and we've been talking about

207
00:09:15,790 --> 00:09:18,490
two different settings: Bandits and Markov decision processes,

208
00:09:18,490 --> 00:09:22,270
as well as frameworks for formerly understanding whether an algorithm is

209
00:09:22,270 --> 00:09:26,655
good or whether it is fast in terms of the amount of data it needs.

210
00:09:26,655 --> 00:09:29,020
And I'll note there that we haven't talked much about

211
00:09:29,020 --> 00:09:31,310
computational complexity for this part of the course,

212
00:09:31,310 --> 00:09:33,555
but there are similar, um,

213
00:09:33,555 --> 00:09:35,180
some of these frameworks can even easily be

214
00:09:35,180 --> 00:09:37,740
extended to talk about polynomial sample complexity.

215
00:09:37,740 --> 00:09:40,010
So often, you can extend these frameworks to also account

216
00:09:40,010 --> 00:09:43,170
for computational complexity requirements.

217
00:09:43,650 --> 00:09:47,205
Okay. So let's continue with Markov decision processes.

218
00:09:47,205 --> 00:09:49,210
What we started seeing last time is that we built up

219
00:09:49,210 --> 00:09:51,285
sort of this expertise on bandits so far,

220
00:09:51,285 --> 00:09:54,040
of thinking of a couple of the main ways we evaluate whether or not

221
00:09:54,040 --> 00:09:57,900
a bandit algorithm is good and approaches to try to achieve that.

222
00:09:57,900 --> 00:10:00,175
So we talked about mathematical regret,

223
00:10:00,175 --> 00:10:02,300
which was the difference between how well we could've

224
00:10:02,300 --> 00:10:06,120
acted and how well we did act in bandits,

225
00:10:06,120 --> 00:10:10,525
and, um, a lot of the work in bandits focuses on regret.

226
00:10:10,525 --> 00:10:14,855
We also talked about two different types of techniques for trying to achieve low regret,

227
00:10:14,855 --> 00:10:17,550
which was optimism under uncertainty,

228
00:10:17,550 --> 00:10:20,300
and then also Thompson sampling.

229
00:10:20,300 --> 00:10:23,180
So trying to be Bayesian and explicitly represent

230
00:10:23,180 --> 00:10:26,440
posterior over what you think might happen when you pull an arm,

231
00:10:26,440 --> 00:10:29,310
or take an action, and using that sort of information.

232
00:10:29,310 --> 00:10:32,715
[NOISE] Then last time we started talking about Markov decision processes

233
00:10:32,715 --> 00:10:36,350
where I argued that very similar ideas are important here,

234
00:10:36,350 --> 00:10:40,045
but- but the problem is a lot more challenging in many ways.

235
00:10:40,045 --> 00:10:45,150
I- and we were talking a little bit about probably approximately correct.

236
00:10:45,150 --> 00:10:47,620
So, in particular, we were talking, uh,

237
00:10:47,620 --> 00:10:50,415
a bit about model-based interval estimation,

238
00:10:50,415 --> 00:10:54,105
which I mentioned was a probably approximately correct algorithm.

239
00:10:54,105 --> 00:10:57,140
And so just to remind ourselves, what did PAC mean?

240
00:10:57,140 --> 00:11:00,910
[NOISE].

241
00:11:00,910 --> 00:11:04,065
And some of you have seen this probably in machine learning.

242
00:11:04,065 --> 00:11:09,140
So probably [NOISE] approximately correct

243
00:11:09,140 --> 00:11:18,070
[NOISE].

244
00:11:18,070 --> 00:11:23,910
Probably approximately correct RL algorithm is one that given an input, epsilon and delta.

245
00:11:24,040 --> 00:11:28,670
So epsilon is gonna specify sort of how good close to optimal we want to be,

246
00:11:28,670 --> 00:11:33,540
and delta's gonna specify with what probability we're gonna want this to occur.

247
00:11:33,540 --> 00:11:41,480
Um, with input epsilon and delta on all but N steps [NOISE].

248
00:11:42,990 --> 00:11:50,080
Our algorithm will select

249
00:11:50,080 --> 00:11:56,320
an action where there,

250
00:11:56,320 --> 00:11:57,820
the Q value of that action,

251
00:11:57,820 --> 00:12:04,749
the true optimal Q value is greater than or equal to,

252
00:12:04,749 --> 00:12:09,950
I'll write down it as V. The best possible you could have for that state,

253
00:12:10,500 --> 00:12:16,060
minus epsilon [NOISE] with

254
00:12:16,060 --> 00:12:22,930
probability at least 1 - delta, and throughout

255
00:12:22,930 --> 00:12:25,194
today I'm going to be a little bit loose about constants,

256
00:12:25,194 --> 00:12:27,250
sometimes this will be 1 - 2 delta,

257
00:12:27,250 --> 00:12:29,320
sometimes there might be a little constant in front of here,

258
00:12:29,320 --> 00:12:31,135
sometimes there might be a little constant in front of there.

259
00:12:31,135 --> 00:12:33,820
I'll put one here just so you can keep that in mind.

260
00:12:33,820 --> 00:12:35,500
There might be small constants there.

261
00:12:35,500 --> 00:12:37,765
Those are just, there might be two or four.

262
00:12:37,765 --> 00:12:40,060
Um, but the important thing is that,

263
00:12:40,060 --> 00:12:42,250
that you're close- very close to optimal,

264
00:12:42,250 --> 00:12:44,500
except for maybe a constant factor away, um,

265
00:12:44,500 --> 00:12:49,279
where N is a polynomial function

266
00:12:49,890 --> 00:12:55,135
of S size of your state space size of your action space gamma,

267
00:12:55,135 --> 00:12:57,520
um, epsilon, delta.

268
00:12:57,520 --> 00:13:00,400
Yeah.

269
00:13:00,400 --> 00:13:03,220
1 over epsilon [inaudible]

270
00:13:03,220 --> 00:13:05,575
1 over epsilon, yes great.

271
00:13:05,575 --> 00:13:07,990
Question was good is, um, are these going to depend on

272
00:13:07,990 --> 00:13:10,375
epsilon or delta or 1 over epsilon or 1 over delta?

273
00:13:10,375 --> 00:13:12,310
Yes, in- inside of all the expressions they'll

274
00:13:12,310 --> 00:13:14,965
end up being 1 over epsilon and 1 over delta.

275
00:13:14,965 --> 00:13:17,440
So you could equally write this as this.

276
00:13:17,440 --> 00:13:23,770
[NOISE] Because essentially N is going to be larger,

277
00:13:23,770 --> 00:13:25,570
if you want to be more accurate.

278
00:13:25,570 --> 00:13:28,090
So that's going to, um, as epsilon gets smaller,

279
00:13:28,090 --> 00:13:30,070
you're going to need more data to be more accurate.

280
00:13:30,070 --> 00:13:32,605
And if you want to be more sure, you're going to be accurate,

281
00:13:32,605 --> 00:13:35,875
you're going to also scale up with that delta.

282
00:13:35,875 --> 00:13:38,920
Okay, and I just want to before we kind of continue further.

283
00:13:38,920 --> 00:13:41,740
I'd like to briefly contrast this with regret

284
00:13:41,740 --> 00:13:45,565
because in the bandit setting we mostly think about regret.

285
00:13:45,565 --> 00:13:49,765
But it's nice to think about what the difference is between PAC and regret,

286
00:13:49,765 --> 00:13:51,355
particularly in online learning.

287
00:13:51,355 --> 00:13:55,390
Meaning like our algorithm's learning online in a MDP and it's learning forever.

288
00:13:55,390 --> 00:13:59,200
Um, which is what regret is telling you.

289
00:13:59,200 --> 00:14:03,070
So regret is saying [NOISE],

290
00:14:03,070 --> 00:14:05,215
Is that large enough in the back, can you see regret?

291
00:14:05,215 --> 00:14:09,770
Okay. So what regret is saying is let's say we start off at a state S_0.

292
00:14:09,770 --> 00:14:14,010
Regret is saying, what if you did the optimal thing from then on wards.

293
00:14:14,010 --> 00:14:16,440
Like, how great would your life have been.

294
00:14:16,440 --> 00:14:18,120
So if you had won that,

295
00:14:18,120 --> 00:14:19,904
you know, first coloring contest,

296
00:14:19,904 --> 00:14:21,390
and that set you up for Harvard,

297
00:14:21,390 --> 00:14:24,180
and set you up to the Supreme Court, like it's fabulous.

298
00:14:24,180 --> 00:14:26,265
But if instead you didn't enter the, that, um,

299
00:14:26,265 --> 00:14:29,400
coloring contest, and you got down here instead.

300
00:14:29,400 --> 00:14:30,990
So you could have had like a +10 there,

301
00:14:30,990 --> 00:14:32,385
but instead you had a 0.

302
00:14:32,385 --> 00:14:34,260
And you went to a different state, which, all right,

303
00:14:34,260 --> 00:14:37,119
you went to a different state in terms of MDP,

304
00:14:37,119 --> 00:14:40,360
where now you're not the person that won the coloring contest.

305
00:14:40,360 --> 00:14:46,060
And so then, you know, [NOISE] your life trajectory was irreversibly ruined.

306
00:14:46,060 --> 00:14:48,400
Um, in this case,

307
00:14:48,400 --> 00:14:52,945
[LAUGHTER] you are judged with respect to not just the,

308
00:14:52,945 --> 00:14:58,600
the actions but the state distribution you could've got to under the optimal policy.

309
00:14:58,600 --> 00:15:01,105
So you're always being judged with,

310
00:15:01,105 --> 00:15:05,320
what could I have reached if from the very beginning I always made optimal decisions?

311
00:15:05,320 --> 00:15:06,940
I always went into the coloring contest.

312
00:15:06,940 --> 00:15:08,170
I always went to Harvard.

313
00:15:08,170 --> 00:15:10,165
You know, I never went to that Stanford place and,

314
00:15:10,165 --> 00:15:12,460
and you got up to the Supreme Court versus,

315
00:15:12,460 --> 00:15:14,950
um, as soon as you make a different decision you

316
00:15:14,950 --> 00:15:17,545
might end up in a different states distribution.

317
00:15:17,545 --> 00:15:19,750
But you're going to look at these gaps.

318
00:15:19,750 --> 00:15:25,480
Okay. So you're going to be judged by the state distribution you ended up

319
00:15:25,480 --> 00:15:27,370
in and the rewards you got there versus

320
00:15:27,370 --> 00:15:30,985
the state distribution you'd get in under the optimal policy and rewards you get there.

321
00:15:30,985 --> 00:15:33,775
So regret in some ways is a pretty harsh criteria.

322
00:15:33,775 --> 00:15:36,490
Because it's saying like you always have to be judged for,

323
00:15:36,490 --> 00:15:39,085
like, if you'd made optimal decisions forever.

324
00:15:39,085 --> 00:15:42,505
PAC is much more reasonable in certain ways.

325
00:15:42,505 --> 00:15:48,850
PAC says, "I'm judging you under the state distribution you get to under your algorithm."

326
00:15:48,850 --> 00:15:50,935
So because it says,

327
00:15:50,935 --> 00:15:55,300
it will take an action that's close to optimal for the state that you're in.

328
00:15:55,300 --> 00:15:56,875
So what does PAC say?

329
00:15:56,875 --> 00:16:00,295
PAC says, okay you started off here.

330
00:16:00,295 --> 00:16:01,960
You didn't enter the coloring contest.

331
00:16:01,960 --> 00:16:03,790
You went to there. Okay, that's too bad.

332
00:16:03,790 --> 00:16:06,805
Um, given that you could've then,

333
00:16:06,805 --> 00:16:10,255
you know, I don't know, entered the next coloring contest, or you didn't.

334
00:16:10,255 --> 00:16:12,940
And I'm going to be judged by that local gap.

335
00:16:12,940 --> 00:16:15,700
I'm going to always only be judged by how optimal am

336
00:16:15,700 --> 00:16:20,485
I given the distribution of states I'm getting to under my algorithm.

337
00:16:20,485 --> 00:16:22,585
So PAC can give,

338
00:16:22,585 --> 00:16:23,965
have much smaller regret.

339
00:16:23,965 --> 00:16:26,345
I'm sorry, much smaller, sort of, um,

340
00:16:26,345 --> 00:16:30,420
negative, ah, differences compared to regret.

341
00:16:30,420 --> 00:16:32,820
Because imagine you have a really harsh MDP,

342
00:16:32,820 --> 00:16:35,715
and you have to make the first right move and then you go to some wonderful land.

343
00:16:35,715 --> 00:16:38,790
I'll see you always toil about in this horrible gridworld.

344
00:16:38,790 --> 00:16:41,725
Um, so in that case regret would compare you to,

345
00:16:41,725 --> 00:16:44,260
if you'd actually made the right first choice, whereas PAC would say,

346
00:16:44,260 --> 00:16:45,700
okay maybe made a bad first choice,

347
00:16:45,700 --> 00:16:47,785
but like you're making the best of things for where you're at,

348
00:16:47,785 --> 00:16:52,550
and you're kind of be near optimal given this bad state-space you've ended up in.

349
00:16:52,890 --> 00:16:56,380
So in some ways you can think of PAC is kind of making the

350
00:16:56,380 --> 00:16:59,260
most of the circumstances you've got yourself into [LAUGHTER],

351
00:16:59,260 --> 00:17:00,610
whereas regret is always judging you

352
00:17:00,610 --> 00:17:02,650
from if you'd make good decisions from the beginning.

353
00:17:02,650 --> 00:17:05,890
I saw a question back there. Yeah. And everyone please remind me of your names.

354
00:17:05,890 --> 00:17:08,020
I know, I'm trying hard, but I sometimes forget. Yeah.

355
00:17:08,020 --> 00:17:09,430
Episodic MDP's?

356
00:17:09,430 --> 00:17:12,790
Great questions. The question is does this extend to episodic MDPs?

357
00:17:12,790 --> 00:17:16,900
Um, so in episodic MDPs just to recall, um,

358
00:17:16,900 --> 00:17:19,450
those are MDPs where we act for h steps or,

359
00:17:19,450 --> 00:17:21,984
or a finite number of steps and then we reset.

360
00:17:21,984 --> 00:17:26,454
In episodic MDPs, regret and PAC are closer becau se normally,

361
00:17:26,454 --> 00:17:28,945
the PAC guarantees we get in that case.

362
00:17:28,945 --> 00:17:31,795
I'm not going to talk too much about those today but, um,

363
00:17:31,795 --> 00:17:35,830
are going to be with respect to the starting state.

364
00:17:35,830 --> 00:17:42,820
So you're going to look at, like V star of S0 versus Q star of S0,

365
00:17:42,820 --> 00:17:44,665
of like the actions you're taking,

366
00:17:44,665 --> 00:17:46,630
or the policy you're following.

367
00:17:46,630 --> 00:17:49,045
So in those cases they start to be closer,

368
00:17:49,045 --> 00:17:52,720
because you're always being judged from the starting state and you can reset.

369
00:17:52,720 --> 00:17:55,600
But in online, like continual learning,

370
00:17:55,600 --> 00:17:58,015
um, for reinforcement learning they can be quite different.

371
00:17:58,015 --> 00:18:00,145
Because the state distributions could be so different. Yeah.

372
00:18:00,145 --> 00:18:02,860
Could you just explain more about where C1

373
00:18:02,860 --> 00:18:05,665
and C2 come from because I don't see them as any like given parameters.

374
00:18:05,665 --> 00:18:06,955
Oh, yes. I just put up,

375
00:18:06,955 --> 00:18:08,275
question about C1 and C2.

376
00:18:08,275 --> 00:18:11,575
I just, I'm going to be very loose with constants today.

377
00:18:11,575 --> 00:18:15,850
Most of these type of regret guarantees are all about orders of magnitude.

378
00:18:15,850 --> 00:18:22,735
So it's stuff like is N a function of S to the 6 or is it a function of S to the 4.

379
00:18:22,735 --> 00:18:25,495
And we generally don't worry about constants too much.

380
00:18:25,495 --> 00:18:27,625
So I just put these in there to say,

381
00:18:27,625 --> 00:18:31,420
some of the different theoretical bounds will have different constants there.

382
00:18:31,420 --> 00:18:34,060
But for today we're just going to kind of ignore those

383
00:18:34,060 --> 00:18:36,880
but just so that you know that there might be constants there.

384
00:18:36,880 --> 00:18:38,620
This might be 1 - 2 delta,

385
00:18:38,620 --> 00:18:40,240
for example, instead of 1 - delta.

386
00:18:40,240 --> 00:18:47,680
[NOISE] Okay, right so that's one of the differences between regret and PAC.

387
00:18:47,680 --> 00:18:49,645
So let's go back to this algorithm now.

388
00:18:49,645 --> 00:18:51,490
Um, and I'll highlight,

389
00:18:51,490 --> 00:18:54,145
so we'll talk a little bit about generalization later today.

390
00:18:54,145 --> 00:18:57,175
But I, I wanted to go through sort of one of,

391
00:18:57,175 --> 00:19:00,025
how do we start to think about whether an algorithm is PAC or not.

392
00:19:00,025 --> 00:19:02,425
I told you that this algorithm is PAC.

393
00:19:02,425 --> 00:19:05,815
But I wanted to talk some about why it's PAC,

394
00:19:05,815 --> 00:19:07,780
and what it means for an algorithm to be PAC,

395
00:19:07,780 --> 00:19:12,835
and are there general sorts of templates that we can use to show an algorithm is PAC.

396
00:19:12,835 --> 00:19:15,055
All the stuff I'm going to talk about right now,

397
00:19:15,055 --> 00:19:18,040
involves tabular settings where we can write down,

398
00:19:18,040 --> 00:19:20,005
um, the value function as a table.

399
00:19:20,005 --> 00:19:22,915
And later we'll talk some about how these ideas extend.

400
00:19:22,915 --> 00:19:25,600
We're particularly picking this algorithm

401
00:19:25,600 --> 00:19:28,120
which is what's known as a reward bonus algorithm.

402
00:19:28,120 --> 00:19:31,255
So we have this nice little reward bonus here.

403
00:19:31,255 --> 00:19:34,870
Because it's going to be easier to extend to the model-free case.

404
00:19:34,870 --> 00:19:39,325
Now one thing I just want to highlight when we look at the MBIE-EB is that,

405
00:19:39,325 --> 00:19:41,440
if we go back and refresh our memories about this,

406
00:19:41,440 --> 00:19:45,085
what we are doing is we are computing the maximum likelihood estimate,

407
00:19:45,085 --> 00:19:47,695
or otherwise known as just adding up the counts and dividing,

408
00:19:47,695 --> 00:19:49,900
of the empirical estimate of the transition model

409
00:19:49,900 --> 00:19:52,390
and the reward model for every state-action pair.

410
00:19:52,390 --> 00:19:55,420
So we look at how many times we've been in a state-action pair,

411
00:19:55,420 --> 00:19:57,160
which next states we transition to,

412
00:19:57,160 --> 00:19:59,620
and we use that to construct an empirical model.

413
00:19:59,620 --> 00:20:02,005
And we do the same for the reward structure.

414
00:20:02,005 --> 00:20:04,210
And then we want to figure out how to act,

415
00:20:04,210 --> 00:20:06,190
we take those empirical models,

416
00:20:06,190 --> 00:20:08,650
and you can think of this,

417
00:20:08,650 --> 00:20:13,300
this operator here as if we're slightly changing our reward model.

418
00:20:13,300 --> 00:20:17,965
So I put it here as the empirical reward plus this bonus.

419
00:20:17,965 --> 00:20:22,420
But you can alternatively think of this as like an R hat prime which is

420
00:20:22,420 --> 00:20:26,970
equal to R hat of SA plus this bonus term.

421
00:20:26,970 --> 00:20:34,165
[NOISE] So you can think of this as like kind of defining a new MDP.

422
00:20:34,165 --> 00:20:37,080
There's a new MDP where the transition model is

423
00:20:37,080 --> 00:20:40,965
T hat and the reward model is R hat prime.

424
00:20:40,965 --> 00:20:44,435
Which is the empirical reward plus this bonus term.

425
00:20:44,435 --> 00:20:46,990
And it's- it's not a real MDP, but,

426
00:20:46,990 --> 00:20:50,530
but that's an MDP we could solve and try to compute the optimal value for and

427
00:20:50,530 --> 00:20:54,295
that's what we're doing here is we construct this sort of optimistic MDP,

428
00:20:54,295 --> 00:20:56,350
where we're using the empirical transition model.

429
00:20:56,350 --> 00:20:58,810
And then we use a reward model that has

430
00:20:58,810 --> 00:21:02,020
really large bonuses in places we haven't visited very much.

431
00:21:02,020 --> 00:21:05,790
[NOISE]

432
00:21:05,790 --> 00:21:08,880
All right. And- and a key thing we're gonna see shortly

433
00:21:08,880 --> 00:21:12,265
is the critical thing is how optimistic to be.

434
00:21:12,265 --> 00:21:17,730
Um, and there's been tons of work on- on trying to make things more or less optimistic.

435
00:21:17,730 --> 00:21:18,870
And if we have time,

436
00:21:18,870 --> 00:21:22,740
I'll show you some other slides about some recent progress in this field.

437
00:21:22,740 --> 00:21:27,120
Okay. So we talked before about this MBIE-EB PAC.

438
00:21:27,120 --> 00:21:30,090
And then now, let's talk a little bit about sort

439
00:21:30,090 --> 00:21:33,240
of what are the sufficient conditions to make something PAC,

440
00:21:33,240 --> 00:21:37,245
and then, how does MBIE-EB satisfy those form of conditions.

441
00:21:37,245 --> 00:21:43,830
So the conditions that I'm gonna talk about are derived basically from this paper, um,

442
00:21:43,830 --> 00:21:48,030
with slight modifications that paper does not- I'll just write down here,

443
00:21:48,030 --> 00:21:56,040
does not analyze [NOISE] MBIE-EB.

444
00:21:56,040 --> 00:21:59,235
So things would have to be a little bit different.

445
00:21:59,235 --> 00:22:02,010
But from a 30,000 foot perspective, this is basically,

446
00:22:02,010 --> 00:22:07,350
a reasonable way to think about why MBIE is a- MBIE-EB is a PAC algorithm.

447
00:22:07,350 --> 00:22:08,640
[NOISE] Okay.

448
00:22:08,640 --> 00:22:13,770
So let's unplug this

449
00:22:13,770 --> 00:22:20,400
or yeah, oops, I'll put these up on the board,

450
00:22:20,400 --> 00:22:23,100
because I think it's helpful to kind of see all of it at once.

451
00:22:23,100 --> 00:22:26,760
Okay. So what is a sufficient set of conditions to make something PAC?

452
00:22:26,760 --> 00:22:32,250
[NOISE] I know that

453
00:22:32,250 --> 00:22:36,285
I found this paper super helpful when I was starting to do PAC proofs in my PhD.

454
00:22:36,285 --> 00:22:46,030
So- so what's a sufficient [NOISE] conditions for PAC?

455
00:22:49,880 --> 00:22:51,930
And the theory is beautiful.

456
00:22:51,930 --> 00:22:54,150
But even for those of you that aren't interested in theory,

457
00:22:54,150 --> 00:22:56,160
I think looking at this is helpful because it gives one

458
00:22:56,160 --> 00:22:58,980
an intuition about what types of properties do your algorithms need to

459
00:22:58,980 --> 00:23:01,650
have in order to be efficient or wha- what types of

460
00:23:01,650 --> 00:23:05,160
properties are sufficient for your algorithm to be efficient?

461
00:23:05,160 --> 00:23:07,695
Okay. So the first one is optimism.

462
00:23:07,695 --> 00:23:10,530
Again, this is not the only set of conditions that are

463
00:23:10,530 --> 00:23:13,890
sufficient- that are sufficient for something to be PAC but here's a set.

464
00:23:13,890 --> 00:23:16,120
So here's optimism.

465
00:23:18,350 --> 00:23:24,110
Optimism. Okay and optimism simply says that,

466
00:23:24,110 --> 00:23:26,360
the computed value you use.

467
00:23:26,360 --> 00:23:30,710
Okay. So this is for this is s_t, this is a_t.

468
00:23:30,710 --> 00:23:34,085
So this is the actual value we compute

469
00:23:34,085 --> 00:23:37,190
like from MBIE-EB that optimistic value we compute.

470
00:23:37,190 --> 00:23:44,330
So this is the computed value of your algorithm [NOISE].

471
00:23:44,330 --> 00:23:48,590
Has to be greater than or equal to the true optimal value for that state-action pair

472
00:23:48,590 --> 00:23:53,540
[NOISE] minus epsilon on all time steps [NOISE].

473
00:23:53,540 --> 00:23:56,790
Okay. So it says that,

474
00:23:56,790 --> 00:23:59,265
whenever we are doing the MBIE-EB calculation,

475
00:23:59,265 --> 00:24:02,400
when we've taken our empirical models and we add in that reward bonus,

476
00:24:02,400 --> 00:24:05,520
we have to pick a reward bonus so that whatever we compute for

477
00:24:05,520 --> 00:24:10,890
the resulting state-action pair is optimistic minus some epsilon on all time steps.

478
00:24:10,890 --> 00:24:12,390
All of this is only gonna need to hold with

479
00:24:12,390 --> 00:24:14,640
high probability but I'll just write it out like this.

480
00:24:14,640 --> 00:24:16,185
So this is the first condition.

481
00:24:16,185 --> 00:24:19,710
The second condition is a little bit more [NOISE]

482
00:24:19,710 --> 00:24:24,525
subtle but I'll say more in a second about the specific cases for this.

483
00:24:24,525 --> 00:24:27,210
So the second thing is what's known as accuracy.

484
00:24:27,210 --> 00:24:29,700
And I'll write down the accuracy first.

485
00:24:29,700 --> 00:24:31,605
So the first thing says,

486
00:24:31,605 --> 00:24:35,055
you need to be optimistic on all time steps.

487
00:24:35,055 --> 00:24:37,590
The second thing is that, you need to be accurate.

488
00:24:37,590 --> 00:24:39,120
Which means the V_t.

489
00:24:39,120 --> 00:24:42,960
This is again, what the algorithm computes [NOISE].

490
00:24:42,960 --> 00:24:44,985
Your algorithm is gonna compute this.

491
00:24:44,985 --> 00:24:49,890
So it's what MBI computes using your optimistic model needs to

492
00:24:49,890 --> 00:24:56,040
be - V pi t of a weird MDP.

493
00:24:56,040 --> 00:24:59,295
And I'll tell you what that, about that weird MDP in a second.

494
00:24:59,295 --> 00:25:01,080
It is not the MDP

495
00:25:01,080 --> 00:25:03,345
I just said that is the nice optimistic MDP.

496
00:25:03,345 --> 00:25:05,040
It is not the real MDP.

497
00:25:05,040 --> 00:25:08,145
It is an MDP that's sort of in the middle of those.

498
00:25:08,145 --> 00:25:13,709
And this type of trick in RL comes up a lot where we sort of construct,

499
00:25:13,709 --> 00:25:15,360
you've seen probably in several proofs now,

500
00:25:15,360 --> 00:25:18,150
where we add and subtract the same term which is sort of

501
00:25:18,150 --> 00:25:21,675
halfway in between say, two different Markov decision processes.

502
00:25:21,675 --> 00:25:24,795
We're gonna play a similar trick here and we're gonna construct

503
00:25:24,795 --> 00:25:29,910
an MDP that is sort of half optimistic and is half like the real MDP.

504
00:25:29,910 --> 00:25:32,040
Again, this doesn't exist in

505
00:25:32,040 --> 00:25:35,485
the real world we're just gonna use it as a tool for our analysis.

506
00:25:35,485 --> 00:25:37,550
So I'll say what this is shortly.

507
00:25:37,550 --> 00:25:40,580
[NOISE] What there's different ways to define this but,

508
00:25:40,580 --> 00:25:41,750
um, it says that,

509
00:25:41,750 --> 00:25:43,430
something that's gonna be closely related to

510
00:25:43,430 --> 00:25:47,880
both your optimistic [NOISE] MDP and the true MDP [NOISE] that your value.

511
00:25:47,880 --> 00:25:51,180
So this is pi t is

512
00:25:51,180 --> 00:25:55,410
the policy you're actually executing at time step t. The- the value that you

513
00:25:55,410 --> 00:26:03,540
compute has to be close to this sort of weird hybrid MDP [NOISE] within epsilon.

514
00:26:03,540 --> 00:26:07,260
So it has to be pretty close to this other MDP.

515
00:26:07,260 --> 00:26:08,820
And the reason for this is that,

516
00:26:08,820 --> 00:26:11,100
this is we're gonna be able to use this to try to bound

517
00:26:11,100 --> 00:26:13,950
how far away we can be from the real MDP.

518
00:26:13,950 --> 00:26:15,510
So why are we gonna need this?

519
00:26:15,510 --> 00:26:18,750
We're gonna need this because optimism would be easy to

520
00:26:18,750 --> 00:26:22,755
hold by just setting our values super high and never updating.

521
00:26:22,755 --> 00:26:25,890
So that's fine but you need to be able to use the information

522
00:26:25,890 --> 00:26:28,800
you have so that eventually you're gonna be acting near-optimally.

523
00:26:28,800 --> 00:26:30,285
So if something really is bad,

524
00:26:30,285 --> 00:26:32,190
you don't want to be really optimistic forever.

525
00:26:32,190 --> 00:26:35,325
And so the accuracy condition is gonna say,

526
00:26:35,325 --> 00:26:37,500
if we've got sort of enough information about

527
00:26:37,500 --> 00:26:42,405
some state-action pairs or value for some of those needs to be fairly close to the,

528
00:26:42,405 --> 00:26:46,080
to a real value [NOISE].

529
00:26:46,080 --> 00:26:54,675
Okay. And then, the third thing [NOISE] is bounded learning complexity.

530
00:26:54,675 --> 00:26:55,080
Okay.

531
00:26:55,080 --> 00:27:08,790
[NOISE].

532
00:27:08,790 --> 00:27:10,020
And this has two parts.

533
00:27:10,020 --> 00:27:12,390
This says, the total number of updates,

534
00:27:12,390 --> 00:27:19,360
total number of queue updates.

535
00:27:20,840 --> 00:27:26,445
So in MBIE-EB, we would update our state-action values.

536
00:27:26,445 --> 00:27:31,230
And we would rerun that sort of optimistic Q value iteration.

537
00:27:31,230 --> 00:27:33,645
The total number of times we do that has to be,

538
00:27:33,645 --> 00:27:35,820
is gonna be bounded as is,

539
00:27:35,820 --> 00:27:42,540
the number of times [NOISE] we visit an unknown pair,

540
00:27:42,540 --> 00:27:48,270
state-action pair and I'll say more what that is in a second [NOISE].

541
00:27:48,270 --> 00:27:50,894
All right. So we're gonna classify

542
00:27:50,894 --> 00:27:52,920
all state-action pairs and we're in

543
00:27:52,920 --> 00:27:55,200
the tabular settings and this is reasonable for us to do,

544
00:27:55,200 --> 00:27:56,865
um, we're going to end up classifying

545
00:27:56,865 --> 00:28:00,060
every single state-action pair into being either known or unknown.

546
00:28:00,060 --> 00:28:02,460
And we're gonna say the total number of times we visit

547
00:28:02,460 --> 00:28:04,980
an unknown state-action pair both of these have to be

548
00:28:04,980 --> 00:28:10,830
bounded [NOISE] by some function.

549
00:28:10,830 --> 00:28:14,565
It is a function of epsilon and delta.

550
00:28:14,565 --> 00:28:19,320
So it means you can't do an infinite number of queue updates and you can't

551
00:28:19,320 --> 00:28:23,625
visit unknown state-action pairs an infinite number of times, like your algorithm can't.

552
00:28:23,625 --> 00:28:25,695
These are conditions on your algorithm.

553
00:28:25,695 --> 00:28:31,140
Okay. So if you can satisfy all of these, then your algorithm is PAC.

554
00:28:31,140 --> 00:28:41,130
So if 1 through 3 are satisfied [NOISE] then,

555
00:28:41,130 --> 00:28:49,395
it'll be epsilon order epsilon optimal [NOISE] on

556
00:28:49,395 --> 00:28:53,550
all but and I'll write this out here just so you can kind of get

557
00:28:53,550 --> 00:28:58,770
a sense of what these type of bounds can look like and all but N which is equal to order.

558
00:28:58,770 --> 00:29:04,260
This- this bound is sample complexity divided

559
00:29:04,260 --> 00:29:10,270
by epsilon times 1 - gamma squared times some log terms.

560
00:29:10,970 --> 00:29:13,755
So essentially, this is saying that if you can be

561
00:29:13,755 --> 00:29:17,130
optimistic accurate with respect to some weird MDP I haven't told you

562
00:29:17,130 --> 00:29:21,000
about and that if your total number of queue updates and the number of times you visit

563
00:29:21,000 --> 00:29:23,790
unknown state-action pairs which I often haven't told you about

564
00:29:23,790 --> 00:29:27,090
exactly how we define that, if that is bounded then,

565
00:29:27,090 --> 00:29:28,485
you're gonna be PAC.

566
00:29:28,485 --> 00:29:32,310
You're gonna be near optimal on all time steps except for a number that

567
00:29:32,310 --> 00:29:36,720
scales as a function of this which is also generally,

568
00:29:36,720 --> 00:29:39,705
a function of the size of the state space and action space.

569
00:29:39,705 --> 00:29:49,310
This is also function of say [NOISE] at epsilon of 1 - delta.

570
00:29:49,310 --> 00:29:51,050
So this is kind of a template.

571
00:29:51,050 --> 00:29:54,395
So if you can show that your algorithm satisfies these properties,

572
00:29:54,395 --> 00:29:56,790
then you can show that it's PAC.

573
00:29:59,360 --> 00:30:08,175
All right. So how does MBIE-EB satisfy these properties?

574
00:30:08,175 --> 00:30:14,370
Well, the first thing we need to show is that MBIE-EB is optimistic. Yeah question.

575
00:30:14,370 --> 00:30:18,690
Ah, so the first question was for three, part one and part two,

576
00:30:18,690 --> 00:30:20,265
do they both have the same bounds,

577
00:30:20,265 --> 00:30:24,180
or are there just two separate bounds with different magnitudes?

578
00:30:24,180 --> 00:30:25,965
Good question, he's asking about,

579
00:30:25,965 --> 00:30:27,285
do you mean the epsilon there?

580
00:30:27,285 --> 00:30:30,510
The total number of Q-updates and the total number of times you visit other states.

581
00:30:30,510 --> 00:30:32,580
Uh, good question, um, so for part three,

582
00:30:32,580 --> 00:30:36,175
as the total number of Q-updates the number of times you visit state-action pairs, um,

583
00:30:36,175 --> 00:30:39,750
they are going to be very closely-related essentially,

584
00:30:39,750 --> 00:30:42,180
whenever you visited another state-action pair,

585
00:30:42,180 --> 00:30:44,040
then you can do a Q-update.

586
00:30:44,040 --> 00:30:46,895
For one and two epsilons-

587
00:30:46,895 --> 00:30:47,980
are they the same?

588
00:30:47,980 --> 00:30:49,930
Yes, yeah. Good question.

589
00:30:49,930 --> 00:30:51,295
So that's what I thought you're asking.

590
00:30:51,295 --> 00:30:52,900
In one and two, um,

591
00:30:52,900 --> 00:30:54,490
the question was, are epsilon the same?

592
00:30:54,490 --> 00:30:57,760
Yes, they're the same. Epsilon is same through 1, 2 and 3.

593
00:30:57,760 --> 00:30:59,245
So if you're designing your algorithm,

594
00:30:59,245 --> 00:31:02,095
1 and 2 and 3 all have to be the same.

595
00:31:02,095 --> 00:31:03,835
Constants probably don't matter.

596
00:31:03,835 --> 00:31:05,920
It can, you know, be 1 minus.

597
00:31:05,920 --> 00:31:09,820
In some of these cases you can be- have a constant in front of the epsilon.

598
00:31:09,820 --> 00:31:10,960
One just has to be a bit careful.

599
00:31:10,960 --> 00:31:13,435
So for here, we'll just put them like that,

600
00:31:13,435 --> 00:31:15,460
okay? Same epsilon everywhere.

601
00:31:15,460 --> 00:31:21,505
Okay. So let's talk first about why MBIE-EB is optimistic.

602
00:31:21,505 --> 00:31:26,110
Um, let's- actually, can we put this up please?

603
00:31:26,110 --> 00:31:27,745
I think that'll be better.

604
00:31:27,745 --> 00:31:30,790
So I'm gonna just reput up MBIE's,

605
00:31:30,790 --> 00:31:34,540
um, bonus term so that you can, uh, see what that looks like.

606
00:31:34,540 --> 00:31:37,480
Okay. So I think this is gonna go up in just a second,

607
00:31:37,480 --> 00:31:43,690
and then just to remind us what the update was for [inaudible].

608
00:31:43,690 --> 00:31:48,040
So when- what we were doing in MBIE-EB is,

609
00:31:48,040 --> 00:31:49,405
we had a state-action pair.

610
00:31:49,405 --> 00:31:53,420
We had our empirical reward for that state-action pair,

611
00:31:53,700 --> 00:31:59,680
plus our bonus, beta divided by square root of n(s,a).

612
00:31:59,680 --> 00:32:02,635
Is that too small in the back, is that okay?

613
00:32:02,635 --> 00:32:04,390
It's okay? Okay, great.

614
00:32:04,390 --> 00:32:06,115
Um, I see at least one person nodding.

615
00:32:06,115 --> 00:32:09,625
So this is our sort of optimistic reward.

616
00:32:09,625 --> 00:32:11,125
You can call this, like R tilde.

617
00:32:11,125 --> 00:32:13,720
This is our optimistic reward.

618
00:32:13,720 --> 00:32:17,605
Plus sum over s'.

619
00:32:17,605 --> 00:32:20,170
This is our, again, empirical transition model.

620
00:32:20,170 --> 00:32:28,045
S' given as a max over a' of Q tilde of s',a'.

621
00:32:28,045 --> 00:32:33,640
Okay. So this would be a backup we could do.

622
00:32:33,640 --> 00:32:35,515
So it's like a Bellman backup,

623
00:32:35,515 --> 00:32:37,990
with our optimistic reward bonus.

624
00:32:37,990 --> 00:32:40,734
And just to remind ourselves,

625
00:32:40,734 --> 00:32:44,320
beta was still gonna be defined as 1 over 1 - gamma,

626
00:32:44,320 --> 00:32:50,650
square root of one half log 2 S,

627
00:32:50,650 --> 00:32:54,080
A, M divided by delta.

628
00:32:55,500 --> 00:33:00,010
All right. So in this case,

629
00:33:00,010 --> 00:33:01,600
what we wanna be able to show, we don't have to think

630
00:33:01,600 --> 00:33:03,865
about known or unknown state-action pairs yet.

631
00:33:03,865 --> 00:33:05,860
We want to show that this value,

632
00:33:05,860 --> 00:33:07,045
when we compute it,

633
00:33:07,045 --> 00:33:11,095
is an upper bound to the true Q star, up to epsilon.

634
00:33:11,095 --> 00:33:13,870
So we wanna be able to show this first optimism condition.

635
00:33:13,870 --> 00:33:16,000
We want to- what we're trying to argue right now is that,

636
00:33:16,000 --> 00:33:19,495
that beta is sufficiently large as a bonus,

637
00:33:19,495 --> 00:33:23,335
that when we do this procedure we're gonna be optimistic.

638
00:33:23,335 --> 00:33:30,175
Okay. So let's step through it here.

639
00:33:30,175 --> 00:33:34,870
Okay. So how do we show that?

640
00:33:34,870 --> 00:33:38,335
Let's think about a particular state-action pair.

641
00:33:38,335 --> 00:33:40,615
So let's think about one state-action pair.

642
00:33:40,615 --> 00:33:51,085
So s,a and let's think about that we visited some n(s,a) times which is less than m, okay?

643
00:33:51,085 --> 00:33:53,875
So in our algorithm,

644
00:33:53,875 --> 00:33:59,215
we are only gonna update our empirical estimates until we have m samples.

645
00:33:59,215 --> 00:34:07,930
So [NOISE] we only use the first m samples

646
00:34:07,930 --> 00:34:15,530
of s,a to compute

647
00:34:15,600 --> 00:34:19,614
R hat and T, okay?

648
00:34:19,614 --> 00:34:22,519
After that we're gonna throw away our data.

649
00:34:22,739 --> 00:34:25,494
So this is like saying the first, um,

650
00:34:25,495 --> 00:34:28,030
m times you visit this particular state-action pair, um,

651
00:34:28,030 --> 00:34:30,880
you can use that data to try to compute an empirical model,

652
00:34:30,880 --> 00:34:34,090
and use it to compute an empirical model before you have m counts,

653
00:34:34,090 --> 00:34:36,940
but after that you're never gonna update.

654
00:34:36,940 --> 00:34:40,449
I'll- I'll just put a side note in there which is, um,

655
00:34:40,449 --> 00:34:43,449
[NOISE] you might think why [LAUGHTER] why should we do this?

656
00:34:43,449 --> 00:34:44,919
And in particular, uh,

657
00:34:44,920 --> 00:34:47,710
there's a really lovely description of

658
00:34:47,710 --> 00:34:50,560
the whole field of machine learning by Tom Mitchell who's one of the- really,

659
00:34:50,560 --> 00:34:52,120
the founders of machine learning where he argues

660
00:34:52,120 --> 00:34:53,800
the whole discipline in machine learning.

661
00:34:53,800 --> 00:34:55,614
The point is to look at, um,

662
00:34:55,614 --> 00:34:58,510
the foundations of how an agent can learn and also

663
00:34:58,510 --> 00:35:02,230
that we design algorithms that continue to improve with more data.

664
00:35:02,230 --> 00:35:04,840
And this is violating that to some extent,

665
00:35:04,840 --> 00:35:06,460
because this is saying that even if you get

666
00:35:06,460 --> 00:35:08,860
10 trillion examples of that state-action pair,

667
00:35:08,860 --> 00:35:11,185
which surely would make your empirical model better,

668
00:35:11,185 --> 00:35:12,460
we're gonna throw all that data out.

669
00:35:12,460 --> 00:35:17,140
[NOISE] Just to give you a sense of why we do that or why this earlier analysis did that,

670
00:35:17,140 --> 00:35:20,395
we do that for the high probability bound.

671
00:35:20,395 --> 00:35:22,615
The idea is that, um,

672
00:35:22,615 --> 00:35:24,490
the high probability bound is gonna work,

673
00:35:24,490 --> 00:35:26,380
like what we saw for bandits of making

674
00:35:26,380 --> 00:35:30,249
sort of upper confidence bounds and kind of guaranteeing that our estimates,

675
00:35:30,249 --> 00:35:33,595
say if the transition model are close to the true values.

676
00:35:33,595 --> 00:35:36,700
And those bounds all hold with high probability.

677
00:35:36,700 --> 00:35:41,275
And so, who here has seen union bounds in different things?

678
00:35:41,275 --> 00:35:43,240
Okay. A few people, but most people have not.

679
00:35:43,240 --> 00:35:46,615
So union bounds are a way to make sure that if you have a number of different events,

680
00:35:46,615 --> 00:35:48,535
all of which hold with high probability,

681
00:35:48,535 --> 00:35:52,090
that the total of those events all hold with high probability.

682
00:35:52,090 --> 00:35:57,085
And that is essentially why here we only use a finite amount of data.

683
00:35:57,085 --> 00:36:00,175
Intellectually, this is completely unsatisfying [LAUGHTER] um,

684
00:36:00,175 --> 00:36:02,230
because you should clearly be able to use more data and

685
00:36:02,230 --> 00:36:04,705
your algorithm should do better with using more data,

686
00:36:04,705 --> 00:36:07,045
and empirically, we use all the data.

687
00:36:07,045 --> 00:36:10,570
Um, one thing that I find really satisfying for last few years is that, uh,

688
00:36:10,570 --> 00:36:13,090
with my student and Tor Lattimore who's

689
00:36:13,090 --> 00:36:15,865
over- who's one of the authors of the bandit book that we recommend,

690
00:36:15,865 --> 00:36:18,760
um, we showed that you can remove this restriction.

691
00:36:18,760 --> 00:36:21,235
You couldn't just continue to use data forever,

692
00:36:21,235 --> 00:36:23,680
by using smarter things than union bounds.

693
00:36:23,680 --> 00:36:26,230
But regardless, for today, we're gonna- we'll do this.

694
00:36:26,230 --> 00:36:28,015
So we're gonna say- we're only gonna,

695
00:36:28,015 --> 00:36:29,800
um, use up to m samples.

696
00:36:29,800 --> 00:36:33,730
Now, let's think about cases where n(s,a) is less than or equal to m. Okay.

697
00:36:33,730 --> 00:36:35,290
So we have up to m samples,

698
00:36:35,290 --> 00:36:36,460
but in general, you know,

699
00:36:36,460 --> 00:36:37,720
it may be one, it might be two.

700
00:36:37,720 --> 00:36:39,715
Some number that is smaller than m,

701
00:36:39,715 --> 00:36:43,510
which is some constant that we have not specified yet, okay?

702
00:36:43,510 --> 00:36:46,420
So what we're gonna look at is for this state-action pair.

703
00:36:46,420 --> 00:36:50,860
We're gonna look at all of the experiences for that state-action pair.

704
00:36:50,860 --> 00:36:52,840
So let's call X_i,

705
00:36:52,840 --> 00:36:56,920
to be defined to be r_i + gamma,

706
00:36:56,920 --> 00:37:00,055
V star of s_i.

707
00:37:00,055 --> 00:37:04,240
This should look quite like what- these were the targets that we had in TD learning.

708
00:37:04,240 --> 00:37:07,900
Um, this is saying that the reward we got on the ith time,

709
00:37:07,900 --> 00:37:09,595
we sampled s and a,

710
00:37:09,595 --> 00:37:14,140
and the next state we got 2 on the ith time we sampled s and a.

711
00:37:14,140 --> 00:37:20,155
So this is from ith visit to s,a.

712
00:37:20,155 --> 00:37:22,150
This is the next state.

713
00:37:22,150 --> 00:37:27,730
Next state [NOISE] Okay.

714
00:37:27,730 --> 00:37:29,095
So we can define this.

715
00:37:29,095 --> 00:37:34,330
So we can think of each of these are gonna have an expectation of

716
00:37:34,330 --> 00:37:43,225
the true Q star of s,a, okay?

717
00:37:43,225 --> 00:37:45,700
Because I've just defined this.

718
00:37:45,700 --> 00:37:47,305
We don't have to know what V star is right now.

719
00:37:47,305 --> 00:37:49,645
We're just analyzing what would happen with these samples.

720
00:37:49,645 --> 00:37:52,045
So if we define our samples to be r,

721
00:37:52,045 --> 00:37:53,440
the real reward we saw,

722
00:37:53,440 --> 00:37:54,895
the real next state we saw.

723
00:37:54,895 --> 00:37:59,120
On average, this is really just Q star of s,a.

724
00:37:59,370 --> 00:38:04,075
All right. So if this is Q star of s,a,

725
00:38:04,075 --> 00:38:07,000
we can think about how many samples do we need until we

726
00:38:07,000 --> 00:38:09,535
have a good approximation of Q star s,a,

727
00:38:09,535 --> 00:38:12,385
or how far away can our average B,

728
00:38:12,385 --> 00:38:17,080
over the averaged- the real empirical average of X_is versus Q star

729
00:38:17,080 --> 00:38:21,790
[NOISE] And we can do this using Hoeffding or other sorts of deviation bounds.

730
00:38:21,790 --> 00:38:23,950
A little bit like what we saw of our bandits.

731
00:38:23,950 --> 00:38:27,055
So for bandits, we looked at if you have a distribution over rewards,

732
00:38:27,055 --> 00:38:29,500
if you have a finite number of samples of that,

733
00:38:29,500 --> 00:38:32,170
how far can it be away from the true mean reward?

734
00:38:32,170 --> 00:38:34,540
And similarly, here we're gonna say if you have a finite number of

735
00:38:34,540 --> 00:38:37,570
samples of the next state and the reward received,

736
00:38:37,570 --> 00:38:41,125
how far away can we be from the true Q star in this case?

737
00:38:41,125 --> 00:38:46,090
Right. So there's some technical details

738
00:38:46,090 --> 00:38:48,565
here because one has to be a little bit careful about,

739
00:38:48,565 --> 00:38:53,620
um, the fact that the data that we gather depends on the history.

740
00:38:53,620 --> 00:38:55,120
So this is, again,

741
00:38:55,120 --> 00:38:57,715
one of the ch- more challenging things in, um,

742
00:38:57,715 --> 00:39:00,145
Markov decision processes in this sense,

743
00:39:00,145 --> 00:39:02,230
or particularly Markov decision processes in that

744
00:39:02,230 --> 00:39:04,465
the data you gathered depends on your algorithm.

745
00:39:04,465 --> 00:39:08,635
So you're gonna get more samples for state-action pairs that you think are gonna be good,

746
00:39:08,635 --> 00:39:11,650
and less samples for state-action pairs that you think are gonna be bad.

747
00:39:11,650 --> 00:39:13,690
So there's coupling.

748
00:39:13,690 --> 00:39:16,495
The data isn't really IID, um,

749
00:39:16,495 --> 00:39:18,175
across the whole distribution,

750
00:39:18,175 --> 00:39:22,180
but sometimes conditioned on the fact that we're sampling for the state-action pair.

751
00:39:22,180 --> 00:39:25,840
The next state and reward are IID because it's Markov.

752
00:39:25,840 --> 00:39:28,510
[NOISE] So just to

753
00:39:28,510 --> 00:39:31,270
give some of- that's just to say we have to be a little bit careful here,

754
00:39:31,270 --> 00:39:37,940
but we can basically do the pro- use things like Hoeffding to say the probability that,

755
00:39:39,120 --> 00:39:43,075
Q star of s,a,

756
00:39:43,075 --> 00:39:47,200
- 1 over n(s,a),

757
00:39:47,200 --> 00:39:52,735
sum over i = 1 to n(s,a) of X_i,

758
00:39:52,735 --> 00:39:54,820
where X_i is just what we defined up there.

759
00:39:54,820 --> 00:40:00,080
The probability that's greater than or equal to beta over square root of n,s,a,

760
00:40:02,270 --> 00:40:07,485
is gonna be less than or equal to the exponential minus 2 beta squared,

761
00:40:07,485 --> 00:40:10,320
1 minus gamma squared, okay?

762
00:40:10,320 --> 00:40:16,360
This is using like Hoeffding or like a similar type of deviation inequality.

763
00:40:16,360 --> 00:40:18,624
You can also use ones that depend on martingales,

764
00:40:18,624 --> 00:40:21,280
for those of you that have seen some of those before.

765
00:40:21,280 --> 00:40:23,530
Regardless, this basically just allows us to say,

766
00:40:23,530 --> 00:40:26,050
as you get more and more samples for this particular state-action pair,

767
00:40:26,050 --> 00:40:29,410
how far away could you be from optimal if you did know B star?

768
00:40:29,410 --> 00:40:31,840
[NOISE] Now we know what beta is.

769
00:40:31,840 --> 00:40:34,285
I put it up there. So if we plug beta into here.

770
00:40:34,285 --> 00:40:38,890
So if you plug beta in [NOISE] the real value for beta in,

771
00:40:38,890 --> 00:40:46,270
you get that this is gonna be equal to delta divided by 2 size of the state space,

772
00:40:46,270 --> 00:40:49,480
size of the action space, m, okay?

773
00:40:49,480 --> 00:40:52,870
So it just says that this holds with high probability.

774
00:40:52,870 --> 00:40:56,785
That the number- as you have more samples, um,

775
00:40:56,785 --> 00:41:02,900
the probability that you're gonna be far away from the true Q star is small, okay?

776
00:41:02,900 --> 00:41:06,340
All right. So we can put that, like,

777
00:41:06,340 --> 00:41:08,395
substitute this result back in,

778
00:41:08,395 --> 00:41:11,950
and we can say what that therefore means is that,

779
00:41:11,950 --> 00:41:16,060
if we look at the union bound across all of this,

780
00:41:16,060 --> 00:41:19,210
um, well here, let me just write

781
00:41:19,210 --> 00:41:22,270
down one more thing which is what does this X_i actually look like.

782
00:41:22,270 --> 00:41:32,940
So X_i, if you say 1 over n(s,a) of sum over i = 1 to n(s,a) of X_i.

783
00:41:32,940 --> 00:41:37,275
What is that? That's actually just the equation that we had up there.

784
00:41:37,275 --> 00:41:43,404
Okay. So it's very similar to- it's your empirical reward,

785
00:41:43,404 --> 00:41:50,265
plus gamma times T Hat s' s,a.

786
00:41:50,265 --> 00:41:55,640
Almost that equation except for you've got B star here, okay?

787
00:41:55,890 --> 00:41:59,650
So this should look really similar to that Q tilde up there.

788
00:41:59,650 --> 00:42:03,280
We're using the empirical rewards here in the emp- empirical transition.

789
00:42:03,280 --> 00:42:08,630
The difference is that here we're using Q star and up there we're using Q tilde.

790
00:42:11,820 --> 00:42:18,905
All right. So what this means is that if you have a number of samples,

791
00:42:18,905 --> 00:42:21,500
then you can bound the difference

792
00:42:21,500 --> 00:42:25,810
between the thing that we're doing up the- the- this thing and the Q_star.

793
00:42:25,810 --> 00:42:28,970
So let's do R_hat of s,

794
00:42:28,970 --> 00:42:33,740
a plus gamma sum over s_prime transition,

795
00:42:33,740 --> 00:42:35,635
the empirical transition model,

796
00:42:35,635 --> 00:42:39,825
s, a, V_star of s prime,

797
00:42:39,825 --> 00:42:48,800
minus Q_star s, a is greater than or equal to minus beta divided by square root n(s, a).

798
00:42:48,800 --> 00:42:52,980
Okay? And this is gonna hold for all T,

799
00:42:52,980 --> 00:42:57,560
s, and a. All right.

800
00:42:57,560 --> 00:42:59,800
So we've used Hoeffding and now we can relate,

801
00:42:59,800 --> 00:43:01,655
um, the empirical reward,

802
00:43:01,655 --> 00:43:02,910
the empirical transition model,

803
00:43:02,910 --> 00:43:07,190
and if someone gave us the optimal Q to what Q_star is.

804
00:43:07,900 --> 00:43:13,395
And then, now, what we wanna do is compare this to what we're- that equation up here.

805
00:43:13,395 --> 00:43:18,420
So I'll just- all right, this is one.

806
00:43:18,420 --> 00:43:21,700
So that equation one up there is what we're actually doing in MBIE-EB.

807
00:43:21,700 --> 00:43:25,375
We keep doing that over and over again until it convergences.

808
00:43:25,375 --> 00:43:27,670
So we take our empirical transition model,

809
00:43:27,670 --> 00:43:29,420
we take our empirical reward model,

810
00:43:29,420 --> 00:43:30,955
we add in this bonus,

811
00:43:30,955 --> 00:43:33,270
we do value iteration until we converge.

812
00:43:33,270 --> 00:43:38,225
And what we would like to do now is to compare what happens to that quantity,

813
00:43:38,225 --> 00:43:40,570
versus what is Q_star.

814
00:43:40,570 --> 00:43:43,930
And we want to show that that quantity up there is

815
00:43:43,930 --> 00:43:47,090
going to be greater than or equal to Q_star, okay?

816
00:43:47,090 --> 00:43:50,075
So we're gonna do that by induction.

817
00:43:50,075 --> 00:43:54,060
I think I'm gonna- [NOISE].

818
00:43:54,060 --> 00:43:55,970
Okay.

819
00:44:06,810 --> 00:44:09,735
So the proof is by induction.

820
00:44:09,735 --> 00:44:13,175
So what we're gonna do is we're gonna get Q_tilde,

821
00:44:13,175 --> 00:44:18,490
i of s, a be the i-th iteration of value iteration.

822
00:44:18,490 --> 00:44:20,280
So this is using that equation 1.

823
00:44:20,280 --> 00:44:24,170
So equation 1 up there.

824
00:44:24,650 --> 00:44:32,880
And we're gonna let V_tilde_i equal the- for a value s just to be,

825
00:44:32,880 --> 00:44:37,340
if we were to take the max action of that Q_tilde.

826
00:44:37,340 --> 00:44:40,325
Okay? For every state and action pair.

827
00:44:40,325 --> 00:44:43,910
And what we're going to assume is that we initialize optimistically,

828
00:44:43,910 --> 00:44:49,645
and we're gonna initialize Q_tilde of 0 for s,

829
00:44:49,645 --> 00:44:53,290
a equal to 1 over 1 minus gamma,

830
00:44:53,290 --> 00:44:56,450
which by definition is greater than Q_star,

831
00:44:56,450 --> 00:44:58,475
is at least as good as Q_star.

832
00:44:58,475 --> 00:45:00,575
So that's our base case.

833
00:45:00,575 --> 00:45:06,280
And again, what is the- what are we trying to do?

834
00:45:06,280 --> 00:45:08,540
We're trying to show optimism here for MBIE-EB.

835
00:45:08,540 --> 00:45:12,545
We're trying to say that if we do this procedure with MBIE-EB, we'll be optimistic.

836
00:45:12,545 --> 00:45:15,150
We're going to do a proof by induction and this is the base case.

837
00:45:15,150 --> 00:45:18,560
So we start off, we initialize our Q_tilde optimistically,

838
00:45:18,560 --> 00:45:20,745
and now we're gonna, um,

839
00:45:20,745 --> 00:45:22,385
assume that this holds.

840
00:45:22,385 --> 00:45:29,965
So we're gonna assume Q_tilde_i of s,

841
00:45:29,965 --> 00:45:34,340
a is gonna- we're gonna assume that Q_tilde_i of s,

842
00:45:34,340 --> 00:45:37,620
a is greater than equal to Q_star,

843
00:45:37,620 --> 00:45:41,340
of s, a for the previous time-step.

844
00:45:41,340 --> 00:45:46,210
Okay? So we're going to- All right.

845
00:45:46,210 --> 00:45:49,765
So let's write out what Q_i + 1 is going to be.

846
00:45:49,765 --> 00:45:54,370
Q_tilde_i + 1 is going to be equal to for s,

847
00:45:54,370 --> 00:45:57,985
a, is going to be equal to our empirical reward,

848
00:45:57,985 --> 00:46:02,025
plus gamma sum over s_prime,

849
00:46:02,025 --> 00:46:05,080
or empirical transition model,

850
00:46:05,120 --> 00:46:11,800
times our V_tilde of i,

851
00:46:11,800 --> 00:46:17,180
of s_prime, + beta over square root n(s, a).

852
00:46:17,180 --> 00:46:22,475
Okay. That's just the same as equation 1.

853
00:46:22,475 --> 00:46:26,975
So now we're going to say that this,

854
00:46:26,975 --> 00:46:31,475
by definition, is going to be greater than or equal to R_hat of s,

855
00:46:31,475 --> 00:46:35,605
a, + gamma sum over s_prime,

856
00:46:35,605 --> 00:46:38,085
our empirical transition model,

857
00:46:38,085 --> 00:46:40,800
times the true V_star.

858
00:46:41,600 --> 00:46:44,780
Because that's our induction- inductive hypothesis.

859
00:46:44,780 --> 00:46:47,205
We assume that this held on the previous time-point.

860
00:46:47,205 --> 00:46:51,135
Okay. + beta, divided by square root n(s, a)

861
00:46:51,135 --> 00:46:55,370
this is by my inductive hypothesis.

862
00:46:55,370 --> 00:46:58,630
We assume that we knew, we have a base case where this holds.

863
00:46:58,630 --> 00:47:01,770
We're going to assume that, that held on the previous iteration,

864
00:47:01,770 --> 00:47:05,660
and then the last part that we need in here is that if we,

865
00:47:05,660 --> 00:47:11,460
the- this part, we look at this part versus this part.

866
00:47:11,460 --> 00:47:16,355
Okay? So if you rearrange this equation,

867
00:47:16,355 --> 00:47:21,350
then you can see that R_hat of s,

868
00:47:21,350 --> 00:47:24,360
a, plus all this stuff,

869
00:47:24,360 --> 00:47:28,920
is greater than or equal to Q_star(s, a)

870
00:47:28,920 --> 00:47:31,090
minus beta,

871
00:47:31,090 --> 00:47:33,915
square root n(s, a).

872
00:47:33,915 --> 00:47:40,010
Okay? So that means that if we substitute that back in, back in over here,

873
00:47:40,010 --> 00:47:45,370
we can say this is equal to Q_star(s, a),

874
00:47:45,370 --> 00:47:48,730
minus beta over square n(s, a),

875
00:47:48,730 --> 00:47:54,745
plus beta, square root n(s, a).

876
00:47:54,745 --> 00:47:57,150
Should go to Q_star.

877
00:47:57,150 --> 00:48:03,470
Okay. So now we've shown optimism.

878
00:48:03,990 --> 00:48:08,400
So the key idea in that case was to say,

879
00:48:08,400 --> 00:48:11,915
we know that we're getting to- that,

880
00:48:11,915 --> 00:48:16,205
we're going to relate this to what would happen if we had the true Q_star,

881
00:48:16,205 --> 00:48:19,135
we showed that if we know the true Q_star on the next timestep,

882
00:48:19,135 --> 00:48:21,835
then doing this one step backup, um,

883
00:48:21,835 --> 00:48:25,480
we can bound how far away we'd be from Q_star in terms of our function beta,

884
00:48:25,480 --> 00:48:27,935
and then we can do an inductive,

885
00:48:27,935 --> 00:48:32,250
an inductive proof to show that if we were optimistic on the previous timestep,

886
00:48:32,250 --> 00:48:34,520
we can always ensure that hel- held at the beginning,

887
00:48:34,520 --> 00:48:36,885
because we used optimistic initialization.

888
00:48:36,885 --> 00:48:41,400
Then we'll continue to be optimistic for all the,

889
00:48:41,400 --> 00:48:44,450
for the resulting Q- Q_hat, Q_star.

890
00:48:44,450 --> 00:48:47,380
So this proves optimism.

891
00:48:47,380 --> 00:48:51,105
Anybody who may have questions about that proof?

892
00:48:51,105 --> 00:48:57,200
Okay. So that's the proof of optimism.

893
00:48:57,350 --> 00:49:02,010
The other key part, and I won't go through the other part in quite as much detail,

894
00:49:02,010 --> 00:49:04,520
but I'll, I'll talk about it briefly at a high level.

895
00:49:04,520 --> 00:49:07,510
The other really important part is accuracy.

896
00:49:07,510 --> 00:49:12,925
I mean, bounded, I'll keep this up in case anyone's still writing.

897
00:49:12,925 --> 00:49:15,380
Accuracy is really important, um,

898
00:49:15,380 --> 00:49:20,150
and the fact that you will eventually become accurate is important.

899
00:49:20,150 --> 00:49:25,015
So the- the intuition for this part is that, um,

900
00:49:25,015 --> 00:49:30,010
you can think of defining a couple different,

901
00:49:30,010 --> 00:49:33,355
well, you can think of defining things as being known or unknown.

902
00:49:33,355 --> 00:49:37,640
Okay. Somebody want me to keep this up or is everyone finished writing?

903
00:49:37,640 --> 00:49:40,070
Raise your hand if you'd like it up.

904
00:49:41,750 --> 00:49:45,595
Okay. Okay. So in many, er,

905
00:49:45,595 --> 00:49:48,820
PAC proofs for finite state-action pairs,

906
00:49:48,820 --> 00:49:50,970
there's this notion of knownness.

907
00:49:50,970 --> 00:49:55,680
So what does it mean? So known state-action pairs.

908
00:49:59,000 --> 00:50:04,290
Intuitively known state-action pairs are gonna be pairs for which we have

909
00:50:04,290 --> 00:50:09,765
sufficient data that their estimated models are close to the true model.

910
00:50:09,765 --> 00:50:12,235
So the intuition here,

911
00:50:12,235 --> 00:50:19,455
intuition (s, a) where R-hat

912
00:50:19,455 --> 00:50:23,715
with (s, a) and T-hat of (s, a),

913
00:50:23,715 --> 00:50:28,785
s prime given (s, a) are close

914
00:50:28,785 --> 00:50:34,920
to true R (s, a) and T (s, a).

915
00:50:34,920 --> 00:50:43,245
[NOISE] So intuitively, if you get more and more data for a particular state-action pair,

916
00:50:43,245 --> 00:50:45,030
we know from Hoeffding etc.,

917
00:50:45,030 --> 00:50:48,765
that your estimated mean is gonna converge to the true mean,

918
00:50:48,765 --> 00:50:53,520
and your transition model is also gonna converge to the true tradition model.

919
00:50:53,520 --> 00:50:56,775
And what we're doing here is we're sort of drawing a line in the sand and we're saying,

920
00:50:56,775 --> 00:50:59,220
"When are things- when do we have enough data for a

921
00:50:59,220 --> 00:51:02,970
state-action pair that we are satisfied with our empirical estimates?"

922
00:51:02,970 --> 00:51:07,110
Where we can bound how close our empirical estimates are to the true models.

923
00:51:07,110 --> 00:51:08,970
And if things are close enough,

924
00:51:08,970 --> 00:51:13,140
then we know that using those allow us to compute a near-optimal policy.

925
00:51:13,140 --> 00:51:17,400
[NOISE] So if everything is known,

926
00:51:17,400 --> 00:51:21,790
if all (s, a) pairs are known,

927
00:51:24,470 --> 00:51:33,225
then we can show that the V_Pi star under your sort of empirical model.

928
00:51:33,225 --> 00:51:40,815
I'll denote that as hat. So this is like your empirical model - V_Pi star,

929
00:51:40,815 --> 00:51:44,700
and I'll put this under your empirical model for your- of your true model,

930
00:51:44,700 --> 00:51:46,500
um, that this is bounded.

931
00:51:46,500 --> 00:51:54,360
[NOISE] I kinda go

932
00:51:54,360 --> 00:51:57,030
through all the details so that you saw a little bit of this on the midterm.

933
00:51:57,030 --> 00:51:59,415
This is often known as the simulation lemma.

934
00:51:59,415 --> 00:52:01,740
If things are close, like if your models are close,

935
00:52:01,740 --> 00:52:03,360
your transition model is close to

936
00:52:03,360 --> 00:52:07,245
the true transition model and your reward models are close to the true reward model,

937
00:52:07,245 --> 00:52:10,230
then if you use those to compute a value function,

938
00:52:10,230 --> 00:52:13,650
your value functions are also close, which is a really cool idea.

939
00:52:13,650 --> 00:52:15,480
It's basically saying, "You can propagate the errors in

940
00:52:15,480 --> 00:52:18,990
your empirical model into the errors that you get in your value functions,

941
00:52:18,990 --> 00:52:21,075
and the errors you get in your policies."

942
00:52:21,075 --> 00:52:23,715
So you can sort of propagate error.

943
00:52:23,715 --> 00:52:25,560
You can go propagate,

944
00:52:25,560 --> 00:52:33,210
[NOISE] propagate empirical predictive error

945
00:52:33,210 --> 00:52:40,020
[NOISE] to control error.

946
00:52:40,020 --> 00:52:41,490
[NOISE] Bless you.

947
00:52:41,490 --> 00:52:45,180
[NOISE] Bless you. So this is- this is really nice because

948
00:52:45,180 --> 00:52:48,720
you can say like, if you have good predictive error,

949
00:52:48,720 --> 00:52:51,495
then you can end up with small control error.

950
00:52:51,495 --> 00:52:54,570
And the known state-action pairs are just providing

951
00:52:54,570 --> 00:52:57,450
a way to sort of quantify whether- you know,

952
00:52:57,450 --> 00:53:01,035
what level do you need in order to be good enough,

953
00:53:01,035 --> 00:53:05,610
and the good enough you need is gonna depend on what that epsilon is you want.

954
00:53:05,610 --> 00:53:11,535
So one really important idea is to think about these known state-action pairs, um,

955
00:53:11,535 --> 00:53:17,790
and what they allow us to do in terms of defining some alternative MDPs.

956
00:53:17,790 --> 00:53:21,630
So, in this case,

957
00:53:21,630 --> 00:53:22,980
I will erase this.

958
00:53:22,980 --> 00:53:30,795
[NOISE] Just how you-

959
00:53:30,795 --> 00:53:33,615
so you know how to alter some of the parts of the proof,

960
00:53:33,615 --> 00:53:35,010
um, we'll go forward.

961
00:53:35,010 --> 00:53:36,375
I won't go through all of it in detail,

962
00:53:36,375 --> 00:53:39,090
but I'm very happy to talk about it offline.

963
00:53:39,090 --> 00:53:45,330
Um, [NOISE] so we can define two other sorts of MDPs.

964
00:53:45,330 --> 00:53:49,365
We can ide- call an MDP M prime,

965
00:53:49,365 --> 00:53:53,740
which is equal- which is a MDP,

966
00:53:53,870 --> 00:53:58,119
where for this (s,a) pair,

967
00:53:58,880 --> 00:54:04,200
it's R and T. So its transition and its reward

968
00:54:04,200 --> 00:54:08,580
dynamic- its dynamics and its reward model are equal

969
00:54:08,580 --> 00:54:18,460
to the true MDP on (s, a) in known set,

970
00:54:20,660 --> 00:54:27,810
else, it's equal to the M tilde model.

971
00:54:27,810 --> 00:54:31,650
So these are where we sort of start to define these slightly weird MDPs.

972
00:54:31,650 --> 00:54:33,630
So this is a model that is not quite

973
00:54:33,630 --> 00:54:37,635
our M tilde model that we're using to do in that equation one,

974
00:54:37,635 --> 00:54:42,510
which we have these kind of like rewards plus bonuses plus our empirical transition model.

975
00:54:42,510 --> 00:54:45,045
And it's not quite the real model.

976
00:54:45,045 --> 00:54:48,255
It's saying, on things that are known,

977
00:54:48,255 --> 00:54:50,280
we're gonna use the real-world model.

978
00:54:50,280 --> 00:54:51,690
Again, we don't know what any of these are,

979
00:54:51,690 --> 00:54:53,620
these are just tools for analysis.

980
00:54:53,620 --> 00:54:56,045
And then on state-action pairs which are unknown,

981
00:54:56,045 --> 00:54:58,460
we're gonna use this optimistic model.

982
00:54:58,460 --> 00:55:00,515
It defines an MDP.

983
00:55:00,515 --> 00:55:02,990
Um, uh, and the reason that this is useful as we can

984
00:55:02,990 --> 00:55:05,360
end up using it to help figure out how does

985
00:55:05,360 --> 00:55:11,835
the MDP that we have relate to the real MDP on state-action pairs that are known?

986
00:55:11,835 --> 00:55:14,910
Okay? This is what MDP we end up using for this,

987
00:55:14,910 --> 00:55:16,620
and then the other one is a similar one,

988
00:55:16,620 --> 00:55:18,210
which is M-hat prime,

989
00:55:18,210 --> 00:55:24,075
which is an MDP equal to M-hat.

990
00:55:24,075 --> 00:55:26,760
So this is just the, uh, uh,

991
00:55:26,760 --> 00:55:35,100
empirical- empirical estimates on

992
00:55:35,100 --> 00:55:40,740
K and equal to M tilde on all others.

993
00:55:40,740 --> 00:55:46,030
[NOISE] So on the known set.

994
00:55:46,040 --> 00:55:49,560
Okay? So this is a MDP,

995
00:55:49,560 --> 00:55:51,300
where for the known set,

996
00:55:51,300 --> 00:55:52,680
we use our empirical estimates,

997
00:55:52,680 --> 00:55:55,875
not the true- true est- not the true models,

998
00:55:55,875 --> 00:55:59,500
and then we use M tilde on all of these other ones.

999
00:56:00,590 --> 00:56:05,940
So the idea is that we can use these different forms of MDPs and we qua- can

1000
00:56:05,940 --> 00:56:11,115
quantify how far off the value is that we get by computing,

1001
00:56:11,115 --> 00:56:16,875
uh, using our- our Q tilde versus the value on these other ones,

1002
00:56:16,875 --> 00:56:20,310
and we can use it to basically do a series of inequalities to

1003
00:56:20,310 --> 00:56:25,650
relate the value we get by executing our policy versus the value we get,

1004
00:56:25,650 --> 00:56:27,435
um, in the true world,

1005
00:56:27,435 --> 00:56:30,075
and to the optimal policy.

1006
00:56:30,075 --> 00:56:33,600
So these are the types of tools that allow us to help prove

1007
00:56:33,600 --> 00:56:41,550
accuracy on state-action pairs that we know about. Yeah.

1008
00:56:41,550 --> 00:56:45,015
So is that M-hat and an M tilde?

1009
00:56:45,015 --> 00:56:46,335
Yes.

1010
00:56:46,335 --> 00:56:46,740
Whether.

1011
00:56:46,740 --> 00:56:50,910
It's M-hat. So M-hat here is the empirical estimates like the- um, uh,

1012
00:56:50,910 --> 00:56:52,620
if you use just, um,

1013
00:56:52,620 --> 00:56:55,140
[NOISE] the counts of the rewards and the count of transition model.

1014
00:56:55,140 --> 00:56:58,680
M tilde is the empirical estimates plus the bonus.

1015
00:56:58,680 --> 00:57:02,430
So the transition model in these two cases would be identical,

1016
00:57:02,430 --> 00:57:05,730
but thi- this one would have the reward bonus there.

1017
00:57:05,730 --> 00:57:08,400
Okay. So intuitively, this,

1018
00:57:08,400 --> 00:57:11,310
uh, allows us to help quantify the accuracy.

1019
00:57:11,310 --> 00:57:16,230
Um, [NOISE] the final thing that I guess I just want to mention briefly,

1020
00:57:16,230 --> 00:57:18,570
uh, of how these types of proofs tends to work is that

1021
00:57:18,570 --> 00:57:21,000
we need this bounded learning complexity.

1022
00:57:21,000 --> 00:57:24,060
We need to make sure that we're not gonna continuously update

1023
00:57:24,060 --> 00:57:28,680
our q function and we're not gonna continuously run into unknown state-action pairs.

1024
00:57:28,680 --> 00:57:31,230
So the last part is to sort of, you know,

1025
00:57:31,230 --> 00:57:33,795
how would we prove 3,

1026
00:57:33,795 --> 00:57:37,650
which is bounded learning complexity.

1027
00:57:37,650 --> 00:57:45,120
[NOISE] And the intuition

1028
00:57:45,120 --> 00:57:48,570
for this is a little bit like the general intuition for optimism.

1029
00:57:48,570 --> 00:57:53,715
So the intuition for optimism was to say if you assume the world is awesome,

1030
00:57:53,715 --> 00:57:55,455
either the world is awesome,

1031
00:57:55,455 --> 00:57:57,735
in which case you have low regret.

1032
00:57:57,735 --> 00:57:59,895
In the case of PAC, that means,

1033
00:57:59,895 --> 00:58:02,205
if you assume the world is awesome and it really is awesome,

1034
00:58:02,205 --> 00:58:03,855
that's like not making a mistake.

1035
00:58:03,855 --> 00:58:07,035
That's like picking the really- the- the action that you pick is good.

1036
00:58:07,035 --> 00:58:08,775
So then you won't suffer,

1037
00:58:08,775 --> 00:58:12,165
um, uh, a worse than epsilon decision.

1038
00:58:12,165 --> 00:58:15,120
And then we want to be able to say that the times where you

1039
00:58:15,120 --> 00:58:17,370
don't make mistakes or where you do make mistakes,

1040
00:58:17,370 --> 00:58:18,795
where you pick an action that's bad,

1041
00:58:18,795 --> 00:58:21,930
which is less than epsilon close is bounded,

1042
00:58:21,930 --> 00:58:23,760
and that's what this is about.

1043
00:58:23,760 --> 00:58:27,940
And the key idea here is the pigeonhole principle.

1044
00:58:32,390 --> 00:58:38,910
So the idea is that if you think about- you don't have to have episodic MDPs,

1045
00:58:38,910 --> 00:58:43,365
but if you ha- um, you can think of dividing your stream of experience into episodes.

1046
00:58:43,365 --> 00:58:45,960
And during each, um, sort of episode,

1047
00:58:45,960 --> 00:58:47,520
you can think about whether it's, uh,

1048
00:58:47,520 --> 00:58:50,490
likely that you're gonna run into an unknown state-action pair.

1049
00:58:50,490 --> 00:58:57,480
So you consider what's the probability that we reach an unknown state-action pair?

1050
00:58:57,480 --> 00:59:00,150
And remember, an unknown state-action pair here is one that we don't

1051
00:59:00,150 --> 00:59:03,000
have good models of like we've only visited it once or something.

1052
00:59:03,000 --> 00:59:06,930
So what's the probability we're gonna reach an unknown state-action pair in T steps.

1053
00:59:06,930 --> 00:59:09,010
[NOISE] Okay.

1054
00:59:09,350 --> 00:59:13,485
So what we can show in this case is that if this is low,

1055
00:59:13,485 --> 00:59:15,645
this is small, if small,

1056
00:59:15,645 --> 00:59:17,220
that probability is small,

1057
00:59:17,220 --> 00:59:18,915
we're being near accurate,

1058
00:59:18,915 --> 00:59:21,540
near- we're being near optimal.

1059
00:59:21,540 --> 00:59:23,610
So if- if it's

1060
00:59:23,610 --> 00:59:27,255
a really small probability that you're gonna reach anything that's unknown,

1061
00:59:27,255 --> 00:59:31,125
we can show that on the known state-action pairs you're being near optimal.

1062
00:59:31,125 --> 00:59:35,505
So if you're unlikely to reach anything that where you have bad models of it,

1063
00:59:35,505 --> 00:59:37,305
you're gonna be near optimal.

1064
00:59:37,305 --> 00:59:44,115
If it's large, [NOISE] this can't happen too many times.

1065
00:59:44,115 --> 00:59:45,600
So if it's large you're gonna visit it,

1066
00:59:45,600 --> 00:59:47,430
it can't happen too many times.

1067
00:59:47,430 --> 00:59:51,580
[NOISE].

1068
00:59:51,580 --> 00:59:53,770
Because if this is large, that means that you're really

1069
00:59:53,770 --> 00:59:55,900
likely to reach an unknown state-action pair.

1070
00:59:55,900 --> 00:59:58,405
Remember, I said that for every state-action pair,

1071
00:59:58,405 --> 01:00:01,270
you only update it at most m times.

1072
01:00:01,270 --> 01:00:04,180
So by the Pigeonhole principle,

1073
01:00:04,180 --> 01:00:07,520
this probability cannot stay high for too long.

1074
01:00:07,800 --> 01:00:12,340
Essentially, you're gonna have a function of like the number of states,

1075
01:00:12,340 --> 01:00:13,479
the number of actions,

1076
01:00:13,479 --> 01:00:16,090
and m. It's larger than that.

1077
01:00:16,090 --> 01:00:21,340
But this is saying that you need to be able to visit each state-action pair m times.

1078
01:00:21,340 --> 01:00:25,525
So that, that goes into our end bound here of the times we're gonna make mistakes.

1079
01:00:25,525 --> 01:00:29,175
And we, we're- we might sort of reach things that are unknown.

1080
01:00:29,175 --> 01:00:32,730
Um, it might take us more steps and we might make some bad decisions along the way.

1081
01:00:32,730 --> 01:00:38,760
But essentially, um, things can only be unknown for m steps for each state-action pair,

1082
01:00:38,760 --> 01:00:42,425
which means that our probability here has to be bounded.

1083
01:00:42,425 --> 01:00:47,770
So eventually, everything has to be known or you have to be acting near optimally.

1084
01:00:47,770 --> 01:00:51,355
And that allows us to show that things have,

1085
01:00:51,355 --> 01:00:54,160
uh, that things are PAC.

1086
01:00:54,160 --> 01:00:57,100
It has bounded. We're gonna make a bounded number of bad decisions.

1087
01:00:57,100 --> 01:00:58,960
So either, we're not going to be reaching

1088
01:00:58,960 --> 01:01:01,435
any part of the state-action space which is unknown.

1089
01:01:01,435 --> 01:01:02,710
So everything we reach,

1090
01:01:02,710 --> 01:01:05,740
we have good models of them and we're using them to make good decisions,

1091
01:01:05,740 --> 01:01:07,870
or we are reaching things, and then in that case,

1092
01:01:07,870 --> 01:01:10,990
we're getting information because we're getting a new observation of

1093
01:01:10,990 --> 01:01:14,155
what it's like to be in that state-action pair and we get only,

1094
01:01:14,155 --> 01:01:18,070
something can only continue to be unknown until we get m counts.

1095
01:01:18,070 --> 01:01:22,160
Do you mind putting down the thing again?

1096
01:01:32,670 --> 01:01:41,560
Okay. So that gives us an overview of why MBIE-EB is PAC as well as sort of why a lot of,

1097
01:01:41,560 --> 01:01:44,650
um, the types of proofs that you do to show things are PAC.

1098
01:01:44,650 --> 01:01:46,195
And I think the,

1099
01:01:46,195 --> 01:01:49,930
the key idea in this is really the sort of notion of optimism, and accuracy,

1100
01:01:49,930 --> 01:01:51,519
and the ability to make progress,

1101
01:01:51,519 --> 01:01:53,935
ability not to be stuck always wandering,

1102
01:01:53,935 --> 01:01:57,920
by decreasing these confidence intervals sufficiently fast.

1103
01:01:59,790 --> 01:02:03,070
Okay. So now let's go back to Bayesian-ness.

1104
01:02:03,070 --> 01:02:08,010
So that kind of concludes the PAC MDP part for a while.

1105
01:02:08,010 --> 01:02:10,785
There's been a lot of exciting recent work in this area.

1106
01:02:10,785 --> 01:02:13,650
Um, I guess let me see if I can just really quickly briefly bring this up.

1107
01:02:13,650 --> 01:02:15,920
Um, this is a form of presentation.

1108
01:02:15,920 --> 01:02:19,945
One of the TAs is giving up at Berkeley today on some of our joint work together.

1109
01:02:19,945 --> 01:02:21,700
Just to highlight here, um,

1110
01:02:21,700 --> 01:02:23,740
in terms of sort of PAC and regret analysis,

1111
01:02:23,740 --> 01:02:25,330
there's been a lot of progress, uh,

1112
01:02:25,330 --> 01:02:26,830
on getting better approaches.

1113
01:02:26,830 --> 01:02:29,455
Um, so over the last few years,

1114
01:02:29,455 --> 01:02:31,315
I mean, some of my grad students, some other really

1115
01:02:31,315 --> 01:02:33,235
nice groups who've been doing a lot of work on this,

1116
01:02:33,235 --> 01:02:36,010
and we're also now trying to have an analysis that is problem dependent,

1117
01:02:36,010 --> 01:02:38,380
which means that if the algorithm has more structure,

1118
01:02:38,380 --> 01:02:42,650
then we should need less data in order to learn to make good decisions.

1119
01:02:42,690 --> 01:02:46,675
Okay, so now let's be Bayesian and see how being Bayesian helps us.

1120
01:02:46,675 --> 01:02:50,545
So we saw Bayesian Bandits and in Bayesian Bandits we said,

1121
01:02:50,545 --> 01:02:53,185
we're gonna assume that we have some parametric knowledge

1122
01:02:53,185 --> 01:02:55,465
about how the rewards are distributed.

1123
01:02:55,465 --> 01:02:59,005
So we're going to think about there being a posterior distribution of rewards,

1124
01:02:59,005 --> 01:03:01,480
and we're going to use that to guide exploration.

1125
01:03:01,480 --> 01:03:05,200
And we particularly talked about this notion of probability matching,

1126
01:03:05,200 --> 01:03:08,080
which is we want to select actions with the probability that they are

1127
01:03:08,080 --> 01:03:12,805
optimal and it turns out that Thompson Sampling allowed us to do that.

1128
01:03:12,805 --> 01:03:16,015
So in these, sort of, approaches,

1129
01:03:16,015 --> 01:03:19,344
it was very helpful if we have conjugate priors,

1130
01:03:19,344 --> 01:03:24,970
which allowed us to analytically compute the posterior over the rewards of an arm given

1131
01:03:24,970 --> 01:03:27,385
the data we've observed and our previous prior

1132
01:03:27,385 --> 01:03:31,195
over the probability of different rewards for that arm.

1133
01:03:31,195 --> 01:03:33,985
So we saw as one example of that the Bernoulli,

1134
01:03:33,985 --> 01:03:37,645
which means that the reward is just 0, 1 and the beta distribution,

1135
01:03:37,645 --> 01:03:40,660
and the beta distribution is one where we can think of

1136
01:03:40,660 --> 01:03:44,005
the alphas as being counts of the number of times we've seen a +1,

1137
01:03:44,005 --> 01:03:47,140
and beta is the number of times we've seen the arm being

1138
01:03:47,140 --> 01:03:51,700
0 and as we get observations of one or either of those outcomes,

1139
01:03:51,700 --> 01:03:54,470
then we just update our beta distribution.

1140
01:03:55,200 --> 01:03:58,960
So that allowed us to define Thompson Sampling for bandits,

1141
01:03:58,960 --> 01:04:01,975
which was this algorithm where we say at each time step,

1142
01:04:01,975 --> 01:04:05,680
we first sample a particular reward for each of the different arms,

1143
01:04:05,680 --> 01:04:08,770
and then we sample

1144
01:04:08,770 --> 01:04:11,080
a reward distribution we compute the expectation

1145
01:04:11,080 --> 01:04:13,840
for those reward distributions and we act accordingly.

1146
01:04:13,840 --> 01:04:16,195
So we saw this for the toe example,

1147
01:04:16,195 --> 01:04:19,195
where we saw that we would sample different rewards, um,

1148
01:04:19,195 --> 01:04:21,550
and then use these to act and at least in that example,

1149
01:04:21,550 --> 01:04:23,950
we were seeing that we happen to exploit much faster

1150
01:04:23,950 --> 01:04:27,025
than what we were seeing with an upper confidence bound approach.

1151
01:04:27,025 --> 01:04:30,835
So a very similar thing can be done in the case of MDPs.

1152
01:04:30,835 --> 01:04:35,575
So now in being Bayesian for Models-Based RL,

1153
01:04:35,575 --> 01:04:38,830
we're going to have a distribution over MDP models.

1154
01:04:38,830 --> 01:04:40,930
So what's the difference here?

1155
01:04:40,930 --> 01:04:46,700
That means we're going to have both transitions and rewards.

1156
01:04:48,630 --> 01:04:53,245
So we're going to have the rewards should look very similar to bandits.

1157
01:04:53,245 --> 01:04:58,045
So the rewards, very similar to bandits.

1158
01:04:58,045 --> 01:05:00,055
You can also use betas.

1159
01:05:00,055 --> 01:05:01,750
If your reward distribution is 0, 1,

1160
01:05:01,750 --> 01:05:03,460
you could also use betas and Bernoulli's.

1161
01:05:03,460 --> 01:05:08,900
So this is very similar to bandits. T is a bit different.

1162
01:05:10,220 --> 01:05:14,220
We're not gonna talk a lot about the different distributions you can use.

1163
01:05:14,220 --> 01:05:16,860
But for example if we're in tabular domains,

1164
01:05:16,860 --> 01:05:19,360
T can be a multinomial,

1165
01:05:20,460 --> 01:05:24,920
and the conjugate prior for multinomial is a Dirichlet.

1166
01:05:28,740 --> 01:05:34,090
Its conjugate. So if you want your,

1167
01:05:34,090 --> 01:05:36,550
your transition model to be a multinomial,

1168
01:05:36,550 --> 01:05:39,190
which is the probability over all the other states and actions,

1169
01:05:39,190 --> 01:05:41,620
then a conjugate prior for that is a Dirichlet,

1170
01:05:41,620 --> 01:05:44,140
which has a very nice intuitive,

1171
01:05:44,140 --> 01:05:47,260
um, description similar to what beta is.

1172
01:05:47,260 --> 01:05:49,540
In beta, you could think of this as just

1173
01:05:49,540 --> 01:05:52,135
being the number of times you've observed 1 or 0.

1174
01:05:52,135 --> 01:05:53,410
If you look at a Dirichlet,

1175
01:05:53,410 --> 01:05:56,980
you can think of this as being the number of times you've reached each of the next states,

1176
01:05:56,980 --> 01:05:58,975
so S1 S2 S3 S4.

1177
01:05:58,975 --> 01:06:01,960
So the Dirichlet distribution would be parameterized

1178
01:06:01,960 --> 01:06:06,140
by a vector for one of each of the states.

1179
01:06:06,690 --> 01:06:12,205
And again, we can use this sort of posterior to update it to allow us to do exploration.

1180
01:06:12,205 --> 01:06:14,980
So in this case,

1181
01:06:14,980 --> 01:06:19,495
we're going to sample an MDP from the posterior and then solve it.

1182
01:06:19,495 --> 01:06:21,925
So if we look at what the algorithm looks like,

1183
01:06:21,925 --> 01:06:24,550
it's going to look very similar to Thompson Sampling for Bandits,

1184
01:06:24,550 --> 01:06:27,835
except for now we're going to start off and we have to define,

1185
01:06:27,835 --> 01:06:31,525
um, a prior over the dynamics and reward model for every single state-action pair.

1186
01:06:31,525 --> 01:06:34,450
So notice this is tabular.

1187
01:06:34,450 --> 01:06:40,190
We're assuming that we have a finite set of S and A.

1188
01:06:40,410 --> 01:06:43,075
So we can write this down. We can write down,

1189
01:06:43,075 --> 01:06:44,380
we have one distribution for

1190
01:06:44,380 --> 01:06:49,045
every single state-action pair for both the dynamics and the reward model,

1191
01:06:49,045 --> 01:06:53,155
and then what happens is that you sample an MDP from these distributions.

1192
01:06:53,155 --> 01:06:54,745
So for every single state-action pair,

1193
01:06:54,745 --> 01:06:58,225
you sample the dynamics model and a reward model.

1194
01:06:58,225 --> 01:07:00,324
And now you have an MDP,

1195
01:07:00,324 --> 01:07:02,170
and so once you have that MDP,

1196
01:07:02,170 --> 01:07:04,790
you compute the optimal value for it.

1197
01:07:05,190 --> 01:07:09,475
So this is obviously more computationally intensive than what we had for bandits,

1198
01:07:09,475 --> 01:07:11,515
but it's certainly a reasonable thing to do.

1199
01:07:11,515 --> 01:07:17,350
And then you act optimally with respect to the Q-star you have for that sampled MDP.

1200
01:07:17,350 --> 01:07:20,755
So this is known as Thompson Sampling for MDPs.

1201
01:07:20,755 --> 01:07:25,150
It also implements probability matching and empirically,

1202
01:07:25,150 --> 01:07:27,175
it can often do really well just like

1203
01:07:27,175 --> 01:07:30,920
Thompson Sampling did really well often for bandits.

1204
01:07:31,620 --> 01:07:36,130
So I think that's probably all I'll say about Thompson Sampling for MDPs.

1205
01:07:36,130 --> 01:07:38,680
There's been, uh, a number of different works on this.

1206
01:07:38,680 --> 01:07:42,715
Just to highlight some people that do some really nice work on this from Stanford.

1207
01:07:42,715 --> 01:07:47,830
Ben Van Roy, Roy's group has a lot of work on this and

1208
01:07:47,830 --> 01:07:54,260
sometimes they call it posterior sampling for MDPs.

1209
01:07:55,080 --> 01:07:59,470
So people like- some of his former students like Dan Russo and Ian Osband.

1210
01:07:59,470 --> 01:08:02,155
Ian's now at Deepmind.

1211
01:08:02,155 --> 01:08:04,570
Dan Russo is now at NYU.

1212
01:08:04,570 --> 01:08:08,620
Uh, they've done some really nice work on these types of spaces. Yes?

1213
01:08:08,620 --> 01:08:12,235
[inaudible] generalized these like non-tabular MDPs?

1214
01:08:12,235 --> 01:08:13,495
Yes. Great question.

1215
01:08:13,495 --> 01:08:16,899
So the question was whether or not we can generalize this to non-tabular MDPs?

1216
01:08:16,899 --> 01:08:18,684
Yes, and I'll talk about that in a second.

1217
01:08:18,685 --> 01:08:23,290
But kinda poorly. [LAUGHTER] But yes, that's the goal.

1218
01:08:23,290 --> 01:08:25,779
All right, anybody have any other questions about,

1219
01:08:25,779 --> 01:08:28,074
sort of, finite state and action scenarios?

1220
01:08:28,075 --> 01:08:31,040
Now we are going to talk a little bit about generalization.

1221
01:08:32,040 --> 01:08:37,020
Okay. All right. So of course,

1222
01:08:37,020 --> 01:08:40,470
everything I've just been saying right now is for finite state and action spaces,

1223
01:08:40,470 --> 01:08:44,354
which is not very satisfying because if we think about these types of bounds,

1224
01:08:44,354 --> 01:08:48,554
um, I said this was polynomial in the size of the state and action space.

1225
01:08:48,555 --> 01:08:50,740
So what if S is infinite?

1226
01:08:51,750 --> 01:08:53,950
I mean, it says we can make

1227
01:08:53,950 --> 01:08:56,290
an infinite number of mistakes and that seems sort of unfortunate.

1228
01:08:56,290 --> 01:08:59,274
Um, so it's not clear that this initial, uh,

1229
01:08:59,274 --> 01:09:02,199
framework is is it all helpful when your state space is either

1230
01:09:02,200 --> 01:09:06,130
infinite or insanely large like the space of pixels.

1231
01:09:06,130 --> 01:09:08,545
Um, though- even though,

1232
01:09:08,545 --> 01:09:10,194
the framing of this is really nice,

1233
01:09:10,194 --> 01:09:13,584
we'd like to be able to take these types of ideas up to generalization,

1234
01:09:13,585 --> 01:09:15,490
but we'd like to figure out how to,

1235
01:09:15,490 --> 01:09:18,189
how to use them in a way that can be practical.

1236
01:09:18,189 --> 01:09:22,629
Now, when we start to- so this is a very active area of current research,

1237
01:09:22,630 --> 01:09:25,330
there has been a lot of different ideas about this for the last few years.

1238
01:09:25,330 --> 01:09:27,310
I do want to highlight that on the theory side,

1239
01:09:27,310 --> 01:09:28,720
we still have a long way to go.

1240
01:09:28,720 --> 01:09:32,200
Uh, as we talked about some with function approximation,

1241
01:09:32,200 --> 01:09:34,120
even just function approximation and doing

1242
01:09:34,120 --> 01:09:36,805
control like off policy control like Q learning,

1243
01:09:36,805 --> 01:09:41,665
we said that we didn't have good asymptotic guarantees for some of the basic algorithms.

1244
01:09:41,665 --> 01:09:44,649
Um, so if we don't even have good asymptotic guarantees,

1245
01:09:44,649 --> 01:09:47,274
it's unlikely that we would have really nice finite.

1246
01:09:47,274 --> 01:09:49,959
These are often known as finite sample guarantees because they

1247
01:09:49,960 --> 01:09:53,875
guarantee that the number of mistakes you're gonna make is finite.

1248
01:09:53,875 --> 01:09:56,590
So we have relatively little theory

1249
01:09:56,590 --> 01:09:58,630
in this case it's something that my group is working on.

1250
01:09:58,630 --> 01:10:01,000
There's several other groups that are also working actively on this,

1251
01:10:01,000 --> 01:10:03,115
but there's a lot still to be done.

1252
01:10:03,115 --> 01:10:06,535
But there has been some really nice empirical results recently.

1253
01:10:06,535 --> 01:10:10,165
Okay. So let's think first about generalization and optimism,

1254
01:10:10,165 --> 01:10:14,005
like optimism under uncertainty where we're now in a really really large state space.

1255
01:10:14,005 --> 01:10:18,310
So let's think about what might need to be modified if everything was extremely large.

1256
01:10:18,310 --> 01:10:19,600
If we go back to this algorithm.

1257
01:10:19,600 --> 01:10:23,619
Well first of all, we had this,

1258
01:10:23,619 --> 01:10:25,135
these sort of counts that

1259
01:10:25,135 --> 01:10:31,555
we're keeping track of sort of for every single state-action pair,

1260
01:10:31,555 --> 01:10:33,830
what the counts were.

1261
01:10:33,830 --> 01:10:36,910
So this isn't going to scale because if

1262
01:10:36,910 --> 01:10:39,460
you're in a scenario where your state spaces is pixels,

1263
01:10:39,460 --> 01:10:42,340
you may never see this same set of pixels again.

1264
01:10:42,340 --> 01:10:47,600
Does anybody have any ideas of how you might extend this to the deep learning setting?

1265
01:10:47,640 --> 01:10:52,165
Like what we would like in this case is some way to quantify our uncertainty,

1266
01:10:52,165 --> 01:10:55,045
our uncertainty over sort of states and actions.

1267
01:10:55,045 --> 01:10:58,090
Um, but we don't wanna do it based on like

1268
01:10:58,090 --> 01:11:02,350
raw counts because then everything will just be one forever. Yeah?

1269
01:11:02,350 --> 01:11:05,845
Could you use some form of like bounded VFA?

1270
01:11:05,845 --> 01:11:09,940
So the suggestion was whether we could do some form of like a,

1271
01:11:09,940 --> 01:11:11,275
a VFA for example.

1272
01:11:11,275 --> 01:11:15,265
Um, yes, we could imagine trying to use some form of, uh,

1273
01:11:15,265 --> 01:11:18,460
some sort of density model or some sort of way to try to either get

1274
01:11:18,460 --> 01:11:22,765
an estimate of how much related information have we see,

1275
01:11:22,765 --> 01:11:24,790
or how many related states that we'd seen in this area.

1276
01:11:24,790 --> 01:11:27,885
[inaudible]

1277
01:11:27,885 --> 01:11:35,760
Somewhere like being able to [NOISE] [inaudible] some sort of [inaudible].

1278
01:11:35,760 --> 01:11:37,830
Yeah. So [inaudible] to use some sort of embedding, absolutely.

1279
01:11:37,830 --> 01:11:39,660
So another thing you could do here too is to

1280
01:11:39,660 --> 01:11:42,060
do some sort of- form of compression of the state space.

1281
01:11:42,060 --> 01:11:44,400
One of the challenges [NOISE] is the right compression in

1282
01:11:44,400 --> 01:11:46,965
the state space depends on the decisions you want to make.

1283
01:11:46,965 --> 01:11:48,930
And so generally, it will be non-stationary.

1284
01:11:48,930 --> 01:11:50,130
But that's not necessarily bad.

1285
01:11:50,130 --> 01:11:52,170
You might just want to go back and forth between those.

1286
01:11:52,170 --> 01:11:53,895
So those sort of ideas are great.

1287
01:11:53,895 --> 01:11:57,689
In general, we want some way to quantify our uncertainty that is going to have to generalize,

1288
01:11:57,689 --> 01:11:59,550
and say similar nearby states.

1289
01:11:59,550 --> 01:12:02,415
Um, if we visited that area of the world a lot,

1290
01:12:02,415 --> 01:12:06,060
then we should have less of a bonus on that in terms of optimism.

1291
01:12:06,060 --> 01:12:07,605
[NOISE] Okay.

1292
01:12:07,605 --> 01:12:09,675
So as I said,

1293
01:12:09,675 --> 01:12:11,010
the sort of counts of s, a and s,

1294
01:12:11,010 --> 01:12:14,385
a, s' are not going to be useful if we're only going to encounter things once.

1295
01:12:14,385 --> 01:12:16,650
Um, another thing that I want to highlight is,

1296
01:12:16,650 --> 01:12:19,770
the methods that I was talking about before were really model-based.

1297
01:12:19,770 --> 01:12:21,885
Lot of the work there is model-based.

1298
01:12:21,885 --> 01:12:23,970
In contrast, a lot less of

1299
01:12:23,970 --> 01:12:26,520
the things that- it's starting to change recently over the last year,

1300
01:12:26,520 --> 01:12:29,190
but in general there's been much less work on

1301
01:12:29,190 --> 01:12:33,285
the model-based approaches for deep learning in terms of RL.

1302
01:12:33,285 --> 01:12:34,980
And I think that's because the model-free and

1303
01:12:34,980 --> 01:12:37,635
the policy search techniques have generally been much better.

1304
01:12:37,635 --> 01:12:39,900
In part because the error that you make in

1305
01:12:39,900 --> 01:12:42,900
your models accumulates a lot as you do planning.

1306
01:12:42,900 --> 01:12:46,020
And so I always remember David Silvers' first talk about this,

1307
01:12:46,020 --> 01:12:48,420
or one of his earliest talks about this from, I think,

1308
01:12:48,420 --> 01:12:51,975
2014 where he showed this beautiful sort of model-based,

1309
01:12:51,975 --> 01:12:55,620
um, uh, simulation, which was horrible for planning.

1310
01:12:55,620 --> 01:12:57,780
So the errors really have to be very good,

1311
01:12:57,780 --> 01:12:59,655
uh, the- the errors have to be very small,

1312
01:12:59,655 --> 01:13:02,460
and it's not clear that the representation you get by

1313
01:13:02,460 --> 01:13:06,315
maximizing for predictive loss is always going to be your best for planning.

1314
01:13:06,315 --> 01:13:08,430
So we'd really like to be able to take the ideas we saw

1315
01:13:08,430 --> 01:13:10,920
before for things like MBIE-EB, um,

1316
01:13:10,920 --> 01:13:14,940
and translate them over to the model-free approach, and think about some way to,

1317
01:13:14,940 --> 01:13:19,905
uh, to encode uncertainty in the deep neural network setting.

1318
01:13:19,905 --> 01:13:23,790
So let's think about doing something like Q-learning, uh, like deep

1319
01:13:23,790 --> 01:13:27,210
Q-learning, and in this case- so I've been a little loose here,

1320
01:13:27,210 --> 01:13:28,875
you could- this could be a target.

1321
01:13:28,875 --> 01:13:32,160
So target, could fix this.

1322
01:13:32,160 --> 01:13:36,420
But think about something like sort of the general Q-learning and Q-target, um,

1323
01:13:36,420 --> 01:13:39,450
where we use the max of our current function approximation or we

1324
01:13:39,450 --> 01:13:42,795
use the max of our current target function approximation.

1325
01:13:42,795 --> 01:13:48,150
So one idea for- inspired from MBIE-EB would be simply to include a bonus term.

1326
01:13:48,150 --> 01:13:50,115
So again, this could be,

1327
01:13:50,115 --> 01:13:52,665
you know, a fixed target.

1328
01:13:52,665 --> 01:13:56,309
But the idea here is that when you are updating your parameters,

1329
01:13:56,309 --> 01:13:59,145
just put in a bonus term for

1330
01:13:59,145 --> 01:14:02,040
that particular state-action pair when you're

1331
01:14:02,040 --> 01:14:05,760
doing your Q-learning update or when you're refitting your weights.

1332
01:14:05,760 --> 01:14:08,190
Of course, we have to know what that Q bonus would be,

1333
01:14:08,190 --> 01:14:10,080
but this would help with the planning aspect.

1334
01:14:10,080 --> 01:14:13,200
So now we're in the model-free environment, or model-free setting,

1335
01:14:13,200 --> 01:14:16,260
and so we could- [NOISE] when we're doing our sort of Q-learning updates,

1336
01:14:16,260 --> 01:14:17,625
let's insert a bonus term.

1337
01:14:17,625 --> 01:14:21,405
So that's why I chose MBIE-EB is the algorithm to show you before

1338
01:14:21,405 --> 01:14:23,010
because I think that those sort of

1339
01:14:23,010 --> 01:14:26,700
reward bonuses are much easier to extend to the model-free case.

1340
01:14:26,700 --> 01:14:30,540
Now, of course, the question is what should that reward bonus be?

1341
01:14:30,540 --> 01:14:33,180
It's got to reflect some sort of uncertainty about

1342
01:14:33,180 --> 01:14:35,370
the future reward from that state-action pair.

1343
01:14:35,370 --> 01:14:39,405
And there's a lot of different approaches that have been trying to make progress on this.

1344
01:14:39,405 --> 01:14:41,430
So Mark Bellemare, um,

1345
01:14:41,430 --> 01:14:46,260
[NOISE] and some of the follow-up papers thought about sort of having a density model,

1346
01:14:46,260 --> 01:14:48,000
trying to, er, explicitly estimate

1347
01:14:48,000 --> 01:14:51,210
a density model over the states or state-action pairs that you've visited.

1348
01:14:51,210 --> 01:14:55,439
Um, other people have done sort of hash based approaches,

1349
01:14:55,439 --> 01:14:57,390
which is sort of more similar to the embedding in a way,

1350
01:14:57,390 --> 01:14:58,620
try to hash your state space,

1351
01:14:58,620 --> 01:15:01,260
and then use those- use counts over that hash state-space,

1352
01:15:01,260 --> 01:15:04,500
and then update your hash function over time.

1353
01:15:04,500 --> 01:15:06,630
So that's some of the work that's come out of Berkeley.

1354
01:15:06,630 --> 01:15:07,770
So there- there's different,

1355
01:15:07,770 --> 01:15:10,155
um, different ways to quantify this.

1356
01:15:10,155 --> 01:15:12,120
Another thing I want to highlight here is that

1357
01:15:12,120 --> 01:15:15,150
these bonus terms are generally computed at the time of visit.

1358
01:15:15,150 --> 01:15:16,905
When we looked at MBIE-EB,

1359
01:15:16,905 --> 01:15:18,780
you could recompute these later.

1360
01:15:18,780 --> 01:15:22,950
So if you- if you're storing things in your episodic replay buffer,

1361
01:15:22,950 --> 01:15:25,275
you might want to update those over time,

1362
01:15:25,275 --> 01:15:27,750
because those state-actions,

1363
01:15:27,750 --> 01:15:30,300
if you now- later sample that reward,

1364
01:15:30,300 --> 01:15:32,340
a next state pair from your, uh,

1365
01:15:32,340 --> 01:15:35,385
replay buffer, you may want to change that bonus term.

1366
01:15:35,385 --> 01:15:39,075
So there's a number of different subtleties when we try to bring us up to deep learning,

1367
01:15:39,075 --> 01:15:41,385
but there's been some really encouraging progress.

1368
01:15:41,385 --> 01:15:46,975
Um, so let me just- just- so in this case,

1369
01:15:46,975 --> 01:15:50,795
what we can see is the initial work from Mark Bellemare's group, um,

1370
01:15:50,795 --> 01:15:53,120
where we compare on Montezuma's Revenge,

1371
01:15:53,120 --> 01:15:55,190
which is considered one of the hardest Atari games,

1372
01:15:55,190 --> 01:15:58,910
so the progress of a standard DQN agent that was using e-greedy,

1373
01:15:58,910 --> 01:16:01,400
um, there's a number of different rooms in Montezuma's Revenge.

1374
01:16:01,400 --> 01:16:03,965
In this case, you can see after 50 million frames,

1375
01:16:03,965 --> 01:16:06,420
um, it was doing incredibly badly.

1376
01:16:06,420 --> 01:16:08,055
Don't- it had only been through two of the rooms.

1377
01:16:08,055 --> 01:16:10,440
Whereas if you use this sort of exploration bonus,

1378
01:16:10,440 --> 01:16:11,760
this one did much, much better,

1379
01:16:11,760 --> 01:16:14,850
so just enormously better by being strategic.

1380
01:16:14,850 --> 01:16:16,760
Okay? So I think that highlights

1381
01:16:16,760 --> 01:16:20,360
the empirical significance of doing this sort of strategic exploration.

1382
01:16:20,360 --> 01:16:23,675
Um, let's think briefly about Thompson sampling.

1383
01:16:23,675 --> 01:16:26,150
So in this case,

1384
01:16:26,150 --> 01:16:28,880
one of the ideas that we did a few years ago is to say,

1385
01:16:28,880 --> 01:16:30,440
you could do Thompson sampling both over

1386
01:16:30,440 --> 01:16:33,025
your representation and the parameters of your model.

1387
01:16:33,025 --> 01:16:36,360
What do I mean by that? I mean that if you have a really large state space,

1388
01:16:36,360 --> 01:16:40,200
you can imagine collapsing your states and doing a dynamic state aggregation,

1389
01:16:40,200 --> 01:16:43,095
and sampling over possible state aggregations

1390
01:16:43,095 --> 01:16:46,080
as a way to sort of do collapsing your state space.

1391
01:16:46,080 --> 01:16:51,060
Uh, and- and Thompson sampling can be extended to sampling over representations.

1392
01:16:51,060 --> 01:16:54,495
So that works well, but it doesn't scale up fully.

1393
01:16:54,495 --> 01:16:56,835
When you want to really scale up,

1394
01:16:56,835 --> 01:16:58,485
if you want to be a model-free,

1395
01:16:58,485 --> 01:17:00,480
it's a little bit different than what we saw before.

1396
01:17:00,480 --> 01:17:02,655
Because before we saw model-based approaches,

1397
01:17:02,655 --> 01:17:07,530
and now we're sort of wanting to sample from a posterior over possible Q stars.

1398
01:17:07,530 --> 01:17:09,945
And it's not clear how to write that down.

1399
01:17:09,945 --> 01:17:13,695
I don't think we've made good progress on that even at the tabular setting.

1400
01:17:13,695 --> 01:17:15,480
Uh, but that's what we're trying to do- people who are

1401
01:17:15,480 --> 01:17:17,055
trying to do for the deep learning setting.

1402
01:17:17,055 --> 01:17:18,900
And there's been a couple of different main approaches.

1403
01:17:18,900 --> 01:17:21,720
Uh, one is again sort of from Ian Osband,

1404
01:17:21,720 --> 01:17:23,415
who was here for his PhD.

1405
01:17:23,415 --> 01:17:25,320
They did bootstrap DQN,

1406
01:17:25,320 --> 01:17:28,710
the idea here is that you can use bootstrapping on your samples,

1407
01:17:28,710 --> 01:17:30,885
[NOISE] and you can build a number of different agents.

1408
01:17:30,885 --> 01:17:32,820
So now you kind of have C,

1409
01:17:32,820 --> 01:17:35,085
Q values instead of just one Q value,

1410
01:17:35,085 --> 01:17:37,950
and then you can act optimistically with respect to that set.

1411
01:17:37,950 --> 01:17:40,320
It's not incredibly effective.

1412
01:17:40,320 --> 01:17:42,555
It gives you some performance gain.

1413
01:17:42,555 --> 01:17:45,045
Um, another thing that we did, uh,

1414
01:17:45,045 --> 01:17:47,790
we sort of- we- someone I was working with before,

1415
01:17:47,790 --> 01:17:50,115
I was involved with one of the earlier versions of this,

1416
01:17:50,115 --> 01:17:52,470
um, and since then they've been continuing to push it forward.

1417
01:17:52,470 --> 01:17:54,990
The idea here is that you kind of fix your embedding,

1418
01:17:54,990 --> 01:17:56,715
you do linear regression on top.

1419
01:17:56,715 --> 01:17:58,500
And that is super simple,

1420
01:17:58,500 --> 01:18:00,090
but if you do Bayesian linear regression,

1421
01:18:00,090 --> 01:18:01,454
you get a notion of uncertainty,

1422
01:18:01,454 --> 01:18:05,250
and that actually gives a lot of performance gains in a lot of cases.

1423
01:18:05,250 --> 01:18:07,860
So you can sort of have like a really little bit of

1424
01:18:07,860 --> 01:18:11,505
uncertainty representation on top of a deep neural network.

1425
01:18:11,505 --> 01:18:13,440
But this is an active area.

1426
01:18:13,440 --> 01:18:16,785
There's a lot of people thinking about this different type of work, uh,

1427
01:18:16,785 --> 01:18:18,510
and I think it's going to continue to be

1428
01:18:18,510 --> 01:18:21,090
a really big area because we still haven't made sufficient progress,

1429
01:18:21,090 --> 01:18:24,370
in how to do exploration plus generalization.

1430
01:18:24,500 --> 01:18:26,985
So just to summarize, I know we've done,

1431
01:18:26,985 --> 01:18:28,755
um, quite a lot of theory in this section.

1432
01:18:28,755 --> 01:18:31,500
The things that you should have- should make sure you are familiar with is to

1433
01:18:31,500 --> 01:18:34,950
understand what is the tension between exploration and exploitation in RL?

1434
01:18:34,950 --> 01:18:37,650
Why this doesn't arise in on- other types of settings?

1435
01:18:37,650 --> 01:18:40,530
You should be able to define these different sorts of criteria for what it

1436
01:18:40,530 --> 01:18:43,575
means to be a good algorithm in terms of PAC or regret,

1437
01:18:43,575 --> 01:18:47,100
um, and be able to map the sort of algorithms that

1438
01:18:47,100 --> 01:18:50,550
we've discussed in detail in class to these different forms of criteria.

1439
01:18:50,550 --> 01:18:51,780
So if I say, you know,

1440
01:18:51,780 --> 01:18:53,910
is this optimism under uncertainty approach,

1441
01:18:53,910 --> 01:18:56,010
is that good for PAC or regret or both?

1442
01:18:56,010 --> 01:18:58,305
You should know that it's good for both.

1443
01:18:58,305 --> 01:19:00,570
So that's the kind of high level that you should

1444
01:19:00,570 --> 01:19:02,610
be able to understand from the things we've been doing,

1445
01:19:02,610 --> 01:19:04,050
and just know that there's a lot of

1446
01:19:04,050 --> 01:19:05,820
really exciting work that's continuing to go on there,

1447
01:19:05,820 --> 01:19:08,190
including defining new metrics of performance.

1448
01:19:08,190 --> 01:19:11,380
And next time, we'll hear about meta-learning. Thanks.

